{
  "id": "http://arxiv.org/abs/2201.02135v5",
  "title": "Deep Reinforcement Learning, a textbook",
  "authors": [
    "Aske Plaat"
  ],
  "abstract": "Deep reinforcement learning has gathered much attention recently. Impressive\nresults were achieved in activities as diverse as autonomous driving, game\nplaying, molecular recombination, and robotics. In all these fields, computer\nprograms have taught themselves to solve difficult problems. They have learned\nto fly model helicopters and perform aerobatic manoeuvers such as loops and\nrolls. In some applications they have even become better than the best humans,\nsuch as in Atari, Go, poker and StarCraft. The way in which deep reinforcement\nlearning explores complex environments reminds us of how children learn, by\nplayfully trying out things, getting feedback, and trying again. The computer\nseems to truly possess aspects of human learning; this goes to the heart of the\ndream of artificial intelligence. The successes in research have not gone\nunnoticed by educators, and universities have started to offer courses on the\nsubject. The aim of this book is to provide a comprehensive overview of the\nfield of deep reinforcement learning. The book is written for graduate students\nof artificial intelligence, and for researchers and practitioners who wish to\nbetter understand deep reinforcement learning methods and their challenges. We\nassume an undergraduate-level of understanding of computer science and\nartificial intelligence; the programming language of this book is Python. We\ndescribe the foundations, the algorithms and the applications of deep\nreinforcement learning. We cover the established model-free and model-based\nmethods that form the basis of the field. Developments go quickly, and we also\ncover advanced topics: deep multi-agent reinforcement learning, deep\nhierarchical reinforcement learning, and deep meta learning.",
  "text": "Aske Plaat\nDeep Reinforcement Learning\nApril 25, 2023\nSpringer Nature\narXiv:2201.02135v5  [cs.AI]  23 Apr 2023\nv\nThis is a preprint of the following work:\nAske Plaat,\nDeep Reinforcement Learning,\n2022,\nSpringer Nature,\nreproduced with permission of Springer Nature Singapore Pte Ltd.\nThe ﬁnal authenticated version is available online at: https://doi.org/10.1007/\n978-981-19-0638-1\nPreface\nDeep reinforcement learning has gathered much attention recently. Impressive\nresults were achieved in activities as diverse as autonomous driving, game playing,\nmolecular recombination, and robotics. In all these ﬁelds, computer programs have\nlearned to solve diﬃcult problems. They have learned to ﬂy model helicopters and\nperform aerobatic manoeuvers such as loops and rolls. In some applications they\nhave even become better than the best humans, such as in Atari, Go, poker and\nStarCraft.\nThe way in which deep reinforcement learning explores complex environments\nreminds us how children learn, by playfully trying out things, getting feedback,\nand trying again. The computer seems to truly possess aspects of human learning;\ndeep reinforcement learning touches the dream of artiﬁcial intelligence.\nThe successes in research have not gone unnoticed by educators, and universities\nhave started to oﬀer courses on the subject. The aim of this book is to provide a\ncomprehensive overview of the ﬁeld of deep reinforcement learning. The book is\nwritten for graduate students of artiﬁcial intelligence, and for researchers and prac-\ntitioners who wish to better understand deep reinforcement learning methods and\ntheir challenges. We assume an undergraduate-level of understanding of computer\nscience and artiﬁcial intelligence; the programming language of this book is Python.\nWe describe the foundations, the algorithms and the applications of deep rein-\nforcement learning. We cover the established model-free and model-based methods\nthat form the basis of the ﬁeld. Developments go quickly, and we also cover more\nadvanced topics: deep multi-agent reinforcement learning, deep hierarchical rein-\nforcement learning, and deep meta learning.\nWe hope that learning about deep reinforcement learning will give you as much\njoy as the many researchers experienced when they developed their algorithms,\nﬁnally got them to work, and saw them learn!\nvii\nviii\nAcknowledgments\nThis book beneﬁted from the help of many friends. First of all, I thank everyone at\nthe Leiden Institute of Advanced Computer Science, for creating such a fun and\nvibrant environment to work in.\nMany people contributed to this book. Some material is based on the book\nthat we used in our previous reinforcement learning course and on lecture notes\non policy-based methods written by Thomas Moerland. Thomas also provided\ninvaluable critique on an earlier draft of the book. Furthermore, as this book was\nbeing prepared, we worked on survey articles on deep model-based reinforcement\nlearning, deep meta-learning, and deep multi-agent reinforcement learning. I thank\nMike Preuss, Walter Kosters, Mike Huisman, Jan van Rijn, Annie Wong, Anna\nKononova, and Thomas Bäck, the co-authors on these articles.\nThanks to reader feedback the 2023 version of this book has been updated to\ninclude the Monte Carlo sampling and the n-step methods, and to provide a better\nexplanation of on-policy and oﬀ-policy learning.\nI thank all members of the Leiden reinforcement learning community for their\ninput and enthusiasm. I thank especially Thomas Moerland, Mike Preuss, Matthias\nMüller-Brockhausen, Mike Huisman, Hui Wang, and Zhao Yang, for their help\nwith the course for which this book is written. I thank Wojtek Kowalczyk for\ninsightful discussions on deep supervised learning, and Walter Kosters for his views\non combinatorial search, as well as for his neverending sense of humor.\nA very special thank you goes to Thomas Bäck, for our many discussions on\nscience, the universe, and everything (including, especially, evolution). Without\nyou, this eﬀort would not have been possible.\nThis book is a result of the graduate course on reinforcement learning that we\nteach in Leiden. I thank all students of this course, past, present, and future, for\ntheir wonderful enthusiasm, sharp questions, and many suggestions. This book was\nwritten for you and by you!\nFinally, I thank Saskia, Isabel, Rosalin, Lily, and Dahlia, for being who they are,\nfor giving feedback and letting me learn, and for their boundless love.\nLeiden,\nDecember 2021\nAske Plaat\nContents\n1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.1\nWhat is Deep Reinforcement Learning? . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nThree Machine Learning Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n1.3\nOverview of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2\nTabular Value-Based Reinforcement Learning . . . . . . . . . . . . . . . . . . .\n23\n2.1\nSequential Decision Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n2.2\nTabular Value-Based Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n2.3\nClassic Gym Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n3\nDeep Value-Based Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . .\n67\n3.1\nLarge, High-Dimensional, Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n3.2\nDeep Value-Based Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n3.3\nAtari 2600 Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\n4\nPolicy-Based Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n4.1\nContinuous Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n4.2\nPolicy-Based Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n4.3\nLocomotion and Visuo-Motor Environments . . . . . . . . . . . . . . . . . . . . 115\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n5\nModel-Based Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n5.1\nDynamics Models of High-Dimensional Problems . . . . . . . . . . . . . . . 126\n5.2\nLearning and Planning Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n5.3\nHigh-Dimensional Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\nix\nx\nCONTENTS\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n6\nTwo-Agent Self-Play . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n6.1\nTwo-Agent Zero-Sum Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n6.2\nTabula Rasa Self-Play Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n6.3\nSelf-Play Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n7\nMulti-Agent Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\n7.1\nMulti-Agent Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\n7.2\nMulti-Agent Reinforcement Learning Agents. . . . . . . . . . . . . . . . . . . . 206\n7.3\nMulti-Agent Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\n8\nHierarchical Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n8.1\nGranularity of the Structure of Problems . . . . . . . . . . . . . . . . . . . . . . . 231\n8.2\nDivide and Conquer for Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n8.3\nHierarchical Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n9\nMeta-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n9.1\nLearning to Learn Related Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n9.2\nTransfer Learning and Meta-Learning Agents . . . . . . . . . . . . . . . . . . . 251\n9.3\nMeta-Learning Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n10\nFurther Developments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\n10.1 Development of Deep Reinforcement Learning . . . . . . . . . . . . . . . . . . 275\n10.2 Main Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\n10.3 The Future of Artiﬁcial Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\nA\nMathematical Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nA.1\nSets and Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nA.2\nProbability Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\nA.3\nDerivative of an Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\nA.4\nBellman Equations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\nB\nDeep Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\nB.1\nMachine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\nB.2\nDeep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\nB.3\nDatasets and Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332\nCONTENTS\nxi\nC\nDeep Reinforcement Learning Suites . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\nC.1\nEnvironments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\nC.2\nAgent Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\nC.3\nDeep Learning Suites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\nTables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\nGlossary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389\nxii\nCONTENTS\nContents\n1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.1\nWhat is Deep Reinforcement Learning? . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.1.1\nDeep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.1.2\nReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.1.3\nDeep Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.1.4\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.1.5\nFour Related Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.1.5.1\nPsychology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n1.1.5.2\nMathematics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n1.1.5.3\nEngineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n1.1.5.4\nBiology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n1.2\nThree Machine Learning Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n1.2.1\nSupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n1.2.2\nUnsupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n1.2.3\nReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n1.3\nOverview of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n1.3.1\nPrerequisite Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n1.3.2\nStructure of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n2\nTabular Value-Based Reinforcement Learning . . . . . . . . . . . . . . . . . . .\n23\n2.1\nSequential Decision Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n2.2\nTabular Value-Based Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n2.2.1\nAgent and Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n2.2.2\nMarkov Decision Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n2.2.2.1\nState 𝑆. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n2.2.2.2\nAction 𝐴. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n2.2.2.3\nTransition 𝑇𝑎. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n2.2.2.4\nReward 𝑅𝑎. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n2.2.2.5\nDiscount Factor 𝛾. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n2.2.2.6\nPolicy 𝜋. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n2.2.3\nMDP Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nxiii\nxiv\nContents\n2.2.3.1\nTrace 𝜏. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n2.2.3.2\nState Value 𝑉. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n2.2.3.3\nState-Action Value 𝑄. . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n2.2.3.4\nReinforcement Learning Objective . . . . . . . . . . . . . .\n38\n2.2.3.5\nBellman Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n2.2.4\nMDP Solution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n2.2.4.1\nHands On: Value Iteration in Gym . . . . . . . . . . . . . . .\n41\n2.2.4.2\nModel-Free Learning . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n2.2.4.3\nExploration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n2.2.4.4\nOﬀ-Policy Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n2.2.4.5\nHands On: Q-learning on Taxi . . . . . . . . . . . . . . . . . .\n55\n2.3\nClassic Gym Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n2.3.1\nMountain Car and Cartpole. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n2.3.2\nPath Planning and Board Games . . . . . . . . . . . . . . . . . . . . . . . .\n60\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n3\nDeep Value-Based Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . .\n67\n3.1\nLarge, High-Dimensional, Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n3.1.1\nAtari Arcade Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n3.1.2\nReal-Time Strategy and Video Games. . . . . . . . . . . . . . . . . . . .\n72\n3.2\nDeep Value-Based Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n3.2.1\nGeneralization of Large Problems with Deep Learning . . . .\n73\n3.2.1.1\nMinimizing Supervised Target Loss . . . . . . . . . . . . .\n74\n3.2.1.2\nBootstrapping Q-Values . . . . . . . . . . . . . . . . . . . . . . .\n75\n3.2.1.3\nDeep Reinforcement Learning Target-Error . . . . .\n76\n3.2.2\nThree Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n3.2.2.1\nCoverage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n3.2.2.2\nCorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n3.2.2.3\nConvergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n3.2.3\nStable Deep Value-Based Learning . . . . . . . . . . . . . . . . . . . . . .\n78\n3.2.3.1\nDecorrelating States . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n3.2.3.2\nInfrequent Updates of Target Weights. . . . . . . . . . .\n80\n3.2.3.3\nHands On: DQN and Breakout Gym Example . . . . .\n80\n3.2.4\nImproving Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n3.2.4.1\nOverestimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n3.2.4.2\nDistributional Methods . . . . . . . . . . . . . . . . . . . . . . . .\n87\n3.3\nAtari 2600 Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n3.3.1\nNetwork Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n3.3.2\nBenchmarking Atari . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\nContents\nxv\n4\nPolicy-Based Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n4.1\nContinuous Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n4.1.1\nContinuous Policies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n4.1.2\nStochastic Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n4.1.3\nEnvironments: Gym and MuJoCo . . . . . . . . . . . . . . . . . . . . . . .\n96\n4.1.3.1\nRobotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n4.1.3.2\nPhysics Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n4.1.3.3\nGames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n4.2\nPolicy-Based Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n4.2.1\nPolicy-Based Algorithm: REINFORCE . . . . . . . . . . . . . . . . . . .\n99\n4.2.2\nBias-Variance Trade-Oﬀin Policy-Based Methods . . . . . . . . 102\n4.2.3\nActor Critic Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n4.2.4\nBaseline Subtraction with Advantage Function . . . . . . . . . . . 105\n4.2.5\nTrust Region Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n4.2.6\nEntropy and Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n4.2.7\nDeterministic Policy Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n4.2.8\nHands On: PPO and DDPG MuJoCo Examples . . . . . . . . . . . . . 114\n4.3\nLocomotion and Visuo-Motor Environments . . . . . . . . . . . . . . . . . . . . 115\n4.3.1\nLocomotion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n4.3.2\nVisuo-Motor Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n4.3.3\nBenchmarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n5\nModel-Based Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n5.1\nDynamics Models of High-Dimensional Problems . . . . . . . . . . . . . . . 126\n5.2\nLearning and Planning Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n5.2.1\nLearning the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n5.2.1.1\nModeling Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . 132\n5.2.1.2\nLatent Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n5.2.2\nPlanning with the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n5.2.2.1\nTrajectory Rollouts and Model-Predictive Control 136\n5.2.2.2\nEnd-to-end Learning and Planning-by-Network . 138\n5.3\nHigh-Dimensional Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n5.3.1\nOverview of Model-Based Experiments . . . . . . . . . . . . . . . . . . 141\n5.3.2\nSmall Navigation Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n5.3.3\nRobotic Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n5.3.4\nAtari Games Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n5.3.5\nHands On: PlaNet Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\nxvi\nContents\n6\nTwo-Agent Self-Play . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n6.1\nTwo-Agent Zero-Sum Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n6.1.1\nThe Diﬃculty of Playing Go . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n6.1.2\nAlphaGo Achievements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n6.2\nTabula Rasa Self-Play Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n6.2.1\nMove-Level Self-Play . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\n6.2.1.1\nMinimax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\n6.2.1.2\nMonte Carlo Tree Search . . . . . . . . . . . . . . . . . . . . . . 168\n6.2.2\nExample-Level Self-Play. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\n6.2.2.1\nPolicy and Value Network . . . . . . . . . . . . . . . . . . . . . 176\n6.2.2.2\nStability and Exploration . . . . . . . . . . . . . . . . . . . . . . 176\n6.2.3\nTournament-Level Self-Play . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\n6.2.3.1\nSelf-Play Curriculum Learning . . . . . . . . . . . . . . . . . 179\n6.2.3.2\nSupervised and Reinforcement Curriculum\nLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n6.3\nSelf-Play Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n6.3.1\nHow to Design a World Class Go Program? . . . . . . . . . . . . . . 182\n6.3.2\nAlphaGo Zero Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n6.3.3\nAlphaZero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n6.3.4\nOpen Self-Play Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n6.3.5\nHands On: Hex in Polygames Example . . . . . . . . . . . . . . . . . . . . 188\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n7\nMulti-Agent Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\n7.1\nMulti-Agent Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\n7.1.1\nCompetitive Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200\n7.1.2\nCooperative Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n7.1.3\nMixed Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n7.1.4\nChallenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\n7.1.4.1\nPartial Observability . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n7.1.4.2\nNonstationary Environments . . . . . . . . . . . . . . . . . . 205\n7.1.4.3\nLarge State Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n7.2\nMulti-Agent Reinforcement Learning Agents. . . . . . . . . . . . . . . . . . . . 206\n7.2.1\nCompetitive Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n7.2.1.1\nCounterfactual Regret Minimization . . . . . . . . . . . . 207\n7.2.1.2\nDeep Counterfactual Regret Minimization . . . . . . . 208\n7.2.2\nCooperative Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\n7.2.2.1\nCentralized Training/Decentralized Execution . . . 210\n7.2.2.2\nOpponent Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n7.2.2.3\nCommunication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n7.2.2.4\nPsychology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n7.2.3\nMixed Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n7.2.3.1\nEvolutionary Algorithms . . . . . . . . . . . . . . . . . . . . . . 213\n7.2.3.2\nSwarm Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\nContents\nxvii\n7.2.3.3\nPopulation-Based Training . . . . . . . . . . . . . . . . . . . . . 216\n7.2.3.4\nSelf-Play Leagues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n7.3\nMulti-Agent Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n7.3.1\nCompetitive Behavior: Poker . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n7.3.2\nCooperative Behavior: Hide and Seek. . . . . . . . . . . . . . . . . . . . 220\n7.3.3\nMixed Behavior: Capture the Flag and StarCraft . . . . . . . . . . 222\n7.3.4\nHands On: Hide and Seek in the Gym Example . . . . . . . . . . . . 224\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\n8\nHierarchical Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n8.1\nGranularity of the Structure of Problems . . . . . . . . . . . . . . . . . . . . . . . 231\n8.1.1\nAdvantages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231\n8.1.2\nDisadvantages. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n8.2\nDivide and Conquer for Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n8.2.1\nThe Options Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n8.2.2\nFinding Subgoals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\n8.2.3\nOverview of Hierarchical Algorithms. . . . . . . . . . . . . . . . . . . . 235\n8.2.3.1\nTabular Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236\n8.2.3.2\nDeep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236\n8.3\nHierarchical Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\n8.3.1\nFour Rooms and Robot Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\n8.3.2\nMontezuma’s Revenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240\n8.3.3\nMulti-Agent Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\n8.3.4\nHands On: Hierarchical Actor Citic Example . . . . . . . . . . . . . . 242\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n9\nMeta-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n9.1\nLearning to Learn Related Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n9.2\nTransfer Learning and Meta-Learning Agents . . . . . . . . . . . . . . . . . . . 251\n9.2.1\nTransfer Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\n9.2.1.1\nTask Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\n9.2.1.2\nPretraining and Finetuning . . . . . . . . . . . . . . . . . . . . 253\n9.2.1.3\nHands-on: Pretraining Example . . . . . . . . . . . . . . . . . 253\n9.2.1.4\nMulti-task Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n9.2.1.5\nDomain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . 255\n9.2.2\nMeta-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\n9.2.2.1\nEvaluating Few-Shot Learning Problems . . . . . . . . 257\n9.2.2.2\nDeep Meta-Learning Algorithms . . . . . . . . . . . . . . . 258\n9.2.2.3\nRecurrent Meta-Learning . . . . . . . . . . . . . . . . . . . . . . 260\n9.2.2.4\nModel-Agnostic Meta-Learning . . . . . . . . . . . . . . . . 261\n9.2.2.5\nHyperparameter Optimization . . . . . . . . . . . . . . . . . 263\n9.2.2.6\nMeta-Learning and Curriculum Learning . . . . . . . . 264\n9.2.2.7\nFrom Few-Shot to Zero-Shot Learning . . . . . . . . . . 264\nxviii\nContents\n9.3\nMeta-Learning Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265\n9.3.1\nImage Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\n9.3.2\nNatural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\n9.3.3\nMeta-Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\n9.3.4\nMeta-World . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\n9.3.5\nAlchemy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\n9.3.6\nHands-on: Meta-World Example . . . . . . . . . . . . . . . . . . . . . . . . . 270\nSummary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n10\nFurther Developments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\n10.1 Development of Deep Reinforcement Learning . . . . . . . . . . . . . . . . . . 275\n10.1.1 Tabular Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n10.1.2 Model-free Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n10.1.3 Multi-Agent Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n10.1.4 Evolution of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . 277\n10.2 Main Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\n10.2.1 Latent Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279\n10.2.2 Self-Play . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279\n10.2.3 Hierarchical Reinforcement Learning . . . . . . . . . . . . . . . . . . . . 279\n10.2.4 Transfer Learning and Meta-Learning . . . . . . . . . . . . . . . . . . . 280\n10.2.5 Population-Based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280\n10.2.6 Exploration and Intrinsic Motivation . . . . . . . . . . . . . . . . . . . . 281\n10.2.7 Explainable AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n10.2.8 Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n10.3 The Future of Artiﬁcial Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\nA\nMathematical Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nA.1\nSets and Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nA.1.1\nSets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nA.1.2\nFunctions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\nA.2\nProbability Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\nA.2.1\nDiscrete Probability Distributions . . . . . . . . . . . . . . . . . . . . . . . 290\nA.2.2\nContinuous Probability Distributions . . . . . . . . . . . . . . . . . . . . 291\nA.2.3\nConditional Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\nA.2.4\nExpectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294\nA.2.4.1\nExpectation of a Random Variable . . . . . . . . . . . . . . 294\nA.2.4.2\nExpectation of a Function of a Random Variable . 295\nA.2.5\nInformation Theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\nA.2.5.1\nInformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\nA.2.5.2\nEntropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\nA.2.5.3\nCross-entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297\nA.2.5.4\nKullback-Leibler Divergence . . . . . . . . . . . . . . . . . . . 297\nA.3\nDerivative of an Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\nA.4\nBellman Equations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\nContents\nxix\nB\nDeep Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\nB.1\nMachine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\nB.1.1\nTraining Set and Test Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302\nB.1.2\nCurse of Dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303\nB.1.3\nOverﬁtting and the Bias-Variance Trade-Oﬀ. . . . . . . . . . . . . . 304\nB.2\nDeep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\nB.2.1\nWeights, Neurons. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308\nB.2.2\nBackpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\nB.2.3\nEnd-to-end Feature Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 311\nB.2.4\nConvolutional Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314\nB.2.5\nRecurrent Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\nB.2.6\nMore Network Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\nB.2.7\nOverﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\nB.3\nDatasets and Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\nB.3.1\nMNIST and ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\nB.3.2\nGPU Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\nB.3.3\nHands On: Classiﬁcation Example . . . . . . . . . . . . . . . . . . . . . . . . 328\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332\nC\nDeep Reinforcement Learning Suites . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\nC.1\nEnvironments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\nC.2\nAgent Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\nC.3\nDeep Learning Suites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\nTables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\nGlossary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389\nChapter 1\nIntroduction\nDeep reinforcement learning studies how we learn to solve complex problems,\nproblems that require us to ﬁnd a solution to a sequence of decisions in high\ndimensional states. To make bread, we must use the right ﬂour, add some salt, yeast\nand sugar, prepare the dough (not too dry and not too wet), pre-heat the oven to\nthe right temperature, and bake the bread (but not too long); to win a ballroom\ndancing contest we must ﬁnd the right partner, learn to dance, practice, and beat\nthe competition; to win in chess we must study, practice, and make all the right\nmoves.\n1.1 What is Deep Reinforcement Learning?\nDeep reinforcement learning is the combination of deep learning and reinforcement\nlearning.\nThe goal of deep reinforcement learning is to learn optimal actions that maximize\nour reward for all states that our environment can be in (the bakery, the dance\nhall, the chess board). We do this by interacting with complex, high-dimensional\nenvironments, trying out actions, and learning from the feedback.\nThe ﬁeld of deep learning is about approximating functions in high-dimensional\nproblems; problems that are so complex that tabular methods cannot ﬁnd exact\nsolutions anymore. Deep learning uses deep neural networks to ﬁnd approximations\nfor large, complex, high-dimensional environments, such as in image and speech\nrecognition. The ﬁeld has made impressive progress; computers can now recog-\nnize pedestrians in a sequence of images (to avoid running over them), and can\nunderstand sentences such as: “What is the weather going to be like tomorrow?”\nThe ﬁeld of reinforcement learning is about learning from feedback; it learns by\ntrial and error. Reinforcement learning does not need a pre-existing dataset to train\non; it chooses its own actions, and learns from the feedback that an environment\nprovides. It stands to reason that in this process of trial and error, our agent will\nmake mistakes (the ﬁre extinguisher is essential to survive the process of learning\n1\n2\n1 Introduction\nLow-Dimensional States\nHigh-Dimensional States\nStatic Dataset\nclassic supervised learning\ndeep supervised learning\nAgent/Environment Interaction tabular reinforcement learning deep reinforcement learning\nTable 1.1 The Constituents of Deep Reinforcement Learning\nto bake bread). The ﬁeld of reinforcement learning is all about learning from success\nas well as from mistakes.\nIn recent years the two ﬁelds of deep and reinforcement learning have come\ntogether, and have yielded new algorithms, that are able to approximate high-\ndimensional problems by feedback on their actions. Deep learning has brought new\nmethods and new successes, with advances in policy-based methods, in model-\nbased approaches, in transfer learning, in hierarchical reinforcement learning, and\nin multi-agent learning.\nThe ﬁelds also exist separately, as deep supervised learning and as tabular re-\ninforcement learning (see Table 1.1). The aim of deep supervised learning is to\ngeneralize and approximate complex, high-dimensional, functions from pre-existing\ndatasets, without interaction; Appendix B discusses deep supervised learning. The\naim of tabular reinforcement learning is to learn by interaction in simpler, low-\ndimensional, environments such as Grid worlds; Chap. 2 discusses tabular reinforce-\nment learning.\nLet us have a closer look at the two ﬁelds.\n1.1.1 Deep Learning\nClassic machine learning algorithms learn a predictive model on data, using methods\nsuch as linear regression, decision trees, random forests, support vector machines,\nand artiﬁcial neural networks. The models aim to generalize, to make predictions.\nMathematically speaking, machine learning aims to approximate a function from\ndata.\nIn the past, when computers were slow, the neural networks that were used\nconsisted of a few layers of fully connected neurons, and did not perform exception-\nally well on diﬃcult problems. This changed with the advent of deep learning and\nfaster computers. Deep neural networks now consist of many layers of neurons and\nuse diﬀerent types of connections.1 Deep networks and deep learning have taken\nthe accuracy of certain important machine learning tasks to a new level, and have\nallowed machine learning to be applied to complex, high-dimensional, problems,\nsuch as recognizing cats and dogs in high-resolution (mega-pixel) images.\nDeep learning allows high-dimensional problems to be solved in real-time; it\nhas allowed machine learning to be applied to day-to-day tasks such as the face-\nrecognition and speech-recognition that we use in our smartphones.\n1 Where many means more than one hidden layer in between the input and output layer.\n1.1 What is Deep Reinforcement Learning?\n3\n1.1.2 Reinforcement Learning\nLet us look more deeply at reinforcement learning, to see what it means to learn\nfrom our own actions.\nReinforcement learning is a ﬁeld in which an agent learns by interacting with\nan environment. In supervised learning we need pre-existing datasets of labeled\nexamples to approximate a function; reinforcement learning only needs an environ-\nment that provides feedback signals for actions that the agent is trying out. This\nrequirement is easier to fulﬁll, allowing reinforcement learning to be applicable to\nmore situations than supervised learning.\nReinforcement learning agents generate, by their actions, their own on-the-ﬂy\ndata, through the environment’s rewards. Agents can choose which actions to\nlearn from; reinforcement learning is a form of active learning. In this sense, our\nagents are like children, that, through playing and exploring, teach themselves a\ncertain task. This level of autonomy is one of the aspects that attracts researchers\nto the ﬁeld. The reinforcement learning agent chooses which action to perform—\nwhich hypothesis to test—and adjusts its knowledge of what works, building up a\npolicy of actions that are to be performed in the diﬀerent states of the world that it\nhas encountered. (This freedom is also what makes reinforcement learning hard,\nbecause when you are allowed to choose your own examples, it is all too easy to\nstay in your comfort zone, stuck in a positive reinforcement bubble, believing you\nare doing great, but learning very little of the world around you.)\n1.1.3 Deep Reinforcement Learning\nDeep reinforcement learning combines methods for learning high-dimensional prob-\nlems with reinforcement learning, allowing high-dimensional, interactive learning.\nA major reason for the interest in deep reinforcement learning is that it works well\non current computers, and does so in seemingly diﬀerent applications. For exam-\nple, in Chap. 3 we will see how deep reinforcement learning can learn eye-hand\ncoordination tasks to play 1980s video games, in Chap. 4 we see how a simulated\nrobot cheetah learns to jump, and in Chap. 6 we see how it can teach itself to play\ncomplex games of strategy to the extent that world champions are beaten.\nLet us have a closer look at the kinds of applications on which deep reinforcement\nlearning does so well.\n1.1.4 Applications\nIn its most basic form, reinforcement learning is a way to teach an agent to operate in\nthe world. As a child learns to walk from actions and feedback, so do reinforcement\nlearning agents learn from actions and feedback. Deep reinforcement learning can\n4\n1 Introduction\nlearn to solve large and complex decision problems—problems whose solution is\nnot yet known, but for which an approximating trial-and-error mechanism exists\nthat can learn a solution out of repeated interactions with the problem. This may\nsound a bit cryptical and convoluted, but approximation and trial and error are\nsomething that we do in real life all the time. Generalization and approximation\nallow us to infer patterns or rules from examples. Trial and error is a method by\nwhich humans learn how to deal with things that are unfamiliar to them (“What\nhappens if I press this button? Oh. Oops.” Or: “What happens if I do not put my leg\nbefore my other leg while moving forward? Oh. Ouch.”).\nSequential Decision Problems\nLearning to operate in the world is a high level goal; we can be more speciﬁc.\nReinforcement learning is about the agent’s behavior. Reinforcement learning can\nﬁnd solutions for sequential decision problems, or optimal control problems, as they\nare known in engineering. There are many situations in the real world where, in\norder to reach a goal, a sequence of decisions must be made. Whether it is baking\na cake, building a house, or playing a card game; a sequence of decisions has\nto be made. Reinforcement learning provides eﬃcient ways to learn solutions to\nsequential decision problems.\nMany real world problems can be modeled as a sequence of decisions [544]. For\nexample, in autonomous driving, an agent is faced with questions of speed control,\nﬁnding drivable areas, and, most importantly, avoiding collisions. In healthcare,\ntreatment plans contain many sequential decisions, and factoring the eﬀects of\ndelayed treatment can be studied. In customer centers, natural language process-\ning can help improve chatbot dialogue, question answering, and even machine\ntranslation. In marketing and communication, recommender systems recommend\nnews, personalize suggestions, deliver notiﬁcations to user, or otherwise optimize\nthe product experience. In trading and ﬁnance, systems decide to hold, buy or\nsell ﬁnancial titles, in order to optimize future reward. In politics and governance,\nthe eﬀects of policies can be simulated as a sequence of decisions before they are\nimplemented. In mathematics and entertainment, playing board games, card games,\nand strategy games consists of a sequence of decisions. In computational creativity,\nmaking a painting requires a sequence of esthetic decisions. In industrial robotics\nand engineering, the grasping of items and the manipulation of materials consists of\na sequence of decisions. In chemical manufacturing, the optimization of production\nprocesses consists of many decision steps, that inﬂuence the yield and quality of\nthe product. Finally, in energy grids, the eﬃcient and safe distribution of energy\ncan be modeled as a sequential decision problem.\nIn all these situations, we must make a sequence of decisions. In all these situa-\ntions, taking the wrong decision can be very costly.\nThe algorithmic research on sequential decision making has focused on two\ntypes of applications: (1) robotic problems and (2) games. Let us have a closer look\nat these two domains, starting with robotics.\n1.1 What is Deep Reinforcement Learning?\n5\nFig. 1.1 Robot Flipping Pancakes [423]\nFig. 1.2 Aerobatic Model Helicopter [3]\nRobotics\nIn principle, all actions that a robot should take can be pre-programmed step-by-step\nby a programmer in meticulous detail. In highly controlled environments, such as\na welding robot in a car factory, this can conceivably work, although any small\nchange or any new task requires reprogramming the robot.\nIt is surprisingly hard to manually program a robot to perform a complex task.\nHumans are not aware of their own operational knowledge, such as what “voltages”\nwe put on which muscles when we pick up a cup. It is much easier to deﬁne a desired\ngoal state, and let the system ﬁnd the complicated solution by itself. Furthermore,\nin environments that are only slightly challenging, when the robot must be able to\nrespond more ﬂexibly to diﬀerent conditions, an adaptive program is needed.\nIt will be no surprise that the application area of robotics is an important driver\nfor machine learning research, and robotics researchers turned early on to ﬁnding\nmethods by which the robots could teach themselves certain behavior.\n6\n1 Introduction\nFig. 1.3 Chess\nFig. 1.4 Go\nThe literature on robotics experiments is varied and rich. A robot can teach itself\nhow to navigate a maze, how to perform manipulation tasks, and how to learn\nlocomotion tasks.\nResearch into adaptive robotics has made quite some progress. For example,\none of the recent achievements involves ﬂipping pancakes [423] and ﬂying an\naerobatic model helicopter [2, 3]; see Figs. 1.1 and 1.2. Frequently, learning tasks are\ncombined with computer vision, where a robot has to learn by visually interpreting\nthe consequences of its own actions.\nGames\nLet us now turn to games. Puzzles and games have been used from the earliest days\nto study aspects of intelligent behavior. Indeed, before computers were powerful\nenough to execute chess programs, in the days of Shannon and Turing, paper\n1.1 What is Deep Reinforcement Learning?\n7\nFig. 1.5 Pac-Man [71]\nFig. 1.6 StarCraft [813]\ndesigns were made, in the hope that understanding chess would teach us something\nabout the nature of intelligence [694, 788].\nGames allow researchers to limit the scope of their studies, to focus on intelli-\ngent decision making in a limited environment, without having to master the full\ncomplexity of the real world. In addition to board games such as chess and Go,\nvideo games are being used extensively to test intelligent methods in computers.\nExamples are Arcade-style games such as Pac-Man [523] and multi-player strategy\ngames such as StarCraft [813]. See Figs. 1.3–1.6.\n1.1.5 Four Related Fields\nReinforcement learning is a rich ﬁeld, that has existed in some form long before the\nartiﬁcial intelligence endeavour had started, as a part of biology, psychology, and\neducation [86, 389, 743]. In artiﬁcial intelligence it has become one of the three main\ncategories of machine learning, the other two being supervised and unsupervised\nlearning [93]. This book is a book of algorithms that are inspired by topics from\nthe natural and social sciences. Although the rest of the book will be about these\n8\n1 Introduction\nFig. 1.7 Classical Conditioning: (1) a dog salivates when seeing food, (2) but initially not when\nhearing a bell, (3) when the sound rings often enough together when food is served, the dog starts\nto associate the bell with food, and (4) also salivates when only the bell rings\nalgorithms, it is interesting to brieﬂy discuss the links of deep reinforcement learning\nto human and animal learning. We will introduce the four scientiﬁc disciplines that\nhave a profound inﬂuence on deep reinforcement learning.\n1.1.5.1 Psychology\nIn psychology, reinforcement learning is also known as learning by conditioning or\nas operant conditioning. Figure 1.7 illustrates the folk psychological idea of how a\ndog can be conditioned. A natural reaction to food is that a dog salivates. By ringing\na bell whenever the dog is given food, the dog learns to associate the sound with\nfood, and after enough trials, the dog starts salivating as soon as it hears the bell,\npresumably in anticipation of the food, whether it is there or not.\nThe behavioral scientists Pavlov (1849–1936) and Skinner (1904–1990) are well-\nknown for their work on conditioning. Phrases such as Pavlov-reaction have entered\nour everyday language, and various jokes about conditioning exist (see, for example,\nFig. 1.8). Psychological research into learning is one of the main inﬂuences on\nreinforcement learning as we know it in artiﬁcial intelligence.\n1.1 What is Deep Reinforcement Learning?\n9\nFig. 1.8 Who is Conditioning Whom?\n1.1.5.2 Mathematics\nMathematical logic is another foundation of deep reinforcement learning. Discrete\noptimization and graph theory are of great importance for the formalization of\nreinforcement learning, as we will see in Sect. 2.2.2 on Markov decision processes.\nMathematical formalizations have enabled the development of eﬃcient planning\nand optimization algorithms, that are at the core of current progress.\nPlanning and optimization are an important part of deep reinforcement learning.\nThey are also related to the ﬁeld of operations research, although there the emphasis\nis on (non-sequential) combinatorial optimization problems. In AI, planning and\noptimization are used as building blocks for creating learning systems for sequential,\nhigh-dimensional, problems that can include visual, textual or auditory input.\nThe ﬁeld of symbolic reasoning is based on logic, it is one of the earliest success\nstories in artiﬁcial intelligence. Out of work in symbolic reasoning came heuristic\nsearch [593], expert systems, and theorem proving systems. Well-known systems\nare the STRIPS planner [242], the Mathematica computer algebra system [119], the\nlogic programming language PROLOG [157], and also systems such as SPARQL for\nsemantic (web) reasoning [24, 81].\nSymbolic AI focuses on reasoning in discrete domains, such as decision trees,\nplanning, and games of strategy, such as chess and checkers. Symbolic AI has driven\nsuccess in methods to search the web, to power online social networks, and to power\nonline commerce. These highly successful technologies are the basis of much of\nour modern society and economy. In 2011 the highest recognition in computer\nscience, the Turing award, was awarded to Judea Pearl for work in causal reasoning\n(Fig. 1.9).2 Pearl later published an inﬂuential book to popularize the ﬁeld [594].\nAnother area of mathematics that has played a large role in deep reinforcement\nlearning is the ﬁeld of continuous (numerical) optimization. Continuous methods are\n2 Joining a long list of AI researchers that have been honored earlier with a Turing award: Minsky,\nMcCarthy, Newell, Simon, Feigenbaum and Reddy.\n10\n1 Introduction\nFig. 1.9 Turing-award winner Judea Pearl\nFig. 1.10 Optimal Control of Dynamical Systems at Work\nimportant, for example, in eﬃcient gradient descent and backpropagation methods\nthat are at the heart of current deep learning algorithms.\n1.1.5.3 Engineering\nIn engineering, the ﬁeld of reinforcement learning is better known as optimal control.\nThe theory of optimal control of dynamical systems was developed by Richard\nBellman and Lev Pontryagin [85]. Optimal control theory originally focused on\ndynamical systems, and the technology and methods relate to continuous optimiza-\ntion methods such as used in robotics (see Fig. 1.10 for an illustration of optimal\ncontrol at work in docking two space vehicles). Optimal control theory is of central\nimportance to many problems in engineering.\nTo this day reinforcement learning and optimal control use a diﬀerent termi-\nnology and notation. States and actions are denoted as 𝑠and 𝑎in state-oriented\n1.1 What is Deep Reinforcement Learning?\n11\nFig. 1.11 Turing-award winners Geoﬀrey Hinton, Yann LeCun, and Yoshua Bengio\nreinforcement learning, where the engineering world of optimal control uses 𝑥and\n𝑢. In this book the former notation is used.\n1.1.5.4 Biology\nBiology has a profound inﬂuence on computer science. Many nature-inspired opti-\nmization algorithms have been developed in artiﬁcial intelligence. An important\nnature-inspired school of thought is connectionist AI.\nMathematical logic and engineering approach intelligence as a top-down de-\nductive process; observable eﬀects in the real world follow from the application of\ntheories and the laws of nature, and intelligence follows deductively from theory.\nIn contrast, connectionism approaches intelligence in a bottom-up fashion. Connec-\ntionist intelligence emerges out of many low level interactions. Intelligence follows\ninductively from practice. Intelligence is embodied: the bees in bee hives, the ants\nin ant colonies, and the neurons in the brain all interact, and out of the connections\nand interactions arises behavior that we recognize as intelligent [97].\nExamples of the connectionist approach to intelligence are nature-inspired al-\ngorithms such as Ant colony optimization [209], swarm intelligence [406, 97],\nevolutionary algorithms [43, 252, 347], robotic intelligence [109], and, last but not\nleast, artiﬁcial neural networks and deep learning [318, 459, 280].\nIt should be noted that both the symbolic and the connectionist school of AI\nhave been very successful. After the enormous economic impact of search and\nsymbolic AI (Google, Facebook, Amazon, Netﬂix), much of the interest in AI in the\nlast two decades has been inspired by the success of the connectionist approach\nin computer language and vision. In 2018 the Turing award was awarded to three\n12\n1 Introduction\nkey researchers in deep learning: Bengio, Hinton, and LeCun (Fig. 1.11). Their most\nfamous paper on deep learning may well be [459].\n1.2 Three Machine Learning Paradigms\nNow that we have introduced the general context and origins of deep reinforcement\nlearning, let us switch gears, and talk about machine learning. Let us see how deep\nreinforcement learning ﬁts in the general picture of the ﬁeld. At the same time, we\nwill take the opportunity to introduce some notation and basic concepts.\nIn the next section we will then provide an outline of the book. But ﬁrst it is\ntime for machine learning. We start at the beginning, with function approximation.\nRepresenting a Function\nFunctions are a central part in artiﬁcial intelligence. A function 𝑓transforms input 𝑥\nto output 𝑦according to some method, and we write 𝑓(𝑥) →𝑦. In order to perform\ncalculations with function 𝑓, the function must be represented as a computer\nprogram in some form in memory. We also write function\n𝑓: 𝑋→𝑌,\nwhere the domain 𝑋and range 𝑌can be discrete or continuous; the dimensionality\n(number of attributes in 𝑋) can be arbitrary.\nOften, in the real world, the same input may yield a range of diﬀerent outputs,\nand we would like our function to provide a conditional probability distribution, a\nfunction that maps\n𝑓: 𝑋→𝑝(𝑌).\nHere the function maps the domain to a probability distribution 𝑝over the range.\nRepresenting a conditional probability allows us to model functions for which the in-\nput does not always give the same output. (Appendix A provides more mathematical\nbackground.)\nGiven versus Learned Function\nSometimes the function that we are interested in is given, and we can represent\nthe function by a speciﬁc algorithm that computes an analytical expression that is\nknown exactly. This is, for example, the case for laws of physics, or when we make\nexplicit assumptions for a particular system.\n1.2 Three Machine Learning Paradigms\n13\nFig. 1.12 Example of learning a function; data points are in blue, a possible learned linear function\nis the red line, which allows us to make predictions ˆ𝑦for any new input 𝑥\nExample: Newton’s second Law of Motion states that for objects with\nconstant mass\n𝐹= 𝑚· 𝑎,\nwhere 𝐹denotes the net force on the object, 𝑚denotes its mass, and 𝑎\ndenotes its acceleration. In this case, the analytical expression deﬁnes the\nentire function, for every possible combination of the inputs.\nHowever, for many functions in the real world, we do not have an analytical\nexpression. Here, we enter the realm of machine learning, in particular of supervised\nlearning. When we do not know an analytical expression for a function, our best\napproach is to collect data—examples of (𝑥, 𝑦) pairs—and reverse engineer or learn\nthe function from this data. See Fig. 1.12.\nExample: A company wants to predict the chance that you buy a shampoo\nto color your hair, based on your age. They collect many data points of\n𝑥∈N, your age (a natural number), that map to 𝑦∈{0, 1}, a binary indicator\nwhether you bought their shampoo. They then want to learn the mapping\nˆ𝑦= 𝑓(𝑥)\nwhere 𝑓is the desired function that tells the company who will buy the\nproduct and ˆ𝑦is the predicted 𝑦(admittedly overly simplistic in this exam-\nple).\nLet us see which methods exist in machine learning to ﬁnd function approxima-\ntions.\n14\n1 Introduction\nThree Paradigms\nThere are three main paradigms for how the observations can be provided in\nmachine learning: (1) supervised learning, (2) reinforcement learning, and (3) unsu-\npervised learning.\n1.2.1 Supervised Learning\nThe ﬁrst and most basic paradigm for machine learning is supervised learning. In\nsupervised learning, the data to learn the function 𝑓(𝑥) is provided to the learning\nalgorithm in (𝑥, 𝑦) example-pairs. Here 𝑥is the input, and 𝑦the observed output\nvalue to be learned for that particular input value 𝑥. The 𝑦values can be thought\nof as supervising the learning process, they teach the learning process the right\nanswers for each input value 𝑥, hence the name supervised learning.\nThe data pairs to be learned from are organized in a dataset, which must be\npresent in its entirety before the algorithm can start. During the learning process,\nan estimate of the real function that generated the data is created, ˆ𝑓. The 𝑥values\nof the pair are also called the input, and the 𝑦values are the label to be learned.\nTwo well-known problems in supervised learning are regression and classiﬁ-\ncation. Regression predicts a continuous number, classiﬁcation a dicrete category.\nThe best known regression relation is the linear relation: the familiar straight line\nthrough a cloud of observation points that we all know from our introductory\nstatistics course. Figure 1.12 shows such a linear relationship ˆ𝑦= 𝑎· 𝑥+ 𝑏. The\nlinear function can be characterized with two parameters 𝑎and 𝑏. Of course, more\ncomplex functions are possible, such as quadratic regression, non-linear regression,\nor regression with higher-order polynomials [210].\nThe supervisory signal is computed for each data item 𝑖as the diﬀerence between\nthe current estimate and the given label, for example by ( ˆ𝑓(𝑥𝑖) −𝑦𝑖)2. Such an error\nfunction ( ˆ𝑓(𝑥) −𝑦)2 is also known as a loss function; it measures the quality of our\nprediction. The closer our prediction is to the true label, the lower the loss. There\nare many ways to compute this closeness, such as the mean squared error loss\nL = 1\n𝑁\nÍ𝑁\n1 ( ˆ𝑓(𝑥𝑖) −𝑦𝑖)2, which is used often for regression over 𝑁observations.\nThis loss function can be used by a supervised learning algorithm to adjust model\nparameters 𝑎and 𝑏to ﬁt the function ˆ𝑓to the data. Some of the many possible\nlearning algorithms are linear regression and support vector machines [93, 647].\nIn classiﬁcation, a relation between an input value and a class label is learned. A\nwell-studied classiﬁcation problem is image recognition, where two-dimensional\nimages are to be categorized. Table 1.2 shows a tiny dataset of labeled images\nof the proverbial cats and dogs. A popular loss function for classiﬁcation is the\ncross-entropy loss L = −Í𝑁\n1 𝑦𝑖log( ˆ𝑓(𝑥𝑖)), see also Sect. A.2.5.3. Again, such a\nloss function can be used to adjust the model parameters to ﬁt the function to the\ndata. The model can be small and linear, with few parameters, or it can be large,\n1.2 Three Machine Learning Paradigms\n15\n“Cat”\n“Cat”\n“Dog”\n“Cat”\n“Dog”\n“Dog”\nTable 1.2 (Input/output)-Pairs for a Supervised Classiﬁcation Problem\nwith many parameters, such as a neural network, which is often used for image\nclassiﬁcation.\nIn supervised learning a large dataset exists where all input items have an\nassociated training label. Reinforcement learning is diﬀerent, it does not assume\nthe pre-existence of a large labeled training set. Unsupervised learning does require\na large dataset, but no user-supplied output labels; all it needs are the inputs.\nDeep learning function approximation was ﬁrst developed in a supervised set-\nting. Although this book is about deep reinforcement learning, we will encounter\nsupervised learning concepts frequently, whenever we discuss the deep learning\naspect of deep reinforcement learning.\n1.2.2 Unsupervised Learning\nWhen there are no labels in the dataset, diﬀerent learning algorithms must be\nused. Learning without labels is called unsupervised learning. In unsupervised\nlearning an inherent metric of the data items is used, such as distance. A typical\nproblem in unsupervised learning is to ﬁnd patterns in the data, such as clusters or\nsubgroups [820, 801].\nPopular unsupervised learning algorithms are 𝑘-means algorithms, and prin-\ncipal component analysis [677, 380]. Other popular unsupervised methods are\ndimensionality reduction techniques from visualization, such as t-SNE [493], mini-\nmum description length [294] and data compression [55]. A popular application of\nunsupervised learning are autoencoders, see Sect. B.2.6 [411, 412].\nThe relation between supervised and unsupervised learning is sometimes char-\nacterized as follows: supervised learning aims to learn the conditional probability\ndistribution 𝑝(𝑥|𝑦) of input data conditioned on a label 𝑦, whereas unsupervised\nlearning aims to learn the a priori probability distribution 𝑝(𝑥) [343].\nWe will encounter unsupervised methods in this book in a few places, specif-\nically, when autoencoders and dimension reduction are discussed, for example,\nin Chap. 5. At the end of this book explainable artiﬁcial intelligence is discussed,\nwhere interpretable models play an important role, in Chap. 10.\n16\n1 Introduction\nFig. 1.13 Agent and Environment\n1.2.3 Reinforcement Learning\nThe last machine learning paradigm is, indeed, reinforcement learning. There are\nthree diﬀerences between reinforcement learning and the previous paradigms.\nFirst, reinforcement learning learns by interaction; in contrast to supervised and\nunsupervised learning, in reinforcement learning data items come one by one. The\ndataset is produced dynamically, as it were. The objective in reinforcement learning\nis to ﬁnd the policy: a function that gives us the best action in each state that the\nworld can be in.\nThe approach of reinforcement learning is to learn the policy for the world by\ninteracting with it. In reinforcement learning we recognize an agent, that does the\nlearning of the policy, and an environment, that provides feedback to the agent’s\nactions (and that performs state changes, see Fig. 1.13). In reinforcement learning,\nthe agent stands for the human, and the environment for the world. The goal of\nreinforcement learning is to ﬁnd the actions for each state that maximize the long\nterm accumulated expected reward. This optimal function of states to actions is\ncalled the optimal policy.\nIn reinforcement learning there is no teacher or supervisor, and there is no static\ndataset. There is, however, the environment, that will tell us how good the state\nis in which we ﬁnd ourselves. This brings us to the second diﬀerence: the reward\nvalue. Reinforcement learning gives us partial information, a number indicating the\nquality of the action that brought us to our state, where supervised learning gives\nfull information: a label that provides the correct answer in that state (Table 1.3). In\nthis sense, reinforcement learning is in between supervised learning, in which all\ndata items have a label, and unsupervised learning, where no data has a label.\nThe third diﬀerence is that reinforcement learning is used to solve sequential\ndecision problems. Supervised and unsupervised learning learn single-step relations\nbetween items; reinforcement learning learns a policy, which is the answer to\na multi-step problem. Supervised learning can classify a set of images for you;\nunsupervised learning can tell you which items belong together; reinforcement\n1.2 Three Machine Learning Paradigms\n17\nConcept\nSupervised Learning\nReinforcement Learning\nInputs 𝑥\nFull dataset of states\nPartial (One state at a time)\nLabels 𝑦\nFull (correct action)\nPartial (Numeric action reward)\nTable 1.3 Supervised vs. Reinforcement Learning\nlearning can tell you the winning sequence of moves in a game of chess, or the\naction-sequence that robot-legs need to take in order to walk.\nThese three diﬀerences have consequences. Reinforcement learning provides the\ndata to the learning algorithm step by step, action by action; whereas in supervised\nlearning the data is provided all at once in one large dataset. The step-by-step\napproach is well suited to sequential decision problems. On the other hand, many\ndeep learning methods were developed for supervised learning and may work\ndiﬀerently when data items are generated one-by-one. Furthermore, since actions\nare selected using the policy function, and action rewards are used to update this\nsame policy function, there is a possibility of circular feedback and local minima.\nCare must be taken to ensure convergence to global optima in our methods. Human\nlearning also suﬀers from this problem, when a stubborn child refuses to explore\noutside of its comfort zone. This topic is is discussed in Sect. 2.2.4.3.\nAnother diﬀerence is that in supervised learning the pupil learns from a ﬁnite-\nsized teacher (the dataset), and at some point may have learned all there is to learn.\nThe reinforcement learning paradigm allows a learning setup where the agent\ncan continue to sample the environment indeﬁnitely, and will continue to become\nsmarter as long as the environment remains challenging (which can be a long time,\nfor example in games such as chess and Go).3\nFor these reasons there is great interest in reinforcement learning, although\ngetting the methods to work is often harder than for supervised learning.\nMost classical reinforcement learning use tabular methods that work for low-\ndimensional problems with small state spaces. Many real world problems are com-\nplex and high-dimensional, with large state spaces. Due to steady improvements\nin learning algorithms, datasets, and compute power, deep learning methods have\nbecome quite powerful. Deep reinforcement learning methods have emerged that\nsuccessfully combine step-by-step sampling in high-dimensional problems with\nlarge state spaces. We will discuss these methods in the subsequent chapters of this\nbook.\n3 In fact, some argue that reward is enough for artiﬁcial general intelligence, see Silver, Singh,\nPrecup, and Sutton [707].\n18\n1 Introduction\nB. Deep Supervised Learning\n2. Tabular Value-Based Reinforcement Learning\n3. Deep Value-Based Reinforcement Learning\n4. Policy-Based Reinforcement Learning\n5. Model-Based Reinforcement Learning\n6. Two-Agent Self-Play\n7. Multi-Agent Reinforcement Learning\n8. Hierarchical Reinforcement Learning\n9. Meta-Learning\n10. Further Developments\nFig. 1.14 Deep Reinforcement Learning is built on Deep Supervised Learning and Tabular Rein-\nforcement Learning\n1.3 Overview of the Book\nThe aim of this book is to present the latest insights in deep reinforcement learning in\na single comprehensive volume, suitable for teaching a graduate level one-semester\ncourse.\nIn addition to covering state of the art algorithms, we cover necessary background\nin classic reinforcement learning and in deep learning. We also cover advanced,\nforward looking developments in self-play, and in multi-agent, hierarchical, and\nmeta-learning.\n1.3.1 Prerequisite Knowledge\nIn an eﬀort to be comprehensive, we make modest assumptions about previous\nknowledge. We assume a bachelor level of computer science or artiﬁcial intelligence,\nand an interest in artiﬁcial intelligence and machine learning. A good introductory\ntextbook on artiﬁcial intelligence is Russell and Norvig: Artiﬁcial Intelligence, A\nModern Approach [647].\n1.3 Overview of the Book\n19\nFigure 1.14 shows an overview of the structure of the book. Deep reinforcement\nlearning combines deep supervised learning and classical (tabular) reinforcement\nlearning. The ﬁgure shows how the chapters are built on this dual foundation.\nFor deep reinforcement learning, the ﬁeld of deep supervised learning is of great\nimportance. It is a large ﬁeld; deep, and rich. Many students may have followed\na course on deep learning, if not, Appendix B provides you with the necessary\nbackground (dashed). Tabular reinforcement learning, on the other hand, may be\nnew to you, and we start our story with this topic in Chap. 2.\nWe also assume undergraduate level familiarity with the Python programming\nlanguage. Python has become the programming language of choice for machine\nlearning research, and the host-language of most machine learning packages. All\nexample code in this book is in Python, and major machine learning environments\nsuch as scikit-learn, TensorFlow, Keras and PyTorch work best from Python. See\nhttps://www.python.org for pointers on how to get started in Python. Use the\nlatest stable version, unless the text mentions otherwise.\nWe assume an undergraduate level of familiarity with mathematics—a basic\nunderstanding of set theory, graph theory, probability theory and information\ntheory is necessary, although this is not a book of mathematics. Appendix A contains\na summary to refresh your mathematical knowledge, and to provide an introduction\nto the notation that is used in the book.\nCourse\nThere is a lot of material in the chapters, both basic and advanced, with many\npointers to the literature. One option is to teach a single course about all topics in\nthe book. Another option is to go slower and deeper, to spend suﬃcient time on the\nbasics, and create a course about Chaps. 2–5 to cover the basic topics (value-based,\npolicy-based, and model-based learning), and to create a separate course about\nChaps. 6–9 to cover the more advanced topics of multi-agent, hierarchical, and\nmeta-learning.\nBlogs and GitHub\nThe ﬁeld of deep reinforcement learning is a highly active ﬁeld, in which theory and\npractice go hand in hand. The culture of the ﬁeld is open, and you will easily ﬁnd\nmany blog posts about interesting topics, some quite good. Theory drives experi-\nmentation, and experimental results drive theoretical insights. Many researchers\npublish their papers on arXiv and their algorithms, hyperparameter settings and\nenvironments on GitHub.\nIn this book we aim for the same atmosphere. Throughout the text we provide\nlinks to code, and we challenge you with hands-on sections to get your hands dirty\nto perform your own experiments. All links to web pages that we use have been\nstable for some time.\n20\n1 Introduction\nWebsite: https://deep-reinforcement-learning.net is the com-\npanion website for this book. It contains updates, slides, and other course\nmaterial that you are welcome to explore and use.\n1.3.2 Structure of the Book\nThe ﬁeld of deep reinforcement learning consists of two main areas: model-free\nreinforcement learning and model-based reinforcement learning. Both areas have\ntwo subareas. The chapters of this book are organized according to this structure.\n•\nModel-free methods\n– Value-based methods: Chap. 2 (tabular) and 3 (deep)\n– Policy-based methods: Chap. 4\n•\nModel-based methods\n– Learned model: Chap. 5\n– Given model: Chap. 6\nThen, we have three chapters on more specialized topics.\n•\nMulti-agent reinforcement learning: Chap. 7\n•\nHierarchical reinforcement learning: Chap. 8\n•\nTransfer and Meta-learning: Chap. 9\nAppendix B provides a necessary review of deep supervised learning.\nThe style of each chapter is to ﬁrst provide the main idea of the chapter in\nan intuitive example, to then explain the kind of problem to be solved, and then\nto discuss algorithmic concepts that agents use, and the environments that have\nbeen solved in practice with these algorithms. The sections of the chapters are\nnamed accordingly: their names end in problem-agent-environment. At the end\nof each chapter we provide questions for quizzes to check your understanding of\nthe concepts, and we provide exercises for larger programming assignments (some\ndoable, some quite challenging). We also end each chapter with a summary and\nreferences to further reading.\nLet us now look in more detail at what topics the chapters cover.\nChapters\nAfter this introductory chapter, we continue with Chap. 2, in which we discuss in\ndetail the basic concepts of tabular (non-deep) reinforcement learning. We start with\nMarkov decision processes and discuss them at length. We will introduce tabular\nplanning and learning, and important concepts such as state, action, reward, value,\n1.3 Overview of the Book\n21\nand policy. We will encounter the ﬁrst, tabular, value-based model-free learning\nalgorithms (for an overview, see Table 2.1). Chapter 2 is the only non-deep chapter\nof the book. All other chapters cover deep methods.\nChapter 3 explains deep value-based reinforcement learning. The chapter covers\nthe ﬁrst deep algorithms that have been devised to ﬁnd the optimal policy. We will\nstill be working in the value-based, model-free, paradigm. At the end of the chapter\nwe will analyze a player that teaches itself how to play 1980s Atari video games.\nTable 3.1 lists some of the many stable deep value-based model-free algorithms.\nValue-based reinforcement learning works well with applications such as games,\nwith discrete action spaces. The next chapter, Chap. 4, discusses a diﬀerent approach:\ndeep policy-based reinforcement learning (Table 4.1). In addition to discrete spaces,\nthis approach is also suited for continuous actions spaces, such as robot arm move-\nment, and simulated articulated locomotion. We see how a simulated half-cheetah\nteaches itself to run.\nThe next chapter, Chap. 5, introduces deep model-based reinforcement learning\nwith a learned model, a method that ﬁrst builds up a transition model of the envi-\nronment before it builds the policy. Model-based reinforcement learning holds the\npromise of higher sample eﬃciency, and thus faster learning. New developments,\nsuch as latent models, are discussed. Applications are both in robotics and in games\n(Table 5.2).\nThe next chapter, Chap. 6, studies how a self-play system can be created for\napplications where the transition model is given by the problem description. This is\nthe case in two-agent games, where the rules for moving in the game determine the\ntransition function. We study how TD-Gammon and AlphaZero achieve tabula rasa\nlearning: teaching themselves from zero knowledge to world champion level play\nthrough playing against a copy of itself (Table 6.2). In this chapter deep residual\nnetworks and Monte Carlo Tree Search result in curriculum learning.\nChapter 7 introduces recent developments in deep multi-agent and team learning.\nThe chapter covers competition and collaboration, population-based methods, and\nplaying in teams. Applications of these methods are found in games such as poker\nand StarCraft (Table 7.2).\nChapter 8 covers deep hierarchical reinforcement learning. Many tasks exhibit\nan inherent hierarchical structure, in which clear subgoals can be identiﬁed. The\noptions framework is discussed, and methods that can identify subgoals, subpolicies,\nand meta policies. Diﬀerent approaches for tabular and deep hierarchical methods\nare discussed (Table 8.1).\nThe ﬁnal technical chapter, Chap. 9, covers deep meta-learning, or learning to\nlearn. One of the major hurdles in machine learning is the long time it takes to learn\nto solve a new task. Meta-learning and transfer learning aim to speed up learning of\nnew tasks by using information that has been learned previously for related tasks;\nalgorithms are listed in Table 9.2. At the end of the chapter we will experiment with\nfew-shot learning, where a task has to be learned without having seen more than a\nfew training examples.\nChapter 10 concludes the book by reviewing what we have learned, and by\nlooking ahead into what the future may bring.\n22\n1 Introduction\nAppendix A provides mathematical background information and notation. Ap-\npendix B provides a chapter-length overview of machine learning and deep super-\nvised learning. If you wish to refresh your knowledge of deep learning, please go to\nthis appendix before you read Chap. 3. Appendix C provides lists of useful software\nenvironments and software packages for deep reinforcement learning.\nChapter 2\nTabular Value-Based Reinforcement Learning\nThis chapter will introduce the classic, tabular, ﬁeld of reinforcement learning, to\nbuild a foundation for the next chapters. First, we will introduce the concepts of\nagent and environment. Next come Markov decision processes, the formalism that\nis used to reason mathematically about reinforcement learning. We discuss at some\nlength the elements of reinforcement learning: states, actions, values, policies.\nWe learn about transition functions, and solution methods that are based on\ndynamic programming using the transition model. There are many situations where\nagents do not have access to the transition model, and state and reward information\nmust be acquired from the environment. Fortunately, methods exist to ﬁnd the\noptimal policy without a model, by querying the environment. These methods,\nappropriately named model-free methods, will be introduced in this chapter. Value-\nbased model-free methods are the most basic learning approach of reinforcement\nlearning. They work well in problems with deterministic environments and discrete\naction spaces, such as mazes and games. Model-free learning makes few demands\non the environment, building up the policy function 𝜋(𝑠) →𝑎by sampling the\nenvironment.\nAfter we have discussed these concepts, it is time to apply them, and to under-\nstand the kinds of sequential decision problems that we can solve. We will look\nat Gym, a collection of reinforcement learning environments. We will also look at\nsimple Grid world puzzles, and see how to navigate those.\nThis is a non-deep chapter: in this chapter functions are exact, states are stored\nin tables, an approach that works as long as problems are small enough to ﬁt in\nmemory. The next chapter shows how function approximation with neural networks\nworks when there are more states than ﬁt in memory.\nThe chapter is concluded with exercises, a summary, and pointers to further\nreading.\n23\n24\n2 Tabular Value-Based Reinforcement Learning\nCore Concepts\n•\nAgent, environment\n•\nMDP: state, action, reward, value, policy\n•\nPlanning and learning\n•\nExploration and exploitation\n•\nGym, baselines\nCore Problem\n•\nLearn a policy from interaction with the environment\nCore Algorithms\n•\nValue iteration (Listing 2.1)\n•\nTemporal diﬀerence learning (Sect. 2.2.4.2)\n•\nQ-learning (Listing 2.6)\nFinding a Supermarket\nImagine that you have just moved to a new city, you are hungry, and you want to\nbuy some groceries. There is a somewhat unrealistic catch: you do not have a map\nof the city and you forgot to charge your smartphone. It is a sunny day, you put\non your hiking shoes, and after some random exploration you have found a way\nto a supermarket and have bought your groceries. You have carefully noted your\nroute in a notebook, and you retrace your steps, ﬁnding your way back to your new\nhome.\nWhat will you do the next time that you need groceries? One option is to\nfollow exactly the same route, exploiting your current knowledge. This option is\nguaranteed to bring you to the store, at no additional cost for exploring possible\nalternative routes. Or you could be adventurous, and explore, trying to ﬁnd a new\nroute that may actually be quicker than the old route. Clearly, there is a trade-oﬀ:\nyou should not spend so much time exploring that you can not recoup the gains of\na potential shorter route before you move elsewhere.\nReinforcement learning is a natural way of learning the optimal route as we go,\nby trial and error, from the eﬀects of the actions that we take in our environment.\nThis little story contained many of the elements of a reinforcement learning\nproblem, and how to solve it. There is an agent (you), an environment (the city), there\n2.1 Sequential Decision Problems\n25\nFig. 2.1 Grid World with a goal, an “un-goal,” and a wall\nare states (your location at diﬀerent points in time), actions (assuming a Manhattan-\nstyle grid, moving a block left, right, forward, or back), there are trajectories (the\nroutes to the supermarket that you tried), there is a policy (that tells which action\nyou will take at a particular location), there is a concept of cost/reward (the length\nof your current path), we see exploration of new routes, exploitation of old routes, a\ntrade-oﬀbetween them, and your notebook in which you have been sketching a\nmap of the city (your local transition model).\nBy the end of this chapter you will have learned which role all these topics play\nin reinforcement learning.\n2.1 Sequential Decision Problems\nReinforcement learning is used to solve sequential decision problems [27, 255].\nBefore we dive into the algorithms, let us have a closer look at these problems, to\nbetter understand the challenges that the agents must solve.\nIn a sequential decision problem the agent has to make a sequence of decisions\nin order to solve a problem. Solving implies to ﬁnd the sequence with the highest\n(expected cumulative future) reward. The solver is called the agent, and the problem\nis called environment (or sometimes the world).\nWe will now discuss basic examples of sequential decision problems.\nGrid Worlds\nSome of the ﬁrst environments that we encounter in reinforcement learning are\nGrid worlds (Fig. 2.1). These environments consist of a rectangular grid of squares,\nwith a start square, and a goal square. The aim is for the agent to ﬁnd the sequence\nof actions that it must take (up, down, left, right) to arrive at the goal square. In\nfancy versions a “loss” square is added, that scores minus points, or a “wall” square,\nthat is impenetrable for the agent. By exploring the grid, taking diﬀerent actions,\nand recording the reward (whether it reached the goal square), the agent can ﬁnd a\nroute—and when it has a route, it can try to improve that route, to ﬁnd a shorter\nroute to the goal.\n26\n2 Tabular Value-Based Reinforcement Learning\nFig. 2.2 The Longleat Hedge Maze in Wiltshire, England\nFig. 2.3 Sokoban Puzzle [136]\nGrid world is a simple environment that is well-suited for manually playing\naround with reinforcement learning algorithms, to build up intuition of what the\nalgorithms do. In this chapter we will model reinforcement learning problems\nformally, and encounter algorithms that ﬁnd optimal routes in Grid world.\nMazes and Box Puzzles\nAfter Grid world problems, there are more complicated problems, with extensive\nwall structures to make navigation more diﬃcult (see Fig. 2.2). Trajectory planning\nalgorithms play a central role in robotics [456, 265]; there is a long tradition of\nusing 2D and 3D mazes for path-ﬁnding problems in reinforcement learning. The\n2.2 Tabular Value-Based Agents\n27\nAgent\nEnvironment\nAction 𝑎𝑡\nState 𝑠𝑡+1\nReward 𝑟𝑡+1\nFig. 2.4 Agent and environment [743]\nTaxi domain was introduced by Dietterich [196], and box-pushing problems such\nas Sokoban have also been used frequently [386, 204, 543, 878], see Fig. 2.3. The\nchallenge in Sokoban is that boxes can only be pushed, not pulled. Actions can have\nthe eﬀect of creating an inadvertent dead-end for into the future, making Sokoban\na diﬃcult puzzle to play. The action space of these puzzles and mazes is discrete.\nSmall versions of the mazes can be solved exactly by planning, larger instances are\nonly suitable for approximate planning or learning methods. Solving these planning\nproblems exactly is NP-hard or PSPACE-hard [169, 322], as a consequence the\ncomputational time required to solve problem instances exactly grows exponentially\nwith the problem size, and becomes quickly infeasible for all but the smallest\nproblems.\nLet us see how we can model agents to act in these types of environments.\n2.2 Tabular Value-Based Agents\nReinforcement learning ﬁnds the best policy to operate in the environment by\ninteracting with it. The reinforcement learning paradigm consists of an agent (you,\nthe learner) and an environment (the world, which is in a certain state, and gives\nyou feedback on your actions).\n2.2.1 Agent and Environment\nIn Fig. 2.4 the agent and environment are shown, together with action 𝑎𝑡, next state\n𝑠𝑡+1, and its reward 𝑟𝑡+1. Let us have a closer look at the ﬁgure.\nThe environment is in a certain state 𝑠𝑡at time 𝑡. Then, the agent performs action\n𝑎𝑡, resulting in a transition in the environment from state 𝑠𝑡to 𝑠𝑡+1 at the next time\nstep, also denoted as 𝑠→𝑠′. Along with this new state comes a reward value 𝑟𝑡+1\n(which may be a positive or a negative value). The goal of reinforcement learning\nis to ﬁnd the sequence of actions that gives the best reward. More formally, the\ngoal is to ﬁnd the optimal policy function 𝜋★that gives in each state the best action\nto take in that state. By trying diﬀerent actions, and accumulating the rewards,\n28\n2 Tabular Value-Based Reinforcement Learning\nthe agent can ﬁnd the best action for each state. In this way, with the reinforcing\nreward values, the optimal policy is learned from repeated interaction with the\nenvironment, and the problem is “solved.”\nIn reinforcement learning the environment gives us only a number as an in-\ndication of the quality of an action that we performed, and we are left to derive\nthe correct action policy from that, as we can see in Fig. 2.4. On the other hand,\nreinforcement learning allows us to generate as many action-reward pairs as we\nneed, without a large hand-labeled dataset, and we can choose ourselves which\nactions to try.\n2.2.2 Markov Decision Process\nSequential decision problems can be modelled as Markov decision processes\n(MDPs) [483]. Markov decision problems have the Markov property: the next state\ndepends only on the current state and the actions available in it (no historical mem-\nory of previous states or information from elsewhere inﬂuences the next state) [352].\nThe no-memory property is important because it makes reasoning about future\nstates possible using only the information present in the current state. If previous\nhistories would inﬂuence the current state, and these would all have to be taken\ninto account, then reasoning about the current state would be much harder or even\ninfeasible.\nMarkov processes are named after Russian mathematician Andrey Markov (1856–\n1922) who is best known for his work on these stochastic processes. See [27, 255]\nfor an introduction into MDPs. The MDP formalism is the mathematical basis under\nreinforcement learning, and we will introduce the relevant elements in this chapter.\nWe follow Moerland [524] and François-Lavet et al. [255] for some of the notation\nand examples in this section.\nFormalism\nWe deﬁne a Markov decision process for reinforcement learning as a 5-tuple\n(𝑆, 𝐴,𝑇𝑎, 𝑅𝑎, 𝛾):\n•\n𝑆is a ﬁnite set of legal states of the environment; the initial state is denoted as 𝑠0\n•\n𝐴is a ﬁnite set of actions (if the set of actions diﬀers per state, then 𝐴𝑠is the\nﬁnite set of actions in state 𝑠)\n•\n𝑇𝑎(𝑠, 𝑠′) = Pr(𝑠𝑡+1 = 𝑠′|𝑠𝑡= 𝑠, 𝑎𝑡= 𝑎) is the probability that action 𝑎in state 𝑠\nat time 𝑡will transition to state 𝑠′ at time 𝑡+ 1 in the environment\n•\n𝑅𝑎(𝑠, 𝑠′) is the reward received after action 𝑎transitions state 𝑠to state 𝑠′\n•\n𝛾∈[0, 1] is the discount factor representing the diﬀerence between future and\npresent rewards.\n2.2 Tabular Value-Based Agents\n29\n2.2.2.1 State 𝑺\nLet us have a deeper look at the Markov-tuple 𝑆, 𝐴,𝑇𝑎, 𝑅𝑎, 𝛾, to see their role in the\nreinforcement learning paradigm, and how, together, they can model and describe\nreward-based learning processes.\nAt the basis of every Markov decision process is a description of the state 𝑠𝑡of\nthe system at a certain time 𝑡.\nState Representation\nThe state 𝑠contains the information to uniquely represent the conﬁguration of the\nenvironment.\nOften there is a straightforward way to uniquely represent the state in a computer\nmemory. For the supermarket example, each identifying location is a state (such\nas: I am at the corner of 8th Av and 27nd St). For chess, this can be the location of\nall pieces on the board (plus information for the 50 move repetition rule, castling\nrights, and en-passant state). For robotics this can be the orientation of all joints of\nthe robot, and the location of the limbs of the robot. For Atari, the state comprises\nthe values of all screen pixels.\nUsing its current behavior policy, the agent chooses an action 𝑎, which is per-\nformed in the environment. How the environment reacts to the action is deﬁned by\nthe transition model 𝑇𝑎(𝑠, 𝑠′) that is internal to the environment, which the agent\ndoes not know. The environment returns the new state 𝑠′, as well as a reward value\n𝑟′ for the new state.\nDeterministic and Stochastic Environment\nIn discrete deterministic environments the transition function deﬁnes a one-step\ntransition, as each action (from a certain old state) deterministically leads to a single\nnew state. This is the case in Grid worlds, Sokoban, and in games such as chess and\ncheckers, where a move action deterministically leads to one new board position.\nAn example of a non-deterministic situation is a robot movement in an envi-\nronment. In a certain state, a robot arm is holding a bottle. An agent-action can be\nturning the bottle in a certain orientation (presumably to pour a drink in a cup). The\nnext state may be a full cup, or it may be a mess, if the bottle was not poured in the\ncorrect orientation, or location, or if something happened in the environment such\nas someone bumping the table. The outcome of the action is unknown beforehand\nby the agent, and depends on elements in the environment, that are not known to\nthe agent.\n30\n2 Tabular Value-Based Reinforcement Learning\n2.2.2.2 Action 𝑨\nNow that we have looked at the state, it is time to look at the second item that\ndeﬁnes an MDP, the action.\nIrreversible Environment Action\nWhen the agent is in state 𝑠, it chooses an action 𝐴to perform, based on its current\nbehavior policy 𝜋(𝑎|𝑠) (policies are explained soon). The agent communicates the\nselected action 𝑎to the environment (Fig. 2.4). For the supermarket example, an\nexample of an action could be walking along a block in a certain direction (such\nas: East). For Sokoban, an action can be pushing a box to a new location in the\nwarehouse. Note that in diﬀerent states the possible actions may diﬀer. For the\nsupermarket example, walking East may not be possible at each street corner, and\nin Sokoban pushing a box in a certain direction will only be possible in states where\nthis direction is not blocked by a wall.\nAn action changes the state of the environment irreversibly. In the reinforcement\nlearning paradigm, there is no undo operator for the environment (nor is there in\nthe real world). When the environment has performed a state transition, it is ﬁnal.\nThe new state is communicated to the agent, together with a reward value. The\nactions that the agent performs in the environment are also known as its behavior,\njust as the actions of a human in the world constitute the human’s behavior.\nDiscrete or Continuous Action Space\nThe actions are discrete in some applications, continuous in others. For example,\nthe actions in board games, and choosing a direction in a navigation task in a grid,\nare discrete.\nIn contrast, arm and joint movements of robots, and bet sizes in certain games, are\ncontinuous (or span a very large range of values). Applying algorithms to continuous\nor very large action spaces either requires discretization of the continuous space\n(into buckets) or the development of a diﬀerent kind of algorithm. As we will see\nin Chaps. 3 and 4, value-based methods work well for discrete action spaces, and\npolicy-based methods work well for both action spaces.\nFor the supermarket example we can actually choose between modeling our\nactions discrete or continuous. From every state, we can move any number of steps,\nsmall or large, integer or fractional, in any direction. We can even walk a curvy\npath. So, strictly speaking, the action space is continuous. However, if, as in some\ncities, the streets are organized in a rectangular Manhattan-pattern, then it makes\nsense to discretize the continuous space, and to only consider discrete actions that\n2.2 Tabular Value-Based Agents\n31\n𝑠\n𝜋\n𝑎\n𝑠′\n𝑡𝑎, 𝑟𝑎\n𝑠\n𝜋\n𝑎\n𝑠′\n𝑡𝑎, 𝑟𝑎\n𝑠\n𝜋\n𝑎, 𝑠′\n𝑡𝑎, 𝑟𝑎\nFig. 2.5 Backup Diagrams for MDP Transitions: Stochastic (left) and Deterministic (middle and\nright) [743]\ntake us to the next street corner. Then, our action space has become discrete, by\nusing extra knowledge of the problem structure.1\n2.2.2.3 Transition 𝑻𝒂\nAfter having discussed state and action, it is time to look at the transition func-\ntion 𝑇𝑎(𝑠, 𝑠′). The transition function 𝑇𝑎determines how the state changes after\nan action has been selected. In model-free reinforcement learning the transition\nfunction is implicit to the solution algorithm: the environment has access to the\ntransition function, and uses it to compute the next state 𝑠′, but the agent has not.\n(In Chap. 5 we will discuss model-based reinforcement learning. There the agent\nhas its own transition function, an approximation of the environment’s transition\nfunction, which is learned from the environment feedback.)\nGraph View of the State Space\nWe have discussed states, actions and transitions. The dynamics of the MDP are\nmodelled by transition function 𝑇𝑎(·) and reward function 𝑅𝑎(·). The imaginary\nspace of all possible states is called the state space. The state space is typically\nlarge. The two functions deﬁne a two-step transition from state 𝑠to 𝑠′, via action 𝑎:\n𝑠→𝑎→𝑠′.\nTo help our understanding of the transitions between states we can use a graph-\nical depiction, as in Fig. 2.5.\nIn the ﬁgure, states and actions are depicted as nodes (vertices), and transitions\nare links (edges) between the nodes. States are drawn as open circles, and actions\nas smaller black circles. In a certain state 𝑠, the agent can choose which action 𝑎to\n1 If we assume that supermarkets are large, block-sized, items that typically can be found on street\ncorners, then we can discretize the action space. Note that we may miss small sub-block-sized\nsupermarkets, because of this simpliﬁcation. Another, better, simpliﬁcation, would be to discretize\nthe action space into walking distances of the size of the smallest supermarket that we expect to\never encounter.\n32\n2 Tabular Value-Based Reinforcement Learning\nperform, that is then acted out in the environment. The environment returns the\nnew state 𝑠′ and the reward 𝑟′.\nFigure 2.5 shows a transition graph of the elements of the MDP tuple 𝑠, 𝑎, 𝑡𝑎, 𝑟𝑎\nas well as 𝑠′, and policy 𝜋, and how the value can be calculated. The root node\nat the top is state 𝑠, where policy 𝜋allows the agent to choose between three\nactions 𝑎, that, following distribution Pr, each can transition to two possible states\n𝑠′, with their reward 𝑟′. In the ﬁgure, a single transition is shown. Please use your\nimagination to picture the other transitions as the graph extends down.\nIn the left panel of the ﬁgure the environment can choose which new state it\nreturns in response to the action (stochastic environment), in the middle panel there\nis only one state for each action (deterministic environment); the tree can then be\nsimpliﬁed, showing only the states, as in the right panel.\nTo calculate the value of the root of the tree a backup procedure can be followed.\nSuch a procedure calculates the value of a parent from the values of the children,\nrecursively, in a bottom-up fashion, summing or maxing their values from the\nleaves to the root of the tree. This calculation uses discrete time steps, indicated\nby subscripts to the state and action, as in 𝑠𝑡, 𝑠𝑡+1, 𝑠𝑡+2, . . .. For brevity, 𝑠𝑡+1 is\nsometimes written as 𝑠′. The ﬁgure shows a single transition step; an episode in\nreinforcement learning typically consists of a sequence of many time steps.\nTrial and Error, Down and Up\nA graph such as the one in the center and right panel of Fig. 2.5, where child nodes\nhave only one parent node and without cycles, is known as a tree. In computer\nscience the root of a tree is at the top, and branches grow downward to the leaves.\nAs actions are performed and states and rewards are returned backup the tree, a\nlearning process is taking place in the agent. We can use Fig. 2.5 to better understand\nthe learning process that is unfolding.\nThe rewards of actions are learned by the agent by interacting with the envi-\nronment, performing the actions. In the tree of Fig. 2.5 an action selection moves\ndownward, towards the leaves. At the deeper states, we ﬁnd the rewards, which we\npropagate to the parent states upwards. Reward learning is learning by backpropa-\ngation: in Fig. 2.5 the reward information ﬂows upward in the diagram from the\nleaves to the root. Action selection moves down, reward learning ﬂows up.\nReinforcement learning is learning by trial and error. Trial is selecting an action\ndown (using the behavior policy) to perform in the environment. Error is moving\nup the tree, receiving a feedback reward from the environment, and reporting that\nback up the tree to the state to update the current behavior policy. The downward\nselection policy chooses which actions to explore, and the upward propagation of\nthe error signal performs the learning of the policy.\nFigures such as the one in Fig. 2.5 are useful for seeing how values are calculated.\nThe basic notions are trial, and error, or down, and up.\n2.2 Tabular Value-Based Agents\n33\n2.2.2.4 Reward 𝑹𝒂\nThe reward function 𝑅𝑎is of central importance in reinforcement learning. It\nindicates the measure of quality of that state, such solved, or distance. Rewards are\nassociated with single states, indicating their quality. However, we are most often\ninterested in the quality of a full decision making sequence from root to leaves\n(this sequence of decisions would be one possible answer to our sequential decision\nproblem).\nThe reward of such a full sequence is called the return, sometimes denoted\nconfusingly as 𝑅, just as the reward. The expected cumulative discounted future\nreward of a state is called the value function 𝑉𝜋(𝑠). The value function 𝑉𝜋(𝑠) is\nthe expected cumulative reward of 𝑠where actions are chosen according to policy\n𝜋. The value function plays a central role in reinforcement learning algorithms; in\na few moments we will look deeper into return and value.\n2.2.2.5 Discount Factor 𝜸\nWe distinguish between two types of tasks: (1) continuous time, long running, tasks,\nand (2) episodic tasks—tasks that end. In continuous and long running tasks it\nmakes sense to discount rewards from far in the future in order to more strongly\nvalue current information at the present time. To achieve this a discount factor 𝛾is\nused in our MDP that reduces the impact of far away rewards. Many continuous\ntasks use discounting, 𝛾≠1.\nHowever, in this book we will often discuss episodic problems, where 𝛾is\nirrelevant. Both the supermarket example and the game of chess are episodic, and\ndiscounting does not make sense in these problems, 𝛾= 1.\n2.2.2.6 Policy 𝝅\nOf central importance in reinforcement learning is the policy function 𝜋. The policy\nfunction 𝜋answers the question how the diﬀerent actions 𝑎at state 𝑠should be\nchosen. Actions are anchored in states. The central question of MDP optimization\nis how to choose our actions. The policy 𝜋is a conditional probability distribution\nthat for each possible state speciﬁes the probability of each possible action. The\nfunction 𝜋is a mapping from the state space to a probability distribution over the\naction space:\n𝜋: 𝑆→𝑝(𝐴)\nwhere 𝑝(𝐴) can be a discrete or continuous probability distribution. For a particular\nprobability (density) from this distribution we write\n𝜋(𝑎|𝑠)\n34\n2 Tabular Value-Based Reinforcement Learning\nExample: For a discrete state space and discrete action space, we may store\nan explicit policy as a table, e.g.:\n𝑠\n𝜋(𝑎=up|𝑠) 𝜋(𝑎=down|𝑠) 𝜋(𝑎=left|𝑠) 𝜋(𝑎=right|𝑠)\n1\n0.2\n0.8\n0.0\n0.0\n2\n0.0\n0.0\n0.0\n1.0\n3\n0.7\n0.0\n0.3\n0.0\netc.\n.\n.\n.\n.\nA special case of a policy is a deterministic policy, denoted by\n𝜋(𝑠)\nwhere\n𝜋: 𝑆→𝐴\nA deterministic policy selects a single action in every state. Of course the deter-\nministic action may diﬀer between states, as in the example below:\nExample: An example of a deterministic discrete policy is\n𝑠\n𝜋(𝑎=up|𝑠) 𝜋(𝑎=down|𝑠) 𝜋(𝑎=left|𝑠) 𝜋(𝑎=right|𝑠)\n1\n0.0\n1.0\n0.0\n0.0\n2\n0.0\n0.0\n0.0\n1.0\n3\n1.0\n0.0\n0.0\n0.0\netc.\n.\n.\n.\n.\nWe would write 𝜋(𝑠= 1) = down, 𝜋(𝑠= 2) = right, etc.\n2.2.3 MDP Objective\nFinding the optimal policy function is the goal of the reinforcement learning prob-\nlem, and the remainder of this book will discuss many diﬀerent algorithms to\nachieve this goal under diﬀerent circumstances. Let us have a closer look at the\nobjective of reinforcement learning. Before we can do so, we will look at traces,\ntheir return, and value functions.\n2.2.3.1 Trace 𝝉\nAs we start interacting with the MDP, at each timestep 𝑡, we observe 𝑠𝑡, take an\naction 𝑎𝑡and then observe the next state 𝑠𝑡+1 ∼𝑇𝑎𝑡(𝑠) and reward 𝑟𝑡= 𝑅𝑎𝑡(𝑠𝑡, 𝑠𝑡+1).\n2.2 Tabular Value-Based Agents\n35\n𝑠\n𝑎\n𝑠\n𝑎\n𝑇𝑇\nFig. 2.6 Single Transition Step versus Full 3-Step Trace/Episode/Trajectory\nRepeating this process leads to a sequence or trace in the environment, which we\ndenote by 𝜏𝑛\n𝑡:\n𝜏𝑛\n𝑡= {𝑠𝑡, 𝑎𝑡, 𝑟𝑡, 𝑠𝑡+1, .., 𝑎𝑡+𝑛, 𝑟𝑡+𝑛, 𝑠𝑡+𝑛+1}\nHere, 𝑛denotes the length of the 𝜏. In practice, we often assume 𝑛= ∞, which means\nthat we run the trace until the domain terminates. In those cases, we will simply\nwrite 𝜏𝑡= 𝜏∞\n𝑡. Traces are one of the basic building blocks of reinforcement learning\nalgorithms. They are a single full rollout of a sequence from the sequential decision\nproblem. They are also called trajectory, episode, or simply sequence (Fig. 2.6 shows\na single transition step, and an example of a three-step trace).\nExample: A short trace with three actions could look like:\n𝜏2\n0 = {𝑠0=1, 𝑎0=up, 𝑟0=−1, 𝑠1=2, 𝑎1=up, 𝑟1=−1, 𝑠2=3, 𝑎2=left, 𝑟2=20, 𝑠3=5}\nSince both the policy and the transition dynamics can be stochastic, we will not\nalways get the same trace from the start state. Instead, we will get a distribution\nover traces. The distribution of traces from the start state (distribution) is denoted\nby 𝑝(𝜏0). The probability of each possible trace from the start is actually given by\nthe product of the probability of each speciﬁc transition in the trace:\n𝑝(𝜏0) = 𝑝0(𝑠0) · 𝜋(𝑎0|𝑠0) · 𝑇𝑎0(𝑠0, 𝑠1) · 𝜋(𝑎1|𝑠1)...\n= 𝑝0(𝑠0) ·\n∞\nÖ\n𝑡=0\n𝜋(𝑎𝑡|𝑠𝑡) · 𝑇𝑎𝑡(𝑠𝑡, 𝑠𝑡+1)\n(2.1)\nPolicy-based reinforcement learning depends heavily on traces, and we will\ndiscuss traces more deeply in Chap. 4. Value-based reinforcement learning (this\nchapter) uses single transition steps.\n36\n2 Tabular Value-Based Reinforcement Learning\nReturn 𝑹\nWe have not yet formally deﬁned what we actually want to achieve in the sequential\ndecision-making task—which is, informally, the best policy. The sum of the future\nreward of a trace is known as the return. The return of trace 𝜏𝑡is:\n𝑅(𝜏𝑡) = 𝑟𝑡+ 𝛾· 𝑟𝑡+1 + 𝛾2 · 𝑟𝑡+2 + ...\n= 𝑟𝑡+\n∞\n∑︁\n𝑖=1\n𝛾𝑖𝑟𝑡+𝑖\n(2.2)\nwhere 𝛾∈[0, 1] is the discount factor. Two extreme cases are:\n•\n𝛾= 0: A myopic agent, which only considers the immediate reward, 𝑅(𝜏𝑡) = 𝑟𝑡\n•\n𝛾= 1: A far-sighted agent, which treats all future rewards as equal, 𝑅(𝜏𝑡) =\n𝑟𝑡+ 𝑟𝑡+1 + 𝑟𝑡+2 + . . .\nNote that if we would use an inﬁnite-horizon return (Eq. 2.2) and 𝛾= 1.0, then the\ncumulative reward may become unbounded. Therefore, in continuous problems,\nwe use a discount factor close to 1.0, such as 𝛾= 0.99.\nExample: For the previous trace example we assume 𝛾= 0.9. The return\n(cumulative reward) is equal to:\n𝑅(𝜏2\n0) = −1 + 0.9 · −1 + 0.92 · 20 = 16.2 −1.9 = 14.3\n2.2.3.2 State Value 𝑽\nThe real measure of optimality that we are interested in is not the return of just one\ntrace. The environment can be stochastic, and so can our policy, and for a given\npolicy we do not always get the same trace. Therefore, we are actually interested\nin the expected cumulative reward that a certain policy achieves. The expected\ncumulative discounted future reward of a state is better known as the value of that\nstate.\nWe deﬁne the state value 𝑉𝜋(𝑠) as the return we expect to achieve when an\nagent starts in state 𝑠and then follows policy 𝜋, as:\n𝑉𝜋(𝑠) = E𝜏𝑡∼𝑝(𝜏𝑡)\n\u0002 ∞\n∑︁\n𝑖=0\n𝛾𝑖· 𝑟𝑡+𝑖|𝑠𝑡= 𝑠\n\u0003\n(2.3)\n2.2 Tabular Value-Based Agents\n37\nExample: Imagine that we have a policy 𝜋, which from state 𝑠can result\nin two traces. The ﬁrst trace has a cumulative reward of 20, and occurs in\n60% of the times. The other trace has a cumulative reward of 10, and occurs\n40% of the times. What is the value of state 𝑠?\n𝑉𝜋(𝑠) = 0.6 · 20 + 0.4 · 10 = 16.\nThe average return (cumulative reward) that we expect to get from state 𝑠\nunder this policy is 16.\nEvery policy 𝜋has one unique associated value function 𝑉𝜋(𝑠). We often omit\n𝜋to simplify notation, simply writing 𝑉(𝑠), knowing a state value is always condi-\ntioned on a certain policy.\nThe state value is deﬁned for every possible state 𝑠∈𝑆. 𝑉(𝑠) maps every state\nto a real number (the expected return):\n𝑉: 𝑆→R\nExample: In a discrete state space, the value function can be represented\nas a table of size |𝑆|.\n𝑠\n𝑉𝜋(𝑠)\n1\n2.0\n2\n4.0\n3\n1.0\netc. .\nFinally, the state value of a terminal state is by deﬁnition zero:\n𝑠= terminal\n⇒\n𝑉(𝑠) := 0.\n2.2.3.3 State-Action Value 𝑸\nIn addition to state values 𝑉𝜋(𝑠), we also deﬁne state-action value 𝑄𝜋(𝑠, 𝑎).2 The\nonly diﬀerence is that we now condition on a state and action. We estimate the\naverage return we expect to achieve when taking action 𝑎in state 𝑠, and follow\npolicy 𝜋afterwards:\n𝑄𝜋(𝑠, 𝑎) = E𝜏𝑡∼𝑝(𝜏𝑡)\n\u0002 ∞\n∑︁\n𝑖=0\n𝛾𝑖· 𝑟𝑡+𝑖|𝑠𝑡= 𝑠, 𝑎𝑡= 𝑎\n\u0003\n(2.4)\n2 The reason for the choice for letter Q is lost in the mists of time. Perhaps it is meant to indicate\nquality.\n38\n2 Tabular Value-Based Reinforcement Learning\nEvery policy 𝜋has only one unique associated state-action value function 𝑄𝜋(𝑠, 𝑎).\nWe often omit 𝜋to simplify notation. Again, the state-action value is a function\n𝑄: 𝑆× 𝐴→R\nwhich maps every state-action pair to a real number.\nExample: For a discrete state and action space, 𝑄(𝑠, 𝑎) can be represented\nas a table of size |𝑆| × |𝐴|. Each table entry stores a 𝑄(𝑠, 𝑎) estimate for the\nspeciﬁc 𝑠, 𝑎combination:\n𝑎=up 𝑎=down 𝑎=left 𝑎=right\n𝑠=1\n4.0\n3.0\n7.0\n1.0\n𝑠=2\n2.0\n-4.0\n0.3\n1.0\n𝑠=3\n3.5\n0.8\n3.6\n6.2\netc.\n.\n.\n.\n.\nThe state-action value of a terminal state is by deﬁnition zero:\n𝑠= terminal\n⇒\n𝑄(𝑠, 𝑎) := 0,\n∀𝑎\n2.2.3.4 Reinforcement Learning Objective\nWe now have the ingredients to formally state the objective 𝐽(·) of reinforcement\nlearning. The objective is to achieve the highest possible average return from the\nstart state:\n𝐽(𝜋) = 𝑉𝜋(𝑠0) = E𝜏0∼𝑝(𝜏0 | 𝜋)\nh\n𝑅(𝜏0)\ni\n.\n(2.5)\nfor 𝑝(𝜏0) given in Eq. 2.1. There is one optimal value function, which achieves\nhigher or equal value than all other value functions. We search for a policy that\nachieves this optimal value function, which we call the optimal policy 𝜋★:\n𝜋★(𝑎|𝑠) = arg max\n𝜋\n𝑉𝜋(𝑠0)\n(2.6)\nThis function 𝜋★is the optimal policy, it uses the arg max function to select the\npolicy with the optimal value. The goal in reinforcement learning is to ﬁnd this\noptimal policy for start state 𝑠0.\nA potential beneﬁt of state-action values 𝑄over state values 𝑉is that state-\naction values directly tell what every action is worth. This may be useful for action\nselection, since, for discrete action spaces,\n𝑎★= arg max\n𝑎∈𝐴\n𝑄★(𝑠, 𝑎)\n2.2 Tabular Value-Based Agents\n39\nthe Q function directly identiﬁes the best action. Equivalently, the optimal policy\ncan be obtained directly from the optimal Q function:\n𝜋★(𝑠) = arg max\n𝑎∈𝐴\n𝑄★(𝑠, 𝑎).\nWe will now turn to construct algorithms to compute the value function and the\npolicy function.\n2.2.3.5 Bellman Equation\nTo calculate the value function, let us look again at the tree in Fig. 2.5 on page 31,\nand imagine that it is many times larger, with subtrees that extend to fully cover\nthe state space. Our task is to compute the value of the root, based on the reward\nvalues at the real leaves, using the transition function 𝑇𝑎. One way to calculate the\nvalue 𝑉(𝑠) is to traverse this full state space tree, computing the value of a parent\nnode by taking the reward value and the sum of the children, discounting this value\nby 𝛾.\nThis intuitive approach was ﬁrst formalized by Richard Bellman in 1957. Bell-\nman showed that discrete optimization problems can be described as a recursive\nbackward induction problem [72]. He introduced the term dynamic programming\nto recursively traverse the states and actions. The so-called Bellman equation shows\nthe relationship between the value function in state 𝑠and the future child state 𝑠′,\nwhen we follow the transition function.\nThe discrete Bellman equation of the value of state 𝑠after following policy 𝜋is:3\n𝑉𝜋(𝑠) =\n∑︁\n𝑎∈𝐴\n𝜋(𝑎|𝑠)\nh ∑︁\n𝑠′∈𝑆\n𝑇𝑎(𝑠, 𝑠′)\n\u0002\n𝑅𝑎(𝑠, 𝑠′) + 𝛾· 𝑉𝜋(𝑠′)\n\u0003i\n(2.7)\nwhere 𝜋is the probability of action 𝑎in state 𝑠,𝑇is the stochastic transition function,\n𝑅is the reward function and 𝛾is the discount rate. Note the recursion on the value\nfunction, and that for the Bellman equation the transition and reward functions\nmust be known for all states by the agent.\nTogether, the transition and reward model are referred to as the dynamics model\nof the environment. The dynamics model is often not known by the agent, and\nmodel-free methods have been developed to compute the value function and policy\nfunction without them.\nThe recursive Bellman equation is the basis of algorithms to compute the value\nfunction, and other relevant functions to solve reinforcement learning problems. In\nthe next section we will study these solution methods.\n3 State-action value and continuous Bellman equations can be found in Appendix A.4.\n40\n2 Tabular Value-Based Reinforcement Learning\nFig. 2.7 Recursion: Droste eﬀect\n2.2.4 MDP Solution Methods\nThe Bellman equation is a recursive equation: it shows how to calculate the value\nof a state, out of the values of applying the function speciﬁcation again on the\nsuccessor states. Figure 2.7 shows a recursive picture, of a picture in a picture, in\na picture, etc. In algorithmic form, dynamic programming calls its own code on\nstates that are closer and closer to the leaves, until the leaves are reached, and the\nrecursion can not go further.\nDynamic programming uses the principle of divide and conquer: it begins with\na start state whose value is to be determined by searching a large subtree, which\nit does by going down into the recursion, ﬁnding the value of sub-states that are\ncloser to terminals. At terminals the reward values are known, and these are then\nused in the construction of the parent values, as it goes up, back out of the recursion,\nand ultimately arrives at the root value itself.\nA simple dynamic programming method to iteratively traverse the state space to\ncalculate Bellman’s equation is value iteration (VI). Pseudocode for a basic version\nof VI is shown in Listing 2.1, based on [15]. Value iteration converges to the optimal\nvalue function by iteratively improving the estimate of 𝑉(𝑠). The value function\n𝑉(𝑠) is ﬁrst initialized to random values. Value iteration repeatedly updates 𝑄(𝑠, 𝑎)\nand 𝑉(𝑠) values, looping over the states and their actions, until convergence occurs\n(when the values of 𝑉(𝑠) stop changing much).\nValue iteration works with a ﬁnite set of actions. It has been proven to converge\nto the optimal values, but, as we can see in the pseudocode in Listing 2.1, it does so\nquite ineﬃciently by essentially repeatedly enumerating the entire state space in a\ntriply nested loop, traversing the state space many times. Soon we will see more\neﬃcient methods.\n2.2 Tabular Value-Based Agents\n41\n1\ndef\nvalue_iteration ():\n2\ninitialize(V)\n3\nwhile\nnot\nconvergence (V):\n4\nfor s in range(S):\n5\nfor a in range(A):\n6\nQ[s,a] = Í\n𝑠′∈𝑆𝑇𝑎(𝑠, 𝑠′) (𝑅𝑎(𝑠, 𝑠′) + 𝛾𝑉[𝑠′])\n7\nV[s] = max_a(Q[s,a])\n8\nreturn V\nListing 2.1 Value Iteration pseudocode\n2.2.4.1 Hands On: Value Iteration in Gym\nWe have discussed in detail how to model a reinforcement learning problem with\nan MDP. We have talked in depth and at length about states, actions, and policies. It\nis now time for some hands-on work, to experiment with the theoretical concepts.\nWe will start with the environment.\nOpenAI Gym\nOpenAI has created the Gym suite of environments for Python, which has become\nthe de facto standard in the ﬁeld [108]. The Gym suite can be found at OpenAI4\nand on GitHub.5 Gym works on Linux, macOS and Windows. An active community\nexists and new environments are created continuously and uploaded to the Gym\nwebsite. Many interesting environments are available for experimentation, to create\nyour own agent algorithm for, and test it.\nIf you browse Gym on GitHub, you will see diﬀerent sets of environments,\nfrom easy to advanced. There are the classics, such as Cartpole and Mountain car.\nThere are also small text environments. Taxi is there, and the Arcade Learning\nEnvironment [71], which was used in the paper that introduced DQN [522], as we\nwill discuss at length in the next chapter. MuJoCo6 is also available, an environment\nfor experimentation with simulated robotics [780], or you can use pybullet.7\nYou should now install Gym. Go to the Gym page on https://gym.openai.com\nand read the documentation. Make sure Python is installed on your system (does\ntyping python at the command prompt work?), and that your Python version is up\nto date (version 3.10 at the time of this writing). Then type\npip install gym\n4 https://gym.openai.com\n5 https://github.com/openai/gym\n6 http://www.mujoco.org\n7 https://pybullet.org/wordpress/\n42\n2 Tabular Value-Based Reinforcement Learning\n1\nimport\ngym\n2\n3\nenv = gym.make(’CartPole -v0’)\n4\nenv.reset ()\n5\nfor _ in range (1000):\n6\nenv.render ()\n7\nenv.step(env. action_space .sample ()) # take a random\naction\n8\nenv.close ()\nListing 2.2 Running the Gym CartPole Environment from Gym\nFig. 2.8 Taxi world [395]\nto install Gym with the Python package manager. Soon, you will also be needing\ndeep learning suites, such as TensorFlow or PyTorch. It is recommended to install\nGym in the same virtual environment as your upcoming PyTorch and TensorFlow\ninstallation, so that you can use both at the same time (see Sect. B.3.3). You may\nhave to install or update other packages, such as numpy, scipy and pyglet, to get\nGym to work, depending on your system installation.\nYou can check if the installation works by trying if the CartPole environment\nworks, see Listing 2.2. A window should appear on your screen in which a Cartpole\nis making random movements (your window system should support OpenGL, and\nyou may need a version of pyglet newer than version 1.5.11 on some operating\nsystems).\nTaxi Example with Value Iteration\nThe Taxi example (Fig. 2.8) is an environment where taxis move up, down, left,\nand right, and pickup and drop oﬀpassengers. Let us see how we can use value\niteration to solve the Taxi problem. The Gym documentation describes the Taxi\nworld as follows. There are four designated locations in the Grid world indicated by\n2.2 Tabular Value-Based Agents\n43\n1\nimport\ngym\n2\nimport\nnumpy as np\n3\n4\ndef\niterate_value_function (v_inp , gamma , env):\n5\nret = np.zeros(env.nS)\n6\nfor sid in range(env.nS):\n7\ntemp_v = np.zeros(env.nA)\n8\nfor\naction in range(env.nA):\n9\nfor (prob , dst_state , reward , is_final) in env.P[sid\n][ action ]:\n10\ntemp_v[action] += prob *( reward + gamma*v_inp[\ndst_state ]*( not\nis_final))\n11\nret[sid] = max(temp_v)\n12\nreturn\nret\n13\n14\ndef\nbuild_greedy_policy (v_inp , gamma , env):\n15\nnew_policy = np.zeros(env.nS)\n16\nfor\nstate_id\nin range(env.nS):\n17\nprofits = np.zeros(env.nA)\n18\nfor\naction in range(env.nA):\n19\nfor (prob , dst_state , reward , is_final) in env.P[\nstate_id ][ action ]:\n20\nprofits[action] += prob *( reward + gamma*v[\ndst_state ])\n21\nnew_policy[state_id] = np.argmax(profits)\n22\nreturn\nnew_policy\n23\n24\n25\nenv = gym.make(’Taxi -v3’)\n26\ngamma = 0.9\n27\ncum_reward = 0\n28\nn_rounds = 500\n29\nenv.reset ()\n30\nfor\nt_rounds\nin range(n_rounds):\n31\n# init\nenv and\nvalue\nfunction\n32\nobservation = env.reset ()\n33\nv = np.zeros(env.nS)\n34\n35\n# solve\nMDP\n36\nfor _ in range (100):\n37\nv_old = v.copy ()\n38\nv = iterate_value_function (v, gamma , env)\n39\nif np.all(v == v_old):\n40\nbreak\n41\npolicy = build_greedy_policy (v, gamma , env).astype(np.int)\n42\n43\n# apply\npolicy\n44\nfor t in range (1000):\n45\naction = policy[ observation ]\n46\nobservation , reward , done , info = env.step(action)\n47\ncum_reward\n+= reward\n48\nif done:\n49\nbreak\n50\nif\nt_rounds % 50 == 0 and\nt_rounds\n> 0:\n51\nprint(cum_reward * 1.0 / (t_rounds + 1))\n52\nenv.close ()\nListing 2.3 Value Iteration for Gym Taxi\n44\n2 Tabular Value-Based Reinforcement Learning\nR(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts oﬀat a\nrandom square and the passenger is at a random location. The taxi drives to the\npassenger’s location, picks up the passenger, drives to the passenger’s destination\n(another one of the four speciﬁed locations), and then drops oﬀthe passenger. Once\nthe passenger is dropped oﬀ, the episode ends.\nThe Taxi problem has 500 discrete states: there are 25 taxi positions, ﬁve possible\nlocations of the passenger (including the case when the passenger is in the taxi),\nand 4 destination locations (25 × 5 × 4).\nThe environment returns a new result tuple at each step. There are six discrete\ndeterministic actions for the Taxi driver:\n0: Move south\n1: Move north\n2: Move east\n3: Move west\n4: Pick up passenger\n5: Drop oﬀpassenger\nThere is a reward of −1 for each action and an additional reward of +20 for\ndelivering the passenger, and a reward of −10 for executing actions pickup and\ndropoﬀillegally.\nThe Taxi environment has a simple transition function, which is used by the\nagent in the value iteration code.8 Listing 2.3 shows an implementation of value\niteration that uses the Taxi environment to ﬁnd a solution. This code is written by\nMikhail Troﬁmov, and illustrates clearly how value iteration ﬁrst creates the value\nfunction for the states, and then that a policy is formed by ﬁnding the best action\nin each state, in the build-greedy-policy function.9\nTo get a feeling for how the algorithms work, please use the value iteration code\nwith the Gym Taxi environment, see to Listing 2.3. Run the code, and play around\nwith some of the hyperparameters to familiarize yourself a bit with Gym and with\nplanning by value iteration. Try to visualize for yourself what the algorithm is\ndoing. This will prepare you for the more complex algorithms that we will look\ninto next.\n2.2.4.2 Model-Free Learning\nThe value iteration algorithm can compute the policy function. It uses the transition\nmodel in its computation. Frequently, we are in a situation when the transition\nprobabilities are not known to the agent, and we need other methods to compute\nthe policy function. For this situation, model-free algorithms have been developed.\n8 Note that the code uses the environment to compute the next state, so that we do not have to\nimplement a version of the transition function for the agent.\n9\nhttps://gist.github.com/geffy/b2d16d01cbca1ae9e13f11f678fa96fd#file-taxi-vi-\npy\n2.2 Tabular Value-Based Agents\n45\nName\nApproach\nRef\nValue Iteration Model-based enumeration\n[72, 15]\nSARSA\nOn-policy temporal diﬀerence model-free [645]\nQ-learning\nOﬀ-policy temporal diﬀerence model-free [831]\nTable 2.1 Tabular Value-Based Approaches\nThe development of these model-free methods is a major milestone of reinforce-\nment learning, and we will spend some time to understand how they work. We will\nstart with value-based model-free algorithms. We will see how, when the agent does\nnot know the transition function, an optimal policy can be learned by sampling\nrewards from the environment. Table 2.1 lists value iteration in conjunction with\nthe value-based model-free algorithms that we cover in this chapter. (Policy-based\nmodel-free algorithms will be covered in Chap. 4.)\nThese algorithms are based on a few principles. First we will discuss how the\nprinciple of sampling can be used to construct a value function. We discuss both\nfull-episode Monte Carlo sampling and single-step temporal diﬀerence learning;\nwe encounter the principle of bootstrapping and the bias-variance trade-oﬀ; and\nwe will see how the value function can be use to ﬁnd the best actions, to form the\npolicy.\nSecond, we will discuss which mechanisms for action selection exist, where we\nwill encounter the exploration/exploitation trade-oﬀ. Third, we will discuss how to\nlearn from the rewards of the selected actions. We will encounter on-policy learning\nand oﬀ-policy learning. Finally, we wil discuss two full algorithms in which all\nthese concepts come together: SARSA and Q-learning. Let us now start by having a\ncloser look at sampling actions with Monte Carlo sampling and temporal diﬀerence\nlearning.\nMonte Carlo Sampling\nA straightforward way to sample rewards is to generate a random episode, and use\nits return to update the value function at the visited states. This approach consists\nof two loops: a simple loop over the time steps of the episode, embedded in a loop\nto sample long enough for the value function to convergence. This approach, of\nrandomly sampling full episodes, has become known as the Monte Carlo approach\n(after the famous casino, because of the random action selection).\nListing 2.4 shows code for the Monte Carlo approach. We see three elements in\nthe code. First, the main variables are initialized. Then a loop for the desired number\nof total samples performs the unrolling of the episodes. For each episode the state,\naction and reward lists are initialized, and then ﬁlled with the samples from the\nenvironment until we hit the terminal state of the episode.10 Then, at the end of\nthe episode, the return is calcuated in variable 𝑔(the return of a state is the sum of\n10 With epsilon-greedy action selection, see next subsection.\n46\n2 Tabular Value-Based Reinforcement Learning\n1\ndef\nmonte_carlo(n_samples , ep_length , alpha , gamma):\n2\n# 0:\ninitialize\n3\nt = 0;\ntotal_t = 0\n4\nQsa = []\n5\n6\n# sample\nn_times\n7\nwhile\ntotal_t\n< n_samples:\n8\n9\n# 1:\ngenerate a full\nepisode\n10\ns = env.reset ()\n11\ns_ep = []\n12\na_ep = []\n13\nr_ep = []\n14\nfor t in range(ep_length):\n15\na = select_action (s, Qsa)\n16\ns_next , r, done = env.step(a)\n17\ns_ep.append(s)\n18\na_ep.append(a)\n19\nr_ep.append(r)\n20\n21\ntotal_t\n+= 1\n22\nif done or\ntotal_t\n>= n_times:\n23\nbreak;\n24\ns = s_next\n25\n26\n# 2: update Q function\nwith a full\nepisode ( incremental\n27\n# implementation )\n28\ng = 0.0\n29\nfor t in\nreversed(range(len(a_ep))):\n30\ns = s_ep[t]; a = a_ep[t]\n31\ng = r_ep[t] + gamma * g\n32\nQsa[s,a] = Qsa[s,a] + alpha * (g - Qsa[s,a])\n33\n34\nreturn\nQsa\n35\n36\ndef\nselect_action (s, Qsa):\n37\n38\n# policy is\negreedy\n39\nepsilon = 0.1\n40\nif np.random.rand () < epsilon:\n41\na = np.random.randint(low=0,high=env.n_actions)\n42\nelse:\n43\na = argmax(Qsa[s])\n44\nreturn a\n45\n46\nenv = gym.make(’Taxi -v3’)\n47\nmonte_carlo(n_samples =10000 ,\nep_length =100 ,\nalpha =0.1 ,\ngamma\n=0.99)\nListing 2.4 Monte Carlo Sampling code\n2.2 Tabular Value-Based Agents\n47\nits discounted future rewards). The learning rate 𝛼is then used to update the 𝑄\nfunction in an incremental implementation.11 The main purpose of the code is to\nillustate full episode learning. Since it is a complete working algorithms, the code\nalso uses on-policy learning with 𝜖-greedy selection, topics that we will discuss in\nthe next subsection.\nThe Monte Carlo approach is a basic building block of value based reinforcement\nlearning. An advantage of the approach is its simplicity. A disadvantage is that a full\nepisode has to be sampled before the reward values are used, and sample eﬃciency\nmay be low. For this reason (and others, as we will soon see) another approach was\ndeveloped, inspired by the way the Bellman equation bootstraps on intermediate\nvalues.\nTemporal Diﬀerence Learning\nRecall that in value iteration the value function was calculated recursively using\nthe values of successor states, following Bellman’s equation (Eq. 2.7).\nBootstrapping is the process of subsequent reﬁnement by which old estimates\nof a value are reﬁned with new updates. It means literally: pull yourself up (out of\nthe swamp) by your boot straps. Bootstrapping solves the problem of computing a\nﬁnal value when we only know how to compute step-by-step intermediate values.\nBellman’s recursive computation is a form of bootstrapping. In model-free learning,\nwe can use a similar approach, when the role of the transition function is replaced\nby a sequence of environment samples.\nA bootstrapping method that can be used to process the samples, and to reﬁne\nthem to approximate the ﬁnal state values, is temporal diﬀerence learning. Temporal\ndiﬀerence learning, TD for short, was introduced by Sutton [740] in 1988. The\ntemporal diﬀerence in the name refers to the diﬀerence in values between two time\nsteps, which are used to calculate the value at the new time step.\nTemporal diﬀerence learning works by updating the current estimate of the state\nvalue 𝑉(𝑠) (the bootstrap-value) with an error value (new minus current) based on\nthe estimate of the next state that it has gotten through sampling the environment:\n𝑉(𝑠) ←𝑉(𝑠) + 𝛼[𝑟′ + 𝛾𝑉(𝑠′) −𝑉(𝑠)]\n(2.8)\nHere 𝑠is the current state, 𝑠′ the new state, and 𝑟′ the reward of the new state.\nNote the introduction of 𝛼, the learning rate, which controls how fast the algorithm\nlearns (bootstraps). It is an important parameter; setting the value too high can be\ndetrimental since the last value then dominates the bootstrap process too much.\nFinding the optimal value will require experimentation. The 𝛾parameter is the\ndiscount rate. The last term −𝑉(𝑠) subtracts the value of the current state, to\ncompute the temporal diﬀerence. Another way to write this update rule is\n11 The incremental implementation works for nonstationary situations, where the transition\nprobabilities may change, hence the previous 𝑄values are subtracted.\n48\n2 Tabular Value-Based Reinforcement Learning\n1\ndef\ntemporal_difference (n_samples , alpha , gamma):\n2\n# 0:\ninitialize\n3\nQsa = []\n4\ns = env.reset ()\n5\n6\nfor t in range(n_samples):\n7\na = select_action (s, Qsa)\n8\ns_next , r, done = env.step(a)\n9\n10\n# update Q function\neach\ntime\nstep\nwith\nmax of action\nvalues\n11\nQsa[s,a] = Qsa[s,a] + alpha * (r + gamma * np.max(Qsa[\ns_next ]) - Qsa[s,a])\n12\n13\nif done:\n14\ns = env.reset ()\n15\nelse:\n16\ns = s_next\n17\n18\nreturn\nQsa\n19\n20\ndef\nselect_action (s, Qsa):\n21\n22\n# policy is\negreedy\n23\nepsilon = 0.1\n24\nif np.random.rand () < epsilon:\n25\na = np.random.randint(low=0,high=env.n_actions)\n26\nelse:\n27\na = argmax(Qsa[s])\n28\nreturn a\n29\n30\nenv = gym.make(’Taxi -v3’)\n31\ntemporal_difference (n_samples =10000 ,\nalpha =0.1 ,\ngamma =0.99)\nListing 2.5 Temporal Diﬀerence Q-learning code\n𝑉(𝑠) ←𝛼[𝑟′ + 𝛾𝑉(𝑠′)] + (1 −𝛼)𝑉(𝑠)\nas the diﬀerence between the new temporal diﬀerence target and the old value.\nNote the absence of transition model 𝑇in the formula; temporal diﬀerence is a\nmodel-free update formula. Listing 2.5 shows code for the TD approach, for the\nstate-action value function. (This code is oﬀ-policy, and uses the same 𝜖-greedy\nselection function as Monte Carlo sampling.)\nThe introduction of the temporal diﬀerence method has allowed model-free\nmethods to be used successfully in various reinforcement learning settings. Most\nnotably, it was the basis of the program TD-Gammon, that beat human world-\nchampions in the game of Backgammon in the early 1990s [763].\n2.2 Tabular Value-Based Agents\n49\nFig. 2.9 High and Low Bias, and High and Low Variance\nBias-Variance Trade-oﬀ\nA crucial diﬀerence between the Monte Carlo method and the temporal diﬀerence\nmethod is the use of bootstrapping to calculate the value function. The use of\nbootstrapping has an important consequence: it trades oﬀbias and variance (see\nFig. 2.9). Monte Carlo does not use bootstrapping. It performs a full episode with\nmany random action choices before it uses the reward. As such, its action choices\nare unbiased (they are fully random), they are not inﬂuenced by previous reward\nvalues. However, the fully random choices also cause a high variance of returns\nbetween episodes. We say that Monte Carlo is a low-bias/high-variance algorithm.\nIn contrast, temporal diﬀerence bootstraps the 𝑄-function with the values of\nthe previous steps, reﬁning the function values with the rewards after each single\nstep. It learns more quickly, at each step, but once a step has been taken, old reward\nvalues linger around in the bootstrapped function value for a long time, biasing the\nfunction value. On the other hand, because these old values are part of the new\nbootstrapped value, the variance is lower. Thus, because of bootstrapping, TD is\na high-bias/low variance method. Figure 2.9 illustrates the concepts of bias and\nvariance with pictures of dart boards.\nBoth approaches have their uses in diﬀerent circumstances. In fact, we can think\nof situations where a middle ground (of medium bias/medium variance) might be\nuseful. This is the idea behind the so-called n-step approach: do not sample a full\nepisode, and also not a single step, but sample a few steps at a time before using\nthe reward values. The n-step algorithm has medium bias and medium variance.\n50\n2 Tabular Value-Based Reinforcement Learning\nFig. 2.10 Single Step Temporal Diﬀerence Learning, N-Step, and Monte Carlo Sampling [743]\nFigure 2.10 from [743] illustrates the relation between Monte Carlo sampling, n-step,\nand temporal diﬀerence learning.\nFind Policy by Value-based Learning\nThe goal of reinforcement learning is to construct the policy with the highest\ncumulative reward. Thus, we must ﬁnd the best action 𝑎in each state 𝑠. In the\nvalue-based approach we know the value functions 𝑉(𝑠) or 𝑄(𝑠, 𝑎). How can that\nhelp us to ﬁnd action 𝑎? In a discrete action space, there is at least one discrete\naction with the highest value. Thus, if we have the optimal state-value 𝑉★, then the\noptimal policy can be found by ﬁnding the action with that value. This relationship\nis given by\n𝜋★= max\n𝜋\n𝑉𝜋(𝑠) = max\n𝑎, 𝜋𝑄𝜋(𝑠, 𝑎)\nand the arg max function ﬁnds the best action for us\n𝑎★= arg max\n𝑎∈𝐴\n𝑄★(𝑠, 𝑎).\nIn this way the optimal policy sequence of best actions 𝜋★(𝑠) can be recovered from\nthe values, hence the name value-based method [846].\nA full reinforcement learning algorithm consists of a rule for the selection part\n(downward) and a rule for the learning part (upward). Now that we know how to\n2.2 Tabular Value-Based Agents\n51\ncalculate the value function (the up-motion in the tree diagram), let us see how\nwe can select the action in our model-free algorithm (the down-motion in the tree\ndiagram).\n2.2.4.3 Exploration\nSince there is no local transition function, model-free methods perform their state\nchanges directly in the environment. This may be an expensive operation, for\nexample, when a real-world robot arm has to perform a movement. The sampling\npolicy should choose promising actions to reduce the number of samples as much\nas possible, and not waste any actions. What behavior policy should we use? It is\ntempting to favor at each state the actions with the highest Q-value, since then we\nwould be following what is currently thought to be the best policy.\nThis approach is called the greedy approach. It appears attractive, but is short-\nsighted and risks settling for local maxima. Following the trodden path based on\nonly a few early samples risks missing a potential better path. Indeed, the greedy\napproach is high bias, using values based on few samples. We run the risk of circular\nreinforcement, if we update the same behavior policy that we use to choose our\nsamples from. In addition to exploiting known good actions, a certain amount of\nexploration of unknown actions is necessary. Smart sampling strategies use a mix\nof the current behavior policy (exploitation) and randomness (exploration) to select\nwhich action to perform in the environment.\nBandit Theory\nThe exploration/exploitation trade-oﬀ, the question of how to get the most reliable\ninformation at the least cost, has been studied extensively in the literature for single\nstep decision problems [346, 845]. The ﬁeld has the colorful name of multi-armed\nbandit theory [30, 443, 279, 632]. A bandit in this context refers to a casino slot\nmachine, with not one arm, but many arms, each with a diﬀerent and unknown\npayout probability. Each trial costs a coin. The multi-armed bandit problem is then\nto ﬁnd a strategy that ﬁnds the arm with the highest payout at the least cost.\nA multi-armed bandit is a single-state single-decision reinforcement learning\nproblem, a one-step non-sequential decision making problem, with the arms repre-\nsenting the possible actions. This simpliﬁed model of stochastic decision making\nallows the in-depth study of exploration/exploitation strategies.\nSingle-step exploration/exploitation questions arise for example in clinical trials,\nwhere new drugs are tested on test-subjects (real people). The bandit is the trial, and\nthe arms are the choice how many of the test subjects are given the real experimental\ndrug, and how many are given the placebo. This is a serious setting, since the cost\nmay be measured in the quality of human lives.\nIn a conventional ﬁxed randomized controlled trial (supervised setup) the sizes\nof the groups that get the experimental drugs and the control group would be ﬁxed,\n52\n2 Tabular Value-Based Reinforcement Learning\nFig. 2.11 Adaptive Trial [5]\nand the conﬁdence interval and the duration of the test would also be ﬁxed. In an\nadaptive trial (bandit setup) the sizes would adapt during the trial depending on\nthe outcomes, with more people getting the drug if it appears to work, and fewer if\nit does not.\nLet us have a look at Fig. 2.11. Assume that the learning process is a clinical trial\nin which three new compounds are tested for their medical eﬀect on test subjects.\nIn the ﬁxed trial (left panel) all test subjects receive the medicine of their group to\nthe end of the test period, after which the data set is complete and we can determine\nwhich of the compounds has the best eﬀect. At that point we know which group has\nhad the best medicine, and which two thirds of the subjects did not, with possibly\nharmful eﬀect. Clearly, this is not a satisfactory situation. It would be better if we\ncould gradually adjust the proportion of the subjects that receive the medicine\nthat currently looks best, as our conﬁdence in our test results increases as the trial\nprogresses. Indeed, this is what reinforcement learning does (Fig. 2.11, right panel).\nIt uses a mix of exploration and exploitation, adapting the treatment, giving more\nsubjects the promising medicine, while achieving the same conﬁdence as the static\ntrial at the end [443, 442].\n𝝐-greedy Exploration\nA popular pragmatic exploration/exploitation approach is to use a ﬁxed ratio of\nexploration versus exploitation. This approach is known as the 𝜖-greedy approach,\nwhich is to mostly try the (greedy) action that currently has the highest policy value\nexcept to explore an 𝜖fraction of times a randomly selected other action. If 𝜖= 0.1\nthen 90% of the times the currently-best action is taken, and 10% of the times a\nrandom other action.\nThe algorithmic choice between greedily exploiting known information and\nexploring unknown actions to gain new information is called the exploration/ex-\nploitation trade-oﬀ. It is a central concept in reinforcement learning; it determines\nhow much conﬁdence we have in our outcome, and how quickly the conﬁdence can\n2.2 Tabular Value-Based Agents\n53\nbe increased and the variance reduced. A second approach is to use an adapative\n𝜖-ratio, that changes over time, or over other statistics of the learning process.\nOther popular approaches to add exploration are to add Dirichlet-noise [425] or\nto use Thompson sampling [770, 648].\n2.2.4.4 Oﬀ-Policy Learning\nIn addition to the selection question, another main theme in the design of full\nreinforcement learning algorithms is which learning method to use. Reinforcement\nlearning is concerned with learning an action-policy from the rewards. The agent\nselects an action to perform, and learns from the reward that it gets back from the\nenvironment. The question is whether the agent should perform updates strictly\non-policy—only learning from its most recent action—or allow oﬀ-policy updates,\nlearning from all available information.\nIn on-policy learning, the learning takes place by using the value of the action\nthat was selected by the policy. The policy determines the action to take, and the\nvalue of that action is used to update the value of the policy function: the learning\nis on-policy.\nThere is, however, an alternative to this straightforward method. In oﬀ-policy\nmethods, the learning takes place by backing up values of another action, not\nnecessarily the one selected by the behavior policy. This method makes sense when\nthe agent explores. When the behavior policy explores, it selects a non-optimal\naction. The policy does not perform the greedy exploitation action; of course,\nthis usually results in an inferior reward value. On-policy learning would then\nblindly backup the value of the non-optimal exploration action. Oﬀ-policy learning,\nhowever, is free to backup another value instead. It makes sense to choose the value\nof the best action, and not the inferior one selected by the exploration policy. The\nadvantage of this oﬀ-policy approach is that it does not pollute the behavior policy\nwith a value that is most likely inferior.\nThe diﬀerence between on-policy and oﬀ-policy is only in how they act when\nexploring the non-greedy action. In the case of exploration, oﬀ-policy learning can\nbe more eﬃcient, by not stubbornly backing up the value of the action selected by\nthe behavior policy, but the value of an older, better, action.\nAn important point is that the convergence behavior of on-policy and oﬀ-policy\nlearning is diﬀerent. In general, tabular reinforcement learning have been proven to\nconverge when the policy is greedy in the limit with inﬁnite exploration (GLIE) [743].\nThis means that (1) if a state is visited inﬁnitely often, that each action is also chosen\ninﬁnitely often, and that (2) in the limit the policy is greedy with respect to the\nlearned 𝑄function. Oﬀ-policy methods learn from the greedy rewards and thus\nconverge to the optimal policy, after having sampled enough states. However, on-\npolicy methods with a ﬁxed 𝜖do not converge to the optimal policy, since they keep\nselecting explorative actions. When we use a variable-𝜖-policy in which the value\n54\n2 Tabular Value-Based Reinforcement Learning\nof 𝜖goes to zero, then on-policy methods do converge, since then they choose, in\nthe limit, the greedy action.12\nA well-known tabular on-policy algorithm is SARSA.13 An even more well-\nknown oﬀ-policy algorithm is Q-learning.\nOn-Policy SARSA\nSARSA is an on-policy algorithm [645]. On-policy learning updates the policy with\nthe action values of the policy. The SARSA update formula is\n𝑄(𝑠𝑡, 𝑎𝑡) ←𝑄(𝑠𝑡, 𝑎𝑡) + 𝛼[𝑟𝑡+1 + 𝛾𝑄(𝑠𝑡+1, 𝑎𝑡+1) −𝑄(𝑠𝑡, 𝑎𝑡)].\n(2.9)\nGoing back to temporal diﬀerence (Eq. 2.8), we see that the SARSA formula looks\nvery much like TD, although now we deal with state-action values.\nOn-policy learning selects an action, evaluates it in the environment, and follows\nthe actions, guided by the behavior policy. The behavior policy is not speciﬁed by\nthe formula, but might be 𝜖-greedy, or an other policy that trades oﬀexploration\nand exploitation. On-policy learning samples the state space following the behavior\npolicy, and improves the policy by backing up values of the selected actions. Note\nthat the term 𝑄(𝑠𝑡+1, 𝑎𝑡+1) can also be written as 𝑄(𝑠𝑡+1, 𝜋(𝑠𝑡+1)), to highlight the\ndiﬀerence with oﬀ-policy learning. SARSA updates its Q-values using the Q-value of\nthe next state 𝑠and the current policy’s action. The primary advantage of on-policy\nlearning is its predictive behavior.\nOﬀ-Policy Q-Learning\nOﬀ-policy learning is more complicated; it may learn its policy from actions that\nare diﬀerent from the one just taken.\nThe best-known oﬀ-policy algorithm is Q-learning [831]. It performs exploiting\nand exploring selection actions as before, but it evaluates states as if a greedy policy\nis used always, even when the actual behavior performed an exploration step.\nThe Q-learning update formula is\n𝑄(𝑠𝑡, 𝑎𝑡) ←𝑄(𝑠𝑡, 𝑎𝑡) + 𝛼[𝑟𝑡+1 + 𝛾max\n𝑎\n𝑄(𝑠𝑡+1, 𝑎) −𝑄(𝑠𝑡, 𝑎𝑡)].\n(2.10)\n12 However, in the next chapter, deep learning is introduced, and a complication arises. In deep\nlearning states and values are no longer exact but are approximated. Now, oﬀ-policy methods\nbecome less stable than on-policy methods. In a neural network, states are “connected” via joint\nfeatures. The max-operator in oﬀ-policy methods pushes up training targets of these connected\nstates. As a consequence deep oﬀ-policy methods may not converge. A so-called deadly triad\nof function approximation, bootstrapping and oﬀ-policy learning occurs that causes unstable\nconvergence. Because of this, with function approximation, on-policy methods are sometimes\nfavored.\n13 The name of the SARSA algorithm is a play on the MDP symbols as they occur in the action\nvalue update formula: 𝑠, 𝑎, 𝑟, 𝑠, 𝑎.\n2.2 Tabular Value-Based Agents\n55\nThe only diﬀerence from on-policy learning is that the 𝛾𝑄(𝑠𝑡+1, 𝑎𝑡+1) term from\nEq. 2.9 has been replaced by 𝛾max𝑎𝑄(𝑠𝑡+1, 𝑎). We now learn from backup values\nof the best action, not the one that was actually evaluated. Listing 2.5 showed\nthe pseudocode for Q-learning. Indeed, the term temporal diﬀerence learning is\nsometimes used for the Q-learning algorithm.\nThe reason that Q-learning is called oﬀ-policy is that it updates its Q-values\nusing the Q-value of the next state 𝑠𝑡+1, and the greedy action (not necessarily the\nbehavior policy’s action—it is learning oﬀthe behavior policy). Oﬀ-policy learning\ncollects all available information and uses it to construct the best target policy.\nSparse Rewards and Reward Shaping\nBefore we conclude this section, we should discuss sparsity. For some environments\na reward exists for each state. For the supermarket example a reward can be calcu-\nlated for each state that the agent has walked to. (The reward is the opposite of the\ncost expended in walking.) Environments in which a reward exists in each state are\nsaid to have a dense reward structure.\nFor other environments rewards may exist for only some of the states. For\nexample, in chess, rewards only exist at terminal board positions where there is a\nwin or a draw. In all other states the return depends on the future states and must be\ncalculated by the agent by propagating reward values from future states up towards\nthe root state 𝑠0. Such an environment is said to have a sparse reward structure.\nFinding a good policy is more complicated when the reward structure is sparse.\nA graph of the landscape of such a sparse reward function would show a ﬂat\nlandscape with a few sharp mountain peaks. Reinforcement learning algorithms use\nthe reward-gradient to ﬁnd good returns. Finding the optimum in a ﬂat landscape\nwhere the gradient is zero, is hard. In some applications it is possible to change the\nreward function to have a shape more amenable to gradient-based optimization\nalgorithms such as we use in deep learning. Reward shaping can make all the\ndiﬀerence when no solution can be found with a naive reward function. It is a way\nof incorporating heuristic knowledge into the MDP. A large literature on reward\nshaping and heuristic information exists [560]. The use of heuristics on board games\nsuch as chess and checkers can also be regarded as reward shaping.\n2.2.4.5 Hands On: Q-learning on Taxi\nTo get a feeling for how these algorithms work in practice, let us see how Q-learning\nsolves the Taxi problem.\nIn Sect. 2.2.4.1 we discussed how value iteration can be used for the Taxi problem,\nprovided that the agent has access to the transition model. We will now see how\nwe solve this problem if we do not have the transition model. Q-learning samples\nactions, and records the reward values in a Q-table, converging to the state-action\n56\n2 Tabular Value-Based Reinforcement Learning\nvalue function. When in all states the best values of the best actions are known,\nthen these can be used to sequence the optimal policy.\nLet us see how a value-based model-free algorithm solves a simple 5 × 5 Taxi\nproblem. Refer to Fig. 2.8 on page 42 for an illustration of Taxi world.\nPlease recall that in Taxi world, the taxi can be in one of 25 locations and there\nare 25 × (4 + 1) × 4 = 500 diﬀerent states that the environment can be in.\nWe follow the reward model as it is used in the Gym Taxi environment. Recall\nthat our goal is to ﬁnd a policy (actions in each state) that leads to the highest\ncumulative reward. Q-learning learns the best policy through guided sampling.\nThe agent records the rewards that it gets from actions that it performs in the\nenvironment. The Q-values are the expected rewards of the actions in the states.\nThe agent uses the Q-values to guide which actions it will sample. Q-values 𝑄(𝑠, 𝑎)\nare stored in an array that is indexed by state and action. The Q-values guide the\nexploration, higher values indicate better actions.\nListing 2.6 shows the full Q-learning algorithm, in Python, after [395]. It uses\nan 𝜖-greedy behavior policy: mostly the best action is followed, but in a certain\nfraction a random action is chosen, for exploration. Recall that the Q-values are\nupdated according to the Q-learning formula:\n𝑄(𝑠𝑡, 𝑎𝑡) ←𝑄(𝑠𝑡, 𝑎𝑡) + 𝛼[𝑟𝑡+1 + 𝛾max\n𝑎\n𝑄(𝑠𝑡+1, 𝑎) −𝑄(𝑠𝑡, 𝑎𝑡)]\nwhere 0 ≤𝛾≤1 is the discount factor and 0 < 𝛼≤1 the learning rate. Note that\nQ-learning uses bootstrapping, and the initial Q-values are set to a random value\n(their value will disappear slowly due to the learning rate).\nQ-learning learns the best action in the current state by looking at the reward\nfor the current state-action combination, plus the maximum rewards for the next\nstate. Eventually the best policy is found in this way, and the taxi will consider the\nroute consisting of a sequence of the best rewards.\nTo summarize informally:\n1. Initialize the Q-table to random values\n2. Select a state 𝑠\n3. For all possible actions from 𝑠select the one with the highest Q-value and travel\nto this state, which becomes the new 𝑠, or, with 𝜖greedy, explore\n4. Update the values in the Q-array using the equation\n5. Repeat until the goal is reached; when the goal state is reached, repeat the process\nuntil the Q-values stop changing (much), then stop.\nListing 2.6 shows Q-learning code for ﬁnding the policy in Taxi world.\nThe optimal policy can be found by sequencing together the actions with the\nhighest Q-value in each state. Listing 2.7 shows the code for this. The number of\nillegal pickups/drop-oﬀs is shown as penalty.\nThis example shows how the optimal policy can be found by the introduction of\na Q-table that records the quality of irreversible actions in each state, and uses that\ntable to converge the rewards to the value function. In this way the optimal policy\ncan be found model-free.\n2.2 Tabular Value-Based Agents\n57\n1\n# Q learning\nfor\nOpenAI\nGym\nTaxi\nenvironment\n2\nimport\ngym\n3\nimport\nnumpy as np\n4\nimport\nrandom\n5\n# Environment\nSetup\n6\nenv = gym.make(\"Taxi -v2\")\n7\nenv.reset ()\n8\nenv.render ()\n9\n# Q[state ,action] table\nimplementation\n10\nQ = np.zeros ([env. observation_space .n, env. action_space .n])\n11\ngamma = 0.7\n# discount\nfactor\n12\nalpha = 0.2\n# learning\nrate\n13\nepsilon = 0.1 # epsilon\ngreedy\n14\nfor\nepisode\nin range (1000):\n15\ndone = False\n16\ntotal_reward = 0\n17\nstate = env.reset ()\n18\nwhile\nnot\ndone:\n19\nif random.uniform (0, 1) < epsilon:\n20\naction = env. action_space .sample () # Explore\nstate\nspace\n21\nelse:\n22\naction = np.argmax(Q[state ]) # Exploit\nlearned\nvalues\n23\nnext_state , reward , done , info = env.step(action) #\ninvoke\nGym\n24\nnext_max = np.max(Q[next_state ])\n25\nold_value = Q[state ,action]\n26\n27\nnew_value = old_value + alpha * (reward + gamma *\nnext_max\n- old_value)\n28\n29\nQ[state ,action] = new_value\n30\ntotal_reward\n+= reward\n31\nstate = next_state\n32\nif\nepisode % 100 == 0:\n33\nprint(\"Episode␣{}␣Total␣Reward:␣{}\".format(episode ,\ntotal_reward ))\nListing 2.6 Q-learning Taxi example, after [395]\nTuning your Learning Rate\nGo ahead, implement and run this code, and play around to become familiar with\nthe algorithm. Q-learning is an excellent algorithm to learn the essence of how\nreinforcement learning works. Try out diﬀerent values for hyperparameters, such\nas the exploration parameter 𝜖, the discount factor 𝛾and the learning rate 𝛼. To\nbe successful in this ﬁeld, it helps to have a feeling for these hyperparameters. A\nchoice close to 1 for the discount parameter is usually a good start, and a choice\nclose to 0 for the learning rate is a good start. You may feel a tendency to do the\nopposite, to choose the learning rate as high as possible (close to 1) to learn as\n58\n2 Tabular Value-Based Reinforcement Learning\n1\ntotal_epochs , total_penalties = 0, 0\n2\nep = 100\n3\nfor _ in range(ep):\n4\nstate = env.reset ()\n5\nepochs , penalties , reward = 0, 0, 0\n6\ndone = False\n7\nwhile\nnot\ndone:\n8\naction = np.argmax(Q[state ])\n9\nstate , reward , done , info = env.step(action)\n10\nif reward\n==\n-10:\n11\npenalties\n+= 1\n12\nepochs\n+= 1\n13\ntotal_penalties\n+=\npenalties\n14\ntotal_epochs\n+= epochs\n15\nprint(f\"Results␣after␣{ep}␣episodes:\")\n16\nprint(f\"Average␣timesteps␣per␣episode:␣{ total_epochs ␣/␣ep}\")\n17\nprint(f\"Average␣penalties␣per␣episode:␣{ total_penalties ␣/␣ep}\")\nListing 2.7 Evaluate the optimal Taxi result, after [395]\nquickly as possible. Please go ahead and see which works best in Q-learning (you\ncan have a look at [231]). In many deep learning environments a high learning rate\nis a recipe for disaster, your algorithm may not converge at all, and Q-values can\nbecome unbounded. Play around with tabular Q-learning, and approach your deep\nlearning slowly, with gentle steps!\nThe Taxi example is small, and you will get results quickly. It is well suited to build\nup useful intuition. In later chapters, we will do experiments with deep learning\nthat take longer to converge, and acquiring intuition for tuning hyperparameter\nvalues will be more expensive.\nConclusion\nWe have now seen how a value function can be learned by an agent without having\nthe transition function, by sampling the environment. Model-free methods use\nactions that are irreversible for the agent. The agent samples states and rewards\nfrom the environment, using a behavior policy with the current best action, and\nfollowing an exploration/exploitation trade-oﬀ. The backup rule for learning is based\non bootstrapping, and can follow the rewards of the actions on-policy, including\nthe value of the occasional explorative action, or oﬀ-policy, always using the value\nof the best action. We have seen two model-free tabular algorithms, SARSA and\nQ-learning, where the value function is assumed to be stored in an exact table\nstructure.\nIn the next chapter we will move to network-based algorithms for high-\ndimensional state spaces, based on function approximation with a deep neural\nnetwork.\n2.3 Classic Gym Environments\n59\nFig. 2.12 Cartpole and Mountain Car\n2.3 Classic Gym Environments\nNow that we have discussed at length the tabular agent algorithms, it is time to\nhave a look at the environments, the other part of the reinforcement learning model.\nWithout them, progress cannot be measured, and results cannot be compared in a\nmeaningful way. In a real sense, environments deﬁne the kind of intelligence that\nour artiﬁcial methods can be trained to perform.\nIn this chapter we will start with a few smaller environments, that are suited\nfor the tabular algorithms that we have discussed. Two environments that have\nbeen around since the early days of reinforcement learning are Mountain car and\nCartpole (see Fig. 2.12).\n2.3.1 Mountain Car and Cartpole\nMountain car is a physics puzzle in which a car on a one-dimensional road is in\na valley between two mountains. The goal for the car is to drive up the mountain\nand reach the ﬂag on the right. The car’s engine can go forward and backward. The\nproblem is that the car’s engine is not strong enough to climb the mountain by itself\nin a single pass [532], but it can do so with the help of gravity: by repeatedly going\nback and forth the car can build up momentum. The challenge for the reinforcement\nlearning agent is to apply alternating backward and forward forces at the right\nmoment.\nCartpole is a pole-balancing problem. A pole is attached by a joint to a movable\ncart, which can be pushed forward or backward. The pendulum starts upright, and\nmust be kept upright by applying either a force of +1 or −1 to the cart. The puzzle\nends when the pole falls over, or when the cart runs too far left or right [57]. Again\nthe challenge is to apply the right force at the right moment, solely by feedback of\nthe pole being upright or too far down.\n60\n2 Tabular Value-Based Reinforcement Learning\n2.3.2 Path Planning and Board Games\nNavigation tasks and board games provide environments for reinforcement learning\nthat are simple to understand. They are well suited to reason about new agent\nalgorithms. Navigation problems, and the heuristic search trees built for board\ngames, can be of moderate size, and are then suited for determining the best action\nby dynamic programming methods, such as tabular Q-learning, A*, branch and\nbound, and alpha-beta [647]. These are straightforward search methods that do not\nattempt to generalize to new, unseen, states. They ﬁnd the best action in a space of\nstates, all of which are present at training time—the optimization methods do not\nperform generalization from training to test time.\nPath Planning\nPath planning (Fig 2.1) is a classic problem that is related to robotics [456, 265]. Pop-\nular versions are mazes, as we have seen earlier (Fig. 2.2). The Taxi domain (Fig. 2.8)\nwas originally introduced in the context of hierarchical problem solving [196]. Box-\npushing problems such as Sokoban are frequently used as well [386, 204, 543, 878],\nsee Fig. 2.3. The action space of these puzzles and mazes is discrete. Basic path and\nmotion planning can enumerate possible solutions [169, 322].\nSmall versions of mazes can be solved exactly by enumeration, larger instances\nare only suitable for approximation methods. Mazes can be used to test algorithms\nfor path ﬁnding problems and are frequently used to do so. Navigation tasks and\nbox-pushing games such as Sokoban can feature rooms or subgoals, that may then\nbe used to test algorithms for hierarchically structured problems [236, 298, 615, 239]\n(Chap. 8). The problems can be made more diﬃcult by enlarging the grid and by\ninserting more obstacles.\nBoard Games\nBoard games are a classic group of benchmarks for planning and learning since\nthe earliest days of artiﬁcial intelligence. Two-person zero-sum perfect information\nboard games such as tic tac toe, chess, checkers, Go, and shogi have been used to\ntest algorithms since the 1950s. The action space of these games is discrete. Notable\nachievements were in checkers, chess, and Go, where human world champions\nwere defeated in 1994, 1997, and 2016, respectively [663, 124, 703].\nThe board games are typically used “as is” and are not changed for diﬀerent\nexperiments (in contrast to mazes, that are often adapted in size or complexity for\nspeciﬁc purposes of the experiment). Board games are used for the diﬃculty of the\nchallenge. The ultimate goals is to beat human grandmasters or even the world\nchampion. Board games have been traditional mainstays of artiﬁcial intelligence,\nmostly associated with the search-based symbolic reasoning approach to artiﬁcial\n2.3 Classic Gym Environments\n61\nintelligence [647]. In contrast, the benchmarks in the next chapter are associated\nwith connectionist artiﬁcial intelligence.\nSummary and Further Reading\nThis has been a long chapter, to provide a solid basis for the rest of the book. We\nwill summarize the chapter, and provide references for further reading.\nSummary\nReinforcement learning can learn behavior that achieves high rewards, using feed-\nback from the environment. Reinforcement learning has no supervisory labels,\nit can learn beyond a teacher, as long as there is an environment that provides\nfeedback.\nReinforcement learning problems are modeled as a Markov decision problem,\nconsisting of a 5-tuple (𝑆, 𝐴,𝑇𝑎, 𝑅𝑎, 𝛾) for states, actions, transition, reward, and\ndiscount factor. The agent performs an action, and the environment returns the\nnew state and the reward value to be associated with the new state.\nGames and robotics are two important ﬁelds of application. Fields of application\ncan be episodic (they end—such as a game of chess) or continuous (they do not\nend—a robot remains in the world). In continuous problems it often makes sense to\ndiscount behavior that is far from the present, episodic problems typically do not\nbother with a discount factor—a win is a win.\nEnvironments can be deterministic (many board games are deterministic—boards\ndon’t move) or stochastic (many robotic worlds are stochastic—the world around a\nrobot moves). The action space can be discrete (a piece either moves to a square or\nit does not) or continuous (typical robot joints move continuously over an angle).\nThe goal in reinforcement learning is to ﬁnd the optimal policy that gives for\nall states the best actions, maximizing the cumulative future reward. The policy\nfunction is used in two diﬀerent ways. In a discrete environment the policy function\n𝑎= 𝜋(𝑠) returns for each state the best action in that sate. (Alternatively the value\nfunction returns the value of each action in each state, out of which the argmax\nfunction can be used to ﬁnd the actions with the highest value.)\nThe optimal policy can be found by ﬁnding the maximal value of a state. The\nvalue function 𝑉(𝑠) returns the expected reward for a state. When the transition\nfunction 𝑇𝑎(𝑠, 𝑠′) is present, the agent can use Bellman’s equation, or a dynamic\nprogramming method to recursively traverse the behavior space. Value iteration is\none such dynamic programming method. Value iteration traverses all actions of\nall states, backing up reward values, until the value function stops changing. The\nstate-action value 𝑄(𝑠, 𝑎) determines the value of an action of a state.\n62\n2 Tabular Value-Based Reinforcement Learning\nBellman’s equation calculates the value of a state by calculating the value of\nsuccessor states. Accessing successor states (by following the action and transition)\nis also called expanding a successor state. In a tree diagram successor states are\ncalled child nodes, and expanding is a downward action. Backpropagating the\nreward values to the parent node is a movement upward in the tree.\nMethods where the agent makes use of the transition model are called model-\nbased methods. When the agent does not use the transition model, they are model-\nfree methods. In many situations the learning agent does not have access to the\ntransition model of the environment, and planning methods cannot be used by the\nagent. Value-based model-free methods can ﬁnd an optimal policy by using only\nirreversible actions, sampling the environment to ﬁnd the value of the actions.\nA major determinant in model-free reinforcement learning is the exploration/ex-\nploitation trade-oﬀ, or how much of the information that has been learned from the\nenvironment is used in choosing actions to sample. We discussed the advantages\nof exploiting the latest knowledge in settings where environment actions are very\ncostly, such as clinial trials. A well-known exploration/exploitation method is 𝜖-\ngreedy, where the greedy (best) action is followed from the behavior policy, except\nin 𝜖times, when random exploration is performed. Always following the policy’s\nbest action runs the risk of getting stuck in a cycle. Exploring random nodes allows\nbreaking free of such cycles.\nSo far we have discussed the action selection operation. How should we process\nthe rewards that are found at nodes? Here we introduced another fundamental\nelement of reinforcement learning: bootstrapping, or ﬁnding a value by reﬁning a\nprevious value. Temporal diﬀerence learning uses the principle of bootstrapping to\nﬁnd the value of a state by adding appropriately discounted future reward values\nto the state value function.\nWe have now discussed both up and down motions, and can construct full model-\nfree algorithms. The best-known algorithm may well be Q-learning, which learns\nthe action-value function of each action in each state through oﬀ-policy temporal\ndiﬀerence learning. Oﬀ-policy algorithms improve the policy function with the\nvalue of the best action, even if the (exploring) behavior action was diﬀerent.\nIn the next chapters we will look at value-based and policy-based model-free\nmethods for large, complex problems, that make use of function approximation\n(deep learning).\nFurther Reading\nThere is a rich literature on tabular reinforcement learning. A standard work for\ntabular value-based reinforcement learning is Sutton and Barto [743]. Two con-\ndensed introductions to reinforcement learning are [27, 255]. Another major work\non reinforcement learning is Bertsekas and Tsitsiklis [86]. Kaelbling has written an\nimportant survey article on the ﬁeld [389]. The early works of Richard Bellman on\ndynamic programming, and planning algorithms are [72, 73]. For a recent treatment\n2.3 Classic Gym Environments\n63\nof games and reinforcement learning, with a focus on heuristic search methods and\nthe methods behind AlphaZero, see [600].\nThe methods of this chapter are based on bootstrapping [72] and temporal\ndiﬀerence learning [740]. The on-policy algorithm SARSA [645] and the oﬀ-policy\nalgorithm Q-Learning [831] are among the best known exact, tabular, value-based\nmodel-free algorithms.\nMazes and Sokoban grids are sometimes procedurally generated [692, 329, 781].\nThe goal for the algorithms is typically to ﬁnd a solution for a grid of a certain\ndiﬃculty class, to ﬁnd a shortest path solution, or, in transfer learning, to learn to\nsolve a class of grids by training on a diﬀerent class of grids [859].\nFor general reference, one of the major textbooks on artiﬁcial intelligence is\nwritten by Russell and Norvig [647]. A more speciﬁc textbook on machine learning\nis by Bishop [93].\nExercises\nWe will end with questions on key concepts, with programming exercises to build\nup more experience,\nQuestions\nThe questions below are meant to refresh your memory, and should be answered\nwith yes, no, or short answers of one or two sentences.\n1. In reinforcement learning the agent can choose which training examples are\ngenerated. Why is this beneﬁcial? What is a potential problem?\n2. What is Grid world?\n3. Which ﬁve elements does an MDP have to model reinforcement learning prob-\nlems?\n4. In a tree diagram, is successor selection of behavior up or down?\n5. In a tree diagram, is learning values through backpropagation up or down?\n6. What is 𝜏?\n7. What is 𝜋(𝑠)?\n8. What is 𝑉(𝑠)?\n9. What is 𝑄(𝑠, 𝑎)?\n10. What is dynamic programming?\n11. What is recursion?\n12. Do you know a dynamic programming method to determine the value of a state?\n13. Is an action in an environment reversible for the agent?\n14. Mention two typical application areas of reinforcement learning.\n15. Is the action space of games typically discrete or continuous?\n16. Is the action space of robots typically discrete or continuous?\n17. Is the environment of games typically deterministic or stochastic?\n64\n2 Tabular Value-Based Reinforcement Learning\n18. Is the environment of robots typically deterministic or stochastic?\n19. What is the goal of reinforcement learning?\n20. Which of the ﬁve MDP elements is not used in episodic problems?\n21. Which model or function is meant when we say “model-free” or “model-based”?\n22. What type of action space and what type of environment are suited for value-\nbased methods?\n23. Why are value-based methods used for games and not for robotics?\n24. Name two basic Gym environments.\nExercises\nThere is an even better way to learn about deep reinforcement learning then reading\nabout it, and that is to perform experiments yourself, to see the learning processes\nunfold before your own eyes. The following exercises are meant as starting points\nfor your own discoveries in the world of deep reinforcement learning.\nConsider using Gym to implement these exercises. Section 2.2.4.1 explains how\nto install Gym.\n1. Q-learning Implement Q-learning for Taxi, including the procedure to derive\nthe best policy for the Q-table. Go to Sect. 2.2.4.5 and implement it. Print the\nQ-table, to see the values on the squares. You could print a live policy as the\nsearch progresses. Try diﬀerent values for 𝜖, the exploration rate. Does it learn\nfaster? Does it keep ﬁnding the optimal solution? Try diﬀerent values for 𝛼, the\nlearning rate. Is it faster?\n2. SARSA Implement SARSA, the code is in Listing 2.8. Compare your results to\nQ-learning, can you see how SARSA chooses diﬀerent paths? Try diﬀerent 𝜖and\n𝛼.\n3. Problem size How large can problems be before converging starts taking too\nlong?\n4. Cartpole Run Cartpole with the greedy policy computed by value iteration. Can\nyou make it work? Is value iteration a suitable algorithm for Cartpole? If not,\nwhy do you think it is not?\n2.3 Classic Gym Environments\n65\n1\n# SARSA\nfor\nOpenAI\nGym\nTaxi\nenvironment\n2\nimport\ngym\n3\nimport\nnumpy as np\n4\nimport\nrandom\n5\n# Environment\nSetup\n6\nenv = gym.make(\"Taxi -v2\")\n7\nenv.reset ()\n8\nenv.render ()\n9\n# Q[state ,action] table\nimplementation\n10\nQ = np.zeros ([env. observation_space .n, env. action_space .n])\n11\ngamma = 0.7\n# discount\nfactor\n12\nalpha = 0.2\n# learning\nrate\n13\nepsilon = 0.1 # epsilon\ngreedy\n14\nfor\nepisode\nin range (1000):\n15\ndone = False\n16\ntotal_reward = 0\n17\ncurrent_state = env.reset ()\n18\nif random.uniform (0, 1) < epsilon:\n19\ncurrent_action = env. action_space .sample () # Explore\nstate\nspace\n20\nelse:\n21\ncurrent_action = np.argmax(Q[ current_state ]) # Exploit\nlearned\nvalues\n22\nwhile\nnot\ndone:\n23\nnext_state , reward , done , info = env.step( current_action )\n# invoke\nGym\n24\nif random.uniform (0, 1) < epsilon:\n25\nnext_action = env. action_space .sample () # Explore\nstate\nspace\n26\nelse:\n27\nnext_action = np.argmax(Q[ next_state ]) # Exploit\nlearned\nvalues\n28\nsarsa_value = Q[next_state , next_action ]\n29\nold_value = Q[current_state , current_action ]\n30\n31\nnew_value = old_value + alpha * (reward + gamma *\nsarsa_value\n- old_value)\n32\n33\nQ[current_state , current_action ] = new_value\n34\ntotal_reward\n+= reward\n35\ncurrent_state = next_state\n36\ncurrent_action = next_action\n37\nif\nepisode % 100 == 0:\n38\nprint(\"Episode␣{}␣Total␣Reward:␣{}\".format(episode ,\ntotal_reward ))\nListing 2.8 SARSA Taxi example, after [395]\nChapter 3\nDeep Value-Based Reinforcement Learning\nThe previous chapter introduced the ﬁeld of classic reinforcement learning. We\nlearned about agents and environments, and about states, actions, values, and policy\nfunctions. We also saw our ﬁrst planning and learning algorithms: value iteration,\nSARSA and Q-learning. The methods in the previous chapter were exact, tabular,\nmethods, that work for problems of moderate size that ﬁt in memory.\nIn this chapter we move to high-dimensional problems with large state spaces\nthat no longer ﬁt in memory. We will go beyond tabular methods and use methods\nto approximate the value function and to generalize beyond trained behavior. We\nwill do so with deep learning.\nThe methods in this chapter are deep, model-free, value-based methods, related\nto Q-learning. We will start by having a closer look at the new, larger, environments\nthat our agents must now be able to solve (or rather, approximate). Next, we will\nlook at deep reinforcement learning algorithms. In reinforcement learning the\ncurrent behavior policy determines which action is selected next, a process that\ncan be self-reinforcing. There is no ground truth, as in supervised learning. The\ntargets of loss functions are no longer static, or even stable. In deep reinforcement\nlearning convergence to 𝑉and 𝑄values is based on a bootstrapping process, and\nthe ﬁrst challenge is to ﬁnd training methods that converge to stable function values.\nFurthermore, since with neural networks the states are approximated based on their\nfeatures, convergence proofs can no longer count on identifying states individually.\nFor many years it was assumed that deep reinforcement learning is inherently\nunstable due to a so-called deadly triad of bootstrapping, function approximation\nand oﬀ-policy learning.\nHowever, surprisngly, solutions have been found for many of the challenges.\nBy combining a number of approaches (such as the replay buﬀer and increased\nexploration) the Deep Q-Networks algorithm (DQN) was able to achieve stable\nlearning in a high-dimensional environment. The success of DQN spawned a large\nresearch eﬀort to improve training further. We will discuss some of these new\nmethods.\nThe chapter is concluded with exercises, a summary, and pointers to further\nreading.\n67\n68\n3 Deep Value-Based Reinforcement Learning\nDeep Learning: Deep reinforcement learning builds on deep supervised\nlearning, and this chapter and the rest of the book assume a basic under-\nstanding of deep learning. When your knowledge of parameterized neural\nnetworks and function approximation is rusty, this is the time to go to\nAppendix B and take an in-depth refresher. The appendix also reviews\nessential concepts such as training, testing, accuracy, overﬁtting and the\nbias-variance trade-oﬀ. When in doubt, try to answer the questions on\npage 332.\nCore Concepts\n•\nStable convergence\n•\nReplay buﬀer\nCore Problem\n•\nAchieve stable deep reinforcement learning in large problems\nCore Algorithm\n•\nDeep Q-network (Listing 3.6)\nEnd-to-end Learning\nBefore the advent of deep learning, traditional reinforcement learning had been\nused mostly on smaller problems such as puzzles, or the supermarket example.\nTheir state space ﬁts in the memories of our computers. Reward shaping, in the\nform of domain-speciﬁc heuristics, can be used to shoehorn the problem into a\ncomputer, for example, in chess and checkers [124, 354, 662]. Impressive results are\nachieved, but at the cost of extensive problem-speciﬁc reward shaping and heuristic\nengineering [600]. Deep learning changed this situation, and reinforcement learning\nis now used on high-dimensional problems that are too large to ﬁt into memory.\nIn the ﬁeld of supervised learning, a yearly competition had created years of\nsteady progress in which the accuracy of image classiﬁcation had steadily improved.\nProgress was driven by the availability of ImageNet, a large database of labeled\n3 Deep Value-Based Reinforcement Learning\n69\nFig. 3.1 Example Game from the Arcade Learning Environment [71]\nFig. 3.2 Atari Experiments on the Cover of Nature\nimages [237, 191], by increases in computation power through GPUs, and by steady\nimprovement of machine learning algorithms, especially in deep convolutional\nneural networks. In 2012, a paper by Krizhevsky, Sutskever and Hinton presented a\nmethod that out-performed other algorithms by a large margin, and approached the\nperformance of human image recognition [431]. The paper introduced the AlexNet\narchitecture (after the ﬁrst name of the ﬁrst author) and 2012 is often regarded as\nthe year of the breakthrough of deep learning. (See Appendix B.3.1 for details.) This\nbreakthrough raised the question whether something similar in deep reinforcement\nlearning could be achieved.\n70\n3 Deep Value-Based Reinforcement Learning\nWe did not have to wait long, only a year later, in 2013, at the deep learning\nworkshop of one of the main AI conferences, a paper was presented on an algorithm\nthat could play 1980s Atari video games just by training on the pixel input of the\nvideo screen (Fig. 3.1). The algorithm used a combination of deep learning and\nQ-learning, and was named Deep Q-Network, or DQN [522, 523]. An illuminating\nvideo of how it learned to play the game Breakout is here.1 This was a breakthrough\nfor reinforcement learning. Many researchers at the workshop could relate to this\nachievement, perhaps because they had spent hours playing Space Invaders, Pac-\nMan and Pong themselves when they were younger. Two years after the presentation\nat the deep learning workshop a longer article appeared in the journal Nature in\nwhich a reﬁned and expanded version of DQN was presented (see Fig. 3.2 for the\njournal cover).\nWhy was this such a momentous achievement? Besides the fact that the problem\nthat was solved was easily understood, true eye-hand coordination of this com-\nplexity had not been achieved by a computer before; furthermore, the end-to-end\nlearning from pixel to joystick implied artiﬁcial behavior that was close to how\nhumans play games. DQN essentially launched the ﬁeld of deep reinforcement learn-\ning. For the ﬁrst time the power of deep learning had been successfully combined\nwith behavior learning, for an imaginative problem.\nA major technical challenge that was overcome by DQN was the instability of\nthe deep reinforcement learning process. In fact, there were convincing theoretical\nanalyses at the time that this instability was fundamental, and it was generally\nassumed that it would be next to impossible to overcome [48, 283, 787, 743], since\nthe target of the loss-function depended on the convergence of the reinforcement\nlearning process itself. By the end of this chapter we will have covered the problems\nof convergence and stability in reinforcement learning. We will have seen how\nDQN addresses these problems, and we will also have discussed some of the many\nfurther solutions that were devised after DQN.\nBut let us ﬁrst have a look at the kind of new, high-dimensional, environments\nthat were the cause of these developments.\n3.1 Large, High-Dimensional, Problems\nIn the previous chapter, Grid worlds and mazes were introduced as basic sequential\ndecision making problems in which exact, tabular, reinforcement learning methods\nwork well. These are problems of moderate complexity. The complexity of a problem\nis related to the number of unique states that a problem has, or how large the state\nspace is. Tabular methods work for small problems, where the entire state space\nﬁts in memory. This is for example the case with linear regression, which has only\none variable 𝑥and two parameters 𝑎and 𝑏, or the Taxi problem, which has a state\nspace of size 500. In this chapter we will be more ambitious and introduce various\n1 https://www.youtube.com/watch?v=TmPfTpjtdgg\n3.1 Large, High-Dimensional, Problems\n71\nFig. 3.3 Atari 2600 console\ngames, most notably Atari arcade games. The state space of a single frame of Atari\nvideo input is 210 × 160 pixels of 256 RGB color values = 25633600.\nThere is a qualitative diﬀerence between small (500) and large (25633600) prob-\nlems.2 For small problems the policy can be learned by loading all states of a problem\nin memory. States are identiﬁed individually, and each has its own best action, that\nwe can try to ﬁnd. Large problems, in contrast, do not ﬁt in memory, the policy\ncannot be memorized, and states are grouped together based on their features (see\nSect. B.1.3, where we discuss feature learning). A parameterized network maps\nstates to actions and values; states are no longer individually identiﬁable in a lookup\ntable.\nWhen deep learning methods were introduced in reinforcement learning, larger\nproblems than before could be solved. Let us have a look at those problems.\n3.1.1 Atari Arcade Games\nLearning actions directly from high-dimensional sound and vision inputs is one of\nthe long-standing challenges of artiﬁcial intelligence. To stimulate this research, in\n2012 a test-bed was created designed to provide challenging reinforcement learning\ntasks. It was called the Arcade learning environment, or ALE [71], and it was based\non a simulator for 1980s Atari 2600 video games. Figure 3.3 shows a picture of a\ndistinctly retro Atari 2600 gaming console.\nAmong other things ALE contains an emulator of the Atari 2600 console. ALE\npresents agents with a high-dimensional3 visual input (210 × 160 RGB video at\n60 Hz, or 60 images per second) of tasks that were designed to be interesting and\nchallenging for human players (Fig. 3.1 showed an example of such a game and\n2 See Sect. B.1.2, where we discuss the curse of dimensionality.\n3 That is, high dimensional for machine learning. 210 × 160 pixels is not exactly high-deﬁnition\nvideo quality.\n72\n3 Deep Value-Based Reinforcement Learning\nFig. 3.4 Screenshots of 4 Atari Games (Breakout, Pong, Montezuma’s Revenge, and Private Eye)\nFig. 3.4 shows a few more). The game cartridge ROM holds 2-4 kB of game code,\nwhile the console random-access memory is small, just 128 bytes (really, just 128\nbytes, although the video memory is larger, of course). The actions can be selected\nvia a joystick (9 directions), which has a ﬁre button (ﬁre on/oﬀ), giving 18 actions\nin total.\nThe Atari games provide challenging eye-hand coordination and reasoning tasks,\nthat are both familiar and challenging to humans, providing a good test-bed for\nlearning sequential decision making.\nAtari games, with high-resolution video input at high frame rates, are an entirely\ndiﬀerent kind of challenge than Grid worlds or board games. Atari is a step closer to\na human environment in which visual inputs should quickly be followed by correct\nactions. Indeed, the Atari benchmark called for very diﬀerent agent algorithms,\nprompting the move from tabular algorithms to algorithms based on function\napproximation and deep learning. ALE has become a standard benchmark in deep\nreinforcement learning research.\n3.1.2 Real-Time Strategy and Video Games\nReal-time strategy games provide an even greater challenge than simulated 1980s\nAtari consoles. Games such as StarCraft (Fig. 1.6) [573], and Capture the Flag [373]\nhave very large state spaces. These are games with large maps, many players, many\npieces, and many types of actions. The state space of StarCraft is estimated at\n3.2 Deep Value-Based Agents\n73\n101685 [573], more than 1500 orders of magnitude larger than Go (10170) [540, 786]\nand more than 1635 orders of magnitude large than chess (1047) [355]. Most real\ntime strategy games are multi-player, non-zero-sum, imperfect information games\nthat also feature high-dimensional pixel input, reasoning, and team collaboration.\nThe action space is stochastic and is a mix of discrete and continuous actions.\nDespite the challenging nature, impressive achievements have been reported\nrecently in three games where human performance was matched or even ex-\nceeded [813, 80, 373], see also Chap. 7.\nLet us have a look at the methods that can solve these very diﬀerent types of\nproblems.\n3.2 Deep Value-Based Agents\nWe will now turn to agent algorithms for solving large sequential decision problems.\nThe main challenge of this section is to create an agent algorithm that can learn a\ngood policy by interacting with the world—with a large problem, not a toy problem.\nFrom now on, our agents will be deep learning agents.\nThe questions that we are faced with, are the following. How can we use deep\nlearning for high-dimensional and large sequential decision making environments?\nHow can tabular value and policy functions 𝑉, 𝑄, and 𝜋be transformed into 𝜃\nparameterized functions 𝑉𝜃, 𝑄𝜃, and 𝜋𝜃?\n3.2.1 Generalization of Large Problems with Deep Learning\nRecall from Appendix B that deep supervised learning uses a static dataset to\napproximate a function, and that the labels are static targets in an optimization\nprocess where the loss-function is minimized.\nDeep reinforcement learning is based on the observation that bootstrapping is\nalso a kind of minimization process in which an error (or diﬀerence) is minimized. In\nreinforcement learning this bootstrapping process converges on the true state value\nand state-action value functions. However, the Q-learning bootstrapping process\nlacks static ground truths; our data items are generated dynamically, and our loss-\nfunction targets move. The movement of the loss-function targets is inﬂuenced by\nthe same policy function that the convergence process is trying to learn.\nIt has taken quite some eﬀort to ﬁnd deep learning algorithms that converge to\nstable functions on these moving targets. Let us try to understand in more detail\nhow the supervised methods have to be adapted in order to work in reinforce-\nment learning. We do this by comparing three algorithmic structures: supervised\nminimization, tabular Q-learning, and deep Q-learning.\n74\n3 Deep Value-Based Reinforcement Learning\n1\ndef\ntrain_sl(data , net , alpha =0.001):\n# train\nclassifier\n2\nfor\nepoch in range(max_epochs ):\n# an epoch is one\npass\n3\nsum_sq = 0\n# reset to zero\nfor\neach\npass\n4\nfor (image , label) in data:\n5\noutput = net. forward_pass (image) # predict\n6\nsum_sq\n+= (output - label)**2 # compute\nerror\n7\ngrad = net.gradient(sum_sq)\n# derivative\nof error\n8\nnet. backward_pass (grad , alpha)\n# adjust\nweights\n9\nreturn\nnet\nListing 3.1 Network training pseudocode for supervised learning\n1\ndef\nqlearn(environment , alpha =0.001 ,\ngamma =0.9 ,\nepsilon =0.05):\n2\nQ[TERMINAL ,_] = 0 # policy\n3\nfor\nepisode\nin range( max_episodes ):\n4\ns = s0\n5\nwhile s not\nTERMINAL: # perform\nsteps of one\nfull\nepisode\n6\na = epsilongreedy (Q[s], epsilon)\n7\n(r, sp) = environment (s, a)\n8\nQ[s,a] = Q[s,a] + alpha *(r+gamma*max(Q[sp])-Q[s,a])\n9\ns = sp\n10\nreturn Q\nListing 3.2 Q-learning pseudocode [831, 743]\n3.2.1.1 Minimizing Supervised Target Loss\nListing 3.1 shows pseudocode for a typical supervised deep learning training algo-\nrithm, consisting of an input dataset, a forward pass that calculates the network\noutput, a loss computation, and a backward pass. See Appendix B or [280] for more\ndetails.\nWe see that the code consists of a double loop: the outer loop controls the training\nepochs. Epochs consist of forward approximation of the target value using the\nparameters, computation of the gradient, and backward adjusting of the parameters\nwith the gradient. In each epoch the inner loop serves all examples of the static\ndataset to the forward computation of the output value, the loss and the gradient\ncomputation, so that the parameters can be adjusted in the backward pass.\nThe dataset is static, and all that the inner loop does is deliver the samples\nto the backpropagation algorithm. Note that each sample is independent of the\nother, samples are chosen with equal probability. After an image of a white horse is\nsampled, the probability that the next image is of a black grouse or a blue moon is\nequally (un)likely.\n3.2 Deep Value-Based Agents\n75\n1\ndef\ntrain_qlearn (environment , Qnet , alpha =0.001 ,\ngamma =0.0 ,\nepsilon =0.05\n2\ns = s0\n# initialize\nstart\nstate\n3\nfor\nepoch in range(max_epochs ): # an epoch is one\npass\n4\nsum_sq = 0\n# reset to zero\nfor\neach\npass\n5\nwhile s not\nTERMINAL: # perform\nsteps of one\nfull\nepisode\n6\na = epsilongreedy (Qnet(s,a)) # net: Q[s,a]-values\n7\n(r, sp) = environment (a)\n8\noutput = Qnet. forward_pass (s, a)\n9\ntarget = r + gamma * max(Qnet(sp))\n10\nsum_sq\n+= (target - output)**2\n11\ns = sp\n12\ngrad = Qnet.gradient(sum_sq)\n13\nQnet. backward_pass (grad , alpha)\n14\nreturn\nQnet\n# Q-values\nListing 3.3 Network training pseudocode for reinforcement learning\n3.2.1.2 Bootstrapping Q-Values\nLet us now look at Q-learning. Reinforcement learning chooses the training exam-\nples diﬀerently. For convergence of algorithms such as Q-learning, the selection rule\nmust guarantee that eventually all states will be sampled by the environment [831].\nFor large problems, this is not the case; this condition for convergence to the value\nfunction does not hold.\nListing 3.2 shows the short version of the bootstrapping tabular Q-learning\npseudocode from the previous chapter. As in the previous deep learning algorithm,\nthe algorithm consists of a double loop. The outer loop controls the Q-value conver-\ngence episodes, and each episode consists of a single trace of (time) steps from the\nstart state to a terminal state. The Q-values are stored in a Python-array indexed\nby 𝑠and 𝑎, since Q is the state-action value. Convergence of the Q-values is as-\nsumed to have occurred when enough episodes have been sampled. The Q-formula\nshows how the Q-values are built up by bootstrapping on previous values, and how\nQ-learning is learning oﬀ-policy, taking the max value of an action.\nA diﬀerence with the supervised learning is that in Q-learning subsequent\nsamples are not independent. The next action is determined by the current policy,\nand will most likely be the best action of the state (𝜖-greedy). Furthermore, the next\nstate will be correlated to the previous state in the trajectory. After a state of the\nball in the upper left corner of the ﬁeld has been sampled, the next sample will\nwith very high probability also be of a state where the ball is close to the upper\nleft corner of the ﬁeld. Training can be stuck in local minima, hence the need for\nexploration.\n76\n3 Deep Value-Based Reinforcement Learning\n3.2.1.3 Deep Reinforcement Learning Target-Error\nThe two algorithms—deep learning and Q-learning—look similar in structure. Both\nconsist of a double loop in which a target is optimized, and we can wonder if\nbootstrapping can be combined with loss-function minimization. This is indeed\nthe case, as Mnih et al. [522] showed in 2013. Our third listing, Listing 3.3, shows\na naive deep learning version of Q-learning [522, 524], based on the double loop\nthat now bootstraps Q-values by minimizing a loss function through adjusting the\n𝜃parameters.\nIndeed, a Q-network can be trained with a gradient by minimizing a sequence\nof loss functions. The loss function for this bootstrap process is quite literally\nbased on the Q-learning update formula. The loss function is the squared diﬀerence\nbetween the new Q-value 𝑄𝜃𝑡(𝑠, 𝑎) from the forward pass and the old update target\n𝑟+ 𝛾max𝑎′ 𝑄𝜃𝑡−1(𝑠′, 𝑎′).4\nAn important observation is that the update targets depend on the previous\nnetwork weights 𝜃𝑡−1 (the optimization targets move during optimization); this is\nin contrast with the targets used in a supervised learning process, that are ﬁxed\nbefore learning begins [522]. In other words, the loss function of deep Q-learning\nminimizes a moving target, a target that depends on the network being optimized.\n3.2.2 Three Challenges\nLet us have a closer look at the challenges that deep reinforcement learning faces.\nThere are three problems with our naive deep Q-learner. First, convergence to the\noptimal Q-function depends on full coverage of the state space, yet the state space is\ntoo large to sample fully. Second, there is a strong correlation between subsequent\ntraining samples, with a real risk of local optima. Third, the loss function of gradient\ndescent literally has a moving target, and bootstrapping may diverge. Let us have a\nlook at these three problems in more detail.\n3.2.2.1 Coverage\nProofs that algorithms such as Q-learning converge to the optimal policy depend on\nthe assumption of full state space coverage; all state-action pairs must be sampled.\nOtherwise, the algorithms will not converge to an optimal action value for each\nstate. Clearly, in large state spaces where not all states are sampled, this situation\ndoes not hold, and there is no guarantee of convergence.\n4 Deep Q-learning is a ﬁxed-point iteration [507]. The gradient of this loss function is ∇𝜃𝑖L𝑖(𝜃𝑖) =\nE𝑠,𝑎∼𝜌(·);𝑠′∼E\n\u0002\u0000𝑟+ 𝛾max𝑎′ 𝑄𝜃𝑖−1 (𝑠′, 𝑎′) −𝑄𝜃𝑖(𝑠, 𝑎)\u0001 ∇𝜃𝑖𝑄𝜃𝑖(𝑠, 𝑎)\n\u0003 where 𝜌is the behavior\ndistribution and E the Atari emulator. Further details are in [522].\n3.2 Deep Value-Based Agents\n77\n3.2.2.2 Correlation\nIn reinforcement learning a sequence of states is generated in an agent/environment\nloop. The states diﬀer only by a single action, one move or one stone, all other\nfeatures of the states remain unchanged, and thus, the values of subsequent samples\nare correlated, which may result in a biased training. The training may cover only\na part of the state space, especially when greedy action selection increases the\ntendency to select a small set of actions and states. The bias can result in the\nso-called specialization trap (when there is too much exploitation, and too little\nexploration).\nCorrelation between subsequent states contributes to the low coverage that we\ndiscussed before, reducing convergence towards the optimal Q-function, increasing\nthe probability of local optima and feedback loops. This happens, for example, when\na chess program has been trained on a particular opening, and the opponent plays\na diﬀerent one. When test examples are diﬀerent from training examples, then\ngeneralization will be bad. This problem is related to out-of-distribution training,\nsee for example [485].\n3.2.2.3 Convergence\nWhen we naively apply our deep supervised methods to reinforcement learning,\nwe encounter the problem that in a bootstrap process, the optimization target is\npart of the bootstrap process itself. Deep supervised learning uses a static dataset\nto approximate a function, and loss-function targets are therefore stable. However,\ndeep reinforcement learning uses as bootstrap target the Q-update from the previous\ntime step, which changes during the optimization.\nThe loss is the squared diﬀerence between the Q-value 𝑄𝜃𝑡(𝑠, 𝑎) and the old\nupdate target 𝑟+ 𝛾max𝑎′ 𝑄𝜃𝑡−1(𝑠′, 𝑎′). Since both depend on parameters 𝜃that are\noptimized, the risk of overshooting the target is real, and the optimization process\ncan easily become unstable. It has taken quite some eﬀort to ﬁnd algorithms that\ncan tolerate these moving targets.\nDeadly Triad\nMultiple works [48, 283, 787] showed that a combination of oﬀ-policy reinforcement\nlearning with nonlinear function approximation (such as deep neural networks)\ncould cause Q-values to diverge. Sutton and Barto [743] further analyze three\nelements for divergent training: function approximation, bootstrapping, and oﬀ-\npolicy learning. Together, they are called deadly triad.\nFunction approximation may attribute values to states inaccurately. In contrast\nto exact tabular methods, that are designed to identify individual states exactly,\nneural networks are designed to individual features of states. These features can be\nshared by diﬀerent states, and values attributed to those features are shared also by\n78\n3 Deep Value-Based Reinforcement Learning\nother states. Function approximation may thus cause mis-identiﬁcation of states,\nand reward values and Q-values that are not assigned correctly. If the accuracy\nof the approximation of the true function values is good enough, then states may\nbe identiﬁed well enough to reduce or prevent divergent training processes and\nloops [523].\nBootstrapping of values builds up new values on the basis of older values. This\noccurs in Q-learning and temporal-diﬀerence learning where the current value\ndepends on the previous value. Bootstrapping increases the eﬃciency of the training\nbecause values do not have to be calculated from the start. However, errors or\nbiases in initial values may persist, and spill over to other states as values are\npropagated incorrectly due to function approximation. Bootstrapping and function\napproximation can thus increase divergence.\nOﬀ-policy learning uses a behavior policy that is diﬀerent from the target policy\nthat we are optimizing for (Sect. 2.2.4.4). When the behavior policy is improved,\nthe oﬀ-policy values may not improve. Oﬀ-policy learning converges generally less\nwell than on-policy learning as it converges independently from the behavior policy.\nWith function approximation convergence may be even slower, due to values being\nassigned to incorrect states.\n3.2.3 Stable Deep Value-Based Learning\nThese considerations discouraged further research in deep reinforcement learn-\ning for many years. Instead, research focused for some time on linear function\napproximators, which have better convergence guarantees. Nevertheless, work\non convergent deep reinforcement learning continued [654, 324, 88, 495], and al-\ngorithms such as neural ﬁtted Q-learning were developed, which showed some\npromise [631, 453, 482]. After the further results of DQN [522] showed convincingly\nthat convergence and stable learning could be achieved in a non-trivial problem,\neven more experimental studies were performed to ﬁnd out under which circum-\nstances convergence can be achieved and the deadly triad can be overcome. Further\nconvergence and diversity-enhancing techniques were developed, some of which\nwe will cover in Sect. 3.2.4.\nAlthough the theory provides reasons why function approximation may preclude\nstable reinforcement learning, there were, in fact, indications that stable training\nis possible. Starting at the end of the 1980s, Tesauro had written a program that\nplayed very strong Backgammon based on a neural network. The program was\ncalled Neurogammon, and used supervised learning from grand-master games [762].\nIn order to improve the strength of the program, he switched to temporal diﬀerence\nreinforcement learning from self-play games [764]. TD-Gammon [763] learned by\nplaying against itself, achieving stable learning in a shallow network. TD-Gammon’s\ntraining used a temporal diﬀerence algorithm similar to Q-learning, approximating\nthe value function with a network with one hidden layer, using raw board input\n3.2 Deep Value-Based Agents\n79\nenhanced with hand-crafted heuristic features [763]. Perhaps some form of stable\nreinforcement learning was possible, at least in a shallow network?\nTD-Gammon’s success prompted attempts with TD learning in checkers [139]\nand Go [738, 154]. Unfortunately the success could not be replicated in these games,\nand it was believed for some time that Backgammon was a special case, well suited\nfor reinforcement learning and self-play [604, 678].\nHowever, as there came further reports of successful applications of deep neural\nnetworks in a reinforcement learning setting [324, 654], more work followed. The\nresults in Atari [523] and later in Go [706] as well as further work [799] have now\nprovided clear evidence that both stable training and generalizing deep reinforce-\nment learning are indeed possible, and have improved our understanding of the\ncircumstances that inﬂuence stability and convergence.\nLet us have a closer look at the methods that are used to achieve stable deep\nreinforcement learning.\n3.2.3.1 Decorrelating States\nAs mentioned in the introduction of this chapter, in 2013 Mnih et al. [522, 523]\npublished their work on end-to-end reinforcement learning in Atari games.\nThe original focus of DQN is on breaking correlations between subsequent states,\nand also on slowing down changes to parameters in the training process to improve\nstability. The DQN algorithm has two methods to achieve this: (1) experience replay\nand (2) infrequent weight updates. We will ﬁrst look at experience replay.\nExperience Replay\nIn reinforcement learning training samples are created in a sequence of interactions\nwith the environment, and subsequent training states are strongly correlated to\npreceding states. There is a tendency to train the network on too many samples\nof a certain kind or in a certain area, and other parts of the state space remain\nunder-explored. Furthermore, through function approximation and bootstrapping,\nsome behavior may be forgotten. When an agent reaches a new level in a game that\nis diﬀerent from previous levels, the agent may forget how to play the other level.\nWe can reduce correlation—and the local minima they cause—by adding a small\namount of supervised learning. To break correlations and to create a more diverse\nset of training examples, DQN uses experience replay. Experience replay introduces\na replay buﬀer [481], a cache of previously explored states, from which it samples\ntraining states at random.5 Experience replay stores the last 𝑁examples in the\nreplay memory, and samples uniformly when performing updates. A typical number\nfor 𝑁is 106 [875]. By using a buﬀer, a dynamic dataset from which recent training\nexamples are sampled, we train states from a more diverse set, instead of only from\n5 Originally experience replay is, as so much in artiﬁcial intelligence, a biologically inspired\nmechanism [506, 572, 482].\n80\n3 Deep Value-Based Reinforcement Learning\nthe most recent one. The goal of experience replay is to increase the independence\nof subsequent training examples. The next state to be trained on is no longer a direct\nsuccessor of the current state, but one somewhere in a long history of previous\nstates. In this way the replay buﬀer spreads out the learning over more previously\nseen states, breaking temporal correlations between samples. DQN’s replay buﬀer\n(1) improves coverage, and (2) reduces correlation.\nDQN treats all examples equal, old and recent alike. A form of importance\nsampling might diﬀerentiate between important transitions, as we will see in the\nnext section.\nNote that, curiously, training by experience replay is a form of oﬀ-policy learning,\nsince the target parameters are diﬀerent from those used to generate the sample.\nOﬀ-policy learning is one of the three elements of the deadly triad, and we ﬁnd that\nstable learning can actually be improved by a special form of one of its problems.\nExperience replay works well in Atari [523]. However, further analysis of replay\nbuﬀers has pointed to possible problems. Zhang et al. [875] study the deadly triad\nwith experience replay, and ﬁnd that larger networks resulted in more instabilities,\nbut also that longer multi-step returns yielded fewer unrealistically high reward\nvalues. In Sect. 3.2.4 we will see many further enhancements to DQN-like algorithms.\n3.2.3.2 Infrequent Updates of Target Weights\nThe second improvement in DQN is infrequent weight updates, introduced in the\n2015 paper on DQN [523]. The aim of this improvement is to reduce divergence\nthat is caused by frequent updates of weights of the target 𝑄-value. Again, the aim\nis to improve the stability of the network optimization by improving the stability\nof the 𝑄-target in the loss function.\nEvery 𝑛updates, the network 𝑄is cloned to obtain target network ˆ𝑄, which is\nused for generating the targets for the following 𝑛updates to 𝑄. In the original\nDQN implementation a single set of network weights 𝜃are used, and the network\nis trained on a moving loss-target. Now, with infrequent updates the weights of the\ntarget network change much slower than those of the behavior policy, improving\nthe stability of the Q-targets.\nThe second network improves the stability of Q-learning, where normally an\nupdate to 𝑄𝜃(𝑠𝑡, 𝑎𝑡) also changes the target at each time step, quite possibly leading\nto oscillations and divergence of the policy. Generating the targets using an older\nset of parameters adds a delay between the time an update to 𝑄𝜃is made and the\ntime the update changes the targets, making oscillations less likely.\n3.2.3.3 Hands On: DQN and Breakout Gym Example\nTo get some hands-on experience with DQN, we will now have a look at how DQN\ncan be used to play the Atari game Breakout.\n3.2 Deep Value-Based Agents\n81\nThe ﬁeld of deep reinforcement learning is an open ﬁeld where most codes of\nalgorithms are freely shared on GitHub and where test environments are available.\nThe most widely used environment is Gym, in which benchmarks such as ALE\nand MuJoCo can be found, see also Appendix C. The open availability of the\nsoftware allows for easy replication, and, importantly, for further improvement of\nthe methods. Let us have a closer look at the code of DQN, to experience how it\nworks.\nThe DQN papers come with source code. The original DQN code from [523]\nis available at Atari DQN.6 This code is the original code, in the programming\nlanguage Lua, which may be interesting to study, if you are familiar with this\nlanguage. A modern reference implementation of DQN, with further improvements,\nis in the (stable) baselines.7 The RL Baselines Zoo even provides a collection of\npretrained agents, at Zoo [603, 270].8 The Network Zoo is especially useful if your\ndesired application happens to be in the Zoo, to prevent long training times.\nInstall Stable Baselines\nThe environment is only half of the reinforcement learning experiment, we also\nneed an agent algorithm to learn the policy. OpenAI also provides implementations\nof agent algorithms, called the Baselines, at the Gym GitHub repository Baselines.9\nMost algorithms that are covered in this book are present. You can download them,\nstudy the code, and experiment to gain an insight into their behavior.\nIn addition to OpenAI’s Baselines, there is Stable Baselines, a fork of the OpenAI\nalgorithms; it has more documentation and other features. It can be found at Stable\nBaselines,10 and the documentation is at docs.11\nThe stable release from the Stable Baselines is installed by typing\npip install stable-baselines\nor\npip install stable-baselines[mpi]\nif support for OpenMPI is desired (a parallel message passing implementation for\ncluster computers). A very quick check to see if everything works is to run the PPO\ntrainer from Listing 3.4. PPO is a policy-based algorithm that will be discussed in\nthe next chapter in Sect. 4.2.5. The Cartpole should appear again, but should now\nlearn to stabilize for a brief moment.\n6 https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner\n7 https://stable-baselines.readthedocs.io/en/master/index.html\n8 https://github.com/araffin/rl-baselines-zoo\n9 https://github.com/openai/baselines\n10 https://github.com/hill-a/stable-baselines\n11 https://stable-baselines.readthedocs.io/en/master/\n82\n3 Deep Value-Based Reinforcement Learning\n1\nimport\ngym\n2\n3\nfrom\nstable_baselines .common.policies\nimport\nMlpPolicy\n4\nfrom\nstable_baselines .common.vec_env\nimport\nDummyVecEnv\n5\nfrom\nstable_baselines\nimport\nPPO2\n6\n7\nenv = gym.make(’CartPole -v1’)\n8\n9\nmodel = PPO2(MlpPolicy , env , verbose =1)\n10\nmodel.learn( total_timesteps =10000)\n11\n12\nobs = env.reset ()\n13\nfor i in range (1000):\n14\naction , _states = model.predict(obs)\n15\nobs , rewards , dones , info = env.step(action)\n16\nenv.render ()\nListing 3.4 Running Stable Baseline PPO on the Gym Cartpole Environment\n1\nfrom\nstable_baselines .common. atari_wrappers\nimport\nmake_atari\n2\nfrom\nstable_baselines .deepq.policies\nimport\nMlpPolicy , CnnPolicy\n3\nfrom\nstable_baselines\nimport\nDQN\n4\n5\nenv = make_atari(’BreakoutNoFrameskip -v4’)\n6\n7\nmodel = DQN(CnnPolicy , env , verbose =1)\n8\nmodel.learn( total_timesteps =25000)\n9\n10\nobs = env.reset ()\n11\nwhile\nTrue:\n12\naction , _states = model.predict(obs)\n13\nobs , rewards , dones , info = env.step(action)\n14\nenv.render ()\nListing 3.5 Deep Q-Network Atari Breakout example with Stable Baselines\nThe DQN Code\nAfter having studied tabular Q-learning on Taxi in Sect. 2.2.4.5, let us now see\nhow the network-based DQN works in practice. Listing 3.5 illustrates how easy\nit is to use the Stable Baselines implementation of DQN on the Atari Breakout\nenvironment. (See Sect. 2.2.4.1 for installation instructions of Gym.)\nAfter you have run the DQN code and seen that it works, it is worthwhile to study\nhow the code is implemented. Before you dive into the Python implementation of\nStable Baselines, let us look at the pseudocode to refresh how the elements of DQN\nwork together. See Listing 3.6. In this pseudocode we follow the 2015 version of\nDQN [523]. (The 2013 version of DQN did not use the target network [522].)\n3.2 Deep Value-Based Agents\n83\n1\ndef dqn:\n2\ninitialize\nreplay_buffer\nempty\n3\ninitialize Q network\nwith\nrandom\nweights\n4\ninitialize\nQt target\nnetwork\nwith\nrandom\nweights\n5\nset s = s0\n6\nwhile\nnot\nconvergence :\n7\n# DQN in Atari\nuses\npreprocessing ; not\nshown\n8\nepsilon -greedy\nselect\naction a in argmax(Q(s,a)) # action\nselection\ndepends\non Q (moving\ntarget)\n9\nsx ,reward = execute\naction in\nenvironment\n10\nappend (s,a,r,sx) to buffer\n11\nsample\nminibatch\nfrom\nbuffer # break\ntemporal\ncorrelation\n12\ntake\ntarget\nbatch R (when\nterminal) or Qt\n13\ndo\ngradient\ndescent\nstep on Q # loss\nfunction\nuses\ntarget\nQt\nnetwork\nListing 3.6 Pseudocode for DQN, after [523]\nDQN is based on Q-learning, with as extra a replay buﬀer and a target network\nto improve stability and convergence. First, at the start of the code, the replay buﬀer\nis initialized to empty, and the weights of the Q network and the separate Q target\nnetwork are initialized. The state 𝑠is set to the start state.\nNext is the optimization loop, that runs until convergence. At the start of each\niteration an action is selected at the state 𝑠, following an 𝜖-greedy approach. The\naction is executed in the environment, and the new state and the reward are stored\nin a tuple in the replay buﬀer. Then, we train the Q-network. A minibatch is sampled\nrandomly from the replay buﬀer, and one gradient descent step is performed. For\nthis step the loss function is calculated with the separate Q-target network ˆ𝑄𝜃, that\nis updated less frequently than the primary Q-network 𝑄𝜃. In this way the loss\nfunction\nL𝑡(𝜃𝑡) = E𝑠,𝑎∼𝜌(·)\nh\u0000E𝑠′∼E(𝑟+ 𝛾max\n𝑎′\nˆ𝑄𝜃𝑡−1(𝑠′, 𝑎′)|𝑠, 𝑎) −𝑄𝜃𝑡(𝑠, 𝑎)\u00012i\nis more stable, causing better convergence; 𝜌(𝑠, 𝑎) is the behavior distribution over\n𝑠and 𝑎, and E is the Atari emulator [522]. Sampling the minibatch reduces the\ncorrelation that is inherent in reinforcement learning between subsequent states.\nConclusion\nIn summary, DQN was able to successfully learn end-to-end behavior policies for\nmany diﬀerent games (although similar and from the same benchmark set). Minimal\nprior knowledge was used to guide the system, and the agent only got to see the\npixels and the game score. The same network architecture and procedure was used\non each game; however, a network trained for one game could not be used to play\nanother game.\n84\n3 Deep Value-Based Reinforcement Learning\nName\nPrinciple\nApplicability\nEﬀectiveness\nDQN [522]\nreplay buﬀer\nAtari\nstable Q learning\nDouble DQN [800]\nde-overestimate values\nDQN\nconvergence\nPrioritized experience [666]\ndecorrelation\nreplay buﬀer\nconvergence\nDistributional [70]\nprobability distr\nstable gradients\ngeneralization\nRandom noise [254]\nparametric noise\nstable gradients more exploration\nTable 3.1 Deep Value-Based Approaches\nThe DQN achievement was an important milestone in the history of deep rein-\nforcement learning. The main problems that were overcome by Mnih et al. [522]\nwere training divergence and learning instability.\nThe nature of most Atari 2600 games is that they require eye-hand reﬂexes. The\ngames have some strategic elements, credit assignment is mostly over a short term,\nand can be learned with a surprisingly simple neural network. Most Atari games\nare more about immediate reﬂexes than about longer term reasoning. In this sense,\nthe problem of playing Atari well is not unlike an image categorization problem:\nboth problems are to ﬁnd the right response that matches an input consisting of a\nset of pixels. Mapping pixels to categories is not that diﬀerent from mapping pixels\nto joystick actions (see also the observations in [400]).\nThe Atari results have stimulated much subsequent research. Many blogs have\nbeen written on reproducing the result, which is not a straightforward task, requir-\ning the ﬁne-tuning of many hyperparameters [58].\n3.2.4 Improving Exploration\nThe DQN results have spawned much activity among reinforcement learning re-\nsearchers to improve training stability and convergence further, and many reﬁne-\nments have been devised, some of which we will review in this section.\nMany of the topics that are covered by the enhancements are older ideas that\nwork well in deep reinforcement learning. DQN applies random sampling of its\nreplay buﬀer, and one of the ﬁrst enhancements was prioritized sampling [666].\nIt was found that DQN, being an oﬀ-policy algorithm, typically overestimates\naction values (due to the max operation, Sect. 2.2.4.4). Double DQN addresses\noverestimation [800], and dueling DDQN introduces the advantage function to\nstandardize action values [830]. Other approaches look at variance in addition to\nexpected value, the eﬀect of random noise on exploration was tested [254], and\ndistributional DQN showed that networks that use probability distributions work\nbetter than networks that only use single point expected values [70].\nIn 2017 Hessel et al. [335] performed a large experiment that combined seven\nimportant enhancements. They found that the enhancements worked well together.\nThe paper has become known as the Rainbow paper, since the major graph showing\nthe cumulative performance over 57 Atari games of the seven enhancements is\n3.2 Deep Value-Based Agents\n85\nFig. 3.5 Rainbow graph: performance over 57 Atari games [335]\nmulti-colored (Fig. 3.5). Table 3.1 summarizes the enhancements, and this section\nprovides an overview of the main ideas. The enhancements were tested on the same\nbenchmarks (ALE, Gym), and most algorithm implementations can be found on the\nOpenAI Gym GitHub site in the baselines.12\n3.2.4.1 Overestimation\nVan Hasselt et al. introduce double deep Q learning (DDQN) [800]. DDQN is based\non the observation that Q-learning may overestimate action values. On the Atari\n2600 games DQN suﬀers from substantial over-estimations. Remember that DQN\nuses Q-learning. Because of the max operation in Q-learning this results in an\noverestimation of the Q-value. To resolve this issue, DDQN uses the Q-Network to\nchoose the action but uses the separate target Q-Network to evaluate the action.\nLet us compare the training target for DQN\n𝑦= 𝑟𝑡+1 + 𝛾𝑄𝜃𝑡(𝑠𝑡+1, arg max\n𝑎\n𝑄𝜃𝑡(𝑠𝑡+1, 𝑎)\nwith the training target for DDQN (the diﬀerence is a single 𝜙)\n𝑦= 𝑟𝑡+1 + 𝛾𝑄𝜙𝑡(𝑠𝑡+1, arg max\n𝑎\n𝑄𝜃𝑡(𝑠𝑡+1, 𝑎).\n12 https://github.com/openai/baselines\n86\n3 Deep Value-Based Reinforcement Learning\nThe DQN target uses the same set of weights 𝜃𝑡twice, for selection and evaluation;\nthe DDQN target use a separate set of weights 𝜙𝑡for evaluation, preventing overes-\ntimation due to the max operator. Updates are assigned randomly to either set of\nweights.\nEarlier Van Hasselt et al. [314] introduced the double Q learning algorithm in\na tabular setting. The later paper shows that this idea also works with a large\ndeep network. They report that the DDQN algorithm not only reduces the over-\nestimations but also leads to better performance on several games. DDQN was\ntested on 49 Atari games and achieved about twice the average score of DQN with\nthe same hyperparameters, and four times the average DQN score with tuned\nhyperparameters [800].\nPrioritized Experience Replay\nDQN samples uniformly over the entire history in the replay buﬀer, where Q-\nlearning uses only the most recent (and important) state. It stands to reason to see\nif a solution in between these two extremes performs well.\nPrioritized experience replay, or PEX, is such an attempt. It was introduced by\nSchaul et al. [666]. In the Rainbow paper PEX is combined with DDQN, and, as we\ncan see, the blue line (with PEX) indeed outperforms the purple line.\nIn DQN experience replay lets agents reuse examples from the past, although\nexperience transitions are uniformly sampled, and actions are simply replayed\nat the same frequency that they were originally experienced, regardless of their\nsigniﬁcance. The PEX approach provides a framework for prioritizing experience.\nImportant actions are replayed more frequently, and therefore learning eﬃciency\nis improved. As measure of importance, standard proportional prioritized replay\nis used, with the absolute TD error to prioritize actions. Prioritized replay is used\nwidely in value-based deep reinforcement learning. The measure can be computed\nin the distributional setting using the mean action values. In the Rainbow paper all\ndistributional variants prioritize actions by the Kullback-Leibler loss [335].\nAdvantage Function\nThe original DQN uses a single neural network as function approximator; DDQN\n(double deep Q-network) uses a separate target Q-Network to evaluate an action.\nDueling DDQN [830], also known as DDDQN, improves on this architecture by\nusing two separate estimators: a value function and an advantage function\n𝐴(𝑠, 𝑎) = 𝑄(𝑠, 𝑎) −𝑉(𝑠).\nAdvantage functions are related to the actor-critic approach (see Chap. 4). An\nadvantage function computes the diﬀerence between the value of an action and the\nvalue of the state. The function standardizes values on a baseline for the actions\n3.3 Atari 2600 Environments\n87\nof a state [293]. Advantage functions provide better policy evaluation when many\nactions have similar values.\n3.2.4.2 Distributional Methods\nThe original DQN learns a single value, which is the estimated mean of the state\nvalue. This approach does not take uncertainty into account. To remedy this, distri-\nbutional Q-learning [70] learns a categorical probability distribution of discounted\nreturns instead, increasing exploration. Bellemare et al. design a new distributional\nalgorithm which applies Bellman’s equation to the learning of distributions, a\nmethod called distributional DQN. Moerland et al. [526, 527] propose uncertain\nvalue networks. Interestingly, a link between the distributional approach and bi-\nology has been reported. Dabney et al. [174] showed correspondence between\ndistributional reinforcement learning algorithms and the dopamine levels in mice,\nsuggesting that the brain represents possible future rewards as a probability distri-\nbution.\nNoisy DQN\nAnother distributional method is noisy DQN [254]. Noisy DQN uses stochastic net-\nwork layers that add parametric noise to the weights. The noise induces randomness\nin the agent’s policy, which increases exploration. The parameters that govern the\nnoise are learned by gradient descent together with the remaining network weights.\nIn their experiments the standard exploration heuristics for A3C (Sect. 4.2.4), DQN,\nand dueling agents (entropy reward and 𝜖-greedy) were replaced with NoisyNet.\nThe increased exploration yields substantially higher scores for Atari (dark red\nline).\n3.3 Atari 2600 Environments\nIn their original 2013 workshop paper Mnih et al. [522] achieved human-level play\nfor some of the games. Training was performed on 50 million frames in total on\nseven Atari games. The neural network performed better than an expert human\nplayer on Breakout, Enduro, and Pong. On Seaqest, Q*Bert, and Space Invaders\nperformance was far below that of a human. In these games a strategy must be\nfound that extends over longer time periods. In their follow-up journal article two\nyears later they were able to achieve human level play for 49 of the 57 games that\nare in ALE [523], and performed better than human-level play in 29 of the 49 games.\nSome of the games still proved diﬃcult, notably games that require longer-\nrange planning, where long stretches of the game do not give rewards, such as in\nMontezuma’s Revenge, where the agent has to walk long distances, and pick up\n88\n3 Deep Value-Based Reinforcement Learning\nFig. 3.6 DQN architecture [361]\na key to reach new rooms to enter new levels. In reinforcement learning terms,\ndelayed credit assignment over long periods is hard. Towards the end of the book we\nwill see Montezuma’s Revenge again, when we discuss hierarchical reinforcement\nlearning methods, in Chap 8. These methods are speciﬁcally developed to take large\nsteps in the state space. The Go-Explore algorithm was able to solve Montezuma’s\nRevenge [221, 222].\n3.3.1 Network Architecture\nEnd-to-end learning of challenging problems is computationally intensive. In ad-\ndition to the two algorithmic innovations, the success of DQN is also due to the\ncreation of a specialized eﬃcient training architecture [523].\nPlaying the Atari games is a computationally intensive task for a deep neural\nnetwork: the network trains a behavior policy directly from pixel frame input.\nTherefore, the training architecture contains reduction steps. To start with, the net-\nwork consists of only three hidden layers (one fully connected, two convolutional),\nwhich is simpler than what is used in most supervised learning tasks.\nThe pixel-images are high-resolution data. Since working with the full resolution\nof 210 × 160 pixels of 128 color-values at 60 frames per second would be computa-\ntionally too intensive, the images are reduced in resolution. The 210 × 160 with\na 128 color palette is reduced to gray scale and 110 × 84 pixels, which is further\ncropped to 84×84. The ﬁrst hidden layer convolves 16 8×8 ﬁlters with stride 4 and\nReLU neurons. The second hidden layer convolves 32 4 × 4 ﬁlters with stride 2 and\nReLU neurons. The third hidden layer is fully connected and consists of 256 ReLU\nneurons. The output layer is also fully connected with one output per action (18\njoystick actions). The outputs correspond to the Q-values of the individual action.\nFigure 3.6 shows the architecture of DQN. The network receives the change in\ngame score as a number from the emulator, and derivative updates are mapped to\n{−1, 0, +1} to indicate decrease, no change, or improvement of the score (the Huber\nloss [58]).\n3.3 Atari 2600 Environments\n89\nTo reduce computational demands further, frame skipping is employed. Only\none in every 3–4 frames was used, depending on the game. To take game history\ninto account, the net takes as input the last four resulting frames. This allows\nmovement to be seen by the net. As optimizer RMSprop is used [642]. A variant\nof 𝜖-greedy is used, that starts with an 𝜖of 1.0 (fully exploring) going down to 0.1\n(90% exploiting).\n3.3.2 Benchmarking Atari\nTo end the Atari story, we discuss two ﬁnal algorithms. Of the many value-based\nmodel-free deep reinforcement learning algorithms that have been developed, one\nmore algorithm that we discuss is R2D2 [397], because of its performance. R2D2\nis not part of the Rainbow experiments, but is a signiﬁcant further improvement\nof the algorithms. R2D2 stands for Recurrent Replay Distributed DQN. It is built\nupon prioritized distributed replay and 5-step double Q-learning. Furthermore, it\nuses a dueling network architecture and an LSTM layer after the convolutional\nstack. Details about the architecture can be found in [830, 295]. The LSTM uses\nthe recurrent state to exploit long-term temporal dependencies, which improve\nperformance. The authors also report that the LSTM allows for better representation\nlearning. R2D2 achieved good results on all 57 Atari games [397].\nA more recent benchmark achievement has been published as Agent57. Agent57\nis the ﬁrst program that achieves a score higher than the human baseline on all 57\nAtari 2600 games from ALE. It uses a controller that adapts the long and short-term\nbehavior of the agent, training for a range of policies, from very exploitative to very\nexplorative, depending on the game [46].\nConclusion\nProgress has come a long way since the replay buﬀer of DQN. Performance has\nbeen improved greatly in value-based model-free deep reinforcement learning and\nnow super-human performance in all 57 Atari games of ALE has been achieved.\nMany enhancements that improve coverage, correlation, and convergence have\nbeen developed. The presence of a clear benchmark was instrumental for progress\nso that researchers could clearly see which ideas worked and why. The earlier\nmazes and navigation games, OpenAI’s Gym [108], and especially the ALE [71],\nhave enabled this progress.\nIn the next chapter we will look at the other main branch of model-free reinforce-\nment learning: policy-based algorithms. We will see how they work, and that they\nare well suited for a diﬀerent kind of application, with continuous action spaces.\n90\n3 Deep Value-Based Reinforcement Learning\nSummary and Further Reading\nThis has been the ﬁrst chapter in which we have seen deep reinforcement learning\nalgorithms learn complex, high-dimensional, tasks. We end with a summary and\npointers to the literature.\nSummary\nThe methods that have been discussed in the previous chapter were exact, tabular\nmethods. Most interesting problems have large state spaces that do not ﬁt into\nmemory. Feature learning identiﬁes states by their common features. Function\nvalues are not calculated exactly, but are approximated, with deep learning.\nMuch of the recent success of reinforcement learning is due to deep learning\nmethods. For reinforcement learning a problem arises when states are approximated.\nSince in reinforcement learning the next state is determined by the previous state,\nalgorithms may get stuck in local minima or run in circles when values are shared\nwith diﬀerent states.\nAnother problem is training convergence. Supervised learning has a static dataset\nand training targets are also static. In reinforcement learning the loss function\ntargets depend on the parameters that are being optimized. This causes further\ninstability. DQN caused a breakthrough by showing that with a replay buﬀer and a\nseparate, more stable, target network, enough stability could be found for DQN to\nconverge and learn how to play Atari arcade games.\nMany further improvements to increase stability have been found. The Rainbow\npaper implements some of these improvements, and ﬁnds that they are complemen-\ntary, and together achieve very strong play.\nFurther Reading\nDeep learning revolutionized reinforcement learning. A comprehensive overview of\nthe ﬁeld is provided by Dong et al. [201]. For more on deep learning, see Goodfellow\net al. [280], a book with much detail on deep learning; a major journal article is [459].\nA brief survey is [27]. Also see Appendix B.\nIn 2013 the Arcade Learning Environment was presented [71, 494]. Experiment-\ning with reinforcement learning was made even more accessible with OpenAI’s\nGym [108], with clear and easy to use Python bindings.\nDeep learning versions of value-based tabular algorithms suﬀer from conver-\ngence and stability problems [787], yet the idea that stable deep reinforcement\nlearning might be practical took hold with [324, 654]. Zhang et al. [875] study\nthe deadly triad with experience replay. Deep gradient TD methods were proven\nto converge for evaluating a ﬁxed policy [88]. Riedmiller et al. relaxed the ﬁxed\n3.3 Atari 2600 Environments\n91\ncontrol policy in neural ﬁtted Q learning algorithm (NFQ) [631]. NFQ builds on\nwork on stable function approximation [282, 229] and experience replay [481], and\nmore recently on least-squares policy iteration [441]. In 2013 the ﬁrst DQN paper\nappeared, showing results on a small number of Atari games [522] with the replay\nbuﬀer to reduce temporal correlations. In 2015 the followup Nature paper reported\nresults in more games [523], with a separate target network to improve training\nconvergence. A well-known overview paper is the Rainbow paper [335, 387].\nThe use of benchmarks is of great importance for reproducible reinforcement\nlearning experiments [328, 371, 410, 365]. For TensorFlow and Keras, see [146, 270].\nExercises\nWe will end this chapter with some questions to review the concepts that we have\ncovered. Next are programming exercises to get some more exposure on how to\nuse the deep reinforcement learning algorithms in practice.\nQuestions\nBelow are some questions to check your understanding of this chapter. Each question\nis a closed question where a simple, single sentence answer is expected.\n1. What is Gym?\n2. What are the Stable Baselines?\n3. The loss function of DQN uses the Q-function as target. What is a consequence?\n4. Why is the exploration/exploitation trade-oﬀcentral in reinforcement learning?\n5. Name one simple exploration/exploitation method.\n6. What is bootstrapping?\n7. Describe the architecture of the neural network in DQN.\n8. Why is deep reinforcement learning more susceptible to unstable learning than\ndeep supervised learning?\n9. What is the deadly triad?\n10. How does function approximation reduce stability of Q-learning?\n11. What is the role of the replay buﬀer?\n12. How can correlation between states lead to local minima?\n13. Why should the coverage of the state space be suﬃcient?\n14. What happens when deep reinforcement learning algorithms do not converge?\n15. How large is the state space of chess estimated to be? 1047, 10170 or 101685?\n16. How large is the state space of Go estimated to be? 1047, 10170 or 101685?\n17. How large is the state space of StarCraft estimated to be? 1047, 10170 or 101685?\n18. What does the rainbow in the Rainbow paper stand for, and what is the main\nmessage?\n19. Mention three Rainbow improvements that are added to DQN.\n92\n3 Deep Value-Based Reinforcement Learning\nExercises\nLet us now start with some exercises. If you have not done so already, install\nGym, PyTorch13 or TensorFlow and Keras (see Sect. 2.2.4.1 and B.3.3 or go to the\nTensorFlow page).14 Be sure to check the right versions of Python, Gym, TensorFlow,\nand the Stable Baselines to make sure that they work well together. The exercises\nbelow are designed to be done with Keras.\n1. DQN Implement DQN from the Stable Baselines on Breakout from Gym. Turn\noﬀDueling and Priorities. Find out what the values are for 𝛼, the training rate,\nfor 𝜖, the exploration rate, what kind of neural network architecture is used,\nwhat the replay buﬀer size is, and how frequently the target network is updated.\n2. Hyperparameters Change all those hyperparameters, up, and down, and note the\neﬀect on training speed, and the training outcome: how good is the result? How\nsensitive is performance to hyperparameter optimization?\n3. Cloud Use diﬀerent computers, experiment with GPU versions to speed up\ntraining, consider Colab, AWS, or another cloud provider with fast GPU (or TPU)\nmachines.\n4. Gym Go to Gym and try diﬀerent problems. For what kind of problems does\nDQN work, what are characteristics of problems for which it works less well?\n5. Stable Baselines Go to the Stable baselines and implement diﬀerent agent algo-\nrithms. Try Dueling algorithms, Prioritized experience replay, but also other\nalgorithm, such as Actor critic or policy-based. (These algorithms will be ex-\nplained in the next chapter.) Note their performance.\n6. Tensorboard With Tensorboard you can follow the training process as it pro-\ngresses. Tensorboard works on log ﬁles. Try TensorBoard on a Keras exercise and\nfollow diﬀerent training indicators. Also try TensorBoard on the Stable Baselines\nand see which indicators you can follow.\n7. Checkpointing Long training runs in Keras need checkpointing, to save valuable\ncomputations in case of a hardware or software failure. Create a large training\njob, and setup checkpointing. Test everything by interrupting the training, and\ntry to re-load the pre-trained checkpoint to restart the training where it left oﬀ.\n13 https://pytorch.org\n14 https://www.tensorflow.org\nChapter 4\nPolicy-Based Reinforcement Learning\nSome of the most successful applications of deep reinforcement learning have a\ncontinuous action space, such as applications in robotics, self-driving cars, and\nreal-time strategy games.\nThe previous chapters introduced value-based reinforcement learning. Value-\nbased methods ﬁnd the policy in a two-step process. First they ﬁnd the best action-\nvalue of a state, for which then the accompanying actions are found (by means of\narg max). This works in environments with discrete actions, where the highest-\nvalued action is clearly separate from the next-best action. Examples of continuous\naction spaces are robot arms that can move over arbitrary angles, or poker bets that\ncan be any monetary value. In these action spaces value-based methods become\nunstable and arg max is not appropriate.\nAnother approach works better: policy-based methods. Policy-based methods\ndo not use a separate value function but ﬁnd the policy directly. They start with a\npolicy function, which they then improve, episode by episode, with policy gradient\nmethods. Policy-based methods are applicable to more domains than value-based\nmethods. They work well with deep neural networks and gradient learning; they\nare some of the most popular methods of deep reinforcement learning, and this\nchapter introduces you to them.\nWe start by looking at applications with continuous action spaces. Next, we look\nat policy-based agent algorithms. We will introduce basic policy search algorithms,\nand the policy gradient theorem. We will also discuss algorithms that combine\nvalue-based and policy-based approaches: the so-called Actor critic algorithms. At\nthe end of the chapter we discuss larger environments for policy-based methods in\nmore depth, where we will discuss progress in visuo-motor robotics and locomotion\nenvironments.\nThe chapter concludes with exercises, a summary, and pointers to further reading.\n93\n94\n4 Policy-Based Reinforcement Learning\nCore Concepts\n•\nPolicy gradient\n•\nBias-variance trade-oﬀ; Actor critic\nCore Problem\n•\nFind a low variance continuous action policy directly\nCore Algorithms\n•\nREINFORCE (Alg. 4.2)\n•\nAsynchronous advantage actor critic (Alg. 4.4)\n•\nProximal policy optimization (Sect. 4.2.5)\nJumping Robots\nOne of the most intricate problems in robotics is learning to walk, or more generally,\nhow to perform locomotion. Much work has been put into making robots walk, run\nand jump. A video of a simulated robot that taught itself to jump over an obstacle\ncourse can be found on YouTube1 [325].\nLearning to walk is a challenge that takes human infants months to master. (Cats\nand dogs are quicker.) Teaching robots to walk is a challenging problem that is\nstudied extensively in artiﬁcial intelligence and engineering. Movies abound on the\ninternet of robots that try to open doors, and fall over, or just try to stand upright,\nand still fall over.2\nLocomotion of legged robots is a diﬃcult sequential decision problem. For each\nleg, many diﬀerent joints are involved. They must be actuated in the right order,\nturned with the right force, over the right duration, to the right angle. Most of\nthese angles, forces, and durations are continuous. The algorithm has to decide\nhow many degrees, Newtons, and seconds, constitute the optimal policy. All these\nactions are continuous quantities. Robot locomotion is a diﬃcult problem, that is\nstudied frequently in policy-based deep reinforcement learning.\n1 https://www.youtube.com/watch?v=hx_bgoTF7bs\n2 See, for example, https://www.youtube.com/watch?v=g0TaYhjpOfo.\n4.1 Continuous Problems\n95\n4.1 Continuous Problems\nIn this chapter, our actions are continuous, and stochastic. We will discuss both of\nthese aspects, and some of the challenges they pose. We will start with continuous\naction policies.\n4.1.1 Continuous Policies\nIn the previous chapter we discussed environments with large state spaces. We will\nnow move our attention to action spaces. The action spaces of the problems that\nwe have seen so far—Grid worlds, mazes, and high-dimensional Atari games—were\nactually action spaces that were small and discrete—we could walk north, east, west,\nsouth, or we could choose from 9 joystick movements. In board games such as chess\nthe action space is larger, but still discrete. When you move your pawn to e4, you\ndo not move it to e4½.\nIn this chapter the problems are diﬀerent. Steering a self driving car requires\nturning the steering wheel a certain angle, duration, and angular velocity, to prevent\njerky movements. Throttle movements should also be smooth and continuous.\nActuation of robot joints is continuous, as we mentioned in the introduction of\nthis chapter. An arm joint can move 1 degree, 2 degrees, or 90 or 180 degrees or\nanything in between.\nAn action in a continuous space is not one of a set of discrete choices, such as\n{𝑁, 𝐸, 𝑊, 𝑆}, but rather a value over a continuous range, such as [0, 2𝜋] or R+; the\nnumber of possible values is inﬁnite. How can we ﬁnd the optimum value in an\ninﬁnite space in a ﬁnite amount of time? Trying out all possible combinations of\nsetting joint 1 to 𝑥degrees and applying force 𝑦in motor 2 will take inﬁnitely long.\nA solution could be to discretize the actions, although that introduces potential\nquantization errors.\nWhen actions are not discrete, the arg max operation can not be used to identify\n“the” best action, and value-based methods are no longer suﬃcient. Policy-based\nmethods ﬁnd suitable continuous or stochastic policies directly, without the inter-\nmediate step of a value function and the need for the arg max operation to construct\nthe ﬁnal policy.\n4.1.2 Stochastic Policies\nWe will now turn to the modeling of stochastic policies.\n96\n4 Policy-Based Reinforcement Learning\nWhen a robot moves its hand to open a door, it must judge the distance correctly.\nA small error, and it may fail (as many movie clips show).3 Stochastic environments\ncause stability problems for value-based methods [480]. Small perturbations in\nQ-values may lead to large changes in the policy of value-based methods. Con-\nvergence can typically only be achieved at slow learning rates, to smooth out the\nrandomness. A stochastic policy (a target distribution) does not suﬀer from this\nproblem. Stochastic policies have another advantage. By their nature they perform\nexploration, without the need to separately code 𝜖-greediness or other exploration\nmethods, since a stochastic policy returns a distribution over actions.\nPolicy-based methods ﬁnd suitable stochastic policies directly. A potential dis-\nadvantage of purely episodic policy-based methods is that they are high-variance;\nthey may ﬁnd local optima instead of global optima, and converge slower than\nvalue-based methods. Newer (actor critic) methods, such as A3C, TRPO, and PPO,\nwere designed to overcome these problems. We will discuss these algorithms later\nin this chapter.\nBefore we will explain these policy-based agent algorithms, we will have a closer\nlook at some of the applications for which they are needed.\n4.1.3 Environments: Gym and MuJoCo\nRobotic experiments play an important role in reinforcement learning. However, be-\ncause of the cost associated with real-world robotics experiments, in reinforcement\nlearning often simulated robotics systems are used. This is especially important in\nmodel-free methods, that tend to have a high sample complexity (real robots wear\ndown when trials run in the millions). These software simulators model behavior\nof the robot and the eﬀects on the environment, using physics models. This pre-\nvents the expense of real experiments with real robots, although some precision\nis lost to modeling error. Two well-known physics models are MuJoCo [780] and\nPyBullet [167]. They can be used easily via the Gym environment.\n4.1.3.1 Robotics\nMost robotic applications are more complicated than the classics such as mazes,\nMountain car and Cart pole. Robotic control decisions involve more joints, directions\nof travel, and degrees of freedom, than a single cart that moves in one dimension.\nTypical problems involve learning of visuo-motor skills (eye-hand coordination,\ngrasping), or learning of diﬀerent locomotion gaits of multi-legged “animals.” Some\nexamples of grasping and walking are illustrated in Fig. 4.1.\n3 Even worse, when a robot thinks it stands still, it may actually be in the process of falling over\n(and, of course, robots can not think, they only wished they could).\n4.1 Continuous Problems\n97\nFig. 4.1 Robot Grasping and Gait [501]\nThe environments for these actions are unpredictable to a certain degree: they\nrequire reactions to disturbances such as bumps in the road, or the moving of objects\nin a scene.\n4.1.3.2 Physics Models\nSimulating robot motion involves modeling forces, acceleration, velocity, and move-\nment. It also includes modeling mass and elasticity for bouncing balls, tactile/grasp-\ning mechanics, and the eﬀect of diﬀerent materials. A physics mechanics model\nneeds to simulate the result of actions in the real world. Among the goals of such a\nsimulation is to model grasping, locomotion, gaits, and walking and running (see\nalso Sect. 4.3.1).\nThe simulations should be accurate. Furthermore, since model-free learning\nalgorithms often involve millions of actions, it is important that the physics sim-\nulations are fast. Many diﬀerent physics environments for model-based robotics\nhave been created, among them Bullet, Havok, ODE and PhysX, see [228] for a\ncomparison. Of the models, MuJoCo [780], and PyBullet [167] are the most popular\nin reinforcement learning, especially MuJoCo is used in many experiments.\nAlthough MuJoCo calculations are deterministic, the initial state of environments\nis typically randomized, resulting in an overall non-deterministic environment.\nDespite many code optimizations in MuJoCo, simulating physics is still an expensive\nproposition. Most MuJoCo experiments in the literature therefore are based on\nstick-like entities, that simulate limited motions, in order to limit the computational\ndemands.\nFigures 4.2 and 4.3 illustrate a few examples of some of the common Gym/MuJoCo\nproblems that are often used in reinforcement learning: Ant, Half-cheetah, and\nHumanoid.\n98\n4 Policy-Based Reinforcement Learning\nFig. 4.2 Gym MuJoCo Ant and Half-Cheetah [108]\nFig. 4.3 Gym MuJoCo Humanoid\n4.1.3.3 Games\nIn real time video games and certain card games the decisions are also continuous.\nFor example, in some variants of poker, the size of monetary bets can be any amount,\nwhich makes the action space quite large (although strictly speaking still discrete).\nIn games such as StarCraft and Capture the Flag, aspects of the physical world are\nmodeled, and movement of agents can vary in duration and speed. The environment\nfor these games is also stochastic: some information is hidden for the agent. This\nincreases the size of the state space greatly. We will discuss these games in Chap. 7\nwhen we discuss multi-agent methods.\n4.2 Policy-Based Agents\nNow that we have discussed the problems and environments that are used with\npolicy-based methods, it is time to see how policy-based algorithms work. Policy-\nbased methods are a popular approach in model-free deep reinforcement learning.\nMany algorithms have been developed that perform well. Table 4.1 lists some of\nthe better known algorithms that will be covered in this chapter.\n4.2 Policy-Based Agents\n99\nName\nApproach\nRef\nREINFORCE Policy-gradient optimization\n[844]\nA3C\nDistributed Actor Critic\n[521]\nDDPG\nDerivative of continuous action function\n[480]\nTRPO\nDynamically sized step size\n[681]\nPPO\nImproved TRPO, ﬁrst order\n[683]\nSAC\nVariance-based Actor Critic for robustness [306]\nTable 4.1 Policy-Based Algorithms: REINFORCE, Asynchronous Advantage Actor Critic, Deep\nDeterministic Policy Gradient, Trust Region Policy Optimization, Proximal Policy Optimization,\nSoft Actor Critic\nWe will ﬁrst provide an intuitive explanation of the idea behind the basic policy-\nbased approach. Then we will discuss some of the theory behind it, as well as\nadvantages and disadvantages of the basic policy-based approach. Most of these\ndisadvantages are alleviated by the actor critic method, that is discussed next.\nLet us start with the basic idea behind policy-based methods.\n4.2.1 Policy-Based Algorithm: REINFORCE\nPolicy-based approaches learn a parameterized policy, that selects actions without\nconsulting a value function.4 In policy-based methods the policy function is repre-\nsented directly, allowing policies to select a continuous action, something that is\ndiﬃcult to do in value-based methods.\nThe Supermarket: To build some intuition on the nature of policy-based\nmethods, let us think back again at the supermarket navigation task, that\nwe used in Chap. 2. In this navigation problem we can try to assess our\ncurrent distance to the supermarket with the Q-value-function, as we have\ndone before. The Q-value assesses the distance of each direction to take; it\ntells us how far each action is from the goal. We can then use this distance\nfunction to ﬁnd our path.\nIn contrast, the policy-based alternative would be to ask a local the way,\nwho tells us, for example, to go straight and then left and then right at the\nOpera House and straight until we reach the supermarket on our left. The\nlocal just gave us a full path to follow, without having to infer which action\nwas the closest and then use that information to determine the way to go.\nWe can subsequently try to improve this full trajectory.\n4 Policy-based methods may use a value function to learn the policy parameters 𝜃, but do not use\nit for action selection.\n100\n4 Policy-Based Reinforcement Learning\nLet us see how we can optimize such a direct policy directly, without the intermedi-\nate step of the Q-function. We will develop a ﬁrst, generic, policy-based algorithm\nto see how the pieces ﬁt together. The explanation will be intuitive in nature.\nThe basic framework for policy-based algorithms is straightforward. We start\nwith a parameterized policy function 𝜋𝜃. We ﬁrst (1) initialize the parameters 𝜃\nof the policy function, (2) sample a new trajectory 𝜏, (3) if 𝜏is a good trajectory,\nincrease the parameters 𝜃towards 𝜏, otherwise decrease them, and (4) keep going\nuntil convergence. Algorithm 4.1 provides a framework in pseudocode. Please note\nthe similarity with the codes in the previous chapter (Listing 3.1–3.3), and especially\nthe deep learning algorithms, where we also optimized function parameters in a\nloop.\nThe policy is represented by a set of parameters 𝜃(these can be the weights in a\nneural network). Together, the parameters 𝜃map the states 𝑆to action probabilities\n𝐴. When we are given a set of parameters, how should we adjust them to improve\nthe policy? The basic idea is to randomly sample a new policy, and if it is better,\nadjust the parameters a bit in the direction of this new policy (and away if it is\nworse). Let us see in more detail how this idea works.\nTo know which policy is best, we need some kind of measure of its quality. We\ndenote the quality of the policy that is deﬁned by the parameters as 𝐽(𝜃). It is\nnatural to use the value function of the start state as our measure of quality\n𝐽(𝜃) = 𝑉𝜋(𝑠0).\nWe wish to maximize 𝐽(·). When the parameters are diﬀerentiable, then all we need\nto do is to ﬁnd a way to improve the gradient\n∇𝜃𝐽(𝜃) = ∇𝜃𝑉𝜋(𝑠0)\nof this expression to maximize our objective function 𝐽(·).\nPolicy-based methods apply gradient-based optimization, using the derivative\nof the objective to ﬁnd the optimum. Since we are maximizing, we apply gradient\nascent. In each time step 𝑡of the algorithm we perform the following update:\n𝜃𝑡+1 = 𝜃𝑡+ 𝛼· ∇𝜃𝐽(𝜃)\nfor learning rate 𝛼∈R+ and performance objective 𝐽, see the gradient ascent\nalgorithm in Alg. 4.1.\nRemember that 𝜋𝜃(𝑎|𝑠) is the probability of taking action 𝑎in state 𝑠. This\nfunction 𝜋is represented by a neural network 𝜃, mapping states 𝑆at the input\nside of the network to action probabilities on the output side of the network. The\nparameters 𝜃determine the mapping of our function 𝜋. Our goal is to update the\nparameters so that 𝜋𝜃becomes the optimal policy. The better the action 𝑎is, the\nmore we want to increase the parameters 𝜃.\nIf we now would know, by some magical way, the optimal action 𝑎★, then we\ncould use the gradient to push each parameter 𝜃𝑡, 𝑡∈trajectory, of the policy, in\nthe direction of the optimal action, as follows\n4.2 Policy-Based Agents\n101\nAlgorithm 4.1 Gradient ascent optimization\nInput: a diﬀerentiable objective 𝐽(𝜃), learning rate 𝛼∈R+, threshold 𝜖∈R+\nInitialization: randomly initialize 𝜃in R𝑑\nrepeat\nSample trajectory 𝜏and compute gradient ∇𝜃\n𝜃←𝜃+ 𝛼· ∇𝜃𝐽(𝜃)\nuntil ∇𝜃𝐽(𝜃) converges below 𝜖\nreturn parameters 𝜃\n𝜃𝑡+1 = 𝜃𝑡+ 𝛼∇𝜋𝜃𝑡(𝑎★|𝑠).\nUnfortunately, we do not know which action is best. We can, however, take a sample\ntrajectory and use estimates of the value of the actions of the sample. This estimate\ncan use the regular ˆ𝑄function from the previous chapter, or the discounted return\nfunction, or an advantage function (to be introduced shortly). Then, by multiplying\nthe push of the parameters (the probability) with our estimate, we get\n𝜃𝑡+1 = 𝜃𝑡+ 𝛼ˆ𝑄(𝑠, 𝑎)∇𝜋𝜃𝑡(𝑎|𝑠).\nA problem with this formula is that not only are we going to push harder on actions\nwith a high value, but also more often, because the policy 𝜋𝜃𝑡(𝑎|𝑠) is the probability\nof action 𝑎in state 𝑠. Good actions are thus doubly improved, which may cause\ninstability. We can correct by dividing by the general probability:\n𝜃𝑡+1 = 𝜃𝑡+ 𝛼ˆ𝑄(𝑠, 𝑎) ∇𝜋𝜃𝑡(𝑎|𝑠)\n𝜋𝜃(𝑎|𝑠) .\nIn fact, we have now almost arrived at the classic policy-based algorithm, REIN-\nFORCE, introduced by Williams in 1992 [844]. In this algorithm our formula is\nexpressed in a way that is reminiscent of a logarithmic cross-entropy loss function.\nWe can arrive at such a log-formulation by using the basic fact from calculus that\n∇log 𝑓(𝑥) = ∇𝑓(𝑥)\n𝑓(𝑥) .\nSubstituting this formula into our equation, we arrive at\n𝜃𝑡+1 = 𝜃𝑡+ 𝛼ˆ𝑄(𝑠, 𝑎)∇𝜃log 𝜋𝜃(𝑎|𝑠).\nThis formula is indeed the core of REINFORCE, the prototypical policy-based\nalgorithm, which is shown in full in Alg. 4.2, with discounted cumulative reward.\nTo summarize, the REINFORCE formula pushes the parameters of the policy\nin the direction of the better action (multiplied proportionally by the size of the\nestimated action-value) to know which action is best.\nWe have arrived at a method to improve a policy that can be used directly to\nindicate the action to take. The method whether the action is discrete, continuous, or\n102\n4 Policy-Based Reinforcement Learning\nAlgorithm 4.2 Monte Carlo policy gradient (REINFORCE) [844]\nInput: A diﬀerentiable policy 𝜋𝜃(𝑎|𝑠), learning rate 𝛼∈R+, threshold 𝜖∈R+\nInitialization: Initialize parameters 𝜃in R𝑑\nrepeat\nGenerate full trace 𝜏= {𝑠0, 𝑎0, 𝑟0, 𝑠1, .., 𝑠𝑇} following 𝜋𝜃(𝑎|𝑠)\nfor 𝑡∈0, . . . , 𝑇−1 do\n⊲Do for each step of the episode\n𝑅←Í𝑇−1\n𝑘=𝑡𝛾𝑘−𝑡· 𝑟𝑘\n⊲Sum Return from trace\n𝜃←𝜃+ 𝛼𝛾𝑡𝑅∇𝜃log 𝜋𝜃(𝑎𝑡|𝑠𝑡)\n⊲Adjust parameters\nend for\nuntil ∇𝜃𝐽(𝜃) converges below 𝜖\nreturn Parameters 𝜃\nstochastic, without having to go through intermediate value or arg max functions\nto ﬁnd it. Algorithm 4.2 shows the full algorithm, which is called Monte Carlo policy\ngradient. The algorithm is called Monte Carlo because it samples a trajectory.\nOnline and Batch\nThe versions of gradient ascent (Alg. 4.1) and REINFORCE (Alg. 4.2) that we show,\nupdate the parameters inside the innermost loop. All updates are performed as the\ntime steps of the trajectory are traversed. This method is called the online approach.\nWhen multiple processes work in parallel to update data, the online approach makes\nsure that information is used as soon as it is known.\nThe policy gradient algorithm can also be formulated in batch-fashion: all gradi-\nents are summed over the states and actions, and the parameters are updated at the\nend of the trajectory. Since parameter updates can be expensive, the batch approach\ncan be more eﬃcient. An intermediate form that is frequently applied in practice\nis to work with mini-batches, trading oﬀcomputational eﬃciency for information\neﬃciency.\nLet us now take a step back and look at the algorithm and assess how well it\nworks.\n4.2.2 Bias-Variance Trade-Oﬀin Policy-Based Methods\nNow that we have seen the principles behind a policy-based algorithm, let us\nsee how policy-based algorithms work in practice, and compare advantages and\ndisadvantages of the policy-based approach.\nLet us start with the advantages. First of all, parameterization is at the core of\npolicy-based methods, making them a good match for deep learning. For value-\nbased methods deep learning had to be retroﬁtted, giving rise to complications\nas we saw in Sect. 3.2.3. Second, policy-based methods can easily ﬁnd stochastic\npolicies; value-based methods ﬁnd deterministic policies. Due to their stochastic\n4.2 Policy-Based Agents\n103\nnature, policy-based methods naturally explore, without the need for methods such\nas 𝜖-greedy, or more involved methods, that may require tuning to work well. Third,\npolicy-based methods are eﬀective in large or continuous action spaces. Small\nchanges in 𝜃lead to small changes in 𝜋, and to small changes in state distributions\n(they are smooth). Policy-based algorithms do not suﬀer (as much) from convergence\nand stability issues that are seen in arg max-based algorithms in large or continuous\naction spaces.\nOn the other hand, there are disadvantages to the episodic Monte Carlo version\nof the REINFORCE algorithm. Remember that REINFORCE generates a full random\nepisode in each iteration, before it assesses the quality. (Value-based methods use a\nreward to select the next action in each time step of the episode.) Because of this,\npolicy-based is low bias, since full random trajectories are generated. However, they\nare also high variance, since the full trajectory is generated randomly (whereas value-\nbased uses the value for guidance at each selection step). What are the consequences?\nFirst, policy evaluation of full trajectories has low sample eﬃciency and high\nvariance. As a consequence, policy improvement happens infrequently, leading to\nslow convergence compared to value-based methods. Second, this approach often\nﬁnds a local optimum, since convergence to the global optimum takes too long.\nMuch research has been performed to address the high variance of the episode-\nbased vanilla policy gradient [57, 421, 420, 293]. The enhancements that have\nbeen found have greatly improved performance, so much so that policy-based\napproaches—such as A3C, PPO, SAC, DDPG—have become favorite model-free\nreinforcement learning algorithms for many applications. The enhancements to\nreduce high variance that we discuss are:\n•\nActor critic introduces within-episode value-based critics based on temporal\ndiﬀerence value bootstrapping;\n•\nBaseline subtraction introduces an advantage function to lower variance;\n•\nTrust regions reduce large policy parameter changes;\n•\nExploration is crucial to get out of local minima and for more robust result; high\nentropy action distributions are often used.\nLet us have a look at these enhancements.\n4.2.3 Actor Critic Bootstrapping\nThe actor critic approach combines value-based elements with the policy-based\nmethod. The actor stands for the action, or policy-based, approach; the critic stands\nfor the value-based approach [743].\nAction selection in episodic REINFORCE is random, and hence low bias. However,\nvariance is high, since the full episode is sampled (the size and direction of the\nupdate can strongly vary between diﬀerent samples). The actor critic approach is\ndesigned to combine the advantage of the value-based approach (low variance) with\nthe advantage of the policy-based approach (low bias). Actor critic methods are\n104\n4 Policy-Based Reinforcement Learning\nAlgorithm 4.3 Actor critic with bootstrapping\nInput: A policy 𝜋𝜃(𝑎|𝑠), a value function 𝑉𝜙(𝑠)\nAn estimation depth 𝑛, learning rate 𝛼, number of episodes 𝑀\nInitialization: Randomly initialize 𝜃and 𝜙\nrepeat\nfor 𝑖∈1, . . . , 𝑀do\nSample trace 𝜏= {𝑠0, 𝑎0, 𝑟0, 𝑠1, .., 𝑠𝑇} following 𝜋𝜃(𝑎|𝑠)\nfor 𝑡∈0, . . . , 𝑇−1 do\nˆ𝑄𝑛(𝑠𝑡, 𝑎𝑡) = Í𝑛−1\n𝑘=0 𝛾𝑘· 𝑟𝑡+𝑘+ 𝛾𝑛· 𝑉𝜙(𝑠𝑡+𝑛)\n⊲𝑛-step target\nend for\nend for\n𝜙←𝜙−𝛼· ∇𝜙\nÍ\n𝑡\n\u0000 ˆ𝑄𝑛(𝑠𝑡, 𝑎𝑡) −𝑉𝜙(𝑠𝑡)\u00012\n⊲Descent value loss\n𝜃←𝜃+ 𝛼· Í\n𝑡[ ˆ𝑄𝑛(𝑠𝑡, 𝑎𝑡) · ∇𝜃log 𝜋𝜃(𝑎𝑡|𝑠𝑡)]\n⊲Ascent policy gradient\nuntil ∇𝜃𝐽(𝜃) converges below 𝜖\nreturn Parameters 𝜃\npopular because they work well. It is an active ﬁeld where many diﬀerent algorithms\nhave been developed.\nThe variance of policy methods can originate from two sources: (1) high variance\nin the cumulative reward estimate, and (2) high variance in the gradient estimate.\nFor both problems a solution has been developed: bootstrapping for better reward\nestimates, and baseline subtraction to lower the variance of gradient estimates. Both\nof these methods use the learned value function, which we denote by 𝑉𝜙(𝑠). The\nvalue function can use a separate neural network, with separate parameters 𝜙, or it\ncan use a value head on top of the actor parameters 𝜃. In this case the actor and\nthe critic share the lower layers of the network, and the network has two separate\ntop heads: a policy and a value head. We will use 𝜙for the parameters of the value\nfunction, to discriminate them from the policy parameters 𝜃.\nTemporal Diﬀerence Bootstrapping\nTo reduce the variance of the policy gradient, we can increase the number of traces\n𝑀that we sample. However, the possible number of diﬀerent traces is exponential\nin the length of the trace for a given stochastic policy, and we cannot aﬀord to\nsample them all for one update. In practice the number of sampled traces 𝑀is small,\nsometimes even 𝑀= 1, updating the policy parameters from a single trace. The\nreturn of the trace depends on many random action choices; the update has high\nvariance. A solution is to use a principle that we known from temporal diﬀerence\nlearning, to bootstrap the value function step by step. Bootstrapping uses the value\nfunction to compute intermediate 𝑛-step values per episode, trading-oﬀvariance\nfor bias. The 𝑛-step values are in-between full-episode Monte Carlo and single step\ntemporal diﬀerence targets.\nWe can use bootstrapping to compute an 𝑛-step target\n4.2 Policy-Based Agents\n105\nˆ𝑄n(𝑠𝑡, 𝑎𝑡) =\n𝑛−1\n∑︁\n𝑘=0\n𝑟𝑡+𝑘+ 𝑉𝜙(𝑠𝑡+𝑛),\nand we can then update the value function, for example on a squared loss\nL(𝜙|𝑠𝑡, 𝑎𝑡) = \u0000 ˆ𝑄𝑛(𝑠𝑡, 𝑎𝑡) −𝑉𝜙(𝑠𝑡)\u00012\nand update the policy with the standard policy gradient but with that (improved)\nvalue ˆ𝑄𝑛\n∇𝜃L(𝜃|𝑠𝑡, 𝑎𝑡) = ˆ𝑄𝑛(𝑠𝑡, 𝑎𝑡) · ∇𝜃log 𝜋𝜃(𝑎𝑡|𝑠𝑡).\nWe are now using the value function prominently in the algorithm, which is param-\neterized by a separate set of parameters, denoted by 𝜙; the policy parameters are\nstill denoted by 𝜃. The use of both policy and value is what gives the actor critic\napproach its name.\nAn example algorithm is shown in Alg. 4.3. When we compare this algorithm\nwith Alg. 4.2, we see how the policy gradient ascent update now uses the 𝑛-step ˆ𝑄𝑛\nvalue estimate instead of the trace return 𝑅. We also see that this time the parameter\nupdates are in batch mode, with separate summations.\n4.2.4 Baseline Subtraction with Advantage Function\nAnother method to reduce the variance of the policy gradient is by baseline subtrac-\ntion. Subtracting a baseline from a set of numbers reduces the variance, but leaves\nthe expectation unaﬀected. Assume, in a given state with three available actions,\nthat we sample action returns of 65, 70, and 75, respectively. Policy gradient will\nthen try to push the probability of each action up, since the return for each action is\npositive. The above method may lead to a problem, since we are pushing all actions\nup (only somewhat harder on one of them). It might be better if we only push up\non actions that are higher than the average (action 75 is higher than the average of\n70 in this example), and push down on actions that are below average (65 in this\nexample). We can do so through baseline subtraction.\nThe most common choice for the baseline is the value function. When we subtract\nthe value 𝑉from a state-action value estimate 𝑄, the function is called the advantage\nfunction:\n𝐴(𝑠𝑡, 𝑎𝑡) = 𝑄(𝑠𝑡, 𝑎𝑡) −𝑉(𝑠𝑡).\nThe 𝐴function subtracts the value of the state 𝑠from the state-action value. It now\nestimates how much better a particular action is compared to the expectation of a\nparticular state.\nWe can combine baseline subtraction with any bootstrapping method to estimate\nthe cumulative reward ˆ𝑄(𝑠𝑡, 𝑎𝑡). We compute\nˆ𝐴n(𝑠𝑡, 𝑎𝑡) = ˆ𝑄𝑛(𝑠𝑡, 𝑎𝑡) −𝑉𝜙(𝑠𝑡)\n106\n4 Policy-Based Reinforcement Learning\nAlgorithm 4.4 Actor critic with bootstrapping and baseline subtraction\nInput: A policy 𝜋𝜃(𝑎|𝑠), a value function 𝑉𝜙(𝑠)\nAn estimation depth 𝑛, learning rate 𝛼, number of episode 𝑀\nInitialization: Randomly initialize 𝜃and 𝜙\nwhile not converged do\nfor 𝑖= 1, . . . , 𝑀do\nSample trace 𝜏= {𝑠0, 𝑎0, 𝑟0, 𝑠1, .., 𝑠𝑇} following 𝜋𝜃(𝑎|𝑠)\nfor 𝑡= 0, . . . , 𝑇−1 do\nˆ𝑄𝑛(𝑠𝑡, 𝑎𝑡) = Í𝑛−1\n𝑘=0 𝛾𝑘· 𝑟𝑡+𝑘+ 𝛾𝑛· 𝑉𝜙(𝑠𝑡+𝑛)\n⊲𝑛-step target\nˆ𝐴n(𝑠𝑡, 𝑎𝑡) = ˆ𝑄𝑛(𝑠𝑡, 𝑎𝑡) −𝑉𝜙(𝑠𝑡)\n⊲Advantage\nend for\nend for\n𝜙←𝜙−𝛼· ∇𝜙\nÍ\n𝑡\n\u0000 ˆ𝐴𝑛(𝑠𝑡, 𝑎𝑡)\u00012\n⊲Descent Advantage loss\n𝜃←𝜃+ 𝛼· Í\n𝑡[ ˆ𝐴𝑛(𝑠𝑡, 𝑎𝑡) · ∇𝜃log 𝜋𝜃(𝑎𝑡|𝑠𝑡)]\n⊲Ascent policy gradient\nend while\nreturn Parameters 𝜃\nand update the policy with\n∇𝜃L(𝜃|𝑠𝑡, 𝑎𝑡) = ˆ𝐴𝑛(𝑠𝑡, 𝑎𝑡) · ∇𝜃log 𝜋𝜃(𝑎𝑡|𝑠𝑡).\nWe have now seen the ingredients to construct a full actor critic algorithm. An\nexample algorithm is shown in Alg. 4.4.\nGeneric Policy Gradient Formulation\nWith these two ideas we can formulate an entire spectrum of policy gradient\nmethods, depending on the type of cumulative reward estimate that they use. In\ngeneral, the policy gradient estimator takes the following form, where we now\nintroduce a new target Ψ𝑡that we sample from the trajectories 𝜏:\n∇𝜃𝐽(𝜃) = E𝜏0∼𝑝𝜃(𝜏0)\nh\n𝑛\n∑︁\n𝑡=0\nΨ𝑡∇𝜃log 𝜋𝜃(𝑎𝑡|𝑠𝑡)\ni\nThere is a variety of potential choices for Ψ𝑡, based on the use of bootstrapping and\nbaseline substraction:\n4.2 Policy-Based Agents\n107\nΨ𝑡= ˆ𝑄𝑀𝐶(𝑠𝑡, 𝑎𝑡) =\n∞\n∑︁\n𝑖=𝑡\n𝛾𝑖· 𝑟𝑖\nMonte Carlo target\nΨ𝑡= ˆ𝑄𝑛(𝑠𝑡, 𝑎𝑡)\n=\n𝑛−1\n∑︁\n𝑖=𝑡\n𝛾𝑖· 𝑟𝑖+ 𝛾𝑛𝑉𝜃(𝑠𝑛)\nbootstrap (𝑛-step target)\nΨ𝑡= ˆ𝐴𝑀𝐶(𝑠𝑡, 𝑎𝑡) =\n∞\n∑︁\n𝑖=𝑡\n𝛾𝑖· 𝑟𝑖−𝑉𝜃(𝑠𝑡)\nbaseline subtraction\nΨ𝑡= ˆ𝐴𝑛(𝑠𝑡, 𝑎𝑡)\n=\n𝑛−1\n∑︁\n𝑖=𝑡\n𝛾𝑖· 𝑟𝑖+ 𝛾𝑛𝑉𝜃(𝑠𝑛) −𝑉𝜃(𝑠𝑡)\nbaseline + bootstrap\nΨ𝑡= 𝑄𝜙(𝑠𝑡, 𝑎𝑡)\nQ-value approximation\nActor critic algorithms are among the most popular model-free reinforcement\nlearning algorithms in practice, due to their good performance. After having dis-\ncussed relevant theoretical background, it is time to look at how actor critic can be\nimplemented in a practical, high performance, algorithm. We will start with A3C.\nAsynchronous Advantage Actor Critic\nMany high performance implementations are based on the actor critic approach. For\nlarge problems the algorithm is typically parallelized and implemented on a large\ncluster computer. A well-known parallel algorithm is Asynchronous advantage actor\ncritic (A3C). A3C is a framework that uses asynchronous (parallel and distributed)\ngradient descent for optimization of deep neural network controllers [521].\nThere is also a non-parallel version of A3C, the synchronous variant A2C [849].\nTogether they popularized this approach to actor critic methods. Figure 4.4 shows\nthe distributed architecture of A3C [382]; Alg. 4.5 shows the pseudocode, from\nMnih et al. [521]. The A3C network will estimate both a value function 𝑉𝜙(𝑠)\nand an advantage function 𝐴𝜙(𝑠, 𝑎), as well as a policy function 𝜋𝜃(𝑎|𝑠). In the\nexperiments on Atari [521], the neural networks were separate fully-connected\npolicy and value heads at the top (orange in Fig. 4.4), followed by joint convolutional\nnetworks (blue). This network architecture is replicated over the distributed workers.\nEach of these workers are run on a separate processor thread and are synced with\nglobal parameters from time to time.\nA3C improves on classic REINFORCE in the following ways: it uses an advantage\nactor critic design, it uses deep learning, and it makes eﬃcient use of parallelism\nin the training stage. The gradient accumulation step at the end of the code can\nbe considered as a parallelized reformulation of minibatch-based stochastic gra-\ndient update: the values of 𝜙or 𝜃are adjusted in the direction of each training\nthread independently. A major contribution of A3C comes from its parallelized\nand asynchronous architecture: multiple actor-learners are dispatched to separate\ninstantiations of the environment; they all interact with the environment and collect\n108\n4 Policy-Based Reinforcement Learning\nFig. 4.4 A3C network [382]\nexperience, and asynchronously push their gradient updates to a central target\nnetwork (just as DQN).\nIt was found that the parallel actor-learners have a stabilizing eﬀect on training.\nA3C surpassed the previous state-of-the-art on the Atari domain and succeeded on\na wide variety of continuous motor control problems as well as on a new task of\nnavigating random 3D mazes using high-resolution visual input [521].\n4.2.5 Trust Region Optimization\nAnother important approach to further reduce the variance of policy methods is\nthe trust region approach. Trust region policy optimization (TRPO) aims to further\nreduce the high variability in the policy parameters, by using a special loss function\nwith an additional constraint on the optimization problem [681].\nA naive approach to speed up an algorithm is to try to increase the step size\nof hyperparameters, such as the learning rate, and the policy parameters. This\napproach will fail to uncover solutions that are hidden in ﬁner grained trajectories,\nand the optimization will converge to local optima. For this reason the step size\nshould not be too large. A less naive approach is to use an adaptive step size that\ndepends on the output of the optimization progress.\nTrust regions are used in general optimization problems to constrain the update\nsize [734]. The algorithms work by computing the quality of the approximation; if\n4.2 Policy-Based Agents\n109\nAlgorithm 4.5 Asynchronous advantage actor-critic pseudocode for each actor-\nlearner thread [521]\nInput: Assume global shared parameter vectors 𝜃and 𝜙and global shared counter 𝑇= 0\nAssume thread-speciﬁc parameter vectors 𝜃′ and 𝜙′\nInitialize thread step counter 𝑡←1\nrepeat\nReset gradients: 𝑑𝜃←0 and 𝑑𝜙←0.\nSynchronize thread-speciﬁc parameters 𝜃′ = 𝜃and 𝜙′ = 𝜙\n𝑡𝑠𝑡𝑎𝑟𝑡= 𝑡\nGet state 𝑠𝑡\nrepeat\nPerform 𝑎𝑡according to policy 𝜋(𝑎𝑡|𝑠𝑡; 𝜃′)\nReceive reward 𝑟𝑡and new state 𝑠𝑡+1\n𝑡←𝑡+ 1\n𝑇←𝑇+ 1\nuntil terminal 𝑠𝑡or 𝑡−𝑡𝑠𝑡𝑎𝑟𝑡== 𝑡𝑚𝑎𝑥\n𝑅=\n\u001a\n0\nfor terminal 𝑠𝑡\n𝑉(𝑠𝑡, 𝜙′) for non-terminal 𝑠𝑡// Bootstrap from last state\nfor 𝑖∈{𝑡−1, . . . , 𝑡𝑠𝑡𝑎𝑟𝑡} do\n𝑅←𝑟𝑖+ 𝛾𝑅\nAccumulate gradients wrt 𝜃′: 𝑑𝜃←𝑑𝜃+ ∇𝜃′ log 𝜋(𝑎𝑖|𝑠𝑖; 𝜃′) (𝑅−𝑉(𝑠𝑖; 𝜙′))\nAccumulate gradients wrt 𝜙′: 𝑑𝜙←𝑑𝜙+ 𝜕(𝑅−𝑉(𝑠𝑖; 𝜙′))2/𝜕𝜙′\nend for\nPerform asynchronous update of 𝜃using 𝑑𝜃and of 𝜙using 𝑑𝜙.\nuntil 𝑇> 𝑇𝑚𝑎𝑥\nit is still good, then the trust region is expanded. Alternatively, the region can be\nshrunk if the divergence of the new and current policy is getting large.\nSchulman et al. [681] introduced trust region policy optimization (TRPO) based\non this ideas, trying to take the largest possible parameter improvement step on a\npolicy, without accidentally causing performance to collapse.\nTo this end, as it samples policies, TRPO compares the old and the new policy:\nL(𝜃) = E𝑡\nh 𝜋𝜃(𝑎𝑡|𝑠𝑡)\n𝜋𝜃old(𝑎𝑡|𝑠𝑡) · 𝐴𝑡\ni\n.\nIn order to increase the learning step size, TRPO tries to maximize this loss function\nL, subject to the constraint that the old and the new policy are not too far away. In\nTRPO the Kullback-Leibler divergence5 is used for this purpose:\nE𝑡[KL(𝜋𝜃old(·|𝑠𝑡), 𝜋𝜃(·|𝑠𝑡))] ≤𝛿.\nTRPO scales to complex high-dimensional problems. Original experiments\ndemonstrated its robust performance on simulated robotic Swimming, Hopping,\nWalking gaits, and Atari games. TRPO is commonly used in experiments and as\na baseline for developing new algorithms. A disadvantage of TRPO is that it is\n5 The Kullback-Leibler divergence is a measure of distance between probability distributions [436,\n93].\n110\n4 Policy-Based Reinforcement Learning\na complicated algorithm that uses second order derivatives; we will not cover\nthe pseudocode here. Implementations can be found at Spinning Up6 and Stable\nBaselines.7\nProximal policy optimzation (PPO) [683] was developed as an improvement\nof TRPO. PPO has some of the beneﬁts of TRPO, but is simpler to implement,\nis more general, has better empirical sample complexity and has better run time\ncomplexity. It is motivated by the same question as TRPO, to take the largest possible\nimprovement step on a policy parameter without causing performance collapse.\nThere are two variants of PPO: PPO-Penalty and PPO-Clip. PPO-Penalty ap-\nproximately solves a KL-constrained update (like TRPO), but merely penalizes the\nKL-divergence in the objective function instead of making it a hard constraint.\nPPO-Clip does not use a KL-divergence term in the objective and has no constraints\neither. Instead it relies on clipping in the objective function to remove incentives\nfor the new policy to get far from the old policy; it clips the diﬀerence between the\nold and the new policy within a ﬁxed range [1 −𝜖, 1 + 𝜖] · 𝐴𝑡.\nWhile simpler than TRPO, PPO is still a complicated algorithm to implement,\nand we omit the code here. The authors of PPO provide an implementation as a\nbaseline.8 Both TRPO and PPO are on-policy algorithms. Hsu et al. [353] reﬂect on\ndesign choices of PPO.\n4.2.6 Entropy and Exploration\nA problem in many deep reinforcement learning experiments where only a frac-\ntion of the state space is sampled, is brittleness: the algorithms get stuck in local\noptima, and diﬀerent choices for hyperparameters can cause large diﬀerences in\nperformance. Even a diﬀerent choice for seed for the random number generator\ncan cause large diﬀerences in performance for many algorithms.\nFor large problems, exploration is important, in value-based and policy-based\napproaches alike. We must provide the incentive to sometimes try an action which\ncurrently seems suboptimal [524]. Too little exploration results in brittle, local,\noptima.\nWhen we learn a deterministic policy 𝜋𝜃(𝑠) →𝑎, we can manually add ex-\nploration noise to the behavior policy. In a continuous action space we can use\nGaussian noise, while in a discrete action space we can use Dirichlet noise [425].\nFor example, in a 1D continuous action space we could use:\n𝜋𝜃,behavior(𝑎|𝑠) = 𝜋𝜃(𝑠) + N (0, 𝜎),\nwhere N (𝜇, 𝜎) is the Gaussian (normal) distribution with hyperparameters mean\n𝜇= 0 and standard deviation 𝜎; 𝜎is our exploration hyperparameter.\n6 https://spinningup.openai.com\n7 https://stable-baselines.readthedocs.io\n8 https://openai.com/blog/openai-baselines-ppo/#ppo\n4.2 Policy-Based Agents\n111\nSoft Actor Critic\nWhen we learn a stochastic policy 𝜋(𝑎|𝑠), then exploration is already partially\nensured due to the stochastic nature of our policy. For example, when we predict\na Gaussian distribution, then simply sampling from this distribution will already\ninduce variation in the chosen actions.\n𝜋𝜃,behavior(𝑎|𝑠) = 𝜋𝜃(𝑎|𝑠)\nHowever, when there is not suﬃcient exploration, a potential problem is the collapse\nof the policy distribution. The distribution then becomes too narrow, and we lose\nthe exploration pressure that is necessary for good performance.\nAlthough we could simply add additional noise, another common approach\nis to use entropy regularization (see Sect. A.2 for details). We then add an addi-\ntional penalty to the loss function, that enforces the entropy 𝐻of the distribution\nto stay larger. Soft actor critic (SAC) is a well-known algorithm that focuses on\nexploration [306, 307].9 SAC extends the policy gradient equation to\n𝜃𝑡+1 = 𝜃𝑡+ 𝑅· ∇𝜃log 𝜋𝜃(𝑎𝑡|𝑠𝑡) + 𝜂∇𝜃𝐻[𝜋𝜃(·|𝑠𝑡)]\nwhere 𝜂∈R+ is a constant that determines the amount of entropy regularization.\nSAC ensures that we will move 𝜋𝜃(𝑎|𝑠) to the optimal policy, while also ensuring\nthat the policy stays as wide as possible (trading oﬀthe two against eachother).\nEntropy is computed as 𝐻= −Í\n𝑖𝑝𝑖log 𝑝𝑖where 𝑝𝑖is the probability of being\nin state 𝑖; in SAC entropy is the negative log of the stochastic policy function\n−log 𝜋𝜃(𝑎|𝑠).\nHigh-entropy policies favor exploration. First, the policy is incentivized to ex-\nplore more widely, while giving up on clearly unpromising avenues. Second, with\nimproved exploration comes improved learning speed.\nMost policy-based algorithms (including A3C, TRPO, and PPO) only optimize\nfor expected value. By including entropy explicitly in the optimization goal, SAC\nis able to increase the stability of outcome policies, achieving stable results for\ndiﬀerent random seeds, and reducing the sensitivity to hyperparameter settings.\nIncluding entropy into the optimization goal has been studied widely, see, for\nexample, [305, 396, 779, 880, 547].\nA further element that SAC uses to improve stability and sample eﬃciency is a\nreplay buﬀer. Many policy-based algorithms are on-policy learners (including A3C,\nTRPO, and PPO). In on-policy algorithms each policy improvement uses feedback\non actions according to the most recent version of the behavior policy. On-policy\nmethods converge well, but tend to require many samples to do so. In contrast, many\nvalue-based algorithms are oﬀ-policy: each policy improvement can use feedback\ncollected at any earlier point during training, regardless of how the behavior policy\nwas acting to explore the environment at the time when the feedback was obtained.\nThe replay buﬀer is such a mechanism, breaking out of local maxima. Large replay\n9 https://github.com/haarnoja/sac\n112\n4 Policy-Based Reinforcement Learning\nbuﬀers cause oﬀ-policy behavior, improving sample eﬃciency by learning from\nbehavior of the past, but also potentially causing convergence problems. Like DQN,\nSAC has overcome these problems, and achieves stable oﬀ-policy performance.\n4.2.7 Deterministic Policy Gradient\nActor critic approaches improve the policy-based approach with various value-based\nideas, and with good results. Another method to join policy and value approaches\nis to use a learned value function as a diﬀerentiable target to optimize the policy\nagainst—we let the policy follow the value function [524]. An example is the deter-\nministic policy gradient [705]. Imagine we collect data 𝐷and train a value network\n𝑄𝜙(𝑠, 𝑎). We can then attempt to optimize the parameters 𝜃of a deterministic\npolicy by optimizing the prediction of the value network:\n𝐽(𝜃) = E𝑠∼𝐷\nh\n𝑛\n∑︁\n𝑡=0\n𝑄𝜙(𝑠, 𝜋𝜃(𝑠))\ni\n,\nwhich by the chain-rule gives the following gradient expression\n∇𝜃𝐽(𝜃) =\n𝑛\n∑︁\n𝑡=0\n∇𝑎𝑄𝜙(𝑠, 𝑎) · ∇𝜃𝜋𝜃(𝑠).\nIn essence, we ﬁrst train a state-action value network based on sampled data,\nand then let the policy follow the value network, by simply chaining the gradients.\nThereby, we push the policy network in the direction of those actions 𝑎that increase\nthe value network prediction, towards actions that perform better.\nLillicrap et al. [480] present Deep deterministic policy gradient (DDPG). It is\nbased on DQN, with the purpose of applying it to continuous action functions.\nIn DQN, if the optimal action-value function 𝑄★(𝑠, 𝑎) is known, then the optimal\naction 𝑎★(𝑠) can be found via 𝑎★(𝑠) = arg max𝑎𝑄★(𝑠, 𝑎). DDPG uses the derivative\nof a continuous function 𝑄(𝑠, 𝑎) with respect to the action argument to eﬃciently\napproximate max𝑎𝑄(𝑠, 𝑎). DDPG is also based on the algorithms Deterministic\npolicy gradients (DPG) [705] and Neurally ﬁtted Q-learning with continuous actions\n(NFQCA) [311], two actor critic algorithms.\nThe pseudocode of DDPG is shown in Alg. 4.6. DDPG has been shown to work\nwell on simulated physics tasks, including classic problems such as Cartpole, Gripper,\nWalker, and Car driving, being able to learn policies directly from raw pixel inputs.\nDDPG is oﬀ-policy and uses a replay buﬀer and a separate target network to achieve\nstable deep reinforcement learning (just as DQN).\n4.2 Policy-Based Agents\n113\nAlgorithm 4.6 DDPG algorithm [480]\nRandomly initialize critic network 𝑄𝜙(𝑠, 𝑎) and actor 𝜋𝜃(𝑠) with weights 𝜙and 𝜃.\nInitialize target network 𝑄′ and 𝜋′ with weights 𝜙′ ←𝜙, 𝜃′ ←𝜃\nInitialize replay buﬀer 𝑅\nfor episode = 1, M do\nInitialize a random process N for action exploration\nReceive initial observation state 𝑠1\nfor t = 1, T do\nSelect action 𝑎𝑡= 𝜋𝜃(𝑠𝑡) + N𝑡according to the current policy and exploration noise\nExecute action 𝑎𝑡and observe reward 𝑟𝑡and observe new state 𝑠𝑡+1\nStore transition (𝑠𝑡, 𝑎𝑡, 𝑟𝑡, 𝑠𝑡+1) in 𝑅\nSample a random minibatch of 𝑁transitions (𝑠𝑖, 𝑎𝑖, 𝑟𝑖, 𝑠𝑖+1) from 𝑅\nSet 𝑦𝑖= 𝑟𝑖+ 𝛾𝑄𝜙′ (𝑠𝑖+1, 𝜋𝜃′ (𝑠𝑖+1))\nUpdate critic by minimizing the loss: 𝐿=\n1\n𝑁\nÍ\n𝑖(𝑦𝑖−𝑄𝜙(𝑠𝑖, 𝑎𝑖))2\nUpdate the actor policy using the sampled policy gradient:\n∇𝜃𝐽≈1\n𝑁\n∑︁\n𝑖\n∇𝑎𝑄𝜙(𝑠, 𝑎) |𝑠=𝑠𝑖,𝑎=𝜇(𝑠𝑖) ∇𝜃𝜋𝜃(𝑠) |𝑠𝑖\nUpdate the target networks:\n𝜙′ ←𝜏𝜙+ (1 −𝜏) 𝜙′\n𝜃′ ←𝜏𝜃+ (1 −𝜏) 𝜃′\nend for\nend for\nDDPG is a popular actor critic algorithm. Annotated pseudocode and eﬃcient\nimplementations can be found at Spinning Up10 and Stable Baselines11 in addition\nto the original paper [480].\nConclusion\nWe have seen quite some algorithms that combine the policy and value approach,\nand we have discussed possible combinations of these building blocks to construct\nworking algorithms. Figure 4.5 provides a conceptual map of how the diﬀerent\napproaches are related, including two approaches that will be discussed in later\nchapters (AlphaZero and Evolutionary approaches).\nResearchers have constructed many algorithms and performed experiments to\nsee when they perform best. Quite a number of actor critic algorithms have been\ndeveloped. Working high-performance Python implementations can be found on\nGitHub in the Stable Baselines.12\n10 https://spinningup.openai.com\n11 https://stable-baselines.readthedocs.io\n12 https://stable-baselines.readthedocs.io/en/master/guide/quickstart.html\n114\n4 Policy-Based Reinforcement Learning\nFig. 4.5 Value-based, policy-based and Actor critic methods [524].\n4.2.8 Hands On: PPO and DDPG MuJoCo Examples\nOpenAI’s Spinning Up provides a tutorial on policy gradient algorithms, complete\nwith TensorFlow and PyTorch versions of REINFORCE to learn Gym’s Cartpole.13\nwith the TensorFlow code14 or PyTorch.15\nNow that we have discussed these algorithms, let us see how they work in\npractice, to get a feeling for the algorithms and their hyperparameters. MuJoCo is the\nmost frequently used physics simulator in policy-based learning experiments. Gym,\nthe (Stable) Baselines and Spinning up allow us to run any mix of learning algorithms\nand experimental environments. You are encouraged to try these experiments\nyourself.\nPlease be warned, however, that attempting to install all necessary pieces of\nsoftware may invite a minor version-hell. Diﬀerent versions of your operating\nsystem, of Python, of GCC, of Gym, of the Baselines, of TensorFlow or PyTorch, and\nof MuJoCo all need to line up before you can see beautiful images of moving arms,\nlegs and jumping humanoids. Unfortunately not all of these versions are backwards-\ncompatible, speciﬁcally the switch from Python 2 to 3 and from TensorFlow 1 to 2\nintroduced incompatible language changes.\nGetting everything to work may be an eﬀort, and may require switching ma-\nchines, operating systems and languages, but you should really try. This is the\n13\nTutorial:\nhttps://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#\nderiving-the-simplest-policy-gradient\n14 TensorFlow: https://github.com/openai/spinningup/blob/master/spinup/examples/\ntf1/pg_math/1_simple_pg.py\n15\nPyTorch:\nhttps://github.com/openai/spinningup/blob/master/spinup/examples/\npytorch/pg_math/1_simple_pg.py\n4.3 Locomotion and Visuo-Motor Environments\n115\ndisadvantage of being part of one of the fastest moving ﬁelds in machine learning\nresearch. If things do not work with your current operating system and Python\nversion, in general a combination of Linux Ubuntu (or macOS), Python 3.7, Ten-\nsorFlow 1 or PyTorch, Gym, and the Baselines may be a good idea to start with.\nSearch on the GitHub repositories or Stackoverﬂow when you get error messages.\nSometimes downgrading to the one-but latest version will be necessary, or ﬁddling\nwith include or library paths.\nIf everything works, then both Spinning up and the Baselines provide convenient\nscripts that facilitate mixing and matching algorithms and environments from the\ncommand line.\nFor example, to run Spinup’s PPO on MuJoCo’s Walker environment, with a\n32 × 32 hidden layer, the following command line does the job:\npython -m spinup.run ppo\n--hid \"[32,32]\" --env Walker2d-v2 --exp_name mujocotest\nTo train DDPG from the Baselines on the Half-cheetah, the command is:\npython -m base-lines.run\n--alg=ddpg --env=HalfCheetah-v2 --num_timesteps=1e6\nAll hyperparameters can be controlled via the command line, providing for a\nﬂexible way to run experiments. A ﬁnal example command line:\npython scripts/all_plots.py\n-a ddpg -e HalfCheetah Ant Hopper Walker2D -f logs/\n-o logs/ddpg_results\nThe Stable Baselines site explains what this command line does.\n4.3 Locomotion and Visuo-Motor Environments\nWe have seen many diﬀerent policy-based reinforcement learning algorithms that\ncan be used in agents with continuous action spaces. Let us have a closer look at\nthe environments that they have been used in, and how well they perform.\nPolicy-based methods, and especially the actor critic policy/value hybrid, work\nwell for many problems, both with discrete and with continuous action spaces.\nPolicy-based methods are often tested on complex high-dimensional robotics appli-\ncations [417]. Let us have a look at the kind of environments that have been used\nto develop PPO, A3C, and the other algorithms.\nTwo application categories are robot locomotion, and visuo-motor interaction.\nThese two problems have drawn many researchers, and many new algorithms have\n116\n4 Policy-Based Reinforcement Learning\nFig. 4.6 Humanoid Standing Up [682]\nbeen devised, some of which were able to learn impressives performance. For each\nof the two problems, we will discuss a few results in more detail.\n4.3.1 Locomotion\nOne of the problems of locomotion of legged entities is the problem of learning gaits.\nHumans, with two legs, can walk, run, and jump, amongst others. Dogs and horses,\nwith four legs, have other gaits, where their legs may move in even more interesting\npatterns, such as the trot, canter, pace and gallop. The challenges that we pose\nrobots are often easier. Typical reinforcement learning tasks are for a one-legged\nrobot to learn to jump, for biped robots to walk and jump, and for a quadruped to\nget to learn to use its multitude of legs in any coordinated fashion that results in\nforward moving. Learning such policies can be quite computationally expensive,\nand a curious simulated virtual animal has emerged that is cheaper to simulate:\nthe two-legged half-cheetah, whose task it is to run forward. We have already seen\nsome of these robotic creatures in Figs. 4.2–4.3.\nThe ﬁrst approach that we will discuss is by Schulman et al. [682]. They report\nexperiments where human-like bipeds and quadrupeds must learn to stand up and\nlearn running gaits. These are challenging 3D locomotion tasks that were formerly\nattempted with hand-crafted policies. Figure 4.6 shows a sequence of states.\nThe challenge in these situations is actually somewhat spectacular: the agent is\nonly provided with a positive reward for moving forward; based on nothing more\nit has to learn to control all its limbs by itself, through trial and error; no hint is\ngiven on how to control a leg or what its purpose is. These results are best watched\nin the movies that have been made16 about the learning process.\n16 Such as the movie from the start of this chapter: https://www.youtube.com/watch?v=hx_\nbgoTF7bs\n4.3 Locomotion and Visuo-Motor Environments\n117\nFig. 4.7 Walker Obstacle Course [325]\nFig. 4.8 Quadruped Obstacle Course [325]\nThe authors use an Advantage actor critic algorithm with trust regions. The\nalgorithm is fully model-free, and learning with simulated physics was reported\nto take one to two weeks of real time. Learning to walk is quite a complicated\nchallenge, as the movies illustrate. They also show the robot learning to scale an\nobstacle run all by itself.\nIn another study, Heess et al. [325] report on end-to-end learning of complex\nrobot locomotion from pixel input to (simulated) motor-actuation. Figure 4.7 shows\nhow a walker scales an obstacle course and Fig. 4.8 shows a time lapse of how a\nquadruped traverses a course. Agents learned to run, jump, crouch and turn as\nthe environment required, without explicit reward shaping or other hand-crafted\nfeatures. For this experiment a distributed version of PPO was used. Interestingly,\nthe researchers stress that the use of a rich—varied, diﬃcult—environment helps to\npromote learning of complex behavior, that is also robust across a range of tasks.\n4.3.2 Visuo-Motor Interaction\nMost experiments in “end-to-end” learning of robotic locomotion are set up so that\nthe input is received directly from features that are derived from the states as calcu-\nlated by the simulation software. A step further towards real-world interaction is to\nlearn directly from camera pixels. We then model eye-hand coordination in visuo-\n118\n4 Policy-Based Reinforcement Learning\nFig. 4.9 DeepMind Control Suite. Top: Acrobot, Ball-in-cup, Cart-pole, Cheetah, Finger, Fish,\nHopper. Bottom: Humanoid, Manipulator, Pendulum, Point-mass, Reacher, Swimmer (6 and 15\nlinks), Walker [755]\nmotor interaction tasks, and the state of the environment has to be inferred from\ncamera or other visual means, and then be translated in joint (muscle) actuations.\nVisuo-motor interaction is a diﬃcult task, requiring many techniques to work\ntogether. Diﬀerent environments have been introduced to test algorithms. Tassa\net al. report on benchmarking eﬀorts in robot locomotion with MuJoCo [755],\nintroducing the DeepMind control suite, a suite of environments consisting of\ndiﬀerent MuJoCo control tasks (see Fig. 4.9). The authors also present baseline\nimplementations of learning agents that use A3C, DDPG and D4PG (distributional\ndistributed deep deterministic policy gradients—an algorithm that extends DDPG).\nIn addition to learning from state derived features, results are presented where\nthe agent learns from 84 × 84 pixel information, in a simulated form of visuo-motor\ninteraction. The DeepMind control suite is especially designed for further research\nin the ﬁeld [757, 511, 508, 510, 509]. Other environment suites are Meta-World [864],\nSurreal [234], RLbench [375].\nVisuo-motor interaction is a challenging problem that remains an active area of\nresearch.\n4.3.3 Benchmarking\nBenchmarking eﬀorts are of great importance in the ﬁeld [212]. Henderson et\nal. [328] published an inﬂuential study of the sensitivity of outcomes to diﬀerent\nhyperparameter settings, and the inﬂuence of non-determinism, by trying to repro-\nduce many published works in the ﬁeld. They ﬁnd large variations in outcomes,\nand in general that reproducibilty of results is problematic. They conclude that\nwithout signiﬁcance metrics and tighter standardization of experimental reporting,\nit is diﬃcult to determine whether improvements over the prior state-of-the-art are\n4.3 Locomotion and Visuo-Motor Environments\n119\nmeaningful [328]. Further studies conﬁrmed these ﬁndings [7],17 and today more\nworks are being published with code, hyperparameters, and environments.\nTaking inspiration from the success of the Arcade Learning Environment in\ngame playing, benchmark suites of continuous control tasks with high state and\naction dimensionality have been introduced [212, 261].18 The tasks include 3D\nhumanoid locomotion, tasks with partial observations, and tasks with hierarchical\nstructure. The locomotion tasks are: Swimmer, Hopper, Walker, Half-cheetah, Ant,\nsimple Humanoid and full Humanoid, with the goal being to move forward as\nfast as possible. These are diﬃcult tasks because of the high degree of freedom of\nmovement. Partial observation is achieved by adding noise or leaving out certain\nparts of the regular observations. The hierarchical tasks consist of low level tasks\nsuch as learning to move, and a high level task such as ﬁnding the way out of a\nmaze.\nSummary and Further Reading\nThis chapter is concerned with the second kind of model-free algorithms: policy-\nbased methods. We summarize what we have learned, and provide pointers to\nfurther reading.\nSummary\nPolicy-based model-free methods are some of the most popular methods of deep\nreinforcement learning. For large, continuous action spaces, indirect value-based\nmethods are not well suited, because of the use of the arg max function to recover\nthe best action to go with the value. Where value-based methods work step-by-step,\nvanilla policy-based methods roll out a full future trajectory or episode. Policy-based\nmethods work with a parameterized current policy, which is well suited for a neural\nnetwork as policy function approximator.\nAfter the full trajectory has been rolled out, the reward and the value of the\ntrajectory is calculated and the policy parameters are updated, using gradient ascent.\nSince the value is only known at the end of an episode, classic policy-based methods\nhave a higher variance than value based methods, and may converge to a local\noptimum. The best known classic policy method is called REINFORCE.\nActor critic methods add a value network to the policy network, to achieve\nthe beneﬁts of both approaches. To reduce variance, 𝑛-step temporal diﬀerence\nbootstrapping can be added, and a baseline value can be subtracted, so that we get\nthe so-called advantage function (which subtracts the value of the parent state from\n17 https://github.com/google-research/rliable\n18 The suite in the paper is called RLlab. A newer version of the suite is named Garage. See also\nAppendix C.\n120\n4 Policy-Based Reinforcement Learning\nthe action values of the future states, bringing their expected value closer to zero).\nWell known actor critic methods are A3C, DDPG, TRPO, PPO, and SAC.19 A3C\nfeatures an asynchronous (parallel, distributed) implementation, DDPG is an actor\ncritic version of DQN for continous action spaces, TRPO and PPO use trust regions\nto achieve adaptive step sizes in non linear spaces, SAC optimizes for expected value\nand entropy of the policy. Benchmark studies have shown that the performance of\nthese actor critic algorithm is as good or better than value-based methods [212, 328].\nRobot learning is among the most popular applications for policy-based method.\nModel-free methods have low sample eﬃciency, and to prevent the cost of wear after\nmillions of samples, most experiments use a physics simulation as environment,\nsuch as MuJoCo. Two main application areas are locomotion (learning to walk,\nlearning to run) and visuo-motor interaction (learning directly from camera images\nof one’s own actions).\nFurther Reading\nPolicy-based methods have been an active research area for some time. Their natural\nsuitability for deep function approximation for robotics applications and other ap-\nplications with continuous action spaces has spurred a large interest in the research\ncommunity. The classic policy-based algorithm is Williams’ REINFORCE [844],\nwhich is based on the policy gradient theorem, see [744]. Our explanation is based\non [220, 192, 255]. Joining policy and value-based methods as we do in actor critic is\ndiscussed in Barto et al. [57]. Mnih et al. [521] introduce a modern eﬃcient parallel\nimplementation named A3C. After the success of DQN a version for the continu-\nous action space of policy-based methods was introduced as DDPG by Lillicrap et\nal. [480]. Schulman et al. have worked on trust regions, yielding eﬃcient popular\nalgorithms TRPO [681] and PPO [683].\nImportant benchmark studies of policy-based methods are Duan et al. [212] and\nHenderson et al. [328]. These papers have stimulated reproducibility in reinforce-\nment learning research.\nSoftware environments that are used in testing policy-based methods are Mu-\nJoCo [780] and PyBullet [167]. Gym [108] and the DeepMind control suite [755]\nincorporate MuJoCo and provide an easy to use Python interface. An active research\ncommunity has emerged around the DeepMind control suite.\nExercises\nWe have come to the end of this chapter, and it is time to test our understanding\nwith questions, exercises, and a summary.\n19 Asynchronous advantage actor critic; Deep deterministic policy gradients; Trust region policy\noptimization; Proximal policy optimization; Soft actor critic.\n4.3 Locomotion and Visuo-Motor Environments\n121\nQuestions\nBelow are some quick questions to check your understanding of this chapter. For\neach question a simple, single sentence answer is suﬃcient.\n1. Why are value-based methods diﬃcult to use in continuous action spaces?\n2. What is MuJoCo? Can you name a few example tasks?\n3. What is an advantage of policy-based methods?\n4. What is a disadvantage of full-trajectory policy-based methods?\n5. What is the diﬀerence between actor critic and vanilla policy-based methods?\n6. How many parameter sets are used by actor critic? How can they be represented\nin a neural network?\n7. Describe the relation between Monte Carlo REINFORCE, 𝑛-step methods, and\ntemporal diﬀerence bootstrapping.\n8. What is the advantage function?\n9. Describe a MuJoCo task that methods such as PPO can learn to perform well.\n10. Give two actor critic approaches to further improve upon bootstrapping and\nadvantage functions, that are used in high-performing algorithms such as PPO\nand SAC.\n11. Why is learning robot actions from image input hard?\nExercises\nLet us now look at programming exercises. If you have not already done so, install\nMuJoCo or PyBullet, and install the DeepMind control suite.20 We will use agent\nalgorithms from the Stable baselines. Furthermore, browse the examples directory\nof the DeepMind control suite on GitHub, and study the Colab notebook.\n1. REINFORCE Go to the Medium blog21 and reimplement REINFORCE. You can\nchoose PyTorch, or TensorFlow/Keras, in which case you will have to improvise.\nRun the algorithm on an environment with a discrete action space, and compare\nwith DQN. Which works better? Run in an environment with a continuous action\nspace. Note that Gym oﬀers a discrete and a continuous version of Mountain\nCar.\n2. Algorithms Run REINFORCE on a Walker environment from the Baselines. Run\nDDPG, A3C, and PPO. Run them for diﬀerent time steps. Make plots. Com-\npare training speed, and outcome quality. Vary hyperparameters to develop an\nintuition for their eﬀect.\n3. Suite Explore the DeepMind control suite. Look around and see what environ-\nments have been provided, and how you can use them. Consider extending an\nenvironment. What learning challenges would you like to introduce? First do\n20 https://github.com/deepmind/dm_control\n21\nhttps://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-\npytorch-df1383ea0baf\n122\n4 Policy-Based Reinforcement Learning\na survey of the literature that has been published about the DeepMind control\nsuite.\nChapter 5\nModel-Based Reinforcement Learning\nThe previous chapters discussed model-free methods, and we saw their success\nin video games and simulated robotics. In model-free methods the agent updates\na policy directly from the feedback that the environment provides on its actions.\nThe environment performs the state transitions and calculates the reward. A dis-\nadvantage of deep model-free methods is that they can be slow to train; for stable\nconvergence or low variance often millions of environment samples are needed\nbefore the policy function converges to a high quality optimum.\nIn contrast, with model-based methods the agent ﬁrst builds its own internal\ntransition model from the environment feedback. The agent can then use this local\ntransition model to ﬁnd out about the eﬀect of actions on states and rewards. The\nagent can use a planning algorithm to play what-if games, and generate policy\nupdates, all without causing any state changes in the environment. This approach\npromises higher quality at lower sample complexity. Generating policy updates\nfrom the internal model is called planning or imagination.\nModel-based methods update the policy indirectly: the agent ﬁrst learns a local\ntransition model from the environment, which the agent then uses to update the\npolicy. Indirectly learning the policy function has two consequences. On the positive\nside, as soon as the agent has its own model of the state transitions of the world, it\ncan learn the best policy for free, without further incurring the cost of acting in\nthe environment. Model-based methods thus may have a lower sample complexity.\nThe downside is that the learned transition model may be inaccurate, and the\nresulting policy may be of low quality. No matter how many samples can be taken\nfor free from the model, if the agent’s local transition model does not reﬂect the\nenvironment’s real transition model, then the locally learned policy function will\nnot work in the environment. Thus, dealing with uncertainty and model bias are\nimportant elements in model-based reinforcement learning.\nThe idea to ﬁrst learn an internal representation of the environment’s transition\nfunction has been conceived many years ago, and transition models have been\nimplemented in many diﬀerent ways. Models can be tabular, or they can be based\non various kinds of deep learning, as we will see.\n123\n124\n5 Model-Based Reinforcement Learning\nThis chapter will start with an example showing how model-based methods\nwork. Next, we describe in more detail diﬀerent kinds of model-based approaches;\napproaches that focus on learning an accurate model, and approaches for planning\nwith an imperfect model. Finally, we describe application environments for which\nmodel-based methods have been used in practice, to see how well the approaches\nperform.\nThe chapter is concluded with exercises, a summary, and pointers to further\nreading.\nCore Concepts\n•\nImagination\n•\nUncertainty models\n•\nWorld models, Latent models\n•\nModel-predictive control\n•\nDeep end-to-end planning and learning\nCore Problem\n•\nLearn and use accurate transition models for high-dimensional problems\nCore Algorithms\n•\nDyna-Q (Alg. 5.3)\n•\nEnsembles and model-predictive control (Alg. 5.4, 5.6)\n•\nValue prediction networks (Alg. 5.5)\n•\nValue iteration networks (Sect. 5.2.2.2)\nBuilding a Navigation Map\nTo illustrate basic concepts of model-based reinforcement learning, we return to\nthe supermarket example.\nLet us compare how model-free and model-based methods ﬁnd their way to the\nsupermarket in a new city.1 In this example we will use value-based Q-learning;\n1 We use distance to the supermarket as negative reward, in order to formulate this as a distance\nminimization problem, while still being able to reason in our familiar reward maximization setting.\n5 Model-Based Reinforcement Learning\n125\nour policy 𝜋(𝑠, 𝑎) will be derived directly from the 𝑄(𝑠, 𝑎) values with arg max,\nand writing 𝑄is in this sense equivalent to writing 𝜋.\nModel-free Q-learning: the agent picks the start state 𝑠0, and uses (for example)\nan 𝜖-greedy behavior policy on the action-value function 𝑄(𝑠, 𝑎) to select the next\naction. The environment then executes the action, computes the next state 𝑠′ and\nreward 𝑟, and returns these to the agent. The agent updates its action-value function\n𝑄(𝑠, 𝑎) with the familiar update rule\n𝑄(𝑠, 𝑎) ←𝑄(𝑠, 𝑎) + 𝛼[𝑟+ 𝛾max\n𝑎\n𝑄(𝑠′, 𝑎) −𝑄(𝑠, 𝑎)].\nThe agent repeats this procedure until the values in the 𝑄-function no longer change\ngreatly.\nThus we pick our start location in the city, perform one walk along a block in an\n𝜖-greedy direction, and record the reward and the new state at which we arrive.\nWe use the information to update the policy, and from our new location, we walk\nagain in an 𝜖-greedy direction using the policy. If we ﬁnd the supermarket, we start\nover again, trying to ﬁnd a shorter path, until our policy values no longer change\n(this may take many environment interactions). Then the best policy is the path\nwith the shortest distances.\nModel-based planning and learning: the agent uses the 𝑄(𝑠, 𝑎) function as behav-\nior policy as before to sample the new state and reward from the environment, and\nto update the policy (𝑄-function). In addition, however, the agent will record the\nnew state and reward in a local transition 𝑇𝑎(𝑠, 𝑠′) and reward function 𝑅𝑎(𝑠, 𝑠′).\nBecause the agent now has these local entries we can also sample from our local\nfunctions to update the policy. We can choose: sample from the (expensive) envi-\nronment transition function, or from the (cheap) local transition function. There\nis a caveat with sampling locally, however. The local functions may contain fewer\nentries—or only high variance entries—especially in the early stages, when few\nenvironment samples have been performed. The usefulness of the local functions\nincreases as more environment samples are performed.\nThus, we now have a local map on which to record the new states and rewards.\nWe will use this map to peek, as often as we like and at no cost, at a location on\nthat map, to update the 𝑄-function. As more environment samples come in, the\nmap will have more and more locations for which a distance to the supermarket is\nrecorded. When glances at the map do not improve the policy anymore, we have to\nwalk in the environment again, and, as before, update the map and the policy.\nIn conclusion, model-free ﬁnds all policy updates outside the agent, from the\nenvironment feedback; model-based also2 uses policy updates from within the agent,\nusing information from its local map (see Fig. 5.1). In both methods all updates\nto the policy are ultimately derived from the environment feedback; model-based\noﬀers a diﬀerent way to use the information to update the policy, a way that may\n2 One option is to only update the policy from the agent’s internal transition model, and not by the\nenvironment samples anymore. However, another option is to keep using the environment samples\nto also update the policy in the model-free way. Sutton’s Dyna [741] approach is a well-known\nexample of this last, hybrid, approach. Compare also Fig. 5.2 and Fig. 5.4.\n126\n5 Model-Based Reinforcement Learning\nValue/Policy\nEnvironment\nModel\nacting\nmodel learning\nplanning\ndirect RL\nFig. 5.1 Direct and Indirect Reinforcement Learning [743]\nbe more information-eﬃcient, by keeping information from each sample within\nthe agent transition model and re-using that information.\n5.1 Dynamics Models of High-Dimensional Problems\nThe application environments for model-based reinforcement learning are the same\nas for model-free; our goal, however, is to solve larger and more complex problems\nin the same amount of time, by virtue of the lower sample complexity and, as it\nwere, a deeper understanding of the environment.\nTransition Model and Knowledge Transfer\nThe principle of model-based learning is as follows. Where model-free methods\nsample the environment to learn the state-to-action policy function 𝜋(𝑠, 𝑎) based\non action rewards, model-based methods sample the environment to learn the state-\nto-state transition function 𝑇𝑎(𝑠, 𝑠′) based on action rewards. Once the accuracy of\nthis local transition function is good enough, the agent can sample from this local\nfunction to improve the policy 𝜋(𝑠, 𝑎) as often as it likes, without incurring the\ncost of actual environment samples. In the model-based approach, the agent builds\nits own local state-to-state transition (and reward) model of the environment, so\nthat, in theory at least, it does not need the environment anymore.\nThis brings us to another reason for the interest in model-based methods. For\nsequential decision problems, knowing the transition function is a natural way of\ncapturing the essence of how the environment works—𝜋gives the next action, 𝑇\ngives the next state.\nThis is useful, for example, when we switch to a related environment. When\nthe transition function of the environment is known by the agent, then the agent\ncan be adapted quickly, without having to learn a whole new policy by sampling\nthe environment. When a good local transition function of the domain is known\nby the agent, then new, but related, problems might be solved eﬃciently. Hence,\n5.2 Learning and Planning Agents\n127\nmodel-based reinforcement learning may contribute to eﬃcient transfer learning\n(see Chap. 9).\nSample Eﬃciency\nThe sample eﬃciency of an agent algorithm tells us how many environment samples\nit needs for the policy to reach a certain accuracy.\nTo achieve high sample eﬃciency, model-based methods learn a dynamics model.\nLearning high-accuracy high-capacity models of high-dimensional problems re-\nquires a high number of training examples, to prevent overﬁtting (see Sect. B.2.7).\nThus, reducing overﬁtting in learning the transition model would negate (some of)\nthe advantage of the low sample complexity that model-based learning of the policy\nfunction achieves. Constructing accurate deep transition models can be diﬃcult in\npractice, and for many complex sequential decision problems the best results are\noften achieved with model-free methods, although deep model-based methods are\nbecoming stronger (see, for example, Wang et al. [828]).\n5.2 Learning and Planning Agents\nThe promise of model-based reinforcement learning is to ﬁnd a high-accuracy\nbehavior policy at a low cost, by building a local model of the world. This will only\nwork if the learned transition model provides accurate predictions, and if the extra\ncost of planning with the model is reasonable.\nLet us see which solutions have been developed for deep model-based reinforce-\nment learning. In Sect. 5.3 we will have a closer look at the performance in diﬀerent\nenvironments. First, in this section, we will look at four diﬀerent algorithmic ap-\nproaches, and at a classic approach: Dyna’s tabular imagination.\nTabular Imagination\nA classic approach is Dyna [741], which popularized the idea of model-based re-\ninforcement learning. In Dyna, environment samples are used in a hybrid model-\nfree/model-based manner, to train the transition model, use planning to improve\nthe policy, while also training the policy function directly.\nWhy is Dyna a hybrid approach? Strict model-based methods update the policy\nonly by planning using the agent’s transition model, see Alg. 5.1. In Dyna, however,\nenvironment samples are used to also update the policy directly (see Fig. 5.2 and\nAlg. 5.2). Thus we get a hybrid approach combining model-based and model-free\nlearning. This hybrid model-based planning is called imagination because looking\nahead with the agent’s own dynamics model resembles imagining environment\n128\n5 Model-Based Reinforcement Learning\nAlgorithm 5.1 Strict Learned Dynamics Model\nrepeat\nSample environment 𝐸to generate data 𝐷= (𝑠, 𝑎, 𝑟′, 𝑠′)\nUse 𝐷to learn 𝑀= 𝑇𝑎(𝑠, 𝑠′), 𝑅𝑎(𝑠, 𝑠′)\n⊲learning\nfor 𝑛= 1, . . . , 𝑁do\nUse 𝑀to update policy 𝜋(𝑠, 𝑎)\n⊲planning\nend for\nuntil 𝜋converges\nEnvironment\nDynamics Model\nPolicy/Value\nlearning\nlearning\nacting\nplanning\nFig. 5.2 Hybrid Model-Based Imagination\nAlgorithm 5.2 Hybrid Model-Based Imagination\nrepeat\nSample env 𝐸to generate data 𝐷= (𝑠, 𝑎, 𝑟′, 𝑠′)\nUse 𝐷to update policy 𝜋(𝑠, 𝑎)\n⊲learning\nUse 𝐷to learn 𝑀= 𝑇𝑎(𝑠, 𝑠′), 𝑅𝑎(𝑠, 𝑠′)\n⊲learning\nfor 𝑛= 1, . . . , 𝑁do\nUse 𝑀to update policy 𝜋(𝑠, 𝑎)\n⊲planning\nend for\nuntil 𝜋converges\nsamples outside the real environment inside the “mind” of the agent. In this approach\nthe imagined samples augment the real (environment) samples at no sample cost.3\nImagination is a mix of model-based and model-free reinforcement learning.\nImagination performs regular direct reinforcement learning, where the environment\nis sampled with actions according to the behavior policy, and the feedback is used\nto update the same behavior policy. Imagination also uses the environment sample\nto update the dynamics model {𝑇𝑎, 𝑅𝑎}. This extra model is also sampled, and\nprovides extra updates to the behavior policy, in between the model-free updates.\nThe diagram in Fig. 5.2 shows how sample feedback is used both for updating\nthe policy directly and for updating the model, which then updates the policy, by\nplanning “imagined” feedback. In Alg. 5.2 the general imagination approach is\nshown as pseudocode.\nSutton’s Dyna-Q [741, 743], which is shown in more detail in Alg. 5.3, is a\nconcrete implementation of the imagination approach. Dyna-Q uses the Q-function\n3 The term imagination is used somewhat loosely in the ﬁeld. In a strict sense imagination refers\nonly to updating the policy from the internal model by planning. In a wider sense imagination\nrefers to hybrid schemes where the policy is updated from both the internal model and the\nenvironment. Sometimes the term dreaming is used for agents imagining environments.\n5.2 Learning and Planning Agents\n129\nAlgorithm 5.3 Dyna-Q [741]\nInitialize 𝑄(𝑠, 𝑎) →R randomly\nInitialize 𝑀(𝑠, 𝑎) →R × 𝑆randomly\n⊲Model\nrepeat\nSelect 𝑠∈𝑆randomly\n𝑎←𝜋(𝑠)\n⊲𝜋(𝑠) can be 𝜖-greedy(𝑠) based on 𝑄\n(𝑠′, 𝑟) ←𝐸(𝑠, 𝑎)\n⊲Learn new state and reward from environment\n𝑄(𝑠, 𝑎) ←𝑄(𝑠, 𝑎) + 𝛼· [𝑟+ 𝛾· max𝑎′ 𝑄(𝑠′, 𝑎′) −𝑄(𝑠, 𝑎)]\n𝑀(𝑠, 𝑎) ←(𝑠′, 𝑟)\nfor 𝑛= 1, . . . , 𝑁do\nSelect ˆ𝑠and ˆ𝑎randomly\n(𝑠′, 𝑟) ←𝑀(ˆ𝑠, ˆ𝑎)\n⊲Plan imagined state and reward from model\n𝑄(ˆ𝑠, ˆ𝑎) ←𝑄(ˆ𝑠, ˆ𝑎) + 𝛼· [𝑟+ 𝛾· max𝑎′ 𝑄(𝑠′, 𝑎′) −𝑄(ˆ𝑠, ˆ𝑎)]\nend for\nuntil 𝑄converges\nas behavior policy 𝜋(𝑠) to perform 𝜖-greedy sampling of the environment. It then\nupdates this policy with the reward, and an explicit model 𝑀. When the model 𝑀\nhas been updated, it is used 𝑁times by planning with random actions to update\nthe Q-function. The pseudocode shows the learning steps (from environment 𝐸)\nand 𝑁planning steps (from model 𝑀). In both cases the Q-function state-action\nvalues are updated. The best action is then derived from the Q-values as usual.\nThus, we see that the number of updates to the policy can be increased without\nmore environment samples. By choosing the value for 𝑁, we can tune how many\nof the policy updates will be environment samples, and how many will be model\nsamples. In the larger problems that we will see later in this chapter, the ratio\nof environment-to-model samples is often set at, for example, 1 : 1000, greatly\nreducing sample complexity. The questions then become, of course: how good is the\nmodel, and: how far is the resulting policy from a model-free baseline?\nHands On: Imagining Taxi Example\nIt is time to illustrate how Dyna-Q works with an example. For that, we turn to one\nof our favorites, the Taxi world.\nLet us see what the eﬀect of imagining with a model can be. Please refer to\nFig. 5.3. We use our simple maze example, the Taxi maze, with zero imagination\n(𝑁= 0), and with large imagination (𝑁= 50). Let us assume that the reward at all\nstates returned by the environment is 0, except for the goal, where the reward is +1.\nIn states the usual actions are present (north, east, west, south), except at borders\nor walls.\nWhen 𝑁= 0 Dyna-Q performs exactly Q-learning, randomly sampling action\nrewards, building up the Q-function, and using the Q-values following the 𝜖-greedy\npolicy for action selection. The purpose of the Q-function is to act as a vessel\nof information to ﬁnd the goal. How does our vessel get ﬁlled with information?\nSampling starts oﬀrandomly, and the Q-values ﬁll slowly, since the reward landscape\n130\n5 Model-Based Reinforcement Learning\nFig. 5.3 Taxi world [395]\nis ﬂat, or sparse: only the goal state returns +1, all other states return 0. In order\nto ﬁll the Q-values with actionable information on where to ﬁnd the goal, ﬁrst the\nalgorithm must be lucky enough to choose a state next to the goal, including the\nappropriate action to reach the goal. Only then the ﬁrst useful reward information\nis found and the ﬁrst non-zero step towards ﬁnding the goal can be entered into\nthe Q-function. We conclude that, with 𝑁= 0, the Q-function is ﬁlled up slowly,\ndue to sparse rewards.\nWhat happens when we turn on planning? When we set 𝑁to a high value, such\nas 50, we perform 50 planning steps for each learning step. As we can see in the\nalgorithm, the model is built alongside the 𝑄-function, from environment returns.\nAs long as the 𝑄-function is still fully zero, then planning with the model will also\nbe useless. But as soon as one goal entry is entered into 𝑄and 𝑀, then planning\nwill start to shine: it will perform 50 planning samples on the M-model, probably\nﬁnding the goal information, and possibly building up an entire trajectory ﬁlling\nstates in the 𝑄-function with actions towards the goal.\nIn a way, the model-based planning ampliﬁes any useful reward information\nthat the agent has learned from the environment, and plows it back quickly into\nthe policy function. The policy is learned much quicker, with fewer environment\nsamples.\nReversible Planning and Irreversible Learning\nModel-free methods sample the environment and learn the policy function 𝜋(𝑠, 𝑎)\ndirectly, in one step. Model-based methods sample the environment to learn the\npolicy indirectly, using a dynamics model {𝑇𝑎, 𝑅𝑎} (as we see in Fig. 5.1 and 5.4,\nand in Alg. 5.1).\n5.2 Learning and Planning Agents\n131\nEnvironment\nDynamics Model\nPolicy/Value\nlearning\nacting\nplanning\nEnvironment\nPolicy/Value\nlearning\nacting\nFig. 5.4 Model-based (left) and Model-free (right). Learning changes the environment state\nirreversibly (single arrow); planning changes the agent state reversibly (undo, double arrow)\nPlanning\nLearning\nTransition model in: Agent\nEnvironment\nAgent can Undo:\nYes\nNo\nState is:\nReversible by agent\nIrreversible by agent\nDynamics:\nBacktrack\nForward only\nData structure:\nTree\nPath\nNew state:\nIn agent\nSample from environment\nReward:\nBy agent\nSample from environment\nSynonyms:\nImagination, simulation Sampling, rollout\nTable 5.1 Diﬀerence between Planning and Learning\nIt is useful to step back for a moment to consider the place of learning and\nplanning algorithms in the reinforcement learning paradigm. Please refer to Table 5.1\nfor a summary of diﬀerences between planning and learning.\nPlanning with an internal transition model is reversible. When the agent uses\nits own transition model to perform local actions on a local state, then the actions\ncan be undone, since the agent applied them to a copy in its own memory [528].4\nBecause of this local state memory, the agent can return to the old state, reversing\nthe local state change caused by the local action that it has just performed. The\nagent can then try an alternative action (which it can also reverse). The agent can\nuse tree-traversal methods to traverse the state space, backtracking to try other\nstates.\nIn contrast to planning, learning is done when the agent does not have access\nto its own transition function 𝑇𝑎(𝑠, 𝑠′). The agent can get reward information by\nsampling real actions in the environment. These actions are not played out inside\nthe agent but executed in the actual environment; they are irreversible and can not\nbe undone by the agent. Learning uses actions that irreversibly change the state of\nthe environment. Learning does not permit backtracking; learning algorithms learn\na policy by repeatedly sampling the environment.\nNote the similarity between learning and planning: learning samples rewards\nfrom the external environment, planning from the internal model; both use the\nsamples to update the policy function 𝜋(𝑠, 𝑎).\n4 In our dreams we can undo our actions, play what-if, and imagine alternative realities.\n132\n5 Model-Based Reinforcement Learning\nFour Types of Model-Based Methods\nIn model-based reinforcement learning the challenge is to learn deep, high-dimen-\nsional transition models from limited data. Our methods should be able to account\nfor model uncertainty, and plan over these models to achieve policy and value\nfunctions that perform as well or better than model-free methods. Let us look in\nmore detail at speciﬁc model-based reinforcement learning methods to see how this\ncan be achieved.\nOver the years, many diﬀerent approaches for high-accuracy high-dimensional\nmodel-based reinforcement learning have been devised. Following [601], we group\nthe methods into four main approaches. We start with two approaches for learning\nthe model, and then two approaches for planning, using the model. For each we will\ntake a few representative papers from the literature that we describe in more depth.\nAfter we have done so, we will look at their performance in diﬀerent environments.\nBut let us start with the methods for learning a deep model ﬁrst.\n5.2.1 Learning the Model\nIn model-based approaches, the transition model is learned from sampling the\nenvironment. If this model is not accurate, then planning will not improve the value\nor policy function, and the method will perform worse than model-free methods.\nWhen the learning/planning ratio is set to 1/1000, as it is in some experiments,\ninaccuracy in the models will reveal itself quickly in a low accuracy policy function.\nMuch research has focused on achieving high accuracy dynamics models for\nhigh-dimensional problems. Two methods to achieve better accuracy are uncertainty\nmodeling and latent models. We will start with uncertainty modeling.\n5.2.1.1 Modeling Uncertainty\nThe variance of the transition model can be reduced by increasing the number\nof environment samples, but there are also other approaches that we will discuss.\nA popular approach for smaller problems is to use Gaussian processes, where\nthe dynamics model is learned by giving an estimate of the function and of the\nuncertainty around the function with a covariance matrix on the entire dataset [93].\nA Gaussian model can be learned from few data points, and the transition model can\nbe used to plan the policy function successfully. An example of this approach is the\nPILCO system, which stands for Probabilistic Inference for Learning Control [188,\n189]. This system was eﬀective on Cartpole and Mountain car, but does not scale to\nlarger problems.\nWe can also sample from a trajectory distribution optimized for cost, and use\nthat to train the policy, with a policy-based method [469]. Then we can optimize\npolicies with the aid of locally-linear models and a stochastic trajectory optimizer.\n5.2 Learning and Planning Agents\n133\nAlgorithm 5.4 Planning with an Ensemble of Models [439]\nInitialize policy 𝜋𝜃and the models ˆ𝑚𝜙1, ˆ𝑚𝜙1, . . . , ˆ𝑚𝜙𝐾\n⊲ensemble\nInitialize an empty dataset 𝐷\nrepeat\n𝐷←sample with 𝜋𝜃from environment 𝐸\nLearn models ˆ𝑚𝜙1, ˆ𝑚𝜙1, . . . , ˆ𝑚𝜙𝐾using 𝐷\n⊲ensemble\nrepeat\n𝐷′ ←sample with 𝜋𝜃from { ˆ𝑚𝜙𝑖}𝐾\n𝑖=1\nUpdate 𝜋𝜃with TRPO using 𝐷′\n⊲planning\nEstimate performance of trajectories ˆ𝜂𝜏(𝜃, 𝜙𝑖) for 𝑖= 1, . . . , 𝐾\nuntil performance converges\nuntil 𝜋𝜃performs well in environment 𝐸\nThis is the approach that is used in Guided policy search (GPS), which been shown\nto train complex policies with thousands of parameters, learning tasks in MuJoCo\nsuch as Swimming, Hopping and Walking.\nAnother popular method to reduce variance in machine learning is the ensemble\nmethod. Ensemble methods combine multiple learning algorithms to achieve better\npredictive performance; for example, a random forest of decision trees often has\nbetter predictive performance than a single decision tree [93, 574]. In deep model-\nbased methods the ensemble methods are used to estimate the variance and account\nfor it during planning. A number of researchers have reported good results with\nensemble methods on larger problems [156, 376]. For example, Chua et al. use an\nensemble of probabilistic neural network models [149] in their approach named\nProbabilistic ensembles with trajectory sampling (PETS). They report good results\non high-dimensional simulated robotic tasks (such as Half-cheetah and Reacher).\nKurutach et al. [439] combine an ensemble of models with TRPO, in ME-TRPO.5\nIn ME-TRPO an ensemble of deep neural networks is used to maintain model\nuncertainty, while TRPO is used to control the model parameters. In the planner,\neach imagined step is sampled from the ensemble predictions (see Alg. 5.4).\nUncertainty modeling tries to improve the accuracy of high-dimensional mod-\nels by probabilistic methods. A diﬀerent approach, speciﬁcally designed for high-\ndimensional deep models, is the latent model approach, which we will discuss\nnext.\n5.2.1.2 Latent Models\nLatent models focus on dimensionality reduction of high-dimensional problems.\nThe idea behind latent models is that in most high-dimensional environments some\nelements are less important, such as buildings in the background that never move\nand that have no relation with the reward. We can abstract these unimportant\nelements away from the model, reducing the eﬀective dimensionality of the space.\n5 A video is available at https://sites.google.com/view/me-trpo. The code is at https:\n//github.com/thanard/me-trpo. A blog post is at [362].\n134\n5 Model-Based Reinforcement Learning\nLatent models do so by learning to represent the elements of the input and the\nreward. Since planning and learning are now possible in a lower-dimensional latent\nspace, the sampling complexity of learning from the latent models improves.\nEven though latent model approaches are often complicated designs, many works\nhave been published that show good results [391, 309, 308, 688, 710, 310, 304]. Latent\nmodels use multiple neural networks, as well as diﬀerent learning and planning\nalgorithms.\nTo understand this approach, we will brieﬂy discuss one such latent-model\napproach: the Value prediction network (VPN) by Oh et al. [564].6 VPN uses four\ndiﬀerentiable functions, that are trained to predict the value [290], Fig. 5.5 shows\nhow the core functions. The core idea in VPN is not to learn directly in the actual\nobservation space, but ﬁrst to transform the state respresentations to a smaller\nlatent representation model, also known as abstract model. The other functions,\nsuch as value, reward, and next-state, then work on these smaller latent states,\ninstead of on the more complex high-dimensional states. In this way, planning and\nlearning occur in a space where states are encouraged only to contain the elements\nthat inﬂuence value changes. Latent space is lower-dimensional, and training and\nplanning become more eﬃcient.\nThe four functions in VPN are: (1) an encoding function, (2) a reward function,\n(3) a value function, and (4) a transition function. All functions are parameterized\nwith their own set of parameters. To distinghuish these latent-based functions\nfrom the conventional observation-based functions 𝑅,𝑉,𝑇they are denoted as\n𝑓𝑒𝑛𝑐\n𝜃𝑒, 𝑓𝑟𝑒𝑤𝑎𝑟𝑑\n𝜃𝑟\n, 𝑓𝑣𝑎𝑙𝑢𝑒\n𝜃𝑣\n, 𝑓𝑡𝑟𝑎𝑛𝑠\n𝜃𝑡\n.\n•\nThe encoding function 𝑓𝑒𝑛𝑐\n𝜃𝑒\n: 𝑠𝑎𝑐𝑡𝑢𝑎𝑙→𝑠𝑙𝑎𝑡𝑒𝑛𝑡maps the observation 𝑠𝑎𝑐𝑡𝑢𝑎𝑙to\nthe abstract state using neural network 𝜃𝑒, such as a CNN for visual observations.\nThis is the function that performs the dimensionality reduction.\n•\nThe latent-reward function 𝑓𝑟𝑒𝑤𝑎𝑟𝑑\n𝜃𝑟\n: (𝑠𝑙𝑎𝑡𝑒𝑛𝑡, 𝑜) →𝑟, 𝛾maps the latent state 𝑠\nand option 𝑜(a kind of action) to the reward and discount factor. If the option\ntakes 𝑘primitive actions, the network should predict the discounted sum of\nthe 𝑘immediate rewards as a scalar. (The role of options is explained in the\npaper [564].) The network also predicts option-discount factor 𝛾for the number\nof steps taken by the option.\n•\nThe latent-value function 𝑓𝑣𝑎𝑙𝑢𝑒\n𝜃𝑣\n: 𝑠𝑙𝑎𝑡𝑒𝑛𝑡→𝑉𝜃𝑣(𝑠𝑙𝑎𝑡𝑒𝑛𝑡) maps the abstract\nstate to its value using a separate neural network 𝜃𝑣. This value is the value of\nthe latent state, not of the actual observation state 𝑉(𝑠𝑎𝑐𝑡𝑢𝑎𝑙).\n•\nThe latent-transition function 𝑓𝑡𝑟𝑎𝑛𝑠\n𝜃𝑡\n: (𝑠𝑙𝑎𝑡𝑒𝑛𝑡, 𝑜) →𝑠′\n𝑙𝑎𝑡𝑒𝑛𝑡maps the latent\nstate to the next latent state, depending also on the option.\nFigure 5.5 shows how the core functions work together in the smaller, latent, space;\nwith 𝑥the observed actual state, and 𝑠the encoded latent state [564].\nThe ﬁgure shows a single rollout step, planning one step ahead. However, a\nmodel also allows looking further into the future, by performing multi-step rollouts.\n6 See https://github.com/junhyukoh/value-prediction-network for the code.\n5.2 Learning and Planning Agents\n135\nFig. 5.5 Architecture of latent model [564]\nAlgorithm 5.5 Multi-step planning [564]\nfunction Q-Plan(s, o, 𝑑)\n𝑟, 𝛾, 𝑉(𝑠′), 𝑠′ →𝑓𝑐𝑜𝑟𝑒\n𝜃\n(𝑠, 𝑜)\n⊲Perform the four latent functions\nif 𝑑= 1 then\nreturn 𝑟+ 𝛾𝑉(𝑠′)\nend if\n𝐴←𝑏-best options based on 𝑟+ 𝛾𝑉𝜃(𝑠′)\n⊲See paper for other expansion strategies\nfor 𝑜′ ∈𝐴do\n𝑞𝑜′ ←Q-Plan(𝑠′, 𝑜′, 𝑑−1)\nend for\nreturn 𝑟+ 𝛾[ 1\n𝑑𝑉(𝑠′) + 𝑑−1\n𝑑\nmax𝑜′∈𝐴𝑞𝑜′ ]\nend function\nOf course, this requires a highly accurate model, otherwise the accumulated inaccu-\nracies diminish the accuracy of the far-into-the-future lookahead. Algorithm 5.5\nshows the pseudocode for a 𝑑-step planner for the value prediction network.\nThe networks are trained with 𝑛-step Q-learning and TD search [709]. Trajec-\ntories are generated with an 𝜖-greedy policy using the planning algorithm from\nAlg. 5.5. VPN achieved good results on Atari games such as Pacman and Seaquest,\noutperforming model-free DQN, and outperforming observation-based planning in\nstochastic domains.\nAnother relevant approach is presented in a sequence of papers by Hafner et\nal. [308, 309, 310]. Their PlaNet and Dreamer approaches use latent models based\non a Recurrent State Space Model (RSSM), that consists of a transition model, an ob-\nservation model, a variational encoder and a reward model, to improve consistency\nbetween one-step and multi-step predictions in latent space [398, 121, 199].\nThe latent-model approach reduces the dimensionality of the observation space.\nDimensionality reduction is related to unsupervised learning (Sect. 1.2.2), and\nautoencoders (Sect. B.2.6). The latent-model approach is also related to world models,\na term used by Ha and Schmidhuber [303, 304]. World models are inspired by the\nmanner in which humans are thought to construct a mental model of the world in\nwhich we live. Ha et al. implement world models using generative recurrent neural\nnetworks that generate states for simulation using a variational autoencoder [411,\n412] and a recurrent network. Their approach learns a compressed spatial and\ntemporal representation of the environment. By using features extracted from the\n136\n5 Model-Based Reinforcement Learning\nworld model as inputs to the agent, a compact and simple policy can be trained to\nsolve a task, and planning occurs in the compressed world. The term world model\ngoes back to 1990, see Schmidhuber [670].\nLatent models and world models achieve promising results and are, despite their\ncomplexity, an active area of research, see, for example [873]. In the next section\nwe will further discuss the performance of latent models, but we will ﬁrst look at\ntwo methods for planning with deep transition models.\n5.2.2 Planning with the Model\nWe have discussed in some depth methods to improve the accuracy of models. We\nwill now switch from how to create deep models, to how to use them. We will\ndescribe two planning approaches that are designed to be forgiving for models that\ncontain inaccuracies. The planners try to reduce the impact of the inaccuracy of the\nmodel, for example, by planning ahead with a limited horizon, and by re-learning\nand re-planning at each step of the trajectory. We will start with planning with a\nlimited horizon.\n5.2.2.1 Trajectory Rollouts and Model-Predictive Control\nAt each planning step, the local transition model𝑇𝑎(𝑠) →𝑠′ computes the new state,\nusing the local reward to update the policy. Due to the inaccuracies of the internal\nmodel, planning algorithms that perform many steps will quickly accumulate model\nerrors [296]. Full rollouts of long, inaccurate, trajectories are therefore problematic.\nWe can reduce the impact of accumulated model errors by not planning too far\nahead. For example, Gu et al. [296] perform experiments with locally linear models\nthat roll out planning trajectories of length 5 to 10. This reportedly works well for\nMuJoCo tasks Gripper and Reacher.\nIn another experiment, Feinberg et al. [238] allow imagination to a ﬁxed look-\nahead depth, after which value estimates are split into a near-future model-based\ncomponent and a distant future model-free component (Model-based value expan-\nsion, MVE). They experiment with horizons of 1, 2, and 10, and ﬁnd that 10 generally\nperforms best on typical MuJoCo tasks such as Swimmer, Walker, and Half-cheetah.\nThe sample complexity in their experiments is better than model-free methods\nsuch as DDPG [705]. Similarly good results are reported by others [376, 393], with\na model horizon that is much shorter than the task horizon.\nModel-Predictive Control\nTaking the idea of shorter trajectories for planning than for learning further, we\narrive at decision-time planning [467], also known as Model-predictive control\n5.2 Learning and Planning Agents\n137\nAlgorithm 5.6 Neural Network Dynamics for Model-Based Deep Reinforcement\nLearning (based on [549])\nInitialize the model ˆ𝑚𝜙\nInitialize an empty dataset 𝐷\nfor 𝑖= 1, . . . , 𝐼do\n𝐷←𝐸𝑎\n⊲sample action from environment\nTrain ˆ𝑚𝜙(𝑠, 𝑎) on 𝐷minimizing the error by gradient descent\nfor 𝑡= 1, . . . , 𝑇Horizon do\n⊲planning\n𝐴𝑡←ˆ𝑚𝜙\n⊲estimate optimal action sequence with ﬁnite MPC horizon\nExecute ﬁrst action 𝑎𝑡from sequence 𝐴𝑡\n𝐷←(𝑠𝑡, 𝑎𝑡)\nend for\nend for\n(MPC) [440, 262]. Model-predictive control is a well-known approach in process\nengineering, to control complex processes with frequent re-planning over a limited\ntime horizon. Model-predictive control uses the fact that many real-world processes\nare approximately linear over a small operating range (even though they can be\nhighly non-linear over a longer range). In MPC the model is optimized for a limited\ntime into the future, and then it is re-learned after each environment step. In this\nway small errors do not get a chance to accumulate and inﬂuence the outcome\ngreatly. Related to MPC are other local re-planning methods. All try to reduce the\nimpact of the use of an inaccurate model by not planning too far into the future\nand by updating the model frequently. Applications are found in the automotive\nindustry and in aerospace, for example for terrain-following and obstacle-avoidance\nalgorithms [394].\nMPC has been used in various deep model learning approaches. Both Finn et\nal. and Ebert et al. [244, 218] use a form of MPC in the planning for their Visual\nforesight robotic manipulation system. The MPC part uses a model that generates\nthe corresponding sequence of future frames based on an image to select the least-\ncost sequence of actions. This approach is able to perform multi-object manipulation,\npushing, picking and placing, and cloth-folding tasks (which adds the diﬃculty of\nmaterial that changes shape as it is being manipulated).\nAnother approach is to use ensemble models for learning the transition model,\nwith MPC for planning. PETS [149] uses probabilistic ensembles [448] for learning,\nbased on cross-entropy-methods (CEM) [183, 99]. In MPC-fashion only the ﬁrst\naction from the CEM-optimized sequence is used, re-planning at every environment-\nstep. Many model-based approaches combine MPC and the ensemble method, as we\nwill also see in the overview in Table 5.2 at the end of the next section. Algorithm 5.6\nshows in pseudocode an example of Model-predictive control (based on [549], only\nthe model-based part is shown).7\nMPC is a simple and eﬀective planning method that is well-suited for use with\ninaccurate models, by restricting the planning horizon and by re-planning. It has\nalso been used with success in combination with latent models [309, 391].\n7 The code is at https://github.com/anagabandi/nn_dynamics.\n138\n5 Model-Based Reinforcement Learning\nIt is now time to look at the ﬁnal method, which is a very diﬀerent approach to\nplanning.\n5.2.2.2 End-to-end Learning and Planning-by-Network\nUp until now, the learning of the dynamics model and its use are performed by\nseparate algorithms. In the previous subsection diﬀerentiable transition models were\nlearned through backpropagation and then the models were used by a conventional\nhand-crafted procedural planning algorithm, such as depth-limited search, with\nhand-coded selection and backup rules.\nA trend in machine learning is to replace all hand-crafted algorithms by diﬀer-\nentiable approaches, that are trained by example, end-to-end. These diﬀerentiable\napproaches often are more general and perform better than their hand-crafted\nversions.8 We could ask the question if it would be possible to make the planning\nphase diﬀerentiable as well? Or, to see if the planning rollouts can be implemented\nin a single computational model, the neural network?\nAt ﬁrst sight, it may seem strange to think of a neural network as something\nthat can perform planning and backtracking, since we often think of a neural\nnetwork as a state-less mathematical function. Neural networks normally perform\ntransformation and ﬁlter activities to achieve selection or classiﬁcation. Planning\nconsists of action selection and state unrolling. Note, however, that recurrent neural\nnetworks and LSTMs contain implicit state, making them a candidate to be used for\nplanning (see Sect. B.2.5). Perhaps it is not so strange to try to implement planning\nin a neural network. Let us have a look at attempts to perform planning with a\nneural network.\nTamar et al. [748] introduced Value Iteration Networks (VIN), convolutional\nnetworks for planning in Grid worlds. A VIN is a diﬀerentiable multi-layer convolu-\ntional network that can execute the steps of a simple planning algorithm [562]. The\ncore idea it that in a Grid world, value iteration can be implemented by a multi-layer\nconvolutional network: each layer does a step of lookahead (refer back to Listing 2.1\nfor value iteration). The value iterations are rolled-out in the network layers 𝑆with\n𝐴channels, and the CNN architecture is shaped speciﬁcally for each problem task.\nThrough backpropagation the model learns the value iteration parameters including\nthe transition function. The aim is to learn a general model, that can navigate in\nunseen environments.\nLet us look in more detail at the value iteration algorithm. It is a simple algorithm\nthat consists of a doubly nested loop over states and actions, calculating the sum of\nrewards Í\n𝑠′∈𝑆𝑇𝑎(𝑠, 𝑠′)(𝑅𝑎(𝑠, 𝑠′) + 𝛾𝑉[𝑠′]) and a subsequent maximization opera-\n8 Note that here we use the term end-to-end to indicate the use of diﬀerentiable methods for\nthe learning and use of a deep dynamics model—to replace hand-crafted planning algorithms to\nuse the learned model. Elsewhere, in supervised learning, the term end-to-end is used diﬀerently,\nto describe learning both features and their use from raw pixels for classiﬁcation—to replace\nhand-crafted feature recognizers to pre-process the raw pixels and use in a hand-crafted machine\nlearning algorithm.\n5.2 Learning and Planning Agents\n139\ntion 𝑉[𝑠] = max𝑎(𝑄[𝑠, 𝑎]). This double loop is iterated to convergence. The insight\nis that each iteration can be implemented by passing the previous value function\n𝑉𝑛and reward function 𝑅through a convolution layer and max-pooling layer. In\nthis way, each channel in the convolution layer corresponds to the Q-function for a\nspeciﬁc action—the innermost loop—and the convolution kernel weights correspond\nto the transitions. Thus by recurrently applying a convolution layer 𝐾times, 𝐾\niterations of value iteration are performed.\nThe value iteration module is simply a neural network that has the capability of\napproximating a value iteration computation. Representing value iteration in this\nform makes learning the MDP parameters and functions natural—by backpropa-\ngating through the network, as in a standard CNN. In this way, the classic value\niteration algorithm can be approximated by a neural network.\nWhy would we want to have a fully diﬀerentiable algorithm that can only give\nan approximation, if we have a perfectly good classic procedural implementation\nthat can calculate the value function 𝑉exactly?\nThe reason is generalization. The exact algorithm only works for known tran-\nsition probabilities. The neural network can learn 𝑇(·) when it is not given, from\nthe environment, and it learns the reward and value functions at the same time. By\nlearning all functions all at once in an end-to-end fashion, the dynamics and value\nfunctions might be better integrated than when a separately hand-crafted planning\nalgorithm uses the results of a learned dynamics model. Indeed, reported results do\nindicate good generalization to unseen problem instances [748].\nThe idea of planning by gradient descent has existed for some time—actually, the\nidea of learning all functions by example has existed for some time—several authors\nexplored learning approximations of dynamics in neural networks [403, 671, 369].\nThe VINs can be used for discrete and continuous path planning, and have been\ntried in Grid world problems and natural language tasks.\nLater work has extended the approach to other applications of more irregular\nshape, by adding abstraction networks [668, 724, 710]. The addition of latent models\nincreases the power and versatility of end-to-end learning of planning and tran-\nsitions even further. Let us look brieﬂy in more detail at one such extension of\nVIN, to illustrate how latent models and planning go together. TreeQN by Farquhar\net al. [236] is a fully diﬀerentiable model learner and planner, using observation\nabstraction so that the approach works on applications that are less regular than\nmazes.\nTreeQN consists of ﬁve diﬀerentiable functions, four of which we have seen in\nthe previous section in Value Prediction Networks [564], Fig. 5.5 on page 135.\n•\nThe encoding function consists of a series of convolutional layers that embed\nthe actual state in a lower dimensional state 𝑠𝑙𝑎𝑡𝑒𝑛𝑡←𝑓𝑒𝑛𝑐\n𝜃𝑒(𝑠𝑎𝑐𝑡𝑢𝑎𝑙)\n•\nThe transition function uses a fully connected layer per action to calculate the\nnext-state representation 𝑠′\n𝑙𝑎𝑡𝑒𝑛𝑡←𝑓𝑡𝑟𝑎𝑛𝑠\n𝜃𝑡\n(𝑠𝑙𝑎𝑡𝑒𝑛𝑡, 𝑎𝑖)𝐼\n𝑖=0.\n•\nThe reward function predicts the immediate reward for every action 𝑎𝑖∈𝐴in\nstate 𝑠𝑙𝑎𝑡𝑒𝑛𝑡using a ReLU layer 𝑟←𝑓𝑟𝑒𝑤𝑎𝑟𝑑\n𝜃𝑟\n(𝑠′\n𝑙𝑎𝑡𝑒𝑛𝑡).\n•\nThe value function of a state is estimated with a vector of weights 𝑉(𝑠𝑙𝑎𝑡𝑒𝑛𝑡) ←\n𝑤⊤𝑠𝑙𝑎𝑡𝑒𝑛𝑡+ 𝑏.\n140\n5 Model-Based Reinforcement Learning\n•\nThe backup function applies a softmax function9 recursively to calculate the tree\nbackup value 𝑏(𝑥) ←Í𝐼\n𝑖=0 𝑥𝑖softmax(𝑥)𝑖.\nThese functions together can learn a model, and can also execute 𝑛-step Q-learning,\nto use the model to update a policy. Further details can be found in [236] and the\nGitHub code.10 TreeQN has been applied on games such as box-pushing and some\nAtari games, and outperformed model-free DQN.\nA limitation of VIN is that the tight connection between problem domain, itera-\ntion algorithm, and network architecture limited the applicability to other problems.\nAnother system that addresses this limitation os Predictron. Like TreeQN, the Pre-\ndictron [710] introduces an abstract model to reduce this limitation. As in VPN, the\nlatent model consists of four diﬀerentiable components: a representation model, a\nnext-state model, a reward model, and a discount model. The goal of the abstract\nmodel in Predictron is to facilitate value prediction (not state prediction) or pre-\ndiction of pseudo-reward functions that can encode special events, such as staying\nalive or reaching the next room. The planning part rolls forward its internal model\n𝑘steps. Unlike VPN, Predictron uses joint parameters. The Predictron has been\napplied to procedurally generated mazes and a simulated pool domain. In both cases\nit out-performed model-free algorithms.\nEnd-to-end model-based learning-and-planning is an active area of research.\nChallenges include understanding the relation between planning and learning [18,\n289], achieving performance that is competitive with classical planning algorithms\nand with model-free methods, and generalizing the class of applications. In Sect. 5.3\nmore methods will be shown.\nConclusion\nIn the previous sections we have discussed two methods to reduce the inaccuracy\nof the model, and two methods to reduce the impact of the use of an inaccurate\nmodel. We have seen a range of diﬀerent approaches to model-based algorithms.\nMany of the algorithms were developed recently. Deep model-based reinforcement\nlearning is an active area of research.\nEnsembles and MPC have improved the performance of model-based reinforce-\nment learning. The goal of latent or world models is to learn the essence of the do-\nmain, reducing the dimensionality, and for end-to-end, to also include the planning\npart in the learning. Their goal is generalization in a fundamental sense. Model-free\nlearns a policy of which action to take in each state. Model-based methods learn\nthe transition model, from state (via action) to state. Model-free teaches you how to\nbest respond to actions in your world, model-based helps you to understand your\nworld. By learning the transition model (and possibly even how to best plan with\nit) it is hoped that new generalization methods can be learned.\n9 The softmax function normalizes an input vector of real numbers to a probability distribution\n[0, 1]; 𝑝𝜃(𝑦|𝑥) = softmax( 𝑓𝜃(𝑥)) =\n𝑒𝑓𝜃(𝑥)\nÍ\n𝑘𝑒𝑓𝜃,𝑘(𝑥)\n10 See https://github.com/oxwhirl/treeqn for the code of TreeQN.\n5.3 High-Dimensional Environments\n141\nThe goal of model-based methods is to get to know the environment so intimitely\nthat the sample complexity can be reduced while staying close to the solution\nquality of model-free methods. A second goal is that the generalization power of\nthe methods improves so much, that new classes of problems can be solved. The\nliterature is rich and contains many experiments of these approaches on diﬀerent\nenvironments. Let us now look at the environments to see if we have succeeded.\n5.3 High-Dimensional Environments\nWe have now looked in some detail at approaches for deep model-based reinforce-\nment learning. Let us now change our perspective from the agent to the environment,\nand look at the kinds of environments that can be solved with these approaches.\n5.3.1 Overview of Model-Based Experiments\nThe main goal of model-based reinforcement learning is to learn the transition\nmodel accurately—not just the policy function that ﬁnds the best action, but the\nfunction that ﬁnds the next state. By learning the full essence of the environment a\nsubstantial reduction of sample complexity can be achieved. Also, the hope is that\nthe model allows us to solve new classes of problems. In this section we will try to\nanswer the question if these approaches have succeeded.\nThe answer to this question can be measured in training time and in run-time\nperformance. For performance, most benchmark domains provide easily measurable\nquantities, such as the score in an Atari game. For model-based approaches, the\nscores achieved by state-of-the-art model-free algorithms such as DQN, DDPG,\nPPO, SAC and A3C are a useful baseline. For training time, the reduction in sample\ncomplexity is an obvious choice. However, many model-based approaches use a\nﬁxed hyperparameter to determine the relation between external environment\nsamples and internal model samples (such as 1 : 1000). Then the number of time\nsteps needed for high performance to be reached becomes an important measure,\nand this is indeed published by most authors. For model-free methods, we often see\ntime steps in the millions per training run, and sometimes even billions. With so\nmany time steps it becomes quite important how much processing each time step\ntakes. For model-based methods, individual time steps may take longer than for\nmodel-free, since more processing for learning and planning has to be performed.\nIn the end, wall-clock time is important, and this is also often published.\nThere are two additional questions. First we are interested in knowing whether\na model-based approach allows new types of problems to be solved, that could\nnot be solved by model-free methods. Second is the question of brittleness. In\nmany experiments the numerical results are quite sensitive to diﬀerent settings of\nhyperparameters (including the random seeds). This is the case in many model-free\n142\n5 Model-Based Reinforcement Learning\nName\nLearning\nPlanning Environment Ref\nPILCO\nUncertainty\nTrajectory Pendulum\n[188]\niLQG\nUncertainty\nMPC\nSmall\n[756]\nGPS\nUncertainty\nTrajectory Small\n[468]\nSVG\nUncertainty\nTrajectory Small\n[326]\nVIN\nCNN\ne2e\nMazes\n[748]\nVProp\nCNN\ne2e\nMazes\n[551]\nPlanning\nCNN/LSTM\ne2e\nMazes\n[298]\nTreeQN\nLatent\ne2e\nMazes\n[236]\nI2A\nLatent\ne2e\nMazes\n[615]\nPredictron\nLatent\ne2e\nMazes\n[710]\nWorld Model\nLatent\ne2e\nCar Racing\n[304]\nLocal Model\nUncertainty\nTrajectory MuJoCo\n[296]\nVisual Foresight Video Prediction MPC\nManipulation\n[244]\nPETS\nEnsemble\nMPC\nMuJoCo\n[149]\nMVE\nEnsemble\nTrajectory MuJoCo\n[238]\nMeta Policy\nEnsemble\nTrajectory MuJoCo\n[156]\nPolicy Optim\nEnsemble\nTrajectory MuJoCo\n[376]\nPlaNet\nLatent\nMPC\nMuJoCo\n[309]\nDreamer\nLatent\nTrajectory MuJoCo\n[308]\nPlan2Explore\nLatent\nTrajectory MuJoCo\n[688]\nL3P\nLatent\nTrajectory MuJoCo\n[873]\nVideo-prediction Latent\nTrajectory Atari\n[563]\nVPN\nLatent\nTrajectory Atari\n[564]\nSimPLe\nLatent\nTrajectory Atari\n[391]\nDreamer-v2\nLatent\nTrajectory Atari\n[310]\nMuZero\nLatent\ne2e/MCTS Atari/Go\n[679]\nTable 5.2 Model-Based Reinforcement Learning Approaches [601]\nand model-based results [328]. However, when the transition model is accurate, the\nvariance may diminish, and some model-based approaches might be more robust.\nTable 5.2 lists 26 experiments with model-based methods [601]. In addition to\nthe name, the table provides an indication of the type of model learning that the\nagent uses, of the type of planning, and of the application environment in which it\nwas used. The categories in the table are described in the previous section, where\ne2e means end-to-end.\nIn the table, the approaches are grouped by environment. At the top are smaller\napplications such as mazes and navigation tasks. In the middle are larger MuJoCo\ntasks. At the bottom are high-dimensional Atari tasks. Let us look in more depth at\nthe three groups of environments: small navigation, robotics, and Atari games.\n5.3 High-Dimensional Environments\n143\n5.3.2 Small Navigation Tasks\nWe see that a few approaches use smaller 2D Grid world navigation tasks such\nas mazes, or block puzzles, such as Sokoban, and Pacman. Grid world tasks are\nsome of the oldest problems in reinforcement learning, and they are used frequently\nto test out new ideas. Tabular imagination approaches such as Dyna, and some\nlatent model and end-to-end learning and planning, have been evaluated with\nthese environments. They typically achieve good results, since the problems are of\nmoderate complexity.\nGrid world navigation problems are quintessential sequential decision problems.\nNavigation problems are typically low-dimensional, and no visual recognition is\ninvolved; transition functions are easy to learn.\nNavigation tasks are also used for latent model and end-to-end learning. Three\nlatent model approaches in Table 5.2 use navigation problems. I2A deals with model\nimperfections by introducing a latent model, based on Chiappa et al. and Buesing et\nal. [144, 121]. I2A is applied to Sokoban and Mini-Pacman by [615, 121]. Performance\ncompares favorably to model-free learning and to planning algorithms (MCTS).\nValue iteration networks introduced the concept of end-to-end diﬀerentiable\nlearning and planning [748, 562], after [403, 671, 369]. Through backpropagation\nthe model learns to perform value iteration. The aim to learn a general model that\ncan navigate in unseen environments was achieved, although diﬀerent extensions\nwere needed for more complex environments.\n5.3.3 Robotic Applications\nNext, we look at papers that use MuJoCo to model continuous robotic problems.\nRobotic problems are high-dimensional problems with continuous action spaces.\nMuJoCo is used by most experiments in this category to simulate the physical\nbehavior of robotic movement and the environment.\nUncertainty modeling with ensembles and MPC re-planning try to reduce or\ncontain inaccuracies. The combination of ensemble methods with MPC is well suited\nfor robotic problems, as we have seen in individual approaches such as PILCO and\nPETS.\nRobotic applications are more complex than Grid worlds; model-free methods\ncan take many time steps to ﬁnd good policies. It is important to know if model-\nbased methods succeed in reducing sample complexity in these problems. When we\nhave a closer look at how well uncertainty modeling and MPC succeed at achieving\nour ﬁrst goal, we ﬁnd a mixed picture.\nA benchmark study by Wang et al. [828] looked into the performance of ensemble\nmethods and Model-predictive control on MuJoCo tasks. It ﬁnds that these methods\nmostly ﬁnd good policies, and do so in signiﬁcantly fewer time steps than model-\nfree methods, typcially in 200k time steps versus 1 million for model-free. So, it\nwould appear that the lower sample complexity is achieved. However, they also\n144\n5 Model-Based Reinforcement Learning\nnote that per time step, the more complex model-based methods perform more\nprocessing than the simpler model-free methods. Although the sample complexity\nmay be lower, the wall-clock time is not, and model-free methods such as PPO\nans SAC are still much faster for many problems. Furthermore, the score that the\npolicy achieves varies greatly for diﬀerent problems, and is sensitive to diﬀerent\nhyperparameter values.\nAnother ﬁnding is that in some experiments with a large number of time steps,\nthe performance of model-based methods plateaus well below model-free per-\nformance, and the performance of the model-based methods themselves diﬀers\nsubstantially. There is a need for further research in deep model-based methods,\nespecially into robustness of results. More benchmarking studies are needed that\ncompare diﬀerent methods.\n5.3.4 Atari Games Applications\nSome experiments in Table 5.2 use the Arcade learning environment (ALE). ALE\nfeatures high-dimensional inputs, and provides one of the most challenging environ-\nments of the table. Especially latent models choose Atari games to showcase their\nperformance, and some do indeed achieve impressive results, in that they are able\nto solve new problems, such as playing all 57 Atari games well (Dreamer-v2) [310]\nand learning the rules of Atari and chess (MuZero) [679].\nHafner et al. published the papers Dream to control: learning behaviors by latent\nimagination, and Dreamer v2 [308, 310]. Their work extends the work on VPN and\nPlaNet with more advanced latent models and reinforcement learning methods [564,\n309]. Dreamer uses an actor-critic approach to learn behaviors that consider rewards\nbeyond the horizon. Values are backpropagated through the value model, similar to\nDDPG [480] and Soft actor critic [306].\nAn important advantage of model-based reinforcement learning is that it can\ngeneralize to unseen environments with similar dynamics [688]. The Dreamer\nexperiments showed that latent models are indeed more robust to unseen envi-\nronments than model-free methods. Dreamer is tested with applications from the\nDeepMind control suite (Sect. 4.3.2).\nValue prediction networks are another latent approach. They outperform model-\nfree DQN on mazes and Atari games such as Seaquest, QBert, Krull, and Crazy\nClimber. Taking the development of end-to-end learner/planners such as VPN\nand Predictron further is the work on MuZero [679, 290, 359]. In MuZero a new\narchitecture is used to learn the transition functions for a range of diﬀerent games,\nfrom Atari to the board games chess, shogi and Go. MuZero learns the transition\nmodel for all games from interaction with the environment.11 The MuZero model\nincludes diﬀerent modules: a representation, dynamics, and prediction function. Like\n11 This is a somewhat unusual usage of board games. Most researchers use board games because\nthe transition function is given (see next chapter). MuZero instead does not know the rules of\nchess, but starts from scratch learning the rules from interaction with the environment.\n5.3 High-Dimensional Environments\n145\nAlphaZero, MuZero uses a reﬁned version of MCTS for planning (see Sect. 6.2.1.2 in\nthe next chapter). This MCTS planner is used in a self-play training loop for policy\nimprovement. MuZero’s achievements are impressive: it is able to learn the rules\nof Atari games as well as board games, learning to play the games from scratch,\nin conjunction with learning the rules of the games. The MuZero achievements\nhave created follow up work to provide more insight into the relationship between\nactual and latent representations, and to reduce the computational demands [290,\n186, 39, 334, 18, 680, 289, 860].\nLatent models reduce observational dimensionality to a smaller model to perform\nplanning in latent space. End-to-end learning and planning is able to learn new\nproblems—the second of our two goals: it is able to learn to generalize navigation\ntasks, and to learn the rules of chess and Atari. These are new problems, that are\nout of reach for model-free methods (although the sample complexity of MuZero is\nquite large).\nConclusion\nIn deep model-based reinforcement learning benchmarks drive progress. We have\nseen good results with ensembles and local re-planning in continuous problems,\nand with latent models in discrete problems. In some applications, both the ﬁrst\ngoal, of better sample complexity, and the other goals, of learning new applications\nand reducing brittleness, are achieved.\nThe experiments used many diﬀerent environments within the ALE and MuJoCo\nsuites, from hard to harder. In the next two chapters we will study multi-agent\nproblems, where we encounter a new set of benchmarks, with a state space of\nmany combinations, including hidden information and simultaneous actions. These\nprovide even more complex challenges for deep reinforcement learning methods.\n5.3.5 Hands On: PlaNet Example\nBefore we go to the next chaper, let us take a closer look at how one of these\nmethods achieves eﬃcient learning of a complex high-dimensional task. We will\nlook at PlaNet, a well-documented project by Hafner et al. [309]. Code is available,12\nscripts are available, videos are available, and a blog is available13 inviting us to\ntake the experiments further. The name of the work is Learning latent dynamics\nfrom pixels, which describes what the algorithm does: use high dimensional visual\ninput, convert it to latent space, and plan in latent space to learn robot locomotion\ndynamics.\n12 https://github.com/google-research/planet\n13 https://planetrl.github.io\n146\n5 Model-Based Reinforcement Learning\nFig. 5.6 Locomotion tasks of PlaNet [309]\nPlaNet solves continuous control tasks that include contact dynamics, partial\nobservability, and sparse rewards. The applications used in the PlaNet experiments\nare: (a) Cartpole (b) Reacher (c) Cheetah (d) Finger (e) Cup and (f) Walker (see\nFig. 5.6). The Cartpole task is a swing-up task, with a ﬁxed viewpoint. The cart can\nbe out of sight, requiring the agent to remember information from previous frames.\nThe Finger spin task requires predicting the location of two separate objects and\ntheir interactions. The Cheetah task involves learning to run. It includes contacts\nof the feet with the ground that requires a model to predict multiple futures. The\nCup task must catch a ball in a cup. It provides a sparse reward signal once the\nball is caught, requiring accurate predictions far into the future. The Walker task\ninvolves a simulated robot that begins lying on the ground, and must learn to stand\nup and then walk. PlaNet performs well on these tasks. On DeepMind control tasks\nit achieves higher accuracy than an A3C or an D4PG agent. It reportedly does so\nusing 5000% fewer interactions with the environment on average.\nIt is instructive to experiment with PlaNet. The code can be found on GitHub.14\nScripts are available to run the experiments with simple one line commands:\npython3 -m planet.scripts.train --logdir /path/to/logdir\n--params ’{tasks:\n[cheetah_run]}’\nAs usual, this does require having the right versions of the right libraries installed,\nwhich may be a challenge and may require some creativity on your part. The\nrequired versions are listed on the GitHub page. The blog also contains videos and\npictures of what to expect, including comparisons to model-free baselines from the\nDeepMind Control Suite (A3C, D4PG).\nThe experiments show the viability of the idea to use rewards and values to\ncompress actual states into lower dimensional latent states, and then plan with\nthese latent states. Value-based compression reduces details in the high-dimensional\nactual states as noise that is not relevant to improve the value function [290]. To\nhelp understand how the actual state map to the latent states, see, for example [186,\n472, 401].\n14 https://github.com/google-research/planet\n5.3 High-Dimensional Environments\n147\nSummary and Further Reading\nThis has been a diverse chapter. We will summarize the chapter, and provide refer-\nences for further reading.\nSummary\nModel-free methods sample the environment using the rewards to learn the policy\nfunction, providing actions for all states for an environment. Model-based methods\nuse the rewards to learn the transition function, and then use planning methods\nto sample the policy from this internal model. Metaphorically speaking: model-\nfree learns how to act in the environment, model-based learns how to be the\nenvironment. The learned transition model acts as a multiplier on the amount of\ninformation that is used from each environment sample. A consequence is that\nmodel-based methods have a lower sample complexity, although, when the agent’s\ntransition model does not perfectly reﬂect the environment’s transition function,\nthe performance of the policy may be worse than a model-free policy (since that\nalways uses the environment to sample from).\nAnother, and perhaps more important aspect of the model-based approach, is\ngeneralization. Model-based reinforcement learning builds a dynamics model of\nthe domain. This model can be used multiple times, for new problem instances,\nbut also for related problem classes. By learning the transition and reward model,\nmodel-based reinforcement learning may be better at capturing the essence of a\ndomain than model-free methods, and thus be able to generalize to variations of\nthe problem.\nImagination showed how to learn a model and use it to ﬁll in extra samples\nbased on the model (not the environment). For problems where tabular methods\nwork, imagination can be many times more eﬃcient than model-free methods.\nWhen the agent has access to the transition model, it can apply reversible\nplanning algorithms, in additon to one-way learning with samples. There is a large\nliterature on backtracking and tree-traversal algorithms. Using a look-ahead of more\nthan one step can increase the quality of the reward even more. When the problem\nsize increases, or when we perform deep multi-step look-ahead, the accuracy of the\nmodel becomes critical. For high-dimensional problems high capacity networks are\nused that require many samples to prevent overﬁtting. Thus a trade-oﬀexists, to\nkeep sample complexity low.\nMethods such as PETS aim to take the uncertainty of the model into account\nin order to increase modeling accuracy. Model-predictive control methods re-plan\nat each environment step to prevent over-reliance on the accuracy of the model.\nClassical tabular approaches and Gaussian Process approaches have been quite\nsuccesful in achieving low sample complexity for small problems [743, 190, 417].\nLatent models observe that in many high-dimensional problems the factors that\ninﬂuence changes in the value function are often lower-dimensional. For example,\n148\n5 Model-Based Reinforcement Learning\nthe background scenery in an image may be irrelevant for the quality of play in\na game, and has no eﬀect on the value. Latent models use an encoder to translate\nthe high-dimensional actual state space into a lower-dimensional latent state space.\nSubsequent planning and value functions work on the (much smaller) latent space.\nFinally, we considered end-to-end model-based algorithms. These fully diﬀeren-\ntiable algorithms not only learn the dynamics model, but also learn the planning\nalgorithm that uses the model. The work on Value iteration networks [748] inspired\nrecent work on end-to-end learning, where both the transition model and the plan-\nning algorithm are learned, end-to-end. Combined with latent models (or World\nmodels [304]) impressive results were achieved [710], and the model and planning\naccuracy was improved to the extent that tabula rasa self-learning of game-rules\nwas achieved, in Muzero [679] for both chess, shogi, Go, and Atari games.\nFurther Reading\nModel-based reinforcement learning promises more sample eﬃcient learning. The\nﬁeld has a long history. For exact tabular methods Sutton’s Dyna-Q is a classical\napproach that illustrates the basic concept of model-based learning [741, 742].\nThere is an extensive literature on the approaches that were discussed in this\nchapter. For uncertainty modeling see [188, 189, 756], and for ensembles [574, 149,\n156, 376, 439, 391, 469, 468, 326, 244]. For Model-predictive control see [549, 262,\n505, 426, 296, 238, 38].\nLatent models is an active ﬁeld of research. Two of the earlier works are [563, 564],\nalthough the ideas go back to World models [304, 403, 671, 369]. Later, a sequence\nof PlaNet and Dreamer papers was inﬂuential [309, 308, 688, 310, 874].\nThe literature on end-to-end learning and planning is also extensive, starting\nwith VIN [748], see [22, 704, 706, 710, 551, 298, 236, 679, 239].\nAs applications became more challenging, notably in robotics, other methods\nwere developed, mostly based on uncertainty, see for surveys [190, 417]. Later, as\nhigh-dimensional problems became prevalent, latent and end-to-end methods were\ndeveloped. The basis for the section on environments is an overview of recent model-\nbased approaches [601]. Another survey is [529], a comprehensive benchmark study\nis [828].\nExercises\nLet us go to the Exercises.\n5.3 High-Dimensional Environments\n149\nQuestions\nBelow are ﬁrst some quick questions to check your understanding of this chapter.\nFor each question a simple, single sentence answer is suﬃcient.\n1. What is the advantage of model-based over model-free methods?\n2. Why may the sample complexity of model-based methods suﬀer in high-\ndimensional problems?\n3. Which functions are part of the dynamics model?\n4. Mention four deep model-based approaches.\n5. Do model-based methods achieve better sample complexity than model-free?\n6. Do model-based methods achieve better performance than model-free?\n7. In Dyna-Q the policy is updated by two mechanisms: learning by sampling the\nenvironment and what other mechanism?\n8. Why is the variance of ensemble methods lower than of the individual machine\nlearning approaches that are used in the ensemble?\n9. What does model-predictive control do and why is this approach suited for\nmodels with lower accuracy?\n10. What is the advantage of planning with latent models over planning with actual\nmodels?\n11. How are latent models trained?\n12. Mention four typical modules that constitute the latent model.\n13. What is the advantage of end-to-end planning and learning?\n14. Mention two end-to-end planning and learning methods.\nExercises\nIt is now time to introduce a few programming exercises. The main purpose of the\nexercises is to become more familiar with the methods that we have covered in this\nchapter. By playing around with the algorithms and trying out diﬀerent hyperpa-\nrameter settings you will develop some intuition for the eﬀect on performance and\nrun time of the diﬀerent methods.\nThe experiments may become computationally expensive. You may want to\nconsider running them in the cloud, with Google Colab, Amazon AWS, or Microsoft\nAzure. They may have student discounts, and they will have the latest GPUs or\nTPUs for use with TensorFlow or PyTorch.\n1. Dyna Implement tabular Dyna-Q for the Gym Taxi environment. Vary the amount\nof planning 𝑁and see how performance is inﬂuenced.\n2. Keras Make a function approximation version of Dyna-Q and Taxi, with Keras.\nVary the capacity of the network and the amount of planning. Compare against\na pure model-free version, and note the diﬀerence in performance for diﬀerent\ntasks and in computational demands.\n150\n5 Model-Based Reinforcement Learning\n3. Planning In Dyna-Q, planning has so far been with single step model samples.\nImplement a simple depth-limited multi-step look-ahead planner, and see how\nperformance is inﬂuenced for the diﬀerent look-ahead depths.\n4. MPC Read the paper by Nagabandi et al. [549] and download the code.15 Acquire\nthe right versions of the libraries, and run the code with the supplied scripts,\njust for the MB (model-based) versions. Note that plotting is also supported by\nthe scripts. Run with diﬀerent MPC horizons. Run with diﬀerent ensemble sizes.\nWhat are the eﬀects on performance and run time for the diﬀerent applications?\n5. PlaNet Go to the PlaNet blog and read it (see previous section).16 Go to the PlaNet\nGitHub site and download and install the code.17 Install the DeepMind control\nsuite,18 and all necessary versions of the support libraries.\nRun Reacher and Walker in PlaNet, and compare against the model-free methods\nD4PG and A3C. Vary the size of the encoding network and note the eﬀect on\nperformance and run time. Now turn oﬀthe encoder, and run with planning on\nactual states (you may have to change network sizes to achieve this). Vary the\ncapacity of the latent model, and of the value and reward functions. Also vary\nthe amount of planning, and note its eﬀect.\n6. End-to-end As you have seen, these experiments are computationally expen-\nsive. We will now turn to end-to-end planning and learning (VIN and MuZero).\nThis exercise is also computationally expensive. Use small applications, such\nas small mazes, and Cartpole. Find and download a MuZero implementation\nfrom GitHub and explore using the experience that you have gained from the\nprevious exercises. Focus on gaining insight into the shape of the latent space.\nTry MuZero-General [215],19 or a MuZero visualization [186] to get insight\ninto latent space.20 (This is a challenging exercise, suitable for a term project or\nthesis.)\n15 https://github.com/anagabandi/nn_dynamics\n16 https://planetrl.github.io\n17 https://github.com/google-research/planet\n18 https://github.com/deepmind/dm_control\n19 https://github.com/werner-duvaud/muzero-general\n20 https://github.com/kaesve/muzero\nChapter 6\nTwo-Agent Self-Play\nPrevious chapters were concerned with how a single agent can learn optimal\nbehavior for its environment. This chapter is diﬀerent. We turn to problems where\ntwo agents operate whose behavior will both be modeled (and, in the next chapter,\nmore than two).\nTwo-agent problems are interesting for two reasons. First, the world around us is\nfull of active entities that interact, and modeling two agents and their interaction is\na step closer to understanding the real world than modeling a single agent. Second,\nin two-agent problems exceptional results were achieved—reinforcement learning\nagents teaching themselves to become stronger than human world champions—and\nby studying these methods we may ﬁnd a way to achieve similar results in other\nproblems.\nThe kind of interaction that we model in this chapter is zero-sum: my win is your\nloss and vice versa. These two-agent zero-sum dynamics are fundamentally diﬀerent\nfrom single-agent dynamics. In single agent problems the environment lets you\nprobe it, lets you learn how it works, and lets you ﬁnd good actions. Although the\nenvironment may not be your friend, it is also not working against you. In two-agent\nzero-sum problems the environment does try to win from you, it actively changes\nits replies to minimize your reward, based on what it learns from your behavior.\nWhen learning our optimal policy we should take all possible counter-actions into\naccount.\nA popular way to do so is to implement the environment’s actions with self-play:\nwe replace the environment by a copy of ourselves. In this way we let ourselves\nplay against an opponent that has all the knowledge that we currently have, and\nagents learn from eachother.\nWe start with a short review of two-agent problems, after which we dive into self-\nlearning. We look at the situation when both agents know the transition function\nperfectly, so that model accuracy is no longer a problem. This is the case, for example,\nin games such as chess and Go, where the rules of the game determine how we can\ngo from one state to another.\nIn self-learning the environment is used to generate training examples for the\nagent to train a better policy, after which the better agent policy is used in this\n151\n152\n6 Two-Agent Self-Play\nenvironment to train the agent, and again, and again, creating a virtuous cycle of\nself-learning and mutual improvement. It is possible for an agent to teach itself\nto play a game without any prior knowledge at all, so-called tabula rasa learning,\nlearning from a blank slate.\nThe self-play systems that we describe in this chapter use model-based methods,\nand combine planning and learning approaches. There is a planning algorithm that\nwe have mentioned a few times, but have not yet explained in detail. In this chapter\nwe will discuss Monte Carlo Tree Search, or MCTS, a highly popular planning\nalgorithm. MCTS can be used in single agent and in two-agent situations, and is\nthe core of many successful applications, including MuZero and the self-learning\nAlphaZero series of programs. We will explain how self-learning and self-play work\nin AlphaGo Zero, and why they work so well. We will then discuss the concept of\ncurriculum learning, which is behind the success of self-learning.\nThe chapter is concluded with exercises, a summary, and pointers to further\nreading.\nCore Concepts\n•\nSelf-play\n•\nCurriculum learning\nCore Problem\n•\nUse a given transition model for self-play, in order to become stronger than the\ncurrent best players\nCore Algorithms\n•\nMinimax (Listing 6.2)\n•\nMonte Carlo Tree Search (Listing 6.3)\n•\nAlphaZero tabula rasa learning (Listing 6.1)\nSelf-Play in Games\nWe have seen in Chap. 5 that when the agent has a transition model of the envi-\nronment, it can achieve greater performance, especially when the model has high\naccuracy. What if the accuracy of our model were perfect, if the agent’s transition\n6 Two-Agent Self-Play\n153\nFig. 6.1 Backgammon and Tesauro\nfunction is the same as the environment’s, how far would that bring us? And what\nif we could improve our environment as part of our learning process, can we then\ntranscend our teacher, can the sorcerer’s apprentice outsmart the wizard?\nTo set the scene for this chapter, let us describe the ﬁrst game where this has\nhappened: backgammon.\nLearning to Play Backgammon\nIn Sect. 3.2.3 we brieﬂy discussed research into backgammon. Already in the early\n1990s, the program TD-Gammon achieved stable reinforcement learning with a\nshallow network. This work was started at the end of the 1980s by Gerald Tesauro,\na researcher at IBM laboratories. Tesauro was faced with the problem of getting a\nprogram to learn beyond the capabilities of any existing entity. (In Fig. 6.1 we see\nTesauro in front of his program; image by IBM Watson Media.)\nIn the 1980s computing was diﬀerent. Computers were slow, datasets were small,\nand neural networks were shallow. Against this background, the success of Tesauro\nis quite remarkable.\nHis programs were based on neural networks that learned good patterns of play.\nHis ﬁrst program, Neurogammon, was trained using supervised learning, based on\ngames of human experts. In supervised learning the model cannot become stronger\nthan the human games it is trained on. Neurogammon achieved an intermediate\nlevel of play [762]. His second program, TD-Gammon, was based on reinforcement\nlearning, using temporal diﬀerence learning and self-play. Combined with hand-\ncrafted heuristics and some planning, in 1992 it played at human championship\nlevel, becoming the ﬁrst computer program to do so in a game of skill [765].\nTD-Gammon is named after temporal diﬀerence learning because it updates\nits neural net after each move, reducing the diﬀerence between the evaluation of\nprevious and current positions. The neural network used a single hidden layer with\nup to 80 units. TD-Gammon initially learned from a state of zero knowledge, tabula\nrasa. Tesauro describes TD-Gammon’s self-play as follows: The move that is selected\nis the move with maximum expected outcome for the side making the move. In other\n154\n6 Two-Agent Self-Play\nwords, the neural network is learning from the results of playing against itself. This\nself-play training paradigm is used even at the start of learning, when the network’s\nweights are random, and hence its initial strategy is a random strategy [764].\nTD-Gammon performed tabula rasa learning, its neural network weights initili-\nazed to small random numbers. It reached world-champion level purely by playing\nagainst itself, learning the game as it played along.\nSuch autonomous self-learning is one of the main goals of artiﬁcial intelligence.\nTD-Gammon’s success inspired many researchers to try neural networks and self-\nplay approaches, culminating eventually, many years later, in high-proﬁle results\nin Atari [522] and AlphaGo [703, 706], which we will describe in this chapter.1\nIn Sect. 6.1 two-agent zero-sum environments will be described. Next, in Sect. 6.2\nthe tabula rasa self-play method is described in detail. In Sect. 6.3 we focus on the\nachievements of the self-play methods. Let us now start with two-agent zero-sum\nproblems.\n6.1 Two-Agent Zero-Sum Problems\nBefore we look into self-play algorithms, let us look for a moment at the two-agent\ngames that have fascinated artiﬁcial intelligence researchers for such a long time.\nGames come in many shapes and sizes. Some are easy, some are hard. The\ncharacteristics of games are described in a fairly standard taxonomy. Important\ncharacteristics of games are: the number of players, whether the game is zero-sum or\nnon-zero-sum, whether it is perfect or imperfect information, what the complexity\nof taking decisions is, and what the state space complexity is. We will look at these\ncharacteristics in more detail.\n•\nNumber of Players One of the most important elements of a game is the number\nof players. One-player games are normally called puzzles, and are modeled as\na standard MDP. The goal of a puzzle is to ﬁnd a solution. Two-player games\nare “real” games. Quite a number of two-player games exist that provide a nice\nbalance between being too easy and being too hard for players (and for computer\nprogrammers) [172]. Examples of two-player games that are popular in AI are\nchess, checkers, Go, Othello, and shogi.\nMulti-player games are played by three or more players. Well-known examples\nof multiplayer games are the card games bridge and poker, and strategy games\nsuch as Risk, Diplomacy, and StarCraft.\n•\nZero Sum versus Non Zero Sum An important aspect of a game is whether it is\ncompetitive or cooperative. Most two-player games are competitive: the win (+1)\nof player A is the loss (−1) of player B. These games are called zero sum because\nthe sum of the wins for the players remains a constant zero. Competition is an\n1 A modern reimplementation of TD-Gammon in TensorFlow is available on GitHub at TD-\nGammon https://github.com/fomorians/td-gammon\n6.1 Two-Agent Zero-Sum Problems\n155\nimportant element in the real world, and these games provide a useful model for\nthe study of conﬂict and strategic behavior.\nIn contrast, in cooperative games the players win if they can ﬁnd win/win\nsituations. Examples of cooperative games are Hanabi, bridge, Diplomacy [429,\n185], poker and Risk. The next chapter will discuss multi-agent and cooperative\ngames.\n•\nPerfect versus Imperfect Information In perfect information games all relevant\ninformation is known to all players. This is the case in typical board games such\nas chess and checkers. In imperfect information games some information may\nbe hidden from some players. This is the case in card games such as bridge and\npoker, where not all cards are known to all players. Imperfect information games\ncan be modeled as partially observable Markov processes, POMDP [569, 693].\nA special form of (im)perfect information games are games of chance, such as\nbackgammon and Monopoly, in which dice play an important role. There is no\nhidden information in these games, and these games are sometimes considered\nto be perfect information games, despite the uncertainty present at move time.\nStochasticity is not the same as imperfect information.\n•\nDecision Complexity The diﬃculty of playing a game depends on the complexity\nof the game. The decision complexity is the number of end positions that deﬁne\nthe value (win, draw, or loss) of the initial game position (also known as the\ncritical tree or proof tree [416]). The larger the number of actions in a position,\nthe larger the decision complexity. Games with small board sizes such as tic tac\ntoe (3 × 3) have a smaller complexity than games with larger boards, such as\ngomoku (19 × 19). When the action space is very large, it can often be treated as\na continuous action space. In poker, for example, the monetary bets can be of\nany size, deﬁning an action size that is practically continuous.\n•\nState Space Complexity The state space complexity of a game is the number of\nlegal positions reachable from the initial position of a game. State space and\ndecision complexity are normally positively correlated, since games with high\ndecision complexity typically have high state space complexity. Determining the\nexact state space complexity of a game is a nontrivial task, since positions may\nbe illegal or unreachable.2 For many games approximations of the state space\nhave been calculated. In general, games with a larger state space complexity are\nharder to play (“require more intelligence”) for humans and computers. Note\nthat the dimensionality of the states may not correlate with the size of the state\nspace, for example, the rules of some of the simpler Atari games limit the number\nof reachable states, although the states themselves are high-dimensional (they\nconsist of many video pixels).\n2 For example, the maximal state space of tic tac toe is 39 = 19683 positions (9 squares of ’X’, ’O’,\nor blank), where only 765 positions remain if we remove symmetrical and illegal positions [661].\n156\n6 Two-Agent Self-Play\nName\nboard state space zero-sum information\nChess\n8 × 8\n1047\nzero-sum\nperfect\nCheckers\n8 × 8\n1018\nzero-sum\nperfect\nOthello\n8 × 8\n1028\nzero-sum\nperfect\nBackgammon\n24\n1020\nzero-sum\nchance\nGo\n19 × 19\n10170\nzero-sum\nperfect\nShogi\n9 × 9\n1071\nzero-sum\nperfect\nPoker\ncard\n10161\nnon-zero\nimperfect\nTable 6.1 Characteristics of games\nFig. 6.2 Deep Blue and Garry Kasparov in May 1997 in New York\nZero-Sum Perfect-Information Games\nTwo-person zero-sum games of perfect information, such as chess, checkers, and\nGo, are among the oldest applications of artiﬁcial intelligence. Turing and Shannon\npublished the ﬁrst ideas on how to write a program to play chess more than 70\nyears ago [788, 694]. To study strategic reasoning in artiﬁcial intelligence, these\ngames are frequently used. Strategies, or policies, determine the outcome. Table 6.1\nsummarizes some of the games that have played an important role in artiﬁcial\nintelligence research.\n6.1.1 The Diﬃculty of Playing Go\nAfter the 1997 defeat of chess world champion Garry Kasparov by IBM’s Deep Blue\ncomputer (Fig. 6.2; image by Chessbase), the game of Go (Fig. 1.4) became the next\n6.1 Two-Agent Zero-Sum Problems\n157\nbenchmark game, the Drosophila3 of AI, and research activity in Go intensiﬁed\nsigniﬁcantly.\nThe game of Go is more diﬃcult than chess. It is played on a larger board\n(19 × 19 vs. 8 × 8), the action space is larger (around 250 moves available in a\nposition versus some 25 in chess), the game takes longer (typically 300 moves\nversus 70) and the state space complexity is much larger: 10170 for Go, versus 1047\nfor chess. Furthermore, rewards in Go are sparse. Only at the end of a long game,\nafter many moves have been played, is the outcome (win/loss) known. Captures are\nnot so frequent in Go, and no good eﬃciently computable heuristic has been found.\nIn chess, in contrast, the material balance in chess can be calculated eﬃciently,\nand gives a good indication of how far ahead we are. For the computer, much of\nthe playing in Go happens in the dark. In contrast, for humans, it can be argued\nthat the visual patterns of Go may be somewhat easier to interpret than the deep\ncombinatorial lines of chess.\nFor reinforcement learning, credit assignment in Go is challenging. Rewards only\noccur after a long sequence of moves, and it is unclear which moves contributed\nthe most to such an outcome, or whether all moves contributed equally. Many\ngames will have to be played to acquire enough outcomes. In conclusion, Go is\nmore diﬃcult to master with a computer than chess.\nTraditionally, computer Go programs followed the conventional chess design\nof a minimax search with a heuristic evaluation function, that, in the case of Go,\nwas based on the inﬂuence of stones (see Sect. 6.2.1 and Fig. 6.4) [515]. This chess\napproach, however, did not work for Go, or at least not well enough. The level of\nplay was stuck at mid-amateur level for many years.\nThe main problems were the large branching factor, and the absence of an\neﬃcient and good evaluation function.\nSubsequently, Monte Carlo Tree Search was developed, in 2006. MCTS is a\nvariable depth adaptive search algorithm, that did not need a heuristic function, but\ninstead used random playouts to estimate board strength. MCTS programs caused\nthe level of play to improve from 10 kyu to 2-3 dan, and even stronger on the small\n9 × 9 board.4 However, again, at that point, performance stagnated, and researchers\nexpected that world champion level play was still many years into the future. Neural\nnetworks had been tried, but were slow, and did not improve performance much.\nPlaying Strength in Go\nLet us compare the three programming paradigms of a few diﬀerent Go programs\nthat have been written over the years (Fig. 6.3). The programs fall into three cat-\negories. First are the programs that use heuristic planning, the minimax-style\n3 Drosophila Melanogaster is also known as the fruitﬂy, a favorite species of genetics researchers\nto test their theories, because experiments produce quick and clear answers.\n4 Absolute beginners in Go start at 30 kyu, progressing to 10 kyu, and advancing to 1 kyu (30k–1k).\nStronger amateur players then achieve 1 dan, progressing to 7 dan, the highest amateur rating for\nGo (1d–7d). Professional Go players have a rating from 1 dan to 9 dan, written as 1p–9p.\n158\n6 Two-Agent Self-Play\nFig. 6.3 Go Playing Strength of Top Programs over the Years [20]\nprograms. GNU Go is a well-known example of this group of programs. The heuris-\ntics in these programs are hand-coded. The level of play of these programs was at\nmedium amateur level. Next come the MCTS-based programs. They reached strong\namateur level. Finally come the AlphaGo programs, in which MCTS is combined\nwith deep self-play. These reached super-human performance. The ﬁgure also shows\nother programs that follow a related approach.\nThus, Go provided a large and sparse state space, providing a highly challenging\ntest, to see how far self-play with a perfect transition function can come. Let us\nhave a closer look at the achievements of AlphaGo.\n6.1.2 AlphaGo Achievements\nIn 2016, after decades of research, the eﬀort in Go paid oﬀ. In the years 2015–2017\nthe DeepMind AlphaGo team played three matches in which it beat all human\nchampions that it played, Fan Hui, Lee Sedol, and Ke Jie. The breakthrough perfor-\nmance of AlphaGo came as a surprise. Experts in computer games had expected\ngrandmaster level play to be at least ten years away.\nThe techniques used in AlphaGo are the result of many years of research, and\ncover a wide range of topics. The game of Go worked very well as Drosophila.\nImportant new algorithms were developed, most notably Monte Carlo Tree Search\n(MCTS), as well as major progress was made in deep reinforcement learning. We\nwill provide a high-level overview of the research that culminated in AlphaGo (that\n6.1 Two-Agent Zero-Sum Problems\n159\nFig. 6.4 Inﬂuence in the game of Go. Empty intersections are marked as being part of Black’s or\nWhite’s Territory\nFig. 6.5 AlphaGo versus Lee Sedol in 2016 in Seoul\nbeat the champions), and its successor, AlphaGo Zero (that learns Go tabula rasa).\nFirst we will describe the Go matches.\nThe games against Fan Hui were played in October 2015 in London as part of\nthe development eﬀort of AlphaGo. Fan Hui is the 2013, 2014, and 2015 European\nGo Champion, then rated at 2p dan. The games against Lee Sedol were played in\nMay 2016 in Seoul, and were widely covered by the media (see Fig. 6.5; image by\n160\n6 Two-Agent Self-Play\nFig. 6.6 AlphaGo on the Cover of Nature\nDeepMind). Although there is no oﬃcial worldwide ranking in international Go,\nin 2016 Lee Sedol was widely considered one of the four best players in the world.\nA year later another match was played, this time in China, against the Chinese\nchampion Ke Jie, who was ranked number one in the Korean, Japanese, and Chinese\nranking systems at the time of the match. All three matches were won convincingly\nby AlphaGo. Beating the best Go players appeared on the cover of the journal\nNature, see Fig. 6.6.\nThe AlphaGo series of programs actually consists of three programs: AlphaGo,\nAlphaGo Zero, and AlphaZero. AlphaGo is the program that beat the human Go\nchampions. It consists of a combination of supervised learning from grandmaster\ngames and from self-play games. The second program, AlphaGo Zero, is a full\nre-design, based solely on self-play. It performs tabula rasa learning of Go, and\nplays stronger than AlphaGo. AlphaZero is a generalization of this program that\nalso plays chess and shogi. Section 6.3 will describe the programs in more detail.\nLet us now have an in-depth look at the self-play algorithms as featured in\nAlphaGo Zero and AlphaZero.\n6.2 Tabula Rasa Self-Play Agents\n161\nAgent2\nAgent1\n𝑟′\n1\n𝑠′\n1\n𝑎2\n𝑟′\n2\n𝑠′\n2\n𝑎1\nFig. 6.7 Agent-Agent World\n6.2 Tabula Rasa Self-Play Agents\nModel-based reinforcement learning showed us that by learning a local transition\nmodel, good sample eﬃciency can be achieved when the accuracy of the model is\nsuﬃcient. When we have perfect knowledge of the transitions, as we have in this\nchapter, then we can plan far into the future, without error.\nIn regular agent-environment reinforcement learning the complexity of the envi-\nronment does not change as the agent learns, and as a consequence, the intelligence\nof the agent’s policy is limited by the complexity of the environment. However, in\nself-play a cycle of mutual improvement occurs; the intelligence of the environment\nimproves because the agent is learning. With self-play, we can create a system\nthat can transcend the original environment, and keep growing, and growing, in\na virtuous cycle of mutual learning. Intelligence emerging out of nothing. This is\nthe kind of system that is needed when we wish beat the best known entity in a\ncertain domain, since copying from a teacher will not help us to transcend it.\nStudying how such a high level of play is achieved is interesting, for three\nreasons: (1) it is exciting to follow an AI success story, (2) it is interesting to see\nwhich techniques were used and how it is possible to achieve beyond-human\nintelligence, and (3) it is interesting to see if we can learn a few techniques that\ncan be used in other domains, beyond two-agent zero-sum games, to see if we can\nachieve super-intelligence there as well.\nLet us have a closer look at the self-learning agent architecture that is used by\nAlphaGo Zero. We will see that two-agent self-play actually consists of three levels\nof self-play: move-level self-play, example-level self-play, and tournament-level\nself-play.\nFirst, we will discuss the general architecture, and how it creates a cycle of\nvirtuous improvement. Next, we will describe the levels in detail.\n162\n6 Two-Agent Self-Play\nTransition Rules\nOpponent\nplaying\nPolicy/Value\nlearning/planning\nacting\nFig. 6.8 Playing with Known Transition Rules\nCycle of Virtuous Improvement\nIn contrast to the agent/environment model, we now have two agents (Fig. 6.7). In\ncomparison with the model-based world of Chap. 5 (Fig. 6.8) our learned model has\nbeen replaced by perfect knowledge of the transition rules, and the environment is\nnow called opponent: the negative version of the same agent playing the role of\nagent2.\nThe goal in this chapter is to reach the highest possible performance in terms of\nlevel of play, without using any hand-coded domain knowledge. In applications such\nas chess and Go a perfect transition model is present. Together with a learned reward\nfunction and a learned policy function, we can create a self-learning system in which\na virtuous cycle of ever improving performance occurs. Figure 6.9 illustrates such a\nsystem: (1) the searcher uses the evaluation network to estimate reward values and\npolicy actions, and the search results are used in games against the opponent in\nself-play, (2) the game results are then collected in a buﬀer, which is used to train\nthe evaluation network in self-learning, and (3) by playing a tournament against\na copy of ourselves a virtuous cycle of ever-increasing function improvement is\ncreated.\nAlphaGo Zero Self-Play in Detail\nLet us look in more detail at how self-learning works in AlphaGo Zero. AlphaGo\nZero uses a model-based actor critic approach with a planner that improves a single\nvalue/policy network. For policy improvement it uses MCTS, for learning a single\ndeep residual network with a policy head and a value head (Sect. B.2.6), see Fig. 6.9.\nMCTS improves the quality of the training examples in each iteration (left panel),\nand the net is trained with these better examples, improving its quality (right panel).\nThe output of MCTS is used to train the evaluation network, whose output is\nthen used as evalution function in that same MCTS. A loop is wrapped around the\nsearch-eval functions to keep training the network with the game results, creating\na learning curriculum. Let us put these ideas into pseudocode.\n6.2 Tabula Rasa Self-Play Agents\n163\nsearch\neval\ntrain\ngame examples\nnew net\nreward\nopponent games\nnet0\nnet1\nnet2\nnet3\n. . .\nMCTS\nMCTS\nMCTS\nMCTS\nFig. 6.9 Self-play loop improving quality of net\n1\nfor\ntourn in range (1, max_tourns ):\n# curric. of\ntournaments\n2\nfor\ngame in range (1, max_games):\n# play a tourn. of games\n3\ntrim(triples)\n# if buffer\nfull: replace\nold\nentries\n4\nwhile\nnot\ngame_over ():\n# generate\nthe\nstates of one\ngame\n5\nmove = mcts(board , eval(net))\n# move is (s,a) pair\n6\ngame_pairs\n+= move\n7\nmake_move_and_switch_side (board , move)\n8\ntriples\n+= add(game_pairs , game_outcome ( game_pairs ))\n# add to buf\n9\nnet = train(net , triples)\n# retrain\nwith (s,a,outc) triples\nListing 6.1 Self-play pseudocode\nThe Cycle in Pseudocode\nConceptually self-play is as ingenious as it is elegant: a double training loop around\nan MCTS player with a neural network as evaluation and policy function that help\nMCTS. Figure 6.10 and Listing 6.1 show the self-play loop in detail. The numbers in\nthe ﬁgure correspond to the line numbers in the pseudocode.\nLet us perform an outside-in walk-through of this system. Line 1 is the main\nself-play loop. It controls how long the execution of the curriculum of self-play\ntournaments will continue. Line 2 executes the training episodes, the tournaments\nof self-play games after which the network is retrained. Line 4 plays such a game\nto create (state, action) pairs for each move, and the outcome of the game. Line 5\ncalls MCTS to generate an action in each state. MCTS performs the simulations\nwhere it uses the policy head of the net in P-UCT selection, and the value head of\nthe net at the MCTS leaves. Line 6 appends the state/action pair to the list of game\n164\n6 Two-Agent Self-Play\n5 game_pairs ←mcts\n5 pol/val ←eval(net(state))\n9 net ←train(net, triples)\n8 triples\n1 tourn: iterate with new net\npolicy/value\nstate\n2/4 game_pairs\nFig. 6.10 A diagram of self-play with line-numbers\nmoves. Line 7 performs the move on the board, and switches color to the other\nplayer, for the next move in the while loop. At line 8 a full game has ended, and the\noutcome is known. Line 8 adds the outcome of each game to the (state, action)-pairs,\nto make the (state, action, outcome)-triples for the network to train on. Note that\nsince the network is a two-headed policy/value net, both an action and an outcome\nare needed for network training. On the last line this triples-buﬀer is then used to\ntrain the network. The newly trained network is used in the next self-play iteration\nas the evaluation function by the searcher. With this net another tournament is\nplayed, using the searcher’s look-ahead to generate a next batch of higher-quality\nexamples, resulting in a sequence of stronger and stronger networks (Fig. 6.9 right\npanel).\nIn the pseudocode we see the three self-play loops where the principle of playing\nagainst a copy of yourself is used:\n1. Move-level: in the MCTS playouts, our opponent actually is a copy of ourselves\n(line 5)—hence, self-play at the level of game moves\n2. Example-level: the input for self-training the approximator for the policy and\nthe reward functions is generated by our own games (line 2)—hence, self-play at\nthe level of the value/policy network.\n3. Tournament-level: the self-play loop creates a training curriculum that starts\ntabula rasa and ends at world champion level. The system trains at the level of\nthe player against itself (line 1)—hence, self-play, of the third kind.\nAll three of these levels use their own kind of self-play, of which we will describe\nthe details in the following sections. We start with move-level self-play.\n6.2.1 Move-Level Self-Play\nAt the innermost level, we use the agent to play against itself, as its own opponent.\nWhenever it is my opponent’s turn to move, I play its move, trying to ﬁnd the best\nmove for my opponent (which will be the worst possible move for me). This scheme\nuses the same knowledge for player and opponent. This is diﬀerent from the real\nworld, where the agents are diﬀerent, with diﬀerent brains, diﬀerent reasoning\n6.2 Tabula Rasa Self-Play Agents\n165\neval\nsearch\nvalue\naction/state\nFig. 6.11 Search-Eval Architecture of Games\nskills, and diﬀerent experience. Our scheme is symmetrical: when we assume that\nour agent plays a strong game, then the opponent is also assumed to play strongly,\nand we can hope to learn from the strong counter play. (We thus assume that our\nagent plays with the same knowledge as we have; we are not trying to consciously\nexploit opponent weaknesses.)5\n6.2.1.1 Minimax\nThis principle of generating the counter play by playing yourself while switching\nperspectives has been used since the start of artiﬁcial intelligence. It is known as\nminimax.\nThe games of chess, checkers and Go are challenging games. The architecture\nthat has been used to program chess and checkers players has been the same since\nthe earliest paper designs of Turing [788]: a search routine based on minimax which\nsearches to a certain depth, and an evaluation function to estimate the score of\nboard positions using heuristic rules of thumb when this depth is reached. In chess\nand checkers, for example, the number of pieces on the board of a player is a crude\nbut eﬀective approximation of the strength of a state for that player. Figure 6.11\nshows a diagram of this classic search-eval architecture.6\nBased on this principle many successful search algorithms have been developed,\nof which alpha-beta is the best known [416, 593]. Since the size of the state space\nis exponential in the depth of lookahead, however, many enhancements had to be\ndeveloped to manage the size of the state space and to allow deep lookahead to\noccur [600].\nThe word minimax is a contraction of maximizing/minimizing (and then reversed\nfor easy pronunciation). It means that in zero-sum games the two players alternate\nmaking moves, and that on even moves, when player A is to choose a move, the\n5 There is also research into opponent modeling, where we try to exploit our opponent’s weak-\nnesses [320, 91, 259]. Here, we assume an identical opponent, which often works best in chess and\nGo.\n6 Because the agent knows the transition function 𝑇, it can calculate the new state 𝑠′ for each\naction 𝑎. The reward 𝑟is calculated at terminal states, where it is equal to the value 𝑣. Hence,\nin this diagram the search function provides the state to the eval function. See [788, 600] for an\nexplanation of the search-eval architecture.\n166\n6 Two-Agent Self-Play\n2\n1\n6\n1\n3\n2\n3\n4\n2\n1\n1\n6\n5\nFig. 6.12 Minimax tree\nbest move is the one that maximizes the score for player A, while on odd moves\nthe best move for player B is the move that minimizes the score for player A.\nFigure 6.12 depicts this situation in a tree. The score values in the nodes are\nchosen to show how minimax works. At the top is the root of the tree, level 0, a\nsquare node where player A is to move.\nSince we assume that all players rationally choose the best move, the value of the\nroot node is determined by the value of the best move, the maximum of its children.\nEach child, at level 1, is a circle node where player B chooses its best move, in order\nto minimize the score for player A. The leaves of this tree, at level 2, are again max\nsquares (even though there is no child to choose from anymore). Note how for each\ncircle node the value is the minimum of its children, and for the square node, the\nvalue is the maximum of the tree circle nodes.\nPython pseudocode for a recursive minimax procedure is shown in Listing 6.2.\nNote the extra hyperparameter d. This is the search depth counting upwards from\nthe leaves. At depth 0 are the leaves, where the heuristic evaluation function is\ncalled to score the board.7 Also note that the code for making moves on the board—\ntransitioning actions into the new states—is not shown in the listing. It is assumed to\nhappen inside the children dictionary. We frivolously mix actions and states in these\nsections, since an action fully determines which state will follow. (At the end of this\nchapter, the exercises provide more detail about move making and unmaking.)\nAlphaGo Zero uses MCTS, a more advanced search algorithm than minimax,\nthat we will discuss shortly.\n7 The heuristic evaluation function is originally a linear combination of hand-crafted heuristic\nrules, such as material balance (which side has more pieces) or center control. At ﬁrst, the linear\ncombinations (coeﬃcients) were not only hand-coded, but also hand-tuned. Later they were trained\nby supervised learning [61, 613, 771, 253]. More recently, NNUE was introduced as a non-linear\nneural network to use as evaluation function in an alpha-beta framework [557].\n6.2 Tabula Rasa Self-Play Agents\n167\n1\nINF = 99999 # a value\nassumed\nto be larger\nthan\neval\never\nreturns\n2\n3\ndef\nminimax(n, d):\n4\nif d <= 0:\n5\nreturn\nheuristic_eval (n)\n6\nelif n[’type ’] == ’MAX ’:\n7\ng = -INF\n8\nfor c in n[’children ’]:\n9\ng = max(g, minimax(c, d -1))\n10\nelif n[’type ’] == ’MIN ’:\n11\ng = INF\n12\nfor c in n[’children ’]:\n13\ng = min(g, minimax(c, d -1))\n14\nreturn g\n15\n16\nprint(\"Minimax␣value:␣\", minimax(root , 2))\nListing 6.2 Minimax code [600]\nFig. 6.13 Three Lines of Play [65]\nBeyond Heuristics\nMinimax-based procedures traverse the state space by recursively following all\nactions in each state that they visit [788]. Minimax works just like a standard\ndepth-ﬁrst search procedure, such as we have been taught in our undergraduate\nalgorithms and data structures courses. It is a straightforward, rigid, approach, that\nsearches all branches of the node to the same search depth.\nTo focus the search eﬀort on promising parts of the tree, researchers have subse-\nquently introduced many algorithmic enhancements, such as alpha-beta cutoﬀs,\niterative deepening, transposition tables, null windows, and null moves [416, 602,\n422, 714, 203].\nIn the early 1990s experiments with a diﬀerent approach started, based on ran-\ndom playouts of a single line of play [6, 102, 118] (Fig. 6.13 and 6.14). In Fig. 6.14\n168\n6 Two-Agent Self-Play\nFig. 6.14 Searching a Tree versus Searching a Path\nthis diﬀerent approach is illustrated. We see a search of a single line of play ver-\nsus a search of a full subtree. It turned out that averaging many such playouts\ncould also be used to approximate the value of the root, in addition to the classic\nrecursive tree search approach. In 2006, a tree version of this approach was intro-\nduced that proved successful in Go. This algorithms was called Monte Carlo Tree\nSearch [164, 117]. Also in that year Kocsis and Szepesvári created a selection rule\nfor the exploration/exploitation trade-oﬀthat performed well and converged to the\nminimax value [419]. Their rule is called UCT, for upper conﬁdence bounds applied\nto trees.\n6.2.1.2 Monte Carlo Tree Search\nMonte Carlo Tree Search has two main advantages over minimax and alpha-beta.\nFirst, MCTS is based on averaging single lines of play, instead of recursively travers-\ning subtrees. The computational complexity of a path from the root to a leaf is\npolynomial in the search depth. The computational complexity of a tree is expo-\nnential in the search depth. Especially in applications with many actions per state\nit is much easier to manage the search time with an algorithm that expands one\npath at a time.8\nSecond, MCTS does not need a heuristic evaluation function. It plays out a line\nof play in the game from the root to an end position. In end-positions the score of\nthe game, a win or a loss, is known. By averaging many of these playouts the value\nof the root is approximated. Minimax has to cope with an exponential search tree,\nwhich it cuts oﬀafter a certain search depth, at which point it uses the heuristic\nto estimate the scores at the leaves. There are, however, games where no eﬃcient\n8 Compare chess and Go: in chess the typical number of moves in a position is 25, for Go this\nnumber is 250. A chess-tree of depth 5 has 255 = 9765625 leaves. A Go-tree of depth 5 has\n2505 = 976562500000 leaves. A depth-5 minimax search in Go would take prohibitively long; an\nMCTS search of 1000 expansions expands the same number of paths from root to leaf in both\ngames.\n6.2 Tabula Rasa Self-Play Agents\n169\nFig. 6.15 Monte Carlo Tree Search [117]\nheuristic evaluation function can be found. In this case MCTS has a clear advantage,\nsince it works without a heuristic score function.\nMCTS has proven to be successful in many diﬀerent applications. Since its\nintroduction in 2006 MCTS has transformed the ﬁeld of heuristic search. Let us see\nin more detail how it works.\nMonte Carlo Tree Search consists of four operations: select, expand, playout,\nand backpropagate (Fig. 6.15). The third operation (playout) is also called rollout,\nsimulation, and sampling. Backpropagation is sometimes called backup. Select is\nthe downward policy action trial part, backup is the upward error/learning part of\nthe algorithm. We will discuss the operations in more detail in a short while.\nMCTS is a succesful planning-based reinforcement learning algorithm, with an\nadvanced exploration/exploitation selection rule. MCTS starts from the initial state\n𝑠0, using the transition function to generate successor states. In MCTS the state\nspace is traversed iteratively, and the tree data structure is built in a step by step\nfashion, node by node, playout by playout. A typical size of an MCTS search is to\ndo 1000–10,000 iterations. In MCTS each iteration starts at the root 𝑠0, traversing a\npath in the tree down to the leaves using a selection rule, expanding a new node,\nand performing a random playout. The result of the playout is then propagated\nback to the root. During the backpropagation, statistics at all internal nodes are\nupdated. These statistics are then used in future iterations by the selection rule to\ngo to the currently most interesting part of the tree.\nThe statistics consist of two counters: the win count 𝑤and the visit count 𝑣.\nDuring backpropagation, the visit count 𝑣at all nodes that are on the path back\nfrom the leaf to the root are incremented. When the result of the playout was a win,\nthen the win count 𝑤of those nodes is also incremented. If the result was a loss,\nthen the win count is left unchanged.\nThe selection rule uses the win rate 𝑤/𝑣and the visit count 𝑣to decide whether\nto exploit high-win-rate parts of the tree or to explore low-visit-count parts. An\n170\n6 Two-Agent Self-Play\n1\ndef\nmonte_carlo_tree_search (root):\n2\nwhile\nresources_left (time , computational\npower):\n3\nleaf = select(root) # leaf = unvisited\nnode\n4\nsimulation_result = rollout(leaf)\n5\nbackpropagate (leaf , simulation_result )\n6\nreturn\nbest_child(root) # or: child\nwith\nhighest\nvisit\ncount\n7\n8\ndef\nselect(node):\n9\nwhile\nfully_expanded (node):\n10\nnode = best_child (node)\n# traverse\ndown\npath of best\nUCT\nnodes\n11\nreturn\nexpand(node.children) or node # no\nchildren/node is\nterminal\n12\n13\ndef\nrollout(node):\n14\nwhile\nnon_terminal (node):\n15\nnode = rollout_policy (node)\n16\nreturn\nresult(node)\n17\n18\ndef\nrollout_policy (node):\n19\nreturn\npick_random (node.children)\n20\n21\ndef\nbackpropagate (node , result):\n22\nif\nis_root(node) return\n23\nnode.stats = update_stats (node , result)\n24\nbackpropagate (node.parent)\n25\n26\ndef\nbest_child(node , c_param =1.0):\n27\nchoices_weights = [\n28\n(c.q / c.n) + c_param * np.sqrt ((np.log(node.n) / c.n))\n# UCT\n29\nfor c in node.children\n30\n]\n31\nreturn\nnode.children[np.argmax( choices_weights )]\nListing 6.3 MCTS pseudo-Python [117, 173]\noften-used selection rule is UCT (Sect. 6.2.1.2). It is this selection rule that governs\nthe exploration/exploitation trade-oﬀin MCTS.\nThe Four MCTS Operations\nLet us look in more detail at the four operations. Please refer to Listing 6.3 and\nFig. 6.15 [117]. As we see in the ﬁgure and the listing, the main steps are repeated\nas long as there is time left. Per step, the activities are as follows.\n1. Select In the selection step the tree is traversed from the root node down until\na leaf of the MCTS search tree is reached where a new child is selected that is\nnot part of the tree yet. At each internal state the selection rule is followed to\n6.2 Tabula Rasa Self-Play Agents\n171\ndetermine which action to take and thus which state to go to next. The UCT rule\nworks well in many applications [419].\nThe selections at these states are part of the policy 𝜋(𝑠) of actions of the state.\n2. Expand Then, in the expansion step, a child is added to the tree. In most cases\nonly one child is added. In some MCTS versions all successors of a leaf are added\nto the tree [117].\n3. Playout Subsequently, during the playout step random moves are played in a\nform of self-play until the end of the game is reached. (These nodes are not added\nto the MCTS tree, but their search result is, in the backpropagation step.) The\nreward 𝑟of this simulated game is +1 in case of a win for the ﬁrst player, 0 in\ncase of a draw, and −1 in case of a win for the opponent.9\n4. Backpropagation In the backpropagation step, reward 𝑟is propagated back up-\nwards in the tree, through the nodes that were traversed down previously. Two\ncounts are updated: the visit count, for all nodes, and the win count, depending\non the reward value. Note that in a two-agent game, nodes in the MCTS tree\nalternate color. If white has won, then only white-to-play nodes are incremented;\nif black has won, then only the black-to-play nodes.\nMCTS is on-policy: the values that are backed up are those of the nodes that\nwere selected.\nPseudocode\nMany websites contain useful resources on MCTS, including example code (see\nListing 6.3).10 The pseudocode in the listing is from an example program for game\nplay. The MCTS algorithm can be coded in many diﬀerent ways. For implementation\ndetails, see [173] and the comprehensive survey [117].\nMCTS is a popular algorithm. An easy way to use it in Python is by installing it\nfrom a pip package (pip install mcts).\nPolicies\nAt the end of the search, after the predetermined iterations have been performed, or\nwhen time is up, MCTS returns the value and the action with the highest visit count.\nAn alternative would be to return the action with the highest win rate. However,\nthe visit count takes into account the win rate (through UCT) and the number of\nsimulations on which it is based. A high win rate may be based on a low number of\nsimulations, and can thus be high variance. High visit counts will be low variance.\n9 Originally, playouts were random (the Monte Carlo part in the name of MCTS) following\nBrügmann’s [118] and Bouzy and Helmstetter’s [102] original approach. In practice, most Go\nplaying programs improve on the random playouts by using databases of small 3 × 3 patterns with\nbest replies and other fast heuristics [269, 165, 137, 702, 170]. Small amounts of domain knowledge\nare used after all, albeit not in the form of a heuristic evaluation function.\n10 https://int8.io/monte-carlo-tree-search-beginners-guide/\n172\n6 Two-Agent Self-Play\nDue to selection rule, high visit count implies high win-rate with high conﬁdence,\nwhile high win rate may be low conﬁdence [117]. The action of this initial state 𝑠0\nconstitutes the deterministic policy 𝜋(𝑠0).\nUCT Selection\nThe adaptive exploration/exploitation behavior of MCTS is governed by the selec-\ntion rule, for which often UCT is chosen. UCT is an adaptive exploration/exploitation\nrule that achieves high performance in many diﬀerent domains.\nUCT was introduced in 2006 by Kocsis and Szepesvári [419]. The paper provides\na theoretical guarantee of eventual convergence to the minimax value. The selection\nrule was named UCT, for upper conﬁdence bounds for multi-armed bandits applied\nto trees. (Bandit theory was also mentioned in Sect. 2.2.4.3).\nThe selection rule determines the way in which the current values of the children\ninﬂuence which part of the tree will be explored more. The UCT formula is\nUCT(𝑎) = 𝑤𝑎\n𝑛𝑎\n+ 𝐶𝑝\n√︄\nln 𝑛\n𝑛𝑎\n(6.1)\nwhere 𝑤𝑎is the number of wins in child 𝑎, 𝑛𝑎is the number of times child 𝑎has\nbeen visited, 𝑛is the number of times the parent node has been visited, and 𝐶𝑝≥0\nis a constant, the tunable exploration/exploitation hyperparameter. The ﬁrst term in\nthe UCT equation, the win rate 𝑤𝑎\n𝑛𝑎, is the exploitation term. A child with a high win\nrate receives through this term an exploitation bonus. The second term\n√︃\nln 𝑛\n𝑛𝑎is for\nexploration. A child that has been visited infrequently has a higher exploration term.\nThe level of exploration can be adjusted by the 𝐶𝑝constant. A low 𝐶𝑝does little\nexploration; a high 𝐶𝑝has more exploration. The selection rule then is to select\nthe child with the highest UCT sum (the familiar arg max function of value-based\nmethods).\nThe UCT formula balances win rate 𝑤𝑎\n𝑛𝑎and “newness”\n√︃\nln 𝑛\n𝑛𝑎in the selection of\nnodes to expand.11 Alternative selection rules have been proposed, such as Auer’s\nUCB1 [30, 31, 32] and P-UCT [638, 504].\nP-UCT\nWe should note that the MCTS that is used in the AlphaGo Zero program is a\nlittle diﬀerent. MCTS is used inside the training loop, as an integral part of the\n11 The square-root term is a measure of the variance (uncertainty) of the action value. The use of\nthe natural logarithm ensures that, since increases get smaller over time, old actions are selected\nless frequently. However, since logarithm values are unbounded, eventually all actions will be\nselected [743].\n6.2 Tabula Rasa Self-Play Agents\n173\nself-generation of training examples, to enhance the quality of the examples for\nevery self-play iteration, using both value and policy inputs to guide the search.\nAlso, in the AlphaGo Zero program MCTS backups rely fully on the value\nfunction approximator; no playout is performed anymore. The MC part in the name\nof MCTS, which stands for the Monte Carlo playouts, really has become a misnomer\nfor this neural network-guided tree searcher.\nFurthermore, selection in self-play MCTS is diﬀerent. UCT-based node selection\nnow also uses the input from the policy head of the trained function approximators,\nin addition to the win rate and newness. What remains is that through the UCT\nmechanism MCTS can focus its search eﬀort greedily on the part with the highest\nwin rate, while at the same time balancing exploration of parts of the tree that are\nunderexplored.\nThe formula that is used to incorporate input from the policy head of the deep\nnetwork is a variant of P-UCT [706, 530, 638, 504] (for predictor-UCT). Let us\ncompare P-UCT with UCT. The P-UCT formula adds the policy head 𝜋(𝑎|𝑠) to\nEq. 6.1\nP-UCT(𝑎) = 𝑤𝑎\n𝑛𝑎\n+ 𝐶𝑝𝜋(𝑎|𝑠)\n√𝑛\n1 + 𝑛𝑎\n.\nP-UCT adds the 𝜋(𝑎|𝑠) term specifying the probability of the action 𝑎to the explo-\nration part of the UCT formula.12\nExploration/Exploitation\nThe search process of MCTS is guided by the statistics values in the tree. MCTS\ndiscovers during the search where the promising parts of the tree are. The tree\nexpansion of MCTS is inherently variable-depth and variable-width (in contrast to\nminimax-based algorithms such as alpha-beta, which are inherently ﬁxed-depth\nand ﬁxed-width). In Fig. 6.16 we see a snapshot of the search tree of an actual MCTS\noptimization. Some parts of the tree are searched more deeply than others [808].\nAn important element of MCTS is the exploration/exploitation trade-oﬀ, that\ncan be tuned with the 𝐶𝑝hyperparameter. The eﬀectiveness of MCTS in diﬀerent\napplications depends on the value of this hyperparameter. Typical initial choices\nfor Go programs are 𝐶𝑝= 1 or 𝐶𝑝= 0.1 [117], although in AlphaGo we see\nhighly explorative choices such as 𝐶𝑝= 5. In general, experience has learned that\nwhen compute power is low, 𝐶𝑝should be low, and when more compute power is\navailable, more exploration (higher 𝐶𝑝) is advisable [117, 434].\n12 Note further the small diﬀerences under the square root (no logarithm, and the 1 in the denomi-\nnator) also change the UCT function proﬁle somewhat, ensuring correct behavior at unvisited\nactions [530].\n174\n6 Two-Agent Self-Play\n0\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n21\n20\n22\n23\n22\n20\n21\n23\n20\n21\n24\n20\n21\n22\n25\n20\n21\n22\n23\n24\n20\n26\n20\n27\n20\n28\n20\n21\n29\n30\n31\n20\n32\n33\n34\n20\n26\n20\n21\n22\n23\n27\n20\n21\n22\n23\n24\n25\n28\n20\n21\n22\n23\n24\n25\n26\n27\n29\n30\n31\n32\n33\n29\n20\n21\n21\n22\n23\n20\n24\n20\n25\n26\n27\n20\n28\n20\n30\n20\n31\n32\n33\n34\n30\n20\n21\n21\n20\n22\n20\n23\n20\n24\n20\n25\n20\n26\n20\n27\n20\n21\n22\n28\n29\n20\n21\n22\n23\n21\n20\n22\n23\n22\n23\n20\n24\n20\n21\n22\n23\n25\n20\n21\n22\n26\n20\n21\n27\n20\n28\n20\n21\n31\n20\n21\n22\n32\n20\n21\n22\n23\n24\n21\n22\n25\n26\n21\n27\n21\n28\n21\n31\n21\n22\n33\n21\n34\n21\n21\n20\n22\n23\n22\n20\n21\n23\n20\n21\n24\n20\n21\n22\n21\n22\n20\n23\n20\n25\n20\n21\n22\n26\n20\n21\n27\n20\n28\n20\n21\n22\n31\n20\n21\n22\n33\n20\n21\n34\n20\n21\n22\n25\n20\n21\n21\n20\n22\n23\n20\n24\n20\n21\n26\n20\n27\n20\n28\n20\n31\n20\n21\n22\n33\n20\n21\n22\n34\n20\n26\n20\n21\n22\n23\n24\n25\n27\n27\n20\n21\n22\n23\n28\n20\n21\n22\n23\n31\n20\n21\n22\n23\n21\n20\n22\n22\n20\n23\n20\n24\n20\n21\n22\n25\n20\n21\n22\n23\n26\n20\n21\n27\n20\n28\n20\n21\n33\n20\n21\n22\n34\n20\n21\n33\n20\n21\n22\n22\n21\n23\n21\n24\n21\n22\n25\n21\n26\n21\n22\n27\n21\n28\n21\n22\n31\n21\n22\n23\n34\n21\n21\n20\n22\n23\n22\n20\n21\n23\n24\n23\n20\n21\n22\n24\n24\n20\n21\n21\n22\n20\n23\n25\n20\n21\n22\n26\n20\n21\n27\n20\n28\n20\n21\n31\n20\n21\n22\n34\n20\n25\n20\n21\n21\n20\n22\n20\n23\n24\n20\n26\n20\n27\n20\n28\n20\n21\n31\n20\n21\n34\n20\n26\n20\n21\n21\n20\n22\n23\n24\n25\n20\n27\n28\n31\n34\n27\n20\n21\n22\n28\n20\n21\n22\n21\n20\n22\n22\n20\n23\n24\n20\n21\n25\n20\n26\n20\n21\n27\n20\n31\n20\n21\n34\n20\n21\n31\n20\n21\n22\n23\n22\n24\n22\n25\n22\n26\n22\n27\n22\n28\n22\n34\n22\n22\n21\n23\n21\n24\n21\n25\n21\n26\n27\n28\n34\n23\n21\n22\n21\n24\n21\n25\n26\n21\n27\n28\n34\n24\n21\n22\n23\n22\n21\n23\n21\n25\n21\n22\n26\n21\n22\n27\n21\n28\n21\n22\n34\n21\n25\n21\n22\n22\n21\n23\n21\n24\n21\n22\n26\n21\n22\n27\n21\n28\n21\n22\n34\n21\n26\n21\n22\n22\n21\n23\n21\n24\n21\n22\n25\n21\n27\n21\n28\n21\n22\n34\n21\n27\n21\n22\n22\n23\n24\n25\n21\n26\n21\n28\n21\n34\n21\n28\n21\n22\n23\n22\n21\n23\n21\n24\n21\n22\n25\n21\n22\n26\n21\n22\n27\n21\n34\n21\n34\n21\n22\n22\n21\n23\n24\n21\n25\n21\n26\n21\n27\n28\n21\n21\n20\n22\n23\n22\n20\n23\n20\n24\n20\n22\n25\n20\n26\n20\n22\n27\n20\n28\n20\n22\n34\n20\n22\n20\n21\n23\n24\n25\n26\n27\n28\n34\n23\n20\n21\n22\n24\n25\n26\n27\n28\n34\n24\n20\n21\n22\n23\n25\n21\n26\n21\n27\n28\n21\n34\n21\n20\n22\n23\n25\n26\n22\n20\n21\n23\n25\n23\n20\n21\n22\n25\n26\n25\n20\n21\n21\n22\n23\n26\n20\n27\n20\n28\n34\n26\n20\n21\n22\n23\n25\n20\n27\n28\n34\n27\n20\n21\n22\n23\n28\n20\n21\n20\n22\n23\n25\n26\n20\n27\n34\n34\n20\n21\n22\n23\n25\n26\n25\n20\n21\n22\n23\n24\n21\n26\n21\n27\n28\n21\n34\n21\n20\n22\n23\n24\n26\n27\n22\n20\n21\n23\n24\n23\n20\n21\n22\n24\n24\n20\n21\n21\n20\n22\n23\n26\n20\n27\n28\n20\n34\n26\n20\n21\n22\n23\n24\n27\n28\n34\n27\n20\n21\n22\n23\n24\n28\n20\n21\n21\n22\n23\n24\n26\n20\n27\n34\n34\n20\n21\n22\n23\n24\n26\n26\n20\n21\n22\n23\n24\n25\n27\n21\n20\n22\n23\n22\n20\n21\n23\n20\n21\n24\n20\n21\n22\n23\n25\n25\n20\n21\n22\n23\n24\n27\n20\n21\n22\n28\n20\n21\n22\n23\n24\n34\n20\n21\n22\n23\n27\n20\n21\n21\n20\n22\n23\n20\n24\n20\n25\n20\n26\n20\n28\n20\n34\n20\n28\n20\n21\n22\n23\n24\n25\n26\n27\n21\n20\n22\n23\n22\n20\n21\n23\n23\n20\n21\n24\n20\n21\n22\n23\n25\n26\n27\n25\n20\n21\n22\n23\n24\n26\n27\n26\n20\n21\n22\n23\n24\n25\n27\n27\n20\n21\n22\n23\n34\n20\n21\n22\n23\n34\n20\n21\n22\n23\n21\n20\n22\n22\n20\n23\n20\n24\n20\n21\n22\n25\n20\n21\n22\n26\n20\n21\n27\n20\n21\n28\n20\n21\n22\n34\n20\n21\n22\n23\n34\n20\n21\n22\n23\n33\n20\n21\n22\n23\n34\n20\n21\n31\n20\n21\n22\n32\n20\n21\n22\n33\n34\n20\n21\n31\n20\n21\n22\n23\n24\n32\n20\n21\n20\n22\n23\n20\n24\n25\n26\n20\n27\n20\n28\n20\n29\n20\n21\n30\n20\n21\n22\n31\n33\n34\n33\n20\n21\n22\n23\n24\n25\n26\n34\n20\n21\n22\n23\n24\n25\n26\nFig. 6.16 Adaptive MCTS tree [434]\nApplications\nMCTS was introduced in 2006 [165, 166, 164] in the context of computer Go pro-\ngrams, following work by Chang et al. [135], Auer et al. [31], and Cazenave and\nHelmstetter [134]. The introduction of MCTS improved performance of Go programs\nconsiderably, from medium amateur to strong amateur. Where the heuristics-based\nGNU Go program played around 10 kyu, Monte Carlo programs progressed to 2-3\ndan in a few years’ time.\nEventually, on the small 9 × 9 board, Go programs achieved very strong play.\nOn the large 19 × 19 board, performance did not improve much beyond the 4-5\ndan level, despite much eﬀort by researchers. It was thought that perhaps the large\naction space of the 19×19 board was too hard for MCTS. Many enhancements were\nconsidered, for the playout phase, and for the selection. As the AlphaGo results\nshow, a crucial enhancement was the introduction of deep function approximation.\nAfter its introduction, MCTS quickly proved successful in other applications,\nboth two agent and single agent: for video games [138], for single player applica-\ntions [117], and for many other games. Beyond games, MCTS revolutionized the\nworld of heuristic search [117]. Previously, in order to achieve best-ﬁrst search,\none had to ﬁnd a domain speciﬁc heuristic to guide the search in a smart way.\nWith MCTS this is no longer necessary. Now a general method exists that ﬁnds the\npromising parts of the search without a domain-speciﬁc heuristic, just by using\nstatistics of the search itself.\nThere is a deeper relation between UCT and reinforcement learning. Grill et\nal. [289] showed how the second term of P-UCT acts as a regularizer on model-free\npolicy optimization [4]. In particular, Jacob et al. [372] showed how MCTS can\nbe used to achieve human-like play in chess, Go, and Diplomacy, by regularizing\nreinforcement learning with supervised learning on human games.\nMCTS in AlphaGo Zero\nFor policy improvement, AlphaGo Zero uses a version of on-policy MCTS that\ndoes not use random playouts anymore. To increase exploration, Dirichlet noise\n6.2 Tabula Rasa Self-Play Agents\n175\nis added to the P-UCT value at the root node, to ensure that all moves may be\ntried. The 𝐶𝑝value of MCTS in AlphaGo is 5, heavily favoring exploration. In\nAlphaGo Zero the value depends on the stage in the learning; it grows during\nself-play. In each self-play iteration 25,000 games are played. For each move, MCTS\nperforms 1600 simulations. In total over a three-day course of training 4.9 million\ngames were played, after which AlphaGo Zero outperformed the previous version,\nAlphaGo [706].\nConclusion\nWe have taken a look into the planning part of AlphaGo Zero’s self-play architecture.\nMCTS consists of a move selection and a statistics backup phase, that corresponds\nto the behavior (trial) and learning (error) from reinforcement learning. MCTS is\nan important algorithm in reinforcement learning, and we have taken a detailed\nlook at the algorithm.\nMove-level self-play is our ﬁrst self-play procedure; it is a procedure that plays\nitself to generate its counter moves. The move-level planning is only one part of\nthe self-play picture. Just as important is the learning part. Let us have a look at\nhow AlphaGo Zero achieves its function approximation. For this, we move to the\nsecond level of self-play: the example level.\n6.2.2 Example-Level Self-Play\nMove-level self-play creates an environment for us that can play our counter-moves.\nNow we need a mechanism to learn from these actions. AlphaGo Zero follows the\nactor critic principle to approximate both value and policy functions. It approximates\nthese functions using a single deep residual neural network with a value-head and a\npolicy-head (Sect. B.2.6). The policy and the value approximations are incorporated\nin MCTS in the selection and backup step.\nIn order to learn, reinforcement learning needs training examples. The training\nexamples are generated at the self-play move level. Whenever a move is played, the\n⟨𝑠, 𝑎⟩state-action pair is recorded, and whenever a full game has been played to the\nend, the outcome 𝑧is known, and the outcome is added to all pairs of game moves,\nto create ⟨𝑠, 𝑎, 𝑧⟩triples. The triples are stored in the replay buﬀer, and sampled\nrandomly to train the value/policy net. The actual implementation in AlphaGo Zero\ncontains many more elements to improve the learning stability.\nThe player is designed to become stronger than the opponent, and this occurs at\nthe example-level. Here it uses MCTS to improve the current policy, improving it\nwith moves that are winning against the opponent’s moves.\nExample-level self-play is our second self-play procedure, the examples are\ngenerated in the self-play games and are used to train the network that is used to\nplay the moves by the two players.\n176\n6 Two-Agent Self-Play\n6.2.2.1 Policy and Value Network\nThe ﬁrst AlphaGo program uses three separate neural networks: for the rollout\npolicy, for the value, and for the selection policy [703]. AlphaGo Zero uses a single\nnetwork, that is tightly integrated in MCTS. Let us have a closer look at this single\nnetwork.\nThe network is trained on the example triples ⟨𝑠, 𝑎, 𝑧⟩from the replay buﬀer.\nThese triples contain search results of the board states of the game, and the two loss\nfunction targets: 𝑎for the actions that MCTS predicts for each board states, and 𝑧\nfor the outcome of the game (win or loss) when it came to an end. The action 𝑎is\nthe policy loss, and the outcome 𝑧is the value loss. All triples for a game consist of\nthe same outcome 𝑧, and the diﬀerent actions that were played at each state.\nAlphaGo Zero uses a dual-headed residual network (a convolutional network\nwith extra skip-links between layers, to improve regularization, see Sect. B.2.6 [321,\n132]). Policy and value loss contribute equally to the loss function [823]. The net-\nwork is trained by stochastic gradient descent. L2 regularization is used to reduce\noverﬁtting. The network has 19 hidden layers, and an input layer and two output\nlayers, for policy and value. The size of the mini-batch for updates is 2048. This\nbatch is distributed over 64 GPU workers, each with 32 data entries. The mini-batch\nis sampled uniformly over the last 500,000 self-play games (replay buﬀer). The\nlearning rate started at 0.01 and went down to 0.0001 during self-play. More details\nof the AlphaGo Zero network are described in [706].\nPlease note the size of the replay buﬀer, and the long training time. Go is a\ncomplex game, with sparse rewards. Only at the end of a long game the win or loss\nis known, and attributing this sparse reward to the many individual moves of a\ngame is diﬃcult, requiring many games to even out errors.\nMCTS is an on-policy algorithm that makes use of guidance in two places: in\nthe downward action selection and in the upward value backup. In AlphaGo Zero\nthe function approximator returns both elements: a policy for the action selection\nand a value for the backup [706].\nFor tournament-level self-play to succeed, the training process must (1) cover\nenough of the state space, must (2) be stable, and must (3) converge. Training targets\nmust be suﬃciently challenging to learn, and suﬃciently diverse. The purpose of\nMCTS is to act as a policy improver in the actor critic setting, to generate learning\ntargets of suﬃcient quality and diversity for the agent to learn.\nLet us have a closer look at these aspects, to get a broader perspective on why it\nwas so diﬃcult get self-play to work in Go.\n6.2.2.2 Stability and Exploration\nSelf-play has a long history in artiﬁcial intelligence, going back to TD-Gammon, 30\nyears ago. Let us look at the challenges in achieving a strong level of play.\nSince in AlphaGo Zero all learning is by reinforcement, the training process must\nnow be even more stable than in AlphaGo, which also used supervised learning from\n6.2 Tabula Rasa Self-Play Agents\n177\ngrandmaster games. The slightest problem in overﬁtting or correlation between\nstates can throw oﬀthe coverage, correlation, and convergence. AlphaGo Zero\nuses various forms of exploration to achieve stable reinforcement learning. Let us\nsummarize how stability is achieved.\n•\nCoverage of the sparse state space is improved by playing a large number of\ndiverse games. The quality of the states is further improved by MCTS look-ahead.\nMCTS searches for good training samples, improving the quality and diversity of\nthe covered states. The exploration part of MCTS should make sure that enough\nnew and unexplored parts of the state space are covered. Dirichlet noise is added\nat the root node and the 𝐶𝑝parameter in the P-UCT formula, that controls the\nlevel of exploration, has been set quite high, around 5 (see also Sect. 6.2.1.2).\n•\nCorrelation between subsequent states is reduced through the use of an experi-\nence replay buﬀer, as in DQN and Rainbow algorithms. The replay buﬀer breaks\ncorrelation between subsequent training examples. Furthermore, the MCTS\nsearch also breaks correlation, by searching deep in the tree to ﬁnd better states.\n•\nConvergence of the training is improved by using on-policy MCTS, and by taking\nsmall training steps. Since the learning rate is small, training target stability is\nhigher, and the risk of divergence is reduced. A disadvantage is that convergence\nis quite slow and requires many training games.\nBy using these measures together, stable generalization and convergence are\nachieved. Although self-play is conceptually simple, achieving stable and high-\nquality self-play in a game as complex and sparse as Go, required slow training\nwith a large number of games, and quite some hyperparameter tuning. There\nare many hyperparameters whose values must be set correctly, for the full list,\nsee [706]. Although the values are published [706], the reasoning behind the val-\nues is not always clear. Reproducing the AlphaGo Zero results is not easy, and\nmuch time is spent in tuning and experimenting to reproduce the AphaGo Zero\nresults [605, 729, 768, 776, 866].\nTwo Views\nAt this point it is useful to step back and reﬂect on the self-play architecture.\nThere are two diﬀerent views. The one view, planning-centric, which we have\nfollowed so far, is of a searcher that is helped by a learned evaluation function\n(which trains on examples from games played against itself). In addition, there is\nmove-level self-play (opponent’s moves are generated with an inverted replica of\nitself) and there is tournament-level self-play (by the value learner).\nThe alternative view, learning-centric, is that a policy is learned by generating\ngame examples from self-play. In order for these examples to be of high quality, the\npolicy-learning is helped by a policy improver, a planning function that performs\nlookahead to create better learning targets (and the planning is performed by making\nmoves by a copy of the player). In addition, there is tournament-level self-play (by\nthe policy learner) and there is move-level self-play (by the policy improver).\n178\n6 Two-Agent Self-Play\nThe diﬀerence in viewpoint is who comes ﬁrst: the planning viewpoint favors\nthe searcher, and the learner is there to help the planner; the reinforcement learning\nviewpoint favors the policy learner, and the planner is there to help improve the\npolicy. Both viewpoints are equally valid, and both viewpoints are equally valuable.\nKnowing of the other viewpoint deepens our understanding of how these complex\nself-play algorithms work.\nThis concludes our discussion of the second type of self-play, the example-level,\nand we move on to the third type: tournament-level self-play.\n6.2.3 Tournament-Level Self-Play\nAt the top level, a tournament of self-play games is played between the two (iden-\ntical) players. The player is designed to increase in strength, learning from the\nexamples at the second level, so that the player can achieve a higher level of play.\nIn tabula rasa self-play, the players start from scratch. By becoming progressively\nstronger, they also become stronger opponents for each other, and their mutual level\nof play can increase. A virtuous cycle of ever increasing intelligence will emerge.\nFor this ideal of artiﬁcial intelligence to become reality, many stars have to line\nup. After TD-Gammon, many researchers have tried to achieve this goal in other\ngames, but were unsuccessful.\nTournament-level self-play is only possible when move-level self-play and\nexample-level self-play work. For move-level self-play to work, both players need\nto have access to the transition function, which must be completely accurate. For\nexample-level self-play to work, the player architecture must be such that it is able\nto learn a stable policy of a high quality (MCTS and the network have to mutually\nimprove eachother).\nTournament-level self-play is the third self-play procedure, where a tournament\nis created with games starting from easy learning tasks, changing to harder tasks,\nincreasing all the way to world champion level training. This third procedure allows\nreinforcement learning to transcend the level of play (“intelligence”) of previous\nteachers.\nCurriculum Learning\nAs mentioned before, the AlphaGo eﬀort consists of three programs: AlphaGo,\nAlphaGo Zero, and AlphaZero. The ﬁrst AlphaGo program used supervised learning\nbased on grandmaster games, followed by reinforcement learning on self-play games.\nThe second program, AlphaGo Zero, used reinforcement learning only, in a self-\nplay architecture that starts from zero knowledge. The ﬁrst program trained many\nweeks, yet the second program needed only a few days to become stronger than\nthe ﬁrst [706, 703].\n6.2 Tabula Rasa Self-Play Agents\n179\nWhy did the self-play approach of AlphaGo Zero learn faster than the original\nAlphaGo that could beneﬁt from all the knowledge of Grandmaster games? Why is\nself-play faster than a combination of supervised and reinforcement learning? The\nreason is a phenomenon called curriculum learning: self-play is faster because it\ncreates a sequence of learning tasks that are ordered from easy to hard. Training\nsuch an ordered sequence of small tasks is quicker than one large unordered task.\nCurriculum learning starts the training process with easy concepts before the\nhard concepts are learned; this is, of course, the way in which humans learn. Before\nwe learn to run, we learn to walk; before we learn about multiplication, we learn\nabout addition. In curriculum learning the examples are ordered in batches from\neasy to hard. Learning such an ordered sequence of batches goes better since under-\nstanding the easy concepts helps understanding of the harder concepts; learning\neverything all at once typically takes longer and may result in lower accuracy.13\n6.2.3.1 Self-Play Curriculum Learning\nIn ordinary deep reinforcement learning the network tries to solve a ﬁxed problem\nin one large step, using environment samples that are not sorted from easy to hard.\nWith examples that are not sorted, the program has to achieve the optimization step\nfrom no knowledge to human-level play in one big, unsorted, leap, by optimizing\nmany times over challenging samples where the error function is large. Overcoming\nsuch a large training step (from beginner to advanced) costs much training time.\nIn contrast, in AlphaGo Zero, the network is trained in many small steps, starting\nagainst a very weak opponent, just as a human child learns to play the game by\nplaying against a teacher that plays simple moves. As our level of play increases,\nso does the diﬃculty of the moves that our teacher proposes to us. Subsequently,\nharder problems are generated and trained for, reﬁning the network that has already\nbeen pretrained with the easier examples.\nSelf-play naturally generates a curriculum with examples from easy to hard. The\nlearning network is always in lock step with the training target—errors are low\nthroughout the training. As a consequence, training times go down and the playing\nstrength goes up.\n6.2.3.2 Supervised and Reinforcement Curriculum Learning\nCurriculum learning has been studied before in psychology and education science.\nSelfridge et al. [689] ﬁrst connected curriculum learning to machine learning, where\nthey trained the proverbial Cartpole controller. First they trained the controller on\nlong and light poles, while gradually moving towards shorter and heavier poles.\nSchmidhuber [672] proposed a related concept, to improve exploration for world\n13 Such a sequence of related learning tasks corresponds to a meta-learning problem. In meta-\nlearning the aim is to learn a new task fast, by using the knowledge learned from previous, related,\ntasks; see Chap. 9.\n180\n6 Two-Agent Self-Play\nFig. 6.17 Eﬀectiveness of a Sorted Curriculum [833]\nmodels by artiﬁcial curiosity. Curriculum learning was subsequently applied to\nmatch the order of training examples to the growth in model capacity in diﬀerent\nsupervised learning settings [225, 432, 77]. Another related development is in devel-\nopmental robotics, where curriculum learning can help to self-organize open-ended\ndevelopmental trajectories [578], related to intrinsic motivation (see Sect. 8.3.2).\nThe AMIGo approach uses curriculum learning to generate subgoals in hierarchical\nreinforcement learning [125].\nTo order the training examples from easy to hard, we need a measure to quantify\nthe diﬃculty of the task. One idea is to use the minimal loss with respect to some of\nthe upper layers of a high-quality pretrained model [833]. In a supervised learning\nexperiment, Weinshall et al. compared the eﬀectiveness of curriculum learning on\na set of test images (5 images of mammals from CIFAR100). Figure 6.17 shows the\naccuracy of a curriculum ordering (green), no curriculum (blue), randomly ordered\ngroups (yellow) and the labels sorted in reverse order (red). Both networks are\nregular networks with multiple convolutional layers followed by a fully connected\nlayer. The large network has 1,208,101 parameters, the small network has 4,557\nparameters. We can clearly see the eﬀectiveness of ordered learning [833].\nProcedural Content Generation\nFinding a good way to order the sequence of examples is often diﬃcult. A possible\nmethod to generate a sequence of tasks that are related is by using procedural con-\ntent generation (PCG) [692, 541]. Procedural content generation uses randomized\nalgorithms to generate images and other content for computer games; the diﬃculty\nof the examples can often be controlled. It is frequently used to automatically gen-\n6.2 Tabula Rasa Self-Play Agents\n181\nerate diﬀerent levels in games, so that they do not have to be all created manually\nby the game designers and programmers [715, 781].14\nThe Procgen benchmark suite has been built upon procedurally generated\ngames [158]. Another popular benchmark is the General video game AI competition\n(GVGAI) [479]. Curriculum learning reduces overﬁtting to single tasks. Justesen et\nal. [388] have used GVGAI to show that a policy easily overﬁts to speciﬁc games,\nand that training over a curriculum improves its generalization to levels that were\ndesigned by humans. MiniGrid is a procedurally generated world that can be used\nfor hierarchical reinforcement learning [143, 621].\nActive Learning\nCurriculum learning is related also related to active learning.\nActive learning is a type of machine learning that is in-between supervised and\nreinforcement learning. Active learning is relevant when labels are in principle\navailable (as in supervised learning) but at a cost.\nActive learning performs a kind of iterative supervised learning, in which the\nagent can choose to query which labels to reveal during the learning process. Active\nlearning is related to reinforcement learning and to curriculum learning, and is for\nexample of interest for studies into recommender systems, where acquiring more\ninformation may come at a cost [691, 640, 178].\nSingle-Agent Curriculum Learning\nCurriculum learning has been studied for many years. A problem is that it is diﬃcult\nto ﬁnd an ordering of tasks from easy to hard in most learning situations. In two-\nplayer self-play the ordering comes natural, and the successes have inspired recent\nwork on single-agent curriculum learning. For example, Laterre et al. introduce\nthe Ranked Reward method for solving bin packing problems [455] and Wang et\nal. presented a method for Morpion Solitaire [824]. Feng et al. use an AlphaZero\nbased approach to solve hard Sokoban instances [239]. Their model is an 8 block\nstandard residual network, with MCTS as planner. They create a curriculum by\nconstructing simpler subproblems from hard instances, using the fact that Sokoban\nproblems have a natural hierarchical structure. This approach was able to solve\nharder Sokoban instances than had been solved before. Florensa et al. [248] study\nthe generation of goals for curriculum learning using a generator network (GAN).\n14 See also generative adversarial networks and deep dreaming, for a connectionist approach to\ncontent generation, Sect. B.2.6.\n182\n6 Two-Agent Self-Play\nName\nApproach\nRef\nTD-Gammon Tabula rasa self-play, shallow network, small alpha-beta search\n[763]\nAlphaGo\nSupervised, self-play, 3×CNN, MCTS\n[703]\nAlphaGo Zero Tabula rasa self-play, dual-head-ResNet, MCTS\n[706]\nAlphaZero\nTabula rasa self-play, dual-head-ResNet, MCTS on Go, chess, shogi [704]\nTable 6.2 Self-Play Approaches\nConclusion\nAlthough curriculum learning has been studied in artiﬁcial intelligence and in\npsychology for some time, it has not been a popular method, since it is diﬃcult\nto ﬁnd well-sorted training curricula [518, 519, 827]. Due to the self-play results,\ncurriculum learning is now attracting more interest, see [552, 837]. Work is reported\nin single-agent problems [552, 239, 198, 455], and in multi-agent games, as we will\nsee in Chap. 7.\nAfter this detailed look at self-play algorithms, it is time to look in more detail\nat the environments and benchmarks for self-play.\n6.3 Self-Play Environments\nProgress in reinforcement learning is determined to a large extent by the application\ndomains that provide the learning challenge. The domains of checkers, backgammon,\nand especially chess and Go, have provided highly challenging domains, for which\nsubstantial progress was achieved, inspiring many researchers.\nThe previous section provided an overview of the planning and learning algo-\nrithms. In this section we will have a closer look at the environments and systems\nthat are used to benchmark these algorithms.\nTable 6.2 lists the AlphaGo and self-play approaches that we discuss in this\nchapter. First we will discuss the three AlphaGo programs, and then we will list\nopen self-play frameworks. We will start with the ﬁrst program, AlphaGo.\n6.3.1 How to Design a World Class Go Program?\nFigure 6.18 shows the playing strength of traditional programs (right panel, in red)\nand diﬀerent versions of the AlphaGo programs, in blue. We see how much stronger\nthe 2015, 2016, and 2017 versions of AlphaGo are than the earlier heuristic minimax\nprogram GnuGo, and two MCTS-only programs Pachi and Crazy Stone.\nHow did the AlphaGo authors design such a strong Go program? Before AlphaGo,\nthe strongest programs used the Monte Carlo Tree Search planning algorithm,\nwithout neural networks. For some time, neural networks were considered to be\n6.3 Self-Play Environments\n183\nFig. 6.18 Performance of AlphaGo Zero [706]\nFig. 6.19 All AlphaGo Networks [703]\ntoo slow for use as value function in MCTS, and random playouts were used, often\nimproved with small pattern-based heuristics [269, 268, 138, 166, 227]. Around 2015\na few researchers tried to improve performance of MCTS by using deep learning\nevaluation functions [155, 22, 267]. These eﬀorts were strenghthened by the strong\nresults in Atari [523].\nThe AlphaGo team also tried to use neural networks. Except for backgammon,\npure self-play approaches had not been shown to work well, and the AlphaGo\nteam did the sensible thing to pretrain the network with the games of human\ngrandmasters, using supervised learning. Next, a large number of self-play games\nwere used to further train the networks. In total, no less than three neural networks\nwere used: one for the MCTS playouts, one for the policy function, and one for the\nvalue function [703].\n184\n6 Two-Agent Self-Play\nThus, the original AlphaGo program consisted of three neural networks and\nused both supervised learning and reinforcement learning. The diagram in Fig. 6.19\nillustrates the AlphaGo architecture. Although this design made sense at the time\ngiven the state of the art of the ﬁeld, and although it did convincingly beat the three\nstrongest human Go players, managing and tuning such a complicated piece of\nsoftware is quite diﬃcult. The authors of AlphaGo tried to improve performance\nfurther by simplifying their design. Could TD-gammon’s elegant self-play-only\ndesign be replicated in Go after all?\nIndeed, a year later a reinforcement learning-only version was ready that learned\nto play Go from zero knowledge, tabula rasa: no grandmaster games, only self-play,\nand just a single neural network [706]. Surprisingly, this simpler version played\nstronger and learned faster. The program was called AlphaGo Zero, since it learned\nfrom zero-knowledge; not a single grandmaster game was learned from, nor was\nthere any heuristic domain knowledge hand coded into the program.\nThis new result, tabula rasa learning in Go with a pure self-play design, inspired\nmuch further research in self-play reinforcement learning.\n6.3.2 AlphaGo Zero Performance\nIn their paper Silver et al. [706] describe that learning progressed smoothly through-\nout the training. AlphaGo Zero outperformed the original AlphaGo after just 36\nhours. The training time for the version of AlphaGo that played Lee Sedol was\nseveral months. Furthermore, AlphaGo Zero used a single machine with 4 tensor\nprocessing units, whereas AlphaGo Lee was distributed over many machines and\nused 48 TPUs.15 Figure 6.18 shows the performance of AlphaGo Zero. Also shown\nis the performance of the raw network, without MCTS search. The importance of\nMCTS is large, around 2000 Elo points.16\nAlphaGo Zero’s reinforcement learning is truly learning Go knowledge from\nscratch, and, as the development team discovered, it did so in a way similar to\nhow humans are discovering the intricacies of the game. In their paper [706] they\npublished a picture of how this knowledge acquisition progressed (Fig. 6.20).\nJoseki are standard corner openings that all Go players become familiar with\nas they learn to play the game. There are beginner’s and advanced joseki. Over\nthe course of its learning, AlphaGo Zero did learn joseki, and it learned them\nfrom beginner to advanced. It is interesting to see how it did so, as it reveals the\nprogression of AlphaGo Zero’s Go intelligence. Figure 6.20 shows sequences from\ngames played by the program. Not to anthropomorphize too much,17 but you can\nsee the little program getting smarter.\n15 TPU stands for tensor processing unit, a low-precision design speciﬁcally developed for fast\nneural network processing.\n16 The basis of the Elo rating is pairwise comparison [226]. Elo is often used to compare playing\nstrength in board games.\n17 Treat as if human\n6.3 Self-Play Environments\n185\nFig. 6.20 AlphaGo Zero is Learning Joseki in a Curriculum from Easy to Hard [706]\nThe top row shows ﬁve joseki that AlphaGo Zero discovered. The ﬁrst joseki\nis one of the standard beginner’s openings in Go theory. As we move to the right,\nmore diﬃcult joseki are learned, with stones being played in looser conﬁgurations.\nThe bottom row shows ﬁve joseki favored at diﬀerent stages of the self-play training.\nIt starts with a preference for a weak corner move. After 10 more hours of training,\na better 3-3 corner sequence is favored. More training reveals more, and better,\nvariations.\nAlphaGo Zero discovered a remarkable level of Go knowledge during its self-play\ntraining process. This knowledge included not only fundamental elements of human\nGo knowledge, but also nonstandard strategies beyond the scope of traditional Go\nknowledge.\nFor a human Go player, it is remarkable to see this kind of progression in com-\nputer play, reminding them of the time when they discovered these joseki themselves.\nWith such evidence of the computer’s learning, it is hard not to anthropomorphize\nAlphaGo Zero.\n6.3.3 AlphaZero\nThe AlphaGo story does not end with AlphaGo Zero. A year after AlphaGo Zero, a\nversion was created with diﬀerent input and output layers that learned to play chess\nand shogi (also known as Japanese chess, Fig. 6.21). AlphaZero uses the same MCTS\nand deep reinforcement learning architecture as for learning to play Go (the only\ndiﬀerences are the input and output layers) [704]. This new program, AlphaZero,\nbeat the strongest chess and shogi programs, Stockﬁsh and Elmo. Both these pro-\ngrams followed a conventional heuristic minimax design, optimized by hand and\n186\n6 Two-Agent Self-Play\nFig. 6.21 A Shogi Board\nFig. 6.22 Elo rating of AlphaZero in Chess, Shogi, and Go [704]\nmachine learning, and improved with many heuristics for decades. AlphaZero used\nzero knowledge, zero grandmaster games, and zero hand-crafted heuristics, yet it\nplayed stronger. The AlphaZero architecture allows not only very strong play, but\nis also a general architecture, suitable for three diﬀerent games.18\nThe Elo rating of AlphaZero in chess, shogi, and Go is shown in Fig. 6.22, [706,\n703]. AlphaZero is stronger than the other programs. In chess the diﬀerence is\nthe smallest. In this ﬁeld the program has beneﬁted from a large community of\nresearchers that have worked intensely on improving performance of the heuristic\nalpha-beta approach. For shogi the diﬀerence is larger.\nGeneral Game Architecture\nAlphaZero can play three diﬀerent games with the same architecture. The three\ngames are quite diﬀerent. Go is a static game of strategy. Stones do not move and are\nrarely captured. Stones, once played, are of strategic importance. In chess the pieces\n18 Although an AlphaZero version that has learned to play Go, cannot play chess. It has to re-learn\nchess from scratch, with diﬀerent input and output layers.\n6.3 Self-Play Environments\n187\nName\nType\nURL\nRef.\nAlphaZero\nGeneral\nAlphaZero in Python\nhttps://github.com/suragnair/alpha-zero-\ngeneral\n[768]\nELF\nGame framework\nhttps://github.com/pytorch/ELF\n[776]\nLeela\nAlphaZero for Chess, Go\nhttps://github.com/LeelaChessZero/lczero [589]\nPhoenixGo AlphaZero-based Go prog. https://github.com/Tencent/PhoenixGo\n[866]\nPolyGames Env. for Zero learning\nhttps://github.com/facebookincubator/\nPolygames\n[133]\nTable 6.3 Self-learning environments\nmove. Chess is a dynamic game where tactics are important. Chess also features\nsudden death, a check-mate can occur in the middle of the game by capturing the\nking. Shogi is even more dynamic since captured pieces can be returned to the\ngame, creating even more complex game dynamics.\nIt is testament to the generality of AlphaZero’s architecture, that games that\ndiﬀer so much in tactics and strategy can be learned so successfully. Conventional\nprograms must be purposely developed for each game, with diﬀerent search hyper-\nparameters and diﬀerent heuristics. Yet the MCTS/ResNet self-play architecture is\nable to learn all three from scratch.\n6.3.4 Open Self-Play Frameworks\nTabula rasa learning for the game of Go is a remarkable achievement that inspired\nmany researchers. The code of AlphaGo Zero and AlphaZero, however, is not public.\nFortunately, the scientiﬁc publications [706, 704] provide many details, allowing\nother researchers to reproduce similar results.\nTable 6.3 summarizes some of the self-learning environments, which we will\nbrieﬂy discuss.\n•\nA0G: AlphaZero General Thakoor et al. [768] created a self-play system called\nAlphaZero General (A0G).19 It is implemented in Python for TensorFlow, Keras,\nand PyTorch, and suitably scaled down for smaller computational resources. It\nhas implementations for 6×6 Othello, tic tac toe, gobang, and connect4, all small\ngames of signiﬁcantly less complexity than Go. Its main network architecture is\na four layer CNN followed by two fully connected layers. The code is easy to\nunderstand in an afternoon of study, and is well suited for educational purposes.\nThe project write-up provides some documentation [768].\n•\nFacebook ELF ELF stands for Extensible Lightweight Framework. It is a framework\nfor game research in C++ and Python [776]. Originally developed for real-time\nstrategy games by Facebook, it includes the Arcade Learning Environment and\n19 https://github.com/suragnair/alpha-zero-general\n188\n6 Two-Agent Self-Play\nthe Darkforest20 Go program [778]. ELF can be found on GitHub.21 ELF also\ncontains the self-play program OpenGo [777], a reimplementation of AlphaGo\nZero (in C++).\n•\nLeela Another reimplementation of AlphaZero is Leela. Both a chess and a Go\nversion of Leela exist. The chess version is based on chess engine Sjeng. The\nGo22 version is based on Go engine Leela. Leela does not come with trained\nweights of the network. Part of Leela is a community eﬀort to compute these\nweights.\n•\nPhoenixGo PhoenixGo is a strong self-play Go program by Tencent [866]. It is\nbased on the AlphaGo Zero architecture.23 A trained network is available as\nwell.\n•\nPolygames PolyGames [133] is an environment for Zero-based learning (MCTS\nwith deep reinforcement learning) inspired by AlphaGo Zero. Relevant learning\nmethods are implemented, and bots for hex, Othello, and Havannah have been\nimplemented. PolyGames can be found on GitHub.24 A library of games is\nprovided, as well as a checkpoint zoo of neural network models.\n6.3.5 Hands On: Hex in Polygames Example\nLet us get some hands-on experience with MCTS-based self-play. We will implement\nthe game of Hex with the PolyGames suite. Hex is a simple and fun board game\ninvented independently by Piet Hein and John Nash in the 1940s. Its simplicity\nmakes it easy to learn and play, and also a popular choice for mathematical analysis.\nThe game is played on a hexagonal board, player A wins if its moves connect the\nright to the left side, and player B wins if top connects to bottom (see Fig. 6.23;\nimage by Wikimedia). A simple page with resources is here;25 extensive strategy\nand background books have been written about hex [319, 115]. We use hex because\nit is simpler than Go, to get you up to speed quickly with self-play learning; we will\nalso use PolyGames.\nClick on the link26 and start by reading the introduction to PolyGames on\nGitHub. Download the paper [133] and familiarize yourself with the concepts\nbehind Polygames. Clone the repository and build it by following the instruc-\ntions. Polygames uses PyTorch,27 so install that too (follow the instructions on the\nPolygames page).\n20 https://github.com/facebookresearch/darkforestGo\n21 https://github.com/pytorch/ELF\n22 https://github.com/gcp/leela-zero\n23 https://github.com/Tencent/PhoenixGo\n24 https://github.com/facebookincubator/Polygames\n25 https://www.maths.ed.ac.uk/~csangwin/hex/index.html\n26 https://github.com/facebookincubator/Polygames\n27 https://pytorch.org\n6.3 Self-Play Environments\n189\nFig. 6.23 A Hex Win for Blue\nPolygames is interfaced via the pypolygames Python package. The games, such\nas Hex, can be found in src/games and are coded in C++ for speed. The command\npypolygames train\nis used to train a game and a model.\nThe command\npypolygames eval\nis used to use a previously trained model.\nThe command\npypolygames human\nallows a human to play against a trained model.\nType\npython -m pypolygames {train,eval,traineval,human} --help\nfor help with each of the commands train, eval, traineval, or human.\nA command to start training a Hex model with the default options is:\npython -m pypolygames train --game_name=\"Hex\"\nTry loading a pre-trained model from the zoo. Experiment with diﬀerent training\noptions, and try playing against the model that you just trained. When everything\nworks, you can also try training diﬀerent games. Note that more complex games\nmay take (very) long to train.\n190\n6 Two-Agent Self-Play\nSummary and Further Reading\nWe will now summarize the chapter, and provide pointers for further reading.\nSummary\nFor two-agent zero-sum games, when the transition function is given by the rules\nof the game, a special kind of reinforcement learning becomes possible. Since the\nagent can perfectly simulate the moves of the opponent, accurate planning far into\nthe future becomes possible, allowing strong policies to be learned. Typically, the\nsecond agent becomes the environment. Previously environments were static, but\nthey will now evolve as the agent is learning, creating a virtuous cycle of increasing\n(artiﬁcial) intelligence in agent (and environment). The promise of this self-play\nsetup is to achieve high levels of intelligence in a speciﬁc ﬁeld. The challenges\nto overcome instability, however, are large, since this kind of self-play combines\ndiﬀerent kinds of unstable learning methods. Both TD-Gammon and AlphaGo Zero\nhave overcome these challenges, and we have described their approach in quite\nsome detail.\nSelf-play is a combination of planning, learning, and a self-play loop. The self-\nplay loop in AlphaGo Zero uses MCTS to generate high-quality examples, which are\nused to train the neural net. This new neural net is then used in a further self-play\niteration to generate more diﬃcult games, and reﬁne the network further (and\nagain, and again, and again). Alpha(Go) Zero thus learns starting at zero knowledge,\ntabula rasa.\nSelf-play makes use of many reinforcement learning techniques. In order to\nensure stable learning, exploration is important. MCTS is used for deep planning.\nThe exploration parameter in MCTS is set high, and convergent training is achieved\nby a low learning rate 𝛼. Because of these hyperparameter settings, and because\nof the sparse rewards in Go, many games have to be played. The computational\ndemands of stable self-play are large.\nAlphaGo Zero uses function approximation of two functions: value and policy.\nPolicy is used to help guide action selection in the P-UCT selection operation in\nMCTS, and value is used instead of random playouts to provide the value function\nat the leaves of the MCTS tree. MCTS has been changed signiﬁcantly to work in\nthe self-play setting. Gone are the random playouts that gave MCTS the name\nMonte Carlo, and much of the performance is due to a high-quality policy and value\napproximation residual network.\nOriginally, the AlphaGo program (not AlphaGo Zero) used grandmaster games\nin supervised learning in addition to using reinforcement learning; it started from\nthe knowledge of grandmaster games. Next came AlphaGo Zero, which does not use\ngrandmaster games or any other domain speciﬁc knowledge. All learning is based\non reinforcement learning, playing itself to build up the knowledge of the game\nfrom zero knowledge. A third experiment has been published, called AlphaZero\n6.3 Self-Play Environments\n191\n(without the “Go”). In this paper the same network architecure and MCTS design\n(and the same learning hyperparameters) were used to learn three games: chess,\nshogi, and Go. This presented the AlphaZero architecture as a general learning\narchitecture, stronger than the best alpha-beta-based chess and shogi programs.\nInterestingly, the all-reinforcement learning AlphaGo Zero architecture was not\nonly stronger than the supervised/reinforcement hybrid AlphaGo, but also faster:\nit learned world champion level play in days, not weeks. Self-play learns quickly\nbecause of curriculum learning. It is more eﬃcient to learn a large problem in many\nsmall steps, starting with easy problems, ending with hard ones, then in one large\nstep. Curriculum learning works both for humans and for artiﬁcial neural networks.\nFurther Reading\nOne of the main interests of artiﬁcial intelligence is the study of how intelligence\nemerges out of simple, basic, interactions. In self-learning systems this is happen-\ning [464].\nThe work on AlphaGo is a landmark achievement in artiﬁcial intelligence. The\nprimary sources of information for AlphaGo are the three AlphaGo/AlphaZero\npapers by Silver et al. [703, 706, 704]. The systems are complex, and so are the\npapers and their supplemental methods sections. Many blogs have been written\nabout AlphaGo that are more accessible. A movie has been made about AlphaGo.28\nThere are also explanations on YouTube.29\nA large literature on minimax and minimax enhancements for games exists,\nan overview is in [600]. A book devoted to building your own state-of-the-art\nself-learning Go bot is Deep Learning and the Game of Go by Pumperla and Fergu-\nson [612], which came out before PolyGames [133].\nMCTS has been a landmark algorithm by itself in artiﬁcial intelligence [164,\n117, 419]. In the contexts of MCTS, many researchers worked on combining MCTS\nwith learned patterns, especially to improve the random rollouts of MCTS. Other\ndevelopments, such as asymmetrical and continuous MCTS, are [531, 530], or\nparallelizations such as [516].\nSupervised learning on grandmaster games was used to improve playouts and\nalso to improve UCT selection. Gelly and Silver published notable works in this\narea [269, 708, 268]. Graf et al. [284] describe experiments with adaptive playouts\nin MCTS with deep learning. Convolutional neural nets were also used in Go by\nClark and Storkey [154, 155], who had used a CNN for supervised learning from a\ndatabase of human professional games, showing that it outperformed GNU Go and\nscored wins against Fuego, a strong open source Go program [227] based on MCTS\nwithout deep learning.\n28 https://www.alphagomovie.com\n29 https://www.youtube.com/watch?v=MgowR4pq3e8\n192\n6 Two-Agent Self-Play\nTesauro’s success inspired many others to try temporal diﬀerence learning.\nWiering et al. and Van der Ree [840, 794] report on self-play and TD learning in\nOthello and Backgammon. The program Knightcap [60, 61] and Beal et al. [62]\nalso use temporal diﬀerence learning on evaluation function features. Arenz [25]\napplied MCTS to chess. Heinz reported on self-play experiments in chess [327].\nSince the AlphaGo results many other applications of machine learning have\nbeen shown to be successful. There is interest in theoretical physics [644, 583], chem-\nistry [385], and pharmacology, speciﬁcally for retrosynthetic molecular design [687]\nand drug design [803]. High-proﬁle results have been achieved by AlphaFold, a\nprogram that can predict protein structures [385, 690].\nTo learn more about curriculum learning, see [837, 77, 502, 248]. Wang et al. [823]\nstudy the optimization target of a dual-headed self-play network in AlphaZeroGen-\neral. The success of self-play has led to interest in curriculum learning in single-\nagent problems [239, 198, 213, 455]. The relation between classical single-agent and\ntwo-agent search is studied by [664].\nExercises\nTo review your knowledge of self-play, here are some questions and exercises. We\nstart with questions to check your understanding of this chapter. Each question is a\nclosed question where a simple, one sentence answer is possible.\nQuestions\n1. What are the diﬀerences between AlphaGo, AlphaGo Zero, and AlphaZero?\n2. What is MCTS?\n3. What are the four steps of MCTS?\n4. What does UCT do?\n5. Give the UCT formula. How is P-UCT diﬀerent?\n6. Describe the function of each of the four operations of MCTS.\n7. How does UCT achieve trading oﬀexploration and exploitation? Which inputs\ndoes it use?\n8. When 𝐶𝑝is small, does MCTS explore more or exploit more?\n9. For small numbers of node expansions, would you prefer more exploration or\nmore exploitation?\n10. What is a double-headed network? How is it diﬀerent from regular actor critic?\n11. Which three elements make up the self-play loop? (You may draw a picture.)\n12. What is tabula rasa learning?\n13. How can tabula rasa learning be faster than reinforcement learning on top of\nsupervised learning of grandmaster games?\n14. What is curriculum learning?\n6.3 Self-Play Environments\n193\nImplementation: New or Make/Undo\nYou may have noticed that the minimax and MCTS pseudocode in the ﬁgures lacks\nimplementation details for performing actions, to arrive at successor states. Such\nboard manipulation and move making details are important for creating a working\nprogram.\nGame playing programs typically call the search routine with the current board\nstate, often indicated with parameter n for the new node. This board can be created\nand allocated anew in each search node, in a value-passing style (local variable).\nAnother option is to pass a reference to the board, and to apply a makemove oper-\nation on the board, placing the stone on the board before the recursive call, and\nan undomove operation removing the stone from the board when it returns back\nout of the recursion (global variable). This reference-passing style may be quicker\nif allocating the memory for a new board is an expensive operation. It may also\nbe more diﬃcult to implement correctly, since the makemove/undomove protocol\nmust be followed strictly on all relevant places in the code. If capture moves cause\nmany changes to the board, then these must be remembered for the subsequent\nundo.\nFor parallel implementations in a shared memory at least all parallel threads\nmust have their own copy of a value-passing style board. (On a distributed memory\ncluster the separate machines will have their own copy of the board by virtue of\nthe distributed memory.)\nExercises\nFor the programming exercises we use PolyGames. See the previous section on how\nto install PolyGames. If training takes a long time, consider using the GPU support\nof Polygames and Pytorch.\n1. Hex Install PolyGames and train a Hex player with self-play. Experiment with\ndiﬀerent board sizes. Keep the training time constant, and draw a graph where\nyou contrast playing strength against board size. Do you see a clear correlation\nthat the program is stronger on smaller boards?\n2. Visualize Install the visualization support torchviz. Visualize the training pro-\ncess using the draw_model script.\n3. Hyperparameters Use diﬀerent models, and try diﬀerent hyperparameters, as\nspeciﬁed in the PolyGames documentation.\n4. MCTS Run an evaluation tournament from the trained Hex model against a\npure MCTS player. See below for tips on make and undo of moves. How many\nnodes can MCTS search in a reasonable search time? Compare the MCTS Hex\nplayer against the self-play player. How many games do you need to play to\nhave statistically signiﬁcant results? Is the random seed randomized or ﬁxed?\nWhich is stronger: MCTS or trained Hex?\nChapter 7\nMulti-Agent Reinforcement Learning\nOn this planet, in our societies, millions of people live and work together. Each\nindividual has their own individual set of goals and performs their actions accord-\ningly. Some of these goals are shared. When we want to achieve shared goals we\norganize ourselves in teams, groups, companies, organizations and societies. In\nmany intelligent species—humans, mammals, birds, insects—impressive displays\nof collective intelligence emerge from such organization [848, 348, 686]. We are\nlearning as individuals, and we are learning as groups. This setting is studied in\nmulti-agent learning: through their independent actions, agents learn to interact\nand to compete and cooperate with other agents, and form groups.\nMost research in reinforcement learning has focused on single agent problems.\nProgress has been made in many topics, such as path-ﬁnding, robot locomotion, and\nvideo games. In addition, research has been performed in two-agent problems, such\nas competitive two-person board games. Both single-agent and two-agent problems\nare questions of optimization. The goal is to ﬁnd the policy with the highest reward,\nthe shortest path, and the best moves and counter moves. The basic setting is one\nof reward optimization in the face of natural adversity or competitors, modeled by\nthe environment.\nAs we move closer toward modeling real-world problems, we encounter another\ncategory of sequential decision making problems, and that is the category of multi-\nagent decision problems. Multi-agent decision making is a diﬃcult problem,\nAgents that share the same goal might collaborate, and ﬁnding a policy with\nthe highest reward for oneself or for the group may include achieving win/win\nsolutions with other agents. Coalition forming and collusion are an integral part of\nthe ﬁeld of multi-agent reinforcement learning.\nIn real-world decision making both competition and cooperation are important.\nIf we want our agents to behave realistically in settings with multiple agents, then\nthey should understand cooperation in order to perform well.\nFrom a computational perspective, studying the emergence of group behavior\nand collective intelligence is challenging: the environment for the agents consists of\nmany other agents; goals may move, many interactions have to be modeled in order\nto be understood, and the world to be optimized against is constantly changing.\n195\n196\n7 Multi-Agent Reinforcement Learning\nA lack of compute power has held back experimental research in multi-agent\nreinforcement learning for some time. The recent growth in compute power and\nadvances in deep reinforcement learning methods are making progress increasingly\npossible.\nMulti-agent decision making problems that have been studied experimentally\ninclude the games of bridge, poker, Diplomacy, StarCraft, Hide and Seek, and\nCapture the Flag. To a large extent, the algorithms for multi-agent reinforcement\nlearning are still being developed. This chapter is full of challenges and development.\nWe will start by reviewing the theoretical framework and deﬁning multi-agent\nproblems. We will look deeper at cooperation and competition, and introduce\nstochastic games, extensive-form games, and the Nash equilibrium and Pareto\noptimality. We will discuss population-based methods and curriculum learning\nin multi-player teams. Next, we will discuss environments on which multi-agent\nreinforcement learning methods can be tested, such as poker and StarCraft. In these\ngames, often team structure plays an important role. The next chapter discusses\nhierarhical methods, which can be used to model team structures.\nThis chapter is concluded, as usual, with exercises, a summary, and pointers to\nfurther reading.\nCore Concepts\n•\nCompetition\n•\nCooperation\n•\nTeam learning\nCore Problem\n•\nFind eﬃcient methods for large competitive and cooperative multi-agent prob-\nlems\nCore Algorithms\n•\nCounterfactual regret minimization (Sect. 7.2.1)\n•\nPopulation-based algorithms (Sect. 7.2.3.1)\n7.1 Multi-Agent Problems\n197\nSelf-Driving Car\nTo illustrate the key components of multi-agent decision making, let us assume a\nself driving car is on its way to the supermarket and is approaching an intersection.\nWhat action should it take? What is the best outcome for the car? For other agents?\nFor society?\nThe goal of the car is to exit the intersection safely and reach the destination.\nPossible decisions are going straight or turning left or right into another lane. All\nthese actions consist of sub-decisions. At each time step, the car can move by\nsteering, accelerating and braking. The car must be able to detect objects, such as\ntraﬃc lights, lane markings, and other cars. Furthermore, we aim to ﬁnd a policy\nthat can control the car to make a sequence of manoeuvres to achieve the goal. In a\ndecision-making setting such as this, two additional challenges arise.\nThe ﬁrst challenge is that we should be able to anticipate the actions of other\nagents. During the decision-making process, at each time step, the robot car should\nconsider not only the immediate value of its own current action but also adapt to\nconsequences of actions by other agents. (For example, it would not be good to\nchoose a certain direction that appears safe early on, stick to it, and not adapt the\npolicy as new information comes in, such as a car heading our way.)\nThe second challenge is that these other agents, in turn, may anticipate the\nactions of our agent in choosing their actions. Our agent needs to take their an-\nticipatory behavior into account in its own policy—recursion at the policy level.\nHuman drivers, for example, often predict likely movements of other cars and then\ntake action in response (such as giving way, or accelerating while merging into\nanother lane).1\nMulti-agent reinforcement learning addresses the sequential decision making\nproblem of multiple autonomous agents that operate in a common stochastic envi-\nronment, each of which aims to maximise their own long-term reward by interacting\nwith the environment and with other agents. Multi-agent reinforcement learning\ncombines the ﬁelds of multi-agent systems [697, 847] and reinforcement learning.\n7.1 Multi-Agent Problems\nWe have seen impressive results of single-agent reinforcement learning in recent\nyears. In addition, important results have been achieved in games such as Go\nand poker, and in autonomous driving. These application domains involve the\nparticipation of more than one agent. The study of interactions between more than\none agent has a long history in ﬁelds such as economics and social choice.\n1 Human drivers have a theory of mind of other drivers. Theory of mind, and the related concept\nof mirror neurons [258], are a psychological theory of empathy and understanding, that allows a\nlimited amount of prediction of future behavior. Theory of mind studies how individuals simulate in\ntheir minds the actions of others, including their simulation of our actions (and of our simulations,\netc.) [54, 332].\n198\n7 Multi-Agent Reinforcement Learning\nMulti-agent problems introduce many new kinds of possibilities, such as co-\noperation and simultaneous actions. These phenomena invalidate some of the\nassumptions of the theory behind single-agent reinforcement learning, and new\ntheoretical concepts have been developed.\nTo begin our study of multi-agent problems, we will start with game theory.\nGame Theory\nGame theory is the study of strategic interaction among rational decision-making\nagents. Game theory originally addressed only the theory of two-person zero-sum\ngames, and was later extended to cooperative games of multiple rational players,\nsuch as simultaneous action, non-zero sum, and imperfect-information games. A\nclassic work in game theory is John von Neumann and Oskar Morgenstern’s Theory\nof Games and Economic Behavior [818, 819], published ﬁrst in 1944. This book has\nlaid the foundation for the mathematical study of economic behavior and social\nchoice. Game theory has been instrumental in developing the theory of common\ngoods and the role of government in a society of independent self-interested rational\nagents. In mathematics and artiﬁcial intelligence, game theory has given a formal\nbasis for the computation of strategies in multi-agent systems.\nThe Markov decision process that we have used to formalize single-agent rein-\nforcement learning assumes perfect information. Most multi-agent reinforcement\nlearning problems, however, are imperfect information problems, where some of\nthe information is private, or moves are simultaneous. We will have to extend our\nMDP model appropriately to be able to model imperfect information.\nStochastic Games and Extensive-Form Games\nOne direct generalization of MDP that captures the interaction of multiple agents\nis the Markov game, also known as the stochastic game [695]. Described by\nLittman [483], the framework of Markov games has long been used to express\nmulti-agent reinforcement learning algorithms [452, 789].\nThis multi-agent version of an MDP is deﬁned as follows [870]. At time 𝑡, each\nagent 𝑖∈𝑁executes an action 𝑎𝑖\n𝑡, for the state 𝑠𝑡of the system. The system then\ntransitions to state 𝑠𝑡+1, and rewards each agent 𝑖with reward 𝑅𝑖\n𝑎𝑡(𝑠𝑡, 𝑠𝑡+1). The\ngoal of agent 𝑖is to optimize its own long-term reward, by ﬁnding the policy 𝜋𝑖:\n𝑆→E(𝐴𝑖) as a mapping from the state space to a distribution over the action space,\nsuch that 𝑎𝑖\n𝑡∼𝜋𝑖(·|𝑠𝑡). Then, the value-function 𝑉𝑖: 𝑆→𝑅of agent 𝑖becomes a\nfunction of the joint policy 𝜋: 𝑆→E(𝐴) deﬁned as 𝜋(𝑎|𝑠) = Π𝑖∈𝑁𝜋𝑖(𝑎𝑖|𝑠).\nTo visualize imperfect information in multiple agents, new diagrams are needed.\nFigure 7.1 shows three schematic diagrams. First, in (a), the familiar agent/environ-\nment diagram is shown that we used in earlier chapters. Next, in (b), the multi-agent\nversion of this diagram for Markov games is shown. Finally, in (c) an extensive-form\ngame tree is shown [870]. Extensive form game trees are introduced to model im-\n7.1 Multi-Agent Problems\n199\nFig. 7.1 Multi-Agent Models [870]\nperfect information. Choices by the agent are shown as solid links, and the hidden\nprivate information of the other agent is shown dashed, as the information set of\npossible situations.\nIn (a), the agent observes the state 𝑠, performs action 𝑎, and receives reward 𝑟\nfrom the environment. In (b), in the Markov game, all agents choose their actions\n𝑎𝑖simultaneously and receiving their individual reward 𝑟𝑖. In (c), in a two-player\nextensive-form game, the agents make decisions on choosing actions 𝑎𝑖. They\nreceive their individual reward 𝑟𝑖(𝑧) at the end of the game, where 𝑧is the score of a\nterminal node for the branch that resulted from the information set. The information\nset is indicated with a dotted line, to signal stochastic behavior, the environment\n(or other agent) chooses an unknown outcome amongst the dotted actions. The\nextensive-form notation is designed for expressing imperfect information games,\nwhere all possible (unknown) outcomes are represented in the information set. In\norder to calculate the value function, the agent has to regard all possible diﬀerent\nchoices in the information sets; the unknown choices of the opponents create a\nlarge state space.\nCompetitive, Cooperative, and Mixed Strategies\nMulti-agent reinforcement learning problems fall into three groups: problems with\ncompetitive behavior, with cooperative behavior, and with mixed behavior. In the\ncompetitive setting, the reward of the agents sums up to zero. A win for one agent\nis a loss for another. In the cooperative setting, agents collaborate to optimize a\ncommon long-term reward. A win for one agent is a win for all agents (example:\nan embankment being built around an area prone to ﬂooding; the embankment\nbeneﬁts all agents in the area). The mixed setting involves both cooperative and\ncompetitive agents, with so-called general-sum rewards, some action may lead to\nwin/win, some other to win/loss.\n200\n7 Multi-Agent Reinforcement Learning\nThese three behaviors are a useful guide to navigate the landscape of multi-agent\nalgorithms, and we will do so in this chapter. Let us have a closer look at each type\nof behavior.\n7.1.1 Competitive Behavior\nOne of the hallmarks of the ﬁeld of game theory is a result by John Nash, who\ndeﬁned the conditions for a stable (and in a certain sense optimal) solution among\nmultiple rational non-cooperative agents. The Nash equilibrium is deﬁned as a\nsituation in which no agent has anything to gain by changing its own strategy. For\ntwo competitive agents the Nash equilibrium is the minimax strategy.\nIn single-agent reinforcement learning the goal is to ﬁnd the policy that max-\nimizes the cumulative future reward of the agent. In multi-agent reinforcement\nlearning the goal is to ﬁnd a combined policy of all agents that simultaneously\nachieves that goal: the multi-policy that for each agent maximizes their cumulative\nfuture reward. If you have a set of competitive (near-)zero exploitability strategies\nthis is called a Nash equilibrium.\nThe Nash equilibrium characterizes an equilibrium point 𝜋★, from which none\nof the agents has an incentive to deviate. In other words, for any agent 𝑖∈𝑁, the\npolicy 𝜋𝑖,★is the best-response to 𝜋−𝑖,★, where −𝑖are all agents except 𝑖[332].\nAn agent that follows a Nash strategy is guaranteed to do no worse than tie,\nagainst any other opponent strategy. For games of imperfect information or chance,\nsuch as many card games, this is an expected outcome. Since the cards are randomly\ndealt, there is no theoretical guarantee that a Nash strategy will win or draw every\nsingle hand, although on average, it cannot do worse than tie against the other\nagents.\nIf the opponents also play a Nash strategy then all will tie. If the opponents make\nmistakes, however, then they can lose some hands, allowing the Nash equilibrium\nstrategy to win. Such a mistake by the opponents would be a deviation from the\nNash strategy, following a hunch or other non-rational reason, despite having no\ntheoretical incentive to do so. A Nash equilibrium plays perfect defence. It does\nnot try to exploit the opponent strategy’s ﬂaws, and instead just wins when the\nopponent makes mistakes [103, 451].\nThe Nash strategy gives the best possible outcome we can achieve against our\nadversaries when they work against us. In the sense that a Nash strategy is on\naverage unbeatable, it is considered to be an optimal strategy, and solving a game\nis equivalent to computing a Nash equilibrium.\nIn a few moments we will introduce a method to calculate Nash strategies, called\ncounterfactual regret minimization, but we will ﬁrst look at cooperation.\n7.1 Multi-Agent Problems\n201\nFig. 7.2 Pareto Frontier of Production-Possibilties\n7.1.2 Cooperative Behavior\nIn single-agent reinforcement learning, reward functions return scalar values: an\naction results in a win or a loss, or a single numeric value. In multi-agent rein-\nforcement learning the reward functions may still return a scalar value, but the\nfunctions may be diﬀerent for each agent; the overall reward function is a vector\nof the individual rewards. In fully cooperative stochastic games, the individual\nrewards are the same, and all agents have the same goal: to maximize the common\nreturn.\nWhen choices in our problem are made in a decentralized manner by a set\nof individual decision makers, the problem can be modeled naturally as a decen-\ntralized partially observable Markov decision process (dec-POMDP) [569]. Large\ndec-POMDPs are hard to solve; in general dec-POMDPs are known to be NEXP-\ncomplete. These problems are not solvable with polynomial-time algorithms and\nsearching directly for an optimal solution in the policy space is intractable [82].\nA central concept in cooperative problems is Pareto eﬃciency, after the work\nof Vilfredo Pareto, who studied economic eﬃciency and income distributions. The\nPareto eﬃcient solution is the situation where no cooperative agent can be better\noﬀwithout making at least one other agent worse oﬀ. All non-Pareto eﬃcient\ncombination are dominated by the Pareto eﬃcient solution. The Pareto frontier is\nthe set of all Pareto eﬃcient combinations, usually drawn as a curve. Figure 7.2\nshows a situation where we can choose to produce diﬀerent quantities of two\nseparate goods, item 1 and item 2 (image by Wikimedia). The grey items close to\nthe origin are choices for which better choices exist. The red symbols all represent\nproduction combinations that are deemed more favorable by consumers [700].\nIn multi-objective reinforcement learning, diﬀerent agents can have diﬀerent\npreferences. A policy is Pareto-optimal when deviating from the policy will make\n202\n7 Multi-Agent Reinforcement Learning\nat least one agent worse oﬀ. We will compare Nash and Pareto soon by looking at\nhow they are related in the prisoner’s dilemma (Table 7.1).\nThe Pareto optimum is the best possible outcome for us where we do not hurt\nothers, and others do not hurt us. It is a cooperative strategy. Pareto equilibria\nassume communication and trust. In some sense, it is the opposite of the non-\ncooperative Nash strategy. Pareto calculates the situation for an cooperative world,\nNash for an competitive world.\nMulti-objective reinforcement learning\nMany real-world problems involve the optimization of multiple, possibly conﬂicting\nobjectives. In a fully cooperative setting, all agents share such a common reward\nfunction. When the reward function consists of diﬀerent values for each agent,\nthe problem is said to be multi-objective: diﬀerent agents may have diﬀerent pref-\nerences or objectives. Note that in principle single-agent problems can also have\nmultiple ojectives, although in this book all single agent problems have had single,\nscalar, rewards. Multi-objective problems become especially relevant in multi-agent\nproblems, where diﬀerent agents may have diﬀerent reward preferences.\nIn decentralized decision processes, the agents must communicate their pref-\nerences to eachother. Such heterogeneity necessitates the incorporation of com-\nmunication protocols into multi-agent reinforcement learning, and the analysis of\ncommunication-eﬃcient algorithms [317, 637].\nMulti-objective reinforcement learning [635, 292, 333, 618] is a generalization of\nstandard reinforcement learning where the scalar reward signal is extended to mul-\ntiple feedback signals, such as one for each agent. A multi-objective reinforcement\nlearning algorithm optimizes multiple objectives at the same time, and algorithms\nsuch as Pareto Q-learning have been introduced for this purpose [802, 634].\n7.1.3 Mixed Behavior\nStarting as a theory for non-cooperative games, game theory has progressed to\ninclude cooperative games, where it analyzes optimal strategies for groups of\nindividuals, assuming that they can communciate or enforce agreements between\nthem about proper strategies. The prisoner’s dilemma is a well-known example of\nthis type of problem [433].2 The prisoner’s dilemma is thought to be invented by\nJohn Nash, although in a more neutral (non cops and robbers) setting.\nThe prisoner’s dilemma is as follows (see Table 7.1). Two burglars, called Row and\nColumn, have been arrested for breaking into a bank. Row and Column understand\nthat they cannot trust the other person, hence they operate in a non-cooperative\nsetting. The police oﬃcer is oﬀering the following options: If you both confess, then\n2 https://plato.stanford.edu/entries/prisoner-dilemma/\n7.1 Multi-Agent Problems\n203\nConfess\nSilent\nDefect Cooperate\nConfess\n(−5, −5)\n(0, −10)\nDefect\nNash\nSilent\n(−10, 0)\n(−2, −2)\nCooperate\nPareto\nTable 7.1 Prisoner’s Dilemma\nyou will both get a light sentence of 5 years. If you both keep quiet, then I will only\nhave evidence to get you a short sentence of 2 years. However, if one of you confesses,\nand the other stays silent, then the confessor will walk free, and the one keeping quiet\ngoes to prison for 10 years. Please tell me your choice tomorrow morning.\nThis leaves our two criminals with a tough, but clear, choice. If the other stays\nsilent, and I confess, then I walk free; if I also stay silent, then we get 2 years in\nprison, so confessing is better for me in this case. If the other confesses, and I also\nconfess, then we both get 5 years; if I then stay silent, then I get 10 years in prison,\nso confessing is again better for me. Whatever the other chooses, confessing gives\nthe lighter sentence, and since they can not coordinate their action, they will each\nindependently confess. Both will get 5 years in prison, even though both would\nhave only gotten 2 years if both would have stayed silent. The police is happy, since\nboth confess and the case is solved.\nIf they would have been able to communicate, or if they would have trusted\neachother, then both would have stayed silent, and both would have gotten oﬀwith\na 2 year sentence.\nThe dilemma faced by the criminals is that, whatever the other does, each is\nbetter oﬀconfessing than remaining silent. However, both also know that if the\nother could have been trusted, or if they could have coordinated their answer, then\na better outcome would have been within reach.\nThe dilemma illustrates individual self-interest against the interest of the group.\nIn the literature on game theory, confessing is also known as defecting, and staying\nsilent is the cooperative choice. The confess/confess situation (both defect) is the\noptimal non-cooperative strategy, the Nash equilibrium, because for each agent\nthat stays silent a strategy exists where the sentence will be made worse (10 years)\nwhen the other agent confesses. Hence the agent will not stay silent but confess, so\nas to limit the loss.\nSilent/silent (both cooperate) is Pareto optimal at 2 years for both, since going\nto all other cases will make at least one agent worse oﬀ.\nThe Nash strategy is non-cooperative, non-communication, non-trust, while\nPareto is the cooperative, communication, trust, outcome.\n204\n7 Multi-Agent Reinforcement Learning\nIterated Prisoner’s Dilemma\nThe prisoner’s dilemma is a one-time game. What would happen if we play this\ngame repeatedly, being able to identify and remember the choices of our opponent?\nCould some kind of communication or trust arise, even if the setting is initially\nnon-cooperative?\nThis question is answered by the iterated version of the prisoner’s dilemma. Inter-\nest in the iterated prisoner’s dilemma grew after a series of publications following a\ncomputer tournament [36, 37]. The tournament was organized by political scientist\nRobert Axelrod, around 1980. Game theorists were invited to send in computer\nprograms to play iterated prisoner’s dilemmas. The programs were organized in a\ntournament and played eachother hundreds of times. The goal was to see which\nstrategy would win, and if cooperation would emerge in this simplest of settings. A\nrange of programs were entered, some using elaborate response strategies based\non psychology, while others used advanced machine learning to try to predict the\nopponent’s actions.\nSurprisingly, one of the simplest strategies won. It was submitted by Anatol\nRapoport, a mathematical psychologist who specialized in the modeling of social\ninteraction. Rapoport’s program played a strategy known as tit for tat. It would start\nby cooperating (staying silent) in the ﬁrst round, and in the next rounds if would\nplay whatever the opponent did in the previous round. Tit for tat thus rewards\ncooperation with cooperation, and punishes defecting with defection—hence the\nname.\nTit for tat wins if it is paired with a cooperative opponent, and does not loose\ntoo much by stubbornly cooperating with a non-cooperative opponent. In the long\nrun, it either ends up in the Pareto optimum or in the Nash equilibrium. Axelrod\nattributes the success of tit for tat to a number of properties. First, it is nice, that is,\nit is never the ﬁrst to defect. In his tournament, the top eight programs played nice\nstrategies. Second, tit for tat is also retaliatory, it is diﬃcult to exploit the strategy\nby the non-cooperative strategies. Third, tit for tat is forgiving, when the opponent\nplays nice, it rewards the play by reverting to being nice, being willing to forget past\nnon-cooperative behavior. Finally, the rule has the advantage of being clear and\npredictable. Others easily learn its behavior, and adapt to it, by being cooperative,\nleading to the mutual win/win Pareto optimum.\n7.1.4 Challenges\nAfter this brief overview of theory, let us see how well practical algorithms are\nable to solve multi-agent problems. With the impressive results in single-agent\nand two-agent reinforcement learning, the interest (and the expectations) in multi-\nagent problems have increased. In the next section, we will have a closer look at\nalgorithms for multi-agent problems, but let us ﬁrst look at the challenges that\nthese algorithms face.\n7.1 Multi-Agent Problems\n205\nMulti-agent problems are studied in competitive, cooperative, and mixed settings.\nThe main challenges faced by multi-agent reinforcement learning are threefold:\n(1) partial observability, (2) nonstationary environments, and (3) large state space.\nAll three of these aspects increase the size of the state space. We will discuss these\nchallenges in order.\n7.1.4.1 Partial Observability\nMost multi-agent settings are imperfect information settings, where agents have\nsome private information that is not revealed to other agents. The private informa-\ntion can be in the form of hidden cards whose value is unknowns, such as in poker,\nblackjack or bridge. In real time strategy games players often do not see the entire\nmap of the game, but parts of the world are obscured. Another reason for imperfect\ninformation can be that the rules of the game allow simultaneous moves, such as\nin Diplomacy, where all agents are determining their next action at the same time,\nand agents have to act without full knowledge of the other actions.\nAll these situations require that all possible states of the world have to considered,\nincreasing the number of possible states greatly in comparison to perfect informa-\ntion games. Imperfect information is best expressed as an extensive-form game. In\nfact, given the size of most multi-agent systems, it quickly becomes unfeasible to\ncommunicate and keep track of all the information of all agents, even if all agents\nwould make their state and intentions public (which they rarely do).\nImperfect information increases the size of the state space, and computing the\nunknown outcomes quickly becomes unfeasible.\n7.1.4.2 Nonstationary Environments\nMoreover, as all agents are improving their policies according to their own interests\nconcurrently, the agents are faced with a nonstationary environment. The environ-\nment’s dynamics are determined by the joint action space of all agents, in which\nthe best policies of agents depend on the best policies of the other agents. This\nmutual dependence creates an unstable situation.\nIn a single agent setting a single state needs to be tracked to calculate the next\naction. In a multi-agent setting, all states and all agents’ policies need to be taken\ninto account, and mutually so. Each agent faces a moving target problem.\nIn multi-agent reinforcement learning the agents learn concurrently, updating\ntheir behavior policies concurrently and often simultaneously [332]. Actions taken\nby one agent aﬀect the reward of other agents, and therefore of the next state. This\ninvalidates the Markov property, which states that all information that is necessary\nto determine the next state is present in the current state and the agent’s action.\nThe powerful arsenal of single-agent reinforcement theory must be adapted before\nit can be used.\n206\n7 Multi-Agent Reinforcement Learning\nTo handle nonstationarity, agents must account for the joint action space of the\nother agents’ actions. The size of the space increases exponentially with the number\nof agents. In two-agent settings the agent has to consider all possible replies of a\nsingle opponent to each of its own moves, increasing the state space greatly. In\nmulti-agent settings the number of replies increases even more, and computing\nsolutions to these problems quickly becomes quite expensive. A large number of\nagents complicates convergence analysis and increases the computational demands\nsubstantially.\nOn the other hand, agents may learn from each other, line up their goals, and\ncollaborate. Collaboration and group forming reduce the number of independent\nagents that must be tracked.\n7.1.4.3 Large State Space\nIn addition to the problems caused by partial observability and nonstationarity, the\nsize of the state space is signiﬁcantly increased in a multi-agent setting, simply\nbecause every additional agent exponentially increases the state space. Furthermore,\nthe action space of multi-agent reinforcement learning is a joint action space whose\nsize increases exponentially with the number of agents. The size of the joint action\nspace often causes scalability issues.\nSolving such a large state space to optimality is infeasible for all but the smallest\nmulti-agent problems, and much work has been done to create models (abstractions)\nthat make simplifying, but sometimes realistic, assumptions to reduce the size of\nthe state space. In the next section we will look at some of these assumptions.\n7.2 Multi-Agent Reinforcement Learning Agents\nIn the preceding section the problem of multi-agent reinforcement learning has\nbeen introduced, as well as game theory and the links with social choice theory.\nWe have also seen that the introduction of multiple agents has, unsurprisingly,\nincreased the size of the state space even more than for single-agent state spaces.\nRecent work has introduced some approaches for solving multi-agent problems,\nwhich we will introduce here.\nFirst, we will discuss an approach based on planning, with the name Counterfac-\ntual regret minimization (CFR). This algorithm is successful for computing complex\nsocial choice problems, such as occur in the game of poker. CFR is suitable for\ncompetitive multi-agent problems.\nSecond, we will discuss cooperative reinforcement learning methods that are\nalso suitable for mixed multi-agent problems.\nThird, we discuss population-based approaches such as evolutionary strategies\nand swarm computing [42, 252, 346]. These approaches are inspired by computional\nbehavior that occurs in nature, such as in ﬂocks of birds and societies of insects [95,\n7.2 Multi-Agent Reinforcement Learning Agents\n207\n207, 97]. Such methods are well-known from single-agent optimization problems;\nindeed, population-based methods are successful in solving many complex and\nlarge single-agent optimization problems (including stochastic gradient descent\noptimization) [653]. In this section, we will see that evolutionary methods are also\na natural match for mixed multi-agent problems, although they are typically used\nfor cooperative problems with many homogeneous agents.\nFinally, we will discuss an approach based on multi-play self-learning, in which\nevolutionary and hierarchical aspects are used. Here diﬀerent groups of reinforce-\nment learning agents are trained against eachother. This approach has been highly\nsuccessful in the games of Capture the Flag and StarCraft, and is suitable for mixed\nmulti-agent problems.\nLet us start with counterfactual regret minimization.\n7.2.1 Competitive Behavior\nThe ﬁrst setting that we will discuss is the competitive setting. This setting is\nstill close to single and two-agent reinforcement learning, that are also based on\ncompetition.\n7.2.1.1 Counterfactual Regret Minimization\nCounterfactual regret minimization (CFR) is an iterative method for approximating\nthe Nash equilibrium of an extensive-form game [881]. CFR is suitable for imperfect\ninformation games (such as poker) and computes a strategy that is (on average)\nnon-exploitable, and that is therefore robust in a competitive setting. Central in\ncounterfactual regret minimization is the notion of regret. Regret is the loss in\nexpected reward that an agent suﬀers for not having selected the best strategy, with\nrespect to ﬁxed choices by the other players. Regret can only be known in hindsight.\nWe can, however, statistically sample expected regret, by averaging the regret that\ndid not happen. CFR ﬁnds the Nash equilibrium by comparing two hypothetical\nplayers against eachother, where the opponent chooses the action that minimizes\nour value.\nCFR is a statistical algorithm that converges to a Nash equilibrium. Just like\nminimax, it is a self-play algorithm that ﬁnds the optimal strategy under the as-\nsumption of optimal play by both sides. Unlike minimax, it is suitable for imperfect\ninformation games, where information sets describe a set of possible worlds that\nthe opponent may hold. Like MCTS it samples, repeating this process for billions\nof games, improving its strategy each time. As it plays, it gets closer and closer\ntowards an optimal strategy for the game: a strategy that can do no worse than tie\nagainst any opponent [379, 749, 451]. The quality of the strategy that it computes is\nmeasured by its exploitability. Exploitability is the maximum amount that a perfect\ncounter-strategy could win (on expectation) against it.\n208\n7 Multi-Agent Reinforcement Learning\nAlthough the Nash equilibrium is a strategy that is theoretically proven to be not\nexploitable, in practice, typical human play is far from the theoretical optimum, even\nfor top players [112]. Poker programs based on counterfactual regret minimization\nstarted beating the world’s best human players in heads-up limit hold’em in 2008,\neven though these programs programs were still very much exploitable by this\nworst-case measure [379].\nMany papers on counterfactual regret minimization are quite technical, and the\ncodes for algorithms are too long to explain here in detail. CFR is in important\nalgorithm that is essential for understanding progress in poker. To make the work\non poker and CFR more accessible, introductory papers and blogs have been written.\nTrenner [784] has written a blog3 in which CFR is used to play Kuhn poker, one of\nthe simplest Poker variants. The CFR pseudocode and the function that calls it\niteratively are shown in Fig. 7.1; for the other routines, see the blog. The CFR code\nworks as follows. First it checks for being in a terminal state and returns the payoﬀ,\njust as a regular tree traversal code does. Otherwise, it retrieves the information\nset and the current regret-matching strategy. It uses the reach probability, which\nis the probability that we reach the current node according to our strategy in the\ncurrent iteration. Then CFR loops over the possible actions (lines 17–24), computes\nthe new reach probabilities for the next game state and calls itself recursively.\nAs there are 2 players taking turns in Kuhn poker, the utility for one player is\nexactly −1 times the utility for the other, hence the minus sign in front of the cfr()\ncall. What is computed here for each action is the counterfactual value. When the\nloop over all possible actions ﬁnishes, the value of the node-value of the current\nstate is computed (line 26), with our current strategy. This value is the sum of the\ncounterfactual values per action, weighted by the likelihood of taking this action.\nThen the cumulative counterfactual regrets are updated by adding the node-value\ntimes the reach probability of the opponent. Finally, the node-value is returned.\nAnother accessible blog post where the algorithm is explained step by step has\nbeen written by Kamil Czarnogòrski,4 with code on GitHub.5\n7.2.1.2 Deep Counterfactual Regret Minimization\nCounterfactual regret minimization is a tabular algorithm that traverses the\nextensive-form game tree from root to terminal nodes, coming closer to the Nash\nequilibrium with each iteration. Tabular algorithms do not scale well to large\nproblems, and researchers often have to use domain-speciﬁc heuristic abstraction\nschemes [658, 260, 110], alternate methods for regret updates [749], or sampling\nvariants [451] to achieve acceptable performance.\n3\nhttps://ai.plainenglish.io/building-a-poker-ai-part-6-beating-kuhn-poker-\nwith-cfr-using-python-1b4172a6ab2d\n4 https://int8.io/counterfactual-regret-minimization-for-poker-ai/\n5 https://github.com/int8/counterfactual-regret-minimization/blob/master/games/\nalgorithms.py\n7.2 Multi-Agent Reinforcement Learning Agents\n209\n1\ndef cfr(\n2\nself ,\n3\ncards: List[str],\n4\nhistory: str ,\n5\nreach_probabilities : np.array ,\n6\nactive_player : int) -> int:\n7\nif\nKuhnPoker. is_terminal (history):\n8\nreturn\nKuhnPoker. get_payoff (history , cards)\n9\n10\nmy_card = cards[ active_player ]\n11\ninfo_set = self. get_information_set (my_card + history)\n12\n13\nstrategy = info_set. get_strategy ( reach_probabilities [\nactive_player ])\n14\nopponent = ( active_player + 1) % 2\n15\ncounterfactual_values = np.zeros(len(Actions))\n16\n17\nfor ix , action in\nenumerate(Actions):\n18\naction_probability = strategy[ix]\n19\n20\nnew_reach_probabilities = reach_probabilities .copy ()\n21\nnew_reach_probabilities [ active_player ] *=\naction_probability\n22\n23\ncounterfactual_values [ix] = -self.cfr(\n24\ncards , history + action , new_reach_probabilities ,\nopponent)\n25\n26\nnode_value = counterfactual_values .dot(strategy)\n27\nfor ix , action in\nenumerate(Actions):\n28\ncounterfactual_regret [ix] = \\\n29\nreach_probabilities [opponent] * (\ncounterfactual_values [ix] - node_value)\n30\ninfo_set. cumulative_regrets [ix] +=\ncounterfactual_regret [\nix]\n31\n32\nreturn\nnode_value\n33\n34\ndef\ntrain(self , num_iterations : int) -> int:\n35\nutil = 0\n36\nkuhn_cards = [’J’, ’Q’, ’K’]\n37\nfor _ in range( num_iterations ):\n38\ncards = random.sample(kuhn_cards , 2)\n39\nhistory = ’’\n40\nreach_probabilities = np.ones (2)\n41\nutil += self.cfr(cards , history , reach_probabilities , 0)\n42\nreturn\nutil\nListing 7.1 Counter-factual regret minimization [784]\n210\n7 Multi-Agent Reinforcement Learning\nFor large problems a deep learning version of the algorithm has been devel-\noped [111]. The goal of deep counterfactual regret minimization is to approximate\nthe behavior of the tabular algorithm without calculating regrets at each individual\ninformation set. It generalizes across similar infosets using approximation of the\nvalue function via a deep neural network with alternating player updates.\n7.2.2 Cooperative Behavior\nCFR is an algorithm for the competitive setting. The Nash-equilibrium deﬁnes the\ncompetitive win/lose multi-agent case—the (−5, −5) situation of the prisoner’s\ndilemma of Table 7.1.\nWe will now move to the cooperative setting. As we have seen, in a cooperative\nsetting, win/win situations are possible, with higher rewards, both for society as\na whole and for the individuals. The Pareto optimum for the prisoner’s dilemma\nexample is (−2, −2), only achievable through norms, trust or cooperation by the\nagents (as close-knit criminal groups aim to achieve, for example, through a code\nof silence), see also Leibo et al. [465].\nThe achievements in single-agent reinforcement learning inspire researchers to\nachieve similar results in multi-agent. However, partial observability and nonsta-\ntionarity create a computational challenge. Researchers have tried many diﬀerent\napproaches, some of which we will cover, although the size of problems for which\nthe current agorithms work is still limited. Wong et al. [846] provide a review of\nthese approaches, open problems in cooperative reinforcement learning are listed\nby Dafoe et al. [175]. First we will discuss approaches based on single-agent rein-\nforcement learning methods, next we will discuss approaches based on opponent\nmodeling, communication, and psychology [846].\n7.2.2.1 Centralized Training/Decentralized Execution\nWhile the dec-POMDP model oﬀers an appropriate framework for cooperative\nsequential decision making under uncertainty, solving a large dec-POMDP is in-\ntractable [82], and therefore many relaxations of the problems have been developed,\nwhere some elements, such as communication, or training, are centralized, to in-\ncrease tractability [846, 751].\nOne of the easiest approaches to train a policy for a multi-agent problem is to\ntrain the collaborating agents with a centralised controller, eﬀectively reducing a\ndecentralized multi-agent computation to a centralized single-agent computation.\nIn this approach all agents send their observations and local policies to a central\ncontroller, that now has perfect information, and that decides which action to\ntake for each agent. However, as large collaborative problems are computationally\nexpensive, the single controller becomes overworked, and this approach does not\nscale.\n7.2 Multi-Agent Reinforcement Learning Agents\n211\nOn the other extreme, we can ignore communication and nonstationarity and let\nagents train separately. In this approach the agents learn an individual action-value\nfunction and view other agents as part of the environment. This approach simpliﬁes\nthe computational demands at the cost of gross oversimpliﬁcation, ignoring multi-\nagent interaction.\nAn in-between approach is centralized training and decentralized execution [427].\nHere agents can access extra information during training, such as other agents’\nobservations, rewards, gradients and parameters. However, they execute their policy\ndecentrally based on their local observations. The local computation and inter-agent\ncommunication mitigate nonstationarity while still modeling partial observability\nand (some) interaction. This approach stabilises the local policy learning of agents,\neven when other agents’ policies are changing [427].\nWhen value functions are learned centrally, how should this function then be\nused for decentral execution by the agents? A popular method is value-function\nfactorization. The Value decomposition networks method (VDN) decomposes the\ncentral value function as a sum of individual value functions [735], who are exe-\ncuted greedily by the agents. QMIX and QTRAN are two methods that improve on\nVDN by allowing nonlinear combinations [628, 720]. Another approach is Multi-\nagent variational exploration (MAVEN) which improves the ineﬃcient exploration\nproblem of QMIX using a latent space model [496].\nPolicy-based methods focus on actor critic approaches, with a centralized critic\ntraining decentralized actors. Counterfactual multi-agent (COMA) uses such a\ncentralized critic to approximate the Q-function that has access to the actors that\ntrain the behavior policies [250].\nLowe et al. [489] introduce a multi-agent version of a popular oﬀ-policy single-\nagent deep policy-gradient algorithm DDPG (Sect. 4.2.7), called MADDPG. It con-\nsiders action policies of other agents and their coordination. MADDPG uses an\nensemble of policies for each agent. It uses a decentralized actor, centralized critic\napproach, with deterministic policies. MADDPG works for both competitive and col-\nlaborative multi-agent problems. An extension for collision avoidance is presented\nby Cai et al. [123]. A popular on-policy single-agent method is PPO. Han et al. [313]\nachieve sample eﬃcient results modeling the continuous Half-cheetah task as a\nmodel-based multi-agent problem. Their model-based, multi-agent, work is inspired\nby MVE [238] (Sect. 5.2.2.1). Yu et al. [863] achieve good results in cooperative\nmulti-agent games (StarCraft, Hanabi, and Particle world) with MAPPO.\nModeling cooperative behavior in reinforcement learning in a way that is compu-\ntationally feasible is an active area of research. Li et al. [474] use implicit coordina-\ntion graphs to model the structure of interactions. They use graph neural networks\nto model the coordination graphs [297] for StarCraft and traﬃc environments [655],\nallowing scaling of interaction patterns that are learned by the graph convolutional\nnetwork.\n212\n7 Multi-Agent Reinforcement Learning\n7.2.2.2 Opponent Modeling\nThe state space of multi-agent problems is large, yet the previous approaches tried\nto learn this large space with adaptations of single-agent algorithms. Another\napproach is to reduce the size of the state space, for example by explicitly modeling\nopponent behavior in the agents. These models can then be used to guide the agent’s\ndecision making, reducing the state space that it has to traverse. Albrecht and Stone\nhave written a survey of approaches [14].\nOne approach to reduce the state space is to assume a set of stationary policies\nbetween which agents switch [232]. The Switching agent model (SAM) [877] learns\nan opponent model from observed trajectories with a Bayesian network. The Deep\nreinforcement open network (DRON) [320] uses two networks, one to learn the\nQ-values, and the other to learn the opponent policy representation.\nOpponent modeling is related to the psychological Theory of Mind [610]. Ac-\ncording to this Theory, people attribute mental states to others, such as beliefs,\nintents, and emotions. Our theory of the minds of others helps us to analyze and\npredict their behavior. Theory of mind also holds that we assume that the other\nhas theory of mind; it allows for a nesting of beliefs of the form: “I believe that you\nbelieve that I believe” [796, 795, 797]. Building on these concepts, Learning with\nopponent-learning awareness (LOLA) anticipates opponent’s behavior [251]. Prob-\nabilistic recursive reasoning (PR2) models our own and our opponent’s behavior as\na hierarchy of perspectives [835]. Recursive reasoning has been shown to lead to\nfaster convergence and better performance [535, 176]. Opponent modeling is also\nan active area of research.\n7.2.2.3 Communication\nAnother step towards modeling the real world is taken when we explicitly model\ncommunication between agents. A fundamental question is how language between\nagents emerges when no predeﬁned communication protocol exists, and how syntax\nand meaning evolve out of interaction [846]. A basic approach to communication is\nwith referential games: a sender sends two images and a message; the receiver then\nhas to identify which of the images was the target [458]. Language also emerges in\nmore complicated versions, or in negotiation between agents [126, 424].\nAnother area where multi-agent systems are frequently used is the study of\ncoordination, social dilemmas, emergent phenomena and evolutionary processes,\nsee, for example [750, 219, 465]. In the card game bridge [716] bidding strategies\nhave been developed to signal to the other player in the team which cards a player\nhas [716]. In the game of Diplomacy, an explicit negotion-phase is part of each\ngame round [185, 430, 584, 21]. Work is ongoing to design communication-aware\nvariants of reinforcement learning algorithms [711, 315].\n7.2 Multi-Agent Reinforcement Learning Agents\n213\n7.2.2.4 Psychology\nMany of the key ideas in reinforcement learning, such as operant conditioning and\ntrial-and-error, originated in cognitive science. Faced with the large state space,\nmulti-agent reinforcement learning methods are moving towards human-like agents.\nIn addition to opponent modeling, studies focus on coordination, pro-social behavior,\nand intrinsic motivation. A large literature exists on emergence of social norms and\ncultural evolution in multi-agent systems [465, 105, 34, 181]. To deal with nonstation-\narity and large states spaces, humans use heuristics and approximation [275, 500].\nHowever, heuristics can lead to biases and suboptimal decision-making [276]. It\nis interesting to see how multi-agent modeling is discovering concepts from psy-\nchology. More research in this area is likely to improve the human-like behavior of\nartiﬁcial agents.\n7.2.3 Mixed Behavior\nTo discuss solution methods for agents in the mixed setting, we will look at one\nimportant approach that is again inspired by biology: population-based algorithms.\nPopulation-based methods such as evolutionary algorithms and swarm comput-\ning work by evolving (or optimizing) a large number of agents at the same time.\nWe will look closer at evolutionary algorithms and at swarm computing, and then\nwe will look at the role they play in multi-agent reinforcement learning.\n7.2.3.1 Evolutionary Algorithms\nEvolutionary algorithms are inspired by bio-genetic processes of reproduction: mu-\ntation, recombination, and selection [40]. Evolutionary algorithms work with large\npopulations of simulated individuals, which typically makes it easy to parallelize\nand run them on large computation clusters.\nEvolutionary algorithms often achieve good results in optimizing diverse prob-\nlems. For example, in optimizing single agent problems, an evolutionary approach\nwould model the problem as a population of individuals, in which each individual\nrepresents a candidate solution to the problem. The candidate’s quality is deter-\nmined by a ﬁtness function, and the best candidates are selected for reproduction.\nNew candidates are created through crossover and mutation of genes, and the cycle\nstarts again, until the quality of candidates stabilizes. In this way an evolution-\nary algorithm iteratively approaches the optimum. Evolutionary algorithms are\nrandomized algorithms, that can circumvent local optima.\nAlthough they are best known for solving single agent optimization problems,\nwe will use them here to model multi-agent problems.\nLet us look in more detail at how an evolutionary algorithm works (see\nAlg. 7.1) [40]. First an initial population is generated. The ﬁtness of each indi-\n214\n7 Multi-Agent Reinforcement Learning\nAlgorithm 7.1 Evolutionary Framework [40]\n1: Generate the initial population randomly\n2: repeat\n3:\nEvaluate the ﬁtness of each individual of the population\n4:\nSelect the ﬁttest individuals for reproduction\n5:\nThrough crossover and mutation generate new individuals\n6:\nReplace the least ﬁt individuals by the new individuals\n7: until terminated\nvidual is computed, and the ﬁttest individuals are selected for reproduction, using\ncrossover and mutation to create a new generation of individuals. The least ﬁt\nindividuals of the old populations are replaced by the new individuals.\nCompared to reinforcement learning, in evolutionary algorithms the agents\nare typically homogeneous, in the sense that the reward (ﬁtness) function for the\nindividuals is the same. Individuals do have diﬀerent genes, and thus diﬀer in\ntheir behavior (policy). In reinforcement learning there is a single current behavior\npolicy, where an evolutionary approach has many candidate policies (individuals).\nThe ﬁtness function can engender in principle both competitive and cooperative\nbehavior between individuals, although a typical optimization scenario is to select\na single individual with the genes for the highest ﬁtness (survival of the ﬁttest\ncompetitor).6\nChanges to genes of individuals (policies) occur explicitly via crossover and\n(random) mutation, and implicitly via selection for ﬁtness. In reinforcement learning\nthe reward is used more directly as a policy goal; in evolutionary algorithms the\nﬁtness does not directly inﬂuence the policy of an individual, only its survival.\nIndividuals in evolutionary algorithms are passive entities that do not communi-\ncate or act, although they do combine to create new individuals.\nThere are similarities and diﬀerences between evolutionary and multi-agent\nreinforcement learning algorithms. First of all, in both approaches the goal is to ﬁnd\nthe optimal solution, the policy that maximizes (social) reward. In reinforcement\nlearning this occurs by learning a policy through interaction with an environment,\nin evolutionary algorithms by evolving a population through survival of the ﬁttest.\nReinforcement learning deals with a limited number of agents whose policy deter-\nmines their actions, evolutionary algorithms deals with many individuals whose\ngenes determine their survival. Policies are improved using a reward function that\nassesses how good actions are, genes mutate and combine, and individuals are\nselected using a ﬁtness function. Policies are improved “in place” and agents do\nnot die, in evolutionary computation the traits (genes) of the best individuals are\nselected and copied to new individuals in the next generation after which the old\ngeneration does die.\nAlthough diﬀerent at ﬁrst sight, the two approaches share many traits, including\nthe main goal: optimizing behavior. Evolutionary algorithms are inherently multi-\n6 Survival of the ﬁttest cooperative group of individuals can also be achieved with an appropriate\nﬁtness function [492].\n7.2 Multi-Agent Reinforcement Learning Agents\n215\nFig. 7.3 A Flock of Starlings, and One Predator\nagent, and may work well in ﬁnding good solutions in large and nonstationary\nsequential decision problems.\n7.2.3.2 Swarm Computing\nSwarm computing is related to evolutionary algorithms [79, 95]. Swarm computing\nfocuses on emerging behavior in decentralized, collective, self-organized systems.\nAgents are typically simple, numerous, and homogeneous, and interact locally with\neach other and the environment. Biological examples of swarm intelligence are\nbehavior in ant colonies, bee-hives, ﬂocks of birds, and schools of ﬁsh, Fig. 7.3 [736];\nimage by Wikimedia. Behavior is typically cooperative through decentralized com-\nmunication mechanisms. In artiﬁcial swarm intelligence, individuals are sometimes\nable to imagine the behavior of other individuals, when they have a Theory of\nmind [54, 258]. Swarm intelligence, or collective intelligence in general, is a form of\ndecentralized computing, as opposed to reinforcement learning, where external al-\ngorithms calculate optimal behavior in a single classical centralized algorithm [848].\nAlthough both approaches work for the mixed setting, evolutionary algorithms\ntend to stress competition and survival of the ﬁttest (Nash), where swarm computing\nstresses cooperation and survival of the group (Pareto).\nA well-known example of artiﬁcial swarm intelligence is Dorigo’s Ant colony op-\ntimization algorithm (ACO) which is a probabilistic optimization algorithm modeled\nafter the pheromone-based communication of biological ants [206, 208, 209].\n216\n7 Multi-Agent Reinforcement Learning\nAlgorithm 7.2 Population Based Training [374]\nprocedure Train(P)\n⊲initial population P\nPopulation P, weights 𝜃, hyperparameters ℎ, model evaluation 𝑝, time step 𝑡\nfor (𝜃, ℎ, 𝑝, 𝑡) ∈P (asynchronously in parallel) do\nwhile not end of training do\n𝜃←step(𝜃|ℎ)\n⊲one step of optimisation using hyperparameters ℎ\n𝑝←eval(𝜃)\n⊲current model evaluation\nif ready( 𝑝, 𝑡, P) then\nℎ′, 𝜃′ ←exploit(ℎ, 𝜃, 𝑝, P)\n⊲use the rest of population for improvement\nif 𝜃≠𝜃′ then\nℎ, 𝜃←explore(ℎ′, 𝜃′, P)\n⊲produce new hyperparameters ℎ\n𝑝←eval(𝜃)\n⊲new model evaluation\nend if\nend if\nupdate P with new (𝜃, ℎ, 𝑝, 𝑡+ 1)\n⊲update population\nend while\nend for\nreturn 𝜃with the highest 𝑝in P\nend procedure\nEmergent behavior in multi-agent reinforcement learning is speciﬁcally studied\nin [487, 374, 534, 325, 50, 464]. For decentralized algorithms related to solving\nmulti-agent problems see, for example [569, 871, 571, 570].\n7.2.3.3 Population-Based Training\nTranslating traditional value or policy-based reinforcement learning algorithms to\nthe multi-agent setting is non-trivial. It is interesting to see that, in contrast, evolu-\ntionary algorithms, ﬁrst designed for single-agent optimization using a population\nof candidate solutions, translate so naturally to the multi-agent setting, where a\npopulation of agents is used to ﬁnd a shared solution that is optimal for society.\nIn evolutionary algorithms agents are typically homogeneous, although they\ncan be heterogeneous, with diﬀerent ﬁtness functions. The ﬁtness functions may\nbe competitive, or cooperative. In the latter case the increase of reward for one\nagent can also imply an increase for other agents, possibly for the entire group, or\npopulation. Evolutionary algorithms are quite eﬀective optimization algorithms.\nSalimans et al. [653] report that evolution strategies rival the performance of stan-\ndard reinforcement learning techniques on modern benchmarks, while being easy\nto parallelize. In particular, evolutionary strategies are simpler to implement (there\nis no need for backpropagation), are easier to scale in a distributed setting, do not\nsuﬀer in settings with sparse rewards, and have fewer hyperparameters.\nEvolutionary algorithms are a form of population-based computation that can\nbe used to compute strategies that are optimal in a game-theoretic Nash sense, or\nthat try to ﬁnd strategies that out-perform other agents. The evolutionary approach\nto optimization is eﬃcient, robust, and easy to parallelize, and thus has some\n7.2 Multi-Agent Reinforcement Learning Agents\n217\nadvantages over the stochastic gradient approach. How does this approach relate\nto multi-agent reinforcement learning, and can it be used to ﬁnd eﬃcient solutions\nfor multi-agent reinforcement learning problems?\nIn recent years a number of research teams have reported successful eﬀorts\nin creating reinforcement learning players for multi-agent strategy games (see\nSect. 7.3). These were all large research eﬀorts, where a range of diﬀerent approaches\nwas used, from self-play reinforcement learning, cooperative learning, hierarchical\nmodeling, and evolutionary computing.\nJaderberg et al. [374] report success in Capture the Flag games with a combination\nof ideas from evolutionary algorithms and self-play reinforcement learning. Here a\npopulation-based approach of self play is used where teams of diverse agents are\ntrained in tournaments against each other. Algorithm 7.2 describes this Population\nbased training (PBT) approach in more detail. Diversity is enhanced through muta-\ntion of policies, and performance is improve through culling of under-performing\nagents.\nPopulation based training uses two methods. The ﬁrst is exploit, which de-\ncides whether a worker should abandon the current solution and focus on a more\npromising one. The second is explore, which, given the current solution and hy-\nperparameters, proposes new solutions to explore the solution space. Members\nof the population are trained in parallel. Their weights 𝜃are updated and eval\nmeasures their current performance. When a member of the population is deemed\nready because it has reached a certain performance threshold, its weights and\nhyperparameters are updated by exploit and explore, to replace the current\nweights with the weights that have the highest recorded performance in the rest\nof the population, and to randomly perturb the hyperparameters with noise. After\nexploit and explore, iterative training continues as before until convergence.\nLet us have a look at this fusion approach for training leagues of players.\n7.2.3.4 Self-Play Leagues\nSelf-play league learning refers to the training of a multi-agent league of individual\ngame playing characters. As we will see in the next section, variants have been\nused to play games such as StarCraft, Capture the Flag, and Hide and Seek.\nLeague learning combines population-based training with self-play training\nas in AlphaZero. In league learning, the agent plays against a league of diﬀerent\nopponents, while being part of a larger team of agents that is being trained. The team\nof agents is managed to have enough diversity in order to provide a stable training\ngoal to reduce divergence or local minima. League learning employs evolutionary\nconcepts such as mutation of behavior policies and culling of under-performing\nagents from the population. The team employs cooperative strategies, in addition\nto competing against the other teams. Agents are trained in an explicit hierarchy.\nThe goal of self-play league training is to ﬁnd stable policies for all agents, that\nmaximize their team reward. In mixed and cooperative settings the teams of agents\nmay beneﬁt from each others’ strength increase [489]. Self-play league learning\n218\n7 Multi-Agent Reinforcement Learning\nEnvironment\nBehavior\nApproach\nRef\nPoker\nCompetitive (Deep) Counterfactual regret minimization [103, 113, 533]\nHide and Seek\nCooperative Self-play, Team hierarchy\n[49]\nCapture the Flag\nMixed\nSelf-play, Hierarchical, Population-based\n[373]\nStarCraft II\nMixed\nSelf-play, Population-based\n[813]\nTable 7.2 Multi-Agent Game Approaches\nalso uses aspects of hierarchical reinforcement learning, a topic that will be covered\nin the next chapter.\nIn the next section we will look deeper into how self-play league learning is\nimplemented in speciﬁc multi-player games.\n7.3 Multi-Agent Environments\nReinforcement learning has achieved quite a few imaginative results in which it\nhas succeeded in emulating behavior that approaches human behavior in the real\nworld. In this chapter we have made a step towards modeling more behavior that is\neven closer to the real world. Let us summarize in this section the results in four\ndiﬀerent multi-agent games, some of which have been published in prestigious\nscientiﬁc journals.\nWe will use the familiar sequence of (1) competitive, (2) cooperative, and (3)\nmixed environments. For each we will sketch the problem, outline the algorithmic\napproach, and summarize the achievements.\nTable 7.2 lists the multi-agent games and their dominant approach.\n7.3.1 Competitive Behavior: Poker\nPoker is a popular imperfect-information game. It is played competitively and\nhuman championships are organized regularly. Poker has been studied for some time\nin artiﬁcial intelligence, and computer poker championships have been conducted\nregularly [53, 89]. Poker is a competitive game. Cooperation (collusion, collaboration\nbetween two players to the detriment of a third player) is possible, but often does not\noccur in practice; ﬁnding the Nash equilibrium therefore is a successful approach\nto playing the game in practice. The method of counterfactual regret minimization\nhas been developed speciﬁcally to make progress in poker.\nThere are many variants of poker that are regularly played. No-limit Texas\nhold’em is a popular variant; the two-player version is called Heads Up and is\neasier to analyse because no opponent collusion can occur. Heads-up no-limit Texas\nhold’em (HUNL) has been the primary AI benchmark for imperfect-information\ngame play for several years.\n7.3 Multi-Agent Environments\n219\nFig. 7.4 Pluribus on the Cover of Science\nPoker has hidden information (the face-down cards). Because of this, agents\nare faced with a large number of possible states; poker is a game that is far more\ncomplex than chess or checkers. The state space of the two-person HUNL version is\nreported to be around 10161 [112]. A further complication is that during the course\nof the game information is revealed by players through their bets; high bets indicate\ngood cards, or the wish of the player to make the opponent believe that this is the\ncase (bluﬃng). Therefore, a player must choose between betting high on good cards,\nand on doing the opposite, so that the opponent does not ﬁnd out too much, and\ncan counter-act.\nIn 2018, one of the top two-player poker programs, Libratus, defeated top human\nprofessionals in HUNL in a 20-day, 120,000-hand competition featuring a $200,000\nprize pool. Brown et al. [112] describe the architecture of Libratus. The program\nconsists of three main modules, one for computing a quick CFR Nash-policy using\na smaller version of the game, a second module for constructing a ﬁner-grained\nstrategy once a later stage of the game is reached, and a third module to enhance\nthe ﬁrst policy by ﬁlling in missing branches.\nIn the experiment against top human players, Libratus analyzed the bet sizes that\nwere played most often by its opponents at the end of each day of the competition.\nThe programs would then calculate a response overnight, in order to improve as\nthe competition proceeded.\nTwo-agent Libratus was originally based on heuristics, abstraction, and tabular\ncounterfactual regret minimization—not on deep reinforcement learning. For multi-\nagent poker the program Pluribus was developed. For Pluribus, the authors used\ndeep counterfactual regret minimization [111], with a 7-layer neural network that\nfollowed the AlphaZero self-play approach. Pluribus defeated top players in six-\nplayer poker [113] (Fig. 7.4). Pluribus uses an AlphaZero approach of self-play in\n220\n7 Multi-Agent Reinforcement Learning\ncombination with search. Another top program, DeepStack, also uses randomly\ngenerated games to train a deep value function network [533].\n7.3.2 Cooperative Behavior: Hide and Seek\nIn addition to competition, cooperative behavior is part of real-world behavior.\nIndeed, cooperation is what deﬁnes our social fabric, and much of our society\nconsists of diﬀerent ways in which we organize ourselves in families, groups,\ncompanies, parties, ﬁlter bubbles, and nations. The question of how voluntary\ncooperation can emerge between individuals has been studied extensively, and\nthe work in tit-for-tat (Sect. 7.1.3) is just one of many studies in this fascinating\nﬁeld [34, 105, 848, 465, 651].\nOne study into emergent cooperation has been performed with a version of the\ngame of Hide and Seek. Baker et al.[49] report on an experiment in which they\nused MuJoCo to build a new multi-agent game environment. The environment was\ncreated with the speciﬁc purpose of studying how cooperation emerged out of the\ncombination of a few simple rules and reward maximation (see Fig. 7.5).\nIn the Hide and Seek experiment, the game is played on a randomly generated\ngrid-world where props are available, such as boxes. The reward function stimu-\nlates hiders to avoid the line of sight of seekers, and vice versa. There are objects\nscattered throughout the environment that the agents can grab and lock in place.\nThe environment contains one to three hiders and one to three seekers, there are\nthree to nine movable boxes, some elongated. There are also two movable ramps.\nWalls and rooms are static and are randomly generated. Agents can see, move, and\ngrab objects. A good way to understand the challenge is to view the videos7 on the\nHide and Seek blog.8\nWith only a visibility-based reward function, the agents are able to learn many\ndiﬀerent skills, including collaborative tool use. For example, hiders learn to cre-\nate shelters by barricading doors or constructing multi-object forts, so that the\nseekers can never see them anymore, until, as a counter strategy, seekers learned\nto use ramps to jump into the shelter. In eﬀect, out of the agents’ interaction a\ntraining curriculum emerges in which the agents learn tasks, many of which require\nsophisticated tool use and coordination.\nHide and Seek features cooperation (inside the team) and competition (between\nthe hiders and the seekers). It uses a self-play version of PPO for policy learning.\nIt is interesting to see how easy cooperation emerges. The game does not have\nexplicit communication for the team to coordinate cooperation, all cooperative\nbehavior emerges out of basic interaction between agents that are guided by their\nreward functions. Cooperative strategies thus emerge out of the game design: the\n7 https://www.youtube.com/watch?v=kopoLzvh5jY&t=10s\n8 https://openai.com/blog/emergent-tool-use/\n7.3 Multi-Agent Environments\n221\nFig. 7.5 Six Strategies in Hide and Seek: Running and Chasing; Fort Building; Ramp Use; Ramp\nDefense; Box Surﬁng; Surf Defense (left-to-right, top-to-bottom) [49]\nhomogeneous reward functions for each team and the environment in which blocks\nare present and follow laws of physics.\nDuring play the agents essentially construct an autocurriculum for them-\nselves [464, 49]. Six diﬀerent behavior strategies are reported, each more advanced\nthan the other, increasing the competitive pressure to ﬁnd counter strategies for\nthe opponent (see Fig. 7.5).\nBaker et al. [49] report that initially, the hiders and seekers learn the basic\nstrategy of running away and chasing. However, after much training (25 million\nepisodes), hiders start to use boxes to construct shelters behind which they hide.\nThen, after another 75 million episodes, the seekers learn to move and use ramps to\njump over obstacles into the shelter. A mere 10 million episodes later, the hiders\nlearn to defend by moving the ramps to the edge and lock them in place out of\nrange of the shelters. Then, after a long 270 million episodes of training, the seekers\nlearned box-surﬁng. They moved a box to the edge of the play area next to the\nlocked ramps. One seeker then used the ramp to climb on top of the box and the\nother seekers push it to the shelter, where the seeker could peek over te edge and\nsee the hiders. Finally, in response, the hiders locked all of the boxes in place before\nbuilding their shelter, and they were safe from the seekers.\nThe Hide and Seek experiment is interesting because of the emergence of diverse\nbehavior strategies. The strategies emerged out of a basic reward function and\nrandom exploration (see also the reward is enough argument [707]).\n222\n7 Multi-Agent Reinforcement Learning\nFig. 7.6 Capture the Flag [373]\nThe emergence of strategies out of basic reward and exploration suggests an\nevolutionary process. However, Hide and Seek does not employ population-based\ntraining or evolutionary algorithms, in contrast to the work in Capture the Flag, in\nthe next section.\n7.3.3 Mixed Behavior: Capture the Flag and StarCraft\nThe world around us exhibits a mix of competitive and cooperative behavior. Team\ncollaboration is an important aspect of human life, and it has been studied exten-\nsively in biology, sociology and artiﬁcial intelligence. It emerges (evolves) out of\nthe most basic settings—the need to achieve an ambitious goal—as we just saw. In\nrecent years many research groups have studied the mixed multi-agent model in\nreal time strategy games, such as Capture the Flag, and StarCraft. We will discuss\nboth games.\nCapture the Flag\nFirst we will discuss the game Capture the Flag, which is played in a Quake III\nArena (see Fig. 7.6). Jaderberg et al. have reported on an extensive experiment with\nthis game [373], in which the agents learn from scratch to see, act, cooperate,\nand compete. In this experiment the agents are trained with population-based\n7.3 Multi-Agent Environments\n223\nself-play [374], Alg. 7.2. The agents in the population are all diﬀerent (they have\ndiﬀerent genes). The population learns by playing against each other, providing\nincreased diversity of teammates and opponents, and a more stable and faster\nlearning process than traditional single-agent deep reinforcement learning methods.\nIn total 30 diﬀerent bots were created and pitted against each other. Agents are part\nof a team, and the reward functions form a hierarchy. A two-layer optimization\nprocess optimizes the internal rewards for winning, and uses reinforcement learning\non the internal rewards to learn the policies.\nIn Capture the Flag, the bots start by acting randomly. After 450,000 games, a bot\nstrategy was found that performed well, and they developed cooperative strategies,\nsuch as following team mates in order to outnumber opponents, and loitering near\nthe enemy base when their team mate has the ﬂag. Again, as in Hide and Seek,\ncooperative strategies emerged out of the basic rules, by combining environment\nfeedback and survival of the ﬁttest.\nThe work on Capture the Flag is notable since it demonstrated that with only\npixels as input an agent can learn to play competitively in a rich multi-agent\nenvironment. To do so it used a combination of population based training, internal\nreward optimization, and temporally hierarchical reinforcement learning (see next\nchapter).\nStarCraft\nThe ﬁnal game that we will discuss in this chapter is StarCraft. StarCraft is a\nmulti-player real-time strategy game of even larger complexity. The state space\nhas been estimated to be on the order of 101685 [573], a very large number. Star-\nCraft features multi-agent decision making under uncertainty, spatial and temporal\nreasoning, competition, team-collaboration, opponent modeling, and real-time plan-\nning. Fig. 1.6 shows a picture of a StarCraft II scene.\nResearch on StarCraft has been ongoing for some time [573], a special StarCraft\nmulti-agent challenge has been introduced [655]. A team from DeepMind has\ncreated a program called AlphaStar. In a series of test matches held in December\n2018, DeepMind’s AlphaStar beat two top players in two-player single-map matches,\nusing a diﬀerent user interface.\nAlphaStar plays the full game of StarCraft II. The neural network was initially\ntrained by supervised learning from anonymized human games, that were then\nfurther trained by playing against other AlphaStar agents, using a population-based\nversion of self-play reinforcement learning [813, 374].9 These agents are used to\nseed a multi-agent reinforcement learning process. A continuous competitive league\nwas created, with the agents of the league playing games in competition against\neach other. By branching from existing competitors, new competitors were added.\nAgents learn from games against other competitors. Population-based learning was\n9 Similar to the ﬁrst approach in AlphaGo, where self-play reinforcement learning was also\nbootstrapped by supervised learning from human games.\n224\n7 Multi-Agent Reinforcement Learning\ntaken further, creating a process that explores the very large space of StarCraft\ngame play by pitting agents against strong opponent strategies, and retaining strong\nearly strategies.\nDiversity in the league is increased by giving each agent its own learning objec-\ntive, such as which competitors it should focus on and which game unit it should\nbuild. A form of prioritized league-self-play actor-critic training is used, called\nprioritized ﬁctitious self-play—details are in [813]. AlphaStar was trained on a\ncustom-built scalable distributed training system using Google’s tensor processing\nunits (TPU). The AlphaStar league was run for 14 days. In this training, each agent\nexperienced the equivalent of 200 years of real-time StarCraft play.\nIn StarCraft players can choose to play one of three alien races: Terran, Zerg\nor Protoss. AlphaStar was trained to play Protoss only, to reduce training time,\nalthough the same training pipeline could be applied to any race. AlphaStar was\nﬁrst tested against a human grandmaster named TLO, a top professional Zerg player\nand a grandmaster level Protoss player. The human player remarked: I was surprised\nby how strong the agent was. AlphaStar takes well-known strategies and turns them\non their head. The agent demonstrated strategies I had not thought of before, which\nmeans there may still be new ways of playing the game that we haven’t fully explored\nyet.\n7.3.4 Hands On: Hide and Seek in the Gym Example\nMany of the research eﬀorts reported in these ﬁnal chapters describe signiﬁcant\neﬀorts by research teams working on complicated and large games. These games\nrepresent the frontier of artiﬁcial intelligence, and research teams use all available\ncomputational and software engineering power that they can acquire to achieve\nthe best results. Also, typically a large amount of time is spent in training, and in\nﬁnding the right hyperparameters for the learning to work.\nReplicating results of this scale is highly challenging, and most research eﬀorts\nare focused on replicating the results on a smaller, more manageable scale, with\nmore manageable computational resources.\nIn this section we will try to replicate some aspects with modest computational\nrequirements. We will focus on Hide and Seek (Sect. 7.3.2). The original code for\nthe Hide and Seek experiments is on GitHub.10 Please visit and install the code.\nHide and Seek uses MuJoCo and the mujoco-worldgen package. Install them and\nthe dependencies with\npip install -r mujoco-worldgen/requirements.txt\npip install -e mujoco-worldgen/\npip install -e multi-agent-emergence-environments/\n10 https://github.com/openai/multi-agent-emergence-environments\n7.3 Multi-Agent Environments\n225\nExamples of environments can be found in the mae_envs/envs folder. You\ncan also build your own environments, starting from the Base environment, in\nmae_envs/envs/base.py, and then adding boxes, ramps, as well as the appropri-\nate wrappers. Look in the other environments for how to do this.\nTry out environments by using the bin/examine script; example usage:\nbin/examine.py base\nSee further the instructions in the GitHub repository.\nMultiplayer Environments\nTo conclude this hands-on section, we mention a multiplayer implementation of the\nArcade Learning Environment. It is presented by Terry et al. [760], who also present\nbaseline performance results for a multiplayer version of DQN, Ape-X DQN, which\nperformed well elsewhere [350, 52]. The environment is also presented as part of\nPettingZoo, a multi-agent version of Gym [761, 791, 414].\nAnother multi-agent research environment is the Google Football Research\nEnvironment [438]. A physics-based simulator is provided, as well as three baseline\nimplementations (DQN, IMPALA, and PPO).\nSummary and Further Reading\nWe will now summarize the chapter and provide pointers to further reading.\nSummary\nMulti-agent reinforcement learning learns optimal policies for environments that\nconsist of multiple agents. The optimal policy of the agents is inﬂuenced by the\npolicies of the other agents, whose policy is also being optimized. This gives rise to\nthe problem of nonstationarity, and agent behavior violates the Markov property.\nMulti-agent reinforcement learning adds the element of cooperative behavior\nto the repertoire of reinforcement learning, which now consists of competition,\ncooperation, and mixed behavior. The ﬁeld is closely related to game theory—the\nbasis of the study of rational behavior in economics. A famous problem of game\ntheory is the prisoner’s dilemma. A famous result of non-cooperative game theory\nis the Nash equilibrium, which is deﬁned as the joint strategy where no player has\nanything to gain by changing their own strategy. A famous result from cooperative\ngame-theory is the Pareto optimum, the situation where no individual can be better\noﬀwithout making someone else worse oﬀ.\n226\n7 Multi-Agent Reinforcement Learning\nWhen agents have private information, a multi-agent problem is partially observ-\nable. Multi-agent problems can be modeled by stochastic games or as extensive form\ngames. The behavior of agents is ultimately determined by the reward functions,\nthat can be homogeneous, or heterogeneous. When agents have diﬀerent reward\nfunctions, multi-agent becomes multi-objective reinforcement learning.\nThe regret of an action is the amount of reward that is missed by the agent for\nnot choosing the actions with the highest payoﬀ. A regret minimization algorithm\nis the stochastic and multi-agent equivalent of minimax. Counterfactual-regret\nminimization is an approach for ﬁnding Nash strategies in competitive multi-agent\ngames, such as poker.\nVariants of single-agent algorithms are often used for cooperative multi-agent\nsituations. The large state space due to nonstationarity and partial observability pre-\ncludes solving large problems. Other promising approaches are opponent modeling\nand explicit communication modeling.\nPopulation-based methods such as evolutionary algorithms and swarm intelli-\ngence are used frequently in multi-agent systems. These approaches are suitable for\nhomogeneous reward functions and competitive, cooperative, and mixed problems.\nEvolutionary methods evolve a population of agents, combining behaviors, and\nselecting the best according to some ﬁtness function. Evolutionary methods are a\nnatural ﬁt for parallel computers and are among the most popular and successful\noptimization algorithms. Swarm intelligence often introduces (rudimentary) forms\nof communication between agents, such as in Ant colony optimization where agents\ncommunicate through artiﬁcial pheromones to indicate which part of the solution\nspace they have traveled.\nFor some of the most complicated problems that have recently been tackled, such\nas StarCraft, Capture the Flag, and Hide and Seek, hierarchical and evolutionary\nprinciples are often combined in league training, where leagues of teams of agents\nare trained in a self-play fashion, and where the ﬁttest agents survive. Current\nachievements require large amounts of computational power, future work is trying\nto reduce these requirements.\nFurther Reading\nMulti-agent learning is a widely studied ﬁeld. Surveys—both on early multi-agent\nreinforcement learning and on deep multi-agent reinforcement learning—can be\nfound in [292, 122, 858, 13, 14, 750, 790, 333, 332, 846]. After Littman [483], Shoham\net al. [698] look deeper into MDP modeling.\nThe classic work on game theory is Von Neumann and Morgenstern [818].\nModern introductions are [545, 180, 259]. Game theory underlies much of the\ntheory of rational behavior in classical economics. Seminal works of John Nash\nare [555, 556, 554]. In 1950 he introduced the Nash equilibrium in his dissertation\nof 28 pages, which won him the Nobel prize in Economics in 1994. A biography and\nﬁlm have been made about the life of John Nash [553].\n7.3 Multi-Agent Environments\n227\nThe game of rock-paper-scissors plays an important role in game theory, and\nthe study of computer poker [822, 89, 641, 113]. Prospect theory [390], introduced\nin 1979, studies human behavior in the face of uncertainty, a topic that evolved into\nthe ﬁeld of behavioral economics [843, 539, 128]. Gigerenzer introduced fast and\nfrugal heuristics to explain human decision making [275].\nFor more intriguing works on the ﬁeld of evolution of cooperation and the\nemergence of social norms, see, for example [105, 37, 34, 33, 35, 331, 336].\nMore recently multi-objective reinforcement learning has been studied, a sur-\nvey is [484]. In this ﬁeld the more realistic assumption is adopted that agents\nhave diﬀerent rewards functions, leading to diﬀerent Pareto optima [802, 537, 841].\nOliehoek et al. have written a concise introduction to decentralized multi-agent\nmodelling [569, 568].\nCounterfactual regret minimization has been fundamental for the success in\ncomputer poker [112, 113, 379, 881]. An often-used Monte Carlo version is published\nin [451]. A combination with function approximation is studied in [111].\nEvolutionary algorithms have delivered highly successful optimization algo-\nrithms. Some entries to this vast ﬁeld are [224, 40, 42, 41, 43]. A related ﬁeld is\nswarm intelligence, where communication between homogeneous agents is tak-\ning place [406, 217, 207, 78]. For further research in multi-agent systems refer\nto [847, 793]. For collective intelligence, see, for example [406, 208, 258, 848, 599].\nMany other works report on evolutionary algorithms in a reinforcement learning\nsetting [731, 409, 536, 838, 148, 163, 653, 842]. Most of these approaches concern\nsingle agent approaches, although some are speciﬁcally applied to multi agent\napproaches [498, 408, 713, 487, 374, 414].\nResearch into benchmarks is active. Among interesting approaches are Pro-\ncedural content generation [781], MuJoCo Soccer [487], and the Obstacle Tower\nChallenge [384]. There is an extensive literature on computer poker. See, for exam-\nple [89, 90, 277, 641, 53, 103, 90, 277, 104, 657, 533]. StarCraft research can be found\nin [815, 655, 813, 655, 573]. Other games studies are [759, 783]. Approaches inspired\nby results in poker and Go are now also being applied with success in no-press\nDiplomacy [584, 21].\nExercises\nBelow are a few quick questions to check your understanding of this chapter. For\neach question a simple, single sentence answer is suﬃcient.\nQuestions\n1. Why is there so much interest in multi-agent reinforcement learning?\n2. What is one of the main challenges of multi-agent reinforcement learning?\n3. What is a Nash strategy?\n228\n7 Multi-Agent Reinforcement Learning\n4. What is a Pareto Optimum?\n5. In a competitive multi-agent system, what algorithm can be used to calculate a\nNash strategy?\n6. What makes it diﬃcult to calculate the solution for a game of imperfect informa-\ntion?\n7. Describe the Prisoner’s dilemma.\n8. Describe the iterated Prisoner’s dilemma.\n9. Name two multi-agent card games of imperfect information.\n10. What is the setting with a heterogeneous reward function usually called?\n11. Name three kinds of strategies that can occur a multi-agent reinforcement learn-\ning.\n12. Name two solution methods that are appropriate for solving mixed strategy\ngames.\n13. What AI method is named after ant colonies, bee swarms, bird ﬂocks, or ﬁsh\nschools? How does it work in general terms?\n14. Describe the main steps of an evolutionary algorithm.\n15. Describe the general form of Hide and Seek and three strategies that emerged\nfrom the interactions of the hiders or seekers.\nExercises\nHere are some programming exercises to become more familiar with the methods\nthat we have covered in this chapter.\n1. CFR Implement counterfactual regret minimization for a Kuhn poker player. Play\nagainst the program, and see if you can win. Do you see possibilities to extend it\nto a more challenging version of poker?\n2. Hide and Seek Implement Hide and Seek with cooperation and competition. Add\nmore types of objects. See if other cooperation behavior emerges.\n3. Ant Colony Use the DeepMind Control Suite and setup a collaborative level and\na competitive level, and implement Ant Colony Optimization. Find problem\ninstances on the web, or in the original paper [208]. Can you implement more\nswarm algorithms?\n4. Football Go to the Google Football blog11 and implement algorithms for football\nagents. Consider using a population-based approach.\n5. StarCraft Go to the StarCraft Python interface,12 and implement a StarCraft\nplayer (highly challenging) [655].\n11 https://ai.googleblog.com/2019/06/introducing-google-research-football.html\n12 https://github.com/deepmind/pysc2\nChapter 8\nHierarchical Reinforcement Learning\nThe goal of artiﬁcial intelligence is to understand and create intelligent behavior;\nthe goal of deep reinforcement learning is to ﬁnd a behavior policy for ever larger\nsequential decision problems.\nBut how does real intelligence ﬁnd these policies? One of the things that humans\nare good at, is dividing a complex task into simpler subproblems, and then solving\nthose tasks, one by one, and combining them as the solution to the larger problem.\nThese subtasks are of diﬀerent scales, or granularity, than the original problem. For\nexample, when planning a trip from your house to a hotel room in a far-away city,\nyou typically only plan the start and the end in terms of footsteps taken in a certain\ndirection. The in-between part may contain diﬀerent modes of transportation that\nget you to your destination quicker, with macro steps, such as taking a trainride or\na ﬂight. During this macro, you do not try out footsteps in diﬀerent directions. Our\ntrip—our policy—is a combination of ﬁne-grain primitive actions and coarse-grain\nmacro actions.\nHierarchical reinforcement learning studies this real-world-inspired approach\nto problem solving. It provides formalisms and algorithms to divide problems\ninto larger subproblems, and then plans with these subpolicies, as if they were\nsubroutines.\nIn principle the hierarchical approach can exploit structure in all sequential deci-\nsion problems, although some problems are easier than others. Some environments\ncan be subdivided into smaller problems in a natural way, such as navigational tasks\non maps, or path-ﬁnding tasks in mazes. Multi-agent problems also naturally divide\ninto hierarchical teams, and have large state spaces where hierarchical methods\nmay help. For other problems, however, it can be hard to ﬁnd eﬃcient macros, or\nit can be computationally intensive to ﬁnd good combinations of macro steps and\nprimitive steps.\nAnother aspect of hierarchical methods is that since macro-actions take large\nsteps, they may miss the global minimum. The best policies found by hierarchical\nmethods may be less optimal than those found by “ﬂat” approaches (although they\nmay get there much quicker).\n229\n230\n8 Hierarchical Reinforcement Learning\nIn this chapter we will start with an example to capture the ﬂavor of hierarchical\nproblem solving. Next, we will look at a theoretical framework that is used to model\nhierarchical algorithms, and at a few examples of algorithms. Finally, we will look\ndeeper at hierarchical environments.\nThe chapter ends with exercises, a summary, and pointers to further reading.\nCore Concepts\n•\nSolve large, structured, problems by divide and conquer\n•\nTemporal abstraction of actions with options\nCore Problem\n•\nFind subgoals and subpolicies eﬃciently, to perform hierarchical abstraction\nCore Algorithms\n•\nOptions framework (Sect. 8.2.1)\n•\nOption critic (Sect. 8.2.3.2)\n•\nHierarchical actor critic (Sect. 8.2.3.2)\nPlanning a Trip\nLet us see how we plan a major trip to visit a friend that lives in another city, with\na hierarchical method. The method would break up the trip in diﬀerent parts. The\nﬁrst part would be to walk to your closet and get your things, and then to get out\nand get your bike. You would go to the train station, and park your bike. You would\nthen take the train to the other city, possibly changing trains enroute if that would\nbe necessary to get you there faster. Arriving in the city your friend would meet\nyou at the station and would drive you to their house.\nA “ﬂat” reinforcement learning method would have at its disposal actions consist-\ning of footsteps in certain directions. This would make for a large space of possible\npolicies, although the ﬁne grain at which the policy would be planned—individual\nfootsteps—would sure be able to ﬁnd the optimal shortest route.\nThe hierarchical method has at its disposal a wider variety of actions—macro\nactions: it can plan a bike ride, a train trip, and getting a ride by your friend. The\nroute may not be the shortest possible (who knows if the train follows the shortest\n8.1 Granularity of the Structure of Problems\n231\nroute between the two cities) but planning will be much faster than painstakingly\noptimizing footstep by footstep.\n8.1 Granularity of the Structure of Problems\nIn hierarchical reinforcement learning the granularity of abstractions is larger than\nthe ﬁne grain of the primitive actions of the environment. When we are preparing\na meal, we reason in large action chunks: chop onion, cook spaghetti, instead of\nreasoning about the actuation of individual muscles in our hands and arms. Infants\nlearn to use their muscles to performs certain tasks as they grow up until it becomes\nsecond nature.\nWe generate subgoals that act as temporal abstractions, and subpolicies that are\nmacro’s of multiple ordinary actions [673]. Temporal abstraction allows us to reason\nabout actions of diﬀerent time scales, sometimes with course grain actions—taking\na train—sometimes with ﬁne grain actions—opening a door—mixing macro actions\nwith primitive actions.\nLet us look at advantages and disadvantages of the hierarchical approach.\n8.1.1 Advantages\nWe will start with the advantages of hierarchical methods [247]. First of all, hierar-\nchical reinforcement learning simpliﬁes problems through abstraction. Problems\nare abstracted into a higher level of aggregation. Agents create subgoals and solve\nﬁne grain subtasks ﬁrst. Actions are abstracted into larger macro actions to solve\nthese subgoals; agents use temporal abstraction.\nSecond, the temporal abstractions increase sample eﬃciency. The number of\ninteractions with the environment is reduced because subpolicies are learned to\nsolve subtasks, reducing the environment interactions. Since subtasks are learned,\nthey can be transfered to other problems, supporting transfer learning.\nThird, subtasks reduce brittleness due to overspecialization of policies. Policies\nbecome more general, and are able to adapt to changes in the environment more\neasily.\nFourth, and most importantly, the higher level of abstraction allows agents to\nsolve larger, more complex problems. This is a reason why for complex multi-agent\ngames such as StarCraft, where teams of agents must be managed, hierarchical\napproaches are used.\nMulti-agent reinforcement learning often exhibits a hierarchical structure; prob-\nlems can be organized such that each agent is assigned its own subproblem, or the\nagents themselves may be structured or organized in teams or groups. There can be\ncooperation within the teams or competition between the teams, or the behavior\ncan be fully cooperative or fully competitive.\n232\n8 Hierarchical Reinforcement Learning\nFlet-Berliac [247], in a recent overview, summarizes the promise of hierarchical\nreinforcement learning as follows: (1) achieve long-term credit assignment through\nfaster learning and better generalization, (2) allow structured exploration by explor-\ning with sub-policies rather than with primitive actions, and (3) perform transfer\nlearning because diﬀerent levels of hierarchy can encompass diﬀerent knowledge.\n8.1.2 Disadvantages\nThere are also disadvantages and challenges associated with hierarchical rein-\nforcement learning. First of all, it works better when there is domain knowledge\navailable about structure in the domain. Many hierarchical methods assume that\ndomain knowledge is available to subdivide the environment so that hierarchical\nreinforcement learning can be applied.\nSecond, there is algorithmic complexity to be solved. Subgoals must be identiﬁed\nin the problem environment, subpolicies must be learned, and termination condi-\ntions are needed. These algorithms must be designed, which costs programmer\neﬀort.\nThird, hierarchical approaches introduce a new type of actions, macro-actions.\nMacros are combinations of primitive actions, and their use can greatly improve\nthe performance of the policy. On the other hand, the number of possible combi-\nnations of actions is exponentially large in their length [44]. For larger problems\nenumeration of all possible macros is out of the question, and the overall-policy\nfunction has to be approximated. Furthermore, at each decision point in a hierar-\nchical planning or learning algorithm we now have the option to consider if any\nof the macro actions improves the current policy. The computational complexity\nof the planning and learning choices increases by the introduction of the macro\nactions [44], and approximation methods must be used. The eﬃciency gains of the\nhierarchical behavioral policy must outweigh the higher cost of ﬁnding this policy.\nFourth, the quality of a behavioral policy that includes macro-actions may be\nless than that of a policy consisting only of primitive actions. The macro-actions\nmay skip over possible shorter routes, that the primitive actions would have found.\nConclusion\nThere are advantages and disadvantages to hierarchical reinforcement learning.\nWhether an eﬃcient policy can be constructed and whether its accuracy is good\nenough depends on the problem at hand, and also on the quality of the algorithms\nthat are used to ﬁnd this policy.\nFor a long time, ﬁnding good subgoals has been a major challenge. With recent\nalgorithmic advances, especially in function approximation, important progress\nhas been made. We will discuss these advances in the next section.\n8.2 Divide and Conquer for Agents\n233\n8.2 Divide and Conquer for Agents\nTo discuss hierarchical reinforcement learning, ﬁrst we will discuss a model, the\noptions framework, that formalizes the concepts of subgoals and subpolicies. Next,\nwe will describe the main challenge of hierarchical reinforcement learning, which\nis sample eﬃciency. Then we will discuss the main part of this chapter: algorithms\nfor ﬁnding subgoals and subpolicies, and ﬁnally we will provide an overview of\nalgorithms that have been developed in the ﬁeld.\n8.2.1 The Options Framework\nA hierarchical reinforcement learning algorithm tries to solve sequential decision\nproblems more eﬃciently by identifying common substructures and re-using sub-\npolicies to solve them. The hierarchical approach has three challenges [619, 435]:\nﬁnd subgoals, ﬁnd a meta-policy over these subgoals, and ﬁnd subpolicies for these\nsubgoals.\nNormally, in reinforcement learning, the agent follows in each state the action\nthat is indicated by the policy. In 1999, Sutton, Precup and Singh [745] introduced\nthe options framework. This framework introduces formal constructs with which\nsubgoals and subpolicies can be incorporated elegantly into the reinforcement\nlearning setting. The idea of options is simple. Whenever a state is reached that\nis a subgoal, then, in addition to following a primitive action suggested by the\nmain policy, the option can be taken. This means that not the main action policy is\nfollowed, but the option policy, a macro action consisting of a diﬀerent subpolicy\nspecially aimed at satisfying the subgoal in one large step. In this way macros are\nincorporated into the reinforcement learning framework.\nWe have been using the terms macro and option somewhat loosely until now;\nthere is, however, a diﬀerence between macros and options. A macro is any group\nof actions, possibly open-ended. An option is a group of actions with a termination\ncondition. Options take in environment observations and output actions until a\ntermination condition is met.\nFormally, an option 𝜔has three elements [16]. Each option 𝜔= ⟨𝐼, 𝜋, 𝛽⟩has the\nfollowing triple:\n𝐼𝜔\nThe initiation set 𝐼⊆𝑆are the states that the option can start from\n𝜋𝜔(𝑎|𝑠)\nThe subpolicy 𝜋: 𝑆× 𝐴→[0, 1] internal to this particular option\n𝛽𝜔(𝑠)\nThe termination condition 𝛽: 𝑆→[0, 1] tells us if 𝜔terminates in 𝑠\nThe set of all options is denoted as Ω. In the options framework, there are thus\ntwo types of policies: the (meta-)policy over options 𝜋Ω(𝜔|𝑠) and the subpolicies\n𝜋𝜔(𝑎|𝑠). The subpolicies 𝜋𝜔are short macros to get from 𝐼𝜔to 𝛽𝜔quickly, using\nthe previously learned macro (subpolicy). Temporal abstractions mix actions of\ndiﬀerent granularity, short and long, primitive action and subpolicy. They allow\n234\n8 Hierarchical Reinforcement Learning\nFig. 8.1 Multi-Room Grid [745]\ntraveling from 𝐼to 𝛽without additional learning, using a previously provided or\nlearned subpolicy.\nOne of the problems for which the options framework works well, is room\nnavigation in a Grid world (Fig. 8.1). In a regular reinforcement learning problem the\nagent would learn to move step by step. In hierarchical reinforcement learning the\ndoors between rooms are bottleneck states, and are natural subgoals. Macro actions\n(subpolicies) are to move to a door in one multi-step action (without considering\nalternative actions along the way). Then we can go to a diﬀerent room, if we choose\nthe appropriate option, using another macro, closer to where the main goal is\nlocated. The four-room problem from the ﬁgure is used in many research works in\nhierarchical reinforcement learning.\nUniversal Value Function\nIn the original options framework the process of identifying the subgoals (the\nhallways, doors) is external. The subgoals have to be provided manually, or by other\nmethods [609, 316, 444, 727]. Subsequently, methods have been published to learn\nthese subgoals.\nOptions are goal-conditioned subpolicies. More recently a generalization to\nparameterized options has been presented in the universal value function, by Schaul\net al. [665]. Universal value functions provide a uniﬁed theory for goal-conditioned\nparameterized value approximators 𝑉(𝑠, 𝑔, 𝜃).\n8.2.2 Finding Subgoals\nWhether the hierarchical method improves over a traditional ﬂat method depends\non a number of factors. First, there should be enough repeating structure in the\ndomain to be exploited (are there many rooms?), second, the algorithm must ﬁnd\n8.2 Divide and Conquer for Agents\n235\nFind\nFind\nName\nAgent\nEnvironment\nSubg Subpol Ref\nSTRIPS\nMacro-actions\nSTRIPS planner\n-\n-\n[241]\nAbstraction Hier. State abstraction\nScheduling/plan.\n+\n+\n[415]\nHAM\nAbstract machines\nMDP/maze\n-\n-\n[588]\nMAXQ\nValue function decomposition\nTaxi\n-\n-\n[196]\nHTN\nTask networks\nBlock world\n-\n-\n[171, 272]\nBottleneck\nRandomized search\nFour room\n+\n+\n[727]\nFeudal\nmanager/worker, RNN\nAtari\n+\n+\n[811, 182]\nSelf p. goal emb. self play subgoal\nMazebase, AntG\n+\n+\n[732]\nDeep Skill Netw. deep skill array, policy distillation Minecraft\n+\n+\n[766]\nSTRAW\nend-to-end implicit plans\nAtari\n+\n+\n[810]\nHIRO\noﬀ-policy\nAnt maze\n+\n+\n[546]\nOption-critic\npolicy-gradient\nFour room\n+\n+\n[45]\nHAC\nactor critic, hindsight exper. repl. Four room ant\n+\n+\n[471, 19]\nModul. pol. hier. bit-vector, intrinsic motivation\nFetchPush\n+\n+\n[590]\nh-DQN\nintrinsic motivation\nMontezuma’s R.\n-\n+\n[435]\nMeta l. sh. hier.\nshared primitives, strength metric Walk, crawl\n+\n+\n[256]\nCSRL\nmodel-based transition dynamics Robot tasks\n-\n+\n[477]\nLearning Repr.\nunsup. subg. disc., intrinsic motiv. Montezuma’s R.\n+\n+\n[619]\nAMIGo\nAdversarially intrinsic goals\nMiniGrid PCG\n+\n+\n[125]\nTable 8.1 Hierarchical Reinforcement Learning Approaches (Tabular and Deep)\nappropriate subgoals (can it ﬁnd the doors?), third, the options that are found\nmust repeat many times (is the puzzle played frequently enough for the option-\nﬁnding cost to be oﬀset?), and, ﬁnally, subpolicies must be found that give enough\nimprovement (are the rooms large enough that options outweigh actions?).\nThe original options framework assumes that the structure of the domain is\nobvious, and that the subgoals are given. When this is not the case, then the subgoals\nmust be found by the algorithm. Let us look at an overview of approaches, both\ntabular and with deep function approximation.\n8.2.3 Overview of Hierarchical Algorithms\nThe options framework provides a convenient formalism for temporal abstraction.\nIn addition to the algorithms that can construct policies consisting of individual\nactions, we need algorithms that ﬁnd the subgoals, and learn the subpolicies. Finding\neﬃcient algorithms for the three tasks is important in order to be able to achieve\nan eﬃciency advantage over ordinary “ﬂat” reinforcement learning.\nHierarchical reinforcement learning is based on subgoals. It implements a top-\nlevel policy over these subgoals, and subpolicies to solve the subgoals. The landscape\nof subgoals determines to a great extent the eﬃciency of the algorithm [216]. In re-\ncent years new algorithms have been developed to ﬁnd sub-policies for options, and\nthe ﬁeld has received a renewed interest [592]. Table 8.1 shows a list of approaches.\n236\n8 Hierarchical Reinforcement Learning\nThe table starts with classic tabular approaches (above the line). It continues with\nmore recent deep learning approaches. We will now look at some of the algorithms.\n8.2.3.1 Tabular Methods\nDivide and conquer is a natural method to exploit hierarchical problem structures.\nA famous early planning system is STRIPS, the Stanford Research Insititute Problem\nSolver, designed by Richard Fikes and Nils Nilsson in the 1970s [241]. STRIPS created\nan extensive language for expressing planning problems, and was quite inﬂuential.\nConcepts from STRIPS are at the basis of most modern planning systems, action\nlanguages, and knowledge representation systems [266, 51, 798]. The concept of\nmacros as open-ended groups of actions was used in STRIPS to create higher-level\nprimitives, or subroutines.\nLater planning-based approaches are Parr and Russell’s hierarchical abstract\nmachines [588] and Dietterich’s MAXQ [196]. Typical applications of these systems\nare the blocks world, in which a robot arm has to manipulate blocks, stacking them\non top of each other, and the taxi world, which we have seen in earlier chapters.\nAn overview of these and other early approaches can be found in Barto et al. [56].\nMany of these early approaches focused on macros (the subpolicies), and require\nthat the experimenters identify the subgoals in a planning language. For problems\nwhere no such obvious subgoals are available, Knoblock [415] showed how abstrac-\ntion hierarchies can be generated, although Backstrom et al. [44] found that doing\nso can be exponentially less eﬃcient. For small room problems, however, Stolle and\nPrecup [727] showed that subgoals can be found in a more eﬃcient way, using a\nshort randomized search to ﬁnd bottleneck states that can be used as subgoals. This\napproach ﬁnds subgoals automatically, and eﬃciently, in a rooms-grid world.\nTabular hierarchical methods were mostly applied to small and low-dimensional\nproblems, and have diﬃculty ﬁnding subgoals, especially for large problems. The\nadvent of deep function approximation methods attracted renewed interest in\nhierarchical methods.\n8.2.3.2 Deep Learning\nFunction approximation can potentially reduce the problem of exponentially ex-\nploding search spaces that plague tabular methods, especially for subgoal discovery.\nDeep learning exploits similarities between states using commonalities between fea-\ntures, and allows larger problems to be solved. Many new methods were developed.\nThe deep learning approaches in hierarchical reinforcement learning typically are\nend-to-end: they generate both appropriate subgoals and their policies.\nFeudal networks is an older idea from Dayan and Hinton in which an explicit\ncontrol hierarchy is built of managers and workers that work on tasks and subtasks,\norganized as in a feudal ﬁefdom [182]. This idea was used 15 years later as a\nmodel for hierarchical deep reinforcement learning by Vezhnevets et al. [811], out-\n8.2 Divide and Conquer for Agents\n237\nperforming non-hierarchical A3C on Montezuma’s Revenge, and performing well\non other Atari games, achieving a similar score as Option-critic [45]. The approach\nuses a manager that sets abstract goals (in latent space) for workers. The feudal idea\nwas also used as inspiration for a multi-agent cooperative reinforcement learning\ndesign [9], on proof of concept cooperative multi-agent problems on pre-speciﬁed\nhierarchies.\nOther deep learning approaches include deep skill networks [766], oﬀ-policy\napproaches [546], and self-play [732]. The latter uses an intrinsic motivation ap-\nproach to learn both a low level actor and the representation of the state space [597].\nSubgoals are learned at the higher level, after which policies are trained at the lower\nlevel. Application environments for deep learning have become more challenging,\nand now include Minecraft, and robotic tasks such as ant navigation in multiple\nrooms, and maze navigation. The approaches outperform basic non-hierarchical\napproaches such as DQN.\nIn STRAW Vezhnevets et al. [810] learns a model of macros-actions, and is\nevaluated on text recognition tasks and on Atari games such as PacMan and Frostbite,\nshowing promising results. Zhang et al. [873] use world models to learn latent\nlandmarks (subgoals) for graph-based planning (see also Sect. 5.2.1.2).\nAlmost two decades after the options framework was introduced, Bacon et\nal. [45] introduced the option-critic approch. Option-critic extends the options\nframework with methods to learn the option subgoal and subpolicy, so that it does\nnot have to be provided externally anymore. The options are learned similar to actor\ncritic using a gradient-based approach. The intra-option policies and termination\nfunctions, as well as the policy over options are learned simultaneously. The user of\nthe Option-critic approach has to specify how many options have to be learned. The\nOption-critic paper reports good results for experiments in a four-room environment\nwith 4 and with 8 options (Fig. 8.2). Option critic learns options in an end-to-end\nfashion that scales to larger domains, outperforming DQN in four ALE games\n(Asterix, Seaquest, Ms. Pacman, Zaxxon) [45].\nLevy et al. [471] presented an approach based on Option critic, called Hierarchical\nactor critic. This approach can learn the goal-conditioned policies at diﬀerent levels\nconcurrently, where previous approaches had to learn them in a bottom up fashion.\nIn addition, Hierarchical actor critic uses a method for learning the multiple levels\nof policies for sparse rewards, using Hindsight experience replay [19]. In typical\nrobotics tasks, the reinforcement learning algorithm learns more from a successful\noutcome (bat hits the ball) than from an unsuccessful outcome (bat misses the\nball, low). In this failure case a human learner would draw the conclusion that we\ncan now reach another goal, being bat misses the ball if we aim low. Hindsight\nexperience replay allows learning to take place by incorporating such adjustments\nof the goal using the beneﬁt of hindsight, so that the algorithm can now also learn\nfrom failures, by pretending that they were the goal that you wanted to reach, and\nlearn from them as if they were.\nHierarchical actor critic has been evaluated on grid world tasks and more complex\nsimulated robotics environments, using a 3-level hierarchy.\n238\n8 Hierarchical Reinforcement Learning\nFig. 8.2 Termination Probabilities Learned with 4 Options by Option-Critic [45]; Options Tend to\nFavor Squares Close to Doors\nA ﬁnal approach that we mention is AMIGo [125], which is related to intrinsic\nmotivation. It uses a teacher to adversarially generate goals for a student. The stu-\ndent is trained with increasingly challenging goals to learn general skills. The system\neﬀectively builds up an automatic curriculum of goals. It is evaluated on MiniGrid, a\nparameterized world that is generated by procedural content generation [621, 143].\nConclusion\nLooking back at the list of advantages and disadvantages at the start of this chap-\nter, we see a range of interesting and creative ideas that achieve the advantages\n(Sect. 8.1.1) by providing methods to address the disadvantages (Sect. 8.1.2). In\ngeneral, the tabular methods are restricted to smaller problems, and often need to\nbe provided with subgoals. Most of the newer deep learning methods ﬁnd subgoals\nby themselves, for which then subpolicies are found. Many promising methods have\nbeen discussed, and most report to outperform one or more ﬂat baseline algorithms.\nThe promising results stimulate further research in deep hierarchical methods,\nand more benchmark studies of large problems are needed. Let us have a closer\nlook at the environments that have been used so far.\n8.3 Hierarchical Environments\n239\nFig. 8.3 Four Rooms, and One Room with Subpolicy and Subgoal [745]\n8.3 Hierarchical Environments\nMany environments for hierarchical reinforcement learning exist, starting with\nmazes and the four-room environment from the options paper. Environments\nhave evolved with the rest of the ﬁeld of reinforcement learning; for hierarchical\nreinforcement learning no clear favorite benchmark has emerged, although Atari\nen MuJoCo tasks are often used. In the following we will review some of the\nenvironments that are used in algorithmic studies. Most hierarchical environments\nare smaller than typically used for model-free ﬂat reinforcement learning, although\nsome studies do use complex environments, such as StarCraft.\n8.3.1 Four Rooms and Robot Tasks\nSutton et al. [745] presented the four rooms problems to illustrate how the options\nmodel worked (Fig. 8.3; left panel). This environment has been used frequently\nin subsequent papers on reinforcement learning. The rooms are connected by\nhallways. Options point the way to these hallways, which lead to the goal 𝐺2 of the\nenvironment. A hierarchical algorithm should identify the hallways as the subgoals,\nand create subpolicies for each room to go to the hallway subgoal (Fig. 8.3; right\npanel).\nThe four-room environment is a toy environment with which algorithms can be\nexplained. More complex versions can be created by increasing the dimensions of\nthe grids and by increasing the number of rooms.\nThe Hierarchical actor critic paper uses the four-room environment as a basis\nfor a robot to crawl through. The agent has to learn both the locomotion task and\nsolving the four-room problem (Fig. 8.4). Other environments that are used for\nhierarchical reinforcement learning are robot tasks, such as shown in Fig. 8.5 [633].\n240\n8 Hierarchical Reinforcement Learning\nFig. 8.4 Ant in Four Rooms [471]\nFig. 8.5 Six Robot Tasks [633]\n8.3.2 Montezuma’s Revenge\nOne of the most diﬃcult situations for reinforcement learning is when there is little\nreward signal, and when it is delayed. The game of Montezuma’s Revenge consists\nof long stretches in which the agent has to walk without the reward changing.\nWithout smart exploration methods this game cannot be solved. Indeed, the game\nhas long been a test bed for research into goal-conditioned and exploration methods.\nFor the state in Fig. 8.6, the player has to go through several rooms while col-\nlecting items. However, to pass through doors (top right and top left corners), the\nplayer needs the key. To pick up the key, the player has to climb down the ladders\n8.3 Hierarchical Environments\n241\nFig. 8.6 Montezuma’s Revenge [71]\nFig. 8.7 Intrinsic Motivation in Reinforcement Learning [712]\nand move towards the key. This is a long and complex sequence before receiving the\nreward increments for collecting the key. Next, the player has to go to the door to\ncollect another increase in reward. Flat reinforcement learning algorithms struggle\nwith this environment. For hierarchical reinforcement the long stretches without\na reward can be an opportunity to show the usefulness of the option, jumping\nthrough the space from states where the reward changes to another reward change.\nTo do so, the algorithm has to be able to identify the key as a subgoal.\nRafati and Noelle [619] learn subgoals in Montezuma’s Revenge, and so do\nKulkarni et al. [435]. Learning to choose promising subgoals is a challenging problem\nby itself. Once subgoals are found, the subpolicies can be learned by introducing\na reward signal for achieving the subgoals. Such intrinsic rewards are related to\nintrinsic motivation and the psychological concept of curiosity [29, 577].\nFigure 8.7 illustrates the idea behind intrinsic motivation. In ordinary reinforce-\nment learning, a critic in the environment provides rewards to the agent. When\nthe agent has an internal environment where an internal critic provides rewards,\nthese internal rewards provide an intrinsic motivation to the agent. This mechanism\naims to more closely model exploration behavior in animals and humans [712]. For\nexample, during curiosity-driven activities, children use knowledge to generate\nintrinsic goals while playing, building block structures, etc. While doing this, they\n242\n8 Hierarchical Reinforcement Learning\nconstruct subgoals such as putting a lighter entity on top of a heavier entity in\norder to build a tower [435, 712]. Intrinsic motivation is an active ﬁeld of research.\nA recent survey is [29].\nMontezuma’s Revenge has also been used as benchmark for the Go-Explore\nalgorithm, that has achieved good results in sparse reward problems, using a goal-\nconditioned policy with cell aggregation [222]. Go-Explore performs a planning-like\nform of backtracking, combining elements of planning and learning in a diﬀerent\nway than AlphaZero.\n8.3.3 Multi-Agent Environments\nMany multi-agent problems are a natural match for hierarchical reinforcement\nlearning, since agents often work together in teams or other hierarchical structure.\nFor multi-agent hierarchical problems a multitude of diﬀerent environments are\nused.\nMakar et al. [499, 273] study cooperative multi-agent learning, and use small tasks\nsuch as a two-agent cooperative trash collection task, the dynamic rescheduling\nof automated guided vehicles in a factory, as well as an environment in which\nagents communicate amongst eachother. Han et al. [312] use a multi-agent Taxi\nenvironment. Tang et al. [753] also use robotic trash collection.\nDue to the computational complexity of multi-agent and hierarchical environ-\nments, many of the environments are of lower dimensionality than what we see\nin model-free and model-based single-agent reinforcement learning. There are a\nfew exceptions, as we saw in the previous chapter (Capture the Flag and StarCraft).\nHowever, the algorithms that were used for these environments were based on\npopulation-based self-play algorithms that are well-suited for parallelization; hier-\narchical reinforcement algorithms of the type that we have discussed in this chapter\nwere of less importance [813].\n8.3.4 Hands On: Hierarchical Actor Citic Example\nThe research reported in this chapter is of a more manageable scale than in some\nother chapters. Environments are smaller, computational demands are more rea-\nsonable. Four-room experiments and experiments with movement of single robot\narms invites experimentation and tweaking. Again, as in the other chapters, the\ncode of most papers can be found online on GitHub.\nHierarchical reinforcement learning is well suited for experimentation because\nthe environments are small, and the concepts of hierarchy, team, and subgoal, are\nintuitively appealing. Debgugging one’s implementation should be just that bit\neasier when the desired behavior of the diﬀerent pieces of code is clear.\n8.3 Hierarchical Environments\n243\nAlgorithm 8.1 Hierarchical Actor Critic [471]\nInput:\nKey agent parameters: number of levels in hierarchy 𝑘, maximum subgoal horizon 𝐻, and\nsubgoal testing frequency 𝜆\nOutput: 𝑘trained actor and critic functions 𝜋0, ..., 𝜋𝑘−1, 𝑄0, ..., 𝑄𝑘−1\nfor 𝑀episodes do\n⊲Train for M episodes\n𝑠←𝑆init, 𝑔←𝐺𝑘−1\n⊲Sample initial state and task goal\ntrain-level(𝑘−1, 𝑠, 𝑔)\n⊲Begin training\nUpdate all actor and critic networks\nend for\nfunction train-level(𝑖:: level, 𝑠:: state, 𝑔:: goal)\n𝑠𝑖←𝑠, 𝑔𝑖←𝑔\n⊲Set current state and goal for level 𝑖\nfor 𝐻attempts or until 𝑔𝑛, 𝑖≤𝑛< 𝑘achieved do\n𝑎𝑖←𝜋𝑖(𝑠𝑖, 𝑔𝑖) + noise (if not subgoal testing)\n⊲Sample (noisy) action from policy\nif 𝑖> 0 then\nDetermine whether to test subgoal 𝑎𝑖\n𝑠′\n𝑖←train-level(𝑖−1, 𝑠𝑖, 𝑎𝑖)\n⊲Train level 𝑖−1 using subgoal 𝑎𝑖\nelse\nExecute primitive action 𝑎0 and observe next state 𝑠′\n0\nend if\n⊲Create replay transitions\nif 𝑖> 0 and 𝑎𝑖missed then\nif 𝑎𝑖was tested then\n⊲Penalize subgoal 𝑎𝑖\nReplay_Buﬀer𝑖←[𝑠= 𝑠𝑖, 𝑎= 𝑎𝑖, 𝑟= Penalty, 𝑠′ = 𝑠′\n𝑖, 𝑔= 𝑔𝑖, 𝛾= 0]\nend if\n𝑎𝑖←𝑠′\n𝑖\n⊲Replace original action with action executed in hindsight\nend if\n⊲Evaluate executed action on current goal and hindsight goals\nReplay_Buﬀer𝑖←[𝑠= 𝑠𝑖, 𝑎= 𝑎𝑖, 𝑟∈{−1, 0}, 𝑠′ = 𝑠′\n𝑖, 𝑔= 𝑔𝑖, 𝛾∈{𝛾, 0}]\nHER_Storage𝑖←[𝑠= 𝑠𝑖, 𝑎= 𝑎𝑖, 𝑟=TBD, 𝑠′ = 𝑠′\n𝑖, 𝑔=TBD, 𝛾=TBD]\n𝑠𝑖←𝑠′\n𝑖\nend for\nReplay_Buﬀer𝑖←Perform HER using HER_Storage𝑖transitions\nreturn 𝑠′\n𝑖\n⊲Output current state\nend function\nTo get you started with hierarchical reinforcement learning we will go to HAC:\nHierarchical actor critic [471]. Algorithm 8.1 shows the pseudocode, where TBD is\nthe subgoal in hindsight [471]. A blog1 with animations has been written, a video2\nhas been made of the results, and the code can be found in GitHub.3\nTo run the hierarchical actor critic experiments, you need MuJoCo and the\nrequired Python wrappers. The code is TensorFlow 2 compatible. When you have\ncloned the repository, run the experiment with\n1 http://bigai.cs.brown.edu/2019/09/03/hac.html\n2 https://www.youtube.com/watch?v=DYcVTveeNK0\n3 https://github.com/andrew-j-levy/Hierarchical-Actor-Critc-HAC-\n244\n8 Hierarchical Reinforcement Learning\npython3 initialize_HAC.py --retrain\nwhich will train a UR5 reacher agent with a 3-level hierarchy. Here is a video4\nthat shows how it should look like after 450 training episodes. You can watch your\ntrained agent with the command\npython3 initialize_HAC.py --test --show\nThe README at the GitHub repository contains more suggestions on what to\ntry. You can try diﬀerent hyperparameters, and you can modify the designs, if you\nfeel like it. Happy experimenting!\nSummary and Further Reading\nWe will now summarize the chapter and provide pointers to further reading.\nSummary\nA typical reinforcement learning algorithm moves in small steps. For a state, it picks\nan action, gives it to the environment for a new state and a reward, and processes\nthe reward to pick a new action. Reinforcement learning works step by small step.\nIn contrast, consider the following problem: in the real world, when we plan a trip\nfrom A to B, we use abstraction to reduce the state space, to be able to reason at a\nhigher level. We do not reason at the level of footsteps to take, but we ﬁrst decide on\nthe mode of transportation to get close to our goal, and then we ﬁll in the diﬀerent\nparts of the journey with small steps.\nHierarchical reinforcement learning tries to mimic this idea: conventional rein-\nforcement learning works at the level of a single state; hierarchical reinforcement\nlearning performs abstraction, solving subproblems in sequence. Temporal ab-\nstraction is described in a paper by Sutton et al. [745]. Hierarchical reinforcement\nlearning uses the principles of divide and conquer to make solving large problems\nfeasible. It ﬁnds subgoals in the space that it solves with subpolicies (macros or\noptions).\nDespite the appealing intuition, progress in hierarchical reinforcement learning\nwas initially slow. Finding these new subgoals and subpolicies is a computationally\nintensive problems that is exponential in the number of actions, and in some\nsituations it is quicker to use conventional “ﬂat” reinforcement learning methods,\nunless domain knowledge can be exploited. The advent of deep learning provided a\n4 https://www.youtube.com/watch?v=R86Vs9Vb6Bc\n8.3 Hierarchical Environments\n245\nboost to hierarchical reinforcement learning, and much progress is being reported\nin important tasks such as learning subgoals automatically, and ﬁnding subpolicies.\nAlthough popular for single-agent reinforcement learning, hierarchical methods\nare also used in multi-agent problems. Multi-agent problems often feature agents\nthat work in teams, that cooperate within, and compete between the teams. Such\nan agent-hierarchy is a natural ﬁt for hierachical solution methods. Hierarchical\nreinforcment learning remains a promising technique.\nFurther Reading\nHierarchical reinforcement learning, and subgoal ﬁnding, have a rich and long\nhistory [248, 588, 745, 609, 196, 316, 444, 56, 592]; see also Table 8.1. Macro-actions\nare a basic approach [316, 626]. Others, using macros, are [855, 852, 214]. The\noptions framework has provided a boost to the development of the ﬁeld [745].\nOther approaches are MAXQ [195] and Feudal networks [811].\nEarlier tabular approaches are [248, 588, 745, 609, 196, 316, 444].\nRecent method are Option-critic [45] and Hierarchical actor-critic [471]. There\nare many deep learning methods for ﬁnding subgoals and subpolicies [471, 582,\n248, 598, 546, 256, 806, 665, 735, 177]. Andrychowicz et al. [19] introduce Hindsight\nexperience replay, which can improve performance for hierarchical methods.\nInstrinsic motivation is a concept from developmental neuroscience that has\ncome to reinforcement learning with the purpose of providing learning signals\nin large spaces. It is related to curiosity. Botvinick et al. [101] have written an\noverview of hierarchical reinforcement learning and neuroscience. Aubret et al. [29]\nprovide a survey of intrinsic motivation for reinforcement learning. Instrinsic\nmotivation is used by [435, 619]. Intrinsic motivation is closely related to goal-\ndriven reinforcement learning [665, 650, 576, 577, 578].\nExercises\nIt is time for the Exercises to test your knowledge.\nQuestions\nBelow are some quick questions to check your understanding of this chapter. For\neach question a simple, single sentence answer should be suﬃcient.\n1. Why can hierarchical reinforcement learning be faster?\n2. Why can hierarchical reinforcement learning be slower?\n3. Why may hierarchical reinforcement learning give an answer of lesser quality?\n4. Is hierachical reinforcement more general or less general?\n246\n8 Hierarchical Reinforcement Learning\n5. What is an option?\n6. What are the three elements that an option consists of?\n7. What is a macro?\n8. What is intrinsic motivation?\n9. How do multi agent and hierarchical reinforcement learning ﬁt together?\n10. What is so special about Montezuma’s Revenge?\nExercises\nLet us go to the programming exercises to become more familiar with the methods\nthat we have covered in this chapter.\n1. Four Rooms Implement a hierarchical solver for the four-rooms environment. You\ncan code the hallway subgoals using domain knowledge. Use a simple tabular,\nplanning, approach. How will you implement the subpolicies?\n2. Flat Implement a ﬂat planning or Q-learning-based solver for 4-rooms. Compare\nthis program to the tabular hierarchical solver. Which is quicker? Which of the\ntwo does fewer environment actions?\n3. Sokoban Implement a Sokoban solver using a hierarchical approach (challeng-\ning). The challenge in Sokoban is that there can be dead-ends in the game that\nyou create rendering the game unsolvable (also see the literature [696, 291]).\nRecognizing these dead-end moves is important. What are the subgoals? Rooms,\nor each box-task is one subgoal, or can you ﬁnd a way to code dead-ends as\nsubgoal? How far can you get? Find Sokoban levels.567\n4. Petting Zoo Choose one of the easier multi-agent problems from the Petting\nZoo [761], introduce teams, and write a hierarchical solver. First try a tabular\nplanning approach, then look at hierarchical actor critic (challenging).\n5. StarCraft The same as the previous exercise, only now with StarCraft (very\nchallenging).\n5 http://sneezingtiger.com/sokoban/levels.html\n6 http://www.sokobano.de/wiki/index.php?title=Level_format\n7 https://www.sourcecode.se/sokoban/levels\nChapter 9\nMeta-Learning\nAlthough current deep reinforcement learning methods have obtained great suc-\ncesses, training times for most interesting problems are high; they are often mea-\nsured in weeks or months, consuming time and resources—as you may have noticed\nwhile doing some of the exercises at the end of the chapters.\nModel-based methods aim to reduce the sample complexity in order to speed\nup learning—but still, for each new task a new network has to be trained from\nscratch. In this chapter we turn to another approach, that aims to re-use information\nlearned in earlier training tasks from a closely related problem. When humans learn\na new task, they do not learn from a blank slate. Children learn to walk and then\nthey learn to run; they follow a training curriculum, and they remember. Human\nlearning builds on existing knowledge, using knowledge from previously learned\ntasks to facilitate the learning of new tasks. In machine learning such transfer of\npreviously learned knowledge from one task to another is called transfer learning.\nWe will study it in this chapter.\nHumans learn continuously. When learning a new task, we do not start from\nscratch, zapping our minds ﬁrst to emptiness. Previously learned task-representa-\ntions allow us to learn new representations for new tasks quickly; in eﬀect, we\nhave learned to learn. Understanding how we (learn to) learn has intrigued artiﬁcial\nintelligence researchers since the early days, and it is the topic of this chapter.\nThe ﬁelds of transfer learning and meta-learning are tightly related. For both,\nthe goal is to speed up learning a new task, using previous knowledge. In transfer\nlearning, we pretrain our parameter network with knowledge from a single task. In\nmeta-learning, we use multiple related tasks.\nIn this chapter, we ﬁrst discuss the concept of lifelong learning, something that\nis quite familiar to human beings. Then we discuss transfer learning, followed\nby meta-learning. Next, we discuss some of the benchmarks that are used to test\ntransfer learning and meta-learning.\nThe chapter is concluded with exercises, a summary, and pointers to further\nreading.\n247\n248\n9 Meta-Learning\nCore Concepts\n•\nKnowledge transfer\n•\nLearning to learn\nCore Problem\n•\nSpeed-up learning with knowledge from related tasks\nCore Algorithms\n•\nPretraining (Listing 9.1)\n•\nModel-Agnostic Meta-Learning (Alg. 9.1)\nFoundation Models\nHumans are good at meta-learning. We learn new tasks more easily after we have\nlearned other tasks. Teach us to walk, and we learn how to run. Teach us to play\nthe violin, the viola, and the cello, and we more easily learn to play the double bass\n(Fig. 9.1).\nCurrent deep learning networks are large, with many layers of neurons and\nmillions of parameters. For new problems, training large networks on large datasets\nor environments takes time, up to weeks, or months—both for supervised and for\nreinforcement learning. In order to shorten training times for subsequent networks,\nthese are often pretrained, using foundation models [96]. With pretraining, some of\nthe exisiting weights of another network are used as starting point for ﬁnetuning a\nnetwork on a new dataset, instead of using a randomly initialized network.\nPretraining works especially well on deeply layered architectures. The reason is\nthat the “knowledge” in the layers goes from generic to speciﬁc: lower layers contain\ngeneric ﬁlters such as lines and curves, and upper layers contain more speciﬁc\nﬁlters such as ears, noses, and mouths (for a face recognition application) [459, 497].\nThese lower layers contain more generic information that is well suited for transfer\nto other tasks.\nFoundation models are large models in a certain ﬁeld, such as image recogni-\ntion, or natural language processing, that are trained extensively on large datasets.\nFoundation models contain general knowledge, that can be specialized for a certain\npurpose. The world of applied deep learning has moved from training a net from\nscratch for a certain problem, to taking a part of an existing net that is trained for a\nrelated problem and then ﬁnetuning it on the new task. Nearly all state-of-the-art\n9 Meta-Learning\n249\nFig. 9.1 Violin, viola, cello and double bass\nFig. 9.2 Imagenet Thumbnails [646]\nvisual perception approaches rely on the same approach: (1) pretrain a convolutional\nnetwork on a large, manually annotated image classiﬁcation dataset and (2) ﬁnetune\nthe network on a smaller, task-speciﬁc dataset [278, 200, 865, 8, 360], see Fig. 9.2 for\nsome thumbnails of Imagenet [646]. For natural language recognition, pretraining\nis also the norm—for example, for large-scale pretrained language models such as\nWord2vec [513, 514], BERT [193] and GPT-3 [617].\nIn this chapter, we will study pretraining, and more.\n250\n9 Meta-Learning\nName\nDataset\nTask\nSingle-task Learning 𝐷𝑡𝑟𝑎𝑖𝑛⊆𝐷, 𝐷𝑡𝑒𝑠𝑡⊆𝐷𝑇= 𝑇𝑡𝑟𝑎𝑖𝑛= 𝑇𝑡𝑒𝑠𝑡\nTransfer Learning\n𝐷1 ≫𝐷2\n𝑇1 ≠𝑇2\nMulti-task Learning 𝐷𝑡𝑟𝑎𝑖𝑛⊆𝐷, 𝐷𝑡𝑒𝑠𝑡⊆𝐷𝑇1 ≠𝑇2\nDomain Adaptation 𝐷1 ≠𝐷2\n𝑇1 = 𝑇2\nMeta-Learning\n{𝐷1, . . . , 𝐷𝑁−1} ≫𝐷𝑁𝑇1, . . . , 𝑇𝑛−1 ≠𝑇𝑁\nTable 9.1 Diﬀerent Kinds of Supervised Learning. A typical single learning task is to classify\npictures of animals into diﬀerent classes. The dataset is split into a train and a testset, and the\ntask (loss function) is the same at train and test time. A transfer learning task uses part of the\nknowledge (network parameters) that are learned on a large dataset to initialize a second network,\nthat is trained subsequently (ﬁne tuned) on a diﬀerent dataset. This second dataset/learning task is\nrelated to the ﬁrst task, so the learning the second task goes faster. For example, having learned to\nrecognize cars, may be useful to speedup recognizing trucks. In multi-task learning several related\ntasks are trained at the same time, possibly beneﬁtting from better regularization. An example\ncould be training spam ﬁlters for diﬀerent users at the same time. Domain adaptation tries to adapt\nthe network to a new dataset of related examples, such as images of pedestrians in diﬀerent light\nconditions. Meta-learning tries to learn meta knowledge, such as hyperparameters over a sequence\nof related (larger) learning tasks, so that a new learning task goes faster. In deep meta-learning\nthese hyperparameters include the initial network parameters, in this sense meta-learning can be\nconsidered to be multi-job transfer learning.\n9.1 Learning to Learn Related Problems\nTraining times for modern deep networks are large. Training AlexNet for ImageNet\ntook 5-6 days on 2 GPUs in 2012 [431], see Sect. B.3.1. In reinforcement learning,\ntraining AlphaGo took weeks [703, 706], in natural language processing, training\nalso takes a long time [193], even excessively long as in the case of GPT-3 [114].\nClearly, some solution is needed. Before we look closer at transfer learning, let us\nhave a look at the bigger picture: lifelong learning.\nWhen humans learn a new task, learning is based on previous experience. Initial\nlearning by infants of elementary skills in vision, speech, and locomotion takes\nyears. Subsequent learning of new skills builds on the previously acquired skills.\nExisting knowledge is adapted, and new skills are learned based on previous skills.\nLifelong learning remains a long-standing challenge for machine learning; in\ncurrent methods the continuous acquisition of information often leads to interfer-\nence of concepts or catastrophic forgetting [701]. This limitation represents a major\ndrawback for deep networks that typically learn representations from stationary\nbatches of training data. Although some advances have been made in narrow do-\nmains, signiﬁcant advances are necessary to approach generally-applicable lifelong\nlearning.\nDiﬀerent approaches have been developed. Among the methods are meta-\nlearning, domain adaptation, multi-task learning, and pretraining. Table 9.1 lists\nthese approaches, together with regular single task learning. The learning tasks\nare formulated using datasets, as in a regular supervised setting. The table shows\n9.2 Transfer Learning and Meta-Learning Agents\n251\nhow the lifelong learning methods diﬀer in their training and test dataset, and the\ndiﬀerent learning tasks.\nThe ﬁrst line shows regular single-task learning. For single-task learning, the\ntraining and test dataset are both drawn from the same distribution (the datasets do\nnot contain the same examples, but they are drawn from the same original dataset\nand the data distribution is expected to be the same), and the task to perform is the\nsame for training and test.\nIn the next line, for transfer learning, networks trained on one dataset are used\nto speedup training for a diﬀerent task, possibly using a much smaller dataset [581].\nSince the datasets are not drawn from the same master dataset, their distribution will\ndiﬀer, and there typically is only an informal notion of how “related” the datasets\nare. However, in practice transfer learning often provides signiﬁcant speedups, and\ntransfer learning, pretraining and ﬁnetuning are currently used in many real-world\ntraining tasks, sometimes using large foundation models as a basis.\nIn multi-task learning, more than one task is learned from one dataset [129].\nThe tasks are often related, such as classiﬁcation tasks of diﬀerent, but related,\nclasses of images, or learning spam ﬁlters for diﬀerent email-users. Regularization\nmay be improved when a neural network is trained on related tasks at the same\ntime [59, 151].\nSo far, our learning tasks were trying to speedup learning diﬀerent tasks with\nrelated data. Domain adaptation switches this around: the task remains the same,\nbut the data changes. In domain adaptation, a diﬀerent dataset is used to perform\nthe same task, such as recognizing pedestrians in diﬀerent light conditions [782].\nIn meta-learning, both datasets and tasks are diﬀerent, although not too diﬀerent.\nIn meta-learning, a sequence of datasets and learning tasks is generalized to learn a\nnew (related) task quickly [106, 351, 363, 667]. The goal of meta-learning is to learn\nhyperparameters over a sequence of learning tasks.\n9.2 Transfer Learning and Meta-Learning Agents\nWe will now introduce transfer learning and meta-learning algorithms. Where in\nnormal learning we would intialize our parameters randomly, in transfer learning\nwe initialize them with (part of) the training results of another training task. This\nother task is related in some way to the new task, for example, a task to recognize\nimages of dogs playing in a forest is initalized on a dataset of dogs playing in a\npark. The parameter transfer of the old task is called pretraining, the second phase,\nwhere the network learns the new task on the new dataset, is called ﬁnetuning. The\npretraining will hopefully allow the new task to train faster.\nWhere transfer learning transfers knowledge from a single previous task, meta-\nlearning aims to generalize knowledge from multiple previous learning tasks. Meta-\nlearning tries to learn hyperparameters over these related learning tasks, that tell\nthe algorithm how to learn the new task. Meta-learning thus aims to learn to learn.\nIn deep meta-learning approaches, the initial network parameters are typically part\n252\n9 Meta-Learning\nof the hyperparameters. Note that in transfer learning we also use (part of) the\nparameters to speed up learning (ﬁnetuning) a new, related task. We can say that\ndeep meta-learning generalizes transfer learning by learning the initial parameters\nover not one but a sequence of related tasks [351, 364]. (Deﬁnitions are still in ﬂux,\nhowever, and diﬀerent authors and diﬀerent ﬁelds have diﬀerent deﬁnitions.)\nTransfer learning has become part of the standard approach in machine learning,\nmeta-learning is still an area of active research. We will look into meta-learning\nshortly, after we have looked into transfer learning, multi-task learning, and domain\nadaptation.\n9.2.1 Transfer Learning\nTransfer learning aims to improve the process of learning new tasks using the\nexperience gained by solving similar problems [606, 774, 773, 585]. Transfer learning\naims to transfer past experience of source tasks and use it to boost learning in a\nrelated target task [581, 879].\nIn transfer learning, we ﬁrst train a base network on a base dataset and task, and\nthen we repurpose some of the learned features to a second target network to be\ntrained on a target dataset and task. This process works better if the features are\ngeneral, meaning suitable to both base and target tasks, instead of speciﬁc to the\nbase task. This form of transfer learning is called inductive transfer. The scope of\npossible models (model bias) is narrowed in a beneﬁcial way by using a model ﬁt\non a diﬀerent but related task.\nFirst we will look at task similarity, then at transfer learning, multi-task learning,\nand domain adaptation.\n9.2.1.1 Task Similarity\nClearly, pretraining works better when the tasks are similar [129]. Learning to play\nthe viola based on the violin is more similar than learning the tables of multiplica-\ntion based on tennis. Diﬀerent measures can be used to measure the similarity of\nexamples and features in datasets, from linear one-dimensional measures to non-\nlinear multi-dimensional measures. Common measures are the cosine similarity for\nreal-valued vectors and the radial basis function kernel [752, 809], but many more\nelaborate measures have been devised.\nSimilarity measures are also used to devise meta-learning algorithms, as we will\nsee later.\n9.2 Transfer Learning and Meta-Learning Agents\n253\n9.2.1.2 Pretraining and Finetuning\nWhen we want to transfer knowledge, we can transfer the weights of the network,\nand then start re-training with the new dataset. Please refer back to Table 9.1. In\npretraining the new dataset is smaller than the old dataset 𝐷1 ≫𝐷2, and we train\nfor a new task 𝑇1 ≠𝑇2, which we want to train faster. This works when the new\ntask is diﬀerent, but similar, so that the old dataset 𝐷1 contains useful information\nfor the new task 𝑇2.\nTo learn new image recognition problems, it is common to use a deep learning\nmodel pre-trained for a large and challenging image classiﬁcation task such as\nthe ImageNet 1000-class photograph classiﬁcation competition. Three examples\nof pretrained models include: the Oxford VGG Model, Google’s Inception Model,\nMicrosoft’s ResNet Model. For more examples, see the Caﬀe Model Zoo,1 or other\nzoos2 where more pre-trained models are shared.\nTransfer learning is eﬀective because the images were trained on a corpus that\nrequires the model to make predictions on a large number of classes, requiring the\nmodel to be general, and since it eﬃciently learns to extract features in order to\nperform well.\nConvolutional neural network features are more generic in lower layers, such\nas color blobs or Gabor ﬁlters, and more speciﬁc to the original dataset in higher\nlayers. Features must eventually transition from general to speciﬁc in the last layers\nof the network [862]. Pretraining copies some of the layers to the new task. Care\nshould be taken how much of the old task network to copy. It is relatively safe to\ncopy the more general lower layers. Copying the more speciﬁc higher layers may\nbe detrimental to performance.\nIn natural language processing a similar situation occurs. In natural language pro-\ncessing, a word embedding is used that is a mapping of words to a high-dimensional\ncontinuous vector where diﬀerent words with a similar meaning have a similar\nvector representation. Eﬃcient algorithms exist to learn these word representations.\nTwo examples of common pre-trained word models trained on very large datasets\nof text documents include Google’s Word2vec model [513] and Stanford’s GloVe\nmodel [596].\n9.2.1.3 Hands-on: Pretraining Example\nTransfer learning and pretraining have become a standard approach to learning\nnew tasks, especially when only a small dataset is present, or when we wish to\nlimit training time. Let us have a look at a hands-on example that is part of the\nKeras distribution (Sect. B.3.3). The Keras transfer learning example provides a\nbasic Imagenet-based approach, getting data from TensorFlow DataSets (TFDS).\nThe example follows a supervised learning approach, although the learning and\n1 https://caffe.berkeleyvision.org/model_zoo.html\n2 https://modelzoo.co\n254\n9 Meta-Learning\n1\nbase_model = keras. applications .Xception(\n2\nweights=’imagenet ’,\n# Load\nweights pre -trained\non\nImageNet.\n3\ninput_shape =(150 , 150, 3),\n4\ninclude_top=False)\n# Do not\ninclude\nthe\nImageNet\nclassifier\nat the top.\n5\n6\nbase_model.trainable = False\nListing 9.1 Pretraining in Keras (1): instantiate model\nﬁne-tuning phase can easily be substituted by a reinforcement learning setup. The\nKeras transfer learning example is at the Keras site,3 and can also be run in a Google\nColab.\nThe most common incarnation of transfer learning in the context of deep learning\nis the following worfklow:\n1. Take layers from a previously trained model.\n2. Freeze them, so as to avoid destroying any of the information they contain during\nfuture training rounds.\n3. Add some new, trainable layers on top of the frozen layers. They will train on\nthe new dataset using the old features as predictions.\n4. Train the new layers on your new (small) dataset.\n5. A last, optional, step, is ﬁne-tuning of the frozen layers, which consists of un-\nfreezing the entire model you obtained above, and re-training it on the new\ndata with a very low learning rate. This can potentially achieve meaningful\nimprovements, by incrementally adapting the pretrained features to the new\ndata.\nLet us look at how this workﬂow works in practice in Keras. At the Keras site we\nﬁnd an accessible example code for pretraining (see Listing 9.1).\nFirst, we instantiate a base model with pretrained weights. (We do not include\nthe classiﬁer on top.) Then, we freeze the base model (see Listing 9.2). Next, we\ncreate a new model on top, and train it.\nThe Keras example contains this example and others, including ﬁne-tuning.\nPlease go to the Keras site and improve your experience with pretraining in practice.\n9.2.1.4 Multi-task Learning\nMulti-task learning is related to transfer learning. In multi-task learning a single\nnetwork is trained at the same time on multiple related tasks [772, 129].\nIn multi-task learning the learning process of one task beneﬁts from the simulta-\nneous learning of the related task. This approach is eﬀective when the tasks have\nsome commonality, such as learning to recognize breeds of dogs and breeds of cats.\n3 https://keras.io/guides/transfer_learning/\n9.2 Transfer Learning and Meta-Learning Agents\n255\n1\ninputs = keras.Input(shape =(150 , 150, 3))\n2\n# The\nbase_model\nis\nrunning\nin\ninference\nmode here ,\n3\n# by\npassing\n‘training=False ‘. This is\nimportant\nfor fine -tuning\n4\nx = base_model(inputs , training=False)\n5\n# Convert\nfeatures\nof shape ‘base_model . output_shape [1:] ‘ to\nvectors\n6\nx = keras.layers. GlobalAveragePooling2D ()(x)\n7\n# A Dense\nclassifier\nwith a single\nunit (binary\nclassification )\n8\noutputs = keras.layers.Dense (1)(x)\n9\nmodel = keras.Model(inputs , outputs)\n10\n11\n12\nmodel.compile(optimizer=keras. optimizers .Adam (),\n13\nloss=keras.losses. BinaryCrossentropy ( from_logits =\nTrue),\n14\nmetrics =[ keras.metrics. BinaryAccuracy ()])\n15\nmodel.fit(new_dataset , epochs =20,\ncallbacks =... ,\nvalidation_data\n=...)\nListing 9.2 Pretraining in Keras (2): create new model and train\nIn multi-task learning, related learning tasks are learned at the same time, whereas\nin transfer learning they are learned in sequence by diﬀerent networks. Multi-task\nlearning improves regularization by requiring the algorithm to perform well on a\nrelated learning task instead of penalizing all overﬁtting uniformly [233, 26]. The\ntwo-headed AlphaGo Zero network optimizes for value and for policy at the same\ntime in the same network [581, 129]. A multi-headed architecture is often used in\nmulti-task learning, although in AlphaGo Zero the two heads are trained for two\nrelated aspects (policy and value) of the same task (playing Go games).\nMulti-task learning has been applied with success to Atari games [405, 404].\n9.2.1.5 Domain Adaptation\nDomain adaptation is necessary when there is a change in the data distribution\nbetween the training dataset and the test dataset (domain shift). This problem is\nrelated to out-of-distribution learning [470]. Domain shifts are common in practical\napplications of artiﬁcial intelligence, such as when items must be recognized in\ndiﬀerent light conditions, or when the background changes. Conventional machine-\nlearning algorithms often have diﬃculty adapting to such changes.\nThe goal of domain adaptation is to compensate for the variation among two data\ndistributions, to be able to reuse information from a source domain on a diﬀerent\ntarget domain [782], see Fig. 9.3 for a backpack in diﬀerent circumstances. As was\nindicated in Table 9.1, domain adaptation applies to the situation where the tasks are\nthe same 𝑇1 = 𝑇2 but the datasets are diﬀerent 𝐷1 ≠𝐷2, although still somewhat\nsimilar. For example, the task may be to recognize a backpack, but in a diﬀerent\norientation, or to recognize a pedestrian, but in diﬀerent lighting.\n256\n9 Meta-Learning\nFig. 9.3 Domain Adaptation: Recognizing Items in Diﬀerent Circumstances is Diﬃcult [301]\nDomain adaptation can be seen as the opposite of pretraining. Pretraining uses\nthe same dataset for a diﬀerent task, while domain adaptation adapts to a new\ndataset for the same task [127].\nIn natural language processing, examples of domain shift are an algorithm\nthat has been trained on news items that is then applied to a dataset of biomedical\ndocuments [179, 733], or a spam ﬁlter that is trained on a certain group of email users,\nwhich is deployed to a new target user [74]. Sudden changes in the environment\n(pandemics, severe weather) can also upset machine learning algorithms.\nThere are diﬀerent techniques to overcome domain shift [168, 872, 856, 834].\nIn visual applications, adaptation can be achieved by re-weighting the samples\nof the ﬁrst dataset, or clustering them for visually coherent sub-domains. Other\napproaches try to ﬁnd transformations that map the source distribution to the\ntarget, or learn a classiﬁcation model and a feature transformation jointly [782].\nAdversarial techniques where feature representations are encouraged to be diﬃcult\nto distinguish can be used to achieve adaptation [792, 850, 211], see also Sect. B.2.6.\n9.2.2 Meta-Learning\nRelated to transfer learning is meta-learning. Where the focus in transfer learning\nis on transferring parameters from a single donor task to a receiver task for further\nﬁnetuning, in meta-learning the focus is on using the knowledge of a number of\ntasks to learn how to learn a new task faster and better. Regular machine learning\n9.2 Transfer Learning and Meta-Learning Agents\n257\nlearns examples for one task, meta-learning aims to learn across tasks. Machine\nlearning learns parameters that approximate the function, meta-learning learns\nhyperparameters about the learning-function.4 This is often expressed as learning\nto learn, a phrase that has deﬁned the ﬁeld ever since its introduction [669, 772,\n774]. The term meta-learning has been used for many diﬀerent contexts, not only\nfor deep learning, but also for tasks ranging from hyperparameter optimization,\nalgorithm selection, to automated machine learning. (We will brieﬂy look at these\nin Sect. 9.2.2.5.)\nDeep meta reinforcement learning is an active area of research. Many algorithms\nare being developed, and much progress is being made. Table 9.2 lists nine algorithms\nthat have been proposed for deep meta reinforcement learning (see the surveys [351,\n364]).\n9.2.2.1 Evaluating Few-Shot Learning Problems\nOne of the challenges of lifelong machine learning is to judge the performance\nof an algorithm. Regular train-test generalization does not capture the speed of\nadaptation of a meta-learning algorithm.\nFor this reason, meta-learning tasks are typically evaluated on their few-shot\nlearning ability. In few-shot learning, we test if a learning algorithm can be made\nto recognize examples from classes from which it has seen only few examples in\ntraining. In few-shot learning prior knowledge is available in the network.\nTo translate few-shot learning to a human setting, we can think of a situation\nwhere a human plays the double bass after only a few minutes of training on the\ndouble bass, but after years on the violin, viola, or cello.\nMeta-learning algorithms are often evaluated with few shot learning tasks, in\nwhich the algorithm must recognize items of which it has only seen a few exam-\nples. This is formalized in the 𝑁-way-𝑘-shot approach [141, 445, 829]. Figure 9.4\nillustrates this process. Given a large dataset D, a smaller training dataset 𝐷is\nsampled from this dataset. The 𝑁-way-𝑘-shot classiﬁcation problem constructs\ntraining dataset 𝐷such that it consists of 𝑁classes, of which, for each class, 𝑘\nexamples are present in the dataset. Thus, the cardinality of |𝐷| = 𝑁· 𝑘.\nA full 𝑁-way-𝑘-shot few-shot learning meta task T consists of many episodes\nin which base tasks T𝑖are performed. A base task consists of a training set and a\ntest set, to test generalization. In few-shot terminology, the training set is called\nthe support set, and the test set is called the query set. The support set has size\n𝑁· 𝑘, the query set consists of a small number of examples. The meta-learning\nalgorithm can learn from the episodes of 𝑁-way-𝑘-shot query/support base tasks,\nuntil at meta-test time the generalization of the meta-learning algorithm is tested\nwith another query, as is illustrated in the ﬁgure.\n4 Associating base learning with parameter learning, and meta-learning wth hyperparameter\nlearning appears to give us a clear distinction; however, in practice the distinction is not so clear\ncut: in deep meta-learning the initialization of the regular parameters is considered to be an\nimportant “hyper”parameter.\n258\n9 Meta-Learning\nFig. 9.4 𝑁-way-𝑘-shot learning [98]\nName\nApproach\nEnvironment Ref\nRecurr. ML\nDeploy recurrent networks on RL problems\n-\n[213, 826]\nMeta Netw.\nFast reparam. of base-learner by distinct meta-learner O-glot, miniIm. [542]\nSNAIL\nAttention mechanism coupled with temporal conv.\nO-glot, miniIm. [517]\nLSTM ML\nEmbed base-learner parameters in cell state of LSTM miniImageNet\n[629]\nMAML\nLearn initialization weights 𝜃for fast adaptation\nO-glot, miniIm. [243]\niMAML\nApprox. higher-order gradients, indep. of optim. path O-glot, miniIm. [623]\nMeta SGD\nLearn both the initialization and updates\nO-glot, miniIm. [476]\nReptile\nMove init. towards task-speciﬁc updated weights\nO-glot, miniIm. [561]\nBayesMAML Learn multiple initializations Θ, jointly optim. SVGD miniImagaNet [861]\nTable 9.2 Meta Reinforcement Learning Approaches [364]\n9.2.2.2 Deep Meta-Learning Algorithms\nWe will now turn our attention to deep meta-learning algorithms. The meta-learning\nﬁeld is still young and active, nevertheless, the ﬁeld is converging on a set of\ndeﬁnitions that we will present here. We start our explanation in a supervised\nsetting.\nMeta-learning is concerned with learning a task T from a set of base-learning\ntasks {T1, T2, T3, . . . } so that a new (related) meta-test-task will reach a high ac-\ncuracy quicker. Each base-learning task T𝑖consists of a dataset 𝐷𝑖and a learning\nobjective, the loss function L𝑖. Thus we get T𝑖= (𝐷𝑖, L𝑖). Each dataset consists\nof pairs of inputs and labels 𝐷𝑖= {(𝑥𝑗, 𝑦𝑗)}, and is split into a training and a\ntestset 𝐷𝑖= {𝐷T𝑖,𝑡𝑟𝑎𝑖𝑛, 𝐷T𝑖,𝑡𝑒𝑠𝑡}. On each training dataset a parameterized model\nˆ𝑓𝜃𝑖(𝐷𝑖,𝑡𝑟𝑎𝑖𝑛) is approximated, with a loss function L𝑖(𝜃𝑖, 𝐷𝑖,𝑡𝑟𝑎𝑖𝑛). The model ˆ𝑓\nis approximated with a deep learning algorithm, that is governed by a set of hyper-\nparameters 𝜔. The particular hyperparameters vary from algorithm to algorithm,\n9.2 Transfer Learning and Meta-Learning Agents\n259\nbut frequently encountered hyperparameters are the learning rate 𝛼, the initial\nparameters 𝜃0, and algorithm constants.\nThis conventional machine learning algorithm is called the base learner. Each\nbase learner task approximates a model ˆ𝑓𝑖by ﬁnding the optimal parameters 𝜃★\n𝑖to\nminimize the loss function on its data set\nT𝑖= ˆ𝑓𝜃★\n𝑖= arg min\n𝜃𝑖\nL𝑖,𝜔(𝜃𝑖, 𝐷𝑖,𝑡𝑟𝑎𝑖𝑛)\nwhile the learning algorithm is governed by hyperparameters 𝜔.\nInner and Outer Loop Optimization\nOne of the most popular deep meta-learning approaches of the last few years\nis optimization-based meta-learning [364]. This approach optimizes the initial\nparameters 𝜃of the network for fast learning of new tasks. Most optimization-based\ntechniques do so by approaching meta-learning as a two-level optimization problem.\nAt the inner level, a base learner makes task-speciﬁc updates to 𝜃for the diﬀerent\nobservations in the training set. At the outer level, the meta-learner optimizes\nhyperparameters 𝜔across a sequence of base tasks where the loss of each task is\nevaluated using the test data from the base tasks 𝐷T𝑖,𝑡𝑒𝑠𝑡[629, 351, 463].\nThe inner loop optimizes the parameters 𝜃, and the outer loop optimizes the\nhyperparameters 𝜔to ﬁnd the best performance on the set of base tasks 𝑖= 0, . . . , 𝑀\nwith the appropriate test data:\n𝜔★= arg min\n𝜔\nLmeta\n|             {z             }\nouter loop\n(arg min\n𝜃𝑖\nLbase\n𝜔\n(𝜃𝑖, 𝐷𝑖,𝑡𝑟𝑎𝑖𝑛)\n|                               {z                               }\ninner loop\n, 𝐷𝑖,𝑡𝑒𝑠𝑡).\nThe inner loop optimizes 𝜃𝑖within the datasets 𝐷𝑖of the tasks T𝑖, and the outer\nloop optimizes 𝜔across the tasks and datasets.\nThe meta loss function optimizes for the meta objective, which can be accuracy,\nspeed, or another goal over the set of base tasks (and datasets). The outcome of the\nmeta optimization is a set of optimal hyperparameters 𝜔★.\nIn optimization-based meta-learning the most important hyperparameters 𝜔are\nthe optimal initial parameters 𝜃★\n0. When the meta-learner only optimizes the initial\nparameters as hyperparameters (𝜔= 𝜃0) then the inner/outer formula simpliﬁes as\nfollows:\n𝜃★\n0 = arg min\n𝜃0\nL\n|      {z      }\nouter loop\n(arg min\n𝜃𝑖\nL(𝜃𝑖, 𝐷𝑖,𝑡𝑟𝑎𝑖𝑛)\n|                         {z                         }\ninner loop\n, 𝐷𝑖,𝑡𝑒𝑠𝑡).\nIn this approach we meta-optimize the initial parameters 𝜃0, such that the loss\nfunction performs well on the test data of the base tasks. Section 9.2.2.4 describes\nMAML, a well-known example of this approach.\n260\n9 Meta-Learning\nFig. 9.5 Workﬂow of recurrent meta-learners in reinforcement learning contexts. State, action,\nreward, and termination ﬂag at time step 𝑡are denotednby 𝑠𝑡, 𝑎𝑡, 𝑟𝑡, and 𝑑𝑡, ℎ𝑡refers to the\nhidden state [213].\nDeep meta-learning approaches are sometimes categorized as (1) similarity-\nmetric-based, (2) model-based, and (3) optimization-based [364]. We will now have\na closer look at two of the nine meta reinforcement learning algorithms from\nTable 9.2. We will look at Recurrent meta-learning and MAML; the former is a\nmodel-based approach, the latter optimization-based.\n9.2.2.3 Recurrent Meta-Learning\nFor meta reinforcement learning approaches to be able to learn to learn, they must be\nable to remember what they have learned across subtasks. Let us see how Recurrent\nmeta-learning learns across tasks.\nRecurrent meta-learning uses recurrent neural networks to remember this knowl-\nedge [213, 826]. The recurrent network serves as dynamic storage for the learned\ntask embedding (weight vector). The recurrence can be implemented by an LSTM\n[826] or by gated recurrent units [213]. The choice of recurrent neural meta-network\n(meta-RNN) determines how well it adapts to the subtasks, as it gradually accumu-\nlates knowledge about the base-task structure.\nRecurrent meta-learning tracks variables 𝑠, 𝑎, 𝑟, 𝑑which denote state, action,\nreward, and termination of the episode. For each task T𝑖, Recurrent meta-learning\ninputs the set of environment variables {𝑠𝑡+1, 𝑎𝑡, 𝑟𝑡, 𝑑𝑡} into a meta-RNN at each\ntime step 𝑡. The meta-RNN outputs an action and a hidden state ℎ𝑡. Conditioned on\nthe hidden state ℎ𝑡, the meta network outputs action 𝑎𝑡. The goal is to maximize\nthe expected reward in each trial (Fig. 9.5). Since Recurrent meta-learning embeds\ninformation from previously seen inputs in hidden state, it is regarded as a model-\nbased meta-learner [364].\nRecurrent meta-learners performed almost as well as model-free baselines on\nsimple 𝑁-way-𝑘-shot reinforcement learning tasks [826, 213]. However, the per-\nformance degrades in more complex problems, when dependencies span a longer\nhorizon.\n9.2 Transfer Learning and Meta-Learning Agents\n261\nFig. 9.6 The Optimization approach aims to learn parameters from which other tasks can be\nlearned quickly. The intuition behind Optimization approaches such as MAML is that when our\nmeta-training set consists of tasks A, B, C, and D, then if our meta-learning algorithm adjusts\nparameters 𝑎and 𝑏to (2, 2), then they are close to either of the four tasks, and can be adjustly\nquickly to them, with few examples (after [364, 243]).\n9.2.2.4 Model-Agnostic Meta-Learning\nThe Model-agnostic meta-learning approach (MAML) [243] is an optimization\napproach that is model-agnostic: it can be used for diﬀerent learning problems, such\nas classiﬁcation, regression, and reinforcement learning.\nAs mentioned, the optimization view of meta-learning is especially focused on\noptimizing the initial parameters 𝜃. The intuition behind the optimization view can\nbe illustrated with a simple regression example (Fig. 9.6). Let us assume that we are\nfaced with multiple linear regression problems 𝑓𝑖(𝑥). The model has two parameters:\n𝑎and 𝑏, ˆ𝑓(𝑥) = 𝑎· 𝑥+ 𝑏. When the meta-training set consists of four tasks, A, B, C,\nand D, then we wish to optimize to a single set of parameters {𝑎, 𝑏} from which\nwe can quickly learn the optimal parameters for each of the four tasks. In Fig. 9.6\nthe point in the middle represents this combination of parameters. The point is\nthe closest to the four diﬀerent tasks. This is how Model-agnostic meta-learning\nworks [243]: by exposing our model to various base tasks, we update the parameters\n𝜃= {𝑎, 𝑏} to good initial parameters 𝜃0 that facilitate quick meta-adaptation.\nLet us look at the process of training a deep learning model’s parameters from a\nfeature learning standpoint [243, 364], where the goal is that a few gradient steps\ncan produce good results on a new task. We build a feature representation that is\nbroadly suitable for many tasks, and by then ﬁne-tuning the parameters slightly\n(primarily updating the top layer weights) we achieve good results—not unlike\ntransfer learning. MAML ﬁnds parameters 𝜃that are easy and fast to ﬁnetune,\nallowing the adaptation to happen in an embedding space that is well suited for\n262\n9 Meta-Learning\nAlgorithm 9.1 MAML for Reinforcement Learning [243]\nRequire: 𝑝(T): distribution over tasks\nRequire: 𝛼, 𝛽: step size hyperparameters\nrandomly initialize 𝜃\nwhile not done do\nSample batch of tasks T𝑖∼𝑝(T)\nfor all T𝑖do\nSample 𝑘trajectories D = {(𝑠1, 𝑎1, ...𝑠𝑇) } using 𝑓𝜃in T𝑖\nEvaluate ∇𝜃LT𝑖( 𝜋𝜃) using D and LT𝑖in Equation 9.1\nCompute adapted parameters with gradient descent: 𝜃′\n𝑖= 𝜃−𝛼∇𝜃LT𝑖( 𝜋𝜃)\nSample trajectories D′\n𝑖= {(𝑠1, 𝑎1, ...𝑠𝑇) } using 𝑓𝜃′\n𝑖in T𝑖\nend for\nUpdate 𝜃←𝜃−𝛽∇𝜃\nÍ\nT𝑖∼𝑝(T) LT𝑖( 𝜋𝜃′\n𝑖) using each D′\n𝑖and LT𝑖in Eq. 9.1\nend while\nfast learning. To put it another way, MAML’s goal is to ﬁnd the point in the middle\nof Fig. 9.6, from where the other tasks are easily reachable.\nHow does MAML work [363, 620, 243]? Please refer to the pseudocode for\nMAML in Alg. 9.1. The learning task is an episodic Markov decision process with\nhorizon 𝑇, where the learner is allowed to query a limited number of sample\ntrajectories for few-shot learning. Each reinforcement learning task T𝑖contains\nan initial state distribution 𝑝𝑖(𝑠1) and a transition distribution 𝑝𝑖(𝑠𝑡+1|𝑠𝑡, 𝑎𝑡). The\nloss LT𝑖corresponds to the (negative) reward function 𝑅. The model being learned,\n𝜋𝜃, is a policy from states 𝑠𝑡to a distribution over actions 𝑎𝑡at each timestep\n𝑡∈{1, ...,𝑇}. The loss for task T𝑖and policy 𝜋𝜃takes the familiar form of the\nobjective (Eq. 2.5):\nLT𝑖(𝜋𝜃) = −E𝑠𝑡,𝑎𝑡∼𝜋𝜃,𝑝T𝑖\n\" 𝑇\n∑︁\n𝑡=1\n𝑅𝑖(𝑠𝑡, 𝑎𝑡, 𝑠𝑡+1)\n#\n.\n(9.1)\nIn 𝑘-shot reinforcement learning, 𝑘rollouts from 𝜋𝜃and task T𝑖, (𝑠1, 𝑎1, ...𝑠𝑇), and\nthe rewards 𝑅(𝑠𝑡, 𝑎𝑡), may be used for adaptation on a new task T𝑖. MAML uses\nTRPO to estimate the gradient both for the policy gradient update(s) and the meta\noptimization [681].\nThe goal is to quickly learn new concepts, which is equivalent to achieving a\nminimal loss in few gradient update steps. The number of gradient steps has to be\nspeciﬁed in advance. For a single gradient update step gradient descent produces\nupdated parameters\n𝜃′\n𝑖= 𝜃−𝛼∇𝜃LT𝑖(𝜋𝜃)\nspeciﬁc to task 𝑖. The meta loss of one gradient step across tasks is\n𝜃←𝜃−𝛽∇𝜃\n∑︁\nT𝑖∼𝑝(T)\nLT𝑖(𝜋𝜃′\n𝑖)\n(9.2)\n9.2 Transfer Learning and Meta-Learning Agents\n263\nwhere 𝑝(T) is a probability distribution over tasks. This expression contains an\ninner gradient ∇𝜽LT𝑖(𝜋𝜃′\n𝑖). Optimizing this meta loss requires computing second-\norder gradients when backpropagating the meta gradient through the gradient\noperator in the meta objective (Eq. 9.2), which is computationally expensive [243].\nVarious algorithms have been inspired by MAML and aim to improve optimization-\nbased meta-learning further [561, 363].\nIn meta reinforcement learning the goal is to quickly ﬁnd a policy for a new\nenvironment using only a small amount of experience. MAML has gained atten-\ntion within the ﬁeld of deep meta-learning, due to its simplicity (it requires two\nhyperparameters), its general applicability, and its strong performance.\n9.2.2.5 Hyperparameter Optimization\nMeta-learning has been around for a long time, long before deep learning became\npopular. It has been applied to classic machine learning tasks, such as regression,\ndecision trees, support vector machines, clustering algorithms, Bayesian networks,\nevolutionary algorithms, and local search [93, 106, 812]. The hyperparameter view\non meta-learning originated here.\nAlthough this is a book about deep learning, it is interesting to brieﬂy discuss\nthis non-deep background, also because hyperparameter optimization is an impor-\ntant technology to ﬁnd a good set of hyperparameters in reinforcement learning\nexperiments.\nMachine learning algorithms have hyperparameters that govern their behavior,\nand ﬁnding the optimal setting for these hyperparameters has long been called\nmeta-learning. A naive approach is to enumerate all combinations and run the\nmachine learning problem for them. For all but the smallest hyperparameter spaces\nsuch a grid search will be prohibitively slow. Among the smarter meta-optimization\napproaches are random search, Bayesian optimization, gradient-based optimization,\nand evolutionary optimization.\nThis meta-algorithm approach has given rise to algorithm conﬁguration re-\nsearch, such as SMAC [366],5 ParamILS [367],6 irace [488],7 and algorithm selection\nresearch [630, 407], such as SATzilla [854].8 Well-known hyperparameter optimiza-\ntion packages include scikit-learn [595],9 scikit-optimize,10 nevergrad [627],11 and\noptuna [11].12 Hyperparameter optimization and algorithm conﬁguration have\n5 https://github.com/automl/SMAC3\n6 http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/\n7 http://iridia.ulb.ac.be/irace/\n8 http://www.cs.ubc.ca/labs/beta/Projects/SATzilla/\n9 https://scikit-learn.org/stable/\n10 https://scikit-optimize.github.io/stable/index.html\n11 https://code.fb.com/ai-research/nevergrad/\n12 https://optuna.org\n264\n9 Meta-Learning\ngrown into the ﬁeld of automated machine learning, or AutoML.13 The AutoML\nﬁeld is a large and active ﬁeld of meta-learning, an overview book is [368].\nAll machine learning algorithms have a bias, diﬀerent algorithms perform better\non diﬀerent types of problems. Hyperparameters constrain this algorithm bias. This\nso-called inductive bias reﬂects the set of assumptions about the data on which the\nalgorithms are based. Learning algorithms perform better when this bias matches\nthe learning problem (for example: CNNs work well on image problems, RNNs on\nlanguage problems). Meta-learning changes this inductive bias, either by choosing\na diﬀerent learning algorithm, by changing the network initialization, or by other\nmeans, allowing the algorithm to be adjusted to work well on diﬀerent problems.\n9.2.2.6 Meta-Learning and Curriculum Learning\nThere is in interesting connection between meta-learning and curriculum learning.\nBoth approaches aim to improve the speed and accuracy of learning, by learning\nfrom a set of subtasks.\nIn meta-learning, knowledge is gained from subtasks, so that the learning of a\nnew, related, task, can be quick. In curriculum learning (Sect. 6.2.3.1) we aim to\nlearn quicker by dividing a large and diﬃcult learning task into a set of subtasks,\nordered from easy to hard.\nThus we can conclude that curriculum learning is a form of meta-learning where\nthe subtasks are ordered from easy to hard, or, equivalently, that meta-learning is\nunordered curriculum learning.\n9.2.2.7 From Few-Shot to Zero-Shot Learning\nMeta-learning uses information from previous learning tasks to learn new tasks\nquicker [774]. Meta-learning algorithms are often evaluated in a few-shot setting,\nto see how well they do in image classiﬁcation problems when they are shown only\nfew training examples. This few-shot learning problem aims to correctly classify\nqueries with little previous support for the new class. In the previous sections we\nhave discussed how meta-learning algorithms aim to achieve few-shot learning. A\ndiscussion of meta-learning would not be complete without mentioning zero-shot\nlearning.\nZero-shot learning (ZSL) goes a step further than few-shot learning. In zero-shot\nlearning an example has to be recognized as belonging to a class without ever\nhaving been trained on an example of this class [454, 580]. In zero-shot learning the\nclasses covered by training instances and the classes we aim to classify are disjoint.\nThis may sound like an impossibility—how can you recognize something you have\nnever seen before?—yet it is something that we, humans, do all the time: having\nlearned to pour coﬀee in a cup, we can also pour tea, even if we never have seen\n13 https://www.automl.org\n9.3 Meta-Learning Environments\n265\ntea before. (Or, having learned to play the violin, the viola, and the cello, we can\nplay the double bass, to some degree, even if we have never played the double bass\nbefore.)\nWhenever we recognize something that we have not seen before, we are actually\nusing extra information (or features). If we recognize a red beak in a picture of a\nbird-species that we have never seen before, then the concepts “red” and “beak” are\nknown to us, because we have learned them in other contexts.\nZero-shot learning recognizes new categories of instances without training\nexamples. Attribute-based zero-shot learning uses separate high-level attribute\ndescriptions of the new categories, based on categories previously learned in the\ndataset. Attributes are an intermediate representation that enables parameter shar-\ning between classes [10]. The extra information can be in the form of textual\ndescription of the class—red, or beak—in addition to the visual information [449].\nThe learner must be able to match text with image information.\nZero-shot learning approaches are designed to learn this intermediate seman-\ntic layer, the attributes, and apply them at inference time to predict new classes,\nwhen they are provided with descriptions in terms of these attributes. Attributes\ncorrespond to high-level properties of the objects which are shared across multiple\nclasses (which can be detected by machines and which can be understood by hu-\nmans). Attribute-based image classiﬁcation is a label-embedding problem where\neach class is embedded in the space of attribute vectors. As an example, if the classes\ncorrespond to animals, possible attributes include has paws, has stripes or is black.\n9.3 Meta-Learning Environments\nNow that we have seen how transfer learning and meta-learning can be imple-\nmented, it is time to look at some of the environments that are used to evaluate the\nalgorithms. We will list important datasets, environments, and foundation models\nfor images, behavior, and text, expanding our scope beyond pure reinforcement\nlearning. We will look at how well the approaches succeed in generalizing quickly\nto new machine learning tasks.\nMany benchmarks have been introduced to test transfer and meta-learning\nalgorithms. Benchmarks for conventional machine learning algorithms aim to oﬀer\na variety of challenging learning tasks. Benchmarks for meta-learning, in contrast,\naim to oﬀer related learning tasks. Some benchmarks are parameterized, where the\ndiﬀerence between tasks can be controlled.\nMeta-learning aims to learn new and related tasks quicker, trading oﬀspeed\nversus accuracy. This raises the question how fast and how accurate meta-learning\nalgorithms are under diﬀerent circumstances. In answering these questions we\nmust keep in mind that the closer the learning tasks are, the easier the task is, and\nthe quicker and the more accurate results will be. Hence, we should carefully look\nat which dataset a benchmark uses when we compare results.\n266\n9 Meta-Learning\nName\nType\nDomain Ref\nALE\nsingle\ngames\n[71]\nMuJoCo\nsingle\nrobot\n[780]\nDeepMind Control single\nrobot\n[755]\nBERT\ntransfer\ntext\n[193]\nGPT-3\ntransfer\ntext\n[617]\nImageNet\ntransfer\nimage\n[237]\nOmniglot\nmeta\nimage\n[445]\nMini-ImageNet\nmeta\nimage\n[814]\nMeta-Dataset\nmeta\nimage\n[785]\nMeta-World\nmeta\nrobot\n[864]\nAlchemy\nmeta\nunity\n[825]\nTable 9.3 Datasets, Environments, and Models for Meta-Learning in Images, Behavior, and Text\nTable 9.3 lists some of the environments that are often used for meta-learning\nexperiments. Some are regular deep learning environments designed for single-task\nlearning (“single”), some are transfer learning and pretraining datasets (“transfer”),\nand some datasets and environments are speciﬁcally designed for meta-learning\nexperiments (“meta”).\nWe will now describe them in more detail. ALE (Sect. 3.1.1), MuJoCo (Sect. 4.1.3)\nand the DeepMind control suite (Sect. 4.3.2) are originally single task deep learning\nenvironments. They are also being used in meta-learning experiments and few-shot\nlearning, often with moderate results, since the tasks are typically not very similar\n(Pong is not like Pac-Man).\n9.3.1 Image Processing\nTraditionally, two datasets have emerged as de facto benchmarks for few-shot image\nlearning: Omniglot [446], and mini-ImageNet [646, 814].\nOmniglot is a dataset for one-shot learning. This dataset contains 1623 diﬀerent\nhandwritten characters from 50 diﬀerent alphabets and contains 20 examples per\nclass (character) [446, 447]. Most recent methods obtain very high accuracy on\nOmniglot, rendering comparisons between them mostly uninformative.\nMini-ImageNet uses the same setup as Omniglot for testing, consisting of 60,000\ncolour images of size 84 × 84 with 100 classes (64/16/20 for train/validation/test)\nand contains 600 examples per class [814]. Albeit harder than Omniglot, most\nrecent methods achieve similar accuracy when controlling for model capacity.\nMeta-learning algorithms such as Bayesian Program Learning and MAML achieved\naccuracies comparable to human performance on Omniglot and ImageNet, with\naccuracies in the high nineties and error rates as low as a few percent [447]. Models\n9.3 Meta-Learning Environments\n267\ntrained on the largest datasets, such as ImageNet, are used as foundation models [96].\nPre-trained models can be downloaded from the Model zoo.14\nThese benchmarks may be too homogeneous for testing meta-learning. In con-\ntrast, real-life learning experiences are heterogeneous: they vary in terms of the\nnumber of classes and examples per class, and are unbalanced. Furthermore, the\nOmniglot and Mini-ImageNet benchmarks measure within-dataset generalization.\nFor meta-learning, we are eventually after models that can generalize to entirely\nnew distributions. For this reason, new datasets are being developed speciﬁcally for\nmeta-learning.\n9.3.2 Natural Language Processing\nIn natural language processing BERT is a well-known pretrained model. BERT\nstands for Bidirectional encoder representations from transformers [193]. It is\ndesigned to pretrain deep bidirectional representations from unlabeled text by\njointly conditioning on two contexts. BERT has shown that transfer learning can\nwork well in natural language tasks. BERT can be used for classiﬁcation tasks such\nas sentiment analysis, question answering tasks, and named entity recognition.\nBERT is a large model, with 345 million parameters [639].\nAn even larger pretrained transformer model is the Generative pretrained trans-\nformer 3, or GPT-3 [617], with 175 billion parameters. The quality of the text\ngenerated by GPT-3 is exceptionally good, it is diﬃcult to distinguish from that\nwritten by humans. In this case, it appears that size matters. OpenAI provides a\npublic interface where you can see for yourself how well it performs.15\nBERT and GPT-3 are large models, that are used more and more as a foundation\nmodel as a basis for other experiments to pretrain on.\n9.3.3 Meta-Dataset\nA recent dataset speciﬁcally designed for meta-learning is Meta-dataset [785].16\nMeta-dataset is a set of datasets, consisting of: Imagenet, Omniglot, Aircraft, Birds,\nTextures, Quick Draw, Fungi, Flower, Traﬃc Signs, and MSCOCO. Thus, the datasets\nprovide a more heterogeneous challenge than earlier single-dataset experiments.\nTriantaﬁllou et al. report results with Matching networks [814], Prototypical\nnetworks [717], ﬁrst-order MAML [243], and Relation networks [737]. As can be\nexpected, the accuracy on the larger (meta) dataset is much lower than on previous\nhomogeneous datasets. They ﬁnd that a variant of MAML performs best, although\nfor classiﬁers that are trained on other datasets most accuracies are between 40%\n14 https://modelzoo.co\n15 https://openai.com/blog/openai-api/\n16 https://github.com/google-research/meta-dataset\n268\n9 Meta-Learning\nFig. 9.7 Multi Task and Meta Reinforcement Learning [864]\nFig. 9.8 Meta-World Tasks [864]\nand 60%, except for Birds and Flowers, which scores in the 70s and 80s, closer to the\nsingle dataset results for Omniglot and Imagenet. Meta-learning for heterogeneous\ndatasets remains a challenging task (see also [141, 775]).\n9.3.4 Meta-World\nFor deep reinforcement learning two traditionally popular environments are ALE\nand MuJoCo. The games in the ALE benchmarks typically diﬀer considerably, which\nmakes the ALE test set challenging for meta-learning, and little success has been\nreported. (There are a few exceptions that apply transfer learning (pretraining) to\nDQN [586, 520, 718] succeeding in multitask learning in a set of Atari games that\nall move a ball.)\nRobotics tasks, on the other hand, are more easily parameterizable. Test tasks can\nbe generated with the desired level of similarity, making robotic tasks amenable to\nmeta-learning testing. Typical tasks such as reaching and learning diﬀerent walking\n9.3 Meta-Learning Environments\n269\ngaits, are more related than two Atari games such as, for example, Breakout and\nSpace Invaders.\nTo provide a better benchmark that is more challenging for meta reinforcement\nlearning, Yu et al. introduced Meta-World [864],17 a benchmark for multi-task\nand meta reinforcement learning (see Fig. 9.7 for their pictorial explanation of\nthe diﬀerence between multi-task and meta-learning). Meta-World consists of 50\ndistinct manipulation tasks with a robotic arm (Fig. 9.8). The tasks are designed\nto be diﬀerent, and contain structure, which can be leveraged for transfer to new\ntasks.\nWhen the authors of Meta-World evaluated six state-of-the-art meta-reinforce-\nment and multi-task learning algorithms on these tasks, they found that general-\nization of existing algorithms to heterogeneous tasks is limited. They tried PPO,\nTRPO, SAC, RL2[213], MAML, and PEARL [624]. Small variations of tasks such\nas diﬀerent object positions can be learned with reasonable success, but the al-\ngorithms struggled to learn multiple tasks at the same time, even with as few as\nten distinct training tasks. In contrast to more limited meta-learning benchmarks,\nMeta-World emphasizes generalization to new tasks and interaction scenarios, not\njust a parametric variation in goals.\n9.3.5 Alchemy\nA ﬁnal meta reinforcement learning benchmark that we will discuss is Alchemy [825].\nThe Alchemy benchmark is a procedurally generated 3D video game [692], im-\nplemented in Unity [383]. Task generation is parameterized, and varying degrees\nof similarity and hidden structure can be chosen. The process by which Alchemy\nlevels are created is accessible to researchers, and a perfect Bayesian ideal observer\ncan be implemented.\nExperiments with two agents are reported, VMPO and IMPALA. The VMPO [722,\n587] agent is based on a gated transformer network. The IMPALA [230, 374] agent\nis based on population-based training with an LSTM core network. Both agents are\nstrong deep learning methods, although not necessarily for meta-learning. Again,\nin both agents meta-learning became more diﬃcult as learning tasks became more\ndiverse. The reported performance on meta-learning was weak.\nThe Alchemy platform can be found on GitHub.18 A human-playable interface\nis part of the environment.\n17 https://meta-world.github.io\n18 https://github.com/deepmind/dm_alchemy\n270\n9 Meta-Learning\n1\nimport\nmetaworld\n2\nimport\nrandom\n3\n4\nprint(metaworld.ML1.ENV_NAMES)\n# Try\navailable\nenvironments\n5\n6\nml1 = metaworld.ML1(’pick -place -v1’) # Construct\nthe\nbenchmark\n7\n8\nenv = ml1. train_classes [’pick -place -v1’]()\n9\ntask = random.choice(ml1. train_tasks )\n10\nenv.set_task(task)\n# Set\ntask\n11\n12\nobs = env.reset ()\n# Reset\nenvironment\n13\na = env. action_space .sample ()\n# Sample an action\n14\nobs , reward , done , info = env.step(a)\n# Step\nthe\nenvironoment\nListing 9.3 Using Meta-World\n9.3.6 Hands-on: Meta-World Example\nLet us get experience with running Meta-World benchmark environments. We will\nrun a few popular agent algorithms on them, such as PPO and TRPO. Meta-World is\nas easy to use as Gym. The code can be found on GitHub,19 and the accompanying\nimplementations of a suite of agent algorithms is called Garage [261].20 Garage\nruns on PyTorch and on TensorFlow.\nStandard Meta-World benchmarks include multi-task and meta-learning setups,\nnamed MT1, MT10, MT50, and ML1, ML10, ML50. The Meta-World benchmark can\nbe installed with pip\npip install git+https://github.com/rlworkgroup/\nmetaworld.git@master\\#egg=metaworld\nNote that Meta-World is a robotics benchmark and needs MuJoCo, so you have\nto install that too.21 The GitHub site contains brief example instructions on the\nusage of the benchmark, please refer to Listing 9.3. The benchmark can be used to\ntest the meta-learning performance of your favorite algorithm, or you can use one\nof the baselines provided in Garage.\nConclusion\nIn this chapter we have seen diﬀerent approaches to learning new and diﬀerent tasks\nwith few or even zero examples. Impressive results have been achieved, although\n19 https://github.com/rlworkgroup/metaworld\n20 https://github.com/rlworkgroup/garage\n21 https://github.com/openai/mujoco-py#install-mujoco\n9.3 Meta-Learning Environments\n271\nmajor challenges remain to learn general adaptation when tasks are more diverse.\nAs is often the case in new ﬁelds, many diﬀerent approaches have been tried. The\nﬁeld of meta-learning is an active ﬁeld of research, aiming to reduce one of the\nmain problems of machine learning, and many new methods will continue to be\ndeveloped.\nSummary and Further Reading\nWe will now summarize the chapter and provide pointers to further reading.\nSummary\nThis chapter is concerned with learning new tasks faster and with smaller datasets\nor lower sample complexity. Transfer learning is concerned with transferring knowl-\nedge that has been learned to solve a taks, to another task, to allow quicker learning.\nA popular transfer learning approach is pretraining, where some network layers\nare copied to intialize a network for a new task, followed by ﬁne tuning, to improve\nperformance on the new task, but with a smaller dataset.\nAnother approach is meta-learning, or learning to learn. Here knowledge of how\na sequence of previous tasks is learned is used to learn a new task quicker. Meta-\nlearning learns hyperparameters of the diﬀerent tasks. In deep meta-learning, the\nset of initial network parameters is usually considered to be such a hyperparameter.\nMeta-learning aims to learn hyperparameters that can learn a new task with only\na few training examples, often using 𝑁-way-𝑘-shot learning. For deep few-shot\nlearning the Model-Agnostic Meta-Learning (MAML) approach is well-known, and\nhas inspired follow-up work.\nMeta-learning is of great importance in machine learning. For tasks that are\nrelated, good results are reported. For more challenging benchmarks, where tasks\nare less related (such as pictures of animals from very diﬀerent species), results are\nreported that are weaker.\nFurther Reading\nMeta-learning is a highly active ﬁeld of research. Good entry points are [882, 351,\n363, 364, 100]. Meta-learning has attracted much attention in artiﬁcial intelligence,\nboth in supervised learning and in reinforcement learning. Many books and surveys\nhave been written about the ﬁeld of meta-learning, see, for example [106, 684, 804,\n667, 836].\n272\n9 Meta-Learning\nThere has been active research interest in meta-learning algorithms for some\ntime, see, for example [669, 75, 676, 812]. Research into transfer learning and meta-\nlearning has a long history, starting with Pratt and Thrun [606, 774]. Early surveys\ninto the ﬁeld are [581, 758, 834], more recent surveys are [872, 879]. Huh et al. focus\non ImageNet [360, 622]. Yang et al. study the relation between transfer learning\nand curriculum learning with Sokoban [859].\nEarly principles of meta-learning are described by Schmidhuber [669, 676]. Meta-\nlearning surveys are [667, 812, 106, 684, 804, 836, 302, 351, 363, 364]. Papers on\nsimilarity-metric meta-learning are [418, 814, 717, 737, 263, 699]. Papers on model-\nbased meta-learning are [213, 826, 659, 542, 517, 223, 264]. Papers on optimization-\nbased meta-learning are many [629, 473, 243, 23, 623, 476, 561, 649, 245, 285, 246,\n861, 83].\nDomain adaptation is studied in [17, 200, 168, 872, 541]. Zero-shot learning\nis an active and promising ﬁeld. Interesting papers are [87, 356, 454, 580, 728,\n10, 851, 449, 636, 197, 622]. Like zero-shot learning, few-shot learning is also a\npopular area of meta-learning research [337, 437, 719, 565, 728]. Benchmark papers\nare [785, 864, 141, 775, 825].\nExercises\nIt is time to test our understanding of transfer learning and meta-learning with\nsome exercises and questions.\nQuestions\nBelow are some quick questions to check your understanding of this chapter. For\neach question a simple, single sentence answer is suﬃcient.\n1. What is the reason for the interest in meta-learning and transfer learning?\n2. What is transfer learning?\n3. What is meta-learning?\n4. How is meta-learning diﬀerent from multi task learning?\n5. Zero-shot learning aims to identify classes that it has not seen before. How is\nthat possible?\n6. Is pretraining a form of transfer learning?\n7. Can you explain learning to learn?\n8. Are the initial network parameters also hyperparameters? Explain.\n9. What is an approach for zero-shot learning?\n10. As the diversity of tasks increases, does meta-learning achieve good results?\n9.3 Meta-Learning Environments\n273\nExercises\nLet us go to the programming exercises to become more familiar with the methods\nthat we have covered in this chapter. Meta-learning and transfer learning exper-\niments are often very computationally expensive. You may need to scale down\ndataset sizes, or skip some exercises as a last resort.\n1. Pretraining Implement pretraining and ﬁne tuning in the Keras pretraining exam-\nple from Sect. 9.2.1.3.22 Do the exercises as suggested, including ﬁnetuning on the\ncats and dogs training set. Note the uses of preprocessing, data augmentation and\nregularization (dropout and batch normalization). See the eﬀects of increasing\nthe number of layers that you transfer on training performance and speed.\n2. MAML Reptile [561] is a meta-learning approach inpspired by MAML, but ﬁrst\norder, and faster, speciﬁcally designed for few-shot learning. The Keras website\ncontains a segment on Reptile.23 At the start a number of hyperparameters are\ndeﬁned: learning rate, step size, batch size, number of meta-learning iterations,\nnumber of evaluation iterations, how many shots, classes, etcetera. Study the\neﬀect of tuning diﬀerent hyperparameters, especially the ones related to few-shot\nlearning: the classes, the shots, and the number of iterations.\nTo delve deeper into few-shot learning, also have a look at the MAML code,\nwhich has a section on reinforcement learning.24 Try diﬀerent environments.\n3. Meta World As we have seen in Sect. 9.3.4, Meta World [864] is an elaborate\nbenchmark suite for meta reinforcement learning. Re-read the section, go to\nGitHub, and install the benchmark.25 Also go to Garage to install the agent algo-\nrithms so that you are able to test their performance.26 See that they work with\nyour PyTorch or TensorFlow setup. First try running the ML1 meta benchmark\nfor PPO. Then try MAML, and RL2. Next, try the more elaborate meta-learning\nbenchmarks. Read the Meta World paper, and see if you can reproduce their\nresults.\n4. ZSL We go from few-shot learning to zero-shot learning. One of the ways in which\nzero-shot learning works is by learning attributes that are shared by classes. Read\nthe papers Label-Embedding for Image Classiﬁcation [10], and An embarrassingly\nsimple approach to zero-shot learning [636], and go to the code [87].27 Implement\nit, and try to understand how attribute learning works. Print the attributes for the\nclasses, and use the diﬀerent datasets. Does MAML work for few-shot learning?\n(challenging)\n22 https://keras.io/guides/transfer_learning/\n23 https://keras.io/examples/vision/reptile/\n24 https://github.com/cbfinn/maml_rl/tree/master/rllab\n25 https://github.com/rlworkgroup/metaworld\n26 https://github.com/rlworkgroup/garage\n27 https://github.com/sbharadwajj/embarrassingly-simple-zero-shot-learning\nChapter 10\nFurther Developments\nWe have come to the end of this book. We will reﬂect on what we have learned. In\nthis chapter we will review the main themes and essential lessons, and we will look\nto the future.\nWhy do we study deep reinforcement learning? Our inspiration is the dream of\nartiﬁcial intelligence; to understand human intelligence and to create intelligent\nbehavior that can supplement our own, so that together we can grow. For reinforce-\nment learning our goal is to learn from the world, to learn increasingly complex\nbehaviors for increasingly complex sequential decision problems. The preceding\nchapters have shown us that many successful algorithms were inspired by how\nhumans learn.\nCurrently many environments consist of games and simulated robots, in the\nfuture this may include human-computer interactions and collaborations in teams\nwith real humans.\n10.1 Development of Deep Reinforcement Learning\nReinforcement learning has made a remarkable transition, from a method that\nwas used to learn small tabular toy problems, to learning simulated robots how to\nwalk, to playing the largest multi-agent real time strategy games, and beating the\nbest humans in Go and poker. The reinforcement learning paradigm is a frame-\nwork in which many learning algorithms have been developed. The framework is\nable to incorporate powerful ideas from other ﬁelds, such as deep learning, and\nautoencoders.\nTo appreciate the versatility of reinforcement learning, and now that we have\nstudied the ﬁeld in great detail, let us have a closer look at how the developments\nin the ﬁeld have proceeded over time.\n275\n276\n10 Further Developments\n10.1.1 Tabular Methods\nReinforcement learning starts with a simple agent/environment loop, where an\nenvironment performs the agent’s actions, and returns a reward (the model-free\napproach). We use a Markov decision process to formalize reinforcement learning.\nThe value and policy functions are initially implemented in a tabular fashion,\nlimiting the method to small environments, since the agent has to ﬁt the function\nrepresentations in memory. Typical environments are Grid world, Cartpole and\nMountain car; a typical algorithm to ﬁnd the optimal policy function is tabular\nQ-learning. Basic principles in the design of these algorithms are exploration,\nexploitation and on-policy/oﬀ-policy learning. Furthermore, imagination, as a form\nof model-based reinforcement learning, was developed.\nThis part of the ﬁeld forms a well-established and stable basis, that is, however,\nonly suitable for learning small single-agent problems. The advent of deep learning\ncaused the ﬁeld to shift into a higher gear.\n10.1.2 Model-free Deep Learning\nInspired by breakthroughs in supervised image recognition, deep learning was also\napplied to Q-learning, causing the Atari breakthrough for deep reinforcement learn-\ning. The basis of the success of deep learning in reinforcement learning are methods\nto break correlations and improve convergence (replay buﬀer and a separate target\nnetwork). The DQN algorithm [523] has become quite well known. Policy-based\nand actor critic approaches work well with deep learning, and are also applicable\nto continuous action spaces. Many model-free actor critic variants have been de-\nveloped [683, 480, 306, 521], they are often tested on simulated robot applications.\nAlgorithms often reach good quality optima, but model-free algorithms have a high\nsample complexity. Tables 3.1 and 4.1 list these algorithms.\nThis part of the ﬁeld—deep model-free value and policy-based algorithms—can\nnow be considered as well-established, with mature algorithms whose behavior is\nwell understood, and with good results for high-dimensional single-agent environ-\nments. Typical high-dimensional environments are the Arcade Learning Environ-\nment, and MuJoCo simulated physics locomotion tasks.\n10.1.3 Multi-Agent Methods\nNext, more advanced methods are covered. In Chap. 5 model-based algorithms\ncombine planning and learning to improve sample eﬃciency. For high-dimensional\nvisual environments they use uncertainty modeling and latent models or world\nmodels, to reduce the dimensionality for planning. The algorithms are listed in\nTable 5.2.\n10.1 Development of Deep Reinforcement Learning\n277\nFurthermore, the step from single-agent to multi-agent is made, enlarging the\ntype of problems that can be modeled, getting closer to real-world problems. The\nstrongest human Go players are beaten with a model-based self-play combination\nof MCTS and a deep actor critic algorithm. The self-play setup performs curriculum\nlearning, learning from previous learning tasks ordered from easy to hard, and is in\nthat sense a form of meta-learning. Variants are shown in Table 6.2.\nFor multi-agent and imperfect information problems, deep reinforcement learn-\ning is used to study competition, emergent collaboration, and hierarchical team\nlearning. In these areas reinforcement learning comes close to multi-agent systems\nand population based methods, such as swarm computing. Parallel population-based\nmethods may be able to learn the policy quicker than gradient-based methods, and\nthey may be a good ﬁt for multi-agent problems.\nAlso, imperfect information multi-agent problems are studied, such as poker;\nfor competitive games counterfactual regret minimization was developed. Research\ninto cooperation is continuing. Early strong results are reported in StarCraft using\nteam collaboration and team competition. In addition, research is being performed\ninto emergent social behavior, connecting the ﬁelds of reinforcement learning to\nswarm computing and multi-agent systems. Algorithms and experiments are listed\nin Table 7.2. A list of hierarchical approaches is shown in Table 8.1.\nIn human learning, new concepts are learned based on old concepts. Transfer\nlearning from foundation models, and meta-learning, aim to re-use existing knowl-\nedge, or even learn to learn. Meta-learning, curriculum learning, and hierarchical\nlearning are emerging as techniques to conquer ever larger state spaces. Table 9.2\nshows meta-learning approaches.\nAll these areas should be considered as advanced reinforcement learning, where\nactive research is still very much occurring. New algorithms are being developed,\nand experiments typically require large amounts of compute power. Furthermore,\nresults are less robust, and require much hyperparameter tuning. More advances\nare needed, and expected.\n10.1.4 Evolution of Reinforcement Learning\nIn contrast to supervised learning, which learns from a ﬁxed dataset, reinforce-\nment learning is a mechanism for learning by doing, just as children learn. The\nagent/environment framework has turned out to be a versatile approach, that can\nbe augmented and enhanced when we try new problem domains, such as high-\ndimensions, multi-agent or imperfect information. Reinforcement learning has\nencompassed methods from supervised learning (deep learning) and unsupervised\nlearning (autoencoders), as well as from population-based optimization.\nIn this sense reinforcement learning has evolved from being a single-agent\nMarkov decision process to a framework for learning, based on agent and environ-\nment. Other approaches can be hooked into this framework, to learn new ﬁelds,\nand to improve performance. These additions can interpret high dimensional states\n278\n10 Further Developments\n(as in DQN), or shrink a state space (as with latent models). When accomodating\nself-play, the framework provided us with a curriculum learning sequence, yielding\nworld class levels of play in two-agent games.\n10.2 Main Challenges\nDeep reinforcement learning is being used to understand more real world sequential\ndecision making situations. Among the applications that motivate these develop-\nments are self driving cars and other autonomous operations, image and speech\nrecognition, decision making, and, in general, acting naturally.\nWhat will the future bring for deep reinforcement learning? The main challenge\nfor deep reinforcement learning is to manage the combinatorial explosion that\noccurs when a sequence of decisions is chained together. Finding the right kind of\ninductive bias can exploit structure in this state space.\nWe list three major challenges for current and future research in deep reinforce-\nment learning:\n1. Solving larger problems faster\n2. Solving problems with more agents\n3. Interacting with people\nThe following techniques address these challenges:\n1. Solving larger problems faster\n•\nReducing sample complexity with latent-models\n•\nCurriculum learning in self-play methods\n•\nHierarchical reinforcement learning\n•\nLearn from previous tasks with transfer learning and meta-learning\n•\nBetter exploration through intrinsic motivation\n2. Solving problems with more agents\n•\nHierarchical reinforcement learning\n•\nPopulation-based self-play league methods\n3. Interacting with people\n•\nExplainable AI\n•\nGeneralization\nLet us have a closer look at these techniques, to see what future developments can\nbe expected for them.\n10.2 Main Challenges\n279\n10.2.1 Latent Models\nChapter 5 discussed model-based deep reinforcement learning methods. In model-\nbased methods a transition model is learned that is then used with planning to\naugment the policy function, reducing sample complexity. A problem for model-\nbased methods in high-dimensional problems is that high-capacity networks need\nmany observations in order to prevent overﬁtting, negating the potential reduction\nin sample complexity.\nOne of the most promising model-based methods is the use of autoencoders\nto create latent models, that compress or abstract from irrelevant observations,\nyielding a lower-dimensional latent state model that can be used for planning in a\nreduced state space. The reduction in sample complexity of model-based approaches\ncan thus be maintained. Latent models create compact world representations, that\nare also used in hierarchical and multi-agent problems, and further work is ongoing.\nA second development in model-based deep reinforcement learning is the use of\nend-to-end planning and learning of the transition model. Especially for elaborate\nself-play designs such as AlphaZero, where an MCTS planner is integrated in self-\nlearning, the use of end-to-end learning is advantageous, as the work on MuZero\nhas shown. Research is ongoing in this ﬁeld where planning and learning are\ncombined [679, 215, 186, 680, 359, 334, 525].\n10.2.2 Self-Play\nChapter 6 discussed learning by self-play in two-agent games. In many two-agent\ngames the transition function is given. When the environment of an agent is played\nby the opponent with the exact same transition function, a self-learning self-play\nsystem can be constructed in which both agent and environment improve eachother.\nWe have discussed examples of cycles of continuous improvement, from tabula-rasa\nto world-champion level.\nAfter earlier results in backgammon [763], AlphaZero achieved landmark results\nin Go, chess, and shogi [706, 704]. The AlphaZero design includes an MCTS planner\nin a self-play loop that improves a dual-headed deep residual network [704]. The\ndesign has spawned much further research, as well as inspired general interest in\nartiﬁcial intelligence and reinforcement learning [213, 464, 687, 552, 455, 239, 679,\n385].\n10.2.3 Hierarchical Reinforcement Learning\nTeam play is important in multi-agent problems, and hierarchical approaches can\nstructure the environment in a hierarchy of agents. Hierarchical reinforcement\nlearning methods are also applied to single agent problems, using principles of\n280\n10 Further Developments\ndivide and conquer. Many large single-agent problems are hierarchically structured.\nHierarchical methods aim to make use of this structure by dividing large problems\ninto smaller subproblems; they group primitive actions into macro actions. When\na policy has been found with a solution for a certain subproblem, then this can\nbe re-used when the subproblem surfaces again. Note that for some problems it is\ndiﬃcult to ﬁnd a hierarchical structure that can be exploited eﬃciently.\nHierarchical reinforcement learning has been studied for some time [150, 247,\n745]. Recent work has been reported on successful methods for deep hierarchical\nlearning and population-based training [475, 471, 546, 633], and more is to be\nexpected.\n10.2.4 Transfer Learning and Meta-Learning\nAmong the major challenges of deep reinforcement learning is the long training\ntime. Transfer learning and meta-learning aim to reduce the long training times,\nby transferring learned knowledge from existing to new (but related) tasks, and by\nlearning to learn from the training of previous tasks, to speedup learning new (but\nrelated) tasks.\nIn the ﬁelds of image recognition and natural language processing it has become\ncommon practice to use networks that are pretrained on ImageNet [191, 237]\nor BERT [193] or other large pretrained networks [548, 92]. Optimization-based\nmethods such as MAML learn better initial network parameters for new tasks, and\nhave spawned much further research.\nZero-shot learning is a meta-learning approach where outside information is\nlearned, such as attributes or a textual description for image content, that is then\nused to recognize individuals from a new class [636, 719, 851, 10]. Meta-learning is\na highly active ﬁeld where more results can be expected.\nFoundation models are large models, such as ImageNet for image recognition,\nand GPT-3 in natural language processing, that are trained extensively on large\ndatasets. They contain general knowledge, that can be specialized for a certain more\nspecialized task. They can also be used for multi-modal tasks, where text and image\ninformation is combined. The DALL-E project is able to create images that go with\ntextual descriptions. See Fig. 10.1 for amusing or beautiful examples (“an armchair\nin the shape of an avocado”) [616]. GPT-3 has also been used to study zero-shot\nlearning, with success, in the CLIP project [625].\n10.2.5 Population-Based Methods\nMost reinforcement learning has focused on one or two-agent problems. However,\nthe world around us is full of many-agent problems. The major problem of multi-\nagent reinforcement learning is to model the large, nonstationary, problem space.\n10.2 Main Challenges\n281\nFig. 10.1 DALL-E, an Algorithm that draws Pictures based on Textual Commands [616]\nRecent work applies self-play methods in a multi-agent setting, where entire\npopulations of agents are trained against eachother. This approach combines aspects\nof evolutionary algorithms (combining and mutating policies, as well as culling\nof underperforming agent policies) and hierarchical methods (modeling of team\ncollaboration).\nPopulation-based training of leagues of agents has achieved success in highly\ncomplex multi-agent games: StarCraft [813] and Capture the Flag [373]. Population-\nbased methods, combining evolutionary principles with self-play, curriculum learn-\ning, and hierarchical methods are active areas of research [653, 374, 512, 817, 408].\n10.2.6 Exploration and Intrinsic Motivation\nThe prime motivator for learning in reinforcement learning is reward. However,\nreward is sparse in many sequential decision problems. Reward shaping tries to\naugment the reward function with heuristc knowledge. Developemental psychology\nargues that learning is (also) based on curiosity, or intrinsically motivated. Intrinsic\nmotivation is a basic curiosity drive to explore for exploration’s sake, deriving\nsatisfaction from the exploration process itself.\nThe ﬁeld of intrinsic motivation is relatively new in reinforcement learning.\nLinks with hierarchical reinforcement learning and the options framework are\nbeing explored, as well as with models of curiosity [674, 660, 161].\nIntrinsic motivation in reinforcement learning can be used for exploring open-\nended environments [29, 712] (see also Sect. 8.3.2). Intrinsically motivated goal-\nconditioned algorithms can train agents to learn to represent, generate and pursue\ntheir own goals [162]. The success of the Go-Explore algorithm in domains with\nsparse rewards also stresses the importance of exploration in reinforcement learn-\ning [222].\n282\n10 Further Developments\n10.2.7 Explainable AI\nExplainable AI (XAI) is closely related to the topics of planning and learning that\nwe discuss in this book, and to natural language processing.\nWhen a human expert suggests an answer, this expert can be questioned to\nexplain the reasoning behind the answer. This is a desirable property, and enhances\nhow much we trust the answer. Most clients receiving advice, be it ﬁnancial or\nmedical, put greater trust in a well-reasoned explanation than in a yes or no answer\nwithout any explanation.\nDecision support systems that are based on classic symbolic AI can often be\nmade to provide such reasoning easily. For example, interpretable models [611],\ndecision trees [614], graphical models [381, 457], and search trees [160] can be\ntraversed and the choices at decision points can be recorded and used to translate\nin a human-understandable argument.\nConnectionist approaches such as deep learning, in contrast, are less inter-\npretable. Their accuracy, however, is typically much higher than the classical ap-\nproaches. Explainable AI aims to combine the ease of interpreting symbolic AI with\nthe accuracy of connectionist approaches [300, 205, 116].\nThe work on soft decision trees [257, 341] and adaptive neural trees [754] has\nshown how hybrid approaches of planning and learning can try to build an explana-\ntory decision tree based on a neural network. These works build in part on model\ncompression [142, 120] and belief networks [323, 558, 685, 184, 69, 767, 107, 28].\nUnsupervised methods can be used to ﬁnd interpretable models [611, 643, 807].\nModel-based reinforcement learning methods aim to perform deep sequential plan-\nning in learned world models [304].\n10.2.8 Generalization\nBenchmarks drive algorithmic progress in artiﬁcial intelligence. Chess and poker\nhave given us deep heuristic search; ImageNet has driven deep supervised learn-\ning; ALE has given us deep Q-learning; MuJoCo and the DeepMind control suite\nhave driven actor critic methods; Omniglot, MiniImagenet and Meta-World drive\nmeta-learning; and StarCraft and other multi-agent games drive hierarchical and\npopulation-based methods.\nAs the work on explainable AI indicates, there is a trend in reinforcement learning\nto study problems that are closer to the real world, through model-based methods,\nmulti-agent methods, meta-learning and hierarchical methods.\nAs deep reinforcement learning will be more widely applied to real-world prob-\nlems, generalization becomes important, as is argued by Zhang et al. [868]. Where\nsupervised learning experiments have a clear training set/test set separation to\nmeasure generalization, in reinforcement learning agents often fail to generalize\nbeyond the environment they were trained in [579, 159]. Reinforcement learning\nagents often overﬁt on their training environment [867, 869, 839, 235, 274]. This\n10.3 The Future of Artiﬁcial Intelligence\n283\nbecomes especially diﬀcult in sim-to-real transfer [876]. One benchmark speciﬁcally\ndesigned to increase generalization is Procgen. It aims to increase environment\ndiversity through procedural content generation, providing 16 parameterizable\nenvironments [158].\nBenchmarks will continue to drive progress in artiﬁcial intelligence, especially\nfor generalization [413].\n10.3 The Future of Artiﬁcial Intelligence\nThis book has covered the stable basis of deep reinforcement learning, as well as\nactive areas of research. Deep reinforcement learning is a highly active ﬁeld, and\nmany more developments will follow.\nWe have seen complex methods for solving sequential decision problems, some of\nwhich are easily solved on a daily basis by humans in the world around us. In certain\nproblems, such as backgammon, chess, checkers, poker, and Go, computational\nmethods have now surpassed human ability. In most other endeavours, such as\npouring water from a bottle in a cup, writing poetry, or falling in love, humans still\nreign supreme.\nReinforcement learning is inspired by biological learning, yet computational and\nbiological methods for learning are still far apart. Human intelligence is general\nand broad—we know much about many diﬀerent topics, and we use our general\nknowledge of previous tasks when learning new things. Artiﬁcial intelligence is\nspecialized and deep—computers can be good at certain tasks, but their intelligence\nis narrow, and learning from other tasks is a challenge.\nTwo conclusion are clear. First, for humans, hybrid intelligence, where human\ngeneral intelligence is augmented by specialized artiﬁcial intelligence, can be highly\nbeneﬁcial. Second, for AI, the ﬁeld of deep reinforcement learning is taking cues\nfrom human learning in hierarchical methods, curriculum learning, learning to\nlearn, and multi-agent cooperation.\nThe future of artiﬁcial intelligence is human.\nAppendices\nAppendix A\nMathematical Background\nThis appendix provides essential mathematical background and establishes the\nnotation that we use in this book. We start with notation of sets and functions,\nthen we discuss probability distributions, expectations, and information theory. You\nwill most likely have seen some of these in previous courses. We will also discuss\nhow to diﬀerentiate through an expectation, which frequently appears in machine\nlearning.\nThis appendix is based on Moerland [524].\nA.1 Sets and Functions\nWe start at the beginning, with sets and functions.\nA.1.1 Sets\nDiscrete set\nA discrete set is a set of countable elements.\nExamples:\n•\n𝑋= {1, 2, .., 𝑛}\n(integers)\n•\n𝑋= {up, down, left, right}\n(arbitrary elements)\n•\n𝑋= {0, 1}𝑑\n(d-dimensional binary space)\n287\n288\nA Mathematical Background\nContinuous set\nA continuous set is a set of connected elements.\nExamples:\n•\n𝑋= [2, 11]\n(bounded interval)\n•\n𝑋= R\n(real line)\n•\n𝑋= [0, 1]𝑑\n(𝑑-dimensional hypercube)\nConditioning a set\nWe can also condition within a set, by using : or |. For example, the discrete probability\n𝑘-simplex, which is what we actually use to deﬁne a discrete probability distribution\nover 𝑘categories, is given by:\n𝑋= {𝑥∈[0, 1]𝑘:\n∑︁\n𝑘\n𝑥𝑘= 1}.\nThis means that 𝑥is a vector of length 𝑘, consisting of entries between 0 and 1,\nwith the restriction that the vector sums to 1.\nCardinality and dimensionality\nIt is important to distinguish the cardinality and dimensionality of a set:\n•\nThe cardinality (size) counts the number of elements in a vector space, for which\nwe write |𝑋|.\n•\nThe dimensionality counts the number of dimensions in the vector space 𝑋, for\nwhich we write Dim(𝑋).\nExamples:\n•\nThe discrete space 𝑋= {0, 1, 2} has cardinality |𝑋| = 3 and dimension-\nality Dim(𝑋) = 1.\n•\nThe discrete vector space 𝑋= {0, 1}4 has cardinality |𝑋| = 24 = 16 and\ndimensionality Dim(𝑋) = 4.\nA.1 Sets and Functions\n289\nFig. A.1 𝑦= 𝑥2\nCartesian product\nWe can combine two spaces by taking the Cartesian product, denoted by ×, which\nconsists of all the possible combinations of elements in the ﬁrst and second set:\n𝑋× 𝑍= {(𝑥, 𝑧) : 𝑥∈𝑋, 𝑧∈𝑍}\nWe can also combine discrete and continuous spaces through Cartesian products.\nExample: Assume 𝑋= {20, 30} and 𝑍= {0, 1}. Then\n𝑋× 𝑍= {(20, 0), (20, 1), (30, 0), (30, 1)}\nAssume 𝑋= R and 𝑍= R. Then 𝑋× 𝑍= R2.\nA.1.2 Functions\n•\nA function 𝑓maps a value in the function’s domain 𝑋to a (unique) value in the\nfunction’s co-domain/range 𝑌, where 𝑋and 𝑌can be discrete or continuous sets.\n•\nWe write the statement that 𝑓is a function from 𝑋to 𝑌as\n𝑓: 𝑋→𝑌\nExamples:\n•\n𝑦= 𝑥2 maps every value in domain 𝑋∈R to range 𝑌∈R+ (see Fig. A.1)\n290\nA Mathematical Background\nA.2 Probability Distributions\nA probability distribution is a mathematical function that gives the probability\nof the occurrence of a set of possible outcomes. The set of possible outcomes is\ncalled the sample space, which can be discrete or continuous, and is denoted by\n𝑋. For example, for ﬂipping a coin 𝑋= {heads, tails}. When we actually sample\nthe variable, we get a particular value 𝑥∈𝑋. For example, for the ﬁrst coin ﬂip\n𝑥1 = heads. Before we actually sample the outcome, the particular outcome value\nis still unknown. We say that it is a random variable, denoted by 𝑋, which always\nhas an associated probability distribution 𝑝(𝑋).\nSample space (a set)\n𝑋\nRandom variable\n𝑋\nParticular value\n𝑥\nDepending on whether the sample space is a discrete or continuous set, the\ndistribution 𝑝(𝑋) and the way to represent it diﬀer. We detail both below, see also\nFig. A.2.\nA.2.1 Discrete Probability Distributions\n•\nA discrete variable 𝑋can take values in a discrete set 𝑋= {1, 2, .., 𝑛}. A particular\nvalue that 𝑋takes is denoted by 𝑥.\n•\nDiscrete variable 𝑋has an associated probability mass function: 𝑝(𝑋), where\n𝑝: 𝑋→[0, 1]. Each possible value 𝑥that the variable can take is associated with\na probability 𝑝(𝑋= 𝑥) ∈[0, 1]. (For example, 𝑝(𝑋= 1) = 0.2, the probability\nthat 𝑋is equal to 1 is 20%.)\n•\nProbability distributions always sum to 1: Í\n𝑥∈𝑋𝑝(𝑥) = 1.\nParameters\nWe represent a probability distribution with parameters. For a discrete distribution\nof size 𝑛, we need 𝑛−1 parameters, {𝑝𝑥=1, .., 𝑝𝑥=𝑛−1}, where 𝑝𝑥=1 = 𝑝(𝑥=1). The\nprobability of the last category follows from the sum to one constraint, 𝑝𝑥=𝑛=\n1 −Í𝑛−1\n𝑖=1 𝑝𝑥=𝑖.\nA.2 Probability Distributions\n291\nExample: A discrete variable 𝑋that can take three values (𝑋= {1, 2, 3}),\nwith associated probability distribution 𝑝(𝑋= 𝑥):\n𝑝(𝑋= 1) 𝑝(𝑋= 2) 𝑝(𝑋= 3)\n0.2\n0.4\n0.4\nRepresenting discrete random variables\nIt is important to realize that we always represent a discrete variable as a vector\nof probabilities. Therefore, the above variable 𝑋does not really take values 𝑋=\n{1, 2, 3}, because 1, 2 and 3 are arbitrary categories (category two is not twice as\nmuch as the ﬁrst category). We could just as well have written 𝑋= {𝑎, 𝑏, 𝑐}. Always\nthink of the possible values of a discrete variable as separate entries. Therefore, we\nshould represent the value of a discrete variable as a vector of probabilities. In the\ndata, when we observe the ground truth, this becomes a one-hot encoding, where\nwe put all mass on the observed class.\nExample: In the above example, we had (𝑋= {1, 2, 3}). Imagine we sample\n𝑋three times and observe 1, 2 and 3, respectively. We would actually\nrepresent these observations as\nObserved category\nRepresentation\n1\n(1, 0, 0)\n2\n(0, 1, 0)\n3\n(0, 0, 1)\nA.2.2 Continuous Probability Distributions\n•\nA continuous variable 𝑋can take values in a continuous set, 𝑋= R (the real line),\nor 𝑋= [0, 1] (a bounded interval).\n•\nContinuous variable 𝑋has an associated probability density function: 𝑝(𝑋),\nwhere 𝑝: 𝑋→R+ (a positive real number).\n•\nIn a continuous set, there are inﬁnitely many values that the random value can\ntake. Therefore, the absolute probability of any particular value is 0.\n•\nWe can only deﬁne absolute probability on an interval, 𝑝(𝑎< 𝑋≤𝑏) =\n∫𝑏\n𝑎𝑝(𝑥).\n(For example, 𝑝(2 < 𝑋≤3) = 0.2, the probability that 𝑋will fall between 2 and\n3 is equal to 20%.)\n292\nA Mathematical Background\nFig. A.2 Examples of discrete (left) versus continuous (right) probability distibution [378]\n•\nThe interpretation of an individual value of the density, like 𝑝(𝑋= 3) = 4, is\nonly a relative probability. The higher the probability 𝑝(𝑋= 𝑥), the higher the\nrelative chance that we would observe 𝑥.\n•\nProbability distributions always sum to 1:\n∫\n𝑥∈𝑋𝑝(𝑥) = 1 (note that this time\nwe integrate instead of sum).\nParameters\nWe need to represent a continuous distribution with a parameterized function, that\nfor every possible value in the sample space predicts a relative probability. Moreover,\nwe need to obey the sum to one constraint. Therefore, there are many parameterized\ncontinuous probability densities. An example is the Normal distribution. A continuous\ndensity is a function 𝑝: 𝑋→R+ that depends on some parameters. Scaling the\nparameters allows variation in the location where we put probability mass.\nExample: A variable 𝑋that can take values on the real line with distribution\n𝑝(𝑥; 𝜇, 𝜎) =\n1\n𝜎\n√\n2𝜋\nexp\n\u0010\n−(𝑥−𝜇)2\n2𝜎2\n\u0011\n.\nHere, the mean parameter 𝜇and standard deviation 𝜎are the parameters.\nWe can change them to change the shape of the distribution, while always\nensuring that it still sums to one. We draw an example normal distribution\nin Fig. A.2, right.\nThe diﬀerences between discrete and continuous probability distributions\nare summarized in Table A.1.\nA.2 Probability Distributions\n293\nTable A.1 Comparison of discrete and continuous probability distributions\nDiscrete distribution\nContinuous distribution\nInput/sample space\nDiscrete set,\n𝑋= {0, 1},\nwith size 𝑛= |𝑋|\nContinuous set,\n𝑋= R\nProbability function\nProbability mass function (pmf)\n𝑝: 𝑋→[0, 1]\nsuch that Í\n𝑥∈𝑋𝑝(𝑥) = 1\nProbability density function (pdf)\n𝑝: 𝑋→R\nsuch that\n∫\n𝑥∈𝑋𝑝(𝑥) = 1\nPossible parametrized\ndistributions\nVarious, but only need simple\nDiscrete\nVarious,\nNormal, Logistic, Beta, etc.\nParameters\n{𝑝𝑥=1, .., 𝑝𝑥=𝑛−1}\nDepends on distribution,\nfor normal: {𝜇, 𝜎}\nNumber of parameters\n𝑛−1 = |𝑋|−1\n(Due to sum to 1 constraint)a\nDepends on distribution,\nfor normal: 2\nExample distribution\nfunction\n𝑝(𝑥= 1) = 0.2\n𝑝(𝑥= 2) = 0.4\n𝑝(𝑥= 3) = 0.4\ne.g. for normal 𝑝(𝑥|𝜇, 𝜎) =\n1\n𝜎\n√\n2𝜋exp\n\u0010\n−(𝑥−𝜇)2\n2𝜎2\n\u0011\nAbsolute probability\n𝑝(𝑥= 1) = 0.2\n𝑝(3 ≤𝑥< 4) =\n∫4\n3 𝑝(𝑥) = 0.3\n(on interval)b\nRelative probability\n-\n𝑝(𝑥= 3) = 7.4\na Due to the sum to 1 constraint, we need one parameter less than the size of the sample space,\nsince the last probability is 1 minus all the others: 𝑝𝑛= 1 −Í𝑛−1\n𝑖=1 𝑝𝑖.\nb Note that for continuous distributions, probabilities are only deﬁned on intervals. The density\nfunction 𝑝(𝑥) only gives relative probabilities, and therefore we may have 𝑝(𝑥) > 1, like\n𝑝(𝑥= 3) = 5.6, which is of course not possible (one should not interpret it as an absolute\nprobability). However, 𝑝(𝑎≤𝑥< 𝑏) =\n∫𝑏\n𝑎𝑝(𝑥) < 1 by deﬁnition.\nA.2.3 Conditional Distributions\n•\nA conditional distribution means that the distribution of one variable depends\non the value that another variable takes.\n•\nWe write 𝑝(𝑋|𝑌) to indicate that the value of 𝑋depends on the value of 𝑌.\n294\nA Mathematical Background\nExample: For discrete random variables, we may store a conditional distri-\nbution as a table of size |𝑋| × |𝑌|. A variable 𝑋that can take three values\n(𝑋= {1, 2, 3}) and variable 𝑌that can take two values (𝑌= {1, 2}). The\nconditional distribution may for example be:\n𝑝(𝑋= 1|𝑌) 𝑝(𝑋= 2|𝑌) 𝑝(𝑋= 3|𝑌)\n𝑌= 1\n0.2\n0.4\n0.4\n𝑌= 2\n0.1\n0.9\n0.0\nNote that for each value 𝑌, 𝑝(𝑋|𝑌) should still sum to 1, it is a valid proba-\nbility distribution. In the table above, each row therefore sums to 1.\nExample: We can similarly store conditional distributions for continuous\nrandom variables, this time mapping the input space to the parameters of\na continuous probability distribution. For example, for 𝑝(𝑌|𝑋) we can as-\nsume a Gaussian distribution 𝑁(𝜇(𝑥), 𝜎(𝑠)), where the mean and standard\ndeviation depend on 𝑥∈R. Then we can for example specify:\n𝜇(𝑥) = 2𝑥,\n𝜎(𝑥) = 𝑥2\nand therefore have\n𝑝(𝑦|𝑥) = 𝑁(2𝑥, 𝑥2)\nNote that for each value of 𝑋, 𝑝(𝑌|𝑋) still integrates to 1, it is a valid\nprobability distribution.\nA.2.4 Expectation\nWe also need the notion of an expectation.\nA.2.4.1 Expectation of a Random Variable\nThe expectation of a random variable is essentially an average. For a discrete variable,\nit is deﬁned as:\nE𝑋∼𝑝(𝑋) [ 𝑓(𝑋)] =\n∑︁\n𝑥∈𝑋\n[𝑥· 𝑝(𝑥)]\n(A.1)\nFor a continuous variable, the summation becomes integration.\nA.2 Probability Distributions\n295\nExample:\nAssume a given 𝑝(𝑋) for a binary variable:\n𝑥𝑝(𝑋= 𝑥)\n0\n0.8\n1\n0.2\nThe expectation is\nE𝑋∼𝑝(𝑋) [ 𝑓(𝑋)] = 0.8 · 0 + 0.2 · 1 = 0.2\n(A.2)\nA.2.4.2 Expectation of a Function of a Random Variable\nMore often, and also in the context of reinforcement learning, we will need the\nexpectation of a function of the random variable, denoted by 𝑓(𝑋). Often, this\nfunction maps to a continuous output.\n•\nAssume a function 𝑓: 𝑋→R, which, for every value 𝑥∈𝑋maps to a\ncontinuous value 𝑓(𝑥) ∈R.\n•\nThe expectation is then deﬁned as follows:\nE𝑋∼𝑝(𝑋) [ 𝑓(𝑋)] =\n∑︁\n𝑥∈𝑋\n[ 𝑓(𝑥) · 𝑝(𝑥)]\n(A.3)\nFor a continuous variable the summation again becomes integration. The formula\nmay look complicated, but it essentially reweights each function outcome by the\nprobability that this output occurs, see the example below.\nExample:\nAssume a given density 𝑝(𝑋) and function 𝑓(𝑥):\n𝑥𝑝(𝑋= 𝑥) 𝑓(𝑥)\n1\n0.2\n22.0\n2\n0.3\n13.0\n3\n0.5\n7.4\nThe expectation of the function can be computed as\nE𝑋∼𝑝(𝑋) [ 𝑓(𝑋)] = 22.0 · 0.2 + 13.0 · 0.3 + 7.4 · 0.5\n= 12.0\nThe same principle applies when 𝑝(𝑥) is a continuous density, only with the\nsummation replaced by integration.\n296\nA Mathematical Background\nFig. A.3 Entropy of a binary discrete variable. Horizontal axis shows the probability that the\nvariable takes value 1, the vertical axis shows the associated entropy of the distribution. High\nentropy implies high spread in the distribution, while low entropy implies little spread.\nA.2.5 Information Theory\nInformation theory studies the amount of information that is present in distributions,\nand the way that we can compare distributions.\nA.2.5.1 Information\nThe information 𝐼of an event 𝑥observed from distribution 𝑝(𝑋) is deﬁned as:\n𝐼(𝑥) = −log 𝑝(𝑥).\nIn words, the more likely an observation is (the higher 𝑝(𝑥)), the less information\nwe get when we actually observe the event. In other words, the information of an\nevent is the (potential) reduction of uncertainty. On the two extremes we have:\n•\n𝑝(𝑥) = 0\n:\n𝐼(𝑥) = −log 0 = ∞\n•\n𝑝(𝑥) = 1\n:\n𝐼(𝑥) = −log 1 = 0\nA.2.5.2 Entropy\nWe deﬁne the entropy 𝐻of a discrete distribution 𝑝(𝑋) as\n𝐻[𝑝] = E𝑋∼𝑝(𝑋) [𝐼(𝑋)]\n= E𝑋∼𝑝(𝑋) [−log 𝑝(𝑋)]\n= −\n∑︁\n𝑥\n𝑝(𝑥) log 𝑝(𝑥)\n(A.4)\nA.2 Probability Distributions\n297\nIf the base of the logarithm is 2, then we measure it in bits. When the base of the\nlogarithm is 𝑒, then we measure the entropy in nats. The continuous version of the\nabove equation is called the continuous entropy or diﬀerential entropy.\nInformally, the entropy of a distribution is a measure of the amount of “uncer-\ntainty” in a distribution, i.e., a measure of its “spread.” We can nicely illustrate this\nwith a binary variable (0/1), where we plot the probability of a 1 against the entropy\nof the distribution (Fig. A.3). We see that on the two extremes, the entropy of the\ndistribution is 0 (no spread at all), while the entropy is maximal for 𝑝(𝑥= 1) = 0.5\n(and therefore 𝑝(𝑥= 0) = 0.5), which gives maximal spread to the distribution.\nExample: The entropy of the distribution in the previous example is:\n𝐻[𝑝] = −\n∑︁\n𝑥\n𝑝(𝑥) log 𝑝(𝑥)\n= −0.2 · ln 0.2 −0.3 · ln 0.3 −0.5 · ln 0.5 = 1.03 nats\n(A.5)\nA.2.5.3 Cross-entropy\nThe cross-entropy is deﬁned between two distributions 𝑝(𝑋) and 𝑞(𝑋) deﬁned\nover the same support (sample space). The cross-entropy is given by:\n𝐻[𝑝, 𝑞] = E𝑋∼𝑝(𝑋) [−log 𝑞(𝑋)]\n= −\n∑︁\n𝑥\n𝑝(𝑥) log 𝑞(𝑥)\n(A.6)\nWhen we do maximum likelihood estimation in supervised learning, then we\nactually minimize the cross-entropy between the data distribution and the model\ndistribution.\nA.2.5.4 Kullback-Leibler Divergence\nFor two distributions 𝑝(𝑋) and 𝑞(𝑋) we can also deﬁne the relative entropy, better\nknown as the Kullback-Leibler (KL) divergence 𝐷KL:\n298\nA Mathematical Background\n𝐷KL[𝑝||𝑞] = E𝑋∼𝑝(𝑋)\nh\n−log 𝑞(𝑋)\n𝑝(𝑋)\ni\n= −\n∑︁\n𝑥\n𝑝(𝑥) log 𝑞(𝑥)\n𝑝(𝑥)\n(A.7)\nThe Kullback-Leibler divergence is a measure of the distance between two\ndistributions. The more two distributions depart from eachother, the higher the KL-\ndivergence will be. Note that the KL-divergence is not symmetrical, 𝐷KL[𝑝||𝑞] ≠\n𝐷KL[𝑞||𝑝] in general.\nFinally, we can also rewrite the KL-divergence as an entropy and cross-entropy,\nrelating the previously introduced quantities:\n𝐷KL[𝑝||𝑞] = E𝑋∼𝑝(𝑋)\nh\n−log 𝑞(𝑋)\n𝑝(𝑋)\ni\n=\n∑︁\n𝑥\n𝑝(𝑥) log 𝑝(𝑥) −\n∑︁\n𝑥\n𝑝(𝑥) log 𝑞(𝑥)\n= 𝐻[𝑝] + 𝐻[𝑝, 𝑞]\n(A.8)\nEntropy, cross-entropy and KL-divergence are common in many machine learning\ndomains, especially to construct loss functions.\nA.3 Derivative of an Expectation\nA key problem in gradient-based optimization, which appears in parts of machine\nlearning, is getting the gradient of an expectation. We will here discuss one well-\nknown method:1 the REINFORCE estimator (in reinforcement learning), which is in\nother ﬁelds also know as the score function estimator, likelihood ratio method, and\nautomated variational inference.\nAssume that we are interested in the gradient of an expectation, where the\nparameters appear in the distribution of the expectation:2\n∇𝜃E𝑥∼𝑝𝜃(𝑥) [ 𝑓(𝑥)]\n(A.9)\nWe cannot sample the above quantity, because we have to somehow move the\ngradient inside the expectation (and then we can sample the expectation to evaluate\n1 Other methods to diﬀerentiate through an expectation is through the reparametrization trick, as\nfor example used in variational auto-encoders, but we will not further treat this topic here.\n2 If the parameters only appear in the function 𝑓(𝑥) and not in 𝑝(𝑥), then we can simply push\nthe gradient through the expectation\nA.4 Bellman Equations\n299\nit). To achieve this, we will use a simple rule regarding the gradient of the log of\nsome function 𝑔(𝑥):\n∇𝑥log 𝑔(𝑥) = ∇𝑥𝑔(𝑥)\n𝑔(𝑥)\n(A.10)\nThis results from simple application of the chain-rule.\nWe will now expand Eq. A.9, where we midway apply the above log-derivative\ntrick.\n∇𝜃E𝑥∼𝑝𝜃(𝑥) [ 𝑓(𝑥)] = ∇𝜃\n∑︁\n𝑥\n𝑓(𝑥) · 𝑝𝜃(𝑥)\ndeﬁnition of expectation\n=\n∑︁\n𝑥\n𝑓(𝑥) · ∇𝜃𝑝𝜃(𝑥)\npush gradient through sum\n=\n∑︁\n𝑥\n𝑓(𝑥) · 𝑝𝜃(𝑥) · ∇𝜃𝑝𝜃(𝑥)\n𝑝𝜃(𝑥)\nmultiply/divide by 𝑝𝜃(𝑥)\n=\n∑︁\n𝑥\n𝑓(𝑥) · 𝑝𝜃(𝑥) · ∇𝜃log 𝑝𝜃(𝑥)\nlog-der. rule (Eq. A.10)\n= E𝑥∼𝑝𝜃(𝑥) [ 𝑓(𝑥) · ∇𝜃log 𝑝𝜃(𝑥)]\nrewrite into expectation\nWhat the above derivation essential does is pushing the derivative inside of the sum.\nThis equally applies when we change the sum into an integral. Therefore, for any\n𝑝𝜃(𝑥), we have:\n∇𝜃E𝑥∼𝑝𝜃(𝑥) [ 𝑓(𝑥)] = E𝑥∼𝑝𝜃(𝑥) [ 𝑓(𝑥) · ∇𝜃log 𝑝𝜃(𝑥)]\n(A.11)\nThis is known as the log-derivative trick, score function estimator, or REINFORCE\ntrick. Although the formula may look complicated, the interpretation is actually sim-\nple. We explain this idea in Fig. A.4. In Sect. 4.2.1 we apply this idea to reinforcement\nlearning.\nA.4 Bellman Equations\nBellman noted that the value function can be written in recursive form, because the\nvalue is also deﬁned at the next states. In his work on dynamic programming [72],\nhe derived recursive equations for 𝑉and 𝑄.\nThe Bellman equations for state-values and state-action values are:\n𝑉𝜋(𝑠) = E𝑎∼𝜋(·|𝑠)E𝑠′∼𝑇𝑎(𝑠)\n\u0002\n𝑟𝑎(𝑠, 𝑠′) + 𝛾· 𝑉𝜋(𝑠′)\n\u0003\n𝑄𝜋(𝑠, 𝑎) = E𝑠′∼𝑇𝑎(𝑠)\n\u0002\n𝑟𝑎(𝑠, 𝑠′) + 𝛾· E𝑎′∼𝜋(·|𝑠′) [𝑄𝜋(𝑠′, 𝑎′)]\n\u0003\nDepending on whether the state and action space are discrete or continuous re-\nspectively, we write out these equations diﬀerently. For a discrete state space and\n300\nA Mathematical Background\nFig. A.4 Graphical illustration of REINFORCE estimator. Left: Example distribution 𝑝𝜃(𝑥) and\nfunction 𝑓(𝑥). When we evaluate the expectation of Eq. A.11, we take 𝑚samples, indicated by\nthe blue dots (in this case 𝑚= 8). The magnitude of 𝑓(𝑥) is shown with the red vertical arrows.\nRight: When we apply the gradient update, each sample pushes up the density at that location, but\nthe magnitude of the push is multiplied by 𝑓(𝑥). Therefore, the higher 𝑓(𝑥), the harder we push.\nSince a density needs to integrate to 1, we will increase the density where we push hardest (in the\nexample on the rightmost sample). The distribution will therefore shift to the right on this update.\ndiscrete action space, we write the expectations as summations:\n𝑉(𝑠) =\n∑︁\n𝑎∈𝐴\n𝜋(𝑎|𝑠)\nh ∑︁\n𝑠′∈𝑆\n𝑇𝑎(𝑠, 𝑠′)\n\u0002\n𝑟𝑎(𝑠, 𝑠′) + 𝛾· 𝑉(𝑠′)\n\u0003i\n𝑄(𝑠, 𝑎) =\n∑︁\n𝑠′∈𝑆\n𝑇𝑎(𝑠, 𝑠′)\n\u0002\n𝑟𝑎(𝑠, 𝑠′) + 𝛾·\n∑︁\n𝑎∈𝐴\n𝜋(𝑎|𝑠)[𝑄(𝑠′, 𝑎′)]\n\u0003\nFor continuous state and action spaces, the summations over policy and transition\nare replaced by integration:\n𝑉(𝑠) =\n∫\n𝑎\n𝜋(𝑎|𝑠)\nh ∫\n𝑠′ 𝑇𝑎(𝑠, 𝑠′)\n\u0002\n𝑟𝑎(𝑠, 𝑠′) + 𝛾· 𝑉(𝑠′)\n\u0003\nd𝑠′i\nd𝑎\nThe same principle applies to the Bellman equation for state-action values:\n𝑄(𝑠, 𝑎) =\n∫\n𝑠′ 𝑇𝑎(𝑠, 𝑠′)\n\u0002\n𝑟𝑎(𝑠, 𝑠′) + 𝛾·\n∫\n𝑎′[𝜋(𝑎′|𝑠′) · 𝑄(𝑠′, 𝑎′)\n\u0003\nd𝑎′i\nd𝑠′\nWe may also have a continuous state space (such as visual input) with a discrete\naction space (such as pressing buttons in a game):\n𝑄(𝑠, 𝑎) =\n∫\n𝑠′ 𝑇𝑎(𝑠, 𝑠′)\n\u0002\n𝑟𝑎(𝑠, 𝑠′) + 𝛾·\n∑︁\n𝑎′\n[𝜋(𝑎′|𝑠′) · 𝑄(𝑠′, 𝑎′)\n\u0003i\nd𝑠′\nAppendix B\nDeep Supervised Learning\nThis appendix provides a chapter-length overview of essentials of machine learning\nand deep learning. Deep reinforcement learning uses much of the machinery of\ndeep supervised learning, and a good understanding of deep supervised learning is\nessential. This appendix should provide you with the basics, in case your knowledge\nof basic machine learning and deep learning is rusty. In doubt? Try to answer the\nquestions on page 332.\nWe will start with machine learning basics. We will discuss training and testing,\naccuracy, the confusion matrix, generalization, and overﬁtting.\nNext, we will provide an overview of deep learning. We will look into neural\nnetworks, error functions, training by gradient descent, end-to-end feature learning,\nand the curse of dimensionality. For neural networks we will discuss convolutional\nnetworks, recurrent networks, LSTM, and measures against overﬁtting.\nFinally, on the practical side, we will discuss TensorFlow, Keras, and PyTorch.\nWe will start with methods for solving large and complex problems.\nB.1 Machine Learning\nThe goal of machine learning is generalization: to create an accurate, predictive,\nmodel of the world. Such an accurate model is said to generalize well to the world.1\nIn machine learning we operationalize this goal with two datasets, a training set\nand a test set.\nThe ﬁeld of machine learning aims to ﬁt a function to approximate an input/out-\nput relation. This can be, for example, a regression function or a classiﬁcation\nfunction. The most basic form of machine learning is when a dataset 𝐷of input/out-\nput pairs is given. We call this form supervised learning, since the learning process\nis supervised by the output values.\n1 Generalization is closely related to the concepts of overﬁtting, regularization, and smoothness,\nas we will see in Sect. B.1.3.\n301\n302\nB Deep Supervised Learning\nPredicted class\nP\nN\nActual P\nTP\nFN\nclass\nN FP\nTN\nPredicted class\nCat\nDog\nActual Cat\n122\n8\nclass\nDog\n3\n9\nTable B.1 Confusion Matrix\nIn machine learning we often deal with large problem domains. We are interested\nin a function that works not just on the particular values on which it was trained,\nbut also in the rest of the problem domain from which the data items were taken.\nIn this section we will ﬁrst see how such a learning process for generalization\nworks; for notation and examples we follow from [524]. Next, we will discuss the\nspeciﬁc problems of large domains. Finally, we will discuss the phenomenon of\noverﬁtting and how it relates to the bias-variance trade-oﬀ.\nB.1.1 Training Set and Test Set\nA machine learning algorithm must learn a function ˆ𝑓(𝑥) →𝑦on a training set\nfrom input values 𝑥(such as images) to approximate corresponding output values\n𝑦(such as image labels). The goal of the machine learning algorithm is to learn\nthis function ˆ𝑓, such that it performs well on the training data and generalizes well\nto the test data. Let us see how we can measure how well the machine learning\nmethods generalize.\nThe notions that are used to assess the quality of the approximation are as\nfollows. Let us assume that our problem is a classiﬁcation task. The elements that\nour function correctly predicts are called the true positives (TP). Elements that\nare correctly identiﬁed as not belonging to a class are the true negatives (TN).\nElements that are mis-identifed as belonging to a class are the false positives (FP),\nand elements that belong to the class but that the predictor misclassiﬁes are the\nfalse negatives (FN).\nConfusion Matrix: The number of true positives divided by the total\nnumber of positives (true and false, TP/(TP+FP)) is called the precision of\nthe classiﬁer. The number of true positives divided by the size of the class\n(TP/(TP+FN)) is the recall, or the number of relevant elements that the\nclassiﬁer could ﬁnd. The term accuracy is deﬁned as the total number of\ntrue positives and negatives divided by the total number of predictions\n(TP+TN)/(TP+TN+FP+FN): how well correct elements are predicted [93].\nThese numbers are often shown in the form of a confusion matrix (Table B.1).\nThe numbers in the cells represent the number of true positives, etc., of the\nexperiment.\nB.1 Machine Learning\n303\nIn machine learning, we are often interested in the accuracy of a method on the\ntraining set, and whether the accuracy on the test set is the same as on the training\nset (generalization).\nIn regression and classiﬁcation problems the term error value is used to indicate\nthe diﬀerence between the true output value 𝑦and the predicted output value\nˆ𝑓(𝑥). Measuring how well a function generalizes is typically performed using a\nmethod called 𝑘-fold cross-validation. This works as follows. When a dataset of\ninput-output examples (𝑥, 𝑦) is present, this set is split into a large training set and\na smaller hold-out test set, typically 80/20 or 90/10. The approximator is trained\non the training set until a certain level of accuracy is achieved. For example, the\napproximator may be a neural network whose parameters 𝜃are iteratively adjusted\nby gradient descent so that the error value on the training set is reduced to a suitably\nlow value. The approximator function is said to generalize well, when the accuracy\nof the approximator is about the same on the test set as it is on the training set.\n(Sometimes a third dataset is used, the validation set, to allow stopping before\noverﬁtting occurs, see Sect. B.2.7.)\nSince the test set and the training set contain examples that are drawn from the\nsame original dataset, approximators can be expected to be able to generalize well.\nThe testing is said to be in-distribution when training and test set are from the same\ndistribution. Out-of-distribution generalization to diﬀerent problems (or transfer\nlearning) is more diﬃcult, see Sect. 9.2.1.\nB.1.2 Curse of Dimensionality\nThe state space of a problem is the space of all possible diﬀerent states (combina-\ntions of values of variables). State spaces grow exponentially with the number of\ndimensions (variables); high-dimensional problems have large state spaces, and\nmodern machine learning algorithms have to be able to learn functions in such\nlarge state spaces.\nExample: A classic problem in AI is image classiﬁcation: predicting what\ntype of object is pictured in an image. Imagine that we have low-resolution\ngreyscale images of 100×100 pixels, where each pixel takes a discrete value\nbetween 0 and 255 (a byte). Then, the input space 𝑋∈{0, 1, . . . , 255}100·100,\nthe machine learning problem has dimensionality 100 · 100 = 10000, and\nthe state space has size 25610000.\nWhen the input space 𝑋is high dimensional, we can never store the entire\nstate space (all possible pixels with all possible values) as a table. The eﬀect of an\nexponential need for observation data as the dimensionality grows, has been called\nthe curse of dimensionality by Richard Bellman [72]. The curse of dimensionality\nstates that the cardinality (number of unique points) of a space scales exponentially\nin the dimensionality of the problem. In a formula, we have that\n304\nB Deep Supervised Learning\n|𝑋| ∼exp(Dim(𝑋)).\nDue to the curse of dimensionality, the size of a table to represent a function\nincreases quickly when the size of the input space increases.\nExample: Imagine we have a discrete input space 𝑋= {0, 1}𝐷that maps\nto a real number, 𝑌= R, and we want to store this function fully as a table.\nWe will show the required size of the table and the required memory when\nwe use 32-bit (4 byte) ﬂoating point numbers:\nDim(𝑋)\n|𝑋|\nMemory\n1\n2\n8 Byte\n5\n25 = 32\n128 Byte\n10\n210 = 1024\n4KB\n20\n220 ≈106\n4MB\n50\n250 ≈1015\n4.5 million TB\n100\n2100 ≈1030\n5 · 1021 TB\n265\n2265 ≈2 · 1080\n-\nThe table shows how quickly exponential growth develops. At a discrete\nspace of size 20, it appears that we are doing alright, storing 4 Megabyte\nof information. However, at a size of 50, we suddenly need to store 4.5\nmillion Terabyte. We can hardly imagine the numbers that follow. At an\ninput dimensionality of size 265, our table would have grown to size 2·1080,\nwhich is close to the estimated number of atoms in the universe.\nSince the size of the state space grows exponentially, but the number of observa-\ntions typically does not, most state spaces in large machine learning problems are\nsparsely populated with observations. An important challenge for machine learning\nalgorithms is to ﬁt good predictive models on sparse data. To reliably estimate a\nfunction, each variable needs a certain number of observations. The number of\nsamples that are needed to maintain statistical signiﬁcance increases exponentially\nas the number of dimensions grows. A large amount of training data is required\nto ensure that there are several samples for each combination of values;2 however,\ndatasets rarely grow exponentially.\nB.1.3 Overﬁtting and the Bias-Variance Trade-Oﬀ\nBasic statistics tells us that, according to the law of large numbers, the more obser-\nvations we have of an experiment, the more reliable the estimate of their value is\n(that is, the average will be close to the expected value) [93]. This has important\n2 Unless we introduce some bias into the problem, by assuming smoothness, implying that there\nare dependencies between variables, and that the “true” number of independent variables is smaller\nthan the number of pixels.\nB.1 Machine Learning\n305\nFig. B.1 Curve ﬁtting: does the curvy red line or the straight dashed blue line best generalize the\ninformation in the data points?\nimplications for the study of large problems where we would like to have conﬁdence\nin the estimated values of our parameters.\nIn small toy problems the number of variables of the model is also small. Single-\nvariable linear regression problems model the function as a straight line 𝑦= 𝑎·𝑥+𝑏,\nwith only one independent variable 𝑥and two parameters 𝑎and 𝑏. Typically, when\nregression is performed with a small number of independent variables, then a\nrelatively large number of observations is available per variable, giving conﬁdence\nin the estimated parameter values. A rule of thumb is that there should be 5 or more\ntraining examples for each variable [769].\nIn machine learning dimensionality typically means the number of variables of a\nmodel. In statistics, dimensionality can be a relative concept: the ratio of the number\nof variables compared to the number of observations. In practice, the size of the\nobservation dataset is limited, and then absolute and relative dimensionality do not\ndiﬀer much. In this book we follow the machine learning deﬁnition of dimensions\nmeaning variables.\nModeling high-dimensional problems well typically requires a model with many\nparameters, the so-called high-capacity models. Let us look deeper into the conse-\nquences of working with high-dimensional problems. To see how to best ﬁt our\nmodel, let us consider machine learning as a curve ﬁtting problem, see Fig. B.1.\nIn many problems, the observations are measurements of an underlying natural\nprocess. The observations therefore contain some measurement noise. The goal\nis (1) that the approximating curve ﬁts the (noisy) observations as accurately as\npossible, but, (2) in order to generalize well, to aim to ﬁt the signal, not the noise.\nHow complex should the approximator be to faithfully capture the essence of a\nnatural, noisy, process? You can think of this question as: how many parameters 𝜃\nthe network should have, or, if the approximator is a polynomial, what the degree\n306\nB Deep Supervised Learning\nEnvironment\nModel\nRelative\nChance of\nObservations\nVariables Dimensionality\n𝑛\n>\n𝑑\nlow\nunderﬁtting, high bias\n𝑛\n<\n𝑑\nhigh\noverﬁtting, high variance\nTable B.2 Observations, Variables, Relative Dimensionality, Overﬁtting\nFig. B.2 Bias-variance trade-oﬀ; few-parameter model: high bias; many-parameter model: high\nvariance\n𝑑of the polynomial should be? The complexity of the approximator is also called\nthe capacity of the model, for the amount of information that it can contain (see\nTable B.2). When the capacity of the approximator 𝑑is lower than the number of\nobservations 𝑛, then the model is likely to underﬁt: the curve is too simple and\ncannot reach all observations well, the error is large and the accuracy is low.3\nConversely, when the number of coeﬃcients 𝑑is as high or higher than the\nnumber of observations 𝑛, then many machine learning procedures wil be able to\nﬁnd a good ﬁt. The error on the training set will be zero, and training accuracy\nreaches 100%. Will this high-capacity approximator generalize well to unseen\nstates? Most likely it will not, since the training and test observations are from\na real-world process and observations contain noise from the training set. The\ntraining noise will be modeled perfectly, and the trained function will have low\naccuracy on the test set and other datasets. A high-capacity model (𝑑> 𝑛) that\ntrains well but tests poorly is said to overﬁt on the training data.\nUnderﬁtting and overﬁtting are related to the so-called bias-variance trade-oﬀ,\nsee Fig. B.2. High-capacity models ﬁt “peaky” high-variance curves that can ﬁt\nsignal and noise of the observations, and tend to overﬁt. Low-capacity models\nﬁt “straighter” low-variance curves that have higher bias and tend to underﬁt\nthe observations. Models with a capacity of 𝑑≈𝑛can have both good bias and\nvariance [93].\n3 Overﬁtting can be reduced in the loss function and training procedure, see Sect. B.2.7.\nB.2 Deep Learning\n307\nFig. B.3 A Single Biological neuron [821]\nPreventing underﬁtting and overﬁtting is a matter of matching the capacity of our\nmodel so that 𝑑matches 𝑛. This is a delicate trade-oﬀ, since reducing the capacity\nalso reduces expressive power of the models. To reduce overﬁtting, we can use\nregularization, and many regularization methods have been devised. Regularization\nhas the eﬀect of dynamically adjusting capacity to the number of observations.\nRegularization—the World is Smooth\nGeneralization is closely related to regularization. Most functions in the real world—\nthe functions that we wish to learn in machine learning—are smooth: near similar\ninput leads to near similar output. Few real-world functions are jagged, or locally\nrandom (when they are, they are often contrived examples).\nThis has lead to the introduction of regularization methods, to restrict or smooth\nthe behavior of models. Regularization methods allow us to use high capacity models,\nlearn the complex function, and then later introduce a smoothing procedure, to\nreduce too much of the randomness and jaggedness. Regularization may for example\nrestrict the weights of variables by moving the values closer to zero, but many\nother methods exist, as we will see. Diﬀerent techniques for regularization have\nbeen developed for high-capacity deep neural models, and we discuss them in\nSect. B.2.7. The goal is to ﬁlter out random noise, but at the same time to allow\nmeaningful trends to be recognized, to smooth the function without restricting\ncomplex shapes [280].\nBefore we delve into regularization methods, let us have a look in more detail at\nhow to implement parameterized functions, for which we will use neural networks\nand deep learning.\nB.2 Deep Learning\nThe architecture of artiﬁcial neural networks is inspired by the architecture of\nbiological neural networks, such as the human brain. Neural networks consist of\n308\nB Deep Supervised Learning\n𝑥1\n𝑥2\n𝑦1\n𝑦2\nFig. B.4 A Fully Connected Shallow Neural Network with 9 Neurons\nneural core cells that are connected by nerve cells [63]. Figure B.3 shows a drawing\nof a biological neuron, with a nucleus, an axon, and dendrites [821].\nFigure B.4 shows a simple fully connected artiﬁcial neural network, with an\ninput layer of two neurons, an output layer of two neurons, and a single hidden\nlayer of ﬁve neurons. A neural network with a single hidden layer is called shallow.\nWhen the network has more hidden layers it is called deep (Fig. B.5).\nIn this section, we provide an overview of artiﬁcial neural networks and\ntheir training algorithms. We provide enough detail to understand the deep\nreinforcement learning concepts in this book. Space does provide a limit to\nhow deep we can go. Please refer to specialized deep learning literature for\nmore information, such as [280].\nWe provide a conceptual overview, which should be enough to successfully\nuse existing high quality deep learning packages, such as TensorFlowa [1]\nor PyTorch [591].b\na https://www.tensorflow.org\nb https://pytorch.org\nB.2.1 Weights, Neurons\nNeural networks consist of neurons and connections, typically organized in layers.\nThe neurons process their input signals as a weighted combination, producing\nan output signal. This calculation is called the activation function, or squashing\nfunction, since it is non-linear. Popular activation functions are the rectiﬁed linear\nunit (ReLU: partly linear, partly zero), the hyperbolic tangent, and the sigmoid\nor logistic function\n1\n1+𝑒−𝑎for neuron activation 𝑎. The neurons are connected\nB.2 Deep Learning\n309\nFig. B.5 Four fully connected hidden layers [652]\nby weights. At each neuron 𝑗the incoming weights 𝑖𝑗are summed Í and then\nprocessed by the activation function 𝜎. The output 𝑜of neuron 𝑗is therefore:\n𝑜𝑗= 𝜎(\n∑︁\n𝑖\n𝑜𝑖𝑤𝑖𝑗)\nfor weight 𝑖𝑗of predecessor neuron 𝑜𝑖. The outputs of this layer of neurons are fed\nto the inputs for the weights of the next layer.\nB.2.2 Backpropagation\nThe neural network as a whole is a parameterized function 𝑓𝜃(𝑥) →ˆ𝑦that converts\ninput to an output approximation. The behavior depends on the parameters 𝜃,\nalso known as the network weights. The parameters are adjusted such that the\nrequired input-output relation is achieved (training the network). This is done by\nminimizing the error (or loss) function that calculates the diﬀerence between the\nnetwork output ˆ𝑦and the training target 𝑦.\nThe training process consists of training epochs, individual passes in which\nthe network weights are optimized towards the target, using a method called\ngradient descent (since the goal is to minimize the error function). An epoch is one\ncomplete pass over the training data. Epochs are usually done in batches. Since\ntraining is an iterative optimization process, it is typical to train for multiple epochs.\nListing B.1 shows simpliﬁed pseudocode for the gradient descent training algorithm\n(based on [280]). When training starts, the weights of the network are initialized to\n310\nB Deep Supervised Learning\n1\ndef\ntrain_sl(data , net , alpha =0.001):\n# train\nclassifier\n2\nfor\nepoch in range(max_epochs ):\n# an epoch is one\npass\n3\nsum_sq = 0\n# reset to zero\nfor\neach\npass\n4\nfor (image , label) in data:\n5\noutput = net. forward_pass (image) # predict\n6\nsum_sq\n+= (output - label)**2 # compute\nerror\n7\ngrad = net.gradient(sum_sq)\n# derivative\nof error\n8\nnet. backward_pass (grad , alpha)\n# adjust\nweights\n9\nreturn\nnet\nListing B.1 Network training pseudocode for supervised learning\nsmall random numbers. Each epoch consists of a forward pass (recognition, usage)\nand a backward pass (training, adjustment). The forward pass is just the regular\nrecognition operation for which the network is designed. The input layer is exposed\nto the input (the image), which is then propagated through the network to the\noutput layers, using the weights and activation functions. The output layer provides\nthe answer, by having a high value at the neuron corresponding to the right label\n(such as Cat or Dog, or the correct number), so that an error can be calculated to be\nused to adjust the weights in the backward pass.\nThe listing shows a basic version of gradient descent, that calculates the gradient\nover all examples in the dataset, and then updates the weights. Batch versions of\ngradient descent update the weights after smaller subsets, and are typically quicker.\nLoss Function\nAt the output layer the propagated value ˆ𝑦is compared with the other part of\nthe example pair, the label 𝑦. The diﬀerence with the label is calculated, yielding\nthe error. The error function is also known as the loss function L. Two common\nerror functions are the mean squared error 1\n𝑛\nÍ𝑛\n𝑖(𝑦𝑖−ˆ𝑦𝑖)2 (for regression) and the\ncross-entropy error −Í𝑀\n𝑖\n𝑦𝑖log ˆ𝑦𝑖(for classiﬁcation of 𝑀classes). The backward\npass uses the diﬀerence between the forward recognition outcome and the true\nlabel to adjust the weights, so that the error becomes smaller. This method uses the\ngradient of the error function over the weights, and is called gradient descent. The\nparameters are adjusted as follows:\n𝜃𝑡+1 = 𝜃𝑡−𝛼∇𝜃𝑡L𝐷( 𝑓𝜃𝑡)\nwhere 𝜃are the network parameters, 𝑡is the optimization time step, 𝛼is the learning\nrate, ∇𝜃𝑡are the current gradient of the loss function of data L𝐷, and 𝑓𝜃is the\nparameterized objective function.\nThe training process can be stopped when the error has been reduced below a\ncertain threshold for a single example, or when the loss on an entire validation set\nB.2 Deep Learning\n311\nhas dropped suﬃciently. More elaborate stopping criteria can be used in relation to\noverﬁtting (see Sect. B.2.7).\nMost neural nets are trained using a stochastic version of gradient descent, or\nSGD [723]. SGD samples a minibatch of size smaller than the total dataset, and\nthereby computes a noisy estimate of the true gradient. This is faster per update\nstep, and does not aﬀect the direction of the gradient too much. See Goodfellow et\nal. [280] for details.\nB.2.3 End-to-end Feature Learning\nLet us now look in more detail at how neural networks can be used to implement\nend-to-end feature learning.\nWe can approximate a function through the discovery of common features\nin states. Let us, again, concentrate on image recognition. Traditionally, feature\ndiscovery was a manual process. Image-specialists would painstakingly pour over\nimages to identify common features in a dataset, such as lines, squares, circles, and\nangles, by hand. They would write small pre-processing algorithms to recognize\nthe features that were then used with classical machine learning methods such\nas decision trees, support vector machines, or principal component analysis to\nconstruct recognizers to classify an image. This hand-crafted method is a labor-\nintensive and error prone process, and researchers have worked to ﬁnd algorithms\nfor the full image recognition process, end-to-end. For this to work, also the features\nmust be learned.\nFor example, if the function approximator consists of the sum of 𝑛features, then\nwith hand-crafted features, only the coeﬃcients 𝑐𝑖of the features in the function\nℎ(𝑠) = 𝑐1 × 𝑓1(𝑠) + 𝑐2 × 𝑓2(𝑠) + 𝑐3 × 𝑓3(𝑠) + . . . + 𝑐𝑛× 𝑓𝑛(𝑠)\nare learned. In end-to-end learning, the coeﬃcients 𝑐𝑖and the features 𝑓𝑖(𝑠) are\nlearned.\nDeep end-to-end learning has achieved great success in image recognition,\nspeech recognition, and natural language processing [431, 287, 853, 193]. End-\nto-end learning is the learning of a classiﬁer directly from high-dimensional, raw,\nun-pre-processed, pixel data, all the way to the classiﬁcation layer, as opposed to\nlearning pre-processed data from intermediate (lower dimensional) hand-crafted\nfeatures.\nWe will now see in more detail how neural networks can perform automated\nfeature discovery. The hierarchy of network layers together can recognize a hier-\narchy of low-to-high level concepts [459, 462]. For example, in face recognition\n(Fig. B.6) the ﬁrst hidden layer may encode edges; the second layer then composes\nand encodes simple structures of edges; the third layer may encode higher-level\nconcepts such as noses or eyes; and the fourth layer may work at the abstraction\nlevel of a face. Deep feature learning ﬁnds what to abstract at which level on its\n312\nB Deep Supervised Learning\nFig. B.6 Layers of features of increasing complexity [462]\nFunction\nInput\nOutput\nDataset\nRegression\ncontinuous (number) continuous (number)\nClassiﬁcation\ndiscrete (image)\ndiscrete (class label)\nEnvironment Value 𝑉\nstate\ncontinuous (number)\nAction-value 𝑄state × action\ncontinuous (number)\nPolicy 𝜋\nstate\naction(-distribution)\nTable B.3 Functions that are Frequently Approximated\nown [76], and can come up with classes of intermediate concepts, that work, but\nlook counterintuitive upon inspection by humans.\nTowards the end of the 1990s the work on neural networks moved into deep\nlearning, a term coined by Dechter in [187]. LeCun et al. [461] published an inﬂu-\nential paper on deep convolutional nets. The paper introduced the architecture\nLeNet-5, a seven-layer convolutional neural net trained to classify handwritten\nMNIST digits from 32 × 32 pixel images. Listing B.2 shows a modern rendering of\nLeNet in Keras. The code straightforwardly lists the layer deﬁnitions.\nEnd-to-end learning is computationally quite demanding. After the turn of the\ncentury, methods, datasets, and compute power had improved to such an extent\nthat full raw, un-pre-processed pictures could be learned, without the intermediate\nstep of hand-crafting features. End-to-end learning proved very powerful, achieving\nhigher accuracy in image recognition than previous methods, and even higher than\nhuman test subjects [431]. In natural language processing, deep transformer models\nsuch as BERT and GPT-2 and 3 have reached equally impressive results [193, 114].\nB.2 Deep Learning\n313\n1\n2\ndef\nlenet_model(img_shape =(28 , 28, 1), n_classes =10, l2_reg =0.,\n3\nweights=None):\n4\n5\n# Initialize\nmodel\n6\nlenet = Sequential ()\n7\n8\n# 2 sets of CRP (Convolution , RELU , Pooling)\n9\nlenet.add(Conv2D (20, (5, 5), padding=\"same\",\n10\ninput_shape =img_shape , kernel_regularizer =l2(\nl2_reg)))\n11\nlenet.add(Activation (\"relu\"))\n12\nlenet.add( MaxPooling2D (pool_size =(2, 2), strides =(2, 2)))\n13\n14\nlenet.add(Conv2D (50, (5, 5), padding=\"same\",\n15\nkernel_regularizer =l2(l2_reg)))\n16\nlenet.add(Activation (\"relu\"))\n17\nlenet.add( MaxPooling2D (pool_size =(2, 2), strides =(2, 2)))\n18\n19\n# Fully\nconnected\nlayers (w/ RELU)\n20\nlenet.add(Flatten ())\n21\nlenet.add(Dense (500 ,\nkernel_regularizer =l2(l2_reg)))\n22\nlenet.add(Activation (\"relu\"))\n23\n24\n# Softmax (for\nclassification )\n25\nlenet.add(Dense(n_classes , kernel_regularizer =l2(l2_reg))\n)\n26\nlenet.add(Activation (\"softmax\"))\n27\n28\nif\nweights\nis not\nNone:\n29\nlenet. load_weights (weights)\n30\n31\n# Return\nthe\nconstructed\nnetwork\n32\nreturn\nlenet\nListing B.2 LeNet-5 code in Keras [461, 832]\nFunction Approximation\nLet us have a look at the diﬀerent kinds of functions that we wish to approximate\nin machine learning. The most basic function establishing an input/output relation\nis regression, which outputs a continuous number. Another important function is\nclassiﬁcation, which outputs a discrete number. Regression and classiﬁcation are\noften learned through supervision, with a dataset of examples (observations) and\nlabels.\nIn reinforcement learning, three functions are typically approximated: the value\nfunction 𝑉(𝑠), that relates states to their expected cumulative future rewards, the\naction-value function 𝑄(𝑠, 𝑎) that relate actions to their values, and the policy\n314\nB Deep Supervised Learning\nfunction 𝜋(𝑠) that relates states to an action (or 𝜋(𝑎|𝑠) to an action distribution).4\nIn reinforcement learning, the functions 𝑉, 𝑄, 𝜋are learned through reinforcement\nby the environment. Table B.3 summarizes these functions.\nB.2.4 Convolutional Networks\nThe ﬁrst neural networks consisted of fully connected layers (Fig. B.5). In image\nrecognition, the input layer of a neural network is typically connected directly to\nthe input image. Higher resolution images therefore need a higher number of input\nneurons. If all layers would have more neurons, then the width of the network\ngrows quickly. Unfortunately, growing a fully connected network (see Fig. B.4) by\nincreasing its width (number of neurons per layer) will increase the number of\nparameters quadratically.\nThe naive solution of high-resolution problem learning is to increase the capacity\nof the model 𝑚. However, because of the problem of overﬁtting, as 𝑚grows, so\nmust the number of examples, 𝑛.\nThe solution lies in using a sparse interconnection structure instead of a fully\nconnected network. Convolutional neural nets (CNNs) take their inspiration from\nbiology. The visual cortex in animals and humans is not fully connected, but locally\nconnected [357, 358, 503]. Convolutions eﬃciently exploit prior knowledge about\nthe structure of the data: patterns reoccur at diﬀerent locations in the data (transla-\ntion invariance), and therefore we can share parameters by moving a convolutional\nwindow over the image.\nA CNN consists of convolutional operators or ﬁlters. A typical convolution\noperator has a small receptive ﬁeld (it only connects to a limited number of neurons,\nsay 5 × 5), whereas a fully connected neuron connects to all neurons in the layer\nbelow. Convolutional ﬁlters detect the presence of local patterns. The next layer\nthus acts as a feature map. A CNN layer can be seen as a set of learnable ﬁlters,\ninvariant for local transformations [280].\nFilters can be used to identify features. Features are basic elements such as edges,\nstraight lines, round lines, curves, and colors. To work as a curve detector, for\nexample, the ﬁlter should have a pixel structure with high values indicating a shape\nof a curve. By then multiplying and adding these ﬁlter values with the pixel values,\nwe can detect whether the shape is present. The sum of the multiplications in the\ninput image will be large if there is a shape that resembles the curve in the ﬁlter.\nThis ﬁlter can only detect a certain shape of curve. Other ﬁlters can detect other\nshapes. Larger activation maps can recognize more elements in the input image.\nAdding more ﬁlters increases the size of the network, which eﬀectively enlarges\nthe activation map. The ﬁlters in the ﬁrst network layer process (“convolve”) the\ninput image and ﬁre (have high values) when a speciﬁc feature that it is built to\n4 In Chap. 5, on model-based learning, we also approximate the transition function 𝑇𝑎(·) and the\nreward function 𝑅𝑎(·).\nB.2 Deep Learning\n315\ndetect is in the input image. Training a convolutional net is training a ﬁlter that\nconsists of layers of subﬁlters.\nBy going through the convolutional layers of the network, increasingly complex\nfeatures can be represented in the activation maps. Once they are trained, they can\nbe used for as many recognition tasks as needed. A recognition task consists of a\nsingle quick forward pass through the network.\nLet us spend some more time on understanding these ﬁlters.\nShared Weights\nIn CNNs the ﬁlter parameters are shared in a layer. Each layer thus deﬁnes a ﬁlter\noperation. A ﬁlter is deﬁned by few parameters but is applied to many pixels of\nthe image; each ﬁlter is replicated across the entire visual ﬁeld. These replicated\nunits share the same parameterization (weight vector and bias) and form a feature\nmap. This means that all the neurons in a given convolutional layer respond to\nthe same feature within their speciﬁc response ﬁeld. Replicating units in this way\nallows for features to be detected regardless of their position in the visual ﬁeld, thus\nconstituting the property of translation invariance.\nThis weight sharing is also important to prevent an increase in the number of\nweights in deep and wide nets, and to prevent overﬁtting, as we shall see later.\nReal-world images consist of repetitions of many smaller elements. Due to this\nso-called translation invariance, the same patterns reappear throughout an image.\nCNNs can take advantage of this. The weights of the links are shared, resulting in a\nlarge reduction in the number of weights that have to be trained. Mathematically\nCNNs put constraints on what the weight values can be. This is a signiﬁcant\nadvantage of CNNs, since the computational requirements of training the weights\nof fully connected layers are prohibitive. In addition, statistical strength is gained,\nsince the eﬀective data per weight increases.\nDeep CNNs work well in image recognition tasks, for visual ﬁltering operations\nin spatial dependencies, and for feature recognition (edges, shapes) [460].5\nCNN Architecture\nConvolutions recognize features—the deeper the network, the more complex the\nfeatures. A typical CNN architecture consists of a number of stacked convolutional\nlayers. In the ﬁnal layers, fully connected layers are used to then classify the inputs.\nIn the convolutional layers, by connecting only locally, the number of weights\nis dramatically reduced in comparison with a fully connected net. The ability of a\nsingle neuron to recognize diﬀerent features, however, is less than that of a fully\nconnected neuron.\n5 Interestingly, this paper was already published in 1989. The deep learning revolution happened\ntwenty years later, when publicly available datasets, more eﬃcient algorithms, and more compute\npower in the form of GPUs were available.\n316\nB Deep Supervised Learning\nFig. B.7 Convolutional network example architecture [652]\nFig. B.8 Max and average 2 × 2 pooling [280]\nBy stacking many such locally connected layers on top of each other we can\nachieve the desired nonlinear ﬁlters whose joint eﬀect becomes increasingly global.6\nThe neurons become responsive to a larger region of pixel space, so that the network\nﬁrst creates representations of small parts of the input, and from these represen-\ntations create larger areas. The network can recognize and represent increasingly\ncomplex concepts without an explosion of weights.\nA typical CNN architecture consists of an architecture of multiple layers of\nconvolution, max pooling, and ReLU layers, topped oﬀby a fully connected layer\n(Fig. B.7).7\nMax Pooling\nA further method for reducing the number of weights is weight pooling. Pooling is\na kind of nonlinear downsampling (expressing the information in lower resolution\n6 Nonlinearity is essential. If all neurons performed linearly, then there would be no need for\nlayers. Linear recognition functions cannot discriminate between cats and dogs.\n7 Often with a softmax function. The softmax function normalizes an input vector of real numbers\nto a probability distribution [0, 1]; 𝑝𝜃(𝑦|𝑥) = softmax( 𝑓𝜃(𝑥)) =\n𝑒𝑓𝜃(𝑥)\nÍ\n𝑘𝑒𝑓𝜃,𝑘(𝑥)\nB.2 Deep Learning\n317\nFig. B.9 RNN 𝑥𝑡is an input vector, ℎ𝑡is the output/prediction, and 𝐴is the RNN [567]\nFig. B.10 RNN unrolled in time [567]\nwith fewer bits). Typically, a 2 × 2 block is sampled down to a scalar value (Fig. B.8).\nPooling reduces the dimension of the network. The most frequently used form is\nmax pooling. It is an important component for object detection [153] and is an\nintegral part of most CNN architectures. Max pooling also allows small translations,\nsuch as shifting the object by a few pixels, or scaling, such as putting the object\ncloser to the camera.\nB.2.5 Recurrent Networks\nImage recognition has had a large impact on network architectures, leading to\ninnovations such as convolutional nets for spatial data.\nSpeech recognition and time series analysis have also caused new architectures to\nbe created, for sequential data. Such sequences can be modeled by recurrent neural\nnets (RNN) [84, 240]. Some of the better known RNNs are Hopﬁeld networks [349],\nand long short-term memory (LSTM) [345].\nFigure B.9 shows a basic recurrent neural network. An RNN neuron is the same\nas a normal neuron, with input, output, and activation function. However, RNN\nneurons have an extra pair of looping input/output connections. Through this\nstructure, the values of the parameters in an RNN can evolve. In eﬀect, RNNs have\na variable-like state.\nTo understand how RNNs work, it helps to unroll the network, as has been done\nin Fig. B.10. The recurrent neuron loops have been drawn as a straight line to show\n318\nB Deep Supervised Learning\nFig. B.11 RNN conﬁgurations [399]\nthe network in a deeply layered style, with connections between the layers. In\nreality the layers are time steps in the processing of the recurrent connections. In a\nsense, an RNN is a deeply layered neural net folded into a single layer of recurrent\nneurons.\nWhere deep convolutional networks are successful in image classiﬁcation, RNNs\nare used for tasks with a sequential nature, such as captioning challenges. In a\ncaptioning task the network is shown a picture, and then has to come up with a\ntextual description that makes sense [816].\nThe main innovation of recurrent nets is that they allow us to work with se-\nquences of vectors. Figure B.11 shows diﬀerent combinations of sequences that we\nwill discuss now, following an accessible and well-illustrated blog on the diﬀerent\nRNN conﬁgurations written by Karpathy [399]. There can be sequences in the input,\nin the output, or in both. The ﬁgure shows diﬀerent rectangles. Each rectangle is\na vector. Arrows represent computations, such as matrix multiply. Input vectors\nare in red, output vectors are in blue, and green vectors hold the state. From left to\nright we see:\n1. One to one, the standard network without RNN. This network maps a ﬁxed-sized\ninput to ﬁxed-sized output, such as an image classiﬁcation task (picture in/class\nout).\n2. One to many adds a sequence in the output. This can be an image captioning\ntask that takes an image and outputs a sentence of words.\n3. Many to one is the opposite, with a sequence in the input. Think for example of\nsentiment analysis (a sentence is classiﬁed for words with negative or positive\nemotional meaning).\n4. Many to many has both a sequence for input and a sequence for output. This\ncan be the case in machine translation, where a sentence in English is read and\nthen a sentence in Français is produced.\n5. Many to many is a related but diﬀerent situation, with synchronized input and\noutput sequences. This can be the case in video classiﬁcation where each frame\nof the video should be labeled.\nB.2 Deep Learning\n319\nFig. B.12 LSTM [399]\nLong Short-Term Memory\nTime series prediction is more complex than conventional regression or classiﬁca-\ntion. It adds the complexity of a sequence dependence among the input variables.\nLSTM (long short-term memory) is a more powerful type of neuron designed to\nhandle sequences. Figure B.12 shows the LSTM module, allowing comparison with\na simple RNN. LSTMs are designed for sequential problems, such as time series,\nand planning. LSTMs were introduces by Hochreiter and Schmidhuber [345].\nRNN training suﬀers from the vanishing gradient problem. For short-term\nsequences this problem may be controllable by the same methods as for deep\nCNN [280]. For long-term remembering LSTM are better suited. LSTMs are fre-\nquently used to solve diverse problems [675, 286, 288, 271], and we encounter them\nat many places throughout this book.\nB.2.6 More Network Architectures\nDeep learning is a highly active ﬁeld of research, in which many advanced network\narchitectures have been developed. We will describe some of the better known\narchitectures.\nResidual Networks\nAn important innovation on CNNs is the residual network architecture, or ResNets.\nThis idea was introduced in the 2015 ImageNet challenge, which He et al. [321]\nwon with a very low error rate of 3.57%. This error rate is actually lower than what\nmost humans achieve: 5–10%. ResNet has no fewer than 152 layers.\nResidual nets introduce skip links. Skip links are connections skipping one or\nmore layers, allowing the training to go directly to other layers, reducing the\neﬀective depth of the network (Fig. B.13). Skip links create a mixture of a shallow\nand a deep network, preventing the accuracy degradation and vanishing gradients\nof deep networks [280].\n320\nB Deep Supervised Learning\nFig. B.13 Residual Net with Skip Links [321]\nGenerative Adversarial Networks\nNormally neural networks are used in forward mode, to discriminate input images\ninto classes, going from high dimensional to low dimensional. Networks can also\nbe run backwards, to generate an image that goes with a certain class, going from\nlow dimensional to high dimensional.8 Going from small to large implies many\npossibilities for the image to be instantiated. Extra input is needed to ﬁll in the\ndegrees of freedom.\nRunning the recognizers backwards, in generative mode, has created an active\nresearch area called deep generative modeling. An important type of generative\nmodel that has made quite an impact is the Generative adversarial network, or\nGAN [281].\nDeep networks are susceptible to adversarial attacks. A well-known problem of\nthe image recognition process is that it is brittle. It was found that if an image is\nslightly perturbed, and imperceptibly to the human eye, deep networks can easily\nbe fooled to characterize an image as the wrong category [747]. This brittleness is\nknown as the one-pixel problem: changing a single unimportant pixel in an image\ncould cause classiﬁers to switch from classifying an image from cat to dog [730, 747].\nGANs are used to generate input images that are slightly diﬀerent from the\noriginal input. GANs generate adversarial examples whose purpose it is to fool the\ndiscriminator (recognizer). The ﬁrst network, the generator, generates an image.\nThe second network, the discriminator, tries to recognize the image. The goal for\nthe generator is to mislead the discriminator, in order to improve the robustness of\nthe discriminator. In this way, GANs can be used to make image recognition more\nrobust. The one-pixel problem has spawned an active area of research to understand\nthis problem, and to make deep networks more robust.\nAnother use of generative networks is to generate artiﬁcial photo-realistic im-\nages known as deep fake images [857] and deep dreaming [402], see Fig. B.14 and\n8 Just like the decoding phase of autoencoders.\nB.2 Deep Learning\n321\nFig. B.14 Deep fakes [857]\nFig. B.15.9 GANs have signiﬁcantly increased our theoretical understanding of deep\nlearning.\nAutoencoders\nAutoencoders and variational autoencoders (VAE) are used in deep learning for\ndimensionality reduction, in unsupervised learning [428, 411, 412]. An autoencoder\nnetwork has a butterﬂy-like architecture, with the same number of neurons in\nthe input and the output layers, but a decreasing layer-size as we go to the center\n(Fig. B.16). The input (contracting) side is said to perform a discriminative action,\nsuch as image classiﬁcation, and the other (expanding) side is generative [281].\nWhen an image is fed to both the input and the output of the autoencoder, results in\nthe center layers are exposed to a compression/decompression process, resulting in\nthe same image, only smoothed. The discriminative/generative process performed\nby autoencoders reduces the dimensionality, and is generally thought of as going\nto the “essence” of a problem, de-noising it [342].\n9 Deep Dream Generator at https://deepdreamgenerator.com\n322\nB Deep Supervised Learning\nFig. B.15 Deep dream [402]\nFig. B.16 Autoencoder, ﬁnding the “essence” of the data in the middle [538]\nAutoencoding illustrates a deep relation between supervised and unsupervised\nlearning. The architecture of an autoencoders consists of an encoder and a decoder.\nThe encoder is a regular discriminative network, as is common in supervised learn-\ning. The decoder is a generative network, creating high dimensional output from low\ndimensional input. Together, the encoder/decoder butterﬂy perform dimensionality\nreduction, or compression, a form of unsupervised learning[294].\nAutoencoders and generative networks are an active area of research.\nB.2 Deep Learning\n323\nFig. B.17 Attention Architecture [47, 490]\nFig. B.18 Example for Attention Mechanism [490]\nAttention Mechanism\nAnother important architecture is the attention mechanism. Sequence to sequence\nlearning occurs in many applications of machine learning, for example, in machine\ntranslation. The dimensionality of the output of basic RNNs is the same as the input.\nThe attention architecture allows a ﬂexible mapping between input and output\ndimensionality [739]. It does so by augmenting the RNNs with an extra network that\nfocuses attention over the sequence of encoder RNN states [47], see Fig. B.17. The\nextra network focuses perception and memory access. It has been shown to achieve\nstate of the art results in machine translation and natural language tasks [47], see\nFig. B.18. The attention mechanism is especially useful for time series forecasting\nand translation.\n324\nB Deep Supervised Learning\nTransformers\nFinally, we discuss the transformer architecture. The transformer architecture is\nintroduced by Vaswani et al. [805]. Transformers are based on the concept of self-\nattention: they use attention encoder-decoder models, but weigh the inﬂuence of\ndiﬀerent parts of the data. They are central to the highly successful BERT [193]\nand GPT-3 [617, 114] natural language models, but have also been successful in\nimaginative text-to-image creation.\nTransformer-based foundation models play an important role in multi-modal\n(text/image) learning [625, 616], such as the one that we saw in Fig. 10.1. Treating\ndeep reinforcement learning as a sequence problem, transformers are also being\napplied to reinforcement learning, with some early success [140, 377]. Transformers\nare an active area of research. For a detailed explanation of how they work, see, for\nexample [12, 94, 805].\nB.2.7 Overﬁtting\nNow that we have discussed network architectures, it is time to discuss an important\nproblem of deep neural networks: overﬁtting, and how we can reduce it. Overﬁtting\nin neural networks can be reduced in a number of ways. Some of the methods are\naimed at restoring the balance between the number of network parameters and the\nnumber of training examples, others on data augmentation and capacity reduction.\nAnother approach is to look at the training process. Let us list the most popular\napproaches [280].\n•\nData Augmentation Overﬁtting occurs when there are more parameters in the\nnetwork than examples to train on. The training dataset is increased through\nmanipulations such as rotations, reﬂections, noise, rescaling, etc. A disadvantage\nof this method is that the computational cost of training increases.\n•\nCapacity Reduction Another solution to overﬁtting lies in the realization that\noverﬁtting is a result of the network having too large a capacity; the network\nhas too many parameters. A cheap way of preventing this situation is to reduce\nthe capacity of the network, by reducing the width and depth of the network.\n•\nDropout A popular method to reduce overﬁtting is to introduce dropout layers\ninto the networks. Dropout reduces the eﬀective capacity of the network by\nstochastically dropping a certain percentage of neurons from the backpropaga-\ntion process [344, 725]. Dropout is an eﬀective and computationally eﬃcient\nmethod to reduce overﬁtting [280].\n•\nL1 and L2 Regularization Regularization involves adding an extra term to the\nloss function that forces the network to not be too complex. The term penalizes\nthe model for using too high weight values. This limits ﬂexibility, but also\nencourages building solutions based on multiple features. Two popular versions\nof this method are L1 and L2 regularization [559, 280].\nB.3 Datasets and Software\n325\n•\nEarly Stopping Early stopping is based on the observation that overﬁtting can be\nregarded as a consequence of so-called overtraining (training that progresses\nbeyond the signal, into the noise). By terminating the training process earlier,\nfor example by using a higher stopping threshold for the error function, we can\nprevent overﬁtting from occurring [130, 607, 608]. A convenient and popular\nway is to add a third set to the training set/test set duo which then becomes a\ntraining set, a test set, and a holdout validation set. After each training epoch,\nthe network is evaluated against the holdout validation set, to see if under- or\noverﬁtting occurs, and if we should stop training. In this way, overﬁtting can be\nprevented dynamically during training [608, 93, 280].\n•\nBatch Normalization Another method is batch normalization. Batch normaliza-\ntion periodically normalizes the input to the layers [370]. This has many beneﬁts,\nincluding a reduction of overﬁtting.\nOverﬁtting and regularization are an important topic of research. In fact, a basic\nquestion is why large neural networks perform so well at all. The network capacity\nis in the millions to billions of parameters, much larger than the number of obser-\nvations, yet networks perform well. There appears to be a regime, beyond where\nperformance suﬀers due to overﬁtting, where performance increases as we continue\nto increase the capacity of our networks [68, 66]. Belkin et al. have performed stud-\nies on interpolation in SGD. Their studies suggests an implicit regularization regime\nof many parameters beyond overﬁtting, where SGD generalizes well to test data,\nexplaining in part the good results in practice of deep learning. Nakkiran et al. report\nsimilar experimental results, termed the double descent phenomenon [550, 392].\nResearch into the nature of overﬁtting is active [68, 491, 66, 67, 867].\nB.3 Datasets and Software\nWe have discussed in depth background concepts in machine learning, and impor-\ntant aspects of the theory of neural networks. It is time to turn our attention to\npractical matters. Here we encounter a rich ﬁeld of data, environments, software\nand blogs on how to use deep learning in practice.\nThe ﬁeld of deep reinforcement learning is an open ﬁeld. Researchers release\ntheir algorithms and code allowing replication of results. Datasets and trained\nnetworks are shared. The barrier to entry is low: high-quality software is available\non GitHub for you to download and start doing research in the ﬁeld. We point to\ncode bases and open software suites at GitHub throughout the book, Appendix C\nhas pointers to software environments and open-source code frameworks.\nThe most popular deep learning packages are PyTorch [591] and TensorFlow [1],\nand its top-level language Keras [147]. Some machine learning and mathematical\npackages also oﬀer deep learning tools, such as scikit-learn, MATLAB and R. In\nthis section we will start with some easy classiﬁcation and behavior examples,\nusing the Keras library. We will use Python as our programming language. Python\nhas become the language of choice for machine learning packages; not just for\n326\nB Deep Supervised Learning\nFig. B.19 Some MNIST images [461]\nPyTorch and TensorFlow, but also for numpy, scipy, scikit-learn, matplotlib, and\nmany other mature and high-quality machine learning libraries. If Python is not\npresent on your computer, or if you want to download a newer version, please go to\nhttps://www.python.org to install it. Note that due to some unfortunate version\nissues of the Stable Baselines and TensorFlow, you may have to install diﬀerent\nversions of Python to get these examples to work, and to use virtual environments\nto manage your software versions.\nSince deep learning is very computationally intensive, these packages typically\nsupport GPU parallelism, which can speedup your training tenfold or more, if you\nhave the right GPU card in your system. Cloud providers of computing, such as\nAWS, Google, Azure, and others, also typically provide modern GPU hardware to\nsupport machine learning, often with student discounts.\nB.3.1 MNIST and ImageNet\nOne of the most important elements for the success in image recognition was the\navailability of good datasets.\nIn the early days of deep learning, the ﬁeld beneﬁted greatly from eﬀorts in\nhandwriting recognition. This application was of great value to the postal service,\nwhere accurate recognition of handwritten zip codes or postal codes allowed great\nimprovements to eﬃcient sorting and delivery of the mail. A standard test set for\nhandwriting recognition was MNIST (for Modiﬁed National Institute of Standards\nand Technology) [461]. Standard MNIST images are low-resolution 32 × 32 pixel\nimages of single handwritten digits (Fig. B.19). Of course, researchers wanted to\nprocess more complex scenes than single digits, and higher-resolution images.\nTo achieve higher accuracy, and to process more complex scenes, networks (and\ndatasets) needed to grow in size and complexity.\nB.3 Datasets and Software\n327\nImageNet\nA major dataset in deep learning is ImageNet [237, 191]. It is a collection of more\nthan 14 million URLs of images that have been hand annotated with the objects\nthat are in the picture. It contains more than 20,000 categories. A typical category\ncontains several hundred training images.\nThe importance of ImageNet for the progress in deep learning is large. The avail-\nability of an accepted standard set of labeled images allowed learning algorithms to\nbe tested and improved, and new algorithms to be created. ImageNet was conceived\nby Fei-Fei Li et al. in 2006, and in later years she developed it further with her group.\nSince 2010 an annual software contest has been organized, the ImageNet Large\nScale Visual Recognition Challenge (ILSVRC) [191]. Since 2012 ILSVRC has been\nwon by deep networks, starting the deep learning boom. The network architecture\nthat won this challenge in that year has become known as AlexNet, after one of its\nauthors [431].\nThe 2012 ImageNet database as used by AlexNet has 14 million labeled images.\nThe network featured a highly optimized 2D two-GPU implementation of 5 convolu-\ntional layers and 3 fully connected layers. The ﬁlters in the convolutional layers are\n11×11 in size. The neurons use a ReLU activation function. In AlexNet images were\nscaled to 256 × 256 RGB pixels. The size of the network was large, with 60 million\nparameters. This causes considerable overﬁtting. AlexNet used data augmentation\nand dropouts to reduce the impact of overﬁtting.\nKrizhevsky et al. won the 2012 ImageNet competition with an error rate of 15%,\nsigniﬁcantly better than the number two, who achieved 26%. Although there were\nearlier reports of CNNs that were successful in applications such as bioinformat-\nics and Chinese handwriting recognition, it was this win of the 2012 ImageNet\ncompetition for which AlexNet has become well known.\nB.3.2 GPU Implementations\nThe deep learning breakthrough around 2012 was caused by the co-occurrence of\nthree major developments: (1) algorithmic advances that solved key problems in\ndeep learning, (2) the availability of large datasets of labeled training data, and (3)\nthe availability of computational power in the form of graphical processing units,\nGPUs.\nThe most expensive operations in image processing and neural network training\nare operations on matrices. Matrix operations are some of the most well-studied\nproblems in computer science. Their algorithmic structure is well understood, and\nfor basic linear algebra operations high-performance parallel implementations for\nCPU exist, such as the BLAS [202, 145].\nGPUs were originally designed for smooth graphics performance in video games.\nGraphical processing requires fast linear algebra computations such as matrix\nmultiply. These are precisely the kind of operations that are at the core of deep\n328\nB Deep Supervised Learning\nFig. B.20 SIMD: Connection Machine 1 and GPU\nlearning training algorithms. Modern GPUs consist of thousands of small arithmetic\nunits that are capable of performing linear algebra matrix operations very fast in\nparallel. This kind of data parallel processing is based on SIMD computing, for single-\ninstruction-multiple-data [249, 330]. SIMD data parallism goes back to designs from\n1960s and 1970s vector supercomputers such as the Connection Machine series\nfrom Thinking Machines [339, 340, 466]. Figure B.20 shows a picture of the historic\nCM-1, and of a modern GPU.\nModern GPUs consist of thousands of processing units optimized to process linear\nalgebra matrix operations in parallel [656, 486, 721], oﬀering matrix performance\nthat is orders of magnitude faster than CPUs [566, 152, 746].\nB.3.3 Hands On: Classiﬁcation Example\nIt is high time to try out some of the material in practice. Let us see if we can do\nsome image recognition ourselves.\nInstalling TensorFlow and Keras\nWe will ﬁrst install TensorFlow. It is possible to run TensorFlow in the cloud, in\nColab, or in a Docker container. Links to ready to run Colab environments are on\nthe TensorFlow website. We will, however, asssume a traditional local installation\non your own computer. All major operating systems are supported: Linux/Ubuntu,\nmacOS, Windows.\nThe programming model of TensorFlow is complex, and not very user friendly.\nFortunately, an easy to use language has been built on top of TensorFlow: Keras.\nB.3 Datasets and Software\n329\nThe Keras language is easy to use, well-documented, and many examples exist to\nget you started. When you install TensorFlow, Keras is installed automatically as\nwell.\nTo install TensorFlow and Keras go to the TensorFlow page on https://www.\ntensorflow.org. It is recommended to make a virtual environment to isolate the\npackage installation from the rest of your system. This is achieved by typing\npython3 -m venv --system-site-packages ./venv\n(or equivalent) to create the virtual environment. Using the virtual environment\nrequires activation:10\nsource ./venv/bin/activate\nYou will most likely run into version issues when installing packages. Note that\nfor deep reinforcement learning we will be making extensive use of reinforcement\nlearning agent algorithms from the so-called Stable Baselines.11 The stable baselines\nwork with version 1 of TensorFlow and with PyTorch, but, as of this writing, not\nwith version 2 of TensorFlow. TensorFlow version 1.14 has been tested to work.12\nInstalling is easy with Python’s pip package manager: just type\npip install tensorflow==1.14\nor, for GPU support,\npip install tensorflow-gpu==1.14\nThis should now download and install TensorFlow and Keras. We will check if\neverything is working by executing the MNIST training example with the default\ntraining dataset.\n10 More installation guidance can be found on the TensorFlow page at https://www.tensorflow.\norg/install/pip.\n11 https://github.com/hill-a/stable-baselines\n12 You may be surprised that so many version numbers were mentioned. Unfortunately not\nall versions are compatible; some care is necessary to get the software to work: Python 3.7.9,\nTensorFlow 1.14.0, Stable Baselines 2, pyglet 1.5.11 worked at the time of writing. This slightly\nembarrassing situation is because the ﬁeld of deep reinforcement learning is driven by a community\nof researchers, who collaborate to make code bases work. When new insights trigger a rewrite that\nloses backward compatibility, as happened with TensorFlow 2.0, frantic rewriting of dependent\nsoftware occurs. The ﬁeld is still a new ﬁeld, and some instability of software packages will remain\nwith us for the foreseeable future.\n330\nB Deep Supervised Learning\n1\nfrom\ntensorflow.keras.models\nimport\nSequential\n2\nfrom\ntensorflow.keras.layers\nimport\nDense\n3\n4\nmodel = Sequential ()\n5\n6\nmodel.add(Dense(units =64,\nactivation =’relu ’))\n7\nmodel.add(Dense(units =10,\nactivation =’softmax ’))\n8\n9\nmodel.compile(loss=’categorical_crossentropy ’,\n10\noptimizer=’sgd ’,\n11\nmetrics =[’accuracy ’])\n12\n13\n# x_train\nand\ny_train\nare\nNumpy\narrays\n--just\nlike in the Scikit -\nLearn\nAPI.\n14\nmodel.fit(x_train , y_train , epochs =5, batch_size =32)\n15\n16\n# evaluation\non test\ndata is simple\n17\nloss_and_metrics = model.evaluate(x_test , y_test , batch_size =128)\n18\n19\n# as is\npredicting\noutput\n20\nclasses = model.predict(x_test , batch_size =128)\nListing B.3 Sequential Model in Keras\nKeras MNIST Example\nKeras is built on top of TensorFlow, and is installed with TensorFlow. Basic Keras\nmirrors the familiar Scikit-learn interface [595].\nEach Keras program speciﬁes a model that is to be learned. The model is the\nneural network, that consists of weights and layers (of neurons). You will specify the\narchitecure of the model in Keras, and then ﬁt the model on the training data. When\nthe model is trained, you can evaluate the loss function on test data, or perform\npredictions of the outcome based on some test input example.\nKeras has two main programming paradigms: sequential and functional. List-\ning B.3 shows the most basic Sequential Keras model, from the Keras documentation,\nwith a two-layer model, a ReLU layer, and a softmax layer, using simple SGD for\nbackpropagation. The Sequential model in Keras has an object-oriented syntax.\nThe Keras documentation is at https://keras.io/getting_started/intro_\nto_keras_for_researchers/. It is quite accessible, and you are encouraged to\nlearn Keras by working through the online tutorials.\nA slightly more useful example is ﬁtting a model on MNIST, see Listing B.4. This\nexample uses a more ﬂexible Keras syntax, the functional API, in which transforma-\ntions are chained on top of the previous layers. This example of Keras code loads\nMNIST images in a training set and a test set, and creates a model of dense ReLU\nlayers using the Functional API. It then creates a Keras model of these layers, and\nprints a summary of the model. Then the model is trained from numpy data, with\nB.3 Datasets and Software\n331\n1\n# Get the\ndata as Numpy\narrays\n2\n(x_train , y_train), (x_test , y_test) = keras.datasets.mnist.\nload_data ()\n3\n4\n# Build a simple\nmodel\n5\ninputs = keras.Input(shape =(28 , 28))\n6\nx = layers. experimental . preprocessing .Rescaling (1.0 / 255)(inputs\n)\n7\nx = layers.Flatten ()(x)\n8\nx = layers.Dense (128 ,\nactivation=\"relu\")(x)\n9\nx = layers.Dense (128 ,\nactivation=\"relu\")(x)\n10\noutputs = layers.Dense (10,\nactivation =\"softmax\")(x)\n11\nmodel = keras.Model(inputs , outputs)\n12\nmodel.summary ()\n13\n14\n# Compile\nthe\nmodel\n15\nmodel.compile(optimizer=\"adam\", loss=\"\nsparse_categorical_crossentropy \")\n16\n17\n# Train\nthe\nmodel\nfor 1 epoch\nfrom\nNumpy\ndata\n18\nbatch_size = 64\n19\nprint(\"Fit␣on␣NumPy␣data\")\n20\nhistory = model.fit(x_train , y_train , batch_size =batch_size ,\nepochs =1)\n21\n22\n# Train\nthe\nmodel\nfor 1 epoch\nusing a dataset\n23\ndataset = tf.data.Dataset. from_tensor_slices (( x_train , y_train)).\nbatch(batch_size)\n24\nprint(\"Fit␣on␣Dataset\")\n25\nhistory = model.fit(dataset , epochs =1)\nListing B.4 Functional MNIST Model in Keras\nthe ﬁt function, for a single epoch, and also from a dataset, and the loss history is\nprinted.\nThe code of the example shows how close Keras is to the way in which we\nthink and reason about neural networks. The examples only show the briefest of\nglimpses to what is possible in Keras. Keras has options for performance monitoring,\nfor checkpointing of long training runs, and for interfacing with TensorBoard, to\nvisualize the training process. TensorBoard is an indispensible tool, allowing you\nto debug your intuition of what should be going on in your network, and what is\ngoing on.\nDeep reinforcement learning is still very much a ﬁeld with more degrees of\nfreedom in experimentation than established practices, and being able to plot the\nprogress of training processes is essential for a better understanding of how the\nmodel behaves. The more you explore Keras, the better you will be able to progress\nin deep reinforcement learning [270].\n332\nB Deep Supervised Learning\nExercises\nBelow are some questions to check your understanding of deep learning. Each\nquestion is a closed question where a simple, single sentence answer is expected.\nQuestions\n1. Datasets are often split into two sub-datasets in machine learning. Which are\nthose two, and why are they split?\n2. What is generalization?\n3. Sometimes a third sub-dataset is used. What is it called and what is it used for?\n4. If we consider observations and model parameters, when is a problem high-\ndimensional and when is it low-dimensional?\n5. How do we measure the capacity of a machine learning model?\n6. What is a danger with low capacity models?\n7. What is a danger with high capacity models?\n8. What is the eﬀect of overﬁtting on generalization?\n9. What is the diﬀerence between supervised learning and reinforcement learning?\n10. What is the diﬀerence between a shallow network and a deep network?\n11. Supervised learning has a model and a dataset, reinforcement learning has which\ntwo central concepts?\n12. What phases does a learning epoch have? What happens in each phase?\n13. Name three factors that were essential for the deep learning breakthrough, and\nwhy.\n14. What is end-to-end learning? Do you know an alternative? What are the advan-\ntages of each?\n15. What is underﬁtting, and what causes it? What is overﬁtting, and what causes\nit? How can you see if you have overﬁtting?\n16. Name three ways to prevent overﬁtting.\n17. Which three types of layers does a neural network have?\n18. How many hidden layers does a shallow neural network have?\n19. Describe how adjusting weights works in a neural network. Hint: think of a\nexamples, labels, forward phase, a backward phase, error functions, gradients.\n20. What is the diﬀerence between a fully connected network and a convolutional\nneural network?\n21. What is max pooling?\n22. Why are shared weights advantageous?\n23. What is feature learning?\n24. What is representation learning?\n25. What is deep learning?\n26. What is an advantage of convolutional neural networks of fully connected neural\nnetworks?\n27. Name two well-known image recognition data sets.\nB.3 Datasets and Software\n333\nExercises\nLet us now start with some exercises. If you have not done so already, install\nPyTorch13 or TensorFlow and Keras (see Sect. 2.2.4.1 or go to the TensorFlow\npage).14 Be sure to check the right versions of Python, TensorFlow, and the Stable\nBaselines to make sure they work well together. The exercises below are meant to\nbe done in Keras.\n1. Generalization Install Keras. Go to the Keras MNIST example. Perform a clas-\nsiﬁcation task. Note how many epochs the training takes, and in testing, how\nwell it generalizes. Perform the classiﬁcation on a smaller training set, how does\nlearning rate change, how does generalization change. Vary other elements: try a\ndiﬀerent optimizer than adam, try a diﬀerent learning rate, try a diﬀerent (deeper)\narchitecture, try wider hidden layers. Does it learn faster? Does it generalize\nbetter?\n2. Overﬁtting Use Keras again, but this time on ImageNet. Now try diﬀerent over-\nﬁtting solutions. Does the training speed change? Does generalization change?\nNow try the hold-out validation set. Do training and generalization change?\n3. Conﬁdence How many runs did you do in the previous exercises, just a single\nrun to see how long training took and how well generalization worked? Try\nto run it again. Do you get the same results? How large is the diﬀerence? Can\nyou change the random seeds of Keras or TensorFlow? Can you calculate the\nconﬁdence interval, how much does the conﬁdence improve when you do 10\nrandomized runs? How about 100 runs? Make graphs with error bars.\n4. GPU It might be that you have access to a GPU machine that is capable of\nrunning PyTorch or TensorFlow in parallel to speed up the training. Install the\nGPU version and check that it recognizes the GPU and is indeed using it.\n5. Parallelism It might be that you have access to a multicore CPU machine. When\nyou are running multiple runs in order to improve conﬁedence, then an easy way\nto speed up your experiment is to spawn multiple jobs at the shell, assigning the\noutput to diﬀerent log ﬁles, and write a script to combine results and draw graphs.\nWrite the scripts necessary to achieve this, test them, and do a large-conﬁdence\nexperiment.\n13 https://pytorch.org\n14 https://www.tensorflow.org\nAppendix C\nDeep Reinforcement Learning Suites\nDeep reinforcement learning is a highly active ﬁeld of research. One reason for\nthe progress is the availability of high quality algorithms and code: high quality\nenvironments, algorithms and deep learning suites are all being made available by\nresearchers along with their research papers. This appendix provides pointers to\nthese codes.\n335\n336\nC Deep Reinforcement Learning Suites\nC.1 Environments\nProgress has beneﬁted greatly from the availability of high quality environments, on\nwhich the algorithms can be tested. We provide pointers to some of the environments\n(Table C.1).\nEnvironment Type\nURL\nRef\nGym\nALE, MuJoCo\nhttps://gym.openai.com\n[108]\nALE\nAtari\nhttps://github.com/mgbellemare/Arcade-\nLearning-Environment\n[71]\nMuJoCo\nSimulated Robot\nhttp://www.mujoco.org\n[780]\nDeepMind Lab 3D navigation\nhttps://github.com/deepmind/lab\n[64]\nControl Suite\nPhysics tasks\nhttps://github.com/deepmind/dm_control\n[755]\nBehavior Suite Core RL\nhttps://github.com/deepmind/bsuite\n[575]\nStarCraft\nPython interface\nhttps://github.com/deepmind/pysc2\n[815]\nOmniglot\nImages\nhttps://github.com/brendenlake/omniglot\n[445]\nMini ImageNet Images\nhttps://github.com/yaoyao-liu/mini-imagenet-\ntools\n[814]\nProcGen\nProcedurally Gen. https://openai.com/blog/procgen-benchmark/\n[158]\nOpenSpiel\nBoard Games\nhttps://github.com/deepmind/open_spiel\n[450]\nRLlib\nScalable RL\nhttps://github.com/ray-project/ray\n[478]\nMeta Dataset\ndataset of datas.\nhttps://github.com/google-research/meta-\ndataset\n[785]\nMeta World\nmeta RL\nhttps://meta-world.github.io\n[864]\nAlchemy\nmeta RL\nhttps://github.com/deepmind/dm_alchemy\n[825]\nGarage\nreproducible RL\nhttps://github.com/rlworkgroup/garage\n[261]\nFootball\nmulti agent\nhttps://github.com/google-research/football\n[438]\nEmergence\nHide and Seek\nhttps://github.com/openai/multi-agent-\nemergence-environments\n[49]\nUnplugged\nOﬄine RL\nhttps://github.com/deepmind/deepmind-\nresearch/tree/master/rl_unplugged\n[299]\nUnity\n3D\nhttps://github.com/Unity-Technologies/ml-\nagents\n[383]\nPolyGames\nboard games\nhttps://github.com/teytaud/Polygames\n[133]\nDopamine\nRL framework\nhttps://github.com/google/dopamine\n[131]\nTable C.1 Reinforcement Learning Environments\nC.2 Agent Algorithms\n337\nC.2 Agent Algorithms\nFor value-based and policy-based methods most mainstream algorithms have been\ncollected and are freely available. For two-agent, multi-agent, hierarchical, and meta\nlearning, the agent algorithms are also on GitHub, but not always in the same place\nas the basic algorithms. Table C.2 provides pointers.\nRepo\nURL\nAlgorithms\nRef\nSpinning Up https://spinningup.openai.com\nTutorial on DDPG, PPO, etc\nOpenAI\nBaselines\nhttps://github.com/openai/\nbaselines\nDQN, PPO, etc\n[194]\nStable Basel. https://stable-baselines.\nreadthedocs.io/en/master/\nRefactored baselines\n[338]\nPlaNet\nhttps://planetrl.github.io\nLatent Model\n[309]\nDreamer\nhttps://github.com/danijar/\ndreamerv2\nLatent Model\n[310]\nVPN\nhttps://github.com/junhyukoh/\nvalue-prediction-network\nValue Prediction Network\n[564]\nMuZero\nhttps://github.com/kaesve/muzero\nReimplementation of MuZero\n[186]\nMCTS\npip install mcts\nMCTS\n[117]\nAlphaZ. Gen https://github.com/suragnair/\nalpha-zero-general\nAZ in Python\n[768]\nELF\nhttps://github.com/\nfacebookresearch/ELF\nFramework for Game research\n[776]\nPolyGames\nhttps://github.com/teytaud/\nPolygames\nZero Learning\n[133]\nCFR\nhttps://github.com/bakanaouji/\ncpp-cfr\nCFR\n[881]\nDeepCFR\nhttps://github.com/\nEricSteinberger/Deep-CFR\nDeep CFR\n[726]\nMADDPG\nhttps://github.com/openai/maddpg\nMulti-agent DDPG\n[489]\nPBT\nhttps://github.com/voiler/\nPopulationBasedTraining\nPopulation Based Training\n[374]\nGo-Explore\nhttps://github.com/uber-\nresearch/go-explore\nGo Explore\n[222]\nMAML\nhttps://github.com/cbfinn/maml\nMAML\n[243]\nTable C.2 Agent Algorithms\n338\nC Deep Reinforcement Learning Suites\nC.3 Deep Learning Suites\nThe two most well-known deep learning suites are TensorFlow and PyTorch. Base-\nTensorFlow has a complicated programming model. Keras has been developed as\nan easy to use layer on top of TensorFlow. When you use TensorFlow, start with\nKeras. Or use PyTorch.\nTensorFlow and Keras are at https://www.tensorflow.org.\nPyTorch is at https://pytorch.org.\nReferences\n1. Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey Dean, Matthieu\nDevin, Sanjay Ghemawat, Geoﬀrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,\nRajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay\nVasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorﬂow: A\nsystem for large-scale machine learning. In 12th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 16), pages 265–283, 2016. 308, 325\n2. Pieter Abbeel, Adam Coates, and Andrew Y Ng. Autonomous helicopter aerobatics through\napprenticeship learning. The International Journal of Robotics Research, 29(13):1608–1639,\n2010. 6\n3. Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y Ng. An application of reinforce-\nment learning to aerobatic helicopter ﬂight. In Advances in Neural Information Processing\nSystems, pages 1–8, 2007. 5, 6\n4. Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and\nMartin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920,\n2018. 174\n5. Abhishek. Multi-arm bandits: a potential alternative to a/b tests https://medium.com/\nbrillio-data-science/multi-arm-bandits-a-potential-alternative-to-a-b-\ntests-a647d9bf2a7e, 2019. 52\n6. Bruce Abramson. Expected-outcome: A general model of static evaluation. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 12(2):182–193, 1990. 167\n7. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Belle-\nmare. Deep reinforcement learning at the edge of the statistical precipice. arXiv preprint\narXiv:2108.13264, 2021. 119\n8. Pulkit Agrawal, Ross Girshick, and Jitendra Malik. Analyzing the performance of multilayer\nneural networks for object recognition. In European Conference on Computer Vision, pages\n329–344. Springer, 2014. 249\n9. Sanjeevan Ahilan and Peter Dayan. Feudal multi-agent hierarchies for cooperative reinforce-\nment learning. arXiv preprint arXiv:1901.08492, 2019. 237\n10. Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding\nfor attribute-based classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 819–826, 2013. 265, 272, 273, 280\n11. Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:\nA next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2623–2631,\n2019. 263\n12. Jay Alammer. The illustrated transformer. https://jalammar.github.io/illustrated-\ntransformer/. 324\n339\n340\nReferences\n13. Stefano Albrecht and Peter Stone. Multiagent learning: foundations and recent trends. In\nTutorial at IJCAI-17 conference, 2017. 226\n14. Stefano Albrecht and Peter Stone. Autonomous agents modelling other agents: A compre-\nhensive survey and open problems. Artiﬁcial Intelligence, 258:66–95, 2018. 212, 226\n15. Ethem Alpaydin. Introduction to Machine Learning. MIT press, 2009. 40, 45\n16. Safa Alver. The option-critic architecture. https://alversafa.github.io/blog/2018/\n11/28/optncrtc.html, 2018. 233\n17. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.\nConcrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016. 272\n18. Ankesh Anand, Jacob Walker, Yazhe Li, Eszter Vértes, Julian Schrittwieser, Sherjil Ozair,\nThéophane Weber, and Jessica B Hamrick. Procedural generalization by planning with\nself-supervised world models. arXiv preprint arXiv:2111.01587, 2021. 140, 145\n19. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,\nBob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.\nIn Advances in Neural Information Processing Systems, pages 5048–5058, 2017. 235, 237, 245\n20. Anonymous. Go AI strength vs. time. Reddit post, 2017. 158\n21. Thomas Anthony, Tom Eccles, Andrea Tacchetti, János Kramár, Ian M. Gemp, Thomas C.\nHudson, Nicolas Porcel, Marc Lanctot, Julien Pérolat, Richard Everett, Satinder Singh, Thore\nGraepel, and Yoram Bachrach. Learning to play no-press diplomacy with best response\npolicy iteration. In Advances in Neural Information Processing Systems, 2020. 212, 227\n22. Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning\nand tree search. In Advances in Neural Information Processing Systems, pages 5360–5370,\n2017. 148, 183\n23. Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. arXiv\npreprint arXiv:1810.09502, 2018. 272\n24. Grigoris Antoniou and Frank Van Harmelen. A Semantic Web Primer. MIT press Cambridge,\nMA, 2008. 9\n25. Oleg Arenz. Monte Carlo Chess. Master’s thesis, Universität Darmstadt, 2012. 192\n26. Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning.\nIn Advances in Neural Information Processing Systems, pages 41–48, 2007. 255\n27. Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep\nreinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26–38, 2017.\n25, 28, 62, 90\n28. John Asmuth, Lihong Li, Michael L Littman, Ali Nouri, and David Wingate. A Bayesian\nsampling approach to exploration in reinforcement learning. In Proceedings of the Twenty-\nFifth Conference on Uncertainty in Artiﬁcial Intelligence, pages 19–26. AUAI Press, 2009.\n282\n29. Arthur Aubret, Laetitia Matignon, and Salima Hassas. A survey on intrinsic motivation in\nreinforcement learning. arXiv preprint arXiv:1908.06976, 2019. 241, 242, 245, 281\n30. Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-oﬀs. Journal of\nMachine Learning Research, 3(Nov):397–422, 2002. 51, 172\n31. Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed\nbandit problem. Machine Learning, 47(2-3):235–256, 2002. 172, 174\n32. Peter Auer and Ronald Ortner. UCB revisited: Improved regret bounds for the stochastic\nmulti-armed bandit problem. Periodica Mathematica Hungarica, 61(1-2):55–65, 2010. 172\n33. Robert Axelrod. An evolutionary approach to norms. The American Political Science Review,\npages 1095–1111, 1986. 227\n34. Robert Axelrod.\nThe complexity of cooperation: Agent-based models of competition and\ncollaboration, volume 3. Princeton university press, 1997. 213, 220, 227\n35. Robert Axelrod. The dissemination of culture: A model with local convergence and global\npolarization. Journal of Conﬂict Resolution, 41(2):203–226, 1997. 227\n36. Robert Axelrod and Douglas Dion.\nThe further evolution of cooperation.\nScience,\n242(4884):1385–1390, 1988. 204\n37. Robert Axelrod and William D Hamilton.\nThe evolution of cooperation.\nScience,\n211(4489):1390–1396, 1981. 204, 227\nReferences\n341\n38. Kamyar Azizzadenesheli, Brandon Yang, Weitang Liu, Emma Brunskill, Zachary C Lipton,\nand Animashree Anandkumar. Surprising negative results for generative adversarial tree\nsearch. arXiv preprint arXiv:1806.05780, 2018. 148\n39. Mohammad Babaeizadeh, Mohammad Taghi Saﬀar, Danijar Hafner, Harini Kannan, Chelsea\nFinn, Sergey Levine, and Dumitru Erhan. Models, pixels, and rewards: Evaluating design\ntrade-oﬀs in visual model-based reinforcement learning. arXiv preprint arXiv:2012.04603,\n2020. 145\n40. Thomas Bäck. Evolutionary Algorithms in Theory and Practice: Evolutionary Strategies,\nEvolutionary Programming, Genetic Algorithms. Oxford University Press, 1996. 213, 214, 227\n41. Thomas Bäck, David B Fogel, and Zbigniew Michalewicz. Handbook of evolutionary compu-\ntation. Release, 97(1):B1, 1997. 227\n42. Thomas Bäck, Frank Hoﬀmeister, and Hans-Paul Schwefel. A survey of evolution strategies.\nIn Proceedings of the fourth International Conference on Genetic Algorithms, 1991. 206, 227\n43. Thomas Bäck and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter\noptimization. Evolutionary Computation, 1(1):1–23, 1993. 11, 227\n44. Christer Backstrom and Peter Jonsson. Planning with abstraction hierarchies can be expo-\nnentially less eﬃcient. In Proceedings of the 14th International Joint Conference on Artiﬁcial\nIntelligence, volume 2, pages 1599–1604, 1995. 232, 236\n45. Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, volume 31, 2017. 235, 237, 238, 245\n46. Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvit-\nskyi, Daniel Guo, and Charles Blundell. Agent57: Outperforming the Atari human benchmark.\narXiv preprint arXiv:2003.13350, 2020. 89\n47. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. 323\n48. Leemon Baird. Residual algorithms: Reinforcement learning with function approximation.\nIn Machine Learning Proceedings 1995, pages 30–37. Elsevier, 1995. 70, 77\n49. Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew,\nand Igor Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint\narXiv:1909.07528, 2019. 218, 220, 221, 336\n50. Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent\ncomplexity via multi-agent competition. arXiv preprint arXiv:1710.03748, 2017. 216\n51. Chitta Baral. Knowledge Representation, Reasoning and Declarative Problem Solving. Cam-\nbridge university press, 2003. 236\n52. Nolan Bard, Jakob N. Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H. Francis Song,\nEmilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, Iain Dunning, Shibl\nMourad, Hugo Larochelle, Marc G. Bellemare, and Michael Bowling. The Hanabi challenge:\nA new frontier for AI research. Artiﬁcial Intelligence, 280:103216, 2020. 225\n53. Nolan Bard, John Hawkin, Jonathan Rubin, and Martin Zinkevich. The annual computer\npoker competition. AI Magazine, 34(2):112, 2013. 218, 227\n54. Simon Baron-Cohen, Alan M Leslie, and Uta Frith. Does the autistic child have a “theory of\nmind”? Cognition, 21(1):37–46, 1985. 197, 215\n55. Andrew Barron, Jorma Rissanen, and Bin Yu. The minimum description length principle in\ncoding and modeling. IEEE Transactions on Information Theory, 44(6):2743–2760, 1998. 15\n56. Andrew G Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement\nlearning. Discrete Event Dynamic Systems, 13(1-2):41–77, 2003. 236, 245\n57. Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements\nthat can solve diﬃcult learning control problems. IEEE Transactions on Systems, Man, and\nCybernetics, (5):834–846, 1983. 59, 103, 120\n58. OpenAI Baselines. DQN https://openai.com/blog/openai-baselines-dqn/, 2017. 84,\n88\n59. Jonathan Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research,\n12:149–198, 2000. 251\n60. Jonathan Baxter, Andrew Tridgell, and Lex Weaver. Knightcap: a chess program that learns\nby combining TD (𝜆) with game-tree search. arXiv preprint cs/9901002, 1999. 192\n342\nReferences\n61. Jonathan Baxter, Andrew Tridgell, and Lex Weaver. Learning to play chess using temporal\ndiﬀerences. Machine Learning, 40(3):243–263, 2000. 166, 192\n62. Don Beal and Martin C. Smith. Temporal diﬀerence learning for heuristic search and game\nplaying. Information Sciences, 122(1):3–21, 2000. 192\n63. Mark F Bear, Barry W Connors, and Michael A Paradiso. Neuroscience, volume 2. Lippincott\nWilliams & Wilkins, 2007. 308\n64. Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich\nKüttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, Julian Schrittwieser, Keith\nAnderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaﬀney, Helen King,\nDemis Hassabis, Shane Legg, and Stig Petersen. Deepmind lab. arXiv preprint arXiv:1612.03801,\n2016. 336\n65. Laurens Beljaards. Ai agents for the abstract strategy game tak. Master’s thesis, Leiden\nUniversity, 2017. 167\n66. Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine\nlearning and the bias-variance trade-oﬀ. arXiv preprint arXiv:1812.11118, 2018. 325\n67. Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features.\narXiv preprint arXiv:1903.07571, 2019. 325\n68. Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overﬁtting or perfect ﬁtting? Risk bounds\nfor classiﬁcation and regression rules that interpolate. In Advances in Neural Information\nProcessing Systems, pages 2300–2311, 2018. 325\n69. Marc Bellemare, Joel Veness, and Michael Bowling. Bayesian learning of recursively factored\nenvironments. In International Conference on Machine Learning, pages 1211–1219, 2013. 282\n70. Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforce-\nment learning. In International Conference on Machine Learning, pages 449–458, 2017. 84,\n87\n71. Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning\nEnvironment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013. 7, 41, 69, 71, 89, 90, 241, 266, 336\n72. Richard Bellman. Dynamic Programming. Courier Corporation, 1957, 2013. 39, 45, 62, 63,\n299, 303\n73. Richard Bellman. On the application of dynamic programing to the determination of optimal\nplay in chess and checkers. Proceedings of the National Academy of Sciences, 53(2):244–247,\n1965. 62\n74. Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of represen-\ntations for domain adaptation. Advances in Neural Information Processing Systems, 19:137,\n2007. 256\n75. Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule.\nTechnical report, Montreal, 1990. 272\n76. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and\nnew perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–\n1828, 2013. 312\n77. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.\nIn Proceedings of the 26th Annual International Conference on Machine Learning, pages 41–48,\n2009. 180, 192\n78. Gerardo Beni. Swarm intelligence. Complex Social and Behavioral Systems: Game Theory and\nAgent-Based Models, pages 791–818, 2020. 227\n79. Gerardo Beni and Jing Wang. Swarm intelligence in cellular robotic systems. In Robots and\nBiological Systems: Towards a New Bionics?, pages 703–712. Springer, 1993. 215\n80. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak,\nChristy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, Rafal\nJózefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pondé\nde Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon\nSidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep\nreinforcement learning. arXiv preprint arXiv:1912.06680, 2019. 73\nReferences\n343\n81. Tim Berners-Lee, James Hendler, and Ora Lassila. The semantic web. Scientiﬁc American,\n284(5):28–37, 2001. 9\n82. Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity\nof decentralized control of markov decision processes. Mathematics of Operations Research,\n27(4):819–840, 2002. 201, 210\n83. Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with\ndiﬀerentiable closed-form solvers. In International Conference on Learning Representations,\n2018. 272\n84. R Bertolami, H Bunke, S Fernandez, A Graves, M Liwicki, and J Schmidhuber. A novel\nconnectionist system for improved unconstrained handwriting recognition. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 31(5), 2009. 317\n85. Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P Bertsekas.\nDynamic Programming and Optimal Control, volume 1. Athena scientiﬁc Belmont, MA, 1995.\n10\n86. Dimitri P Bertsekas and John Tsitsiklis. Neuro-Dynamic Programming. MIT Press Cambridge,\n1996. 7, 62\n87. Shrisha Bharadwaj.\nEmbarrsingly simple zero shot learning.\nhttps://github.com/\nchichilicious/embarrsingly-simple-zero-shot-learning, 2018. 272, 273\n88. Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba\nSzepesvári. Convergent temporal-diﬀerence learning with arbitrary smooth function ap-\nproximation. In Advances in Neural Information Processing Systems, pages 1204–1212, 2009.\n78, 90\n89. Darse Billings, Aaron Davidson, Jonathan Schaeﬀer, and Duane Szafron. The challenge of\npoker. Artiﬁcial Intelligence, 134(1-2):201–240, 2002. 218, 227\n90. Darse Billings, Aaron Davidson, Terence Schauenberg, Neil Burch, Michael Bowling, Robert\nHolte, Jonathan Schaeﬀer, and Duane Szafron. Game-tree search with adaptation in stochastic\nimperfect-information games. In International Conference on Computers and Games, pages\n21–34. Springer, 2004. 227\n91. Darse Billings, Denis Papp, Jonathan Schaeﬀer, and Duane Szafron. Opponent modeling in\npoker. AAAI/IAAI, 493:499, 1998. 165\n92. Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python:\nanalyzing text with the natural language toolkit. O’Reilly Media, Inc., 2009. 280\n93. Christopher M Bishop. Pattern Recognition and Machine Learning. Information science and\nstatistics. Springer Verlag, Heidelberg, 2006. 7, 14, 63, 109, 132, 133, 263, 302, 304, 306, 325\n94. Peter Bloem. Transformers http://peterbloem.nl/blog/transformers. 324\n95. Christian Blum and Daniel Merkle. Swarm Intelligence: Introduction and Applications. Springer\nScience & Business Media, 2008. 207, 215\n96. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,\nShyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie S. Chen, Kathleen\nCreel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin\nDurmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn,\nTrevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha,\nTatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu,\nJing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti,\nGeoﬀKeeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna,\nand Rohith Kuditipudi. On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258, 2021. 248, 267\n97. Eric Bonabeau, Marco Dorigo, and Guy Theraulaz. Swarm Intelligence: From Natural to\nArtiﬁcial Systems. Oxford University Press, 1999. 11, 207\n98. Borealis. Few shot learning tutorial https://www.borealisai.com/en/blog/tutorial-\n2-few-shot-learning-and-meta-learning-i/. 258\n99. Zdravko I Botev, Dirk P Kroese, Reuven Y Rubinstein, and Pierre L’Ecuyer. The cross-entropy\nmethod for optimization. In Handbook of Statistics, volume 31, pages 35–59. Elsevier, 2013.\n137\n344\nReferences\n100. Matthew Botvinick, Sam Ritter, Jane X Wang, Zeb Kurth-Nelson, Charles Blundell, and Demis\nHassabis. Reinforcement learning, fast and slow. Trends in Cognitive Sciences, 23(5):408–422,\n2019. 271\n101. Matthew M Botvinick, Yael Niv, and Andew G Barto. Hierarchically organized behavior and\nits neural foundations: a reinforcement learning perspective. Cognition, 113(3):262–280, 2009.\n245\n102. Bruno Bouzy and Bernard Helmstetter. Monte Carlo Go developments. In Advances in\nComputer Games, pages 159–174. Springer, 2004. 167, 171\n103. Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up Limit\nHold’em poker is solved. Science, 347(6218):145–149, 2015. 200, 218, 227\n104. Michael H. Bowling, Nicholas Abou Risk, Nolan Bard, Darse Billings, Neil Burch, Joshua\nDavidson, John Alexander Hawkin, Robert Holte, Michael Johanson, Morgan Kan, Bryce\nParadis, Jonathan Schaeﬀer, David Schnizlein, Duane Szafron, Kevin Waugh, and Martin\nZinkevich. A demonstration of the polaris poker system. In Proceedings of The 8th Interna-\ntional Conference on Autonomous Agents and Multiagent Systems, volume 2, pages 1391–1392,\n2009. 227\n105. Robert Boyd and Peter J Richerson. Culture and the Evolutionary Process. University of\nChicago press, 1988. 213, 220, 227\n106. Pavel Brazdil, Christophe Giraud Carrier, Carlos Soares, and Ricardo Vilalta. Metalearning:\nApplications to data mining. Springer Science & Business Media, 2008. 251, 263, 271, 272\n107. Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of\nexpensive cost functions, with application to active user modeling and hierarchical reinforce-\nment learning. arXiv preprint arXiv:1012.2599, 2010. 282\n108. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016. 41, 89, 90, 98,\n120, 336\n109. Rodney A Brooks. Intelligence without representation. Artiﬁcial Intelligence, 47(1-3):139–159,\n1991. 11\n110. Noam Brown, Sam Ganzfried, and Tuomas Sandholm. Hierarchical abstraction, distributed\nequilibrium computation, and post-processing, with application to a champion No-Limit\nTexas Hold’em agent. In AAAI Workshop: Computer Poker and Imperfect Information, 2015.\n208\n111. Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret\nminimization. In International Conference on Machine Learning, pages 793–802. PMLR, 2019.\n210, 219, 227\n112. Noam Brown and Tuomas Sandholm. Superhuman AI for Heads-up No-limit poker: Libratus\nbeats top professionals. Science, 359(6374):418–424, 2018. 208, 219, 227\n113. Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science,\n365(6456):885–890, 2019. 218, 219, 227\n114. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeﬀrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems, 2020. 250, 312, 324\n115. Cameron Browne. Hex Strategy. AK Peters/CRC Press, 2000. 188\n116. Cameron Browne, Dennis JNJ Soemers, and Eric Piette. Strategic features for general games.\nIn KEG@ AAAI, pages 70–75, 2019. 282\n117. Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling,\nPhilipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon\nColton. A survey of Monte Carlo Tree Search methods. IEEE Transactions on Computational\nIntelligence and AI in Games, 4(1):1–43, 2012. 168, 169, 170, 171, 172, 173, 174, 191, 337\n118. Bernd Brügmann. Monte Carlo Go. Technical report, Syracuse University, 1993. 167, 171\nReferences\n345\n119. Bruno Buchberger, George E Collins, Rüdiger Loos, and Rudolph Albrecht. Computer algebra\nsymbolic and algebraic computation. ACM SIGSAM Bulletin, 16(4):5–5, 1982. 9\n120. Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In\nProceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining, pages 535–541, 2006. 282\n121. Lars Buesing, Theophane Weber, Sébastien Racaniere, SM Eslami, Danilo Rezende, David P\nReichert, Fabio Viola, Frederic Besse, Karol Gregor, Demis Hassabis, and Daan Wierstra.\nLearning and querying fast generative models for reinforcement learning. arXiv preprint\narXiv:1802.03006, 2018. 135, 143\n122. Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multi-\nagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C\n(Applications and Reviews), 38(2):156–172, 2008. 226\n123. Zhiyuan Cai, Huanhui Cao, Wenjie Lu, Lin Zhang, and Hao Xiong. Safe multi-agent rein-\nforcement learning through decentralized multiple control barrier functions. arXiv preprint\narXiv:2103.12553, 2021. 211\n124. Murray Campbell, A Joseph Hoane Jr, and Feng-Hsiung Hsu. Deep Blue. Artiﬁcial Intelligence,\n134(1-2):57–83, 2002. 60, 68\n125. Andres Campero, Roberta Raileanu, Heinrich Küttler, Joshua B Tenenbaum, Tim Rocktäschel,\nand Edward Grefenstette. Learning with AMIGo: Adversarially motivated intrinsic goals. In\nInternational Conference on Learning Representations, 2020. 180, 235, 238\n126. Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, and Stephen Clark.\nEmergent communication through negotiation. In International Conference on Learning\nRepresentations, 2018. 212\n127. Thomas Carr, Maria Chli, and George Vogiatzis. Domain adaptation for reinforcement\nlearning on the atari. In Proceedings of the 18th International Conference on Autonomous\nAgents and MultiAgent Systems, AAMAS ’19, Montreal, pages 1859–1861, 2018. 256\n128. Edward Cartwright. Behavioral Economics. Routledge, 2018. 227\n129. Rich Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997. 251, 252, 254, 255\n130. Rich Caruana, Steve Lawrence, and C Lee Giles. Overﬁtting in neural nets: Backpropagation,\nconjugate gradient, and early stopping. In Advances in Neural Information Processing Systems,\npages 402–408, 2001. 325\n131. Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G Belle-\nmare. Dopamine: A research framework for deep reinforcement learning. arXiv preprint\narXiv:1812.06110, 2018. 336\n132. Tristan Cazenave. Residual networks for computer Go. IEEE Transactions on Games, 10(1):107–\n110, 2018. 176\n133. Tristan Cazenave, Yen-Chi Chen, Guan-Wei Chen, Shi-Yu Chen, Xian-Dong Chiu, Julien\nDehos, Maria Elsa, Qucheng Gong, Hengyuan Hu, Vasil Khalidov, Cheng-Ling Li, Hsin-I Lin,\nYu-Jin Lin, Xavier Martinet, Vegard Mella, Jérémy Rapin, Baptiste Rozière, Gabriel Synnaeve,\nFabien Teytaud, Olivier Teytaud, Shi-Cheng Ye, Yi-Jun Ye, Shi-Jim Yen, and Sergey Zagoruyko.\nPolygames: Improved zero learning. arXiv preprint arXiv:2001.09832, 2020. 187, 188, 191, 336,\n337\n134. Tristan Cazenave and Bernard Helmstetter. Combining tactical search and Monte-Carlo in\nthe game of Go. In Proceedings of the 2005 IEEE Symposium on Computational Intelligence\nand Games (CIG05), Essex University, volume 5, pages 171–175, 2005. 174\n135. Hyeong Soo Chang, Michael C Fu, Jiaqiao Hu, and Steven I Marcus. An adaptive sampling\nalgorithm for solving Markov decision processes. Operations Research, 53(1):126–139, 2005.\n174\n136. Yang Chao. Share and play new sokoban levels. http://Sokoban.org, 2013. 26\n137. Guillaume Chaslot. Monte-Carlo tree search. PhD thesis, Maastricht University, 2010. 171\n138. Guillaume Chaslot, Sander Bakkes, Istvan Szita, and Pieter Spronck. Monte-Carlo tree search:\nA new framework for game AI. In AIIDE, 2008. 174, 183\n139. Kumar Chellapilla and David B Fogel. Evolving neural networks to play checkers without\nrelying on expert knowledge. IEEE Transactions on Neural Networks, 10(6):1382–1391, 1999.\n79\n346\nReferences\n140. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\nvia sequence modeling. arXiv preprint arXiv:2106.01345, 2021. 324\n141. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A\ncloser look at few-shot classiﬁcation. In International Conference on Learning Representations,\n2019. 257, 268, 272\n142. Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and\nacceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017. 282\n143. Maxime Chevalier-Boisvert, Lucas Willems, and Sumans Pal. Minimalistic gridworld en-\nvironment for OpenAI Gym https://github.com/maximecb/gym-minigrid, 2018. 181,\n238\n144. Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent envi-\nronment simulators. In International Conference on Learning Representations, 2017. 143\n145. Jaeyoung Choi, Jack J Dongarra, and David W Walker. PB-BLAS: a set of parallel block basic\nlinear algebra subprograms. Concurrency: Practice and Experience, 8(7):517–535, 1996. 327\n146. François Chollet. Keras. https://keras.io, 2015. 91\n147. François Chollet. Deep learning with Python. Manning Publications Co., 2017. 325\n148. Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. Back to basics: Benchmarking canoni-\ncal evolution strategies for playing Atari. In Proceedings of the Twenty-Seventh International\nJoint Conference on Artiﬁcial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, pages\n1419–1426, 2018. 227\n149. Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement\nlearning in a handful of trials using probabilistic dynamics models. In Advances in Neural\nInformation Processing Systems, pages 4754–4765, 2018. 133, 137, 142, 148\n150. Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural\nnetworks. In International Conference on Learning Representations, 2016. 280\n151. Carlo Ciliberto, Youssef Mroueh, Tomaso Poggio, and Lorenzo Rosasco. Convex learning of\nmultiple tasks and their structure. In International Conference on Machine Learning, pages\n1548–1557. PMLR, 2015. 251\n152. Dan Cireşan, Ueli Meier, Luca Maria Gambardella, and Jürgen Schmidhuber. Deep, big,\nsimple neural nets for handwritten digit recognition. Neural Computation, 22(12):3207–3220,\n2010. 328\n153. Dan Cireşan, Ueli Meier, and Jürgen Schmidhuber. Multi-column deep neural networks for\nimage classiﬁcation. In 2012 IEEE Conference on Computer Vision and Pattern Recognition,\nProvidence, RI, US, pages 3642–3649, 2012. 317\n154. Christopher Clark and Amos Storkey. Teaching deep convolutional neural networks to play\nGo. arxiv preprint. arXiv preprint arXiv:1412.3409, 1, 2014. 79, 191\n155. Christopher Clark and Amos Storkey. Training deep convolutional neural networks to play\nGo. In International Conference on Machine Learning, pages 1766–1774, 2015. 183, 191\n156. Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter\nAbbeel. Model-based reinforcement learning via meta-policy optimization. In 2nd Annual\nConference on Robot Learning, CoRL 2018, Zürich, Switzerland, pages 617–629, 2018. 133, 142,\n148\n157. William F Clocksin and Christopher S Mellish. Programming in Prolog: Using the ISO standard.\nSpringer Science & Business Media, 1981. 9\n158. Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation\nto benchmark reinforcement learning. In International Conference on Machine Learning, pages\n2048–2056. PMLR, 2020. 181, 283, 336\n159. Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying\ngeneralization in reinforcement learning. In International Conference on Machine Learning,\npages 1282–1289, 2018. 282\n160. Helder Coelho and Luis Moniz Pereira. Automated reasoning in geometry theorem proving\nwith prolog. Journal of Automated Reasoning, 2(4):329–390, 1986. 282\nReferences\n347\n161. Cédric Colas, Pierre Fournier, Mohamed Chetouani, Olivier Sigaud, and Pierre-Yves Oudeyer.\nCurious: intrinsically motivated modular multi-goal reinforcement learning. In International\nConference on Machine Learning, pages 1331–1340. PMLR, 2019. 281\n162. Cédric Colas, Tristan Karch, Olivier Sigaud, and Pierre-Yves Oudeyer. Intrinsically motivated\ngoal-conditioned reinforcement learning: a short survey. arXiv preprint arXiv:2012.09830,\n2020. 281\n163. Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O Stanley,\nand JeﬀClune. Improving exploration in evolution strategies for deep reinforcement learning\nvia a population of novelty-seeking agents. In Advances in Neural Information Processing\nSystems, pages 5032–5043, 2018. 227\n164. Rémi Coulom. Eﬃcient selectivity and backup operators in Monte-Carlo Tree Search. In\nInternational Conference on Computers and Games, pages 72–83. Springer, 2006. 168, 174, 191\n165. Rémi Coulom. Monte-Carlo tree search in Crazy Stone. In Proceedings Game Programming\nWorkshop, Tokyo, Japan, pages 74–75, 2007. 171, 174\n166. Rémi Coulom. The Monte-Carlo revolution in Go. In The Japanese-French Frontiers of Science\nSymposium (JFFoS 2008), Roscoﬀ, France, 2009. 174, 183\n167. Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,\nrobotics and machine learning. http://pybullet.org, 2016–2019. 96, 97, 120\n168. Gabriela Csurka. Domain adaptation for visual applications: A comprehensive survey. In\nDomain Adaptation in Computer Vision Applications, Advances in Computer Vision and\nPattern Recognition, pages 1–35. Springer, 2017. 256, 272\n169. Joseph Culberson. Sokoban is PSPACE-complete. Technical report, University of Alberta,\n1997. 27, 60\n170. Joseph C Culberson and Jonathan Schaeﬀer. Pattern databases. Computational Intelligence,\n14(3):318–334, 1998. 171\n171. Ken Currie and Austin Tate. O-plan: the open planning architecture. Artiﬁcial Intelligence,\n52(1):49–86, 1991. 235\n172. Wojciech Marian Czarnecki, Gauthier Gidel, Brendan Tracey, Karl Tuyls, Shayegan Omid-\nshaﬁei, David Balduzzi, and Max Jaderberg. Real world games look like spinning tops. In\nAdvances in Neural Information Processing Systems, 2020. 154\n173. Kamil Czarnogórski. Monte Carlo Tree Search beginners guide https://int8.io/monte-\ncarlo-tree-search-beginners-guide/, 2018. 170, 171\n174. Will Dabney, Zeb Kurth-Nelson, Naoshige Uchida, Clara Kwon Starkweather, Demis Hassabis,\nRémi Munos, and Matthew Botvinick. A distributional code for value in dopamine-based\nreinforcement learning. Nature, pages 1–5, 2020. 87\n175. Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R McKee, Joel Z\nLeibo, Kate Larson, and Thore Graepel. Open problems in cooperative AI. arXiv preprint\narXiv:2012.08630, 2020. 210\n176. Zhongxiang Dai, Yizhou Chen, Bryan Kian Hsiang Low, Patrick Jaillet, and Teck-Hua Ho.\nR2-B2: recursive reasoning-based Bayesian optimization for no-regret learning in games. In\nInternational Conference on Machine Learning, pages 2291–2301. PMLR, 2020. 212\n177. Christian Daniel, Herke Van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference\nfor determining options in reinforcement learning. Machine Learning, 104(2):337–357, 2016.\n245\n178. Shubhomoy Das, Weng-Keen Wong, Thomas Dietterich, Alan Fern, and Andrew Emmott.\nIncorporating expert feedback into active anomaly discovery. In 2016 IEEE 16th International\nConference on Data Mining (ICDM), pages 853–858. IEEE, 2016. 181\n179. Hal Daumé III. Frustratingly easy domain adaptation. In ACL 2007, Proceedings of the 45th\nAnnual Meeting of the Association for Computational Linguistics, June 23-30, 2007, Prague,\n2007. 256\n180. Morton D Davis. Game Theory: a Nontechnical Introduction. Courier Corporation, 2012. 226\n181. Richard Dawkins and Nicola Davis. The Selﬁsh Gene. Macat Library, 2017. 213\n182. Peter Dayan and Geoﬀrey E Hinton. Feudal reinforcement learning. In Advances in Neural\nInformation Processing Systems, pages 271–278, 1993. 235, 236\n348\nReferences\n183. Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on\nthe cross-entropy method. Annals of Operations Research, 134(1):19–67, 2005. 137\n184. Luis M De Campos, Juan M Fernandez-Luna, José A Gámez, and José M Puerta. Ant colony\noptimization for learning Bayesian networks. International Journal of Approximate Reasoning,\n31(3):291–311, 2002. 282\n185. Dave De Jonge, Tim Baarslag, Reyhan Aydoğan, Catholijn Jonker, Katsuhide Fujita, and\nTakayuki Ito. The challenge of negotiation in the game of diplomacy. In International\nConference on Agreement Technologies, pages 100–114. Springer, 2018. 155, 212\n186. Joery A. de Vries, Ken S. Voskuil, Thomas M. Moerland, and Aske Plaat. Visualizing MuZero\nmodels. arXiv preprint arXiv:2102.12924, 2021. 145, 146, 150, 279, 337\n187. Rina Dechter. Learning while searching in constraint-satisfaction problems. AAAI, 1986. 312\n188. Marc Deisenroth and Carl E Rasmussen. PILCO: a model-based and data-eﬃcient approach\nto policy search. In Proceedings of the 28th International Conference on Machine Learning\n(ICML-11), pages 465–472, 2011. 132, 142, 148\n189. Marc Peter Deisenroth, Dieter Fox, and Carl Edward Rasmussen. Gaussian processes for\ndata-eﬃcient learning in robotics and control. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 37(2):408–423, 2013. 132, 148\n190. Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for\nrobotics. In Foundations and Trends in Robotics 2, pages 1–142. Now publishers, 2013. 147,\n148\n191. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 248–255. Ieee, 2009. 69, 280, 327\n192. Mohit Deshpande. Deep RL policy methods https://mohitd.github.io/2019/01/20/\ndeep-rl-policy-methods.html. 120\n193. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training\nof deep bidirectional transformers for language understanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019, Minneapolis, 2018. 249, 250, 266, 267, 280,\n311, 312, 324\n194. Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec\nRadford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OpenAI baselines.\nhttps://github.com/openai/baselines, 2017. 337\n195. Thomas G Dietterich. The MAXQ method for hierarchical reinforcement learning. In\nInternational Conference on Machine Learning, volume 98, pages 118–126, 1998. 245\n196. Thomas G Dietterich. Hierarchical reinforcement learning with the MAXQ value function\ndecomposition. Journal of Artiﬁcial Intelligence Research, 13:227–303, 2000. 27, 60, 235, 236,\n245\n197. Chuong B Do and Andrew Y Ng. Transfer learning for text classiﬁcation. Advances in Neural\nInformation Processing Systems, 18:299–306, 2005. 272\n198. Thang Doan, Joao Monteiro, Isabela Albuquerque, Bogdan Mazoure, Audrey Durand, Joelle\nPineau, and R Devon Hjelm. On-line adaptative curriculum learning for GANs. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 3470–3477, 2019. 182, 192\n199. Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan Schaal, Marc\nToussaint, and Sebastian Trimpe. Probabilistic recurrent state-space models. arXiv preprint\narXiv:1801.10395, 2018. 135\n200. JeﬀDonahue, Yangqing Jia, Oriol Vinyals, Judy Hoﬀman, Ning Zhang, Eric Tzeng, and Trevor\nDarrell. Decaf: A deep convolutional activation feature for generic visual recognition. In\nInternational Conference on Machine Learning, pages 647–655. PMLR, 2014. 249, 272\n201. Hao Dong, Zihan Ding, and Shanghang Zhang. Deep Reinforcement Learning. Springer, 2020.\n90\n202. Jack J Dongarra, Jeremy Du Croz, Sven Hammarling, and Richard J Hanson. An extended set\nof FORTRAN basic linear algebra subprograms. ACM Transactions on Mathematical Software,\n14(1):1–17, 1988. 327\nReferences\n349\n203. Christian Donninger. Null move and deep search. ICGA Journal, 16(3):137–143, 1993. 167\n204. Dorit Dor and Uri Zwick. Sokoban and other motion planning problems. Computational\nGeometry, 13(4):215–228, 1999. 27, 60\n205. Derek Doran, Sarah Schulz, and Tarek R Besold. What does explainable AI really mean? A\nnew conceptualization of perspectives. arXiv preprint arXiv:1710.00794, 2017. 282\n206. Marco Dorigo. Optimization, learning and natural algorithms. PhD Thesis, Politecnico di\nMilano, 1992. 215\n207. Marco Dorigo and Mauro Birattari. Swarm intelligence. Scholarpedia, 2(9):1462, 2007. 207,\n227\n208. Marco Dorigo, Mauro Birattari, and Thomas Stutzle. Ant colony optimization. IEEE Compu-\ntational Intelligence Magazine, 1(4):28–39, 2006. 215, 227, 228\n209. Marco Dorigo and Luca Maria Gambardella. Ant colony system: a cooperative learning\napproach to the traveling salesman problem. IEEE Transactions on Evolutionary Computation,\n1(1):53–66, 1997. 11, 215\n210. Norman R Draper and Harry Smith. Applied Regression Analysis, volume 326. John Wiley &\nSons, 1998. 14\n211. Yuntao Du, Zhiwen Tan, Qian Chen, Xiaowen Zhang, Yirong Yao, and Chongjun Wang. Dual\nadversarial domain adaptation. arXiv preprint arXiv:2001.00153, 2020. 256\n212. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking\ndeep reinforcement learning for continuous control. In International Conference on Machine\nLearning, pages 1329–1338, 2016. 118, 119, 120\n213. Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2:\nFast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779,\n2016. 192, 258, 260, 269, 272, 279\n214. Ishan P Durugkar, Clemens Rosenbaum, Stefan Dernbach, and Sridhar Mahadevan. Deep\nreinforcement learning with macro-actions. arXiv preprint arXiv:1606.04615, 2016. 245\n215. Werner Duvaud and Aurèle Hainaut. MuZero general: Open reimplementation of muzero.\nhttps://github.com/werner-duvaud/muzero-general, 2019. 150, 279\n216. Zach Dwiel, Madhavun Candadai, Mariano Phielipp, and Arjun K Bansal. Hierarchical policy\nlearning is sensitive to goal space design. arXiv preprint arXiv:1905.01537, 2019. 235\n217. Russell C Eberhart, Yuhui Shi, and James Kennedy. Swarm Intelligence. Elsevier, 2001. 227\n218. Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual\nforesight: Model-based deep reinforcement learning for vision-based robotic control. arXiv\npreprint arXiv:1812.00568, 2018. 137\n219. Tom Eccles, Edward Hughes, János Kramár, Steven Wheelwright, and Joel Z Leibo. Learning\nreciprocity in complex sequential social dilemmas. arXiv preprint arXiv:1903.08082, 2019. 212\n220. Adrien\nLucas\nEcofet.\nAn\nintuitive\nexplanation\nof\npolicy\ngradient\nhttps:\n//towardsdatascience.com/an-intuitive-explanation-of-policy-gradient-\npart-1-reinforce-aa4392cbfd3c. 120\n221. Adrien Ecoﬀet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and JeﬀClune. Go-explore:\na new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019. 88\n222. Adrien Ecoﬀet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and JeﬀClune. First return,\nthen explore. Nature, 590(7847):580–586, 2021. 88, 242, 281, 337\n223. Harrison Edwards and Amos Storkey. Towards a neural statistician. In International Confer-\nence on Learning Representations, 2017. 272\n224. Agoston E Eiben and Jim E Smith. What is an evolutionary algorithm? In Introduction to\nEvolutionary Computing, pages 25–48. Springer, 2015. 227\n225. Jeﬀrey L Elman. Learning and development in neural networks: The importance of starting\nsmall. Cognition, 48(1):71–99, 1993. 180\n226. Arpad E Elo. The Rating of Chessplayers, Past and Present. Arco Pub., 1978. 184\n227. Markus Enzenberger, Martin Muller, Broderick Arneson, and Richard Segal. Fuego—an\nopen-source framework for board games and Go engine based on Monte Carlo tree search.\nIEEE Transactions on Computational Intelligence and AI in Games, 2(4):259–270, 2010. 183, 191\n350\nReferences\n228. Tom Erez, Yuval Tassa, and Emanuel Todorov. Simulation tools for model-based robotics:\nComparison of Bullet, Havok, MuJoCo, Ode and Physx. In 2015 IEEE International Conference\non Robotics and Automation (ICRA), pages 4397–4404. IEEE, 2015. 97\n229. Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement\nlearning. Journal of Machine Learning Research, 6:503–556, April 2005. 91\n230. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam\nDoron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:\nScalable distributed deep-RL with importance weighted actor-learner architectures. In\nInternational Conference on Machine Learning, pages 1407–1416. PMLR, 2018. 269\n231. Eyal Even-Dar, Yishay Mansour, and Peter Bartlett. Learning rates for Q-learning. Journal of\nmachine learning Research, 5(1), 2003. 58\n232. Richard Everett and Stephen Roberts. Learning against non-stationary agents with opponent\nmodelling and deep reinforcement learning. In 2018 AAAI Spring Symposium Series, 2018.\n212\n233. Theodoros Evgeniou and Massimiliano Pontil. Regularized multi-task learning. In Proceedings\nof the tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\npages 109–117. ACM, 2004. 255\n234. Linxi Fan, Yuke Zhu, Jiren Zhu, Zihua Liu, Orien Zeng, Anchit Gupta, Joan Creus-Costa,\nSilvio Savarese, and Li Fei-Fei. Surreal: Open-source reinforcement learning framework and\nrobot manipulation benchmark. In Conference on Robot Learning, pages 767–782, 2018. 118\n235. Jesse Farebrother, Marlos C Machado, and Michael Bowling. Generalization and regularization\nin DQN. arXiv preprint arXiv:1810.00123, 2018. 282\n236. Gregory Farquhar, Tim Rocktäschel, Maximilian Igl, and SA Whiteson. TreeQN and ATreeC:\nDiﬀerentiable tree planning for deep reinforcement learning. In International Conference on\nLearning Representations, 2018. 60, 139, 140, 142, 148\n237. Li Fei-Fei, Jia Deng, and Kai Li. Imagenet: Constructing a large-scale image database. Journal\nof Vision, 9(8):1037–1037, 2009. 69, 266, 280, 327\n238. Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey\nLevine. Model-based value estimation for eﬃcient model-free reinforcement learning. arXiv\npreprint arXiv:1803.00101, 2018. 136, 142, 148, 211\n239. Dieqiao Feng, Carla P Gomes, and Bart Selman. Solving hard AI planning instances using\ncurriculum-driven deep reinforcement learning. arXiv preprint arXiv:2006.02689, 2020. 60,\n148, 181, 182, 192, 279\n240. Santiago Fernández, Alex Graves, and Jürgen Schmidhuber. An application of recurrent\nneural networks to discriminative keyword spotting. In International Conference on Artiﬁcial\nNeural Networks, pages 220–229. Springer, 2007. 317\n241. Richard E Fikes, Peter E Hart, and Nils J Nilsson. Learning and executing generalized robot\nplans. Artiﬁcial Intelligence, 3:251–288, 1972. 235, 236\n242. Richard E Fikes and Nils J Nilsson. STRIPS: A new approach to the application of theorem\nproving to problem solving. Artiﬁcial Intelligence, 2(3-4):189–208, 1971. 9\n243. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for fast\nadaptation of deep networks. In International Conference on Machine Learning, pages 1126–\n1135. PMLR, 2017. 258, 261, 262, 263, 267, 272, 337\n244. Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017\nIEEE International Conference on Robotics and Automation (ICRA), pages 2786–2793. IEEE,\n2017. 137, 142, 148\n245. Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning.\nIn International Conference on Machine Learning, pages 1920–1930. PMLR, 2019. 272\n246. Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic Model-Agnostic Meta-Learning.\nIn Advances in Neural Information Processing Systems, pages 9516–9527, 2018. 272\n247. Yannis Flet-Berliac.\nThe promise of hierarchical reinforcement learning.\nhttps://\nthegradient.pub/the-promise-of-hierarchical-reinforcement-learning/, March\n2019. 231, 232, 280\nReferences\n351\n248. Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation\nfor reinforcement learning agents. In International Conference on Machine Learning, pages\n1515–1528. PMLR, 2018. 181, 192, 245\n249. Michael J Flynn. Some computer organizations and their eﬀectiveness. IEEE Transactions on\nComputers, 100(9):948–960, 1972. 328\n250. Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon White-\nson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 32, 2018. 211\n251. Jakob N Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and\nIgor Mordatch. Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326,\n2017. 212\n252. David B Fogel. An introduction to simulated evolutionary optimization. IEEE Transactions\non Neural Networks, 5(1):3–14, 1994. 11, 206\n253. David B Fogel, Timothy J Hays, Sarah L Hahn, and James Quon. Further evolution of a\nself-learning chess program. In Computational Intelligence in Games, 2005. 166\n254. Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex\nGraves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and\nShane Legg. Noisy networks for exploration. In International Conference on Learning\nRepresentations, 2018. 84, 87\n255. Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and Joelle Pineau.\nAn introduction to deep reinforcement learning. Foundations and Trends in Machine Learning,\n11(3-4):219–354, 2018. 25, 28, 62, 120\n256. Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared\nhierarchies. In International Conference on Learning Representations, 2018. 235, 245\n257. Nicholas Frosst and Geoﬀrey Hinton. Distilling a neural network into a soft decision tree. In\nProceedings of the First International Workshop on Comprehensibility and Explanation in AI\nand ML, 2017. 282\n258. Vittorio Gallese and Alvin Goldman. Mirror neurons and the simulation theory of mind-\nreading. Trends in Cognitive Sciences, 2(12):493–501, 1998. 197, 215, 227\n259. Sam Ganzfried and Tuomas Sandholm. Game theory-based opponent modeling in large\nimperfect-information games. In The 10th International Conference on Autonomous Agents\nand Multiagent Systems, volume 2, pages 533–540, 2011. 165, 226\n260. Sam Ganzfried and Tuomas Sandholm. Endgame solving in large imperfect-information\ngames. In Proceedings of the 2015 International Conference on Autonomous Agents and Multia-\ngent Systems, pages 37–45, 2015. 208\n261. The garage contributors. Garage: A toolkit for reproducible reinforcement learning research.\nhttps://github.com/rlworkgroup/garage, 2019. 119, 270, 336\n262. Carlos E Garcia, David M Prett, and Manfred Morari. Model predictive control: Theory and\npractice—a survey. Automatica, 25(3):335–348, 1989. 137, 148\n263. Victor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In International\nConference on Learning Representations, 2017. 272\n264. Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton,\nMurray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural\nprocesses. In International Conference on Machine Learning, pages 1704–1713. PMLR, 2018.\n272\n265. Alessandro Gasparetto, Paolo Boscariol, Albano Lanzutti, and Renato Vidoni. Path planning\nand trajectory planning algorithms: A general overview. In Motion and Operation Planning\nof Robotic Systems, pages 3–27. Springer, 2015. 26, 60\n266. Michael Gelfond and Vladimir Lifschitz. Action languages. Electronic Transactions on Artiﬁcial\nIntelligence, 2(3–4):193–210, 1998. 236\n267. Sylvain Gelly, Levente Kocsis, Marc Schoenauer, Michele Sebag, David Silver, Csaba\nSzepesvári, and Olivier Teytaud. The grand challenge of computer Go: Monte Carlo tree\nsearch and extensions. Communications of the ACM, 55(3):106–113, 2012. 183\n268. Sylvain Gelly and David Silver. Achieving master level play in 9 × 9 computer Go. In AAAI,\nvolume 8, pages 1537–1540, 2008. 183, 191\n352\nReferences\n269. Sylvain Gelly, Yizao Wang, and Olivier Teytaud. Modiﬁcation of UCT with patterns in\nMonte-Carlo Go. Technical Report RR-6062, INRIA, 2006. 171, 183, 191\n270. Aurélien Géron. Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools,\nand techniques to build intelligent systems. O’Reilly Media, Inc., 2019. 81, 91, 331\n271. Felix A Gers, Jürgen Schmidhuber, and Fred Cummins.\nLearning to forget: Continual\nprediction with LSTM. In Ninth International Conference on Artiﬁcial Neural Networks ICANN\n99. IET, 1999. 319\n272. Malik Ghallab, Dana Nau, and Paolo Traverso. Automated Planning: theory and practice.\nElsevier, 2004. 235\n273. Mohammad Ghavamzadeh, Sridhar Mahadevan, and Rajbala Makar. Hierarchical multi-agent\nreinforcement learning. Autonomous Agents and Multi-Agent Systems, 13(2):197–229, 2006.\n242\n274. Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine.\nWhy generalization in RL is diﬃcult: Epistemic POMDPs and implicit partial observability.\nAdvances in Neural Information Processing Systems, 34, 2021. 282\n275. Gerd Gigerenzer and Daniel G Goldstein. Reasoning the fast and frugal way: models of\nbounded rationality. Psychological review, 103(4):650, 1996. 213, 227\n276. Thomas Gilovich, Dale Griﬃn, and Daniel Kahneman. Heuristics and Biases: The Psychology\nof Intuitive Judgment. Cambridge university press, 2002. 213\n277. Andrew Gilpin and Tuomas Sandholm. A competitive Texas Hold’em poker player via\nautomated abstraction and real-time equilibrium computation. In Proceedings of the National\nConference on Artiﬁcial Intelligence, volume 21, page 1007, 2006. 227\n278. Ross Girshick, JeﬀDonahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for\naccurate object detection and semantic segmentation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 580–587, 2014. 249\n279. John C Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal\nStatistical Society: Series B (Methodological), 41(2):148–164, 1979. 51\n280. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge,\n2016. 11, 74, 90, 307, 308, 309, 311, 314, 316, 319, 324, 325\n281. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems, pages 2672–2680, 2014. 320, 321\n282. Geoﬀrey J Gordon. Stable function approximation in dynamic programming. In Machine\nLearning Proceedings 1995, pages 261–268. Elsevier, 1995. 91\n283. Geoﬀrey J Gordon. Approximate solutions to Markov decision processes. Carnegie Mellon\nUniversity, 1999. 70, 77\n284. Tobias Graf and Marco Platzner. Adaptive playouts in Monte-Carlo tree search with policy-\ngradient reinforcement learning. In Advances in Computer Games, pages 1–11. Springer, 2015.\n191\n285. Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griﬃths. Recasting\ngradient-based meta-learning as hierarchical bayes. In International Conference on Learning\nRepresentations, 2018. 272\n286. Alex Graves, Santiago Fernández, and Jürgen Schmidhuber. Bidirectional LSTM networks for\nimproved phoneme classiﬁcation and recognition. In International Conference on Artiﬁcial\nNeural Networks, pages 799–804. Springer, 2005. 319\n287. Alex Graves, Abdel-rahman Mohamed, and Geoﬀrey Hinton. Speech recognition with deep\nrecurrent neural networks. In 2013 IEEE International Conference on Acoustics, Speech and\nSignal Processing, pages 6645–6649. IEEE, 2013. 311\n288. Klaus Greﬀ, Rupesh K Srivastava, Jan Koutník, Bas R Steunebrink, and Jürgen Schmidhuber.\nLSTM: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems,\n28(10):2222–2232, 2017. 319\n289. Jean-Bastien Grill, Florent Altché, Yunhao Tang, Thomas Hubert, Michal Valko, Ioannis\nAntonoglou, and Rémi Munos. Monte-carlo tree search as regularized policy optimization.\nIn International Conference on Machine Learning, pages 3769–3778. PMLR, 2020. 140, 145, 174\nReferences\n353\n290. Christopher Grimm, André Barreto, Satinder Singh, and David Silver. The value equiva-\nlence principle for model-based reinforcement learning. In Advances in Neural Information\nProcessing Systems, 2020. 134, 144, 145, 146\n291. Nathan Grinsztajn, Johan Ferret, Olivier Pietquin, Philippe Preux, and Matthieu Geist. There\nis no turning back: A self-supervised approach for reversibility-aware reinforcement learning.\narXiv preprint arXiv:2106.04480, 2021. 246\n292. Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey.\nArtiﬁcial Intelligence Review, pages 1–49, 2021. 202, 226\n293. Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert Babuska. A survey of actor-\ncritic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on\nSystems, Man, and Cybernetics, Part C (Applications and Reviews), 42(6):1291–1307, 2012. 87,\n103\n294. Peter D Grünwald. The minimum description length principle. MIT press, 2007. 15, 322\n295. Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare,\nand Remi Munos. The reactor: A fast and sample-eﬃcient actor-critic agent for reinforcement\nlearning. In International Conference on Learning Representations, 2018. 89\n296. Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep Q-\nlearning with model-based acceleration. In International Conference on Machine Learning,\npages 2829–2838, 2016. 136, 142, 148\n297. Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored MDPs.\nIn Advances in Neural Information Processing Systems, volume 1, pages 1523–1530, 2001. 211\n298. Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, Sébastien Racanière, Theophane\nWeber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, Greg Wayne, David Silver,\nand Timothy P. Lillicrap. An investigation of model-free planning. In International Conference\non Machine Learning, pages 2464–2473, 2019. 60, 142, 148\n299. Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gómez Colmenarejo,\nKonrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. RL\nunplugged: Benchmarks for oﬄine reinforcement learning. arXiv preprint arXiv:2006.13888,\n2020. 336\n300. David Gunning. Explainable artiﬁcial intelligence (XAI). Defense Advanced Research Projects\nAgency (DARPA), 2, 2017. 282\n301. Xifeng Guo, Wei Chen, and Jianping Yin. A simple approach for unsupervised domain\nadaptation. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 1566–\n1570. IEEE, 2016. 256\n302. Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-\nreinforcement learning of structured exploration strategies. In Advances in Neural Information\nProcessing Systems, pages 5307–5316, 2018. 272\n303. David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In\nAdvances in Neural Information Processing Systems, pages 2450–2462, 2018. 135\n304. David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.\n134, 135, 142, 148, 282\n305. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning\nwith deep energy-based policies. In International Conference on Machine Learning, pages\n1352–1361. PMLR, 2017. 111\n306. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International\nConference on Machine Learning, pages 1861–1870. PMLR, 2018. 99, 111, 144, 276\n307. Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,\nVikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-\ncritic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. 111\n308. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to con-\ntrol: Learning behaviors by latent imagination. In International Conference on Learning\nRepresentations, 2020. 134, 135, 142, 144, 148\n354\nReferences\n309. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee,\nand James Davidson. Learning latent dynamics for planning from pixels. In International\nConference on Machine Learning, pages 2555–2565, 2019. 134, 135, 137, 142, 144, 145, 146, 148,\n337\n310. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with\ndiscrete world models. In International Conference on Learning Representations, 2021. 134,\n135, 142, 144, 148, 337\n311. Roland Hafner and Martin Riedmiller. Reinforcement learning in feedback control. Machine\nLearning, 84(1-2):137–169, 2011. 112\n312. Dongge Han, Wendelin Boehmer, Michael Wooldridge, and Alex Rogers. Multi-agent hi-\nerarchical reinforcement learning with dynamic termination. In Paciﬁc Rim International\nConference on Artiﬁcial Intelligence, pages 80–92. Springer, 2019. 242\n313. Dongge Han, Chris Xiaoxuan Lu, Tomasz Michalak, and Michael Wooldridge. Multiagent\nmodel-based credit assignment for continuous control, 2021. 211\n314. Hado V Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems,\npages 2613–2621, 2010. 86\n315. Matthew John Hausknecht. Cooperation and Communication in Multiagent Deep Reinforcement\nLearning. PhD thesis, University of Texas at Austin, 2016. 212\n316. Milos Hauskrecht, Nicolas Meuleau, Leslie Pack Kaelbling, Thomas L Dean, and Craig\nBoutilier. Hierarchical solution of Markov decision processes using macro-actions. In UAI\n’98: Proceedings of the Fourteenth Conference on Uncertainty in Artiﬁcial Intelligence, University\nof Wisconsin Business School, Madison, Wisconsin, 1998. 234, 245\n317. Conor F. Hayes, Roxana Radulescu, Eugenio Bargiacchi, Johan Källström, Matthew Mac-\nfarlane, Mathieu Reymond, Timothy Verstraeten, Luisa M. Zintgraf, Richard Dazeley,\nFredrik Heintz, Enda Howley, Athirai A. Irissappane, Patrick Mannion, Ann Nowé, Gabriel\nde Oliveira Ramos, Marcello Restelli, Peter Vamplew, and Diederik M. Roijers. A practical\nguide to multi-objective reinforcement learning and planning. arXiv preprint arXiv:2103.09568,\n2021. 202\n318. Simon Haykin. Neural Networks: a Comprehensive Foundation. Prentice Hall, 1994. 11\n319. Ryan B Hayward and Bjarne Toft. Hex: The Full Story. CRC Press, 2019. 188\n320. He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daumé III. Opponent modeling in deep\nreinforcement learning. In International Conference on Machine Learning, pages 1804–1813.\nPMLR, 2016. 165, 212\n321. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 770–778, 2016. 176, 319, 320\n322. Robert A Hearn and Erik D Demaine. Games, Puzzles, and Computation. CRC Press, 2009.\n27, 60\n323. David Heckerman, Dan Geiger, and David M Chickering. Learning Bayesian networks: The\ncombination of knowledge and statistical data. Machine Learning, 20(3):197–243, 1995. 282\n324. Nicolas Heess, David Silver, and Yee Whye Teh. Actor-critic reinforcement learning with\nenergy-based policies. In European Workshop on Reinforcement Learning, pages 45–58, 2013.\n78, 79, 90\n325. Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval\nTassa, Tom Erez, Ziyu Wang, SM Eslami, Martin Riedmiller, and David Silver. Emergence of\nlocomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017. 94, 117,\n216\n326. Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa.\nLearning continuous control policies by stochastic value gradients. In Advances in Neural\nInformation Processing Systems, pages 2944–2952, 2015. 142, 148\n327. Ernst A Heinz. New self-play results in computer chess. In International Conference on\nComputers and Games, pages 262–276. Springer, 2000. 192\n328. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David\nMeger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence, 2018. 91, 118, 119, 120, 142\nReferences\n355\n329. Mark Hendrikx, Sebastiaan Meijer, Joeri Van Der Velden, and Alexandru Iosup. Procedural\ncontent generation for games: A survey. ACM Transactions on Multimedia Computing,\nCommunications, and Applications, 9(1):1–22, 2013. 63\n330. John L Hennessy and David A Patterson. Computer Architecture: a Quantitative Approach.\nElsevier, 2017. 328\n331. Joseph Henrich, Robert Boyd, and Peter J Richerson. Five misunderstandings about cultural\nevolution. Human Nature, 19(2):119–137, 2008. 227\n332. Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A\nsurvey of learning in multiagent environments: Dealing with non-stationarity. arXiv preprint\narXiv:1707.09183, 2017. 197, 200, 205, 226\n333. Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. A survey and critique of multiagent\ndeep reinforcement learning. Autonomous Agents and Multi-Agent Systems, 33(6):750–797,\n2019. 202, 226\n334. Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt, Laurent Sifre,\nTheophane Weber, David Silver, and Hado van Hasselt. Muesli: Combining improvements in\npolicy optimization. In International Conference on Machine Learning, pages 4214–4226, 2021.\n145, 279\n335. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will\nDabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining\nimprovements in deep reinforcement learning. In AAAI, pages 3215–3222, 2018. 84, 85, 86, 91\n336. Francis Heylighen. What makes a Meme Successful? Selection Criteria for Cultural Evolution.\nAssociation Internationale de Cybernetique, 1998. 227\n337. Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel,\nMatthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot\ntransfer in reinforcement learning. In International Conference on Machine Learning, pages\n1480–1490. PMLR, 2017. 272\n338. Ashley Hill, Antonin Raﬃn, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene\nTraore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert,\nAlec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https:\n//github.com/hill-a/stable-baselines, 2018. 337\n339. W Daniel Hillis. New computer architectures and their relationship to physics or why\ncomputer science is no good. International Journal of Theoretical Physics, 21(3-4):255–262,\n1982. 328\n340. W Daniel Hillis and Lewis W Tucker. The CM-5 connection machine: A scalable supercom-\nputer. Communications of the ACM, 36(11):30–41, 1993. 328\n341. Geoﬀrey Hinton, Oriol Vinyals, and JeﬀDean. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531, 2015. 282\n342. Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with\nneural networks. Science, 313(5786):504–507, 2006. 321\n343. Geoﬀrey E Hinton and Terrence Joseph Sejnowski, editors. Unsupervised Learning: Founda-\ntions of Neural Computation. MIT press, 1999. 15\n344. Geoﬀrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut-\ndinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv\npreprint arXiv:1207.0580, 2012. 324\n345. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation,\n9(8):1735–1780, 1997. 317, 319\n346. John Holland. Adaptation in natural and artiﬁcial systems: an introductory analysis with\napplication to biology. Control and Artiﬁcial Intelligence, 1975. 51, 206\n347. John H Holland. Genetic algorithms. Scientiﬁc American, 267(1):66–73, 1992. 11\n348. Bert Hölldobler and Edward O Wilson.\nThe Superorganism: the Beauty, Elegance, and\nStrangeness of Insect Societies. WW Norton & Company, 2009. 195\n349. John J Hopﬁeld. Neural networks and physical systems with emergent collective compu-\ntational abilities. Proceedings of the National Academy of Sciences, 79(8):2554–2558, 1982.\n317\n356\nReferences\n350. Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Has-\nselt, and David Silver. Distributed prioritized experience replay. In International Conference\non Learning Representations, 2018. 225\n351. Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in\nneural networks: A survey. arXiv preprint arXiv:2004.05439, 2020. 251, 252, 257, 259, 271, 272\n352. Ronald A Howard. Dynamic programming and Markov processes. New York: John Wiley,\n1964. 28\n353. Chloe Ching-Yun Hsu, Celestine Mendler-Dünner, and Moritz Hardt. Revisiting design\nchoices in proximal policy optimization. arXiv preprint arXiv:2009.10897, 2020. 110\n354. Feng-Hsiung Hsu. Behind Deep Blue: Building the computer that defeated the world chess\nchampion. Princeton University Press, 2004. 68\n355. Feng-Hsiung Hsu, Thomas Anantharaman, Murray Campbell, and Andreas Nowatzyk. A\ngrandmaster chess machine. Scientiﬁc American, 263(4):44–51, 1990. 73\n356. R Lily Hu, Caiming Xiong, and Richard Socher. Zero-shot image classiﬁcation guided by\nnatural language descriptions of classes: A meta-learning approach. In Advances in Neural\nInformation Processing Systems, 2018. 272\n357. David H Hubel and Torsten N Wiesel. Shape and arrangement of columns in cat’s striate\ncortex. The Journal of Physiology, 165(3):559–568, 1963. 314\n358. David H Hubel and Torsten N Wiesel. Receptive ﬁelds and functional architecture of monkey\nstriate cortex. The Journal of Physiology, 195(1):215–243, 1968. 314\n359. Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain,\nSimon Schmitt, and David Silver. Learning and planning in complex action spaces. In\nInternational Conference on Machine Learning, pages 4476–4486, 2021. 144, 279\n360. Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes ImageNet good for transfer\nlearning? arXiv preprint arXiv:1608.08614, 2016. 249, 272\n361. Jonathan Hui. RL—DQN Deep Q-network https://medium.com/@jonathan_hui/rl-dqn-\ndeep-q-network-e207751f7ae4. Medium post. 88\n362. Jonathan Hui. Model-based reinforcement learning https://medium.com/@jonathan_hui/\nrl-model-based-reinforcement-learning-3c2b6f0aa323. Medium post, 2018. 133\n363. Mike Huisman, Jan van Rijn, and Aske Plaat. Metalearning for deep neural networks. In\nPavel Brazdil et al., editors, Metalearning: Applications to data mining. Springer, 2022. 251,\n262, 263, 271, 272\n364. Mike Huisman, Jan N. van Rijn, and Aske Plaat. A survey of deep meta-learning. Artiﬁcial\nIntelligence Review, 2021. 252, 257, 258, 259, 260, 261, 271, 272, 383\n365. Matthew Hutson. Artiﬁcial Intelligence faces reproducibility crisis. Science, 359:725–726,\n2018. 91\n366. Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization\nfor general algorithm conﬁguration. In International Conference on Learning and Intelligent\nOptimization, pages 507–523. Springer, 2011. 263\n367. Frank Hutter, Holger H Hoos, Kevin Leyton-Brown, and Thomas Stützle. ParamILS: an\nautomatic algorithm conﬁguration framework. Journal of Artiﬁcial Intelligence Research,\n36:267–306, 2009. 263\n368. Frank Hutter, Lars Kotthoﬀ, and Joaquin Vanschoren. Automated Machine Learning: Methods,\nSystems, Challenges. Springer Nature, 2019. 264\n369. Roman Ilin, Robert Kozma, and Paul J Werbos. Eﬃcient learning in cellular simultaneous\nrecurrent neural networks—the case of maze navigation problem. In 2007 IEEE International\nSymposium on Approximate Dynamic Programming and Reinforcement Learning, pages 324–\n329, 2007. 139, 143, 148\n370. Sergey Ioﬀe. Batch renormalization: Towards reducing minibatch dependence in batch-\nnormalized models. In Advances in Neural Information Processing Systems, pages 1945–1953,\n2017. 325\n371. Riashat Islam, Peter Henderson, Maziar Gomrokchi, and Doina Precup. Reproducibility\nof benchmarked deep reinforcement learning tasks for continuous control. arXiv preprint\narXiv:1708.04133, 2017. 91\nReferences\n357\n372. Athul Paul Jacob, David J Wu, Gabriele Farina, Adam Lerer, Anton Bakhtin, Jacob Andreas,\nand Noam Brown. Modeling strong and human-like gameplay with KL-regularized search.\narXiv preprint arXiv:2112.07544, 2021. 174\n373. Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia\nCastañeda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas\nSonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray\nKavukcuoglu, and Thore Graepel. Human-level performance in 3D multiplayer games with\npopulation-based reinforcement learning. Science, 364(6443):859–865, 2019. 72, 73, 218, 222,\n281\n374. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, JeﬀDonahue,\nAli Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando,\nand Koray Kavukcuoglu. Population based training of neural networks. arXiv preprint\narXiv:1711.09846, 2017. 216, 217, 223, 227, 269, 281, 337\n375. Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. RLbench: the\nrobot learning benchmark & learning environment. IEEE Robotics and Automation Letters,\n5(2):3019–3026, 2020. 118\n376. Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:\nModel-based policy optimization. In Advances in Neural Information Processing Systems,\npages 12498–12509, 2019. 133, 136, 142, 148\n377. Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence\nmodeling problem. arXiv preprint arXiv:2106.02039, 2021. 324\n378. Andrew A Jawlik. Statistics from A to Z: Confusing concepts clariﬁed. John Wiley & Sons,\n2016. 292\n379. Michael Johanson, Nolan Bard, Marc Lanctot, Richard G Gibson, and Michael Bowling.\nEﬃcient Nash equilibrium approximation through Monte Carlo counterfactual regret mini-\nmization. In AAMAS, pages 837–846, 2012. 207, 208, 227\n380. Ian T Jolliﬀe and Jorge Cadima. Principal component analysis: a review and recent de-\nvelopments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and\nEngineering Sciences, 374(2065):20150202, 2016. 15\n381. Michael Irwin Jordan. Learning in Graphical Models, volume 89. Springer Science & Business\nMedia, 1998. 282\n382. Arthur Juliani. Simple reinforcement learning with tensorﬂow part 8: Asynchronous actor-\ncritic agents (A3C) https://medium.com/emergent-future/simple-reinforcement-\nlearning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-\nc88f72a5e9f2, 2016. 107, 108\n383. Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen, Jonathan Harper, Chris\nElion, Chris Goy, Yuan Gao, Hunter Henry, Marwan Mattar, and Danny Lange. Unity: A\ngeneral platform for intelligent agents. arXiv preprint arXiv:1809.02627, 2018. 269, 336\n384. Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter\nHenry, Adam Crespi, Julian Togelius, and Danny Lange. Obstacle tower: A generalization\nchallenge in vision, control, and planning. arXiv preprint arXiv:1902.01378, 2019. 227\n385. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-\nneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex\nBridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino\nRomera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen,\nDavid Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas\nBerghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray\nKavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure\nprediction with AlphaFold. Nature, 596(7873):583–589, 2021. 192, 279\n386. Andreas Junghanns and Jonathan Schaeﬀer. Sokoban: Enhancing general single-agent search\nmethods using domain knowledge. Artiﬁcial Intelligence, 129(1-2):219–251, 2001. 27, 60\n387. Niels Justesen, Philip Bontrager, Julian Togelius, and Sebastian Risi. Deep learning for video\ngame playing. IEEE Transactions on Games, 12(1):1–20, 2019. 91\n358\nReferences\n388. Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius,\nand Sebastian Risi. Illuminating generalization in deep reinforcement learning through\nprocedural level generation. arXiv preprint arXiv:1806.10729, 2018. 181\n389. Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A\nsurvey. Journal of Artiﬁcial Intelligence Research, 4:237–285, 1996. 7, 62\n390. Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk.\nIn Handbook of the Fundamentals of Financial Decision Making: Part I, pages 99–127. World\nScientiﬁc, 2013. 227\n391. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad\nCzechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Ryan Sepassi,\nGeorge Tucker, and Henryk Michalewski. Model-based reinforcement learning for Atari.\narXiv:1903.00374, 2019. 134, 137, 142, 148\n392. Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin L. Edelman, Tristan Yang,\nBoaz Barak, and Haofeng Zhang. SGD on neural networks learns functions of increasing\ncomplexity. In Advances in Neural Information Processing Systems, pages 3491–3501, 2019.\n325\n393. Gabriel Kalweit and Joschka Boedecker. Uncertainty-driven imagination for continuous deep\nreinforcement learning. In Conference on Robot Learning, pages 195–206, 2017. 136\n394. Reza Kamyar and Ehsan Taheri. Aircraft optimal terrain/threat-based trajectory planning\nand control. Journal of Guidance, Control, and Dynamics, 37(2):466–483, 2014. 137\n395. Satwik Kansal and Brendan Martin. Learn data science webpage., 2018. 42, 56, 57, 58, 65, 130\n396. Hilbert J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal\nof statistical mechanics: theory and experiment, 2005(11):P11011, 2005. 111\n397. Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent\nexperience replay in distributed reinforcement learning. In International Conference on\nLearning Representations, 2018. 89\n398. Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick Van der Smagt. Deep vari-\national Bayes ﬁlters: Unsupervised learning of state space models from raw data. arXiv\npreprint arXiv:1605.06432, 2016. 135\n399. Andrej Karpathy.\nThe unreasonable eﬀectiveness of recurrent neural networks. http:\n//karpathy.github.io/2015/05/21/rnn-effectiveness/. Andrej Karpathy Blog, 2015.\n318, 319\n400. Andrej Karpathy.\nDeep reinforcement learning: Pong from pixels. http://karpathy.\ngithub.io/2016/05/31/rl/. Andrej Karpathy Blog, 2016. 84\n401. Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent\nnetworks. arXiv preprint arXiv:1506.02078, 2015. 146\n402. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs\nfor improved quality, stability, and variation. In International Conference on Learning Repre-\nsentations, 2018. 320, 322\n403. Henry J Kelley. Gradient theory of optimal ﬂight paths. American Rocket Society Journal,\n30(10):947–954, 1960. 139, 143, 148\n404. Stephen Kelly and Malcolm I Heywood. Multi-task learning in Atari video games with\nemergent tangled program graphs. In Proceedings of the Genetic and Evolutionary Computation\nConference, pages 195–202. ACM, 2017. 255\n405. Stephen Kelly and Malcolm I Heywood. Emergent tangled program graphs in multi-task\nlearning. In IJCAI, pages 5294–5298, 2018. 255\n406. James Kennedy. Swarm intelligence. In Handbook of Nature-Inspired and Innovative Comput-\ning, pages 187–219. Springer, 2006. 11, 227\n407. Pascal Kerschke, Holger H Hoos, Frank Neumann, and Heike Trautmann. Automated\nalgorithm selection: Survey and perspectives. Evolutionary Computation, 27(1):3–45, 2019.\n263\n408. Shauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel, Evren Tumer, Santiago\nMiret, Yinyin Liu, and Kagan Tumer. Collaborative evolutionary reinforcement learning. In\nInternational Conference on Machine Learning, pages 3341–3350. PMLR, 2019. 227, 281\nReferences\n359\n409. Shauharda Khadka and Kagan Tumer. Evolutionary reinforcement learning. arXiv preprint\narXiv:1805.07917, 2018. 227\n410. Khimya Khetarpal, Zafarali Ahmed, Andre Cianﬂone, Riashat Islam, and Joelle Pineau. Re-\nevaluate: Reproducibility in evaluating reinforcement learning algorithms. In Reproducibility\nin Machine Learning Workshop, ICML, 2018. 91\n411. Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International\nConference on Learning Representations, 2014. 15, 135, 321\n412. Diederik P Kingma and Max Welling. An introduction to variational autoencoders. Found.\nTrends Mach. Learn., 12(4):307–392, 2019. 15, 135, 321\n413. Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. A survey of generali-\nsation in deep reinforcement learning. arXiv preprint arXiv:2111.09794, 2021. 283\n414. Daan Klijn and AE Eiben. A coevolutionairy approach to deep multi-agent reinforcement\nlearning. arXiv preprint arXiv:2104.05610, 2021. 225, 227\n415. Craig A Knoblock. Learning abstraction hierarchies for problem solving. In AAAI, pages\n923–928, 1990. 235, 236\n416. Donald E Knuth and Ronald W Moore. An analysis of alpha-beta pruning. Artiﬁcial Intelli-\ngence, 6(4):293–326, 1975. 155, 165, 167\n417. Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.\nThe International Journal of Robotics Research, 32(11):1238–1274, 2013. 115, 147, 148\n418. Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for\none-shot image recognition. In ICML Deep Learning workshop, volume 2. Lille, 2015. 272\n419. Levente Kocsis and Csaba Szepesvári. Bandit based Monte-Carlo planning. In European\nConference on Machine Learning, pages 282–293. Springer, 2006. 168, 171, 172, 191\n420. Vijay R Konda and John N Tsitsiklis.\nActor–critic algorithms.\nIn Advances in Neural\nInformation Processing Systems, pages 1008–1014, 2000. 103\n421. Vijaymohan R Konda and Vivek S Borkar. Actor–Critic-type learning algorithms for Markov\nDecision Processes. SIAM Journal on Control and Optimization, 38(1):94–123, 1999. 103\n422. Richard E Korf. Depth-ﬁrst iterative-deepening: An optimal admissible tree search. Artiﬁcial\nintelligence, 27(1):97–109, 1985. 167\n423. Petar Kormushev, Sylvain Calinon, and Darwin G Caldwell. Robot motor skill coordination\nwith em-based reinforcement learning. In 2010 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, pages 3232–3237. IEEE, 2010. 5, 6\n424. Satwik Kottur, José MF Moura, Stefan Lee, and Dhruv Batra. Natural language does not\nemerge ’naturally’ in multi-agent dialog. In Proceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2017, Copenhagen, pages 2962–2967, 2017.\n212\n425. Samuel Kotz, Narayanaswamy Balakrishnan, and Norman L Johnson. Continuous Multivariate\nDistributions, Volume 1: Models and Applications. John Wiley & Sons, 2004. 53, 110\n426. Basil Kouvaritakis and Mark Cannon. Model Predictive Control. Springer, 2016. 148\n427. Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal\nfor decentralized planning. Neurocomputing, 190:82–94, 2016. 211\n428. Mark A Kramer. Nonlinear principal component analysis using autoassociative neural\nnetworks. AIChE journal, 37(2):233–243, 1991. 321\n429. Sarit Kraus, Eithan Ephrati, and Daniel Lehmann. Negotiation in a non-cooperative envi-\nronment. Journal of Experimental & Theoretical Artiﬁcial Intelligence, 3(4):255–281, 1994.\n155\n430. Sarit Kraus and Daniel Lehmann. Diplomat, an agent in a multi agent environment: An\noverview. In IEEE International Performance Computing and Communications Conference,\npages 434–438, 1988. 212\n431. Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in Neural Information Processing Systems, pages\n1097–1105, 2012. 69, 250, 311, 312, 327\n432. Kai A Krueger and Peter Dayan. Flexible shaping: How learning in small steps helps.\nCognition, 110(3):380–394, 2009. 180\n360\nReferences\n433. Steven Kuhn. Prisoner’s Dilemma. The Stanford Encyclopedia of Philosophy, https://\nplato.stanford.edu/entries/prisoner-dilemma/, 1997. 202\n434. Jan Kuipers, Aske Plaat, Jos AM Vermaseren, and H Jaap van den Herik. Improving multi-\nvariate Horner schemes with Monte Carlo tree search. Computer Physics Communications,\n184(11):2391–2395, 2013. 173, 174\n435. Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical\ndeep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In\nAdvances in Neural Information Processing Systems, pages 3675–3683, 2016. 233, 235, 241, 242,\n245\n436. Solomon Kullback and Richard A Leibler. On information and suﬃciency. The Annals of\nMathematical Statistics, 22(1):79–86, 1951. 109\n437. Yen-Ling Kuo, Boris Katz, and Andrei Barbu. Encoding formulas as deep networks: Rein-\nforcement learning for zero-shot execution of LTL formulas. arXiv preprint arXiv:2006.01110,\n2020. 272\n438. Karol Kurach, Anton Raichuk, Piotr Stańczyk, Michał Zajac, Olivier Bachem, Lasse Espeholt,\nCarlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly.\nGoogle research football: A novel reinforcement learning environment. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 34, pages 4501–4510, 2020. 225, 336\n439. Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble\ntrust-region policy optimization. In International Conference on Learning Representations,\n2018. 133, 148\n440. W Hi Kwon, AM Bruckstein, and T Kailath. Stabilizing state-feedback design via the moving\nhorizon method. International Journal of Control, 37(3):631–643, 1983. 137\n441. Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine\nLearning Research, 4:1107–1149, Dec 2003. 91\n442. Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. The\nAnnals of Statistics, pages 1091–1114, 1987. 52\n443. Tze Leung Lai and Herbert Robbins. Asymptotically eﬃcient adaptive allocation rules.\nAdvances in Applied Mathematics, 6(1):4–22, 1985. 51, 52\n444. John E Laird, Paul S Rosenbloom, and Allen Newell. Chunking in Soar: the anatomy of a\ngeneral learning mechanism. Machine learning, 1(1):11–46, 1986. 234, 245\n445. Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning\nof simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science Society,\nvolume 33, 2011. 257, 266, 336\n446. Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept\nlearning through probabilistic program induction. Science, 350(6266):1332–1338, 2015. 266\n447. Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. The Omniglot challenge:\na 3-year progress report. Current Opinion in Behavioral Sciences, 29:97–104, 2019. 266\n448. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable\npredictive uncertainty estimation using deep ensembles. In Advances in Neural Information\nProcessing Systems, pages 6402–6413, 2017. 137\n449. Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen\nobject classes by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision\nand Pattern Recognition, pages 951–958. IEEE, 2009. 265, 272\n450. Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinícius Flores Zambaldi, Satyaki\nUpadhyay, Julien Pérolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omid-\nshaﬁei, Daniel Hennes, Dustin Morrill, Paul Muller, Timo Ewalds, Ryan Faulkner, János\nKramár, Bart De Vylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud,\nMatthew Lai, Julian Schrittwieser, Thomas W. Anthony, Edward Hughes, Ivo Danihelka, and\nJonah Ryan-Davis. Openspiel: A framework for reinforcement learning in games. arXiv\npreprint arXiv:1908.09453, 2019. 336\n451. Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H Bowling. Monte carlo sampling\nfor regret minimization in extensive games. In Advances in Neural Information Processing\nSystems, pages 1078–1086, 2009. 200, 207, 208, 227\nReferences\n361\n452. Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien\nPérolat, David Silver, and Thore Graepel. A uniﬁed game-theoretic approach to multiagent\nreinforcement learning. In Advances in Neural Information Processing Systems, pages 4190–\n4203, 2017. 198\n453. Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement\nlearning. In The 2010 International Joint Conference on Neural Networks (IJCNN), pages 1–8.\nIEEE, 2010. 78\n454. Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zero-data learning of new tasks. In\nAAAI, volume 1, page 3, 2008. 264, 272\n455. Alexandre Laterre, Yunguan Fu, Mohamed Khalil Jabri, Alain-Sam Cohen, David Kas, Karl\nHajjar, Torbjorn S Dahl, Amine Kerkeni, and Karim Beguir. Ranked reward: Enabling self-play\nreinforcement learning for combinatorial optimization. arXiv preprint arXiv:1807.01672, 2018.\n181, 182, 192, 279\n456. Jean-Claude Latombe. Robot Motion Planning, volume 124. Springer Science & Business\nMedia, 2012. 26, 60\n457. Steﬀen L Lauritzen. Graphical Models, volume 17. Clarendon Press, 1996. 282\n458. Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and\nthe emergence of (natural) language. In International Conference on Learning Representations,\n2017. 212\n459. Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, 521(7553):436,\n2015. 11, 12, 90, 248, 311\n460. Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne\nHubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recogni-\ntion. Neural Computation, 1(4):541–551, 1989. 315\n461. Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based learning\napplied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 312, 313,\n326\n462. Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief\nnetworks for scalable unsupervised learning of hierarchical representations. In Proceedings\nof the 26th Annual International Conference on Machine Learning, pages 609–616. ACM, 2009.\n311, 312\n463. Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric\nand subspace. In International Conference on Machine Learning, pages 2927–2936. PMLR,\n2018. 259\n464. Joel Z Leibo, Edward Hughes, Marc Lanctot, and Thore Graepel. Autocurricula and the\nemergence of innovation from social interaction: A manifesto for multi-agent intelligence\nresearch. arXiv preprint arXiv:1903.00742, 2019. 191, 216, 221, 279\n465. Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multi-\nagent reinforcement learning in sequential social dilemmas. In Proceedings of the 16th\nConference on Autonomous Agents and MultiAgent Systems, AAMAS 2017, São Paulo, Brazil,\npages 464–473, 2017. 210, 212, 213, 220\n466. Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N.\nGanmukhi, Jeﬀrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre,\nDavid S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert C. Zak. The network architec-\nture of the connection machine CM-5. In Proceedings of the fourth annual ACM Symposium\non Parallel Algorithms and Architectures, pages 272–285, 1992. 328\n467. Matteo Leonetti, Luca Iocchi, and Peter Stone. A synthesis of automated planning and\nreinforcement learning for eﬃcient, robust decision-making. Artiﬁcial Intelligence, 241:103–\n130, 2016. 136\n468. Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search\nunder unknown dynamics. In Advances in Neural Information Processing Systems, pages\n1071–1079, 2014. 142, 148\n469. Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on\nMachine Learning, pages 1–9, 2013. 132, 148\n362\nReferences\n470. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Oﬄine reinforcement learning:\nTutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n255\n471. Andrew Levy, George Konidaris, Robert Platt, and Kate Saenko. Learning multi-level hierar-\nchies with hindsight. In International Conference on Learning Representations, 2019. 235, 237,\n240, 243, 245, 280\n472. Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the\nloss landscape of neural nets. In Advances in Neural Information Processing Systems, pages\n6391–6401, 2018. 146\n473. Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441,\n2017. 272\n474. Sheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deep\nimplicit coordination graphs for multi-agent reinforcement learning. In AAMAS ’21: 20th\nInternational Conference on Autonomous Agents and Multiagent Systems, 2021. 211\n475. Siyuan Li, Rui Wang, Minxue Tang, and Chongjie Zhang. Hierarchical reinforcement learning\nwith advantage-based auxiliary rewards. In Advances in Neural Information Processing Systems,\npages 1407–1417, 2019. 280\n476. Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-SGD: learning to learn quickly for\nfew-shot learning. arXiv preprint arXiv:1707.09835, 2017. 258, 272\n477. Zhuoru Li, Akshay Narayan, and Tze-Yun Leong. An eﬃcient approach to model-based\nhierarchical reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 31, 2017. 235\n478. Eric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E\nGonzalez, Michael I Jordan, and Ion Stoica. RLlib: abstractions for distributed reinforcement\nlearning. In International Conference on Machine Learning, pages 3059–3068, 2018. 336\n479. Diego Pérez Liébana, Simon M Lucas, Raluca D Gaina, Julian Togelius, Ahmed Khalifa,\nand Jialin Liu. General video game artiﬁcial intelligence. Synthesis Lectures on Games and\nComputational Intelligence, 3(2):1–191, 2019. 181\n480. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In\nInternational Conference on Learning Representations, 2016. 96, 99, 112, 113, 120, 144, 276\n481. Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and\nteaching. Machine Learning, 8(3-4):293–321, 1992. 79, 91\n482. Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report,\nCarnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993. 78, 79\n483. Michael L Littman. Markov games as a framework for multi-agent reinforcement learning.\nIn Machine Learning Proceedings 1994, pages 157–163. Elsevier, 1994. 28, 198, 226\n484. Chunming Liu, Xin Xu, and Dewen Hu. Multiobjective reinforcement learning: A comprehen-\nsive overview. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45(3):385–398,\n2014. 227\n485. Hao Liu and Pieter Abbeel. Hybrid discriminative-generative training via contrastive learning.\narXiv preprint arXiv:2007.09070, 2020. 77\n486. Hui Liu, Song Yu, Zhangxin Chen, Ben Hsieh, and Lei Shao. Sparse matrix-vector multipli-\ncation on NVIDIA GPU. International Journal of Numerical Analysis & Modeling, Series B,\n3(2):185–191, 2012. 328\n487. Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, and Thore Grae-\npel. Emergent coordination through competition. In International Conference on Learning\nRepresentations, 2019. 216, 227\n488. Manuel López-Ibáñez, Jérémie Dubois-Lacoste, Leslie Pérez Cáceres, Mauro Birattari, and\nThomas Stützle. The irace package: Iterated racing for automatic algorithm conﬁguration.\nOperations Research Perspectives, 3:43–58, 2016. 263\n489. Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent\nActor-Critic for mixed cooperative-competitive environments. In Advances in Neural Infor-\nmation Processing Systems, pages 6379–6390, 2017. 211, 217, 337\nReferences\n363\n490. Gabriel Loye.\nThe attention mechanism. https://blog.floydhub.com/attention-\nmechanism/. 323\n491. Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the\neﬀectiveness of SGD in modern over-parametrized learning. In International Conference on\nMachine Learning, pages 3331–3340, 2018. 325\n492. Xiaoliang Ma, Xiaodong Li, Qingfu Zhang, Ke Tang, Zhengping Liang, Weixin Xie, and\nZexuan Zhu. A survey on cooperative co-evolutionary algorithms. IEEE Transactions on\nEvolutionary Computation, 23(3):421–441, 2018. 214\n493. Laurens van der Maaten and Geoﬀrey Hinton. Visualizing data using t-SNE. Journal of\nMachine Learning Research, 9:2579–2605, Nov 2008. 15\n494. Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and\nMichael Bowling. Revisiting the arcade learning environment: Evaluation protocols and\nopen problems for general agents. Journal of Artiﬁcial Intelligence Research, 61:523–562, 2018.\n90\n495. Hamid Reza Maei, Csaba Szepesvári, Shalabh Bhatnagar, and Richard S Sutton. Toward\noﬀ-policy learning control with function approximation. In International Conference on\nMachine Learning, 2010. 78\n496. Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-\nagent variational exploration. In Advances in Neural Information Processing Systems, pages\n7611–7622, 2019. 211\n497. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan\nLi, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised\npretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pages\n181–196, 2018. 248\n498. Somdeb Majumdar, Shauharda Khadka, Santiago Miret, Stephen McAleer, and Kagan Tumer.\nEvolutionary reinforcement learning for sample-eﬃcient multiagent coordination. In Inter-\nnational Conference on Machine Learning, 2020. 227\n499. Rajbala Makar, Sridhar Mahadevan, and Mohammad Ghavamzadeh. Hierarchical multi-agent\nreinforcement learning. In Proceedings of the Fifth International Conference on Autonomous\nAgents, pages 246–253. ACM, 2001. 242\n500. Julian N Marewski, Wolfgang Gaissmaier, and Gerd Gigerenzer. Good judgments do not\nrequire complex cognition. Cognitive Processing, 11(2):103–121, 2010. 213\n501. Vince Martinelli.\nHow robots autonomously see, grasp, and pick. https://www.\ntherobotreport.com/grasp-sight-picking-evolve-robots/, 2019. 97\n502. Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum\nlearning. IEEE Trans. Neural Networks Learn. Syst., 31(9):3732–3740, 2020. 192\n503. Masakazu Matsugu, Katsuhiko Mori, Yusuke Mitari, and Yuji Kaneda. Subject independent\nfacial expression recognition with robust face detection using a convolutional neural network.\nNeural Networks, 16(5-6):555–559, 2003. 314\n504. Kiminori Matsuzaki. Empirical analysis of PUCT algorithm with evaluation functions of\ndiﬀerent quality. In 2018 Conference on Technologies and Applications of Artiﬁcial Intelligence\n(TAAI), pages 142–147. IEEE, 2018. 172, 173\n505. David Q Mayne, James B Rawlings, Christopher V Rao, and Pierre OM Scokaert. Constrained\nmodel predictive control: Stability and optimality. Automatica, 36(6):789–814, 2000. 148\n506. James L McClelland, Bruce L McNaughton, and Randall C O’Reilly. Why there are comple-\nmentary learning systems in the hippocampus and neocortex: insights from the successes and\nfailures of connectionist models of learning and memory. Psychological Review, 102(3):419,\n1995. 79\n507. Francisco S Melo and M Isabel Ribeiro. Convergence of Q-learning with linear function\napproximation. In 2007 European Control Conference (ECC), pages 2671–2678. IEEE, 2007. 76\n508. Josh Merel, Arun Ahuja, Vu Pham, Saran Tunyasuvunakool, Siqi Liu, Dhruva Tirumala, Nico-\nlas Heess, and Greg Wayne. Hierarchical visuomotor control of humanoids. In International\nConference on Learning Representations, 2019. 118\n364\nReferences\n509. Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, and Bence Ölveczky.\nDeep neuroethology of a virtual rodent. In International Conference on Learning Representa-\ntions, 2020. 118\n510. Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne,\nYee Whye Teh, and Nicolas Heess. Neural probabilistic motor primitives for humanoid\ncontrol. In International Conference on Learning Representations, 2019. 118\n511. Josh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg\nWayne, and Nicolas Heess. Learning human behaviors from motion capture by adversarial\nimitation. arXiv preprint arXiv:1707.02201, 2017. 118\n512. Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink, Olivier Francon,\nBala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duﬀy, and Babak Hodjat. Evolving\ndeep neural networks. In Artiﬁcial Intelligence in the Age of Neural Networks and Brain\nComputing, pages 293–312. Elsevier, 2019. 281\n513. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word\nrepresentations in vector space. In International Conference on Learning Representations, 2013.\n249, 253\n514. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Distributed\nrepresentations of words and phrases and their compositionality. In Advances in Neural\nInformation Processing Systems, 2013. 249\n515. Jonathan K Millen. Programming the game of Go. Byte Magazine, 1981. 157\n516. S Ali Mirsoleimani, Aske Plaat, Jaap Van Den Herik, and Jos Vermaseren. Scaling Monte\nCarlo tree search on Intel Xeon Phi. In Parallel and Distributed Systems (ICPADS), 2015 IEEE\n21st International Conference on, pages 666–673. IEEE, 2015. 191\n517. Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive\nmeta-learner. In International Conference on Learning Representations, 2018. 258, 272\n518. Tom M Mitchell. The need for biases in learning generalizations. Technical Report CBM-TR-\n117, Department of Computer Science, Rutgers University, 1980. 182\n519. Tom M Mitchell. The discipline of machine learning. Technical Report CMU-ML-06-108,\nCarnegie Mellon University, School of Computer Science, Machine Learning, 2006. 182\n520. Akshita Mittel and Purna Sowmya Munukutla. Visual transfer between Atari games using\ncompetitive reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, pages 0–0, 2019. 268\n521. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,\nTim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep rein-\nforcement learning. In International Conference on Machine Learning, pages 1928–1937, 2016.\n99, 107, 108, 109, 120, 276\n522. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv\npreprint arXiv:1312.5602, 2013. 41, 70, 76, 78, 79, 82, 83, 84, 87, 91, 154\n523. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig\nPetersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,\nDaan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-\nforcement learning. Nature, 518(7540):529–533, 2015. 7, 70, 78, 79, 80, 81, 82, 83, 87, 88, 91,\n183, 276\n524. Thomas Moerland. Continuous Markov decision process and policy search. Lecture notes\nfor the course reinforcement learning, Leiden University, 2021. 28, 76, 110, 112, 114, 287, 302\n525. Thomas M Moerland. The Intersection of Planning and Learning. PhD thesis, Delft University\nof Technology, 2021. 279\n526. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Eﬃcient exploration with\ndouble uncertain value networks. arXiv preprint arXiv:1711.10789, 2017. 87\n527. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. The potential of the return\ndistribution for exploration in RL. arXiv preprint arXiv:1806.04242, 2018. 87\n528. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. A framework for reinforcement\nlearning and planning. arXiv preprint arXiv:2006.15009, 2020. 131\nReferences\n365\n529. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Model-based reinforcement\nlearning: A survey. arXiv preprint arXiv:2006.16712, 2020. 148\n530. Thomas M Moerland, Joost Broekens, Aske Plaat, and Catholijn M Jonker. A0C: Alpha zero\nin continuous action space. arXiv preprint arXiv:1805.09613, 2018. 173, 191\n531. Thomas M Moerland, Joost Broekens, Aske Plaat, and Catholijn M Jonker. Monte Carlo tree\nsearch for asymmetric trees. arXiv preprint arXiv:1805.09218, 2018. 191\n532. Andrew William Moore. Eﬃcient memory-based learning for robot control. Technical\nReport UCAM-CL-TR-209, University of Cambridge, UK, https://www.cl.cam.ac.uk/\ntechreports/UCAM-CL-TR-209.pdf, 1990. 59\n533. Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan Bard, Trevor\nDavis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level\nartiﬁcial intelligence in heads-up no-limit poker. Science, 356(6337):508–513, 2017. 218, 220,\n227\n534. Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-\nagent populations. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32,\n2018. 216\n535. Pol Moreno, Edward Hughes, Kevin R McKee, Bernardo Avila Pires, and Théophane We-\nber. Neural recursive belief states in multi-agent reinforcement learning. arXiv preprint\narXiv:2102.02274, 2021. 212\n536. David E Moriarty, Alan C Schultz, and John J Grefenstette. Evolutionary algorithms for\nreinforcement learning. Journal of Artiﬁcial Intelligence Research, 11:241–276, 1999. 227\n537. Hossam Mossalam, Yannis M Assael, Diederik M Roijers, and Shimon Whiteson. Multi-\nobjective deep reinforcement learning. arXiv preprint arXiv:1610.02707, 2016. 227\n538. Hussain Mujtaba. Introduction to autoencoders. https://www.mygreatlearning.com/\nblog/autoencoder/, 2020. 322\n539. Sendhil Mullainathan and Richard H Thaler. Behavioral economics. Technical report,\nNational Bureau of Economic Research, 2000. 227\n540. Martin Müller. Computer Go. Artiﬁcial Intelligence, 134(1-2):145–179, 2002. 73\n541. Matthias Müller-Brockhausen, Mike Preuss, and Aske Plaat. Procedural content generation:\nBetter benchmarks for transfer reinforcement learning. In Conference on Games, 2021. 180,\n272\n542. Tsendsuren Munkhdalai and Hong Yu. Meta networks. In International Conference on Machine\nLearning, pages 2554–2563. PMLR, 2017. 258, 272\n543. Yoshio Murase, Hitoshi Matsubara, and Yuzuru Hiraga. Automatic making of Sokoban\nproblems. In Paciﬁc Rim International Conference on Artiﬁcial Intelligence, pages 592–600.\nSpringer, 1996. 27, 60\n544. Derick Mwiti.\nReinforcement learning applications. https://neptune.ai/blog/\nreinforcement-learning-applications. 4\n545. Roger B Myerson. Game Theory. Harvard university press, 2013. 226\n546. Oﬁr Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-eﬃcient hierarchical\nreinforcement learning. In Advances in Neural Information Processing Systems, pages 3307–\n3317, 2018. 235, 237, 245, 280\n547. Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap\nbetween value and policy based reinforcement learning. In Advances in Neural Information\nProcessing Systems, pages 2775–2785, 2017. 111\n548. Prakash M Nadkarni, Lucila Ohno-Machado, and Wendy W Chapman. Natural language pro-\ncessing: an introduction. Journal of the American Medical Informatics Association, 18(5):544–\n551, 2011. 280\n549. Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network\ndynamics for model-based deep reinforcement learning with model-free ﬁne-tuning. In 2018\nIEEE International Conference on Robotics and Automation (ICRA), pages 7559–7566, 2018.\n137, 148, 150\n550. Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.\nDeep double descent: Where bigger models and more data. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, 2020. 325\n366\nReferences\n551. Nantas Nardelli, Gabriel Synnaeve, Zeming Lin, Pushmeet Kohli, Philip HS Torr, and Nicolas\nUsunier. Value propagation networks. In 7th International Conference on Learning Represen-\ntations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2018. 142, 148\n552. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter\nStone. Curriculum learning for reinforcement learning domains: A framework and survey.\nJournal Machine Learning Research, 2020. 182, 279\n553. Sylvia Nasar. A Beautiful Mind. Simon and Schuster, 2011. 226\n554. John Nash. Non-cooperative games. Annals of mathematics, pages 286–295, 1951. 226\n555. John F Nash. Equilibrium points in 𝑛-person games. Proceedings of the National Academy of\nSciences, 36(1):48–49, 1950. 226\n556. John F Nash Jr. The bargaining problem. Econometrica: Journal of the econometric society,\npages 155–162, 1950. 226\n557. Yu Nasu. Eﬃciently updatable neural-network-based evaluation functions for computer\nshogi. The 28th World Computer Shogi Championship Appeal Document, 2018. 166\n558. Richard E Neapolitan. Learning Bayesian networks. Pearson Prentice Hall, Upper Saddle\nRiver, NJ, 2004. 282\n559. Andrew Y Ng. Feature selection, L1 vs. L2 regularization, and rotational invariance. In\nProceedings of the Twenty-ﬁrst International Conference on Machine Learning, page 78. ACM,\n2004. 324\n560. Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transfor-\nmations: Theory and application to reward shaping. In International Conference on Machine\nLearning, volume 99, pages 278–287, 1999. 55\n561. Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms.\narXiv preprint arXiv:1803.02999, 2018. 258, 263, 272, 273\n562. Sufeng Niu, Siheng Chen, Hanyu Guo, Colin Targonski, Melissa C Smith, and Jelena Kovačević.\nGeneralized value iteration networks: Life beyond lattices. In Thirty-Second AAAI Conference\non Artiﬁcial Intelligence, 2018. 138, 143\n563. Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-\nconditional video prediction using deep networks in Atari games. In Advances in Neural\nInformation Processing Systems, pages 2863–2871, 2015. 142, 148\n564. Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in\nNeural Information Processing Systems, pages 6118–6128, 2017. 134, 135, 139, 142, 144, 148,\n337\n565. Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization\nwith multi-task deep reinforcement learning. In International Conference on Machine Learning,\npages 2661–2670. PMLR, 2017. 272\n566. Kyoung-Su Oh and Keechul Jung. GPU implementation of neural networks. Pattern Recogni-\ntion, 37(6):1311–1314, 2004. 328\n567. Chris Olah. Understanding LSTM networks. http://colah.github.io/posts/2015-08-\nUnderstanding-LSTMs/, 2015. 317\n568. Frans A Oliehoek. Decentralized POMDPs. In Reinforcement Learning, pages 471–503.\nSpringer, 2012. 227\n569. Frans A Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs.\nSpringer, 2016. 155, 201, 216, 227\n570. Frans A Oliehoek, Matthijs TJ Spaan, Christopher Amato, and Shimon Whiteson. Incremental\nclustering and expansion for faster optimal planning in Dec-POMDPs. Journal of Artiﬁcial\nIntelligence Research, 46:449–509, 2013. 216\n571. Shayegan Omidshaﬁei, Jason Pazis, Christopher Amato, Jonathan P How, and John Vian.\nDeep decentralized multi-task multi-agent reinforcement learning under partial observability.\nIn International Conference on Machine Learning, pages 2681–2690. PMLR, 2017. 216\n572. Joseph O’Neill, Barty Pleydell-Bouverie, David Dupret, and Jozsef Csicsvari. Play it again:\nreactivation of waking experience and memory. Trends in Neurosciences, 33(5):220–229, 2010.\n79\nReferences\n367\n573. Santiago Ontanón, Gabriel Synnaeve, Alberto Uriarte, Florian Richoux, David Churchill, and\nMike Preuss. A survey of real-time strategy game AI research and competition in StarCraft.\nIEEE Transactions on Computational Intelligence and AI in Games, 5(4):293–311, 2013. 72, 73,\n223, 227\n574. David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of\nArtiﬁcial Intelligence Research, 11:169–198, 1999. 133, 148\n575. Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva,\nKatrina McKinney, Tor Lattimore, Csaba Szepesvári, Satinder Singh, Benjamin Van Roy,\nRichard S. Sutton, David Silver, and Hado van Hasselt. Behaviour suite for reinforcement\nlearning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 2020. 336\n576. Pierre-Yves Oudeyer and Frederic Kaplan. How can we deﬁne intrinsic motivation? In the 8th\nInternational Conference on Epigenetic Robotics: Modeling Cognitive Development in Robotic\nSystems. Lund University Cognitive Studies, Lund: LUCS, Brighton, 2008. 245\n577. Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? A typology of\ncomputational approaches. Frontiers in Neurorobotics, 1:6, 2009. 241, 245\n578. Pierre-Yves Oudeyer, Frederic Kaplan, and Verena V Hafner. Intrinsic motivation systems for\nautonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2):265–\n286, 2007. 180, 245\n579. Charles Packer, Katelyn Gao, Jernej Kos, Philipp Krähenbühl, Vladlen Koltun, and Dawn Song.\nAssessing generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282,\n2018. 282\n580. Mark M Palatucci, Dean A Pomerleau, Geoﬀrey E Hinton, and Tom Mitchell. Zero-shot\nlearning with semantic output codes. In Advances in Neural Information Processing Systems\n22, 2009. 264, 272\n581. Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on\nKnowledge and Data Engineering, 22(10):1345–1359, 2010. 251, 252, 255, 272\n582. Aleksandr I Panov and Aleksey Skrynnik. Automatic formation of the structure of ab-\nstract machines in hierarchical reinforcement learning with state clustering. arXiv preprint\narXiv:1806.05292, 2018. 245\n583. Giuseppe Davide Paparo, Vedran Dunjko, Adi Makmal, Miguel Angel Martin-Delgado, and\nHans J Briegel. Quantum speedup for active learning agents. Physical Review X, 4(3):031002,\n2014. 192\n584. Philip Paquette, Yuchen Lu, Steven Bocco, Max Smith, O-G Satya, Jonathan K Kummerfeld,\nJoelle Pineau, Satinder Singh, and Aaron C Courville. No-press diplomacy: Modeling multi-\nagent gameplay. In Advances in Neural Information Processing Systems, pages 4476–4487,\n2019. 212, 227\n585. German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter.\nContinual lifelong learning with neural networks: A review. Neural Networks, 113:54–71,\n2019. 252\n586. Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and\ntransfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015. 268\n587. Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayaku-\nmar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing\ntransformers for reinforcement learning. In International Conference on Machine Learning,\npages 7487–7498. PMLR, 2020. 269\n588. Ronald Parr and Stuart J Russell. Reinforcement learning with hierarchies of machines. In\nAdvances in Neural Information Processing Systems, pages 1043–1049, 1998. 235, 236, 245\n589. Gian-Carlo Pascutto. Leela zero. https://github.com/leela-zero/leela-zero, 2017.\n187\n590. Alexander Pashevich, Danijar Hafner, James Davidson, Rahul Sukthankar, and Cordelia\nSchmid. Modulated policy hierarchies. arXiv preprint arXiv:1812.00025, 2018. 235\n591. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\n368\nReferences\nKöpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In Advances in Neural Information Processing Systems,\npages 8024–8035, 2019. 308, 325\n592. Shubham Pateria, Budhitama Subagdja, Ah-hweewee Tan, and Chai Quek. Hierarchical\nreinforcement learning: A comprehensive survey. ACM Computing Surveys (CSUR), 54(5):1–\n35, 2021. 235, 245\n593. Judea Pearl. Heuristics: Intelligent Search Strategies for Computer Problem Solving. Addison-\nWesley, Reading, MA, 1984. 9, 165\n594. Judea Pearl and Dana Mackenzie. The Book of Why: the New Science of Cause and Eﬀect. Basic\nBooks, 2018. 9\n595. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,\nOlivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake\nVanderPlas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and\nEdouard Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning\nResearch, 12(Oct):2825–2830, 2011. 263, 330\n596. Jeﬀrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for\nword representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1532–1543, 2014. 253\n597. Alexandre Péré, Sébastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer. Unsupervised\nlearning of goal spaces for intrinsically motivated goal exploration. In International Conference\non Learning Representations, 2018. 237\n598. Karl Pertsch, Oleh Rybkin, Frederik Ebert, Shenghao Zhou, Dinesh Jayaraman, Chelsea\nFinn, and Sergey Levine. Long-horizon visual planning with goal-conditioned hierarchical\npredictors. In Advances in Neural Information Processing Systems, 2020. 245\n599. Aske Plaat. De vlinder en de mier / The butterﬂy and the ant—on modeling behavior in\norganizations. Inaugural lecture. Tilburg University, 2010. 227\n600. Aske Plaat. Learning to Play: Reinforcement Learning and Games. Springer Verlag, Heidelberg,\nhttps://learningtoplay.net, 2020. 63, 68, 165, 167, 191\n601. Aske Plaat, Walter Kosters, and Mike Preuss. High-accuracy model-based reinforcement\nlearning, a survey. arXiv preprint arXiv:2107.08241, 2021. 132, 142, 148, 383\n602. Aske Plaat, Jonathan Schaeﬀer, Wim Pijls, and Arie De Bruin. Best-ﬁrst ﬁxed-depth minimax\nalgorithms. Artiﬁcial Intelligence, 87(1-2):255–293, 1996. 167\n603. Matthias Plappert. Keras-RL. https://github.com/keras-rl/keras-rl, 2016. 81\n604. Jordan B Pollack and Alan D Blair. Why did TD-gammon work? In Advances in Neural\nInformation Processing Systems, pages 10–16, 1997. 79\n605. Aditya Prasad. Lessons from implementing alphazero https://medium.com/oracledevs/\nlessons-from-implementing-alphazero-7e36e9054191, 2018. 177\n606. Lorien Y Pratt. Discriminability-based transfer between neural networks. In Advances in\nNeural Information Processing Systems, pages 204–211, 1993. 252, 272\n607. Lutz Prechelt. Automatic early stopping using cross validation: quantifying the criteria.\nNeural Networks, 11(4):761–767, 1998. 325\n608. Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55–69.\nSpringer, 1998. 325\n609. Doina Precup, Richard S Sutton, and Satinder P Singh. Planning with closed-loop macro\nactions. In Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous\nSystems, pages 70–76, 1997. 234, 245\n610. David Premack and Guy Woodruﬀ. Does the chimpanzee have a theory of mind? Behavioral\nand Brain Sciences, 1(4):515–526, 1978. 212\n611. Hugo M Proença and Matthijs van Leeuwen. Interpretable multiclass classiﬁcation by\nmdl-based rule lists. Information Sciences, 512:1372–1393, 2020. 282\n612. Max Pumperla and Kevin Ferguson. Deep Learning and the Game of Go. Manning, 2019. 191\n613. J Ross Quinlan. Learning eﬃcient classiﬁcation procedures and their application to chess\nend games. In Machine Learning, pages 463–482. Springer, 1983. 166\nReferences\n369\n614. J Ross Quinlan. Induction of decision trees. Machine Learning, 1(1):81–106, 1986. 282\n615. Sébastien Racanière, Theophane Weber, David P. Reichert, Lars Buesing, Arthur Guez,\nDanilo Jimenez Rezende, Adrià Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yu-\njia Li, Razvan Pascanu, Peter W. Battaglia, Demis Hassabis, David Silver, and Daan Wierstra.\nImagination-augmented agents for deep reinforcement learning. In Advances in Neural\nInformation Processing Systems, pages 5690–5701, 2017. 60, 142, 143\n616. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. In\nInternational Conference on Machine Learning, 2021. 280, 281, 324\n617. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\nImproving lan-\nguage understanding by generative pre-training. https://openai.com/blog/language-\nunsupervised/, 2018. 249, 266, 267, 324\n618. Roxana Rădulescu, Patrick Mannion, Diederik M Roijers, and Ann Nowé. Multi-objective\nmulti-agent decision making: a utility-based analysis and survey. Autonomous Agents and\nMulti-Agent Systems, 34(1):1–52, 2020. 202\n619. Jacob Rafati and David C Noelle. Learning representations in model-free hierarchical rein-\nforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33,\npages 10009–10010, 2019. 233, 235, 241, 245\n620. Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature\nreuse? Towards understanding the eﬀectiveness of maml. In International Conference on\nLearning Representations, 2020. 262\n621. Roberta Raileanu and Tim Rocktäschel. RIDE: rewarding impact-driven exploration for\nprocedurally-generated environments. In International Conference on Learning Representa-\ntions, 2020. 181, 238\n622. Rajat Raina, Andrew Y Ng, and Daphne Koller. Constructing informative priors using\ntransfer learning. In Proceedings of the 23rd international conference on Machine learning,\npages 713–720, 2006. 272\n623. Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with\nimplicit gradients. In Advances in Neural Information Processing Systems, 2019. 258, 272\n624. Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Eﬃcient\noﬀ-policy meta-reinforcement learning via probabilistic context variables. In International\nConference on Machine Learning, pages 5331–5340. PMLR, 2019. 269\n625. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on\nMachine Learning, 2021. 280, 324\n626. Jette Randlov. Learning macro-actions in reinforcement learning. In Advances in Neural\nInformation Processing Systems, pages 1045–1051, 1998. 245\n627. J. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https://\nGitHub.com/FacebookResearch/Nevergrad, 2018. 263\n628. Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster,\nand Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent\nreinforcement learning. In International Conference on Machine Learning, pages 4295–4304.\nPMLR, 2018. 211\n629. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In\nInternational Conference on Learning Representations, 2017. 258, 259, 272\n630. John R. Rice. The algorithm selection problem. Advances in Computers, 15(65-118):5, 1976.\n263\n631. Martin Riedmiller. Neural ﬁtted Q iteration—ﬁrst experiences with a data eﬃcient neural\nreinforcement learning method. In European Conference on Machine Learning, pages 317–328.\nSpringer, 2005. 78, 91\n632. Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the\nAmerican Mathematical Society, 58(5):527–535, 1952. 51\n370\nReferences\n633. Frank Röder, Manfred Eppe, Phuong DH Nguyen, and Stefan Wermter. Curious hierarchical\nactor-critic reinforcement learning. In International Conference on Artiﬁcial Neural Networks,\npages 408–419. Springer, 2020. 239, 240, 280\n634. Diederik M Roijers, Willem Röpke, Ann Nowé, and Roxana Rădulescu. On following pareto-\noptimal policies in multi-objective planning and reinforcement learning. In Multi-Objective\nDecision Making Workshop, 2021. 202\n635. Diederik M Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey\nof multi-objective sequential decision-making. Journal of Artiﬁcial Intelligence Research,\n48:67–113, 2013. 202\n636. Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot\nlearning. In International Conference on Machine Learning, pages 2152–2161, 2015. 272, 273,\n280\n637. Willem Röpke, Roxana Radulescu, Diederik M Roijers, and Ann Ann Nowé. Communication\nstrategies in multi-objective normal-form games. In Adaptive and Learning Agents Workshop\n2021, 2021. 202\n638. Christopher D Rosin. Multi-armed bandits with episode context. Annals of Mathematics and\nArtiﬁcial Intelligence, 61(3):203–230, 2011. 172, 173\n639. Denis Rothman. Transformers for Natural Language Processing. Packt Publishing, 2021. 267\n640. Neil Rubens, Mehdi Elahi, Masashi Sugiyama, and Dain Kaplan. Active learning in rec-\nommender systems. In Recommender Systems Handbook, pages 809–846. Springer, 2015.\n181\n641. Jonathan Rubin and Ian Watson. Computer poker: A review. Artiﬁcial intelligence, 175(5-\n6):958–987, 2011. 227\n642. Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint\narXiv:1609.04747, 2016. 89\n643. Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions\nand use interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019. 282\n644. Ben Ruijl, Jos Vermaseren, Aske Plaat, and Jaap van den Herik. Hepgame and the simpliﬁca-\ntion of expressions. arXiv preprint arXiv:1405.6369, 2014. 192\n645. Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems.\nTechnical report, University of Cambridge, Department of Engineering Cambridge, UK, 1994.\n45, 54, 63\n646. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-\nFei. Imagenet large scale visual recognition challenge. International Journal of Computer\nVision, 115(3):211–252, 2015. 249, 266\n647. Stuart J Russell and Peter Norvig. Artiﬁcial intelligence: a modern approach. Pearson Education\nLimited, Malaysia, 2016. 14, 18, 60, 61, 63\n648. Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial\non Thompson sampling. Found. Trends Mach. Learn., 11(1):1–96, 2018. 53\n649. Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osin-\ndero, and Raia Hadsell. Meta-learning with latent embedding optimization. In International\nConference on Learning Representations, 2019. 272\n650. Richard M Ryan and Edward L Deci. Intrinsic and extrinsic motivations: Classic deﬁnitions\nand new directions. Contemporary Educational Psychology, 25(1):54–67, 2000. 245\n651. Jordi Sabater and Carles Sierra. Reputation and social network analysis in multi-agent\nsystems. In Proceedings of the First International Joint Conference on Autonomous Agents and\nMultiagent Systems: Part 1, pages 475–482, 2002. 220\n652. Sumit Saha. A comprehensive guide to convolutional neural networks—the ELI5 way.\nhttps://towardsdatascience.com/a-comprehensive-guide-to-convolutional-\nneural-networks-the-eli5-way-3bd2b1164a53. Towards Data Science, 2018. 309, 316\n653. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies\nas a scalable alternative to reinforcement learning. arXiv:1703.03864, 2017. 207, 216, 227, 281\n654. Brian Sallans and Geoﬀrey E Hinton. Reinforcement learning with factored states and actions.\nJournal of Machine Learning Research, 5:1063–1088, Aug 2004. 78, 79, 90\nReferences\n371\n655. Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas\nNardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon\nWhiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International\nConference on Autonomous Agents and MultiAgent Systems, AAMAS ’19, Montreal, 2019. 211,\n223, 227, 228\n656. Jason Sanders and Edward Kandrot. CUDA by example: an introduction to general-purpose\nGPU programming. Addison-Wesley Professional, 2010. 328\n657. Tuomas Sandholm. The state of solving large incomplete-information games, and application\nto poker. AI Magazine, 31(4):13–32, 2010. 227\n658. Tuomas Sandholm. Abstraction for solving large incomplete-information games. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence, volume 29, 2015. 208\n659. Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap.\nMeta-learning with memory-augmented neural networks. In International Conference on\nMachine Learning, pages 1842–1850, 2016. 272\n660. Vieri Giuliano Santucci, Pierre-Yves Oudeyer, Andrew Barto, and Gianluca Baldassarre.\nIntrinsically motivated open-ended learning in autonomous robots. Frontiers in Neurorobotics,\n13:115, 2020. 281\n661. Steve Schaefer.\nMathematical recreations. http://www.mathrec.org/old/2002jan/\nsolutions.html, 2002. 155\n662. Jonathan Schaeﬀer. One Jump Ahead: Computer Perfection at Checkers. Springer Science &\nBusiness Media, 2008. 68\n663. Jonathan Schaeﬀer, Robert Lake, Paul Lu, and Martin Bryant. Chinook, the world man-\nmachine checkers champion. AI Magazine, 17(1):21, 1996. 60\n664. Jonathan Schaeﬀer, Aske Plaat, and Andreas Junghanns. Unifying single-agent and two-\nplayer search. Information Sciences, 135(3-4):151–175, 2001. 192\n665. Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function\napproximators. In International Conference on Machine Learning, pages 1312–1320, 2015. 234,\n245\n666. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.\nIn International Conference on Learning Representations, 2016. 84, 86\n667. Tom Schaul and Jürgen Schmidhuber. Metalearning. Scholarpedia, 5(6):4650, 2010. 251, 271,\n272\n668. Daniel Schleich, Tobias Klamt, and Sven Behnke. Value iteration networks on multiple\nlevels of abstraction. In Robotics: Science and Systems XV, University of Freiburg, Freiburg im\nBreisgau, Germany, 2019. 139\n669. Jürgen Schmidhuber. Evolutionary Principles in Self-Referential Learning, or on Learning how\nto Learn: the Meta-Meta-. . . Hook. PhD thesis, Technische Universität München, 1987. 257,\n272\n670. Jürgen Schmidhuber. Making the world diﬀerentiable: On using self-supervised fully recur-\nrent neural networks for dynamic reinforcement learning and planning in non-stationary\nenvironments. Technical report, Inst. für Informatik, 1990. 136\n671. Jürgen Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning\nin reactive environments. In 1990 IJCNN International Joint Conference on Neural Networks,\npages 253–258. IEEE, 1990. 139, 143, 148\n672. Jürgen Schmidhuber. Curious model-building control systems. In Proceedings International\nJoint Conference on Neural Networks, pages 1458–1463, 1991. 179\n673. Jürgen Schmidhuber. Learning to generate sub-goals for action sequences. In Artiﬁcial neural\nnetworks, pages 967–972, 1991. 231\n674. Jürgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building\nneural controllers. In Proc. of the international conference on simulation of adaptive behavior:\nFrom animals to animats, pages 222–227, 1991. 281\n675. Jürgen Schmidhuber, F Gers, and Douglas Eck. Learning nonregular languages: A comparison\nof simple recurrent networks and LSTM. Neural Computation, 14(9):2039–2041, 2002. 319\n676. Jürgen Schmidhuber, Jieyu Zhao, and MA Wiering. Simple principles of metalearning.\nTechnical report, IDSIA, 1996. 272\n372\nReferences\n677. Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. Kernel principal component\nanalysis. In International Conference on Artiﬁcial Neural Networks, pages 583–588. Springer,\n1997. 15\n678. Nicol N Schraudolph, Peter Dayan, and Terrence J Sejnowski. Temporal diﬀerence learning of\nposition evaluation in the game of Go. In Advances in Neural Information Processing Systems,\npages 817–824, 1994. 79\n679. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre,\nSimon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy\nLillicrap, and David Silver. Mastering Atari, go, chess and shogi by planning with a learned\nmodel. Nature, 588(7839):604–609, 2020. 142, 144, 148, 279\n680. Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis\nAntonoglou, and David Silver. Online and oﬄine reinforcement learning by planning with a\nlearned model. arXiv preprint arXiv:2104.06294, 2021. 145, 279\n681. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust\nregion policy optimization. In International Conference on Machine Learning, pages 1889–1897,\n2015. 99, 108, 109, 120, 262\n682. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-\ndimensional continuous control using generalized advantage estimation. In International\nConference on Learning Representations, 2016. 116\n683. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 99, 110, 120, 276\n684. Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning. Neural\nNetworks, 16(1):5–9, 2003. 271, 272\n685. Marco Scutari. Learning Bayesian networks with the bnlearn R package. Journal of Statistical\nSoftware, 35(i03), 2010. 282\n686. Thomas D Seeley. The honey bee colony as a superorganism. American Scientist, 77(6):546–\n553, 1989. 195\n687. Marwin HS Segler, Mike Preuss, and Mark P Waller. Planning chemical syntheses with deep\nneural networks and symbolic AI. Nature, 555(7698):604, 2018. 192, 279\n688. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak\nPathak. Planning to explore via self-supervised world models. In International Conference on\nMachine Learning, 2020. 134, 142, 144, 148\n689. Oliver G Selfridge, Richard S Sutton, and Andrew G Barto. Training and tracking in robotics.\nIn International Joint Conference on Artiﬁcial Intelligence, pages 670–672, 1985. 179\n690. Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,\nChongli Qin, Augustin Zídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones,\nStig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver,\nKoray Kavukcuoglu, and Demis Hassabis. Improved protein structure prediction using\npotentials from deep learning. Nature, 577(7792):706–710, 2020. 192\n691. Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-\nMadison Department of Computer Sciences, 2009. 181\n692. Noor Shaker, Julian Togelius, and Mark J Nelson. Procedural Content Generation in Games.\nSpringer, 2016. 63, 180, 269\n693. Guy Shani, Joelle Pineau, and Robert Kaplow. A survey of point-based POMDP solvers.\nAutonomous Agents and Multi-Agent Systems, 27(1):1–51, 2013. 155\n694. Claude E Shannon. Programming a computer for playing chess. In Computer Chess Com-\npendium, pages 2–13. Springer, 1988. 7, 156\n695. Lloyd S Shapley. Stochastic games. In Proceedings of the National Academy of Sciences,\nvolume 39, pages 1095–1100, 1953. 198\n696. Yaron Shoham and Gal Elidan. Solving Sokoban with forward-backward reinforcement\nlearning. In Proceedings of the International Symposium on Combinatorial Search, volume 12,\npages 191–193, 2021. 246\n697. Yoav Shoham and Kevin Leyton-Brown. Multiagent Systems: Algorithmic, Game-Theoretic,\nand Logical Foundations. Cambridge University Press, 2008. 197\nReferences\n373\n698. Yoav Shoham, Rob Powers, and Trond Grenager. Multi-agent reinforcement learning: a\ncritical survey. Technical report, Stanford University, 2003. 226\n699. Pranav Shyam, Shubham Gupta, and Ambedkar Dukkipati. Attentive recurrent comparators.\nIn International Conference on Machine Learning, pages 3173–3181. PMLR, 2017. 272\n700. Robin C Sickles and Valentin Zelenyuk. Measurement of productivity and eﬃciency. Cambridge\nUniversity Press, 2019. 201\n701. Daniel L Silver, Qiang Yang, and Lianghao Li. Lifelong machine learning systems: Beyond\nlearning algorithms. In 2013 AAAI Spring Symposium Series, 2013. 250\n702. David Silver. Reinforcement learning and simulation based search in the game of Go. PhD\nthesis, University of Alberta, 2009. 171\n703. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den\nDriessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot,\nSander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy\nLillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mas-\ntering the game of Go with deep neural networks and tree search. Nature, 529(7587):484,\n2016. 60, 154, 176, 178, 182, 183, 186, 191, 250\n704. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,\nKaren Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that\nmasters chess, shogi, and Go through self-play. Science, 362(6419):1140–1144, 2018. 148, 182,\n185, 186, 187, 191, 279\n705. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.\nDeterministic policy gradient algorithms. In International Conference on Machine Learning,\npages 387–395, 2014. 112, 136\n706. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\nGuez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy\nLillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis\nHassabis. Mastering the game of Go without human knowledge. Nature, 550(7676):354, 2017.\n79, 148, 154, 173, 175, 176, 177, 178, 182, 183, 184, 185, 186, 187, 191, 250, 279\n707. David Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. Artiﬁcial\nIntelligence, page 103535, 2021. 17, 221\n708. David Silver, Richard S Sutton, and Martin Müller. Reinforcement learning of local shape in\nthe game of Go. In International Joint Conference on Artiﬁcial Intelligence, volume 7, pages\n1053–1058, 2007. 191\n709. David Silver, Richard S Sutton, and Martin Müller. Temporal-diﬀerence search in computer\nGo. Machine Learning, 87(2):183–219, 2012. 135\n710. David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley,\nGabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, and Thomas Degris.\nThe predictron: End-to-end learning and planning. In Proceedings of the 34th International\nConference on Machine Learning, pages 3191–3199, 2017. 134, 139, 140, 142, 148\n711. David Simões, Nuno Lau, and Luís Paulo Reis. Multi agent deep learning with cooperative\ncommunication. Journal of Artiﬁcial Intelligence and Soft Computing Research, 10, 2020. 212\n712. Satinder Singh, Andrew G Barto, and Nuttapong Chentanez. Intrinsically motivated rein-\nforcement learning. Technical report, University of Amherst, Mass, Department of Computer\nScience, 2005. 241, 242, 281\n713. Satinder Singh, Richard L Lewis, Andrew G Barto, and Jonathan Sorg. Intrinsically motivated\nreinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous\nMental Development, 2(2):70–82, 2010. 227\n714. David J Slate and Lawrence R Atkin. Chess 4.5—the northwestern university chess program.\nIn Chess skill in Man and Machine, pages 82–118. Springer, 1983. 167\n715. Gillian Smith. An analog history of procedural content generation. In Foundations of Digital\nGames, 2015. 181\n716. Stephen J Smith, Dana Nau, and Tom Throop. Computer bridge: A big win for AI planning.\nAI magazine, 19(2):93–93, 1998. 212\n374\nReferences\n717. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning.\nIn Advances in Neural Information Processing Systems, pages 4077–4087, 2017. 267, 272\n718. Doron Sobol, Lior Wolf, and Yaniv Taigman. Visual analogies between Atari games for\nstudying transfer learning in RL. arXiv preprint arXiv:1807.11074, 2018. 268\n719. Sungryull Sohn, Junhyuk Oh, and Honglak Lee. Hierarchical reinforcement learning for\nzero-shot generalization with subtask dependencies. In Advances in Neural Information\nProcessing Systems, pages 7156–7166, 2018. 272, 280\n720. Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran:\nLearning to factorize with transformation for cooperative multi-agent reinforcement learning.\nIn International Conference on Machine Learning, pages 5887–5896. PMLR, 2019. 211\n721. Fengguang Song and Jack Dongarra. Scaling up matrix computations on shared-memory\nmanycore systems with 1000 CPU cores. In Proceedings of the 28th ACM International\nConference on Supercomputing, pages 333–342. ACM, 2014. 328\n722. H. Francis Song, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan Clark, Hubert Soyer,\nJack W. Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala, Nicolas Heess, Dan Belov,\nMartin A. Riedmiller, and Matthew M. Botvinick. V-MPO: on-policy maximum a posteriori\npolicy optimization for discrete and continuous control. In International Conference on\nLearning Representations, 2019. 269\n723. Mei Song, A Montanari, and P Nguyen. A mean ﬁeld view of the landscape of two-layers\nneural networks. In Proceedings of the National Academy of Sciences, volume 115, pages\nE7665–E7671, 2018. 311\n724. Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal\nplanning networks. In International Conference on Machine Learning, pages 4739–4748, 2018.\n139\n725. Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Dropout: a simple way to prevent neural networks from overﬁtting. The Journal of\nMachine Learning Research, 15(1):1929–1958, 2014. 324\n726. Eric Steinberger.\nSingle deep counterfactual regret minimization.\narXiv preprint\narXiv:1901.07621, 2019. 337\n727. Martin Stolle and Doina Precup. Learning options in reinforcement learning. In International\nSymposium on Abstraction, Reformulation, and Approximation, pages 212–223. Springer, 2002.\n234, 235, 236\n728. Lise Stork, Andreas Weber, Jaap van den Herik, Aske Plaat, Fons Verbeek, and Katherine\nWolstencroft. Large-scale zero-shot learning in the wild: Classifying zoological illustrations.\nEcological Informatics, 62:101222, 2021. 272\n729. Darin Straus. Alphazero implementation and tutorial. https://towardsdatascience.com/\nalphazero-implementation-and-tutorial-f4324d65fdfc, 2018. 177\n730. Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling\ndeep neural networks. IEEE Transactions on Evolutionary Computation, 2019. 320\n731. Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley,\nand JeﬀClune. Deep neuroevolution: Genetic algorithms are a competitive alternative for\ntraining deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567,\n2017. 227\n732. Sainbayar Sukhbaatar, Emily Denton, Arthur Szlam, and Rob Fergus. Learning goal embed-\ndings via self-play for hierarchical reinforcement learning. arXiv preprint arXiv:1811.09083,\n2018. 235, 237\n733. Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016. 256\n734. Wenyu Sun and Ya-Xiang Yuan. Optimization Theory and Methods: Nonlinear Programming,\nvolume 1. Springer Science & Business Media, 2006. 108\n735. Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,\nMax Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, and Thore Graepel.\nValue-decomposition networks for cooperative multi-agent learning. In Proceedings of the\n17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS 2018,\nStockholm, Sweden, 2017. 211, 245\nReferences\n375\n736. Peter Sunehag, Guy Lever, Siqi Liu, Josh Merel, Nicolas Heess, Joel Z Leibo, Edward Hughes,\nTom Eccles, and Thore Graepel. Reinforcement learning agents acquire ﬂocking and symbiotic\nbehaviour in simulated ecosystems. In Artiﬁcial Life Conference Proceedings, pages 103–110.\nMIT Press, 2019. 215\n737. Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.\nLearning to compare: Relation network for few-shot learning. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1199–1208, 2018. 267, 272\n738. Ilya Sutskever and Vinod Nair. Mimicking Go experts with convolutional neural networks.\nIn International Conf. on Artiﬁcial Neural Networks, pages 101–110. Springer, 2008. 79\n739. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014. 323\n740. Richard S Sutton. Learning to predict by the methods of temporal diﬀerences. Machine\nLearning, 3(1):9–44, 1988. 47, 63\n741. Richard S Sutton. Integrated architectures for learning, planning, and reacting based on\napproximating dynamic programming. In Machine Learning Proceedings 1990, pages 216–224.\nElsevier, 1990. 125, 127, 128, 129, 148\n742. Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM\nSigart Bulletin, 2(4):160–163, 1991. 148\n743. Richard S Sutton and Andrew G Barto. Reinforcement learning, An Introduction, Second Edition.\nMIT Press, 2018. 7, 27, 31, 50, 53, 62, 70, 74, 77, 103, 126, 128, 147, 172\n744. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient\nmethods for reinforcement learning with function approximation. In Advances in Neural\nInformation Processing Systems, pages 1057–1063, 2000. 120\n745. Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: a\nframework for temporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112(1-\n2):181–211, 1999. 233, 234, 239, 244, 245, 280\n746. Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Eﬃcient processing of deep\nneural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295–2329, 2017. 328\n747. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian\nGoodfellow, and Rob Fergus. Intriguing properties of neural networks. In International\nConference on Learning Representations, 2013. 320\n748. Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration\nnetworks. In Advances in Neural Information Processing Systems, pages 2154–2162, 2016. 138,\n139, 142, 143, 148\n749. Oskari Tammelin. Solving large imperfect information games using CFR+. arXiv preprint\narXiv:1407.5042, 2014. 207, 208\n750. Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru,\nJaan Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement\nlearning. PloS one, 12(4):e0172395, 2017. 212, 226\n751. Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In\nInternational Conference on Machine Learning, pages 330–337, 1993. 210\n752. Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. Introduction to Data Mining. Pearson\nEducation India, 2016. 252\n753. Hongyao Tang, Jianye Hao, Tangjie Lv, Yingfeng Chen, Zongzhang Zhang, Hangtian Jia,\nChunxu Ren, Yan Zheng, Zhaopeng Meng, Changjie Fan, and Li Wang. Hierarchical deep\nmultiagent reinforcement learning with temporal abstraction. arXiv preprint arXiv:1809.09332,\n2018. 242\n754. Ryutaro Tanno, Kai Arulkumaran, Daniel C Alexander, Antonio Criminisi, and Aditya Nori.\nAdaptive neural trees. In International Conference on Machine Learning, pages 6166–6175,\n2019. 282\n755. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David\nBudden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin\nRiedmiller. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. 118, 120, 266, 336\n376\nReferences\n756. Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex\nbehaviors through online trajectory optimization. In 2012 IEEE/RSJ International Conference\non Intelligent Robots and Systems, pages 4906–4913, 2012. 142, 148\n757. Yuval Tassa, Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez,\nJosh Merel, Tom Erez, Timothy Lillicrap, and Nicolas Heess. dm_control: Software and tasks\nfor continuous control. arXiv preprint arXiv:2006.12983, 2020. 118\n758. Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A\nsurvey. Journal of Machine Learning Research, 10(Jul):1633–1685, 2009. 272\n759. Shoshannah Tekofsky, Pieter Spronck, Martijn Goudbeek, Aske Plaat, and Jaap van den Herik.\nPast our prime: A study of age and play style development in Battleﬁeld 3. IEEE Transactions\non Computational Intelligence and AI in Games, 7(3):292–303, 2015. 227\n760. Justin K Terry and Benjamin Black. Multiplayer support for the arcade learning environment.\narXiv preprint arXiv:2009.09341, 2020. 225\n761. Justin K Terry, Benjamin Black, Ananth Hari, Luis Santos, Clemens Dieﬀendahl, Niall L\nWilliams, Yashas Lokesh, Caroline Horsch, and Praveen Ravi. Pettingzoo: Gym for multi-\nagent reinforcement learning. arXiv preprint arXiv:2009.14471, 2020. 225, 246\n762. Gerald Tesauro. Neurogammon wins Computer Olympiad. Neural Computation, 1(3):321–323,\n1989. 78, 153\n763. Gerald Tesauro. TD-gammon: A self-teaching backgammon program. In Applications of\nNeural Networks, pages 267–285. Springer, 1995. 48, 78, 79, 182, 279\n764. Gerald Tesauro. Temporal diﬀerence learning and TD-Gammon. Communications of the\nACM, 38(3):58–68, 1995. 78, 154\n765. Gerald Tesauro. Programming backgammon using self-teaching neural nets. Artiﬁcial\nIntelligence, 134(1-2):181–199, 2002. 153\n766. Chen Tessler, Shahar Givony, Tom Zahavy, Daniel Mankowitz, and Shie Mannor. A deep\nhierarchical approach to lifelong learning in minecraft. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, volume 31, 2017. 235, 237\n767. Marc Teyssier and Daphne Koller. Ordering-based search: A simple and eﬀective algorithm\nfor learning Bayesian networks. arXiv preprint arXiv:1207.1429, 2012. 282\n768. Shantanu Thakoor, Surag Nair, and Megha Jhunjhunwala. Learning to play othello without\nhuman knowledge. Stanford University CS238 Final Project Report, 2017. 177, 187, 337\n769. Sergios Theodoridis and Konstantinos Koutroumbas. Pattern recognition. Academic Press,\n1999. 305\n770. William R Thompson. On the likelihood that one unknown probability exceeds another in\nview of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933. 53\n771. Sebastian Thrun. Learning to play the game of chess. In Advances in Neural Information\nProcessing Systems, pages 1069–1076, 1995. 166\n772. Sebastian Thrun. Is learning the 𝑛-th thing any easier than learning the ﬁrst? In Advances\nin Neural Information Processing Systems, pages 640–646. Morgan Kaufman, 1996. 254, 257\n773. Sebastian Thrun. Explanation-based neural network learning: A lifelong learning approach,\nvolume 357. Springer, 2012. 252\n774. Sebastian Thrun and Lorien Pratt. Learning to Learn. Springer, 2012. 252, 257, 264, 272\n775. Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking\nfew-shot image classiﬁcation: a good embedding is all you need? In European Conference on\nComputer Vision, 2020. 268, 272\n776. Yuandong Tian, Qucheng Gong, Wenling Shang, Yuxin Wu, and C Lawrence Zitnick. ELF:\nAn extensive, lightweight and ﬂexible research platform for real-time strategy games. In\nAdvances in Neural Information Processing Systems, pages 2659–2669, 2017. 177, 187, 337\n777. Yuandong Tian, Jerry Ma, Qucheng Gong, Shubho Sengupta, Zhuoyuan Chen, and\nC. Lawrence Zitnick. ELF OpenGo. https://github.com/pytorch/ELF, 2018. 188\n778. Yuandong Tian and Yan Zhu. Better computer Go player with neural network and long-term\nprediction. In International Conference on Learning Representations, 2016. 188\n779. Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in Neural\nInformation Processing Systems, pages 1369–1376, 2007. 111\nReferences\n377\n780. Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based\ncontrol. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages\n5026–5033, 2012. 41, 96, 97, 120, 266, 336\n781. Julian Togelius, Alex J Champandard, Pier Luca Lanzi, Michael Mateas, Ana Paiva, Mike\nPreuss, and Kenneth O Stanley. Procedural content generation: Goals, challenges and\nactionable steps. In Artiﬁcial and Computational Intelligence in Games. Schloss Dagstuhl-\nLeibniz-Zentrum für Informatik, 2013. 63, 181, 227\n782. Tatiana Tommasi, Martina Lanzi, Paolo Russo, and Barbara Caputo. Learning the roots of\nvisual domain shift. In European Conference on Computer Vision, pages 475–482. Springer,\n2016. 251, 255, 256\n783. Armon Toubman, Jan Joris Roessingh, Pieter Spronck, Aske Plaat, and Jaap Van Den Herik.\nDynamic scripting with team coordination in air combat simulation. In International Confer-\nence on Industrial, Engineering and other Applications of Applied Intelligent Systems, pages\n440–449. Springer, 2014. 227\n784. Thomas Trenner. Beating kuhn poker with CFR using python. https://ai.plainenglish.\nio/building-a-poker-ai-part-6-beating-kuhn-poker-with-cfr-using-python-\n1b4172a6ab2d. 208, 209\n785. Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross\nGoroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle.\nMeta-dataset: A dataset of datasets for learning to learn from few examples. In International\nConference on Learning Representations, 2020. 266, 267, 272, 336\n786. John Tromp. Number of legal Go states. http://tromp.github.io/go/legal.html, 2016.\n73\n787. John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diﬀference learning with\nfunction approximation. In Advances in Neural Information Processing Systems, pages 1075–\n1081, 1997. 70, 77, 90\n788. Alan M Turing. Digital Computers Applied to Games. Pitman & Sons, 1953. 7, 156, 165, 167\n789. Karl Tuyls, Julien Perolat, Marc Lanctot, Joel Z Leibo, and Thore Graepel. A generalised\nmethod for empirical game theoretic analysis.\nIn Proceedings of the 17th International\nConference on Autonomous Agents and MultiAgent Systems, AAMAS 2018, Stockholm, Sweden,\n2018. 198\n790. Karl Tuyls and Gerhard Weiss. Multiagent learning: Basics, challenges, and prospects. AI\nMagazine, 33(3):41–41, 2012. 226\n791. Paul Tylkin, Goran Radanovic, and David C Parkes. Learning robust helpful behaviors in\ntwo-player cooperative atari environments. In Proceedings of the 20th International Conference\non Autonomous Agents and MultiAgent Systems, pages 1686–1688, 2021. 225\n792. Eric Tzeng, Judy Hoﬀman, Kate Saenko, and Trevor Darrell. Adversarial discriminative\ndomain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 7167–7176, 2017. 256\n793. Wiebe Van der Hoek and Michael Wooldridge. Multi-agent systems. Foundations of Artiﬁcial\nIntelligence, 3:887–928, 2008. 227\n794. Michiel Van Der Ree and Marco Wiering. Reinforcement learning in the game of Othello:\nlearning against a ﬁxed opponent and learning from self-play. In IEEE Adaptive Dynamic\nProgramming and Reinforcement Learning, pages 108–115. IEEE, 2013. 192\n795. Max J van Duijn. The Lazy Mindreader: a Humanities Perspective on Mindreading and Multiple-\nOrder Intentionality. PhD thesis, Leiden University, 2016. 212\n796. Max J Van Duijn, Ineke Sluiter, and Arie Verhagen. When narrative takes over: The rep-\nresentation of embedded mindstates in Shakespeare’s Othello. Language and Literature,\n24(2):148–166, 2015. 212\n797. Max J Van Duijn and Arie Verhagen. Recursive embedding of viewpoints, irregularity, and\nthe role for a ﬂexible framework. Pragmatics, 29(2):198–225, 2019. 212\n798. Frank Van Harmelen, Vladimir Lifschitz, and Bruce Porter. Handbook of Knowledge Represen-\ntation. Elsevier, 2008. 236\n799. Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph\nModayil. Deep reinforcement learning and the deadly triad. arXiv:1812.02648, 2018. 79\n378\nReferences\n800. Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with Double\nQ-Learning. In AAAI, volume 2, page 5. Phoenix, AZ, 2016. 84, 85, 86\n801. Matthijs Van Leeuwen and Arno Knobbe. Diverse subgroup set discovery. Data Mining and\nKnowledge Discovery, 25(2):208–242, 2012. 15\n802. Kristof Van Moﬀaert and Ann Nowé. Multi-objective reinforcement learning using sets of\npareto dominating policies. Journal of Machine Learning Research, 15(1):3483–3512, 2014.\n202, 227\n803. Gerard JP Van Westen, Jörg K Wegner, Peggy Geluykens, Leen Kwanten, Inge Vereycken,\nAnik Peeters, Adriaan P IJzerman, Herman WT van Vlijmen, and Andreas Bender. Which\ncompound to select in lead optimization? Prospectively validated proteochemometric models\nguide preclinical development. PloS One, 6(11):e27518, 2011. 192\n804. Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018. 271, 272\n805. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998–6008, 2017. 324\n806. Vivek Veeriah, Tom Zahavy, Matteo Hessel, Zhongwen Xu, Junhyuk Oh, Iurii Kemaev, Hado\nvan Hasselt, David Silver, and Satinder Singh. Discovery of options via meta-learned subgoals.\narXiv preprint arXiv:2102.06741, 2021. 245\n807. Alfredo Vellido, José David Martín-Guerrero, and Paulo JG Lisboa. Making machine learning\nmodels interpretable. In ESANN, volume 12, pages 163–172, 2012. 282\n808. Jos AM Vermaseren. New features of form. arXiv preprint math-ph/0010025, 2000. 173\n809. Jean-Philippe Vert, Koji Tsuda, and Bernhard Schölkopf. A primer on kernel methods. In\nKernel Methods in Computational Biology, volume 47, pages 35–70. MIT press Cambridge,\nMA, 2004. 252\n810. Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John\nAgapiou, and Koray Kavukcuoglu. Strategic attentive writer for learning macro-actions. In\nAdvances in Neural Information Processing Systems, pages 3486–3494, 2016. 235, 237\n811. Alexander Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David\nSilver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In\nIntl Conf on Machine Learning, pages 3540–3549. PMLR, 2017. 235, 236, 245\n812. Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artiﬁcial\nIntelligence Review, 18(2):77–95, 2002. 263, 272\n813. Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik,\nJunyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk\nOh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai,\nJohn P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets, Rémi Leblond, Tobias Pohlen,\nValentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom Le Paine, Çaglar Gülçehre,\nZiyu Wang, Tobias Pfaﬀ, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina\nMcKinney, Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis\nHassabis, Chris Apps, and David Silver. Grandmaster level in starcraft II using multi-agent\nreinforcement learning. Nature, 575(7782):350–354, 2019. 7, 73, 218, 223, 224, 227, 242, 281\n814. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks\nfor one shot learning. In Advances in Neural Information Processing Systems, pages 3630–3638,\n2016. 266, 267, 272, 336\n815. Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\nMichelle Yeo, Alireza Makhzani, Heinrich Küttler, John P. Agapiou, Julian Schrittwieser,\nJohn Quan, Stephen Gaﬀney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt,\nDavid Silver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David\nLawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II: A new challenge for\nreinforcement learning. arXiv:1708.04782, 2017. 227, 336\n816. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural\nimage caption generator. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 3156–3164, 2015. 318\n817. Vanessa Volz, Jacob Schrum, Jialin Liu, Simon M Lucas, Adam Smith, and Sebastian Risi.\nEvolving mario levels in the latent space of a deep convolutional generative adversarial\nReferences\n379\nnetwork. In Proceedings of the Genetic and Evolutionary Computation Conference, pages\n221–228, 2018. 281\n818. John Von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior.\nPrinceton University Press, 1944. 198, 226\n819. John Von Neumann, Oskar Morgenstern, and Harold William Kuhn. Theory of Games and\nEconomic Behavior (commemorative edition). Princeton University Press, 2007. 198\n820. Jilles Vreeken, Matthijs Van Leeuwen, and Arno Siebes. Krimp: mining itemsets that compress.\nData Mining and Knowledge Discovery, 23(1):169–214, 2011. 15\n821. Loc Vu-Quoc. Neuron and myelinated axon. https://commons.wikimedia.org/w/index.\nphp?curid=72816083, 2018. 307, 308\n822. Douglas Walker and Graham Walker. The Oﬃcial Rock Paper Scissors Strategy Guide. Simon\nand Schuster, 2004. 227\n823. Hui Wang, Michael Emmerich, Mike Preuss, and Aske Plaat. Alternative loss functions in\nAlphaZero-like self-play. In 2019 IEEE Symposium Series on Computational Intelligence (SSCI),\npages 155–162, 2019. 176, 192\n824. Hui Wang, Mike Preuss, Michael Emmerich, and Aske Plaat. Tackling Morpion Solitaire with\nAlphaZero-like Ranked Reward reinforcement learning. In 22nd International Symposium on\nSymbolic and Numeric Algorithms for Scientiﬁc Computing, SYNASC 2020, Timisoara, Romania,\n2020. 181\n825. Jane X. Wang, Michael King, Nicolas Porcel, Zeb Kurth-Nelson, Tina Zhu, Charlie Deck, Pe-\nter Choy, Mary Cassin, Malcolm Reynolds, H. Francis Song, Gavin Buttimore, David P.\nReichert, Neil C. Rabinowitz, Loic Matthey, Demis Hassabis, Alexander Lerchner, and\nMatthew Botvinick. Alchemy: A structured task distribution for meta-reinforcement learning.\narXiv:2102.02926, 2021. 266, 269, 272, 336\n826. Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Rémi Munos,\nCharles Blundell, Dharshan Kumaran, and Matthew Botvinick. Learning to reinforcement\nlearn. arXiv preprint arXiv:1611.05763, 2016. 258, 260, 272\n827. Panqu Wang and Garrison W Cottrell. Basic level categorization facilitates visual object\nrecognition. arXiv preprint arXiv:1511.04103, 2015. 182\n828. Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois,\nShunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based\nreinforcement learning. arXiv:1907.02057, 2019. 127, 143, 148\n829. Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few\nexamples: A survey on few-shot learning. ACM Computing Surveys, 53(3):1–34, 2020. 257\n830. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas.\nDueling network architectures for deep reinforcement learning. In International Conference\non Machine Learning, pages 1995–2003, 2016. 84, 86, 89\n831. Christopher JCH Watkins. Learning from Delayed Rewards. PhD thesis, King’s College,\nCambridge, 1989. 45, 54, 63, 74, 75\n832. Eddie Weill. LeNet in Keras on Github. https://github.com/eweill/keras-deepcv/\ntree/master/models/classification. 313\n833. Daphna Weinshall, Gad Cohen, and Dan Amir. Curriculum learning by transfer learning:\nTheory and experiments with deep networks. In International Conference on Machine Learning,\npages 5235–5243, 2018. 180\n834. Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning.\nJournal of Big data, 3(1):1–40, 2016. 256, 272\n835. Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive rea-\nsoning for multi-agent reinforcement learning. In International Conference on Learning\nRepresentations, 2019. 212\n836. Lilian Weng. Meta-learning: Learning to learn fast. Lil’Log https://lilianweng.github.\nio/lil-log/2018/11/30/meta-learning.html, November 2018. 271, 272\n837. Lilian Weng. Curriculum for reinforcement learning https://lilianweng.github.io/\nlil-log/2020/01/29/curriculum-for-reinforcement-learning.html.\nLil’Log, Jan-\nuary 2020. 182, 192\n380\nReferences\n838. Shimon Whiteson. Evolutionary computation for reinforcement learning. In Marco A.\nWiering and Martijn van Otterlo, editors, Reinforcement Learning, volume 12 of Adaptation,\nLearning, and Optimization, pages 325–355. Springer, 2012. 227\n839. Shimon Whiteson, Brian Tanner, Matthew E Taylor, and Peter Stone. Protecting against\nevaluation overﬁtting in empirical reinforcement learning. In 2011 IEEE Symposium on\nAdaptive Dynamic Programming and Reinforcement Learning (ADPRL), pages 120–127. IEEE,\n2011. 282\n840. Marco A Wiering. Self-play and using an expert to learn to play backgammon with temporal\ndiﬀerence learning. JILSA, 2(2):57–68, 2010. 192\n841. Marco A Wiering, Maikel Withagen, and Mădălina M Drugan. Model-based multi-objective\nreinforcement learning. In 2014 IEEE Symposium on Adaptive Dynamic Programming and\nReinforcement Learning (ADPRL), pages 1–6. IEEE, 2014. 227\n842. Daan Wierstra, Tom Schaul, Jan Peters, and Jürgen Schmidhuber. Natural evolution strategies.\nIn IEEE Congress on Evolutionary Computation, pages 3381–3387, 2008. 227\n843. Nick Wilkinson and Matthias Klaes. An Introduction to Behavioral Economics. Macmillan\nInternational Higher Education, 2017. 227\n844. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist rein-\nforcement learning. Machine Learning, 8(3-4):229–256, 1992. 99, 101, 102, 120\n845. Ian H Witten. The apparent conﬂict between estimation and control—a survey of the\ntwo-armed bandit problem. Journal of the Franklin Institute, 301(1-2):161–189, 1976. 51\n846. Annie Wong, Thomas Bäck, Anna V. Kononova, and Aske Plaat. Deep multiagent reinforce-\nment learning: Challenges and directions. Artiﬁcial Intelligence Review, 2022. 50, 210, 212,\n226\n847. Michael Wooldridge. An Introduction to Multiagent Systems. Wiley, 2009. 197, 227\n848. Anita Williams Woolley, Christopher F Chabris, Alex Pentland, Nada Hashmi, and Thomas W\nMalone. Evidence for a collective intelligence factor in the performance of human groups.\nscience, 330(6004):686–688, 2010. 195, 215, 220, 227\n849. Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-\nregion method for deep reinforcement learning using kronecker-factored approximation. In\nAdvances in Neural Information Processing Systems, pages 5279–5288, 2017. 107\n850. Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Addressing appearance change in out-\ndoor robotics with adversarial domain adaptation. In 2017 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 1551–1558. IEEE, 2017. 256\n851. Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning—a\ncomprehensive evaluation of the good, the bad and the ugly. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 41(9):2251–2265, 2018. 272, 280\n852. Yuchen Xiao, Joshua Hoﬀman, and Christopher Amato. Macro-action-based deep multi-agent\nreinforcement learning. In Conference on Robot Learning, pages 1146–1161. PMLR, 2020. 245\n853. Wayne Xiong, Lingfeng Wu, Fil Alleva, Jasha Droppo, Xuedong Huang, and Andreas Stolcke.\nThe microsoft 2017 conversational speech recognition system 2017 conversational speech\nrecognition system. In IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 5934–5938. IEEE, 2018. 311\n854. Lin Xu, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Satzilla: portfolio-based\nalgorithm selection for sat. Journal of Artiﬁcial Intelligence Research, 32:565–606, 2008. 263\n855. Sijia Xu, Hongyu Kuang, Zhuang Zhi, Renjie Hu, Yang Liu, and Huyang Sun. Macro action\nselection with deep reinforcement learning in starcraft. In AAAI Artiﬁcial Intelligence and\nInteractive Digital Entertainment, volume 15, pages 94–99, 2019. 245\n856. Wen Xu, Jing He, and Yanfeng Shu. Transfer learning and deep domain adaptation. In\nAdvances in Deep Learning. IntechOpen, 2020. 256\n857. Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using inconsistent head poses. In\nICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing,\npages 8261–8265. IEEE, 2019. 320, 321\n858. Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from\ngame theoretical perspective. arXiv preprint arXiv:2011.00583, 2020. 226\nReferences\n381\n859. Zhao Yang, Mike Preuss, and Aske Plaat. Transfer learning and curriculum learning in\nsokoban. arXiv preprint arXiv:2105.11702, 2021. 63, 272\n860. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering Atari\ngames with limited data. arXiv preprint arXiv:2111.00210, 2021. 145\n861. Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.\nBayesian model-agnostic meta-learning. In Proceedings of the 32nd International Conference\non Neural Information Processing Systems, pages 7343–7353, 2018. 258, 272\n862. Jason Yosinski, JeﬀClune, Yoshua Bengio, and Hod Lipson. How transferable are features in\ndeep neural networks? In Neural Information Processing Systems, pages 3320–3328, 2014. 253\n863. Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising\neﬀectiveness of PPO in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.\n211\n864. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn,\nand Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta\nreinforcement learning. In Conference on Robot Learning, pages 1094–1100. PMLR, 2020. 118,\n266, 268, 269, 272, 273, 336\n865. Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks.\nIn European Conference on Computer Vision, pages 818–833. Springer, 2014. 249\n866. Qinsong Zeng, Jianchang Zhang, Zhanpeng Zeng, Yongsheng Li, Ming Chen, and Sifan Liu.\nPhoenixGo. https://github.com/Tencent/PhoenixGo, 2018. 177, 187, 188\n867. Amy Zhang, Nicolas Ballas, and Joelle Pineau. A dissection of overﬁtting and generalization\nin continuous reinforcement learning. arXiv preprint arXiv:1806.07937, 2018. 282, 325\n868. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Under-\nstanding deep learning (still) requires rethinking generalization. Communications of the\nACM, 64(3):107–115, 2021. 282\n869. Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overﬁtting in\ndeep reinforcement learning. arXiv preprint arXiv:1804.06893, 2018. 282\n870. Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multi-agent reinforcement learning: A\nselective overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019. 198, 199\n871. Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized\nmulti-agent reinforcement learning with networked agents. In International Conference on\nMachine Learning, pages 5872–5881. PMLR, 2018. 216\n872. Lei Zhang. Transfer adaptation learning: A decade survey. arXiv:1903.04687, 2019. 256, 272\n873. Lunjun Zhang, Ge Yang, and Bradly C Stadie. World model as a graph: Learning latent\nlandmarks for planning. In International Conference on Machine Learning, pages 12611–12620.\nPMLR, 2021. 136, 142, 237\n874. Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew J Johnson, and Sergey\nLevine. Solar: Deep structured representations for model-based reinforcement learning. In\nInternational Conference on Machine Learning, pages 7444–7453, 2019. 148\n875. Shangtong Zhang and Richard S Sutton. A deeper look at experience replay. arXiv preprint\narXiv:1712.01275, 2017. 79, 80, 90\n876. Wenshuai Zhao, Jorge Peña Queralta, and Tomi Westerlund. Sim-to-real transfer in deep\nreinforcement learning for robotics: a survey. In 2020 IEEE Symposium Series on Computational\nIntelligence (SSCI), pages 737–744. IEEE, 2020. 283\n877. Yan Zheng, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, and Changjie\nFan. A deep bayesian policy reuse approach against non-stationary agents. In 32nd Neural\nInformation Processing Systems, pages 962–972, 2018. 212\n878. Neng-Fa Zhou and Agostino Dovier. A tabled Prolog program for solving Sokoban. Funda-\nmenta Informaticae, 124(4):561–575, 2013. 27, 60\n879. Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui\nXiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE,\n109(1):43–76, 2020. 252, 272\n880. Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy\ninverse reinforcement learning. In AAAI, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.\n111\n382\nReferences\n881. Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret\nminimization in games with incomplete information. In Advances in Neural Information\nProcessing Systems, pages 1729–1736, 2008. 207, 227, 337\n882. Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast\ncontext adaptation via meta-learning. In International Conference on Machine Learning, pages\n7693–7702. PMLR, 2019. 271\nList of Tables\n1.1\nThe Constituents of Deep Reinforcement Learning . . . . . . . . . . . . . . .\n2\n1.2\n(Input/output)-Pairs for a Supervised Classiﬁcation Problem. . . . . . .\n15\n1.3\nSupervised vs. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.1\nTabular Value-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n3.1\nDeep Value-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n4.1\nPolicy-based Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n5.1\nDiﬀerence between Planning and Learning . . . . . . . . . . . . . . . . . . . . . . 131\n5.2\nModel-Based Reinforcement Learning Approaches [601] . . . . . . . . . . 142\n6.1\nCharacteristics of games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n6.2\nSelf-Play Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n6.3\nSelf-learning environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n7.1\nPrisoner’s Dilemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n7.2\nMulti-Agent Game Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n8.1\nHierarchical Reinforcement Learning Approaches . . . . . . . . . . . . . . . . 235\n9.1\nDiﬀerent kinds of supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n9.2\nMeta Reinforcement Learning Approaches [364] . . . . . . . . . . . . . . . . . 258\n9.3\nMeta-Learning Datasets and Environments . . . . . . . . . . . . . . . . . . . . . . 266\nA.1\nComparison of discrete and continuous probability distributions . . . 293\nB.1\nConfusion Matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302\nB.2\nObservations, Variables, Relative Dimensionality, Overﬁtting . . . . . . 306\nB.3\nFunctions that are Frequently Approximated . . . . . . . . . . . . . . . . . . . . . 312\nC.1\nReinforcement Learning Environments . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n383\n384\nList of Tables\nC.2\nAgent Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\nGlossary\n𝐴\naction in a state. 30\n𝐴\nadvantage function in actor critic. 101\n𝐶𝑝\nexploration/exploitation constant in MCTS; high is more exploration. 169\n𝐷\ndataset. 297\n𝐼𝜔\ninitiation set for option 𝜔. 229\n𝑄\nstate-action value. 37\n𝑅\nreward. 33\n𝑆\nstate. 29\n𝑇\ntransition. 31\n𝑉\nvalue. 36\nΩ\nset of options 𝜔(hierarchical reinforcement learning). 229\n𝛼\nlearning rate. 46\n𝛽𝜔(𝑠)\ntermination condition for option 𝜔at state 𝑠. 229\n𝜖-greedy\nexploration/exploitation rule that selects an 𝜖fraction of random exploration actions. 48\n𝛾\ndiscount rate, to reduce the importance of future rewards. 33\nL\nloss function. 306\nT𝑖\nbase-learning task T𝑖= (𝐷𝑖,𝑡𝑟𝑎𝑖𝑛, L𝑖), part of a meta-learning task. 254\n𝜔\noption (hierarchical reinforcement learning). 229\n𝜔\nhyperparameters (meta-learning). 254\n385\n386\nGlossary\n𝜙\nparameters for the value network in actor critic (as opposed to 𝜃, the policy parameters). 100\n𝜋\npolicy. 33\n𝜏\ntrajectory, trace, episode, sequence. 35\n𝜃\nparameters, weights in the neural network. 69, 100, 299\nA2C\nadvantage actor critic. 103\nA3C\nasynchronous advantage actor critic. 103\naccuracy\nthe total number of true positives and negatives divided by the total number of predictions. 299\nACO\nant colony optimization. 211\nALE\natari learning environment. 67\nBERT\nbidirectional encoder representations from transformers. 263\nbootstrapping\nold estimates of a value are reﬁned with new updates. 45\nCFR\ncounterfactual regret minimization. 202\nCPU\ncentral processing unit. 323\nD4PG\ndistributional distributed deep deterministic policy gradient. 114\nDDDQN\ndueling double deep Q-network. 82\nDDPG\ndeep deterministic policy gradient. 108\nDDQN\ndouble deep Q-network. 81\ndeep learning\ntraining a deep neural network to approximate a function, used for high-dimensional problems.\n2\ndeep reinforcement learning\napproximating value, policy, and transition functions with a deep neural network. 3\ndeep supervised learning\napproximating a function with a deep neural network; often for regression of image classiﬁca-\ntion. 297\nDQN\ndeep Q-network. 75\nentropy\nmeasure of the amount of uncertainty in a distribution. 292\nexploitation\nselecting actions as suggested by the current best policy 𝜋(𝑠). 48\nexploration\nselecting other actions than those that the policy 𝜋(𝑠) suggests. 47\nGlossary\n387\nfew-shot learning\ntask with which meta learning is often evaluated, to see how well the meta-learner can learn\nwith only a few training examples. 253\nﬁnetuning\ntraining the pre-trained network on the new dataset. 247\nfunction approximation\napproximation of a mathematical function, a main goal of machine learning, often performed\nby deep learning. 12\nGAN\ngenerative adversarial network. 316\nGPT-3\ngenerative pretrained transformer 3. 263\nGPU\ngraphical processing unit. 322\nhyperparameters\ndetermine the behavior of a learning algorithm; Base-learning learns parameters 𝜃, meta-\nlearning learns hyperparameters 𝜔. 267\nLSTM\nlong short-term memory. 313\nmachine learning\nlearning a function or model from data. 12\nMADDPG\nmulti agent DDPG. 207\nMAML\nmodel-agnostic meta-learning. 257\nMarkov decision process\nstochastic decision process that has the Markov (no-memory) property: the next state depends\nonly on the current state and the action. 28\nmeta-learning\nlearning to learn hyperparameters; Use a sequence of related tasks to learn a new task quicker.\n247\nMuJoCo\nmulti Joint dynamics with Contact. 92\noptimization\nﬁnd an optimal element in a space; used in many aspects in machine learning. 9\noverﬁtting\nhigh-capacity models can overtrain, where they model the signal and the noise, instead of just\nthe signal. 302\nparameters\nthe parameters 𝜃(weights of a neural network) connect the neurons, together they determine\nthe functional relation between input and output. 299\nPBT\npopulation based training. 213\nPETS\nprobabilisitic ensemble with trajectory sampling. 129\nPEX\nprioritized experience replay. 82\nPILCO\nprobabilistic inference for learning control. 128\n388\nGlossary\nPPO\nproximal policy optimization. 106\npretraining\nparameter transfer of the old task to the new task. 247\nREINFORCE\nREward Increment = Non-negative Factor × Oﬀset Reinforcement × Characteristic Eligibility.\n97\nreinforcement learning\nagent learns a policy for a sequential decision problem from environment feedback on its\nactions. 3\nSAC\nsoft actor critic. 107\nSARSA\nstate action reward state action. 50\nsequential decision problem\nproblem consisting of a sequence of decisions. 4\nsupervised learning\ntraining a predictive model on a labeled dataset. 3\nTD\ntemporal diﬀerence. 45\nTPU\ntensor processing unit. 180\ntransfer learning\nusing part of a network (pretraining) to speedup learning (ﬁnetuning) on a new dataset. 247\nTRPO\ntrust region policy optimization. 104\nunsupervised learning\nclustering elements in an unlabeled dataset based on an inherent metric. 15\nVI\nvalue iteration. 40\nVIN\nvalue iteration network. 134\nVPN\nvalue prediction network. 130\nXAI\nexplainable artiﬁcial intelligence. 278\nzero-shot learning\nan example has to be recognized as belonging to a class without ever having been trained on\nan example of this class. 260\nZSL\nzero-shot learning. 260\nIndex\n𝑘-fold cross validation, 303\nA0G, 187\nA3C, 107\naccuracy, 302\naction, 33\nactive learning, 3\nactive learning, 181\nactor critic, 103\nadaptive neural trees, 282\nadvantage function, 86, 103\nadversarial attack, 320\nagent, 16, 27\nAgent57, 89\nalchemy benchmark, 269\nALE, 71\nAlexNet, 312, 327\nalpha-beta, 167\nAlphaStar, 223\nAlphaZero General, 187\nAMIGo, 237\nAnt colony optimization, 215\nApe-X DQN, 225\nArcade Learning Environment, 71\nAtari, 71\nAtari 2600, 71\nattention networks, 323\nautoencoder, 321\nAutoML, 264\nbackgammon, 48, 152\nbackpropagation, 308\nbandit theory, 51\nbaseline function, 103\nbatch updates, 102\nbehavior policy, 51\nBellman equation, 39\nBellman, Richard, 39\nBengio, Yoshua, 12\nBERT, 267, 312\nbias-variance trade-oﬀ, 49, 304\nbootstrapping, 47, 53\nbrittleness, 110, 111, 142, 320\ncaptioning challenge, 317\nCapture the Flag, 222\ncartpole, 59\nCFR, 207\nchess, 7\nCNN, 312\ncollective intelligence, 215\ncompetitive, 199\ncompetitive behavior, 207\nconﬁdence, 51\ncontinuous action space, 30\ncontinuous task, 33\nconvergence, 77, 84\nconvolutional network, 312\nconvolutional networks, 314\ncooperation, 210\ncooperative, 199\ncorrelation, 77\ncounterfactual regret minimization, 207\ncoverage, 76\ncoverage, correlation, convergence, 76\ncross validation, 303\ncross-entropy, 297\ncurriculum learning, 178, 179\ncurse of dimensionality, 303\ndan (Go rank), 157\ndata augmentation, 324\nDDPG, 112\nDDQN, 85\n389\n390\nIndex\ndeadly triad, 77\ndecentralized computing, 215\ndecorrelation, 79\ndeep deterministic policy gradient, 112\ndeep dreaming, 321\ndeep fake, 320, 321\nDeepMind control suite, 118\nDeepStack, 219\ndeterministic environment, 29\ndevelopmental robotics, 178\ndiscrete action space, 30\ndistributional DQN, 87\ndivide and conquer, 40\ndomain adaptation, 255\ndownward selection, 32\ndropouts, 324\ndueling DDQN, 86\ndynamic programming, 39, 40\ndynamics model, 39\nearly stopping, 324\nELF, 188\nElmo, 186\nElo, Arpad, 184\nend-to-end learning, 311\nentropy, 110\nenvironment, 16, 27\nepisode, 34\nepisodic task, 33\nepoch, 308\nerror value, 302\nevaluation function, 164\nevolutionary algorithms, 213\nevolutionary strategies, 213\nexperience replay, 79\nexplainable AI, 282\nexploitation, 51\nexploration, 51\nfeature discovery, 311\nfew-shot learning, 257, 264\nfoundation model, 249\nfoundation models, 248\nfunction approximation, 302\ngame theory, 198\nGAN, 181, 320\nGaussian processes, 132\ngeneral-sum, 199\ngeneralization, 282, 302, 303\ngenerative adversarial networks, 320\nGLIE convergence, 53\nGo, 7\ngoal-conditioned, 240\nGoogle Football, 228\nGoogle Research Football, 225\nGPT-3, 267, 312\nGPU, 327\ngradient descent, 309\ngraph view of state space, 31\nGym, 41\nheuristic, 164\nHex, 188\nHide and Seek, 220\nhierarchical actor critic, 242\nhierarchical methods, 279\nhierarchical reinforcement learning, 229\nHinton, Geoﬀrey, 12\nholdout validation set, 324\nhyperparameter optimization, 263\nhypothesis testing, 3\nImageNet, 68, 327\nimagenet, 327\nIMPALA, 225\nin-distribution learning, 256\ninductive bias, 263\ninterpretable models, 282\nintrinsic motivation, 240\nintrinsic motivation, 178, 281\nirace, 263\nirreversible actions, 30, 131\niterated prisoner’s dilemma, 204\nKeras, 312, 326, 328\nKL divergence, 297\nKnightcap, 192\nknowledge representation, 282\nKocsis, Levente, 172\nkyu (Go rank), 157\nlatent models, 279\nleague learning, 217\nlearning, 126, 130, 131\nlearning rate tuning, 57\nLeCun, Yann, 12\nLeela, 188\nLeNet-5, 312\nLibratus, 218\nlifelong learning, 250\nlocomotion, 116\nLSTM, 317, 319\nmachine learning, 302\nMADDPG, 210, 211\nmake-move, 193\nMAML, 261\nIndex\n391\nMAPPO, 211\nMarkov decision process, 28\nmax pooling, 316\nMCTS, 169\nbackup, 171\nexpand, 171\nplayout, 171\nselect, 170\nmeta dataset benchmark, 267\nmeta learning, 256, 280\nmeta world benchmark, 268, 270\nminimax, 164\nmixed, 199\nMNIST, 312, 326\nmodel compression, 282\nmodel-predictive control, 137\nMonte Carlo Sampling, 45\nMonte Carlo Tree Search, 169\nMontezuma’s revenge, 88, 240\nmountain car, 59\nMuJoCo, 96\nmulti-agent DDPG, 210\nmulti-armed bandit, 51, 172\nmulti-task learning, 252, 254\nn-step, 49\nN-way-k-shot learning, 257\nNash equilibrium, 200\nneural ﬁtted Q learning, 78\nneural networks, 307\nNeurogammon, 78\nnevergrad, 263\nNNUE, 166\nnoisy DQN, 87\nnon-stationarity, 205\nobjective, 38\noﬀ-policy learning, 53\nOmniglot, 266\non-policy learning, 53\none-pixel problem, 320\nonline updates, 102\nOpenAI Gym, 41\nOpenGo, 188\nopponent modeling, 212\noptuna, 263\nout-of-distribution training, 76\noverﬁtting, 305, 324\nP-UCT, 172, 173\nParamILS, 263\nPareto optimum, 201\npattern database, 171\nPavlov, 8\nPETS, 133\nPetting Zoo, 225\nPhoenixGo, 188\nplanning, 40, 130, 131\nplanning and learning, 130\nPluribus, 219\npoker, 218\npolicy, 33\npolicy function, 33\nPolygames, 188\nPOMDP, 155\npopulation based self-play, 280\npopulation-based strategies, 213\nPPO, 110\nprecision, 302\nprioritized DDQN, 86\nprisoner’s dilemma, 202\nprocedural content generation, 180\nproximal policy optimization, 110\nPyBullet, 96\nPyTorch, 326\nQ-learning, 54\nquery set, 257\nR2D2, 89\nrainbow algorithms, 84\nrandom number seed, 110\nrecall, 302\nrecurrent neural network, 317\nrecursion, 40\nregularization, 324\nreinforcement learning, 16, 27\nreplay buﬀer, 79\nresidual network, 176, 319\nResNet, 319\nreversible actions, 130, 131\nreward, 55\nreward is enough, 221\nreward shaping, 55\nRNN, 317\nrobustness, 142\nSAC, 111\nsample eﬃciency, 127\nSARSA, 54\nSATzilla, 263\nscikit-learn, 263\nscikit-optimize, 263\nsearch-eval, 164\nself-learning, 163\nself-play, 163, 279\nsequence, 34\nsequential decision problem, 4\n392\nIndex\nSGD, 308, 311\nshared weights, 315\nShogi, 186\nSkinner, 8\nSMAC, 263\nsmoothness assumption, 307\nsoft actor critic, 111\nsoft decision trees, 282\nsoftmax, 316\nsparse reward, 55\nstable baselines, 81\nStarCraft, 7, 223\nstate, 29\nstate space, 303\nstochastic environment, 29\nstochastic gradient descent, 311\nStockﬁsh, 186\nsupervised learning, 14\nsupport set, 257\nswarm computing, 215\nSzepesvári, Csaba, 172\ntabula rasa, 182\ntabular method, 44\nTD, 47\nTD-Gammon, 78, 152\ntemporal abstraction, 231\ntemporal diﬀerence, 47\nTensorFlow, 326\nTesauro, Gerald, 78, 153\ntest set, 302\ntext to image, 317\ntheory of mind, 212, 215\nTit for Tat, 204\ntrace, 34\ntraining set, 302\ntrajectory, 34\ntransfer learning, 252\ntransformers, 267, 324\ntransposition table, 167\ntrial and error, 3, 32\nTRPO, 108\ntrust region policy optimization, 108\nUCT, 172\nunderﬁtting, 305\nundo-move, 193\nuniversal value function, 234\nunsupervised learning, 15, 321\nupward learning, 32\nVAE, 321\nvalue function, 33\nvalue iteration, 40\nvalue prediction network, 134\nvariance, 51\nvariational autoencoder, 321\nweight sharing, 315\nXAI, 282\nzero-shot learning, 264\nzero-sum game, 156\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2022-01-04",
  "updated": "2023-04-23"
}