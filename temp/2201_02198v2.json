{
  "id": "http://arxiv.org/abs/2201.02198v2",
  "title": "3D Intracranial Aneurysm Classification and Segmentation via Unsupervised Dual-branch Learning",
  "authors": [
    "Di Shao",
    "Xuequan Lu",
    "Xiao Liu"
  ],
  "abstract": "Intracranial aneurysms are common nowadays and how to detect them\nintelligently is of great significance in digital health. While most existing\ndeep learning research focused on medical images in a supervised way, we\nintroduce an unsupervised method for the detection of intracranial aneurysms\nbased on 3D point cloud data. In particular, our method consists of two stages:\nunsupervised pre-training and downstream tasks. As for the former, the main\nidea is to pair each point cloud with its jittered counterpart and maximise\ntheir correspondence. Then we design a dual-branch contrastive network with an\nencoder for each branch and a subsequent common projection head. As for the\nlatter, we design simple networks for supervised classification and\nsegmentation training. Experiments on the public dataset (IntrA) show that our\nunsupervised method achieves comparable or even better performance than some\nstate-of-the-art supervised techniques, and it is most prominent in the\ndetection of aneurysmal vessels. Experiments on the ModelNet40 also show that\nour method achieves the accuracy of 90.79\\% which outperforms existing\nstate-of-the-art unsupervised models.",
  "text": "3D Intracranial Aneurysm Classiﬁcation and Segmentation via Unsupervised\nDual-branch Learning\nDi Shao\nDeakin University\n75 Pigdons Rd, Waurn Ponds, 3216, Australia\nshaod@deakin.edu.au\nXuequan Lu\nDeakin University\n75 Pigdons Rd, Waurn Ponds, 3216, Australia\nxuequan.lu@deakin.edu.au\nXiao Liu\nDeakin University\n75 Pigdons Rd, Waurn Ponds, 3216, Australia\nxiao.liu@deakin.edu.au\nAbstract\nIntracranial aneurysms are common nowadays and how\nto detect them intelligently is of great signiﬁcance in digi-\ntal health. While most existing deep learning research fo-\ncused on medical images in a supervised way, we introduce\nan unsupervised method for the detection of intracranial\naneurysms based on 3D point cloud data. In particular, our\nmethod consists of two stages: unsupervised pre-training\nand downstream tasks. As for the former, the main idea is to\npair each point cloud with its jittered counterpart and max-\nimise their correspondence. Then we design a dual-branch\ncontrastive network with an encoder for each branch and\na subsequent common projection head. As for the latter,\nwe design simple networks for supervised classiﬁcation and\nsegmentation training. Experiments on the public dataset\n(IntrA) show that our unsupervised method achieves com-\nparable or even better performance than some state-of-the-\nart supervised techniques, and it is most prominent in the\ndetection of aneurysmal vessels. Experiments on the Mod-\nelNet40 also show that our method achieves the accuracy\nof 90.79% which outperforms existing state-of-the-art un-\nsupervised models.\n1. Introduction\nIntracranial aneurysms can result in a high rate of mor-\ntality, and their classiﬁcation and segmentation are of great\nsigniﬁcance. Existing research mainly focused on image\ndata which involve regular pixels [9, 18, 25, 26, 28, 29].\nWhile 3D geometric data such as point clouds can de-\npict more useful information, the research on analysing in-\ntracranial aneurysms using point cloud data has been very\nsparsely exploited to date. Thanks to [38], a point cloud\ndataset including aneurysmal segments and healthy ves-\nsel segments has been published. They have conducted a\nbenchmark using state-of-the-art point-based networks that\ncan directly consume 3D points instead of 2D pixels.\nThere are many networks available for consuming\npoint cloud data, for example, PointNet [20], PointNet++\n[21], SpiderCNN [37], PointCNN [14], SO-Net [13] and\nDGCNN [32].\nPointNet is a seminal method for taking\n3D points as input and used for 3D point cloud classiﬁca-\ntion and segmentation. Later on, other point-based methods\nhave been proposed to improve the performance. Since they\nare all supervised learning methods, annotated data are re-\nquired for training. However, annotation often requires ex-\nperts and signiﬁcant amounts of time, especially for large\ndatasets and medical data.\nWith the above analysis in mind, we design an unsuper-\nvised representation learning method that consumes point\nclouds of vessel segments. The contrastive learning concept\ninspires our method. In particular, we ﬁrst generate a pair of\naugmented sample of the original point cloud which should\nhave a distinctly difference. Next, we design a dual-branch\ncontrastive network with an encoder for each branch and a\nfollow-up common projection head to facilitate the unsu-\npervised training with a contrastive loss. As for the down-\nstream tasks, we ﬁrst use the unsupervised trained model\nto output the representations. Then, we design simple net-\nworks and train it by taking the representations as input\nto classify or segment intracranial aneurysms. Note that\nwe design two unsupervised networks and two correspond-\ning downstream networks to fulﬁll two different tasks (i.e.\nclassiﬁcation and segmentation). Supervised methods often\nneed a large scale of labelled data for achieving satisfac-\n1\narXiv:2201.02198v2  [eess.IV]  17 Jan 2022\ntory performance. Compared with them, our method does\nnot require labels in unsupervised training, and it can utilise\na small scale of labelled data for downstream training. In\nsummary, our contributions in this paper include:\n• We propose a simple yet effective method for unsu-\npervised representation learning on 3D point clouds of\nvessel segments.\n• We invent a useful augmentation method for generat-\ning pairs of each vessel segment.\n• We propose a dual-branch contrastive network with an\nencoder for each branch.\n• We conduct comprehensive experiments and compare\nwith state-of-the-art point-based techniques to demon-\nstrate the superior performance of our method.\n2. Related work\n2.1. Deep Learning on Intracranial Aneurysms\nIntracranial aneurysms are associated with a high mortal-\nity rate. Therefore, the detection of intracranial aneurysms\nis crucial for human health.\nTraditional methods rely\ngreatly on prior knowledge, which is often inferior to deep\nlearning in terms of capability and accuracy. Due to the\nexcellent performance of deep learning in processing med-\nical images, there are many deep learning methods to de-\ntect intracranial aneurysms [24]. [18] proposed a convo-\nlutional neural network-based detection system. The sys-\ntem used a 6-layer (CNN) and maximum intensity pro-\njection (MIP) algorithm based on the MRA images. This\nmethod can achieve almost 100% accuracy for detecting\naneurysms greater than 7 mm in diameter. However, it was\nless sensitive for small vascular aneurysms. To improve\nthis, [28] used the full U-net convolution architecture to\npredict aneurysm size based on the detection. [29] applied\nthe ResNet-18 network to the MRA images and performed\na secondary evaluation on the already detected image data\nto enhance the detection sensitivity. To better segment the\nshape of intracranial aneurysms, [26] utilized DeepMedic\n[10] with 2-pathway architecture and 11-layer convolution\nto segment intracranial aneurysms from the MRA images\non the basis of detection. The above method for detect-\ning intracranial aneurysms used data that are stacked with\n2D images. To sum up, nearly all works focused on deal-\ning with medical images rather than 3D geometry like point\nclouds.\n2.2. Point-Based Networks\nNeural network models for the classiﬁcation and seg-\nmentation of 3D point cloud data have achieved noticeable\nsuccesses. [20] proposed PointNet to directly process point\nsets. To obtain permutation invariance and transformation\ninvariance of point clouds, PointNet used the symmetric\nfunction and T-net to design the network. It had good re-\nsults for global features extraction of point clouds. How-\never, it ignored the geometric relationship among points\nand limited the extraction of local features.\nTo address\nthis issue, [21] proposed PointNet++ using a hierarchical\nneural network. It used the point sampling and grouping\nstrategy to extract local features of point clouds.\nHow-\never, PointNet++ did not reveal the spatial distribution of\nthe input point cloud. SO-Net [13] constructed the Self-\nOrganizing Map (SOM) [12] to model the spatial distribu-\ntion of the input point cloud. It allowed SO-Net to adjust\nthe receptive ﬁeld overlap and performed hierarchical fea-\nture extraction. Unlike SO-Net with adjusting the percep-\ntual ﬁeld of the hierarchical network, PointCNN [14] pro-\nposed the χ-transformation to process the point cloud data\nso that the point cloud data can be weighted or permuted.\nThus, it improved the extraction of local features. In addi-\ntion, SpiderCNN [37] proposed SpiderConv, i.e. parameter-\nized convolutional Filters, to implement convolutional oper-\nations on disordered point clouds. DGCNN [32] proposed\na convolutional-like operation by constructing local neigh-\nbourhood graphs and applying convolutional operations on\nthe edges. It connected adjacent point pairs to exploit the\nlocal geometric structure. In addition to common tasks like\nclassiﬁcation and segmentation, point based networks have\nalso been developed to address other tasks [31,40]. In sum-\nmary, there has been great progress in analyzing point cloud\ndata in a supervised manner.\n2.3. Unsupervised 3D Point Cloud Learning\nAll the above deep neural networks can classify and seg-\nment the point cloud data well. However, considering the\ncomplexity of labelling 3D data, it is difﬁcult to get enough\ndata with expert labelling for supervised training in many\nscenarios. Therefore, it is meaningful for exploiting un-\nsupervised or self-supervised learning for 3D point cloud\ndata. PointContrast [36] proposed an Unsupervised frame-\nwork with U-net as the backbone network. And it demon-\nstrated the transferability of representation learning to 3D\npoint cloud data and the performance enhancement of pre-\ntraining to downstream tasks. Lu et al. [15, 16] attempted\nto address skeleton learning on point cloud sequence data.\nJiang et al. [8] introduced a simple yet effective unsuper-\nvised learning method on point cloud that only considers\nrotation as the transformation. Info3D [22] proposed to ex-\ntend the InfoMax [30] and contrastive learning principles on\n3D shapes. It maximized the mutual information between\n3D objects and their “chunks” to improve the representation\nin the aligned dataset. FoldingNet [39] proposed an autoen-\ncoder with graph pooling and MLP layers using the folding\noperation to deform 2D grids into object surfaces. How-\never, in 3D medical point cloud data, unsupervised methods\n2\nare still in great demand. We propose an unsupervised rep-\nresentation learning method, which shows excellent perfor-\nmance for the classiﬁcation and segmentation of point cloud\nbased intracranial aneurysms.\n3. Method\nOur method consists of two stages which are unsuper-\nvised learning and downstream tasks. In stage 1, we ﬁrst\nperform augmentation on each point cloud to get a pair\nof augmented samples which are different in pose (Section\n3.1). We then get two representations of a pair of data in\na high-dimensional space by means of the dual-branch en-\ncoders, which enables each branch of the encoder to extract\ndistinct features. Next, we map representations to a low-\ndimensional vector [6] with a projection head to improve\nnetwork training speed (Section 3.2). Last, we employ a\ncontrastive loss to encourage the representations of the pair\nof point clouds output by the encoders to be similar in the\nhigh-dimensional space (Section 3.3). In stage 2, the trained\nmodel is used to output unsupervised representations for the\ndownstream task (Section 3.4). The downstream task will\nevaluate the effectiveness of unsupervised learning. Figure\n1 presents the architecture of our method.\n3.1. Data Augmentation\nWe use data augmentation to generate different samples\nfor each point cloud. To generate a pair of data, we con-\nsider using data augmentation methods, including jittered,\nperturbation, and rotation transformations.\nAfter experi-\nments, it was found that jittered as data augmentation in\nboth branches gave the best results in the downstream tasks,\nindicating a more discriminative representation learned by\nthe upstream network. Ablation experiments will be pre-\nsented in Section 4.4.\nWe take a batch of point clouds with mini-batch size\nN and input them into the data augmentation module. As\nshown in the top of Figure 1, for a sample in the mini-batch,\nwe use the jittered function to obtain a pair of samples: one\nis the jittered point cloud xi and the other is the jittered\npoint cloud xj. In this way, we have a batch size of 2N in\nthis mini-batch. We randomly select {xi, xj} as a positive\npair, and the other N −1 pairs, which consist of one of pos-\nitive sample and one of the other samples, are regarded as\nnegative samples in this mini-batch.\n3.2. Dual-branch Encoders and Projection Head\nAs shown in the left bottom of Figure 2, each pair of\nsamples need to be passed through dual-branch encoders\nf(·) to obtain two representations hi and hj. Features are\nrespectively extracted from a pair of data using two differ-\nent encoders. Experimentally, we have also compared two\ndifferent encoders with a common encoder. Ablation exper-\niments will be presented in Section 4.4.\nTwo encoders are PointNet [20] and PointNet++ [21],\nrespectively. The reason for choosing PointNet and Point-\nNet++ is that PointNet can extract global features while\nPointNet++ can extract local features. This design high-\nlights distinctions in features and allows for more distinctive\nrepresentation.\nClassiﬁcation. The ﬁrst encoder utilizes three consec-\nutive 1D convolutional layers and a max-pooling layer to\nobtain the representation vector h (1024-dimensional). The\nsecond encoder consists of three abstraction levels. Each\nlayer abstracts and processes the point set to create a new\npoint set with fewer elements. The input to the abstraction\nlayer consists of an n × (d + c) matrix formed by n points\nwith d-dim coordinates and c-dim point features. n is the\nnumber of points in a point cloud sample. It outputs a point\nset group of size n1 × k × (d + c) by sampling n1 centroids\nand grouping them, where each group corresponds to a local\nregion. k is the number of points sampled from the centroid\npoint’s neighbourhood. The subsequent pointnet layer out-\nputs a local region feature vector n1 × (d + c1). We take all\nthe sampled points as a group in the last abstraction layer\nand output the representation vector h (1024-dimensional).\nWe design three linear layers as our projection head g(·) to\nmap each 1024-dimensional representation vector to a 128-\ndimensional vector z.\nSegmentation. The segmentation encoders are based on\nthe encoders for classiﬁcation. The 1024-dimensional rep-\nresentation vector output by encoder 1 is copied n times to\nform an n tensor. Concatenating it with the n×1024 tensor\nobtained from the last convolutional layer gives a n × 2048\ntensor. As such, this tensor contains both global features\nand features for each point.\nThe encoder 2 extends the\nclassiﬁcation’s encoder 2 by adding three propagation lay-\ners. We adopt distance-based interpolation and a skip-link\nacross levels propagation strategy. In a propagation level,\nwe propagate point features from n1 × (d + c1) points to\nn points. We achieve feature propagation by concatenat-\ning interpolating feature values c1 of n1 points with skip\nlinked point features from the set abstraction level c of the\nn points. It outputs a n × (d + c1 + c) vector, which is\nthen passed through the unit pointnet to obtain a n × 1024\ntensor. The 1024-dimensional vector output from the ab-\nstraction layer is copied n times and concatenated with the\nn × 1024-dimensional tensor output from the propagation\nlayer to obtain the ﬁnal n × 2048 representation tensor.\nThe tensor obtained by encoder 1 and encoder 2 are max-\npooled separately to obtain a 2048-dimensional represen-\ntation vector.\nThese two vectors are used as the feature\nrepresentation h for the downstream segmentation network.\nWe design two linear layers as our projection head g(·) to\nmap each 2048-dimensional representation vector to a 512-\ndimensional vector z.\n3\nFigure 1. The architecture of our method including data augmentation, encoder, projection head, loss function and downstream tasks. We\nﬁrst jitter a point cloud x to construct a pair (xi, xj). The representation vectors zi and zj to the pair of point clouds are then extracted via\nthe dual-branch encoders and mapping head, and network optimisation is performed by a contrastive loss.The representation h obtained by\ndual-branch encoders will be used for downstream tasks.\n3.3. Contrastive Loss\nWe use a contrastive loss function similar to [4]. With\nthis loss function, unsupervised learning can effectively\nlearn separable features for point clouds. After the projec-\ntion head g(·), for each sample in the mini-batch, we obtain\nthe projection representation z. For the pair xi and xj, we\nuse their projection representations zi and zj to measure the\ncosine similarity between the two samples, as follows:\nsi,j =\nzi⊤zj\n(∥zi∥∥zj∥)\n(1)\nIntuitively, the similarity for a positive sample pair\nshould be high. A combination pair of a positive and a neg-\native sample should be low. Then, we use to get a simi-\nlar probability of each positive sample pair in a mini-batch.\n⊮[k̸=i] ∈{0, 1} is an indicator function evaluating to 1 iff\nk ̸= i. The equation for calculating the probability of simi-\nlarity is as follows:\nS(i, j) =\nexp(si,j)\nP2N\nk=1 ⊮k!=iexp(si,k)\n(2)\nWe use the negative logarithm to calculate the loss of\nthe sample pair. This loss has been used in previous works\n[19,27,35]. τ denotes a temperature parameter which scales\nthe input and expands the range of cosine similarity. This\nloss is known as the normalized temperature-scaled cross-\nentropy loss [2,19] as follows:\nl(i, j) = −log\nexp(si,j/τ)\nP2N\nk=1 ⊮k!=iexp(si,k/τ)\n(3)\nWe calculate the average loss of both (i, j) and (j, i) in\nthe mini-batch. Based on this loss, the representation of the\nencoder and projection head improves over time, and the\ntrained network places similar samples closer in the repre-\nsentation space. Speciﬁcally, the loss function is given by:\nL =\n1\n2N\nN\nX\nk=1\n[l(2k −1, 2k) + l(2k, 2k −1)]\n(4)\n3.4. Downstream Tasks\nWe design two simple downstream networks to evaluate\nthe unsupervised learned representations for classiﬁcation\nand segmentation, respectively. Each point cloud of a vessel\nsegment is fed into the unsupervised dual-branch encoders\nto obtain two representations. We then concatenate the two\nrepresentations into one and use this representation as input\nto train the downstream network. As for the binary classiﬁ-\ncation task, we use four linear layers (512, 256, 128, 2) as\nthe downstream network. Regarding the segmentation task,\nwe employ four 1D convolutional layers (1024, 512, 256,\nm), where m is the number of segmentation labels.\n4. Evaluation\n4.1. Datasets\nIntrA [38] consists of complete models of aneurysms,\ngenerated vessel segments and annotated aneurysm seg-\nments. IntrA collected 103 3D models of the entire cerebral\nvasculature by reconstructing 2D MRA images scanned for\npatients. IntrA generated 1,909 vessel segments from the\ncomplete model, including 1,694 healthy vessel segments\nand 215 aneurysmal segments. Additionally, 116 aneurysm\nsegments were manually annotated for each point. In IntrA,\neach sample was represented as a 3D point cloud. Each\npoint p is a 6D vector composing of its coordinates and\nnormal vector. Following IntrA, we combined the gener-\nated vessel segments and manually annotated aneurysms to\n4\nFigure 2. Dual-branch Encoders and projection head. “conv1d”: 1D convolution, “linear”: fully connected layers, “mlp” stand for multi-\nlayer perceptron. The numbers in brackets represent the layer sizes. All convolutions and fully connected layers include batchnorm and\nELU.\nachieve a total of 2,025 samples. These 2,025 vessel seg-\nments will be used as the dataset for our unsupervised train-\ning. All 2,025 vessel segments will be used for the down-\nstream classiﬁcation task.\n116 annotated aneurysm seg-\nments will be used for the downstream segmentation task.\nModelNet-40 [34] is a collection of 40 categories and\n12,311 models culled from the mesh surfaces of CAD mod-\nels.\nFollowing previous practice, 9,843 models are em-\nployed for training, and 2,468 for testing. Each point cloud\nhas 2,048 points, and all of the points’ coordinates are nor-\nmalised to the unit sphere. Each point is a 6D vector made\nup of its coordinates and normal vector.\nWe take 1,024\npoints from each item and augment the data by jittered. This\ndataset is used for comparisons of our method with other\nunsupervised methods.\n4.2. Experimental Setting\nFor unsupervised training, we use the Adam optimizer\nwith a weight decay of 10−6. The mini-batch size is set to\n32. The number of epochs is 200. The initial learning rate is\n10−3. The learning rate is scheduled to be multiplied by 0.5\nin every 10 epochs. We use jittered as the data augmentation\nmethod, which directly adds Guassian noise to every coor-\ndinate and normal information of input point clouds. In the\n5\nSupervised\nNetwork\n#.Points\nV.(%)\nA.(%)\nF1-Score\nSpiderCNN [37]\n512\n1024\n2048\n98.05\n97.28\n97.82\n84.58\n87.9\n84.89\n0.8692\n0.8722\n0.8662\nSO-Net [13]\n512\n1024\n2048\n98.76\n98.88\n98.88\n84.24\n81.21\n83.94\n0.8840\n0.8684\n0.8850\nPointCNN [14]\n512\n1024\n2048\n98.38\n98.79\n98.95\n78.25\n81.28\n85.81\n0.8494\n0.8748\n0.9044\nDGCNN [32]\n512/10\n1024/20\n2048/40\n95.22\n95.34\n97.93\n60.73\n72.21\n83.40\n0.6578\n0.7376\n0.8594\nPointNet++ [21]\n512\n1024\n2048\n98.52\n98.52\n98.76\n86.69\n88.51\n87.31\n0.8928\n0.9029\n0.9016\nPointNet [20]\n512\n1024\n2048\n94.45\n94.98\n93.74\n67.66\n64.96\n69.50\n0.6909\n0.6835\n0.6916\nUnsupervised\nFoldingNet [39]\n512\n1024\n2048\n91.37\n91.83\n91.64\n77.41\n78.28\n79.54\n0.6159\n0.6241\n0.6316\nOur(single PN)\n512\n1024\n2048\n94.33\n94.21\n94.84\n75.55\n78.33\n77.05\n0.7233\n0.7424\n0.7408\nOur(single PN++)\n512\n1024\n2048\n95.33\n95.63\n95.74\n80.55\n83.60\n83.41\n0.7679\n0.7968\n0.7988\nOur(dual)\n512\n1024\n2048\n96.74\n97.45\n95.41\n82.35\n84.28\n89.47\n0.8296\n0.8613\n0.8226\nTable 1. Classiﬁcation results of each method. The additional input K is required for DGCNN. PN: PointNet, PN++: PointNet++.\nencoder 2, the number of points sampled from the centroid\npoint’s neighbourhood k is set to [32, 64, “None”]. “None”\nmeans that all points are sampled. The projection head out-\nputs a feature z, the dimension of which is set to 128 for the\nclassiﬁcation task and 512 for the segmentation task. In the\nloss function, the temperature parameter τ is set to 0.5.\nFor the downstream network, the optimizer, the num-\nber of epochs, representation dimensions, initial learning\nrate, learning rate decay schedule and mini-batch size are\nthe same as those in unsupervised training.\nWe sample\n512,1024 and 2048 points separately in each point cloud\nfor both experiments. For classiﬁcation task weight decay\nis set to 10−6 and size of linear is set to [512, 256, 128, a].\na is the number of categories in the point cloud. For seg-\nmentation task, weight decay is set to 1.0 and size of MLPs\nis set to [1024, 512, 256, b]. b is the number of categories\nof points in the point cloud.\nExperiments were implemented using PyTorch on a\nGeForce GTX 1080 GPU. For IntrA dataset, the time for\nboth unsupervised training on classiﬁcation and segmenta-\ntion is approximately 1 hour. The downstream classiﬁca-\ntion and segmentation training are approximately 40 min-\nutes and 50 minutes, respectively.\n4.3. Experimental Results\nWe evaluate the classiﬁcation task and the segmentation\ntask separately on IntrA [38].To demonstrate the general-\nisation of our method, we also perform the classiﬁcation\ntask on ModelNet-40 [34], and then compare our method\nwith start-of-the-art unsupervised methods to verify the ef-\nfectiveness of our method.\nClassiﬁcation task. On IntrA, we evaluate the perfor-\nmance using three metrics: (1) V. Accuracy, measuring the\npercentage of correctly predicted healthy vessels’ samples\n6\nover all healthy vessels’ samples, (2) A. Accuracy, indicat-\ning the percentage of correctly predicted aneurysm vessels’\nsamples over all aneurysm vessels’ samples, (3) F1 score,\nrepresenting the harmonic average of precision and recall\nand evaluating the quality of the model. On ModelNet-40,\nwe evaluate the performance using overall accuracy.\nAs shown in Table 1, as for our dual-branch encoders\nmethod with PointNet and PointNet++ backbones (i.e. PN,\nPN++), 1,024 sample points have the best results in terms\nof the F1 score and V. Accuracy compared with other num-\nbers of sample points. The results for 512 sample points\nare still impressive, though the number of points in each\npoint cloud is much smaller. Although the 2,048 sample\npoint result is not the best in terms of F1 score and V. ac-\ncuracy. Notice that our method in 2,048 sample point has\nthe best A. Accuracy results compared with all mentioned\nmethods. The ability to identify aneurysms is essential in\nthis case. Furthermore, we ﬁnd that the A. accuracy in-\ncrease with more sample points. Compared with other su-\npervised methods, our results are better than the supervised\nPointNet in all metrics. Besides, it also outperforms more\nadvanced supervised networks such as DGCNN in general.\nResult of 1,024 sample points is very close to SO-Net and\nSpiderCNN in terms of F1-Score. We also compare our\nmethod with FoldingNet, one of the most representative un-\nsupervised methods. Obviously, our method performs bet-\nter on all metrics. The effectiveness of unsupervised learn-\ning is inherently limited due to the unsupervised nature, and\nthe tubular structure of intracranial aneurysms is much less\nprominent compared with other data. It causes our method\n(dual) to perform less well than supervised PN++.\nMethod\nModelNet40(%)\nSPH [11]\n68.2%\nLFD [3]\n75.5%\nT-L Network [5]\n74.4%\nVConv-DAE [23]\n75.5%\n3D-GAN [33]\n83.3%\nLatent-GAN [1]\n85.7%\nFoldingNet [39]\n88.4%\nPointCapsNet [41]\n88.9%\nMultiTask [7]\n89.1%\nOur(dual)\n90.79%\nTable 2. Classiﬁcation accuracy of unsupervised learning on Mod-\nelNet40.\nAs shown in Table 2, we compare the performance of our\nmodel with other unsupervised methods on ModelNet40\n[34]. we can see that our method outperforms all other un-\nsupervised methods, which again conﬁrms its effectiveness\nin unsupervised representation learning.\nSegmentation task.\nFollowing [38], we evaluate the\nNetwork\n#.Points\nIoU V.(%)\nIoU A.(%)\nSO-Net\n512\n1024\n2048\n94.22\n94.42\n94.46\n80.14\n80.99\n81.40\nPN++\n512\n1024\n2048\n93.42\n93.35\n93.24\n76.22\n76.38\n76.21\nPointCNN\n512\n1024\n2048\n92.49\n93.47\n93.59\n70.65\n74.11\n73.58\nSpiderCNN\n512\n1024\n2048\n90.16\n87.95\n87.02\n67.25\n61.60\n58.32\nPointGrid\n16/2\n16/4\n32/2\n78.32\n79.49\n80.11\n35.82\n38.23\n42.42\nPointNet\n512\n1024\n2048\n73.99\n75.23\n74.22\n37.30\n37.07\n37.75\nOur(PN)\n512\n1024\n2048\n80.05\n82.54\n81.65\n44.66\n46.55\n48.45\nOur(PN++)\n512\n1024\n2048\n80.05\n82.05\n82.65\n40.66\n41.42\n42.45\nOur(dual)\n512\n1024\n2048\n82.25\n84.35\n82.65\n48.66\n50.92\n51.45\nTable 3. Segmentation results of each network.\nsegmentation performance using two metrics: (1) V. IoU,\nindicating the IoU of heathly vessel, and (2) A. IoU, indi-\ncating the IoU of aneurysm vessel.\nAs shown in Table 3, as for our method (dual), 1,024\nsample points have the best results in terms of V. IoU.\nBut 2,048 sample points have the best results in terms of\nA. IoU. In comparison, our method outperforms the su-\npervised PointNet on both V. IoU and A. IoU. Notice that\nour method is better than more advanced supervised net-\nworks like PointGrid. Compared to the supervised Point-\nNet, which is trained with only 116 labelled samples, our\nmethod is able to learn unsupervised features from a much\nwider range of data, thus facilitating downstream network\ntraining. Our method generally generates better results with\nincreasing the point number, and produces better results\nthan the supervised PointNet in both metrics. Our method\n(PN++) is still inferior to the supervised PointNet++, which\nis considered to be limited by the unsupervised nature.\n7\n4.4. Ablation Studies\nWe explore the factors that make our method effective\nthrough ablation experiments. We conduct two ablation ex-\nperiments to further understand which data augmentation is\nmore effective, and the effect of dual-branch encoders. We\nalso analyse the effectiveness of our method in the case of\nsparse labels. The ablation experiments sample 1,024 points\nin each point cloud.\nAugmentation. We try to ﬁnd the best data augmen-\ntation method for our unsupervised method, by consider-\ning three different augmentation methods including rota-\ntion, jittered and perturbation.\nAugmentation\nV.(%)\nA.(%)\nF1-Score\nrotation\n95.23\n75.52\n0.7637\nperturbation\n95.35\n81.87\n0.8121\njittered & perturbation\n95.63\n81.76\n0.8240\njittered\n97.45\n84.28\n0.8613\nTable 4. Ablation study on augmentation.\nRotation means randomly rotating the point cloud along\nthe Y -axis. Perturbation means randomly rotating the point\ncloud by a small angle along the XY Z-axis. Jittered is\nthe addition of Gaussian noise to the XY Z coordinates and\nnormal information of the point cloud. As shown in Table\n4, jittered is the best data augmentation for both branches,\nand the method with both jittered and perturbation is the\nsecond and generally better than the perturbation for both\nbranches. The data augmentation with both branches rota-\ntion is the least effective. Based on the results, we can ﬁnd\nthat the data augmentation using jittered allows the encoder\nto learn the distinctive features of the point cloud more ef-\nfectively, thereby giving better results.\nFigure 3. Ablation study on dual-branch encoders.\nDual-branch encoders. In order to investigate the effec-\ntiveness of dual-branch encoders, experiments are designed\nto compare it with the traditional single encoder method. As\nshown in Figure 3, the dual-branch encoders method has the\nbest performance for both classiﬁcation and segmentation\ntasks, in particular for the classiﬁcation task. Besides, the\nsingle encoder method based on the more advanced PN++\nis inferior to the PN-based method. This is probably due to\ncontrastive learning depends largely on global information.\nBased on these results, we have the following ﬁndings:\n• Dual-branch encoders are able to extract more discrim-\ninative features. In particular, the two encoders in our\ndesign are PN and PN++ respectively where PN fo-\ncuses on global features and PN++ on local features.\n• Contrastive learning can better understand the distinc-\ntions between the features extracted by the two en-\ncoders. Therefore it is more effective than a single en-\ncoder. In our design, PN and PN++ as encoders can\nbetter highlight the distinctions between the local and\nglobal features of a point cloud sample, allowing the\ncontrastive loss to optimise the network more effec-\ntively.\n• Contrastive learning is excellent at describing objects\nas a whole, but is weak at describing them at the point\nscale. Our method achieves outstanding results in clas-\nsiﬁcation tasks, but is moderately effective in segmen-\ntation tasks. This is because our contrastive learning\nis not a comparison between points but between point\nclouds as a whole.\nNetwork\nLabel(%)\nV.(%)\nA.(%)\nF1-Score\nPointNet\n10\n5\n1\n87.43\n86.53\n84.86\n53.33\n42.85\n30.58\n0.3298\n0.3012\n0.2485\nPointNet++\n10\n5\n1\n94.37\n92.89\n89.55\n70.58\n63.23\n45.07\n0.7111\n0.6370\n0.4637\nOur(dual)\n10\n5\n1\n95.34\n94.39\n90.84\n71.19\n67.27\n58.31\n0.7294\n0.6935\n0.5712\nTable 5. Ablation study for limited labeled Data.\nLimited Labeled data. In real-world situations, we fre-\nquently lack sufﬁcient labeled data. We divide the origi-\nnal dataset into two parts, A and B, assume A to be the\nunlabeled data and B to be the labeled data, to represent\nsuch circumstance. The percentages of labeled data are set\nto 10%, 5% and 1%, respectively. In unsupervised learn-\ning, we use A+B to pre-train the model, and then use B\nfor training the downstream tasks. Because of the nature\nof supervised learning, only B is used for other supervised\ntraining. The experiments are set up with a classiﬁcation\n8\ntask and performed on the IntrA dataset. As shown in Ta-\nble 5, the accuracy of the classiﬁcation gradually decreases\nas the amount of annotated data decreased. However, the\naccuracy of our model consistently outperforms that of the\nsupervised models. This suggests that our method is more\nrobust by making use of the unlabeled data for unsupervised\nlearning. An interesting point is to combine evolutionary\noptimization with the proposed method to enhance the per-\nformance on limited labeled data [17].\n5. Conclusion\nIn this work, we have presented an unsupervised rep-\nresentation learning method for the classiﬁcation and seg-\nmentation of 3D intracranial aneurysms. It ﬁrst augments a\npoint cloud into two samples, and pairs them up for going\nthrough the dual-branch encoders and a subsequent com-\nmon projection head. Distinctive features are learned by\nmaximising the correspondence for a pair. The representa-\ntions learned by the unsupervised trained encoders are used\nas input for the downstream tasks. Experiments demon-\nstrated that our method is effective in learning unsupervised\nrepresentations and can achieve better or comparable per-\nformance than state-of-the-art supervised and unsupervised\nlearning methods.\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas Guibas.\nRepresentation learning and adver-\nsarial generation of 3d point clouds.\narXiv preprint\narXiv:1707.02392, 2(3):4, 2017. 7\n[2] Philip Bachman, R Devon Hjelm, and William Buchwalter.\nLearning representations by maximizing mutual information\nacross views. arXiv preprint arXiv:1906.00910, 2019. 4\n[3] Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming\nOuhyoung. On visual similarity based 3d model retrieval. In\nComputer graphics forum, volume 22, pages 223–232. Wi-\nley Online Library, 2003. 7\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning, pages 1597–1607. PMLR, 2020. 4\n[5] Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Ab-\nhinav Gupta. Learning a predictable and generative vector\nrepresentation for objects. In European Conference on Com-\nputer Vision, pages 484–499. Springer, 2016. 7\n[6] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensional-\nity reduction by learning an invariant mapping. In 2006 IEEE\nComputer Society Conference on Computer Vision and Pat-\ntern Recognition (CVPR’06), volume 2, pages 1735–1742.\nIEEE, 2006. 3\n[7] Kaveh Hassani and Mike Haley. Unsupervised multi-task\nfeature learning on point clouds.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 8160–8171, 2019. 7\n[8] Jincen Jiang, Xuequan Lu, Wanli Ouyang, and Meili Wang.\nUnsupervised representation learning for 3d point cloud\ndata. arXiv preprint arXiv:2110.06632, 2021. 2\n[9] Bio Joo, Sung Soo Ahn, Pyeong Ho Yoon, Sohi Bae, Beom-\nseok Sohn, Yong Eun Lee, Jun Ho Bae, Moo Sung Park,\nHyun Seok Choi, and Seung-Koo Lee. A deep learning al-\ngorithm may automate intracranial aneurysm detection on mr\nangiography with high diagnostic performance. European\nRadiology, 30:5785–5793, 2020. 1\n[10] Konstantinos Kamnitsas, Enzo Ferrante, Sarah Parisot,\nChristian Ledig, Aditya V Nori, Antonio Criminisi, Daniel\nRueckert, and Ben Glocker.\nDeepmedic for brain tumor\nsegmentation.\nIn International workshop on Brainlesion:\nGlioma, multiple sclerosis, stroke and traumatic brain in-\njuries, pages 138–149. Springer, 2016. 2\n[11] Michael Kazhdan,\nThomas Funkhouser,\nand Szymon\nRusinkiewicz. Rotation invariant spherical harmonic repre-\nsentation of 3 d shape descriptors. In Symposium on geome-\ntry processing, volume 6, pages 156–164, 2003. 7\n[12] Teuvo Kohonen. The self-organizing map. Proceedings of\nthe IEEE, 78(9):1464–1480, 1990. 2\n[13] Jiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self-\norganizing network for point cloud analysis.\nIn Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 9397–9406, 2018. 1, 2, 6\n[14] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan\nDi, and Baoquan Chen.\nPointcnn:\nConvolution on χ-\ntransformed points.\nIn Proceedings of the 32nd Interna-\ntional Conference on Neural Information Processing Sys-\ntems, pages 828–838, 2018. 1, 2, 6\n[15] Xuequan Lu, Honghua Chen, Sai-Kit Yeung, Zhigang Deng,\nand Wenzhi Chen. Unsupervised articulated skeleton extrac-\ntion from point set sequences captured by a single depth cam-\nera. In Thirty-Second AAAI Conference on Artiﬁcial Intelli-\ngence, 2018. 2\n[16] Xuequan Lu, Zhigang Deng, Jun Luo, Wenzhi Chen, Sai-Kit\nYeung, and Ying He. 3d articulated skeleton extraction using\na single consumer-grade depth camera. Computer Vision and\nImage Understanding, 188:102792, 2019. 2\n[17] T. Nakane, N. Bold, H. Sun, X. Lu, T. Akashi, and C. Zhang.\nApplication of evolutionary and swarm optimization in com-\nputer vision: a literature survey. IPSJ Transactions on Com-\nputer Vision and Applications, 12(1):1–34, 2020. 9\n[18] Takahiro Nakao, Shouhei Hanaoka, Yukihiro Nomura, Issei\nSato, Mitsutaka Nemoto, Soichiro Miki, Eriko Maeda, Take-\nharu Yoshikawa, Naoto Hayashi, and Osamu Abe.\nDeep\nneural network-based computer-assisted detection of cere-\nbral aneurysms in mr angiography. Journal of Magnetic Res-\nonance Imaging, 47(4):948–953, 2018. 1, 2\n[19] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 4\n[20] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classiﬁcation\nand segmentation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 652–660,\n2017. 1, 2, 3, 6\n9\n[21] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. arXiv preprint arXiv:1706.02413, 2017. 1, 2,\n3, 6\n[22] Aditya Sanghi. Info3d: Representation learning on 3d ob-\njects using mutual information maximization and contrastive\nlearning.\nIn European Conference on Computer Vision,\npages 626–642. Springer, 2020. 2\n[23] Abhishek Sharma, Oliver Grau, and Mario Fritz. Vconv-dae:\nDeep volumetric shape learning without object labels.\nIn\nEuropean Conference on Computer Vision, pages 236–250.\nSpringer, 2016. 7\n[24] Z Shi, B Hu, UJ Schoepf, RH Savage, DM Dargis, CW Pan,\nXL Li, QQ Ni, GM Lu, and LJ Zhang. Artiﬁcial intelligence\nin the management of intracranial aneurysms: current status\nand future perspectives. American Journal of Neuroradiol-\nogy, 41(3):373–379, 2020. 2\n[25] Zhao Shi, Chongchang Miao, U Joseph Schoepf, Rock H\nSavage, Danielle M Dargis, Chengwei Pan, Xue Chai, Xiu Li\nLi, Shuang Xia, Xin Zhang, et al. A clinically applicable\ndeep-learning model for detecting intracranial aneurysm in\ncomputed tomography angiography images. Nature commu-\nnications, 11(1):1–11, 2020. 1\n[26] T Sichtermann, A Faron, R Sijben, N Teichert, J Freiherr,\nand M Wiesmann.\nDeep learning–based detection of in-\ntracranial aneurysms in 3d tof-mra.\nAmerican Journal of\nNeuroradiology, 40(1):25–32, 2019. 1, 2\n[27] Kihyuk Sohn. Improved deep metric learning with multi-\nclass n-pair loss objective. In Proceedings of the 30th Inter-\nnational Conference on Neural Information Processing Sys-\ntems, pages 1857–1865, 2016. 4\n[28] Joseph N Stember, Peter Chang, Danielle M Stember,\nMichael Liu, Jack Grinband, Christopher G Filippi, Philip\nMeyers, and Sachin Jambawalikar.\nConvolutional neural\nnetworks for the detection and measurement of cerebral\naneurysms on magnetic resonance angiography. Journal of\ndigital imaging, 32(5):808–815, 2019. 1, 2\n[29] Daiju Ueda, Akira Yamamoto, Masataka Nishimori, Taro\nShimono, Satoshi Doishita, Akitoshi Shimazaki, Yutaka\nKatayama,\nShinya Fukumoto,\nAntoine Choppin,\nYuki\nShimahara, et al.\nDeep learning for mr angiography:\nautomated detection of cerebral aneurysms.\nRadiology,\n290(1):187–194, 2019. 1, 2\n[30] Petar Veliˇckovi´c, William Fedus, William L Hamilton, Pietro\nLi`o, Yoshua Bengio, and R Devon Hjelm. Deep graph info-\nmax. arXiv preprint arXiv:1809.10341, 2018. 2\n[31] Weijia Wang, Xuequan Lu, Dasith de Silva Edirimuni,\nXiao Liu, and Antonio Robles-Kelly.\nDeep point cloud\nnormal estimation via triplet learning.\narXiv preprint\narXiv:2110.10494, 2021. 2\n[32] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\nMichael M Bronstein, and Justin M Solomon.\nDynamic\ngraph cnn for learning on point clouds. Acm Transactions\nOn Graphics (tog), 38(5):1–12, 2019. 1, 2, 6\n[33] Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T Free-\nman, and Joshua B Tenenbaum.\nLearning a probabilistic\nlatent space of object shapes via 3d generative-adversarial\nmodeling. arXiv preprint arXiv:1610.07584, 2016. 7\n[34] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d\nshapenets: A deep representation for volumetric shapes. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1912–1920, 2015. 5, 6, 7\n[35] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination.\nIn Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 3733–\n3742, 2018. 4\n[36] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas\nGuibas, and Or Litany.\nPointcontrast: Unsupervised pre-\ntraining for 3d point cloud understanding. In European Con-\nference on Computer Vision, pages 574–591. Springer, 2020.\n2\n[37] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao.\nSpidercnn: Deep learning on point sets with parameterized\nconvolutional ﬁlters. In Proceedings of the European Con-\nference on Computer Vision (ECCV), pages 87–102, 2018.\n1, 2, 6\n[38] Xi Yang, Ding Xia, Taichi Kin, and Takeo Igarashi. Intra: 3d\nintracranial aneurysm dataset for deep learning. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2656–2666, 2020. 1, 4, 6, 7\n[39] Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Fold-\ningnet: Point cloud auto-encoder via deep grid deformation.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 206–215, 2018. 2, 6, 7\n[40] Dongbo Zhang, Xuequan Lu, Hong Qin, and Ying He. Point-\nﬁlter: Point cloud ﬁltering via encoder-decoder modeling.\nIEEE Transactions on Visualization and Computer Graph-\nics, 27(3):2015–2027, 2020. 2\n[41] Yongheng Zhao, Tolga Birdal, Haowen Deng, and Federico\nTombari.\n3d point capsule networks.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1009–1018, 2019. 7\n10\n",
  "categories": [
    "eess.IV",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2022-01-06",
  "updated": "2022-01-17"
}