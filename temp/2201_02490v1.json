{
  "id": "http://arxiv.org/abs/2201.02490v1",
  "title": "Audio representations for deep learning in sound synthesis: A review",
  "authors": [
    "Anastasia Natsiou",
    "Sean O'Leary"
  ],
  "abstract": "The rise of deep learning algorithms has led many researchers to withdraw\nfrom using classic signal processing methods for sound generation. Deep\nlearning models have achieved expressive voice synthesis, realistic sound\ntextures, and musical notes from virtual instruments. However, the most\nsuitable deep learning architecture is still under investigation. The choice of\narchitecture is tightly coupled to the audio representations. A sound's\noriginal waveform can be too dense and rich for deep learning models to deal\nwith efficiently - and complexity increases training time and computational\ncost. Also, it does not represent sound in the manner in which it is perceived.\nTherefore, in many cases, the raw audio has been transformed into a compressed\nand more meaningful form using upsampling, feature-extraction, or even by\nadopting a higher level illustration of the waveform. Furthermore, conditional\non the form chosen, additional conditioning representations, different model\narchitectures, and numerous metrics for evaluating the reconstructed sound have\nbeen investigated. This paper provides an overview of audio representations\napplied to sound synthesis using deep learning. Additionally, it presents the\nmost significant methods for developing and evaluating a sound synthesis\narchitecture using deep learning models, always depending on the audio\nrepresentation.",
  "text": "arXiv:2201.02490v1  [cs.SD]  7 Jan 2022\nAudio representations for deep learning in sound\nsynthesis: A review\nAnastasia Natsiou\nTechnological University of Dublin\nDublin, Ireland\nanastasia.natsiou@tudublin.ie\nSe´an O’Leary\nTechnological University of Dublin\nDublin, Ireland\nsean.oleary@tudublin.ie\nAbstract—The rise of deep learning algorithms has led many\nresearchers to withdraw from using classic signal processing\nmethods for sound generation. Deep learning models have\nachieved expressive voice synthesis, realistic sound textures,\nand musical notes from virtual instruments. However, the most\nsuitable deep learning architecture is still under investigation.\nThe choice of architecture is tightly coupled to the audio\nrepresentations. A sound’s original waveform can be too dense\nand rich for deep learning models to deal with efﬁciently -\nand complexity increases training time and computational cost.\nAlso, it does not represent sound in the manner in which it is\nperceived. Therefore, in many cases, the raw audio has been\ntransformed into a compressed and more meaningful form using\nupsampling, feature-extraction, or even by adopting a higher level\nillustration of the waveform. Furthermore, conditional on the\nform chosen, additional conditioning representations, different\nmodel architectures, and numerous metrics for evaluating the\nreconstructed sound have been investigated. This paper provides\nan overview of audio representations applied to sound synthesis\nusing deep learning. Additionally, it presents the most signiﬁ-\ncant methods for developing and evaluating a sound synthesis\narchitecture using deep learning models, always depending on\nthe audio representation.\nIndex Terms—Sound representations, Deep learning, Genera-\ntive models, Sound synthesis.\nI. INTRODUCTION\nSound generation algorithms synthesize a time domain\nwaveform. This waveform should be coherent and appropriate\nfor its intended use. These waveforms can convey complex\nand varied information. Deep generative networks [1] have\ndemonstrated great potential for such tasks having been used\nfor the synthesis of a range of sounds, from pleasant pieces\nof music to natural speech [2]. These models discover latent\nrepresentations based on the distribution of the initial data and\nthen sample from this distribution to generate new acoustic\nsignals with the same properties as the original ones. In\nmany cases, the deep learning models can operate along with\nsignal processing algorithms and enhance their expression\ncapabilities [3] [4].\nThe representation of the sound embraced by the deep\nneural network plays a major role on the development of the\nalgorithm. Raw time domain audio is a rich representation\nwhich leads to massive information making the network com-\nputationally expensive and therefore slow. Compressed Time-\nfrequency representations based on spectrograms can decrease\nthe computer power needed but the parameter detection and\nsynthesis of the sound is usually a challenging task and the loss\nof information can cause signiﬁcant reconstruction error [5].\nParameters extracted from state-of-the-art vocoders have also\nbeen proposed for deep neural network applications [6]. These\nparameters demonstrate a potential in marrying the deep gener-\native models with statistical parametric synthesizers. Finally,\ncontemporary investigations allow the network to determine\nthe feature needed for the task [7]. Linguistic and acoustic\nfeatures can be encoded into latent representations such as\nembeddings.\nApart from an overview of the audio representations existing\nin sound synthesis implementations, this paper additionally\nquotes popular schemes for conditioning a deep generative\nnetwork with auxiliary data. Conditioning in generative models\ncan control the aspects of the synthesis and lead to new\nsamples with speciﬁc characteristics [8]. Furthermore, the\npaper highlights examples of deep generative models for audio\ngeneration applications. Deep neural networks have demon-\nstrated remarkable progress in the ﬁeld demonstrating im-\npressive results. A ﬁnal section discusses evaluation processes\nfor synthesised sound. Subjective evaluation via listening tests\nare generally considered the most reliable measure of quality.\nHowever, multiple other metrics for assessing a generative\nmodel have been proposed converting both acoustic signals\nto intermediate representations of them to be examined. Con-\nsequently, audio representations assume an essential role not\nonly as input data but also inﬂuence the network architecture,\nthe conditioning technique as well as the evaluation process.\nII. INPUT REPRESENTATIONS\nIn the literature, numerous audio representations have been\nproved beneﬁcial for audio synthesis applications. Many times,\ncomparisons have been conducted between different forms of\nthe sound to reveal the most appropriate representation for\na speciﬁc deep learning architecture. Raw audio and time-\nfrequency representations usually present the ﬁrst attempts in\nsuch experiments. However, recent studies also look to higher\nlevel forms that offer more meaningful description such as\nembeddings, or multiple sound features like the fundamental\nfrequency, loudness and features extracted by state-of-the-art\nvocoders such as WORLD [9]. The table I summarizes the\nadvantages and disadvantages of each sound representation.\nA. Waveform - Raw Audio\nThe term raw audio is often used to refer to a waveform\nencoded using pulse code modulation (PCM). This is the\nsampling of a continuous waveform in time and amplitude.\nIt represents the waveform as a sequence of numbers, each\nnumber representing an amplitude sample at a chosen sam-\npling frequency. In order for this discrete sequence of samples\nto capture all the necessary information, the highest frequency\nin the signal should adhere to the Nyquist-Shannon theorem\n[10]. According to this theorem, only frequencies of less\nthan half the sampling frequency can be reproduced from\nthe sampled signal. A typical sampling frequency for audio\napplications is 44.1kHz. Each real number is assigned to the\napproximate ﬁxed value in a ﬁnite set of discrete numbers.\nThe most common levels for quantization are stored in 8\nbits (256 levels), 16 bits (65536 levels) and 24 bits (16.8\nmillion levels). Therefore, a sound with a duration of one\nsecond sampled at 44.1kHz generates 44100 samples. This\nrepresentations is considered extremely informative even for\ndeep learning networks.\nIn order for the outcome of the deep learning model to be\nmore effective, a pre-processing step can be used to reduce the\nquantization range of the raw audio. Many research approaches\n[11] [12] [13] apply µ-law to decrease the possible values\nof each prediction. µ-law is presented in Eq. 1 where −1 <\nx < 1 and µ equals the number of levels created after the\ntransformation.\nf(x) = sgn(x)ln(1 + µ|x|)\nln(1 + µ)\n(1)\nAlthough non-linear quantization processes such as µ-law\nreceived much attention the last years, the majority of the\nexisting papers use a normalized high resolution signal as input\n[14]. Finally, other applications include linear quantization of\nthe input waveform [15] [16] and different designs for most\nand less signiﬁcant bits [17].\nB. Spectrograms\nA spectrogram is a time/frequency visual representation of\nsound. A spectrogram can be obtained via the Short Time\nFourier Transform (STFT), where the Fourier Transform is ap-\nplied to overlapping segments of the waveform. The Discrete\nFourier Transform (DFT) is presented by the equation Eq. 2\nfor k = 0, 1, .., N −1 where N is the number of samples and\nk is the number of segments. The spectrogram uses just the\nabsolute values of the STFT, discarding the phase. This type\nof spectrograms has been used in many by a variety of papers\n[18] [19] [20].\nX(k) =\nN−1\nX\nn=0\nx(n)e−jωkn\n(2)\nApart from the original spectrogram, deep learning ar-\nchitectures have also experimented with non linear spectro-\ngrams such as mel-spectrograms [21] [22] [23] [24] [25]\n[26] [27] or Constant-Q Transformations (CQT) [28]. The\nmel-spectrogram is generated by the application of perceptual\nﬁlters on the DFT called mel-ﬁlter bands. The most common\nformula for encoding mel-ﬁlter bands is presented by Eq. 3\nwhere f is the frequency in Hertz. However, other models\nhave captured the perceptual transformation applying a linear\ntransformation until 1kHz and a logarithmic above this thresh-\nold.\nmel = 2595log10(1 + f\n700)\n(3)\nCQT is another time-frequency representation where the\nfrequencies are geometrically spaced. The centre frequencies\nof the ﬁlters are calculated from the result of the formula\nωk = 2\nk\nb ω0 where k = 1, 2, ..kmax and b is a constant\nnumber. The bandwidth of each frequency then comes as\nδk = ωk+1 −ωk = ωk(2 1\nb −1) and therefore the frequency\nresolution is determined by the Eq. 4 where Q is the quality\nfactor.\nQ = ωk\nδk\n= (2\n1\nb −1)−1\n(4)\nCQT is a representation with different frequency resolution\nin low and high frequencies. However, the phase part is\ndiscarded and the representation is in most of the cases\nirreversible. Following this argument, Velasco et al [29] pro-\nposed an invertible CQT based on nonstationary Gabor frames.\nAnother variation of CQT is rainbowgrams. Rainbowgrams\nproposed by Engel et al [7] using colors to encode time\nderivatives of the phase.\nIn addition, more complicated spectrogram-based represen-\ntations have also been investigated. GANSynth [30] conducted\nexperiments with numerous spectrograms including scaled\nlogarithmic amplitude and phase of the STFT, increased reso-\nlution of the original spectrogram or applied mel-ﬁlters. Also,\nthey examined an Instantaneous Frequency based spectrogram\nwhere the phase of the STFT is scaled and unwraped (add −π)\nand then the ﬁnite difference between the frames is computed.\nOther applications [31] [32], also, made comparisons between\nraw audio and spectrogram to uncover the most functional\nrepresentation for their deep learning model.\nC. Acoustic Features\nOvercoming the wealth of acoustic information presenting\nin a sound waveform, various studies extract perceptual fea-\ntures from the original signal. These acoustic features can be\nrepresented by phoneme inputs [33], fundamental frequency\nand spectral features [34] or multiple information such as\nthe velocity, instrument, pitch and time [35]. Other imple-\nmentations have included cepstral coefﬁcients [4] [36] or a\nvariety of linguistic and acoustic features [37] [13]. Finally,\nwidely recommended parameters have also been extracted by\nthe WORLD vocoder [6] [38] [39].\nD. Embeddings\nEmbeddings initially introduced by Natural Language Pro-\ncessing (NLP) in order to convert a word or sentence into\na real-valued vector. This approach assisted the process of\ntext in deep learning applications by inserting the property\nof closer embeddings in vector space to encode words with\nsimilar meaning. The same approach has been adopted by\nsound processing to reduce the dimensionality of the signal\n[40] [22], enhance the timbre synthesis [3] or even generate\na more interpretable representation [41] [42] to effectively\nextract parameters for a synthesizer. In [7] an autoencoder\ngenerates a latent representation to condition a WaveNet model\nwhile Dhariwal et al [43] implemented three separate encoders\nto generate vectors with different temporal resolutions.\nE. Symbolic\nIn music processing, the term symbolic refers to the use of\nrepresentations such as Musical Instrument Digital Interface\n(MIDI) or piano rolls. MIDI is a technical standard that\ndescribes a protocol, a digital interface or the link for the\nsimultaneous operation between multiple electronic musical\ninstruments. A MIDI ﬁle demonstrates the notes being played\nin every time step. Usually this ﬁle consists of information of\nthe instrument being played, the pitch and its velocity. MidiNet\n[44] is one of the most popular implementations using MIDI\nto generate music pieces.\nPiano roll constitutes a more dense representation of MIDI.\nA piece of music can be represented by a binary N × T\nmatrix where N is the number of playable notes and T is the\nnumber of timesteps. In MuseGAN [45], Generative Adversar-\nial Networks (GANs) have been applied for music generation\nusing multiple-track piano-roll representation. Also, in DeepJ\n[46], they scaled the representation matrix between 0 and 1 to\ncapture the note’s dynamics. The most notable disadvantage\nof symbolic representations is that holding a note and replay-\ning a note are demonstrated by the same representation. To\ndifferentiate these two stages, DeepJ included a second matrix\ncalled replay along with the original matrix play.\nIII. CONDITIONING REPRESENTATIONS\nNeural networks are able to generate sound based on the\nstatistical distribution of the training data. The more uniform\nthe input data to the network is, the more natural outcome can\nbe achieved. However, in cases where the amount of training\ndata is not sufﬁcient, additional data with similar properties\ncan be included by applying conditioning methods. Following\nthese techniques, the generated sound can be conditioned on\nspeciﬁc traits such as speaker’s voice [47] [27], independent\npitch [3] [48] [36], linguistic features [49] [17] or latent\nrepresentations [4] [45]. Instead of one-hot-embeddings, some\nimplementations have also used a confusion matrix to capture\na variation of emotions [39], while others provided supple-\nmentary positional information of each segment conditioning\nmusic to the artist or genre [43]. After training, the user\nis able to decide between the conditioning properties of the\nsynthesised sound.\nA. Additional Input\nThe simplest strategy for applying conditioning to deep\nlearning architectures is by including auxiliary input data\nwhile training. Two types of conditioning have been proposed,\nglobal and local [11] [50]. In global conditioning, additional\nlatent representations can be appended across all training data.\nGlobal conditioning can encode speaker’s voice or linguistics\nfeatures. Local conditioning usually refers to supplementary\ntimeseries with lower sampling rate than the original waveform\nor even mel-spectrograms, logarithmic fundamental frequency\nor auxiliary pitch information.\nWaveNet has achieved one of the most effective strategies\nfor conditioning deep neural networks [51]. Therefore, later\nsound generation schemes adopted a WaveNet network for\nconditioning. The majority of these works conditioned their\nmodel to spectrograms [31] [52] [49] [53] [33] [54] [28] [55]\nwhile others included linguistic features and pitch information\n[12] [56], phoneme encodings [6], features extracted from the\nSTRAIGHT vocoder [57] or even MIDI representations [58].\nAlthough it has been proven that convolutional networks\nare capable of effective conditioning, other architectures can\nuse auxiliary input data as well. Recurrent neural networks\nsuch as LSTMs have been adopted conditioning as frame-\nlevel auxiliary feature vectors [59] or as one-hot representation\nencoding music style [46]. Autoencoders can be conditioned\nincluding additional input to the encoder [23] [36] but also as\ninput only to the decoder [40].\nB. Input to the Generator\nGenerative Adversarial Networks (GANs) consist of two\nseparate networks, the Generator and the Discriminator. Fol-\nlowing the fundamental properties of GANs, the Generator\nconverts random noise to structured data while the Discrimi-\nnator endeavors to classify a signal as original or generated.\nFor applying conditioning in GANs, the most common tech-\nnique constitutes a biased input to the Generator. In sound\nsynthesis, a well established conditioning method includes the\nmel-spectrogram as input to the Generator [60] [16] [19].\nThis way, the synthesised sound is not just a product of a\nspeciﬁc distribution but it also obtains desirable properties. For\nexample, it can be enforced to conditioning on predetermined\ninstrument or voice. Furthermore, a Generator conditioned on\nspectrograms can also be used as a vocoder [61]. In addition\nto the mel-spectrogram, other implementations have been con-\nditioned on raw audio [62], one-hot vectors to encode musical\npitch [30], linguistic features [13], or latent representations to\nidentify speaker [63].\nC. Other\nAt last, other variations of conditioning have been intro-\nduced as well. Kim et al [14] adjusted conditioning through the\nloss function. They estimated an auxiliary probability density\nusing mel-spectrograms for local conditioning. Pink et al [64]\napplied bias terms in every layer of the convolutional network\nusing also mel-spectrograms. Extra bias to the network has\nbeen also proposed by [65] to encode linguistic features\nwhile in [7] every layer was biased with a different linear\nprojection of the temporal embeddings. In [38] linguistic\nfeatures were added to the output of each hidden layer in\nTABLE I\nOVERVIEW OF SOUND REPRESENTATIONS\nRepresentation\nPapers\nAdvantages\nDisadvantages\nComments\nWaveform\n[11] [12]\n[13] [14] [15]\n[16] [17]\n-Completely describes the waveform.\n-Directly generates the output waveform.\n-Computationally expensive.\n-Unstructured representation that does not\nreﬂect sound perception.\nUsed as input or\nconditional\nrepresentation\nSpectrograms\n[18] [19]\n[20] [21] [22]\n[23] [24] [25]\n[26] [27] [28]\n-Interpretable representations that are\nrelated to sound perception.\n-Easy to illustrate/plot.\n-Typically phase is discarded meaning it is\nnot directly invertible.\nUsed as input or\nconditional\nrepresentation\nAcoustic features\n[33] [34]\n[35] [4] [36]\n[37] [13] [6]\n[38] [39]\n-Compressed, descriptive representation of\naspects of sound.\n-Difﬁcult to synthesize waveforms with\nlong term coherence.\n-Typically don’t fully specify sounds.\nMostly used for\nconditioning\nLatent\nrepresentations\n[3] [22] [40]\n[41] [42] [7]\n[43]\n-Similar sounds lead to smaller distance in\nmulti-dimensional space.\n-Compressed representation.\n-Losses in decoding.\n-Can be difﬁcult to interpret.\nMostly VAEs\nSymbolic\n[44] [45]\n[46]\n-Meaningful description of musical\ncontent.\n-Very high level description of audio.\nthe Generator while in [44] a new network was introduced by\nthe name conditionerCNN to work along with the Generator\nencoding chords for melody generation. Finally, Juvela et al\n[37] conducted a comparative study of conditioning methods.\nIV. METHODS\nDuring recent years, deep learning models have signiﬁcantly\ncontributed to research on sound generation. Using a variety\nof deep learning algorithms, multiple representations have\nbeen applied. The most common architectures include autore-\ngressive methods, variational autoencoders (VAE), adversarial\nnetworks and normalising ﬂows. However, many approaches\ncan fall in more than one category.\nA. Autoregressive\nAutoregressive models deﬁne a category of generative mod-\nels where every new sample in a sequence of data depends\non previous samples. Autoregressive deep neural networks\ncan be represented by architectures that demonstrate this\ncontinuation implicitely or explicitely. Conventional methods\nthat implicitely indicate a time-related manner are the recurrent\nneural networks. These models are able to recall previous data\ndynamically using complex hidden state. SampleRNN [15] is\none well established research work that applies hierarchical\nrecurrent neural networks such as GRU and LSTM on different\ntemporal resolutions for sound synthesis. In order to illustrate\nthe temporal behaviour of the network, Mehri et al conducted\nexperiments to test the model’s memory by injecting one sec-\nond of silence between two random sequential samples. Other\nsigniﬁcant papers on autoregressive models using recurrent\nneural networks are WaveRNN [17], MelNet [27] or LPCNet\n[4]. In WaveRNN, they introduced a method for reducing the\nsampling time by using a batch of short sequences instead of\na unique long sequence while maintaining high quality sound.\nGenerative models where the synthesis of the sequential\nsamples follows a conditional probability distribution like\nthe one in Eq.5 are able to explicitly demonstrate temporal\ndependencies.\np(X) =\nT\nY\nt−1\np(xt|x1, ..., xt−1)\n(5)\nWaveNet [11] presents the most inﬂuential architecture of\nexplicit autoregressive models. The probability distribution\ncan be imitated by a stack of convolutional layers. However,\nto improve efﬁciency, the sequential data passes through a\nstack of dilated causal convolutional layers where the input\ndata are masked to skip some dependencies. Following a\nsimilar scheme, FFTNet [48] takes advantage of convolutional\nnetworks mimicking the FFT algorithm while upsampling the\ninput data. However, to clear up the confusion, convolutional\nnetworks do not always lead to autoregressive models [20].\nAutoregressive models have been initially proposed for\nsequential data. Therefore, in sound synthesis, raw audio is\nordinarily used as the input representation. However, many\nauxiliary representations have been applied conditioning the\naudio generation on a variety of properties. More details about\nconditioning techniques for autoregressive models have been\nalready presented in Section III.\nSince autoregressive models can be applied on sequential\ndata, they are well established in sound generation related\ntopics. Autoregressive models are easy to train and they can\nmanipulate data in real time. Furthermore, convolutional-based\nmodels can be trained in parallel. Nevertheless, although these\nmodels can be paralleled during training, the generation is\nsequential and therefore slow. Synthesised data are affected\nonly by previous samples, providing half way dependencies.\nFinally, the generation can be consistent to speciﬁc properties\nfor a deﬁnite number of samples and the outcome often lacks\nglobal structure.\nB. Normalizing Flow\nNormalizing ﬂows constitute a family of generative models\nconsisting of multiple simple distributions for transforming\ninput data to latent representations. A sequence of simple,\ninvertible and computationally inexpensive mappings z ∽p(z)\ncan model a reversible complex one. This complex transfor-\nmation is presented in Eq. 6 and the inverse can be achieved\nby repeatedly changing the variables as shown in Eq. 7. The\nmapping functions, then, can be parametrised by a deep neural\nnetwork.\nx = f0 ◦f1 ◦... ◦fk(z)\n(6)\nz = f −1\nk\n◦f −1\nk−1 ◦... ◦f −1\n0 (x)\n(7)\nWaveGlow [21], a ﬂow-based generative network, can syn-\nthesise sound from its mel-spectrogram. By applying an Afﬁne\nCoupling Layer and a 1x1 Invertible Convolution, the model\naims to maximise the likelihood of the training data. The\nimplementation has been proposed by NVIDIA and it is able\nto generate sound in real time. Insightful alternatives have also\nbeen proposed on normalising ﬂows by using only a single loss\nfunction, without any auxiliary loss terms [14] or by applying\ndilated 2-D convolutional layers [64].\nFinally, in order to reduce the number of repeated iterations\nneeded by normalising ﬂows, they have been merged with\nautoregressive methods. This architecture manages to increase\nthe performance of autoregressive models since the sampling\ncan be processed in parallel. Using Inverse Autoregressive\nFlows (IAF), Oord et al increased the efﬁciency of WaveNet\n[12]. Their implementation follows a ”probability density\ndistillation” where a pre-trained WaveNet model is used as\na teacher and scores the samples a WaveNet student outputs.\nThis way, the student can be trained in accordance with the dis-\ntribution of the teacher. A similar approach has been adopted\nby ClariNet [31], where a Gaussian inverse autoregressive\nﬂow is applied on WaveNet to train a text-to-wave neural\narchitecture.\nC. Adversarial Learning\nUnlike the Inverse Autoregressive Flows where a pre-trained\nteacher network assist a student model, in adversarial learning,\ntwo neural networks match against each other in a two-player\nminimax game. The fundamental architecture of Generative\nAdversarial Networks (GANs) is based on two models, the\nGenerator (G) and the Discriminator (D). The Generator maps\na latent representation to the data space. In a vanilla GAN, the\nGenerator maps random noise to a desirable representation.\nFor sound synthesis this representation could be raw audio\nor spectrogram. This desired representation, original or gener-\nated, is used as input to the Discriminator which is trained to\ndistinguish between real and fake data. The maximum beneﬁt\nfrom GANs is acquired when the Generator produces perfect\ndata and the Discriminator is not able to differentiate between\nreal and fake data.\nFrom a more technical point of view, the Discriminator\nis trained using only the distribution of the original data.\nIts purpose is to maximise the probability of identifying\nreal and generated data. On the other hand, the Generator\nis trained through the Discriminator. Information about the\noriginal distribution of the dataset are concealed from it and\nits aim is to minimise the error of the Discriminator. This\nminimax game can be summarised by the Eq. 8.\nmin\nG max\nD V (D, G) = Ex∽pdata(x)[log D(x)]\n+Ez∽pz(z)[log(1 −D(G(z)))]\n(8)\nOn the ﬁeld of sound generation a variety of implemen-\ntations have been proposed using numerous representations.\nIn [30], spectrograms were generated using upsampling con-\nvolutions for fast generation while in [32], they investigated\nwhether waveform or spectrograms are more effective for\nGANs applying the Wasserstein loss function. In Parallel\nWaveGAN [60], a teacher-student scheme was adopted using\nnon-autoregressive WaveNet in order to improve WaveGAN’s\nefﬁciency. Yamamoto et al [16] applied GANs using a IAF\ngenerator optimised by a probability density distillation algo-\nrithm. Also, in GAN-TTS [13], they examined an ensemble of\nDiscriminators to generate acoustic features using the Hinge\nloss function along with [61] [63]. Lastly, GANs have also\nbeen applied in a variety of applications such as text-to-\nspeech applications [19], speech synthesis [66] [67], speech\nenhancement [62] or symbolic music generation [45].\nD. Variational Autoencoders\nAn autoencoder is one of the fundamental deep learning\narchitectures consisting of two separate networks, an encoder\nand a decoder. The encoder compresses the input data into\na latent representation while the decoder synthesises data\nfrom the learned latent space. The original scheme of an\nautoencoder was initially created for dimensionality reduction\npurposes. Although theoretically the decoder bear some re-\nsemblance to the generator of GANs, the model is not well\nqualiﬁed for the synthesis of new examples. The network\nendeavors to reconstruct the original input, therefore it lacks\nof expressiveness.\nTo use autoencoders as generative models, variational au-\ntoencoders have been proposed [68]. In this architecture, the\nencoder ﬁrst models a latent distribution and then the network\nsamples from the distribution to generate latent examples. The\nsuccess of the variational autoencoders is mostly based on\nthe Kullback–Leibler (KL) divergence used as a loss function.\nThe encoder introduces a new distribution q(z|X) to estimate\np(z|X) as much as possible by minimising the KL divergence.\nThe complete loss function is demonstrated in the Eq. 9 where\nthe ﬁrst term (called reconstruction loss) is applied on the ﬁnal\nlayer and the second term (called regularization loss) adjusts\nthe latent layer.\nL = Ez∽q(z|X)[log p(X|z)] −DKL[q(z|X)||p(z)]\n(9)\nMany variations of VAE have been applied for sound\ngeneration topics. In [3] they used VAE with feedforward\nnetworks and an additive synthesiser to reproduce monophonic\nmusical notes. In [69] and [40] they applied convolutional\nlayers while in [36] a Variational Parametric Synthesiser was\nproposed using a conditional VAE.\nA modiﬁcation of variational autoencoders proposed for\nmusic synthesis is VQ-VAE [43]. In this approach, the network\nis trained to encode the input data into a sequence of discrete\ntokens. Jukebox introduces this method to ﬂatten the data and\nprocess it using autoregressive Transformers.\nV. EVALUATION\nAlthough in the last decade generative models presented\nsigniﬁcant improvements, a deﬁnitive evaluation process still\nremains an open question. Many mathematical metrics have\nbeen proposed for perceptually evaluating the generative sound\nand usually a transformation to another audio representation\nhave been adopted. However, despite the numerous attempts,\nnone of these metrics are as reliable as the subjective evalution\nof human listeners.\nA. Perceptual Evaluation\nHuman evaluation usually accounts for the mean opinion\nscore between a group of listeners. To conduct the study,\nmany researchers used crowdMOS [70], a user-friendly toolkit\nfor performing listening evaluations. As well as the mean\nopinion score, a conﬁdence interval is also been computed.\nFurthermore, in order to attract an accountable number of\nsubjects with speciﬁc characteristics, Amazon Mechanical\nTurk has been widely used. In many cases, raters have been\nasked to pass a hearing test [21], keep headphones on [61]\n[11], or only native speakers for evaluating speech have been\nasked [60] [61] [54].\nIn these mean opinion score tests, subjects have been asked\nto rate a sound in a ﬁve-point Likert scale in terms of\npleasantness [21], naturalness [11] [13] [63], sound quality\n[17] or speaker diversity [32]. In addition, subjects have been\nrequested to express a preference between sounds of two\ngenerative models hearing the same pitch [30] or speech [17]\n[11]. Finally, for evaluating WaveGAN [32], humans listened\nto digits between one to ten and were asked to indicate which\nnumber they heard.\nB. Number of Statistically-Different Bins\nThe Number of Statistically-Different Bins (NDB) is a\nmetric for unconditional generative models in order to es-\ntimate the diversity of the synthesised examples. Clustering\ntechniques are applied on the training data creating cells of\nsimilar properties. Then, the same algorithm tries to categorise\nthe generated data into the cells. If a generated example does\nnot belong to a predeﬁned cluster, then the generated sound\nis statistically signiﬁcantly different.\nGANSynth [30] used k-means to map the log spectrogram\nof the generated sound into k = 50 Voronoi cells. As well\nas Mean Opinion Score and the Number of Statistically-\nDifferent Bins, GANSynth also used Inception Score, Pitch\nAccuracy and Pitch Entropy and Frechet Inception Distance\nfor evaluation purposes. The rest of the metrics will be\nanalysed in the following sections. A similar set of evaluation\nmetrics has also been adopted by [50] including NDB.\nC. Inception Score\nThe Inception Score (IS) is another perceptual metric which\ncorrelates with human evaluation and is mostly adopted by\nGANs. For the Inception Score, a pre-trained Inception classi-\nﬁer is applied to the output of the generative model. In order to\nmeasure the diversity of the synthesised data, the IS calculates\nthe KL divergence between the model scores P(y|x) and the\nmarginal distribution P(y) as it can be expressed by the Eq.10\nfor every possible class [71]. The IS is maximised when the\ngenerated examples belong to only one class and every class\nis predicted equally often.\nIS = exp(ExDKL(P(y|x)||P(y)))\n(10)\nIn [30] and [7], a pitch classiﬁer is trained on spectrograms\nof the NSynth dataset while in WaveGAN [32], the classiﬁer\nis trained on normalised log mel-spectrograms having zero\nmean and unit variance. Finally, metrics like Pitch Accuracy\nand Pitch Entropy or a nearest neighbour technique have been\nadopted by GANSynth and WaveGAN respectively to further\nevaluate the efﬁciency of their Inception Score. Finally, in [50]\nthey also applied a modiﬁed inception score.\nD. Distances-based measurements\nThis evaluation category includes metrics that measure the\ndistance between representations of the original data and\nthe distribution of the generated examples. Binkowski et\nal. proposed two distance-based metrics, the Fr´echet Deep-\nSpeech Distance (FDSD) and the Kernel DeepSpeech Distance\n(KDSD) [13] for evaluating their text-to-speech model. The\ntwo metrics make use of the the Fr´echet distance and the\nMaximum Mean Discrepancy respectively on audio features\nextracted by a speech recognition model.\nThe Fr´echet or 2-Wasserstein distance has been proposed\nby other research papers as well. Engel et al [30] applied\nthe Fr´echet Inception Distance on features extracted by a\npitch classiﬁer while Kilgour et al [72] used this distance\nto measure the intensity of a distortion in generated sound.\nHowever, although many researchers report successful results\nusing 2-Wasserstein, Donahue et al [63] reported that a similar\nevaluation metric did not produce a desirable outcome in their\nexperiments.\nDistances-based measurements have also been investigated\nindividually by separate parameter estimations. In [3] distances\nbetween the generated loudness and fundamental frequency of\nsynthesised and training data are used.\nE. Spectral Convergence\nThe Spectral Convergence expresses the mean difference\nbetween the original and the generated spectrogram. It has\nbeen applied by [43] [20] [57] [40] in order to evaluate their\nsynthesised music. The Spectral Convergence can be expressed\nby the Eq.11 which is also identiﬁed as the minimization\nprocess of the Grifﬁn-Lim algorithm.\nSC =\nv\nu\nu\nt\nP\nn,m |S(n, m) −eS(n, m)|2\nP\nn,m S(n, m)\n(11)\nF. Log Likelihood\nA ﬁnal evaluation metric includes a Negative Log Like-\nlihood (NLL) [17] [15] and an objective Conditional Log\nLikelihood (CLL) [14] usually measured in bits per audio\nsample.\nVI. CONCLUSION\nThe choice of audio representation is one of the most\nsigniﬁcant factors in the development of deep learning models\nfor sound synthesis. Numerous representations have been pro-\nposed by previous researchers focusing on different properties.\nRaw audio is a direct representation demanding notable mem-\nory and computational cost. It is also not considered for eval-\nuating purposes since different waveforms can perceptually\nproduce the same sound. Spectrograms can overcome some of\nthe disadvantages of raw audio and have been considered as\nan alternative for training as well as for evaluation. However,\nreconstructing the original sound from its spectrogram is a\nchallenging task since it may produce sound suffering from\ndistortions and lack of phase coherence. Recently, other audio\nrepresentations have received much attention such as latent\nrepresentations, embeddings and acoustic features but they all\nrequire a powerful decoder. The choice of audio representation\nis still very much dependent on the application.\nVII. ACKNOWLEDGMENTS\nThis publication has emanated from research supported in\npart by a grant from Science Foundation Ireland under Grant\nnumber 18/CRT/6183. For the purpose of Open Access, the\nauthor has applied a CC BY public copyright licence to\nany Author Accepted Manuscript version arising from this\nsubmission’.\nREFERENCES\n[1] H. Gm, M. K. Gourisaria, M. Pandey, and S. S. Rautaray, “A compre-\nhensive survey and analysis of generative models in machine learning,”\nComputer Science Review, vol. 38, p. 100285, Nov. 2020.\n[2] M. Huzaifah and L. Wyse, “Deep generative models for musical au-\ndio synthesis,” arXiv:2006.06426 [cs, eess, stat], Nov. 2020.\narXiv:\n2006.06426.\n[3] J. Engel, L. Hantrakul, C. Gu, and A. Roberts, “DDSP: Differentiable\nDigital Signal Processing,” arXiv:2001.04643 [cs, eess, stat], Jan. 2020.\narXiv: 2001.04643.\n[4] J.-M. Valin and J. Skoglund, “LPCNET: Improving Neural Speech\nSynthesis through Linear Prediction,” in ICASSP 2019 - 2019 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), (Brighton, United Kingdom), pp. 5891–5895, IEEE, May\n2019.\n[5] P. Govalkar, J. Fischer, F. Zalkow, and C. Dittmar, “A Comparison of\nRecent Neural Vocoders for Speech Signal Reconstruction,” in 10th\nISCA Speech Synthesis Workshop, pp. 7–12, ISCA, Sept. 2019.\n[6] M. Blaauw and J. Bonada, “A Neural Parametric Singing Synthesizer,”\narXiv:1704.03809 [cs], Aug. 2017. arXiv: 1704.03809.\n[7] J. Engel, C. Resnick, A. Roberts, S. Dieleman, D. Eck, K. Simonyan, and\nM. Norouzi, “Neural Audio Synthesis of Musical Notes with WaveNet\nAutoencoders,” arXiv:1704.01279 [cs], Apr. 2017. arXiv: 1704.01279.\n[8] R. Manzelli, V. Thakkar, A. Siahkamari, and B. Kulis, “Conditioning\nDeep Generative Raw Audio Models for Structured Automatic Music,”\narXiv:1806.09905 [cs, eess, stat], June 2018. arXiv: 1806.09905.\n[9] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: A Vocoder-Based\nHigh-Quality Speech Synthesis System for Real-Time Applications,”\nIEICE Transactions on Information and Systems, vol. E99.D, no. 7,\npp. 1877–1884, 2016.\n[10] C. E. Shannont, “Communication in the Presence of Noise,” PROCEED-\nINGS OF THE I.R.E., p. 12, 1949.\n[11] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves,\nN. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “WaveNet: A Gener-\native Model for Raw Audio,” arXiv:1609.03499 [cs], Sept. 2016. arXiv:\n1609.03499.\n[12] A. v. d. Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals,\nK. Kavukcuoglu, G. v. d. Driessche, E. Lockhart, L. C. Cobo, F. Stim-\nberg, N. Casagrande, D. Grewe, S. Noury, S. Dieleman, E. Elsen,\nN. Kalchbrenner, H. Zen, A. Graves, H. King, T. Walters, D. Belov, and\nD. Hassabis, “Parallel WaveNet: Fast High-Fidelity Speech Synthesis,”\narXiv:1711.10433 [cs], Nov. 2017. arXiv: 1711.10433.\n[13] M.\nBi´nkowski,\nJ.\nDonahue,\nS.\nDieleman,\nA.\nClark,\nE.\nElsen,\nN. Casagrande, L. C. Cobo, and K. Simonyan, “High Fidelity Speech\nSynthesis with Adversarial Networks,” arXiv:1909.11646 [cs, eess],\nSept. 2019. arXiv: 1909.11646.\n[14] S. Kim, S.-g. Lee, J. Song, J. Kim, and S. Yoon, “FloWaveNet : A\nGenerative Flow for Raw Audio,” arXiv:1811.02155 [cs, eess], May\n2019. arXiv: 1811.02155.\n[15] S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain, J. Sotelo,\nA. Courville, and Y. Bengio, “SampleRNN: An Unconditional End-\nto-End Neural Audio Generation Model,” arXiv:1612.07837 [cs], Feb.\n2017. arXiv: 1612.07837.\n[16] R. Yamamoto, E. Song, and J.-M. Kim, “Probability density distil-\nlation with generative adversarial networks for high-quality parallel\nwaveform generation,” arXiv:1904.04472 [cs, eess], Aug. 2019. arXiv:\n1904.04472.\n[17] N. Kalchbrenner, E. Elsen, K. Simonyan, S. Noury, N. Casagrande,\nE.\nLockhart,\nF.\nStimberg,\nA.\nv.\nd.\nOord,\nS.\nDieleman,\nand\nK. Kavukcuoglu, “Efﬁcient Neural Audio Synthesis,” arXiv:1802.08435\n[cs, eess], June 2018. arXiv: 1802.08435.\n[18] Y. Wang, R. J. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly,\nZ. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis,\nR. Clark, and R. A. Saurous, “Tacotron: Towards End-to-End Speech\nSynthesis,” arXiv:1703.10135 [cs], Apr. 2017. arXiv: 1703.10135.\n[19] P. Neekhara, C. Donahue, M. Puckette, S. Dubnov, and J. McAuley, “Ex-\npediting TTS Synthesis with Adversarial Vocoding,” arXiv:1904.07944\n[cs, eess], July 2019. arXiv: 1904.07944.\n[20] S. O. Arik, H. Jun, and G. Diamos, “Fast Spectrogram Inversion using\nMulti-head Convolutional Neural Networks,” IEEE Signal Processing\nLetters, vol. 26, pp. 94–98, Jan. 2019. arXiv: 1808.06719.\n[21] R. Prenger, R. Valle, and B. Catanzaro, “WaveGlow: A Flow-based\nGenerative Network for Speech Synthesis,” arXiv:1811.00002 [cs, eess,\nstat], Oct. 2018. arXiv: 1811.00002.\n[22] K. Peng, W. Ping, Z. Song, and K. Zhao, “Non-Autoregressive Neu-\nral Text-to-Speech,” arXiv:1905.08459 [cs, eess], June 2020.\narXiv:\n1905.08459.\n[23] C. Aouameur, P. Esling, and G. Hadjeres, “Neural Drum Machine\n: An Interactive System for Real-time Synthesis of Drum Sounds,”\narXiv:1907.02637 [cs, eess], Nov. 2019. arXiv: 1907.02637.\n[24] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y.\nLiu, “FastSpeech: Fast, Robust and Controllable Text to Speech,”\narXiv:1905.09263 [cs, eess], Nov. 2019. arXiv: 1905.09263.\n[25] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu,\n“FastSpeech 2: Fast and High-Quality End-to-End Text to Speech,”\narXiv:2006.04558 [cs, eess], Mar. 2021. arXiv: 2006.04558.\n[26] R. Liu, B. Sisman, F. Bao, G. Gao, and H. Li, “WaveTTS: Tacotron-\nbased TTS with Joint Time-Frequency Domain Loss,” arXiv:2002.00417\n[cs, eess], Apr. 2020. arXiv: 2002.00417.\n[27] S. Vasquez and M. Lewis, “MelNet: A Generative Model for Audio in\nthe Frequency Domain,” arXiv:1906.01083 [cs, eess, stat], June 2019.\narXiv: 1906.01083.\n[28] S. Huang, Q. Li, C. Anil, X. Bao, S. Oore, and R. B. Grosse, “Tim-\nbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical\nTimbre Transfer,” arXiv:1811.09620 [cs, eess, stat], May 2019. arXiv:\n1811.09620.\n[29] G. A. Velasco, N. Holighaus, M. D¨orﬂer, and T. Grill, “CONSTRUCT-\nING AN INVERTIBLE CONSTANT-Q TRANSFORM WITH NON-\nSTATIONARY GABOR FRAMES,” p. 8, 2011.\n[30] J. Engel, K. K. Agrawal, S. Chen, I. Gulrajani, C. Donahue, and\nA. Roberts, “GANSYNTH: ADVERSARIAL NEURAL AUDIO SYN-\nTHESIS,” p. 17, 2019.\n[31] W. Ping, K. Peng, and J. Chen, “ClariNet: Parallel Wave Generation in\nEnd-to-End Text-to-Speech,” p. 13.\n[32] C. Donahue, J. McAuley, and M. Puckette, “ADVERSARIAL AUDIO\nSYNTHESIS,” p. 16, 2019.\n[33] Y. Wang, D. Stanton, Y. Zhang, R. Skerry-Ryan, E. Battenberg, J. Shor,\nY. Xiao, F. Ren, Y. Jia, and R. A. Saurous, “Style Tokens: Unsupervised\nStyle Modeling, Control and Transfer in End-to-End Speech Synthesis,”\np. 10.\n[34] X. Wang, S. Takaki, and J. Yamagishi, “Neural source-ﬁlter waveform\nmodels for statistical parametric speech synthesis,” arXiv:1904.12088\n[cs, eess, stat], Nov. 2019. arXiv: 1904.12088.\n[35] A. Defossez, N. Zeghidour, N. Usunier, L. Bottou, and F. Bach, “SING:\nSymbol-to-Instrument Neural Generator,” p. 11.\n[36] K. Subramani, P. Rao, and A. D’Hooge, “Vapar Synth - A Variational\nParametric Model for Audio Synthesis,” in ICASSP 2020 - 2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), (Barcelona, Spain), pp. 796–800, IEEE, May 2020.\n[37] L. Juvela, B. Bollepalli, V. Tsiaras, and P. Alku, “GlotNet—A Raw\nWaveform Model for the Glottal Excitation in Statistical Parametric\nSpeech Synthesis,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 27, pp. 1019–1030, June 2019.\n[38] S. Yang, L. Xie, X. Chen, X. Lou, X. Zhu, D. Huang, and H. Li,\n“Statistical Parametric Speech Synthesis Using Generative Adversarial\nNetworks Under A Multi-task Learning Framework,” arXiv:1707.01670\n[cs], July 2017. arXiv: 1707.01670.\n[39] G. E. Henter, J. Lorenzo-Trueba, X. Wang, and J. Yamagishi, “Deep\nEncoder-Decoder Models for Unsupervised Learning of Controllable\nSpeech Synthesis,” arXiv:1807.11470 [cs, eess, stat], Sept. 2018. arXiv:\n1807.11470.\n[40] A. Bitton, P. Esling, and T. Harada, “Neural Granular Sound Synthesis,”\narXiv:2008.01393 [cs, eess], Aug. 2020. arXiv: 2008.01393.\n[41] P. Esling, A. Chemla-Romeu-Santos, and A. Bitton, “Generative timbre\nspaces: regularizing variational auto-encoders with perceptual metrics,”\narXiv:1805.08501 [cs, eess], Oct. 2018. arXiv: 1805.08501.\n[42] P. Esling, N. Masuda, A. Bardet, R. Despres, and A. Chemla-Romeu-\nSantos, “Universal audio synthesizer control with normalizing ﬂows,”\narXiv:1907.00971 [cs, eess, stat], July 2019. arXiv: 1907.00971.\n[43] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever,\n“Jukebox: A Generative Model for Music,” arXiv:2005.00341 [cs, eess,\nstat], Apr. 2020. arXiv: 2005.00341.\n[44] L.-C. Yang, S.-Y. Chou, and Y.-H. Yang, “MidiNet: A Convolutional\nGenerative Adversarial Network for Symbolic-domain Music Genera-\ntion,” arXiv:1703.10847 [cs], July 2017. arXiv: 1703.10847.\n[45] H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang, “MuseGAN:\nMulti-track Sequential Generative Adversarial Networks for Symbolic\nMusic Generation and Accompaniment,” arXiv:1709.06298 [cs, eess,\nstat], Nov. 2017. arXiv: 1709.06298.\n[46] H. H. Mao, T. Shin, and G. W. Cottrell, “DeepJ: Style-Speciﬁc Music\nGeneration,” 2018 IEEE 12th International Conference on Semantic\nComputing (ICSC), pp. 377–382, Jan. 2018. arXiv: 1801.00887.\n[47] Y. Zhao, S. Takaki, H.-T. Luong, J. Yamagishi, D. Saito, and N. Mine-\nmatsu, “Wasserstein GAN and Waveform Loss-Based Acoustic Model\nTraining for Multi-Speaker Text-to-Speech Synthesis Systems Using a\nWaveNet Vocoder,” IEEE Access, vol. 6, pp. 60478–60488, 2018.\n[48] Z. Jin, A. Finkelstein, G. J. Mysore, and J. Lu, “Fftnet: A Real-\nTime Speaker-Dependent Neural Vocoder,” in 2018 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\n(Calgary, AB), pp. 2251–2255, IEEE, Apr. 2018.\n[49] S. Arik, G. Diamos, A. Gibiansky, J. Miller, K. Peng, W. Ping,\nJ. Raiman, and Y. Zhou, “Deep Voice 2: Multi-Speaker Neural Text-\nto-Speech,” arXiv:1705.08947 [cs], Sept. 2017. arXiv: 1705.08947.\n[50] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, “DiffWave: A\nVersatile Diffusion Model for Audio Synthesis,” arXiv:2009.09761 [cs,\neess, stat], Mar. 2021. arXiv: 2009.09761.\n[51] J. Boilard, P. Gournay, and R. Lefebvre, “A Literature Review of\nWaveNet: Theory, Application and Optimization,” p. 17.\n[52] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen,\nY. Zhang, Y. Wang, R. J. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgian-\nnakis, and Y. Wu, “Natural TTS Synthesis by Conditioning WaveNet\non Mel Spectrogram Predictions,” arXiv:1712.05884 [cs], Feb. 2018.\narXiv: 1712.05884.\n[53] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang,\nJ. Raiman, and J. Miller, “Deep Voice 3: Scaling Text-to-Speech with\nConvolutional Sequence Learning,” arXiv:1710.07654 [cs, eess], Feb.\n2018. arXiv: 1710.07654.\n[54] N. Li, S. Liu, Y. Liu, S. Zhao, M. Liu, and M. Zhou, “Neural Speech\nSynthesis with Transformer Network,” arXiv:1809.08895 [cs], Jan.\n2019. arXiv: 1809.08895.\n[55] M. Angrick, C. Herff, E. Mugler, M. C. Tate, M. W. Slutzky, D. J.\nKrusienski, and T. Schultz, “Speech synthesis from ECoG using densely\nconnected 3D convolutional neural networks,” Journal of Neural Engi-\nneering, vol. 16, p. 036019, June 2019.\n[56] S. O. Arık, M. Chrzanowski, A. Coates, G. Diamos, A. Gibiansky,\nY. Kang, X. Li, J. Miller, A. Ng, J. Raiman, S. Sengupta, and\nM. Shoeybi, “Deep Voice: Real-time Neural Text-to-Speech,” p. 10.\n[57] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and T. Toda,\n“Speaker-Dependent WaveNet Vocoder,” in Interspeech 2017, pp. 1118–\n1122, ISCA, Aug. 2017.\n[58] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-Z. A. Huang,\nS. Dieleman, E. Elsen, J. Engel, and D. Eck, “Enabling Factorized\nPiano Music Modeling and Generation with the MAESTRO Dataset,”\narXiv:1810.12247 [cs, eess, stat], Jan. 2019. arXiv: 1810.12247.\n[59] Z.-H. Ling, Y. Ai, Y. Gu, and L.-R. Dai, “Waveform Modeling and\nGeneration Using Hierarchical Recurrent Neural Networks for Speech\nBandwidth Extension,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 26, pp. 883–894, May 2018.\n[60] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel WaveGAN: A fast\nwaveform generation model based on generative adversarial networks\nwith multi-resolution spectrogram,” arXiv:1910.11480 [cs, eess], Feb.\n2020. arXiv: 1910.11480.\n[61] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh,\nJ. Sotelo, A. de Brebisson, Y. Bengio, and A. Courville, “MelGAN:\nGenerative Adversarial Networks for Conditional Waveform Synthesis,”\narXiv:1910.06711 [cs, eess], Dec. 2019. arXiv: 1910.06711.\n[62] S. Pascual, A. Bonafonte, and J. Serr`a, “SEGAN: Speech Enhancement\nGenerative Adversarial Network,” arXiv:1703.09452 [cs], June 2017.\narXiv: 1703.09452.\n[63] J. Donahue, S. Dieleman, M. Bi´nkowski, E. Elsen, and K. Simonyan,\n“End-to-End Adversarial Text-to-Speech,” arXiv:2006.03575 [cs, eess],\nMar. 2021. arXiv: 2006.03575.\n[64] W. Ping, K. Peng, K. Zhao, and Z. Song, “WaveFlow: A Compact Flow-\nbased Model for Raw Audio,” p. 11.\n[65] K. Rao, F. Peng, H. Sak, and F. Beaufays, “Grapheme-to-phoneme\nconversion using Long Short-Term Memory recurrent neural networks,”\nin 2015 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), (South Brisbane, Queensland, Australia),\npp. 4225–4229, IEEE, Apr. 2015.\n[66] K. Oyamada, H. Kameoka, T. Kaneko, K. Tanaka, N. Hojo, and H. Ando,\n“Generative adversarial network-based approach to signal reconstruction\nfrom magnitude spectrogram,” in 2018 26th European Signal Processing\nConference (EUSIPCO), (Rome), pp. 2514–2518, IEEE, Sept. 2018.\n[67] Y. Saito, S. Takamichi, and H. Saruwatari, “Statistical Parametric Speech\nSynthesis Incorporating Generative Adversarial Networks,” IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, vol. 26,\npp. 84–96, Jan. 2018.\n[68] D. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,”\narXiv:1312.6114 [cs, stat], May 2014. arXiv: 1312.6114.\n[69] A. Pandey and D. Wang, “A New Framework for CNN-Based Speech\nEnhancement in the Time Domain,” IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, vol. 27, pp. 1179–1188, July 2019.\n[70] F. Ribeiro, D. Florˆencio, C. Zhang, and M. Seltzer, “crowdMOS: An\nApproach for Crowdsourcing Mean Opinion Score Studies,” p. 4.\n[71] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and\nX. Chen, “Improved Techniques for Training GANs,” arXiv:1606.03498\n[cs], June 2016. arXiv: 1606.03498.\n[72] K. Kilgour, M. Zuluaga, D. Roblek, and M. Shariﬁ, “Fr\\’echet Audio\nDistance: A Metric for Evaluating Music Enhancement Algorithms,”\narXiv:1812.08466 [cs, eess], Jan. 2019. arXiv: 1812.08466.\n",
  "categories": [
    "cs.SD",
    "cs.LG",
    "eess.AS"
  ],
  "published": "2022-01-07",
  "updated": "2022-01-07"
}