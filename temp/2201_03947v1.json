{
  "id": "http://arxiv.org/abs/2201.03947v1",
  "title": "Active Reinforcement Learning -- A Roadmap Towards Curious Classifier Systems for Self-Adaptation",
  "authors": [
    "Simon Reichhuber",
    "Sven Tomforde"
  ],
  "abstract": "Intelligent systems have the ability to improve their behaviour over time\ntaking observations, experiences or explicit feedback into account. Traditional\napproaches separate the learning problem and make isolated use of techniques\nfrom different field of machine learning such as reinforcement learning, active\nlearning, anomaly detection or transfer learning, for instance. In this\ncontext, the fundamental reinforcement learning approaches come with several\ndrawbacks that hinder their application to real-world systems: trial-and-error,\npurely reactive behaviour or isolated problem handling. The idea of this\narticle is to present a concept for alleviating these drawbacks by setting up a\nresearch agenda towards what we call \"active reinforcement learning\" in\nintelligent systems.",
  "text": "ACTIVE REINFORCEMENT LEARNING\nA Roadmap Towards Curious Classiﬁer Systems for Self-Adaptation\nSimon Reichhuber1\na and Sven Tomforde1\nb\n1Intelligent Systems, University of Kiel, Hermann-Rodewald-Str. 3, Kiel, Germany\n{sir,st}@informatik.uni-kiel.de\nKeywords:\nOrganic Computing, active learning, reinforcement learning, intelligent systems, active reinforcement\nlearning, learning classiﬁer systems\nAbstract:\nIntelligent systems have the ability to improve their behaviour over time taking observations, experiences or\nexplicit feedback into account. Traditional approaches separate the learning problem and make isolated use of\ntechniques from different ﬁeld of machine learning such as reinforcement learning, active learning, anomaly\ndetection or transfer learning, for instance. In this context, the fundamental reinforcement learning approaches\ncome with several drawbacks that hinder their application to real-world systems: trial-and-error, purely reac-\ntive behaviour or isolated problem handling. The idea of this article is to present a concept for alleviating these\ndrawbacks by setting up a research agenda towards what we call “active reinforcement learning” in intelligent\nsystems.\n1\nINTRODUCTION\nInformation and communication technology faces a\ntrend towards increasingly complex solutions, e.g.,\ncharacterised by the laws of Moore (Moore, 1965)\nand Glass (Glass, 2002). As a consequence, tradi-\ntional concepts for design, development, and main-\ntenance have reached their limits.\nWithin the last\ndecade, a paradigm shift in engineering such sys-\ntems has been postulated that claims to master com-\nplexity issues by means of self-adaptation and self-\norganisation. Concepts and techniques emerged that\nmove traditional design-time decisions to runtime and\nfrom the system engineer to the systems themselves.\nAs a result, intelligent and autonomously acting sys-\ntems are targeted, with the self-adapting and self-\norganising (SASO) systems domain serving as an\numbrella for several research initiatives focusing on\nthese issues, including Organic Computing (M¨uller-\nSchloer and Tomforde, 2017), Autonomic Comput-\ning (Kephart and Chess, 2003), Interwoven Systems\n(Tomforde et al., 2014), or Self-aware Computing\nSystems (Kounev et al., 2017).\nThe basic idea is in all cases that individual sys-\ntems react autonomously to changing conditions, ﬁnd\nappropriate reactions, and optimise this process over\na\nhttps://orcid.org/0000-0001-8951-8962\nb\nhttps://orcid.org/0000-0002-5825-8915\ntime—resulting in intelligent system behaviour. For\nthe remainder of this article, we deﬁne such an “intel-\nligent system” (according to (Tomforde et al., 2017))\nas a computing system that achieves or maintains a\ncertain level of performance, even when operating in\nenvironments that change over time and even if it is\nexposed to disturbances or emergent situations. Such\nan intelligent system is autonomously alerting its own\nbehaviour with the goal to improve it over time.\nA keystone in this deﬁnition is the ability of an\nintelligent system to learn autonomously at runtime\n(D’Angelo et al., 2019). This means that approaches\nbased on massive training data or continuous feed-\nback/supervision by users are not feasible. In turn,\nthe system has to ﬁgure out what to do in which\nsituation:\nthe classic reinforcement learning (RL)\nparadigm combined with further mechanisms from\nthe domain of machine learning such as anomaly de-\ntection, transfer learning, or collaborative learning\n(D’Angelo et al., 2020).\nSeveral approaches have been presented where\nvarying RL techniques are used for enabling self-\nimproving\nruntime\nadaptation\nand\norganisation.\nHowever, these approaches come with several draw-\nbacks that hinder their application to real-world sys-\ntems: trial-and-error, purely reactive behaviour, iso-\nlated problem handling, etc. The idea of this article\nis to present a concept for alleviating these drawbacks\nby setting up a research agenda towards what we call\narXiv:2201.03947v1  [cs.LG]  11 Jan 2022\n“active reinforcement learning” in intelligent systems.\nThe remainder of this article is organised as fol-\nlows: Section 2 describes the technical background\nof this article. This includes a basic understanding of\nhow intelligent systems are designed, fundamentals\nfor reinforcement learning and its utilisation in intel-\nligent systems, as well as a brief introduction of the\nkey ideas of active learning. Afterwards, Section 3 de-\nscribes our concept for active reinforcement learning,\nwhich results in the deﬁnition of a research road-map\npresented in Section 4. Finally, Section 5 summarises\nthe article and gives an outlook to future work.\n2\nBACKGROUND\nThis section brieﬂy describes basic concepts neces-\nsary for understanding the remainder of this article.\nWe initially introduce our model of an intelligent\nsystem with an emphasis on learning autonomously\nat runtime.\nAfterwards, we explain the basic RL\nparadigm and the most popular variant as used in in-\ntelligent systems. Finally, we summarise the concept\nof active learning (AL) with a special focus on stream\ndata consideration as most prominent variant for util-\nisation in intelligent systems.\n2.1\nSystem model\nWe assume a SASO system S to consist of a poten-\ntially large set A of autonomous subsystems ai in a vir-\ntual, physical or hybrid environment. We refer to the\nterm (sub)system using the terminology from the Or-\nganic Computing domain (M¨uller-Schloer and Tom-\nforde, 2017), and, for a better readability, omit the\nsub if it is clear from the context that not the over-\nall system is meant (synonyms are entity or agent).\nEach ai is equipped with sensors and actuators. In-\nternally, each ai distinguishes between a productive\nsystem part (PS, responsible for the basic purpose of\nthe system) and a control mechanism (CM, respon-\nsible for controlling the behaviour of the PS and de-\nciding about relations to other subsystems). This cor-\nresponds to the separation of concerns between Sys-\ntem under Observation and Control (SuOC) and Ob-\nserver/Controller tandem in the terminology of Or-\nganic Computing (OC) (Tomforde et al., 2011b) or\nManaged Resource and Autonomic Manager in terms\nof Autonomic Computing (Kephart and Chess, 2003).\nFigure 1 illustrates this concept with its input and out-\nput relations.\nEach subsystem in the overall system can assume\ndifferent conﬁgurations.\nSuch a conﬁguration typ-\nically consists of different components. We deﬁne\nProductive System\nControl Mechanism (CM)\nSensors\nActuators\nENVIRONMENT\nObservation \nof raw data\nExecution of\ninterventions\nGoals\nUser\nFigure 1: Schematic illustration of a subsystem ai.\nthe entire conﬁguration space of a subsystem ai as\nCartesian product Ci = ci1 × ··· × cim, where cij are\nthe components of the conﬁguration. The user guides\nthe behaviour of ai using a utility function U and (in\nmost cases) does not intervene at the decision level:\nActual decisions are taken by the productive system\nand the CM. We model each subsystem to act au-\ntonomously, i.e., there are no control hierarchies in\nthe overall system. Consider, e.g., a router a1 in a\ncomputer network as an illustrating example follow-\ning the ideas of (Tomforde et al., 2010). It can take\nvarying conﬁgurations into account, such as the pro-\ncessed network protocol or parameter settings (Tom-\nforde et al., 2009). E.g., an interval c11 = [0,100] for\nthe timeout parameter in seconds and the set c12 =\n{1,2,...,16} for the buffer size in kilobyte. The en-\ntire conﬁguration space of system a1 would then be\nC1 = [0,100]×{1,2,...,16}.\nBesides the conﬁguration space, we consider a lo-\ncal reward. In particular, each subsystem estimates\nthe success of its decisions at runtime—as a response\nto actions taken before. This is realised based on a\nfeedback mechanism, with feedback possibly stem-\nming from the environment of the subsystem (i.e., di-\nrect feedback) or from manual reward assignments\n(i.e., indirect feedback). This resembles the classic\nreinforcement model (Sutton and Barto, 1998), where\nthe existence of such a reward is one of the basic as-\nsumptions.\n2.2\nReinforcement learning for\nself-adaptation in intelligent systems\nThe basic reinforcement learning (RL) paradigm re-\nlies on a continuous interplay of learner and environ-\nment. The control mechanism of an intelligent system\ncontinuously monitors the environmental conditions\nand thereby perceives the current state of the environ-\nment (st). The index t refers to the point in time of the\nperception indicating that the RL loop is performed\nin discrete time steps. Based on this state descrip-\ntion, the RL learner decides about the next action (at)\nthat manipulates the environment (e.g., a change of\nparameters of a controlled productive system). This\nresults in a possibly changed state of the environ-\nment. Hence, in the next time step (t +1) an updated\nstate (st+1) is presented to the learner. In addition, the\nlearner receives a feedback signal (rt+1) that quanti-\nﬁes the success of the action (if an immediate reward\nis possible) or is used to assess the ’value’ of the cur-\nrent situation. Figure 2 illustrates this process.\nRL Learner\nEnvironment\naction\n𝑎𝑡\n𝑟𝑡+1\n𝑠𝑡+1\nstate\n𝑠𝑡\nreward\n𝑟𝑡\nFigure 2: The basic RL model\nSeveral different RL techniques are available in\nliterature. However, the utilisation of RL in intelligent\nsystem (especially for adaptive self-conﬁguration, for\ninstance) come with a set of requirements rendering\nmost of the techniques not applicable–the most im-\nportant are:\n1. Decisions are taken based on sensor information,\nwhich is typically available in terms of time-series\nof different real-valued attributes.\nAs a conse-\nquence, the RL technique has to deal with real\nvalues.\n2. An intelligent system typically uses more than just\na few sensor values, i.e. it has to master a situa-\ntion space Rd with d deﬁning the number of di-\nmensions of this space.\n3. The efﬁciency of the RL technique depends di-\nrectly on the number of possible choices, since\nlearning requires testing different possibilities.\nConsequently, a simple list of situation-action\nmappings is not feasible and the learner needs the\nability to generalise.\nFollowing these requirements, the class of “Learn-\ning Classiﬁer Systems” (LCS) (Wilson, 2000) in gen-\neral and the variant “Extended Classiﬁer System”\n(XCS) by Wilson (Wilson, 1995) in particular have\nproven to provide suitable techniques. An alternative\nis the usage of Deep Reinforcmeent Learning tech-\nnology that suffers from the missing interpretability\n(i.e., this is typically neglected in SASO systems since\nte user or administrator has no insights in what the\nsystem will do and why). An overview of LCS vari-\nants can be found in (Sigaud and Wilson, 2007). In\ncontrast, the konlwedge of LCS is stored in an ex-\nplicit rule-base that is evolved over time. LCS/XCS\nare ﬂexible, evolutionary rule-based RL systems—\nwe rely on the XCS-R variant (’R’ for real values) in\nthe following, since this is the variant used in intelli-\ngent/SASO systems (D’Angelo et al., 2019). XCS-R\nrelies on a population [P] of classiﬁers, where each of\nthese classiﬁers is an “IF-THEN” rule with evaluation\ncriteria that partition the input space X and thus ap-\nproximate the problem space locally. A steady-state\nniche genetic algorithm (GA) is responsible for im-\nproving the coverage in the match set [M] (that con-\ntains all classiﬁers where the condition part matches\nthe current conditions). Based on the set of match-\ning classiﬁers, a ﬁtness-weighted prediction is calcu-\nlated for all actions proposed by classiﬁers contained\nin [M]—using these values, a roulette-wheel approach\nis followed to select the action to be applied. All clas-\nsiﬁers in [M] promoting the selected action are trans-\nferred to the action set [A] for later consideration. For\nlearning purposes, a discount is taken from the clas-\nsiﬁers contained in [M], a (delayed) reward is con-\nsidered, and the classiﬁers of the last action set (i.e.,\nthose being responsible for the last behaviour) are up-\ndated accordingly (i.e., using a learning rate α). Fig-\nure 3 illustrates the process for the variant XCS.\nThe classiﬁers are usually initialised with par-\ntially predeﬁned initial but also randomly selected\nvalues. A covering mechanism generates new clas-\nsiﬁers if [M] contains too few or inappropriate clas-\nsiﬁers. However, this is a rather reactive behaviour.\nAs mentioned before, a steady-state niche GA is con-\nstantly creating new knowledge in form of classi-\nﬁers: Successful classiﬁers are selected to be repro-\nduced, recombined, and mutated to search the local\nneighbourhood of the environmental niche with the\nresult of a better coverage. In particular, this means\nthat a globally optimal subregion of the input space\nis evolved that is characterised by a high accuracy\nin predicting the corresponding payoff (or reward).\nAgain, this GA can be considered to act re-actively\nsince it is performed in cycles and only considers\n’good’ classiﬁers without any kind of creativity. Fig-\nure 4 illustrates an example of a population of XCS in\na two-dimensional search problem: The space is un-\nevenly covered by classiﬁers with different size of the\ncondition part and different evaluation values. In gen-\neral, XCS keeps track of the three primary attributes p\n(prediction of the expected payoff), ε (prediction error\nobserved over time), and F (’ﬁtness’ of the classiﬁer)\nand several further ’book keeping’ attributes such as\nthe experience (number of occurrences in the action\nset), numerosity (number of contained sub-classiﬁers\nafter subsumption), etc.\nEnvironment\nDetectors\nEffectors\n[(0.12;0.15);…;(0.81;0.89)]   A-12  43   .05   .98 \n[(0.75;0.85);…;(0.88;0.89)]   A-05  32   .23   .61 \n[(0.02;0.25);…;(0.80;0.95)]   A-12  23   .66   .51 \n[(0.10;0.13);…;(0.85;0.88)]   A-19  49   .01   .99 \n[(0.08;0.21);…;(0.77;0.79)]   A-03  38   .29   .62 \n[(0.12;0.15);…;(0.81;0.89)]   A-28  09   .47   .42 \nCondition\nAction  𝑝\n𝜀\n𝐹\n…\nPopulation\n[P]\nCheck\nCopy matching classifiers\nMatch\nSet\n[M]\n[(0.12;0.15);…;(0.81;0.89)]   A-12  43   .05   .98\n[(0.02;0.25);…;(0.80;0.95)]   A-12  23   .66   .51 \n[(0.10;0.13);…;(0.85;0.88)]   A-19  49   .01   .99\n[(0.12;0.15);…;(0.81;0.89)]   A-28  09   .47   .42\n[(0.13;0.16);…;(0.90;0.96)]   A-19  46   .02   .96 \nPerception: (0.13,…,0.85)\nGA\nA-12    A-19   A-28\n36.15  47.52  9.00\nPrediction Array\n[(0.10;0.13);…;(0.85;0.88)]   A-19  49   .01   .99\n[(0.13;0.16);…;(0.90;0.96)]   A-19  46   .02   .96\nAction Set [A]\nAction \nselection\n“A-19“\nDelay=1\nReward\n𝑟𝑡+1\nPrevious Action Set\n[𝐴]𝑡−1\nmax\n+\nP\nUpdate:\n•\nFitness\n•\nError\n•\nPrediction\nFigure 3: Schematic illustration of XCS-R with example population.\nAccuracy:\nStrength: \nExperience:\n0.76\n0.75\n7\nFeature A\nFeature B\nAccuracy:\nStrength: \nExperience:\n0.86\n0.98\n12\nAccuracy:\nStrength: \nExperience:\n0. 36\n0.18\n17\nAccuracy:\nStrength: \nExperience:\n0.61\n0.82\n11\nAccuracy:\nStrength: \nExperience:\n0.92\n0.99\n19\nAccuracy:\nStrength: \nExperience:\n0.55\n0.59\n14\nAccuracy:\nStrength: \nExperience:\n0.61\n0.48\n8\nAccuracy:\nStrength: \nExperience:\n0.02\n0.03\n3\nAction ignored\nAction A\nAction B\nLegend\nAction C\nFigure 4: Example for a population of XCS covering the condition space.\nAlthough XCS has proven to be applicable to real-\nworld problems (e.g., (Tomforde et al., 2011a)), it\ncomes with some major drawbacks:\n1. Exploration vs. exploitation is done by a roulette-\nwheel approach considering the p and F values\nof classiﬁers in [M] only. There is no controlled\nexploration behaviour and no adaptation of the\nlearning rate α.\n2. XCS builds up and manages its population in a\npurely reactive manner: It generates and adapts\nclassiﬁers only in those parts of the input space\nwhere it observes stimuli.\nIt does not gener-\nate classiﬁers for other niches pro-actively and,\nhence, is not prepared for new conditions.\n3. XCS generates new classiﬁers either by genetic\noperations in a niche (i.e., the GA) or randomly\n(the covering mechanism). It does not use existing\nknowledge of the entire population or relies on a\ngoal-oriented mechanism for the covering part.\n4. The number of classiﬁers contained in the pop-\nulation is limited. Consequently, XCS contains\nmechanisms to aggregate classiﬁers into one and\nto delete less promising classiﬁers. However, it\ndoes not actively manage its population.\n5. The population just stores the best actions fol-\nlowing the current experiences. However, it does\nnot make use of knowledge about unsuccessful at-\ntempts.\n6. The approach can handle concept drifts in a purely\npassive manner, but it does not contain mecha-\nnisms to detect this at runtime and ﬁnd efﬁcient\nreactions.\nThe idea of this article is to develop the concept\nfor a novel XCS variant that is able to handle these\nissues. In particular, this means to turn the passive or\nreactive XCS into an active or proactively acting sys-\ntem, to which we refer as active reinforcement learn-\ning. Therefore, we make use of concepts from the ac-\ntive learning (Settles, 2009; Settles, 2012) domain—\nwhich is introduced in the next paragraphs.\n2.3\nActive Learning\nActive Learning (AL) is a semi-supervised learning\nparadigm that is based on managing the sample selec-\ntion process in the sense that it selects the most infor-\nmative samples for labelling. The goal is to achieve a\nhigh performance (e.g., classiﬁcation accuracy) with\nas few samples as possible. Application areas where\nAL has been successfully applied include: drug de-\nsign (Kangas et al., 2014), text classiﬁcation (Chu\net al., 2011), or malicious software detection (Nissim\net al., 2014). The term AL has initially been deﬁned\nby Settles as follows: ”“Active learning systems at-\ntempt to overcome the labelling bottleneck by asking\nqueries in the form of unlabelled instances to be la-\nbelled by an oracle. In this way, the active learner\naims to achieve high accuracy using as few labelled\ninstances as possible, thereby minimising the cost of\nobtaining labelled data.” (Settles, 2009).\nIn general, we distinguish between three ma-\njor AL paradigms:\nstream-based active learning\n(SAL) (Atlas et al., 1990), membership query learn-\ning (MQL) (Angluin, 1988), and pool-based active\nlearning (PAL) (Lewis and Gale, 1994). The SAL\nparadigm assumes a stream of data, i.e., the data\npoints show up one after the other and the learner has\nto decide either to ’buy’ (i.e., query an oracle) a label\nor not. In particular, this means if the learner decides\nnot to buy a label for a speciﬁc data point, he will not\nbe able to access that data point again.\nIn the MQL paradigm, the learner works indepen-\ndently from a stream of incoming samples and can\npossibly request labels for any unlabelled instance in\nthe input space. In particular, this means to allow for\nqueries that the learner generates de novo.\nIn contrast, the standard PAL process starts with\na large pool of unlabelled samples and a small set of\nlabelled samples. Figure 5 illustrates such a pool with\nalready labelled samples (blue and orange) and un-\nlabelled samples (grey). The training process is or-\nganised in cycles and consists of the following steps:\n(1) A model such as a classiﬁer is trained using the\nalready labelled samples, (2) The selection strategy\nof the active learner identiﬁes a query set (the next\nsamples to be labelled) from the pool of unlabelled\nsamples, (3) The selected samples are presented to\nthe oracle (e.g., a human), which provides label in-\nformation, and (4) The knowledge model is updated.\nFinally, (5) the process terminates if a stopping con-\ndition is met, otherwise the next learning cycle is\nstarted.\nPAL heavily relies on the particular implementa-\ntion of the selection strategy. Hence, several alterna-\ntives are available in literature—the most prominent\nexamples include the following:\n• Random selection, also called passive sampling:\nSelects instances randomly for labelling, which\nmeans that it is free of heuristics and parameters.\n• Uncertainty sampling:\nSelect those instances\nwhere the learner are least certain about the label,\nwith different measures being used for quantiﬁca-\ntion of this uncertainty (e.g., posterior, margin, or\nentropy) (Atlas et al., 1990).\n• Ensemble-based strategy: The basic idea here is\nto train different base classiﬁers by using differ-\nent subsets of the training data and choose the next\nsample to be queried by identifying the strongest\ndisagreement between these base classiﬁers (Se-\nung et al., 1992).\n• Expected Error Reduction: Estimates the general-\nisation error of a classiﬁcation model given a vali-\ndation set, which means that no labels are required\n(Roy and McCallum, 2001).\n• Density weighted Uncertainty Sampling (DWUS)\nAn uncertainty score is weighted with a candi-\ndate’s density. Thereby, outlier are unlikely to be\nconsidered for labelling (Donmez et al., 2007).\nIn general, all these strategies aim at assessing the\ncurrent knowledge of the classiﬁer and, based on this\nassessment, determine the most informative samples.\nIn the following section, we describe how this basic\nidea in combination with the different approaches of\nthese basic selection strategies can be used to turn\npurely reactive RL systems into actively learning RL\nsystems.\n3\nACTIVE REINFORCEMENT\nLEARNING\nWe use the term ’active reinforcement learning’\n(ARL) for RL systems that are self-aware of their\nFeature A\nFeature B\nUnclassified\nClass A\nClass B\nLegend\nFigure 5: Example of labelled and unlabelled samples in a pool-based AL setting.\nknowledge and adapt their own learning behaviour ac-\ncordingly. This is in contrast to the previously seen\npurely reactive and passive strategies typically fol-\nlowed by RL systems (see Section 2.2. Figure 6 il-\nlustrates an abstract view of an XCS presented in Fig-\nure 3 and adds an adaptation mechanism on-top of it\nthat follows the design concept of Organic Comput-\ning (Tomforde et al., 2011b). In general, active RL\ncomprises two different aspects that need to be con-\nsidered subsequently: i) establishing self-awareness\nof the own knowledge and ii) active control of knowl-\nedge discovery parts of the RL system. This corre-\nsponds to the observer and controller parts of the de-\nsign concept. We summarise the corresponding tasks\nin the following paragraphs.\n+\nObserver\n•\nKnowledge assessment\n•\nUncertainty modelling\n•\nConcept drift detection\nController\n•\nModel-based rule generation\n•\nAdaptive learning rate\n•\nAdaptive exploration\nFigure 6: Self-adaptive XCS with observer/controller tasks.\n3.1\nAspects of ARL\nObservation tasks: As basis for active management\nof knowledge and guided reinforcement learning be-\nhaviour, the system needs to become aware of its\nown, currently existing knowledge. This knowledge\nis stored in the population (i.e., the available classi-\nﬁers as illustrated by Figure 4): in the entirety of the\nsearch space covered by condition parts, in the diver-\nsity of actions, and in the different evaluation criteria\n(i.e., prediction, error, ﬁtness—but also experience,\nnumerosity, etc.). In order to allow for a kind of self-\nawareness of the existing knowledge, an active rein-\nforcement learning systems needs the following capa-\nbilities:\n• Assessing the quality (i.e., the accuracy of pre-\ndictions of rewards/feedback in combination with\nthe strength of the expected reward) of exist-\ning knowledge as distribution model for the en-\ntire search space:\nThis allows for identifying\nregions that have insufﬁcient knowledge (which\nthen needs to be addressed actively by the con-\ntroller part).\n• Modelling the uncertainty of the existing knowl-\nedge: Assessing the certainty assigned to the dif-\nferent regions of the search space can be done\nusing the aspect ’experience’ per classiﬁer ag-\ngregated over the different classiﬁers covering\na niche.\nThis allows for determining a score\nfor each possible niche that—in relation to other\nniches— provides the basis for deciding if this\nniche needs to be explored in more detail.\n• Assessing the ’appropriateness’ of the knowledge\nfor the next steps: An RL system typically tra-\nverses the search space in terms of a trajectory.\nThis means that there is a local dependency be-\ntween the current situation and the next situation,\nsince this is the result of the chosen action and\nenvironmental inﬂuences. In order to act actively,\nthe RL system needs to make use of these trajecto-\nries and predict the set of next situations (probably\nprobabilistically to quantify the uncertainty as-\nsigned to these predictions). Based on this, it can\nassess in advance if appropriate classiﬁers will be\navailable for the corresponding match and action\nsets. Otherwise, alternatives need to be generated.\n• Detecting meta-features of the learning problem:\nTypically, RL assumes that the underlying learn-\ning problem is static, i.e., challenges such as con-\ncept drift and concept shift do not occur.\nAl-\nternatively, it is assumed that concept drift is so\nslow that the evolution of the population automati-\ncally addresses this problem. However, intelligent\nsystems have to operate under real-world condi-\ntions and based on sensor/actuator constellations,\nwhich may result in challenges such as i) integra-\ntion of novel components, ii) hardware character-\nistics such as wear, or iii) any kind of novel envi-\nronmental processes. Consequently, the systems\nneeds possibilities to detect such changes in the\nunderlying problem domain, possibly resulting in\nre-consideration of already learned knowledge.\nControl tasks: Taking the different mechanisms\nof the observer part as input, the RL system has to\nadapt its own learning behaviour accordingly. In gen-\neral, this means to manage parameters such as learn-\ning speed but also to generate promising classiﬁers for\ninappropriately covered niches of the search space. To\nallow for a kind of self-management of the learning\nbehaviour, an ARL systems needs the following ca-\npabilities:\n• Model-based classiﬁer generation: Instead of ran-\ndomly generating new classiﬁers or applying a ge-\nnetic algorithm to the action or match set, the RL\nsystem needs more sophisticated techniques for\nclassiﬁer generation that make use of the available\nknowledge.\n• Adaptive exploration rate: Currently, XCS de-\ncides about the next action based on the infor-\nmation contained in the prediction array by ap-\nplying a roulette-wheel approach. This gives a\nhigher probability to better performing actions\n(i.e., exploitation) and lower probability to worse\nperforming actions (i.e., exploration). However,\nsuch a static assignment of probabilities based on\nthe niche is a good idea at startup, it result in\nundesired exploration when the population is al-\nready converged. Consequently, there are differ-\nent strategies available in literature that aim at\ncontrolling the learning rate.\nAn ideal strategy\ncomes with a high learning rate when the popula-\ntion is not appropriate (e.g., startup or after a con-\ncept drift/shift) and with no exploration if the pop-\nulation converged to the optimum. Consequently,\nthe RL system needs a mechanism to assess the\ncurrent exploration needs and to control the ex-\nploration probability accordingly.\n• Adaptive learning rate: The selection probability\nis just one means to parameterise the learning be-\nhaviour in XCS. A second aspect is given by the\nlearning rate that is responsible for controlling to\nwhich degree a classiﬁer’s evaluation criteria are\nshifted into the direction of the actually received\nreward/feedback signal. Here, the same argumen-\ntation holds as above, resulting in the need to con-\ntrol the learning rate dynamically at runtime.\n• Population management: The size of the popu-\nlation is limited in XCS to allow for a ’selec-\ntion pressure’, i.e., less performing classiﬁers are\ncontinuously replaced by novel candidates. The\nresult of this process is that, after convergence,\nthe population is (theoretically) perfectly opti-\nmised to the underlying problem and only stores\nthe most promising responses for occurring situ-\nations. However, in the presence of permanent\nchange, deleting information about sub-optimal or\neven bad behaviour (and consequently the encod-\ning classiﬁers) is highly inefﬁcient. Reasons in-\nclude: i) A concept drift may render deleted clas-\nsiﬁers necessary again, ii) generating new clas-\nsiﬁers efﬁciently means to avoid bad behaviour,\netc. As a counter measure, the RL system needs\na mechanism for active management of the cur-\nrent population which makes, e.g., use of a backup\nmemory that allows for ’constructive forgetting’\nand ’remembering’.\nThere are further highly important aspects when\nturning XCS into an ARL system, which need to be\ntaken into consideration:\n• XCS is just one RL paradigm, although probably\nthe most prominent in operating intelligent sys-\ntems. However, the insights gained in developing\nan active XCS need to be transferred to other RL\nparadigms for generalisation purposes.\n• An intelligent system seldom operates alone. In\nturn, use cases often contain several similar sys-\ntems operating in a shared environment.\nCon-\nsequently, the mechanisms identiﬁed before can\nmake use of even more existing knowledge (i.e.,\nthe populations of other systems of the same kind)\nvia communication. This, however, may increase\nthe uncertainty related to the knowledge informa-\ntion since the underlying characteristics (such as\nthe sensor equipment) may not be identical.\n3.2\nState of the art in active\nreinforcement learning\nAssessment of the existing knowledge: Stein et al.\ndeﬁned so-called ’knowledge gaps’ as parts of the in-\nput space (niches) that are not sufﬁciently covered by\nclassiﬁers (Stein et al., 2018). This means that the\nniche is either not covered at all or only covered by\nclassiﬁers with low performance values. The authors\npropose to derive an abstract representation of the\nclassiﬁer distribution that is used to identify niches\nwhere knowledge is needed. In (Stein et al., 2017a),\na basic combination with approaches from the ﬁeld\nof AL has been derived that should make use of con-\ncepts such as query synthesis, uncertainty sampling,\nand query by committee—however, this just provides\nthe idea and can be seen as preliminary work to this\ncontribution.\nConsidering external knowledge in LCS: Espe-\ncially for Michgan-style LCS, Urbanowicz et al. pre-\nsented a concept to integrate the knowledge of exter-\nnal experts to establish a guided discovery process of\nnovel classiﬁers (Urbanowicz et al., 2012; Urbanow-\nicz and Moore, 2015). In general, the approach is to\nmake use of a set of hand-crafted heuristics (i.e., ’ex-\npert knowledge’) to improve the generation of novel\nclassiﬁers in comparison to the random-based strat-\negy typically used. This means that the probabilities\nof actions covered in situations of classiﬁers under\nconstruction are pre-deﬁned (i.e., derived in an ofﬂine\npre-processing step) and not adaptive to the underly-\ning problem.\nAs an alternative, Najar et al. presented a learning\nmodel where a human ’teacher’ guides the RL system\n(a robot) by using teaching signals (i.e., showing what\nto do) (Najar et al., 2015a; Najar et al., 2015b). The\nidea is to allow the learner to imitate the behaviour\nof the teacher and therefore improve its learning ef-\nﬁciency. However, the approach is limited to cases\nwhere learner and teacher can explicitly do the same\ntask, where a (human) teacher is available during the\nentire learning process, and where knowledge is al-\nready available by teachers.\nIn the context of Organic Computing (M¨uller-\nSchloer and Tomforde, 2017), a safety-based genera-\ntion of novel classiﬁers has been presented (Tomforde\net al., 2011a). Here, the LCS-variant XCS is not al-\nlowed to generate new behaviour. In contrast, it oper-\nates on the existing classiﬁers only. The GA responsi-\nble for discovering appropriate actions in speciﬁc sit-\nuations has been removed from the online component\nand coupled with a simulation of the underlying in-\ntelligent system. As a result, only tested behaviour is\nadded to the population. To remain operable (i.e., ﬁnd\nanswers in case of missing knowledge), the covering\ncomponent copies the nearest classiﬁer (i.e., based on\nthe Euclidean distance deﬁned in the search space of\nthe condition parts), widens its condition part to ﬁt the\nnew situation, and uses this as default classiﬁer (Pro-\nthmann et al., 2011). However, the approach requires\nthe existence of a ’digital twin’ during the discovery\nprocess.\nAlso in the context of OC, Stein et al. proposed\nto make use of interpolation techniques to generate\nnew classiﬁers (Stein et al., 2016). The basic idea\nis that the best action is probably a compromise of\nthe actions proposed by the best surrounding classi-\nﬁers in the niche. Consequently, novel classiﬁers are\ngenerated not by using a simulation-coupled GA as in\n(Tomforde et al., 2011a; Prothmann et al., 2011) but\nby interpolation of the neighbours—which is much\nfaster. The approach has been further extended to al-\nternative usage patterns of interpolation (Stein et al.,\n2017b) and a concept for proactive knowledge con-\nstruction that, however, has not been realised yet\n(Stein et al., 2017a).\nStill, this assumes a gradual\nchange in the action rather than allowing for shifts in\nthe action distributions.\nFollowing a more generic approach, Nakata et al.\ndeveloped an example of a weighted complete action\nmap, which is intended to evolve a population that is\ncomplete in terms of learning the entire situation-to-\naction-mapping. In particular, this means to assign\nmore classiﬁers to the highest-return actions for each\nstate, for instance (Nakata et al., 2015). This contin-\nues work by Kovac (see e.g. (Kovacs, 1999), where\nhe investigates the trade-off between strength and ac-\ncuracy and consequently emphasises issues related to\nthe change in ﬁtness.\nManagement of the population: In (Butz and\nSigaud, 2011), Butz and Sigaud proposed a differ-\nent approach to classiﬁer deletion. They handled the\nselection problem (i.e., identiﬁcation of classiﬁers to\nbe deleted to keep the population size below a given\nthreshold) locally rather than globally as before. This\nis intended to avoid deleting classiﬁers in niches that\ncan be considered as knowledge gaps using the word-\ning of Stein et al. from (Stein et al., 2018). However,\nthis does not tackle the problem of actively managing\nthe population.\nIn XCS, a subsumption mechanism combines sim-\nilar rules, and a randomised deletion mechanism re-\nmoves classiﬁers of a low ﬁtness from the population.\nIn (Fredivianus et al., 2010), the discovery compo-\nnent is altered by introducing a modiﬁed rule com-\nbining technique. The goal is to create maximally\ngeneral classiﬁers that match as many inputs as pos-\nsible while still being exact in their predictions. The\napproach considers previously learnt knowledge and\ninfers generalised classiﬁers from the existing popula-\ntion. It has been extended to real-valued instances of\nXCS in (Fredivianus et al., 2012). Despite providing a\nheuristic for rule combining, the approach covers just\none aspect of the management problem.\nImbalanced data: RL systems in intelligent sys-\ntems learn in a reactive manner by considering the\ncurrent environmental conditions as input. In other\nwords, the different input situations the RL system\nhas to react on are highly imbalanced, with some\nregions of the search space being never or only ex-\ntremely seldom covered.\nThe learning mechanism\nof XCS under imbalanced data has been investigated\nand theoretically modelled by Orriols-Puig et al. in\n(Orriols-Puig and Bernad´o-Mansilla, 2009). The au-\nthors focus on rare cases and react to this challenge\nby adapting XCS’ parameters (i.e., the learning rate\nand the threshold for GA activation). However, this\ndoes not explicitly address the problem of identifying\nlimited knowledge or steering the exploration—but it\nprovides a basis for techniques to modify XCS param-\neters at runtime.\nNovelty search: LCS are part of the overall ﬁeld\nof evolutionary algorithms (EAs). Here, approaches\nsuch as the ’Novelty Search Algorithm’ (Lehman and\nStanley, 2008) by Lehman and Stanley have been pre-\nsented that exchange the traditional evolution process\nbased on a ﬁtness function by one that puts an em-\nphasis on searching for novel behaviours. This may\nbe beneﬁcial if the search space of possible actions\nis unknown and consequently can provide an element\nfor action discovery in active RL systems.\nExploration strategies:\nInitially, RL systems\nused static exploration strategies with the ε-greedy\nstrategy as probably most popular one (Sutton and\nBarto, 1998).\nHere, a certain fraction of attempts\n(ε) are dedicated to exploration, while the remain-\ning 1 −ε fraction is used for exploiting the currently\nbest known action.\nVariants of static exploration\nschemes include a random strategy (also called ran-\ndom walk, i.e., selecting randomly which action to\nuse), or a softmax strategy (i.e., an ε-greedy approach\nwith modiﬁed selection probabilities to better reﬂect\nthe currently learned reinforcement values) (Sutton\nand Barto, 1998).\nIn contrast, the idea of the ’decreasing ε-strategy’\n(Vermorel and Mohri, 2005) is to have a higher prob-\nability for an explorative action selection at the begin-\nning and to decrease this towards the end of the learn-\ning process. In this way, an attempt is made to ensure\nappropriate exploration of the search space before the\nagent ﬁnally makes primarily exploitative action se-\nlections. The limitations of this approach include the\nstatic decreasing behaviour (i.e., no real adaptiveness)\nand the characteristics of real-world problems, since\nthere are typically no ﬁnal states. A similar alternative\nis the ’ε ﬁrst’ strategy (Vermorel and Mohri, 2005)–\nhere, only two different values for ε are considered: a\nhigh one (e.g., 1) at the beginning and a low one (e.g.,\n0 or close to 0) after sufﬁcient exploration. The limi-\ntations are similar as before but further emphasise the\nneed to deﬁne ’sufﬁcient’ exploration.\nA third category of exploration strategies is de-\nﬁned by the ’meta-softmax-strategy’ (Schweighofer\nand Doya, 2003). The idea is to adapt the learning\nrate, the discount factor and the exploration probabil-\nity dynamically in response to the difference of two\nvalues calculated from the feedback signal: a ’mid-\nterm reward’ (i.e., considering a few rewards) and a\n’long-term reward’ (i.e., considering the rewards of a\nlonger window). For both, the averaged rewards are\ndetermined for the considered window and if the dif-\nference is above a pre-deﬁned threshold, the explo-\nration criteria (i.e., learning rate, discount factor, and\nexploration probability) are increased. This is already\nadaptive in the sense of our notion of ARL, but it\nstill follows a reactive mechanism rather than a well-\ndeﬁned active decision. Closely related to the ’meta\nsoftmax’ strategy is the class of ’value-difference-\nbased-exploration’ strategies (Tokic, 2010). Here, the\nexploration parameters of the learning strategy are de-\ntermined based on the difference of the ’values’ of the\nindividual actions. In particular, this value difference\ndenotes the product of the learning rate and the tem-\nporal difference error. It can not be mapped directly to\nthe evaluation criteria of XCS, since there is no indi-\nvidual ’value’ of a classiﬁer. However, the prediction\nerror in XCS can be understood as a corresponding\nindicator. We have to consider differences between\nexpected reward/feedback and observation as one as-\npect of the strategy to decide about adaptations of the\nexplorative parameters.\n4\nRESEARCH ROADMAP\nTo realise ARL based on XCS, different challenges\nneed to be addressed. In this section, we outline a\ncorresponding research roadmap. Therefore, we de-\nﬁne the most urgent challenges that contribute to the\naspects of ARL as introduced previously. We organ-\nise these challenges in terms of observer, controller, or\noverall system tasks. For each challenge, we initially\ndiscuss the goal and subsequently propose a concept\nhow this challenge can be addressed using. Thereby,\nwe either make use of concepts from the state of the\nart or we put an emphasis on possible strategies to to\nclose the gap in research.\n4.1\nObserver-related challenges\nThe goal of the observer part is to establish a self-\nassessment of the existing knowledge and a subse-\nquent prediction of possible next states. This mainly\ncomprises the challenges assessment of the popula-\ntion, prediction of the next states, and modelling the\nentire learning problem.\nChallenge 1 – Assessment of the existing popu-\nlation: The goal of the ﬁrst challenge is to establish a\ncontinuous self-assessment of the existing population.\nThis mainly refers to the task of identifying less cov-\nered niches or niches with inappropriate knowledge.\nThe different AL strategies as outlined in Sec-\ntion 2.3 are already fulﬁlling the task of assessing the\ninput space. However, the major difference is that the\npopulation is a set of hyper-rectangles covering the\nsearch space (see Figure 4) rather than the individual\npoints in the AL approach (i.e., the samples in Fig-\nure 5). This means that we have to turn the sample-\ndistribution into a continuous distribution, e.g. by us-\ning kernel density estimation techniques. However,\nthe hyper-rectangles deﬁning the condition parts of\nthe classiﬁers are combined with a kind of label in-\nformation (i.e., actions), but they also consider uncer-\ntainty values (i.e., the evaluation criteria such as accu-\nracy, strength, experience or numerosity). The assess-\nment must be able to derive values for these aspects\nas well. Further, an ordering of knowledge gaps (i.e.\na ranking) will most certainly be based on several cri-\nteria aggregated to an individual score. Here, aspects\nsuch as described for the 4DS strategy in AL can play\na major role, see (Reitmaier and Sick, 2013).\nChallenge 2 – Prediction of the next match sets:\nThe goal of the second challenge is to predict the next\nobservations to check whether the match set will be\nsuitable or not. This can then serve as basis to proac-\ntively generate new classiﬁers.\nA possible approach is to model the sequence of\nsituations as a sliding window approach. Within this\nwindow, a trajectory can be determined — which is\nthen used to predict possible next situations. To fur-\nther improve the predictions, this has to take aspects\nsuch as velocity and probability of occurrence into ac-\ncount. In addition, the prediction may become subject\nto runtime learning as well by using state-of-the-art\ntechniques. Based on this, the match set can be gen-\nerated and an analysis of the contained classiﬁers can\nbe performed: Is there enough diversity? Is the over-\nall ﬁtness-weighted prediction above a certain thresh-\nold? Is the niche sufﬁciently covered?\nChallenge 3 – Learning problem modelling:\nThe goal of the third challenge is to generalise the ﬁrst\nchallenge by explicitly modelling the problem space\nnext to the knowledge stored in the population.\nA possible approach lies in the integration of an\nadditional, continuously-deﬁned representation of the\nlearning problem that is update based on any received\nfeedback. This can be done, for instance, by train-\ning a neural network taking the situation as input and\nproviding the most appropriate action as output (or es-\ntimating the payoff for a situation-action pair). Such\na solution – and therefore the reason why we favour\nthe XCS-based approach – suffers from the represen-\ntation being not interpretable by humans and not ex-\nplaining the behaviour. As an additional knowledge\nbase, this may serve as input for more efﬁcient and\nreliable classiﬁer generation and management.\n4.2\nController-related challenges\nThe goal of the controller part is to increase the de-\ngree of autonomy of (reinforcement-based) learning\nin intelligent technical systems. This actually turns\nthe RL system into an ARL system by making use of\nthe information provided by the observer part—and it\nmainly comprises the challenges population manage-\nment, controlled classiﬁer generation, adaptive explo-\nration probability, adaptive learning rate, and control\nof the adaptation speed.\nChallenge 4 – Population management: The\ngoal of population management is to turn the reac-\ntive replacement approach with a deﬁned maximum\nnumber of classiﬁers in the population into a proac-\ntive management.\nA possible approach is to introduce external of-\nﬂine memories that serve as reservoir for removed\nclassiﬁers. The knowledge encoded in these classi-\nﬁers can also be used as reference when generating\nnew ones. Especially if a change in the structure of\nthe learning problem is noticed, the system can check\nif it could switch back to previously removed clas-\nsiﬁers, which would perfectly cover oscillating be-\nhaviour, for instance. Such a mechanism needs to be\naugmented with a suitable selection scheme as well as\na mechanism that ideally abstracts from the individ-\nual classiﬁers. Thereby, not only the memorisation\nof older classiﬁers plays an important role, but also\ntechniques to realise ’constructive forgetting’, i.e. ex-\nplicitly select knowledge to be removed to allow for\nnovel behaviour (which not necessarily refers to ’bad’\nclassiﬁers as currently).\nChallenge 5 – Controlled generation of new\nrules: The goal of this aspect is to proactively gen-\nerate new classiﬁers.\nThere are already a few attempts in literature:\n(Tomforde et al., 2011a) uses a simulator to gener-\nate new classiﬁers and (Stein et al., 2017a) describes\ninterpolation-based ideas.\nFollowing these ideas,\nnew classiﬁers can be generated either using side-\nknowledge (i.e., the external neural network men-\ntioned above), simulation, external memories of re-\nplaced classiﬁers or the neighboured classiﬁers. The\nscope can be extended towards incorporation of other\nopportunistically available knowledge sources as ex-\nplained in (Calma et al., 2017). This is closely related\nto the concept of curiosity, which is discussed in detail\nin (Wu and Miao, 2013).\nChallenge 6 – Active conﬁguration of explo-\nration probability: The goal of this challenge is to\nreplace the roulette-wheel exploration approach by an\nadaptive mechanism.\nAs outlined in Section 3.2, there are several adap-\ntive exploration schemes available in literature. They\ncan serve as starting point for altering the decision\nlogic of XCS as well. However, this needs to be com-\nbined with change detection techniques or novelty de-\ntection techniques (such as (Gruhl et al., 2021) tak-\ning, e.g., the trajetories through the search space as\ninput) to identify conditions where the reinforcement\nbehaviour is differing from previous and expected be-\nhaviour.\nChallenge 7 – Adaptive control of the learning\nrate: The goal of this aspect is to replace the static\ndeﬁnition of the learning rate by adaptive techniques.\nSimilar to the challenge before, cases where the\nunderlying learning problem is changing require a\nfaster adaptation of the existing knowledge. In turn,\nalready settled knowledge bases require smaller or no\nupdate at all to avoid oscillations due to noise feed-\nback. A solution can adapt the reinforcement rate in\na way that α is increased in case of changing condi-\ntions (e.g. detected drift). Alternatively, the value of\nα depends on the niche of the search space (number\nof classiﬁers, ﬁtness, numerosity, etc) and can be de-\nﬁned per-niche rather then globally.\nChallenge 8 – Adaptation speed: The goal of\nthis aspect is to replace the static cycle-based activa-\ntion scheme of the learning technique with an adap-\ntive variant.\nCurrently, the RL loop is performed in given cy-\ncles of ﬁxed duration.\nHowever, the frequency of\nadaptation performed by the intelligent system is not\nnecessarily desirable to be constant. For instance, in\ntrafﬁc control the conditions are almost constant dur-\ning the night, resulting in seldom adaptation needs.\nOn the other hand, rush hour handling would bene-\nﬁt from even shorter adaptation cycles. This maps\nalso to the idea that the learner is able to dynamically\nchange the number of observations until it decides to\nadapt. Consequently, an approach could rely on as-\nsessing the stability of the observed condition and –\nbased on such a score – adapt the frequency of adap-\ntation. However, this has to avoid self-lock-in prob-\nlems, for instance.\n4.3\nSystem-related challenges\nBesides the observer and controller tasks, there are\nsystem-wide challenges for ARL. This mainly com-\nprises the challenges multi-dimensional feedback sig-\nnals, collaborative awareness, and indirect feedback.\nChallenge 9 – Active reward requests: The goal\nof this aspect is to integrate human users as additional\nknowledge source.\nIn accordance with the basic idea of AL, we in-\ntroduce the user or administrator of the system as ad-\nditional knowledge source. For unknown conditions\nor alternative classiﬁers, the system may query the\nuser for a feedback signal that is not stemming from\nthe observed environment. This means to fully in-\ntegrate the self-assessment of the knowledge distri-\nbution combined with the uncertainty estimation as\nknown in AL and incorporate the corresponding se-\nlection strategies, possibly with a given budget and\nonly if a user is available. This entails the need for\na availability models for the user, maybe even aug-\nmented with (learned) models of their expertise.\nChallenge 10 – Multi-dimensional feedback\nsignals: The goal of this aspect is to replace the\nreward signal by a vector comprising several utility\nfunctions at the same time.\nA typical intelligent system has to tackle more\nthan one goal at the same time. Integrating the differ-\nent utility values into one score is always a trade-off\nand looses information. Consequently, the algorith-\nmic logic needs to be adapted in a way that the single\nvalue is replaced by a vector representation. This has\nimpact on all stages of the learning process. However,\nthere is a ﬁrst solution available (Becker et al., 2012)\nthat serves as a basis for tackling the problem.\nChallenge 11 – Indirect feedback signals and\nfeedback with uncertainty: The goal of the last as-\npect is to combine the explicit with possible further\nimplicit feedback signals.\nA possible approach establishes a mechanism that\ntries to determine additional feedback signals for con-\nsideration.\nAn intuitive approach relies on using\nneighbouring systems of the same kind that act in a\nshared environment: Thy can provide (negative) feed-\nback about adaptation decisions that have a negative\nimpact on the utility of the neighbours. This would\nturn the methodology as deﬁned in (Rudolph et al.,\n2019) into a direct representation. However, this also\nimplies that feedback signals have to be considered in\ndifferent ways, e.g. based on an uncertainty value or\nthe type of feedback (indirect vs. direct).\n5\nCONCLUSION\nThis paper discussed the limitations of current tech-\nniques applied to the self-adaptation task in intelli-\ngent systems. The major observation is that currently\nthe speciﬁc learning parts are processed in an isolated\nmanner. In particular, the main learning technique\nis typically a reinforcement learner that learns in a\npurely reactive manner. On the other hand, machine\nlearning paradigms such as anomaly/novelty detec-\ntion or active learning are able to provide a better self-\nawareness of the underlying observed behaviour and\nprocesses.\nBased on this observation, we proposed a concept\nto integrate current sophisticated reinforcement learn-\ning techniques — in particular, the class of Learning\nClassiﬁer Systems — with concepts from the other\ndomain. The goal is to establish an integrated ap-\nproach, which we called ’active reinforcement learn-\ning’. The major advantage of such a technique lies\nin the ’proactiveness’, i.e. the possibility to act self-\ndetermined (optimised, planned) rather than purely\nreactive.\nWe used the basic design pattern from the domain\nof Organic Computing, i.e. the Observer/Controller\npattern, as a reference model for intelligent systems.\nBased on the constituent parts, we derived a research\nroadmap towards closing the gap to an active rein-\nforcement learning system. This resulted in the deﬁ-\nnition of nine challenges. However, this list does not\nclaim to be exhaustive, but reﬂects the most urgent\nsteps towards an ARL approach. In our current and\nfuture work, we focus on these challenges.\nREFERENCES\nAngluin, D. (1988). Queries and concept learning. Machine\nlearning, 2(4):319–342.\nAtlas, L. E., Cohn, D. A., and Ladner, R. E. (1990). Train-\ning connectionist networks with queries and selective\nsampling. In Advances in neural information process-\ning systems, pages 566–573.\nBecker, C., H¨ahner, J., and Tomforde, S. (2012).\nFlex-\nibility in organic systems - remarks on mechanisms\nfor adapting system goals at runtime. In Proc. of 9th\nInt. Conf. on Inf. in Control, Automation and Robotics,\npages 287–292.\nButz, M. V. and Sigaud, O. (2011). Xcsf with local deletion:\npreventing detrimental forgetting. In Proc. of 13th An.\nConf. Companion on Genetic and Evolutionary Com-\nputation, pages 383–390. ACM.\nCalma, A., Kottke, D., Sick, B., and Tomforde, S. (2017).\nLearning to learn:\nDynamic runtime exploitation\nof various knowledge sources and machine learning\nparadigms.\nIn Proc. 2nd IEEE Int. Workshops on\nFoundations and Applications of Self* Systems, pages\n109–116.\nChu, W., Zinkevich, M., Li, L., Thomas, A., and Tseng,\nB. (2011).\nUnbiased online active learning in data\nstreams. In Proc. of 17th ACM SIGKDD Int. Conf.\non Knowledge discovery and data mining, pages 195–\n203. ACM.\nD’Angelo, M., Gerasimou, S., Ghahremani, S., Grohmann,\nJ., Nunes, I., Pournaras, E., and Tomforde, S. (2019).\nOn learning in collective self-adaptive systems: state\nof practice and a 3d framework.\nIn Proc of 14th\nSEAMS@ICSE, pages 13–24.\nD’Angelo, M., Ghahremani, S., Gerasimou, S., Grohmann,\nJ., Nunes, I., Tomforde, S., and Pournaras, E. (2020).\nLearning to learn in collective adaptive systems: Min-\ning design patterns for data-driven reasoning. In 2020\nIEEE Int. Conf. on Autonomic Computing and Self-\nOrganizing Systems, Companion, pages 121–126.\nDonmez, P., Carbonell, J. G., and Bennett, P. N. (2007).\nDual strategy active learning. In European Conference\non Machine Learning, pages 116–127. Springer.\nFredivianus, N., Kara, K., and Schmeck, H. (2012). Stay\nreal!: XCS with rule combining for real values. In\nGenetic and Evolutionary Computation Conference,\npages 1493–1494.\nFredivianus, N., Prothmann, H., and Schmeck, H. (2010).\nXCS revisited: A novel discovery component for the\nextended classiﬁer system.\nIn Simulated Evolution\nand Learning - 8th Int. Conf., pages 289–298.\nGlass, R. L. (2002). Facts and Fallacies of Software Engi-\nneering. Agile Software Development. Addison Wes-\nley, Boston, US.\nGruhl, C., Sick, B., and Tomforde, S. (2021). Novelty de-\ntection in continuously changing environments. Fu-\nture Gener. Comput. Syst., 114:138–154.\nKangas, J. D., Naik, A. W., and Murphy, R. F. (2014). Efﬁ-\ncient discovery of responses of proteins to compounds\nusing active learning. BMC bioinformatics, 15(1):143.\nKephart, J. and Chess, D. (2003). The Vision of Autonomic\nComputing. IEEE Computer, 36(1):41–50.\nKounev, S., Lewis, P., Bellman, K. L., Bencomo, N., Ca-\nmara, J., Diaconescu, A., Esterle, L., Geihs, K., Giese,\nH., G¨otz, S., Inverardi, P., Kephart, J. O., and Zisman,\nA. (2017). The Notion of Self-aware Computing. In\nSelf-Aware Computing Systems, pages 3–16. Springer.\nKovacs, T. (1999). Strength or accuracy? ﬁtness calcula-\ntion in learning classiﬁer systems. In Int. Worksh. on\nLearning Classiﬁer Sys., pages 143–160. Springer.\nLehman, J. and Stanley, K. O. (2008). Exploiting open-\nendedness to solve problems through the search for\nnovelty. In ALIFE, pages 329–336.\nLewis, D. D. and Gale, W. A. (1994). A sequential algo-\nrithm for training text classiﬁers. In SIGIR’94, pages\n3–12. Springer.\nMoore, G. E. (1965). Cramming more components onto\nintegrated circuits. Electronics Mag., 38(8):114 – 117.\nM¨uller-Schloer, C. and Tomforde, S. (2017). Organic Com-\nputing – Techncial Systems for Survival in the Real\nWorld. Autonomic Systems. Birkh¨auser Verlag.\nNajar, A., Sigaud, O., and Chetouani, M. (2015a). Social-\ntask learning for hri. In Int. Conf. on Social Robotics,\npages 472–481. Springer.\nNajar, A., Sigaud, O., and Chetouani, M. (2015b). Socially\nguided xcs: using teaching signals to boost learning.\nIn Proc. of Companion to 2015 An. Conf. on Genetic\nand Evolutionary Comp., pages 1021–1028. ACM.\nNakata, M., Lanzi, P. L., Kovacs, T., Browne, W. N., and\nTakadama, K. (2015).\nHow should learning classi-\nﬁer systems cover a state-action space?\nIn Proc. of\nCEC15, pages 3012–3019. IEEE.\nNissim, N., Moskovitch, R., Rokach, L., and Elovici, Y.\n(2014). Novel active learning methods for enhanced\npc malware detection in windows os. Expert Systems\nwith Applications, 41(13):5843–5857.\nOrriols-Puig, A. and Bernad´o-Mansilla, E. (2009). Evolu-\ntionary rule-based systems for imbalanced data sets.\nSoft Computing, 13(3):213.\nProthmann, H., Tomforde, S., Branke, J., H¨ahner, J.,\nM¨uller-Schloer, C., and Schmeck, H. (2011).\nOr-\nganic Trafﬁc Control.\nIn Organic Computing – A\nParadigm Shift for Complex Systems, pages 431 – 446.\nBirkh¨auser, Basel.\nReitmaier, T. and Sick, B. (2013). Let us know your deci-\nsion: Pool-based active training of a generative clas-\nsiﬁer with the selection strategy 4ds. Information Sci-\nences, 230:106–131.\nRoy, N. and McCallum, A. (2001). Toward optimal active\nlearning through sampling estimation of error reduc-\ntion. In Proc. of 18th Int. Conf. on Machine Learning,\npages 441–448. Morgan Kaufman.\nRudolph, S., Tomforde, S., and H¨ahner, J. (2019). Mutual\ninﬂuence-aware runtime learning of self-adaptation\nbehavior. ACM Trans. Auton. Adapt. Syst., 14(1):4:1–\n4:37.\nSchweighofer, N. and Doya, K. (2003). Meta-learning in\nreinforcement learning. Neural Networks, 16(1):5–9.\nSettles, B. (2009). Active learning literature survey. Techni-\ncal report, University of Wisconsin-Madison Depart-\nment of Computer Sciences.\nSettles, B. (2012). Active learning. Synthesis Lect. on Art.\nInt. and Machine Learning, 6(1):1–114.\nSeung, H. S., Opper, M., and Sompolinsky, H. (1992).\nQuery by committee.\nIn Proceedings of the ﬁfth\nannual workshop on Computational learning theory,\npages 287–294. ACM.\nSigaud, O. and Wilson, S. (2007). Learning classiﬁer sys-\ntems: a survey. Soft Comp., 11(11):1065–1078.\nStein, A., Maier, R., and H¨ahner, J. (2017a). Toward curious\nlearning classiﬁer systems: Combining xcs with active\nlearning concepts. In GECCO17 Companion, pages\n1349–1356. ACM.\nStein, A., Rauh, D., Tomforde, S., and H¨ahner, J. (2016).\nAugmenting the algorithmic structure of XCS by\nmeans of interpolation. In Architecture of Computing\nSystems 2016, pages 348–360.\nStein, A., Rauh, D., Tomforde, S., and H¨ahner, J. (2017b).\nInterpolation in the extended classiﬁer system: An ar-\nchitectural perspective. Journal of Systems Architec-\nture, 75:79–94.\nStein, A., Tomforde, S., Diaconescu, A., H¨ahner, J., and\nM¨uller-Schloer, C. (2018). A concept for proactive\nknowledge construction in self-learning autonomous\nsystems. In Proc. of 3rd Int. Worksh. on Foundations\nand Applications of Self* Sys., pages 204–213.\nSutton, R. S. and Barto, A. G. (1998). Introduction to Rein-\nforcement Learning. MIT Press, 1st edition.\nTokic, M. (2010). Adaptive ε-greedy exploration in rein-\nforcement learning based on value differences. In An.\nConf. on Art. Int., pages 203–210. Springer.\nTomforde, S., Brameshuber, A., H¨ahner, J., and M¨uller-\nSchloer, C. (2011a). Restricted On-line Learning in\nReal-world Systems. In Proc. of CEC11, pages 1628\n– 1635. IEEE.\nTomforde, S., H¨ahner, J., and Sick, B. (2014). Interwoven\nSystems. Informatik-Spektrum, 37(5):483–487. Ak-\ntuelles Schlagwort.\nTomforde, S., Hurling, B., and H¨ahner, J. (2010). Dynamic\ncontrol of mobile ad-hoc networks - Network protocol\nparameter adaptation using Organic Network Control.\nIn Proc. of 7th Int. Conf. on Inf. in Control, Automa-\ntion, and Robotics, pages 28–35. INSTICC.\nTomforde, S., Prothmann, H., Branke, J., H¨ahner, J., Mnif,\nM., M¨uller-Schloer, C., Richter, U., and Schmeck, H.\n(2011b).\nObservation and Control of Organic Sys-\ntems. In Organic Computing - A Paradigm Shift for\nComplex Systems, pages 325 – 338. Birkh¨auser.\nTomforde, S., Sick, B., and M¨uller-Schloer, C. (2017).\nOrganic\ncomputing\nin\nthe\nspotlight.\nCoRR,\nabs/1701.08125.\nTomforde, S., Steffen, M., H¨ahner, J., and M¨uller-Schloer,\nC. (2009). Towards an Organic Network Control Sys-\ntem. In Proc. of the 6th ATC, pages 2 – 16. Springer.\nUrbanowicz, R. and Moore, J. (2015). Exstracs 2.0: de-\nscription and evaluation of a scalable learning classi-\nﬁer system. Ev. int., 8(2-3):89–116.\nUrbanowicz, R. J., Granizo-Mackenzie, D., and Moore,\nJ. H. (2012). Using expert knowledge to guide cov-\nering and mutation in a michigan style learning clas-\nsiﬁer system to detect epistasis and heterogeneity. In\nInt. Conf. on Parallel Problem Solving from Nature,\npages 266–275. Springer.\nVermorel, J. and Mohri, M. (2005). Multi-armed bandit al-\ngorithms and empirical evaluation. In ECML05, pages\n437–448. Springer.\nWilson, S. W. (1995). Classiﬁer Fitness Based on Accuracy.\nEvolutionary Computation, 3(2):149–175.\nWilson, S. W. (2000). Get real! xcs with continuous-valued\ninputs. In Learning Classiﬁer Systems, pages 209–\n219. Springer.\nWu, Q. and Miao, C. (2013). Curiosity: From psychology\nto computation. ACM Computing Surveys, 46(2):18.\n",
  "categories": [
    "cs.LG",
    "68T05",
    "I.2.4; I.2.6"
  ],
  "published": "2022-01-11",
  "updated": "2022-01-11"
}