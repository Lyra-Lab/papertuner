{
  "id": "http://arxiv.org/abs/2201.05599v1",
  "title": "Smart Magnetic Microrobots Learn to Swim with Deep Reinforcement Learning",
  "authors": [
    "Michael R. Behrens",
    "Warren C. Ruder"
  ],
  "abstract": "Swimming microrobots are increasingly developed with complex materials and\ndynamic shapes and are expected to operate in complex environments in which the\nsystem dynamics are difficult to model and positional control of the microrobot\nis not straightforward to achieve. Deep reinforcement learning is a promising\nmethod of autonomously developing robust controllers for creating smart\nmicrorobots, which can adapt their behavior to operate in uncharacterized\nenvironments without the need to model the system dynamics. Here, we report the\ndevelopment of a smart helical magnetic hydrogel microrobot that used the soft\nactor critic reinforcement learning algorithm to autonomously derive a control\npolicy which allowed the microrobot to swim through an uncharacterized\nbiomimetic fluidic environment under control of a time varying magnetic field\ngenerated from a three-axis array of electromagnets. The reinforcement learning\nagent learned successful control policies with fewer than 100,000 training\nsteps, demonstrating sample efficiency for fast learning. We also demonstrate\nthat we can fine tune the control policies learned by the reinforcement\nlearning agent by fitting mathematical functions to the learned policy's action\ndistribution via regression. Deep reinforcement learning applied to microrobot\ncontrol is likely to significantly expand the capabilities of the next\ngeneration of microrobots.",
  "text": "Behrens, Ruder.     University of Pittsburgh.     Preprint. \n 1 \nSmart Magnetic Microrobots Learn to Swim with Deep \nReinforcement Learning \n \nAuthors  \n \nMichael R. Behrens1 \n \nWarren C. Ruder*1,2 \n \nAffiliations  \n \n1Department of Bioengineering, University of Pittsburgh; Pittsburgh, PA, USA. \n2Department of Mechanical Engineering, Carnegie Mellon University; Pittsburgh, PA, \nUSA. \n*Corresponding author Email: warrenr@pitt.edu \n \nOne-Sentence Summary  \nDeep reinforcement learning was used to autonomously develop a robust controller for helical \nmagnetic microrobots. \n \nAbstract  \nSwimming microrobots are increasingly developed with complex materials and dynamic shapes \nand are expected to operate in complex environments in which the system dynamics are difficult \nto model and positional control of the microrobot is not straightforward to achieve. Deep \nreinforcement learning is a promising method of autonomously developing robust controllers for \ncreating smart microrobots, which can adapt their behavior to operate in uncharacterized \nenvironments without the need to model the system dynamics. Here, we report the development \nof a smart helical magnetic hydrogel microrobot that used the soft actor critic reinforcement \nlearning algorithm to autonomously derive a control policy which allowed the microrobot to swim \nthrough an uncharacterized biomimetic fluidic environment under control of a time varying \nmagnetic field generated from a three-axis array of electromagnets.  The reinforcement learning \nagent learned successful control policies with fewer than 100,000 training steps, demonstrating \nsample efficiency for fast learning. We also demonstrate that we can fine tune the control policies \nlearned by the reinforcement learning agent by fitting mathematical functions to the learned \npolicy’s action distribution via regression. Deep reinforcement learning applied to microrobot \ncontrol is likely to significantly expand the capabilities of the next generation of microrobots.  \n \n \n \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n 2 \nIntroduction \nUntethered swimming microrobotic systems \nhave received significant research attention for \nperforming micromanipulation tasks and particularly for \ntheir potential therapeutic biomedical applications [1, 2].  \nMicrorobots operating remotely inside the human body \nhave potential to enable minimally invasive medical \nprocedures including targeted drug, cell, or other cargo \ndelivery [3-6], tissue biopsy [7], thermotherapy [8], and \nblood clot removal [9]. Controlling these miniature \ndevices within complex and dynamic environments such \nas the human body can present a significant engineering \nchallenge [10].  This challenge is in part because the \ndesign of microrobotic systems is trending towards the \nuse of complex composite materials [6, 11-13], dynamic \nmorphologies \n[14-17], \nand \nintegrated \nbiological \ncomponents [18-22]. These features add layers of \nfunctionality to microrobotic systems, but can create \ndifficulties when constructing accurate dynamic and \nkinematic models of microrobotic behavior, making it \nespecially complex and challenging to use classical \nfeedback control systems to control microrobot behavior \n[5, 14, 23]. Additionally, the environmental dynamics \nencountered by a biomedical microrobot inside the \nhuman body may be variable, complex, and poorly \ncharacterized [16, 24, 25].  \nAs one potential pathway to overcome these \nchallenges, we can observe and adopt the strategies of \nnatural biological agents that have evolved to operate in \ncomplex, unpredictable environments. Many biological \nsystems can adapt to learn new behaviors based on \nexperience, allowing them to thrive in a wide range of \ncomplex and variable environmental conditions by \ntailoring their behavior to suit the environment. Systems \ncapable of learning adaptive behavioral patterns based \non past events are ubiquitous in nature and are found \nacross all levels of biological hierarchy, including in \nbiochemical networks [26], bacteria [26, 27], nematode \nworms [28], insects [29], plants [30], adaptive immune \nsystems [31],  and animal behavior [32]. Inspired by the \nwide-ranging applicability of adaption and learning to \nthe success of living organisms, engineered microrobotic \nsystems that learn new behaviors from past experience \ncould enable new capabilities for complex microrobotic \nsystems [33].  \nReinforcement learning (RL) is a biomimetic \nmachine learning optimization technique inspired by the \nadaptive behavior of real-world organisms [34] that can \nenable learning behaviors in artificial engineered \nsystems [35]. In RL, an agent observes the state of an \nenvironment, and chooses actions to perform in the \nenvironment to achieve a task specified by a reward \nsignal, which is typically predefined. The reward signal \nis used to teach the agent to perform actions to maximize \nthe expected future rewards, which enables the agent to \nlearn to perform the task better based on past experience.  \nRL algorithms have achieved success in a range of \ncomplex robotic control applications [35-40]. For \nexample, RL for robotic control has been demonstrated \nto create robotic control policies that achieve better \nperformance than many humans at complex tasks such \nas grasping and accurate throwing of irregularly shaped \nobjects into bins [39].  RL algorithms have also been \nshown to exceed human level performance in complex \nvirtual tasks with large possible state spaces that cannot \nbe tractably and exhaustively modeled, such as the game \nof Go [41].  Machine learning techniques including RL \nhave already demonstrated promise for developing \npolicies to control microrobot behavior. In simulation, \nRL agents have been trained to control microrobot \nbehavior for solving navigation and swimming \nchallenges in heterogenous fluids [42, 43]. An early RL \nalgorithm, Q-learning, has also been shown to be \neffective for controlling the behavior of laser-driven \nmicroparticles in a discretized grid environment [44]. \nOther similar machine learning techniques such as \nBayesian optimization have been demonstrated to learn \nwalking \ngates \nfor \ndifficult-to-model \nmagneto-\nelastomeric millirobots [45].  However, control of \nswimming microrobots that use deep reinforcement \nlearning to operate in dynamic, biomimetic, and \nmicrofluidic environments with clinically relevant \nmagnetic actuation has yet to be reported. \nIn this work, we demonstrate that deep \nreinforcement learning based on the Soft Actor Critic \nalgorithm [46] can be used to create smart soft helical \nmagnetic \nmicrorobots \nthat \nautonomously \nlearn \noptimized swimming behaviors when actuated with non-\nuniform, nonlinear, and time-varying magnetic fields in \na physical fluid environment. Our RL microrobots \nlearned successful actuation policies without any a priori \nknowledge about the dynamics of the microrobot, the \nelectromagnetic actuator, or the environment (Figure \n1a). These results demonstrate the potential of \nreinforcement learning for developing high performance \nmulti-input, multi-output (MIMO) controllers for \nmicrorobots without the need for explicit system \nmodeling. This capability to autonomously learn model-\nfree microrobot control algorithms could significantly \nreduce the time and resources required to develop high \nperformance microrobotic systems [33].  \n \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n 3 \n \n \nFigure 1. Microrobots with unknown dynamics in uncharacterized environments can be controlled with deep \nreinforcement learning. (a) Microrobotic systems are designed with a great variety of shapes, sizes, materials, and \nactuation methods, and are often operated in challenging environments. Controllers based on artificial deep neural \nnetworks trained with reinforcement learning (RL) can factor in all of these complex dynamic systems and inputs to \ncreate model-free microrobot controllers to create adaptive microrobots. (b) Our microrobotic system consisted of a \nhelical agar magnetic robot (HAMR) in a circular polydimethylsiloxane (PDMS) fluidic track that was given the task \nof moving to a target position along the track under control of a multi-axis electromagnet (Magneturret). Images of \nthe HAMR in the arena were captured with an overhead camera. (c) The smart microrobot control system used a \nneural network trained with the Soft Actor Critic (SAC) reinforcement learning algorithm to generate commands for \nthe Magneturret. In the control loop, the stream of images from the overhead camera was processed to generate state \ninformation that was then fed into the actor neural network, which returned a set of continuous actions that were used \nto control the currents in the Magneturret. State (s), action (a), reward (r), and next state (s’) information was stored \nin a replay buffer which was used update actor and critic neural networks off policy in a learning loop.  \n \n \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n 4 \nResults  \nIn order to create an environment where we \ncould test the hypothesized efficacy of RL control \nsystems for microrobots, we first designed and built a \nphysical, \nbiomimetic, \nfluidic \narena \nwith \nmultidimensional magnetic actuation, and deployed a \nmagnetic microrobot in the arena whose design was \ninspired by the work of Kumar and colleagues [4]. In our \nexperimental setup (Figure 1b), a helical agar magnetic \nrobot (HAMR) was tasked with swimming clockwise \nthrough a fluid filled lumen in a polydimethylsiloxane \n(PDMS) arena under control of a non-uniform rotating \nmagnetic field generated by a three-axis array of \nelectromagnetic coils (Magneturret). An overhead \ncamera was used to track the position of the HAMR in \nthe channel. The camera was used to pass images to a \ncontrol algorithm consisting of an image processing \nmodule and a neural network which generated \ncommands for the Magneturret (Figure 1c). The goal of \nthe control system for our remotely actuated microrobot \nwas to manipulate the shape and magnitude of the \nactuating energy field in order to move the microrobot to \nachieve an intended dynamic behavior. The controller \nneural network was trained via a reinforcement learning \nagent using the soft actor critic algorithm.  \nThe \nfundamental \ncontrol \nproblem \nwas \nencapsulated by this question: how should the currents \nin the electromagnetic coils be modulated in order to \ncreate a magnetic field that places forces and torques on \nthe HAMR sufficient to drive its locomotion toward a \nspecific target? Achieving this task usually requires an \naccurate dynamic model of the complete system, \nincluding the dynamics of the robot, the environment, \nand the actuator [23]. Significant work has been done \ndeveloping dynamic and kinematic models for different \nmicrorobots and actuators [14, 47-49]. These models are \noften developed by making simplifying assumptions \nabout the system such as uniform magnetization [47], \nideal shape [47], and system linearity [50, 51], which \ncould lead to behavioral deviations between the physical \nsystem and the modeled system. The difficulty in \naccurately modeling the dynamics of microrobot \nbehavior increases significantly for microrobots with \ncomplex \nmagnetization \nprofiles, \nsoft \nmaterial \ncomposition, or active shape-changing capabilities [13, \n14, 17, 52, 53].  \nInstead of explicitly modeling the dynamics of \nthe magnetic actuator and the HAMR within the \nenvironment and specifying a controller, we performed \nthe much simpler task of specifying the desired behavior \nof the HAMR in the form of a reward signal. The agent \nobserved the state of the environment along with a \nreward signal containing information about actions that \nlead towards the successful completion of the task. The \nRL agent started without any a priori information about \nthe task and had to learn to perform the task by sampling \nactions from the space of all possible actions and \nlearning which actions resulted in behavior that was \nrewarded. \nThe RL controller required us to develop and \nformulate the task as well as the associated reward \nsignal. At the beginning of each training episode, a target \nposition was defined, 20° clockwise from the starting \nposition of the HAMR in the circular channel. The \nobjective of the RL agent was to develop an action \npolicy, 𝜋𝜋, which maximized the total value of the \nrewards it would receive if it followed that policy in the \nfuture. When the environment was in state, 𝑠𝑠, the agent \nchose an action, 𝑎𝑎, from the policy according to \n𝑎𝑎~ 𝜋𝜋(∙|𝑠𝑠), probabilistically selecting from a distribution \nof possible actions available in that state. The agent \nreceived a reward when it selected actions that moved \nthe HAMR clockwise through the circular lumen \ntowards the target, and it received a negative reward \nwhen it moved the HAMR counterclockwise. If the \nHAMR reached the target within the allotted time, the \nagent was given a large bonus reward, and the target \nposition was advanced 20°. The reward function we \nselected was  \n𝑟𝑟(𝑠𝑠, 𝑎𝑎) = ∆𝜃𝜃𝑟𝑟+ 1000 𝐢𝐢𝐢𝐢 (𝜃𝜃𝑟𝑟= 𝜃𝜃𝑔𝑔)         (1) \nwhere 𝜃𝜃𝑟𝑟 is the angular position of the HAMR in the \nchannel in degrees, 𝜃𝜃𝑔𝑔 is the angular position of the goal, \nand ∆𝜃𝜃𝑟𝑟 is the change in angular position of the HAMR \nas a result taking of action 𝑎𝑎 in state 𝑠𝑠.  \nThe reinforcement learning problem was \nformalized as a Markov decision process [34], in which \nat time 𝑡𝑡, the state of the system 𝑠𝑠𝑡𝑡 is observed by the \nagent. The agent performs an action 𝑎𝑎𝑡𝑡 which changes \nthe state of the environment to 𝑠𝑠𝑡𝑡′, yielding a reward \n𝑟𝑟𝑡𝑡(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡). This process continues for the duration of the \ntask, yielding a trajectory of the form (𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡, 𝑟𝑟𝑡𝑡, 𝑠𝑠𝑡𝑡+1, \n𝑎𝑎𝑡𝑡+1, 𝑟𝑟𝑡𝑡+1, 𝑠𝑠𝑡𝑡+2…). The goal of the RL agent is to \nidentify an optimal policy 𝜋𝜋∗(𝑎𝑎|𝑠𝑠) for selecting actions, \nbased on state observations, that maximize the rewards \nreceived for following the policy. Over the course of \ntraining, the agent autonomously learned a control policy \nby trying actions in the environment, observing the \nreward obtained by performing those actions, and \nmodifying its future behavior in order to maximize the \nexpected future return.  \nThe control problem for our microrobotic \nsystem was formulated as an episodic, discrete time \nproblem with a continuous action space and continuous \nstate space. The state space consisted of all the possible \nstates for the system: the position of the HAMR within \nthe channel, the speed of the HAMR, the shape of the \nmagnetic fields, the time remaining in the episode, and \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n 5 \nrelative position of the robot to the target position in the \nchannel. The action space consisted of four continuous \nactions, which controlled the magnitudes and phase \nangles for sinusoidal currents in the Magneturret. While \nthe current waveforms could theoretically take on an \ninfinite number of shapes, we chose to define the applied \nwaveforms as sinusoids to bound the space of possible \nactions that the agent could take. Sinusoidal currents \nwere chosen because these can be used to generate \nrotating \nmagnetic \nfields \nin \nother \nthree-axis \nelectromagnetic actuators for microrobots, such as \nHelmholtz coils [54].  \nEntropy regularized deep reinforcement learning \nenabled continuous microrobot control \nWe selected the Soft Actor Critic RL algorithm \n(SAC) for this research. SAC is a maximum entropy RL \nalgorithm that seeks to balance the expected future \nrewards with the information entropy of the policy [46]. \nIn other words, SAC learns a policy that successfully \ncompletes the task while acting as randomly as possible, \nwhich in practice often leads to robust policies that are \ntolerant of perturbations in environmental conditions \n[36]. SAC had previously proven useful for real-world \nrobotic tasks with high-dimensional, continuous state \nand action spaces [36, 38], which suggested that it would \nbe applicable our microrobotic control problem. In \npreviously \nreported \napplications \nof \nreal-world \nreinforcement learning with physical systems [55], SAC \nwas demonstrated to be highly sample efficient, \nrequiring relatively few environmental interactions in \norder to develop a successful policy. Sample efficiency \nis critical when performing reinforcement learning with \nreal-world robotics (i.e. not simulated) in order to reduce \nwear and tear on the system, and in order to minimize the \ntime needed to learn a policy [37]. \n  \nThe SAC algorithm seeks to develop an optimal \nstochastic policy 𝜋𝜋∗: \n𝜋𝜋∗= 𝑎𝑎𝑎𝑎𝑎𝑎max\n𝜋𝜋\n∑𝔼𝔼(𝑠𝑠𝑡𝑡,𝑎𝑎𝑡𝑡)~𝜋𝜋[𝑟𝑟(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡) +\n𝑡𝑡\n 𝛼𝛼ℋ(𝜋𝜋(∙|𝑠𝑠𝑡𝑡)]  \n \n \n \n(2) \nwhere ℋ is the information entropy of the policy and 𝛼𝛼 \nis a temperature hyperparameter, which balances the \nrelative impact of the policy entropy against the expected \nfuture rewards. Here, we used a version of the SAC \nalgorithm in which the temperature is automatically \ntuned via gradient descent so that the entropy of the \npolicy continually matches a target entropy, ℋഥ, which \nwe selected to be -4 (-𝐷𝐷𝐷𝐷𝐷𝐷 of the actions space) using a \nheuristic suggested by Haarnoja et al [56]. A full \nderivation of the soft actor critic algorithm is beyond the \nscope of this paper, but interested readers are directed to \nHaarnoja et al. [46]. Briefly, the SAC algorithm uses an \nagent called the actor, denoted as 𝜋𝜋, which is a deep \nneural network that takes the state of the system 𝑠𝑠𝑡𝑡 as \ninput, and returns action 𝑎𝑎𝑡𝑡 as output. A value function \nis created to rate the value of taking actions when in \nparticular states, and instantiated using two critic neural \nnetworks 𝑄𝑄1,2(𝑠𝑠, 𝑎𝑎) which take states and actions as \ninput, and return values corresponding to the relative \nvalue of taking action 𝑎𝑎𝑡𝑡 in state 𝑠𝑠𝑡𝑡. Two Q networks are \ntrained in order to reduce overestimation in the value \nfunction. Environmental transitions in the form of \n(𝑠𝑠, 𝑎𝑎, 𝑟𝑟, 𝑠𝑠′, 𝑑𝑑) sets are recorded in an experience replay \nbuffer, 𝐷𝐷, where 𝑑𝑑 is a done flag denoting a terminal \nstate, set either when the microrobot has reached the \ngoal, or when the episode has timed out. The SAC \nalgorithm learns off-policy by randomly sampling \nminibatches of past experiences from 𝐷𝐷, and performing \nstochastic gradient descent over the minibatch in order \nto minimize loss functions for the actor network, 𝜋𝜋, critic \nnetworks, 𝑄𝑄1 and 𝑄𝑄2, and temperature parameter, 𝛼𝛼. \nOver the course of learning, the parameters of the actor \nand critic neural networks are updated so that the \nbehavior of the policy approaches the optimum policy, \n𝜋𝜋∗. A detailed version of the soft actor critic algorithm \nfor microrobot control that we used in this study is \navailable in Algorithm 1. Neural network architectures \nand \nhyperparameters \nused \nare \navailable \nin \nSupplementary Table 1.  \nHardware to control magnetic microrobots with \nreinforcement learning \nMagnetic fields created by electromagnetic \ncoils are common actuators used for magnetic \nmicrorobots and have significant potential for clinical \nmedical applications [3, 57, 58]. Magnetic fields act on \na magnetic microrobot by imparting forces and torques \non the robot. For a microrobot with a magnetic moment, \n𝒎𝒎, in a magnetic field, 𝑩𝑩, the robot experiences a force \n𝑭𝑭 according to \n𝑭𝑭= ∇(𝒎𝒎∙𝑩𝑩)   \n \n(3) \nIn a non-uniform magnetic field (i.e., a magnetic field \nwith a spatial gradient), a ferromagnetic or paramagnetic \nmicrorobot feels force in the direction of increasing \nmagnetic field gradient. The magnetic microrobot also \nexperiences a torque according to  \n𝝉𝝉= 𝒎𝒎× 𝑩𝑩  \n \n \n(4) \nwhich acts to align the magnetic moment of the \nmicrorobot with the direction of the magnetic field. \nWhen the magnetic field is rotated so that the direction \nof 𝑩𝑩 is constantly changing, it is possible to use this \ntorque to impart spin to the microrobot at the frequency \nof the rotating magnetic field, up to the step out \nfrequency of the robot [47]. If the spinning microrobot is \nhelically shaped, rotation can be transduced into forward \nmotion so that the microrobot swims similar to how \nbacterial are propelled by flagella [59]. This non-\nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n 6 \nreciprocal helical swimming is efficient in low Reynolds \nnumber fluidic environments commonly encountered by \nmicrorobots [59]. Because of the efficiency of this \nswimming mode, and because the magnetic torque \navailable to a microrobot decreases more slowly with \ndistance compared to the force [23], magnetic \nmicrorobots designed for long range magnetic operation \nare often helically shaped [6, 11, 60]. For this reason, we \nselected a helical magnetic microrobot, the HAMR, as \nour model system. \n \n \nAlgorithm 1: Soft Actor Critic for Microrobot Control \n1: Initialize policy parameters 𝜎𝜎, Q-function parameters 𝜔𝜔1, 𝜔𝜔2, and empty FIFO replay buffer 𝐷𝐷 \n2: Set target Q-function parameters equal to main parameters 𝜔𝜔𝑡𝑡𝑡𝑡𝑡𝑡𝑔𝑔𝑔𝑔𝑔𝑔,𝑛𝑛 ← 𝜔𝜔𝑛𝑛  \n3: Initialize ℋഥ = − number of actions (4), 𝛼𝛼= 1 \n4: Observe initial state 𝑠𝑠𝑡𝑡=0, and calculate 𝜃𝜃𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟∈(0,360°) \n5: Set 𝜃𝜃𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔 ← 𝜃𝜃𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟+ 20°,  𝜃𝜃𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔 ∈(0,360°)  \n6: Data Collection Process: Repeat \n7: If new 𝜋𝜋𝜎𝜎 is available: update  \n8:   While t steps < 33 or done = false  \n9:     select action 𝑎𝑎𝑡𝑡 ~ 𝜋𝜋𝜎𝜎(∙|𝑠𝑠𝑡𝑡) \n10:     Execute 𝑎𝑎𝑡𝑡 in the environment \n11:     For j in range (3) \n12:       Wait 0.3 seconds \n13:       Observe next state 𝑠𝑠′𝑗𝑗,  reward 𝑟𝑟𝑗𝑗(𝑠𝑠′𝑗𝑗), and done 𝑑𝑑 (1 If 𝜃𝜃𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟= 𝜃𝜃𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔  𝑂𝑂𝑂𝑂 𝑡𝑡=  33 𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸 0) \n14:     End For \n15:     Set 𝑠𝑠′𝑡𝑡←(𝑠𝑠′\n𝑗𝑗=1, 𝑠𝑠′\n𝑗𝑗=2, 𝑠𝑠′\n𝑗𝑗=3), 𝑟𝑟𝑡𝑡←∑\n𝑟𝑟𝑗𝑗\n𝑗𝑗\n \n16:     Store transition (𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡, 𝑟𝑟𝑡𝑡, 𝑠𝑠′𝑡𝑡, 𝑑𝑑) in replay buffer 𝐷𝐷 \n17:     Set 𝑠𝑠𝑡𝑡= 𝑠𝑠′𝑡𝑡 \n18:     If done:  set 𝜃𝜃𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔 ← 𝜃𝜃𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟+ 20°,  \n19:    t++ \n20:   End While \n \n21: Set 𝜃𝜃𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔 ← 𝜃𝜃𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟+ 20°, t = 0 \n22: Training Process: Repeat \n23: If number of updates < number of transitions in 𝐷𝐷 \n24:    Randomly sample batch of transitions, 𝐵𝐵= {(𝑠𝑠, 𝑎𝑎, 𝑟𝑟, 𝑠𝑠′, 𝑑𝑑)} from 𝐷𝐷 \n25:    Compute targets for the Q functions: \n \n \n𝑦𝑦(𝑟𝑟, 𝑠𝑠′, 𝑑𝑑) = 𝑟𝑟+  𝛾𝛾(1 −𝑑𝑑) ቀ𝑄𝑄𝜔𝜔𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡,𝑖𝑖(𝑠𝑠′, 𝑎𝑎෤′ ) − 𝛼𝛼𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 𝜋𝜋𝜎𝜎(𝑠𝑠′) ቁ,  𝑎𝑎෥′~ 𝜋𝜋𝜎𝜎(∙|𝑠𝑠′) \n26:    Update Q-functions using \n \n \n𝛻𝛻𝜔𝜔𝑖𝑖\n1\n|𝐵𝐵| ∑(𝑠𝑠,𝑎𝑎,𝑟𝑟,𝑠𝑠′,𝑑𝑑)∈𝐵𝐵\nቀ𝑄𝑄𝜔𝜔𝑖𝑖(𝑠𝑠, 𝑎𝑎) −𝑦𝑦(𝑟𝑟, 𝑠𝑠′, 𝑑𝑑)ቁ\n2\n \n \nfor 𝑖𝑖= 1,2 \n27:    Update Policy using \n \n \n𝛻𝛻𝜎𝜎\n1\n|𝐵𝐵| ∑𝑠𝑠 ∈ 𝐵𝐵\n൫𝑄𝑄𝜔𝜔𝑖𝑖(𝑠𝑠, 𝑎𝑎෤𝜎𝜎(𝑠𝑠) ) − 𝛼𝛼𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 𝜋𝜋𝜎𝜎(𝑎𝑎෤𝜎𝜎(𝑠𝑠)|𝑠𝑠) ൯, 𝑎𝑎෤𝜎𝜎 ~ 𝜋𝜋𝜎𝜎(∙|𝑠𝑠) \n28:    Update temperature 𝛼𝛼 using \n \n \n𝛻𝛻𝜎𝜎\n1\n|𝐵𝐵| ∑𝑠𝑠 ∈ 𝐵𝐵\n(−𝛼𝛼𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 𝜋𝜋𝜎𝜎(𝑎𝑎|𝑠𝑠 ) − 𝛼𝛼𝐻𝐻), 𝑎𝑎 ~ 𝜋𝜋𝜎𝜎(∙|𝑠𝑠) \n29:     Update the target Q-functions using \n \n \n𝜔𝜔𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡,𝑛𝑛← 𝜔𝜔𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡,𝑛𝑛+ (1 −𝜏𝜏)𝜔𝜔𝑛𝑛  \n \nfor 𝑛𝑛= 1,2 \n30: End If \n31: Send latest 𝜋𝜋𝜎𝜎 to data collection process every minute \n32: Until convergence \nThe HAMR that we created for this study was \ncomposed of a 2% w/v agar hydrogel, which was \nuniformly diffused with 10% w/v iron oxide nanopowder \nto form a magnetically susceptible soft polymer (Figure \n2a) [4]. This magnetic agar solution was heated to \nmelting temperature and a syringe was used to inject the \nliquid \ninto \na \nhelical \nmold \ncreated \nusing \na \nstereolithography 3D printer (Figure 2b). The agar in the \nmold solidified and the robots were removed with a \nmetal needle and stored long-term in deionized (DI) \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n 7 \nwater. The HAMRs molded for this study were 4.4 mm \nin length, 1 mm in diameter, and asymmetrical from head \nto tail, with flat head and a pointed tail (Figure 2c,d). \nMicrorobots formed with this technique have been \npreviously shown to be controllable within rotating \nmagnetic fields, and to perform biomedical functions \nsuch as cell delivery [4] and active biofilm removal in \nthe root canal of human teeth [54]. For our application, \nthis HAMR design had several advantages. The HAMRs \nwere simple to manufacture at low cost with batch \nfabrication methods. The HAMRs were small enough to \nact as helical swimming robots in a flow regime with \nReynolds number ~1, but large enough, about the size of \na small grain of rice, to be easily manipulated and \nvisualized without the use of microscopes or other \nmicromanipulation tools. Because the HAMRs swim \nwith non-reciprocal, helical motion in the presence of a \nrotating magnetic field, a very common motif in \nmicrorobotic research [11, 60, 61], insights gained from \nthis study could readily be extended to other \nmicrorobotic systems with similar characteristics. \nBecause the HAMRs were made of soft hydrogel, they \nwere flexible and deformable. Soft bodied robots have \nmany favorable characteristics for in vivo use such as \ndeformability to fit through irregular shaped channels \nand enhanced biocompatibility (e.g., by matching the \nelastic modulus of the robot to the biological \nenvironment) [62]. These characteristics make soft-\nbodied \nmicrorobots \nappealing \nfor \nbiomedical \napplications, but it can be more difficult to create \naccurate dynamic models for soft-bodied microrobots \n[16]. Our method of using reinforcement learning to \ndevelop control systems without explicit modeling could \nbe particularly useful for soft microrobots due to this \nmodeling constraint. Finally, despite being soft-bodied, \nthe hydrogel structure of the HAMR did not experience \nnoticeable wear over the course of several months of \ncontinuous use, thus meeting a practical reinforcement \nlearning constraint that the system not be susceptible to \nsignificant wear and tear during extended use which \nwould cause a distribution shift in the collected data as \nthe dynamic properties of the system degraded [55]. \nAs an actuator for our microrobot system, we \ndeveloped a three-axis magnetic coil actuator –the \nMagneturret– which contained six permalloy-core \nmagnetic coils arranged on the faces of a 3D-printed \nacrylonitrile butadiene styrene (ABS) plastic cube \n(Figure 2e). The two coils on opposite sides of the central \ncube along each axis were wired together in series so that \nthey both contribute to the generation of a magnetic field \nalong their respective axis. Each of the three coils, \nhereafter referred to as the X, Y, and Z coils, were driven \nwith a sinusoidal current generated by a pulse width \nmodulated (PWM) signal created by a microcontroller \nand amplified in an H-bridge motor driver. The resulting \nmagnetic field, produced by the superposition of the \nmagnetic fields from the three coils, could be modulated \nby varying the frequency, amplitude, and phase angle of \nthe sine current waves in each coil. To cool the coils and \nprevent thermal damage, the Magneturret was sealed \nwith epoxy resin into a 3D-printed housing and coolant \nwas continuously pumped through while the coil was \noperating (Figure 2f). The RL agent was given direct \ncontrol over the magnitude and phase angles of the \nsinusoidal driving currents in the X and Y-axis coils of \nthe Magneturret (Table 1). The Z-axis magnitude was \ncalculated as the larger of the two magnitudes in X and \nY, and the Z-axis phase angle was fixed. The sinusoidal \ncurrents in each axis used a fixed angular frequency of \n100 rad/s (15.9 Hz).\n \nMagnetic Coil Control Parameters \nControl Variable \nSymbol \nSource \nRange \nFrequency \nf \nFixed \n15.9 Hz (ω=2πf=100 rad/s) \nMagnitude X \nMX \nRL agent \n[-1,1] unitless \nMagnitude Y \nMY \nRL agent \n[-1,1] unitless \nMagnitude Z \nMZ \nmax(|MX|,|MY|) \n[0,1] unitless \nPhase Angle X \nφX \nRL agent \n[0,2π] radians \nPhase Angle Y \nφY \nRL agent \n[0,2π] radians \nControl Equations \nCurrent in X-axis coil \nIx = Mxsin(ft + φx)                                               (5) \nCurrent in Y-axis coil \nIy = Mysin(ft + φy)                                               (6) \nCurrent in Z-axis coil \nIz = Mzsin(ft)                                                         (7) \nTable 1. Control inputs for electromagnet waveforms. \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n 8 \n \nFigure 2. Hardware for real-world control of magnetic microrobots using reinforcement learning. (a) Helical \nAgar Magnetic Robot (HAMR) schematic. (b) HAMRs were fabricated by molding molten magnetic hydrogel in 3D \nprinted molds. Scale bar = 10 mm. (c) HAMR with a United States 5-cent coin. Scale Bar = 5 mm. (d) HAMRs were \ncomposed of agar hydrogel infused with iron oxide nanopowder. Scale bar = 1 mm. (e) The Magneturret was \ncomposed of six coils of copper magnet wire wrapped around permalloy cores, positioned on the faces of a central 3D \nprinted cube. (f) The Magneturret was enclosed within a 3D printed housing and sealed with epoxy, and glycerol \ncoolant was continuously pumped through the Magneturret housing. Scale bar = 20 mm. (g) A circular PDMS track \nwith a rectangular cross section used as an arena for the HAMR. A black, circular fiducial marker indicates the center \nof the arena. Scale bar = 10 mm. (h) The PDMS arena was submerged in a water-filled petri dish placed on top of the \nMagneturret, with an acrylic LED light sheet as a backlight for uniform bottom-up illumination. Scale bar = 30 mm. \n(i) The complete hardware system.  \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n 9 \nWe chose to operate our HAMR in a circular, \nfluid-filled track for this study. This arena served as a \nsimple environment which mimics the tortuous in vivo \nluminal environments that microrobots operating in the \nbody might encounter, while providing a simple \nenvironment for us to establish a robust proof-of-concept \nreinforcement learning control system (Figure 2g). The \nHAMR could swim in a complete circle within this \narena, and no human intervention was required to reset \nthe position of the robot in the environment during \ntraining, which facilitated automated learning [37]. The \narena was constructed by pouring polydimethylsiloxane \n(PDMS) over a polyvinyl chloride ring with an outer \ndiameter of 34 mm and a 1.7 mm x 3 mm rectangular \ncross section. The PDMS was then cured, and plasma \nbonded to a second flat sheet of cured PDMS to form a \nrectangular lumen for the HAMR to swim. PDMS is \ntransparent, allowing us to see the robot in the arena and \nto visually track it with an overhead camera. During \nlong-term learning experiments, the PDMS arena was \nsubmerged in a petri dish filled with DI water in order to \nprevent the formation of air bubbles in the channel due \nto evaporation over the course of an experiment. This \npetri dish was then placed on top of the Magneturret, \nwith the center of the Z-axis coil aligned with the center \nof the circular track (Figure 2h). A black rubber wafer \nwas placed into the center of the arena on top of the \nPDMS to act as a fiducial marker so that the center of the \narena could easily be identified with image processing. \nA diffuse white LED backlight was positioned between \nthe Magneturret and the PDMS arena for uniform \nbottom-up illumination which facilitated simple image \nprocessing by binary thresholding to identify the position \nof the microrobot in the channel.  \nWe did not perform an extensive analysis to \nidentify the shape or magnitude of the magnetic field \ncreated by the Magneturret, or to model the swimming \ndynamics of the HAMR in the PDMS arena. We \nhypothesized that we would be able to develop a high-\nperformance control system using RL without going \nthrough the effort of developing a system model first. \nReinforcement learning can be used to learn \nmicrorobot control policies \nState information of the microrobotic system \nwas derived by using image processing to create a state \nvector-based input, which was passed to the RL agent. \nThe angular position, 𝜃𝜃𝑟𝑟, of the microrobot in the \nchannel was calculated with image processing by binary \nthresholding and simple morphological operations. The \ncamera was deliberately run with a slow shutter speed so \nthat the images were intentionally washed out to remove \nnoise. This simplified the task of using binary \nthresholding operations to identify the position of the \nHAMR and the center of the channel. The angular \nposition of the HAMR in the channel was measured \nrelative to the fiducial marker in the center of the circular \narena. This information, as well as the position of the \ngoal, 𝜃𝜃𝑔𝑔, the last action taken by the agent \n(𝑀𝑀𝑥𝑥,𝑡𝑡−1, 𝑀𝑀𝑦𝑦,𝑡𝑡−1, 𝜑𝜑𝑥𝑥,𝑡𝑡−1, 𝜑𝜑𝑦𝑦,𝑡𝑡−1), \nand \nthe \ntime, \n𝑡𝑡 , remaining in the episode were used to create a state \nvector.  \nReinforcement learning is based on the \nmathematics of Markov decision processes, which \ntheoretically require the full state of the system to be \navailable to the agent in order for convergence to be \nguaranteed [34]. In our particular implementation, the \nvelocity of the HAMR at any given time could not be \ndetermined from a single still-frame observation, so the \ntotal state of the system given to the agent at each time \nstep was composed of three concatenated sub-\nobservations taken 0.3 seconds apart, for a total step time \nof 0.9 seconds (see Algorithm 1, steps 11-15). This \nallowed the agent to infer the velocity of the HAMR \nbased on differences between the three sub-observations. \nThis technique of batching sequential observations for \nimproving the observability of the system for RL has \nbeen used successfully in domains such as Atari video \ngames, in which the agent learned from raw pixel data \ngathered from sequential screenshots of the game [63].  \nAt the beginning of each learning trial, the actor \nand critic neural network parameters were randomly \ninitialized. We allowed the RL agent to train for a \nmaximum of 100,000 times steps, using a fixed ratio of \none gradient update per environmental step, which has \nbeen shown to reduce training speed in exchange for \nhigher training stability [64]. 100,000 environment steps \nwere adequate time for effective actuation policies to be \nlearned that continuously moved the HAMR clockwise \naround the arena (Figure 3a). It is commonly reported \nwhen using reinforcement learning that several million \nenvironmental steps are necessary to derive a successful \npolicy [37, 63], so this result shows the sample efficiency \nof SAC, which is critically important for RL tasks that \nare trained using physical systems instead of in \nsimulation. A time-lapse movie of the HAMR recorded \nduring the learning process is shown in supplementary \nmovie 1.  We tracked the net movement of the HAMR \nduring the training process, and each training session \nended with the microrobot going continuously around \nthe track in a clockwise direction (Figure 3b).  \n \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n10 \n \nFigure 3. Reinforcement learning yielded successful control policies for the HAMR within 100k time steps. (a) \nWe trained the agent for a total of 100k time steps per training session. The trace represents the average and standard \ndeviation of the return from three successful training runs. (b) Successful training resulted in policies which, after an \ninitial learning period, achieved consistent forward motion, with the HAMR moving continuously around the circular \narena. (c) We recorded the actions and state of the agent over 100k training steps. The majority of actions taken by \nthe agent for the first 20k steps resulted in no motion (i). As the agent learned, the action distribution became bimodal \n(ii-v), with a second peak appearing at ~5 degrees per action, indicating the agent was increasingly taking actions \nwhich resulted in forward movement. (d) Averaging the velocity distribution over the training period showed that at \nsome time between 40k and 60k steps the agent had learned to achieve net positive movement.  (e) The RL agent \nlearned policies that moved the microrobot around the full circular arena with helical swimming motion.  \n \nWe recorded each action taken by the agent \nduring training sessions and the resultant change in state \nof the microrobot. For a single training run, we plotted \nthe distribution of actions as a function of the resultant \nchange in HAMR position, ∆𝜃𝜃𝑟𝑟 (Figure 3c). We \nseparated the total 100k steps into 5 bins of 20k steps \neach.  The distribution of actions over the first 20k time \nsteps (Figure 3c, i) is centered around a sharp peak of \nactions which result in no net movement, as we would \nexpect from an agent with little experience randomly \nexploring the space of possible actions. By the second \nbatch of 20k steps (Figure 3c, ii), a pattern emerged in \nwhich the action distribution shifted to a bimodal \ndistribution in which most actions still result in no net \nmovement, but a second peak on the positive side \nindicates a trend towards selecting actions which result \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n11 \nin clockwise movement. However, during this phase of \ntraining, the net motion of the robot remained close to \nzero (Figure 3d), because of fattening of the negative tail \nin the action distribution. As the learning process \ncontinued, the distribution continued to shift until the \naverage movement was clockwise, with a second peak \naround 5 degrees per time step, and a narrow tail \nrepresenting few actions, which caused the robot to \nmove in the counterclockwise direction (Figure 3c, v).  \nOnce the training sessions were complete, we \nevaluated the learned policies to test their performance. \nFor evaluating policies, we used the highest performing \npolicy parameters learned during a training session by \nmonitoring a rolling average of the return over the last \n100 episodes and saving the policy parameters each time \nthe rolling average performance exceeded the last best \nperforming model (Supplementary Figure 1). This was \ndone because we sometimes observed a drop in \nperformance after the peak performance was achieved in \ntraining, possibly due to overfitting. Early stopping, or \nselecting a policy before performance degradation has \noccurred, is a common technique used to prevent \noverfitting in neural networks [65]. Successful policies \nwere able to move the robot indefinitely around the \ncomplete circular track (Figure 3e, Supplementary \nMovie 2).  \nThe soft actor critic algorithm learns a \ncontinuous stochastic policy, 𝜋𝜋, sampling actions from \nthe policy according to 𝑎𝑎𝑡𝑡 ~ 𝜋𝜋(∙|𝑠𝑠𝑡𝑡), in which the \nactions selected during training are randomly sampled \nfrom a Gaussian distribution, and the agent learns the \nmean μ and the variance of this distribution over the \ncourse of training [46]. This is done in order to explore \nthe space of possible actions during training. During \ntraining the agent seeks to balance the sum of future \nrewards with the information entropy of the policy by \nmaximizing an entropy regularized objective function, \nand the policy entropy corresponds to the explore/exploit \ntradeoff the agent makes during training. However, once \nthe policies were trained, performance during policy \nevaluation could be increased by selecting actions from \nthe mean of the distribution without further stochastic \nexploration according to 𝑎𝑎𝑡𝑡=  𝜇𝜇(𝑠𝑠𝑡𝑡). This deterministic \nevaluation led to an increase in the proportion of actions \ntaken by the agent which resulted in positive motion \n(Figure 4a). We compared the total average velocity \nachieved by all the trained policies in both deterministic \nand stochastic action selection modes, which showed \nthat deterministic action selection led to higher \nperformance (Figure 4b).  \nWe next examined the distribution of the action \nvalues chosen by the RL agent when evaluated \ndeterministically according to 𝑎𝑎𝑡𝑡=  𝜇𝜇(𝑠𝑠𝑡𝑡). For each of \nthe four actions (Mx, My, φx, φy) (Figure 4c) taken by the \npolicy over 3000 time steps, we plotted the value of the \naction against the position of the HAMR, 𝜃𝜃𝑟𝑟 (Figure 4d). \nThe plotted actions are color-coded according to ∆𝜃𝜃𝑟𝑟 \n(Figure 4e), with red actions indicating positive forward \nmotion and blue actions indicating retrograde motion. \nThe majority of actions taken by each of the three \npolicies during evaluation resulted in positive motion. \nEach of the three policies followed similar patterns, in \nwhich the phase angle of the X coil was held relatively \nconstant for all 𝜃𝜃𝑟𝑟, and the magnitude of the X coil varied \naccording to 𝜃𝜃𝑟𝑟. The Y coil was controlled by actuating \nthe phase angle as a function of position and holding the \nmagnitude relatively constant. One consistent pattern \nacross all learned policies is that the magnitudes tended \nto hold steady close to the maximum or minimum values \nof -1 and positive 1, regardless of 𝜃𝜃𝑟𝑟. This would result \nin the largest amplitude current sine waves, which we \nwould expect because stronger magnetic fields would be \nable to create more powerful torques on the HAMR.  \n \n \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n12 \n \n \nFigure 4. Evaluating the learning performance of the RL agent during multiple training sessions. (a) Following \nthree training sessions for the RL agent, we evaluated the performance of the learned stochastic policy, 𝜋𝜋, and the \ndeterministic policy 𝜇𝜇 for 3k steps without additional learning. In each policy, the performance was higher while \nselecting actions deterministically from 𝜇𝜇. (b) The average velocity over 3k evaluation steps for each of the three \ntrained policies during evaluation for stochastic and deterministic action selection, ± standard deviation. (c) Schematic \nshowing the control parameters for the sine waves used to drive the current in the Magneturret. The RL agent had \ncontrol over the Magnitude M and the phase angle φ in the X and Y coils. (d) Schematic showing the angular position \n𝜃𝜃𝑟𝑟 of the robot in the circular channel. (F) Actions taken by the policies during deterministic evaluation plotted \naccording to 𝜃𝜃𝑟𝑟 at the beginning of the action, and color coded according to the resultant velocity during that action.   \n \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n13 \nOptimizing the RL-trained policies via regression \n \nWe observed that the microrobot control \npolicies learned by the RL agent sometimes performed \nactions that were obviously non-optimal (resulting in \nnegative motion). We could likely further increase the \nperformance of the learned polices by using techniques \nlike hyperparameter tuning and longer training times \n[64]. However, by observing the behavior of the RL \nagent, we hypothesized that if we could distill the \npolicies learned by the network into mathematical \nfunctions of the state variables, we might achieve a \nhigher level of performance (Figure 5a). To test this, we \nchose one of the policies, and fit regression models to the \ndata in order to create continuous control signals as a \nfunction of the robot position 𝜃𝜃𝑟𝑟. First, we examined \npolicy 1 (Figure 4e). This policy was acting by \nmodulating the magnitude in the X coil in what \napproximated a square wave pattern and modulating the \nphase angle in the Y coil in what appeared closer to a \nsine wave. The other two actions were held \napproximately constant regardless of the position of the \nrobot. From all 3000 actions taken during policy \nevaluation, we selected the subset of actions taken by \npolicy which had resulted in a positive movement of at \nleast 3°, discarding the lower performing actions for this \nanalysis. We then fit sinusoidal regression models to the \n𝑀𝑀𝑥𝑥 and 𝜑𝜑𝑦𝑦 action distributions, and also fit a square \nwave to 𝑀𝑀𝑥𝑥 (Figure 5b). The resulting policies are shown \nin Figure 5a as solid black lines superimposed over the \naction distribution. The sine wave policy (Figure 5c) and \nthe mixed sine/square wave policy (Figure 5d) that we \ndeveloped with the regression models were then used to \ncontrol the HAMR.  \nThe sinusoidal policy achieved the highest level \nof performance (Figure 5e), achieving the highest \naverage HAMR velocity of all policies tested in this \nstudy, while the square/sine policy performed slightly \nworse than the neural network policy on which it was \nbased. Since these mathematical policies only use 𝜃𝜃𝑟𝑟 as \nthe input, it is possible that we could further increase the \nperformance of mathematically inferred policies by \ntaking other parts of the state vector into account.  \n \nFigure 5. Control policies learned by the RL agent could be translated into continuous functions in order to \nincrease performance. (a) The policies learned by the RL agent were used as a basis to derive higher-performance \npolicies via regression of the positive action distribution. (b) For one of the policies that the RL agent learned, the \nactions were plotted as a function of 𝜃𝜃𝑟𝑟 and mathematical functions were fit to the subset of actions, which yielded \nHAMR velocities greater than 3 degrees per step. Sine waves and square waves were fit to the data via regression \n(shown in black), and those mathematical function were used to control the HAMR for 1k time steps.  (c) The results \nof running the sinusoidal policy and (d) the sine/square policy. (e) Comparing the average velocity of the HAMR \nwhen controlled by each policy. \n \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n14 \nDiscussion \nHere, we have reported the development of a \nclosed-loop control system for magnetic helical \nmicrorobots, \nwhich \nwas \nimplemented \nusing \nreinforcement learning to discover control policies \nwithout the need for any dynamic system modeling. \nContinuous control policies for high-dimensional action \nspaces were represented by deep neural networks for \neffective control of magnetic fields to actuate a helical \nmicrorobot within a fluid-filled lumen.  Compared with \npreviously reported control systems for magnetic \nmicrorobots [23], we believe that the system we have \npresented possesses a number of key advantages. \nElectromagnetic actuation systems for microrobots are \neither air core, such as Helmholtz coils and Maxwell \ncoils, or contain soft magnetic materials in the core \nwhich enhance the strength of the generated magnetic \nfield, but can lead to nonlinearities when summing the \ncombined effect of fields from multiple coils with \nsaturated magnetic cores [66]. Such nonlinearities make \nmodeling the behavior of the system more difficult [67], \nparticularly when the coils are run with high enough \npower to magnetically saturate the core material. \nAdditionally, when controlling microrobots with \npermanent magnets, those magnets are often modeled as \ndipole sources for simplicity [68], and the actual \nbehavior of the physical system may not match the \nidealized model behavior. Neural network-based \ncontrollers trained with RL learn control policies from \nobserving the actual behavior of the physical system, and \ndeep neural networks can accurately model non-linear \nfunctions [69]. Control policies learned with RL will \nautomatically take into account the real system \ndynamics, and this model-free control approach can \ngreatly simplify the job of the microrobotic engineer.  \nSignificant work has been done to create \naccurate dynamic models of rigid helical microrobots, \nand many microrobots are relatively straightforward to \ncontrol using these models and classical control systems \n[47]. However, many recently developed microrobotic \nsystems are composed of soft, shape changing materials, \nwhich are inherently harder to model than rigid bodies \n[5, 13, 14, 16, 17]. Soft microrobots such as helical \ngrippers, which can kinematically change their \nconfiguration during operation, might also be difficult to \naccurately model [7]. Here, we have shown that our \nalgorithm was able to control a soft helical microrobot \nwithout any dynamic modeling on the part of the control \nsystem designers. RL based microrobot control could \nenhance both the capabilities of novel microrobot \ndesigns and increase the efficiency of researchers by \nallowing the RL agent to do the work of developing a \nhigh-performance controller. RL based controllers may \nbe able to exceed the performance of classical control \nsystems based on simplified models (e.g., linearized \nmodels) because the RL agent is able to learn based on \nthe observed physical behavior of the system, and deep \nneural networks are capable of accurately modeling any \nobserved nonlinearities that the microrobotic system \nmight exhibit.  \nOther control strategies that have been \nsuccessfully applied for microrobot control could be \nenhanced with RL. Algorithms which have been used to \ncontrol soft microrobots such as force-current mapping \nwith PID control and path planning algorithms [5, 23, \n67] could potentially be combined with reinforcement \nlearning in order to optimize the gains in the PID \ncontrollers, and adapt to changes in environmental \nconditions by a process of continuous learning, or to \noptimize for multiple variables. Force-current mapping \nalgorithms used to control microrobots are also often \ncreated with assumptions of linearity in magnetic field \nsuperposition, which could be violated with soft \nmagnetic cores in the driving coils [50], a limitation that \ncould be potentially overcome by the nonlinear function \napproximation capabilities of deep neural networks. \nAnother potential combination of RL with classical \ncontrol has been demonstrated by Zeng et al [39], who \nused a residual physics model in order to fine tune a \nphysics-based model of a grasping and throwing \nproblem for a robotic arm. Using such methods, \nimprecise models of microrobot dynamics could be \nimproved by fine tuning their parameters based on a data \ndriven RL approach, leading to increased performance.  \nWe have demonstrated that it is possible to \nlearn microrobot control policies with reinforcement \nlearning based on no prior knowledge, and then fine tune \nthe performance of the policy by fitting continuous \nmathematical functions to the learned policy behaviors. \nWhile the sine and sine/square policies that we created \nbased on analyzing the learned policies might have been \nidentified by a first principles analysis of the problem, \nthe policies derived by the neural networks could \npotentially uncover useful behaviors, which would not \nbe suspected or created by human engineers, particularly \nfor systems that are difficult to model. This ability to \ndetect subtle patterns from high dimensional data in a \nmodel-free RL approach could ultimately lead to state-\nof-the-art control policies that exceed the performance of \nhuman-designed policies, as has been seen with \nreinforcement learning algorithms in tasks like playing \nGo [41] and classic Atari games [63]. \nIn our experimentation, we found that the RL \nagent could learn successful polices from state vector \ninputs, but this system could likely be expanded to use \nand raw image data as input, as has been demonstrated \nin other RL-based robotic control research [56]. The \nability to derive control models directly from high-\ndimensional data could make RL-based microrobot \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n15 \ncontrol applicable for a broad class of biomedical \nimaging modes in which the state of the system might be \nrepresented by MRI, X-ray, ultrasound, or other \nbiomedical imaging methods [24]. Using higher \ndimension inputs like images has the potential to encode \nricher policies which respond to objects in the field of \nview such as obstacles which could impede the forward \nprogress of the microrobot but would not be observable \nfrom lower dimensional feedback available in a state \nvector representation.  In complex environments in \nwhich environmental factors such as lumen shape, fluid \nflow profiles, surface interactions, and biological \ninteractions are likely to be a significant factor [2], the \nability to use machine vision for state representation \ncould significantly improve microrobot performance. \nAll these points strongly favor the use of RL for \ndeveloping the next generation of microrobot control \nsystems.  \nMethods \nHelical agar magnetic robot. The HAMR was \nconstructed based on a method published by Hunter et al \n[4]. The structure of the robot was formed from a 2% w/v \nagar-based hydrogel (Fisher Cat. No. BP1423-500). The \nagar was melted to above 80 degrees Celsius and mixed \nwith iron oxide nanopowder (Sigma Aldrich Cat. No. \n637106) to a total concentration of 10% w/v. This mix \nwas injected into a helical 3D printed mold printed on \nthe Elegoo Mars stereolithography 3D printer to form a \nhelical microrobot 4.4 mm in length. The microrobot was \nmanually removed from the mold after cooling and \nsolidifying, and stored in deionized water until use. \nBecause the yield of this batch fabrication technique was \nnot 100%, robots used for subsequent experiments were \nchosen based on their morphology and responsiveness to \nmagnetic fields. \nCircular swimming arena. The PDMS swimming \narena was created by molding Sylgard 184 elastomer \n(Sigma Aldrich Cat. No. 761036) over a thin 3mm tall \nsection of polyvinyl chloride pipe (31 mm Inner \nDiameter, 34mm Outer diameter) Access holes for the \nmicrorobot were cut, and then the molded PDMS was \nplasma bonded to a thin uniform sheet of PDMS to close \nthe channel, and cured overnight at 65° C.  \nMagneturret. The Magneturret was constructed by \nwinding 6 identical coils with 400 turns each of 30-gauge \nmagnet wire (Remington Industries Cat. No. 30H200P) \naround a 0.26 inch diameter permalloy core (National \nElectronic Alloys Cat. No. HY-MU 80 Rod .260 AS \nDRAWN) cut to a length of 20 mm. These coils were \nfixed to the sides of an Acrylonitrile butadiene styrene \n(ABS) 3D printed cube with quick set epoxy. The coil \nwas enclosed in a 3D printed housing printed in Zortrax \nZ-glass filament with a Zortrax M200 printer and sealed \nwith epoxy. Glycerol coolant was pumped through the \nhousing with a liquid CPU cooling system (Thermaltake \nCat. No. CL-W253-CU12SW-A). The coils were \nenergized by creating sinusoidal currents with an \nArduino STEMtera breadboard, which took serial \ncommands from the RL agent over USB and turned them \ninto PWM signals which were sent to two Pololu Dual \nG2 High-Power Motor Driver 24v14 Shields. The power \nsupply used to power the coils and was a Madewell 24V \nDC power supply. \nOverhead camera. The overhead camera was an \nAlvium 1800 U-500c with a 6mm fixed focal length lens \nfrom Edmund Optics. The camera used to take images \nfor the state was set at a long exposure so that the HAMR \nand the center mark were the only visible objects in the \nimage. A second identical camera placed above the arena \nat a slight angle was used to simultaneously record \nnormal exposure video of the HAMR in the arena during \noperation, so that the features in the image were not \nwashed out.  \nRL algorithm. The soft actor critic RL agent was \ndeveloped in Python, using Tensorflow 2.0 for creating \nthe neural network models. This was run on a desktop \nworkstation from Lambda Labs. Separate processes were \nused for data collection and updating the neural networks \nso that the two operations could run in parallel. The full \nalgorithm details are available in Supplementary \nAlgorithm 1.  \nSupplementary Materials: \nFig S1: Highest performing policy parameters during \ntraining \nTable S1: Hyperparameters \nMovie S1: Training process time course. \nMovie S2: HAMR swimming around the track \n \n \n \n \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n16 \nReferences \n \n1. \nWang, B., et al., Trends in Micro-\n/Nanorobotics: Materials Development, \nActuation, Localization, and System \nIntegration for Biomedical Applications. \nAdvanced Materials, 2020. n/a(n/a): p. \n2002047. \n2. \nCeylan, H., et al., Translational prospects \nof untethered medical microrobots. \nProgress in Biomedical Engineering, \n2019. 1(1): p. 012002. \n3. \nJeon, S., et al., Magnetically actuated \nmicrorobots as a platform for stem cell \ntransplantation. Science Robotics, 2019. \n4(30): p. eaav4317. \n4. \nHunter, E.E., et al., Toward Soft Micro \nBio Robots for Cellular and Chemical \nDelivery. IEEE Robotics and Automation \nLetters, 2018. 3(3): p. 1592-1599. \n5. \nScheggi, S., et al. Magnetic motion control \nand planning of untethered soft grippers \nusing ultrasound image feedback. in 2017 \nIEEE International Conference on \nRobotics and Automation (ICRA). 2017. \n6. \nDong, M., et al., 3D-Printed Soft \nMagnetoelectric Microswimmers for \nDelivery and Differentiation of Neuron-\nLike Cells. Advanced Functional \nMaterials, 2020. 30(17): p. 1910323. \n7. \nJin, Q., et al., Untethered Single Cell \nGrippers for Active Biopsy. Nano Letters, \n2020. 20(7): p. 5383-5390. \n8. \nPark, J., et al., Magnetically Actuated \nDegradable Microrobots for Actively \nControlled Drug Release and \nHyperthermia Therapy. Advanced \nHealthcare Materials, 2019. 8(16): p. \n1900213. \n9. \nHosney, A., et al. In vitro validation of \nclearing clogged vessels using \nmicrorobots. in 2016 6th IEEE \nInternational Conference on Biomedical \nRobotics and Biomechatronics (BioRob). \n2016. \n10. \nMedina-Sánchez, M. and O.G. Schmidt, \nMedical microbots need better imaging \nand control. Nature News, 2017. \n545(7655): p. 406. \n11. \nWang, X., et al., 3D Printed \nEnzymatically Biodegradable Soft Helical \nMicroswimmers. Advanced Functional \nMaterials, 2018. 28(45): p. 1804107. \n12. \nAlapan, Y., et al., Multifunctional surface \nmicrorollers for targeted cargo delivery in \nphysiological blood flow. Science \nRobotics, 2020. 5(42): p. eaba5726. \n13. \nZhang, J., et al., Voxelated three-\ndimensional miniature magnetic soft \nmachines via multimaterial heterogeneous \nassembly. Science Robotics, 2021. 6(53): \np. eabf0112. \n14. \nHu, W., et al., Small-scale soft-bodied \nrobot with multimodal locomotion. Nature, \n2018. 554(7690): p. 81-85. \n15. \nBreger, J.C., et al., Self-Folding Thermo-\nMagnetically Responsive Soft \nMicrogrippers. ACS Applied Materials & \nInterfaces, 2015. 7(5): p. 3398-3405. \n16. \nMedina-Sánchez, M., et al., Swimming \nMicrorobots: Soft, Reconfigurable, and \nSmart. Advanced Functional Materials, \n2018. 28(25): p. 1707228. \n17. \nXu, T., et al., Millimeter-scale flexible \nrobots with programmable three-\ndimensional magnetization and motions. \nScience Robotics, 2019. 4(29): p. \neaav4494. \n18. \nRicotti, L., et al., Biohybrid actuators for \nrobotics: A review of devices actuated by \nliving cells. Science Robotics, 2017. \n2(12): p. eaaq0495. \n19. \nZhang, H., et al., Dual-responsive \nbiohybrid neutrobots for active target \ndelivery. Science Robotics, 2021. 6(52): p. \neaaz9519. \n20. \nXu, H., et al., Sperm Micromotors for \nCargo Delivery through Flowing Blood. \nACS Nano, 2020. 14(3): p. 2982-2993. \n21. \nAlapan, Y., et al., Soft erythrocyte-based \nbacterial microswimmers for cargo \ndelivery. Science Robotics, 2018. 3(17): p. \neaar4423. \n22. \nWei, T.-Y. and W.C. Ruder, Engineering \ncontrol circuits for molecular robots using \nsynthetic biology. APL Materials, 2020. \n8(10): p. 101104. \n23. \nXu, T., et al., Magnetic Actuation Based \nMotion Control for Microrobots: An \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n17 \nOverview. Micromachines, 2015. 6: p. \n1346-1364. \n24. \nAziz, A., et al., Medical Imaging of \nMicrorobots: Toward In Vivo \nApplications. ACS Nano, 2020. 14(9): p. \n10865-10893. \n25. \nYasa, I.C., et al., Elucidating the \ninteraction dynamics between \nmicroswimmer body and immune system \nfor medical microrobots. Science \nRobotics, 2020. 5(43). \n26. \nStock, J.B., A.J. Ninfa, and A.M. Stock, \nProtein phosphorylation and regulation of \nadaptive responses in bacteria. \nMicrobiological reviews, 1989. 53(4): p. \n450-490. \n27. \nTagkopoulos, I., Y.-C. Liu, and S. \nTavazoie, Predictive Behavior Within \nMicrobial Genetic Networks. Science, \n2008. 320(5881): p. 1313-1317. \n28. \nArdiel, E.L. and C.H. Rankin, An elegant \nmind: learning and memory in \nCaenorhabditis elegans. Learning & \nmemory, 2010. 17(4): p. 191-201. \n29. \nGiurfa, M., Learning and cognition in \ninsects. WIREs Cognitive Science, 2015. \n6(4): p. 383-395. \n30. \nCalvo Garzón, P. and F. Keijzer, Plants: \nAdaptive behavior, root-brains, and \nminimal cognition. Adaptive Behavior, \n2011. 19(3): p. 155-171. \n31. \nBonilla, F.A. and H.C. Oettgen, Adaptive \nimmunity. Journal of Allergy and Clinical \nImmunology, 2010. 125(2, Supplement 2): \np. S33-S40. \n32. \nDickinson, A., Contemporary animal \nlearning theory. Vol. 1. 1980: CUP \nArchive. \n33. \nTsang, A.C.H., et al., Roads to Smart \nArtificial Microswimmers. Advanced \nIntelligent Systems, 2020. 2(8): p. \n1900137. \n34. \nSutton, R.S. and A.G. Barto, Introduction \nto reinforcement learning. Vol. 135. 1998: \nMIT press Cambridge. \n35. \nKober, J., J.A. Bagnell, and J. Peters, \nReinforcement learning in robotics: A \nsurvey. The International Journal of \nRobotics Research, 2013. 32(11): p. 1238-\n1274. \n36. \nHaarnoja, T., et al., Learning to walk via \ndeep reinforcement learning. arXiv \npreprint arXiv:1812.11103, 2018. \n37. \nZhu, H., et al., The Ingredients of Real-\nWorld Robotic Reinforcement Learning. \narXiv preprint arXiv:2004.12570, 2020. \n38. \nTuran, M., et al., Learning to Navigate \nEndoscopic Capsule Robots. IEEE \nRobotics and Automation Letters, 2019. \n4(3): p. 3075-3082. \n39. \nZeng, A., et al., TossingBot: Learning to \nThrow Arbitrary Objects With Residual \nPhysics. IEEE Transactions on Robotics, \n2020. 36(4): p. 1307-1319. \n40. \nTan, J., et al., Sim-to-real: Learning agile \nlocomotion for quadruped robots. arXiv \npreprint arXiv:1804.10332, 2018. \n41. \nSilver, D., et al., Mastering the game of \nGo without human knowledge. \n42. \nColabrese, S., et al., Flow navigation by \nsmart microswimmers via reinforcement \nlearning. Physical review letters, 2017. \n118(15): p. 158004. \n43. \nYang, Y., M.A. Bevan, and B. Li, \nEfficient Navigation of Colloidal Robots \nin an Unknown Environment via Deep \nReinforcement Learning. Advanced \nIntelligent Systems, 2020. 2(1): p. \n1900106. \n44. \nMuiños-Landin, S., et al., Reinforcement \nlearning with artificial microswimmers. \nScience Robotics, 2021. 6(52): p. \neabd9285. \n45. \nDemir, S.O., et al., Task space adaptation \nvia the learning of gait controllers of \nmagnetic soft millirobots. The \nInternational Journal of Robotics \nResearch, 2021: p. 02783649211021869. \n46. \nHaarnoja, T., et al. Soft actor-critic: Off-\npolicy maximum entropy deep \nreinforcement learning with a stochastic \nactor. 2018. PMLR. \n47. \nWang, X., et al., Dynamic Modeling of \nMagnetic Helical Microrobots. IEEE \nRobotics and Automation Letters, 2021: p. \n1-1. \n48. \nCharreyron, S.L., et al., Modeling \nElectromagnetic Navigation Systems. \nIEEE Transactions on Robotics, 2021: p. \n1-13. \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n18 \n49. \nZhang, J., et al., Reliable Grasping of \nThree-Dimensional Untethered Mobile \nMagnetic Microgripper for Autonomous \nPick-and-Place. IEEE Robotics and \nAutomation Letters, 2017. 2(2): p. 835-\n840. \n50. \nDenasi, A. and S. Misra. A robust \ncontroller for micro-sized agents: The \nprescribed performance approach. in \n2016 International Conference on \nManipulation, Automation and Robotics at \nSmall Scales (MARSS). 2016. \n51. \nPawashe, C., et al., Two-Dimensional \nAutonomous Microparticle Manipulation \nStrategies for Magnetic Microrobots in \nFluidic Environments. IEEE Transactions \non Robotics, 2012. 28(2): p. 467-477. \n52. \nHuang, H.-W., et al., Soft micromachines \nwith programmable motility and \nmorphology. Nature Communications, \n2016. 7(1): p. 12263. \n53. \nLi, H., et al., Magnetic actuated pH-\nresponsive hydrogel-based soft micro-\nrobot for targeted drug delivery. Smart \nMaterials and Structures, 2016. 25(2): p. \n027001. \n54. \nHwang, G., et al., Catalytic antimicrobial \nrobots for biofilm eradication. Science \nRobotics, 2019. 4(29): p. eaaw2388. \n55. \nIbarz, J., et al., How to train your robot \nwith deep reinforcement learning: lessons \nwe have learned. The International Journal \nof Robotics Research, 2021. 40(4-5): p. \n698-721. \n56. \nHaarnoja, T., et al., Soft actor-critic \nalgorithms and applications. arXiv \npreprint arXiv:1812.05905, 2018. \n57. \nYan, X., et al., Multifunctional biohybrid \nmagnetite microrobots for imaging-guided \ntherapy. Science Robotics, 2017. 2(12): p. \neaaq1155. \n58. \nSitti, M., et al., Biomedical Applications of \nUntethered Mobile Milli/Microrobots. \nProceedings of the IEEE, 2015. 103(2): p. \n205-224. \n59. \nAbbott, J.J., et al., How Should \nMicrorobots Swim? The International \nJournal of Robotics Research, 2009. \n28(11-12): p. 1434-1447. \n60. \nMedina-Sánchez, M., et al., Cellular \nCargo Delivery: Toward Assisted \nFertilization by Sperm-Carrying \nMicromotors. Nano Letters, 2016. 16(1): \np. 555-561. \n61. \nLee, S., et al., Microrobots: A Needle-\nType Microrobot for Targeted Drug \nDelivery by Affixing to a Microtissue \n(Adv. Healthcare Mater. 7/2020). \nAdvanced Healthcare Materials, 2020. \n9(7): p. 2070019. \n62. \nHu, C., S. Pané, and B.J. Nelson, Soft \nMicro- and Nanorobotics. Annual Review \nof Control, Robotics, and Autonomous \nSystems, 2018. 1(1): p. 53-75. \n63. \nMnih, V., et al., Human-level control \nthrough deep reinforcement learning. \nNature, 2015. 518(7540): p. 529-533. \n64. \nFu, J., et al. Diagnosing bottlenecks in \ndeep q-learning algorithms. PMLR. \n65. \nCaruana, R., S. Lawrence, and L. Giles, \nOverfitting in neural nets: \nBackpropagation, conjugate gradient, and \nearly stopping. Advances in neural \ninformation processing systems, 2001: p. \n402-408. \n66. \nKummer, M.P., et al., OctoMag: An \nElectromagnetic System for 5-DOF \nWireless Micromanipulation. IEEE \nTransactions on Robotics, 2010. 26(6): p. \n1006-1017. \n67. \nOngaro, F., et al., Design of an \nElectromagnetic Setup for Independent \nThree-Dimensional Control of Pairs of \nIdentical and Nonidentical Microrobots. \nIEEE Transactions on Robotics, 2019. \n35(1): p. 174-183. \n68. \nMahoney, A.W. and J.J. Abbott, Five-\ndegree-of-freedom manipulation of an \nuntethered magnetic device in fluid using \na single permanent magnet with \napplication in stomach capsule \nendoscopy. The International Journal of \nRobotics Research, 2015. 35(1-3): p. 129-\n147. \n69. \nArulkumaran, K., et al., Deep \nReinforcement Learning: A Brief Survey. \nIEEE Signal Processing Magazine, 2017. \n34(6): p. 26-38. \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n19 \nAcknowledgements \nFunding: \nNational Institutes of Health through the Director’s New Innovator Award, DP2-GM132934 (WCR) \nNational Science Foundation, DMR 1709238 (WCR)  \nAir Force Office of Scientific Research, FA9550-18-1-0262 (WCR) \nOffice of Naval Research, N00014-17-1-2306 (WCR) \nNational Institutes of Health Cellular Approaches to Tissue Engineering and Regeneration Training Program \ntraineeship T32- EB001026 (MRB)  \nWilliam Kepler Whiteford Faculty Fellowship from the Swanson School of Engineering at the University of Pittsburgh \n(WCR) \n \nAuthor Contributions: \nConceptualization: MRB and WCR \nMethodology: MRB and WCR  \nInvestigation: MRB \nVisualization: MRB \nFunding Acquisition: WCR and MRB \nWriting –original draft: MRB and WCR \nWriting – Review and Editing: MRB and WCR \n \nCompeting Interests: \nThe authors declare that they have no competing interests. \n \nAcknowledgments: \nWe gratefully thank Haley C. Fuller and Ting-Yen Wei for scientific discussions and manuscript suggestions. \n \nData and Materials Availability: \nAll data are available in the main text or the supplementary materials. The code used in this work is available at \nhttps://github.com/Synthetic-Automated-Systems/RUDER_MBOT_RL \n \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n20 \nSUPPLEMENTARY MATERIALS \n \nSmart Magnetic Microrobots Learn to Swim with Deep Reinforcement Learning \n \nMichael R. Behrens1 and Warren C. Ruder1,2,* \n \n1Department of Bioengineering \nUniversity of Pittsburgh, Pittsburgh, PA, USA \n \n2Department of Mechanical Engineering \nCarnegie Mellon University, Pittsburgh, PA, USA \n \n*Corresponding author: warrenr@pitt.edu \n \n \n \n \n \n \n \n \n \nContents \n \nSupplementary Figure 1: Highest performing policy parameters during training \nSupplementary Table 1: Hyperparameters \nSupplementary Movie 1: Training process time course \nSupplementary Movie 2: HAMR swimming around the track \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n21 \n \nSupplementary Figure S1. Selecting the highest performing policy parameters during training. During training \nof the three policies, the average return from the policy was continually monitored with a rolling average of the last \n100 episodes. Whenever the average return during the last 100 episodes was higher than at any point previously \nrecorded during training, the parameters (i.e., the weights and biases) of the actor network, 𝜋𝜋, were saved. For \nevaluating the performance of the policy after learning, the highest performing parameters saved during the training \nsession were used, rather than the policy parameters of the network at the end of the training session. This was done \nbecause we sometimes observed a decrease in the performance of the policy at the end of training.  \n \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n22 \nSupplementary Table 1: Hyperparameters \nState actor network architecture \n1. Input Shape: (7,3)  \n2. (1)→Dense layer with 256 neurons. activation: Relu \n3. (2)→Dropout (0.2) \n4. (3)→Dense layer with 256 neurons. Activation: Relu \n5. (4)→Dropout(0.2) \n6. (5)→Output shape: (4)  \n \nState critic network \narchitecture \n1. State Input Shape: (7,3) \n2. (1)→Dense layer with 16 neurons, activation: Relu \n3. Action Input Shape: (4) \n4. (3)→Dense layer with 16 neurons, activation: Relu \n5. (2,4)→Concatenate Sate and Action Input  \n6. (5)→Dense layer with 256 neurons. activation: Relu \n7. (6)→Dropout (0.2) \n8. (7)→Dense layer with 256 neurons. Activation: Relu \n9. (8)→Dropout(0.2) \n10. (9)→Output shape: (1)  \n \nConvolutional actor \nnetwork architecture \n1. Input Shape: (64,64,3) \n2. (1)→2D convolutional layer, 16 filters, 3x3 kernel, \nactivation: Relu \n3. (2)→Max pooling(2,2) \n4. (3)→2D convolutional layer, 32 filters, 3x3 kernel, \nactivation: Relu \n5. (4)→Max pooling(2,2) \n6. (5)→2D convolutional layer, 64 filters, 3x3 kernel, \nactivation: Relu \n7. (6)→Max pooling(2,2) \n8. (7)→Flatten \n9. (8)→Dense layer with 64 neurons. activation: Relu \n10. (9)→Dense layer with 256 neurons. activation: Relu \n11. (10)→Dropout (0.2) \n12. (11)→Dense layer with 256 neurons. Activation: Relu \n13. (12)→Dropout(0.2) \n14. (13)→Output shape: (4)  \n \nConvolutional critic \nnetwork architecture \n1. State Input Shape: (64,64,3) \n2. (1)→2D convolutional layer, 16 filters, 3x3 kernel, \nactivation: Relu \n3. (2)→Max pooling(2,2) \n4. (3)→2D convolutional layer, 32 filters, 3x3 kernel, \nactivation: Relu \n5. (4)→Max pooling(2,2) \n6. (5)→2D convolutional layer, 64 filters, 3x3 kernel, \nactivation: Relu \n7. (6)→Max pooling(2,2) \n8. (7)→Flatten \n9. (8)→Dense layer with 64 neurons. activation: Relu \n10. Action Input Shape: (4) \n11. (10)→Dense layer with 16 neurons, activation: Relu \n12. (9,11)→Concatenate Sate and Action Inputs \n13. (12)→Dense layer with 256 neurons. activation: Relu \n14. (13)→Dropout (0.2) \n15. (14)→Dense layer with 256 neurons. Activation: Relu \n16. (15)→Dropout(0.2) \n17. (16) →Output Shape: (1) \n \nBehrens, Ruder.     University of Pittsburgh.     Preprint. \n \n23 \nExperience replay buffer size: \n100,000 \nBatch Size \n256 \nLearning Rate \n0.0003 \nGamma \n0.99 \nTau \n0.005 \nNumber of Actions \n4 \nTarget Entropy \n-4 (negative dim of number of actions) \nPolicy Update Period \n1 minute \nTotal Step Duration \n0.9 Seconds (three concatenated observations spaced 0.3 \nseconds apart) \n \nGoal Distance \n20 Degrees \nEpisode Length \n33 Steps \nGradient update steps per \nenvironmental step: \n1 \n \n \nSupplementary movies and code are available on Github at: \n https://github.com/Synthetic-Automated-Systems/RUDER_MBOT_RL \n \nSupplementary Movie S1: Training process time course. This movie shows an accelerated time course of the \nHAMR’s movements during training. The agent starts out randomly exploring actions, and as training progresses the \nHAMR begins to move continuously in a clockwise direction around the arena. \nSupplementary Movie S2: HAMR swimming around the track. This movie shows the swimming action of the \nHAMR in real time, for one complete trip around the circular track during evaluation of state-based policy No. 1. \n \n",
  "categories": [
    "cs.RO",
    "cs.AI",
    "cs.SY",
    "eess.SY",
    "I.2.9"
  ],
  "published": "2022-01-14",
  "updated": "2022-01-14"
}