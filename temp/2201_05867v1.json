{
  "id": "http://arxiv.org/abs/2201.05867v1",
  "title": "Transferability in Deep Learning: A Survey",
  "authors": [
    "Junguang Jiang",
    "Yang Shu",
    "Jianmin Wang",
    "Mingsheng Long"
  ],
  "abstract": "The success of deep learning algorithms generally depends on large-scale\ndata, while humans appear to have inherent ability of knowledge transfer, by\nrecognizing and applying relevant knowledge from previous learning experiences\nwhen encountering and solving unseen tasks. Such an ability to acquire and\nreuse knowledge is known as transferability in deep learning. It has formed the\nlong-term quest towards making deep learning as data-efficient as human\nlearning, and has been motivating fruitful design of more powerful deep\nlearning algorithms. We present this survey to connect different isolated areas\nin deep learning with their relation to transferability, and to provide a\nunified and complete view to investigating transferability through the whole\nlifecycle of deep learning. The survey elaborates the fundamental goals and\nchallenges in parallel with the core principles and methods, covering recent\ncornerstones in deep architectures, pre-training, task adaptation and domain\nadaptation. This highlights unanswered questions on the appropriate objectives\nfor learning transferable knowledge and for adapting the knowledge to new tasks\nand domains, avoiding catastrophic forgetting and negative transfer. Finally,\nwe implement a benchmark and an open-source library, enabling a fair evaluation\nof deep learning methods in terms of transferability.",
  "text": "Transferability in Deep Learning: A Survey\nTransferability in Deep Learning: A Survey\nJunguang Jiang\njiangjunguang1123@outlook.com\nSchool of Software, BNRist, Tsinghua University\nBeijing 100084, China\nYang Shu ∗\nshu-y18@mails.tsinghua.edu.cn\nSchool of Software, BNRist, Tsinghua University\nBeijing 100084, China\nJianmin Wang\njimwang@tsinghua.edu.cn\nSchool of Software, BNRist, Tsinghua University\nBeijing 100084, China\nMingsheng Long †\nmingsheng@tsinghua.edu.cn\nSchool of Software, BNRist, Tsinghua University\nBeijing 100084, China\nEditor: Leslie Pack Kaelbling\nAbstract\nThe success of deep learning algorithms generally depends on large-scale data, while humans\nappear to have inherent ability of knowledge transfer, by recognizing and applying relevant\nknowledge from previous learning experiences when encountering and solving unseen tasks.\nSuch an ability to acquire and reuse knowledge is known as transferability in deep learning.\nIt has formed the long-term quest towards making deep learning as data-eﬃcient as human\nlearning, and has been motivating fruitful design of more powerful deep learning algorithms.\nWe present this survey to connect diﬀerent isolated areas in deep learning with their relation\nto transferability, and to provide a uniﬁed and complete view to investigating transferability\nthrough the whole lifecycle of deep learning. The survey elaborates the fundamental goals\nand challenges in parallel with the core principles and methods, covering recent cornerstones\nin deep architectures, pre-training, task adaptation and domain adaptation. This highlights\nunanswered questions on the appropriate objectives for learning transferable knowledge and\nfor adapting the knowledge to new tasks and domains, avoiding catastrophic forgetting and\nnegative transfer. Finally, we implement a benchmark and an open-source library, enabling\na fair evaluation of deep learning methods in terms of transferability.\nKeywords:\nDeep learning, transferability, pre-training, adaptation, library, benchmark\n∗. Equal contribution\n†. Correspondence to: Mingsheng Long <mingsheng@tsinghua.edu.cn>.\n©2000 Marina Meil˘a and Michael I. Jordan.\narXiv:2201.05867v1  [cs.LG]  15 Jan 2022\nJiang et al.\nContents\n1\nIntroduction\n3\n1.1\nTerminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nOverview\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2\nPre-Training\n7\n2.1\nPre-Training Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2\nSupervised Pre-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2.1\nMeta-Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2.2\nCausal Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.3\nUnsupervised Pre-Training\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.3.1\nGenerative Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.3.2\nContrastive Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.4\nRemarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n3\nAdaptation\n21\n3.1\nTask Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3.1.1\nCatastrophic Forgetting . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.1.2\nNegative Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.1.3\nParameter Eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n3.1.4\nData Eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n3.1.5\nRemarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.2\nDomain Adaptation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n3.2.1\nStatistics Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n3.2.2\nDomain Adversarial Learning . . . . . . . . . . . . . . . . . . . . . .\n36\n3.2.3\nHypothesis Adversarial Learning . . . . . . . . . . . . . . . . . . . .\n39\n3.2.4\nDomain Translation\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n3.2.5\nSemi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . .\n42\n3.2.6\nRemarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n4\nEvaluation\n45\n4.1\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n4.2\nLibrary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n4.3\nBenchmark\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n4.3.1\nPre-Training\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n4.3.2\nTask Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n4.3.3\nDomain Adaptation\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n5\nConclusion\n50\n2\nTransferability in Deep Learning: A Survey\n1. Introduction\nDeep learning (LeCun et al., 2015) is a class of machine learning algorithms that utilize\nmultiple processing layers to learn representations of data with multiple levels of abstraction.\nThese multiple processing layers, also called deep neural networks (DNNs), are empowered\nwith the ability to discover diﬀerent explanatory factors of variation behind the intricate\nstructured data (Bengio et al., 2013). With essential advances in network architectures,\ntraining strategies and computation devices, deep learning has made breakthroughs or even\nrevolutions in various areas, such as computer vision (Krizhevsky et al., 2012; He et al.,\n2016), natural language processing (Radford et al., 2018), speech processing (Amodei et al.,\n2016), computational biology (Senior et al., 2020), games (Silver et al., 2016; Vinyals et al.,\n2019) and so forth. Despite its great success in these important areas, deep learning is still\nfaced with the grand challenge of data eﬃciency. Most mainstream deep learning methods\nrequire big datasets in the order of millions or even trillions to achieve good performance,\nyet collecting and annotating such huge amount of data for each new task or domain are\nexpensive and even prohibitive. This data eﬃciency challenge heavily impedes the adoption\nof deep learning to a wider spectrum of application scenarios.\nAn eﬀective solution to this challenge is to explore the transferability in deep learning.\nTransferability is a foundational ability of human learning: human beings can gain relevant\nknowledge from other related problems and apply it to handle new problems with extremely\nfew samples (Thrun and Pratt, 1998). In deep learning, transferability refers to the ability\nof deep neural networks to extract transferable representations from some source tasks and\nthen adapt the gained representations to improve learning in related target tasks (Bengio,\n2012). Recent advances in deep learning reveal that deep models trained via upstream tasks\non large-scale data tend to yield good transferability to a variety of downstream tasks, such\nas visual object detection (Ren et al., 2015), natural language understanding (Devlin et al.,\n2019), to name a few. Transferability has become the central property of deep learning for\nimproving data eﬃciency. It is on par with generalizability, interpretability, and robustness\nfor bridging the gap between machine learning and human learning.\nPre-Trained \nModel\nLabeled / \nUnlabeled\nPre-Training\nUpstream Task\nAdapted \nModel\nAdaptation\nDownstream Task\nTarget Domain\nSource Domain\nUpstream\nData\nDownstream\nData\nFigure 1: The two-stage lifecycle of most deep learning applications. In the ﬁrst stage, the\ndeep model is pre-trained on an upstream task with large-scale data (labeled or unlabeled)\nfor gaining transferable knowledge. In the second stage, the pre-trained model is adapted\nto a downstream task in the target domain with labeled data; If the downstream task only\nhas unlabeled data, then additional labeled data from another source domain of identical\nlearning task but diﬀerent data distribution will be used to improve performance.\n3\nJiang et al.\nTowards gaining and applying knowledge with good transferability, the lifecycle of many\ndeep learning applications is divided into two stages: pre-training and adaptation (Figure 1).\nThe goal of the pre-training stage is to gain the transferable knowledge. The deep models are\npre-trained on an upstream task with large-scale data (either labeled or unlabeled) to learn\ndisentangled representations or reusable parameters that are transferable to a variety of\ndownstream tasks. The goal of the adaptation stage is to reuse the transferable knowledge.\nThe pre-trained models are adapted to a downstream task in the target domain with labeled\ndata, and the previously learned knowledge enables better generalization with fewer labeled\nsamples. When the downstream task only has unlabeled data, additional labeled data from\nanother source domain of identical learning task but diﬀerent data distribution will be used\nto improve the data eﬃciency of the adapted model (Ganin and Lempitsky, 2015).\nIt is helpful to highlight the diﬀerence underlying the transferability in the two stages.\nThe pre-training stage focuses mainly on the generic transferability, i.e., obtaining a general\ntransferable representation that can improve the performance of as many downstream tasks\nas possible. In contrast, the adaptation stage pays attention to the speciﬁc transferability,\ni.e., how to exploit the transferable knowledge in pre-trained models for a speciﬁc kind of\ndownstream tasks, or how to improve the transferability between related domains of the\nsame downstream task. The generic transferability is attractive since it may beneﬁt many\ndownstream tasks without additional cost or special design. Yet it may ignore the special\nstructures of downstream tasks that are crucial for stronger transferability, thus the speciﬁc\ntransferability is still necessary in many cases. Recently, the gap between the pre-training\nstage and the adaptation stage is getting closer. Several pre-training methods are designed\nto obtain fast model adaptation ability in the adaptation stage (Finn et al., 2017), while\nsome adaptation methods try to convert downstream tasks into pre-training tasks to make\nfull use of the generic transferability of pre-trained models (Brown et al., 2020).\nTransferability lies at the core of the whole lifecycle of deep learning, yet diﬀerent areas\nsuch as domain adaptation (Zhuang et al., 2021) and continual learning (Delange et al.,\n2021), mainly explore transferability in a partial regime of the lifecycle. This is not enough\nto achieve a complete picture of transferability. Thereby, we present this survey to connect\ndiﬀerent isolated areas in deep learning with their relation to transferability, and to provide\na uniﬁed and complete view to investigate transferability through the whole lifecycle of deep\nlearning. Due to the broadness of the scope and the limitation of the space, we do not aim\nto cover all methods towards transferability. Instead, we elaborate on the core principles\nand methods and then give a brief review of the expanded literature. We further implement\nTLlib, a high-quality open library to provide a fair evaluation of typical methods. We hope\nthis survey can highlight the grand picture of transferability in deep learning, and provide a\nuseful navigation to researchers interested in improving the data eﬃciency of deep learning.\n1.1 Terminology\nForemost, we give several deﬁnitions related to transferability, and the summary of notations\nand their descriptions used in this survey can be found in Table 1. Denote the input space\nas X and the output space as Y, and assume that there exists an unknown labeling function\nf : X 7→Y. Formally, a task corresponds to learning an underlying labeling function f. To\nlearn a task, we ﬁrst collect a set of samples bD = {x1, ..., xn}, which are drawn independently\n4\nTransferability in Deep Learning: A Survey\nTable 1: Notations and descriptions used in the survey.\nX\nInput space\nY\nOutput space\nD\nA ﬁxed but unknown distribution over X\nbD\nEmpirical distribution of a sample drawn i.i.d. from D\nP(·)\nProbability of an event\nE(·)\nExpectation of a random variable\nU\nUpstream data\nS\nSource domain in downstream data\nT\nTarget domain in downstream data\nH\nHypothesis space\nh\nA hypothesis in the hypothesis space H\nψ\nFeature generator\nθ\nHypothesis parameter\nx\nModel input\ny\nModel output\nz\nHidden activation of the feature generator\nD\nA discriminator to distinguish diﬀerent distributions\nand identically distributed (i.i.d.) from some ﬁxed but unknown distribution D. Formally,\na domain is a marginal probability distribution P(X) deﬁned on a certain input space X.\nConsider a set of hypotheses H and a speciﬁc loss function ℓ: Y × Y 7→R+, the objective\nof the learner is to select a hypothesis h ∈H that yields the lowest generalization error,\nminh∈H Ex∼Dℓ(h(x), f(x)).\nDeﬁnition 1 (Transferability) Given a source domain S with learning task tS and a\ntarget domain T with learning task tT , transferability is the ability of gaining transferable\nknowledge from tS on S and reusing the knowledge to decrease the generalization error of\ntT on T , under the distribution shift S ̸= T or the task discrepancy tS ̸= tT .\nIn the deep learning lifecycle (Figure 1), the pre-training stage aims to gain transferable\nknowledge via learning on upstream task with large-scale data, while the adaptation stage\naims to reuse the pre-trained knowledge to improve the data eﬃciency in downstream tasks.\nThe upstream and downstream are diﬀerent in both learning tasks and data distributions.\nTo conform with the literature, in the pre-training stage, we will replace the notions of source\ndomain/task with the widely-used upstream data/task, denoted as U and tU respectively.\n1.2 Overview\nThe survey is organized around how to acquire and utilize the transferability in deep learning\nthroughout its whole lifecycle, including pre-training, adaptation, and evaluation (Figure 2).\n• Pre-Training. We ﬁrst brieﬂy discuss some important model architectures that make\npre-trained representations transferable. Then we elaborate on supervised pre-training\nand unsupervised pre-training, which are distinguished by the availability of labeled\nor unlabeled data for pre-training. In supervised pre-training, we cover both standard\npractices commonly used in the industry and research advances in academia to acquire\n5\nJiang et al.\nLifecycle\nArchitecture\nPre-Training\nSupervised\nPre-Training\nUnsupervised\nPre-Training\nStandard\nPre-Training\nMeta\nLearning\nCasual\nLearning\nGenerative\nLearning\nContrastive\nLearning\nAdaptation\nTask\nAdaptation\nDomain\nAdaptation\nCatastrophic\nForgetting\nNegative\nTransfer\nParameter\nEfficiency\nStatistics\nMatching\nDomain\nAdversarial\nHypothesis\nAdversarial\nDomain\nTranslation\nSemi-Supervised\nLearning\nEvaluation\nDatasets\nBenchmark\nData\nEfficiency\nCore\nMethod\nLearning\nSetup\nDomain\nGeneralization\nOOD\nGeneralization\nFew-shot\nLearning\nZero-shot\nLearning\nLibrary\nPrompt\nLearning\nFigure 2: Overview of this survey. The survey is organized around the lifecycle (pre-training,\nadaptation, and evaluation) of deep learning applications and focuses on the core problems\nand methods towards transferability. Besides, we brieﬂy review related learning setups.\ntransferability on the labeled data. In unsupervised pre-training, we cover the latest\ndesigns of proper pre-training tasks on unlabeled data to gain transferability.\n• Adaptation. We mainly elaborate on task adaptation and domain adaptation, which\nare divided by whether there exists another related source domain in addition to the\npre-trained model for boosting the downstream task performance. In task adaptation,\nwe ﬁrst pinpoint several open problems caused by the discrepancy between upstream\ntasks and downstream tasks, then illustrate how diﬀerent task adaptation paradigms\n(Yosinski et al., 2014; Brown et al., 2020) close the task discrepancy to better utilize\nthe transferability. In domain adaptation, we ﬁrst pinpoint the most inﬂuential theo-\nries for closing the distribution shift (Ben-David et al., 2006, 2010a), then elaborate\nhow to derive solid learning algorithms (Long et al., 2015; Ganin and Lempitsky, 2015)\nfrom these theories to enhance the transferability of deep models across domains.\n• Evaluation. We mainly investigate the transferability gained and reused by diﬀerent\npre-training and adaptation methods on several large-scale datasets released recently\nin the literature. Note that we omit some small-scale and relatively obsolete datasets\nto make our benchmark concise and easy to report. To facilitate fair evaluation and full\nreproduction of existing algorithms, we open source TLlib, a high-quality library along\nwith this survey at https://github.com/thuml/Transfer-Learning-Library.\nPre-training and adaptation lie at the core methods towards transferability. In parallel\nwith them, there are some ﬁelds that are also closely related to the transferability in deep\nlearning, such as domain generalization (Gulrajani and Lopez-Paz, 2021), out-of-distribution\n(OOD) generalization (Bengio et al., 2021), few-shot learning (Chen et al., 2019a), etc.\nRecent evaluation shows that these learning setups can largely beneﬁt from the advancement\nin pre-training and adaptation and we will give a brief review to them in the related sections.\n6\nTransferability in Deep Learning: A Survey\n2. Pre-Training\nDespite yielding unprecedented performances on various machine learning tasks, the deep\nlearning methods require large amounts of labeled data to generalize well. This data hungry\nnature limits their application to a wide variety of domains and tasks, especially to scenarios\nshort of data and annotations. Pre-training, which obtains transferable representations or\nmodels from upstream tasks with large-scale data to boost the performance on downstream\ntasks, is one of the most common and practical solutions to the problem of data scarcity. In\nthis section, we will ﬁrst review some important model architectures that have a great impact\non the transferability of pre-trained representations in Section 2.1. Then we elaborate on\nhow to gain knowledge of improved transferability via supervised pre-training on large-scale\nlabeled data in Section 2.2 and via unsupervised pre-training on much larger unlabeled data\nin Section 2.3. Figure 3 overviews the recent cornerstones of pre-training methods.\n2.1 Pre-Training Model\nPre-training has a big interplay with the model architecture. On the one hand, pre-training\ntechniques, such as greedy layerwise unsupervised pre-training (Bengio et al., 2007), have\neased the training of many deep architectures. On the other hand, as neural networks evolve\nfrom shallow to deep, they have a larger capacity to capture knowledge by pre-training from\nlarge-scale data, which increases their transferability to downstream tasks.\nModel architecture has a great inﬂuence on the transferability of knowledge obtained via\npre-training. Kornblith et al. (2019) ﬁnd that the performance of the pre-trained models on\nthe downstream tasks is highly correlated with the accuracy on the pre-training tasks, which\nsuggests that improving the performance on the pre-training task serves as a direct way for\nimproving transferability. The depth of the architecture, or more precisely, the capacity of\n2015\n2017\n2016\n2018\n2019\n2020\n2021\nBatchNorm\nIRM\nGPT\nTransformer\nBERT\nPre-Training\nModel\nSupervised\nPre-Training\nUnsupervised\nPre-Training\nResNet\nViT\nBiT\nMANN\nMAML\nRIM\nGPT3\nMAE\nCPC\nMoCo\nSSP\nInstDisc\nSimCLR\nDeep InfoMax\nRoBERTA\nT5\nCLIP\nLayerNorm\nGroupNorm\nImageNet\nDAT\nWSP\nSIN\nMeta Transfer\nXLNet\nXLM\nALBERT\nBART\nSimSiam\nFigure 3: Cornerstones of pre-training methods for gaining knowledge of transferability.\n7\nJiang et al.\nthe model, is deemed the most critical factor to its transferability. However, training very\ndeep neural networks have remained a grand diﬃculty for decades. He et al. (2016) observe\na degradation of training accuracy by increasing the network depth, which implies that\ndeeper models are more diﬃcult to optimize. Instead of ﬁtting a desired mapping h(x) by\na few stacked layers, they proposed Residual Network (ResNet) to explicitly ﬁt a residual\nmapping δ(x) := h(x) −x and then recast the original mapping into δ(x) + x. As a result,\nResNet improves feature and gradient ﬂows and enables end-to-end training of hundreds of\nand even thousands of layers, allowing the capacity of pre-trained models to scale up easily.\nIoﬀe and Szegedy (2015) hypothesize that the optimization diﬃculty also comes from the\ninternal covariate shift caused by layerwise transformation. To stabilize training very deep\nmodels, they proposed Batch Normalization (BatchNorm) (Ioﬀe and Szegedy, 2015), which\nperforms normalization for each training mini-batch within the architecture. This design is\nextensively used by ResNet. Kolesnikov et al. (2020) ﬁnd that BatchNorm is suboptimal for\ntransfer due to the requirement of distribution-dependent moving averaged statistics. They\nproposed Big Transfer (BiT) to replace BatchNorm by GroupNorm (Wu and He, 2018),\nwhich generates pre-trained models of strong performance on downstream tasks.\nThe pre-training paradigm also reshapes the design of model architectures. In classic\nsupervised learning, models usually have strong inductive bias such as the local connectivity\nassumption in Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN).\nA strong inductive bias makes pre-training of deep models more data-eﬃcient and generalize\nbetter when training data is scarce, yet on the other hand, it also limits the expressiveness\nand transferability of the deep models when there is large-scale data for pre-training. Thus,\nTransformer (Vaswani et al., 2017) removes the local connectivity assumption and models\nthe global dependencies between every two tokens. The connection weights are dynamically\ncomputed by the self-attention mechanism and then the feature aggregation in Transformer\ndepends on these attentions calculated from the input sequence, while the token positions in\nthe sequence are encoded by positional embedding. Transformers are powerful for sequence\nmodeling in natural language processing, and Vision Transformer (ViT) (Dosovitskiy et al.,\n2021) extends them to computer vision. ViT splits an image into ﬁxed-size patches, linearly\nembeds each of them, adds positional embeddings, and feeds the resulting sequence of vec-\ntors to a standard Transformer encoder. In summary, Transformer makes least assumptions\non the structural information of data, which makes Transformer an expressive architecture\nfor storing the transferable knowledge extracted by pre-training on large amounts of training\ndata (Devlin et al., 2019; Radford et al., 2018).\nAllowed Hypotheses\nAll Hypotheses\nInductive Learning\nInductive Transfer\nAll Hypotheses\nAllowed Hypotheses\nSearch\nSearch\nFigure 4: Designed inductive bias (left) and learned inductive bias from pre-training (right).\n8\nTransferability in Deep Learning: A Survey\nIn some sense, pre-training provides a learned inductive bias for the downstream tasks\n(Torrey and Shavlik, 2010). Many downstream tasks only have hundreds or thousands of\nlabeled samples, yet the pre-trained Transformers with hundreds of millions of parameters\ncan generalize well after ﬁne-tuning on such small data. To explain this phenomenon, Agha-\njanyan et al. (2021) empirically show that pre-training minimizes the intrinsic dimension\n(Li et al., 2018), which measures the number of parameters required to closely approximate\nthe optimization problem. Further, an intrinsic-dimension generalization bound is given,\nindicating that the pre-trained parameters implicitly aﬀect the inductive bias of models\nand a larger pre-trained model might correspond to a smaller allowed hypothesis space dur-\ning ﬁne-tuning (see Figure 4). The success of Transformer reveals that as the amount of\npre-training data increases, the learned inductive bias is able to outperform the manually\ndesigned inductive bias in terms of transferability.\n2.2 Supervised Pre-Training\nSupervised pre-training aims to obtain models on large-scale labeled data and then transfers\nthese models to boost downstream tasks (see Figure 5). Supervised pre-training is commonly\nemployed in computer vision, where image classiﬁcation on ImageNet (Deng et al., 2009;\nRussakovsky et al., 2015) is often used as the pre-training task. The pre-trained models can\nbe transferred to downstream tasks by reusing the representations from the feature generator\n(Sermanet et al., 2013). Donahue et al. (2014) ﬁnd that the generic visual representations\npre-trained on ImageNet outperforms many conventional feature descriptors on various\nobject recognition tasks. Yosinski et al. (2014) ﬁnd that transferring the pre-trained models\nby ﬁne-tuning the whole models yields better generalization performance on new tasks.\ngenerator\nhead\nlabeling\nfunction\n!\n\"\n#\n$#\n%!\"#\nFigure 5: Standard supervised pre-training. The model is composed of a feature generator\nand a task-speciﬁc head. The goal is to obtain a feature generator capturing transferable\nknowledge from large-scale labeled data. After pre-training, the feature generator is adapted\nto downstream tasks, while the task-speciﬁc head is usually discarded.\nAmong the factors that inﬂuence the transferability of pre-trained models, the quantity\nand quality of the pre-training data might be the most important. BiT (Kolesnikov et al.,\n2020) emphasizes that training on larger datasets is vital for better transferability. Yet the\ndata labeling is labor-exhaustive and time-consuming, which limits the possible size of the\nannotation data. To break this limitation, Mahajan et al. (2018) explore Weakly Supervised\nPre-training (WSP) on IG-1B-Targeted, a dataset of billions of images with social media\nhashtags. Yalniz et al. (2019) further explore web-scale Semi-Supervised Pre-training (SSP)\non YFCC100M, a dataset of billions of unlabeled images along with a relatively smaller set\nof task-speciﬁc labeled data. These methods improve clearly against the counterpart trained\n9\nJiang et al.\nwith only clean labeled data and achieve stronger transfer performance. On the other hand,\nDomain Adaptive Transfer (DAT) (Ngiam et al., 2018) studies the inﬂuence of data quality\nand ﬁnds that using more data does not necessarily lead to better transferability, especially\nwhen the dataset is extremely large. Thus, an importance weighting strategy is proposed\nto carefully choose the pre-training data that are most relevant to the target task. Cui et al.\n(2018) also ﬁnd that pre-training on more similar upstream data improves transferability to\nﬁne-grained downstream tasks. They propose to estimate domain similarity via the Earth\nMover’s Distance to choose proper pre-training data. Geirhos et al. (2019) ﬁnd that models\ntrained supervisedly on ImageNet are biased towards textures in images, and propose to\npre-train with a Stylized ImageNet (SIN), which ﬁxes the texture bias and encourages the\nmodels to learn shape-based representations of better transferability.\nWhile standard supervised pre-training is powerful when there are enough labeled data,\nit still has drawbacks that may limit the transferability of the model. For instance, standard\nsupervised pre-trained models are vulnerable to adversarial examples (Goodfellow et al.,\n2015), and Salman et al. (2020) enhance the adversarial robustness of the pre-trained models\nto achieve better transferability. In addition, there are alternative pre-training methods for\nimproving the transferability of deep models. Section 2.2.1 will elaborate on meta-learning,\nwhich aims to obtain pre-trained models that adapt to downstream tasks with less training\ntime and less training data. Section 2.2.2 will review causal learning, which aims to obtain\ndistributionally robust and generalizable pre-trained models.\n2.2.1 Meta-Learning\nStandard supervised pre-training gains transferable representations to boost the learning of\nnew tasks. However, it still requires to ﬁne-tune the pre-trained models with hundreds or\nthousands of labeled data and with many gradient updates when adapting to the new task.\nIn contrast, people have the ability to quickly adapt to diﬀerent related new tasks with few\nlabeled data. Meta-learning, also known as learning to learn (Schmidhuber, 1987), aims to\npursue such kind of eﬃcient transferability in the pre-training stage.\nThe core idea of meta-learning is to equip the model with some meta knowledge φ that\ncaptures intrinsic properties of diﬀerent learning tasks, which is called meta-training. When\nfacing a new task, the learned meta knowledge could help the target model θ adapt to the\ntask faster, which is called meta-testing. Meta-learning is based on a simple machine learning\nprinciple that test and training conditions should be matched. As shown in Figure 6(a),\nto simulate the fast adaptation condition during meta-testing, the meta-training data is\nconstructed into a collection of n learning tasks, and each task i ∈[n] contains a training\nset Dtr\ni for adaptation to this task and a test set Dts\ni for evaluation1. As shown in Figure 6(b),\nthe learning objective of meta-training is a bi-level optimization problem,\nφ∗= arg max\nφ\nn\nX\ni=1\nlog P(θi(φ)|Dts\ni ),\nwhere θi(φ) = arg max\nθ\nlog P(θ|Dtr\ni , φ).\n(1)\nHere the inner level optimization updates the model θ with the training set Dtr\ni using meta\nknowledge φ, and the outer level optimization evaluates the updated model with the test\n1. Dts is a surrogate test set used during meta-training to simulate diﬀerent tasks and improve the model.\nIt is diﬀerent from the true test set in the general setting in machine learning.\n10\nTransferability in Deep Learning: A Survey\ntraining set\ntest set\nmeta-\ntraining\n…\n…\nmeta-\ntest\n…\n…\n(a) Learning Setup\ntest set\ntraining set\nmeta knowledge\n&$\n%&\n&$\n%!\n'\n(\n!%&\n#%&\n%'(()&\n$#%&\n#%!\n!%!\n%*\"%)&\n$#%!\n(b) Architecture\nFigure 6: Learning setup and architecture for meta-learning. (a) Meta-learning consists of\ntwo phases, meta-training and meta-testing. Meta-training gains meta knowledge φ from\ntraining tasks to help the model θ adapt quickly to a new task in meta-testing, where each\ntask consists of a training set and a test set. (b) In the inner level optimization, the model θ\nis updated with the training set Dtr\ni using meta knowledge φ. In the outer level optimization,\nthe updated model is evaluated on the test set Dts\ni to ﬁnd better meta knowledge φ.\nset Dts\ni to ﬁnd better meta knowledge of stronger transferability. The key to enhancing the\ntransferability of meta-learning methods is to design a proper form of meta knowledge.\nMemory-Based Meta-Learning considers memory mechanisms as the meta knowl-\nedge. A controller writes knowledge extracted from training data Dtr\ni into the memory, and\nreads from the memory to adapt the base learner θ to make predictions on test data Dts\ni . The\nparameter of the controller is updated to ﬁnd transferable knowledge. Memory-Augmented\nNeural Network (MANN) (Santoro et al., 2016) stores bound sample representation-class la-\nbel information in the external memory, which can then be retrieved as features for making\npredictions when a sample from the same class is presented. Meta Network (Munkhdalai\nand Yu, 2017) designs another memory mechanism where a base learner provides informa-\ntion about the status of the current task while the meta learner interacts with the external\nmemory to generate parameters for the base learner to quickly learn the new task. Memory-\nbased meta-learning methods improve transferability in various downstream tasks, such as\nfew-shot classiﬁcation and reinforcement learning. However, they require a careful design of\nthe black-box architecture to incorporate the memory mechanism, and it is unclearer what\nis stored and retrieved in the memory and why it helps adapt the model.\nOptimization-Based Meta-Learning considers a good initialization of the model\nas the meta knowledge. The motivation of Model-Agnostic Meta-Learning (MAML) (Finn\net al., 2017) is to explicitly seek for an initialization that is most transferable for ﬁne-tuning,\ni.e., only a small amount of gradient steps and a few labeled data are needed for the model\nto generalize to a new task. To learn such an initialization, for each sampled task i ∈[n],\nthe model φ is ﬁrst updated on its training data Dtr\ni using one gradient step of size α,\nθi = φ −α∇φL(φ, Dtr\ni ).\n(2)\nwhich mimics the situation of ﬁne-tuning the model from the starting point of φ. As meta\nknowledge, φ should have good transferablity, such that for all tasks i ∈[n], the ﬁne-tuned\n11\nJiang et al.\nparameters θi could perform well on the test set Dts\ni ,\nmin\nφ\nn\nX\ni=1\nL(θi(φ), Dts\ni ) =\nn\nX\ni=1\nL(φ −α∇φL(φ, Dtr\ni ), Dts\ni ).\n(3)\nThe meta knowledge of MAML is high-dimensional, hindering MAML from deeper models.\nTo tackle it, Meta Transfer (Sun et al., 2019a) uses standard pre-training for initialization\nand performs meta-training with light-weight neuron operations (e.g. scaling and shifting\nover tasks), which reduces the training tasks needed to acquire the meta knowledge. Raghu\net al. (2020) ﬁnd that feature reuse of the backbone is the predominant reason for eﬃcient\nlearning on downstream tasks with MAML. They thus propose the Almost No Inner Loop\nalgorithm, which performs inner loop updates and task adaptation only on the task-speciﬁc\nhead layer. Another limitation of MAML is that the ﬁxed meta knowledge is globally shared\nby all tasks. To break this, Latent Embedding Optimization (Rusu et al., 2019) performs\ngradient-based meta-learning in a low-dimensional latent space, and learns data-dependent\nlatent embedding as meta knowledge to generate target model parameters. Yao et al. (2019)\nperform Hierarchically Structured Meta-Learning over hierarchical tasks based on clustering\nstructures and learns to tailor transferable meta knowledge to diﬀerent tasks.\nWhile meta-learning methods enable fast model adaptation across tasks, they are weak\nin transferring to data from diﬀerent domains, and some sophisticated methods even perform\nworse than standard pre-training baselines (Chen et al., 2019a). Thus, Omni-Training (Shu\net al., 2021a) incorporates both standard pre-training and meta-training in a framework\nwith a tri-ﬂow architecture to equip the pre-trained model with both domain transferability\nacross diﬀerent distributions and task transferability for fast adaptation across related tasks.\n2.2.2 Causal Learning\nIt remains diﬃcult for supervised pre-training to obtain a transferable representation that\ngeneralizes well to an out-of-distribution (OOD) domain (Bengio et al., 2021). In contrast,\nhumans have the ability to adapt to diﬀerent domains or new environments. Causal learning\naims to pursue such kind of extrapolated transferability in the pre-training stage.\nThe core idea of causal learning is to equip the model with some causal mechanisms\nthat capture independent and disentangled aspects of the complex real-world distributions.\nWhen the distribution changes, only one or several causal mechanisms change, with others\nremaining invariant, which could result in better out-of-distribution (OOD) generalization.\nThe causal mechanisms are described by Structural Causal Models. As shown in Figure 7,\ncausal mechanisms consider a set of variables as the vertices of a directed acyclic graph, and\neach edge represents a mechanism of direct causation in that the parents directly aﬀect the\nassignment of the child. This induces a canonical factorization of the joint distribution of\nthese variables into the disentangled distribution of them conditioned on their parents. The\nindependent causal mechanism principle states that given its mechanism, the conditional\ndistribution of each variable does not inform or inﬂuence the other mechanisms (Sch¨olkopf\net al., 2012; Peters et al., 2017). This implies that small distribution changes should only\naﬀect the causal mechanisms along with the disentangled factorization in a sparse and local\nway (Sch¨olkopf et al., 2021), thereby enabling transferability towards diﬀerent distributions.\nThe key problem of causal learning is to obtain the variables governed by independent causal\n12\nTransferability in Deep Learning: A Survey\nhead\ncausal relation\nvariable\ngenerator\naffected\nvariable\ndifferent\nenvironments\nhead\ncausal mechanisms\n!+\n&,\n&-\n!.\n$#+\n$#.\nFigure 7: Causal mechanisms consider a set of observations or variables as the vertices of\na directed acyclic graph, where each edge corresponds to a mechanism of direct causation.\nCausal learning seeks a model with variables governed by certain causal mechanisms, and if\nthe environment or distribution changes, only part of the causal mechanisms will be aﬀected.\nmechanisms. One way is to explicitly introduce independence with the modular models.\nAnother common practice is to leverage the invariance assumption that causal relationships\nremain invariant across distributions.\nModular Model. Recurrent Independent Mechanism (RIM) (Goyal et al., 2021) takes\na modular model composed of several modules of diﬀerent functions, where each module is a\nrecurrent cell such as LSTM or GRU (Cho et al., 2014) and represents a causal mechanism.\nTo obtain independence in distinct modules, RIM introduces attention between the hidden\nstates of each module and the current inputs. For speciﬁc inputs, only the most relevant\nmodules with larger attention are activated and updated, which forms competition between\ndiﬀerent modules and encourages their independence. RIM is shown to capture independent\ncausal mechanisms and generalize well over diﬀerent temporal patterns.\nInvariant Learning. The invariance assumption indicates that the conditional proba-\nbility of the target output given its direct cause should be invariant across all environments\nor distributions. Invariant Causal Prediction (ICP) (Peters et al., 2016) uncovers indepen-\ndent causal mechanisms by performing a statistical test to ﬁnd the subset of the variables\nsatisfying the invariance assumption. Invariant Risk Minimization (IRM) (Arjovsky et al.,\n2019) extends this idea to representation learning and learns a good representation such\nthat the conditional probability of the target output given the representation should be\ninvariant across training environments. Formally, given a data representation ψ : X →Z\nand training environments Etr, the conditional probability between the representation and\nthe output is invariant if there is a classiﬁer h : Z →Y simultaneously optimal for all the\nenvironments. This can be formalized as the following constrained optimization problem,\nmin\nψ:X→Z,h:Z→Y\nX\ne∈Etr\nϵe(h ◦ψ),\nsubject to h ∈arg min\n¯h:Z→Y\nϵe(¯h ◦ψ), for all e ∈Etr,\n(4)\nwhere ϵe(h◦ψ) refers to the expected error of the predictor h◦ψ on the environment e. The\ntransferability across environments relies on how the invariance across training environments\nimplies invariance across all environments. Thus, the diversity of training environments is\nimportant for gaining transferability. IRM can be extended to complex situations where the\ncausal relations are deﬁned on some latent variables that need to be extracted from data.\n13\nJiang et al.\n2.3 Unsupervised Pre-Training\nBeing a canonical successful approach, supervised pre-training still requires a large amount\nof labeled data which are expensive to annotate and only available in certain ﬁelds. This\nhinders pre-training on huge-scale data and limits its transferability to particular tasks.\nTo break this shackle, unsupervised learning (Bengio, 2012), typically in the form of self-\nsupervised learning, is used for pre-training on very large unlabeled data to acquire generally\ntransferable knowledge. To improve the transferability on downstream tasks, it is crucial to\ndesign a proper self-supervised task for pre-training. According to the type of task, we can\ndivide common unsupervised pre-training methods into generative learning and contrastive\nlearning, which will be discussed in Sections 2.3.1 and 2.3.2 respectively.\n2.3.1 Generative Learning\nGenerative learning is underpinned by the idea of learning to generate data distribution\nP(X) for unsupervised pre-training. It aims to learn the intrinsic representation in data\nand has been commonly used for pre-training deep neural networks (Bengio et al., 2007). As\nshown in Figure 8, we employ an encoder fθ that maps the perturbed input ˜x into a latent\nrepresentation z = fθ(˜x) and a decoder gθ that maps the representation back to derive a\nreconstructed version of the input bx = gθ(z). The model is then optimized by minimizing\nthe reconstruction error Lgen(bx, x). Most generative pre-training methods are based on two\nmodels: Autoregressive Model, which generates future inputs given only past inputs, and\nAutoencoding Model, which generates full inputs given partial inputs.\n \nencoder\ndecoder\nperturbation\n!\n)!\n\"\n$!\n%/)(\nFigure 8: Generative pre-training tries to reconstruct the original input x from a perturbed\ninput ˜x. The generative learning task shall encourage the learned representation z to capture\nthe intrinsic and transferable explanatory factors from the data.\nAutoregressive Model approximates the distribution of a sequence by predicting each\nentry conditioned on its previous context, which is called Language Modeling (LM) task\nin NLP. As shown in Figure 9, given a text sequence x1:T = [x1, x2, ..., xT ], the learning\nobjective of LM is to maximize the conditional probability of each entry xt,\nmax\nθ\nT\nX\nt=1\nlog Pθ(xt|xt−k, · · · , xt−1),\n(5)\nwhere k is the size of the context window and θ is the parameter of the neural network.\nGenerative Pre-Training (GPT) (Radford et al., 2018) explores unsupervised pre-training\nof Transformer with LM on the BooksCorpus (Zhu et al., 2015) dataset with over 7000\n14\nTransferability in Deep Learning: A Survey\nunpublished books. This equips the model with great transferability to various NLP tasks,\nsuch as question answering, commonsense reasoning, and so on. The advantage of LM is\nthat it models the context dependency while the drawback is that it only encodes contextual\ninformation from one direction, yet contextual representations encoded in both directions\nmay be more suitable to many downstream tasks, such as natural language inference.\nAutoencoding Model approximates the data distribution by generating original data\nfrom encoded representations. Vincent et al. (2008) hypothesize that a good representation\nshould also be robust to partial corruption of the input. Thus Denoising Autoencoder (Vin-\ncent et al., 2008) is trained to reconstruct the original input x with the corrupted input\n˜x. Inspired from Denoising Autoencoder, BERT (Devlin et al., 2019) adopts the Masked\nLanguage Modeling (MLM) task as a pre-training task to overcome the drawback of the\nunidirectional LM. As shown in Figure 9, MLM ﬁrst randomly masks out some tokens\nm(x) from the input sentences x with a special [MASK] token and then trains the models\nto predict the masked tokens by the rest of the tokens x\\m(x),\nmax\nθ\nX\nx∈m(x)\nlog Pθ(x|x\\m(x)).\n(6)\nMasked pre-training has also been used in many other areas. For instance, Masked Au-\ntoencoders (MAE) (He et al., 2021) pre-trains vision transformers on large-scale unlabeled\nimage datasets using the image generation task. The diﬃculty is that the signals are highly\nredundant in images, thus it is hard for generative tasks, such as ﬁlling a few missing pixels,\nto capture high-level knowledge from data. To tackle this issue, MAE randomly masks a\nvery large portion of patches, forcing the model to go beyond low-level understanding and\nreconstruct the whole image based on a small subset of visible patches, which improves\nits transferability to semantic-level tasks. For another instance, to pre-train Graph Neu-\nral Network (GNN) (Garcia and Bruna, 2018) for transferable representations, Attribute\nMasking (Hu et al., 2020) conceals node or edge attributes and asks GNNs to predict those\nattributes based on neighboring structures, which can capture the regularities of attributes\ndistribution over diﬀerent graph structures, such as the chemistry rules in molecular graphs,\nand improve transferability on the downstream node or edge classiﬁcation tasks.\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\n(b) MLM\n(a) LM\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\n(c) PLM\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\n(d) Seq2Seq MLM\n*,\n$*-\n*-\n$*0\n*0\n$*1\n*,\n$*,\n[MASK]\n$*-\n*0\n$*0\n*-\n$*1\n*1\n$*0\n*0\n$*,\n*,\n$*-\n[MASK]\n$*0\n*0\n$+,\n+,\n$+-\nFigure 9: Attention visibility in Transformer (Trm) for language models. (a) LM maximizes\nthe probabilities of all words conditioned on their previous words. (b) MLM maximizes the\nprobabilities of random masked words conditioned on all unmasked words. (c) PLM per-\nmutes the original sequence and then performs autoregression. (d) Seq2Seq MLM encodes\nthe input masked sequence x and then decodes the output masked tokens y sequentially.\n15\nJiang et al.\nCombining Autoregressive and Autoencoding Models. In MLM, some special\ntokens, such as [MASK], are only used in pre-training while absent in the downstream tasks,\nleading to the mismatch between the pre-training phase and the ﬁne-tuning phase. To mit-\nigate this discrepancy, Permuted Language Modeling (PLM) (Yang et al., 2019) randomly\nsamples a permutation of the sequence and then performs autoregression on the permuted\nsequence to predict the last few tokens. To explore the limits of transferability of knowledge\ngained in diﬀerent generative pre-training methods, T5 (Raﬀel et al., 2020) uniﬁes all text-\nbased language tasks into the text-to-text format and then adopts a Sequence-to-Sequence\nMLM (Seq2Seq MLM), where the encoder processes a masked sequence and the decoder\nsequentially generates the masked tokens in an autoregression manner.\nThe design of unsupervised pre-training tasks has a great inﬂuence on the transferability\nto the downstream tasks, thus many eﬀorts have been made to optimize the pre-training\ntasks and exploit better training objectives.\nRoBERTa (Liu et al., 2019b) explores the\nunder-training issue of BERT and highlights that training with more data, longer sequences,\nand dynamically changed masking patterns helps the model transfer better. Besides, MLM\nrandomly masks out some independent words, which are the smallest semantic units in\nEnglish but may not have complete semantics in other languages, such as Chinese. Thus,\nERNIE (Baidu) (Sun et al., 2019b) introduces entity-level and phrase-level masking, where\nmultiple words that represent the same semantic meaning are masked. This achieves good\ntransferability on Chinese NLP tasks. To improve transferability to tasks where span selec-\ntion is important, such as question answering and coreference resolution, SpanBERT (Joshi\net al., 2020) masks a random variable length of span in the text and trains the span boundary\nrepresentations to predict the entire content of the masked span. BART (Lewis et al., 2020)\nintroduces more perturbation functions such as sentence permutation, document rotation,\ntoken deletion, and text inﬁlling for more transferable pre-trained models.\nThe generative pre-training on large-scale data greatly improves the transferability of\nmodels and even enables few-shot task transfer.\nBy scaling up the model size to 175B\nand pre-training on the corpus over 500GB, GPT-3 (Brown et al., 2020) obtains impressive\ntransferability. Using only task demonstrations and a few examples, GPT-3 achieves better\nperformance than prior state-of-the-art ﬁne-tuning approaches on some tasks. The success\nof GPT-3 comes from the fact that the web-scale corpus contains a vast amount of natu-\nral language sentences, which potentially demonstrate diﬀerent tasks without explicit task\nsymbols. A high-capacity language model trained on such data would perform unsupervised\nmulti-task learning and absorb transferable knowledge to handle downstream tasks. The\ngenerative pre-training on large-scale data also improves the transferability across domains.\nMultilingual BERT (Pires et al., 2019) is pre-trained with MLM on Wikipedia texts from\n104 languages and then achieves great cross-lingual transferability in the downstream tasks,\nwhere each language can be considered as a domain. Further, XLM (Lample and Conneau,\n2019) introduces the translation language modeling task, which extends MLM to parallel\nbilingual sentence pairs, encouraging more transferable representations across language.\n2.3.2 Contrastive Learning\nContrastive learning utilizes the idea of learning to compare for unsupervised pre-training.\nAs shown in Figure 10, two diﬀerent views, query xq and key xk, are constructed from the\n16\nTransferability in Deep Learning: A Survey\noriginal data x. Encoders will map diﬀerent views into latent representations and decoders\nwill further map the representation to the metric space. The model is learned by minimizing\nthe distance between query and key of the same instance. We will review three typical con-\ntrastive learning methods widely used in pre-training: Mutual Information Maximization\n(uses the global context and the local features as diﬀerent views), Relative Position Predic-\ntion (uses diﬀerent local components as diﬀerent views), and Instance Discrimination (uses\ndata augmentations to generate diﬀerent views of the same instance). Diﬀerent ways of\ngenerating and comparing diﬀerent views encourage these methods to respectively capture\nthe global-local relation, local-local relation and global-global relation of the training data.\n \nquery\nencoder\nview 1\nview 2\nquery\ndecoder\nkey\nencoder\nkey\ndecoder\n%2*(%\n!\n!3\n!4\n\"3\n\"4\n,\n-\nFigure 10: Contrastive pre-training aims to minimize the similarity between the query q\nand the key k that are generated from diﬀerent views of the same data input x.\nMutual Information Maximization.\nDeep InfoMax (Hjelm et al., 2019) aims to ac-\nquire transferable representations from the relation between the high-level global context\nand the low-level local features. Given input x, Deep InfoMax learns an encoder ψ to maxi-\nmize the mutual information between its input and output of the same instance. The mutual\ninformation can be estimated and bounded by training a discriminator to distinguish be-\ntween their joint distribution and the product of their marginals. Using Noise-Contrastive\nEstimation (NCE), the training objective of Deep InfoMax becomes,\nmax\nψ\nEx∼U\n\"\nD(x, ψ(x)) −Ex′∼eU\n\u0010\nlog\nX\nx′\neD(x′,ψ(x))\u0011#\n,\n(7)\nwhere x is the input sampled from the training distribution U of upstream task, x′ is an-\nother input sampled from eU = U, and D is the discriminator to distinguish between the joint\ndistribution and the product of marginals. A parallel work, Contrastive Predictive Coding\n(CPC) (Oord et al., 2019), also maximizes the mutual information between pairs of global\nrepresentation and local representation. Given a sequence input, CPC processes it with an\nencoder and summarizes the results into a context by an autoregression model. Then it\nmaximizes the mutual information between the summarized context and the hidden repre-\nsentation of the future observation in the sequence, which guides the learned representations\nto capture information for predicting future samples.\nMutual information maximization has been used to obtain pre-trained models on many\ndata formats, such as Deep InfoMax on image data and CPC on sequence data. On graph\ndata, Deep Graph Infomax (Veliˇckovi´c et al., 2019) maximizes the mutual information be-\ntween a node’s local representations and the k-hop neighborhoods’ context representations.\n17\nJiang et al.\nOn multimodal data, Contrastive Language-Image Pre-training (CLIP) (Radford et al.,\n2021) maximizes the mutual information between the image and the corresponding text\nin a multimodal embedding space. After training with a large-scale dataset of image-text\npairs from the Internet, it enables the zero-shot transfer of the model to downstream tasks,\ncompetitive with the prior task-speciﬁc supervised models.\nRelative Position Prediction.\nNext Sentence Prediction (NSP) (Devlin et al., 2019),\nwhich is ﬁrst introduced in BERT, acquires transferable representations from the relation\nbetween local parts. Speciﬁcally, NSP uses a binary classiﬁer to predict whether two sen-\ntences are coherent from the training corpus, aiming to enhance the transferability to tasks\nwith multiple sentences, such as question answering and natural language inference. How-\never, subsequent work questions the necessity of NSP tasks (Yang et al., 2019; Liu et al.,\n2019c) and Lan et al. (2020) conjecture that NSP only forces the model to learn topic pre-\ndiction, rather than more diﬃcult coherence prediction. Since inter-sentence coherence is\nimportant to many downstream tasks, ALBERT (Lan et al., 2020) introduces a sentence-\norder prediction task, where two consecutive segments from the same document are taken\nas positive examples, and the same segments with order swapped are taken as negative\nexamples. Similar ideas are also explored in vision, where the pre-training task is to predict\nrelative positions of two patches from an image (Doersch et al., 2015).\nInstance Discrimination.\nInstDisc (Wu et al., 2018) aims to learn transferable repre-\nsentations from the relation between instances. Given n instances, an encoder ψ is trained\nto distinguish each instance from others, i.e., minimize the distance between the query q\nand key k+ from the same instance (also called positive samples) and maximize the distance\nbetween that of diﬀerent instances (also called negative samples),\nmin\nψ −log\nexp(q · k+/τ)\nPK\nj=0 exp(q · kj/τ)\n,\n(8)\nwhere τ is a temperature hyper-parameter and the sum is over one positive and K negative\nsamples. Note that the computation of the features of all samples and the non-parametric\nsoftmax is costly especially when the number of training instances n is extremely large. To\ntackle this issue, negative sampling is used to approximate the softmax, i.e., K < n.\nThe discriminability of representations to contrast one instance from another instance\nis closely related to the transferability on downstream tasks. Thus, many eﬀorts have been\nmade to increase the number and improve the quality of keys. As shown in Figure 11, Inst-\nDisc (Wu et al., 2018) uses a memory bank to store the latest updated representations for\neach key, which increases the number of negative samples, yet may result in less consistent\nrepresentations. Momentum Contrast (MoCo) (He et al., 2020) maintains a dynamic queue\nof encoded features to enlarge the size of negative samples and encodes the keys with a\nmomentum-updated encoder, which increases encoding consistency between diﬀerent sam-\nples in the queue and improves the quality of keys. The way how the positive samples and\nnegative samples are constructed is also important for transferability. Contrastive Multiview\nCoding (CMC) (Tian et al., 2020) takes multiple views, rather than multiple augmentations,\nof the same instance as positive samples and achieves better transferability. SimCLR (Chen\net al., 2020) emphasizes that data augmentations play a crucial role in implicitly deﬁning\n18\nTransferability in Deep Learning: A Survey\nsimilarity &\ndissimilarity\nencoder\nmemory\nbank\nsampling\nencoder\nmomentum\nencoder\nmoving\naverage\nsimilarity &\ndissimilarity\nencoder\nsimilarity &\ndissimilarity\nencoder\nencoder\nsimilarity\nencoder\npredictor\n(a) InstDisc\n(b) MoCo\n(c) SimCLR\n(d) SimSiam\nstop gradient \npredictor\n!\n!3\n,\n-\n!\n!3\n,\n-\n!4\n!\n!3\n,\n-\n!4\n!\n!3\n,\n-\n!4\nFigure 11: Comparison of diﬀerent contrastive learning mechanisms. (a) InstDisc samples\nthe keys from a memory bank. (b) MoCo encodes the new keys on the ﬂy by a momentum\nencoder and maintains a queue of keys. (c) SimCLR encodes the keys and queries in the same\nbatch with the same encoder and adds a nonlinear predictor to improve the representation.\n(d) SimSiam applies an MLP predictor on one side and applies a stop-gradient operation\non the other side, and maximizes the similarity in two views without using negative pairs.\ndiﬀerent pretext tasks, and the composition of stronger augmentations leads to better trans-\nferability even without the need for a memory bank or a queue. The introduction of negative\nsamples is to avoid trivial solutions that all outputs collapse to a constant. However, BYOL\n(Grill et al., 2020) ﬁnds that when maximizing the similarity between two augmentations\nof one image, negative sample pairs are not necessary. Further, SimSiam (Chen and He,\n2021) ﬁnds that momentum encoder is also not necessary while a stop-gradient operation\napplied on one side is enough for learning transferable representations.\nCompared with supervised pre-training, contrastive pre-training leads to competitive\nperformance on downstream classiﬁcation tasks and even better performance on various\nother downstream tasks, such as object detection and semantic segmentation. To explain the\nstronger transferability of contrastive pre-training, Zhao et al. (2021) observe that standard\nsupervised pre-training usually transfers high-level semantic knowledge, while contrastive\npre-training usually transfers low-level and mid-level representations. When the target tasks\nare diﬀerent from the supervised pre-trained tasks, the supervised pre-training methods\nhave the risk of over-ﬁtting the semantic discriminative parts of objects deﬁned by the class\nlabels, which hurts the transferability. On the contrary, the contrastive pre-training tasks\nlead to more holistic modeling of the objects, which relaxes the task misalignment issue and\nachieves better transferability for widespread downstream tasks.\n2.4 Remarks\nWhile standard supervised pre-training is well established, its transferability also depends on\nthe relationship between the pre-training task and the target task, and no pre-training task\ncan dominate all downstream tasks. He et al. (2019) show that compared with the random\ninitialization, supervised pre-training on ImageNet only speeds up the convergence of object\ndetection on the COCO dataset, but does not lead to better ﬁnal accuracy. Raghu et al.\n(2019) observe similar phenomena in medical imaging, where training lightweight models\nfrom scratch perform comparably with transferring from ImageNet pre-trained models.\nAbnar et al. (2022) explore the limits of large-scale supervised pre-training and ﬁnd that\nas the pre-training accuracy increases by scaling up data, model size and training time,\n19\nJiang et al.\nthe performance of downstream tasks gradually saturates and there are even some extreme\nscenarios where performance on pre-training and downstream tasks are at odds with each\nother. These controversial results encourage us to rethink the common practice of supervised\npre-training and design new supervised pre-training strategies for speciﬁc ﬁelds, especially\nwhen large gaps exist between the pre-training and target tasks.\nTable 2: Comparison between diﬀerent pre-training methods.\nMethod\nModality Scalability1\nTask Scalability2\nData Eﬃciency3\nLabeling Cost4\nStandard Pre-Training\n⋆⋆⋆\n⋆⋆\n⋆⋆⋆\n⋆\nMeta-Learning\n⋆⋆⋆\n⋆\n⋆\n⋆\nCausal Learning\n⋆⋆\n⋆\n⋆\n⋆\nGenerative Learning\n⋆⋆\n⋆⋆⋆\n⋆⋆⋆\n⋆⋆⋆\nContrastive Learning\n⋆\n⋆⋆⋆\n⋆⋆⋆\n⋆⋆⋆\n1 Modality Scalability: whether models can be pre-trained on various modalities, such as text, graph.\n2 Task Scalability: whether pre-trained models can be easily transferred to diﬀerent downstream tasks.\n3 Data Eﬃciency: whether stronger transferability can be yielded from large-scale pre-training.\n4 Labeling Cost: whether relies on manual data labeling.\nTable 2 compares pre-training methods from four perspectives: modality scalability, task\nscalability, data eﬃciency, and labeling cost. Though meta-learning enables fast adaptation\nto new tasks, it mainly considers related tasks such as reinforcement learning under envi-\nronments with small changing factors, while standard pre-training can transfer to broader\ntask gaps such as from image classiﬁcation to object detection. Besides, the existing meta-\nlearning and causal learning methods are empirically veriﬁed only on small datasets, and it\nremains unclear whether they can acquire stronger transferability via pre-training on large-\nscale data. Despite the promising performance without manual labeling, unsupervised pre-\ntrained models require a large number of gradient steps for ﬁne-tuning to downstream tasks.\nAlso, strong data augmentations are required by contrastive learning to gain transferability,\nbut they are not easy to design in other modalities, such as text and graphs. Finally, the\ndesign of unsupervised pre-training tasks remains heuristic, lacking solid analysis on how\nthe task shift is bridged and what enables the transferability of these models.\nAcquiring transferability only through the pre-training stage may limit our horizon. As\nthe shift in tasks and domains naturally exists between the pre-training and adaptation\nstages, many pre-training methods are tailored to adaptation. Unsupervised pre-training\naims to improve the transferability to downstream tasks by exploring diﬀerent kinds of self-\nsupervised tasks to reduce the task discrepancy between pre-training and adaptation, or by\nenlarging the size and diversity of the upstream data to reduce the upstream-downstream\ndiscrepancy. The distribution shift commonly tackled by domain adaptation (Ganin and\nLempitsky, 2015) also inﬂuences the transferability of the pre-trained model. For instance,\nthe data distribution in a speciﬁc domain, such as biological and scientiﬁc literature, is quite\ndiﬀerent from that in the general pre-training domain and may degrade transferability, thus\nBioBert (Lee et al., 2020b) and SciBERT (Beltagy et al., 2019) perform pre-training on\ndomain-speciﬁc data to improve the transferability on domain-speciﬁc tasks.\n20\nTransferability in Deep Learning: A Survey\n3. Adaptation\nWhile pre-training on large-scale datasets can gain transferable knowledge in deep models,\nperforming task adaptation with the target data is still necessary for most applications, as\nthe target task is usually diﬀerent from the pre-training task. When the labeled data for\nthe target task is not enough, domain adaptation from a related source domain with labeled\ndata to boost the performance on the target domain is also necessary in many applications.\nWe will review task adaptation and domain adaptation in Sections 3.1 and 3.2 respectively.\n3.1 Task Adaptation\nIn task adaptation, there exist a pre-trained model hθ0 and a target domain bT = {xi, yi}m\ni=1\nof m labeled samples. The goal is to ﬁnd a hypothesis hθ : X 7→Y in the space H using the\npre-trained model and target data to achieve a low generalization risk ϵT (hθ). In general,\nthere are two simple ways to adapt a pre-trained model to the downstream tasks: feature\ntransfer and ﬁne-tuning. Feature transfer freezes the weights of the pre-trained models and\ntrains a linear classiﬁer on top of that. In contrast, ﬁne-tuning uses the pre-trained models\nto initialize the target model parameters and update these parameters during training.\nFeature transfer is fast in training and eﬃcient in parameter storage, yet ﬁne-tuning yields\nbetter performance (Yosinski et al., 2014), and has become a common practice for task\nadaptation in both vision and NLP (Girshick et al., 2014; Devlin et al., 2019).\nVanilla ﬁne-tuning, which tunes the pre-trained models by empirical risk minimization\non the target data, has been widely used in various downstream tasks and scenarios. How-\never, vanilla ﬁne-tuning still suﬀers from several issues, including catastrophic forgetting and\nnegative transfer. We will introduce how to alleviate these issues in Sections 3.1.1 and 3.1.2.\nBesides, as the parameters of deep models keep increasing and some of them reach billions\nor trillions, parameter eﬃciency and data eﬃciency have become increasingly important in\ntask adaptation. We will give an introduction on how to explore the transferability in the\npre-trained models to solve these problems in Sections 3.1.3 and 3.1.4. Overall, Figure 12\nshows the progress made by the task adaptation algorithms to solve diﬀerent problems.\n2017\n2016\n2018\n2019\n2020\n2021\nLogME\nLEEP\nDiff Pruning\nTaskonomy\nBSS\nSide-Tuning\nAdapter Tuning\nResidual Adapter\nCatastrophic\nForgetting\nNegative\nTransfer\nParameter\nEfﬁciency\nData\nEfﬁciency\nLWF\nULMFiT\nDELTA\nCo-Tuning\nDAPT\nZoo-Tuning\nGPT3\nPreﬁx Tuning\nInstruction-Tuning\nEWC\nPiggyBack\nMatching Net\nProtoNet\nT5\nFigure 12: Cornerstones of task adaptation methods for applying transferable knowledge.\n21\nJiang et al.\n3.1.1 Catastrophic Forgetting\nCatastrophic forgetting, which was ﬁrst studied in lifelong learning, refers to the tendency\nof neural networks to lose knowledge acquired from previous tasks when learning new tasks\n(Kirkpatrick et al., 2017). In the ﬁne-tuning scenario where labeled data is usually scarce,\nit will lead to the overﬁtting of models on the target data. This phenomenon is also called\nrepresentational collapse, i.e., the degradation of generalizable representations during the\nﬁne-tuning stages (Aghajanyan et al., 2021). The most simple way to avoid catastrophic\nforgetting might be selecting a small learning rate and adopting an early-stopping strategy,\nwhich avoids updating the parameters too much.\nHowever, this strategy may lead the\nmodel to falling into the local minimal, especially when there is a large gap between the\npre-training parameters and the optimal parameters for the downstream task.\nYosinski et al. (2014) ﬁnd that the transferability of diﬀerent layers is not the same —\nthe ﬁrst layers learn general features, the middle layers learn semantic features and the last\nlayers learn task-speciﬁc features. Thus, to make the model retain the knowledge acquired in\nthe pre-training task and ﬁt the target task well at the same time, diﬀerent layers should not\nbe treated the same. Speciﬁcally, the ﬁrst layers should retain more pre-trained knowledge\nwhile the last layers should adapt more to the downstream tasks. Inspired by this ﬁnding,\nDAN (Long et al., 2015) sets the learning rate of the task-speciﬁc head to be 10 times larger\nthan that of the lower layers, which is simple yet eﬀective when the labeled data is scarce\nor the target domain is close with the pre-training domain. ULMFiT (Howard and Ruder,\n2018) gradually unfreezes the model starting from the last layers to the ﬁrst layers, which\neﬀectively retains general knowledge in the ﬁrst layers. To automatically determine which\nlayers should be ﬁne-tuned or frozen for each sample, Spottune (Guo et al., 2019) proposes\na policy network that is deployed to output the routing decision based on the input of each\nsample and is jointly trained with the main model during ﬁne-tuning.\nDomain Adaptive Tuning reveals that an important source of catastrophic forgetting\nis the dataset shift between the pre-training and the target domain. To bridge such shift,\nULMFiT (Howard and Ruder, 2018) and DAPT (Gururangan et al., 2020) ﬁrst tune the\npre-trained model on data related to the target domain, or simply data of the target task,\nwith the pre-training task. Then they ﬁne-tune the adaptive-tuned model on the target\ntask (Figure 13). Usually, the pre-training task is unsupervised, thus further pre-training\nwith in-domain data can provide rich information about the target data distribution for\nbetter task adaptation with no additional labeling costs. The two stages, domain adaptive\ntuning and regular ﬁne-tuning, in the above methods can also be done jointly via multi-\ntask learning. SiATL (Chronopoulou et al., 2019) adds an auxiliary language model loss to\nthe task-speciﬁc optimization function, which alleviates catastrophic forgetting and learns\ntask-speciﬁc features at the same time.\nRegularization Tuning is another way to prevent the models from deviating far away\nfrom the pretrained ones. The optimization objective with a general regularization is\nmin\nθ\nm\nX\ni=1\nL(hθ(xi), yi) + λ · Ω(θ),\n(9)\nwhere L is the loss function, Ωis a general form of regularization, and λ is the trade-\noﬀbetween them. A typical regularization in supervised learning is L2 penalty, Ω(θ) =\n22\nTransferability in Deep Learning: A Survey\nInit\nModel\nPre-trained\nModel\nAdaptive\nPre-trained\nFinal\nModel\npre-train\n \nadaptive\ntune\nﬁne-tune\n.′\n.\n0\nFigure 13: Domain Adaptive Tuning often consists of two consecutive steps: ﬁrst, adaptive-\ntune on an auxiliary domain T ′ that is related to the target domain using the pre-training\ntask; second, ﬁne-tune on the target domain T using the target learning task.\n1\n2||θ||2\n2, which drives the weights θ to zero to control the model complexity. Diﬀerent from\ntypical supervised learning, in ﬁne-tuning, there exists a pre-trained model hθ0 setting a\nreference that can be used to deﬁne the hypothesis space (Figure 4). Thus, Elastic Weight\nConsolidation (EWC) (Kirkpatrick et al., 2017) constrains the distance between the weights\nof the pre-trained and ﬁne-tuned networks (Figure 14) to overcome catastrophic forgetting,\nΩ(θ) =\nX\nj\n1\n2Fj\n\r\rθj −θ0\nj\n\r\r2\n2 ,\n(10)\nwhere F is the estimated Fisher information matrix. EWC is based on the assumption\nthat the networks with similar weights should produce similar outputs. However, due to\nthe complex structures of deep networks, similar parameters do not necessarily produce\nthe same output, and the same output may also come from completely diﬀerent model\nparameters. Thus, DELTA (Li et al., 2019) constraints the behavior, i.e., the feature maps\nof the model by selecting the discriminative features with a supervised attention mechanism\nand regularizing the distance of these features between pre-trained and ﬁne-tuned networks.\nLearning Without Forgetting (LWF) (Li and Hoiem, 2018) constrains the output prediction\nof the model by encouraging the model’s response for old tasks to keep the same throughout\nthe ﬁne-tuning process (Figure 14). Regularization on the output often performs better than\nregularization on the parameters or the features, yet the latter two have better scalability\nand versatility to more complex downstream tasks.\nbackbone\ntarget\nhead\npre-trained\nhead\nnoise\npre-trained\nbackbone\ninitialize\nL2\nL\n2\n…\n…\npre-trained models\nﬁne-tuned models\n…\nL\n2\n…\n…\npre-trained models\nﬁne-tuned models\n…\nL2\n(c) LWF\n(a) EWC\n(b) DELTA\n&\n&=\nℎ=\nℎ\n#$\n$\n#$=\n$=\n%!&'\n%>+!\"+??\n)=\n#\n)=\n@\n)#\n)@\n!\n)=\n#\n)=\n@\n)#\n)@\n\"=\n#\n\"=\n@A#\n\"=\n#\n\"=\n@A#\n!\n,!\n!\nFigure 14: Regularization methods for task adaptation that avoids catastrophic forgetting.\nBlue: pre-trained parameters; Red: ﬁne-tuned parameters. (a) EWC regularizes the param-\neters of the new models θ and that of the pre-trained models θ0 with weighted L2-penalty.\n(b) DELTA regularizes the feature maps of the new models z and that of the pre-trained\nmodels z0. (c) LWF enforces the output of the old tasks by0 close to the initial response y0.\n23\nJiang et al.\nAn explanation for the eﬀect of regularization is that it makes the hypothesis smoother.\nTherefore, TRADES (Zhang et al., 2019a) and SMART (Jiang et al., 2020) directly enforce\nthe smoothness of the hypothesis by encouraging the output of the model to not change\nmuch when injecting a small perturbation to the input,\nΩ(θ) =\nm\nX\ni=1\nmax\n||exi−xi||p≤ϵ Ls(hθ(exi), hθ(xi)),\n(11)\nwhere ϵ > 0 is a small positive, Ls is the distance between two predictions, such as the\nsymmetrized KL-divergence in classiﬁcation and the squared loss in regression.\nApart from the training objective, an alternative regularization approach is through\nthe parameter updating strategy. Stochastic Normalization (Kou et al., 2020) randomly\nreplaces the target statistics in the batch-normalization layer (Ioﬀe and Szegedy, 2015)\nwith their pre-trained statistics, which serves as an implicit regularization by avoiding over-\ndepending on the target statistics. Mixout (Lee et al., 2020a) randomly replaces part of the\nmodel parameters with their pre-trained weights during ﬁne-tuning to mitigate catastrophic\nforgetting. Child-Tuning (Xu et al., 2021) selects a subset of parameters (child network) by\nsome criterion and only updates them during ﬁne-tuning. In some senses, the above methods\ndecrease the hypothesis space to preserve the transferability in pre-trained models.\n3.1.2 Negative Transfer\nAlthough the paradigm of pre-training and ﬁne-tuning has been used in various downstream\ntasks, it does not necessarily produce a positive eﬀect, which is known as negative transfer\n(Rosenstein, 2005). Wang et al. (2019d) propose to quantitatively measure the degree of\nnegative transfer across diﬀerent domains and we extend this idea to the paradigm of pre-\ntraining and ﬁne-tuning.\nDeﬁnition 2 (Negative Transfer Gap) Let hθ(U, T ) be a hypothesis obtained by adapt-\ning the model pre-trained from the upstream data U to the target data T , and hθ(∅, T ) be a\nhypothesis obtained by training from scratch on T , then negative transfer gap is deﬁned as\nNTG = ϵT (hθ(U, T )) −ϵT (hθ(∅, T )),\n(12)\nand negative transfer occurs if NTG is positive and vice versa.\nFirst, negative transfer will happen when the relatedness between the upstream task\nand downstream task is not strong, e.g. Next Sentence Prediction pre-training task will hurt\ntoken-level classiﬁcation tasks (Liu et al., 2019b). Negative transfer will also happen when\nthere is a large shift between the pre-training domain and the target domain, e.g. for legal\ndocuments classiﬁcation, pre-training only on legal documents is better than pre-training\non more diverse datasets (Zheng et al., 2021). Second, negative transfer depends on the size\nof the labeled target dataset (Wang et al., 2019d). For example, He et al. (2019) empirically\nshow that on large-scale object detection datasets (e.g. COCO), ImageNet pre-training is\nnot beneﬁcial when training for enough iterations. Third, negative transfer depends on the\ntask adaptation algorithms. An ideal adaptation algorithm should promote positive transfer\nbetween related tasks while avoiding negative transfer between unrelated tasks. In practice,\nhowever, these two goals are often contradictory and result in the dilemma: approaches that\npromote larger positive transfer will suﬀer from severer negative transfer (Figure 15).\n24\nTransferability in Deep Learning: A Survey\ntask \nrelatedness\naggressive\nconservative\nnegative\ntransfer\npositive\ntransfer\ntransfer\nperformance\nFigure 15: Dilemma to promote positive transfer and avoid negative transfer: Aggressive\nstrategies that promote larger positive transfer will suﬀer from severer negative transfer;\nConservative strategies can decrease negative transfer, yet lead to smaller positive transfer.\nEnhancing Safe Transfer.\nOne way to avoid negative transfer is to recognize and re-\nject harmful knowledge in the pre-trained model. Chen et al. (2019b) observe that with\nsuﬃcient training data, the spectral components with small singular values vanish dur-\ning ﬁne-tuning, indicating that small singular values correspond to detrimental pre-trained\nknowledge and may cause negative transfer. Thus BSS penalizes smaller singular values to\nsuppress untransferable spectral components for safe transfer. Jang et al. (2019) meta-learns\nthe weights determining which pairs of layers should be matched and to what extent the\nknowledge should be transferred, which rejects irrelevant information during transfer. Zoo-\ntuning (Shu et al., 2021b) enables adaptive transfer from a zoo of models, which adaptively\naggregates multiple pre-trained models to derive the target model using data-dependent\ngating mechanisms to highlight transferable parameters. Another way to mitigate negative\ntransfer of the pre-trained model is to fully explore the target data. Self-Tuning (Wang et al.,\n2021) proposes a pseudo group contrastive mechanism to explore the intrinsic structure of\nthe target data in the process of ﬁne-tuning with standard supervised objective.\nChoosing Pre-trained Models.\nWith the fast development of deep learning, a large zoo\nof pre-trained models are available, thus a simpler way to avoid negative transfer is to select a\nmodel that is pre-trained on the upstream data/task relevant to the downstream data/task.\nThe most common practice to choose pre-trained models is based on rich past experience or\nthrough heavy experiments. To facilitate faster selection, Taskonomy (Zamir et al., 2018)\nproposes a fully computational approach for explicitly modeling the relationship between\n26 diﬀerent visual tasks. Another more eﬃcient strategy to select pre-trained models is to\npredict the transferability of pre-trained models. LEEP (Nguyen et al., 2020) constructs\nan empirical predictor by estimating the joint distribution over pre-trained labels and the\ntarget labels, and then uses the log expectation of the empirical predictor to measure the\ntransferability. LogME (You et al., 2021) proposes to predict the ﬁne-tuning performance\nfrom the compatibility of features {zi = ψ(xi)}m\ni=1 and labels {yi}m\ni=1. Still, these methods\nmay underestimate strong but non-linear features. He et al. (2021) show that features from\ncontrastive pre-training, such as MoCo v3 (Chen et al., 2021a), have higher linear probing\naccuracy while worse fully ﬁne-tuning results than generative pre-training, such as MAE\n(He et al., 2021), indicating that the linear separability of the pre-trained features is not\nthe sole metric for evaluating transferability.\n25\nJiang et al.\n(a) Feature Transfer\n(b) Fine-Tuning\n+\n(c) Side-Tuning\npre-trained\nmodules\n+\nadapter\nŏ\n(d) Adapter Tuning\nFigure 16: Comparison on how diﬀerent adaptation methods freeze (blue) and tune (red)\npre-trained parameters. (a) Feature transfer freezes all the pre-trained parameters. (b)\nFine-tuning re-trains all the pre-trained parameters. (c) Side-tuning trains a lightweight\nconditioned side network that is fused with the ﬁxed pre-trained network using summation.\n(d) Adapter-Tuning inserts adapter modules for tuning into each frozen pre-trained layer.\n3.1.3 Parameter Efficiency\nFine-tuning large pre-trained models yields strong performances on many downstream tasks\n(Radford et al., 2018; Devlin et al., 2019), yet it is not parameter eﬃcient since it generates\na full set of model parameters for each downstream task, which will cause unacceptable\nstorage cost as the number of tasks increases. The simplest solution is Multi-task Learning\n(Caruana, 1997), i.e., ﬁne-tuning a single model to solve multiple target tasks, which might\nbe mutually beneﬁcial to each other (He et al., 2017; Liu et al., 2019a). Yet when diﬀerent\ntarget tasks are weakly related, multi-task learning will underperform ﬁne-tuning for each\ntask separately. Also, multi-task learning requires simultaneous access to all target tasks,\nwhich is not feasible in online scenarios where the target tasks arrive in sequence. Hereafter,\nwe will introduce new tuning paradigms proposed to improve parameter eﬃciency.\nResidual Tuning.\nInspired by the fact that approximation to a diﬀerence is easier than\nthe original function (He et al., 2016), Side-Tuning (Zhang et al., 2019b) adds a small side\nnetwork hside to adapt the frozen pre-trained model hpretrained for the target task and obtain\na combined model h(x) = αhpretrained(x)+(1−α)hside(x), where α is a weight that changes\nduring training. When there is a big gap between the pre-trained model and the downstream\ntask, it may be diﬃcult to learn the residuals of the entire model. Thus, Adapter Tuning\n(Houlsby et al., 2019) inserts residual adapter modules into each frozen layer. Residual\nAdapter was ﬁrst introduced for learning multiple visual domains (Rebuﬃet al., 2017) and\nconsists of a skip connection, such that it is set as a near-identity function and will not\nimpair the whole model when the training starts. By choosing a much smaller amount of\nparameters for the adapters, Adapter Tuning can extend pre-trained models to new tasks\nwithout increasing much storage cost. Houlsby et al. (2019) ﬁnd that Adapter Tuning with\nonly 3.6% tunable parameters can match the performance of the fully ﬁne-tuned BERT on\nthe GLUE benchmark (Wang et al., 2019a), revealing the great potential of this method.\nParameter Diﬀerence Tuning.\nWhile residual adapter tuning changes the model acti-\nvations by adding new modules, parameter diﬀerence tuning extends the pre-trained models\nthrough a task-speciﬁc parameter diﬀerence vector,\nθtask = θpretrained ⊕δtask,\n(13)\n26\nTransferability in Deep Learning: A Survey\nwhere ⊕is the element-wise addition function, θpretrained is the ﬁxed pre-trained parameters\nand δtask is the tuned task-speciﬁc diﬀerence vector. Instead of storing a copy of θtask for\nevery task, diﬀerence tuning only needs to store a single copy of θpretrained and a copy of δtask\nfor every task. As long as the size of δtask can be reduced, we can achieve parameter eﬃcient\nmodels. To this end, DiﬀPruning (Guo et al., 2021) utilizes L0-norm penalty (Louizos\net al., 2018) to encourage sparsity of the diﬀerence vector δtask. Aghajanyan et al. (2021)\nadopt FastFood transform M (Li et al., 2018) to convert δtask into a low-dimensional vector\nδlow, i.e., δtask = δlowM. The element-wise addition can also be replaced by element-wise\nmultiplication. For instance, Piggyback (Mallya and Lazebnik, 2018) multiplies real-valued\nmask weights to the pre-trained parameters, i.e., θtask = θpretrained ⊙δtask during training.\nAfter training, the mask weights δtask are passed through a thresholding function to obtain\nbinary-valued masks, further reducing the parameter storage of δtask at inference.\nThe essential diﬀerence between the above two tuning methods lies in their diﬀerent\nassumptions about the root of transferability. Residual tuning assumes that transferability\nis encoded in the behaviors of each module, i.e., the features output by each module. When\nadapting to the downstream tasks, we only need to add some task-speciﬁc behaviors by\nstacking the pre-trained modules with the residual adapter modules. In contrast, parameter\ndiﬀerence tuning assumes that transferability lies in the pre-trained parameters. Most of the\npre-trained parameters can be reused, and only a small part of them need to be adapted to\nthe downstream tasks, thus we only need to store the increment. Another thing to mention\nis that when limiting the size of the residual adapters or the complexity of the diﬀerence\nvector, these methods naturally overcome the catastrophic forgetting issue in Section 3.1.1.\n3.1.4 Data Efficiency\nCurrently, when ﬁne-tuning large pre-trained models, hundreds or even thousands of labeled\nsamples are still required to achieve strong performance on a speciﬁc downstream task,\nwhich limits the application of the “pre-training and ﬁne-tuning” paradigm to wider range\nof tasks where labeled data are expensive to collect. In contrast, people can adapt to a\nnew task with extremely few labeled samples, which is known as few-shot learning, or even\nwith no labeled samples, which is known as zero-shot learning. Considering the lifecycle of\ndeep learning, we can tackle this problem in three ways. The ﬁrst is to improve the cross-\ntask transferability of the pre-trained models, such as by increasing the model capacity or\nthe pre-training dataset size, which is mentioned in Section 2. The second is to transfer\nfrom another labeled source domain where labeled data is cheaper to collect, which will be\ndiscussed in Section 3.2. The last is to reformulate the target task to close its gap with the\npre-trained models, which is the focus of this part.\nMetric Learning.\nFine-tuning in low data regimes will easily cause over-ﬁtting as updat-\ning the model of large-scale parameters using few labeled samples is ill-posed. In contrast,\nmany non-parametric methods, such as nearest neighbors, can deal with low-sample regimes\nwithout suﬀering from catastrophic forgetting. To combine the advantages of parametric\nand non-parametric methods, Matching Net (Vinyals et al., 2016) uses an attention mech-\nanism over the learned representations to predict the classes for the query samples, which\ncan be interpreted as weighted nearest neighbors. Since labeled data is severely limited,\nProtoNet (Snell et al., 2017) adds a stronger inductive bias that there exists a single pro-\n27\nJiang et al.\ntotype representation for each class, where each prototype is the mean of the features of\nthe labeled samples in each class, and classiﬁcation boils down to ﬁnding the nearest pro-\ntotype. Since no gradient update is performed on the feature representation, choosing a\nproper distance metric that has good transferability across tasks plays an important role.\nA common choice is the cosine distance, which explicitly reduces the intra-class variations\nand improves the cross-task transferability. Chen et al. (2019a) ﬁnd that by replacing the\nlinear classiﬁer with a cosine-distance based classiﬁer, the naive feature transfer method\nwithout ﬁne-tuning serves as a strong baseline in few-shot learning.\nPrompt Learning.\nPrompting, ﬁrstly proposed in GPT-3 (Brown et al., 2020), is another\nway to reformulate the downstream task to make it similar to the solved pre-training task.\nIn ﬁne-tuning, models will take input x and predict an output y as P(y|x). In prompting,\nthe original input x is modiﬁed by a prompt template into a new string ˜x that has unﬁlled\nslots, then the pre-trained language model will ﬁll ˜x to obatin a ﬁnal string bx and derive the\noutput y from bx (Liu et al., 2021b). Table 3 provides an example of prompting methods.\nTable 3: An example of prompting method from Liu et al. (2021b).\nName\nNotation\nExample\nInput\nx\nI love this ﬁlm.\nOutput\ny\npositive\nPrompt Template\nfprompt(x)\n[X] Overall, it was a [Z] ﬁlm.\nPrompt\n˜x\nI love this ﬁlm. Overall, it was a [Z] ﬁlm.\nFilled Prompt\nbx\nI love this movie. Overall, it was a good movie.\nThe advantage of prompting is that it enables few-shot or even zero-shot task adaptation.\nThe strong cross-task transferability stems from the implicit multiple tasks such as question-\nanswering that language models are forced to learn on the large-scale pre-training corpus.\nHowever, this transferability requires a large model capacity to deal with potential implicit\ntasks and is also very sensitive to the choice of prompts. Thus, the disadvantage is that it\nintroduces the necessity for prompt engineering, i.e., ﬁnding the best prompts to solve each\ndownstream task, which is work-heavy and time-consuming especially on large datasets.\nPLM\nCLS\nTAG\nGEN\nﬁne-tune\nPLM\nCLS\nTAG\nGEN\nPLM\nCLS\nModel:\nTask:\nPrompt:\nTAG\nGEN\nCLS\nTAG\nGEN\nMultiple\nInstruction\nTasks:\nCLS,\nTAG,\netc.\nFLAN\nCLS\nTAG\nGEN\n(a) Pretrain-Finetune (BERT, T5)\n(b) Prompting (GPT-3)\n(c) Instruction Tuning (FLAN)\nFigure 17: (a) Fine-tuning generates a new set of parameters for each downstream task. (b)\nPrompting ﬁxes the pre-trained parameters and ﬁnds task-speciﬁc prompts to solve each\ndownstream task. (c) Instruction Tuning tunes the pre-trained models on instruction-format\ndataset and uses the obtained model to do inference with multiple downstream tasks.\n28\nTransferability in Deep Learning: A Survey\nCombining prompting and ﬁne-tuning may tackle this problem (Figure 17). PET-TC\n(Schick and Sch¨utze, 2020) tunes the parameters of pre-trained language models in prompt\nlearning while Preﬁx-Tuning (Li and Liang, 2021) adds additional prompt-related parame-\nters and tunes these parameters. Instruction Tuning (Wei et al., 2022) explicitly ﬁne-tunes\nthe pre-trained models on a mixture of datasets expressed through natural language instruc-\ntions (similar to the ﬁlled prompt) and obtain Fine-tuned LAnguage Model (FLAN), which\nlargely increases the models’ transferability to unseen tasks. In summary, prompt learning\nhas provided a revolutionary way on how to utilize the transferability of pre-trained models.\n3.1.5 Remarks\nTable 4 gives a comparison of diﬀerent task adaptation methods. Fine-tuning (including\nvanilla ﬁne-tuning, domain adaptive tuning, and regularization tuning) has better perfor-\nmance when there are enough labeled data in the downstream tasks. In contrast, prompt\nlearning requires much fewer labeled data to achieve decent performance, yet its applica-\ntions are still limited to NLP and it is still non-trivial to extend it to vision or other areas.\nFine-tuning is the most parameter-ineﬃcient since it generates a full set of model parame-\nters for each downstream task, while residual tuning, diﬀerence tuning, and prompt learning\nare all parameter eﬃcient. Also, these latter methods naturally mitigate the catastrophic\nforgetting problem, but negative transfer is still a hard problem to be resolved.\nTable 4: Comparison between diﬀerent task adaptation methods.\nAdaptation\nPerformance1\nData\nEﬃciency2\nParameter\nEﬃciency3\nModality\nScalability4\nTask\nScalability5\nFeature Transfer\n⋆\n⋆⋆\n⋆⋆⋆\n⋆⋆⋆\n⋆⋆⋆\nVanilla Fine-tuning\n⋆⋆⋆\n⋆\n⋆\n⋆⋆⋆\n⋆⋆⋆\nDomain Adaptive Tuning\n⋆⋆⋆\n⋆⋆\n⋆\n⋆⋆\n⋆⋆⋆\nRegularization Tuning\n⋆⋆⋆\n⋆⋆\n⋆\n⋆⋆⋆\n⋆\nResidual Tuning\n⋆⋆\n⋆⋆\n⋆⋆\n⋆⋆\n⋆⋆\nParameter Diﬀerence Tuning\n⋆⋆\n⋆⋆\n⋆⋆\n⋆⋆⋆\n⋆⋆⋆\nMetric Learning\n⋆\n⋆⋆⋆\n⋆⋆⋆\n⋆⋆⋆\n⋆\nPrompt Learning\n⋆⋆\n⋆⋆⋆\n⋆⋆⋆\n⋆\n⋆\n1 Adaptation Performance: performance when there are large-scale labeled data in downstream tasks.\n2 Data Eﬃciency: performance when there are only small-scale labeled data in downstream tasks.\n3 Parameter Eﬃciency: whether can control parameters when the number of downstream tasks increases.\n4 Modality Scalability: whether can adapt pre-trained models to various modalities, such as text, graph.\n5 Task Scalability: whether can adapt pre-trained models to diﬀerent downstream tasks, such as detection.\nThe motivation of many task adaptation methods can be understood from the perspec-\ntive of transferability. For instance, domain adaptive tuning aims to bridge the domain\ndiscrepancy between the pre-training task and the downstream task by further obtaining a\npre-trained model on the target data distribution. Prompt learning aims to bridge the task\ndiscrepancy between the pre-training task and the downstream task by reformulating all\nthe tasks to the same format. In this scenario, when all tasks can be expressed in the same\nform, the diﬀerence between the pre-training task and the downstream task is only the shift\nin data distributions, i.e., task adaptation becomes the domain adaptation problem.\n29\nJiang et al.\n3.2 Domain Adaptation\nThe pre-training and ﬁne-tuning paradigm has greatly improved the state-of-the-arts for\ndiverse machine learning problems and applications, and the pre-trained deep networks can\nbe easily adapted to the tasks at hand even with a small amount of labeled data. However,\nin many practical scenarios, there is no labeled training data and thus there is the demand\nto transfer a deep network from a source domain where labeled training data is available to\na target domain where only unlabeled data exists (Chen et al., 2012; Glorot et al., 2011). In\nthis situation, the deep models still suﬀer from performance degradations due to distribution\nshift (Quionero-Candela et al., 2009). Thus, domain adaptation is proposed to reduce the\ndistribution shift between training and testing domains (Pan and Yang, 2010).\nMany methods have been proposed for domain adaptation in the shallow regime, either\nby re-weighting or selecting samples from the source domain (Sugiyama et al., 2008) or\nseeking an explicit feature space transformation from the source distribution into the target\ndistribution (Gong et al., 2013). As seminal methods, Huang et al. (2007); Pan et al. (2011);\nLong et al. (2013) explicitly match the distributions in the kernel-reproducing Hilbert space,\nwhile Gong et al. (2012) map the principal axes associated with each of the distributions.\nThis survey will focus on deep domain adaptation, where adaptation modules are embedded\nin deep architectures to match data distributions across domains.\nIn unsupervised domain adaptation (UDA), there is a source domain bS = {(xs\ni, ys\ni )}n\ni=1\nof n labeled samples and a target domain bT = {xt\ni}m\ni=1 of m unlabeled samples. The goal\nof a learning algorithm is to ﬁnd a hypothesis h : X 7→Y in the hypothesis space H with\na low target risk ϵT (h) = E(xt,yt)∼T [ℓ(h(xt), yt)] with no access to the labels of T , where\nℓ: Y × Y →R+ is a loss function. Several seminal theories have been proposed to tackle\nthis problem and the the main idea of them is to bound the target risk ϵT (h) by the source\nrisk ϵS(h) and a distribution distance. In this survey, we will focus on the theory of H∆H-\nDivergence (Ben-David et al., 2006, 2010a; Mansour et al., 2009) and Disparity Discrepancy\n(Zhang et al., 2019c) and illustrate how to derive diﬀerent algorithms from these theories.\nFirst, using triangle inequalities, we can relate the target risk to the source risk as follows.\nTheorem 3 (Bound with Disparity) Assume that the loss function ℓis symmetric and\nobeys the triangle inequality. Deﬁne the disparity between any two hypotheses h and h′ on\ndistribution D as\nϵD(h, h′) = E(x,y)∼D[ℓ(h(x), h′(x))].\n(14)\nThen the target risk ϵT (h) can be bounded by\nϵT (h) ⩽ϵS (h) + [ϵS (h∗) + ϵT (h∗)] + |ϵS (h, h∗) −ϵT (h, h∗)| ,\n(15)\nwhere h∗= arg minh∈H [ϵS (h) + ϵT (h)] is the ideal joint hypothesis, ϵideal = ϵS (h∗) + ϵT (h∗)\nis the ideal joint error, |ϵS (h, h∗) −ϵT (h, h∗)| is the disparity diﬀerence between S and T .\nIt is a common assumption in domain adaptation that the ideal joint error ϵideal shall be\nsuﬃciently small, otherwise domain adaptation will be infeasible, the impossibility theorem\n(Ben-David et al., 2010b). The goal is reduced to bound the disparity diﬀerence. However,\nsince the ideal hypothesis h∗is unknown due to the unavailability of labeled target data, the\ndisparity diﬀerence cannot be estimated directly. To this end, H∆H-Divergence (Ben-David\net al., 2006, 2010a) is proposed to measure the upper bound of the disparity diﬀerence.\n30\nTransferability in Deep Learning: A Survey\nDeﬁnition 4 (H∆H-Divergence) Deﬁne H∆H ≜{h|h = h1 ⊗h2, h1, h2 ∈H} as the\nsymmetric diﬀerence hypothesis space of H, where ⊗stands for the XOR operator. Then\nthe H∆H-Divergence between S and T is\ndH∆H(S, T ) ≜sup\nh,h′∈H\n\f\fϵS\n\u0000h, h′\u0001\n−ϵT\n\u0000h, h′\u0001\f\f .\nFor binary classiﬁcation problem with the 01-loss, ℓ(y, y′) = 1(y ̸= y′), we have\ndH∆H(S, T ) =\nsup\nδ∈H∆H\n|ES [δ(x) ̸= 0] −ET [δ (x) ̸= 0]| .\nThe main advantage of the H∆H-Divergence is that it can be estimated from ﬁnite un-\nlabeled samples of source and target domains. However, it is generally hard to compute and\noptimize. Thus, it is approximated by training a domain discriminator D that separates the\nsource and target samples (Ben-David et al., 2006; Ganin and Lempitsky, 2015). Assume\nthat the family of the discriminators is rich enough, such as the multilayer perceptrons\n(MLP) that is universal approximator to any functions, to contain H∆H, i.e., H∆H ⊂HD.\nThe H∆H-Divergence can be further bounded by supD∈HD |ES [D(x) = 1] + ET [D (x) = 0]|,\nwhich gives rise to the domain adversarial methods in Section 3.2.2. The H∆H-Divergence\ncan also be estimated in a nonparametric way by replacing H∆H with a proper function\nspace F, which induces the statistics matching methods in Section 3.2.1.\nThe following theorem is the earliest theory in domain adaptation, which establishes the\ngeneralization bound based on the H∆H-Divergence for binary classiﬁcation problems.\nTheorem 5 (Ben-David et al. (2010a)) Let H be a binary hypothesis space of VC di-\nmension d. If bS and bT are samples of size m each, then for any δ ∈(0, 1), with probability\nat least 1 −δ, for every h ∈H,\nϵT (h) ≤ϵS(h) + dH∆H( bS, bT ) + ϵideal + 4\ns\n2d log(2m) + log(2\nδ)\nm\n.\n(16)\nThis bound sheds key insights into algorithm designs. However, it has the limit of being\nbased on the particular 01-loss. Thus, Mansour et al. (2009) extend the domain adaptation\ntheory to a general class of loss functions satisfying the symmetry and subadditivity.\nTheorem 6 (Mansour et al. (2009)) Assume that the loss function ℓis symmetric and\nobeys the triangle inequality, and deﬁne h∗\nS = arg minh∈H ϵS(h) and h∗\nT = arg minh∈H ϵT (h)\nas the ideal hypotheses for the source and target domains, then for every h ∈H,\nϵT (h) ≤ϵS(h, h∗\nS) + dH∆H(S, T ) + ϵ,\n(17)\nwhere ϵS(h, h∗\nS) stands for the source risk and ϵ = ϵT (h∗\nT ) + ϵS(h∗\nT , h∗\nS) for the capacity to\nadapt. Further, let ℓbe bounded, ∀(y, y′) ∈Y2, ℓ(y, y′) ≤M for some M > 0, and deﬁned as\nℓ(y, y′) = |y −y′|q for some q. If bS and bT are samples of size n and m each, with probability\nat least 1 −δ, we have\ndH∆H(S, T ) ≤dH∆H( bS, bT ) + 4q(Rn,S(H) + Rm,T (H)) + 3M\n s\nlog 4\nδ\n2n\n+\ns\nlog 4\nδ\n2m\n!\n,\n(18)\n31\nJiang et al.\nwhere Rn,D is the expected Rademacher Complexity (Bartlett and Mendelson, 2002) with\nrespect to distribution D and sample size n.\nNote that the H∆H-Divergence bounds are still loose since the supremum is taken over\nboth h′ ∈H and h ∈H. Observing that h is known as the source classiﬁer, the Disparity\nDiscrepancy (Zhang et al., 2019c) provides a tighter bound by computing directly on H.\nDeﬁnition 7 (Disparity Discrepancy) Given a binary hypothesis space H and a speciﬁc\nhypothesis h∈H, the Disparity Discrepancy induced by h′ ∈H is deﬁned by\ndh,H(S, T ) = sup\nh′∈H\n\u0000ET 1[h′ ̸= h] −ES1[h′ ̸= h]\n\u0001\n(19)\nSince the supremum is only take over h′ ∈H, estimating and minimizing the disparity\ndiscrepancy jointly through a minimax game can be done much more easily. The disparity\ndiscrepancy can well measure the distribution shift and yields a tighter generalization bound.\nTheorem 8 (Zhang et al. (2019c)) Let bS and bT be samples of size n and m each. For\nany δ > 0 and every binary classiﬁer h ∈H, with probability at least 1 −3δ, we have\nϵT (h) ≤ϵ b\nS(h) + dh,H( bS, bT ) + ϵideal + 2Rn,S(H)\n+ 2Rn,S(H∆H) + 2\ns\nlog 2\nδ\n2n\n+ 2Rm,T (H∆H) +\ns\nlog 2\nδ\n2m .\n(20)\nThe disparity discrepancy can be further extended to the multiclass classiﬁcation prob-\nlem with hypothesis space F of scoring functions f : X × Y →R and margin loss, which is\ngoing beyond existing bounds and closer to the choices for real tasks (Zhang et al., 2019c).\nDeﬁnition 9 (Margin Disparity Discrepancy) Given a scoring hypothesis space F,\ndenote the margin of a real hypothesis f at a labeled example (x, y) as ρf(x, y) ≜1\n2(f(x, y)−\nmaxy′̸=y f(x, y′)), the labeling function induced by f as hf : x 7→arg maxy∈Y f(x, y), and\nthe margin loss as\nΦρ(x) ≜\n\n\n\n\n\n0\nρ ≤x\n1 −x/ρ\n0 ≤x ≤ρ\n1\nx ≤0\n,\n(21)\nthen the margin disparity between f and f′ on distribution D is\nϵ(ρ)\nD (f′, f) = E(x,y)∼D[Φρ(ρf′(x, hf(x))].\n(22)\nGiven a speciﬁc hypothesis f ∈F, the Margin Disparity Discrepancy is deﬁned by\nd(ρ)\nf,F(S, T ) = sup\nf′∈F\n[ϵ(ρ)\nT (f′, f) −ϵ(ρ)\nS (f′, f)].\n(23)\nNote that the margin disparity satisﬁes the nonnegativity and subadditivity, but not the\nsymmetry. Thus Theorem 6 cannot apply here and a new generalization bound is derived.\n32\nTransferability in Deep Learning: A Survey\nTheorem 10 (Zhang et al. (2019c)) Given the same settings with Deﬁnition 9, for any\nδ > 0, with probability at least 1 −3δ, the following margin bound holds for all scoring\nfunctions f ∈F,\nϵT (f) ≤ϵ(ρ)\nb\nS (f) + d(ρ)\nf,F( bS, bT ) + ϵideal + 2k2\nρ Rn,S(Π1F)\n+ k\nρRn,S(ΠHF) + 2\ns\nlog 2\nδ\n2n\n+ k\nρRm,T (ΠHF) +\ns\nlog 2\nδ\n2m ,\n(24)\nwhere ΠHF ≜{x 7→f(x, h(x))|h ∈H, f ∈F} is the scoring version of the symmetric hy-\npothesis space H∆H, Π1F ≜{x 7→f(x, y)|y ∈Y, f ∈F} and ϵideal = minf∗∈F{err(ρ)\nS (f∗)+\nerr(ρ)\nT (f∗)} is the ideal joint error in terms of margin loss, k is the number of classes.\nThis margin bound suggests that a proper margin ρ could yield better generalization on\nthe target domain. Theorems 8 and 10 together form the theoretical basis of the hypothesis\nadversarial methods in Section 3.2.3. Note that the supremum in both the H∆H-Divergence\nand Disparity Discrepancy will become meaningless, when the allowed hypothesis space H\nis too large, which is common in deep neural networks. Thus, pre-training the deep neural\nnetworks on large-scale upstream data to decrease the allowed hypotheses is still necessary\nfor the domain adversarial methods and hypothesis adversarial methods.\nA ﬁnal important note is that while there are no theoretical guarantees for some well-\nestablished methods, they have also achieved quite strong performance in practice, such as\nthe domain translation methods in Section 3.2.4 and the semi-supervised learning methods\nin Section 3.2.5. Figure 18 highlights the cornerstones of domain adaptation methods in\ndeep learning, which rely on the reuse of transferability gained in pre-trained deep models.\n2015\n2017\n2016\n2018\n2019\n2020\n2021\nDAN\nAdaBN\nTransNorm\nDANN\nFCN-wild\nD-adapt\nBSP\nRegDA\nMCD\nMDD\nSelf-Ensemble\nMCC\nJDOT\nJAN\nAdaptSeg\nDA-Faster\nCDAN\nADVENT\nSWDA\nSWD\nDD\nPixelDA\nCycleGAN\nDTN\nCyCADA\nCBST\nStatistics\nMatching\nDomain\nAdversarial\nLearning\nHypothesis\nAdversarial\nLearning\nDomain\nTranslation\nSemi\nSupervised\nLearning\nADDA\nCAN\nDeep Coral\nDeep JDOT\nDSN\nCoGAN\nRSP\nMMT\nFigure 18: The cornerstones of domain adaptation methods in deep learning.\n33\nJiang et al.\n3.2.1 Statistics Matching\nWe have introduced several seminal theories on the generalization bounds for domain adap-\ntation, which are all based on the hypothesis-induced distribution distances. These distances\nare less intuitive because they rely on unknown hypotheses and cannot be computed before\nlearning the hypotheses. In this section, we ﬁrst introduce another family of metrics on the\nspace of probability measures well-studied in probability theory, which provide interpretable\nand complementary properties to the hypothesis-induced distribution distances and relate\nclosely to a large set of domain adaptation algorithms (Long et al., 2015, 2017).\nDeﬁnition 11 (Maximum Mean Discrepancy) Given two probability distributions S\nand T on a measurable space X, the integral probability metric (Redko et al., 2020) is\ndeﬁned as dF(S, T ) ≜supf∈F\n\f\fEx∼S[f(x)] −Ex∼T [f(x)]\n\f\f, where F is a class of bounded\nfunctions on X. Sriperumbudur et al. (2010) further restrict F as the unit ball in Reproduc-\ning Kernel Hilbert Space (RKHS) Hk endowed with a characteristic kernel k, F = {f ∈Hk :\n||f||Hk ≤1}, leading to the maximum mean discrepancy (MMD) (Gretton et al., 2012a),\nd2\nMMD(S, T ) =\n\r\rEx∼S[φ(x)] −Ex∼T [φ(x)]\n\r\r2\nHk,\n(25)\nwhere φ(x) is a feature map associated with kernel k such that k(x, x′) = ⟨φ(x), φ(x′)⟩. It can\nbe proved from probability theory that S = T if and only if dF(S, T ) = 0 or d2\nMMD(S, T ) = 0.\nTheorem 12 (Redko et al. (2020)) Given the same settings with Deﬁnition 11, let ℓbe\na convex loss function with a parametric form ℓ(y, y′) = |y −y′|q for some q. Then for any\nδ > 0, with probability at least 1 −δ, the following bound holds for all h ∈F,\nϵT (h) ≤ϵS(h) + dMMD( bS, bT ) + ϵideal\n+ 2\nnEx∼S[\np\ntr(KS)] + 2\nmEx∼T [\np\ntr(KT )] +\ns\nlog 2\nδ\n2n\n+\ns\nlog 2\nδ\n2m ,\n(26)\nwhere KS and KT are the kernel matrices computed on samples from S and T , respectively.\nThis bound has several advantages compared to previous theories. First, it is hypothesis-\nfree and does not require estimating hypotheses to measure the distribution distance. Sec-\nond, the complexity term does not depend on the Vapnik-Chervonenkis dimension. Third,\nthe unbiased estimate of MMD can be computed in linear time. Fourth, minimizing MMD\nhas a nice interpretation of statistics matching in the probability space. These advantages\nmake the bound particularly useful to underpin several seminal algorithms.\nDeep Domain Confusion (DDC) (Tzeng et al., 2014) applies MMD with a linear kernel\nto a single feature layer of the deep network, yet it has limited power for closing the domain\ngap since linear kernel is not characteristic and cannot ensure MMD to be a probability\nmetric. Thereby, Deep Adaptation Network (DAN) (Long et al., 2015, 2019) introduces\nthe multiple-kernel variant of MMD (MK-MMD) (Gretton et al., 2012b,a) to measure the\ndomain relatedness, employing a convex combination of multiple characteristic kernels such\nas Gaussian kernel to make the function space F rich enough and enhance the distinguishing\npower of MK-MMD. Besides, as shown in Figure 19, multiple domain-speciﬁc layers are\nadapted by MK-MMD, which enables learning transferable features for domain adaptation.\n34\nTransferability in Deep Learning: A Survey\nMK-\nMMD\nMK-\nMMD\nMK-\nMMD\nJMMD\n…\n…\n×\n×\ntied\ntied\n(a) DAN\n(b) JAN\n!!\n!%\n\"!,\n\"!-\n\"!0\n$#!\n\"%,\n\"%-\n\"%0\n#!\n%!\"#\n1\n!!\n!%\n\"!,\n\"! 5\n$#!\n\"%,\n\"% 5\n#!\n1\n',\n' 5\n',\n' 5\n%!\"#\nFigure 19: The cornerstone methods of statistics matching: (a) DAN adapts the marginal\ndistributions of activations in multiple task-speciﬁc layers with MK-MMD. (b) JAN adapts\nthe joint distributions of the feature activations and classiﬁcation predictions with JMMD.\nDAN mainly reduces the shift in the feature distribution and ignores that in the label\ndistribution. Take AlexNet as example, the feature distribution shift mainly exists in layers\nfc6 and fc7 while the label distribution shift mainly exists in layer fc8. Joint Adaptation\nNetwork (JAN) (Long et al., 2017) proposes Joint Maximum Mean Discrepancy (JMMD) to\nmeasure the shift in joint distributions P(Xs, Ys) and Q(Xt, Yt). Denoting the activations\nof adapted layers L as {(zs1\ni , . . . , zs|L|\ni\n)}n\ni=1 and {(zt1\nj , . . . , zt|L|\nj\n)}m\nj=1, JMMD is deﬁned as\nd2\nJMMD( bS, bT ) =\n\r\r\rEi∈[n] ⊗l∈L φl(zsl\ni ) −Ej∈[m] ⊗l∈L φl(ztl\nj )\n\r\r\r\n2\nHk\n(27)\nwhere φl is the feature map associated with kernel kl for layer l and ⊗is the outer product.\nA characteristic kernel widely used in MMD is the Gaussian kernel, or k(x1, x2) =\nexp(−||x1 −x1||2/2σ2). After Taylor expansion, MMD can be considered as a weighted\nsum of distances between all orders of statistic moments. Thus, the statistic moments can\nbe directly used to measure the distribution distance. For instance, deep CORAL (Sun and\nSaenko, 2016) uses the second-order statistics (covariance) to measure distribution distance,\nwhich is frustratingly easy yet useful. Center moment discrepancy (CMD) (Zellinger et al.,\n2017) further considers an explicit order-wise matching of higher-order moments.\nOne disadvantage of MMD is that it cannot take into account the geometry of the data\ndistribution when estimating the discrepancy between two domains. Thus, Joint Distribu-\ntion Optimal Transport (JDOT) (Courty et al., 2017) is introduced into domain adaptation,\nand Deep JDOT (Damodaran et al., 2018) further extends it to deep networks. Another dis-\nadvantage is that minimizing MMD on the instance representation has the risk of changing\nthe feature scale, while regression tasks are fragile to feature scaling. Thus, Representation\nSubspace Distance (RSD) (Chen et al., 2021b) closes the domain shift through orthogonal\nbases of the representation spaces, which are free from feature scaling.\nInstead of explicitly matching the statistics moments of feature distributions, Adaptive\nBatch Normalization (AdaBN) (Li et al., 2017) implicitly minimizes domain discrepancy by\naligning BatchNorm Ioﬀe and Szegedy (2015) statistics. The hypothesis is that task-related\nknowledge is stored in the weight matrix while domain-related knowledge is represented by\nBatchNorm statistics. Thus AdaBN replaces the mean and variance of all BatchNorm layers\nwith those estimated on the target domain at inference to reduce the domain shift. However,\n35\nJiang et al.\ngenerator\nclassiﬁer\ndiscriminator\ngradient reverse layer\ngenerator\nclassiﬁer\ndiscriminator\nstop gradient \n×\n×\n(a) DANN\n(b) CDAN\n!!\n!%\n\"!\n$#!\n\"%\n#!\n%!\"#\n1\n2!\n2%\n%6788\nℎ\n4\n!!\n!%\n\"!\n$#!\n\"%\n#!\n%!\"#\n%9678\nℎ\n4\nℎ\n$#%\n1\nFigure 20: Both DANN and CDAN have a feature generator network ψ, a classiﬁer h, and\na domain discriminator D connected to ψ via a gradient reversal layer. (a) In DANN, the\ndiscriminator D is trained to distinguish between domains while the generator ψ tries to\nmake the feature distributions indistinguishable for the discriminator. (b) In CDAN, the\ndiscriminator D is conditioned on the classiﬁer prediction by via a multilinear map z ⊗by.\nit is risky that AdaBN excludes the statistics on the target domain from training. Thus,\nTransferable Normalization (TransNorm) (Wang et al., 2019c) applies domain-speciﬁc mean\nand variance at both training and inference to capture suﬃcient statistics of both domains.\nFinally, both MMD and JMMD may misalign samples from diﬀerent classes due to a\nsuboptimal modeling of the discriminative structure. To alleviate this problem, Contrastive\nAdaptation Network (CAN) (Kang et al., 2019) alternatively estimates the labels of target\nsamples through clustering, and adapts the feature representations in a class-wise manner.\nBesides, CAN uses class-aware sampling for both domains to improve adaptation eﬃciency.\n3.2.2 Domain Adversarial Learning\nDomain Adversarial Neural Network.\nAn important milestone for modeling distribu-\ntions is the Generative Adversarial Net (GAN) (Goodfellow et al., 2014). Inspired by GAN,\nDomain Adversarial Neural Network (DANN) (Ganin and Lempitsky, 2015; Ganin et al.,\n2016) integrates a two-player game into domain adaptation (Figure 20). The ﬁrst player is\nthe domain discriminator D trained to distinguish the source features from the target fea-\ntures and the second player is the feature generator ψ trained simultaneously to confuse the\ndomain discriminator. As mentioned in Section 3.2, the upper bound of H∆H-Divergence\nbetween feature distributions can be estimated by training the domain discriminator D,\nLDANN(ψ) = max\nD Exs∼b\nS log[D(zs)] + Ext∼bT log[1 −D(zt)],\n(28)\nwhere z = ψ(x) is the feature representation for x. The objective of the feature generator\nψ is to minimize the source error as well as the H∆H-Divergence bounded by Equation 28,\nmin\nψ,h E(xs,ys)∼b\nSLCE(h(zs), ys) + λLDANN(ψ),\n(29)\nwhere LCE is the cross-entropy loss, λ is a hyper-parameter that trades oﬀsource error and\ndomain adversary. Minimizing the cross-entropy loss will lead to discriminative representa-\ntions while decreasing the domain adversarial loss will result in transferable representations.\n36\nTransferability in Deep Learning: A Survey\nDANN aligns the marginal feature distributions through adversarial training. However,\nthis may be insuﬃcient when the feature-label joint distributions change between domains.\nBesides, the feature distribution is usually multimodal in multi-class classiﬁcation, thus even\nif the discriminator is fully confused, there is no guarantee that the two feature distributions\nare similar (Arora et al., 2017). To tackle these two issues, Conditional Domain Adversarial\nNetwork (CDAN) (Long et al., 2018) conditions features z on classiﬁer predictions by = h(z)\nand introduces multilinear map z ⊗by instead of z as the input to domain discriminator D:\nLCDAN(ψ) = max\nD Exs∼b\nS log[D(zs ⊗bys)] + Ext∼bT log[1 −D(zt ⊗byt)].\n(30)\nConditioning on by, CDAN fully captures cross-variance between the feature representation\nand classiﬁer prediction, resulting in better alignment of the joint distributions.\nTzeng et al. (2015) align the class distributions explicitly by transferring the similarity\nstructure in classes from the source domain to the target domain. Speciﬁcally, the average\noutput probability of data from each class is computed over the source samples as soft labels.\nThen the model is optimized to match the distribution over classes to the soft labels.\nImprovements.\nDANN integrates a gradient reverse layer (GRL) into the standard ar-\nchitecture to implement the minimax between the feature generator and domain classiﬁer.\nHowever, this optimizing strategy might not work well in practice due to gradient vanishing,\nwhich is also a major problem in training GANs. For instance, when the generated target\nfeatures zt are very distinguishable from source features such that D(zt) = 0, the gradient\nfor the feature generator is small and vice versa. This makes the optimization of the feature\ngenerator diﬃcult. Thus, Adversarial Discriminative Domain Adaptation (ADDA) (Tzeng\net al., 2017) splits the optimization of feature generator ψ and domain classiﬁer D into two\nindependent objectives, where the maximization of D remains unchanged, but the objective\nof ψ becomes\nmin\nψ Ext∼bT −log[D(zt)].\n(31)\nThis assigns small gradients for source-like target samples and larger gradients for the other\ntarget samples. It has the same ﬁxed-point properties as GRL while making the optimiza-\ntion easier for the feature generator. Although adversarial domain adaptation enhances the\nfeature transferability, i.e. the ability of feature representations to bridge the discrepancy\nacross domains, studies (Chen et al., 2019c) reveal that it is at the expense of deteriorating\nthe discriminability, i.e. the easiness of separating categories over the ﬁxed feature represen-\ntations of both domains. Spectral analysis shows that only the eigenvectors corresponding\nto the largest singular values tend to carry transferable knowledge, while other eigenvectors\nmay reﬂect domain variations and thus be overly penalized in domain adversarial training.\nHowever, these eigenvectors may convey crucial discriminative information, and thus the\ndiscriminability is weakened. To tackle this transferability-discriminability dilemma, Batch\nSpectral Penalization (BSP) (Chen et al., 2019c) penalizes the largest singular values so that\nthe other eigenvectors can be relatively strengthened to boost the feature discriminability.\nDomain Separation Network (DSN) (Bousmalis et al., 2016) introduces a private subspace\nfor each domain, which preserves domain-speciﬁc information, such as background and low-\nlevel image statistics. Then domain alignment is performed safely in the shared subspace,\nwhich is orthogonal to the private subspace responsible for the discriminative tasks.\n37\nJiang et al.\ndiscriminator\nD\nsegmentator\nsegmentation output\nD\ndiscriminator\nfeature extractor\nglobal feature\nD\ndiscriminator\nfeature extractor\ninstance feature\nD\ndiscriminator\nfeature extractor\nlocal feature\nsource domain\ntarget domain\n(a) global-level feature alignment \n(b) instance-level feature alignment \n(c) local-level feature alignment \n(d) local-level output alignment \nFigure 21: Where to adapt in diﬀerent methods. (a) DANN performs alignment on global\nfeatures. (b) DA-Faster performs alignment on both image-level and instance-level features.\n(c) SWDA performs alignment on local features. (d) Adapt-SegMap performs alignment on\nlocal outputs.\nDomain Adversarial Leanring in Real-World Scenarios.\nDomain adversarial learn-\ning has largely improved the performance on unlabeled target domain and has been widely\nadopted in many applications (Hoﬀman et al., 2016; Chen et al., 2018). However, real-world\nscenarios are much more complex. Here we list several situations that are often encountered\nyet not well resolved, and review some existing solutions to them.\nWhich part to adapt is unknown. In image recognition, we only need to classify the input.\nYet in applications such as object detection, we need to locate the Region of Interests (RoIs)\nﬁrst and then classify them. Due to the distribution shift across domains, the localization\nof RoIs on the target domain is not reliable, thus which part to adapt in the adversarial\ntraining is unknown. To alleviate this problem, as shown in Figure 21, DA-Faster (Chen\net al., 2018) performs domain alignment at both image level and instance level, where the\nimage-level alignment is believed to improve the localization of RoIs on the target domain.\nSWDA (Saito et al., 2019) argues that alignment of the local features is more eﬀective than\nthat of the global features for better localization. Although the above adversarial training\nmethods have improved the transferability of object detectors, the discriminability might\nget lost in the adversarial adaptation process as mentioned by BSP (Chen et al., 2019c).\nSince discriminability is crucial for the localization of RoIs, D-adapt (Jiang et al., 2022)\nintroduces parameter-independent category adaptor and bounding box adaptor to decouple\nadversarial adaptation from detector training, which yields sharp improvement.\nThere are structural dependencies between labels of each sample. In low-level classiﬁca-\ntion tasks, such as semantic segmentation or token classiﬁcation (Named Entity Recognition,\nParts-of-speech tagging, etc.), adaptation on features (Hoﬀman et al., 2016) may not be a\ngood option, because the feature of each pixel or each token is high-dimensional and there\n38\nTransferability in Deep Learning: A Survey\nexist many pixels or tokens in a single sample. Every coin has two sides. Compared to\nthe high-level classiﬁcation tasks, the output space of these low-level tasks contains much\nricher information of the distribution, e.g., scene layout and context, and thus adaptation\non the output space can reduce the domain shift. As shown in Figure 21, Adapt-SegMap\n(Tsai et al., 2018) trains a discriminator to distinguish whether the segmentation output is\nfrom the source or the target, while the feature generator is encouraged to generate similar\nsegmentation outputs across domains. It explicitly aligns the output distributions of target\nand source domains, and implicitly adapts the feature distributions. Further, ADVENT\n(Vu et al., 2019) minimizes the distribution distance on the self-information distributions,\nwhere the entropy of segmentation outputs is fed to the discriminator. In this way, the con-\nditional information is neglected while more attention is paid to the structural dependencies\nbetween local semantics.\n3.2.3 Hypothesis Adversarial Learning\nInevitably, there is still a gap between the proxy distance used in previous methods and\nthe hypothesis-induced discrepancies in the theories. To close this gap, Maximum Classiﬁer\nDiscrepancy (MCD) (Saito et al., 2018) starts to estimate and optimize H∆H-Divergence in\na fully parameterized way. As shown in Figure 22, MCD maximizes the discrepancy between\ntwo classiﬁers’ outputs to detect target samples far from the support of source distribution,\ni.e. estimate H∆H-Divergence. A feature generator then learns to generate target features\nnear the support to minimize the discrepancy, i.e. minimize the domain discrepancy. MCD\nuses the L1 distance to calculate the discrepancy, while Sliced Wasserstein Discrepancy\n(SWD) (Lee et al., 2019) adopts the Wasserstein metric, which is the natural geometry for\nprobability measures induced by the optimal transport theory. In theory MCD is closer to\nH∆H-Divergence, yet in experiments it is slow in convergence and very sensitive to hyper-\nparameters. The reason is that there exist two classiﬁers h and h′ in MCD that maximize\nthe discrepancy, which makes the minimax optimization hard to reach equilibrium.\nDisparity Discrepancy (DD) (Zhang et al., 2019c) provides a tighter bound by taking\nsupremum in hypothesis space H rather than H∆H. This will signiﬁcantly ease the minimax\noptimization. As shown in Figure 22, DD introduces an adversarial classiﬁer h′ sharing the\nsame hypothesis space with h. The supremum in dh,H(S, T ) is approximated by\nLDD(h, ψ) = max\nh′\nExs∼b\nSLs \u0002\nh′(ψ(xs)), h(ψ(xs))\n\u0003\n−Ext∼bT Lt \u0002\nh′(ψ(xt)), h(ψ(xt))\n\u0003\n,\n(32)\nwhere Ls and Lt are speciﬁc loss functions deﬁned on the source domain and target domain\nrespectively. Based on the theory (Zhang et al., 2019c), when the adversarial classiﬁer h′ is\nclose to the supremum, minimizing the following terms will decrease the target error ϵT ,\nmin\nψ,h E(xs,ys)∼b\nSLCE(h(ψ(xs)), ys) + λLDD(h, ψ),\n(33)\nwhere λ is a tradeoﬀhyper-parameter. An intuitive explanation is that DD is looking for an\nadversarial classiﬁer h′ that predicts correctly on the source domain while making diﬀerent\npredictions from h on the target domain. And then the feature generator ψ is encouraged\nto generate features near the decision boundary to avoid such situations.\n39\nJiang et al.\nfreeze\ngenerator\nupdate\nclassiﬁer\nMaximize discrepancy on target \ngenerator\nadversarial\nclassiﬁer\nclassiﬁer\ngradient reverse layer\nstop gradient \n(a) MCD (Step B)\n(c) MDD / DD \nfreeze\nclassiﬁer\nMinimize discrepancy on target\n(a) MCD (Step C)\nupdate\ngenerator\n!%\n1\nℎ,\nℎ-\n$#,\n%\n$#-\n%\n!%\n1\nℎ,\nℎ-\n$#,\n%\n$#-\n%\n!\n1\nℎ\nℎ′\n$#\n$#:\n%!\"#\n% ; 66\nFigure 22: (a) The training of MCD has three steps. Step A: Both the classiﬁers and the\nfeature generator are trained to classify the source samples correctly. Step B: The classiﬁers\nh1 and h2 learn to maximize the discrepancy on the target samples. (b) Step C: The feature\ngenerator ψ learns to minimize the discrepancy on the target samples. (c) DD and MDD\nintroduce an adversarial classiﬁer h′ to maximize the discrepancy and trains the feature\ngenerator ψ to minimize the source error as well as the discrepancy.\nHowever, DD is still limited to the 01 loss in the classiﬁcation setting. Based on scoring\nfunctions and margin loss, Margin Disparity Discrepancy (MDD) (Zhang et al., 2019c) goes\na crucial step forward and provides a margin theory for the multi-class classiﬁcation setting.\nThe margin ρ is attained by introducing parameter γ ≜exp ρ in the disparity discrepancy,\nLMDD(h, ψ) = max\nh′\nγ Exs∼b\nS log\n\u0002\nσh(ψ(xs))(h′(ψ(xs)))\n\u0003\n+ Ext∼bT log\n\u0002\n1 −σh(ψ(xt))(h′(ψ(xt)))\n\u0003\n,\n(34)\nwhere σ is the softmax function. A proper γ can constrain h′ in a hypothesis space of proper\nsize to avoid overestimation of the generalization bound. Note that in Equation 34, the loss\non the source domain is the standard cross-entropy, while that on the target domain is a\nmodiﬁed cross-entropy to avoid gradient vanishing and ease the optimization of h′.\nIn principle, DD can be easily extended to regression problems by replacing the classiﬁers\nin Figure 22 with regressors and choosing Ls and Lt as the L1 or L2 loss commonly used\nin regression. It has been extended to both keypoint detection (Jiang et al., 2021) and\nbounding box localization task (Jiang et al., 2022). To tackle the challenge caused by the\nhigh-dimensional output space in the keypoint detection, Regressive Domain Adaptation\n(RegDA) (Jiang et al., 2021) introduces a spatial probability distribution to describe the\nsparse density of the output space and uses it to guide the optimization of the adversarial\nregressor h′. In an expectation sense, this reduces the size of the hypothesis space of h′ and\navoids overestimation of the generalization bound in Zhang et al. (2019c).\n3.2.4 Domain Translation\nDomain translation is the task of mapping raw data of text, image, audio, and other data\nmodality from the source distribution S to the target distribution T . In domain adaptation\nproblems, we can use translation models, usually based on Generate Adversarial Networks\n(GAN) (Goodfellow et al., 2014), to obtain labeled source domain in the target style, i.e.\ntranslated into the target distribution. Training on such stylized source domain can yield\nbetter transferability than models trained on the original source domain.\n40\nTransferability in Deep Learning: A Survey\nclassiﬁer\ndiscriminator\ngenerator\n(b) Cycle Consistency\n(a) Domain Translation\n(c) Semantic Consistency\npixel space\nsemantic space\n!!\n!%\n5\nℎ∘1\n!!→#\n\"#!→#\n#!\n4\n%!\"#\n%<78\n!\n\"\n!\n\"\n#\n$\n#\n%\nFigure 23: (a) The architecture for PixelDA includes a generator network G, an adversarial\ndiscriminator D and a task-speciﬁc classiﬁer h on feature extractor ψ. (b) Cycle-consistency\nloss: after we translate from source domain to target domain, we should recover the source\ndata if translating back again. (c) Semantic-consistency loss: translating between domains\nshould not change the semantic labels of the samples, where f is the labeling function.\nGANs reason about the marginal distribution, i.e., from a random vector, the generator\nnetwork should synthesize data that resembles one that is drawn from the true distribution.\nHowever, marginal distribution is not enough for domain adaptation, thus Coupled Gen-\nerative Adversarial Networks (CoGAN) (Liu and Tuzel, 2016) learns a joint distribution\nof multi-domain images from data, i.e., from a random vector, multiple generators should\ngenerate paired data that are from diﬀerent distributions and share the same labels. By\nenforcing a weight-sharing constraint between diﬀerent generators, CoGAN learns a joint\ndistribution without the existence of corresponding images in diﬀerent domains. Then the\nshared labels of the target samples are used to train the target model.\nA more common objective of domain translation is to learn a mapping G : S →T such\nthat the generated sample G(x) is indistinguishable from the training samples of the target\ndomain. As shown in Figure 23, PixelDA (Bousmalis et al., 2017) introduces an adversarial\ndiscriminator D to distinguish between translated samples and target samples,\nLGAN(G) = max\nD Ex∼b\nS log [1 −D(G(x))] + Ex∼bT log [D(x)] .\n(35)\nThe generator G tries to synthesize samples G(x) that look similar to images from the target\ndomain by minG LGAN(G). The task-speciﬁc classiﬁer h and feature extractor ψ are trained\nsupervisedly on the target-style generated data by minψ,h E(x,y)∼b\nSLsup(h ◦ψ(G(x)), y).\nCycle Consistency.\nWhile GAN can learn a mapping between two datasets, the desired\nmapping may not be obtained. The source sample may be projected to an irrelevant target\nsample, destroying the structure or content of the original samples. Besides, multiple source\nsamples may be mapped to the same target sample, leading to the well-known problem of\nmode collapse (Goodfellow et al., 2014). Therefore, CycleGAN (Zhu et al., 2017) introduces\nan additional mapping F : T →S from target to source and adds a constraint of cycle\nconsistency to reduce the space of possible mapping functions (Figure 23). Mathematically,\ncycle consistency requires F and G to be bijections and inverse of each other. In practice,\nCycleGAN constrains F(G(x)) ≈x and G(F(x)) ≈x, which preserves the structure or\ncontent of samples to obtain more meaningful mappings. CycleGAN has been widely used\nin domain adaptation problems, such as image classiﬁcation (Hoﬀman et al., 2018), semantic\nsegmentation (Hoﬀman et al., 2018), person re-identiﬁcation (Wei et al., 2018), robotic\n41\nJiang et al.\ngrasping (Bousmalis et al., 2018), object detection (Kim et al., 2019), etc. The idea goes\nbeyond the ﬁeld of image translation and is widely used in other ﬁelds, such as unsupervised\nmachine translation (Lample et al., 2017), where the cycle consistency is also called back-\ntranslation. Unsupervised machine translation can further be used for cross-lingual domain\nadaptation tasks (Conneau et al., 2018), where a language is a domain.\nSemantic Consistency.\nCycleGAN is a general-purpose translation model for vision\ntasks and is apt at style transfer between datasets. However, it is diﬃcult for CycleGAN to\nmaintain the semantic information. It has been experimentally shown that when mapping\nfrom source to target, the problem of label ﬂipping will easily occur (Bousmalis et al., 2017;\nHoﬀman et al., 2018). As a result, there will exist a lot of noisy labels in the translated\ndataset, hurting the performance of the target model. Thus, ensuring semantic consistency\nis important for translation-based domain adaptation. Formally, given the labeling function\nf, the labels assigned to sample x should be consistent with that of the translated sample,\ni.e., f(x) = f(G(x)) (Figure 23). Since function f is not accessible, several proxy functions\nhave been proposed to approximate the semantic consistency (Taigman et al., 2017; Hoﬀman\net al., 2018; Bousmalis et al., 2018). Given a proxy function hp and a distance measure d,\nthe objective is to reduce the semantic inconsistency,\nmin\nG Lsc(G, hp) = d(hp(x), hp(G(x))).\n(36)\nDTN (Taigman et al., 2017) and SimGAN (Shrivastava et al., 2017) use the feature extractor\nas a proxy function and the goal is to translate the low-level style while keeping the high-level\nfeatures invariant. PersonGAN (Wei et al., 2018) uses the foreground crop of the person\nimage as the proxy function of the person’s identity, which ensures that a person’s identity\nremains the same before and after translation. However, the constraint on feature or pixel\nspace might be too strong, making it diﬃcult to change the low-level style. Therefore, Cycle-\nconsistent Adversarial Adaptation (CyCADA) (Hoﬀman et al., 2018) utilizes a pre-trained\nsource model as a proxy function to encourage generating samples that have consistent\npredictions under the function. This proxy function eﬀectively avoids label ﬂipping during\nthe translation of handwritten digit images, yet it is still not perfect. When faced with\nthe dataset shift of real-world problems, the predictions on the generated samples are not\nreliable and may provide incorrect guidance to G.\nWhen the domain diﬀerence is primarily low-level, such as textures, illumination, and\ncolor, translation can eﬀectively close the domain gap. But when the domain is diﬀerent at\nthe high level, such as from diﬀerent camera angles, translation may fail to adapt domains.\nTherefore, translation methods at the low-level and adaptation methods at the high-level\nmentioned in Section 3.2.1-3.2.3 are complementary and can be combined in practical appli-\ncations (Hoﬀman et al., 2018). For example, Generate to Adapt (Sankaranarayanan et al.,\n2018) directly uses the generative task as an auxiliary task to align features across domains.\n3.2.5 Semi-Supervised Learning\nUnsupervised Domain Adaptation (UDA) is closely related to Semi-Supervised Learning\n(SSL) since both of them aim at generalizing from the labeled samples to the unlabeled\nsamples. The diﬀerence is that in SSL, both the labeled and unlabeled samples come from\nthe same distribution while in UDA, the source and target distributions diﬀer. Thus, SSL\n42\nTransferability in Deep Learning: A Survey\ntasks can be considered as a special case of UDA tasks and some SSL methods can be\napplied in UDA tasks. Since there is still no theoretical guarantee for SSL methods in the\nUDA scenario, the ﬁrst question to answer is, under what assumptions can we use SSL in\nUDA? There are mainly three assumptions in SSL (Chapelle et al., 2006). (1) Smoothness\nAssumption: if two samples x1, x2 residing in a high-density region are close, then so should\nbe their corresponding outputs y1, y2 . (2) Cluster Assumption: if points are in the same\ncluster, they are likely to be of the same class. It can also be interpreted as the Low-density\nSeparation Assumption, where the decision boundary should lie in the low-density regions.\n(3) Manifold Assumption: the high-dimensional data shall lie roughly on a low-dimensional\nmanifold. Both smoothness assumption and cluster assumption are helpful for classiﬁcation,\nbut not for regression problems. Thus SSL is used more commonly in classiﬁer adaptation.\nHere we review several SSL methods applied to the UDA problems.\nConsistency Regularization encourages consistent predictions for similar data points.\nSimilar data points are generated by performing diﬀerent data augmentations on the same\ndata point. While many augmentation techniques are proposed for images, few are available\nfor other data formats, such as texts and time series. Thus this type of method is limited to\ncertain data modalities. Self-Ensemble (French et al., 2018) applies mean-teacher, a typical\nconsistency regularization method, to image domain adaptation. The teacher model, which\nis an Exponential Moving Average (EMA) of the student model, will generate predictions to\ntrain the student model on the target domain. Due to the domain shift, the predictions are\nnoisy, thus Mutual Mean-Teaching (MMT) (Ge et al., 2020) uses two collaborative networks\njointly optimized under the supervision of mutual teacher models.\nEntropy Minimization encourages the model to make conﬁdent (i.e., low-entropy)\npredictions on unlabeled data. It serves as an auxiliary term in many domain adaptation\nmethods (Long et al., 2016, 2018; Saito et al., 2018; Shu et al., 2018; Vu et al., 2019). The\nrisk is that the predictions on the target domain are not reliable, and entropy minimization\nmay hurt the performance of the model. Thus, Minimum Class Confusion (MCC) (Jin et al.,\n2020) introduces a weight for each instance, where uncertain samples have smaller weights\nto avoid minimizing entropy on the incorrectly classiﬁed samples. MCC further minimizes\nthe instance-weighted confusion between diﬀerent classes, which is simple yet frustratingly\neﬀective. Source Hypothesis Transfer (Liang et al., 2020) adopts an information maximiza-\ntion loss with a fair diversity-promoting objective, which circumvents the trivial solutions\nin entropy minimization that all unlabeled data have the same one-hot encoding.\nPseudo-Labeling produces proxy labels on unlabeled data and uses these noisy labels\ntogether with the labeled data to train the model. In self-training, a conﬁdence threshold\nis used to ﬁlter out unreliable proxy labels, which may fail in UDA since the model is likely\nto be biased towards well-transferred classes while ignoring other hard classes. Thus, Class-\nBalanced Self-Training (CBST) (Zou et al., 2018) uses a class-wise conﬁdence threshold.\nStill, large noise exists in the generated pseudo labels on the target domain, and the standard\nCross-Entropy (CE) loss has been shown to be sensitive to label noise (Zhang et al., 2017).\nTowards this problem, Zhang and Sabuncu (2018) propose the Generalized Cross-Entropy\n(GCE) loss as an eﬀective solution (Rusak et al., 2021; Liu et al., 2021a),\nLGCE(x, ˜y) = 1/q · (1 −h˜y(x)q),\n(37)\nwhere q ∈(0, 1] is a hyper-parameter to trade-oﬀbetween the CE loss and the MAE loss.\n43\nJiang et al.\n3.2.6 Remarks\nDiﬀerent domain adaptation methods are compared from several perspectives in Table 5.\nFirst, statistics matching, domain adversarial learning, and hypothesis adversarial learning\nmethods are derived from theory, enjoying theoretical guarantees while domain translation\nand semi-supervised learning methods are still in the empirical regime. Second, the former\nthree categories of methods work in the feature space or the output space and are highly re-\nlated to speciﬁc tasks, and some are tightly integrated to speciﬁc architectures. In contrast,\ntranslation methods work in the input space and are relatively independent of speciﬁc tasks.\nHowever, translation models and semi-supervised learning are dependent on speciﬁc data\nformat, and are hard to scale to diﬀerent modalities. Finally, statistics matching methods\nare based on nonparametric distances, which are data-eﬃcient but weak in expressiveness,\nthereby more suitable for low-data regimes. In contrast, domain adversarial learning and\nhypothesis adversarial learning methods are based on parametric distances, which can only\nbe measured throughout learning, but are more performant when scaling up data.\nTable 5: Comparison between diﬀerent domain adaptation methods.\nAdaptation\nPerformance1\nData\nEﬃciency2\nModality\nScalability3\nTask\nScalability4\nTheory\nGuarantee5\nStatistics Matching\n⋆\n⋆⋆⋆\n⋆⋆⋆\n⋆⋆\n⋆⋆⋆\nDomain Adversarial Learning\n⋆⋆\n⋆⋆\n⋆⋆⋆\n⋆⋆\n⋆⋆⋆\nHypothesis Adversarial Learning\n⋆⋆⋆\n⋆⋆\n⋆⋆⋆\n⋆⋆\n⋆⋆⋆\nDomain Translation\n⋆⋆\n⋆\n⋆\n⋆⋆⋆\n⋆\nSemi-Supervised Learning\n⋆⋆\n⋆⋆\n⋆⋆\n⋆\n⋆\n1 Performance: performance when there are large-scale data in source and target domains.\n2 Data Eﬃciency: performance when there are only small-scale data in source and target domains.\n3 Modality Scalability: whether can adapt the model to various modalities, such as text, time series.\n4 Task Scalability: whether can adapt the model to diﬀerent tasks, such as regression, detection.\n5 Theory Guarantee: whether the generalization error of target domain can be bounded in adaptation.\nDomain adaptation is closely related to pre-training and task adaptation. First, pre-\ntraining can boost the transferability in domain adaptation, since pre-training will reduce\nthe allowed hypothesis space and decrease the generalization bound on the target domain,\nas mentioned in Section 3.2. Thus pre-training on the source domain also serves as the\nﬁrst step in many domain adaptation methods, such as RegDA (Jiang et al., 2021). Pre-\ntraining also provides some new solutions for domain adaptation. When there exists a large\nunlabeled target domain, a feasible solution is to ﬁrst perform unsupervised pre-training on\nthe target domain, and then ﬁne-tune with the labeled data on the source domain. This is\nwidely adopted in cross-lingual adaptation (Lample and Conneau, 2019).\nWhen using pre-trained models for domain adaptation, we will also encounter the prob-\nlems in task adaptation, such as the catastrophic forgetting mentioned in 3.1.1. Thus, many\ndomain adaptation methods babysit the learning rates to avoid catastrophic forgetting\n(Long et al., 2015; Ganin and Lempitsky, 2015). Compared with task adaptation, domain\nadaptation increases the restriction on the task space, where the task of the source domain\nand that of the target domain must be the same. Due to this restriction, domain adaptation\nhas a strict theoretical guarantee. But this restriction is sometimes hard to satisfy in prac-\n44\nTransferability in Deep Learning: A Survey\ntice since we cannot ensure whether the category on the unlabeled target domain is exactly\nthe same as the source domain (Busto and Gall, 2017). Therefore, real-world adaptation is\noften a mix of task adaptation and domain adaptation. How to explore the transferability\nin such a practical open-domain scenario is a problem to be solved.\n4. Evaluation\nEvaluation serves as a means for (1) measuring the performance of diﬀerent architectures,\ndiﬀerent pre-training and adaptation methods, and (2) understanding the strengths and lim-\nitations of diﬀerent methods. This section will elaborate on the evaluation of transferability,\nwhich is deﬁned by the performance on the target task or domain. We believe that the eval-\nuation of diﬀerent methods should be performed on large-scale datasets for a practical and\nmeaningful comparison. Thus, in Section 4.1 we list some large-scale datasets that are suit-\nable for evaluating transferability in deep learning. Since diﬀerent methods are often based\non diﬀerent codebases, a fair comparison between them is rather diﬃcult. To ﬁll this blank,\nin Section 4.2 we propose an open-source library, TLlib, to better evaluate transferability of\ndiﬀerent methods in a uniﬁed framework. Finally, Section 4.3 provides several benchmarks\nfor evaluating both the cross-task transferability and cross-domain transferability.\n4.1 Datasets\nTo evaluate the transferability in deep learning, we list several datasets that are large-scale\nin the number of samples and categories, the richness of tasks, and the diversity of domains.\nThe General Language Understanding Evaluation (GLUE) (Wang et al., 2019a) is one\nof the most famous benchmarks in NLP. As shown in Table 6, it consists of nine sentence or\nsentence-pair language understanding tasks, covering a diverse range of dataset sizes, text\ngenres, and degrees of diﬃculty. It is widely used to evaluate the cross-task transferability\nof diﬀerent pre-training and task adaptation methods.\nTable 6: Descriptions and statistics of the GLUE datasets.\nCorpus\n#Train\n#Test\nMetrics\nTask\nDomain\nCoLA\n8.5k\n1k\nMatthews corr\nacceptability\nmisc.\nSST-2\n67k\n1.8k\nacc.\nsentiment\nmovie reviews\nMRPC\n3.7k\n1.7k\nacc./F1\nparaphrase\nnews\nSTS-B\n7k\n1.4k\nPearson/Spearman corr\nsentence similarity\nmisc.\nQQP\n364k\n391k\nacc./F1\nparaphrase\nsocial QA questions\nMNLI\n393k\n20k\nmatched acc./mismatched acc.\nNLI\nmisc.\nQNLI\n105k\n5.4k\nacc\nQA/NLI\nWikipedia\nRTE\n2.5k\n3k\nacc\nNLI\nnews, Wikipedia\nWNLI\n634\n146\nacc\ncoreference/NLI\nﬁction books\nIn contrast, there is no common benchmark to evaluate the transferability of diﬀerent\nmethods in computer vision. Table 7 lists some of the widely used vision datasets. Food-101,\nCIFAR-10, CIFAR-100, SUN397, Stanford Cars, FGVC Aircraft, DTD, Oxford-III Pets,\nCaltech-101, Oxford 102 Flowers are used to evaluate the transferability of diﬀerent archi-\ntectures under task discrepancy (Kornblith et al., 2019). ImageNet-R(endition) (Hendrycks\net al., 2021) and ImageNet-Sketch (Wang et al., 2019b) are two variants of the ImageNet,\n45\nJiang et al.\nmainly used to evaluate the cross-domain transferablity of diﬀerent architectures and pre-\ntraining methods. DomainNet (Peng et al., 2019) has multiple domains sharing the same\ncategory space, and is used to evaluate the cross-domain transferablity of diﬀerent domain\nadaptation methods under large domain shift.\nTable 7: Descriptions and statistics of typical vision datasets.\nDataset\n#Train\n#Test\n#Classes\nMetric\nDomain\nFood-101\n75,750\n25,250\n101\ntop-1\nphotos and real world images\nCIFAR-10\n50,000\n10,000\n10\ntop-1\nphotos and real world images\nCIFAR-100\n50,000\n10,000\n100\ntop-1\nphotos and real world images\nSUN397\n19,850\n19,850\n397\ntop-1\nphotos and real world images\nStanford Cars\n8,144\n8,041\n196\ntop-1\nphotos and real world images\nFGVC Aircraft\n6,667\n3,333\n100\nmean\nper-class\nphotos and real world images\nDescribable\nTextures (DTD)\n3,760\n1,880\n47\ntop-1\nphotos and real world images\nOxford-III Pets\n3,680\n3,369\n37\nmean\nper-class\nphotos and real world images\nCaltech-101\n3,060\n6,084\n102\nmean\nper-class\nphotos and real world images\nOxford 102 Flowers\n2,040\n6,149\n102\nmean\nper-class\nphotos and real world images\nImageNet-R\n-\n30k\n200\ntop-1\nart, cartoons, deviantart, graﬃti,\nembroidery, graphics, origami,\npaintings, patterns, plastic objects,\nplush objects, sculptures, sketches,\ntattoos, toys, and video games\nImageNet-Sketch\n-\n50k\n1000\ntop-1\nsketch\nDomainNet-c\n33,525\n14,604\n365\ntop-1\nclipart images\nDomainNet-p\n50,416\n21,850\n365\ntop-1\nartistic paintings\nDomainNet-r\n120,906\n52,041\n365\ntop-1\nphotos and real world images\nDomainNet-s\n48,212\n20,916\n365\ntop-1\nsketch\n4.2 Library\nTo make up for the lack of a uniﬁed codebase in some areas, we propose an open and ongoing\nlibrary, TLlib. This library implements many representative adaptation algorithms in a uni-\nﬁed way, allowing quantitative, fair, reproducible comparisons between diﬀerent algorithms\nand promoting seamless integration of diﬀerent pre-training or adaptation methods.\nLibrary Usage.\nFirst, we give a short description of how to use TLlib using DANN as an\ninstance. In the original implementation of DANN, the domain adversarial loss, domain dis-\ncriminator, feature generator, and classiﬁer are tightly coupled together in one nn.Module,\nwhich causes the diﬃculty of reuse, e.g., the entire algorithm needs re-implementation when\nthe input data is changed from image to text. Yet in this case, the domain adversarial loss\nand the domain discriminator remain unchanged and shall be reused. Therefore, in TLlib,\nmodels and loss functions are decoupled. When using DANN for any case, users need only\nto initialize a domain discriminator and pass it to the domain adversarial loss module, and\nthen use this module in the same way as the cross-entropy loss module deﬁned in PyTorch\n(example code below). TLlib provides friendly and coherent APIs for supported algorithms.\nDetailed usages of these algorithms can be found at the documentation.\n46\nTransferability in Deep Learning: A Survey\n>>> # define the domain discriminator\n>>> from dalib.modules.domain_discriminator import DomainDiscriminator\n>>> discriminator = DomainDiscriminator(in_feature=1024, hidden_size=1024)\n>>> # define the domain adversarial loss module\n>>> from dalib.adptation.dann import DomainAdversarialLoss\n>>> dann = DomainAdversarialLoss(discriminator, reduction=’mean’)\n>>> # features from the source and target domain\n>>> f_s, f_t = torch.randn(20, 1024), torch.randn(20, 1024)\n>>> # calculate the final loss\n>>> loss = dann(f_s, f_t)\nDesign Philosophy.\nTLlib is designed to be extendible by researchers and simple for\npractitioners. Currently, there are mainly two types of algorithm implementations. One is to\nencapsulate each algorithm in a Trainer, whose typical representative is PyTorch-Lighting.\nUsers only need to feed the training data to it and do not need to care about the speciﬁc\ntraining process. Another strategy is to encapsulate the core loss function in each algo-\nrithm, and users need to implement the complete training process by themselves. A typical\nrepresentative is PyTorch (Paszke et al., 2019). Although the former method is easier to\nuse, it is less extendible. Since it is often necessary to adjust the training process in diﬀerent\ntransfer learning scenarios, TLlib adopts the latter method for better extendibility. We try\nour best to make TLlib easy to start with, e.g., we support the automatic download of most\ncommon transfer learning datasets so that users do not need to spend time on data prepa-\nration. Our code is in PyTorch-style and we provide training examples of diﬀerent transfer\nalgorithms in diﬀerent scenarios, which allows users to quickly adapt to TLlib as long as\nthey have learned PyTorch before. For more convenient algorithm selection, we provide a\ncomprehensive benchmark among all those libraries. For faster algorithm reproduction, we\nprovide training scripts for all the results in the benchmark.\nTLlib is released under the MIT License and available at https://github.com/thuml/\nTransfer-Learning-Library. Documentation and tutorials are available on its website.\n4.3 Benchmark\nThis section will present a benchmark of typical pre-training and adaptation methods on\nthe large-scale datasets described in Section 4.1. Since such a benchmark is missing in the\nliterature, we produce the results using the open library TLlib implemented in Section 4.2.\n4.3.1 Pre-Training\nProtocols.\nThe transferability of pre-training methods is evaluated on the target task,\nwhere the adaptation process and data augmentations are kept the same for fair comparison.\nHyper-parameters in adaptation are selected by the performance of target validation data.\nResults.\nFor the pre-training methods, the transferability cross diﬀerent tasks and across\ndiﬀerent domains should be evaluated. Tables 8 and 9 list the performance on various down-\nstream tasks with diﬀerent architectures and pre-training tasks. It can be concluded that\narchitectures and pre-training methods have a great impact on the cross-task transferability\nof deep networks. Table 10 lists the performance on ImageNet-Sketch and ImageNet-R with\n47\nJiang et al.\ndiﬀerent architectures and pre-training tasks. Architectures and pre-training strategies also\ngreatly inﬂuence the cross-domain transferability in deep learning.\nTable 8: Cross-task transferability benchmark. Results of diﬀerent architectures and pre-\ntraining methods are reported from the GLUE leaderboard. BiLSTM+ELMo (Peters et al.,\n2018) serves as the baseline. GPT (Radford et al., 2018), BERTLarge (Devlin et al., 2019), T5\n(Raﬀel et al., 2020), and ERNIE (Sun et al., 2019b) have diﬀerent architectures. RoBERTa\n(Liu et al., 2019c), XLM (Lample and Conneau, 2019), and SpanBERT (Joshi et al., 2020)\nshare the same architecture as BERTLarge but employ diﬀerent pre-training methods.\nModel\nCoLA\nSST-2\nMRPC\nSTS-B\nQQP\nMNLIm\nMNLImm\nQNLI\nRTE\nAvg\nHuman\nBaselines\n66.4\n97.8\n86.3\n92.7\n80.4\n92\n92.8\n91.2\n93.6\n88.1\nBiLSTM\n+ELMo\n36.0\n90.4\n84.9\n73.3\n64.8\n76.4\n76.1\n79.9\n56.8\n71.0\nGPT\n45.4\n91.3\n82.3\n80.0\n70.3\n82.1\n81.4\n88.1\n56.0\n75.2\nBERTLarge\n60.5\n94.9\n89.3\n86.5\n72.1\n86.7\n85.9\n92.7\n70.1\n82.1\nT5\n71.6\n97.5\n92.8\n93.1\n90.6\n92.2\n91.9\n96.9\n92.8\n91.0\nERNIE\n75.5\n97.8\n93.9\n93.0\n90.9\n92.3\n91.7\n97.3\n92.6\n91.7\nRoBERTa\n67.8\n96.7\n92.3\n92.2\n90.2\n90.8\n90.2\n95.4\n88.2\n89.3\nXLM\n62.9\n95.6\n90.7\n88.8\n89.8\n89.1\n88.5\n94.0\n76.0\n86.2\nSpanBERT\n64.3\n94.8\n90.9\n89.9\n89.5\n88.1\n87.7\n94.3\n79.0\n86.5\nTable 9: Cross-task transferability benchmark. Results on image recognition using diﬀerent\npre-training methods, including SimCLR (Chen et al., 2020) and BYOL (Grill et al., 2020).\nModel\nPre-Training Food CIFAR10 CIFAR100 SUN397 Cars Aircraft DTD Pets Caltech101 Flowers Avg\nResNet50\nRandom Init 86.9\n95.9\n80.2\n53.6\n91.4\n85.9\n64.8\n81.5\n72.6\n92.0\n80.5\nSimCLR\n88.2\n97.7\n85.9\n63.5\n91.3\n88.1\n73.2\n89.2\n92.1\n97.0\n86.6\nBYOL\n88.5\n97.8\n86.1\n63.7\n91.6\n88.1\n76.2\n91.7\n93.8\n97.0\n87.5\nResNet50\nSupervised\nPre-Trained\non ImageNet\n87.8\n96.8\n84.5\n64.7\n91.7\n86.6\n75.2\n92.5\n91.8\n97.5\n86.9\nResNet101\n87.6\n97.7\n87.0\n64.8\n91.7\n85.6\n75.4\n94.0\n93.1\n97.9\n87.5\nResNet152\n87.6\n97.9\n87.6\n66.0\n92.0\n85.3\n74.9\n94.5\n93.2\n97.4\n87.6\nTable 10: Cross-domain transferability benchmark. Results are reported from the PyTorch-\nImage-Models (Wightman, 2019) on ImageNet using diﬀerent architectures and pre-training\nmethods. SSP refers to semi-supervised pre-training on YFCC100M (Yalniz et al., 2019).\nWSP refers to weakly supervised pre-training on IG-1B-Targeted (Mahajan et al., 2018).\nModel\nPre-Training\nParam Count\nImageNet-Sketch\nImageNetR\ntop-1\ntop-5\ntop-1\ntop-5\nResNet50\nStandard\nPre-Trained\non ImageNet\n25.6\n29.6\n46.8\n40.4\n54.7\nResNet152d\n60.2\n37.9\n58.4\n49.3\n64.4\nViTlarge, patch16\n304.3\n51.8\n73.7\n64.3\n76.2\nResNext101 32x8d\nStandard\n88.8\n29.4\n48.5\n42.6\n58.3\nSSP\n34.1\n55.6\n49.2\n65.5\nWSP\n54.9\n77.5\n75.9\n86.2\nSSP+WSP\n56.4\n78.9\n75.6\n87.1\n48\nTransferability in Deep Learning: A Survey\n4.3.2 Task Adaptation\nProtocols.\nWe follow the common practice in the community as described in Kornblith\net al. (2019). Training iterations and data augmentations are kept the same for diﬀerent\ntask adaptation methods for a fair comparison. Hyper-parameters, such as learning rate\nand weight decay, of each method are selected by the performance on target validation data.\nResults.\nWe mainly investigate the cross-task transferability between diﬀerent task adap-\ntation methods. Tables 11 and 12 compare the performance of downstream tasks with dif-\nferent task adaptation methods. Note that previous methods usually only report results\non individual datasets, such as Aircraft and Stanford Cars, where regularization tuning\nperforms better than vanilla ﬁne-tuning by a large margin. But the average improvements\nbrought by diﬀerent task adaptation methods on a large number of datasets are still limited.\nThus, we can conclude that the eﬀectiveness of diﬀerent task adaptation algorithms largely\ndepends on the relatedness between the target task and the pre-training task.\nTable 11: Cross-task transferability benchmark. GLUE performance with diﬀerent task\nadaptation methods, including SMART (Jiang et al., 2020), Adapter-Tuning (Houlsby et al.,\n2019) and DiﬀPruning (Guo et al., 2021). Results are reported from their original papers.\nModel\nTask\nAdaptation\nNew Params\nPer Task\nCoLA SST-2 MRPC STS-B QQP MNLIm MNLImm QNLI RTE Avg\nRoBERTa\nvanilla\n100%\n67.8\n96.7\n92.3\n92.2\n90.2\n90.8\n90.2\n95.4\n88.2 89.3\nSMART\n100%\n65.1\n97.5\n93.7\n92.9\n90.1\n91.0\n90.8\n95.4\n87.9 89.4\nBERTLarge\nvanilla\n100%\n60.5\n94.9\n89.3\n86.5\n72.1\n86.7\n85.9\n92.7\n70.1 82.1\nAdapter\n2.10%\n59.2\n94.3\n88.7\n87.3\n89.4\n85.4\n85.0\n92.4\n71.6 83.7\nDiﬀPruning\n0.50%\n61.1\n94.1\n89.7\n86.0\n-\n86.4\n86.0\n93.3\n70.6\n-\nTable 12: Cross-task transferability benchmark. Accuracy (%) on image classiﬁcation with\ndiﬀerent task adaptation methods: LWF (Li and Hoiem, 2018), DELTA (Li et al., 2019),\nBSS (Chen et al., 2019b), Bi-Tuning (Zhong et al., 2020). Results are reproduced by TLlib.\nTask\nAdaptation\nFood\nCIFAR10\nCIFAR100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech101\nFlowers\nAvg\nResNet50\n85.1\n96.9\n84.1\n80.7\n87.8\n80.1\n74.4\n93.2\n92.9\n96.5\n87.2\nLWF\n83.9\n96.5\n83.6\n79.5\n87.4\n82.2\n76.3\n94.0\n91.7\n97.1\n87.2\nDELTA\n83.8\n95.9\n83.7\n73.6\n88.1\n82.3\n75.6\n94.2\n92.5\n97.0\n86.7\nBSS\n85.0\n96.6\n84.2\n80.4\n88.4\n81.8\n74.3\n93.3\n93.0\n96.6\n87.4\nBi-Tuning\n85.7\n97.1\n84.3\n80.7\n90.3\n84.8\n74.6\n93.5\n93.4\n97.5\n88.2\n4.3.3 Domain Adaptation\nProtocols.\nWe follow the standard protocols for unsupervised domain adaptation (Long\net al., 2015; Ganin and Lempitsky, 2015).\nTraining iterations and data augmentations\nare kept the same for diﬀerent methods for a fair comparison. For each method, hyper-\nparameters are selected on one task and then kept the same for all other tasks, requiring the\nhyper-parameters of each method to transfer across tasks. This selection strategy is more\n49\nJiang et al.\nexecutable than the importance-weighted cross-validation (Sugiyama et al., 2007) and can\nbe applied to various practical applications, thus it is widely adopted by many competitions.\nResults.\nTables 13 and 14 give the classiﬁcation performance of diﬀerent domain adapta-\ntion methods on DomainNet and ImageNet. We ﬁnd that many state-of-the-art methods on\nsmall datasets do not perform well on large-scale datasets. This ﬁeld shall pay more atten-\ntion to improving the cross-domain transferability of deep models on large-scale datasets.\nTable 13: Cross-domain transferability benchmark. Accuracy (%) for unsupervised domain\nadaptation on DomainNet. Results are reproduced from TLlib.\nDomainNet\nc\u0001p\nc\u0001r\nc\u0001s\np\u0001c\np\u0001r\np\u0001s\nr\u0001c\nr\u0001p\nr\u0001s\ns\u0001c\ns\u0001p\ns\u0001r\nAvg\nResNet101\n32.7\n50.6\n39.4\n41.1\n56.8\n35.0\n48.6\n48.8\n36.1\n49.0\n34.8\n46.1\n43.3\nDAN (2015)\n38.8\n55.2\n43.9\n45.9\n59.0\n40.8\n50.8\n49.8\n38.9\n56.1\n45.9\n55.5\n48.4\nDANN (2016)\n37.9\n54.3\n44.4\n41.7\n55.6\n36.8\n50.7\n50.8\n40.1\n55.0\n45.0\n54.5\n47.2\nADDA (2017)\n38.4\n54.1\n44.1\n43.5\n56.7\n39.2\n52.8\n51.3\n40.9\n55.0\n45.4\n54.5\n48.0\nJAN (2017)\n40.5\n56.7\n45.1\n47.2\n59.9\n43.0\n54.2\n52.6\n41.9\n56.6\n46.2\n55.5\n50.0\nCDAN (2018)\n40.4\n56.8\n46.1\n45.1\n58.4\n40.5\n55.6\n53.6\n43.0\n57.2\n46.4\n55.7\n49.9\nMCD (2018)\n37.5\n52.9\n44.0\n44.6\n54.5\n41.6\n52.0\n51.5\n39.7\n55.5\n44.6\n52.0\n47.5\nMDD (2019c)\n42.9\n59.5\n47.5\n48.6\n59.4\n42.6\n58.3\n53.7\n46.2\n58.7\n46.5\n57.7\n51.8\nTable 14: Cross-domain transferability benchmark. Accuracy (%) for unsupervised domain\nadaptation on ImageNet-scale datasets. Results are reproduced from TLlib.\nTask\nImageNet\u0001ImageNet-R\nImageNet\u0001ImageNet-Sketch\nModel\nResNet50\nig resnext101 32x8d\nSource Only\n35.6\n54.9\nDAN (Long et al., 2015)\n39.8\n55.7\nDANN (Ganin et al., 2016)\n52.7\n56.5\nJAN (Long et al., 2017)\n41.7\n55.7\nCDAN (Long et al., 2018)\n53.9\n58.2\nMCD (Saito et al., 2018)\n46.7\n55.0\nMDD (Zhang et al., 2019c)\n56.2\n62.4\n5. Conclusion\nIn this paper, we investigate how to acquire and apply transferability in the whole lifecycle\nof deep learning. In the pre-training section, we focus on how to improve the transferability\nof the pre-trained models by designing architecture, pre-training task, and training strategy.\nIn the task adaptation section, we discuss how to better preserve and utilize the transferable\nknowledge to improve the performance of target tasks. In the domain adaptation section, we\nillustrate how to bridge the domain gap to increase the transferability for real applications.\nThis survey connects many isolated areas with their relation to transferability and provides\na uniﬁed perspective to explore transferability in deep learning. We expect this study will\nattract the community’s attention to the fundamental role of transferability in deep learning.\nAcknowledgments\nThis work was supported by the NSFC Grants (62022050 and 62021002), the Beijing Nova\nProgram (Z201100006820041), and the BNRist Innovation Fund (BNR2021RC01002).\n50\nTransferability in Deep Learning: A Survey\nReferences\nSamira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the\nlimits of large scale pre-training. In ICLR, 2022.\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains\nthe eﬀectiveness of language model ﬁne-tuning. In ACL, 2021.\nDario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Bat-\ntenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al.\nDeep speech 2: End-to-end speech recognition in english and mandarin. In ICML, 2016.\nMartin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz.\nInvariant risk\nminimization. arXiv preprint arXiv:1907.02893, 2019.\nSanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and\nequilibrium in generative adversarial nets (GANs). In ICML, 2017.\nPeter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk\nbounds and structural results. In JMLR, 2002.\nIz Beltagy, Kyle Lo, and Arman Cohan. Scibert: Pretrained language model for scientiﬁc\ntext. In EMNLP, 2019.\nS. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory\nof learning from diﬀerent domains. Machine Learning, 79, page 151–175, 2010a.\nShai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of repre-\nsentations for domain adaptation. In NeurIPS, 2006.\nShai Ben-David, Tyler Lu, Teresa Luu, and David Pal. Impossibility theorems for domain\nadaptation. In AISTATS, pages 129–136, 2010b.\nYoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In\nICML workshop, 2012.\nYoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise\ntraining of deep networks. In NeurIPS, 2007.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review\nand new perspectives. TPAMI, 35(8):1798–1828, 2013.\nYoshua Bengio, Yann Lecun, and Geoﬀrey Hinton. Deep learning for ai. Communications\nof the ACM, 64(7):58–65, 2021.\nKonstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Du-\nmitru Erhan. Domain separation networks. In NeurIPS, 2016.\nKonstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Kr-\nishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks.\nIn CVPR, 2017.\n51\nJiang et al.\nKonstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal\nKalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, Sergey Levine,\nand Vincent Vanhoucke. Using simulation and domain adaptation to improve eﬃciency\nof deep robotic grasping. In ICRA, 2018.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. In NeurIPS, 2020.\nPau Panareda Busto and Juergen Gall. Open set domain adaptation. In ICCV, 2017.\nRich Caruana. Multitask learning. Technical report, 1997.\nOlivier Chapelle, Bernhard Sch¨olkopf, and Alexander Zien.\nSemi-Supervised Learning\n(Adaptive Computation and Machine Learning). The MIT Press, 2006. ISBN 0262033585.\nMinmin Chen, Zhixiang Xu, Kilian Q. Weinberger, and Fei Sha. Marginalized denoising\nautoencoders for domain adaptation. In ICML, 2012.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A simple frame-\nwork for contrastive learning of visual representations. In ICML, 2020.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A\ncloser look at few-shot classiﬁcation. In ICLR, 2019a.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR,\n2021.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised\nvision transformers. arXiv preprint arXiv:2104.02057, 2021a.\nXinyang Chen, Sinan Wang, Bo Fu, Mingsheng Long, and Jianmin Wang. Catastrophic\nforgetting meets negative transfer: Batch spectral shrinkage for safe transfer learning. In\nNeurIPS, 2019b.\nXinyang Chen, Sinan Wang, Mingsheng Long, and Jianmin Wang.\nTransferability vs.\ndiscriminability: Batch spectral penalization for adversarial domain adaptation. In ICML,\n2019c.\nXinyang Chen, Sinan Wang, Jianmin Wang, and Mingsheng Long. Representation subspace\ndistance for domain adaptation regression. In ICML, 2021b.\nYuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive\nfaster R-CNN for object detection in the wild. In CVPR, 2018.\nKyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi\nBougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using\nRNN encoder–decoder for statistical machine translation. In EMNLP, 2014.\n52\nTransferability in Deep Learning: A Survey\nAlexandra Chronopoulou, Christos Baziotis, and Alexandros Potamianos. An embarrass-\ningly simple approach for transfer learning from pretrained language models. In NAACL,\n2019.\nAlexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman,\nHolger Schwenk, and Veselin Stoyanov. XNLI: evaluating cross-lingual sentence repre-\nsentations. In EMNLP, 2018.\nNicolas Courty, R´emi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distri-\nbution optimal transportation for domain adaptation. In NeurIPS, 2017.\nYin Cui, Yang Song, Chen Sun, Andrew Howard, and Serge Belongie. Large scale ﬁne-\ngrained categorization and domain-speciﬁc transfer learning. In CVPR, pages 4109–4118,\n2018.\nBharath Bhushan Damodaran, Benjamin Kellenberger, R´emi Flamary, Devis Tuia, and\nNicolas Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised\ndomain adaptation. In ECCV, 2018.\nMatthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis,\nGreg Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting\nin classiﬁcation tasks. TPAMI, page 1–20, 2021.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In CVPR, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training\nof deep bidirectional transformers for language understanding. In NAACL, 2019.\nCarl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation\nlearning by context prediction. In ICCV, 2015.\nJeﬀDonahue, Yangqing Jia, Oriol Vinyals, Judy Hoﬀman, Ning Zhang, Eric Tzeng, and\nTrevor Darrell. Decaf: A deep convolutional activation feature for generic visual recog-\nnition. In ICML, 2014.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain\nGelly, Jakob Uszkoreit, and Neil Houlsby.\nAn Image is Worth 16x16 Words: Trans-\nformers for Image Recognition at Scale. In ICLR, 2021.\nChelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-agnostic meta-learning for fast\nadaptation of deep networks. In ICML, 2017.\nGeoﬀrey French, Michal Mackiewicz, and Mark H. Fisher.\nSelf-ensembling for domain\nadaptation. In ICLR, 2018.\nYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropaga-\ntion. In ICML, 2015.\n53\nJiang et al.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle,\nFran¸cois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of\nneural networks. JMLR, 17(59):1–35, 2016.\nVictor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In ICLR,\n2018.\nYixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-teaching: Pseudo label reﬁnery\nfor unsupervised domain adaptation on person re-identiﬁcation. In ICLR, 2020.\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann,\nand Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape\nbias improves accuracy and robustness. In ICLR, 2019.\nRoss Girshick, JeﬀDonahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies\nfor accurate object detection and semantic segmentation. In CVPR, 2014.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\nDomain adaptation for large-scale\nsentiment classiﬁcation: A deep learning approach. In ICML, 2011.\nBoqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic ﬂow kernel for unsuper-\nvised domain adaptation. In CVPR, 2012.\nBoqing Gong, Kristen Grauman, and Fei Sha. Connecting the dots with landmarks: Dis-\ncriminatively learning domain-invariant features for unsupervised domain adaptation. In\nICML, 2013.\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. In NeurIPS,\n2014.\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy.\nExplaining and harnessing\nadversarial examples. 2015.\nAnirudh Goyal, Alex Lamb, Jordan Hoﬀmann, Shagun Sodhani, Sergey Levine, Yoshua\nBengio, and Bernhard Sch¨olkopf. Recurrent independent mechanisms. In ICLR, 2021.\nArthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch¨olkopf, and Alexan-\nder Smola. A kernel two-sample test. JMLR, 13(25):723–773, 2012a.\nArthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimil-\niano Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for\nlarge-scale two-sample tests. In NeurIPS, 2012b.\nJean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\nlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap\nyour own latent - a new approach to self-supervised learning. In NeurIPS, 2020.\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR,\n2021.\n54\nTransferability in Deep Learning: A Survey\nDemi Guo, Alexander Rush, and Yoon Kim. Parameter-eﬃcient transfer learning with diﬀ\npruning. In ACL, 2021.\nYunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and Rogerio\nFeris. Spottune: transfer learning through adaptive ﬁne-tuning. In CVPR, 2019.\nSuchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug\nDowney, and Noah A. Smith. Don’t stop pretraining: Adapt language models to domains\nand tasks. In ACL, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In CVPR, 2016.\nKaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick. Mask r-cnn. In ICCV,\n2017.\nKaiming He, Ross Girshick, and Piotr Doll´ar. Rethinking imagenet pre-training. In ICCV,\n2019.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast\nfor unsupervised visual representation learning. In CVPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked\nautoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan\nDorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Stein-\nhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-\ndistribution generalization. ICCV, 2021.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,\nAdam Trischler, and Yoshua Bengio. Learning deep representations by mutual informa-\ntion estimation and maximization. In ICLR, 2019.\nJudy Hoﬀman, Dequan Wang, Fisher Yu, and Trevor Darrell. Fcns in the wild: Pixel-level\nadversarial and constraint-based adaptation. 2016.\nJudy Hoﬀman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei\nEfros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In\nICML, 2018.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Larous-\nsilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-eﬃcient transfer\nlearning for NLP. In ICML, 2019.\nJeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\nﬁcation. In ACL, 2018.\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and\nJure Leskovec. Pre-training graph neural networks. In ICLR, 2020.\n55\nJiang et al.\nJiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Sch¨olkopf, and Alex Smola.\nCorrecting sample selection bias by unlabeled data. In NeurIPS, 2007.\nSergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network train-\ning by reducing internal covariate shift. In ICML, 2015.\nYunhun Jang, Hankook Lee, Sung Ju Hwang, and Jinwoo Shin. Learning what and where\nto transfer. In ICML, 2019.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao.\nSMART: robust and eﬃcient ﬁne-tuning for pre-trained natural language models through\nprincipled regularized optimization. In ACL, 2020.\nJunguang Jiang, Yifei Ji, Ximei Wang, Yufeng Liu, Jianmin Wang, and Mingsheng Long.\nRegressive domain adaptation for unsupervised keypoint detection. In CVPR, 2021.\nJunguang Jiang, Baixu Chen, Jianmin Wang, and Mingsheng Long. Decoupled adaptation\nfor cross-domain object detection. In ICLR, 2022.\nYing Jin, Ximei Wang, Mingsheng Long, and Jianmin Wang. Minimum class confusion for\nversatile domain adaptation. In ECCV, 2020.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy.\nSpanBERT: Improving pre-training by representing and predicting spans. In TACL, 2020.\nGuoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation\nnetwork for unsupervised domain adaptation. In CVPR, 2019.\nTaekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, and Changick Kim. Diversify\nand match: A domain adaptive representation learning paradigm for object detection. In\nCVPR, 2019.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,\nAndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-\nBarwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114\n(13):3521–3526, 2017.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain\nGelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In\nECCV, 2020.\nSimon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer\nbetter? In CVPR, 2019.\nZhi Kou, Kaichao You, Mingsheng Long, and Jianmin Wang. Stochastic normalization. In\nNeurIPS, 2020.\nAlex Krizhevsky, Ilya Sutskever, and Geoﬀrey E. Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In NeurIPS, 2012.\n56\nTransferability in Deep Learning: A Survey\nGuillaume Lample and Alexis Conneau.\nCross-lingual language model pretraining.\nIn\nNeurIPS, 2019.\nGuillaume Lample, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised machine\ntranslation using monolingual corpora only. In ICLR, 2017.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and\nRadu Soricut. Albert: A lite bert for self-supervised learning of language representations.\nIn ICLR, 2020.\nYann LeCun, Yoshua Bengio, and Geoﬀrey Hinton.\nDeep learning.\nNature, 521(7553):\n436–444, 2015.\nChen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasser-\nstein discrepancy for unsupervised domain adaptation. In CVPR, 2019.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Eﬀective regularization to\nﬁnetune large-scale pretrained language models. In ICLR, 2020a.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for\nbiomedical text mining. Bioinformatics, 36(4):1234–1240, 2020b.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising Sequence-to-\nSequence Pre-training for Natural Language Generation, Translation, and Comprehen-\nsion. In ACL, 2020.\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic\ndimension of objective landscapes. In ICLR, 2018.\nXiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for genera-\ntion. In ACL, 2021.\nXingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, Zeyu Chen, and Jun\nHuan. Delta: Deep learning transfer using feature map with attention for convolutional\nnetworks. In ICLR, 2019.\nYanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch\nnormalization for practical domain adaptation. In ICLR Workshop, 2017.\nZhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 40(12):2935–2947,\n2018.\nJian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data?\nsource hypothesis transfer for unsupervised domain adaptation. In ICML, 2020.\nHong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation.\nIn NeurIPS, 2021a.\nMing-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In NeurIPS, 2016.\n57\nJiang et al.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neu-\nbig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural\nlanguage processing, 2021b.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural\nnetworks for natural language understanding. In ACL, 2019a.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:1907.11692, 2019b.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nBERT pretraining approach. 2019c.\nMingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer\nfeature learning with joint distribution adaptation. In ICCV, 2013.\nMingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable\nfeatures with deep adaptation networks. In ICML, 2015.\nMingsheng Long, Jianmin Wang, and Michael I. Jordan. Unsupervised domain adaptation\nwith residual transfer networks. In NeurIPS, 2016.\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning\nwith joint adaptation networks. In ICML, 2017.\nMingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Conditional adver-\nsarial domain adaptation. In NeurIPS, 2018.\nMingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transfer-\nable representation learning with deep adaptation networks. TPAMI, 41(12):3071–3085,\n2019.\nChristos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks\nthrough l 0 regularization. In ICLR, 2018.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yix-\nuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly\nsupervised pretraining. In ECCV, 2018.\nArun Mallya and Svetlana Lazebnik. Piggyback: Adding multiple tasks to a single, ﬁxed\nnetwork by learning to mask. In ECCV, 2018.\nYishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning\nbounds and algorithms. In COLT, 2009.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In ICML, 2017.\n58\nTransferability in Deep Learning: A Survey\nJiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc V Le, and Ruom-\ning Pang.\nDomain adaptive transfer learning with specialist models.\narXiv preprint\narXiv:1811.07056, 2018.\nCuong Nguyen, Tal Hassner, Matthias Seeger, and Cedric Archambeau.\nLeep: A new\nmeasure to evaluate transferability of learned representations. In ICML, 2020.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. NeurIPS, 2019.\nSinno Jialin Pan and Qiang Yang. A survey on transfer learning. TKDE, pages 1345–1359,\n2010.\nSinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation\nvia transfer component analysis. TNNLS, pages 199–210, 2011.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, An-\ndreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank\nChilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An\nimperative style, high-performance deep learning library. In NeurIPS, 2019.\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment\nmatching for multi-source domain adaptation. In ICCV, 2019.\nJonas Peters, Peter B¨uhlmann, and Nicolai Meinshausen. Causal inference by using invari-\nant prediction: identiﬁcation and conﬁdence intervals. Journal of the Royal Statistical\nSociety. Series B (Statistical Methodology), pages 947–1012, 2016.\nJonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf. Elements of causal inference:\nfoundations and learning algorithms. The MIT Press, 2017.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\nLee, and Luke Zettlemoyer. Deep contextualized word representations. In NAACL, 2018.\nTelmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual bert? In\nACL, 2019.\nJ. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset shift in\nmachine learning. The MIT Press, 2009.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. Technical report, OpenAI, 2018.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision. In ICML, 2021.\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer. JMLR, 21(140):1–67, 2020.\n59\nJiang et al.\nAniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals.\nRapid learning or\nfeature reuse? towards understanding the eﬀectiveness of maml. In ICLR, 2020.\nMaithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Under-\nstanding transfer learning for medical imaging. In NeurIPS, 2019.\nS-A Rebuﬃ, H. Bilen, and A. Vedaldi.\nLearning multiple visual domains with residual\nadapters. In NeurIPS, 2017.\nIevgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Youn`es Bennani. A\nsurvey on domain adaptation theory: learning bounds and theoretical guarantees, 2020.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\nobject detection with region proposal networks. In NeurIPS, 2015.\nMichael T. Rosenstein. To transfer or not to transfer. In NeurIPS, 2005.\nEvgenia. Rusak, Steﬀen Schneider, Peter Gehler, Oliver Bringmann, Wieland Brendel, and\nMatthias Bethge. Adapting imagenet-scale models to complex distribution shifts with\nself-learning. arXiv preprint arXiv:2104.12928, 2021.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhi-\nheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg,\nand Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 115(3):211–\n252, 2015.\nAndrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon\nOsindero, and Raia Hadsell. Meta-learning with latent embedding optimization. In ICLR,\n2019.\nKuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classi-\nﬁer discrepancy for unsupervised domain adaptation. In CVPR, 2018.\nKuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. Strong-weak distri-\nbution alignment for adaptive object detection. In CVPR, 2019.\nHadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do\nadversarially robust imagenet models transfer better? In NeurIPS, 2020.\nSwami Sankaranarayanan, Yogesh Balaji, Carlos D. Castillo, and Rama Chellappa. Gener-\nate to adapt: Aligning domains using generative adversarial networks. In CVPR, 2018.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lilli-\ncrap. Meta-learning with memory-augmented neural networks. In ICML, 2016.\nTimo Schick and Hinrich Sch¨utze. Exploiting cloze questions for few-shot text classiﬁcation\nand natural language inference. In EACL, 2020.\nJ¨urgen Schmidhuber. Evolutionary principles in self-referential learning. PhD thesis, Tech-\nnische Universit¨at M¨unchen, 1987.\n60\nTransferability in Deep Learning: A Survey\nBernhard Sch¨olkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris\nMooij. On causal and anticausal learning. In ICML, 2012.\nBernhard Sch¨olkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalch-\nbrenner, Anirudh Goyal, and Yoshua Bengio.\nToward causal representation learning.\nProceedings of the IEEE, 109(5):612–634, 2021.\nAndrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim\nGreen, Chongli Qin, Augustin ˇZ´ıdek, Alexander WR Nelson, Alex Bridgland, et al. Im-\nproved protein structure prediction using potentials from deep learning.\nNature, 577\n(7792):706–710, 2020.\nPierre Sermanet, David Eigen, Xiang Zhang, Micha¨el Mathieu, Rob Fergus, and Yann\nLeCun. Overfeat: Integrated recognition, localization and detection using convolutional\nnetworks. arXiv preprint arXiv:1312.6229, 2013.\nAshish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russell\nWebb. Learning from simulated and unsupervised images through adversarial training.\nIn CVPR, 2017.\nRui Shu, Hung H. Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsuper-\nvised domain adaptation. In ICLR, 2018.\nYang Shu, Zhangjie Cao, Jinghan Gao, Jianmin Wang, and Mingsheng Long. Omni-training\nfor data-eﬃcient deep learning. arXiv preprint arXiv:2110.07510, 2021a.\nYang Shu, Zhi Kou, Zhangjie Cao, Jianmin Wang, and Mingsheng Long.\nZoo-tuning:\nAdaptive transfer from a zoo of models. In ICML, 2021b.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van\nDen Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc\nLanctot, et al. Mastering the game of go with deep neural networks and tree search.\nNature, 529(7587):484–489, 2016.\nJake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learn-\ning. In NeurIPS, 2017.\nBharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Sch¨olkopf, and\nGert R. G. Lanckriet. Hilbert space embeddings and metrics on probability measures.\nJMLR, 2010.\nMasashi Sugiyama, Matthias Krauledat, and Klaus-Robert M¨uller. Covariate shift adapta-\ntion by importance weighted cross validation. JMLR, 8(35):985–1005, 2007.\nMasashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul Buenau, and Motoaki\nKawanabe.\nDirect importance estimation with model selection and its application to\ncovariate shift adaptation. In NeurIPS, 2008.\nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adap-\ntation. In ECCV, 2016.\n61\nJiang et al.\nQianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for\nfew-shot learning. In CVPR, 2019a.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danx-\niang Zhu, Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge\nintegration. arXiv preprint arXiv:1904.09223, 2019b.\nYaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation.\nIn ICLR, 2017.\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media,\n1998.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV,\n2020.\nLisa Torrey and Jude Shavlik. Transfer learning. 2010.\nYi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and\nManmohan Chandraker. Learning to adapt structured output space for semantic seg-\nmentation. In CVPR, 2018.\nEric Tzeng, Judy Hoﬀman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain\nconfusion: Maximizing for domain invariance. 2014.\nEric Tzeng, Judy Hoﬀman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer\nacross domains and tasks. In ICCV, pages 4068–4076, 2015.\nEric Tzeng, Judy Hoﬀman, Kate Saenko, and Trevor Darrell. Adversarial discriminative\ndomain adaptation. In CVPR, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS,\n2017.\nPetar Veliˇckovi´c, William Fedus, William L Hamilton, Pietro Li`o, Yoshua Bengio, and\nR Devon Hjelm. Deep graph infomax. In ICLR, 2019.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting\nand composing robust features with denoising autoencoders. In ICML, 2008.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks\nfor one shot learning. In NeurIPS, 2016.\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik,\nJunyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al.\nGrandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575\n(7782):350–354, 2019.\n62\nTransferability in Deep Learning: A Survey\nTuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Perez. Advent:\nAdversarial entropy minimization for domain adaptation in semantic segmentation. In\nCVPR, 2019.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bow-\nman. GLUE: A multi-task benchmark and analysis platform for natural language under-\nstanding. In ICLR, 2019a.\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing.\nLearning robust global\nrepresentations by penalizing local predictive power. In NeurIPS, 2019b.\nXimei Wang, Ying Jin, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Transferable\nnormalization: Towards improving transferability of deep neural networks. In NeurIPS,\n2019c.\nXimei Wang, Jinghan Gao, Mingsheng Long, and Jianmin Wang.\nSelf-tuning for data-\neﬃcient deep learning. In ICML, 2021.\nZirui Wang, Zihang Dai, Barnab´as P´oczos, and Jaime G. Carbonell. Characterizing and\navoiding negative transfer. In CVPR, 2019d.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester,\nNan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot\nlearners. In ICLR, 2022.\nLonghui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Person transfer gan to bridge domain\ngap for person re-identiﬁcation. In CVPR, 2018.\nRoss\nWightman.\nPytorch\nimage\nmodels.\nhttps://github.com/rwightman/\npytorch-image-models, 2019.\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\nZhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua Lin. Unsupervised feature learning\nvia non-parametric instance discrimination. In CVPR, 2018.\nRunxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and\nFei Huang. Raise a child in large language model: Towards eﬀective and generalizable\nﬁne-tuning. In EMNLP, 2021.\nI Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale\nsemi-supervised learning for image classiﬁcation. arXiv preprint arXiv:1905.00546, 2019.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and\nQuoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding.\nIn NeurIPS, 2019.\nHuaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. Hierarchically structured meta-\nlearning. In ICML, 2019.\n63\nJiang et al.\nJason Yosinski, JeﬀClune, Yoshua Bengio, and Hod Lipson. How transferable are features\nin deep neural networks? In NeurIPS, 2014.\nKaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long. Logme: Practical assessment\nof pre-trained models for transfer learning. In ICML, 2021.\nAmir Roshan Zamir, Alexander Sax, William B. Shen, Leonidas J. Guibas, Jitendra Malik,\nand Silvio Savarese. Taskonomy: Disentangling task transfer learning. In CVPR, 2018.\nWerner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschl¨ager, and Susanne\nSaminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation\nlearning. In ICLR, 2017.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Under-\nstanding deep learning requires rethinking generalization. In ICLR, 2017.\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and\nMichael I. Jordan. Theoretically principled trade-oﬀbetween robustness and accuracy.\nIn ICML, 2019a.\nJeﬀrey O. Zhang, Alexander Sax, Amir Zamir, Leonidas J. Guibas, and Jitendra Malik.\nSide-tuning: Network adaptation via additive side networks. 2019b.\nYuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and\nalgorithm for domain adaptation. In ICML, 2019c.\nZhilu Zhang and Mert R. Sabuncu. Generalized cross entropy loss for training deep neural\nnetworks with noisy labels. In NeurIPS, 2018.\nNanxuan Zhao, Zhirong Wu, Rynson W. H. Lau, and Stephen Lin. What makes instance\ndiscrimination good for transfer learning? In ICLR, 2021.\nLucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, and Daniel E. Ho. When\ndoes pretraining help? assessing self-supervised learning for law and the casehold dataset.\nIn ICAIL, 2021.\nJincheng Zhong, Ximei Wang, Zhi Kou, Jianmin Wang, and Mingsheng Long. Bi-tuning of\npre-trained representations. arXiv preprint arXiv:2011.06182, 2020.\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image\ntranslation using cycle-consistent adversarial networks. In ICCV, 2017.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio\nTorralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual expla-\nnations by watching movies and reading books. In ICCV, 2015.\nFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui\nXiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the\nIEEE, 109(1):43–76, 2021.\nYang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, and Jinsong Wang. Unsupervised domain\nadaptation for semantic segmentation via class-balanced self-training. In ECCV, 2018.\n64\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-01-15",
  "updated": "2022-01-15"
}