{
  "id": "http://arxiv.org/abs/2201.07986v3",
  "title": "Unsupervised Graph Poisoning Attack via Contrastive Loss Back-propagation",
  "authors": [
    "Sixiao Zhang",
    "Hongxu Chen",
    "Xiangguo Sun",
    "Yicong Li",
    "Guandong Xu"
  ],
  "abstract": "Graph contrastive learning is the state-of-the-art unsupervised graph\nrepresentation learning framework and has shown comparable performance with\nsupervised approaches. However, evaluating whether the graph contrastive\nlearning is robust to adversarial attacks is still an open problem because most\nexisting graph adversarial attacks are supervised models, which means they\nheavily rely on labels and can only be used to evaluate the graph contrastive\nlearning in a specific scenario. For unsupervised graph representation methods\nsuch as graph contrastive learning, it is difficult to acquire labels in\nreal-world scenarios, making traditional supervised graph attack methods\ndifficult to be applied to test their robustness. In this paper, we propose a\nnovel unsupervised gradient-based adversarial attack that does not rely on\nlabels for graph contrastive learning. We compute the gradients of the\nadjacency matrices of the two views and flip the edges with gradient ascent to\nmaximize the contrastive loss. In this way, we can fully use multiple views\ngenerated by the graph contrastive learning models and pick the most\ninformative edges without knowing their labels, and therefore can promisingly\nsupport our model adapted to more kinds of downstream tasks. Extensive\nexperiments show that our attack outperforms unsupervised baseline attacks and\nhas comparable performance with supervised attacks in multiple downstream tasks\nincluding node classification and link prediction. We further show that our\nattack can be transferred to other graph representation models as well.",
  "text": "Unsupervised Graph Poisoning Attack via Contrastive Loss\nBack-propagation\nSixiao Zhangâ€ \nzsx57575@gmail.com\nUniversity of Technology Sydney\nHongxu Chenâˆ—â€ \nhongxu.chen@uts.edu.au\nUniversity of Technology Sydney\nXiangguo Sun\nsunxiangguo@seu.edu.cn\nSoutheast University\nYicong Li\nYicong.Li@student.uts.edu.au\nUniversity of Technology Sydney\nGuandong Xuâˆ—\nguandong.xu@uts.edu.au\nUniversity of Technology Sydney\nABSTRACT\nGraph contrastive learning is the state-of-the-art unsupervised\ngraph representation learning framework and has shown compara-\nble performance with supervised approaches. However, evaluating\nwhether the graph contrastive learning is robust to adversarial at-\ntacks is still an open problem because most existing graph adversar-\nial attacks are supervised models, which means they heavily rely on\nlabels and can only be used to evaluate the graph contrastive learn-\ning in a specific scenario. For unsupervised graph representation\nmethods such as graph contrastive learning, it is difficult to acquire\nlabels in real-world scenarios, making traditional supervised graph\nattack methods difficult to be applied to test their robustness. In\nthis paper, we propose a novel unsupervised gradient-based ad-\nversarial attack that does not rely on labels for graph contrastive\nlearning. We compute the gradients of the adjacency matrices of\nthe two views and flip the edges with gradient ascent to maximize\nthe contrastive loss. In this way, we can fully use multiple views\ngenerated by the graph contrastive learning models and pick the\nmost informative edges without knowing their labels, and there-\nfore can promisingly support our model adapted to more kinds\nof downstream tasks. Extensive experiments show that our attack\noutperforms unsupervised baseline attacks and has comparable\nperformance with supervised attacks in multiple downstream tasks\nincluding node classification and link prediction. We further show\nthat our attack can be transferred to other graph representation\nmodels as well.\nCCS CONCEPTS\nâ€¢ Information systems â†’Data mining.\nKEYWORDS\nGraph Representation Learning, Graph Contrastive Learning, Ad-\nversarial Attack.\n1\nINTRODUCTION\nGraph structured data can be commonly seen in our daily life, such\nas social networks [15, 34], biological networks [39], e-commercial\nnetworks [31], etc. Real-world graphs usually contain rich informa-\ntion and can be used for various tasks including recommendation,\nmolecular structure classification, and community detection. To\nâˆ—Corresponding author.\nâ€ Both authors contributed equally to this research.\nSupervised \nattack\nUnsupervised \nattack\noriginal graph with node labels\npoisoned graph for node classification\noriginal graph without labels\npoisoned graph for various tasks\nâ€¦\nFigure 1: An illustration of the difference between super-\nvised attacks and unsupervised attacks. Supervised attacks\nrequire labels and are targeted at certain downstream tasks,\nwhereas unsupervised attacks do not require labels and are\ntargeted at multiple downstream tasks.\nextract the information contained in graphs, researchers have been\nexploring various machine learning methods for graph learning,\nincluding DeepWalk [17], node2vec [10], matrix factorization [19],\ngraph neural networks [20] and graph convolutional networks [14].\nGraph representation learning plays an important role in these\napproaches, where the goal is to project nodes/graphs into a low-\ndimensional embedding space that preserves the structural and\nfeature information [5, 6]. The quality of the embeddings directly\ninfluences the performance of downstream tasks such as link pre-\ndiction, node/graph classification, community detection, etc.\nHowever, recent studies have shown that graph learning models\nare vulnerable to adversarial attacks [12], where a small augmenta-\ntion in the graph, such as adding/deleting edges/nodes and altering\nfeatures, can lead to a large performance drop. The quality of the em-\nbeddings learned by graph representation models is also sensitive\nto such attacks [3, 33], and will in turn affects the performance of\nthe downstream tasks. In addition, adversarial attacks in the graph\ndomain are easy to conduct. For example, the attacker can attack\na social network by simply creating some fake users and estab-\nlishing connections with other people by following their accounts.\nThe e-commercial networks can also be attacked by creating fake\nuser profiles and writing fake reviews. Therefore, it is important to\nstudy the robustness of various graph learning models and develop\nmodels that are robust to adversarial attacks.\narXiv:2201.07986v3  [cs.LG]  27 Jan 2022\nRecently, researchers have been exploring the robustness of\nsupervised and semi-supervised graph representation models\n[9, 13, 35, 36, 40], but the robustness of unsupervised graph rep-\nresentation models still remains an open challenge. Due to the\ncomplexity and scale-freeness of many real-world graphs, the la-\nbels are hard to acquire in most cases [25]. It is also impractical\nto do manual annotation for unlabeled large graphs. Thus, un-\nsupervised graph representation learning becomes an important\nbranch in graph representation learning. However, most state-of-\nthe-art graph representation learning methods are supervised or\nsemi-supervised [14, 24]. Traditional unsupervised methods, such\nas DeepWalk [17] and node2vec [10], have a relatively lower perfor-\nmance on downstream tasks compared with supervised methods.\nHowever, unsupervised graph representation learning has again\nattracted the attention of researchers in recent years because of the\nsuccess of contrastive learning [18, 28, 30, 37, 38]. It has emerged\nas a new state-of-the-art unsupervised learning framework and\nhas shown comparable performance with supervised and semi-\nsupervised baselines. It generates different views of the original\ngraph using stochastic augmentations and adopts a special con-\ntrastive loss to learn embeddings by comparing these views, which\nalso makes graph contrastive learning models more robust to ad-\nversarial attacks compared with other graph representation meth-\nods [30]. However, such conclusions are obtained using existing\ngraph adversarial attacks, which are mostly supervised attacks\n[9, 27, 40, 41], i.e. they need labels to conduct the attack as shown\nin Fig. 1. In real-world scenarios, it is difficult to attack contrastive\nlearning with supervised attacks because the labels are hard to\nacquire. Therefore, this raises a new challenge that how to attack\nunsupervised graph representation learning such as contrastive\nlearning without knowing the labels? Previous work by Bojchevski\net al. [2] has shown that unsupervised graph learning methods\nbased on random walks such as DeepWalk can be attacked in an\nunsupervised way by approximating the influence of a single aug-\nmentation in an optimization perspective. However, their method\nis limited to random walk and we will show in our experiments\nthat it can not perform well on graph contrastive learning.\nAttacking graph contrastive learning in an unsupervised way\nis a non-trivial task. Our goal is to poison the graph so that the\noverall performance on various downstream tasks is degraded. But\nwe canâ€™t use the downstream task to measure the quality of the\nembeddings. Therefore, how to explicitly measure the quality of the\nembeddings is the first problem needed to be solved. In addition,\nthe stochastic augmentation process provides robustness against\nadversarial attacks, so the attack needs to be insensitive to stochastic\naugmentations. Besides, a powerful graph attack model should let\nthe poisoned graph be deterrent as much as possible, which means\nour generated poisoned graphs should be also transferable to the\nother graph representation models.\nTo solve the above challenges, we propose a novel unsupervised\nadversarial attack based on gradient ascent on graph contrastive\nlearning for node embeddings, which is called Contrastive Loss\nGradient Attack (CLGA). To the best of our knowledge, this is the\nfirst work aiming at attacking graph contrastive learning in an\nunsupervised manner. Specifically, we compute the gradient of the\ncontrastive loss w.r.t. the adjacency matrix, and flip the edges with\nthe largest gradients, causing the contrastive loss to be damaged.\nWe show by extensive experiments that CLGA outperforms other\nexisting unsupervised attack baselines and has comparable per-\nformance with some of the supervised attack methods. We also\nshow that CLGA can be transferred to other graph representation\nmodels. To guarantee reproducibility, we open the source code at 1.\nIn summary, our contributions are as follows:\nâ€¢ We propose CLGA, a gradient-based unsupervised attack\nmethod targeting graph contrastive learning. Unlike most\nsupervised attack models, CLGA does not rely on labels and\ncan degrade the quality of the learned embeddings and thus\naffect the performance of various downstream tasks.\nâ€¢ We show by extensive experiments that CLGA outperforms\nunsupervised attack baselines and has comparable perfor-\nmance with some of the supervised attack methods on three\nbenchmark datasets and on both node classification and link\nprediction tasks.\nâ€¢ We also show that CLGA can be transferred to other graph\nrepresentation models such as GCN and DeepWalk.\nâ€¢ We visualize the learned embeddings to show how CLGA\ninfluences the quality of them.\nIn the following content, we first introduce the related work and\npreliminaries for graph adversarial attack and graph contrastive\nlearning. Next we introduce how CLGA works. Moreover, we show\nthe experiment results on three benchmark datasets and compare\nCLGA with several state-of-the-art baseline attacks. At last, we\nprovide the conclusion of the paper.\n2\nRELATED WORK\n2.1\nGraph Adversarial Attack\nThe robustness of various machine learning models is a hot topic\nin the research community. A small perturbation in the training\ndata might lead to a big change in the model performance. How\nto attack models and how to defend against such attacks are two\nmain streams of this field. In the graph domain, people have already\nproposed many successful adversarial attack methods. RL-S2V [9]\nadopts reinforcement learning to attack graph neural networks by\nadding/deleting edges that have a positive effect on the attackerâ€™s\ngoal. Later, NIPA [21] and ReWatt [16] also use reinforcement learn-\ning to attack GNNs but only by injecting nodes and rewiring edges\nrespectively. Some works try to model the attack as an optimiza-\ntion problem. PGD and MinMax [27] optimize the negative cross-\nentropy loss using the gradient. Nettack [40] iteratively selects\naugmentations by calculating the score/loss of each possible aug-\nmentation and chooses the one that maximizes the loss. Besides,\nbecause of the strong ability of gradients to locate the most im-\nportant instances, gradient-based attack methods have emerged\nas new state-of-the-arts, such as Mettack [41] and FGA [7], where\nthey both use the gradient of the classification loss w.r.t. the ad-\njacency matrix to select the edges to augment. However, all the\nattack methods mentioned above are supervised attack methods,\nwhich means that they require labels to conduct the attack, either\nby using the ground-truth labels or by using the predictions of the\ntarget model as labels. They cannot be directly used to attack an\nunsupervised model that aims at learning pre-trained embeddings.\n1https://github.com/RinneSz/CLGA\n2\nBojchevski et al. [2] have proposed an unsupervised optimization-\nbased attack method to attack graph embedding methods based on\nrandom walks such as DeepWalk without using the labels. But we\nwill show that it does not work very well for the state-of-the-art\ncontrastive learning framework.\n2.2\nGraph Contrastive Learning\nRecently, because of the great success of contrastive learning in com-\nputer vision [8, 11], researchers have begun to explore contrastive\nlearning in the graph domain and have achieved competitive perfor-\nmance compared with supervised and semi-supervised models. DGI\n[25] maximizes the mutual information between patch represen-\ntations and corresponding high-level summaries of graphs. CSSL\n[32] first defines four types of basic augmentation operations, edge\ndeletion, node deletion, edge insertion and node insertion, wherein\nall of them choose nodes/edges randomly. Then they randomly sam-\nple a sequence of operations to apply to the graph. GraphCL [30]\nmaximizes the agreement between two views by uniform perturba-\ntions including node dropping, edge perturbation, attribute masking\nand subgraphs. GCC [18] uses random walk on the r-ego network\nto generate different subgraphs, and learns embeddings by com-\nparing these subgraphs. GRACE [37] learns node embeddings by\nrandomly removing edges and masking features. GCA [38] further\nimproves GRACE by designing new augmentation schemes that are\naimed to keep important structures and attributes unchanged, while\nperturbing possibly unimportant links and features. These works\ndiffer in the specific augmentation strategy, such as adding/deleting\nedges/nodes, masking features, and extracting subgraphs. The loss\nfunctions used in these works share the same idea. They treat the\nembeddings of the same instance in different augmented views\nas positive pairs and treat the embeddings of different instances\nas negative pairs. Later, bootstrapped graph contrastive learning\nmodels have been proposed by Thakoor et al [22] and Che et al [4],\nwhich do not require negative pairs. Such a method refers to two\nneural networks, the online network and the target network. Two\naugmented views are the input to the two networks respectively.\nThe online network is trained to predict the target network output,\nand the target network is updated by an exponential moving aver-\nage of the online network. After training, the encoder of the online\nnetwork is used to compute the representations of the downstream\ntasks.\n3\nPRELIMINARIES\n3.1\nGraph Contrastive Learning on Node\nEmbeddings\nA graph ğºis defined as ğº= (ğ‘‰, ğ¸), where ğ‘‰is the set of vertices\n(nodes) and ğ¸is the set of edges. It can also be represented by the\nadjacency matrix ğ´âˆˆRğ‘Ã—ğ‘, where ğ‘is the number of nodes.\nSome graphs also have features ğ‘‹âˆˆRğ‘Ã—ğ‘‘associated with nodes,\nwhere ğ‘‘is the number of features. The goal of graph contrastive\nlearning is to learn an encoder ğ‘“(ğ´,ğ‘‹) that outputs embeddings\nof each node. Such embeddings can then be used for downstream\ntasks including node classification and link prediction.\nIn this paper, we focus on node-level contrastive learning. So we\nwill introduce how node-level graph contrastive learning works\nfollowing the state-of-the-art GCA model [38]. A general graph\ncontrastive learning framework consists of three steps. First, two\nstochastic augmentation processes ğ‘¡1 and ğ‘¡2 are applied to the orig-\ninal graph to obtain two different views. Typical augmentation\nstrategies include edge dropping/insertion, feature masking, sub-\ngraph extracting, etc. Second, the two views are fed into a shared\nencoder ğ‘“(ğ´,ğ‘‹) to obtain node embeddings. Each node will have\ntwo embeddings corresponding to the two views. Third, a con-\ntrastive loss is applied to the embeddings to push positive node\npairs close to each other and push negative node pairs away from\neach other. Specifically, the loss for the ğ‘–-th node in the first view is\nğ‘™(ğ‘’1\nğ‘–,ğ‘’2\nğ‘–) = âˆ’log\nğ‘’ğ›½(ğ‘’1\nğ‘–,ğ‘’2\nğ‘–)/ğœ\nğ‘’ğ›½(ğ‘’1\nğ‘–,ğ‘’2\nğ‘–)/ğœ+ Ã\nğ‘—â‰ ğ‘–(ğ‘’ğ›½(ğ‘’1\nğ‘–,ğ‘’1\nğ‘—)/ğœ+ ğ‘’ğ›½(ğ‘’1\nğ‘–,ğ‘’2\nğ‘—)/ğœ)\n(1)\nwhere ğ‘’1\nğ‘–and ğ‘’2\nğ‘–denote the embeddings of the ğ‘–-th node in the first\nand second view respectively. ğ›½is a similarity function, e.g. cosine\nsimilarity. ğœis a temperature parameter. Note that the above loss\nis non-symmetric for ğ‘’1\nğ‘–and ğ‘’2\nğ‘–. So we need to add it up with its\ncounterpart ğ‘™(ğ‘’2\nğ‘–,ğ‘’1\nğ‘–). The final loss L is the sum of ğ‘™for all nodes,\nwhich is\nL =\nğ‘\nâˆ‘ï¸\nğ‘–=1\nğ‘™(ğ‘’1\nğ‘–,ğ‘’2\nğ‘–) + ğ‘™(ğ‘’2\nğ‘–,ğ‘’1\nğ‘–)\n(2)\nBy minimizing L, the model treats the embeddings of the same node\nin the two views as positive pairs and the embeddings of different\nnodes in either the same view or different views as negative pairs.\nBecause of the stochastic augmentation procedure, the learned\nembeddings are more robust to the small perturbations in the graph\ncompared with other conventional graph representation models.\n3.2\nGraph Adversarial Attack\nAdversarial attack aims to add noise to the data to degrade the\nperformance of the target model. For graph data, the common noise\nincludes edge-level, node-level, and graph-level augmentations,\nsuch as adding/deleting edges/nodes, augmenting features, and\nadding fake graphs for graph classification.\nAccording to the goal of the attacker, adversarial attacks can be\ndivided into two categories:\nâ€¢ Untargeted Attack: force the model to have bad overall\nperformance on all given instances.\nâ€¢ Targeted Attack: force the target model to have bad per-\nformance on a subset of instances.\nA successful untargeted attack will make the target model produce\na biased prediction for every query, and a targeted attack will make\nthe target model produce a biased prediction for only certain queries.\nAs a result, when testing the performance of the model on a test set,\na successful untargeted attack will make the model have a worse\noverall accuracy compared with the model trained on clean data. A\nsuccessful targeted attack will only make the model produce wrong\npredictions on a specific subset of the test set, while still producing\ncorrect predictions for other data, and thus the overall accuracy\nmay not be significantly reduced.\nBesides, according to the capacity of the attacker, adversarial\nattacks can be divided into two categories:\n3\nShared \nencoder\nContrastive Loss\nt1\nt2\nâˆ†1= ğœ•ğ¿\nğœ•ğ´1\nâˆ†2= ğœ•ğ¿\nğœ•ğ´2\nğ´, ğ‘‹\nğ´1, ğ‘‹1\nğ´2, ğ‘‹2\n0.1\n0.3\n0.8\n0.1\n0.0\n0.8\n0.1\n0.3\n0.0\nBack-propagation\nBack-propagation\nEmbeddings\nFigure 2: CLGA framework. The clean adjacency matrix ğ´and feature matrix ğ‘‹are augmented by two stochastic augmen-\ntations ğ‘¡1 and ğ‘¡2 to obtain two views ğ´1,ğ‘‹1 and ğ´2,ğ‘‹2. The two views are then fed into a shared encoder to obtain the node\nembeddings. The contrastive loss is computed using the embeddings. We then back-propagate the contrastive loss to obtain\nthe gradients of the two viewsâ€™ adjacency matrices. The gradients are used for selecting the edges that will be flipped in our\nattack.\nâ€¢ Poisoning Attack: attack happens before the target model\nis trained. The training data of the target model is poisoned\nand is being used to train the target model.\nâ€¢ Evasion Attack: attack happens after the model is trained.\nThe model is trained on clean training data and is fixed.\nPoisoning attacks are a stronger attack since the attacker can affect\nthe learning process of the target model. For evasion attacks, since\nthe target model is fixed, what we can do is to perturb the data to\ncause misclassifications. For example, the attacker can modify the\ngraph of a protein molecule to make it be misclassified as another\ntype of protein.\nFor node representation learning on graphs, evasion attack is\nhard to conduct due to the transductive setting where all nodes\nare seen by the target model during training. Therefore, in this\npaper, we focus on the untargeted poisoning attack since this is best\nsuited for attacking node representation learning models. Targeted\npoisoning attacks are a special case for untargeted poisoning attacks\nand can be achieved by restricting the target nodes.\n3.3\nGraph Convolutional Networks\nThe graph convolutional network proposed by Kipf et al. [14] is\nusually used as the encoder ğ‘“in graph contrastive learning because\nof its state-of-the-art performance in modeling graph-structured\ndata. Given the adjacency matrix ğ´and feature matrix ğ‘‹, the output\nğ‘‹â€² of a typical GCN layer is computed by\nğ‘‹â€² = eğ·âˆ’1/2 e\nğ´eğ·âˆ’1/2ğ‘‹Î˜\n(3)\nwhere e\nğ´= ğ´+ ğ¼denotes the adjacency matrix with self-loops. eğ·\nis a diagonal degree matrix and eğ·ğ‘–ğ‘–= Ã\nğ‘—e\nğ´ğ‘–ğ‘—. Î˜ is the parameter\nmatrix of the GCN layer. Activation functions such as ReLU and\nSigmoid are applied at the output of each layer. In conventional GCN\nmodels, Î˜ is the only learnable parameter in Eq. 3 and is updated by\ngradient descent. However, to poison the graph, we need to know\nthe gradient on the adjacency matrix ğ´and luckily it is also easy\nto be deduced by Eq. 3. We will introduce how we attack graph\ncontrastive learning using the gradient of the adjacency matrix in\nthe next section.\n4\nCONTRASTIVE LOSS GRADIENT ATTACK\n4.1\nOverview\nWe want to design an untargeted poisoning attack, where the goal\nis to poison the graph data such that the overall quality of the em-\nbeddings learned by graph contrastive learning is degraded, which\nleads to worse performance in multiple downstream tasks. The con-\ntrastive loss used in contrastive learning is a natural measurement\nof the embedding quality. Therefore, we choose to poison the graph\nby maximizing the contrastive loss. In our attack, we only augment\nedges and do not augment features, because features are auxiliary\ninformation and not all the graphs have features. We formulate our\nproblem as:\nmax\n^ğ´\nL(ğ‘“ğœƒâ€²(ğ´1,ğ‘‹1), ğ‘“ğœƒâ€²(ğ´2,ğ‘‹2))\ns.t.\nğœƒâ€² = arg min\nğœƒ\nL(ğ‘“ğœƒ(ğ´1,ğ‘‹1), ğ‘“ğœƒ(ğ´2,ğ‘‹2)),\n(ğ´1,ğ‘‹1) = ğ‘¡1( ^ğ´,ğ‘‹), (ğ´2,ğ‘‹2) = ğ‘¡2( ^ğ´,ğ‘‹), âˆ¥ğ´âˆ’^ğ´âˆ¥= ğœ.\n(4)\nhere L is the contrastive loss stated in Eq. 2. ğ‘“is the encoder and ğœƒ\nis the set of parameters of the encoder. ğ´and ğ‘‹are the adjacency\nmatrix and feature matrix of the clean graph. ^ğ´is the poisoned\nadjacency matrix. ğ‘¡1 and ğ‘¡2 are two stochastic augmentations. ğ´1\nand ğ‘‹1 are the adjacency matrix and feature matrix of the first view,\nand ğ´2 and ğ‘‹2 are those of the second view. The last constraint\nindicates that the number of augmented edges is bounded by a\ngiven threshold ğœ.\nThis is a bi-level optimization problem and is hard to be solved\ndirectly. Inspired by [41], we propose to use meta-gradients, i.e.\ngradients w.r.t. the adjacency matrix. We back-propagate the con-\ntrastive loss to obtain the gradients of the adjacency matrix and\nupdate the adjacency matrix to maximize the loss.\n4.2\nGradient based Attack\nTo solve Eq. 4, we propose to use gradient ascent on the adjacency\nmatrix. The key idea is to flip the edges with the largest gradient\nvalues and the correct gradient directions, and thus the contrastive\n4\nloss will be maximized. For example, if an observed edge (which\nmeans the corresponding entry is 1 in the adjacency matrix) has\na negative gradient, then deleting it (from 1 to 0 in the adjacency\nmatrix) will be very likely to increase the loss. Similarly, for an\nunobserved edge that has a large positive gradient, adding it to the\ngraph will also increase the loss. An illustration of how we acquire\nthe gradients is shown in Fig. 2. Specifically, if we use a differentiable\nencoder ğ‘“(ğ´,ğ‘‹), e.g. graph convolutional network (GCN), we can\neasily obtain the gradients of the two viewsâ€™ adjacency matrix ğ´1\nand ğ´2:\nÎ”1 = ğœ•L\nğœ•ğ´1\n=\nğœ•L\nğœ•ğ‘“(ğ´1,ğ‘‹1) Â· ğœ•ğ‘“(ğ´1,ğ‘‹1)\nğœ•ğ´1\n(5)\nÎ”2 = ğœ•L\nğœ•ğ´2\n=\nğœ•L\nğœ•ğ‘“(ğ´2,ğ‘‹2) Â· ğœ•ğ‘“(ğ´2,ğ‘‹2)\nğœ•ğ´2\n(6)\nIdeally, the most informative edges usually contribute largely to\nthe graph learning model because their loss gradients on the clean\nadjacency matrix Î” = ğœ•L\nğœ•ğ´usually have larger absolute values. To\nfind these edges, we need to differentiate through the stochastic\naugmentations ğ‘¡1 and ğ‘¡2 to obtain Î” as shown in Fig. 2. Unfortu-\nnately, the stochastic augmentation process ğ‘¡might contain some\nindifferentiable policy such as adding/deleting nodes and extracting\nsubgraphs, making the problem even harder. To solve this problem,\nwe do not back-propagate through ğ‘¡and only use Î”1 and Î”2 to\nhelp select which edges to flip. Another problem is that we can-\nnot directly use Î”1 or Î”2 to select edges because the adjacency\nmatrices of the two views are different from the clean adjacency\nmatrix due to the stochastic augmentations. That means the edges\nhaving the largest gradients in these two views (Î”1 and Î”2) might\nnot be the truly important ones in the original graph. With the\nabove discussion, we can find that the core challenge is how to\ncombine these two gradient matrices Î”1 and Î”2 to find the most\ninformative edges and in the meanwhile, alleviate the bias caused\nby the stochastic augmentations (ğ‘¡1 and ğ‘¡2) as much as possible.\nBut what exactly is the form of the bias caused by the stochastic\naugmentations? We answer this question with a small example. As\nwe know, conventional graph contrastive learning learns embed-\ndings that are insensitive to the change of the graph by updating\nthe parameters of the encoder. It tries to eliminate the difference\nbetween the two views. If we assume that the adjacency matrices\nof the two views are learnable parameters, how will the adjacency\nmatrices be like? Apparently they will become identical. Consider\nthat if the two views are already identical, then every edge will\nhave a zero gradient. If we now apply a stochastic augmentation\nand one single edge is changed, where it exists in the first view but\ndoes not exist in the second view, it will have a negative gradient in\nthe first view and a positive gradient in the second view, pushing\nthe corresponding entries 1 and 0 closer to each other. In this case,\nthis augmented edge will have a much larger gradient compared\nwith other edges, but can we say that it is more informative than\nother edges? Of course not, because the large gradient is actually\ncaused by the stochastic augmentation. Instead, if we add the two\ngradients up, where one is positive and another is negative, the\nresult will be close to 0 and we will thus acquire a relatively accu-\nrate measurement of how important this edge is by successfully\nalleviating the bias introduced by the stochastic augmentation.\nWith the above example, we propose to solve the problem by\nusing two little tricks. First, we add Î”1 and Î”2 up to alleviate the\nbias introduced by the stochastic augmentations:\nÎ”0 = Î”1 + Î”2\n(7)\nIn addition, we add up gradients of ğ¾random stochastic augmenta-\ntion pairs to further alleviate the bias caused by some rare cases:\nÎ”â€² =\nğ¾\nâˆ‘ï¸\nğ‘˜=1\nÎ”ğ‘˜\n0 = Î”ğ‘˜\n1 + Î”ğ‘˜\n2\n(8)\nwhere for each ğ‘˜, Î”ğ‘˜\n1 and Î”ğ‘˜\n2 is obtained by a random stochastic\naugmentation pair ğ‘¡ğ‘˜\n1 and ğ‘¡ğ‘˜\n2. We use Î”â€² to select the edges that\nhave the largest absolute gradients and correct gradient directions\nto flip.\nSpecifically, if for an edge, the gradients in the two views are\nboth positive, it suggests that even if the stochastic augmentations\nare applied, the two views both prefer a smaller value in the corre-\nsponding entry which will lead to a smaller contrastive loss. Instead,\nincreasing the value in this entry by adding this edge (if it does\nnot exist) is very likely to increase the contrastive loss. This also\napplies to the case where the gradients are both negative. In our\nmethod, we add the two gradients to obtain a larger absolute value\nand this edge will be ranked at a higher position. If one of the\ngradients is positive and another is negative, it is usually caused by\nthe stochastic augmentation on itself or its neighborhood. In this\ncase, the information contained in its gradients is more about how\nto compensate for the heavily augmented neighborhood, or how\nto alleviate the changes caused by the stochastic augmentations.\nWe are unable to tell how important this edge itself is w.r.t. the\ncontrastive loss, so we hope that this edge will be ranked in a lower\nplace when we are picking which edges to flip. We can achieve\nthis by adding the two gradients up to get a new value that has a\nsmaller absolute value than before, and it will thus have a lower\nranking when we rank all the edges.\nTo further improve the attack, in each iteration we only pick\none edge to flip. Specifically, in the first iteration, we train the\ncontrastive model with the clean graph. Then we compute Î”â€² and\nselect one edge with the largest gradient and correct direction. We\nflip the selected edge and use the new adjacency matrix (which\ndiffers from the clean adjacency only on this single edge) to retrain\nthe contrastive model in the next iteration. So if we are going to\nflip 100 edges, we will need to retrain the model 100 times. This\niterative process helps us to better locate informative edges.\nThe overall algorithm is shown in algorithm 1.\n4.3\nComplexity Analysis\n4.3.1\nTime Complexity. The time complexity of CLGA itself is low\nsince we only need to rank the gradients and select the desired\nedges. What is truly time-consuming is the training of the target\nmodel, because we need to retrain it in each iteration. Therefore,\nfor complex models whose time complexity is large, it will take a\nrelatively long time for the poisoned graph to be generated. How-\never, we can alleviate this issue by doing some approximations at\nthe price of attack performance. For example, we can select multi-\nple edges in each iteration, or we can train the target model with\npre-training or early-stopping because we do not necessarily need\n5\nAlgorithm 1: CLGA\nInput: Clean adjacency matrix ğ´, feature matrix ğ‘‹, differentiable\nencoder ğ‘“, stochastic augmentation set ğ‘‡, augmentation\nthreshold ğœ, number of iterations ğ¾;\nOutput: Poisoned graph ^ğ´;\n1: ğ‘–= 0; ^ğ´= ğ´;\n2: while ğ‘–< ğœdo\n3:\nTrain ğ‘“with ^ğ´and ğ‘‹;\n4:\nÎ”â€² = 0;\n5:\nfor ğ‘˜= 1 to ğ¾do\n6:\nSample two stochastic augmentations ğ‘¡ğ‘˜\n1,ğ‘¡ğ‘˜\n2 âˆˆğ‘‡;\n7:\nObtain two views (ğ´ğ‘˜\n1,ğ‘‹ğ‘˜\n1 ) = ğ‘¡ğ‘˜\n1 ( ^ğ´,ğ‘‹),\n(ğ´ğ‘˜\n2,ğ‘‹ğ‘˜\n2 ) = ğ‘¡ğ‘˜\n2 ( ^ğ´,ğ‘‹);\n8:\nForward propagate (ğ´ğ‘˜\n1,ğ‘‹ğ‘˜\n1 ), (ğ´ğ‘˜\n2,ğ‘‹ğ‘˜\n2 ) through ğ‘“and\ncompute contrastive loss L;\n9:\nObtain the gradients of ğ´ğ‘˜\n1 and ğ´ğ‘˜\n2 w.r.t. the contrastive\nloss, Î”ğ‘˜\n1 = ğœ•L\nğœ•ğ´ğ‘˜\n1 , Î”ğ‘˜\n2 = ğœ•L\nğœ•ğ´ğ‘˜\n2 ;\n10:\nÎ”â€² = Î”â€² + Î”ğ‘˜\n1 + Î”ğ‘˜\n2;\n11:\nend for\n12:\nFlip one edge with both the largest absolute gradient in Î”â€²\nand the correct direction, i.e., if the index of the edge is\n[ğ‘š,ğ‘›], then it should satisfy either ^ğ´[ğ‘š,ğ‘›] = 1,\nÎ”â€²[ğ‘š,ğ‘›] < 0 or ^ğ´[ğ‘š,ğ‘›] = 0, Î”â€²[ğ‘š,ğ‘›] > 0;\n13:\n^ğ´[ğ‘š,ğ‘›] = 1 âˆ’^ğ´[ğ‘š,ğ‘›];\n14:\nFreeze the chosen edge and avoid being flipped again in\nnext iterations;\n15:\nğ‘–= ğ‘–+ 1;\n16: end while\nthe target model to be fully converged as long as the order of the\nranking is correct.\n4.3.2\nSpace Complexity. Since we are computing the gradients of\nthe whole adjacency matrix, the memory requirement is ğ‘‚(ğ‘2),\nwhere ğ‘is the number of nodes. This is expensive for graphs with\na large number of nodes. However, we can also alleviate this issue\nby only considering a subset of nodes and edges. For real-world\nscale-free graphs, it is impractical for the attackers to be able to\nmodify the whole graph, where they can only manipulate a small\nsubset of nodes and edges. In this case, we only need to compute\nthe gradients of the subset, which only has a small memory cost.\n5\nEXPERIMENTS\nIn our experiments, we aim to answer the following research ques-\ntions:\nâ€¢ Can CLGA successfully degrade the performance of various\ndownstream tasks of graph contrastive learning?\nâ€¢ Does CLGA outperform other unsupervised and supervised\nattacks?\nâ€¢ Is the poisoned graph generated by CLGA able to degrade\nthe performance of other graph representation models?\nTo answer these questions, we compare CLGA with five state-of-\nthe-art representative graph untargeted poisoning attacks on three\nbenchmark datasets. We evaluate the quality of the embeddings\nDataset\n# Nodes\n# Edges\n# Features\n# Classes\nCora\n2708\n5278\n1433\n7\nCiteSeer\n3327\n4552\n3703\n6\nPolBlogs\n1490\n16715\nNone\n2\nTable 1: Dataset statistics.\nby two downstream tasks: node classification and link prediction.\nWe further show the transferability of CLGA by evaluating other\ngraph representation models on the obtained poisoned graph.\n5.1\nSetup\n5.1.1\nDatasets. We use three popular public available benchmark\ndatasets. The two of them are citation networks Cora and CiteSeer\nfrom Yang et al. [29]. We also use PolBlog dataset from Adamic\net al. [1], which is a graph of political blogs. There are features\nassociated with each node in Cora and CiteSeer. However, there\nare no features for PolBlog. The statistics of the three datasets are\nshown in Table 1.\n5.1.2\nBaselines. We compare CLGA with five baseline untargeted\npoisoning attacks, including PGD [27], DICE [26], MinMax [27],\nMetattack [41], and the unsupervised node embedding attack pro-\nposed by Bojchevski et al. [2]. Among these baselines, only Bo-\njchevski et al. [2] is unsupervised and does not need labels, which\nis the same as our method. Since the labels are used as extra knowl-\nedge in the other four supervised attacks, they are expected to have\na better performance compared with unsupervised ones. But we\nwill show that CLGA has comparable performance and can even\noutperform some of the supervised baselines in some cases.\n5.1.3\nExperimental Settings. For Metattack, MinMax and PGD, we\nuse a 2-layer GCN as the surrogate model to acquire the poisoned\ngraphs. For all the attack methods, we first generate the poisoned\ngraphs and then feed them to the state-of-the-art contrastive learn-\ning framework GCA proposed by Zhu et al. [38]. Because a 2-layer\nGCN is used as the encoder in GCA and PolBlog does not have fea-\ntures, we randomly initialize a 32-dimensional feature for each node\nin PolBlog. We set the augmentation threshold ğœto be 1%/5%/10%\nof the number of edges, i.e. we are allowed to modify at most\n1%/5%/10% edges. For instance, if the number of edges in the clean\ngraph is 1000, then under 1% threshold, we can add or delete at\nmost 10 edges. For a fair comparison, we fix the hyperparameters\nof GCA across experiments as suggested by Zhu et al. [38], includ-\ning the temperature ğœand stochastic augmentation rates, where\nğœ= 0.4 and the edge dropping rates are 0.3 and 0.4 and the feature\ndropping rates are 0.1 and 0.0 for two views respectively. We use\nAdam optimizer and set the learning rate to be 0.01.\nFor the downstream node classification task, we train a simple\nlogistic regression model using the learned embeddings and report\nthe classification accuracy. When training, we use the public split\nfrom [29] for Cora and CiteSeer. For PolBlog, we randomly split the\nnodes into 10%/10%/80% train/test/val set. We run each experiment\n10 times and report the average.\nFor downstream link prediction task, we use a 2-layer MLP as\nthe projection head to project the learned embeddings into a new\nlatent space. We train the MLP with negative sampling and margin\n6\nAttack\nCora\nCiteSeer\nPolBlog\n1%\n5%\n10%\n1%\n5%\n10%\n1%\n5%\n10%\nSupervised\nMetattack\n0.7586\n0.6928\n0.6168\n0.5920\n0.3986\n0.2952\n0.8208\n0.8039\n0.8011\nPGD\n0.7680\n0.7592\n0.7402\n0.6098\n0.6198\n0.6056\n0.8100\n0.8010\n0.7987\nMinMax\n0.7624\n0.7218\n0.6174\n0.6302\n0.5254\n0.5618\n0.8016\n0.7913\n0.7986\nDICE\n0.7712\n0.7642\n0.7240\n0.6256\n0.5774\n0.5246\n0.8107\n0.7847\n0.7394\nUnsupervised\nBojchevski et al. [2]\n0.7490\n0.7710\n0.7670\n0.6442\n0.6448\n0.6608\n0.8187\n0.8042\n0.7892\nCLGA\n0.7316\n0.7188\n0.6814\n0.6368\n0.5906\n0.5368\n0.8088\n0.7944\n0.7726\nTable 2: Node classification accuracy of logistic regression model trained after GCA. 1%/5%/10% denote the maximum number\nof edges allowed to be augmented. The boldfaced ones are the best in either supervised or unsupervised approaches.\nAttack\nCora\nCiteSeer\nPolBlog\n1%\n5%\n10%\n1%\n5%\n10%\n1%\n5%\n10%\nSupervised\nMetattack\n0.9010\n0.8733\n0.8500\n0.9109\n0.8853\n0.8544\n0.8617\n0.8585\n0.8635\nPGD\n0.9143\n0.9073\n0.9073\n0.9169\n0.9248\n0.9057\n0.8605\n0.8584\n0.8625\nMinMax\n0.9116\n0.9004\n0.8944\n0.9145\n0.8890\n0.8981\n0.9145\n0.8890\n0.8981\nDICE\n0.9046\n0.8828\n0.8593\n0.9137\n0.8918\n0.8679\n0.8551\n0.8450\n0.8352\nUnsupervised\nBojchevski et al. [2]\n0.9164\n0.9099\n0.9101\n0.9239\n0.9168\n0.9196\n0.8593\n0.8543\n0.8587\nCLGA\n0.9012\n0.8741\n0.8420\n0.9114\n0.8911\n0.8610\n0.8584\n0.8598\n0.8563\nTable 3: Link prediction AUC of the MLP trained after GCA. 1%/5%/10% denote the maximum number of edges allowed to be\naugmented. The boldfaced ones are the best and the underlined ones are the second best among all approaches.\nloss, i.e.,\nğ‘™= âˆ’\nâˆ‘ï¸\nğ‘–,ğ‘—,ğ‘˜\nlog(ğœ(ğ›½(ğ‘’ğ‘–,ğ‘’ğ‘—) âˆ’ğ›½(ğ‘’ğ‘–,ğ‘’ğ‘˜)))\n(9)\nwhere ğ‘’ğ‘–is the projected embedding of node ğ‘–, ğ›½is the cosine\nsimilarity function, ğœis the sigmoid function. Node ğ‘–and node ğ‘—\nis a positive pair, e.g. they are linked by an observed edge. Node\nğ‘–and node ğ‘˜is a randomly sampled negative pair, e.g. they are\nnot linked in the training set. For all three datasets, we split the\nedges into 70%/20%/10% train/test/val set, and only the edges in\nthe training set are used to train the contrastive model. We report\nthe area under curve (AUC) score. We also run each experiment 10\ntimes and report the average.\n5.2\nComparing Node Classification\nPerformance\nTable 2 shows the accuracy of the downstream logistic regression\nmodel trained on the embeddings learned by GCA under each attack.\nWe can observe that, CLGA significantly outperforms the unsuper-\nvised baseline Bojchevski et al. [2], and can even outperform some\nof the supervised baselines in certain situations. For example, on\nCora, CLGA outperforms PGD and DICE in all cases, and outper-\nforms Metattack and MinMax at 1% and 1%/5% augmentation rates\nrespectively.\nAs the augmentation rate increases from 1% to 10%, the node\nclassification accuracy continuously drops under Metattack, DICE,\nand CLGA. However, for the other three attacks, a larger augmen-\ntation rate does not guarantee a lower node classification accuracy.\nThis suggests that PGD, MinMax, and Bojchevski et al. [2] fail to\ncapture the edges that are truly important to graph contrastive\nlearning framework, or in other words, graph contrastive learning\nis robust to such attacks.\nWe can also observe that Metattack has the best overall perfor-\nmance and is very effective on Cora and CiteSeer. This is because\nthat Metattack is originally designed for attacking an end-to-end\nGCN model for node classification. By utilizing the labels and the\ngradients, it can accurately locate the most informative edges that\naffect the node classification accuracy. However, such an approach\nis not guaranteed to work well on other downstream tasks, such\nas link prediction. Instead, we will show that our CLGA has a\ngood performance not only on node prediction, but also on link\nprediction.\n5.3\nComparing Link Prediction Performance\nTable 3 shows the AUC scores of the three datasets under different\nattacks. We have several observations and conclusions regarding\nthe results.\nFirst, CLGA outperforms Bojchevski et al. [2] except for 5% aug-\nmentation rate on PolBlog, showing that CLGA is more effective\nthan Bojchevski et al. [2]. Second, CLGA has comparable perfor-\nmance with supervised attacks. In most cases, CLGA is the second\nbest among all six attacks. This suggests that, even without labels,\nCLGA can effectively reduce the downstream link prediction per-\nformance for graph contrastive learning and achieve comparable\nor even better performance than supervised attacks.\nTogether with the observations in the node classification sce-\nnario, our unsupervised attack CLGA is shown to be able to reduce\nboth the downstream node classification performance and link\nprediction performance of graph contrastive learning, and has com-\nparable performance with state-of-the-art supervised untargeted\npoisoning attacks.\n5.4\nTransferability\nAlthough CLGA has a good performance on graph contrastive learn-\ning, it is still unknown how it works for other graph representation\nmodels. Here we further compare the transferability of CLGA with\n7\nAttack\nNode Classification\nLink Prediction\nDeepWalk\nGCN\nDeepWalk\nGCN\n1%\n5%\n10%\n1%\n5%\n10%\n1%\n5%\n10%\n1%\n5%\n10%\nMetattack\n0.6766\n0.6370\n0.5554\n0.7830\n0.7300\n0.6330\n0.8777\n0.8629\n0.8319\n0.8655\n0.8378\n0.8182\nBojchevski et al. [2]\n0.6826\n0.6436\n0.6276\n0.8240\n0.7900\n0.7950\n0.8852\n0.8873\n0.8810\n0.8675\n0.8634\n0.8542\nCLGA\n0.6720\n0.6459\n0.6376\n0.7860\n0.7770\n0.7690\n0.8799\n0.8612\n0.8390\n0.8667\n0.8546\n0.8146\nTable 4: Transferability of attacks on Cora for DeepWalk and GCN. We report the classification accuracy for node classification\nand AUC score for link prediction. 1%/5%/10% denote the maximum number of edges allowed to be augmented. The boldfaced\nones are the best and the underlined ones are the second best among the three attacks.\n(a) Clean graph\n(b) Bojchevski et al. [2]\n(c) Metattack\n(d) CLGA\nFigure 3: Visualization of node embeddings learned by GCA under different attacks on Cora. Different Colors represent dif-\nferent classes of nodes.\nBojchevski et al. [2] and Metattack. We use the poisoned graphs gen-\nerated by the three attacks to train DeepWalk and a 2-layer GCN. We\nevaluate the quality of the embeddings on both node classification\nand link prediction. For node classification, the learned embeddings\nare used to train a logistic regression model. For link prediction,\nwe use a 2-layer MLP as a projection head and train it with the loss\nin Eq. 9. We report the accuracy for node classification and AUC\nscore for link prediction in Table 4. Other experimental settings are\nkept consistent with previous experiments. Note that even though\nthe attack proposed by Bojchevski et al. [2] is originally designed\nfor methods based on random walks such as DeepWalk, our CLGA\ncan still achieve comparable performance with it on DeepWalk for\nthe node classification task, and even outperforms it on the link\nprediction task. Moreover, CLGA outperforms Bojchevski et al. [2]\non GCN. And CLGA even has comparable performance with Metat-\ntack at 1% augmentation rate for node classification, and at 1%/10%\naugmentation rates for link prediction. These observations suggest\nthat CLGA can be transferred to other graph representation models.\n5.5\nVisualization\nWe visualize the learned embeddings by GCA under Metattack,\nBojchevski et al. [2], and CLGA, to show how our attack influences\nthe distribution of embeddings. The t-SNE [23] results are shown\nin Fig. 3. We can observe that the distributions of embeddings by\nMetattack and CLGA are denser than Bojchevski et al. [2] and clean\nembeddings, especially in the center, which is considered to be the\nmain reason why Metattack and CLGA outperform others. Nodes\nin the center are hard to be well classified because such nodes have\na similar distance to each cluster and we canâ€™t tell which class\nit belongs to with high confidence. This suggests that CLGA can\nreduce the quality of the embeddings by pushing embeddings to\nthe center in Fig. 3, or in other words, pushing them close to the\nintersections/decision boundaries of the clusters, which makes the\nembeddings harder to be classified.\n6\nCONCLUSION\nIn this paper, we introduce Contrastive Loss Gradient Attack\n(CLGA), an unsupervised untargeted poisoning attack for attacking\ngraph contrastive learning. This is the first work to attack graph\ncontrastive learning in an unsupervised manner without using la-\nbels. The quality of the learned embeddings is damaged and the\nperformance of various downstream tasks is degraded. We show by\nextensive experiments that our CLGA outperforms unsupervised\nbaselines and has comparable and even better performance with\nsupervised baselines. We also show that CLGA can be transferred\nto other graph representation models such as DeepWalk and GCN.\nACKNOWLEDGMENT\nThe work has been supported by Australian Research Coun-\ncil under grants DP220103717, DP200101374, LP170100891, and\nLE220100078.\nREFERENCES\n[1] Lada A Adamic and Natalie Glance. 2005. The political blogosphere and the 2004\nUS election: divided they blog. In Proceedings of the 3rd international workshop\non Link discovery. 36â€“43.\n[2] Aleksandar Bojchevski and Stephan GÃ¼nnemann. 2019. Adversarial attacks on\nnode embeddings via graph poisoning. In International Conference on Machine\nLearning. PMLR, 695â€“704.\n[3] Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng Cui,\nWenwu Zhu, and Junzhou Huang. 2019. The General Black-box Attack Method\nfor Graph Neural Networks. arXiv preprint arXiv:1908.01297 (2019).\n[4] Feihu Che, Guohua Yang, Dawei Zhang, Jianhua Tao, Pengpeng Shao, and Tong\nLiu. 2020. Self-supervised Graph Representation Learning via Bootstrapping.\narXiv preprint arXiv:2011.05126 (2020).\n[5] Hongxu Chen, Hongzhi Yin, Tong Chen, Quoc Viet Hung Nguyen, Wen-Chih\nPeng, and Xue Li. 2019. Exploiting centrality information with graph convolutions\nfor network representation learning. In 2019 IEEE 35th International Conference\non Data Engineering (ICDE). IEEE, 590â€“601.\n8\n[6] Hongxu Chen, Hongzhi Yin, Weiqing Wang, Hao Wang, Quoc Viet Hung Nguyen,\nand Xue Li. 2018. PME: projected metric embedding on heterogeneous networks\nfor link prediction. In Proceedings of the 24th ACM SIGKDD International Confer-\nence on Knowledge Discovery & Data Mining. 1177â€“1186.\n[7] Jinyin Chen, Yangyang Wu, Xuanheng Xu, Yixian Chen, Haibin Zheng, and\nQi Xuan. 2018. Fast gradient attack on network embedding. arXiv preprint\narXiv:1809.02797 (2018).\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A\nsimple framework for contrastive learning of visual representations. In Interna-\ntional conference on machine learning. PMLR, 1597â€“1607.\n[9] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song.\n2018. Adversarial attack on graph structured data. In International conference on\nmachine learning. PMLR, 1115â€“1124.\n[10] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for\nnetworks. In Proceedings of the 22nd ACM SIGKDD international conference on\nKnowledge discovery and data mining. 855â€“864.\n[11] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momen-\ntum contrast for unsupervised visual representation learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9729â€“9738.\n[12] Wei Jin, Yaxin Li, Han Xu, Yiqi Wang, and Jiliang Tang. 2020. Adversarial\nattacks and defenses on graphs: A review and empirical study. arXiv preprint\narXiv:2003.00653 (2020).\n[13] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang.\n2020. Graph structure learning for robust graph neural networks. In Proceedings\nof the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining. 66â€“74.\n[14] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph\nconvolutional networks. In Proceedings of ICLR.\n[15] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. 2008. Sorec: social\nrecommendation using probabilistic matrix factorization. In Proceedings of the\n17th ACM conference on Information and knowledge management. 931â€“940.\n[16] Yao Ma, Suhang Wang, Tyler Derr, Lingfei Wu, and Jiliang Tang. 2019. Attacking\ngraph convolutional networks via rewiring. arXiv preprint arXiv:1906.03750\n(2019).\n[17] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning\nof social representations. In Proceedings of the 20th ACM SIGKDD international\nconference on Knowledge discovery and data mining. 701â€“710.\n[18] Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding,\nKuansan Wang, and Jie Tang. 2020. Gcc: Graph contrastive coding for graph\nneural network pre-training. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining. 1150â€“1160.\n[19] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint\narXiv:1205.2618 (2012).\n[20] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele\nMonfardini. 2008. The graph neural network model. IEEE transactions on neural\nnetworks 20, 1 (2008), 61â€“80.\n[21] Yiwei Sun, Suhang Wang, Xianfeng Tang, Tsung-Yu Hsieh, and Vasant Honavar.\n2019. Node injection attacks on graphs via reinforcement learning. arXiv preprint\narXiv:1909.06543 (2019).\n[22] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, RÃ©mi Munos,\nPetar VeliÄkoviÄ‡, and Michal Valko. 2021. Bootstrapped Representation Learning\non Graphs. arXiv preprint arXiv:2102.06514 (2021).\n[23] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.\nJournal of machine learning research 9, 11 (2008).\n[24] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2018. Graph attention networks. In Proceedings of ICLR.\n[25] Petar VeliÄkoviÄ‡, William Fedus, William L Hamilton, Pietro LiÃ², Yoshua Bengio,\nand R Devon Hjelm. 2019. Deep graph infomax. In Proceedings of ICLR.\n[26] Marcin Waniek, Tomasz P Michalak, Michael J Wooldridge, and Talal Rahwan.\n2018. Hiding individuals and communities in a social network. Nature Human\nBehaviour 2, 2 (2018), 139â€“147.\n[27] Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong,\nand Xue Lin. 2019. Topology attack and defense for graph neural networks: An\noptimization perspective. arXiv preprint arXiv:1906.04214 (2019).\n[28] Haoran Yang, Hongxu Chen, Lin Li, Philip S Yu, and Guandong Xu. 2021. Hyper\nMeta-Path Contrastive Learning for Multi-Behavior Recommendation. arXiv\npreprint arXiv:2109.02859 (2021).\n[29] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016. Revisiting semi-\nsupervised learning with graph embeddings. In International conference on ma-\nchine learning. PMLR, 40â€“48.\n[30] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and\nYang Shen. 2020. Graph contrastive learning with augmentations. Advances in\nNeural Information Processing Systems 33 (2020).\n[31] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor\nPrasanna. 2019. Graphsaint: Graph sampling based inductive learning method.\narXiv preprint arXiv:1907.04931 (2019).\n[32] Jiaqi Zeng and Pengtao Xie. 2020. Contrastive self-supervised learning for graph\nclassification. arXiv preprint arXiv:2009.05923 (2020).\n[33] Hengtong Zhang, Tianhang Zheng, Jing Gao, Chenglin Miao, Lu Su, Yaliang Li,\nand Kui Ren. 2019. Towards data poisoning attack against knowledge graph\nembedding.\n[34] Sixiao Zhang, Hongxu Chen, Xiao Ming, Lizhen Cui, Hongzhi Yin, and Guandong\nXu. 2021. Where are we in embedding spaces? A Comprehensive Analysis on\nNetwork Embedding Approaches for Recommender Systems. arXiv preprint\narXiv:2105.08908 (2021).\n[35] Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Lizhen Cui, and Xiangliang\nZhang. 2021. Graph Embedding for Recommendation against Attribute Inference\nAttacks. In Proceedings of the Web Conference 2021. 3002â€“3014.\n[36] Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2019. Robust graph\nconvolutional networks against adversarial attacks. In Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery & Data Mining.\n1399â€“1407.\n[37] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020.\nDeep graph contrastive representation learning. arXiv preprint arXiv:2006.04131\n(2020).\n[38] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020.\nGraph Contrastive Learning with Adaptive Augmentation.\narXiv preprint\narXiv:2010.14945 (2020).\n[39] Marinka Zitnik and Jure Leskovec. 2017. Predicting multicellular function through\nmulti-layer tissue networks. Bioinformatics 33, 14 (2017), i190â€“i198.\n[40] Daniel ZÃ¼gner, Amir Akbarnejad, and Stephan GÃ¼nnemann. 2018. Adversarial\nattacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining. 2847â€“2856.\n[41] Daniel ZÃ¼gner and Stephan GÃ¼nnemann. 2019. Adversarial attacks on graph\nneural networks via meta learning. arXiv preprint arXiv:1902.08412 (2019).\n9\n",
  "categories": [
    "cs.LG",
    "cs.IR"
  ],
  "published": "2022-01-20",
  "updated": "2022-01-27"
}