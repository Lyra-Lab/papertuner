{
  "id": "http://arxiv.org/abs/2201.08670v2",
  "title": "Context-Tuning: Learning Contextualized Prompts for Natural Language Generation",
  "authors": [
    "Tianyi Tang",
    "Junyi Li",
    "Wayne Xin Zhao",
    "Ji-Rong Wen"
  ],
  "abstract": "Recently, pretrained language models (PLMs) have had exceptional success in\nlanguage generation. To leverage the rich knowledge encoded by PLMs, a simple\nyet powerful paradigm is to use prompts in the form of either discrete tokens\nor continuous embeddings. In existing studies, these prompting methods are\ntypically independent of the inputs, lacking sufficient consideration of input\nsemantics. To address this issue, we propose a novel continuous prompting\napproach, called context-tuning, to fine-tuning PLMs for natural language\ngeneration. Firstly, the prompts are derived based on the input text to elicit\nuseful knowledge from PLMs for generation. We refer to such prompts as\ncontextualized prompts. Secondly, we use continuous inverse prompting to\nimprove the process of natural language generation by modeling an inverse\ngeneration process from output to input, making the generated text more\nrelevant to the inputs. Furthermore, we utilize a lightweight context-tuning\nmethod that fine-tunes only 0.12% of the parameters while maintaining good\nperformance. Our code is publicly available at\nhttps://github.com/RUCAIBox/Context-Tuning.",
  "text": "Context-Tuning: Learning Contextualized Prompts\nfor Natural Language Generation\nTianyi Tang1,4, Junyi Li1,3, Wayne Xin Zhao1,4,5 B and Ji-Rong Wen1,2,4\n1Gaoling School of Artiﬁcial Intelligence, Renmin University of China\n2School of Information, Renmin University of China\n3DIRO, Université de Montréal\n4Beijing Key Laboratory of Big Data Management and Analysis Methods\n5Beijing Academy of Artiﬁcial Intelligence, Beijing, 100084, China\nsteventianyitang@outlook.com lijunyi@ruc.edu.cn batmanfly@gmail.com\nAbstract\nRecently, pretrained language models (PLMs)\nhave had exceptional success in language gen-\neration.\nTo leverage the rich knowledge\nencoded by PLMs, a simple yet powerful\nparadigm is to use prompts in the form of\neither discrete tokens or continuous embed-\ndings.\nIn existing studies, these prompting\nmethods are typically independent of the in-\nputs, lacking sufﬁcient consideration of input\nsemantics. To address this issue, we propose a\nnovel continuous prompting approach, called\ncontext-tuning, to ﬁne-tuning PLMs for natu-\nral language generation. Firstly, the prompts\nare derived based on the input text to elicit\nuseful knowledge from PLMs for generation.\nWe refer to such prompts as contextualized\nprompts.\nSecondly, we use continuous in-\nverse prompting to improve the process of nat-\nural language generation by modeling an in-\nverse generation process from output to in-\nput, making the generated text more relevant\nto the inputs.\nFurthermore, we utilize a\nlightweight context-tuning method that ﬁne-\ntunes only 0.12% of the parameters while\nmaintaining good performance.\nOur code\nis publicly available at https://github.\ncom/RUCAIBox/Context-Tuning.\n1\nIntroduction\nNatural language generation (a.k.a. text genera-\ntion) aims to produce plausible and readable text in\nhuman language from input data (Li et al., 2022).\nRecently, large-scale pretrained language models\n(PLMs) such as BART (Lewis et al., 2020) have\nhad exceptional success in language generation.\nTo leverage the encoded knowledge from PLMs,\nprompting methods have been proposed (Liu et al.,\n2021), where the original input to PLMs has been\nextended by prepending discrete tokens or continu-\nous embeddings (called prompts). Following this\nparadigm, this work aims to study how to develop\nB Corresponding author\nTitle\n::::::::\nLive-action:::::::::\nmedium\nis\ninferior\nto\nanimation medium\nStatic Prompts: Write a story about: Title\nContextualized Prompts: p1···k Title pk+1···2k\nStory\nI think that live-action works can’t be considered\nart. They feel more like:::::::::::\ndocumentaries or :::::\ntheater\n::::\npieces with :::\nCGI combined. The superiority of\nanimated works is that they are more abstract and\nimaginative and characters show more emotion\nand variety of designs.\nTable 1: Example inputs (titles) and outputs (stories)\nof generation dataset CMV. Static prompts are human-\nwritten instructions, which are independent of input ti-\ntles. p1···k and pk+1···2k are contextualized prompts,\nwhich are derived conditioned on the input title. The\nwavy line and underline denote the corresponding in-\nformation between input and output.\nmore effective prompting methods for text genera-\ntion based on PLMs.\nEarly methods focused on human-written (dis-\ncrete) prompts by manually constructing task-\nspeciﬁc prompt templates (Raffel et al., 2020; Rad-\nford et al., 2019), such as “TL;DR:” for the sum-\nmarization task. Recent work has further proposed\nutilizing continuous prompts (Li and Liang, 2021;\nLester et al., 2021) for text generation. Continu-\nous prompts consist of trainable parameters that\ndo not correspond to real tokens and can be eas-\nily optimized during ﬁne-tuning. However, exist-\ning prompting approaches typically adopt static\nprompts for generation, i.e., the prompts contain\ntask-related information but remain the same for\ndifferent input texts.\nIn this work, we mainly focus on challeng-\ning open-ended generation, such as story gener-\nation (Fan et al., 2018) and review generation (Li\net al., 2019). Under this setting, the input text usu-\nally contains very limited information, while the\ntask goal is to generate an output sequence con-\ntaining informative contents based on the given\nlimited input. The example in Table 1 aims to gen-\narXiv:2201.08670v2  [cs.CL]  3 Oct 2022\nerate a story about the topics of “live-action” and\n“animation”. In such a case, it requires in-depth\nbackground knowledge about the two topics. As\nwe can see, static prompts such as “Write a story\nabout:” are independent of the input title, making\nit difﬁcult to capture the related aspects for this gen-\neration task. Instead of static prompts, we argue\nthat contextualized prompts (as shown in Table 1)\nderived based on the input title will be more suited\nfor this setting.\nTo address the above issues, we propose\nContext-Tuning, a novel continuous prompting\napproach to ﬁne-tuning PLMs for natural language\ngeneration. Our approach has three major techni-\ncal contributions. Firstly, the prompts are derived\nbased on input text to enrich the input by elicit-\ning related knowledge from PLMs. Speciﬁcally,\nby concatenating limited input and a sequence of\n“[MASK]” tokens into BERT (Devlin et al., 2019),\nwe leverage its excellent mask-ﬁlling ability to pre-\ndict these tokens, and the last hidden state of tokens\ncan be used as prompt vectors. Since the prompts\nare highly related to the input context, we refer to\nthem as contextualized prompts. Secondly, to fur-\nther enhance the relevance between the generated\ntext and the input text, we extend inverse prompt-\ning (Zou et al., 2021) by incorporating continuous\nprompts. We refer to them as continuous inverse\nprompting. By maximizing the likelihood of pre-\ndicting inputs conditioned on the generated text\nand continuous prompts, context-tuning can gen-\nerate texts highly relevant to the input text. More-\nover, to ease the training burden, we propose to use\na lightweight context-tuning method (Ben Zaken\net al., 2022) that only ﬁne-tunes the bias term of all\nmodel parameters. In this way, we can achieve a\ncomparable performance (98.0% of the full-tuned\nperformance) by only tuning 0.12% of the parame-\nters, compared to full-tuned context-tuning.\nTo our knowledge, we are the ﬁrst to encode\ninput-related information into continuous prompts\nfor text generation. Our context-tuning approach\ncan elicit relevant knowledge according to speciﬁc\ninput text and enhance the relevance between the\ngenerated text and the input text. We compare our\nmethod with several baseline models for the eval-\nuation of four natural language generation tasks.\nExtensive experiments demonstrate the effective-\nness of our proposed context-tuning approach.\n2\nRelated Work\nNatural Language Generation.\nNatural lan-\nguage generation is one of the most challenging\nﬁelds in natural language processing (NLP). It aims\nto produce human-readable text from input text.\nCurrent state-of-the-art results for many genera-\ntion tasks are based on ﬁne-tuning PLMs, such\nas text summarization (Lewis et al., 2020), dia-\nlogue system (Zhang et al., 2020), and data-to-\ntext generation (Ribeiro et al., 2021). As men-\ntioned in Liu et al. (2021), controlled text gener-\nation is relevant to our input-dependent method.\nThe goal of controlled text generation is to di-\nrect the generated texts into speciﬁc styles (Hu\net al., 2017), lengths (Kikuchi et al., 2016), or key-\nwords (Dou et al., 2021). In contrast, our contex-\ntualized prompts elicit knowledge from PLMs to\nenrich the input rather than control the speciﬁc\nproperties of generated text.\nPrompting\nLearning.\nPrompting\nmethods\nprepend task instructions to the input and generate\nthe output from PLMs.\nMost typical methods\nutilize manually designed task-speciﬁc prompts\nto adapt to different generation tasks (Radford\net al., 2019; Raffel et al., 2020).\nHowever, it\nis time-consuming and laborious to construct\nhuman-written prompts for various generation\ntasks. As a result, recent research has concen-\ntrated on automating the search for discrete\nprompts (Shin et al., 2020; Gao et al., 2021).\nNonetheless, searching for prompts over discrete\nspace is challenging to optimize due to the\nnon-differentiable issues and continuous nature\nof neural networks. To handle these problems,\nmany studies propose optimizing continuous\nprompts (Lester et al., 2021; Li and Liang, 2021),\nwhich are more expressive and ﬂexible for any task.\nAmong these works, preﬁx-tuning (Li and Liang,\n2021) and prompt tuning (Lester et al., 2021) are\ntwo representatives focused on text generation\nand natural language understanding (NLU) tasks,\nrespectively.\nCompared with these continuous\napproaches, our context-tuning encodes the context\ninformation of inputs into the contextualized\nprompts and adopts continuous inverse prompting\nto enhance relevance further.\nMost existing prompting methods (Schick and\nSchütze, 2021a; Shin et al., 2020; Lester et al.,\n2021) focus on NLU tasks, which are choice\nquestions that can easily be converted into ﬁlling\nContextualized Prompt Generator\nM\nM\n⋯\nM\nM\n⋯\n𝑥𝑥1\n𝑥𝑥𝑙𝑙\n⋯\nPretrained Language Model\n𝒑𝒑1\n𝑐𝑐\n𝒑𝒑𝑘𝑘\n𝑐𝑐\n⋯\n𝒑𝒑𝑘𝑘+1\n𝑐𝑐\n⋯\n𝒑𝒑2𝑘𝑘\n𝑐𝑐\n𝑥𝑥1\n𝑥𝑥𝑙𝑙\n⋯\nInverse Prompt Encoder\n𝒑𝒑1\n𝑖𝑖\n𝒑𝒑𝑘𝑘\n𝑖𝑖\n⋯\n𝒑𝒑𝑘𝑘+1\n𝑖𝑖\n⋯\n𝒑𝒑2𝑘𝑘\n𝑖𝑖\n𝑦𝑦𝑗𝑗,1\n𝑦𝑦𝑗𝑗,𝑛𝑛𝑗𝑗\n⋯\n𝑦𝑦𝑗𝑗\n(1)\n⋮\n𝑦𝑦𝑗𝑗\n(𝑏𝑏)\n5.89   \n4.12\nContextualized\nPrompts\ninverse prompts\ninput sequence\nPr(𝒳𝒳|𝒴𝒴, 𝒫𝒫𝑖𝑖)\nPr(𝒴𝒴|𝒳𝒳, 𝒫𝒫𝑐𝑐)\nInverse\nPrompting\ncandidate\nsentence\nFigure 1: The overview of the proposed context-tuning. “[M]” denotes the mask token “[MASK]”. By combining\nthe forward probability Pr(Y|X, Pc) (the left part) and backward probability Pr(X|Y, Pi) (the right part), we\nselect the sequence y(i)\nj\nwith the highest combined scores from all the candidates.\n“[MASK]” tasks. However, text generation aims\nto generate a sequence of tokens, in contrast to a\nfew options in limited space. For example, preﬁx-\ntuning (Li and Liang, 2021) and GENPET (Schick\nand Schütze, 2021b) have employed prompting\nmethods for text generation. However, they mainly\nfocus on lightweight ﬁne-tuning or few-shot learn-\ning and do not achieve great performance under\nfull tuning settings. In contrast, our context-tuning\ncan improve performance under full tuning settings,\nand the lightweight strategy tunes only 0.2% of the\nparameters while retaining good performance.\n3\nThe Proposed Approach\nIn this section, we present the proposed context-\ntuning to ﬁne-tune PLMs for natural language\ngeneration. We ﬁrst introduce the contextualized\nprompts based on the input text for generating in-\nformative text. To further enhance the relevance of\nthe generated text to the input, we utilize contin-\nuous inverse prompting to enforce the prediction\nof inputs given the generated text and continuous\nprompts. Figure 1 presents an overall illustration\nof the proposed context-tuning approach.\nFor natural language generation, we consider a\ngeneral task setting, where the model generates\nthe output sequence Y conditioned on the input\nsequence X = ⟨x1, . . . , xl⟩. The output text is\nusually composed of multiple sentences: Y =\n{yj : ⟨yj,1, . . . , yj,t, . . . , yj,nj⟩}m\nj=1. In context-\ntuning, we introduce contextualized prompts Pc =\n⟨pc\n1, . . . , pc\nk⟩into the input side. Thus, the prompt-\nbased generation task can be formulated as:\nPr(Y|X, Pc) = Pr(y1, . . . , ym|x1, . . . , xl, Pc).\n(1)\n3.1\nContextualized Prompts\nInstead of static prompts (Lester et al., 2021) (ir-\nrelevant to input), we use contextualized prompts,\nwhich are expected to provide additional informa-\ntion, such as world knowledge, commonsense and\ntask information extracted from PLMs to enrich the\nlimited input text.\nMasked Prompt Learning.\nSpeciﬁcally, unlike\npreﬁx-tuning (Li and Liang, 2021), which prepends\na sequence of static vectors to each layer of PLMs,\nwe append a sequence of k continuous vectors on\nboth the left and right sides of the input sequence\nX (2k vectors in total). Inspired by the masked\nlanguage modeling task of BERT (Devlin et al.,\n2019), we use BERT as the prompt generator to\nderive the contextualized prompt vectors. We ﬁrst\nplace a sequence of k “[MASK]” tokens on both\nsides of the input X as:\ne\nX = [MASK]1,...,k, X, [MASK]k+1,...,2k.\n(2)\nBy feeding e\nX as the input of the prompt generator,\nwe can obtain the top-layer representations of these\n“[MASK]” tokens:\n˜pc\n1, . . . , ˜pc\nk\n|\n{z\n}\npreﬁx prompts\n/ ˜pc\nk+1, . . . , ˜pc\n2k\n|\n{z\n}\nsufﬁx prompts\n= Prompt-Generator( e\nX).\n(3)\nAfter shown in Section 4.4, we set k to 150 with\nthe best performance. Compared with randomly-\ninitialized prompts, our BERT-based prompt learn-\ning method can better learn the dependency be-\ntween the prompts and input texts.\nAligning to Word Embeddings.\nSince these\nprompt vectors are latent embeddings, we further\nalign them to the semantic space of word embed-\ndings by designing a two-step semantic mapping\noperator. For the ﬁrst step, BERT predicts the prob-\nability distribution over its vocabulary based on\nthese top-layer representations:\nPr(w| e\nX) = softmax(W V ˜pc\nk),\n(4)\nwhere W V is a trainable matrix.\nFor the sec-\nond step, we multiply the probability distribution\nPr(w| e\nX) with the word embedding matrix E and\nobtain the ﬁnal contextualized prompt vectors:\npc\nk = E · Pr(w| e\nX).\n(5)\nWe consider these mapped vectors as contextu-\nalized prompts. Intuitively, the above semantic\nmapping can be considered as a weighted average\nof word embeddings according to their probabili-\nties. Compared with existing continuous prompts,\nour contextualized prompt vectors can better corre-\nspond to real word embeddings in semantic space,\nas shown in Section 4.6.\nApplying the Prompts.\nAfter obtaining the con-\ntextualized prompts, we combine these prompt vec-\ntors and the word embeddings of X as the input\nof PLMs for generating the output text Y. Speciﬁ-\ncally, we utilize BART as the base PLM to generate\ntext by minimizing the cross-entropy loss function:\nLc = −log Pr(Y|X, Pc)\n(6)\n= −log Pr(Y|pc\n1···k, x1···l, pc\nk+1···2k),\nwhere pc\n1···k denotes pc\n1, . . . , pc\nk, x1···l denotes\nx1, . . . , xl, and pc\nk+1···2k denotes pc\nk+1, . . . , pc\n2k.\nBy leveraging the encoded knowledge from PLM,\nthe contextualized prompts are helpful to generate\ninformative output texts.\n3.2\nContinuous Inverse Prompting\nAlthough contextualized prompts can improve the\ninformativeness of output, it still suffers from the\noff-topic generation issue as the text length in-\ncreases (Zou et al., 2021). To deal with this is-\nsue, we propose continuous inverse prompting to\nenhance the relevance in an inverse manner from\noutput to input.\nCompared to the previous in-\nverse prompting that depends on artiﬁcial construc-\ntion (Zou et al., 2021), our inverse prompting is\nbased on continuous prompts, which can be ﬂexi-\nbly optimized during ﬁne-tuning.\nAlgorithm 1 The algorithm procedure for genera-\ntion process of context-tuning.\nRequire: Model parameters Θ(c) and Θ(i), beam size b and\nmaximum number of sentences nm\n1: Input: An input sequence X\n2: Output: A generated sequence Y\n3: Initialize step j = 0\n4: while j < nm do\n5:\nDerive contextualized prompts Pc based on X\n6:\nGenerate b candidate sentences y(1)\nj , . . . , y(b)\nj\naccord-\ning to Eq. 6\n7:\nUtilize continuous inverse prompts Pi to compute the\nlikelihood of candidate sentences according to Eq. 7\n8:\nChoose the best sentence as ys based on Eq. 8\n9:\nTerminate the loop if ys contains the end of sentence\ntoken\n10:\nUpdate j = j + 1\n11: end while\n12: Concatenate y1, . . . , yj as generated sequence Y\n13: return Y\nOutput-to-Input Relevance Enhancement.\nTo\nmodel the relevance of output Y to input X, we\nhypothesize that the output text is highly relevant\nto the input text if we can recover the input based\non the output. Nevertheless, in some text gener-\nation tasks, it is non-intuitive to generate the in-\nput text given the output text. Hence, we utilize\nprompts to mitigate this issue. We introduce con-\ntinuous inverse prompts Pi and append them on\nboth sides of the output Y. Then, we utilize an-\nother PLM to measure the conditional probability\nPr(X|Y, Pi). Considering the output text Y might\nbe much longer than the input text X, we further\nmodel the probability at the sentence level:\nLi = −log Pr(X|Y, Pi)\n(7)\n= −\nm\nX\nj=1\nlog Pr(X|pi\n1···k, yj,1, . . . , yj,nj, pi\nk+1···2k),\nwhere pi\n1···k denotes pi\n1, . . . , pi\nk and pi\nk+1···2k\ndenotes pi\nk+1, . . . , pi\n2k.\nUnlike contextualized\nprompts in Section 3.1, we expect inverse prompts\nto reﬂect better the relationship between Y and X,\nwhich is dependent on the task rather than the input.\nThus, the inverse prompts are static and continuous\nin our approach.\nGeneration with Inverse Prompting.\nWith the\ntwo techniques mentioned above in the generation\nprocess, we utilize a modiﬁed beam search algo-\nrithm shown in Algorithm 1 to generate the se-\nquence Y with the highest combined probability:\nY = argmax\nY\nlog Pr(Y|X, Pc) + λ log Pr(X|Y, Pi),\n(8)\nwhere λ is a hyper-parameter to balance these two\nprobabilities. We set λ to 4.0 with the best balance\nof performance.\nIn contrast to contextualized prompts that enrich\nthe input information, continuous inverse prompt-\ning makes the generation process more controllable.\nEven for latter generated sentences, it can still en-\nforce them to adhere to the input topic.\n3.3\nDiscussion and Learning\nIn this part, we present the model discussion and\noptimization.\nDiscussion and Comparison.\nWe use contextu-\nalized prompts (Eq. 6) to elicit useful knowledge\nfrom PLMs for different inputs. As a compari-\nson, previous continuous prompting methods (Li\nand Liang, 2021; Lester et al., 2021) adopt static\nprompts, which are irrelevant to the input. Besides,\nwe propose continuous inverse prompting (Eq. 7)\nto enforce the relevance of long output text by con-\nsidering a generation process from output to input.\nDifferent from the original inverse prompting, our\ninverse prompting is based on continuous prompts,\nwhich can be optimized during ﬁne-tuning.\nConsidering that our method involves an-\nother PLM and more parameters, we propose a\nlightweight context-tuning approach. Following\nBen Zaken et al. (2022), we only ﬁne-tune the bias\nterm of each parameter, resulting in ﬁne-tuning\nonly 0.12% of the parameters of complete mod-\nels. In the meanwhile, preﬁx-tuning (Li and Liang,\n2021) and prompt tuning (Lester et al., 2021) freeze\nthe PLM and only ﬁne-tune the parameters of\nprompts. Preﬁx-tuning ﬁne-tunes prompts in each\nlayer and tunes 16.4% of the BART parameters,\nwhile prompt tuning only ﬁne-tunes the prompt\nconcatenated to the input, resulting in ﬁne-tuning\n0.05% of the BART parameters.\nOptimization.\nWe use the base version of BERT\nas our prompt generator. The number of prompt\nvectors k is set to 150. We utilize the base version\nof BART for text generation. The hyper-parameter\nλ in Eq. 8 is set to 4.0. There are two sets of\ntrainable parameters in contextualized prompts and\ncontinuous inverse prompting, denoted by Θ(c) and\nΘ(i), respectively. First, we optimize Θ(c), includ-\ning BERT and BART, according to Eq. 6. Mean-\nwhile, we optimize Θ(i) according to the inverse\ngeneration loss using Eq. 7. During inference, we\ncombine them and select sentences that are both\nDataset\n#Train\n#Valid\n#Test\n#Input\n#Output\nWP\n53,516\n4,000\n2,000\n25.48\n150.64\nROC\n176,688\n9,816\n4,909\n9.02\n40.72\nCMV\n42,462\n6,480\n7,562\n17.89\n104.10\nWIKIP\n69,288\n8,661\n8,662\n3.38\n194.72\nTable 2: Statistics of our datasets after preprocessing.\n#Train, #Valid and, #Test denote the number of exam-\nples in training, valid, and test datasets, respectively.\n#Input and #Output denote the average number of to-\nkens in the input and output text.\ninformative and relevant to the input text based on\nAlgorithm 1 and Eq. 8.\n4\nExperiment\nIn this section, we ﬁrst set up the experiments and\nthen report the results and analysis.\n4.1\nExperimental Setup\n4.1.1\nConstruction of the Datasets\nTo measure the performance of our proposed\ncontext-tuning, we evaluate it on four open-\nended text generation tasks: WRITINGPROMPTS\n(WP), ROCSTORIES (ROC), CHANGEMYVIEW\n(CMV), and WIKIPLOTS (WIKIP). Speciﬁcally,\nWP (Fan et al., 2018) consists of pairs of story\npremises and responses from the WritingPrompts\nforum.\nROC (Mostafazadeh et al., 2016) is a\ndataset consisting of ﬁve-sentence commonsense\nstories. Here, we use the ﬁrst sentence as the input\nto generate the following four sentences. For WP\nand ROC, we utilize the version provided by Guan\net al. (2021) for a fair comparison. CMV (Hua\nand Wang, 2020) contains pairs of post statements\non a controversial issue, which are collected from\nReddit. WIKIP1 is a collection of story plots from\nWikipedia. We use the sub-header word to generate\nthe full story.\nSince some dataset outputs are signiﬁcantly long,\nwe discard examples where the text contains more\nthan 512 tokens due to the length limitation of\nPLMs. We summarize the statistics of four datasets\nafter preprocessing in Table 2.\n4.1.2\nBaseline Methods\nWe consider the following baselines as compar-\nisons: GPT-2, BART, T5, HINT, preﬁx-tuning,\nand prompt tuning. Among these baselines, GPT-\n2 (Radford et al., 2019), BART (Lewis et al.,\n1https://github.com/markriedl/\nWikiPlots\nModels\nWRITINGPROMPTS\nROCSTORIES\n#Para\nBLUE-1\nBLUE-2\nDist-1\nDist-4\nBLUE-1\nBLUE-2\nDist-1\nDist-4\nFull ﬁne-tuning\nGPT-2\n24.94\n9.03\n1.40\n35.38\n31.45\n14.26\n2.21\n58.63\n1.2×108\nT5\n20.76\n7.41\n1.25\n27.77\n31.31\n14.23\n2.22\n54.31\n2.2×108\nBART\n28.42\n11.31\n2.11\n62.05\n32.95\n15.35\n2.70\n68.88\n1.4×108\nHINT\n22.40\n8.40\n–\n31.30\n33.40\n15.40\n–\n69.30\n1.4×108\nContext-Tuning\n29.88\n11.85\n2.49\n67.78\n34.65\n16.60\n3.16\n75.53\n2.5×108\nLightweight ﬁne-tuning\nPrompt Tuning\n16.26\n5.18\n3.71\n69.68\n27.27\n10.49\n2.44\n62.12\n7.6×104\nPreﬁx-Tuning\n28.39\n10.76\n1.72\n58.34\n30.62\n13.51\n2.51\n67.19\n2.3×107\nContext-Tuning\n29.45\n10.90\n1.78\n62.89\n32.24\n14.30\n2.49\n68.92\n3.0×105\nCHANGEMYVIEW\nWIKIPLOTS\nBLUE-1\nBLUE-2\nDist-1\nDist-4\nBLUE-1\nBLUE-2\nDist-1\nDist-4\nFull ﬁne-tuning\nGPT-2\n23.39\n8.32\n0.75\n37.18\n23.74\n9.33\n0.90\n38.39\n1.2×108\nT5\n20.89\n7.79\n1.01\n42.48\n14.83\n6.09\n1.33\n39.25\n2.2×108\nBART\n25.69\n9.77\n1.11\n61.21\n27.12\n11.54\n1.82\n49.54\n1.4×108\nContext-Tuning\n26.11\n10.00\n0.99\n57.38\n27.80\n11.82\n2.05\n51.96\n2.5×108\nLightweight ﬁne-tuning\nPrompt Tuning\n22.54\n8.11\n1.43\n64.60\n18.64\n6.98\n2.78\n58.53\n7.6×104\nPreﬁx-Tuning\n25.72\n9.84\n0.96\n54.66\n27.30\n11.60\n1.95\n51.05\n2.3×107\nContext-Tuning\n28.83\n10.96\n0.97\n57.79\n26.93\n11.53\n2.48\n59.34\n3.0×105\nTable 3: Performance comparison of different methods for open-ended text generation tasks. Dist-n is short for\nDistinct-n. Bold and underlined fonts denote the best and second-best methods (the same below). #Para denotes\nthe number of ﬁne-tuned parameters in each method. The results of HINT are from its original paper (Guan et al.,\n2021). “–” means HINT does not compute the corresponding result.\n2020), and T5 (Raffel et al., 2020) are three\nprevalent PLMs for natural language generation;\nHINT (Guan et al., 2021) is a strong baseline\nmodel specially designed for generating long and\ncoherent texts; preﬁx-tuning (Li and Liang, 2021)\nand prompt tuning (Lester et al., 2021) are the re-\ncently proposed lightweight models using contin-\nuous prompts for generation tasks. We utilize the\nbase version for all PLMs for a fair comparison.\n4.1.3\nImplementation Details\nFor ﬁne-tuning settings, we consider two strategies:\nfull ﬁne-tuning and lightweight ﬁne-tuning, to com-\npare our methods with different baselines. In the\nlightweight ﬁne-tuning settings, our context-tuning\nonly tunes the bias term of each parameter.\nIn all experiments, we utilize the Adam opti-\nmizer and set β1 = 0.9, β2 = 0.999, ϵ = 1×10−8.\nWe train our model for 20 epochs and utilize the\nmodel with the best performance on the validation\nset for generation. During inference, we apply the\nnucleus sampling with p = 0.9 and temperature\nof 0.7. We train our model using NVIDIA A100\nGPUs on Ubuntu 18.04 and employ the NLP open-\nsource library Transformers (Wolf et al., 2020) and\ntext generation library TextBox (Li et al., 2021).\n4.1.4\nEvaluation Metrics\nTo evaluate the performance of different methods\nof natural language generation, we adopt two au-\ntomatic evaluation metrics, including BLEU (Pa-\npineni et al., 2002) and Distinct (Li et al., 2016).\nSpeciﬁcally, BLEU evaluates the quality of gen-\nerated and real text, while Distinct measures the\ndiversity of generated texts.\n4.2\nPerformance Comparison\nWe present the results of different methods on gen-\neration tasks in Table 3.\nFirst, we can see that BART performs best com-\npared to other PLMs on these generation tasks. Pre-\ntrained on the large-scale corpus, PLMs can better\nunderstand natural language and ﬂuently express\nhuman language. We consider the better perfor-\nModels\nB-1\nB-2\nD-1\nD-4\nContext-Tuning\n29.88\n11.85\n2.49\n67.78\nw/o Continuous w Manual\n- human-written prompt1\n27.96\n10.93\n1.92\n59.97\n- human-written prompt2\n29.14\n11.56\n2.09\n61.69\nw/o BERT w RoBERTa\n27.31\n10.73\n1.56\n57.51\nw/o Semantic Mapping\n29.19\n11.33\n1.72\n58.57\nw/o Inverse Prompting\n29.31\n11.45\n2.19\n64.09\nTable 4:\nAblation analysis on WRITINGPROMPTS\ndataset.\nmance of BART is due to the encoder-decoder ar-\nchitecture and the DAE pretraining task. That is\nthe major reason we adopt BART as our base gen-\neration model.\nSecond,\nthe recently proposed continuous\nprompting methods, preﬁx-tuning, and prompt tun-\ning do not achieve ideal performance in these tasks.\nThis ﬁnding shows that natural language generation\ntasks are more challenging than NLU tasks. Only\nﬁne-tuning a few parameters cannot outperform\nfull ﬁne-tuning.\nFinally, our model outperforms all the baselines\n(including the strong baseline HINT) over four\ntasks under both full and lightweight tuning set-\ntings. The reason is that our context-tuning utilizes\ncontextualized prompts, which can serve as queries\nto elicit input-relevant knowledge from PLMs. Un-\nder the lightweight ﬁne-tuning settings, our context-\ntuning has superior results to preﬁx-tuning, only\nwith 1.3% of the parameters of preﬁx-tuning and\n0.2% of the parameters of BART. Some of the per-\nformance under lightweight settings can even out-\nperform the full tuning, which may be a solution to\ncatastrophic forgetting (Wang et al., 2021). And a\nmajor reason is that preﬁx-tuning and prompt tun-\ning adopt static prompts, which are task-speciﬁc\nand unrelated to the context information.\n4.3\nAblation Analysis\nIn this part, we construct ablation experiments\nto test the effectiveness of our proposed context-\ntuning. In contrast to previous prompt-based stud-\nies, our context-tuning has made several improve-\nments.\nFirst, compared with manual prompts,\nwe propose a continuous prompting approach to\nﬁne-tuning PLMs. Second, we adopt BERT as\nthe prompt generator to derive the contextualized\nprompt vectors with semantic mapping. Finally, we\nutilize inverse prompting to enhance the relevance\nof the generated texts further. Here, we would like\nModels\nTT (%)\nFlu.\nInfo.\nRel.\nCoh.\nGPT-2\n81.20\n3.90\n3.27\n3.77\n3.50\nT5\n61.48\n3.58\n3.02\n3.64\n3.25\nBART\n77.17\n3.82\n3.27\n3.74\n3.59\nContext-Tuning\n82.83\n4.12\n3.47\n3.94\n3.85\nGold\n94.00\n4.26\n3.90\n4.33\n4.01\nTable 5: Turing test (TT) and human evaluation on\nWRITINGPROMPTS. “Gold” indicates the ground-truth\ntexts. Flu., Info., Rel., and Coh. denote ﬂuency, infor-\nmativeness, relevance, and coherence, respectively.\nto examine how each factor contributes to the ﬁ-\nnal performance. To see this, we prepare several\nvariants for a comparison:\n• w/o Continuous w Manual:\nthe variant\nremoves the continuous prompts but utilizes two\nkinds of human-written prompts, i.e., prompt1:\n“Title:\n$Input Story:”\nand prompt2:\n“Given the title $Input, please\nwrite the following story:”.\n• w/o BERT w RoBERTa: the variant replaces\nBERT with RoBERTa (Liu et al., 2019) to form the\nprompt generator.\n• w/o Semantic Mapping: the variant does not\nalign to word embeddings and directly utilizes the\ntop-layer representations of “[MASK]” tokens in\nthe prompt generator.\n• w/o Inverse Prompting: the variant removes in-\nverse prompting (Eq. 8) from our proposed context-\ntuning.\nFrom Table 4, we can see that variants replac-\ning continuous prompts with manual prompts are\nworse than the model with continuous prompts.\nThe performance of manual prompts is sensitive to\ndifferent instructions and does not always lead to\ngains. This veriﬁes the effectiveness of utilizing\ncontinuous prompts rather than discrete ones for\ntext generation tasks. The variants replacing the\nBERT-based prompt generator with RoBERTa are\nworse than the full model. We further observe a\nslight performance drop when our method removes\nthe semantic mapping and inverse prompting. This\nimplies that the proposed semantic mapping and\ncontinuous inverse prompting approaches can en-\nforce the informativeness relevance of output text.\n4.4\nModel Sensitivity\nIn this part, we construct sensitivity analyses w.r.t.\nthe number k of prompt vectors on the WRITING-\nPROMPTS dataset.\nIn contextualized prompt learning, the number\n#Prompt\nB-1\nB-2\nD-1\nD-4\nk = 50\n27.82\n11.00\n2.28\n63.77\nk = 100\n28.55\n11.12\n2.17\n63.69\nk = 150\n29.31\n11.45\n2.19\n64.09\nk = 200\n28.48\n11.22\n1.97\n61.09\nTable 6: Performance tuning on WRITINGPROMPTS\ndataset. We do not utilize continuous inverse prompt-\ning methods here.\nof prompt vectors is a key factor that inﬂuences the\nperformance of our model. A longer sequence of\nprompt vectors means more trainable parameters\nand, therefore, more expressive power. Here, we\nwill examine how it affects the ﬁnal performance\nof our context-tuning. Given the statistics in Ta-\nble 2, we vary the number of prompt vectors in the\nset {50, 100, 150, 200}. We separately train our\nmodel with different numbers of prompt vectors\nand do not utilize continuous inverse prompting\nmethods for convenience. As shown in Table 6,\nthe performance of our model gradually improves\nas the number of prompt vectors increases up to\na threshold, and then a performance drop occurs.\nMore importantly, our model achieves the best per-\nformance with 150 prompt vectors over baselines.\n4.5\nHuman Evaluation\nBesides automatic evaluation, we further conduct\na human evaluation for testing the effectiveness of\nour approach. We randomly select 500 input texts\nfrom the test set of the WRITINGPROMPTS dataset.\nWe collect the stories generated by GPT-2, BART,\nT5, and context-tuning, then shufﬂe them for hu-\nman evaluation. Following Zou et al. (2021), we\ninvite ten human judges to assign scores to a gener-\nated text concerning four factors of quality, namely\ninformativeness (how much it provides valuable\nand meaningful information), relevance (how rel-\nevant it is according to the input contexts), coher-\nence evaluates (how coherent both intra and inter\nsentences are) and ﬂuency (how likely a human\nproduces the generated text).\nWe adopt a 5-point Likert scale as the scoring\nmechanism, in which 5-point means “very satisfy-\ning”, and 1-point means “very terrible”. Further-\nmore, inspired by Zou et al. (2021), we design a\nTuring test where a human judge is asked to dis-\ntinguish whether a human produces the given text.\nThe detailed evaluation guidelines and examples\nare listed in Figure 2, Figure 3, Figure 4, and Fig-\nure 5 in the Appendix.\nWe present the human evaluation results in Table\n5. It can be seen that our model is better than the\nthree baselines with a large margin. The major\nreason is that we utilize the contextualized prompts\nderived from the input text. Our contextualized\nprompts can extract knowledge from PLMs and\nserve as additional input information to be fed into\nPLMs, which improves the informativeness of the\ngenerated text. Moreover, the proposed continuous\ninverse prompting method enhances the relevance\nof the generated text to the input.\n4.6\nQualitative Analysis\nIn this part, we present an intuitive analysis of why\nour model works well.\nTable 7 presents an example story from the\nWRITINGPROMPTS dataset and the generated story\nby our model and two baselines, i.e., GPT-2 and\nBART. As we can see, there is limited information\nin the input premise, besides several keywords such\nas nature, documentary, and Pokémon.\nFirst, we can see that the story generated by\nour context-tuning is highly relevant to the input\ntext and conveys richer semantic information. A\nprimary reason might be that our contextualized\nprompts can elicit input-relevant knowledge from\nPLMs for generating more informative text. Al-\nthough PLMs perform well in generating ﬂuent\ntext, we can see that GPT-2 and BART are still\nprone to generating unmeaningful and irrelevant\ncontent, such as “I love you” and “language”.\nFurthermore, to probe whether our contextual-\nized prompts contain input-relevant knowledge, we\nﬁnd close actual words for a better explanation.\nWe use 2k contextualized prompts in total, and for\neach continuous prompt, we recall the word in the\nBERT vocabulary with the closest cosine distance\nto it. Finally, we select some words from 2k re-\ncalled words and showcase them grouped by four\naspects in the row closer words to contextualized\nprompts of Table 7. As we can see, most recalled\nkeywords are included in the story generated by\nour context-tuning. It shows that our contextu-\nalized prompts can better capture input-relevant\nknowledge. For example, the keywords “forest”,\n“woods”, and “animal” are closely related to the\naspect of nature.\nTitle\nWrite a Nature Documentary about Pokémon .\nGPT-2\n<newline> it’s been a long time since i’ve seen one of these things. <newline> i don’t know how long it has been, but i know that it will be long.\ni have seen it before, and i will see it again. it is the only thing i can think of that makes sense to me, because it makes me feel like i am part of\nsomething greater than myself. and that is why i want to know more about it. because if i could, i would know what it was like to be human. to\nhave a sense of purpose and purposelessness, to feel that there is a purpose in everything that you do, even if you are just a shell of what you once\nwere. but that does not make it any less of a feeling. that feeling that i feel when i look into the eyes of another human, when they look at me and\nsay, “ i love you. ”\nBART\nthe pokémon are the most powerful of all theokémon in the world. <newline> they are the best of the best, but they are also the most dangerous.\nthey have the ability to move faster than any otherokémon on the planet, and they are able to communicate with each other in a way that is impossible\nfor humans to understand. however, they are not able to ﬁght each other, because they are unable to communicate in a language that is not their\nown. they can only communicate through their own language, which is very different from any other language that humans can understand. they\nare very difﬁcult to understand, and can not communicate in any language that can be understood by humans. they also have a very limited ability\nto speak, so they can not understand the language that they are speaking. in fact, they have no ability to communicate at all, because their language\nis very similar to that of a human.\nCloser words to contextualized prompts: forest trees marching winds food .............\nhistorically .....\nstory .......\nmovie::\npet:::::\npikachu ::::::\nanimation whilst critical asking write\nContext-\nTuning\n“ :::::\npokémon! ” <newline> “ what are you talking about? ” ...\nthe......\nman........\nasked. “ i’m talking about a:::::\npokéball. it’s called ::::\npikachu, and it lives in the\nforest. it lives on the edge of the forest, where it can eat anything it wants to eat. it is the only thing that can keep it alive. it can live in the woods,\nbut it can also be eaten by any other creature. it has the ability to eat anything that it wants, and can even eat any other animal it wants. ” ...\nhe .....\nsaid.\nthe man looked at the man, and said, “ i don’t know what you’re talking about, but i do know that it can be eaten. ” the two men looked at each\nother, and ....\nthe .....\nman........\nspoke, “ you’ve got to be kidding me. ”\nTable 7: The generated examples of the given title from the WRITINGPROMPTS dataset. Marks in closer words to\ncontextualized prompts and texts generated by context-tuning refer to four chosen aspects, i.e., nature, documen-\ntary, Pokémon, and the story generation task.\n5\nConclusion\nThis paper presents a novel continuous prompting\napproach, i.e., context-tuning, to ﬁne-tuning PLMs\nfor natural language generation. The core idea is\nto inject input-related context information into con-\ntinuous prompts, called contextualized prompts, to\nenhance the informativeness of generation. The\ncontextualized prompts can elicit input-relevant\nknowledge from PLMs to enrich the input text. Fur-\nthermore, to enhance the relevance of the generated\ntext to the inputs, we adopt a continuous inverse\nprompting method to reﬁne the forward genera-\ntion process by modeling an inverse generation\nprocess from output to input. We also propose a\nlightweight method for efﬁcient training. Extensive\nexperiments on four generation tasks have demon-\nstrated the effectiveness of our model in ﬁne-tuning\nPLMs for text generation tasks.\nIn future work, we will consider integrating more\ntypes of context information (e.g., sentiment) to\nderive more expressive prompts and investigate\nhow our model could be applied to other tasks.\nAcknowledgement\nThis work was partially supported by Beijing Natu-\nral Science Foundation under Grant No. 4222027,\nBeijing Outstanding Young Scientist Program un-\nder Grant No. BJJWZYJH012019100020098 and\nBeijing Academy of Artiﬁcial Intelligence (BAAI).\nXin Zhao is the corresponding author.\nReferences\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. BitFit: Simple parameter-efﬁcient ﬁne-tuning\nfor transformer-based masked language-models. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 1–9, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nZi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao\nJiang, and Graham Neubig. 2021. GSum: A gen-\neral framework for guided neural abstractive summa-\nrization. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4830–4842, Online. Association for\nComputational Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nJian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wen-\nbiao Ding, and Minlie Huang. 2021. Long text gen-\neration by modeling sentence-level and discourse-\nlevel coherence. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 6379–6393, Online. Association for\nComputational Linguistics.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P. Xing. 2017. Toward con-\ntrolled generation of text.\nIn Proceedings of the\n34th International Conference on Machine Learning,\nvolume 70 of Proceedings of Machine Learning Re-\nsearch, pages 1587–1596. PMLR.\nXinyu Hua and Lu Wang. 2020. PAIR: Planning and\niterative reﬁnement in pre-trained transformers for\nlong text generation.\nIn Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 781–793, Online.\nAssociation for Computational Linguistics.\nYuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya\nTakamura, and Manabu Okumura. 2016.\nControl-\nling output length in neural encoder-decoders.\nIn\nProceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing, pages\n1328–1338, Austin, Texas. Association for Compu-\ntational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMike\nLewis,\nYinhan\nLiu,\nNaman\nGoyal,\nMar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016.\nA diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nJunyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xiaox-\nuan Hu, Puzhao Xie, Zhipeng Chen, Zhuohao Yu,\nWayne Xin Zhao, and Ji-Rong Wen. 2021. TextBox:\nA uniﬁed, modularized, and extensible framework\nfor text generation. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Con-\nference on Natural Language Processing: System\nDemonstrations, pages 30–39, Online. Association\nfor Computational Linguistics.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie,\nand Ji-Rong Wen. 2022. A survey of pretrained lan-\nguage models based text generation. arXiv preprint\narXiv:2201.05273.\nJunyi Li, Wayne Xin Zhao, Ji-Rong Wen, and Yang\nSong. 2019.\nGenerating long and informative re-\nviews with aspect-aware coarse-to-ﬁne decoding. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1969–\n1979, Florence, Italy. Association for Computational\nLinguistics.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n4582–4597, Online. Association for Computational\nLinguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016.\nA cor-\npus and cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation.\nIn Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019.\nLan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020.\nExploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nLeonardo F. R. Ribeiro, Martin Schmitt, Hinrich\nSchütze, and Iryna Gurevych. 2021. Investigating\npretrained language models for graph-to-text gen-\neration.\nIn Proceedings of the 3rd Workshop on\nNatural Language Processing for Conversational AI,\npages 211–227, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255–269, Online. Association for Com-\nputational Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. Few-shot\ntext generation with natural language instructions.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n390–402, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts.\nIn Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4222–4235, Online. Association for Computational\nLinguistics.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-\nanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang,\nand Ming Zhou. 2021. K-Adapter: Infusing Knowl-\nedge into Pre-Trained Models with Adapters.\nIn\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 1405–1418, On-\nline. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020.\nDIALOGPT : Large-\nscale generative pre-training for conversational re-\nsponse generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics:\nSystem Demonstrations, pages 270–\n278, Online. Association for Computational Linguis-\ntics.\nXu Zou, Da Yin, Qingyang Zhong, Hongxia Yang,\nZhilin Yang, and Jie Tang. 2021. Controllable gen-\neration from pre-trained language models via in-\nverse prompting. In Proceedings of the 27th ACM\nSIGKDD Conference on Knowledge Discovery &\nData Mining, KDD ’21, page 2450–2460. Associa-\ntion for Computing Machinery.\nThank you for taking time out of your busy schedule to participate in our scientiﬁc research evaluation!\nOur research work is to let the machine generate corresponding story, for a given title, and hope that it is\nas close as possible to what humans write. Hence, we need to evaluate whether it meets the standards that\npeople think.\nIn this task, you will see a title, idea or introduction such as:\nYou have a very shitty type of precognition .\nThen you’ll see a corresponding story or comment, either written by a human or a machine, but you don’t\nknow which one was written, and you’ll have to rate it based on the following standard:\n• Turing Test: whether the text was written by a human;\n• Fluency: whether the text has good form, logical reading and smooth sentences;\n• Informative: whether the text contains meaningful content and will not be boring to read;\n• Relevance: whether the text is highly relevant to the input context;\n• Coherence: whether the logic is coherent and not contradictory;\nFor Turing Test, just judge the text by instinct. For the other four factors, we adopt a 5-point likert scale as\nthe scoring mechanism, in which 5-point means “very satisfying”, and 1-point means “very terrible”.\nFigure 2: Evaluation guidelines for WRITINGPROMPTS dataset.\nFluency: whether the text has good form, logical reading and smooth sentences.\n• 1 point example: I was a exterminator on earth. I killed every insect and animal I killed in my life.\nIf I win, I go to heaven, lose, and go to hell. I was an exterminator in hell. I was the exterminator of\nthe world. I exterminated every insect, animal, and animal that I killed. I didn’t have to kill them all.\nI just had to do it. I had to kill all of them. I couldn’t let them get away with it. So I killed them all,\nand I did it all over again. And again and again. I don’t know how long I’ve been doing this, but it’s\nbeen a long time. (containing some grammatical errors and repetitive contents)\n• 1 point example: I followed you into the dark . We carved our names into the bark . “ Forever and\nalways Through short nights and through long days . ” But that tree withered so long Its voice died\nin sweet silent song . Dead and gone We left and lost our life bond . I always remember your words\nOf comfort and joy like the birds . Sweet and sad ; It was all I never had . For I carved both names\non that tree No kind woman would be with me . No more hope ; You and me , the end of a rope You\nwere not ever real and I know that now . “ Always to love you ” , I end with that vow . (there is no\ncontinuity between the words of the sentence, and the content is intermittent)\n• 3 point example: I’ve been trying to kill my master for years. I’ve tried to kill him for years, but he’s\nalways been there for me. He’s the only one who knows what I’m going to do, and I don’t care. I ’ll\nkill him if I have to. But I can’t do it anymore. I haven’t been able to do it for years now. I can not do\nit any more. I just want to go back to my master. I want to be with him again. But he won’t let me go\nback. I know it’s not fair, but I just need to get back to him. (each sentence is grammatically correct\nand ﬂuent, but contains certain repetitions and discontinuities in semantics)\n• 3 point example: It’s been a long time since I’ve seen her. She’s always been there for me. I’m not\nsure how long I have been here, but I know she’s here. I know I ’ll never see her again. I don’t know if\nshe ’ll ever see me again. But I know it’s time. I can feel it in my bones, in my skin, in the bones of my\nbones. I can’t help but think of her. I remember her when I ﬁrst met her, when I was young. She was\nso beautiful, so full of life. I couldn’t wait to meet her again, to see her smile again. (sentences are\nﬂuent, but similar words are used repeatedly in the sentence, resulting in ambiguous meaning\nand confusing)\n• 5 point example: Long ago his heart had warmed , three thousand years - long enough to mourn , the\ndeeds of past and of damnation , stripped of humanity and of his station . He resided in the pits of hell\nthe oldest friend of satan , waiting as the centuries pass watching hells inﬂation , resting on brimstone\nas passing devils chatter and laugh , who is this old man and what sin has made him . a curious young\nman with a glint in his eye asks his sentence , and with creaks and groans the old man rose for the ﬁrst\ntime in ages , he look at the spirit and with a heavy sigh he came out with , I ’m god and I made this .\n• 5 point example: Tell us your faults ? Really ? This was the question - the shibboleth - that unlocked\nthe cosmos ? The Masters could have picked a scientist to answer but they feared she might mask\nignorance . They could have picked from our global leaders bit they feared that they would mask\ndeceit . They could have picked a holy man but feared he would mask violence , oppression , hate ,\nintolerance ... the list of disqualifying sins was almost too long to enumerate . So they picked Josh\nThornton , a 45 year old MBA in human resources . “ Our greatest weakness ? Well , I think we work\na little too hard and , as a race , we might be a bit of a perfectionist . ”\nFigure 3: Evaluation examples for ﬂuency factor.\nInformative: whether the text contains meaningful content and will not be boring to read.\n• 1 point example: I’ve been doing this for years. I’ve been doing this for years. I’ve been doing this\nfor years. I’ve been doing this for years. I’ve been doing this for years. I’ve been doing this for years.\nI’ve been doing this for years. I’ve been doing this for years. I’ve been doing this for years. I’ve been\ndoing this for years. I’ve been doing this for years. I’ve been doing this for years. I’ve been doing this\nfor years. I’ve been doing this for years. I’ve been doing this for years. (sentence repetition)\n• 1 point example: “ I’m the First ****bender. ” “ What’s that? ” I asked. “ I don’t know, ” he replied.\n“ But you’re the ﬁrst ****bender, right? ” “ Yeah, ” I said. “ You’re not the ﬁrst. ” “ Well, I guess\nthat’s true. ” He paused. “ What do you mean? ” He looked at me. “ That’s right. ” I looked at him. “\nIt’s not like you’ve ever been the ﬁrst... ” He trailed off. “ No, it’s just... ” I trailed off, “ I’ve never\nbeen the First... ” (conversation sentences are very short, and they are all safe replies)\n• 3 point example: “ What the hell is this? ” “ I don’t know, ” I said. “ I’m not sure what to do with\nthis. I’ve been doing this for a long time, and it’s not like I ’ll be able to do anything about it. ” He\nlooked at me with a confused look on his face. “ What the fuck? ” I asked. “ You’re not going to do\nthis again. ” I replied. “ It’s just a matter of time, ” he said. I looked at the frying pan again. “ Don’t\nworry, it ’ll work. It ’ll do. ” “ What’s the problem? ” He asked. I nodded. “ Well, I guess I ’d better\nget out of here. ” The frying pan buzzed at me and text appeared reading “ level 18 cooking required\nto use object. ” (containing rich dialogue, but the content of the dialogue is meaningless)\n• 3 point example: The stars fell like rain, and we danced. The moon was bright, and the stars danced.\nThe stars were bright, but the stars were not bright. They were bright and the sky was blue. We danced,\nbut we didn’t know what to do. We danced and we sang and we laughed and we talked and we cried\nand we screamed and we played and we giggled and we thought we were going to die, but it wasn’t. It\nwas just the stars falling like rain. And we danced, and it was beautiful. (the article has a certain\ncontent at the beginning, but it is more verbose at the end)\n• 5 point example: She is the woman you have loved since the day you met her . Everyday she has a\nsmile on her face beautiful as ever . You love her but are afraid of what she will say when you tell her\n. It was raining and you ran for shelter , a small roof at the bus stop . Tired and panting you barely\nnotice her sitting beside you , she calls your name . You jump a little bit and become nervous when\nyou recognize her . You stare at each other not knowing what to do or say , and then she kisses you .\nAn alarm sounds , you wake up in your room all alone , another dream (a whole story)\n• 5 point example: I feel like it ’s worth pointing out that a lot of these are kind of situational . I think\nall of these rules are good to follow if what you ’re writing is something you ’re trying to submit\nto someone formally or something , but a lot of them are not important in casual writing ( such as\nsomeone ’s dialogue or something like that ) . For example , “ literally ” has been used for hyperbole\nfor a very long time – I know I ’ve heard that Nathaniel Hawthorne did it , and I hardly think he was\nthe ﬁrst . It pisses a lot of people off but it ’s not like it ’s a new phenomenon and it ’s not like it ’s\na corruption of the language . Things about spelling and homonyms and stuff should probably be\nfollowed just about all the time though . (using concrete examples)\nFigure 4: Evaluation examples for informative factor.\nCoherence evaluates how content is coherent considering both intra- and inter-sentence correlation.\n• 1 point example: i don’t know if it’s a good thing or a bad thing, but i’ve found that if i’m going to\nwrite a story, i need to be able to get it out of my head. i think that’s the most important thing. if i\nwant to write something, i have to know what i want it to be. i have a lot of ideas, but they’re just not\ngood enough. i’ll try to ﬁnd a way to get them out, but if i can’t ﬁnd the right words to write, i will\nprobably have to go back and re - write it. (no semantic connection between sentences)\n• 1 point example: “ I’m sorry, ” I said to myself. “ I don’t know what to do. ” I replied. “ I just want\nto see you again. ” I looked at my reﬂection, and it was the same. I couldn’t tell if it was a reﬂection or\na reﬂection, but I knew that it was my reﬂection. I looked back at the mirror, and I saw that I was the\none who was going to be my future soulmate. I smiled, and said, “ I love you, ” and walked away.\n• 3 point example: I have a very shitty type of precognition. I don’t know what it is, but it’s something\nI have to do. I’m not sure why I do it, but I do. I’ve been doing it for years now, and I haven’t been\nable to ﬁgure out why. It’s not like I have any control over it. I can’t control what I do, or what I say,\nor how I act. I can only control the way I act, how I react, and how I feel. I have no control over my\nactions, and no control of my emotions. I just have to control my emotions, and that’s all I can do.\n(after careful consideration, many logical contradictions were found)\n• 3 point example: I ’d like to take a moment to appreciate Sir Terry Pratchett. I’ve read a lot of his\nwork, and I’m not sure if it’s because of his writing style or because of the way he wrote it. I don’t\nknow if he’s a good writer, or if he is a bad writer, but I do know that he is one of the best writers I\nhave ever read. I think that’s why I love him so much. I also think that he has a great sense of humor,\nand that he doesn’t have a bad sense of humour. (some repeated information, but other content is\nok)\n• 5 point example: You eagerly await your pizza to come because you ordered from this new Italian\nPizza owed by two brother , you remember that one of their names are Mario but you forgot the other\n. The Pizza ﬁnally arrives a bit late from this tall guy dressed in green . You pay him take , take the\npizza but forget to tip . When you start eating you get a bit dizzy so you lay down and fall asleep quite\nquickly . You wake up in a in a place covered in mushrooms with a little man dressed as a mushroom\ntelling you that “ You need to save the princess ” . (smooth connection between context)\n• 5 point example: When 1st purge happened , no one thought people would attack each other . A\ndesperate party know only as Al Queda broke the rules and decided that it would do what no one\nelse would have done . Bomb Manhattan . That single move destroyed not only the Republicans and\nthe Democrats , it also destroyed morale . Hundreds of fully armed fat Politicians ﬂed to the streets ,\nscreaming out jibberish and shooting anyone they see . Millions lay dead as all parties Jump onto their\njets towards Manhattan , preparing to be included in the giant Cesspit of a war know as the Purge .\nWhen the Morning came . There were no victors . Only that the red dawn came and claimed .\nFigure 5: Evaluation examples for coherence factor.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-01-21",
  "updated": "2022-10-03"
}