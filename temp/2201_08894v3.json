{
  "id": "http://arxiv.org/abs/2201.08894v3",
  "title": "Reinforcement Learning for Personalized Drug Discovery and Design for Complex Diseases: A Systems Pharmacology Perspective",
  "authors": [
    "Ryan K. Tan",
    "Yang Liu",
    "Lei Xie"
  ],
  "abstract": "Many multi-genic systemic diseases such as neurological disorders,\ninflammatory diseases, and the majority of cancers do not have effective\ntreatments yet. Reinforcement learning powered systems pharmacology is a\npotentially effective approach to design personalized therapies for untreatable\ncomplex diseases. In this survey, state-of-the-art reinforcement learning\nmethods and their latest applications to drug design are reviewed. The\nchallenges on harnessing reinforcement learning for systems pharmacology and\npersonalized medicine are discussed. Potential solutions to overcome the\nchallenges are proposed. In spite of successful application of advanced\nreinforcement learning techniques to target-based drug discovery, new\nreinforcement learning strategies are needed to address systems\npharmacology-oriented personalized de novo drug design.",
  "text": "Reinforcement Learning for Personalized Drug Discovery and Design for \nComplex Diseases: A Systems Pharmacology Perspective \n \nRyan K. Tan1, Yang Liu1, Lei Xie1,2,3,* \n \n1Department of Computer Science, Hunter College, The City University of New York \n \n2Ph.D. Program in Computer Science, Biology & Biochemistry, The Graduate Center, The City \nUniversity of New York \n \n3Helen and Robert Appel Alzheimerâ€™s Disease Research Institute, Feil Family Brain & Mind \nResearch Institute, Weill Cornell Medicine, Cornell University \n \n*Correspondence should be addressed \n \n \n \n \n \nAbstract \n \nMany multi-genic systemic diseases such as neurological disorders, inflammatory diseases, and \nthe majority of cancers do not have effective treatments yet. Reinforcement learning powered \nsystems pharmacology is a potentially effective approach to design personalized therapies for \nuntreatable complex diseases. In this survey, state-of-the-art reinforcement learning methods and \ntheir latest applications to drug design are reviewed. The challenges on harnessing reinforcement \nlearning for systems pharmacology and personalized medicine are discussed. Potential solutions \nto overcome the challenges are proposed. In spite of successful application of advanced \nreinforcement learning techniques to target-based drug discovery, new reinforcement learning \nstrategies are needed to address systems pharmacology-oriented personalized de novo drug design. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n1. Introduction \n \nDrug discovery and development is a costly and heavily time-consuming process. Despite \nmassive investment of time and money, it is infeasible to explore the whole chemical space of \ndrug-like compounds, which is composed of around 1033 small molecules [1], by using \nconventional technologies. Owing to the improvement of computer power, tremendous progress \nin deep learning (DL) and emergence of quantum computing, computer-aided drug design has the \npotential to dramatically speed up the drug discovery process. With a reasonably reliable and \naccurate computational model, it is possible to synthesize and test a small number of compounds \nprecisely interacting with expected drug target(s) and achieve desirable clinical outcomes. \nRecently, researchers have introduced various methods to generate novel molecules or optimize \nexisting molecules towards designed properties. The mostly used methods include generative \nadversarial neural-networks (GAN) [2], variational autoencoder (VAE) [3], normalizing flow [4, \n5], and reinforcement learning (RL) [6]. The molecules generated by GAN, VAE, and normalizing \nflow are biased to specific data distributions. They generally lack the ability to explore the \nunknown space that has a distribution shift or directly optimize molecules toward specific targets. \nRL, on the other hand, is able to learn or tune a generative model specifically toward the properties \nof interest and enable the model to generate molecules that have different distribution from the \ntraining data. However, RL is usually less efficient compared with other methods. Training a \nmodel with RL from scratch will either cost a long time or lead to a model that is hard to converge. \nThus, recent studies tend to combine pre-training or adversarial training with RL to take the \nadvantage of the exploitation ability of transfer or adversarial learning [7â€“9] and the exploration \npower of RL.  \nExisting efforts in applying RL to drug discovery mainly follow conventional one-drug-one-\ngene target-based paradigm. Although target-based drug discovery is mostly successful in tackling \nmono-genic diseases whose etiologies are driven by a single gene, it suffers from high failure rate \nespecially for multi-genic, multi-factorial, heterogeneous diseases. Moreover, a drug rarely \ninteracts only with its primary target in human bodies. Off-target effects are common, and may \ncontribute to therapeutic effects or side effects [10]. Therefore, systems pharmacology that targets \na gene-gene interaction network instead of a single gene and is tailored to individual patients has \nemerged as a new drug discovery paradigm for complex diseases. However, unlike target-based \ncompound screening that can be easily measured by drug-target binding affinities, advances of \nsystems pharmacology are hindered by the lack of effective read-outs for high-throughput \ncompound screening. Powered by the development of many high-throughput cell-based \nphenotypic detection methods, phenotype-based drug discovery starts to gain an increasing \nattention in recent years due to its ability to identify drug lead compounds in a physiologically \nrelevant condition [11]. Phenotype-based drug discovery is a target agnostic and empirical \napproach to exploit new drugs with no prior knowledge about the drug target or mechanism of \naction in a disease [12]. The use of molecular signatures as phenotype read-outs makes it possible \nto not only establish robust drug-disease associations but also deconvolute drug targets from the \nunbiased phenotype screening. Additionally, phenotype-based drug discovery has the power to \nexploit drugs for rare or poorly understood diseases such as neurological disorders and many types \nof cancers. Recently, several computational methods have been developed for high-throughput \nphenotype compound screening using chemical-induced gene expressions [13] and images [14] as \nread-outs. Tremendous progress in protein structure predictions [15, 16] and development of new \nmethods for exploring dark chemical genomics space [17] significantly enhance our ability to \ndeconvolute genome-wide target profiles for dark proteins that are not readily accessible by \nexperimental methods. Another fundamental challenge in systems pharmacology-oriented \nprecision drug design is to transfer compound activity in cell-line and animal models into \ntherapeutic efficacy in an individual patient. Although human tissue-based organoid and ex vivo \nmodels have been developed for anti-cancer drug testing, they are expensive and often infeasible \nand even unethical in many disease areas. Thus, computational approaches such as transfer \nlearning which can predict new target variables (e.g., tumor growth) of unseen samples (e.g., \npatients) from a model trained by an existing data set with different distributions (e.g., cell line \nscreens) and targets (e.g., IC50 of cell viability) can be an indispensable tool to fill in the \nknowledge gap between in vitro bioassays and in vivo clinical endpoints of drug candidates. These \ncomputational tools pave the way for systems pharmacology-oriented high-throughput compound \nscreening for personalized drug discovery.  \n \nThis review will be organized as follows. We will first give a brief overview of RL, including \ndefinitions of some key concepts, problem setting and formulation of leading methods. Then we \nwill survey the recent developments of applying deep RL to drug discovery. Finally, we will \nhighlight the challenges and opportunities of RL in systems pharmacology and personalized \nmedicine.  \n \n2. Overview of reinforcement learning \n \nIn reinforcement learning, there are usually two main characters -- an agent and an environment. \nThe agent is the key component of RL that makes sequential decisions, and the environment is the \nworld that the agent lives in. A typical RL problem can be considered as training an agent to \ninteract with an environment that follows a Markov Decision Process (MDP) [18].  \nIn each interaction (with the environment), the agent receives the information of the current \nstate ğ‘ \"âˆˆ ğ’® and performs an action ğ‘\"âˆˆğ’œ accordingly, where ğ’® and ğ’œ are state and action spaces.1 \nAfter performing an action ğ‘\", the agent will transition to a new state ğ‘ \"() and receive a reward ğ‘Ÿ\". \nThese are characterized by underlying state transition dynamics P : ğ’® Ã— ğ’œ Â® âˆ†(ğ’®) and the reward \nfunction ğ‘Ÿ : ğ’® Ã— ğ’œ  Â® â„, i.e., P(ğ‘ \"()| ğ‘ \", ğ‘\") and ğ‘Ÿ(ğ‘ \", ğ‘\") are the probability and reward of taking \naction ğ‘\" in state ğ‘ \" and then transitioning into state ğ‘ \"(). This process repeats indefinitely or until \na predefined termination condition is met. The sequence of states and actions followed in this \nprocess constitutes a so-called trajectory ğœ, e.g., at time ğ‘¡, ğœ\" = {ğ‘ ), ğ‘), ğ‘ 5, ğ‘5, â€¦, ğ‘ \", ğ‘\"}. \nMoreover, with a discount factor ğ›¾ âˆˆ (0, 1], we can define the discounted cumulative reward under \na trajectory ğœ as ğ‘…(ğœ) = âˆ‘\nğ›¾\"<)ğ‘Ÿ( ğ‘ \", ğ‘\")\n=\n\">)\n. An MDP â„³ can be represented as a tuple of all \ncomponents mentioned above along with an initial state distribution ğœ‡, i.e., â„³= {ğœ‡, ğ’®, ğ’œ, P, ğ‘Ÿ, \nğ›¾}.  \nIn the typical setting of MDP, agent behaves by following a (stationary) policy ğœ‹, which \nspecifies a decision-making strategy in which the agent chooses an action ğ‘\" adaptively only based \non its current state ğ‘ \" . Precisely, a stochastic policy is specified as ğœ‹ : ğ’® Â® âˆ†(ğ’œ) while a \ndeterministic policy is of the form ğœ‹ : ğ’® Â® ğ’œ.  \nGiven an MDP â„³ and a policy ğœ‹, we can define some functions that measures the quality of \nbeing in a state ğ‘  or taking an action ğ‘ upon ğ‘  in the long run. Specifically, we can define the value \n                                                \n1 We use ğ‘¡ to track the time-step, e.g., ğ‘ \" is the state at time t. \nfunction ğ‘‰â„³\nğœ‹ : ğ’® Â® â„ that gives discounted sum of future rewards at a state ğ‘  following an \narbitrary policy ğœ‹:  \n \nğ‘‰â„³\nğœ‹(ğ‘ ) = ğ”¼ğœ~ğ›µğœ‹(ğœ)Fâˆ‘\nğ›¾ğ‘¡âˆ’1ğ‘Ÿ(ğ‘ ğ‘¡, ğ‘ğ‘¡) |\nâˆ\nğ‘¡=1\n ğœ‹,ğ‘ 1 = ğ‘ J  \n \n \n \n \n(1) \nwhere ğ‘‡P(ğœ) = âˆ\nğœ‹(ğ‘\"|ğ‘ \")ğ‘ƒ(ğ‘ \"()|ğ‘ \", ğ‘\")\n=\n\">)\n \n \nSimilarly, the action-value function (or Q-function) ğ‘„â„³\nğœ‹: ğ’® Ã— ğ’œÂ® â„ can be defined as: \n \nğ‘„â„³\nğœ‹(ğ‘ , ğ‘) = ğ”¼ğœ~ğ›µğœ‹(ğœ)Fâˆ‘\nğ›¾ğ‘¡âˆ’1ğ‘Ÿ(ğ‘ ğ‘¡, ğ‘ğ‘¡) |\nâˆ\nğ‘¡=1\n ğœ‹, ğ‘ 1 = ğ‘ , ğ‘1 = ğ‘J  \n \n \n \n(2) \n \nWith ğ‘‰â„³\nğœ‹(ğ‘ ) and ğ‘„â„³\nğœ‹(ğ‘ ,ğ‘), we can also define another value function, the advantage function \nğ´â„³\nğœ‹(ğ‘ , ğ‘) = ğ‘„â„³\nğœ‹(ğ‘ , ğ‘) âˆ’ğ‘‰â„³\nğœ‹(ğ‘ ), which measures the relative reward that the agent could obtain by \ntaking a particular step ğ‘ upon ğ‘  (compared to an average action).  \nThe objective of RL is to learn a policy ğœ‹ that optimizes the expectation of accumulated reward \nunder a particular initial state distribution ğœ‡: \n \nmaxPâˆˆÎ  JZ(ğœ‹) = maxPâˆˆÎ  ğ”¼[\\~Z[ğ‘‰â„³\nP (ğ‘ ))]  \n \n \n \n \n \n(3)2 \n                               =  maxğœ‹âˆˆ_ ğ”¼ğœ~ğ›µğœ‹(ğœ)[ğ‘…(ğœ)]  \n \n \n \n \n \n(4) \nwhere ğ‘‡P(ğœ) = ğœ‡(ğ‘ )) âˆ\nğœ‹(ğ‘\"|ğ‘ \")ğ‘ƒ(ğ‘ \"()|ğ‘ \", ğ‘\")\n=\n\">)\n \n \nWith the basics of RL terminology and notation, we will review several leading RL algorithms \nin this section, with the focus on the mathematical formulation and foundational design. We will \nstart with the model-free RL including value-based methods, policy gradient and actor-critic, and \nthen briefly introduce model-based RL. A non-exhaustive taxonomy of RL algorithms can be \nfound in Figure 1.  \n \n \nFigure 1. A taxonomy of RL algorithms \n                                                \n2 Please note in the following subsections, we will drop the subscripts of â„³ and ğœ‡ from value functions and the \nobjective function JZ(ğœ‹) respectively given working under the same MDP â„³. \n \n \nValue-based Methods \nA common way to optimize the RL objective J(ğœ‹) is by the observation that if the optimal \nvalue function or Q-function could be accurately estimated, we can easily recover an optimal \npolicy. For instance, given the optimal Q-function ğ‘„âˆ—(ğ‘ , ğ‘), an optimal policy ğœ‹âˆ—(ğ‘ ) could be \nobtained by \nğœ‹âˆ—(ğ‘ ) = argmaxbâˆˆğ’œğ‘„âˆ—(ğ‘ , ğ‘)  \n \n \n \n \n \n \n \n(5) \nDynamic Programming (DP) is a classic approach to approximate these desired value \nfunctions assuming a perfect model of the environment is given, and the number of states and \nactions is small so that value functions can be represented in some lookup tables [18]. The \nfoundation of DP is centered on bellman optimality equation and bellman expectation equation, \nand they can be defined as follows with respect to Q-function:  \nğ‘„âˆ—(ğ‘ , ğ‘) = ğ‘Ÿ(ğ‘ , ğ‘) + ğ›¾ğ”¼[d~e[maxbf ğ‘„âˆ—(ğ‘ d, ğ‘d)]  \n \n \n \n \n(6) \nğ‘„P(ğ‘ , ğ‘) =  ğ‘Ÿ(ğ‘ , ğ‘) +  ğ›¾ğ”¼bd~P,[d~e[ğ‘„P(ğ‘ d, ğ‘d)]   \n \n \n \n \n(7) \n       Where ğ‘ d is the successor state and ğ‘d is the action performed at ğ‘ d. \nAccording to these equations, two helpful mathematical operations, bellman optimality \noperator ğ’¯: â„|ğ’®||ğ’œ| Â® â„|ğ’®||ğ’œ|  and bellman evaluation operator ğ’¯P âˆ¶ â„|ğ’®||ğ’œ| Â® â„|ğ’®||ğ’œ|  can be \nconstructed as follows: \n(ğ’¯ğ‘„)(ğ‘ , ğ‘) âˆ¶=  ğ‘Ÿ(ğ‘ , ğ‘) + ğ›¾ğ”¼[d~e[maxğ‘â€²ğ‘„(ğ‘ â€², ğ‘â€²)]   \n \n \n \n \n(8) \n(ğ’¯ğœ‹ğ‘„)(ğ‘ , ğ‘) âˆ¶=  ğ‘Ÿ(ğ‘ , ğ‘) +  ğ›¾ğ”¼bd~P,[d~e[ğ‘„(ğ‘ â€², ğ‘â€²)]   \n \n \n \n \n(9) \n \nThese operators map any Q-function from â„|ğ’®||ğ’œ| to another Q-function in the same space. Itâ€™s \nhelpful to consider the Q-function as a vector of length |ğ’®||ğ’œ| and the operators are just some \ntransformations that take the vector and output another vector with the same dimensions. More \nspecifically, considering ğ’¯ and a state-action value ğ‘„(ğ‘ , ğ‘), ğ’¯ can be viewed as an assignment \nstatement that updates the original value of ğ‘„(ğ‘ , ğ‘) with the one derived from the RHS of (8). \nThere are two pleasant properties of these operators, related to contraction mapping, which help \nthe design of the classic DP algorithms. First, given any arbitrary Q-function ğ‘„, repeatedly \napplying ğ’¯P or ğ’¯ on ğ‘„ yields ğ‘„P  or ğ‘„âˆ— respectively. This property can directly turn Bellman \noperators into update rules, providing an iterative algorithm for approximating a desired Q-\nfunction. Moreover, ğ’¯P and ğ’¯ have unique fixed points ğ‘„P  and ğ‘„âˆ— such that ğ’¯Pğ‘„ğœ‹= ğ‘„ğœ‹ and \nğ’¯ğ‘„âˆ—= ğ‘„*. This second property serves as the termination condition of many classic DP algorithms \nand the building blocks of some advanced RL algorithms, such as actor-critic.  \nSince, in practice, the complete knowledge of the environment is usually unknown, and the \nnumber of states and actions can be arbitrarily large, dynamic programming is thus limited to some \nrestricted problems by its assumptions and function representation. The value-based RL, \nsometimes known as approximate dynamic programming, provides a class of algorithms that \novercome these problems, extending the framework of iterative dynamic programming with \nmodified bellman operators and function approximation. The essence of this approach is to modify \nthe update rule by approximating the bellman operators with some empirical estimators, i.e., \nestimating the RHS of equations (8) and (9) with sampling [19]. In the simplest case of using a \nsingle sample < ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€² > where ğ‘Ÿ= ğ‘Ÿ(ğ‘ , ğ‘), the empirical bellman operators ğ’¯jğ‘„ and ğ’¯P\nk ğ‘„ can \nbe constructed as [20â€“22]: \n \n(ğ’¯jğ‘„)(ğ‘ , ğ‘) âˆ¶=  ğ‘Ÿ(ğ‘ , ğ‘) + ğ›¾maxbfğ‘„(ğ‘ â€², ğ‘â€²)  \n \n \n \n \n \n(10)3 \n(ğ’¯P\nk ğ‘„)(ğ‘ , ğ‘) âˆ¶= ğ‘Ÿ(ğ‘ , ğ‘) + ğ›¾ğ‘„(ğ‘ â€², ğœ‹(ğ‘ â€²))  \n \n \n \n \n \n(11) \n \nA classic algorithm under this category is the Fitted Q-Iteration (FQI) [23]. In FQI, the \nalgorithm first gathers a dataset ğ·= {< ğ‘ n,  ğ‘n,  ğ‘Ÿn,  ğ‘ n() >}n\n|p|where state ğ‘ n  and action  ğ‘n  are \ndrawn from a pre-defined distribution ğœˆ, and the next state ğ‘ n() and the reward ğ‘Ÿn, are obtained \nfrom an unknown state transition probability function ğ‘ƒ(â‹…|ğ‘ n,  ğ‘n) and reward function ğ‘Ÿ(ğ‘ n ,  ğ‘n) \nof the environment. With the initialization of a parameterized Q-function ğ‘„s , the Q-values \nğ‘„s(ğ‘ n,  ğ‘n) are updated towards their target ğ’¯jğ‘„s(ğ‘ n,  ğ‘n) and the update rule can be formulated as \n \nğœƒâ† minimizes âˆ‘\nyğ’¯jğ‘„s(ğ‘ n, ğ‘n) âˆ’ğ‘„s(ğ‘ n, ğ‘n)z\n5\n|p|\nn>)\n  \n \n \n \n \n(12) \nWhere ğ’¯jğ‘„s(ğ‘ n, ğ‘n) = ğ‘Ÿn + ğ›¾maxb{|\\ğ‘„s(ğ‘ n(), ğ‘n()) \n \nEquation (12) can be interpreted as applying Monte-Carlo approximation of Bellman \noptimality ğ’¯j to Q-function ğ‘„s through minimizing the square loss over ğœƒ. And since the Q-values \nin FQI are estimated by a parameterized function rather than a lookup table (as in DP), they are \nclosely correlated to each other, implying that a small update of ğœƒ may benefit some Q-values but \npush others away from their targets. Therefore, unlike supervised learning where the ground truth \nlabels remain unchanged, the targets ğ’¯jğ‘„ defined in FQI may vary every time when the parameter \nchanges, which introduces instability in training. Consequently, depending on the generalization \nand extrapolation ability of the function approximation, the contraction mapping property of DP \ncannot guarantee convergence under parametrized Q-functions, especially for those using neural \nnetworks [24]. However, there exist other variants of FQI using non-parametric approximation \narchitecture, such as k-nearest-neighbor and totally randomized trees, showing strong convergence \nproperty [25â€“28].   \nRegarding the value-based methods using neural networks, Deep Q-Network algorithm, \nshowing strong performance in a variety of ATARI games, can be viewed as an instantiation of \nFQI in online settings [29]. The Q-function approximator of DQN can be any typical neural \nnetwork, referred to as Q-network. The framework of DQN uses two heuristics to limit the \ninstability inherited from FQI with neural networks. First, a separated network called target \nnetwork is introduced solely for computing the targets (interpreted as the ground truth) due to their \ninconsistency. Compared to Q-network, the target network is updated less frequently to keep the \ntarget fixed for an extra amount of time. With this strategy, DQN prevents the instability from \npropagating too quickly and thus reduces the risk of divergence.  \nMoreover, Deep Q-Network is an online learning algorithm which follows its current policy \nfor exploring and data sampling. Without proper care, this may deliver a policy that has inferior \n                                                \n3 It is worth noting that the empirical bellman operator constructed in (10) is an analogue to Q-learning [30] with the \nlearning rate ğ›¼= 1. Theoretically, if we want to turn (10) directly into an update rule, a large number of successor \nstates {ğ‘ n\nd}n\n~ are required for convergence, i.e., yğ’¯jğ‘„z(ğ‘ , ğ‘) âˆ¶= ğ‘Ÿ(ğ‘ , ğ‘) + ğ›¾âˆ‘maxbfğ‘„(ğ‘ n\nd, ğ‘â€²)\n~\nn\n [21]. Otherwise, a new \nalgorithm with a customized update rule needs to be proposed [22]. \nperformance since previously visited state-action pairs with high rewards may not guarantee to be \nrevisited due to the stochasticity of the system. Experience Replay, a replay buffer, is introduced \nto solve this issue by keeping all samples < ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€² > from the last N steps in the memory where \nN is usually very large. Besides, ER selects samples from the memory following the uniform \ndistribution for every mini-batch update. This sampling method, though simple, effectively breaks \nthe temporal correlation between samples in the same trajectory, which helps reduce bias. With \nlarge memory and uniform sampling, mini-batch samples constructed under ER are more \nrepresentative than those from alternative methods such as Q-learning [30] which uses only one \nsample for an update. Additionally, a large buffer provides good coverage of the state-action space, \nwhich makes the policy training more exploratory and consistent, thus increasing the chance of \nachieving a more desirable policy.  \nIn addition to target network and experience replay, there are other advanced approaches that \ncould help improve the stability and efficiency of policy learning including Double DQN [31] for \noverestimation reduction, Multi-step learning for accurate target estimation [24], Dueling \nNetworks [32] for enhanced Q-function representation and Prioritized Experience Replay (PER) \nfor efficient TD-error minimization [33], along with other extensions such as Distributional DQN \n[34], Quantile Regression DQN (QR-DQN) [35], and Fully parameterized Quantile Function (FQF) \n[36].  \n \nPolicy gradient \nUnlike the value-based approach that learns a desired policy by estimating the optimal value \nfunctions, the Policy Gradient (PG) methods work directly on the objective function defined in (4) \nor some equivalent ones. In the setting of this review, we limit our focus on (4). Specifically, with \na policy Ï€ parameterized by Î¸ âˆˆ Î˜ ÃŒ â„d, the goal of PG is to find a Î¸ that maximizes J(ğœ‹s): \nmaxsâˆˆâ€¢J(ğœ‹s)    \n \n \n \n \n \n \n \n \n(13) \nSince the search space has now shifted from the policy space Î  in (4) to Î˜ which is continuous \nand has fixed dimensions, various numerical optimization methods could be utilized to solve (13). \nGradient ascent is one direct approach to this, which iteratively moves in the direction of steepest \nascent as defined by the gradient for maximizing J(ğœ‹s). Following is the expression of the gradient \nfor J(ğœ‹s): \nâˆ‡sJ(ğœ‹s) =  âˆ‡sğ”¼â€¢~â€šÆ’â€(â€¦)[ğ‘…(ğœ)]  \n                 = ğ”¼â€¢~â€šÆ’â€(â€¢)[âˆ‘\nâˆ‡s\n=\n\">)\nlogğœ‹s(ğ‘\"|ğ‘ \")ğ‘…(ğœ)]  \n \n \n \n \n(14) \nWhere ğ‘…(ğœ) = âˆ‘\nğ›¾\"<)ğ‘Ÿ( ğ‘ \", ğ‘\")\n=\n\">)\n. \nAt each update iteration ğ‘˜, gradient ascent, with a fixed step size a, follows the update rule: \nğœƒâ€°() = ğœƒâ€° + ğ›¼âˆ‡sÅ J(ğœ‹sÅ )  \n \n \n \n \n \n \n \n(15) \nSince the policy gradient âˆ‡sJ(ğœ‹s) is an expectation over all possible trajectories, it can be \ndirectly estimated by a set of samples where each is a trajectory collected by the agent interacting \nwith the environment while following policy ğœ‹s. With the set of samples denoted as ğ· and the \nlength of trajectories assumed to be ğ», we can estimate âˆ‡sJ(ğœƒ) as: \n \nâˆ‡sJ(ğœƒ) â‰ˆ\n)\n|p| âˆ‘\nâˆ‡s logğ‘‡Ps(ğœ)[ğ‘…(ğœ)]\nâ€¢âˆˆp\n   \n \n \n \n \n \n(16) \n              â‰ˆ\n)\n|p| âˆ‘\nâˆ‘\nâˆ‡s logğœ‹syğ‘n,\"â€¢ğ‘ n,\"z ğ‘…(ğœ)\nÅ½\n\">)\n|p|\nn>)\n   \n \n \n \n \n(17) \nWhere ğ‘…(ğœ) = âˆ‘\nğ›¾\"<)ğ‘Ÿ(ğ‘ \", ğ‘\")\nÅ½\n\">)\n. \n \nWith equation (17), we have derived our first policy gradient method, commonly known as \nREINFORCE [37] which serves as the foundation of PG-based methods.  \n \nBecause âˆ‡sJ(ğœ‹s) is an expectation, a long-standing issue centered on PG-based methods is to \nderive an unbiased estimator of âˆ‡sJ(ğœ‹s) with possibly low variance. For instance, ğ‘…(ğœ) in (17) \ncan be replaced by a smaller term âˆ‘\nğ›¾\"f<\"ğ‘Ÿ(ğ‘ \"f, ğ‘\"f)\nÅ½\n\"f>\"\n thanks to causality that the policy at time \nğ‘¡d cannot affect the reward at time ğ‘¡ when ğ‘¡ < ğ‘¡d. This gives us a new unbiased estimator of \nâˆ‡sJ(ğœ‹s) with reduced variance called Reward-to-go [38]: \n \nâˆ‡sJ(ğœƒ) â‰ˆ\n)\n|p| âˆ‘\nâˆ‘\nğ›¾\"<)âˆ‡s logğœ‹syğ‘n,\"â€¢ğ‘ n,\"z yâˆ‘\nğ›¾\"f<\"ğ‘Ÿ(ğ‘ n,\"f, ğ‘n,\"f)\nÅ½\n\"f>\"\nz\nÅ½\n\">)\n|p|\nn>)\n  \n \n(18) \nRegarding the policy gradient âˆ‡sJ(ğœ‹s), it can be viewed as an analogue of the gradient in \nmaximum likelihood where the term logğœ‹syğ‘n,\"â€¢ğ‘ n,\"z  in equations (17) and (18) is the log \nlikelihood of the policy ğœ‹s given data <ğ‘ n,\", ğ‘n,\">. Unlike maximum likelihood, the log likelihood \nin PG is now weighted by some discounted cumulative rewards, such as ğ‘…(ğœ)  or \nâˆ‘\nğ›¾\"f<\"ğ‘Ÿ(ğ‘ n,\"f, ğ‘n,\"f)\nÅ½\n\"f>\"\n. Thus, updating the parameter ğœƒ with âˆ‡sJ(ğœ‹s) would change the policy \naccording to the sign of ğ‘…(ğœ) or âˆ‘\nğ›¾\"f<\"ğ‘Ÿ(ğ‘ n,\"f, ğ‘n,\"f)\nÅ½\n\"f>\"\n, e.g., the chance of selecting ğ‘n,\" given ğ‘ n,\" \nincreases if ğ‘…(ğœ) is positive or decreases otherwise.  \nOn the other hand, if the reward function only outputs positive values and some mediocre \nactions are randomly selected for parameter updates, the agent may tend to choose these inferior \nactions even their absolute values of logğœ‹syğ‘n,\"â€¢ğ‘ n,\"z are small. A popular strategy to address this \nunstable issue is to introduce an extra term, called baseline b, which can be shown to always keep \nthe estimator of âˆ‡sJ(Î¸) unbiased while reducing its variance if selected properly [39]. For instance, \nby choosing an average reward over sampled trajectories as baseline, the term \nyâˆ‘\nğ›¾\"f<\"ğ‘Ÿ(ğ‘ n,\"f, ğ‘n,\"f)\nÅ½\n\"f>\"\nz âˆ’ğ‘ in (19) is centered around 0, which improves the sampling \nefficiency by distinguishing actions with their relative rewards. \nâˆ‡sJ(Î¸) â‰ˆ\n)\n|p| âˆ‘\nâˆ‘\nğ›¾\"<)âˆ‡s logğœ‹syğ‘n,\"â€¢ğ‘ n,\"z\nÅ½\n\">)\n[\n|p|\nn>)\nyâˆ‘\nğ›¾\"f<\"ğ‘Ÿ(ğ‘ n,\"f, ğ‘n,\"f)\nÅ½\n\"f>\"\nz - b] \n(19) \nWhere b can be \n)\n|p| âˆ‘\nğ‘…(ğœ)\n|p|\nn>)\n \nThus, with proper care of designing the baseline, the policy ğœ‹s could eliminate the inferior \nactions and choose more desirable ones with higher probability. In the next subsection, a learnable \nbaseline will be introduced, which is more stable and significantly reduces the variance of the \nclassic policy gradient estimators.  \nActor-Critic \nActor-critic algorithms can be viewed as an approach that combines policy gradients with \nvalue-based methods to optimize J(Î¸). Generally, it takes a policy ğœ‹s as an actor to interact with \nthe environment, while maintaining a learnable value function as the critic to evaluate the actorâ€™s  \nactions [24, 40]. The simplest form of actor-critic called Q actor-critic [40] can be derived directly \nfrom the reward-to-go and we denote âˆ‡sJ(Î¸)\nâ€˜  below as the estimator of âˆ‡sJ(Î¸) from (18): \nğ”¼Fâˆ‡sJ(Î¸)\nâ€˜ J = ğ”¼â€™\n)\n|p| âˆ‘\nâˆ‘\nğ›¾\"<)ğ‘„Pâ€\nk (ğ‘ \", ğ‘\")âˆ‡s logğœ‹syğ‘n,\"â€¢ğ‘ n,\"z\nÅ½\n\">)\n|p|\nn>)\nâ€œ  \n                    = ğ”¼â€™\n)\n|p| âˆ‘\nâˆ‘\nğ›¾\"<)ğ”¼Fğ‘„Pâ€\nk (ğ‘ \", ğ‘\")â€¢ğ‘ \", ğ‘\"Jâˆ‡s logğœ‹syğ‘n,\"â€¢ğ‘ n,\"z\nÅ½\n\">)\n|p|\nn>)\nâ€œ  \n                    = ğ”¼â€™ )\n|p| âˆ‘\nâˆ‘\nğ›¾\"<)ğ‘„Pâ€(ğ‘ \", ğ‘\")âˆ‡s logğœ‹syğ‘n,\"â€¢ğ‘ n,\"z\nÅ½\n\">)\n|p|\nn>)\nâ€œ  \n \n \n(20) \nWhere ğ‘„Pâ€\nk (ğ‘ \", ğ‘\") = âˆ‘\nğ›¾\"f<\"ğ‘Ÿ(ğ‘ n,\"f, ğ‘n,\"f)\nÅ½\n\"f>\"\n \nA direct advantage of this new estimator, the one inside the expectation of (20), is that it has \nless variance than the reward-to-go. This can be shown following the tower property of conditional \nprobability and the definition of variance. The intuition is that ğ‘„Pâ€\nk (ğ‘ \", ğ‘\") is indeed the cumulative \nreward of a single sample trajectory that estimates ğ‘„Pâ€(ğ‘ \", ğ‘\"), which inevitably has larger \nvariance and so does the reward-to-go. In practice, we usually estimate ğ‘„Pâ€ by a parameterized Q-\nfunction ğ‘„â€\nPâ€and update ğœ” following a similar procedure as Fitted Q-Iteration described in the \nprevious subsection of value-based methods. \nSimilar to (19), a baseline can be incorporated into Q actor-critic to further reduce its variance. \nA favorable choice is using a value function ğ‘‰â€“\nPâ€  parameterized by ğœ‘ where âˆ‡sJ(ğœ‹s) can be \nfurther reduced to a form of advantage function: \nâˆ‡sJ(Î¸) â‰ˆ\n)\n|p| âˆ‘\nâˆ‘\nğ›¾\"<)âˆ‡s logğœ‹syğ‘n,\"â€¢ğ‘ n,\"z\nÅ½\n\">)\nğ´â€,â€“\nPâ€ yğ‘ n,\", ğ‘n,\"z\n|p|\nn>)\n   \n \n \n(21) \nWhere ğ´â€,â€“\nPâ€ yğ‘ n,\", ğ‘n,\"z = ğ‘„â€\nPâ€yğ‘ n,\", ğ‘n,\"z âˆ’ğ‘‰â€“\nPâ€yğ‘ n,\"z \nThe derived PG estimator in (21) is the naÃ¯ve advantage actor-critic [24, 41, 42]. As the \nadvantage function measures the performance difference between a specific action and the average \naction in a given state, itâ€™s usually considered as a favorable critic to evaluate the actor and \ndetermine which action should be chosen more often.  \nMoreover, âˆ‡sJ(ğœ‹s) can be approximated in another way as specified in (22) by the observation \nthat given a sample < ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€² >, the sum of ğ‘Ÿ and ğ›¾ğ‘‰P(ğ‘ â€²) is a sample estimate of ğ‘„P (ğ‘ , ğ‘): \nâˆ‡sJ(Î¸) â‰ˆ\n)\n|p| âˆ‘\nâˆ‘\nğ›¾\"<)âˆ‡s logğœ‹syğ‘n,\"â€¢ğ‘ n,\"zğ´â€“\nPâ€yğ‘ n,\", ğ‘n,\"z\nÅ½\n\">)\n|p|\nn>)\n \n \n \n  \n(22) \nWhere ğ´â€“\nPâ€yğ‘ n,\", ğ‘n,\"z = ğ‘Ÿyğ‘ n,\", ğ‘n,\"z + ğ›¾ğ‘‰â€“\nPâ€yğ‘ n,\"()z âˆ’ğ‘‰â€“\nPâ€yğ‘ n,\"z \nThe unbiased estimator of âˆ‡sJ(ğœ‹s) derived in (22) is called TD-error actor-critic [41, 43, 44]. \nTheoretically, it has a higher variance than the naÃ¯ve advantage actor-critic in (21) as we replace \nğ‘„â€\nPyğ‘ n,\", ğ‘n,\"z  with a single sample estimate ğ‘Ÿyğ‘ n,\", ğ‘n,\"z + ğ›¾ğ‘‰â€“\nPâ€yğ‘ n,\"()z . But in practice the \nestimator of âˆ‡sJ(ğœ‹s) in (22) is more stable for policy training and has less bias as it only requires \na function estimator for the value function ğ‘‰â€“\nPâ€ while the naÃ¯ve advantage actor-critic requires an \nextra function estimator ğ‘„â€\nPâ€ for Q-function other than ğ‘‰â€“\nPâ€, which thus needs more samples to \nachieve a comparable performance. Owing to the favorable property of TD-error actor-critic, \nprecisely the term of ğ´â€“\nPâ€yğ‘ n,\", ğ‘n,\"z, it has been widely adapted to many advanced actor-critic \nalgorithms such as Asynchronous Advantage Actor Critic (A3C) [42], Trust Region Policy \nOptimization (TRPO) [45] and Proximal Policy Optimization (PPO) [46].  \nThe methods discussed so far in this subsection, as well as the ones mentioned in PG, are \ngenerally on-policy methods where the target policy being learned (i.e., evaluated and improved) \nis used as the one to interact with the environment and generate samples, which is commonly \nreferred to as behavior policy [24]. On the other hand, in off-policy setting, agent usually learns a \npolicy that is different from the one being executing. Some typical examples are the methods \nintroduced in the value-based RL, such as FQI, Q-learning and DQN, where the samples used for \ntraining are not exactly generated by the current learning policy, the target policy. There is a class \nof off-policy actor-critic methods, such as Deep Deterministic Policy Gradient (DDPG) [47], Twin \nDelayed DDPG (TD3) [48] and Soft Actor-Critic (SAC) [49], that adopt the replay buffer and \nheavily depend on Q-function estimator for approximating desired policy. These methods, in \npractice, are usually more sample efficient yet less robust to hyperparameter settings than their on-\npolicy counterparts mainly due to the incorporation of off-policy samples and sometimes \nconvoluted tricks for stabilizing training.  \nModel-based RL \n \nModel-based RL is a general term that refers to a broad class of algorithms, which is widely \nseen as a potential approach to improve the sample efficiency of model-free RL [50â€“52]. Unlike \nmodel-free RL which learns a policy ğœ‹s  and/or a value function, without modeling the \nenvironment, model-based RL explicitly estimates the transition dynamics ğ‘ƒ(ğ‘ \"()|ğ‘ \", ğ‘\"), which \nwe denote ğ‘ƒËœ(ğ‘ \"()|ğ‘ \", ğ‘\", ) parameterized by ğœ“, and uses it for planning [53] or policy learning. \nSince model-based RL has not been well standardized and there exist many variants, we will \nbriefly summarize three important types of algorithms. A common class of model-based RL \nalgorithms, related to shooting methods [54], learns the dynamics model ğ‘ƒËœ, and uses it to derive \na local planning strategy for action selection. This is typically achieved by formulating and solving \nreceding horizon problems posed in model-predictive control [55] using various trajectory \noptimization methods [56, 57]. Other model-based RL methods, such as PILCO, learn a policy ğœ‹s \nin addition to the dynamics model ğ‘ƒËœ, and employ backpropagation through time with respect to \nthe expected future reward for policy search and improvement [58â€“60]. Dyna-style algorithms, \nanother set of model-based methods, incorporate the dynamic model ğ‘ƒËœ to the general model-free \nframework for augmenting samples and accelerating policy learning [61â€“64]. Specifically, training \nunder this category usually iterates between two steps: first, the agent learns ğ‘ƒËœ and ğœ‹s with a set \nof real experiences {< ğ‘ n,  ğ‘n,  ğ‘Ÿn,  ğ‘ n() >}n\n~  gathered from interacting with the environment. \nSecond, a number of â€˜syntheticâ€™ samples {< ğ‘ Ì‚n,  ğ‘â€ºn,  ğ‘ŸÌ‚n,  ğ‘ Ì‚n() >}n\nÅ“ are generated using the current \npolicy ğœ‹s under the learned dynamic model ğ‘ƒËœ and another round of policy update is performed \nwith these â€˜syntheticâ€™ samples. Such an iterative process can significantly boost the sample \nefficiency of pure model-free learning once ğ‘ƒËœ precisely estimates the environmental dynamics.  \n \n3. State-of-the-arts in applying reinforcement learning to drug discovery \n \n \n \nFigure 2. RL-based methods for drug design  \n \n      There are different approaches in applying reinforcement learning to drug discovery depending \non the objectives [65â€“67]. Distributional learning, a common task of computational drug design, \nis to generate a set of molecules distributed differently from an existing dataset, which satisfy one \nor more predefined requirements. One typical approach is to pretrain a generative model with some \nwell-established dataset and incorporate it into a RL framework for optimization. In this case, the \ngenerative model is trained as learning a policy that maximizes a RL loss function as defined in \n(3), where the reward is customized for the objective(s). Another set of problems, usually referred \nto as goal-directed learning, requires searching for the exact molecule with specific properties. \nThese tasks are usually formulated as solving a combinatorial optimization problem [68, 69], \nwhich falls exactly into the region that could be potentially solved by RL [70, 71]. In this section, \nwe will briefly review different adaptations of RL algorithms, as defined in section 2, to drug \ndiscovery, with a focus on the practical strategy and design of RL-based generative models. A \nnon-exhaustive taxonomy of RL-based methods for drug design can be found in Figure 2. \n \nModels with policy gradient \n \nPolicy gradient method has been adapted to a variety of RL-based generative models for \ndistributional learning owing to its policy stochasticity and learning capability for high-\ndimensional state and/or action spaces. Marcus et al. developed an approach REINVENT for \ndistributional learning based on REINFORCE algorithm, in which they first pre-trained a recurrent \nneural network (RNN) known as prior policy that could generate a set of samples with a similar \nstructural distribution as ChEMBL [72]. Then the agent RNN for learning target policy was \ninitialized with the same architecture and parameters as the pre-trained RNN and later tuned using \nRL to achieve higher expected score while keeping the new policy close to the prior policy. The \ngenerated molecules thus have a similar structural distribution as the ChEMBL dataset while being \noptimized towards the target properties. Since the agent RNN is pre-trained, the RL searching \nefficiency has been increased whereas the exploration capability of this model is somewhat limited.  \nRecently, the attention to polypharmacology is constantly increasing owing to its therapeutic \npotential in some complex pathologies. Dual-target ligand generative network (DLGN), leveraging \nRL and adversarial training, was developed to generate molecules that have bioactivities toward \ntwo targets [73]. With SMILES string as input, DLGN uses an RNN-based generator to produce \nnovel molecules that satisfy the predefined constraints. To make generated molecules dual-\ntargeted, DLGN utilizes two discriminators to monitor the generative process and encourages the \ngenerated molecules lying in the intersection of the two bioactive-compound distributions. The \ndrawback of this model is that, although the generator does not need to be trained with labeled \ndata, the discriminators require reliable labeled data to control the qualities of generated molecules. \nAs graph is a more natural way to represent a molecule, graph neural networks (GNNs) are \nwidely used in computational drug discovery. You et al. developed a graph convolutional policy \nnetwork (GCPN) that generates molecules under some guidance towards desired objectives while \nrestricting the output molecules to some specific chemical rules [8]. To achieve this goal, they \nleveraged RL for molecule generation and optimization under a simulated environment with a \ndesigned reward function, and used expert pre-training and adversarial training to incorporate prior \nknowledge for guidance. In another work, Sara et al. extended REINVENT with gated graph neural \nnetworks to generate molecules with desired properties [74]. To overcome the difficulty of RL \ntraining, they introduced a best agent reminder (BAR) loss, which is calculated based on the \nactions given by the current agent and the best agent, and shown to significantly improve the speed \nof convergence and the final performance. MolecularRNN is another interesting work based on \nthe REINFORCE algorithm that utilizes graph data structure to represent molecules while \nexploiting RNN as the generator. It employs a dual network model, consisting of both node-level \nand edge-level RNNs, to predict the next atom and bond given an intermediate generated molecular \ngraph [75]. With valency-based rejection sampling constraints, MolecularRNN achieves 100% \nvalidity of the generated molecules in the experiments.  \nTo balance the exploration and exploitation, Xuhan et al. employed two pretrained RNNs in \nDrugEx to generate novel molecules [76]. The two RNNs share the same architecture while having \ndifferent internal parameters. One of the RNN, which serves as the exploration network, is \npretrained on the ZINC database and then fine-tuned with desired molecules to memorize the \ndistribution of the potential drug space. This exploration network will not be updated during \ntraining. The other RNN, performing as exploitation network, is only pretrained on ZINC database, \nfollowed by policy gradient update to generate molecules with high pIC50 values for properties of \ninterests. During the training phase, the exploration and the exploitation networks are randomly \nassigned to generate the next token based on a predefined â€˜exploring rateâ€™. This helps the generated \nmolecules to be well diverse while maintaining high pIC50 value for target proteins. To improve \nthe balance of exploration and exploitation, Tiago et al. extended DrugEx by introducing a \ndynamically adaptive â€˜exploring rateâ€™ which is determined by the property of two latest batches of \ngenerated molecules [77]. Additionally, they added a penalty in reward for improving novelty \nwhen the diversity of the latest generated molecules decreases. \n \nModels with Deep Q-networks (DQNs) \n \nValue-based RL methods are usually more stable and sample efficient than those using \npolicy gradient [24]. In addition, as the policy learned by value-based approach is deterministic, \nitâ€™s more natural to choose these methods for solving the goal-directed problems. Molecule Deep \nQ-Network (MolDQN) based on bootstrapped-DQN is a leading method for goal-directed \nlearning [78]. By allowing only valid actions, MolDQN guarantees that the generated molecules \nare 100 % valid. The molecule generation or optimization starts from an empty or a seed \nmolecule in the form of Morgan fingerprint. In the constrained optimization task, MolDQN \noptimizes the seed molecule toward the target property while maintaining its similarity to the \noriginal molecule above a designed threshold. The advantage of MolDQN is that it does not \ndepend on a pre-trained model for molecule generation, and thus it is not biased to any observed \nchemical space. Therefore, MolDNQ can in principle generate novel chemical structures or \nmolecules with desired properties but may require considerable time for exploring to achieve \nfavorable performance. \nTang et al. developed an advanced deep Q-learning network with the fragment-based drug \ndesign (ADQN-FBDD) to generate molecules specifically targeting a protein with known 3D \nstructure [79]. They designed a practical reward by considering drug-likeness, as well as specific \nfragments and pharmacophores which are protein-structure dependent. Although this approach has \na obvious limitation, which is the high dependency of 3D structures of target proteins, a recent \nbreakthrough in protein structure prediction [15] may provide it with a wide range of application \nscenarios. \n \nModels with actor-critic \n \nAs mentioned in section 2, the actor-critic method improves the sampling efficiency of policy \ngradient by learning a parameterized value function (critic). It usually involves two networks, a \npolicy network and a variance-reduced value network. Intuitively, the actor, the policy network, \ndecides what action to take based on the current policy and state whereas the critic, value network, \nevaluates the action and informs the actor how the policy should be adjusted. With the guidance \nfrom the critic, the policy training process usually becomes more stable and efficient [80]. \nGregor et al. developed a novel actor-critic architecture for 3D molecule design that exploits \nthe symmetries of the design process [81]. Specifically, it employs a state embedding network to \nobtain rotation-covariant and -invariant state representations of 3D molecular graphs, which \nimproves the generalization of the policy network and enables it to generate more complex 3D \nstructure than previous approaches. Niclas et al. introduced a fragment-based RL framework based \non actor-critic where both actor and critic are modeled with bidirectional long short-term memory \n(LSTM) networks [82]. As the research is focused more on the constrained optimization task and \nthe molecular fragments are used as atomic actions, a novel encoding approach using a balanced \nbinary tree was developed to represent the fragments. With the help of this design, the RL agent is \nmore capable of distinguishing promising steps from those mediocre ones.  \nGenerating molecules in silicon is far from the final goal of developing drugs. The first step \nafter molecular design is to synthesize designed compounds and test their effects with wet \nexperiments. However, not all computational designed molecules are synthesizable. This seriously \naffects the practical value of computational drug design. Some studies try to resolve this by taking \nsynthesizability into consideration while generating molecules. Reaction-driven objective \nreinforcement (REACTOR) empowered by actor-critic method, for example, defines the state-\naction trajectory in RL as a sequence of chemical reactions, and thus not only improves the \nsynthesizability of generated molecules, but also speeds up the exploration rate of the model in the \nchemical space [83]. Additionally, REACTOR employs a synchronous version of A3C which can \nperform parallelized policy search and thus tremendously improves the efficiency of the policy \ntraining. In addition to REACTOR, there are other works that leverage actor-critic methods for \nsynthesis-oriented molecule generation, such as Towered Actor-Critic (TAC) [84] and Policy \nGradient for Forward Synthesis (PGFS) [85]. \n \nRemarks on RL algorithms \n \nWith the jumbo chemical searching space and limited pretraining samples, machine learning \nalgorithms like GAN and VAE have difficulties to generate a set of molecules distributing \ndifferently from the training dataset. Moreover, such methods are inherently hard to be adapted for \ngoal-directed tasks, especially for those related to exhaustive search. Under such circumstances, \nRL, which does not rely on training data and can be adopted to skew the underlying distribution \nof the (pretrained) generative model or directly learn a policy to optimize a seed molecule, is a \npromising approach to tackle various drug discovery problems. However, the current RL methods \napplied for molecule generation are generally sample inefficient especially in high-dimensional \nsearch space under multiple constraints, which is usually the case for drug-like molecule space. \nRecent studies tend to incorporate pretraining and adversarial training into conventional RL \nframework to overcome this difficulty. Moreover, the quality of the molecules generated by RL-\nbased generative model highly relies on the reward function. Thus, in future studies, improving \nthe sample efficiency of RL and the accuracy of the property predicting model is the key to bring \ncomputational drug design to practical production. \n \n4. Challenges and opportunities of reinforcement learning in systems \npharmacology and personalized medicine \n \n \n \nFigure 3. Illustration of a RL framework for systems pharmacology-oriented lead optimization.  \n \nFigure 3 illustrates a RL framework for systems pharmacology-oriented personalized lead \noptimization and drug design. The molecule and the environment together constitute a state. A \nmolecule generator (agent) will first take an action based on the current state and policy to generate \na new molecule or modify a seed molecule (e.g., replacing a hydrogen atom with a methyl group). \nThen, multiplex phenotypic responses (cell viability, drug-target profile, chemical-induced gene \nexpression, pharmacokinetics etc.) in an individual patient (environment) will be predicted for the \nnewly generated molecule by machine learning, biophysics, systems biology, or other methods, \nand these responses are used as the reward for policy training. A new policy will be learned based \non observed actions, states, and rewards by performing an optimization with a multi-objective RL \n(MORL) algorithm, and a new molecule will be generated again from the updated policy. Unlike \ntarget-based compound screening where only chemical space is needed to be explored, systems \npharmacology-oriented personalized drug discovery needs to optimize the interplay of chemicals, \nthe druggable genome, and high-dimensional omics characterizations of disease models or patients \n(environment). Several barriers need to be overcome when applying RL to systems pharmacology \nand precision medicine. These include the exploration of out-of-distribution samples, \ngeneralization power of RL, adaptive multi-objective optimization [86â€“88], and activity cliffs of \nquantitative structure-activity relationship (QSAR) space [89].    \n \nOut-of-distribution reward function. Although rewards for several molecular properties (e.g., \nlogP and druglikeness) can be directly calculated from a given molecular structure, reward \nfunctions that are the most important for drug actions such as binding affinity and chemical-\ninduced gene expression need to be obtained from a predictive model that is dependent on machine \nlearning, mathematical or physics-based modeling. In spite of tremendous advances in deep \nlearning and availability of diverse omics data sets, robust and accurate predictions of genome-\nwide drug-target interactions and molecular phenotypic read-outs in a physiological relevant \ncondition remain as an unsolved challenging problem. The problem is rooted in biased, noisy, and \nincomplete omics data and inherited from the limitation of machine learning. In the case of \ngenome-wide drug-target prediction, only less than 10% of gene families have known small \nmolecule ligands. The remaining 90% gene families are dark matters in the chemical genomics \nspace [17]. Even for mostly studied G-protein coupled receptors (GPCRs), more than 99% \nreceptors are orphans, i.e., their endogenous or exogenous ligands are unknown. Similarly, only a \nsmall number of cell lines have annotated drug response data. It is a fundamental challenge to \ngeneralize a â€œwell-trainedâ€ machine learning model to unseen data (e.g., patients), which lie out-\nof-the-distribution (OOD) of the training data (e.g., cell lines), so as to successfully predict \noutcomes from conditions that the model has never before encountered. While deep learning is \ncapable, in theory, of simulating any functional mapping, its generalization power is notoriously \nlimited in the case of distribution shifts [90].  \n \nGeneralization power of RL. The conventional MDP defined in section 2 is assumed that the \nproblem setting is stationary, i.e., the transition dynamics and the reward function do not change \nover time, and the agent fully observes the underlying state, i.e., the observation received by the \nagent includes perfect information of the current state. Thus, the conventional RL methods, \nespecially those falling into the model-free RL, may perform poorly in an environment that is non-\nstationary (e.g., from one patient to another) with partially observed state though they perform well \nin the conventional setting [91, 92]. Additionally, directly employing conventional RL methods to \nsolve an MDP with corrupted3 or sparse reward4 could fail catastrophically [93, 94]. The partially \nobserved, non-stationary or corrupted sparse-reward environment is the exact situation in systems \npharmacology. Due to observed chemical activity and omics data highly biased and incomplete, it \nis likely that a novel molecule is an OOD sample. Thus, no reliable reward can be assigned to this \nmolecule as mentioned above. Additionally, drug response data mainly comes from cell lines or \ndisease models. The environment in a cell line or animal model could be dramatically different \nfrom human bodies. A naÃ¯ve adaptation of standard RL systems is not sufficient for systems \npharmacology. New methods are needed to improve the robustness, generalizability and \ntransferability of RL. \n \nAdaptive multi-objective optimization. To design drugs that optimize the system-level \nresponses to maximize therapeutic effects and minimize side effects, it is needed for a RL \nalgorithm to optimize multiple (sometimes conflict) objectives such as pharmacokinetics, blood-\nbrain barrier permeation, drug binding affinities to multiple targets, or chemical-induced gene \nexpression profiles. Although RL methods have been developed for multiple objectives [78], the \nfinal reward function in these methods is an linear combination of the reward functions of \nindividual objectives with a priori defined weights.  It is often difficult to define such weights. \nMoreover, the weight may be altered when an environment changes. For example, gene expression \nprofiles can be dramatically different for different patients and in different disease states. Thus, for \nconventional RL, typically with fixed weight, a generative model (the policy) needs to be trained \nfor different conditions. In addition, it is more computationally challenging to find optimal \nsolutions in the framework of Pareto optimization for the multi-objective drug design due to the \nhigh dimensionality and uncertainty of omics and bioassay data. It has been suggested that Pareto \n                                                \n3 In the corrupted reward problem, observed reward may not be an unbiased estimate of the true reward (e.g., the \nresponse data from cell lines is different than that of human bodies) \n4 In a dense-reward setting, almost every state-action pair in a trajectory is assigned with a reward. On the contrary, \nif rewards are not available for most of the state-action pairs, the setting is considered as sparse-reward. \noptimization should be integrated with evolutionary algorithms or other complexity reduction \nmethods [86, 95]. \n \nActivity cliff of QSAR. Reward drop is a phenomenon that the reward ğ‘Ÿ\" received from a \ntrajectory {ğ‘ ), ğ‘), ğ‘ 5, ğ‘5, â€¦, ğ‘ \", ğ‘\"} suddenly drops or oscillates dramatically within certain range \ndue to the nonsmoothness of the reward function, and many advanced RL algorithms have suffered \nfrom this problem [96]. Unfortunately, in computational molecule design, a well-known \nphenomenon in QSAR is activity cliff, in which a slight modification of chemical structure may \nlead to a dramatic activity change. The activity cliff is more complicated in systems pharmacology \nthan single-targeted drug discovery. For example, the replace of a methyl group with an ethyl \ngroup for a chemical compound that has moderate binding affinities to two targets may increase \nthe binding affinity to one of targets, but completely destroy the binding to another target due to \nsteric clash. As a result, the phenotype response modulated by this compound could be changed \nsignificantly. Such unpleasant property (of QSAR landscape) thus increases the difficulty of policy \nlearning for RL-based drug design.  \n \nTo address aforementioned challenges in adapting RL to systems pharmacology and precision \nmedicine, a synergistic integration of latest advances in RL with new development in machine \nlearning and other related fields is needed.   \n \nImproving robustness, generalizability, and transferability of RL algorithms. Recently, Ghosh \net al, showed that optimal generalization of RL at test-time corresponds to solving a partially-\nobserved Markov decision process (POMDP) that is induced by the agentâ€™s epistemic uncertainty \nabout the test environment [92]. They proposed an algorithm, LEEP, which uses an ensemble of \npolicies to approximately learn the Bayes-optimal policy for maximizing test-time performance. \nIn another study, Agarwal et al. proposed meta-reward learning to achieve the generalizability of \nRL in a sparse-reward environment [97]. In principle, these techniques can be adapted for systems \npharmacology. \nThe majority of existing work in RL for drug design is based on the model-free approach. \nModel-based RL [6] may provide new opportunities for addressing the challenges in systems \npharmacology-oriented drug discovery for precision medicine. Different from model-free \napproach, model-based method typically employs a learned dynamics model to facilitate policy \ntraining. It is more powerful in predicting future reward, has higher sample efficiency, and bears \nstronger transferability and generality than the model-free approach. The ability of predicting \nfuture reward may help to avoid activity cliffs. Sample efficiency will be helpful to alleviate issues \nin data sparsity, biasness, and noisiness. Transferability and generalizability are critical to address \nthe OOD problem.  \nSeveral algorithms have been developed to solve multi-objective optimization problems in RL. \nYang et al. introduced an envelope Q-learning method that can quickly adapt and solve new tasks \nwith different preferences [98]. The capability of learning a single Q function over all preferences \nis important for personalized drug discovery. In another study, Chen et al. proposed a two-stage \nmodel [99] for multi-objective deep RL. At the first stage, a multi-policy soft actor-critic algorithm \nis applied to collaboratively learn multiple policies with different targets, in which each policy \ntargets on a specific scalarized objective. At the second stage, a multi-objective covariance matrix \nadaptation evolution strategy is applied to fine-tune the policy-independent parameters.  \n \nNew methods to improve reward functions for OOD data. A plethora of machine learning \napproaches including self-supervised learning, transfer learning, semi-supervised learning, meta-\nlearning and their combinations have been recently developed to address out-of-distribution \nproblems in compound screening in terms of chemicals, proteins and cell lines. Self-supervised \nlearning has enjoyed a great success in Natural Language Processing, image recognition, and \nprotein sequences modeling. Cai et al. have proposed a DIstilled Sequence Alignment Embedding \n(DISAE) transformer for predicting ligand binding to orphan receptors [100]. DISAE has been \nfurther extended to out-of-distribution prediction of receptor activities of ligand binding, \nspecifically, agonist vs antagonist [101]. An out-of-cluster meta-learning algorithm has been \nproposed to explore dark chemical genomics space that includes all Pfam families [102]. Self-\nsupervised learning and semi-supervised learning have also been applied to explore chemical space \n[103]. Transfer learning is particularly useful in predicting drug responses (both cell viability and \ngene expressions) for novel cell lines [104] or translating in vitro compound screens to clinical \noutcomes in patients [105]. These methods, when applied to reward functions, could improve the \nperformance of RL in systems pharmacology.  \nBiophysics-based methods such as molecular dynamics (MD) simulation, quantum chemistry, \nand protein-ligand docking (PLD), can be directly applied to evaluate the chemical properties, and \nthus can be used as reward functions. MD simulation and quantum chemistry calculation are \ncomputationally expensive. Emergence of quantum computing may make it feasible to incorporate \nthem directly into RL as reward functions [106]. With the advent of high-accuracy protein \nstructural models, such as AlphaFold2 [15], it now becomes feasible to use PLD to predict ligand-\nbinding sites and poses on dark proteins, on a genome-wide scale. However, PLD suffers from a \nhigh false-positive rate due to poor modeling of protein dynamics, solvation effects, crystallized \nwaters, and other challenges [107]; often, small-molecule ligands will indiscriminately 'stick' to \nconcave, pocket-like patches on protein surfaces. For these reasons, although AlphaFold2 can \naccurately predict many protein structures, the relatively low reliability of PLD still poses a \nsignificant limitation, even with a limitless supply of predicted structures [108]. Thus, the direct \napplication of PLD remains a challenge for predicting ligand binding to dark proteins. Recently, \nCai et al. have proposed an end-to-end sequence-structure-function learning framework PortalCG \n[17]: protein structure information is not used as a fixed input, but rather as an intermediate layer \nthat can be tuned using various structural and functional information. PortalCG significantly \noutperforms the direct use of PLD for predicting ligand binding to dark proteins. Thus, it could be \nan effective strategy to incorporate biophysics domain-knowledge into deep learning frameworks \nas constraints or regularizations.  \n \n5. Conclusion \n \nIn spite of the great success of RL in GO board game, computer games, and other settings with \nwell-defined environments, its application to drug discovery is still in its infancy. Current efforts \nin RL mainly focus on target-based drug design. Although RL is a promising technique, its actual \nvalue in the target-based drug discovery is overestimated in the current stage. The major hurdle \ncomes from the reward function that is based on predicted binding affinities and needs to be \nobtained from either a machine learning approach or a physics-based scoring. Both of them are \nnot reliable and accurate enough when applied to chemical compounds with novel structures. For \na machine learning approach, it remains a great challenge to predict an OOD sample, i.e., a \ngenerated molecule that falls outside the distribution of chemicals in the training data for activity \npredictions. As a result, the structure of active compounds inferred from RL may not be \nsignificantly different from those in the training data. In terms of physical-based scoring, there is \na lack of computationally efficient and accurate methods to model multiple factors including \nconformational dynamics, solvent effects, hydrogen bonding, entropies, crystallized waters, etc., \nwhich contribute to the protein-ligand binding affinity. Consequently, the scoring function is still \nsuboptimal and unreliable. In the authorsâ€™ humble opinion, existing RL approaches to de novo \ndrug design are scarcely fruitful in regard to the structural novelty of chemical compounds without \nthe significant improvement of efficiency and accuracy of binding affinity predictions.  \nAlthough current efforts in RL mainly center around the target-based drug design, systems \npharmacology and personalized medicine have emerged as new paradigms in drug discovery. They \nhave advantages over the conventional â€œone-drug-one-geneâ€ approach when tackling multi-genic, \nheterogeneous diseases. On the other hand, systems pharmacology-oriented and personalized drug \ndesign imposes new challenges on RL-based drug design. Conceptually, systems pharmacology \nhas not been fully appreciated by the pharmaceutical and biotechnology industry. Technically, \nthere are few high-quality labeled data available for training a generalizable machine learning \nmodel to predict molecular phenotypic readouts suitable for systems pharmacology-oriented and \npersonalized drug design. Thus, the OOD problem in systems pharmacology is more serious than \nthe target-based drug design. Besides the OOD issue that incapacitates the reward function, the \nsuccess of RL in systems pharmacology-oriented and personalized drug design needs to overcome \nadditional roadblocks. Firstly, the generalizability and transferability are the central challenges for \nthe deployment of RL in systems pharmacology as the environments are partially observable or \nchanged dramatically (e.g., from cell lines to human tissues). Secondly, RL needs to optimize \nmultiple dynamic and often conflict rewards in a non-stationary environment while the dynamic \nmulti-objective optimization still remains as an unsolved research problem. Finally, it is necessary \nto integrate multiple heterogeneous, noisy, and high-dimensional omics data for successful \nsystems pharmacology modeling, which is a challenging task under intensive investigations.  Thus, \nthe realization of the full potential of RL in drug discovery relies on not only new developments \nin RL but also advances in other fields that are beyond RL. Specifically, RL needs to be \nsynergistically integrated with unsupervised/supervised machine learning, biophysics, systems \nbiology, multi-omics technology, and quantum computing.   \n \n \nReferences \n1. \nPolishchuk PG, Madzhidov TI, Varnek A. Estimation of the Size of Drug-like Chemical \nSpace Based on GDB-17 Data. J Comput Aided Mol Des. 2013;27(8):675â€“679. \n2. \nGoodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, \nBengio Y. Generative Adversarial Networks. Commun ACM. 2014;63(11):139â€“144. \n3. \nKingma DP, Welling M. Auto-Encoding Variational Bayes. 2nd Int Conf Learn Represent \nICLR 2014 - Conf Track Proc. 2013. \n4. \nRezende DJ, Mohamed S. Variational Inference with Normalizing Flows. 32nd Int Conf \nMach Learn ICML 2015. 2015;2:1530â€“1538. \n5. \nMadhawa K, Ishiguro K, Nakago K, Abe M. GraphNVP: An Invertible Flow Model for \nGenerating Molecular Graphs. 2019. \n6. \nPack Kaelbling L, Littman ML, Moore AW, Hall S. Reinforcement Learning: A Survey. J \nArtiicial Intell Res. 1996;4:237â€“285. \n7. \nOlivecrona M, Blaschke T, Engkvist O, Chen H. Molecular De-Novo Design through \nDeep Reinforcement Learning. J Cheminform. 2017;9(1):48. \n8. \nYou J, Liu B, Ying R, Pande V, Leskovec J. Graph Convolutional Policy Network for \nGoal-Directed Molecular Graph Generation. Adv Neural Inf Process Syst. 2018;2018-\nDecem:6410â€“6421. \n9. \nGuimaraes G, Sanchez-Lengeling B, Outeiral C, Luis P, Farias C, Aspuru-Guzik A. \nObjective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence \nGeneration Models. \n10. \nXie L, Xie L, Kinnings SL, Bourne PE. Novel Computational Approaches to \nPolypharmacology as a Means to Define Responses to Individual Drugs. Annu Rev \nPharmacol Toxicol. 2012;52:361â€“379. \n11. \nSwinney DC, Lee JA. Recent Advances in Phenotypic Drug Discovery. F1000Research. \n2020;9. \n12. \nMoffat JG, Vincent F, Lee JA, Eder J, Prunotto M. Opportunities and Challenges in \nPhenotypic Drug Discovery: An Industry Perspective. Nat Rev Drug Discov 2017 168. \n2017;16(8):531â€“543. \n13. \nPham TH, Qiu Y, Zeng J, Xie L, Zhang P. A Deep Learning Framework for High-\nThroughput Mechanism-Driven Phenotype Compound Screening and Its Application to \nCOVID-19 Drug Repurposing. Nat Mach Intell. 2021;3(3):247â€“257. \n14. \nMÃ©ndez-Lucio O, Zapata PAM, Wichard J, RouquiÃ© D, Clevert D-A. Cell Morphology-\nGuided De Novo Hit Design by Conditioning Generative Adversarial Networks on \nPhenotypic Image Features. 2020. \n15. \nJumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, et al. Highly \nAccurate Protein Structure Prediction with AlphaFold. Nature. 2021;596(7873):583â€“589. \n16. \nBaek M, DiMaio F, Anishchenko I, Dauparas J, Ovchinnikov S, Lee GR, et al. Accurate \nPrediction of Protein Structures and Interactions Using a Three-Track Neural Network. \nScience. 2021;373(6557):871â€“876. \n17. \nCai T, Xie L, Chen M, Liu Y, He D, Zhang S, Mura C, Bourne PE, Xie L. Exploration of \nDark Chemical Genomics Space via Portal Learning: Applied to Targeting the \nUndruggable Genome and COVID-19 Anti-Infective Polypharmacology. arXiv Prepr \narXiv211114283. 2021. \n18. \nBELLMAN R. A Markovian Decision Process. J Math Mech. 1957;6(5):679â€“684. \n19. \nMunos R. Error Bounds for Approximate Value Iteration. In Proceedings of the 20th \nNational Conference on Artificial Intelligence - Volume 2; AAAIâ€™05; AAAI Press, 2005; \npp 1006â€“1011. \n20. \nHaskell WB, Jain R, Kalathil D. Empirical Dynamic Programming. Math Oper Res. \n2013;41(2):402â€“429. \n21. \nSzepesvÃ¡ri C, Munos R. Finite Time Bounds for Sampling Based Fitted Value Iteration. \nICML 2005 - Proc 22nd Int Conf Mach Learn. 2005881â€“888. \n22. \nGhavamzadeh M, Kappen H, Azar M, Munos R. Speedy Q-Learning. In Advances in \nNeural Information Processing Systems; Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, \nF., Weinberger, K. Q., Eds.; Curran Associates, Inc., 2011; Vol. 24. \n23. \nGordon GJ. Approximate Solutions to Markov Decision Processes; Carnegie Mellon \nUniversity, 1999. \n24. \nSutton RS, Barto AG. Reinforcement Learning: An Introduction; MIT press, 2018. \n25. \nErnst D, Geurts P, Wehenkel L. Tree-Based Batch Mode Reinforcement Learning. J Mach \nLearn Res. 2005;6:503â€“556. \n26. \nOrmoneit D, Glynn P. Kernel-Based Reinforcement Learning in Average-Cost Problems. \nIEEE Trans Automat Contr. 2002;47(10):1624â€“1636. \n27. \nOrmoneit D, Sen Åš. Kernel-Based Reinforcement Learning. Mach Learn. 2002;49(2):161â€“\n178. \n28. \nShen J, Yang LF. Theoretically Principled Deep RL Acceleration via Nearest Neighbor \nFunction Approximation. In Proceedings of the AAAI Conference on Artificial \nIntelligence; 2021; Vol. 35, pp 9558â€“9566. \n29. \nMnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, et al. Human-\nLevel Control through Deep Reinforcement Learning. Nat 2015 5187540. \n2015;518(7540):529â€“533. \n30. \nWatkins CJCH, Dayan P. Q-Learning. Mach Learn. 1992;8(3â€“4):279â€“292. \n31. \nHasselt H. Double Q-Learning. Adv Neural Inf Process Syst. 2010;23:2613â€“2621. \n32. \nWang Z, Schaul T, Hessel M, Hasselt H, Lanctot M, Freitas N. Dueling Network \nArchitectures for Deep Reinforcement Learning. In International conference on machine \nlearning; 2016; pp 1995â€“2003. \n33. \nSchaul T, Quan J, Antonoglou I, Silver D. Prioritized Experience Replay. arXiv Prepr \narXiv151105952. 2015. \n34. \nBellemare MG, Dabney W, Munos R. A Distributional Perspective on Reinforcement \nLearning. 34th Int Conf Mach Learn ICML 2017. 2017;1:693â€“711. \n35. \nDabney W, Rowland M, Bellemare MG, Munos R. Distributional Reinforcement Learning \nwith Quantile Regression. 32nd AAAI Conf Artif Intell AAAI 2018. 20172892â€“2901. \n36. \nYang D, Zhao L, Lin Z, Qin T, Bian J, Liu T. Fully Parameterized Quantile Function for \nDistributional Reinforcement Learning. Adv Neural Inf Process Syst. 2019;32. \n37. \nWilliams RJ. Simple Statistical Gradient-Following Algorithms for Connectionist \nReinforcement Learning. Mach Learn. 1992;8(3):229â€“256. \n38. \nGhavamzadeh M, Mahadevan S. Hierarchical Policy Gradient Algorithms. Comput Sci \nDep Fac Publ Ser. 2003173. \n39. \nGreensmith E, Bartlett PL, Baxter J. Variance Reduction Techniques for Gradient \nEstimates in Reinforcement Learning. J Mach Learn Res. 2004;5(9). \n40. \nKonda VR, Tsitsiklis JN. Actor-Critic Algorithms. In Advances in neural information \nprocessing systems; 2000; pp 1008â€“1014. \n41. \nDegris T, Pilarski PM, Sutton RS. Model-Free Reinforcement Learning with Continuous \nAction in Practice. In 2012 American Control Conference (ACC); 2012; pp 2177â€“2182. \n42. \nMnih V, Badia AP, Mirza M, Graves A, Lillicrap T, Harley T, Silver D, Kavukcuoglu K. \nAsynchronous Methods for Deep Reinforcement Learning. In International conference on \nmachine learning; 2016; pp 1928â€“1937. \n43. \nSchulman J, Moritz P, Levine S, Jordan M, Abbeel P. High-Dimensional Continuous \nControl Using Generalized Advantage Estimation. arXiv Prepr arXiv150602438. 2015. \n44. \nKimura H, Kobayashi S. An Analysis of Actor/Critic Algorithms Using Eligibility Traces: \nReinforcement Learning with Imperfect Value Function. In ICML; 1998. \n45. \nSchulman J, Levine S, Abbeel P, Jordan MI, Moritz P. Trust Region Policy Optimization. \nArXiv. 2015;abs/1502.0. \n46. \nSchulman J, Wolski F, Dhariwal P, Radford A, Klimov O. Proximal Policy Optimization \nAlgorithms. ArXiv. 2017;abs/1707.0. \n47. \nLillicrap TP, Hunt JJ, Pritzel A, Heess NMO, Erez T, Tassa Y, Silver D, Wierstra D. \nContinuous Control with Deep Reinforcement Learning. CoRR. 2016;abs/1509.0. \n48. \nFujimoto S, van Hoof H, Meger D. Addressing Function Approximation Error in Actor-\nCritic Methods. ArXiv. 2018;abs/1802.0. \n49. \nHaarnoja T, Zhou A, Abbeel P, Levine S. Soft Actor-Critic: Off-Policy Maximum \nEntropy Deep Reinforcement Learning with a Stochastic Actor. In ICML; 2018. \n50. \nKaiser Å, Babaeizadeh M, MiÅ‚os P, Zej OsÃ­ Nski B, Campbell RH, Czechowski K, et al. \nModel-Based Reinforcement Learning for Atari. 2019. \n51. \nAtkeson CG, Santamaria JC. A Comparison of Direct and Model-Based Reinforcement \nLearning. In Proceedings of International Conference on Robotics and Automation; 1997; \nVol. 4, pp 3557â€“3564 vol.4. \n52. \nBerkenkamp F, Turchetta M, Schoellig A, Krause A. Safe Model-Based Reinforcement \nLearning with Stability Guarantees. In Advances in Neural Information Processing \nSystems; Guyon, I., Luxburg, U. V, Bengio, S., Wallach, H., Fergus, R., Vishwanathan, \nS., Garnett, R., Eds.; Curran Associates, Inc., 2017; Vol. 30. \n53. \nLaValle SM. Planning Algorithms; Cambridge University Press, 2006. \n54. \nRao A V. A Survey of Numerical Methods for Optimal Control. Adv Astronaut Sci. \n2009;135(1):497â€“528. \n55. \nQin SJ, Badgwell TA. A Survey of Industrial Model Predictive Control Technology. \nControl Eng Pract. 2003;11(7):733â€“764. \n56. \nChua K, Calandra R, McAllister R, Levine S. Deep Reinforcement Learning in a Handful \nof Trials Using Probabilistic Dynamics Models. Adv Neural Inf Process Syst. 2018;2018-\nDecember:4754â€“4765. \n57. \nNagabandi A, Kahn G, Fearing RS, Levine S. Neural Network Dynamics for Model-\nBased Deep Reinforcement Learning with Model-Free Fine-Tuning. Proc - IEEE Int Conf \nRobot Autom. 20177579â€“7586. \n58. \nDeisenroth M, Rasmussen C. PILCO: A Model-Based and Data-Efficient Approach to \nPolicy Search.; 2011. \n59. \nLevine S, Koltun V. Guided Policy Search. In International conference on machine \nlearning; PMLR, 2013; pp 1â€“9. \n60. \nHeess N, Wayne G, Silver D, Lillicrap T, Tassa Y, Erez T. Learning Continuous Control \nPolicies by Stochastic Value Gradients. Adv Neural Inf Process Syst. 2015;2015-\nJanuary:2944â€“2952. \n61. \nSutton RS. Integrated Architectures for Learning, Planning, and Reacting Based on \nApproximating Dynamic Programming. In In Proceedings of the Seventh International \nConference on Machine Learning; Morgan Kaufmann, 1990; pp 216â€“224. \n62. \nKurutach T, Clavera I, Duan Y, Tamar A, Abbeel P. Model-Ensemble Trust-Region \nPolicy Optimization. 6th Int Conf Learn Represent ICLR 2018 - Conf Track Proc. 2018. \n63. \nParmas P, Rasmussen CE, Peters J, Doya K. PIPPS: Flexible Model-Based Policy Search \nRobust to the Curse of Chaos. 35th Int Conf Mach Learn ICML 2018. 2019;9:6463â€“6472. \n64. \nJanner M, Fu J, Zhang M, Levine S. When to Trust Your Model: Model-Based Policy \nOptimization. Adv Neural Inf Process Syst. 2019;32. \n65. \nPolykovskiy D, Zhebrak A, Sanchez-Lengeling B, Golovanov S, Tatanov O, Belyaev S, et \nal. Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation \nModels. Front Pharmacol. 2020;11. \n66. \nBrown N, Fiscato M, Segler MHS, Vaucher AC. GuacaMol: Benchmarking Models for de \nNovo Molecular Design. J Chem Inf Model. 2019;59(3):1096â€“1108. \n67. \nNeil D, Segler MHS, Guasch L, Ahmed M, Plumbley D, Sellwood M, Brown N. \nExploring Deep Recurrent Models with Reinforcement Learning for Molecule Design. In \nICLR; 2018. \n68. \nYang X, Aasawat TK, Yoshizoe K. Practical Massively Parallel Monte-Carlo Tree Search \nApplied to Molecular Design. 2020. \n69. \nAhn S, Kim J, Lee H, Shin J. Guiding Deep Molecular Optimization with Genetic \nExploration. Adv Neural Inf Process Syst. 2020;2020-December. \n70. \nBengio Y, Lodi A, Prouvost A. Machine Learning for Combinatorial Optimization: A \nMethodological Tour dâ€™horizon. Eur J Oper Res. 2021;290(2):405â€“421. \n71. \nBello I, Pham H, Le Q V., Norouzi M, Bengio S. Neural Combinatorial Optimization with \nReinforcement Learning. 5th Int Conf Learn Represent ICLR 2017 - Work Track Proc. \n2016. \n72. \nOlivecrona M, Blaschke T, Engkvist O, Chen H. Molecular De-Novo Design through \nDeep Reinforcement Learning. J Cheminform. 2017;9(1):1â€“14. \n73. \nLu F, Li M, Min X, Li C, Zeng X. De Novo Generation of Dual-Target Ligands Using \nAdversarial Training and Reinforcement Learning. Brief Bioinform. 2021;22(6). \n74. \nAtance SR, Diez JV, Engkvist O, Olsson S, Mercado R. De Novo Drug Design Using \nReinforcement Learning with Graph-Based Deep Generative Models. 2021. \n75. \nPopova M, Shvets M, Oliva J, Isayev O. MolecularRNN: Generating Realistic Molecular \nGraphs with Optimized Properties. arXiv Prepr arXiv190513372. 2019. \n76. \nLiu X, Ye K, van Vlijmen HWT, IJzerman AP, van Westen GJP. An Exploration Strategy \nImproves the Diversity of de Novo Ligands Using Deep Reinforcement Learning: A Case \nfor the Adenosine A2A Receptor. J Cheminform. 2019;11(1):1â€“16. \n77. \nPereira T, Abbasi M, Ribeiro B, Arrais JP. Diversity Oriented Deep Reinforcement \nLearning for Targeted Molecule Generation. J Cheminform. 2021;13(1). \n78. \nZhou Z, Kearnes S, Li L, Zare RN, Riley P. Optimization of Molecules via Deep \nReinforcement Learning. Sci Rep. 2019;9(1):1â€“10. \n79. \nTang B, He F, Liu D, Fang M, Wu Z, Xu D. AI-Aided Design of Novel Targeted Covalent \nInhibitors against SARS-CoV-2. bioRxiv  Prepr Serv Biol. 2020. \n80. \nWen J, Kumar S, Gummadi R, Schuurmans D. Characterizing the Gap Between Actor-\nCritic and Policy Gradient. arXiv Prepr arXiv210606932. 2021. \n81. \nSimm GNC, Pinsler R, CsÃ¡nyi G, HernÃ¡ndez-Lobato JM. Symmetry-Aware Actor-Critic \nfor 3D Molecular Design. ArXiv. 2021;abs/2011.1. \n82. \nStÃ¥hl N, Falkman G, Karlsson A, Mathiason G, BostrÃ¶m J. Deep Reinforcement Learning \nfor Multiparameter Optimization in de Novo Drug Design. 2019. \n83. \nHorwood J, Noutahi E. Molecular Design in Synthetically Accessible Chemical Space via \nDeep Reinforcement Learning. ACS Omega. 2020;5(51):32984â€“32994. \n84. \nGottipati SK, Pathak Y, Sattarov B, Sahir, Nuttall R, Amini M, Taylor ME, Chandar APS. \nTowered Actor Critic For Handling Multiple Action Types In Reinforcement Learning \nFor Drug Discovery. In AAAI; 2021. \n85. \nGottipati SK, Sattarov B, Niu S, Pathak Y, Wei H, Liu S, et al. Learning To Navigate The \nSynthetically Accessible Chemical Space Using Reinforcement Learning. In ICML; 2020. \n86. \nLambrinidis G, Tsantili-Kakoulidou A. Challenges with Multi-Objective QSAR in Drug \nDiscovery. Expert Opin Drug Discov. 2018;13(9):851â€“859. \n87. \nLi Z, Cho BR, Melloy BJ. Quality by Design Studies on Multi-Response Pharmaceutical \nFormulation Modeling and Optimization. J Pharm Innov. 2013;8(1):28â€“44. \n88. \nCruz-Monteagudo M, SchÃ¼rer S, Tejera E, PÃ©rez-Castillo Y, Medina-Franco JL, SÃ¡nchez-\nRodrÃ­guez A, Borges F. Systemic QSAR and Phenotypic Virtual Screening: Chasing \nButterflies in Drug Discovery. Drug Discov Today. 2017;22(7):994â€“1007. \n89. \nCruz-Monteagudo M, Medina-Franco JL, PÃ©rez-Castillo Y, Nicolotti O, Cordeiro MNDS, \nBorges F. Activity Cliffs in Drug Discovery: Dr Jekyll or Mr Hyde? Drug Discov Today. \n2014;19(8):1069â€“1080. \n90. \nScholkopf B, Locatello F, Bauer S, Ke NR, Kalchbrenner N, Goyal A, Bengio Y. Towards \nCausal Representation Learning. Proc IEEE. 2021;109(5):612â€“634. \n91. \nChandak Y, Theocharous G, Shankar S, White M, Mahadevan S, Thomas PS. Optimizing \nfor the Future in Non-Stationary MDPs. 37th Int Conf Mach Learn ICML 2020. \n2020;PartF168147-2:1391â€“1402. \n92. \nGhosh D, Rahme J, Kumar A, Zhang A, Adams RP, Levine S. Why Generalization in RL \nIs Difficult: Epistemic POMDPs and Implicit Partial Observability. Adv Neural Inf \nProcess Syst. 2021;34. \n93. \nEveritt T, Krakovna V, Orseau L, Legg S. Reinforcement Learning with a Corrupted \nReward Channel. IJCAI Int Jt Conf Artif Intell. 2017;0:4705â€“4713. \n94. \nRiedmiller M, Hafner R, Lampe T, Neunert M, Degrave J, Van De Wiele T, Mnih V, \nHeess N, Springenberg T. Learning by Playing - Solving Sparse Reward Tasks from \nScratch. 35th Int Conf Mach Learn ICML 2018. 2018;10:6910â€“6919. \n95. \nSÃ¡nchez-RodrÃ­guez A, PÃ©rez-Castillo Y, SchÃ¼rer SC, Nicolotti O, Mangiatordi GF, \nBorges F, Cordeiro MNDS, Tejera E, Medina-Franco JL, Cruz-Monteagudo M. From \nFlamingo Dance to (Desirable) Drug Discovery: A Nature-Inspired Approach. Drug \nDiscov Today. 2017;22(10):1489â€“1502. \n96. \nDong Y, Zhang S, Liu X, Zhang Y, Shen T. Variance Aware Reward Smoothing for Deep \nReinforcement Learning. Neurocomputing. 2021;458:327â€“335. \n97. \nAgarwal R, Liang C, Schuurmans D, Norouzi M. Learning to Generalize from Sparse and \nUnderspecified Rewards. 36th Int Conf Mach Learn ICML 2019. 2019;2019-June:184â€“\n194. \n98. \nYang R, Sun X, Narasimhan K. A Generalized Algorithm for Multi-Objective \nReinforcement Learning and Policy Adaptation. Adv Neural Inf Process Syst. 2019;32. \n99. \nChen D, Wang Y, Gao W. A Two-Stage Multi-Objective Deep Reinforcement Learning \nFramework. Front Artif Intell Appl. 2020;325:1063â€“1070. \n100. Cai T, Lim H, Abbu KA, Qiu Y, Nussinov R, Xie L. MSA-Regularized Protein Sequence \nTransformer toward Predicting Genome-Wide Chemical-Protein Interactions: Application \nto GPCRome Deorphanization. J Chem Inf Model. 2021;61(4):1570â€“1582. \n101. Cai T, Abbu KA, Liu Y, Xie L. DeepREAL: A Deep Learning Powered Multi-Scale \nModeling Framework Towards Predicting Out-of-Distribution Receptor Activity of \nLigand Binding. bioRxiv. 20212021.09.12.460001. \n102. Mistry J, Chuguransky S, Williams L, Qureshi M, Salazar GA, Sonnhammer ELL, et al. \nPfam: The Protein Families Database in 2021. Nucleic Acids Res. 2021;49(D1):D412â€“\nD419. \n103. Liu Y, Wu Y, Shen X, Xie L. COVID-19 Multi-Targeted Drug Repurposing Using Few-\nShot Learning. Front Bioinforma. 2021;0:18. \n104. Liu Q, Qiu Y, Xie L. Predictive Modeling of Multiplex Chemical Phenomics for Novel \nCells and Patients: Applied to Personalized Alzheimerâ€™s Disease Drug Repurposing. \nbioRxiv. 20212021.08.09.455708. \n105. He D, Liu Q, Xie L. Robust Prediction of Patient-Specific Clinical Response to Unseen \nDrugs From in Vitro Screens Using Context-Aware Deconfounding Autoencoder. \nbioRxiv. 2021. \n106. Marx V. Biology Begins to Tangle with Quantum Computing. Nat Methods 2021 187. \n2021;18(7):715â€“719. \n107. Grinter SZ, Zou X. Challenges, Applications, and Recent Advances of Protein-Ligand \nDocking in Structure-Based Drug Design. Molecules. 2014;19(7):10150â€“10176. \n108. Jaiteh M, RodrÃ­guez-Espigares I, Selent J, Carlsson J. Performance of Virtual Screening \nagainst GPCR Homology Models: Impact of Template Selection and Treatment of \nBinding Site Plasticity. PLOS Comput Biol. 2020;16(3):e1007680. \n \n \n",
  "categories": [
    "q-bio.BM",
    "cs.LG"
  ],
  "published": "2022-01-21",
  "updated": "2022-02-23"
}