{
  "id": "http://arxiv.org/abs/2201.09267v1",
  "title": "Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey",
  "authors": [
    "Benyamin Ghojogh",
    "Ali Ghodsi",
    "Fakhri Karray",
    "Mark Crowley"
  ],
  "abstract": "This is a tutorial and survey paper on metric learning. Algorithms are\ndivided into spectral, probabilistic, and deep metric learning. We first start\nwith the definition of distance metric, Mahalanobis distance, and generalized\nMahalanobis distance. In spectral methods, we start with methods using scatters\nof data, including the first spectral metric learning, relevant methods to\nFisher discriminant analysis, Relevant Component Analysis (RCA), Discriminant\nComponent Analysis (DCA), and the Fisher-HSIC method. Then, large-margin metric\nlearning, imbalanced metric learning, locally linear metric adaptation, and\nadversarial metric learning are covered. We also explain several kernel\nspectral methods for metric learning in the feature space. We also introduce\ngeometric metric learning methods on the Riemannian manifolds. In probabilistic\nmethods, we start with collapsing classes in both input and feature spaces and\nthen explain the neighborhood component analysis methods, Bayesian metric\nlearning, information theoretic methods, and empirical risk minimization in\nmetric learning. In deep learning methods, we first introduce reconstruction\nautoencoders and supervised loss functions for metric learning. Then, Siamese\nnetworks and its various loss functions, triplet mining, and triplet sampling\nare explained. Deep discriminant analysis methods, based on Fisher discriminant\nanalysis, are also reviewed. Finally, we introduce multi-modal deep metric\nlearning, geometric metric learning by neural networks, and few-shot metric\nlearning.",
  "text": "To appear as a part of an upcoming textbook on dimensionality reduction and manifold learning.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\nBenyamin Ghojogh\nBGHOJOGH@UWATERLOO.CA\nDepartment of Electrical and Computer Engineering,\nMachine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada\nAli Ghodsi\nALI.GHODSI@UWATERLOO.CA\nDepartment of Statistics and Actuarial Science & David R. Cheriton School of Computer Science,\nData Analytics Laboratory, University of Waterloo, Waterloo, ON, Canada\nFakhri Karray\nKARRAY@UWATERLOO.CA\nDepartment of Electrical and Computer Engineering,\nCentre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, ON, Canada\nMark Crowley\nMCROWLEY@UWATERLOO.CA\nDepartment of Electrical and Computer Engineering,\nMachine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada\nAbstract\nThis is a tutorial and survey paper on metric\nlearning. Algorithms are divided into spectral,\nprobabilistic, and deep metric learning. We ﬁrst\nstart with the deﬁnition of distance metric, Ma-\nhalanobis distance, and generalized Mahalanobis\ndistance.\nIn spectral methods, we start with\nmethods using scatters of data, including the\nﬁrst spectral metric learning, relevant methods to\nFisher discriminant analysis, Relevant Compo-\nnent Analysis (RCA), Discriminant Component\nAnalysis (DCA), and the Fisher-HSIC method.\nThen, large-margin metric learning, imbalanced\nmetric learning, locally linear metric adapta-\ntion, and adversarial metric learning are covered.\nWe also explain several kernel spectral methods\nfor metric learning in the feature space.\nWe\nalso introduce geometric metric learning meth-\nods on the Riemannian manifolds. In probabilis-\ntic methods, we start with collapsing classes in\nboth input and feature spaces and then explain\nthe neighborhood component analysis methods,\nBayesian metric learning, information theoretic\nmethods, and empirical risk minimization in met-\nric learning. In deep learning methods, we ﬁrst\nintroduce reconstruction autoencoders and super-\nvised loss functions for metric learning. Then,\nSiamese networks and its various loss functions,\ntriplet mining, and triplet sampling are explained.\nDeep discriminant analysis methods, based on\nFisher discriminant analysis, are also reviewed.\nFinally, we introduce multi-modal deep metric\nlearning, geometric metric learning by neural\nnetworks, and few-shot metric learning.\n1. Introduction\nDimensionality reduction and manifold learning are used\nfor feature extraction from raw data. A family of dimen-\nsionality reduction methods is metric learning which learns\na distance metric or an embedding space for separation of\ndissimilar points and closeness of similar points. In su-\npervised metric learning, we aim to discriminate classes\nby learning an appropriate metric. Dimensionality reduc-\ntion methods can be divided into spectral, probabilistic,\nand deep methods (Ghojogh, 2021).\nSpectral methods\nhave a geometrical approach and usually are reduced to\ngeneralized eigenvalue problems (Ghojogh et al., 2019a).\nProbabilistic methods are based on probability distribu-\ntions. Deep methods use neural network for learning. In\neach of these categories, there exist several metric learn-\ning methods. In this paper, we review and introduce the\nmost important metric learning algorithms in these cate-\ngories. Note that there exist some other surveys on met-\nric learning such as (Yang & Jin, 2006; Yang, 2007; Kulis,\n2013; Bellet et al., 2013; Wang & Sun, 2015; Su´arez et al.,\n2021). A survey speciﬁc to deep metric learning is (Kaya\n& Bilge, 2019). A book on metric learning is (Bellet et al.,\narXiv:2201.09267v1  [stat.ML]  23 Jan 2022\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n2\n2015). Finally, some Python toolboxes for metric learning\nare (Su´arez et al., 2020; De Vazelhes et al., 2020; Musgrave\net al., 2020). The remainder of this paper is organized as\nfollows. Section 2 deﬁnes distance metric and the general-\nized Mahalanobis distance. Sections 3, 4, and 5 introduce\nand discuss spectral, probabilistic, and deep metric learn-\ning methods, respectively. Finally, section 6 concludes the\npaper. The table of contents can be found at the end of\npaper.\nRequired Background for the Reader\nThis paper assumes that the reader has general knowledge\nof calculus, probability, linear algebra, and basics of opti-\nmization.\n2. Generalized Mahalanobis Distance Metric\n2.1. Distance Metric\nDeﬁnition 1 (Distance metric). Consider a metric space\nX. A distance metric is a mapping d : X × X →[0, ∞)\nwhich satisﬁes the following properties:\n• non-negativity: d(xi, xj) ≥0\n• identity: d(xi, xj) = 0 ⇐⇒xi = xj\n• symmetry: d(xi, xj) = d(xj, xi)\n• triangle inequality:\nd(xi, xj)\n≤\nd(xi, xk) +\nd(xk, xj)\nwhere xi, xj, xk ∈X.\nAn example of distance metric is the Euclidean distance:\n∥xi −xj∥2 :=\nq\n(xi −xj)⊤(xi −xj).\n(1)\n2.2. Mahalanobis Distance\nThe Mahalanobis distance is another distance metric which\nwas originally proposed in (Mahalanobis, 1930).\nDeﬁnition 2 (Mahalanobis distance (Mahalanobis, 1930)).\nConsider a d-dimensional metric space X. Let two clouds\nor sets of points X1 and X2 be in the data, i.e., X1, X2 ∈X.\nA point is considered in each set, i.e., xi ∈X1 and xj ∈\nX2. The Mahalanobis distance between the two points is:\n∥xi −xj∥Σ :=\nq\n(xi −xj)⊤Σ−1(xi −xj),\n(2)\nwhere Σ ∈Rd×d is the covariance matrix of data in the\ntwo sets X1 and X2.\nIf the points xi and xj are the means of the sets X1 and X2,\nrespectively, as the representatives of the sets, this Maha-\nlanobis distance is a good measure of distance of the sets\n(McLachlan, 1999):\n∥µ1 −µ2∥Σ :=\nq\n(µ1 −µ2)⊤Σ−1(µ1 −µ2),\n(3)\nFigure 1. An example for comparison of the Euclidean and Ma-\nhalanobis distances.\nwhere µ1 and µ2 are the means of the sets X1 and X2,\nrespectively.\nLet X1 := {x1,i}n1\ni=1 and X2 := {x2,i}n2\ni=1. The unbiased\nsample covariance matrices of these two sets are:\nΣ1 :=\n1\nn1 −1\nn1\nX\ni=1\n(x1,i −µ1)(x1,i −µ1)⊤,\nand Σ2 similarly. The covariance matrix Σ can be an un-\nbiased sample covariance matrix (McLachlan, 1999):\nΣ :=\n1\nn1 + n2 −2\n\u0010\n(n1 −1)Σ1 + (n2 −1)Σ2\n\u0011\n.\nThe Mahalanobis distance can also be deﬁned between a\npoint x and a cloud or set of points X (De Maesschalck\net al., 2000). Let µ and Σ be the mean and the (sample)\ncovariance matrix of the set X. The Mahalanobis distance\nof x and X is:\n∥x −µ∥Σ :=\nq\n(x −µ)⊤Σ−1(x −µ).\n(4)\nRemark 1 (Justiﬁcation of the Mahalanobis distance\n(De Maesschalck et al., 2000)). Consider two clouds of\ndata, X1 and X2, depicted in Fig. 1. We want to compute\nthe distance of a point x from these two data clouds to see\nwhich cloud this point is closer to. The Euclidean distance\nignores the scatter/variance of clouds and only measures\nthe distances of the point from the means of clouds. Hence,\nin this example, it says that x belongs to X1 because it is\ncloser to the mean of X1 compared to X2. However, the\nMahalanobis distance takes the variance of clouds into ac-\ncount and says that x belongs to X2 because it is closer\nto its scatter compared to X1. Visually, human also says x\nbelongs to X2; hence, the Mahalanobis distance has per-\nformed better than the Euclidean distance by considering\nthe variances of data.\n2.3. Generalized Mahalanobis Distance\nDeﬁnition 3 (Generalized Mahalanobis distance). In Ma-\nhalanobis distance, i.e. Eq. (2), the covariance matrix\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n3\nΣ and its inverse Σ−1 are positive semi-deﬁnite. We can\nreplace Σ−1 with a positive semi-deﬁnite weight matrix\nW ⪰0 in the squared Mahalanobis distance. We name\nthis distance a generalized Mahalanobis distance:\n∥xi −xj∥W :=\nq\n(xi −xj)⊤W (xi −xj).\n∴\n∥xi −xj∥2\nW := (xi −xj)⊤W (xi −xj).\n(5)\nWe deﬁne the generalized Mahalanobis norm as:\n∥x∥W :=\n√\nx⊤W x.\n(6)\nLemma 1 (Triangle inequality of norm). Let ∥.∥be a norm.\nUsing the Cauchy-Schwarz inequality, it satisﬁes the trian-\ngle inequality:\n∥xi + xj∥≤∥xi∥+ ∥xj∥.\n(7)\nProof.\n∥xi + xj∥2 = (xi + xj)⊤(xi + xj)\n= ∥xi∥2 + ∥xj∥2 + 2x⊤\ni xj\n(a)\n≤∥xi∥2 + ∥xj∥2 + 2∥xi∥∥xj∥\n= (∥xi∥+ ∥xj∥)2,\nwhere (a) is because of the Cauchy-Schwarz inequality,\ni.e., x⊤\ni xj ≤∥xi∥∥xj∥. Taking second root from the sides\ngives Eq. (7). Q.E.D.\nProposition 1. The generalized Mahalanobis distance is a\nvalid distance metric.\nProof. We show that the characteristics in Deﬁnition 1 are\nsatisﬁed:\n• As W ⪰0, Eq. (5) is non-negative.\n• identity: if ∥xi −xj∥W = 0, according to Eq. (5),\nwe have xi −xj = 0 =⇒xi = xj. If xi = xj, we\nhave ∥xi −xj∥W = 0 according to Eq. (5).\n• symmetry:\n∥xi −xj∥W\n=\np\n(xi −xj)⊤W (xi −xj)\n=\np\n(xj −xi)⊤W (xj −xi) = ∥xj −xi∥W .\n• triangle inequality: ∥xi −xj∥W = ∥xi −xk + xk −\nxj∥W\n(7)\n≤∥xi −xk∥W + ∥xk −xj∥W .\nRemark 2. It is noteworthy that W ⪰0 is required so\nthat the generalized Mahalanobis distance is convex and\nsatisﬁes the triangle inequality.\nRemark 3. The weight matrix W in Eq. (5) weights the\ndimensions and determines some correlation between di-\nmensions of data points. In other words, it changes the\nspace in a way that the scatters of clouds are considered.\nRemark 4. The Euclidean distance is a special case of\nthe Mahalanobis distance where the weight matrix is the\nidentity matrix, i.e., W = I (cf. Eqs. (1) and (5)). In other\nwords, the Euclidean distance does not change the space\nfor computing the distance.\nProposition 2 (Projection in metric learning). Consider\nthe eigenvalue decomposition of the weight matrix W in\nthe generalized Mahalanobis distance with V and Λ as the\nmatrix of eigenvectors and the diagonal matrix of eigenval-\nues of the weight, respectively. Let U := V Λ(1/2). The\ngeneralized Mahalanobis distance can be seen as the Eu-\nclidean distance after applying a linear projection onto the\ncolumn space of U:\n∥xi −xj∥2\nW = (U ⊤xi −U ⊤xj)⊤(U ⊤xi −U ⊤xj)\n= ∥U ⊤xi −U ⊤xj∥2\n2.\n(8)\nIf U ∈Rd×p with p ≤d, the column space of the projec-\ntion matrix U is a p-dimensional subspace.\nProof. By the eigenvalue decomposition of W , we have:\nW = V ΛV ⊤(a)\n= V Λ(1/2)Λ(1/2)V ⊤(b)\n= UU ⊤,\n(9)\nwhere (a) is because W is positive semi-deﬁnite so all its\neigenvalues are non-negative and can be written as multi-\nplication of its second roots. Also, (b) is because we deﬁne\nU := V Λ(1/2). Substituting Eq. (9) in Eq. (5) gives:\n∥xi −xj∥2\nW = (xi −xj)⊤UU ⊤(xi −xj)\n= (U ⊤xi −U ⊤xj)⊤(U ⊤xi −U ⊤xj)\n= ∥U ⊤xi −U ⊤xj∥2\n2.\nQ.E.D. It is noteworthy that Eq. (9) can also be obtained\nusing singular value decomposition rather than eigenvalue\ndecomposition. In that case, the matrices of right and left\nsingular vectors are equal because of symmetry of W .\n2.4. The Main Idea of Metric Learning\nConsider a d-dimensional dataset {xi}n\ni=1 ⊂Rd of size n.\nAssume some data points are similar in some sense. For\nexample, they have similar pattern or the same characteris-\ntics. Hence, we have a set of similar pair points, denotes by\nS. In contrast, we can have dissimilar points which are dif-\nferent in pattern or characteristics. Let the set of dissimilar\npair points be denoted by D. In summary:\n(xi, xj) ∈S if xi and xj are similar,\n(xi, xj) ∈D if xi and xj are dissimilar.\n(10)\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n4\nFigure 2. Visualizing metric learning in 2D: (a) the contour of Eu-\nclidean distance which does not properly discriminate classes, and\n(b) the contour of Euclidean distance which is better in discrimi-\nnation of classes.\nThe measure of similarity and dissimilarity can be belong-\ning to the same or different classes, if class labels are avail-\nable for dataset. In this case, we have:\n(xi, xj) ∈S if xi and xj are in the same class,\n(xi, xj) ∈D if xi and xj are in different classes.\n(11)\nIn metric learning, we learn the weight matrix so that the\ndistances of similar points become smaller and the dis-\ntances of dissimilar points become larger. In this way, the\nvariance of similar and dissimilar points get smaller and\nlarger, respectively. A 2D visualization of metric learning\nis depicted in Fig. 2. If the class labels are available, met-\nric learning tries to make the intra-class and inter-class vari-\nances smaller and larger, respectively. This is the same idea\nas the idea of Fisher Discriminant Analysis (FDA) (Fisher,\n1936; Ghojogh et al., 2019b).\n3. Spectral Metric Learning\n3.1. Spectral Methods Using Scatters\n3.1.1. THE FIRST SPECTRAL METHOD\nThe ﬁrst metric learning method was proposed in (Xing\net al., 2002). In this method, we minimize the distances\nof the similar points by the weight matrix W where this\nmatrix is positive semi-deﬁnite:\nminimize\nW\nX\n(xi,xj)∈S\n∥xi −xj∥2\nW\nsubject to\nW ⪰0.\nHowever, the solution of this optimization problem is triv-\nial, i.e., W = 0. Hence, we add a constraint on the dis-\nsimilar points to have distances larger than some positive\namount:\nminimize\nW\nX\n(xi,xj)∈S\n∥xi −xj∥2\nW\nsubject to\nX\n(xi,xj)∈D\n∥xi −xj∥W ≥α,\nW ⪰0.\n(12)\nwhere α > 0 is some positive number such as α = 1.\nLemma 2 ((Xing et al., 2002)). If the constraint in Eq. (12)\nis squared, i.e., P\n(xi,xj)∈D ∥xi −xj∥2\nW ≥α, the solution\nof optimization will have rank 1. Hence, we are using a\nnon-squared constraint in the optimization problem.\nProof. If the constraint in Eq. (12) is squared, the problem\nis equivalent to (see (Ghojogh et al., 2019b, Appendix B)\nfor proof):\nmaximize\nW\nP\n(xi,xj)∈D ∥xi −xj∥2\nW\nP\n(xi,xj)∈S ∥xi −xj∥2\nW\n,\nwhich is a Rayleigh-Ritz quotient (Ghojogh et al., 2019a).\nWe can restate ∥xi −xj∥2\nW as:\nX\n(xi,xj)∈S\n∥xi −xj∥2\nW = tr(W ΣS),\nX\n(xi,xj)∈D\n∥xi −xj∥2\nW = tr(W ΣD),\n(13)\nwhere tr(.) denotes the trace of matrix and:\nΣS :=\nX\n(xi,xj)∈S\n(xi −xj)(xi −xj)⊤,\nΣD :=\nX\n(xi,xj)∈D\n(xi −xj)(xi −xj)⊤.\n(14)\nHence, we have:\nP\n(xi,xj)∈D ∥xi −xj∥2\nW\nP\n(xi,xj)∈S ∥xi −xj∥2\nW\n= tr(W ΣD)\ntr(W ΣS)\n(9)\n= tr(UU ⊤ΣD)\ntr(UU ⊤ΣS)\n(a)\n= tr(U ⊤ΣDU)\ntr(U ⊤ΣSU)\n=\nPd\ni=1 u⊤ΣDu\nPd\ni=1 u⊤ΣSu\n,\nwhere (a) is because of the cyclic property of trace and (b)\nis because U = [u1, . . . , ud]. Maximizing this Rayleigh-\nRitz quotient results in the following generalized eigen-\nvalue problem (Ghojogh et al., 2019a):\nΣDu1 = λΣSu1,\nwhere u1 is the eigenvector with largest eigenvalue and the\nother eigenvectors u2, . . . , ud are zero vectors. Q.E.D.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n5\nThe Eq. (12) can be restated as a maximization problem:\nmaximize\nW\nX\n(xi,xj)∈D\n∥xi −xj∥W\nsubject to\nX\n(xi,xj)∈S\n∥xi −xj∥2\nW ≤α,\nW ⪰0.\n(15)\nWe can solve this problem using projected gradient method\n(Ghojogh et al., 2021c) where a step of gradient ascent is\nfollowed by projection onto the two constraint sets:\nW := W + η ∂\n∂W\n\u0010\nX\n(xi,xj)∈D\n∥xi −xj∥W\n\u0011\n,\nW := arg min\nQ\n\u0010\n∥Q −W ∥2\nF s.t.\nX\n(xi,xj)∈S\n∥xi −xj∥2\nQ ≤α\n\u0011\n,\nW := V diag(max(λ1, 0), . . . , max(λd, 0)) V ⊤,\nwhere η\n> 0 is the learning rate and V and Λ =\ndiag(λ1, . . . , λd) are the eigenvectors and eigenvalues of\nW , respectively (see Eq. (9)).\n3.1.2. FORMULATING AS SEMIDEFINITE\nPROGRAMMING\nAnother metric learning method is (Ghodsi et al., 2007)\nwhich minimizes the distances of similar points and maxi-\nmizes the distances of dissimilar points. For this, we min-\nimize the distances of similar points and the negation of\ndistances of dissimilar points. The weight matrix should\nbe positive semi-deﬁnite to satisfy the triangle inequality\nand convexity. The trace of weight matrix is also set to\na constant to eliminate the trivial solution W = 0. The\noptimization problem is:\nminimize\nW\n1\n|S|\nX\n(xi,xj)∈S\n∥xi −xj∥2\nW\n−1\n|D|\nX\n(xi,xj)∈D\n∥xi −xj∥2\nW\nsubject to\nW ⪰0,\ntr(W ) = 1,\n(16)\nwhere |.| denotes the cardinality of set.\nLemma 3 ((Ghodsi et al., 2007)). The objective function\ncan be simpliﬁed as:\n1\n|S|\nX\n(xi,xj)∈S\n∥xi −xj∥2\nW −1\n|D|\nX\n(xi,xj)∈D\n∥xi −xj∥W\n= vec(W )⊤\u0010 1\n|S|\nX\n(xi,xj)∈S\nvec\n\u0000(xi −xj)(xi −xj)⊤\u0001\n−1\n|D|\nX\n(xi,xj)∈D\nvec\n\u0000(xi −xj)(xi −xj)⊤\u0001\u0011\n,\n(17)\nwhere vec(.) vectorizes the matrix to a vector (Ghojogh\net al., 2021c).\nProof. See (Ghodsi et al., 2007, Section 2.1) for proof.\nAccording to Lemma 3, Eq. (16) is a Semideﬁnite Pro-\ngramming (SDP) problem. It can be solved iteratively us-\ning the interior-point method (Ghojogh et al., 2021c).\n3.1.3. RELEVANT TO FISHER DISCRIMINANT\nANALYSIS\nAnother metric learning method is (Alipanahi et al., 2008)\nwhich has two approaches, introduced in the following.\nThe relation of metric learning with Fisher discriminant\nanalysis (Fisher, 1936; Ghojogh et al., 2019b) was dis-\ncussed in this paper (Alipanahi et al., 2008).\n– Approach 1:\nAs W ⪰0, the weight matrix can be\ndecomposed as in Eq. (9), i.e., W = UU ⊤. Hence, we\nhave:\n∥xi −xj∥2\nW\n(5)\n= (xi −xj)⊤W (xi −xj)\n(a)\n= tr\n\u0000(xi −xj)⊤W (xi −xj)\n\u0001\n(9)\n= tr\n\u0000(xi −xj)⊤UU ⊤(xi −xj)\n\u0001\n(b)\n= tr\n\u0000U ⊤(xi −xj)(xi −xj)⊤U\n\u0001\n,\n(18)\nwhere (a) is because a scalar is equal to its trace and (b) is\nbecause of the cyclic property of trace. We can substitute\nEq. (18) in Eq. (16) to obtain an optimization problem:\nminimize\nU\n1\n|S|\nX\n(xi,xj)∈S\ntr\n\u0000U ⊤(xi −xj)(xi −xj)⊤U\n\u0001\n−1\n|D|\nX\n(xi,xj)∈D\ntr\n\u0000U ⊤(xi −xj)(xi −xj)⊤U\n\u0001\nsubject to\ntr(UU ⊤) = 1,\n(19)\nwhose objective variable is U. Note that the constraint\nW ⪰0 is implicitly satisﬁed because of the decomposi-\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n6\ntion W = UU ⊤. We deﬁne:\nΣ′\nS := 1\n|S|\nX\n(xi,xj)∈S\n(xi −xj)(xi −xj)⊤(13)\n=\n1\n|S|ΣS,\nΣ′\nD :=\n1\n|D|\nX\n(xi,xj)∈D\n(xi −xj)(xi −xj)⊤(13)\n=\n1\n|D|ΣD.\n(20)\nHence, Eq. (19) can be restated as:\nminimize\nU\ntr(U ⊤(Σ′\nS −Σ′\nD)U)\nsubject to\ntr(UU ⊤) = 1,\n(21)\nwhose Lagrangian is (Ghojogh et al., 2021c):\nL = tr(U ⊤(Σ′\nS −Σ′\nD)U) −λ(tr(UU ⊤) −1).\nTaking derivative of the Lagrangian and setting it to zero\ngives:\n∂L\n∂U = 2(Σ′\nS −Σ′\nD)U −2λU\nset= 0\n=⇒(Σ′\nS −Σ′\nD)U = λU,\n(22)\nwhich is the eigenvalue problem for (Σ′\nS −Σ′\nD) (Ghojogh\net al., 2019a). Hence, U is the eigenvector of (Σ′\nS −Σ′\nD)\nwith the smallest eigenvalue because Eq. (19) is a mini-\nmization problem.\n– Approach 2: We can change the constraint in Eq. (21) to\nhave orthogonal projection matrix, i.e., U ⊤U = I. Rather,\nwe can make the rotation of the projection matrix by the\nmatrix Σ′\nS be orthogonal, i.e., U ⊤Σ′\nSU = I. Hence, the\noptimization problem becomes:\nminimize\nU\ntr(U ⊤(Σ′\nS −Σ′\nD)U)\nsubject to\nU ⊤Σ′\nS U = I,\n(23)\nwhose Lagrangian is (Ghojogh et al., 2021c):\nL = tr(U ⊤(Σ′\nS −Σ′\nD)U) −tr(Λ⊤(U ⊤Σ′\nS U −I)).\n∂L\n∂U = 2(Σ′\nS −Σ′\nD)U −2Σ′\nS UΛ\nset= 0\n=⇒(Σ′\nS −Σ′\nD)U = Σ′\nS UΛ,\n(24)\nwhich is the generalized eigenvalue problem for (Σ′\nS −\nΣ′\nD, Σ′\nS) (Ghojogh et al., 2019a). Hence, U is a matrix\nwhose columns are the eigenvectors sorted from the small-\nest to largest eigenvalues.\nThe optimization problem is similar to the optimization of\nFisher discriminant analysis (FDA) (Fisher, 1936; Ghojogh\net al., 2019b) where Σ′\nS and Σ′\nD are replaced with the intra-\nclass and inter-class covariance matrices of data, respec-\ntively. This shows the relation of this method with FDA.\nIt makes sense because both metric learning and FDA have\nthe same goal and that is decreasing and increasing the vari-\nances of similar and dissimilar points, respectively.\n3.1.4. RELEVANT COMPONENT ANALYSIS (RCA)\nSuppose the n data points can be divided into c clusters,\nor so-called chunklets. If class labels are available, classes\nare the chunklets. If Xl denotes the data of the l-th cluster\nand µl is the mean of Xl, the summation of intra-cluster\nscatters is:\nRd×d ∋Sw := 1\nn\nc\nX\nl=1\nX\nxi∈Xl\n(xi −µl)(xi −µl)⊤. (25)\nRelevant Component Analysis (RCA) (Shental et al., 2002)\nis a metric learning method. In this method, we ﬁrst apply\nPrincipal Component Analysis (PCA) (Ghojogh & Crow-\nley, 2019) on data using the total scatter of data. Let the\nprojection matrix of PCA be denoted by U. After projec-\ntion onto the PCA subspace, the summation of intra-cluster\nscatters is bSw := U ⊤SwU because of the quadratic char-\nacteristic of covariance. RCA uses bSw as the covariance\nmatrix in the Mahalanobis distance, i.e., Eq. (2). Accord-\ning to Eq. (8), the subspace of RDA is obtained by the\neigenvalue (or singular value) decomposition of bS\n−1\nw\n(see\nEq. (9)).\n3.1.5. DISCRIMINATIVE COMPONENT ANALYSIS\n(DCA)\nDiscriminative Component Analysis (DCA) (Hoi et al.,\n2006) is another spectral metric learning method based on\nscatters of clusters/classes. Consider the c clusters, chun-\nklets, or classes of data. The intra-class scatter is as in Eq.\n(25). The inter-class scatter is:\nRd×d ∋Sb := 1\nn\nc\nX\nl=1\nc\nX\nj=1\n(µl −µj)(µl −µj)⊤, or\nRd×d ∋Sb := 1\nn\nc\nX\nl=1\n(µl −µ)(µl −µ)⊤,\n(26)\nwhere µl is the mean of the l-th class and µ is the total\nmean of data. According to Proposition 2, metric learning\ncan be seen as Euclidean distance after projection onto the\ncolumn space of a projection matrix U where W = UU ⊤.\nSimilar to Fisher discriminant analysis (Fisher, 1936; Gho-\njogh et al., 2019b), DCA maximizes the inter-class vari-\nance and minimizes the intra-class variance after projec-\ntion. Hence, its optimization is:\nmaximize\nU\ntr(U ⊤SbU)\ntr(U ⊤SwU)\n,\n(27)\nwhich is a generalized Rayleigh-Ritz quotient.\nThe so-\nlution U to this optimization problem is the generalized\neigenvalue problem (Sb, Sw) (Ghojogh et al., 2019a). Ac-\ncording to Eq. (9), we can set the weight matrix of the\ngeneralized Mahalanobis distance as W = UU ⊤where\nU is the matrix of eigenvectors.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n7\n3.1.6. HIGH DIMENSIONAL DISCRIMINATIVE\nCOMPONENT ANALYSIS\nAnother spectral method for metric learning is (Xiang et al.,\n2008) which minimizes and maximizes the intra-class and\ninter-class variances, respectively, by the the same opti-\nmization problem as Eq.\n(27) with an additional con-\nstraint on the orthogonality of the projection matrix, i.e.,\nU ⊤U = I. This problem can be restated by posing penalty\non the denominator:\nmaximize\nU\ntr(U ⊤(Sb −λSw)U)\nsubject to\nU ⊤U = I,\n(28)\nwhere λ > 0 is the regularization parameter. The solution\nto this problem is the eigenvalue problem for Sb −λSw.\nThe eigenvectors are the columns of U and the weight ma-\ntrix of the generalized Mahalanobis is obtained using Eq.\n(9).\nIf the dimensionality of data is large, computing the eigen-\nvectors of (Sb −λSw) ∈Rd×d is very time-consuming.\nAccording to (Xiang et al., 2008, Theorem 3), the op-\ntimization problem (28) can be solved in the orthogonal\ncomplement space of the null space of Sb + Sw without\nloss of any information (see (Xiang et al., 2008, Appendix\nA) for proof). Hence, if d ≫1, we ﬁnd U as follows.\nLet X := [x1, . . . , xn] ∈Rd×n be the matrix of data.\nLet Aw and Ab be the adjacency matrices for the sets S\nand D, respectively. For example, if (xi, xj) ∈S, then\nAw(i, j) = 1; otherwise, Aw(i, j) = 0. If Lw and Lb\nare the Laplacian matrices of Aw and Ab, respectively,\nwe have Sw = 0.5XLwX⊤and Sb = 0.5XLbX⊤(see\n(Belkin & Niyogi, 2002; Ghojogh et al., 2021d) for proof).\nWe have tr(Sw + Sb) = tr(X(0.5Lw + 0.5Lb)X⊤) =\ntr(X⊤X(0.5Lw + 0.5Lb)) because of the cyclic property\nof trace. If the rank of L := X⊤X(0.5Lw + 0.5Lb) ∈\nRn×n is r ≤n, it has r non-zero eigenvalues which we\ncompute its corresponding eigenvectors. We stack these\neigenvectors to have V ∈Rd×r. The projected intra-class\nand inter-class variances after projection onto the column\nspace of V are S′\nw := V ⊤SwV and S′\nb := V ⊤SbV , re-\nspectively. Then, we use S′\nw and S′\nb in Eq. (28) and the\nweight matrix of the generalized Mahalanobis is obtained\nusing Eq. (9).\n3.1.7. REGULARIZATION BY LOCALLY LINEAR\nEMBEDDING\nThe spectral metric learning methods using scatters can be\nmodeled as maximization of the following Rayleigh–Ritz\nquotient (Baghshah & Shouraki, 2009):\nmaximize\nU\nP\n(xi,xj)∈S ∥xi −xj∥W\nP\n(xi,xj)∈D ∥xi −xj∥W + λΩ(U),\nsubject to\nU ⊤U = I,\n(29)\nwhere W = UU ⊤(see Eq. (9)), λ > 0 is the regular-\nization parameter, and Ω(U) is a penalty or regularization\nterm on the projection matrix U. This optimization max-\nimizes and minimizes the distances of the similar and dis-\nsimilar points, respectively. According to Section 3.1.3,\nEq. (29) can be restated as:\nmaximize\nU\ntr(U ⊤SbU)\ntr(U ⊤SwU) + λΩ(U)\n,\nsubject to\nU ⊤U = I.\n(30)\nAs was discussed in Proposition 2, metric learning can be\nseen as projection onto a subspace. The regularization term\ncan be linear reconstruction of every projected point by its\nk Nearest Neighbors (kNN) using the same reconstruction\nweights as before projection (Baghshah & Shouraki, 2009).\nThe weights for linear reconstruction in the input space can\nbe found as in locally linear embedding (Roweis & Saul,\n2000; Ghojogh et al., 2020a). If sij denotes the weight of\nxj in reconstruction of xi and N(xi) is the set of kNN for\nxi, we have:\nminimize\nsij\nn\nX\ni=1\n\r\r\rxi −\nX\nxj∈N(xi)\nsijxj\n\r\r\r\n2\n2,\nsubject to\nX\nxj∈N(xi)\nsij = 1.\nThe solution of this optimization is (Ghojogh et al., 2020a):\ns∗\nij =\nG−1\ni 1\n1⊤G−1\ni 1,\nwhere Gi := (xi1⊤−Xi)⊤(xi1⊤−Xi) in which\nXi ∈Rd×k denotes the stack of kNN for xi. We de-\nﬁne S∗:= [s∗\nij] ∈Rn×n. The regularization term can be\nreconstruction in the subspace using the same reconstruc-\ntion weights as in the input space (Baghshah & Shouraki,\n2009):\nΩ(U) :=\nn\nX\ni=1\n\r\r\rU ⊤x −\nX\nxj∈N(xi)\ns∗\nijU ⊤xj\n\r\r\r\n2\n2\n= tr(U ⊤XEX⊤U),\n(31)\nwhere X = [x1, . . . , xn] ∈Rd×n and Rn×n ∋E :=\n(I −S∗)⊤(I −S∗). Putting Eq. (31) in Eq. (30) gives:\nmaximize\nU\ntr(U ⊤SbU)\ntr\n\u0000U ⊤(Sw + λXEX⊤)U\n\u0001,\nsubject to\nU ⊤U = I.\n(32)\nThe solution to this optimization problem is the gener-\nalized eigenvalue problem (Sb, Sw + λXEX⊤) where\nU has the eigenvectors as its columns (Ghojogh et al.,\n2019a). According to Eq. (9), the weight matrix of met-\nric is W = UU ⊤.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n8\n3.1.8. FISHER-HSIC MULTI-VIEW METRIC LEARNING\n(FISH-MML)\nFisher-HSIC Multi-view Metric Learning (FISH-MML)\n(Zhang et al., 2018) is a metric learning method for multi-\nview data.\nIn multi-view data, we have different types\nof features for every data point. For example, an image\ndataset, which has a descriptive caption for every image, is\nmulti-view. Let X(r) := {x(r)\ni }n\ni=1 be the features of data\npoints in the r-th view, c be the number of classes/clusters,\nand v be the number of views. According to Proposition\n2, metric learning is the Euclidean distance after projection\nwith U. The inter-class scatter of data, in the r-th view, is\ndenoted by S(r)\nb\nand calculated using Eqs. (26). The total\nscatter of data, in the r-th view, is denoted by S(r)\nt\nand is\nthe covariance of data in that view.\nInspired by Fisher discriminant analysis (Fisher, 1936;\nGhojogh et al., 2019b), we maximize the inter-class vari-\nances of projected data, Pv\nr=1 tr(U ⊤S(r)\nb U), to dis-\ncriminate the classes after projection.\nAlso, inspired\nby principal component analysis (Ghojogh & Crowley,\n2019), we maximize the total scatter of projected data,\nPv\nr=1 tr(U ⊤S(r)\nt U), for expressiveness. Moreover, we\nmaximize the dependence of the projected data in all views\nbecause various views of a point should be related. A mea-\nsure of dependence between two random variables X and\nY is the Hilbert-Schmidt Independence Criterion (HSIC)\n(Gretton et al., 2005) whose empirical estimation is:\nHSIC(X, Y ) =\n1\n(n −1)2 tr(KxHKyH),\n(33)\nwhere Kx and Ky are kernel matrices over X and Y vari-\nables, respectively, and H := I −(1/n)11⊤is the cen-\ntering matrix. The HSIC between projection of two views\nX(r) and X(w) is:\nHSIC(U ⊤X(r), U ⊤X(w))\n(33)\n∝tr(K(r)HK(w)H)\n(a)\n= tr(X(r)⊤UU ⊤X(r)HK(w)H)\n(b)\n= tr(U ⊤X(r)HK(w)HX(r)⊤U)\nwhere (a) is because we use the linear kernel for U ⊤X(r),\ni.e., K(r) := (U ⊤X(r))⊤U ⊤X(r) and (b) is because of\nthe cyclic property of trace.\nIn summary, we maximize the summation of inter-class\nscatter, total scatter, and the dependence of views, which\nis:\nv\nX\nr=1\n\u0000tr(U ⊤S(r)\nb U) + λ1tr(U ⊤S(r)\nt U)\n+ λ2tr(U ⊤X(r)HK(w)HX(r)⊤U)\n\u0001\n=\nv\nX\nr=1\ntr\n\u0000U ⊤(S(r)\nb\n+ λ1S(r)\nt\n+ λ2X(r)HK(w)HX(r)⊤)U\n\u0001\n,\nwhere λ1, λ2 > 0 are the regularization parameters. The\noptimization problem is:\nmaximize\nU\nv\nX\nr=1\ntr\n\u0000U ⊤(S(r)\nb\n+ λ1S(r)\nt\n+ λ2X(r)HK(w)HX(r)⊤)U\n\u0001\nsubject to\nU ⊤U = I,\n(34)\nwhose solution is the eigenvalue problem for S(r)\nb\n+\nλ1S(r)\nt\n+λ2X(r)HK(w)HX(r)⊤where U has the eigen-\nvectors as its columns (Ghojogh et al., 2019a).\n3.2. Spectral Methods Using Hinge Loss\n3.2.1. LARGE-MARGIN METRIC LEARNING\nk-Nearest Neighbors (kNN) classiﬁcation is highly im-\npacted by the metric used for measuring distances between\npoints.\nHence, we can use metric learning for improv-\ning the performance of kNN classiﬁcation (Weinberger\net al., 2006; Weinberger & Saul, 2009). Let yij = 1 if\n(xi, xj) ∈S and yij = 0 if (xi, xj) ∈D. Moreover, we\nconsider kNN for similar points where we ﬁnd the nearest\nneighbors of every point among the similar points to that\npoint. Let ηij = 1 if (xi, xj) ∈S and xj is among kNN\nof xi. Otherwise, ηij = 0. The optimization problem for\nﬁnding the best weigh matrix in the metric can be (Wein-\nberger et al., 2006; Weinberger & Saul, 2009):\nminimize\nW\nn\nX\ni=1\nn\nX\nj=1\nηij∥xi −xj∥2\nW\n+ λ\nn\nX\ni=1\nn\nX\nj=1\nn\nX\nl=1\nηij(1 −yil)\nh\n1\n+ ∥xi −xj∥2\nW −∥xi −xl∥2\nW\ni\n+,\nsubject to\nW ⪰0,\n(35)\nwhere λ > 0 is the regularization parameter, and [.]+ :=\nmax(., 0) is the standard Hinge loss.\nThe ﬁrst term in Eq. (35) pushes the similar neighbors\nclose to each other. The second term in this equation is the\ntriplet loss (Schroff et al., 2015) which pushes the similar\nneighbors to each other and pulls the dissimilar points away\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n9\nfrom one another. This is because minimizing ∥xi −xj∥2\nW\nfor ηij = 1 decreases the distances of similar neighbors.\nMoreover, minimizing −∥xi −xl∥2\nW for 1 −yil = 1 (i.e.,\nyil = 0) is equivalent to maximizing ∥xi −xl∥2\nW which\nmaximizes the distances of dissimilar points. Minimizing\nthe whole second term forces the distances of dissimilar\npoints to be at least greater that the distances of similar\npoints up to a threshold (or margin) of one. We can change\nthe margin by changing 1 in this term with some other pos-\nitive number. In this sense, this loss is closely related to the\ntriplet loss for neural networks (Schroff et al., 2015) (see\nSection 5.3.5).\nEq. (35) can be restated using slack variables ξijl, ∀i, j, l ∈\n{1, . . . , n}. The Hinge loss in term [1 + ∥xi −xj∥2\nW −\n∥xi −xl∥2\nW ]+ requires to have:\n1 + ∥xi −xj∥2\nW −∥xi −xl∥2\nW ≥0\n=⇒∥xi −xl∥2\nW −∥xi −xj∥2\nW ≤1.\nIf ξijl ≥0, we can have sandwich the term ∥xi −xl∥2\nW −\n∥xi −xj∥2\nW in order to minimize it:\n1 −ξijl ≤∥xi −xl∥2\nW −∥xi −xj∥2\nW ≤1.\nHence, we can replace the term of Hinge loss with the slack\nvariable. Therefore, Eq. (35) can be restated as (Wein-\nberger et al., 2006; Weinberger & Saul, 2009):\nminimize\nW , {ξijl}\nn\nX\ni=1\nn\nX\nj=1\nηij∥xi −xj∥2\nW\n+ λ\nn\nX\ni=1\nn\nX\nj=1\nn\nX\nl=1\nηij(1 −yil) ξijl\nsubject to\n∥xi −xl∥2\nW −∥xi −xj∥2\nW ≥1 −ξijl,\n∀(xi, xj) ∈S, ηij = 1, (xi, xl) ∈D,\nξijl ≥0,\nW ⪰0.\n(36)\nThis optimization problem is a semideﬁnite programming\nwhich can be solved iteratively using interior-point method\n(Ghojogh et al., 2021c).\nThis problem uses triplets of similar and dissimilar points,\ni.e., {xi, xj, xl} where (xi, xj) ∈S, ηij = 1, (xi, xl) ∈\nD. Hence, triplets should be extracted randomly from the\ndataset for this metric learning. Solving semideﬁnite pro-\ngramming is usually slow and time-consuming especially\nfor large datasets. Triplet minimizing can be used for ﬁnd-\ning the best triplets for learning (Poorheravi et al., 2020).\nFor example, the similar and dissimilar points with small-\nest and/or largest distances can be used to limit the number\nof triplets (Sikaroudi et al., 2020a). The reader can also re-\nfer to for Lipschitz analysis in large margin metric learning\n(Dong, 2019).\n3.2.2. IMBALANCED METRIC LEARNING (IML)\nImbalanced Metric Learning (IML) (Gautheron et al.,\n2019) is a spectral metric learning method which handles\nimbalanced classes by further decomposition of the similar\nset S and dissimilar set D. Suppose the dataset is com-\nposed of two classes c0 and c1. Let S0 and S1 denote the\nsimilarity sets for classes c0 and c1, respectively. We de-\nﬁne pairs of points taken randomly from these sets to have\nsimilarity and dissimilarity sets (Gautheron et al., 2019):\nSim0 ⊆S0 × S0,\nSim1 ⊆S1 × S1,\nDis0 ⊆S0 × S1,\nDis1 ⊆S1 × S0.\nThe optimization problem of IML is:\nminimize\nW\nλ\n4|Sim0|\nX\n(xi,xj)∈Sim0\n\u0002\n∥xi −xj∥2\nW −1\n\u0003\n+\n+\nλ\n4|Sim1|\nX\n(xi,xj)∈Sim1\n\u0002\n∥xi −xj∥2\nW −1\n\u0003\n+\n+ 1 −λ\n4|Dis0|\nX\n(xi,xj)∈Dis0\n\u0002\n−∥xi −xj∥2\nW + 1 + m\n\u0003\n+\n+ 1 −λ\n4|Dis1|\nX\n(xi,xj)∈Dis1\n\u0002\n−∥xi −xj∥2\nW + 1 + m\n\u0003\n+\n+ γ∥W −I∥2\nF\nsubject to\nW ⪰0,\n(37)\nwhere |.| denotes the cardinality of set, [.]+ := max(., 0)\nis the standard Hinge loss, m > 0 is the desired margin be-\ntween classes, and λ ∈[0, 1] and γ > 0 are the regulariza-\ntion parameters. This optimization pulls the similar points\nto have distance less than 1 and pushes the dissimilar points\naway to have distance more than m + 1. Also, the regular-\nization term ∥W −I∥2\nF tries to make the weight matrix is\nthe generalized Mahalanobis distance close to identity for\nsimplicity of metric. In this way, the metric becomes close\nto the Euclidean distance, preventing overﬁtting, while sat-\nisfying the desired margins in distances.\n3.3. Locally Linear Metric Adaptation (LLMA)\nAnother method for metric learning is Locally Linear Met-\nric Adaptation (LLMA) (Chang & Yeung, 2004). LLMA\nperforms nonlinear and linear transformations globally and\nlocally, respectively. For every point xl, we consider its k\nnearest (similar) neighbors. The local linear transformation\nfor every point xl is:\nRd ∋yl := xl + Bπi,\n(38)\nwhere B ∈Rd×k is the matrix of biases, Rk ∋πi =\n[πi1, . . . , πik]⊤, and πij := exp(−∥xi −xj∥2\n2/2w2) is a\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n10\nGaussian measure of similarity between xi and xj. The\nvariables B and w are found by optimization.\nIn this method, we minimize the distances between the lin-\nearly transformed similar points while the distances of sim-\nilar points are tried to be preserved after the transformation:\nminimize\n{yi}n\ni=1,B,w,σ\nX\n(yi,yj)∈S\n∥yi −yj∥2\n2\n+ λ\nn\nX\ni=1\nn\nX\nj=1\n(qij −dij)2 exp(−d2\nij\nσ2 ),\n(39)\nwhere λ > 0 is the regularization parameter, σ2\n2 is the\nvariance to be optimized, and dij := ∥xi −xj∥2 and\nqij := ∥yi −yj∥2. This objective function is optimized\niteratively until convergence.\n3.4. Relevant to Support Vector Machine\nInspired\nby\nν-Support\nVector\nMachine\n(ν-SVM)\n(Sch¨olkopf et al., 2000),\nthe weight matrix in the\ngeneralized Mahalanobis distance can be obtained as\n(Tsang et al., 2003):\nminimize\nW ,γ,{ξil}\n1\n2∥W ∥2\n2 + λ1\n|S|\nX\n(xi,xj)∈S\n∥xi −xj∥2\nW\n+ λ2\n\u0010\nνγ + 1\n|D|\nX\n(xi,xl)∈D\nξil\n\u0011\nsubject to\nW ⪰0,\nγ ≥0,\n∥xi −xj∥2\nW −∥xi −xl∥2\nW ≥γ −ξil,\n∀(xi, xj) ∈S, (xi, xl) ∈D,\nξil ≥0,\n∀(xi, xl) ∈D,\n(40)\nwhere λ1, λ2 > 0 are regularization parameters. Using\nKKT conditions and Lagrange multipliers (Ghojogh et al.,\n2021c), the dual optimization problem is (see (Tsang et al.,\n2003) for derivation):\nmaximize\n{αij}\nX\n(xi,xj)∈D\nαij(xi −xj)⊤W (xi −xj)\n−1\n2\nX\n(xi,xj)∈D\nX\n(xk,xl)∈D\nαijαkl((xi −xj)⊤(xk −xl))2\n+ λ1\n|S|\nX\n(xi,xj)∈D\nX\n(xk,xl)∈S\nαij((xi −xj)⊤(xk −xl))2\nsubject to\n1\nλ2\nX\n(xi,xj)∈D\nαij ≥ν,\nαij ∈[0, λ2\n|D|],\n(41)\nwhere {αij} are the dual variables.\nThis problem is a\nquadratic programming problem and can be solved using\noptimization solvers.\n3.5. Relevant to Multidimensional Scaling\nMultidimensional Scaling (MDS) tries to preserve the dis-\ntance after projection onto its subspace (Cox & Cox, 2008;\nGhojogh et al., 2020b). We saw in Proposition 2 that metric\nlearning can be seen as projection onto the column space of\nU where W = UU ⊤. Inspired by MDS, we can learn a\nmetric which preserves the distances between points after\nprojection onto the subspace of metric (Zhang et al., 2003):\nminimize\nW\nn\nX\ni=1\nn\nX\nj=1\n(∥xi −xj∥2\n2 −∥xi −xj∥2\nW )2\nsubject to\nW ⪰0.\n(42)\nIt can be solved using any optimization method (Ghojogh\net al., 2021c).\n3.6. Kernel Spectral Metric Learning\nLet k(xi, xj) := φ(xi)⊤φ(xj) be the kernel function over\ndata points xi and xj, where φ(.) is the pulling function to\nthe Reproducing Kernel Hilbert Space (RKHS) (Ghojogh\net al., 2021e). Let Rn×n ∋K := Φ(X)⊤Φ(X) be the\nkernel matrix of data. In the following, we introduce some\nof the kernel spectral metric learning methods.\n3.6.1. USING EIGENVALUE DECOMPOSITION OF\nKERNEL\nOne of the kernel methods for spectral metric learning is\n(Yeung & Chang, 2007). It has two approaches; we explain\none of its approaches here. The eigenvalue decomposition\nof the kernel matrix is:\nK =\np\nX\nr=1\nβ2\nrαrα⊤\nr\n(a)\n=\np\nX\nr=1\nβ2\nrKr\n(43)\nwhere p is the rank of kernel matrix, β2\nr is the non-negative\nr-th eigenvalue (because K ⪰0), αr ∈Rn is the r-th\neigenvector, and (a) is because we deﬁne Kr := αrα⊤\nr .\nWe can consider {β2\nr}p\nr=1 as learnable parameters and not\nthe eigenvalues. Hence, we learn {β2\nr}p\nr=1 for the sake of\nmetric learning. The distance metric of pulled data points\nto RKHS is (Sch¨olkopf, 2001; Ghojogh et al., 2021e):\n∥φ(xi)−φ(xj)∥2\n2\n= k(xi, xi) + k(xj, xj) −2k(xi, xj).\n(44)\nIn metric learning, we want to make the distances of sim-\nilar points small; hence the objective to be minimized is:\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n11\nHence, we have:\nX\n(xi,xj)∈S\n∥φ(xi) −φ(xj)∥2\n2\n=\nX\n(xi,xj)∈S\nk(xi, xi) + k(xj, xj) −2k(xi, xj)\n(43)\n=\np\nX\nr=1\nβ2\nr\nX\n(xi,xj)∈S\nkr(xi, xi) + kr(xj, xj)\n−2kr(xi, xj)\n(a)\n=\np\nX\nr=1\nβ2\nr\nX\n(xi,xj)∈S\n(ei −ej)⊤Kr(ei −ej)\n(b)\n=\np\nX\nr=1\nβ2\nrfr\n(c)\n= β⊤DSβ,\nwhere (a) is because ei is the vector whose i-th element\nis one and other elements are zero, (b) is because we\ndeﬁne fr := P\n(xi,xj)∈S(ei −ej)⊤Kr(ei −ej), and\n(c) is because we deﬁne DS := diag([f1, . . . , fp]⊤) and\nβ := [β1, . . . , βp]⊤. By adding a constraint on the sum-\nmation of {β2\nr}p\nr=1, the optimization problem for metric\nlearning is:\nminimize\nβ\nβ⊤DSβ\nsubject to\n1⊤β = 1.\n(45)\nThis optimization is similar to the form of one of the opti-\nmization problems in locally linear embedding (Roweis &\nSaul, 2000; Ghojogh et al., 2020a). The Lagrangian for this\nproblem is (Ghojogh et al., 2021c):\nL = β⊤DSβ −λ(1⊤β −1),\nwhere λ is the dual variable. Taking derivative of the La-\ngrangian w.r.t. the variables and setting to zero gives:\n∂L\n∂β = 2DSβ −λ1\nset= 0 =⇒β = λ\n2 D−1\nS 1,\n∂L\n∂λ = 1⊤β −1\nset= 0 =⇒1⊤β = 1,\n=⇒λ\n2 1⊤D−1\nS 1 = 1 =⇒λ =\n2\n1⊤D−1\nS 1\n=⇒β =\nD−1\nS 1\n1⊤D−1\nS 1.\nHence, the optimal β is obtained for metric learning in the\nRKHS where the distances of similar points is smaller than\nin the input Euclidean space.\n3.6.2. REGULARIZATION BY LOCALLY LINEAR\nEMBEDDING\nThe method (Baghshah & Shouraki, 2009), which was in-\ntroduced in Section 3.1.7, can be kernelized. Recall that\nthis method used locally linear embedding for regulariza-\ntion.\nAccording to the representation theory (Ghojogh\net al., 2021e), the solution in the RKHS can be represented\nas a linear combination of all pulled data points to RKHS:\nΦ(U) = Φ(X)T ,\n(46)\nwhere X = [x1, . . . , xn] and T ∈Rn×p (p is the dimen-\nsionality of subspace) is the coefﬁcients.\nWe deﬁne the similarity and dissimilarity adjacency matri-\nces as:\nAS(i, j) :=\n\u001a 1\nif (xi, xj) ∈S,\n0\notherwise.\nAD(i, j) :=\n\u001a 1\nif (xi, xj) ∈D,\n0\notherwise.\n(47)\nLet Lw and Lb denote the Laplacian matrices (Ghojogh\net al., 2021d) of these adjacency matrices:\nLw := DS −AS(i, j),\nLb := DD −AD(i, j),\nwhere DS(i, i)\n:=\nPn\nj=1 AS(i, j) and DD(i, i)\n:=\nPn\nj=1 AD(i, j) are diagonal matrices. The terms in the\nobjective of Eq. (32) can be restated using Laplacian of\nadjacency matrices rather than the scatters:\nmaximize\nU\ntr(U ⊤LbU)\ntr\n\u0000U ⊤(Lw + λXEX⊤)U\n\u0001,\nsubject to\nU ⊤U = I.\n(48)\nAccording to the representation theory, the pulled Lapla-\ncian matrices to RKHS are Φ(Lb) = Φ(X)LbΦ(X)⊤\nand Φ(Lw) = Φ(X)LwΦ(X)⊤. Hence, the numerator\nof Eq. (32) in RKHS becomes:\ntr(Φ(U)⊤Φ(X)LbΦ(X)⊤Φ(U))\n= tr\n\u0000T ⊤Φ(X)⊤Φ(X)LbΦ(X)⊤Φ(X)T\n\u0001\n(a)\n= tr\n\u0000T ⊤KxLbKxT\n\u0001\n,\nwhere (a) is because of the kernel trick (Ghojogh et al.,\n2021e), i.e.,\nKx := Φ(X)⊤Φ(X).\n(49)\nsimilarly, the denominator of Eq. (32) in RKHS becomes:\ntr\n\u0000Φ(U)⊤(Φ(X)LwΦ(X)⊤+ λΦ(X)EΦ(X)⊤)Φ(U)\n\u0001\n(46)\n= tr\n\u0000T ⊤Φ(X)⊤(Φ(X)LwΦ(X)⊤\n+ λΦ(X)EΦ(X)⊤)Φ(X)T\n\u0001\n(a)\n= tr\n\u0000T ⊤Kx(Lw + λE)KxT\n\u0001\n,\nwhere (a) is because of the kernel trick (Ghojogh et al.,\n2021e). The constrain in RKHS becomes:\nΦ(U)⊤Φ(U)\n(46)\n= T ⊤Φ(X)⊤Φ(X)T\n(a)\n= T ⊤KxT ,\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n12\nwhere (a) is because of the kernel trick (Ghojogh et al.,\n2021e). The Eq. (32) in RKHS is:\nmaximize\nT\ntr\n\u0000T ⊤KxLbKxT\n\u0001\ntr\n\u0000T ⊤Kx(Lw + λE)KxT\n\u0001,\nsubject to\nT ⊤KxT = I.\n(50)\nIt can be solved using projected gradient method (Ghojogh\net al., 2021c) to ﬁnd the optimal T . Then, the projected\ndata onto the subspace of metric is found as:\nΦ(U)⊤Φ(X)\n(46)\n= T ⊤Φ(X)⊤Φ(X)\n(a)\n= T ⊤Kx,\n(51)\nwhere (a) is because of the kernel trick (Ghojogh et al.,\n2021e).\n3.6.3. REGULARIZATION BY LAPLACIAN\nAnother\nkernel\nspectral\nmetric\nlearning\nmethod\nis\n(Baghshah & Shouraki, 2010) whose optimization is in the\nform:\nminimize\nΦ(X)\n1\n|S|\nX\n(xi,xj)∈S\n∥φ(xi) −φ(xj)∥2\n2 + λΩ(Φ(X)),\nsubject to\n∥φ(xi) −φ(xj)∥2\n2 ≥c,\n∀(xi, xj) ∈D,\n(52)\nwhere c > 0 is a hyperparameter and λ > 0 is the regu-\nlarization parameter. Consider the kNN graph of data with\nan adjacency matrix A ∈Rn×n whose (i, j)-th element is\none if xi and xj are neighbors and is zero otherwise. Let\nthe Laplacian matrix of this adjacency matrix be denoted\nby L.\nIn this method, the regularization term Ω(Φ(X)) can\nbe the objective of Laplacian eigenmap (Ghojogh et al.,\n2021d):\nΩ(Φ(X)) := 1\n2n\nn\nX\ni=1\nn\nX\nj=1\n∥φ(xi) −φ(xj)∥2\n2A(i, j)\n(a)\n= tr(Φ(X)LΦ(X)⊤)\n(b)\n= tr(LΦ(X)⊤Φ(X))\n(c)\n= tr(LKx),\nwhere (a) is according to (Belkin & Niyogi, 2001) (see\n(Ghojogh et al., 2021d) for proof), (b) is because of the\ncyclic property of trace, and (c) is because of the ker-\nnel trick (Ghojogh et al., 2021e). Moreover, according to\nEq. (44), the distance in RKHS is ∥φ(xi) −φ(xj)∥2\n2 =\nk(xi, xi) + k(xj, xj) −2k(xi, xj). We can simplify the\nterm in Eq. (52) as:\n1\n|S|\nX\n(xi,xj)∈S\n∥φ(xi) −φ(xj)∥2\n2\n(44)\n=\n1\n|S|\nX\n(xi,xj)∈S\nk(xi, xi) + k(xj, xj) −2k(xi, xj)\n= 1\n|S|\nX\n(xi,xj)∈S\n(ei −ej)⊤Kx(ei −ej)\n(a)\n= tr(ESKx),\nwhere (a) is because the scalar is equal to its trace\nand we use the cyclic property of trace, i.e., (ei −\nej)⊤Kx(ei −ej) = tr((ei −ej)⊤Kx(ei −ej)) =\ntr((ei −ej)(ei −ej)⊤Kx), and then we deﬁne ES :=\n(1/|S|) P\n(xi,xj)∈S(ei −ej)(ei −ej)⊤.\nHence, Eq. (52) can be restated as:\nminimize\nKx\ntr(ESKx) + λ tr(LKx),\nsubject to\nk(xi, xi) + k(xj, xj) −2k(xi, xj) ≥c,\n∀(xi, xj) ∈D,\nKx ⪰0,\n(53)\nnoticing that the kernel matrix is positive semideﬁnite. This\nproblem is a Semideﬁnite Programming (SDP) problem\nand can be solved using the interior point method (Gho-\njogh et al., 2021c). The optimal kernel matrix can be de-\ncomposed using eigenvalue decomposition to ﬁnd the em-\nbedding of data in RKHS, i.e., Φ(X):\nKx = V ⊤ΣV = V ⊤Σ(1/2Σ(1/2)V\n(49)\n= Φ(X)⊤Φ(X),\nwhere V and Σ are the eigenvectors and eigenvalues, (a) is\nbecause Kx ⪰0 so its eigenvalues are non-negative can be\ntaken second root of, and (b) is because we get Φ(X) :=\nΣ(1/2)V .\n3.6.4. KERNEL DISCRIMINATIVE COMPONENT\nANALYSIS\nHere, we explain the kernel version of DCA (Hoi et al.,\n2006) which was introduced in Section 3.1.5.\nLemma 4. The generalized Mahalanobis distance metric\nin RKHS, with the pulled weight matrix to RKHS denoted\nby Φ(W ), can be seen as measuring the Euclidean dis-\ntance in RKHS after projection onto the column subspace\nof T where T is the coefﬁcient matrix in Eq. (46). In other\nwords:\n∥φ(xi)−φ(xj)∥2\nΦ(W ) = ∥ki −kj∥2\nT T ⊤\n= (ki −kj)⊤T T ⊤(ki −kj),\n(54)\nwhere\nki\n:=\nk(X, xi)\n=\nΦ(X)⊤φ(xi)\n=\n[k(x1, xi), . . . , k(xn, xi)]⊤∈Rn is the kernel vector be-\ntween X and xi.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n13\nProof. We can have the decomposition of the weight ma-\ntrix, i.e. Eq. (9), in RKHS which is:\nΦ(W ) = Φ(U)Φ(U)⊤.\n(55)\nThe generalized Mahalanobis distance metric in RKHS is:\n∥φ(xi) −φ(xj)∥2\nΦ(W )\n(9)\n= (φ(xi) −φ(xj))⊤Φ(U)Φ(U)⊤(φ(xi) −φ(xj))\n=\n\u0000Φ(U)⊤φ(xi) −Φ(U)⊤φ(xj)\n\u0001⊤\n\u0000Φ(U)⊤φ(xi) −Φ(U)⊤φ(xj)\n\u0001\n(46)\n=\n\u0000T ⊤Φ(X)⊤φ(xi) −T ⊤Φ(X)⊤φ(xj)\n\u0001⊤\n\u0000T ⊤Φ(X)⊤φ(xi) −T ⊤Φ(X)⊤φ(xj)\n\u0001\n(a)\n=\n\u0000T ⊤ki −T ⊤kj\n\u0001⊤\u0000T ⊤ki −T ⊤kj\n\u0001\n=\n\u0000ki −kj\n\u0001⊤T T ⊤\u0000ki −kj\n\u0001\n= ∥ki −kj∥2\nT T ⊤,\nwhere (a) is because of the kernel trick, i.e., k(X, xi) =\nΦ(X)⊤φ(xi). Q.E.D.\nLet νl := [ 1\nnl\nPnl\ni=1 k(x1, xi), . . . , 1\nnl\nPnl\ni=1 k(xn, xi)]⊤∈\nRn where nl denotes the cardinality of the l-th class. Let\nKw and Kb be the kernelized versions of Sw and Sb,\nrespectively (see Eqs. (25) and (26)). If Xl denotes the l-th\nclass, we have:\nRn×n ∋Kw := 1\nn\nc\nX\nl=1\nX\nxi∈Xl\n(ki −νl)(ki −νl)⊤\n(56)\nRn×n ∋Kb := 1\nn\nc\nX\nl=1\nc\nX\nj=1\n(νl −νj)(νl −νj)⊤.\n(57)\nWe saw the metric in RKHS can be seen as projection onto\na subspace with the projection matrix T . Therefore, Eq.\n(27) in RKHS becomes (Hoi et al., 2006):\nmaximize\nT\ntr(T ⊤KbT )\ntr(T ⊤KwT )\n,\n(58)\nwhich is a generalized Rayleigh-Ritz quotient.\nThe so-\nlution T to this optimization problem is the generalized\neigenvalue problem (Kb, Kw) (Ghojogh et al., 2019a).\nThe weight matrix of the generalized Mahalanobis distance\nis obtained by Eqs. (46) and (55).\n3.6.5. RELEVANT TO KERNEL FISHER DISCRIMINANT\nANALYSIS\nHere, we explain the kernel version of the metric learning\nmethod (Alipanahi et al., 2008) which was introduced in\nSection 3.1.3.\nAccording to Eq. (54), we have:\n∥φ(xi) −φ(xj)∥2\nΦ(W ) = (ki −kj)⊤T T ⊤(ki −kj)\n(a)\n= tr\n\u0000(ki −kj)⊤T T ⊤(ki −kj)\n\u0001\n(b)\n= tr\n\u0000T ⊤(ki −kj)(ki −kj)⊤T\n\u0001\n,\nwhere (a) is because a scalar it equal to its trace and (b) is\nbecause of the cyclic property of trace. Hence, Eq. (20) in\nRKHS becomes:\n1\n|S|\nX\n(xi,xj)∈S\ntr\n\u0000T ⊤(ki −kj)(ki −kj)⊤T\n\u0001\n= tr\n\u0010\nT ⊤\u0000 1\n|S|\nX\n(xi,xj)∈S\n(ki −kj)(ki −kj)⊤T\n\u0001\u0011\n= tr(T ⊤Σφ\nST ),\nand likewise:\n1\n|D|\nX\n(xi,xj)∈D\ntr\n\u0000T ⊤(ki −kj)(ki −kj)⊤T\n\u0001\n= tr(T ⊤Σφ\nDT ),\nwhere:\nΣφ\nS := 1\n|S|\nX\n(xi,xj)∈S\n(ki −kj)(ki −kj)⊤,\nΣφ\nD :=\n1\n|D|\nX\n(xi,xj)∈D\n(ki −kj)(ki −kj)⊤.\nHence, in RKHS, the objective of the optimization problem\n(23) becomes tr(T ⊤(Σφ\nS −Σφ\nD)T ⊤). We change the con-\nstraint in Eq. (23) to U ⊤U = I. In RKHS, this constraint\nbecomes:\nΦ(U)⊤Φ(U)\n(46)\n= T ⊤Φ(X)⊤Φ(X)T\n(49)\n= T ⊤KxT\nset= I,\nFinally, (23) in RKHS becomes:\nminimize\nT\ntr(T ⊤(Σφ\nS −Σφ\nD)T )\nsubject to\nT ⊤KxT = I,\n(59)\nwhose solution is a generalized eigenvalue problem (Σφ\nS −\nΣφ\nD, Kx) where T is the matrix of eigenvectors.\nThe\nweight matrix of the generalized Mahalanobis distance is\nobtained by Eqs. (46) and (55). This is relevant to kernel\nFisher discriminant analysis (Mika et al., 1999; Ghojogh\net al., 2019b) which minimizes and maximizes the intra-\nclass and inter-class variances in RKHS.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n14\n3.6.6. RELEVANT TO KERNEL SUPPORT VECTOR\nMACHINE\nHere, we explain the kernel version of the metric learning\nmethod (Tsang et al., 2003) which was introduced in Sec-\ntion 3.4. It is relevant to kernel SVM. Using kernel trick\n(Ghojogh et al., 2021e) and Eq. (54), the Eq. (41) can be\nkernelized as (Tsang et al., 2003):\nmaximize\n{αij}\nX\n(xi,xj)∈D\nαijT ⊤(kii + kjj −2kij)\n−1\n2\nX\n(xi,xj)∈D\nX\n(xk,xl)∈D\nαijαkl(kik −kil −kjk + kjl)2\n+ λ1\n|S|\nX\n(xi,xj)∈D\nX\n(xk,xl)∈S\nαij(kik −kil −kjk + kjl)2\nsubject to\n1\nλ2\nX\n(xi,xj)∈D\nαij ≥ν,\nαij ∈[0, λ2\n|D|],\n(60)\nwhich is a quadratic programming problem and can be\nsolved by optimization solvers.\n3.7. Geometric Spectral Metric Learning\nSome spectral metric learning methods are geometric\nmethods which use Riemannian manifolds. In the follow-\ning, we introduce the mist well-known geometric methods.\nThere are some other geometric methods, such as (Hauberg\net al., 2012), which are not covered for brevity.\n3.7.1. GEOMETRIC MEAN METRIC LEARNING\nOne of the geometric spectral metric learning is Geometric\nMean Metric Learning (GMML) (Zadeh et al., 2016). Let\nW be the weight matrix in the generalized Mahalanobis\ndistance for similar points.\n– Regular GMML: In GMML, we use the inverse of\nweight matrix, i.e. W −1 , for the dissimilar points. The\noptimization problem of GMML is (Zadeh et al., 2016):\nminimize\nW\nX\n(xi,xj)∈S\n∥xi −xj∥2\nW\n+\nX\n(xi,xj)∈D\n∥xi −xj∥2\nW −1\nsubject to\nW ⪰0.\n(61)\nAccording to Eq. (13), this problem can be restated as:\nminimize\nW\ntr(W ΣS) + tr(W −1ΣD)\nsubject to\nW ⪰0,\n(62)\nwhere ΣS and ΣD are deﬁned in Eq. (14). Taking deriva-\ntive of the objective function w.r.t. W and setting it to zero\ngives:\n∂\n∂W\n\u0000tr(W ΣS) + tr(W −1ΣD)\n\u0001\n= ΣS −W −1ΣDW −1 set= 0 =⇒ΣD = W ΣSW .\n(63)\nThis equation is the Riccati equation (Riccati, 1724) and\nits solution is the midpoint of the geodesic connecting Σ−1\nS\nand ΣD (Bhatia, 2007, Section 1.2.13).\nLemma 5 ((Bhatia, 2007, Chapter 6)). The geodesic curve\nconnecting two points Σ1 and Σ2 on the Symmetric Pos-\nitive Deﬁnite (SPD) Riemannian manifold is denoted by\nΣ1♯tΣ2 and is computed as:\nΣ1♯tΣ2 := Σ(1/2)\n1\n\u0000Σ(−1/2)\n1\nΣ2Σ(−1/2)\n1\n\u0001tΣ(1/2)\n1\n,\n(64)\nwhere t ∈[0, 1].\nHence, the solution of Eq. (63) is:\nW = Σ−1\nS ♯(1/2)ΣD\n(64)\n= Σ(−1/2)\nS\n\u0000Σ(1/2)\nS\nΣDΣ(1/2)\nS\n\u0001(1/2)Σ(−1/2)\nS\n.\n(65)\nThe proof of Eq. (65) is as follows (Hajiabadi et al., 2019):\nΣD\n(63)\n= W ΣSW\n=⇒Σ(1/2)\nS\nΣDΣ(1/2)\nS\n= Σ(1/2)\nS\nW ΣSW Σ(1/2)\nS\n=⇒(Σ(1/2)\nS\nΣDΣ(1/2)\nS\n)(1/2)\n= (Σ(1/2)\nS\nW ΣSW Σ(1/2)\nS\n)(1/2)\n=⇒(Σ(1/2)\nS\nΣDΣ(1/2)\nS\n)(1/2)\n(a)\n= ((Σ(1/2)\nS\nW Σ(1/2)\nS\n)(Σ(1/2)\nS\nW Σ(1/2)\nS\n))(1/2)\n= (Σ(1/2)\nS\nW Σ(1/2)\nS\n)\n=⇒Σ(−1/2)\nS\n(Σ(1/2)\nS\nΣDΣ(1/2)\nS\n)(1/2)Σ(−1/2)\nS\n= Σ(−1/2)\nS\n(Σ(1/2)\nS\nW Σ(1/2)\nS\n)Σ(−1/2)\nS\n= W ,\nwhere (a) is because ΣS ⪰0 so its eigenvalues are non-\nnegative and the matrix of eigenvalues can be decomposed\nby the second root in its eigenvalue decomposition to have\nΣS = Σ(1/2)\nS\nΣ(1/2)\nS\n.\n– Regularized GMML: The matrix ΣS might be singular\nor near singular and hence non-invertible. Therefore, we\nregularize Eq. (62) to make the weight matrix close to a\nprior known positive deﬁnite matrix W 0.\nminimize\nW\ntr(W ΣS) + tr(W −1ΣD)\n+ λ\n\u0000tr(W W −1\n0 ) + tr(W −1W 0) −2d\n\u0001\n,\nsubject to\nW ⪰0,\n(66)\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n15\nwhere λ > 0 is the regularization parameter. The regular-\nization term is the symmetrized log-determinant divergence\nbetween W and W 0. Taking derivative of the objective\nfunction w.r.t. W and setting it to zero gives:\n∂\n∂W\n\u0000tr(W ΣS) + tr(W −1ΣD) + λtr(W W −1\n0 )\n+ λtr(W −1W 0) −2λd\n\u0001\n= ΣS −W −1ΣDW −1 + λW −1\n0\n+ λW −1W 0W −1 set= 0\n=⇒ΣD + λW 0 = W (ΣS + λW −1\n0 )W ,\nwhich is again a Riccati equation (Riccati, 1724) whose\nsolution is the midpoint of the geodesic connecting (ΣS +\nλW −1\n0 )−1 and (ΣD + λW 0):\nW = (ΣS + λW −1\n0 )−1♯(1/2)(ΣD + λW 0).\n(67)\n– Weighted GMML: Eq. (62) can be restated as:\nminimize\nW\nδ2(W , Σ−1\nS ) + δ2(W , ΣD)\nsubject to\nW ⪰0,\n(68)\nwhere δ(., .) is the Riemannian distance (or Fr´echet mean)\non the SPD manifold (Arsigny et al., 2007, Eq 1.1):\nδ(Σ1, Σ2) := ∥log(Σ(−1/2)\n2\nΣ1Σ(−1/2)\n2\n)∥F ,\nwhere ∥.∥F is the Frobenius norm. We can weight the ob-\njective in Eq. (68):\nminimize\nW\n(1 −t)δ2(W , Σ−1\nS ) + tδ2(W , ΣD)\nsubject to\nW ⪰0,\n(69)\nwhere t ∈[0, 1] is a hyperparameter. The solution of this\nproblem is the weighted version of Eq. (67):\nW = (ΣS + λW −1\n0 )−1♯t(ΣD + λW 0).\n(70)\n3.7.2. LOW-RANK GEOMETRIC MEAN METRIC\nLEARNING\nWe can learn a low-rank weight matrix in GMML (Bhutani\net al., 2018), where the rank of wight matrix is set to be\np ≪d:\nminimize\nW\ntr(W ΣS) + tr(W −1ΣD)\nsubject to\nW ⪰0,\nrank(W ) = p.\n(71)\nWe can decompose it using eigenvalue decomposition as\ndone in Eq. (9), i.e., W = V ΛV ⊤= UU ⊤, where we\nonly have p eigenvectors and p eigenvalues. Therefore, the\nsizes of matrices are V ∈Rd×p, Λ ∈Rp×p, and U ∈\nRd×p. By this decomposition, the objective function in Eq.\n(71 can be restated as:\ntr(V ΛV ⊤ΣS) + tr(V Λ−1V ⊤ΣD)\n(a)\n= tr(ΛV ⊤ΣSV ) + tr(Λ−1V ⊤ΣDV )\n(b)\n= tr(ΛeΣS) + tr(Λ−1 eΣD),\nwhere (V ⊤)−1 = V because it is orthogonal, (a) is be-\ncause of the cyclic property of trace, and (b) is because we\ndeﬁne eΣS := V ⊤ΣSV and eΣD := V ⊤ΣDV . Notic-\ning that the matrix of eigenvectors V is orthogonal, the Eq.\n(71) is restated to:\nminimize\nΛ,V\ntr(ΛeΣS) + tr(Λ−1 eΣD)\nsubject to\nΛ ⪰0,\nV ⊤V = I,\n(72)\nwhere rank(W ) = p is automatically satisﬁed by tak-\ning V\n∈Rd×p and Λ ∈Rp×p in the decomposition.\nThis problem can be solved by the alternative optimization\n(Ghojogh et al., 2021c). If the variable V is ﬁxed, min-\nimization w.r.t. Λ is similar to the problem (62); hence,\nits solution is similar to Eq. (65), i.e., Λ = eΣ\n−1\nS ♯(1/2) eΣD\n(see Eq. (64) for the deﬁnition of ♯t). If Λ is ﬁxed, the\northogonality constraint V ⊤V = I can be modeled by V\nbelonging to the Grassmannian manifold G(p, d) which is\nthe set of p-dimensional subspaces of Rd. To sum up, the\nalternative optimization is:\nΛ(τ+1) = (V (τ)⊤ΣSV (τ))−1♯(1/2)(V (τ)⊤ΣDV (τ)),\nV (τ+1) := arg\nmin\nV ∈G(p,d)\n\u0010\ntr(Λ(τ+1)V ⊤ΣSV )\n+ tr((Λ(τ+1))−1V ⊤ΣDV )\n\u0011\n,\nwhere τ is the iteration index. Optimization of V can be\nsolved by Riemannian optimization (Absil et al., 2009).\n3.7.3. GEOMETRIC MEAN METRIC LEARNING FOR\nPARTIAL LABELS\nPartial label learning (Cour et al., 2011) refers to when a set\nof candidate labels is available for every data point. GMML\ncan be modiﬁed to be used for partial label learning (Zhou\n& Gu, 2018). Let Yi denote the set of candidate labels\nfor xi. If there are q candidate labels in total, we denote\nyi = [yi1, . . . , yiq]⊤∈{0, 1}q where yij is one if the j-th\nlabel is a candidate label for xi and is zero otherwise. We\ndeﬁne X+\ni\n:= {xj|j = 1, . . . , n, j ̸= i, Yi ∩Yj ̸= ∅}\nand X−\ni\n:= {xj|j = 1, . . . , n, Yi ∩Yj = ∅}. In other\nwords, X+\ni and X−\ni are the data points which share and\ndo not share some candidate labels with xi, respectively.\nLet N +\ni\nbe the indices of the k nearest neighbors of xi\namong X+\ni . Also, let N −\ni\nbe the indices of points in X−\ni\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n16\nwhose distance from xi are smaller than the distance of\nthe furthest point in N +\ni\nfrom xi. In other words, N −\ni\n:=\n{j|j = 1, . . . , n, xj ∈X−\ni , ∥xi−xj∥2 ≤maxt∈N +\ni ∥xi−\nxt∥2}.\nLet w(1)\ni\n= [w(1)\ni,t , ∀t ∈N +\ni ]⊤∈Rk contain the probabili-\nties that each of the k neighbors of xi share the same label\nwith xi. It can be estimated by linear reconstruction of yi\nby the neighbor yt’s:\nminimize\nw(1)\ni\n1\nq\n\r\ryi −\nX\nt∈N +\ni\nw(1)\ni,t yt\n\r\r2\n2 + λ1\nk\nX\nt∈N +\ni\n(w(1)\ni,t )2\nsubject to\nw(1)\ni,t ≥0,\nt ∈N +\ni ,\nwhere λ1 > 0 is the regularization parameter. Let w(2)\ni\n=\n[w(2)\ni,t , ∀t ∈N +\ni ]⊤∈Rk denote the coefﬁcients for lin-\near reconstruction of xi by its k nearest neighbors. It is\nobtained as:\nminimize\nw(2)\ni\n\r\rxi −\nX\nt∈N +\ni\nw(2)\ni,t xt\n\r\r2\n2\nsubject to\nw(2)\ni,t ≥0,\nt ∈N +\ni .\nThese two optimization problems are quadratic program-\nming and can be solved using the interior point method\n(Ghojogh et al., 2021c).\nThe main optimization problem of GMML for partial labels\nis (Zhou & Gu, 2018):\nminimize\nW\ntr(W Σ′\nS) + tr(W −1Σ′\nD)\nsubject to\nW ⪰0,\n(73)\nwhere:\nΣ′\nS :=\nn\nX\ni=1\n P\nt∈N +\ni w(1)\ni,t (xi −xt)(xi −xt)⊤\nP\nt∈N +\ni w(1)\ni,t\n+ λ\n\u0010\nxi −\nX\nt∈N +\ni\nw(2)\ni,t xt\n\u0011\u0010\nxi −\nX\nt∈N +\ni\nw(2)\ni,t xt\n\u0011⊤\n!\n,\nΣ′\nD :=\nn\nX\ni=1\nX\nt∈N −\ni\n(xi −xt)(xi −xt)⊤.\nMinimizing the ﬁrst term of Σ′\nS in tr(W Σ′\nS) decreases\nthe distances of similar points which share some candidate\nlabels. Minimizing the second term of Σ′\nS in tr(W Σ′\nS)\ntries to preserve linear reconstruction of xi by its neigh-\nbors after projection onto the subspace of metric. Minimiz-\ning tr(W −1Σ′\nD) increases the the distances of dissimilar\npoints which do not share any candidate labels. The prob-\nlem (73) is similar to the problem (62); hence, its solution\nis similar to Eq. (65), i.e., W = Σ′\nS\n−1♯(1/2)Σ′\nD (see Eq.\n(64) for the deﬁnition of ♯t).\n3.7.4. GEOMETRIC MEAN METRIC LEARNING ON SPD\nAND GRASSMANNIAN MANIFOLDS\nThe GMML method (Zadeh et al., 2016), introduced in\nSection 3.7.1, can be implemented on Symmetric Positive\nDeﬁnite (SPD) and Grassmannian manifolds (Zhu et al.,\n2018). If Xi, Xj ∈Sd\n++ is a point on the SPD manifold,\nthe distance metric on this manifold is (Zhu et al., 2018):\ndW (T i, T j) := tr\n\u0000W (T i −T j)(T i −T j)\n\u0001\n,\n(74)\nwhere W ∈Rd×d is the weight matrix of metric and\nT i := log(Xi) is the logarithm operation on the SPD\nmanifold. The Grassmannian manifold Gr(k, d) is the k-\ndimensional subspaces of the d-dimensional vector space.\nA point in Gr(k, d) is a linear subspace spanned by a full-\nrank Xi ∈Rd×k which is orthogonal, i.e., X⊤\ni Xi = I.\nIf M ∈Rd×r is any matrix, We deﬁne X′\ni in a way\nthat M ⊤X′\ni is the orthogonal components of M ⊤Xi. If\nRd×d ∋T ij := X′\niX\n′⊤\ni\n−X′\njX\n′⊤\nj , the distance on the\nGrassmannian manifold is (Zhu et al., 2018):\ndW (T ij) := tr\n\u0000W T ijT ij\n\u0001\n,\n(75)\nW ∈Rd×d is the weight matrix of metric.\nSimilar to the optimization problem of GMML, i.e. Eq.\n(61), we solve the following problem for the SPD manifold:\nminimize\nW\nX\n(T i,T j)∈S\ntr\n\u0000W (T i −T j)(T i −T j)\n\u0001\n+\nX\n(T i,T j)∈D\ntr\n\u0000W −1(T i −T j)(T i −T j)\n\u0001\nsubject to\nW ⪰0.\n(76)\nLikewise, for the Grassmannian manifold, the optimization\nproblem is:\nminimize\nW\nX\n(T i,T j)∈S\ntr\n\u0000W T ijT ij\n\u0001\n+\nX\n(T i,T j)∈D\ntr\n\u0000W −1T ijT ij\n\u0001\nsubject to\nW ⪰0.\n(77)\nSuppose, for the SPD manifold, we deﬁne:\nΣ′\nS :=\nX\n(T i,T j)∈S\n(T i −T j)(T i −T j),\nΣ′\nD :=\nX\n(T i,T j)∈D\n(T i −T j)(T i −T j).\nand, for the Grassmannian manifold, we deﬁne:\nΣ′\nS :=\nX\n(T i,T j)∈S\nT ijT ij,\nΣ′\nD :=\nX\n(T i,T j)∈D\nT ijT ij.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n17\nHence, for either SPD or Grassmannian manifold, the opti-\nmization problem becomes Eq. (62) in which ΣS and ΣD\nare replaced with Σ′\nS and Σ′\nD, respectively.\n3.7.5. METRIC LEARNING ON STIEFEL AND SPD\nMANIFOLDS\nAccording to Eq. (9), the weight matrix in the metric can\nbe decomposed as W = V ΛV ⊤. If we do not restrict V\nand Λ to be the matrices of eigenvectors and eigenvalues\nas in Eq. (9), we can learn both V ∈Rd×p and Λ ∈Rp×p\nby optimization (Harandi et al., 2017). The optimization\nproblem in this method is:\nminimize\nV ,Λ\nX\n(xi,xj)∈S\nlog(1 + qij)\n+\nX\n(xi,xj)∈D\nlog(1 + q−1\nij )\n+ λ\n\u0010\ntr(ΛΛ−1\n0 ) −log\n\u0000det(ΛΛ−1\n0 )\n\u0001\n−p\n\u0011\nsubject to\nV ⊤V = I,\nΛ ⪰0,\n(78)\nwhere λ > 0 is the regularization parameter, det(.) denotes\nthe determinant of matrix, and qij models Gaussian distri-\nbution with the generalized Mahalanobis distance metric:\nqij := exp(∥xi −xj∥V ΛV ⊤).\nThe constraint V ⊤V = I means that the matrix V belongs\nto the Stiefel manifold St(p, d) := {V ∈Rd×p|V ⊤V =\nI} and the constraint Λ ⪰0 means Λ belongs to the SPD\nmanifold Sp\n++. Hence, these two variables belong to the\nproduct manifold St(p, d) × Sp\n++. Hence, we can solve\nthis optimization problem using Riemannian optimization\nmethods (Absil et al., 2009). This method can also be ker-\nnelized; the reader can refer to (Harandi et al., 2017, Sec-\ntion 4) for its kernel version.\n3.7.6. CURVILINEAR DISTANCE METRIC LEARNING\n(CDML)\nLemma 6 ((Chen et al., 2019)). The generalized Maha-\nlanobis distance can be restated as:\n∥xi −xj∥2\nW =\np\nX\nl=1\n∥ul∥2\n2\n\u0010 Z Tl(xj)\nTl(xi)\n∥ul∥2 dt\n\u00112\n,\n(79)\nwhere ul ∈Rd is the l-th column of U in Eq. (9), t ∈R,\nand Tl(x) ∈R is the projection of x satisfying (ulTl(x) −\nx)⊤ul = 0.\nProof.\n∥xi −xj∥2\nW = (xi −xj)⊤W (xi −xj)\n(9)\n= (xi −xj)⊤UU ⊤(xi −xj) = ∥U ⊤(xi −xj)∥2\n2\n= ∥[u⊤\n1 (xi −xj), . . . , u⊤\np (xi −xj)]⊤∥2\n2\n=\np\nX\nl=1\n\u0000u⊤\nl (xi −xj)\n\u00012\n(a)\n=\np\nX\nl=1\n∥ul∥2\n2∥xi −xj∥2\n2 cos2(ul, xi −xj)\n(b)\n=\np\nX\nl=1\n∥ul∥2\n2∥ulTl(xi) −ulTl(xj)∥2\n2,\nwhere (a) is because of the law of cosines and (b) is be-\ncause of (ulTl(x)−x)⊤ul = 0. The distance ∥ulTl(xi)−\nulTl(xj)∥2 can be replaced by the length of the arc be-\ntween Tl(xi) and Tl(xj) on the straight line ult for t ∈R.\nThis gives the Eq. (79). Q.E.D.\nThe condition (ulTl(x) −x)⊤ul = 0 is equivalent to\nﬁnding the nearest neighbor to the line ult, ∀t ∈R, i.e.,\nTl(x) := arg mint∈R ∥ult −x∥2\n2 (Chen et al., 2019). This\nequation can be generalized to ﬁnd the nearest neighbor to\nthe geodesic curve θl(t) rather than the line ult:\nTθl(x) := arg min\nt∈R ∥θl(t) −x∥2\n2.\n(80)\nHence, we can replace the arc length of the straight line in\nEq. (79) with the arc length of the curve:\n∥xi −xj∥2\nW =\np\nX\nl=1\nαl\n\u0010 Z Tθl(xj)\nTθl(xi)\n∥θ′\nl(t)∥2 dt\n\u00112\n,\n(81)\nwhere θ′\nl(t) is derivative of θl(t) w.r.t.\nt and αl :=\n(\nR 1\n0 ∥θ′\nl(t)∥2 dt)2 is the scale factor. The Curvilinear Dis-\ntance Metric Learning (CDML) (Chen et al., 2019) uses\nthis approximation of distance metric by the above curvy\ngeodesic on manifold, i.e., Eq.\n(81).\nThe optimization\nproblem in CDML is:\nminimize\nΘ\n1\nn\nn\nX\ni=1\nL(∥xi −xj∥2\nW ; yij) + λΩ(Θ),\n(82)\nwhere n is the number of points, Θ := [θ1, . . . , θp], yij =\n1 if (xi, xj) ∈S and yij = 0 if (xi, xj) ∈D, ∥xi −\nxj∥2\nW is deﬁned in Eq. (81), λ > 0 is the regularization\nparameter, L(.) is some loss function, and Ω(Θ) is some\npenalty term. The optimal Θ, obtained from Eq. (82), can\nbe used in Eq. (81) to have the optimal distance metric. A\nrecent follow-up of CDML is (Zhang et al., 2021).\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n18\n3.8. Adversarial Metric Learning (AML)\nAdversarial Metric Learning (AML) (Chen et al., 2018)\nuses adversarial learning (Goodfellow et al., 2014; Gho-\njogh et al., 2021b) for metric learning. On one hand, we\nhave a distinguishment stage which tries to discriminate\nthe dissimilar points and push similar points close to one\nanother. On the other hand, we have an confusion or adver-\nsarial stage which tries to fool the metric learning method\nby pulling the dissimilar points close to each other and\npushing the similar points away. The distinguishment and\nconfusion stages are trained simultaneously and they make\neach other stronger gradually.\nFrom\nthe\ndataset,\nwe\nform\nrandom\npairs\nX\n:=\n{(xi, x′\ni)}n/2\ni=1.\nIf xi and x′\ni are similar points, we set\nyi = 1 and if they are dissimilar, we have yi = −1.\nWe also generate some random new points in pairs X g :=\n{(xg\ni , xg′\ni )}n/2\ni=1. The generated points are updated itera-\ntively by optimization of the confusion stage to fool the\nmetric. The loss functions for both stages are Eq. (61) used\nin geometric mean metric learning (see Section 3.7.1).\nThe alternative optimization (Ghojogh et al., 2021c) used\nin AML is:\nW (t+1) := arg min\nW\n\u0010 X\nyi=1\n∥xi −x′\ni∥2\nW\n+\nX\nyi=−1\n∥xi −x′\ni∥2\nW −1 + λ1\n\u0000 X\nyi=1\n∥xg(t)\ni\n−xg′(t)\ni\n∥2\nW\n+\nX\nyi=−1\n∥xg(t)\ni\n−xg′(t)\ni\n∥2\nW −1\n\u0001\u0011\n,\nX g(t+1) := arg min\nX ′\n\u0010 X\nyi=−1\n∥xi −x′\ni∥2\nW (t+1)\n+\nX\nyi=1\n∥xi −x′\ni∥2\n(W (t+1))−1 + λ2\n\u0000 n/2\nX\ni=1\n∥xi −xg\ni ∥2\nW (t+1)\n+\nn/2\nX\ni=1\n∥x′\ni −xg′\ni ∥2\nW (t+1)\n\u0001\u0011\n,\n(83)\nuntil convergence, where λ1, λ2 > 0 are the regularization\nparameters. Updating W and X g are the distinguishment\nand confusion stages, respectively. In the distinguishment\nstage, we ﬁnd a weight matrix W to minimize the distances\nof similar points in both X and X g and maximize the dis-\ntances of dissimilar points in both X and X g. In the con-\nfusion stage, we generate new points X g to adversarially\nmaximize the distances of similar points in X and adver-\nsarially minimize the distances of dissimilar points in X.\nIn this stage, we also make the points xg\ni and xg′\ni similar to\ntheir corresponding points xi and x′\ni, respectively.\n4. Probabilistic Metric Learning\nProbabilistic methods for metric learning learn the weight\nmatrix in the generalized Mahalanobis distance using prob-\nability distributions. They deﬁne some probability distribu-\ntion for each point accepting other points as its neighbors.\nOf course, the closer points have higher probability for be-\ning neighbors.\n4.1. Collapsing Classes\nOne probabilistic method for metric learning is collapsing\nsimilar points to the same class while pushing the dissim-\nilar points away from one another (Globerson & Roweis,\n2005). The probability distribution between points for be-\ning neighbors can be a Gaussian distribution which uses the\ngeneralized Mahalanobis distance as its metric. The distri-\nbution for xi to take xj as its neighbor is (Goldberger et al.,\n2005):\npW\nij :=\nexp(−∥xi −xj∥2\nW )\nP\nk̸=i exp(−∥xi −xk∥2\nW ),\nj ̸= i,\n(84)\nwhere we deﬁne the normalization factor, also called the\npartition function, as Zi := P\nk̸=i exp(−∥xi −xk∥2\nW ).\nThis factor makes the summation of distribution one. Eq.\n(84) is a Gaussian distribution whose covariance matrix is\nW −1 because it is equivalent to:\npW\nij := 1\nZi\nexp\n\u0000−(xi −xj)⊤W (xi −xj)\n\u0001\n.\nWe want the similar points to collapse to the same point af-\nter projection onto the subspace of metric (see Proposition\n2). Hence, we deﬁne the desired neighborhood distribution\nto be a bi-level distribution (Globerson & Roweis, 2005):\np0\nij :=\n\u001a\n1\nif (xi, xj) ∈S\n0\nif (xi, xj) ∈D.\n(85)\nThis makes all similar points of a group/class a same point\nafter projection.\n4.1.1. COLLAPSING CLASSES IN THE INPUT SPACE\nFor making pW\nij close to the desired distribution p0\nij, we\nminimize the KL-divergence between them (Globerson &\nRoweis, 2005):\nminimize\nW\nn\nX\ni=1\nn\nX\nj=1,j̸=i\nKL(p0\nij ∥pW\nij )\nsubject to\nW ⪰0.\n(86)\nLemma 7 ((Globerson & Roweis, 2005)). Let the the\nobjective function in Eq.\n(86) be denoted by c\n:=\nPn\ni=1\nPn\nj=1,j̸=i KL(p0\nij ∥pW\nij ). The gradient of this func-\ntion w.r.t. W is:\n∂c\n∂W =\nn\nX\ni=1\nn\nX\nj=1,j̸=i\n(p0\nij −pW\nij )(xi −xj)(xi −xj)⊤.\n(87)\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n19\nProof. The derivation is similar to the derivation of gradi-\nent in Stochastic Neighbor Embedding (SNE) and t-SNE\n(Hinton & Roweis, 2003; van der Maaten & Hinton, 2008;\nGhojogh et al., 2020c). Let:\nR ∋rij := d2\nij = ||xi −xj||2\nW .\n(88)\nBy changing xi, we only have change impact in dij and\ndji (or rij and rji) for all j’s. According to chain rule, we\nhave:\n∂c\n∂W =\nX\ni,j\n\u0000 ∂c\n∂rij\n∂rij\n∂W + ∂c\n∂rji\n∂rji\n∂W\n\u0001\n.\nAccording to Eq. (88), we have:\nrij = ||xi −xj||2\nW = tr((xi −xj)⊤W (xi −xj))\n(a)\n= tr((xi −xj)(xi −xj)⊤W )\n=⇒∂rij\n∂W = (xi −xj)(xi −xj)⊤,\nrji = ||xj −xi||2\nW = ||xi −xj||2\nW = rij\n=⇒∂rji\n∂W = (xi −xj)(xi −xj)⊤,\nwhere (a) is because of the cyclic property of trace. There-\nfore:\n∴\n∂c\n∂W = 2\nX\ni,j\n\u0000 ∂c\n∂rij\n\u0001\n(xi −xj)(xi −xj)⊤.\n(89)\nThe dummy variables in cost function can be re-written as:\nc =\nX\nk\nX\nl̸=k\np0(l|k) log( p0(l|k)\npW (l|k))\n=\nX\nk̸=l\np0(l|k) log( p0(l|k)\npW (l|k))\n=\nX\nk̸=l\n\u0000p0(l|k) log(p0(l|k)) −p0(l|k) log(pW (l|k))\n\u0001\n,\nwhose ﬁrst term is a constant with respect to pW (l|k) and\nthus to W . We have:\nR ∋∂c\n∂rij\n= −\nX\nk̸=l\np0(l|k)∂(log(pW (l|k)))\n∂rij\n.\nAccording to Eqs. (84) and (88), the pW (l|k) is:\npW (l|k) :=\nexp(−d2\nkl)\nP\nk̸=f exp(−d2\nkf) =\nexp(−rkl)\nP\nk̸=f exp(−rkf).\nWe take the denominator of pW (l|k) as:\nβ :=\nX\nk̸=f\nexp(−d2\nkf) =\nX\nk̸=f\nexp(−rkf).\n(90)\nWe have log(pW (l|k)) = log(pW (l|k)) + log β −log β =\nlog(pW (l|k) β) −log β. Therefore:\n∴\n∂c\n∂rij\n= −\nX\nk̸=l\np0(l|k)∂\n\u0000log(pW (l|k)β) −log β\n\u0001\n∂rij\n= −\nX\nk̸=l\np0(l|k)\n\u0014∂\n\u0000log(pW (l|k)β)\n\u0001\n∂rij\n−∂\n\u0000log β\n\u0001\n∂rij\n\u0015\n= −\nX\nk̸=l\np0(l|k)\n\u0014\n1\npW (l|k)β\n∂\n\u0000pW (l|k)β\n\u0001\n∂rij\n−1\nβ\n∂β\n∂rij\n\u0015\n.\nThe pW (l|k)β is:\npW (l|k)β =\nexp(−rkl)\nP\nf̸=k exp(−rkf) ×\nX\nk̸=f\nexp(−rkf)\n= exp(−rkl).\nTherefore, we have:\n∴\n∂c\n∂rij\n=\n−\nX\nk̸=l\np0(l|k)\n\u0014\n1\npW (l|k)β\n∂\n\u0000exp(−rkl)\n\u0001\n∂rij\n−1\nβ\n∂β\n∂rij\n\u0015\n.\nThe ∂\n\u0000exp(−rkl)\n\u0001\n/∂rij is non-zero for only k = i and\nl = j; therefore:\n∂\n\u0000exp(−rij)\n\u0001\n∂rij\n= −exp(−rij),\n∂β\n∂rij\n=\n∂P\nk̸=f exp(−rkf)\n∂rij\n= ∂exp(−rij)\n∂rij\n= −exp(−rij).\nTherefore:\n∴\n∂c\n∂rij\n=\n−\n\u0012\np0\nij\nh −1\npW\nij β exp(−rij)\ni\n+ 0 + · · · + 0\n\u0013\n−\nX\nk̸=l\np0(l|k)\nh 1\nβ exp(−rij)\ni\n.\nWe have P\nk̸=l p0(l|k) = 1 because summation of all pos-\nsible probabilities is one. Thus:\n∂c\n∂rij\n= −p0\nij\nh −1\npW\nij β exp(−rij)\ni\n−\nh 1\nβ exp(−rij)\ni\n= exp(−rij)\nβ\n|\n{z\n}\n=pW\nij\nh p0\nij\npW\nij\n−1\ni\n= p0\nij −pW\nij .\n(91)\nSubstituting the obtained derivative in Eq. (89) gives Eq.\n(87). Q.E.D.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n20\nThe optimization problem (86) is convex; hence, it has a\nunique solution. We can solve it using any optimization\nmethod such as the projected gradient method, where after\nevery gradient descent step, we project the solution onto\nthe positive semi-deﬁnite cone (Ghojogh et al., 2021c):\nW := W −η ∂c\n∂W ,\nW := V diag(max(λ1, 0), . . . , max(λd, 0)) V ⊤,\nwhere η\n> 0 is the learning rate and V and Λ =\ndiag(λ1, . . . , λd) are the eigenvectors and eigenvalues of\nW , respectively (see Eq. (9)).\n4.1.2. COLLAPSING CLASSES IN THE FEATURE SPACE\nAccording to Eq. (54), the distance in the feature space can\nbe stated using kernels as ∥ki −kj∥2\nT T ⊤where ki ∈Rn is\nthe kernel vector between dataset X and the point xi. We\ndeﬁne R := T T ⊤∈Rn×n. Hence, in the feature space,\nEq. (84) becomes:\npR\nij :=\nexp(−∥ki −kj∥2\nR)\nP\nk̸=i exp(−∥ki −kk∥2\nR),\nj ̸= i.\n(92)\nThe gradient in Eq. (87) becomes:\n∂c\n∂R =\nn\nX\ni=1\nn\nX\nj=1,j̸=i\n(p0\nij −pR\nij)(ki −kj)(ki −kj)⊤.\n(93)\nAgain, we can ﬁnd the optimal R using projected gradi-\nent method. This gives us the optimal metric for collapsing\nclasses in the feature space (Globerson & Roweis, 2005).\nNote that we can also regularize the objective function, us-\ning the trace operator or Frobenius norm, for avoiding over-\nﬁtting.\n4.2. Neighborhood Component Analysis Methods\nNeighborhood Component Analysis (NCA) is one of the\nmost well-known probabilistic metric learning methods. In\nthe following, we introduce different variants of NCA.\n4.2.1. NEIGHBORHOOD COMPONENT ANALYSIS\n(NCA)\nIn the original NCA (Goldberger et al., 2005), the probabil-\nity that xj takes xi as its neighbor is as in Eq. (84), where\nwe assume pW\nii = 0 by convention:\npW\nij :=\n(\nexp(−∥xi−xj∥2\nW )\nP\nk̸=i exp(−∥xi−xk∥2\nW )\nif j ̸= i\n0\nif j = i.\n(94)\nConsider the decomposition of the weight matrix of metric\nas in Eq. (9), i.e., W = UU ⊤. Let Si denote the set of\nsimilar points to xi where (xi, xj) ∈S. The optimization\nproblem of NCA is to ﬁnd a U to maximize this probability\ndistribution for similar points (Goldberger et al., 2005):\nmaximize\nU\nX\n(xi,xj)∈S\npW\nij =\nn\nX\ni=1\nX\nxj∈Si\npW\nij =\nn\nX\ni=1\npW\ni ,\n(95)\nwhere:\npW\ni\n:=\nX\nxj∈Si\npW\nij .\n(96)\nNote that the required constraint W ⪰0 is already satis-\nﬁed because of the decomposition in Eq. (84).\nLemma 8 ((Goldberger et al., 2005)). Suppose the objec-\ntive function of Eq. (95) is denoted by c. The gradient of\nthis cost function w.r.t. U is:\n∂c\n∂U = 2\nn\nX\ni=1\n\u0010\npW\ni\nn\nX\nk=1\npW\nik (xi −xk)(xi −xk)⊤\n−\nX\nxj∈Si\npW\nij (xi −xj)(xi −xj)⊤\u0011\nU.\n(97)\nThe derivation of this gradient is similar to the approach\nin the proof of Lemma 7. We can use gradient ascent for\nsolving the optimization.\nAnother approach is to maximize the log-likelihood of\nneighborhood probability (Goldberger et al., 2005):\nmaximize\nU\nn\nX\ni=1\nlog\n\u0010 X\nxj∈Si\npW\nij\n\u0011\n,\n(98)\nwhose gradient is (Goldberger et al., 2005):\n∂c\n∂U = 2\nn\nX\ni=1\n\u0010\nn\nX\nk=1\npW\nik (xi −xk)(xi −xk)⊤\n−\nP\nxj∈Si pW\nij (xi −xj)(xi −xj)⊤\nP\nxj∈Si pW\nij\n\u0011\nU.\n(99)\nAgain, gradient ascent can give us the optimal U. As ex-\nplained in Proposition 2, the subspace is metric is the col-\numn space of U and projection of points onto this subspace\nreduces the dimensionality of data.\n4.2.2. REGULARIZED NEIGHBORHOOD COMPONENT\nANALYSIS\nIt is shown by some experiments that NCA can overﬁt to\ntraining data for high-dimensional data (Yang & Laakso-\nnen, 2007). Hence, we can regularize it to avoid overﬁtting.\nIn regularized NCA (Yang & Laaksonen, 2007), we use the\nlog-posterior of the matrix U which is equal to:\nP(U|xi, Si) = P(xi, Si|U) P(U)\nP(xi, Si)\n,\n(100)\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n21\naccording to the Bayes’ rule. We can use Gaussian distri-\nbution for the prior:\nP(U) =\nd\nY\nk=1\nd\nY\nl=1\nc exp(−λ(U(k, l))2),\n(101)\nwhere c > 0 is a constant factor including the normaliza-\ntion factor, λ > 0 is the inverse of variance, and U(k, l) is\nthe (k, l)-th element of U ∈Rd×d. Note that we can have\nU ∈Rd×p if we truncate it to have p leading eigenvectors\nof W (see Eq. (9)). The likelihood\nP(xi, Si|U) ∝exp\n\u0010\nX\n(xi,xj)∈S\npW\nij\n\u0011\n.\n(102)\nThe regularized NCA maximizes the log-posterior (Yang &\nLaaksonen, 2007):\nlog P(U|xi, Si)\n(100)\n= log P(xi, Si|U) + log P(U)\n−log P(xi, Si)\n|\n{z\n}\nconstant w.r.t. U\n(a)\n=\nX\n(xi,xj)∈S\npW\nij −λ∥U∥2\nF ,\nwhere (a) is because of Eqs. (101) and (102) and ∥.∥F de-\nnotes the Frobenius norm. Hence, the optimization prob-\nlem of regularized NCA is (Yang & Laaksonen, 2007):\nmaximize\nU\nX\n(xi,xj)∈S\npW\nij −λ∥U∥2\nF ,\n(103)\nwhere λ > 0 can be seen as the regularization parameter.\nThe gradient is similar to Eq. (97) but plus the derivative\nof the regularization term which is −2λU.\n4.2.3. FAST NEIGHBORHOOD COMPONENT ANALYSIS\n– Fast NCA: The fast NCA (Yang et al., 2012) acceler-\nates NCA by using k-Nearest Neighbors (kNN) rather than\nusing all points for computing the neighborhood distribu-\ntion of every point. Let Ni and Mi denote the kNN of xi\namong the similar points to xi (denoted by Si) and dissim-\nilar points (denoted by Di), respectively. Fast NCA uses\nfollowing probability distribution for xi to take xi as its\nneighbor (Yang et al., 2012):\npW\nij :=\n(\nexp(−∥xi−xj∥W )\nP\nxk∈Ni∪Mi exp(−∥xi−xk∥W )\nif xk ∈Ni ∪Mi\n0\notherwise.\n(104)\nThe optimization problem of fast NCA is similar to Eq.\n(103):\nmaximize\nU\nn\nX\ni=1\nX\nxj∈Mi\npW\nij −λ∥U∥2\nF ,\n(105)\nwhere pW\nij is Eq. (104) and U is the matrix in the decom-\nposition of W (see Eq. (9)).\nLemma 9 ((Yang et al., 2012)). Suppose the objective\nfunction of Eq. (105) is denoted by c. The gradient of this\ncost function w.r.t. U is:\n∂c\n∂U =\nn\nX\ni=1\n\u0010\npW\ni\nX\nxk∈Ni\npW\nik (xi −xk)(xi −xk)⊤\n+ (pW\ni\n−1)\nX\nxj∈Mi\npW\nij (xi −xj)(xi −xj)⊤\u0011\nU −2λU.\n(106)\nThis is similar to Eq. (97). See (Yang et al., 2012) for\nthe derivation. We can use gradient ascent for solving the\noptimization.\n– Kernel Fast NCA: According to Eq. (54), the distance\nin the feature space is ∥ki −kj∥2\nT T ⊤where ki ∈Rn is\nthe kernel vector between dataset X and the point xi. We\ncan use this distance metric in Eq. (104) to have kernel fast\nNCA (Yang et al., 2012). Hence, the gradient of kernel fast\nNCA is similar to Eq. (106):\n∂c\n∂T =\nn\nX\ni=1\n\u0010\npW\ni\nX\nxk∈Ni\npW\nik (ki −kk)(ki −kk)⊤\n+ (pW\ni\n−1)\nX\nxj∈Mi\npW\nij (ki −kj)(ki −kj)⊤\u0011\nT −2λT .\n(107)\nAgain, we can ﬁnd the optimal T using gradient ascent.\nNote that the same technique can be used to kernelize the\noriginal NCA.\n4.3. Bayesian Metric Learning Methods\nIn this section, we introduce the Bayesian metric learning\nmethods which use variational inference (Ghojogh et al.,\n2021a) for metric learning. In Bayesian metric learning, we\nlearn a distribution for the distance metric between every\ntwo points; we sample the pairwise distances from these\nlearned distributions.\nFirst, we provide some deﬁnition required in these meth-\nods. According to Eq. (9), we can decompose the weight\nmatrix in the metric using the eigenvalue decomposition.\nAccordingly, we can approximate this matrix by:\nW ≈V xΛV ⊤\nx ,\n(108)\nwhere V x contains the eigenvectors of XX⊤and Λ =\ndiag([λ1, . . . , λd]⊤) is the diagonal matrix of eigenvalues\nwhich we learn in Bayesian metric learning. Let X and\nY denote the random variables for data and labels, respec-\ntively, and let λ = [λ1, . . . , λd]⊤∈Rd denote the learn-\nable eigenvalues. Let vl\nx ∈Rd denote the l-th column of\nV x. We deﬁne wij = [w1\nij, . . . , wd\nij]⊤:= [((v1\nx)⊤(xi −\nxj))2, . . . , ((vd\nx)⊤(xi −xj))2]⊤∈Rd. The reader should\nnot confuse wij with W which is the weight matrix of met-\nric in out notations.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n22\n4.3.1. BAYESIAN METRIC LEARNING USING SIGMOID\nFUNCTION\nOne of the Bayesian metric learning methods is (Yang\net al., 2007). We deﬁne:\nyij :=\n\u001a 1\nif (xi, xj) ∈S\n−1\nif (xi, xj) ∈D.\n(109)\nWe can consider a sigmoid function for the likelihood\n(Yang et al., 2007):\nP(Y |X, Λ) =\n1\n1 + exp(yij(Pd\nl=1 λlwl\nij −µ))\n,\n(110)\nwhere µ > 0 is a threshold. We can also derive an evidence\nlower bound for P(S, D); we do not provide the deriva-\ntion for brevity (see (Yang et al., 2007) for derivation of\nthe lower bound). As in the variational inference, we max-\nimize this lower bound for likelihood maximization (Gho-\njogh et al., 2021a). We assume a Gaussian distribution with\nmean mλ ∈Rd and covariance V λ ∈Rd×d for the dis-\ntribution P(λ). By maximizing the lower bound, we can\nestimate these parameters as (Yang et al., 2007):\nV T :=\n\u0010\nδI + 2\nX\n(xi,xj)∈S\ntanh(ξs\nij)\n4ξs\nij\nwijw⊤\nij\n+ 2\nX\n(xi,xj)∈D\ntanh(ξd\nij)\n4ξd\nij\nwijw⊤\nij\n\u0011−1\n,\n(111)\nmT := V T\n\u0010\nδγ0 −1\n2\nX\n(xi,xj)∈S\nwij + 1\n2\nX\n(xi,xj)∈D\nwij\n\u0011\n,\n(112)\nwhere δ > 0 and γ0 are hyper-parameters related to the\npriors on the weight matrix of metric and the threshold.\nWe deﬁne the following variational parameter (Yang et al.,\n2007):\nξs\nij :=\nq\n(m⊤\nT wij)2 + w⊤\nijV T wij,\n(113)\nfor (xi, xj) ∈S. We similarly deﬁne the variational pa-\nrameter ξd\nij for (xi, xj) ∈D. The variables V T , mT , ξs\nij,\nand ξd\nij are updated iteratively by Eqs. (115), (116), and\n(113), respectively, until convergence. After these param-\neters are learned, we can sample the eigenvalues from the\nposterior, λ ∼N(mT , V T ). These eigenvalues can be\nused in Eq. (108) to obtain the weight matrix in the met-\nric. Note that Bayesian metric learning can also be used for\nactive learning (see (Yang et al., 2007) for details).\n4.3.2. BAYESIAN NEIGHBORHOOD COMPONENT\nANALYSIS\nBayesian NCA (Wang & Tan, 2017) using variational in-\nference (Ghojogh et al., 2021a) in the NCA formulation. If\nNim denotes the dataset index of the m-th nearest neighbor\nof xi, we deﬁne W j\ni := [wij −wiNi1, . . . , wij −wiNik] ∈\nRd×k.\nAs in the variational inference (Ghojogh et al.,\n2021a), we consider an evidence lower-bound on the log-\nlikelihood:\nlog(P(Y |X, Λ)) >\nn\nX\ni=1\nX\nxj∈Ni\n\u0010\n−1\n2λ⊤W j\niH(W j\ni)⊤λ\n+ b⊤\nij(W j\ni)⊤λ −cij\n\u0011\n,\nwhere Ni was deﬁned before in Section 4.2.3, H := 1\n2(I−\n1\nk+111⊤) ∈Rk×k is the centering matrix, and:\nRk ∋bij := Hψij\n−exp\n \nψij −log\n\u0010\n1 +\nX\nxt∈Ni\nexp\n\u0000(wij −wit)⊤λ\n\u0001\u0011!\n,\n(114)\nin which ψij ∈Rk is the learnable variational parameter.\nSee (Wang & Tan, 2017) for the derivation of this lower-\nbound. The sketch of this derivation is using Eq. (84) but\nfor the kNN among the similar points, i.e., Ni. Then, the\nlower-bound is obtained by a logarithm inequality as well\nas the Bohning’s quadratic bound (Murphy, 2012).\nWe assume a Gaussian distribution for the prior of λ with\nmean m0 ∈Rd and covariance V 0 ∈Rd×d. This prior\nis assumed to be known. Likewise, we assume a Gaussian\ndistribution with mean mT ∈Rd and covariance V T ∈\nRd×d for the posterior P(X, Λ|Y ). Using Bayes’ rule and\nthe above lower-bound on the likelihood, we can estimate\nthese parameters as (Wang & Tan, 2017):\nV T :=\n\u0010\nV −1\n0\n+\nn\nX\ni=1\nX\nxj∈Ni\nW j\niH(W j\ni)⊤\u0011−1\n,\n(115)\nmT := V T\n\u0010\nV −1\n0 m0 +\nn\nX\ni=1\nX\nxj∈Ni\nW j\nibij\n\u0011\n.\n(116)\nThe variational parameter can also be obtained by (Wang\n& Tan, 2017):\nψij := (W j\ni)⊤mT .\n(117)\nThe variables bij, V T , mT , and ψij are updated iteratively\nby Eqs. (114), (115), (116), and (117), respectively, until\nconvergence.\nAfter these parameters are learned, we can sample the\neigenvalues from the posterior, λ ∼N(mT , V T ). These\neigenvalues can be used in Eq. (108) to obtain the weight\nmatrix in the metric. Alternatively, we can directly sample\nthe distance metric from the following distribution:\n∥xi −xj∥2\nW ∼N(w⊤\nijmT , w⊤\nijV T wij).\n(118)\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n23\n4.3.3. LOCAL DISTANCE METRIC (LDM)\nLet the set of similar and dissimilar points for the point xi\nbe denoted by Si and Di, respectively. In Local Distance\nMetric (LDM) (Yang et al., 2006), we consider the follow-\ning for the likelihood:\nP(yi|xi) =\nX\nxj∈Si\nexp(−∥xi −xj∥2\nW )\n×\n\u0010 X\nxj∈Si\nexp(−∥xi −xj∥2\nW )\n+\nX\nxj∈Di\nexp(−∥xi −xj∥2\nW )\n\u0011−1\n. (119)\nIf we consider Eq. (108) for decomposition of the weight\nmatrix, the log-likelihood becomes:\nn\nX\ni=1\nlog(P(yi|xi, Λ)) =\nn\nX\ni=1\nlog\n\u0010 X\nxj∈Si\nexp\n\u0000−\nd\nX\nl=1\nλlwl\nij\n\u0001\u0011\nn\nX\ni=1\nlog\n\u0010 X\nxj∈Si\nexp\n\u0000−\nd\nX\nl=1\nλlwl\nij\n\u0001\n+\nX\nxj∈Di\nexp\n\u0000−\nd\nX\nl=1\nλlwl\nij\n\u0001\u0011\n.\nWe want to maximize this log-likelihood for learning the\nvariables {λ1, . . . , λd}. An evidence lower bound on this\nlog-likelihood can be (Yang et al., 2006):\nn\nX\ni=1\nlog(P(yi|xi, Λ)) ≥\nn\nX\ni=1\nX\nxj∈Si\nφij\nd\nX\nl=1\nλlwl\nij\n−\nn\nX\ni=1\nlog\n\u0010 X\nxj∈Si\nexp\n\u0000−\nd\nX\nl=1\nλlwl\nij\n\u0001\n+\nX\nxj∈Di\nexp\n\u0000−\nd\nX\nl=1\nλlwl\nij\n\u0001\u0011\n,\n(120)\nwhere φij is the variational parameter which is:\nφij :=\nexp\n\u0000−Pd\nl=1 λlwl\nij\n\u0001\nP\nxj∈Si exp\n\u0000−Pd\nl=1 λlwl\nij\n\u0001×\n\u0010\n1 +\nexp\n\u0000−Pd\nl=1 λlwl\nij\n\u0001\nP\nxj∈Si exp\n\u0000−Pd\nl=1 λlwl\nij\n\u0001\n\u0011−1\n.\n(121)\nSee (Yang et al., 2006) for derivation of the lower bound.\nIteratively, we maximize the lower bound, i.e. Eq. (120),\nand update φij by Eq.\n(121).\nThe learned parameters\n{λ1, . . . , λd} can be used in Eq. (108) to obtain the weight\nmatrix in the metric.\n4.4. Information Theoretic Metric Learning\nThere exist information theoretic approaches for metric\nlearning where KL-divergence (relative entropy) or mutual\ninformation is used.\n4.4.1. INFORMATION THEORETIC METRIC LEARNING\nWITH A PRIOR WEIGHT MATRIX\nOne of the information theoretic methods for metric learn-\ning is using a prior weight matrix (Davis et al., 2007) where\nwe consider a known weight matrix W 0 as the regularizer\nand try to minimize the KL-divergence between the distri-\nbutions with W and W 0:\nKL(pW0\nij ∥pW\nij ) :=\nn\nX\ni=1\nn\nX\nj=1\npW0\nij log\n\u0010pW0\nij\npW\nij\n\u0011\n.\n(122)\nThere are both ofﬂine and online approaches for metric\nlearning using batch and streaming data, respectively.\n– Ofﬂine Information Theoretic Metric Learning: We\nconsider a Gaussian distribution, i.e. Eq. (84), for the prob-\nability of xi taking xj as its neighbor, i.e. pW\nij . While\nwe make the weight matrix similar to the prior weight ma-\ntrix through KL-divergence, we ﬁnd a weight matrix which\nmakes all the distances of similar points less than an up-\nper bound u > 0 and all the distances of dissimilar points\nlarger than a lower bound l (where l > u). Note that,\nfor Gaussian distributions, the KL divergence is related to\nthe LogDet Dld(., .) between covariance matrices (Dhillon,\n2007); hence, we can say:\nKL(pW0\nij ∥pW\nij ) = 1\n2Dld(W −1\n0 , W −1) = 1\n2Dld(W , W 0)\n(a)\n= tr(W W −1\n0 ) −log(det(W W −1\n0 )) −n,\nwhere (a) is because of the deﬁnition of LogDet. Hence,\nthe optimization problem can be (Davis et al., 2007):\nminimize\nW\nDld(W , W 0)\nsubject to\n∥xi −xj∥2\nW ≤u,\n∀(xi, xj) ∈S,\n∥xi −xj∥2\nW ≥l,\n∀(xi, xj) ∈D.\n(123)\n– Online Information Theoretic Metric Learning: The\nonline information theoretic metric learning (Davis et al.,\n2007) is suitable for streaming data. For this, we use the\nofﬂine approach where the known weight matrix W 0 is\nlearned weight matrix by the data which have been received\nso far. Consider the time slot t where we have been ac-\ncumulated some data until then and some new data points\nare received at this time. The optimization problem is Eq.\n(123) where W 0 = W t which is the learned weight matrix\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n24\nso far at time t. Note that if there is some label information\navailable, we can incorporate it in the optimization problem\nas a regularizer.\n4.4.2. INFORMATION THEORETIC METRIC LEARNING\nFOR IMBALANCED DATA\nDistance Metric by Balancing KL-divergence (DMBK)\n(Feng et al., 2018) can be used for imbalanced data where\nthe cardinality of classes are different. Assume the classes\nhave Gaussian distributions where µi ∈Rd and Σi ∈\nRd×d denote the mean and covariance of the i-th class. Re-\ncall the projection matrix U in Eq. (9) and Proposition\n2. The KL-divergence between the probabilities of the i-th\nand j-th classes after projection onto the subspace of metric\nis (Feng et al., 2018):\nKL(pi∥pj) = 1\n2\n\u0010\nlog\n\u0000det(U ⊤ΣjU)\n\u0001\n−log\n\u0000det(U ⊤ΣiU)\n\u0001\n+ tr\n\u0000(U ⊤ΣjU)−1U ⊤(Σi + Dij)U\n\u0001\u0011\n,\n(124)\nwhere Dij := (µi −µj)(µi −µj)⊤. To cancel the ef-\nfect of cardinality of classes in imbalanced data, we use the\nnormalized divergence of classes:\neij :=\nninjKL(pi∥pj)\nP\n1≤k<l≤c nknlKL(pk∥pl),\n(125)\nwhere ni and c denote the number of the i-th class and\nthe number of classes, respectively. We maximize the geo-\nmetric mean of this divergence between pairs of classes to\nseparate classes after projection onto the subspace of met-\nric. A regularization term is used to increase the distances\nof dissimilar points and a constraint is used to decrease the\nsimilar points (Feng et al., 2018):\nmaximize\nW\nlog\n\u0010\u0010\nY\n1≤i<j≤c\neij\n\u0011\n1\nc(c−1) \u0011\n+ λ\nX\n(xi,xj)∈D\n∥xi −xj∥W\nsubject to\nX\n(xi,xj)∈S\n∥xi −xj∥2\nW ≤1,\nW ⪰0,\n(126)\nwhere λ > 0 is the regularization parameter. This problem\ncan be solved using projected gradient method (Ghojogh\net al., 2021c).\n4.4.3. PROBABILISTIC RELEVANT COMPONENT\nANALYSIS METHODS\nRecall the Relevant Component Analysis (RCA method)\n(Shental et al., 2002) which was introduced in Section\n3.1.4. Here, we introduce probabilistic RCA (Bar-Hillel\net al., 2003; 2005) which uses information theory. Suppose\nthe n data points can be divided into c clusters, or so-called\nchunklets. Let Xl denote the data of the l-th chunklet and\nµl be the mean of Xl. Consider Eq. (9) for decomposition\nof the weight matrix in the metric where the column-space\nof U is the subspace of metric. Let projection of data onto\nthis subspace be denoted by Y = U ⊤X, the projected data\nin the l-th chunklet be Yl, and µy\nl be the mean of Yl.\nIn probabilistic RCA, we maximize the mutual informa-\ntion between data and the projected data while we want\nthe summation of distances of points in a chunklet from\nthe mean of chunklet is less than a threshold or mar-\ngin m > 0.\nThe mutual information is related to the\nentropy as I(X, Y ) := H(Y ) −H(Y |X); hence, we\ncan maximize the entropy of projected data H(Y ) rather\nthan the mutual information. Because Y = U ⊤X, we\nhave H(Y ) ∝det(U). According to Eq. (9), we have\ndet(U) ∝det(W ). Hence, the optimization problem can\nbe (Bar-Hillel et al., 2003; 2005):\nmaximize\nW\ndet(W )\nsubject to\nc\nX\nl=1\nX\nyi∈Yl\n∥yi −µy\nl ∥2\nW ≤m,\nW ⪰0.\n(127)\nThis preserves the information of data after projection\nwhile the inter-chunklet variances are upper-bounded by a\nmargin.\nIf we assume Gaussian distribution for each chunklet with\nthe covariance matrix Σl for the l-th chunklet, we have\ndet(W ) ∝log(det(U ⊤ΣlU)) because of the quadratic\ncharacteristic of covariance. In this case, the optimization\nproblem becomes:\nmaximize\nU\nc\nX\nl=1\nlog(det(U ⊤ΣlU))\nsubject to\nc\nX\nl=1\nX\nyi∈Yl\n∥yi −µy\nl ∥2\nUU ⊤≤m,\n(128)\nwhere W ⪰0 is already satisﬁed because of Eq. (9).\n4.4.4. METRIC LEARNING BY INFORMATION\nGEOMETRY\nAnother information theoretic methods for metric learning\nis using information geometry in which kernels on data and\nlabels are used (Wang & Jin, 2009). Let L ∈Rc×n denote\nthe one-hot encoded labels of n data points with c classes\nand let X ∈Rd×n be the data points. The kernel matrix\non the labels is KL = Y ⊤Y + λI whose main diagonal\nis strengthened by a small positive number λ to have a full\nrank. Recall Proposition 2 and Eq. (9) where U is the\nprojection matrix onto the subspace of metric. The kernel\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n25\nmatrix over the projected data, Y = U ⊤X, is:\nKY =Y ⊤Y = (U ⊤X)⊤(U ⊤X)\n= X⊤UU ⊤X\n(9)\n= X⊤W X.\n(129)\nWe can minimize the KL-divergence between the distribu-\ntions of kernels KY and KL (Wang & Jin, 2009):\nminimize\nW\nKL(KY ∥KL)\nsubject to\nW ⪰0.\n(130)\nFor simplicity, we assume Gaussian distributions for the\nkernels. The KL divergence between the distributions of\ntwo matrices, KY ∈Rn×n and KL ∈Rn×n, with Gaus-\nsian distributions is simpliﬁed to (Wang & Jin, 2009, The-\norem 1):\nKL(KY ∥KL) = 1\n2\n\u0010\ntr(K−1\nL KY ) + log(det(KL))\n−log(det(KY )) −n\n\u0011\n(129)\n∝\n1\n2\n\u0010\ntr(K−1\nL X⊤W X) + log(det(KL))\n−log(det(W )) −n\n\u0011\n.\nAfter ignoring the constant terms w.r.t. W , we can restate\nEq. (130) to:\nminimize\nW\ntr(K−1\nL X⊤W X) −log(det(W ))\nsubject to\nW ⪰0.\n(131)\nIf we take the derivative of the objective function in Eq.\n(131) and set it to zero, we have:\n∂c\n∂W = XK−1\nL X⊤−W −1 set= 0\n=⇒W = (XK−1\nL X⊤)−1.\n(132)\nNote that the constraint W ⪰0 is already satisﬁed by the\nsolution, i.e., Eq. (132).\nAlthough this method has used kernels, it can be kernelized\nfurther. We can also have a kernel version of this method\nby using Eq. (54) as the generalized Mahalanobis distance\nin the feature space, where T (deﬁned in Eq. (46)) is the\nprojection matrix for the metric. Using this in Eqs. (131)\nand (132) can give us the kernel version of this method. See\n(Wang & Jin, 2009) for more information about it.\n4.5. Empirical Risk Minimization in Metric Learning\nWe can learn the metric by minimizing some empirical\nrisk. In the following, some metric learning metric learning\nmethods by risk minimization are introduced.\n4.5.1. METRIC LEARNING USING THE SIGMOID\nFUNCTION\nOne of the metric learning methods by risk minimization is\n(Guillaumin et al., 2009). The distribution for xi to take xj\nas its neighbor can be stated using a sigmoid function:\npW\nij :=\n1\n1 + exp(∥xi −xj∥2\nW −b),\n(133)\nwhere b > 0 is a bias, because close-by points should have\nlarger probability.\nWe can maximize and minimize this\nprobability for similar and dissimilar points, respectively:\nmaximize\nW\nn\nX\ni=1\nn\nX\nj=1\nyij log(pW\nij ) + (1 −yij) log(1 −pW\nij )\nsubject to\nW ⪰0,\n(134)\nwhere yij is deﬁned in Eq. (109). This can be solved using\nprojected gradient method (Ghojogh et al., 2021c). This\noptimization can be seen as minimization of the empirical\nrisk where close-by points are pushed toward each other\nand dissimilar points are pushed away to have less error.\n4.5.2. PAIRWISE CONSTRAINED COMPONENT\nANALYSIS (PCCA)\nPairwise\nConstrained\nComponent\nAnalysis\n(PCCA)\n(Mignon & Jurie, 2012) minimizes the following empirical\nrisk to minimize and maximize the distances of similar\npoints and dissimilar points, respectively:\nminimize\nU\nn\nX\ni=1\nn\nX\nj=1\nlog\n\u0010\n1 + exp\n\u0000yij(∥xi −xj∥2\nUU ⊤−b)\n\u0001\u0011\n,\n(135)\nwhere yij is deﬁned in Eq. (109), b > 0 is a bias, W ⪰0\nis already satisﬁed because of Eq. (9). This can be solved\nusing projected gradient method (Ghojogh et al., 2021c)\nwith the gradient (Mignon & Jurie, 2012):\n∂c\n∂U = 2\nn\nX\ni=1\nn\nX\nj=1\nyij\n1 + exp\n\u0000yij(∥xi −xj∥2\nUU ⊤−b)\n\u0001\n× (xi −xj)(xi −xj)⊤U.\n(136)\nNote that we can have kernel PCCA by using Eq. (54).\nIn other words, we can replace ∥xi −xj∥2\nUU ⊤and (xi −\nxj)(xi −xj)⊤U with ∥ki −kj∥2\nT T ⊤and (ki −kj)(ki −\nkj)⊤T , respectively, to have PCCA in the feature space.\n4.5.3. METRIC LEARNING FOR PRIVILEGED\nINFORMATION\nIn some applications, we have a dataset with privileged in-\nformation where for every point, we have two feature vec-\ntor; one for the main feature (denoted by {xi}n\ni=1) and\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n26\none for the privileged information (denoted by {zi}n\ni=1).\nA metric learning method for using privileged information\nis (Yang et al., 2016) where we minimize and maximize the\ndistances of similar and dissimilar points, respectively, for\nthe main features. Simultaneously, we make the distances\nof privileged features close to the distances of main fea-\ntures. Having these two simultaneous goals, we minimize\nthe following empirical risk (Yang et al., 2016):\nminimize\nW 1,W 2\nn\nX\ni=1\nlog\n\u0010\n1+\nexp\n\u0000yij (∥xi −xj∥2\nW 1 −∥zi −zj∥2\nW 2)\n\u0001\u0011\nsubject to\nW 1 ⪰0,\nW 2 ⪰0.\n(137)\n5. Deep Metric Learning\nWe saw in Sections 3 and 4 that both spectral and prob-\nabilistic metric learning methods use the generalized Ma-\nhalanobis distance, i.e. Eq. (5), and learn the weight ma-\ntrix in the metric. Deep metric learning, however, has a\ndifferent approach. The methods in deep metric learning\nusually do not use a generalized Mahalanobis distance but\nthey earn an embedding space using a neural network. The\nnetwork learns a p-dimensional embedding space for dis-\ncriminating classes or the dissimilar points and making the\nsimilar points close to each other. The network embeds\ndata in the embedding space (or subspace) of metric. Then,\nany distance metric d(., .) : Rp × Rp →R can be used\nin this embedding space. In the loss functions of network,\nwe can use the distance function d(., .) in the embedding\nspace. For example, an option for the distance function is\nthe squared ℓ2 norm or squared Euclidean distance:\nd\n\u0000f(x1\ni ), f(x2\ni )\n\u0001\n:= ∥f(x1\ni ) −f(x2\ni )∥2\n2,\n(138)\nwhere f(xi) ∈Rp denotes the output of network for the\ninput xi as its p-dimensional embedding.\nWe train the\nnetwork using mini-batch methods such as the mini-batch\nstochastic gradient descent and denote the mini-batch size\nby b. The shared weights of sub-networks are denoted by\nthe learnable parameter θ.\n5.1. Reconstruction Autoencoders\n5.1.1. TYPES OF AUTOENCODERS\nAn autoencoder is a model consisting of an encoder E(.)\nand a decoder D(.). There are several types of autoen-\ncoders. All types of autoencoders learn a code layer in the\nmiddle of encoder and decoder. Inferential autoencoders\nlearn a stochastic latent space in the code layer between\nthe encoder and decoder. Variational autoencoder (Gho-\njogh et al., 2021a) and adversarial autoencoder (Ghojogh\net al., 2021b) are two important types of inferential autoen-\ncoders. Another type of autoencoder is the reconstruction\nautoencoder consisting of an encoder, transforming data to\na code, and a decoder, transforming the code back to the\ndata. Hence, the decoder reconstructs the input data to the\nencoder. The code is a representation for data. Each of\nthe encoder and decoder can be multiple layers of neural\nnetwork with activation functions.\n5.1.2. RECONSTRUCTION LOSS\nWe denote the input data point to the encoder by x ∈\nRd where d is the dimensionality of data.\nThe recon-\nstructed data point is the output of decoder and is denoted\nby bx ∈Rd. The representation code, which is the out-\nput of encoder and the input of decoder, is denoted by\nf(x) := E(x) ∈Rp. We have bx = D(E(x)) = D(f(x)).\nIf the dimensionality of code is greater than the dimension-\nality of input data, i.e. p > d, the autoencoder is called\nan over-complete autoencoder (Goodfellow et al., 2016).\nOtherwise, if p < d, the autoencoder is an under-complete\nautoencoder (Goodfellow et al., 2016). The loss function of\nreconstruction autoencoder tries to make the reconstructed\ndata close to the input data:\nminimize\nθ\nb\nX\ni=1\n\u0010\nd\n\u0000xi, bxi\n\u0001\n+ λΩ(θ)\n\u0011\n,\n(139)\nwhere λ ≥0 is the regularization parameter and Ω(θ) is\nsome penalty or regularization on the weights. Here, the\ndistance function d(., .) is deﬁned on Rd×Rd. Note that the\npenalty term can be regularization on the code f(xi). If the\nused distance metric is the squared Euclidean distance, this\nloss is named the regularized Mean Squared Error (MSE)\nloss.\n5.1.3. DENOISING AUTOENCODER\nA problem with over-complete autoencoder is that its train-\ning only copies each feature of data input to one of the neu-\nrons in the code layer and then copies it back to the corre-\nsponding feature of output layer. This is because the num-\nber of neurons in the code layer is greater than the number\nof neurons in the input and output layers. In other words,\nthe networks just memorizes or gets overﬁt. This coping\nhappens by making some of the weights equal to one (or\na scale of one depending on the activation functions) and\nthe rest of weights equal to zero. To avoid this problem in\nover-complete autoencoders, one can add some noise to the\ninput data and try to reconstruct the data without noise. For\nthis, Eq. (139) is used while the input to the network is the\nmini-batch plus some noise. This forces the over-complete\nautoencoder to not just copy data to the code layer. This\nautoencoder can be used for denoising as it reconstructs\nthe data without noise for a noisy input.\nThis network\nis called the Denoising Autoencoder (DAE) (Goodfellow\net al., 2016).\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n27\n5.1.4. METRIC LEARNING BY RECONSTRUCTION\nAUTOENCODER\nThe under-complete reconstruction autoencoder can be\nused for metric learning and dimensionality reduction, es-\npecially when p ≪d. The loss function for learning a low-\ndimensional representation code and reconstructing data by\nthe autoencoder is Eq. (139). The code layer between the\nencoder and decoder is the embedding space of metric.\nNote that if the activation functions of all layers are lin-\near, the under-complete autoencoder is reduced to Principal\nComponent Analysis (Ghojogh & Crowley, 2019). Let U l\ndenote the weight matrix of the l-th layer of network, ℓe\nbe the number of layers of encoder, and ℓd be the number\nof layers of decoder. With linear activation function, the\nencoder and decoder are:\nencoder:\nRp ∋f(xi) = U ⊤\nℓeU ⊤\nℓe−1 . . . U ⊤\n1\n|\n{z\n}\nU ⊤\ne\nxi,\ndecoder:\nRd ∋bxi = U 1 . . . U ℓd−1U ℓd\n|\n{z\n}\nU d\nf(xi),\nwhere linear projection by ℓprojection matrices can be re-\nplaced by linear projection with one projection matrices U e\nand U d.\nFor learning complicated data patterns, we can use non-\nlinear activation functions between layers of the encoder\nand decoder to have nonlinear metric learning and dimen-\nsionality reduction. It is noteworthy that nonlinear neural\nnetwork can be seen as an ensemble or concatenation of\ndimensionality reduction (or feature extraction) and kernel\nmethods. The justiﬁcation of this claim is as follows. Let\nthe dimensionality for a layer of network be U ∈Rd1×d2\nso it connects d1 neurons to d2 neurons. Two cases can\nhappen:\n• If d1 ≥d2, this layer acts as dimensionality reduction\nor feature extraction because it has reduced the dimen-\nsionality of its input data. If this layer has a nonlin-\near activation function, the dimensionality reduction\nis nonlinear; otherwise, it is linear.\n• If d1\n<\nd2, this layer acts as a kernel method\nwhich maps its input data to the high-dimensional fea-\nture space in some Reproducing Kernel Hilbert Space\n(RKHS). This kernelization can help nonlinear separa-\ntion of some classes which are not separable linearly\n(Ghojogh et al., 2021e). An example use of kernel-\nization in machine learning is kernel support vector\nmachine (Vapnik, 1995).\nTherefore, a neural network is a complicated feature extrac-\ntion method as a concatenation of dimensionality reduction\nand kernel methods. Each layer of network learns its own\nfeatures from data.\nFigure 3. The structure of network for metric learning with super-\nvised loss function.\n5.2. Supervised Metric Learning by Supervised Loss\nFunctions\nVarious loss functions exist for supervised metric learning\nby neural networks. Supervised loss functions can teach\nthe network to separate classes in the embedding space\n(Sikaroudi et al., 2020b). For this, we use a network whose\nlast layer is for classiﬁcation of data points. The features\nof the one-to-last layer can be used for feature embedding.\nThe last layer after the embedding features is named the\nclassiﬁcation layer. The structure of this network is shown\nin Fig. 3. Let the i-th point in the mini-batch be denoted by\nxi ∈Rd and its label be denoted by yi ∈R. Suppose the\nnetwork has one output neuron and its output for the input\nxi is denoted by fo(xi) ∈R. This output is the estimated\nclass label by the network. We denote output of the the\none-to-last layer by f(xi) ∈Rp where p is the number of\nneurons in that layer which is equivalent to the dimension-\nality of the embedding space. The last layer of network,\nconnecting the p neurons to the output neuron is a fully-\nconnected layer. The network until the one-to-last layer\ncan be any feed-forward or convolutional network depend-\ning on the type of data. If the network is convolutional, it\nshould be ﬂattened at the one-to-last layer. The network\nlearns to classify the classes, by the supervised loss func-\ntions, so the features of the one-to-last layers will be dis-\ncriminating features and suitable for embedding.\n5.2.1. MEAN SQUARED ERROR AND MEAN ABSOLUTE\nVALUE LOSSES\nOne of the supervised losses is the Mean Squared Error\n(MSE) which makes the estimated labels close to the true\nlabels using squared ℓ2 norm:\nminimize\nθ\nb\nX\ni=1\n(fo(xi) −yi)2.\n(140)\nOne problem with this loss function is exaggerating out-\nliers because of the square but its advantage is its differen-\ntiability. Another loss function is the Mean Absolute Error\n(MAE) which makes the estimated labels close to the true\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n28\nlabels using ℓ1 norm or the absolute value:\nminimize\nθ\nb\nX\ni=1\n|fo(xi) −yi|.\n(141)\nThe distance used in this loss is also named the Manhattan\ndistance. This loss function does not have the problem of\nMSE and it can be used for imposing sparsity in the em-\nbedding. It is not differentiable at the point f(xi) = yi but\nas the derivatives are calculated numerically by the neural\nnetwork, this is not a big issue nowadays.\n5.2.2. HUBER AND KL-DIVERGENCE LOSSS\nAnother loss function is the Huber loss which is a combi-\nnation of the MSE and MAE to have the advantages of both\nof them:\nminimize\nθ\nb\nX\ni=1\n\u001a 0.5(fo(xi) −yi)2\nif |fo(xi) −yi| ≤δ\nδ(|fo(xi) −yi| −0.5δ)\notherwise.\n(142)\nKL-divergence loss function makes the distribution of the\nestimated labels close to the distribution of the true labels:\nminimize\nθ\nKL(P(f(x))∥P(y)) =\nb\nX\ni=1\nf(xi) log(f(xi)\nyi\n).\n(143)\n5.2.3. HINGE LOSS\nIf there are two classes, i.e. c = 2, we can have true labels\nas yi ∈{−1, 1}. In this case, a possible loss function is the\nHinge loss:\nminimize\nθ\nb\nX\ni=1\n\u0002\nm −yi fo(xi)\n\u0003\n+,\n(144)\nwhere [·]+ := max(·, 0) and m > 0 is the margin. If the\nsigns of the estimated and true labels are different, the loss\nis positive which should be minimized. If the signs are the\nsame and |fo(xi)| ≥m, then the loss function is zero. If\nthe signs are the same but |fo(xi)| < m, the loss is positive\nand should be minimized because the estimation is correct\nbut not with enough margin from the incorrect estimation.\n5.2.4. CROSS-ENTROPY LOSS\nFor any number of classes, denoted by c, we can have a\ncross-entropy loss. For this loss, we have c neurons, rather\nthan one neuron, at the last layer. In contrast to the MSE,\nMAE, Huber, and KL-divergence losses which use linear\nactivation function at the last layer, cross-entropy requires\nsoftmax or sigmoid activation function at the last layer so\nthe output values are between zero and one. For this loss,\nwe have c outputs, i.e. fo(xi) ∈Rc (continuous values\nbetween zero and one), and the true labels are one-hot en-\ncoded, i.e., yi ∈{0, 1}c. This loss is deﬁned as:\nminimize\nθ\n−\nb\nX\ni=1\nc\nX\nl=1\n(yi)l log\n\u0000fo(xi)l\n\u0001\n,\n(145)\nwhere (yi)l and fo(xi)l denote the l-th element of yi\nand fo(xi), respectively. Minimizing this loss separates\nclasses for classiﬁcation; this separation of classes also\ngives us discriminating embedding in the one-to-last layer\n(Sikaroudi et al., 2020b; Boudiaf et al., 2020).\nThe reason for why cross-entropy can be suitable for metric\nlearning is theoretically justiﬁed in (Boudiaf et al., 2020),\nexplained in the following. Consider the mutual informa-\ntion between the true labels Y and the estimated labels\nfo(X):\nI(fo(X); Y ) = H(fo(X)) −H(fo(X)|Y )\n(146)\n= H(Y ) −H(Y |fo(X)),\n(147)\nwhere H(.) denotes entropy. On the one hand, Eq. (146)\nhas a generative view which exists in the metric learning\nloss functions generating embedding features. Eq. (147),\none the other hand, has a discriminative view used in the\ncross-entropy loss function. Therefore, the metric learning\nlosses and the cross-entropy loss are related. It is shown in\n(Boudiaf et al., 2020, Proposition 1) that the cross-entropy\nis an upper-bound on the metric learning losses so its min-\nimization for classiﬁcation also provides embedding fea-\ntures.\nIt is noteworthy that another supervised loss function is\ntriplet loss, introduced in the next section. Triplet loss can\nbe used for both hard labels (for classiﬁcation) and soft la-\nbels (for similarity and dissimilarity of points). The triplet\nloss also does not need a last classiﬁcation layer; therefore,\nthe embedding layer can be the last layer for this loss.\n5.3. Metric Learning by Siamese Networks\n5.3.1. SIAMESE AND TRIPLET NETWORKS\nOne of the important deep metric learning methods is\nSiamese network which is widely used for feature extrac-\ntion. Siamese network, originally proposed in (Bromley\net al., 1993), is a network consisting of several equivalent\nsub-networks sharing their weights. The number of sub-\nnetworks in a Siamese network can be any number but it\nusually is two or three. A Siamese network with three sub-\nnetworks is also called a triplet network (Hoffer & Ailon,\n2015). The weights of sub-networks in a Siamese network\nare trained in a way that the intra- and inter-class vari-\nances are decreased and increased, respectively. In other\nwords, the similar points are pushed toward each other\nwhile the dissimilar points are pulled away from one an-\nother. Siamese networks have been used in various appli-\ncations such as computer vision (Schroff et al., 2015) and\nnatural language processing (Yang et al., 2020).\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n29\nFigure 4. The structure of Siamese network with (a) two and (b) three sub-networks.\n5.3.2. PAIRS AND TRIPLETS OF DATA POINTS\nDepending on the number of sub-networks in the Siamese\nnetwork, we have loss functions for training. The loss func-\ntions of Siamese networks usually require pairs or triplets\nof data points. Siamese networks do not use the data points\none by one but we need to make pairs or triplets of points\nout of dataset for training a Siamese network. For mak-\ning the pairs or triplets, we consider every data point as the\nanchor point, denoted by xa\ni . Then, we take one of the sim-\nilar points to the anchor point as the positive (or neighbor)\npoint, denoted by xp\ni . We also take one of the dissimilar\npoints to the anchor point as the negative (or distant) point,\ndenoted by xn\ni . If class labels are available, we can use\nthem to ﬁnd the positive point as one of the points in the\nsame class as the anchor point, and to ﬁnd the the nega-\ntive point as one of the points in a different class from the\nanchor point’s class. Another approach is to augment the\nanchor point, using one of the augmentation methods, to\nobtain a positive points for the anchor point (Khodadadeh\net al., 2019; Chen et al., 2020).\nFor Siamese networks with two sub-networks, we make\npairs of anchor-positive points {(xa\ni , xp\ni )}nt\ni=1 and anchor-\nnegative points {(xa\ni , xn\ni )}nt\ni=1, where nt is the num-\nber of pairs.\nFor Siamese networks with three sub-\nnetworks, we make triplets of anchor-positive-negative\npoints {(xa\ni , xp\ni , xn\ni )}nt\ni=1, where nt is the number of\ntriplets. If we consider every point of dataset as an anchor,\nthe number of pairs/triplets is the same as the number of\ndata points, i.e., nt = n.\nVarious loss functions of Siamese networks use pairs or\ntriplets of data points to push the positive point towards\nthe anchor point and pull the negative point away from it.\nDoing this iteratively for all pairs or triplets will make the\nintra-class variances smaller and the inter-class variances\nlarger for better discrimination of classes or clusters. Later\nin the following, we introduce some of the loss functions\nfor training a Siamese network.\n5.3.3. IMPLEMENTATION OF SIAMESE NETWORKS\nA Siamese network with two and three sub-networks is de-\npicted in Fig. 4. We denote the output of Siamese network\nfor input x ∈Rd by f(x) ∈Rp where p is the dimensional-\nity of embedding (or the number of neurons at the last layer\nof the network) which is usually much less than the dimen-\nsionality of data, i.e., p ≪d. Note that the sub-networks\nof a Siamese network can be any fully-connected or convo-\nlutional network depending on the type of data. The used\nnetwork structure for the sub-networks is usually called the\nbackbone network.\nThe weights of sub-networks are shared in the sense that\nthe values of their weights are equal. Implementation of a\nSiamese network can be done in two ways:\n1. We can implement several sub-networks in the mem-\nory. In the training phase, we feed every data point\nin the pairs or triplets to one of the sub-networks and\ntake the outputs of sub-networks to have f(xa\ni ), f(xp\ni ),\nand f(xn\ni ). We use these in the loss function and up-\ndate the weights of only one of the sub-networks by\nbackpropagation (Ghojogh et al., 2021c). Then, we\ncopy the updated weights to the other sub-networks.\nWe repeat this for all mini-batches and epochs until\nconvergence. In the test phase, we feed the test point\nx to only one of the sub-networks and get the output\nf(x) as its embedding.\n2. We can implement only one sub-network in the mem-\nory. In the training phase, we feed the data points in\nthe pairs or triplets to the sub-network ont by one and\ntake the outputs of sub-network to have f(xa\ni ), f(xp\ni ),\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n30\nFigure 5. Visualization of what contrastive and triplet losses do: (a) a triplet of anchor (green circle), positive (blue circle), and negative\n(red diamond) points, (b) the effect of contrastive loss making a margin between the anchor and negative point, and (c) the effect of\ntriplet loss making a margin between the positive and negative points.\nand f(xn\ni ). We use these in the loss function and up-\ndate the weights of the sub-network by backpropaga-\ntion (Ghojogh et al., 2021c). We repeat this for all\nmini-batches and epochs until convergence. In the test\nphase, we feed the test point x to the sub-network and\nget the output f(x) as its embedding.\nThe advantage of the ﬁrst approach is to have all the sub-\nnetworks ready and we do not need to feed the points of\npairs or triplets one by one. Its disadvantage is using more\nmemory. As the number of points in the pairs or triplets is\nsmall (i.e., only two or three), the second approach is more\nrecommended as it is memory-efﬁcient.\n5.3.4. CONTRASTIVE LOSS\nOne loss function for Siamese networks is the contrastive\nloss which uses the anchor-positive and anchor-negative\npairs of points. Suppose, in each mini-batch, we have b\npairs of points {(x1\ni , x2\ni )}b\ni=1 some of which are anchor-\npositive and some are anchor-negative pairs. The points\nin an anchor-positive pair are similar, i.e. (x1\ni , x2\ni ) ∈S,\nand the points in an anchor-negative pair are dissimilar, i.e.\n(x1\ni , x2\ni ) ∈D, where S and D denote the similar and dis-\nsimilar sets.\n– Contrastive Loss: We deﬁne:\nyi :=\n\u001a 0\nif (x1\ni , x2\ni ) ∈S\n1\nif (x1\ni , x2\ni ) ∈D.\n∀i ∈{1, . . . , nt}.\n(148)\nThe main contrastive loss was proposed in (Hadsell et al.,\n2006) and is:\nminimize\nθ\nb\nX\ni=1\n\u0010\n(1 −yi)d\n\u0000f(x1\ni ), f(x2\ni )\n\u0001\n+ yi\n\u0002\n−d\n\u0000f(x1\ni ), f(x2\ni )\n\u0001\n+ m\n\u0003\n+\n\u0011\n,\n(149)\nwhere m > 0 is the margin and [.]+ := max(., 0) is the\nstandard Hinge loss. The ﬁrst term of loss minimizes the\nembedding distances of similar points and the second term\nmaximizes the embedding distances of dissimilar points.\nAs shown in Fig. 5-b, it tries to make the distances of simi-\nlar points as small as possible and the distances of dissimi-\nlar points at least greater than a margin m (because the term\ninside the Hinge loss should become close to zero).\n– Generalized Contrastive Loss: The yi, deﬁned in Eq.\n(148), is used in the contrastive loss, i.e., Eq. (149). This\nvariable is binary and a hard measure of similarity and dis-\nsimilarity. Rather than this hard measure, we can have a\nsoft measure of similarity and dissimilarity, denoted by ψi,\nwhich states how similar x1\ni and x2\ni are. This measure is\nbetween zero (completely similar) and one (completely dis-\nsimilar). It can be either given by the dataset as a hand-set\nmeasure or can be computed using any similarity measure\nsuch as the cosine function:\n[0, 1] ∋ψi := 1\n2\n\u0000−cos(x1\ni , x2\ni ) + 1\n\u0001\n.\n(150)\nIn this case, the pairs {(x1\ni , x2\ni )}b\ni=1 need not be completely\nsimilar or dissimilar points but they can be any two ran-\ndom points from the dataset with some level of similar-\nity/dissimilarity. The generalized contrastive loss general-\nizes the contrastive loss using this soft measure of similar-\nity (Leyva-Vallina et al., 2021):\nminimize\nθ\nb\nX\ni=1\n\u0010\n(1 −ψi)d\n\u0000f(x1\ni ), f(x2\ni )\n\u0001\n+ ψi\n\u0002\n−d\n\u0000f(x1\ni ), f(x2\ni )\n\u0001\n+ m\n\u0003\n+\n\u0011\n.\n(151)\n5.3.5. TRIPLET LOSS\nOne of the losses for Siamese networks with three sub-\nnetworks is the triplet loss (Schroff et al., 2015) which uses\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n31\nthe triplets in mini-batches, denoted by {(xa\ni , xp\ni , xn\ni )}b\ni=1.\nIt is deﬁned as:\nminimize\nθ\nb\nX\ni=1\nh\nd\n\u0000f(xa\ni ), f(xp\ni )\n\u0001\n−d\n\u0000f(xa\ni ), f(xn\ni )\n\u0001\n+ m\ni\n+,\n(152)\nwhere m > 0 is the margin and [.]+ := max(., 0) is the\nstandard Hinge loss. As shown in Fig. 5-c, because of the\nused Hinge loss, this loss makes the distances of dissimilar\npoints greater than the distances of similar points by at least\na margin m; in other words, there will be a distance of at\nleast margin m between the positive and negative points.\nThis loss desires to eventually have:\nd\n\u0000f(xa\ni ), f(xp\ni )\n\u0001\n+ m ≤d\n\u0000f(xa\ni ), f(xn\ni )\n\u0001\n,\n(153)\nfor all triplets. The triplet loss is closely related to the cost\nfunction for spectral large margin metric learning (Wein-\nberger et al., 2006; Weinberger & Saul, 2009) (see Section\n3.2.1). It is also noteworthy that using the triplet loss as\nregularization for cross-entropy loss has been shown to in-\ncrease robustness of network to some adversarial attacks\n(Mao et al., 2019).\n5.3.6. TUPLET LOSS\nIn triplet loss, i.e. Eq. (152), we use one positive and one\nnegative point per anchor point.\nThe tuplet loss (Sohn,\n2016) uses several negative points per anchor point. If k\ndenotes the number of negative points per anchor point and\nxn,j\ni\ndenotes the j-th negative point for xi, the tuplet loss\nis (Sohn, 2016):\nminimize\nθ\nb\nX\ni=1\nk\nX\nj=1\nh\nd\n\u0000f(xa\ni ), f(xp\ni )\n\u0001\n−d\n\u0000f(xa\ni ), f(xn,j\ni\n)\n\u0001\n+ m\ni\n+.\n(154)\nThis loss function pushes multiple negative points away\nfrom the anchor point simultaneously.\n5.3.7. NEIGHBORHOOD COMPONENT ANALYSIS LOSS\nNeighborhood Component Analysis (NCA) (Goldberger\net al., 2005) was originally proposed as a spectral metric\nlearning method (see Section 4.2.1). After the success of\ndeep learning, it was used as the loss function of Siamese\nnetworks where we minimize the negative log-likelihood\nusing Gaussian distribution or the softmax form within the\nmini-batch. Assume we have c classes in every mini-batch.\nWe denote the class index of xi by c(xi) and the data points\nof the j-th class in the mini-batch by Xj. The NCA loss is:\nminimize\nθ\n−\nb\nX\ni=1\nlog\n\u0010\nexp\n\u0000−d\n\u0000f(xa\ni ), f(xp\ni )\n\u0001\u0001\n×\nh\nc\nX\nj=1,j̸=c(xi)\nX\nxn\nj ∈Xj\nexp\n\u0000−d\n\u0000f(xa\ni ) −f(xn\nj )\n\u0001\u0001i−1\u0011\n.\n(155)\nThe numerator minimizes the distances of similar points\nand the denominator maximizes the distances of dissimilar\npoints.\n5.3.8. PROXY NEIGHBORHOOD COMPONENT\nANALYSIS LOSS\nComputation of terms, especially the normalization factor\nin the denominator, is time- and memory-consuming in the\nNCA loss function (see Eq. (155)). Proxy-NCA loss func-\ntions deﬁne some proxy points in the embedding space of\nnetwork and use them in the NCA loss to accelerate com-\nputation and make it memory-efﬁcient (Movshovitz-Attias\net al., 2017). The proxies are representatives of classes in\nthe embedding space and they can be deﬁned in various\nways. The simplest way is to deﬁne the proxy of every\nclass as the mean of embedded points of that class. Of\ncourse, new mini-batches come during training. We can ac-\ncumulate the embedded points of mini-batches and update\nthe proxies after training the network by every mini-batch.\nAnother approach for deﬁning proxies is to cluster the em-\nbedded points into c clusters (e.g., by K-means) and use the\ncentroid of clusters.\nLet the set of proxies be denotes by P whose cardinality\nis the number of classes, i.e., c. Every embedded point is\nassigned to one of the proxies by (Movshovitz-Attias et al.,\n2017):\nΠ(f(xi)) := arg min\nπ∈P ∥f(xi) −π∥2\n2,\n(156)\nor we can assign every point to the proxy of its own class.\nLet pij denote the proxy associated with the j-th class. The\nProxy-NCA loss is the NCA loss, i.e. Eq. (155), but using\nproxies (Movshovitz-Attias et al., 2017):\nminimize\nθ\n−\nb\nX\ni=1\nlog\n\u0010\nexp\n\u0000−d\n\u0000f(xa\ni ), Π(f(xp\ni ))\n\u0001\u0001\n×\nh\nc\nX\nj=1,j̸=c(xi)\nexp\n\u0000−d\n\u0000f(xa\ni ) −πj\n\u0001\u0001i−1\u0011\n.\n(157)\nIt is shown in (Movshovitz-Attias et al., 2017) that the\nProxy-NCA loss, i.e. Eq. (157), is an upper-bound on\nthe NCA loss, i.e. Eq. (155); hence, its minimization also\nachieves the goal of NCA. Comparing Eqs. (155) and (157)\nshows that Proxy-NCA is faster and more efﬁcient than\nNCA because it uses only proxies of negative classes rather\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n32\nthan using all negative points in the mini-batch. Proxy-\nNCA has also been used in feature extraction from medi-\ncal images (Teh & Taylor, 2020). It is noteworthy that we\ncan incorporate temperature scaling (Hinton et al., 2014) in\nthe Proxy-NCA loss. The obtained loss is named Proxy-\nNCA++ (Teh et al., 2020) and is deﬁned as:\nminimize\nθ\n−\nb\nX\ni=1\nlog\n\u0010\nexp\n\u0000−d\n\u0000f(xa\ni ), Π(f(xp\ni ))\n\u0001\n× 1\nτ\n\u0001\n×\nh\nc\nX\nj=1,j̸=c(xi)\nexp\n\u0000−d\n\u0000f(xa\ni ) −πj\n\u0001\n× 1\nτ\n\u0001i−1\u0011\n,\n(158)\nwhere τ > 0 is the temperature which is a hyper-parameter.\n5.3.9. SOFTMAX TRIPLET LOSS\nConsider a mini-batch containing points from c classes\nwhere c(xi) is the class index of xi and Xj denotes the\npoints of the j-th class in the mini-batch. We can use the\nsoftmax function or the Gaussian distribution for the prob-\nability that the point xi takes xj as its neighbor. Similar to\nEq. (84) or Eq. (155), we can have the softmax function\nused in NCA (Goldberger et al., 2005):\npij :=\nexp\n\u0000−d\n\u0000f(xi), f(xj)\n\u0001\u0001\nPb\nk̸=i,k=1 exp\n\u0000−d\n\u0000f(xi), f(xk)\n\u0001\u0001,\nj ̸= i.\n(159)\nAnother approach for the softmax form is to use inner prod-\nuct in the exponent (Ye et al., 2019):\npij :=\nexp\n\u0000f(xi)⊤f(xj)\n\u0001\nPb\nk=1,k̸=i exp\n\u0000f(xi)⊤f(xk)\n\u0001,\nj ̸= i.\n(160)\nThe loss function for training the network can be the nega-\ntive log-likelihood which can be called the softmax triplet\nloss (Ye et al., 2019):\nminimize\nθ\n−\nb\nX\ni=1\n\u0010\nX\nxj∈Xc(xi)\nlog(pij)\n−\nX\nxj̸∈Xc(xi)\nlog(1 −pij)\n\u0011\n.\n(161)\nThis decreases and increases the distances of similar points\nand dissimilar points, respectively.\n5.3.10. TRIPLET GLOBAL LOSS\nThe triplet global loss (Kumar BG et al., 2016) uses the\nmean and variance of the anchor-positive pairs and anchor-\nnegative pairs. It is deﬁned as:\nminimize\nθ\n(σ2\np + σ2\nn) + λ [µp −µn + m]+,\n(162)\nwhere λ > 0 is the regularization parameter, m > 0 is the\nmargin, the means of pairs are:\nµp := 1\nb\nb\nX\ni=1\nd\n\u0000f(xa\ni ), f(xp\ni )\n\u0001\n,\nµn := 1\nb\nb\nX\ni=1\nd\n\u0000f(xa\ni ), f(xn\ni )\n\u0001\n,\nand the variances of pairs are:\nσ2\np := 1\nb\nb\nX\ni=1\n\u0010\nd\n\u0000f(xa\ni ), f(xp\ni )\n\u0001\n−µp\n\u00112\n,\nσ2\nn := 1\nb\nb\nX\ni=1\n\u0010\nd\n\u0000f(xa\ni ), f(xn\ni )\n\u0001\n−µn\n\u00112\n.\nThe ﬁrst term of this loss minimizes the variances of\nanchor-positive and anchor-negative pairs.\nThe second\nterm, however, discriminates the anchor-positive pairs from\nthe anchor-negative pairs. Hence, the negative points are\nseparated from the positive points.\n5.3.11. ANGULAR LOSS\nFor a triplet (xa\ni , xp\ni , xn\ni ), consider a triangle whose ver-\ntices are the anchor, positive, and negative points. To sat-\nisfy Eq. (153) in the triplet loss, the angle at the vertex\nxn\ni should be small so the edge d\n\u0000f(xa\ni ), f(xn\ni )\n\u0001\nbecomes\nlarger than the edge d\n\u0000f(xa\ni ), f(xp\ni )\n\u0001\n. Hence, we need to\nhave and upper bound α > 0 on the angle at the vertex xn\ni .\nIf xc\ni := (xa\ni + xp\ni )/2, the angular loss is deﬁned to be\n(Wang et al., 2017):\nminimize\nθ\nb\nX\ni=1\nh\nd\n\u0000f(xa\ni ), f(xp\ni )\n\u0001\n−4 tan2\u0000α d\n\u0000f(xa\ni ), f(xc\ni)\n\u0001\u0001i\n+.\n(163)\nThis loss reduces the distance of the anchor and positive\npoints and increases the distance of anchor and xc\ni and the\nupper bound α. This increases the distance of the anchor\nand negative points for discrimination of dissimilar points.\n5.3.12. SOFTTRIPLE LOSS\nIf we normalize the points to have unit length, Eq. (153)\ncan be restated by using inner products:\nf(xa\ni )⊤f(xp\ni ) + m ≤f(xa\ni )⊤f(xn\ni ),\n(164)\nwhose margin is not exactly equal to the margin in Eq.\n(153).\nConsider a Siamese network whose last layer’s\nweights are {wl ∈Rp}c\nl=1 where p is the dimensionality of\nthe one-to-last layer and c is the number of classes and the\nnumber of output neurons. We consider k centers for the\nembedding of every class; hence, we deﬁne wj\nl ∈Rp as\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n33\nwl for its j-th center. It is shown in (Qian et al., 2019) that\nsoftmax loss results in Eq. (164). Therefore, we can use the\nSoftTriple loss for training a Siamese network (Qian et al.,\n2019):\nminimize\nθ\n−\nb\nX\ni=1\nlog\n\u0010\nexp(λ(si,yi −δ))\n×\n\u0000exp(λ(si,yi −δ)) +\nc\nX\nl=1,l̸=yi\nexp(λsi,l)\n\u0001−1\u0011\n,\n(165)\nwhere λ, δ > 0 are hyper-parameters, yi is the label of xi,\nand:\nsi,l :=\nk\nX\nj=1\nexp\n\u0000f(xi)⊤wj\nl\n\u0001\nPk\nt=1 exp\n\u0000f(xi)⊤wt\nl\n\u0001f(xi)⊤wk\nl .\nThis loss increases and decreases the intra-class and inter-\nclass distances, respectively.\n5.3.13. FISHER SIAMESE LOSSES\nFisher Discriminant Analysis (FDA) (Fisher, 1936; Gho-\njogh et al., 2019b) decreases the intra-class variance and\nincreases the inter-class variance by maximizing the Fisher\ncriterion. This idea is very similar to the idea of loss func-\ntions for Siamese networks. Hence, we can combine the\nmethods of FDA and Siamese loss functions.\nConsider a Siamese network whose last layer is denoted by\nthe projection matrix U. We consider the features of the\none-to-last layer in the mini-batch. The covariance matri-\nces of similar points and dissimilar points (one-to-last layer\nfeatures) in the mini-batch are denoted by SW and SB.\nThese covariances become U ⊤SW U and U ⊤SBU, re-\nspectively, after the later layer’s projection because of the\nquadratic characteristic of covariance. As in FDA, we can\nmaximize the Fisher criterion or equivalently minimize the\nnegative Fisher criterion:\nminimize\nU\ntr(U ⊤SW U) −tr(U ⊤SBU).\nThis problem is ill-posed because it increases the to-\ntal covariance of embedded data to increase the term\ntr(U ⊤SBU). Hence, we add minimization of the total\ncovariance as the regularization term:\nminimize\nU\ntr(U ⊤SW U) −tr(U ⊤SBU)\n+ ϵtr(U ⊤ST U),\nwhere ϵ ∈(0, 1) is the regularization parameter and ST is\nthe covariance of all points of the mini-batch in the one-to-\nlast layer. The total scatter can be written as the summation\nof SW and SB; hence:\ntr(U ⊤SW U) −tr(U ⊤SBU) + ϵtr(U ⊤ST U)\n= tr\n\u0000U ⊤(SW −SW + ϵSW + ϵSB)U\n\u0001\n= (2 −λ)tr(U ⊤SW U) −λtr(U ⊤SBU),\nwhere λ := 1 −ϵ. Inspired by Eq. (152), we can have the\nfollowing loss, named the Fisher discriminant triplet loss\n(Ghojogh et al., 2020f):\nminimize\nθ\nh\n(2 −λ)tr(U ⊤SW U)\n−λtr(U ⊤SBU) + m\ni\n+,\n(166)\nwhere m > 0 is the margin. Backpropagating the error of\nthis loss can update both U and other layers of network.\nNote that the summation over the mini-batch is integrated\nin the computation of covariance matrices SW and SB.\nInspired by Eq. (149), we can also have the Fisher discrim-\ninant contrastive loss (Ghojogh et al., 2020f):\nminimize\nθ\n(2 −λ)tr(U ⊤SW U)\n+\n\u0002\n−λtr(U ⊤SBU) + m\n\u0003\n+.\n(167)\nNote that the variable yi used in the contrastive loss (see Eq.\n(148)) is already used in computation of the covariances\nSW and SB. There exist some other loss functions inspired\nby Fisher discriminant analysis but they are not used for\nSiamese networks. Those methods will be introduced in\nSection 5.4.\n5.3.14. DEEP ADVERSARIAL METRIC LEARNING\nIn deep adversarial metric learning (Duan et al., 2018), neg-\native points are generated in an adversarial learning (Good-\nfellow et al., 2014; Ghojogh et al., 2021b). In this method,\nwe have a generator G(.) which tries to generate nega-\ntive points fooling the metric learning. Using triplet inputs\n{(xa\ni , xp\ni , xn\ni )}b\ni=1, the loss function of generator is (Duan\net al., 2018):\nLG :=\nb\nX\ni=1\n\u0010\n∥G(xa\ni , xp\ni , xn\ni ) −xa\ni ∥2\n2\n+ λ1∥G(xa\ni , xp\ni , xn\ni ) −xn\ni ∥2\n2\n+ λ2\n\u0002\nd(f(xa\ni ), f(G(xa\ni , xp\ni , xn\ni )))\n−d(f(xa\ni ), f(xp\ni )) + m\n\u0003\n+\n\u0011\n,\n(168)\nwhere λ1, λ2 > 0 are the regularization parameters. This\nloss makes the generated negative point close to the real\nnegative point (to be negative) and the anchor point (for\nfooling metric learning adversarially).\nThe Hinge loss\nmakes the generated negative point different from the an-\nchor and positive points so it also acts like a real negative.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n34\nIf LM denotes any loss function for Siamese network, such\nas the triplet loss, the total loss function in deep adversarial\nmetric learning is minimizing LG+λ3LM where λ3 > 0 is\nthe regularization parameter (Duan et al., 2018). It is note-\nworthy that there exists another adversarial metric learning\nwhich is not for Siamese networks but for cross-modal data\n(Xu et al., 2019a).\n5.3.15. TRIPLET MINING\nIn every mini-batch containing data points from c classes,\nwe can select and use triplets of data points in different\nways. For example, we can use all similar and dissimi-\nlar points for every anchor point as positive and negative\npoints, respectively. Another approach is to only use some\nof the similar and dissimilar points within the mini-batch.\nThese approaches for selecting and using triplets are called\ntriplet mining (Sikaroudi et al., 2020a). In the following,\nwe review some of the most important triplet mining meth-\nods. We use triplet mining methods for the triplet loss, i.e.,\nEq. (152). Suppose b is the mini-batch size, c(xi) is the\nclass index of xi, Xj denotes the points of the j-th class in\nthe mini-batch, and X denotes the data points in the mini-\nbatch.\n– Batch-all: Batch-all triplet mining (Ding et al., 2015)\nconsiders every point in the mini-batch as an anchor point.\nAll points in the mini-batch which are in the same class the\nanchor point are used as positive points. All points in the\nmini-batch which are in a different class from the class of\nanchor point are used as negative points:\nminimize\nθ\nb\nX\ni=1\nX\nxj∈Xc(xi)\nX\nxk∈X\\Xc(xi)\nh\nd\n\u0000f(xi), f(xj)\n\u0001\n−d\n\u0000f(xi), f(xk)\n\u0001\n+ m\ni\n+.\n(169)\nBatch-all mining makes use of all data points in the mini-\nbatch to utilize all available information.\n– Batch-hard: Batch-hard triplet mining (Hermans et al.,\n2017) considers every point in the mini-batch as an anchor\npoint. The hardest positive, which is the farthest point from\nthe anchor point in the same class, is used as the positive\npoint. The hardest negative, which is the closest point to\nthe anchor point from another class, is used as the negative\npoint:\nminimize\nθ\nb\nX\ni=1\nh\nmax\nxj∈Xc(xi) d\n\u0000f(xi), f(xj)\n\u0001\n−\nmin\nxk∈X\\Xc(xi)\nd\n\u0000f(xi), f(xk)\n\u0001\n+ m\ni\n+.\n(170)\nBath-hard mining uses hardest points so that the net-\nwork learns the hardest cases.\nBy learning the hardest\ncases, other cases are expected to be learned properly.\nLearning the hardest cases can also be justiﬁed by the\nopposition-based learning (Tizhoosh, 2005).\nBatch-hard\nmining has been used in many applications such as person\nre-identiﬁcation (Wang et al., 2019).\n– Batch-semi-hard:\nBatch-semi-hard triplet mining\n(Schroff et al., 2015) considers every point in the mini-\nbatch as an anchor point. All points in the mini-batch which\nare in the same class the anchor point are used as positive\npoints. The hardest negative (closest to the anchor point\nfrom another class), which is farther than the positive point,\nis used as the negative point:\nminimize\nθ\nb\nX\ni=1\nX\nxj∈Xc(xi)\nh\nd\n\u0000f(xi), f(xj)\n\u0001\n−\nmin\nxk∈X\\Xc(xi)\n\b\nd\n\u0000f(xi), f(xk)\n\u0001\n|\nd\n\u0000f(xi), f(xk)\n\u0001\n> d\n\u0000f(xi), f(xj)\n\u0001\t\n+ m\ni\n+.\n(171)\n– Easy-positive: Easy-positive triplet mining (Xuan et al.,\n2020) considers every point in the mini-batch as an anchor\npoint. The easiest positive (closest to the anchor point from\nthe same class) is used as the positive point. All points in\nthe mini-batch which are in a different class from the class\nof anchor point are used as negative points:\nminimize\nθ\nb\nX\ni=1\nX\nxk∈X\\Xc(xi)\nh\nmin\nxj∈Xc(xi) d\n\u0000f(xi), f(xj)\n\u0001\n−d\n\u0000f(xi), f(xk)\n\u0001\n+ m\ni\n+.\n(172)\nWe can use this triplet mining approach in NCA loss func-\ntion such as in Eq. (160). For example, we can have (Xuan\net al., 2020):\nminimize\nθ\nb\nX\ni=1\n\u0012\nmin\nxj∈Xc(xi) exp\n\u0000f(xi)⊤f(xj)\n\u0001\n×\n\u0010\nmin\nxj∈Xc(xi) exp\n\u0000f(xi)⊤f(xj)\n\u0001\n+\nX\nxk∈X\\Xc(xi)\nexp\n\u0000f(xi)⊤f(xk)\n\u0001\u0011−1\u0013\n,\n(173)\nwhere the embeddings for all points of the mini-batch are\nnormalized to have length one.\n– Lifted embedding loss:\nThe lifted embedding loss\n(Oh Song et al., 2016) is related to the anchor-positive dis-\ntance and the smallest (hardest) anchor-negative distance:\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n35\nminimize\nθ\nb\nX\ni=1\nX\nxj∈Xc(xi)\n\u0010h\nd(f(xi), f(xj))\n+ max\n\u0010\nmax\nxk∈X\\Xc(xi)\n\b\nm −d(f(xi), f(xk))\n\t\n,\nmax\nxl∈X\\Xc(xj )\n\b\nm −d(f(xj), f(xl))\n\t\u0011i\n+\n\u00112\n,\n(174)\nThis loss is using triplet mining because of using extreme\ndistances. Alternatively, another version of this loss func-\ntion uses logarithm and exponential operators (Oh Song\net al., 2016):\nminimize\nθ\nb\nX\ni=1\nX\nxj∈Xc(xi)\n\u0010h\nd(f(xi), f(xj))\n+ log\n\u0010\nX\nxk∈X\\Xc(xi)\nexp\n\u0000m −d(f(xi), f(xk))\n\u0001\n,\nX\nxl∈X\\Xc(xj )\nexp\n\u0000m −d(f(xj), f(xl))\n\u0001\u0011i\n+\n\u00112\n.\n(175)\n– Hard mining center-triplet loss: Let the mini-batch\ncontain data points from c classes.\nHard mining cen-\nter–triplet loss (Lv et al., 2019) considers the mean of ev-\nery class as an anchor point. The hardest (farthest) positive\npoint and the hardest (closest) negative point are used in\nthis loss as (Lv et al., 2019):\nminimize\nθ\nc\nX\nl=1\nh\nmax\nxj∈Xc(¯xl)\nd\n\u0000f(¯xl), f(xj)\n\u0001\n−\nmin\nxk∈X\\Xc(¯xl)\nd\n\u0000f(¯xl), f(xk)\n\u0001\n+ m\ni\n+.\n(176)\nwhere ¯xl denotes the mean of the l-th class.\n– Triplet loss with cross-batch memory: A version of\ntriplet loss can be (Wang et al., 2020a):\nminimize\nθ\nb\nX\ni=1\n\u0012\n−\nX\nxj∈Xc(xi)\nf(xi)⊤f(xj)\n+\nX\nxk∈X\\Xc(xi)\nf(xi)⊤f(xk)\n\u0013\n.\n(177)\nThis triplet loss can use a cross-batch memory where we\naccumulate a few latest mini-batches. Every coming mini-\nbatch updates the memory. Let the capacity of the memory\nbe w points and the mini-batch size be b. Let exi denote the\ni-th data point in the memory. The triplet loss with cross-\nbatch memory is deﬁned as (Wang et al., 2020a):\nminimize\nθ\nb\nX\ni=1\n\u0012\n−\nX\nexj∈Xc(xi)\nf(xi)⊤f(exj)\n+\nX\nexk∈X\\Xc(xi)\nf(xi)⊤f(exk)\n\u0013\n,\n(178)\nwhich takes the positive and negative points from the mem-\nory rather than from the coming mini-batch.\n5.3.16. TRIPLET SAMPLING\nRather than using the extreme (hardest or easiest) posi-\ntive and negative points (Sikaroudi et al., 2020a), we can\nsample positive and negative points from the points in the\nmini-batch or from some distributions. There are several\napproaches for the positive and negative points to be sam-\npled (Ghojogh, 2021):\n• Sampled by extreme distances of points,\n• Sampled randomly from classes,\n• Sampled by distribution but from existing points,\n• Sampled stochastically from distributions of classes.\nThese approaches are used for triplet sampling. The ﬁrst\napproach was introduced in Section 5.3.15. The ﬁrst, sec-\nond, and third approaches sample the positive and negative\npoints from the set of points in the mini-batch. This type of\nsampling is called survey sampling (Ghojogh et al., 2020e).\nThe third and fourth approaches sample points from distri-\nbutions stochastically. In the following, we introduce some\nof the triplet sampling methods.\n– Distance weighted sampling: Distance weighted sam-\npling (Wu et al., 2017) is a method in the third approach,\ni.e., sampling by distribution but from existing points. The\ndistribution of the pairwise distances is proportional to (Wu\net al., 2017):\nP\n\u0000d(f(xi), f(xj))\n\u0001\n∼\n\u0000d(f(xi), f(xj))\n\u0001p−2×\n\u0010\n1 −0.25\n\u0000d(f(xi), f(xj))\n\u00012\u0011(b−3)/2\n,\nwhere b is the number of points in the mini-batch and p is\nthe dimensionality of embedding space (i.e., the number of\nneurons in the last layer of the Siamese network). In ev-\nery mini-batch, we consider every point once as an anchor\npoint. For an anchor point, we consider all points of the\nmini-batch which are in a different class as candidates for\nthe negative point. We sample a negative point, denoted by\nxn\n∗from these candidates (Wu et al., 2017):\nxn\n∗∼min\n\u0010\nλ, P−1\u0000d(f(xi), f(xj))\n\u0001\u0011\n,\n∀j ̸= i,\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n36\nwhere λ > 0 is a hyperparameter to ensure that all can-\ndidates have a chance to be chosen. This sampling is per-\nformed for every mini-batch. The loss function in distance\nweighted sampling is (Wu et al., 2017):\nminimize\nθ\nb\nX\ni=1\nX\nxj∈Xc(xi)\nh\nd\n\u0000f(xi), f(xj)\n\u0001\n−d\n\u0000f(xi), f(xn\n∗)\n\u0001\n+ m\ni\n+.\n(179)\n– Sampling by Bayesian updating theorem: We can\nsample triplets from distributions of classes which is the\nforth approach of sampling, mentioned above. One method\nfor this sampling is using the Bayesian updating theorem\n(Sikaroudi et al., 2021) which is updating the posterior by\nthe Bayes’ rule from some new data. In this method, we as-\nsume p-dimensional Gaussian distribution for every class\nin the embedding space where p is the dimensionality of\nembedding space. We accumulate the embedded points for\nevery class when the new mini-batches are introduced to\nthe network. The distributions of classes are updated based\non both the existing points available so far and the new-\ncoming data points. It can be shown that the posterior of\nmean and covariance of a Gaussian distribution is a normal\ninverse Wishart distribution (Murphy, 2007). The mean\nand covariance of a Gaussian distribution have a general-\nized Student-t distribution and inverse Wishart distribution,\nrespectively (Murphy, 2007). Let the so-far available data\nhave sample size n0, mean µ0, and covariance Σ0. Also,\nlet the newly coming data have sample size n′, mean µ′,\nand covariance Σ′. We update the mean and covariance by\nexpectation of these distributions (Sikaroudi et al., 2021):\nµ0 ←E(µ | x0) = n′µ′ + n0µ0\nn′ + n0\n,\nΣ0 ←E(Σ | x0) =\nΥ−1\nn′+n0−p−1,\n∀n′+n0 >p+1,\nwhere:\nRd×d ∋Υ := n′Σ′ + n0Σ0\n+\nn′\n1n0\nn′\n1 + n0\n(µ0 −µ′)(µ0 −µ′)⊤.\nThe updated mean and covariance are used for Gaussian\ndistributions of the classes. Then, we sample triplets from\nthe distributions of classes rather than from the points of\nmini-batch. We consider every point of the new mini-batch\nas an anchor point and sample a positive point from the dis-\ntribution of the same class. We sample c−1 negative points\nfrom the distributions of c −1 other classes. If this triplet\nsampling procedure is used with triplet and contrastive loss\nfunctions, the approach is named Bayesian Updating with\nTriplet loss (BUT) and Bayesian Updating with NCA loss\n(BUNCA) (Sikaroudi et al., 2021).\n– Hard negative sampling: Let the anchor, positive, and\nnegative points be denoted by xa, xp, and xn, respectively.\nConsider the following distributions for the negative and\npositive points (Robinson et al., 2021):\nP(xn) ∝αPn(xn) + (1 −α)Pp(xn),\nPn(x) ∝exp\n\u0000βf(xa)⊤f(x)\n\u0001\nP(x|c(x) ̸= c(xa)),\nPp(x) ∝exp\n\u0000βf(xa)⊤f(x)\n\u0001\nP(x|c(x) = c(xa)),\nwhere α ∈(0, 1) is a hyper-parameter. The loss function\nwith hard negative sampling is (Robinson et al., 2021):\nminimize\nθ\n−\nb\nX\ni=1\nExp∼Pp(x) log\n\u0012\nexp\n\u0000f(xa\ni )⊤f(xp)\n\u0001\n\u0010\nexp\n\u0000f(xa\ni )⊤f(xp)\n\u0001\n+ Exn∼P(xn)\n\u0002\nexp\n\u0000f(xa\ni )⊤f(xn)\n\u0001\u0003\u0011−1\u0013\n,\n(180)\nwhere positive and negative points are sampled from posi-\ntive and negative distributions deﬁned above. The expecta-\ntions can be estimated using the Monte Carlo approxima-\ntion (Ghojogh et al., 2020e). This time of triplet sampling\nis a method in the fourth type of triplet sampling, i.e., sam-\npling stochastically from distributions of classes.\n5.4. Deep Discriminant Analysis Metric Learning\nDeep discriminant analysis metric learning methods use the\nidea of Fisher discriminant analysis (Fisher, 1936; Gho-\njogh et al., 2019b) in deep learning, for learning an embed-\nding space which separates classes. Some of these meth-\nods are deep probabilistic discriminant analysis (Li et al.,\n2019), discriminant analysis with virtual samples (Kim &\nSong, 2021), Fisher Siamese losses (Ghojogh et al., 2020f),\nand deep Fisher discriminant analysis (D´ıaz-Vico et al.,\n2017; D´ıaz-Vico & Dorronsoro, 2019). The Fisher Siamese\nlosses were already introduced in Section 5.3.13.\n5.4.1. DEEP PROBABILISTIC DISCRIMINANT ANALYSIS\nDeep probabilistic discriminant analysis (Li et al., 2019)\nminimizes the inverse Fisher criterion:\nminimize\nθ\nE[tr(cov(f(x)|y))]\ntr(cov(E[f(x)|y])) =\nPb\ni=1 E[var(f(xi)|yi)]\nPb\ni=1 var(E[f(xi)|yi])\n(a)\n=\nPb\ni=1 E[var(f(xi)|yi)]\nPb\ni=1\n\u0000var(f(xi)) −E[var(f(xi)|yi)]\n\u0001\n(b)\n=\nPb\ni=1\nPc\nl=1 P(y = l)var(f(xi)|yi = l)\nPb\ni=1\n\u0000var(f(xi)) −Pc\nl=1 P(y = l)var(f(xi)|yi = l)\n\u0001,\n(181)\nwhere b is the mini-batch size, c is the number of classes,\nyi is the class label of xi, cov(.) denotes covariance, var(.)\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n37\ndenotes variance, P(y = l) is the prior of the l-th class (es-\ntimated by the ratio of class population to the total number\nof points in the mini-batch), (a) is because of the law of\ntotal variance, and (b) is because of the deﬁnition of ex-\npectation. The numerator and denominator represent the\nintra-class and inter-class variances, respectively.\n5.4.2. DISCRIMINANT ANALYSIS WITH VIRTUAL\nSAMPLES\nIn discriminant analysis metric learning with virtual sam-\nples (Kim & Song, 2021), we consider any backbone net-\nwork until the one-to-last layer of neural network and a last\nlayer with linear activation function. Let the outputs of the\none-to-last layer be denoted by {f′(xi)}b\ni=1 and the weights\nof the last layer be U. We compute the intra-class scatter\nSW and inter-class scatter SB for the one-to-last layer’s\nfeatures {f′(xi)}b\ni=1. If we see the last layer as a Fisher\ndiscriminant analysis model with projection matrix U, the\nsolution is the eigenvalue problem (Ghojogh et al., 2019a)\nfor S−1\nW SB. Let λj denote the j-th eigenvalue of this prob-\nlem.\nAssume Sb and Db denote the similar and dissimilar points\nin the mini-batch where |Sb| = |Db| = q. We deﬁne (Kim\n& Song, 2021):\ngp := [exp(−f′(xi)⊤f′(xj)) | (xi, xj) ∈Sb]⊤∈Rq,\ngn := [exp(−f′(xi)⊤f′(xj)) | (xi, xj) ∈Db]⊤∈Rq,\nsctr := 1\n2q\nq\nX\ni=1\n\u0000gp(i) + gn(i)\n\u0001\n,\nwhere g(i) is the i-th element of g. We sample q num-\nbers, namely virtual samples, from the uniform distribution\nU(sctr −ϵ¯λ, sctr + ϵ¯λ) where ϵ is a small positive num-\nber and ¯λ is the mean of eigenvalues λj’s. The q virtual\nsamples are put in a vector r ∈Rq.\nThe loss function for discriminant analysis with virtual\nsamples is (Kim & Song, 2021):\nminimize\nθ,U\n1\nq\nq\nX\ni=1\nh1\nq gp(i) ∥r∥1 −1\nq gn(i) ∥r∥1 + m\ni\n+\n−10−5 tr(U ⊤SBU)\ntr(U ⊤SW U)\n,\n(182)\nwhere ∥.∥1 is the ℓ1 norm, [.]+ := max(., 0), m > 0 is the\nmargin, and the second term is maximization of the Fisher\ncriterion.\n5.4.3. DEEP FISHER DISCRIMINANT ANALYSIS\nIt is shown in (Hart et al., 2000) that the solution to the fol-\nlowing least squares problem is equivalent to the solution\nof Fisher discriminant analysis:\nminimize\nw0∈Rc,W ∈Rd×c\n1\n2∥Y −1n×1w⊤\n0 −XW ∥2\nF ,\n(183)\nwhere ∥.∥F is the Frobenius norm, X ∈Rn×d is the row-\nwise stack of data points, Y := HEΠ−(1/2) ∈Rn×c\nwhere H := I −(1/n)11⊤∈Rn×n is the centering ma-\ntrix, E ∈{0, 1}n×c is the one-hot-encoded labels stacked\nrow-wise, Π ∈Rc×c is the diagonal matrix whose (l, l)-th\nelement is the cardinality of the l-th class.\nDeep Fisher discriminant analysis (D´ıaz-Vico et al., 2017;\nD´ıaz-Vico & Dorronsoro, 2019) implements Eq. (183) by\na nonlinear neural network with loss function:\nminimize\nθ\n1\n2∥Y −f(X; θ)∥2\nF ,\n(184)\nwhere θ is the weights of network, X ∈Rn×d denotes\nthe row-wise stack of points in the mini-batch of size b,\nY := HEΠ−(1/2) ∈Rb×c is computed in every mini-\nbatch, and f(.) ∈Rb×c is the row-wise stack of output em-\nbeddings of the network. After training, the output f(x) is\nthe embedding for the input point x.\n5.5. Multi-Modal Deep Metric Learning\nData has several modals where a separate set of features\nis available for every modality of data. In other words,\nwe can have several features for every data point. Note\nthat the dimensionality of features may differ. Multi-modal\ndeep metric learning (Roostaiyan et al., 2017) addresses\nthis problem in metric learning. Let m denote the num-\nber of modalities. Consider m stacked autoencoders each\nof which is for one of the modalities. The l-th autoencoder\ngets the l-th modality of the i-th data point, denoted by xl\ni,\nand reconstructs it as output, denoted by bxl\ni. The embed-\nding layer, or the layer between encoder and decoder, is\nshared between all m autoencoders. We denote the output\nof this shared embedding layer by f(xi). The loss function\nfor training the m stacked autoencoders with the shared\nembedding layer can be (Roostaiyan et al., 2017):\nminimize\nθ\nb\nX\ni=1\nm\nX\nl=1\n∥xl\ni −bxl\ni∥2\n2\n+ λ1\nb\nX\ni=1\nX\nxj∈Xc(xi)\n\u0002\nd(f(xi), f(xj)) −m1\n\u0003\n+\n+ λ2\nb\nX\ni=1\nX\nxj∈X\\Xc(xi)\n\u0002\n−d(f(xi), f(xj)) + m2\n\u0003\n+,\n(185)\nwhere λ1, λ2 > 0 are the regularization parameters and\nm1, m2 > 0 are the margins.\nThe ﬁrst term is the re-\nconstruction loss and the second and third terms are for\nmetric learning which collapses each class to a margin m1\nand discriminates classes by a margin m2. This loss func-\ntion is optimized in a stacked autoencoder setup (Hinton &\nSalakhutdinov, 2006; Wang et al., 2014). Then, it is ﬁne-\ntuned by backpropagation (Ghojogh et al., 2021f). After\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n38\ntraining, the embedding layer can be used for embedding\ndata points. Note that another there exists another multi-\nmodal deep metric learning, which is (Xu et al., 2019a).\n5.6. Geometric Metric Learning by Neural Network\nThere exist some works, such as (Huang & Van Gool,\n2017), (Hauser, 2017), and (Hajiabadi et al., 2019),\nwhich have implemented neural networks on the Rieman-\nnian manifolds.\nLayered geometric learning (Hajiabadi\net al., 2019) implements Geometric Mean Metric Learn-\ning (GMML) (Zadeh et al., 2016) (recall Section 3.7.1) in\na neural network framework. In this method, every layer\nof network is a metric layer which projects the output of\nits previous layer onto the subspace of its own metric (see\nProposition 2 and Proposition 8).\nFor the l-th layer of network, we denote the weight matrix\n(i.e., the projection matrix of metric) and the output of layer\nfor the i-th data point by U l and xi,l, respectively. Hence,\nthe metric in the l-th layer models ∥xi,l −xj,l∥U lU ⊤\nl . Con-\nsider the dataset of n points X ∈Rd×n. We denote the\noutput of the l-th layer by Xl ∈Rd×n. The projection of a\nlayer onto its metric subspace is Xl = U ⊤\nl Xl−1.\nEvery layer solves the optimization problem of GMML\n(Zadeh et al., 2016),\ni.e.,\nEq.\n(61).\nFor this,\nwe\nstart\nfrom\nthe\nﬁrst\nlayer\nand\nproceed\nto\nthe\nlast layer by feed-propagation.\nThe l-th layer com-\nputes ΣS and ΣD for Xl−1 by Eq.\n(14).\nThen,\nthe solution of optimization (61) is computed which\nis the Eq.\n(65),\ni.e.,\nW l\n=\nΣ−1\nS ♯(1/2)ΣD\n=\nΣ(−1/2)\nS\n\u0000Σ(1/2)\nS\nΣDΣ(1/2)\nS\n\u0001(1/2)Σ(−1/2)\nS\n. Then, using Eq.\n(9), we decompose the obtained W l to ﬁnd U l. Then,\ndata points are projected onto the metric subspace as Xl =\nU ⊤\nl Xl−1.\nIf we want the output of layers lie on the positive semi-\ndeﬁnite manifold, the activation function of every layer can\nbe projection onto the positive semi-deﬁnite cone (Ghojogh\net al., 2021c):\nXl := V diag(max(λ1, 0), . . . , max(λd, 0)) V ⊤,\nwhere V and {λ1, . . . , λd} are the eigenvectors and eigen-\nvalues of Xl, respectively.\nThis activation function is\ncalled the eigenvalue rectiﬁcation layer in (Huang &\nVan Gool, 2017). Finally, it is noteworthy that there is an-\nother work, named backprojection (Ghojogh et al., 2020d),\nwhich has similar idea but in the Euclidean and Hilbert\nspaces and not in the Riemannian space.\n5.7. Few-shot Metric Learning\nFew-shot learning refers to learning from a few data points\nrather than from a large enough dataset. Few-shot learn-\ning is used for domain generalization to be able to use for\nunseen data in the test phase (Wang et al., 2020b). The\ntraining phase of few-shot learning is episodic where in\nevery iteration or so-called episode of training, we have a\nsupport set and a query set. In other words, the training\ndataset is divided into mini-batches where every mini-batch\ncontains a support set and a query set (Triantaﬁllou et al.,\n2020). Consider a training dataset with ctr classes and a\ntest dataset with cte classes. As mentioned before, test and\ntraining datasets are usually disjoint in few-shot learning\nso it is useful for domain generalization. In every episode,\nalso called the task or the mini-batch, we train using some\n(and not all) training classes by randomly sampling from\nclasses.\nThe support set is Ss := {(xs,i, ys,i)}|Ss|\ni=1 where x and y\ndenote the data point and its label, respectively. The query\nset is Sq := {(xq,i, yq,i)}|Sq|\ni=1. The training data of every\nepisode (mini-batch) is the union of the support and query\nsets. At every episode, we randomly sample cs classes out\nof the total ctr classes of training dataset, where we usually\nhave cs ≪ctr. Then, we sample ks training data points\nfrom these cs selected classes. These cs × ks = |Ss| form\nthe support set. This few-shot setup is called cs-way, ks-\nshot in which the support set contains cs classes and ks\npoints in every class. The number of classes and every\nclass’s points in the query set of every episode may or may\nnot be the same as in the support set.\nIn every episode of the training phase of few-shot learning,\nwe update the network weights by back-propagating error\nusing the support set. These updated weights are not ﬁnal-\nized yet. We feed the query set to the network with the up-\ndated weights and back-propagate error using the query set.\nThis second back-propagation with the query set updates\nthe weights of network ﬁnally at the end of episode. In\nother words, the query set is used to evaluate how good the\nupdate by support set are. This learning procedure for few-\nshot learning is called meta-learning (Finn et al., 2017).\nThere are several family of methods for few-shot learning,\none of which is some deep metric learning methods. Vari-\nous metric learning methods have been proposed for learn-\ning from few-shot data. For example, Siamese network,\nintroduced in Section 5.3, has been used for few-shot learn-\ning (Koch et al., 2015; Li et al., 2020). In the following, we\nintroduce two metric learning methods for few-shot learn-\ning.\n5.7.1. MULTI-SCALE METRIC LEARNING\nMulti-scale metric learning (Jiang et al., 2020) learns the\nembedding space by learning multiple scales of middle fea-\ntures in the training process. It has several steps. In the ﬁrst\nstep, we use a pre-trained network with multiple output lay-\ners which produce several different scales of features for\nboth the support and query sets. In the second step, within\nevery scale of support set, we take average of the ks fea-\ntures in every class. This gives us cs features for every\nscale in the support set. This and the features of the query\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n39\nset are fed to the third step. In the third step, we feed ev-\nery scale to a sub-network where larger scales are fed to\nsub-networks with more number of layers as they contain\nmore information to process. These sub-networks are con-\ncatenated to give a scalar output for every data point with\nmultiple scales of features. Hence, we obtain a scalar score\nfor every data point in the support and query sets. Finally,\na combination of a classiﬁcation loss function, such as the\ncross-entropy loss (see Eq. (145)), and triplet loss (see Eq.\n152) is used in the support-query setup explained before.\n5.7.2. METRIC LEARNING WITH CONTINUOUS\nSIMILARITY SCORES\nAnother few-shot metric learning is (Xu et al., 2019b)\nwhich takes pairs of data points as the input support and\nquery sets. For the pair (xi, xj), consider binary similarity\nscore, yij, deﬁned as:\nyij :=\n\u001a 1\nif (xi, xj) ∈S\n0\nif (xi, xj) ∈D.\n(186)\nwhere S and D denote the sets of similar and dissimilar\npoints, respectively. We can deﬁne continuous similarity\nscore, y′\nij, as (Xu et al., 2019b):\ny′\nij :=\n\u001a\n(β −1)d(xi, xj) + 1\nif (xi, xj) ∈S\n−αd(xi, xj) + α\nif (xi, xj) ∈D,\n(187)\nwhere 0 < α < β < 1 and d(xi, xj) is the normalized\nsquared Euclidean distance (we normalize distances within\nevery mini-batch). The ranges of these continuous similar-\nities are:\ny′\nij ∈\n\u001a [β, 1]\nif (xi, xj) ∈S\n[0, α]\nif (xi, xj) ∈D.\nIn every episode (mini-batch), the pairs are fed to a net-\nwork with several feature vector outputs. For every pair\n(xi, xj), these feature vectors are fed to another network\nwhich outputs a scalar similarity score sij. The loss func-\ntion of metric learning in this method is (Xu et al., 2019b):\nmaximize\nθ\nX\n(xi,xj)∈X\n(1 + λ)(sij −y′\nij)2,\nsubject to\nβ ≤sij, y′\nij ≤1\nif\nyij = 1,\n0 ≤sij, y′\nij ≤α\nif\nyij = 0,\n(188)\nwhere λ > 0 is the regularization parameter and X is\nthe mini-batch of the support or query set depending on\nwhether it is the phase of support or query.\n6. Conclusion\nThis was a tutorial and survey on spectral, probabilistic,\nand deep metric learning. We started with deﬁning dis-\ntance metric. In spectral methods, we covered methods us-\ning scatters of data, methods using Hinge loss, locally lin-\near metric adaptation, kernel methods, geometric methods,\nand adversarial metric learning. In probabilistic category,\nwe covered collapsing classes, neighborhood component\nanalysis, Bayesian metric learning, information theoretic\nmethods, and empirical risk minimization approaches. In\ndeep learning methods, we explain reconstruction autoen-\ncoders, supervised loss functions, Siamese networks, deep\ndiscriminant analysis methods, multi-modal learning, geo-\nmetric deep metric learning, and few-shot metric learning.\nReferences\nAbsil, P-A, Mahony, Robert, and Sepulchre, Rodolphe.\nOptimization algorithms on matrix manifolds. Princeton\nUniversity Press, 2009.\nAlipanahi, Babak, Biggs, Michael, and Ghodsi, Ali. Dis-\ntance metric learning vs. Fisher discriminant analysis. In\nProceedings of the 23rd national conference on Artiﬁcial\nintelligence, volume 2, pp. 598–603, 2008.\nArsigny, Vincent, Fillard, Pierre, Pennec, Xavier, and Ay-\nache, Nicholas. Geometric means in a novel vector space\nstructure on symmetric positive-deﬁnite matrices. SIAM\njournal on matrix analysis and applications, 29(1):328–\n347, 2007.\nBaghshah,\nMahdieh\nSoleymani\nand\nShouraki,\nSaeed Bagheri.\nSemi-supervised metric learning\nusing pairwise constraints. In Twenty-First International\nJoint Conference on Artiﬁcial Intelligence, 2009.\nBaghshah,\nMahdieh\nSoleymani\nand\nShouraki,\nSaeed Bagheri.\nKernel-based metric learning for\nsemi-supervised clustering. Neurocomputing, 73(7-9):\n1352–1361, 2010.\nBar-Hillel, Aharon, Hertz, Tomer, Shental, Noam, and\nWeinshall, Daphna. Learning distance functions using\nequivalence relations. In Proceedings of the 20th inter-\nnational conference on machine learning (ICML-03), pp.\n11–18, 2003.\nBar-Hillel, Aharon, Hertz, Tomer, Shental, Noam, Wein-\nshall, Daphna, and Ridgeway, Greg. Learning a maha-\nlanobis metric from equivalence constraints. Journal of\nmachine learning research, 6(6), 2005.\nBelkin, Mikhail and Niyogi, Partha. Laplacian eigenmaps\nand spectral techniques for embedding and clustering. In\nAdvances in neural information processing systems, vol-\nume 14, pp. 585–591, 2001.\nBelkin, Mikhail and Niyogi, Partha. Laplacian eigenmaps\nand spectral techniques for embedding and clustering. In\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n40\nAdvances in neural information processing systems, pp.\n585–591, 2002.\nBellet, Aur´elien, Habrard, Amaury, and Sebban, Marc. A\nsurvey on metric learning for feature vectors and struc-\ntured data. arXiv preprint arXiv:1306.6709, 2013.\nBellet, Aur´elien, Habrard, Amaury, and Sebban, Marc.\nMetric learning. Synthesis Lectures on Artiﬁcial Intel-\nligence and Machine Learning, 9(1):1–151, 2015.\nBhatia, Rajendra. Positive deﬁnite matrices. Princeton uni-\nversity press, 2007.\nBhutani, Mukul, Jawanpuria, Pratik, Kasai, Hiroyuki, and\nMishra, Bamdev.\nLow-rank geometric mean metric\nlearning. arXiv preprint arXiv:1806.05454, 2018.\nBoudiaf, Malik, Rony, J´erˆome, Ziko, Imtiaz Masud,\nGranger, Eric, Pedersoli, Marco, Piantanida, Pablo, and\nAyed, Ismail Ben. A unifying mutual information view\nof metric learning: cross-entropy vs. pairwise losses. In\nEuropean Conference on Computer Vision, pp. 548–564.\nSpringer, 2020.\nBromley, Jane, Bentz, James W, Bottou, L´eon, Guyon,\nIsabelle, LeCun, Yann, Moore, Cliff, S¨ackinger, Ed-\nuard, and Shah, Roopak.\nSignature veriﬁcation us-\ning a “Siamese” time delay neural network.\nInterna-\ntional Journal of Pattern Recognition and Artiﬁcial In-\ntelligence, 7(04):669–688, 1993.\nChang, Hong and Yeung, Dit-Yan. Locally linear metric\nadaptation for semi-supervised clustering. In Proceed-\nings of the twenty-ﬁrst international conference on Ma-\nchine learning, pp. 20, 2004.\nChen, Shuo, Gong, Chen, Yang, Jian, Li, Xiang, Wei, Yang,\nand Li, Jun. Adversarial metric learning. arXiv preprint\narXiv:1802.03170, 2018.\nChen, Shuo, Luo, Lei, Yang, Jian, Gong, Chen, Li, Jun,\nand Huang, Heng. Curvilinear distance metric learning.\nAdvances in Neural Information Processing Systems, 32,\n2019.\nChen, Ting, Kornblith, Simon, Norouzi, Mohammad, and\nHinton, Geoffrey. A simple framework for contrastive\nlearning of visual representations. In International con-\nference on machine learning, pp. 1597–1607, 2020.\nCour, Timothee, Sapp, Ben, and Taskar, Ben.\nLearning\nfrom partial labels. The Journal of Machine Learning\nResearch, 12:1501–1536, 2011.\nCox, Michael AA and Cox, Trevor F. Multidimensional\nscaling. In Handbook of data visualization, pp. 315–347.\nSpringer, 2008.\nDavis, Jason V, Kulis, Brian, Jain, Prateek, Sra, Suvrit, and\nDhillon, Inderjit S. Information-theoretic metric learn-\ning. In Proceedings of the 24th international conference\non Machine learning, pp. 209–216, 2007.\nDe Maesschalck, Roy, Jouan-Rimbaud, Delphine, and\nMassart, D´esir´e L. The mahalanobis distance. Chemo-\nmetrics and intelligent laboratory systems, 50(1):1–18,\n2000.\nDe Vazelhes, William, Carey, CJ, Tang, Yuan, Vauquier,\nNathalie, and Bellet, Aur´elien.\nmetric-learn: Metric\nlearning algorithms in Python.\nJournal of Machine\nLearning Research, 21:138–1, 2020.\nDhillon, JVDI. Differential entropic clustering of multi-\nvariate Gaussians. Advances in Neural Information Pro-\ncessing Systems, 19:337, 2007.\nD´ıaz-Vico, David and Dorronsoro, Jos´e R.\nDeep least\nsquares Fisher discriminant analysis. IEEE transactions\non neural networks and learning systems, 31(8):2752–\n2763, 2019.\nD´ıaz-Vico, David, Omari, Adil, Torres-Barr´an, Alberto,\nand Dorronsoro, Jos´e Ram´on. Deep Fisher discriminant\nanalysis. In International Work-Conference on Artiﬁcial\nNeural Networks, pp. 501–512. Springer, 2017.\nDing, Shengyong, Lin, Liang, Wang, Guangrun, and Chao,\nHongyang. Deep feature learning with relative distance\ncomparison for person re-identiﬁcation. Pattern Recog-\nnition, 48(10):2993–3003, 2015.\nDong, Minghzi. Metric learning with Lipschitz continuous\nfunctions. PhD thesis, UCL (University College Lon-\ndon), 2019.\nDuan, Yueqi, Zheng, Wenzhao, Lin, Xudong, Lu, Jiwen,\nand Zhou, Jie. Deep adversarial metric learning. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 2780–2789, 2018.\nFeng, Lin, Wang, Huibing, Jin, Bo, Li, Haohao, Xue, Min-\ngliang, and Wang, Le. Learning a distance metric by\nbalancing KL-divergence for imbalanced datasets. IEEE\nTransactions on Systems, Man, and Cybernetics: Sys-\ntems, 49(12):2384–2395, 2018.\nFinn, Chelsea, Abbeel, Pieter, and Levine, Sergey. Model-\nagnostic meta-learning for fast adaptation of deep net-\nworks. In International Conference on Machine Learn-\ning, pp. 1126–1135, 2017.\nFisher, Ronald A. The use of multiple measurements in\ntaxonomic problems. Annals of eugenics, 7(2):179–188,\n1936.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n41\nGautheron, L´eo, Habrard, Amaury, Morvant, Emilie, and\nSebban, Marc. Metric learning from imbalanced data. In\n2019 IEEE 31st International Conference on Tools with\nArtiﬁcial Intelligence (ICTAI), pp. 923–930. IEEE, 2019.\nGhodsi, Ali, Wilkinson, Dana F, and Southey, Finnegan.\nImproving embeddings by ﬂexible exploitation of side\ninformation. In IJCAI, pp. 810–816, 2007.\nGhojogh, Benyamin. Data Reduction Algorithms in Ma-\nchine Learning and Data Science. PhD thesis, Univer-\nsity of Waterloo, 2021.\nGhojogh, Benyamin and Crowley, Mark.\nUnsupervised\nand supervised principal component analysis: Tutorial.\narXiv preprint arXiv:1906.03148, 2019.\nGhojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.\nEigenvalue and generalized eigenvalue problems: Tuto-\nrial. arXiv preprint arXiv:1903.11240, 2019a.\nGhojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.\nFisher and kernel Fisher discriminant analysis: Tutorial.\narXiv preprint arXiv:1906.09436, 2019b.\nGhojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and\nCrowley,\nMark.\nLocally linear embedding and\nits variants:\nTutorial and survey.\narXiv preprint\narXiv:2011.10925, 2020a.\nGhojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and\nCrowley, Mark.\nMultidimensional scaling, Sammon\nmapping, and Isomap:\nTutorial and survey.\narXiv\npreprint arXiv:2009.08136, 2020b.\nGhojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and\nCrowley, Mark.\nStochastic neighbor embedding with\nGaussian and Student-t distributions: Tutorial and sur-\nvey. arXiv preprint arXiv:2009.10301, 2020c.\nGhojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.\nBackprojection for training feedforward neural networks\nin the input and feature spaces. In International Con-\nference on Image Analysis and Recognition, pp. 16–24.\nSpringer, 2020d.\nGhojogh, Benyamin, Nekoei, Hadi, Ghojogh, Aydin, Kar-\nray, Fakhri, and Crowley, Mark. Sampling algorithms,\nfrom survey sampling to Monte Carlo methods: Tutorial\nand literature review. arXiv preprint arXiv:2011.00901,\n2020e.\nGhojogh, Benyamin, Sikaroudi, Milad, Shaﬁei, Sobhan,\nTizhoosh, Hamid R, Karray, Fakhri, and Crowley, Mark.\nFisher discriminant triplet and contrastive losses for\ntraining Siamese networks. In 2020 international joint\nconference on neural networks (IJCNN), pp. 1–7. IEEE,\n2020f.\nGhojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and\nCrowley, Mark.\nFactor analysis, probabilistic princi-\npal component analysis, variational inference, and vari-\national autoencoder: Tutorial and survey. arXiv preprint\narXiv:2101.00734, 2021a.\nGhojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and\nCrowley, Mark.\nGenerative adversarial networks and\nadversarial autoencoders: Tutorial and survey.\narXiv\npreprint arXiv:2111.13282, 2021b.\nGhojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and\nCrowley, Mark. KKT conditions, ﬁrst-order and second-\norder optimization, and distributed optimization: Tu-\ntorial and survey.\narXiv preprint arXiv:2110.01858,\n2021c.\nGhojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and\nCrowley, Mark.\nLaplacian-based dimensionality re-\nduction including spectral clustering, Laplacian eigen-\nmap, locality preserving projection, graph embedding,\nand diffusion map: Tutorial and survey. arXiv preprint\narXiv:2106.02154, 2021d.\nGhojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and\nCrowley, Mark. Reproducing kernel Hilbert space, Mer-\ncer’s theorem, eigenfunctions, Nystr¨om method, and use\nof kernels in machine learning: Tutorial and survey.\narXiv preprint arXiv:2106.08443, 2021e.\nGhojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and\nCrowley, Mark. Restricted Boltzmann machine and deep\nbelief network: Tutorial and survey.\narXiv preprint\narXiv:2107.12521, 2021f.\nGloberson, Amir and Roweis, Sam. Metric learning by col-\nlapsing classes. Advances in neural information process-\ning systems, 18:451–458, 2005.\nGoldberger, Jacob, Hinton, Geoffrey E, Roweis, Sam T,\nand Salakhutdinov, Ruslan R. Neighbourhood compo-\nnents analysis. In Advances in neural information pro-\ncessing systems, pp. 513–520, 2005.\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\nBing, Warde-Farley, David, Ozair, Sherjil, Courville,\nAaron, and Bengio, Yoshua. Generative adversarial nets.\nAdvances in neural information processing systems, 27,\n2014.\nGoodfellow, Ian, Bengio, Yoshua, and Courville, Aaron.\nDeep learning. MIT press, 2016.\nGretton, Arthur, Bousquet, Olivier, Smola, Alex, and\nSch¨olkopf, Bernhard. Measuring statistical dependence\nwith Hilbert-Schmidt norms. In International conference\non algorithmic learning theory, pp. 63–77. Springer,\n2005.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n42\nGuillaumin,\nMatthieu,\nVerbeek,\nJakob,\nand Schmid,\nCordelia. Is that you? metric learning approaches for\nface identiﬁcation. In 2009 IEEE 12th international con-\nference on computer vision, pp. 498–505. IEEE, 2009.\nHadsell, Raia, Chopra, Sumit, and LeCun, Yann. Dimen-\nsionality reduction by learning an invariant mapping. In\n2006 IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’06), volume 2,\npp. 1735–1742. IEEE, 2006.\nHajiabadi, Hamideh, Godaz, Reza, Ghasemi, Morteza, and\nMonseﬁ, Reza. Layered geometric learning. In Inter-\nnational Conference on Artiﬁcial Intelligence and Soft\nComputing, pp. 571–582. Springer, 2019.\nHarandi, Mehrtash, Salzmann, Mathieu, and Hartley,\nRichard.\nJoint dimensionality reduction and metric\nlearning: A geometric take. In International Conference\non Machine Learning, pp. 1404–1413. PMLR, 2017.\nHart, Peter E, Stork, David G, and Duda, Richard O. Pat-\ntern classiﬁcation. Wiley Hoboken, 2000.\nHauberg, Søren, Freifeld, Oren, and Black, Michael J. A\ngeometric take on metric learning. In Advances in neural\ninformation processing systems, volume 25, pp. 2033–\n2041, 2012.\nHauser, Michael B.\nPrinciples of Riemannian geometry\nin neural networks. In Advances in neural information\nprocessing systems, pp. 2807—-2816, 2017.\nHermans, Alexander, Beyer, Lucas, and Leibe, Bastian. In\ndefense of the triplet loss for person re-identiﬁcation.\narXiv preprint arXiv:1703.07737, 2017.\nHinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Distilling\nthe knowledge in a neural network. In NIPS 2014 Deep\nLearning Workshop, 2014.\nHinton, Geoffrey E and Roweis, Sam T. Stochastic neigh-\nbor embedding. In Advances in neural information pro-\ncessing systems, pp. 857–864, 2003.\nHinton, Geoffrey E and Salakhutdinov, Ruslan R. Reduc-\ning the dimensionality of data with neural networks. Sci-\nence, 313(5786):504–507, 2006.\nHoffer, Elad and Ailon, Nir. Deep metric learning using\ntriplet network. In International workshop on similarity-\nbased pattern recognition, pp. 84–92. Springer, 2015.\nHoi, Steven CH, Liu, Wei, Lyu, Michael R, and Ma,\nWei-Ying.\nLearning distance metrics with contextual\nconstraints for image retrieval.\nIn 2006 IEEE Com-\nputer Society Conference on Computer Vision and Pat-\ntern Recognition (CVPR’06), volume 2, pp. 2072–2078.\nIEEE, 2006.\nHuang, Zhiwu and Van Gool, Luc. A Riemannian network\nfor SPD matrix learning. In Thirty-First AAAI Confer-\nence on Artiﬁcial Intelligence, 2017.\nJiang, Wen, Huang, Kai, Geng, Jie, and Deng, Xinyang.\nMulti-scale metric learning for few-shot learning. IEEE\nTransactions on Circuits and Systems for Video Technol-\nogy, 31(3):1091–1102, 2020.\nKaya, Mahmut and Bilge, Hasan S¸akir. Deep metric learn-\ning: A survey. Symmetry, 11(9):1066, 2019.\nKhodadadeh,\nSiavash,\nB¨ol¨oni,\nLadislau,\nand Shah,\nMubarak. Unsupervised meta-learning for few-shot im-\nage classiﬁcation.\nIn Advances in neural information\nprocessing systems, 2019.\nKim, Dae Ha and Song, Byung Cheol.\nVirtual sample-\nbased deep metric learning using discriminant analysis.\nPattern Recognition, 110:107643, 2021.\nKoch, Gregory, Zemel, Richard, Salakhutdinov, Ruslan,\net al. Siamese neural networks for one-shot image recog-\nnition.\nIn ICML deep learning workshop, volume 2.\nLille, 2015.\nKulis, Brian. Metric learning: A survey. Foundations and\nTrends® in Machine Learning, 5(4):287–364, 2013.\nKumar BG, Vijay, Carneiro, Gustavo, and Reid, Ian. Learn-\ning local image descriptors with deep Siamese and triplet\nconvolutional networks by minimising global loss func-\ntions. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pp. 5385–5394,\n2016.\nLeyva-Vallina, Mar´ıa, Strisciuglio, Nicola, and Petkov,\nNicolai.\nGeneralized contrastive optimization of\nSiamese networks for place recognition. arXiv preprint\narXiv:2103.06638, 2021.\nLi, Li, Doroslovaˇcki, Miloˇs, and Loew, Murray H. Dis-\ncriminant analysis deep neural networks. In 2019 53rd\nannual conference on information sciences and systems\n(CISS), pp. 1–6. IEEE, 2019.\nLi, Xiaomeng, Yu, Lequan, Fu, Chi-Wing, Fang, Meng,\nand Heng, Pheng-Ann. Revisiting metric learning for\nfew-shot image classiﬁcation. Neurocomputing, 406:49–\n58, 2020.\nLv, Xinbi, Zhao, Cairong, and Chen, Wei. A novel hard\nmining center-triplet loss for person re-identiﬁcation. In\nChinese Conference on Pattern Recognition and Com-\nputer Vision (PRCV), pp. 199–210. Springer, 2019.\nMahalanobis, Prasanta Chandra.\nOn tests and measures\nof group divergence. Journal of the Asiatic Society of\nBengal, 26:541–588, 1930.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n43\nMao, Chengzhi, Zhong, Ziyuan, Yang, Junfeng, Vondrick,\nCarl, and Ray, Baishakhi. Metric learning for adversarial\nrobustness. Advances in neural information processing\nsystems, 2019.\nMcLachlan, Goeffrey J. Mahalanobis distance. Resonance,\n4(6):20–26, 1999.\nMignon, Alexis and Jurie, Fr´ed´eric.\nPCCA: A new ap-\nproach for distance learning from sparse pairwise con-\nstraints. In 2012 IEEE conference on computer vision\nand pattern recognition, pp. 2666–2672. IEEE, 2012.\nMika,\nSebastian,\nRatsch,\nGunnar,\nWeston,\nJason,\nScholkopf, Bernhard, and Mullers, Klaus-Robert. Fisher\ndiscriminant analysis with kernels. In Neural networks\nfor signal processing IX: Proceedings of the 1999 IEEE\nsignal processing society workshop (cat. no. 98th8468),\npp. 41–48. Ieee, 1999.\nMovshovitz-Attias,\nYair,\nToshev,\nAlexander,\nLeung,\nThomas K, Ioffe, Sergey, and Singh, Saurabh. No fuss\ndistance metric learning using proxies. In Proceedings of\nthe IEEE International Conference on Computer Vision,\npp. 360–368, 2017.\nMurphy, Kevin P.\nConjugate Bayesian analysis of the\nGaussian distribution.\nTechnical report, University of\nBritish Colombia, 2007.\nMurphy, Kevin P. Machine learning: a probabilistic per-\nspective. MIT press, 2012.\nMusgrave, Kevin, Belongie, Serge, and Lim, Ser-Nam. Py-\ntorch metric learning. arXiv preprint arXiv:2008.09164,\n2020.\nOh Song, Hyun, Xiang, Yu, Jegelka, Stefanie, and\nSavarese, Silvio. Deep metric learning via lifted struc-\ntured feature embedding. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 4004–4012, 2016.\nPoorheravi,\nParisa Abdolrahim,\nGhojogh,\nBenyamin,\nGaudet, Vincent, Karray, Fakhri, and Crowley, Mark.\nAcceleration of large margin metric learning for nearest\nneighbor classiﬁcation using triplet mining and stratiﬁed\nsampling. Journal of Computational Vision and Imaging\nSystems, 6(1), 2020.\nQian, Qi, Shang, Lei, Sun, Baigui, Hu, Juhua, Li, Hao, and\nJin, Rong. SoftTriple loss: Deep metric learning without\ntriplet sampling. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pp. 6450–\n6458, 2019.\nRiccati, Jacobo.\nAnimadversiones in aequationes differ-\nentiales secundi gradus. Actorum Eruditorum Supple-\nmenta, 8(1724):66–73, 1724.\nRobinson, Joshua, Chuang, Ching-Yao, Sra, Suvrit, and\nJegelka, Stefanie. Contrastive learning with hard neg-\native samples. In International Conference on Learning\nRepresentations, 2021.\nRoostaiyan, Seyed Mahdi, Imani, Ehsan, and Baghshah,\nMahdieh Soleymani. Multi-modal deep distance metric\nlearning.\nIntelligent Data Analysis, 21(6):1351–1369,\n2017.\nRoweis, Sam T and Saul, Lawrence K. Nonlinear dimen-\nsionality reduction by locally linear embedding. Science,\n290(5500):2323–2326, 2000.\nSch¨olkopf, Bernhard. The kernel trick for distances. Ad-\nvances in neural information processing systems, pp.\n301–307, 2001.\nSch¨olkopf,\nBernhard,\nSmola,\nAlex\nJ,\nWilliamson,\nRobert C, and Bartlett, Peter L. New support vector al-\ngorithms. Neural computation, 12(5):1207–1245, 2000.\nSchroff, Florian, Kalenichenko, Dmitry, and Philbin,\nJames. FaceNet: A uniﬁed embedding for face recog-\nnition and clustering. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pp.\n815–823, 2015.\nShental, Noam, Hertz, Tomer, Weinshall, Daphna, and\nPavel, Misha. Adjustment learning and relevant com-\nponent analysis. In European conference on computer\nvision, pp. 776–790. Springer, 2002.\nSikaroudi, Milad, Ghojogh, Benyamin, Safarpoor, Amir,\nKarray, Fakhri, Crowley, Mark, and Tizhoosh, Hamid R.\nOfﬂine versus online triplet mining based on ex-\ntreme distances of histopathology patches. In Interna-\ntional Symposium on Visual Computing, pp. 333–345.\nSpringer, 2020a.\nSikaroudi, Milad, Safarpoor, Amir, Ghojogh, Benyamin,\nShaﬁei,\nSobhan,\nCrowley,\nMark,\nand\nTizhoosh,\nHamid R.\nSupervision and source domain impact on\nrepresentation learning: A histopathology case study. In\n2020 42nd Annual International Conference of the IEEE\nEngineering in Medicine & Biology Society (EMBC), pp.\n1400–1403. IEEE, 2020b.\nSikaroudi, Milad, Ghojogh, Benyamin, Karray, Fakhri,\nCrowley, Mark, and Tizhoosh, Hamid R.\nBatch-\nincremental triplet sampling for training triplet networks\nusing Bayesian updating theorem. In 2020 25th Inter-\nnational Conference on Pattern Recognition (ICPR), pp.\n7080–7086. IEEE, 2021.\nSohn, Kihyuk. Improved deep metric learning with multi-\nclass n-pair loss objective. In Advances in neural infor-\nmation processing systems, pp. 1857–1865, 2016.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n44\nSu´arez, Juan-Luis, Garc´ıa, Salvador, and Herrera, Fran-\ncisco.\npyDML: A Python library for distance metric\nlearning. Journal of Machine Learning Research, 21:\n96–1, 2020.\nSu´arez, Juan Luis, Garc´ıa, Salvador, and Herrera, Fran-\ncisco. A tutorial on distance metric learning: Mathe-\nmatical foundations, algorithms, experimental analysis,\nprospects and challenges.\nNeurocomputing, 425:300–\n322, 2021.\nTeh, Eu Wern and Taylor, Graham W. Learning with less\ndata via weakly labeled patch classiﬁcation in digital\npathology.\nIn 2020 IEEE 17th International Sympo-\nsium on Biomedical Imaging (ISBI), pp. 471–475. IEEE,\n2020.\nTeh, Eu Wern, DeVries, Terrance, and Taylor, Graham W.\nProxyNCA++: Revisiting and revitalizing proxy neigh-\nborhood component analysis. In European Conference\non Computer Vision (ECCV), pp. 448–464. Springer,\n2020.\nTizhoosh, Hamid R.\nOpposition-based learning: a new\nscheme for machine intelligence. In International con-\nference on computational intelligence for modelling,\ncontrol and automation and international conference on\nintelligent agents, web technologies and internet com-\nmerce (CIMCA-IAWTIC’06), volume 1, pp. 695–701.\nIEEE, 2005.\nTriantaﬁllou, Eleni, Zhu, Tyler, Dumoulin, Vincent, Lam-\nblin, Pascal, Evci, Utku, Xu, Kelvin, Goroshin, Ross,\nGelada, Carles, Swersky, Kevin, Manzagol, Pierre-\nAntoine, et al. Meta-dataset: A dataset of datasets for\nlearning to learn from few examples. In International\nConference on Learning Representations, 2020.\nTsang, Ivor W, Kwok, James T, Bay, C, and Kong, H. Dis-\ntance metric learning with kernels. In Proceedings of the\nInternational Conference on Artiﬁcial Neural Networks,\npp. 126–129, 2003.\nvan der Maaten, Laurens and Hinton, Geoffrey. Visualizing\ndata using t-SNE. Journal of machine learning research,\n9(Nov):2579–2605, 2008.\nVapnik, Vladimir. The nature of statistical learning theory.\nSpringer science & business media, 1995.\nWang, Dong and Tan, Xiaoyang. Bayesian neighborhood\ncomponent analysis. IEEE transactions on neural net-\nworks and learning systems, 29(7):3140–3151, 2017.\nWang, Fei and Sun, Jimeng.\nSurvey on distance metric\nlearning and dimensionality reduction in data mining.\nData mining and knowledge discovery, 29(2):534–564,\n2015.\nWang, Jian, Zhou, Feng, Wen, Shilei, Liu, Xiao, and Lin,\nYuanqing. Deep metric learning with angular loss. In\nProceedings of the IEEE International Conference on\nComputer Vision, pp. 2593–2601, 2017.\nWang, Shijun and Jin, Rong. An information geometry ap-\nproach for distance metric learning. In Artiﬁcial intelli-\ngence and statistics, pp. 591–598. PMLR, 2009.\nWang, Wei, Ooi, Beng Chin, Yang, Xiaoyan, Zhang,\nDongxiang, and Zhuang, Yueting. Effective multi-modal\nretrieval based on stacked auto-encoders. Proceedings of\nthe VLDB Endowment, 7(8):649–660, 2014.\nWang, Xiao, Chen, Ziliang, Yang, Rui, Luo, Bin, and\nTang, Jin. Improved hard example mining by discover-\ning attribute-based hard person identity. arXiv preprint\narXiv:1905.02102, 2019.\nWang, Xun, Zhang, Haozhi, Huang, Weilin, and Scott,\nMatthew R. Cross-batch memory for embedding learn-\ning.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 6388–\n6397, 2020a.\nWang, Yaqing, Yao, Quanming, Kwok, James T, and Ni,\nLionel M. Generalizing from a few examples: A survey\non few-shot learning. ACM Computing Surveys (CSUR),\n53(3):1–34, 2020b.\nWeinberger, Kilian Q and Saul, Lawrence K. Distance met-\nric learning for large margin nearest neighbor classiﬁca-\ntion. Journal of machine learning research, 10(2), 2009.\nWeinberger, Kilian Q, Blitzer, John, and Saul, Lawrence K.\nDistance metric learning for large margin nearest neigh-\nbor classiﬁcation.\nIn Advances in neural information\nprocessing systems, pp. 1473–1480, 2006.\nWu, Chao-Yuan, Manmatha, R, Smola, Alexander J, and\nKrahenbuhl, Philipp. Sampling matters in deep embed-\nding learning. In Proceedings of the IEEE International\nConference on Computer Vision, pp. 2840–2848, 2017.\nXiang, Shiming, Nie, Feiping, and Zhang, Changshui.\nLearning a Mahalanobis distance metric for data cluster-\ning and classiﬁcation. Pattern recognition, 41(12):3600–\n3612, 2008.\nXing, Eric, Jordan, Michael, Russell, Stuart J, and Ng, An-\ndrew. Distance metric learning with application to clus-\ntering with side-information. Advances in neural infor-\nmation processing systems, 15:521–528, 2002.\nXu, Xing, He, Li, Lu, Huimin, Gao, Lianli, and Ji, Yanli.\nDeep adversarial metric learning for cross-modal re-\ntrieval. World Wide Web, 22(2):657–672, 2019a.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n45\nXu, Xinyi, Cao, Huanhuan, Yang, Yanhua, Yang, Erkun,\nand Deng, Cheng. Zero-shot metric learning. In Inter-\nnational Joint Conference on Artiﬁcial Intelligence, pp.\n3996–4002, 2019b.\nXuan, Hong, Stylianou, Abby, and Pless, Robert. Improved\nembeddings with easy positive triplet mining. In Pro-\nceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision, pp. 2474–2482, 2020.\nYang, Liu. An overview of distance metric learning. In\nProceedings of the computer vision and pattern recogni-\ntion conference, 2007.\nYang, Liu and Jin, Rong.\nDistance metric learning: A\ncomprehensive survey. Michigan State Universiy, 2(2):\n4, 2006.\nYang, Liu, Jin, Rong, Sukthankar, Rahul, and Liu, Yi. An\nefﬁcient algorithm for local distance metric learning. In\nProceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 2, pp. 543–548, 2006.\nYang, Liu, Jin, Rong, and Sukthankar, Rahul. Bayesian\nactive distance metric learning. In Conference on Un-\ncertainty in Artiﬁcial Intelligence (UAI), 2007.\nYang, Liu, Zhang, Mingyang, Li, Cheng, Bendersky,\nMichael, and Najork, Marc.\nBeyond 512 tokens:\nSiamese multi-depth transformer-based hierarchical en-\ncoder for long-form document matching. In Proceedings\nof the 29th ACM International Conference on Informa-\ntion & Knowledge Management, pp. 1725–1734, 2020.\nYang, Wei, Wang, Kuanquan, and Zuo, Wangmeng. Fast\nneighborhood component analysis.\nNeurocomputing,\n83:31–37, 2012.\nYang, Xun, Wang, Meng, Zhang, Luming, and Tao,\nDacheng. Empirical risk minimization for metric learn-\ning using privileged information. In IJCAI International\nJoint Conference on Artiﬁcial Intelligence, 2016.\nYang, Zhirong and Laaksonen, Jorma. Regularized neigh-\nborhood component analysis. In Scandinavian Confer-\nence on Image Analysis, pp. 253–262. Springer, 2007.\nYe, Mang, Zhang, Xu, Yuen, Pong C, and Chang, Shih-\nFu.\nUnsupervised embedding learning via invariant\nand spreading instance feature. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 6210–6219, 2019.\nYeung, Dit-Yan and Chang, Hong. A kernel approach for\nsemisupervised metric learning. IEEE Transactions on\nNeural Networks, 18(1):141–149, 2007.\nZadeh, Pourya, Hosseini, Reshad, and Sra, Suvrit. Geomet-\nric mean metric learning. In International conference on\nmachine learning, pp. 2464–2471, 2016.\nZhang, Changqing, Liu, Yeqing, Liu, Yue, Hu, Qinghua,\nLiu, Xinwang, and Zhu, Pengfei. FISH-MML: Fisher-\nHSIC multi-view metric learning. In IJCAI, pp. 3054–\n3060, 2018.\nZhang, Hangbin, Wong, Raymond K, and Chu, Victor W.\nCurvilinear collaborative metric learning with macro-\nmicro attentions. In 2021 International Joint Conference\non Neural Networks (IJCNN), pp. 1–8. IEEE, 2021.\nZhang, Zhihua, Kwok, James T, and Yeung, Dit-Yan. Para-\nmetric distance metric learning with label information.\nIn IJCAI, volume 1450, 2003.\nZhou, Yu and Gu, Hong. Geometric mean metric learning\nfor partial label data.\nNeurocomputing, 275:394–402,\n2018.\nZhu, Pengfei, Cheng, Hao, Hu, Qinghua, Wang, Qilong,\nand Zhang, Changqing. Towards generalized and efﬁ-\ncient metric learning on riemannian manifold. In IJCAI,\npp. 3235–3241, 2018.\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n46\nContents\n1\nIntroduction\n1\n2\nGeneralized Mahalanobis Distance Metric\n2\n2.1\nDistance Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.2\nMahalanobis Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.3\nGeneralized Mahalanobis Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.4\nThe Main Idea of Metric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n3\nSpectral Metric Learning\n4\n3.1\nSpectral Methods Using Scatters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3.1.1\nThe First Spectral Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3.1.2\nFormulating as Semideﬁnite Programming\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.1.3\nRelevant to Fisher Discriminant Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.1.4\nRelevant Component Analysis (RCA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.1.5\nDiscriminative Component Analysis (DCA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.1.6\nHigh Dimensional Discriminative Component Analysis . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.1.7\nRegularization by Locally Linear Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.1.8\nFisher-HSIC Multi-view Metric Learning (FISH-MML)\n. . . . . . . . . . . . . . . . . . . . . .\n8\n3.2\nSpectral Methods Using Hinge Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.2.1\nLarge-Margin Metric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.2.2\nImbalanced Metric Learning (IML)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.3\nLocally Linear Metric Adaptation (LLMA)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.4\nRelevant to Support Vector Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.5\nRelevant to Multidimensional Scaling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.6\nKernel Spectral Metric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.6.1\nUsing Eigenvalue Decomposition of Kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.6.2\nRegularization by Locally Linear Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.6.3\nRegularization by Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.6.4\nKernel Discriminative Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.6.5\nRelevant to Kernel Fisher Discriminant Analysis . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.6.6\nRelevant to Kernel Support Vector Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.7\nGeometric Spectral Metric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.7.1\nGeometric Mean Metric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.7.2\nLow-rank Geometric Mean Metric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.7.3\nGeometric Mean Metric Learning for Partial Labels . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.7.4\nGeometric Mean Metric Learning on SPD and Grassmannian Manifolds . . . . . . . . . . . . . .\n16\n3.7.5\nMetric Learning on Stiefel and SPD Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.7.6\nCurvilinear Distance Metric Learning (CDML) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.8\nAdversarial Metric Learning (AML) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4\nProbabilistic Metric Learning\n18\n4.1\nCollapsing Classes\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.1.1\nCollapsing Classes in the Input Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.1.2\nCollapsing Classes in the Feature Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.2\nNeighborhood Component Analysis Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n47\n4.2.1\nNeighborhood Component Analysis (NCA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.2.2\nRegularized Neighborhood Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.2.3\nFast Neighborhood Component Analysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.3\nBayesian Metric Learning Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.3.1\nBayesian Metric Learning Using Sigmoid Function . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4.3.2\nBayesian Neighborhood Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4.3.3\nLocal Distance Metric (LDM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.4\nInformation Theoretic Metric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.4.1\nInformation Theoretic Metric Learning with a Prior Weight Matrix . . . . . . . . . . . . . . . . .\n23\n4.4.2\nInformation Theoretic Metric Learning for Imbalanced Data . . . . . . . . . . . . . . . . . . . .\n24\n4.4.3\nProbabilistic Relevant Component Analysis Methods . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.4.4\nMetric Learning by Information Geometry\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.5\nEmpirical Risk Minimization in Metric Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n4.5.1\nMetric Learning Using the Sigmoid Function . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n4.5.2\nPairwise Constrained Component Analysis (PCCA) . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n4.5.3\nMetric Learning for Privileged Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n5\nDeep Metric Learning\n26\n5.1\nReconstruction Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5.1.1\nTypes of Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5.1.2\nReconstruction Loss\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5.1.3\nDenoising Autoencoder\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5.1.4\nMetric Learning by Reconstruction Autoencoder . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n5.2\nSupervised Metric Learning by Supervised Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n5.2.1\nMean Squared Error and Mean Absolute Value Losses . . . . . . . . . . . . . . . . . . . . . . .\n27\n5.2.2\nHuber and KL-Divergence Losss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5.2.3\nHinge Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5.2.4\nCross-entropy Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5.3\nMetric Learning by Siamese Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5.3.1\nSiamese and Triplet Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5.3.2\nPairs and Triplets of Data Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n5.3.3\nImplementation of Siamese Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n5.3.4\nContrastive Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n5.3.5\nTriplet Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n5.3.6\nTuplet Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n5.3.7\nNeighborhood Component Analysis Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n5.3.8\nProxy Neighborhood Component Analysis Loss . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n5.3.9\nSoftmax Triplet Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n5.3.10 Triplet Global Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n5.3.11 Angular Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n5.3.12 SoftTriple Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n5.3.13 Fisher Siamese Losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n5.3.14 Deep Adversarial Metric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n5.3.15 Triplet Mining\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.3.16 Triplet Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n5.4\nDeep Discriminant Analysis Metric Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.4.1\nDeep Probabilistic Discriminant Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nSpectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey\n48\n5.4.2\nDiscriminant Analysis with Virtual Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n5.4.3\nDeep Fisher Discriminant Analysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n5.5\nMulti-Modal Deep Metric Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n5.6\nGeometric Metric Learning by Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.7\nFew-shot Metric Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.7.1\nMulti-scale Metric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.7.2\nMetric Learning with Continuous Similarity Scores . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n6\nConclusion\n39\n",
  "categories": [
    "stat.ML",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2022-01-23",
  "updated": "2022-01-23"
}