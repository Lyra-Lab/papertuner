{
  "id": "http://arxiv.org/abs/2201.09568v1",
  "title": "Pearl: Parallel Evolutionary and Reinforcement Learning Library",
  "authors": [
    "Rohan Tangri",
    "Danilo P. Mandic",
    "Anthony G. Constantinides"
  ],
  "abstract": "Reinforcement learning is increasingly finding success across domains where\nthe problem can be represented as a Markov decision process. Evolutionary\ncomputation algorithms have also proven successful in this domain, exhibiting\nsimilar performance to the generally more complex reinforcement learning.\nWhilst there exist many open-source reinforcement learning and evolutionary\ncomputation libraries, no publicly available library combines the two\napproaches for enhanced comparison, cooperation, or visualization. To this end,\nwe have created Pearl (https://github.com/LondonNode/Pearl), an open source\nPython library designed to allow researchers to rapidly and conveniently\nperform optimized reinforcement learning, evolutionary computation and\ncombinations of the two. The key features within Pearl include: modular and\nexpandable components, opinionated module settings, Tensorboard integration,\ncustom callbacks and comprehensive visualizations.",
  "text": "PEARL: PARALLEL EVOLUTIONARY AND REINFORCEMENT\nLEARNING LIBRARY\nA PREPRINT\nRohan Tangri∗\nDanilo P. Mandic∗\nAnthony G. Constantinides∗\nJanuary 25, 2022\nABSTRACT\nReinforcement learning is increasingly ﬁnding success across domains where the problem can\nbe represented as a Markov decision process. Evolutionary computation algorithms have also\nproven successful in this domain, exhibiting similar performance to the generally more com-\nplex reinforcement learning. Whilst there exist many open-source reinforcement learning and\nevolutionary computation libraries, no publicly available library combines the two approaches\nfor enhanced comparison, cooperation, or visualization.\nTo this end, we have created Pearl\n(https://github.com/LondonNode/Pearl), an open source Python library designed to allow\nresearchers to rapidly and conveniently perform optimized reinforcement learning, evolutionary\ncomputation and combinations of the two. The key features within Pearl include: modular and\nexpandable components, opinionated module settings, Tensorboard integration, custom callbacks and\ncomprehensive visualizations.\nKeywords Reinforcement Learning · Evolutionary Computation · Python · Open Source\n1\nIntroduction\nReinforcement learning (RL) is a growing area of research with impressive results in robotics (OpenAI et al. [2018]),\ncommunications (Zhang et al. [2019]) and ﬁnance (Balch et al. [2019]). With the necessity to process increasing\nvolumes of data, the algorithm complexity has also been growing. In addition, different classes of agents tend to\nhave differing fundamentals; for example, off-policy SAC (Haarnoja et al. [2019]) vs on-policy PPG (Cobbe et al.\n[2021]). This, in turn, tends to increase complexity in software libraries attempting to unify these ideas in a single\nstructure (Pineau et al. [2020]). Furthermore, evolutionary computation (EC) is competitive with RL whilst often being\nalgorithmically simpler, although this may come at the cost of data inefﬁciency (Salimans et al. [2017]). Methodologies\ncombining both approaches in hybrid algorithms are also emerging, and have been shown to outperform both individual\ntechniques separately (Majid et al. [2021], Pourchot and Sigaud [2018]). This has also highlighted a void in the literature\nand available libraries, especially given the need for highly modular software allowing researchers to rapidly prototype\nand test new algorithms in both the EC and RL frameworks for comparison and cooperation.\nTo this end, we introduce Pearl (Parallel Evolutionary and Reinfocement Learning Library), a Python library designed\nfor rapid prototyping and testing of adaptive decision making algorithms, including tools for both RL and EC. Supported\nuse cases are as follows: 1) Single agent RL 2) Multi-agent RL 3) EC in a trajectory environment with observations,\nactions and rewards 4) EC for optimizing black box static functions (useful for direct testing of EC algorithm properties\nin differently shaped functions) 5) Hybrid algorithms combining RL and EC to optimize policies. The deep learning\ncomponents are built in PyTorch (Paszke et al. [2019]) while further details of other dependencies can be found in\nthe pyproject.toml documentation. The Pearl package provides modular and extensible components which can\nbe plugged into compatible agents for ease of experimentation and resilience to code changes. Opinionated settings\ndataclasses are also used to group together module parameters and give user suggestions. In addition, custom callbacks\nare implemented to allow users to inject unique logic into their agents during training (Rafﬁn et al. [2021]). Finally,\n∗Department of Electrical and Electronic Engineering, Imperial College London, UK, Correspondence to: Rohan Tangri\n<rohan.tangri16@imperial.ac.uk>\narXiv:2201.09568v1  [cs.LG]  24 Jan 2022\nPearl\nA PREPRINT\nPearl offers Tensorboard integration for real time training analysis, while executable scripts introduce a basic command\nline interface (CLI) for the visualization of complex results and basic demonstrations of implemented agents.\n2\nLibrary Design\nAgent\nLogger\nBuffer\nOpenAI Gym\n Envrionment\nfit()\nnum_steps\nbatch_size\nactor_epochs\ncritic_epochs\ntrain_frequency\nstep()\naction\nreset()\nstep_env()\nobservation\nnum_steps\n_fit()\nbatch_size\nactor_epochs\ncritic_epochs\nadd_reward()\nreward\nadd_train_log()\ntrain_log\nwrite_log()\nstep\ncheck_episode_done()\ndone\nsample()\nbatch_size\nflatten_env\ndtype\nadd_trajectory()\nobservation\naction\nreward\nnext_observation\ndone\nActor Critic\nforward()\nobservations\nforward()\nobservations\nforward_critics()\nobservations\nactions\nActor\nEncoder\nTorso\nHead\nforward()\nobservations\nactions\nCritic\nEncoder\nTorso\nHead\nActor Updater\n__call__()\nmodel\nExplorer\n__call__()\nactor\nobservation\nstep\nCallbacks\non_step()\nstep\nCritic Updater\n__call__()\nmodel\nSignal Processing\nFigure 1: Module ﬂow for training an agent\nPearl minimizes the library learning curve by allowing EC, RL and hybrid agents to be derived from a single base class.\nThe high level training ﬂow for an agent derived from the base class is illustrated in Figure 1. The structure of Pearl\nincludes the following modules:\n• The models represent neural network structures. An actor-critic object is provided that accepts a separate or\nshared actor and critic network as a template. This template can then be used to generate a population of actor\nand critic networks. Methods are provided for an easy forward pass through either population. An individual\nnetwork can also be represented as a vector of network parameters, so the populations of actors and critics\ncan each be represented by a matrix, and a matrix can be used to update each population. Finally, there is\nthe notion of a global network, which acts as the average of the population networks to be used post-training.\nAs shown in Figure 1, each actor and critic is designed as a sequence of an encoder, a torso and a head. The\nencoder processes the input; for example, by concatenating the state observation and action for the continuous\nQ function network. The torso embodies the deep layers; in our case, a multilayer perceptron. Finally, the\nhead dictates the output; for example, a categorical distribution for an actor. Each actor and critic can also be\nspeciﬁed to have an associated target network, which will scale with the population in the actor-critic object.\nFurthermore, a dummy model is provided that doesn’t utilize any deep structure but can still be used in the\nactor-critic object to allow for EC in static black box optimization tasks.\n• The agents combine the modules within Pearl, and act as the interface between the user and the algorithm. The\nbeneﬁt of this single interface is a high level of abstraction. Generally, this comes at the cost of a potentially\nconfusing long list of inputs (Pardo [2020]), but within Pearl this is avoided through the use of settings\ndataclasses which group together the various module parameters and give user suggestions on what inputs are\nneeded and their types. Such beneﬁts are not found in dictionary objects which are often used for this task.\nImplementations of EC, RL and hybrid agents are provided as examples.\n• The buffers handle the storing and sampling of agent trajectories in the environment. To promote ﬂexibility,\nall trajectories in the buffer can be collected, or alternatively, they can be randomly sampled, or only the last\nn trajectories can be selected. For off-policy buffers, a single data structure is used to store both trajectory\nobservations and next observations, thus halving memory requirements. This stems from the sequential nature\nof observations with the information ’blended’ only at episode end.\n• The updaters handle iterative updates for neural networks and any other adaptive or iterative models. There\nare three classes of updater: actor updaters, critic updaters and evolutionary updaters.\n• The signal processing module designates the ﬂow of information in both RL and EC, with methods such as\nthe generalized advantage estimate (Schulman et al. [2018]) or sample approximations for KL divergence\n(Schulman [2021]). This further enhances modularity, since these are implemented as procedural methods\nwhich can be straightforwardly used for experimenting with different updater algorithms.\n2\nPearl\nA PREPRINT\n• The explorers are responsible for processing actions given by a deterministic actor network, and thus improve\nexploration. This mainly involves adding noise to algorithms such as DDPG. Another useful feature is the\nability to randomly explore the environment, independent of the actor network, for the ﬁrst n steps of training.\n• The logger handles useful training statistics which are written to Tensorboard and a log ﬁle. By default, the\nTensorboard logs are split into three sections: the reward section monitors the rewards the agent receives, the\nloss section monitors the actor and critic loss values, and the metrics section monitors useful metrics such as\nthe policy entropy or KL divergence. This is particularly useful for monitoring model convergence.\n• The callbacks are designed to allow the user to inject unique logic into the training loop at every time step.\nThis allows for useful features, such as saving the current model and early stopping.\n3\nVisualization and Scripts\nPearl provides manifold ways to visualize results, such as live analysis via Tensorboard, as shown in Fig-\nure 2a.\nThis is useful for monitoring progress in long training runs.\nTo accommodate more com-\nplicated plots, for example, comparing different runs with plot titles, axes titles and legends, a plot-\nting script can be executed via a CLI. An exemplar plot is shown in Figure 2b, which is created by\nthe command: python -m pearll.plot -p runs/DQN-demo runs/DQN-parallel-demo runs/DeepES-demo\n–-metric reward –-titles Cartpole-v0 –-num-cols 1 –-legend DQN DQN-2 ES –-window 20.\n(a) Tensorboard live training monitor\n0.0\n0.5\n1.0\n1.5\n2.0\nStep\n1e4\n0\n50\n100\n150\n200\nReward\nCartpole-v0\nDQN\nDQN-2\nES\n(b) Post training plot via CLI\nFigure 2: Visualization capabilities within Pearl\nFurthermore, a demo script is also provided that allows users to quickly demonstrate agents in simple environments\nwithout the need to open any Python editors. For example, the command python -m pearll.demo –-agent her\nwill run a DQN agent with the HER buffer in a discrete bit ﬂipping environment (Andrychowicz et al. [2017]). These\nare useful to quickly verify that agents are working as expected and act as integration tests.\n4\nAdamES\nTo demonstrate the utility of Pearl for rapidly prototyping, testing and visualizing new RL and EC algorithms, we\nimplement AdamES, a novel EC algorithm which aims to combine OpenAI’s Natural Evolutionary Strategy (ES)\n(Salimans et al. [2017]) with the Adam optimizer equation (Kingma and Ba [2015]).\n4.1\nAlgorithm\nSuppose there exists a continuous, differentiable function, F(θ). The ES equation mirrors the standard gradient descent\nequation by making a sample gradient approximation (Salimans et al. [2017]). Meanwhile, the optimization domain\nhas extended stochastic gradient descent through the use of momentum (Qian [1999]) and dampening (Dauphin et al.\n[2015]) for improved stability and convergence speed; however, these concepts are not applied in the ES algorithm. In\nAdamES, we replace the gradient terms of the Adam optimizer with the ES approximation. In this way, we add the\nconcepts of momentum and dampening to the update equation to increase convergence speed and steady state stability\nfor a minor algorithmic change. The code for the AdamES algorithm within the Pearl framework is in Appendix A.\n3\nPearl\nA PREPRINT\nAlgorithm 1 AdamES, all operations on vectors are element-wise.\nInput: initial policy parameters θ0, momentum exponential weight β1 ∈[0, 1], dampening exponential weight\nβ2 ∈[0, 1], sampling noise standard deviation σ, population size n, learning rate α\nmomentum term m0 ←0\ndampening term v0 ←0\nfor t = 0, 1, 2, . . . , T −1 do\nSample ϵ1, . . . , ϵn ∼N(0, I)\nRi = F(θt + σϵi) for i = 1, . . . , n\n▷Compute population returns\n∇F(θt) ≈\n1\nnσ\nPn\ni=1 Riϵi\n▷Compute gradient approximation\nmt+1 ←(1 −β1)∇F(θt) + β1mt\n▷Update momentum term\nvt+1 ←(1 −β2)[∇F(θt) ⊙∇F(θt)] + β2vt\n▷Update dampening term\nˆmt+1 ←mt+1/(1 −(β1)t)\nˆvt+1 ←vt+1/(1 −(β2)t)\nθt+1 ←θt + [α ˆmt+1/(\np\nˆvt+1 + γ)]\n▷γ = fuzz term to avoid divide by 0 error\nend for\nOutput: optimized policy θT\n4.2\nExperiments\nThe learning curves of AdamES and ES on a set of multidimensional continuous optimization functions are shown in\nFigure 3 and the KL divergence between population iterations are shown in Figure 4. Pearl allows for easy and rapid\ngeneration of these plots via the CLI. Different metrics logged by default via Tensorboard can be plotted using the\n–-metric ﬂag, which can be set to plot rewards, divergences, entropies, actor losses or critic losses. Furthermore, a log\nscale can be used for the y-axis by including the ﬂag –-log-y. Finally, the plot name and ﬁle types can be set by the\ncommands –-save-path and –-save-types, which allows the same plot to be saved in many different formats, by\ndefault, a PDF.\nHyperparameters used in the experiments plotted in Figure 3 and Figure 4 are listed in Appendix B. AdamES achieves\nfaster convergence speed as shown directly by the learning curves and the larger KL divergence between population\niterations while the algorithm has not reached a maximum. At the same time, while the steady state KL divergence of\nAdamES has a higher variance than the steady state KL divergence of ES, it also has a smaller expected value. This\nindicates that AdamES has a greater stability around the solution found compared to ES.\n0\n50\n100\n150\n200\n250\nStep\n-200\n-150\n-100\n-50\n0\nReward\nSphere\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n-4\n-3\n-2\n-1\n0\nReward\nMatyas\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n-14\n-12\n-10\n-8\n-6\n-4\nReward\nAckley\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n-100,000\n-75,000\n-50,000\n-25,000\n0\nReward\nBeale\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n30\n40\n50\n60\nReward\nEggHolder\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n-800\n-600\n-400\n-200\n0\nReward\nHimmelblau\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n-60\n-50\n-40\n-30\n-20\nReward\nRastrigin\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n-2,000\n-1,000\n0\nReward\nRosenbrock\nES\nAdamES\nFigure 3: Learning curves\n4\nPearl\nA PREPRINT\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nSphere\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n10\n5\n10\n4\n10\n3\nKL Divergence\nMatyas\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nAckley\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n10\n5\n10\n4\n10\n3\nKL Divergence\nBeale\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nEggHolder\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nHimmelblau\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nRastrigin\nES\nAdamES\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nRosenbrock\nES\nAdamES\nFigure 4: Population divergence with log scale\n5\nConclusions and Future Work\nThis paper has introduced Pearl, a library especially designed to allow researchers to rapidly prototype and test new\nideas in order to optimize decision making algorithms in reinforcement learning type environments. Through easy\naccess to interpretable tools, Pearl facilitates experimentation, further research and innovation, particularly at the overlap\nbetween RL and EC.\nFuture work will involve new agents and features such as the Intrinsic Curiosity Module (Pathak et al. [2017]) as well\nas the expansion of the models module to other types of neural networks, such as transformers (Vaswani et al. [2017]).\n5\nPearl\nA PREPRINT\nReferences\nOpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Józefowicz, Bob McGrew, Jakub W. Pachocki,\nJakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh\nTobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous in-hand manipulation. CoRR,\nabs/1808.00177, 2018. URL http://arxiv.org/abs/1808.00177.\nZiyao Zhang, Liang Ma, Konstantinos Poularakis, Kin K. Leung, and Lingfei Wu. Dq scheduler: Deep reinforcement\nlearning based controller synchronization in distributed sdn. In ICC 2019 - 2019 IEEE International Conference on\nCommunications (ICC), pages 1–7, 2019. doi:10.1109/ICC.2019.8761183.\nTucker Hybinette Balch, Mahmoud Mahfouz, Joshua Lockhart, Maria Hybinette, and David Byrd. How to evaluate\ntrading strategies: Single agent market replay or multiple agent interactive simulation?, 2019.\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu,\nAbhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algorithms and applications, 2019.\nKarl W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic policy gradient. In Marina Meila and Tong\nZhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 2020–2027. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.\npress/v139/cobbe21a.html.\nJoelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché Buc,\nEmily Fox, and Hugo Larochelle. Improving reproducibility in machine learning research (a report from the neurips\n2019 reproducibility program), 2020.\nTim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative\nto reinforcement learning, 2017.\nAmjad Yousef Majid, Serge Saaybi, Tomas van Rietbergen, Vincent Francois-Lavet, R Venkatesha Prasad, and Chris\nVerhoeven. Deep reinforcement learning versus evolution strategies: A comparative survey, 2021.\nAloïs Pourchot and Olivier Sigaud. CEM-RL: combining evolutionary and gradient-based methods for policy search.\nCoRR, abs/1810.01222, 2018. URL http://arxiv.org/abs/1810.01222.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zem-\ning Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito,\nMartin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin-\ntala.\nPytorch: An imperative style, high-performance deep learning library.\nIn H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Process-\ning Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.\nAntonin Rafﬁn, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-\nbaselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1–8,\n2021. URL http://jmlr.org/papers/v22/20-1364.html.\nFabio Pardo. Tonic: A deep reinforcement learning library for fast prototyping and benchmarking. arXiv preprint\narXiv:2011.07537, 2020.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous\ncontrol using generalized advantage estimation, 2018.\nJohn Schulman. Approximating kl divergence, 2021. URL http://joschu.net/blog/kl-approx.html.\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh\nTobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. CoRR, abs/1707.01495, 2017. URL\nhttp://arxiv.org/abs/1707.01495.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015.\nNing Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12(1):145–151, 1999.\nISSN 0893-6080. doi:https://doi.org/10.1016/S0893-6080(98)00116-6. URL https://www.sciencedirect.com/\nscience/article/pii/S0893608098001166.\nYann N. Dauphin, Harm de Vries, and Yoshua Bengio. Equilibrated adaptive learning rates for non-convex optimization,\n2015.\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised\nprediction. In ICML, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need, 2017.\n6\nPearl\nA PREPRINT\nA\nAdamES Code\nclass AdamES(BaseAgent):\ndef __init__(\nself,\nenv: VectorEnv,\nmodel: ActorCritic,\nupdater_class: Type[BaseEvolutionUpdater] = NoisyGradientAscent,\nlearning_rate: float = 1,\nmomentum_weight: float = 0.9,\ndamping_weight: float = 0.999,\nbuffer_class: Type[BaseBuffer] = RolloutBuffer,\nbuffer_settings: BufferSettings = BufferSettings(),\naction_explorer_class: Type[BaseExplorer] = BaseExplorer,\nexplorer_settings: ExplorerSettings = ExplorerSettings(start_steps=0),\ncallbacks: Optional[List[Type[BaseCallback]]] = None,\ncallback_settings: Optional[List[CallbackSettings]] = None,\nlogger_settings: LoggerSettings = LoggerSettings(),\ndevice: Union[T.device, str] = \"auto\",\nrender: bool = False,\nseed: Optional[int] = None,\n) -> None:\nsuper().__init__(\nenv=env,\nmodel=model,\naction_explorer_class=action_explorer_class,\nexplorer_settings=explorer_settings,\nbuffer_class=buffer_class,\nbuffer_settings=buffer_settings,\nlogger_settings=logger_settings,\ncallbacks=callbacks,\ncallback_settings=callback_settings,\ndevice=device,\nrender=render,\nseed=seed,\n)\nself.learning_rate = learning_rate\nself.momentum_weight = momentum_weight\nself.damping_weight = damping_weight\nself.updater = updater_class(model=self.model)\nself.m = 0\nself.v = 0\nself.adam_step = 1\ndef _adam(self, grad: np.floating) -> np.floating:\n\"\"\"Adam optimizer update\"\"\"\nself.m = (1 - self.momentum_weight) * grad + self.momentum_weight * self.m\nself.v = (1 - self.damping_weight) * (\ngrad * grad\n) + self.damping_weight * self.v\nm_adj = self.m / (1 - (self.momentum_weight ** self.adam_step))\nv_adj = self.v / (1 - (self.damping_weight ** self.adam_step))\nself.adam_step += 1\nreturn m_adj / (np.sqrt(v_adj) + 1e-8)\ndef _fit(\nself, batch_size: int, actor_epochs: int = 1, critic_epochs: int = 1\n) -> Log:\ndivergences = np.zeros(actor_epochs)\nentropies = np.zeros(actor_epochs)\ntrajectories = self.buffer.all(flatten_env=False)\nrewards = trajectories.rewards.squeeze()\n7\nPearl\nA PREPRINT\nrewards = filter_rewards(rewards, trajectories.dones.squeeze())\nif rewards.ndim > 1:\nrewards = rewards.sum(axis=-1)\nscaled_rewards = scale(rewards)\ngrad_approx = np.dot(self.updater.normal_dist.T, scaled_rewards) / (\nnp.mean(self.updater.std) * self.env.num_envs\n)\noptimization_direction = self._adam(grad_approx)\nfor i in range(actor_epochs):\nlog = self.updater(\nlearning_rate=self.learning_rate,\noptimization_direction=optimization_direction,\n)\ndivergences[i] = log.divergence\nentropies[i] = log.entropy\nself.buffer.reset()\nreturn Log(divergence=divergences.sum(), entropy=entropies.mean())\nB\nHyperparameters\nn\n10\nσ\n1\nα\n0.1\nβ1\n0.9\nβ2\n0.999\nseed\n0\nC\nLearning Rate Performance\nFigure 5 and Figure 6 show how the learning curves and population KL divergences change as learning rate, α, is\nadjusted. The key thing to note is the nonlinear relationship in both convergence speed and steady state divergence.\nD\nSampling Noise Performance\nFigure 7 and Figure 8 show how the learning curves and population KL divergences change as population sampling\nnoise standard deviation, σ, is adjusted. Because the population size is kept constant, the larger sampling noise standard\ndeviation leads to reduced information density and more noisy gradient ascent approximations. This results in slower\nconvergence and worse steady state divergence. However, it is often useful to increase this parameter to help avoid local\ntraps. When this is necessary, the population size should also be increased to maintain information density.\nE\nPopulation Size Performance\nFigure 9 and Figure 10 show how the learning curves and population KL divergences change as population size, n, is\nadjusted. With a constant sampling noise, increasing population size increases the information density of sampling,\nleading to better steady state dynamics. The effect on convergence speed is dependent on the function being optimized\ndue to the ratio of gradient approximations used in the momentum and dampening terms.\n8\nPearl\nA PREPRINT\n0\n50\n100\n150\n200\n250\nStep\n-125\n-100\n-75\n-50\n-25\n0\nReward\nSphere\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-6\n-4\n-2\n0\nReward\nMatyas\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-10\n-7.5\n-5\n-2.5\n0\nReward\nAckley\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-10\n-8\n-6\n-4\n-2\n0\nReward\nBeale\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-400\n-200\n0\nReward\nEggHolder\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-20\n-10\n0\nReward\nHimmelblau\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-40\n-30\n-20\n-10\n0\nReward\nRastrigin\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-6,000\n-4,000\n-2,000\n0\nReward\nRosenbrock\n0.5\n1.5\n2.5\nFigure 5: Learning curves with different learning rates\n0\n50\n100\n150\n200\n250\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\nKL Divergence\nSphere\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n5\n10\n4\n10\n3\n10\n2\nKL Divergence\nMatyas\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\nKL Divergence\nAckley\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\nKL Divergence\nBeale\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n4\n10\n3\n10\n2\nKL Divergence\nEggHolder\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\nKL Divergence\nHimmelblau\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\nKL Divergence\nRastrigin\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\nKL Divergence\nRosenbrock\n0.5\n1.5\n2.5\nFigure 6: Population divergence with log scale and different learning rates\n0\n50\n100\n150\n200\n250\nStep\n-100\n-80\n-60\n-40\n-20\n0\nReward\nSphere\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-2\n-1.5\n-1\n-0.5\n0\nReward\nMatyas\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-10\n-7.5\n-5\n-2.5\n0\nReward\nAckley\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-40\n-30\n-20\n-10\n0\nReward\nBeale\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n0\n5\n10\n15\n20\n25\nReward\nEggHolder\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-200\n-150\n-100\n-50\n0\nReward\nHimmelblau\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-40\n-30\n-20\n-10\n0\nReward\nRastrigin\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n-500,000\n-400,000\n-300,000\n-200,000\n-100,000\n0\nReward\nRosenbrock\n0.5\n1.5\n2.5\nFigure 7: Learning curves with different population sampling noise\n9\nPearl\nA PREPRINT\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nKL Divergence\nSphere\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nKL Divergence\nMatyas\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nKL Divergence\nAckley\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nKL Divergence\nBeale\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nKL Divergence\nEggHolder\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nKL Divergence\nHimmelblau\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nKL Divergence\nRastrigin\n0.5\n1.5\n2.5\n0\n50\n100\n150\n200\n250\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nKL Divergence\nRosenbrock\n0.5\n1.5\n2.5\nFigure 8: Population divergence with log scale and different population sampling noise\n0\n100\n200\n300\nStep\n-30\n-20\n-10\n0\nReward\nSphere\n10\n20\n30\n0\n100\n200\n300\nStep\n-15\n-10\n-5\n0\nReward\nMatyas\n10\n20\n30\n0\n100\n200\n300\nStep\n-12.5\n-10\n-7.5\n-5\n-2.5\n0\nReward\nAckley\n10\n20\n30\n0\n100\n200\n300\nStep\n-10,000\n-5,000\n0\nReward\nBeale\n10\n20\n30\n0\n200\n400\n600\n800\nStep\n40\n50\n60\nReward\nEggHolder\n10\n20\n30\n0\n100\n200\n300\nStep\n-100\n-75\n-50\n-25\n0\nReward\nHimmelblau\n10\n20\n30\n0\n100\n200\n300\nStep\n-50\n-40\n-30\n-20\n-10\n0\nReward\nRastrigin\n10\n20\n30\n0\n100\n200\n300\nStep\n-12,500\n-10,000\n-7,500\n-5,000\n-2,500\n0\nReward\nRosenbrock\n10\n20\n30\nFigure 9: Learning curves with different population size\n0\n100\n200\n300\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nSphere\n10\n20\n30\n0\n100\n200\n300\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nMatyas\n10\n20\n30\n0\n100\n200\n300\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nAckley\n10\n20\n30\n0\n100\n200\n300\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nBeale\n10\n20\n30\n0\n200\n400\n600\n800\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nEggHolder\n10\n20\n30\n0\n100\n200\n300\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nHimmelblau\n10\n20\n30\n0\n100\n200\n300\nStep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nRastrigin\n10\n20\n30\n0\n100\n200\n300\nStep\n10\n6\n10\n5\n10\n4\n10\n3\nKL Divergence\nRosenbrock\n10\n20\n30\nFigure 10: Population divergence with log scale and different population size\n10\n",
  "categories": [
    "cs.LG",
    "cs.NE"
  ],
  "published": "2022-01-24",
  "updated": "2022-01-24"
}