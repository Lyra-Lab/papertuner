{
  "id": "http://arxiv.org/abs/2201.12093v2",
  "title": "PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings",
  "authors": [
    "Qiyu Wu",
    "Chongyang Tao",
    "Tao Shen",
    "Can Xu",
    "Xiubo Geng",
    "Daxin Jiang"
  ],
  "abstract": "Learning sentence embeddings in an unsupervised manner is fundamental in\nnatural language processing. Recent common practice is to couple pre-trained\nlanguage models with unsupervised contrastive learning, whose success relies on\naugmenting a sentence with a semantically-close positive instance to construct\ncontrastive pairs. Nonetheless, existing approaches usually depend on a\nmono-augmenting strategy, which causes learning shortcuts towards the\naugmenting biases and thus corrupts the quality of sentence embeddings. A\nstraightforward solution is resorting to more diverse positives from a\nmulti-augmenting strategy, while an open question remains about how to\nunsupervisedly learn from the diverse positives but with uneven augmenting\nqualities in the text field. As one answer, we propose a novel Peer-Contrastive\nLearning (PCL) with diverse augmentations. PCL constructs diverse contrastive\npositives and negatives at the group level for unsupervised sentence\nembeddings. PCL performs peer-positive contrast as well as peer-network\ncooperation, which offers an inherent anti-bias ability and an effective way to\nlearn from diverse augmentations. Experiments on STS benchmarks verify the\neffectiveness of PCL against its competitors in unsupervised sentence\nembeddings.",
  "text": "PCL: Peer-Contrastive Learning with Diverse Augmentations for\nUnsupervised Sentence Embeddings\nQiyu Wu1∗, Chongyang Tao2, Tao Shen2, Can Xu2, Xiubo Geng2, Daxin Jiang2†\n1The University of Tokyo, Tokyo, Japan\n2Microsoft Corporation\n1qiyuw@g.ecc.u-tokyo.ac.jp\n2{chotao,shentao,caxu,xigeng,djiang}@microsoft.com\nAbstract\nLearning sentence embeddings in an unsuper-\nvised manner is fundamental in natural lan-\nguage processing.\nRecent common practice\nis to couple pre-trained language models with\nunsupervised contrastive learning, whose suc-\ncess relies on augmenting a sentence with\na semantically-close positive instance to con-\nstruct contrastive pairs.\nNonetheless, exist-\ning approaches usually depend on a mono-\naugmenting strategy, which causes learning\nshortcuts towards the augmenting biases and\nthus corrupts the quality of sentence embed-\ndings.\nA straightforward solution is resort-\ning to more diverse positives from a multi-\naugmenting strategy, while an open question\nremains about how to unsupervisedly learn\nfrom the diverse positives but with uneven aug-\nmenting qualities in the text ﬁeld.\nAs one\nanswer, we propose a novel Peer-Contrastive\nLearning (PCL) with diverse augmentations.\nPCL constructs diverse contrastive positives\nand negatives at the group level for unsuper-\nvised sentence embeddings.\nPCL performs\npeer-positive contrast as well as peer-network\ncooperation, which offers an inherent anti-bias\nability and an effective way to learn from di-\nverse augmentations.\nExperiments on STS\nbenchmarks verify the effectiveness of PCL\nagainst its competitors in unsupervised sen-\ntence embeddings.1\n1\nIntroduction\nSentence embedding learning, which aims at deriv-\ning semantically meaningful ﬁxed-sized vectors\nfor sentences, is a natural language processing\n(NLP) technique of great signiﬁcance, especially\nfor time-sensitive downstream tasks (Reimers and\nGurevych, 2019). Recently, contrastive learning\n(CL) is proven effective to learn representation (Wu\n∗Work done during the internship at Microsoft.\n† Corresponding author.\n1Our implementation is available at https://github.\ncom/qiyuw/PeerCL.\nAugmenting\nOrder\nN-gram\nBag-of-words\nShufﬂed Sentence\n×\n×\n✓\nInversed Sentence\n×\n✓\n✓\nWord Repetition\n✓\n×\n✓\nWord Deletion\n✓\n×\n×\nTable 1: Text augmentation strategies change semantics in\nthe sentence but still has shortcuts to learn. Employing limited\nnumber of strategies causes learning shortcuts towards the\naugmenting bias.\net al., 2018; Tian et al., 2020; He et al., 2020) and\nsubstantially improve its performance (Yan et al.,\n2021; Gao et al., 2021) when coupling with pre-\ntrained language models (PLMs). The main idea\nof contrastive learning for sentence embedding is\npulling semantic neighbors together and pushing\nsemantic non-neighbors apart (Hadsell et al., 2006),\nwhich naturally requires effective contrastive pairs.\nAs effective contrastive pairs are usually scarce\nand require much human effort to collect, how to\nlearn sentence embeddings in an fully unsupervised\nmanner has become a challenging yet attractive re-\nsearch area (Wang et al., 2021; Giorgi et al., 2021).\nThe key to unsupervised contrastive learning for\nsentence embedding is to augment a given anchor\nsentence with an effective positive instance to con-\nstruct the pairs. Hence, many efforts have been\nmade to design augmentation methods by adding\nnoises or using heuristics, which mainly fall into\ntwo categories in terms of augmentation format –\ndiscrete and continuous. The former operates di-\nrectly on words or n-grams in the sentence, e.g.,\nsynonym substitution (Su et al., 2021b), shufﬂing\nand word deletion (Yan et al., 2021). The latter\noperates on latent embeddings derived by neural\nencoder(s), e.g., SimCSE with twice dropouts (Gao\net al., 2021). However, these existing approaches\nusually depend on a mono-augmenting format (i.e.,\neither discrete or continuous) with a limited number\nof augmenting strategies, which suffer from learn-\narXiv:2201.12093v2  [cs.CL]  19 Oct 2022\nEncode\nEncode\nEncode\nContrast\nContrast\nContrast\n𝑓𝜃\n𝑓𝜃\n𝑓𝜃\n(a) Conventional CL\nPeer-positive \nContrast\nPeer-network \nCooperation\nEncode\nEncode\nEncode\n𝑓𝜃\n𝑓𝜃\n𝑓𝜃\n𝑓𝜃′\n𝑓𝜃′\n𝑓𝜃′\nPeer-positive \nContrast\nPeer-positive \nContrast\nPeer-network \nCooperation\n(b) PCL with diverse augmentations\nEmbedding Space\nSimilarity\nSimilarity\nSimilarity\nSimilarity\nAgreement\n(c) Cooperative learning objective\nFigure 1: The main idea of PCL. Circles and triangles denote contrastive instances encoded by fθ and fθ′, respectively. The red\nones denote ineffective positives, blue ones denote effective positives, and gray ones denote negatives. (a) Conventional CL\non ineffective positive instances causes shortcut learning towards mono-augmenting biases. (b) PCL improves the probability\nof ‘at-least-one’ effective positives and performs contrasts among peer positives and cooperation between peer networks. PCL\nmaintains two networks, and each network learns from its peer network to achieve a common agreement. (c) Cooperative\nlearning objective considers peer-positive contrasts to achieve PCL with diverse augmentations. The top panel illustrates the\nconsistency of embedding space, while the bottom panel illustrates the agreement between the peer networks.\ning shortcuts (Ilyas et al., 2019; Du et al., 2021)\ntowards the augmenting biases and thus corrupt\nthe quality of learned embeddings. For example,\nlearning shortcuts caused by discrete augmenting\nbiases are shown in Table 1, and SimCSE based\nsolely on dropout in continuous format is biased\ntowards the sentence length (Wu et al., 2021b).\nTo prevent the learning shortcuts caused by the\npotential biases from the mono-augmenting strat-\negy, one straightforward solution coming into our\nmind is that we can consider more diverse aug-\nmentations for a given sentence in both continuous\nand discrete formats. Besides learning from di-\nverse instances for inherent anti-bias ability, it can\nalso bring a great opportunity for more effective\nlearning. In particular, controlling the qualities of\nthe noisy and heuristic augmentations for different\nsentences is almost impossible2. As illustrated in\nFigure 1(a), the resulting contrastive instances may\nbecome ineffective and even poisonous for conven-\ntional CL. Nonetheless, diverse instances from var-\nious augmentation strategies can notably improve\nthe possibility of at-least-one effective positives in\nthe contrastive instances, so how to leverage the\nrich relations among the diverse augmentations for\nmore effective CL is worth exploiting.\nTo this end, we propose a brand-new Peer-\nContrastive Learning (PCL) with diverse augmen-\ntations, and an illustration of its overall framework\n2It may cause poisonous positive. For example, given a\nsentence “A dog is chasing a cat.”, a possible shufﬂed sentence\nis “A cat is chasing a dog.”, which is semantically different.\nis shown in Figure 1(b). Firstly, PCL not only per-\nforms the vanilla positive-negative contrasts but\nalso takes the opportunity to learn the rich struc-\ntured relations among the diverse positives (i.e.,\npeer-positive) to highlight the more possibly effec-\ntive ones. Then, to learn the structured relations in\na fully unsupervised manner, we propose a coop-\nerative learning framework consisting of two peer\nembedding networks (i.e., peer-network). The two\nnetworks learn from each other to prevent error re-\ninforcement in sole-network and achieve a common\nagreement from different views (as shown in Fig-\nure 1(c)). Consequently, the sentence embedding\nnetwork is equipped with (i) anti-bias abilities by\nCL on the diverse augmentations and (ii) improved\neffectiveness by the unsupervised PCL, leading to\na high quality of sentence embeddings.\nWe conduct experiments on 7 standard semantic\ntextual similarity (STS) tasks (Agirre et al., 2012,\n2013, 2014, 2015, 2016; Cer et al., 2017; Marelli\net al., 2014) to evaluate PCL. Results demonstrate\nthat PCL signiﬁcantly outperforms state of the art\non 7 STS tasks. Typically, PCL achieves a 2.85%\nimprovement over the previous best approach in the\naveraged Spearman’s correlation of 7 STS tasks in\nthe BERTbase setting. PCL also outperforms previ-\nous approaches across different PLMs initialization\nand model sizes. Moreover, ablation study and anal-\nysis show that the two proposed components, i.e.,\npeer-positive contrasts and peer-network coopera-\ntion, are both capable of improving unsupervised\nsentence embedding learning.\n2\nPeer-Contrastive Learning (PCL)\nThis section begins with a formal deﬁnition of un-\nsupervised sentence embeddings, followed by de-\ntailed formulations of our PCL with diverse aug-\nmentations (§2.1 and §2.2). Lastly, we will elab-\norate on our training and inference procedure for\nunsupervised sentence embeddings (§2.3).\nUnsupervised Sentence Embedding.\nGiven a\nsentence xi ∼X, the target of this task is to learn\na neural network fθ (parameterized by θ) without\nany human-labeled data. Then, the network can be\napplied to x and derive a dense real-valued vector\nrepresentation, i.e., hi = fθ(xi) ∈Rd. Conse-\nquently, hi can be used to represent the seman-\ntics of xi and fulﬁll downstream sentence-related\ntasks, e.g., semantic textual similarity. Thereby,\nthis task depends on the designs of unsupervised\n(a.k.a self-supervised) objectives based on X to\nlearn fθ effectively.\n2.1\nContrastive Representation Learning\nThe recent common practice of representation\nlearning in an unsupervised manner is contrastive\nlearning (Zhang et al., 2020; Yan et al., 2021; Gao\net al., 2021), which aims to learn effective repre-\nsentations to pull similar instances together and\npush apart the other instances. Thereby, compared\nto supervised contrastive learning that has already\noffered contrastive pairs, how to augment the given\nanchor (e.g., an image and sentence) with effective\npositive and negative instances to construct the con-\ntrastive pairs is critical in the unsupervised scenario.\nMore recently, a simple contrastive learning frame-\nwork (SimCLR) is proposed in visual representa-\ntion learning (Chen et al., 2020), which constructs\npositive instances by different views (e.g., chop)\nof an image then learn to minimize the following\nInfoNCE loss.\nL(c)\nθ (X, δ; θ) =\n(1)\n−Exi∼X\n\"\nlog\nes[fθ(xi),fθ(δ(xi))]/τ\nP\nxj∼X∧j̸=i∨δ(xi) es[fθ(xi),fθ(xj)]/τ\n#\n,\nwhere δ(·) denotes using a different view of the\nimage xi as the positive instance during visual\ncontrastive learning, xj denotes negative instances\nagainst xi to construct contrastive pairs with δ(·),\nand s[·, ·] denotes a similarity metric between two\ndense vectors. And L(c)\nθ\ndenotes this loss function\nis optimized w.r.t the subscript θ. It is also notewor-\nthy that xj ∼X is usually implemented by using\nother in-batch instances during mini-batch SGD\n(a.k.a in-batch negatives).\nHowever, when switching to unsupervised sen-\ntence embedding, augmenting an input sentence by\na fully random chop or permutation may become\nvery intractable. This is because these operations\nare most likely to destroy the original sentence in\nboth semantics and syntax and cause trivial posi-\ntive augmentations. Hence, many research efforts\nhave been made to design δ for effective positive\naugmentations in the NLP community. These ef-\nforts mainly fall into two categories in terms of\naugmentation format – ‘discrete’ and ‘continuous’.\nDiscrete augmentation format denotes operating di-\nrectly on the inputted sentence, where δ is deﬁned\nas word deletion, shufﬂing (Yan et al., 2021), back\ntranslation (Xie et al., 2020), etc. In contrast, con-\ntinuous ones operate on hidden states or network\nparameters, where δ is deﬁned as network twice\ndropout (Gao et al., 2021), etc.\nNonetheless, compared to visual contrastive\nlearning that barely introduces new data distribu-\ntion for the positive instances, such heuristic aug-\nmentation methods in the text ﬁeld cause shortcut\nlearning (Ilyas et al., 2019; Du et al., 2021) – each\nmethod exposes the learning procedure to potential\nbiases towards the augmented instances and thus\ncorrupts the quality of learned embeddings. There-\nfore, existing unsupervised contrastive sentence\nembedding works usually depend on the limited-\nor even mono-augmenting methods for their posi-\ntive instances and inevitably suffer from the biases\nin the positive instances.\n2.2\nContrast-Cooperation with Peers\nTo prevent learning shortcuts caused by the po-\ntential biases from mono-augmenting strategy and\nexploit rich relations among diverse augmenta-\ntions for more effective positives, we propose a\nbrand-new contrastive learning method, called peer-\ncontrastive learning (PCL). Besides the vanilla\ncontrastive objective, it contains a novel ‘contrast-\ncooperation’ learning mechanism, which we will\ndetail in the following.\n2.2.1\nMulti-Augmenting Strategy\nFirst, we adopt a multi-augmenting strategy for ex-\ntensively diverse augmentations. Given a sentence\nxi ∼X, it considers extensive augmentation meth-\nods from both continuous and discrete perspectives.\nThis can be formally written as\n∆= {δk|δk ∈∆(c) ∪∆(d)},\n(2)\nwhere ∆denotes a set of multiple augmentation\nmethods from both the continuous ∆(c) set and\ndiscrete ∆(d) set, and |∆| = K. Then, we can\nobtain diverse augmented positives by applying ∆\nto a sentence xi ∼X, i.e.,\nˆXi = {xi\nk = δk(xi)|δk ∈∆}.\n(3)\nThe contrastive sentence embedding based on\ndiverse positives can mitigate the biases towards\nmono-augmenting strategy, but it comes with a\ndouble-edged sword. That is, the ˆxi\nk ∼ˆXi varies\na lot with many factors (e.g., input sentence xi\nand augmentation method δk), making it hard to\ncontrol the quality of each xi\nk. To one extreme,\none augmentation can become ineffective and even\npoisonous if its semantics is largely changed and\nthus corrupt the model.\n2.2.2\nContrast among Peer Positives\nTo effectively learn from the uneven qualities of\nthe augmented positives, we propose a brand-new\npeer-contrastive learning framework that not only\nperforms the vanilla positive-negative contrast but\na positive-positive contrast. This is because our\ndiverse augmentations provide a great opportunity\nto model rich structured relations among the posi-\ntives and improve the probability of ‘at-least-one’\neffective positive in ˆXi. And the positive-positive\ncontrast can mimic ‘peer-competition’ to highlight\nmore likely effective positives but weaken the oth-\ners’ effects by suppressing them.\nFormally, we ﬁrst derive a group-wise probabil-\nity distribution by contrasting the anchor xi with\nboth diverse positives ˆXi and in-batch negatives\nxj ∼X ∧j ̸= i. That is,\npP-Cf\nθ1,θ2(xi) := P-Cf(xi, ∆(d); θ1, θ2) =\n(4)\nsoftmax({s[fθ1(xi), fθ2(ˆxi\nk)/τ]}ˆxi\nk∼ˆ\nXi+\n{s[fθ1(xi), fθ2(xj)/τ]}xj∼X∧j̸=i),\nwhere ‘+’ here denotes a union of two sets. Identi-\ncal to the vanilla contrastive sentence embedding\n(Gao et al., 2021), we also leverage a softmax\nnormalization to fulﬁll peer-contrast among aug-\nmented positives. Please note we introduce θ1 and\nθ2 for clear deliveries in the remaining sections,\nand the two parameters here can be either tied (i.e.,\nθ1 = θ2 = θ) or not. Although using the aug-\nmented positives to ‘compete’ each peer sounds\nattractive for contrastive learning, one critical ques-\ntion remains about how to learn merely from effec-\ntive positives and guide the positive-peer contrasts\npP-Cf\nθ1,θ2(xi) in a fully unsupervised way.\n2.2.3\nCooperation across Peer Networks\nWe propose a cooperative learning framework to\nlearn contrasts among the augmented positives. It\ncontains two peer embedding networks, and the\ntwo networks learn from each other to prevent er-\nror reinforcement in sole-network and achieve a\ncommon agreement from different views. Specif-\nically, we ﬁrst build a peer network θ′ which acts\nlike a momentum encoder (He et al., 2020) to coop-\neratively learn with θ. Here, θ and θ′ can be untied\nor even heterogeneous. Then, we present the loss of\nthe momentum-like cooperative learning, which is\na combination of two Kullback–Leibler divergence\nlosses. That is\nL(p)\nθ,θ′(X, ∆; θ, θ′) =\n(5)\nExi[KL[pP-Cf\nθ,θ (xi), pP-Cf\nθ′,θ′(xi)]+\nKL[pP-Cf\nθ,θ′ (xi), pP-Cf\nθ′,θ (xi)]].\nWe call this ‘momentum-like’ since θ′ is not\nstrictly a history of θ for more different views\nbut depends on the second KL term to pre-\nvent signiﬁcant divergence from θ. Meanwhile,\nthe ﬁrst KL term is to reach an agreement be-\ntween the main network θ and its peer network\nθ′. This ‘learning-from-agreement’ paradigm, in-\ncluding mutual-distillation (Zhang et al., 2018)\nand denosing-by-agreement (Wei et al., 2020), is\nproven effective in improving performance and\nlearning from label noises by prior supervised\nworks. In contrast, we hold a distinct motivation\nthat the implicit relations in a group of diverse\npositives and in-batch negatives are expected to un-\nsupervisedly match each peer embedding network\nwith another view (e.g., structures and parameters).\nRemark.\nThe meaning of ‘peer’ has two folds:\n(i) It denotes that we want to learn the rich struc-\ntured relations among the diverse augmented posi-\ntives to highlight the effective ones; (ii) It involves\na cooperative learning framework based on peer\nnetworks for modeling positive-positive contrasts\nto achieve PCL with diverse augmentations.\n2.3\nTraining and Inference\nTraining Objective.\nWe write the loss as a com-\nbination of (i) our proposed contrast-cooperation\nlearning for both θ and θ′ simultaneously to high-\nlight more effective positives and (ii) vanilla con-\ntrastive learning that is applied to θ and θ′ sepa-\nrately and based on our diverse augmentations ∆\nfor their strong anti-bias initializations. That is,\nL(PCL) = L(p)\nθ,θ′(X, ∆; θ, θ′)+\n(6)\nβ\nX\nδk∈∆\nh\nL(c)\nθ (X, δk; θ) + L(c)\nθ (X, δk; θ′)\ni\n,\nwhere β is a hyperparameter to control if the train-\ning inclines to vanilla CL for the anti-bias purpose.\nHence, β could be annealing to provide strong unbi-\nased initializations for different views at the begin-\nning and then focus on contrast-cooperation with\npeers for more effective learning.\nInference.\nDue to the symmetrical learning\nparadigm, we empirically found θ and θ′ achieve\ncomparable performance in our pilot experiments.\nNonetheless, we only use the main embedding net-\nwork θ rather than ensembles them to encode each\nsentence for fair comparisons with its competitors.\n3\nExperiments\n3.1\nUnsupervised Corpus and Benchmark\nWe train and evaluate our model in a fully unsuper-\nvised manner. Following Gao et al. (2021), we train\nour model on 106 randomly sampled sentences\nfrom Wikipedia English. We evaluate our model\non the semantic textual similarity (STS) tasks with-\nout using any STS training data. We report re-\nsults on 7 datasets, namely the STS benchmark\n(STSb) (Cer et al., 2017) the SICK-Relatedness\n(SICK-R) dataset (Marelli et al., 2014) and the STS\ntasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014,\n2015, 2016) (STS12-STS16). These datasets pro-\nvide a gold standard semantic similarity between\n0 and 5 for each sentence pair, which include texts\nfrom various domains, and we obtain them from\nthe SentEval toolkit (Conneau and Kiela, 2018).\n3.2\nImplementation of PCL\nAugmentation Strategies\nIn this paper we uti-\nlize ﬁve unsupervised augmentation strategies that\nare commonly adopted in previous works (Wei\nand Zou, 2019; Yan et al., 2021; Gao et al., 2021).\nAugmentations from discrete perspectives ∆(d) in-\ncludes: 1) Shufﬂed Sentence (SS) shufﬂes the posi-\ntion of words in the sentence; 2) Inverted Sentence\n(IS) inverts the original sentence as the augmented\nsample; 3) Words Repetition (WR) duplicates part\nof words and randomly insert them into the original\nsentences; 4) Words Deletion (WD) deletes part of\nwords in the sentences. The augmentations from\nthe continuous perspective ∆(c) include Dropout\n(DP). It generates augmentation instances in the\nembedding level by passing the original sentence\nagain into the encoder with different dropout masks.\nMore implementation details about augmentation\nare presented in § A due to the page limit.\nNetwork Implementation\nWe initialize the net-\nworks θ and θ\n′ with the PLMs checkpoint down-\nloaded from Huggingface’s Transformers3 of\nBERT (Devlin et al., 2019) or RoBERTa (Liu et al.,\n2019). The encoder consists of 12 and 24 Trans-\nformer layers for the base and large model, respec-\ntively. The hidden size is set to 768 and 1024, and\nthe number of attention heads is set to 12 and 16\nfor base and large models, respectively. We choose\nthe representation of the [CLS] token as the em-\nbedding of the input sentence. The hyperparameter\nβ is set to 1 for training simplicity without tuning,\nand the number of augmentations K is 9 for base\nmodels. Due to the computation resource limita-\ntion, particularly for large models, we set K to 4,\nand the two networks θ and θ\n′ are tied, in which\nthe cooperative learning is performed between the\ntwo passes through the network.\nTraining Setups.\nWe follow common practices\nand carry out preliminary grid search on the devel-\nopment set of STSb to decide the hyper-parameter\nconﬁguration. The learning rate is set to 3e-5 for\nbase models and 1e-5 for large models, respec-\ntively. Except for learning rate, We use the same\ntraining hyper-parameters for all experiments with\nthe batch size of 64 and the maximum length of\n32. The temperature parameter τ is set to 0.05, and\nthe dropout probability is set to 0.1. We train our\nmodel for 1 epoch and evaluate the model on the\nSTSb development set every 125 steps, and keep\nthe best checkpoint by following Gao et al. (2021).\nEvaluation Setups.\nWe evaluate PCL on 7 STS\ntasks, including STS12-STS16, STSb, and SICK-R\nas introduced in § 3.1. No training data of STS\ntasks are used during training and evaluation. Gao\n3https://github.com/huggingface/transformers\nModel\nSTS12\nSTS13\nSTS14\nSTS15\nSTS16\nSTSb\nSICK-R\nAvg.\nGloVe embeddings (avg.)\n55.14\n70.66\n59.73\n68.25\n63.66\n58.02\n53.76\n61.32\nBERTbase (ﬁrst-last avg.)\n39.70\n59.38\n49.67\n66.03\n66.19\n53.87\n62.06\n56.70\nBERTbase-ﬂow\n58.40\n67.10\n60.85\n75.16\n71.22\n68.66\n64.47\n66.55\nBERTbase-whitening\n57.83\n66.90\n60.90\n75.08\n71.31\n68.24\n63.73\n66.28\nIS-BERTbase\n56.77\n69.24\n61.21\n75.23\n70.16\n69.21\n64.25\n66.58\nCT-BERTbase\n61.63\n76.80\n68.47\n77.50\n76.48\n74.31\n69.19\n72.05\nConSERTbase\n64.64\n78.49\n69.07\n79.72\n75.95\n73.97\n67.31\n72.74\nSG-OPTbase\n66.84\n80.13\n71.23\n81.56\n77.17\n77.23\n68.16\n74.62\nBERTbase-Mirror\n69.10\n81.10\n73.00\n81.90\n75.70\n78.00\n69.10\n75.50\nSimCSE-BERTbase\n68.40\n82.41\n74.38\n80.91\n78.56\n76.85\n72.23\n76.25\nPCL-BERT†\nbase\n72.84\n83.81\n76.52\n83.06\n79.32\n80.01\n73.38\n78.42\n– Avg. of seeds†∗\n72.74\n83.36\n76.05\n83.07\n79.26\n79.72\n72.75\n78.14\nRoBERTabase (ﬁrst-last avg.)\n40.88\n58.74\n49.07\n65.63\n61.48\n58.55\n61.63\n56.57\nRoBERTabase-whitening\n46.99\n63.24\n57.23\n71.36\n68.99\n61.36\n62.91\n61.73\nDeCLUTR-RoBERTabase\n52.41\n75.19\n65.52\n77.12\n78.63\n72.41\n68.62\n69.99\nRoBERTabase-Mirror\n66.60\n82.70\n74.00\n82.40\n79.70\n79.60\n69.70\n76.40\nSimCSE-RoBERTabase\n70.16\n81.77\n73.24\n81.36\n80.65\n80.22\n68.56\n76.57\nPCL-RoBERTa†\nbase\n71.13\n82.38\n75.40\n83.07\n81.98\n81.63\n69.72\n77.90\n– Avg. of seeds†∗\n71.54\n82.70\n75.38\n83.31\n81.64\n81.61\n69.19\n77.91\nBERTlarge-ﬂow\n65.20\n73.39\n69.42\n74.92\n77.63\n72.26\n62.50\n70.76\nSG-OPTlarge\n67.02\n79.42\n70.38\n81.72\n76.35\n76.16\n70.20\n74.46\nConSERTlarge\n70.69\n82.96\n74.13\n82.78\n76.66\n77.53\n70.37\n76.45\nSimCSE-RoBERTalarge\n72.86\n83.99\n75.62\n84.77\n81.80\n81.98\n71.26\n78.90\nPCL-RoBERTa†\nlarge\n74.08\n84.36\n76.42\n85.49\n81.76\n82.79\n71.51\n79.49\n– Avg. of seeds†∗\n73.76\n84.59\n76.81\n85.37\n81.66\n82.89\n70.33\n79.34\nPCL-BERT†\nlarge\n74.87\n86.11\n78.29\n85.65\n80.52\n81.62\n73.94\n80.14\n– Avg. of seeds†∗\n74.89\n85.88\n78.33\n85.30\n80.13\n81.39\n73.66\n79.94\nTable 2: The models’ performance comparison on STS tasks. We report the Spearman’s correlation ρ (%) on 7 STS datasets.\nWe highlight the highest numbers among models with the same pre-trained encoder. †: Our models. ∗: We also run our models\nﬁve times with different random seeds and report the average of these ﬁve results on each column as the ﬁnal number.\net al. (2021) has studied the evaluation settings\nfor sentence embedding. We adopt their sugges-\ntions and follow the standard settings in Sentence-\nBERT (Reimers and Gurevych, 2019). Speciﬁcally,\nwe do not train an additional regressor for STSb\nand SICK-R, use Spearman’s correlation as the\nmetric, concatenate all tasks and report the overall\nSpearman’s correlation. To fairly compare with\nprevious approaches, we use the evaluation scripts\nreleased by Gao et al. (2021)4. Moreover, we train\nour model for ﬁve times with different random\nseeds and report the average of these ﬁve results.\nWe also evaluate PCL on 7 transfer tasks (Con-\nneau and Kiela, 2018). PCL achieves competi-\ntive performance and the detailed results are pre-\nsented in Appendix C.2. As mentioned in previous\nworks (Reimers and Gurevych, 2019; Gao et al.,\n2021), the main goal of sentence embeddings is to\ncluster semantically similar sentences. Hence we\n4https://github.com/princeton-nlp/SimCSE\nonly take STS as the main results in this paper.\n3.3\nCompetitive Baselines\nWe compare our model with previous state-of-the-\nart unsupervised sentence embedding approaches,\nincluding basic embedding approaches (e.g. aver-\nage of GloVe (Pennington et al., 2014), BERT (De-\nvlin et al., 2019) or RoBERTa (Liu et al., 2019)\nembeddings) and contemporary contrastive learn-\ning approaches (e.g. IS-BERT (Zhang et al., 2020),\nConSERT (Yan et al., 2021), SG-OPT (Kim et al.,\n2021), Contrastive Tension (Carlsson et al., 2021),\nDeCLUTR (Giorgi et al., 2021), Mirror-BERT (Liu\net al., 2021b) and SimCSE (Gao et al., 2021)).\nSGPT (Muennighoff, 2022) and Sentence-T5 (Ni\net al., 2021) are proposed with new paradigm and\nfar larger models, which underperform with compa-\nrable model size. Trans-Encoder (Liu et al., 2021a)\nproposes a cooperative method with in-domain\npairwise data for mutual beneﬁts of bi- and cross-\nencoder, making the results incomparable. Please\nrefer to § B for more details.\n3.4\nMain Quantitative Results\nExperimental results on STS tasks are shown in\nTable 2. We can ﬁnd that our PCL signiﬁcantly\noutperforms the previous best result on all seven\ntasks as well as the average STS score with a large\nmargin compared to the baseline methods based\non BERTbase or RoBERTabase PLMs. Speciﬁcally,\nPCL improves the previous best result on average\nSTS score from 76.25 to 78.42 for BERTbase and\n76.57 to 77.91 for RoBERTabase, respectively. As\nSimCSE (Gao et al., 2021) did not report their\nperformance on BERTlarge, we compare all large\nmodels together, and the results are shown in the\nlast rows of the table. We can observe that PCL\noutperforms the best result on all tasks apart from\nthe STS16. Despite this, our PCL still obtain an im-\nprovement from 78.90 to 80.14 on the average STS\nscore. Our PCL achieves more signiﬁcant improve-\nment over the base models than the large models,\nAnd even so, PCL still outperforms SimCSE on al-\nmost all tasks in large models and all tasks in base\nmodels, which shows that PCL is effective across\ndifferent model sizes and different types of PLMs.\n3.5\nAnalysis of Diverse Augmentations\nDiversity and the number of augmentations are two\ncrucial factors of PCL. In this section, we test the\nperformance of PCL with varying K and diversity.\nFor PCL and all variants of PCL in this section, we\ntrain them for 5 times with different random seeds,\nand take the average as the ﬁnal results.\nThe number of augmentations.\nTo mitigate the\nmodel bias towards the mono-augmenting strat-\negy, we propose to augment the input sentence\nwith a group of positive instances. The number of\naugmentations K is a crucial hyper-parameter in\nthis framework. To check if the performance of\nPCL is sensitive to K, we conduct experiments on\nPCL-BERTbase with varying K on 7 STS tasks and\nreport the average STS score. We keep the diver-\nsity of augmentations ∆as much as possible when\nK > 1. As shown in Figure 2, the performance\nof PCL maintains an upward trend with increasing\nK. This indicates that multiple augmentation strat-\negy improves unsupervised sentence embeddings\ncompared with learning with mono-augmenting\nstrategy. This supports our motivation that con-\ntrastive learning with mono-augmenting strategy\ncauses learning shortcuts. More detailed results on\n1\n3\n5\n7\n9\n76.5\n77\n77.5\n78\n77.06\n77.12\n77.59\n77.75\n78.14\nNumber of augmentations\nAvg. Spearman’s correlation\nFigure 2: Effect of the number of augmentations.\nall 7 STS tasks are presented in § C.3.\nThe diversity of augmentations.\nAnother criti-\ncal factor is the diversity of augmentations. We\nﬁx K = 9 and reduce the diversity of augmenta-\ntions ∆to check if PCL is sensitive to the diver-\nsity. We conduct experiments on PCL-BERTbase\nwith K = 9 but only use one type of augmen-\ntation strategy from discrete and continuous per-\nspective, respectively. In other words, we keep at\nleast one DP augmentation for all variants. We\ncompare PCL with ﬁve mono-augmentation vari-\nants that are denoted as PCLDP, PCLSS, PCLIS,\nPCLWR and PCLWD, respectively. The details of\naugmentation strategies are introduced in § A. Av-\nerage Spearman’s correlation scores of 7 STS tasks\nare shown in the Figure 3. Experimental results\nshow that PCL signiﬁcantly outperforms its mono-\naugmenting variants, even keeping the K constant,\nindicating a better generalization. This supports our\nmotivation that PCL with diverse augmentations\ncan mitigate the shortcut learning biased towards\nmono-augmenting strategy. Particularly, SimCSE\nutilizes dual-dropout to construct the contrastive\npairs, hence the PCLDP variant (9 positive instances\ngenerated by the dropout) can be regarded as Sim-\nCSE w/ 9 augmented positive samples. Our pro-\nposed contrasts and cooperation among peers im-\nprove SimCSE from 76.25 to 77.14, but the score\nis still lower than PCL with a large margin. This is\nanother piece of evidence that shows the advantage\nof the diversity of augmentations. The detailed\nresults on all 7 STS tasks are presented in § C.3.\n3.6\nAblation Study\nWe ﬁrst check the impact of the proposed two\ncomponents of PCL, peer-network cooperation and\npeer-positive contrast, i.e., the two terms in Equa-\ntion 6 respectively. We designed two variants of\nPCL on PCL-BERTbase by removing the coopera-\ntion loss and contrast loss, which are denoted as\nPCLDP\nPCLSS\nPCLIS\nPCLWR\nPCLWD\nPCL\n75\n76\n77\n78\n77.14\n76.3\n76.97\n77.27\n77.17\n78.14\nAvg. Spearman’s correlation\nMono-Augmentation\nDiverse Augmentation\nFigure 3: Effect of the diversity of augmentations.\nTasks\nPCL\nPCLnoP\nPCLnoC\nSimCSE\nSTS12\n72.74\n71.15\n72.58\n68.40\nSTS13\n83.36\n83.07\n80.62\n82.41\nSTS14\n76.05\n75.72\n74.15\n74.38\nSTS15\n83.07\n82.93\n82.31\n80.91\nSTS16\n79.26\n78.37\n79.23\n78.56\nSTSb\n79.72\n78.67\n78.47\n76.85\nSICK-R\n72.75\n70.37\n72.06\n72.23\nAvg.\n78.14\n77.12\n77.06\n76.25\nTable 3: Ablation study on PCL-BERTbase. PCLnoP denotes\nPCL w/o peer-network cooperation. PCLnoC denotes PCL w/o\npeer-positive contrast.\nPCLnoP and PCLnoC respectively. To ensure the net-\nworks have the essential ability to learn sentence\nembeddings, we keep the contrastive loss with a\nsingle DP augmentation for PCLnoC, which is equal\nto the setting in Figure 2 when K = 1. We train\neach variant for ﬁve times with different random\nseeds and take the average of these seeds as the\nﬁnal results. As in Table 3, the average scores of\nPCL drop by 1.02 and 1.08 when removing the co-\noperation loss and contrast loss, respectively. This\nindicates that our proposed peer cooperation and\npeer contrast are both beneﬁcial to unsupervised\nlearning of sentence embeddings. Among the two\ncomponents, the peer cooperation loss plays a more\nimportant role as it incorporates contrasts among\npeer positives and peer networks, enabling an inher-\nent anti-bias ability and an effective way to learn\nfrom diverse augmentations.\nFixed peer encoder vs. trainable peer encoder.\nParticularly, We are also curious about the impact\nof ‘learning from agreement’ (i.e., the second term\nin Equation 5) in the cooperative learning objec-\ntive. Therefore, we further test additional vari-\nants of PCL with a ﬁxed peer encoder, denoted\nas PCLFixedP. Speciﬁcally, we download a check-\npoint of SimCSE as the peer encoder but ﬁx its\nSTS12\nSTS13\nSTS14\nSTS15\nSTS16\nSTSb\nSICK-R\nAvg.\n70\n72.5\n75\n77.5\n80\n82.5\nSpearman’s correlation\nPCLFixP\nPCL\nFigure 4: Ablation study on whether updating the extra peer\nencoder. PCLFixedP denotes a variant of PCL that cooperatively\nlearning with a ﬁxed peer network.\nparameters while training. Experimental results\nshow that the performance of PCLFixedP drops with\na large margin on STS12, STS13, STS14, STS15,\nSTSb, and the average STS. The reason may be\nthat although SimCSE is by far the best practice\nof sentence embeddings, it is still biased towards\na mono-augmenting strategy. Hence, cooperative\nlearning with a biased peer network can be harmful\nto the network with diverse augmentations. This\nalso indicates that it is necessary to simultaneously\nupdate the two peer networks and learn the agree-\nment between them in PCL. Furthermore, there\ncan be a considerable discrepancy between the em-\nbedding spaces produced by two methods, which\nhinders the cooperative training of two networks.\n4\nRelated Works\nUnsupervised Sentence Embedding.\nCommon\npractice of unsupervised sentence embedding is\nto take the average of pre-trained word embed-\ndings (Mikolov et al., 2013; Pennington et al.,\n2014) PLMs, like BERT (Devlin et al., 2019) or\nRoBERTa (Liu et al., 2019), Wu et al. (2021a) takes\nthe average of word embeddings as context embed-\nding to enhance the language pre-training. Other\nworks also take the [CLS] embedding from the\nlast layer of PLMs with post-processing (Li et al.,\n2020; Su et al., 2021a). Some works (Kiros et al.,\n2015; Logeswaran and Lee, 2018; Hill et al., 2016)\ndirectly train a deep model for sentence embed-\ndings using co-occurrence information. Recent ap-\nproaches couple PLMs with CL (Zhang et al., 2020;\nYan et al., 2021; Kim et al., 2021; Carlsson et al.,\n2021; Giorgi et al., 2021; Gao et al., 2021; Xie\net al., 2022) with a particular single strategy to con-\nstruct contrastive pairs. It is straightforward to ex-\ntend mono-augmentation into multi-augmentation\nto learn expressive representations. For example,\nCLEAR (Wu et al., 2020) uses various token/span\nmanipulations for noise-invariant representations\nwhile Mirror-BERT (Liu et al., 2021b) employs\nseveral fast augmentation strategies.\nHowever,\nthese methods usually take the augmented positives\nequally, regardless of the uncontrollable qualities.\nThereby, we take a step further to consider the\ncontrasts among the augmented positives to ﬁgure\nout which augmentation is relatively reasonable.\nThis is achieved by our novel cooperative learning\nmethod with peer networks. More related to our\nwork, ESimCSE (Wu et al., 2021b) found learning\non dual-dropout causes sentence length bias so it\nemploys another augmentation strategy, i.e., word\nrepetition, to prevent the length bias. However,\nword repetition introduces learning shortcut by\nitself, not to mention it makes the sentence unnatu-\nral and even semantics-wrong. To circumvent this\ndilemma, we propose exhaustive augmentations\nto ensure “at-least-one” true positive and reduce\nlearning shortcuts by complementary augmenting\nstrategies. By doing so, PCL achieve a better per-\nformance on STS tasks. Please refer to § D.1 for\nmore discussion details.\nContrastive Learning.\nThe main idea of CL is\nto pull semantic close neighbors close and push\nnon-neighbors apart (Hadsell et al., 2006; Zbontar\net al., 2021). It is shown to be a successful way to\nlearn representation. Approaches in computer vi-\nsion (CV) (Chen et al., 2017; Wu et al., 2018; Tian\net al., 2020; He et al., 2020; Zbontar et al., 2021) try\nto make an image to be invariant to transformations\non itself, while remaining discriminative to other\nimages. More references in CV are discussed in\nthe recent survey (Jaiswal et al., 2021). CL is also\ncoupled with PLMs to learn sentence embeddings.\nBut, recent works (Xiao et al., 2021) argue that\nlearning invariance to particular transformations\nmay be harmful to the robustness of the model.\nThis also supports our idea of leveraging diverse\naugmentations to improve unsupervised sentence\nembeddings from another angle.\nLearning from Agreement.\nAnother line of\nwork close to ours is learning from agreement, e.g.,\nDecoupling (Malach and Shalev-Shwartz, 2017),\nCo-teaching (Han et al., 2018; Yu et al., 2019),\nand mutual CL Yang et al. (2021). This paradigm\nhas been proven effective in improving model per-\nformance and learning with label noises by prior\nfully-supervised works. As text data is discrete and\ncompositional, qualities of multiple augmentations\ncan be uneven, which may corrupt the generaliza-\ntion of sentence embeddings. Besides widely used\nregularization like dropout (Srivastava et al., 2014)\nand weight decay (Krogh and Hertz, 1991), we con-\nsider learning from agreement paradigm to offer a\nrobust way to learn from our diverse positives.\n5\nConclusion\nIn this paper, we propose a brand-new contrastive\nlearning framework, dubbed as peer-contrastive\nlearning (PCL), to capture rich relations among\ndiverse positive peers and highlight effective pos-\nitives, which are learned by cooperative learning\nby peer networks. Besides inherent anti-bias abil-\nity by diverse augmentations, it can learn from\nunsupervised corpus more effectively than vanilla\ncontrastive in the text ﬁeld. Experiments show that\nthe number and diversity of augmentations are cru-\ncial to PCL. Ablation study also shows that the two\ncomponents of PCL, i.e., peer-positive contrast and\npeer-network cooperation, are both beneﬁcial to\nunsupervised CL for sentence embeddings.\nLimitation.\nWe also recognize that our PCL\nframework has its certain limitations: (i) Due to\npeer positives and encoders, our framework needs\nhigher (~3×, i.e., 2.5 GPU-hours for base mod-\nels) computation overheads compared to vanilla\nCL. Nonetheless, the acceptable extra overhead\nand same inference makes our framework still prac-\ntical and scalable. (ii) As a work addressing the\ngeneral shortcut learning problem in a fundamen-\ntal task, the proposed PCL is only evaluated on\nresources in English. It can be further extend to\nmore applications such as in low-resource or other\nlanguages. (iii) The performance of our framework\nrelies on the choice of augmentation methods, and\nit is hard to strictly claim which combination of\nthe methods is optimal except experimental veri-\nﬁcation. Although we have analysed the effect of\nvarying combinations of augmentations with ex-\ntensive experiments, we can only select several\nwidely-adopted augmentations to evaluate the gen-\neral effectiveness of our framework.\nAcknowledgement\nWe thank Yoshimasa Tsuruoka, Ryokan Ri and\nJing Zhou for valuable discussions.\nQiyu Wu\nwas supported by JST SPRING, Grant Number\nJPMJSP2108.\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, et al. 2015.\nSemEval-2015 task 2: Se-\nmantic textual similarity, english, spanish and pilot\non interpretability. In Proceedings of SemEval.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Rada Mihalcea, German Rigau, and Janyce\nWiebe. 2014. SemEval-2014 task 10: Multilingual\nsemantic textual similarity.\nIn Proceedings of Se-\nmEval.\nEneko Agirre, Carmen Banea, Daniel Cer, Mona\nDiab, Aitor Gonzalez Agirre, Rada Mihalcea, Ger-\nman Rigau Claramunt, and Janyce Wiebe. 2016.\nSemEval-2016 task 1: Semantic textual similarity,\nmonolingual and cross-lingual evaluation. In Pro-\nceedings of SemEval.\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor\nGonzalez-Agirre. 2012.\nSemEval-2012 task 6: A\npilot on semantic textual similarity. In Proceedings\nof SemEval.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. * SEM 2013 shared\ntask: Semantic textual similarity. In Proceedings of\nSemEval.\nFredrik Carlsson, Amaru Cuba Gyllensten, Evan-\ngelia Gogoulou, Erik Ylipää Hellqvist, and Magnus\nSahlgren. 2021. Semantic re-tuning with contrastive\ntension. In ICLR.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemEval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation.\narXiv preprint\narXiv:1708.00055.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In In-\nternational conference on machine learning, pages\n1597–1607. PMLR.\nTing Chen, Yizhou Sun, Yue Shi, and Liangjie Hong.\n2017.\nOn sampling strategies for neural network-\nbased collaborative ﬁltering. In Proceedings of the\n23rd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pages 767–\n776.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An\nevaluation toolkit for universal sentence representa-\ntions. arXiv preprint arXiv:1803.05449.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. ArXiv, abs/1810.04805.\nMengnan Du, Varun Manjunatha, R. Jain, Ruchi Desh-\npande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun,\nand Xia Hu. 2021. Towards interpreting and miti-\ngating shortcut learning behavior of nlu models. In\nNAACL.\nPhilipp Dufter and Hinrich Schütze. 2020.\nIdentify-\ning elements essential for bert’s multilinguality. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4423–4437.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence\nembeddings. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nJohn Giorgi, Osvald Nitski, Gary D Bader, and\nBo Wang. 2021.\nDeclutr: Deep contrastive learn-\ning for unsupervised textual representations. ArXiv,\nabs/2006.03659.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. 2006.\nDimensionality reduction by learning an invariant\nmapping. In 2006 IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR’06), volume 2, pages 1735–1742. IEEE.\nBo Han, Quanming Yao, Xingrui Yu, Gang Niu,\nMiao Xu, Weihua Hu, Ivor W Tsang, and Masashi\nSugiyama. 2018. Co-teaching: Robust training of\ndeep neural networks with extremely noisy labels.\nIn NeurIPS.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss B. Girshick. 2020.\nMomentum contrast for\nunsupervised visual representation learning. 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 9726–9735.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1367–1377.\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras,\nLogan Engstrom, Brandon Tran, and Aleksander\nMadry. 2019.\nAdversarial examples are not bugs,\nthey are features. ArXiv, abs/1905.02175.\nAshish Jaiswal,\nAshwin Ramesh Babu,\nMoham-\nmad Zaki Zadeh, Debapriya Banerjee, and Fillia\nMakedon. 2021.\nA survey on contrastive self-\nsupervised learning. Technologies, 9(1):2.\nTaeuk Kim, Kang Min Yoo, and Sang goo Lee. 2021.\nSelf-guided contrastive learning for bert sentence\nrepresentations. ArXiv, abs/2106.07345.\nRyan Kiros,\nYukun Zhu,\nRuss R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in Neural Information Processing Systems,\nvolume 28. Curran Associates, Inc.\nAnders Krogh and John A Hertz. 1991.\nA simple\nweight decay can improve generalization. In NIPS.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nEMNLP.\nFangyu Liu, Yunlong Jiao, Jordan Massiah, Em-\nine Yilmaz, and Serhii Havrylov. 2021a.\nTrans-\nencoder:\nUnsupervised sentence-pair modelling\nthrough self-and mutual-distillations. arXiv preprint\narXiv:2109.13059.\nFangyu Liu, Ivan Vuli´c, Anna Korhonen, and Nigel\nCollier. 2021b. Fast, effective, and self-supervised:\nTransforming masked language models into univer-\nsal lexical and sentence encoders. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1442–1459.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nLajanugen Logeswaran and Honglak Lee. 2018. An ef-\nﬁcient framework for learning sentence representa-\ntions. In International Conference on Learning Rep-\nresentations.\nEran Malach and Shai Shalev-Shwartz. 2017. Decou-\npling\" when to update\" from\" how to update\". In\nProceedings of the 31st International Conference on\nNeural Information Processing Systems, pages 961–\n971.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, Roberto Zamparelli,\net al. 2014. A SICK cure for the evaluation of com-\npositional distributional semantic models.\nIn Pro-\nceedings of LREC.\nTomas Mikolov, Kai Chen, Gregory S. Corrado, and\nJeffrey Dean. 2013. Efﬁcient estimation of word rep-\nresentations in vector space. In ICLR.\nNiklas Muennighoff. 2022.\nSgpt:\nGpt sentence\nembeddings for semantic search.\narXiv preprint\narXiv:2202.08904.\nJianmo Ni, Gustavo Hernández Ábrego, Noah Con-\nstant, Ji Ma, Keith B Hall, Daniel Cer, and Yin-\nfei Yang. 2021. Sentence-t5: Scalable sentence en-\ncoders from pre-trained text-to-text models. arXiv\npreprint arXiv:2108.08877.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nbert:\nSentence embeddings using siamese bert-\nnetworks. In EMNLP-IJCNLP, pages 3982–3992.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nJianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.\n2021a. Whitening sentence representations for bet-\nter semantics and faster retrieval.\narXiv preprint\narXiv:2103.15316.\nPeng Su, Yifan Peng, and K. Vijay-Shanker. 2021b.\nImproving BERT model using contrastive learning\nfor biomedical relation extraction. In Proceedings\nof the 20th Workshop on Biomedical Language Pro-\ncessing, pages 1–10.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. 2020.\nContrastive multiview coding. In ECCV.\nKexin Wang, Nils Reimers, and Iryna Gurevych. 2021.\nTsdae: Using transformer-based sequential denois-\ning auto-encoder for unsupervised sentence embed-\nding learning. In EMNLP, pages 671–688.\nHongxin Wei, Lei Feng, Xiangyu Chen, and Bo An.\n2020.\nCombating noisy labels by agreement: A\njoint training method with co-regularization. 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 13723–13732.\nJason Wei and Kai Zou. 2019. Eda: Easy data augmen-\ntation techniques for boosting performance on text\nclassiﬁcation tasks. ArXiv, abs/1901.11196.\nQiyu Wu, Chen Xing, Yatao Li, Guolin Ke, Di He, and\nTie-Yan Liu. 2021a. Taking notes on the ﬂy helps\nlanguage pre-training. In International Conference\non Learning Representations.\nXing Wu, Chaochen Gao, Liangjun Zang, Jizhong Han,\nZhongyuan Wang, and Songlin Hu. 2021b.\nEs-\nimcse: Enhanced sample building method for con-\ntrastive learning of unsupervised sentence embed-\nding. ArXiv, abs/2109.04380.\nZhirong Wu, Yuanjun Xiong, Stella X. Yu, and\nDahua Lin. 2018.\nUnsupervised feature learning\nvia non-parametric instance discrimination.\n2018\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 3733–3742.\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian\nKhabsa, Fei Sun, and Hao Ma. 2020. Clear: Con-\ntrastive learning for sentence representation. arXiv\npreprint arXiv:2012.15466.\nTete Xiao, Xiaolong Wang, Alexei A. Efros, and Trevor\nDarrell. 2021.\nWhat should not be contrastive in\ncontrastive learning. ArXiv, abs/2008.05659.\nQizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang\nLuong, and Quoc V. Le. 2020.\nUnsupervised\ndata augmentation for consistency training. arXiv:\nLearning.\nYutao Xie, Qiyu Wu, Wei Chen, and Tengjiao Wang.\n2022. Stable contrastive learning for self-supervised\nsentence embeddings with pseudo-siamese mutual\nlearning.\nIEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 30:3046–3059.\nYuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng\nZhang, Wei Wu, and Weiran Xu. 2021.\nCon-\nsert: A contrastive framework for self-supervised\nsentence representation transfer.\narXiv preprint\narXiv:2105.11741.\nChuanguang Yang, Zhulin An, Linhang Cai, and\nYongjun Xu. 2021.\nMutual contrastive learn-\ning for visual representation learning.\nArXiv,\nabs/2104.12565.\nXingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor\nTsang, and Masashi Sugiyama. 2019.\nHow does\ndisagreement help generalization against label cor-\nruption?\nIn International Conference on Machine\nLearning, pages 7164–7173. PMLR.\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun,\nand Stephane Deny. 2021.\nBarlow twins:\nSelf-\nsupervised learning via redundancy reduction.\nIn\nProceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 12310–12320.\nPMLR.\nYan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim,\nand Lidong Bing. 2020. An unsupervised sentence\nembedding method by mutual information maxi-\nmization. In EMNLP, pages 1601–1610.\nYing Zhang, Tao Xiang, Timothy M Hospedales, and\nHuchuan Lu. 2018. Deep mutual learning. In Pro-\nceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, pages 4320–4328.\nA\nAugmentation Strategies\nWe propose diverse augmentation strategies for\neach sentence. In this paper we utilize ﬁve unsuper-\nvised augmentation strategies that are commonly\nadopted in previous works (Wei and Zou, 2019;\nYan et al., 2021; Gao et al., 2021). Augmentations\nfrom discrete perspectives ∆(d) includes: 1) Shuf-\nﬂed Sentence (SS) shufﬂes the position of words in\nthe sentence. SS corrupts the order of the original\nsentence but preserves the semantic information of\nwords; 2) Inverted Sentence (IS) inverts the origi-\nnal sentence as the augmented sample. Apart from\nthe reading order, IS preserves all language prop-\nerties even including n-gram statistics (Dufter and\nSchütze, 2020); 3) Words Repetition (WR) dupli-\ncates part of words and randomly insert them into\nthe original sentences; 4) Words Deletion (WD)\ndeletes part of words in the sentences. WD and\nWR change the length and words of the original\nsentence but roughly preserve the reading order.\nThe deletion and repetition ratio are empirically set\nto 0.2. The augmentations from the continuous per-\nspective ∆(c) include Dropout (DP). It generates\naugmentation instances in the embedding level by\npassing the original sentence again into the encoder\nwith different dropout masks. The above ﬁve strate-\ngies can be repeatedly applied in practice. As there\nis randomness in the processes of augmentation and\nencoding, repeatedly generated instances with the\nsame strategy can be regarded as diverse positives.\nBut the diversity may accordingly decline. Note\nthat the primary goal of this paper is to verify the\neffectiveness of our PCL framework, hence all of\nthe chosen augmentations are common and simple.\nWe speculate that our PCL can be further improved\nwith more ﬁne-tuned augmentation strategies.\nB\nBaselines\nWe compare PCL with previous state-of-the-art un-\nsupervised sentence embedding approaches. Basic\napproaches include taking the average of GloVe,\nBERT, or RoBERTa embeddings. Besides, BERT-\nwhitening and BERT-ﬂow post-process the em-\nbeddings distribution of BERT. We also compare\nPCL with recent approaches using contrastive learn-\ning, including IS-BERT, ConSERT, SG-OPT, Con-\ntrastive Tension, DeCLUTR, and SimCSE. The\nfollowing are the details of these baselines,\n• GloVe (Pennington et al., 2014) maps words\ninto a meaningful space where the distance be-\ntween words is related to semantic similarity.\nThe results of the average of GloVe embed-\ndings are from Reimers and Gurevych (2019).\n• Su et al. (2021a) takes the average of the ﬁrst\nand last layers of BERT (Devlin et al., 2019)\nor RoBERTa (Liu et al., 2019) embeddings.\nWe report the results from Gao et al. (2021).\n• BERT-whitening (Su et al., 2021a) and BERT-\nﬂow (Li et al., 2020) post-process the embed-\ndings distribution of BERT. We report the re-\nsults from Gao et al. (2021) for a fair compar-\nison.\n• IS-BERT (Zhang et al., 2020) encourages the\nrepresentation of a speciﬁc sentence to encode\nall aspects of its local context information, us-\ning local contexts derived from other input\nsentences as negative examples for contrastive\nlearning. We report the results from the origi-\nnal paper.\n• ConSERT (Yan et al., 2021) contrasts a pair\nof sentences augmented by different augmen-\ntation methods. We report the results in the\noriginal paper.\n• SG-OPT (Kim et al., 2021) is a contrastive\nlearning method using self-guidance. The re-\nsults are from the original paper.\n• Contrastive Tension (CT) (Carlsson et al.,\n2021) propose a training objective that aligns\nthe embeddings of the same sentence encoded\nby two different encoders. We report the re-\nsults from Gao et al. (2021).\n• DeCLUTR (Giorgi et al., 2021) is a con-\ntrastive approach that takes different spans\nfrom the same document as contrastive pairs.\nThe results are from Gao et al. (2021)\n• Mirror-BERT (Liu et al., 2021b) employs sev-\neral fast augmentation strategies for effective\nrepresentations. The results are from the orig-\ninal paper.\n• SimCSE (Gao et al., 2021) contrasts a pair of\nembeddings of one sentence encoded with dif-\nferent dropout masks. The results are from the\noriginal paper. We had re-run SimCSE with\nsame setups and it performs worse than the\nnumbers reported in the original paper (e.g.,\n75.36 averaged over 5 seeds on BERTbase).\nModel\nMR\nCR\nSUBJ\nMPQA\nSST\nTREC\nMRPC\nAvg.\nModels w/o PLMs\nGloVe embeddings (avg.)\n77.25\n78.30\n91.17\n87.85\n80.18\n83.00\n72.87\n81.52\nSkip-thought\n76.50\n80.10\n93.60\n87.10\n82.00\n92.20\n73.00\n83.50\nBase Models\nAvg. BERT embeddings\n78.66\n86.25\n94.37\n88.66\n84.40\n92.80\n69.54\n84.94\nBERT-[CLS] embedding\n78.58\n84.85\n94.21\n88.23\n84.13\n91.40\n71.13\n84.66\nIS-BERTbase\n81.09\n87.18\n94.96\n88.75\n85.96\n88.64\n74.24\n85.83\nSimCSE-BERTbase\n81.18\n86.46\n94.45\n88.88\n85.50\n89.80\n74.43\n85.81\nSimCSE-RoBERTabase\n81.04\n87.74\n93.28\n86.94\n86.60\n84.60\n73.68\n84.84\nOurs-BERTbase\n80.11\n85.25\n94.22\n89.15\n85.12\n87.40\n76.12\n85.34\nOurs-RoBERTabase\n81.83\n87.55\n92.92\n87.21\n87.26\n85.20\n76.46\n85.49\nLarge Models\nSimCSE-RoBERTalarge\n82.74\n87.87\n93.66\n88.22\n88.58\n92.00\n69.68\n86.11\nOurs-BERTlarge\n82.47\n87.87\n95.04\n89.59\n87.75\n93.00\n76.00\n87.39\nOurs-RoBERTalarge\n84.47\n89.06\n94.60\n89.26\n89.02\n94.20\n74.96\n87.94\nTable 4: Transfer task results (measured as accuracy).\nWe reported higher numbers for a fair compar-\nison.\nDue to the surge of this topic, many concurrent\nworks emerge with two trends: new model structure\nand more in-domain data. SGPT (Muennighoff,\n2022) and Sentence-T5 (Ni et al., 2021) are pro-\nposed with new paradigm and far larger models,\nwhich underperform with comparable model size.\nTrans-Encoder (Liu et al., 2021a) proposes a coop-\nerative method with in-domain pairwise data for\nmutual beneﬁts of bi- and cross-encoder, making\nthe results incomparable.\nC\nAdditional Experimental results\nC.1\nComparison of controlled setups\nWe can also ﬁnd some variants of SimCSE in our\ncontrolled experiments.\nFor example, PCLnoP\nin Table 3 is regarded as SimCSE w/ multi-\naugmentations. PCLK=1 in Figure 2 can be re-\ngarded as SimCSE w/ peer-network cooperation,\nand PCLDP in Figure 3 is regarded as SimCSE\nw/ 9 dropout augmented positive samples. As we\ndiscussed in the § 3.5 and § 3.6, the comparison\nbetween the variants of SimCSE and PCL show\nthe advantages and importance of our proposed\npeer-contrast and peer-cooperation.\nC.2\nTransfer tasks\nWe also evaluate PCL on 7 transfer tasks (Con-\nneau and Kiela, 2018).\nAs the Table 4 shows,\nPCL achieves competitive performance compared\nwith baselines. Note that as mentioned in previous\nworks (Reimers and Gurevych, 2019; Gao et al.,\n2021), the main goal of sentence embeddings is to\ncluster semantically similar sentences. Hence we\nonly take STS as the main results in this paper.\nC.3\nDetailed experimental results on analysis\nof diverse augmentations\nIn this section, we present detailed results of exper-\niments of diverse augmentations on all 7 STS tasks.\nWe test the performance of PCL with varying K\nand diversity. Experimental results are shown in\nTable 5. As the results and our analysis in § 3.5\nshow, the performance of PCL maintains an up-\nward trend with increasing K. Besides, it is also\nshown that PCL signiﬁcantly outperforms its mono-\naugmenting variants, even keeping the K of them\nconstant, which indicates a better generalization.\nAs a result, PCL with more diverse augmentations\nperforms better. We also speculate that our PCL\ncan be further improved with larger K and more\nﬁne-tuned augmentation strategies.\nD\nDiscussion\nD.1\nDistinction between PCL and other\ncontemporary methods.\nUnsupervised sentence embedding w/ multi-\nple positive augmentations.\nIt’s straightfor-\nward to extend mono-augmentation into multi-\naugmentation to learn expressive representations.\nFor example, CLEAR (Wu et al., 2020) uses var-\nious token/span manipulations for noise-invariant\nrepresentations while Mirror-BERT (Liu et al.,\nModel\nSTS12\nSTS13\nSTS14\nSTS15\nSTS16\nSTSb\nSICK-R\nAvg.\nEffect of the number of augmentations\nPCLK=1\n72.58\n80.62\n74.15\n82.31\n79.23\n78.47\n72.06\n77.06\nPCLK=3\n72.66\n82.96\n74.44\n81.94\n78.38\n77.93\n71.55\n77.12\nPCLK=5\n73.44\n81.80\n74.59\n82.63\n79.40\n79.05\n72.25\n77.59\nPCLK=7\n73.49\n81.93\n74.84\n82.24\n79.75\n79.37\n72.62\n77.75\nPCLK=9\n72.74\n83.36\n76.05\n83.07\n79.26\n79.72\n72.75\n78.14\nEffect of the diversity of augmentations\nPCLDP\n71.20\n82.53\n74.66\n82.67\n78.92\n78.06\n71.94\n77.14\nPCLSS\n70.60\n80.73\n74.11\n82.18\n78.90\n77.91\n69.69\n76.30\nPCLIS\n70.95\n81.31\n74.51\n82.24\n79.23\n78.44\n72.09\n76.97\nPCLWR\n71.82\n82.56\n74.75\n82.34\n78.85\n78.72\n71.88\n77.27\nPCLWD\n73.08\n81.84\n74.17\n82.50\n78.81\n78.52\n71.23\n77.17\nPCL\n72.74\n83.36\n76.05\n83.07\n79.26\n79.72\n72.75\n78.14\nTable 5: Effect of the number and diversity of augmentations. We report the Spearman’s correlation ρ (%) on 7\nSTS datasets. All variants are run for ﬁve times with different random seeds and the average of these ﬁve results\non each column is reported as the ﬁnal number.\n2021b) employs several fast augmentation strate-\ngies for effective representations. However, these\nmethods usually take the augmented positives\nequally, regardless of the uncontrolable qualities.\nFor example, given “Two men are wrestling on the\nﬂoor”, we get “Two men are squirming on the ﬂoor”\nand “Two persons are wrestling on the ﬂoor” by\nword replacement, but only the 2nd is reasonable.\nThereby, we take a step further to consider the\ncontrasts among the augmented positives to ﬁgure\nout which augmentation is relatively reasonable.\nThis is achieved by our novel cooperative learning\nmethod with peer networks.\nUnsupervised sentence embedding for anti-bias.\nMore related to our work, ESimCSE (Wu et al.,\n2021b) found learning on dual-dropout causes sen-\ntence length bias so it employs another augmenta-\ntion strategy, i.e., word repetition, to prevent the\nlength bias. However, word repetition introduces\nlearning shortcut (i.e., order and BoW as in Table\n1) by itself, not to mention it makes the sentence\nunnatural and even semantics-wrong (e.g., repeat-\ning “no”). To circumvent this dilemma, we propose\nexhaustive augmentations to ensure “at-least-one”\ntrue positive and reduce learning shortcuts by com-\nplementary augmenting strategies (See Figure 1\nand 3: if we employ every strategy, the shortcuts\ncan be blocked). Nonetheless, PCL still has a bet-\nter performance (78.42 vs. 78.27) compared with\nESimCSE on STS tasks.\nCooperative learning has more parameters, is it\nthe reason leading better performance?\nOne\nof the advantages of our cooperative learning is\nto effectively learn from the positives with uncon-\ntrolled qualities, or it can be also interpreted as\n‘noisy labels’. In the noisy circumstance, more pa-\nrameters not necessarily lead to better performance.\nAnd it can be anticipated that it possibly leads to\nworse performance because the over ﬁtting in the\nnoisy positives.\nD.2\nEfﬁciency & Impact of augmentations.\nEfﬁciency\nDue to peer positives and encoders,\nour framework needs higher (~3×, i.e., 2.5\nGPU-hours for base models) computation over-\nheads compared to vanilla CL (Gao et al., 2021).\nNonetheless, the same inference makes our frame-\nwork still practical and scalable. Since PCL con-\ntains more loss items, we do not see any signiﬁcant\ndifference in convergence time.\nImpact of augmentations\nThe performance of\nour framework relies on the augmentation methods,\nand it is hard to claim which combination of the\nmethods is optimal except experimental veriﬁca-\ntion. In this work, we only intuitively select several\nmethods without extensive trials. We have illus-\ntrated the performance of mono-augmentation in\nFigure 3.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-01-28",
  "updated": "2022-10-19"
}