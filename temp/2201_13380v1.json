{
  "id": "http://arxiv.org/abs/2201.13380v1",
  "title": "Deep Learning Macroeconomics",
  "authors": [
    "Rafael R. S. Guimaraes"
  ],
  "abstract": "Limited datasets and complex nonlinear relationships are among the challenges\nthat may emerge when applying econometrics to macroeconomic problems. This\nresearch proposes deep learning as an approach to transfer learning in the\nformer case and to map relationships between variables in the latter case.\nAlthough macroeconomists already apply transfer learning when assuming a given\na priori distribution in a Bayesian context, estimating a structural VAR with\nsignal restriction and calibrating parameters based on results observed in\nother models, to name a few examples, advance in a more systematic transfer\nlearning strategy in applied macroeconomics is the innovation we are\nintroducing. We explore the proposed strategy empirically, showing that data\nfrom different but related domains, a type of transfer learning, helps identify\nthe business cycle phases when there is no business cycle dating committee and\nto quick estimate a economic-based output gap. Next, since deep learning\nmethods are a way of learning representations, those that are formed by the\ncomposition of multiple non-linear transformations, to yield more abstract\nrepresentations, we apply deep learning for mapping low-frequency from\nhigh-frequency variables. The results obtained show the suitability of deep\nlearning models applied to macroeconomic problems. First, models learned to\nclassify United States business cycles correctly. Then, applying transfer\nlearning, they were able to identify the business cycles of out-of-sample\nBrazilian and European data. Along the same lines, the models learned to\nestimate the output gap based on the U.S. data and obtained good performance\nwhen faced with Brazilian data. Additionally, deep learning proved adequate for\nmapping low-frequency variables from high-frequency data to interpolate,\ndistribute, and extrapolate time series by related series.",
  "text": "UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL \nFACULDADE DE CIÊNCIAS ECONÔMICAS \nPROGRAMA DE PÓS-GRADUAÇÃO EM ECONOMIA \n \n \n \n \n \nRAFAEL ROCKENBACH DA SILVA GUIMARÃES \n \n \n \n \n \nDEEP LEARNING MACROECONOMICS \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nPorto Alegre \n2021 \n \n \n \n2 \n \nRAFAEL ROCKENBACH DA SILVA GUIMARÃES \n \n \n \n \n \nDEEP LEARNING MACROECONOMICS \n \n \n \n \n \n \nThesis \npresented \nin \npartial \nfulfillment \nof \nthe \nrequirements for the degree of Doctor in Economics. \n \nSupervisor: Prof. Dr. Sergio Marley Modesto Monteiro \n \n \n \n \n \n \n \n \n \n \n \n \nPorto Alegre \n2021 \n \n \n \n \n \n3 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCIP - Catalogação na Publicação \n \n \n \n \nGuimarães, Rafael \nDeep Learning Macroeconomics / Rafael Guimarães. -\n- 2021. \n92 f. \nOrientador: Sérgio Monteiro. \n \n \nTese (Doutorado) -- Universidade Federal do Rio \nGrande do Sul, Faculdade de Ciências Econômicas, \nPrograma de Pós-Graduação em Economia, Porto \nAlegre, BR-RS, 2021. \n \n1. Business cycle. 2. Output gap. 3. Transfer \nlearning. 4. Deep learning. 5. Frequency conversion. \n \nI. Monteiro, Sérgio, orient.  II. Título. \n \n \n \n \n \n \nElaborada pelo Sistema de Geração Automática de Ficha Catalográfica da UFRGS com os \ndados fornecidos pelo(a) autor(a). \n \n \n \n \n \n \n4 \n \nRAFAEL ROCKENBACH DA SILVA GUIMARÃES \n \n \nDEEP LEARNING MACROECONOMICS \n \n \nTese submetida ao Programa de Pós-Graduação em \nEconomia da Faculdade de Ciências Econômicas da \nUFRGS, como requisito parcial para obtenção do título \nde Doutor em Economia. \n \n \nAprovada em: Porto Alegre, 8 de novembro de 2021. \n \nBANCA EXAMINADORA: \n \n____________________________________________________________________________ \nProf. Dr. Sergio Marley Modesto Monteiro – Orientador  \nUniversidade Federal do Rio Grande do Sul – UFRGS  \n \n____________________________________________________________________________ \nProf. Dr. Alessandro Donadio Miebach \nUniversidade Federal do Rio Grande do Sul - UFRGS \n \n____________________________________________________________________________ \nProf. Dr. Daniel Oliveira Cajueiro \nUniversidade de Brasília - UnB \n \n____________________________________________________________________________ \nProf. Dr. Wagner Piazza Gaglianone \nBanco Central do Brasil \n \n \n \n5 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThis thesis work is dedicated to Fernanda, Rafaela, and Felipe Preto Guimarães. \n \n \n \n6 \n \nACKNOWLEDGEMENTS \n \n \n \nThe view I have on the acknowledgments is those scenes at awards, like in The Oscars, where the \nwinner has little time to talk and a massive list of people to thank. Time runs out, sound cuts out, \nbut there are still people on the list. In this journey in search of a doctorate, countless people \nhelped me. I will name a few that have had a more direct influence. \nFirstly, my wife Fernanda and my children Rafaela and Felipe. They had to live with the \nday-to-day life of a doctoral student. It wasn't easy for them at times, but their support has always \nbeen unconditional. As I work at the Central Bank of Brazil, I have access to co-workers whose \nqualifications resulted in excellent suggestions and comments. Special thanks to Oswaldo \nBaumgarten Filho, friend and mentor for the deep learning topic, and Vera Maria Schneider, \nfriend and person who identified my potential to work in the Economics Department. Thanks to \nMarcelo Antonio Thomaz de Aragão and Wagner Piazza Gaglianone, who had reviewed my \nwork in internal seminars, offering excellent suggestions. I would also like to thank UniBC, \nwhich performs extraordinary work towards the qualification of the Central Bank of Brazil staff. \nThanks to Professor Marcelle Chauvet for accepting my proposal to work together and \nwelcoming me to the University of California, Riverside. Being in a foreign country and \nattending classes with people of different nationalities has expanded my skills. The panels' \ncomposition that evaluates the project and the thesis are crucial. Thanks to professors Alessandro \nDonadio Miebach, Daniel Oliveira Cajueiro, Flávio Augusto Ziegelmann, Nelson Seixas dos \nSantos and Wagner Piazza Gaglianone for their invaluable contributions. Finally, thanks to my \nadvisor Professor Sérgio Marley Modesto Monteiro. We worked together for the first time during \nmy Master's, and since then, I have been confident that he would be my first choice as a thesis \nadvisor. As a result, I received the confidence and autonomy necessary to carry out this work. \nFurthermore, his guidance was instrumental in transforming my ideas into a doctoral thesis. \n \n \n \n7 \n \nRESUMO \n \n \nConjuntos de dados limitados e complexas relações não-lineares estão entre os desafios que \npodem surgir ao se aplicar econometria a problemas macroeconômicos. Esta pesquisa propõe a \naprendizagem profunda como uma abordagem para transferir aprendizagem no primeiro caso e \npara mapear relações entre variáveis no último caso. Várias técnicas de aprendizado de máquina \nestão incorporadas à estrutura econométrica, mas o aprendizado profundo continua focado na \nprevisão de séries temporais. A abordagem proposta aqui também está relacionada ao \nreconhecimento de padrões, mas onde o aprendizado profundo alcançou desempenho de ponta: \nprogressivamente utilizando várias camadas de processamento para extrair dos dados brutos \nabstrações de mais alto nível. \n \nPrimeiramente, a aprendizagem por transferência é proposta como uma estratégia \nadicional para a macroeconomia empírica. Embora os macroeconomistas já apliquem a \naprendizagem por transferência ao assumir uma dada distribuição a priori em um contexto \nBayesiano, estimar um VAR estrutural com restrição de sinal e calibrar parâmetros com base em \nresultados observados em outros modelos, para citar alguns exemplos, avançar em uma estratégia \nmais sistemática de transferência de aprendizagem em macroeconomia aplicada é a inovação que \nestamos introduzindo. Ao desenvolver estratégias de modelagem econômica, a falta de dados \npode ser um problema que a aprendizagem por transferência pode corrigir. Começamos por \napresentar conceitos teóricos relacionados à transferência de aprendizagem e propomos uma \nconexão com uma tipologia relacionada a modelos macroeconômicos. Em seguida, exploramos a \nestratégia proposta empiricamente, mostrando que os dados de domínios diferentes, mas \nrelacionados, um tipo de aprendizagem por transferência, ajudam a identificar as fases do ciclo de \nnegócios quando não há comitê de datação do ciclo de negócios e a estimar rapidamente um hiato \ndo produto de base econômica. Em ambos os casos, a estratégia também ajuda a melhorar o \naprendizado quando os dados são limitados. A abordagem integra a ideia de armazenar \nconhecimento obtido de especialistas em economia de uma região e aplicá-lo a outras áreas \ngeográficas. O primeiro é capturado com um modelo de rede neural profunda supervisionado e o \nsegundo aplicando-o a outro conjunto de dados, um procedimento de adaptação de domínio. No \ngeral, há uma melhora na classificação com a aprendizagem por transferência em comparação \n \n \n \n8 \n \ncom os modelos de base. Até onde sabemos, a abordagem combinada de aprendizagem profunda \ne transferência é subutilizada para aplicação a problemas macroeconômicos, indicando que há \nmuito espaço para o desenvolvimento de pesquisas. \n \nEm segundo lugar, uma vez que os métodos de aprendizagem profunda são uma forma \nde aprender representações, aquelas que são formadas pela composição de várias transformações \nnão lineares, para produzir representações mais abstratas, aplicamos a aprendizagem profunda \npara mapear variáveis de baixa frequência a partir de variáveis de alta frequência. Existem \nsituações em que sabemos, às vezes por construção, que existe uma relação entre as variáveis de \nentrada e saída, mas essa relação é difícil de mapear, um desafio no qual os modelos de \naprendizagem profunda têm apresentado excelente desempenho. \n \nOs resultados obtidos mostram a adequação de modelos de aprendizagem profunda \naplicados a problemas macroeconômicos. Primeiro, os modelos aprenderam a classificar os ciclos \nde negócios dos Estados Unidos corretamente. Em seguida, aplicando o aprendizado de \ntransferência, eles foram obtiveram sucesso na identificação dos ciclos de negócios de dados \nbrasileiros e europeus fora da amostra. Na mesma linha, os modelos aprenderam a estimar o hiato \ndo produto com base nos dados americanos e obtiveram bom desempenho frente aos dados \nbrasileiros. Em ambos os casos, a estratégia proposta surge como uma ferramenta suplementar \npotencial para os governos e o setor privado conduzirem suas atividades à luz das condições \neconômicas nacionais e internacionais. Além disso, o aprendizado profundo se mostrou adequado \npara mapear variáveis de baixa frequência a partir de dados de alta frequência para interpolar, \ndistribuir e extrapolar séries temporais por séries relacionadas. A aplicação dessa técnica em \ndados brasileiros mostrou-se compatível com benchmarks baseados em outras técnicas. \n \nPalavras-chave: Ciclo de negócios. Hiato do produto. Transferência de aprendizagem. \nAprendizagem profunda. Conversão de frequência. \n \n \n \n9 \n \nABSTRACT \n \n \nLimited datasets and complex nonlinear relationships are among the challenges that may emerge \nwhen applying econometrics to macroeconomic problems. This research proposes deep learning \nas an approach to transfer learning in the former case and to map relationships between variables \nin the latter case. Several machine learning techniques are incorporated into the econometric \nframework, but deep learning remains focused on time-series forecasting. The approach proposed \nhere is also related to pattern recognition, but where deep learning has achieved state-of-the-art \nperformance: progressively using multiple layers to extract higher-level features from the raw \ninput. \n \nFirstly, transfer learning is proposed as an additional strategy for empirical \nmacroeconomics. Although macroeconomists already apply transfer learning when assuming a \ngiven a priori distribution in a Bayesian context, estimating a structural VAR with signal \nrestriction and calibrating parameters based on results observed in other models, to name a few \nexamples, advance in a more systematic transfer learning strategy in applied macroeconomics is \nthe innovation we are introducing. When developing economics modeling strategies, the lack of \ndata may be an issue that transfer learning can fix. We start presenting theoretical concepts \nrelated to transfer learning and proposed a connection with a typology related to macroeconomic \nmodels. Next, we explore the proposed strategy empirically, showing that data from different but \nrelated domains, a type of transfer learning, helps identify the business cycle phases when there is \nno business cycle dating committee and to quick estimate an economic-based output gap. In both \ncases, the strategy also helps to improve the learning when data is limited. The approach \nintegrates the idea of storing knowledge gained from one region’s economic experts and applying \nit to other geographic areas. The first is captured with a supervised deep neural network model, \nand the second by applying it to another dataset, a domain adaptation procedure. Overall, there is \nan improvement in the classification with transfer learning compared to baseline models. To the \nbest of our knowledge, the combined deep and transfer learning approach is underused for \napplication to macroeconomic problems, indicating that there is plenty of room for research \ndevelopment. \n \n \n \n \n10 \n \n \nSecondly, since deep learning methods are a way of learning representations, those that \nare formed by the composition of multiple non-linear transformations, to yield more abstract \nrepresentations, we apply deep learning for mapping low-frequency from high-frequency \nvariables. There are situations where we know, sometimes by construction, that there is a \nrelationship be-tween input and output variables, but this relationship is difficult to map, a \nchallenge in which deep learning models have shown excellent performance. \n \nThe results obtained show the suitability of deep learning models applied to \nmacroeconomic problems. First, models learned to classify United States business cycles \ncorrectly. Then, applying transfer learning, they were able to identify the business cycles of out-\nof-sample Brazilian and European data. Along the same lines, the models learned to estimate the \noutput gap based on the U.S. data and obtained good performance when faced with Brazilian \ndata. In both cases, the proposed strategy emerges as a potential supplementary tool for \ngovernments and the private sector to conduct their activities in the light of national and \ninternational economic conditions. Additionally, deep learning proved adequate for mapping low-\nfrequency variables from high-frequency data to interpolate, distribute, and extrapolate time \nseries by related series. The application of this technique to Brazilian data proved to be \ncompatible with benchmarks based on other techniques. \n \nKeywords: Business cycle. Output gap. Transfer learning. Deep learning. Frequency conversion. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n11 \n \nLIST OF FIGURES \n \n \nFigure 1 -  Transfer learning overview - reproduced from Yosinski et al. (2014) . . . . . . . . .  22 \nFigure 2 - Different settings of transfer - reproduced from Pan and Yang (2010)  . . .. . . . . . 26 \nFigure 3 - Transfer Learning settings and Macroeconomic typologies . . . . . . . . . . . . . . . . . 27 \nFigure 4 - Deep Learning (U.S)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  46 \nFigure 5 - Transfer Learning (Euro Area and Brazil)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 \nFigure 6 - Models specifications and results  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  49 \nFigure 7 - AUC out-of-sample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 \nFigure 8 - US Output GAP - Congress Budget Office (CBO) and Deep Learning models . . 55 \nFigure 9 - Models specifications and results  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 \nFigure 10 - Output GAP - Central Bank of Brazil (BCB) and Transfer Learning Model . . . .  56 \nFigure 11 - MAE out-of-sample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 \nFigure 12 - Training and validation curves level . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 \nFigure 13 - RIDE mapping GDP level . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 \nFigure 14 - Training and validation curves YoY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 \nFigure 15 - RIDE mapping GDP YoY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 \nFigure 16 - Chow Lin, IBC-Br and FGV Monitor fitting GDP . . . . . . . . . . . . . . . . . . . . . . . . 70 \nFigure 17 - RIDE, IBC-Br and FGV Monitor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 \nFigure 18 - RIDE extrapolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 \nFigure 19 - U.S. raw data for learning Business Cycle.  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  74 \nFigure 20 - U.S. raw data for learning Output Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 \n \n \n \n12 \n \nLIST OF TABLES \n \n \nTable 1 - Relationship between Traditional Machine Learning and Transfer Learning . . . . 24 \nTable 2 - Out-of-Sample Evaluation Metrics for Alternative Classifiers . . . . . . . . . . . . . . . 29 \nTable 3 - Hyperparameters space  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 \nTable 4 - A comparison between two approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 \nTable 5 - Dataset for Business cycle identification: summary . . . . . . . . . . . . . . . . . . . . . . . . 75 \nTable 6 - Dataset for Business cycle identification: descriptive statistics . . . . . . . . . . . . . . . 76 \nTable 7 - Dataset for Output Gap Estimation: summary  . . . . . . . . . . . . . . . . . . . . . . . . . . .  78 \nTable 8 - Dataset for Output Gap Estimation: descriptive statistics . . . . . . . . . . . . . . . . . . . 78 \nTable 9 - Dataset for RIDE: summary  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 \nTable 10 - Dataset for RIDE: level descriptive statistics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 \nTable 11 - Dataset for RIDE: year over year descriptive statistics . . . . . . . . . . . . . . . . . . . . . 81 \n \n \n \n13 \n \nCONTENTS \n \n \n1 \nINTRODUCTION. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 \n \n2 \nTRANSFER LEARNING AND MACROECONOMICS . . . . . . . . . . . . . . . . .  18 \n2.1   \nMacroeconomic models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 \n \n2.2 \nAn additional strategy for empirical macroeconomics . . . . . . . . . . . . . . . . . . . . 20 \n2.2.1 \nInductive Transfer Learning . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . .  24 \n2.2.2 \nTransductive Transfer Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 \n2.2.3 \nUnsupervised Transfer Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .25 \n2.2.4 \nTransfer Learning settings and Macroeconomic typologies . . . . . . . . . . . . . . . . . .  26 \n3 \nEMPIRICAL SETUP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  28 \n3.1 \nAlgorithms for Transfer Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 \n3.1.1 \nDeep Learning  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 \n3.1.2 \nCost function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  34 \n3.1.3 \nOptimization procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  34 \n3.2 \nPractical Methodology  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  35 \n3.2.1 \nHyperparameters space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 \n3.3 \nFeature representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 \n3.3.1 \nRobustness: baseline and benchmark models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 \n3.3.2 \nReal-time data  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 \n3.3.3 \nEconomics school of thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 \n3.4 \nSmall data, big data, good data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 \n3.4.1 \nRobustness: locked and unlocked models  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 \n3.5 \nCross-sectional data into feed-forward neural network  . . . . . . . . . . . . . . . . . .  40 \n3.5.1 \nRobustness: sequential approach with LSTM  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 \n3.6 \nNormalization, unbalanced, seasonality, stationarity, missing values . . . . . . . . 41 \n4 \nTRANSFER LEARNING FOR BUSINESS CYCLE . . . . . . . . . . . . . . . . . . . . . 42 \n \n \n \n14 \n \n4.1 \nRelated Literature  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 \n4.2 \nImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 \n4.3 \nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . 45 \n \n5 \nTRANSFER LEARNING FOR OUTPUT GAP  . . . . . . . . . . . . . . . . . . . . . . . . . 51 \n5.1 \nRelated Literature  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  51 \n5.2 \nImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 \n5.3 \nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  53 \n \n6 \nREPRESENTATION LEARNING FOR INTERPOLATION, DISTRIBUTION, \nAND EXTRAPOLATION OF TIME SERIES BY RELATED SERIES \n(RIDE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  58 \n6.1 \nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 \n6.2 \nLiterature  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 \n6.2.1 \nInterpolation, Distribution and Extrapolation of Time Series by Related Series . . . . . 59 \n6.2.2 \nArti cial Neural Networks and Economics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 \n6.3 \nMapping GDP  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 \n6.3.1 \nImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 \n6.3.2 \nResults  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  65 \n6.3.2.1 Interpolation and Distribution  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 \n6.3.2.2 Extrapolation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 \n \n7 \nCONCLUSION. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  72 \n \n \nAPPENDIX A – DATA DESCRIPTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  74 \n \n \nAPPENDIX B – SELECTED CODES  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 \n \n \nREFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n \n \n \n15 \n \n1   \nINTRODUCTION \n \n \nEconometrics, the branch of economics concerned with applying statistical and mathematical \nmethods to economic problems, is a relevant and constantly expanding area of research. One of \nthe questions always open is where to use a particular method, especially when presenting \nremarkable performance in other areas of knowledge, as deep learning nowadays. \n \nLimited datasets and complex nonlinear relationships are among the challenges that may \nemerge when applying econometrics to macroeconomic problems. This research proposes deep \nlearning as an approach to transfer learning in the former case and to map relationships between \nvariables in the latter case. Several machine learning techniques are incorporated into the \neconometric framework, but deep learning remains focused on time-series forecasting. The \napproach proposed here is also related to pattern recognition, but where deep learning has \nachieved state-of-the-art performance: progressively using multiple layers to extract higher-level \nfeatures from the raw input. \n \nA major assumption in machine learning (ML) and data mining algorithms is that the \ntraining and future data must be in the same feature space and have the same distribution. \nHowever, in many real-world applications, this assumption may not hold (Pan and Yang, 2010). \nCan we use information from adult humans to train an intelligent system for diagnosing infant \nheart disease? Such a problem is known as transfer learning. The population of interest is called \nthe target domain, for which labels are usually not available and training a classifier might be not \npossible. However, if data from a similar population is available, it could be used as a source of \nadditional information (Kouw and Loog, 2018, 2). For example, we may find that learning to \nrecognize apples might help to recognize pears. Similarly, learning to play the electronic organ \nmay help facilitate learning the piano. The study of transfer learning (TL) is motivated by the fact \nthat people can intelligently apply knowledge learned previously to solve new problems faster or \nwith better solutions (Pan and Yang, 2010, 2). Thus, TL refers to the situation where what has \nbeen learned in one setting (e.g., distribution P1) is exploited to improve generalization in another \nsetting (say, distribution P2). The learner must perform two or more different tasks, but it is \n \n \n \n16 \n \nassumed that many of the factors that explain the variations in P1 are relevant to the variations \nthat need to be captured for learning P2 (Goodfellow et al., 2016, 534). \n \nArtificial intelligence (AI) has recently gained considerable prominence due to \nperformances in autonomous vehicles, intelligent robots, image and speech recognition, \nautomatic translations, and medical and law usage (Makridakis, 2017). In Economics, the \napplication of machine learning methods, an AI technique, is not new, and in a way, it has \nfollowed the phases of use in other areas. This has extended from the earliest attempts in the \n1940s, followed by the rising expectations and the results in the 1960s, through the period of \nfrustration in the 1970s, to the continuity of its use by a small group of researchers in the 1980s, \nand the resurgence in the 1990s (Stergiou and Siganos, 2011). Finally, from the beginning of the \n21st century, significant progress has been observed in many areas, attracting attention and \nresearch funding. \n \nMeanwhile in Macroeconomics, according to Vines and Wills (2018), during the Great \nModeration (mid-1980s to 2007), the New Keynesian Dynamic Stochastic General Equilibrium \n(DSGE) model had become the benchmark model: the one taught to students at the start of the \nfirst-year graduate macro course. But the benchmark model has limitations. What new ideas are \nneeded (Vines and Wills, 2018, 2)? \n \nIn Chapter 2, we present the typology of macroeconomic models introduced by \nBlanchard (2018) and the transfer learning setting proposed by Pan and Yang (2010). If this is \ntrue that the transfer knowledge capacity, or more broadly the representation learning, observed \nin other scientific areas must hold in economics, we face an underutilized tool that can contribute \nto applied macroeconomics. Although macroeconomists already apply transfer learning when \nassuming a given a priori distribution in a Bayesian context, estimating a structural VAR with \nsignal restriction and calibrating parameters based on results observed in other models, to name a \nfew examples, advance in a more systematic transfer learning strategy in applied \nmacroeconomics is the new idea we are introducing. \n \nWe proceed, in Chapter 3, with the empirical setup and a literature review on the \nalgorithms used for transfer learning, and we present in more detail our choice, deep neural \nnetworks, or deep learning, because it has shown excellent performance for transfer learning, but \n \n \n \n17 \n \nalso because it is significantly less black box than it was in the past. Since feature importance is \nrelevant in economics, neural networks usually are not the first choice. However, interpretability \nin AI is an active area of research with significant advances, both by researchers’ determination \nand by the requirements of companies and governments to adopt these models for decision-\nmaking. Partial Dependence Plots, Permutation Feature Importance, Shapley Value, and \nIntegrated Gradients are available methods to compute feature relevance, to name a few. Then, in \nChapters 4 and 5, we advance empirically by exploring transfer learning capability when applied \nto business cycle identification and output gap estimation. \n \nLastly, in Chapter 6 we explore deep learning for mapping the low-frequency gross \ndomestic product from high-frequency variables because there are situations where we know, \nsometimes by construction, that there is a relationship between input and output variables, but \nthis relationship is di cult to map, a challenge in which deep learning models have shown \nexcellent performance, especially towards a data-centric view, where we hold the code fixed and \ninteractively improve the data. \n \n \n \n18 \n \n2    \nTRANSFER LEARNING AND MACROECONOMICS \n \n \n2.1     Macroeconomic models \nMore than forty years ago, when Sims (1980) proposed vector autoregressions (VARs) as an \nalternative strategy for empirical macroeconomics1, he asserted that existing econometric analysis \nstrategies related to macroeconomics were subject to a number of serious objections, some \nrecently formulated, some old. For instance, empirical macroeconomists sometimes express \nfrustration at the limited amount of information in economic time series, and it does not \ninfrequently turn out that models reflecting rather different behavioral hypotheses fit the data \nabout equally well (Sims, 1980, 15). \n \nRecently, Vines and Wills (2018), in the Rebuilding Macroeconomic Theory Project, \nclaims that the need to change macroeconomic theory is similar to the situation in the 1930s, at \nthe time of the Great Depression, and in the 1970s, when in inflationary pressures were \nunsustainable. They asked several leading macroeconomists to describe how the benchmark New \nKeynesian model might be rebuilt in the wake of the 2008 crisis (Vines et al., 2018). \n \nAs one can see, it has been an area in constant evolution, but still with many challenges. \nThe number of macroeconomic models is countless, and we see no restriction to use, or at least to \ntry, the strategy proposed in Section 2.2 to any of them. Still, here we restrict our environment to \nthe typology proposed by Blanchard (2018). According to him (Blanchard, 2018, 43-44), there \nshould be five kinds of general equilibrium models: a common core, plus foundational theory, \npolicy, toy, and forecasting models. The different classes of models have a lot to learn from each \nother, but the goal of full integration has proven counterproductive. We need different \nmacroeconomic models for different purposes. One type is not better than the others. (Blanchard, \n2018, 52-53) explains each type: \n \n \n1 VARs are useful statistical devices for evaluating alternative macroeconomic models. His suggestion has stood \nthe test of time well. In the early days, VARs played an important role in the evaluation of alternative models. \nThey continue to play that role today (Christiano, 2012). \n \n \n \n19 \n \n1. The purpose of the DSGE models is to explore the macro implications of distortions or sets \nof distortions. To allow for a productive discussion, they must be built around a largely \nagreed-upon common core. Each model then explores additional distortions, be it bounded \nrationality, asymmetric information, different forms of heterogeneity, etc. These models \nshould aim to be close to reality, but not through ad hoc additions and repairs, such as \narbitrary and undocumented higher-order costs introduced only to deliver more realistic lag \nstructures. Fitting reality closely should be left to policy models. \n \n2. Foundational models’ purpose is to make a deep theoretical point, likely of relevance to \nnearly any macro model, but not pretending to capture reality closely. Some examples of \nthis type are the consumption-loan model of Paul Samuelson, the overlapping generation \nmodel of Peter Diamond, the models of money by Neil Wallace or Randy Wright, the \nequity premium model of Edward Prescott and Rajnish Mehra, the search models of Peter \nDiamond, Dale Mortensen, and Chris Pissarides. \n \n3. The purpose of policy models is to help policy, study the dynamic effects of specific \nshocks, and explore alternative policies. For these models, capturing actual dynamics is \nclearly essential. So is having enough theoretical structure that the model can be used to \ntrace the effects of shocks and policies. But the theoretical structure must by necessity be \nlooser than for DSGE: aggregation and heterogeneity lead to much more complex \naggregate dynamics than a tight theoretical model can hope to capture. Old-fashioned \npolicy models started from theory as motivation and then let the data speak, equation by \nequation. Some new-fashioned models start from a DSGE structure and then let the data \ndetermine the richer dynamics. In any case, for this class of models, the rules of the game \nhere must be different than for DSGEs. Does the model t well, for example, in the sense of \nbeing consistent with the dynamics of a VAR characterization? Does it capture well the \neffects of past policies? Does it allow us to think about alternative policies? \n \n4. Examples of toy models are the many variations of the IS-LM model, the Mandell-Fleming \nmodel, the RBC model, and the New Keynesian model. As the list indicates, some may be \nonly loosely based on theory, others more explicitly so. But they have the same purpose. \n \n \n \n20 \n \nThey allow for a quick first pass at some questions and present the essence of the answer \nfrom a more complicated model or a class of models. For the researcher, they may come \nbefore writing a more elaborate model or after, once the elaborate model has been worked \nout. How close they remain formally to theory is not a relevant criterion here. They are art \nas much science. But art is of much value. \n \n5. The purpose of forecasting models is straightforward: give the best forecasts. And this is \nthe only criterion by which to judge them. If a theory is useful in improving the forecasts, \nthen the theory should be used. If it is not, it should be ignored. The issues are then \nstatistical, from over-parameterization to how to deal with the instability of the underlying \nrelations. \n \n2.2     An additional strategy for empirical macroeconomics \nWhen developing one or more modeling strategies described in Section 2.1, the lack of data may \nbe an issue that transfer learning helps to fix. As we show empirically in chapters 4 and 5, data \nfrom different but related domains, a type of transfer learning, help improve the task at hand. \n  \nAlthough macroeconomists already apply transfer learning when assuming a given a \npriori distribution in a Bayesian context, estimating a structural VAR with signal restriction and \ncalibrating parameters based on results observed in other models, to name a few examples, \nadvance in a more systematic transfer learning strategy in applied macroeconomics is our \nproposition. Then, for empirical macroeconomics, it should be feasible to apply transfer learning \non data as an additional strategy to either (Pan and Yang, 2010, 5): \n \n• \ninstance-transfer: re-weight some labeled data in the source domain for use in the target \ndomain. \n \n• \nfeature-representation-transfer: find a good feature representation that reduces difference \nbetween the source and the target domains and the error of classification and regression \nmodels. \n \n \n \n \n21 \n \n• \nparameter-transfer: discover shared parameters or priors between the source domain and \ntarget domain models, which can bene t for transfer learning. \n \n• \nrelational-knowledge-transfer: build mapping of relational knowledge between the \nsource and the target domains. \n \n \nSimilar to the characterization of toy models by Blanchard (2018), applying transfer \nlearning to macroeconomics is an art as much science, but art is of much value. The first step in \ndeveloping such an approach is collecting, organizing, and evaluating the task-related available \ndata. As a data-driven method, transfer learning performance relies on the quality of the data. \nNext, the researcher must choose one or more machine learning algorithms that will learn with \nthe data. Pan and Yang (2010) and Weiss et al. (2016) review some options, and in Section 3.1.1, \nwe describe deep learning, our choice. Finally, one evaluates the results, transfers learning, and \ncompares it with the outcomes of other modeling strategies if it is the case. \n \nIn the conventional transfer learning approach, we first train a base network on a base \ndataset and task, and then we repurpose the learned features or transfer them to a second target \nnetwork to be trained on a target dataset and task. This process will tend to work if the features \nare general, meaning suitable to both base and target tasks, instead of specific to the base task \n(Yosinski et al., 2014). Figure 1 illustrates these dynamics2. \n \nThe foregoing discussion is based on a relevant, comprehensive survey about transfer \nlearning by Pan and Yang (2010). Consider that a domain  consist of two components: a feature \nspace \n and a marginal probability distribution \n, where \n. For example, if \n \n2 The base networks (top two rows) are trained using standard deep learning procedures on datasets A and B. The \nlabeled rectangles (e.g., WA1) represent the weight vector learned for that layer, with the color indicating which \ndataset the layer was originally trained on. The vertical, ellipsoidal bars between weight vectors represent the \nactivations of the network at each layer. The target networks (bottom two rows) represent transfer learning strategies. \nThe first n weight layers of the network (in this example, n = 3) are copied from a network trained on one dataset \n(e.g., A), and then the entire network is trained on the other dataset (e.g., B). Usually, the first n layers are either \nlocked during training or allowed to learn. \n \n \n \n \n22 \n \nour learning task is a document classification, and each term is taken as a binary feature, then  \nis the space of all term vectors,  is the \n term vector corresponding to some documents, and   \n \nFigure 1 - Transfer learning overview - reproduced from Yosinski et al. (2014). \n \n \n \n \n \n \n \n \n \n \n \n \nis a particular learning sample. In general, if two domains are different, then they may have \ndifferent feature spaces or different marginal probability distributions. Given a specific domain, \n, a task consists of two components: a label space \n and an objective predictive \nfunction \n, denoted by \n, which is not observed but can be learned from the training \ndata, which consist of pairs \n, where \n and \n. The function \n can be used to \npredict the corresponding label, \n, of a new instance . From a probabilistic viewpoint, \n \ncan be written as \n. In our document classification example,  is the set of all labels, which \n \n \n \n23 \n \nis True, False for a binary classification task, and \n is \"True\" or \"False\". For simplicity, let’s \nconsider the case where there is one source domain \n, and one target domain, \n, as this is by  \nfar the most popular of the research works in the literature. More specifically, lets denote the  \nsource domain data as \n, where \n is the data instance \nand \n is the corresponding class label. In the document classification example, \n can be  \na set of term vectors together with their associated true or false class labels. Similarly, lets denote \nthe target domain data as \n, where the input \n is in \n and \n is the corresponding output. In most cases, \n (Pan and Yang, 2010, 3). \nDefinition 1 (Transfer Learning) Given a source domain \n and learning task \n, a target domain \n and learning task \n, transfer learning aims to help improve the learning of the target \npredictive function \n in \n using the knowledge in \n and \n, where \n, or \n. \nIn the above definition, a domain is a pair \n. Thus, the condition \n implies \nthat either \n or \n. Similarly, a task is defined as a pair \n. \nThus, the condition \n implies that either \n or \n. When the \ntarget and source domains are the same, i.e., \n, and their learning tasks are the same, i.e., \n, the learning problem becomes a traditional machine learning problem. When the \ndomains are different, then either (1) the feature spaces between the domains are different, i.e., \n, or (2) the feature spaces between the domains are the same but the marginal \nprobability distributions between domain data are different, i.e., \n, where \n \nand \n. Pan and Yang (2010) summarized the relationship between traditional machine \nlearning and various transfer learning settings, categorizing transfer learning under three sub-\nsettings (Table 2.1), then we can read Definition 1 as: Given a source domain \n and learning \ntask \n, a target domain \n and learning task \n, \n \n1. \ninductive transfer learning aims to help improve the learning of the target predictive \nfunction \n in \n using the knowledge in \n and , where \n. \n \n \n \n24 \n \n2. \ntransductive transfer learning aims to help improve the learning of the target predictive \nfunction \n in \n using the knowledge in \n and , where \n and \n. \n3. \nunsupervised transfer learning aims to help improve the learning of the target predictive \nfunction \n in \n using the knowledge in \n and , where \n and \n and \n are not \nobservable. \n \nTable 1 - Relationship between Traditional Machine Learning and Transfer Learning \nLearning Settings \nSource and Target Domains \nSource and Target Tasks \nTraditional Machine Learning \nthe same \nthe same \nInductive Transfer Learning \nthe same \ndifferent but related \nTransductive Transfer Learning \ndifferent but related \nthe same \nUnsupervised Transfer Learning \ndifferent but related \ndifferent but related \n \n \n2.2.1 \nInductive Transfer Learning \nDefinition 2 (Inductive Transfer Learning) Given a source domain \n and a learning task \n, a \ntarget domain \n and a learning task \n, inductive transfer learning aims to help improve the \nlearning of the target predictive function \n in \n using the knowledge in \n and \n, where \n.  \nBased on the above definition, a few labeled data in the target domain are required as the \ntraining data to induce the target predictive function. This setting has two cases: (1) labeled data \nin the source domain are available; (2) labeled data in the source domain are unavailable while \nunlabeled data in the source domain are available. Most transfer learning approaches in this \nsetting focus on the former case (Pan and Yang, 2010, 4-5). Inductive transfer learning can be \nused to instance-transfer, feature-representation-transfer, parameter-transfer and relational-\nknowledge-transfer. \n \n \n \n \n25 \n \n2.2.2 \nTransductive Transfer Learning \nDefinition 3 (Transductive Transfer Learning) Given a source domain \n and a corresponding \nlearning task \n, a target domain \n and a corresponding learning task \n, transductive transfer \nlearning aims to improve the learning of the target predictive function \n in \n using the \nknowledge in \n and \n, where \n and \n.  \nIn addition, some unlabeled target domain data must be available at training time. This \ndefinition covers the work of Arnold et al. (2007), since they considered domain adaptation, \nwhere the difference lies between the marginal probability distribution of source and target data, \ni.e., the tasks are the same, but the domains are different (Pan and Yang, 2010, 8). Transductive \ntransfer learning can be used to instance-transfer and feature-representation-transfer, and it is the \nsetting we explore in chapters 4 and 5 in two macroeconomic problems well suited for this \napproach, business cycle identification and output gap estimation, respectively. The intuitive \nbehind the feature-representation-transfer is to learn a good feature representation for the target \ndomain. In this case, the knowledge used to transfer across domains is encoded into the learned \nfeature representation. With the new feature representation, the performance of the target task is \nexpected to improve significantly. \n2.2.3 \nUnsupervised Transfer Learning \nDefinition 4 (Unsupervised Transfer Learning) Given a source domain \n with a learning task \n, a target domain \n and a corresponding learning task \n, unsupervised transfer learning aims \nto help improve the learning of the target predictive function \n in \n using the knowledge in \n and \n, where \n and \n and \n are not observable. \nBased on this definition, no labeled data are observed in the source and target domains in \ntraining, since in unsupervised transfer learning, the predicted labels are latent variables, such as \nclusters or reduced dimensions. Unsupervised transfer learning can be used to feature-\nrepresentation-transfer. \n \n \n \n \n \n \n26 \n \n2.2.4 \nTransfer Learning settings and Macroeconomic typologies \n \nAs mentioned in 2.2.2, we apply transductive transfer learning in chapters 4 and 5. However, \ndepending on the problem type and the available data, the path may di er, as shown in Figure 2. \n \nFigure 2 - Different settings of transfer - reproduced from Pan and Yang (2010) \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nStarting through the shortest path, when there is no labeled data in both source and target \ndomain, generally a situation in which we have data, possibly even a lot of data, but not a clear \ndefinition or classification of the existing relationships. This scenario is one of the most \nchallenging and one of the most promising, as it is minimal or even no human intervention in \nassigning the relationship between the data. Some possible macroeconomics applications are \nclustering for regional studies and dimensionality reduction for input variables. \n \nNext, Figure 2 shows two related cases to inductive transfer learning, where labeled data \nare available in a target domain, and source and target domains are the same. In case 1, the self-\ntaught learning, there is no labeled data in a source domain, while in case 2, the multi-task \nlearning, labeled data are available in a source domain, where the source and target tasks are \nlearned simultaneously. From a macroeconomics perspective, relevant application could be \n \n \n \n27 \n \nfinding priors, or initialization parameters for models, or applying natural language processing \n(NLP) for sentiment analysis on monetary policy committee statements. Suppose a sentiment \nanalysis problem in which the researcher wants to classify monetary policy committee statements \nof a given country in hawkish or dovish. Still, there is insufficient textual data because it is a \nrecently implemented regime in the country. The researcher can then use a model previously \ntrained with data from other countries and then use an inductive transfer learning strategy. \n \nLastly, the two situations of transductive transfer learning, where labeled data are \navailable only in a source domain with two alternative assumptions: different domains but single \ntask, the domain adaptation case, and single domain and task, the sample selection \nbias/covariance shift. The latter is a recurring problem in macroeconomics and, therefore, with \nwell-developed frameworks, which does not prevent considering transfer learning as an \nadditional option. The former case, domain adaptation, is what we explore in chapters 4 and 5. \nNot all combinations between the macroeconomic models (Blanchard, 2018) typologies and the \ntransfer learning (Pan and Yang, 2010) settings will necessarily succeed, but certainly, each one \ncan be explored. In Figure 3 we speculate about some possibilities. \n \nFigure 3 - Transfer Learning settings and Macroeconomic typologies \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n28 \n \n3   EMPIRICAL SETUP \n \n \n3.1   Algorithms for Transfer Learning \n \nAfter discovering which knowledge can be transferred, learning algorithms need to be developed \nto transfer the knowledge (Pan and Yang, 2010, 3). To name a few, Dai et al. (2007a) proposed a \nboosting algorithm, TrAdaBoost, which is an extension of the AdaBoost algorithm, to address the \ninductive transfer learning problems, Jebara (2004) proposed to select features for multi-task \nlearning with Support Vector Machines (SVM), Ruckert and Kramer (2008) designed a kernel-\nbased approach to inductive transfer, which aims at finding a suitable kernel for the target data, \nMihalkova et al. (2007) proposed an algorithm that transfers relational knowledge with Markov \nLogic Networks (MLN) across relational domains, Dai et al. (2007b) extended a traditional Naive \nBayesian classifier for the transductive transfer learning problems. See Pan and Yang (2010) and \nWeiss et al. (2016) for many other examples. \n \nDue to the availability of various machine learning (ML) methods and considering that \nwe are, especially in economics, in the explanatory era of its applications, works often apply \nseveral ML approaches to a specific dataset to compare their performances, a strategy known as \nhorse-race, as in Tiffin (2016), Cook and Hall (2017), Garcia et al. (2017), Gu et al. (2018), and \nPiger (2020). Makridakis et al. (2018) go further to compare various non-ML and ML forecasting \nmethods. Another usual approach, our choice, is selecting an appropriate strategy for the specific \ntask in advance. Deep learning is the most suitable choice for having achieved state-of-the-art \nperformance in pattern recognition and because, according to Bengio (2012), deep learning seems \nwell suited to transfer learning because it focuses on learning representations and, in particular, \non abstract representations which ideally disentangle the factors of variation present in the input. \nLearning representations of the data make it easier to extract useful information when building \nclassifiers or other predictors, and, in the case of probabilistic models, a good representation is \noften one that captures the posterior distribution of the underlying explanatory factors for the \nobserved input (Bengio et al., 2013). Additionally, deep learning is significantly less black box \nthan it was in the past. Since feature importance is relevant in economics, neural networks usually \nare not the first choice. However, interpretability in AI is an active area of research with \n \n \n \n29 \n \nsignificant advances, both by researchers’ determination and by the requirements of companies \nand governments to adopt these models for decision-making. Partial Dependence Plots, \nPermutation Feature Importance, Shapley Value, and Integrated Gradients are available methods \nto compute feature relevance, to name a few. We did not advance in feature relevance estimation \nhere, but it is already possible when necessary. \n \nTable 2 - Out-of-Sample Evaluation Metrics for Alternative Classifiers \n \nClassifier \n \nQPS \n AUROC \nNaïve Bayes \n  \n  \nNarrow \n 0.058  \n0.990 \nReal Activity \n 0.064  \n0.974 \nBroad \n 0.074  \n0.968 \nkNN \n  \n  \nNarrow \n 0.030  \n0.989 \nReal Activity \n 0.033  \n0.978 \nBroad \n 0.055  \n0.990 \nRandom Forest / Extra Trees   \n  \nNarrow \n 0.034  \n0.988 \nReal Activity \n 0.032  \n0.988 \nBroad \n 0.036  \n0.989 \nBoosting \n  \n  \nNarrow \n 0.043  \n0.980 \nReal Activity \n 0.037  \n0.978 \nBroad \n 0.039  \n0.982 \nLVQ \n  \n  \nNarrow \n 0.043  \n0.938 \nReal Activity \n 0.046  \n0.930 \nBroad \n 0.038  \n0.952 \nDFMS \n  \n  \nNarrow \n 0.041  \n0.997 \nReal Activity \n 0.047  \n0.992 \nEnsemble \n  \n  \nNarrow \n 0.034  \n0.992 \nReal Activity \n 0.029  \n0.993 \nBroad \n 0.031  \n0.993 \nReproduced from Piger (2020). \n  \n  \n \n \n \n30 \n \n \nOur choice of a specific algorithm can also be understood in the context of the data-\ncentric versus model-centric view. According to Ng (2021), in a model-centric view, we hold the \nfixed data and interactively improve code/model: take the data you have and develop a model that \ndoes as well as possible on it. On the other hand, in the data-centric view, we hold the code fixed \nand interactively improve the data: the quality of data is paramount and, with tools to improve the \ndata quality, multiple models can have a good performance. To cite an example, Piger (2020) \ncompared the performance of various machine learning techniques for business cycle prediction \nin the United States, finding similar results among them (Table 2), indicating that those \nexcellent results were obtained mainly due to the quality of the data used to ingest into the \nmodels. \n \n3.1.1   Deep Learning \nA deep neural network, also known as deep learning (DL), is an artificial neural network \n(ANN) with multiple layers hidden between the input and output layers (Bengio, 2009). Deep \nlearning is a sub-field within machine learning that is based on algorithms for learning multiple \nlevels of representation in order to model complex relationships among data (Deng and Yu, 2014, \n200). In essence, almost all DL algorithms can be described as a combined specification of a data \nset, a cost function, an optimization3 procedure, and a model (Goodfellow et al., 2016). The \nanalytical function corresponding to one of the simplest forms of an ANN, the feed-forward \nnetwork, can be written as follows (Bishop, 1994, 118-9). In a feed-forward network having two \nlayers there are d inputs, M hidden units and c output units, where  and  are the network input \nand output variables, respectively. The output of the jth hidden unit is obtained by first forming a \nweighted linear combination of the d input values, and adding a bias, to give \n                                                  \n(3.1)\n \n \n3 Deep learning algorithms involve optimization in many contexts. See (Goodfellow et al., 2016, 271-325) for a \ncomprehensive discussion. \n \n \n \n31 \n \nHere \n denotes a weight in the first layer, going from input i to hidden unit j, and \n denotes \nthe bias for hidden unit j. The bias term for the hidden units is made explicit by the inclusion of \nan extra input variable \n whose value is permanently set at \n. This can be represented \nanalytically by rewriting (3.1) in the form \n                                                       \n(3.2)\n \nThe activation of hidden unit j is then obtained by transforming the linear sum in (3.2) \nusing an activation function \n to give \n                                                         (3.3) \nThe outputs of the network are obtained by transforming the activations of the hidden \nunits using a second layer of processing elements. Thus, for each output unit k, we construct a \nlinear combination of the outputs of the hidden units of the form \n                                              \n(3.4)\n \nAgain, we can absorb the bias into the weights to give \n                                                     \n(3.5)\n \nwhich can be represented by including an extra hidden unit with activation \n . The \nactivation of the kth output unit is then obtained by transforming this linear combination using a \nnon-linear activation function, to give \n                                                        (3.6) \n \n \n \n32 \n \nHere we have used the notation \n for the activation function of the output units to \nemphasize that this need not be the same function as used for the hidden units. If we combine \n(3.2), (3.3), (3.5) and (3.6) we obtain an explicit expression for the complete function in the form \n                                         \n(3.7)\n \nThese models are called feed-forward because information flows through the function \nbeing evaluated from inputs , through the intermediate computations used to define the function, \nand finally to the output target . There are no feedback connections in which the outputs of the  \nmodels are fed back into itself. When processing sequential data is required, there are alternatives \nlike recurrent neural networks (RNN), that process an input sequence one element at a time, \nmaintaining in their hidden units a state vector that implicitly contains information about the \nhistory of all past elements of the sequence (LeCun et al., 2015). The long short-term memory \n(LSTM), a gated RNN, is one of the most effective sequence models used in practical \napplications (Goodfellow et al., 2016, 404). The LSTM forward propagation equations for a \nshallow recurrent network architecture are given below (Goodfellow et al., 2016, 405-406). \nInstead of a unit that simply applies an element-wise nonlinearity to the affine transformation of \ninputs and recurrent units, LSTM recurrent networks have \"LSTM cells\" that have an internal \nrecurrence (a self-loop), in addition to the outer recurrence of the RNN. Each cell has the same \ninputs and outputs as an ordinary recurrent network, but also has more parameters and a system \nof gating units that controls the flow of information. The most important component is the state \nunit \n, which has a linear self-loop. The self-loop weight, or the associated time constant, is  \ncontrolled by a forget gate unit \n, for time step t and cell i, which sets this weight to a value \nbetween 0 and 1 via sigmoid unit: \n                                \n(3.8)\n \n \n \n \n33 \n \nwhere \n is the current input vector and \n is the current hidden layer vector, containing the \noutputs of all the LSTM cells, and \n, \n, \n are respectively biases, input weights, and \nrecurrent weights for the forget gates. The LSTM cell internal state is thus updated as follows, \nbut with a conditional self-loop weight \n: \n                 \n(3.9)\n \nwhere ,  and \n respectively denote the biases, input weights, and recurrent weights into the  \nLSTM cell. The external input gate unit \n is computed similarly to the forget gate, with a \nsigmoid unit to obtain a gating value between 0 and 1, but with its own parameters: \n                            \n(3.10)\n \nThe output \n of the LSTM cell can also be shut off, via the output gate \n, which \nalso uses a sigmoid unit for gating: \n                                              (3.11) \n                           \n(3.12)\n \nwhich has parameters \n, \n and \n for its biases, input weights and recurrent weights, \nrespectively. \n \n \n \n \n \n \n34 \n \n3.1.2 \nCost function \n \nAccording to (Goodfellow et al., 2016, 173-4), the cost functions for neural networks are \nmore or less the same as those for other parametric models, such as linear models. The total cost \nfunction used to train a neural network will often combine cost functions with a regularization \nterm in order to prevent overfitting4. Hence, we can think of three situations concerning the \noverfitting problem, where the model family being trained either 1) exclude the correct data-\ngenerating process corresponding to underfitting and inducing bias, or 2) match the true data \ngenerating process, or 3) include the generating process but also many other possible generating \nprocesses - the overfitting regime where variance rather than bias dominates the estimation error. \nThe goal of regularization is to take a model from the third regime into the second regime \n(Goodfellow et al., 2016, 224). Regularization is any modification we make to a learning \nalgorithm that is intended to reduce its generalization error but not its training error. \nRegularization is one of the central concerns of the field of machine learning, rivaled in its \nimportance only by optimization (Goodfellow et al., 2016, 117). \n \n3.1.3 \nOptimization procedure \n \nWhen we use a machine learning algorithm, we sample the training set, and then use it \nto choose the parameters to reduce training set error. We then sample the test set. Ander this \nprocess, the expected test error is greater than or equal to the expected value of training error. The \nfactors that determine how well a machine learning algorithm will perform are its ability to: 1) \nmake the training error small; 2) make the gap between training and test error small (Goodfellow \net al., 2016, 109). \n \nBack-propagation is a method to calculate a gradient that is needed in the calculation of \nthe weights to be used when training the network. Back-propagation is a special case of an older \nand more general technique called automatic differentiation. In the context of learning, back-\npropagation is commonly used by the optimization algorithm to adjust the weight of neurons by \ncalculating the gradient of the loss function. This technique is also sometimes called backward \npropagation of errors, because the error is calculated at the output and distributed back through \n \n4 See Goodfellow et al. (2016), Chapters 6 and 7 for further discussion. \n \n \n \n35 \n \nthe network layers. (Goodfellow et al., 2016, 213) describe the general back-propagation \nprocedure. \n \nThe problem of determining the capacity of a deep learning model is especially difficult \nbecause the capabilities of the optimization algorithm limit the effective capacity, and we have \nlittle theoretical understanding of the general non-convex optimization problems involved in deep \nlearning (Goodfellow et al., 2016, 112). Even though most practitioners, for many years, believed \nthat local minima were a common problem plaguing neural network optimization, today that does \nnot appear to be the case. The problem remains an active area of research, but experts now \nsuspect that, for sufficiently large neural networks, most local minima have a low-cost function \nvalue, and that it is not essential to find the correct global minimum rather than to find a point in \nparameter space that has low but not minimal cost (Goodfellow et al., 2016, 282). \n \n3.2 \nPractical Methodology \nSince machine learning practitioners cannot prove, at least until now, which model is the best for \neach situation, we search for successful deep artificial neural networks that can exhibit small \ndifferences between training and test performance (Zhang et al., 2016). According to Goodfellow \net al. (2016), successful applying deep learning techniques requires more than just a good \nknowledge of what algorithms exist and the principles that explain how they work. A good \nmachine learning practitioner also needs to know how to choose an algorithm for a particular \napplication and how to monitor and respond to feedback obtained from experiments to improve a \nmachine learning system. See (Goodfellow et al., 2016, 416-435) for more details about \nperformance metrics, default baseline models, determining whether to gather more data, selecting \nhyperparameters, and debugging strategies. \n \nThe models in Chapters 4, 5 and 6 were built using TensorFlow5, an interface for \nexpressing machine learning algorithms, and an implementation for executing such algorithms. \nTensorFlow is flexible and can be used to express a wide variety of algorithms, including training \nand inference algorithms for deep neural network models. It has been used to conduct research \n \n5 https://www.tensorflow.org/. \n \n \n \n36 \n \nand deploy machine learning systems into production across more than a dozen areas of computer \nscience and other elds (Abadi et al., 2015). \n \n3.2.1 \nHyperparameters space \n \nThe primary architectural considerations are choosing the depth of the network and the \nwidth of each layer. Deeper networks are often able to use far fewer units per layer and far fewer \nparameters, as well as frequently generalizing to the test set, but they also tend to be harder to \noptimize. The ideal network architecture for a task must be found via experimentation guided by \nmonitoring the validation set error (Goodfellow et al., 2016, 194). The optimal set of \nhyperparameters was obtained using Hyperband (Li et al., 2018) from Keras Tuner6. We \ndelimited the grid search by hyperparameters according to Table 3 by looking for parsimonious \nmodels and observing what is generally adopted in the literature. As a robustness test, we \nretrained the models with others loss functions, the squared hinge (Chapter 4) and the adaptative \ngradient algorithm (AdaGrad) (Duchi et al., 2011) (Chapter 5), finding similar results. \n \nTable 3 - Hyperparameters space \nHyperparameter \nGrid search \nStep \nDense network depth \n1 - 4 \n+1 \nDense network hidden units \n16 - 256 \n2 \nLSTM network hidden units \n16 - 256 \n2 \nLambda regularizer \n0.0001 - 0.01 \n10 \nLearning rate \n0.0001 - 0.01 \n10 \nActivation function \nReLU, Tanh, Sigmoid \nOR \n \n \n \n \n \n6 https://www.tensorflow.org/tutorials/keras/keras_tuner. \n \n \n \n37 \n \n3.3 \nFeature representation \nA good feature representation should be able to reduce the difference in distributions between \ndomains as much as possible, while at the same time preserving essential properties of the \noriginal data (Pan et al., 2011). When we transfer learning, though, there are additional issues. \nThe challenge is to overcome the differences between the domains so that a classifier trained on \nthe source domain generalizes well to the target domain, but generalizing across distributions can \nbe di cult, and it is not clear which conditions have to be satisfied for a classifier to perform well \n(Kouw and Loog, 2018). From a macroeconometrics perspective, we could assume that the input \nvariables’ marginal contributions in explaining the business cycle phases are similar. However, \nthey are probably not equal for the different economic areas, as well as the respective input \nvariables’ sample covariance matrix. Since we are applying transductive transfer learning \n(subsection 2.2.2), we can presume two circumstances (Pan and Yang, 2010, 4): (1) the feature \nspaces between the source and target domains are different; (2) the feature spaces between \ndomains are the same, but the marginal probability distributions of the input data are different. \nThe latter case is related to domain adaptation. Our approach handles this well when the models \nsuccessfully build a common representation space for the different domains, which can be \nempirically verified when there are labels for the target domain, allowing evaluation by some \nerror criteria. Otherwise, it remains an open question. \n \n3.3.1   Robustness: baseline and benchmark models \n \n \nOur baseline models are a logistic model (Chapter 4), a linear regression model (Chapter \n5), and deep learning models without the transfer learning approach. They function as a reference \nto verify if negative transfer (Torrey and Shavlik, 2009) is not occurring, a situation in which the \ntransfer reduces the learning. And, as benchmarks, we compare our results with those released by \nentities that apply widely accepted techniques to identify the business cycle and estimate the \noutput gap, notably the Centre for Economic Policy Research (CEPR), the Brazilian Business \nCycle Dating Committee (CODACE), and the Central Bank of Brazil. For Chapter 6, as bench-\nmarks, we have the Chow and Lin (1971) method, the GDP Monitor from Instituto Brasileiro de \nEconomia (2015), and the Central Bank Economic Activity Index (IBC-Br). The choice of these \n \n \n \n38 \n \nbenchmarks is relevant because of the risk of overfitting7. In other words, no matter how complex \nthe relationships are between the data, there is always the risk of a neural network mapping the \nentire data set, resulting in overfitting, and subsequently failing in terms of generalization by not \ncorrectly classifying data not previously observed. That is why the test set is so relevant. That is \nwhy our strategy of using data from a country other than the one used for network training as a \ntest set is a critical factor in evaluating the models’ performance. \n \n3.3.2 \nReal-time data \n \nDepending on the task at hand, the real-time data approach assumes relevance when \nbuilding the database. For instance, if the objective is forecasting the business cycle, as in Piger \n(2020), we could use vintages containing the first release of data related to economic activity, \nwhose posterior revision can significantly affect the results. Notably, the models proposed here \ndo not require adjustments to deal with real-time data. Instead, use real-time data to ingest the \nmodels, which will scan for the optimal results based on this information because machine \nlearning models today largely reflect the patterns of their training data (Google, 2021). \n \n3.3.3 \nEconomics school of thought \n \nA supervised deep learning model learns from the features-targets without the need for \nstrong assumptions about its relation, which prevents the expert from choosing an underlying \neconomics school of thought to set up a model, although the variable selection might represent it. \nThe algorithm maps the input-output relation variables according to the training and validation \ndata. For instance, when we choose the National Bureau Economic Research (NBER)8 turning \npoints classification data as output label (Chapter 4), or the output gap based on the potential \noutput estimated by the U.S. Congressional Budget Office (CBO)9 as the target (Chapter 5), the \ndeep learning algorithm will learn from them how to classify the business cycle and to measure \nthe output gap, implicitly following the same schools of thought. \n \n \n7 According to , for every \n, there exists a neural network with ReLU activations of depth , width \n, and \n weights that can represent any function on a sample of size  in  dimensions. \n8 The most followed classification for U.S. business cycle. \n9 An economic-based estimation. See Shackleton (2018). \n \n \n \n39 \n \n3.4 \nSmall data, big data, good data \nA deep learning model is capable of handling a large number of explanatory variables (features) \nas indicator series (Chapter 6). However, there are some caveats, as the more features we use \nduring learning, the more data preprocessing and computer processing e orts will be required. For \nexample, if we train for the United States output gap estimation using dozens of features, we will \nneed the same quantity of features to transfer learning to other datasets or additional \npreprocessing strategies, as in Jackson and Rege (2019) that have fed an ANN with dynamic \nfactors. Although we are not working with big data, the models proposed here can deal with high-\ndimensional structured data. For unstructured data, however, modifications to the models would \nbe necessary. However, it is essential to emphasize that good data is sought, whether small or big. \n \nIt is also true when transfer learning, that improves learning in a new task by transferring \nknowledge from a related task that has already been learned (Torrey and Shavlik, 2009). More \nspecifically, domain adaptation can be considered a special set of transfer learning that aims at \ntransferring shared knowledge across different but related tasks or domains. An example of the \ncurrent state of the art is transfer learning with MobileNets (Howard et al., 2017). The MobileNet \nmodel is an open-source code that learned how to classify 1,000 categories from more than \n1,500,000 images. It has some verification points to start from freezing the previous points’ \nparameters and training a new model that recognizes images that are not necessarily among the \n1,000 categories previously learned. This new model would start from the representation learning \npresent in the parameters transferred from MobileNet. For example, in a simple exercise, \npresenting only 100 new images of gestures used for the game stone, paper, and scissors, a \ntransfer learning model can identify each new category with high precision10. Without transfer \nlearning from MobileNet, the number of images needed to train this new model would be much \nhigher. \n \n3.4.1 \nRobustness: locked and unlocked models \n \nWhen transfer learning, the weight layers of the network for Euro and Brazilian data \n(Chapter 4), or just Brazilian data (Chapter 5), were copied from the network trained on the U.S. \n \n10 https://github.com/lmoroney/dlaicourse/tree/master/TensorFlow, in TensorFlow Deployment, Course 1, Week 4. \n \n \n \n40 \n \ndata, as in the last row of Figure 1, except that we do not retrain the parameters. It is the locked \nmodels and works as if these two datasets function as out-of-sample, also known as label \nextension. Additionally, we unlock the last layers and retrain the parameters, a way of relaxing \nthe macroeconometrics assumptions about input variables as mentioned before. In a big data \ncontext, as in the MobileNet example mentioned before, unlocked models are expected to \nperform better than locked models because they use information from the target dataset for \ntraining. However, in smaller databases, like ours, negative transfer learning can occur, a \nsituation in which some information from the target dataset ends up introducing noise that \ndamages the performance on training step. \n \n3.5 \nCross-sectional data into feed-forward neural network \nAs in the dynamic factor model with Markov-switching introduced by Chauvet (1998), the deep \nlearning approach accounts for the idea of business cycles as the simultaneous, asymmetrical, and \nnonlinear movement of economic activity in various sectors. This data-driven framework is \nflexible enough to be training with different features, the independent variables, and targets, the \ndependent variables. The option for a feed-forward network as a deep learning model, which \nrepresents memory-less models, derives from the focus on contemporary movement between the \nselected input variables and the business cycle and the output gap to explore the informational \ncontent of their cross-section distributions. This implies disregarding the time dependence \nobserved on the variables and shuffling the data to break it. The resulting model accounts just for \ncoincident relations. One advantage is the reduced computational cost for training compared to \nconfigurations that map the temporal dependency, which may not be very significant in small \ndatasets but is very relevant when working with big data. Although we are not working with big \ndata, we are looking for network configurations that are suitable for this case. Another relevant \npoint is that we emphasize the focus on contemporary relationships between inputs and labels, as \nour main objective is to transfer learning (Chapters 4 and 5) and convert the frequency of \nvariables (Chapter 6). In this sense, we reinforce the need to understand the limitations of our \napproach, emphasizing that models trained to detect correlations should not be used to make \ncausal inferences or imply that they can. \n \n \n \n \n \n41 \n \n3.5.1 \nRobustness: sequential approach with LSTM \n \nTo account for time dependency as an alternative strategy, we create additional models \nin Chapters 4 and 5 including a long short-term memory (LSTM) layer. \n \n3.6 \nNormalization, unbalanced, seasonality, stationarity, missing values \nA few more remarks about the empirical setup. Normalization, or standardization, is an important \nstep that has empirically shown positive results for learning in deep learning models. In Chapter \n6, for instance, the scale of the features we used differs by several orders of magnitude, making \nlearning slower or even less optimal if we did not adopt normalization when pre-processing the \ndata. In the same sense, in classification models such as in Chapter 4, it must be considered when \nthe database is unbalanced, that is, disproportional between positive and negative events, making \nadjustments in the base or in the model, such as setting an initial bias. \n \nSeasonality and stationarity are issues to be considered. Although the diagnosis itself \nand how to correct these characteristics is not always trivial, the better the quality of the data \ningested in the model, the better its performance should be. In our case, we chose to run \nalternative models in Chapter 4 because input variables show an increasing trend over time \n(Appendix A), although this is not always feasible, especially when dealing with big data. Thus, \nwe use seasonally adjusted, level, and first difference data. In the case of FNN models with a \ncross-sectional approach, an additional step in data pre-processing is shuffling the data, which \nalso minimizes seasonality and non-stationarity effects. In Chapter 6, where there is a mix of \nstationary and non-stationary, seasonal, and non-seasonal variables, we run models with observed \ndata and, alternatively, with year over year transformation. \n \nFinally, missing values can be handled prior to ingesting data into the models, with \ntraditional strategies to impute data, or during model training, more common in cases of \nunstructured data such as texts, where the model itself can predict a missing word in a sentence. \nNaturally, the late procedure could increase the time and affect the performance during the model \ntraining. In the current state of the art, treating missing values in the pre-processing data phase \nwith the most suitable technique for each circumstance is the most recommended approach. \n \n \n \n \n42 \n \n4  \nTRANSFER LEARNING FOR BUSINESS CYCLE \n \n \nMonitoring business cycle phases is a traditional task in applied macroeconomics. Progressive \nmarket integration has induced a worldwide interest in analyzing cyclical fluctuations through \neconomic indicators Chauvet (2001). Changes in exchange rates, outputs, consumption, inflation, \nand interest rates in different parts of the world can influence the effectiveness of government \npolicies and the competitive position of businesses, even those not directly related to international \noperations (Chauvet and Yu, 2006, p.43). As a result, a wide range of techniques has been \ndeveloped since the seminal work by Burns and Mitchell (1946). Recently, new approaches have \nemerged due to the progress in machine learning (ML) research, centering on building models \nthat achieve better forecasting performance than the non-ML models or that identify turning \npoints more timely (Piger, 2020). \n \nThis Chapter11 is organized as follows. Section 4.1 presents a literature review. In \nSection 4.2, we discuss the implementation details of the proposed new strategy (Section 2.2) \nwhen applied to Business Cycle identification, and, in Section 4.3, the empirical findings are \npresented. \n \n4.1 \nRelated Literature \nBusiness cycles are recurrent sequences of alternating phases of expansion and contraction \namong many economic activities (Burns and Mitchell, 1946). According to Harding and Pagan \n(2005), there are three ways in the literature to describe what we mean by a cycle, depending on \nwhether the focus is on the fluctuation of the level of economic activity, the level of economic \nactivity less a permanent component, or the growth rate of economic activity. Stock and Watson \n(2014) examine two approaches to identifying turning points, the (i) average-then-date, which \ndescribes the dating of reference cycles using a single highly aggregated series, such as GDP, and \nthe (ii) date-than-average, the approach of the pioneers of business cycle dating, who considered \n \n11 A modified version of this Chapter was published in Central Bank of Brazil Working Paper Series (Chauvet and \nGuimaraes, 2021). \n \n \n \n \n43 \n \na large number of disaggregated series. Their contributions provide a nonparametric definition of \na turning point and produce standard errors for the date-then-average chronologies. \n \nIn the United States, the NBER Business Cycle Dating Committee provides a \nchronology of business cycle expansion and recession dates. According to Piger (2020), because \nthe NBER methodology is not explicitly formalized, literature has worked to develop and \nevaluate formal statistical methods for establishing the historical dates of economic recessions \nand expansions in both the U.S. and international data. Estrella and Mishkin (1998), Estrella et al. \n(2000), Issler and Vahid (2006), Kauppi and Saikkonen (2008), Rudebusch and Williams (2009) \nand Fossati (2016) use an available historical indicator of the class, such as the NBER dates, to \nestimate the parameters of models such as logit or probit ones. This strategy is called a supervised \nclassifier in the statistical classification literature, in contrast to unsupervised classifiers, which \nendogenously determine the classes. Unsupervised classifiers have also been used, with the \nprimary example being the Markov-switching (MS) framework of Hamilton (1989), which \nbecome a relevant tool for applied work in economics. Chauvet (1998) proposes a dynamic factor \nmodel with Markov-switching (DFMS) to identify expansion and recession phases from a group \nof coincident indicators and Chauvet and Hamilton (2005), Chauvet and Piger (2008) and \nCamacho et al. (2018) evaluate the performance of variants of this DFMS model to identify \nNBER turning points in real-time. See Piger (2020) for a comprehensive review. \n \nApplied ML papers related to business cycles can be separated depending on whether \nthe main focus is predicting or identifying turning points and phases. For example, Hoptro et al. \n(1991), Qi (2001), Klinkenberg (2003), Nasr et al. (2007), Berge (2013), Ma (2015), Garbellano \n(2016), Nyman and Ormerod (2017), and James et al. (2019) have applied machine learning \ntechniques such as artificial neural networks, support vector machines, boosting, k-nearest \nneighbor, and random forest to forecasting turning points, recessions, or business cycles phases \nmainly in the US, but also other countries12. These studies have generally reported some \nimprovements over non-ML strategies. The other set of papers is concerned about identifying the \nturning points for real-time classification. Morik and Ruping (2002), Giusto and Piger (2017), \nSoybilgen (2018), Ra not and Benoit (2019) and Jackson and Rege (2019) have applied inductive \n \n12 United Kingdom, Japan, West Germany, and Lebanon. \n \n \n \n \n44 \n \nlogic programming, learning vector quantization, random forest, boosting, k-nearest neighbor and \nartificial neural networks fed with dynamic factors. Piger (2020), in a comprehensive analysis, \ncompares five ML techniques with DFMS. These studies have reported quickly and accurately \nturning points identification. \n \nLastly, some literature is dedicated to the study of business cycles worldwide, as in \nChauvet and Yu (2006), Cuba-Borda et al. (2018), Abberger et al. (2020), and the reference \nturning points of the OECD Composite Leading Indicators13. \n \n4.2 \nImplementation \nThe feature selection comprises the coincident variables indicated by NBER14 as the fundamental: \ngross domestic product (GDP), income, employment, industrial production, and wholesale-retail \nsales. Quarterly data is adopted because this is the frequency at which some relevant variables for \nthe classification of the business cycle are available, and at this frequency, opposed to higher \nones, the data usually carry less noise, which may facilitate the training and the transfer learning. \nWe computed the first difference of the logarithm of the input features, capturing the growth rate \n(Harding and Pagan, 2005, 152-154). Alternatively, we run the model without this \ntransformation, i.e., features in level. In both cases, the features are normalized. To have a \ncommon starting point for each dataset, we restrict the series’ start to eliminate missing values. \nFor example, we do not acquire data for the Euro area before 2005 because we do not have \nemployment data before this year for all the selected countries. Even if we used some strategy to \nextend this variable, as mentioned in 3.6, given that the others started in 1995, it would not be \nenough to incorporate another recession period according to CEPR data. We adopted a U.S. \ndataset for deep learning and two datasets, with data from Brazil and Europe, for transfer \nlearning. The target values are the business cycle chronology provided by the NBER, the \nCODACE, and the CEPR Euro Area Business Cycle dating committees, respectively. Appendix \n \n13 Available at https://www.oecd.org/sdd/leading-indicators. \n14 The NBER does not de ne a recession in terms of two consecutive quarters of decline in real GDP. Rather, a \nrecession is a significant decline in economic activity spread across the economy, lasting more than a few months, \nnormally visible in real GDP, real income, employment, industrial production, and wholesale-retail sales. Source: \nhttps://www.nber.org/cycles.html. \n \n \n \n \n45 \n \nA summarizes the information about all series, mostly from the Federal Reserve Economic Data \n(FRED)15 dataset, provided by the Federal Reserve Bank of St. Louis. Our data les are available \nat https://github.com/rrsguim/PhD_Economics. \n \nFollowing Piger (2020), the area under the ROC curve (AUC) is the objective function \nto maximize in the validation step when training the deep learning model. This metric is desirable \nhere for being scale-invariant, measuring how well predictions are ranked, rather than their \nabsolute values, and classification-threshold-invariant, measuring the quality of the model’s \npredictions irrespective of what classification threshold is chosen. \n \nBeginning with the deep learning step, we split the U.S. dataset into train, validation, \nand test sets. Then, we de ne a function that creates a neural network with hidden layers, ReLU as \nactivation function, a dropout layer to reduce overfitting, and a sigmoid output layer that returns \nthe probability of recession. Next, we retrain the model with the optimal hyperparameters, \nselected with Hyperband, to evaluate the results in both datasets, source and target, with binary \ncross-entropy as a loss function and Adam (Kingma and Ba, 2017) for optimization. \n \nAll codes used are available at https://github.com/rrsguim/PhD_Economics. In \nAppendix B we reproduce selected parts of the best performance transfer learning model for \nbusiness cycle identification. \n \n4.3 \nResults \nBefore presenting the results, it is relevant to assert that contributing to the turning points forecast \nimprovement is not the objective of this chapter. The predicting of the next business cycle phase \nturning point, which becomes relevant at times like the current one when many countries are in \nthe recession phase due to the COVID-19 pandemic, is a well-consolidated part of the business \ncycle research field (Section 4.1) and whose state of the art is similar to macroeconomics \nforecasting in general. However, when focusing on identifying the business cycle phase, we \naddress challenges as the absence of a business cycle dating committee and the limited datasets. \nAnother aspect to emphasize is that applying transfer learning to the Brazilian and the Euro area \n \n15 https://fred.stlouisfed.org/. \n \n \n \n \n46 \n \ndata is a choice related to the availability of committees that explicitly adopt the same, or very \nsimilar, classification approach than the committee (NBER) selected when training the models. \nThis choice allows empirically evaluate the proposed method performance from labels in the \nsource and target datasets. Good performance increases confidence to apply it to unlabeled \ndatasets. \n \nThe method proposed in Subsection 2.2.1 involves two stages: the first one consists of a \nmachine learning model that learns to perform a specific task based on a dataset; the second \nfocuses on applying that learning to another dataset. The deep learning models, our choice as \nmentioned in 3.1.1, have learned how to classify business cycle phases. The transfer learning \nperformance, stage 2, proved to be superior to our expectations. Figures 4, 5, 6, and 7 consolidate \nthe results found. Results are slightly different each time the models are run due to different \ncompositions in the selected data sets for training, validation, and test, especially in the cross-\nsectional approach. \n \nFigure 4 - Deep Learning (U.S) \n \n \n \n \n \n \n \n \n \n \n \n \n \nThe comparison in Figure 4 shows the performance of the deep learning step: the U.S. \nbusiness cycles, where shadow areas indicate recession phases according to NBER, and the feed-\nforward neural networks (FNN) model with the cross-sectional approach estimation, where the \nblue lines are showing the recession probability on each quarter. The alternative model that \n \n \n \n47 \n \naccounts for time dependency exhibited exceptional results as well, with almost all points \ncorrectly classified. Concerning the baseline models with U.S. data, we observed a significantly \nsuperior performance, measured by the AUC with out-of-sample data, of the deep learning \nmodels with data in first difference (1df). The outcomes of the baseline models for the nineteen \nselected countries in Europe (EURO-19), where there is no transfer learning step, do not motivate \nconfidence because the FNN model has a perfect classification with AUC=1, while the \nalternative model that includes an LSTM layer is unable to identify crises (AUC=0.5). This point \nhighlights one of the problems that the proposed methodology seeks to solve: identifying \nbusiness cycles when data is limited. Note, in Figure 5, that there are only two recessions for the \nEURO-19 in the period under analysis so that models trained only on these data show \ninconsistent results. Regarding Brazil’s outcomes, whose period contains more than one \nrecession, the baseline models show more satisfactory performance (Figure 6). We expected \nthese results because machine learning methods are good at pattern recognition. The results align \nwith the other machine learning strategies adopted in the literature (Section 4.1). There is no \ninnovation here, but the confirmation that deep learning models correctly classify the business \ncycle phases. \n \nHowever, the transfer learning step, which is not in the business cycle literature to the \nbest of our knowledge, exceeded our expectations. Figure 5 compares EURO-19’s CEPR \nclassification, Brazil’s CODACE classification, and the transfer learning models results. These \nresults refer to the locked models, meaning the estimates for EURO-19 and Brazil operate as if \nthey were out-of-sample because the parameters trained with U.S. data are locked in the transfer \nlearning phase when applied to target EURO-19 and Brazil datasets. Figure 6 presents the details, \nand Figure 7 a summary comparison of each models’ performance. Differences in the sizes of \nout-of-sample data sets reflect the need for adjustments according to time series, cross-sectional, \nlocked, and unlocked strategies. Overall, there is an improvement in the classification with \ntransfer learning compared to baseline models. Although both showed excellent results, the \nLSTM models performed better than the FNN models, and locked models overperform unlocked \nmodels16. On the other hand, the models didn’t learn well with data in level, requiring \n \n16 As mentioned in 3.4.1, better performance is generally expected on unlocked models, but the opposite can occur in \ncases such as when dealing with small data. \n \n \n \n \n48 \n \ntransformation. As mentioned in Section 3.2, successfully applying deep learning techniques \nrequires monitoring and responding to feedback obtained from experiments, which is data driven. \nNot necessarily the same configuration will emerge from different datasets, which demands \nfinding the best hyperparameters to perform well the task at hand. \n \nFigure 5 - Transfer Learning (Euro Area and Brazil) \n \n \n \n \n49 \n \nFigure 6 - Models specifications and results \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n50 \n \nFigure 7 - AUC out-of-sample \n \n \n \n51 \n \n5   \nTRANSFER LEARNING FOR OUTPUT GAP \n \n \nMeasures of the gap between actual and potential activity are used frequently as indicators of the \neconomic cycle and play a vital role in monetary and fiscal policy (Koske and Pain, 2008). As a \nresult, the wide range of techniques that have been developed since the seminal work by Okun \n(1962) can be classified in different ways, such as between observed and unobserved components \nmethods or between pure statistical versus economic base approaches each with its pros and cons. \nThe emergence of new approaches applied to business cycles due to machine learning research \nprogress (Piger, 2020) can also be explored for the output gap, given that they are related \nconcepts. \n \nThis Chapter is organized as follows. Section 5.1 presents a literature review. In Section \n5.2, we discuss the implementation details of the proposed new strategy (Section 2.2) when \napplied to Output Gap estimation, and, in Section 5.3, the empirical findings are presented. \n \n5.1 \nRelated Literature \nHow much output can the economy produce under conditions of full employment? Okun’s \nseminal paper (Okun, 1962) begins with this question and describes two related concepts, \npotential output, and the output gap. Potential output differs from actual only because the \npotential concept depends on the assumption that aggregate demand is exactly at the level that \nyields a rate of unemployment equal to four percent of the civilian labor force17. If, in fact, \naggregate demand is lower, part of potential GNP (Gross National Product, a measure of output.) \nis not produced; there is unrealized potential or a gap between actual and potential output (Okun, \n1962). Since then, various definitions of potential output, as well as estimation strategies, have \nbeen proposed and used in the literature, depending on the investigator’s objectives, although \nOkun’s definition is still the main reference concept for economic policy-makers, including \ncentral banks (Proietti et al., 2007). \n \n17 It was a reference to full employment at that time, as he discusses in the paper. \n \n \n \n \n52 \n \n \nAccording to Proietti et al. (2007), a useful classification is between unobserved \ncomponents and observed components methods. Harvey and Jaeger (1993) trend-cycle \ndecomposition of output and the Hodrick and Prescott (1981) HP filter are examples of univariate \napproaches of unobserved components, while Kuttner (1994), Gerlach and Smets (1999) and \nApel and Jansson (1999) focus on multivariate approaches. Observed components methods rely \non the Beveridge and Nelson (1981) decomposition and on structural vector autoregressive \n(VAR) models, which have been used by Blanchard and Quah (1989) and St-Amant and van \nNorden (1997). St-Amant and van Norden (1997) states that univariate methods, such as the HP \nfilter, are not reliable to measure the output gap. Hybrid methods that combine univariate and \nstructural relationships have proved to be hard to estimate, they may not be robust to reasonable \nalternative calibrations, and it is not easy to calculate their appropriate confidence intervals. \nHowever, methods combining estimated dynamics with structural information o er an interesting \nalternative. Another conceivable classification, as in D’Auria et al. (2010), is pure statistical \nversus economic base approaches, the latter being preferred because of the possibility of \nexamining the underlying economic factors which are driving any observed changes in the \npotential output indicator and consequently the opportunity of establishing a meaningful link \nbetween policy reform measures with actual outcomes. Hamilton (2018) goes further and claims \nthat, although the HP filter method continues today to be adopted in academic research, policy \nstudies, and analysis by private-sector economists, it is intended to produce a stationary \ncomponent from an I(4) series. However, in practice, it can fail to do so and invariably imposes a \nhigh cost. It introduces spurious dynamic relations that are purely an artifact of the filter and have \nno basis in the true data-generating process. He proposes a regression of the variable at date t + h \non the four most recent values as an alternative approach. Okun (1962) foresaw the challenges \nmentioned above in his seminal work: the quantification of potential output - and output gap - is, \nat best, an uncertain estimate and not a rm, precise measure. Chauvet and Guimaraes (2021) \nproposed a transfer learning strategy to identify business cycle phases when data are limited or \nthere is no business cycle data committee. The approach integrates the idea of storing knowledge \ngained from one region’s economic experts and applying it to other geographic areas. Here, we \nadopt the same strategy to measure the output gap. \n \n \n \n \n \n53 \n \n5.2 \nImplementation \nThe feature selection comprises variables usually incorporated in economic-based approaches: \nun-employment rate, capacity utilization, business cycle phase, and total factor productivity \nparity with U.S. when transfer learning. Quarterly data is adopted because this is the frequency at \nwhich some relevant variables for the estimation of the output gap are available, and at this \nfrequency, opposed to higher ones, the data usually carry less noise what may facilitate the \ntraining and the transfer learning. We adopted a U.S. dataset for deep learning and data from \nBrazil for transfer learning. The target18 values are calculated based on the CBO’s potential \noutput for the U.S. and the Brazilian Central Bank’s output gap for Brazil. Table 7 and Figure 20 \nin the Appendix summarizes all series information from the FRED19 dataset provided by the \nFederal Reserve Bank of St. Louis and from the Series Management System (SGS)20 dataset \nprovided \nby \nthe \nBrazilian \nCentral \nBank. \nOur \ndata \nles \nare \navailable \nat \nhttps://github.com/rrsguim/PhD_Economics. \n \nBeginning with the deep learning step, we split the U.S. dataset into train, validation, \nand test sets. Then, we de ne a function that creates a neural network with hidden layers, ReLU, \ntanh, or sigmoid as activation functions, and L1 (Lasso) regularization to reduce overfitting. \nNext, we retrain the model with the optimal hyperparameters, selected with Hyperband, to \nevaluate the results in both datasets, source, and target, with mean squared error as a loss function \nand the adaptive moment estimation (Adam) (Kingma and Ba, 2017) for optimization. All codes \nused and other results are available at https://github.com/rrsguim/PhD_Economics. See Appendix \nB for selected codes. \n \n5.3 \nResults \nBefore presenting the results, it is relevant to assert that evaluate the output gaps disclosed by \ndifferent sources is not the objective of this chapter. As it is an unobservable variable, there will \n \n18 This Chapter aims to observe the results of an empirical application to the method proposed in 2.2, and not \nevaluate the output gaps disclosed by different sources. \n19 https://fred.stlouisfed.org/. \n20 https://www3.bcb.gov.br/sgspub. \n \n \n \n \n54 \n \nalways be some dispute about its most appropriate value. It is common for analysts to observe \nmore than one source for decision-making. However, by focusing on learning how to estimate the \noutput gap based on data disclosure from a selected source, we address challenges like quickly \nassess an economic-based output gap and the limited datasets. Another aspect to emphasize is \nthat applying transfer learning to the Brazilian area data allows empirically evaluate the proposed \nmethod performance from labels in the source and target datasets. The theoretical model, \nhowever, enables the application to any region. Good performance increases confidence to apply \nit to unlabeled datasets also. \n \nThe method proposed in Subsection 2.2 involves two stages: the first one consists of a \nmachine learning model that learns to perform a specific task based on a dataset; the second \nfocuses on applying that learning to another dataset. The deep learning models, our choice as \nmentioned in 3.1.1, have learned how to estimate the output gap. The transfer learning \nperformance, stage 2, proved to be superior to our expectations. Figures 8, 9, 10, and 11 \nconsolidate the results found. Results are slightly different each time the models are run due to \ndifferent compositions in the selected data sets for training, validation, and test. \n \nFigures 8 shows the performance of the deep learning step: the U.S. output gap based on \nthe CBO’s potential output, compared with the FNN and the LSTM models. Concerning the \nbaseline models with U.S. and Brazilian data, Figure 9, we observed a superior performance, \nmeasured by the mean absolute error (MAE) with out-of-sample data, of the feed-forward neural \nnetwork (FNN) models. We expected these results because machine learning methods are good at \nfitting curves. There is no innovation here, but the confirmation that deep learning models \ncorrectly identify the output gap. \n \n \n \n55 \n \nFigure 8 - US Output GAP - Congress Budget Office (CBO) and Deep Learning models \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 9 - Models specifications and results \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n56 \n \n \nHowever, the transfer learning step, which is not in the output gap literature to the best \nof our knowledge, exceeded our expectations. Figure 10 shows a comparison between the output \ngap estimated by the locked transfer learning model and the one by the Central Bank of Brazil \nwhich the methodology is based on a Bayesian model estimation (Banco Central do Brasil, \n2020a), a different approach. It must be emphasized that, since it refers to the locked model, \nBrazil’s estimate operates as if it was out-of-sample because the parameters trained with U.S data \nare locked in the transfer learning phase, with no retraining with Brazilian data. The similarity is \nimpressive because the machine learned, based on the U.S. data, how to estimate the output GAP \nand then transfer this knowledge to Brazilian data obtaining similar results of the Brazilian \nCentral Bank specialists21, with just one extra piece of information about Brazil, the total factor \nproductivity (TFP) level at current purchasing power parities with the U.S. Additionally, the good \nperformance of transfer learning is observed, despite the period under analysis showing a \nnegative trend22 of the output gap, indicating that the model could learn in the presence of non-\nstationarity. However, depending on the researcher’s goals, further adjustments to the data may \nbe necessary. Figure 11 presents a summary comparison of the performance of each model. \n \nFigure 10 - Output GAP - Central Bank of Brazil (BCB) and Transfer Learning Model \n \n \n \n \n \n \n \n \n \n \n \n \n \n21 We rebuilt the dataset based on the graph and text data at (Banco Central do Brasil, 2020b, 56) because it does not \npublish the output gap series. \n22 Stationarity is expected, by definition, in large series of the output gap. \n \n \n \n57 \n \n \nFigure 11 - MAE out-of-sample \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n58 \n \n6 \nREPRESENTATION LEARNING FOR INTERPOLATION, DISTRIBUTION, AND \nEXTRAPOLATION OF TIME SERIES BY RELATED SERIES (RIDE) \n \n \n6.1 \nIntroduction \n \nEconomic and financial analysts commonly use time series modeling to predict future values, \nanalyze their salient properties and characteristics, and monitor the current state of the economy. \nHowever, as it is not uncommon that some relevant variables are not available with the desired \ntime and frequency, there is a research program aimed at transforming low-frequency economic \nvariables into high-frequency variables (Pavía-Miralles, 2010). The procedure by Chow and Lin \n(1971), a generalization of Friedman (1962), is a seminal reference to traditional statistical \nmethods that use related series, also called indicators. The main attraction of this approach, based \non a linear regression model set of indicators with first-order stationary autoregressive errors, is \nits simplicity (Mazzi and Proietti, 2015). However, these methods are restricted to a limited \nnumber of indicator series due, until the recent past, to data and computation restrictions. \nCurrently, large data sets are readily available, and models with hundreds of parameters are easily \nestimated (Angelini et al., 2006), allowing new approaches to open questions23, such as using \ndata with daily frequency, which is increasingly available24. \n \nThe purpose of this Chapter is to apply deep learning for mapping low-frequency from \nhigh-frequency variables25. Since deep learning methods are a way of learning representations, \nthose that are formed by the composition of multiple non-linear transformations, with the goal of \nyielding more abstract - and ultimately more useful - representations (Bengio et al., 2013), and \nalso inspired by Chow and Lin (1971), we denominate our strategy Representation Learning for \nInterpolation, Distribution, and Extrapolation of Time Series by Related Series (RIDE). \n \n23 In a comprehensive analysis, Pavía-Miralles (2010) describes the main techniques used, their advantages and \ndisadvantages, as well as the open questions related to this eld of research. \n24 (Chow and Lin, 1971, 372) mention that despite dealing only with estimating a monthly series from their quarterly \ndata and considering related monthly series, the theory, with minimal modifications, applies to the estimation of \nother frequencies. \n25 We are not constructing a latent variable indicator to evaluate the state of the economy. Since the seminal article \nby Burns and Mitchell (1946), an extensive literature has developed along these lines, with emphasis on the dynamic \nfactor models, e.g., Stock and Watson (2011) and Giannone et al. (2013). \n \n \n \n \n59 \n \n6.2 \nLiterature \n \n6.2.1 \nInterpolation, Distribution and Extrapolation of Time Series by Related Series \n \nMilton Friedman was an American statistician and economist awarded the Nobel Prize in \nEconomics in 1976 and known as the father of monetarism. He was an intellectual leader with \nvast production. In 1962, Friedman published The Interpolation of Time Series by Related Series, \na detailed analysis of the procedures adopted to construct time series related to economic \nphenomena. He highlighted the problem of estimating intermediate values that emerge, for \nexample, when using data from a biennial census for annual estimation of national income or \neven aggregating accounting information between groups of banks that report them in different \nperiods and frequencies. One of the most common operations performed in this process is \ninterpolation: estimating some component for dates for which it is not directly available from \nknown values of that component for other dates (Friedman, 1962, 729). A closely related \noperation, also widely used, is the distribution of a known total of a time unit among the shorter \ntime units, of which the longest unit is composed (Friedman, 1962, 730). Another related \noperation is the extrapolation of some com-ponents to more recent dates to obtain current \nnumbers before available reference data that will be used later in the interpolation (Friedman, \n1962, 730). Directing his analysis to interpolation, he first states that this process, when done \nbased only on the series being interpolated, takes the form of an old and well-known \nmathematical problem with extensive literature but rarely used in the construction of economics \ntime series. The procedure most used incorporates series known or assumed to be related to the \nseries to be interpolated (Friedman, 1962, 730). He argues, however, that this procedure has not \nbeen explored more rigorously, noting that each researcher uses their ad hoc procedure. Friedman \nestablishes that the interpolation of a series from related series involves two steps: 1) selection of \nthe related series to be used; 2) interpolation of the target series from related series. He clarifies \nthat his focus is on techniques for interpolation and that he will not deal with selection (step 1). \nMore specifically, he will deal with the restricted case where only one related series is used \n(Friedman, 1962, 731). After systematically presenting the methods used and associated errors \nand discussing various technical details involved in the choices to be made, he presents his \nconclusions (Friedman, 1962, 751-2), highlighting: i) mathematical interpolation and \n \n \n \n60 \n \ninterpolation from related series are not substitute methods, they are complementary; ii) related \nseries can improve interpolation; iii) correlation between series and deviations from trends are \nrelevant points; iv) interpolation must be performed only on the part of the series that is unknown \nfor the dates for which the interpolation will be performed, never generally covering the entire \nperiod. \n \nChow and Lin (1971) propose a unified approach to the problems of interpolation, \nextrapolation, and distribution that Friedman (1962), despite acknowledging to be related, dealt \nwith them in isolation. Like Friedman (1962), they point out that the variable selection process is \nbeyond their scope (Chow and Lin, 1971, 372). As a solution, they present the derivation of an  \nunbiased linear best estimator, assuming that the monthly observations of the  series to be \nestimated satisfy a multiple regression relationship with  related series \n. In the sample \nperiod of \n months, where  is the number of quarters, the relation is \n                                                          (6.1) \nwhere  is \nx ,  is \nx  and  is a random vector with zero mean and covariance matrix , \nand \n                                               (6.2) \nis the least squares estimator resulting from the regression coefficients using \n quarterly \nobservations of the sample, with the vectors subscripted with a dot indicating that they refer to \nquarterly data, and \n                                                        (6.3) \nis the x  residual vector of the regression using quarterly data. \n \nChow and Lin (1971) describe details of their approach, such as the treatment distinction \nbetween flow and stock series or the estimation of  in three different ways. Thus, according to \nthe authors, with a straightforward application of the unbiased linear best estimate theory to a \n \n \n \n61 \n \nregression model, the problems of interpolation, distribution, and extrapolation of a time series by \nrelated series can be solved in a unified way. The resulting estimator applies to all these cases. \nThe usefulness of this method in practice, concerning the estimation of monthly economic time \nseries, certainly depends on the validity of the assumed regression model or the possibility of \nfinding related series that make the regression model a good approximation to reality. According \nto (Pavía-Miralles, 2010, 455), Chow and Lin (1971) is probably the most influential and cited \narticle in the field of research related to interpolation, distribution, and extrapolation of time \nseries that use related series, or indicators. This procedure has two main advantages over those \nthat do not use indicators: i) better foundations in constructing hypotheses, and ii) they are more \nefficient because they adopt relevant economic and statistical information. Moreover, as the main \ndisadvantage, it indicates that the results obtained are influenced by choice of indicators. \n \n6.2.2 \nArtificial Neural Networks and Economics \n \nDirecting our attention to applied papers in economics, we begin with White (1988) \nwhich reports the results of a project using neural network modeling and learning techniques to \nsearch for and decode nonlinear regularities in asset price movements, based on the daily returns \nof the IBM common shares. In the same year, Dutta and Shekhar (1988) apply neural networks to \npredict the ratings of corporate bonds, finding evidence that it is a useful approach to \ngeneralization problems in such non-conservative domains. In 1991, Kuan and White (1991) \npublished Artificial Neural Networks: An Econometric Perspective, a reference document for \neconometrics. According to Tkacz and Hu (1999), the use of neural networks in economics was \nstill in its relative infancy, and the paper by Kuan and White (1991) was likely the definitive \nintroduction of neural networks to the econometrics literature. In a business report, Schwartz \n(1992) states that in the world of finance, anything that provides even a slight edge over rivals \ncan mean millions in extra profits. Thus, investment professionals are turning to gurus who o er \nexotic computer technologies such as neural networks and genetic algorithms. Forecasting corn \nfutures, Kohzadi et al. (1995) found that the prediction error of a neural network model was \nbetween 18 and 40 percent lower than that of an ARIMA model, using different forecasting \nperformance criteria. Tal and Nazareth (1995) reports that, in 1994, the Canadian Imperial Bank \nof Commerce replaced its index-based Leading Indicators with a neural network-based system \nand that the performance to that date had been very encouraging. Portugal (1995) provides an \n \n \n \n62 \n \nempirical comparative evaluation of the performance of the artificial neural network to the \nproblem of economic time series forecast, performing exercises in the gross industrial output of \nthe state of Rio Grande do Sul (Brazil) to find mixed results. Swanson and White (1997) applied \nneural networks to predict macroeconomic variables, contrasting different linear and nonlinear \nmodels using a wide array of out-of-sample forecasting performance indicators. Herbrich et al. \n(1998) provide an overview of existing economic applications of neural networks, distinguished \nin three types: classification of economic agents, time series prediction, and the modeling of \nboundedly rational agents. Tkacz and Hu (1999) pointed out that linear models are, in effect, \nconstrained neural network models, forecast output growth using neural networks and compare \nthe performance of such models with traditional linear specifications. They conclude that the best \nneural network models outperform the best linear models by between 15% and 19% of their data, \nimplying that neural network models can be exploited for noticeable gains in forecast accuracy. \nAccording to them, the gains in forecast accuracy seem to originate from the ability of neural \nnetworks to capture asymmetric relationships. Blake (1999) makes straightforward analogies \nbetween artificial neural networks and models more familiar to economists. He also applies \nartificial neural network to model GDP growth using variables that could be expected to lead the \ngrowth cycle or predict likely future growth disturbances for six major European economies: \nFrance, Germany, Italy, the Netherlands, Spain and the UK. \n \nMore recently, Varian and Choi (2009) found that simple seasonal autoregressive models and \nfixed-effects models that include relevant Google Trends variables tend to outperform models \nthat exclude these predictors. Analyzing data from Chile, Carriere-Swallow and Labbe (2010) \npresents evidence that the inclusion of information on Google search queries improves both the \nin- and out-of-sample accuracy of car sales models. Varian (2014) states that econometricians, \nstatisticians, and data-mining specialists often seek insights that can be extracted from the data, \nand while the most common tool used for summarization is (linear) regression analysis, machine \nlearning offers a set of tools that can usefully summarize various sorts of nonlinear relationships \nin the data. To overcome the problem of reliable data on economic livelihoods in the developing \nworld, Jean et al. (2016) developed a method of estimating consumption expenditure and asset \nwealth using high-resolution satellite imagery. With a similar goal to ours but applying other \ntechniques of machine-learning named elastic net and random forest, Tiffin (2016) built a \n \n \n \n63 \n \nnowcasting indicator for Lebanon’s GDP and achieved good results within an ensemble model. \nLastly, Makridakis et al. (2018) are extremely positive about the enormous potential of ML \nmethods for forecasting, noting that these methods have been proposed in the academic literature \nas alternatives to statistical ones for time series forecasting with scant evidence of their relative \nperformance in terms of accuracy and computational requirements. \n \nAfter reviewing this literature, we realize that artificial neural networks, to the best of our \nknowledge, were not applied to interpolate, distribute, and extrapolate time series by related \nseries until now. Table 4 summarizes a comparison between the Chow and Lin (1971) and our \napproach. \n \nTable 4 - A comparison between two approaches \nChow and Lin \nRIDE \n \n \nPerformance metrics \nMinimize error \nMinimize error \nInfluenced by the indicator’s choice \nYes \nYes \nUnified approach \nYes \nYes \nAssumption about variables relationship \nLinear \nNon-linear \nAnalytic demonstration \nYes \nNo \nBig data input \nHard to optimize \nFits well \n \n \n \n \n6.3 \nMapping GDP \nA relevant task in applied economics is finding metrics that represent the current state of the \neconomy. There is vast information out there, but it is not usually straightforward to isolate the \nsignal from the noise. The official gross domestic product (GDP), this well-known metric \ncustomarily disclosed by government statistical institutes worldwide and accompanied by several \nsegments, are usually published quarterly, sometimes yearly, and with lag that can reach several \nmonths, given the complexities involved in the calculations of the National Account systems. As \na result, many coincident indicators with a wide range of techniques have been developed in \n \n \n \n64 \n \neconomics. The relevance of this variable is indisputable, both for the monetary authority and for \ndiverse economic agents. What is the state of the (...) economy right now? Is it expanding or \nshrinking, and by how much? These are questions that official GDP statistics try to answer, but \nthey take time to be published. Can we obtain a quicker answer using other data sources? (Hinds \net al., 2017, 35). For that purpose, economists have developed models to estimate economic \nactivity in response to the regular ow of data. \n \nHere, we apply deep learning (3.1.1) as a strategy for mapping the Brazilian GDP. With \nthe end-to-end approach allowed by this technique, we propose to map a set of high-frequency \nvariables to fit the GDP. When evaluating an end-to-end strategy, we want to observe whether \ndeep learning models, known as universal approximators, perform well to this task. Since the \nhigher frequency variable is generally not observable, as in the typical case where we want a \nmonthly from a quarterly available GDP, we do not know the actual value to verify the \nperformance, so we adopted two approaches, i) as highlighted in 3.1.3, the factors that determine \nhow well a machine learning algorithm will perform are its ability to make the training error \nsmall, and make the gap between training and test error small (Goodfellow et al., 2016, 109), and \nii) comparing the results with other approaches, but understanding that none finds the true values. \n \n6.3.1 \nImplementation \n \nBy definition, GDP is the sum of final goods and services produced in an economy over \na period, but the access to its components and weights is not straightforward. From the \nperspective of an end-to-end approach, GDP is a composite index we want to map from their \ncomponents, even if we have only indirect measures or so-called proxy variables. It should be \nnoted, however, that these proxies are more related to the gross value of production, while we \nwant the added value; for details, see World Bank (2009). In our strategy, the neural network \nmust minimize this problem in its optimization process and because of its non-linear structure, \nwhose performance is measured by the resulting accuracy. \n \nIn the task of mapping the Brazilian GDP, we adopted twenty-three monthly variables as \nindicators, as presented in the Appendix A, Table 9, from January 1996 until March 2021. As \nmentioned in 6.2, the selection of variables is a relevant point. In the first place, our choice \nreflects the knowledge about which types of variables are related to GDP. It is an ad hoc decision \n \n \n \n65 \n \nand follows the logic of two of the most relevant activity indexes for the Brazilian economy, \nnamely, the GDP Monitor from Instituto Brasileiro de Economia (2015), and the Central Bank \nEconomic Activity Index (IBC-Br) from Banco Central do Brasil (2016). These indexes, mainly \nfollowed by agents interested in evaluating the current state of the Brazilian economy, adopt the \nstrategy known as accounting indices, that is, an index composed of many proxy variables that \nare known to be related to the variables used to prepare the official GDP. Naturally, this choice is \nnot perfect, and it encounters additional obstacles. Specifically for our application, we opted for \nseries available for more extended periods, which reduces their quantity. Proxies’ variables \nappear, change, and are discarded over time. If we restrict our sample to a shorter period, we \nwould have more but smaller series. Choosing a longer versus a wider database is a common \ntrade-o in this type of problem. \n \nWe computed the input features and the target in level26 and year over year (YoY) \npercentage change transformation, both normalized. We split the dataset into train, validation, \nand test sets. Then, we de ne a function that creates a neural network with hidden layers, ReLU, \ntanh, or sigmoid as activation functions, and L1 (Lasso) or L2 (Ridge) regularization to reduce \noverfitting. Next, we retrain the model with the optimal hyperparameters, selected with \nHyperband, to evaluate the results, with mean squared error as a loss function and the adaptive \nmoment estimation (Adam) (Kingma and Ba, 2017) for optimization. A browser-based version of \nthe \nproposed \nmodel, \nwithout \nthe \ngrid \nsearch \nfunction, \nis \navailable \nat \nhttp://www.deeplearningeconomics.com/RIDE/. \n \n6.3.2 \nResults \n \nThe results demonstrate the deep learning’s suitability for interpolation, distribution, and \nextrapolation of time series by related series. Figures 12 to 18 consolidate the results found, that \nare slightly different each time the models are run due to different compositions in the selected \ndata sets for training, validation, and test. In terms of learning, the curves of the training and \nvalidation set behave as expected (Figures 12 and 14), that is, over time (epochs), the highest \n \n26 In this case, the resulting monthly GDP is not stationary and presents seasonality. Additional adjustments, \ncommonly required for monthly GDP analyses, can be made a posteriori outside the proposed model. \n \n \n \n \n66 \n \nerror reduces and stabilizes at a low level, being the validation error a little higher than the \ntraining error. \n \n6.3.2.1 Interpolation and Distribution \n \nThe interpolation and distribution concepts (Friedman, 1962) described in 6.2 have \nevolved in ways that are sometimes taken to be similar, as noted by Issler and Notini (2014)27. A \nsubtle difference in the application to GDP data can be seen when some quarters referring to the \ntarget (GDP) within the sample are missing, but there are monthly indicators for these periods, in \nwhich case it would be necessary to interpolate GDP values, in addition to distributing them in \nthe quarter. \n \nThe performance of the deep learning model to generate monthly data is shown in \nFigures 13 and 15, where we see the good fitting related to the official quarterly data, and in \nfigures 16 and 17 the comparison of RIDE model in level with benchmarks for monthly GDP \ndata, namely the Chow and Lin (1971) method, the GDP Monitor from Instituto Brasileiro de \nEconomia (2015), and the Central Bank Economic Activity Index (IBC-Br)28 from Banco Central \ndo Brasil (2016). The RIDE correlation (year-over-year growth) with IBC-Br is 0.9266 and with \nGDP Monitor is 0.9213. Despite using different strategies, the different approaches result in a \nmonthly variable that follows GDP. As we have commented before, none of them can be taken as \nthe authentic higher frequency movement, but they are still essential to guide the decision-\nmaking process of governments and the private sector. \n \n \n \n \n \n27 First, a word of caution about using the term interpolation here. GDP is a ow variable for which we possess \nquarterly observations we want to distribute within the months in the quarter. [...] the problem of allocating a \nquarterly ow as such is referred to as distribution, whereas interpolation estimates monthly values of stock variables \nfrom quarterly values. Despite this technical distinction for ow and stock variables, several authors still refer to \ninterpolated GDP, a term that is now ingrained in the literature, being the reason why we employ it here (Issler and \nNotini, 2014, 7). \n \n28 The IBC-Br presents a level shift in 2015 due to methodological issues. \n \n \n \n \n67 \n \n6.3.2.2 Extrapolation \n \nLastly, we can use the higher frequency indicator series to infer the GDP already \nrealized but not yet disclosed. There are many questions related to this area of research, one of \nthe most relevant being the difference between near real-time versus real-time proxies29. \nHowever, these details are not the object of this work. What we can show is the extrapolation \ncapacity of the proposed model. The Figure 18 compares the Brazilian GDP realized between \n2020Q3 and 2021Q1 with an out-of-sample RIDE estimate. Like Chow and Lin (1971) and \nothers, RIDE is a global model where the parameters are calculated based on the entire sample; \ntherefore, in cases where the indicator series extends beyond the period cover by the endpoints of \nthe benchmark series, the target will be extrapolated with the global target/indicator ratio. \nFurthermore, like in the others approaches, when using RIDE the analyst can diversify the use of \nvariables, sample periods and evaluate performance one or several steps ahead, in real or near \nreal-time. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n29 Further advice from Kliesen and McCracken (2016) from his work about tracking the US economy is that users of \nnowcasting models should be aware that most of the monthly source data are initially sample-based estimates. This \nmeans, in effect, that the initial estimates are subject to repeated revision as new information becomes available. \n \n \n \n \n68 \n \nFigure 12 - Training and validation curves level \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 13 - RIDE mapping GDP level \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n69 \n \nFigure 14 - Training and validation curves YoY \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 15 - RIDE mapping GDP YoY \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n70 \n \nFigure 16 - Chow Lin, IBC-Br and FGV Monitor fitting GDP \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n71 \n \nFigure 17 - RIDE, IBC-Br and FGV Monitor \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 18 - RIDE extrapolation \n \n \n \n \n \n72 \n \n7  \nCONCLUSION \n \n \nIn this work, deep learning was applied to macroeconomics in two approaches: to transfer \nlearning and to interpolate, distribute, and extrapolate time series by related series. \n \nTransfer learning was proposed as an additional strategy for empirical macroeconomics. \nFirst, we presented theoretical concepts related to transfer learning and proposed a connection \nwith a typology related to macroeconomic models. At this point, there is already an enormous \npotential to be explored in the connection between transfer learning and applied macroeconomics. \nThen, we refer to several algorithms used for transfer learning and detail our choice, artificial \nneural networks with multiple layers hidden between the input and output layers, also known as \ndeep learning, because it has shown excellent performance for transfer learning, but also because \nit is significantly less black box than it was in the past. Since feature importance is relevant in \neconomics, neural networks usually are not the first choice. However, interpretability in AI is an \nactive area of research with significant advances, both by researchers’ determination and by the \nrequirements of companies and governments to adopt these models for decision-making. Partial \nDependence Plots, Permutation Feature Importance, Shapley Value, and Integrated Gradients are \navailable methods to compute feature relevance, to name a few. Secondly, we explore the \nproposed strategy empirically. As we showed, data from different but related domains, a type of \ntransfer learning, helped to identify the business cycle phases when there was no business cycle \ndating committee or to estimate a quick first economic-based output gap. In both cases, the \nproposed strategy also helped to improve the learning when data was limited. The approach \ndemonstrated excellent empirical performance with data from the US, Europe and Brazil, \nemerging as a potential supplementary tool for governments and the private sector to conduct \ntheir activities in the light of national and international economic conditions. The proposed \nstrategy allows a quick application of the economic-based models to other geographic areas. \nOnce trained, the machine inferred the business cycle state and the output gap based on the theory \nchosen for training. To the best of our knowledge, the combined deep and transfer learning \napproach is underused for application to macroeconomic problems, indicating that there is plenty \nof room for research development. \n \n \n \n \n73 \n \n \nAdditionally, since deep learning methods are a way of learning representations, we \napplied deep learning for mapping low-frequency from high-frequency variables because there \nare situations where we know, sometimes by construction, that there is a relationship between \ninput and output variables, but this relationship is di cult to map, a challenge in which deep \nlearning models have shown excellent performance, especially towards a data-centric view, \nwhere we hold the code fixed and interactively improve the data. The results obtained from \nBrazilian data demonstrate deep learning’s suitability for this task. \n \nAs a caveat, it is essential to mention that transfer learning depends not only on properly \ntuned databases and correctly specified models but also on some critical choices of the \neconometrician. For example, this work uses data from the American economy for data training \nand the Brazilian and European economies for transfer. Even though they are regions with their \nspecificities, it can be argued that they are pretty diversified and present relevant similarities. \nHowever, suppose that the transfer learning for business cycle classification was applied to a less \ndiversified economy, approaching monoculture. Would the good results persist? Intuitively, it is \nnot expected, which requires that the macroeconomist make different choices, such as using a \ndatabase for training a source country or region that is more similar to the target region. This fact \nreinforces the premise that the techniques used here require qualified professionals to take \nrelevant strategic decisions that might affect the results. \n \nNext, this research aims (i) improve the models to enable unstructured data types such as \ntexts and images as inputs, and (ii) in the context of data-centric versus model-centric AI \ndevelopment, move towards the former, building databases to ingest into automated machine \nlearning models. \n \n \n \n74 \n \nAPPENDIX A - DATA DESCRIPTION \n \n \nFigure 19 - U.S. raw data for learning Business Cycle. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n75 \n \n \nTable 5 - Dataset for Business cycle identification summary \nFRED Code \nSeries \nStart \nCharacteristics of the raw data \nSource \nUNITED STATES \nUSRECQ \nNBER based Recession Indicator \n1967:Q1 Recession: 1 = true ; 0 = false \nNBER \nGDPC1 \nReal Gross Domestic Product \n1967:Q1 Billions of Chained 2012 Dollars, s.a. FRED \nPIECTR \nReal personal income ex current transfers \n1967:Q1 Billions of Chained 2012 Dollars, s.a. FRED \nPRS85006013 \nNonfarm Business Sector employment index 1967:Q1 Index 2012 = 100, s.a. \nFRED \nIPB50001SQ \nIndustrial production index \n1967:Q1 Index 2012 = 100, s.a. \nFRED \nCQRMTSPL \nReal manufacturing and trade ind. sales \n1967:Q1 Millions of Chained 2012 Dollars, s.a., FRED \nEURO AREA \nN/A \nCEPR based Recession Indicator \n2005:Q1 Recession: 1 = true | 0 = false \nCEPR \nCLVMNACSCAB1GQEA19 Real Gross Domestic Product (19 countries) 2005:Q1 Millions of Chained 2010 Euros, s.a. \nFRED \nNAEXKP02EZQ189S \nPrivate Final Consumption Expenditure \n2005:Q1 Billions of Chained 2012 Dollars, s.a. FRED \nLFESEETTEZQ647S \nEmployees \n2005:Q1 Persons, s.a. \nFRED \nPRMNTO01EZQ657S \nTotal Manufacturing Production \n2005:Q1 Growth Rate Previous Period, s.a. \nFRED \nSLRTTO01EZQ657S \nVolume of Total Retail Trade sales \n2005:Q1 Growth Rate Previous Period \nFRED \nBRAZIL \nN/A \nCODACE based Recession Indicator \n2000:Q1 Recession: 1 = true | 0 = false \nCODACE \nNAEXKP01BRQ652S \nTotal Gross Domestic Product \n2000:Q1 Chained 2000 Real, s.a. \nFRED \nNAEXKP02BRQ189S \nPrivate Final Consumption Expenditure \n2000:Q1 Chained 2000 Real, s.a. \nFRED \nN/A \nRegistered Employees Index \n2000:Q1 Index Dez-2001 = 100, s.a. \nBCB \nBRAPROINDQISMEI \nProduction of Total Industry \n2000:Q1 Index 2015 = 100, s.a. \nFRED \nBRASARTQISMEI \nTotal Retail Trade \n2000:Q1 Index 2015 =100, s.a. \nFRED \n \n \n \n \n \n \n \n76 \n \n \nTable 6 - Dataset for Business cycle identification descriptive statistics \nSeries \ncount \nmean \nstd \nmin \nmax \nUNITED STATES \nNBER based Recession Indicator \n211 \n0.127962 \n0.334842 \n0.000000 \n1.000000 \nReal Gross Domestic Product \n211 \n0.006844 \n0.007829 \n-0.021876 \n0.037915 \nReal personal income ex current transfers \n211 \n0.006885 \n0.009104 \n-0.039786 \n0.033266 \nNonfarm Business Sector employment index \n211 \n0.003678 \n0.006319 \n-0.023668 \n0.019575 \nIndustrial production index \n211 \n0.005290 \n0.014928 \n-0.068358 \n0.041548 \nReal manufacturing and trade ind. sales \n211 \n0.006486 \n0.013956 \n-0.049767 \n0.049969 \nEURO AREA \nCEPR based Recession Indicator \n59 \n0.183333 \n0.390205 \n0.000000 \n1.000000 \nReal Gross Domestic Product (19 countries) \n59 \n0.002860 \n0.006591 \n-0.032153 \n0.011921 \nPrivate Final Consumption Expenditure \n59 \n0.002141 \n0.003526 \n-0.005379 \n0.009745 \nEmployees \n59 \n0.002086 \n0.003732 \n-0.010767 \n0.007213 \nTotal Manufacturing Production \n59 \n0.001266 \n0.021143 \n-0.108820 \n0.033571 \nVolume of Total Retail Trade sales \n59 \n0.001792 \n0.005575 \n-0.018082 \n0.010880 \nBRAZIL \nCODACE based Recession Indicator \n79 \n0.225000 \n0.420217 \n0.000000 \n1.000000 \nTotal Gross Domestic Product \n79 \n0.005596 \n0.011576 \n-0.040186 \n0.024456 \nPrivate Final Consumption Expenditure \n79 \n0.006708 \n0.011400 \n-0.031874 \n0.028647 \nRegistered Employees Index \n79 \n0.007989 \n0.008475 \n-0.012135 \n0.024315 \nProduction of Total Industry \n79 \n0.002106 \n0.023298 \n-0.101341 \n0.050569 \nTotal Retail Trade \n79 \n0.008367 \n0.015890 \n-0.033902 \n0.039346 \n \n \n \n \n \n \n \n \n77 \n \n \nFigure 20 - U.S. raw data for learning Output Gap \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n78 \n \nTable 7 - Dataset for Output Gap Estimation: summary \nFRED Code \nSeries \nStart \nCharacteristics of the raw data \nSource \nUNITED STATES \nUSRECQ \nNBER based Recession Indicator \n1967:Q1 Recession: 1 = true ; 0 = false \nNBER \nGDPC1 \nReal Gross Domestic Product \n1967:Q1 Billions of Chained 2012 Dollars, s.a. FRED \nGDPPOT \nReal potential Gross Domestic Product \n1967:Q1 Billions of Chained 2012 Dollars \nFRED \nUNRATE \nUnemployment rate \n1967:Q1 percent, s.a., quarterly average \nFRED \nTCU \nCapacity utilization index \n1967:Q1 percent, s.a., quarterly average \nFRED \nBRAZIL \nN/A \nCODACE based Recession Indicator \n2012:Q2 Recession: 1 = true | 0 = false \nCODACE \nNAEXKP01BR \nTotal Gross Domestic Product \n2012:Q2 Chained 2000 Real, s.a. \nFRED \nTFP \nTotal Factor Productivity Level (PPP Brazil and U.S.) \n2012:Q2 Index U.S. = 1 \nFRED \nN/A \nRegistered Employees Index \n2000:Q1 Index Dez-2001 = 100, s.a. \nBCB \n \n \n             Table 8 - Dataset for Output Gap Estimation: descriptive statistics \n \nSeries \ncount \nmean \nstd \nmin \nmax \nUNITED STATES \nOutput GAP \n215 \n-1.182651 \n2.345025 \n-10.990000 \n3.980000 \nNBER Indicator \n215 \n0.134884 \n0.342397 \n0.000000 \n1.000000 \nUnemployment rate \n215 \n6.091907 \n1.730741 \n3.400000 \n13.070000 \nCapacity utilization index \n215 \n80.094558 \n4.292762 \n65.970000 \n88.530000 \nBRAZIL \nOutput GAP \n34 \n-1.026471 \n2.316385 \n-4.900000 \n2.400000 \nCODACE Indicator \n34 \n0.411765 \n0.499554 \n0.000000 \n1.000000 \nCapacity utilization index \n34 \n76.961765 \n4.366775 \n61.400000 \n82.730000 \nTotal Factor Productivity \n34 \n0.544765 \n0.037840 \n0.506000 \n0.608000 \nRegistered Employees Index \n34 \n10.074118 \n2.557549 \n6.470000 \n14.270000 \n \n \n \n79 \n \nTable 9 - Dataset for RIDE: summary \nCode \nSeries \nRange \nSource \ngdp \nGross domestic product - Target \n1996:M1 to 2021:M3 IBGE \nepeRes \nElectricity consumption: household 1996:M1 to 2021:M3 Empresa de Pesquisa Energetica \nepeCom \nElectricity consumption: commerce 1996:M1 to 2021:M3 Empresa de Pesquisa Energetica \nepeInd \nElectricity consumption: industry \n1996:M1 to 2021:M3 Empresa de Pesquisa Energetica \nexpQ \nExport goods: quantum \n1996:M1 to 2021:M3 Funcex \nimpQ \nImport good: quantum \n1996:M1 to 2021:M3 Funcex \nexpBasic \nExport goods: raw material \n1996:M1 to 2021:M3 Funcex \nexpSemi \nExport goods: semi \n1996:M1 to 2021:M3 Funcex \nexpManuf \nExport goods: industrial \n1996:M1 to 2021:M3 Funcex \nexpAgroDolar Export goods: agro \n1996:M1 to 2021:M3 Funcex \nanfaveaProd \nVehicles production \n1996:M1 to 2021:M3 IPEA data \nanfaveaLic \nVehicles sales \n1996:M1 to 2021:M3 IPEA data \ncagedTransf \nFormal jobs: industry \n1996:M1 to 2021:M3 Ministry of Labor \ncagedConstr \nFormal jobs: construction \n1996:M1 to 2021:M3 Ministry of Labor \ncagedCom \nFormal jobs: commerce \n1996:M1 to 2021:M3 Ministry of Labor \ncagedServ \nFormal jobs: services \n1996:M1 to 2021:M3 Ministry of Labor \nipa \nIndustrial index prices \n1996:M1 to 2021:M3 IPEA data \ncambio \nExchange rate \n1996:M1 to 2021:M3 IPEA data \npoup \nSavings \n1996:M1 to 2021:M3 BCB \nm1 \nM1 money supply \n1996:M1 to 2021:M3 BCB \nm2 \nM2 money supply \n1996:M1 to 2021:M3 BCB \nibov \nStock market \n1996:M1 to 2021:M3 IPEA data \nembi \nSovereign risk \n1996:M1 to 2021:M3 IPEA data \nRecRFBIPCA Taxes \n1996:M1 to 2021:M3 Receita Federal do Brasil \n \n \n \n \n \n80 \n \nTable 10 - Dataset for RIDE: level descriptive statistics \nSeries \ncount \nmean \nstd \nmin \nmax \nGross domestic product \n294 \n4.932874 \n0.190581 \n4.573057 \n5.184117 \nElectricity: household \n294 \n9.011303 \n0.248722 \n8.532279 \n9.465525 \nElectricity: commerce \n294 \n8.535415 \n0.315680 \n7.881182 \n9.012377 \nElectricity: industry \n294 \n9.456611 \n0.160437 \n9.077152 \n9.673193 \nExport goods: quantum \n294 \n4.433535 \n0.384342 \n3.443938 \n4.974524 \nImport good: quantum \n294 \n4.735525 \n0.382289 \n3.694116 \n5.403533 \nExport goods: raw material \n294 \n4.544279 \n0.605518 \n2.989211 \n5.560105 \nExport goods: semi \n294 \n4.510472 \n0.294411 \n3.706719 \n4.994912 \nExport goods: industrial \n294 \n4.273739 \n0.306697 \n3.439135 \n4.718231 \nExport goods: agro \n294 \n6.975650 \n0.979179 \n4.695358 \n8.618163 \nVehicles production \n294 \n12.148554 \n0.446944 \n7.521318 \n12.772318 \nVehicles sales \n294 \n12.108557 \n0.386983 \n10.669234 \n12.948200 \nFormal jobs: industry \n294 \n8.810780 \n0.121875 \n8.586517 \n9.002094 \nFormal jobs: construction \n294 \n7.724365 \n0.207142 \n7.452128 \n8.112908 \nFormal jobs: commerce \n294 \n8.839708 \n0.265457 \n8.452849 \n9.162869 \nFormal jobs: services \n294 \n9.547007 \n0.230508 \n9.233353 \n9.829924 \nIndustrial index prices \n294 \n5.635644 \n0.554824 \n4.603969 \n6.465277 \nExchange rate \n294 \n0.80994 \n0.388919 \n-0.021632 \n1.700375 \nSavings \n294 \n19.362327 \n0.796986 \n17.978074 \n20.669209 \nM1 money supply \n294 \n18.797382 \n0.822260 \n16.940591 \n20.057678 \nM2 money supply \n294 \n20.502025 \n0.932023 \n18.964322 \n22.002361 \nStock market \n294 \n10.480367 \n0.752160 \n8.762259 \n11.661035 \nSovereign risk \n294 \n5.940150 \n0.622108 \n4.955827 \n7.781139 \nTaxes \n294 \n11.398053 \n0.360862 \n10.569928 \n12.011231 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n81 \n \n \nTable 11 - Dataset for RIDE: year over year descriptive statistics \n \nSeries \ncount \nmean \nstd \nmin \nmax \nGross domestic product \n282 \n0.021008 \n0.032664 \n-0.114355 \n0.092091 \nElectricity: household \n282 \n0.033728 \n0.065733 \n-0.272383 \n0.0196253 \nElectricity: commerce \n282 \n0.041736 \n0.067517 \n-0.252346 \n0.230900 \nElectricity: industry \n282 \n0.016326 \n0.067701 \n-0.169337 \n0.242843 \nExport goods: quantum \n282 \n0.056994 \n0.116426 \n-0.257154 \n0.436872 \nImport good: quantum \n282 \n0.054362 \n0.179768 \n-0.319535 \n0.904999 \nExport goods: raw material \n282 \n0.097804 \n0.174419 \n-0.393541 \n0.891475 \nExport goods: semi \n282 \n0.044599 \n0.144330 \n-0.254536 \n0.568317 \nExport goods: industrial \n282 \n0.040478 \n0.154273 \n-0.382852 \n0.493096 \nExport goods: agro \n282 \n0.171863 \n0.367463 \n-0.510009 \n1.630786 \nVehicles production \n282 \n0.037177 \n0.236936 \n-0.993097 \n1.634799 \nVehicles sales \n282 \n0.034511 \n0.228297 \n-0.759697 \n1.617758 \nFormal jobs: industry \n282 \n0.006597 \n0.036823 \n-0.085214 \n0.086855 \nFormal jobs: construction \n282 \n0.005617 \n0.070574 \n-0.150847 \n0.147416 \nFormal jobs: commerce \n282 \n0.028729 \n0.028792 \n-0.033498 \n0.072822 \nFormal jobs: services \n282 \n0.024881 \n0.027269 \n-0.024992 \n0.108166 \nIndustrial index prices \n282 \n0.081762 \n0.073732 \n-0.051507 \n0.403623 \nExchange rate \n282 \n0.086137 \n0.216944 \n-0.254942 \n0.826610 \nSavings \n282 \n0.118122 \n0.081935 \n-0.25782 \n0.390536 \nM1 money supply \n282 \n0.135416 \n0.126388 \n-0.052146 \n0.695404 \nM2 money supply \n282 \n0.132639 \n0.082459 \n-0.003356 \n0.430383 \nStock market \n282 \n0.108480 \n0.371873 \n-0.881164 \n1.414691 \nSovereign risk \n282 \n0.059905 \n0.499091 \n-0.708559 \n2.683333 \nTaxes \n282 \n0.044488 \n0.091628 \n-0.325202 \n0.367138 \n \n \n \n82 \n \nAPPENDIX B - SELECTED CODES \n \nneg, pos = np.bincount(raw_data['NBER']) \ntotal = neg + pos \n# set initial bias \ninitial_bias = np.log([pos/neg]) \n# Use a utility from sklearn to split out dataset \ntrain_df, test_df = train_test_split(log_1df, test_size=0.4, random_state=0, \nshuffle=False) \ntrain_df, val_df = train_test_split(train_1df, test_size=0.3, random_state=0, \nshuffle=False) \n# Form np arrays of labels and features \ntrain_labels = np.array(train_df.pop('NBER')) \nval_labels = np.array(val_df.pop('NBER')) \ntest_labels = np.array(test_df.pop('NBER')) \n \ntrain_features = np.array(train_df) \nval_features = np.array(val_df) \ntest_features = np.array(test_df) \nscaler = StandardScaler() \ntrain_features = scaler.fit_transform(train_features) \nval_features = scaler.transform(val_features) \ntest_features = scaler.transform(test_features) \nMETRICS = [ \n    keras.metrics.TruePositives(name='tp'), \n    keras.metrics.FalsePositives(name='fp'), \n    keras.metrics.TrueNegatives(name='tn'), \n    keras.metrics.FalseNegatives(name='fn'),  \n    keras.metrics.BinaryAccuracy(name='accuracy'), \n    keras.metrics.Precision(name='precision'), \n    keras.metrics.Recall(name='recall'), \n    keras.metrics.AUC(name='auc'), \n] \n \ndef make_model(hp, metrics = METRICS, output_bias = initial_bias):  \n    output_bias = tf.keras.initializers.Constant(output_bias) \n \n    model = keras.Sequential() \n \n    # Tune the number of units in the layers \n    # Choose an optimal value between 16-256 \n    hp_units = hp.Int('units', min_value = 16, max_value = 256, step = 16) \n \n    model.add(keras.layers.LSTM(units = hp_units, input_shape = \n(1,train_features.shape[-1],), dropout = 0.3)) #LSTM layer \n \n \n \n \n83 \n \n    model.add(keras.layers.Dense(units = hp_units, activation = 'relu')) \n#Dense1 \n    model.add(keras.layers.Dense(units = hp_units, activation = 'relu')) \n#Dense2 \n    model.add(keras.layers.Dense(units = hp_units, activation = 'relu')) \n#Dense3 \n    model.add(keras.layers.Dense(units = hp_units, activation = 'relu')) \n#Dense4 \n \n    model.add(keras.layers.Dropout(0.5)) # To prevent overfiting \n \n    model.add(keras.layers.Dense(1, activation='sigmoid', \nbias_initializer=output_bias)) # Output layer \n \n    # Tune the learning rate for the optimizer  \n    # Choose an optimal value from 0.01, 0.001, or 0.0001 \n    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-\n4]) \n \n    model.compile(optimizer = keras.optimizers.Adam(learning_rate = \nhp_learning_rate), \n        loss = keras.losses.BinaryCrossentropy(),  \n        metrics = metrics) \n \n    return model \ntuner = kt.Hyperband(make_model, \n                    kt.Objective('val_auc', direction='max'), #maximizes area \nunder the ROC curve \n                    max_epochs = 10, \n                    factor = 3,) \n \ntuner.search(train_features, train_labels, \n            epochs=50, \n            validation_data=(val_features, val_labels), callbacks = \n[ClearTrainingOutput()]) \n \n#Get the optimal hyperparameters \nbest_hps = tuner.get_best_hyperparameters(num_trials = 1)[0] \n \n#Retrain the model with the optimal hyperparameters \nmodel = tuner.hypermodel.build(best_hps) \nbaseline_history = model.fit(train_features, train_labels, \n                            epochs=50, \n                            validation_data=(val_features, val_labels)) \n \n \n \n84 \n \nREFERENCES \n \n \nABADI, M., AGARWAL, A., BARHAM, P., BREVDO, E., CHEN, Z., CITRO, C., \nCORRADO, G.S., DAVIS, A., DEAN, J., DEVIN, M., GHEMAWAT, S., GOODFELLOW, I., \nHARP, A., IRVING, G., ISARD, M., JIA, Y., JOZEFOWICZ, R., KAISER, L., KUDLUR, M., \nLEVENBERG, J., MANE, D., MONGA, R., MOORE, S., MURRAY, D., OLAH, C., \nSCHUSTER, M., SHLENS, J., STEINER, B., SUTSKEVER, I., TALWAR, K., TUCKER, P., \nVANHOUCKE, V., VASUDEVAN, V., VIEGAS, F., VINYALS, O., WARDEN, P., \nWATTENBERG, M., WICKE, M., YU, Y., ZHENG, X., 2015. TensorFlow: Large-scale \nmachine learning on heterogeneous systems. URL: https://www.tensorflow.org/. software \navailable from tensorflow.org. \n \nABBERGER, K., GRAFF, M., CAMPELO, A.J., GOUVEIA, A.C.L., MULLER, O., STURM, \nJ.E., 2020. The Global Economic Barometers: Composite indicators for the world economy. \nResearch Working Paper KOF WP 471-20. KOF Swiss Economic Institute. \n \nANGELINI, E., HENRY, J., MARCELLINO, M., 2006. Interpolation and backdating with a \nlarge information set. Journal of Economic Dynamics and Control 30, 2693-2724. \n \nAPEL, M., JANSSON, P., 1999. A theory-consistent system approach for estimating potential \noutput and the nairu. Economics Letters, 271-275. \n \nARNOLD, A., NALLAPATI, R., COHEN, W., 2007. A comparative study of methods for \ntransductive transfer learning, in: Proceedings of the 7th IEEE International Conference on Data \nMining Workshops, pp. 77-82. \n \nBANCO CENTRAL DO BRASIL, 2016. Inflation report 18, 23-27. \n \nBANCO CENTRAL DO BRASIL, 2020a. Inflation report 22, 57-62. \n \nBANCO CENTRAL DO BRASIL, 2020b. Inflation report 22, 56. \n \nBENGIO, Y., 2009. Learning Deep Architectures for AI. Foundations and Trends in Machine \nLearning 2, 1-127. \n \nBENGIO, Y., 2012. Deep learning of representations for unsupervised and transfer learning, in: \nGuyon, I., Dror, G., Lemaire, V., Taylor, G., Silver, D. (Eds.), Proceedings of ICML Workshop \non Unsupervised and Transfer Learning, JMLR Workshop and Conference Proceedings, \nBellevue, Washington, USA. pp. 17-36. \n \nBENGIO, Y., COURVILLE, A., VINCENT, P., 2013. Representation learning: A review and \nnew perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence 35, 1798-\n1828. \n \n \n \n85 \n \nBERGE, T.J., 2013. Predicting recessions with leading indicators: model averaging and selection \nover the business cycle. Research Working Paper RWP 13-05. Federal Reserve Bank of Kansas \nCity. \n \nBEVERIDGE, S., NELSON, C.R., 1981. A new approach to the decomposition of economic \ntime series into permanent and transitory components with particular attention to the \nmeasurement of the business cycle. Journal of Monetary Economics, 151-174. \n \nBISHOP, C.M., 1994. Neural Networks for Pattern Recognition. Oxford University Press. \n \nBLAKE, A., 1999. An Artificial Neural Network System of Leading Indicators. National \nInstitute of Economic and Social Research (NIESR) Discussion Papers 144. National Institute of \nEconomic and Social Research. \n \nBLANCHARD, O., 2018. On the future of macroeconomic models, in: Vines, D., Wills, S. \n(Eds.), Rebuilding macroeconomic theory. Oxford Review of Economic Policy, pp. 43-54. \n \nBLANCHARD, O., QUAH, D., 1989. The dynamic effects of aggregate demand and aggregate \nsupply disturbances. American Economic Review, 653-658. \n \nBURNS, A.F., MITCHELL, W.C., 1946. Measuring Business Cycles. National Bureau of \nEconomic Research, Inc. \n \nCAMACHO, M., PEREZ-QUIROS, G., PONCELA, P., 2018. Markov-switching dynamic factor \nmodels in real time. International Journal of Forecasting 34, 598-611. \n \nCARRIERE-SWALLOW, Y., LABBE, F., 2010.  Nowcasting with google trends in an emerging \nmarket. Working Papers of the Central Bank of Chile. \n \nCHAUVET, M., 1998. An econometric characterization of business cycle dynamics with factor \nstructure and regime switching. International Economic Review 39, 969-96. \n \nCHAUVET, M., 2001. A monthly indicator of Brazilian GDP. Brazilian Review of Econometrics \n21, 1-47. \n \nCHAUVET, M., GUIMARAES, R., 2021. Transfer Learning for Business Cycle Identification. \nWorking Paper Series 545. Banco Central do Brasil. \n \nCHAUVET, M., HAMILTON, J.D., 2005. Dating Business Cycle Turning Points. NBER \nWorking Papers. National Bureau of Economic Research, Inc. \n \nCHAUVET, M., PIGER, J., 2008. A comparison of the real-time performance of business cycle \ndating methods. Journal of Business Economic Statistics 26, 42-49. \n \n \n \n \n86 \n \nCHAUVET, M., YU, C., 2006. International business cycles: G7 and OECD countries. \nEconomic Review 91, 43-54. \n \nCHOW, G., LIN, A., 1971. Best linear unbiased interpolation, distribution and extrapolation of \ntime series by related series. The Review of Economics and Statistics 53, 372-375. \n \nCHRISTIANO, L.J., 2012. Christopher A. Sims and vector autoregressions. The Scandinavian \nJournal of Economics 4, 1-23. \n \nCOOK, T.R., HALL, A., 2017. Macroeconomic Indicator Forecasting with Deep Neural \nNetworks. Research Working Paper RWP 17-11. Federal Reserve Bank of Kansas City. \n \nCUBA-BORDA, P., MECHANICK, A., RAFFO, A., 2018. Monitoring the World Economy: A \nGlobal Conditions Index. IFDP Notes. Board of Governors of the Federal Reserve System (U.S.). \n \nDAI, W., YANG, Q., XUE, G., YU, Y., 2007a. Boosting for transfer learning, in: Proceedings of \nthe 24th International Conference on Machine Learning, pp. 193-200. \n \nDai, W., Yang, Q., Xue, G., Yu, Y., 2007b. Transferring naive bayes classi ers for text classi \ncation, in: Proceedings of the 22nd AAAI Conference on Arti cial Intelligence, pp. 540{545. \n \nD’AURIA, F., DENIS, C., HAVIK, K., MORROW, K.M., PLANAS, C., RACIBORSKI, R., \nROGER, W., ROSSI, A., 2010. The production function methodology for calculating potential \ngrowth rates and output gaps. European Economy - Economic Papers 2008 - 2015 420. \nDirectorate General Economic and Financial Affairs (DG ECFIN), European Commission. \n \nDENG, L., YU, D., 2014. Deep Learning: Methods and Applications. Technical Report. \nMicrosoft. \n \nDUCHI, J.C., HAZAN, E., SINGER, Y., 2011. Adaptive sub gradient methods for online \nlearning and stochastic optimization. Journal of Machine Learning Research 12, 2121-2159. \n \nDUTTA, S., SHEKHAR, S., 1988. Bond rating: A non-conservative application of neural \nnetworks, in: IEEE Int Conf on Neural Networks, IEEE. pp. 443-450. \n \nESTRELLA, A., MISHKIN, F., 1998. Predicting U.S. recessions: Financial variables as leading \nindicators. The Review of Economics and Statistics 80, 45-61. \n \nESTRELLA, A., RODRIGUES, A.P., SCHICH, S., 2000. How stable is the predictive power of \nthe yield curve? Evidence from Germany and the United States. Staff Reports 113. Federal \nReserve Bank of New York. \n \nFOSSATI, S., 2016. Dating US business cycles with macro factors. Studies in Nonlinear \nDynamics & Econometrics 20, 529-547. \n \n \n \n \n87 \n \nFRIEDMAN, M., 1962. The interpolation of time series by related series. Journal of the \nAmerican Statistical Association 57, 729-757. \n \nGARBELLANO, J., 2016. Nowcasting recessions with machine learning: new tools for \npredicting the business cycle, in: Thesis. \n \nGARCIA, M.G.P., MEDEIROS, M.C., VASCONCELOS, G.F.R., 2017. Real-time inflation \nforecasting with high-dimensional models: The case of Brazil. International Journal of \nForecasting 33, 679-693. \n \nGERLACH, S., SMETS, F., 1999. Output gaps and monetary policy in the emu area. European \nEconomic Review, 801-812. \n \nGIANNONE, D., BANBURA, M., MODUGNO, M., 2013. Now-casting and the real-time data \now. ECB Working Paper. \n \nGIUSTO, A., PIGER, J., 2017. Identifying business cycle turning points in real time with vector \nquantization. International Journal of Forecasting 33, 174-184. \n \nGOODFELLOW, I., BENGIO, Y., COURVILLE, A., 2016. Deep Learning. MIT Press. \n \nGOOGLE, 2021. Understand the limitations of your dataset and model. URL: https://ai.google/ \nresponsibilities/responsible-ai-practices/. \n \nGU, S., KELLY, B., XIU, D., 2018. Empirical asset pricing via machine learning. Chicago Booth \nResearch Paper 18. \n \nHAMILTON, J.D., 1989. A New Approach to the Economic Analysis of Nonstationary Time \nSeries and the Business Cycle. Econometrica 57, 357-384. \n \nHAMILTON, J.D., 2018. Why you should never use the Hodrick-Prescott filter. The Review of \nEconomics and Statistics, 831-843. \n \nHARDING, D., PAGAN, A., 2005. A suggested framework for classifying the modes of cycle \nresearch. Journal of Applied Econometrics 20, 151-159. \n \nHARVEY, A.C., JAEGER, A., 1993. Detrending, stylized facts and the business cycle. Journal of \nApplied Econometrics, 231-247. \n \nHERBRICH, R., KEILBACH, M., GRAEPEL, T., BOLLMANN-SDORRA, P., OBERMAYER, \nK., 1998. Neural networks in economics: Background, applications, and new developments, in: \nAdvances in Computational Economics: Computational Techniques for Modelling Learning in \nEconomics, Kluwer Academics. pp. 169-196. \n \n \n \n \n \n \n88 \n \nHINDS, S., RIMMINGTON, \nL., DANCE, H., \nGILLHAM, \nJ., \nSENTANCE, A., \nHAWKSWORTH, J., 2017. A machine learning approach to estimating current GDP growth. \nReport. PriceWaterhouseCoopers. \n \nHODRICK, R.J., PRESCOTT, E., 1981. Post-War U.S. Business Cycles: An Empirical \nInvestigation. Discussion Papers 451. Northwestern University, Center for Mathematical Studies \nin Economics and Management Science. \n \nHOPTRO, R.G., BRAMSON, M.J., HALL, T.J., 1991. Forecasting economic turning points with \nneural nets, in: IJCNN-91-Seattle International Joint Conference on Neural Networks, pp. 347-\n352. \n \nHOWARD, A.G., ZHU, M., CHEN, B., KALENICHENKO, D., WANG, W., WEYAND, T., \nANDREETTO, M., ADAM, H., 2017. Mobilenets: Efficient convolutional neural networks for \nmobile vision applications. arXiv:1704.04861. \n \nINSTITUTO BRASILEIRO DE ECONOMIA, 2015. Metodologia do indicador preliminar, \nFundacao Getulio Vargas. \n \nISSLER, J.V., NOTINI, H.H., 2014. Estimating Brazilian monthly GDP: a state-space approach. \nEnsaios Economicos 757, 1-24. \n \nISSLER, J.V., VAHID, F., 2006. The missing link: using the NBER recession indicator to \nconstruct coincident and leading indices of economic activity. Journal of Econometrics 132, 281-\n303. \n \nJACKSON, B., REGE, M., 2019. Machine learning for classification of economic recessions, in \n2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science \n(IRI), pp. 31-38. \n \nJAMES, A., ABU-MOSTAFA, Y.S., QIAO, X., 2019. Nowcasting recessions using the SVM \nmachine learning algorithm. arXiv:1903.03202. \n \nJEAN, N., BURKE, M., XIE, M., DAVIS, W.M., LOBELL, D.B., ERMON, S., 2016. \nCombining satellite imagery and machine learning to predict poverty. Science 353, 790-794. \n \nJEBARA, T., 2004. Multi-task feature and kernel selection for SVMS, in: Proceedings of the \n21st International Conference on Machine Learning. \n \nKAUPPI, H., SAIKKONEN, P., 2008. Predicting U.S. recessions with dynamic binary response \nmodels. The Review of Economics and Statistics 90, 777-791. \n \nKINGMA, D.P., BA, J., 2017. Adam: A method for stochastic optimization. arXiv:1412.6980. \n \nKLIESEN, K.L., MCCRACKEN, M.W., 2016. Tracking the U.S. economy with nowcasts. The \nRegional Economist. Federal Reserve Bank of St. Louis. \n \n \n \n \n89 \n \nKLINKENBERG, R., 2003. Predicting Phases in Business Cycles Under Concept Drift. Working \nPaper Series. University of Dortmund, Department of Computer Science. \n \nKOHZADI, N., BOYD, M.S., KAASTRA, I., KERMANSHAHI, B.S., SCUSE, D., 1995. Neural \nnetworks for forecasting: An introduction. Canadian Journal of Agricultural Economics/Revue \ncanadienne d’agroeconomie 43, 463-474. \n \nKOSKE, I., PAIN, N., 2008. The Usefulness of Output Gaps for Policy Analysis. OECD \nEconomics Department Working Papers 621. OECD Publishing. \n \nKOUW, W., LOOG, M., 2018. An introduction to domain adaptation and transfer learning. \nCoRR abs/1812.11806. URL: http://arxiv.org/abs/1812.11806. \n \nKUAN, C.M., WHITE, H., 1991. Artificial neural networks: an econometric perspective. \nEconometric Reviews 13, 1-91. \n \nKUTTNER, K.N., 1994. Estimating potential output as a latent variable. Journal of Business and \nEconomic Statistics, 361-368. \n \nLECUN, Y., BENGIO, Y., HINTON, G., 2015. Deep learning. Nature 521, 436-444. \n \nLI, L., JAMIESON, K., DESALVO, G., ROSTAMIZADEH, A., TALWALKAR, A., 2018. \nHyperband: A novel bandit-based approach to hyperparameter optimization. arXiv:1603.06560. \n \nMA, J.B., 2015. Applications of machine learning in forecasting recessions: boosting United \nStates and Japan. Ph.D. thesis. Princeton. \n \nMAKRIDAKIS, S., 2017. The forthcoming artificial intelligence (AI) revolution: Its impact on \nsociety and firms. Futures 90, 46-60. \n \nMAKRIDAKIS, S., SPILIOTIS, E., ASSIMAKOPOULOS, V., 2018. Statistical and machine \nlearning forecasting methods: Concerns and ways forward. PLoS ONE 13. \n \nMAZZI, G.L., PROIETTI, T., 2015. Multivariate temporal disaggregation. Eurostat Handbook on \nRapid Estimates 13, 729-757. \n \nMIHALKOVA, L., HUYNH, T., MOONEY, R.J., 2007. Mapping and revising Markov logic \nnetworks for transfer learning, in: Proceedings of the 22nd AAAI Conference on Artificial \nIntelligence, pp. 608-614. \n \nMORIK, K., RUPING, S., 2002. A multstrategy approach to the classification of phases in \nbusiness cycles, in: Elomaa, T., Mannila, H., Toivonen, H. (Eds.), Machine Learning: ECML \n2002, Springer Berlin Heidelberg. pp. 307-318. \n \nNASR, G.E., DIBEH, G., ACHKAR, A., 2007. Predicting business cycle turning points with \nneural networks in an information-poor economy, in: SCSC. \n \n \n \n \n90 \n \nNG, A., 2021. Machine Learning Engineering for Production (MLOps) Specialization Course 1, \nin: www.deeplearning.ai. \n \nNYMAN, R., ORMEROD, P., 2017. Predicting economic recessions using machine learning \nalgorithms. arXiv:1701.01428. \n \nOKUN, A., 1962. Potential GNP: Its measurement and significance, in: Proceedings of the \nBusiness and Economic Statistics Section of the American Statistical Association. \n \nPAN, S.J., TSANG, I.W., KWOK, J.T., YANG, Q., 2011. Domain adaptation via transfer \ncomponent analysis. IEEE Transactions on Neural Networks 22, 199-210. \n \nPAN, S.J., YANG, Q., 2010. A survey on transfer learning. IEEE Transactions on Knowledge \nand Data Engineering 22, 1345-1359. \n \nPAVÍA-MIRALLES, J., 2010. A survey of methods to interpolate, distribute and extrapolate time \nseries. Journal of Service Science and Management, 449-463. \n \nPIGER, J., 2020. Turning points and classification, in: Fuleky, P. (Ed.), Macroeconomic \nForecasting in the Era of Big Data. Springer International Publishing, pp. 585-624. \n \nPORTUGAL, M.S., 1995. Neural networks versus time series methods: a forecasting exercise. \nRevista Brasileira de Economia 49, 611-629. \n \nPROIETTI, T., MUSSO, A., WESTERMANN, T., 2007. Estimating potential output and the \noutput gap for the euro area: a model-based production function approach. Empirical Economics, \n85-113. \n \nQI, M.H., 2001. Predicting US recessions with leading indicators via neural network models. \nPh.D. Thesis. Princeton. \n \nRAFINOT, T., BENOIT, S., 2019. Investing Through Economic Cycles with Ensemble Machine \nLearning Algorithms. Working Papers. HAL. \n \nRUCKERT, U., KRAMER, S., 2008. Kernel-based inductive transfer, in: Machine Learning and \nKnowledge Discovery in Databases, ser. Lecture Notes in Computer Science. European \nConference, ECML/PKDD 2008, pp. 220-233. \n \nRUDEBUSCH, G., WILLIAMS, J., 2009. Forecasting recessions: The puzzle of the enduring \npower of the yield curve. Journal of Business Economic Statistics 27, 492-503. \n \nSCHWARTZ, I.E., 1992. Where neural networks are already at work, Business Week. pp. 136-\n137. \n \nSHACKLETON, R., 2018. Estimating and Projecting Potential Output Using CBO’s Forecasting \nGrowth Model. Working Paper 2018-03. Congressional Budget Office. \n \n \n \n \n91 \n \nSIMS, C.A., 1980. Macroeconomics and reality. Econometrica 48, 1-48. \n \nSOYBILGEN, B., 2018. Identifying US business cycle regimes using dynamic factors and neural \nnetwork models. MPRA Paper. University Library of Munich, Germany. \n \nST-AMANT, P., VAN NORDEN, S., 1997. Measurement of the Output Gap: A Discussion of \nRecent Research at the Bank of Canada. Technical Report 79. Bank of Canada. \n \nSTERGIOU, C., SIGANOS, D., 2011. Neural networks. Technical Report. Imperial College \nLondon. \n \nSTOCK, J., WATSON, M., 2011. Dynamic Factor Models. Oxford University Press, Oxford. \n \nSTOCK, J.H., WATSON, M.W., 2014. Estimating turning points using large data sets. Journal of \nEconometrics 178, 368-381. \n \nSWANSON, N., WHITE, H., 1997. Forecasting economic time series using flexible versus fixed \nspecification and linear versus nonlinear econometric models. International Journal of \nForecasting 13, 439{461. \n \nTAL, B., NAZARETH, L., 1995. Artificial Intelligence and Economic Forecasting. Canadian \nBusiness Economics. Canadian Imperial Bank of Commerce. \n \nTIFFIN, A., 2016. Seeing in the dark: a machine-learning approach to nowcasting in Lebanon. \nIMF Working Paper. \n \nTKACZ, G., HU, S., 1999. Forecasting GDP Growth Using Artificial Neural Networks. Staff \nWorking Papers 99-3. Bank of Canada. \n \nTORREY, L., SHAVLIK, J., 2009. Transfer learning, in: Olivas, E.S., Guerrero, J.D.M., Sober, \nM.M., Benedito, J.R.M., Lopez, A. (Eds.), Handbook Of Research On Machine Learning \nApplications and Trends: Algorithms, Methods and Techniques (2 Volumes). Chapter 11. \n \nVARIAN, H.R., 2014. Big data: new tricks for econometrics. Journal of Economic Perspectives \n28, 3-28. \n \nVARIAN, H.R., CHOI, H., 2009. Predicting the present with google trends, in: http:// \ngoogleresearch.blogspot.com/2009/04/predicting-present-with-google-trends.html. \n \nVINES, D., WILLS, S., 2018. The rebuilding macroeconomic theory project: an analytical \nassessment, in: Vines, D., Wills, S. (Eds.), Rebuilding macroeconomic theory. Oxford Review of \nEconomic Policy, pp. 1-42. \n \nVINES, D., WILLS, S., (eds.), 2018. Rebuilding macroeconomic theory. Volume 34. Oxford \nReview of Economic Policy. \n \n \n \n \n92 \n \nWEISS, K., KHOSHGOFTAAR, T.M., WANG, D., 2016. A survey of transfer learning. Journal \nof Big Data 3, 1-40. \n \nWHITE, H., 1988. Economic prediction using neural networks: the case of ibm daily stock \nreturns, in: IEEE 1988 International Conference on Neural Networks, pp. 451-458. \n \nWORLD BANK, 2009. System of national accounts 2008 (english), in: http://documents. \nworldbank.org/curated/en/417501468164641001/System-of-national-accounts-2008. \n \nYOSINSKI, J., CLUNE, J., BENGIO, Y., LIPSON, H., 2014.   How transferable are features in \ndeep neural networks? in:   Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., \nWeinberger, K.Q.  (Eds.), Advances in Neural Information Processing Systems, pp.  3320-3328. \n \nZHANG, C., BENGIO, S., HARDT, M., RECHT, B., VINYALS, O., 2016. Understanding deep \nlearning \nrequires \nrethinking \ngeneralization. \nCoRR \nabs/1611.03530. \nURL: \nhttp://arxiv.org/abs/1611. 03530. \n \n",
  "categories": [
    "econ.EM",
    "cs.LG"
  ],
  "published": "2022-01-31",
  "updated": "2022-01-31"
}