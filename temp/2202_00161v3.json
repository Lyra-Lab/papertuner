{
  "id": "http://arxiv.org/abs/2202.00161v3",
  "title": "CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery",
  "authors": [
    "Michael Laskin",
    "Hao Liu",
    "Xue Bin Peng",
    "Denis Yarats",
    "Aravind Rajeswaran",
    "Pieter Abbeel"
  ],
  "abstract": "We introduce Contrastive Intrinsic Control (CIC), an algorithm for\nunsupervised skill discovery that maximizes the mutual information between\nstate-transitions and latent skill vectors. CIC utilizes contrastive learning\nbetween state-transitions and skills to learn behavior embeddings and maximizes\nthe entropy of these embeddings as an intrinsic reward to encourage behavioral\ndiversity. We evaluate our algorithm on the Unsupervised Reinforcement Learning\nBenchmark, which consists of a long reward-free pre-training phase followed by\na short adaptation phase to downstream tasks with extrinsic rewards. CIC\nsubstantially improves over prior methods in terms of adaptation efficiency,\noutperforming prior unsupervised skill discovery methods by 1.79x and the next\nleading overall exploration algorithm by 1.18x.",
  "text": "CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nMichael Laskin 1 Hao Liu 1 Xue Bin Peng 1 Denis Yarats 2 3 Aravind Rajeswaran 3 Pieter Abbeel 1 4\nAbstract\nWe introduce Contrastive Intrinsic Control (CIC),\nan algorithm for unsupervised skill discovery that\nmaximizes the mutual information between state-\ntransitions and latent skill vectors. CIC utilizes\ncontrastive learning between state-transitions and\nskills to learn behavior embeddings and maxi-\nmizes the entropy of these embeddings as an in-\ntrinsic reward to encourage behavioral diversity.\nWe evaluate our algorithm on the Unsupervised\nReinforcement Learning Benchmark, which con-\nsists of a long reward-free pre-training phase fol-\nlowed by a short adaptation phase to downstream\ntasks with extrinsic rewards. CIC substantially\nimproves over prior methods in terms of adapta-\ntion efﬁciency, outperforming prior unsupervised\nskill discovery methods by 1.79× and the next\nleading overall exploration algorithm by 1.18×.1\n1. Introduction\nDeep Reinforcement Learning (RL) is a powerful approach\ntoward solving complex control tasks in the presence of\nextrinsic rewards. Successful applications include playing\nvideo games from pixels (Mnih et al., 2015), mastering\nthe game of Go (Silver et al., 2017; 2018), robotic locomo-\ntion (Schulman et al., 2016; 2017; Peng et al., 2018) and\ndexterous manipulation (Rajeswaran et al., 2018; OpenAI,\n2018; 2019) policies. While effective, the above advances\nproduced agents that are unable to generalize to new down-\nstream tasks beyond the one they were trained to solve.\nHumans and animals on the other hand are able to acquire\nskills with minimal supervision and apply them to solve a\nvariety of downstream tasks. In this work, we seek to train\nagents that acquire skills without supervision with gener-\nalization capabilities by efﬁciently adapting these skills to\ndownstream tasks.\nOver the last few years, unsupervised RL has emerged as\n1UC Berkeley 2NYU 3MetaAI 4Covariant. Correspondence to:\nMichael Laskin <mlaskin@berkeley.edu>.\n1Project website and code:\nhttps://sites.google.\ncom/view/cicrl/\nz1\nz2\nz3\nFigure 1. This work deals with unsupervised skill discovery\nthrough mutual information maximization. We introduce Con-\ntrastive Intrinsic Control (CIC) – a new unsupervised RL algorithm\nthat explores and adapts more efﬁciently than prior methods.\na promising framework for developing RL agents that can\ngeneralize to new tasks. In the unsupervised RL setting,\nagents are ﬁrst pre-trained with self-supervised intrinsic re-\nwards and then ﬁnetuned to downstream tasks with extrinsic\nrewards. Unsupervised RL algorithms broadly fall into three\ncategories - knowledge-based, data-based, and competence-\nbased methods2. Knowledge-based methods maximize the\nerror or uncertainty of a predictive model (Pathak et al.,\n2017; 2019; Burda et al., 2019). Data-based methods max-\nimize the entropy of the agent’s visitation (Liu & Abbeel,\n2021a; Yarats et al., 2021b). Competence-based methods\nlearn skills that generate diverse behaviors (Eysenbach et al.,\n2019; Gregor et al., 2017). This work falls into the latter\ncategory of competence-based exploration methods.\nUnlike\nknowledge-based\nand\ndata-based\nalgorithms,\ncompetence-based algorithms simultaneously address both\nthe exploration challenge as well as distilling the gener-\nated experience in the form of reusable skills. This makes\nthem particularly appealing, since the resulting skill-based\npolicies (or skills themselves) can be ﬁnetuned to efﬁ-\nciently solve downstream tasks. While there are many self-\nsupervised objectives that can be utilized, our work falls into\n2These categories for exploration algorithms were introduced\nby Srinivas & Abbeel (2021) and inspired by Oudeyer et al. (2007).\narXiv:2202.00161v3  [cs.LG]  30 Mar 2022\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nLeap forward \nJog \nWalk left \nFlip upright\nNudge brick\nFigure 2. Qualitative visualizations of unsupervised skills discovered in Walker, Quadruped, and Jaco arm environments. The Walker\nlearns to balance and move, the Quadruped learns to ﬂip upright and walk, and the 6 DOF robotic arm learns how to move without locking.\nUnlike prior competence-based methods for continuous control which evaluate on OpenAI Gym (e.g. Eysenbach et al. (2019)), which\nreset the environment when the agent loses balance, CIC is able to learn skills in ﬁxed episode length environments which are much\nharder to explore (see Appendix J).\na family of methods that learns skills by maximizing the\nmutual information between visited states and latent skill\nvectors. Many earlier works have investigated optimizing\nsuch objectives (Eysenbach et al., 2019; Gregor et al., 2017;\nKwon, 2021; Sharma et al., 2020). However, competence-\nbased methods have been empirically challenging to train\nand have under-performed when compared to knowledge\nand data-based methods (Laskin et al., 2021).\nIn this work, we take a closer look at the challenges of\npre-training agents with competence-based algorithms. We\nintroduce Contrastive Intrinsic Control (CIC) – an explo-\nration algorithm that uses a new estimator for the mutual\ninformation objective. CIC combines particle estimation\nfor state entropy (Singh et al., 2003; Liu & Abbeel, 2021a)\nand noise contrastive estimation (Gutmann & Hyv¨arinen,\n2010) for the conditional entropy which enables it to both\ngenerate diverse behaviors (exploration) and discriminate\nhigh-dimensional continuous skills (exploitation). To the\nbest of our knowledge, CIC is the ﬁrst exploration algo-\nrithm to utilize noise contrastive estimation to discriminate\nbetween state transitions and latent skill vectors. Empiri-\ncally, we show that CIC adapts to downstream tasks more\nefﬁciently than prior exploration approaches on the Unsu-\npervised Reinforcement Learning Benchmark (URLB). CIC\nachieves 79% higher returns on downstream tasks than prior\ncompetence-based algorithms and 18% higher returns than\nthe next-best exploration algorithm overall.\n2. Background and Notation\nMarkov Decision Process: We operate under the assump-\ntion that our system is described by a Markov Decision\nProcess (MDP) (Sutton & Barto, 2018). An MDP consiss\nof the tuple (S, A, P, r, γ) which has states s ∈S, actions\na ∈A, transition dynamics p(s′|s, a) ∼P, a reward func-\ntion r, and a discount factor γ. In an MDP, at each timestep t,\nan agent observes the current state s, selects an action from\na policy a ∼π(·|s), and then observes the reward and next\nstate once it acts in the environment: r, s′ ∼env.step(a).\nNote that usually r refers to an extrinsic reward. However, in\nthis work we will ﬁrst be pre-training an agent with intrinsic\nrewards rint and ﬁnetuning on extrinsic rewards rext.\nFor convenience we also introduce the variable τ(s) which\nrefers to any function of the states s. For instance τ can be\na single state, a pair of states, or a sequence depending on\nthe algorithm. Our method uses τ = (s, s′) to encourage\ndiverse state transitions while other methods have different\nspeciﬁcations for τ. Importantly, τ does not denote a state-\naction trajectory, but is rather shorthand for any function\nof the states encountered by the agent. In addition to the\nstandard MDP notation, we will also be learning skills z ∈\nZ and our policy will be skill-conditioned a ∼π(·|s, z).\nUnsupervised Skill Discovery through Mutual Informa-\ntion Maximization: Most competence-based approaches\nto exploration maximize the mutual information between\nstates and skills. Our work and a large body of prior re-\nsearch (Eysenbach et al., 2019; Sharma et al., 2020; Gregor\net al., 2017; Achiam et al., 2018; Lee et al., 2019; Liu &\nAbbeel, 2021b) aims to maximize a mutual information\nobjective with the following general form:\nI(τ; z) = H(z) −H(z|τ) = H(τ) −H(τ|z)\n(1)\nCompetence-based algorithms use different choices for τ\nand can condition on additional information such as actions\nor starting states. For a full summary of competence-based\nalgorithms and their objectives see Table 2 in Appendix D.\nLower Bound Estimates of Mutual Information:\nThe\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nNoise Contrastive Loss\nquery\nkey\nk-NN \nReplay Buffer\nEntropy\nConditional Entropy\nIntrinsic Reward\nRepresentation Learning\nReplay Buffer\nFigure 3. Architecture illustrating the practical implementation of CIC . During a gradient update step, random τ = (s, s′) tuples are\nsampled from the replay buffer, then a particle estimator is used to compute the entropy and a noise contrastive loss to compute the\nconditional entropy. The contrastive loss is backpropagated through the entire architecture. The entropy and contrastive terms are then\nscaled and added to form the intrinsic reward. The RL agent is optimized with a DDPG (Lillicrap et al., 2016).\nmutual information I(s; z) is intractable to compute directly.\nSince we wish to maximize I(s; z), we can approximate this\nobjective by instead maximizing a lower bound estimate.\nMost known mutual information maximization algorithms\nuse the variational lower bound introduced in Barber &\nAgakov (2003):\nI(τ; z) = H(z) −H(z|τ) ≥H(z) + E[log q(z|τ)]\n(2)\nThe variational lower bound can be applied to both decom-\npositions of the mutual information. The design decisions\nof a competence-based algorithm therefore come down to (i)\nwhich decomposition of I(τ; z) to use, (ii) whether to use\ndiscrete or continuous skills, (iii) how to estimate H(z) or\nH(τ), and ﬁnally (iv) how to estimate H(z|τ) or H(τ|z).\n3. Motivation\nResults from the recent Unsupervised Reinforcement Learn-\ning Benchmark (URLB) (Laskin et al., 2021) show that\ncompetence-based approaches underperform relative to\nknowledge-based and data-based baselines on DeepMind\nControl (DMC). We argue that the underlying issue with cur-\nrent competence-based algorithms when deployed on harder\nexploration environments like DMC has to do with the cur-\nrently used estimators for I(τ; z) rather than the objective\nitself. To produce structured skills that lead to diverse behav-\niors, I(τ; z) estimators must (i) explicitly encourage diverse\nbehaviors and (ii) have the capacity to discriminate between\nhigh-dimensional continuous skills. Current approaches do\nnot satisfy both criteria.\nCompetence-base algorithms do not ensure diverse be-\nhaviors: Most of the best known competence-based ap-\nproaches (Eysenbach et al., 2019; Gregor et al., 2017;\nAchiam et al., 2018; Lee et al., 2019), optimize the ﬁrst\ndecomposition of the mutual information H(z) −H(z|τ).\nThe issue with this decomposition is that while it ensures\ndiversity of skill vectors it does not ensure diverse behav-\nior from the policy, meaning max H(z) does not imply\nmax H(τ). Of course, if H(z) −H(z|τ) is maximized and\nthe skill dimension is sufﬁciently large, then H(τ) will also\nbe maximized implicitly. Yet in practice, to learn an accurate\ndiscriminator q(z|τ), the above methods assume skill spaces\nthat are much smaller than the state space (see Table 2), and\nthus behavioral diversity may not be guaranteed. In con-\ntrast, the decomposition I(τ; z) = H(τ) −H(τ|z) ensures\ndiverse behaviors through the entropy term H(τ). Meth-\nods that utilize this decomposition include Liu & Abbeel\n(2021b); Sharma et al. (2020).\nWhy it is important to utilize high-dimensional skills: Once\na policy is capable of generating diverse behaviors, it is\nimportant that the discriminator can distill these behaviors\ninto distinct skills. If the set of behaviors outnumbers the\nset of skills, this will result in degenerate skills – when one\nskill maps to multiple different behaviors. It is therefore im-\nportant that the discriminator can accommodate continuous\nskills of sufﬁciently high dimension. Empirically, the dis-\ncriminators used in prior work utilize only low-dimensional\ncontinuous skill vectors. DIAYN (Eysenbach et al., 2019)\nutilized 16 dimensional skills, DADS (Sharma et al., 2020)\nutilizes continuous skills of dimension 2−5, while APS (Liu\n& Abbeel, 2021b), an algorithm that utilizes successor fea-\ntures (Barreto et al., 2016; Hansen et al., 2020) for the\ndiscriminator, is only capable of learning continuous skills\nwith dimension 10. We show how small skill spaces can\nlead to ineffective exploration in a simple gridworld setting\nin Appendix H and evidence that skill dimension affects\nperformance in Fig. 6.\nOn the importance of benchmarks for evaluation: While\nprior competence-based approaches such as DIAYN (Eysen-\nbach et al., 2019) were evaluated on OpenAI Gym (Brock-\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nResets when agent \nloses balance\nResets are fixed \nat 1000 steps\nOpenAI Gym Hopper\nDeepMind Control Hopper\nFigure 4. To empirically demonstrate issues inherent to competence-based exploration methods, we run DIAYN (Eysenbach et al., 2019)\nand compare it to ICM (Pathak et al., 2017) and a Fixed baseline where the agent receives an intrinsic reward of 1.0 for each timestep and\nno extrinsic reward on both OpenAI Gym (episode resets when agent loses balance) and DeepMind Control (DMC) (episode is ﬁxed for\n1k steps) Hopper environments. Since Gym and DMC rewards are on different scales, we normalize rewards based on the maximum\nreward achieved by any algorithm ( 1k for Gym, 3 for DMC). While DIAYN is able to achieve higher extrinsic rewards than ICM on\nGym, the Fixed intrinsic reward baseline performs best. However, on DMC the Fixed and DIAYN agents achieve near-zero reward while\nICM does not. This is consistent with ﬁndings of prior work that DIAYN is able to learn diverse behaviors in Gym (Eysenbach et al.,\n2019) as well as the observation that DIAYN performs poorly on DMC environments (Laskin et al., 2021)\nman et al., 2016), Gym environment episodes terminate\nwhen the agent loses balance thereby leaking some aspects\nof extrinsic signal to the exploration agent. On the other\nhand, DMC episodes have ﬁxed length. We show in Fig 4\nthat this small difference in environments results in large per-\nformance differences. Speciﬁcally, we ﬁnd that DIAYN is\nable to learn diverse skills in Gym but not in DMC, which is\nconsistent with both observations from DIAYN and URLB\npapers. Due to ﬁxed episode lengths, DMC tasks are harder\nfor reward-free exploration since agents must learn to bal-\nance without supervision.\n4. Method\n4.1. Contrastive Intrinsic Control\nFrom Section 3 we are motivated to ﬁnd a lower bound for\nI(τ; z) with a discriminator that is capable of supporting\nhigh-dimensional continuous skills3. Additionally, we wish\nto increase the diversity of behaviors so that the discrimina-\ntor can continue learning new skills throughout training. To\nimprove the discriminator, we propose to utilize noise con-\ntrastive estimation (NCE) (Gutmann & Hyv¨arinen, 2010) be-\ntween state-transitions and latent skills as a lower bound for\nI(τ; z).4 It has been shown previously that such estimators\nprovide a valid lower bound for mutual information (Oord\net al., 2018). However, to the best of our knowledge, this\nis the ﬁrst work to investigate contrastive representation\nlearning for intrinsic control.\n3In high-dimensional state-action spaces the number of distinct\nbehaviors can be quite large.\n4Note that τ is not a trajectory but some function of states.\nRepresentation Learning: Speciﬁcally, we propose to learn\nembeddings with the following representation learning ob-\njective, which is effectively CPC between state-transitions\nand latent skills:\nI(τ; z) ≥E[f(τ, z) −log 1\nN\nN\nX\nj=1\nexp(f(τj, z))].\n(3)\nwhere f(τ, z) is any real valued function. For convenience,\nwe deﬁne the discriminator log q(τ|z) as\nlog q(τ|z) := f(τ, z) −log 1\nN\nN\nX\nj=1\nexp(f(τj, z)).\n(4)\nFor our practical algorithm, we parameterize this function\nas f(τ, z) = gψ1(τ)⊤gψ2(z)/∥gψ1(τ)∥∥gψ2(z)∥T where\nτ = (s, s′) is a transition tuple, gψk are neural encoders,\nand T is a temperature parameter. This inner product is\nsimilar to the one used in SimCLR (Chen et al., 2020).\nThe representation learning loss backpropagates gradients\nfrom the NCE loss which maximizes similarity between\nstate-transitions and corresponding skills.\nFNCE(τ) =\ngψ1(τi)⊤gψ2(zi)\n∥gψ1(τi)∥∥gψ2(zi)∥T\n−log 1\nN\nN\nX\nj=1\nexp\n\u0012\ngψ1(τj)⊤gψ2(zi)\n∥gψ1(τj)∥∥gψ2(zi)∥T\n\u0013\n(5)\nWe provide pseudocode for the CIC representation learning\nloss below:\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nFigure 5. We report the aggregate statistics using stratiﬁed bootstrap intervals (Agarwal et al., 2021) for 12 downstream tasks on URLB\nwith 10 seeds, so each statistic for each algorithm has 120 seeds in total. We ﬁnd that overall, CIC achieves leading performance on URLB\nin terms of the IQM, mean, and OG statistics. As recommended by Agarwal et al. (2021), we use the IQM as our primary performance\nmeasure. In terms of IQM, CIC improves upon the next best skill discovery algorithm (APS) by 79% and the next best algorithm overall\n(ProtoRL) by 18%.\n1 \"\"\"\n2 PyTorch-like pseudocode for the CIC loss\n3 \"\"\"\n4\n5 def cic_loss(s, s_next, z, temp):\n6\n\"\"\"\n7\n- states: s, s_next (B, D)\n8\n- skills: z (B, D)\n9\n\"\"\"\n10\n11\ntau = concat(s, s_next, dim=1)\n12\n13\nquery = query_net(z)\n14\nkey = key_net(tau)\n15\n16\nquery = normalize(query, dim=1)\n17\nkey = normalize(key, dim=1)\n18\n19\n\"\"\"\n20\npositives are on diagonal\n21\nnegatives are off diagonal\n22\n\"\"\"\n23\n24\nlogits = matmul(query, key.T) / temp\n25\nlabels = arange(logits.shape[0])\n26\n27\nloss = cross_entropy(logits, labels)\n28\n29\nreturn loss\nListing 1. Pseudocode for the CIC loss\nIntrinsic reward: Although we have a representation learn-\ning objective, we still need to specify the intrinsic reward for\nthe algorithm for which there can be multiple choices. Prior\nworks consider specifying an intrinsic reward that is propor-\ntional to state-transition entropy (Liu & Abbeel, 2021a), the\ndiscriminator (Eysenbach et al., 2019), a similarity score be-\ntween states and skills (Warde-Farley et al., 2018), and the\nuncertainty of the discriminator (Strouse et al., 2021). We\ninvestigate each of these choices and ﬁnd that an intrinsic\nreward that maximizes state-transition entropy coupled with\nrepresentation learning via the CPC loss deﬁned in Sec. 4.1\nis the simplest variant that also performs well (see Table 1).\nFor the intrinsic reward, we use a particle estimate (Singh\net al., 2003; Beirlant, 1997) as in Liu & Abbeel (2021a)\nof the state-transition entropy. Similar to Liu & Abbeel\n(2021a); Yarats et al. (2021b) we estimate the entropy up\nto a proportionality constant, because we want the agent to\nmaximize entropy rather than estimate its exact value.\nThe APT particle entropy estimate is proportional to the\ndistance between the current visited state transition and\npreviously seen neighboring points.\nHparticle(τ) ∝1\nNk\nNk\nX\nh⋆\ni ∈Nk\nlog ∥hi −h⋆\ni ∥\n(6)\nwhere hi is an embedding of τi shown in Fig. 3, h∗\ni is a kNN\nembedding, Nk is the number of kNNs, and N −1 is the\nnumber of negatives. The total number of elements in the\nsummation is N because it includes one positive.\nExplore and Exploit: With these design choices the two\ncomponents of the CIC algorithm can be interpreted as\nexploration with intrinsic rewards and exploitation using\nrepresentation learning to distill behaviors into skills. The\nmarginal entropy maximizes the diversity of state-transition\nembeddings while the contrastive discriminator log q(τ|z)\nencourages exploitation by ensuring that skills z lead to\npredictable states τ. Together the two terms incentivize\nthe discovery of diverse yet predictable behaviors from the\nRL agent. While CIC shares a similar intrinsic reward\nstructure to APT (Liu & Abbeel, 2021a), we show that the\nnew representation learning loss from the CIC estimator\nresults in substantial performance gains in Sec 6.\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nProjected\nNot projected\n0.0\n0.1\n0.2\n0.3\nNormalized Score\n(a) Skill projection\n4\n8\n16\n32\n64\n128\nSkill dimension\n0.0\n0.1\n0.2\n0.3\nNormalized score\n(b) Skill dimension\nZero\nRand.\nCEM \n Beta CEM \n Gauss.Grid \n Sweep\n0.0\n0.1\n0.2\n0.3\n0.4\nNormalized Score\n(c) Skill adaptation\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nMean skill value\n0.0\n0.2\n0.4\n0.6\n0.8\nNormalized Score\nQuad. Stand\nQuad. Run\n(d) Skill grid sweep\nFigure 6. Design choices for pre-training and adapting with skills have signiﬁcant impact on performance. In (a) and (b) the agent’s\nzero-shot performance is evaluated while sampling skills randomly while in (c) and (d) the agent’s performance is evaluated after\nﬁnetuning the skills vector. (a) we show empirically that the projecting skill vectors after sampling them from noise signiﬁcantly improves\nthe agent’s performance. (b) The skill dimension is a crucial hyperparameter and, unlike prior methods, CIC scales to large skill vectors\nachieving optimal performance at 64 dimensional skills. (c) We test several adapation strategies and ﬁnd that a simple grid search performs\nbest given the small 4k step adaptation budget, (d) Choosing the right skill vector has substantial impact on performance and grid sweeping\nallows the agent to select the appropriate skill.\n5. Practical Implementation\nOur practical implementation consists of two main compo-\nnents: the RL optimization algorithm and the CIC archi-\ntecture. For fairness and clarity of comparison, we use the\nsame RL optimization algorithm for our method and all\nbaselines in this work. Since the baselines implemented in\nURLB (Laskin et al., 2021) use a DDPG5 (Lillicrap et al.,\n2016) as their backbone, we opt for the same DDPG archi-\ntecture to optimize our method as well (see Appendix B).\nCIC Architecture: We use a particle estimator as in Liu &\nAbbeel (2021a) to estimate H(τ). To compute the varia-\ntional density q(τ|z), we ﬁrst sample skills from uniform\nnoise z ∼p(z) where p(z) is the uniform distribution over\nthe [0, 1] interval. We then use two MLP encoders to em-\nbed gψ1(τ) and gψ2(z), and optimize the parameters ψ1, ψ2\nwith the CPC loss similar to SimCLR (Chen et al., 2020)\nsince f(τ, z) = gψ1(τ)T gψ2(z). We ﬁx the hyperparame-\nters across all domains and downstream tasks. We refer the\nreader to the Appendices E and F for the full algorithm and\na full list of hyperparameters.\nAdapting to downstream tasks:\nTo adapt to downstream\ntasks we follow the same procedure for competence-based\nmethod adaptation as in URLB (Laskin et al., 2021). During\nthe ﬁrst 4k environment interactions we populate the DDPG\nreplay buffer with samples and use the extrinsic rewards col-\nlected during this period to ﬁnetune the skill vector z. While\nit’s common to ﬁnetune skills with Cross Entropy Adapta-\ntion (CMA), given our limited budget of 4k samples (only 4\nepisodes) we ﬁnd that a simple grid sweep of skills over the\n5It was recently was shown that a DDPG achieves state-of-the-\nart performance (Yarats et al., 2021a) on DeepMind Control (Tassa\net al., 2018) and is more stable than SAC (Haarnoja et al., 2018)\non this benchmark.\ninterval [0, 1] produces the best results (see Fig. 6). After\nthis, we ﬁx the skill z and ﬁnetune the DDPG actor-critic\nparameters against the extrinsic reward for the remaining\n96k steps. Note that competence-based methods in URLB\nalso ﬁnetune their skills during the ﬁrst 4k ﬁnetuning steps\nensuring a fair comparison between the methods. The full\nadaptation procedure is detailed in Appendix E.\n6. Experimental Setup\nEnvironments We evaluate our approach on tasks from\nURLB, which consists of twelve downstream tasks across\nthree challenging continuous control domains for explo-\nration algorithms – walker, quadruped, and Jaco arm.\nWalker requires a biped constrained to a 2D vertical plane\nto perform locomotion tasks while balancing. Quadruped is\nmore challenging due to a higher-dimensional state-action\nspace and requires a quadruped to in a 3D environment to\nlearn locomotion skills. Jaco arm is a 6-DOF robotic arm\nwith a three-ﬁnger gripper to move and manipulate objects\nwithout locking. All three environments are challenging in\nthe absence of an extrinsic reward.\nBaselines: We compare CIC to baselines across all three ex-\nploration categories. Knowledge-based basedlines include\nICM (Pathak et al., 2017), Disagreement (Pathak et al.,\n2019), and RND (Burda et al., 2019). Data-based baselines\nincude APT (Liu & Abbeel, 2021a) and ProtoRL (Yarats\net al., 2021b). Competence-based baselines include DI-\nAYN (Eysenbach et al., 2019), SMM (Lee et al., 2019), and\nAPS (Liu & Abbeel, 2021b). The closest baselines to CIC\nare APT, which is similar to CIC but without state-skill CPC\nrepresentation learning (no discriminator), and APS which\nuses the same decomposition of mutual information as CIC\nand also uses a particle entropy estimate for H(τ). The\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nmain difference between APS and CIC is that APS uses\nsuccessor features while CIC uses a contrastive estimator\nfor the discriminator. For further details regarding baselines\nwe refer the reader to Appendix C.\nEvaluation: We follow an identical evaluation to the 2M\npre-training setup in URLB. First, we pre-train each RL\nagent with the intrinsic rewards for 2M steps. Then, we\nﬁnetune each agent to the downstream task with extrinsic\nrewards for 100k steps. All baselines were run for 10 seeds\nper downstream task for each algorithm using the code and\nhyperparameters provided by URLB (Laskin et al., 2021).\nBuilt on top of URLB, CIC is also run for 10 seeds per\ntask. A total of 1080 = 9 algorithms × 12 tasks × 10 seeds\nexperiments were run for the main results. Importantly, all\nbaselines and CIC use a DDPG agent as their backbone.\nTo ensure that our evaluation statistics are unbiased we use\nstratiﬁed bootstrap conﬁdence intervals to report aggregate\nstatistics across M runs with N seeds as described in Rli-\nable (Agarwal et al., 2021) to report statistics for our main\nresults in Fig. 5. Our primary success metric is the in-\nterquartile mean (IQM) and the Optimality Gap (OG). IQM\ndiscards the top and bottom 25% of runs and then computes\nthe mean. It is less susceptible to outliers than the mean\nand was shown to be the most reliable statistic for reporting\nresults for RL experiments in Agarwal et al. (2021). OG\nmeasures how far a policy is from optimal (expert) perfor-\nmance. To deﬁne expert performance we use the convention\nin URLB, which is the score achieved by a randomly ini-\ntialized DDPG after 2M steps of ﬁnetuning (20x more steps\nthan our ﬁnetuning budget).\n7. Results\nWe investigate empirical answers to the following research\nquestions: (Q1) How does CIC adaptation efﬁciency com-\npare to prior competence-based algorithms and exploration\nalgorithms more broadly? (Q2) Which intrinsic reward in-\nstantiation of CIC performs best? (Q3) How do the two\nterms in the CIC objective affect algorithm performance?\n(Q4) How does skill selection affect the quality of the pre-\ntrained policy?\n(Q5) Which architecture details matter\nmost?\nAdaptation efﬁciency of CIC and exploration baslines:\nExpert normalized scores of CIC and exploration algorithms\nfrom URLB are shown in Fig. 3. We ﬁnd that CIC sub-\nstantially outperforms prior competence-based algorithms\n(DIAYN, SMM, APS) achieving a 79% higher IQM than\nthe next best competence-based method (APS) and, more\nbroadly, achieving a 18% higher IQM than the next best\noverall baseline (ProtoRL). In further ablations, we ﬁnd that\nthe contributing factors to CIC’s performance are its ability\nto accommodate substantially larger continuous skill spaces\nthan prior competence-based methods.\nIntrinsic reward speciﬁcation: The intrinsic reward for\ncompetence-based algorithms can be instantiated in many\ndifferent ways. Here, we analyze intrinsic reward for CIC\nwith the form rint = H(τ) + D(τ, z), where D is some\nfunction of (τ, z). Prior works, select D to be (i) the dis-\ncriminator (Liu & Abbeel, 2021b), (ii) a cosine similarity\nbetween embeddings (Warde-Farley et al., 2018), (iii) uncer-\ntainty of the discriminator (Strouse et al., 2021), and (iv) just\nthe entropy D(τ, z) = 0 (Liu & Abbeel, 2021a). We run\nCIC with each of these variants on the walker and quadruped\ntasks and measure the ﬁnal mean performance across the\ndownstream tasks (see Tab. 1). The results show that the\nentropy-only intrinsic reward performs best. For this reason\nthe intrinsic reward and representation learning aspects of\nCIC are decoupled as shown in Eq. ??. We hypothesize\nthat the reason why a simple entropy-only intrinsic reward\nworks well is that state-skill CPC representation learning\nclusters similar behaviors together. Since redundant behav-\niors are clustered, maximizing the entropy of state-transition\nembeddings produces increasingly diverse behaviors.\ndisc.\nsimilarity\nuncertainty\nentropy\nwalker\n0.80\n0.79\n0.78\n0.82\nquad.\n0.44\n0.63\n0.75\n0.74\nmean\n0.62\n0.71\n0.77\n0.78\nTable 1. Analyzing four different intrinsic reward speciﬁcations\nfor CIC, we ﬁnd that entropy-based intrinsic reward performs\nbest, suggesting that the CIC discriminator is primarily useful for\nrepresentation learning. These are normalized scores averaged\nover 3 seeds across 8 downstream tasks (24 runs per data point).\nThe importance of representation learning: To what ex-\ntent does representation learning with state-skill CPC (see\nEq. 3) affect the agent’s exploration capability? To answer\nthis question we train the CIC agent with the entropy in-\ntrinsic reward with and without the representation learning\nauxiliary loss for 2M steps. The zero-shot reward plotted\nin Fig. 7 indicates that without representation learning the\npolicy collapses. With representation learning, the agent is\nable to discover diverse skills evidenced by the non-zero\nreward. This result suggests that state-skill CPC representa-\ntion learning is a critical part of CIC.\nQualitative analysis of CIC behaviors: Qualitatively, we\nﬁnd that CIC is able to learn locomotion behaviors in DMC\nwithout extrinsic information such as early termination as in\nOpenAI Gym. While most skills are higher entropy and thus\nmore chaotic, we show in Fig 2 that structured behaviors can\nbe isolated by ﬁxing a particular skill vector. For example,\nin the walker and quadruped domains - balancing, walking,\nand ﬂipping skills can be isolated. For more qualitative\ninvestigations we refer the reader to Appendix I.\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nEnv. Steps\n1e6\n0\n100\n200\n300\n400\n500\nReward\nZero-shot extrinsic rewards\nFull CIC: Particle entropy and CIC rep. learning\nAblation 1: Particle entropy but no CIC rep. learning\nAblation 2: CIC rep. learning but no particle entropy\nFigure 7. Mean zero-shot extrinsic rewards for Quadruped stand\nover 3 seeds with and without state-skill representation learning.\nWithout representation learning, the algorithm collapses. Similarly,\nwith CIC representation learning but no entropy term (in which\ncase we use the discriminator as the intrinsic reward) the policy\nalso collapses. Note that there is no ﬁnetuning happening here.\nWe’re showing the task-speciﬁc extrinsic reward during reward-\nfree pre-training as a way to sense-check exploration policy.\nSkill architecture and adaptation ablations: We ﬁnd that\nprojecting the skill to a latent space before inputting it as the\nkey for the contrastive loss is an important design decision\n(see Fig. 6a), most likely because this reduces the diversity\nof the skill vector making the discriminator task simpler.\nWe also ﬁnd empirically that the skill dimension is an im-\nportant hyperparameter and that larger skills results in better\nzero-shot performance (see Fig. 6b), which empirically sup-\nports the hypothesis posed in Section 3 and Appendix H that\nlarger skill spaces are important for internalizing diverse\nbehaviors. Interestingly, CIC zero-shot performance is poor\nin lower skill dimensions (e.g. dim(z) < 10), suggesting\nthat when dim(z) is small CIC performs no better than prior\ncompetence-based methods such as DIAYN, and that scal-\ning to larger skills enables CIC to pre-train effectively.\nTo measure the effect of skill ﬁnetuning described in Sec-\ntion 5, we sweep mean skill values along the interval of the\nuniform prior [0, 1] with a budget of 4k total environment in-\nteractions and read out the performance on the downstream\ntask. By sweeping, we mean simply iterating over the in-\nterval [0, 1] with ﬁxed step size (e.g. v = 0, 0.1, . . . , 0.9, 1)\nand setting zi = v for all i. This is not an optimal skill\nsampling strategy but works well due to the extremely lim-\nited number of samples for skill selection. We evaluate this\nablation on the Quadruped Stand and Run downstream tasks.\nThe results shown in Fig. 6 indicate that skill selection can\nsubstantially affect zero-shot downstream task performance.\n8. Conclusion\nWe have introduced a new competence-based algorithm –\nContrastive Intrinsic Control (CIC) – which enables more\neffective exploration than prior unsupervised skill discovery\nalgorithms by explicitly encouraging diverse behavior while\ndistilling predictable behaviors into skills with a contrastive\ndiscriminator. We showed that CIC is the ﬁrst competence-\nbased approach to achieve leading performance on URLB.\nWe hope that this encourages further research in developing\nRL agents capable of generalization.\n9. Acknowledgements\nWe would like to thank Ademi Adeniji, Xinyang Geng,\nFangchen Liu for helpful discussions. We would also like to\nthank Phil Bachman for useful feedback. This work was par-\ntially supported by Berkeley DeepDrive, NSF AI4OPT AI\nInstitute for Advances in Optimization under NSF 2112533,\nand the Ofﬁce of Naval Research grant N00014-21-1-2769.\nReferences\nAchiam, J., Edwards, H., Amodei, D., and Abbeel, P.\nVariational option discovery algorithms. arXiv preprint\narXiv:1807.10299, 2018.\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A.,\nand Bellemare, M. G. Deep reinforcement learning at the\nedge of the statistical precipice, 2021.\nBarber, D. and Agakov, F. V. The im algorithm: A vari-\national approach to information maximization. In Ad-\nvances in neural information processing systems, 2003.\nBarreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul,\nT., Van Hasselt, H., and Silver, D. Successor features\nfor transfer in reinforcement learning. arXiv preprint\narXiv:1606.05312, 2016.\nBeirlant, J. Nonparametric entropy estimation: An overview.\nInternational Journal of the Mathematical Statistics Sci-\nences, 6:17–39, 1997.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. Openai gym.\narXiv preprint arXiv:1606.01540, 2016.\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. Explo-\nration by random network distillation. In International\nConference on Learning Representations, 2019.\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P.,\nand Joulin, A. Unsupervised learning of visual features\nby contrasting cluster assignments. In Advances in Neural\nInformation Processing Systems, 2020.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. E.\nA simple framework for contrastive learning of visual\nrepresentations. In International conference on machine\nlearning, 2020.\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nEysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity\nis all you need: Learning skills without a reward function.\nIn International Conference on Learning Representations,\n2019.\nFlorensa, C., Duan, Y., and Abbeel, P. Stochastic neural\nnetworks for hierarchical reinforcement learning. In Inter-\nnational Conference on Learning Representations, 2018.\nGregor, K., Rezende, D. J., and Wierstra, D. Variational\nintrinsic control. In International Conference on Learning\nRepresentations, 2017.\nGutmann, M. and Hyv¨arinen, A. Noise-contrastive esti-\nmation: A new estimation principle for unnormalized\nstatistical models. In Teh, Y. W. and Titterington, M.\n(eds.), Proceedings of the Thirteenth International Con-\nference on Artiﬁcial Intelligence and Statistics, volume 9\nof Proceedings of Machine Learning Research, pp. 297–\n304, Chia Laguna Resort, Sardinia, Italy, 13–15 May\n2010. PMLR. URL https://proceedings.mlr.\npress/v9/gutmann10a.html.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft\nactor-critic: Off-policy maximum entropy deep reinforce-\nment learning with a stochastic actor. In International\nConference on Machine Learning, 2018.\nHansen, S., Dabney, W., Barreto, A., Warde-Farley, D.,\nde Wiele, T. V., and Mnih, V. Fast task inference with\nvariational intrinsic successor features. In International\nConference on Learning Representations, 2020.\nKwon, T.\nVariational intrinsic control revisited.\nIn In-\nternational Conference on Learning Representations,\n2021. URL https://openreview.net/forum?\nid=P0p33rgyoE.\nLaskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive\nunsupervised representations for reinforcement learning.\nIn International Conference on Machine Learning, 2020.\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K.,\nCang, C., Pinto, L., and Abbeel, P. Urlb: Unsupervised\nreinforcement learning benchmark, 2021. URL https:\n//openreview.net/forum?id=lwrPkQP_is.\nLee, L., Eysenbach, B., Parisotto, E., Xing, E. P., Levine,\nS., and Salakhutdinov, R. Efﬁcient exploration via state\nmarginal matching. CoRR, abs/1906.05274, 2019.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T.,\nTassa, Y., Silver, D., and Wierstra, D. Continuous con-\ntrol with deep reinforcement learning. In International\nConference on Learning Representations, 2016.\nLiu, H. and Abbeel, P.\nBehavior from the void:\nUnsupervised active pre-training.\narXiv preprint\narXiv:2103.04551, 2021a.\nLiu, H. and Abbeel, P. APS: active pretraining with suc-\ncessor features. In International Conference on Machine\nLearning, 2021b.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529, 2015.\nOord, A. v. d., Li, Y., and Vinyals, O. Representation learn-\ning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748, 2018.\nOpenAI. Learning dexterous in-hand manipulation. CoRR,\nabs/1808.00177, 2018.\nOpenAI. Solving rubik’s cube with a robot hand. ArXiv,\nabs/1910.07113, 2019.\nOudeyer, P.-Y., Kaplan, F., and Hafner, V. V.\nIntrinsic\nmotivation systems for autonomous mental development.\nIEEE transactions on evolutionary computation, 11(2):\n265–286, 2007.\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T.\nCuriosity-driven exploration by self-supervised predic-\ntion. In International Conference on Machine Learning,\n2017.\nPathak, D., Gandhi, D., and Gupta, A. Self-supervised ex-\nploration via disagreement. In International Conference\non Machine Learning, 2019.\nPeng, X. B., Abbeel, P., Levine, S., and van de Panne,\nM. Deepmimic: Example-guided deep reinforcement\nlearning of physics-based character skills. ACM Trans.\nGraph., 37:143:1–143:14, 2018.\nPoole, B., Ozair, S., van den Oord, A., Alemi, A., and\nTucker, G. On variational bounds of mutual informa-\ntion.\nIn Chaudhuri, K. and Salakhutdinov, R. (eds.),\nProceedings of the 36th International Conference on\nMachine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings of\nMachine Learning Research, pp. 5171–5180. PMLR,\n2019. URL http://proceedings.mlr.press/\nv97/poole19a.html.\nRajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schul-\nman, J., Todorov, E., and Levine, S. Learning complex\ndexterous manipulation with deep reinforcement learning\nand demonstrations. In Proceedings of Robotics: Science\nand Systems (RSS), 2018.\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel,\nP. High-dimensional continuous control using generalized\nadvantage estimation. In International Conference on\nLearning Representations, 2016.\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nSharma, A., Gu, S., Levine, S., Kumar, V., and Hausman,\nK. Dynamics-aware unsupervised discovery of skills. In\nInternational Conference on Learning Representations,\n2020.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou,\nI., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\nBolton, A., et al. Mastering the game of go without\nhuman knowledge. Nature, 550(7676):354, 2017.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,\nM., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-\npel, T., et al. A general reinforcement learning algorithm\nthat masters chess, shogi, and go through self-play. Sci-\nence, 362(6419):1140–1144, 2018.\nSingh, H., Misra, N., Hnizdo, V., Fedorowicz, A., and Dem-\nchuk, E. Nearest neighbor estimates of entropy. American\nJournal of Mathematical and Management Sciences, 23\n(3-4):301–321, 2003.\nSrinivas,\nA. and Abbeel,\nP.\nUnsupervised learn-\ning\nfor\nreinforcement\nlearning,\n2021.\nURL\nhttps://icml.cc/media/icml-2021/\nSlides/10843_QHaHBNU.pdf.\nStrouse, D., Baumli, K., Warde-Farley, D., Mnih, V., and\nHansen, S. Learning more skills through optimistic ex-\nploration. CoRR, abs/2107.14226, 2021. URL https:\n//arxiv.org/abs/2107.14226.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction. MIT Press, 2018.\nTassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D.\nd. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq,\nA., et al.\nDeepmind control suite.\narXiv preprint\narXiv:1801.00690, 2018.\nTschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S.,\nand Lucic, M. On mutual information maximization\nfor representation learning. In 8th International Con-\nference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net,\n2020. URL https://openreview.net/forum?\nid=rkxoh24FPH.\nWarde-Farley, D., de Wiele, T. V., Kulkarni, T., Ionescu, C.,\nHansen, S., and Mnih, V. Unsupervised control through\nnon-parametric discriminative rewards, 2018.\nYarats, D., Fergus, R., Lazaric, A., and Pinto, L. Master-\ning visual continuous control: Improved data-augmented\nreinforcement learning, 2021a.\nYarats, D., Fergus, R., Lazaric, A., and Pinto, L. Rein-\nforcement learning with prototypical representations. In\nInternational Conference on Machine Learning, 2021b.\nZahavy, T., Barreto, A., Mankowitz, D. J., Hou, S.,\nO’Donoghue, B., Kemaev, I., and Singh, S. B. Discover-\ning a set of policies for the worst case reward, 2021.\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nA. Competence-based Exploration Algorithms\nThe competence-based algorithms considered in this work aim to maximize I(τ; s). The algorithms differ by ho they\ndecompose mutual information, whether they explicitly maximize behavioral entropy, their skill space (discrete or continuous)\nand their intrinsic reward structure. We provide a list of common competence-based algorithms in Table 2.\nTable 2. Prior Competence-based Unsupervised Skill Discovery Algorithms\nAlgorithm\nIntrinsic Reward\nDecomposition\nExplicit max H(τ)\nSkill Dim.\nSkill Space\nSSN4HRL (Florensa et al., 2018)\nlog qψ(z|st)\nH(z) −H(z|τ)\nNo\n6\ndiscrete\nVIC (Gregor et al., 2017)\nlog qψ(z|sH))\nH(z) −H(z|τ)\nNo\n60\ndiscrete\nVALOR (Achiam et al., 2018)\nlog qψ(z|s1:H)\nH(z) −H(z|τ)\nNo\n64\ndiscrete\nDIAYN (Eysenbach et al., 2019)\nlog qψ(z|st)\nH(z) −H(z|τ)\nNo\n128\ndiscrete\nDADS (Sharma et al., 2020)\nqψ(s′|z, s) −P\ni log q(s′|zi, s)\nH(τ) −H(τ|z)\nYes\n5\ncontinuous\nVISR (Hansen et al., 2020)\nlog qψ(z|st)\nH(z) −H(z|τ)\nNo\n10\ncontinuous\nAPS (Liu & Abbeel, 2021b)\nFSuccessor(s|z) + Hparticle(s)\nH(τ) −H(τ|z)\nYes\n10\ncontinuous\nTable 3. A list of competence-based algorithms. We describe the intrinsic reward optimized by each method and the decomposition of the\nmutual information utilized by the method. We also note whether the method explicitly maximizes state transition entropy. Finally, we\nnote the maximal dimension used in each work and whether the skills are discrete or continuous. All methods prior to CIC only support\nsmall skill spaces, either because they are discrete or continuous but low-dimensional.\nB. Deep Deterministic Policy Gradient (DDPG)\nA DDPG is an actor-critic RL algorithm that performs off-policy gradient updates and learns a Q function Qφ(s, a) and an\nactor πθ(a|s). The critic is trained by satisfying the Bellman equation.\nLQ(φ, D) = E(st,at,rt,st+1)∼D\n\u0014\u0010\nQφ(st, at) −rt −γQ¯φ(st+1, πθ(st+1)\n\u00112\u0015\n.\n(7)\nHere, ¯φ is the Polyak average of the parameters φ. As the critic minimizes the Bellman error, the actor maximizes the\naction-value function.\nLπ(θ, D) = Est∼D [Qφ(st, πθ(st))] .\n(8)\nC. Baselines\nFor baselines, we choose the existing set of benchmarked unsupervised RL algorithms on URLB. We provide a quick\nsummary of each method. For more detailed descriptions of each baseline we refer the reader to URLB (Laskin et al., 2021)\nCompetence-based Baselines: CIC is a competence-based exploration algorithm. For baselines, we compare it to DI-\nAYN (Eysenbach et al., 2019), SMM (Lee et al., 2019), and APS (Liu & Abbeel, 2021b). Each of these algorithms is\ndescribed in Table 2. Notably, APS is a recent state-of-the-art competence-based method that is the most closely related\nalgorithm to the CIC algorithm.\nKnowledge-based Baselines: For knowledge-based baselines, we compare to ICM (Pathak et al., 2017), Disagree-\nment (Pathak et al., 2019), and RND (Burda et al., 2019). ICM and RND train a dynamics model and random network\nprediction model and deﬁne the intrinsic reward to be proportional to the prediction error. Disagreement trains an ensemble\nof dynamics models and deﬁnes the intrinsic reward to be proportional to the uncertainty of an ensemble.\nData-based Baselines: For data-based baselines we compare to APT (Liu & Abbeel, 2021a) and ProtoRL (Yarats et al.,\n2021b). Both methods use a particle estimator to estimate the state visitation entropy. ProtoRL also performs discrete\ncontrastive clustering as in Caron et al. (2020) as an auxiliary task and uses the resulting clusters to compute the particle\nentropy. While ProtoRL is more effective than APT when learning from pixels, on state-based URLB APT is competitive\nwith ProtoRL. Our method CIC is effectively a skill-conditioned APT agent with a contrastive discriminator.\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nD. Relation to Prior Skill Discovery Methods\nThe most closely relatd prior algorithms to CIC are APT (Liu & Abbeel, 2021a) and APS (Liu & Abbeel, 2021b). Both\nCIC and APS use the H(τ) −H(τ|z) decomposition of the mutual information and both used a particle estimator (Singh\net al., 2003) to compute the state entropy as in Liu & Abbeel (2021a). The main difference between CIC and APS is the\ndiscriminator. APS uses successor features as in Hansen et al. (2020) for its discriminator while CIC uses a noise contrastive\nestimator. Unlike successor features, which empirically only accommodate low-dimensional continuous skill spaces (see\nTable 2), the noise contrastive discriminator is able to leverage higher continuous dimensional skill vectors. Like APT, CIC\nhas an intrinsic reward that maximizes H(τ). However, CIC also does contrastive skill learning to shape the embedding\nspace and outputs a skill-conditioned policy.\nThe CIC discriminator is similar to the one used in DISCERN (Warde-Farley et al., 2018), a goal-conditioned unsupervised\nRL algorithm. Both methods use a contrastive discriminator by sampling negatives and computing an inner product between\nqueries and keys. The main differences are (i) that DISCERN maximizes I(τ; g) where g are image goal embeddings\nwhile CIC maximizes I(τ; z) where z are abstract skill vectors; (ii) DISCERN uses the DIAYN-style decomposition\nI(τ; g) = H(g) −H(g|τ) while CIC decomposes through H(τ) −H(τ|z), and (iii) DISCERN discards the H(g) term by\nsampling goals uniformly while CIC explicitly maximizes H(τ). While DISCERN and CIC share similarities, DISCERN\noperates over image goals while CIC operates over abstract skill vectors so the two methods are not directly comparable.\nFinally, another similar algorithm to CIC is DADS (Sharma et al., 2020) which also decomposes through H(τ) −H(τ|z).\nWhile CIC uses a contrastive density estimate for the discriminator, DADS uses a maximum likelihood estimator similar\nto DIAYN. DADS maximizes I(s′|s, z) and estimates entropy H(s′|s) by marginalizing over z such that H(s′|s) =\n−log P\ni q(s′|s, zi) while CIC uses a particle estimator.\nE. Full CIC Algorithm\nThe full CIC algorithm with both pre-training and ﬁne-tuning phases is shown in Algorithm 1. We pre-train CIC for 2M\nsteps, and ﬁnetune it on each task for 100k steps.\nAlgorithm 1 Contrastive Intrinsic Control\nRequire: Initialize all networks: encoders gψ1 and gψ2, actor πθ, critic Qφ, replay buffer D.\nRequire: Environment (env), M downstream tasks Tk, k ∈[1, . . . , M].\nRequire: pre-train NPT = 2M and ﬁne-tune NFT = 100K steps.\n1: for t = 1..NPT do\n▷Part 1: Unsupervised Pre-training\n2:\nSample and encode skill z ∼p(z) and z ←gψ2(z)\n3:\nEncode state st ←gψ1(st) and sample action at ←πθ(st, z) + ϵ where ϵ ∼N(0, σ2)\n4:\nObserve next state st+1 ∼P(·|st, at)\n5:\nAdd transition to replay buffer D ←D ∪(st, at, st+1)\n6:\nSample a minibatch from D, compute contrastive loss in Eq.3 and update encoders gψ1, gψ2, compute CIC intrinsic reward with\nEq. ?? and update actor πθ and critic Qφ\n7: end for\n8: for Tk ∈[T1, . . . , TM] do\n▷Part 2: Supervised Fine-tuning\n9:\nInitialize all networks with weights from pre-training phase and an empty replay buffer D.\n10:\nfor t = 1 . . . 4, 000 do\n11:\nTake random action at ∼N(0, 1)\n12:\nSelect skill with grid sweep over unit interval [0, 1] every 100 steps\n13:\nSample minibatch from D and update actor πθ and critic Qφ\n14:\nend for\n15:\nFix skill z that achieved highest extrinsic reward during grid sweep.\n16:\nfor t = 4, 000 . . . NFT do\n17:\nEncode state st ←gψ1(st) and sample action at ←πθ(st, z) + ϵ where ϵ ∼N(0, σ2)\n18:\nObserve next state and reward st+1, rext\nt\n∼P(·|st, at)\n19:\nAdd transition to replay buffer D ←D ∪(st, at, rext\nt\n, st+1)\n20:\nSample minibatch from D and update actor πθ and critic Qφ.\n21:\nend for\n22:\nEvaluate performance of RL agent on task Tk\n23: end for\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nF. Hyper-parameters\nBaseline hyperparameters are taken from URLB (Laskin et al., 2021), which were selected by performing a grid sweep over\ntasks and picking the best performing set of hyperparameters. Except for the skill dimension, hyperparameters for CIC are\nborrowed from URLB.\nTable 4. Hyper-parameters used for CIC .\nDDPG hyper-parameter\nValue\nReplay buffer capacity\n106\nAction repeat\n1\nSeed frames\n4000\nn-step returns\n3\nMini-batch size\n1024\nSeed frames\n4000\nDiscount (γ)\n0.99\nOptimizer\nAdam\nLearning rate\n10−4\nAgent update frequency\n2\nCritic target EMA rate (τQ)\n0.01\nFeatures dim.\n1024\nHidden dim.\n1024\nExploration stddev clip\n0.3\nExploration stddev value\n0.2\nNumber pre-training frames\n2 × 106\nNumber ﬁne-turning frames\n1 × 105\nCIC hyper-parameter\nValue\nSkill dim\n64 continuous\nPrior\nUniform [0,1]\nSkill sampling frequency (steps)\n50\nState net arch. gψ1(s)\ndim(O) →1024 →1024 →64 ReLU MLP\nSkill net arch. gψ2(z)\n64 →1024 →1024 →64 ReLU MLP\nPrediction net arch.\n64 →1024 →1024 →64 ReLU MLP\nG. Raw Numerical Results\nWe provide a list of raw numerical results for ﬁnetuning CIC and baselines in Tables 5 and 6. All baselines were run using\nthe code provided by URLB (Laskin et al., 2021) for 10 seeds per downstream task.\nStatistic\nICM\nDis.\nRND\nAPT\nProto\nDIAYN\nAPS\nSMM\nCIC\n% CIC > APS\n% CIC > Proto\nMedian ↑\n0.45\n0.56\n0.58\n0.62\n0.66\n0.44\n0.47\n0.22\n0.76\n+61%\n+15%\nIQM ↑\n0.41\n0.51\n0.61\n0.65\n0.65\n0.40\n0.43\n0.25\n0.77\n+79%\n+18%\nMean ↑\n0.43\n0.51\n0.63\n0.66\n0.65\n0.44\n0.46\n0.35\n0.76\n+65%\n+17%\nOG ↓\n0.57\n0.49\n0.37\n0.35\n0.35\n0.56\n0.54\n0.65\n0.24\n-44%\n-68%\nTable 5. Statics for downstream task normalized scores for CIC and baselines from URLB (Laskin et al., 2021). CIC improves over both\nthe prior leading competence-based method APS (Liu & Abbeel, 2021b) and overall next-best exploration algorithm ProtoRL (Yarats\net al., 2021b) across all readout statistics. Each data point is a statistic computed using 10 seeds and 12 downstream tasks (120 experiments\nper data point). The statistics are computed using RLiable (Agarwal et al., 2021).\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nPre-trainining for 2 × 106 environment steps\nDomain\nTask\nExpert\nDDPG\nCIC\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\nWalker\nFlip\n799\n538±27\n631 ± 34\n417±16\n346±13\n474±39\n544±14\n456±12\n450±24\n319±17\n465±20\nRun\n796\n325±25\n486 ± 25\n247±21\n208±15\n406±30\n392±26\n306±13\n426±26\n158±8\n134±16\nStand\n984\n899±23\n959 ± 2\n859±23\n746±34\n911±5\n942±6\n917±27\n924±12\n695±46\n721±44\nWalk\n971\n748±47\n885 ± 28\n627±42\n549±37\n704±30\n773±70\n792±41\n770±44\n498±27\n527±79\nQuadruped\nJump\n888\n236±48\n595 ± 42\n178±35\n389±62\n637±12\n648±18\n617±44\n96±7\n660±43\n463±51\nRun\n888\n157±31\n505 ± 47\n110±18\n337±30\n459±6\n492±14\n373±33\n96±6\n433±29\n281±17\nStand\n920\n392±73\n761 ± 54\n312±68\n512±89\n766±43\n872±23\n716±56\n123±11\n851±43\n542±53\nWalk\n866\n229±57\n723 ± 43\n126±27\n293±37\n536±39\n770±47\n412±54\n80±6\n576±81\n436±79\nJaco\nReach bottom left\n193\n72±22\n138 ± 9\n111±11\n124±7\n110±5\n103±8\n129±8\n45±7\n39±6\n76±8\nReach bottom right\n203\n117±18\n145 ± 7\n97±9\n115±10\n117±7\n100±6\n132±8\n46±11\n38±5\n88±11\nReach top left\n191\n116±22\n153 ± 7\n82±14\n106±12\n99±6\n73±12\n123±9\n36±3\n19±4\n68±6\nReach top right\n223\n94±18\n163 ± 4\n103±11\n139±7\n100±6\n90±10\n159±7\n47±6\n28±6\n76±10\nTable 6. Performance of CIC and baselines on state-based URLB after ﬁrst pre-training for 2 × 106 steps and then ﬁnetuning with\nextrinsic rewards for 1 × 105. All baselines were run for 10 seeds per downstream task for each algorithm using the code provided by\nURLB (Laskin et al., 2021). A total of 1080 = 9 algorithms × 12 tasks × 10 seeds experiments were run.\nH. Toy Example to Illustrate the Need for Larger Skill Spaces\nFigure 8. A gridworld example motivating the need for large skill spaces. In this environment, we place an agent in a 10 × 10 gridworld\nand provide the agent access to four discrete skills. We show that the mutual information objective can be maximized by mapping these\nfour skills to the nearest neighboring states resulting in low behavioral diversity and exploring only four of the hundred available states.\nWe illustrate the need for larger skill spaces with a gridworld example. Suppose we have an agent in a 10 × 10 sized\ngridworld and that we have four discrete skills at our disposal. Now let τ = s and consider how we may achieve maximal\nI(τ; z) in this setting. If we decompose I(τ; z) = H(z) −H(z|τ) then we can achieve maximal H(z) by sampling the four\nskills uniformly z ∼p(z). We can achieve H(z|τ) = 0 by mapping each skill to a distinct neighboring state of the agent.\nThus, our mutual information is maximized but as a result the agent only explores four out of the hundrend available states\nin the gridworld.\nNow suppose we consider the second decomposition I(τ; z) = H(τ) −H(τ|z). Since the agent is maximizing H(τ) it is\nlikely to visit a diverse set of states at ﬁrst. However, as soon as it learns an accurate discriminator we will have H(τ|z)\nand again the skills can be mapped to neighboring states to achieve minimal conditional entropy. As a result, the skill\nconditioned policy will only be able to reach four out of the hundrend possible states in this gridworld. This argument is\nshown visually in Fig. 8.\nSkill spaces that are too large can also be an issue. Consider if we had 100 skills at our disposal in the same gridworld. Then\nthe agent could minimize the conditional entropy by mapping each skill to a unique state which would result in the agent\nmemorizing the environment by ﬁnding a one-to-one mapping between states and skills. While this is a potential issue it\nhas not been encountered in practice yet since current competence-based methods support small skill spaces relative to the\nobservation space of the environment.\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nI. Qualitative Analysis of Skills\nWe provide two additional qualitative analyses of behaviors learned with the CIC algorithm. First, we take a simple\npointmass setting and set the skill dimension to 1 in order to ablate the skills learned by the CIC agent in a simple setting.\nWe sweep over different values of z and plot the behavioral ﬂow vector ﬁeld (direction in which point mass moves) in Fig.9.\nWe ﬁnd that the pointmass learns skills that produce continuous motion and that the direction of the motion changes as a\nfunction of the skill value. Near the origin the pointmass learns skills that span all directions, while near the edges the point\nmass learns to avoid wall collisions. Qualitatively, many behaviors are periodic.\nPointmass with 1 skill\nBehavior flow for different skill values\nFigure 9. Learning curves for ﬁnetuning pre-trained agents for 100k steps. Task performance is aggregated for each domain, such that\neach curve represents the mean normalized scores over 4 × 10 = 40 seeds. The shaded regions represent the standard error. CIC surpasses\nthe performance of the prior state-of-the-art on Walker and Jaco tasks while tying on Quadruped. CIC is the only algorithm that performs\nconsistently well across all three domains.\nQualitatively, we ﬁnd that methods like DIAYN that only support low dimensional skill vectors and do not explicitly\nincentivize diverse behaviors in their objective produce policies that map skills to a small set of static behaviors. These\nbehaviors shown in Fig. 10 are non-trivial but also have low behavioral diversity and are not particularly useful for solving\nthe downstream task. This observation is consistent with Zahavy et al. (2021) where the authors found that DIAYN maps to\nstatic “yoga” poses in DeepMind Control. In contrast, behaviors produce by CIC are dynamic resulting ﬂipping, jumping,\nand locomotive behaviors that can then be adapted to efﬁciently solve downstream tasks.\nJ. OpenAI Gym vs. DeepMind control: How Early Termination Leaks Extrinsic Signal\nPrior work on unsupervised skill discovery for continuous control (Eysenbach et al., 2019; Sharma et al., 2020) was\nevaluated on OpenAI Gym (Brockman et al., 2016) and showed diverse exploration on Gym environments. However, Gym\nenvironment episodes terminate early when the agent loses balance, thereby leaking information about the extrinsic task (e.g.\nbalancing or moving). However, DeepMind Control (DMC) episodes have a ﬁxed length of 1k steps. In DMC, exploration\nis therefore harder since the agent needs to learn to balance without any extrinsic signal.\nTo evaluate whether the difference in the two environments has impact on competence-based exploration, we run DIAYN\non the hopper environments from both Gym and DMC. We compare to ICM, a popular exploration baseline, and a Fixed\nbaseline where the agent receives an intrinsic reward of 1 for each timestep and no algorithms receive extrinsic rewards. We\nthen measure the extrinsic reward, which loosely corresponds to the diversity of behaviors learned. Our results in Fig. 4\nshow that indeed DIAYN is able to learn diverse behaviors in Gym but not in DMC while ICM is able to learn diverse\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nFigure 10. Qualitative visualization of DIAYN and CIC pre-training on the Walker and Quadruped domains from URLB. Conﬁrming\nﬁndings in prior work (Zahavy et al., 2021), we also ﬁnd that DIAYN policies produce static but non-trivial behaviors mapping to “yoga”\nposes while CIC produces diverse and dynamic behaviors such as walking, ﬂipping, and standing. Though it’s hard to see from these\nimages, all the DIAYN skills get stuck in frozen poses while the CIC skills are producing dynamic behavior with constant motion.\nbehaviors in both environments. Interestingly, the Fixed baseline achieves the highest reward on the Gym environment by\nlearning to stand and balance. These results further motivate us to evaluate on URLB which is built on top of DMC.\nK. CIC vs Other Types of Contrastive Learning for RL\nContrastive learning in CIC is different than prior vision-based contrastive learning in RL such as CURL (Laskin et al.,\n2020), since we are not performing contrastive learning over augmented images but rather over state transitions and skills.\nThe contrastive objective in CIC is used for unsupervised learning of behaviors while in CURL it is used for unsupervised\nlearning of visual features.\nWe provide pseudocode for the CIC loss below:\n1 def discriminator_loss(states, next_states, skills, temp):\n2\n\"\"\"\n3\n- states and skills are sampled from replay buffer\n4\n- skills were sampled from uniform dist [0,1] during agent rollout\n5\n- states / next_states: dim (B, D_state)\n6\n- skills: dim (B, D_skill)\n7\n\"\"\"\n8\n9\ntransitions = concat(states, next_states, dim=1)\n10\n11\nquery = skill_net(skills) # (B, D_hidden) -> (B, D_hidden)\n12\nkey = transition_net(transitions) # (B, 2*D_state) -> (B, D_hidden)\n13\n14\nquery = normalize(query, dim=1)\n15\nkey = normalize(key, dim=1)\n16\n17\nlogits = matmul(query, key.T) / temp # (B, B)\n18\nlabels = arange(logits.shape[0])\n19\n20\n# positives are on diagonal, negatives are off diagonal\n21\n# for each skill, negatives are sampled from transitions\n22\n# while skills are fixed\n23\nloss = cross_entropy(logits, labels)\n24\n25\nreturn loss\nListing 2. CIC discriminator loss\nThis is substantially different from prior contrastive learning works in RL such as CURL (Laskin et al., 2020), which\nCIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery\nperform contrastive learning over images.\n1 def curl_loss(obs, W, temp):\n2\n\"\"\"\n3\n- observation images are sampled from replay buffer\n4\n- obs: dim (B, C, H, W)\n5\n- W: projection matrix (D_hidden, D_hidden)\n6\n\"\"\"\n7\n8\nquery = aug(obs)\n9\nkey = aug(obs)\n10\n11\nquery = cnn_net(query) # (B, D_hidden)\n12\nkey = cnn_net(key) # (B, D_hidden)\n13\n14\nlogits = matmul(matmul(query, W), key.T) / temp # (B, B)\n15\nlabels = arange(logits.shape[0])\n16\n17\n# positives are on diagonal\n18\n# negatives are off diagonal\n19\nloss = cross_entropy(logits, labels)\n20\n21\nreturn loss\nListing 3. CURL contrastive loss\nL. On estimates of Mutual Information\nIn this work we have presented CIC - a new competence-based algorithm that achieves leading performance on URLB\ncompared to prior unsupervised RL methods.\nOne might wonder whether estimating the exact mutual information (MI) or maximizing the tightest lower bound thereof is\nreally the goal for unsupervised RL. In unsupervised representation learning, state-of-the-art methods like CPC and SimCLR\nmaximize the lower bound of MI based on Noise Contrastive Estimation (NCE). However, as proven in CPC (Oord et al.,\n2018) and illustrated in Poole et al. (2019) NCE is upper bounded by log N, meaning that the bound is loose when the MI\nis larger than log N. Nevertheless, these methods have been repeatedly shown to excel in practice. In Tschannen et al.\n(2020) the authors show that the effectiveness of NCE results from the inductive bias in both the choice of feature extractor\narchitectures and the parameterization of the employed MI estimators.\nWe have a similar belief for unsupervised RL - that with the right parameterization and inductive bias, the MI objective\nwill facilitate behavior learning in unsupervised RL. This is why CIC lower bounds MI with (i) the particle based entropy\nestimator to ensure explicit exploration and (ii) a contrastive conditional entropy estimator to leverage the power of\ncontrastive learning to discriminate skills. As demonstrated in our experiments, CIC outperforms prior methods, showing\nthe effectiveness of optimizing an intrinsic reward with the CIC MI estimator.\nM. Limitations\nWhile CIC achieves leading results on URLB, we would also like to address its limitations. First, in this paper we only\nconsider MDPs (and not partially observed MDPs) where the full state is observable. We focus on MDPs because generating\ndiverse behaviors in environments with large state spaces has been the primary bottleneck for competence-based exploration.\nCombining CIC with visual representation learning to scale this method to pixel-based inputs is a promising future direction\nfor research not considered in this work. Another limitation is that our adaptation strategy to downstream tasks requires\nﬁnetuning. Since we learn skills, it would be interesting to investigate alternate ways of adapting that would enable zero-shot\ngeneralization such as learning generalized reward functions during pre-training.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-02-01",
  "updated": "2022-03-30"
}