{
  "id": "http://arxiv.org/abs/2202.00470v1",
  "title": "An Assessment of the Impact of OCR Noise on Language Models",
  "authors": [
    "Konstantin Todorov",
    "Giovanni Colavizza"
  ],
  "abstract": "Neural language models are the backbone of modern-day natural language\nprocessing applications. Their use on textual heritage collections which have\nundergone Optical Character Recognition (OCR) is therefore also increasing.\nNevertheless, our understanding of the impact OCR noise could have on language\nmodels is still limited. We perform an assessment of the impact OCR noise has\non a variety of language models, using data in Dutch, English, French and\nGerman. We find that OCR noise poses a significant obstacle to language\nmodelling, with language models increasingly diverging from their noiseless\ntargets as OCR quality lowers. In the presence of small corpora, simpler models\nincluding PPMI and Word2Vec consistently outperform transformer-based models in\nthis respect.",
  "text": "An Assessment of the Impact of OCR Noise on Language Models\nKonstantin Todorov\na and Giovanni Colavizza\nb\nInstitute for Logic, Language and Computation (ILLC), University of Amsterdam, The Netherlands\nkztodorov@outlook.com, g.colavizza@uva.nl\nKeywords:\nMachine Learning, Language Models, Optical Character Recognition (OCR)\nAbstract:\nNeural language models are the backbone of modern-day natural language processing applications. Their use\non textual heritage collections which have undergone Optical Character Recognition (OCR) is therefore also\nincreasing. Nevertheless, our understanding of the impact OCR noise could have on language models is still\nlimited. We perform an assessment of the impact OCR noise has on a variety of language models, using\ndata in Dutch, English, French and German. We ﬁnd that OCR noise poses a signiﬁcant obstacle to language\nmodelling, with language models increasingly diverging from their noiseless targets as OCR quality lowers.\nIn the presence of small corpora, simpler models including PPMI and Word2Vec consistently outperform\ntransformer-based models in this respect.\n1\nINTRODUCTION\nStatistical neural language models have become\nthe backbone of modern-day natural language pro-\ncessing (NLP) applications. They have proven high\ncapabilities in learning complex linguistic features\nand for transferable, multi-purpose adaptability Qiu\net al. (2020), in particular with the recent success of\ncontextual models like BERT Devlin et al. (2019).\nLanguage models’ main objective is to assign prob-\nabilities to sequences of linguistic units, such as sen-\ntences made of words, possibly beneﬁting from auxil-\niary tasks. The success of neural language models has\nfostered a signiﬁcant amount of work on understand-\ning how they work internally Rogers et al. (2020).\nIn Digital/Computational Humanities, language mod-\nels are primarily used as components of NLP archi-\ntectures and to perform well-posed tasks. Examples\ninclude Handwritten/Optical Character Recognition\n(H/OCR) Kahle et al. (2017), Named Entity Recogni-\ntion (NER) and linkage Ehrmann et al. (2020), mod-\nelling semantic change Shoemark et al. (2019), anno-\ntating historical corpora Coll Ardanuy et al. (2020),\ntranslating heritage metadata Banar et al. (2020), and\nmany more.\nAn open challenge for neural language models are\nlow-resource settings: languages or tasks where lan-\nguage data is comparatively scarce, and where an-\nnotations are few Hedderich et al. (2021). This is a\na\nhttps://orcid.org/0000-0002-7445-4676\nb\nhttps://orcid.org/0000-0002-9806-084X\nknown issue for many underrepresented languages,\nincluding when a language is distinctively appropri-\nated for example via dialects and idiomatic expres-\nsions Nguyen et al. (2016). Consequences include\nthe possible exclusion of speakers of low-resource\nlanguages, cognitive and societal biases, the reduc-\ntion of linguistic diversity and often poor general-\nization Ruder (2020). Historical language data are\nalso comparatively less abundant and sparser than\nmodern-day data Piotrowski (2012); Ehrmann et al.\n(2016).\nWhat is more, historical language data often poses\ntwo additional challenges: noise and variation. Noise\ncomes from errors which should be corrected and\ntheir impact mitigated, variation instead is a charac-\nteristic of language which may constitute a useful sig-\nnal. Modern-day NLP methods, including language\nmodels, ‘overcome’ noise by sheer data size – yet\nnoise can still remain a problem – and often ﬂatten-\nout linguistic variation. Therefore, when employed\non real-world applications in low-resource settings,\nor when applied out-of-domain, these methods might\nfall short and, crucially, they might fail to appropri-\nately deal with noise and variation.\nIn this work, we contribute to bridge this gap by\nposing the following question: what is the impact on\nlanguage models of textual noise caused by OCR?\nWhile recent work has focused on the impact of\nOCR noise on downstream tasks Hill and Hengchen\n(2019); van Strien et al. (2020); Todorov and Colav-\nizza (2020), less is known about language mod-\narXiv:2202.00470v1  [cs.CL]  26 Jan 2022\nels in this respect.\nWe therefore proceed by con-\nsidering a most basic empirical setting, expanding\nfrom van Strien et al. (2020).\nFirst, we use data\nin multiple languages available in two versions: as\nground truth, assumed correct and veriﬁed by hu-\nmans, and as OCRed texts, containing varying de-\ngrees of noise from automated text extraction. The\ndata we use is small when compared to modern-\nday language modelling standards, yet of realistic\nsize in a Digital/Computational Humanities setting.\nFurthermore, we consider language models trained\nfrom scratch and do not cover ﬁne-tuning or lan-\nguage model adaptation here. Lastly, for each lan-\nguage model under consideration we compare the re-\nsults of two identically-conﬁgured language models,\ntrained on these two versions of the same corpus, by\ninspecting how similar their learned vector spaces are\nupon convergence. In this way, we assume no uni-\nversal baseline, but instead compare language models\nindependently.\nWhile transfer learning on language models is of\nextreme importance to model NLP applications Ruder\n(2019), we do not consider it here.\nThe reason is\nthe difﬁculty in establishing a consistent comparison.\nWhile comparing the vector spaces of language mod-\nels trained from scratch is possible upon convergence,\nthis is more problematic for ﬁne tuning since it is\ndifﬁcult to know when a ﬁne tuned model has actu-\nally converged to an accurate model of the new do-\nmain. This issue is particularly severe when using\nsmall datasets to ﬁne tune.\nIn fact, in the evalua-\ntion setting described above, the best result would be\nachieved by performing no ﬁne tuning at all. Indeed,\nin this case, both the ground truth and the OCR mod-\nels would be perfectly identical. A way around is to\nuse extrinsic evaluation, and consider (the similarity\nin performance on) downstream tasks instead. While\nextrinsic evaluation has its merits, it does not allow\nto perform a direct assessment of a language model,\nbut only one limited to its usefulness for downstream\ntasks. We therefore consider extrinsic evaluation to\nbe complementary to the approach we pursue here.\n2\nRELATED WORK\nNeural Language Models\nVector representations\nof linguistic units, referred to as embeddings, have\nbeen instrumental in the success of modern neural\nNLP. On the one hand, as statistical models of lan-\nguage when trained on unsupervised objectives and,\non the other hand, as components in larger NLP ar-\nchitectures Xia et al. (2020); Qiu et al. (2020). Very\npopular models include Word2Vec Mikolov et al.\n(2013b,a) and BERT Devlin et al. (2019). A com-\nmon theme of neural language modelling research\nover time seems to be that increasing larger param-\neters and datasets lead to better results Brown et al.\n(2020); Raffel et al. (2020). More recently, attention\nis increasing for low-resource languages which do not\nyet possess the amount of data or resources which are\nreadily available for, say, English Ruder (2020); Hed-\nderich et al. (2021). As a consequence, promising\nwork is emerging on effective small language mod-\nels Schick and Sch¨utze (2021).\nLanguage Models and Noise.\nA challenge in lan-\nguage modelling which is often – yet not necessar-\nily – occurring in low-resource settings is noise. We\ncan consider noise as unwanted errors in the texts, in-\ntroduced by processing steps. Examples include er-\nrors in audio to text recognition or in transcription.\nNoise can be caused by humans, machines, commu-\nnication channels; it can be systematic or not. Sub-\nstantial work on noise and language models has so\nfar focused on robustness to adversarial attacks Pruthi\net al. (2019). Some approaches to language modelling\ncan indeed be more resilient to noise than others,\ndespite often having been introduced for other rea-\nsons (primarily dealing with out-of-vocabulary words\nand being language agnostic). BERT, for example,\nhas been proven sensitive to (non-adversarial) human\nnoise Sun et al. (2020); Kumar et al. (2020). Exam-\nples of models that can be more resilient to noise in-\nclude typological language models Gerz et al. (2018);\nPonti et al. (2019), sub-word or character-level lan-\nguage models Kim et al. (2016); Zhu et al. (2019);\nMa et al. (2020), byte-pair encoding Sennrich et al.\n(2016), and their extension in recent tokenization-free\nmodels (Heinzerling and Strube, 2018; Clark et al.,\n2021; Xue et al., 2021), yet their use as noise-resilient\nlanguage models remains to be fully assessed.\nAssessing the Impact of OCR Noise.\nA grow-\ning body of work is focused on assessing and mit-\nigating the impact of OCR noise.\nAn area of ac-\ntive work is that of post-OCR correction, which at-\ntempts to automatically improve on a noisy text after\nOCR H¨am¨al¨ainen and Hengchen (2019); Boros et al.\n(2020); Nguyen et al. (2020); Todorov and Colavizza\n(2020). Several recent contributions have assessed the\nimpact of OCR noise using extrinsic evaluation and\nconsidering a variety of downstream tasks, includ-\ning topic modelling, text classiﬁcation, named en-\ntity recognition and information retrieval, among oth-\ners Hamdi et al. (2019); Hill and Hengchen (2019);\nvan Strien et al. (2020); Boros et al. (2020). While\nmore systematic comparisons are needed, the general\ntrend seems to indicate that OCR texts are often ‘good\nenough’ to be used for downstream tasks.\nIt is our intent to contribute to this growing body\nof work by assessing the impact of OCR noise on a\nselection of mainstream language models, in view of\ninforming their use and future development.\n3\nEXPERIMENTAL SETUP\nIn this section we introduce the language models\nwhich we consider for this study and present the data\nwe used for our experiments. Lastly, we detail the\nevaluation procedure to assess their resilience to OCR\nnoise.\n3.1\nLanguage Models\nThere are several popular techniques for language\nmodelling.\nModern-day language models are typi-\ncally vector-based: they map linguistic units (e.g., to-\nkens) to vectors of a given size. How these vectors\nare updated and learned during training depends on\nthe model at hand and the task(s) it focuses on. In\norder to compare different and popular approaches,\nin this study we consider Word2Vec, namely Skip-\nGram with Negative Sampling (SGNS) and Continu-\nous Bag-of-words (CBOW) Mikolov et al. (2013b,a),\nand GloVe Pennington et al. (2014). Furthermore, we\nconsider attention-based models able to make use of\nthe context of an occurrence at inference time and\nnot just at training time, namely BERT Devlin et al.\n(2019) and ALBERT Lan et al. (2020) (the latter a\nvery fast variant of the former).\nFinally, we also\ninclude a language model based on co-occurrence\ncounts re-weighted using Positive Pointwise Mutual\nInformation (PPMI), as a baseline Church and Hanks\n(1989).\nPMI.\nis a measure of association that quantiﬁes the\nlikelihood of a co-occurrence of two words taking into\naccount the independent frequency of the two words\nin the corpus. Positive PMI (PPMI) leaves out nega-\ntive values, under the assumption that they might cap-\nture unreliable and therefore uninformative statistics\nabout the word pair. Formally, PMI and PPMI are\ncalculated as follows:\nPMI(w,c) = log2\nP(w,c)\nP(w)P(c)\n(1)\nPPMI(w,c) = max(PMI(w,c),0)\n(2)\nWhere P(w,c) is the probability of the word w oc-\ncurring together with word c, and the denominator\ncontains the individual probabilities of these words\noccurring independently.\nSkip-gram.\npositions each word in a vector space\nsuch that similar words are closer to each other. This\nis achieved by using the self-supervised task of pre-\ndicting the context words given a speciﬁc target word.\nTo improve the speed of convergence and ease com-\nputation, we use negative sampling. If we consider\nthe original target and context words as positive ex-\namples, we sample negative ones at random from the\ncorpus during training.\nCBOW.\nuses the mirror task compared to Skip-\ngram, by predicting the target word using context\nwords. For both models, the amount of context words\nis conﬁgurable and called window size of the model.\nGloVe.\nis similar to PPMI, in that training is\nperformed on the aggregated word-to-word co-\noccurrence matrix from a corpus, following the intu-\nition that these encode a form of meaning.\nBERT.\nstands for Bidirectional Encoder Represen-\ntations for Transformers that, at the time of introduc-\ntion, gave state-of-the-art results on a wide variety of\ntasks. BERT is a multi-layered bi-directional trans-\nformer encoder. It uses self-supervised tasks such as\nmasked language modelling and next sentence predic-\ntion. We apply the former to train our BERT model.\nThe original model uses sub-word tokenization and it\ngenerates contextualised word representation at infer-\nence time.\nALBERT.\nuses most of BERT’s design choices.\nHowever, this model differs by introducing two\nparameter-reduction techniques that lower memory\nconsumption and drastically decrease training time.\nWe implement the overall pipeline for our exper-\niments and PPMI ourselves. For CBOW and SGNS\nwe use a popular Python library, Gensim Rehurek and\nSojka (2010), while for BERT and ALBERT we rely\non HuggingFace Wolf et al. (2020). We remark again\nthat we always do training from scratch, without\nusing any pre-trained model.\nFor all models, we use standard default for\nthe used architecture tokenization.\nFor Skip-gram,\nCBOW and GloVe we use simple rules, such as clean-\ning digits, punctuation and multiple white-spaces. For\nBERT and ALBERT we use Byte-level BPE tok-\nenizer, as introduced by OpenAI and which works on\nsub-word level (Wang et al., 2020). For these models,\nwe additionally split the data at 500 characters due\nto the design limitations in the original implementa-\ntion. Finally, we use default conﬁgurations for the\ntransformers, namely a vocabulary size of 30,522 for\nBERT and of 30,000 for ALBERT, and an vector size\nequal to 768. For ALBERT, we set the number of\nhidden layers and attention heads to 12 and the inter-\nmediate size to 3072 so that these are equal to their\ncounterparts in BERT.\n3.2\nData\nWe make use of the datasets provided by the In-\nternational Conference on Document Analysis and\nRecognition (ICDAR) 2017 Chiron et al. (2017) and\n2019 Rigaud et al. (2019) competitions on Post-OCR\ntext correction. These two datasets combined include\nten European languages of which we consider four\nin this study. The data is provided in three versions:\nOCR, aligned OCR and ground-truth. We make use\nof the aligned OCR and ground-truth versions for the\npurpose of this study and combine the training and\nevaluation sets together.\nFrom Table 1 we show how different the four lan-\nguages are in terms of corpus size. Dutch and En-\nglish languages contain comparatively fewer docu-\nments but of longer average size, while French and\nGerman have more, usually shorter documents. In or-\nder to assess whether having more data for languages\nwith a smaller corpus would alter our results, we ex-\nperimented with adding data from the National Li-\nbrary of Australia’s Trove newspaper archive 1 to the\nEnglish corpus, but discovered that it does not lead to\nany differences in the outcomes of our experiments\nwhen tested on several of our conﬁgurations.\nWe\ntherefore leave this out and only report results using\nICDAR 2017+2019 data in what follows.\nOCR error rates, per language and averaged over\ndocuments are given in Table 2, alongside the dis-\ntribution of character error rates in Figure 1a and of\nword error rates in Figure 1b. The error rates on char-\nacter level are calculated by comparing characters on\nthe same position in the OCR and aligned ground-\ntruth versions. Word error rates are calculated fol-\nlowing the de facto standard approach of word errors\nto processed words Morris et al. (2004). Documents\nwhich are having misaligned OCR and ground-truth\nversions are excluded from the error rates calculation.\nIt is worth noting that these represent a signiﬁcant\nproportion for the German language (Table 1).\nBefore using each corpus for training, we perform\nthe following pre-processing steps for all of our con-\nﬁgurations except for BERT and ALBERT. We remove\nnumbers and punctuation from the data, lowercase all\n1https://trove.nla.gov.au.\n(a) Character-level\n(b) Word-level\nFigure 1: OCR error rates per language, as distribution of\ndocument scores.\ncharacters, substitute multiple white-spaces with one,\nalso removing leading and trailing white-spaces in the\nprocess, and ﬁnally split the different words into to-\nkens. We then build the vocabulary for each version\nof each corpus – ending up with two versions per cor-\npus: OCR and ground truth – and remove all tokens\nthat occur less than ﬁve times overall (per version).\nWe pick this number following previous research and\nin order to reduce the computational requirements of\ntraining our models. For transformer-based models\ninstead, we only split words into sub-word tokens and\nreplace multiple white-spaces with one.\nThese are\ntypical pre-processing steps in view of presenting as\nmuch contextual information as possible to the model.\n3.3\nEvaluation\nOur goal is to compare two versions of the same\nmodel conﬁguration, each trained from scratch, on the\nOCR and the ground-truth versions of the same cor-\npus. We remind the reader that our evaluation there-\nfore does not provide any indication of the relative\nbeneﬁt of using one language model versus another\nin terms of their capacity to represent language or\nTable 1: Dataset statistics calculated on ground truth corpora versions. The column Aligned reports the number of documents\nwhere OCR versions and ground truth are perfectly aligned. The column Split reports the number of documents after document\nsplitting for transformer-based models, which require equally-sized data points.\nDocuments\nCharacters per doc.\nTotal\nTotal\nAligned (% of total)\nSplit\nAvg\nMin\nMax\ncharacters\nDutch\n150\n149 (99.3%)\n5346\n4593\n42\n16,028\n688,934\nEnglish\n963\n951 (98.8%)\n51,689\n6866\n2\n869,953\n6,612,108\nFrench\n3993\n3616 (90.6%)\n84,676\n2660\n2\n195,177\n10,620,966\nGerman\n10,032\n1738 (17.3%)\n128,662\n1581\n126\n16,187\n15,856,445\nTable 2: OCR error rates per language, averaged over doc-\numents.\nLanguage\nError rate level\nCharacter\nWord\nDutch\n0.286\n0.536\nEnglish\n0.075\n0.146\nFrench\n0.064\n0.193\nGerman\n0.240\n0.813\nperform on downstream tasks. Rather, we assess and\ncompare the resilience of each model to OCR noise.\nOur evaluation procedure is composed of the fol-\nlowing steps:\n1. For each corpus/language, we start by taking the\nintersection of the words that are part of all vo-\ncabularies/models.\nFor transformer-based mod-\nels, which do not have word-level vocabularies,\nwe use the vocabulary intersection from the other\nmodels.\n2. For transformer-based models, we output the hid-\nden states for all words from the intersected vo-\ncabulary. For words which are split into multiple\nsub-word tokens due to BERT and ALBERT tok-\nenization, we take the mean values. This approach\nleaves out the contextual information from the in-\nferred embeddings, yet it ensures proper compar-\nison with different architectures.\n3. For each corpus/language, model conﬁguration,\nand word in the vocabulary intersection, we com-\npute the cosine similarity with every other word\nin the vocabulary intersection.\n4. We then compare the amount of overlap in the\ntop N fraction of closest words (neighbors) in the\ntwo versions of the same corpus (OCR and ground\ntruth). In this way, we are able to assess to what\nextent the two models agree on what is the neigh-\nborhood for each word in the vocabulary intersec-\ntion.\n5. Since different corpora/languages possess varying\nvocabulary sizes, we use the percentage over the\ntotal dataset-speciﬁc vocabulary intersection. N\nis thus ranging from 0.01 (1%) to 1.0 (100% top\nneighbors).\n6. Following Gonen et al. (2020), we use neighbor\noverlap as our main evaluation metric.\nThat is\nthe proportion of overlapping neighbours for any\nword in the vocabulary intersection, deﬁned using\nthe following formula:\noverlap@k(rOCR,rtruth) = |rk\nOCR ∩rk\ntruth|\nk\n(3)\nWhere rk\nOCR are the top-k neighbors of word r in\nthe OCR model, and rk\ntruth in the ground truth\nmodel respectively. k is taken to correspond to\nthe top N fraction of neighbors for each speciﬁc\nvocabulary intersection.\n7. For example, if we have a vocabulary intersection\nof size 1000 and evaluate at N = 0.01, we would\ntake the k = 10 closest words for each word. If\non average 5 out of 10 words correspond in the\ntwo versions/models, we would have an average\n0.5 overlap score (Equation 3).\nFurther empirical settings are as follows. In agree-\nment and to ensure compatibility with previous work\non Word2Vec embeddings, we use embedding size\nof 300 for English, German and French languages\nand 320 for Dutch Tulkens et al. (2016). We use the\nAdamW optimizer Loshchilov and Hutter (2018) for\ntransformer based models. We further assess models\nusing two learning rates – which we label fast and\nslow – that are comparatively higher/lower in order\nto verify the effect of learning speed on our evalua-\ntion. Since BERT and ALBERT usually beneﬁt from\nlower learning rates compared to simpler models such\nas CBOW and Skip-gram, we adjust accordingly. The\nlearning rates which correspond to fast and slow for\neach model are shown in Table 3, they have been se-\nlected following commonly used default values in the\nliterature.\nTo control for the stochasticity of the training pro-\ncess, we show all results by averaging three differ-\nFigure 2: Neighbourhood overlaps (Dutch).\n95% boot-\nstrapped conﬁdence intervals are provided at .01 intervals.\nFigure 3: Neighbourhood overlaps (English). 95% boot-\nstrapped conﬁdence intervals are provided at .01 intervals.\nent runs for each hyper-parameter conﬁguration and\nmodel type.\nTable 3: Model parameter size comparison and default\nlearning rates.\nModel\nNumber of\nparameters\nLearning rates\nFast\nSlow\nPPMI\nN/A\nGloVe\nN/A\nCBOW\n1.7M\n1e−3\n1e−4\nSkip-Gram\n7.5M\nBERT\n108M\n1e−4\n1e−5\nALBERT\n12M\n4\nRESULTS\nOur results are shown in a series of mirrored plots,\none per language. The plots show the neighborhood\noverlap (Eq. 3) on the y axis, at varying values of N\nfrom 0.01 (1%) to 1.0 (100%), on the x axis. Nat-\nurally, higher values of N are somewhat less interest-\ning and more trivial than lower values of N (overlap of\nFigure 4: Neighbourhood overlaps (French). 95% boot-\nstrapped conﬁdence intervals are provided at .01 intervals.\nFigure 5: Neighbourhood overlaps (German). 95% boot-\nstrapped conﬁdence intervals are provided at .01 intervals.\nthe most similar neighbors), therefore each plot also\nshows a zoom-in inset for values on N between 0.01\n(1%) and 0.2 (20%). Conﬁdence intervals are added\nand point to the signiﬁcance of all results.\nStarting with Dutch, in Figure 2, we can appre-\nciate how CBOW (slow learning rate) and PPMI ap-\npear to be most resilient to OCR noise, followed by\nSkip-gram (fast learning rate) and GloVe. Every other\nmodel conﬁguration performs much worse at lower\nvalues of N. Importantly, the Dutch corpus we use\nis small in comparison to other corpora, and contains\npoor quality OCR.\nNext, we show results for English in Figure 3. The\nEnglish corpus has a good quality OCR and is of av-\nerage size in our study. In this setting, PPMI followed\nby Skip-gram models perform best, notwithstanding\na good performance of BERT (slow learning rate) at\nvery low values of N. All other models play catch-up.\nThe results are quite clear for French, in Figure 4,\nwhich is a corpus of size and quality comparable to\nthose of English. For French, Skip-gram, CBOW and\nPPMI models perform consistently better in resist-\ning the impact of OCR noise. Compared, contextual\nBERT and ALBERT are lagging behind, along with\nGloVe.\nLastly, we report results for German in Figure 5.\nThe German corpus is the largest in size, but the worst\nin terms of OCR quality. The low OCR quality clearly\nshows in the comparatively lower overlap scores over-\nall.\nSurprisingly, for German the best performing\nmodels are BERT and ALBERT, in particular on a\nslow learning rate. It is worth noting that a fast Skip-\ngram conﬁguration is also close.\nOur results show clear trends. Firstly, the impact\nof OCR quality is overall quite signiﬁcant: all models\ntrained on OCR data diverge from ground truth. Lan-\nguage models trained on lower OCR quality corpora\nconsistently reach lower overlap scores, as shown for\nDutch and German. For example, the best perform-\ning models for French, which has good OCR qual-\nity, reach over 80% overlap at low N (5%), while this\noverlap score is only reached at very high values for\nGerman (55%). The effect of the size of a corpus does\nnot seem to be signiﬁcant, yet results for German, the\nonly language where transformer-based models out-\nperform the competition, warrant further scrutiny in\nthis respect. In terms of language models, simpler\nseems to be better.\nPPMI, Skip-gram and CBOW\nmodels consistently perform above transformer-based\nmodels (BERT, ALBERT) in terms of resilience to\nOCR noise. The only exception in this respect is Ger-\nman, which is plagued by OCR errors but is also the\nlargest corpus available. Furthermore, BERT consis-\ntently outperforms ALBERT, hinting at the fact that\nthe gains in training speed come at a cost. GloVe does\nnot appear to be particularly resilient to OCR noise\neither. Lastly, the variety and lack of consistency of\nresults when using fast and slow learning rates un-\nderlines the importance of choosing the right hyper-\nparameters.\nWe end our results by highlighting a set of limita-\ntions which constitute interesting directions for future\nwork. Firstly, our results hold within the remits of\nthe datasets and models we focused on: using more\naligned data – which is unfortunately costly to create\n–, in more languages of comparable corpus sizes, and\nwith more varied contents and OCR quality would all\nbe useful future contributions. Furthermore, the qual-\nity of the ground truth itself, which we took here for\ngranted, might warrant further scrutiny. Future work\ncould also focus on hyper-parameter ﬁne tuning in\norder to reach the maximum performance with any\ngiven model, as well as on assessing how many data\nare necessary to reach a certain desired result with a\ngiven language model. Next, as we discussed at the\nbeginning, our results would beneﬁt from a comple-\nmentary study focused on the extrinsic evaluation of\nthe impact of OCR on language models, in particular\nwhen used as components for machine learning archi-\ntectures focused on downstream tasks. Related to this\npoint, we decided not to consider pre-trained models:\napproaching the same research question via extrinsic\nevaluation would allow to overcome such limitation.\n5\nCONCLUSION\nWe have assessed the impact of OCR noise on a\nvariety of language models. Using data in Dutch, En-\nglish, French and German, of different sizes and OCR\nqualities, we considered two aligned versions of the\nsame corpus, one OCRed and one manually corrected\n(ground truth). Two identical instances of each lan-\nguage model were in turn trained from scratch over\nthese two versions of the same corpus, and the simi-\nlarity of the resulting vector spaces was assessed us-\ning word neighborhood overlap. This approach al-\nlowed us to assess and compare the resilience of each\nlanguage model to OCR noise, independently.\nWe show that OCR noise signiﬁcantly impacts\nlanguage models, with a steep degradation of re-\nsults for corpora with lower OCR quality. Further-\nmore, we show that ‘simpler’ language models, in-\ncluding PPMI and the Word2Vec family (Skip-gram\nand CBOW) are often more resilient to OCR noise\nthan recent transformer-based models (BERT, AL-\nBERT). We also show that the choice of key hyper-\nparameters, such as the learning rate, signiﬁcantly im-\npacts results as well. The size of a corpus might also\nbe an important factor, as suggested by transformer-\nbased models performing best with the largest corpus\navailable (German), yet more experiments are needed\nto untangle its effects.\nWhile several limitations and opportunities for fu-\nture work have been discussed, we believe the two\nmost important next steps to be the development of\nmultilingual evaluation corpora of similar size and\nOCR quality in order to study the impact of OCR\nnoise more systematically, and the need to comple-\nment our study with an extrinsic assessment on down-\nstream tasks making use of (possibly pre-trained) lan-\nguage models.\nNevertheless, we have shown that\nOCR noise poses a signiﬁcant obstacle to language\nmodelling. In the presence of small datasets, simpler\nmodels including PPMI and Word2Vec are more re-\nsilient to OCR noise than transformer-based models\ntrained from scratch.\nCODE AND DATA AVAILABILITY\nAll data is openly provided by the International\nConference on Document Analysis and Recognition\n(ICDAR) 2017 Chiron et al. (2017) and 2019 Rigaud\net al. (2019) competitions on Post-OCR text correc-\ntion.\nOur code base is publicly available and described\nat https://doi.org/10.5281/zenodo.5799211 (Todorov\nand Colavizza, 2021).\nREFERENCES\nBanar, N., Lasaracina, K., Daelemans, W., and Keste-\nmont, M. (2020). Transfer Learning for Digital Heritage\nCollections: Comparing Neural Machine Translation at\nthe Subword-level and Character-level.\nIn Proceed-\nings of the 12th International Conference on Agents and\nArtiﬁcial Intelligence, pages 522–529, Valletta, Malta.\nSCITEPRESS - Science and Technology Publications.\nBoros, E., Hamdi, A., Linhares Pontes, E., Cabrera-Diego,\nL. A., Moreno, J. G., Sidere, N., and Doucet, A. (2020).\nAlleviating Digitization Errors in Named Entity Recog-\nnition for Historical Documents. In Proceedings of the\n24th Conference on Computational Natural Language\nLearning, pages 431–441, Online. Association for Com-\nputational Linguistics.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,\nWinter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. (2020).\nLanguage Models are Few-Shot Learners. In Larochelle,\nH., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H.,\neditors, Advances in Neural Information Processing Sys-\ntems, volume 33, pages 1877–1901. Curran Associates,\nInc.\nChiron, G., Doucet, A., Coustaty, M., and Moreux, J.-P.\n(2017).\nICDAR 2017 Competition on Post-OCR Text\nCorrection. In 2017 14th IAPR International Conference\non Document Analysis and Recognition (ICDAR), vol-\nume 1, pages 1423–1428. IEEE.\nChurch, K. and Hanks, P. (1989). Word Association Norms,\nMutual Information, and Lexicography. In 27th Annual\nMeeting of the Association for Computational Linguis-\ntics, pages 76–83, Vancouver, British Columbia, Canada.\nAssociation for Computational Linguistics.\nClark, J. H., Garrette, D., Turc, I., and Wieting, J. (2021).\nCanine: Pre-training an Efﬁcient Tokenization-Free En-\ncoder for Language Representation.\nColl Ardanuy, M., Nanni, F., Beelen, K., Hosseini, K., Ahn-\nert, R., Lawrence, J., McDonough, K., Tolfo, G., Wilson,\nD. C., and McGillivray, B. (2020). Living Machines: A\nstudy of atypical animacy. In Proceedings of the 28th\nInternational Conference on Computational Linguistics,\npages 4534–4545, Barcelona, Spain (Online). Interna-\ntional Committee on Computational Linguistics.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2019). BERT: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding.\nIn Proceedings\nof the 2019 Conference of the North, pages 4171–4186,\nMinneapolis, Minnesota. Association for Computational\nLinguistics.\nEhrmann, M., Colavizza, G., Rochat, Y., and Kaplan, F.\n(2016). Diachronic Evaluation of NER Systems on Old\nNewspapers.\nEhrmann,\nM.,\nRomanello,\nM.,\nFl¨uckiger,\nA.,\nand\nClematide, S. (2020). Overview of CLEF HIPE 2020:\nNamed Entity Recognition and Linking on Historical\nNewspapers. In Arampatzis, A., Kanoulas, E., Tsikrika,\nT., Vrochidis, S., Joho, H., Lioma, C., Eickhoff, C.,\nN´ev´eol, A., Cappellato, L., and Ferro, N., editors, Exper-\nimental IR Meets Multilinguality, Multimodality, and In-\nteraction, volume 12260, pages 288–310. Springer Inter-\nnational Publishing, Cham. Series Title: Lecture Notes\nin Computer Science.\nGerz, D., Vuli´c, I., Ponti, E. M., Reichart, R., and Ko-\nrhonen, A. (2018). On the Relation between Linguis-\ntic Typology and (Limitations of) Multilingual Lan-\nguage Modeling.\nIn Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 316–327, Brussels, Belgium. Association\nfor Computational Linguistics.\nGonen, H., Jawahar, G., Seddah, D., and Goldberg, Y.\n(2020). Simple, Interpretable and Stable Method for De-\ntecting Words with Usage Change across Corpora. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics, page 538–555. As-\nsociation for Computational Linguistics.\nH¨am¨al¨ainen, M. and Hengchen, S. (2019). From the Paft\nto the Fiiture: a Fully Automatic NMT and Word Em-\nbeddings Method for OCR Post-Correction. In Proceed-\nings of the International Conference on Recent Advances\nin Natural Language Processing (RANLP 2019), pages\n431–436, Varna, Bulgaria. INCOMA Ltd.\nHamdi, A., Jean-Caurant, A., Sidere, N., Coustaty, M., and\nDoucet, A. (2019). An Analysis of the Performance of\nNamed Entity Recognition over OCRed Documents. In\n2019 ACM/IEEE Joint Conference on Digital Libraries\n(JCDL), pages 333–334, Champaign, IL, USA. IEEE.\nHedderich, M. A., Lange, L., Adel, H., Str¨otgen, J., and\nKlakow, D. (2021). A Survey on Recent Approaches for\nNatural Language Processing in Low-Resource Scenar-\nios. arXiv:2010.12309 [cs]. arXiv: 2010.12309.\nHeinzerling,\nB. and Strube,\nM. (2018).\nBPEmb:\nTokenization-free Pre-trained Subword Embeddings in\n275 Languages. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Evalua-\ntion (LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nHill, M. J. and Hengchen, S. (2019). Quantifying the impact\nof dirty OCR on historical text analysis: Eighteenth Cen-\ntury Collections Online as a case study. Digital Scholar-\nship in the Humanities, 34(4):825–843.\nKahle, P., Colutto, S., Hackl, G., and Muhlberger, G.\n(2017). Transkribus - A Service Platform for Transcrip-\ntion, Recognition and Retrieval of Historical Documents.\nIn 2017 14th IAPR International Conference on Docu-\nment Analysis and Recognition (ICDAR), pages 19–24,\nKyoto. IEEE.\nKim, Y., Jernite, Y., Sontag, D., and Rush, A. M. (2016).\nCharacter-Aware Neural Language Models. In Proceed-\nings of the Thirtieth AAAI Conference on Artiﬁcial In-\ntelligence, AAAI’16, pages 2741–2749. AAAI Press.\nevent-place: Phoenix, Arizona.\nKumar, A., Makhija, P., and Gupta, A. (2020). Noisy Text\nData: Achilles’ Heel of BERT. In Proceedings of the\nSixth Workshop on Noisy User-generated Text (W-NUT\n2020), pages 16–21, Online. Association for Computa-\ntional Linguistics.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.,\nand Soricut, R. (2020).\nALBERT: A Lite BERT for\nSelf-supervised Learning of Language Representations.\narXiv:1909.11942 [cs]. arXiv: 1909.11942.\nLoshchilov, I. and Hutter, F. (2018). Fixing Weight Decay\nRegularization in Adam.\nMa, W., Cui, Y., Si, C., Liu, T., Wang, S., and Hu, G.\n(2020).\nCharBERT: Character-aware Pre-trained Lan-\nguage Model.\nProceedings of the 28th International\nConference on Computational Linguistics.\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a).\nEfﬁcient estimation of word representations in vector\nspace. In Bengio, Y. and LeCun, Y., editors, 1st Inter-\nnational Conference on Learning Representations, ICLR\n2013, Scottsdale, Arizona, USA, May 2-4, 2013, Work-\nshop Track Proceedings.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. (2013b). Distributed representations of words\nand phrases and their compositionality. In Burges, C.\nJ. C., Bottou, L., Welling, M., Ghahramani, Z., and\nWeinberger, K. Q., editors, Advances in Neural Informa-\ntion Processing Systems, volume 26. Curran Associates,\nInc.\nMorris, A. C., Maier, V., and Green, P. (2004).\nFrom\nWER and RIL to MER and WIL: improved evaluation\nmeasures for connected speech recognition. In INTER-\nSPEECH, page 4.\nNguyen, D., Do˘gru¨oz, A. S., Ros´e, C. P., and de Jong, F.\n(2016). Computational Sociolinguistics: A Survey. Com-\nputational Linguistics, 42(3):537–593.\nNguyen, T. T. H., Jatowt, A., Nguyen, N.-V., Coustaty,\nM., and Doucet, A. (2020).\nNeural Machine Transla-\ntion with BERT for Post-OCR Error Detection and Cor-\nrection. In Proceedings of the ACM/IEEE Joint Confer-\nence on Digital Libraries in 2020, pages 333–336, Vir-\ntual Event China. ACM.\nPennington, J., Socher, R., and Manning, C. (2014). GloVe:\nGlobal Vectors for Word Representation. In Proceedings\nof the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP), volume 14, pages 1532–\n1543.\nPiotrowski, M. (2012). Natural Language Processing for\nHistorical Texts.\nSynthesis Lectures on Human Lan-\nguage Technologies, 5(2):1–157.\nPonti, E. M., O’Horan, H., Berzak, Y., Vuli´c, I., Reichart,\nR., Poibeau, T., Shutova, E., and Korhonen, A. (2019).\nModeling Language Variation and Universals: A Survey\non Typological Linguistics for Natural Language Pro-\ncessing. Computational Linguistics, 45(3):559–601.\nPruthi, D., Dhingra, B., and Lipton, Z. C. (2019). Com-\nbating adversarial misspellings with robust word recog-\nnition. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n5582–5591, Florence, Italy. Association for Computa-\ntional Linguistics.\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X.\n(2020). Pre-trained models for natural language process-\ning: A survey. Science China Technological Sciences,\n63(10):1872–1897.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Ex-\nploring the Limits of Transfer Learning with a Uniﬁed\nText-to-Text Transformer. Journal of Machine Learning\nResearch, 21(140):1–67.\nRehurek, R. and Sojka, P. (2010). Software Framework for\nTopic Modelling with Large Corpora. In Proceedings of\nLREC 2010 workshop New Challenges for NLP Frame-\nworks, pages 46–50, Valletta, Malta. University of Malta.\nRigaud, C., Doucet, A., Coustaty, M., and Moreux, J.-P.\n(2019).\nICDAR 2019 Competition on Post-OCR Text\nCorrection. In 2019 International Conference on Doc-\nument Analysis and Recognition (ICDAR), pages 1588–\n1593. IEEE.\nRogers, A., Kovaleva, O., and Rumshisky, A. (2020). A\nPrimer in BERTology:\nWhat We Know About How\nBERT Works. Transactions of the Association for Com-\nputational Linguistics, 8:842–866.\nRuder, S. (2019).\nNeural Transfer Learning for Natural\nLanguage Processing. PhD thesis, National University\nof Ireland, Galway.\nRuder, S. (2020). Why You Should Do NLP Beyond En-\nglish. http://ruder.io/nlp-beyond-english.\nSchick, T. and Sch¨utze, H. (2021).\nIt’s Not Just Size\nThat Matters: Small Language Models Are Also Few-\nShot Learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Technolo-\ngies, pages 2339–2352, Online. Association for Compu-\ntational Linguistics.\nSennrich, R., Haddow, B., and Birch, A. (2016). Neural Ma-\nchine Translation of Rare Words with Subword Units. In\nProceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Pa-\npers), pages 1715–1725, Berlin, Germany. Association\nfor Computational Linguistics.\nShoemark, P., Liza, F. F., Nguyen, D., Hale, S., and\nMcGillivray, B. (2019).\nRoom to Glo: A Systematic\nComparison of Semantic Change Detection Approaches\nwith Word Embeddings. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pages\n66–76, Hong Kong, China. Association for Computa-\ntional Linguistics.\nSun, L., Hashimoto, K., Yin, W., Asai, A., Li, J., Yu, P.,\nand Xiong, C. (2020). Adv-BERT: BERT is not robust\non misspellings! Generating nature adversarial samples\non BERT.\nTodorov, K. and Colavizza, G. (2020).\nTransfer Learn-\ning for Historical Corpora: An Assessment on Post-\nOCR Correction and Named Entity Recognition. In Pro-\nceedings of the Workshop on Computational Humanities\nResearch (CHR 2020) : Amsterdam, the Netherlands,\nNovember 18-20, 2020, CEUR Workshop Proceedings,\n1613-0073, 2723, pages 310–339, Amsterdam. Aachen:\nCEUR-WS.\nTodorov,\nK. and Colavizza,\nG. (2021).\nZenodo,\nktodorov/historical-ocr, An assessment of the impact of\nOCR noise on language models.\nTulkens, S., Emmery, C., and Daelemans, W. (2016). Eval-\nuating Unsupervised Dutch Word Embeddings as a Lin-\nguistic Resource. In Chair), N. C. C., Choukri, K., De-\nclerck, T., Grobelnik, M., Maegaard, B., Mariani, J.,\nMoreno, A., Odijk, J., and Piperidis, S., editors, Proceed-\nings of the Tenth International Conference on Language\nResources and Evaluation (LREC 2016), Paris, France.\nEuropean Language Resources Association (ELRA).\nvan Strien, D., Beelen, K., Ardanuy, M., Hosseini, K.,\nMcGillivray, B., and Colavizza, G. (2020). Assessing\nthe Impact of OCR Quality on Downstream NLP Tasks.\nIn Proceedings of the 12th International Conference on\nAgents and Artiﬁcial Intelligence, pages 484–496, Val-\nletta, Malta. SCITEPRESS - Science and Technology\nPublications.\nWang, C., Cho, K., and Gu, J. (2020).\nNeural machine\ntranslation with byte-level subwords.\nIn Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, vol-\nume 34, pages 9154–9160.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\nand et al. (2020). HuggingFace’s Transformers: State-of-\nthe-art Natural Language Processing. arXiv:1910.03771\n[cs]. arXiv: 1910.03771.\nXia, P., Wu, S., and Van Durme, B. (2020). Which *BERT?\nA Survey Organizing Contextualized Encoders. In Pro-\nceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages 7516–\n7533, Online. Association for Computational Linguis-\ntics.\nXue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S.,\nKale, M., Roberts, A., and Raffel, C. (2021). Byt5: To-\nwards a token-free future with pre-trained byte-to-byte\nmodels.\nZhu, Y., Heinzerling, B., Vuli´c, I., Strube, M., Reichart,\nR., and Korhonen, A. (2019).\nOn the Importance of\nSubword Information for Morphological Tasks in Truly\nLow-Resource Languages. In Proceedings of the 23rd\nConference on Computational Natural Language Learn-\ning (CoNLL), pages 216–226, Hong Kong, China. Asso-\nciation for Computational Linguistics.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2022-01-26",
  "updated": "2022-01-26"
}