{
  "id": "http://arxiv.org/abs/2202.02371v2",
  "title": "Boundary-aware Information Maximization for Self-supervised Medical Image Segmentation",
  "authors": [
    "Jizong Peng",
    "Ping Wang",
    "Marco Pedersoli",
    "Christian Desrosiers"
  ],
  "abstract": "Unsupervised pre-training has been proven as an effective approach to boost\nvarious downstream tasks given limited labeled data. Among various methods,\ncontrastive learning learns a discriminative representation by constructing\npositive and negative pairs. However, it is not trivial to build reasonable\npairs for a segmentation task in an unsupervised way. In this work, we propose\na novel unsupervised pre-training framework that avoids the drawback of\ncontrastive learning. Our framework consists of two principles: unsupervised\nover-segmentation as a pre-train task using mutual information maximization and\nboundary-aware preserving learning. Experimental results on two benchmark\nmedical segmentation datasets reveal our method's effectiveness in improving\nsegmentation performance when few annotated images are available.",
  "text": "Boundary-aware Information Maximization for\nSelf-supervised Medical Image Segmentation\nJizong Peng∗\nETS Montreal\njizong.peng.1@etsmtl.net\nPing Wang\nETS Montreal\nping.wang.1@ens.etsmtl.ca\nChristian Desrosiers\nETS Montreal\nchristian.desrosiers@etsmtl.ca\nMarco Pedersoli\nETS Montreal\nmarco.pedersoli@etsmtl.ca\nAbstract\nUnsupervised pre-training has been proven as an effective approach to boost various\ndownstream tasks given limited labeled data. Among various methods, contrastive\nlearning learns a discriminative representation by constructing positive and negative\npairs. However, it is not trivial to build reasonable pairs for a segmentation task in\nan unsupervised way. In this work, we propose a novel unsupervised pre-training\nframework that avoids the drawback of contrastive learning. Our framework\nconsists of two principles: unsupervised over-segmentation as a pre-train-task\nusing Mutual information maximization and boundary-aware preserving learning.\nExperimental results on two benchmark medical segmentation datasets reveal\nour method’s effectiveness in improving segmentation performance when few\nannotated images are available.\n1\nIntroduction\nSupervised deep learning approaches have achieved outstanding performance in a wide range of\nsegmentation tasks (Ronneberger et al., 2015; Badrinarayanan et al., 2017; Chen et al., 2018).\nHowever, these approaches often require a large amount of labeled images which are difﬁcult to\nobtain for medical imaging applications (Cheplygina et al., 2019; Peng & Wang, 2021). Unsupervised\nrepresentation learning (Jing & Tian, 2020; Liu et al., 2021) has emerged as an effective technique\nto boost the performance of a segmentation model without the need for annotated data. In such\ntechnique, a model is pre-trained to perform a given pretext task, for example puzzle-solving (Noroozi\n& Favaro, 2016; Taleb et al., 2021), rotation prediction (Komodakis & Gidaris, 2018; Gidaris et al.,\n2018), colorization (Zhang et al., 2016) or contrastive-based instance discrimination (Hjelm et al.,\n2018; Chen et al., 2020; He et al., 2020), and then ﬁne-tuned with a small set of labeled examples.\nAmong these self-supervised methods, contrastive learning has become a prevailing strategy for\npre-training medical image segmentation models (Chaitanya et al., 2020; Zeng et al., 2021; Peng\net al., 2021b). The core idea of this strategy is to learn, without pixel-wise annotations, an image\nrepresentation which can discriminate related images (e.g., two transformations of the same image)\nfrom non-related ones. Most contrastive learning approaches for segmentation apply a contrastive\nloss on the global representation of images, which typically corresponds to the features produced\nby the network’s encoder. Experimental results have shown that pre-training the encoder with this\nloss and then ﬁne-tuning the whole network with few labeled examples can lead to signiﬁcant\nimprovements (Peng et al., 2021b).\nRecent works have also demonstrated the beneﬁt of using contrastive learning on the decoder’s feature\nmaps during pre-training (Chaitanya et al., 2020; Peng et al., 2021b). In this case, the contrastive\n∗Corresponding author\nPreprint. Under review.\narXiv:2202.02371v2  [eess.IV]  16 Feb 2022\nloss is applied at each position of the feature map, which helps learn a local representation of the\nimage. However, choosing the pairs of positive and negative examples that need to be contrasted\nis more challenging for these dense feature maps without pixel-wise annotations. Firstly, the meta-\ninformation in medical data (e.g., subject ID, slice position, etc.) is typically found at the image level,\nand is therefore not applicable to local contrastive learning. To tackle this problem, current methods\nusually adopt a stride sampling strategy where, for a given anchor position in the feature map, local\nrepresentations located at a sufﬁcient distance are regarded as negative, while those that are close\nbut obtained under different image transforms are considered as positive (Chaitanya et al., 2020).\nAs we show in our experiments (see Section 4.1), this weak spatial prior unfortunately leads to low\nimprovements when used in pre-training. Another problem comes from the fact that medical images\nfor segmentation are often dominated by non-informative background regions, which reduces the\neffectiveness of local contrastive learning in this setting. Additionally, standard contrastive learning\ntechniques such as (Hjelm et al., 2018) typically need large batch sizes to have a sufﬁcient amount of\nhigh-quality negative example pairs. This constraint can be hard to meet in the case of learning dense\nfeatures. Despite important efforts, the improvement brought by local contrastive learning in medical\nimage segmentation remains relatively marginal (Chaitanya et al., 2020).\nIn this paper, we propose a boundary-aware information maximization approach for unsupervised\nrepresentation learning and experimentally demonstrate its usefulness for medical image segmentation.\nOur approach focuses on the dense features in the decoder of a segmentation network, and seeks to\ngroup them into clusters that correspond to meaningful regions in the image. The proposed learning\nobjective is based on the Information Invariant Clustering (IIC) method (Ji et al., 2019), but overcomes\nthree major drawbacks of this method: i) its optimization difﬁculty, caused in part by minimizing the\nentropy of cluster assignments, which often leads to sub-optimal solutions; ii) its lack of clustering\nconsistency for different random transformations; iii) the poor correspondence of clusters obtained\nby this method with region boundaries in the image. As illustrated in Fig. 1, our boundary-aware\ninformation maximization approach learns clusters that better correspond to relevant anatomical\nstructures of the image. This is achieved by improving IIC in two important ways. First, we augment\nthe learning objective of IIC, which maximizes the mutual information of local feature embeddings\nfor two different transformations of the same image, to make the joint cluster probability close to a\nuniform diagonal matrix. This improves optimization and leads to clusters that are well balanced\nand also consistent across different image transformations. Second, we propose a boundary-aware\nloss based on the cross-correlation between the spatial entropy of clusters and image edges, which\nhelps the learned cluster be more representative of important regions in the image. Our experimental\nresults reveal this loss to be especially effective for the segmentation of regions with irregular shape.\nCompared to contrastive learning, our method does not require to compute positive or negative pairs,\nand does not need a sophisticated sampling mechanism or large batch sizes. Through an extensive set\nof experiments involving four different medical image segmentation tasks, we demonstrate the high\neffectiveness of our unsupervised representation learning method for pre-training a segmentation\nmodel, before ﬁne-tuning it with few labeled images. Our results show the proposed method to\noutperform by a large margin several state-of-the-art self-supervised and semi-supervised approaches\nfor segmentation, and to reach a performance close to full supervision with only a few labeled\nexamples.\n2\nThe proposed method\nIn unsupervised representation pre-training, we are given a set of N images D = {xi}N\ni=1, with\nxi ∈RΩ, where Ωis the image space. We seek to learn a useful representation by pre-training a\ndeep segmentation network fθ(·) = fD(fE(·)) comprised of encoder fE(·) and a decoder fD(·). In\nour setting, a good representation can boost segmentation performance when ﬁne-tuning the whole\nnetwork with very limited labeled data. To help understand our method, we summarize in Table 1 the\nmain notations used in the paper.\nOur representation operates on dense embeddings taken from some intermediate layer of the decoder.\nOur goal is to group these local embeddings into clusters reﬂecting meaningful anatomical structures\nin the input images, without requiring any labels. Three separate loss functions are used to achieve\nthis goal. The ﬁrst loss maximizes the MI between corresponding local feature embeddings obtained\nfrom an input image and its transformed version. Since computing MI between continuous variables\nis complex, as in recent works (Ji et al., 2019; Peng et al., 2021a), we project features to a discrete\n2\nTable 1: Notations used in the paper\nImage dataset:\nD = {xi ∈RΩ}N\ni=1\nPixel index:\nΩ= [1, . . . , W × H]\nDense embedding index:\nΩ′ = [1, . . . , W/N × H/N]\n(K−1)-simplex:\n∆K = {p ∈[0, 1]K, P\nk\npk = 1}\nDense embedding:\ns = s(x) ∈RΩ′×C\nCluster projection:\ng(s) ∈∆Ω′×K\nImage transform:\nT (·)\nCluster probabilities:\nbpi = g(s(T (xi))), epi = g(T (s(xi)))\nCluster marginals:\nbp =\n1\nN\nPN\ni=1 bpi, ep =\n1\nN\nPN\ni=1 epi\nJoint distribution:\nPjoint =\n1\nN\nPN\ni=1 bpi · ep⊺\ni\nEntropy:\nH(X) = −EX[log p(X)]\nJoint entropy:\nHjoint(X, Y ) = −EX,Y [log p(X, Y )]\nspace representing clusters, where MI is easy to obtain. However, maximizing the MI between cluster\nassignments has two important drawbacks. Firstly, it assumes that the number of clusters is known in\nadvance and that these clusters are balanced (i.e., represent regions of the same size in the image).\nSecondly, as it involves minimizing entropy, the direct optimization of MI often leads to poor local\nminima, as the network becomes quickly conﬁdent in cluster assignments that are not useful (see\nthe clusters in Figure 1, for α=0). Our ﬁrst loss term addresses this problem by combining two\ncomplementary objectives: 1) minimizing the entropy of the cluster assignment joint distribution,\nwhich encourages clusters to be balanced and conﬁdent but is also ﬂexible to ignore some irrelevant\nclusters; 2) making the matrix of this joint distribution close to a diagonal matrix, which helps the\noptimization avoid poor minina and learn a better representation. Another problem with the simple\nMI maximization approach for unsupervised representation learning is that the clusters may not align\nwith geometric cues such as edges in the input image. In the second loss of our model, we tackle this\nproblem by forcing the regions with high cluster entropy, which corresponds to boundaries between\nclusters, to be correlated with edges in the image. Finally, to help the network capture the global\ncontext of images, we include a contrastive learning loss that exploits available meta-labels (e.g.,\nslice position in a MRI volume) to make the global features obtained by the encoder fE similar for\nimages with the same meta-label. We present a conceptual diagram of our proposed method in Fig. 4\nof the Appendix and detail the three loss functions in the following sub-sections.\nα = 0.0\nα = 0.5\nα = 1.0\nFigure 1: Inﬂuence of α on joint matrix Pjoint (ﬁrst row), cluster assignment (second row), and the\nuncertainty of the cluster (third row). The ﬁrst column shows the joint matrix before optimization,\nthe input image and the groud-truth segmentation respectively. Using a combination of MI and\ncross-entropy loss (α = 0.5) provides the most meaningful unsupervised segmentation.\n3\nImage\nGround Truth Super-Pixel\nIMSAT\nIIC\nOurs\n(α=0.0)\nOurs\n(α=0.25)\nOurs\n(α=0.5)\nOurs\n(α=0.75)\nOurs\n(α=1.0)\nFigure 2: Visual inspection of cluster assignments for unsupervised pre-training\n2.1\nImproved MI-based loss for dense pre-training\nWe seek to cluster the dense embeddings in feature maps s taken from a given hidden layer of the\ndecoder fD(·). Following (Peng et al., 2021a), we use mutual information maximization to perform\nclustering. The MI between two random variables X and Y (i.e., the cluster assignment for two\nimages) corresponds to the KL divergence between their joint distribution p(X, Y ) and the product\nof their marginal distributions p(X) and p(Y ):\nI(X, Y ) = DKL\n\u0000p(X, Y ) || p(X) p(Y )\n\u0001\n(1)\nAlternatively, MI can also be deﬁned as the difference between the combined entropy of marginals\nand the entropy of the joint distribution:\nI(X, Y ) = H(X) + H(Y ) −H(X, Y )\n= −EX[log EY [p(X, Y )]] −EY [log EX[p(X, Y )]] + EX,Y [log p(Y, X)]\n(2)\nwhere H(·) is the entropy of the variable. This deﬁnition reveals that maximizing MI leads to high-\nentropy (uniform) distributions for X and Y , thus avoiding trivial solutions assigning all examples to\na single cluster. It also results in a low entropy of the joint distribution, corresponding to conﬁdent\ncluster assignments.\nLet bpi = g(s(T (xi))) and epi = g(T (s(xi))) be cluster probabilities in feature maps from a given\nlayer of the decoder, obtained by applying a random transformation T (·) on the input image xi or the\nfeature maps s(xi). Function g is a 1×1 convolutional layer followed by a K-way softmax projecting\nthe feature maps to a distribution over K clusters. As in IIC, we estimate the joint distribution using\nthe average outer product between cluster probabilities bpi and epi:\nPjoint = 1\nN\nN\nX\ni=1\nbpi · ep⊺\ni .\n(3)\nPjoint thus has a dimensionality of K×K, and P(j,k)\njoint is the joint probability of assigning s(T (xi)) to\ncluster j and T (s(xi)) to cluster k. Following Equ. (2), the MI between the corresponding random\nvariables X and Y can be written as\nI(X, Y ) = H(bp) + H(ep) −H(Pjoint)\n(4)\nwhere bp= 1\nN\nP\ni\nbpi, ep= 1\nN\nP\ni\nepi are the cluster marginals which can computing by summing over\nthe rows or columns of the joint distribution matrix. Maximizing the entropy of marginals encourages\nthe network to assign an even number of samples to each cluster, and avoids trivial solutions where\nmost clusters are empty. On the other hand, minimizing the entropy of the joint, H(Pjoint), forces the\nnetwork to have conﬁdent cluster assignments.\nClustering the dense embeddings by maximizing MI poses two optimization problems. First, since\nwe are minimizing the entropy of the joint, the network can get stuck in conﬁdent but incorrect cluster\n4\nassignments which remain the same throughout optimization. Another problem stems from the fact\nthat MI is invariant to the ordering of clusters, hence any permutation of the joint distribution matrix\nyields an equivalent solution. The challenge of maximizing MI is illustrated in the ﬁrst row of Fig. 1,\nwhere the left-most image is the initial joint matrix Pjoint before optimization and the one in the\nsecond column (α = 0) is Pjoint after maximizing MI. We see that only a few clusters are actually\nused, making the entropy of marginals low and therefore also the MI.\nTo alleviate these problems, we consider the entropy of the joint distribution, given by\nH(Pjoint) = −\nK\nX\nj=1\nK\nX\nk=1\nP(j,k)\njoint log P(j,k)\njoint .\n(5)\nA solution where cluster assignments are balanced, conﬁdent and perfectly consistent across\ntransformations, would give a joint distribution matrix with diagonal elements equal to 1/K and\noff-diagonal elements to 0. To guide the optimization toward this desirable solution, we introduce a\npseudo-label of the joint matrix Ppseud = 1\nK IK, where IK is the K×K identity matrix, and modify\nthe entropy of the joint as follows:\nH′\nα(Pjoint) = −\nK\nX\nj=1\nK\nX\nk=1\n\u0000(1 −α) · P(j,k)\njoint + αP(j,k)\npseud\n\u0001\nlog P(j,k)\njoint\n(6)\nIn this modiﬁed formulation, α is a mixing coefﬁcient ranging from 0 to 1. If α equals to 0, H′\nα(Pjoint)\nreduces to H(Pjoint), while α = 1 corresponds to a cross-entropy loss guiding the joint matrix towards\nthe pre-deﬁned diagonal solution Ppseud.\nSince the joint distribution matrix is computed over a batch of examples, minimizing the cross-entropy\nbetween Ppseud and Pjoint is not the same as minimizing the cross-entropy between individual cluster\nassignments bpi and epi. Nevertheless, a relationship can be derived between these two concepts, as\ndescribed in the following proposition.\nProposition 2.1. The term added in (6) corresponds the cross-entropy between the diagonal joint\nPpseud = 1\nK IK and Pjoint, which is bounded as follows:\nlog K ≤H( 1\nK IK, Pjoint) ≤1\nN\nN\nX\ni=1\nH(u, bpi) + H(u, epi),\n(7)\nwhere u is the vector such that uk = 1\nK for k = 1, . . . , K.\nProof. See Appendix C.\nOur proposed MI loss can be thus expressed as\nLMI = −I′\nα(Pjoint) = H′\nα(Pjoint) −H(bp) −H(ep)\n(8)\nAs we will show in experiments, purely minimizing the cross-entropy between Ppseud and Pjoint (i.e.,\nusing α = 1) does not give optimal results. This is because the true number of clusters is not known,\nand forcing an arbitrary number of clusters to be balanced is too restrictive. By using a value of α\nbetween 0 and 1, as shown in Figure 1, enables the network to ignore non-relevant clusters and focus\non the most important ones.\n2.2\nBoundary-aware alignment loss for dense feature clustering\nClustering dense embeddings based on LMI results in balanced and conﬁdent clusters, but these\nclusters do not need to be spatially regular or align with region boundaries in the image. To be useful\nfor the downstream segmentation task, a good representation should capture anatomic structures in\nthe images, whose contours often correspond to regions with strong intensity gradients (i.e., edges).\nBased on this idea, we propose to use local cross-correlation to match the boundaries of clusters,\nwhich correspond to regions with high entropy, with edges in the image. Our cross-correlation loss is\ndeﬁned as follows:\nLCC =\nX\ni∈Ω\n\u0012\nP\nj∈N(i)\n\u0000φj −ˆφ(i)\n\u0001\n·\n\u0000ϕj −ˆϕ(i)\n\u0001\u0013\n2\n\u0012\nP\nj∈N(i)\n\u0000φj −ˆφ(i)\n\u00012\n\u0013\n·\n\u0012\nP\nj∈N(i)\n\u0000ϕj −ˆϕ(i)\n\u00012\n\u0013\n(9)\n5\nTable 2: 3D DSC on test set when ﬁne-tuned using a few labeled data. Listed methods are applied in\na pre-training stage. (Dec) means that the loss is applied to dense embeddings in feature maps of the\ndecoder, and (Enc) to the global features at the end of the encoder.\nMethods\nACDC-LV\nACDC-RV\nACDC-Myo\nPROMISE12\n1 scan 2 scans 4 scans mean 1 scan 2 scans 4 scans mean 1 scans 2 scans 4 scans mean 4 scans 6 scans 8 scans mean\nBaseline\n67.13\n74.49\n84.81\n75.48 51.82\n60.50\n64.18\n58.84\n54.05\n67.56\n76.00\n65.87\n49.91\n71.53\n78.04\n66.49\nFull Sup.\n92.26\n86.80\n88.07\n89.65\nIIC (Dec)\n71.96\n82.84\n85.43\n80.08 56.92\n63.58\n64.93\n61.81\n58.31\n70.22\n74.98\n67.83\n54.21\n72.97\n80.05\n69.07\nIMSAT (Dec)\n57.59\n75.38\n76.76\n69.91 34.36\n47.81\n47.42\n43.20\n50.52\n64.51\n71.91\n62.23\n55.20\n74.46\n81.50\n70.38\nContrast (Dec)\n64.37\n77.69\n84.36\n75.47 50.75\n56.34\n50.88\n52.66\n54.40\n70.11\n74.05\n69.19\n54.22\n63.52\n82.47\n66.74\nOurs (Dec) (only MI)\n83.63\n86.94\n89.33\n86.63 66.63\n73.78\n73.85\n71.42\n73.65\n77.39\n81.92\n77.08\n65.36\n78.42\n81.92\n75.23\nOurs (Dec) (only CC)\n64.85\n67.03\n79.31\n70.70 44.30\n50.33\n54.52\n49.72\n49.46\n60.13\n69.64\n59.74\n42.48\n73.69\n80.31\n65.50\nOurs (Dec) (MI+CC)\n84.04\n88.52\n89.31\n87.29 76.86\n79.13\n75.92\n77.30\n76.93\n79.59\n81.97\n79.49\n68.13\n78.75\n82.82\n76.30\nContrast (Enc)\n80.59\n85.68\n87.78\n84.10 68.91\n73.54\n72.70\n71.72\n67.30\n77.22\n79.58\n74.70\n63.54\n78.24\n81.72\n74.50\nContrast (Enc+Dec)\n77.98\n85.97\n88.42\n84.12 66.47\n72.82\n76.69\n71.99\n64.96\n76.98\n78.76\n73.57\n60.68\n77.97\n80.53\n73.06\nContrast (Enc)+Ours (Dec)\n84.48\n87.85\n90.04\n87.45 75.42\n79.73\n78.89\n78.01\n74.30\n78.43\n82.82\n78.52\n69.76\n80.47\n82.09\n77.44\nIn this loss, φ measures the edge response of a Sobel ﬁlter on the input image, while ϕ is a spatial map\nof cluster distribution entropy. ˆφ(i) and ˆϕ(i) denote the mean value in a local window N(i) centered\non position i, respectively for φ and ϕ. We note that a similar loss is often used in medical image\nregistration (Balakrishnan et al., 2019), where images of two different modalities or acquisitions need\nto be aligned. Unlike L2 loss, which imposes a strict equivalence between distributions, this loss can\ncapture correlation in local variance even when images have very different distributions of intensity.\n2.3\nContrastive loss for global feature learning\nWhile the ﬁrst two losses aim to regularize the local representation of dense feature maps in the\ndecoder, the next one focuses on learning a global representation of the image. Toward this goal, we\nconsider the features produced by the encoder fE(xi), which summarize the global context of an\ninput image xi, and project it into a low-dimensional representation zi. Similar to (Chaitanya et al.,\n2020), we regularize global representation zi using a contrastive loss exploiting available meta-labels:\nLcon = −1\n2N\n2N\nX\ni=1\n1\n|Si|\nX\nj∈Si\nlog\nexp\n\u0000z⊤\ni zj/τ\n\u0001\nP\na∈S\\{i}\nexp\n\u0000z⊤\ni za/τ\n\u0001.\n(10)\nIn the loss, S = {i |1 ≤i ≤2N} is the index set of an augmented batch, where each image\nis randomly transformed twice. Moreover, G(i) the meta-label of image i and Si = {j | G(j) =\nG(i), 1 ≤j ≤2N, i ̸= j} are the indexes of images within the same meta-label as i. As described\nin Section E, we divide volumetric images into different partitions, and use the partition index of\neach 2D image (slice in the volume) as meta-label. τ is a small temperature factor that helps gradient\ndescent optimization by smoothing the landscape of the loss objective.\n2.4\nOur uniﬁed pre-training objective\nOur ﬁnal objective for unsupervised representation learning combines all three objectives as follows:\nLtotal =\nLcon\n|{z}\nglobal embedding\n+ λLCC + LMI\n|\n{z\n}\ndense features\n(11)\nLcon is applied on the global representation of the encoder, and therefore it inﬂuences only the encoder.\nConversely, LMI and LCC are used on the dense features of the penultimate layer of the decoder,\nhence they affect the parameters of the whole network. These three losses are learned jointly in a\nsingle pre-training step. See Fig. 4 in the Appendix for a graphical illustration.\n3\nExperimental setup\nTo assess the performance of our proposed pre-training method, we performed extensive experiments\non two clinically-relevant segmentation datasets. In this section, we present brieﬂy the experimental\nsetting, employed dataset, as well as implementation details. We include more details in Appendix D\nand F.\n6\n3.1\nDataset and evaluation metrics\nTwo clinically-relevant benchmark dataset are chosen for our experiments: the automatic cardiac\ndiagnosis challenge (ACDC) (Bernard & et al., 2018), and the Prostate MR image segmentation 2012\nchallenge (PROMISE12) (Litjens et al., 2014) dataset. Three foreground classes are delineated for\nACDC dataset, which includes left ventricle endocardium (LV), left ventricle myocardium (Myo),\nright ventricle endocardium (RV), and we consider them as three binary segmentation tasks. Due\nto the high anisotropic resolution in both datasets, we consider the 2D slices of volumetric images\nas separate examples, and randomly split them into training, validation and test sets, so that no\ntwo images of the same scan are in the same set. To evaluate methods in a setting with limited\nannotation, we randomly select images from a few scans of the training set as our labeled data set,\nand consider all the images of the training set as unlabeled. We detail the data pre-processing and\naugmentation in Appendix D. For all datasets, we used the 3D Dice similarity coefﬁcient (DSC),\nwhich measures the overlap between the predicted labels S and the corresponding ground truth labels\nG: DSC(S, G) = 2×|S∩G|\n|S|+|G| . In all experiments, we reconstruct the 3D segmentation for each scan\nby aggregating the predictions made for 2D slices and report the 3D DSC metric for the test set\ncorresponding to the best-performing epoch on the validation set.\n3.2\nComparable methods and ablation variants\nWe compare our proposed method with clustering-based and contrastive-based self-supervised\nlearning approaches, as well as six state-of-the-art semi-supervised segmentation methods.\nIMSAT (Hu et al., 2017) and IIC (Ji et al., 2019) also employ MI maximization as the optimization\ncriterion and cluster the local embeddings pixel-wisely. The contrastive learning method relies on the\nconstruction of positive and negative pairs. For dense embedding, positive pairs are embeddings in\nthe same position undergoing different intensity transformations, where negative pairs are deﬁned\nas embeddings with sufﬁcient large distances. Six semi-supervised segmentation methods are also\ntested, and we present the details of each method in Appendix E. Lastly, we boost the best performing\nsemi-supervised method with pre-trained weights from our pre-training methods.\n3.3\nImplementation details\nWe employ U-Net (Ronneberger et al., 2015) as our segmentation network architecture, which consists\nof ﬁve symmetric encoder/decoder blocks with skip connections. We extract the local embeddings\nin the decoder layer before the last 1×1 convolution, thus they have the same spatial resolution\nas the input image. These embeddings are then projected to probabilities over K cluster using a\nprojector comprised of a 1×1 convolution and a K-way softmax. We ﬁx K = 40 for all datasets.\nHyper-parameter α is introduced in our method and we ﬁxed it to 0.5 for all experiments. Image\ntransformation T (·) consists of gamma correction and random afﬁne transformation. Our proposed\nmethod follows a two-stage training strategy: pre-train for representation learning and ﬁne-tune\nfor evaluation this representation on the downstream segmentation task. In the pre-train stage, we\noptimize the network in an unsupervised way on all training images without pixel-wise annotation,\nresulting in a set of network parameters θ. We evaluate the quality of these pre-trained weights in a\nseparate ﬁne-tune stage by creating a second segmentation network initialized with these parameters,\nand ﬁne-tuning the whole network using only a few labeled scans. The comparison with other\nSOTA semi-supervised methods is performed with the same setting as in the ﬁne-tune stage, and we\nreport their test DSC performances on their own best hyper-parameters determined by validation\nperformance using grid search. We provide detailed explanation on network architecture, training\nprotocols, transformation T (·), and hyper-parameters used in each method in Appendix F.\n4\nExperimental results\nIn this section, we ﬁrst compare our method against clustering-based and contrastive-based methods.\nThen, we evaluate all components of our method as our ablation variants. Finally, we compare our\nmethod with the most promising approaches for semantic segmentation in medical imaging, with\nreduced training data.\n7\nTable 3: Impact of α for our proposed LMI.\nα\nACDC-LV\nACDC-RV\nACDC-Myo\n1 scan 2 scans 4 scans 1 scan 2 scans 4 scans 1 scans 2 scans 4 scans\n0.0\n61.58\n78.09\n81.27\n31.95\n23.50\n41.64\n54.74\n58.15\n69.45\n0.25 84.36\n87.68\n89.32\n38.98\n59.73\n59.39\n76.93\n79.59\n81.97\n0.5\n84.04\n88.52\n89.31\n76.86\n79.13\n75.92\n76.27\n79.81\n82.36\n0.75 82.02\n87.81\n89.03\n76.76\n79.41\n75.49\n73.14\n78.79\n81.79\n1.0\n81.31\n85.58\n88.66\n73.34\n76.37\n70.44\n71.90\n79.47\n81.22\nTable 4: Impact of our proposed boundary-aware loss LCC.\nλCC\nACDC-LV\nACDC-RV\nACDC-Myo\n1 scan 2 scans 4 scans 1 scan 2 scans 4 scans 1 scans 2 scans 4 scans\n0.0\n83.63\n86.94\n89.33\n66.63\n73.78\n73.85\n73.65\n77.39\n81.92\n0.1\n80.54\n85.40\n87.80\n66.99\n76.00\n75.33\n72.62\n77.03\n81.39\n1.0\n84.04\n88.52\n89.31\n76.86\n79.13\n75.92\n76.93\n79.59\n81.97\n4.0\n78.36\n84.60\n85.48\n70.84\n75.39\n70.88\n73.74\n76.65\n81.33\n4.1\nComparison with cluster based methods and ablation variants\nTable 2 reports the test 3D DSC performance for different representation learning methods on the\nACDC and PROMISE12 datasets. At the top of the table, we report the number of labeled scans used\nfor every result. Reported values are the average over three independent runs with different random\nseeds. Methods presented here all adopt a pre-train and ﬁne-tune strategy with a few annotated scans.\nUpper and lower bounds: We present results for Baseline, which uses only the annotated scans with\ncross-entropy as standard supervised loss, and for Full Supervision, where the same loss is used with\nall available training examples. These represent lower and upper bounds on the expected performance\nfor different methods.\nCluster-based methods: We present in the next two rows the performance for IIC and IMSAT.\nThese two methods employ MI as the optimization objective and perform clustering on local\nembeddings with K clusters. IIC brings consistent improvements across all four tested classes\n(4.6%, 3.14%, 1.96%, and 2.58%), while IMSAT leads to a worse performance for the ACDC dataset.\nWe visualize their pre-trained clusters in Fig. 2, showing that these methods fail to ﬁnd balanced\nclusters corresponding to meaningful regions of the image.\nContrastive-based method: We then report in the next row the performance obtained using contrastive\nlearning only on dense features of the decoder. Surprisingly, we observe that optimizing the contrastive\nobjective with grid-based positive and negative pairs provides no beneﬁt for the segmentation tasks.\nThis is due to very weak guidance offered by contrasting dense embeddings.\nOur ablations: We then present in the next three lines the performance for our proposed ablation\nvariants. Our modiﬁed LMI alone leads to substantial improvements compared to the original\nIIC: 6.54%, 9.61%, 9.25%, and 6.16% are observed for the four classes. These improvements\nclearly indicate the advantage of introducing a pseudo-mask Ppseud to guide the learning of the joint\nprobability matrix. LCC aligns cluster boundaries with image edges, but does not help segmentation\non its own since predicted clusters are not consistent across images and transformations. Last, we\nobserve that combining our proposed LMI and LCC lead to signiﬁcant improvements over using LMI\nalone. These improvements are particularly notable for RV and Myo classes, which are more complex\nand rely more on image edges.\nGlobal feature pre-training: The last three rows report the performance of methods employing\ncontrastive learning on global features. The method Contrast (Enc) which only optimizes Lcon\nsigniﬁcantly improves the segmentation quality given a few labeled scans.\nHowever, these\nimprovements are still inferior to the Ours (Dec) (MI+CC) variant which, unlike Contrast (Enc), does\nnot use meta-labels. Contrast (Enc+Dec), which combines global and local contrastive objectives,\nleads to marginal improvements. Our proposed method is complementary to the global contrastive\nbased method. We report in the last row the performance of our proposed method combining all three\nlosses: Lcon, LMI and LCC. This method achieves the highest accuracy on 10 out of 16 cases, and\nsecond rank for remaining cases. Further, it yields average DSC improvement over Baseline as large\nas 17.35%, 23.60%, 20.25% and 17.85%, for the LV, RV, Myo and Prostate tasks, respectively.\n8\nTable 5: Impact of number of clusters K.\nK\nACDC-LV\nACDC-RV\nACDC-Myo\n1 scan 2 scans 4 scans 1 scan 2 scans 4 scans 1 scans 2 scans 4 scans\n5\n66.88\n73.95\n77.33\n49.32\n52.10\n51.74\n38.52\n48.18\n49.25\n10\n81.00\n86.11\n87.73\n70.75\n74.37\n71.78\n71.16\n77.48\n80.14\n20\n69.72\n73.37\n74.68\n59.63\n63.13\n54.00\n76.03\n79.34\n82.30\n40\n84.04\n88.52\n89.31\n76.86\n79.13\n75.92\n76.27\n79.81\n82.36\n60\n85.25\n88.12\n89.74\n70.28\n74.78\n76.39\n75.96\n79.09\n82.68\nTable 6: We compare the performance of our method with other pre-training approaches and state-\nof-the-art semi-supervised methods on 3D DSC on test set when ﬁne-tuned using a few labeled\ndata.\nMethods\nACDC-LV\nACDC-RV\nACDC-Myo\nPROMISE12\n1 scan 2 scans 4 scans mean 1 scan 2 scans 4 scans mean 1 scans 2 scans 4 scans mean 4 scans 6 scans 8 scans mean\nBaseline\n67.13\n74.49\n84.81\n75.48 51.82\n60.50\n64.18\n58.84\n54.05\n67.56\n76.00\n65.87\n49.91\n71.53\n78.04\n66.49\nContrast (Enc+Dec)\n77.98\n85.97\n88.42\n84.12 66.47\n72.82\n76.69\n71.99\n64.96\n76.98\n78.76\n73.57\n60.68\n77.97\n80.53\n73.06\nOurs (pre-train)\n84.48\n87.85\n90.04\n87.45 75.42\n79.73\n78.89\n78.01\n74.30\n78.43\n82.82\n78.52\n69.76\n80.47\n82.09\n77.44\nEntropy Min.\n73.79\n80.26\n86.84\n80.30 56.18\n62.09\n66.27\n61.51\n57.23\n71.10\n76.28\n68.20\n59.78\n76.09\n78.98\n71.62\nMixUp\n73.30\n76.30\n84.42\n78.01 61.23\n63.60\n63.14\n62.66\n55.74\n69.80\n73.84\n66.46\n52.09\n75.59\n81.11\n69.60\nMean Teacher (MT)\n83.13\n87.02\n87.70\n85.95 61.61\n68.76\n67.21\n65.86\n61.55\n75.32\n78.42\n71.76\n84.71\n85.97\n86.93\n85.87\nUA-MT\n81.08\n85.03\n87.19\n84.43 62.06\n67.91\n66.64\n65.54\n59.26\n73.68\n78.61\n70.52\n66.16\n81.79\n84.40\n77.45\nICT\n76.87\n78.41\n86.34\n80.54 60.31\n63.42\n68.35\n64.03\n55.91\n71.77\n77.90\n68.53\n63.97\n77.92\n81.39\n74.43\nAdv. Train.\n75.31\n74.85\n85.85\n78.67 55.29\n62.25\n64.58\n60.71\n57.68\n70.39\n75.94\n68.00\n71.50\n78.63\n81.35\n77.16\nMT\n+\nContrast\n(Enc+Dec)\n86.37\n89.57\n90.40\n88.78 75.53\n78.42\n77.22\n77.06\n76.11\n80.21\n82.00\n79.44\n76.16\n82.89\n84.85\n81.30\nMT + Ours (pre-train) 90.25\n91.36\n91.04\n90.88 80.16\n81.50\n78.97\n80.21\n78.71\n83.33\n83.61\n81.88\n85.64\n85.60\n88.45\n86.56\n4.2\nVisualization of pre-trained cluster assignments\nTo better understand our boundary-aware information maximization method, we visualize in Fig. 2\ndifferent cluster assignments obtained by our ablation variants and compared methods. Clusters\nobtained at the end of the unsupervised pre-training are illustrated by different colors. We also\ncompare these clusters with the SLIC super-pixel algorithm (Achanta et al., 2012) that groups pixels\nbased on both intensity and spatial information. We note that IMSAT, IIC and Ours (α = 0) produce\nhighly unbalanced clusters, where a few clusters dominate a large portion of pixels and resulting\nclusters do not correspond well to anatomical structures of the image. In contrast, our proposed\nvariants with α ≥0.25 clearly capture the main structures in cardiac MR images, without any\npixel-wise annotation. In most cases, it is able to successfully separate the LV, RV and Myo classes\nfrom the background. Additionally, PROMISE12 images present less contrast but our method still\nproduces relatively better anatomical structures compared with traditional Super-pixel methods, IIC\nand IMSAT. This explains the huge improvements brought by our method when ﬁne-tune the network\nusing a few labeled images. Last, we notice that contrastive-based approaches can also boost the\nsegmentation performance. However, they lack the intepretability of clusters provided by our method.\nImage /\nGradient\nλCC =0.0\nλCC =0.1\nλCC =1.0\nλCC =4.0\nFigure 3: Boundary loss effect. Upper: Input image and different pre-trained clusters; Down: Image\nedges and entropy map for each cluster. Black color refers to certain cluster regions while bright\nregions reﬂect uncertain predictions.\n9\n4.3\nImpact of α in our proposed LMI objective\nTo evaluate the impact of the proposed pseudo-label for the joint distribution matrix, we vary different\nα based on one of our best performing case and report the results in Table 3. It can be seen that\nincreasing α from 0 to 0.25 introduces large improvements for all segmentation tasks, which conﬁrms\nthe poor guidance of the IIC objective. Interestingly, we notice that α = 1 does not lead to the best\nperformance. This might be because clustering pixels into K = 40 regions of similar sizes breaks the\nanatomical structures of a given image, and a relatively lower α provides a softer guidance that helps\npreserve these structures. We conﬁrm this by visualizing in Fig. 1 the joint matrix Pjoint, the cluster\nassignment, as well as the uncertainty of these clusters for different α.\n4.4\nImpact of boundary-aware loss LCC\nOur boundary-aware loss LCC is a key component to boost performance for harder segmentation tasks\nsuch as RV and Myo. To determine the usefulness of this loss, similar to the previous experiment,\nwe vary λCC ranging from 0.0 to 4.0 for our best performing case, and present the results in Table\n4. Clearly, increasing λCC from 0.0 to 1.0 improves the segmentation performance for all tasks, in\nparticular for the RV and Myo classes whose boundary mainly follows the image edges. In Fig. 3, we\nshow the cluster boundaries separate images obtained with the different λCC. The boundary-aware loss\nsuccessfully guides the cluster boundaries towards image edges and reduces the over-segmentation of\npixels around the boundaries.\n4.5\nImpact of over-segmented clustering numbers K\nOur MI-based method converts continuous feature vectors to a discrete distribution over clusters. The\ncluster number K is another important hyper-parameter for our method. In this ablation experiment,\nwe measure the impact of K by varying it from 5 to 60. Table 5 and Fig. 5 in Appendix G show the\nDSC performance and the corresponding cluster assignment obtained with unsupervised pre-training.\nK = 5 leads to a weak segmentation performance, which can be explained by a collapsed cluster\nassignment. The cluster maps become more balanced with the increase of K and gradually reﬂect\nthe cardiac structures in the image. However, using K =60 does not give a better performance in\nsegmentation tasks, since the resulting clusters over-segment the image and capture less relevant\nregions.\n4.6\nComparison with state-of-the art methods\nWe compare our method with other approaches that aim to improve training with few annotated\nimages/scans. Table 6 presents results for various semi-supervised learning approaches. For a\nmore detailed explanation of the experimental setup of each method, see Appendix E. To have\na fair comparison, for all methods, we used grid search on the validation set to tune the hyper-\nparameters and report the corresponding test performance. For most methods, the improvement with\nrespect to the baseline trained with only the supervised loss is quite limited and varies depending\non the segmentation task and the number of annotated scans used. Among different methods,\nMT offers stable improvements across all tasks and reaches competitive performance compared\nwith contrast (Dec+Dec) and Ours (pre-train) for the LV and Prostate classes, mainly due to its\ntemporally-ensembled teacher network which provides stable prediction proposals for unlabeled\nimages. However, semi-supervised methods such as MT are normally trained with randomly\ninitialized parameters and can thus be further improved with our proposed pre-train approach\nas initialization of the network. To test this idea, we ran MT with two different initializations, one\nfrom contrastive-based Contrast (Enc+Dec) and the other from Ours (pre-train). The results in\nthe last two rows of Table 6 indicate that a further improved segmentation is obtained by simply\ninitializing the network parameters with these pre-trained checkpoints: Contrast (Enc+Dec) boosted\nthe performance of MT by 2.82%, 11.20% and 7.68% for LV, RV, and Myo, while these improvements\nincrease to 4.93%, 14.35% and 10.12% when Ours (pre-train) is used as initialization. In summary,\nour boundary-aware algorithm for unsupervised representation learning can boost state-of-the-art\nsemi-supervised segmentation approaches to achieve excellent segmentation quality, even when only\nan extremely small amount of labeled examples are available.\n10\n5\nDiscussion and conclusion\nIn this paper, we presented a boundary-aware information maximization method for the unsupervised\npre-training of models for medical image segmentation. This method complements the global\ncontrastive loss and can highly improve the performance of a segmentation network when annotated\ndata is scarce. It was shown that, with a reduced amount of unlabeled images, our method can learn\na useful local representation on dense feature maps during pre-training, without any supervisory\nsignal. Furthermore, as shown in our visualization of results, the clusters obtained by the pre-trained\ncheckpoint enhance intepretability. We compared our method with recent self-supervised learning\napproaches, based on clustering and contrastive learning, as well as six strong semi-supervised\nsegmentation algorithms. Results on two benchmark datasets demonstrate the outstanding accuracy\nof our method. In particular, the combination of our method with Mean Teacher yields unprecedented\nperformance, reaching close to full supervision with a single scan.\nReferences\nAchanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., and S¨usstrunk, S. Slic superpixels compared\nto state-of-the-art superpixel methods.\nIEEE transactions on pattern analysis and machine\nintelligence, 34(11):2274–2282, 2012.\nAhn, E., Feng, D., and Kim, J. A spatial guided self-supervised clustering network for medical image\nsegmentation. In International Conference on Medical Image Computing and Computer-Assisted\nIntervention, pp. 379–388. Springer, 2021.\nBadrinarayanan, V., Kendall, A., and Cipolla, R. Segnet: A deep convolutional encoder-decoder\narchitecture for image segmentation.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, 39(12):2481–2495, 2017. doi: 10.1109/TPAMI.2016.2644615.\nBai, W., Chen, C., Tarroni, G., Duan, J., Guitton, F., Petersen, S. E., Guo, Y., Matthews, P. M., and\nRueckert, D. Self-supervised learning for cardiac mr image segmentation by anatomical position\nprediction. In International Conference on Medical Image Computing and Computer-Assisted\nIntervention, pp. 541–549. Springer, 2019.\nBalakrishnan, G., Zhao, A., Sabuncu, M. R., Guttag, J., and Dalca, A. V. Voxelmorph: a learning\nframework for deformable medical image registration. IEEE transactions on medical imaging, 38\n(8):1788–1800, 2019.\nBernard, O. and et al.\nDeep learning techniques for automatic MRI cardiac multi-structures\nsegmentation and diagnosis: Is the problem solved? IEEE Transactions on Medical Imaging, 37\n(11):2514–2525, Nov 2018. ISSN 0278-0062. doi: 10.1109/TMI.2018.2837502.\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A. Unsupervised learning of\nvisual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.\nChaitanya, K., Erdil, E., Karani, N., and Konukoglu, E.\nContrastive learning of global and\nlocal features for medical image segmentation with limited annotations.\narXiv preprint\narXiv:2006.10511, 2020.\nChen, L., Bentley, P., Mori, K., Misawa, K., Fujiwara, M., and Rueckert, D. Self-supervised learning\nfor medical image analysis using image context restoration. Medical image analysis, 58:101539,\n2019.\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A. L. Deeplab: Semantic\nimage segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848, 2018. doi:\n10.1109/TPAMI.2017.2699184.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of\nvisual representations. In International conference on machine learning, pp. 1597–1607. PMLR,\n2020.\n11\nCheplygina, V., de Bruijne, M., and Pluim, J. P. Not-so-supervised: a survey of semi-supervised,\nmulti-instance, and transfer learning in medical image analysis. Medical image analysis, 54:\n280–296, 2019.\nCho, J. H., Mall, U., Bala, K., and Hariharan, B. Picie: Unsupervised semantic segmentation\nusing invariance and equivariance in clustering. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 16794–16804, 2021.\nFang, T., Liang, Z., Shao, X., Dong, Z., and Li, J. Self-supervised multi-view clustering for\nunsupervised image segmentation. In International Conference on Artiﬁcial Neural Networks, pp.\n113–125. Springer, 2021.\nGidaris, S., Singh, P., and Komodakis, N. Unsupervised representation learning by predicting image\nrotations. arXiv preprint arXiv:1803.07728, 2018.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual\nrepresentation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 9729–9738, 2020.\nHjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and\nBengio, Y. Learning deep representations by mutual information estimation and maximization. In\nInternational Conference on Learning Representations, 2018.\nHu, W., Miyato, T., Tokui, S., Matsumoto, E., and Sugiyama, M. Learning discrete representations\nvia information maximizing self-augmented training. In International conference on machine\nlearning, pp. 1558–1567. PMLR, 2017.\nHu, X., Zeng, D., Xu, X., and Shi, Y. Semi-supervised contrastive learning for label-efﬁcient\nmedical image segmentation. In International Conference on Medical Image Computing and\nComputer-Assisted Intervention, pp. 481–490. Springer, 2021.\nJi, X., Henriques, J. F., and Vedaldi, A. Invariant information clustering for unsupervised image\nclassiﬁcation and segmentation. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 9865–9874, 2019.\nJing, L. and Tian, Y. Self-supervised visual feature learning with deep neural networks: A survey.\nIEEE transactions on pattern analysis and machine intelligence, 2020.\nKomodakis, N. and Gidaris, S. Unsupervised representation learning by predicting image rotations.\nIn International Conference on Learning Representations (ICLR), 2018.\nLi, S., Zhang, C., and He, X. Shape-aware semi-supervised 3d semantic segmentation for medical\nimages.\nIn International Conference on Medical Image Computing and Computer-Assisted\nIntervention, pp. 552–561. Springer, 2020.\nLitjens, G., Toth, R., van de Ven, W., Hoeks, C., Kerkstra, S., van Ginneken, B., Vincent, G., Guillard,\nG., Birbeck, N., Zhang, J., et al. Evaluation of prostate segmentation algorithms for mri: the\npromise12 challenge. Medical image analysis, 18(2):359–373, 2014.\nLiu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. On the variance of the adaptive\nlearning rate and beyond. arXiv preprint arXiv:1908.03265, 2019.\nLiu, X., Zhang, F., Hou, Z., Mian, L., Wang, Z., Zhang, J., and Tang, J. Self-supervised learning:\nGenerative or contrastive. IEEE Transactions on Knowledge and Data Engineering, 2021.\nNoroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles.\nIn European conference on computer vision, pp. 69–84. Springer, 2016.\nOuyang, C., Bifﬁ, C., Chen, C., Kart, T., Qiu, H., and Rueckert, D. Self-supervision with superpixels:\nTraining few-shot medical image segmentation without annotation. In European Conference on\nComputer Vision, pp. 762–780. Springer, 2020.\nPeng, J. and Wang, Y. Medical image segmentation with limited supervision: A review of deep\nnetwork models. IEEE Access, 2021.\n12\nPeng, J., Pedersoli, M., and Desrosiers, C. Boosting semi-supervised image segmentation with global\nand local mutual information regularization. arXiv preprint arXiv:2103.04813, 2021a.\nPeng, J., Wang, P., Desrosiers, C., and Pedersoli, M. Self-paced contrastive learning for semi-\nsupervisedmedical image segmentation with meta-labels. arXiv preprint arXiv:2107.13741, 2021b.\nPerone, C. S. and Cohen-Adad, J.\nDeep semi-supervised segmentation with weight-averaged\nconsistency targets. In Deep learning in medical image analysis and multimodal learning for\nclinical decision support, pp. 12–19. Springer, 2018.\nRonneberger, O., Fischer, P., and Brox, T.\nU-Net: Convolutional Networks for Biomedical\nImage Segmentation, pp. 234–241.\nSpringer International Publishing, Cham, 2015.\nISBN\n978-3-319-24574-4. doi: 10.1007/978-3-319-24574-4 28. URL https://doi.org/10.1007/\n978-3-319-24574-4_28.\nShen, H., Wang, R., Zhang, J., and McKenna, S. J. Boundary-aware fully convolutional network\nfor brain tumor segmentation. In International Conference on Medical Image Computing and\nComputer-Assisted Intervention, pp. 433–441. Springer, 2017.\nTaleb, A., Lippert, C., Klein, T., and Nabi, M. Multimodal self-supervised learning for medical image\nanalysis. In International Conference on Information Processing in Medical Imaging, pp. 661–673.\nSpringer, 2021.\nVerma, V., Kawaguchi, K., Lamb, A., Kannala, J., Bengio, Y., and Lopez-Paz, D. Interpolation\nconsistency training for semi-supervised learning. arXiv preprint arXiv:1903.03825, 2019.\nVu, T.-H., Jain, H., Bucher, M., Cord, M., and P´erez, P. Advent: Adversarial entropy minimization\nfor domain adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 2517–2526, 2019.\nWei, Z., Shi, F., Song, H., Ji, W., and Han, G. Attentive boundary aware network for multi-scale\nskin lesion segmentation with adversarial training. Multimedia Tools and Applications, 79(37):\n27115–27136, 2020.\nXue, Y., Tang, H., Qiao, Z., Gong, G., Yin, Y., Qian, Z., Huang, C., Fan, W., and Huang, X.\nShape-aware organ segmentation by predicting signed distance maps. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 34, pp. 12565–12572, 2020.\nYu, L., Wang, S., Li, X., Fu, C.-W., and Heng, P.-A. Uncertainty-aware self-ensembling model\nfor semi-supervised 3d left atrium segmentation. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, pp. 605–613. Springer, 2019.\nZeng, D., Wu, Y., Hu, X., Xu, X., Yuan, H., Huang, M., Zhuang, J., Hu, J., and Shi, Y. Positional\ncontrastive learning for volumetricmedical image segmentation. arXiv preprint arXiv:2106.09157,\n2021.\nZhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization.\narXiv preprint arXiv:1710.09412, 2017a.\nZhang, R., Isola, P., and Efros, A. A. Colorful image colorization. In European conference on\ncomputer vision, pp. 649–666. Springer, 2016.\nZhang, Y., Yang, L., Chen, J., Fredericksen, M., Hughes, D. P., and Chen, D. Z. Deep adversarial\nnetworks for biomedical image segmentation utilizing unannotated images. In International\nconference on medical image computing and computer-assisted intervention, pp. 408–416. Springer,\n2017b.\nZhou, Z., Sodha, V., Pang, J., Gotway, M. B., and Liang, J. Models genesis. Medical image analysis,\n67:101840, 2021.\n13\nA\nDiagram\nFigure 4: Schematic Diagram of our proposed method. Our method consists of three individual\nobjectives. Global contrastive loss Lcon enforces images with similar anatomical structures to be pull\nclose, providing global context information for the encoder. Our MI-based loss LMI aims to cluster\nthe dense local embeddings into K balanced and conﬁdent classes, while our boundary-aware loss\nLCC aligns the boundaries of these clusters to image edges.\nB\nRelated work\nInspired by the recent success of representation learning, various approaches based on pre-training\nhave been investigated for segmentation. These approaches seek to acquire a discriminative image\nrepresentation from unlabeled data, in an independent pre-train stage. In medical image segmentation,\nChen et al. (2019); Bai et al. (2019) proposed to predict the relative position of patches in MR images.\nTaleb et al. (2021) extended the jigsaw puzzle solving pretext task to images from multiple MRI\nmodalities. Zhou et al. (2021) proposed Models Genesis, a denoising auto-encoder that reconstructs an\nMR image given its degraded version as input. Contrastive learning has also shown promising results\nto boost the performance of downstream tasks using unlabeled images. In this approach, a network\nis pre-trained to bring closer the feature embeddings of an image under different transformations\n(positive pairs), while pushing away those from different images (negative pairs). This idea was used\nto learn a global representation at the end of the network’s encoder, using meta-labels on anatomical\nsimilarity or subject ID to deﬁne the positive pairs (Chaitanya et al., 2020; Peng et al., 2021b; Zeng\net al., 2021). To also pre-train the decoder, the method in (Chaitanya et al., 2020) deﬁned positive\nor negative embedding pairs based on their spatial distance in a feature map, those with a large\ndistance considered as negative while those at the same spatial position but coming from different\ntransformations as positives. Hu et al. (2021) proposed using small set of pixel-wise annotations to\nguide the learning of dense features in pre-training. The feature embeddings of pixels with the same\nlabel are considered as positive pairs and are then clustered together by the contrastive loss. While\nthis guided approach helps learn a better local representation, it requires manual annotations and\ntherefore it is not unsupervised. Ouyang et al. (2020) instead employed superpixels for the contrastive\nobjective, however their approach is deﬁned in the context of few-shot segmentation.\nClustering has also been used to pre-train a network with unlabeled images (Caron et al., 2020; Ji\net al., 2019; Cho et al., 2021; Fang et al., 2021). Surprisingly, only a few papers have explored this\nself-supervised learning approach for medical image segmentation (Peng et al., 2021a; Ahn et al.,\n2021). Our method extends the IIC deep clustering approach (Ji et al., 2019) with an improved\nloss that encourages clusters to be consistent across different transformations and follow the region\nboundaries in the image.\nAccurately predicting the boundaries of anatomical structures, tissues or lesions is essential for\nmedical image segmentation, and boundary-aware training methods have been widely explored. To\nachieve this goal, most methods use a multi-task learning strategy with a secondary loss function\nfocusing on boundary information (Li et al., 2020; Xue et al., 2020; Shen et al., 2017). Another\napproach adopts a discriminator to embed the ground-truth boundary (Wei et al., 2020). Unlike our\nboundary-aware information maximization method for unsupervised representation learning, these\napproaches require annotated images and thus are limited to supervised or semi-supervised settings.\n14\nC\nProof of Proposition 2.1\nProof. For the lower bound, we use the inequality H(P, Q) = DKL(P ∥Q) + H(P) to obtain\nH( 1\nK IK, Pjoint) = DKL( 1\nK IK ∥Pjoint)\n|\n{z\n}\n≥0\n+ H( 1\nK IK)\n(12)\n≥−\nK\nX\nj=1\nK\nX\nk=1\n1\nK 1[j = k] log\n\u0000 1\nK 1[j = k]\n\u0001\n(13)\n= −\nK\nX\nk=1\n1\nK log 1\nK = log K.\n(14)\nFor the upper-bound, we use the Jensen inequality to get\nH( 1\nK IK, Pjoint) = −\nK\nX\nj=1\nK\nX\nk=1\n1\nK 1[j = k] log P(j,k)\njoint\n(15)\n= −\nK\nX\nk=1\n1\nK log\n\u0012 1\nN\nN\nX\ni=1\nbpik epik\n\u0013\n(16)\n≤−1\nN\nN\nX\ni=1\nK\nX\nk=1\n1\nK\n\u0000log bpik + log epik\n\u0001\n(17)\n= 1\nN\nN\nX\ni=1\nH(u, bpi) + H(u, epi)\n□\n(18)\nD\nDatasets\nWe assess the performance of our proposed method and compare it with other SOTA approaches on\ntwo clinically-relevant datasets: the automatic cardiac diagnosis challenge (ACDC) and the Prostate\nMR image segmentation 2012 challenge (PROMISE12). These two datasets cover different anatomical\nstructures, present different acquisition resolutions, and are widely used to verify the effectiveness of\nsemi-supervised segmentation algorithms.\nACDC dataset:\nThe ACDC datasetb consists of 200 short-axis cine-MRI scans from 100\npatients, evenly distributed in 5 subgroups: normal, myocardial infarction, dilated cardiomyopathy,\nhypertrophic cardiomyopathy, and abnormal right ventricles. For each patient, two annotated scans\ncorrespond to end-diastolic (ED) and end-systolic (ES) phases are provided, which were acquired on\n1.5T and 3T systems with resolutions ranging from 0.70 × 0.70 mm to 1.92 × 1.92 mm in-plane\nand 5 mm to 10 mm through-plane. Three regions of interest: left ventricle endocardium (LV), left\nventricle myocardium (Myo), right ventricle endocardium (RV) are labeled from background and\ndelineated pixel-wisely by human experts. We consider the 3D-MRI scans as 2D images through-\nplane due to the high anisotropic acquisition resolution, and re-sample them to a ﬁx space ranging of\n1.0 × 1.0 mm. Following (Peng et al., 2021b), we normalize the pixel intensities based on the 1%\nand 99% percentile of the intensity histogram for each scan. Normalized slices are then cropped to\n384 × 384 pixels, coarsely centered based on the foreground delineation of the ground truth. We\nselect slices from 175 random scans as our training set, from which we again randomly select 1, 2 or\n4 scansc as our labeled data, representing representing 0.5% to 2% of all available data, and consider\nothers as unlabeled data. We then randomly divide the remaining 25 scans into a validation set and a\ntest set, comprised of 8 scans and 17 scans respectively. Both the validation and test sets were set\naside during model optimization. For experiments with pre-train and ﬁne-tune strategies, we use all\ntraining data without any pixel-wise annotation for pre-train and evaluate the representation ability\nbPublicly-available by https://www.creatis.insa-lyon.fr/Challenge/acdc/index.html\ncThese labeled splits were also kept untouched for experiments employing pre-train and ﬁne-tune strategies,\nas well as those relying on semi-supervised losses.\n15\nby ﬁne-tuning the obtained network on a few labeled scans via a cross-entropy loss. We also used\nvarious data augmentation transformations T (·) for both labeled and unlabeled images, including\nrandom crops of 224 × 224 pixels, random ﬂip, random rotation, and color jitter from the Pillow\nlibrary. It is also worthy to notice that the main experimental results from this dataset were obtained\nby considering three-class segmentation as three binary segmentation tasks.\nPROMISE12 dataset: Our second datasetd focuses on prostate segmentation and is composed of\nmulti-centric transversal T2-weighted MR images from 50 subjects. These images were acquired\nfrom different vendors and with various acquisition protocols, and are thus representative of typical\nMR images acquired in a clinical setting. Image resolution ranges from 15 × 256 × 256 voxels to\n54 × 512 × 512 voxels with a spacing ranging from 2 × 0.27 × 0.27 mm to 4 × 0.75 × 0.75\nmm. Also in this case, we slice these volumetric images into 2D images along the short-axis and\nresized them to a resolution of 256 × 256 pixels. We equally performed a normalization on pixel\nintensities based on 1% and 99% percentile for each scan. We randomly selected 40 scans as training\ndata, 3 scans for validation, and 7 scans for testing. To test methods in annotation-scarcity regime,\nwe chose 4, 6, and 8 scans from these training examples as the labeled images, while keeping\nothers as unlabeled. We also employed rich data transformation prior to T (·). These Pillow-based\ntransformations include random crop of 224 × 224 pixels, random ﬂip, random rotation within a\nrange of [−10◦, 10◦], and color jitter.\nE\nDetailed comparison methods\nWe implement various state-of-the-art methods for comparison, including:\nContrastive-based method (Chaitanya et al., 2020): As our closest work, this approach acquires\ndiscriminative representation from unlabeled images via two global and local contrastive learning.\nGlobal contrastive learning pre-trains the encoder of the segmentation network to distinguish\nthe global context information such as the anatomical similarities between two slices, while\nlocal contrastive learning focuses on dense embeddings and enforces pixels undergoing different\ntransformations to be close and pixels at different spatial locations to be pushed away. We refer\nContrast (Enc) as our PyTorch re-implementation of the variant using only global contrastive\nobjective, Contrast (Dec) as the variant employing only local contrastive learning, and Contrast\n(Enc+Dec) as the full implementation using both contrastive objectives. For Contrast (Enc) variants,\nwe pre-train the encoder of the segmentation network to distinguish whether two slices comes from\nsimilar slice position by assuming the volumetric scans are coarsely aligned. Towards this goal,\nwe manually split an ACDC scans into three partitions, while we ﬁxed the partition numbers for\nPROMISE12 as 5, similar to Peng et al. (2021b). A nonlinear projector is used to convert the global\nrepresentation to representation vector, comprised of an average pooling layer, 2-layer MLP with\nLeakyReLU as the activation, and a normalization layer. For the variants using local contrastive\nlearning, we take the dense embeddings from the layer before last 1 × 1 convolutions. These dense\nembeddings are then projected to pxiel-wise vectors by a dense projector, consisting of an adaptive\naverage pooling of size 20 × 20 to reduce the spatial size, 2-layer MLP with 1 × 1 convolutions\nwith LeakyReLU as the activation, and a normalization layer. Positive and negative pairs are then\ndeﬁned on these 20 × 20 grid, similar to Chaitanya et al. (2020). It is worthy to point out that we\nonly optimize the parameters for the encoder in the pre-train stage only when global contrastive loss\nis used, while the whole network except last 1 × 1 convolution is optimized when local contrastive\nloss is employed. We employ two stage strategy: pre-train and ﬁne-tune to evaluate the quality of the\nlearned representation.\nIIC (Dec) (Ji et al., 2019): This is the original method proposed in (Ji et al., 2019) for unsupervised\nimage clustering, which corresponds to our optimization objective LMI with α = 0.0. This method has\nbeing successfully used in image clustering, as well as for unsupervised natural image segmentation,\nbut it can only work well with coarse classes. We follow the exact protocol and hyper-parameters as\nour proposed method and report the 3D DSC score on the test set.\nIMSAT (Dec) (Hu et al., 2017): This method seeks to maximize MI over categorical distributions\nfrom dense embeddings in a similar but different formulation: I = I(X, p(X)), where X is the\nimage set while p(X) is the cluster assignment distribution given X. This objective has been\nsuccessfully applied in image clustering (Hu et al., 2017). We adapt this loss for dense embedding\ndPublicly-available by https://promise12.grand-challenge.org/\n16\nclustering and this method shares the same experimental protocols as our proposed method. We\nequally evaluate its performance using pre-train and ﬁne-tune strategy.\nEntropy Minimization (EM) (Vu et al., 2019): This method has been successfully applied in semi-\nsupervised classiﬁcation and segmentation with domain gap, and imposes a low conditional entropy\non unlabeled images: Lent = −\n1\n|Du||Ω|\nP\nx∈Du\nP\ni∈Ωpi(x) log(pi(x)). By increasing its conﬁdence\nfor unlabeled images, the network pushes the decision boundary away from dense regions of the\ninput space, therefore improving generalization. For this method, we performed a hyper-parameter\nsearch on the coefﬁcient balancing the cross-entropy and Lent, from 1 × 10−4 to 1.0. We evaluate\nthis method in a standard semi-supervised setting with randomly initialized network parameters.\nMixUp (Zhang et al., 2017a): We also evaluated the effectiveness of mixup, an effective data\nargumentation strategy on medical image segmentation, following Chaitanya et al. (2020). In this\nmethod, we interpolate two labeled images with an index sampled from Beta(α, α) distribution and\nenforce the network to output the prediction as the interpolation of the two annotations. We ﬁx α = 1\nand the coefﬁcient weighting the mixup loss is selected by grid search from 1 × 10−5 to 0.1.\nMean Teacher (MT) (Perone & Cohen-Adad, 2018): This semi-supervised segmentation method\nadopts a teacher-student framework, in which two networks sharing the same architecture learn\nfrom each other. Given an unlabeled image x, the student model ps(·) seeks to minimize the\nprediction difference with the teacher network pt(·), whose weights are a temporal exponential\nmoving average (EMA) of the student’s: LMT = −\n1\n|Du||Ω|\nP\nx∈Du\nP\ni∈Ω|ps\ni(x) −pt\ni(x)|2. We ﬁx\nthe decay coefﬁcient to 0.99. The coefﬁcient balancing the supervised and regularization losses is\nselected by grid search, from 1 × 10−4 to 10.\nUncertainty-aware Mean Teacher (UA-MT) (Yu et al., 2019): This semi-supervised approach\nintroduces uncertainty for teacher network, which is achieved by Monte-Carlo dropout through\nmultiple inferences. In our implementation, we forward through the teacher network unlabeled\nimages four times and the uncertainty is obtained by computing the pixel-wise entropy of these\npredictions. We then use a linearly increased threshold T, ranging from 3\n4 × log(K) to log(K) to\nexclude from LMT pixels having high uncertainty. We keep other settings the same as our Mean\nTeacher method and evaluate method’s performance in a standard semi-supervised setting.\nInterpolation Consistency Training (ICT) (Verma et al., 2019): The next method we tested applies\nmixup method with teacher-student framework. In this approach, interpolated images are obtained\nby mixing up two unlabeled images. The student network is encouraged to output the prediction as\nthe interpolation of their predictions given by the teacher network. We follow (Verma et al., 2019)\nto set α as 0.1 and again grid search the weighting coefﬁcient for the regularization objective, from\n1 × 10−5 to 0.1.\nAdversarial training(AT) (Zhang et al., 2017b): Our last method trains a segmentation network\nand a classiﬁer-based discriminator jointly in a min-max game. The core idea is to enforce the\nsegmentation predictions on unlabeled images being indistinguishable from those of labeled images,\nthus aligning the output distributions between labeled and unseen images. This method works\nparticularly well in a scenario where the image scans present large variability causing a domain gap.\nWe evaluate this method in a standard semi-supervised setting and grid-search the regularization\ncoefﬁcient, from 1 × 10−6 to 0.1.\nF\nImplementation details\nNetwork Architecture: We used U-Net (Ronneberger et al., 2015) as our main network architecture,\nwhich consists of ﬁve symmetric blocks of encoder and decoder. As shown in Fig. 4, we assign\ndifferent names to these blocks and our global embeddings and dense embeddings are taken from\nconv5 and upconv2. A ﬁrst nonlinear projector is used to convert the global representation to\nrepresentation vector, comprised of an average pooling layer, 2 MLP layers with LeakyReLU as\nthe activation, followed by a normalization layer. In contrast, we simply employ a linear projector,\nincluding 1 × 1 convolution followed by a K-way softmax for the dense embeddings. Learnable\nparameters are optimized using stochastic gradient descent (SGD) with a RAdam Optimizer (Liu\net al., 2019).\nTraining hyper-parameters: Our main experiments adopt the two-stage training strategy: pre-\ntraining the whole network on all training data without labels and ﬁne-tune it with a few labeled\n17\nscan. For both stages, we employed a learning rate decay strategy, where the initial learning rate\nlr is increased N times in the ﬁrst 10 epochs, followed by a cosine decay strategy for the rest\nNepoch training epochs. We set lr = 5 × 10−7, N = 400, and Nepoch = 50 for the ACDC in\npre-train stage, lr = 1 × 10−7, N = 200, and Nepoch = 50 for ACDC in ﬁne-tune stage. As for the\nPROMISE12 dataset, we simply modify lr to 1 × 10−6 for the ﬁne-tune stage. We deﬁne an epoch in\nour experiments as the N update iterations, within which images are randomly selected from their\nrespective dataset with replacement. For ACDC, we ﬁxed N as 200 iterations while for PROMISE12,\nwe increase N to 400. For concurrent methods employing semi-supervised setting, we follow exactly\nthe same conﬁguration as adopted in ﬁne-tune stage. As shown in Equ. (11), our method requires\nonly one weighting coefﬁcient which balances the importance of our LCC and we simply set it to 1.0\nfor both datasets.\nDetails on the transformation T (·): Our proposed method heavily relies on T (·) to create\ntransformation equivalent pairs of cluster distribution: bp = g(s(T (x))) and ep = g(T (s(x))).\nWe set T (·) as the cascade of intensity transformations and geometric transformations. When T (·)\ntakes an input x as the raw image, we apply gamma correction within a range of [0.5, 2.0], as\nwell as a set of random afﬁne transformation, consisting of random scale within a range of [0.8,\n1.3], random rotation within a range of [−45◦, 45◦], and random translation within a range of\n[−10%, 10%]. Whereas when T (·) takes the input as the embedding s of the image x, we ignore\nthe intensity transformation and apply only the random afﬁne transformation with the same random\nstate corresponding to those applied with the raw image x. These augmentations operate on PyTorch\ntensors and are publicly-available at https://github.com/PhoenixDL/rising.git\nG\nPre-trained cluster assignment maps for different K\nWe show in Fig. 5 the pre-trained cluster assignment for different number of clusters K. One\ncan see that a small K learns a collapsed cluster assignment, which leads to a weak segmentation\nperformance (see Table 5). This is probably because a small cluster number reduces the capacity to\ncapture the structure information of such images. With the increase of K, the cluster maps become\nmore balanced and gradually reﬂect the cardiac structures of the image. However, when taking a\nlarge cluster numbers K = 60, the resulted clusters over-segment the images, leading to fractured\nanatomical structures. In this case, it can decrease the downstream segmentation tasks.\nImage\nK =5\nK =10\nK =20\nK =40\nK =60\nFigure 5: Pre-trained cluster assignment with respect to different K\nH\nImpact of batch size on pre-training\nAs contrastive-based pre-training often requires a large batch size, which is hard to satisfy for dense\nprediction tasks, such as segmentation. Our last ablation study investigates the performance stability\ngiven relatively small batch size B. Table 7 lists the 3D test DSC for ACDC dataset with reduced\nbatch size for contrastive-based and one of our best performing variant. It can be seen that with\nreduced batch size, segmentation performances reduces for both methods. However, our proposed\nmethod still outperforms contrastive-based approach for almost all cases given a very small batch.\n18\nTable 7: Impact of batch size B\nB\nACDC-LV\nACDC-RV\nACDC-Myo\n1 scan 2 scans 4 scans 1 scan 2 scans 4 scans 1 scans 2 scans 4 scans\nContrast (Enc+Dec)\n6\n72.16\n86.02\n87.37\n63.30\n69.25\n72.25\n61.51\n73.57\n76.51\n12\n75.52\n84.23\n88.31\n63.26\n71.41\n73.89\n65.88\n76.26\n78.77\n18\n77.98\n85.97\n88.42\n66.47\n72.82\n76.69\n64.96\n76.98\n78.76\nOurs (MI+CC)\n6\n81.61\n85.76\n88.21\n67.39\n67.04\n66.08\n71.18\n77.41\n80.20\n12\n81.46\n87.89\n88.72\n68.15\n76.33\n74.96\n74.84\n78.54\n82.58\n18\n84.04\n88.52\n89.31\n76.86\n79.13\n75.92\n76.93\n79.59\n81.97\nI\nVisual results for segmentation\nGround Truth\nBaseline\nMT\nAT\nContrast\n(Enc+Dec)\nOurs\n(pre-train)\nContrast\n(Enc+Dec)\n+MT\nOurs (pre-train)\n+MT\nFigure 6: Visual comparison of tested methods on test images. Rows 1–2: LV; Rows 3–5: RV; Rows\n6–7: Myo; Row 8-10: PROMISE12.\n19\n",
  "categories": [
    "eess.IV",
    "cs.CV"
  ],
  "published": "2022-02-04",
  "updated": "2022-02-16"
}