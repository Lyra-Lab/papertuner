{
  "id": "http://arxiv.org/abs/2202.03371v1",
  "title": "Cedille: A large autoregressive French language model",
  "authors": [
    "Martin Müller",
    "Florian Laurent"
  ],
  "abstract": "Scaling up the size and training of autoregressive language models has\nenabled novel ways of solving Natural Language Processing tasks using zero-shot\nand few-shot learning. While extreme-scale language models such as GPT-3 offer\nmultilingual capabilities, zero-shot learning for languages other than English\nremain largely unexplored. Here, we introduce Cedille, a large open source\nauto-regressive language model, specifically trained for the French language.\nOur results show that Cedille outperforms existing French language models and\nis competitive with GPT-3 on a range of French zero-shot benchmarks.\nFurthermore, we provide an in-depth comparison of the toxicity exhibited by\nthese models, showing that Cedille marks an improvement in language model\nsafety thanks to dataset filtering.",
  "text": "CEDILLE:\nA LARGE AUTOREGRESSIVE LANGUAGE MODEL IN FRENCH\nMartin Müller∗\nFlorian Laurent∗\nCedille AI1\nhello@cedille.ai\nABSTRACT\nScaling up the size and training of autoregressive language models has enabled novel ways of solving\nNatural Language Processing tasks using zero-shot and few-shot learning. While extreme-scale\nlanguage models such as GPT-3 offer multilingual capabilities, zero-shot learning for languages\nother than English remain largely unexplored. Here, we introduce Cedille, a large open source\nauto-regressive language model, speciﬁcally trained for the French language. Our results show that\nCedille outperforms existing French language models and is competitive with GPT-3 on a range\nof French zero-shot benchmarks. Furthermore, we provide an in-depth comparison of the toxicity\nexhibited by these models, showing that Cedille marks an improvement in language model safety\nthanks to dataset ﬁltering.\n1\nIntroduction\nLarge autoregressive language models have drawn wide\nattention due to their zero-shot and few-shot capabilities,\nallowing them to be used for a wide variety of Natural Lan-\nguage Processing tasks without the need for task-speciﬁc\nﬁnetuning or annotation data [1, 2]. Additionally, previ-\nous work highlights the improved sample and compute\nefﬁciency of larger models, generally justifying the move\ntowards larger models [3].\nAlthough large language models, such as GPT-3 [2], have\nbeen trained on multilingual corpuses, the performance on\nNLP tasks may vary signiﬁcantly between languages. As-\nsessing zero-shot performance in non-English languages\nis challenging due to the limited number of human-curated\nbenchmarks available. However, with the exception of re-\ncent work in machine translation [4], multilingual models\ngenerally perform worse than mono- or bilingual language\nmodels [5].\nMonolingual autoregressive language models in French\nhave previously been proposed. GPT-fr [6] and PAGnol [7]\nhave been trained on ﬁltered versions of Common Crawl2\nand CCNet [8], respectively. Both works highlight the im-\nportance of deduplicating and ﬁltering of pre-training data\nand use decoder-only transformer architectures, closely\nfollowing the GPT models with model sizes reaching 1B\nand 1.5B parameters, respectively. It’s worth noting that\nthese works do not directly compare performance against\nextreme-scale large multilingual models, such as GPT-3,\nin particular with regard to zero-shot tasks.\nPrevious work on the various encoding biases in large lan-\nguage models highlights the importance of dataset curation\nand documentation [9, 10]. Experiments conducted on\nGPT-3 (which has been trained on 570GB of text data\nfrom Common Crawl) show that the model may gener-\nate toxic sentences even when prompted with non-toxic\ntext [11]. Although applying ﬁltering of training data using\nautomated toxicity scores may introduce classiﬁer-speciﬁc\nbiases [12], this technique remains more effective than\n∗Authors contributed equally, order is random\n1Coteries SA, EPFL Innovation Park, Lausanne, Switzerland\n2https://commoncrawl.org/\narXiv:2202.03371v1  [cs.CL]  7 Feb 2022\ndecoder-based detoxiﬁcation using methods such as swear\nword ﬁlters, PPLM [13], soft prompt tuning [14] or toxicity\ncontrol tokens [15].\nAs a consequence of the aforementioned risks, the trend\ntowards larger models coincides with a trend to not release\nmodels publicly. Controlling access to large language mod-\nels may protect against certain bad actors but also limits\nreproducibility and research efforts to mitigate the negative\nproperties of such models. In a push for building models in\nthe open, EleutherAI, a grassroot collective of researchers,\nreleased GPT-J [16], a 6B parameter English language\nmodel. This model was trained on the Pile [20], a 825GB\ntext corpus by the same collective.\nThe contributions of this paper are as follows: (1) We intro-\nduce Cedille, an openly available French language model\nbuilt on GPT-J, which is capable of achieving competitive\nzero-shot performance against existing French language\nmodels and GPT-3. (2) We release the toxicity scores\nof the complete French C4 dataset, and (3) we provide a\ncomparison of Cedille’s toxicity to other language models\n(including GPT-3).\n2\nMethods\n2.1\nModel architecture\nOur model architecture is identical to GPT-J [16]. GPT-J\nuses a similar transformer architecture to the one used in\n6.7B GPT-3 with three main differences: (1) No sparse\nattention patterns were used; (2) the dimension of the atten-\ntion head was increased from 128 to 256; and (3) Rotary\npositional embeddings [17] were used instead of sinusoidal\nembeddings. See Table 1 for more details.\nNumber of parameters\n6,053,381,344\nNumber of layers N\n28\nModel dimensions dmodel\n4096\nFeed-forward dimension dff\n16,384\nNumber of attention heads nheads\n16\nHead dimension dhead\n256\nContext size\n2048\nVocab size\n50,257\nTable 1: Cedille model details.\n2.2\nTraining data\nCedille is trained on a ﬁltered version of the French part\nof the multilingual C4 (mC4) dataset [18], which contains\n332M documents or 1.1TB of uncompressed text. mC4 is\nextracted from 71 Common Crawl snapshots (years 2013\nto 2020) and uses CLD33, a small feed-forward neural net-\nwork, for language identiﬁcation. mC4 ﬁltered out pages\nof less than three lines of at least 200 characters.\nWe apply two different forms of ﬁltering to the dataset 1)\ntoxicity ﬁltering using the Detoxify model [19] and 2) loss\nﬁltering using the FlauBERT model [20]. For both ﬁltering\nsteps we compute the metric on a per document level of the\nentire base dataset. In some cases chunking the documents\ninto splits of 1200 characters was necessary due to the\nﬁxed context size of the used models. Chunks smaller than\n600 characters were not evaluated. The predictions were\nrun on TPU v3-8 machines with 8-fold data parallelism\neach.\nEach percentile as well as the tails of both the loss and the\ntoxicity distribution were sampled and manually inspected\nto ﬁnd suitable cut-off values for ﬁltering. The inspection\nof these samples revealed that both toxicity and loss values\nwere appropriate4. We removed documents correspond-\ning to a toxicity score higher than 0.5, corresponding to\n0.25% of the content (0.8M documents). For the loss ﬁl-\ntering we considered the loss distribution of each of the\n2048 ﬁles and removed documents below a 0.2 percentile\nloss (corresponding to a loss value of roughly 4.5) and\nabove an absolute loss value of 10. This corresponded to\na removal of roughly 20% of all documents (66M docu-\nments). The combined ﬁltering led to a ﬁnal training set of\n265M documents, which corresponds to roughly 773GB\nof uncompressed text.\nThe text was then run through the fix_text method of\nthe Python library ftfy [21] using NFKC normalization\nand encoded using the unmodiﬁed GPT-2 tokenizer. Docu-\nments were simply concatenated and split into samples of\n2049 tokens. The ﬁnal training set yielded a total of 130M\nsamples corresponding to 268B tokens.\n2.3\nTraining process\nCedille was trained starting from the ofﬁcial GPT-J model\ncheckpoint using the mesh-transformer-jax codebase [22].\nTraining was conducted on a v3-128 TPU VM using 16-\nfold data parallelism and 8-fold model sharding. For all\nour experiments we used an effective batch size of 256.\nWe used a linear warmup of 42k steps up to a peak learning\nrate of 5e-5 and a cosine decay to 1e-5. Weight decay was\nset to 0.1. Cedille was trained for 150k steps, which corre-\nsponds to 0.3 epochs on the training set or 78.7B tokens.\nThe starting and ﬁnal training perplexities were 6.13 and\n3.89, respectively. During training we monitored the loss\non a dataset of French news stories published too recently\nto be part of the training data.\n3https://github.com/google/cld3\n4Despite the positive visual inspection a bug in the loss computation was discovered much later in the analysis. Further investiga-\ntion revealed that roughly 10% of samples were wrongly included in the ﬁnal dataset as a result. Although it cannot be fully ruled\nout we do not believe that a systematic bias was introduced.\n2\n2.4\nEvaluation\nZero-shot performance was evaluated using a forked ver-\nsion of the lm-evaluation-harness codebase [23]. In par-\nticular, we added a different way of evaluating perplexity\nusing strides (see section 3.1), implemented the various\nbenchmarks discussed in this work, and integrated the\nmesh-transformer-jax library (for evaluating checkpoints\non TPUs) and the Pagnol model families. Benchmarking\nwas conducted on v3-8 TPU VMs and on A100 GPUs.\nToxicity evaluation was conducted using a modiﬁed ver-\nsion of the real-toxicity-prompts codebase5. The main\ndifference is the use of the Detoxify model in order\nto predict toxicity (see section 4).\nOur adapted code-\nbase is available at https://github.com/coteries/\nreal-toxicity-prompts.\n3\nTasks\n3.1\nPerplexity\nModel\n#params\nByte-PPL\nToken-PPL\nGPT-3 (ada)\n1.3Ba\n1.930\n7.952\nGPT-3 (babbage)\n6.7B\n1.973\n6.447\nGPT-3 (curie)\n13B\n1.809\n5.082\nGPT-3 (davinci)\n175B\n1.656\n3.993\nGPT-J\n6.05B\n1.746\n5.797\nCedille\n6.05B\n1.646\n3.932\nPagnol (small)\n124M\n1.852\n17.802\nPagnol (medium)\n335M\n1.775\n14.623\nPagnol (large)\n773M\n1.725\n12.791\nGPT-fr (base)\n1B\n2.090\n11.882\nTable 2: Byte-level and token-level perplexity scores on the\nWikiText-fr benchmark (lower is better).\naOpenAI hasn’t ofﬁcially disclosed the size of the models\nprovided by their API, however recent experiments suggest the\nmapping presented in the table [24].\nZero-shot perplexity was evaluated on the test subset of\nthe WikiText-fr6 dataset [6], containing articles from the\nFrench Wikipedia which are part of the “quality articles” or\n“good articles” categories, similar to the English WikiText-\n103 dataset [25]. The test set contains 589k words or 3.7M\ncharacters of cleaned French text from 60 articles. We eval-\nuated perplexity by concatenating the text without further\npreprocessing and using a sliding window approach [26]\nwith a stride of 512 tokens. Therefore models with a con-\ntext window of 1024 tokens (GPT-fr, Pagnol) had 512\ntokens of context, whereas models with a context window\nof 2048 tokens had 1536 tokens of context. Table 2 shows\nthe summed log likelihoods both normalized by number\nof characters and by number of tokens. Note that the\ntoken-level perplexity for GPT-fr and Pagnol is not directly\ncomparable to the other models, as they are not using the\n(English) GPT-2 tokenizer.\nCedille achieves the lowest perplexity score out of the an-\nalyzed models, clearly outcompeting existing French lan-\nguage models and narrowly outcompeting GPT-3 (davinci).\nUnsurprisingly, models with larger context windows gen-\nerally perform better at this task. It is noteworthy that the\ntest dataset is likely contained in the training data as no\ndataset-speciﬁc ﬁltering of the training data was conducted\nas part of this work.\n3.2\nSummarization\nWe evaluated the summarization capabilities on the Orange-\nSum benchmark, as introduced in the BARThez work [27]\nas a French equivalent of XSum [28]. The benchmark con-\ntains news articles published between February 2011 and\nSeptember 2020, scraped from the French website “Orange\nActu”. The models were given the news article in the test\nsubset using the following prompt:\n{article text}\\nPour résumer :\nThe models were tasked to generate 100 tokens using top-k\nof 2 and a temperature of 1, following the methodology\nin [1]. We used greedy decoding (top-k = 1) for GPT-3,\nsince at the time of this work being conducted, the API\ndidn’t allow for other top-k values. When the prompt ex-\nceeded the context window of the model it was left-side\ntruncated. The output was then clipped to contain at most 3\nsentences (using simplistic sentence splitting at the period\ncharacter). Table 3 shows the ROUGE score [29] of the\noutput compared to the title of the corresponding articles.\nModel\nR1\nR2\nRL\nGPT-3 (ada)\n13.95\n4.75\n11.59\nGPT-3 (babbage)\n4.62\n1.76\n3.86\nGPT-3 (curie)\n5.28\n2.21\n4.42\nGPT-3 (davinci)\n15.49\n5.82\n13.05\nGPT-J\n14.46\n4.72\n11.68\nCedille\n14.74\n4.83\n11.86\nPagnol (small)\n8.52\n1.61\n7.24\nPagnol (medium)\n8.98\n1.86\n7.55\nPagnol (large)\n9.19\n1.85\n7.71\nGPT-fr (base)\n10.15\n2.60\n8.27\nTable 3: Performance of summarization in French. Shown are\nthe ROUGE scores on the OrangeSum dataset (higher is better).\nGenerally, we observed some variance due to the non-\ngreedy sampling procedure. However, computational limi-\n5https://github.com/allenai/real-toxicity-prompts\n6https://huggingface.co/datasets/asi/wikitext_fr\n3\ntations and cost made it difﬁcult to estimate this variance.\nWe also observed that the choice of the preﬁx (“Pour ré-\nsumer :”) strongly inﬂuences the scores. Some of the\nevaluated models are also more likely to generate bullet\npoint summaries, rather than a single sentence, which may\nagain lead to different sentence splitting. This may ex-\nplain the increased score for GPT-3 (ada) compared to\nlarger GPT-3 models. Nevertheless, the scores provided\nin Table 3 give some rough indication of summarization\nperformance.\n3.3\nQuestion Answering (QA)\nQuestion answering (QA) was evaluated on FQuAD\n(French Question Answering Dataset) [30], a dataset in-\nspired by the English SQuAD equivalent [31]. The models\nwere evaluated on the validation subset, which contains\n3188 human-curated question-answer pairs, based on 768\nhigh-quality French Wikipedia articles.\nModel\nF1\nExact match (%)\nGPT-3 (ada)\n19.09\n4.48\nGPT-3 (babbage)\n26.16\n8.81\nGPT-3 (curie)\n39.49\n17.84\nGPT-3 (davinci)\n-\n-\nGPT-J\n26.14\n6.96\nCedille\n34.59\n12.23\nPagnol (small)\n10.66\n0.43\nPagnol (medium)\n13.80\n0.84\nPagnol (large)\n17.67\n2.72\nGPT-fr (base)\n15.15\n2.03\nTable 4: Question-answering F1 and exact match scores in\nFrench on the FQuAD benchmark (higher is better).\nThe models were evaluated using the SQuAD v2 met-\nric [31], which also takes into consideration “no answer”\nprobabilities, i.e. cases when no answer to a particular\nquestion is possible given the context. The models were\ntasked to generate 100 tokens and at most 1 sentence using\ngreedy sampling and the following prompt:\nTitre:\n{title}\\nContexte:\n{context}\\n\\n\nQuestion:\n{question}\\n\\nRéponse:\nThe “no answer” probabilities were calculated against the\nstring:\n{prompt} Sans réponse.\nHowever, all questions in the evaluated data contained\nexactly one answer.\nThe results in Table 4 show that GPT-3 is very competitive\non this task, with GPT-3 (curie) outperforming Cedille\nand all other evaluated models. GPT-3 (davinci) was not\nevaluated on this task for cost reasons, as OpenAI did not\nsupport our request for funding at the time of writing. The\nresults may be contrasted to a ﬁnetuned version of Camem-\nBERT [32] which yields F1 of 88% and best match of 78%\non this dataset [30].\n3.4\nTranslation\nZero-shot translation was evaluated for the language pair\nEnglish and French on the WMT14 dataset [33]. Tradi-\ntionally, such benchmarks are evaluated using the BLEU\nscore [34]. The datasets contains 3003 samples each and\nare provided by the sacrebleu library [35]. The zero-shot\ntask is formulated using the following pattern:\n{source_lang} phrase:\n{text}\\n{target_lang}\nphrase:\nWhere source_lang and target_lang are French and\nEnglish, respectively, depending on the direction. Greedy\nsampling is used to generate 256 tokens. The output was\nclipped to at most 1 sentence.\nCedille outperforms other models for the direction English\nto French, highlighting the strong French writing capabil-\nities (see Table 5). Likewise, GPT-3 (davinci) performs\nbetter for the French to English direction. Monolingual\nmodels, such as Pagnol and GPT-fr perform worse at this\ntask presumably due to the limited amount of English that\nwas part of their pretraining data. Often, smaller models\nwere unable to follow the instructions and simply repeated\nthe context in the given language. As opposed to summa-\nrization and question-answering benchmarks, the target is\ngenerally not part of the context, therefore simply repeating\nthe input normally results in a low score.\nAs of 2021, dedicated neural machine translation solutions,\nsuch as Very Deep Transformers, reach 46.4 BLEU for\nEnglish to French translation [36].\nModel\nBLEU (en→fr)\nBLEU (fr→en)\nGPT-3 (ada)\n2.71\n16.64\nGPT-3 (babbage)\n3.20\n24.56\nGPT-3 (curie)\n13.45\n27.15\nGPT-3 (davinci)\n20.40\n27.70\nGPT-J\n14.71\n26.06\nCedille\n24.89\n20.59\nPagnol (small)\n0.76\n1.20\nPagnol (medium)\n1.07\n1.48\nPagnol (large)\n1.06\n3.47\nGPT-fr (base)\n1.47\n1.57\nTable 5: BLEU scores for ranslation on WMT14 for the English-\nFrench language pair (higher is better).\n4\nToxicity analysis\nIn order to evaluate the toxicity of the model we closely\nfollowed the work conducted in [11]. We studied the case\n4\nof unprompted (i.e. conditioned only on a start-of-sentence\ntoken) and prompted generation.\nThe original work in [11] used the Perspective API, a ser-\nvice that uses machine learning classiﬁers to estimate the\nperceived toxicity of text. In this work, we employ the\nDetoxify tool [19] instead. We made this choice as the\nunderlying models used by Perspective evolve with time\nand are not released publicly, which limits experimental\nreproducibility.\nDetoxify assigns a toxicity score between 0 and 1, with 1\ndenoting “a very hateful, aggressive, or disrespectful com-\nment”. We refer to content with a score > 0.5 as “toxic”.\nWe use the “multilingual” Detoxify model from release\nv0.4.0, and compare the toxicity of Cedille output to 3\nother models: GPT-2 (117M), GPT-3 (davinci), GPT-J and\nGPT-fr (base).\n4.1\nUnprompted toxicity\nFor the unprompted toxicity we analyze the expected max-\nimum toxicity, i.e. the expected worst-case toxicity score\ngiven N unprompted generations. Figure 1 shows boot-\nstrap estimates (1000 iterations) of the expected maximum\ntoxicity for N generations with variance bounds as shades.\nIn this setting, Cedille consistently generates content with\nlower expected maximum toxicity than GPT-2, GPT-J, and\nGPT-3. After 100 generations, this value is under 0.5 for\nGPT-fr and Cedille (0.41 and 0.48, respectively), which\nmeans that the worst content from these models is not\nexpected to be toxic. This is in contrast with the other\nmodels, for which maximum expected toxicity values are\n0.64, 0.54 and 0.56.\nAfter 10K generations, Cedille and GPT-fr are the only\nmodels for which the expected worst outputs don’t reach\na toxicity level of 1.0 We expect all other models to have\nat least one output that is maximally toxic as detected by\nDetoxify. Generally the two models that perform best are\nGPT-fr and Cedille, which were both trained on carefully\nﬁltered datasets, pointing to the importance of dataset cu-\nration when considering the safety of language models.\nWithout any conditioning, the multilingual models almost\nexclusively generate English content: this is the case of\nGPT-2, GPT-J and GPT-3. However, with the Detoxify\nmodel being multilingual, the toxicity scores remain com-\nparable.\n4.2\nPrompted toxicity\nFor prompted toxicity we used a set of 50 French prompts\nwith values of toxicity spanning the full range, with a mean\nof 0.34. The set of prompts was selected randomly from\nthe RealToxicityPrompt dataset and manually translated\nfrom English to French by a French native speaker. We\nused a smaller number of prompts than in [11] due to lim-\nited computing resources. The French prompts cause the\nmultilingual models (GPT-2, GPT-J and GPT-3) to gener-\nate French content. For each prompt, each model generates\n50 completions. We used nucleus sampling with p = 0.9\nto generate up to 20 tokens per continuation, following the\nprotocol from [11].\nTable 6 shows two properties: 1) the expected maximum\ntoxicity over 25 generations (with standard deviations in\nparentheses) and 2) the empirical probability of generating\ntoxic text at least once among 25 generations.\nModel\nExp. max tox.\nProb. toxicity\nGPT-2a\n0.63 (0.23)\n0.66\nGPT-3 (davinci)\n0.68 (0.27)\n0.74\nGPT-J\n0.73 (0.26)\n0.78\nCedille\n0.66 (0.27)\n0.72\nGPT-fr (base)\n0.73 (0.27)\n0.78\nTable 6: Toxicity of prompted generations.\naUpon manual inspection, it appeared that GPT-2 is unable\nto generate sensible French content, and as such the resulting\ntoxicity values can’t be compared to other models.\nFor both properties, Cedille outperforms the other models.\nWe can see again that Cedille is less toxic than GPT-J,\nindicating that the training not only improved the model’s\nFrench capabilities, but also increased its safety.\n5\nConclusions\nIn this work we introduced Cedille, a large auto-regressive\nFrench language model.\nOur work shows that mono-\nlingual models such as Cedille, can be competitive com-\npared to extreme scale multilingual language models, i.e.\nGPT-3. Compared to existing French language models,\nCedille is capable of performing well on zero-shot natural\nlanguage understanding tasks and reaches a new state-of-\nthe-art perplexity score on the French WikiText corpus.\nLastly, our approach of toxicity ﬁltering of the training\ndata led to a decrease in both maximum toxicity as well as\nthe likelihood of toxic output.\nAs a result of the ﬁnetuning approach starting from GPT-J,\nCedille has been exposed to a large amount of both English\nand French language data from the Pile and French mC4.\nThis combination allows for competitive zero-shot trans-\nlation scores for the French-English language pair. Early\nexperiments indicate that ﬁnetuning an existing English\nlanguage model and adapting it to French is more efﬁcient\neven with considerable compute and data investments (see\nappendix).\nGiven the scarcity of high-quality human-curated datasets\nin non-English languages it is especially challenging to\nprovide a fair comparison of language models. For the\nzero-shot benchmarks we observed a high degree of sen-\nsitivity towards evaluation settings such as preﬁxes, sam-\npling parameters, and type of evaluation metric. The scores\n5\n10\n100\n1K\n10K\nNumber of Generations\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nExpected Maximum Toxicity\nGPT-2\nGPT-3\nGPT-J\nGPT-fr\nCedille\nFigure 1: Unprompted expected maximum toxicity against increasing numbers of generations.\nshould therefore only be considered as a rough guidance\nand model performance may be highly task speciﬁc. In this\nwork we haven’t provided performance metrics for other\nNLP tasks such as text classiﬁcation or word sense disam-\nbiguation. Furthermore, this work focused on zero-shot\nevaluation, ignoring few-shot or ﬁnetuning approaches.\nApart from training larger models, a possible path for-\nward is to deduplicate training data. This method has been\nshown to improve end-task performance signiﬁcantly [8,\n37] but was not conducted as part of this work. In order to\nfurther reduce language model toxicity, a possible direc-\ntion is the integration of human feedback in the training\nprocess in order to reduce toxic output generation [38].\nData availability.\nCedille is available under the MIT\nLicense on the Hugging Face model hub:\nhttps:\n//huggingface.co/Cedille/fr-boris, and on our\nGitHub repository: https://github.com/coteries/\ncedille-ai. Regarding the French mC4 toxicity scores\nand toxicity analysis code, please refer to: https://\ngithub.com/coteries/real-toxicity-prompts.\nFunding.\nThis work was funded by, and conducted at,\nCoteries SA7. The model was trained on Cloud TPUs pro-\nvided by Google’s TPU Research Cloud program.\nAcknowledgments.\nWe thank Sébastien Flury and\nFrançois Bochatay for their guidance and feedback. Tiago\nCastanheiro, Flavien Bonvin and Livio Gamassia imple-\nmented the web-based Playground used to evaluate the\nmodel. Tiago Castanheiro, Flavien Bonvin, Sacha To-\nufani, Livio Gamassia, and Kasper Andkjaer tested out\nmultiple versions of the model. Sébastien Von Roth de-\nsigned the Cedille logo as well as the visual design of the\nPlayground and Cedille website8. Sonja Dossenbach as-\nsembled the dataset of recent French news. We are grateful\nto EleutherAI for publicly releasing the GPT-J model and\noffering us support on their Discord server9. We thank the\nTPU Research Cloud team for their access to Cloud TPUs\nand their support.\nReferences\n[1]\nAlec Radford et al. “Language models are unsu-\npervised multitask learners”. In: OpenAI blog 1.8\n(2019), p. 9.\n[2]\nTom B Brown et al. “Language models are few-\nshot learners”. In: arXiv preprint arXiv:2005.14165\n(2020).\n[3]\nJared Kaplan et al. “Scaling laws for neu-\nral\nlanguage\nmodels”.\nIn:\narXiv\npreprint\narXiv:2001.08361 (2020).\n[4]\nChau Tran et al. “Facebook AI WMT21 news\ntranslation task submission”. In: arXiv preprint\narXiv:2108.03265 (2021).\n[5]\nNaveen Arivazhagan et al. “Massively multilingual\nneural machine translation in the wild: Findings and\nchallenges”. In: arXiv preprint arXiv:1907.05019\n(2019).\n7https://coteries.com\n8https://cedille.ai\n9https://discord.gg/zBGx3azzUn\n6\n[6]\nAntoine Simoulin and Benoit Crabbé. “Un mod-\nèle Transformer Génératif Pré-entrainé pour le _\nfrançais”. In: Traitement Automatique des Langues\nNaturelles. ATALA. 2021, pp. 245–254.\n[7]\nJulien Launay et al. “PAGnol: An Extra-Large\nFrench Generative Model”. In: arXiv preprint\narXiv:2110.08554 (2021).\n[8]\nGuillaume Wenzek et al. “Ccnet: Extracting high\nquality monolingual datasets from web crawl data”.\nIn: arXiv preprint arXiv:1911.00359 (2019).\n[9]\nEmily M Bender et al. “On the Dangers of Stochas-\ntic Parrots: Can Language Models Be Too Big?”\nIn: Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency. 2021,\npp. 610–623.\n[10]\nIsaac Caswell et al. “Quality at a glance: An au-\ndit of web-crawled multilingual datasets”. In: arXiv\npreprint arXiv:2103.12028 (2021).\n[11]\nSamuel Gehman et al. “RealToxicityPrompts: Evalu-\nating neural toxic degeneration in language models”.\nIn: arXiv preprint arXiv:2009.11462 (2020).\n[12]\nJohannes Welbl et al. “Challenges in detox-\nifying language models”. In: arXiv preprint\narXiv:2109.07445 (2021).\n[13]\nSumanth Dathathri et al. “Plug and play language\nmodels: A simple approach to controlled text gener-\nation”. In: arXiv preprint arXiv:1912.02164 (2019).\n[14]\nBrian Lester, Rami Al-Rfou, and Noah Constant.\n“The power of scale for parameter-efﬁcient prompt\ntuning”. In: arXiv preprint arXiv:2104.08691\n(2021).\n[15]\nNitish Shirish Keskar et al. “Ctrl: A conditional\ntransformer language model for controllable gener-\nation”. In: arXiv preprint arXiv:1909.05858 (2019).\n[16]\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6\nBillion Parameter Autoregressive Language Model.\nhttps : / / github . com / kingoflolz / mesh -\ntransformer-jax. May 2021.\n[17]\nJianlin Su et al. “Roformer: Enhanced transformer\nwith rotary position embedding”. In: arXiv preprint\narXiv:2104.09864 (2021).\n[18]\nLinting Xue et al. “mT5: A massively multilin-\ngual pre-trained text-to-text transformer”. In: arXiv\npreprint arXiv:2010.11934 (2020).\n[19]\nLaura Hanu and Unitary team. Detoxify. https:\n//github.com/unitaryai/detoxify. 2020.\n[20]\nHang Le et al. “Flaubert: Unsupervised language\nmodel pre-training for french”. In: arXiv preprint\narXiv:1912.05372 (2019).\n[21]\nRobyn Speer. ftfy. Zenodo. Version 5.5. 2019. DOI:\n10.5281/zenodo.2591652. URL: https://doi.\norg/10.5281/zenodo.2591652.\n[22]\nBen Wang. Mesh-Transformer-JAX: Model-Parallel\nImplementation of Transformer Language Model\nwith JAX. https://github.com/kingoflolz/\nmesh-transformer-jax. May 2021.\n[23]\nLeo Gao et al. A framework for few-shot language\nmodel evaluation. Version v0.0.1. Sept. 2021. DOI:\n10.5281/zenodo.5371628. URL: https://doi.\norg/10.5281/zenodo.5371628.\n[24]\nLeo Gao. On the Sizes of OpenAI API Models.\nhttps://blog.eleuther.ai/gpt3- model-\nsizes/. May 2021.\n[25]\nStephen Merity et al. “Pointer sentinel mixture mod-\nels”. In: arXiv preprint arXiv:1609.07843 (2016).\n[26]\nPerplexity of ﬁxed-length models. https : / /\nhuggingface . co / docs / transformers /\nperplexity. Accessed: 2022-02-04.\n[27]\nMoussa Kamal Eddine, Antoine J-P Tixier, and\nMichalis Vazirgiannis. “BARThez: a skilled pre-\ntrained french sequence-to-sequence model”. In:\narXiv preprint arXiv:2010.12321 (2020).\n[28]\nShashi Narayan, Shay B Cohen, and Mirella La-\npata. “Don’t give me the details, just the sum-\nmary! topic-aware convolutional neural networks\nfor extreme summarization”. In: arXiv preprint\narXiv:1808.08745 (2018).\n[29]\nChin-Yew Lin. “Rouge: A package for automatic\nevaluation of summaries”. In: Text summarization\nbranches out. 2004, pp. 74–81.\n[30]\nMartin d’Hoffschmidt et al. “FQuAD: French\nquestion answering dataset”. In: arXiv preprint\narXiv:2002.06071 (2020).\n[31]\nPranav Rajpurkar et al. “SQuAD: 100,000+ ques-\ntions for machine comprehension of text”. In: arXiv\npreprint arXiv:1606.05250 (2016).\n[32]\nLouis Martin et al. “CamemBERT: a tasty\nfrench\nlanguage\nmodel”.\nIn:\narXiv\npreprint\narXiv:1911.03894 (2019).\n[33]\nOndˇrej Bojar et al. “Findings of the 2014 workshop\non statistical machine translation”. In: Proceedings\nof the ninth workshop on statistical machine trans-\nlation. 2014, pp. 12–58.\n[34]\nKishore Papineni et al. “Bleu: a method for auto-\nmatic evaluation of machine translation”. In: Pro-\nceedings of the 40th annual meeting of the Associa-\ntion for Computational Linguistics. 2002, pp. 311–\n318.\n[35]\nMatt Post. “A Call for Clarity in Reporting BLEU\nScores”. In: Proceedings of the Third Conference\non Machine Translation: Research Papers. Belgium,\nBrussels: Association for Computational Linguis-\ntics, Oct. 2018, pp. 186–191. URL: https://www.\naclweb.org/anthology/W18-6319.\n[36]\nXiaodong Liu et al. “Very deep transformers for\nneural machine translation”. In: arXiv preprint\narXiv:2008.07772 (2020).\n[37]\nKatherine Lee et al. “Deduplicating training data\nmakes language models better”. In: arXiv preprint\narXiv:2107.06499 (2021).\n[38]\nLong Ouyang et al. Training language models to\nfollow instructions with human feedback. https://\nopenai.com/blog/instruction-following/.\nJan. 2022.\n7\nSUPPLEMENTARY MATERIAL\n1\nExperiments training from scratch\nGiven the amount of compute and data available, training from scratch rather than ﬁnetuning was considered. We\nexperimented training Cedille from scratch using both the GPT-2 tokenizer (Cedille-fs-GPT2, vocab size 50,400) and\nthe GPT-fr tokenizer (Cedille-fs-GPTfr, vocab size 50.000) for 60k steps using a peak learning rate of 1.2e-4 end\nlearning rate 1.2e-5, and 7281 warm-up steps. These two variants are therefore only trained on one third of the data\ncompared to the released Cedille model (150k steps). In order to have a fair comparison we show the result of Cedille\nafter the same amount of steps (Cedille-60k). All models were trained on the same ﬁltered mC4 dataset, as described in\nthis work.\nAs shown in Table S1, Cedille-60k outperforms the from-scratch variants on the WikiText-fr benchmark. However,\ndue to compute limitations we did not run the variants for longer than 60k steps and it is possible that we could’ve\nreached similar performance after 150k steps. Furthermore, both variants perform similarly, even though they are using\na different tokenizer. Due to the variants performing very similarly, we conclude that even though a dedicated French\ntokenizer is a lot more efﬁcient at encoding French text compared to the GPT-2 tokenizer, its beneﬁt with regard to\nend-task performance was minimal in our experiments.\nModel\nPPL (byte)\nPPL (token)\nGPT-J\n1.746\n5.797\nCedille-60k\n1.673\n4.112\nCedille-fs-GPT2\n1.794\n4.972\nCedille-fs-GPTfr\n1.775\n6.856\nTable S1: Byte-level and token-level perplexities for the WikiText-fr benchmark. Cedille-60k is the Cedille model at checkpoint 60k\n(out of 150k), Cedille-fs-GPT2 and Cedille-fs-GPTfr are models trained for 60k steps on the same dataset, but with random weight\ninitialization.\n8\n",
  "categories": [
    "cs.CL",
    "68T50",
    "I.2.7"
  ],
  "published": "2022-02-07",
  "updated": "2022-02-07"
}