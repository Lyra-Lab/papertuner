{
  "id": "http://arxiv.org/abs/2202.04048v2",
  "title": "Integrating question answering and text-to-SQL in Portuguese",
  "authors": [
    "Marcos Menon José",
    "Marcelo Archanjo José",
    "Denis Deratani Mauá",
    "Fábio Gagliardi Cozman"
  ],
  "abstract": "Deep learning transformers have drastically improved systems that\nautomatically answer questions in natural language. However, different\nquestions demand different answering techniques; here we propose, build and\nvalidate an architecture that integrates different modules to answer two\ndistinct kinds of queries. Our architecture takes a free-form natural language\ntext and classifies it to send it either to a Neural Question Answering\nReasoner or a Natural Language parser to SQL. We implemented a complete system\nfor the Portuguese language, using some of the main tools available for the\nlanguage and translating training and testing datasets. Experiments show that\nour system selects the appropriate answering method with high accuracy (over\n99\\%), thus validating a modular question answering strategy.",
  "text": "Integrating question answering and text-to-SQL\nin Portuguese\nMarcos Menon Jos´e1[0000−0003−4663−4386] ⋆, Marcelo Archanjo\nJos´e2[0000−0001−7153−0402], Denis Deratani Mau´a3[0000−0003−2297−6349], and\nF´abio Gagliardi Cozman1[0000−0003−4077−4935]\n1 Escola Polit´ecnica, Universidade de S˜ao Paulo, S˜ao Paulo, Brazil\n2 Center for Artiﬁcial Intelligence (C4AI)\n3 Instituto de Matem´atica e Estat´ıstica, Universidade de S˜ao Paulo, Brazil\n{marcos.jose, marcelo.archanjo, denis.maua, fgcozman}@usp.br\nAbstract. Deep learning transformers have drastically improved sys-\ntems that automatically answer questions in natural language. However,\ndiﬀerent questions demand diﬀerent answering techniques; here we pro-\npose, build and validate an architecture that integrates diﬀerent mod-\nules to answer two distinct kinds of queries. Our architecture takes a\nfree-form natural language text and classiﬁes it to send it either to a\nNeural Question Answering Reasoner or a Natural Language parser to\nSQL. We implemented a complete system for the Portuguese language,\nusing some of the main tools available for the language and translating\ntraining and testing datasets. Experiments show that our system selects\nthe appropriate answering method with high accuracy (over 99%), thus\nvalidating a modular question answering strategy.\nKeywords: Question Answering · Transformers Networks · Natural Lan-\nguage Processing in Portuguese · Natural Language Interfaces to Databases\n1\nIntroduction\nQuestion Answering (QA) has undergone a signiﬁcant evolution over the past\nfew years with the emergence of transformer neural networks [17]. Consider\nthe Stanford Question Answering Dataset (SQuAD) task [13]: state-of-the-art\nperformance increased from about 70% accuracy in 2018 to over 90% in 2020 [23],\nthus surpassing human performance.\nDespite their success, transformer-based neural models pre-trained on mas-\nsive datasets [12,13] still fail to answer satisfactorily certain types of questions.\nThis is partly because many scalable approaches mostly repeat information in\nthe corpus, and approaches that perform reasoning scale poorly [1].\nIn practice, it seems that diﬀerent strategies better address diﬀerent types of\nquestions. While factual and simple common-sense questions can be properly an-\nswered by neural question answering models with access to sizeable unstructured\n⋆Author to whom correspondence should be addressed.\narXiv:2202.04048v2  [cs.CL]  21 Sep 2022\n2\nM. M. Jos´e et al.\ntext corpora, questions that require complex reasoning or chaining of information\nare best answered by systems based on relational databases.\nTo exploit the best of both approaches, we propose here a new architec-\nture that combines neural models trained with large, unstructured datasets, and\nneural models trained to respond to database queries states in natural language\n(text-to-SQL interfaces). Additionally, we target texts in the Portuguese lan-\nguage, as there has been little eﬀort in developing question answering tools for\nthis language.\nThe proposed architecture takes natural-language questions and selects the\nappropriate answerer using a text classiﬁer. We experimented with two pop-\nular text classiﬁers: a naive Bayes classiﬁer and a neural classiﬁer based on\nthe Portuguese pre-trained transformer network BERTimbau (Portuguese BERT\nmodel) [16], the latter showing superior performance. Once a question is clas-\nsiﬁed, it is fed either into a neural reader-retriever model composed of a BM25\n[14] retriever and a PTT5 (Portuguese T5 model) [3] reader, or into the text-\nto-SQL model mRAT-SQL (multilingual Relation-Aware Transformer SQL) [7]\nthat uses a mT5 (multilingual T5 model) [20] model to produce a SQL query\nthat is then run to generate an answer.\nOur approach diﬀers from previous eﬀorts that combine techniques to QA.\nFor example, in the landmark work on the Watson engine [6], the question is\nfeed to an ensemble of predictors and their conﬁdences scores are then used to\ndecide which answer to return (if any). The work of [10] uses BM25 and T5, in\nwhich the ﬁrst select both tables and passages related to the question, and then\nT5 has to decide and answer in SQL or text, thus using a single model to answer\nboth types of questions.\nWe evaluated our approach on a new biomedical domain QA benchmark built\nby combining and translating to Portuguese three publicly available datasets: the\nMS MARCO dataset with factual questions [12], and the text-to-SQL datasets\nSpider [21] and MIMICSQL [18]. The use of a closed-domain (biomedicine) en-\nsures that the classiﬁer learns to diﬀerentiate between question types and not\nbetween diﬀerent domains. All the relevant code, models and data are publicly\navailable.4\n2\nBackground and Tools\nIn this paper, we resort to text classiﬁers and question answering methods.\nText classiﬁcation algorithms can be divided into classical approaches and\ndeep neural networks. In the ﬁrst case, in general, the text is transformed into a\nsparse vector and one of the conventional machine learning techniques, such as\nSupport Vector Machines, is used to classify it. The state-of-the-art deep learning\napproach uses pre-trained transformers networks since they allow the transfer\nlearning, and the encoder layers can make good use of context information in\nthe input text.\n4 https://github.com/C4AI/Integrating-Question-Answering-and-Text-to-SQL-in-Portuguese\nIntegrating question answering and text-to-SQL in Portuguese\n3\nFig. 1. Neural Question Answering Reasoner using the reader-retriever architecture.\nQuestion Answering systems that deal with factual questions usu-\nally resort to two blocks: a retriever and a reader. The retriever gathers passages\n(small chunks of text) that are concatenated with the question; the reader gen-\nerates the answer (Fig. 1). Some proposals use neural transformers both in the\nretriever and the reader [8], while others use keyword-based models for the re-\ntriever [4].\nFor the translation of textual questions to SQL, known as NL2SQL,\nthere are rule-based approaches, machine-learning approaches, and mixtures of\nboth. Unlike traditional QA, NL2SQL usually deals with context-speciﬁc ques-\ntions and idiosyncratic patterns. For example, the question “How many appoint-\nments are there?” is meaningless without context, and must be grounded on the\nfacts and schema of a speciﬁed database; e.g., a table named Appointment.\n3\nProposed Architecture\nFig. 2 summarizes our proposal: a free-form natural language question is fed into\na text classiﬁer that outputs a classiﬁcation of either 1-factual or 2-text-to-SQL,\nand processed accordingly. Clearly, more classes can be added to the classiﬁer to\naddress other kinds of questions and other domains. Factual questions are sent\nto a retriever (Fig. 2.b), which sends back passages related to the question. The\nquestion and the passages go to the reader (Fig. 2.c), which ﬁnally produces the\nanswer (Fig. 2.e).\nWe tested two text classiﬁers: a naive Bayes classiﬁer with one-hot encoding\nof unigrams as features, and the BERTimbau language model, a version of the\nBERT model [5] that uses a bidirectional attention mechanism, pre-trained on\na corpus of Brazilian Portuguese documents.\nWe used BM25 [14], a keyword-based technique, as retriever, and PTT5 [3], a\nPortuguese trained transformer-based encoder-decoder neural network model, as\nreader. BM25 deals with sparse representation text document retrieval by rank-\ning passages based on their similarity with the question text. This pipeline was\nbased on the DEEPAG´E system, an implementation by Ca¸c˜ao et al. [2], which\nanswered questions from the PAQ dataset [9] about the environment translated\ninto Portuguese.\nQuestions that require relational databases are sent to a dedicated NL2SQL\nmodule. One of the most successful techniques for this is the RAT-SQL+GAP 5\n5 Relation-Aware Transformer SQL Generation-Augmented Pretraining.\n4\nM. M. Jos´e et al.\nFig. 2. Proposed architecture that deals both with factual and text-to-SQL questions.\nsystem [15] where, machine learning and a BART-large model are applied. We\nused a multilingual version of that system, named mRAT-SQL [7], with a mT5-\nlarge model (Fig. 2.d) for Text-to-SQL, which sends back a query that answers\nthe original question (Fig. 2.e).\nA signiﬁcant challenge is integrating these systems; each one has speciﬁc li-\nbraries, requirements and heavy computational demands. We developed a cross-\nplatform communication scheme through the ﬁle system using a shared mount\npoint to send and receive the questions and answers in diﬀerent machines inde-\npendent of their operating systems.\n4\nQuestion Answering Datasets\nIn order to build a system in Portuguese, it was necessary to use a dataset with\nquestions and answers in that language. As most of the resources are nowadays\navailable in English, we applied automatic translation to existing datasets in\nEnglish, thus translating questions using Google Translate and, in the case of\nMS MARCO, all the contexts and answers for training BM25+PTT5.\nThe MS MARCO dataset [12] consists of questions that were anonymized\nfrom search queries of users of the Bing platform. One of its signiﬁcant advan-\ntages is that it is based on actual questions that people ask in contrast to other\nartiﬁcially created datasets. In the end, human editors wrote down answers to\neach of the questions with the support of retrieved passages from texts on the\ninternet. To get questions from the closed domain of medicine, a partition cre-\nated by Ying Xu et al. [19] was used, which separated the question and answer\npairs using LDA topic modeling and clustering (they made the data accessible\nvia API6). Among the six available domains, the one of interest for this research\nwas biomedicine, with 31620 question/answer pairs. Table 1 shows some exam-\nples of translated natural language questions from the MS MARCO dataset.\nTo build a dataset for the classiﬁer, 112 natural language questions were se-\nlected from Spider [21] as training dataset and 100 as testing dataset.7 Those\n6 The instructions to download can be found in the project github: https://github.\ncom/ibm-aur-nlp/domain-specific-QA.\n7 Spider dataset is a popular resource that contains 200 databases with multiples\ntables under 138 domains: https://yale-lily.github.io/spider\nIntegrating question answering and text-to-SQL in Portuguese\n5\nTable 1. Questions Examples.\nDataset\nTranslated Question\nMS MARCO O que ´e f´ıstula?\nMS MARCO De onde se ramiﬁca a art´eria descendente posterior?\nSpider\nQuais s˜ao os nomes dos pacientes que marcaram uma consulta?\nSpider\nQuais s˜ao os nomes dos cientistas designados para qualquer projeto?\nMIMICSQL\nquantos pacientes com menos de 45 anos?\nMIMICSQL\nencontre o n´umero de pacientes ´unicos com diagn´ostico de miopia.\nquestions are related to 4 databases8 about health and medical domains. To-\ngether with questions taken from Spider, we added a total of 10,000 questions\ntaken from the MIMICSQL [18] set, a dataset consisting of questions whose\nanswers are SQL queries built automatically and reviewed by human editors.\nTable 1 shows more examples of natural language questions from Spider and\nfrom MIMICSQL,9 both translated from English to Portuguese.\nTo conclude this section, we oﬀer a few comments on the datasets. To build\na model that correctly classiﬁes each of the classes, the questions must have\ncharacteristics that distinguish them. Looking at Table 1, we see that some of\nthe Text to SQL questions are quite speciﬁc, like: “Quais s˜ao os nomes dos\npacientes que marcaram uma consulta?” (“What are names of patients who\nmade an appointment?”). Instead, MS MARCO-derived questions like “O que\n´e f´ıstula?” (“What is ﬁstula?”) are much more factual and the answer must be\naccessed via a knowledge base or corpus. Our dataset is a combination of two\ndiﬀerent types of datasets. One could argue that this introduces artifacts and\ntrivializes the classiﬁcation task. While we plan to evolve this research making\nthe dataset ecologically valid [11], we also argue that current QA datasets do\nshare some of the characteristics of our fabricated dataset, with questions that\nare quite easy for humans to categorize into factual or SQL-based.\n5\nExperiments\nExperiments were run in a number of machines: AMD Ryzen 9 3950X 16-Core\nProcessor, 64GB RAM, 2 GPUs NVidia GeForce RTX 3090 24GB running\nUbuntu 20.04.2 LTS for both the PTT5 reader and mRAT-SQL; and AMD\nRyzen 9 3950X 16-Core Processor, 32GB RAM, 2 GPUs NVidia GeForce RTX\n3080 10GB running Ubuntu 20.04.2 LTS for the classiﬁer and BM25.10\n8 hospital 1\n100\nquestions\n(test),\nprotein institute\n20\nquestions\n(train),\nmedicine enzyme interaction 44 questions (train), scientist 1 48 questions (train).\n9 Text-to-SQL Generation for Question Answering on Electronic Medical Records\nGithub: https://github.com/wangpinggl/TREQS.\n10 We used well-known implementations of naive Bayes [22] and transformers. In\nparticular, the tranformers HuggingFace library, at https://huggingface.co/\ntransformers/, and also simpletransformers at https://simpletransformers.ai/\ndocs/installation/.\n6\nM. M. Jos´e et al.\n5.1\nClassiﬁer\nAs the construction of each of the original datasets was diﬀerent, it is possible to\nnotice that there are some diﬀerences in the format of the questions. The biggest\none is that in the MIMICSQL and Spider datasets, most questions came with\npunctuation (interrogation or period), while in MS MARCO this did not happen\nwith the same frequency. Therefore, a cleanup was performed, taking away the\nﬁnal punctuation and some special characters so that the datasets would be\nmore similar. This was important because the trained model did not learn to\nseparate by unwanted information but to classify due to the content and type of\nquestion.\nThe team decided to match the instance number between the two dataset\nclasses for classiﬁer training. So, there were 8000 training and 1000 validation\nMIMIC questions and 112 separate for Spider training (from protein institute,\nscientist 1 and medicine enzyme: the ones related to biomedicine), we separated\nthe ﬁrst 9112 training questions from the MS MARCO dataset.\nIn order to obtain a correct evaluation of both models, Naive Bayes Classiﬁer\nand BERTimbau,11 and to compare them, cross-validation was performed in the\ndatabase with 10 folds — dividing the database into 10% pieces and training\nwith the other 90% 10 diﬀerent times. This allows a comparison between the\nmodels and to verify the standard deviation between the pieces of training and\nthus its generalizability.\nOnce the best model is decided based on the cross-validation result, it will be\ntested by checking its performance separately for each dataset. All test questions\nfrom MS MARCO biomedical, 4743, all from test MIMIC, 1000, and 100 from\nthe database Hospital 1 from Spider were selected for this step.\n5.2\nQuestion Answering Reasoner\nMS MARCO is made available with the questions gathered with their respec-\ntive answers and the context of the answer. In order to simulate a real system,\nthe questions were separated from their contexts, and the joining of all these\ndocuments formed the Knowledge Base (KB).\nThe BM25’s function is to search inside this KB to help the reader answer\nthe question. Thus, following the literature, the KB was broken into passages\nof up to 100 words (respecting punctuation), and when a question arrives, the\nmodel removes the K-passages closest to that question. The value of K chosen\nwas 5 following the lighter model of the work by Ca¸c˜ao et al. [2].\nFinally, the PTT5 pre-trained neural network was trained 12 using the ques-\ntions together with the respective 5 passages selected for the BM25 using the\ntotality of the 22134 training questions of MS MARCO. After training, the model\nwas tested with 4743 test questions and the Exact Match and Macro Average\n11 Trained using 5 epochs, learning rate of 5e-5, batch size of 32 and maximum sequence\nlength of 512.\n12 Trained using 25 epochs, learning rate of 2e-5, batch size of 32 and maximum se-\nquence length of 512.\nIntegrating question answering and text-to-SQL in Portuguese\n7\nTable 2. Top: results of the classiﬁers Naive Bayes and BERTimbau in a 10 fold cross\nvalidation in the training dataset. Bottom: results of the BERTimbau classiﬁer in the\ndiﬀerent test datasets.\nClassiﬁer Algorithm Validation F1-Score\nNaive Bayes Classiﬁer\n98.2 ± 0.3 %\nBERTimbau\n99.9 ± 0.01 %\nMS MARCO Test Accuracy Spider Test Accuracy MIMIC Test Accuracy\n99.8 %\n98.0%\n99.6%\nF1-Score metrics were veriﬁed based on an adaptation of the codes provided by\nthe SQuAD dataset team.\n6\nResults and Analyses\nThe entire architecture has inference time that depends on the type of question\nit receives. While it takes an average of 7.4 seconds to answer a factual question,\nquestions that are based on SQL tables take only 3.2 seconds on average. This\nwas expected because the Question Answering Reasoner depends on two subsys-\ntems (BM25 and PTT5) while NL2SQL depends only on one (mRAT-SQL).\nWe start by analyzing the classiﬁers. Both of them produced high F1-Scores13\nin cross-validation with 10 diﬀerent folds. While the naive Bayes classiﬁer is faster\nand lighter than BERTimbau, we decided to adopt the latter for the next steps\nas it got the best results in the validation (99.9%), as shown in Table 2(top).\nIn the testing datasets, BERTimbau kept the best results from the cross-\nvalidation, Table 2(bottom). It is possible to infer that the results of Spider\nwere slightly inferior because there were fewer training instances, but it was still\nexcellent since the model managed to generalize from similar questions MIMIC.\nThese results demonstrate that questions in diﬀerent classes are indeed quite\ndiﬀerent on average. Our proposed modular architecture does not signiﬁcantly\ndecrease the performance of the techniques that come in the following steps.\nAs much as the BERTimbau based model has a close-to-perfect result, it is in-\nteresting to analyze which of the questions in the test set are wrongly classiﬁed.\nOne of the questions in the MIMIC set is “Como os Nephrocaps s˜ao admin-\nistrados” (“How are Nephrocaps managed”), which could very well be consid-\nered a factual question given that this information could come from a text. MS\nMARCO’s question “Qual ´e o prop´osito do pedido de Hektoen Enteric Agar”\n(“What is the purpose of Hektoen’s request Enteric Agar”) was misclassiﬁed,\nprobably because many of the SQL questions are about speciﬁc people like “how\nmany patients Doctor X has seen this month”.\nOverall, results obtained with the MS MARCO Biomedical Dataset (trans-\nlated into Portuguese) are consistent with the literature [2] with Macro Average\n13 This is the standard F1-score for classiﬁcation, not the Macro Average F1-Score.\n8\nM. M. Jos´e et al.\nF1-Score of 32.0%. It is worth remembering that there is an inevitable vari-\nance depending on the dataset used, and, in the case of MS MARCO, there\nare responses that are longer than the PAQ dataset, which can increase the\ncomplexity.\nWe should note that in our implementation, mRAT-SQL with mT5-large was\nﬁne-tuned with a multilingual dataset with four languages: English (original),\nPortuguese, Spanish, and French (these three translated versions using Google\nTranslator). This extensive training dataset leads to the best results in Por-\ntuguese, even better than just training solely with the dataset in Portuguese [7].\nIn short, the architecture works as expected; we now present some examples\nthat are related to an actual application.\nWhen the model receives the question: “O que causa dor nas costas” (“What\ncauses back pain”) that is in the MS MARCO test group, it sends the question\nto the Question Answering Reasoner since the question seems factual and must\nlook for the answer in texts. So, the ﬁnal answer you get is: “A dor nas costas\n´e causada por uma queda ou levantamento pesado.” (“Back pain is caused by a\nfall or heavy lifting.”), since one of the retrieved texts is: “A dor nas costas pode\nvir de repente e durar menos de seis semanas (aguda), que pode ser causada por\numa queda ou levantamento pesado” (“Back pain can come on suddenly and last\nfor less than six weeks (acute), which can be caused by a fall or heavy lifting.”).\nAs for the question “Encontre o procedimento mais caro.” (“Find the most\nexpensive procedure.”), the model correctly classiﬁes it as an SQL question. Thus,\nthe mRAT-SQL model returns the answer “SELECT Procedures.Name FROM\nProcedures ORDER BY Procedures.Cost Asc LIMIT 1” which performs all the\nqueries correctly.\n7\nConclusion\nThis paper proposed and validated a new Question Answering architecture that\nclassiﬁes natural language questions in Portuguese to feed them to appropriate\nsystems, either a neural reader-retriever or a natural language parser to SQL.\nExperiments with real data demonstrated that a simple classiﬁer can achieve\nnear-perfect results in the closed biomedical domain. The possibility of integrat-\ning more subsystems is encouraging; while our original goal has been attained,\nthere is still room for including other models and techniques.\nSince the dataset used merges diﬀerent sources, it is possible that the results\nare overestimated. Future work to mitigate this is to increase the number of\nsources and to perform data augmentation.\n8\nAcknowledgments\nThis work was partly supported by Ita´u Unibanco S.A. through the Programa\nde Bolsas Ita´u (PBI) of the Centro de Ciˆencia de Dados da Universidade de\nS˜ao Paulo (C2D-USP); by the Center for Artiﬁcial Intelligence (C4AI) through\nIntegrating question answering and text-to-SQL in Portuguese\n9\nsupport from the S˜ao Paulo Research Foundation (FAPESP grant #2019/07665-\n4) and from the IBM Corporation; by CNPq grants no. 312180/2018-7 and\n304012/2019-0, and CAPES Finance Code 001.\nReferences\n1. Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dan-\ngers of stochastic parrots: Can language models be too big? In: Proc. of\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency. p.\n610–623. Association for Computing Machinery, New York, NY, USA (2021).\nhttps://doi.org/10.1145/3442188.3445922\n2. Ca¸c˜ao, F.N., Jos´e, M.M., Oliveira, A.S., Spindola, S., Costa, A.H.R., Cozman, F.G.:\nDeepag´e: Answering questions in portuguese about the brazilian environment. In:\nIntelligent Systems. pp. 419–433. Springer International Publishing (2021)\n3. Carmo, D., Piau, M., Campiotti, I., Nogueira, R., de Alencar Lotufo, R.: PTT5:\npretraining and validating the T5 model on brazilian portuguese data. CoRR\nabs/2008.09144 (2020), https://arxiv.org/abs/2008.09144\n4. Chen, D., Fisch, A., Weston, J., Bordes, A.: Reading Wikipedia to answer\nopen-domain questions. In: Proc. of the 55th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers). pp. 1870–\n1879. Association for Computational Linguistics, Vancouver, Canada (2017).\nhttps://doi.org/10.18653/v1/P17-1171\n5. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirec-\ntional transformers for language understanding. In: Proc. of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers). pp. 4171–4186. Association for\nComputational Linguistics (2019). https://doi.org/10.18653/v1/n19-1423\n6. Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A.A.,\nLally, A., Murdock, J.W., Nyberg, E., Prager, J., Schlaefer, N., Welty, C.: Building\nwatson: An overview of the deepQA project. AI Magazine 31(3), 59–79 (2010).\nhttps://doi.org/10.1609/aimag.v31i3.2303\n7. Jose, M.A., Cozman, F.G.: mRAT-SQL+GAP: A Portuguese Text-to-SQL Trans-\nformer. In: Intelligent Systems, Lecture Notes in Computer Science, vol. 13074, pp.\n511–525. Springer International Publishing (2021). https://doi.org/10.1007/978-3-\n030-91699-2\n8. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K¨uttler, H.,\nLewis, M., Yih, W.t., Rockt¨aschel, T., Riedel, S., Kiela, D.: Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks. In: Advances in Neural Information\nProcessing Systems. vol. 33, pp. 9459–9474. Curran Associates, Inc. (2020)\n9. Lewis, P., Wu, Y., Liu, L., Minervini, P., K¨uttler, H., Piktus, A., Stenetorp, P.,\nRiedel, S.: Paq: 65 million probably-asked questions and what you can do with\nthem. Transactions of the Association for Computational Linguistics 9, 1098–1115\n(10 2021). https://doi.org/10.1162/tacl a 00415\n10. Li, A.H., Ng, P., Xu, P., Zhu, H., Wang, Z., Xiang, B.: Dual reader-parser\non hybrid textual and tabular evidence for open domain question answering.\nIn: Proc. of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language\n10\nM. M. Jos´e et al.\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, Au-\ngust 1-6, 2021. pp. 4078–4088. Association for Computational Linguistics (2021).\nhttps://doi.org/10.18653/v1/2021.acl-long.315\n11. McCallum, A., Penn, G., Munteanu, C., Zhu, X.: Ecological validity and\nthe\nevaluation\nof\nspeech\nsummarization\nquality.\n2012\nIEEE\nWorkshop\non\nSpoken\nLanguage\nTechnology,\nSLT\n2012\n-\nProc.\npp.\n467–472\n(2012).\nhttps://doi.org/10.1109/SLT.2012.6424269\n12. Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng,\nL.: MS MARCO: A human generated MAchine reading COmprehension dataset.\nCEUR Workshop Proc. 1773, 1–10 (2016)\n13. Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.: SQuAD: 100,000+ questions for\nmachine comprehension of text. In: Proc. of the 2016 Conference on Empirical\nMethods in Natural Language Processing. pp. 2383–2392. Association for Compu-\ntational Linguistics, Austin, Texas (2016). https://doi.org/10.18653/v1/D16-1264\n14. Robertson, S., Zaragoza, H.: The probabilistic relevance framework: Bm25 and\nbeyond. Foundations and Trends in Information Retrieval 3(4), 333–389 (2009).\nhttps://doi.org/10.1561/1500000019\n15. Shi, P., Ng, P., Wang, Z., Zhu, H., Li, A.H., Wang, J., Nogueira dos Santos, C., Xi-\nang, B.: Learning contextual representations for semantic parsing with generation-\naugmented pre-training. Proc. of the AAAI Conference on Artiﬁcial Intelligence\n35(15), 13806–13814 (2021)\n16. Souza, F., Nogueira, R., Lotufo, R.: Bertimbau: Pretrained bert models for brazil-\nian portuguese. In: Cerri, R., Prati, R.C. (eds.) Intelligent Systems. pp. 403–417.\nSpringer International Publishing (2020)\n17. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nu., Polosukhin, I.: Attention is all you need. In: Proc. of the 31st International\nConference on Neural Information Processing Systems. p. 6000–6010. Curran As-\nsociates Inc., Red Hook, NY, USA (2017)\n18. Wang, P., Shi, T., Reddy, C.K.: Text-to-sql generation for question answer-\ning on electronic medical records. In: WWW ’20: The Web Conference 2020,\nTaipei,\nTaiwan,\nApril\n20-24,\n2020.\npp.\n350–361.\nACM\n/\nIW3C2\n(2020).\nhttps://doi.org/10.1145/3366423.3380120\n19. Xu, Y., Zhong, X., Yepes, A.J.J., Lau, J.H.: Forget me not: Reducing catas-\ntrophic forgetting for domain adaptation in reading comprehension. In: 2020\nInternational Joint Conference on Neural Networks (IJCNN). pp. 1–8 (2020).\nhttps://doi.org/10.1109/IJCNN48605.2020.9206891\n20. Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A.,\nBarua, A., Raﬀel, C.: mT5: A massively multilingual pre-trained text-to-\ntext transformer. In: Proc. of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies. pp. 483–498. Association for Computational Linguistics (2021).\nhttps://doi.org/10.18653/v1/2021.naacl-main.41\n21. Yu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li, I., Yao,\nQ., Roman, S., Zhang, Z., Radev, D.: Spider: A large-scale human-labeled dataset\nfor complex and cross-domain semantic parsing and text-to-SQL task. In: Proc. of\nthe 2018 Conference on Empirical Methods in Natural Language Processing. pp.\n3911–3921. Association for Computational Linguistics, Brussels, Belgium (2018).\nhttps://doi.org/10.18653/v1/D18-1425\n22. Zhang, H.: The optimality of Naive Bayes. Proc. of the Seventeenth International\nFlorida Artiﬁcial Intelligence Research Society Conference, FLAIRS 2004 2, 562–\n567 (2004)\nIntegrating question answering and text-to-SQL in Portuguese\n11\n23. Zhang, Z., Yang, J., Zhao, H.: Retrospective reader for machine reading comprehen-\nsion. Proc. of the AAAI Conference on Artiﬁcial Intelligence 35(16), 14506–14514\n(2021)\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-02-08",
  "updated": "2022-09-21"
}