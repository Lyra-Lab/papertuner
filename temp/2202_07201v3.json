{
  "id": "http://arxiv.org/abs/2202.07201v3",
  "title": "Holistic Adversarial Robustness of Deep Learning Models",
  "authors": [
    "Pin-Yu Chen",
    "Sijia Liu"
  ],
  "abstract": "Adversarial robustness studies the worst-case performance of a machine\nlearning model to ensure safety and reliability. With the proliferation of\ndeep-learning-based technology, the potential risks associated with model\ndevelopment and deployment can be amplified and become dreadful\nvulnerabilities. This paper provides a comprehensive overview of research\ntopics and foundational principles of research methods for adversarial\nrobustness of deep learning models, including attacks, defenses, verification,\nand novel applications.",
  "text": "Holistic Adversarial Robustness of Deep Learning Models\nPin-Yu Chen1, Sijia Liu2\n1 IBM Research\n2 Michigan State University\npin-yu.chen@ibm.com, liusiji5@msu.edu\nAbstract\nAdversarial robustness studies the worst-case performance\nof a machine learning model to ensure safety and reliabil-\nity. With the proliferation of deep-learning-based technology,\nthe potential risks associated with model development and\ndeployment can be ampliﬁed and become dreadful vulnera-\nbilities. This paper provides a comprehensive overview of re-\nsearch topics and foundational principles of research methods\nfor adversarial robustness of deep learning models, including\nattacks, defenses, veriﬁcation, and novel applications.\n1\nIntroduction\nDeep learning (LeCun, Bengio, and Hinton 2015) is a core\nengine that drives recent advances in artiﬁcial intelligence\n(AI) and machine learning (ML), and it has broad impacts\non our society and technology. However, there is a growing\ngap between AI technology’s creation and its deployment\nin the wild. One critical example is the lack of robustness,\nincluding natural robustness to data distribution shifts, abil-\nity in generalization and adaptation to new tasks, and worst-\ncase robustness when facing an adversary (also known as ad-\nversarial robustness). According to a recent Gartner report1,\n30% of cyberattacks by 2022 will involve data poisoning,\nmodel theft or adversarial examples. However, the industry\nseems underprepared. In a survey of 28 organizations span-\nning small as well as large organizations, 25 organizations\ndid not know how to secure their AI/ML systems (Kumar\net al. 2020). Moreover, various practical vulnerabilities and\nincidences incurred by AI-empowered systems have been re-\nported in real life, such as Adversarial ML Threat Matrix2\nand AI Incident Database3.\nTo prepare deep-learning enabled AI systems for the real\nworld and to familiarize researchers with the error-prone\nrisks hidden in the lifecycle of AI model development and\ndeployment – spanning from data collection and process-\ning, model selection and training, to model deployment and\nsystem integration – this paper aims to provide a holistic\noverview of adversarial robustness for deep learning mod-\nels. The research themes include (i) attack (risk identiﬁca-\n1https://www.gartner.com/smarterwithgartner/gartner-top-10-\nstrategic-technology-trends-for-2020\n2https://github.com/mitre/advmlthreatmatrix\n3https://incidentdatabase.ai/\nFigure 1: Holistic view of adversarial attack categories and\ncapabilities (threat models) in the training and deployment\nphases. The three types of attacks highlighted in colors (poi-\nsoning/backdoor/evasion attack) are the major focus of this\npaper. In the deployment phase, the target (victim) can be an\naccess-limited black-box system (e.g. a prediction API) or a\ntransparent white-box model.\ntion and demonstration), (ii) defense (threat detection and\nmitigation), (iii) veriﬁcation (robustness certiﬁcate), and (iv)\nnovel applications. In each theme, the fundamental concepts\nand key research principles will be presented in a uniﬁed\nand organized manner. This paper takes an overarching and\nholistic approach to introduce adversarial robustness of deep\nlearning models based on the terminology of an AI lifecycle\nin development and deployment, which differs from existing\nsurvey papers that provide an in-depth discussion on a spe-\nciﬁc threat model. The main goal of this paper is to deliver a\nprimer that provides basic concepts, systematic knowledge,\nand categorization of this rapidly evolving research ﬁeld to\nthe general audience and the broad AI/ML research commu-\nnity.\nFigure 1 shows the lifecycle of AI development and de-\nployment and different adversarial threats corresponding to\nattackers’ capabilities (also known as threat models). The\nlifecycle is further divided into two phases. The training\nphase includes data collection and pre-processing, as well as\nmodel selection (e.g. architecture search and design), hyper-\nparameter tuning, model parameter optimization, and vali-\ndation. After model training, the model is “frozen” (ﬁxed\nmodel architecture and parameters) and is ready for de-\nployment. Before deployment, there are possibly some post-\narXiv:2202.07201v3  [cs.LG]  5 Jan 2023\nSymbol\nMeaning\nfθ : Rd 7→[0, 1]K\nK-way neural network classiﬁcation model parameterized by θ\nlogit : Rd 7→RK\nlogit (pre-softmax) representation\n(x, y)\ndata sample x and its associated groundtruth label y\nˆyθ(x) ∈[K]\ntop-1 label prediction of x by fθ\nxadv\nadversarial example of x\nδ\nadversarial perturbation to x for evasion attack\n∆\nuniversal trigger pattern for backdoor attack\nt ∈[K]\ntarget label for targeted attack\nloss(fθ(x), y)\nclassiﬁcation loss (e.g. cross entropy)\ng\nattcker’s loss function\nDtrain / Dtest\noriginal training / testing dataset\nT\ndata transformation function\nTable 1: Mathematical notation.\nAttack\nObjective\nPoisoning\nDesign a poisoned dataset Dpoison such that models trained\non Dpoison fail to generalize on Dtest (i.e. ˆyθ(xtest) ̸= ytest)\nBackdoor\nEmbed a trigger ∆with a target label t to Dtrain such that\nˆyθ(xtest) = ytest but ˆyθ(xtest + ∆) = t\nEvasion (untargeted)\nGiven fθ, ﬁnd xadv such that xadv is similar to x but ˆyθ(xadv) ̸= y\nEvasion (targeted)\nGiven fθ, ﬁnd xadv such that xadv is similar to x but ˆyθ(xadv) = t\nTable 2: Objectives of adversarial attacks.\nhoc model adjustment steps such as model compression and\nquantiﬁcation for memory/energy reduction, calibration or\nrisk mitigation. The frozen model providing inference/pre-\ndiction can be deployed in a white-box or black-box man-\nner. The former means the model details are transparent to\na user (e.g. releasing the model architecture and pre-trained\nweights for neural networks), while the latter means a user\ncan access model predictions but does not know what the\nmodel is (i.e., an access-limited model), such as a prediction\nAPI. The gray-box setting is a mediocre scenario that as-\nsumes a user knows partial information about the deployed\nmodel. In some cases, a user may have knowledge of the\ntraining data and the deployed model is black-box, such as\nin the case of an AI automation service that only returns\na model prediction portal based on user-provided training\ndata. We also note that these two phases can be recurrent: a\ndeployed model can re-enter the training phase with contin-\nuous model/data updates.\nThroughout this paper, we focus on adversarial robust-\nness of neural networks for classiﬁcation tasks. Many princi-\nples in classiﬁcation can be naturally extended to other ma-\nchine learning tasks, which will be discussed in Section 4.\nBased on Figure 1, this paper will focus on training-phase\nand deployment-phase attacks driven by the limitation of\ncurrent ML techniques. While other adversarial threats con-\ncerning model/data privacy and integrity are also crucial,\nsuch as model stealing, membership inference, data leak-\nage, and model injection, which will not be covered in this\npaper. We also note that adversarial robustness of non-deep-\nlearning models such as support vector machines has been\ninvestigated. We refer the readers to (Biggio and Roli 2018)\nfor the research evolution in adversarial machine learning.\nTable 1 summarizes the main mathematical notations. We\nuse [K] = {1, 2, . . . , K} to denote the set of K class labels.\nWithout loss of generality, we assume the data inputs are\nvectorized (ﬂattened) as d-dimensional real-valued vectors,\nand the output (class conﬁdence) of the K-way neural net-\nwork classiﬁer fθ is nonnegative and sum to 1 (e.g. softmax\nas the ﬁnal layer), that is, PK\nk=1[fθ(·)]k = 1. The adversar-\nial robustness of real-valued continuous data modalities such\nas image, audio, time series, and tabular data can be studied\nbased on a uniﬁed methodology. For discrete data modalities\nsuch as texts and graphs, one can leverage their real-valued\nembeddings (Lei et al. 2019), latent representations, or con-\ntinuous relaxation of the problem formulation (e.g. topology\nattack in terms of edge addition/deletion in graph neural net-\nworks (Xu et al. 2019a)). Unless speciﬁed, in what follows\nwe will not further distinguish data modalities.\n2\nAttacks\nThis section will cover mainstream adversarial threats that\naim to manipulate the prediction and decision-making of an\nAI model through training-phase or deployment-phase at-\ntacks. Table 2 summarizes their attack objectives.\n2.1\nTraining-Phase Attacks\nTraining-phase attacks assume the ability to modify the\ntraining data to achieve malicious attempts on the result-\ning model, which can be realized through noisy data collec-\ntion such as crowdsourcing. Speciﬁcally, the memorization\neffect of deep learning models (Zhang et al. 2017; Carlini\net al. 2019b) can be leveraged as vulnerabilities. We note\nthat sometimes the term “data poisoning” entails both poi-\nsoning and backdoor attacks, though their attack objectives\nare different.\nPoisoning attack\naims to design a poisoned dataset\nDpoison such that models trained on Dpoison will fail to gen-\neralize on Dtest (i.e. ˆyθ(xtest) ̸= ytest) while the training loss\nremains similar to clean data. The poisoned dataset Dpoison\ncan be created by modifying the original training dataset\nDtrain, such as label ﬂipping, data addition/deletion, and fea-\nture modiﬁcation. The rationale is that training on Dpoison\nwill land on a “bad” local minimum of model parameters.\nTo control the amount of data modiﬁcation and reduce\nthe overall accuracy on Dtest (i.e. test accuracy), poisoning\nattack often assumes the knowledge of the target model and\nits training method (Jagielski et al. 2018). (Liu et al. 2020b)\nproposes black-box poisoning with additional conditions on\nthe training loss function. Targeted poisoning attack aims\nat manipulating the prediction of a subset of data samples\nin Dtest, which can be accomplished by clean-label poison-\ning (small perturbations to a subset of Dtrain while keeping\ntheir labels intact) (Shafahi et al. 2018; Zhu et al. 2019) or\ngradient-matching poisoning (Geiping et al. 2021).\nBackdoor attack\nis also known as Trojan attack. The cen-\ntral idea is to embed a universal trigger ∆to a subset of\ndata samples in Dtrain with a modiﬁed target label t (Gu\net al. 2019). Examples of trigger patterns are a small patch\nin images and a speciﬁc text string in sentences. Typically,\nbackdoor attack only assumes access to the training data and\ndoes not assume the knowledge of the model and its training.\nThe model fθ trained on the tampered data is called a back-\ndoored (Trojan) model. Its attack objective has two folds:\n(i) High standard accuracy in the absence of trigger – the\nbackdoored model should behave like a normal model (same\nmodel trained on untampered data), i.e., ˆyθ(xtest) = ytest. (ii)\nHigh attack success rate in the presence of trigger – the back-\ndoored model will predict any data input with the trigger as\nthe target label t, i.e., ˆyθ(xtest+∆) = t. Therefore, backdoor\nattack is stealthy and insidious. The trigger pattern can also\nbe made input-aware and dynamic (Nguyen and Tran 2020).\nThere is a growing concern of backdoor attacks in emerg-\ning machine learning systems featuring collaborative model\ntraining with local private data, such as federated learning\n(Bhagoji et al. 2019; Bagdasaryan et al. 2020). Backdoor\nattacks can be made more insidious by leveraging the in-\nnate local model/data heterogeneity and diversity (Zawad\net al. 2021). (Xie et al. 2020) proposes distributed backdoor\nattacks by trigger pattern decomposition among malicious\nclients to make the attack more stealthy and effective. We\nalso refer the readers to the detailed survey of these two at-\ntacks in (Goldblum et al. 2020).\n2.2\nDeployment-Phase Attacks\nThe objective of deployment-phase attacks is to ﬁnd a “sim-\nilar” example T (x) of x such that the ﬁxed model fθ\nNorm\nMeaning\nℓ0\nnumber of modiﬁed features\nℓ1\ntotal changes in modiﬁed features\nℓ2\nEuclidean distance between x and xadv\nℓ∞\nmaximal change in modiﬁed features\nTable 3: ℓp norm similarity measures for additive perturba-\ntion δ = xadv −x. The change in each feature (dimension)\nbetween x and xadv is measured in absolute value.\nwill evade its prediction from the original groundtruth la-\nbel y. The evasion condition can be further separated into\ntwo cases: (i) untargeted attack such that fθ(x) = y but\nfθ(T (x)) ̸= y, or (ii) targeted attack such that fθ(x) = y\nbut fθ(T (x)) = t, t ̸= y. Such T (x) is known as an adver-\nsarial example4 of x (Biggio et al. 2013; Szegedy et al. 2014;\nGoodfellow, Shlens, and Szegedy 2015), and it can be inter-\npreted as out-of-distribution sample or generalization error\n(Stutz, Hein, and Schiele 2019).\nData Similarity.\nDepending on data characteristics, spec-\nifying a transformation function T (·) that preserves data\nsimilarity between an original sample x and its transformed\nsample T (x) is a core mission for evasion attacks. The trans-\nformation can also be a composite function of semantic-\npreserving changes (Hsiung et al. 2022). A common prac-\ntice to select T (·) is through a simple additive perturbation\nδ such that xadv = x + δ, or through domain-speciﬁc knowl-\nedge such as rotation, object translation, and color changes\nfor image data (Hosseini and Poovendran 2018; Engstrom\net al. 2019). For additive perturbation (either on data input\nor parameter(s) simulating semantic changes), the ℓp norm\n(p ≥1) of δ deﬁned as ∥δ∥p ≜\n\u0010Pd\ni=1 |δi|p\u00111/p\nand the\npseudo norm ℓ0 are surrogate metrics for measuring simi-\nlarity distance. Table 3 summarizes popular choices of ℓp\nnorms and their meanings. Take image as an example, ℓ0\nnorm is used to design few-pixel (patch) attacks (Su, Var-\ngas, and Sakurai 2019), ℓ1 norm is used to generate sparse\nand small perturbations (Chen et al. 2018b), ℓ∞norm is\nused to conﬁne maximal changes in pixel values (Szegedy\net al. 2014), and mixed ℓp norms can also be used (Xu et al.\n2019b).\nEvasion Attack Taxonomy.\nEvasion attacks can be cate-\ngorized based on attackers’ knowledge of the target model.\nFigure 2 illustrates the taxonomy of different attack types.\nWhite-box attack assumes complete knowledge about the\ntarget model, including model architecture and model pa-\nrameters. Consequently, an attacker can exploit the auto\ndifferentiation function offered by deep learning packages,\nsuch as backpropagation (input gradient) from the model\noutput to the model input, to craft adversarial examples.\n4Sometimes adversarial example may carry a broader meaning\nof any data sample xadv that leads to incorrect prediction and there-\nfore dropping the dependence to a reference sample x, such as un-\nrestricted adversarial examples (Brown et al. 2018).\nFigure 2: Taxonomy and illustration of evasion attacks.\nBlack-box attack assumes an attacker can only observe\nthe model prediction of a data input (that is, a query) and\ndoes not know any other information. The target model can\nbe viewed as a black-box function and thus backpropagation\nfor computing input gradient is infeasible without knowing\nthe model details. In the soft-label black-box attack setting,\nan attacker can observe (parts of) class predictions and their\nassociated conﬁdence scores. In the hard-label black-box at-\ntack (decision-based) setting, an attacker can only observe\nthe top-1 label prediction, which is the least information re-\nquired to be returned to remain the utility of the model. In\naddition to attack success rate, query efﬁciency is also an im-\nportant metric for the performance evaluation of black-box\nattacks.\nTransfer attack is a branch of black-box attack that uses\nadversarial examples generated from a white-box surrogate\nmodel to attack the target model. The surrogate model can\nbe either pre-trained (Liu et al. 2017) or distilled from a set\nof data samples with soft labels given by the target model for\ntraining (Papernot, McDaniel, and Goodfellow 2016; Paper-\nnot et al. 2017).\nAttack formulation.\nThe process of ﬁnding an adversar-\nial perturbation δ can be formulated as a constrained opti-\nmization problem with a speciﬁed attack loss g(δ|x, y, t, θ)\nreﬂecting the attack objective (t is omitted for untargeted\nattack). The variation in problem formulations and solvers\nwill lead to different attack algorithms. We specify three ex-\namples below. Without loss of generality, we use ℓp norm as\nthe similarity measure (distortion) and untargeted attack as\nthe objective, and assume that all feasible data inputs lie in\nthe scaled space S = [0, 1]d.\n• Minimal-distortion formulation:\nMinimizeδ:x+δ∈S ∥δ∥p subject to ˆyθ(x + δ) ̸= ˆyθ(x) (1)\n• Penalty-based formulation:\nMinimizeδ:x+δ∈S ∥δ∥p + λ · g(δ|x, y, θ)\n(2)\n• Budget-based (norm bounded) formulation:\nMinimizeδ:x+δ∈S g(δ|x, y, θ) subject to ∥δ∥p ≤ϵ\n(3)\nFor untargeted attacks, the attacker’s loss can be the nega-\ntive classiﬁcation loss g(δ|x, y, θ) = −loss(fθ(x + δ), y)\nor the truncated class margin loss (using either logit or soft-\nmax output) deﬁned as g(δ|x, y, θ) = max{[logit(x+δ)]y −\nmaxk∈[K],k̸=y[logit(x + δ)]k + κ, 0}. The margin loss sug-\ngests that g(δ|x, y, θ) achieves minimal value (i.e. 0) when\nthe top-1 class conﬁdence score excluding the original class\ny satisﬁes maxk∈[K],k̸=y[logit(x+δ)]k ≥logit(x+δ)]y+κ,\nwhere κ ≥0 is a tuning parameter governing their conﬁ-\ndence gap. Similarly, for targeted attacks, the attacker’s loss\ncan be g(δ|x, y, t, θ) = loss(fθ(x+δ), t) or g(δ|x, y, t, θ) =\nmax{maxk∈[K],k̸=t[logit(x + δ)]k −[logit(x + δ)]t + κ, 0}.\nWhen implementing black-box attacks, the logit margin loss\ncan be replaced with the observable model output log fθ.\nThe attack formulation can be generalized to the univer-\nsal perturbation setting such that it simultaneously evades\nall model predictions. The universality can be w.r.t. data\nsamples (Moosavi-Dezfooli et al. 2017), model ensembles\n(Tram`er et al. 2018), or various data transformations (Atha-\nlye and Sutskever 2018). (Wang et al. 2021a) shows that\nmin-max optimization can yield effective universal pertur-\nbations.\nSelected Attack Algorithms.\nWe show some white-box\nand black-box attack algorithms driven by the three afore-\nmentioned attack formulations. For the minimal-distortion\nformulation, the attack constraint ˆyθ(x + δ) ̸= ˆyθ(x) can\nbe rewritten as maxk∈[K],k̸=y[fθ(x + δ)]k ≥[fθ(x + δ)]y,\nwhich can be used to linearize the local decision boundary\naround x and allow for efﬁcient projection to the closest lin-\nearized decision boundary, leading to white-box attack al-\ngorithms such as DeepFool (Moosavi-Dezfooli, Fawzi, and\nFrossard 2016) and fast adaptive boundary (FAB) attack\n(Croce and Hein 2020). For the penalty-based formulation,\none can use change-of-variable on δ to convert to an uncon-\nstrained optimization problem and then use binary search on\nλ to ﬁnd the smallest λ leading to successful attack (i.e.,\ng = 0), known as Carlini-Wagner (C&W) white-box attack\n(Carlini and Wagner 2017b). For the budget-based formula-\ntion, one can apply projected gradient descent (PGD), lead-\ning to the white-box PGD attack (Madry et al. 2018). At-\ntack algorithms using input gradients of the loss function\nare called gradient-based attacks.\nBlack-box attack algorithms often adopt either the\npenalty-based or budget-based formulation. Since the in-\nput gradient of the attacker’s loss is unavailable to obtain\nin the black-box setting, one principal approach is to per-\nform gradient estimation using model queries and then use\nthe estimated gradient to replace the true gradient in white-\nbox attack algorithms, leading to the zeroth-order optimiza-\ntion (ZOO) based black-box attacks (Chen et al. 2017). The\nchoices in gradient estimators (Tu et al. 2019; Nitin Bhagoji\net al. 2018; Liu et al. 2018, 2019; Ilyas et al. 2018) and\nZOO solvers (Zhao et al. 2019, 2020b; Ilyas, Engstrom, and\nMadry 2019) will give rise to different attack algorithms.\nHard-label black-box attacks can still adopt ZOO princi-\nples by spending extra queries to explore local loss land-\nscapes for gradient estimation (Cheng et al. 2019, 2020b;\nChen, Jordan, and Wainwright 2020), which is more query-\nefﬁcient than random exploration (Brendel, Rauber, and\nBethge 2018). We refer the readers to (Liu et al. 2020a) for\nmore details on ZOO methods and applications.\nPhysical adversarial example\nis a prediction-evasive\nphysical adversarial object. Examples include stop sign\n(Eykholt et al. 2018), eyeglass (Sharif et al. 2016), phys-\nical patch (Brown et al. 2017), 3D printing (Athalye and\nSutskever 2018), T-shirt (Xu et al. 2020b), and facial\nmakeup (Lin et al. 2022).\n3\nDefenses and Veriﬁcation\nDefenses are adversarial threat detection and mitigation\nstrategies, which can be divided into empirical and certi-\nﬁed defenses. We note that the interplay between attack and\ndefense is essentially a cat-and-mouse game. Many seem-\ningly successful empirical defenses were later weakened by\nadvanced attacks that are defense-aware, which gives a false\nsense of adversarial robustness due to information obfusca-\ntion (Carlini and Wagner 2017a; Athalye, Carlini, and Wag-\nner 2018). Consequently, defenses are expected to be fully\nevaluated against the best possible adaptive attacks that are\ndefense-aware (Carlini et al. 2019a; Tramer et al. 2020).\nWhile empirical robustness refers to the model perfor-\nmance against a set of known attacks, it may fail to serve as\na proper robustness indicator against advanced and unseen\nattacks. To address this issue, certiﬁed robustness is used to\nensure the model is provably robust given a set of attack con-\nditions (threat models). Veriﬁcation can be viewed as a pas-\nsive certiﬁed defense in the sense that its goal is to quantify\na given model’s (local) robustness with guarantees.\n3.1\nEmpirical Defenses\nEmpirical defenses are hardening methods applied during\nthe training/deployment phase to improve adversarial ro-\nbustness without provable guarantees of their effectiveness.\nFor training-phase attacks, data ﬁltering and model ﬁne-\ntuning are major approaches. For instance, (Tran, Li, and\nMadry 2018) shows removing outliers using learned latent\nrepresentations and retraining the model can reduce the poi-\nson effect. To inspect whether a pre-trained model has a\nbackdoor or not, Neural Cleanse (Wang et al. 2019) reverse-\nengineers potential trigger patterns for detection. (Wang\net al. 2020) proposes data-efﬁcient detectors that require\nonly one sample per class and are made data-free for con-\nvolutional neural networks. (Zhao et al. 2020a) exploits the\nmode connectivity in the loss landscape to recover a back-\ndoored model using limited clean data.\nFor deployment-phase attacks, adversarial input detec-\ntion schemes that exploit data characteristics such as spa-\ntial or temporal correlations are shown to be effective, such\nas the detection of audio adversarial examples using tempo-\nral dependency (Yang et al. 2019). For training adversarially\nrobust models, adversarial training that aims to minimize\nthe worst-case loss evaluated by perturbed examples gener-\nated during training is so far the strongest empirical defense\n(Madry et al. 2018). Speciﬁcally, the standard formulation of\nadversarial training can be expressed as the following min-\nmax optimization over training samples {xi, yi}n\ni=1:\nmin\nθ\nn\nX\ni=1\nmax\n∥δi∥p≤ϵ loss(fθ(xi + δ), yi)\n(4)\nThe worst-case loss corresponding to the inner maximiza-\ntion step is often evaluated by gradient-based attacks such\nas PGD attack (Madry et al. 2018). Variants of adversarial\ntraining methods such as TRADES (Zhang et al. 2019) and\ncustomized adversarial training (CAT) (Cheng et al. 2020a)\nhave been proposed for improved robustness. (Cheng et al.\n2021) proposes attack-independent robust training based on\nself-progression. On 18 different ImageNet pre-trained mod-\nels, (Su et al. 2018) unveils an undesirable trade-off between\nstandard accuracy and adversarial robustness. This trade-off\ncan be improved with unlabeled data (Carmon et al. 2019;\nStanforth et al. 2019). A similar study on vision transform-\ners is presented in (Shao et al. 2022). (Paul and Chen 2022)\nextends the analysis to a variety of robustness aspects be-\nyond adversarial robustness.\n3.2\nCertiﬁed Defenses\nCertiﬁed defenses provide performance guarantees on hard-\nened models. Adversarial attacks are ineffective if their\nthreat models fall within the provably robust conditions.\nFor training-phase attacks, (Steinhardt, Koh, and Liang\n2017) proposes certiﬁed data sanitization against poison-\ning attacks. (Weber et al. 2020) proposes randomized data\ntraining for certiﬁed defense against backdoor attacks. For\ndeployment-phase attacks, randomized smoothing is an ef-\nfective and model-agnostic approach that adds random\nnoises to data input to “smooth” the model and perform\nmajority voting on the model predictions. The certiﬁed ra-\ndius (region) in ℓp-norm perturbation ensuring consistent\nclass prediction can be computed by information-theoretical\napproach (Li et al. 2019), differential privacy (Lecuyer\net al. 2019), Neyman-Pearson lemma (Cohen, Rosenfeld,\nand Kolter 2019), or higher-order certiﬁcation (Mohapa-\ntra et al. 2020a). The certiﬁed defense is also recently ex-\ntended to robustify black-box victim models by leveraging\nthe technique of denoised randomized smoothing (Salman\net al. 2020; Zhang et al. 2022b).\n3.3\nVeriﬁcation\nVeriﬁcation is often used in certifying local robustness\nagainst evasion attacks. Given a neural network fθ and\na data sample x, veriﬁcation (in its simplest form) aims\nto maximally certify an ℓp-norm bounded radius r on the\nperturbation δ to ensure the model prediction on the per-\nturbed sample x + δ is consistent as long as δ is within\nthe certiﬁed region. That is, for any δ such that ∥δ∥p ≤r,\nˆyθ(x + δ) = ˆyθ(x). The certiﬁed radius is a robustness cer-\ntiﬁcate relating to the distance to the closest decision bound-\nary, which is computationally challenging (NP-complete)\nfor neural networks (Katz et al. 2017). However, its esti-\nmate (hence not a certiﬁcate) can be efﬁciently computed\nand used as a model-agnostic robustness metric, such as\nthe CLEVER score (Weng et al. 2018b). To address the\nnon-linearity induced by layer propagation in neural net-\nworks, solving for a certiﬁed radius is often cast as a relaxed\noptimization problem. The methods include convex poly-\ntope (Wong and Kolter 2018), semideﬁnite programming\n(Raghunathan, Steinhardt, and Liang 2018), dual optimiza-\ntion (Dvijotham et al. 2018), layer-wise linear bounds (Weng\net al. 2018a), and interval bound propagation (Gowal et al.\n2019). The veriﬁcation tools are also expanded to support\ngeneral network architectures (Zhang et al. 2018; Boopathy\net al. 2019; Xu et al. 2020a) and semantic adversarial ex-\namples (Mohapatra et al. 2020b). The intermediate certiﬁed\nresults can be used to train a more certiﬁable model (Wong\net al. 2018; Boopathy et al. 2021). However, scalability to\nlarge-sized neural networks remains a major challenge in\nveriﬁcation.\n4\nRemarks and Discussion\nHere we make several concluding remarks and discussions.\nNovel Applications.\nThe insights from studying adversar-\nial robustness have led to several new use cases. Adversarial\nperturbation and data poisoning are used in generating con-\ntrastive explanations (Dhurandhar et al. 2018), personal pri-\nvacy protection (Shan et al. 2020), data/model watermark-\ning and ﬁngerprinting (Sablayrolles et al. 2020; Aramoon,\nChen, and Qu 2021; Wang et al. 2021c), data-limited trans-\nfer learning (Tsai, Chen, and Ho 2020; Yang, Tsai, and Chen\n2021), and visual prompting (Bahng et al. 2022; Chen et al.\n2022b,a). Adversarial examples with proper design are also\nefﬁcient data augmentation tools to simultaneously improve\nmodel generalization and adversarial robustness (Hsu et al.\n2022; Lei et al. 2019). Other noteworthy applications in-\nclude image synthesis (Santurkar et al. 2019) generating\ncontrastive explanations (Dhurandhar et al. 2018), robust\ntext CAPTCHAs (Shao et al. 2021), reverse engineering of\ndeception (Gong et al. 2022), uncertainty calibration (Tang,\nChen, and Ho 2022), and molecule discovery (Hoffman et al.\n2022).\nAdversarial Robustness Beyond Classiﬁcation and In-\nput Perturbation.\nThe formulations and principles in at-\ntacks and defenses for classiﬁcation can be analogously ap-\nplied to other machine learning tasks. Examples include\nsequence-to-sequence translation (Cheng et al. 2020c) and\nimage captioning (Chen et al. 2018a). Beyond input pertur-\nbation, the robustness of model parameter perturbation (Tsai\net al. 2021) also relates to model quantiﬁcation (Weng et al.\n2020) and energy-efﬁcient inference (Stutz et al. 2020).\nInstilling Adversarial Robustness into Foundation Mod-\nels.\nAs foundation models (Bommasani et al. 2021) adapt\ntask-independent pre-training for general representation\nlearning followed by task-speciﬁc ﬁne-tuning for fast adap-\ntation, it is of utmost importance to understand (i) how to in-\ncorporate adversarial robustness into foundation model pre-\ntraining and (ii) how to maximize adversarial robustness\ntransfer from pre-training to ﬁne-tuning. (Fan et al. 2021;\nWang et al. 2021b) show promising results in adversarial ro-\nbustness preservation and transfer in meta learning and con-\ntrastive learning. The rapid growth and intensifying demand\non foundation models create a unique opportunity to advo-\ncate adversarial robustness as a necessary native property\nin next-generation trustworthy AI tools and call for novel\nmethods for evaluating representational robustness, such as\nin (Ko et al. 2022).\nPractical Adversarial Robustness at Scale.\nFrom an in-\ndustrial viewpoint, current solutions to strengthen adversar-\nial robustness may not be ideal because of the unacceptable\nperformance drop on the original task and the poor scalabil-\nity of effective defenses to industry-scale large deep learn-\ning models and systems. While there are some efforts for\nenabling adversarial training at scale, such as (Zhang et al.\n2022a), the notable tradeoff between standard accuracy and\nrobust accuracy may not be a favorable solution for busi-\nness adoption. An alternative can be rethinking the evalu-\nation methodology of adversarial robustness. For example,\ninstead of aiming to mitigate the robustness-accuracy trade-\noff, we can compare the unilateral robustness gain under the\nconstraint of making minimal (or even zero) harm to the\noriginal model utility (e.g. test accuracy). Moreover, an ideal\ndefense should be lightweight and deployable in a plug-and-\nplay manner for any given model, instead of demanding to\ntrain a model from scratch for improved robustness.\nReferences\nAramoon, O.; Chen, P.-Y.; and Qu, G. 2021. Don’t Forget to\nSign the Gradients! MLSyS, 3.\nAthalye, A.; Carlini, N.; and Wagner, D. 2018. Obfuscated\ngradients give a false sense of security: Circumventing de-\nfenses to adversarial examples. ICML.\nAthalye, A.; and Sutskever, I. 2018. Synthesizing robust ad-\nversarial examples. ICML.\nBagdasaryan, E.; Veit, A.; Hua, Y.; Estrin, D.; and\nShmatikov, V. 2020. How to backdoor federated learning.\nAISTATS.\nBahng, H.; Jahanian, A.; Sankaranarayanan, S.; and Isola, P.\n2022. Visual Prompting: Modifying Pixel Space to Adapt\nPre-trained Models. arXiv preprint arXiv:2203.17274.\nBhagoji, A. N.; Chakraborty, S.; Mittal, P.; and Calo, S.\n2019. Analyzing federated learning through an adversarial\nlens. ICML, 634–643.\nBiggio, B.; Corona, I.; Maiorca, D.; Nelson, B.; ˇSrndi´c, N.;\nLaskov, P.; Giacinto, G.; and Roli, F. 2013. Evasion attacks\nagainst machine learning at test time. ECML PKDD.\nBiggio, B.; and Roli, F. 2018. Wild patterns: Ten years after\nthe rise of adversarial machine learning. Pattern Recogni-\ntion, 84: 317–331.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.;\nArora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut,\nA.; Brunskill, E.; et al. 2021. On the opportunities and risks\nof foundation models. arXiv preprint arXiv:2108.07258.\nBoopathy, A.; Weng, T.-W.; Chen, P.-Y.; Liu, S.; and Daniel,\nL. 2019. CNN-cert: An efﬁcient framework for certifying\nrobustness of convolutional neural networks. AAAI.\nBoopathy, A.; Weng, T.-W.; Liu, S.; Chen, P.-Y.; Zhang, G.;\nand Daniel, L. 2021. Fast Training of Provably Robust Neu-\nral Networks by SingleProp. AAAI.\nBrendel, W.; Rauber, J.; and Bethge, M. 2018. Decision-\nBased Adversarial Attacks: Reliable Attacks Against Black-\nBox Machine Learning Models. ICLR.\nBrown, T. B.; Carlini, N.; Zhang, C.; Olsson, C.; Christiano,\nP.; and Goodfellow, I. 2018. Unrestricted adversarial exam-\nples. arXiv preprint arXiv:1809.08352.\nBrown, T. B.; Man´e, D.; Roy, A.; Abadi, M.; and Gilmer, J.\n2017. Adversarial patch. arXiv preprint arXiv:1712.09665.\nCarlini, N.; Athalye, A.; Papernot, N.; Brendel, W.; Rauber,\nJ.; Tsipras, D.; Goodfellow, I.; Madry, A.; and Kurakin, A.\n2019a. On evaluating adversarial robustness. arXiv preprint\narXiv:1902.06705.\nCarlini, N.; Liu, C.; Erlingsson, ´U.; Kos, J.; and Song, D.\n2019b. The secret sharer: Evaluating and testing unintended\nmemorization in neural networks. USENIX Security, 267–\n284.\nCarlini, N.; and Wagner, D. 2017a. Adversarial examples are\nnot easily detected: Bypassing ten detection methods. ACM\nWorkshop on Artiﬁcial Intelligence and Security, 3–14.\nCarlini, N.; and Wagner, D. 2017b. Towards evaluating the\nrobustness of neural networks. IEEE S&P, 39–57.\nCarmon, Y.; Raghunathan, A.; Schmidt, L.; Liang, P.; and\nDuchi, J. C. 2019. Unlabeled data improves adversarial ro-\nbustness. NeurIPS.\nChen, A.; Lorenz, P.; Yao, Y.; Chen, P.-Y.; and Liu, S.\n2022a. Visual Prompting for Adversarial Robustness. arXiv\npreprint arXiv:2210.06284.\nChen, A.; Yao, Y.; Chen, P.-Y.; Zhang, Y.; and Liu, S. 2022b.\nUnderstanding and Improving Visual Prompting: A Label-\nMapping Perspective. arXiv preprint arXiv:2211.11635.\nChen, H.; Zhang, H.; Chen, P.-Y.; Yi, J.; and Hsieh, C.-J.\n2018a. Attacking visual language grounding with adversar-\nial examples: A case study on neural image captioning. ACL,\n1: 2587–2597.\nChen, J.; Jordan, M. I.; and Wainwright, M. J. 2020. Hop-\nskipjumpattack: A query-efﬁcient decision-based attack.\nIEEE S&P.\nChen, P.-Y.; Sharma, Y.; Zhang, H.; Yi, J.; and Hsieh, C.-J.\n2018b. EAD: elastic-net attacks to deep neural networks via\nadversarial examples. AAAI, 10–17.\nChen, P.-Y.; Zhang, H.; Sharma, Y.; Yi, J.; and Hsieh, C.-J.\n2017. ZOO: Zeroth Order Optimization Based Black-box\nAttacks to Deep Neural Networks Without Training Substi-\ntute Models. ACM Workshop on Artiﬁcial Intelligence and\nSecurity, 15–26.\nCheng, M.; Chen, P.-Y.; Liu, S.; Chang, S.; Hsieh, C.-J.; and\nDas, P. 2021. Self-Progressing Robust Training. AAAI.\nCheng, M.; Le, T.; Chen, P.-Y.; Yi, J.; Zhang, H.; and Hsieh,\nC.-J. 2019. Query-efﬁcient hard-label black-box attack: An\noptimization-based approach. ICLR.\nCheng, M.; Lei, Q.; Chen, P.-Y.; Dhillon, I.; and Hsieh, C.-J.\n2020a. Cat: Customized adversarial training for improved\nrobustness. arXiv preprint arXiv:2002.06789.\nCheng, M.; Singh, S.; Chen, P. H.; Chen, P.-Y.; Liu, S.; and\nHsieh, C.-J. 2020b.\nSign-OPT: A Query-Efﬁcient Hard-\nlabel Adversarial Attack. ICLR.\nCheng, M.; Yi, J.; Zhang, H.; Chen, P.-Y.; and Hsieh, C.-J.\n2020c. Seq2Sick: Evaluating the Robustness of Sequence-\nto-Sequence Models with Adversarial Examples. AAAI.\nCohen, J. M.; Rosenfeld, E.; and Kolter, J. Z. 2019. Certiﬁed\nadversarial robustness via randomized smoothing. ICML.\nCroce, F.; and Hein, M. 2020. Minimally distorted adver-\nsarial examples with a fast adaptive boundary attack. ICML,\n2196–2205.\nDhurandhar, A.; Chen, P.-Y.; Luss, R.; Tu, C.-C.; Ting, P.;\nShanmugam, K.; and Das, P. 2018. Explanations based on\nthe missing: Towards contrastive explanations with pertinent\nnegatives. NeurIPS.\nDvijotham, K.; Stanforth, R.; Gowal, S.; Mann, T. A.; and\nKohli, P. 2018. A Dual Approach to Scalable Veriﬁcation of\nDeep Networks. UAI, 1(2): 3.\nEngstrom, L.; Tran, B.; Tsipras, D.; Schmidt, L.; and Madry,\nA. 2019.\nExploring the landscape of spatial robustness.\nICML, 1802–1811.\nEykholt, K.; Evtimov, I.; Fernandes, E.; Li, B.; Rahmati, A.;\nXiao, C.; Prakash, A.; Kohno, T.; and Song, D. 2018. Robust\nphysical-world attacks on deep learning visual classiﬁcation.\nCVPR, 1625–1634.\nFan, L.; Liu, S.; Chen, P.-Y.; Zhang, G.; and Gan, C. 2021.\nWhen Does Contrastive Learning Preserve Adversarial Ro-\nbustness from Pretraining to Finetuning? NeurIPS, 34.\nGeiping, J.; Fowl, L.; Huang, W. R.; Czaja, W.; Taylor, G.;\nMoeller, M.; and Goldstein, T. 2021. Witches’ Brew: Indus-\ntrial Scale Data Poisoning via Gradient Matching. ICLR.\nGoldblum,\nM.;\nTsipras,\nD.;\nXie,\nC.;\nChen,\nX.;\nSchwarzschild, A.; Song, D.; Madry, A.; Li, B.; and\nGoldstein, T. 2020. Data Security for Machine Learning:\nData Poisoning, Backdoor Attacks, and Defenses.\narXiv\npreprint arXiv:2012.10544.\nGong, Y.; Yao, Y.; Li, Y.; Zhang, Y.; Liu, X.; Lin, X.;\nand Liu, S. 2022.\nReverse Engineering of Impercep-\ntible Adversarial Image Perturbations.\narXiv preprint\narXiv:2203.14145.\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Explain-\ning and harnessing adversarial examples. ICLR.\nGowal, S.; Dvijotham, K. D.; Stanforth, R.; Bunel, R.; Qin,\nC.; Uesato, J.; Arandjelovic, R.; Mann, T.; and Kohli, P.\n2019. Scalable veriﬁed training for provably robust image\nclassiﬁcation. ICCV, 4842–4851.\nGu, T.; Liu, K.; Dolan-Gavitt, B.; and Garg, S. 2019. Bad-\nNets: Evaluating Backdooring Attacks on Deep Neural Net-\nworks. IEEE Access, 7: 47230–47244.\nHoffman, S. C.; Chenthamarakshan, V.; Wadhawan, K.;\nChen, P.-Y.; and Das, P. 2022. Optimizing molecules using\nefﬁcient queries from property evaluations. Nature Machine\nIntelligence, 4(1): 21–31.\nHosseini, H.; and Poovendran, R. 2018. Semantic adversar-\nial examples. CVPR Workshops, 1614–1619.\nHsiung, L.; Tsai, Y.-Y.; Chen, P.-Y.; and Ho, T.-Y. 2022. To-\nwards Compositional Adversarial Robustness: Generalizing\nAdversarial Training to Composite Semantic Perturbations.\narXiv preprint arXiv:2202.04235.\nHsu, C.-Y.; Chen, P.-Y.; Lu, S.; Liu, S.; and Yu, C.-M. 2022.\nAdversarial Examples can be Effective Data Augmentation\nfor Unsupervised Machine Learning. In AAAI.\nIlyas, A.; Engstrom, L.; Athalye, A.; and Lin, J. 2018.\nBlack-box Adversarial Attacks with Limited Queries and In-\nformation. ICML.\nIlyas, A.; Engstrom, L.; and Madry, A. 2019. Prior convic-\ntions: Black-box adversarial attacks with bandits and priors.\nICLR.\nJagielski, M.; Oprea, A.; Biggio, B.; Liu, C.; Nita-Rotaru,\nC.; and Li, B. 2018. Manipulating machine learning: Poi-\nsoning attacks and countermeasures for regression learning.\nIEEE S&P, 19–35.\nKatz, G.; Barrett, C.; Dill, D. L.; Julian, K.; and Kochen-\nderfer, M. J. 2017. Reluplex: An efﬁcient SMT solver for\nverifying deep neural networks. International Conference\non Computer Aided Veriﬁcation, 97–117.\nKo, C.-Y.; Chen, P.-Y.; Mohapatra, J.; Das, P.; and Daniel,\nL. 2022. SynBench: Task-Agnostic Benchmarking of Pre-\ntrained Representations using Synthetic Data. arXiv preprint\narXiv:2210.02989.\nKumar, R. S. S.; Nystr¨om, M.; Lambert, J.; Marshall, A.;\nGoertzel, M.; Comissoneru, A.; Swann, M.; and Xia, S.\n2020. Adversarial machine learning-industry perspectives.\nIEEE S&P Workshops, 69–75.\nLeCun, Y.; Bengio, Y.; and Hinton, G. 2015. Deep learning.\nNature.\nLecuyer, M.; Atlidakis, V.; Geambasu, R.; Hsu, D.; and Jana,\nS. 2019. Certiﬁed robustness to adversarial examples with\ndifferential privacy. IEEE S&P, 656–672.\nLei, Q.; Wu, L.; Chen, P.-Y.; Dimakis, A. G.; Dhillon, I. S.;\nand Witbrock, M. 2019. Discrete Adversarial Attacks and\nSubmodular Optimization with Applications to Text Classi-\nﬁcation. SysML.\nLi, B.; Chen, C.; Wang, W.; and Carin, L. 2019. Certiﬁed\nadversarial robustness with additive noise. NeurIPS.\nLin, C.-S.; Hsu, C.-Y.; Chen, P.-Y.; and Yu, C.-M. 2022.\nReal-World Adversarial Examples Via Makeup.\nIn IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2854–2858.\nLiu, S.; Chen, P.-Y.; Chen, X.; and Hong, M. 2019. signSGD\nvia Zeroth-Order Oracle. ICLR.\nLiu, S.; Chen, P.-Y.; Kailkhura, B.; Zhang, G.; Hero, A.; and\nVarshney, P. K. 2020a. A Primer on Zeroth-Order Optimiza-\ntion in Signal Processing and Machine Learning. IEEE Sig-\nnal Processing Magazine.\nLiu, S.; Kailkhura, B.; Chen, P.-Y.; Ting, P.; Chang, S.; and\nAmini, L. 2018. Zeroth-order stochastic variance reduction\nfor nonconvex optimization. NeurIPS, 3731–3741.\nLiu, S.; Lu, S.; Chen, X.; Feng, Y.; Xu, K.; Al-Dujaili,\nA.; Hong, M.; and O’Reilly, U.-M. 2020b. Min-max op-\ntimization without gradients: Convergence and applications\nto black-box evasion and poisoning attacks. ICML, 6282–\n6293.\nLiu, Y.; Chen, X.; Liu, C.; and Song, D. 2017.\nDelv-\ning into transferable adversarial examples and black-box at-\ntacks. ICLR.\nMadry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and\nVladu, A. 2018. Towards Deep Learning Models Resistant\nto Adversarial Attacks. ICLR.\nMohapatra, J.; Ko, C.-Y.; Weng, T.-W.; Chen, P.-Y.; Liu, S.;\nand Daniel, L. 2020a. Higher-Order Certiﬁcation for Ran-\ndomized Smoothing. NeurIPS.\nMohapatra, J.; Weng, T.-W.; Chen, P.-Y.; Liu, S.; and Daniel,\nL. 2020b. Towards verifying robustness of neural networks\nagainst a family of semantic perturbations. CVPR, 244–252.\nMoosavi-Dezfooli, S.-M.; Fawzi, A.; Fawzi, O.; and\nFrossard, P. 2017.\nUniversal Adversarial Perturbations.\nCVPR, 86–94.\nMoosavi-Dezfooli, S.-M.; Fawzi, A.; and Frossard, P. 2016.\nDeepfool: a simple and accurate method to fool deep neural\nnetworks. CVPR, 2574–2582.\nNguyen, A.; and Tran, A. 2020. Input-aware dynamic back-\ndoor attack. NeurIPS.\nNitin Bhagoji, A.; He, W.; Li, B.; and Song, D. 2018. Prac-\ntical Black-box Attacks on Deep Neural Networks using Ef-\nﬁcient Query Mechanisms. ECCV, 154–169.\nPapernot, N.; McDaniel, P.; and Goodfellow, I. 2016. Trans-\nferability in machine learning: from phenomena to black-\nbox attacks using adversarial samples.\narXiv preprint\narXiv:1605.07277.\nPapernot, N.; McDaniel, P.; Goodfellow, I.; Jha, S.; Celik,\nZ. B.; and Swami, A. 2017.\nPractical black-box attacks\nagainst machine learning. ACM Asia Conference on Com-\nputer and Communications Security, 506–519.\nPaul, S.; and Chen, P.-Y. 2022. Vision transformers are ro-\nbust learners. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 36, 2071–2081.\nRaghunathan, A.; Steinhardt, J.; and Liang, P. 2018. Certi-\nﬁed a against adversarial examples. ICLR.\nSablayrolles, A.; Douze, M.; Schmid, C.; and J´egou, H.\n2020. Radioactive data: tracing through training. ICML.\nSalman, H.; Sun, M.; Yang, G.; Kapoor, A.; and Kolter, J. Z.\n2020.\nDenoised smoothing: A provable defense for pre-\ntrained classiﬁers. Advances in Neural Information Process-\ning Systems, 33: 21945–21957.\nSanturkar, S.; Tsipras, D.; Tran, B.; Ilyas, A.; Engstrom, L.;\nand Madry, A. 2019. Image synthesis with a single (robust)\nclassiﬁer. arXiv preprint arXiv:1906.09453.\nShafahi, A.; Huang, W. R.; Najibi, M.; Suciu, O.; Studer, C.;\nDumitras, T.; and Goldstein, T. 2018. Poison frogs! targeted\nclean-label poisoning attacks on neural networks. NeurIPS,\n6103–6113.\nShan, S.; Wenger, E.; Zhang, J.; Li, H.; Zheng, H.; and Zhao,\nB. Y. 2020. Fawkes: Protecting privacy against unauthorized\ndeep learning models. USENIX Security.\nShao, R.; Shi, Z.; Yi, J.; Chen, P.-Y.; and Hsieh, C.-J.\n2021. Robust Text CAPTCHAs Using Adversarial Exam-\nples. arXiv preprint arXiv:2101.02483.\nShao, R.; Shi, Z.; Yi, J.; Chen, P.-Y.; and Hsieh, C.-J.\n2022. On the Adversarial Robustness of Vision Transform-\ners. Transactions on Machine Learning Research.\nSharif, M.; Bhagavatula, S.; Bauer, L.; and Reiter, M. K.\n2016. Accessorize to a crime: Real and stealthy attacks on\nstate-of-the-art face recognition. ACM CCS, 1528–1540.\nStanforth, R.; Fawzi, A.; Kohli, P.; et al. 2019. Are Labels\nRequired for Improving Adversarial Robustness? NeurIPS.\nSteinhardt, J.; Koh, P. W.; and Liang, P. 2017. Certiﬁed de-\nfenses for data poisoning attacks. NeurIPS.\nStutz, D.; Chandramoorthy, N.; Hein, M.; and Schiele, B.\n2020. Bit Error Robustness for Energy-Efﬁcient DNN Ac-\ncelerators. arXiv preprint arXiv:2006.13977.\nStutz, D.; Hein, M.; and Schiele, B. 2019. Disentangling ad-\nversarial robustness and generalization. CVPR, 6976–6987.\nSu, D.; Zhang, H.; Chen, H.; Yi, J.; Chen, P.-Y.; and Gao,\nY. 2018. Is robustness the cost of accuracy? A comprehen-\nsive study on the robustness of 18 deep image classiﬁcation\nmodels. ECCV.\nSu, J.; Vargas, D. V.; and Sakurai, K. 2019. One pixel at-\ntack for fooling deep neural networks. IEEE Transactions\non Evolutionary Computation, 23(5): 828–841.\nSzegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,\nD.; Goodfellow, I.; and Fergus, R. 2014. Intriguing proper-\nties of neural networks. ICLR.\nTang, Y.-C.; Chen, P.-Y.; and Ho, T.-Y. 2022.\nNeu-\nral Clamping: Joint Input Perturbation and Temperature\nScaling for Neural Network Calibration.\narXiv preprint\narXiv:2209.11604.\nTramer, F.; Carlini, N.; Brendel, W.; and Madry, A. 2020. On\nadaptive attacks to adversarial example defenses. NeurIPS.\nTram`er, F.; Kurakin, A.; Papernot, N.; Boneh, D.; and Mc-\nDaniel, P. 2018.\nEnsemble Adversarial Training: Attacks\nand Defenses. ICLR.\nTran, B.; Li, J.; and Madry, A. 2018. Spectral signatures in\nbackdoor attacks. NeurIPS, 8000–8010.\nTsai, Y.-L.; Hsu, C.-Y.; Yu, C.-M.; and Chen, P.-Y. 2021.\nFormalizing Generalization and Adversarial Robustness of\nNeural Networks to Weight Perturbations. NeurIPs, 34.\nTsai, Y.-Y.; Chen, P.-Y.; and Ho, T.-Y. 2020. Transfer learn-\ning without knowing: Reprogramming black-box machine\nlearning models with scarce data and limited resources.\nICML, 9614–9624.\nTu, C.-C.; Ting, P.; Chen, P.-Y.; Liu, S.; Zhang, H.; Yi,\nJ.; Hsieh, C.-J.; and Cheng, S.-M. 2019.\nAutozoom:\nAutoencoder-based zeroth order optimization method for at-\ntacking black-box neural networks. AAAI, 33: 742–749.\nWang, B.; Yao, Y.; Shan, S.; Li, H.; Viswanath, B.; Zheng,\nH.; and Zhao, B. Y. 2019. Neural cleanse: Identifying and\nmitigating backdoor attacks in neural networks. IEEE S&P,\n707–723.\nWang, J.; Zhang, T.; Liu, S.; Chen, P.-Y.; Xu, J.; Fardad, M.;\nand Li, B. 2021a. Adversarial attack generation empowered\nby min-max optimization. NeurIPS, 34.\nWang, R.; Xu, K.; Liu, S.; Chen, P.-Y.; Weng, T.-W.; Gan,\nC.; and Wang, M. 2021b. On Fast Adversarial Robustness\nAdaptation in Model-Agnostic Meta-Learning. ICLR.\nWang, R.; Zhang, G.; Liu, S.; Chen, P.-Y.; Xiong, J.; and\nWang, M. 2020.\nPractical detection of trojan neural net-\nworks: Data-limited and data-free cases. ECCV, 222–238.\nWang, S.; Wang, X.; Chen, P. Y.; Zhao, P.; and Lin, X.\n2021c.\nCharacteristic Examples: High-Robustness, Low-\nTransferability Fingerprinting of Neural Networks. IJCAI.\nWeber, M.; Xu, X.; Karlas, B.; Zhang, C.; and Li, B. 2020.\nRab: Provable robustness against backdoor attacks. arXiv\npreprint arXiv:2003.08904.\nWeng, T.-W.; Zhang, H.; Chen, H.; Song, Z.; Hsieh, C.-J.;\nBoning, D.; Dhillon, I. S.; and Daniel, L. 2018a. Towards\nFast Computation of Certiﬁed Robustness for ReLU Net-\nworks. International Coference on ICML.\nWeng, T.-W.; Zhang, H.; Chen, P.-Y.; Yi, J.; Su, D.; Gao, Y.;\nHsieh, C.-J.; and Daniel, L. 2018b. Evaluating the Robust-\nness of Neural Networks: An Extreme Value Theory Ap-\nproach. ICLR.\nWeng, T.-W.; Zhao, P.; Liu, S.; Chen, P.-Y.; Lin, X.; and\nDaniel, L. 2020. Towards Certiﬁcated Model Robustness\nAgainst Weight Perturbations. AAAI, 6356–6363.\nWong, E.; and Kolter, Z. 2018. Provable defenses against\nadversarial examples via the convex outer adversarial poly-\ntope. ICML.\nWong, E.; Schmidt, F. R.; Metzen, J. H.; and Kolter, J. Z.\n2018. Scaling provable adversarial defenses. NeurIPS.\nXie, C.; Huang, K.; Chen, P.-Y.; and Li, B. 2020. DBA:\nDistributed Backdoor Attacks against Federated Learning.\nICLR.\nXu, K.; Chen, H.; Liu, S.; Chen, P.-Y.; Weng, T.-W.; Hong,\nM.; and Lin, X. 2019a.\nTopology attack and defense for\ngraph neural networks: An optimization perspective. IJCAI.\nXu, K.; Liu, S.; Zhao, P.; Chen, P.-Y.; Zhang, H.; Fan, Q.;\nErdogmus, D.; Wang, Y.; and Lin, X. 2019b. Structured ad-\nversarial attack: Towards general implementation and better\ninterpretability. ICLR.\nXu, K.; Shi, Z.; Zhang, H.; Huang, M.; Chang, K.-W.;\nKailkhura, B.; Lin, X.; and Hsieh, C.-J. 2020a. Automatic\nperturbation analysis on general computational graphs.\nNeurIPS.\nXu, K.; Zhang, G.; Liu, S.; Fan, Q.; Sun, M.; Chen, H.;\nChen, P.-Y.; Wang, Y.; and Lin, X. 2020b. Adversarial t-\nshirt! evading person detectors in a physical world. ECCV.\nYang, C.-H. H.; Tsai, Y.-Y.; and Chen, P.-Y. 2021.\nVoice2Series: Reprogramming Acoustic Models for Time\nSeries Classiﬁcation. In ICML.\nYang, Z.; Li, B.; Chen, P.-Y.; and Song, D. 2019. Character-\nizing Audio Adversarial Examples Using Temporal Depen-\ndency. ICLR.\nZawad, S.; Ali, A.; Chen, P.-Y.; Anwar, A.; Zhou, Y.; Bara-\ncaldo, N.; Tian, Y.; and Yan, F. 2021. Curse or Redemption?\nHow Data Heterogeneity Affects the Robustness of Feder-\nated Learning. AAAI.\nZhang, C.; Bengio, S.; Hardt, M.; Recht, B.; and Vinyals,\nO. 2017. Understanding deep learning requires rethinking\ngeneralization. ICLR.\nZhang, G.; Lu, S.; Zhang, Y.; Chen, X.; Chen, P.-Y.; Fan,\nQ.; Martie, L.; Horesh, L.; Hong, M.; and Liu, S. 2022a.\nDistributed adversarial training to robustify deep neural net-\nworks at scale.\nIn Uncertainty in Artiﬁcial Intelligence,\n2353–2363. PMLR.\nZhang, H.; Weng, T.-W.; Chen, P.-Y.; Hsieh, C.-J.; and\nDaniel, L. 2018. Efﬁcient neural network robustness cer-\ntiﬁcation with general activation functions. NeurIPS, 4944–\n4953.\nZhang, H.; Yu, Y.; Jiao, J.; Xing, E.; El Ghaoui, L.; and Jor-\ndan, M. 2019. Theoretically principled trade-off between\nrobustness and accuracy. ICML, 7472–7482.\nZhang, Y.; Yao, Y.; Jia, J.; Yi, J.; Hong, M.; Chang, S.; and\nLiu, S. 2022b. How to Robustify Black-Box ML Models?\nA Zeroth-Order Optimization Perspective. In International\nConference on Learning Representations.\nZhao, P.; Chen, P.-Y.; Das, P.; Ramamurthy, K. N.; and Lin,\nX. 2020a. Bridging Mode Connectivity in Loss Landscapes\nand Adversarial Robustness. ICLR.\nZhao, P.; Chen, P.-Y.; Wang, S.; and Lin, X. 2020b.\nTo-\nwards Query-Efﬁcient Black-Box Adversary with Zeroth-\nOrder Natural Gradient Descent. AAAI.\nZhao, P.; Liu, S.; Chen, P.-Y.; Hoang, N.; Xu, K.; Kailkhura,\nB.; and Lin, X. 2019. On the Design of Black-box Adver-\nsarial Examples by Leveraging Gradient-free Optimization\nand Operator Splitting Method. ICCV, 121–130.\nZhu, C.; Huang, W. R.; Li, H.; Taylor, G.; Studer, C.; and\nGoldstein, T. 2019. Transferable clean-label poisoning at-\ntacks on deep neural nets. ICML, 7614–7623.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CR"
  ],
  "published": "2022-02-15",
  "updated": "2023-01-05"
}