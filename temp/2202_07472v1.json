{
  "id": "http://arxiv.org/abs/2202.07472v1",
  "title": "Sequential Bayesian experimental designs via reinforcement learning",
  "authors": [
    "Hikaru Asano"
  ],
  "abstract": "Bayesian experimental design (BED) has been used as a method for conducting\nefficient experiments based on Bayesian inference. The existing methods,\nhowever, mostly focus on maximizing the expected information gain (EIG); the\ncost of experiments and sample efficiency are often not taken into account. In\norder to address this issue and enhance practical applicability of BED, we\nprovide a new approach Sequential Experimental Design via Reinforcement\nLearning to construct BED in a sequential manner by applying reinforcement\nlearning in this paper. Here, reinforcement learning is a branch of machine\nlearning in which an agent learns a policy to maximize its reward by\ninteracting with the environment. The characteristics of interacting with the\nenvironment are similar to the sequential experiment, and reinforcement\nlearning is indeed a method that excels at sequential decision making.\n  By proposing a new real-world-oriented experimental environment, our approach\naims to maximize the EIG while keeping the cost of experiments and sample\nefficiency in mind simultaneously. We conduct numerical experiments for three\ndifferent examples. It is confirmed that our method outperforms the existing\nmethods in various indices such as the EIG and sampling efficiency, indicating\nthat our proposed method and experimental environment can make a significant\ncontribution to application of BED to the real world.",
  "text": "Bachelor thesis\nSequential Bayesian experimental designs via\nreinforcement learning\nHikaru Asano\nSystems Innovation, Faculty of Engineering, The University of Tokyo\nFebruary 16, 2022\nSupervised by Prof. Takashi Goda\narXiv:2202.07472v1  [cs.LG]  14 Feb 2022\nAbstract\nBayesian experimental design (BED) has been used as a method for conducting eﬃcient experiments\nbased on Bayesian inference. The existing methods, however, mostly focus on maximizing the expected\ninformation gain (EIG); the cost of experiments and sample eﬃciency are often not taken into account.\nIn order to address this issue and enhance practical applicability of BED, we provide a new approach\nSequential Experimental Design via Reinforcement Learning to construct BED in a sequential manner\nby applying reinforcement learning in this paper. Here, reinforcement learning is a branch of machine\nlearning in which an agent learns a policy to maximize its reward by interacting with the environment.\nThe characteristics of interacting with the environment are similar to the sequential experiment, and\nreinforcement learning is indeed a method that excels at sequential decision making.\nBy proposing a new real-world-oriented experimental environment, our approach aims to maximize\nthe EIG while keeping the cost of experiments and sample eﬃciency in mind simultaneously. We con-\nduct numerical experiments for three diﬀerent examples. It is conﬁrmed that our method outperforms\nthe existing methods in various indices such as the EIG and sampling eﬃciency, indicating that our\nproposed method and experimental environment can make a signiﬁcant contribution to application of\nBED to the real world.\nContents\n1\nIntroduction\n1\n2\nRelated works\n3\n3\nFormulation\n4\n3.1\nBayesian Experimental Design\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3.1.1\nExpected Information Gain . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3.1.2\nContrastive Information Bounds\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.1.3\nExtended Expected Information Gain\n. . . . . . . . . . . . . . . . . . . . . . .\n6\n3.2\nReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4\nExperimental Design via Reinforcement Learning\n9\n4.1\nBatch Design and Sequential Design . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.2\nMyopic and Holistic Views . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3\nCost of Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n4.4\nSequential Experimental Designs via Reinforcement Learning . . . . . . . . . . . . . .\n11\n5\nExperiment\n14\n5.1\nLocation ﬁnding\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n5.2\nSource inversion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n5.3\nDeath process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n6\nConclusion\n25\n1\nChapter 1\nIntroduction\nExperimentation is one of the essential elements of the scientiﬁc method. We collect data by conducting\nexperiments, and we discover scientiﬁc knowledge based on the data. Experiments are very practical\nfor science and engineering, but they are also often very costly. Regardless of the discipline such as\nphysics, psychology, and sociology, conducting experiments can be expensive in terms of both cost\nand time. For example, a large-scale physics experiment requires a lot of money to conduct a single\nexperiment. Also, drug trial involves the use of an uncertain drug, so there is a need to obtain a lot of\nknowledge while minimizing the cost in terms of side eﬀects to the subjects. As a result, we often want\nto discover as much as possible with as little cost. In other words, there are eﬃcient experiments and\nineﬃcient experiments, and we want to design and conduct eﬃcient experiments. The eﬃciency of\nsuch an experiment can be viewed from the perspective of how to minimize the cost of the experiment,\nand also from the perspective of how to maximize the information obtained within a limited cost. For\nexample, in human experiments, the number of data may be small due to the diﬃculty of collecting\nsubjects.In the case of a human experiment, for example, the number of data can be reduced due\nto the diﬃculty of collecting subjects. In a situation where data is diﬃcult to obtain, designing an\neﬃcient experiment is also an important aspect of the eﬃciency of the experiment.\nThe design of eﬃcient experiments has been studied in the ﬁeld of experimental designs [11, 31,\n7, 39, 48]. In the past, methods such as those using Fisher information matrix were proposed for\nconstructing good experimental designs; there are many alphabetic optimality criteria known in the\nliterature, see [36]. However, these methods are not necessarily eﬀective for non-linear models [9, 44].\nIn recent years, in order to design eﬃcient experiments for nonlinear models, much attention has also\nbeen focused on methods based on Bayesian inference. Using Bayesian inference, it is possible to\nupdate the probability distribution of model’s parameters conditional on the data gathered from an\nexperiment and to evaluate the goodness of the experiment by using the mutual information between\nthe model before and after the update, which measures how much the uncertainty of parameters\ncan be reduced on average by the experiment. The amount of the mutual information between the\npre-and post-experiment models is generally referred to as the expected information gain (EIG). The\nexperimental design maximizing the EIG is called (optimal) Bayesian experimental design (BED)\n[31, 7, 47, 46, 45, 53, 56]. BED has been mainly used in the batch form, in which all the individual\n1\nexperiments are determined in advance [24, 37]. However, it is more natural to think that eﬃcient\nexperimental conditions are not entirely known in advance but become apparent gradually.\nSequential design, in which experimental conditions are determined sequentially, is attracting\nattention to solve the problem of the batch design. The key diﬀerence from the batch design is that\nwe optimize a policy (instead of the design itself) on how to set the experimental condition depending\non the earlier conditions and the corresponding observations in an iterative manner. Sequential design\noften lead to experiments more eﬃciently than the batch design in environments where data is available\nsequentially [13, 25, 27]. However, the sequential design also has problems. The existing methods\nthat construct sequential designs may not be easy to apply to complex environments because the\nexperimental environment is often restricted to simple model settings [33, 6, 40, 30]. In addition,\nmost of the existing methods require to compute the EIG during each sequential step based on the\ndata observed so far. In general, the cost of such computation is very high [14, 18], which makes\nit challenging to apply in situations where real-time experiment planning is required. To solve this\nproblem, a method of planning experiments according to previously learned policies is proposed [13,\n25].\nThe conventional methods to construct sequential experimental designs are eﬃcient in maximiz-\ning the EIG, but may not be good at reducing the cost of experiments. For example, consider an\nexperiment to ﬁnd a sound source by sampling sound intensity pointwise while moving on the ground.\nIn such an experiment, the location of the sound source is expected to be most accurately identiﬁed\nby optimizing the sequence of sample points so that the EIG is maximized. To make the cost of\nexperiment small, on the other hand, the sequence of sample points should be designed in a way that\nthe distance traveled is minimized. Minimizing the cost is also important in the following aspects: it\ncan increase the number of experimental trials to obtain more accurate estimates, and can deepen the\nanalysis by conducting experiments in diﬀerent environments.\nIn addition, it is desirable to make the amount of data required to learn a policy for designing\nexperiments small [38, 57]. Any method requiring a large amount of training data cannot be applied\nto practical complex experimental environments.\nThis way, to apply experimental designs in real\nexperiments, it is important to consider the cost of the experiment and the eﬃciency of policy learning\nsimultaneously, as well as maximizing the EIG. In this study, by applying reinforcement learning\nand proposing a novel real-world-oriented experimental environment, we provide a new approach\nto construct sequential BED achieving these goals at the same time, which can enhance practical\napplicability of BED substantially.\n2\nChapter 2\nRelated works\nThe experimental designs have been studied in terms of how to allocate resources for experiments,\nand classical methods have been proposed to construct optimal designs using the Fisher information\nmatrix [41, 2]. Such methods are quite eﬀective for linear models but can only lead to local optimal\nsolutions for non-linear models [9, 44]. On the contrary, the BED represents the amount of information\nobtained from an experiment by using Bayesian updating. More precisely, the BED employs the EIG as\na quality criterion of the experimental design, which coincides with the (expected) mutual information\nbetween the prior and posterior probability distributions of model parameters [31, 7]. This method is\nalso eﬀective for non-linear models and has been applied to various problems [47, 6, 8, 46, 45, 53, 56].\nThe major technical diﬃculty of the BED is in that computing the EIG is not easy when Bayesian\nupdates of probability distributions cannot be given in a closed-form. To approximate the EIG or\nthe poster distribution, one can use Markov chain Monte Carlo method [1], importance sampling\nincluding Laplace approximation [32, 3, 4], variational inference [14] and multilevel Monte Carlo\nmethod [16, 18, 4]. However, the cost of performing these computations is still not suﬃciently low for\npractical applications.\nThe experimental designs can be categorized into batch designs and sequential designs. The batch\ndesigns are eﬀective when the experimental data are obtained in a batch manner [24, 37]. However,\nin many experiments, the observed data is obtained sequentially and the sequential designs naturally\nﬁt better than the batch designs [25, 27, 13]. However, the existing works on sequential designs have\nbeen hampered by the myopic nature of the policy [33], the low degree of freedom in policy [43],\nand the computational cost of the policy [25]. Moreover, the existing construction methods require\nto compute the EIG in every sequential step [26, 27], which makes it hard to apply for real-time\nsequential design. Recently a sophisticated method has been proposed to enable real-time decision\nmaking by learning the policy in advance for designing experiments [13]. Our current work builds\non it, and applies the framework of reinforcement learning to deal with the cost of experiment and\nsample eﬃciency simultaneously.\n3\nChapter 3\nFormulation\nIn this section, we give overviews on the BED and reinforcement learning, respectively.\n3.1\nBayesian Experimental Design\nRecently the BED has been studied quite intensively as a way of experimental designs to tackle with\nnon-linear complex models. In what follows, we ﬁrst formulate the EIG as a quality criterion of the\nBED, and then give its contrastive bounds. Moreover, we extend the EIG to the sequential data\ngenerating process.\n3.1.1\nExpected Information Gain\nThe goal of the BED is to ﬁnd the optimal design (for the batch case) or policy (for the sequential\ncase) π = π∗that minimizes the information entropy of the latent variable θ in a model by conducting\nexperiments based on the condition π. This corresponds to maximizing the EIG whose formulation\nfor the batch design is given as follows.\nBefore conducting the experiment, the information entropy of the latent variable, H[p(θ)], is given\nby\nH[p(θ)] = −\nZ\nθ\np(θ) log p(θ)dθ\n(3.1)\n= −Eθ[log p(θ)]],\n(3.2)\nwhere p(θ) denotes the prior probability distribution of the latent variable θ. The following Bayes\nformula expresses that the distribution of θ is updated by conducting an experiment based on the\ndesign π and obtaining the observation y:\np(θ|π, y) = p(y|θ, π)p(θ)\np(y|π)\n.\n(3.3)\nHere, p(y|θ, π) describes a generative model of the observable y given θ and π. It follows from (3.3)\n4\nthat the information entropy of the posterior distribution, H [p(θ|π, y)], is given by\nH[p(θ|π, y)] =\nZ\nθ\np(θ|π, y) log p(θ|π, y)dθ\n=\nZ\nθ\np(θ|π, y) log p(y|θ, π)p(θ)\np(y|π)\ndθ\n= −Ep(θ|π,y)\n\u0014\nlog p(y|θ, π)p(θ)\np(y|π)\n\u0015\n.\n(3.4)\nThe diﬀerence between (3.1) and the expectation of (3.4) with respect to y is regarded as the\namount of information gained by the BED π, and is nothing but the EIG:\nU(π) := Ep(y|π) [H[p(θ)] −H[p(θ|π, y)]]\n= −\nZ\ny\nZ\nθ\np(y|π)p(θ) log p(θ)dθdy +\nZ\ny\nZ\nθ\np(y|π)p(θ|π, y) log p(y|θ, π)p(θ)\np(y|π)\ndθdy\n= −\nZ\ny\np(y|π)dy\nZ\nθ\np(θ) log p(θ)dθ +\nZ\ny\nZ\nθ\np(y|π)p(y|θ, π)p(θ)\np(y|π)\nlog p(y|θ, π)p(θ)\np(y|π)\ndθdy\n= −\nZ\nθ\np(θ) log p(θ)dθ +\nZ\ny\nZ\nθ\np(y|θ, π)p(θ) log p(y|θ, π)p(θ)\np(y|π)\ndθdy\n= −\nZ\ny\nZ\nθ\np(y|θ, π)p(θ) log p(θ)dθdy +\nZ\ny\nZ\nθ\np(y|θ, π)p(θ) log p(y|θ, π)p(θ)\np(y|π)\ndθdy\n= Ep(y|θ,π)Ep(θ)\n\u0014\n−log p(θ) + log p(y|θ, π)p(θ)\np(y|π)\n\u0015\n= Ep(y|θ,π)Ep(θ) [log p(y|θ, π) −log p(y|π)] .\n(3.5)\n3.1.2\nContrastive Information Bounds\nAs already stated, the goal of the BED is to ﬁnd a design π = π∗that maximizes the EIG U(π).\nHowever, calculating (3.5) is generally challenging. This is because, in order to evaluate p(y|π) exactly,\nit is necessary to perform the integral calculation\nR\nθ p(y|π, θ)p(θ)dθ. These integrals are in general\ndiﬃcult to compute analytically. In such cases, it is necessary to obtain the EIG by approximation. In\norder to obtain an unbiased estimate of the EIG, it is necessary to take the nested expectation, as is\nclear from (3.5). And ﬁnding the nested expectation value is generally known to be computationally\nexpensive.\nFor example, if we want the root mean square error to be ϵ for the EIG using the Monte Carlo\nmethod, we need O(ϵ−3)[42, 3, 46]. Methods using multilevel Monte Carlo[16, 17, 15, 3, 18, 42, 4] or\na combination of Laplace approximation and importance sampling[32] have been proposed to perform\nsuch calculations eﬃciently. However, even using such a method, the computational complexity of\nO(ϵ−2(log ϵ−1)2) or O(ϵ−2(log ϵ−1)2) is required to compute the nested expectation calculation. The\nmethod to approximate the EIG using variational inference has also been proposed[14]. However, this\nmethod is also computationally expensive because it requires optimization for approximation.\nIn general, ﬁnding the optimal design π∗requires iterative optimization. The large computational\ncost of the pointwise EIG evaluation means that the overall computational cost of optimization is\nenormous [60, 42]. To address this issue, an unbiased estimator for the gradient ∇θU(θ) has been\n5\nintroduced and combined with stochastic gradient-based optimization algorithms to search for π∗\neﬃciently in [19]. However, the research outcomes of this direction are still immature, so that we\nemploy a computationally-cheap, lower bound on the EIG as a quality criterion in this paper.\nTo be precise, we use contrastive information bounds (CID), which have proven quite eﬀective in a\nrecent work [13]. Here CID can be computed as follows. First, we sample the latent variable θ0 ∼p(θ)\nand generate the observation y by following the generating process determined by p(y | θ0, π). Then,\nwe generate L independent contrastive samples θ1:L ∼p(θ). By using the initially sampled θ0, the\nobservation y, and the L contrastive samples θ1:L, CID is given by\nIL = log\np(y|θ0, π)\n1\nL+1\nPL\nl=0 p(y|θl, π)\n.\n(3.6)\nThe expected value of CID, L, is deﬁned by\nL(π, L) = Ep(θ0:L)Ep(y|θ0,π)\n\"\nlog\np(y|θ0, π)\n1\nL+1\nPL\nl=0 p(y|θl, π)\n#\n.\n(3.7)\n3.1.3\nExtended Expected Information Gain\nSo far, we have formulated the EIG for the batch design π. We can easily modify the formulation\nfor the sequential design in which π denotes a policy and the observation y is generated sequentially.\nFirst, let us rewrite the EIG given in (3.5) into the plural form:\nU(π) = Ep(y0:t|π)Ep(θ|y0:t,π) [log p(y0:t|θ, π) −log p(y0:t|π)]\n(3.8)\nwhere y0:t represents a set of the observations given discretely from time 0 to time t.\nSimilarly, the CID can be modiﬁed into the plural form. As before, we ﬁrst sample the latent\nvariable θ0 ∼p(θ). Then, according to the policy π, we obtain the initial experimental design ξ0\nand generate the corresponding observation y0.\nDepending on this result, we obtain the second\nexperimental design ξ1 and generate the corresponding observation y1. Repeating this procedure, we\ncan get a sequence of the experimental designs ξ0:t and the observations y0:t. Now the CID for the\nsequential case is given by\nIL = log\np(y0:t|θ0, ξ0:t)\n1\nL+1\nPL\nl=0 p(y0:t|θl, ξ0:t)\n.\n(3.9)\nNote that p(y0:t|θ, ξ0:t) in (3.9) is conditionally independent if the observable yt depends only on the\ncorresponding design ξt. In such cases, p(y0:t|θ, ξ0:t) can be expressed as\np(y0:t|θ, ξ0:t) =\ntY\ni=0\np(yi|θ, ξi).\n(3.10)\n3.2\nReinforcement Learning\nReinforcement learning is a research ﬁeld that aims to ﬁnd an optimal decision rule (policy) for\nsequential decision making. It is also a branch of machine learning characterized by the notion of\n6\nreward r, where decision rules are learned to maximize the expected value of the reward. The two\nmost essential elements in reinforcement learning are the agent and the environment. The agent tries\nto maximize the reward given to itself by interacting with the environment. The agent is ﬁrst given\nan initial state s0 ∈S by the environment to interact with. Given a state from the environment, the\nagent outputs an action at ∈A based on its own policy at each time t according to at ∼π(a|st).\nThe environment then returns the next state st+1 and the reward rt ∈R to the agent based on the\ntransition function P(st+1, rt|st, at). Through these interactions, the agent learns the policy π∗that\nmaximizes the expected reward V π(s) deﬁned by\nV π(s) = E\n\" n\nX\nk=0\nγkr(st+k, at+k) | st = s\n#\n,\n(3.11)\nwhere γ represents the decay rate. (3.11) is called the value function.\nIt is also possible to deﬁne Q-value function by adding the action at at time t to the conditions of\nthe value function as follows:\nQπ(s, a) = E\n\" n\nX\nk=0\nγkr(st, at) | st = s, at = a\n#\n.\n(3.12)\nHere the evaluation for a state/action pair requires n + 1 steps of information. The number of states\nthat need to be considered to ﬁnd the optimal policy grows exponentially with n. Performing such\ncalculation is impractical. In reinforcement learning, the famous Bellman equation is used to compute\n(3.12) eﬃciently. Using the Bellman equation, (3.12) can be expressed as follows.\nQπ(st, at) = r(st, at) + γE [V π(st+1)] .\n(3.13)\nBy using the Bellman equation, the information required to compute the Q-value is signiﬁcantly\nreduced, that is, st, at and st+1.\nReinforcement learning can be divided into two main approaches in how it learns the policy: value-\nbased approach and policy-based approach. In the value-based approach, the agent approximates the\nQ-value in order to take the optimal action, and takes action based on this function. For example,\nthe greedy agent determines the action as at = argmaxa∈AQπ(st, a). A typical example of the value-\nbased approach is DQN (Deep Q Network) [35, 55, 49, 58, 12, 5, 23], which has been applied to\nvarious environments such as video games with high results. The value-based approach is known to\nbe eﬀective when the action space A takes discrete values. However, its performance is known to be\ndegraded when the action space A takes continuous values, such as in robot control [10].\nOn the other hand, the agent learns its policy directly in the policy-based approach. A typical\nexample of this approach is REINFORCE [59], which trains a policy network by approximating the\nQ function by taking the expected values of rewards obtained from multiple episodes.\nAlthough\nREINFORCE is a straightforward idea to learn a policy network using actual observed rewards, the\ntraining becomes unstable due to the variability in the rewards used to train the policy network. In\norder to solve this problem, the Actor-Critic algorithm [29, 1] uses two networks, the critic-network\nto evaluate the policy and the actor-network to decide the agent’s action. This way the instability of\n7\npolicy evaluation occurring in REINFORCE is mitigated, and the learning process can be stabilized.\nFurthermore, reinforcement learning has a problem in the learning process that it becomes diﬃcult to\nimprove the reward once the policy is degraded. Some methods have been proposed to stabilize the\nlearning process by limiting the amount of parameter updates during the update process [52, 50].\nIn the policy-based approach, a policy gradient is often used to compute the gradient g for the\npolicy network as follows [51]:\ng = E\n\" ∞\nX\nt=0\nΦ(st, at)∇φ log πφ(at|st)\n#\n,\n(3.14)\nwhere φ represents the parameters of the policy network, and Φ(st, at) represents the value of the\nstate at time t. We can use various functions for Φ(st, at), but the advantage function is often used to\nreduce the variance. As shown below, we can deﬁne the advantage function using the value function\nand the Q-value function\nAπ(st, at) = Qπ(st, at) −V π(st).\n(3.15)\nWhen training the agent network, many policy gradient algorithms use the method called on-\npolicy learning, where the policy for determining the agent’s action and the policy for updating the\nnetwork need to be the same [34, 52, 50]. When using such a method, once the network is trained,\nthe data used for training cannot be reused. In reinforcement learning, sample eﬃciency refers to how\neﬃciently we can use samples in such training. The sample eﬃciency of on-policy learning is poor\nbecause the samples used for training needs to be discarded after each training step. In recent years,\noﬀ-policy learning has been proposed for policy gradients, but these methods are not practical because\nthe learning results strongly depend on the hyperparameters [22]. One of the reasons why the learning\nresults depend on hyperparameters is the low search capability. Due to it, the agent converges to\na local optimum solution that depends on the hyperparameters. Low search capability also leads to\npoor performance in high-dimensional environment.\nThe Soft Actor-Critic (SAC) method was proposed as a solution to these problems [21]. To improve\nthe search capability, the entropy term of the policy is added to the objective function in SAC, as\ndescribed in detail later.\nIn this paper, we use reinforcement learning as a method for sequential experimental design because\nit is desirable to apply experimental designs to more complex and large-scale experimental environ-\nments where the action space takes continuous values. In addition, it is necessary to assume that the\ncomputational cost of simulating the experiment is exceptionally high. We propose an experimental\ndesign based on SAC as a method that satisﬁes these requirements.\n8\nChapter 4\nExperimental Design via\nReinforcement Learning\nIn this section, we explain some concepts to explain the problems of the existing approaches. Then,\nour proposed method Sequential Experimental Design via Reinforcement Learning is described.\n4.1\nBatch Design and Sequential Design\nWe have described the batch design as a method of determining all the experimental designs in advance,\nwhereas the sequential design as a method of planning an experiment sequentially by a policy based\non the data obtained from the earlier experiments. Therefore, the diﬀerences of batch and sequential\ndesigns can be found in the conditions given to π. Now, consider the case where T experiments are\nconducted between time 0 and time T −1. In such a case, in batch design, π coincides with a set of\nthe individual designs ξ0:T−1 at the times t = 0, 1, . . . , T −1. In contrast, in sequential design, the\ndesign ξt at each time t is sampled according to the policy π by\nξt ∼π(ξ|θ, s0:t−1),\n(4.1)\nwhere st denotes the pair (ξt, yt) of the design and the observation at each time t. Since the experiment\nat t is designed based on the state s0:t−1 in sequential design, it is more ﬂexible than batch design.\n4.2\nMyopic and Holistic Views\nAlthough sequential design is often more eﬀective than batch design, it is challenging to make holis-\ntic decisions in sequential design. In other words, at each time t, if the design of the experiment ξt\nprioritizes the reward rt, i.e., the EIG, at that time only, the reward obtained in the whole exper-\niments might decrease. Therefore, we need to formulate the diﬀerence between myopic and holistic\nexperimental designs by using the EIG.\nFirst, a myopic experimental design can be thought of as a policy that maximizes only the diﬀerence\nbetween the information entropy of the current θ and that of θ when a new state st is obtained.\n9\nFormally, the myopic experimental design is equal to a policy that maximizes the following objective\nfunction Ut at each time t.\nUt = H [P(θ|s0:t−1)] −H [P(θ|s0:t)]\n(4.2)\nAn experimental design that only considers the amount of information we can obtain during one step\ncan be regarded as a myopic design. Designs that are optimized by an objective function such as\n(4.2) are likely to converge to a locally optimal solution. In fact, the previous work shows that the\ndesigns trained by myopic rewards, such as the above, perform worse than the designs trained by\nholistic rewards [25]. In addition, when considering rewards such as (4.2), Bayesian updating must be\nperformed at each time t, which is generally very computationally expensive.\nTo solve the problems of greediness in experimental design and reward calculation, the preceding\nstudy showed that the amount of information obtained between each step of an experiment sums up to\nthe amount of information obtained in the entire experiment, and further formulated the information\nobtained in the whole experiment [13]. This means that there is the following relationship between\nthe sum of information obtained between each step of the experiment and the amount of information\nobtained in the entire experiment:\nE\n\"T−1\nX\nt=0\nUt\n#\n= H [P(θ)] −H [P(θ|s0:T−1)] .\n(4.3)\nIn addition, by developing the information obtained in the whole experiment in a diﬀerentiable form,\na novel algorithm called deep adaptive design (DAD) has been proposed [13].\nBy using a contrastive lower bound on the amount of information obtained in the entire experiment\nas given in (3.7), the training of DAD is performed by the reparametrized gradient\ndLT (πφ, L)\ndφ\n= Ep(θ0:L)p(ϵ0:T −1)\n\u0014dIL (θ0:L, s0:T−1)\ndφ\n\u0015\n(4.4)\nwhere ϵ0:T−1 is the observation error in the observation of the experiment.\n4.3\nCost of Experiment\nThe equation (4.4) allows us to formulate a sequential experimental design from a holistic perspective.\nSince the objective function (4.4) is permutation-invariant with respect to the state s0:T−1, DAD ex-\nploits this property for eﬃcient and eﬀective training [13]. For example, suppose that the experimental\ndesigns ξs, ξt, and ξu are performed at times s, t, and u, respectively. The value of the objective (4.4)\ndoes not change even if the order in which the experiment is performed is not the same as s, t, and u.\nIn order to take the cost of experiments into account more carefully, however, we do not exploit\nthis permutation-invariance property of the objective deliberately in this study. For example, when\ncollecting three experimental data by moving a sampling point on the ground, it is desirable to sample\nthe data in order of distance from the starting point to reduce the total cost of experiment. This\naspect is not respected if only a permutation-invariant objective is used.\n10\nTo consider the sequential nature of the design of experiment, it is desirable to express the reward\nof the experiment in the form of dynamic programming as formulated in [25]. By setting up such an\nobjective function, we can acquire a holistic perspective and, at the same time, consider the order\nin which the experiments are conducted. However, in the previous work [25], the state space S is\ndelimited into a grid to calculate the objective function based on dynamic programming, and also the\nobjective function is calculated using the form of Riemann integration. Such a method should be only\nuseful when the state space S is low-dimensional or takes discrete space. To solve these problems,\nmethods to approximate the objective function using neural networks or other methods have been\nused in reinforcement learning, and this paper also follows this approach.\n4.4\nSequential Experimental Designs via Reinforcement Learning\nIn reinforcement learning, an agent learns a reward-maximizing policy π by interacting with its en-\nvironment. We design experiments by interacting with the experimental environment. The charac-\nteristics of this type of experimental design are very similar to the agent-environment relationship in\nreinforcement learning. Based on the above features, we consider the experiment as an environment\nin reinforcement learning.\nTo treat the experimental design as the environment in reinforcement learning, we need to introduce\nseveral deﬁnitions as below. First, the action at in the experimental design is thought of as the amount\nof change between consecutive designs ξt−1 and ξt. In this setting, the current design ξt is given by\nξt = ξt−1 + at.\n(4.5)\nFor example, if we take samples by moving an observation point in space, the action corresponds to\nthe amount of movement from the current point, and the action space A can be given by (a subspace\nof) the three-dimensional Euclidean space. Then, the state s in the experimental design corresponds\nto the pair of the design and the observation result. So, the current state st is given by st = (ξt, yt),\nand the observed value y is by y = p(y|ξ, θ).\nFinally, we deﬁne the reward function. In this work, the amount of information obtained by an\nexperiment is measured by using CID (3.9) from the perspective of computational cost. Nevertheless,\ncomputing CID during each time step is computationally expensive in learning process. To reduce\nsuch costs, we calculate the CID only at the end of the experiment, i.e., at the end of an episode.\nTherefore, the reward is computed as follows:\nr(s0:T , a0:T ) =\n\n\n\n0\nif episode is not ﬁnished,\nIL(s0:T , a0:T )\nif episode is ﬁnished.\n(4.6)\nNote that calculating the CID only at the end of the experiment and using it as the objective function\nis the same as in DAD. To design a sequential experiment that simultaneously takes both the EIG\nand the cost of the experiment, it is necessary to set up a sequential objective function. We do this\nby applying reinforcement learning in this study.\n11\nAdditionally, this paper aims to provide eﬀective real-time experiment plannings for complex and\nlarger-scale problems. To achieve this goal, SAC is considered to be eﬀective because of its high sample\neﬃciency and high performance, even for continuous-valued action spaces. SAC can reduce the impact\nof hyperparameters on learning by increasing the search capability. The basic idea to enhance the\nsearch capability is based on soft Q-learning [20], which adds an entropy term to the usual Q-value\nfunction (3.12). The objective function in soft Q-learning is given by\nQπ(s, a) = E\n\"\nrt +\nT\nX\nk=1\nγk(rt+k + αH(π(·|st+k)) | st = s, at = a, π\n#\n,\n(4.7)\nwhere H(π(·|st+k)) denotes the entropy of a policy. Maximizing the entropy of a policy means that\nit is more likely to take a diﬀerent action at given the same state st.\nSoft Q-learning maximizes\nthe reward and the entropy at the same time, as in (4.7), which makes it possible to take a balance\nbetween exploration and exploitation. In addition, the parameter α in (4.7) represents the entropy\ntemperature, which adjusts the balance between search and exploitation during the training. The\nBellman equation can be adapted to (4.7) as well as in standard Q-learning.\nQπ(st, at) = r(st, at) + γE [V (st)] ,\n(4.8)\nV (st) = Eat∼π [Q(st, at) −α log π(at, st)] .\n(4.9)\nAs you can see, the objective function in soft Q-learning can be expressed by adding an entropy term\nto the usual value function.\nSAC is a method that applies the soft Q-learning to the actor-critic algorithm. As in the usual\nactor-critic algorithm, SAC is composed of two networks: the actor-network and the critic-network.\nThe critic-network is trained by using the squared residual error\nJQ(ψ) = E\n\u00141\n2\n\u0000Qψ (st, at) −\n\u0000r (st, at) + γEst+1∼p\n\u0002\nV ¯ψ (st+1)\n\u0003\u0001\u00012\n\u0015\n,\n(4.10)\nwhere ψ denotes the set of the parameters of the critic-network and ¯ψ denotes that of the target\nnetwork. The target network is a network that is used only when training the critic-network. It is\nknown that the use of the target network stabilizes the training process [55].\nThe actor-network is trained by minimizing the KL-divergence with the softmax of (4.7). There-\nfore, the objective function of policyπ can be expressed as follows:\nJψ(φ) = Eat∼π\n\"\nDKL\n \nπφ\n\u0000·|St\n\u0001\n∥exp\n\u0000 1\nαQψ (st, ·)\n\u0001\nZψ (st)\n!#\n,\n(4.11)\nwhere φ denotes the set of the parameters of the actor-network and Zψ (st) denotes the normalization\nconstant of the softmax of (4.7). Here, Zψ (st) can be ignored since it is independent of φ. Therefore,\nthe objective function of the actor-network can be expressed more simply by\nJψ(φ) = Eat∼π [log πφ (at | st) −Qψ (st, at)] .\n(4.12)\nThe whole process of our algorithm is described in Algorithm 1.\n12\nAlgorithm 1 Sequential Experimental Design via Reinforcement Learning\nInitialize policy and value network parameters φ, ψ\nInitialize target network parameters ¯ψ\nSet learning rates λQ, λπ and weight τ\nfor iteration = 1,2,... do\nsample latent variable θ0\nwhile episode is ﬁnished do\nSelect action at ∼π(at|st) and receive reward rt and next statest+1\nStore transition(st, at, rt, st+1) in R\nsample (st, at, st+1, rt) from replay buﬀerR\nψ ←ψ −λQ∇ψJV (ψ)\nφ ←φ −λπ∇φJV (φ)\n¯ψ ←τψ + (1 −τ) ¯ψ)\nend while\nend for\n13\nChapter 5\nExperiment\nThe main goal of this paper is to bring experimental designs closer to real-world applications. In order\nto achieve this goal, it is desirable to construct experimental designs not only sequentially but also in\nreal-time. In the earlier works, many sequential construction algorithms have been proposed [26, 27],\nbut most of them require a large computational cost to make designs and are not necessarily suitable\nfor real-time decision making.\nDAD proposed in [13] is a good algorithm for policy optimization\nbecause it is built upon an computationally-cheaper objective function.\nIn addition, DAD is the\nbest performing algorithm for real-world oriented experimental environments.\nTherefore, DAD is\nan appropriate algorithm to compare with the performance of our proposed algorithm Sequential\nExperimental Design via Reinforcement Learning (RL-SED, in short).\nWe also compare with the\nperformance of a randomly chosen policy as a baseline.\nIn the following experiments, we consider three diﬀerent problem settings. Each problem setting,\ni.e., each “experimental environment” in the terminology of reinforcement learning, has been originally\nused by the previous works [13, 25, 54], respectively. In order to consider the realism of the experi-\nment and the cost of the experiment, we have made several changes from the original experimental\nenvironments in this study. In the previous works, there were signiﬁcant changes found in the con-\nsecutive designs, i.e., at in the update (4.5) is quite large in magnitude, which leads to an unrealistic\nsequential design in practical settings. Even more, some did not consider the sequential nature of the\nexperiments.\nIn what follows, we describe the problem setting, focusing on the diﬀerences from the original\nexperimental environments, and present the results for each of the three test cases, respectively. In\nevery problem setting, we use CID (3.9) as a quality criterion (reward) of experimental designs, and\nthe number of episodes required for the learning until a suﬃcient convergent as a sample eﬃciency.\nIn each of the three experimental environments, the DAD and RL-SED agents use the state s0:t−1\nto determine the action at\nDetailed information about the experimental environment and additional results should be written\nin the supplementary material.\n14\n5.1\nLocation ﬁnding\nIn this example, we construct an environment based on the experimental environment called “Location\nﬁnding” [13]. This is a problem to infer the locations of multiple unknown sound sources in space by\nsampling the intensity of the sound at a moving point. Our task here is to optimize a policy on the\ntrajectory of the moving point. We assume an environment where three sound sources are randomly\nplaced in location ﬁnding. The location of each source θi is sampled by θ ∼N(0, σ2\n1), where N(0, σ2\n1)\ndenotes a Gaussian distribution with mean 0 and variance σ2\n1.\nIn this environment, the agent outputs distance traveled from the current location as its action at.\nAfter updating the design ξt according to the action, the agent observes the intensity of the sound.\nThe sound intensity µt at the observation point ξt is given by\nµt = b +\n3\nX\nk=1\nα\nm+ ∥θk −ξt ∥2 .\n(5.1)\nThe sound observation contains noise. Therefore, the observed value yt can be obtained according to\nthe following probability distribution.\nyt ∼N(µt, σ2\n2)\n(5.2)\nIn order to consider the cost of the experiment, we restrict the distance that the agent (i.e., the\nmoving point) can travel during a time step to d1. Furthermore, we also set a limit on the distance\nthat the agent can travel in one episode to d2. Thus, an episode ends when one of two conditions is\nmet: either the max episode steps T is reached, or the total travel distance exceeds d2.\nIn order to consider the cost of the experiment, we restrict the distance that the agent (i.e., the\nmoving point) can travel during one time step to d1.\nThis reduces the amount of change in the\nexperimental designs between consecutive steps and makes the cost of the experiment not too large.\nWe also set a limit on the distance that the agent can travel in one episode to d2. Thus, an episode\nends when one of two conditions is met: either the max episode steps T is reached, or the total travel\ndistance exceeds d2. By putting these restrictions, we can consider two eﬃciencies simultaneously:\nthe amount of the EIG from the experiment and the cost of experiment. These constraints can also\nbe regarded as helpful in considering applications to real-world experimental environments.\nIn this experiment, we used Contrastive information bounds (CID) as the agent’s reward, which\nis computed as follows:\nIL = log\np(y0:t|θ0, ξ0:t)\n1\nL+1\nPL\nl=0 p(y0:t|θl, ξ0:t)\n.\n(5.3)\nThe detailed parameters of the experimental environment are shown in Table 5.1.\nFigure 5.1 shows the learning curves of random policy, DAD and RL-SED (ours).\nTable 5.2\nshows the average reward at the end of the episode and also the number of episodes required to\nachieve a suﬃcient convergence of the training. From Figure 5.1 and Table 5.2, we can see that our\nalgorithm outperforms DAD on the average reward. In addition, our algorithm successfully completes\n15\nTable 5.1: Parameters in Location ﬁnding problem\nParameter\nValue\nStandard deviation of sound source, σ1\n40\nα\n1\nBase signal, b\n0.3\nMax signal, m\n10−4\nStandard deviation of noise, σ2\n0.5\nMax distance for each step, d1\n3\nMax distance for each episode, d2\n50\nMax episode steps, T\n100\nInner sample size of CID, L\n2000\nTable 5.2: Mean of ﬁnal reward and total episode for training\nenvironment\nalgorithm\nreward\ntotal episode\nLocation ﬁnding\nrandom\n3.278\n-\nDAD\n4.164\n750000\nRL-SED\n5.955\n15000\nSource inversion\nrandom\n3.586\n-\nDAD\n5.051\n40000\nRL-SED\n5.510\n3000\nDeath process\nrandom\n1.630\n-\nDAD\n1.506\n1000000\nRL-SED\n2.042\n15000\nthe learning process within a quite fewer number of episodes. This is partially because our algorithm\nis based on oﬀ-policy learning, whereas DAD is based on on-policy learning.\nFigures 5.2 and 5.3 show two examples of trajectories obtained by DAD and RL-SED, respectively.\nWe can conﬁrm from the ﬁgures how each algorithm samples in the two environments with diﬀerent\nsource locations. As can be seen in Figure 5.2, DAD samples in a vortex-like fashion. In addition,\nthe two trajectories have a similar shape, even though the positions of the sources are quite diﬀerent.\nOn the other hand, by comparing the two trajectories in Figure 5.3, we can see that both trajectories\nmove in a similar way only at the beginning of the experiment. As the experiment progresses, we can\nsee that they take diﬀerent routes, as if they are trying to ﬁnd the sound sources adaptively. This\ndiﬀerence in the trajectories between DAD and RL-SED implies that RL-SED is able to capture the\nsequential nature of the experiment more accurately than DAD. As a result, RL-SED is able to adapt\naccurately even in situations where the locations of the sound sources change signiﬁcantly, and to\nobtain higher rewards for identifying the locations of the sound sources.\n16\nFigure 5.1: Learning curves for the location ﬁnding problem\nWe refer to this ability of the agent to respond to individual situations as the generalization\nperformance. In order to quantitatively analyze the generalization performance, we consider diﬀerent\nenvironments with diﬀerent distributions p(θ) of the source locations, and conduct experiments. In\nthe original environment for training agents, each of the three sources is independently sampled from\na Gaussian distribution with mean 0 and standard deviation 40. In the additional experiments, we\nconsider three environments with diﬀerent standard deviations of 20, 40 (original), and 60 for the\nGaussian distribution, and conduct experiments in each of them. The agents used in the experiments\nare the ones that achieved the highest performance during the training.\nTable 5.3 shows the results of the additional experiments.\nAs we can see, DAD has the best\nperformance in the original environment, whereas RL-SED performs best when the standard deviation\nis 20 and the performance deteriorates as the standard deviation increases. As the results of the random\npolicy show, in environments where the standard deviation is small, the distance between the sources\nis closer, which is expected to reduce the diﬃculty of the experiment. Due to the high generalization\nperformance of RL-SED, it is able to achieve high performance even in environments diﬀerent from\nthose in which it was trained. On the other hand, the performance of DAD deteriorates in spite of\nthe experiments in simple environments, which indicates a lower generalization performance of DAD.\nWhen the standard deviation is 60, the performance of each algorithm deteriorates, but the ratio of\nreduction from the original environment is smaller for RL-SED. These results suggest that RL-SED\nhas a higher generalization performance than DAD.\nFrom the quantitative results of the experiments as well as the visualization of the experimental\ndesigns, we can conclude that our algorithm works quite well in terms of both performance and sample\neﬃciency and achieves a higher ability for sequential decision making than the existing methods.\n17\nExperiment order\nFirst\nLast\nSound intensity\nLow\nHigh\nFigure 5.2: Sample trajectories of DAD\nExperiment order\nFirst\nLast\nSound intensity\nLow\nHigh\nFigure 5.3: Sample trajectories of RL-SED\n18\nTable 5.3: Result for diﬀerent environments\nalgorithm\nenvironment\nreward\nRatio to the original\nrandom\nstd=20\n4.797\n1.15\nstd=40\n4.164\n1.0\nstd=60\n3.499\n0.840\nDAD\nstd=20\n5.503\n0.960\nstd=40\n5.731\n1.0\nstd=60\n5.133\n0.900\nRL-SED\nstd=20\n6.638\n1.040\nstd=40\n6.385\n1.0\nstd=60\n6.065\n0.950\n5.2\nSource inversion\nWe take an example called “Contaminant source inversion problem” originally used in [25]. In this\nexperimental environment, we assume a situation in which chemical substances gradually diﬀuse in\nthe air. During the experiment, the wind is blowing in space, and the chemicals move through space,\ndiﬀusing as if swept by the wind. The location of chemical source θ and the strength of wind w are\nsampled by following probability distributions.\nθ ∼N(0, σ2\n1)\n(5.4)\nw ∼N(0, σ2\n2)\n(5.5)\nIn this situation, the goal of the experimental design is to identify the source of the chemicals.\nSimilarly to the location-ﬁnding problem, we consider measuring the concentration yt of the chemical\nat a moving point and our task is to optimize a policy on the trajectory of the moving point. The\nconcentration of chemicals µt at observation point ξt is given by\nµt\n=\ns\n√\n2π(√1.2 + 4Dtd) exp\n\u0012\n−∥θ + dw(td) −ξt ∥2\n2(1.2 + 4Dtd)\n\u0013\n(5.6)\ndw(td)\n=\n10w(td −1)\n(5.7)\nwhere td, dw(td) are the total travel distance, and cumulative net displacement due to wind up to time\nstep t, respectively. The total travel distance is the sum of the distances traveled by time t. In existing\nstudies, the argument of dw is simply t. However, to consider the experiment’s sequential nature, we\nhave adopted the travel distance td as the argument of dw. Such a change implicitly imposes a cost\non the agent for making a large move since this modiﬁcation creates a situation where the material\nmoves and diﬀuses more according to the amount of the move.\nThe observation contains noise. Therefore, the observed value yt can be obtained according to the\nfollowing probability distribution.\n19\nTable 5.4: Parameters in Source inversion problem\nParameter\nValue\nStandard deviation of chemical source, σ1\n10\nStandard deviation of wind, σ2\n0.1\nConcentration strength, s\n30\nDiﬀusion coeﬃcient, D\n0.1\nStandard deviation of noise, σ3\n0.5\nMax distance for each step, d1\n1\nMax distance for each episode, d2\n50\nMax episode steps, T\n100\nInner sample size of CID, L\n2000\nyt ∼N(µt, σ2\n3)\n(5.8)\nIn this source-inversion problem, the distance that the agent can travel during one observation step\nis restricted to d1, as in location ﬁnding, so as to consider the cost of the experiment. We also set a limit\non the distance that the agent can travel in one episode to d2. In addition, although the direction and\nstrength of the wind were ﬁxed in the original experimental environment, they are generated randomly\nin our experimental environment as (5.5). Before this modiﬁcation, since the direction and speed of\nthe material moving are constant, it is possible to identify the source by moving the measurement\npoint only in a speciﬁc direction. However, when both the strength and direction of the wind are\nrandom, the importance of sequential decision making increases. This is because if the wind strength\nvaries, the concentration distribution of the material can be made broader as the material diﬀuses over\ntime. Because of these characteristics, we consider the above modiﬁcations eﬀective for more realistic\nsequential decision making problems.\nIn this experiment, we used Contrastive information bounds (CID) as the agent’s reward.\nThe detailed parameters of the experimental environment are shown in Table 5.4.\nFigure 5.4 shows the learning curves of random policy, DAD and RL-SED (ours). Table 5.2 shows\nthe mean reward of each algorithm at the end of the training and the number of episodes used for\ntraining. From Table 5.2, we can see that our algorithm outperforms DAD on average. In addition,\nour algorithm requires fewer episodes for learning than DAD, indicating that the learning process is\nmore eﬃcient.\nWe conducted additional experiments to evaluate the generalization performance of each model.\nIn this experiment, we prepared three environments with diﬀerent standard deviations of the Gaussian\ndistribution to compare the agents’ performance in environments with diﬀerent distributions of the\nlocation of the chemicals. We use the agents who achieved the highest performance during the training.\nThe results for each environment are shown in the table 5.5.\n20\nFigure 5.4: Learning curves for the source inversion problem\nTable 5.5: Result for diﬀerent environments in source inversion problem\nalgorithm\nenvironment\nreward\nRatio to the original\nrandom\nσ1=5\n4.716\n1.31\nσ1=10\n3.586\n1.0\nσ1=15\n3.465\n0.966\nDAD\nσ1=5\n5.232\n1.01\nσ1=10\n5.190\n1.0\nσ1=15\n4.948\n0.953\nRL-SED\nσ1=5\n5.971\n1.05\nσ1=10\n5.716\n1.0\nσ1=15\n5.402\n0.945\n21\nAs the random policy results show, the experiment’s diﬃculty increases as σ1 increases. The DAD\nand RL-SED agents performed best in the σ1 = 5 environment and performed worst in the σ1 = 15\nenvironment. The increase in performance at σ1 = 5 was higher for RL-SED, and the decrease in\nperformance at σ1 = 15 was smaller for DAD. However, the performance of RL-SED greatly exceeded\nthat of DAD in all environments.\nFrom the results and the properties of the experiments, we can claim that our algorithm outper-\nforms the existing methods in terms of performance, sample eﬃciency, and the ability for sequential\ndecision making.\n5.3\nDeath process\nThe death process is an epidemiological model that describes the number of infected people [8], and\nthe basic setup for this experiment is adapted from the previous works [28, 13]. In the death process\nexample, the problem is to estimate the infection rate by observing the number of people infected with\ninfectious diseases for which the probability of infection increases over time. Speciﬁcally, the infection\nprobability η is described by\nη = 1 −exp(−ξθ),\n(5.9)\nwhere ξ denotes the time since the spread of infection and θ is a random latent variable representing\nthe infectivity obtained by θ ∼TruncatedNormal(µθ, σθ, min = 0, max = ∞). In the design of the\nexperiment, the interval time at from the time of the previous observation is the output of the action\nin (4.5). In the ﬁrst observation, the output a0 is used as the ﬁrst observation time ξ0. In every\nobservation, a certain number of people are inspected. The number of infected people in the test is\ngiven as the observed value yt, and the following distribution determines yt.\nyt ∼Binominal(N, η)\n(5.10)\nη = 1 −exp(−θξt)\n(5.11)\nIn the death process, to make the experimental environment more realistic, ξt should be monoton-\nically increasing as a function of t, whereas, in the existing works, ξt can be freely chosen to set up a\nsequential problem. However, in our experimental environment, by putting the constraint at > 0, ξt\nis forced to be monotonically increasing. To set these constraints, we used the Softplus function for\nthe last layer of each agent.\nIn this experiment, we used CID as the agent’s reward.\nThe detailed parameters of the experimental environment are shown in Table 5.6.\nTable 5.2 shows the mean reward of the performance of each algorithm at the end of the training\nand the number of episodes required for training. From Table 5.2, we can see that our algorithm again\noutperforms DAD on the mean reward. The table also shows that DAD has a lower performance than\nrandom policy, indicating a failure in learning a good policy in this example.\nWe conducted additional experiments to evaluate the generalization performance of each model. In\nthis experiment, we prepared three environments with diﬀerent means of the distribution to compare\n22\nTable 5.6: Parameters in death process problem\nParameter\nValue\nMean of infection rate, µθ\n1.0\nStandard deviation of infection rate, σθ\n1.0\nNumber of observations, T\n4\nNumber of infected people, N\n50\nInner sample size of CID, L\n2000\nFigure 5.5: Learning curves for the death process problem\n23\nTable 5.7: Result for diﬀerent environments in death process problem\nalgorithm\nenvironment\nreward\nRatio to the original\nrandom\nµθ=0.5\n1.553\n0.953\nµθ=1.0\n1.630\n1.0\nµθ=1.5\n1.680\n1.03\nDAD\nµθ=0.5\n1.553\n0.989\nµθ=1.0\n1.570\n1.0\nµθ=1.5\n1.391\n0.886\nRL-SED\nµθ=0.5\n2.256\n1.04\nµθ=1.0\n2.163\n1.0\nµθ=1.5\n1.965\n0.908\nthe agents’ performance in environments with diﬀerent distributions of the infection rate. We use the\nagents who achieved the highest performance during the training. The results for each environment\nare shown in Table 5.7.\nFigure 5.5 shows the learning curves of random policy, DAD and RL-SED (ours).\nAs shown\nin Table 5.7, the performance of the random policy improved as the mean value µθ increased. In\ncontrast, the performance of DAD and RL-SED improved as µθ became smaller. These results suggest\nthat it is challenging to explain the diﬃculty of the experiment by the size of µθ.\nHowever, the\ndiﬃculty in explaining the diﬃculty of the experiment is not a problem in evaluating the generalization\nperformance of the agent.\nThe DAD performed best at µθ = 1.0 where it was trained. However, at µθ = 0.5 and µθ = 1.5,\nthe performance of the agent decreased. The performance of DAD was the same as that of random\npolicy at µθ = 0.5, and lower than that of random policy in all other environments. From this, we\ncan argue that DAD has poor generalization performance on death process problems and that it does\nnot learn well. RL-SED achieved the best performance at µθ = 0.5. In addition, at µθ = 1.5, the\nperformance decreased compared to the trained environment, but the reduction rate was lower than\nthat of DAD.\nFrom the above results, we can claim that RL-SED can learn better than existing algorithms in\nthe death process problem and has higher generalization performance.\n24\nChapter 6\nConclusion\nIn this paper, we have proposed a novel method Sequential Experimental Designs via Reinforcement\nLearning to eﬃciently learn a policy to construct Bayesian experimental designs in a sequential manner.\nWith some additional modiﬁcations to the previously-studied examples, we have enabled to quantify\nthe cost of the experiment and sample eﬃciency, both of which are essential aspects in applying the\nexperimental designs to real-world experiments. Our proposed method successfully learns a policy with\nhigher rewards than the compared methods in all three experimental environments tested. Moreover,\nwe showed that our approach is more stable and the sample eﬃciency is also better. In conclusion,\nthis present work can make a signiﬁcant contribution to the application of experimental designs to\nreal-world, large-scale, complex problems.\nAs we have discussed, the concepts of experimental design and reinforcement learning have very\nsimilar characteristics in terms of the interaction between the agent and the environment. In this\npaper, we have connected these two (apparently diﬀerent) domains and conﬁrmed the eﬀectiveness\nof our method. It might be interesting to transfer more state-of-the-art techniques in reinforcement\nlearning to construction of sequential experimental designs.\nBesides, although the proposed method performed quite well in many aspects as compared to the\nknown methods, the use of s0:t for sequential decision making resulted in huge state space. The size\nof the state space is a problem that must be solved when assuming a more complex experimental\nenvironment. It will be necessary to investigate how to reduce the state space given to the policy in\nthe future.\n25\nBibliography\n[1] B. Amzal, F. Bois, E. Parent, and C. Robert. Bayesian-optimal design via interacting particle\nsystems. Journal of the American Statistical Association, Vol. 101, pp. 773–785, 2006.\n[2] A. C. Atkinson and A. N. Donev. Optimum Experimental Designs. Oxford University Press,\n1992.\n[3] J. Beck, B. M. Dia, L. Espath, Q. Long, and R. Tempone. Fast bayesian experimental design:\nLaplace-based importance sampling for the expected information gain. Computer Methods in\nApplied Mechanics and Engineering, Vol. 334, pp. 523–553, 2018.\n[4] J. Beck, M. B. Dia, L. Espath, and R. Tempone. Multilevel double loop monte carlo and stochastic\ncollocation methods with importance sampling for bayesian optimal experimental design. Inter-\nnational Journal for Numerical Methods in Engineering, Vol. 121, No. 15, pp. 3482–3503, 2020.\n[5] M. G. Bellemare, W. Dabney, and R. Munos.\nA distributional perspective on reinforcement\nlearning. In Proceedings of the 34th International Conference on Machine Learning, pp. 449–458,\n2017.\n[6] B. P. Carlin, J. B. Kadane, and A. E. Gelfand. Approaches for optimal sequential decision analysis\nin clinical trials. Biometrics, Vol. 54, No. 3, pp. 964–975, 1998.\n[7] K. Chaloner and I. Verdinelli. Bayesian experimental design: a review. Statistical Science, Vol. 10,\npp. 273–304, 1995.\n[8] A. Cook, G. Gibson, and C. Gilligan. Optimal observation times in experimental epidemic pro-\ncesses. Biometrics, Vol. 64, No. 3, pp. 860–868, 2008.\n[9] H. Dette. A note on bayesian c- and d-optimal designs in nonlinear regression models. The Annals\nof Statistics, Vol. 24, No. 3, pp. 1225–1234, 1996.\n[10] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement\nlearning for continuous control. In Proceedings of The 33rd International Conference on Machine\nLearning, pp. 1329–1338, 2016.\n[11] R. A. Fisher. The Design of Experiments. Hafner Publishing Company, 1966.\n26\n[12] M. Fortunato, M. G. Azar, B. Piot, J. Menick, M. Hessel, I. Osband, A. Graves, V. Mnih,\nR. Munos, D. Hassabis, O. Pietquin, C. Blundell, and S. Legg. Noisy networks for exploration.\nIn International Conference on Learning Representations, 2018.\n[13] A. Foster, D. R Ivanova, I. Malik, and T. Rainforth. Deep adaptive design: amortizing sequential\nbayesian experimental design. In Proceedings of the 38th International Conference on Machine\nLearning, pp. 3384–3395, 2021.\n[14] A. Foster, M. Jankowiak, E. Bingham, P. Horsfall, Y Teh, T. Rainforth, and N. D. Goodman.\nVariational bayesian optimal experimental design. In 33rd Conference on Neural Information\nProcessing Systems, 2019.\n[15] M Giles. Multilevel Monte Carlo methods. Acta Numerica, Vol. 24, pp. 259–328, 2015.\n[16] M. B. Giles. Multilevel monte carlo path simulation. Operations Research, Vol. 56, No. 3, pp.\n607–617, 2008.\n[17] Michael B Giles. Multilevel monte carlo methods. Monte Carlo and Quasi-Monte Carlo Methods\n2012, pp. 83–103, 2013.\n[18] T. Goda, T. Hironaka, and T. Iwamoto. Multilevel monte carlo estimation of expected information\ngains. Stochastic Analysis and Applications, Vol. 38, pp. 581–600, 2018.\n[19] T. Goda, T. Hironaka, W. Kitade, and A. Foster.\nUnbiased mlmc stochastic gradient-based\noptimization of bayesian experimental designs, 2020.\n[20] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based\npolicies. In Proceedings of the 34th International Conference on Machine Learning, pp. 1352–1361,\n2017.\n[21] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: oﬀ-policy maximum entropy\ndeep reinforcement learning with a stochastic actor. In Proceedings of the 35th International\nConference on Machine Learning, pp. 1861–1870, 2018.\n[22] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement\nlearning that matters. In Proceedings of the AAAI conference on artiﬁcial intelligence, Vol. 32,\n2018.\n[23] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot,\nM. G. Azar, and D. Silver. Rainbow: combining improvements in deep reinforcement learning.\nIn The Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[24] X. Huan and Y. M. Marzouk. Simulation-based optimal bayesian experimental design for non-\nlinear systems. Journal of Computational Physics, Vol. 232, No. 1, pp. 288–317, 2013.\n27\n[25] X. Huan and Y. M. Marzouk. Sequential bayesian optimal experimental design via approximate\ndynamic programming, 2016.\n[26] S. Jiang, H. Chai, J. Gonzalez, and R. Garnett. Binoculars for eﬃcient, nonmyopic sequential\nexperimental design. In Proceedings of the 37th International Conference on Machine Learning,\npp. 4794–4803, 2020.\n[27] S. Kleinegesse, C. C. Drovandi, and M. U. Gutmann. Sequential bayesian experimental design\nfor implicit models via mutual information, 2020.\n[28] S. Kleinegesse and M. U. Gutmann. Bayesian experimental design for implicit models by mutual\ninformation neural estimation. In Proceedings of the 37th International Conference on Machine\nLearning, pp. 5316–5326, 2020.\n[29] V. Konda and J. Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information Processing\nSystems 12, 1999.\n[30] R. J. Lewis and D. A. Berry. Group sequential clinical trials: a classical evaluation of bayesian\ndecision-theoretic designs. Journal of the American Statistical Association, Vol. 89, No. 428, pp.\n1528–1534, 1994.\n[31] D. Lindley. On a measure of the information provided by an experiment. Annals of Mathematical\nStatistics, Vol. 27, pp. 986–1005, 1956.\n[32] Q. Long, M. Scavino, R. Tempone, and S. Wang. Fast estimation of expected information gains\nfor bayesian experimental designs based on laplace approximations. Computer Methods in Applied\nMechanics and Engineering, Vol. 259, pp. 24–39, 2013.\n[33] J. McGree, C. Drovandi, M. H. Thompson, J. Eccleston, S. Duﬀull, K. Mengersen, A. Pettitt,\nand T. K. Goggin. Adaptive bayesian compound designs for dose ﬁnding studies. Journal of\nStatistical Planning and Inference, Vol. 142, pp. 1480–1492, 2012.\n[34] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.\nAsynchronous methods for deep reinforcement learning. In Proceedings of The 33rd International\nConference on Machine Learning, pp. 1928–1937, 2016.\n[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. A.\nRiedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,\nD. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep rein-\nforcement learning. Nature, Vol. 518, pp. 529–533, 2015.\n[36] D. C. Montgomery. Design and Analysis of Experiments. John Wiley & Sons, Inc., 2006.\n[37] P. M¨uller and G. Parmigiani. Optimal design via curve ﬁtting of monte carlo experiments. Journal\nof the American Statistical Association, Vol. 90, No. 432, pp. 1322–1330, 1995.\n28\n[38] R. Munos, T. Stepleton, A. Harutyunyan, and M. Bellemare. Safe and eﬃcient oﬀ-policy rein-\nforcement learning. In Advances in Neural Information Processing Systems 29, 2016.\n[39] J. I. Myung, D. R. Cavagnaro, and M. Pitt. A tutorial on adaptive design optimization. Journal\nof Mathematical Psychology, Vol. 57, No. 3-4, pp. 53–67, 2013.\n[40] L. Pronzato and ´E. Thierry. Sequential experimental design and response optimisation. Statistical\nMethods and Applications, Vol. 11, pp. 277–292, 2002.\n[41] F. Pukelsheim. On linear regression designs which maximize information. Journal of Statistical\nPlanning and Inference, pp. 339–364, 1980.\n[42] T. Rainforth, R. Cornish, H. Yang, and A. Warrington. On nesting monte carlo estimators. In\nProceedings of the 35th International Conference on Machine Learning, pp. 4267–4276, 2018.\n[43] D. Rossell and P. M¨uller. Sequential stopping for high-throughput experiments. Biostatistics, pp.\n75–86, 2013.\n[44] E. G. Ryan, C. C. Drovandi, J. M. McGree, and A. N. Pettitt. A review of modern computational\nalgorithms for bayesian optimal design. International Statistical Review, Vol. 84, No. 1, pp. 128–\n154, 2016.\n[45] E. G. Ryan, C. Drovandi, and A. Pettitt. Fully bayesian experimental design for pharmacokinetic\nstudies. Entropy, Vol. 17, pp. 1063–1089, 2015.\n[46] E. G. Ryan, C. Drovandi, M. H. Thompson, and A. Pettitt. Towards bayesian experimental design\nfor nonlinear models that require a large number of sampling times. Computational Statistics &\nData Analysis, Vol. 70, pp. 45–60, 2014.\n[47] K. J. Ryan. Estimating expected information gains for experimental designs with application to\nthe random fatigue-limit model. Journal of Computational and Graphical Statistics, Vol. 12, pp.\n585–603, 2003.\n[48] T. J. Santner, B. J. Williams, and W. I. Notz. The Design and Analysis of Computer Experiments.\nSpringer, 2003.\n[49] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. In 4th Interna-\ntional Conference on Learning Representations, 2016.\n[50] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization.\nIn Proceedings of the 32nd International Conference on Machine Learning, pp. 1889–1897, 2015.\n[51] J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. High-dimensional continuous\ncontrol using generalized advantage estimation, 2018.\n[52] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms, 2017.\n29\n[53] B. Shababo, B. Paige, A. Pakman, and L. Paninski. Bayesian inference and online experimental\ndesign for mapping neural microcircuits. In Advances in Neural Information Processing Systems\n26, 2013.\n[54] X. Sheng and Y. Hu.\nMaximum likelihood multiple-source localization using acoustic energy\nmeasurements with wireless sensor networks. IEEE Transactions on Signal Processing, Vol. 53,\npp. 44–53, 2005.\n[55] H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In\nProceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, pp. 2094–2100, 2016.\n[56] J. Vanlier, C. A. Tiemann, P. Hilbers, and N. Riel. A bayesian approach to targeted experiment\ndesign. Bioinformatics, Vol. 28, pp. 1136–1142, 2012.\n[57] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas. Sample\neﬃcient actor-critic with experience replay, 2017.\n[58] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. de Freitas. Dueling network\narchitectures for deep reinforcement learning. In Proceedings of The 33rd International Conference\non Machine Learning, pp. 1995–2003, 2016.\n[59] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine Learning, Vol. 8, pp. 229–256, 1992.\n[60] S. Zheng, J. Pacheco, and J. Fisher.\nA robust approach to sequential information theoretic\nplanning. In Proceedings of the 35th International Conference on Machine Learning, pp. 5936–\n5944, 2018.\n30\n",
  "categories": [
    "cs.LG",
    "stat.ME"
  ],
  "published": "2022-02-14",
  "updated": "2022-02-14"
}