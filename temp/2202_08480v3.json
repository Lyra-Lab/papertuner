{
  "id": "http://arxiv.org/abs/2202.08480v3",
  "title": "Eliciting Structural and Semantic Global Knowledge in Unsupervised Graph Contrastive Learning",
  "authors": [
    "Kaize Ding",
    "Yancheng Wang",
    "Yingzhen Yang",
    "Huan Liu"
  ],
  "abstract": "Graph Contrastive Learning (GCL) has recently drawn much research interest\nfor learning generalizable node representations in a self-supervised manner. In\ngeneral, the contrastive learning process in GCL is performed on top of the\nrepresentations learned by a graph neural network (GNN) backbone, which\ntransforms and propagates the node contextual information based on its local\nneighborhoods. However, nodes sharing similar characteristics may not always be\ngeographically close, which poses a great challenge for unsupervised GCL\nefforts due to their inherent limitations in capturing such global graph\nknowledge. In this work, we address their inherent limitations by proposing a\nsimple yet effective framework -- Simple Neural Networks with Structural and\nSemantic Contrastive Learning} (S^3-CL). Notably, by virtue of the proposed\nstructural and semantic contrastive learning algorithms, even a simple neural\nnetwork can learn expressive node representations that preserve valuable global\nstructural and semantic patterns. Our experiments demonstrate that the node\nrepresentations learned by S^3-CL achieve superior performance on different\ndownstream tasks compared with the state-of-the-art unsupervised GCL methods.\nImplementation and more experimental details are publicly available at\n\\url{https://github.com/kaize0409/S-3-CL.}",
  "text": "Eliciting Structural and Semantic Global Knowledge in\nUnsupervised Graph Contrastive Learning\nKaize Ding*, Yancheng Wang*, Yingzhen Yang, and Huan Liu\nArizona State University\nSchool of Computing and Augmented Intelligence\nkaize.ding@asu.edu, yancheng.wang@asu.edu, yingzhen.yang@asu.edu, huan.liu@asu.edu\nAbstract\nGraph Contrastive Learning (GCL) has recently drawn much\nresearch interest for learning generalizable node representa-\ntions in a self-supervised manner. In general, the contrastive\nlearning process in GCL is performed on top of the represen-\ntations learned by a graph neural network (GNN) backbone,\nwhich transforms and propagates the node contextual informa-\ntion based on its local neighborhoods. However, nodes sharing\nsimilar characteristics may not always be geographically close,\nwhich poses a great challenge for unsupervised GCL efforts\ndue to their inherent limitations in capturing such global graph\nknowledge. In this work, we address their inherent limitations\nby proposing a simple yet effective framework â€“ Simple Neural\nNetworks with Structural and Semantic Contrastive Learning\n(S3-CL). Notably, by virtue of the proposed structural and\nsemantic contrastive learning algorithms, even a simple neural\nnetwork can learn expressive node representations that pre-\nserve valuable global structural and semantic patterns. Our\nexperiments demonstrate that the node representations learned\nby S3-CL achieve superior performance on different down-\nstream tasks compared with the state-of-the-art unsupervised\nGCL methods. Implementation and more experimental details\nare publicly available at https://github.com/kaize0409/S-3-CL.\n1\nIntroduction\nLearning expressive node representations of graph-structured\ndata plays an essential role in a variety of real-world ap-\nplications, ranging from social network analysis (Kipf and\nWelling 2017), to drug discovery (Fout et al. 2017), to ï¬-\nnancial fraud detection (Ding et al. 2019). Recently, graph\nneural networks (GNNs), which generally follow a recur-\nsive message-passing scheme, have emerged as powerful\narchitectures in graph machine learning (Kipf and Welling\n2017; VeliË‡ckoviÂ´c et al. 2018; Hamilton, Ying, and Leskovec\n2017; Wu et al. 2019; Ding et al. 2020; Wang et al. 2020).\nThough GNNs are empirically effective in handling super-\nvised or semi-supervised graph machine learning tasks, the\nlabor-intensive and resource-expensive data labeling cost is\nmeanwhile unbearable (Ding et al. 2022b; Zhang et al. 2022;\nDing et al. 2022c). To relieve the burdensome reliance on\nhuman-annotated labels, unsupervised (self-supervised) node\nrepresentation learning with GNNs has drawn much research\n*Indicates equal contribution.\nCopyright Â© 2023, Association for the Advancement of Artiï¬cial\nIntelligence (www.aaai.org). All rights reserved.\nattention lately (Kipf and Welling 2016; VeliË‡ckoviÂ´c et al.\n2019; You et al. 2020).\nMore recently, contrastive learning (He et al. 2020; Chen\net al. 2020b) has been actively explored to advance the perfor-\nmance of GNNs in graph self-supervised learning (VeliË‡ckoviÂ´c\net al. 2019; You et al. 2020; Hassani and Khasahmadi 2020;\nQiu et al. 2020; Zhu et al. 2020b). In general, graph con-\ntrastive learning (GCL) methods learn representations by\ncreating two augmented views of each graph element and\nmaximizing the agreement between the encoded represen-\ntations of the two augmented views. Correspondingly, the\nrelevant view pairs (positive) will be pulled together, and the\nirrelevant view pairs (negative) will be pushed away in the la-\ntent space. With only non-semantic labels, unsupervised GCL\ncan provide generalizable node representations for various\ndownstream tasks (You et al. 2020; Hassani and Khasahmadi\n2020; Du et al. 2021), becoming a prevailing paradigm in\nunsupervised node representation learning.\nDespite the success, the research of unsupervised GCL\nis still in its infancy â€“ most of the existing GCL methods\nlearn node representations based on the information from\nthe local neighborhoods due to the shallow property of con-\nventional GNNs. While for real-world graphs, nodes sharing\nsimilar characteristics may not always be geographically\nclose, requiring the learning algorithm to retain such â€œglobalâ€\nawareness. However, it is a non-trivial task for the exist-\ning GCL methods built on top of shallow GNNs since they\nhave inherent limitations in capturing either structural global\nknowledge or semantic global knowledge. Speciï¬cally: (i)\nfrom the structural perspective, long-range node interactions\nare highly desired for capturing structural global knowledge,\nespecially for many downstream tasks that have large prob-\nlem radii (Alon and Yahav 2021). To this end, a straightfor-\nward way is to employ a deeper GNN encoder to encode the\naugmented graphs. However, directly stacking multiple GNN\nlayers will not only lead to information distortion caused by\nthe oversmoothing issue (Chen et al. 2020a), but also intro-\nduce additional training parameters that hamper the model\ntraining efï¬ciency; and (ii) from the semantic perspective,\nexisting unsupervised GCL methods predominately focus on\ninstance-level contrast that leads to a latent space where all\nnodes are well-separated and each node is locally smooth (Li\net al. 2021) (i.e., input with different augmentations have sim-\nilar representations), while the underlying semantic structure\narXiv:2202.08480v3  [cs.LG]  4 Dec 2022\n(i.e., intra-cluster compactness and inter-cluster separability)\nof the input graph is largely ignored (Li et al. 2021). The lack\nof prior knowledge of ground-truth labels (e.g., cluster/class\nnumbers) leaves a signiï¬cant gap for unsupervised GCL to\nconsolidate the semantic structure from a global view in the\nlatent space. Yet, how to bridge this gap remains unattended.\nIn this paper, we address the aforementioned limitations by\nproposing a simple yet effective GCL framework, namely, S3-\nCL (Simple Neural Networks with Structural and Semantic\nContrastive Learning). The proposed two new contrastive\nlearning algorithms enable the framework to outperform other\nGCL counterparts with a much simpler and parameter-less\nencoding backbone, such as an MLP or even a one-layer neu-\nral network. To capture long-range node interactions without\noversmoothing, the structural contrastive learning algorithm\nï¬rst generates multiple augmented views of the input graph\nbased on different feature propagation scales (i.e., multi-scale\nfeature propagation). Then by performing contrastive learn-\ning on the node representations learned from the local and\nmultiple high-order views, the encoder network can improve\nnode-wise discrimination by exploiting the consistency be-\ntween the local and global structure information of each node.\nIn the meantime, the semantic contrastive learning algorithm\nfurther enhances intra-cluster compactness and inter-cluster\nseparability to better consolidate the semantic structure from\na global view. Speciï¬cally, it infers the clusters among nodes\nand their corresponding prototypes by a new Bayesian non-\nparametric algorithm and then performs semantic contrastive\nlearning to enforce those nodes that are semantically simi-\nlar to cluster around their corresponding cluster prototypes\nin the latent space. By jointly optimizing the structural and\nsemantic contrastive losses, the pre-trained encoder network\ncan learn highly expressive node representations for various\ndownstream tasks without using any human-annotated labels.\nWe summarize our contributions as follows:\nâ€¢ We develop a new GCL framework S3-CL, which can learn\nexpressive node representations in a self-supervised fashion\nby using a simple and parameter-less encoding backbone.\nâ€¢ We propose structural and semantic contrastive learning\nalgorithms, which can be used for explicitly capturing the\nglobal structural and semantic patterns of the input graph.\nâ€¢ We conduct extensive experiments to show that our ap-\nproach signiï¬cantly outperforms the state-of-the-art GCL\ncounterparts on various downstream tasks.\n2\nPreliminaries\nWe start by introducing the notations used throughout the\npaper. An attributed graph with N nodes can be formally\nrepresented by G = (V, E, X), where V = {v1, v2, . . . , vN}\nand E âŠ†V Ã— V denote the set of nodes and edges respec-\ntively. Let A âˆˆ{0, 1}NÃ—N be the adjacency matrix of graph\nG. Aij = 1 if and only if (vi, vj) âˆˆE. ËœA stands for the adja-\ncency matrix for a graph with added self-loops I. We let D\nand ËœD denote the diagonal degree matrix of A and ËœA respec-\ntively. xi is the i-th row of the attribute matrix X âˆˆRNÃ—D,\nwhich denotes the feature of node vi. Hence, an attributed\ngraph can also be described as G = (X, A) for simplicity.\nGraph Contrastive Learning. In general, graph contrastive\nlearning aims to pre-train a graph encoder that can maximize\nthe node-wise agreement between two augmented views of\nthe same graph element in the latent space via a contrastive\nloss. Generally, given an attributed graph G = (X, A), two\ndifferent augmented views of the graph, denoted as G(1) =\n(X(1), A(1)) and G(2) = (X(2), A(2)), are generated through\nthe data augmentation function(s). The node representations\non G1 and G(2) are denoted as H(1) = fÎ¸(X(1), A(1)) and\nH(2) = fÎ¸(X(2), A(2)), where fÎ¸(Â·) is an encoder network.\nThe agreement between the node representations is com-\nmonly measured through Mutual Information (MI). Thus, the\ncontrastive objective can be generally formulated as:\nmax\nÎ¸\nN\nX\ni=1\nMI(h(1)\ni , h(2)\ni ).\n(1)\nFollowing\nthis\nformulation,\nDeep\nGraph\nInfomax\n(DGI) (VeliË‡ckoviÂ´c et al. 2019) is the ï¬rst method that con-\ntrasts the patch representations with high-level graph rep-\nresentations by maximizing their mutual information. MV-\nGRL (Hassani and Khasahmadi 2020) adopts graph diffusion\nto generate an augmented view, and contrast representations\nof ï¬rst-order neighbors with a graph diffusion. GCC (Qiu\net al. 2020) and GRACE (Zhu et al. 2020a) create the aug-\nmented views by sampling subgraphs. MERIT (Jin et al.\n2021) adopts a siamese self-distillation network and per-\nforms contrastive learning across views and networks at the\nsame time. Nonetheless, existing unsupervised GCL meth-\nods only focus on short-range node interactions and are also\nineffective in capturing the semantic structure of graphs.\n3\nMethodology\nIn this paper, we propose a novel graph contrastive learning\nframework S3-CL for unsupervised/self-supervised node rep-\nresentation learning. The overall framework is illustrated in\nFigure 1. Our proposed framework consists of three main\ncomponents: (i) a simple (e.g., 1-layer) encoder network;\n(ii) a structural contrastive learning algorithm; and (iii) a\nsemantic contrastive learning algorithm.\n3.1\nStructural Contrastive Learning\nExisting GCL methods for unsupervised node representation\nlearning aim to achieve node-wise discrimination by max-\nimizing the agreement between the representations of the\nsame graph element in different augmented views. Despite\ntheir success, they commonly ignore the global structure\nknowledge due to the limitations of either the adopted data\naugmentation function or the GNN encoder. In this work,\nwe propose the structural contrastive learning algorithm,\nwhich enables a simple neural network to capture both local\nand global structural knowledge by performing contrastive\nlearning on multi-scale augmented graph views.\nMulti-scale Feature Propagation. In order to capture long-\nrange node interactions without suffering the oversmoothing\nissue, in our structural contrastive learning algorithm, we\npropose to adopt multi-scale feature propagation to augment\nthe input graph from the structural perspective. Compared to\narbitrarily modifying the graph structure such as perturbing\nğ‘“ğœ½!\nâ€¦\nğ‘“ğœ½\nMomentum \nUpdate\nâ€¦\nğ‘¼(#)\nğ‘¼(%)\nâ€¦\nğ‘¼(&)\nğ‘¯'\nPseudo Labels\nLatent Space\nğ¿()*\nğ‘”+\nSemantic Prototypes\nğ¿,-.\nNode Representation \nâ€¦\nNode-wise Contrasting\nâ€¦\nâ€¦\nğ‘¯\nâ€¦\nğ’‰!\nğ’‰\"\nğ’‰#\nâ€¦\nMomentum \nEncoder\nPrototype \nInference\nProjection\nHead\nEncoder\nSemantic Contrastive Learning\nMulti-scale\nFeature Propagation\nStructural Contrastive Learning\nğ—'(%)\nğ—'(&)\nğ—'(#)\nFigure 1: Illustration of the overall framework S3-CL for self-supervised node representation learning.\nedges or nodes, feature propagation not only allows incor-\nporating long-range node interactions but also mitigates the\nnoises in the original graph (Ding et al. 2022b). Unlike exist-\ning GCL algorithms that perform only two augmentations for\neach instance, we perform feature propagation with different\nscales to generate L augmented feature matrices { Â¯X(l)}L\nl=1,\neach of which encodes the l-hop node interactions in the\ngraph. Then each augmented feature matrix Â¯X(l) can be en-\ncoded by a encoder network fÎ¸(Â·) and the corresponding\nnode representations can be computed by:\nH(l) = fÎ¸( Â¯X(l)) = ReLU( Â¯X(l)Î˜),\nÂ¯X(l) = TlX,\n(2)\nwhere T âˆˆRNÃ—N is a generalized transition matrix and we\ntake T = ËœAsym = ËœDâˆ’1/2 ËœA ËœDâˆ’1/2 in this work. H(1) is\nlearned from a local view as the message-passing is only en-\nabled between direct neighbors, while {H(l)}L\nl=2 are learned\nfrom a set of high-order views that encode the long-range\nnode interactions at different scales.\nIt is noteworthy that our model inherently separates the fea-\nture propagation step, i.e., Â¯X(l) = TlX, and transformation\nstep, i.e., fÎ¸( Â¯X(l)) into the data augmentation and represen-\ntation learning modules, respectively. Compared to standard\nGNNs that couple the two steps together in each layer, this de-\ncoupling strategy allows the model to perform the high-order\nfeature propagation without conducting non-linear transfor-\nmations, reducing the risk of over-smoothing (Feng et al.\n2020; Ding et al. 2022a) in contrastive learning. In the mean-\ntime, we can use a much simpler encoding backbone to trans-\nform the augmented features to node representations without\nstacking multiple GNN layers.\nStructural Contrastive Objective. In our structural con-\ntrastive learning, we aim to maximize the agreement between\nthe representations of each node learned from the local view\nand its different high-order views by maximizing their mutual\ninformation. Instead of directly contrasting the output of the\nencoder network, we follow previous research in contrastive\nlearning (Chen et al. 2020b) and apply a projection head\ngÏˆ(Â·) to the node representations computed by the encoder\nnetwork. As such, the representations we contrast in our\nstructural contrastive learning can be denoted by {U(l)}L\nl=1,\nwhere U(l) = gÏˆ(H(l)), and gÏˆ(Â·) is a two-layer MLP in\nour implementation.\nIn our work, we adopt InfoNCE (Oord, Li, and Vinyals\n2018) to estimate the lower bound of the mutual information\nbetween the node representations learned from a local view\nU(1) and different high-order views {U(l)}L\nl=2 of the input\ngraph. The loss function of structural contrastive learning can\nbe deï¬ned as:\nLstr = âˆ’\nN\nX\ni=1\nL\nX\nl=2\nlog\nexp(u(1)\ni\nÂ· u(l)\ni /Ï„1)\nPM+Lâˆ’1\nj=1\nexp(u(1)\ni\nÂ· u(l)\nj /Ï„1)\n, (3)\nwhere\nÏ„1\nis\nthe\ntemperature\nparameter.\nNote\nthat\n{u(l)\nj }M+Lâˆ’1\nj=1\ncontains Lâˆ’1 positive examples and M nega-\ntive examples sampled from augmented views of other nodes.\nBy performing the proposed structural contrastive learning\nbased on multi-scale augmentations of the input graph, the en-\ncoder network fÎ¸(Â·) not only encourages accurate node-wise\ndiscrimination but also captures multi-scale global structural\nknowledge during the learning process. The resulted node\nrepresentations H can be computed by feeding the mixed-\norder propagated features Â¯X to the encoder network as:\nH = fÎ¸( Â¯X) = ReLU( Â¯XÎ˜),\nÂ¯X = 1\nL\nL\nX\nl=1\nTlX.\n(4)\nThis enables the learned node representations to preserve\nboth local and global structure information compared with\ndirectly using TLX (Xu et al. 2018; Feng et al. 2020).\n3.2\nSemantic Contrastive Learning\nDespite the structural contrastive learning algorithm can pro-\nvide better node-wise discrimination by exploiting the global\nstructural knowledge based on the multi-scale propagated\nfeatures, it has the same limitation as existing GCL efforts â€“\ncannot explicitly encode the semantic structure of the input\ngraph. To further capture the semantic global knowledge,\nwe propose a semantic contrastive learning algorithm that\nencourages the intra-cluster compactness and inter-cluster\nseparability in the semantic latent space.\nSince the prior knowledge of node clusters is unknown,\nwe propose to iteratively infer the clusters among nodes and\nthe corresponding prototypes based on the learned node rep-\nresentations, and perform semantic contrastive learning to\npromote those nodes that are semantically similar clustering\naround their corresponding cluster prototypes.\nWe denote the cluster prototype representation via a matrix\nC âˆˆRKÃ—Dâ€², where K is the number of prototypes inferred\nfrom the data. We use ck to denote the k-th row of C, which\nis the representation of the k-th prototype in the latent space.\nThe prototype assignments or pseudo labels of nodes are de-\nnoted by Z = {zi}n\ni=1, where zi âˆˆ{1, ..., K} is the pseudo\nlabel of node vi.\nBayesian Non-parametric Prototype Inference. A key\nfunction of our semantic contrastive learning algorithm is\nto infer highly representative cluster prototypes. However,\nthe optimal number of clusters is unknown under the setting\nof unsupervised node representation learning. To bridge the\ngap, we propose a Bayesian non-parametric prototype infer-\nence algorithm to approximate the optimal number of clusters\nand simultaneously compute the cluster prototypes. Speciï¬-\ncally, we build a Dirichlet Process Mixture Model (DPMM)\nand assume the distribution of node representations is a mix-\nture of Gaussians, in which each component is used to model\nthe prototype of a cluster. Note that the components share\nthe same ï¬xed covariance matrix ÏƒI. The DPMM model is\ndeï¬ned as:\nG âˆ¼DP(G0, Î±),\nÏ†i âˆ¼G,\nhi âˆ¼N(Ï†i, ÏƒI),\n(5)\nwhere G is a Gaussian distribution drawn from the Dirichlet\nprocess DP(G0, Î±), and Î± is the concentration parameter for\nDP(G0, Î±). Ï†i is the mean of the Gaussian sampled for node\nrepresentation hi. G0 is the prior over means of the Gaussians.\nWe take G0 to be a zero-mean Gaussian N(0, ÏI), where ÏI\nis the covariance matrix.\nNext, we use a collapsed Gibbs sampler (Resnik and\nHardisty 2010) to infer the Gaussian components. The Gibbs\nsampler iteratively samples pseudo labels for the nodes given\nthe means of the Gaussian components and samples the\nmeans of the Gaussian components given the pseudo labels of\nthe nodes. Following (Kulis and Jordan 2011), such a process\nis almost equivalent to K-Means when the variance of the\nGaussian components Ïƒ â†’0. The almost zero variance elim-\ninates the need to estimate the variance Ïƒ, thus making the\ninference efï¬cient. Let ËœK denote the number of inferred pro-\ntotypes at the current iteration step, the prototype assignment\nupdate can be formulated as:\nzi = arg min\nk\n{dik} ,\ndik =\n\u001a\n||hi âˆ’ck||2\nfor k = 1, ..., ËœK\nÎ¾\nfor k = ËœK + 1,\n(6)\nwhere dik is the distance to determine the pseudo labels of\nnode representation hi. Î¾ is the margin to initialize a new\nprototype. In practice, we choose the value of Î¾ by perform-\ning cross-validation on each dataset. With the formulation in\nEquation (6), a node will be assigned to the prototype mod-\neled by the Gaussian component corresponding to the closest\nmean of Gaussian, unless the squared Euclidean distance to\nthe closest mean is greater than Î¾. After obtaining the pseudo\nlabels, the cluster prototype representations can be computed\nby: ck = P\nzi=k hi/P\nzi=k 1, for k = 1, ..., ËœK.\nNote that we iteratively update prototype assignments and\nprototype representations till convergence, and we set the\nnumber of prototypes K to be the number of inferred pro-\ntotypes ËœK afterward. Afterwards, we reï¬ne the cluster pro-\ntotypes using label propagation and we attach the details in\nSupplementary B.2 due to the space limit.\nSemantic Contrastive Objective. After obtaining the pro-\ntotype assignments Z and prototype representations C, our\nsemantic contrastive objective aims to consolidate the seman-\ntic structure (i.e., intra-cluster compactness and inter-cluster\nseparability) of the learned node representation H by updat-\ning the encoder parameter Î¸. To this end, we maximize the\nlikelihood of each node in the graph given Î¸ and C:\nQ(Î¸) =\nN\nX\nn=1\nlog p(xi|Î¸, C)\n=\nN\nX\nn=1\nlog\nK\nX\nk=1\np(xi, k|Î¸, C),\n(7)\nwhere p is the probability density function. Directly opti-\nmizing log-likelihood Q(Î¸) is intractable as the labels of\nnodes are unknown. Instead, we optimize the variational\nlower bound of Q(Î¸), given by:\nQ(Î¸) â‰¥\nN\nX\ni=1\nK\nX\nk=1\np(k|xi) log p(xi, k|Î¸, C)\np(k|xi)\n=\nN\nX\ni=1\nK\nX\nk=1\np(k|xi) log p(xi, k|Î¸, C)\nâˆ’\nN\nX\ni=1\nK\nX\nk=1\np(k|xi) log p(k|xi).\n(8)\nNote that we can drop the second term of the right-hand side\nof Equation (8) as it is a constant. To maximize the remaining\npart PN\ni=1\nPK\nk=1 p(k|xi) log p(xi, k|Î¸, C), we can estimate\np(k|xi) by p(k|xi, Î¸, C) = 1{k=zi}, as we assign xi to clus-\nter zi given C in our DPMM model. Thus, we can maximize\nQ(Î¸) by minimizing the following loss function:\nLsem = âˆ’\nN\nX\ni=1\nlog p(xi, zi|Î¸, C).\n(9)\nUnder the assumption of a uniform prior distribution\nof xi over different prototypes, we have p(xi, zi|Î¸, C) âˆ\np(xi|zi, Î¸, C). Since the distribution of node representa-\ntion around each prototype generated by the DPMM is an\nisotropic Gaussian, after applying â„“2 normalization on the\nrepresentation of nodes and prototypes, we can estimate\np(xi|zi, Î¸, C) by:\np(xi|zi, Î¸, C) =\nexp(hi Â· czi/Ï„2)\nPK\nk=1 exp(hi Â· ck/Ï„2)\n,\n(10)\nwhere czi is the representations of zi-th prototype. The tem-\nperature parameter Ï„2 âˆÏƒ2 is related to the concentration\nof node representation around each prototype, and Ïƒ is the\nvariance of the Gaussians in the DPMM model deï¬ned by\nEquation (5). For the simplicity of training, we directly take\nÏ„2 as a hyperparameter. Taking Equation (10) into Equa-\ntion (9), we can maximize Q(Î¸) by minimizing the following\nloss function:\nLsem = âˆ’\nN\nX\ni=1\nlog\nexp(hi Â· czi/Ï„2)\nPK\nk=1 exp(hi Â· ck/Ï„2)\n.\n(11)\n3.3\nModel Learning\nGiven the proposed S3-CL learning framework, our goal is to\nlearn expressive node representations that preserve both valu-\nable structural and semantic knowledge without any semantic\nlabels. In this section, we will introduce the overall loss func-\ntion, and also the optimization of the proposed framework\nwith regard to the network parameters, prototype assignments,\nand prototype representations.\nOverall Loss. To train our model in an end-to-end fashion\nand learn the encoder fÎ¸(Â·), we jointly optimize both the\nstructural and semantic contrastive learning losses. The over-\nall objective function is deï¬ned as:\nL = Î³Lstr + (1 âˆ’Î³)Lsem,\n(12)\nwhere we aim to minimize L during training, and Î³ is a\nbalancing parameter to control the contribution of each con-\ntrastive learning loss. For the sake of the stability of the train-\ning of the encoder, we apply our Bayesian non-parametric\nprototype inference algorithm on the node representations\ncomputed by a momentum encoder (He et al. 2020).\nNotably, in semantic contrastive learning, the computed\npseudo labels Z can be utilized in the negative example\nsampling process in our structural contrastive learning to\navoid sampling bias issues (Chuang et al. 2020). We select\nnegative samples in Equation (3) for each node from nodes\nassigned to different prototypes.\nModel Optimization via EM. Speciï¬cally, we adopt EM\nalgorithm to alternately estimate the posterior distribution\np(zi|xi, Î¸, C) and optimize the network parameters Î¸. We\ndescribe the details for the E-step and M-step applied in our\nmethods as follows:\nâ€¢ E-step. In this step, we ï¬x the network parameter Î¸, and\nestimate the prototypes C and the prototype assignment\nZ with our proposed Bayesian non-parametric prototype\ninference algorithm (more details in Supplementary B.1).\nâ€¢ M-step. Given the posterior distribution computed by\nthe E-step, we aim to maximize the expectation of log-\nlikelihood Q(Î¸), by directly optimizing the semantic con-\ntrastive loss function Lsem. In order to perform structural\nand semantic contrastive learning at the same time, we\ninstead optimize a joint overall loss function as formulated\nin Equation (12).\nAlgorithm 1: The learning algorithm of S3-CL.\nInput: Attribute matrix X; adjacency matrix A;\npropagation step L\nOutput: Pretrained encoder network fÎ¸(Â·)\n1 Initialize encoder parameter Î¸ and Î¸â€²\n2 while not converge do\n3\nCompute node representations of different augmented\nviews {H(l)}L\nl=1 and {U(l)}L\nl=1\n4\nCompute the prototype representations C and prototype\nassignments Z\nâ–·E-step Update\n5\nCalculate loss Lstr and Lsem by Equation (3) and\nEquation (11), respectively\n6\nL = Î³Lstr + (1 âˆ’Î³)Lsem\n7\nUpdate Î¸ by minimizing L\nâ–·M-step Update\n8\nUpdate momentum encoder Î¸â€²\n9 return the encoder network fÎ¸(Â·)\nAlgorithm 1 outlines the learning process of the proposed\nframework. After the self-supervised pre-training is done, the\npre-trained encoder can be directly used to generate node\nrepresentations for various downstream tasks.\n4\nExperiments\n4.1\nExperimental Settings\nEvaluation Datasets. In our experiments, we evaluate S3-\nCL on six public benchmark datasets that are widely used\nfor node representation learning, including Cora (Sen et al.\n2008), Citeseer (Sen et al. 2008), Pubmed (Namata et al.\n2012), Amazon-P (Shchur et al. 2018), Coauthor CS (Shchur\net al. 2018) and ogbn-arxiv (Hu et al. 2020). Cora, Citeseer,\nand Pubmed are the three most widely used citation networks.\nAmazon-P is a co-purchase graph and Coauthor CS is a co-\nauthorship graph. The ogbn-arxiv is a large-scale citation\ngraph benchmark dataset.\nCompared Methods. To demonstrate the effectiveness\nof our proposed method, six state-of-the-art graph self-\nsupervised learning methods are compared in our experi-\nments, including DGI (VeliË‡ckoviÂ´c et al. 2019), MVGRL (Has-\nsani and Khasahmadi 2020), GMI (Peng et al. 2020),\nGRACE (Zhu et al. 2020a), MERIT (Jin et al. 2021), and\nSUGRL (Mo et al. 2022). As we consider node classiï¬-\ncation as our downstream task, we also include ï¬ve rep-\nresentative supervised node classiï¬cation methods, namely\nMLP (VeliË‡ckoviÂ´c et al. 2019), LP (Zhu, Ghahramani, and Laf-\nferty 2003), GCN (Kipf and Welling 2017), GAT (VeliË‡ckoviÂ´c\net al. 2018), and SGC (Wu et al. 2019), as baselines for\nthe evaluation on the node classiï¬cation task. To evaluate\nthe model performance for node clustering, we compare S3-\nCL against methods including K-Means (Lloyd 1982), GAE\n(Kipf and Welling 2016), adversarially regularized GAE\n(ARGA) and VGAE (ARVGA) (Pan et al. 2018), GALA\n(Park et al. 2019), DGI, DBGAN (Zheng et al. 2020), MV-\nGRL, MERIT, and SUGRL.\n4.2\nEvaluation Results\nNode Classiï¬cation. To evaluate the trained encoder net-\nwork, we adopt a linear evaluation protocol by training a\nMethods\nCora\nCiteseer\nPubmed\nAmazon-P\nCoauthor CS\nogbn-arxiv\nSUPERVISED\nMLP\n55.2 Â± 0.4\n46.5 Â± 0.5\n71.4 Â± 0.3\n78.5 Â± 0.2\n76.5 Â± 0.3\n55.5 Â± 0.2\nLP (Zhu, Ghahramani, and Lafferty 2003)\n68.0 Â± 0.5\n45.3 Â± 0.6\n63.0 Â± 0.3\n75.4 Â± 0.0\n74.3 Â± 0.0\n68.3 Â± 0.0\nGCN (Kipf and Welling 2017)\n81.7 Â± 0.4\n70.5 Â± 0.3\n79.4 Â± 0.4\n87.3 Â± 1.0\n91.8 Â± 0.1\n71.7 Â± 0.3\nGAT (VeliË‡ckoviÂ´c et al. 2018)\n83.0 Â± 0.7\n72.5 Â± 0.7\n79.0 Â± 0.3\n86.2 Â± 1.5\n90.5 Â± 0.7\n73.2 Â± 0.2\nSGC (Wu et al. 2019)\n81.5 Â± 0.2\n73.1 Â± 0.1\n79.7 Â± 0.4\n88.3 Â± 1.1\n91.5 Â± 0.3\n69.8 Â± 0.2\nSELF-SUPERVISED + FINE-TUNING\nDGI (VeliË‡ckoviÂ´c et al. 2019)\n81.7 Â± 0.6\n71.5 Â± 0.7\n77.3 Â± 0.6\n83.1 Â± 0.3\n90.0 Â± 0.3\n67.1 Â± 0.4\nGMI (Peng et al. 2020)\n82.7 Â± 0.2\n73.0 Â± 0.3\n80.1 Â± 0.2\n85.1 Â± 0.0\n91.0 Â± 0.0\n69.6 Â± 0.3\nMVGRL (Hassani and Khasahmadi 2020)\n82.9 Â± 0.7\n72.6 Â± 0.7\n79.4 Â± 0.3\n87.3 Â± 0.1\n91.3 Â± 0.1\n71.3 Â± 0.2\nGRACE (Zhu et al. 2020a)\n80.0 Â± 0.4\n71.7 Â± 0.6\n79.5 Â± 1.1\n81.8 Â± 0.8\n90.1 Â± 0.8\n71.1 Â± 0.2\nMERIT (Jin et al. 2021)\n83.1 Â± 0.6\n74.0 Â± 0.7\n80.1 Â± 0.4\n88.8 Â± 0.4\n92.4 Â± 0.4\n71.7 Â± 0.1\nSUGRL (Mo et al. 2022)\n83.4 Â± 0.5\n73.0 Â± 0.5\n81.9 Â± 0.5\n88.5 Â± 0.2\n92.2 Â± 0.5\n69.3 Â± 0.2\nS3-CL (ours)\n84.5 Â± 0.4\n74.6 Â± 0.4\n80.8 Â± 0.3\n89.0 Â± 0.5\n93.1 Â± 0.4\n72.8 Â± 0.3\nTable 1: Node classiï¬cation performance comparison on benchmark datasets.\nMethods\nCora\nCiteseer\nPubmed\nACC\nNMI\nARI\nACC\nNMI\nARI\nACC\nNMI\nARI\nK-Means\n49.2\n32.1\n22.9\n54.0\n30.5\n27.8\n59.5\n31.5\n28.1\nGAE (Kipf and Welling 2016)\n59.6\n42.9\n34.7\n40.8\n17.6\n12.4\n67.2\n27.7\n27.9\nARGA (Pan et al. 2018)\n64.0\n44.9\n35.2\n57.3\n35.0\n34.1\n66.8\n30.5\n29.5\nARVGA (Pan et al. 2018)\n64.0\n45.0\n37.4\n54.4\n26.1\n24.5\n69.0\n29.0\n30.6\nGALA (Park et al. 2019)\n74.5\n57.6\n53.1\n69.3\n44.1\n44.6\n69.3\n32.7\n32.1\nDGI (VeliË‡ckoviÂ´c et al. 2019)\n55.4\n41.1\n32.7\n51.4\n31.5\n32.6\n58.9\n27.7\n31.5\nDBGAN (Zheng et al. 2020)\n74.8\n56.0\n54.0\n67.0\n40.7\n41.4\n69.4\n32.4\n32.7\nMVGRL (Hassani and Khasahmadi 2020)\n73.2\n56.2\n51.9\n68.1\n43.2\n43.4\n69.3\n34.4\n32.3\nMERIT (Jin et al. 2021)\n73.6\n57.1\n52.8\n68.9\n43.9\n44.1\n69.5\n34.7\n32.8\nSUGRL (Mo et al. 2022)\n73.9\n58.5\n53.0\n70.5\n45.8\n47.0\n69.5\n35.0\n33.4\nS3-CL (ours)\n75.1\n60.7\n56.6\n71.2\n46.3\n48.5\n71.3\n36.0\n34.7\nTable 2: Node clustering performance comparison on benchmark datasets.\nseparate logistic regression classiï¬er on top of the learned\nnode representations. We follow the evaluation protocols in\nprevious works (VeliË‡ckoviÂ´c et al. 2019; Hu et al. 2020) for\nnode classiï¬cation. The mean classiï¬cation accuracy with\nstandard deviation on the test nodes after 10 runs of training is\nreported. To avoid the out-of-memory issue when evaluating\nMVGRL, GRACE, and MERIT on the ogbn-arxiv dataset,\nwe subsample 512 nodes as negative samples for each node\nduring the self-supervised learning phase.\nThe node classiï¬cation results of different methods are\nreported in Table 1. We can clearly see that S3-CL outper-\nforms the state-of-the-art self-supervised node representation\nlearning methods across the ï¬ve public benchmarks. Such\nsuperiority mainly stems from two factors: (i) our approach\nS3-CL grants each node access to information of nodes in\na larger neighborhood; (ii) S3-CL infers the semantic infor-\nmation of nodes, and enforces intra-cluster compactness and\ninter-cluster separability on the node representation. With the\nhelp of this extra information, node representations generated\nby S3-CL are more informative and distinctive. Without ac-\ncess to labels, S3-CL even outperforms supervised methods\nlike SGC and GAT.\nNode Clustering. To evaluate the quality of the node repre-\nsentations learned by different methods, we conduct exper-\niments on node clustering. We follow the same evaluation\nprotocol as in (Hassani and Khasahmadi 2020). K-Means is\napplied on the learned node representation to get clustering\nresults. We use accuracy (ACC), normalized mutual informa-\ntion (NMI), and adjusted rand index (ARI) to measure the\nperformance of clustering. We report the averaged clustering\nresults over 10 times of execution.\nThe clustering results are displayed in Table 2. It is ob-\nserved that our approach achieves remarkable performance\ngain over compared methods. For example, the NMI on Cora\nis improved by 2.2% against the previous SOTA method\nSUGRL. Such improvement greatly attributes to the fact that\nS3-CL explores the semantic information of nodes instead\nof enforcing node-wise discrimination alone as other GCL\nmethods. Thus, the node representation learned by S3-CL\nworks well for clustering algorithms.\nNode Classiï¬cation with Few Labels. We further evaluate\nthe impact of label rate on the downstream node classiï¬cation\ntask. Speciï¬cally, we evaluate all self-supervised learning\nmethods from Table 1 under different low-resource settings.\nThe results in Figure 2 show that our proposed framework\nS3-CL can still outperform existing methods given a lower\nlabel rate. It validates that the node representations learned\nby our approach S3-CL can encode valuable structural and\nsemantic knowledge from the input graph. As a result, the\nnode representations can be effectively used for the node\nclassiï¬cation task even with an extremely small label ratio.\nEffect of Feature Propagation. Next, we investigate the\n1%\n2%\n3%\n4%\nLabel Rate\n72.5\n75.0\n77.5\n80.0\nAccuracy (%)\n(a) Cora\n0.5%\n1%\n0.2%\n0.3%\nLabel Rate\n62.5\n65.0\n67.5\n70.0\n72.5\nAccuracy (%)\n(b) Citeseer\n0.05%\n0.1%\n0.2%\n0.3%\nLabel Rate\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\nAccuracy (%)\nDGI\nGMI\nMVGRL\nGRACE\nMERIT\nSUGRL\nS3-CL\n(c) Pubmed\nFigure 2: Node classiï¬cation results with limited training labels.\n1\n2\n5\n10\n15\n20\nL\n30\n40\n50\n60\n70\n80\nAccuracy (%)\n(a) Cora\n1\n2\n5\n10\n15\n20\nL\n30\n40\n50\n60\n70\nAccuracy (%)\n(b) Citeseer\n1\n2\n5\n10\n15\n20\nL\n30\n40\n50\n60\n70\nAccuracy (%)\nMVGRL\nGRACE\nMERIT\nSUGRL\nS3-CL\n(c) Pubmed\nFigure 3: Node clustering results of GCL methods with various propagation steps (L).\neffect of multi-scale feature propagation in the structural\ncontrastive learning by altering the propagation steps L. A\nlarger L allows message-passing within a larger neighbor-\nhood for learning the node representations. To demonstrate\nthe power of our approach in utilizing structural global knowl-\nedge, we compare S3-CL against GRACE, MVGRL, MERIT,\nand SUGRL with different numbers of layers L. The node\nclustering accuracy of different methods is shown in Figure 3.\nBy increasing the propagation steps (number of layers), we\ncan clearly observe that existing unsupervised GCL methods\nseverely degrade due to the oversmoothing issue. In contrast,\nS3-CL consistently achieves improved performance by mak-\ning use of information in a larger neighborhood for node\nrepresentation learning.\nAblation Study. To validate the effectiveness of the struc-\ntural contrastive learning and semantic contrastive learning in\nS3-CL, we conduct an ablation study on Citesser, Cora, and\nPubmed with two variants of S3-CL, each of which has one\nof the contrastive learning components removed. The node\nclassiï¬cation results are shown in Table 3. We can observe\nthat the performance of S3-CL degrades when any of the\ncomponents are removed. Our S3-CL using all components\nachieves the best performance as the structural and semantic\ncontrastive components complement each other. Hence, the\neffectiveness of each component is veriï¬ed.\nMethod\nCiteseer\nCora\nPubmed\nw/o structural\n73.1Â±0.2\n83.3Â±0.3\n80.0Â± 0.3\nw/o semantic\n71.9Â±0.4\n82.2Â±0.5\n79.3Â± 0.2\nS3-CL\n74.6Â±0.4\n84.5Â±0.4\n80.8Â±0.3\nTable 3: Ablation study on contrastive components.\nRepresentation Visualization. To visually show the supe-\nrior quality of the node representations learned by S3-CL, we\nuse t-SNE to visualize and compare the learned node repre-\nsentations between S3-CL and the best-performing baseline\non Citeseer, i.e., MERIT. The visualization results are shown\nin Figure 4, where each dot represents the representation of a\nnode, and the color of the dot denotes its ground-truth label.\nFrom the ï¬gure, we can observe that though some classes can\nbe identiï¬ed by MERIT, the boundaries between different\nclasses are unclear. Our proposed model is able to enforce\nbetter intra-cluster compactness and inter-cluster separability.\n(a) MERIT\n(b) S3-CL\nFigure 4: Representation visualization on the Citeseer dataset.\n5\nConclusion\nIn this paper, we propose a new GCL framework named S3-\nCL, which can effectively capture the global knowledge from\nboth structural and semantic perspectives for unsupervised\nnode representation learning. By jointly optimizing the struc-\ntural and semantic contrastive learning losses, we can build\nthe encoder network with simple neural networks to learn\nexpressive node representations for different downstream\ntasks without using any human-annotated labels. We conduct\nextensive experiments and demonstrate that S3-CL can out-\nperform the state-of-the-art unsupervised GCL counterparts\non multiple benchmark graph datasets.\nAcknowledgments\nThis work is supported by NSF (No. 2229461), ARO (No.\nW911NF2110088), ONR (No. N00014-21-1-4002), and ARL\n(No. W911NF2020124).\nReferences\nAlon, U.; and Yahav, E. 2021. On the bottleneck of graph\nneural networks and its practical implications. In ICLR.\nCaron, M.; Bojanowski, P.; Joulin, A.; and Douze, M. 2018.\nDeep Clustering for Unsupervised Learning of Visual Fea-\ntures. In ECCV.\nCaron, M.; Bojanowski, P.; Mairal, J.; and Joulin, A. 2019.\nUnsupervised pre-training of image features on non-curated\ndata. In ICCV.\nCaron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.;\nand Joulin, A. 2020. Unsupervised learning of visual features\nby contrasting cluster assignments. NeurIPS.\nChen, D.; Lin, Y.; Li, W.; Li, P.; Zhou, J.; and Sun, X. 2020a.\nMeasuring and relieving the over-smoothing problem for\ngraph neural networks from the topological view. In AAAI.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020b.\nA simple framework for contrastive learning of visual repre-\nsentations. In ICML.\nChuang, C.-Y.; Robinson, J.; Yen-Chen, L.; Torralba, A.;\nand Jegelka, S. 2020. Debiased contrastive learning. arXiv\npreprint arXiv:2007.00224.\nDing, K.; Li, J.; Bhanushali, R.; and Liu, H. 2019. Deep\nanomaly detection on attributed networks. In SDM.\nDing, K.; Wang, J.; Caverlee, J.; and Liu, H. 2022a. Meta\nPropagation Networks for Graph Few-shot Semi-supervised\nLearning. In AAAI.\nDing, K.; Wang, J.; Li, J.; Shu, K.; Liu, C.; and Liu, H.\n2020. Graph prototypical networks for few-shot learning on\nattributed networks. In CIKM.\nDing, K.; Xu, Z.; Tong, H.; and Liu, H. 2022b. Data augmen-\ntation for deep graph learning: A survey. SIGKDD Explo-\nrations.\nDing, K.; Zhang, C.; Tang, J.; Chawla, N.; and Liu, H. 2022c.\nToward Graph Minimally-Supervised Learning. In KDD.\nDu, Y.; Wang, S.; Guo, X.; Cao, H.; Hu, S.; Jiang, J.; Varala,\nA.; Angirekula, A.; and Zhao, L. 2021. GraphGT: Machine\nLearning Datasets for Deep Graph Generation and Transfor-\nmation. In NeurIPS.\nFeng, W.; Zhang, J.; Dong, Y.; Han, Y.; Luan, H.; Xu, Q.;\nYang, Q.; Kharlamov, E.; and Tang, J. 2020. Graph random\nneural networks for semi-supervised learning on graphs. In\nNeurIPS.\nFout, A.; Byrd, J.; Shariat, B.; and Ben-Hur, A. 2017. Protein\nInterface Prediction using Graph Convolutional Networks. In\nNeurIPS.\nHamilton, W. L.; Ying, R.; and Leskovec, J. 2017. Inductive\nRepresentation Learning on Large Graphs. In NeurIPS.\nHassani, K.; and Khasahmadi, A. H. 2020. Contrastive multi-\nview representation learning on graphs. In ICML.\nHe, K.; Fan, H.; Wu, Y.; Xie, S.; and Girshick, R. 2020.\nMomentum contrast for unsupervised visual representation\nlearning. In CVPR.\nHu, W.; Fey, M.; Zitnik, M.; Dong, Y.; Ren, H.; Liu, B.;\nCatasta, M.; and Leskovec, J. 2020. Open graph benchmark:\nDatasets for machine learning on graphs. In NeurIPS.\nHuang, J.; and Gong, S. 2021. Deep clustering by semantic\ncontrastive learning. arXiv preprint arXiv:2103.02662.\nHuang, P.; Huang, Y.; Wang, W.; and Wang, L. 2014. Deep\nembedding network for clustering. In ICPR.\nJin, M.; Zheng, Y.; Li, Y.-F.; Gong, C.; Zhou, C.; and Pan, S.\n2021. Multi-Scale Contrastive Siamese Networks for Self-\nSupervised Graph Representation Learning. In IJCAI.\nKingma, D. P.; and Ba, J. L. 2014. Adam: A method for\nstochastic optimization. In ICLR.\nKipf, T. N.; and Welling, M. 2016. Variational graph auto-\nencoders. arXiv preprint arXiv:1611.07308.\nKipf, T. N.; and Welling, M. 2017. Semi-Supervised Classiï¬-\ncation with Graph Convolutional Networks. In ICLR.\nKlicpera, J.; Bojchevski, A.; and GÃ¼nnemann, S. 2019. Pre-\ndict then propagate: Graph neural networks meet personal-\nized pagerank. In ICLR.\nKulis, B.; and Jordan, M. I. 2011. Revisiting k-means: New\nalgorithms via Bayesian nonparametrics.\narXiv preprint\narXiv:1111.0352.\nLi, J.; Zhou, P.; Xiong, C.; and Hoi, S. C. 2021. Prototypi-\ncal contrastive learning of unsupervised representations. In\nICLR.\nLloyd, S. 1982. Least squares quantization in PCM. IEEE\ntransactions on information theory.\nMo, Y.; Peng, L.; Xu, J.; Shi, X.; and Zhu, X. 2022. Simple\nUnsupervised Graph Representation Learning. AAAI.\nNamata, G.; London, B.; Getoor, L.; Huang, B.; and EDU, U.\n2012. Query-driven active surveying for collective classiï¬ca-\ntion. In Workshop on MLG.\nOord, A. v. d.; Li, Y.; and Vinyals, O. 2018. Representation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nPan, S.; Hu, R.; Long, G.; Jiang, J.; Yao, L.; and Zhang, C.\n2018. Adversarially Regularized Graph Autoencoder for\nGraph Embedding. In IJCAI.\nPark, J.; Lee, M.; Chang, H. J.; Lee, K.; and Choi, J. Y. 2019.\nSymmetric graph convolutional autoencoder for unsupervised\ngraph representation learning. In ICCV, 6519â€“6528.\nPeng, Z.; Huang, W.; Luo, M.; Zheng, Q.; Rong, Y.; Xu,\nT.; and Huang, J. 2020. Graph Representation Learning via\nGraphical Mutual Information Maximization. In WWW.\nQiu, J.; Chen, Q.; Dong, Y.; Zhang, J.; Yang, H.; Ding, M.;\nWang, K.; and Tang, J. 2020. GCC: Graph Contrastive Cod-\ning for Graph Neural Network Pre-Training. In KDD.\nResnik, P.; and Hardisty, E. 2010. Gibbs sampling for the\nuninitiated. Technical report, Maryland Univ College Park\nInst for Advanced Computer Studies.\nSen, P.; Namata, G.; Bilgic, M.; Getoor, L.; Galligher, B.;\nand Eliassi-Rad, T. 2008. Collective classiï¬cation in network\ndata. AI Magazine.\nShchur, O.; Mumme, M.; Bojchevski, A.; and GÃ¼nnemann,\nS. 2018. Pitfalls of Graph Neural Network Evaluation. In\nNeurIPS Relational Representation Learning Workshop.\nVeliË‡ckoviÂ´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; LiÃ²,\nP.; and Bengio, Y. 2018. Graph Attention Networks. In ICLR.\nVeliË‡ckoviÂ´c, P.; Fedus, W.; Hamilton, W. L.; LiÃ², P.; Bengio,\nY.; and Hjelm, R. D. 2019. Deep Graph Infomax. In ICLR.\nWang, J.; Ding, K.; Hong, L.; Liu, H.; and Caverlee, J. 2020.\nNext-item recommendation with sequential hypergraphs. In\nSIGIR.\nWu, F.; Souza, A.; Zhang, T.; Fifty, C.; Yu, T.; and Wein-\nberger, K. 2019. Simplifying graph convolutional networks.\nIn ICML.\nXu, K.; Li, C.; Tian, Y.; Sonobe, T.; Kawarabayashi, K.-i.;\nand Jegelka, S. 2018. Representation learning on graphs with\njumping knowledge networks. In ICML.\nXu, M.; Wang, H.; Ni, B.; Guo, H.; and Tang, J. 2021. Self-\nsupervised graph-level representation learning with local and\nglobal structure. In ICML.\nYou, Y.; Chen, T.; Sui, Y.; Chen, T.; Wang, Z.; and Shen, Y.\n2020. Graph contrastive learning with augmentations. In\nNeurIPS.\nZhang, C.; Ding, K.; Li, J.; Zhang, X.; Ye, Y.; Chawla, N. V.;\nand Liu, H. 2022. Few-Shot Learning on Graphs: A Survey.\nIn IJCAI.\nZheng, S.; Zhu, Z.; Zhang, X.; Liu, Z.; Cheng, J.; and Zhao,\nY. 2020. Distribution-induced bidirectional generative adver-\nsarial network for graph representation learning. In CVPR.\nZhou, D.; Bousquet, O.; Lal, T. N.; Weston, J.; and SchÃ¶lkopf,\nB. 2004. Learning with local and global consistency. In\nNeurIPS.\nZhu, X.; Ghahramani, Z.; and Lafferty, J. D. 2003. Semi-\nsupervised learning using gaussian ï¬elds and harmonic func-\ntions. In ICML.\nZhu, Y.; Xu, Y.; Yu, F.; Liu, Q.; Wu, S.; and Wang, L. 2020a.\nDeep Graph Contrastive Representation Learning. In ICML\nWorkshop.\nZhu, Y.; Xu, Y.; Yu, F.; Liu, Q.; Wu, S.; and Wang, L. 2020b.\nGraph Contrastive Learning with Adaptive Augmentation. In\nTheWebConf.\nA\nDataset Details\nFor the experiments on Cora, Citeseer, and Pubmed datasets,\nwe use the same train/validation/test data splits adopted by\n(Kipf and Welling 2017; VeliË‡ckoviÂ´c et al. 2019). For the\nAmazon-P and Coauthor CS datasets, we follow the same\ntrain/validation/test data splits in (Shchur et al. 2018). For\nthe experiments with ogbn-arxiv, we follow the OGB bench-\nmarking protocol (Hu et al. 2020). Speciï¬cally, for the node\nclassiï¬cation with few labels task, we sample partial labeled\nnodes from the original training set and use the same valida-\ntion and test splits as the standard node classiï¬cation task.\nNote that for each of the datasets, we run the experiment 10\ntimes and report the average performance.\nDataset\nNodes\nEdges\nFeatures\nClasses\nCora\n2,708\n5,429\n1,433\n7\nCiteseer\n3,327\n4,732\n3,703\n6\nPubmed\n19,717\n44,338\n500\n3\nAmazon-P\n7,650\n238,162\n745\n8\nCoauthor CS\n18,333\n81,894\n6,805\n15\nogbn-arxiv\n169,343\n1,166,243\n128\n40\nTable 4: The statistics of the datasets.\nB\nImplementation Details\nWe implement our proposed framework in PyTorch and opti-\nmize it with Adam (Kingma and Ba 2014). All experiments\nare conducted on a Nvidia Tesla v100 16GB GPU. We set\nL to 10 in our multi-scale feature propagation. The output\ndimension of our encoder network is ï¬xed to 512. The num-\nber of hidden units for the MLP projector is set to 2048. We\nset the number of negative samples M in Lstr to 512. For\nthe training of S3-CL, we ï¬rst pre-train the encoder network\nby minimizing the structural contrastive loss Lstr and ini-\ntialize the model parameters with the pre-trained weights.\nAfter that, we optimize the model as illustrated in Algo-\nrithm 1. We tune the balancing parameter Î³ within the search\nspace {0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}. The value of Î¾ in our\nBayesian non-parametric prototype inference algorithm is\nselected from {0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0, 5}.\nFor all the baseline methods, we also use Adam as the opti-\nmizer. We do grid search for the learning rate in {1 Ã— 10âˆ’5,\n5Ã—10âˆ’5, 1Ã—10âˆ’4, 5Ã—10âˆ’4, 1Ã—10âˆ’3, 5Ã—10âˆ’3, 1Ã—10âˆ’2,\n5 Ã— 10âˆ’2, 1 Ã— 10âˆ’1, 5 Ã— 10âˆ’1 } on different datasets. The\ntemperature parameters Ï„1 and Ï„2 are set to 1 in our ex-\nperiments. Our implementation of S3-CL can be found at\nhttps://github.com/kaize0409/S-3-CL.\nB.1\nBayesian Non-parametric Prototype\nInference Algorithm\nTo better illustrate the process of Bayesian non-parametric\nprototype inference, We summarize the steps in Algorithm 2.\nB.2\nPrototype Reï¬nement via Label Propagation\nConsidering the fact that the pseudo labels inferred by the\nBayesian non-parametric algorithm could be inaccurate, we\nfurther reï¬ne the pseudo labels by label propagation (Zhou\net al. 2004) on the graph. This way we can smooth the\ninaccurate pseudo labels and reï¬ne the cluster prototype\nrepresentations by leveraging graph structure knowledge.\nFirstly, we convert the prototype assignments Z to a one-\nhot pseudo label matrix Z âˆˆRNÃ—K, where Zik = 1 if and\nonly if zi = k. Following the idea of Personalized PageR-\nank (PPR) (Klicpera, Bojchevski, and GÃ¼nnemann 2019), the\npseudo labels after T aggregation steps Z(T ) are updated by:\nZ(t+1) = (1 âˆ’Î²) ËœAsymZ(t) + Î²Z(0),\n(13)\nwhere Z(0) = Z and Î² can be considered as the teleport prob-\nability in PPR. Next, we convert the propagated results Z(T )\nback to hard pseudo labels by setting zi = arg maxk Z(T )\nik\nfor i âˆˆ{1, ..., N}.\nAlgorithm 2: Algorithm of Bayesian Non-parametric\nPrototype Inference.\nInput: Node representation h1, ..., hN; threshold to\ngenerate new prototype Î²\nOutput: Prototype representation C, pseudo labels Z, and\nnumber of clusters K\n1 Initialize ËœK = 1, zi = 1 and c1 =\n1\nN\nPN\ni=1 hi.\n2 while not converge do\n3\nfor i = 1 to N do\n4\nUpdate the pseudo label zi according to Eq.(6)\n5\nIf mink dik > Î², set ËœK = ËœK + 1\n6\nfor k = 1 to ËœK do\n7\nCompute the prototype representation\nck = P\nzi=k hi\n8 Set K = ËœK\n9 return C, Z, and K\nC\nAdditional Experimental Results\nC.1\nStudy on Prototype Inference.\nIn the proposed semantic contrastive learning, we propose\na Bayesian non-parametric algorithm to infer prototypes of\nnode representations. During the training, a new prototype\nwill be instantiated if the distance between all existing proto-\ntype representations and the representation of a node is larger\nthan a threshold Î².\nDatasets\nCora\nCiteseer\nPubmed\nÎ²\n0.20\n0.15\n0.35\nEstimated K\n8\n6\n3\nClasses\n7\n6\n3\nBaseline\n84.8Â±0.6\n74.6Â±0.3\n80.7Â±0.2\nS3-CL\n84.5Â±0.4\n74.6Â±0.4\n80.8Â±0.3\nTable 5: Ablation study on prototype inference.\nTo verify the effectiveness of the prototype inference algo-\nrithm, we design a baseline model that adopts K-Means clus-\ntering algorithm to infer prototypes as in (Li et al. 2021). For\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nÎ³\n82\n83\n84\n85\nAccuracy (%)\nS3-CL\n(a) Cora\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nÎ³\n72\n73\n74\n75\nAccuracy (%)\nS3-CL\n(b) Citeseer\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nÎ³\n78\n79\n80\n81\nAccuracy (%)\nS3-CL\n(c) Pubmed\nFigure 5: Effect analysis on the value of balancing parameter (Î³).\na fair comparison, label propagation is also applied to reï¬ne\nthe prototypes obtained by the baseline model. The results\nshown in Tabel 5 demonstrate that the proposed Bayesian\nnon-parametric prototype inference algorithm is good enough\nto capture semantic information of the nodes, even without\nthe knowledge on the number of classes.\nC.2\nEffect of Balancing Parameter.\nIn this subsection, we study the effect of the balancing param-\neter Î³ in the ï¬nal loss function on three datasets, including\nCora, Citeseer and Pubmed. Note that we also have similar\nobservations on the other datasets. The results from Figure 5\nshow that S3-CL usually achieve the best results with balanc-\ning parameter Î³ set between 0.4 to 0.6.\nC.3\nEffect of Negative Example Sampling\nWe study the inï¬‚uence of the size of sampled negative ex-\namples in the structural contrastive loss of S3-CL. The re-\nsults are shown in Table 6. We can see that as the number\nof sampled negative examples increase from a small value,\nthe performance of S3-CL can be improved. However, fur-\nther improving the number of sampled negative examples to\nM > 512 does not lead to better performance.\nM\nCora\nCiteseer\nPubmed\n64\n84.0Â±0.2\n74.1Â± 0.3\n80.3Â±0.3\n256\n84.2Â±0.4\n74.4Â± 0.4\n80.3Â±0.2\n512\n84.5Â±0.4\n74.6Â± 0.4\n80.8Â±0.3\n1024\n84.6Â±0.7\n74.6Â± 0.6\n80.7Â±0.6\n2048\n84.6Â±0.5\n74.5Â± 0.2\n80.8Â±0.5\nTable 6: Effect analysis on the value of balancing parameter\n(Î³) in the overall loss function.\nC.4\nEfï¬ciency Analysis.\nIn this subsection, we evaluate the model efï¬ciency of differ-\nent methods in terms of training time, memory consumption,\nand parameter size. We ï¬rst compare S3-CL with existing\nunsupervised GCL methods and show the results in Table 7.\nThough the baseline methods only use shallow GNNs that\ncannot capture global structural knowledge, S3-CL still has\nless memory consumption and smaller parameter size, and\nalso is competitive with the most efï¬cient baselines DGI and\nSUGRL in terms of training time per epoch. The main reason\nis that our approach uses multi-scale feature propagation to\ncapture the global structural knowledge, which saves the aug-\nmentation time during the training and allows the network\nencoder to be built with one-layer neural network.\nMoreover, for each of the existing unsupervised GCL meth-\nods, we increase the encoder network depth to 10 and show\ntheir efï¬ciency evaluation results in Table 8. It is noteworthy\nthat by increasing the depth of the GNN encoder, existing\nmethods have to introduce more model parameters to capture\nlong-range node interactions, leading to slower training speed\nand larger memory consumption. In this case, our proposed\nframework S3-CL has the best efï¬ciency for capturing the\nsame scale of global structural knowledge.\nC.5\nEffect Analysis on temperature parameters.\nIn this subsection, we examine the effect of the temperature\nparameters Ï„1 and Ï„2. Speciï¬cally, we vary the value between\n{0.2, 0.4, 0.6, 0.8, 1} for each temperature parameter, and\nreport the results in Table 9 and Table 10. It is observed\nthat S3-CL generally performs better when the temperature\nparameters are close to 1.\nD\nAdditional Related Work\nDeep Clustering. Clustering-based unsupervised representa-\ntion learning is of particular interest recently. Various meth-\nods are proposed to jointly optimize feature learning and im-\nage clustering. Deep Embedding Clustering (DEC) (Huang\net al. 2014) learns a mapping from the data space to a lower-\ndimensional feature space, in which, it iteratively optimizes\na clustering objective. DeepCluster (Caron et al. 2018) uses\nk-means to assign pseudo-labels to learn visual represen-\ntations. This method scales to large uncurated dataset and\ncan be used for pre-training of supervised networks. Follow-\ning that, DeeperCluster (Caron et al. 2019) is proposed to\ncombine the objective of self-supervised learning and clus-\ntering. SwAV (Caron et al. 2020) further improves this idea\nby simultaneously clustering the data while enforcing con-\nsistency between cluster assignments produced for different\nviews. SCL (Huang and Gong 2021) also seeks to impose\nthe semantic information into the unlabelled training data\nthrough a clustering objective. Recently, a similar work that\nlearns global semantic clustering information is also proposed\nfor graph classiï¬cation (Xu et al. 2021). However, previous\nclustering-based self-supervised learning methods adopt a\npreset number of clusters. In contrast, we propose a Bayesian\nMethods\nCora\nCiteseer\nPubmed\nTime (s)\nMemory (MB)\nParams\nTime (s)\nMemory (MB)\nParams\nTime (s)\nMemory (MB)\nParams\nDGI\n4.2\n3817\n7.3Ã—105\n3.5\n3950\n1.9Ã—106\n8.4\n3750\n2.6Ã—105\nGMI\n6.2\n4155\n9.9Ã—105\n4.7\n4306\n2.2Ã—106\n10.2\n4028\n5.2Ã—105\nMVGRL\n7.6\n2301\n9.9Ã—105\n4.4\n2608\n2.2Ã—106\n13.6\n2430\n5.2Ã—105\nMERIT\n9.2\n1801\n7.3Ã—105\n6.5\n1825\n1.9Ã—106\n15.2\n1723\n2.6Ã—105\nSUGRL\n4.2\n1610\n9.7Ã—105\n3.6\n1754\n2.6Ã—106\n10.3\n1740\n3.9Ã—105\nS3-CL\n5.2\n1405\n7.3Ã—105\n3.9\n1660\n1.9Ã—106\n12.3\n1580\n2.6Ã—105\nTable 7: Efï¬ciency comparisons between S3-CL and baseline methods w.r.t. training time (seconds/epoch), memory occupation\n(MB) and number of parameters.\nMethods\nCora\nCiteseer\nPubmed\nTime (s)\nMemory (MB)\nParams\nTime (s)\nMemory (MB)\nParams\nTime (s)\nMemory (MB)\nParams\nDGI\n5.9\n3967\n3.1Ã—106\n4.5\n4071\n3.3Ã—106\n9.9\n3874\n2.6Ã—106\nGMI\n7.3\n4275\n3.4Ã—106\n5.9\n4430\n4.6Ã—106\n12.0\n4120\n2.9Ã—106\nMVGRL\n9.2\n2507\n3.4Ã—106\n6.5\n2908\n4.6Ã—106\n14.8\n2750\n2.9Ã—106\nMERIT\n11.0\n2105\n3.1Ã—106\n7.8\n2023\n3.3Ã—106\n16.3\n2010\n2.6Ã—106\nSUGRL\n5.5\n1820\n1.2Ã—106\n4.7\n1904\n2.8Ã—106\n11.9\n1867\n5.4Ã—105\nS3-CL\n5.2\n1405\n7.3Ã—105\n3.9\n1660\n1.9Ã—106\n12.3\n1580\n2.6Ã—105\nTable 8: Efï¬ciency comparisons between S3-CL and baseline methods (10-layer encoder network) w.r.t. training time (sec-\nonds/epoch), memory occupation (MB) and number of parameters.\nÏ„1\nCora\nCiteseer\nPubmed\n1\n84.5Â±0.4\n74.6Â± 0.4\n80.8Â±0.3\n0.8\n84.4Â±0.4\n74.6Â± 0.4\n80.7Â±0.2\n0.6\n84.5Â±0.4\n74.3Â± 0.6\n80.6Â±0.4\n0.4\n84.4Â±0.6\n74.6Â± 0.3\n80.5Â±0.6\n0.2\n84.1Â±0.5\n74.4Â± 0.3\n80.4Â±0.5\nTable 9: Effect analysis on the value of Ï„1.\nÏ„1\nCora\nCiteseer\nPubmed\n1\n84.5Â±0.4\n74.6Â± 0.4\n80.8Â±0.3\n0.8\n84.3Â±0.5\n74.6Â± 0.4\n80.7Â±0.3\n0.6\n84.4Â±0.3\n74.3Â± 0.5\n80.8Â±0.4\n0.4\n84.2Â±0.6\n74.6Â± 0.4\n80.3Â±0.7\n0.2\n84.2Â±0.5\n74.3Â± 0.6\n80.2Â±0.6\nTable 10: Effect analysis on the value of Ï„2.\nnon-parametric prototype inference algorithm in our seman-\ntic contrastive learning to learn the number of clusters and\nthe representation of cluster prototypes at the same time.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-02-17",
  "updated": "2022-12-04"
}