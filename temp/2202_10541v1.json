{
  "id": "http://arxiv.org/abs/2202.10541v1",
  "title": "Online Learning for Orchestration of Inference in Multi-User End-Edge-Cloud Networks",
  "authors": [
    "Sina Shahhosseini",
    "Dongjoo Seo",
    "Anil Kanduri",
    "Tianyi Hu",
    "Sung-soo Lim",
    "Bryan Donyanavard",
    "Amir M. Rahmani",
    "Nikil Dutt"
  ],
  "abstract": "Deep-learning-based intelligent services have become prevalent in\ncyber-physical applications including smart cities and health-care. Deploying\ndeep-learning-based intelligence near the end-user enhances privacy protection,\nresponsiveness, and reliability. Resource-constrained end-devices must be\ncarefully managed in order to meet the latency and energy requirements of\ncomputationally-intensive deep learning services. Collaborative end-edge-cloud\ncomputing for deep learning provides a range of performance and efficiency that\ncan address application requirements through computation offloading. The\ndecision to offload computation is a communication-computation co-optimization\nproblem that varies with both system parameters (e.g., network condition) and\nworkload characteristics (e.g., inputs). On the other hand, deep learning model\noptimization provides another source of tradeoff between latency and model\naccuracy. An end-to-end decision-making solution that considers such\ncomputation-communication problem is required to synergistically find the\noptimal offloading policy and model for deep learning services. To this end, we\npropose a reinforcement-learning-based computation offloading solution that\nlearns optimal offloading policy considering deep learning model selection\ntechniques to minimize response time while providing sufficient accuracy. We\ndemonstrate the effectiveness of our solution for edge devices in an\nend-edge-cloud system and evaluate with a real-setup implementation using\nmultiple AWS and ARM core configurations. Our solution provides 35% speedup in\nthe average response time compared to the state-of-the-art with less than 0.9%\naccuracy reduction, demonstrating the promise of our online learning framework\nfor orchestrating DL inference in end-edge-cloud systems.",
  "text": "Online Learning for Orchestration of Inference in\nMulti-User End-Edge-Cloud Networks\nSINA SHAHHOSSEINI, University of California, Irvine, USA\nDONGJOO SEO, University of California, Irvine, USA\nANIL KANDURI, University of Turku, Finland\nTIANYI HU, University of California, Irvine, USA\nSUNG-SOO LIM, Kookmin University, South Korea\nBRYAN DONYANAVARD, San Diego State University, USA\nAMIR M. RAHMANI, University of California, Irvine, USA\nNIKIL DUTT, University of California, Irvine, USA\nDeep-learning-based intelligent services have become prevalent in cyber-physical applications including\nsmart cities and health-care. Deploying deep-learning-based intelligence near the end-user enhances privacy\nprotection, responsiveness, and reliability. Resource-constrained end-devices must be carefully managed\nin order to meet the latency and energy requirements of computationally-intensive deep learning services.\nCollaborative end-edge-cloud computing for deep learning provides a range of performance and efficiency that\ncan address application requirements through computation offloading. The decision to offload computation is a\ncommunication-computation co-optimization problem that varies with both system parameters (e.g., network\ncondition) and workload characteristics (e.g., inputs). On the other hand, deep learning model optimization\nprovides another source of tradeoff between latency and model accuracy. An end-to-end decision-making\nsolution that considers such computation-communication problem is required to synergistically find the\noptimal offloading policy and model for deep learning services. To this end, we propose a reinforcement-\nlearning-based computation offloading solution that learns optimal offloading policy considering deep learning\nmodel selection techniques to minimize response time while providing sufficient accuracy. We demonstrate\nthe effectiveness of our solution for edge devices in an end-edge-cloud system and evaluate with a real-setup\nimplementation using multiple AWS and ARM core configurations. Our solution provides 35% speedup in the\naverage response time compared to the state-of-the-art with less than 0.9% accuracy reduction, demonstrating\nthe promise of our online learning framework for orchestrating DL inference in end-edge-cloud systems.\nAdditional Key Words and Phrases: Edge Computing, Online Learning, Computation Offloading, Neural\nNetwork\n1\nINTRODUCTION\nDeep-learning (DL) is advancing real-time and interactive user services in domains such as au-\ntonomous vehicles, natural language processing, healthcare, and smart cities [35]. Due to user\ndevice resource constraints, deep learning kernels are often deployed on cloud infrastructure to\nmeet computational demands [3]. However, unpredictable network constraints including signal\nstrength and delays affect real-time cloud services [18]. Edge computing has emerged to comple-\nment cloud services, bringing compute capacity closer to the user-end devices [47]. A collaborative\nend-edge-cloud architecture is essential to provide deep-learning-based services with acceptable\nlatency to user-end devices [30]. The edge paradigm increases offloading opportunities for resource-\nconstrained user-end devices. Offloading DL services in a 3-tier end-edge-cloud architecture is a\ncomplex optimization problem considering: (i) diversity in system parameters including heteroge-\nneous computing resources, network constraints, and application characteristics, and (ii) dynamicity\nof DL service environment including workload arrival rate, user traffic, and multi-dimensional\nperformance requirements (e.g., application accuracy, response time) [11, 37, 38].\nExisting offloading strategies for DL tasks are based on the assumptions that (i) all DL tasks have\nsimilar compute intensity and require similar communication bandwidth, (ii) offloading improves\n, Vol. 1, No. 1, Article . Publication date: February 2022.\narXiv:2202.10541v1  [cs.LG]  21 Feb 2022\nperformance, and (iii) latency is guaranteed with offloaded tasks. However, these assumptions do\nnot hold in practice due to dynamically varying application and network characteristics, where the\ncomputation-communication and accuracy-performance tradeoffs are inconsistent and nontrivial\n[11, 38, 42]. Under varying system dynamics, such offloading strategies limit the gains from using\nthe edge and cloud resources. Further, model optimization techniques such as quantization and\npruning can reduce the computation complexity of DL tasks by sacrificing the model accuracy [9, 40].\nConsidering model optimization techniques in conjunction with offloading provides opportunities to\ninfluence the computation-communication trade-off [41]. This exposes an alternative to offloading\nin resource constrained devices executing DL inference. Finding the optimal choice between\noffloading the DL tasks to edge and cloud layers and using optimized models for inference at local\ndevices results in a high-dimensional optimization problem.\nUnderstanding the underlying system dynamics and intricacies among computation, commu-\nnication, accuracy, and latency is necessary to orchestrate the DL services on multi-level edge\narchitectures. Reinforcement learning is an effective approach to develop such an understanding\nand interpret the varying dynamics of such systems [29, 32]. Reinforcement learning allows a\nsystem to identify complex dynamics between influential system parameters and make a decision\nonline to optimize objectives such as response time [39]. We propose to employ online reinforce-\nment learning to orchestrate DL services for multi-users over the end-edge-cloud system. Our\ncontributions are:\nâ€¢ Runtime orchestration scheme for DL inference services on multi-user end-edge-cloud net-\nworks. The orchestrator uses reinforcement learning to perform platform offloading and DL\nmodel selection at runtime to optimize response time provided accuracy requirements.\nâ€¢ Implementation of our online learning solution on a real end-edge-cloud test-bed and demon-\nstration of its effectiveness in comparison with state-of-the-art [36] edge orchestration\nstrategies.\n2\nBACKGROUND\nIn this section, we present the relevant background and significance of orchestrating DL workloads\non end-edge-cloud architecture.\n2.1\nOffloading DL Workloads in End-Edge-Cloud Architecture\nComputation offloading techniques offload an application (or a task within an application) to\nan external device such as cloud servers [25]. Offloading is typically done in order to improve\nperformance or efficiency of devices [3]. DL workloads on end-devices are conventionally offloaded\nto cloud servers, but delay-sensitive services for distributed systems rely on performing inference\nat the edge as an alternative [47]. Inference at the edge can provide cloud-like compute capability\ncloser to the user devices, reducing data transmission and network traffic load. Edge offloading\ncan provide relatively predictable and reliable performance compared to cloud offloading, as there\nis less workload and network variance [18] [16]. In the context of the end-edge-cloud paradigm,\ncomputation offloading techniques partition workloads and distribute tasks among multiple layers\n(local device, edge device, cloud servers) such that the performance and efficiency objectives are\nmet.\nThe collaborative end-edge-cloud architecture provides execution choices such that each work-\nload can be executed on the device, on the edge, on the cloud, or a combination of these layers.\nEach execution choice effects the performance and energy consumption of the user end device,\nbased on the system parameters such as hardware capabilities, network conditions, and workload\ncharacteristics. A distributed end-edge-cloud system consists of the following layers:\n2\nâ€¢ application layer: provides user level access to a set of services to be delivered by computing\nnodes\nâ€¢ platform layer: provides a set of capabilities to connect, monitor and control end/edge/cloud\nnodes in a network\nâ€¢ network layer: provides connectivity for data and control transfer between different physical\ndevices across multiple\nâ€¢ hardware layer: provides hardware capabilities for computing nodes in the system\nEach layer presents a diverse set of requirements, constraints, and opportunities to tradeoff per-\nformance and efficiency that vary over time. For example, the application layer focuses on the\nuserâ€™s perception of algorithmic correctness of services, while the platform layer focuses on im-\nproving system parameters such as energy drain and data volume migrated across nodes. Both\napplication and platform layers have different measurable metrics and controllable parameters to\nexpose different opportunities that can be exploited for meeting overall objectives. In the case of DL\ninference, different DL model structures present opportunities in the application layer, and different\ncomputation offloading decisions in a collaborative end-edge-cloud system present opportunities\nin the platform layer, both for optimizing the execution while meeting required model accuracy.\n2.2\nIntelligence for Orchestration\nRuntime system dynamics affect orchestration strategies significantly in addition to requirements\nand opportunities. Sources of runtime variation across the system stack include workload of a spe-\ncific computing node, connectivity and signal strength of the network, mobility and interaction of a\ngiven user, etc. Considering cross-layer requirements, opportunities, and runtime variations provide\nnecessary feedback to make appropriate choices on system configurations such offloading policies.\nIdentifying optimal orchestration considering the cross-layer opportunities and requirements in\nthe face of varying system dynamics is a challenging problem. Making the optimal orchestration\nchoice considering these varying dynamics is an NP-hard problem, while brute force search of a\nlarge configuration space is impractical for real-time applications. Understanding the requirements\nat each level of the system stack and translating them into measurable metrics enables appropriate\norchestration decision making. Heuristic, rule-based, and closed-loop feedback control solutions are\nnot efficient until reaching convergence, which requires long periods of time [39]. To address these\nlimitations, reinforcement learning approaches have been adapted for the computation offloading\nproblem [36]. Reinforcement learning builds specific models based on data collected over initial\nepochs, and dramatically improves the prediction accuracy [39].\n3\nMOTIVATION\nThis section presents a comprehensive investigation of DL inference for multi-users in end-edge-\ncloud systems. We examine the scenario using a real setup including five AWS a1.medium instances\nwith single ARM-core as end-node devices connected to an AWS a1.large instance as edge device\nand an AWS a1.xlarge instance as cloud node. We conduct experiments for DL inferences with\nthe MobileNetV1 model while varying (i) network connection, (ii) number of active users, and\n(iii) accuracy requirement. We consider three possible execution choices: (i) on device, (ii) on\nedge, and (iii) on cloud. The device, edge, and cloud execution choices represent executing the\ninference completely on the local device, on the edge, and on the cloud respectively. The detailed\nspecifications for the end-edge-cloud setup appear in Section 5.3.\n3\nFig. 1. Impact of varying system and application dynamics on performance for MobileNet application. (a)\nResponse time on user-end device, edge and cloud layers with regular and weak network conditions. (b)\nAverage response time with varying number of active users for different computing schemes. (c) Average\nresponse time achieved with varying levels of average accuracy.\n3.1\nImpact of System Dynamics on Inference Performance\nNetwork. We consider two possible levels of network connections: (i) a low-latency (regular)\nnetwork that has the signal strength for better connectivity, and (ii) a high-latency (weak) network\nthat has a weaker signal with poor connectivity. Figure 1 (a) shows the response time of MobileNet\napplication on user device, edge, and cloud layers with regular and weak networks. With a regular\nnetwork, the response time is highest for executing the application on the user end device. The\nresponse time decreases as the computation is offloaded to edge and cloud layers, with the higher\ncomputational resources. With a weak network, the response time of the edge and cloud layers is\nhigher, as the poor signal strength adds delay. The response time of the edge node in this case is\nhigher than the cloud layer, given the lower compute capacity of the edge node. Performance of the\nuser end device is independent of the network connection, resulting in lowest response time. This\ndemonstrates the spectrum of response times achievable with compute nodes at different layers,\nunder varying network constraints. For example, the best execution choice with a regular network\nis the cloud layer, whereas it is the local execution with a weak network.\nUsers. We examine user variability by considering multiple simultaneously active users ranging\nfrom 1 to 5. Figure 1 (b) shows the average response time with varying number of users. The\naverage response time remains constant when running the application on a user end device, i.e.,\neach user executes the application on their local device. When offloaded to the edge layer, the\naverage response time increases significantly as the number of users increase. This is attributed to\nthe increased network load with multiple simultaneously active users as well as limited resources\nat the edge layer to handle several user requests concurrently. The average response time also\nincreases when offloaded to the cloud layer as the number of simultaneous users increases. However,\nthe response time is lower when compared to the edge layer, since the cloud layer has a larger\nvolume of resources to handle multiple simultaneous user requests.\nAccuracy. We demonstrate the impact of varying DL models on performance under different\nsystem dynamics. We select between eight models with Top-5 accuracy between %72.8 and %89.9,\nwhile also considering all three layers for execution, and between 1 and 5 simultaneously active\nusers. Figure 1 (c) shows the average response time achieved with varying levels of average accuracy\n4\nTable 1. Reinforcement Learning Based Works. CO represents the computation offloading technique. HW\nand APP represents knobs belong to the hardware and application layer, respectively.\nRelated Works\nReal System\nEvaluation\nMulti-User\nEnd-to-End\nActions\n[6, 8, 27, 33, 45]\nâœ—\nâœ—\nâœ—\nCO\n[19]\nâœ“\nâœ—\nâœ—\nCO,HW\n[1, 7, 17, 20, 24, 36, 43]\nâœ—\nâœ“\nâœ—\nCO\nOurs\nâœ“\nâœ“\nâœ“\nCO,APP\nover a multi-dimensional space of different execution choice and different number of users. Each\npoint in Figure 1 (c) represents a unique case of an execution choice (among device, edge, and\ncloud), number of active users (among 1 to 5), and accuracy level. We present the average response\ntime achieved with different levels of accuracy. As expected, the response time increases with\nincrease in model accuracy. However, we observe tradeoffs among different response times between\naccuracy and number of active users. For instance, it is possible to support multiple users within\nthe response time of servicing a single user, by lowering the model accuracy.\nConsidering the three major sources of variations in number of users, network conditions, and\nmodel accuracy, finding an optimal choice of execution for end-edge-cloud architectures at runtime\nis challenging. As such architectures scale in the number of users and edge nodes, the accuracy-\nperformance Pareto-space becomes increasingly cumbersome for finding an optimal configuration\namong the fine-grained choices. Brute force and smart search algorithms do not offer practically\nfeasible solutions to orchestrate applications in real-time. While machine learning algorithms\ncan identify near-optimal configuration choices, they require exhaustive training, considering\ncontinuously varying system dynamics. We propose to employ online reinforcement learning to\nunderstand the volatility of system dynamics and make near-optimal orchestration decisions in\nreal-time to improve the response time of DL inferencing on end-edge-cloud architectures.\n3.2\nRelated Work\nWe categorize research related to optimally deploying DL services at the edge in two ways: (i) work\nrelated to deploying DL inference tasks over the end-edge-cloud collaborative architecture, and (ii)\nwork related to adopting reinforcement learning methods to optimally offload tasks.\nDL Inference in End-edge-cloud Networks. Prior works propose frameworks to decompose DL\ninference into tasks and perform distributed computations. In these works, a DL model can be\npartitioned vertically or horizontally along the end-edge-cloud architecture. Generally, DL models\nare partitioned according to the compute cost of model layers and required bandwidth for each layer\nto be distributed among the end-edge-cloud [15, 16, 34, 46]. These works find the optimal partition\npoints based on traditional optimization techniques and offer design-time optimal solutions. Some\nefforts try to reduce the computation overhead of DL tasks through various model optimization\nmethods such as quantization. These methods transform or re-design models to fit them into\nresource-constrained edge devices with little loss in accuracy [10, 12, 26]. AdaDeep [22] proposes\na Deep Reinforcement Learning method to optimally select from a pool of compressed models\naccording to available resources. However, AdaDeep relies only on the model selection technique\nwhile our work combines computation offloading and model selection techniques to achieve the\noptimal response time.\nLearning-based Offloading. Prior works address the offloading problem to optimize different\nobjectives including latency and energy consumption. Most of the works formulate the offloading\n5\nproblem with limited number of influential parameters and adopt online learning techniques\nwith numerical evaluation [1, 6â€“8, 17, 20, 27, 33, 43, 45]. Lu et. al. [24] propose a Deep Recurrent\nQ-Learning algorithm based on Long Short Term Memory network to minimize the latency for\nmulti-service nodes in large-scale heterogeneous MEC and multi-dependence in mobile tasks. The\nalgorithm is evaluated in iFogSim simulator with Google Cluster Trace. [36] proposes a Q-Learning\nbased algorithm to minimize energy by considering various parameters in task characteristics and\nresource availability. Young Geun et al. [19] propose a reinforcement learning based offloading\ntechnique for energy efficient deep learning inference in the edge-cloud architecture. The work\nfocuses on the learning for heterogeneous systems and lacks a comprehensive solution for multi-\nusers end-edge-cloud systems. Haung et al. [14] uses a supervised learning algorithm for complicated\nradio situations and communication analysis and prediction to make an optimal actions to obtain\na high quality of service (QoS). However, our proposed work employs model-free reinforcement\nlearning algorithm to find the optimal orchestration scheme. There have been other efforts which\napply game theory algorithms to address the orchestration problem in the network. Apostolopoulos\net al. [2] propose a decentralized risk-aware data offloading framework using non-cooperative game\nformulation. The work uses a model-based game theory algorithm to find the optimal offloading\ndecision which makes difference with our proposed model-free reinforcement learning approach.\nModel-based algorithm use a predictive internal model of the system to seek outcomes while\navoiding the consequence of trial-and-error in real-time. The approach is sensitive to model bias\nand suffers from model errors between the predicted and actual behavior leading to sub-optimal\norchestration decisions. Our Model-free RL technique operates with no assumptions about the\nsystemâ€™s dynamic or consequences of actions required to learn a policy.\nSome works have been applied traditional optimization techniques to optimize the computation\noffloading problem [5]. Yuan et al. [48] model a profit-maximized collaborative computation of-\nfloading and resource allocation algorithm to maximize the profit of systems and meet the required\nresponse time. In another work, Bi et al. [4] propose a partial computation offloading method to\nminimize the total energy consumed by mobile devices. The work formulates the problem and\noptimizes using a novel hybrid meta-heuristic algorithm. Considering systems are unknown with\ndynamic behavior, the traditional optimization techniques are not applicable for runtime decision-\nmaking. Table 1 positions our work with respect to state-of-the-art solutions. Our solution uses RL\nto optimally orchestrate DL inference in multi-user networks considering offloading and DL model\nselection techniques combined together.\n3.3\nContributions\nThe ideal DL inference deployment provides maximum inference accuracy and minimum response\ntime. Figure 2 shows an abstract overview of our target multi-layered architecture for online\ncomputation offloading of DL services. We consider three layers viz., user-end device, edge and\ncloud. Further, we classify this architecture into virtual system layers that include application,\nplatform, network and hardware layers. Each of the virtual system layers provide sensory inputs for\nmonitoring system and application dynamics such as DL model parameters, accuracy requirements,\navailability of devices for execution, network characteristics, and hardware capabilities. The Decision\nIntelligence component in Figure 2 periodically monitors resource availability from all virtual\nsystem layers to determine appropriate execution choice and DL models to achieve the required\nQoS (e.g, accuracy, response time). Decision Intelligence analyzes the system parameters to make\norchestration decisions in terms of model selection, accuracy configuration, and offloading choices.\nThe orchestrator is a software component that is hosted at the cloud layer and enforces the\norchestration decisions upon receiving a service request from the user-end devices.\n6\nPlatform Layer\nNetwork Layer\nApplication Layer\nHardware Layer\nDecision \nIntelligence\nEnd-edge-cloud System \nIntelligent Orchestration\nEnd\nEdge \nCloud \nService Request\nService Request\nOrchestrator\nModel parameters \nand accuracy\nAvailability\nNetwork \nCondition\nHardware \nCapabilities\nModel selection\nDevice selection\nAccuracy config.\nVirtual System Layers\nFig. 2. Intelligent orchestration of DL inference in end-edge-cloud architectures.\nFinding an optimal computation policy including offloading and model selection to optimize\nobjectives (e.g., accuracy, response time) is considered an NP-hard problem. The problem generally\ncan be solved using traditional optimization techniques such as heuristic-based methods, meta-\nheuristic methods, or exact solutions. Considering systems are unknown with dynamic behavior,\nthe traditional optimization techniques are not applicable for runtime decision-making to optimize\nobjectives. Modeling an unexplored high-dimensional system is feasible using model-free reinforce-\nment learning techniques [39]. Model-free RL operates with no assumptions about the systemâ€™s\ndynamic or consequences of actions required to learn a policy. Model-free RL builds the policy\nmodel based on data collected through trial-and-error learning over epochs [39]. In this work, we\nuse model-free reinforcement learning to deploy DL inference at the edge by considering offload-\ning and model selection. Some works have been proposed to address the computation offloading\nproblem using online techniques [1, 6â€“8, 17, 19, 27, 33, 43, 45]. However, there is no relevant work\nto investigate the integration of online learning with DL inference deployment. Therefore, the\nliterature suffers from some shortcomings that are summarized as follows:\nâ€¢ Cross-layer Optimization: online solutions have not previously coordinated offloading and\nmodel optimizations together. As Table 1 shows, all related work relies on only computation\noffloading (CO). To the best of our knowledge, for the first time, this paper considers both\ncomputation offloading and application-level adjustment (APP) together in order to achieve\nrequired QoS.\nâ€¢ Real System Evaluation: most RL-based solutions in the literature are numerically evaluated.\nSome works have been proposed and evaluated with simulators. As Table 1 shows, the\nliterature lacks a real hardware implementation for online learning framework. This paper\nimplements the online system on real hardware devices which leads to realistic evaluation of\nonline agentâ€™s overhead.\nâ€¢ End-to-End Solution: end-to-end solution considers a service from the moment a request\nis issued from the end-node device to delivering results to itself. Table 1 illustrates that the\nliterature lacks an end-to-end solution.\n7\nTable 2. Notation descriptions\nNotation\nDescription\nğ‘†\nend-node device\nğ¸\nedge device\nğ¶\ncloud device\nğ‘ƒ\nprocessor utilization\nğ‘€\nmemory utilization\nğµ\nnetwork condition\nğ‘œ\noffloading decision\nğ‘œğ‘—\nğ‘–\noffloading decision for end-node ğ‘–to resource ğ‘—\nğ‘\nnumber of end-node devices\nğ‘‘ğ‘˜\nDL model ğ‘˜\nğ‘™\nnumber of available DL models\nğ‘‡ğ‘—\nğ‘Ÿğ‘’ğ‘ \nresponse time for offloading DL task to resource ğ‘—\nğ›¼\nlearning rate\nğ›¾\ndiscount factor\n4\nONLINE LEARNING FRAMEWORK\nOur goal is to make offloading decisions and inference model selections in order to minimize\ninference latency while achieving acceptable accuracy. To do so, we first define the optimization\nproblem, then we propose a reinforcement learning agent to solve the problem. Table 2 defines the\nnotation used for the problem definition.\n4.1\nSystem Model and Problem Formulation\nAll computing devices in the end-edge-cloud system are represented by (S,E,C) whereğ‘†= {ğ‘†1,ğ‘†2, ..,ğ‘†ğ‘›}\nrepresents a set of end-node devices whose number is ğ‘; ğ¸represents the edge layer (in our case,\na single device); C represents the cloud layer. Each end-node device requires a DL inference pe-\nriodically. The inference model is selected from a pool of optimized models where each model\nhas different characteristics including computational complexity and model accuracy. All device\nresources are represented in a tuple {ğ‘ƒğ‘–, ğ‘€ğ‘–, ğµğ‘–} where ğ‘ƒğ‘–represents processor utilization of device ğ‘–;\nğ‘€ğ‘–represents available memory for device ğ‘–; ğµğ‘–represents networkâ€™s connection condition between\nthe device ğ‘–and upper layerâ€™s node.\nThe computation offloading decision determines whether each end-node device should offload an\ninference to higher-layer computing resources, or perform computation locally. The offload decision\nfor each end-node device is represented by a tuple ğ‘œğ‘–= {ğ‘œğ‘†\nğ‘–,ğ‘œğ¸\nğ‘–,ğ‘œğ¶\nğ‘–} where ğ‘œğ‘—\nğ‘–represents offloading\ndecision to layer ğ‘—. If end-node device ğ‘–executes at layer ğ‘—âˆˆ{ğ‘†, ğ¸,ğ¶}, then ğ‘œğ‘—\nğ‘–= 1; otherwise it\nmust be zero. For a given end-node device ğ‘–, the sum of all offloading decisions P{ğ‘†,ğ¸,ğ¶}\nğ‘—\nğ‘œğ‘—\nğ‘–must\nequal 1. ğ‘œ= {ğ‘œ1,ğ‘œ2, ...,ğ‘œğ‘›} represents the offloading decision vector for all end-node devices. The\ninference model selection determines the implementation of the model deployed for each inference\non each end-node device. Each end-node device ğ‘†ğ‘–can perform inference with one of ğ‘™DL models\n{ğ‘‘1,ğ‘‘2,ğ‘‘3, ...,ğ‘‘ğ‘™}.\nIn general, response time is the total time between making a request to a service and receiving\nthe result [31]. In our case, response time is the sum of the round trip transmission time from an\nend-node device to the node that performs the computation, plus the computation time. Response\ntime ğ‘‡ğ‘Ÿğ‘’ğ‘ for a request from end-node device ğ‘–with offload decision tuple ğ‘œğ‘–= {ğ‘œğ‘†\nğ‘–,ğ‘œğ¸\nğ‘–,ğ‘œğ¶\nğ‘–} can be\nsummarized as follows:\nğ‘‡ğ‘Ÿğ‘’ğ‘ ğ‘–= ğ‘œğ‘†\nğ‘–.ğ‘‡ğ‘†\nğ‘Ÿğ‘’ğ‘ + ğ‘œğ¸\nğ‘–.ğ‘‡ğ¸\nğ‘Ÿğ‘’ğ‘ + ğ‘œğ¶\nğ‘–.ğ‘‡ğ¶\nğ‘Ÿğ‘’ğ‘ \n(1)\n8\nState\n(CPU Utilization, Memory \nUtilization, Network \nCondition)\nAction\nQ-Value 0\nQ-Value 1\nQ-Value 2\nQ-Value N\nA1\nA2\nâ€¦\nAN\nS1\nQ0\nQ1\nâ€¦\nQN\nS2\nQ0\nQ1\nâ€¦\nQN\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nSM\nQN\nQN\nQN\nQN\nQ-Table\nQ-Network\nFig. 3. Proposed reinforcement learning agent with Q-Learning and Deep Q-Learning algorithms. Q-Learning\nuses a Q-Table to storeğ‘„(ğ‘†,ğ´) values, Deep Q-Learning estimates Q-Values with a neural network architecture.\nOur objective is to minimize the average response time while satisfying the average accuracy\nconstraint. The problem is formulated in the following formula:\nP1: min\n1\nğ‘\nğ‘\nâˆ‘ï¸\nğ‘–=1\nğ‘‡ğ‘Ÿğ‘’ğ‘ ğ‘–(ğ‘œğ‘–,ğ‘‘ğ‘˜)\ns.t.\nğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦> ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘\n(2)\nwhere ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦is the spatial average accuracy for simultaneous DL inferences.\n4.2\nReinforcement Learning Agent\nReinforcement learning (RL) is widely used to automate intelligent decision making based on\nexperience. Information collected over time is processed to formulate a policy which is based on a\nset of rules. Each rule consists three major components viz., (a) state, (b) action, and (c) reward.\nAmong the various RL algorithms [39], Q-learning has low execution overhead, which makes it a\ngood candidate for runtime invocation. However, it is ineffective for large space problems. There\nare two main problems with Q-learning for large space problems [28]: (a) required memory to save\nand update the Q-Values increases as the number of actions and state increases. (b) required time to\npopulate the table with accurate estimates is impractical for the large Q-table. In our case, increasing\nnumber of users will increase the problemâ€™s space dimension. The reason is more number of users\nleads to more number of rows and columns in the Q-table. Therefore, it takes more time to explore\nevery state and update the Q-values. Due to the curse of dimensionality, function approximation is\nmore appealing [28]. The Deep Q-Learning (DQL) algorithm combines the Q-Learning algorithm\nwith deep neural networks. DQL uses Neural Network architecture to estimate the Q-function by\nreplacing the need for a table to store the Q-values. In this work, we build an RL agent using two\nreinforcement learning algorithms: (a) an epsilon-greedy Q-Learning and (b) a Deep Q-Learning\nalgorithms. We evaluate the RL agent with the mentioned algorithms considering different problem\ncomplexities. Figure 3 depicts high-level black diagram for our agent. The RL agent is invoked at\nruntime for intelligent orchestration decisions. In general, the agent is composed as follows:\n9\nState Space: Our state vector is composed of CPU utilization, available memory, and bandwidth\nper each computing resource. Table 3 shows the discrete values for each component of the state.\nThe state vector at time step ğœis defined as follows:\nğ‘†ğœ= {ğ‘ƒğ¸, ğ‘€ğ¸, ğµğ¸, ğ‘ƒğ¶, ğ‘€ğ¶, ğµğ¶, ğ‘ƒğ‘†1, ğ‘€ğ‘†1, ğµğ‘†1, ..., ğ‘ƒğ‘†ğ‘›, ğ‘€ğ‘†ğ‘›, ğµğ‘†ğ‘›}\n(3)\nAction Space: The action vector consists of which inference model to deploy, and which layer to\nassign the inference. We limit the edge and cloud devices to always use the high accuracy inference\nmodel, and the end-node devices have a choice of ğ‘™different models. Therefore, the action space is\ndefined as ğ‘ğœ= {ğ‘œğ‘–,ğ‘‘ğ‘—} where ğ‘–âˆˆ{ğ‘†, ğ¸,ğ¶} and ğ‘‘ğ‘—âˆˆ{ğ‘‘1,ğ‘‘2, ...,ğ‘‘ğ‘™}.\nTable 3. State Discrete Values\nState\nDiscrete Values\nDescription\nğ‘ƒğ‘†ğ‘–\nAvailable, Busy\nEnd-node CPU Utilization\nğ‘€ğ‘†ğ‘–\nAvailable, Busy\nEnd-node Memory Utilization\nğµğ‘†ğ‘–\nRegular, Weak\nEnd-node Available Bandwidth\nğ‘ƒğ¸\nNine discrete levels\nEdge CPU Utilization\nğ‘€ğ¸\nAvailable, Busy\nEdge Memory Utilization\nğµğ¸\nRegular, Weak\nEdge Available Bandwidth\nğ‘ƒğ¶\nNine discrete levels\nCloud CPU Utilization\nğ‘€ğ¶\nAvailable, Busy\nCloud Memory Utilization\nğµğ¶\nRegular, Weak\nCloud Available Bandwidth\nReward Function: The reward function is defined as the negative average response time of DL\ninference requests. In our case, the agent seeks to minimize the average response time. To ensure\nthe agent minimizes the average response time while satisfying the accuracy constraint, the reward\nğ‘…is calculated as follows:\nif ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦> threshold:\nğ‘…ğœâ†âˆ’ğ´ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’ğ‘…ğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’ğ‘‡ğ‘–ğ‘šğ‘’\nelse:\nğ‘…ğœâ†âˆ’ğ‘€ğ‘ğ‘¥ğ‘–ğ‘šğ‘¢ğ‘šğ‘…ğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’ğ‘‡ğ‘–ğ‘šğ‘’\n(4)\nTo apply the accuracy constraint, the minimum possible reward is assigned when the accuracy\nthreshold is violated. On the other hand, when the selected action satisfies the average accuracy\nconstraint, the reward is negative average response time.\n4.2.1\nQ-Learning Algorithm. Q-Learning algorithm is a model-free reinforcement learning al-\ngorithm to learn the value of an action in a particular state. The algorithm does not require a\nmodel of the environment where it can handle problems with stochastic transitions and rewards\nwithout requiring adaptations. The Q-Learning algorithm stores data in a Q-table. The structure\nof a Q-Learning agent is a table with the states as rows and the actions as the columns. Each cell\nof the Q-table stores a Q-value, which estimates the cumulative immediate and future reward of\nthe associated state-action pair. Epsilon-greedy is a common enhancement to Q-Learning that\nhelps avoid getting stuck at local optima [39]. Algorithm 1 defines our agentâ€™s logic with the\nepsilon-greedy Q-Learning:\nLine\nDescription\n3:\nFirst the agent determines the current system state from the resource monitors.\n10\nAlgorithm 1 Q-Learning Algorithm\n1: Initialization in design time:\nğœrepresents time step\nğ‘†ğœrepresents state at ğœ\nğ´ğœrepresents action at ğœ\n2: while system is on do\n3:\nFrom Resource Monitoring:\nğ‘†ğœâ†State at step ğœ\n4:\nif ğ‘…ğ´ğ‘ğ·< ğœ–then\n5:\nChoose random action ğ´ğœ\n6:\nelse\n7:\nChoose action ğ´ğœwith largest ğ‘„(ğ‘†ğœ,ğ´ğœ)\n8:\nend if\n9:\nMonitor the response time for each devices\n10:\nCalculate reward ğ‘…ğœ\n11:\nFrom Resource Monitoring:\nğ‘†ğœ+1 â†State at step ğœ+ 1\n12:\nChoose action ğ´ğœ+1 with the largest ğ‘„(ğ‘†ğœ+1,ğ´ğœ+1)\n13:\nTo Updating Qtable:\nğ‘„(ğ‘†ğœ,ğ´ğœ) â†ğ‘„(ğ‘†ğœ,ğ´ğœ) + ğ›¼[ğ‘…ğœ+ ğ›¾.ğ‘„(ğ‘†ğœ+1,ğ´ğœ+1) âˆ’ğ‘„(ğ‘†ğœ,ğ´ğœ)]\n14:\nğ‘†ğœâ†ğ‘†ğœ+1\n15: end while\n4-8:\nNext, either the state-action pair (ğ‘†ğœ,ğ´ğœ) with the highest ğ‘„-value is identified to choose\nthe next action to take, or a random action is selected with probability ğœ–.\n9-10:\nThe selected action is applied and normal execution resumes. After all inferences are\ncompleted, the reward ğ‘…ğœfor the execution period is calculated based on measured\nresponse time.\n11-12: Based on the resource monitors, the new state ğ´ğœ+1 is identified, along with the state-\naction pair with highest Q-value.\n13:\nThe Q-value of the previous state-action pair is updated.\n14:\nThe current state is updated, and the loop continues.\n4.2.2\nDeep Q-Learning Algorithm. Q-Learning has been applied to solve many real-world problems.\nHowever, it is unable to solve high-dimensional problems with many inputs and outputs [28] as it\nis impractical to represent the Q-function as a Q-table for large pair of ğ‘†and ğ´. In addition, it is\nunable to transverse ğ‘„(ğ‘†,ğ´) pairs. Therefore, a neural network is used to estimate the Q-values. The\nDeep Q-learning Network (DQN) inputs include current state and possible action, and outputs the\ncorresponding Q-value of the given action input. The neural network approximation is capable of\nhandling high dimensional space problems [44]. One of the main problems with Deep Q-Learning is\nstability [28]. In order to reduce the instability caused by training on correlated sequential data, we\nimprove the DQL algorithm with replay buffer technique [21]. During the training, we calculate the\nloss and its gradient using a mini-batch from the buffer. Every time the agent takes a step (moves\nto the next state after choosing an action), we push a record into the buffer. Algorithm 2 defines\nDeep Q-Learning algorithm which is described below:\nLine\nDescription\n4:\nFirst the agent determines the current system state from the resource monitors.\n11\n4:9\nNext, either the state-action pair ğ‘†ğœ,ğ´ğœwith the highest Q-value estimated by neural\nnetwork (ğœƒ) is identified to choose the next action to take, or a random action is selected\nwith probability ğœ–.\n10:11 The selected action is applied and normal execution resumes. After all inferences are\ncompleted, the reward ğ‘…ğœfor the execution period is calculated based on measured\nresponse time.\n12:\nAt each time step, each record (ğ‘†ğœ,ğ´ğœ,ğ‘…ğœ,ğ‘†ğœ+1) is added to a circular buffer ğ·called the\nreplay buffer.\n13:\nWe randomly sample Batch Size records from the buffer and then feed it to the network\nas mini-batch.\n14:\nWe calculate the temporal difference loss on the mini-batch and perform a gradient\ndescent calculation to update the network. The temporal difference loss function calculates\nthe mean-square error of the predicted and target Q-values as the loss of the mini-batch.\n15:\nThe current state is updated, and the loop continues.\nThere is few study about the theoretical analysis of the computational complexity of reinforce-\nment learning because of the problem itself that reinforcement learning solves is hard to be explicitly\nmodeled. Reinforcement learning is in the nature of trail-and-error and exploration-and-exploitation,\nwhich involves randomness and makes it difficult to be theoretically analyzed.\nThe complexity of problem for Bruteforce strategy is discussed in the following: Brute-force strategy\nsearches the entire StateÃ—Action space of the problem and sort corresponding Q-values in order to\nfind out the optimal action. Therefore, we can define the state space complexity as follows:\n(ğ¿ğ¶ğ‘ƒğ‘ˆÃ— ğ¿ğ‘ğ‘’ğ‘¡ğ‘¤ğ‘œğ‘Ÿğ‘˜Ã— ğ¿ğ‘€ğ‘’ğ‘šğ‘œğ‘Ÿğ‘¦)ğ‘(ğ¿â€²\nğ¶ğ‘ƒğ‘ˆÃ— ğ¿â€²\nğ‘ğ‘’ğ‘¡ğ‘¤ğ‘œğ‘Ÿğ‘˜Ã— ğ¿â€²\nğ‘€ğ‘’ğ‘šğ‘œğ‘Ÿğ‘¦)2\n(5)\nAlgorithm 2 Deep Q-Learning Algorithm with Experience Replay\n1: Initialization in design time:\nğœrepresents time step\nğ‘†ğœrepresents state at ğœ\nğ´ğœrepresents action at ğœ\nInitialize replay buffer ğ·to capacity ğ‘\nInitialize action-value function ğ‘„with random weight ğœƒ\n2: for epoch = 1, Epochs do\n3:\nfor episode = 1, Episodes do\n4:\nFrom Resource Monitoring:\nğ‘†ğœâ†State at step ğœ\n5:\nif ğ‘…ğ´ğ‘ğ·< ğœ–then\n6:\nChoose random action ğ´ğœ\n7:\nelse\n8:\nChoose action ğ´ğœwith largest ğ‘„ğœƒ(ğ‘†ğœ,ğ´ğœ)\n9:\nend if\n10:\nMonitor the response time for each devices\n11:\nCalculate reward ğ‘…ğœ\n12:\nStore the record (ğ‘†ğœ,ğ´ğœ,ğ‘…ğœ,ğ‘†ğœ+1) into buffer D\n13:\nSample random mini-batch of records from buffer D\n14:\nTo Updating Q-Network:\nCompute temporal difference loss with respect to the network parameter ğœƒ\n15:\nğ‘†ğœâ†ğ‘†ğœ+1\n16:\nend for\n17: end for\n12\nIntelligent Service\nResource Monitoring\nIntelligent Service\nResource Monitoring\nIntelligent Service\nResource Monitoring\nIntelligent Service\nResource Monitoring\nIntelligent Service\nResource Monitoring\nResource Monitoring\nIntelligent Service\nResource Monitoring\nQuality of Service Goal\nUser Inputs\nIntelligent Orchestrator\nState\nQ-Table\nOrchestration\n/Actions\nIntelligent Service\nRL Model Training\nRL Model Testing\nResource Info\nReward\nS\nA\nRequest (1)\nRequest (2)\nRequest Orchestration (3)\nDecision (4)\nDecision (4)\nUpdate (5)\nUpdate (5)\nCloud\nEdge\nEnd\nEnd\nEnd\nEnd\nEnd\nTime\nNetwork \nCondition\nFig. 4. Orchestration framework with online learning for orchestrating DL inference.\nwhere, ğ‘stands for the number of end-devices. ğ¿ğ¶ğ‘ƒğ‘ˆ, ğ¿ğ‘€ğ‘’ğ‘šğ‘œğ‘Ÿğ‘¦, and ğ¿ğ‘ğ‘’ğ‘¡ğ‘¤ğ‘œğ‘Ÿğ‘˜represent the number\nof CPU, Memory, and Network condition levels for end devices, respectively. In addition, ğ¿â€²\nğ¶ğ‘ƒğ‘ˆ,\nğ¿â€²\nğ‘€ğ‘’ğ‘šğ‘œğ‘Ÿğ‘¦, and ğ¿â€²\nğ‘ğ‘’ğ‘¡ğ‘¤ğ‘œğ‘Ÿğ‘˜represent the number of CPU, Memory, and Network condition levels for\nedge and cloud devices, respectively. Besides, the action space is defined as (ğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿğ‘œğ‘“ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ )ğ‘.\nTherefore, the complexity is defined as follows:\n(ğ¿ğ¶ğ‘ƒğ‘ˆÃ— ğ¿ğ‘ğ‘’ğ‘¡ğ‘¤ğ‘œğ‘Ÿğ‘˜Ã— ğ¿ğ‘€ğ‘’ğ‘šğ‘œğ‘Ÿğ‘¦)ğ‘Ã— (ğ¿â€²\nğ¶ğ‘ƒğ‘ˆÃ— ğ¿â€²\nğ‘ğ‘’ğ‘¡ğ‘¤ğ‘œğ‘Ÿğ‘˜Ã— ğ¿â€²\nğ‘€ğ‘’ğ‘šğ‘œğ‘Ÿğ‘¦)2 Ã— (ğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿğ‘œğ‘“ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ )ğ‘\n(6)\nThe reinforcement learning agent requires distinct state-action pairs for training the Deep Q-\nnetwork. To generate distinct state-action pair vectors, our proposed framework supports execution\nrequests that are submitted by all the end-devices synchronously. With synchronous requests, we\neliminate the discrepancy of different optimal actions for the same state vector.\n5\nFRAMEWORK SETUP\nIn this section we describe our proposed framework for dynamic computation offloading based on\nonline learning, targeted at multi-layered end-edge-cloud architecture.\n5.1\nFramework Architecture\nFigure 4 shows our proposed framework for end-edge-cloud architecture, integrating service\nrequests, resource monitoring, and intelligent orchestration. The Intelligent Orchestrator (IO) acts as\nan RL-agent for making computation offloading and model selection decisions. The end-device layer\nconsists of multiple user-end devices. Each end-device has two software components: (i) Intelligent\nService - an image classification kernel with DL models of varying compute intensity and prediction\naccuracy; (ii) Resource Monitoring - a periodic service that collects devicesâ€™ system parameters\nincluding CPU and memory utilization, and network condition, and broadcasts the information\nto the edge and cloud layers. Both the edge and cloud layers also have the Intelligent Service and\nResource Monitoring components. The Intelligent Orchestrator acts a centralized RL-agent that is\nhosted at the cloud layer for inference orchestration. The agent collects resource information\n(e.g., processor utilization, available memory, available bandwidth) from Resource Monitoring\ncomponents throughout the network. The agent also gathers the reward information (i.e., response\ntime) from the environment in order to learn an optimal policy. The agent builds the Q-function\nbased on the RL algorithm. It builds a Q-Table for Q-Learning algorithm and a Q-Network for\nDeep Q-Learning algorithm based on cumulative reward obtained from the environment over time.\nQuality of Service Goal provides the required QoS for the system (i.e., the accuracy constraint).\nFigure 4 illustrates the procedure step-wise of the inference service in our framework. The\nend-device layer consists of resource-constrained devices that periodically make requests to a DL\n13\ninference service (step 1). The requests are passed through the edge layer (step 2) to the cloud device\nto be processed by Intelligent Orchestrator (step 3). The agent determines where the computation\nshould be executed, and delivers the Decision to the network (step 4). Each device updates the agent\nafter it performs an inference with the response time information of the requested service (step\n5). In addition, all devices in the framework send the available resource information including the\nprocessor utilization, available memory, and network condition to the cloud device (step 5).\n5.2\nBenchmarks and Scenarios\nMobileNets are small, low-latency deep learning models trained for efficient execution of image\nclassification on resource-constrained devices [13]. For DL workloads, we consider MobileNetV1\nimage classification application as the benchmark [13]. We deploy the MobileNetV1 service for\nend-node classification. We consider eight different MobileNet models (ğ‘‘0 through ğ‘‘7) with varying\nlevels of accuracy and performance. Each model among ğ‘‘0 through ğ‘‘7 has varying number of\nMultiply-Accumulate units (MACs), MAC width and data format (e.g., FP32 and Int8), exposing\nmodels with different accuracy-performance trade-offs. Table 4 summarizes the MobileNet models\nwe consider, detailing the number of Multiply-Accumulates (MACs), MAC width and data formats\n(e.g., FP32 and Int8). The multiplier width is used to reduce a networkâ€™s size uniformly at each\nlayer. For a given layer and multiplier width, the number of input channels and the number of\noutput channels is decreased and increased, respectively, by a factor of the width multiplier. During\nthe orchestration phase, we select an appropriate model from ğ‘‘0-ğ‘‘7 to achieve the target level of\nclassification accuracy while maximizing the performance.\nOur framework supports multiple end-devices, networked with edge and cloud layers. For\nevaluation purposes, we set the maximum number of simultaneously active user devices to five.\nEach user-end device is connected to a single edge device, and can request a DL inference service to\nthe cloud layer. The cloud layer hosts the IO that contains the RL agent, which handles the inference\nservice requests. Upon on each service request, the RL agent is invoked to determine: (i) where\nthe request should be processed and (ii) what DL model should be executed for the corresponding\nrequest. The RL agentâ€™s goal is to minimize average response time for all end-node devices while\nsatisfying the accuracy constraint. This enforces quality control by imposing a strict threshold\non the average DL model accuracy. In this work, we conduct experiments under four unique\nscenarios with varying network conditions. Each scenario represents a combination of regular (R)\nand weak (W) network signal strength over five user-end devices (S1-S5) and 1 edge device (E).\nThe experimental scenarios are summarized in Table 5. The regular network has no transmission\ndelay, while we add 20ms delay to all outgoing packets to emulate the weak connection behavior.\nTable 4. MobileNet Models [13]\n#\nModel\nMillion MACs\nType\nTop-1\nAccuracy (%)\nTop-5\nAccuracy (%)\nğ‘‘0\n1.0 MobileNetV1-224\n569\nFP32\n70.9\n89.9\nğ‘‘1\n0.75 MobileNetV1-224\n317\nFP32\n68.4\n88.2\nğ‘‘2\n0.5 MobileNetV1-224\n150\nFP32\n63.3\n84.9\nğ‘‘3\n0.25 MobileNetV1-224\n41\nFP32\n49.8\n74.2\nğ‘‘4\n1.0 MobileNetV1-224\n569\nInt8\n70.1\n88.9\nğ‘‘5\n0.75 MobileNetV1-224\n317\nInt8\n66.8\n87.0\nğ‘‘6\n0.5 MobileNetV1-224\n150\nInt8\n60.7\n83.2\nğ‘‘7\n0.25 MobileNetV1-224\n41\nInt8\n48.0\n72.8\n14\nTable 5. Experiment Environment Setup. ğ‘…andğ‘Šrepresent Regular and Weak network condition, respectively.\nExperiment\nS1\nS2\nS3\nS4\nS5\nE\nEXP-A\nR\nR\nR\nR\nR\nR\nEXP-B\nR\nW\nR\nW\nR\nW\nEXP-C\nW\nW\nW\nR\nR\nR\nEXP-D\nW\nW\nW\nW\nW\nW\nEach experimental scenario in Table 5 shows the network condition of the specific device. Putting\ntogether the five different user devices and one edge device forms a unique combination of varying\nnetwork conditions per each experimental scenario.\n5.3\nExperimental Setup\nThe platform consists of five AWS a1.medium instances with single ARM-core as end-devices\nconnected to an AWS a1.large instance as edge device and an AWS a1.xlarge instance as cloud\nnode. Table 6 summarizes device specifications in details. DL model inferences are executed on\nprocessor cores on all nodes using ARM-NN SDK [23]. The inference engine is a set of open-source\nLinux software tools that enables machine learning workloads on ARM-core-based devices. The\nframeworkâ€™s message passing protocol is implemented using web services deployed at each node.\nSection 7.2 provides our analysis on frameworkâ€™s setup overhead.\n5.4\nHyper-parameters and RL Training\nAn RL agent has a number of hyper-parameters that impact its effectiveness (e.g., learning rate,\nepsilon, discount factor, and decay rate). The ideal values of parameters depend on the problem\ncomplexity, which in our case scales with the number of users (i.e., active end-node devices). In order\nto determine the learning rate and discount factor, we evaluated values between 0 and 1 for each\nhyper-parameters. We observed that a higher learning rate converges faster to the optimal, meaning\nthe more the reward is reflected to the Q-values, better the agent works. We also observed that a\nlower discount factor is better. This means that the consecutive actions have a weak relationship,\nso that giving less weight to the rewards in the near future improves the convergence time. Table 7\nshows the different problem configurations we used to determine the hyper-parameters. We train\nthe agent with two different learning algorithms (See Section 4.2). Our Q-Learning agent initializes\na Q-table with Q-values of zero, and chooses actions using an ğœ–âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦policy where ğœ–is the\nexploration rate. We initially set ğœ–= 1, meaning the agent selects a random action with probability\n1, otherwise it selects an action that gives the maximum future reward (i.e., Q-value) in the given\nstate. Although we perform probabilistic exploration continuously, we decay the exploration by\nepsilon decay parameter (See Table 7) per agent invocation. The Deep Q-Learning agent uses\ndifferent neural network structure for different number of users as the problem complexity changes.\nTable 6. Device Specification\nNode\nType\nvCPUs Memory\n(GiB)\nFrequency\n(GHz)\nArchitecture\nEnd\n1\n2\n2.3\naarch64\nEdge\n2\n4\n2.3\naarch64\nCloud\n4\n8\n2.3\naarch64\n15\nTable 7. Hyper-parameter values\nQ-Learning\nDeep Q-Learning\nNumber\nof Users\nLearning\nRate (ğ›¼)\nEpsilon\nDecay\nLearning\nRate (ğ›¼)\nEpsilon\nDecay\n1\n0.9\n1e-1\nâˆ’\nâˆ’\n2\n0.9\n1e-2\nâˆ’\nâˆ’\n3\n0.9\n1e-2\n1e-3\n0.4\n4\n0.9\n1e-3\n1e-3\n0.7\n5\n0.9\n1e-4\n1e-3\n0.9\nWe train DNN models with two fully connected layers where the hidden layers have 48, 64, 128\nneurons for three, four, and five devices, respectively. We implement the experience replay as a\nFIFO buffer with size equal to 1000. In order to update the network, at each step, we randomly\nsample 64 records from the buffer and then use them as a mini-batch. We use ğœ–âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦policy to\ntrain the Deep Q-network, where we initially set the ğœ–equal to 1.\n6\nEVALUATION RESULTS AND ANALYSIS\nIn this section, we demonstrate the effectiveness of our online learning based inference orchestration.\nWe evaluate our approach on the multi-layered end-edge-cloud framework, described in Section 4.\nOur approach features online reinforcement learning for intelligent orchestration, DL inference\nservices and end-edge-cloud architectures, targeting DL inference performance. [36] presents\nstate-of-the-art machine learning based orchestration for end-edge-cloud architecture baseline. For\na fair comparison, we evaluate our approach against the strategy proposed in [36], which integrates\nthe aforementioned features of our approach.\n6.1\nPerformance Analysis\nWe evaluate our agentâ€™s ability to identify the optimal orchestration decision at each invocation.\nThrough reinforcement learning, the agent predicts orchestration decisions including offloading\npolicy and DL model configuration to maximize performance and meet the accuracy threshold. At\ndesign time, we determine the true optimal configuration in any given conditions of workloads,\nnetwork, and number of active users using a brute force search. First, we compare our reinforce-\nment learning based Intelligent Orchestratorâ€™s (IO) prediction accuracy against this true optimal\nconfiguration. Our proposed approach with both Q-Learning and Deep Q-Learning algorithm has\nyielded a 100% prediction accuracy in comparison with the true optimal configuration. Thus, our\nreinforcement learning based orchestration decisions always converge with the optimal solution.\nNext, we evaluate our agentâ€™s efficacy by comparing it with a representative state-of-the-art [36]\nbaseline in terms of performance and accuracy. To implement the baseline policy into our frame-\nwork, we limit the agent to actions that specify offloading decisions ğ‘ğœ= {ğ‘œğ‘–}, using the most\naccurate DL model. We additionally compare fixed orchestrations for points of reference. The fixed\nsolution is limited to configurations where all end-devices either (a) perform the most accurate\nDL inference execution locally, (b) offload to the edge, or (c) offload to the cloud. In the following\nsubsections, we demonstrate the efficacy of our proposed agent to find the optimal configuration\nin presence of different number of users (up to five). Then, we investigate its ability to adapt to\nnetwork variations and evaluate its overhead. We explain the impact of varying DL models on the\nperformance under different system dynamics and elaborate how the proposed agent follows the\ndefined constraints.\n16\n0\n250\n500\n750\n1000\nAvg Response Time (ms)\n1\n2\n3\n4\n5\nNumber of Users\n0\n20\n40\n60\n80\n100\nAvg Accuracy (%)\nDevice\nEdge\nCloud\nSOTA [36]\nOurs- Min\nOurs- 80%\nOurs- 85%\nOurs- 89%\nOurs- Max\nFig. 5. Results of the framework within Exp-A for different number of active users.\n6.1.1\nUser Variability. To evaluate the user variability, we consider up to five simultaneously active\nuser-end devices, keeping the network constraints constant. We consider five different levels of\naccuracy thresholds viz., Min, 80%, 85%, 89%, and Max. Min refers to the accuracy threshold for\ncomputing where no constraint is applied to the learning algorithms (See Equation 4) and Max\nrepresents the accuracy threshold for computing where the average accuracy constraint is set to\n89.9%. We present the average response time and average accuracy for each of these thresholds using\nour proposed approach. For evaluation, we also present the average response time and accuracy\nmetrics achieved with the state-of-the-art baseline approach [36], and three fixed orchestration\ndecisions viz., device only, edge only and cloud only.\nFixed Strategies Figure 5 shows the average response time and accuracy for different numbers\nof active users for regular network conditions (represented by scenario Exp-A in Table 5), using\ndifferent orchestration strategies. The x-axis represents the number of active users. Each bar\nrepresents a different orchestration decision made by using the corresponding orchestration strategy.\nWith the device only strategy, each user-end device executes the inference service on the local\ndevice. Thus, varying number of users has no effect on the average response time in this case. With\nthe edge and cloud only strategies, simultaneous requests contend for edge and cloud resources.\nThis increases the average response time significantly, as the number of users increase. For instance,\nthe fixed edge only strategy with five active users leads to an average response time of 1140ms,\nwhile it is 665ğ‘šğ‘ with cloud only strategy. Higher volume of available resources at the cloud layer\nresults in relatively better average response time in comparison with the edge only strategy. On\n17\nthe other hand, the average response time with the device only strategy is 459ğ‘šğ‘ , representing the\noptimal case.\nBaseline With the SOTA [36] approach, the average response time remains constant until the\nnumber of users is two. This is due to the orchestration decision of distributing the services across\nedge and cloud layers. As the number of users increase to three, the service requests contend\nfor resources, leading to an increase in the average response time. With the number of users\nincreasing from three through five, the average response time increases, but at a relatively lower\nrate, exhibiting efficient utilization of the edge and cloud resources. As the number of users increase,\nthe efficiency of the baseline approach over the fixed strategies is more prominent. Both the baseline\nand fixed strategies are agnostic to model selection and configuration, retaining the maximum\nprediction accuracy of the inference service. Thus the average accuracy remains constant with the\naforementioned strategies, as shown in Figure 5.\nOur proposed solution Our proposed solution achieves the same average response time in\ncomparison with the baseline for the Max accuracy scenario. When the accuracy threshold is\nrelaxed, our reinforcement learning based intelligent orchestrator selects appropriate models\n(among ğ‘‘0-ğ‘‘7) to improve the average response time. As the number of users increase, our solution\nleverages the model selection combined with offloading technique to address the potential increase\nin response time. With appropriate model selection, our approach reduces the compute intensity,\nand consequently maintains a lower average response time even with the increasing number of\nusers. Trivially, the average response time with our approach is lower as the accuracy threshold is\nreduced. However, it should be noted that we enforce the boundaries on tolerable loss of accuracy\nwith our model selection decisions. Figure 5 shows the average response time and average accuracy\nwith our solution over different scenarios of accuracy thresholds and varying number of users. Our\nsolution provides up to 35% improvement in the average response time in comparison with the\nbaseline, within a tolerable loss of 0.9% accuracy. Table 8 shows the orchestration decisions of our\nagent for different numbers of active users, and also over four different experimental scenarios\n(Table 5). We present the orchestration decision and the average response time achieved with each\ndecision, for the maximum accuracy threshold scenario.\n6.1.2\nNetwork variation. We consider two possible levels of network connection: (i) a regular\nnetwork that has low latency, and (ii) a weak network that has high latency. We add 20ms delay to\nall outgoing packets to emulate the weak connection behavior. With varying network conditions,\nthere is an increased delay with offloading decisions across the network. Both the baseline and fixed\napproaches are affected by the weak network conditions, resulting in a higher average response\ntime. The fixed strategies employ the trivial device, edge and cloud only offloading decisions,\nsuffering higher latency. The baseline approach is confined to only an intelligent offloading strategy,\nwhich also results in higher average response time inevitably. On the other hand, our proposed\nsolution adapts to varying network conditions by opportunistically exploiting the accuracy trade-\noffs through model selection. This way, we address for the latency penalty levied by weak network\nconditions by reducing the compute intensity of the workloads, within the tolerable accuracy\nbounds.\nTable 9 shows the orchestration decisions made by our intelligent orchestrator, average response\ntime, and average accuracy achieved over varying networking conditions. Each experiment scenario\n(Exp-A through Exp-D) combines different network conditions for each node in the network (See\nTable 5). For example, in Exp-A, all the nodes are connected with regular network, whereas in\nExp-B, nodes ğ‘†1, ğ‘†3, and ğ‘†5 have regular connections and the rest have weak connections. We set\nthe number of active users to five.\nModel Selection Within each experiment scenario, the average response is lower as the accuracy\n18\nTable 8. Detailed offloading decisions of our agent for different number of active users in all four experiments\n(Maximum Accuracy Threshold). For example, in Exp-A, the orchestrator offloads the most accurate DL\ninference execution (ğ‘‘0) to the cloud device (ğ‘‘0,ğ¶for end-node ğ‘†1). In the presence of five active users,\nthe decisions are {ğ‘‘0, ğ¸}, {ğ‘‘0, ğ¿}, {ğ‘‘0, ğ¿}, {ğ‘‘0,ğ¶}, and {ğ‘‘0, ğ¿} for end-nodes ğ‘†1 to ğ‘†5, respectively. In this\ncase, ğ‘†1, ğ‘†2, and ğ‘†4 perform DL inference execution of the ğ‘‘0 model locally (ğ¿). ğ‘†0 and ğ‘†3 offload inference\nexecution of the ğ‘‘0 model to the edge (ğ¸) and cloud (ğ¶), respectively.\nEnd-node Devices\nExperiments\nNumber of Users\nS1\nS2\nS3\nS4\nS5\nAvg Res (ms)\nDecision\nExp-A\n1\nğ‘‘0,ğ¶\nâˆ’\nâˆ’\nâˆ’\nâˆ’\n363.47\n2\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nâˆ’\nâˆ’\nâˆ’\n363.17\n3\nğ‘‘0,ğ¶\nğ‘‘0, ğ¿\nğ‘‘0, ğ¸\nâˆ’\nâˆ’\n397.53\n4\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0, ğ¸\nğ‘‘0,ğ¶\nâˆ’\n410.35\n5\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¿\n418.91\nExp-B\n1\nğ‘‘0, ğ¸\nâˆ’\nâˆ’\nâˆ’\nâˆ’\n403.30\n2\nğ‘‘0, ğ¸\nğ‘‘0,ğ¶\nâˆ’\nâˆ’\nâˆ’\n416.78\n3\nğ‘‘0, ğ¸\nğ‘‘0,ğ¶\nğ‘‘0, ğ¿\nâˆ’\nâˆ’\n431.90\n4\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nâˆ’\n457.96\n5\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\n472.88\nExp-C\n1\nğ‘‘0,ğ¶\nâˆ’\nâˆ’\nâˆ’\nâˆ’\n471.65\n2\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nâˆ’\nâˆ’\nâˆ’\n467.80\n3\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nâˆ’\nâˆ’\n488.21\n4\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nâˆ’\n480.70\n5\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\n464.59\nExp-D\n1\nğ‘‘0, ğ¿\nâˆ’\nâˆ’\nâˆ’\nâˆ’\n585.68\n2\nğ‘‘0, ğ¸\nğ‘‘0,ğ¶\nâˆ’\nâˆ’\nâˆ’\n527.39\n3\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nâˆ’\nâˆ’\n491.77\n4\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nâˆ’\n501.07\n5\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\n506.62\nthreshold is relaxed. ğ‘‘0 through ğ‘‘7 represent models with different response time and accuracy\nlevels. For instance, models ğ‘‘0, ğ‘‘4, ğ‘‘2, ğ‘‘7 and ğ‘‘7 are selected respectively for accuracy thresholds\nranging from Max through Min in Exp-A. Our proposed orchestrator explores the Pareto-optimal\nspace of model selection and offloading choice, combining the opportunities at application and\nplatform layers simultaneously. For instance in Exp-A, maintaining an accuracy level of 89% results\nin an average response time of 269.8ms, by i) setting the models to ğ‘‘4, ğ‘‘4, ğ‘‘4, ğ‘‘0, and ğ‘‘4 on devices\nS1-S5, and ii) device configurations to L (local device), L, L, E (edge) and L for S1-S5. However,\nthe average response time can be improved by sacrificing the accuracy within a pre-determined\ntolerable level. For instance, by lowering the accuracy threshold by 4% (from 89% to 85%), the\naverage response time can be reduced by 88% (from 269ms to 143ms) by i) setting the models to\nğ‘‘2, ğ‘‘6, ğ‘‘5, ğ‘‘6, and ğ‘‘5 on devices S1-S5, and ii) device configurations to L (local device), L, L, L\nand L for S1-S5. With varying network conditions, our solution explores the offloading and model\nselection Pareto-optimal space at run-time to predict the optimal orchestration decisions.\nFor example, in Exp-D, our framework obtains 356.75ms on average response time with signifi-\ncantly weak network connectivity, while it can adapt to regular connectivity in Exp-A to obtain\n269.80ms on average response time. In this case, the average accuracy is 89.1% which shows 0.8%\nerror with the maximum average accuracy. The baseline [36] orchestrates the most accurate DL\n19\nTable 9. Results of the proposed framework for different accuracy constraints for different experiments (five\nusers). For example, in Exp-D with 89% average accuracy constraint, our framework orchestrates ğ‘†1, ğ‘†2, ğ‘†3,\nand ğ‘†4 to execute DL inference using model ğ‘‘4 locally and offload inference execution using model ğ‘‘0 at the\ncloud. However, the baseline obtains the maximum accuracy by executing the most accurate DL inference\nlocally for ğ‘†1, ğ‘†4, and ğ‘†5 while offloading ğ‘‘0 to the edge and cloud for ğ‘†3 and ğ‘†2, respectively.\nEnd-node Devices\nExperiments\nConstraint\nS1\nS2\nS3\nS4\nS5\nAvg Res (ms)\nAvg Acc (%)\nDecision\nExp-A\nMin\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\n72.08\n72.80\n80%\nğ‘‘7, ğ¿\nğ‘‘6, ğ¿\nğ‘‘6, ğ¿\nğ‘‘6, ğ¿\nğ‘‘6, ğ¿\n103.88\n81.11\n85%\nğ‘‘2, ğ¿\nğ‘‘6, ğ¿\nğ‘‘5, ğ¿\nğ‘‘6, ğ¿\nğ‘‘5, ğ¿\n143.81\n85.06\n89%\nğ‘‘4, ğ¿\nğ‘‘4, ğ¿\nğ‘‘4, ğ¿\nğ‘‘0, ğ¸\nğ‘‘4, ğ¿\n269.80\n89.10\nMax\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¿\n418.91\n89.90\nExp-B\nMin\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\n106.76\n72.80\n80%\nğ‘‘6, ğ¿\nğ‘‘3, ğ¿\nğ‘‘6, ğ¿\nğ‘‘6, ğ¿\nğ‘‘6, ğ¿\n139.92\n83.23\n85%\nğ‘‘5, ğ¿\nğ‘‘5, ğ¿\nğ‘‘6, ğ¿\nğ‘‘6, ğ¿\nğ‘‘2, ğ¿\n176.21\n85.05\n89%\nğ‘‘4, ğ¿\nğ‘‘4, ğ¿\nğ‘‘0, ğ¸\nğ‘‘4, ğ¿\nğ‘‘4, ğ¿\n303.50\n89.10\nMax\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\n472.88\n89.90\nExp-C\nMin\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\n119.28\n72.80\n80%\nğ‘‘6, ğ¿\nğ‘‘6, ğ¿\nğ‘‘7, ğ¿\nğ‘‘6, ğ¿\nğ‘‘6, ğ¿\n149.52\n81.11\n85%\nğ‘‘5, ğ¿\nğ‘‘6, ğ¿\nğ‘‘5, ğ¿\nğ‘‘6, ğ¿\nğ‘‘5, ğ¿\n190.76\n85.47\n89%\nğ‘‘4, ğ¿\nğ‘‘4, ğ¿\nğ‘‘4, ğ¿\nğ‘‘4, ğ¿\nğ‘‘0,ğ¶\n318.45\n89.10\nMax\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\n464.59\n89.90\nExp-D\nMin\nğ‘‘7, ğ¿\nğ‘‘6, ğ¿\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\nğ‘‘7, ğ¿\n158.53\n72.80\n80%\nğ‘‘6, ğ¿\nğ‘‘6, ğ¿\nğ‘‘6, ğ¿\nğ‘‘7, ğ¿\nğ‘‘6, ğ¿\n182.53\n81.12\n85%\nğ‘‘2, ğ¿\nğ‘‘6, ğ¿\nğ‘‘6, ğ¿\nğ‘‘5, ğ¿\nğ‘‘5, ğ¿\n225.32\n85.06\n89%\nğ‘‘4, ğ¿\nğ‘‘4, ğ¿\nğ‘‘4, ğ¿\nğ‘‘4, ğ¿\nğ‘‘0,ğ¶\n356.75\n89.10\nMax\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\n506.62\n89.90\nTable 10. Results of the state-of-the-art [36] in all four experiments.\nEnd-node Devices\nExperiments\nS1\nS2\nS3\nS4\nS5\nAvg Res (ms)\nAvg Acc (%)\nDecision\nExp-A\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¿\n418.91\n89.9\nExp-B\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\n472.88\n89.9\nExp-C\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\n464.59\n89.9\nExp-D\nğ‘‘0, ğ¿\nğ‘‘0,ğ¶\nğ‘‘0, ğ¸\nğ‘‘0, ğ¿\nğ‘‘0, ğ¿\n506.62\n89.9\ninference execution to obtain 506.62ms and 418.9ms average response time in Exp-D and Exp-A,\nrespectively. Orchestration decisions of the baseline approach over different experimental scenarios\nis summarized in Table 10. Although our proposed framework and the baseline can adapt to net-\nwork variability, our agent provides additional trade-off opportunities to deploy different models\ncombined with offloading technique. This leads to up to 35% speedup while sacrificing less than 1%\naverage accuracy.\n20\n0\n1\n2\nStep\nÃ—104\n0\n500\n1000\n1500\nâˆ¥Rewardâˆ¥\nThree End-devices\n0.0\n0.5\n1.0\nStep\nÃ—105\n0\n500\n1000\n1500\nâˆ¥Rewardâˆ¥\nFour End-devices\n0\n1\nStep\nÃ—106\n0\n500\n1000\n1500\nâˆ¥Rewardâˆ¥\nFive End-devices\n0\n1\nStep\nÃ—104\n0\n500\n1000\n1500\nâˆ¥Rewardâˆ¥\nThree End-devices\n0.0\n0.5\n1.0\nStep\nÃ—105\n0\n500\n1000\n1500\nâˆ¥Rewardâˆ¥\nFour End-devices\n0\n1\nStep\nÃ—106\n0\n500\n1000\n1500\nâˆ¥Rewardâˆ¥\nFive End-devices\nAccuracy-Min\nAccuracy-80%\nAccuracy-85%\nAccuracy-Max\n(a) Q-Learning Algorithm\n(b) Deep Q-Learning Algorithm\nFig. 6. Training overhead for multi-user networks with Q-Learning and Deep Q-Learning algorithms\nunder different accuracy constraints (See Algorithm 1 and 2, respectively).\n0.0\n0.5\n1.0\n1.5\n2.0\nStep\nÃ—106\n200\n400\n600\n800\nâˆ¥Rewardâˆ¥\n(a) Q-Learning Algorithm\n0.0\n0.5\n1.0\n1.5\nStep\nÃ—105\n200\n400\n600\n800\n1000\nâˆ¥Rewardâˆ¥\n(b) Deep Q-Learning Algorithm\nTransferLearning-80%\nTransferLearning-85%\nScratch-80%\nScratch-85%\nFig. 7. Transfer learning strategy can be used to alleviate the convergence time. In our experiments, the\nstrategy improves the convergence time up to 12.5Ã— and 3.3Ã— for Q-Learning and Deep Q-Learning for five\nEnd-devices, respectively. For example, the training phase for Q-Learning algorithm under 80% accuracy\nconstraint converges at 10.5 Ã— 105 steps. While, using the transfer learning it converges at 8.2 Ã— 104 steps.\n21\nTable 11. Training convergence time for three, four , and five End-devices with Q-Learning and Deep Q-\nLearning algorithms compared with SOTA [36] and Bruteforce strategy (See Section 4)\n.\nNumber of Users\nConstraint\nQ-Learning (step #)\nDeep Q-Learning (step #)\nSOTA [36]\nBruteforce (step #)\n3\nMin\n6.6 Ã— 103\n1.0 Ã— 104\n-\n6.6 Ã— 108\n80%\n1.8 Ã— 103\n1.0 Ã— 104\n-\n6.6 Ã— 108\n85%\n0.8 Ã— 103\n1.0 Ã— 104\n-\n6.6 Ã— 108\nMax\n6.7 Ã— 103\n1.0 Ã— 104\n2.0 Ã— 103\n6.6 Ã— 108\n4\nMin\n9.0 Ã— 104\n3.0 Ã— 104\n-\n5.3 Ã— 1010\n80%\n8.0 Ã— 104\n4.0 Ã— 104\n-\n5.3 Ã— 1010\n85%\n4.0 Ã— 104\n4.0 Ã— 104\n-\n5.3 Ã— 1010\nMax\n9.0 Ã— 104\n4.0 Ã— 104\n5.0 Ã— 103\n5.3 Ã— 1010\n5\nMin\n10.5 Ã— 105\n6.0 Ã— 104\n-\n4.2 Ã— 1012\n80%\n10.5 Ã— 105\n6.0 Ã— 104\n-\n4.2 Ã— 1012\n85%\n5.6 Ã— 105\n7.0 Ã— 104\n-\n4.2 Ã— 1012\nMax\n10.5 Ã— 105\n7.0 Ã— 104\n2.5 Ã— 104\n4.2 Ã— 1012\n6.2\nOverhead Analysis\nDeveloping a global RL agent for optimal runtime orchestration decisions in an end-edge-cloud\nsystem incurs overhead to multiple sources. We evaluate the sources of overhead in both exploration\nand exploitation phases to demonstrate the feasibility of our proposed solution.\n6.2.1\nExploration Overhead. We evaluate the time required by the proposed agent for the training\nphase to identify an optimal policy. Figure 6 shows the training phase for different numbers\nof end-devices under different accuracy constraints. We train the agent with Q-Learning and\nDeep Q-Learning algorithms under different accuracy constraints (See Figure 6.(a) and 6.(b),\nrespectively). The convergence time for five devices with different policies are summarized in\nTable 11. Q-Learning agent converges faster than Deep Q-Learning agent for the three End-devices\nscenario. However, increasing the number of End-devices leads to the more complex problem. Deep\nQ-Learning agent converges up to 17.5Ã— faster than Q-Learning agent for the five End-devices\nscenario. In other words, Deep Q-Learning algorithm converges faster for high-dimensional space\nproblems. Furthermore, SOTA converges faster since the agent only uses limited actions (3 actions\nfor computation offloading) making a low-dimensional space problem.\nIn addition, we observe that the training phase can be accelerated by exploiting previous experi-\nences in similar scenarios known as transfer learning strategy. Figure 7 shows that the strategy can\nalleviate the convergence up to 12.5Ã— and 3.3Ã— for Q-Learning and Deep Q-Learning algorithms,\nrespectively. In the transfer learning strategy, we train a model with minimum accuracy threshold\nfrom scratch. Then, we initialize model with the trained model to reduce the convergence time. In\nconclusion, the Deep Q-Learning algorithm with the transfer learning strategy can speedup the\nconvergence time up to 57.7Ã— in comparison with Q-Learning algorithm for the five End-devices\nscenario.\n6.2.2\nRun-time Overhead. The agent is invoked periodically at runtime, imposing overhead on DL\ninference execution. We evaluate the following components individually:\n(a) Resource Monitoring: A continuous resource monitoring service imposes runtime overhead in\nterms of DL inference response time. Figure 8 shows that the latency overhead for all layers is\nnegligible (less than 0.8% of minimum response time overall).\n(b) Message Broadcasting: Sharing resource usage and orchestration decision information over the\nnetwork potentially increases DL inference response time. Table 3 shows the additional network\n22\nDevice\nEdge\nCloud\n0\n200\n400\n600\nLatency (us)\nFig. 8. Resource Monitoring Overhead\nlatency for different network conditions. The request is the latency required to send an input image\nto a higher layer, and dominates the sources of network overhead. We observe that the broadcasting,\nin total, does not impose more than 2% of overall response time.\n(c) Intelligent Orchestrator: The Q-Learning agentâ€™s logic itself takes on average 0.6ms to execute in\nthe cloud. While, the Deep Q-Learning agentâ€™s step takes 11ğ‘šğ‘ on average to execute using NVIDIA\nRTX 5000 in cloud. During exploitation, our trained agent identifies the optimal orchestration\ndecision within five invocations. We conclude that after an agent is trained, the improvements of\n35% in average response time compared to prior art justifies the total overhead of our agent.\nTable 12. Message Broadcasting Overhead\nRegular\nWeak\nRequest\n20 ms\n137 ms\nUpdate\n0.4 ms\n2 ms\nDecision\n1 ms\n2 ms\nTotal\n21.4 ms\n141 ms\n7\nCONCLUSION\nCross-layer optimization that considers both model optimization and computation offloading\ntogether provides an opportunity to enhance performance while satisfying accuracy requirements.\nIn this paper, for the first time, we proposed an online learning framework for DL inference\nin end-edge-cloud systems by coordinating tradeoffs synergistically at both the application and\nsystem layers. The proposed reinforcement learning-based online learning framework adopts model\noptimization techniques with computation offloading to find the minimum average response time\nfor DL inference services while meeting an accuracy constraint. Using this method, we observed\nup to 35% speedup for average response time while sacrificing less than %0.9 accuracy on a real\nend-edge-cloud system when compared to prior art. Our approach shows that online learning can\nbe deployed effectively for orchestrating DL inference in end-edge-cloud systems, and opens the\ndoor for further research in online learning for this important and growing area.\nREFERENCES\n[1] Md Golam Rabiul Alam, Mohammad Mehedi Hassan, Md ZIa Uddin, Ahmad Almogren, and Giancarlo Fortino. 2019.\nAutonomic computation offloading in mobile edge for IoT applications. Future Generation Computer Systems 90 (2019),\n149â€“157.\n23\n[2] Pavlos Athanasios Apostolopoulos, Eirini Eleni Tsiropoulou, and Symeon Papavassiliou. 2020. Risk-aware data\noffloading in multi-server multi-access edge computing environment. IEEE/ACM Transactions on Networking 28, 3\n(2020), 1405â€“1418.\n[3] Marco V Barbera, Sokol Kosta, Alessandro Mei, and Julinda Stefa. 2013. To offload or not to offload? the bandwidth\nand energy costs of mobile cloud computing. In 2013 Proceedings Ieee Infocom. IEEE, 1285â€“1293.\n[4] Jing Bi, Haitao Yuan, Shuaifei Duanmu, MengChu Zhou, and Abdullah Abusorrah. 2020. Energy-Optimized Par-\ntial Computation Offloading in Mobile-Edge Computing With Genetic Simulated-Annealing-Based Particle Swarm\nOptimization. IEEE Internet of Things Journal 8, 5 (2020), 3774â€“3785.\n[5] Shichao Chen, Qijie Li, Mengchu Zhou, and Abdullah Abusorrah. 2021. Recent advances in collaborative scheduling of\ncomputing tasks in an edge computing paradigm. Sensors 21, 3 (2021), 779.\n[6] Xianfu Chen, Honggang Zhang, Celimuge Wu, Shiwen Mao, Yusheng Ji, and Medhi Bennis. 2018. Optimized compu-\ntation offloading performance in virtual edge computing systems via deep reinforcement learning. IEEE Internet of\nThings Journal 6, 3 (2018), 4005â€“4018.\n[7] Zhao Chen and Xiaodong Wang. 2018. Decentralized computation offloading for multi-user mobile edge computing: A\ndeep reinforcement learning approach. arXiv preprint arXiv:1812.07394 (2018).\n[8] BaiChuan Cheng, ZhiLong Zhang, and DanPu Liu. 2019. Dynamic Computation Offloading Based on Deep Reinforce-\nment Learning. In 12th EAI International Conference on Mobile Multimedia Communications, Mobimedia 2019. European\nAlliance for Innovation (EAI).\n[9] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. A survey of model compression and acceleration for deep\nneural networks. arXiv preprint arXiv:1710.09282 (2017).\n[10] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. Binaryconnect: Training deep neural networks\nwith binary weights during propagations. In Advances in neural information processing systems. 3123â€“3131.\n[11] Amir Erfan Eshratifar, Mohammad Saeed Abrishami, and Massoud Pedram. 2019. JointDNN: an efficient training and\ninference engine for intelligent mobile cloud computing services. IEEE Transactions on Mobile Computing (2019).\n[12] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights and connections for efficient neural\nnetwork. In Advances in neural information processing systems. 1135â€“1143.\n[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto,\nand Hartwig Adam. 2017. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861 (2017).\n[14] Xin-Lin Huang, Xiaomin Ma, and Fei Hu. 2018. Machine learning and intelligent communications. Mobile Networks\nand Applications 23, 1 (2018), 68â€“70.\n[15] Hyuk-Jin Jeong, Hyeon-Jae Lee, Chang Hyun Shin, and Soo-Mook Moon. 2018. IONN: Incremental offloading of neural\nnetwork computations from mobile devices to edge servers. In Proceedings of the ACM Symposium on Cloud Computing.\n401â€“411.\n[16] Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovinski, Trevor Mudge, Jason Mars, and Lingjia Tang. 2017.\nNeurosurgeon: Collaborative intelligence between the cloud and mobile edge. ACM SIGARCH Computer Architecture\nNews 45, 1 (2017), 615â€“629.\n[17] Hongchang Ke, Jian Wang, Hui Wang, and Yuming Ge. 2019. Joint Optimization of Data Offloading and Resource\nAllocation With Renewable Energy Aware for IoT Devices: A Deep Reinforcement Learning Approach. IEEE Access 7\n(2019), 179349â€“179363.\n[18] Hakima Khelifi, Senlin Luo, Boubakr Nour, Akrem Sellami, Hassine Moungla, Syed Hassan Ahmed, and Mohsen\nGuizani. 2018. Bringing deep learning at the edge of information-centric internet of things. IEEE Communications\nLetters 23, 1 (2018), 52â€“55.\n[19] Young Geun Kim and Carole-Jean Wu. 2020. Autoscale: Energy efficiency optimization for stochastic edge inference\nusing reinforcement learning. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO).\nIEEE, 1082â€“1096.\n[20] Ji Li, Hui Gao, Tiejun Lv, and Yueming Lu. 2018. Deep reinforcement learning based computation offloading and\nresource allocation for MEC. In 2018 IEEE Wireless Communications and Networking Conference (WCNC). IEEE, 1â€“6.\n[21] Long-Ji Lin. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine\nlearning 8, 3-4 (1992), 293â€“321.\n[22] Sicong Liu, Junzhao Du, Kaiming Nan, Atlas Wang, Yingyan Lin, et al. 2020. AdaDeep: A Usage-Driven, Automated\nDeep Model Compression Framework for Enabling Ubiquitous Intelligent Mobiles. arXiv preprint arXiv:2006.04432\n(2020).\n[23] Arm Ltd. [n.d.]. IP Products: Arm NN. https://developer.arm.com/ip-products/processors/machine-learning/arm-nn\n[24] Haifeng Lu, Chunhua Gu, Fei Luo, Weichao Ding, and Xinping Liu. 2020. Optimization of lightweight task offloading\nstrategy for mobile edge computing based on deep reinforcement learning. Future Generation Computer Systems 102\n(2020), 847â€“861.\n24\n[25] Pavel Mach and Zdenek Becvar. 2017. Mobile edge computing: A survey on architecture and computation offloading.\nIEEE Communications Surveys & Tutorials 19, 3 (2017), 1628â€“1656.\n[26] Bradley McDanel, Surat Teerapittayanon, and HT Kung. 2017. Embedded binarized neural networks. arXiv preprint\narXiv:1709.02260 (2017).\n[27] Minghui Min, Liang Xiao, Ye Chen, Peng Cheng, Di Wu, and Weihua Zhuang. 2019. Learning-based computation\noffloading for IoT devices with energy harvesting. IEEE Transactions on Vehicular Technology 68, 2 (2019), 1930â€“1941.\n[28] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,\nMartin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement\nlearning. nature 518, 7540 (2015), 529â€“533.\n[29] Seyed Sajad Mousavi, Michael Schukat, and Enda Howley. 2016. Deep reinforcement learning: an overview. In\nProceedings of SAI Intelligent Systems Conference. Springer, 426â€“440.\n[30] Burhan A Mudassar, Jong Hwan Ko, and Saibal Mukhopadhyay. 2018. Edge-cloud collaborative processing for\nintelligent internet of things: A case study on smart surveillance. In 2018 55th ACM/ESDA/IEEE Design Automation\nConference (DAC). IEEE, 1â€“6.\n[31] M.R. Nakhkash et al. 2019. Analysis of Performance and Energy Consumption of Wearable Devices and Mobile\nGateways in IoT Applications. In COINS.\n[32] Jihong Park, Sumudu Samarakoon, Mehdi Bennis, and MÃ©rouane Debbah. 2019. Wireless network intelligence at the\nedge. Proc. IEEE 107, 11 (2019), 2204â€“2239.\n[33] Guanhua Qiao, Supeng Leng, and Yan Zhang. 2019. Online learning and optimization for computation offloading in\nD2D edge computing and networks. Mobile Networks and Applications (2019), 1â€“12.\n[34] Xukan Ran, Haolianz Chen, Xiaodan Zhu, Zhenming Liu, and Jiasi Chen. 2018. Deepdecision: A mobile deep learning\nframework for edge video analytics. In IEEE INFOCOM 2018-IEEE Conference on Computer Communications. IEEE,\n1421â€“1429.\n[35] JÃ¼rgen Schmidhuber. 2015. Deep learning in neural networks: An overview. Neural networks 61 (2015), 85â€“117.\n[36] Tanmoy Sen and Haiying Shen. 2019. Machine Learning based Timeliness-Guaranteed and Energy-Efficient Task\nAssignment in Edge Computing Systems. In 2019 IEEE Conference on Fog and Edge Computing. IEEE, 1â€“10.\n[37] Sina Shahhosseini, Arman Anzanpour, Iman Azimi, Sina Labbaf, DongJoo Seo, Sung-Soo Lim, Pasi Liljeberg, Nikil Dutt,\nand Amir M Rahmani. 2021. Exploring computation offloading in IoT systems. Information Systems (2021), 101860.\n[38] Sina Shahhosseini, Iman Azimi, Arman Anzanpour, Axel Jantsch, Pasi Liljeberg, Nikil Dutt, and Amir M Rahmani.\n2019. Dynamic Computation Migration at the Edge: Is There an Optimal Choice?. In Proceedings of the 2019 on Great\nLakes Symposium on VLSI. ACM, 519â€“524.\n[39] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.\n[40] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. 2017. Efficient processing of deep neural networks: A\ntutorial and survey. Proc. IEEE 105, 12 (2017), 2295â€“2329.\n[41] Ben Taylor, Vicent Sanz Marco, Willy Wolff, Yehia Elkhatib, and Zheng Wang. 2018. Adaptive deep learning model\nselection on embedded systems. ACM SIGPLAN Notices 53, 6 (2018), 31â€“43.\n[42] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. 2017. Distributed deep neural networks over the\ncloud, the edge and end devices. In 2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS).\nIEEE, 328â€“339.\n[43] Ziling Wei, Baokang Zhao, Jinshu Su, and Xicheng Lu. 2018. Dynamic edge computation offloading for Internet of\nThings with energy harvesting: A learning method. IEEE Internet of Things Journal 6, 3 (2018), 4436â€“4447.\n[44] Phil Winder. 2020. Reinforcement Learning. Oâ€™Reilly Media.\n[45] Jie Xu and Shaolei Ren. 2016. Online learning for offloading and autoscaling in renewable-powered mobile edge\ncomputing. In 2016 IEEE Global Communications Conference (GLOBECOM). IEEE, 1â€“6.\n[46] Mengwei Xu, Feng Qian, Mengze Zhu, Feifan Huang, Saumay Pushp, and Xuanzhe Liu. 2019. Deepwear: Adaptive\nlocal offloading for on-wearable deep learning. IEEE Transactions on Mobile Computing 19, 2 (2019), 314â€“330.\n[47] Ashkan Yousefpour, Caleb Fung, Tam Nguyen, Krishna Kadiyala, Fatemeh Jalali, Amirreza Niakanlahiji, Jian Kong,\nand Jason P Jue. 2018. All one needs to know about fog computing and related edge computing paradigms. (2018).\n[48] Haitao Yuan and MengChu Zhou. 2020. Profit-maximized collaborative computation offloading and resource allocation\nin distributed cloud and edge computing systems. IEEE Transactions on Automation Science and Engineering (2020).\n25\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-02-21",
  "updated": "2022-02-21"
}