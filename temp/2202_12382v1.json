{
  "id": "http://arxiv.org/abs/2202.12382v1",
  "title": "Fine-Grained Prediction of Political Leaning on Social Media with Unsupervised Deep Learning",
  "authors": [
    "Tiziano Fagni",
    "Stefano Cresci"
  ],
  "abstract": "Predicting the political leaning of social media users is an increasingly\npopular task, given its usefulness for electoral forecasts, opinion dynamics\nmodels and for studying the political dimension of polarization and\ndisinformation. Here, we propose a novel unsupervised technique for learning\nfine-grained political leaning from the textual content of social media posts.\nOur technique leverages a deep neural network for learning latent political\nideologies in a representation learning task. Then, users are projected in a\nlow-dimensional ideology space where they are subsequently clustered. The\npolitical leaning of a user is automatically derived from the cluster to which\nthe user is assigned. We evaluated our technique in two challenging\nclassification tasks and we compared it to baselines and other state-of-the-art\napproaches. Our technique obtains the best results among all unsupervised\ntechniques, with micro F1 = 0.426 in the 8-class task and micro F1 = 0.772 in\nthe 3-class task. Other than being interesting on their own, our results also\npave the way for the development of new and better unsupervised approaches for\nthe detection of fine-grained political leaning.",
  "text": "Journal of Artiﬁcial Intelligence Research 73 (2022) 633-672\nSubmitted 06/2021; published 02/2022\nFine-Grained Prediction of Political Leaning on Social Media\nwith Unsupervised Deep Learning\nTiziano Fagni\ntiziano.fagni@iit.cnr.it\nStefano Cresci\nstefano.cresci@iit.cnr.it\nInstitute of Informatics and Telematics (IIT)\nNational Research Council (CNR)\nvia G. Moruzzi 1, 56124 Pisa, Italy\nAbstract\nPredicting the political leaning of social media users is an increasingly popular task,\ngiven its usefulness for electoral forecasts, opinion dynamics models and for studying the\npolitical dimension of polarization and disinformation.\nHere, we propose a novel unsupervised technique for learning ﬁne-grained political\nleaning from the textual content of social media posts. Our technique leverages a deep\nneural network for learning latent political ideologies in a representation learning task.\nThen, users are projected in a low-dimensional ideology space where they are subsequently\nclustered. The political leaning of a user is automatically derived from the cluster to which\nthe user is assigned. We evaluated our technique in two challenging classiﬁcation tasks and\nwe compared it to baselines and other state-of-the-art approaches. Our technique obtains\nthe best results among all unsupervised techniques, with micro F1 = 0.426 in the 8-class\ntask and micro F1 = 0.772 in the 3-class task.\nOther than being interesting on their\nown, our results also pave the way for the development of new and better unsupervised\napproaches for the detection of ﬁne-grained political leaning.\n1. Introduction\nSince the advent of Facebook and Twitter, politicians have had an increasing online pres-\nence in order to reach out to as many potential electors as possible. As of today, digital\ncampaigning (including social media) has become mandatory, as people are massively con-\nsuming political content from social platforms1. Recently, 20% of interviewed social media\nusers admitted to have changed their minds about a political issue because of something\nthey read on social media2. Political activity on social media is also positively correlated\nto oﬄine political activism (e.g., attending oﬄine political events) (Vaccari et al., 2015).\nPolitically-interested users are keen to know the stance of their friends, to read about candi-\ndates and campaigns, and to discuss pressing issues and election results (Grčar et al., 2017;\nTucker et al., 2018). In spite of the relatively small readership of online platforms compared\nto that of traditional media (e.g., TVs, newspapers, and radio channels), the sociopolitical\nrelevance of social media is still massive. In fact, second-order eﬀects – typical of complex\nsystems – allow for signiﬁcant portions of the political social media content to be discussed\nalso on traditional media, thus somehow still making it into the minds of people who don’t\neven use social media at all (Benkler et al., 2017).\n1. www.journalism.org/2018/09/10/news-use-across-social-media-platforms-2018/\n2. www.pewinternet.org/2016/10/25/the-political-environment-on-social-media/\n©2022 AI Access Foundation. All rights reserved.\narXiv:2202.12382v1  [cs.SI]  23 Feb 2022\nFagni & Cresci\nGiven this picture, it comes with little surprise that the task of learning the political\nleaning of social media users recently received a surge of attention. In literature, this task\nis also referred to as political stance, ideology, polarity or alignment prediction. Firstly, it\nrepresents a natural extension to the early eﬀorts by social and political scientists at this task.\nIn fact, ideology lies at the core of many theories in political science and has long been used\nto investigate individual behavior and preferences, governmental relations, and links between\nthem (Bond & Messing, 2015). Traditional estimates are based on explicit preferences, such\nas roll-call votes, co-sponsorship records, and records of ﬁnancial contributions to political\ncampaigns. However, these data are typically available only for a few political ﬁgures (e.g.,\nroll-call votes) or for a limited number of ordinary individuals, they are hard to acquire,\nand they are made available or updated infrequently. These limitations make ﬁne-grained,\ncontinuous, large-scale analyses of political preferences challenging, if not outright infeasible.\nConversely, social media represent a trove of both explicit and structured (e.g., likes and\nsocial relationships), as well as implicit and unstructured (e.g., text), data about the habits\nand preferences, including political ones, of millions of users. As such, many social and\npolitical scientists recently turned their attention to political analyses on social media – e.g.,\nby estimating political leaning from social media data and by comparing such estimates with\nmore traditional ones (Tucker et al., 2018). Meanwhile, also computer scientists found value\nin learning users political leaning, for a myriads of goals, such as: to forecast the outcome of\nelections (Tumasjan et al., 2010; Ahmed et al., 2016); to estimate accurate priors for models\nof opinion diﬀusion (Dandekar et al., 2013; Mäs & Flache, 2013); to measure and mitigate\nonline polarization (Wong et al., 2016; Garimella et al., 2017; Nizzoli et al., 2021); to measure\nthe eﬀects of information operations, disinformation campaigns and propaganda (Nikolov\net al., 2021; Tardelli et al., 2022; Cinelli et al., 2020; Ferrara et al., 2020); to explore the\npolitical dimension of bad actors, such as social bots and trolls (Hegelich & Janetzko, 2016;\nRizoiu et al., 2018; Luceri et al., 2019; Yan et al., 2021; Cresci, 2020).\nExisting approaches to the prediction of political leaning mainly focus on analyzing only\nthe social or interaction networks (Garimella et al., 2016; Wong et al., 2016), or only the\ncontent of shared messages (Pla & Hurtado, 2014; Di Giovanni et al., 2018; Yan et al., 2019;\nPreoţiuc-Pietro et al., 2017), with few exceptions where content and networks are simulta-\nneously considered (Lahoti et al., 2018; Aldayel & Magdy, 2019). Network-based approaches\nare grounded on the assumption that ideologically-similar users are likely to interact with,\nor to follow, each other. A ﬁrst limitation arises when this assumption is violated – namely,\nin all those cases where like-minded users never interact, or in those equally-frequent cases\nwhere opposing users interact (e.g., to argue or to convince each other). There also exist\nusers that do not follow others, or that follow a very limited number of accounts, which in-\nevitably complicates network-based approaches. Notable examples of this kind are @POTUS\nin the US and media outlets/journalists that do not follow other accounts, for neutrality\nreasons, but that represent interesting subjects of political leaning analyses. Another limita-\ntion involves the large amounts of data needed for the analysis (e.g., the social or interaction\ngraph), which are seldom promptly available. Content-based approaches are instead mainly\nlimited by the intrinsic diﬃculty of processing natural language, and by the need for large\ncorpora of manually-annotated messages and language-speciﬁc resources.\nMoreover, the\nmajority of existing solutions adopt supervised approaches, which have been shown to lack\n634\nFine-Grained Prediction of Political Leaning on Social Media\ngeneralizability and to suﬀer from the limited availability of comprehensive and reliable\nground-truth datasets (Cohen & Ruths, 2013).\n1.1 Our Approach\nOur goal in this work is that of developing an unsupervised content-based technique for\npredicting the political leaning of social media users. We will focus on two diﬀerent tasks: (i)\nthe prediction of the preferred political party of a user (ﬁne-grained task), which in political\nscience literature is typically referred to as party identiﬁcation; and (ii) the prediction of its\npolitical pole (coarse-grained task). For bipolar systems, the latter task simply involves the\nprediction of left-right ideology, which for US data is typically measured in a continuous,\none-dimensional space, with techniques such as the well-known DW-NOMINATE (Poole &\nRosenthal, 1985). Notably, labels obtained for the two tasks represent diﬀerent user traits\nand should not be equated or used interchangeably. For instance, the diﬀerence between the\npreferred party and the ideological position of a user in the left-right scale is straightforward\nwhen considering the shifts that parties exhibit between diﬀerent elections (Busch, 2016).\nThis is particularly true for the application and evaluation scenario of our work: the tripolar\nItalian political system (Pasquino, 2019). Nonetheless, we are interested in evaluating the\neﬃcacy of our proposed method in solving each of the two tasks separately.\nIn contrast with previous work, where political ideology and leaning were considered as\nsynonyms, here we make an important distinction. By drawing upon deﬁnitions from the\nOxford English Dictionary, we deﬁne ideology3 as a latent set of concepts that forms the\nbasis of a user’s political preferences. Instead, we deﬁne leaning4 as the practical political\npreferences of a user (e.g., its preferred party). Our approach for predicting political leaning,\nindependently on the task (i.e., the desired ﬁne or coarse prediction granularity), directly\nstems from the previous deﬁnitions. In fact, we ﬁrst adopt an unsupervised approach to\nlearn informative political representations of social media users. We then project users into\na lower-dimensional space derived from their latent representations, which corresponds to\nthe political ideology space. Finally, we leverage the topology of the political ideology space\nto infer the political leaning of each user. As such, our predicted leanings strictly depend\non the latent ideologies learned for every user.\n1.2 Contributions\nOperationally, we propose a novel unsupervised solution for estimating the political leaning\nof social media users that is able to overcome the main limitations of previous approaches.\nOur method follows the scheme shown in Figure 1. Our solution initially leverages a deep\nneural network for learning latent users representations. Then, we feed these representations\nto a UMAP model in order to project and position users in a latent political ideology space.\nFinally, we leverage properties of the ideology space to infer the political leaning of every\nuser, via clustering. We evaluate our proposed method and those used for comparisons on\ntwo challenging tasks. Speciﬁcally, we learn both ﬁne-grained (i.e., party-level) and coarse-\ngrained (i.e., pole-level) political leaning of Twitter users. Our solution achieves state-of-\nthe-art results in both tasks, compared to existing unsupervised techniques. Speciﬁcally,\n3. https://www.lexico.com/en/definition/ideology\n4. https://www.lexico.com/en/definition/leaning\n635\nFagni & Cresci\nTwitter\ndata for\npolitical\nparties\nTwitter\ndata for\ngeneric\nusers\nBuild tweet\nclassifier for\npolitical parties \nRepresentation\nlearning step:\nuse tweet\nclassifier to\nextract high level\npolitical features\nfor users and\nparties\nProjection of\nusers and parties\ninto a latent\npolitical ideology\nspace\nClustering of\nusers and\nparties: each\ncluster found\nidentifies a\nspecific political\nparty in the\nideology space\nFigure 1: Outline of our proposal.\nit achieves F1 = 0.43 and F1 = 0.77 when predicting ﬁne- and coarse-grained leanings,\nwhereas other unsupervised techniques and baselines achieve F1 ≤0.35 and F1 ≤0.71,\nrespectively. Our technique is exclusively based on the textual content of user-generated\nsocial media posts. However, despite exploiting solely this noisy data source, it achieves\nperformances that are comparable or even better than techniques that leverage cleaner\nsignals (e.g., social relationships and interactions such as retweets and likes). This makes\nour technique particularly valuable since it obtains state-of-the-art performance without\nthe need for gathering explicit user preferences or data-demanding network representations.\nIn addition, by adopting an unsupervised deep learning approach, we are also language-\nindependent, we avoid the need for manually-annotated corpora and linguistic resources,\nand we improve the generalizability of our results with respect to the traditional supervised\napproaches that are intrinsically limited by the availability of accurate and extensive ground-\ntruth datasets (Cohen & Ruths, 2013; Cresci, 2020).\nOur main contributions can be summarized as in the following:\n• We provide a state-of-the-art unsupervised method for learning both ﬁne-grained and\ncoarse-grained political leaning of social media users.\n• Our nuanced solution disentangles the sub-tasks of learning latent political ideologies\nfrom that of inferring political leanings, which were mixed and overlapping in previous\nworks.\n• We demonstrate the usefulness of unsupervised deep learning and projection with\nUMAP, to accurately position users within a latent ideology space.\n• We show the proﬁtability of leveraging the topology of the learned ideology space to\ninfer political leaning via clustering.\n1.3 Reproducibility\nOur data are publicly available for scientiﬁc purposes5.\n5. https://doi.org/10.5281/zenodo.5793346\n636\nFine-Grained Prediction of Political Leaning on Social Media\n1.4 Roadmap\nThe remainder of the paper is organized as follows. In Section 2 we discuss previous works on\nthe prediction of political leaning from social media. Then, before presenting our solution,\nin Section 3 we outline the political context in which our study is positioned and we provide\ndetails about our dataset.\nSection 4 describes our deep learning approach for learning\nlatent political ideologies of social media users. In Section 5 we discuss our approach for\npositioning users in a latent political ideology space, and for inferring their political leaning.\nExperiments6 and results are presented in Section 6, while Section 7 draws conclusions and\nhighlights promising directions for future work.\n2. Related Work\nIn this section we brieﬂy survey extant literature for the prediction of political leaning. We\nsplit previous works based on the information used to make predictions.\n2.1 Content-Based Approaches\nAmong the ﬁrst approaches at this task are those solely based on the analysis of the textual\ncontent of messages. Pla and Hurtado (2014) investigated the use of sentiment analysis\nfeatures. They trained a supervised classiﬁcation model capable of labeling users based on\ntheir coarse-grained leaning – namely, as either left-leaning, right-leaning, center-leaning or\nundeﬁned. Similarly, Di Giovanni et al. (2018) leveraged a set of linguistic syntactic features\nin a supervised classiﬁcation task. The goal of their system was that of learning the political\npreference of Twitter accounts towards the 4 main parties in Italy. Despite focusing on\nﬁne-grained (i.e., party-level) predictions, Di Giovanni et al. worked with only 4 parties,\ninstead of the 8 considered in our present work.\nThe previous works are representatives of a rather large body of work based on supervised\ncontent classiﬁcation. Results obtained by these systems are however disputed by Cohen\nand Ruths (2013), since they tend to overestimate performances by focusing on politically\nactive users (instead of normal or politically inactive users) and since their classiﬁcation\nperformances rapidly plummet when applied outside of the narrow range of examples used\nfor training the systems. Similar results were also recently obtained by Yan et al. (2019),\nwho evaluated the generalizability of text-based supervised systems for classifying partisan-\nship and political ideology. Speciﬁcally, the authors built 3 datasets derived from the US\nCongressional Record, polarized media websites, and political wikis. Then, they trained a\nset of supervised classiﬁers on a dataset and they evaluated their performance in classifying\ntexts from the other datasets. Among the supervised algorithms used for text classiﬁcation\nare logistic regression as well as deep learning-based classiﬁers such as Marginalized Stacked\nDenoising Autoencoders and Semi-Supervised Recursive Autoencoders. Results show the\ndiﬃculty of supervised and semi-supervised systems in generalizing from one dataset to\nanother, thus motivating research and experimentation with unsupervised approaches.\n6. Throughout the manuscript we use the term “experiment” with its conventional meaning in computer\nscience – that is, an analysis, measurement, or evaluation campaign. This is diﬀerent from its mean-\ning in other disciplines (i.e., the social sciences) where experimental approaches involve treatments or\ninterventions and are opposed to observational ones.\n637\nFagni & Cresci\nAnother major drawback of supervised classiﬁcation is that political leaning is typically\nprovided as a discrete (e.g., binary) variable. A ﬁrst improvement over these works was done\nby Preoţiuc-Pietro et al. (2017), who predicted the political orientation of Twitter users\non a 7 point scale ranging from “very conservative” to “very liberal”, with several points\nreserved for moderate users.\nThey leveraged several features extracted from the tweets\nposted by the analyzed users, including features derived from LIWC, sentiment, topics,\nnamed entities and word2vec, and the prediction was performed with simple supervised\nclassiﬁcation algorithms (e.g., logistic regression). Conversely, more recent works moved\ntowards unsupervised approaches. Kulshrestha et al. (2017) proposed a system where the\nleaning is obtained by measuring the similarity between the topic vectors of users, with\nthose of known seed democrats and seed republicans. The political leaning was provided for\neach user in the [0, 1] continuous range. This system is unsupervised, however it requires\nknown sets of seed users, raising the question as to how to obtain such sets. Moreover, an\nadditional challenge to face when developing systems for predicting continuous (rather than\nbinary or crisp) leanings, is the lack of ground-truth values for training or evaluating the\nsystem.\n2.2 Network-Based Approaches\nApproaches purely focused on network characteristics currently represent only a minority\nof existing works.\nBarberá (2015) built a Bayesian spatial model of the Twitter social\nnetwork that is based on homophilic network properties. The political leaning of each user\nis determined via Ideal Point Estimation. Similarly, Bond and Messing (2015) exploited user\nlikes to Facebook pages to obtain estimates of political ideology for both parties, politicians,\nand ordinary users. Estimates are computed via Singular Value Decomposition (SVD) of an\nagreement matrix, which corresponds to a normalized adjacency matrix derived by projecting\nthe bipartite matrix of user likes to parties onto the set of parties. Instead, Wong et al. (2016)\ncomputed political leaning by solving a convex optimization problem. By leveraging Twitter\ndata, the objective function embeds signals derived from both the analysis of retweeting\nbehaviors and features of the retweet networks.\nThese previous works are unsupervised\nand provide leaning estimates in the [0, 1] continuous range. Notably, these works, as well\nas all others that output one-dimensional scores, can only be applied to bipolar systems\n(e.g., to binary prediction tasks). This means that they are not suitable for application\nto the detection of ﬁne-grained political leaning, a task that demands the prediction of\nmultiple classes (i.e., the possible political parties), nor to the detection of coarse-grained\npolitical leaning in those systems that have more than two poles. An example of the latter\nis the current Italian political system (Pasquino, 2019), to which we apply our proposed\nmethodology. The usefulness of the so-called left-right scale, operationalized as the [−1, 1]\nor [0, 1] continuous range, is also questioned by Bauer et al. (2017), who found that diﬀerent\nindividuals assign diﬀerent meanings to the “left” and “right” concepts. As such, estimates\nbased on a unique left-right scale for all individuals risk being biased and inaccurate. More\nbroadly, Bauer et al. also raised the issue of self-reports, such as those obtained from survey\nrespondents, as a ground-truth for training automated systems. In fact, many recent studies\nuncovered severe biases in self-reports, which motivates research on alternative means of\nobtaining ground-truth measurements (Bastick, 2021; Verbeij et al., 2021).\n638\nFine-Grained Prediction of Political Leaning on Social Media\nA recent interactions-based state-of-the-art unsupervised approach is presented by Dar-\nwish et al. (2020). Authors built user representations based on the users they retweeted.\nThen, they experimented with several projection and dimensionality reduction techniques,\nsuch as t-SNE and UMAP. Finally, they clustered projected users and labelled clusters via\nmanual inspection. As a result of this process, each user is assigned to the label of the clus-\nter to which it belongs. The system presented by Darwish et al. (2020) has been employed\nalso for predicting the political bias of media outlets and famous public characters (Ste-\nfanov et al., 2019), and to estimate the polarization of Twitter users with respect to certain\ndebated topics and political issues (Darwish, 2018).\nThe aforementioned work is the most similar existing solution with respect to our present\ncontribution. However, contrarily to (Darwish et al., 2020), we do not explicitly exploit\nretweets between users, but we rather leverage the noisy textual content of their tweets.\nConsequently, a crucial component in our solution is the deep learning network used to learn\nlatent user representations from tweets. In addition, we make diﬀerent choices with respect\nto the techniques used for dimensionality reduction, projection and clustering. Finally, we\nautomatically label clusters based on the labels of the pivots contained in each cluster, rather\nthen with manual intervention. In our work, we also evaluate systems on a more challenging\ntask than that tackled by Darwish et al. (2020) (e.g., binary classiﬁcation), demonstrating\nand discussing the advantages of our solution.\n2.3 Mixed Approaches\nAnother large body of work is based on a combination of content and network analysis. The\nadvantage of simultaneously exploiting both textual content and network representations,\nsuch as those resulting from user interactions, was recently motivated and quantiﬁed by Al-\ndayel and Magdy (2019). Speciﬁcally, they found that several diﬀerent dimensions of online\nproﬁles and activities can provide useful signals to predict stance and leaning. Among them,\nsome of the most informative signals can be extracted from user posts, user interactions with\nother users, websites visited, and user likes to other content on the platform.\nAmong the ﬁrst works to jointly exploit content and interaction networks is (Conover\net al., 2011). Authors exploited features derived from hashtags and from the retweet net-\nwork, in a supervised binary classiﬁcation task. Similarly, also Pennacchiotti and Popescu\n(2011) focused on supervised binary political classiﬁcation. Their system is fed with fea-\ntures encompassing proﬁle, tweeting behavior, linguistic, social, and interaction network\ninformation. Being based on supervised classiﬁcation, both previous works still suﬀer from\nthe limitations outlined by Cohen and Ruths (2013) and can only provide a dichotomic\nestimate of polarity. The work by Lahoti et al. (2018) instead provided interesting advances\non this task. It is a state-of-the-art unsupervised framework based on non-negative matrix\nfactorization, which learns a shared latent space between users and content. Similarly to\nother already-surveyed works, political leaning is considered as a one-dimensional continuous\nvariable in the [0, 1] range. However, the framework can be used to model more than one\nvariable at a time (e.g., ideology, popularity), which represents an interesting improvement\nover previous works.\nOne of the limitations of network-based and mixed approaches is the need for explicit so-\ncial relationships or user preferences (e.g., likes, retweets). In fact, it has been demonstrated\n639\nFagni & Cresci\nthat extracting these information is a data- and time-demanding task and that such infor-\nmation is not always available (e.g., due to platform data-access restrictions) (Cresci et al.,\n2015). In turn, this decreases the applicability of such techniques and hinders large-scale\nsocial media analyses. Contrarily, our proposed technique achieves comparable or better\nperformances while only exploiting the textual content of user posts, which are readily\navailable.\n3. Preliminaries and Data\nThis section provides preliminary information on the political landscape in which our anal-\nyses take place. Furthermore, it provides details on our dataset and its labeling.\n3.1 The Italian Political Landscape\nWe focus our study on politically-active Italian Twitter users. Thus, our aim for this work is\npredicting the leaning of Italian Twitter users, within the current Italian political spectrum.\nBefore delving into the details of our methodology, we ﬁrst outline the Italian political\nlandscape as of November 2020.\nThe last Italian general elections were held in March 2018, and resulted in the populist\nparty Five-star Movement (M5S) winning the election with 32.7% votes, followed by the\ncenter-left Democratic Party (PD) with 18.7% votes and the far-right League (LE) party that\nobtained 17.4% votes. Despite receiving slightly more votes than LE, PD is considered one of\nthe losers of the election, since it dropped from 40.8% votes received at the 2014 European\nelections, to 18.7% in 2018. The last major Italian party is the center-right Forward Italy\n(FI) that obtained 14.0% votes. Based on this outcome, a coalition government was formed\nin May 2018 by M5S and LE. This lasted until August 2019, when a government crisis\ninitiated by LE led to the formation of a new coalition government in early September. This\ngovernment, which is still in charge at the time of writing, is led by M5S and PD, together\nwith other minor parties7.\nThe peculiarity of the current Italian political landscape is\nrepresented by the populist and anti-establishment M5S, whose members refuse to position\nin the traditional left-right bipolar paradigm since they regard M5S as a non-party8. As a\nconsequence, the coarse-grained Italian political landscape is a tripolar system consisting of\nright-leaning parties, left-leaning parties, and the M5S (Pasquino, 2019). Notably, carrying\nout predictions of political leaning in a tripolar system has implications on the techniques\nused for the analysis, since some of the existing ones have been speciﬁcally designed for\nbipolar systems (e.g., left vs right, liberals vs conservatives, in favor vs against a given\ntopic).\nIn addition to the aforementioned parties, in this study we also consider 4 minor parties\nthat together accounted for 8% votes in the 2018 general elections, thus covering the whole\nextent of the Italian political spectrum and including both major and minor parties. Table 1\nsummarizes the main information, name and color conventions for all considered parties and\ntheir leaders. Henceforth, we refer to the party Twitter accounts as our pivots, since they\n7. https://en.wikipedia.org/wiki/Conte_II_Cabinet\n8. https://en.wikipedia.org/wiki/Five_Star_Movement\n640\nFine-Grained Prediction of Political Leaning on Social Media\nleaning\nparty name\nparty handle\nleader handle\nlabel\ncolor\n#users\nRIGHT\nCasaPound Italy\n@casapounditalia\n@distefanoTW\nCPI\n2,997\nBrothers of Italy\n@FratellidItaIia\n@GiorgiaMeloni\nFdI\n2,507\nLeague\n@legasalvini\n@matteosalvinimi\nLE\n2,705\nForward Italy\n@forza_italia\n@berlusconi\nFI\n746\nM5S\nFive-star Movement\n@Mov5Stelle\n@luigidimaio\nM5S\n3,206\nLEFT\nDemocratic Party\n@pdnetwork\n@nzingaretti\nPD\n2,377\n+Europe\n@piu_europa\n@bendellavedova\n+E\n4,335\nCommunist Ref.\n@direzioneprc\n@maurizioacerbo\nPRC\n1,326\nTable 1: Information about the 8 Italian parties, and their leaders, considered in this study.\nRows are grouped according to the coarse-grained political leaning, representing the tripolar\nItalian political system.\nplay an important role in the estimation of political leaning. Notably, the only preliminary\ndata needed by our framework are (i) the pivots, and (ii) their coarse-grained leaning.\n3.2 Twitter Dataset\nOur aim for this work is to develop a framework for estimating political leaning in an un-\nsupervised fashion (i.e., with no manual labeling involved). To combine the strengths of\nlabeled datasets (e.g., rich, high-quality data) with those of unsupervised approaches (e.g.,\ngeneralizability, no bias or errors due to manual labeling), our desiderata is to acquire a\ndataset that is implicitly labeled, with respect to political alignment. We met our desider-\nata by leveraging favorited (i.e., liked) tweets, and by considering political likes as proxies\nfor political leaning. Other options, also adopted in some previous works, could have in-\nvolved the exploitation of retweets or follower relationships to political parties. However, we\nconsider likes to be stronger indicators of political preference (Aldayel & Magdy, 2019).\nOperationally, we ﬁrst crawled the Twitter timelines of our pivots. Then, for each col-\nlected tweet, we obtained a list of users that liked that tweet. At the end of this process\nwe obtained a bipartite graph linking 20,199 users to our 8 considered parties, based on\nexplicit user likes to party tweets. The number of users that liked at least one party tweet\nis reported in the last column of Table 1, for every party. We completed our data collection\nby crawling the most recent 200 tweets from the timelines of all 20,199 users, which resulted\nin more than 3.6M tweets, in total. When building the dataset, we only included users\nwhose timeline contained at least 25 tweets. For each user, we collected at most up to 200\ntweets. This data collection process roughly covered the months of August to early October\n2019. On average, user timelines include 179.3 tweets, evenly distributed during our data\ncollection period. Finally, we performed a stratiﬁed sampling to split our dataset into a\ntraining (90% – 18,169 users), a validation (3% – 604 users) and a test (7% – 1,426 users)\npartition. As a result of our splitting strategy, the distributions of parties and poles across\nthe 3 data partitions are comparable.\n641\nFagni & Cresci\nFigure 2: Louvain clustering of the weighted user-similarity network.\nEdge weights are\nbased on user likes to party tweets. Clusters are color-coded and each cluster is associated\nto a political party. User labels resulting from this clustering are used as ground-truth for\nevaluating predictions of user political leaning.\n3.3 Ground-Truth Labeling\nSince we do not know the preferred party of the users in our dataset, we obtain a ground\ntruth for our task by leveraging user likes to party tweets. Speciﬁcally, we ﬁrst build the\nbipartite graph of users and party tweets, where links between nodes represent user likes to\nparty tweets. Next, we project the bipartite graph onto the subset of user nodes, obtaining\na weighted, undirected user-similarity network. Links in this network represent similarity\nbetween users. In order to build this network and to compute the similarity between users,\nwe adopt a simple weighting scheme based on the frequency of common associations in the\nbipartite graph. In other words, the similarity between two users is measured as the number\nof tweets liked by both users. Finally, we cluster users in this network with the Louvain\ncommunity detection algorithm (Blondel et al., 2008). Each user is then labeled with the\npolitical party corresponding to the cluster it belongs to.\nFigure 2 shows the clustered\nuser-similarity network derived from our dataset. Clusters are color-coded and determine\nthe ground-truth label for each user.\nAs shown, in this representation user clusters are\nsharply deﬁned. The vast majority of users only has edges connecting to other users of the\nsame cluster, with only a few edges connecting users across diﬀerent clusters. In turn, this\nimplies that user likes to party tweets are a very strong signal of political alignment. In\nthe following, we describe our approach for the challenging task of inferring user political\nleaning from tweets, which represent a much more noisy signal than likes.\n642\nFine-Grained Prediction of Political Leaning on Social Media\n4. Learning Latent Political Ideologies\nDetermining users political leaning from the analysis of the content posted on social media\nis a challenging task. One challenge stems from the need to ﬁnd a clever way to focus the\nanalysis on politically-relevant content only. Indeed, a typical user’s timeline is ﬁlled with\nposts related to several diﬀerent topics (e.g. sport, spare time, work, politics) that embrace\nall aspects of the user’s life.\nThe ﬁrst diﬃcult step is therefore related to splitting the\nrelevant contents (i.e., those related to politics) from the rest of the messages that, within\nthis context, simply represent noisy and unhelpful data for inferring users political leaning.\nA second critical aspect, given a post covering political topics, is to properly measure how\nsuch message is politically close to the typical ideology of a speciﬁc party or pole. This\nmeasure can predict how much the post is in agreement toward a speciﬁc ideology, so having\naccess to enough messages in a user’s timeline that convey this information can contribute\nto accurately estimate its ﬁnal political leaning.\nWe sorted out both these critical issues by proposing a novel unsupervised process or-\nganized in seven high-level steps, shown in Figure 3. In step 1 we build an automatic tweet\nclassiﬁer for assessing if a tweet has been produced by a certain political party. Details on\nhow the classiﬁer is trained are given in Section 4.1. In steps 2, 3, and 4 we leverage the\nclassiﬁer to compute vector representations for users and parties. We exploit representa-\ntions of users and parties to identify a subset of users that are particularly similar to the\nconsidered parties. These steps of our methodology are described in Section 4.2. In step\n5 we automatically analyze the tweets from the subset of users whose representations are\nsimilar to those of the parties. In particular, for each of such users we select a subset of\nhis/her tweets that conveys explicit political opinions. In step 6 we use these user-generated\ntweets as additional training examples in a second training phase of our tweet classiﬁer,\nsince they represent political tweets with diﬀerent characteristics than those already seen by\nthe classiﬁer (i.e., those obtained from the oﬃcial party accounts rather than from ordinary\nusers). The ﬁnal classiﬁer is used in step 7 to compute the ﬁnal vectors of users and parties.\nEach computed vector corresponds to the latent representation of a user. In other words,\nlearned vectors allow to position users in a shared latent political ideology space. Steps 5,\n6, and 7 are described in Section 4.3. Finally, in Section 5 we describe how we leverage the\nrelative positions of users in the latent political ideology space to infer the preferred party\nfor each user.\n4.1 Predicting the Political Relevance of a Tweet\nIn this step we are interested in measuring the degree of agreement of a relevant political\ntweet with the typical political ideology of each party involved in this study. Before describ-\ning how this step works, it is helpful to deﬁne what a relevant tweet is. We deem a tweet to\nbe politically relevant if it expresses a subjective opinion in favour or against a speciﬁc party,\na party leader, a speciﬁc person, or a political position ideologically known to be near to a\ncertain party. In this context, possible examples of relevant tweets are unquoted retweets of\ntweets posted by political parties or leaders, unquoted retweets of messages of other users\nwhere they express a political opinion on something, tweets replaying to political leaders\nwhere a user shows its appreciation or a negative attitude toward the author of message.\n643\nFagni & Cresci\nTweets from\npolitical\nparties\nTrain \nparty classiﬁer\nStep\n1\nParty classiﬁer\nTweets from\npolitical\nparties\nParty classiﬁer\nGenerate party\nvectors\nParty vectors\n(pivots)\nTweets from\ngeneric users\nParty classiﬁer\nGenerate user\nvectors\nUser vectors\nParty vectors\n(pivots)\nUser vectors\nCompute most similar\nusers to political parties\nand automatically label\nthem\nMost similar users\nParty classiﬁer\nTweets from\nmost similar\nusers\nIdentify tweets  with high\nrelevance to political\ncontents and annotate\nthem with corresponding\nuser's label\nEnrichment\ntweets\nTweets from\npolitical\nparties\nEnrichment\ntweets\nTrain \"enriched\" party\nclassiﬁer\nEnriched party\nclassiﬁer\nStep\n2\nStep\n3\nStep\n4\nStep\n5\nStep\n6\nTweets from\npolitical\nparties\nTweets from\ngeneric users\nEnriched party\nclassiﬁer\nGenerate ﬁnal  user\nand party vectors\nStep\n7\nVectors (users and parties)\nto be mapped directly to\nideology space\nFigure 3: High-level overview of the proposed unsupervised strategy to map users to a latent\npolitical ideology space.\nGiven a tweet, it is possible to quickly determine if the text is politically relevant by\nleveraging an automatic multiclass classiﬁer which has learned, from examples of political\ntweets, to predict if a tweet has been produced by a speciﬁc political party. Indeed, such\nclassiﬁer should be able to assign not only proper labels (i.e., the most probable party that\ncould have produced that tweet) but also to estimate a conﬁdence in its decision which can\nbe seen as a “relevance level” of the tweet with respect to political topics9. Such type of\nsolution normally requires a manual annotated dataset. Here, we obtained the same result in\n9. The higher is the conﬁdence score of the classiﬁer, the higher is the probability that the tweet content is\nexpressing something which is politically relevant.\n644\nFine-Grained Prediction of Political Leaning on Social Media\n@user  #today is a great day!\n@\nu\n...\ny\n!\n2\n3\n...\n45\n46\n0.23, 0.13, ..., 0.23\n0.23, 0.45, ..., 0.43\n0.11, 0.12, ..., 0.15\n0.67, 0.05, ..., 0.09\n0.19, 0.41, ..., 0.63\n0.23, 0.22, ..., 0.01\n0.41, 0.39, ..., 0.22\n0.22, 0.44, ..., 0.49\nRows activated into\nembeddings matrix are\npassed to next layer\nembeddings size = 32\nCNN 1D (256 ﬁlters, window = 3)\nTransformer (heads = 2, ﬁlters = 256)\nGlobal Max Pooling + dropout (0.2)\nDense (size = 32)\nInput \ntweet\nTweet \ncharacter\nencoding\nEmbeddings\nmapping\nSpatial\nfeatures\nTemporal\nfeatures\nInformative\nfeature\n selection\nFeatures \ncompression\nFinal party\nclassiﬁcation\nPRC\n+E\nPD\nM5S\nFI\nLE\nFdI\nCPI\nEmbeddings\ntable\nFigure 4: Neural network architecture of our party classiﬁer.\nan unsupervised way by exploiting the implicit relationship between tweets and the Twitter\naccounts that have produced them. In particular, we focus on the oﬃcial accounts of the\n8 considered parties and we use their timelines to automatically build a labeled dataset.\nIn this way, each tweet posted by party account P is labeled as generated by political\nparty P and the problem we solve is the prediction of the party that produced a tweet\nonly based on the textual content of the tweet itself. Our approach thus resembles labeling\nschemes by distant supervision (Marchetti-Bowick & Chambers, 2012). Building our dataset\nby focusing on party accounts also has two important practical implications that simplify\nsolving our task. The ﬁrst implication is that we are certain that the labels assigned to\ntweets are correct. This allows us to train a classiﬁer on a real gold-standard thus avoiding\nsub-optimal solutions caused by biases and labeling errors introduced during error-prone\nmanual labeling operations (Misra et al., 2016; Pandey et al., 2019). The second and most\nimportant implication is that each considered timeline is “clean” (i.e., not noisy) and contains\nonly politically-relevant tweets10. Such politically-relevant tweets are typically in favour of\nthe party, of its leader, or of some action proposed, and only seldom against another political\ncompetitor.\nBy following the strategy described above, we built a dataset composed by all the tweets\n(in the form of original contents or retweets) posted in the timeline of the considered 8\npolitical parties. For each party we selected the most recent 3,000 tweets. At the end of\nthis process we obtained an almost balanced dataset composed of 23,791 labeled tweets. By\nleveraging these data, we built the political party classiﬁer using the neural network archi-\ntecture shown in Figure 4. We used a character-based encoding to obtain an initial vector\nfor each tweet. Our method uses an embeddings character table which is learned during the\n10. It is extremely rare that an oﬃcial account of a political party posts something which is not related to\npolitics.\n645\nFagni & Cresci\ntraining phase. Each tweet vector is thus mapped into a new vector using the embeddings\ntable and next passed to a CNN layer (LeCun et al., 1998) with the aim of extracting spatial\nfeatures – i.e., those that are invariant to the locations where they occur. This set of fea-\ntures is then processed by a transformer layer (Vaswani et al., 2017) that extracts the most\ninformative temporal recurrent patterns from the data. The gathered information are thus\nﬁltered and compressed before being used to produce the ﬁnal classiﬁcation of the tweet.\n4.2 Extracting a Politically Relevant User Vector\nThe party classiﬁer built in the previous section can be employed to extract high level\nfeatures that express the political attitude of a user with respect to all parties. In partic-\nular, by processing the entire timeline of a user with such classiﬁer we can identify which\ntweets are politically relevant (i.e., tweets classiﬁed with medium/high conﬁdence scores)\nand which political parties the user’s opinion aligns with.\nMore formally, let us deﬁne\nTU =\n\b\ntU\ni | i = 1, ..., min (200, |TU|)\n\t\nas the timeline of user U where ti is its i-th most\nrecent tweet and |TU| is the number of tweets available in the whole timeline of U. Let us\nalso declare P = {PRC, +E, PD, M5S, FI, LE, FdI, CPI} as the set of the 8 considered parties\nsuch that P1 = PRC, P2 = +E, and so on. We deﬁne the party classiﬁer as the function C\nmapping a tweet t to a score vector as in the following:\nC : t ∈R280 →[sP1, sP2, . . . , sP8] ∈R8\n(1)\nwhere sPi ∈[0, 1] is the score assigned by classiﬁer C to the tweet t for party Pi. Given\nthe timeline of user U, we can compute SU,i,k as the set of the best k scores obtained for a\nspeciﬁc party Pi, as in the following:\nSU,i,k = {maxk {C (t)i} | t ∈TU} ∈Rk\n(2)\nGiven the previous deﬁnitions, we can ﬁnally deﬁne how to extract a politically relevant\nuser vector:\nV k\nU = [SU,1,k, SU,2,k, . . . , SU,8,k] ∈R8k\n(3)\nThe vector V k\nU is a concatenation of the best tweet scores measured on the relevance to\neach party, which is indicative of the interests and the leaning shown by the user toward\na speciﬁc political ideology. In this work, we ﬁxed k = 5 in Equation (3) based on early\nexperimentation demonstrating this value to yield reliable measures of the degree of interest\nshown by a user for a speciﬁc political party. Indeed on the one hand, a larger k would\nrequire the user to post a lot of political content in order not to penalize excessively the\nweight of a speciﬁc political stance. On the other hand, a smaller k would require to have\nan extremely accurate party classiﬁer.\n4.3 Unsupervised Data Enrichment to Improve Tweet Party Classiﬁcation\nAs for all supervised classiﬁers, the party classiﬁer built in Section 4.1 works well when\nanalyzing tweets whose writing style is similar to that of tweets used in the training dataset.\nIn particular, using party accounts as positive seeds in the dataset construction phase, poses\nsome limitations to the learned classiﬁer for correctly handling the true tweet distribution.\n646\nFine-Grained Prediction of Political Leaning on Social Media\nIndeed, oﬃcial party tweets are typically written in a clean, formal and institutional lan-\nguage. In addition and as previously anticipated, they also typically provide facts in support\nof the work of the party or of its political leader. On the contrary, political tweets from aver-\nage users have diﬀerent linguistic characteristics. Their writing style is informal and tweets\ncontain abbreviations, slang, and jargon expressions. Regarding the opinions conveyed in a\ntypical user tweet, sometimes users support a political party or leader. However, oftentimes\nusers also express strong disagreement toward an opposing political opinion or politician. In\nparticular, a considerable set of users tend to provide more destructive opinions (e.g., harsh\ncomments against someone or something) than constructive ones (Nizzoli et al., 2021). In a\nfew edge cases, the political opinions expressed in a user’s timeline are exclusively against\nsomething or someone. Because of this, it is important to transfer such nuances to the party\ntweet classiﬁer during its training phase, in order to be able to infer accurate user political\nideologies.\nGiven these motivations, here we propose an unsupervised strategy to enrich original\ntraining data with labeled tweet examples coming from all types of users. This enrichment\nprocess is aimed at providing also negative tweet examples to the tweet party classiﬁer,\nin addition to the positive tweets from the oﬃcial parties, and can be summarized in the\nfollowing steps:\n1. Obtain the vector representations for the pivot (i.e., party) accounts. This can be\nachieved with Equation (3).\n2. Select those training-set users that are most similar to each party account.\n2.1. For each training-set user, we obtain its vector representation with Equation (3)\nand we compute its cosine similarity with respect to the vector of each party.\n2.2. For each party, we sort users based on their similarity and we select users laying\nabove the 99-th percentile of the similarity distribution (i.e., the most similar\nones). We automatically assign the label of the party to each user matching this\ncondition.\n3. Select tweets to be used as an enrichment for training the tweet party classiﬁer. To\nreach this goal, we analyze the timelines of the users selected at the previous step. For\neach selected user, we use the party classiﬁer C to predict the political relevance of\nall the tweets in the user’s timeline. Then, we retain only those tweets for which C\nyielded a score sPi ≥Th for at least one party Pi. For large values of the threshold\nTh, this results in selecting only those tweets for which our classiﬁer provided strong\npredictions. Such tweets are used as enrichment tweets in a second training phase of\nthe classiﬁer. The ground-truth label assigned to those tweets is that of its author,\nassigned at step 2.1 of this procedure.\nNotably, this label that we inferred in an\nunsupervised fashion is likely to be correct since we are considering users that are very\nsimilar to a given party in the political ideology space. Overall, this process allows to\nexpand the training set by ingesting tweets from average users in addition to those of\noﬃcial party accounts, while still retaining a high conﬁdence of the new tweets’ labels.\n4. Build a new party classiﬁer C′ using both the original training dataset and the enrich-\nment data, using the same architecture shown in Figure 4. As a consequence of the\n647\nFagni & Cresci\noriginal tweet\ntranslated tweet\nLc\nLo\n√\n@Mov5Stelle Invece di tagliare la rappre-\nsentanza, bastava dimezzare gli stipendi.\nUna legge ordinaria, sicuramente più ve-\nloce come iter di una legge costituzionale.\n@Mov5Stelle Instead of cutting the rep-\nresentation, it was enough to halve the\nsalaries. An ordinary law, a procedure cer-\ntainly faster than a constitutional law.\nPRC\nM5S\n√\nSalvini chiede I pieni poteri (!!!), sappi-\natelo...\nSalvini asks for full powers (!!!), you should\nbe aware of this...\n+E\nLE\n√\nRT @gennaromigliore: A Genova la polizia\nrompe le ossa al cronista #StefanoOrigone\ne protegge le canaglie di #CasaPound:\nbisogna sanzionare...\nRT @gennaromigliore: In Genoa the po-\nlice breaks the bones of the reporter #Ste-\nfanoOrigone and protects the villains of\n#CasaPound: you have to punish...\nPD\nCPI\n√\n#CasaPound\nSi\narriverà\ndavvero\nallo\nsgombero dei \"fascisti del Terzo Millen-\nnio\" dal palazzo di 6 piani che hanno oc-\ncupato abusivamente da 15 anni nel centro\ndi Roma?\n#CasaPound Will there really be the evac-\nuation of the \"fascists of the Third Mil-\nlennium\" from the 6-storey building they\nhave squatted in the center of Rome for 15\nyears?\nM5S\nCPI\n√\n@dariofrance @nzingaretti Sarebbe un gov-\nerno peggio di questo. Peggio del #pd non\nci sta niente in circolazione!\n@dariofrance @nzingaretti It would be a\nworse government than this.\nThere is\nnothing around that is worse than #pd!\nFI\nPD\n√\nUn Altro PD Idiota contro Salvini ciabat-\ntoni\nAnother PD-idiot against Salvini moron\nLE\nLE\n√\nEnnesimo strafalcione geograﬁco per il\n#M5S: questa volta il vento del cambia-\nmento sposta addirittura le regioni! Spe-\nriamo i fondi arrivino davvero in #Molise,\nsenza nulla togliere alle #Marche..\nYet another geographical blunder for the\n#M5S: this time the wind of change even\nmoves the regions! We hope the funds re-\nally arrive in #Molise, without detracting\nanything from the #Marche..\nFdI\nM5S\n√\n@virginiaraggi La compatisco. Fra un paio\nd’anni, allo scadere del suo mandato, lei\nﬁnirà nell’oblio come merita. CasaPound\nsarà sempre al suo posto.\n@virginiaraggi I sympathize with you. In\na couple of years, at the end of your term,\nyou will be forgotten as you deserve. Cas-\naPound will always be in its place.\nCPI\nCPI\n⊘\nLe chiacchiere fanno i pidocchi, i mac-\ncheroni riempiono la pancia\nThe chatter makes the lice, the macaroni\nﬁll the belly\nFI\nCPI\n⊘\nIn bocca al lupo alle ragazze e ai ragazzi\ndella mia commissione. Domattina si parte\n#notteprimadegliesami\nGood luck to the girls and boys of my\ncommission. Tomorrow morning we begin\n#nightbeforeexams\nPD\nCPI\nTable 2: Examples of tweets included in the enriched training-set of the party tweet classiﬁer.\nPolitically-relevant tweets are marked with √, irrelevant ones are marked with ⊘. The label\ninitially assigned by C to each example is reported in column Lo, while the label corrected\nthrough the usage of the minimal distance from a party account is reported in the Lc column.\nenriched training-set, the new classiﬁer C′ is more accurate than C, especially with\nregards to informal and negative political tweets.\nIn our preliminary experiments, we found that Th = 0.5 is a reasonable value to get\nboth several thousands of new labeled examples and a large variety of new tweets featuring\nsubstantial diﬀerences in their writing style, with respect to the typical tweets of an oﬃcial\nparty account. In detail, by applying the aforementioned process: (i) in step 2 we selected\n1,462 users equally distributed among all parties, and (ii) at the end of step 3 we obtained an\nenrichment dataset composed of 8,753 new labeled tweets. As shown in Table 2, this method\nis obviously not perfect but it ensures to obtain a plentiful variety of diﬀerent examples that\n648\nFine-Grained Prediction of Political Leaning on Social Media\n(a) Our technique.\n(b) Word2vec.\nFigure 5: UMAP projections of the latent political ideology spaces learned by our pro-\nposed technique and via word2vec. Colors encode ground-truth party labels. Larger circles\nhighlight the position of the oﬃcial accounts for each considered party (i.e., our pivots).\nhelp to improve the precision and the generalizability of the enriched party classiﬁer C′, in\nan unsupervised fashion.\n5. Predicting Political Leaning\nBy using the encoding scheme presented in Section 4.2, we are able to analyze the proposed\nmethod from a qualitative point of view and to map each user into a position within a shared\nlatent political ideology space. This mapping is built directly onto user vectors with the aim\nof projecting users over a bidimensional geometric space in such a way to (i) minimize the\ndistance between similar users having close political ideas and (ii) maximize the distance\nbetween users having diﬀerent political opinions. To perform feature reduction and to map\nthe latent user vectors in R40 to an equivalent space in R2, we leveraged UMAP with default\nparameters (McInnes et al., 2018). The mapping obtained from training data11 is shown in\nFigures 5a and 6a, where users are respectively colored according to their party and pole\nlabels.\nRegarding party projections, the ﬁrst observation is that many users supporting a speciﬁc\nparty are concentrated in the neighborhood of the party itself (indicated by large circle\npoints).\nThis is a quite strong indication that user feature representations provided by\nEquation (3) properly describe the political stance of the parties. In addition, when users\nhave enough political content in their timeline, the same method also allows to position them\nnear to their preferred party. This ﬁrst result is veriﬁed for all parties, and particularly so\nfor the left-leaning ones and for the M5S. Regarding right-leaning parties (right-hand side\nof the ﬁgures), although this trend is conﬁrmed, the situation is more ﬂuid with users of\nFI clearly separated from the users of the other 3 right-leaning parties (CPI, FdI and LE).\n11. Here, to better highlight data distribution in the political ideological space, we used training data because\nthe amount of users is far bigger than those contained in test data and the distribution of points is\npractically the same (i.e., there is no drift between training and test data).\n649\nFagni & Cresci\n(a) Our technique.\n(b) Word2vec.\nFigure 6: UMAP projections of the latent political ideology spaces learned by our proposed\ntechnique and via word2vec. Colors encode ground-truth pole labels (right-leaning, left-\nleaning, and M5S). Larger circles highlight the position of the oﬃcial accounts for each\nconsidered party (i.e., our pivots).\nIndeed, supporters of the latter parties, in addition to forming clear clusters positioned\naround oﬃcial party accounts, are spread over wide areas of the political ideology space,\nalso creating regions where users of diﬀerent parties are mixed together. This feature of our\nlearned political ideology space is in agreement with the Italian political landscape, where\nthese 3 far-right parties hold similar stances with respect to many political issues (Pasquino,\n2019), and with the opinions expressed by their electors. By analyzing Figure 5a, it is also\nworth noting that even the central area of the ideology space contains a mixture of users\nbelonging to diﬀerent parties. Also this situation is expected and understandable, since it\nrepresents undecided users and users that hardly share any political content at all.\nWhen considering pole projections shown in Figure 6a, we can see that there is a clear\nseparation between the three poles, with only the central region of the ideology space charac-\nterized by a physiological group of users whose political stance is not uniform, for the same\nmotivations given before. Quite naturally, these qualitative results suggest that the ﬁne-\ngrained task (i.e., party prediction) represents a much more challenging problem than the\ncoarse-grained one (i.e., pole prediction). This naturally results from the minimal diﬀerences\nbetween some of the considered parties.\nFor comparative purposes, in Figures 5b and 6b we show the latent political ideology\nspace obtained with a user encoding based on word2vec (Mikolov et al., 2013) instead of\nthe one learned with our method. Word2vec is a very popular embeddings method that al-\nready demonstrated an excellent encoding power on many NLP tasks. In this case we used\nword2vec algorithm provided by gensim library12 to build from scratch a custom model op-\ntimized for this speciﬁc context by learning the latent space directly from the used Twitter\ndataset. In order to build the new word2vec model, we applied a minimal step of prepro-\ncessing to raw textual data. In particular, we transformed texts into lowercase and removed\n12. https://radimrehurek.com/gensim/\n650\nFine-Grained Prediction of Political Leaning on Social Media\nall stopwords. With the resulting data, we built a word2vec model keeping only the most\nfrequent 50,000 words. Each tweet is thus vectorized by computing the mean of the sum of\nvector embeddings of each word occurring in the text. The user vector is ﬁnally obtained\nby computing the mean of the tweet vectors extracted from the user’s timeline. Diﬀerently\nfrom our method, the word2vec encoding seems unable to clearly separate the diﬀerent po-\nlitical parties, as demonstrated by several regions of the ideology space featuring a mixture\nof users from diﬀerent parties. Another major drawback of this approach is represented\nby the vicinity between the accounts of several diﬀerent parties. While in Figure 5a each\npivot held a speciﬁc position in the ideology space, clearly separated from that of other\nparties, in Figure 5b several pivots end up laying in the same area of the ideology space,\nwhich inevitably hinders party separability and the prediction of users’ political leaning.\nRegarding pole predictions, the situation improves. However, it does not reach the level of\ndata separation obtained with our proposed technique. In summary, these ﬁndings suggest\nthat word2vec encoding, in this particular context, is sub-optimal and not able to properly\nmodel the semantics of political ideologies of the diﬀerent parties.\nBased on the favorable properties of our learned latent political ideology space, the\nunsupervised prediction of user political leaning can be achieved by applying a clustering\nalgorithm directly to the bidimensional projected user vectors. Without loss of generality,\nin this work we assume that we know the number of clusters we want to obtain at the end\nof clustering process (i.e., 8 clusters for the party prediction task and 3 clusters for pole\nprediction task). The steps needed by the clustering process are summarized the following:\n1. Projection of the users into a new bigger latent space based on the similarity of users.\nEach distinct user is seen as a separated feature in this new space and the feature\nvector of each user is generated by computing its pairwise distance to all the other\nusers. This step was originally proposed by Darwish et al. (2020) and demonstrated\nto improve the subsequent clustering step.\n2. Feature reduction using UMAP to prevent the curse-of-dimensionality due to data\nsparseness (Domingos, 2012).\n3. Feature standardization by subtracting the mean and scaling the features to unit\nvariance.\n4. Data clustering using the KMeans, GaussianMixture, or MeanShift algorithms. This\nclustering step is based on the implementations provided by the sklearn Python soft-\nware package13.\nThe ﬁrst 3 steps of the above list are optional and can be used only in speciﬁc cases\nwhere they improve clustering accuracy. For the experiments reported in the next section,\nwe followed the approach used by both Darwish et al. (2020) and Di Giovanni et al. (2018),\nand we evaluated diﬀerent conﬁgurations on the validation partition of the dataset. Then,\nwe used the best conﬁguration obtained on the validation set to label users of the test set.\nThe details about the speciﬁc conﬁgurations that we used are given in Section 6.1.2.\n13. https://scikit-learn.org/stable/\n651\nFagni & Cresci\n6. Experiments and Results\nGiven the previously described method for predicting the political leaning of social media\nusers, in this section we evaluate its predictions for test-set users of our dataset, with respect\nto the ground truth labels and to the predictions yielded by a number of baselines and other\nstate-of-the-art techniques. Furthermore, we also validate our method by applying it to\npredict the political leaning of the Italian members of the European parliament (MEPs).\nFinally, we carry out a set of additional experiments to assess the sensitivity of our method\nto a number of relevant factors (e.g., distance from the pivots in the latent ideology space,\nnumber of tweets, number of retweets) and we provide a thorough analysis and discussion\nof our classiﬁcation errors.\n6.1 Experimental Settings\nThis section provides details on the evaluation settings and on the techniques used in our\nperformance comparisons.\n6.1.1 Evaluation\nAll techniques considered in our experiments are evaluated on two tasks. The aim of the\nﬁrst task is the prediction of ﬁne-grained political leaning, which concerns with associating\neach user to its preferred political party. The second – simpler – task is the prediction of\ncoarse-grained political leaning, where each user is assigned to a political pole (e.g., left-\nleaning, right-leaning, or leaning towards M5S). Similarly to previous work, we evaluate\neach task as a multiclass classiﬁcation task. However, our experiments are considerably\nmore challenging than those typically performed in previous works, due the larger number\nof involved classes. In fact, previous methods for predicting political leaning were typically\nevaluated in a binary classiﬁcation setting (e.g., predicting left- vs right-leaning users). Here\ninstead, our ﬁne-grained task encompasses 8 possible classes, while our coarse-grained task\nadmits 3 classes. Given the moderate class imbalance for both the ﬁne- and coarse-grained\ntasks, visible in Table 1, for each evaluated method we report both the micro and macro\nversions of precision, recall and F1-measure.\n6.1.2 Comparisons\nIn the upcoming sections, we report experimental results for several techniques, including\ndiﬀerent conﬁgurations of our present proposal, strong and weak baselines, and other state-\nof-the-art techniques. In the following, we brieﬂy introduce each technique that we imple-\nmented and evaluated, starting from 3 interesting conﬁgurations of our proposal. Wherever\nmeaningful, each technique is labeled by separately specifying the approach used for learning\nideologies and that used for making predictions (i.e., ideologies + predictions).\nParties + clustering: This method is based on the latent ideologies learned with our proposed unsu-\npervised approach. For learning ideologies, we apply only the steps described in Sections 4.1\nand 4.2, without the enrichment step introduced in Section 4.3. Predictions are performed\nvia clustering, as detailed in Section 5, by applying step 1, step 2 with a feature reduction\nto 64 features, and step 4 using GaussianMixture with default parameters. These clustering\nsettings are used for both prediction tasks.\n652\nFine-Grained Prediction of Political Leaning on Social Media\nParties enriched + distance: For this method we use unsupervised ideologies learned with all 3 steps\ndescribed in Section 4, thus also including the enrichment step. Predictions are obtained by\nassigning each user to the party of the pivot the user is nearest to. This method represents\na strong unsupervised baseline that leverages our learned ideologies, combined with a simple\nprediction strategy.\nParties enriched + clustering: This is our most complete method. It is fully unsupervised, it leverages\nall steps for learning latent ideologies as well as clustering for obtaining predictions. The\nclustering process for the party prediction task is performed by applying only step 3 and step\n4 using KMeans as the clustering algorithm. For the pole prediction task, we used instead\nstep 1, step 2 with a feature reduction to 64 features, and step 4 using the KMeans algorithm.\nIn addition to our 3 unsupervised contributions reported above, we also experimented\nwith a number of baselines and other approaches, which we brieﬂy describe in the following.\nRandom classiﬁer: Simple unsupervised baseline that outputs random predictions.\nMajority classiﬁer: Simple supervised baseline that always outputs the majority class.\nWord2vec + clustering: Unsupervised method where latent ideologies are learned by leveraging word2vec\nembeddings, while predictions on both tasks are obtained via clustering applying only step 4\nwith the KMeans algorithm.\nRetweets + clustering: This method implements the state-of-the-art unsupervised technique pro-\nposed by Darwish et al. (2020). It ﬁrst learns user representations based on user retweets, then\nit obtains predictions on both tasks via clustering by applying step 1 and step 4 implemented\nwith the MeanShift clustering algorithm.\nSupervised enriched + clustering: This method is similar to the parties enriched + clustering one,\nwith the exception of how the enrichment step is carried out. To this end, this method feeds\nback into the tweet classiﬁer those tweets for which the classiﬁer outputted wrong predictions\ndespite having a high conﬁdence. Given that this method exploits ground-truth labels for the\nenrichment step and clustering for obtaining predictions, it is classiﬁed as semi-supervised.\nThe clustering process is performed by applying step 1, step 2 and step 4. On the party\nprediction task, we reduced features to 64 dimensions and we used KMeans as the clustering\nalgorithm. Diﬀerently, on the pole prediction task, we reduced features to 2 dimensions and\nwe applied the GaussianMixture algorithm with default parameters.\nParties enriched + SVC: This overall semi-supervised method leverages all our proposed steps for\nlearning unsupervised latent ideologies, including enrichment. Then, predictions are obtained\nby training a supervised SVC classiﬁer.\nSupervised enriched + SVC: Here we exploit our supervised enriched ideologies in conjunction with\nan SVC classiﬁer. Both steps used in this method are thus supervised.\nWord2vec + SVC: Ideologies used in this semi-supervised method are obtained via word2vec embed-\ndings, while predictions are yielded by an SVC classiﬁer.\n6.2 Results\nIn the remainder of this section we present and discuss experimental results for the 2 con-\nsidered tasks: coarse- and ﬁne-grained prediction of political leaning.\nWe ﬁrst compare\nresults of our method to those of the several others that we evaluated. While discussing\n653\nFagni & Cresci\nmethod\nmacro\nmicro\nideologies\npredictions\nprecision\nrecall\nF1\nprecision\nrecall\nF1\n–\nrandom classiﬁer\n0.124\n0.125\n0.120\n0.143\n0.126\n0.131\nword2vec\nclustering\n0.128\n0.111\n0.106\n0.171\n0.139\n0.142\n‡\nretweets\nclustering\n0.370\n0.293\n0.301\n0.420\n0.346\n0.354\nour contributions\nparties\nclustering\n0.390\n0.344\n0.342\n0.443\n0.354\n0.372\nparties enriched\ndistance\n0.419\n0.370\n0.324\n0.489\n0.339\n0.330\nparties enriched\nclustering\n0.472\n0.434\n0.426\n0.517\n0.421\n0.426\n‡: (Darwish et al., 2020)\nTable 3: Performance comparison of unsupervised methods for ﬁne-grained (party) predic-\ntion of political leaning. The best result for each evaluation metric is shown in bold font.\nsuch comparisons, we particularly focus on the diﬀerences in performance between our best\nproposal and the technique introduced by Darwish et al. (2020), which is considered the\nstate-of-the-art for unsupervised prediction of political leaning. Then, we measure and dis-\ncuss the performance gap between unsupervised approaches with respect to semi-supervised\nand supervised ones. Finally, we investigate the impact of retweets and distance from pivots,\nfor predicting the political leaning of social media users.\n6.2.1 Prediction of Political Leaning: Unsupervised Approaches\nWe begin by evaluating the performance of unsupervised techniques on the ﬁne-grained\nprediction task. This is the core contribution of our work and results of this evaluation and\ncomparison are shown in Table 3. Our 3 contributions are compared to a random classiﬁer,\nto the technique proposed by Darwish et al. (2020) and to an approach based on word2vec\n+ clustering.\nAll results reported in Table 3 are moderate, at best. This shows the diﬃculty of the\nﬁne-grained task. Among the evaluated techniques, our parties enriched + clustering method\nachieves the best results in each evaluation metric, with micro and macro F1 = 0.426.\nThis is our most complete proposal that takes full advantage of all the steps described in\nSections 4 and 5. The second-best method is parties + clustering, with micro F1 = 0.372 and\nmacro F1 = 0.342. The diﬀerence in performance between these 2 methods is solely due to\nthe enrichment step, that we introduced in Section 4.3, and that allows learning better user\nrepresentations as demonstrated by these results. The state-of-the-art technique by Darwish\net al. (2020) achieves the third-best results, conﬁrming its overall value. Interestingly, the\nparties enriched + distance strong baseline achieves performances that are only slightly worse\nthan those of the previous methods. This further demonstrates the informativeness of the\nlatent user ideologies that we learned. Contrarily, both the word2vec + clustering and the\nsimple random classiﬁer baseline obtain unsatisfactory performances, with micro F1 = 0.142\nand 0.131, respectively. This result is particularly interesting for the word2vec + clustering\napproach. In fact, it suggests that the user representations learned by word2vec in this\ncontext, are not suitable for a prediction step via clustering.\n654\nFine-Grained Prediction of Political Leaning on Social Media\nmethod\nmacro\nmicro\nideologies\npredictions\nprecision\nrecall\nF1\nprecision\nrecall\nF1\n–\nrandom classiﬁer\n0.323\n0.321\n0.310\n0.370\n0.324\n0.339\nword2vec\nclustering\n0.429\n0.426\n0.415\n0.491\n0.480\n0.471\n‡\nretweets\nclustering\n0.804\n0.657\n0.688\n0.758\n0.719\n0.710\nour contributions\nparties\nclustering\n0.599\n0.587\n0.586\n0.665\n0.633\n0.640\nparties enriched\ndistance\n0.758\n0.575\n0.569\n0.728\n0.698\n0.659\nparties enriched\nclustering\n0.751\n0.752\n0.750\n0.776\n0.772\n0.772\n‡: (Darwish et al., 2020)\nTable 4: Performance comparison of unsupervised methods for coarse-grained (pole) pre-\ndiction of political leaning. The best result for each evaluation metric is shown in bold\nfont.\nTable 4 shows the evaluation results of the same methods for the coarse-grained task.\nDiﬃculty-wise, this task is similar to those tackled in previous works (Barberá, 2015; Kul-\nshrestha et al., 2017; Di Giovanni et al., 2018; Darwish et al., 2020). As such, these results\nare comparable to those reported in existing literature. In particular, all methods greatly\nimprove and the best ones obtain rather good performances. As for the ﬁne-grained task, the\nparties enriched + clustering method achieves the best results, with a balanced and promis-\ning micro F1 = 0.772 and macro F1 = 0.750. Also the method by Darwish et al. (2020)\ngreatly improves, reaching the second-best overall result with micro F1 = 0.710 and macro\nF1 = 0.688, and the best macro precision. The 2 other techniques based on our approach\nobtain comparable results, with micro F1 ≈0.65 and macro F1 ≈0.57. Finally, word2vec\n+ clustering and the random classiﬁer again obtain markedly worse results, thus conﬁrming\nthe underwhelming performance already measured for the ﬁne-grained task.\nThe large improvement measured by both our parties enriched + clustering method and\nthe technique by Darwish et al. (2020) between Tables 3 and 4, demonstrate that many of\nthe mistakes made by these techniques in the ﬁne-grained task consisted in misclassifying\na user of a given party to a diﬀerent party of the same pole, rather than to a party to\nthe opposite of the political spectrum. This is expected and is due to the diﬃculty of the\nﬁne-grained task. Furthermore, it explains why the same techniques obtain strikingly better\nresults when evaluated for the prediction of poles instead of parties. Figure 7 helps to clarify\nthis point. It shows the ﬁne-grained confusion matrices of the 2 techniques, together with\nthe marginal distributions of both ground-truth and predicted labels. First of all, the ﬁgure\nclearly highlights that more data points lay on the matrix diagonal in Figure 7a compared\nto Figure 7b. This visually explains the better overall performance of our technique with\nregards to that by Darwish et al. (2020). In addition, it shows the existence of two darker-\ncolored 3 × 3 squares laying in the bottom-right and top-left corner of Figure 7a. To a lower\nextent, the same also occurs in Figure 7b. These regions of the confusion matrices allow\nto visualize the mistakes that we mentioned earlier – that is, wrong party classiﬁcations\nthat become correct predictions when techniques are evaluated pole-wise. The fact that\nthese regions are more visible in Figure 7a than in Figure 7b explains the better results\n655\nFagni & Cresci\n0\n100\n200\n300\n400\nCPI\nFdI\nLE\nFI\nM5S\nPD\n+E\nPRC\nPRC\n+E\nPD\nM5S\nFI\nLE\nFdI\nCPI\n0\n100\n200\n300\n400\nground truth\nusers count\nusers count\npredictions\n0\n25\n50\n75\n100\nusers (%)\n(a) Our technique.\n0\n100\n200\n300\n400\n500\nCPI\nFdI\nLE\nFI\nM5S\nPD\n+E\nPRC\nPRC\n+E\nPD\nM5S\nFI\nLE\nFdI\nCPI\n0\n100 200 300 400 500\nground truth\nusers count\nusers count\npredictions\n0\n25\n50\n75\n100\nusers (%)\n(b) Darwish et al.\nFigure 7: Comparison of the confusion matrices, with marginal distributions, for ﬁne-grained\n(party) predictions between our proposed technique and the unsupervised method by Dar-\nwish et al. (2020). Correct predictions lay on the matrix diagonal.\nin Table 4 of our technique with respect to Darwish et al. (2020), especially regarding the\nmacro F1 metric where we achieve 0.750 vs 0.688. Figure 7 also shows that our technique\nis particularly good at predicting center-leaning parties, such as PD, M5S and FI, for which\nwe obtain almost ﬂawless predictions. Contrarily, we have more diﬃculties in predicting far-\nright and far-left parties. Moreover, both techniques exhibit a bias towards overestimating\nright-leaning parties.\nOn top of that, Darwish et al.\nalso overestimate +E and almost\ncompletely neglect PD and FI.\nOverall, results presented in Tables 3 and 4 and in Figure 7 demonstrate that it is\nvery challenging to distinguish between the diﬀerent parties that lay on the same side of\nthe political spectrum. In order to provide an even more detailed breakdown of our party\npredictions, in Figure 8 we show ground-truth and prediction densities, for each party.\nSpeciﬁcally in each subﬁgure, the scatter plot distribution shows where ground-truth users\nof a given party are positioned within the shared ideology space. Overlaid, the contour\nlines show the distribution of the test-set users predicted by our technique for that party.\nIn other words, the maps of our ideology space shown in Figure 8 somewhat resemble the\nsaliency maps used in computer vision systems for diagnostic purposes (Adebayo et al.,\n2018). Following from our previous explanation, the best results are achieved when the\nhighest contour density perfectly overlaps the bright-colored dots. Examples of this kind\nare Figures 8d, 8e, 8f, 8h. Contrarily, many mistakes are made in those cases where regions\nof bright-colored dots are not contained within any contour line, as in Figures 8c and 8g. In\naddition to highlighting parties for which we are able to yield good predictions and those for\nwhich we are not, Figure 8 also allows to understand some of the reasons for our mistakes.\nFor example it is evident that for each party, the majority of users is tightly clustered in a\nrestricted area of the ideology space. At the same time however, a minority of users appear to\nbe spread out throughout all the ideology space, possibly also invading a region populated\nby members of another party.\nThis represents a limitation of our method for learning\n656\nFine-Grained Prediction of Political Leaning on Social Media\n(a)\nCPI.\n(b)\nFdI.\n(c)\nLE.\n(d)\nFI.\n(e)\nM5S.\n(f)\nPD.\n(g)\n+E.\n(h)\nPRC.\nFigure 8: Comparison between the distribution of ground-truth users and of our predictions,\nwithin the latent ideology space. For each party, the distribution of ground-truth users is\nshown as a density-colored scatter plot. The distribution of our predictions is shown with\ncontour lines.\nideologies or an intrinsic limitation of working with noisy textual data, which inevitably\nresults in wrong predictions at clustering time. For some parties, this drawback is more\nprominent than for others. For instance, a large portion of LE users completely overlaps the\nhighest-density region of FdI. Such users will be erroneously predicted as supporters of FdI\nby our technique. Similarly, despite having a high-density cluster that we correctly detected,\nusers of +E also appear to be spread-out across the top-left quadrant of the ideology space,\nwhich makes it diﬃcult to cluster them all together. For the future, it will be interesting\nto evaluate and diagnose novel techniques for learning latent political ideologies and for\npredicting political leaning, by means of this visualization technique.\n6.2.2 Comparison with Supervised and Semi-Supervised Approaches\nResults presented in the previous section highlighted the advantages of the proposed parties\nenriched + clustering technique with respect to all other unsupervised techniques and base-\nlines. However, previous works showed that the additional information exploited by super-\nvised and semi-supervised techniques (e.g., ground-truth labels of the training-set) typically\nallow to yield better prediction performance compared to unsupervised approaches. Such\nperformance is however hardly generalizable, since it strongly depends on the training set\nused for learning models. As a consequence, performances reported for supervised and semi-\nsupervised techniques often represent overestimations of the capability to predict political\nleaning in the wild (Cohen & Ruths, 2013; Yan et al., 2019).\nFollowing this previous line of research, here we are interested in evaluating the per-\nformance gap between the best unsupervised technique (parties enriched + clustering) and\n657\nFagni & Cresci\nmethod\nmacro\nmicro\nideologies\npredictions\ntype\nprecision\nrecall\nF1\nprecision\nrecall\nF1\nparties enriched\nclustering\n0.472\n0.434\n0.426\n0.517\n0.421\n0.426\nsupervised enriched\nclustering\n0.433\n0.394\n0.392\n0.485\n0.384\n0.411\nparties enriched\nSVC\n0.555\n0.453\n0.474\n0.532\n0.513\n0.500\nword2vec\nSVC\n0.601\n0.468\n0.485\n0.574\n0.554\n0.536\n–\nmajority classiﬁer\n0.027\n0.125\n0.044\n0.046\n0.215\n0.076\nsupervised enriched\nSVC\n0.606\n0.453\n0.481\n0.551\n0.517\n0.504\n: unsupervised\n: semi-supervised\n: supervised\nTable 5: Performance comparison of the best unsupervised method against semi-supervised\nand supervised ones, for ﬁne-grained (party) prediction of political leaning. The best result\nfor each evaluation metric is shown in bold font.\nmethod\nmacro\nmicro\nideologies\npredictions\ntype\nprecision\nrecall\nF1\nprecision\nrecall\nF1\nparties enriched\nclustering\n0.751\n0.752\n0.750\n0.776\n0.772\n0.772\nsupervised enriched\nclustering\n0.748\n0.745\n0.745\n0.787\n0.785\n0.785\nparties enriched\nSVC\n0.828\n0.769\n0.789\n0.821\n0.819\n0.816\nword2vec\nSVC\n0.877\n0.822\n0.841\n0.875\n0.875\n0.871\n–\nmajority classiﬁer\n0.131\n0.333\n0.188\n0.156\n0.395\n0.223\nsupervised enriched\nSVC\n0.822\n0.761\n0.780\n0.822\n0.823\n0.816\n: unsupervised\n: semi-supervised\n: supervised\nTable 6: Performance comparison of the best unsupervised method against semi-supervised\nand supervised ones, for coarse-grained (pole) prediction of political leaning. The best result\nfor each evaluation metric is shown in bold font.\nsemi-supervised and supervised ones. Table 5 shows results of this comparison for the ﬁne-\ngrained prediction task, while Table 6 presents results for the coarse-grained task. Results\npresented in both tables conﬁrm previous ﬁndings and show that the best unsupervised\ntechnique is outperformed by the best supervised and semi-supervised ones. The best over-\nall results are achieved by the semi-supervised word2vec + SVC technique, with micro F1\n= 0.536 on the ﬁne-grained task and micro F1 = 0.871 on the coarse-grained one. Macro\nresults are only slightly worse in both tasks. Thus, the performance gap between the best\nunsupervised technique and the best overall technique is in the region of 0.11 micro F1 on\nthe ﬁne-grained task and 0.10 micro F1 on the coarse-grained one. Taking into account the\ndiﬀerences previously reported in Tables 3 and 4 between unsupervised techniques, these\nlast results represent non-negligible yet modest diﬀerences in performance. Notably, the\nparties enriched + clustering unsupervised technique is also capable of beating the supervised\nenriched + clustering technique in the challenging ﬁne-grained task, in addition to largely\nbeating the simple majority classiﬁer supervised baseline in both tasks.\nAn interesting result that clearly emerges from Tables 5 and 6 is the superiority of all the\napproaches based on SVC classiﬁers for the prediction step. Independently on the method-\nology used for obtaining political ideologies and on the overall approach to the task (e.g.,\n658\nFine-Grained Prediction of Political Leaning on Social Media\nsemi-supervised or supervised), the 3 methods leveraging an SVC consistently obtained the\n3 best overall results in both the ﬁne- and coarse-grained tasks. An important contribu-\ntion to these positive results is given by the data distribution of the diﬀerent splits of our\ndataset. In fact, as anticipated in Section 3.2, our data splitting strategy implied that no\ndrift is present between the training and test partitions of our dataset. Under this favorable\nlaboratory condition, supervised classiﬁers are able to maximize their learning phase on data\ninstances in the training-set, and to eﬀectively carry over what they learnt to the test-set.\nHowever, it is known that real-world conditions are characterized by issues such as concept\ndrift that limit the generalizability of supervised approaches (Lu et al., 2018). In presence\nof concept drift, or of any other factor that shifts the test distribution away from the one\nused in training, supervised approaches end up being unreliable. Instead, unsupervised ap-\nproaches, such as the one proposed in our work, are able to better adapt to possible drifts.\nFor instance, with reference to the ideology space shown in Figure 8, while a supervised\nclassiﬁer learns ﬁxed decision boundaries for the diﬀerent parties based on the data distri-\nbution of the training-set, our unsupervised clustering approach is capable of highlighting\nregions of the ideology space featuring high density, independently on their position.\n6.2.3 Validation: Concept Drift\nThe results presented so far are computed on the test-set split of our dataset, obtained via\na stratiﬁed random sampling of the users as explained in Section 3.2. However, as antici-\npated in the previous section, a more rigorous evaluation can be conducted by assessing the\nperformance of our technique on a time-dependent test-set, by assigning users to either the\ntraining-, validation-, or test-set according to the time when they tweeted. The advantage\nof this evaluation strategy is that, in general, time-wise splits are more representative of the\nconditions in which machine learning models are used, as they allow to test a model’s ability\nto withstand issues that emerge through time, such as concept-drift (Lu et al., 2018). In\nturn, a model capable of withstanding such issues would open up the possibility to carry out\nlongitudinal analyses and even to nowcast political leanings (Lampos & Cristianini, 2012;\nAvvenuti et al., 2017; Tsakalidis et al., 2018). For these reasons, we performed an additional\nexperiment by evaluating our method in this, more stringent, condition.\nSpeciﬁcally, we ﬁrst obtained a new time-wise test-set that contains 5,524 users that\nonly tweeted after August 15, 2019. All other users from our dataset are assigned to either\nthe training- or validation-set, which contain users that tweeted before the threshold date.\nNext, we used our best method (i.e., parties enriched + clustering) to repeat both the party\n(ﬁne-grained) and the pole (coarse-grained) prediction tasks.\nFinally, we compared the\nresults obtained by our method on the time-dependent test-set with those obtained on the\noriginal (random) one. Regarding party predictions, our method obtains macro F1 = 0.388\nand micro F1 = 0.389 on the time-dependent test-set, whereas it obtained both macro and\nmicro F1 = 0.426 on the random one. For pole predictions, our method obtains macro F1\n= 0.721 and micro F1 = 0.771 on the time-dependent test-set, whereas it obtained macro\nF1 = 0.750 and micro F1 = 0.772 on the random one. Summarizing, the more stringent\nevaluation resulted in a maximum of 9% performance decrease on the party prediction task,\nand in a maximum of 4% performance decrease on the pole prediction one. The overall\npositive results of our unsupervised method are conﬁrmed.\n659\nFagni & Cresci\n6.2.4 Validation: Members of the European Parliament\nThe ground-truth labels for users of our dataset are implicitly derived from user likes to party\ntweets, as detailed in Section 3.3. On the one hand this labeling strategy removed the need\nfor a manual labeling of our dataset and avoided possible human errors and biases entering\nour ground-truth (Pandey et al., 2019). On the other hand however, an automatic labeling\nstrategy does not necessarily exclude the risk of inconsistencies or wrong labels. In order to\nfurther validate the correctness of our predictions, we also applied our technique to a small\nset of users whose preferred party and pole are publicly known. Speciﬁcally, we focused on\nthe Italian members of the European parliament (MEPs) as of 2019. These represent active\npoliticians whose party and pole aﬃliations can be used as reliable ground-truth labels in\nour prediction tasks. Notably, the majority of MEPs also has an oﬃcial Twitter account\nlinked to its public page on the European parliament website14.\nSimilarly to the users in our Twitter dataset, we thus collected 200 tweets for each of the\n34 Italian MEPs with an oﬃcial Twitter account. Then, we applied our best method (i.e.,\nparties enriched + clustering) to predict the party leaning of the MEPs. Finally, we evaluated\nour method by comparing its predictions with the party aﬃliations of the MEPs.\nWe\nrepeated these steps also for the method by Darwish et al. (2020), thus enabling a comparison\nof our results with those of this state-of-the-art technique. On this party prediction task, our\nmethod achieved macro F1 = 0.651 and micro F1 = 0.823. Instead, the method by Darwish\net al. (2020) achieved macro F1 = 0.377 and micro F1 = 0.662. Overall, our results are\nvery positive, conﬁrming the good performance of our technique and its superiority with\nrespect to the best existing unsupervised competitor. We also note that the results reported\nin this section are better than those reported in Tables 3 and 5.\nThis is expected by\nconsidering that MEPs are very politically-active users with clear political inclinations. As\nsuch predicting their political leaning represents a simpler task with respect to predicting\nthe political leaning of generic Twitter users.\n6.2.5 Sensitivity Analysis: Distance\nIn this section and in the subsequent ones, we report results of a sensitivity analysis that\nwe carried out on the best unsupervised technique that we proposed: parties enriched +\nclustering.\nThe rationale for the analysis discussed in this section stems from results of Tables 3\nand 4. In particular, the evaluation of the unsupervised approaches highlighted the promis-\ning performance of the parties enriched + distance baseline. Given an ideology space, this\ntechnique assigns a label to each user based on its distance to the pivots. In other words, the\nsimple distance-based prediction strategy employed in this baseline was capable of yielding\npositive results. This suggests that the distance from the pivots in the ideology space is a\nrelevant parameter that has an impact on our predictions. With the goal of evaluating this\nfacet, in Figure 9 we show results of an analysis where we evaluated the performance of our\nparties enriched + clustering technique, as a function of user distance from the pivots. Results\nshown in ﬁgure conﬁrm our previous intuition. When only evaluating predictions for users\nthat lay near to one of the pivots, our results are extremely accurate. For instance, when\nconsidering only users whose min-max normalized distance ≤0.2, our technique obtains\n14. https://www.europarl.europa.eu/meps/en/home\n660\nFine-Grained Prediction of Political Leaning on Social Media\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ndistance\nF1\nmetric: \nmicro F1\nmacro F1\ntask: \npoles\nparties\nFigure 9: Performance evaluation of our parties enriched + clustering unsupervised technique\nas a function of user distance from the pivots. For users that lay near to one of the pivots,\nwe are able to provide ﬁne- (party) and coarse-grained (pole) predictions with exceptional\naccuracy. Instead, most of our mistakes occur for users that are positioned far away from\nall pivots.\nmicro F1 > 0.90 and > 0.98 on the ﬁne- and coarse-grained task, respectively. Exceptional\nresults indeed, considering the diﬃculty of the tasks at hand. As we include in our evalua-\ntion also users who lay further away from any pivot, our results worsen. At the end of our\nevaluation, when we consider all users independently on their distance, we end up with the\nsame results already reported in Tables 3 and 4.\nThe decreasing trend shown in Figure 9 demonstrates that the accuracy of our predictions\nstrongly depends on a user’s distance to the pivots. In turn, this facet can be exploited to\ncomplement our predictions with a conﬁdence score that states how likely a given prediction\nis to be correct. For users that lay near to one of our pivots in the ideology space, we are\nable to provide predictions with large conﬁdence scores (e.g., > 90%). Conversely, for users\nthat lay far away from all pivots, we are still able to provide a prediction, but with a much\nlower conﬁdence.\nThese results also suggest one possible strategy for quickly improving the performance\nof our technique – that is, increasing the number of pivots. This simple operation would\nreduce the average distance of users from the pivots, thus allowing to obtain overall better\nperformances. However, having a large number of pivots requires additional manual eﬀort\nand it would also move the approach towards a semi-supervised one, with all the implications\npreviously discussed. We remark that for all the experiments in this work, we used the\nminimum possible number of pivots: one for each considered political party.\n6.2.6 Sensitivity Analysis: Tweets\nOur proposed technique is based on the analysis of the textual content of the tweets shared\nby social media users. One important aspect to consider when evaluating our technique is\nthus its sensitivity to the number of available tweets per user. Intuitively, users for which\nonly a small number of tweets are available represent more challenging predictions than\nusers who shared many tweets. With the goal of evaluating this aspect, Figure 10 shows the\n661\nFagni & Cresci\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n25\n50\n100\n150\n200\ntweets\nmicro F1\nmethod: \nour contribution\nDarwish et al.\ntask: \npoles\nparties\nFigure 10: Performance evaluation of Darwish et al. (2020) and of our parties enriched +\nclustering unsupervised technique, as a function of the number of user tweets. Our technique\nconsistently beats the competitor, especially in the challenging party prediction (i.e., ﬁne-\ngrained) task.\nBoth techniques show overall stable performance for users having ≥100\ntweets. For users having < 100 tweets the performance of both techniques starts decreasing.\nFor < 40 tweets performances rapidly plummet.\nperformance of our technique and of Darwish et al. (2020), as a function of the number of\nuser tweets, for both tasks. We recall that we collected maximum 200 tweets per user and\nthat we discarded users for which we could collect < 25 tweets.\nResults in Figure 10 show that our technique consistently beats the competitor in both\ntasks and for all users, independently on the number of their tweets. The only exception\nis represented by users having < 40 tweets, on the ﬁne-grained party prediction task, for\nwhich Darwish et al. obtain slightly better results than us. Apart from this, our method\nalways achieves superior results, especially in the challenging party prediction task. Inter-\nestingly, both techniques show overall stable performance for users having ≥100 tweets.\nInstead, as expected, for users having < 100 tweets the performance of both techniques\nstarts decreasing. The decreasing trend is particularly steep for users having < 40 tweets,\nfor which the performances of both techniques rapidly plummet.\nIn addition to serving as further evaluation of our technique, these results also provide\nguidance for applying tweet-based predictors of political leaning in-the-wild. In fact, our\nexperiments suggest that near-optimal results can be expected for users with ≥100 tweets,\nand reduced performance otherwise. In particular, predictions obtained for users having\n< 40 or 50 tweets should undergo additional scrutiny and validation, since misclassiﬁcations\nare frequent under these operating conditions.\n6.2.7 Sensitivity Analysis: Retweets\nIn many recent works, retweets have been used as a strong signal in several prediction tasks\non social media, including the estimation of stance and political leaning (Aldayel & Magdy,\n2019), degree of automation (Mazza et al., 2019), extent of coordination among users (Nizzoli\net al., 2021) and percentage of fake messages in an online discussion (Tardelli et al., 2022), to\nname but a few notable examples. In particular, one of the techniques that we evaluated in\n662\nFine-Grained Prediction of Political Leaning on Social Media\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n50\n100\n150\n200\nretweets\nmicro F1\nmethod: \nour contribution\nDarwish et al.\ntask: \npoles\nparties\nFigure 11: Performance evaluation of Darwish et al. (2020) and of our parties enriched\n+ clustering unsupervised technique, as a function of the number of user retweets. Both\ntechniques show degraded performance when classifying users with few retweets, showing\nthe importance of this signal for the task. However, our technique consistently outperforms\nthe competitor and does not suﬀer from a steep performance drop for users with ≤15\nretweets.\nthis work solely depends on retweets for estimating political leaning (Darwish et al., 2020).\nIn addition, also our proposed technique uses that information, although not as explicitly\nas by Darwish et al. (2020). In fact, as explained in Section 4, in our work retweets only\npartly contribute to document embeddings, which in turn contribute to our latent user\nrepresentations.\nSimilarly to the previous experiment, here we were interested in evaluating the impact\nthat retweets have on the predictions of political leaning generated by our technique and by\nthat of Darwish et al. (2020). To carry out this experiment, we repeatedly evaluated both\ntechniques on subsets of test-set users featuring diﬀerent numbers of retweets, starting from\nusers with no retweets at all, and concluding our experiment with users with 200 retweets (the\nmaximum number of tweets that we collected per each user). Results are shown in Figure 11.\nAs expected, both techniques achieve worse results for users with few retweets, conﬁrming\nthe informativeness of this feature. Our proposed technique consistently outperforms the\none from Darwish et al. (2020) and the gap between the 2 shows only minor ﬂuctuations\nalong the x axis. However, a marked diﬀerence is shown for users that feature an extremely\nlow number of retweets. Indeed for users with ≤15 retweets, the performance of Darwish\net al. (2020) plummets in both prediction tasks. On the contrary, our technique exhibits a\ndiﬀerent behavior, as it does not appear to be impacted so negatively by an extremely low\nnumber of retweets. The diﬀerence between the behavior of the 2 techniques is explained by\nconsidering that retweets are the only information exploited by Darwish et al. (2020), while\nthey are an important – yet minor – part of all the information that our technique leverages.\n663\nFagni & Cresci\n6.2.8 Limitations and Open Challenges\nIn this section we carry out a detailed analysis of the main types of errors made by our\nproposed method. To reach this goal, we manually selected a set of users that were projected\nby our technique to a region of the latent ideology space that is not associated with their\nground-truth party label. For these users that were projected far from their party – and thus,\nthat were subsequently wrongly labeled by the clustering step – we manually analyzed their\nTwitter timelines, so as to identify the root causes for our misclassiﬁcations. This analysis\ncontributes to highlighting current limitations in content-based unsupervised approaches\nto the prediction of social media political leaning, also highlighting open challenges and\nvaluable directions for future research.\nWe ﬁrst evaluated major misclassiﬁcations – namely, cases where users were projected\nto a region of the ideology space related to parties of the opposite pole with respect to the\nground-truth party of the users. For example, users favoring a left-leaning party (e.g., PD)\nthat were erroneously projected to a region of the ideology space associated with extreme-\nright parties (e.g., LE, FdI, CPI). These cases yield errors both in the ﬁne-grained party\nprediction task, as well as in the coarse-grained pole prediction task. Overall, the total\nnumber of these major misclassiﬁcations is small, but it is nonetheless interesting to assess\nthe causes for these errors. The analysis of these major misclassiﬁcations revealed that some\nof our ground-truth labels contrast with the information contained in the tweets from the\nuser timeline. Ground-truth labels were automatically obtained from user likes to party\ntweets. Instead, our classiﬁcations are derived from user tweets. Thus, the majority of cases\nof major misclassiﬁcations are related to users that liked many tweets by a given party,\nbut that support a diﬀerent party in their own tweets. This is a peculiar and interesting\nbehavior that, to the best of our knowledge, is undocumented. The existence of a subset\nof users exhibiting this behavior mandates to carefully consider the source of ground-truth\nlabels in future works, since considering user likes or following relationships might convey\ndiﬀerent and contrasting information with respect to that obtainable from user tweets. In\nthe remaining cases of major misclassiﬁcations by our system, we were not able to correctly\ndetect the political leaning of the user mainly due to: (i) wrong understanding of tweet\nsemantics (more on this in the following); or (ii) an objective diﬃculty in understanding the\npolitical orientation of the user, due to ambiguous and contrasting political content. This\nlatter case is not a limitation of our technique, since also human evaluators would struggle\nto reliably provide predictions for certain users, but rather an inherent challenge in the\nclassiﬁcation of users that express few or ambiguous political positions. Such challenge has\nalready been noted in earlier works on this same task (Cohen & Ruths, 2013), as well as on\nother social media-related tasks (Cresci et al., 2018).\nWe also assessed causes of minor misclassiﬁcations – namely, cases where a user is labeled\nwith a wrong party in the ﬁne-grained party classiﬁcation task, but it is correctly labeled in\nthe coarse-grained pole prediction task. In such cases, several misclassiﬁcations are caused\nby a wrong interpretation of tweet semantics or by the weight (i.e., the importance) that\nour system assigned to certain political tweets. In fact, the political orientation of a user is\nnot a binary concept, but it is rather a nuanced concept often involving ideas and opinions\nthat align with the political line of more than one party. In particular, it is common for a\nuser to support opinions from multiple politically-close parties. To this regard our system,\n664\nFine-Grained Prediction of Political Leaning on Social Media\noriginal tweet\ntranslated tweet\nparty\nscore\nType 1: tweets in favour of a party/politician, that receive low scores for that party:\n@matteosalvinimi\nMatteo\nSalvini\ngli\nitaliani\nsceglieranno\nil\nmiglior\nMatteo\n(Salvini)\n@matteosalvinimi Matteo Salvini Italians\nwill choose the best Matteo (Salvini)\nLE\n0.126\n@LegaSalvini Mitico Matteo Salvini sei il\nnostro capitano\n@LegaSalvini Mythical Matteo Salvini you\nare our captain\nLE\n0.066\nType 2: tweets against a party/politician, that receive high scores for that party:\nDeve andare in pensione. Berlusconi ormai\nè fulminato.\nHe has to retire. Berlusconi is stoned.\nFI\n0.454\n@DSantanche\n@FratellidItaIia\ntornare?\ndevi cominciare a crescere Santanchè, hai\nnovant’anni e \"ragioni\" come una lattante\n@DSantanche\n@FratellidItaIia\ncoming\nback?\nyou have to grow Santanchè, you\nare ninety years old and you still \"think\"\nlike a baby\nFdI\n0.470\nType 3: tweets with limited/no political information, that receive high scores for a party:\nRT @oss_romano:\n#27agosto #rasseg-\nnastampa Un mondo di fraternità e pace\nè possibile. Il #Papa incoraggia le inizia-\ntive per dare attuazione ...\nRT @oss_romano: # 27agosto #rasseg-\nnastampa A world of fraternity and peace\nis possible. The #Pope encourages initia-\ntives to implement ...\nPRC\n0.658\nRT @visit_lazio: Tra le 100 esperienze al\nmondo da vivere, il settimanale @TIME\ninclude il @Castello_Severa nella lista\nworld’s greatest ...\nRT @visit_lazio: Among the 100 experi-\nences in the world to live, the @TIME mag-\nazine includes @Castello_Severa in the\nworld’s greatest ...\nM5S\n0.480\nType 4: tweets with local, subjective, or very speciﬁc information, that receive high scores for a party:\nRT @c_appendino: Asfalto nuovo per via\nCigna. Una buona notizia per i tanti cit-\ntadini che transitano su questa importante\narteria\nRT @c_appendino:\nNew asphalt on via\nCigna. Good news for the many citizens\nwho move through this important thor-\noughfare\nM5S\n0.773\nRT @virginiaraggi:\nPartiti i lavori di\nrestauro della Fontana delle Rane nel\nquartiere Coppedè. L’intervento è il primo\ndi questa portata\nRT @virginiaraggi:\nRestoration work on\nthe Fontana delle Rane in the Coppedè dis-\ntrict has begun.\nThe intervention is the\nﬁrst of this magnitude\nM5S\n0.602\nTable 7: Examples of problematic tweets that were incorrectly assessed by our tweet party\nclassiﬁer. For each of the 4 main types of problematic tweets, we report some examples\nspecifying the reference political party to which the error is referred and the corresponding\nscore computed by the tweet party classiﬁer. Scores are in the [0, 1] range.\nas a human evaluator would do, weighs the available information as best as possible, but\nwith an inevitable degree of uncertainty. Finally, we also analyzed errors for users projected\nto the central (i.e., most uncertain) area of the ideology space of Figure 5a, ﬁnding that the\nprojection errors are mainly due to one of the following reasons: (i) users with insuﬃcient\npolitical content in their timeline; (ii) automated accounts that produce extremely varied\ncontent (i.e., news bots); and (iii) users that repeatedly attack certain political parties and\nleaders, but that do not explicitly support any party15. Challenges related to the analysis\nof the ﬁrst category of users are well-known in literature. For instance, Cohen and Ruths\n15. For these users, we know who they do not support, but we do not know who they do support.\n665\nFagni & Cresci\n(2013) refer to them as politically inactive users. Instead, challenges related to the analysis\nof bots and political antagonists, are rather undocumented, despite the widespread presence\nof both these types of accounts in our online ecosystems (Lokot & Diakopoulos, 2016; Nizzoli\net al., 2021).\nGiven that our projections are based on the scores assigned to user tweets by the tweet\nparty classiﬁer, wrong predictions by our technique are typically due to initial errors by the\ntweet party classiﬁer. We now turn our attention to these errors, so as to provide practical\nexamples of problematic tweets. Our analysis highlighted 4 main categories of problematic\ntweets, summarized in Table 7. A ﬁrst set of errors is due to problematic tweets of type\n1 (tweets in favour of a party/politician, that receive low scores for that party). Here, our\nclassiﬁer was unable to provide high scores for the correct party because of the limited\nnumber of these tweets used to train it. Increasing our dataset, or anyway feeding more\ntweets of this type to the classiﬁer, would likely remove this type of error. Errors due to\nthe second type of problematic tweets (tweets against a party/politician, that receive high\nscores for that party) are more challenging. First of all, those tweets do not express support\nfor any party nor candidate. Thus, there is an intrinsic diﬃculty in assigning a high score\nfor a party. Moreover, they negatively – yet explicitly – mention a party, which tricked our\nclassiﬁer into giving a high score for that party. This second issue implies that our deep\nlearning tweet party classiﬁer was unable to correctly “understand” the meaning of those\ntweets. This problem can be mitigated by implementing the classiﬁer with more complex\nand powerful deep learning architectures, such as those based on modern pretrained language\nmodels (e.g., BERT, T5). These state-of-the-art natural language understanding systems are\ncapable of grasping subtle nuances in the language used for or against a given political party.\nAs such, they would contribute to reducing this type of errors. The third type of problematic\ntweets (tweets with limited/no political information, that receive high scores for a party)\nare due to the challenges of classifying items that do not convey any useful information\nfor the machine learning task at hand. In this situation, classiﬁers usually yield unreliable\npredictions. One possible way of solving this issue is by carrying out an additional ﬁltering\nstep in our analysis pipeline. For instance, we could train a separate binary classiﬁer to\ndistinguish between politically-related and unrelated tweets. Then, only politically-related\ntweets would be given to the party tweet classiﬁer for computing a party score. The last\ntype of problematic tweets (tweets with local, subjective, or very speciﬁc information, that\nreceive high scores for a party) represents another big challenge. In the examples from the\nbottom rows of Table 7, a user is expressing positive opinions about the local administration.\nNotably, the high scores given by our classiﬁer do not necessarily represent errors, in a strict\nsense. In fact, appreciation for the work of a local administration surely conveys a certain\nextent of political information. However, for these users our classiﬁer should have given\nmore weight to other, more explicit, political tweets with respect to those supporting the\nlocal administration.\n7. Conclusions\nWe proposed a novel unsupervised technique for estimating the political leaning of social\nmedia users.\nOur solution leverages a deep neural network in a representation learning\ntask, for analyzing user tweets and for learning latent political ideologies. Then, users are\n666\nFine-Grained Prediction of Political Leaning on Social Media\nprojected in a low-dimensional ideology space and are subsequently clustered. The political\nleaning of a user is automatically derived from the cluster to which the user is assigned.\nWe evaluated our technique on two prediction tasks and we compared it to baselines and\nother state-of-the-art approaches. The ﬁne-grained task aims to infer the preferred political\nparty of each user, out of 8 possible parties. The – easier – coarse-grained task aims to\ninfer the high-level political leaning of each user, in a 3-class classiﬁcation task. Among all\nunsupervised techniques that we evaluated, our proposed one achieved the best results in\nboth tasks, with micro F1 = 0.426 and 0.772, respectively for the ﬁne- and coarse-grained\ntask. It also achieved comparable results to some of the semi-supervised and supervised\ntechniques. However, the best unsupervised technique is outperformed by the best semi-\nsupervised and supervised ones, given the additional information that the latter exploit.\nMoving forward, we demonstrated that we can exploit the topology of our learned ideology\nspace to assign a conﬁdence score to our predictions, thus allowing to retain only those\npredictions for which the conﬁdence meets a desired threshold. Finally, we analyzed the\nrelationship between our predictions and the number of tweets and retweets performed by\nusers, showing that our technique is able to provide accurate predictions also for users who\ntweet or retweet sporadically, contrarily to other state-of-the-art methods.\nOur results advance the state-of-the-art for unsupervised prediction of political leaning\n– an increasingly popular task. For the future we aim to provide additional contributions\nby devising better techniques for learning latent political ideologies, a step where there is\nstill large room for improvement. To this regard, another interesting direction of research\ninvolves providing interpretations to the dimensions of the latent ideology space. As shown\nin our results, the diﬀerent parties seem to position themselves in diﬀerent regions of the\nspace. Hence, being able to interpret the main dimensions of the ideology space could provide\nadditional and valuable information. For the future we also aim at learning aspect-based\nstances on a number of politically-relevant issues (e.g., immigration, economy, rights, and\nmore). Finally, we plan to leverage our technique for carrying out a longitudinal analysis\naimed at investigating ﬂuctuations in leaning due to important real-world events, such as\nduring electoral campaigns. This latter experimental setup would also be valuable toward\nassessing the robustness of our system, as well as of others tackling the same task, to known\nissues such as concept drift and other temporal variations.\nAcknowledgments\nCorrespondence should be addressed to Dr. Stefano Cresci (stefano.cresci@iit.cnr.it).This\nresearch is supported in part by the EU H2020 Program under the scheme “INFRAIA-\n01-2018-2019: Research and Innovation Action” grant agreement #871042 SoBigData++:\nEuropean Integrated Infrastructure for Social Mining and Big Data Analytics.\nReferences\nAdebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., & Kim, B. (2018). Sanity\nchecks for saliency maps.\nIn The 32nd Annual Conference on Advances in Neural\nInformation Processing Systems (NeurIPS’18), pp. 9505–9515.\n667\nFagni & Cresci\nAhmed, S., Jaidka, K., & Skoric, M. M. (2016). Tweets and votes: A four-country comparison\nof volumetric and sentiment analysis approaches. In The 10th International AAAI\nConference on Web and Social Media (ICWSM’16). AAAI.\nAldayel, A., & Magdy, W. (2019). Your stance is exposed! Analysing possible factors for\nstance detection on social media. Proceedings of the ACM on Human-Computer In-\nteraction, 3(CSCW), 1–20.\nAvvenuti, M., Cresci, S., La Polla, M. N., Meletti, C., & Tesconi, M. (2017). Nowcasting\nof earthquake consequences using big social data. IEEE Internet Computing, 21(6),\n37–45.\nBarberá, P. (2015). Birds of the same feather tweet together: Bayesian ideal point estimation\nusing Twitter data. Political Analysis, 23(1), 76–91.\nBastick, Z. (2021). Would you notice if fake news changed your behavior? An experiment\non the unconscious eﬀects of disinformation. Computers in Human Behavior, 116,\n106633.\nBauer, P. C., Barberá, P., Ackermann, K., & Venetz, A. (2017). Is the left-right scale a valid\nmeasure of ideology?. Political Behavior, 39(3), 553–583.\nBenkler, Y., Faris, R., Roberts, H., & Zuckerman, E. (2017). Study: Breitbart-led right-wing\nmedia ecosystem altered broader media agenda. Columbia Journalism Review, 3.\nBlondel, V. D., Guillaume, J.-L., Lambiotte, R., & Lefebvre, E. (2008). Fast unfolding of\ncommunities in large networks. Journal of statistical mechanics: Theory and experi-\nment, 2008(10), P10008.\nBond, R., & Messing, S. (2015).\nQuantifying social media’s political space: Estimating\nideology from publicly revealed preferences on Facebook. American Political Science\nReview, 109(1), 62–78.\nBusch, K. B. (2016). Estimating parties’ left-right positions: Determinants of voters’ per-\nceptions’ proximity to party ideology. Electoral studies, 41, 159–178.\nCinelli, M., Cresci, S., Galeazzi, A., Quattrociocchi, W., & Tesconi, M. (2020). The limited\nreach of fake news on Twitter during 2019 European elections.\nPLoS One, 15(6),\ne0234689.\nCohen, R., & Ruths, D. (2013). Classifying political orientation on Twitter: It’s not easy!.\nIn The 7th International AAAI Conference on Web and Social Media (ICWSM’13).\nAAAI.\nConover, M. D., Gonçalves, B., Ratkiewicz, J., Flammini, A., & Menczer, F. (2011). Predict-\ning the political alignment of Twitter users. In The 3rd IEEE International Conference\non Social Computing (SocialCom’11). IEEE.\nCresci, S. (2020). A decade of social bot detection. Communications of the ACM, 63(10),\n72–83.\nCresci, S., Cimino, A., Avvenuti, M., Tesconi, M., & Dell’Orletta, F. (2018). Real-world\nwitness detection in social media via hybrid crowdsensing. In The 12th International\nAAAI Conference on Web and Social Media (ICWSM’18), pp. 576–579. AAAI.\n668\nFine-Grained Prediction of Political Leaning on Social Media\nCresci, S., Di Pietro, R., Petrocchi, M., Spognardi, A., & Tesconi, M. (2015). Fame for sale:\nEﬃcient detection of fake Twitter followers. Decision Support Systems, 80, 56–71.\nDandekar, P., Goel, A., & Lee, D. T. (2013).\nBiased assimilation, homophily, and the\ndynamics of polarization. Proceedings of the National Academy of Sciences, 110(15),\n5791–5796.\nDarwish, K. (2018). To Kavanaugh or not to Kavanaugh: That is the polarizing question.\narXiv preprint arXiv:1810.06687.\nDarwish, K., Stefanov, P., Aupetit, M., & Nakov, P. (2020).\nUnsupervised user stance\ndetection on Twitter. In The 14th International AAAI Conference on Web and Social\nMedia (ICWSM’20), Vol. 14, pp. 141–152. AAAI.\nDi Giovanni, M., Brambilla, M., Ceri, S., Daniel, F., & Ramponi, G. (2018). Content-based\nclassiﬁcation of political inclinations of Twitter users. In The 2018 IEEE International\nConference on Big Data (BigData’18). IEEE.\nDomingos, P. (2012). A few useful things to know about machine learning. Communications\nof the ACM, 55(10), 78–87.\nFerrara, E., Cresci, S., & Luceri, L. (2020). Misinformation, manipulation and abuse on\nsocial media in the era of COVID-19. Journal of Computational Social Science, 3,\n271–277.\nGarimella, K., De Francisci Morales, G., Gionis, A., & Mathioudakis, M. (2016). Quantifying\ncontroversy in social media. In The 9th International Conference on Web Search and\nData Mining (WSDM’16). ACM.\nGarimella, K., De Francisci Morales, G., Gionis, A., & Mathioudakis, M. (2017). Reducing\ncontroversy by connecting opposing views. In The 10th International Conference on\nWeb Search and Data Mining (WSDM’17). ACM.\nGrčar, M., Cherepnalkoski, D., Mozetič, I., & Novak, P. K. (2017). Stance and inﬂuence of\nTwitter users regarding the Brexit referendum. Computational Social Networks, 4(1),\n6.\nHegelich, S., & Janetzko, D. (2016). Are social bots on Twitter political actors? Empirical\nevidence from a Ukrainian social botnet. In The 10th International AAAI Conference\non Web and Social Media (ICWSM’16). AAAI.\nKulshrestha, J., Eslami, M., Messias, J., Zafar, M. B., Ghosh, S., Gummadi, K. P., & Kara-\nhalios, K. (2017). Quantifying search bias: Investigating sources of bias for political\nsearches in social media. In The 20th Conference on Computer-Supported Cooperative\nWork and Social Computing (CSCW’17). ACM.\nLahoti, P., Garimella, K., & Gionis, A. (2018). Joint non-negative matrix factorization for\nlearning ideological leaning on Twitter. In The 11th International Conference on Web\nSearch and Data Mining (WSDM’18). ACM.\nLampos, V., & Cristianini, N. (2012). Nowcasting events from the social Web with statistical\nlearning. ACM Transactions on Intelligent Systems and Technology (TIST), 3(4), 1–\n22.\n669\nFagni & Cresci\nLeCun, Y., Bottou, L., Bengio, Y., & Haﬀner, P. (1998). Gradient-based learning applied\nto document recognition. Proceedings of the IEEE, 86(11), 2278–2324.\nLokot, T., & Diakopoulos, N. (2016). News bots: Automating news and information dissem-\nination on Twitter. Digital Journalism, 4(6), 682–699.\nLu, J., Liu, A., Dong, F., Gu, F., Gama, J., & Zhang, G. (2018). Learning under concept\ndrift: A review.\nIEEE Transactions on Knowledge and Data Engineering, 31(12),\n2346–2363.\nLuceri, L., Deb, A., Badawy, A., & Ferrara, E. (2019). Red bots do it better: Comparative\nanalysis of social bot partisan behavior. In Companion Proceedings of the 28th Web\nConference (WWW’19 Companion), pp. 1007–1012. IW3C2.\nMarchetti-Bowick, M., & Chambers, N. (2012). Learning for microblogs with distant su-\npervision: Political forecasting with Twitter. In The 13th Conference of the European\nChapter of the Association for Computational Linguistics (EACL’12), pp. 603–612.\nMäs, M., & Flache, A. (2013). Diﬀerentiation without distancing. Explaining bi-polarization\nof opinions without negative inﬂuence. PLoS One, 8(11), e74516.\nMazza, M., Cresci, S., Avvenuti, M., Quattrociocchi, W., & Tesconi, M. (2019). RTbust: Ex-\nploiting temporal patterns for botnet detection on Twitter. In The 11th International\nACM Web Science Conference (WebSci’19), pp. 183–192. ACM.\nMcInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform manifold approximation and\nprojection for dimension reduction. arXiv preprint arXiv:1802.03426.\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Eﬃcient estimation of word repre-\nsentations in vector space. arXiv preprint arXiv:1301.3781.\nMisra, I., Lawrence Zitnick, C., Mitchell, M., & Girshick, R. (2016). Seeing through the\nhuman reporting bias: Visual classiﬁers from noisy human-centric labels. In The 29th\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR’16), pp. 2930–\n2939.\nNikolov, D., Flammini, A., & Menczer, F. (2021).\nRight and left, partisanship predicts\n(asymmetric) vulnerability to misinformation. The Harvard Kennedy School Misinfor-\nmation Review, 0(0), 1–13.\nNizzoli, L., Tardelli, S., Avvenuti, M., Cresci, S., & Tesconi, M. (2021). Coordinated behav-\nior on social media in 2019 UK General Election. In The 15th International AAAI\nConference on Web and Social Media (ICWSM’21). AAAI.\nPandey, R., Castillo, C., & Purohit, H. (2019). Modeling human annotation errors to design\nbias-aware systems for social stream processing. In The 11th ACM/IEEE International\nConference on Advances in Social Networks Analysis and Mining (ASONAM’19), pp.\n374–377. IEEE/ACM.\nPasquino, G. (2019). The state of the Italian Republic. Contemporary Italian Politics, 11(2),\n195–204.\nPennacchiotti, M., & Popescu, A.-M. (2011). Democrats, republicans and starbucks aﬃ-\ncionados: User classiﬁcation in Twitter.\nIn The 17th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining (KDD’11). ACM.\n670\nFine-Grained Prediction of Political Leaning on Social Media\nPla, F., & Hurtado, L.-F. (2014). Political tendency identiﬁcation in Twitter using sen-\ntiment analysis techniques. In The 25th International Conference on Computational\nLinguistics (COLING’14). ACL.\nPoole, K. T., & Rosenthal, H. (1985).\nA spatial model for legislative roll call analysis.\nAmerican Journal of Political Science, 29(2), 357–384.\nPreoţiuc-Pietro, D., Liu, Y., Hopkins, D., & Ungar, L. (2017). Beyond binary labels: political\nideology prediction of Twitter users. In The 55th Annual Meeting of the Association\nfor Computational Linguistics (ACL’17), pp. 729–740.\nRizoiu, M.-A., Graham, T., Zhang, R., Zhang, Y., Ackland, R., & Xie, L. (2018). #De-\nbateNight: The role and inﬂuence of socialbots on Twitter during the 1st 2016 US\npresidential debate. In The 12th International AAAI Conference on Web and Social\nMedia (ICWSM’18). AAAI.\nStefanov, P., Darwish, K., & Nakov, P. (2019). Predicting the topical stance of media and\npopular Twitter users. In The 11th International Conference on Social Informatics\n(SocInfo’19).\nTardelli, S., Avvenuti, M., Tesconi, M., & Cresci, S. (2022). Detecting inorganic ﬁnancial\ncampaigns on Twitter. Information Systems, 103, 101769.\nTsakalidis, A., Aletras, N., Cristea, A. I., & Liakata, M. (2018). Nowcasting the stance of\nsocial media users in a sudden vote: The case of the Greek referendum. In The 27th\nACM International Conference on Information and Knowledge Management (CIKM),\npp. 367–376. ACM.\nTucker, J. A., Guess, A., Barberá, P., Vaccari, C., Siegel, A., Sanovich, S., Stukal, D., &\nNyhan, B. (2018). Social media, political polarization, and political disinformation: A\nreview of the scientiﬁc literature. William and Flora Hewlett Foundation.\nTumasjan, A., Sprenger, T. O., Sandner, P. G., & Welpe, I. M. (2010). Predicting elec-\ntions with Twitter: What 140 characters reveal about political sentiment. In The 4th\nInternational AAAI Conference on Web and Social Media (ICWSM’10). AAAI.\nVaccari, C., Valeriani, A., Barberá, P., Bonneau, R., Jost, J. T., Nagler, J., & Tucker, J. A.\n(2015). Political expression and action on social media: Exploring the relationship\nbetween lower-and higher-threshold political activities among twitter users in Italy.\nJournal of Computer-Mediated Communication, 20(2), 221–239.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &\nPolosukhin, I. (2017). Attention is all you need. In The 31st Annual Conference on\nAdvances in Neural Information Processing Systems (NeurIPS’17), pp. 5998–6008.\nVerbeij, T., Pouwels, J. L., Beyens, I., & Valkenburg, P. M. (2021).\nThe accuracy and\nvalidity of self-reported social media use measures among adolescents. Computers in\nHuman Behavior Reports, 3.\nWong, F. M. F., Tan, C. W., Sen, S., & Chiang, M. (2016). Quantifying political leaning\nfrom tweets, retweets, and retweeters. IEEE Transactions on Knowledge and Data\nEngineering, 28(8), 2158–2172.\n671\nFagni & Cresci\nYan, H., Das, S., Lavoie, A., Li, S., & Sinclair, B. (2019). The congressional classiﬁcation\nchallenge: Domain speciﬁcity and partisan intensity. In The 20th ACM Conference on\nEconomics and Computation (EC’19), pp. 71–89. ACM.\nYan, H. Y., Yang, K.-C., Menczer, F., & Shanahan, J. (2021). Asymmetrical perceptions of\npartisan political bots. New Media & Society, 23(10), 3016–3037.\n672\n",
  "categories": [
    "cs.SI",
    "cs.AI",
    "cs.CL",
    "cs.CY",
    "cs.LG"
  ],
  "published": "2022-02-23",
  "updated": "2022-02-23"
}