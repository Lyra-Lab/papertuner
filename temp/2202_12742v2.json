{
  "id": "http://arxiv.org/abs/2202.12742v2",
  "title": "Learning Relative Return Policies With Upside-Down Reinforcement Learning",
  "authors": [
    "Dylan R. Ashley",
    "Kai Arulkumaran",
    "Jürgen Schmidhuber",
    "Rupesh Kumar Srivastava"
  ],
  "abstract": "Lately, there has been a resurgence of interest in using supervised learning\nto solve reinforcement learning problems. Recent work in this area has largely\nfocused on learning command-conditioned policies. We investigate the potential\nof one such method -- upside-down reinforcement learning -- to work with\ncommands that specify a desired relationship between some scalar value and the\nobserved return. We show that upside-down reinforcement learning can learn to\ncarry out such commands online in a tabular bandit setting and in CartPole with\nnon-linear function approximation. By doing so, we demonstrate the power of\nthis family of methods and open the way for their practical use under more\ncomplicated command structures.",
  "text": "Learning Relative Return Policies With Upside-Down\nReinforcement Learning\nDylan R. Ashley ∗1,2,3\nKai Arulkumaran 4,5\nJ¨urgen Schmidhuber 1,2,3,6,7\nRupesh Kumar Srivastava 7\n1 The Swiss AI Lab IDSIA, Lugano, Switzerland\n2 Universit`a della Svizzera italiana (USI), Lugano, Switzerland\n3 Scuola universitaria professionale della Svizzera italiana (SUPSI), Lugano, Switzerland\n4 ARAYA Inc., Tokyo, Japan\n5 Imperial College London, London, UK\n6 AI Initiative, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia\n7 NNAISENSE, Lugano, Switzerland\nAbstract\nLately, there has been a resurgence of interest in using supervised learning to solve reinforcement learning problems.\nRecent work in this area has largely focused on learning command-conditioned policies. We investigate the potential\nof one such method—upside-down reinforcement learning—to work with commands that specify a desired relationship\nbetween some scalar value and the observed return. We show that upside-down reinforcement learning can learn to\ncarry out such commands online in a tabular bandit setting and in CartPole with non-linear function approximation.\nBy doing so, we demonstrate the power of this family of methods and open the way for their practical use under more\ncomplicated command structures.\nKeywords:\nupside-down\nreinforcement\nlearning,\ncommand-conditioned\npolicies, reinforcement learning, supervised learning, artiﬁcial\nneural networks\nAcknowledgements\nThis work was supported by the European Research Council (ERC, Advanced Grant Number 742870) and the Swiss\nNational Supercomputing Centre (CSCS, Project s1090). We also thank both the NVIDIA Corporation for donating a\nDGX-1 as part of the Pioneers of AI Research Award and IBM for donating a Minsky machine.\n∗Correspondence to dylan.ashley@idsia.ch\narXiv:2202.12742v2  [cs.LG]  10 May 2022\n1\nIntroduction\nArtiﬁcial neural networks in their current incarnation are better suited to solving supervised learning problems than they\nare for solving reinforcement learning problems. Recently, a family of techniques based on upside-down reinforcement\nlearning (Schmidhuber, 2019; Srivastava et al., 2019) has been proposed, which solve RL problems by framing them\nas supervised learning problems. These techniques all focus on directly learning a command-conditioned policy; they\nexplicitly learn a mapping from states and commands to actions. Already, these methods have had remarkable success\nin solving ofﬂine RL problems (Chen et al., 2021; Janner et al., 2021; Kumar et al., 2019), but still struggle to achieve\ncompetitive results in online reinforcement learning problems.\nThis work investigates the learnability of morethan commands: commands that specify a goal in the form of a scalar value,\na horizon, and the desired relation between the given scalar and the observed return under the given horizon. Here we\nshow that these commands are learnable online with traditional UDRL in simple settings. By doing so, we demonstrate\nthe practical potential of the ﬂexibility of commands offered by the UDRL framework. We hope that this ﬂexibility can\nbe leveraged in the future to empower these methods to render them competitive in complicated online and continual\nlearning problems.\n2\nRelated Work\nThe idea of leveraging iterated supervised learning to solve reinforcement learning dates back to at least the work on\nreward-weighted regression by Peters and Schaal (2007), who brought the earlier work of Dayan and Hinton (1997)\nto the domain of operational space control and RL. However, Peters and Schaal (2007) only looked at the immediate-\nreward RL setting. This was extended to the episodic setting separately by Wierstra et al. (2008a) and then by Kober and\nPeters (2011). Wierstra et al. (2008a) went even further and also extended RWR to partially observable Markov decision\nprocesses, whereas Kober and Peters (2011) applied it to motor learning in robotics. Separately, Wierstra et al. (2008b)\nextended RWR to perform ﬁtness maximization for evolutionary methods. Hachiya et al. (2009) and Hachiya et al. (2011)\nlater found a way of reusing old samples to improve RWR’s sample complexity. Much later, Peng et al. (2019) modiﬁed\nRWR to produce an algorithm for off-policy RL, using deep neural networks as function approximators.\nUpside-down reinforcement learning as a command-conditioned method of using SL for RL emerged in Schmidhuber\n(2019) and Srivastava et al. (2019), with Ghosh et al. (2021) afterwards introducing a similar idea in a multi-goal context.\nKumar et al. (2019) applied UDRL to ofﬂine RL and sometime later Chen et al. (2021) and then Janner et al. (2021) showed\nthe potential of the UDRL framework to be competitive in this context when paired with the Transformer architecture of\nVaswani et al. (2017). Furuta et al. (2021) generalized this Transformer variant to solve a broader class of problems.\n3\nBackground\nReinforcement learning considers an agent receiving rewards through interacting with an environment. RL is usually\nmodeled as a Markov decision process where, at each step t, the agent observes the current state of the environment st ∈S,\nselects an action at ∈A, and consequently receives a reward rt+1 ∈R. We say that the rule an agent follows to select\nactions is its policy, which we write as π : S ×A →R where π(s, a) is the probability of the agent taking action a when the\nenvironment is in state s. The objective is typically to ﬁnd a policy that maximizes the sum of the temporally-discounted\nrewards—known as the return.\nUpside-down reinforcement learning breaks the RL problem in two, using commands as an intermediary. Speciﬁcally,\nit divides the agent into two sub-agents: a sub-agent that interacts with the environment in an attempt to carry out\na command, i.e., a worker; and a sub-agent that issues commands to the worker that maximize the expected value of\nthe return, i.e., a manager. The problem of learning an optimal policy then becomes the problem of having both sub-\nagents learn to carry out their respective roles optimally. The key beneﬁt of this formulation is that the worker solves\na supervised learning problem which allows us to bring a portion of the RL problem’s complexity into the domain of\nsupervised learning, which—as we previously remarked—is the principal domain of ANNs.\nTraditionally, commands in UDRL are given as desire-horizon pairs, i.e., (d, h) with d ∈R and h ∈N. Semantically, these\ncommands direct the agent to achieve a return equal to d in the next h steps. Training in UDRL is done with the hindsight\nmethod wherein the agent is trained to predict what action it took given the current state and command (g, h) where g\nis the actual return observed. For UDRL to solve online RL reward-maximization problems, the commands being issued\nby the manager should demand higher and higher returns as time goes on.\nMorethan units—as ﬁrst described in Schmidhuber (2019)—act as an additional element of the command. They serve\nto denote the relation between the true desired return and d. In their simplest form, morethan units capture a boolean\nrelationship, i.e., “get a return larger than this value”. However, the concept of morethan units is ﬂexible enough to\ninstead encode a ternary, additive, multiplicate, or more complicated relationship.\n1\n4\nExperimental Setup\nWe experiment with two reinforcement learning settings here: a simple six-armed bandit and the well-known CartPole\ndomain (Barto et al., 1983). Our toy bandit domain is intended to give us a deep look at how upside-down reinforcement\nlearning operates with the morethan unit in a basic deterministic tabular setting. With our CartPole domain, we hope\nto evaluate the learnability of commands with morethan units when working with non-linear function approximation in\nmore mainstream reinforcement learning domains.\nIn our six-armed bandit domain, pulling the i-th arm always results in a reward of exactly i. If the agent wanted to\nmaximize the return, it should thus always pull the 6-th arm. Since we are working with UDRL, though, in both domains,\nwe issue commands to the agent in the form (d, h, m) where d is the desired return, h is a horizon, and m is the morethan\nunit: a ternary digit which denotes the desired relation between d and h. If m is set to −1, then the agent is commanded\nto obtain a return of less than d in the next h steps; if m is set to 0, then the agent should obtain a return of exactly d in\nthe next h steps; and, ﬁnally, if m is set to 1, then the agent should obtain a return greater than d in the next h steps.\nOur implementation of UDRL follows the one used by Srivastava et al. (2019) with some domain-speciﬁc adaptations.\nFor the bandit setting, we exploit the single-step episodes by learning a two-layered policy network that uses a one-hot\nencoding of the command as input. Here our network uses ReLU activation and orthogonal initialization and is trained\nusing SGD under a step size of 0.01.\nWhen acting in the bandit setting, we always issue a command with m set to 1. We record the results of these actions in\na 100-episode experienced replay buffer. To train the policy network with this buffer, we sample a batch of 16 episodes\nand generate a permutation to pair each episode in the sample to another random episode in the sample. Recall that each\nof these samples will be in the form bi = ((di, hi, mi, gi), ai), where di is a desired return, hi is a horizon (here always 1),\nmi is the morethan unit (again here always 1), gi is an observed return, and ai is an action. For two samples bi and bj,\nwe train the network to predict ai given an input (gi, hi, m) where m set to 1 if gj > gi, 0 if gj = gi, and −1 if gj < gi.\nThis novel sampling strategy is critical here as it provides us with a non-parametric way of providing in-distribution\nsamples to the network. For the bandit setting, we train once using a randomly permuted batch and once with the batch\npermuted such that each bi is matched with itself. We add 16 exploratory episodes to the buffer and repeat this training\nprocess 16 times for each iteration of the UDRL algorithm. We report the results of 10 runs of this with 25 000 steps each\nin Section 5.\nOur network in the CartPole setting has a single hidden layer of 32 gated fast weights with Tanh activation and orthog-\nonal initialization. We use Adam (Kingma and Ba, 2014) to train this network under a step-size of 0.0008 and with the\nother hyperparameters set as recommended in Kingma and Ba (2014).\nFor the CartPole setting, we use a similar training regimen as in the bandit setting but use an unbounded buffer to which\nwe add 5 episodes in each iteration. Here, in each iteration, we train with 800 batches of size 256 for which we generate\nseven rather than two independent permutations (one of which matches samples to themselves as in the bandit setting).\nNote that the ability to generate exponentially more samples here is a key advantage of training with morethan units.\nWe report the results of 30 runs of the above with 500 000 steps each in Section 5.\n5\nResults\nTo evaluate learning with morethan units in the bandit setting, we look at how the action probability density of the ﬁnal\nlearned command-conditioned policy changes as a function of d and m. The mean of these densities over the 10 runs\nis shown in Figure 1. Of crucial importance here, the learned action probabilities density is concentrated in the correct\nregions for each setting of the morethan unit. For example, when the morethan unit is set to 0, the action probability\ndensity is highly concentrated in the i-th action for each i desired return. Interestingly, when the morethan unit is set\nto −1, the action probability density clusters around the (i −1)-th action for each of the i desired returns, but when the\nmorethan unit is set to 1, the action probability density is clustered around the most-rewarding action.\nTo evaluate learning with morethan units in the CartPole domain, we compare the observed return as a function of d\nand m when acting under the ﬁnal learned command-conditioned policy. The results of this over the 30 runs is given\nin Figure 2. A similar but somewhat inverted relation appears here compared to the bandit results. When the morethan\nbit is set to 0, the observed returns roughly correspond to the desired returns and only start to taper off slightly as the\ndesired return reaches the higher end of the spectrum. Likewise, when the morethan bit is set to 1, the observed return is\nconsistently only marginally higher than the desired return for most of the values tested. However, when the morethan\nbit is set to −1, the observed returns are always drastically smaller than d.\n2\n1\n2\n3\n4\n5\n6\nAction\n0\n1\n2\n3\n4\n5\n6\nDesire\nMorethan = -1\n1\n2\n3\n4\n5\n6\nAction\n0\n1\n2\n3\n4\n5\n6\nDesire\nMorethan = 0\n1\n2\n3\n4\n5\n6\nAction\n0\n1\n2\n3\n4\n5\n6\nDesire\nMorethan = 1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 1: The mean action probabilities of the learned command-conditioned policy in the bandit setting. Note how the\nprobability densities seem to cluster around the highest-valued valid action.\n0\n250\n500\nDesired Return\n0\n250\n500\nReturn\nMorethan = -1\nMorethan =  0\nMorethan =  1\nFigure 2: The desired return versus the observed return under the learned command-conditioned policy in the CartPole\nsetting. Standard deviation is shown with shading. Note how the observed returns center around the lower-end of the\nvalid returns.\n6\nDiscussion\nThis work aimed to show that morethan units were a learnable concept in the context of upside-down reinforcement\nlearning. The results presented in Section 5 clearly demonstrate that this is the case. Importantly, these results show\nthat morethan units can be used with non-linear function approximation in mainstream online reinforcement learning\ndomains and thus have the potential to be usable in some of the more advanced applications of reinforcement learning.\nIn our bandit experiments, the emphasis of the learned policy on maximal valid rewards throughout was unexpected.\nWe hypothesize that this is due to the restrictive buffer size mixed with the training regimen whereby we perpetually\ndemand increasingly large returns. This change in demand means that an experience replay buffer will be increasingly\nﬁlled by large returns causing UDRL to increasingly oversamples large returns when training its policy network. The\nconsequence of this is that UDRL will be prone to a speciﬁc form of forgetting. We hypothesize that this may be part of\nthe reason that the UDRL paradigm has achieved its greatest successes primarily in the ofﬂine RL setting.\nAn initial inspection of the results of the CartPole experiment seems to produce something inconsistent with the above\nhypothesis. However, recall here that the buffer size is unbounded. This means that even though the buffer is increas-\ningly ﬁlled with large values, the probability of a training sample having the morethan unit set to −1 grows as training\nprogresses. Conversely, the probability of a training sample having the morethan unit set to 1 shrinks as training pro-\ngresses. Together with our previous hypothesis, this suggests that the distribution of training samples drastically affects\nthe exact nature of the learned policy here, even when not otherwise affecting its validity.\n3\n7\nConclusion and Future Work\nIn this work, we set out to demonstrate the ﬂexibility afforded by the use of commands in upside-down reinforcement\nlearning. We accomplished this by experimenting with commands in the form (d, h, m), where d is a return, h is a horizon,\nand m is a morethan unit which denotes whether the agent is being directed to receive a return greater than, less than,\nor equal to d in the next h steps. We showed that commands of this form are learnable in two reinforcement learning\nenvironments: a six-armed bandit and the well-known CartPole environment. Through doing this, we hope to pave the\nway for later work to leverage advanced command structures in the hopes of successfully applying UDRL and related\nmethods to challenging online RL problems.\nFuture work will look at other advanced command structures. We also plan to experiment with the morethan unit under\nmore complicated neural network architectures—such as the Transformer architecture. This would, in turn, allow us to\nunderstand better the utility of morethan units in signiﬁcantly more challenging applications.\nWe note that the challenges faced by the worker portion of the upside-down reinforcement learning agent are similar to\nthe challenges faced by continual learning agents. Future work will investigate how this relationship can be leveraged\nto specify good command structures for UDRL and related algorithms.\nReferences\nBarto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difﬁcult learning\ncontrol problems. IEEE Transactions on Systems, Man, and Cybernetics, 13(5), 834–846. https://doi.org/10.1109/\nTSMC.1983.6313077\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., & Mordatch, I. (2021). Decision\ntransformer: Reinforcement learning via sequence modeling. arXiv. https://arxiv.org/abs/2106.01345\nDayan, P., & Hinton, G. E. (1997). Using expectation-maximization for reinforcement learning. Neural Computation, 9(2),\n271–278. https://doi.org/10.1162/neco.1997.9.2.271\nFuruta, H., Matsuo, Y., & Gu, S. S. (2021). Generalized decision transformer for ofﬂine hindsight information matching. arXiv.\nhttps://arxiv.org/abs/2111.10364\nGhosh, D., Gupta, A., Reddy, A., Fu, J., Devin, C. M., Eysenbach, B., & Levine, S. (2021). Learning to reach goals via\niterated supervised learning. Proceedings of the International Conference on Learning Representations. https : / /\nopenreview.net/forum?id=rALA0Xo6yNJ\nHachiya, H., Peters, J., & Sugiyama, M. (2009). Efﬁcient sample reuse in em-based policy search. Proceedings of the Joint\nEuropean Conference on Machine Learning and Knowledge Discovery in Databases, 5781, 469–484. https://doi.org/10.\n1007/978-3-642-04180-8\\ 48\nHachiya, H., Peters, J., & Sugiyama, M. (2011). Reward-weighted regression with sample reuse for direct policy search in\nreinforcement learning. Neural Computation, 23(11), 2798–2832. https://doi.org/10.1162/NECO\\ a\\ 00199\nJanner, M., Li, Q., & Levine, S. (2021). Reinforcement learning as one big sequence modeling problem. arXiv. https://arxiv.org/\nabs/2106.02039\nKingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv. https://arxiv.org/abs/1412.6980\nKober, J., & Peters, J. (2011). Policy search for motor primitives in robotics. Machine Learning, 84(1-2), 171–203. https:\n//doi.org/10.1007/s10994-010-5223-6\nKumar, A., Peng, X. B., & Levine, S. (2019). Reward-conditioned policies. arXiv. https://arxiv.org/abs/1912.13465\nPeng, X. B., Kumar, A., Zhang, G., & Levine, S. (2019). Advantage-weighted regression: Simple and scalable off-policy reinforce-\nment learning. arXiv. https://arxiv.org/abs/1910.00177\nPeters, J., & Schaal, S. (2007). Reinforcement learning by reward-weighted regression for operational space control. Pro-\nceedings of the 24th International Conference on Machine Learning, 227, 745–750. https://doi.org/10.1145/1273496.\n1273590\nSchmidhuber, J. (2019). Reinforcement learning upside down: Don’t predict rewards - just map them to actions. arXiv. https:\n//arxiv.org/abs/1912.02875\nSrivastava, R. K., Shyam, P., Mutz, F., Jaskowski, W., & Schmidhuber, J. (2019). Training agents using upside-down reinforce-\nment learning. arXiv. https://arxiv.org/pdf/1912.02877.pdf\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention\nis all you need. Advances in Neural Information Processing Systems, 30, 5998–6008. http://papers.nips.cc/paper/\n7181-attention-is-all-you-need.pdf\nWierstra, D., Schaul, T., Peters, J., & Schmidhuber, J. (2008a). Episodic reinforcement learning by logistic reward-weighted\nregression. Proceedings of the 18th International Conference on Artiﬁcial Neural Networks, 5163, 407–416. https://doi.\norg/10.1007/978-3-540-87536-9\\ 42\nWierstra, D., Schaul, T., Peters, J., & Schmidhuber, J. (2008b). Fitness expectation maximization. Proceedings of the 10th\nInternational Conference on Parallel Problem Solving from Nature, 5199, 337–346. https://doi.org/10.1007/978-3-\n540-87700-4\\ 34\n4\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "I.2.6"
  ],
  "published": "2022-02-23",
  "updated": "2022-05-10"
}