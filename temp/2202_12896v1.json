{
  "id": "http://arxiv.org/abs/2202.12896v1",
  "title": "Photonic reinforcement learning based on optoelectronic reservoir computing",
  "authors": [
    "Kazutaka Kanno",
    "Atsushi Uchida"
  ],
  "abstract": "Reinforcement learning has been intensively investigated and developed in\nartificial intelligence in the absence of training data, such as autonomous\ndriving vehicles, robot control, internet advertising, and elastic optical\nnetworks. However, the computational cost of reinforcement learning with deep\nneural networks is extremely high and reducing the learning cost is a\nchallenging issue. We propose a photonic on-line implementation of\nreinforcement learning using optoelectronic delay-based reservoir computing,\nboth experimentally and numerically. In the proposed scheme, we accelerate\nreinforcement learning at a rate of several megahertz because there is no\nrequired learning process for the internal connection weights in reservoir\ncomputing. We perform two benchmark tasks, CartPole-v0 and MountanCar-v0 tasks,\nto evaluate the proposed scheme. Our results represent the first hardware\nimplementation of reinforcement learning based on photonic reservoir computing\nand pave the way for fast and efficient reinforcement learning as a novel\nphotonic accelerator.",
  "text": "Photonic reinforcement learning based on optoelectronic \nreservoir computing \n \nKazutaka Kanno1,* and Atsushi Uchida1 \n1Department of Information and Computer Sciences, Saitama University 255 Shimo-Okubo, Sakura-ku, Saitama City, \nSaitama 338–8570, Japan \n*Corresponding author: E-mail address: kkanno@mail.saitama-u.ac.jp \n \nABSTRACT \nReinforcement learning has been intensively investigated and developed in artificial intelligence in the absence of training \ndata, such as autonomous driving vehicles, robot control, internet advertising, and elastic optical networks. However, the \ncomputational cost of reinforcement learning with deep neural networks is extremely high and reducing the learning cost \nis a challenging issue. We propose a photonic on-line implementation of reinforcement learning using optoelectronic delay-\nbased reservoir computing, both experimentally and numerically. In the proposed scheme, we accelerate reinforcement \nlearning at a rate of several megahertz because there is no required learning process for the internal connection weights in \nreservoir computing. We perform two benchmark tasks, CartPole-v0 and MountanCar-v0 tasks, to evaluate the proposed \nscheme. Our results represent the first hardware implementation of reinforcement learning based on photonic reservoir \ncomputing and pave the way for fast and efficient reinforcement learning as a novel photonic accelerator. \n \n \n \n \nIntroduction \nMachine learning in artificial intelligence has been the primary automation tool used in processing large amounts of data \nin communications and information technologies [1-4]. Reinforcement learning is a machine learning scheme involved in \ntraining an action policy to maximize the total reward in a particular situation or environment [5]. Various applications \nhave been studied for reinforcement learning, such as autonomous driving vehicles [6], robot control [7], communication \nsecurity [8], and elastic optical networks [9]. Recently, many algorithms used for reinforcement learning have been actively \ndeveloped. For example, an algorithm based on a deep neural network (Agent57) has been proposed in 2020 [10]. This \nscheme has achieved a score that is above the human baseline on all 57 Atari 2600 games. In addition, simulated policy \nlearning is one of the model-based reinforcement learning schemes [11]. This algorithm requires fewer training time steps. \nMoreover, a multi-agent reinforcement learning scheme (AlphaStar) has been proposed [12]. This scheme achieves real-\ntime processing at 30 ms and almost outperforms human players in the online game StarCraft II.  \nDeep neural networks have often been used for reinforcement learning based on Q-learning, known as the deep Q \nnetwork [13]. The deep Q network is trained to produce the value of action in a particular state. The technique of deep Q \nnetworks has contributed to the development of reinforcement learning. However, learning the connection weights of deep \nneural networks using reinforcement learning entails high computation costs because of the repeated training of network \nweights from vast playing data [14,15]. This fact indicates the need for a large number of parameters used for learning to \nimprove the performance of deep neural networks, known as overparameterization [15-17]. Large-scale \noverparameterization has several hundred million parameters for deep learning [15], and the training time is required for \ndays or weeks using the deep Q network on GPU [15]. Several techniques have been proposed to reduce learning costs, \nsuch as prioritized experienced replay [18]. However, the prioritized experienced replay speeds up only by a factor of two. \nA more efficient implementation than deep neural networks is required for reinforcement learning. \nReservoir computing has attracted significant attention in various research fields because it is capable of fast learning \nthat results in reduced computational/training costs compared to other recurrent neural networks [19,20]. Reservoir \ncomputing is a computation framework used for information processing and supervised learning [21,22]. The main \nadvantage of reservoir computing is that only the output weights (readout weights) are trained using a simple learning rule, \nrealizing a fast-learning process, and enabling a reduction in its computational cost. \nRecently, physical implementations of reservoir computing and its hardware implementations have been intensively \nstudied [23-28]. Specifically, the photonic implementation of reservoir computing based on the idea of photonic \naccelerators [29] can realize fast information processing with low learning costs [30-35]. A previous study reported the \nrealization of speech recognition at 1.1 GB/s using photonic reservoir computing [36]. This result suggested the reduction \nof computational cost and fast processing speed in photonic reservoir computing. However, photonic reservoir computing \nhas been applied to supervised learning, and no hardware implementation of reservoir computing for reinforcement learning \nhas been reported yet.  \nHardware implementations including photonics for reinforcement learning have a demand for the applications in edge \ncomputing. In edge computing, data processing is executed close to the data source without connecting to a powerful server \ncomputer through a network [37]. Edge computing requires low power, memory budget, processing speed, and efficiency \n[37]. Therefore, the hardware implementation of reinforcement learning based on photonic reservoir computing is a \npromising candidate for edge computing.  \nHere, we demonstrate the photonic on-line implementation of reinforcement learning based on optoelectronic delay-\nbased reservoir computing, both experimentally and numerically. The photonic reservoir computing is implemented based \non an optoelectronic time-delayed system [30,38,39] and is used to select an agent’s action to evaluate the action-value \nfunction. The output weights in reservoir computing are trained based on the reward obtained from the reinforcement \nlearning environment, where Q-learning is used to update the output weights in reservoir computing. We perform two \nbenchmark tasks, CartPole-v0 and MountainCar-v0, for the evaluation of our proposed scheme. Our demonstration is a \nnovel on-line hardware implementation of reinforcement learning based on photonic reservoir computing. \n \n \nFigure 1 Schematic diagram of reinforcement learning based on delay-based reservoir computing. \n \nResults \nReinforcement learning based on reservoir computing \nFigure 1 shows a schematic of reinforcement learning based on reservoir computing, incorporating a decision-making agent \nand an environment [5]. The agent affects the future state of the environment by its actions, and the environment provides \nrewards to every action of the agent. The objective of the agent is to maximize the total reward. However, the agent has no \ninformation regarding a good action policy. Here, we consider the action-value function 𝑄(𝐬𝑛, 𝑎𝑛) for state 𝐬𝑛 and action \n𝑎𝑛 at the 𝑛-th time step [5]. The agent selects an action with the highest Q value in each state, and the total reward is \nincreased if the agent initially knows the value of Q. However, the Q function is usually unknown. In various previous \nstudies, the Q function was replaced by deep neural networks. Deep neural networks were trained to approximate the Q \nfunction using some methods, including Q-learning [13]. Here, the Q function is replaced by photonic delay-based reservoir \ncomputing to reduce the learning cost and realize fast processing. The reservoir computing consists of three layers: input, \nreservoir, and output. We explain about the three layers for reinforcement learning. Table 1 summarizes the variables used \nin the following explanation. \n \nTable 1. Parameters used for reinforcement learning. \nSymbol \nParameter \n𝑁 \nNumber of virtual nodes in reservoir \n𝜏 \nDelay time of reservoir \n𝜃 \nNode interval used for time-multiplexing \n𝐬𝑛 \nVector for environmental state at the time step 𝑛 for reinforcement learning task \nNonlinear\nNode\nFeedback\nNode\nEnvironment\nAction：\nReward：\nState：\nInput layer\nReservoir\nOutput layer\n𝑠𝑖,𝑛 \n𝑖-th element of the state vector 𝐬𝑛 \n𝑁𝑠 \nNumber of elements for state vector 𝐬𝑛 \n𝐌 \n𝑁× (𝑁𝑠+ 1) matrix for input mask in preprocessing input signal \n𝑚𝑝,𝑞 \nElement of the mask matrix 𝐌 in row p and column q \n𝐮𝑛 \nInput vector generated from the state 𝐬𝑛 after preprocessing \n𝑢𝑖,𝑛 \n𝑖-th element of the input vector 𝐮𝑛 \n𝜇 \nInput scaling coefficient introduced in preprocessing \n𝑏 \nInput bias introduced in preprocessing \n𝑢(𝑡) \nInput signal generated from temporally stretching input vector 𝐮𝑛 \n𝑇𝑚 \nMask period given by 𝑁𝑠× 𝜃 \n𝑄(𝒔𝑛, 𝑎) \nAction-value function for the state 𝒔𝑛 and the action 𝑎 \n𝐰𝑎 \nWeight vector for the reservoir output corresponding to the action 𝑎 \n𝑤𝑖,𝑎 \n𝑗-th element of the output weight 𝐰𝑎 corresponding to the action 𝑎 \n𝐯𝑛 \nNode vector extracted from the temporal response of the reservoir to the input state \n𝑣𝑖,𝑛 \n𝑗-th element of the node vector 𝐯𝑛 for the 𝑛-th input state \n𝛼 \nConstant step-size parameter for updating the reservoir weights in Q-learning \n𝛾 \nDiscount rate for future expected reward in Q-learning \n \nIn delay-based reservoir computing, the reservoir consists of a nonlinear element and a feedback loop [40]. In this scheme, \nthe nodes in the network are virtually implemented by dividing the temporal output into short node intervals 𝜃. The number \nof nodes 𝑁 is given by 𝑁= 𝜏/𝜃, where 𝜏 is the feedback delay time of the reservoir. The definition of the virtual nodes \nresults in an easier implementation because it does not require preparing many spatial nodes to construct a network.  \nIn the input layer, the 𝑛 -th input data into the reservoir is the state vector given by the environment 𝐬𝑛\n𝑇=\n(𝑠1,𝑛, 𝑠2,𝑛, ⋯, 𝑠𝑁𝑠,𝑛), where 𝑁𝑠 is the number of state elements and the superscript 𝑇 represents the transpose operation. \nThe state vector is preprocessed via the masking procedure before injecting into the reservoir as follows [40,41]: \n𝐮𝑛\n𝑇= (𝜇𝑠1,𝑛, 𝜇𝑠2,𝑛, … , 𝜇𝑠𝑁𝑠,𝑛, 𝑏)𝐌= (𝜇𝐬𝑛\n𝑇, 𝑏)𝐌, \n(1) \nwhere 𝐌 is the mask matrix with 𝑁× (𝑁𝑠+ 1) elements, 𝜇 is the scaling factor for the input state 𝐬𝑛. The value of the \nmask is randomly obtained from a uniform distribution of [−1, 1]. The mask acts as random connection weights from \ninput data to reservoir nodes. We represent the 𝑖 -th element of the preprocessed input vector 𝐮𝑛  as 𝑢𝑖,𝑛 . The vector \nelement 𝑢𝑖,𝑛 corresponds to the input data into the 𝑖-th virtual node. An input signal injected into the reservoir is generated \nby temporally stretching the elements of 𝐮𝑛 to the node interval 𝜃 as follows: \n𝑢(𝑡+ (𝑛−1)𝑇𝑚) = 𝑢𝑖,𝑛 ((𝑖−1)𝜃≤𝑡< 𝑖𝜃), \n(2) \nwhere 𝑇𝑚 is the signal period of each input data and is called the mask period. The period 𝑇𝑚 is given by 𝑇𝑚= 𝑁𝜃 and \ncorresponds to the feedback delay time of the reservoir 𝜏. The input signal 𝑢(𝑡) is injected into the reservoir to generate \na response signal. \nWe note that an input bias 𝑏 is added to Eq. (1). The input bias prevents the signal 𝐮𝑛 from being equal to zero when \nthe elements of 𝐬𝑛 are close to zero. Moreover, the input bias leads to different nonlinearities for each virtual node. We \nconsider the input data 𝑢𝑖,𝑛  for the 𝑖 -th virtual node defined as 𝜇(𝑚1,𝑖𝑠1,𝑛+ 𝑚2,𝑖𝑠2,𝑛+ ⋯+ 𝑚𝑁𝑠,𝑖𝑠𝑁𝑠,𝑛) + 𝑏𝑚𝑁𝑠+1,𝑖 , \nwhere 𝑚𝑝,𝑞 is an element of the mask matrix 𝐌 in the row 𝑝 and column 𝑞. The representation of 𝑢𝑖,𝑛 indicates that \nthe input data for the 𝑖-th node oscillates with the center on the bias 𝑏𝑚𝑁𝑠+1,𝑖. The center point of the oscillation in the \ninput data is different for each node because the element 𝑚𝑁𝑠+1,𝑖 of the mask matrix is different for each node. A different \npart of the nonlinear function that represent the relationship of the input and output in the reservoir is used for each node \nbecause of the bias 𝑏𝑚𝑁𝑠+1,𝑖, leading to different nonlinearities for each node. Therefore, adding an input bias enhances \nthe approximation of the reservoir. \nIn the output layer, the output of reservoir computing is calculated from the weighted linear combination of virtual node \nstates. The reservoir output is considered as the action-value function 𝑄(𝐬𝑛, 𝑎)  for reinforcement learning. Then, the \naction-value function 𝑄(𝐬𝑛, 𝑎) is given as: \n𝑄(𝐬𝑛, 𝑎) = ∑𝑤𝑗,𝑎𝑣𝑗,𝑛\n𝑁\n𝑗=1\n= 𝐰𝑎\n𝑇𝐯𝑛, \n(3) \nwhere 𝑣𝑗,𝑛 is the 𝑗-th virtual node state for the 𝑛-th input and 𝑤𝑗,𝑎 is the output weight corresponding to the 𝑗-th virtual \nnode for the action 𝑎. The vector 𝐯𝑛 and 𝐰𝑎 are given as 𝐯𝑛\n𝑇= (𝑣1,𝑛, 𝑣2,𝑛, … , 𝑣𝑁,𝑛) and 𝐰𝑎\n𝑇= (𝑤1,𝑎, 𝑤2,𝑎, … , 𝑤𝑁,𝑎), \nrespectively. The number of reservoir outputs corresponds to the number of actions. In reinforcement learning, the action \nwith the highest 𝑄 value is selected. \nHere, we use Q-learning algorithm to train the reservoir weights [5]. The update rule based on Q-learning is off-policy \nlearning [5], where the action used for training differs from the selected action. In the Q-learning method, the maximum of \nthe Q function max𝑎𝑄(𝐬𝑛+1, 𝑎) for the action 𝑎 at the next state 𝐬𝑛+1 is used, and the actual action is not always used \nfor training. In our scheme, 𝑄(𝐬𝑛, 𝑎)  is approximated using reservoir computing by considering a one-step temporal \ndifference error 𝛿𝑛= 𝑟𝑛+1 + 𝛾max𝑎𝑄(𝐬𝑛+1, 𝑎) −𝑄(𝐬𝑛, 𝑎𝑛) and the square of the temporal difference error as the loss \nfunction. Then, the update rule for the reservoir weights is: \n𝐰𝑎𝑛←𝐰𝑎𝑛+ 𝛼[𝑟𝑛+1 + 𝛾max\n𝑎\n𝐰𝑎\n𝑇𝐯𝑛+1 −𝐰𝑎𝑛\n𝑇𝐯𝑛] 𝐯𝑛, \n(4) \nwhere 𝛼  is the constant step-size parameter and 𝛾  is the discount rate for a future expected reward. These \nhyperparameters should be appropriately selected for a successful computation. We set 𝛼 as a small positive value and is \nrelated to the training speed. Moreover, 𝛾 is set to a positive value of less than one. The details of the training algorithm \nare described in the Methods section. \nOur scheme is demonstrated in both numerical simulation and experiment using an optoelectronic delay system [42]. \nFigure 2(a) shows the schematic model of an optoelectronic delay system. The system has been applied to explore complex \nphenomena such as dynamical bifurcation, chaos, and chimera states [43]. Moreover, the application of this system in \nphysical reservoir computing has also been studied [37,38]. The system is composed of a laser diode (LD), a Mach-Zehnder \nmodulator (MZM), and an optical fiber for delayed feedback. In particular, the modulator provides a nonlinear transfer \nfunction 𝑐𝑜𝑠2(⋅) from the electrical inputs to the optical outputs. The optical signal is transmitted through the optical fiber \nwith a delay time of 𝜏 and is transformed into an electric signal using a photodetector (PD). The electric signal is fed back \nto the MZM after the signal passes through an electric amplifier (AMP). An input signal for reservoir computing is injected \ninto the reservoir by coupling with the feedback signal. The temporal dynamics of the system is described using simple \ndelay differential equations [44]. We use delay differential equations for the numerical verifications of the proposed scheme. \nThe delay differential equations are described in the Methods section. In our experiment, we employ a system similar to \nthe scheme shown in Fig. 2(a), except for the absence of the delayed feedback, as shown in Fig. 2(b). Thus, the proposed \nsystem is considered as an extreme learning machine, which has been studied as a machine-learning scheme [45]. The \ndetails of the experimental setup and online procedure for reinforcement learning are described in the Methods section. \nIn both numerical simulation and experiment, the number of nodes 𝑁 is 600, and the node interval 𝜃 is 0.4 ns. Then, \nthe mask interval 𝑇𝑚 is given as 𝑇𝑚= 𝑁𝜃= 240 ns. The feedback delay time is fixed at the same value as the mask \ninterval in various studies on delay-based reservoir computing [36,40]. However, it has been reported that the slight \nmismatch between the delay time and the mask interval enhances the performance of information processing [30,46]. \nTherefore, we set the feedback delay time to 𝜏= 236.6 ns (𝜏= 𝑇𝑚−𝜃). \n \n \n \nFigure 2 (a) Schematic diagram of the optoelectronic delay system for reservoir computing. An input signal is \npreprocessed before injecting into the reservoir and added to a feedback signal. Reservoir node states are extracted from \nthe temporal response of the reservoir and are shown as red circles. In the schematic diagram, MZM is the Mach-Zehnder \nmodulator, PD is the photodetector, BPF is the bandpass filter, and AMP is the electric amplifier. (b) Experimental setup \nfor reinforcement learning. The system has no delayed feedback, and the detected signal at the PD is not fed back to the \nMZM. In the personal computer (PC), environmental states in reinforcement learning tasks are calculated and the \nmasking procedure is applied. The input data preprocessed in the PC is transferred to the arbitrary waveform generator \n(AWG). The optical signal from the MZM is detected at the PD, and the detected power is adjusted using the optical \nattenuator (ATT). The detected signal at the PD is measured at the digital oscilloscope (OSC). The AWG and the OSC \nare controlled by the PC in an on-line manner. \n \nNumerical and experimental results of reinforcement learning using benchmark tasks \nWe evaluate our reinforcement learning scheme based on delay-based reservoir computing using a reinforcement learning \ntask, known as CartPole-v0 in OpenAI Gym [47]. An un-actuated joint attaches a pole to a cart that moves along a \nfrictionless track. The goal of the task is to keep the pole upright during an episode. An episode has a length of 200 time \nsteps. A reward of +1 is provided for every time steps if the pole remains upright. The task is solved when the pole \nremained upright for 100 consecutive episodes. The details of the CartPole-v0 task are described in the Methods section. \nLD\nMZM\nPD\nb\nAMP\nBPF\nbias\nInput layer\n(a)\nAWG\nOSC\nPC\nLD\nISO\nMZM\nATT\nPD\nISO\nAmp\nBias\n(b)\n \n \nFigure 3 (a) Numerical and (b) experimental results of the CartPole-v0 task. The result shows the total reward for each \nepisode. The total reward of 200 indicates that the pole keeps upright over an episode. The black and red curves show \nthe case with and without the input bias (𝑏= 0.8 and 𝑏= 0.0), respectively. \n \nFigure 3(a) shows the numerical results of the total reward as the episode is increased for the CartPole-v0 task. The total \nreward of 200 indicates that the pole remains upright over an episode and the task is solved successfully. We compare the \ncases with and without the input bias 𝑏. The input bias is applied (𝑏= 0.80) for the case of the black curve in Fig. 3(a). \nThe pole cannot be kept upright for the first several episodes. However, the total reward becomes 200 and the pole becomes \nupright as the number of episodes increases. The total reward of 200 is always obtained in consecutive 100 episodes from \nthe 30th to 300th episode. Therefore, the CartPole-v0 task is successfully solved. However, for the case without input bias \n(𝑏= 0, the red curve), the total reward does not reach 200 and the pole cannot keep upright for all episodes. The comparison \nof the black and red curves indicates that the input bias is required to solve the task. When no input bias is introduced, it \nwas observed that only one action (push to the left or right) is selected regardless of the state. When the input bias is \nintroduced, the action that prevents the pole from tilting is selected. It is considered that the input bias contributes to training \nthe reservoir so that the reservoir can identify the state.  \nOur scheme requires 130 episodes for solving the task and it is faster than the result presented in [48], where more than \n150 episodes are required for solving the task using a deep neural network with prioritized experienced replay. Another \nscheme using double deep neural network provided a similar performance as our scheme [49]. The double deep neural \nnetwork requires a similar number of episodes to our scheme; however, the number of training parameters is large (150,531 \nparameters). Therefore, our scheme requires less training costs than these existing schemes. \nFigure 3(b) shows the experimental result for the CartPole-v0 task. The input bias is introduced for the black curve. The \ntotal reward reaches 200 at the 110th episode and keeps the total reward until the 300th episode, indicating that the task is \nsuccessfully solved in the experiment. We found that the total reward increases slowly in the experimental result than in \nthe numerical result. We speculate that the measurement noise in the experiment perturbs the Q value estimated by the \nreservoir. The noise prevents the increase of the total reward. A proper action may not be selected due to the influence of \nnoise when the difference between the Q values of the two actions is too small. Therefore, it is necessary to learn the Q \nvalues until their difference becomes sufficiently large to ensure the selection of a proper action in a noisy environment. In \naddition, time-delayed feedback may affect the speed of increase in the total reward. The time-delayed feedback provides \na memory effect for the reservoir. If the reservoir has a memory effect, it can learn the state-action value function including \n0\n50\n100\n150\n200\n0\n50\n100\n150\n200\n250\n300\nTotal reward\nEpisode index\nWith input bias\nWithout input bias\n(a)\n(b)\n0\n50\n100\n150\n200\n0\n50\n100\n150\n200\n250\n300\nTotal reward\nEpisode index\nWith input bias\nWithout\ninput bias\nthe past states. The introduction of time-delayed feedback is equivalent to expanding the dimension of the input state space \nand can approximate a complex state-action value function. Here, no time-delayed feedback is introduced in the experiment, \ntherefore, the speed of increase in the total reward in the experiment is slower than that in the numerical simulation. \n \n \nFigure 4 (a) Numerical and (b) experimental results of the MountainCar-v0 task. The black curve represents the total \nreward for each episode. The moving average of the total reward is represented as the red curve. The average is calculated \nfrom the past 100 episodes. The total reward of −200 indicates that the car does not reach the top of the mountain. A \nlarger value of the total reward indicates that the car reaches the top of the mountain at a smaller number of steps. (c) \nThe total reward for each episode, where the reservoir weight at the 180th episode in (b) is used in experiment. The \nweight is not updated in (c). The blue dashed line corresponds to the total reward of −110 that indicates that the task \nis successfully solved. \n \nWe demonstrate another benchmark task called the MountainCar-v0 task provided by OpenAI Gym [47]. This task aims \nto make a car reach the top of the mountain by accelerating the car to the right or left. One episode for the task consists of \n200 steps. A reward of -1 is given for every step until an episode ends. An episode is terminated if the cart reaches the top \nof the mountain. Therefore, a higher value of the total reward is obtained if the cart reaches the top of the mountain faster. \nSolving this task is defined as obtaining an average reward of -110.0 for 100 consecutive trials [47]. The details of \nMountainCar-v0 task are described in the Methods section. \nFigure 4(a) shows the numerical results of the total reward as the episode is increased for the MountainCar-v0 task. The \nblack curve in Fig. 4(a) shows the total reward for each episode, and the red curve represents the moving average of the \ntotal reward calculated from the past 100 episodes. The total reward is -200 in the first several episodes, indicating that the \ncar does not reach the top of the mountain at all. The total reward increases as the number of episodes increases, indicating \nthe car reaches the top of the mountain. The average reward exceeds -110 at the 267th episode, indicating that the task is \nsolved using our scheme. \n Figure 4(b) shows the experimental result for the MountainCar-v0 task. The moving average of the total reward (red \ncurve) increases as the number of episodes increases. However, the moving average does not reach the blue dashed line \n(the total reward of -110). The number of consecutive episodes at which a high value of the total reward is obtained is small \nin the experiment. For example, a large value of the total reward from -120 to -80 is obtained during 23 consecutive episodes \nfrom the 170th episode in the black curve of Fig. 4(b). However, the moving average (red curve) cannot reach the reward \nof -110. The reason the reservoir cannot obtain a large value of the total reward is due to a negative value of the reward. \n-200\n-180\n-160\n-140\n-120\n-100\n-80\n0\n50\n100\n150\n200\n250\n300\nTotal reward\nEpisode index\n-200\n-180\n-160\n-140\n-120\n-100\n-80\n0\n50\n100\n150\n200\n250\n300\nTotal reward\nEpisode index\n(a)\n(b)\n-200\n-180\n-160\n-140\n-120\n-100\n-80\n0\n50\n100\n150\nTotal reward\nEpisode index\n(c)\nThe negative value of the reward makes the reservoir trained not to select an action in a state. Therefore, as the episodes \ncontinue at which a large value of the total reward is obtained, the action policy for getting a large value of the total reward \nbecomes not to be selected. \nWe consider utilizing a fixed reservoir weight to prevent from decreasing the total reward due to a negative reward. We \nuse the reservoir weights obtained at the 180th episode in the experiment of Fig. 4(b), and the weights are fixed during the \nexperiment, i.e., the weights are not updated in the training procedure. Figure 4(c) shows the total reward for each episode \nin this experiment. The moving average of the total reward (red curve) exceeds -110 at the 141st episode. Therefore, the \ntask is solved if the weights are not updated. Additionally, this indicates that the trained weight works appropriately if the \nexperimental setup conditions are slightly changed, such as the detected power at the PD. Therefore, the trained weights \nare robust against perturbations in the system parameters. \n \n \n \nFigure 5 (a) Maximum of the average total reward as a function of the input bias 𝑏. The red solid (with diamonds) and \nblue dashed (circles) curves represent the case with (𝜅= 0.9) and without (𝜅= 0) delayed feedback. The average total \nreward is calculated using a moving window from the past 100 episodes. The error bar represents the maximum and \nminimum values for 10 trials. (b) Maximum of the average total reward as a function of the feedback strength 𝜅. The \ninput bias is set to be 𝑏= 0.9, 0.7, and 0.5 for the black solid (circles), red dashed (diamonds), and blue dotted \n(squares) curves, respectively. The plotted value is the maximum of the average total reward in 100 consecutive episodes. \nThe error bar represents the maximum and minimum values for 10 trials.  \n \nWe numerically investigate the dependence of the performance on the input bias in the MountainCar-v0 task. Figure 5 \nshows the numerical results of the maximum value of the average total reward as the input bias b is changed for the \nMountainCar-v0 task. In Fig. 5(a), the solid red curve represents the maximum moving average of the total reward in 1000 \nepisodes. The maximum total reward is averaged for ten trials, with each trial consisting of 1000 episodes. The total reward \nis close to zero for a small input bias (𝑏≤0.5). A large total reward value is obtained for a large input bias (𝑏> 0.5). This \nresult indicates that the input bias is necessary for solving the task. The input bias with a value close to one is suitable for \nincreasing the total reward. The result is related to the normalized half-wave voltage (𝑉𝜋) of the MZM, where the normalized \nvoltage is equal to one in our numerical simulation. The input bias nearly equal to one can produce the nonlinearity of the \nMZM (cos2(⋅)), and the nonlinearity can assist in identifying different input states. \nFurthermore, we investigate the effect of the time-delayed feedback in the numerical simulation. In Fig. 5(a), the blue \ndashed curve represents the maximum moving average of the total reward generated using the reservoir without time-\n-200\n-180\n-160\n-140\n-120\n-100\n0\n0.5\n1\n1.5\nMax. value of total reward\nInput bias b\n-200\n-180\n-160\n-140\n-120\n-100\n0\n0.5\n1\n1.5\nMax. value of total reward\nFeedback strength \nb = 0.5\nb = 0.7\nb = 0.9\n(a)\n(b)\ndelayed feedback (𝜅= 0 ). At the input bias of 𝑏= 0.85 , the total reward of -130.29 is obtained. Thus, the reservoir \nsuccessfully trains for the car to reach the top of the mountain if the reservoir has no delayed feedback. However, the \nperformance is lower than in the case with delayed feedback (the solid red curve). Therefore, the presence of the time-\ndelayed feedback can enhance the performance of reinforcement learning.  \nFor more detailed investigation, Figure 5(b) shows the dependence of the total reward on the feedback strength 𝜅. \nDifferent values of the input bias are used for each of the three curves. For the small value of the input bias (blue curve, \n𝑏= 0.50), the total reward is almost equal to -200 for different feedback strengths. This result indicates that the adjustment \nof the feedback strength cannot enhance the performance when the input bias is too small. For the black (𝑏= 0.90) and \nred (𝑏= 0.70) curves, a large value of the total reward is obtained at the feedback strength of approximately one. However, \nthe total reward decreases as the feedback strength increases over one. When the feedback strength becomes larger than \none, the temporal dynamics of the optoelectronic system changes from a steady state to a periodic oscillation, though the \nreservoir has no input signal. The reservoir may produce different response signals to the same driving inputs when the \ntemporal dynamics of the reservoir is periodic or chaotic. Therefore, the reservoir does not have consistency [50], which \nis the reproducibility of response signals in a nonlinear dynamical system driven repeatedly by a same input signal. If there \nis no consistency in the response signals of the reservoir, the reservoir cannot successfully learn the Q function since \ndifferent input states cannot be identified. Therefore, the reservoir provides high performance at the vicinity of the \nbifurcation point 𝜅= 1, called the edge of chaos. In reservoir computing, it has been reported that the condition of the \nedge of chaos can enhance the performance in many studies [51]. In addition, our results show that the performance for \nreinforcement learning can be enhanced at the edge of chaos. \n \nDiscussion \nWe introduced an input bias for preprocessing the input state in reinforcement learning. The input bias has the same role \nas a bias introduced in the general neuron models that controls the firing frequency. Our results show that the input bias is \nnecessary for solving the reinforcement tasks in our scheme. Here, the activation function of the virtual node of the reservoir \nis cos2(⋅), and an input bias is used to control the initial position of cos2(⋅) function. For example, if the input bias is set \nnear the extreme value of cos2 𝜓 (𝜓= 0, ±𝑛𝜋/2, and 𝑛 is an integer), the reservoir does not respond well with respect \nto the change in the input signal. In contrast, when the input bias is set to a quadrature point (𝜓= ±𝜋/4), the reservoir \nshows a large response with respect to the change in the input signal. Therefore, it is possible to adjust the sensitivity of \nthe virtual nodes to the input signal by changing the input bias. In the presence of the input bias, different input states are \ndistinguished well, which enhances the performance of reinforcement learning based on reservoir computing. We show \nthat input bias has a significant effect on reinforcement learning in our scheme. \nWe emphasize that one action of reinforcement learning is potentially determined by the processing rate of reservoir \ncomputing at a frequency of 4.2 MHz in our scheme, where one virtual network is constructed from a time series with \n𝑁𝜃= 240 ns (𝑁 is 600 and 𝜃 is 0.4 ns). Further, we increase the processing speed by decreasing the node interval 𝜃 \nwith a faster photonic dynamical system. In addition, the size of the trained parameters (600) is smaller than that for deep \nneural networks (e.g., 480 Mega parameters for ImageNet [15-17]). The hardware implementation of photonic reservoir \ncomputing is promising for realizing fast and efficient reinforcement learning. \nThe number of training parameters is reduced in reinforcement learning based on reservoir computing, compared with \ndeep neural networks. However, reservoir computing may produce less performance than deep neural networks for more \ncomplex tasks. Therefore, our future works are the application of our scheme to more complex tasks and the comparison \nwith conventional algorithms based on deep neural networks. In addition, the effect of memory provided by the reservoir \nis an important issue. Memory capacity is one of the essential characteristics of reservoir computing. The reservoir that \nincorporates past information to train the Q function could perform better on the tasks that require long-term memory. \nTherefore, the investigation of the memory effect of the reservoir on the performance of reinforcement learning is another \nresearch topic in future work. \nTo summarize our study, we numerically and experimentally demonstrated the on-line implementation of reinforcement \nlearning based on optoelectronic reservoir computing, which consists of a laser diode, a Mach-Zehnder modulator, and a \nfiber delay line. We demonstrated two benchmark tasks of the CartPole-v0 and MoutainCar-v0 tasks using our proposed \nscheme. The results show that the state-action value function in reinforcement learning is trained, and their tasks are solved \nsuccessfully using photonic reservoir computing. To the best of our knowledge, this is the first on-line hardware \nimplementation of reinforcement learning based on photonic reservoir computing. In particular, reservoir computing is \nused to approximate the Q-function, and the output weights of the reservoir are trained with Q-learning. The high-\ndimensional mapping between the states and Q-values for reinforcement learning is trained by reservoir computing. The \nspeed of one action is determined by the processing rate of reservoir computing at 4.2 MHz (240 ns) in our experiment. \nThe hardware implementation of reinforcement learning based on photonic reservoir computing is promising for fast \nand efficient reinforcement learning as a novel photonic accelerator. Our scheme can be applied for edge computing in \nreal-time distributed control and adaptive channel selection in optical communications.  \n \nMethods \nDetails of training algorithm for reinforcement learning \nWe present a training procedure for reinforcement learning in this section. We consider that a state in a reinforcement \nlearning task is updated at every step, and the step index is 𝑛. The update is repeated until termination conditions for the \ntask are satisfied. One episode consists of all steps until the task is completed. In the algorithm, the reservoir weight 𝐰𝑎 \nis initialized with a value randomly sampled from a uniform distribution of [-0.1 0.1]. In each episode, the following \nprocedure is repeated from the step index 𝑛= 1  until termination conditions are satisfied. The state of the task is \ninitialized, which is regarded as 𝐬1. The input signal 𝑢(𝑡) injected into the reservoir is generated by preprocessing the \nstate 𝐬𝑛 using Eqs. (1) and (2). The input signal 𝑢(𝑡) is injected into the reservoir and the response signal of the reservoir \nis obtained. A node state 𝐯𝑛  is extracted from the response signal. The Q value corresponding to each action 𝑎  is \ncalculated from the node state 𝐯𝑛 and the reservoir weight 𝐰𝑎 using Eq. (3). The action 𝑎 with the highest Q value is \nselected at the step index 𝑛. The state in the task is updated using the selected action 𝑎𝑛. Then, a reward 𝑟𝑛+1 and the \nnext state 𝐬𝑛+1 is given. A set of the states, action, and reward (𝐬𝑛, 𝐬𝑛+1, 𝑎𝑛, 𝑟𝑛+1) is preserved. The reservoir weight is \nupdated using Eq. (4). The step index is updated from 𝑛 to 𝑛+ 1. The above procedure is repeated until termination \nconditions are satisfied. The total reward is given from the sum of the rewards in all steps. Algorithm 1 shows the pseudo \ncode corresponding to the above procedure. \nIn the training process, we employ the experienced replay method [52]. In this method, the observed data (state, action, \nand reward) are preserved in the memory, sampled randomly, and used for training. The randomly sampled data is referred \nto as the mini-batch. The size of the mini-batch and the number of preserved data are hyperparameters. Using the randomly \nsampled and preserved data for training may reduce the correlation between the data used for training and exhibits easier \nconvergence of the Q-learning. The number of memories and the size of the mini-batch for experience replay are 4,000 and \n256, respectively. \nMoreover, we used the 𝜀-greedy method for the action selection. The value of 𝜀 is reduced as the number of episodes \nincreases. The value of 𝜀  is updated by 𝜀= 𝜀0 + (1 −𝜀0) exp(−𝑘𝜀𝑛𝑒𝑝) , where 𝑛𝑒𝑝  is the episode index of the \nreinforcement learning task and 𝑘𝜀 is the attenuation coefficient. The coefficient 𝑘𝜀 is fixed at 𝑘𝜀= 0.04 here. The \nvalue of 𝜀 converges to the value 𝜀0 = 0.01 as the number of episodes increases. \n \nAlgorithm 1 Pseudo code for reinforcement learning based on photonic reservoir computing. \nInitialize a reservoir weight 𝐰𝑎 with a value randomly sampled from a uniform distribution of [-0.1 0.1] \nFOR 𝑖= 1 to the number of episodes \nInitialize a state in a reinforcement learning task \nSet a total reward to 0 \nInitialize a step index 𝑗= 1 for the task \nREPEAT \nGet the state 𝐬𝑗 in the task \nGenerate an input signal 𝑢(𝑡) from preprocessing the state 𝐬𝑗 using Eqs. (1) and (2) \nInput 𝑢(𝑡) into the reservoir and extracted a node state 𝐯𝑗 from the response output of the reservoir \nCalculate Q value for each action from the node state 𝐯𝑗 and the reservoir weight 𝐰𝑎 using Eq. (3) \nSelect the action 𝑎𝑗 with the highest Q value \nUpdate the state 𝐬𝑗 to 𝐬𝑗+1 using the selected action 𝑎𝑗 and getthereward 𝑟𝑗+1 \nPreserve a set (𝐬𝑗, 𝐬𝑗+1, 𝑎𝑗, 𝑟𝑗+1) \nUpdate the reservoir weight from preserved sets using Eq. (4) \nAdd the reward 𝑟𝑗 to the total reward \nUpdate 𝑗 to 𝑗+ 1 \nUNTIL termination conditions for the task are satisfied \nPrint the total reward \nEND FOR \n \nNumerical model for an optoelectronic delay system \nOptoelectronic delay systems [42] have been studied for delay-based reservoir computing [30,38,39], using the following \ndelay differential equations [43]: \n𝜏𝐿\n𝑑𝑥(𝑡)\n𝑑𝑡\n= −(1 + 𝜏𝐿\n𝜏𝐻\n) 𝑥(𝑡) −𝑦(𝑡) + 𝛽cos2 [𝜅𝑥(𝑡−𝜏) + 𝜋\n4 𝑢(𝑡) + 𝜙0] + 𝜉(𝑡), \n(5) \n𝜏𝐻\n𝑑𝑦(𝑡)\n𝑑𝑡\n= 𝑥(𝑡), \n(6) \nwhere 𝑥 is the normalized output of MZM, 𝜏𝐿 and 𝜏𝐻 are the time constants describing the low-pass and high-pass \nfilters related to the frequency bandwidths of the system components, relatively, 𝛽  is the feedback strength \n(dimensionless), 𝜙0 is the offset phase of MZM, 𝑢(𝑡) is the input signal injected into the reservoir, and 𝜉(𝑡) is the \nwhite Gaussian noise with properties 〈𝜉(𝑡)〉= 0 and 〈𝜉(𝑡)𝜉(𝑡0)〉= 𝛿(𝑡−𝑡0), where 〈⋅〉 denotes the ensemble average \nand 𝛿(𝑡) is Dirac’s delta function. Table 2 shows the parameter values used. A personal computer (DELL, CPU: Intel \nCore i7-7700 3.60 GHz, RAM: 16.0 GB, Windows 10) was used in numerical simulation.  \n \nTable 2. Parameter values used in numerical simulations. \nSymbol \nParameter \nValue \n(2𝜋𝜏𝐿)−1 \nLow-pass cutoff frequency \n12.5 × 109 Hz \n(2𝜋𝜏𝐻)−1 \nHigh-pass cutoff frequency \n0.625 × 106 Hz \n𝜏 \nFeedback delay time \n239.6 × 10−9 s \n𝛽 \nDimensionless gain \n1.0 \n𝜅 \nDimensionless feedback strength \n0.9 \n𝜙0 \nBias point for MZM \n−0.25𝜋 rad \n𝜇 \nScaling coefficient for input state \n0.6 \n \nExperimental setup \nFigure 2(b) shows the experimental setup used in the experiments. The system has no delayed feedback for simple \nimplementation, and the system corresponds to an extreme learning machine [47]. A distributed-feedback laser diode (LD, \nNTT electronics, NLK1C5GAAA) with an optical wavelength of 1,547 nm was used as the optical source. The lasing \nthreshold of the LD was 11.6 mA, and the driving current was 30.0 mA. The optical output of the LD was injected into a \nMach-Zehnder modulator (MZM, EO Space, AX-0MKS-20-PFA-PFA-LV-UL), where a bias controller (BC, IXBlue, \nMBC-AN-LAB) was inserted to stabilize the operation bias of the MZM. The bias was stabilized at the quadrature point. \nMoreover, a modulation signal was generated from an arbitrary waveform generator (AWG, Tektronix, AWG70002A, 25 \nGSample/s, 10 bit vertical resolution) and transferred to the MZM after amplification by an electric amplifier (AMP, IXBlue, \nDR-AN-10-HO). A photodetector (PD, Newport, 1554-B, 12-GHz bandwidth) was used to detect the optical signal of the \nMZM, and the detected power was 0.280 mW on the condition of no modulation input. The detected signal at the PD was \ntransferred to a digital oscilloscope (OSC, Tektronix, DPO72304SX, 23 GHz bandwidth) and sampled at 50 GSample/s. \nThe signal amplitude injected into the MZM and the half-wave voltage 𝑉𝜋 of the MZM are important for successful \ncomputation. The signal amplitude is determined from the input scaling 𝜇 and the bias scaling 𝑏, the output amplitude of \nthe AWG, and the amplification gain of the AMP. The output amplitude of the AWG is 0.30 V at the peak-to-peak value. \nThe amplification gain of the AMP is typically 30 dB under small-signal conditions. The half-wave voltage of the MZM \nwas 𝑉𝜋= 4 V. The input signal was preprocessed using Eq. (1) in the personal computer. The input scaling 𝜇 and the \nbias scaling 𝑏 for preprocessing is fixed at 𝜇= 0.50 and 𝑏= 0.40, respectively. These parameter values are different \nfrom those used for the numerical simulation because the signal amplitude in the experiments depends on these parameter \nvalues and the condition of the output amplitude of the AWG. In our experiments, the values of 𝜇= 0.50 and 𝑏= 0.40 \nproduce an electric signal with an amplitude nearly equal to the half-wave voltage of the MZM. The condition of the bias \nscaling 𝑏 is consistent with the value for successful computation in our numerical simulation (see Fig. 5). \n \nExperimental online procedure for reinforcement learning \nIn our experiment, the digital oscilloscope (OSC) and the arbitrary waveform generator (AWG) were controlled by the \npersonal computer (PC). Initially, the state of the reinforcement learning task was calculated using the PC. Then, an input \nsignal was generated from the state by applying a masking procedure for reservoir computing. The input signal was \ngenerated from the AWG. The signal was amplified by AMP and injected into the MZM. The optical output of the MZM \nwas modulated based on the injected signal. The optical signal was transformed into an electric signal at the PD. The \nelectric signal was acquired by the OSC and was then transferred to the PC. The node states of the reservoir were extracted \nfrom the signal. The output of reservoir computing was calculated from the weighted sum of the node states and \ncorresponded to the Q value for each action in a reinforcement learning task. An action was selected based on the Q values, \nand the state of the reinforcement learning task was updated based on the selected action. In addition, the reservoir weights \nwere updated based on Q-learning. The above procedure was repeated until the reinforcement learning task was terminated. \nThis procedure for reinforcement learning was executed under the control of OSC, AWG, and PC in an on-line manner. \nIn the experiment, pre- and post-processing are implemented in the personal computer although the reservoir is \nhardware-implemented. Therefore, the processing speed of the experiment is restricted to the software processing in the \npersonal computer. Here, the decision of an action was executed at about 0.5 s. The hardware implementation of the pre- \nand post-processing in photonic reservoir computing has been studied [53]. The processing speed can be accelerated by \nimplementing the pre- and post-processing in hardware, such as field programmable gate array (FPGA). \n \nCartPole-v0 task \nThe CartPole-v0 task is a benchmark task for reinforcement learning given by the OpenAI Gym [47]. In this task, we \nconsider a pole attached by an un-actuated joint to a cart that moves along a frictionless track with four states: cart position, \ncart velocity, pole angle, and pole velocity at the tip. These states are initialized to uniform random values. The agent’s \naction is to push the cart to the right (+1) and left (-1). The goal of the task is to keep the pole upright during an episode \nwith a length of 200 timesteps, and the task is considered solved when the pole remains upright for 100 consecutive episodes. \nA reward of +1 is provided for every time steps while the pole remains upright. The episode ends when the pole is more \nthan 12° from the vertical or the cart position moves more than 2.4 units or less than -2.4 units from the center. The \nmagnitudes of the cart position and pole velocity were normalized to the range of [-1.0 to 1.0] before injecting into the \nreservoir. The hyperparameters for reinforcement learning are fixed at 𝛼= 0.000400 and 𝛾= 0.995, respectively. \n \nMountainCar-v0 task \nThe MountainCar-v0 task is provided by OpenAI Gym [47]. The goal of this task is for a car to reach the top of the mountain \nby accelerating the car to the right or left. The observable states of the task are the cart position and cart velocity. In the \ninitial state, the cart position is randomly set from a uniform distribution [-0.6 to -0.4], and the cart velocity is fixed at zero. \nThe agent’s action is to push the cart to the left, neutral, and push the cart to the right. A reward of -1 is given for every \nstep until an episode ended. An episode consists of 200 steps and is terminated if the cart reaches the top of the mountain. \nThe hyperparameters for reinforcement learning are fixed at 𝛼= 0.000010 and 𝛾= 0.995, respectively. \n \nReferences \n1. \nAndrae, A. & Edler, T. On Global Electricity Usage of Communication Technology: Trends to 2030. \nChallenges 6, 117-157 (2015). \n2. \nHaghighat, M. H. & Li, J. Intrusion detection system using voting-based neural network. Tsinghua Science and \nTechnology 26, 484-495 (2021). \n3. \nZhang, J. & Xu, Q. Attention-aware heterogeneous graph neural network. Big Data Mining and Analytics 4, \n233-241 (2021). \n4. \nBie, Y. & Yang, Y. A multitask multiview neural network for end-to-end aspect-based sentiment analysis. Big \nData Mining and Analytics 4, 195-207 (2021). \n5. \nSutton, Richard S. & Barto, Andrew G., Reinforcement Learning: An Introduction (The MIT Press, \nCambridge, 2018), 2nd ed. \n6. \nZhou, W. et al. Multi-target tracking for unmanned aerial vehicle swarms using deep reinforcement learning. \nNeurocomputing 466, 285-297 (2021). \n7. \nZhu, K. & Zhang, T. Deep reinforcement learning based mobile robot navigation: A review. Tsinghua Science \nand Technology 26, 674-691 (2021). \n8. \nSharma, P. et al. Role of machine learning and deep learning in securing 5G-driven industrial IoT applications. \nAd Hoc Networks 123, 102685 (2021). \n9. \nChen, X. et al. DeepRMSA: a deep reinforcement learning framework for routing, modulation and spectrum \nassignment in elastic optical networks. Journal of Lightwave Technology, 37, 4155-4163 (2019). \n10. \nBadia, A. P. et al. Agent57: Outperforming the Atari Human Benchmark. Preprint at \nhttps://arxiv.org/abs/2003.13350 (2020). \n11. \nKaiser, Ł. et al. Model based reinforcement learning for Atari. in Proc of International Conference on Learning \nRepresentations (ICLR) 2020 (2020). \n12. \nVinyals, O., Babuschkin, I., Czarnecki, W.M. et al. Grandmaster level in StarCraft II using multi-agent \nreinforcement learning. Nature 575, 350–354 (2019). \n13. \nMnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529-533 (2015). \n14. \nGraves, A. et al. Hybrid computing using a neural network with dynamic external memory. Nature 538, 471-\n476 (2016). \n15. \nThompson, N. C., Greenewald, K., Lee, K., & Manso, G. F., The computational limits of deep learning. \nPreprint at https://arxiv.org/abs/2007.05558v1 (2020). \n16. \nSoltanolkotabi, M., Javanmard, A., & D Lee, J., Theoretical insights into the optimization landscape of over-\nparameterized shallow neural networks. IEEE Transactions on Information Theory 65, 742–769 (2019). \n17. \nXie, Q., Minh-Thang, L., Eduard, H., & Quoc V., L., Self-Training With Noisy Student Improves ImageNet \nClassification. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, \n10687-10698 (2020). \n18. \nSchaul, T., Quan, J., Antonoglou, I., & Silver, D., Prioritized experience replay. Preprint at \nhttps://arxiv.org/abs/1511.05952 (2016). \n19. \nChang, H. & Futagami, K., Reinforcement learning with convolutional reservoir computing. Appl. Intell. 50, \n2400-2410 (2020).  \n20. \nSzita, I., Gyenes, V., & Lőrincz, A., Reinforcement Learning with Echo State Networks. ICANN2006 4131, \n830-839 (2006). \n21. \nJaeger, H. & Haas, H., Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless \ncommunication. Science 304, 78-80 (2004). \n22. \nLukoševičius, M., & Jaeger H., Reservoir computing approaches to recurrent neural network training. Comput. \nSci. Rev. 3, 127-149 (2009). \n23. \nTanaka, G. et al. Recent advances in physical reservoir computing: A review. Neural Networks 115, 100-123 \n(2019). \n24. \nTorrejon, J. et al. Neuromorphic computing with nanoscale spintronic oscillators. Nature 547, 428-431 (2017). \n25. \nNakajima, K., Hauser, H., Li, T., & Pfeifer, R., Information processing via physical soft body. Sci. Rep. 5, \n10487 (2015). \n26. \nShastri, B. J. et al. Photonics for artificial intelligence and neuromorphic computing. Nat. Photon. 15, 102-114 \n(2021). \n27. \nGenty, G. et al. Machine learning and applications in ultrafast photonics. Nat. Photon. 15, 91-101 (2021). \n28. \nMoughames, J. et al. Three-dimensional waveguide interconnects for scalable integration of photonic neural \nnetworks. Optica 7, 640-646 (2020). \n29. \nKitayama, K. et al. Novel frontier of photonics for data processing—photonic accelerator. APL Photonics 4, \n090901 (2019). \n30. \nPaquot, Y. et al. Optoelectronic reservoir computing. Sci. Rep. 2, 287 (2012). \n31. \nMartinenghi, R., Rybalko, S., Jacquot, M., Chembo, Y. K., & Larger, L., Photonic nonlinear transient \ncomputing with multiple-delay wavelength dynamics. Phys. Rev. Lett. 108, 244101 (2012). \n32. \nBueno, J., Brunner, D., Soriano, M. C., & Fischer, I., Conditions for reservoir computing performance using \nsemiconductor lasers with delayed optical feedback. Opt. Express 25, 2401-2412 (2017). \n33. \nDuport, F., Schneider, B., Smerieri, A., Haelterman, M. & Massar, S., All-optical reservoir computing. Opt. \nExpress 20, 22783-22795 (2012). \n34. \nSugano, C., Kanno, K., & Uchida, A., Reservoir computing using multiple lasers with feedback on a photonic \nintegrated circuit. IEEE J. Sel. Top. Quantum Electron. 26, 1500409 (2020). \n35. \nAntonik, P., Marsal, N., Brunner, D., & Rontani, D., Human action recognition with a large-scale brain-\ninspired photonic computer. Nat. Mach. Intell. 1, 530-537 (2019). \n36. \nBrunner, D., Soriano, M. C., Mirasso, C. R., & Fischer, I., Parallel photonic information processing at gigabyte \nper second data rates using transient states. Nat. Commun. 4, 1364 (2013). \n37. \nMarchisio, A. et al. Deep Learning for Edge Computing: Current Trends, Cross-Layer Optimizations, and \nOpen Research Challenges. in Proc of 2019 IEEE Computer Society Annual Symposium on VLSI (ISVLSI) \n553-559 (2019). \n38. \nLarger, L. et al. Photonic information processing beyond turing: an optoelectronic implementation of reservoir \ncomputing. Opt. Express 20, 3241-3249 (2012). \n39. \nLarger, L. et al. High-speed photonic reservoir computing using a time-delay-based architecture: Million \nwords per second classification. Phys. Rev. X 7, 011015 (2017). \n40. \nAppeltant, L. et al. Information processing using a single dynamical node as a complex system. Nat. Commun. \n2, 468 (2011). \n41. \nSoriano, M. C. et al. Optoelectronic reservoir computing: tackling noise-induced performance degradation. \nOpt. Express 21, 12-20 (2013). \n42. \nLarger, L. & Dudley, J. M., Nonlinear dynamics: Optoelectronic chaos. Nature 465, 41-42 (2010). \n43. \nChembo, Y. K., Brunner, D., Jacquot, M. & Larger, L. Optoelectronic oscillators with time-delayed feedback. \nRev. Mod. Phys. 91, 035006 (2019). \n44. \nMurphy, T. E. et al. Complex dynamics and synchronization of delayed-feedback nonlinear oscillators. Phil. \nTrans. R. Soc. A 368, 343-366 (2010). \n45. \nOrtín, S. et al. Aunified framework for reservoir computing and extreme learning machines based on a single \ntime-delayed neuron. Sci. Rep. 5, 14945 (2015). \n46. \nStelzer, F., Röhm, A., Lüdge, K., & Yanchuk, S., Performance boost of time-delay reservoir computing by \nnon-resonant clock cycle. Neural Networks 124, 158-169 (2020). \n47. \nBrockman, G. et al. OpenAI Gym. Preprint at https://arxiv.org/abs/1606.01540 (2016). \n48. \nKumar, S. Balancing a CartPole System with Reinforcement Learning - A Tutorial. Preprint at \nhttps://arxiv.org/abs/2006.04938 (2020). \n49. \nVan Hasselt, H., Guez, A., & Silver, D. Deep reinforcement learning with double q-learning. in Proc of \nThirtieth AAAI conference on artifficial intelligence (2016). \n50. \nUchida, A., McAllister, R., & Roy, R. Consistency of nonlinear system response to complex drive signals. \nPhys. Rev. Lett. 93 244102 (2004). \n51. \nNakayama, J., Kanno, K., & Uchida, A. Laser dynamical reservoir computing with consistency: an approach \nof a chaos mask signal. Opt. Express 24, 8679-8692 (2016). \n52. \nO'Neill, J., Pleydell-Bouverie, B., Dupret, D., & Csicsvari, J., Play it again: reactivation of waking experience \nand memory. Trends Neurosci. 33, 220-229 (2010). \n53. \nDuport, F., Smerieri, A., Akrout, A. Haelterman, M., & Massar, S., Fully analogue photonic reservoir \ncomputer. Sci. Rep. 6, 22381 (2016). \n \nAcknowledgments \nThis study was supported in part by JSPS KAKENHI (JP19H00868 and JP20K15185), JST CREST JP-MJCR17N2, and \nthe Telecommunications Advancement Foundation. \n \nAuthor contributions \nAll authors contributed to the development and/or implementation of the idea. \nK. K. performed the numerical simulations and analyzed the data. K. K. and A. U. contributed to the discussion of the \nresults. K. K. and A. U. contributed to writing the manuscript. \n \nCompeting interests \nThe authors declare that they have no competing interests. \n \n \n \n",
  "categories": [
    "cs.ET",
    "nlin.CD",
    "physics.optics"
  ],
  "published": "2022-02-25",
  "updated": "2022-02-25"
}