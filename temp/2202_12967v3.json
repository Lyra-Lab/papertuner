{
  "id": "http://arxiv.org/abs/2202.12967v3",
  "title": "Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates",
  "authors": [
    "Souradeep Dutta",
    "Kaustubh Sridhar",
    "Osbert Bastani",
    "Edgar Dobriban",
    "James Weimer",
    "Insup Lee",
    "Julia Parish-Morris"
  ],
  "abstract": "Long horizon robot learning tasks with sparse rewards pose a significant\nchallenge for current reinforcement learning algorithms. A key feature enabling\nhumans to learn challenging control tasks is that they often receive expert\nintervention that enables them to understand the high-level structure of the\ntask before mastering low-level control actions. We propose a framework for\nleveraging expert intervention to solve long-horizon reinforcement learning\ntasks. We consider \\emph{option templates}, which are specifications encoding a\npotential option that can be trained using reinforcement learning. We formulate\nexpert intervention as allowing the agent to execute option templates before\nlearning an implementation. This enables them to use an option, before\ncommitting costly resources to learning it. We evaluate our approach on three\nchallenging reinforcement learning problems, showing that it outperforms\nstate-of-the-art approaches by two orders of magnitude. Videos of trained\nagents and our code can be found at:\nhttps://sites.google.com/view/stickymittens",
  "text": "Exploring with Sticky Mittens:\nReinforcement Learning with Expert Interventions\nvia Option Templates\nSouradeep Dutta1, Kaustubh Sridhar1, Osbert Bastani1, Edgar Dobriban1, James Weimer2,\nInsup Lee1, Julia Parish-Morris3\n1University of Pennsylvania, 2Vanderbilt University, 3Children’s Hospital of Philadelphia\nAbstract: Long horizon robot learning tasks with sparse rewards pose a signif-\nicant challenge for current reinforcement learning algorithms. A key feature en-\nabling humans to learn challenging control tasks is that they often receive ex-\npert intervention that enables them to understand the high-level structure of the\ntask before mastering low-level control actions. We propose a framework for\nleveraging expert intervention to solve long-horizon reinforcement learning tasks.\nWe consider option templates, which are speciﬁcations encoding a potential op-\ntion that can be trained using reinforcement learning. We formulate expert in-\ntervention as allowing the agent to execute option templates before learning an\nimplementation. This enables them to use an option, before committing costly\nresources to learning it. We evaluate our approach on three challenging reinforce-\nment learning problems, showing that it outperforms state-of-the-art approaches\nby two orders of magnitude. Videos of trained agents and our code can be found\nat: https://sites.google.com/view/stickymittens\nKeywords: Sample-Efﬁcient Reinforcement Learning, Expert Intervention, Op-\ntions, Planning with Primitives\n1\nIntroduction\nReinforcement learning is an effective tool to solve difﬁcult tasks such as robot planning and loco-\nmotion [1] but exploration is still a challenge. Options are an RL tool to circumvent this problem [2].\nDesigned to achieve intermediate subgoals. For instance, in robot grasping tasks, an option might\nenable the robot to grasp a block, which is a subgoal needed to build a tower out of blocks. The goal\nis to learn a policy mapping each state to an option, instead of a concrete action to take.\nWhen learning to perform complex visual-motor skills, humans often rely on expert interventions to\nhelp them escape these challenging reward plateaus. For instance, the sticky-mittens experiment [3]\nconsiders infants who have not yet learned to grasp objects. They give a subset of these infants\nmittens covered with Velcro hooks and allow them to play with toys ﬁtted with Velcro loops, making\nit signiﬁcantly easier for them to grasp these toys. Even if the Velcro is taken away, these babies\nlearn how to grasp objects signiﬁcantly faster than infants not exposed to this experience. In other\nwords, enabling infants to explore unreachable parts of the state space helps guide them towards\nskills that are worth learning. This is a well known phenomenon in developmental psychology,\nwhich extends beyond ﬁne motor skills.\nIn this paper, we design an RL algorithm based on the idea from the sticky-mittens experiment.\nThe agent has access to an alternative Markov Decision Process (MDP) where the agent can leap\nmultiple states without ﬁrst learning a policy to do so in the original MDP. We term such a jumping\nmechanism an option template. Option templates are described using a set of initial states and a set\nof ﬁnal states. The idea of providing external help in the learning phase is referred to as primitives\nor skills in literature. It offers a practical way to speed-up learning in a realistic setting. For instance,\n6th Conference on Robot Learning (CoRL 2022), Auckland, New Zealand.\narXiv:2202.12967v3  [cs.LG]  17 Nov 2022\nusing parameterized action spaces for RoboCup in [4], stitching independent behaviors in [5, 6] and\nproviding action primitives in [7, 8, 9] to mention a few.\nIn the more typical RL framework the agent ﬁrst uses reinforcement learning to train an option to\nimplement the speciﬁcation of each option template. The issue with this strategy is that, in RL\nenvironments with large state spaces the options learnt need to be of a fairly generic nature, in order\nto be useful. Which is hard without a knowledge of the state distribution, where the options will be\ninvoked. Using option templates we decouple the implementation of an option from its utility.\nIn more detail, our algorithm performs an alternating search over policies: at each iteration, it ﬁrst\noptimizes the high-level policy over the current option templates, and then learns options to imple-\nment the option templates. In our experiments, we demonstrate that by leveraging option templates,\nour algorithm can achieve orders of magnitude reduction in sample complexity compared to the\ntypical strategy of learning the options upfront and then planning with these learned options.\nContributions:\nOur contributions are: (1) an RL algorithm that leverages option-templates, and\n(2) an empirical comparison with state-of-the-art RL techniques on three challenging environments,\ndemonstrating order-of-magnitudes reduction in sample complexity.\n2\nBackground\nWe consider the RL setting where an agent interacts with an environment modeled as an MDP [10].\nDeﬁnition 2.1 (MDP). A Markov Decision Process (MDP) is a tuple E = (S, A, P, R, γ, I), where\nS ⊆Rn is the set of states, A ⊆Rm is the set of actions, P(s′|s, a) is the probability of transitioning\nfrom state s to s′ upon taking action a, R(s, a) is the reward accrued in state s upon taking action\na, γ ∈[0, 1) is the discount factor, and I is the initial state distribution.\nFigure 1: Option vs Option\nTemplate.\nThe value of a state for policy π is the expected return Rπ(s0)\nstarting from state s0, while executing the policy π at every step,\nat = π(st). The optimal policy π∗maximizes the reward start-\ning from the initial state distribution–i.e., π∗= arg max V I(π),\nwhere V I(π) = Es0∈I[Rπ(s0)]. Semi-MDPs (SMDPs) extend this\nframework by allowing temporally extended actions—i.e., policies\nexecuted over multiple steps—called options [2].\nDeﬁnition 2.2 (Option). An option o is a tuple o := (I, T, π),\nwhere I ⊆S are initial states, T : S →[0, 1] the termination\ncondition, and π : S →A is a policy.\nFor now, assume that we have a set O of options available for an MDP. Intuitively, options constrain\nthe search space over policies, thereby guiding exploration performed by the agent towards more\npromising action sequences. We now consider policies Π : S →O that map a state s ∈I (i.e.,\ns is a valid initial state for o) to an option o = Π(s). The agent uses option policy π until the\ntermination condition holds. More precisely, after taking action a = π(s) and transitioning to state\ns′ ∼P(·|s, a), it stops using π with probability T(st+1) and chooses another option o′ = Π(s′);\notherwise, it continues using π.\nSuppose that an option o is invoked at time t in state st, and the system follows a k step trajectory\nTro(st, k) = (st, at, rt, st+1, . . . , st+k). We consider policies such that for t ≤i < t + k, the\naction ai depends on the entire history Tro(st, i) from t until t + i. This renders the behavior non-\nMarkovian. Denoting the set of all such state-action trajectories as H, we can allow option policies\nπ : H →A, and termination conditions T : H →[0, 1] to be semi-Markovian. Now, we recall the\ndeﬁnition of Semi-MDPs as an extension of MDPs:\nDeﬁnition 2.3 (Semi-MDP, Sutton et al. [2]). A semi-MDP is a tuple (S, O, P, R, γ, I), where S\nis a set of states, O is a set of semi-Markovian options, P is the transition probability between states,\nR is the reward function, γ ∈[0, 1) is the discount factor, and I is the initial state distribution.\nThe state-prediction part of the model for taking an option o, in state s ∈I and transitioning to s′,\nis given by Sutton et al. [2]: P(s′|s, o) = P∞\nk=1 γkp(s′, s, k), where p(s′, s, k) is the probability\nthat the option terminates in s′ after k steps from the time the option is invoked. The option-\nvalue form of standard value iteration is, for s ∈S and o ∈O, [2, eq. 12], [11] Q∗\nO(s, o) =\nR(s, o) + P\ns′∈S P(s′|s, o) maxo′∈O Q∗\nO(s′, o′), where s′ is the state the system reaches after exe-\ncuting option o starting from state s. SMDP value learning updates the Q-values at the end of each\n2\noption termination as [2, p. 195]: Qi+1(s, o) ←Qi(s, o)+α[rc+γk maxo′∈O Qi(s′, o′)−Qi(s, o)],\nfor similar deﬁnitions of k, s, s′, with rc being the cumulative discounted reward over the time hori-\nzon, and α ∈(0, 1) the learning rate. Choosing a function approximator parameterized by θ to\nrepresent Q(s, o; θ), with a loss given by : L(θi) = [r + γk maxo′∈O Qi(s′, o′; θi) −Qi(s, o; θi)]2,\nallows one to use standard Q-learning [12] methods to train with options.\n3\nLearning with Option Templates\n3.1\nOption Templates\nMotivated by the sticky-mittens experiment, we consider an environment where in certain states, the\nagent can call a “help switch”, called an option template, that immediately transitions it from a state\nst to a different state st+k; this transition captures the desired result of executing an (unimplemented)\noption. We denote a trajectory of this MDP starting from state st under policy π by Trπ(st).\nDeﬁnition 3.1 (Option template). Option template ao is a tuple (I, T, P), where I ⊆S is the\ninitial states, T : S →[0, 1] is the termination condition, and P is a distribution over states s such\nthat T(s) = 1.\nAn option template similar to an option, shifts control to the expert until the termination condition is\nreached; this termination condition T is intended to capture the satisfaction of some sub-goal along\nwith a timeout mechanism. Assume the termination condition at time t + k of an option template\ntaken in state st is of the form T(st, k) = Fo(st+k)∨(k > k∗), where the function Fo : S →{0, 1}\ncaptures visitation of some key states, and k∗is an upper limit on the number of steps.\nWe denote the set (or subset) of option templates by AO. By the end of training, the agent must learn\na policy to implement each option template that it uses (i.e., the ﬁnal policy cannot depend on option\ntemplates). To ensure feasiblity, we assume there is an a priori unknown policy πo such that Trπo(st)\nsatisﬁes T (denoted as Trπo(st) |= T) with probability at least 1 −δ, for some hyperparameter δ ∈\n[0, 1). Formally, the probability of success of a given policy πo can be expressed as P((πo, I, E) |=\nT) ≥(1 −δ)—i.e., the trajectory starting from a state in I under the control of πo in environment\nE satisﬁes T with probability at least 1 −δ. This success probability can be estimated by empirical\nrollouts of πo.\nWe organize option-templates in a hierarchical fashion depending on degrees of help. Option tem-\nplates that traverse longer sequences of transitions allow faster learning due to fewer decisions. Thus,\nwe consider a sequence of environments E0, E1, . . . , En−1 = E, equipped with option templates of\nvarying capabilities organized in a hierarchical fashion.\nDeﬁnition 3.2 (Environment Level El). An environment El at learning level l is the original envi-\nronment E with the primitive actions A replaced by option templates Al\nO.\nThe goal of the levels is to gradually increase difﬁculty of the learning task while guiding the agent\nat each level in a way that is similar to curriculum learning [13]. In particular, learning longer\ntimescale option templates typically precedes learning shorter ones.\nAssumption 3.3 (Realizability). For any option template ao ∈Al\nO at level l, given by (I, T, P),\nthere is a policy πao : S →Al+1\nO\nat the following level l + 1 such that for any s0 ∈I, Tro(s0) |= T\nwith probability at least 1 −δ.\nThat is, any option template in the current level is realizable by a policy in the following level.\n3.2\nLearning with Option Templates\nOur algorithm for RL with option templates is presented in Algorithm 1.\n3\nAlgorithm 1 Learning with Option Templates\nInput: Environments [E0, E1, . . . , En−1]\nOutput:\nA set of options D = {oq | q ∈\nQ} that provides an implementation of each\noption template aq\no, where q ∈Q indexes the\noption templates.\n1: Initialize D = {}\n2: for level l ∈{1, . . . , n} do\n3:\nLet Al−1\no\nbe the set of option templates\non level l −1\n4:\nBuild reward functions {Rq | aq\no ∈\nAl−1\no\n}\n5:\nfor aq\no ∈Al−1\no\ndo\n6:\nπq\no =LearnOptionPolicy(El, Rq, aq\no, D)\n7:\noq := (aq\no, πq\no)\n8:\nD = D ∪{oq}\n9:\nend for\n10: end for\noutput D\nFigure 2: Visualization of learning with op-\ntion templates.\nAlgorithm 2 Learn Option Policy\nInput: Environment E, Reward Function R,\nOption template ao = (I, T, P), Set D of op-\ntions\nParameter: Threshold δ, Max Episodes L\nOutput:\nPolicy πo as an implementation of\nao\n1: Initialize policy πo\n2: for episode = 1, . . . , L do\n3:\nExecutionStack = []\n4:\no ←Top-level option from D (or ao if\nD = ∅)\n5:\nPush o onto ExecutionStack\n6:\nSample initial state s ∼I\n7:\nwhile not done do\n8:\no ←Pull option from Execution-\nStack\n9:\no′ ←πo(s)\n10:\nif IsOption(o′, ao) then\n11:\n(πo, s)\n←\nStepAndTrain(E, R, πo, s)\n12:\nelse if ¬HasImplementation(o′, D)\nthen\n13:\ns ←Teleport(o′, s)\n14:\nelse\n15:\nPush o′ to ExecutionStack\n16:\nend if\n17:\nend while\n18:\nExit loop if AvgReward ≥1 −δ\n19: end for\noutput πo\nIntuitively, at each iteration, it “ﬂattens” the option templates used on the previous level l −1 based\non the ones available at the current level l. At each level l, we ﬁrst design reward functions Rq\nfor each option template aq\no ∈Al−1\no\n(Line 4); this reward function is later used to learn a policy\nπq\no implementing aq\no. For the base case l = 1, we take the reward function Rq to be the reward\nfunction for the original environment E; thus, learning the policy for the option-template at this\nlevel amounts to accomplishing the goals of E using the (unique) option template aq\no ∈A0\no. For\nsubsequent iterations l > 1, Rq encodes the goal of achieving the termination condition of aq\no, based\non its termination condition (Line 4).\nGiven Rq, our algorithm learns a policy that maximizes Rq using the options available at the cur-\nrent level l (Line 6). For l = 1, we assume there is a single option template A0\no = {aq\no}; the\ncorresponding policy πq\no aims to achieve the goal in the original environment E, so we refer to the\nresulting option oq = (aq\no, πq\no) as the top-level option. For l > 1, πq\no implements an option template\naq\no ∈Al−1\no\nat level l −1 using the option templates available at the current level l. Importantly, this\nprocess leverages policies learned so far to generate initial states from which to learn πq\no. Once we\nhave learned πq\no, we add the resulting option oq = (aq\no, πq\no) to D.\nRecall that the termination condition for an option starting at state s0 is T(s0, k∗) = Fo(sk) ∨(k >\nk∗). The goal of training πq\no is to satisfy this condition with high probability. Hence, when training\npolicy πq\no for option aq\no, we choose the reward function Rq to be Fo, and additionally restrict the\nlength of each episode to k∗time-steps.\nLearning option policies. Next, we describe LearnOptionPolicy, which Algorithm 2 uses to learn\na policy for option template aq\no. For simplicity, we denote the current environment by E, the current\nreward function by R, the option template by ao, and the target policy by πo. The goal of πo is to\nachieve the termination condition T for ao = (I, T, P) from initial states s ∈I.\n4\n(a) Visualization of craft envi-\nronment for the get gem task:\nThe agent (red square) uses the\naxe in its inventory to break the\nstone and retrieve the gem.\n(b) Visualization of fetch and\nstack environment: Each block\nhas to be placed at the location\nrepresented by the color-coded\nsphere.\n(c) Visualization of the Google football\nenvironment: The agents learn to use 11\nplayers of the left team and score goals.\nFigure 3: Craft, Fetch and GFootball environments.\nOne challenge is sampling an initial state s ∈I from which to train πo. To do so, this subroutine\nleverages access to the previously learned options D; it uses options in D until it arrives at a state\ns ∈I where ao is called. In more detail, it samples s by executing the top-level option in D until\nit reaches a state where ao is called. In general, executing an option o either relies on executing its\npolicy πo in D if it exists, or by executing its “teleport” functionality. Our algorithm keeps track\nof the execution of options for which πo exists using a stack, which is initialized with the top-level\noption (Line 3-5). The structure is visualized in Figure 2. Then, while executing the current option\no, it obtains the next option o′ to execute, which is processed in one of three ways:\n• If o′ is the option for ao (i.e., it has a matching initial state, termination condition, and a\ncurrent policy ˜πo, checked by IsOption on Line 10), then it takes steps to train πo (Line\n11); as described below.\n• If o′ does not have an implementation (checked by HasImplementation on Line 12), then it\nteleports—i.e., we sample s ∼P (Line 13).\n• Otherwise, o′ has an implementation, so it pushes o′ onto the stack (Line 15).\nFinally, once we are at a state s where ao is called, the subroutine StepAndTrain takes control and\ntrains πo using a standard reinforcement learning algorithm—i.e., by collecting rewards from s and\ntaking a policy gradient step once it reaches a terminal state or hits the timeout of k∗steps. Then,\nthe current episode continues from the state reached.Once the current episode reaches a ﬁnal state,\nthen the algorithm continues to the next episode. This process continues until the average reward of\nπo exceeds a threshold 1 −δ, or until a maximum number of episodes L is reached.\n4\nExperiments\n4.1\nExperiments on Planning tasks in the Craft Environment\nDescription of environment: The craft environment [14, 15] is a 2D world based on the Minecraft\ngame, where an agent has to complete various hierarchical tasks with sparse rewards. The envi-\nronment is represented by a 12 × 12 grid with cells containing raw resources (e.g., WOOD, IRON),\ncrafting areas (e.g., WORKBENCH), obstacles (e.g., STONE, WATER) and valuable items (GOLD or\nGEM). There are four move actions (up, down, left, right) and a special USE action. To grab an item,\nthe agent moves to a neighboring cell and applies the USE action. Table 1 shows the hierarchical\narrangement of the get gem task; the details for get gold are in the Appendix.\nOption template at each environment: Our option templates (in Table 1) capture the hierarchy\nof task-dependencies in the environment. For example, it is easier to get GEM when the agent has\naccess to AXE (see Table 1). Thus, at the topmost level the agent can “ask” for an AXE, and via\nprimitive actions, use it to break stone and get GEM to realize the importance of an AXE.\nImplementation: In each task of each learning level, we use a vanilla actor-critic algorithm [16]\nwith a network with one hidden layer of 100 neurons for both the actor and critic. The input to the\nnetwork is a feature vector consisting of one-hot encodings of the items in each cell in a 5 × 5 grid\n5\nTask\nPolicy Sketch\nget wood\n-\nget iron\n-\nmake stick\nget wood →use anvil\nmake axe\nmake stick →get iron\n→use workbench\nget gem\nmake axe →break stone\nTask\nLearning\noption templates\nlevel\nget gem\n1\n{give axe, primitive actions}\nmake axe\n2\n{give stick, give iron, primitive actions}\nmake stick\n3\n{give wood, primitive actions}\nget iron\n3\n{primitive actions only}\nget wood\n4\n{primitive actions only}\nTable 1: Get gem hierarchical task: [LEFT] policy sketches [14] & [RIGHT] option templates for\neach task in the hierarchy. The order of the rows represent the learning order in the two alternatives.\nFigure 4: Average reward vs episodes for solving each hierarchical sub-task. We compare option\ntemplates (ours) and option-value iteration (baseline) for the task get gem in craft environment.\naround the agent along with a one-hot encoding of its inventory. At the topmost level, the agent is\ngiven a reward of 1 if and only if GOLD or GEM are obtained. After training, we obtain a actor\nnetwork for individual tasks at different learning levels.\nComparison with option-value iteration: We implement option value iteration [2] as a compari-\nson with standard hierarchical learning. In option value iteration, we learn options bottom-up, using\noptions learnt at a lower level to accomplish sub-tasks of options at the level above. That is the\nagent learns to implement lower level sub-tasks ﬁrst before it learns how to use them. We plot av-\nerage reward as a function of episodes for each sub-task of get gem in Figure 4; for get gold the\nFigure is in Appendix A.2. Figure 4 shows that option templates obtain higher average rewards than\noption-value iteration at all levels (detailed discussion in Appendix A.2).\nTask\nEpisodes\nCurriculum learning\nOption templates\n[14]\nget gem\n> 3 × 106\n12826.0 ± 2613.0\nmake axe\n> 2.7 × 106\n11283.0 ± 2255.0\nmake stick\n> 1.3 × 106\n5026.0 ± 2231.0\nTable 2: Comparison of total episodes (and stan-\ndard deviations over ten random seeds) to train an\nagent to solve the get gem task via option tem-\nplates and curriculum learning [14].\nComparison with curriculum learning [14]:\nAdditionally, we compare our method with the\ncurriculum learning algorithm employed in An-\ndreas et al. [14] and observe a 100 fold de-\ncrease in the number of episodes required to\ntrain an agent for the get gem and get gold tasks\n(see Table 2 for get gem and Appendix for get\ngold). For the proposed method, we report the\ntotal episodes it requires for average reward to\nstay consistently above 0.8. Averaged over ten\nruns for different random seeds. The calcula-\ntion of the total episodes for each task, includes\nall the sub-tasks in its hierarchy. For instance,\nthe episodes for get gem completion via option templates include episodes required for complet-\ning make axe, make stick, get wood and get iron. The latter two tasks are straightforward, and not\nincluded in the table.\n4.2\nExperiments on Manipulation Tasks in the Fetch and Stack Environment\nDescription of environment: This continuous action space environment, introduced in Lanier [17],\nconsists of a robotic arm and a platform with blocks placed on it in random positions; see Figure 3b.\nThe goal is for the arm to move, lift, and release the colored blocks in the location and stacking order\nspeciﬁed by the corresponding color-coded spheres. The action consists of torques for 3 degrees-of-\nfreedom actuation, and an on-off control input opens and closes the gripper. The environment offers\na sparse reward for each block placed at the goal location.\n6\nTask(s)\nlevel\nOption templates\nFetch & Stack\nN blocks\n1\n{Place block i at its goal\nlocation}i=1,...,N\nPlace block i at\nits goal location\n2\n{Reach block i,\nPick\nblock i & reach goal,\nRelease block i & lift,\nDo nothing}i=1,...,N\nTable 3: Option templates for fetch & stack.\nTask(s)\nlevel\nOption templates\nWin game\n1\n{Attack\nand\nscore\ngoals, Defend}\nAttack\nand\nscore goals\n2\n{Maintain ball pos-\nsession,\nCharge\nto\nthe opponent’s goal,\nand Shoot}\nTable 4: Option templates for gfootball.\n(a) Average reward vs timesteps\nfor option templates and the\nbaseline.\n(b) Average reward vs timesteps\nfor option templates and the\nbaseline.\n(c) Comparison of time steps and corre-\nsponding average goal difference.\nFigure 5: Results on the Fetch & Stack and GFootball environments.\nLearning levels and option templates: We introduce two learning levels with tasks and option\ntemplates described in Table 3. In this environment, the primitive actions are not continuous-space\nactions. Instead, at the lowest level (level 2), we expose the agent to options which are implemented\nwith proportional feedback controllers [18]. Such simple primitives have the ability to improve ex-\nploration and can be easily transferred among different learning scenarios. Further, the hierarchical\nstructure improves the agent’s learning speed as compared to a case where the agent has to learn to\nuse the options at the bottom level (2) directly (see Appendix A.3 for more details).\nImplementation: We use a standard DQN [12] for each level with four hidden layers of 300\nneurons each. The inputs to the agent are the 3D coordinates of the different blocks, their goals, and\nthe states of the gripper arm. The agent has 150 and 200 steps for stacking three and four blocks,\nrespectively, in the correct order. We also supply demonstrations to speed up learning.\nBaseline: We consider a baseline that directly exposes the agent to all option templates, at level 2\nof the hierarchy. Figure 3b shows the average rewards as a function of episodes. As can be seen,\nthe baseline only recieves one-third reward since it only succeeds in pushing the bottom block to its\nlocation. It does not learn to stack even after twice the number of learning steps as our method. This\ndemonstrates the challenge in learning longer duration tasks without a higher level guiding policy.\nComparison with Learning with Demonstrations [1]:\nIn learning with demonstrations [1], the\nauthors use demonstration traces, by combining behavioral cloning along with a Q-ﬁlter mechanism\nto speed-up the learning. This allows them to evade an expensive exploration phase in the early\nstages of the learning. This method takes upwards of 3.5 × 108 and 8 × 108 timesteps to learn\nstacking of three and four blocks respectively which is three orders of magnitude larger than our\naverage learning time of 4.5 × 105 and 6 × 105 timesteps respectively (from 5 random seeds).\n4.3\nExperiments on Multi-Robot Tasks in the GFootball Environment\nDescription of environment:\nThe Google football (gfootball) environment [19] is an 11 vs. 11\ngame of soccer. The opponents are controlled by an inbuilt game engine with game-play at three\nlevels of difﬁculty (easy, medium, and hard). An RL agent can control up to 11 players on the left\nteam. The agent provides each player with one of 19 actions such as a direction to move (e.g. top,\ntop-right, bottom-left, etc.), type of pass (short, long or high), shoot, or toggles for dribbling and\nsprinting. We provide more details in Appendix A.4.\nLearning levels and option templates: We create two learning levels with option templates as\ngiven in Table 4. The primitive actions in this environment are defend, maintain ball possession,\ncharge to the opponent’s goal and shoot which are options implemented with simple planers and\nopen-loop controllers. Similarly, we implement option templates at level 1 (see Appendix A.4).\n7\nImplementation:\nWe train a standard DQN [12] for each level. The input to every network is a\n139 element vector consisting of left and right team states, ball state, score, and one-hot encodings\nof ball ownership and game mode. For level 1, for all levels of difﬁculty, we use a network with 2\nhidden layers of 500 neurons each. For level 2, we use a network with 5, 6, and 7 hidden layers for\neasy, medium and hard respectively where each hidden layer has 500 neurons.\nBaseline: We consider a baseline where the agent is directly exposed to all the primitive options.\nFrom Figure 5c, we ﬁnd that the baseline is unable to compete with option template learning even\nafter twice the number of steps of our method (numerical values of Figure 5c are in Appendix A.4).\nThe error bars represent standard deviation over 5 random seeds.\nComparison with IMPALA and DQN from Kurach et al. [19]: We compare our agent with the\nagents in Kurach et al. [19]. As can be seen in Figure 5c, option template learning can achieve\nsimilar (in easy) or better (in medium and hard) performance than the IMAPALA and DQN agents\nin two orders of magnitude fewer steps.\n5\nRelated Work\nWe present a detailed discussion on related work in Appendix A.1 and summarize closely relevant\nliterature here.\nOn expert help via primitives or skills: Various recent works utilize expert help in the form of\nprimitives or skills [4, 5, 6, 7, 8, 9, 20]. This takes place via parameterized action spaces [4],\nstitching together independent task schemas (or skills) [5, 6, 9, 20] or learning parameters of action\nprimitves [7, 9, 8]. In all of these cases, learning takes place within the traditional hierarchical\nframework, i.e., bottom-up (see Figure 2), where a sub-task is learnt before the policy that uses it.\nOur method proposes a framework to learn top-down instead with large-improvements in sample-\nefﬁciency over traditional bottom-up learning. With minimal changes in the implementation of their\nprimitives and skills, the above diverse strategies can also beneﬁt with shorter learning times by\nutilizing our framework to learn top-down.\nOn expert help with humans-in-the-loop: Other approaches to expert intervention include explicit\nhelp with humans-in-the-loop required throughout training [21, 22]. In contrast, in our method,\nhuman effort is only required at the beginning to create the option template hierarchies.\nOn exploration in high-dimensional tasks: An alternative approach to counter the need for ex-\nploration in high dimensional tasks is through the use of demonstrations [1] for long horizon plan-\nning. The intuition is that introducing a degree of behavioral cloning of expert demonstrations helps\nreduce the amount of exploration the agent has to perform. A different approach known as Hind-\nsight Experience Replay (HER) [23] incorporates the goal information into the state, using a failed\nterminal state as an alternative goal to reward the transitions leading to it. These methods are or-\nthogonal to our approach. By themselves, they are unable to achieve the large degree of reduction\nin sample-complexity seen with our framework. Yet, alongside expert help, they further improve\nsample-efﬁciency as seen in our experiments.\n6\nLimitations and Conclusion\nWe have proposed an approach that incorporates option templates into reinforcement learning. Our\nexperiments show that this strategy can drastically reduce sample complexity by implementing tele-\nportation. This can be a potential limitation. In simulation, implementing teleportation is typically\nstraightforward. Policies may be trained in simulation before being deployed on a real robot. For\nreal-world environments, there are several strategies for implementing teleportation. First, for chal-\nlenging skills such as grasping an object, teleportation can be implemented via a temporary crutch\nthat simpliﬁes the skill. For example, in the “sticky-mittens” experiments (a key motivation for our\nwork); the analog for a grasping robot would be to attach velcro to its grippers and to the objects\nto make them easy to pick up. Second, teleportation can be implemented via a handcrafted policy\nthat eventually achieves the goal but possibly in a suboptimal way. For instance, teleportation in the\nFetch and GFootball environments are implemented using handcrafted policies. While these policies\nare used to train the RL policy, the RL policy eventually signiﬁcantly outperforms them.\n8\nAcknowledgements\nThis work was supported in part by ARO W911NF-20-1-0080 and AFRL and DARPA FA8750-\n18-C-0090. Any opinions, ﬁndings, conclusions or recommendations expressed in this material are\nthose of the authors and do not necessarily reﬂect the views of the Air Force Research Labora-\ntory (AFRL), the Army Research Ofﬁce (ARO), the Defense Advanced Research Projects Agency\n(DARPA), the Department of Defense, or the United States Government.\nReferences\n[1] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming exploration\nin reinforcement learning with demonstrations. CoRR, abs/1709.10089, 2017. URL http:\n//arxiv.org/abs/1709.10089.\n[2] R. S. Sutton, D. Precup, and S. Singh.\nBetween mdps and semi-mdps: A framework for\ntemporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112(1):181–211, 1999.\nISSN 0004-3702. doi:https://doi.org/10.1016/S0004-3702(99)00052-1. URL https://www.\nsciencedirect.com/science/article/pii/S0004370299000521.\n[3] L. van den Berg and G. Gredeb¨ack. The sticky mittens paradigm: A critical appraisal of current\nresults and explanations. Developmental Science, 24(5):e13036, September 2021. doi:https:\n//doi.org/10.1111/desc.13036. URL https://onlinelibrary.wiley.com/doi/abs/10.\n1111/desc.13036.\n[4] M. Hausknecht and P. Stone. Deep reinforcement learning in parameterized action space. In\nProceedings of the International Conference on Learning Representations (ICLR), May 2016.\n[5] P. MacAlpine, M. Depinet, J. Liang, and P. Stone. UT austin villa: Robocup 2014 3d simula-\ntion league competition and technical challenge champions. In R. A. C. Bianchi, H. L. Akin,\nS. Ramamoorthy, and K. Sugiura, editors, RoboCup 2014: Robot World Cup XVIII [papers\nfrom the 18th Annual RoboCup International Symposium, Jo˜ao Pessoa, Brazil, July 15, vol-\nume 8992 of Lecture Notes in Computer Science, pages 33–46. Springer, 2014. doi:10.1007/\n978-3-319-18615-3 3. URL https://doi.org/10.1007/978-3-319-18615-3_3.\n[6] A. Barreto, D. Borsa, S. Hou, G. Comanici, E. Ayg¨un, P. Hamel, D. Toyama, S. Mourad,\nD. Silver, D. Precup, et al. The option keyboard: Combining skills in reinforcement learning.\nAdvances in Neural Information Processing Systems, 32, 2019.\n[7] M. Dalal, D. Pathak, and R. Salakhutdinov. Accelerating robotic reinforcement learning via\nparameterized action primitives. CoRR, abs/2110.15360, 2021. URL https://arxiv.org/\nabs/2110.15360.\n[8] S. Nasiriany, H. Liu, and Y. Zhu. Augmenting reinforcement learning with behavior primitives\nfor diverse manipulation tasks. arXiv preprint arXiv:2110.03655, 2021.\n[9] R. Chitnis, S. Tulsiani, S. Gupta, and A. Gupta. Efﬁcient bimanual manipulation using learned\ntask schemas. In 2020 IEEE International Conference on Robotics and Automation (ICRA),\npages 1149–1155. IEEE, 2020.\n[10] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n[11] A. G. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. Dis-\ncrete Event Dynamic Systems Volume 13, 13:2003, 2003.\n[12] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.\nPlaying atari with deep reinforcement learning, 2013.\n[13] G. Hacohen and D. Weinshall. On the power of curriculum learning in training deep networks.\nCoRR, abs/1904.03626, 2019. URL http://arxiv.org/abs/1904.03626.\n[14] J. Andreas, D. Klein, and S. Levine. Modular multitask reinforcement learning with policy\nsketches. In International Conference on Machine Learning, pages 166–175. PMLR, 2017.\n9\n[15] F. Behbahani. Craft environment. https://github.com/Feryal/craft-env, 2018.\n[16] V. Konda and J. Tsitsiklis. Actor-critic algorithms. Society for Industrial and Applied Mathe-\nmatics, 42, 04 2001.\n[17] J. B. Lanier. Curiosity-driven multi-criteria hindsight experience replay. University of Cali-\nfornia, Irvine, 2019.\n[18] K. J. Astrom and R. M. Murray. Feedback Systems: An Introduction for Scientists and Engi-\nneers. Princeton University Press, USA, 2008. ISBN 0691135762.\n[19] K. Kurach, A. Raichuk, P. Sta´nczyk, M. Zajac, O. Bachem, L. Espeholt, C. Riquelme, D. Vin-\ncent, M. Michalski, O. Bousquet, et al.\nGoogle research football: A novel reinforcement\nlearning environment. arXiv preprint arXiv:1907.11180, 2019.\n[20] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine, and P. Sermanet. Learning\nlatent plans from play. In Conference on robot learning, pages 1113–1132. PMLR, 2020.\n[21] Q. Li, Z. Peng, and B. Zhou. Efﬁcient learning of safe driving policy via human-ai copilot\noptimization. arXiv preprint arXiv:2202.10341, 2022.\n[22] J. Spencer, S. Choudhury, M. Barnes, M. Schmittle, M. Chiang, P. Ramadge, and S. Srinivasa.\nExpert intervention learning. Autonomous Robots, 46(1):99–113, 2022.\n[23] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,\nP. Abbeel, and W. Zaremba. Hindsight experience replay. CoRR, abs/1707.01495, 2017. URL\nhttp://arxiv.org/abs/1707.01495.\n[24] C. Watkins and P. Dayan. Technical note: Q-learning. Machine Learning, 8:279–292, 05 1992.\ndoi:10.1007/BF00992698.\n[25] A. Akhmetzyanov, R. Yagfarov, S. Gafurov, M. Ostanin, and A. Klimchik. Continuous control\nin deep reinforcement learning with direct policy derivation from q network. In T. Ahram,\nR. Taiar, V. Gremeaux-Bader, and K. Aminian, editors, Human Interaction, Emerging Tech-\nnologies and Future Applications II, pages 168–174, Cham, 2020. Springer International Pub-\nlishing.\n[26] F.-H. Hsu. Behind Deep Blue: Building the Computer That Defeated the World Chess Cham-\npion. Princeton University Press, USA, 2002. ISBN 0691090653.\n[27] D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. Driessche, J. Schrittwieser,\nI. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalch-\nbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis.\nMastering the game of go with deep neural networks and tree search. Nature, 529:484–489,\n01 2016. doi:10.1038/nature16961.\n[28] A. Levy, R. P. Jr., and K. Saenko. Hierarchical actor-critic. CoRR, abs/1712.00948, 2017. URL\nhttp://arxiv.org/abs/1712.00948.\n[29] C. Yang, K. Yuan, Q. Zhu, W. Yu, and Z. Li. Multi-expert learning of adaptive legged locomo-\ntion. CoRR, abs/2012.05810, 2020. URL https://arxiv.org/abs/2012.05810.\n[30] L. Marzari, A. Pore, D. Dall’Alba, G. Aragon-Camarasa, A. Farinelli, and P. Fiorini. Towards\nhierarchical task decomposition using deep reinforcement learning for pick and place subtasks.\nCoRR, abs/2102.04022, 2021. URL https://arxiv.org/abs/2102.04022.\n[31] J. Winder, S. Milani, M. Landen, E. Oh, S. Parr, S. Squire, M. desJardins, and C. Matuszek.\nPlanning with abstract learned models while learning transferable subtasks. Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, 34(06):9992–10000, Apr. 2020. doi:10.1609/aaai.\nv34i06.6555. URL https://ojs.aaai.org/index.php/AAAI/article/view/6555.\n[32] K. Jothimurugan, O. Bastani, and R. Alur. Abstract value iteration for hierarchical reinforce-\nment learning. CoRR, abs/2010.15638, 2020. URL https://arxiv.org/abs/2010.15638.\n10\n[33] D. Abel, N. Umbanhowar, K. Khetarpal, D. Arumugam, D. Precup, and M. Littman. Value\npreserving state-action abstractions. In S. Chiappa and R. Calandra, editors, Proceedings of\nthe Twenty Third International Conference on Artiﬁcial Intelligence and Statistics, volume 108\nof Proceedings of Machine Learning Research, pages 1639–1650. PMLR, 26–28 Aug 2020.\nURL https://proceedings.mlr.press/v108/abel20a.html.\n[34] Y. Yang, J. Inala, O. Bastani, Y. Pu, A. Solar-Lezama, and M. Rinard. Program synthesis\nguided reinforcement learning, 02 2021.\n[35] N. Gopalan, M. desJardins, M. L. Littman, J. MacGlashan, S. Squire, S. Tellex, J. Winder, and\nL. L. S. Wong. Planning with abstract markov decision processes. In ICAPS, 2017.\n[36] K. Jothimurugan, S. Bansal, O. Bastani, and R. Alur. Compositional reinforcement learning\nfrom logical speciﬁcations. CoRR, abs/2106.13906, 2021. URL https://arxiv.org/abs/\n2106.13906.\n[37] C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-P´erez.\nIntegrated task and motion planning. Annual review of control, robotics, and autonomous\nsystems, 4:265–293, 2021.\n[38] A. Levy, G. Konidaris, R. Platt, and K. Saenko. Learning multi-level hierarchies with hindsight.\narXiv preprint arXiv:1712.00948, 2017.\n[39] X. Yang, Z. Ji, J. Wu, Y.-K. Lai, C. Wei, G. Liu, and R. Setchi. Hierarchical reinforcement\nlearning with universal policies for multistep robotic manipulation. IEEE Transactions on\nNeural Networks and Learning Systems, 2021.\n11\nA\nAppendix\nA.1\nRelated Work\nRecent progress in RL has partly been made possible by combining rich function classes like deep\nneural networks with powerful techniques such as Q-learning [24] and actor-critic approaches [16].\nIn recent years huge strides have been made in training agents for complex tasks, such as for playing\nAtari games with discrete and continuous action spaces [12, 25]. Additionally, RL has achieved\nhuman-level performance in games like chess [26] and Go [27].\nHowever, it is becoming increasingly clear that simple exploration is not enough to circumvent the\ncurse of dimensionality in environments with long horizons and sparse rewards. As a remedy the\nbroad area of Hierarchical Reinforcement Learning (HRL) attempts to decompose RL problems\ninto multiple levels of abstraction— temporal, spatial, or otherwise. Many works deploy separate\npolicies over different time horizons and action spaces [11, 28, 29, 30]. Temporal abstraction in\nplanning can be traced back at least to Sutton et al. [2], where the options were introduced to refer\nto lower level policies. In most existing research in hierarchical RL, learning a sub-task precedes\nthe learning of a policy which uses it. This includes [31, 32, 33, 34, 35]. This paper is set apart\nby the fact it does not rely on an apriori knowledge of the starting distributions of the sub-tasks\nas required by some related literature on hierarchical RL, such as [36]. We further elaborate on\nliterature relevant to the two important facets of our approach below.\nOn expert help via primitives or skills: Various recent works utilize expert help in the form of\nprimitives or skills [4, 5, 6, 7, 8, 9, 20]. This takes place via parameterized action spaces [4],\nstitching together independent task schemas (or skills) [5, 6, 9, 20] or learning parameters of action\nprimitves [7, 9, 8]. In all of these cases, learning takes place within the traditional hierarchical\nframework, i.e., bottom-up (see Figure 2), where a sub-task is learnt before the policy that uses it.\nOur method proposes a framework to learn top-down instead with large-improvements in sample-\nefﬁciency over traditional bottom-up learning. With minimal changes in the implementation of their\nprimitives and skills, the above diverse strategies can also beneﬁt with shorter learning times by\nutilizing our framework to learn top-down.\nOn expert help with humans-in-the-loop: Other approaches to expert intervention include explicit\nhelp with humans-in-the-loop required throughout training [21, 22]. In contrast, in our method,\nhuman effort is only required at the beginning to create the option template hierarchies.\nOn hierarchical learning in robotics: Task and motion planning (TAMP) methods [37] are alter-\nnate hierarchical approaches for tackling long-horizon problems in robotics. They generally high-\nlight the interplay of motion-level and task-level planning, with the task-level planner constraining\nthe motion-level planner. In contrast, our approach allows the task-level planner to learn using fea-\nsible paths which can be satisﬁed by the motion-level planner. This allows us to achieve high reward\nwith few samples.\nOn exploration in high-dimensional tasks: An alternative approach to counter the need for ex-\nploration in high dimensional tasks is through the use of demonstrations [1] for long horizon plan-\nning. The intuition is that introducing a degree of behavioral cloning of expert demonstrations helps\nreduce the amount of exploration the agent has to perform. A different approach known as Hind-\nsight Experience Replay (HER) [23] incorporates the goal information into the state, using a failed\nterminal state as an alternative goal to reward the transitions leading to it. These methods are or-\nthogonal to our approach. By themselves, they are unable to achieve the large degree of reduction\nin sample-complexity seen with our framework. Yet, alongside expert help, they further improve\nsample-efﬁciency as seen in our experiments.\n12\nTask\nPolicy Sketch\nget wood\n-\nget iron\n-\nmake bridge\nget wood →get iron\n→use factory\nget gold\nmake bridge →\nuse bridge on water\nTask\nLearning\nOption templates\nlevel\nget gold\n1\n{give bridge, primitive actions}\nmake bridge\n2\n{give wood, give iron, primitive\nactions}\nget wood\n3\n{primitive actions}\nget iron\n3\n{primitive actions}\nTable 5: Get gem hierarchical task: [LEFT] policy sketches [14] & [RIGHT] option templates for\neach task in the hierarchy. The order of the rows represent the learning order in the two alternatives.\nTask\nEpisodes\nCurriculum learning\nOption templates\n[14]\n(Ours)\nget gold\n> 2.65 × 106\n10516.0 ± 2833.0\nmake bridge\n> 1.8 × 106\n7974.0 ± 1853.0\nTable 6: Comparison of total episodes (and standard deviations over ten random seeds) to train an\nagent to solve the get gem task via option templates and curriculum learning [14].\nA.2\nAdditional Details of Experiments on the Craft Environment\nA.2.1\nResults on the get gold Task.\nFigure 6: Average reward vs episodes for solving each hierarchical sub-task. We compare option\ntemplates (our method) and option-value iteration (baseline) for the task get gold in craft environ-\nment.\nA.2.2\nAdditional Environment Details\nFor a fair comparison with [14], we set the maximum steps in the environment for get wood and get\niron to 100; for make stick to 200; for make bridge to 300; for make axe and get gold to 400; and for\nget gem to 500.\nA.2.3\nHyperparameters\nThe Adam optimizer was used to train all networks. The batch size was always set to the size of the\nmemory. The exploration (ϵ) is exponentially reduced. For all tasks, we use a discount factor (γ)\nof 0.99 for primitive actions and 0.99d for option templates. The exponent (d) is the expected task\nhorizon and is reported in Table 7. Other hyperparameters are also given in Table 7.\nIn option value iteration, we use the exact task horizon which is available to the agent since options\nare learnt bottom-up. The other hyperparameters in option value iteration are the same as those\nreported for option templates.\nA.2.4\nDiscussion\nFigures 4 and 6 show that option templates obtain a higher average reward than option-value iter-\nation at all levels. In particular, at level 2, the two curves appear closer since the option templates\n13\nParameters\nget gem\nmake axe\nmake stick\nget gold\nmake bridge\nexpected task horizon (d)\n100\n50\n40\n100\n50\nmemory size (transitions)\n20×d\nlearning frequency (steps)\n20×d\nlearning rate\n0.001\nTable 7: Hyperparameters for option templates in craft environment.\ncurve converges to a lower value than in levels 1 and 3, likely due to the tasks on level 2 being\nmore difﬁcult. We believe the gap occurs because policies at the levels below the one currently\nbeing learned typically do not perform perfectly in option-value iteration. Further, these errors may\n“confuse” training the option in option-value iteration. Whereas in our approach which uses option\ntemplates which do not make these errors, we avoid the aforementioned issue.\nA.3\nAdditional Details of Experiments on the Fetch and Stack Environment\nThe Fetch and Stack environment is implemented inside the MuJoCo physics engine designed to\nsimulate multi-body interactions including contacts, joints and collision. Unlike craft, this envi-\nronment does not permit the implementation of option-templates as part of its action space. We\ncircumvent this problem by adding action primitives using simple state-feedback controllers. Each\noption-template is implemented in this fashion, for different learning levels (see Table 3), until the\nlowest level is reached. The agent treats the state feedback controllers as option-templates during\ntraining phase. As intended, the agent only gets access to read the states after the option-templates\nreach the termination condition. The option templates can also terminate after a timeout (80 steps\nfor level 1 and 30 steps for level 2).\nA.3.1\nHyperparameters\nThe Adam optimizer was used to train all networks. The batch size was always set to 20% of the\nmemory size. The exploration (ϵ) is exponentially reduced. All other hyperparameters are given in\nTable 8. The baseline used the same hyperparameters as option templates.\nParameters\nlevel 1\nlevel 2\ndiscount factor (γ)\n0.2\n0.8\nmemory size (transitions)\n100\nlearning frequency\nevery 1500 steps for 3 blocks\nevery 2000 steps for 4 blocks\nlearning rate\n0.001\nTable 8: Hyperparameters for option templates in fetch and stack environment.\nA.3.2\nComparison to Learning multi-level hierarchies with hindsight [38]\nWe also compared our method with Levy et al. [38]’s Hindsight Actor Critic (HAC) algorithm in the\nFetch and Stack environment on the three block stacking task and clearly notice that it cannot learn\nto stack even two blocks in more than 10 times our learning time.\nHAC [38] obtain an average reward of only 0.138 after (67.2 ± 0.1) × 105 steps (obtained over 5\nrandom seeds). In comparison, we obtain an average reward of 1 after a learning duration of only\n(4.5 ± 0.1) × 105 timesteps.\nWe note that, we use the high-level / low-level learning strategy described in Yang et al. [39] and\nonly mention the high-level learning time (which is much lower than the low-level learning time)\nfor Levy et al. [38] here for a fair comparison. We plot the variation of rewards at all levels with our\nmethod, baseline and HAC [38] in Figure 7.\nFurther, the method proposed in Yang et al. [39] called Universal Option Framework is shown to\noutperform Levy et al. [38]’s Hindsight Actor Critic. Even with this boost, according to Yang et al.\n[39], their method attains a 0.7 average reward at the high-level only after (480±3.2)×105 timesteps.\n14\nMethod\nEasy\nMedium\nHard\navg. goal\nsteps\navg. goal\nsteps\navg. goal\nsteps\ndifference\ndifference\ndifference\nOption\n5.79 ± 0.92\n0.3 M\n3.0 ± 0.71\n0.3 M\n2.0 ± 0.18\n0.3 M\nTemplates\n(win game)\n(win game)\n(win game)\n+ 1.8 M\n+ 1.8 M\n+ 1.8 M\n(attack)\n(attack)\n(attack)\n= 2.1 M\n= 2.1 M\n= 2.1 M\nBaseline\n1.2 ± 0.22\n4.2 M\n0.8 ± 0.33\n4.2 M\n0.26 ± 0.1\n4.2 M\n1-Player\n5.14 ± 2.88\n500 M\n−0.36 ± 0.11\n500 M\n−0.47 ± 0.48\n500 M\nImpala [19]\n1-Player DQN\n8.16 ± 1.05\n500 M\n2.01 ± 0.27\n500 M\n0.27 ± 0.56\n500 M\nDQN [19]\nTable 9: Numerical values of Figure 5c: comparison of time steps and corresponding average goal\ndifference (including standard deviation across 5 random seeds) for option templates (controlling\n11-players), baseline (controlling 11-players), and single-player agents [19].\nFigure 7: Average reward vs episodes for all levels with option templates (our method), the baseline\nand HAC [38] in the Fetch and Stack environment.\nA.4\nAdditional Details of Experiments on the GFootball Environment\nThe GFootball environment returns a unit reward for each goal scored. For our approach, we con-\nsider an agent controlling all 11 players; in contrast, the implementations of some baselines we\ncompare to only control one player. The task where the agent must control all 11 players is signiﬁ-\ncantly more challenging, and we are unaware of any 11-player baseline. This is because, our action\nspace is much larger, and difﬁcult for exploration.\nSimilar to fetch and stack, since the environment does not have any teleportation support, we imple-\nment option templates using simple planners and open-loop controllers (described below). We also\nprovide the numerical values plotted in Figure 6 in Table 9.\nA.4.1\nAdditional Environment and Option Template Details\nThe dimensions of the football ﬁeld is bounded by the following limits : [−1, 1] along the x-\ncoordinate, and [−0.42, 0.42] along the y-coordinate. Of the 19 available actions, there are 8 move-\nment actions, one for each of the following directions - {top, top-right, right, bottom-right, bottom,\nbottom-left, left, and top-left }. There are three actions for passing - { long pass, short pass and high\npass}, and one for shooting. There are actions to toggle the following modes - { sprinting, dribbling\nand movement}. The environment permits an action to let the game engine pick a default move.\nThe implementations of the option templates in Table 4 are described below :\n15\nCharge to the opponent’s goal: This option template makes the ball controlling player on the\nagent’s team run in a straight line to the center of the opponent’s goal. The precondition for this\noption template being that the agent’s team has the ball. The option template terminates when a\nplayer with the ball is within a distance of 0.3 units from the goal (1,0). If the precondition is\nsatisﬁed, the player sprints in one of the above 8 directions, depending on the angle between the ball\ncontrolling player and the opponent’s goal.\nMaintain ball possession: This option template enables the agent’s team to maintain ball possession\nby either passing the ball to a teammate or move slowly towards the goal while avoiding opponent\nplayers who can intercept the ball. When no opponent interceptors are identiﬁed ahead of the ball\ncontrolling player, in one of 5 relevant directions (top, top-right, right, bottom-right or bottom), the\nplayer moves in the identiﬁed free direction. Which by default, is towards the opponent’s side of the\nﬁeld. This attacker is supported by two players (wings). Otherwise, if no free space is identiﬁed,\nthe ball controlling player passes the ball to a teammate with the least number of opponents around\nhim who can intercept the pass. The choice of action (short, long, and high - pass) is based on the\ndistance between the two players.\nShoot: This option template is the same as the environment action to shoot a goal.\nAttack and score goals: This option template is a composition of charge to the opponent’s goal,\nmaintain ball possession and shoot. When the x coordinate of the ball controlling player is greater\nthan or equal to 0.5, the ball controlling player charges to the goal. When this attacking player is\nwithin 0.25 units from the goal, the player takes a shot. Otherwise, the agent’s team simply maintains\nball possession.\nDefend: This option template replicates the inbuilt game engine to block goal attempts by the other\nteam and get ball possession.\nAll of the above option templates will also terminate after 200 steps. Additionally, for level 1, we\nmask all but the one hot encoding of ball ownership in the input. Further, while it is possible to\ncombine defend with attack and score goals to play the game, they are unable to achieve similar\nperformance levels as compared to the baseline or option templates.\nA.4.2\nHyperparameters\nThe Adam optimizer was used to train all networks. The batch size was always set to 40% of the\nmemory size. The exploration (ϵ) is exponentially reduced. All other hyperparameters are given in\nTable 10. The hyperparameters for level 2 in Table 10 were also utilized in the baseline.\nParameters\nlevel 1\nlevel 2\neasy\nmedium\nhard\neasy\nmedium\nhard\ndiscount factor (γ)\n0.9\n0.93\n0.93\n0.96\nmemory size (transitions)\n6000\n6000\n3000\n3000\nlearning frequency\nevery 3000 steps\nevery 3000 steps\nevery 30,000 steps\nlearning rate\n0.001\n0.001\n0.01\n0.005\nTable 10: Hyperparameters for option templates in gfootball environment.\n16\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2022-02-25",
  "updated": "2022-11-17"
}