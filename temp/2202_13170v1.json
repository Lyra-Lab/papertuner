{
  "id": "http://arxiv.org/abs/2202.13170v1",
  "title": "Unsupervised Domain Adaptive Salient Object Detection Through Uncertainty-Aware Pseudo-Label Learning",
  "authors": [
    "Pengxiang Yan",
    "Ziyi Wu",
    "Mengmeng Liu",
    "Kun Zeng",
    "Liang Lin",
    "Guanbin Li"
  ],
  "abstract": "Recent advances in deep learning significantly boost the performance of\nsalient object detection (SOD) at the expense of labeling larger-scale\nper-pixel annotations. To relieve the burden of labor-intensive labeling, deep\nunsupervised SOD methods have been proposed to exploit noisy labels generated\nby handcrafted saliency methods. However, it is still difficult to learn\naccurate saliency details from rough noisy labels. In this paper, we propose to\nlearn saliency from synthetic but clean labels, which naturally has higher\npixel-labeling quality without the effort of manual annotations. Specifically,\nwe first construct a novel synthetic SOD dataset by a simple copy-paste\nstrategy. Considering the large appearance differences between the synthetic\nand real-world scenarios, directly training with synthetic data will lead to\nperformance degradation on real-world scenarios. To mitigate this problem, we\npropose a novel unsupervised domain adaptive SOD method to adapt between these\ntwo domains by uncertainty-aware self-training. Experimental results show that\nour proposed method outperforms the existing state-of-the-art deep unsupervised\nSOD methods on several benchmark datasets, and is even comparable to\nfully-supervised ones.",
  "text": "Unsupervised Domain Adaptive Salient Object Detection Through\nUncertainty-Aware Pseudo-Label Learning\nPengxiang Yan1,2*, Ziyi Wu1*, Mengmeng Liu1, Kun Zeng1, Liang Lin1, Guanbin Li1,3‚Ä†\n1Sun Yat-sen University 2ByteDance Inc. 3Shenzhen Research Institute of Big Data\nyanpx@live.com, wuzy39@mail2.sysu.edu.cn, liumm97@outlook.com,\nzengkun2@mail.sysu.edu.cn, linliang@ieee.org, liguanbin@mail.sysu.edu.cn\nAbstract\nRecent advances in deep learning signiÔ¨Åcantly boost the per-\nformance of salient object detection (SOD) at the expense\nof labeling larger-scale per-pixel annotations. To relieve the\nburden of labor-intensive labeling, deep unsupervised SOD\nmethods have been proposed to exploit noisy labels generated\nby handcrafted saliency methods. However, it is still difÔ¨Åcult\nto learn accurate saliency details from rough noisy labels. In\nthis paper, we propose to learn saliency from synthetic but\nclean labels, which naturally has higher pixel-labeling quality\nwithout the effort of manual annotations. SpeciÔ¨Åcally, we Ô¨Årst\nconstruct a novel synthetic SOD dataset by a simple copy-\npaste strategy. Considering the large appearance differences\nbetween the synthetic and real-world scenarios, directly train-\ning with synthetic data will lead to performance degradation\non real-world scenarios. To mitigate this problem, we pro-\npose a novel unsupervised domain adaptive SOD method to\nadapt between these two domains by uncertainty-aware self-\ntraining. Experimental results show that our proposed method\noutperforms the existing state-of-the-art deep unsupervised\nSOD methods on several benchmark datasets, and is even\ncomparable to fully-supervised ones.\nIntroduction\nSalient object detection (SOD) aims to accurately locate and\nsegment out the most visually distinctive object region in\na scene. In recent years, the development of deep convo-\nlutional neural networks (DCNN) signiÔ¨Åcantly boosts the\nperformance of salient object detection (Wei et al. 2020;\nQin et al. 2019; Wang et al. 2018) and has taken place of\nconventional hand-crafted feature-based algorithms (Zhang\net al. 2015; Zhu et al. 2014; Li et al. 2013) to become the\ndominant methods in salient object detection. However, such\npromising performance comes at a cost of a large number of\npixel-wise annotated images to train the DCNN-based mod-\nels. Moreover, to ensure the quality and consistency of label-\ning, it generally requires multiple human annotators to anno-\ntate Ô¨Åne pixel-level masks for the same image (Li et al. 2017;\nFan et al. 2018). The time-consuming and laborious labeling\nwork limits the amount of training data and thus hampers the\nfurther development of DCNN-based SOD methods.\n*The Ô¨Årst two authors have equal contribution. ‚Ä†Corresponding\nauthor is Guanbin Li (Email: liguanbin@mail.sysu.edu.cn).\nCopyright ¬© 2022, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nTarget Domain\nSource Domain\n(d) EDNL\n(Deep USOD)\n(f) Ours \n(UDASOD)\n(e) Source only\n(w/o DA)\nTest \n(a) Target Image and Noisy Labels\n(b) Source Image and Synthetic Label\n(c) Test Image\non Target Domain\nTrain\nFigure 1: Deep unsupervised salient object detection\n(USOD) achieved by two training settings. Existing deep\nUSOD algorithms are mainly trained on (a) real-world im-\nages (target domain) with noisy labels generated by tra-\nditional USOD methods. While we propose to exploit (b)\nthe synthetic saliency data (source domain) for training.\nHowever, due to the discrepancy between two domains, the\nsaliency detector trained only on synthetic data ((e) Source\nonly) without domain adaption (DA) usually fails to per-\nforms well on (c) real images. To solve this problem, we pro-\npose (f) an unsupervised domain adaptive SOD (UDASOD)\nmethod, which can generate more accurate saliency pre-\ndictions than (d) the best-performing deep USOD method\nEDNL (Zhang, Xie, and Barnes 2020).\nTo alleviate the burden of pixel-wise labeling but take full\nadvantage of the end-to-end training advantages of DCNN,\nweakly-supervised (Zhang et al. 2020b; Zeng et al. 2019;\nLi, Xie, and Lin 2018) and deep unsupervised (Zhang, Xie,\nand Barnes 2020; Nguyen et al. 2019; Zhang et al. 2018)\nSOD algorithms have been proposed. Weakly-supervised\nSOD algorithms mainly focus on learning saliency inference\nfrom simple but clean manual annotations, such as image\nclasses (Li, Xie, and Lin 2018), image captions (Zeng et al.\n2019), and scribbles (Zhang et al. 2020b). While deep unsu-\npervised SOD methods aim to learn saliency detection with-\nout resorting to any manual annotations. Existing deep un-\nsupervised SOD methods mainly focus on learning from the\ndense noisy labels generated by single (Zhang et al. 2020a)\nor multiple (Zhang, Xie, and Barnes 2020; Nguyen et al.\n2019; Zhang et al. 2018; Zhang, Han, and Zhang 2017) tra-\narXiv:2202.13170v1  [cs.CV]  26 Feb 2022\nditional unsupervised SOD methods (as shown in Fig. 1 (a) ),\nwhich can be achieved through noise modeling (Zhang, Xie,\nand Barnes 2020; Zhang et al. 2018) or pseudo-label self-\ntraining (Zhang et al. 2020a; Nguyen et al. 2019; Zhang,\nHan, and Zhang 2017). However, traditional unsupervised\nmethods that rely on manual features and speciÔ¨Åc saliency\npriors are arduous to deal with the complex situation of\nlow foreground/background contrast. The generated pseudo-\nlabels are rich in noise and are almost impossible to be\nrepaired in iterative training based on pseudo-labels, espe-\ncially for the boundary of the salient objects.\nInstead of struggling with the generated noisy labels of\nreal images, in this paper, we propose that learning saliency\nfrom the synthetic but clean labels (Fig. 1 (b)) would be yet\nanother feasible solution. There are massive object images\nwith transparent backgrounds as well as pure background\nimages without salient objects that can be easily collected\nfrom the design resources or photography websites on the\nInternet. Since the salient objects of a scene are usually the\nforeground objects, we construct a new large-scale synthetic\nsalient object detection (SYNSOD) dataset with clean labels\nby simply copying foreground objects and pasting them on\nthe background images. The SYNSOD dataset can be ap-\nplied to existing fully-supervised SOD methods to relieve\nthe burden of manual annotations. However, as shown in\nFig. 1, due to the presence of large appearance differences\nbetween real images (target domain) and synthetic images\n(source domain) known as ‚Äúdomain gap‚Äù, the model directly\ntrained on SYNSOD (Fig. 1 (e)) fails to performs well on\nthe real-world dataset such as DUTS (Wang et al. 2017).\nTo resolve the above issues, we propose a novel unsu-\npervised domain adaptive salient object detection (UDA-\nSOD) algorithm to adapt the DCNN-based saliency detec-\ntor trained on the synthetic dataset to the real-world SOD\ndatasets. The proposed UDASOD algorithm is an iterative\nmethod that exploits an uncertainty-aware pseudo-learning\n(UPL) strategy to achieve adaption between two domains.\nSpeciÔ¨Åcally, in each round of iteration, UDASOD leverages\nthe source images with synthetic labels and the target images\nwith weighted pseudo-labels to jointly train the saliency de-\ntector. After the training of each round, UPL dynamically\nupdates the training set and pseudo-labels of the target do-\nmain through three major steps, including pixel-wise uncer-\ntainty estimation, image-level sample selection and pixel-\nwise pseudo-label reweighting. The main contributions of\nthis paper can be summarized as follows:\n‚Ä¢ To our knowledge, we are the Ô¨Årst attempt to achieve\nSOD by exploiting unsupervised domain adaption from\nsynthetic data, which varies from existing deep unsuper-\nvised SOD algorithms targeted at noisy labels.\n‚Ä¢ We construct a synthetic SOD dataset and further pro-\npose UDASOD that exploits uncertainty-aware pseudo-\nlabel learning to adapt the saliency detector trained on\nthe synthetic dataset to real-world scenarios.\n‚Ä¢ Experimental results show that our proposed domain\nadaptive SOD method outperforms all existing state-of-\nthe-art deep unsupervised SOD methods and is compara-\nble to the fully-supervised ones.\nRelated Work\nSalient Object Detection\nConventional SOD is mainly achieved by different saliency\npriors or handcrafted features (Zhang et al. 2015; Zhu et al.\n2014; Li et al. 2013). Recent advances in DCNNs signiÔ¨Å-\ncantly boost the performance of SOD at a cost of numer-\nous pixel-wise annotations (Chen et al. 2020; Wei, Wang,\nand Huang 2020; Pang et al. 2020; Liu et al. 2019; Wang\net al. 2018). To mitigate the labeling costs, weakly super-\nvised SOD is proposed to learn saliency under weak su-\npervision such as image classes (Li, Xie, and Lin 2018),\ncaptions (Zeng et al. 2019), and scribbles (Zhang et al.\n2020b). Deep unsupervised SOD is further proposed to learn\nsaliency without resorting to any manual annotations. Ex-\nisting deep unsupervised methods mainly rely on learning\nfrom noise labels generated by conventional SOD methods,\nwhich can be achieved through noise modeling (Zhang, Xie,\nand Barnes 2020; Zhang et al. 2018) or pseudo-label self-\ntraining (Zhang et al. 2020a; Nguyen et al. 2019). In this\npaper, we propose to solve SOD from a novel perspective,\ni.e., learning from synthetic but clean labels.\nUnsupervised Domain Adaption\nUnsupervised domain adaptation (UDA) aims to transfer\nthe knowledge learned from the label-rich source domain\nto an unlabeled target domain. It is widely studied on var-\nious vision tasks such as image classiÔ¨Åcation (Sener et al.\n2016), object detection (Chen et al. 2018), semantic segmen-\ntation (Chen et al. 2017), etc. Among these tasks, seman-\ntic segmentation shares most characteristics with SOD. The\nprimary approach of UDA for semantic segmentation is to\nminimize the discrepancy between two domain distributions\nthrough adversarial learning (Hoffman et al. 2018; Luo et al.\n2019; Tsai et al. 2018). There are some self-training-based\nUDA methods (Zou et al. 2018, 2019) that assign pseudo-\nlabels to conÔ¨Ådent target samples and directly use pseudo-\nlabels as target domain supervision to reduce domain mis-\nmatch. To the best of our knowledge, we are the Ô¨Årst attempt\nto relieve the burden of large-scale manual annotations by\nleveraging UDA on salient object detection.\nPseudo-Label Learning\nPseudo-label learning, which is initially explored in semi-\nsupervised learning scenario (Lee et al. 2013), has recently\nattracted wide attention due to its simplicity and effective-\nness. The goal of pseudo-label learning is to fully exploit\nthe unlabeled data by generating and updating pseudo-labels\nfor unlabeled samples with a model trained on labeled data.\nThus, it can be applied to beneÔ¨Åt various tasks such as semi-\nsupervised learning (Lee et al. 2013; Yan et al. 2019), do-\nmain adaption (Zheng and Yang 2021; Li et al. 2020), and\nnoisy label learning (Zhang et al. 2020a; Tanaka et al. 2018).\nThere are also some SOD methods (Li, Xie, and Lin 2018;\nNguyen et al. 2019) that exploit the pseudo-label learn-\ning technique. Different from them, our proposed method\nexploits an uncertainty-aware pseudo-learning strategy that\ntreats each pseudo-label differently and is also free of time-\nconsuming post-processing like fully-connected CRF.\n(a) Foreground Object\n(b) Background\n(c) Synthetic Image\n(d) Synthetic Label\nFigure 2: Examples of the dataset construction of SYN-\nSOD. Each foreground object is matched with a unique\nbackground to generate a synthetic sample through a simple\ncopy-paste strategy. The pixel-level synthetic label can be\nobtained from the alpha channel of the foreground image.\nProposed Dataset\nIn this section, we detail the proposed SYNSOD dataset\nfrom the following aspects.\nImage Collection. As salient objects are usually the\nforeground objects of a scene, we can intuitively obtain a\nsynthetic image with salient objects by pasting the fore-\nground objects on a background image. Thus, to construct\na novel synthetic SOD dataset, we Ô¨Årst collect a large num-\nber of object images with transparent backgrounds (RGBA\ncolor) from several websites with non-copyrighted design\nresources, each of which contains single or multiple ob-\njects of diverse appearances and categories. Next, we collect\nbackground photos from multiple non-copyrighted photog-\nraphy websites, which contains various non-salient scenes\nsuch as forest, grass, sky, ocean, etc. The collection process\nis executed through a designed spider program and images\nwith low resolution will be automatically removed.\nData Generation. Given the premise of the collected\nforeground and background images, we can easily generate\nthe synthetic SOD dataset by a simple copy-paste strategy.\nAs shown in Fig. 2, we match each foreground object image\nwith a unique non-salient background image. Then, we ran-\ndomly scale the object image with a ratio ranging from 0.5 to\n1.1. Next, we set an object center in the background image to\ncover its surrounding pixels with the non-transparent object\npixels, resulting in the synthetic image for SOD. The pixel-\nlevel synthetic label can be easily obtained by binarizing the\ncorresponding alpha channel of the foreground object pixels\nin a synthetic image. In this way, we construct a large-scale\nsynthetic SOD dataset (SYNSOD), containing 11,197 syn-\nthetic images and corresponding pixel-level labels.\nDataset Statistics. As shown in Fig. 3, we present the fol-\nlowing dataset statistics on our proposed SYNSOD dataset\nand Ô¨Åve public benchmark SOD datasets (Wang et al. 2017;\nYan et al. 2013; Yang et al. 2013; Li and Yu 2015; Li et al.\n2014). 1) Object size. As shown in Fig. 3 (a), the ratio\nof salient object size in SYNSOD ranges from 0.39% to\n86.96% (avg.: 14.72%), yielding a border range. 2) Cen-\nter bias. To reveal the degree of center bias, we compute\nthe average saliency maps over all images of each dataset.\nAs shown in Fig. 3 (b), SYNSOD is center-biased and the\ndegree of center-bias is slightly stronger than others, which\nshows strong domain gaps with other real-world datasets.\nDUTS\nDUT-O\nECSSD\nHKU-IS\nSYNSOD\nPASCAL-S\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nDUTS\nECSSD\nDUT-O\nHKU-IS\nPASCAL-S\nSYNSOD\n0-10% (small)\n10%-20% (medium)\n20%-30% (large)\n>30% (huge)\nFrequency\n(a) Distribution of  object size\n(b) Center bias\nFigure 3: Statistics of our proposed SYNSOD dataset in-\ncluding distribution of salient object size and center bias.\nMethodology\nProblem Formulation\nTo achieve SOD without resorting to manual annotations or\nnoisy labels, we propose to learn saliency from the syn-\nthetic but clean labels through a novel unsupervised do-\nmain adaptive salient object detection (UDASOD) frame-\nwork. As shown in Fig. 4, UDASOD is formulated as an it-\nerative training paradigm, which can leverage existing deep\nlearning-based saliency detectors to learn saliency predic-\ntion from synthetic source data and unsupervisedly adapt it\nto the real target scenarios. To fully exploit unlabeled target\nimages, UDASOD is jointly trained with the pseudo-labels\nof target images and the synthetic labels of source images.\nTo formulate UDASOD, we start with the synthetic train-\ning set denoted as the source domain Dsrc = {(Is, ys)}Si\ns=1,\nwhere Is is a synthetic RGB color image of size H √ó W,\nys ‚àà{0, 1}H√óW is the corresponding binary saliency map,\nand Si is the number of source images in round i. The pro-\nposed UDASOD framework will unsupervisedly adapt the\nsaliency detector from the synthetic dataset to the real SOD\ndataset denoted as target domain Dtrg = {(It, ÀÜyt)}T i\nt=1,\nwhere It is a real RGB color image, ÀÜyt ‚àà[0, 1]H√óW is\nthe corresponding pseudo-label, and T i is the number of\npseudo-labels in round i. Thus, the training process of round\ni can be formulated as optimization of the network parame-\nters Œ∏ of the saliency detector as follows:\nŒ∏i = arg min\nŒ∏\nL(Œ∏, i),\n(1)\nwhere the loss function L(Œ∏, i) under the joint supervision\nof source Dsrc and target Dtrg domains is deÔ¨Åned as:\nL(Œ∏|i) = Lsrc(Œ∏|Ii\ns, Y i\ns ) + Ltrg(Œ∏|Ii\nt, ÀÜY i\nt ).\n(2)\nHere, Ys and ÀÜYt denote the set of synthetic source labels\nand the set of pseudo target labels, respectively. Lsrc and\nLtrg refer to the speciÔ¨Åc loss calculation of source and target\nsamples, which will be detailed in the following.\nHowever, since the pseudo-labels of target domain are\ngenerated by the saliency detector initially trained on the\nsource domain, the pseudo-labels inevitably contain incor-\nrect pixel-level prediction due to the signiÔ¨Åcant distribution\ngap between the two domains. To avoid error accumulation\nin the iterative training process, we propose that the sam-\nples of the target domain need to be carefully selected to\nSaliency Detector\n(b) Image-level Sample \nSelection\nUpdate target training\nset for next round\n(a) Consistency-Based Uncertainty Estimation\nTarget Image\nSource Image\nSource Label\nTarget Pseudo-Label\nVariance map\nTarget Domain\nUncertainty-Aware Pseudo-Label Learning\nFlip\nFDA\nOrigin\nScale\n‚Ä¶\nPrediction\nPseudo-Label Weights\n‚Ñíùë†ùëüùëê\n‚Ñíùë°ùëüùëî\nAugmentation\n‚Ä¶\n‚Ä¶\nSelection\nRank\n(c) Pixel-level Pseudo-Label Reweighting\nTarget Image\nPseudo-Label\nLoss Weights\nReverse\nFigure 4: The overall framework of our proposed unsupervised domain adaptive salient object detection method. It iteratively\nlearns saliency from the synthetic labels (source domain) and pseudo-labels of real images (target domain). The pseudo-labels\nwill be dynamically updated after each training round through an uncertainty-aware pseudo-label learning strategy that con-\ntains three major steps, i.e., (a) consistency-based uncertainty estimation, (b) image-level sample selection, and (c) pixel-wise\npseudo-label reweighting. We use three kinds of data augmentations for consistency-based uncertainty estimation, including\n1) horizontal Ô¨Çipping (Flip), 2) rescale input image to 224 √ó 224 (Scale), and 3) randomly swap image style with other target\nimages via FDA (Yang and Soatto 2020).\nparticipate in the training, and each pixel of the selected\nsample should be adaptively assigned different weights.\nTherefore, the loss function for each predicted saliency map\np ‚àà[0, 1]H√óW is formulated with a weight matrix œâ ‚àà\n(0, 1]H√óW as follows:\nL(y, p, œâ) =\nH\nX\nh=1\nW\nX\nw=1\nœâ(h,w)‚Ñì(y(h,w), p(h,w)),\n(3)\nwhere ‚Ñì(.) denotes the binary cross-entropy loss for each\npixel and y ‚àà[0, 1]H√óW denotes the dense label of p. Then,\nthe loss function for the source and target samples can be\nformulated as:\nLsrc(Œ∏|Xi\ns, Y i\ns ) =\nSi\nX\ns=1\nL(ys, pŒ∏(Is), œâs),\n(4)\nLtrg(Œ∏|Xi\nt, Y i\nt ) =\nTi\nX\nt=1\nL(ÀÜyt, pŒ∏(It), œât),\n(5)\nwhere pŒ∏(I) denotes the prediction of saliency detector with\nparameters Œ∏ for input image I. In practice, we only as-\nsign different pixel-wise weights to pseudo-labels while set-\nting œâs = 1 ‚ààRH√óW in source domain. At the end\nof each round, the target training set with pseudo-labels\nwill be dynamically updated and assigned with pixel-level\nweights based on our proposed uncertainty-aware pseudo-\nlabel learning strategy.\nUncertainty-Aware Pseudo-Label Learning\nInstead of equally using all the pseudo-labels, we propose to\nselect target pseudo-labels and assign pixels with different\nweights through an uncertainty-aware pseudo-label learning\nstrategy (UPL) that contains the following three major steps.\n1) Consistency-Based Uncertainty Estimation. To up-\ndate the target pseudo-labels, we Ô¨Årst perform consistency-\nbased uncertainty estimation. SpeciÔ¨Åcally, as shown in\nFig. 4, given a saliency detector with Ô¨Åxed parameters ÀúŒ∏,\nwe feed each real target image It into the saliency detector\nto obtain its pseudo-label ÀÜyt = pÀúŒ∏(It). To model the un-\ncertainty of a target pseudo-label, we consider the following\ntwo aspects. First, the saliency detector will be robust to dif-\nferent small noises on target samples of high-conÔ¨Ådence /\nlow-uncertainty. Second, as it is recognized data augmenta-\ntion can be regarded as a noise injection method (Xie et al.\n2020), we model the uncertainty by evaluating the consis-\ntency of the saliency predictions of the target image It under\nmultiple data augmentations. The salient prediction under\nthe data augmentation {Œ±j(.)}N\nj=1 can be formulated as:\nÀúyj\nt = Œ±‚àí1\nj (pÀúŒ∏(Œ±j(It))).\n(6)\nHere, we only adopt the data augmentation Œ±(.) that can be\nreversed and Œ±‚àí1(.) will be applied for each saliency pre-\ndiction Àúyj\nt to transform it back to the same condition (e.g.,\ndirection, scale) as the pseudo-label ÀÜyt. Inspired by (Zheng\nand Yang 2021), we leverage variance to evaluate the consis-\ntency of the pseudo-label and other saliency predictions of\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.4\n0.6\n0.8\n1.0\nPrecision\nDUTS-TE\nEDNL\nSCRIB\nUSPS\nMWS\nASMO\nMINet\nLDF\nBAS\nDGRL\nOURS\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.4\n0.6\n0.8\n1.0\nPrecision\nECSSD\nEDNL\nSCRIB\nUSPS\nMWS\nASMO\nMINet\nLDF\nBAS\nDGRL\nOURS\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.4\n0.6\n0.8\nPrecision\nDUT-O\nEDNL\nSCRIB\nUSPS\nMWS\nASMO\nMINet\nLDF\nBAS\nDGRL\nOURS\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.4\n0.6\n0.8\n1.0\nPrecision\nHKU-IS\nEDNL\nSCRIB\nUSPS\nMWS\nASMO\nMINet\nLDF\nBAS\nDGRL\nOURS\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.4\n0.6\n0.8\n1.0\nPrecision\nPASCAL-S\nEDNL\nSCRIB\nUSPS\nMWS\nASMO\nMINet\nLDF\nBAS\nDGRL\nOURS\nFigure 5: Quantitative comparison with state-of-the-art SOD methods in terms of Precision-Recall curves.\ndifferent variants of data augmentations. For simpliÔ¨Åcation,\nwe let Àúy1\nt = ÀÜyt. The variance map vt of the sample It can be\nformulated as:\nV ar(It, ÀúŒ∏) = E[(Àúyj\nt ‚àí1\nN (\nN\nX\nj=1\nÀúyj\nt ))2],\n(7)\nwhere E(.) denotes the mathematical expectation. The dense\nvariance map vt ‚ààRH√óW can be used to represent the\npixel-level uncertainty of the target pseudo-label ÀÜyt.\n2) Image-level Sample Selection (ISS). Since the\nsaliency detector is generally weak in the early training stage\nand is gradually improved during iterative training, we pro-\npose that 1) only the pseudo-labels of low uncertainty should\nbe selected and 2) the number of pseudo-label should slowly\nincrease with the increase of training rounds. As shown in\nFig. 4, the variance maps can reÔ¨Çect the pixel-level uncer-\ntainty of the target pseudo-labels, where red and blue indi-\ncate high and low uncertainty, respectively. Thus, to rank the\ntarget sample by their uncertainty, we introduce the image-\nlevel uncertainty score U based on the mean value of vari-\nance (Eq. (7)). The uncertainty score of the target image It\ncan be formulated as:\nU(It, ÀúŒ∏) =\n1\nHW\nH\nX\nh=1\nW\nX\nw=1\nV ar(It, ÀúŒ∏)(h,w).\n(8)\nWe rank all the target domain samples according to the un-\ncertain score and select a certain proportion of target sam-\nples with low uncertainty for each round. The proportion\nwill increase with the improvement of the saliency detec-\ntor. Note that here we also empirically discard those pseudo-\nlabels composed of nearly all salient or non-salient pixels.\n3) Pixel-wise Pseudo-Label Reweighting (PPR). Al-\nthough the selected target pseudo-labels generally reÔ¨Çect a\nlow-uncertainty level, there still exists high uncertainty re-\ngions such as object boundaries as shown in their variance\nmaps. Therefore, we suggest that each pixel of the pseudo-\nlabels should be treated differently during the training pro-\ncess and further propose a pixel-wise pseudo-label reweight-\ning strategy ‚Ñ¶based on the variance maps V ar. The pixel-\nwise weight matrix wt ‚àà(0, 1]H√óW mentioned in Eq. (5)\ncan be replaced by ‚Ñ¶(It, ÀúŒ∏) that is computed as:\n‚Ñ¶(It, ÀúŒ∏) = exp(‚àík V ar(It, ÀúŒ∏)),\n(9)\nwhere k ‚ààR+ indicates the descent degree of the soft\nweights. We set k = 20 in our experiments.\nExperiments\nExperimental Setup\nImplementation Details. We adopt ResNet-50-based (He\net al. 2016) LDF (Wei et al. 2020) as our saliency detec-\ntor. During training, we adopt SYNSOD (11,197 images)\nas the source domain and the training set of DUTS (Wang\net al. 2017) (10,533 images) as the target domain. We set\nthe total number of training rounds to six. The proportion\nof the selected source and target domain samples are set to\n{1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125} and {0.0, 0.1, 0.2,\n0.4, 0.6, 0.6} respectively in the six rounds. The source sam-\nples are randomly selected while the target samples are se-\nlected via the proposed image-level sample selection strat-\negy ISS. We use an SGD optimizer and adopt the linear one\ncycle learning rate policy (Smith and Topin 2019) to sched-\nule each training round. The whole training process takes\nabout 20 hours with a batch size of 32 on a workstation with\na NVDIA GTX 1080 GPU. During testing, each image is\nresized to 352 √ó 352, and fed into the network for saliency\nprediction without any post-processing. More implementa-\ntion details are provided in the supplemental materials.\nDatasets and Evaluation Metrics. To evaluate the per-\nformance of our method, we conduct testing on six real-\nworld benchmark SOD datasets including DUTS-TE (Wang\net al. 2017) (5,017 images), ECSSD (Yan et al. 2013)\n(1,000 images), DUT-O (Yang et al. 2013) (5,168 images),\nHKU-IS (Li and Yu 2015) (4,447 images), PASCAL-S (Li\net al. 2014) (850 images), SOD (Movahedi and Elder 2010)\n(300 images). We adopt four widely used evaluation met-\nrics, i.e., precision-recall (PR) curve, mean absolute error\n(MAE, M) (Perazzi et al. 2012), weighted F-measure (F w\nŒ≤ )\n(Margolin, Zelnik-Manor, and Tal 2014), and S-measure\n(Sm) (Fan et al. 2017).\nComparison with State-of-the-Art\nQuantitative Comparison. In Table 1, we compare our\nmethod with eight fully supervised deep saliency prediction\nmethods: R3Net, DGRL, Capsal, TSPOA, BASNet, MINet,\nGateNet, LDF, two handcrafted unsupervised methods :\nMB+, RBD, and Ô¨Åve deep weakly-/un-supervised methods:\nASMO, MWS, SCRIB, USPS, EDNL. For a fair compari-\nson, we evaluate all the saliency maps provided by the au-\nthors with the same evaluation code. As shown in the ta-\nble, our method consistently outperforms existing weakly-\nsupervised and unsupervised SOD methods by a large mar-\ngin over all six datasets. SpeciÔ¨Åcally our method achieves\nan average gain of 3.65%, 5.56% 1.61% w.r.t Sm, F w\nŒ≤\nMethod\nSup.\nDUTS-TE\nECSSD\nDUT-O\nHKU-IS\nPASCAL-S\nSOD\nSm ‚Üë\nF w\nŒ≤ ‚Üë\nM ‚Üì\nSm ‚Üë\nF w\nŒ≤ ‚Üë\nM ‚Üì\nSm ‚Üë\nF w\nŒ≤ ‚Üë\nM ‚Üì\nSm ‚Üë\nF w\nŒ≤ ‚Üë\nM ‚Üì\nSm ‚Üë\nF w\nŒ≤ ‚Üë\nM ‚Üì\nSm ‚Üë\nF w\nŒ≤ ‚Üë\nM ‚Üì\nR3Net (Deng et al. 2018)\nF&D\n.836\n.713\n.066\n.903\n.860\n.056\n.818\n.679\n.071\n.892\n.833\n.048\n.809\n.730\n.104\n.738\n.700\n.136\nDGRL (Wang et al. 2018)\nF&D\n.842\n.774\n.050\n.903\n.891\n.041\n.806\n.709\n.062\n.894\n.875\n.036\n.836\n.800\n.072\n.774\n.738\n.103\nCapsal (Zhang et al. 2019)\nF&D\n.819\n.689\n.063\n.828\n.775\n.073\n.677\n.489\n.099\n.852\n.780\n.058\n.838\n.790\n.073\n.684\n.573\n.152\nTSPOA (Liu et al. 2019)\nF&D\n.860\n.767\n.049\n.907\n.876\n.046\n.818\n.697\n.061\n.902\n.862\n.038\n.841\n.779\n.078\n.775\n.718\n.115\nBASNet (Wang et al. 2018)\nF&D\n.866\n.803\n.048\n.916\n.904\n.037\n.836\n.751\n.056\n.909\n.889\n.032\n.836\n.795\n.077\n.772\n.728\n.112\nMINet (Pang et al. 2020)\nF&D\n.884\n.825\n.037\n.925\n.911\n.033\n.833\n.738\n.056\n.919\n.897\n.029\n.856\n.814\n.064\n.805\n.768\n.092\nGateNet (Zhao et al. 2020)\nF&D\n.885\n.809\n.040\n.920\n.894\n.040\n.838\n.729\n.055\n.915\n.880\n.033\n.858\n.801\n.069\n.801\n.753\n.098\nLDF (Wei et al. 2020)\nF&D\n.892\n.845\n.034\n.924\n.915\n.034\n.839\n.752\n.052\n.919\n.904\n.028\n.862\n.826\n.061\n.800\n.765\n.093\nMB+ (Zhang et al. 2015)\nU&H\n.595\n.307\n.149\n.595\n.389\n.199\n.612\n.331\n.143\n.609\n.383\n.166\n.528\n.296\n.224\n.490\n.280\n.255\nRBD (Zhu et al. 2014)\nU&H\n.567\n.278\n.305\n.667\n.423\n.271\n.572\n.288\n.310\n.648\n.385\n.271\n.621\n.389\n.297\n.612\n.398\n.305\nASMO (Li, Xie, and Lin 2018)\nW&D\n.697\n.488\n.116\n.802\n.702\n.110\n.752\n.559\n.101\n.804\n.701\n.086\n.714\n.578\n.152\n.669\n.551\n.185\nMWS (Zeng et al. 2019)\nW&D\n.759\n.586\n.091\n.828\n.716\n.096\n.756\n.527\n.109\n.818\n.685\n.084\n.767\n.614\n.134\n.702\n.571\n.166\nUSPS (Nguyen et al. 2019)\nU&D\n.788\n.700\n.068\n.862\n.844\n.062\n.793\n.698\n.063\n.876\n.857\n.041\n.773\n.715\n.108\n.713\n.659\n.143\nEDNL (Zhang, Xie, and Barnes 2020)\nU&D\n.820\n.701\n.065\n.871\n.827\n.068\n.783\n.633\n.076\n.884\n.838\n.046\n.819\n.739\n.095\n.739\n.669\n.142\nSCRIB (Zhang et al. 2020b)\nW&D\n.803\n.709\n.062\n.865\n.835\n.059\n.785\n.669\n.068\n.865\n.831\n.047\n.796\n.736\n.094\n.727\n.668\n.129\nOurs\nU&D\n.846\n.783\n.050\n.899\n.885\n.043\n.808\n.711\n.059\n.897\n.879\n.035\n.822\n.773\n.080\n.788\n.750\n.095\nTable 1: Quantitative comparison with state-of-the-art SOD methods on six datasets in terms of S-measure Sm ‚Üë, weighted\nF-measure F w\nŒ≤ ‚Üë, and MAE M ‚Üì. ‚Üëand ‚Üìindicate larger and smaller is better, respectively. The best performance of fully-\nsupervised and weak-/un-supervised methods is marked in bold, respectively. ‚ÄòSup.‚Äô denotes supervision type. ‚ÄòF&D‚Äô means\nfully-supervised and deep learning-based methods. ‚ÄòU&H‚Äô means unsupervised and handcrafted methods. ‚ÄòW&D‚Äô refers to\nweakly-supervised and deep learning-based methods. ‚ÄòU&D‚Äô means unsupervised and deep learning-based methods.\nFully supervised & Deep Learning\nHandcrafted\nWeakly-/Un-supervised & Deep Learning\nImage\nGT\nCapsal\nBASNet\nMINet\nLDF\nMB+\nRBD\nASMO\nMWS\nUSPS\nSCRIB\nEDNL\nOurs\nFigure 6: Visual comparisons of different types of SOD methods, where each row displays an input image. Our proposed\nmethod (Ours) consistently generates saliency maps close to the ground truth (GT).\nand M compared with previous state-of-the-art weakly-\nsupervised method SCRIB (Zhang et al. 2020b) on six\ndatasets. As for previous state-of-the-art deep unsupervised\nmethod EDNL (Zhang, Xie, and Barnes 2020), our approach\nobtains an average gain of 2.4%, 6,23%, 2.15% w.r.t Sm,\nF w\nŒ≤ and M over six datasets. Moreover, the performance\nof our proposed UDASOD method is comparable to state-\nof-the-art fully-supervised SOD methods, and even better\nthan several of them, such as R3Net (Deng et al. 2018),\nDGRL (Wang et al. 2018), TSPOA (Liu et al. 2019). Fig. 5\npresents the precision-recall curves of different SOD meth-\nods on Ô¨Åve datasets, where weakly-/un-supervised methods\nare represented by dotted lines. From the Ô¨Ågure, we can ob-\nserve that our method overall lies above other weakly-/un-\nsupervised methods and is even comparable to some fully\nsupervised methods.\nQualitative Comparison. Fig. 6 presents several repre-\nsentative visual examples of predicted saliency maps. These\nexamples reÔ¨Çect various scenarios, including small object\n(1st row), object with a complex background (2nd row),\nobject with thread-like boundary (3rd row), low contrast\nbetween salient object and image background (4th row),\nand object with a border-connected region (5th row). It can\nbe seen that our proposed method produces accurate and\ncomplete saliency maps with sharp boundaries and coher-\nent details, which consistently outperforms the weakly-/un-\nsupervised models and even some fully supervised models.\nAblation Study\nEffectiveness of UDASOD. To demonstrate the effective-\nness of our proposed unsupervised domain adaptive salient\nobject detection (UDASOD) through the uncertainty-aware\npseudo-label learning (UPL) strategy, we conduct the abla-\ntion study from the following aspects and report the perfor-\nmance of different variants in Table 2.\n1) Synthetic Data. The saliency detector trained with only\nsynthetic source data (Source only) achieves comparable\nperformance to other unsupervised models (as shown in Ta-\nble 1), indicating the feasibility of learning salient object de-\ntection from the proposed synthetic dataset SYNSOD.\n2) Unsupervised Domain Adaption. Introducing the un-\nlabeled real target data through vanilla pseudo-label learn-\ning (Vanilla PL) strategy can improve the performance of\nsource only model, which demonstrates that a simple unsu-\nMethod\nTraining\nUPL\nDUTS-TE (Wang et al. 2017)\nECSSD (Yan et al. 2013)\nDUT-O (Yang et al. 2013)\nHKU-IS (Li and Yu 2015)\nPASCAL-S (Li et al. 2014)\nSource\nTarget\nISS\nPPR\nSm ‚Üë\nF w\nŒ≤ ‚Üë\nM ‚Üì\nSm ‚Üë\nF w\nŒ≤ ‚Üë\nM ‚Üì\nSm ‚Üë\nF w\nŒ≤ ‚Üë\nM ‚Üì\nSm ‚Üë\nF w\nŒ≤ ‚Üë\nM ‚Üì\nSm ‚Üë\nF w\nŒ≤ ‚Üë\nM ‚Üì\nSource only\n‚àö\n.802\n.695\n.066\n.873\n.836\n.060\n.752\n.608\n.079\n.863\n.814\n.056\n.795\n.725\n.103\nVanilla PL\n‚àö\n‚àö\n.818\n.724\n.067\n.877\n.845\n.057\n.769\n.643\n.084\n.875\n.836\n.049\n.800\n.732\n.100\nUPL w/o ISS\n‚àö\n‚àö\n‚àö\n.823\n.741\n.063\n.883\n.861\n.052\n.778\n.666\n.077\n.880\n.850\n.044\n.805\n.749\n.092\nUPL w/o PPR\n‚àö\n‚àö\n‚àö\n.842\n.777\n.052\n.894\n.878\n.046\n.803\n.704\n.064\n.894\n.875\n.037\n.822\n.774\n.082\nUPL (Ours)\n‚àö\n‚àö\n‚àö\n‚àö\n.846\n.783\n.050\n.899\n.885\n.043\n.808\n.711\n.059\n.897\n.879\n.035\n.822\n.774\n.080\nTable 2: Ablation study on Ô¨Åve benchmark datasets using S-measure Sm ‚Üë, weighted F-measure F w\nŒ≤ ‚Üë, and MAE M ‚Üì.\nAugmentation\nDUTS-TE\nECSSD\nDUT-O\nHKU-IS\nF w\nŒ≤ ‚Üë\nM ‚Üì\nF w\nŒ≤ ‚Üë\nM ‚Üì\nF w\nŒ≤ ‚Üë\nM ‚Üì\nF w\nŒ≤ ‚Üë\nM ‚Üì\nScale\n.765\n.055\n.873\n.049\n.664\n.072\n.869\n.039\nFDA\n.774\n.053\n.872\n.047\n.700\n.067\n.874\n.038\nFlip\n.777\n.053\n.879\n.045\n.697\n.066\n.875\n.038\nFlip+Scale+FDA\n.783\n.050\n.885\n.043\n.711\n.059\n.879\n.035\nTable 3: Sensitivity to different kinds of data augmentation\nin consistency-based uncertainty estimation.\npervised domain adaption through pseudo-label learning can\nhelp to mitigate the domain gap between the synthetic and\nreal domains. While our proposed method (UPL) can fur-\nther boost the performance of vanilla PL by a large margin\nby exploiting image-level sample selection (ISS) and pixel-\nlevel pseudo-label reweighting (PPR).\n3) Uncertainty-Aware Pseudo-Label Learning. To further\nverify the effectiveness of each component in the proposed\nUPL. We conduct the ablation study by removing PPR and\nISS from UPL, respectively, i.e., UPL w/o PPR and ISS w/o\nISS in Table 2. Compared to UPL, the performance of UPL\nw/o PPR slightly drops on Ô¨Åve datasets, which indicates\nthat the selected low uncertainty pseudo-labels still contain\nsome misclassiÔ¨Åed pixels and the PPR module can alleviate\nthe noise of pseudo labels by adjusting the weights of pix-\nels. UPL w/o ISS is iteractively trained with all the target\npseudo-labels without image-level selection, resulting in a\nsevere performance degradation compared to UPL. Theoret-\nically, image-level selection can be approximated as a spe-\ncial case of pixel-level reweighting. However, in practice,\nusing only pixel-level reweighting (UPL w/o ISS) performs\nworse than image-level selection (UPL w/o PPR). We con-\njecture that without image-level selection, the pseudo-labels\nof those high-uncertainty samples naturally have lots of mis-\nclassiÔ¨Åed pixels that will be suppressed by the pixel-wise\nreweighting. As suggested by(Shin et al. 2020), this will lead\nto sparse pseudo-labels and inevitably increase the difÔ¨Åculty\nof network convergence. Whereas, the ISS and PPR mod-\nules are complementary to each other and can further boost\nthe performance of our proposed method.\nSensitivity\nto\nData\nAugmentation.\nOur\nproposed\nmethod leverages multiple data augmentations as noise in-\njection methods to estimate the uncertainty of pseudo-labels.\nTo demonstrate that our method is applicable to different\ndata augmentations, we report the performance using the\naugmentations mentioned in UPL. As shown in Table 3, our\nproposed method is not limited to a single kind of data aug-\nmentation. When applying only one data augmentation (i.e.,\nFlip, Scale, FDA) the proposed uncertainty-aware pseudo-\nlabel learning (UPL) strategy can still work and outperform\nthe vanilla pseudo-label learning strategy (Vanilla PL in Ta-\nble 2) by a large margin, which indicates the robustness\nImage\nGT\nRound 0\nRound 1\nRound 2\nRound 3\nRound 4\nRound 5\n.000\n.025\n.050\n.075\n.100\n.125\nDUTS-TE\nECSSD\nDUT-O\nHKU-IS\nPASCAL-S\nRound 0\nRound 1\nRound 2\nRound 3\nRound 4\nRound 5\nMAE\n(a) MAE of each training round \n(b) Example of saliency prediction on each training round\nFigure 7: Quantitative and visual performance of our pro-\nposed method on each training round.\nof our proposed UPL. Moreover, when combining different\ndata augmentations (Flip+Scale+FDA), the performance of\nUPL can be further improved as the combination leads to\nmore stable uncertainty measurement.\nSensitivity to Training Rounds. Our proposed method\nadopts an iterative training paradigm that contains multi-\nple rounds. To show the performance of each training round\nmore intuitively, we present the MAE results and predicted\nsaliency maps in Fig. 7. As shown in Fig. 7 (a), MAE is\nconsistently improved with the increase of training rounds\nover all datasets. Moreover, as shown in Fig. 7 (b), the non-\nsalient pixels of the predicted saliency map are gradually\nsuppressed and lead to a more accurate result.\nConclusion\nIn this paper, we propose to tackle deep unsupervised salient\nobject detection from a novel perspective, i.e., learning from\nsynthetic but clean labels. To achieve this goal, we construct\na new synthetic salient object detection dataset and intro-\nduce a novel unsupervised domain adaptive salient object\ndetection framework to learn and adapt from the synthetic\ndataset. SpeciÔ¨Åcally, the proposed algorithm exploiting an\nuncertainty-aware pseudo-label learning strategy to mitigate\nthe domain gap between the synthetic source domain and\nthe real target domain. Extensive experiments on multiple\nbenchmark datasets demonstrate the effectiveness and ro-\nbustness of our proposed method, which makes it superior\nto all state-of-the-art deep unsupervised methods and even\ncomparable to fully-supervised methods.\nAcknowledgements\nThis work was supported in part by the Chinese Key-\nArea Research and Development Program of Guangdong\nProvince (2020B0101350001), in part by the Guangdong\nBasic and Applied Basic Research Foundation under Grant\nNo.2020B1515020048, in part by the National Natural\nScience Foundation of China under Grant No.61976250,\nNo.U1811463 and No.61906049, and in part by the Guang-\ndong Provincial Key Laboratory of Big Data Computing, the\nChinese University of Hong Kong, Shenzhen.\nReferences\nChen, Y.; Li, W.; Sakaridis, C.; Dai, D.; and Van Gool, L. 2018.\nDomain adaptive faster r-cnn for object detection in the wild. In\nProceedings of the IEEE Conference on Computer Vision and Pat-\ntern recognition, 3339‚Äì3348.\nChen, Y.-H.; Chen, W.-Y.; Chen, Y.-T.; Tsai, B.-C.; Frank Wang,\nY.-C.; and Sun, M. 2017. No more discrimination: Cross city adap-\ntation of road scene segmenters. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, 1992‚Äì2001.\nChen, Z.; Xu, Q.; Cong, R.; and Huang, Q. 2020. Global context-\naware progressive aggregation network for salient object detection.\nIn Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence,\nvolume 34, 10599‚Äì10606.\nDeng, Z.; Hu, X.; Zhu, L.; Xu, X.; Qin, J.; Han, G.; and Heng, P.-\nA. 2018. R3net: Recurrent residual reÔ¨Ånement network for saliency\ndetection. In Proceedings of the 27th International Joint Confer-\nence on ArtiÔ¨Åcial Intelligence, 684‚Äì690.\nFan, D.-P.; Cheng, M.-M.; Liu, J.-J.; Gao, S.-H.; Hou, Q.; and\nBorji, A. 2018. Salient objects in clutter: Bringing salient object\ndetection to the foreground. In Proceedings of the European Con-\nference on Computer Vision, 186‚Äì202.\nFan, D.-P.; Cheng, M.-M.; Liu, Y.; Li, T.; and Borji, A. 2017.\nStructure-measure: A new way to evaluate foreground maps. In\nProceedings of the IEEE International Conference on Computer\nVision, 4548‚Äì4557.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 770‚Äì778.\nHoffman, J.; Tzeng, E.; Park, T.; Zhu, J.-Y.; Isola, P.; Saenko, K.;\nEfros, A.; and Darrell, T. 2018. Cycada: Cycle-consistent adver-\nsarial domain adaptation. In International Conference on Machine\nLearning, 1989‚Äì1998.\nLee, D.-H.; et al. 2013.\nPseudo-label: The simple and efÔ¨Åcient\nsemi-supervised learning method for deep neural networks.\nIn\nWorkshop on challenges in representation learning, ICML, vol-\nume 3.\nLi, G.; Kang, G.; Liu, W.; Wei, Y.; and Yang, Y. 2020. Content-\nconsistent matching for domain adaptive semantic segmentation.\nIn European Conference on Computer Vision, 440‚Äì456.\nLi, G.; Xie, Y.; and Lin, L. 2018. Weakly supervised salient object\ndetection using image labels. In Proceedings of the AAAI Confer-\nence on ArtiÔ¨Åcial Intelligence, volume 32.\nLi, G.; Xie, Y.; Lin, L.; and Yu, Y. 2017. Instance-level salient\nobject segmentation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2386‚Äì2395.\nLi, G.; and Yu, Y. 2015. Visual saliency based on multiscale deep\nfeatures. In Proceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, 5455‚Äì5463.\nLi, X.; Lu, H.; Zhang, L.; Ruan, X.; and Yang, M.-H. 2013.\nSaliency detection via dense and sparse reconstruction. In Pro-\nceedings of the IEEE International Conference on Computer Vi-\nsion, 2976‚Äì2983.\nLi, Y.; Hou, X.; Koch, C.; Rehg, J. M.; and Yuille, A. L. 2014.\nThe secrets of salient object segmentation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\n280‚Äì287.\nLiu, Y.; Zhang, Q.; Zhang, D.; and Han, J. 2019. Employing deep\npart-object relationships for salient object detection. In Proceed-\nings of the IEEE/CVF International Conference on Computer Vi-\nsion, 1232‚Äì1241.\nLuo, Y.; Liu, P.; Guan, T.; Yu, J.; and Yang, Y. 2019. SigniÔ¨Åcance-\naware information bottleneck for domain adaptive semantic seg-\nmentation. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, 6778‚Äì6787.\nMargolin, R.; Zelnik-Manor, L.; and Tal, A. 2014. How to evalu-\nate foreground maps? In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 248‚Äì255.\nMovahedi, V.; and Elder, J. H. 2010. Design and perceptual val-\nidation of performance measures for salient object segmentation.\nIn Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition Workshops, 49‚Äì56.\nNguyen, D. T.; Dax, M.; Mummadi, C. K.; Ngo, T.-P.-N.; Nguyen,\nT. H. P.; Lou, Z.; and Brox, T. 2019. DeepUSPS: Deep Robust Un-\nsupervised Saliency Prediction With Self-Supervision. In Proceed-\nings of the Conference on Neural Information Processing Systems\n(NeurIPS).\nPang, Y.; Zhao, X.; Zhang, L.; and Lu, H. 2020. Multi-scale inter-\nactive network for salient object detection. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 9413‚Äì9422.\nPerazzi, F.; Kr¬®ahenb¬®uhl, P.; Pritch, Y.; and Hornung, A. 2012.\nSaliency Ô¨Ålters: Contrast based Ô¨Åltering for salient region detec-\ntion. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 733‚Äì740.\nQin, X.; Zhang, Z.; Huang, C.; Gao, C.; Dehghan, M.; and Jager-\nsand, M. 2019. Basnet: Boundary-aware salient object detection.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 7479‚Äì7489.\nSener, O.; Song, H. O.; Saxena, A.; and Savarese, S. 2016. Learn-\ning transferrable representations for unsupervised domain adapta-\ntion. In Proceedings of the 30th International Conference on Neu-\nral Information Processing Systems, 2118‚Äì2126.\nShin, I.; Woo, S.; Pan, F.; and Kweon, I. S. 2020.\nTwo-phase\nPseudo Label DensiÔ¨Åcation for Self-training based Domain Adap-\ntation. In European Conference on Computer Vision, 532‚Äì548.\nSmith, L. N.; and Topin, N. 2019. Super-convergence: Very fast\ntraining of neural networks using large learning rates. In ArtiÔ¨Å-\ncial Intelligence and Machine Learning for Multi-Domain Opera-\ntions Applications, volume 11006, 1100612. International Society\nfor Optics and Photonics.\nTanaka, D.; Ikami, D.; Yamasaki, T.; and Aizawa, K. 2018. Joint\noptimization framework for learning with noisy labels.\nIn Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 5552‚Äì5560.\nTsai, Y.-H.; Hung, W.-C.; Schulter, S.; Sohn, K.; Yang, M.-H.; and\nChandraker, M. 2018. Learning to adapt structured output space for\nsemantic segmentation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern recognition, 7472‚Äì7481.\nWang, L.; Lu, H.; Wang, Y.; Feng, M.; Wang, D.; Yin, B.; and\nRuan, X. 2017. Learning to detect salient objects with image-level\nsupervision. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 136‚Äì145.\nWang, T.; Zhang, L.; Wang, S.; Lu, H.; Yang, G.; Ruan, X.; and\nBorji, A. 2018. Detect globally, reÔ¨Åne locally: A novel approach\nto saliency detection. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 3127‚Äì3135.\nWei, J.; Wang, S.; and Huang, Q. 2020. F3Net: Fusion, Feedback\nand Focus for Salient Object Detection. In Proceedings of the AAAI\nConference on ArtiÔ¨Åcial Intelligence, volume 34, 12321‚Äì12328.\nWei, J.; Wang, S.; Wu, Z.; Su, C.; Huang, Q.; and Tian, Q. 2020.\nLabel decoupling framework for salient object detection. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 13025‚Äì13034.\nXie, Q.; Dai, Z.; Hovy, E.; Luong, T.; and Le, Q. 2020. Unsuper-\nvised Data Augmentation for Consistency Training. In Advances\nin Neural Information Processing Systems, volume 33, 6256‚Äì6268.\nYan, P.; Li, G.; Xie, Y.; Li, Z.; Wang, C.; Chen, T.; and Lin, L.\n2019. Semi-supervised video salient object detection using pseudo-\nlabels. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, 7284‚Äì7293.\nYan, Q.; Xu, L.; Shi, J.; and Jia, J. 2013. Hierarchical saliency\ndetection. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 1155‚Äì1162.\nYang, C.; Zhang, L.; Lu, H.; Ruan, X.; and Yang, M.-H. 2013.\nSaliency detection via graph-based manifold ranking.\nIn Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 3166‚Äì3173.\nYang, Y.; and Soatto, S. 2020. Fda: Fourier domain adaptation for\nsemantic segmentation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 4085‚Äì4095.\nZeng, Y.; Zhuge, Y.; Lu, H.; Zhang, L.; Qian, M.; and Yu, Y. 2019.\nMulti-source weak supervision for saliency detection. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 6074‚Äì6083.\nZhang, D.; Han, J.; and Zhang, Y. 2017. Supervision by fusion:\nTowards unsupervised learning of deep salient object detector. In\nProceedings of the IEEE International Conference on Computer\nVision, 4048‚Äì4056.\nZhang, J.; Dai, Y.; Zhang, T.; Harandi, M. T.; Barnes, N.; and Hart-\nley, R. 2020a. Learning Saliency from Single Noisy Labelling: A\nRobust Model Fitting Perspective. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence.\nZhang, J.; Sclaroff, S.; Lin, Z.; Shen, X.; Price, B.; and Mech, R.\n2015. Minimum barrier salient object detection at 80 fps. In Pro-\nceedings of the IEEE International Conference on Computer Vi-\nsion, 1404‚Äì1412.\nZhang, J.; Xie, J.; and Barnes, N. 2020.\nLearning noise-aware\nencoder-decoder from noisy labels by alternating back-propagation\nfor saliency detection. In Proceedings of the European Conference\non Computer Vision, 349‚Äì366.\nZhang, J.; Yu, X.; Li, A.; Song, P.; Liu, B.; and Dai, Y. 2020b.\nWeakly-supervised salient object detection via scribble annota-\ntions. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 12546‚Äì12555.\nZhang, J.; Zhang, T.; Dai, Y.; Harandi, M.; and Hartley, R. 2018.\nDeep unsupervised saliency detection: A multiple noisy labeling\nperspective. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 9029‚Äì9038.\nZhang, L.; Zhang, J.; Lin, Z.; Lu, H.; and He, Y. 2019. Capsal:\nLeveraging captioning to boost semantics for salient object detec-\ntion. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 6024‚Äì6033.\nZhao, X.; Pang, Y.; Zhang, L.; Lu, H.; and Zhang, L. 2020. Sup-\npress and balance: A simple gated network for salient object detec-\ntion. In Proceedings of European Conference on Computer Vision,\n35‚Äì51.\nZheng, Z.; and Yang, Y. 2021. Rectifying pseudo label learning via\nuncertainty estimation for domain adaptive semantic segmentation.\nInternational Journal of Computer Vision, 1‚Äì15.\nZhu, W.; Liang, S.; Wei, Y.; and Sun, J. 2014. Saliency optimiza-\ntion from robust background detection. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2814‚Äì\n2821.\nZou, Y.; Yu, Z.; Kumar, B.; and Wang, J. 2018. Unsupervised do-\nmain adaptation for semantic segmentation via class-balanced self-\ntraining. In Proceedings of the European Conference on Computer\nVision, 289‚Äì305.\nZou, Y.; Yu, Z.; Liu, X.; Kumar, B.; and Wang, J. 2019. ConÔ¨Å-\ndence regularized self-training. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 5982‚Äì5991.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-02-26",
  "updated": "2022-02-26"
}