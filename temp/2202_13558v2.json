{
  "id": "http://arxiv.org/abs/2202.13558v2",
  "title": "CINO: A Chinese Minority Pre-trained Language Model",
  "authors": [
    "Ziqing Yang",
    "Zihang Xu",
    "Yiming Cui",
    "Baoxin Wang",
    "Min Lin",
    "Dayong Wu",
    "Zhigang Chen"
  ],
  "abstract": "Multilingual pre-trained language models have shown impressive performance on\ncross-lingual tasks. It greatly facilitates the applications of natural\nlanguage processing on low-resource languages. However, there are still some\nlanguages that the current multilingual models do not perform well on. In this\npaper, we propose CINO (Chinese Minority Pre-trained Language Model), a\nmultilingual pre-trained language model for Chinese minority languages. It\ncovers Standard Chinese, Yue Chinese, and six other ethnic minority languages.\nTo evaluate the cross-lingual ability of the multilingual model on ethnic\nminority languages, we collect documents from Wikipedia and news websites, and\nconstruct two text classification datasets, WCM (Wiki-Chinese-Minority) and\nCMNews (Chinese-Minority-News). We show that CINO notably outperforms the\nbaselines on various classification tasks. The CINO model and the datasets are\npublicly available at http://cino.hfl-rc.com.",
  "text": "CINO: A Chinese Minority Pre-trained Language Model\nZiqing Yang†, Zihang Xu†, Yiming Cui‡†∗, Baoxin Wang†, Min Lin,\nDayong Wu†, Zhigang Chen†§\n†State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, Beijing, China\n‡Research Center for SCIR, Harbin Institute of Technology, Harbin, China\n§Jilin Kexun Information Technology Co., Ltd., Changchun, China\n†{zqyang5,zhxu13,ymcui,bxwang2,dywu2,zgchen}@iflytek.com\n‡ymcui@ir.hit.edu.cn\nAbstract\nMultilingual\npre-trained\nlanguage\nmodels\nhave shown impressive performance on cross-\nlingual tasks. It greatly facilitates the applica-\ntions of natural language processing on low-\nresource languages. However, there are still\nsome languages that the current multilingual\nmodels do not perform well on.\nIn this pa-\nper, we propose CINO (Chinese Minority Pre-\ntrained Language Model), a multilingual pre-\ntrained language model for Chinese minority\nlanguages. It covers Standard Chinese, Yue\nChinese, and six other ethnic minority lan-\nguages. To evaluate the cross-lingual ability of\nthe multilingual model on ethnic minority lan-\nguages, we collect documents from Wikipedia\nand news websites, and construct two text\nclassiﬁcation datasets, WCM (Wiki-Chinese-\nMinority) and CMNews (Chinese-Minority-\nNews).\nWe show that CINO notably out-\nperforms the baselines on various classiﬁ-\ncation tasks.\nThe CINO model and the\ndatasets are publicly available at http://\ncino.hfl-rc.com.\n1\nIntroduction\nThe multilingual pre-trained language model\n(MPLM) is known for its ability to understand\nmultiple languages, and its surprising zero-shot\ncross-lingual ability (Wu and Dredze, 2019). The\nzero-shot cross-lingual transfer ability enables the\nMPLM to be applied on the target languages with\nlimited or even no annotated data by ﬁne-tuning the\nMPLM on the source language with rich annotated\ndata. MPLMs greatly facilitate transferring the cur-\nrent NLP technologies to low-resource languages\nand reduce the cost of developing NLP applications\nfor low-resource languages.\nThe existing public MPLMs such as mBERT\n(Devlin et al., 2019), XLM (Conneau and Lam-\nple, 2019) and XLM-R (Conneau et al., 2020) can\n∗Email corresponding.\nhandle 100 languages, but there are still some chal-\nlenges on low-resource languages understanding:\n• The size of pre-training corpora of some low-\nresource languages is small compared to the\nhigh-resource languages. This bias towards high-\nresource languages may harm the performance\non low-resource languages.\n• There are thousands of living languages in the\nworld, but many languages have not been covered\nin the existing MPLMs, especially indigenous or\nethnic minority languages. For example, Tibetan,\na language spoken mainly by Tibetans around\nTibetan Plateau, is absent from the CC-100 cor-\npus. Therefore, the XLM-R tokenizer can not\ntokenize Tibetan scripts correctly, and XLM-R is\nnot good at understanding Tibetan texts.\nRecently, more advanced MPLMs have been pro-\nposed, such as ERNIE-M (Ouyang et al., 2021),\nVECO (Luo et al., 2021) and Unicoder (Huang\net al., 2019). These models focus on multilingual\ntraining objectives, such as leveraging parallel sen-\ntences to improve the alignment between different\nlanguages, and have improved notably over XLM-\nR. However, these models have not paid attention\nto the low-resource languages, so the problem re-\nmains unsolved.\nFor the above reasons, it is necessary to develop\nmultilingual pre-trained language models for low-\nresource and ethnic minority languages. In this\npaper, we focus on Chinese minority languages. In\nChina, Standard Chinese (Mandarin Chinese) is\nthe predominant language. Besides Standard Chi-\nnese, we consider several most spoken minority\nlanguages. These languages are in different lan-\nguage families with varying writing systems, as\nsummarized in Table 1.\nAlthough each of the listed minority languages\nis spoken by at least millions of people, their digital\ncorpus resources are quite limited. For example, in\narXiv:2202.13558v2  [cs.CL]  21 Sep 2022\nISO Code\nLanguage Name\nLanguage Family\nWriting System\nzh\nStandard Chinese (Mandarin)\nSino-Tibetan\nChinese characters\nyue\nYue Chinese (Cantonese)\nSino-Tibetan\nChinese characters\nbo\nTibetan\nSino-Tibetan\nTibetan script\nmn\nMongolian\nMongolic\nTraditional Mongolian script\nug\nUyghur\nTurkic\nUyghur Arabic alphabet\nkk\nKazakh\nTurkic\nKazakh Arabic alphabet\nza\nZhuang\nKra-Dai\nLatin alphabet\nko\nKorean\nIsolate\nHangul\nTable 1: Families and writing systems of the languages covered by CINO.\nthe CC-100 corpus used by XLM-R, the size of the\nUyghur (ug) corpus is 0.4 GB, which is about 1%\nof the Chinese (Simpliﬁed) corpus (46.9 GB); also,\nthere are no Tibetan (bo) or (traditional) Mongolian\n(mn) corpora in the CC-100.\nWe propose a multilingual pre-trained language\nmodel named CINO (Chinese Minority Pre-trained\nLanguage Model), which covers Standard Chinese,\nYue Chinese (Cantonese) and six ethnic minority\nlanguages. As far as we know, this is the ﬁrst multi-\nlingual pre-trained language model for the Chinese\nminority languages. CINO largely has the same\nstructure as XLM-R and has been adapted for mi-\nnority languages by resizing its vocabulary and\nadopting a fast masked language modeling objec-\ntive for the pre-training.\nThe reason for training a multilingual pre-trained\nmodel rather than multiple monolingual pre-trained\nmodels is threefold. First, a multilingual model is\nmore convenient than multiple monolingual mod-\nels. Second, for low-resource languages, multilin-\ngual pre-training leads to better performance than\nmonolingual pre-training (Conneau et al., 2020;\nWu and Dredze, 2020). Third, a multilingual pre-\ntrained model provides cross-lingual transfer abil-\nity, which reduces the data annotation cost for\nlow-resource languages. Studies have also shown\nthat pre-training with more languages leads to bet-\nter cross-lingual performance on low-resource lan-\nguages (Conneau et al., 2020).\nThe public natural language understanding tasks\nin Chinese minority languages are extremely lim-\nited. In this work, we construct two multilingual\ndatasets from two data sources to support evaluat-\ning the zero-shot cross-lingual ability of MPLMs\non the Chinese minority languages: (1) The WCM\n(Wiki-Chinese-Minority) dataset is a multilingual\ntext classiﬁcation dataset built from Wikipedia cor-\npora, with 10 classes, consisting of 63k examples.\n(2) CMNews (Chinese Minority News) dataset is\na multilingual news classiﬁcation dataset with 8\nclasses, built from the crawled news and the pre-\nexisting news datasets, consisting of 57k examples.\nTo evaluate CINO from different perspectives,\nwe run experiments on Tibetan News Classiﬁcation\nCorpus (TNCC), Korean news topic classiﬁcation\n(YNAT), WCM, and CMNews. Results show that\nCINO has acquired the ability of minority language\nunderstanding and outperforms the existing base-\nlines on the Chinese minority languages.\nTo summarize, our contributions are:\n• We introduce CINO, the ﬁrst multilingual pre-\ntrained language model for Chinese minority\nlanguages. Besides Standard Chinese, CINO\ncovers Yue Chinese and six ethnic minority\nlanguages.\n• We construct two multilingual text classiﬁca-\ntion datasets for Chinese minority languages.\nThey are used for evaluating the cross-lingual\nand multilingual abilities of the ethnic minor-\nity language model.\n• Experiments show that CINO achieves no-\ntable improvements over the baselines. Fur-\nthermore, by making the model public, CINO\nwill be a useful resource on Chinese minority\nlanguages and facilitate related research.\n2\nRelated Work\n2.1\nPre-trained Language Models\nMultilingual Pre-trained Language Models. De-\nvlin et al. (2019) introduced the ﬁrst multilin-\ngual pre-trained language model mBERT trained\nwith Masked Language Modeling (MLM). Con-\nneau and Lample (2019) proposed Translation\nLanguage Modeling (TLM) to train the multilin-\ngual model with cross-lingual supervision. Since\nthen, various kinds of multilingual pre-training ob-\njectives have been proposed. Unicoder (Huang\net al., 2019) trains the model with the objec-\ntives including cross-lingual word recovery, cross-\nlingual paraphrase classiﬁcation and cross-lingual\nMLM. InfoXLM (Chi et al., 2021) proposed a pre-\ntraining task based on contrastive learning from an\ninformation-theoretic perspective. Pan et al. (2021)\nalso introduced an alignment method based on con-\ntrastive learning. Cao et al. (2020) proposed an\nexplicit word-level alignment procedure. ERNIE-\nM (Ouyang et al., 2021) integrates back-translation\ninto the pre-training process. VECO (Luo et al.,\n2021) uses a cross-attention module to build the\ninterdependence between languages explicitly. In\nthis work, we only use non-parallel data and an\nobjective similar to MLM for pre-training CINO.\nNon-English Pre-trained Language Models\nand Benchmarks. Many pre-trained models have\nbeen trained on English corpora, or corpora that\nare heavily biased toward English. To make NLP\ntechniques accessible to people from different cul-\ntures, researchers have developed pre-trained mod-\nels and benchmarks targeting different languages:\nFlauBERT and the FLUE benchmark for French\n(Le et al., 2020), KLUE-BERT and the KLUE\nbenchmark for Korean (Park et al., 2021), In-\ndoBERT and the IndoLEM benchmark for Indone-\nsian (Koto et al., 2020), and there are Chinese-\nBERT-wwm (Cui et al., 2021) and Arabic BERT\nAraBERT (Antoun et al., 2020). However, there are\nno pre-trained language models targeting Chinese\nethnic minority languages.\n2.2\nLanguage Diversity in China\nThere are 56 ethnic groups and more than 80 lan-\nguages in China. Standard Chinese (Mandarin) is\nthe ofﬁcial language, spoken mainly by ethnic Han\nChinese, which accounts for more than 90% of\nthe total population. Ethnic minorities have their\nown languages. According to the study in Moseley\n(2010), the ethnic minority languages Mongolian,\nUyghur, Kazakh, Tibetan,Yi, and Korean are safe\n(ﬁve of them are covered by CINO), which are spo-\nken by about 25 million people, while the rest are\nin unsafe or endangered status.\nBesides the ethnic minority languages, there are\ndialects and varieties of Chinese across the country.\nIn this work, we consider Yue Chinese (also known\nas Cantonese), a widely used group of varieties of\nChinese in Southern China and have been carried\nby immigrants to Southeast Asia and many other\nparts of the world.\nSome languages in Table 1 are spoken and\nwidely used in more than one country, such as Ko-\nrean, Mongolian and Kazakh. In this work, we\nnamed them as minority languages based on their\nstatus in China.\n3\nCINO Model\nIn this section, we present the CINO model struc-\nture and the pre-training methodology. We de-\nnote by N the number of pre-training languages,\nCi the monolingual corpus of the ith language\n(i = 1, . . . , N). Let ni be the number of sentences\nand li be the mean sequence length in Ci. Let ci\nrepresent the total number of tokens of Ci.\n3.1\nModel Structure\nCINO is a multilingual transformer-based model\nwith the same architecture as XLM-R. For the\nCINO-base, it has 12 layers, 768 hidden states, and\n12 attention heads; for the CINO-large, it has 24\nlayers, 1024 hidden states, and 16 attention heads.\nThe main differences between CINO and XLM-R\nare the word embeddings and the tokenizer. We\nstart from the word embeddings and the tokenizer\nof XLM-R and adapt them for the minority lan-\nguages by vocabulary extension and vocabulary\npruning, as depicted in Figure 1.\nVocabulary Extension. The original XLM-R\ntokenizer does not recognize Tibetan scripts and\nTraditional Mongolian scripts, so we extend the\nXLM-R tokenizer and XLM-R word embeddings\nmatrix with additional tokens.\nWe train sentence-piece tokenizers for Tibetan\nand Mongolian on their monolingual pre-training\ncorpora respectively. Each of the tokenizers has\na vocabulary size of 16,000. Then we merge the\nvocabularies from the Tibetan and Mongolian to-\nkenizers into the original XLM-R tokenizer. The\nmerged tokenizer has a vocabulary size of 274,701.\nTo extend the word embeddings, we resize the\noriginal word embeddings matrix of shape V × D\nto V ′ × D by appending new rows, where D is the\nhidden size, V is the original vocabulary size, V ′ is\nthe new vocabulary size. The new rows represent\nthe word vectors of the new tokens from the merged\ntokenizer. They are initialized with a Gaussian\ndistribution of mean 0.0 and variance 0.02.\nVocabulary Pruning. Next, we prune the word\nXLM-R\nTokenizer\n\nTibetan\nTokenizer\nMongolian\nTokenizer\nMerged\nTokenizer\nCINO \nTokenizer\n250k\n16k\n16k\n275k\n135k\nExtension\nPruning\nPre-training\nCorpora\nFigure 1: We extend the XLM-R tokenizer with a Ti-\nbetan tokenizer and a Mongolian tokenizer, then re-\nmove the redundant tokens to obtain the CINO tok-\nenizer.\nembeddings matrix to reduce the model size. We to-\nkenize the pre-training corpora with the merged to-\nkenizer, and remove all the tokens that have not ap-\npeared in the corpora from the merged tokenizer’s\nvocabulary and the word embeddings matrix. The\nabove process discards 139,342 tokens.\nFinally, we obtain the CINO model structure\nwith a vocabulary size of 135,359, a model size of\n728 MB for the base model, 1.7 GB for the large\nmodel, 68% and 79% size of XLM-R-base and\nXLM-R-large, respectively. A smaller vocabulary\nsize leads to not only a memory-friendly model\nbut also a faster model by reducing the cost of\ncomputing the log-softmax in the MLM task. The\ntime cost of each iteration in pre-training is reduced\nby approximately 35% by reducing the vocabulary\nsize from 270k to 140k.\n3.2\nPre-training\nWe adopt the MLM objective for pre-training. In\naddition, we apply the following strategies for bal-\nancing training data and faster pre-training.\n3.2.1\nResampling Strategy\nTo balance the data size between high-resource\nand low-resource languages, Conneau and Lample\n(2019) and Chi et al. (2021) have applied a multi-\nnomial sampling strategy. An example in the ith\nlanguage is sampled with the probability\npi =\nnα\ni\nPN\nk nα\nk\n,\n(1)\nwhere α ∈(0, 1] is a hyperparameter.\nbo\nkk&ug\nko\nmn\nza\nzh&yue\ntotal\n0\n20000\n40000\n60000\n80000\n100000\n120000\n140000\nVocabulary Size\n25616\n45816\n57110\n43451\n11421\n107380\n135359\nFigure 2: The vocabulary size counted from the corpus\nof each language. We merge the vocabularies of the\nlanguages that have similar writing systems.\nHowever, if the mean sequence lengths of differ-\nent corpora are different, it may lead to an unde-\nsired data bias.1 To see this, we use ˜ci to denote the\nnumber of tokens seen during training. We have\n˜ci ∝pili and ˜ci = Kci for all i = 1 . . . N if α = 1.\nK is a constant that only depends on the number of\ntraining steps. If two languages i and j that have\nthe same number of tokens, i.e., ci = cj, but with\nni > nj and li < lj. With the sampling ratio in\n(1), we get ˜ci < ˜cj if α < 1 although the original\ncorpora are of the same size. To remedy this, we\nintroduce the dependence on the mean sequence\nlength li. The sampling probability is\npi =\nnα\ni /lβ\ni\nPN\nk nα\nk/lβ\nk\n,\n(2)\nwhere β ∈[0, 1]. Setting β = 1 −α, the number\nof training tokens in the ith language is\n˜ci ∝pili ∝nα\ni l1−β\ni\n= (nili)α = cα\ni .\n(3)\nTherefore, corpora of equal size will be trained\nwith an equal number of tokens.\n3.2.2\nFast Masking Language Modeling\nTable 1 shows that the languages we consider have\ndistinguished writing systems, which implies that\nthe vocabulary of each language only takes up a\nfraction of the whole vocabulary, as shown in Fig-\nure 2. By taking advantage of this fact, the com-\nputational costs can be reduced if the model only\nmakes MLM predictions over the vocabulary of the\n1In most cases, we could join short sequences to form long\nsequences of a uniform length. But some corpora we use\nconsist of short sentences. Joining them as a long sequence\nleads to semantically incoherence.\nspeciﬁc language of the input examples rather than\nthe whole vocabulary.\nSuppose the example is in the ith language. We\ndenote by V the full vocabulary, and Vi ⊂V the vo-\ncabulary of the ith language, which is obtained by\ntokenizing the ith language’s monolingual corpus.\nLet (c, x) denote the input text sequence, where\nx is the masked token, and c is the context. By\nlimiting the prediction of the masked token to Vi,\nthe MLM loss of the masked token x is\nL(i)\nMLM = −log\nexp(g(c) · E(x))\nP\nx′∈Vi exp (g(c) · E(x′)), (4)\nwhere g(·) is the transformer encoder and E(·) is\nthe look-up operation that returns the embeddings.\nIn order to calculate the loss (4) efﬁciently, dur-\ning training, we group examples by language so\nthat each batch contains examples in a single lan-\nguage.\nWith the objective (4) for pre-training, we have\nobserved 10% time reduction and no signiﬁcant\nperformance drop compared to the original MLM\nobjective, which predicts over the whole vocabu-\nlary. Combined with the speedup by vocabulary\npruning, the pre-training time cost is reduced by\nabout 40% in total.\n4\nText Classiﬁcation Datasets for\nMinority Languages\nMultilingual tasks have been used widely to eval-\nuate the cross-lingual transferability of multilin-\ngual models (Hu et al., 2020). Nevertheless, the\npre-existing multilingual datasets hardly cover the\nChinese ethnic minority languages.\nFor exam-\nple, Tibetan, Mongolian and Uyghur have never\nappeared in any task in the XTREME bench-\nmark. To evaluate the cross-lingual transferabil-\nity of CINO, we construct two text classiﬁcation\ndatasets WCM (Wikipedia-Chinese-Minority) and\nCMNews (Chinese-Minority-News).\n4.1\nWCM Dataset\nData Collection and Annotation. WCM is based\non the data from Wikipedia. It covers seven lan-\nguages: Mongolian, Tibetan, Uyghur, Kazakh, Ko-\nrean, Cantonese, and Standard Chinese. We build\nthe dataset from the Wikipedia page dumps and\nthe Wikipedia category dumps2 of the languages in\nquestion.\n2https://dumps.wikimedia.org/other\nTo annotate the data, we ﬁrst generate a category\ngraph for each language. Each node represents a\ncategory, and each edge stands for the afﬁliation\nbetween a pair of categories. By referring to the\ncategory system of Chinese Wikipedia, we choose\nten categories for the classiﬁcation task: Art, Geog-\nraphy, History, Nature, Science, Personage, Tech-\nnology, Education, Economy, and Health. Then,\nwe start from the categories of each page and back-\ntrack along the routes in the category graph until\nreaching one of the ten target categories, and we\nset this category as the label of that page. Owing\nto some afﬁliation conﬂicts, like one subcategory\nbelonging to two categories simultaneously, we re-\nconstructed the graph by removing certain edges\nbetween the 10 target categories and their subcate-\ngories which are assessed as unreasonable by our\nhuman evaluation team.\nData Cleaning. After getting the labeled data,\nwe apply several strategies to improve the quality of\nthe datasets. We remove dirty data like large blocks\nof URLs and ﬁle paths. Then, the examples are\nﬁltered by their lengths (after being tokenized by\nthe CINO tokenizer) by removing those examples\nshorter than 20 or longer than 1024 tokens.3\nSubsampling.\nSince there are both high-\nresource languages like Korean and low-resource\nlanguages like Uyghur, we down-sample the data in\nthe high-resource languages and the high-resource\ncategories to balance the numbers of examples\namong different languages and different categories.\nWe ﬁx the size of the training set (Chinese articles)\nto 32K and downsample the datasets of the lan-\nguages with abundant articles to about 5% ∼20%\nsize of the training set. Similarly, we also down-\nsampled some categories if they dominate in some\nlanguages. We did not apply the above process to\nUyghur due to its extreme scarcity.\nFinally, we obtain 63,137 examples. WCM con-\ntains the train/dev/test set for Standard Chinese and\nonly test sets for other languages. The detailed\ndistribution is listed in Appendix C.\n4.2\nCMNews Dataset\nData Collection and Annotation. To collect the\nminority language examples, we crawl the news\nfrom the news websites in ethnic minority lan-\nguages and record the category to which each\nnews item belongs. To collect the Chinese news,\n3We discard examples that are too long because long ex-\namples likely cover multiple topics while we assign a single\nlabel to each example.\nDataset\nmn\nbo\nug\nkk\nko\nyue\nzh\nTotal\nWCM\n# Samples\n27\n5\n4\n52\n43\n49\n20\n200\n# Correctly Labeled\n24\n4\n4\n49\n34\n43\n19\n177\nMatching Acc\n88.9%\n80%\n100%\n94.2%\n79.1%\n87.8%\n95.0%\n88.5%\nCMNews\n# Samples\n11\n34\n24\n14\n10\n23\n84\n200\n# Correctly Labeled\n8\n31\n24\n14\n10\n20\n80\n187\nMatching Acc\n72.7%\n91.2%\n100%\n100%\n100%\n87.0%\n95.2%\n93.5%\nTable 2: Results of human evaluation of the sampled examples from WCM and CMNews.\nwe reuse the pre-existing dataset SogouCS News\n(Wang et al., 2008) and CAIL 2018 (Xiao et al.,\n2018). We select the appropriate categories and\ndown-sample the two datasets to make the whole\ndataset more balanced.\nAfter gathering the raw data from all the lan-\nguages, we ﬁrst merge the categories that have\nsimilar meanings (for example, we merge the cat-\negories Finance and Economy). Since the deﬁni-\ntion of news category may vary from website to\nwebsite and language to language, we remove the\ncategories that are not consistent in different lan-\nguages by manually checking a sampled subset.\nWe also remove the categories that do not appear\nin more than two languages. Finally, we obtain\na dataset containing eight categories: Education,\nSports, Health, Tourism, Legal, Economy, Culture,\nand Society.\nData Cleaning.\nThe crawled news is much\ncleaner than the Wikipedia pages, and each docu-\nment naturally belongs to only one category. There-\nfore we only perform length ﬁltering by keeping\nthe documents that contain more than 30 tokens\nafter tokenization.\nThe dataset contains 56,764 examples in total.\nWe split the dataset into a training set and a devel-\nopment set. The detailed distribution is listed in\nAppendix C.\n4.3\nHuman Evaluation\nTo assess the quality of the datasets, we randomly\nsample 200 examples from WCM and 200 exam-\nples from CMNews and manually check whether\nthe contents of the examples match their labels.\nThe results are shown in Table 2. Matching Acc\ndenotes how many examples match their labels\nunder human evaluation. We ﬁnd that 88.5% of\nthe sampled examples from WCM and 93.5% of\nthe sampled examples from CMNews are correctly\nlabeled, which shows CMNews has less noise.\n5\nExperiments\n5.1\nPre-training Setup\nPre-training Data. We randomly sample a subset\ndataset from the public base version of WuDao-\nCorpora (Yuan et al., 2021) as the Standard Chi-\nnese corpus; the corpora of the minority languages\nare in-house data, consisting of short monolingual\nsentences. The total corpora size is 28 GB. The\nstatistics of the pre-training corpora are listed in\nAppendix A.\nExperiment Settings. CINO is trained with the\nfast MLM objective (4) with the masking probabil-\nity is 0.2 and the max sequence length 256. We\ninitialize the parameters of CINO with XLM-R. We\nuse the AdamW optimizer (Loshchilov and Hut-\nter, 2019) with the peak learning rate of 2e-4 for\nthe base model and 1e-4 for the large model. The\nlearning rate is scheduled with 10k and 5k warmup\nsteps followed by a linear decay for the base and\nthe large model respectively. The sampling hyper-\nparameter α is set to 0.7. We train the model with\nthe batch size of 4,096 for 150k steps for the base\nmodel, and the batch size of 8,192 for 75k steps\nfor the large model. The pre-training is performed\non 16 NVIDIA A100 GPUs. The full pre-training\nhyperparameters are summarized in Appendix B.1.\n5.2\nDownstream Evaluation\nHow does CINO perform on the newly introduced\nlanguages? How does CINO perform on the lan-\nguages pre-existing in XLM-R? Does CINO show\nmultilingual and cross-lingual abilities? To an-\nswer these questions, we evaluate CINO on (1)\nTibetan News Classiﬁcation Corpus (Qun et al.,\n2017) (TNCC); (2) Korean news topic classiﬁca-\ntion (Park et al., 2021) (YNAT); (3) WCM and\nCMNews. On TNCC and YNAT,4 we evaluate\nthe in-language model performance, i.e., we train\nand evaluate the model on the same language. On\n4The splitting sizes of TNCC and YNAT are listed in Ap-\npendix C.\nWCM and CMNews, we evaluate the cross-lingual\nability. We describe the details in Section 5.4.\nFor each task and each model, we run the exper-\niment ﬁve times with different seeds and report the\nmean metrics. The ﬁne-tuning hyperparameters of\neach experiment are listed in Appendix B.2.\n5.3\nBaselines\nBesides the common multilingual pre-trained mod-\nels mBERT and XLM-R, we compare CINO mod-\nels with the following baselines on some tasks.\nXLM-R-Ext. We extend and prune the vocabulary\nof XLM-R as described in Section 3.1. This model\nis the un-pretrained CINO. The embeddings of Ti-\nbetan and Mongolian are randomly initialized, and\nthe other parameters are the same as XLM-R.\nKLUE-BERT-base. This is a Korean pre-trained\nmodel proposed in Park et al. (2021). Although\nKLUE-BERT-base is a base-sized model, it outper-\nforms other large models on the YNAT task except\nfor XLM-R-large.\nTextCNN is a simple and light-weight model for\ntext classiﬁcation tasks (Kim, 2014). The word\nembedding dimension is set to 300. After the em-\nbedding layer, we apply three convolution layers\nin parallel with the number of out-channels 100,\nkernel size 3,4, and 5, respectively. Finally, we con-\ncatenate the outputs from the convolution layers\nand apply a two-layer fully-connected network with\nReLU activation to perform the classiﬁcation. We\ntrain the TextCNN from scratch with randomly ini-\ntialized model parameters and word embeddings.\nWord2vec (Tibetan). We ﬁrst train the word em-\nbeddings using word2vec (Mikolov et al., 2013a,b)\non the TNCC training set. The embedding dimen-\nsion is set to 300. To perform the classiﬁcation task,\nwe average the word embeddings of each sample,\nthen feed the results to a trainable linear layer that\noutputs the logits.\n5.4\nResults and Discussions\n5.4.1\nTNCC\nHow does CINO perform on the newly intro-\nduced language? We evaluate CINO on TNCC, a\nTibetan classiﬁcation dataset with 12 classes. The\noriginal work (Qun et al., 2017) proposes a news\ntitle classiﬁcation and a news document classiﬁca-\ntion. Here we conduct the news document classi-\nﬁcation only. The task is to predict the topic of\neach document. Because there are no ofﬁcial splits\navailable, we split the dataset into a training set,\nModel\nTNCC Dev\nTNCC Test\nAcc\nMacro-F1\nAcc\nMacro-F1\nTextCNN\n69.4\n65.7\n62.8\n66.6\nWord2vec (Tibetan)\n70.1\n67.7\n70.2\n68.0\nbase models\nmBERT\n22.9\n4.8\n22.8\n5.5\nmBERT (p.t.)\n63.9\n56.2\n61.8\n56.4\nXLM-R-base\n35.1\n20.2\n31.1\n21.1\nXLM-R-base (p.t.)\n34.2\n21.5\n31.4\n19.9\nXLM-R-Ext-base\n55.7\n43.2\n55.0\n42.1\nCINO-base\n74.8\n71.4\n73.1\n70.0\nlarge models\nXLM-R-large\n35.7\n26.4\n32.8\n27.3\nXLM-R-Ext-large\n31.6\n13.0\n29.2\n12.2\nCINO-large\n76.3\n73.7\n75.4\n72.9\nTable 3: Model performance on the Dev and Test sets\nof Tibetan text classiﬁcation task TNCC. p.t. is short\nfor pre-tokenized.\nModel\nYNAT Dev\nAcc\nMacro-F1\nmBERT (Park et al., 2021)\n-\n82.6†\nXLM-R-base (Park et al., 2021)\n-\n84.5†\nXLM-R-large (Park et al., 2021)\n-\n87.3†\nKLUE-RoBERTa-large (Park et al., 2021)\n-\n85.9†\nKLUE-BERT-base (Park et al., 2021)\n-\n87.0†\nbase models\nmBERT\n82.9\n82.8\nXLM-R-base\n85.1\n85.0\nKLUE-BERT-base\n87.0\n87.1\nCINO-base\n86.1\n85.9\nlarge models\nXLM-R-large\n87.0\n86.8\nCINO-large\n87.3\n87.0\nTable 4: Model performance on the Dev set of Korean\ntext classiﬁcation task YNAT. The results marked with\n† are taken from the KLUE paper (Park et al., 2021).\nThe rest results are from our experiments.\na development set and a test set with a ratio of\n8:1:1. Since the texts in the dataset have been pre-\ntokenized (spaces have been added between words),\nwe remove the spaces between words and tokenize\nthe texts with the pre-trained tokenizer unless other-\nwise speciﬁed. We select the best checkpoint based\non its macro-F1 score. We also report the accuracy\nscore for reference.\nThe results are listed in Table 3. Compared\namong the pre-trained models, XLM-R series have\nlow scores since the vocabulary is not adapted for\nthe Tibetan language and has not been pre-trained\non the Tibetan corpus. While XLM-R-Ext-base has\nan extended vocabulary and signiﬁcantly outper-\nforms XLM-R-base even without being pre-trained\nWCM\nzh →min.\nModel\nbo\nkk\nko\nmn\nug\nyue\nzh\nAvg (Minorities)\nAvg (All)\nbase models\nXLM-R-base\n19.0\n16.7\n43.2\n15.2\n23.3\n58.3\n78.1\n29.3\n36.2\nCINO-base\n36.2\n43.2\n44.9\n39.1\n33.4\n59.7\n78.0\n42.6\n47.6\nlarge models\nXLM-R-large\n18.4\n32.9\n43.8\n22.2\n27.8\n60.0\n77.3\n34.2\n40.3\nCINO-large\n40.6\n44.8\n44.8\n41.6\n28.8\n59.8\n79.2\n43.3\n48.4\nCMNews\nmin. →zh\nModel\nbo\nkk\nko\nmn\nug\nyue\nzh\nAvg (Minorities)\nAvg (All)\nbase models\nXLM-R-base\n38.1\n69.6\n88.3\n35.1\n77.5 (67.7/88.6)\n87.8\n58.6\n66.1\n65.0\nCINO-base\n85.5\n79.2\n89.0\n77.3\n77.4 (77.0/78.0)\n86.9\n68.8\n82.6\n80.6\nlarge models\nXLM-R-large\n30.1\n80.8\n88.9\n30.8\n85.1 (76.4/91.0)\n87.5\n63.6\n67.2\n66.7\nCINO-large\n86.8\n83.0\n90.3\n79.4\n78.8 (68.4/91.3)\n87.9\n71.2\n84.4\n82.5\nTable 5: Model performance on the WCM and CMNews. The metric on each language is macro-F1. Avg (Minori-\nties) is the mean score over languages other than zh; Avg (All) is the mean score over all languages. We bold any\nscore within 0.1 of the best on each language. The results in the parentheses are the min and the max values of ﬁve\nruns.\non the target language. Finally, by pre-training on\nthe minority languages corpora, CINO is adapted\nto the new language and outperforms XLM-R and\nXLM-R-Ext notably.\nmBERT achieves better results when ﬁne-tuned\non the pre-tokenized data (but there are still many\ntokens being mapped to [UNK]). Due to the dif-\nference in the tokenization algorithms used by\nmBERT and XLM-R, XLM-R does not beneﬁt\nfrom using pre-tokenized data.\nTextCNN and Word2vec (Tibetan) surprisingly\nachieve competitive scores and outperforms XLM-\nR-Ext-base. It is possibly due to the difﬁculty in the\noptimization of large models such as XLM-R with\nlimited training data. As we continue increasing\nthe model size, the performance gets worse, as can\nbe seen from comparing the scores of XLM-R-base-\nExt and XLM-R-large-Ext.\n5.4.2\nYNAT\nHow does CINO perform on the minority lan-\nguages pre-existing in XLM-R? We evaluate\nCINO on YNAT, a Korean text classiﬁcation\ndataset with 7 classes. We select the best check-\npoint based on its macro-F1 score. The results are\nlisted in Table 4. CINO-base outperforms XLM-R-\nbase, while CINO-large is better than XLM-R-large\nby our reimplementation but lower than the score\nreported in Park et al. (2021). CINO-large is also\ncomparable to KLUE-BERT-base.\nNotice that Korean is not a low-resource lan-\nguage in XLM-R (the size of the Korean corpus\nis 54 GB in the CC-100), thus XLM-R may have\nlearned Korean well. To signiﬁcantly outperform\nXLM-R and KLUE-BERT-base, we expect that\nlonger training time and more data are required.\n5.4.3\nWCM and CMNews\nDoes\nCINO\nshow\nmultilingual\nand\ncross-\nlingual abilities? We use these two datasets to\nevaluate the cross-lingual and multilingual abilities.\nWe take macro-F1 as the metric on each language,\nand the Avg is the arithmetic mean of the macro-F1\nscores.\nOn the WCM dataset, we train models on the\nChinese training set and test it on all the languages,\nso the results show how well the model transfers the\nknowledge from Chinese to the minority languages;\nthe best checkpoint of each run is selected based\non its score on Chinese; On the CMNews dataset,\nwe train models on the minority languages and the\nChinese data is zero-shot; the best checkpoint is\nselected based on its score on minority languages.\nThe results are listed in Table 5.\nOn WCM, Avg (Minorities) score shows that\nCINO has superior zero-shot performance over\nXLM-R. By inspecting the detailed performance on\neach language, we see that CINO most signiﬁcantly\noutperforms XLM-R on Tibetan, Kazakh, Mongo-\nlian and Uyghur, which have been insufﬁciently\npre-trained in XLM-R.\nOn CMNews, because CINO has been adapted\nto minority languages, it learns more effectively\nthan XLM-R by leveraging the examples in all the\nlanguages. zh score shows that CINO transfers\nbetter than XLM-R. CINO also outperforms XLM-\nR on almost all the minority languages except for\nug, where there is a large gap. To ﬁnd out the\nreason, we list the min and the max ug scores of\nﬁve runs. We see that there is a large variance.\nCINO-large achieves the highest score among all\nruns, but its average score is lower than XLM-R-\nlarge. The unstable performance may be the main\nreason that explains the gap.\n6\nDiscussion on Limitations\nCoverage of ethnic minority languages. Due to\nthe scarcity of minority language corpora, CINO\nonly covers Standard Chinese and some of the most\npopular minority languages and dialects. While\nbeing spoken by millions of people, some lan-\nguages, such as the Yi language, are omitted in\nthis study since we can not ﬁnd sufﬁcient data for\npre-training.\nPre-training objectives. In our early trials of\nmultilingual pre-training, we leveraged both mono-\nlingual and bilingual parallel data, and combined\nthe MLM objective with a cross-lingual alignment\nobjective, similar to the TLM objective used in Chi\net al. (2021) and Conneau and Lample (2019). In-\ntuitively, parallel data contain more information\nthan monolingual data. However, we have not ob-\nserved signiﬁcant improvements over pre-training\nwith only monolingual data and the MLM objec-\ntive. The performance of CINO may be improved\nif parallel data can be effectively used.\nLanguages from different cultures. Among\nthe languages in Table 1, some are cross-border\nlanguages. The cross-border languages are spoken\nin more than one country and are inﬂuenced by\nlocal cultures. How well does the model that has\nbeen trained on the corpus collected in one country\ntransfer to the corpus collected in another country?\nIf the writing systems of the language are different\n(for example, Mongolian is written in Cyrillic in\nMongolia, while it is written in traditional Mon-\ngolian script in China), to what extent do writing\nsystems inﬂuence the model performance? We ex-\npect future work to address these questions.\n7\nConclusion\nIn this paper, we introduce CINO, a multilingual\npre-trained language model for Chinese minority\nlanguages. It takes the same structure as XLM-R\nbut with a different vocabulary and is pre-trained\nwith an adapted MLM objective to reduce compu-\ntational costs. We build multilingual text classiﬁca-\ntion datasets WCM from Wikipedia and CMNews\nfrom ethnic minority news for zero-shot ability\nevaluation on the Chinese minority languages. We\nevaluate CINO on several text classiﬁcation tasks.\nThe results show that CINO achieves notable im-\nprovements over the existing baselines.\nAcknowledgments\nWe would like to thank all anonymous reviewers\nfor their thorough review and for providing con-\nstructive comments to improve our paper.\nReferences\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding.\nIn Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15, Marseille, France. Eu-\nropean Language Resource Association.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Mul-\ntilingual alignment of contextual word representa-\ntions. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHeyan Huang, and Ming Zhou. 2021. InfoXLM: An\ninformation-theoretic framework for cross-lingual\nlanguage model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3576–3588, On-\nline. Association for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining.\nIn Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 7057–7067.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and\nZiqing Yang. 2021. Pre-training with whole word\nmasking for chinese bert.\nIEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing,\n29:3504–3514.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. CoRR, abs/2003.11080.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2485–2494,\nHong Kong, China. Association for Computational\nLinguistics.\nYoon Kim. 2014.\nConvolutional neural networks\nfor sentence classiﬁcation.\nIn Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751,\nDoha, Qatar. Association for Computational Lin-\nguistics.\nFajri Koto, Afshin Rahimi, Jey Han Lau, and Timothy\nBaldwin. 2020. IndoLEM and IndoBERT: A bench-\nmark dataset and pre-trained language model for In-\ndonesian NLP.\nIn Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 757–770, Barcelona, Spain (Online). Interna-\ntional Committee on Computational Linguistics.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoit Crabbé, Laurent Besacier, and Didier\nSchwab. 2020. FlauBERT: Unsupervised language\nmodel pre-training for French.\nIn Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 2479–2490, Marseille, France. Euro-\npean Language Resources Association.\nIlya Loshchilov and Frank Hutter. 2019.\nDecou-\npled weight decay regularization.\nIn 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nFuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin\nBi, Songfang Huang, Fei Huang, and Luo Si.\n2021.\nVECO: Variable and ﬂexible cross-lingual\npre-training for language understanding and gener-\nation.\nIn Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 3980–3994, Online. Association for Computa-\ntional Linguistics.\nTomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013a. Efﬁcient estimation of word represen-\ntations in vector space.\nIn 1st International Con-\nference on Learning Representations, ICLR 2013,\nScottsdale, Arizona, USA, May 2-4, 2013, Workshop\nTrack Proceedings.\nTomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013b. Distributed rep-\nresentations of words and phrases and their com-\npositionality.\nIn Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States, pages 3111–\n3119.\nC. Moseley. 2010. Atlas of the World’s Languages in\nDanger. Memory of peoples Series. UNESCO Pub-\nlishing.\nXuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\nHao Tian, Hua Wu, and Haifeng Wang. 2021.\nERNIE-M: Enhanced multilingual representation by\naligning cross-lingual semantics with monolingual\ncorpora.\nIn Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 27–38, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nLin Pan, Chung-Wei Hang, Haode Qi, Abhishek Shah,\nSaloni Potdar, and Mo Yu. 2021. Multilingual BERT\npost-pretraining alignment.\nIn Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 210–219, On-\nline. Association for Computational Linguistics.\nSungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik\nCho, Jiyoon Han, Jangwon Park, Chisung Song, Jun-\nseong Kim, Yongsook Song, Taehwan Oh, Joohong\nLee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong,\nInkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo\nKim, Myeonghwa Lee, Seongbo Jang, Seungwon\nDo, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee,\nKyumin Park, Jamin Shin, Seonghyun Kim, Lucy\nPark, Alice Oh, Jung-Woo Ha, and Kyunghyun Cho.\n2021. Klue: Korean language understanding evalua-\ntion.\nNuo Qun, Xing Li, Xipeng Qiu, and Xuanjing Huang.\n2017.\nEnd-to-end neural text classiﬁcation for ti-\nbetan.\nIn Chinese Computational Linguistics and\nNatural Language Processing Based on Naturally\nAnnotated Big Data - 16th China National Confer-\nence, CCL 2017, - and - 5th International Sympo-\nsium, NLP-NABD 2017, Nanjing, China, October\n13-15, 2017, Proceedings, volume 10565 of Lec-\nture Notes in Computer Science, pages 472–480.\nSpringer.\nRadim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-\nwork for Topic Modelling with Large Corpora. In\nProceedings of the LREC 2010 Workshop on New\nChallenges for NLP Frameworks, pages 45–50, Val-\nletta, Malta. ELRA.\nhttp://is.muni.cz/\npublication/884893/en.\nCanhui Wang, Min Zhang, Shaoping Ma, and Liyun\nRu. 2008. Automatic online news issue construction\nin web environment. In Proceedings of the 17th In-\nternational Conference on World Wide Web, WWW\n2008, Beijing, China, April 21-25, 2008, pages 457–\n466. ACM.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nChaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao\nTu, Zhiyuan Liu, Maosong Sun, Yansong Feng,\nXianpei Han, Zhen Hu, Heng Wang, and Jian-\nfeng Xu. 2018.\nCAIL2018: A large-scale legal\ndataset for judgment prediction.\nArXiv preprint,\nabs/1807.02478.\nSha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding,\nXiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie\nTang. 2021. Wudaocorpora: A super large-scale chi-\nnese corpora for pre-training language models. AI\nOpen, 2:65–68.\nA\nStatistics of the Pre-training Corpora\nThe corpus size and mean sequence length for pre-\ntraining are listed in Table 6. The sequence lengths\nare obtained by counting the tokens after tokeniza-\ntion. For Standard Chinese (zh), we concatenate or\ntruncate each example to the max sequence length,\nwhile for other languages, we do not concatenate\nthe examples but keep them unchanged.\nLanguage\n# Tokens\nMean Sequence Length\nbo\n130M\n13.4\nkk\n238M\n60.7\nko\n170M\n20.0\nmn\n337M\n25.7\nug\n1B\n23.1\nyue\n276M\n12.6\nza\n23M\n58.1\nzh\n1.2B\n254\nTable 6: Corpus size and mean sequence length of each\nlanguage in the pre-training data.\nB\nHyperparameters\nB.1\nPre-training Hyperparameters\nHyperparameter\nBase Model\nLarge Model\nBatch Size\n4,096\n8,192\nWarmup Steps\n10k\n5k\nTraining Steps\n150k\n75k\nPeak Learning Rate\n2e-4\n1e-4\nMax Length\n256\n256\nMLM probability\n0.2\n0.2\nAdam ϵ\n1e-8\n1e-8\nAdam β1\n0.9\n0.9\nAdam β2\n0.999\n0.999\nGradient Clipping\n1.0\n1.0\nWeight Decay\n0\n0\nSampling α\n0.7\n0.7\nTable 7: Hyperparameters used for pretraining CINO\nmodels.\nTable 7 presents the full set of the hyperparame-\nters used for pre-training CINO models.\nB.2\nFine-tuning Hyperparameters\nThe hyperparameters for ﬁne-tuning on the down-\nstream tasks is listed in Table 9. The batch size is\n32 for all experiments except Word2vec (Tibetan),\nDataset\n# Train\n# Dev\n# Test\n# Classes\nTNCC\n7,359\n191\n923\n12\nYNAT\n45,678\n9,106\n-\n7\nTable 8: Number of examples in TNCC and YNAT.\nof which batch size is 16. The learning rate is\nscheduled with 10% warmup steps followed by a\nlinear decay.\nWe use Gensim ( ˇReh˚uˇrek and Sojka, 2010)\nto train the Word2vec embeddings, and set\nmin_count = 1, vector_size = 300. Other\nparameters take the default values.\nC\nStatistics of the Datasets\nThe sizes of TNCC and YNAT are shown in Table\n8. Detailed data distribution of WCM is listed in\nTable 10. Detailed data distribution of CMNews is\nlisted in Table 11.\nModel\nTNCC\nYNAT\nWCM\nCMNews\nLR\nEpochs\nLR\nEpochs\nLR\nEpochs\nLR\nEpochs\nWord2vec (Tibetan)\n3e-2\n20\n-\n-\n-\n-\n-\n-\nTextCNN\n1e-4\n40\n-\n-\n-\n-\n-\n-\nmBERT\n3e-5\n40\n2e-5\n5\n-\n-\n-\n-\nKLUE-BERT-base\n-\n-\n3e-5\n3\n-\n-\n-\n-\nXLM-R-base\n5e-5\n40\n3e-5\n3\n1e-5\n20\n3e-5\n5\nCINO-base\n5e-5\n40\n3e-5\n3\n1e-5\n20\n3e-5\n5\nXLM-R-large\n3e-5\n40\n2e-5\n3\n1e-5\n20\n3e-5\n5\nCINO-large\n3e-5\n40\n2e-5\n3\n1e-5\n20\n3e-5\n5\nTable 9: Hyperparameters used for downstream ﬁne-tuning.\nCategory\nmn\nbo\nug\nkk\nko\nyue\nzh-train\nzh-test\nzh-dev\nArts\n135\n141\n3\n348\n806\n387\n2657\n335\n331\nGeography\n76\n339\n256\n572\n1197\n1550\n12854\n1644\n1589\nHistory\n66\n111\n0\n491\n776\n499\n1771\n248\n227\nNature\n7\n0\n7\n361\n442\n606\n1105\n110\n134\nNatural Science\n779\n133\n20\n880\n532\n336\n2314\n287\n317\nPersonage\n1402\n111\n0\n169\n684\n1230\n7706\n924\n953\nTechnology\n191\n163\n8\n515\n808\n329\n1184\n152\n134\nEducation\n6\n1\n0\n1392\n439\n289\n936\n118\n130\nEconomy\n205\n0\n0\n637\n575\n445\n922\n109\n113\nHealth\n106\n111\n6\n893\n299\n272\n551\n73\n67\nTotal\n2973\n1110\n300\n6258\n6558\n5943\n32000\n4000\n3995\nTable 10: Number of examples in each category and language in WCM.\nSplit\nCategory\nbo\nkk\nko\nmn\nug\nyue\nzh\nTrain\nEducation\n626\n364\n378\n187\n423\n880\n1979\nSports\n66\n133\n321\n556\n1216\n70\n1978\nHealth\n1309\n153\n40\n31\n240\n1358\n2000\nTourism\n1128\n12\n43\n102\n1078\n0\n1998\nLegal\n433\n283\n283\n294\n19\n22\n2000\nEconomy\n399\n107\n192\n510\n0\n1080\n1877\nCulture\n1834\n231\n228\n118\n0\n0\n1995\nSociety\n898\n149\n147\n543\n1132\n169\n1935\nTotal\n6693\n1432\n1632\n2341\n4108\n3579\n15762\nDev\nEducation\n418\n243\n253\n125\n282\n587\n1000\nSports\n44\n89\n215\n371\n811\n48\n1000\nHealth\n874\n103\n28\n21\n160\n906\n1000\nTourism\n752\n8\n30\n68\n719\n0\n1000\nLegal\n289\n190\n189\n196\n14\n15\n1000\nEconomy\n266\n72\n129\n341\n0\n721\n1000\nCulture\n1223\n155\n152\n80\n0\n0\n1000\nSociety\n600\n100\n99\n362\n756\n113\n1000\nTotal\n4466\n960\n1095\n1564\n2742\n2390\n8000\nTable 11: Number of examples in each category and language in CMNews.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-02-28",
  "updated": "2022-09-21"
}