{
  "id": "http://arxiv.org/abs/2202.13589v3",
  "title": "Unsupervised Point Cloud Representation Learning with Deep Neural Networks: A Survey",
  "authors": [
    "Aoran Xiao",
    "Jiaxing Huang",
    "Dayan Guan",
    "Xiaoqin Zhang",
    "Shijian Lu",
    "Ling Shao"
  ],
  "abstract": "Point cloud data have been widely explored due to its superior accuracy and\nrobustness under various adverse situations. Meanwhile, deep neural networks\n(DNNs) have achieved very impressive success in various applications such as\nsurveillance and autonomous driving. The convergence of point cloud and DNNs\nhas led to many deep point cloud models, largely trained under the supervision\nof large-scale and densely-labelled point cloud data. Unsupervised point cloud\nrepresentation learning, which aims to learn general and useful point cloud\nrepresentations from unlabelled point cloud data, has recently attracted\nincreasing attention due to the constraint in large-scale point cloud\nlabelling. This paper provides a comprehensive review of unsupervised point\ncloud representation learning using DNNs. It first describes the motivation,\ngeneral pipelines as well as terminologies of the recent studies. Relevant\nbackground including widely adopted point cloud datasets and DNN architectures\nis then briefly presented. This is followed by an extensive discussion of\nexisting unsupervised point cloud representation learning methods according to\ntheir technical approaches. We also quantitatively benchmark and discuss the\nreviewed methods over multiple widely adopted point cloud datasets. Finally, we\nshare our humble opinion about several challenges and problems that could be\npursued in future research in unsupervised point cloud representation learning.\nA project associated with this survey has been built at\nhttps://github.com/xiaoaoran/3d_url_survey.",
  "text": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n1\nUnsupervised Point Cloud Representation\nLearning with Deep Neural Networks: A Survey\nAoran Xiao∗, Jiaxing Huang∗, Dayan Guan, Xiaoqin Zhang, Shijian Lu,and Ling Shao Fellow, IEEE\nAbstract—Point cloud data have been widely explored due to its superior accuracy and robustness under various adverse situations.\nMeanwhile, deep neural networks (DNNs) have achieved very impressive success in various applications such as surveillance and\nautonomous driving. The convergence of point cloud and DNNs has led to many deep point cloud models, largely trained under the\nsupervision of large-scale and densely-labelled point cloud data. Unsupervised point cloud representation learning, which aims to learn\ngeneral and useful point cloud representations from unlabelled point cloud data, has recently attracted increasing attention due to the\nconstraint in large-scale point cloud labelling. This paper provides a comprehensive review of unsupervised point cloud representation\nlearning using DNNs. It ﬁrst describes the motivation, general pipelines as well as terminologies of the recent studies. Relevant\nbackground including widely adopted point cloud datasets and DNN architectures is then brieﬂy presented. This is followed by an\nextensive discussion of existing unsupervised point cloud representation learning methods according to their technical approaches. We\nalso quantitatively benchmark and discuss the reviewed methods over multiple widely adopted point cloud datasets. Finally, we share\nour humble opinion about several challenges and problems that could be pursued in the future research in unsupervised point cloud\nrepresentation learning. A project associated with this survey has been built at https://github.com/xiaoaoran/3d url survey.\nIndex Terms—Point cloud, unsupervised representation learning, self-supervised learning, deep learning, transfer learning, 3D vision,\npre-training, deep neural network\n!\n1\nINTRODUCTION\n3D acquisition technologies have experienced fast devel-\nopment in recent years. This can be witnessed by different\n3D sensors that have become increasingly popular in both\nindustrial and our daily lives such as LiDAR sensors in\nautonomous vehicles, RGB-D cameras in Kinect and Apple\ndevices, 3D scanners in various reconstruction tasks, etc.\nMeanwhile, 3D data of different modalities such as meshes,\npoint clouds, depth images and volumetric grids, which\ncapture accurate geometric information for both objects and\nscenes, have been collected and widely applied in different\nareas such as autonomous driving, robotics, medical treat-\nment, remote sensing, etc.\nPoint cloud as one source of ubiquitous and widely used\n3D data can be directly captured with entry-level depth sen-\nsors before triangulating into meshes or converting to vox-\nels. This makes it easily applicable to various 3D scene un-\nderstanding tasks [1] such as 3D object detection and shape\nanalysis, semantic segmentation, etc. With the advance of\ndeep neural networks (DNNs), point cloud understanding\nhas attracted increasing attention as observed by a large\nnumber of deep architectures and deep models developed\nin recent years [2]. On the other hand, effective training of\n•\nAoran Xiao and Jiaxing Huang are co-ﬁrst authors.\n•\nAoran Xiao, Jiaxing Huang and Shijian Lu are with the School of\nComputer Science and Engineering, Nanyang Technological University,\nSingapore.\n•\nDayan Guan is with Mohamed bin Zayed University of Artiﬁcial Intelli-\ngence, United Arab Emirates.\n•\nXiaoqin Zhang is with Key Laboratory of Intelligent Informatics for Safety\n& Emergency of Zhejiang Province, Wenzhou University, China.\n•\nLing Shao is with UCAS-Terminus AI Lab, UCAS.\n•\nCorresponding authors: Shijian Lu (shijian.lu@ntu.edu.sg) and Xiaoqin\nZhang (zhangxiaoqinnan@gmail.com)\nInput space (w/o labels)\nFeature space\nDNN\nUnsupervised \npre-training\nPre-text\ntask\nInput space (w/ labels)\nDNN\nKnowledge\nTransfer\nObject classification\nObject part segmentation\nSemantic segmentation\nObject detection\nInstance segmentation\nDownstream\ntasks\nSupervised\nfine-tuning\n…\nFig. 1: The general pipeline of unsupervised representation\nlearning on point clouds: Deep neural networks are ﬁrst pre-\ntrained with unannotated point clouds via unsupervised\nlearning over certain pre-text tasks. The learned unsuper-\nvised point cloud representations are then transferred to\nvarious downstream tasks to provide network initialization,\nwith which the pre-trained networks are ﬁne-tuned with a\nsmall amount of annotated task-speciﬁc point cloud data.\ndeep networks requires large-scale human-annotated train-\ning data such as 3D bounding boxes for object detection and\npoint-wise annotations for semantic segmentation, which\nare usually laborious and time-consuming to collect due to\n3D view changes and visual inconsistency between human\nperception and point cloud display. Efﬁcient collection of\nlarge-scale annotated point clouds has become one bottle-\nneck for effective design, evaluations, and deployment of\narXiv:2202.13589v3  [cs.CV]  27 Mar 2023\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n2\nLearning through point cloud \ncompletion (Sec. 5.1.4)\nLearning through point cloud \nup-sampling (Sec. 5.1.3)\nLearning through point cloud \nGAN (Sec. 5.1.2)\nLearning through point cloud \nself-reconstruction (Sec. 5.1.1)\nUnsupervised Representation Learning of Point Cloud\nGeneration-based methods \n(Sec. 5.1)\nContext-based methods \n(Sec. 5.2)\nMultiple modal-based methods\n(Sec. 5.3)\nLearning with context similarity \n(Sec. 5.2.1)\nLearning with spatial context \nstructure (Sec. 5.2.2)\nLearning with temporal context \nstructure (Sec. 5.2.3)\nLocal descriptor-based methods \n(Sec. 5.4)\nFig. 2: Taxonomy of existing unsupervised methods in point cloud representation learning.\ndeep networks while handling various real-world tasks [3].\nUnsupervised representation learning (URL), which\naims to learn robust and general feature representations\nfrom unlabelled data, has recently been studied intensively\nfor mitigating the laborious and time-consuming data an-\nnotation challenge. As Fig. 1 shows, URL works in a similar\nway to pre-training which learns useful knowledge from un-\nlabelled data and transfers the learned knowledge to various\ndownstream tasks [4]. More speciﬁcally, URL can provide\nuseful network initialization with which well-performing\nnetwork models can be trained with a small amount of la-\nbelled and task-speciﬁc training data without suffering from\nmuch over-ﬁtting as compared with training from random\ninitialization. URL can thus help reduce training data and\nannotations which has demonstrated great effectiveness in\nthe areas of natural language processing (NLP) [5], [6], 2D\ncomputer vision [7], [8], [9], [10], etc.\nSimilar to URL from other types of data such as texts\nand 2D images, URL of point clouds has recently attracted\nincreasing attention in the computer vision research com-\nmunity. A number of URL techniques have been reported\nwhich are typically achieved by designing different pre-text\ntasks such as 3D object reconstruction [11], partial object\ncompletion [12], 3D jigsaws solving [13], etc. However, URL\nof point clouds still lags far behind as compared with its\ncounterparts in NLP and 2D computer vision tasks. For\nthe time being, training from scratch on various target\nnew data is still the prevalent approach in most existing\n3D scene understanding development. At the other end,\nURL from point cloud data is facing increasing problems\nand challenges, largely due to the lack of large-scale and\nhigh-quality point cloud data, uniﬁed deep backbone ar-\nchitectures, generalizable technical approaches, as well as\ncomprehensive public benchmarks.\nIn addition, URL for point clouds is still short of sys-\ntematic survey that can offer a clear big picture about this\nnew yet challenging task. To ﬁll up this gap, this paper\npresents a comprehensive survey on the recent progress in\nunsupervised point cloud representation learning from the\nperspective of datasets, network architectures, technical ap-\nproaches, performance benchmarking, and future research\ndirections. As shown in Fig. 2, we broadly group existing\nmethods into four categories based on their pretext tasks,\nincluding URL methods using data generation, global and\nlocal contexts, multimodality data and local descriptors,\nmore details to be discussed in the ensuing subsections.\nThe major contributions of this work are threefold:\n1)\nIt presents a comprehensive review of the recent\ndevelopment in unsupervised point cloud represen-\ntation learning. To the best of our knowledge, it is\nthe ﬁrst survey that provides an overview and big\npicture for this exciting research topic.\n2)\nIt studies the most recent progress of unsuper-\nvised point cloud representation learning, including\na comprehensive benchmarking and discussion of\nexisting methods over multiple public datasets.\n3)\nIt shares several research challenges and potential\nresearch directions that could be pursued in unsu-\npervised point cloud representation learning.\nThe rest of this survey is organized as follows: In Section\n2, we introduce background knowledge of unsupervised\npoint cloud learning including term deﬁnition, common\ntasks of point cloud understanding and relevant surveys to\nthis work. Section 3 introduces widely-used datasets and\ntheir characteristics. Section 4 introduces commonly used\ndeep point cloud architectures with typical models that\nare frequently used for point cloud URL. In Section 5 we\nsystematically review the methods for point cloud URL.\nSection 6 summarizes and compares the performances of\nexisting methods on multiple benchmark datasets. At last,\nwe list several promising future directions for unsupervised\npoint cloud representation learning in Section 7.\n2\nBACKGROUND\n2.1\nBasic concepts\nWe ﬁrst deﬁne all relevant terms and concepts that are to be\nused in the ensuing sections.\nPoint cloud data: A point cloud P is a set of vectors P =\n{p1, ..., pN} where each vector represents one point pi =\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n3\n[Ci, Ai]. Here, Ci ∈R1×3 refers to 3D coordinate (xi, yi, zi)\nof the point, and Ai refers to feature attributes of the point\nsuch as RGB values, LiDAR intensity, normal values, etc.,\nwhich are optional and variational depending on 3D sensors\nas well as applications.\nSupervised learning: Under the paradigm of deep learning,\nsupervised learning aims to train deep network models by\nusing labelled training data.\nUnsupervised learning: Unsupervised learning aims to\ntrain networks by using unlabelled training data.\nUnsupervised representation learning: URL is a sub-\nset of unsupervised learning. It aims to learn meaning-\nful representations from data without using any data la-\nbels/annotations, where the learned representations can be\ntransferred to different downstream tasks. Some literature\nalternatively uses the term “self-supervised learning”.\nSemi-supervised learning: In semi-supervised learning,\ndeep networks are trained with a small amount of labelled\ndata and a large amount of unlabelled data. It aims to mit-\nigate data annotation constraints by learning from a small\namount of labelled data and a large amount of unlabelled\ndata that have similar distributions.\nPre-training: Network pre-training learns with certain pre-\ntext tasks over other datasets. The learned parameters are\noften employed for model initialization for further ﬁne-\ntuning with various task-speciﬁc data.\nTransfer learning: Transfer learning aims to transfer knowl-\nedge across tasks, modalities or datasets. A typical scenario\nrelated to this survey is to perform unsupervised learning\nfor pre-training for transferring the learned knowledge from\nunlabelled data to various downstream networks.\n2.2\nCommon 3D understanding tasks\nThis subsection introduces common 3D understanding tasks\nincluding object-level tasks in object classiﬁcation and ob-\nject part segmentation and scene-level tasks in 3D object\ndetection, semantic segmentation and instance segmenta-\ntion. These tasks have been widely adopted to evaluate\nthe quality of point cloud representations that are learned\nvia various unsupervised learning methods, which will be\ndiscussed in detail in Section 6.\n2.2.1\nObject classiﬁcation\nObject classiﬁcation aims to classify point cloud objects\ninto a number of pre-deﬁned categories. Two evaluation\nFig. 3: Illustration of object part segmentation: The ﬁrst row\nshows a few object samples including airplane, motorcycle,\nand table from the ShapeNetPart dataset [14]. The second\nrow shows segmentation ground truth with different parts\nas highlighted by different colors.\nmetrics are most frequently used: The overall Accuracy (OA)\nrepresents the averaged accuracy for all instances in the\ntest set; The mean class accuracy (mAcc) represents the mean\naccuracy of all object classes for the test set.\n2.2.2\nObject part segmentation\nObject part segmentation is an important task for point\ncloud representation learning. It aims to assign a part cat-\negory label (e.g., airplane wing, table leg, etc.) to each point\nas illustrated in Fig. 3. The mean Intersection over Union\n(mIoU) [15] is the most widely adopted evaluation metric.\nFor each instance, IoU is computed for each part belonging\nto that object category. The mean of the part IoUs represents\nthe IoU of that object instance. The overall IoU is computed\nas the average of IoUs over all test instances while category-\nwise IoU (or class IoU) is calculated as the mean over\ninstances under that category.\n2.2.3\n3D object detection\n3D object detection on point clouds is a crucial and indis-\npensable task for many real-world applications, such as\nautonomous driving and domestic robots. The task aims\nto localize objects in the 3D space, i.e. 3D object bounding\nboxes as illustrated in Fig. 4. The average precision (AP)\nmetric has been widely used for evaluations in 3D object\ndetection [16], [17].\n(a) ScanNet-V2 dataset\n(b) KITTI dataset\nFig. 4: Illustration of 3D bounding boxes in point cloud ob-\nject detection: The two graphs show 3D bounding boxes in\ndatasets ScanNet-V2 [18] and KITTI [19] which are cropped\nfrom [16] and [20], respectively.\n2.2.4\n3D semantic segmentation\n3D semantic segmentation on point clouds is another critical\ntask for 3D understanding as illustrated in Fig. 5. Different\nfrom the object part segmentation that segments point cloud\nobjects, 3D semantic segmentation aims to assign a category\n(a) A raw sample\n(b) Semantic annotations\nFig. 5: Illustration of semantic point cloud segmentation: For\nthe point cloud sample from S3DIS [21] on the left, the graph\non the right shows the corresponding ground truth where\ndifferent categories are highlighted by different colors.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n4\nTABLE 1: Summary of commonly used datasets for training and evaluations in prior URL studies with point clouds.\nDataset\nYear\n#Samples\n#Classes\nType\nRepresentation\nLabel\nKITTI [19]\n2013\n15K frames\n8\nOutdoor driving\nRGB & LiDAR\nBounding box\nModelNet10 [27]\n2015\n4,899 objects\n10\nSynthetic object\nMesh\nObject category label\nModelNet40 [27]\n2015\n12,311 objects\n40\nSynthetic object\nMesh\nObject category label\nShapeNet [14]\n2015\n51,190 objects\n55\nSynthetic object\nMesh\nObject/part category label\nSUN RGB-D [28]\n2015\n5K frames\n37\nIndoor scene\nRGB-D\nBounding box\nS3DIS [21]\n2016\n272 scans\n13\nIndoor scene\nRGB-D\nPoint category label\nScanNet [18]\n2017\n1,513 scans\n20\nIndoor scene\nRGB-D & mesh\nPoint category label & Bounding box\nScanObjectNN [29]\n2019\n2,902 objects\n15\nReal-world object\nPoints\nObject category label\nONCE [30]\n2021\n1M scenes\n5\nOutdoor driving\nRGB & LiDAR\nBounding box\nlabel to each point in scene-level point clouds with much\nhigher complexity. The widely adopted evaluation metrics\nincludes OA, mIoU over semantic categories and mAcc.\n2.2.5\n3D instance segmentation\n3D instance segmentation aims to detect and delineate each\ndistinct object of interest in scene-level point clouds as\nillustrated in Fig. 6. On top of semantic segmentation that\nconsiders the semantic category only, instance segmentation\nassigns each object a unique identity. Mean Average Preci-\nsion (mAP) has been widely adopted for the quantitative\nevaluation of this task.\n(a) A raw sample\n(b) Instance annotations\nFig. 6: Illustration of instance segmentation on point clouds:\nFor the point cloud sample from ScanNet-V2 [18] on the\nleft, the graph on the right shows the corresponding ground\ntruth with different instances highlighted by different colors.\n2.3\nRelevant surveys\nTo the best of our knowledge, this paper is the ﬁrst survey\nthat reviews unsupervised point cloud learning compre-\nhensively. Several relevant but different surveys have been\nperformed. For example, several papers reviewed recent\nadvances for deep supervised learning on point clouds:\nIoannidou et al. [22] reviewed deep learning approaches\non 3D data; Xie et al. [23] provided a literature review on\npoint cloud segmentation task; Guo et al. [2] provided a\ncomprehensive and detailed survey on deep learning of\npoint cloud for multiple tasks including classiﬁcation, detec-\ntion, tracking, and segmentation. In addition, several works\nreviewed unsupervised representation learning on other\ndata modalities: Jing et al. [24] introduced advances on un-\nsupervised representation learning in 2D computer vision;\nLiu et al. [25] looked into latest progress about unsupervised\nrepresentation learning methods in 2D computer vision,\nNLP, and graph learning; Qi et al. [26] introduced recent\nprogress on small data learning including unsupervised-\nand semi-supervised methods.\n3\nPOINT CLOUD DATASETS\nIn this section, we summarize the commonly used datasets\nfor training and evaluating unsupervised point cloud rep-\nresentation learning. As listed in Table 1, existing work\nlearns unsupervised point cloud representations mainly\nfrom 1) synthetic object datasets including ModelNet [27]\nand ShapeNet [14], or 2) real scene datasets including Scan-\nNet [18] and KITTI [19]. In addition, various tasks-speciﬁc\ndatasets have been collected which can be used for ﬁne-\ntuning downstream models, such as ScanObjectNN [29],\nModelNet40 [27], and ShapeNet [14] for point cloud classiﬁ-\ncation, ShapeNetPart [14] for part segmentation, S3DIS [21],\nScanNet [18], or Synthia4D [31] for semantic segmentation,\nindoor datasets SUNRGB-D [28] and ScanNet [18] as well as\noutdoor dataset ONCE [30] for object detection.\n•ModelNet10/ModelNet40 [27]: ModelNet is a synthetic\nobject-level dataset for 3D classiﬁcation. The original Mod-\nelNet provides CAD models represented by vertices and\nfaces. Point clouds are generated by sampling from the\nmodels uniformly. ModelNet40 contains 13,834 objects of 40\ncategories, among which 9,843 objects form the training set\nand the rest form the test set. ModelNet10 consists of 3,377\nsamples of 10 categories, which are split into 2,468 training\nsamples and 909 testing samples.\n•ShapeNet [14]: ShapeNet contains synthetic 3D objects of\n55 categories. It was curated by collecting CAD models from\nonline open-sourced 3D repositories. Similar to ModelNet,\nsynthetic objects in ShapeNet are complete, aligned, and\nwith no occlusion or background. Its extension ShapeNet-\nPart has 16,881 objects of 16 categories and is represented\nby point clouds. Each object consists of 2 to 6 parts, and in\ntotal there are 50 part categories in the dataset.\n•ScanObjectNN [29]: ScanObjectNN is a real object-level\ndataset, where 2,902 3D point cloud objects of 15 categories\nare constructed from the scans captured in real indoor\nscenes. Different from synthetic object datasets, point cloud\nobjects in ScanObjectNN are noisy (including background\npoints, occlusions, and holes in objects) and not axis-\naligned.\n•S3DIS [21]: Stanford Large-Scale 3D Indoor Spaces (S3DIS)\ndataset contains over 215 million points scanned from 6\nlarge-scale indoor areas in 3 ofﬁce buildings, where each\narea is 6,000 square meters. The scans are represented as\npoint clouds with point-wise semantic labels of 13 object\ncategories.\n•ScanNet-V2 [18]: ScanNet-V2 is an RGB-D video dataset\ncontaining 2.5 million views in more than 1500 scans, which\nare captured in indoor scenes such as ofﬁces and living\nrooms and annotated with 3D camera poses, surface re-\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n5\nconstructions, as well as semantic and instance labels for\nsegmentation.\n•SUN RGB-D [28]: SUN RGB-D dataset is a collection of\nsingle view RGB-D images collected from indoor environ-\nments. There are in total 10,335 RGB-D images annotated\nwith amodal, and 3D oriented object bounding boxes of 37\ncategories.\n•KITTI [19]: KITTI is a pioneer outdoor dataset providing\ndense point clouds from a LiDAR sensor together with\nother modalities including front-facing stereo images and\nGPS/IMU data. It provides 200k 3D boxes over 22 scenes\nfor 3D object detection.\n•ONCE [30]: ONCE dataset has 1 million LiDAR scenes\nand 7 million corresponding camera images. There are 581\nsequences in total, where 560 sequences are unlabelled\nand used for unsupervised learning, and 10 sequences are\nannotated and used for testing. It provides an unsupervised\nlearning benchmark for object detection in outdoor environ-\nments.\nThe publicly available datasets for URL of point clouds\nare still limited in both data size and scene variety, especially\ncompared with the image and text datasets that have been\nused for 2D computer vision and NLP research. For exam-\nple, there are 800 million words in BooksCorpus and 2,500\nmillion words in English Wikipedia that is able to provide\ncomprehensive data sources for unsupervised representa-\ntion learning in NLP [32]; ImageNet [33] has more than\n10 million images for unsupervised visual representation\nlearning. Large-scale and high-quality point cloud data are\nhighly demanded for future research on this topic, and we\nprovide a detailed discussion of this issue in Section 7.\n4\nCOMMON DEEP ARCHITECTURES\nOver the last decade, deep learning has been playing a\nmore important role in point-cloud processing and under-\nstanding. This can be observed by the abundance of deep\narchitectures that have been developed in recent years.\nDifferent from traditional 3D vision that transforms point\nclouds to structures like Octrees [34] or Hashed Voxel Lists\n[35], deep learning favors more amenable structures for\ndifferentiability and/or efﬁcient neural processing which\nhave achieved very impressive performance over various\n3D tasks.\nAt the other end, DNN-based point cloud processing and\nunderstanding lags far behind as compared with its coun-\nterparts in NLP and 2D computer vision. This is especially\ntrue for the task of unsupervised representation learning,\nlargely due to the lack of regular representations in point\ncloud data. Speciﬁcally, word embeddings and 2D images\nhave regular and well-deﬁned structures, but point clouds\nrepresented by unordered point sets have no such universal\nand structural data format.\nIn this section, we introduce deep architectures that have\nbeen explored for the URL of point clouds. Deep learning for\npoint clouds achieved signiﬁcant progress during the last\ndecade and we see the abundance of 3D deep architectures\nand 3D models being proposed. However, we do not have\nuniversal and ubiquitous “3D backbones” like VGG [36] or\nResNet [37] in 2D computer vision. We thus focus on those\nfrequently used architectures in the URL of point clouds in\nthis survey. For clarity of description, we group them into\nﬁve categories broadly, namely, point-based architectures,\ngraph-based architectures, sparse voxel-based architectures,\nspatial CNN-based architectures, and Transformer-based\narchitectures. Note other deep architectures also exist for\nvarious 3D tasks as discussed in [2], such as projection-\nbased networks [38], [39], [40], [41], [42], [43], recurrent\nneural networks [44], [45], [46], 3D capsule networks [47],\netc. However, they were not often employed for the URL\ntask and thus are not detailed in this survey.\n4.1\nPoint-based deep architectures\nPoint-based networks were designed to process raw point\nclouds directly without point data transformations before-\nhand. Independent point features are usually ﬁrst extracted\nby stacking networks with Multi-Layer Perceptrons (MLPs),\nwhich are then aggregated into global features with sym-\nmetric aggregation functions.\nPointNet [15] is a pioneer point-based network as shown\nin Fig. 7. It stacks several MLP layers to learn point-wise\nfeatures independently and forwards the learned features\nto a max-pooling layer to extract global features for permu-\ntation invariance. To improve PointNet, Qi et al. proposed\nPointNet++ [48] to learn local geometry details from the\nneighborhood of points, where the set abstraction level\nincludes sampling layer, grouping layer, and PointNet layer\nfor learning local and hierarchical features. PointNet++\nachieves great success in multiple 3D tasks including object\nclassiﬁcation and semantic segmentation. By taking Point-\nNet++ as the backbone, Qi et al. designed VoteNet [16],\nthe ﬁrst point-based 3D object detection network. VoteNet\nadopts the Hough voting strategy, which generates new\npoints around object centers and groups them with the\nsurrounding points to produce 3D box proposals.\nmax pooling\ninput points\n𝑛×3\nshared\nMLPs\n𝑛×𝑚\n1×𝑚\nFig. 7: A simpliﬁed architecture of PointNet [15] for point\ncloud object classiﬁcation, where parameters n and m de-\nnote point number and feature dimension, respectively.\n4.2\nGraph-based deep architectures\nGraph-based networks treat point clouds as graphs in Eu-\nclidean space with vertexes being points and edges cap-\nturing neighboring point relations as illustrated in Fig. 8.\nIt works with graph convolution where ﬁlter weights are\nconditioned on edge labels and dynamically generated for\nindividual input samples. This allows to reduce the degrees\nof freedom in the learned models by enforcing weight\nsharing and extracting localized features that can capture\ndependencies among neighboring points.\nThe Dynamic Graph Convolutional Neural Network\n(DGCNN) [49] is a typical graph-based network that has\nbeen frequently used for URL for point clouds. It is stacked\nwith a graph convolution module named EdgeConv that\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n6\n𝑋!\n𝑋\"\n𝑋#\n𝑋$\n𝑋%\n𝑋&\n𝐶\nGCN\n𝑍!\n𝑍\"\n𝑍#\n𝑍$\n𝑍%\n𝑍&\n𝐹\n𝑌#\n𝑌%\n𝑋!\n𝑋\"\n𝑋#\n𝑋$\n𝑋%\n𝑋&\n𝐶\n𝑍!\n𝑍\"\n𝑍#\n𝑍$\n𝑍%\n𝑍&\n𝐹\n𝑌#\n𝑌%\nFig. 8: Schematic depiction of graph convolutional network\n(GCN): Each graph consists of multiple vertexes represent-\ning points Xi or features Zi (highlighted by circular dots),\nas well as edges connecting the vertexes representing point\nrelations (shown as black lines). C denotes input channels,\nF denotes output feature dimensions, and Yi denotes labels.\nperforms convolution on graph dynamically in the feature\nspace. DGCNN integrates EdgeConv into the basic version\nof PointNet structures for learning global shape properties\nand semantic characteristics for point cloud understanding.\n4.3\nSparse voxel-based deep architectures\nThe voxel-based architecture voxelizes point clouds into 3D\ngrids before applying 3D CNN on the volumetric repre-\nsentations. Due to the sparseness of point cloud data, It\noften involves huge computation redundancy or sacriﬁces\nthe representation accuracy while processing a large number\nof points. To overcome this constrain, [50], [51], [52], [53]\nadopt sparse tensor as the basic unit where point clouds\nare represented with a data list and an index list. Unlike\nstandard convolution operation that employs sliding win-\ndows (im2col function in PyTorch and TensorFlow) to build\nthe computational pipeline, sparse convolution [50] collects\nall atomic operations including convolution kernel elements\nand saves them in a Rulebook as computation instructions.\nRecently, Choy et al. proposed Minkowski Engine [51]\nthat introduces generalized sparse convolution and an auto-\ndifferentiation library for sparse tensors. On top of that, Xie\net al. [54] adopted a uniﬁed U-Net [55] architecture and\nbuilt a backbone network (SR-UNet as shown in Fig. 9)\nfor unsupervised pre-training. The learned encoder can be\ntransferred to different downstream tasks such as classiﬁca-\ntion, object detection, and semantic segmentation.\n4.4\nSpatial CNN-based deep architectures\nSpatial CNN-based networks have been developed to ex-\ntend the capabilities of regular-grid CNNs to analyze irreg-\nularly spaced point clouds. They can be divided into con-\ntinuous and discrete convolutional networks according to\nK=3;D=32\nN=2;D=32\nN=3;D=64\nN=4;D=128\nN=6;D=256\nN=2;D=128\nN=2;D=128\nN=2;D=96\nN=2;D=96\nK=1;D=#class\nK=2;S=2\nK=3;S=1\nK=3;S=1\n+\nSparse Conv\nConv ResBlock\nSparse (De)Conv\nDeConv ResBlock\nK: Kernel size\nD: Output dimension\nN: Repeated layer number\nConv/DeConv layers are followed by BN+ReLU\nFig. 9: An illustration of SR-UNet [54] that adopts a uniﬁed\nU-Net [55] architecture for sparse convolution. The graph is\nreproduced based on [54].\n𝜔𝜔1\n𝜔𝜔2\n𝜔𝜔3\n𝜔𝜔4\n𝜔𝜔5\n𝜔𝜔6\n𝜔𝜔7\n𝜔𝜔8\n𝜔𝜔9\n𝑝𝑝\n𝑞𝑞1\n𝑞𝑞2\n𝑞𝑞5\n𝑞𝑞3\n𝑞𝑞4\n𝑝𝑝\n𝑞𝑞1\n𝑞𝑞2\n𝑞𝑞5\n𝑞𝑞3\n𝑞𝑞4\n𝑝𝑝\n𝑞𝑞1\n𝑞𝑞2\n𝑞𝑞5\n𝑞𝑞3\n𝑞𝑞4\n(a) Neighboring points\n(b) Continuous convolution\n(c) Discrete convolution\nFig. 10: An illustration of 3D spatial convolution including\ncontinuous and discrete convolutions. Parameters p and qi\ndenote the center point and its neighboring points, respec-\ntively. The graph is reproduced based on [2].\nthe convolutional kernels [2]. As Fig. 10 shows, continuous\nconvolutional networks deﬁne the convolutional kernels in\na continuous space, where the weights of neighboring points\nare determined by their spatial distribution relative to the\ncenter point. Differently, discrete convolutional networks\noperate on regular grids and deﬁne the convolutional ker-\nnels in a discrete space where neighboring points have ﬁxed\noffsets relative to the center point. One typical example of\ncontinuous convolution models is RS-CNN [56] which has\nbeen widely adopted for URL of point clouds. Speciﬁcally,\nRS-CNN extracts geometric topology relations among local\ncenters with their surrounding points, and it learns dynamic\nweights for convolutions.\n4.5\nTransformer-based deep architectures\nOver the last few years, Transformers have made astound-\ning progress in the research areas of NLP [32], [59] and\n2D image processing [58], [60] due to their structural su-\nperiority and versatility. They have also been introduced\ninto the area of point cloud processing [57], [61] recently.\nFig. 11 shows a standard Transformer architecture for URL\nof point clouds [57], which contains a stack of Transformer\nblocks [59] and each block consists of a multi-head self-\nattention layer and a feed-forward network. The unsuper-\nvised pre-trained Transformer encoder can be used for ﬁne-\ntuning downstream tasks such as object classiﬁcation and\nsemantic segmentation, etc.\nToken Masking\n…\n…\nTransformer Encoder\n…\nMasked Point Modeling Head\nPoint embeddings\nEmbedded\npatches\nNorm\nMulti-head\nattention\nNorm\nMLP\n+\n+\n𝐿𝐿×\n[cls]\nContrastive\nlearning\nInput token\nPred. token\nMasked token\nCLS token\nTransformer encoder\nFig. 11: The architecture of point cloud Transformer that\nwas used for unsupervised pre-training in Point-BERT [57].\nMore network details can be found in [57]. The ﬁgure is\nreproduced based on [57], [58].\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n7\n5\nUNSUPERVISED\nPOINT\nCLOUD\nREPRESENTA-\nTION LEARNING\nIn this section, we review existing URL methods for point\nclouds. As shown in Fig. 2, we broadly group existing\nmethods into four categories according to their pretext tasks,\nincluding generative-based methods, context-based meth-\nods, multiple modal-based methods, and local descriptor-\nbased methods. With this taxonomy, we sort out existing\nmethods and systematically introduce them in the ensuing\nsubsections of this section.\n5.1\nGeneration-based methods\nGeneration-based URL methods for point clouds involve the\nprocess of generating point cloud objects in training. Ac-\ncording to the employed pre-text tasks, they can be further\ngrouped into four subcategories including point cloud self-\nreconstruction (for generating point cloud objects that are\nthe same as the input), point cloud GAN (for generating fake\npoint cloud objects), point cloud up-sampling (for generat-\ning objects with denser point clouds but similar shapes) and\npoint cloud completion (for predicting missing parts from\nincomplete point cloud objects). The ground truth of these\nURL methods are point clouds themselves. Hence, these\nmethods require no human annotations and can learn in\nan unsupervised manner. Table 2 shows a list of generation-\nbased methods.\n5.1.1\nLearning through point cloud self-reconstruction\nNetworks for self-reconstruction usually encode point cloud\nsamples into representation vectors and decode them back\nto the original input data, where shape information and\nsemantic structures are extracted during this process. It\nbelongs to one typical URL approach since it does not\ninvolve any human annotations. One representative net-\nwork is autoencoder [81] which has an encoder network and\na decoder network as illustrated in Fig. 12. The encoder\ncompresses and encodes a point cloud object into a low-\ndimensional embedding vector (i.e., codeword) [66], which is\nthen decoded back to the 3D space by the decoder.\nThe model is optimized by forcing the ﬁnal output to be\nthe same as the input. During this process, the encoding is\nvalidated and learns by attempting to regenerate the input\nfrom the encoding whereas the autoencoder learns low-\ndimension representations by training the network to ig-\nnore insigniﬁcant data (“noise”) [82]. Permutation invariant\nlosses [83] are widely adopted as the training objective to\nEncoder\nDecoder\nCodeword\nInput object\nOutput object\nFig. 12: An illustration of AutoEncoder in unsupervised\npoint cloud representation learning: The Encoder learns to\nrepresent a point cloud object by a Codeword vector while the\nDecoder reconstructs the Output Object from the Codeword.\ndescribe how the input and output point cloud objects are\nsimilar to each other. They can be measured by Chamfer\nDistance LCD or Earth Mover’s Distance LEMD as follows:\nLCD =\nX\np∈P\nmin\np′∈P ′ ||p −p′||2 +\nX\np′∈P ′\nmin\np∈P ||p′ −p||2\n(1)\nLEMD =\nmin\nφ:P →P ′\nX\nx∈P\n||p −φ(p)2||2\n(2)\nWhere P and P ′ denote input and output point clouds of\nthe same size, φ : P →P ′ is bijection, and p & p′ are points.\nSelf-reconstruction has been one of the most widely\nadopted pre-text tasks for URL from point clouds over the\nlast decade. By assuming that point cloud representations\nshould be generative in 3D space and predictable from 2D\nspace, Girdhar et al. proposed TL-Net [63] that employs a\n3D autoencoder to reconstruct 3D volumetric grids and a\n2D convolutional network to learn 2D features from the\nprojected images. Yang et al. designed FoldingNet [66] that\nintroduces a folding-based decoder that deforms a canonical\n2D grid onto the underlying 3D object surface of a point\ncloud object. Li et al. proposed SO-Net [67] that introduces\nself-organizing map to learn hierarchical features of point\nclouds via self-reconstruction. Zhao et al. [47] extended the\ncapsule network [84] into 3D point cloud processing and the\ndesigned 3D capsule network can learn generic representa-\ntions from unstructured 3D data. Gao et al. [75] proposed a\ngraph-based autoencoder that can learn intrinsic patterns of\npoint-cloud structures under both global and local transfor-\nmations. Chen et al. [85] designed a deep autoencoder that\nexploits graph topology inference and ﬁltering for extracting\ncompact representations from 3D point clouds.\nSeveral studies explore global and local geometries to\nlearn robust representations from point cloud objects [71],\n[72]. For example, [71] introduces hierarchical self-attention\nin the encoder for information aggregation, and a recurrent\nneural network (RNN) as the decoder for point cloud recon-\nstruction locally and globally. [72] presents MAP-VAE that\nintroduces a half-to-half prediction task that ﬁrst splits a\npoint cloud object into a front half and a back half and then\ntrains an RNN to predict the back half sequence from the\ncorresponding front half sequence. Several studies instead\nformulate point cloud reconstruction as a point distribution\nlearning task [73], [74], [77]. For example, [73] presents\nPointFlow which generates 3D point clouds by modelling\nthe distribution of shapes and that of points given shapes.\n[74] presents a probabilistic framework that extracts unsu-\npervised shape descriptors via point distribution learning,\nwhich associates each point with a Gaussian and models\npoint clouds as the distribution of points. [77] presents\nan autoregressive model Pointgrow that generates diverse\nand realistic point cloud samples either from scratch or\nconditioned on semantic contexts.\nFurther, several studies learn point cloud representations\nfrom different object resolutions [69], [78], [86]. For example,\nGadelha et al. [69] designed an autoencoder with a multi-\nresolution tree structure that learns point cloud representa-\ntions via coarse-to-ﬁne analysis. Yang et al. [78] proposed\nan autoencoder with a seed generation module that allows\nextraction of input-dependent point-wise features in mul-\ntiple stages with gradually increasing resolution. Chen et\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n8\nTABLE 2: Summary of generation-based methods for unsupervised representation learning of point clouds.\nMethod\nPublished in\nCategory\nContribution\nVConv-DAE [62]\nECCV 2016\nCompletion\nLearning by predicting missing parts in 3D grids\nTL-Net [63]\nECCV 2016\nReconstruction\nLearning by 3D generation and 2D prediction\n3D-GAN [64]\nNeurIPS 2016\nGAN\nPioneer GAN for 3D voxels\n3D-DescriptorNet [65]\nCVPR 2018\nCompletion\nlearning with energy-based models for point cloud completion\nFoldingNet [66]\nCVPR 2018\nReconstruction\nlearning by folding 3D object surfaces\nSO-Net [67]\nCVPR 2018\nReconstruction\nPerforming hierarchical feature extraction on individual points and SOM nodes\nLatent-GAN [68]\nICML 2018\nGAN\nPioneer GAN for raw point clouds and latent embeddings\nMRT [69]\nECCV 2018\nReconstruction\nA new point cloud autoencoder with multi-grid architecture\nVIP-GAN [70]\nAAAI 2019\nGAN\nLearning by solving multi-views inter-prediction tasks for objects\nG-GAN [11]\nICLR 2019\nGAN\nPioneer GAN with graph convolution for point clouds\n3DCapsuleNet [47]\nCVPR 2019\nReconstruction\nLearning with 3D point-capsule network\nL2G-AE [71]\nACM MM 2019\nReconstruction\nLearning by global and local reconstruction of point clouds\nMAP-VAE [72]\nICCV 2019\nReconstruction\nLearning by 3D reconstruction and half-to-half prediction\nPointFlow [73]\nICCV 2019\nReconstruction\nLearning by modeling point clouds as a distribution of distributions\nPDL [74]\nCVPR 2020\nreconstruction\nA probabilistic framework for point distribution learning\nGraphTER [75]\nCVPR 2020\nReconstruction\nProposed a graph-based autoencoder for point clouds\nSA-Net [76]\nCVPR 2020\nCompletion\nLearning by completing point cloud objects with a skip-attention mechanism\nPointGrow [77]\nWACV 2020\nReconstruction\nAn autoregressive model that can recurrently generate point cloud samples.\nPSG-Net [78]\nICCV 2021\nReconstruction\nLearning by reconstruct point cloud objects with seed generation\nOcCo [12]\nICCV 2021\nCompletion\nLearning by completing occluded point cloud objects\nPoint-Bert [57]\nCVPR 2022\nReconstruction\nLearning for Transformers by recovering masked tokens of 3D objects\nPoint-MAE [79]\nECCV 2022\nReconstruction\nAutoencoder transformer recovers masked parts from input data\nPoint-M2AE [80]\nNeurIPS 2022\nReconstruction\nMasked autoencoder with hierarchical point cloud encoding and reconstruction.\nG\nDATA\nD\n𝑧𝑧\n𝑦𝑦\n𝑥𝑥\nReal data distribution\nGenerator\nDiscriminator\nPredefined latent space\nCompete\nFig. 13: An illustration of GAN which typically consists of\na generator G and a discriminator D that ﬁght with each\nother during the training process (in the form of a zero-sum\ngame, where one agent’s gain is another agent’s loss).\nal. [86] proposed to learn sampling-invariant features by\nreconstructing point cloud objects of different resolutions\nand minimizing Chamfer distances between them.\n5.1.2\nLearning through point cloud GAN\nGenerative and Adversarial Network (GAN) [87] is a typical\ndeep generative network. As demonstrated in Fig. 13, it\nconsists of a generator and a discriminator. The generator\naims to synthesize as realistic data samples as possible\nwhile the discriminator tries to differentiate real samples\nand synthesized samples. GAN thus learns to generate new\ndata with the same statistics as the training set and the\nmodeling can be formulated as a min-max problem:\nmin\nG max\nD LGAN = log D(x) + log(1 −D(G(z))),\n(3)\nwhere G is the generator and D represents the discriminator.\nx and z represent a real sample and a randomly sampled\nnoise vector from a distribution p(z), respectively.\nWhen training GANs for URL of point clouds, the gener-\nator learns from either a sampled vector or a latent embed-\nding to generate point cloud instances, while the discrimina-\ntor tries to distinguish whether input point clouds are from\nreal data distribution or generated data distribution. The\ntwo sub-networks ﬁght with each other during the train-\ning process and the discriminator learns to extract useful\nfeature representations for point cloud object recognition.\nThe learning process involves no human annotations thus\nthe networks can be trained in an unsupervised learning\nmanner. After that, the learned discriminator is extended\ninto various downstream tasks such as object classiﬁcation\nor part segmentation by ﬁne-tuning the model.\nSeveral networks employ GAN for URL for point clouds\nsuccessfully [11], [64], [68], [88]. For example, Wu et al. [64]\nproposed the ﬁrst GAN model applying for 3D voxels.\nHowever, the voxelization process either sacriﬁces the repre-\nsentation accuracy or incurs huge redundancies. Achlioptas\net al. proposed Latent-GAN [68] as the ﬁrst GAN model for\nraw point clouds. Li et al. [88] further proposed a point cloud\nGAN model with a hierarchical sampling and inference\nnetwork that learns a stochastic procedure to generate new\npoint cloud objects. Valsesia et al. [11] designed the ﬁrst\ngraph-based GAN model to extract localized features from\npoint clouds. These methods evaluated the generalization\nof the learned representations by ﬁne-tuning them to the\nhigh-level downstream 3D tasks.\n5.1.3\nLearning through point cloud up-sampling\nAs shown in Fig. 14, given a set of points, point cloud up-\nsampling aims to generate a denser set of points with similar\ngeometries. This task requires deep point cloud networks\nto learn underlying geometries of 3D shapes without any\nsupervision, and the learned representations can be used\nfor ﬁne-tuning in 3D downstream tasks.\nLi et al. [89] introduced GAN into the point cloud up-\nsampling task and presented PU-GAN to learn a variety\nof point distributions from the latent space by up-sampling\npoints over patches on object surfaces. The generator aims\nto produce up-sampled point clouds while the discrimi-\nnator tries to distinguish whether its input point cloud\nis produced by the generator or the real one. Similar to\nGANs introduced in Section 5.1.2, the learned discriminator\ncan be transferred in downstream tasks. Remelli et al. [90]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n9\nDNN\nFig. 14: An illustration of point cloud up-sampling: The\nnetwork DNN learns point cloud representations by solving\na pre-text task that reproduces an object with the same\ngeometry but denser point distribution.\ndesigned an autoencoder that can up-sample sparse point\nclouds into dense representations. The learned weight of\nthe encoder can also be used as initialization weights for\ndownstream tasks as described in Section 5.1.1. Though\npoint cloud up-sampling is attracting increasing attention\nin recent years [89], [91], [92], [93], [94], [95], it is largely\nevaluated by the quality of generated point clouds while its\nperformance in transfer learning has not been well studied.\n5.1.4\nLearning through point cloud completion\nPoint cloud completion is a task to predict arbitrary missing\nparts based on the rest of the 3D point clouds. To achieve\nthis target, deep networks need to learn inner geometric\nstructures and semantic knowledge of the 3D objects so\nas to correctly predict missing parts. On top of that, the\nlearned representations can be transferred to downstream\ntasks. The whole process involves no human annotations\nand thus belongs to unsupervised representation learning.\nPoint cloud completion has been an active research area\nover the past decade [76], [112], [113], [114], [115], [116],\n[117] with evaluation in different URL benchmarks [12],\n[62], [65], [76]. A pioneer work VConv-DAE [62] voxelizes\npoint cloud objects into volumetric grids and learns object\nshape distributions with an autoencoder by predicting the\nmissing voxels from the rest parts. Xie et al. [65] designed\n3D-DescriptorNet for probabilistic modeling of volumetric\nshape patterns. Achlioptas et al. [68] introduced the ﬁrst\nDNN for raw point cloud completion which is a point-\nbased network with an encoder-decoder structure. Yuan et\nal. [113] proposed a Point Completion Network, an autoen-\ncoder structured network for learning useful representa-\nCompletion\nOcclusion\nCompleted\npoint cloud\nOccluded\npoint clouds\nOriginal\npoint cloud\nCamera\nview-points\nFig. 15: The pipeline of OcCo [12]. Taking occluded point\ncloud objects as input, an encoder-decoder model is trained\nto complete the occluded point clouds, where the encoder\nlearns point cloud representations and the decoder learns\nto generate complete objects. The learned encoder weights\ncan be used for network initialization for downstream tasks.\nThe ﬁgure is from [12] with author’s permission.\ntions by repairing incomplete point cloud objects. Wen et\nal. [76] proposed SA-Net, which introduces a skip-attention\nmechanism in the encoder that selectively transfers geomet-\nric information from the local regions to the decoder for\ngenerating complete point cloud objects. Wang et al. [12]\nproposed to learn an encoder-decoder model that recovers\nthe occluded points by different camera views as shown in\nFig. 15. The encoder parameters are used as initialization\nfor downstream tasks including classiﬁcation, part segmen-\ntation, and semantic segmentation.\nRecently, recovering missing parts from masked input\nas the pre-text task of URL has been proved remarkably\nsuccessful in NLP [5], [6] and 2D computer vision [10].\nSuch idea has also been investigated in 3D point cloud\nlearning [57], [79], [110], [118]. For example, Yu et al. [57]\nproposed a Point-BERT paradigm that pre-trains point cloud\nTransformers through a masked point modeling task. They\nuse a discrete variational autoencoder to generate tokens\nfor object patches and randomly masked out the tokens to\ntrain the Transformer to recover the original complete point\ntokens. The representations learned by Point-BERT can be\nwell transferred to new tasks and domains such as object\nclassiﬁcation and object part segmentation.\n5.2\nContext-based methods\nContext-based methods are another important category of\nURL of point clouds that has attracted increasing attention\nin recent years. Different from generation-based methods\nthat learn representations in a generative way, these meth-\nods employ discriminative pre-text tasks to learn different\ncontexts of point clouds including context similarity, spatial\ncontext structures, and temporal context structures. The\ndesigned pre-text tasks require no human annotations and\nTable 3 lists the recent methods.\n5.2.1\nLearning with context similarity\nThis type of method learns unsupervised representations\nof point clouds by exploring underlying context similarities\nbetween samples. A typical approach is contrastive learning,\nwhich has demonstrated superior performances in both\n2D vision [7], [8], [119] and 3D vision [3], [54], [104] in\nrecent years. Fig. 16 provides an illustration of instance-wise\ncontrastive learning. Given one input point cloud object\ninstance as the anchor, its augmented views are deﬁned\nas the positive samples while other different instances are\nnegative samples. The network learns representations of\npoint clouds by optimizing a self-discrimination task, i.e.\n…\nData \naugmentation\nAnchor\nPositive\nsample\nNegative\nsamples\nInput space\nDNN\nFeature space\nQuery\nPositive\nkey\nNegative\nkeys\nFig. 16: An illustration of instance contrastive learning that\nlearns locally smooth representations by self-discrimination,\nwhich pulls Query (from the Anchor sample) close to Positive\nKey (from Positive Samples) and pushes it away from Negative\nKeys (from Negative Samples).\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n10\nTABLE 3: Summary of context-based methods for unsupervised representation learning of point clouds.\nMethod\nPublished in\nCategory\nContribution\nMultiTask [96]\nICCV 2019\nHybrid\nLearning by clustering, reconstruction, and self-supervised classiﬁcation\nJigsaw3D [13]\nNeurIPS 2019\nSpatial-context\nLearning by solving 3D jigsaws\nConstrast&Cluster [97]\n3DV 2019\nHybrid\nLearning by contrasting and clustering with GNN\nGLR [98]\nCVPR 2020\nHybrid\nLearning by global-local reasoning for 3D objects\nInfo3D [99]\nECCV 2020\nContext-similarity\nLearning by contrasting global and local parts of objects\nPointContrast [54]\nECCV 2020\nContext-similarity\nLearning by contrasting different views of scene point clouds\nACD [100]\nECCV 2020\nContext-similarity\nLearning by contrasting convex components decomposed from 3D objects\nRotation3D [101]\n3DV 2020\nSpatial-context\nLearning by predicting rotation angles\nHNS [102]\nACM MM 2021\nContext-similarity\nLearning by contrasting local patches of 3D objects with hard negative sampling\nCSC [3]\nCVPR 2021\nContext-similarity\nTechniques to improve contrasting scene point cloud views\nSTRL [1]\nICCV 2021\nTemporal-context\nLearning spatio-temporal data invariance from point cloud sequences\nRandomRooms [103]\nICCV 2021\nContext-similarity\nConstructing pseudo scenes with synthetic objects for contrastive learning\nDepthContrast [104]\nICCV 2021\nContext-similarity\nJoint contrastive learning with points and voxels\nSelfCorrection [105]\nICCV 2021\nHybrid\nLearning by distinguishing and restoring destroyed objects\nPC-FractalDB [106]\nCVPR 2022\nContext-similarity\nLeveraging fractal geometry to generate high-quality pre-training data\n4dcontrast [107]\nECCV 2022\nTemporal-context\nLearning by contrasting dynamic correspondences from 3D scene sequences\nDPCo [108]\nECCV 2022\nContext-similarity\nA uniﬁed contrastive-learning framework for point cloud pre-training\nProposalContrast [109]\nECCV 2022\nContext-similarity\nPre-training 3D detectors by contrasting region proposals\nMaskPoint [110]\nECCV 2022\nContext-similarity\nLearning by discriminating masked object points and sampled noise points\nFAC [111]\nCVPR 2023\nContext-similarity\nLearning by contrasting between grouped foreground and background\nquery (feature of the anchor) should be close to the positive\nkeys (features of positive samples) and faraway from its\nnegative keys (features of negative samples). This learning\nstrategy groups representations of similar samples together\nin an unsupervised manner and helps networks to learn\nsemantic structures from unlabelled data distribution. The\nInfoNCE loss [120] deﬁned below and its variants are often\nemployed as the objective function in training:\nLInfoNCE = −log\nexp (q · k+/τ)\nPK\ni=0 exp(q · ki/τ)\n,\n(4)\nwhere q is encoded query, {k0, k1, k2, ...} are keys with k+\nbeing the positive key, τ is a temperature hyper-parameter\nthat controls how the distribution concentrates.\nSimilar to generation-based methods, different con-\ntrastive learning methods [99], [100], [102], [121], [122] have\nbeen proposed to learn representations on synthetic single ob-\njects. For example, Sanghi et al. [99] proposed to learn useful\nfeature representations by maximizing mutual information\nbetween synthetic objects and their local parts. Wang et\nal. [121] proposed a hybrid contrastive learning strategy that\nuses objects of different resolutions for instance-level con-\ntrast for capturing hierarchical global representations and\nsimultaneously contrasted points and instances for learning\nlocal features. Gadelha et al. [100] decompose 3D objects\ninto convex components and construct positive pairs among\nthe same components and negative pairs among different\ncomponents for contrastive learning. Du et al. [102] intro-\nduced a hard negative sampling strategy into the contrastive\nlearning between instances and local parts. Besides, Rao et\nal. [98] uniﬁed contrastive learning, normal estimation, and\nself-reconstruction into the same framework and formulated\na multi-task learning method.\nRecently, Xie et. al proposed PointContrast [54], a con-\ntrastive learning framework that learns representations of\nscene point clouds as illustrated in Fig. 17. The work shows,\nfor the ﬁrst time, that network weights pre-trained on 3D\nscene partial frames can lead to performance boosts when\nﬁne-tuned on multiple 3D high-level tasks including object\nclassiﬁcation, semantic segmentation, and object detection.\nFirstly, dense correspondences are extracted between two\n!!\n!\"\n\"!\n\"\"\nSparse Res-U-Net\n#!\n#\"\n(−)\n(+)\nShared weights\nFig. 17: The pipeline of PointContrast [54]: Two scans x1 and\nx2 of the same scene captured from two different viewpoints\nare transformed by T1 and T2 for data augmentation. The\ncorrespondence mapping between the two views is com-\nputed to minimize the distance for matched point features\nand maximize the distance for unmatched point features for\ncontrastive learning. The graph is extracted from [54] with\nauthors’ permission.\naligned views of ScanNet [18] to build point pairs and\npoint-level contrastive learning is then conducted with a\nuniﬁed backbone (SR-UNet). Finally, the learned model\nwas transferred to multiple downstream 3D tasks including\nclassiﬁcation, semantic segmentation, and object detection\nwith consistent performance gains.\nSince PointContrast brought new insights that the un-\nsupervised representation learned from scene-level point\nclouds can generalize across domains and boost high-\nlevel scene understanding tasks, several unsupervised pre-\ntraining works are proposed for scene-level 3D tasks. Con-\nsidering that PointContrast focuses on point-level alignment\nwithout capturing spatial conﬁgurations and contexts in\nscenes, Hou et al. [3] integrated spatial contexts into the\npre-training objective by partitioning the space into spatially\ninhomogeneous cells for correspondence matching. Hou et\nal. [123] built a multi-modal contrastive learning framework\nthat models 2D multi-view correspondences as well as\n2D-3D correspondences with geometry-to-image alignment.\nWhile the aforementioned works [3], [54], [123] require 3D\ndata captured from multiple camera views, Zhang et al. [104]\nproposed DepthContrast that can work with single-view\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n11\nDNN\nSupervision\nPre-processing\nFig. 18: The pipeline of 3DJigsaw [13]: An object is split into\nvoxels where each point is assigned with a voxel label. The\nsplit voxels are randomly rearranged via pre-processing,\nand a deep neural network is trained to predict the voxel\nlabel for each point. The graph is reproduced based on [13].\ndata. Instead of using real point clouds as previous methods,\nRao et al. [103] generated synthetic scenes and objects from\nShapeNet [14]for network pre-training.\nAnother unsupervised approach to learn context similar-\nity is clustering. In this approach, samples are ﬁrst grouped\ninto clusters by clustering algorithms such as K-Means [124]\nand each sample is assigned a cluster ID as pseudo-label.\nThen networks are trained in a supervised manner to learn\nsemantic structures of data distribution. The learned pa-\nrameters are used for model initialization for ﬁne-tuning\nvarious downstream tasks. A typical example is Deep-\nClustering [125] which is the ﬁrst unsupervised clustering\nmethod for 2D visual representation learning. However, no\nprior studies adopted a purely clustering strategy for URL\nof point clouds. Instead, hybrid approaches are proposed\nby integrating clustering with other unsupervised learn-\ning approaches (e.g., self-reconstruction [96] or contrastive\nlearning [97]) for learning more robust representations.\n5.2.2\nLearning with spatial context structure\nPoint clouds with spatial coordinates provides accurate\ngeometric description of 3D shapes of objects and scenes.\nThe rich spatial contexts in point clouds can be exploited in\npre-text tasks for URL. For example, networks can be trained\nto sort out the relation of different object parts. Likewise, the\nlearned parameters can be used for model initialization for\ndownstream tasks. Since no human annotations are required\nin training, the key is to design effective pre-text tasks to\nexploit spatial contexts as URL objectives.\nThe method Jigsaw3D [13] proposed by Sauder et al.\nis one of the pioneer works that use spatial context for\nURL of point clouds. As illustrated in Fig. 18, objects are\nﬁrst split into voxels where each point is assigned a voxel\nlabel. The network is then fed with randomly rearranged\npoint clouds and optimized by predicting correct voxel label\nfor each point. During the training, the network aims to\nextract spatial relations and geometric information from\npoint clouds. In their following work [126], another pre-text\ntask was designed to predict one of ten spatial relationships\nof two local parts from the same object. Inspired by the\n2D method that predicts image rotations [127], Poursaeed\net al. [101] proposed to learn representations by predicting\nrotation angles of 3D objects. Thabet et al. [128] designed\n\u001f\u001e\u001d\u001c\u001b\n\u001f\u001e\u001d\u001c\u001b\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\n\u0018\u0019\u001c\u0017\u001a\u001d\u0016\u001c\u0019\n\u001a\u001e\u0019\u0018\u001e\u0017\u0016\u0015\u0017\u001b\u0014\u0013\u0012\u0011\n\u0010\u000f\u0012\u000e\u0017\u001b\u0016\u0015\u0017\u001b\u0014\u0013\u0012\u0011\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\n\u0018\u0019\u001c\u0017\u001a\u001d\u0016\u001c\u0019\n\u0018\u0019\u001a\u001b\u0015\u001d\u0016\u001c\u0019\n\r\u0013\f\f\n\u000b\u0012\u000f\n\u0018\u0017\u001e\u001b\n\u000b\u0012\u000f\n\u0018\u0017\u001e\u001b\u0016\t\u001b\u0013\u001d\n\b\u0013\u0007\u0018\u001e\u000e\u0016\u0006\u0007\u0017\u0012\u000f\u000e\u0017\nFig. 19: The pipeline of STRL [1]: An Online Network learns\nspatial and temporal structures from two neighbouring\npoint cloud frames Xu and Xv. The ﬁgure is adopted from\n[1] with authors’ permission.\na pre-text task that predicts the next point in a point se-\nquence deﬁned by Morton-order Space Filling Curve. Chen\net al. [105] proposed to learn the spatial context of objects by\ndistinguishing the distorted parts of a shape from the correct\nones. Sun et al. [129] introduced a mix-and-disentangle task\nto exploit spatial context cues.\n5.2.3\nLearning with temporal context structure\nPoint cloud sequence is a common type of point cloud data\nthat consists of consecutive point cloud frames. For exam-\nple, there are indoor point cloud sequences transformed\nfrom RGB-D video frames [18] and LiDAR sequential data\n[19], [130], [131] with continuous point cloud scans with\neach scan collected by one sweep of LiDAR sensors. Point\ncloud sequences contain rich temporal information that can\nbe extracted by designing pre-text tasks and used as super-\nvision signals to train DNNs. The learned representations\ncan be transferred to downstream tasks.\nRecently, Huang et al. [1] proposed a Spatio-Temporal\nRepresentation Learning (STRL) framework as illustrated in\nFig. 19. STRL extends BYOL [8] from 2D vision to 3D vision\nand extracts spatial and temporal representation from point\nclouds. It treats two neighboring point cloud frames as pos-\nitive pairs and minimizes the mean squared error between\nthe learned feature representations of sample pairs. Chen\net al. [107] exploit synthetic 3D shapes moving in static 3D\nenvironments to create dynamic scenarios and sample pairs\nin the temporal order. They conduct contrastive learning to\nlearn 3D representations with dynamic understanding.\nUnsupervised learning with temporal context structures\nhas proved its effectiveness in both 2D computer vision\ntasks [132], [133], [134], [135] and 3D computer vision\ntasks [1], [107]. As discussed in Section 7, this direction\nis very promising but more research is needed for better\nharvesting the temporal contextual information.\n5.3\nMultiple modal-based methods\nDifferent modalities such as images [19] and natural lan-\nguage descriptions [136] can provide additional informa-\ntion for point-cloud data. Modeling relationships across\nmodalities can be designed as pre-text tasks for URL which\nhelps networks to learn more robust and comprehensive\nrepresentations. Likewise, the learned parameters can be\nused as initialization weights for various downstream tasks.\nSeveral recent work [4], [137] exploits the correspon-\ndences across 3D point cloud objects and 2D images for\nURL. For example, Jing et al. [4] render 3D objects with\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n12\nCNN\nGCN\nCross-view\ncorrespondence\nCross-modality \ncorrespondence \nFCN\nFig. 20: The pipeline CMCV [4]: CMCV employs a 2D CNN\nto extract 2D features from rendered views of 3D objects\nand a 3D GCN to extract 3D features from point clouds.\nThe two types of features are concatenated by a two-layer\nfully connected network (FCN) to predict cross-modality\ncorrespondences. The graph is reproduced based on [4].\ndifferent camera views into 2D images for learning from\nmulti-modality data. As Fig. 20 shows, they employ a 2D\nCNN and a 3D GCN to extract image features and point\ncloud features, respectively, and then conduct contrastive\nlearning on intra-modal correspondences and cross-modal\ncorrespondences. Their study shows that both pre-trained\n2D CNN and 3D GCN achieved better classiﬁcation as\ncompared with random initialization. Differently, Wang et\nal. [138] project point clouds into colored images and then\nfeed them into an image pre-trained model with frozen\nweights to extract representative features for downstream\ntasks. However, how to learn unsupervised point cloud rep-\nresentations with other modalities such as text descriptions\nand audio data remains an under-explored ﬁeld. We expect\nmore studies in this promising research direction.\n5.4\nLocal descriptor-based methods\nThe aforementioned methods aim to learn semantic struc-\ntures of point clouds for high-level understanding, while\nthe local descriptor-based methods focus on learning repre-\nsentations for low-level tasks. For example, Deng et al. [139]\nintroduced PPF-FoldNet that extracts rotation-invariant 3D\nlocal descriptors for 3D matching [140]. Several works [141],\n[142] exploit non-rigid shape correspondence extraction as\npre-text tasks for URL of point clouds, aiming to ﬁnd\nthe point-to-point correspondence of two deformable 3D\nshapes. Jiang et al. [143] explore unsupervised 3D registra-\ntion for ﬁnding the optimal rigid transformation that can\nalign the source point cloud to the target precisely.\nThe performances of existing local descriptor-based\nmethods are mainly evaluated on low-level tasks. However,\nhow to adapt the learned feature representations toward\nother high-level tasks is rarely discussed. We expect more\nrelated research in the future.\n5.5\nPros and Cons\nGeneration-based methods have been extensively studied\nin 3D URL, thanks to their ability to recover the original data\ndistribution without assuming any downstream tasks. How-\never, most existing research focuses on object-level point\nclouds, characterized by limited point numbers and data\nvariability, restricting their applicability to object classiﬁca-\ntion and part segmentation tasks. Additionally, these meth-\nods demonstrate limited effectiveness in scene-level tasks,\nsuch as 3D object detection and semantic segmentation, due\nto the difﬁculty of generating scene-level point clouds with\ncomplex distribution, rich noises and sparsity variation, and\nvarious occlusions. Nonetheless, generation-based methods\nachieve very impressive progress in 2D images [10] recently,\ndemonstrating their great potential for handling 3D point-\ncloud data. More efforts are expected in scene-level tasks as\nwell as various downstream applications.\nContext-based methods have recently become a prevalent\napproach in scene-level tasks, such as 3D semantic segmen-\ntation, 3D instance segmentation, and 3D object detection,\nthanks to their ability in addressing complex real-world\ndata. However, they are still facing several challenges. The\nﬁrst is hard-example mining which is crucial to effective\ncontrastive learning. Beyond that, designing effective self-\nsupervision is also challenging for context-based methods,\nespecially while considering generalization across various\ntasks and applications.\nMultiple modal-based methods allow leveraging addi-\ntional data modalities for enriching the distribution of point\nclouds. Pair-wise correspondences between point clouds\nand other data modalities also offer additional supervision,\nthereby enhancing the learned unsupervised point cloud\nrepresentations. However, multi-modality methods are still\nfacing several challenges. For example, acquiring large-scale\npair-wise data is often a non-trivial task, and so does the\ndesign of effective cross-domain tasks. In addition, how to\nlearn an effective homogeneous representation space across\nmultiple modalities remains a very open research problem.\nLocal descriptor-based methods offer distinct advantages\nin capturing detailed spatial cues and exploiting low-level\nposition information. However, these methods are limited in\ntheir ability of transferring learned representations to high-\nlevel recognition models, which restricts their application\nscope in more complex and abstract recognition tasks.\n6\nBENCHMARK PERFORMANCES\nWe benchmark representative 3D URL methods with two\nwidely adopted evaluation metrics. The benchmarking is\nperformed over public point-cloud data, where all perfor-\nmances are extracted from the corresponding papers.\n6.1\nEvaluation Criteria\nThere are two metrics that have been widely adopted for\nevaluating the quality of the learned unsupervised point-\ncloud representations.\n• Linear classiﬁcation ﬁrst applies a pre-trained unsuper-\nvised model to extract features from certain labelled data. It\nthen trains a supervised linear classiﬁer with the extracted\nfeatures together with the corresponding labels, where the\nquality of the pre-learned unsupervised representations is\nevaluated by the performance of the trained linear classiﬁer\nover test data. Hence, the linear classiﬁcation can be viewed\nas a type of representation learning metric which provides\ncluster analysis in an implicit way.\n• Fine-tuning optimizes a pre-trained unsupervised model\nusing labelled data from downstream tasks. It can assess the\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n13\nTABLE 4: Comparing linear shape classiﬁcation on ModelNet10 and ModelNet40 [27]: Linear SVM classiﬁers are trained\nwith representations learned by different unsupervised methods. Accuracy highlighted by * was obtained by pre-training\nwith multi-modal data. [T] denotes models with modiﬁed Transformers. [ST] denotes models with standard Transformers.\nMethod\nYear\nPre-text task\nBackbone\nPre-train dataset\nModelNet10\nModelNet40\nSupervised\nlearning\n2017\nN.A.\nPointNet [15]\nN.A.\n-\n89.2\n2017\nPointNet++ [48]\n-\n90.7\n2019\nDGCNN [49]\n-\n93.5\n2019\nRSCNN [56]\n-\n93.6\n2021\n[T]PointTransformer [144]\n-\n93.7\n2022\n[ST]Transformer [57]\n-\n91.4\nSPH [145]\n2003\nGeneration\n-\nShapeNet\n79.8\n68.2\nLFD [146]\n2003\nGeneration\n-\nShapeNet\n79.9\n75.5\nTL-Net [63]\n2016\nGeneration\n-\nShapeNet\n-\n74.4\nVConv-DAE [62]\n2016\nGeneration\n-\nShapeNet\n80.5\n75.5\n3D-GAN [64]\n2016\nGeneration\n-\nShapeNet\n91.0\n83.3\n3D DescriptorNet [65]\n2018\nGeneration\n-\nShapeNet\n-\n92.4\nFoldingNet [66]\n2018\nGeneration\n-\nModelNet40\n91.9\n84.4\nFoldingNet [66]\n2018\nGeneration\n-\nShapeNet\n94.4\n88.4\nLatent-GAN [68]\n2018\nGeneration\n-\nModelNet40\n92.2\n87.3\nLatent-GAN [68]\n2018\nGeneration\n-\nShapeNet\n95.3\n85.7\nMRTNet [69]\n2018\nGeneration\n-\nShapeNet\n86.4\n-\nVIP-GAN [70]\n2019\nGeneration\n-\nShapeNet\n94.1\n92.0\n3DCapsuleNet [47]\n2019\nGeneration\n-\nShapeNet\n-\n88.9\nPC-GAN [88]\n2019\nGeneration\n-\nModelNet40\n-\n87.8\nL2G-AE [71]\n2019\nGeneration\n-\nShapeNet\n95.4\n90.6\nMAP-VAE [72]\n2019\nGeneration\n-\nShapeNet\n94.8\n90.2\nPointFlow [73]\n2019\nGeneration\n-\nShapeNet\n93.7\n86.8\nMultiTask [96]\n2019\nHybrid\n-\nShapeNet\n-\n89.1\nJigsaw3D [13]\n2019\nContext\nPointNet\nShapeNet\n91.6\n87.3\nJigsaw3D [13]\n2019\nContext\nDGCNN\nShapeNet\n94.5\n90.6\nClusterNet [97]\n2019\nContext\nDGCNN\nShapeNet\n93.8\n86.8\nCloudContext [126]\n2019\nContext\nDGCNN\nShapeNet\n94.5\n89.3\nNeuralSampler [90]\n2019\nGeneration\n-\nShapeNet\n95.3\n88.7\nPointGrow [77]\n2020\nGeneration\n-\nShapeNet\n85.8\n-\nInfo3D [99]\n2020\nContext\nPointNet\nShapeNet\n-\n89.8\nInfo3D [99]\n2020\nContext\nDGCNN\nShapeNet\n-\n91.6\nACD [100]\n2020\nContext\nPointNet++\nShapeNet\n-\n89.8\nPDL [74]\n2020\nGeneration\n-\nShapeNet\n-\n84.7\nGLR [98]\n2020\nHybrid\nPointNet++\nShapeNet\n94.8\n92.2\nGLR [98]\n2020\nHybrid\nRSCNN\nShapeNet\n94.6\n92.2\nSA-Net-cls [76]\n2020\nGeneration\n-\nShapeNet\n-\n90.6\nGraphTER [75]\n2020\nGeneration\n-\nModelNet40\n-\n89.1\nRotation3D [101]\n2020\nContext\nPointNet\nShapeNet\n-\n88.6\nRotation3D [101]\n2020\nContext\nDGCNN\nShapeNet\n-\n90.8\nMID [121]\n2020\nContext\nHRNet\nShapeNet\n-\n90.3\nGTIF [85]\n2020\nGeneration\nHRNet\nShapeNet\n95.9\n89.6\nHNS [102]\n2021\nContext\nDGCNN\nShapeNet\n-\n89.6\nParAE [147]\n2021\nGeneration\nPointNet\nShapeNet\n-\n90.3\nParAE [147]\n2021\nGeneration\nDGCNN\nShapeNet\n-\n91.6\nCMCV [4]\n2021\nMulti-modal\nDGCNN\nShapeNet\n-\n89.8*\nGSIR [86]\n2021\nContext\nDGCNN\nModelNet40\n-\n90.4\nSTRL [1]\n2021\nContext\nPointNet\nShapeNet\n-\n88.3\nSTRL [1]\n2021\nContext\nDGCNN\nShapeNet\n-\n90.9\nPSG-Net [78]\n2021\nGeneration\nPointNet++\nShapeNet\n-\n90.9\nSelfCorrection [105]\n2021\nHybrid\nPointNet\nShapeNet\n93.3\n89.9\nSelfCorrection [105]\n2021\nHybrid\nRSCNN\nShapeNet\n95.0\n92.4\nOcCo [12]\n2021\nGeneration\n[ST]Transformer\nShapeNet\n-\n92.1\nCrossPoint [137]\n2022\nMulti-modal\nPointNet\nShapeNet\n-\n89.1*\nCrossPoint [137]\n2022\nMulti-modal\nDGCNN\nShapeNet\n-\n91.2*\nPoint-BERT [57]\n2022\nGeneration\n[ST]Transformer\nShapeNet\n-\n93.2\nPoint-MAE [79]\n2022\nGeneration\n[ST]Transformer\nShapeNet\n-\n93.8\nquality of the pre-learned unsupervised representations by\nevaluating the performance of the ﬁne-tuned model over\ndownstream test data, i.e. how much performance gains\ncould be obtained by unsupervised pre-training compared\nto the random initialization.\nNote URL can be evaluated with other quantitative met-\nrics. For example, reconstruction error [66] can tell how well\nthe learned representations encode the raw point clouds.\nDifferent clustering metrics such as Normalized Mutual In-\nformation [96] could complement the linear-classiﬁcation\nmetric. However, these metrics are mostly task-speciﬁc, e.g.,\nthe reconstruction error may not evaluate the representation\nof scene-level point clouds well due to their inherent noise,\nocclusion, and sparsity. In fact, few generic metrics can\ndirectly and explicitly evaluate the quality of the learned 3D\nunsupervised representations despite its critical importance\nto 3D URL studies. More research along this direction is\nneeded to advance this research ﬁeld further.\nBeyond quantitative metrics, unsupervised feature rep-\nresentations can be evaluated in a qualitative manner. For\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n14\nexample, t-SNE (t-Distributed Stochastic Neighbor Embed-\nding) [148] has been widely adopted to compress the dimen-\nsion of the learned feature representations and visualize the\ncompressed feature embeddings.\n6.2\nObject-level tasks\n6.2.1\nObject classiﬁcation\nObject classiﬁcation is the most widely used task in eval-\nuations since the majority of existing works learn point\ncloud representations on object-level point cloud datasets.\nAs described in Section 6.1, both two types of protocols are\nwidely adopted including the linear classiﬁcation protocol\nand the ﬁne-tuning protocol.\nTable 4 summarizes the performance of the linear classi-\nﬁcation by existing methods. Speciﬁcally, linear classiﬁers\nare trained with the representations learned by different\nunsupervised methods on the ShapeNet or ModelNet40\ndataset, and the classiﬁcation results over the testing set\nover ModelNet10 and ModelNet40 are reported. For com-\nparison, we also list supervised learning performances of\nthe same backbone models over the same datasets. It can\nbe seen that the performances of unsupervised learning\nmethods keep improving and some methods have even\nsurpassed supervised learning methods, demonstrating the\neffectiveness and great potential of URL of point clouds.\nTable 5 lists ﬁne-tuning performance on the ModelNet40\nand ScanObjectNN datasets. We can see that classiﬁcation\nmodels initialized with unsupervised pre-trained weights\nalways achieve better classiﬁcation performances as com-\npared with random initialization, regardless of backbone\narchitectures. On the other hand, the performance gaps are\nstill limited, largely due to the limited size and diversity of\nthe pre-training datasets (i.e., ShapeNet and ModelNet40)\nand the simplicity of existing backbone models. In com-\nparison, thanks to the much larger pre-training datasets\nTABLE 5: Comparisons of unsupervised pre-training per-\nformance over the object classiﬁcation datasets ModelNet40\nand OBJ-BG split in ScanObjecNN. Performance numbers\nare presented in the format of ”A/B”, with ”A” indicating\ntraining classiﬁcation models from scratch with random\ninitialization and ”B” indicating ﬁne-tuning classiﬁcation\nmodels that are initialized with unsupervised pre-trained\nmodels. Performance under “A” may vary due to different\nimplementations as reported in the corresponding papers.\nMethod\nBackbone\nModelNet40\nScanObjectNN\nJigsaw3D [13]\nPointNet [15]\n89.2/89.6(+0.4) 73.5/76.5(+3.0)\nInfo3D [99]\nPointNet [15]\n89.2/90.2(+1.0) -/-\nSelfCorrection [105] PointNet [15]\n89.1/90.0(+0.9) -/-\nOcCo [12]\nPointNet [15]\n89.2/90.1(+0.9) 73.5/80.0(+6.5)\nParAE [147]\nPointNet [15]\n89.2/90.5(+1.3) -/-\nJigsaw3D [13]\nPCN [113]\n89.3/89.6(+0.3) 78.3/78.2(-0.1)\nOcCo [12]\nPCN [113]\n89.3/90.3(+1.0) 78.3/80.4(+2.1)\nGLR [98]\nRSCNN [56]\n91.8/92.2(+0.5) -/-\nSelfCorrection [105] RSCNN [56]\n91.7/93.0(+1.3) -/-\nJigsaw3D [13]\nDGCNN [49]\n92.2/92.4(+0.2) 82.4/82.7(+0.3)\nInfo3D [99]\nDGCNN [49]\n93.5/93.0(-0.5)\n-/-\nOcCo [12]\nDGCNN [49]\n92.5/93.0(+0.5) 82.4/83.9(+1.6)\nParAE [147]\nDGCNN [49]\n92.2/92.9(+0.7) -/-\nSTRL [1]\nDGCNN [49]\n92.2/93.1(+0.9) -/-\nOcCo [12]\nTransformer [57] 91.2/92.2(+1.0) 79.9/84.9(+5.0)\nPoint-BERT [57]\nTransformer [57] 91.2/93.4(+2.2) 79.9/87.4(+7.5)\nImageNet [33] and the more powerful backbone network\nResNet [37], the state-of-the-art methods for unsupervised\npre-training of 2D images are able to achieve more sig-\nniﬁcant performance gains in the classiﬁcation task. As\ndiscussed in Section 7, we expect more diverse datasets and\nmore advanced and generous backbone models that can set\nstronger foundations for this ﬁeld.\n6.2.2\nObject part segmentation\nTable 6 presents the benchmarking of object part segmen-\ntation on the ShapeNetPart dataset [14] using the linear\nclassiﬁcation protocol (i.e., ”Unsup.” in Table 6) and the\nﬁne-tuning protocol (i.e., ”Trans.” in Table 6) as described\nin Section 6.1. As the table shows, the performance gaps be-\ntween unsupervised and supervised learning (i.e., ”Unsup.”\nvs. ”Sup.”) are decreasing. In addition, unsupervised pre-\ntraining achieves better performance in most cases under\nthe ﬁne-tuning protocol (i.e., ”Trans.” vs. ”Sup.”), though\nthe improvement is still limited.\n6.3\nScene-level tasks\nAs discussed in Section 5.2, unsupervised pre-training in\nscene-level tasks has recently become prevalent due to its\nenormous potential in various applications. This comes with\na series of 3D URL studies that investigate the effective-\nness of pre-training over different scene-level point cloud\ndatasets. We provide a comprehensive benchmarking of\nthese methods with respect to different 3D tasks.\nTables 7 and 8 show the performances of semantic seg-\nmentation on the S3DIS [21] dataset. We summarized them\nseparately since different ﬁne-tuning setups have been used\nTABLE 6: Comparison of 3D URL methods for shape part\nsegmentation over ShapeNetPart [14]. ”Unsup.” denotes\nlinear classiﬁcation of the learned unsupervised point fea-\ntures. ”Trans.” is presented in a format of ”A/B”, where ”A”\nis obtained with segmentation models trained from scratch\nwith random initialization, and ”B” is obtained by ﬁne-\ntuning segmentation models that are initialized with un-\nsupervised pre-trained models. We also provide supervised\nperformances (”Sup.”) of different backbone models with\nrandom initialization (extracted from the original papers).\nURL Method\nType\nBackbone\nclass mIoU\ninstance mIoU\nN.A.\nSup.\nPointNet\n80.4\n83.7\nSup.\nPointNet++\n81.9\n85.1\nSup.\nDGCNN\n82.3\n85.1\nSup.\nRSCNN\n84.0\n86.2\nSup.\nTransformer\n83.4\n85.1\nLatent-GAN [68]\nUnsup.\n-\n57.0\n-\nMAP-VAE [72]\nUnsup.\n-\n68.0\n-\nCloudContext [126] Unsup.\nDGCNN\n-\n81.5\nGraphTER [75]\nUnsup.\n-\n78.1\n81.9\nMID [121]\nUnsup.\nHRNet\n83.4\n84.6\nHNS [102]\nUnsup.\nDGCNN\n79.9\n82.3\nCMCV [4]\nUnsup.\nDGCNN\n74.7\n80.8\nSO-Net [67]\nTrans.\nSO-Net\n-/-\n84.6/84.9(+0.3)\nJigsaw3D [13]\nTrans.\nDGCNN\n82.3/83.1(+0.8) 85.1/85.3(+0.2)\nMID [121]\nTrans.\nHRNet\n84.6/85.2(+0.6) 85.5/85.8(+0.3)\nCMCV [4]\nTrans.\nDGCNN\n77.6/79.1(+1.5) 83.0/83.7(+0.7)\nOcCo [12]\nTrans.\nPointNet\n82.2/83.4(+1.2)\n-/-\nOcCo [12]\nTrans.\nDGCNN\n84.4/85.0(+0.6)\n-/-\nOcCo [12]\nTrans. Transformer 83.4/83.4(+0.0) 85.1/85.1(+0.0)\nPoint-BERT [57]\nTrans. Transformer 83.4/84.1(+0.7) 85.1/85.6(+0.5)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n15\nTABLE 7: Semantic segmentation on S3DIS [21]: It compares supervised training with random weight initialization and\nﬁne-tuning with pre-trained weights learned from unsupervised pre-training tasks. It uses DGCNN as the segmentation\nmodel, which is trained on different single Areas and tested on Area 5 (upper part) and Area 6 (lower part).\nMethod\nOA on area 5 with different train area\nmIoU on area 5 with different train area\nArea1\nArea2\nArea3\nArea4\nArea6\nArea1\nArea2\nArea3\nArea4\nArea6\nfrom scratch\n82.9\n81.2\n82.8\n82.8\n83.1\n43.6\n34.6\n39.9\n39.4\n43.9\nJigsaw3D [13]\n83.5(+0.6)\n81.2(+0.0)\n84.0(+1.2)\n82.9(+0.1)\n83.3(+0.2)\n44.7(+1.1)\n34.9(+0.3)\n42.4(+2.5)\n39.9(+0.5)\n43.9(+0.0)\nParAE [147]\n91.8(+8.9)\n82.3(+1.1)\n89.5(+6.7)\n88.2(+5.4)\n86.4(+3.3)\n53.5(+9.9)\n38.5(+3.9)\n48.4(+8.5)\n45.0(+5.6)\n49.2(+5.3)\nMethod\nOA on area 6 with different train area\nmIoU on area 6 with different train area\nArea1\nArea2\nArea3\nArea4\nArea5\nArea1\nArea2\nArea3\nArea4\nArea5\nfrom scratch\n84.6\n70.6\n77.7\n73.6\n76.9\n57.9\n38.9\n49.5\n38.5\n48.6\nSTRL [1]\n85.3(+0.7)\n72.4(+1.8)\n79.1(+1.4)\n73.8(+0.2)\n77.3(+0.4)\n59.2(+1.3)\n39.2(+0.8)\n51.9(+2.4)\n39.3(+0.8)\n49.5(+0.9)\nTABLE 8: Performances for semantic segmentation on\nS3DIS [21]. Upper part: Models are tested on Area5 (Fold#1)\nand trained on the rest of the data. Lower part: Six-fold\ncross-validation over three runs.\nMethod\nBackbone\nmACC\nmIoU\nfrom scratch\nSR-UNet\n75.5\n68.2\nPointConstrast [54]\n77.0\n70.9\nDepthContrast [104]\n-\n70.6\nMethod\nBackbone\nOA\nmIoU\nfrom scratch\nPointNet\n78.2\n47.0\nJigsaw3D [13]\n80.1\n52.6\nOcCo [12]\n82.0\n54.9\nfrom scratch\nPCN\n82.9\n51.1\nJigsaw3D [13]\n83.7\n52.2\nOcCo [12]\n85.1\n53.4\nfrom scratch\nDGCNN\n83.7\n54.9\nJigsaw3D [13]\n84.1\n55.6\nOcCo [12]\n84.6\n58.0\nTABLE 9: Comparison of pre-training effects by differ-\nent unsupervised learning methods. The benchmarking is\n3D object detection task over datasets SUN RGB-D [28]\nand ScanNet-V2 [18]. “@0.25“ and “@0.5“ represent per-\ncategory results of average precision (AP) with IoU thresh-\nold 0.25 (mAP@0.25) and 0.5 (mAP@0.5), respectively.\nMethod\nBackbone\nSUN RGB-D\nScanNet-V2\n@0.5\n@0.25\n@0.5\n@0.25\nfrom scratch\nSR-UNet\n31.7\n55.6\n35.4\n56.7\nPointConstrast [54]\n34.8\n57.5\n38.0\n58.5\nPC-FractalDB [106]\n35.9\n57.1\n37.0\n59.4\nfrom scratch\nVoteNet\n32.9\n57.7\n33.5\n58.6\nSTRL [1]\n-\n58.2\n-\n-\nRandRooms [103]\n35.4\n59.2\n36.2\n61.3\nDepthContrast [104]\n-\n-\n-\n62.2\nCSC [3]\n33.6\n-\n-\n-\nPointContrast [54]\n34.0\n-\n38.0\n-\n4DContrast [107]\n34.4\n-\n39.3\n-\nfrom scratch\nPointNet++\n-\n57.5\n-\n58.6\nPointContrast [54]\n-\n57.9\n-\n58.5\nRandRooms [103]\n-\n59.2\n-\n61.3\nDepthContrast [104]\n-\n60.7\n-\n-\nPC-FractalDB [106]\n33.9\n59.4\n38.3\n61.9\nDPCo [108]\n35.6\n59.8\n41.5\n64.2\nfrom scratch\nH3DNet\n39.0\n60.1\n48.1\n67.3\nRandRooms [103]\n43.1\n61.6\n51.5\n68.6\nin prior works. In Table 7, the unsupervised pre-trained\nDGCNN is ﬁne-tuned on every single area of S3DIS and\ntested on either Area 5 (the upper part of table) or Area\n6 (the lower part of the table). Table 8 instead shows the\nperformance of ﬁne-tuning different segmentation networks\nwith the whole dataset by following the one-fold (in the\nupper part of the table) and six-fold cross-validation setups\nTABLE 10: Object detection performance on dataset ONCE\n[30]. The baseline is trained from scratch. Unsupervised\nlearning methods are used for pre-training models. Usmall,\nUmedian, and Ularge represent small, medium, and large\namounts of unlabelled data that are used for unsupervised\nlearning, respectively.\nMethod\nVehicle\nPedestrian\nCyclist\nmAP\nBaseline [149]\n69.7\n26.1\n59.9\n51.9\nUsmall\nBYOL [8]\n67.6\n17.2\n53.4\n46.1 (-5.8)\nPointContrast [54]\n71.5\n22.7\n58.0\n50.8 (-0.1)\nSwAV [150]\n72.3\n25.1\n60.7\n52.7 (+0.8)\nDeepCluster [125]\n72.1\n27.6\n50.3\n53.3 (+1.4)\nUmedian\nBYOL [8]\n69.7\n27.3\n57.2\n51.4 (-0.5)\nPointContrast [54]\n70.2\n29.2\n58.9\n52.8 (+0.9)\nSwAV [150]\n72.1\n28.0\n60.2\n53.4 (+1.5)\nDeepCluster [125]\n72.1\n30.1\n60.5\n54.2 (+2.3)\nUlarge\nBYOL [8]\n72.2\n23.6\n60.5\n52.1 (+0.2)\nPointContrast [54]\n73.2\n27.5\n58.3\n53.0 (+1.1)\nSwAV [150]\n72.0\n30.6\n60.3\n54.3 (+2.4)\nDeepCluster [125]\n71.9\n30.5\n60.4\n54.3 (+2.4)\nTABLE 11: Performances of instance segmentation on\ndatasets S3DIS [21] and ScanNet-V2 [18]. It reports the mean\nof average precision (mAP) across all semantic classes with\na 3D IoU threshold of 0.25.\nMethod\nBackbone\nS3DIS\nScanNet\nfrom scratch\nSR-UNet\n59.3\n53.4\nPointContrast [54]\n60.5\n55.8\nCSC [3]\n63.4\n56.5\n4DContrast [107]\n-\n57.6\n(in the lower part of the table), respectively.\nWe also summarize existing works that handle unsu-\npervised pre-training for object detection. Tables 9 and 10\nshow their performances over indoor datasets including\nSUN RGB-D [28] and ScanNet-V2 [18] as well as outdoor\nLiDAR dataset ONCE [30], respectively. In addition, several\nworks investigated unsupervised pre-training for instance\nsegmentation. We summarize their performance over S3DIS\n[21] and ScanNet-V2 [18] in Table 11.\nIt is inspiring to see that unsupervised learning repre-\nsentation can generalize across domains and boost perfor-\nmances over multiple high-level 3D tasks as compared with\ntraining from scratch. These experiments demonstrate the\nhuge potential of URL of point clouds in saving expensive\nhuman annotations. However, the improvements are still\nlimited and we expect more research in this area.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n16\n7\nFUTURE DIRECTION\nURL of point clouds has achieved signiﬁcant progress\nduring the last decade. We share several potential future\nresearch directions of this research ﬁeld in this section.\nUniﬁed 3D backbones are needed: One major reason\nof the great success of deep learning in 2D computer vision\nis the standardization of CNN architectures with VGG [36],\nResNet [37], etc. For example, the uniﬁed backbone struc-\ntures greatly facilitate knowledge transfer across different\ndatasets and tasks. For 3D point clouds, similar develop-\nment is far under-explored, despite a variety of 3D deep\narchitectures that have been recently reported. This can\nbe observed from the URL methods in tables in Section\n6 most of which adopted very different backbone models.\nThis impedes the development of 3D point cloud networks\nin scalable design and efﬁcient deployment in various new\ntasks. Designing certain universal backbones that can be as\nubiquitous as ResNet in 2D computer vision is crucial for\nthe advance of 3D point cloud networks including unsuper-\nvised point cloud representation learning.\nLarger datasets are needed: As described in Section 3,\nmost existing URL datasets were originally collected for the\ntask of supervised learning. Since point cloud annotation is\nlaborious and time-consuming, these datasets are severely\nconstrained in data size and data diversity and are not\nsuitable for URL with point clouds which usually requires\nlarge amounts of point clouds of good size and diversity.\nThis issue well explains the trivial improvements by URL\nin tables in Section 6. Hence, it is urgent to collect large-\nscale and high-quality unlabelled point cloud datasets of\nsufﬁcient diversity in terms of object-level and scene-level\npoint clouds, indoor and outdoor point clouds, etc.\nUnsupervised pre-training for scene-level tasks: As\ndescribed in Section 5.2, most earlier research focuses on\nobject-level point cloud processing though several pioneer\nstudies [1], [3], [54], [103], [123] explored how to pre-\ntrain DNNs on scene-level point clouds for improving vari-\nous scene-level downstream tasks such as object detection\nand instance segmentation. Prior studies show that the\nlearned unsupervised representations can effectively gen-\neralize across domains and tasks. Hence, URL of scene-level\npoint clouds deserves more attention as a new direction due\nto its great potential in a variety of applications. On the\nother hand, the research along this line remains at a nascent\nstage, largely due to the constraints in network architectures\nand datasets. We foresee that more related research will be\nconducted in the near future.\nLearning representations from multi-modal data: 3D\nsensors are often equipped with other sensors that can\ncapture additional and complementary information to point\nclouds. For example, depth cameras are often equipped with\noptical sensors for capturing better appearance information.\nLiDAR sensors, optical sensors, GPU, and IMU are often in-\nstalled together as a sensor suite to capture complementary\ninformation and provide certain redundancy in autonomous\nvehicles and mobile robot navigation. Unsupervised learn-\ning from such multi-modal data has attracted increasing\nattention in recent years. For example, learning correspon-\ndences among multi-modal data has been explored as pre-\ntext tasks for unsupervised learning as described in Sec-\ntion 5.3. However, the study along this line of research\nremains under-investigated and we expect more related\nresearch point clouds, RGB images, depth maps, etc.\nLearning Spatio-temporal representations: 3D sensors\nthat support capturing sequential point clouds are becoming\nincreasingly popular nowadays. Rich temporal information\nfrom point cloud streams can be extracted as useful su-\npervision signals for unsupervised learning while most of\nthe existing works still focus on static point clouds. We\nexpect that more effective pretext tasks will be designed that\ncan effectively learn spatio-temporal representations from\nunlabelled sequential point cloud frames.\n8\nCONCLUSION\nUnsupervised representation learning aims to learn effective\nrepresentations from unannotated data, which has demon-\nstrated impressive progress in the research with point cloud\ndata. This paper presents a contemporary survey of un-\nsupervised representation learning of point clouds. It ﬁrst\nintroduces the widely adopted datasets and deep network\narchitectures. A comprehensive taxonomy and detailed re-\nview of methods are then presented. Following that, rep-\nresentative methods are discussed and benchmarked over\nmultiple 3D point cloud tasks. Finally, we share our humble\nopinions about several potential future research directions.\nWe hope that this work can lay a strong and sound foun-\ndation for future research in unsupervised representation\nlearning from point cloud data.\nACKNOWLEDGMENTS\nThis project is funded in part by the Ministry of Education\nSingapore, under the Tier-1 scheme with project number\nRG18/22. It is also supported in part under the RIE2020\nIndustry Alignment Fund – Industry Collaboration Projects\n(IAF-ICP) Funding Initiative, as well as cash and in-kind\ncontributions from Singapore Telecommunications Limited\n(Singtel), through Singtel Cognitive and Artiﬁcial Intelli-\ngence Lab for Enterprises (SCALE@NTU).\nREFERENCES\n[1]\nS. Huang, Y. Xie, S.-C. Zhu, and Y. Zhu, “Spatio-temporal\nself-supervised representation learning for 3d point clouds,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 6535–6545.\n[2]\nY. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun,\n“Deep learning for 3d point clouds: A survey,” IEEE transactions\non pattern analysis and machine intelligence, 2020.\n[3]\nJ. Hou, B. Graham, M. Nießner, and S. Xie, “Exploring data-\nefﬁcient 3d scene understanding with contrastive scene con-\ntexts,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 15 587–15 597.\n[4]\nL. Jing, L. Zhang, and Y. Tian, “Self-supervised feature learning\nby cross-modality and cross-view correspondences,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2021, pp. 1581–1591.\n[5]\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever\net al., “Language models are unsupervised multitask learners,”\nOpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[6]\nJ. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of\ndeep bidirectional transformers for language understanding,” in\nProceedings of NAACL-HLT, 2019, pp. 4171–4186.\n[7]\nK. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum\ncontrast for unsupervised visual representation learning,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 9729–9738.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n17\n[8]\nJ.-B.\nGrill,\nF.\nStrub,\nF.\nAltch´e,\nC.\nTallec,\nP.\nRichemond,\nE. Buchatskaya, C. Doersch, B. Pires, Z. Guo, M. Azar et al.,\n“Bootstrap your own latent: A new approach to self-supervised\nlearning,” in Neural Information Processing Systems, 2020.\n[9]\nX. Chen, H. Fan, R. Girshick, and K. He, “Improved base-\nlines with momentum contrastive learning,” arXiv preprint\narXiv:2003.04297, 2020.\n[10]\nK. He, X. Chen, S. Xie, Y. Li, P. Doll´ar, and R. Girshick, “Masked\nautoencoders are scalable vision learners,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2022, pp. 16 000–16 009.\n[11]\nD. Valsesia, G. Fracastoro, and E. Magli, “Learning localized\ngenerative models for 3d point clouds via graph convolution,”\nin International conference on learning representations, 2018.\n[12]\nH. Wang, Q. Liu, X. Yue, J. Lasenby, and M. J. Kusner, “Unsu-\npervised point cloud pre-training via occlusion completion,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), October 2021, pp. 9782–9792.\n[13]\nJ. Sauder and B. Sievers, “Self-supervised deep learning on point\nclouds by reconstructing space,” Advances in Neural Information\nProcessing Systems, vol. 32, pp. 12 962–12 972, 2019.\n[14]\nA. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang,\nZ. Li, S. Savarese, M. Savva, S. Song, H. Su et al., “Shapenet:\nAn\ninformation-rich\n3d\nmodel\nrepository,”\narXiv\npreprint\narXiv:1512.03012, 2015.\n[15]\nC. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep\nlearning on point sets for 3d classiﬁcation and segmentation,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2017, pp. 652–660.\n[16]\nC. R. Qi, O. Litany, K. He, and L. J. Guibas, “Deep hough voting\nfor 3d object detection in point clouds,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2019, pp.\n9277–9286.\n[17]\nS. Shi, X. Wang, and H. Li, “Pointrcnn: 3d object proposal\ngeneration and detection from point cloud,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition,\n2019, pp. 770–779.\n[18]\nA. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and\nM. Nießner, “Scannet: Richly-annotated 3d reconstructions of\nindoor scenes,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2017, pp. 5828–5839.\n[19]\nA. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets\nrobotics: The kitti dataset,” The International Journal of Robotics\nResearch, vol. 32, no. 11, pp. 1231–1237, 2013.\n[20]\nC. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum\npointnets for 3d object detection from rgb-d data,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition,\n2018, pp. 918–927.\n[21]\nI. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer,\nand S. Savarese, “3d semantic parsing of large-scale indoor\nspaces,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2016, pp. 1534–1543.\n[22]\nA. Ioannidou, E. Chatzilari, S. Nikolopoulos, and I. Kompatsiaris,\n“Deep learning advances in computer vision with 3d data: A\nsurvey,” ACM Computing Surveys (CSUR), vol. 50, no. 2, pp. 1–38,\n2017.\n[23]\nY. Xie, J. Tian, and X. X. Zhu, “Linking points with labels in 3d:\nA review of point cloud semantic segmentation,” IEEE Geoscience\nand Remote Sensing Magazine, vol. 8, no. 4, pp. 38–59, 2020.\n[24]\nL. Jing and Y. Tian, “Self-supervised visual feature learning with\ndeep neural networks: A survey,” IEEE transactions on pattern\nanalysis and machine intelligence, 2020.\n[25]\nX. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and\nJ. Tang, “Self-supervised learning: Generative or contrastive,”\nIEEE Transactions on Knowledge and Data Engineering, 2021.\n[26]\nG.-J. Qi and J. Luo, “Small data challenges in big data era: A\nsurvey of recent progress on unsupervised and semi-supervised\nmethods,” IEEE Transactions on Pattern Analysis and Machine Intel-\nligence, 2020.\n[27]\nZ. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao,\n“3d shapenets: A deep representation for volumetric shapes,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2015, pp. 1912–1920.\n[28]\nS. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d\nscene understanding benchmark suite,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2015, pp. 567–\n576.\n[29]\nM. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung,\n“Revisiting point cloud classiﬁcation: A new benchmark dataset\nand classiﬁcation model on real-world data,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2019, pp.\n1588–1597.\n[30]\nJ. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li,\nC. Ye, W. Zhang, Z. Li et al., “One million scenes for autonomous\ndriving: Once dataset,” arXiv preprint arXiv:2106.11037, 2021.\n[31]\nG. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez,\n“The synthia dataset: A large collection of synthetic images for\nsemantic segmentation of urban scenes,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2016,\npp. 3234–3243.\n[32]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” arXiv preprint arXiv:1810.04805, 2018.\n[33]\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,\n“Imagenet: A large-scale hierarchical image database,” in 2009\nIEEE conference on computer vision and pattern recognition.\nIeee,\n2009, pp. 248–255.\n[34]\nA. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, and\nW. Burgard, “Octomap: An efﬁcient probabilistic 3d mapping\nframework based on octrees,” Autonomous robots, vol. 34, no. 3,\npp. 189–206, 2013.\n[35]\nM. Nießner, M. Zollh¨ofer, S. Izadi, and M. Stamminger, “Real-\ntime 3d reconstruction at scale using voxel hashing,” ACM Trans-\nactions on Graphics (ToG), vol. 32, no. 6, pp. 1–11, 2013.\n[36]\nK. Simonyan and A. Zisserman, “Very deep convolutional net-\nworks for large-scale image recognition,” in International Confer-\nence on Learning Representations, 2015.\n[37]\nK. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2016, pp. 770–778.\n[38]\nH. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-\nview convolutional neural networks for 3d shape recognition,” in\nProceedings of the IEEE international conference on computer vision,\n2015, pp. 945–953.\n[39]\nT. Yu, J. Meng, and J. Yuan, “Multi-view harmonized bilinear\nnetwork for 3d object recognition,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n186–194.\n[40]\nB. Yang, W. Luo, and R. Urtasun, “Pixor: Real-time 3d object\ndetection from point clouds,” in Proceedings of the IEEE conference\non Computer Vision and Pattern Recognition, 2018, pp. 7652–7660.\n[41]\nZ. Yang and L. Wang, “Learning relationships for multi-view 3d\nobject recognition,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2019, pp. 7505–7514.\n[42]\nX. Wei, R. Yu, and J. Sun, “View-gcn: View-based graph con-\nvolutional network for 3d shape analysis,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 1850–1859.\n[43]\nA. Xiao, X. Yang, S. Lu, D. Guan, and J. Huang, “Fps-net: A\nconvolutional fusion network for large-scale lidar point cloud\nsegmentation,” ISPRS Journal of Photogrammetry and Remote Sens-\ning, vol. 176, pp. 237–249, 2021.\n[44]\nQ. Huang, W. Wang, and U. Neumann, “Recurrent slice networks\nfor 3d segmentation of point clouds,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n2626–2635.\n[45]\nX. Ye, J. Li, H. Huang, L. Du, and X. Zhang, “3d recurrent neural\nnetworks with context fusion for point cloud semantic segmen-\ntation,” in Proceedings of the European conference on computer vision\n(ECCV), 2018, pp. 403–417.\n[46]\nC. Zou, E. Yumer, J. Yang, D. Ceylan, and D. Hoiem, “3d-prnn:\nGenerating shape primitives with recurrent neural networks,” in\nProceedings of the IEEE International Conference on Computer Vision,\n2017, pp. 900–909.\n[47]\nY. Zhao, T. Birdal, H. Deng, and F. Tombari, “3d point capsule\nnetworks,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2019, pp. 1009–1018.\n[48]\nC. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep\nhierarchical feature learning on point sets in a metric space,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[49]\nY. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.\nSolomon, “Dynamic graph cnn for learning on point clouds,”\nAcm Transactions On Graphics (tog), vol. 38, no. 5, pp. 1–12, 2019.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n18\n[50]\nB. Graham, M. Engelcke, and L. Van Der Maaten, “3d semantic\nsegmentation with submanifold sparse convolutional networks,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 9224–9232.\n[51]\nC. Choy, J. Gwak, and S. Savarese, “4d spatio-temporal convnets:\nMinkowski convolutional neural networks,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2019, pp. 3075–3084.\n[52]\nH. Tang, Z. Liu, X. Li, Y. Lin, and S. Han, “TorchSparse: Efﬁcient\nPoint Cloud Inference Engine,” in Conference on Machine Learning\nand Systems (MLSys), 2022.\n[53]\nH. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, and S. Han,\n“Searching efﬁcient 3d architectures with sparse point-voxel con-\nvolution,” in European conference on computer vision.\nSpringer,\n2020, pp. 685–702.\n[54]\nS. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, and O. Litany, “Point-\ncontrast: Unsupervised pre-training for 3d point cloud under-\nstanding,” in European Conference on Computer Vision.\nSpringer,\n2020, pp. 574–591.\n[55]\nO. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional\nnetworks for biomedical image segmentation,” in International\nConference on Medical image computing and computer-assisted inter-\nvention.\nSpringer, 2015, pp. 234–241.\n[56]\nY. Liu, B. Fan, S. Xiang, and C. Pan, “Relation-shape convolu-\ntional neural network for point cloud analysis,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 2019, pp. 8895–8904.\n[57]\nX. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, “Point-\nbert: Pre-training 3d point cloud transformers with masked point\nmodeling,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 19 313–19 322.\n[58]\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” ICLR, 2021.\n[59]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[60]\nZ. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n“Swin transformer: Hierarchical vision transformer using shifted\nwindows,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2021, pp. 10 012–10 022.\n[61]\nH. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point trans-\nformer,” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), October 2021, pp. 16 259–16 268.\n[62]\nA. Sharma, O. Grau, and M. Fritz, “Vconv-dae: Deep volumetric\nshape learning without object labels,” in European Conference on\nComputer Vision.\nSpringer, 2016, pp. 236–250.\n[63]\nR. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta, “Learning\na predictable and generative vector representation for objects,”\nin European Conference on Computer Vision.\nSpringer, 2016, pp.\n484–499.\n[64]\nJ. Wu, C. Zhang, T. Xue, W. T. Freeman, and J. B. Tenenbaum,\n“Learning a probabilistic latent space of object shapes via 3d\ngenerative-adversarial modeling,” in Proceedings of the 30th Inter-\nnational Conference on Neural Information Processing Systems, 2016,\npp. 82–90.\n[65]\nJ. Xie, Z. Zheng, R. Gao, W. Wang, S.-C. Zhu, and Y. N. Wu,\n“Learning descriptor networks for 3d shape synthesis and anal-\nysis,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 8629–8638.\n[66]\nY. Yang, C. Feng, Y. Shen, and D. Tian, “Foldingnet: Point cloud\nauto-encoder via deep grid deformation,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2018,\npp. 206–215.\n[67]\nJ. Li, B. M. Chen, and G. H. Lee, “So-net: Self-organizing network\nfor point cloud analysis,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2018, pp. 9397–9406.\n[68]\nP. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, “Learn-\ning representations and generative models for 3d point clouds,”\nin International conference on machine learning.\nPMLR, 2018, pp.\n40–49.\n[69]\nM. Gadelha, R. Wang, and S. Maji, “Multiresolution tree networks\nfor 3d point cloud processing,” in Proceedings of the European\nConference on Computer Vision (ECCV), 2018, pp. 103–118.\n[70]\nZ. Han, M. Shang, Y.-S. Liu, and M. Zwicker, “View inter-\nprediction gan: Unsupervised representation learning for 3d\nshapes by learning global shape memories to support local view\npredictions,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 33, no. 01, 2019, pp. 8376–8384.\n[71]\nX. Liu, Z. Han, X. Wen, Y.-S. Liu, and M. Zwicker, “L2g auto-\nencoder: Understanding point clouds by local-to-global recon-\nstruction with hierarchical self-attention,” in Proceedings of the\n27th ACM International Conference on Multimedia, 2019, pp. 989–\n997.\n[72]\nZ. Han, X. Wang, Y.-S. Liu, and M. Zwicker, “Multi-angle point\ncloud-vae: Unsupervised feature learning for 3d point clouds\nfrom multiple angles by joint self-reconstruction and half-to-\nhalf prediction,” in 2019 IEEE/CVF International Conference on\nComputer Vision (ICCV).\nIEEE, 2019, pp. 10 441–10 450.\n[73]\nG. Yang, X. Huang, Z. Hao, M.-Y. Liu, S. Belongie, and B. Har-\niharan, “Pointﬂow: 3d point cloud generation with continuous\nnormalizing ﬂows,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2019, pp. 4541–4550.\n[74]\nY. Shi, M. Xu, S. Yuan, and Y. Fang, “Unsupervised deep shape\ndescriptor with point distribution learning,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 9353–9362.\n[75]\nX. Gao, W. Hu, and G.-J. Qi, “Graphter: Unsupervised learn-\ning of graph transformation equivariant representations via\nauto-encoding node-wise transformations,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 7163–7172.\n[76]\nX. Wen, T. Li, Z. Han, and Y.-S. Liu, “Point cloud completion by\nskip-attention network with hierarchical folding,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, 2020, pp. 1939–1948.\n[77]\nY. Sun, Y. Wang, Z. Liu, J. Siegel, and S. Sarma, “Point-\ngrow: Autoregressively learned point cloud generation with self-\nattention,” in Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, 2020, pp. 61–70.\n[78]\nJ. Yang, P. Ahn, D. Kim, H. Lee, and J. Kim, “Progressive seed\ngeneration auto-encoder for unsupervised point cloud learning,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 6413–6422.\n[79]\nY. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, and L. Yuan,\n“Masked autoencoders for point cloud self-supervised learning,”\nin Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23–27, 2022, Proceedings, Part II.\nSpringer, 2022,\npp. 604–621.\n[80]\nR. Zhang, Z. Guo, P. Gao, R. Fang, B. Zhao, D. Wang, Y. Qiao, and\nH. Li, “Point-m2ae: multi-scale masked autoencoders for hierar-\nchical point cloud pre-training,” Advances in neural information\nprocessing systems, 2022.\n[81]\nM. A. Kramer, “Nonlinear principal component analysis using\nautoassociative neural networks,” AIChE journal, vol. 37, no. 2,\npp. 233–243, 1991.\n[82]\nAutoencoder,\n“Autoencoder\n—\nWikipedia,\nthe\nfree\nencyclopedia,” 2022, [Online; accessed 16-Feb-2022]. [Online].\nAvailable: https://en.wikipedia.org/wiki/Autoencoder\n[83]\nH. Fan, H. Su, and L. J. Guibas, “A point set generation network\nfor 3d object reconstruction from a single image,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2017,\npp. 605–613.\n[84]\nS. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing be-\ntween capsules,” Advances in neural information processing systems,\nvol. 30, 2017.\n[85]\nS. Chen, C. Duan, Y. Yang, D. Li, C. Feng, and D. Tian, “Deep\nUnsupervised Learning of 3D Point Clouds via Graph Topology\nInference and Filtering,” IEEE Transactions on Image Processing,\nvol. 29, pp. 3183–3198, 2020.\n[86]\nH. Chen, S. Luo, X. Gao, and W. Hu, “Unsupervised learning\nof geometric sampling invariant representations for 3d point\nclouds,” in 2021 IEEE/CVF International Conference on Computer\nVision Workshops (ICCVW).\nIEEE Computer Society, 2021, pp.\n893–903.\n[87]\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative ad-\nversarial nets,” Advances in neural information processing systems,\nvol. 27, 2014.\n[88]\nC.-L. Li, M. Zaheer, Y. Zhang, B. Poczos, and R. Salakhutdinov,\n“Point cloud gan,” arXiv preprint arXiv:1810.05795, 2018.\n[89]\nR. Li, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, “Pu-gan:\na point cloud upsampling adversarial network,” in Proceedings\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n19\nof the IEEE/CVF International Conference on Computer Vision, 2019,\npp. 7203–7212.\n[90]\nE.\nRemelli,\nP.\nBaque,\nand\nP.\nFua,\n“Neuralsampler:\nEu-\nclidean point cloud auto-encoder and sampler,” arXiv preprint\narXiv:1901.09394, 2019.\n[91]\nL. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, “Pu-net:\nPoint cloud upsampling network,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n2790–2799.\n[92]\nW. Yifan, S. Wu, H. Huang, D. Cohen-Or, and O. Sorkine-\nHornung, “Patch-based progressive 3d point set upsampling,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 5958–5967.\n[93]\nY. Qian, J. Hou, S. Kwong, and Y. He, “Pugeo-net: A geometry-\ncentric network for 3d point cloud upsampling,” in European\nConference on Computer Vision.\nSpringer, 2020, pp. 752–769.\n[94]\nG. Qian, A. Abualshour, G. Li, A. Thabet, and B. Ghanem,\n“Pu-gcn: Point cloud upsampling using graph convolutional\nnetworks,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 11 683–11 692.\n[95]\nR. Li, X. Li, P.-A. Heng, and C.-W. Fu, “Point cloud upsampling\nvia disentangled reﬁnement,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2021, pp.\n344–353.\n[96]\nK. Hassani and M. Haley, “Unsupervised multi-task feature\nlearning on point clouds,” in Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, 2019, pp. 8160–8171.\n[97]\nL. Zhang and Z. Zhu, “Unsupervised feature learning for point\ncloud understanding by contrasting and clustering using graph\nconvolutional neural networks,” in 2019 International Conference\non 3D Vision (3DV).\nIEEE, 2019, pp. 395–404.\n[98]\nY. Rao, J. Lu, and J. Zhou, “Global-local bidirectional reasoning\nfor unsupervised representation learning of 3d point clouds,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 5376–5385.\n[99]\nA. Sanghi, “Info3d: Representation learning on 3d objects using\nmutual information maximization and contrastive learning,” in\nEuropean Conference on Computer Vision.\nSpringer, 2020, pp. 626–\n642.\n[100] M. Gadelha, A. RoyChowdhury, G. Sharma, E. Kalogerakis,\nL. Cao, E. Learned-Miller, R. Wang, and S. Maji, “Label-efﬁcient\nlearning on point clouds using approximate convex decomposi-\ntions,” in European Conference on Computer Vision. Springer, 2020,\npp. 473–491.\n[101] O. Poursaeed, T. Jiang, H. Qiao, N. Xu, and V. G. Kim, “Self-\nsupervised learning of point clouds via orientation estimation,”\nin 2020 International Conference on 3D Vision (3DV).\nIEEE, 2020,\npp. 1018–1028.\n[102] B. Du, X. Gao, W. Hu, and X. Li, “Self-contrastive learning\nwith hard negative sampling for self-supervised point cloud\nlearning,” in Proceedings of the 29th ACM International Conference\non Multimedia, 2021, pp. 3133–3142.\n[103] Y. Rao, B. Liu, Y. Wei, J. Lu, C.-J. Hsieh, and J. Zhou, “Ran-\ndomrooms: Unsupervised pre-training from synthetic shapes and\nrandomized layouts for 3d object detection,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2021, pp.\n3283–3292.\n[104] Z. Zhang, R. Girdhar, A. Joulin, and I. Misra, “Self-supervised\npretraining of 3d features on any point-cloud,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV),\nOctober 2021, pp. 10 252–10 263.\n[105] Y. Chen, J. Liu, B. Ni, H. Wang, J. Yang, N. Liu, T. Li, and Q. Tian,\n“Shape self-correction for unsupervised point cloud understand-\ning,” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision, 2021, pp. 8382–8391.\n[106] R. Yamada, H. Kataoka, N. Chiba, Y. Domae, and T. Ogata, “Point\ncloud pre-training with natural 3d structures,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), June 2022, pp. 21 283–21 293.\n[107] Y. Chen, M. Nießner, and A. Dai, “4dcontrast: Contrastive learn-\ning with dynamic correspondences for 3d scene understanding,”\nin Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23–27, 2022, Proceedings, Part XXXII.\nSpringer,\n2022, pp. 543–560.\n[108] L. Li and M. Heizmann, “A closer look at invariances in self-\nsupervised pre-training for 3d vision,” in Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,\nProceedings, Part XXX.\nSpringer, 2022, pp. 656–673.\n[109] J. Yin, D. Zhou, L. Zhang, J. Fang, C.-Z. Xu, J. Shen, and W. Wang,\n“Proposalcontrast: Unsupervised pre-training for lidar-based 3d\nobject detection,” in Computer Vision–ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part\nXXXIX.\nSpringer, 2022, pp. 17–33.\n[110] H. Liu, M. Cai, and Y. J. Lee, “Masked discrimination for self-\nsupervised learning on point clouds,” in Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,\nProceedings, Part II.\nSpringer, 2022, pp. 657–675.\n[111] K. Liu, A. Xiao, X. Zhang, S. Lu, and L. Shao, “Fac: 3d rep-\nresentation learning via foreground aware feature contrast,” in\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2023.\n[112] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry,\n“A papier-mˆach´e approach to learning 3d surface generation,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 216–224.\n[113] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, “Pcn: Point\ncompletion network,” in 2018 International Conference on 3D Vision\n(3DV).\nIEEE, 2018, pp. 728–737.\n[114] Z. Huang, Y. Yu, J. Xu, F. Ni, and X. Le, “Pf-net: Point fractal\nnetwork for 3d point cloud completion,” in 2020 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR), 2020,\npp. 7659–7667.\n[115] M. Liu, L. Sheng, S. Yang, J. Shao, and S.-M. Hu, “Morphing\nand sampling network for dense point cloud completion,” in\nProceedings of the AAAI conference on artiﬁcial intelligence, vol. 34,\nno. 07, 2020, pp. 11 596–11 603.\n[116] W. Zhang, Q. Yan, and C. Xiao, “Detail preserved point cloud\ncompletion via separated feature aggregation,” in Computer\nVision–ECCV 2020: 16th European Conference, Glasgow, UK, August\n23–28, 2020, Proceedings, Part XXV 16.\nSpringer, 2020, pp. 512–\n528.\n[117] C. Xie, C. Wang, B. Zhang, H. Yang, D. Chen, and F. Wen,\n“Style-based point generator with adversarial rendering for point\ncloud completion,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2021, pp. 4619–4628.\n[118] K. Fu, P. Gao, S. Liu, R. Zhang, Y. Qiao, and M. Wang, “Pos-\nbert: Point cloud one-stage bert pre-training,” arXiv preprint\narXiv:2204.00989, 2022.\n[119] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple\nframework for contrastive learning of visual representations,”\nin International conference on machine learning.\nPMLR, 2020, pp.\n1597–1607.\n[120] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv preprint arXiv:1807.03748,\n2018.\n[121] P.-S. Wang, Y.-Q. Yang, Q.-F. Zou, Z. Wu, Y. Liu, and X. Tong,\n“Unsupervised 3d learning for shape analysis via multiresolution\ninstance discrimination,” ACM Trans. Graphic, 2020.\n[122] J. Jiang, X. Lu, W. Ouyang, and M. Wang, “Unsupervised rep-\nresentation learning for 3d point cloud data,” arXiv preprint\narXiv:2110.06632, 2021.\n[123] J. Hou, S. Xie, B. Graham, A. Dai, and M. Nießner, “Pri3d:\nCan 3d priors help 2d representation learning?” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV),\nOctober 2021, pp. 5693–5702.\n[124] J. A. Hartigan and M. A. Wong, “Algorithm as 136: A k-means\nclustering algorithm,” Journal of the royal statistical society. series c\n(applied statistics), vol. 28, no. 1, pp. 100–108, 1979.\n[125] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep cluster-\ning for unsupervised learning of visual features,” in Proceedings\nof the European Conference on Computer Vision (ECCV), 2018, pp.\n132–149.\n[126] J. Sauder and B. Sievers, “Context prediction for unsupervised\ndeep learning on point clouds,” arXiv preprint arXiv:1901.08396,\nvol. 2, no. 4, p. 5, 2019.\n[127] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised repre-\nsentation learning by predicting image rotations,” in International\nConference on Learning Representations, 2018.\n[128] A. Thabet, H. Alwassel, and B. Ghanem, “Self-supervised learn-\ning of local features in 3d point clouds,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops, 2020, pp. 938–939.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n20\n[129] C. Sun, Z. Zheng, X. Wang, M. Xu, and Y. Yang, “Point\ncloud pre-training by mixing and disentangling,” arXiv preprint\narXiv:2109.00452, 2021.\n[130] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stach-\nniss, and J. Gall, “Semantickitti: A dataset for semantic scene\nunderstanding of lidar sequences,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2019, pp. 9297–9307.\n[131] A. Xiao, J. Huang, D. Guan, F. Zhan, and S. Lu, “Transfer\nlearning from synthetic to real lidar point cloud for semantic\nsegmentation,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 36, no. 3, 2022, pp. 2795–2803.\n[132] C. Feichtenhofer, H. Fan, B. Xiong, R. Girshick, and K. He, “A\nlarge-scale study on unsupervised spatiotemporal representation\nlearning,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 3299–3309.\n[133] X. Song, S. Zhao, J. Yang, H. Yue, P. Xu, R. Hu, and H. Chai,\n“Spatio-temporal contrastive domain adaptation for action recog-\nnition,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 9787–9795.\n[134] K. Hu, J. Shao, Y. Liu, B. Raj, M. Savvides, and Z. Shen, “Contrast\nand order representations for video self-supervised learning,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 7939–7949.\n[135] H. Kuang, Y. Zhu, Z. Zhang, X. Li, J. Tighe, S. Schwertfeger,\nC. Stachniss, and M. Li, “Video contrastive learning with global\ncontext,” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision, 2021, pp. 3195–3204.\n[136] D. Z. Chen, A. X. Chang, and M. Nießner, “Scanrefer: 3d object\nlocalization in rgb-d scans using natural language,” in European\nConference on Computer Vision.\nSpringer, 2020, pp. 202–221.\n[137] M. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri,\nK. Thilakarathna, and R. Rodrigo, “Crosspoint: Self-supervised\ncross-modal contrastive learning for 3d point cloud understand-\ning,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2022, pp. 9902–9912.\n[138] Z. Wang, X. Yu, Y. Rao, J. Zhou, and J. Lu, “P2p: Tuning pre-\ntrained image models for point cloud analysis with point-to-pixel\nprompting,” Advances in neural information processing systems,\n2022.\n[139] H. Deng, T. Birdal, and S. Ilic, “Ppf-foldnet: Unsupervised learn-\ning of rotation invariant 3d local descriptors,” in Proceedings of the\nEuropean Conference on Computer Vision (ECCV), 2018, pp. 602–618.\n[140] A.\nZeng,\nS.\nSong,\nM.\nNießner,\nM.\nFisher,\nJ.\nXiao,\nand\nT. Funkhouser, “3dmatch: Learning local geometric descriptors\nfrom rgb-d reconstructions,” in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2017, pp. 1802–1811.\n[141] Y. Zeng, Y. Qian, Z. Zhu, J. Hou, H. Yuan, and Y. He, “Corrnet3d:\nunsupervised end-to-end learning of dense correspondence for\n3d point clouds,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2021, pp. 6052–6061.\n[142] I. Lang, D. Ginzburg, S. Avidan, and D. Raviv, “Dpc: Unsuper-\nvised deep point correspondence via cross and self construction,”\nin 2021 International Conference on 3D Vision (3DV).\nIEEE, 2021,\npp. 1442–1451.\n[143] H. Jiang, Y. Shen, J. Xie, J. Li, J. Qian, and J. Yang, “Sampling\nnetwork guided cross-entropy method for unsupervised point\ncloud registration,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2021, pp. 6128–6137.\n[144] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point trans-\nformer,” in Proceedings of the IEEE/CVF international conference on\ncomputer vision, 2021, pp. 16 259–16 268.\n[145] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz, “Rotation\ninvariant spherical harmonic representation of 3 d shape descrip-\ntors,” in Symposium on geometry processing, vol. 6, 2003, pp. 156–\n164.\n[146] D.-Y. Chen, X.-P. Tian, Y.-T. Shen, and M. Ouhyoung, “On visual\nsimilarity based 3d model retrieval,” in Computer graphics forum,\nvol. 22, no. 3.\nWiley Online Library, 2003, pp. 223–232.\n[147] B. Eckart, W. Yuan, C. Liu, and J. Kautz, “Self-supervised learning\non 3d point clouds by learning discrete generative models,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 8248–8257.\n[148] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.”\nJournal of machine learning research, vol. 9, no. 11, 2008.\n[149] Y. Yan, Y. Mao, and B. Li, “Second: Sparsely embedded convolu-\ntional detection,” Sensors, vol. 18, no. 10, p. 3337, 2018.\n[150] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and\nA. Joulin, “Unsupervised learning of visual features by contrast-\ning cluster assignments,” Advances in Neural Information Process-\ning Systems, vol. 33, pp. 9912–9924, 2020.\nAoran Xiao received his B.Sc. and M.Sc. degree\nfrom Wuhan University, China in 2016 and 2019,\nrespectively. He is currently pursuing the Ph.D.\ndegree with the school of computer science and\nengineering at Nanyang Technological Univer-\nsity, Singapore. His research interests lie in point\ncloud processing, computer vision, and remote\nsensing.\nJiaxing Huang received his B.Eng. and M.Sc.\nin EEE from the University of Glasgow, UK, and\nthe Nanyang Technological University (NTU),\nSingapore, respectively. He is currently a Re-\nsearch Associate and Ph.D. student with School\nof Computer Science and Engineering, NTU,\nSingapore. His research interests include com-\nputer vision and machine learning.\nDayan Guan is currently a Research Scientist\nat Mohamed bin Zayed University of Artiﬁcial In-\ntelligence, United Arab Emirates. Before that, he\nhad been a Research Fellow at Nanyang Tech-\nnological University from Nov 2019 to Mar 2022.\nIn Sep 2019, he received his Ph.D. from Zhejiang\nUniversity, China. His research interests include\ncomputer vision, pattern recognition and deep\nlearning.\nXiaoqin Zhang is a senior member of the IEEE.\nHe received the B.Sc. degree in electronic in-\nformation science and technology from Central\nSouth University, China, in 2005, and the Ph.D.\ndegree in pattern recognition and intelligent sys-\ntem from the National Laboratory of Pattern\nRecognition, Institute of Automation, Chinese\nAcademy of Sciences, China, in 2010. He is\ncurrently a Professor with Wenzhou University,\nChina. He has published more than 100 papers\nin international and national journals, and inter-\nnational conferences, including IEEE T-PAMI, IJCV, IEEE T-IP, IEEE T-\nNNLS, IEEE T-C, ICCV, CVPR, NIPS, IJCAI, AAAI, and among others.\nHis research interests include in pattern recognition, computer vision,\nand machine learning.\nShijian Lu is an Associate Professor with the\nSchool of Computer Science and Engineering\nat the Nanyang Technological University, Singa-\npore. He received his PhD in electrical and com-\nputer engineering from the National University of\nSingapore. His major research interests include\nimage and video analytics, visual intelligence,\nand machine learning.\nLing Shao is the Chief Scientist of Terminus\nGroup and the President of Terminus Interna-\ntional. He was the founding CEO and Chief Sci-\nentist of the Inception Institute of Artiﬁcial Intel-\nligence, Abu Dhabi, UAE. His research interests\ninclude computer vision, deep learning, medical\nimaging and vision and language. He is a fellow\nof the IEEE, the IAPR, the BCS and the IET.\n",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "published": "2022-02-28",
  "updated": "2023-03-27"
}