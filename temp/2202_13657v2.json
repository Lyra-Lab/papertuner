{
  "id": "http://arxiv.org/abs/2202.13657v2",
  "title": "Avalanche RL: a Continual Reinforcement Learning Library",
  "authors": [
    "Nicolò Lucchesi",
    "Antonio Carta",
    "Vincenzo Lomonaco",
    "Davide Bacciu"
  ],
  "abstract": "Continual Reinforcement Learning (CRL) is a challenging setting where an\nagent learns to interact with an environment that is constantly changing over\ntime (the stream of experiences). In this paper, we describe Avalanche RL, a\nlibrary for Continual Reinforcement Learning which allows to easily train\nagents on a continuous stream of tasks. Avalanche RL is based on PyTorch and\nsupports any OpenAI Gym environment. Its design is based on Avalanche, one of\nthe more popular continual learning libraries, which allow us to reuse a large\nnumber of continual learning strategies and improve the interaction between\nreinforcement learning and continual learning researchers. Additionally, we\npropose Continual Habitat-Lab, a novel benchmark and a high-level library which\nenables the usage of the photorealistic simulator Habitat-Sim for CRL research.\nOverall, Avalanche RL attempts to unify under a common framework continual\nreinforcement learning applications, which we hope will foster the growth of\nthe field.",
  "text": "Avalanche RL: a Continual Reinforcement\nLearning Library\nNicol`o Lucchesi, Antonio Carta, Vincenzo Lomonaco, and Davide Bacciu\n1 Deparment of Computer Science, University of Pisa\n2 nicolo.lucchesi@gmail.com\n3 antonio.carta@di.unipi.it\n4 vincenzo.lomonaco@unipi.it\n5 bacciu@unipi.it\nAbstract. Continual Reinforcement Learning (CRL) is a challenging\nsetting where an agent learns to interact with an environment that is\nconstantly changing over time (the stream of experiences). In this paper,\nwe describe Avalanche RL, a library for Continual Reinforcement Learn-\ning which allows users to easily train agents on a continuous stream of\ntasks. Avalanche RL is based on PyTorch [23] and supports any Ope-\nnAI Gym [4] environment. Its design is based on Avalanche [16], one of\nthe most popular continual learning libraries, which allow us to reuse a\nlarge number of continual learning strategies and improve the interac-\ntion between reinforcement learning and continual learning researchers.\nAdditionally, we propose Continual Habitat-Lab, a novel benchmark and\na high-level library which enables the usage of the photorealistic simula-\ntor Habitat-Sim [28] for CRL research. Overall, Avalanche RL attempts\nto unify under a common framework continual reinforcement learning\napplications, which we hope will foster the growth of the ﬁeld.\nKeywords: Continual Learning · Reinforcement Learning · Reproducibil-\nity.\n1\nIntroduction\nRecent advances in data-driven algorithms, the so-called Deep Learning rev-\nolution, has shown the possibility for AI algorithms to achieve unprecedented\nperformances on a narrow set of speciﬁc tasks. On the contrary, humans are able\nto quickly learn new tasks and generalize to novel scenarios. Continual Learning\n(CL) in the same way seeks to develop data-driven algorithms able to incremen-\ntally learn behaviors from a stream of data. Reinforcement Learning (RL) is yet\nanother Machine Learning paradigm which formulates the learning process as\na sequence of interactions between an agent and the environment. The agent\nmust learn oﬀof this interaction how to achieve a goal of a particular task by\ntaking actions in the environment while receiving a (scalar) reward. Continual\nReinforcement Learning (CRL) combines the non-stationarity assumption of a\nstream of data with the RL setting, having an agent learn multiple tasks in se-\nquence.\narXiv:2202.13657v2  [cs.LG]  24 Mar 2022\n2\nN. Lucchesi et al.\nWhile still in its early stages, CRL has seen a rising interest in publications\nin recent years (according to Dimensions [10] data). To support this growth, we\nfocus on benchmarks and tools, introducing AvalancheRL: we extend Avalanche\n[16], the staple framework for Continual or Lifelong Learning, to support Rein-\nforcement Learning in order to seamlessly train agents on a continuous stream\ntasks.\nExisting RL libraries [25,21,6,24] do not focus on lifelong applications and force\nusers to write custom code to develop continual solutions. Avalanche gives us re-\nusability by providing pre-implemented CL strategies as well as code structure\nwhen experimenting with them, but lacked support altogether when coming to\nRL. Related CRL projects instead either focus on providing a speciﬁc benchmark\n[33] or combine multiple frameworks results [22], limiting the overall ﬂexibility\nand methods customization options.\nAvalanche RL attempts to address both problems aiming to oﬀer a malleable\nframework encompassing a variety of RL algorithms with ﬁne-grained control\nover their internals, leveraging pre-existing CL techniques to learn eﬃciently\nfrom the interaction with multiple environments. In particular, we support any\nenvironment exposing the OpenAI Gym gym.Env interface.\nThe availability of compelling benchmarks has always lead the progress of data-\ndriven algorithms [14,13,5], therefore our second eﬀort is aimed at providing a\nchallenging dataset for realistic Continual Reinforcement Learning.\nHabitat-Lab allows an embodied agent to roam a photorealistic (typically in-\ndoor) scene in the attempt of solving a particular task; unfortunately, it does\nnot oﬀer support for the continual scenario. Therefore, we developed Continual\nHabitat-Lab, a high-level library enabling the usage of Habitat-Sim [28] for CRL,\nallowing the creation of sequences of tasks while integrating with Avalanche RL.\nWe ﬁrst outline the design principles that guided the development of Avalanche\nRL (Section 2), describe its structure (Figure 1) and go over the main features\nof the framework with code examples (Section 3). We then introduce Continual\nHabitat-Lab and describe its integration with Avalanche RL (Section 4).\nAll the source code of the work hereby presented is publicly available on GitHub\nfor both Avalanche RL6 and Continual Habitat-Lab7.\n2\nDesign Principles\nAvalanche RL is built as an extension of Avalanche [16], and it retains the\nsame design principles and a similar API. The target users are practitioners\nand researchers, and therefore the library must be simple, allowing to setup an\n6 https://github.com/continualAI/avalanche-rl\n7 https://github.com/NickLucche/continual-habitat-lab\nAvalanche RL: a Continual Reinforcement Learning Library\n3\nFig. 1: Avalanche RL core-functionalities overview. The Benchmarks module ca-\npabilities, providing access to a stream of environments, are addressed in Section\n3.1. Data is obtained through (parallel, Section 3.3) interaction with the stream\nand it is consumed by the algorithm in the learning process, as motivated in\nSection 3.2. Streams can be easily created through benchmark generators (right-\nhand side).\nexperiment with a few lines of code, as well as highly customizable. As a result,\nAvalanche RL provides high-level APIs with ready-to-use components, as well\nas low-level features that allow heavy customization of existing implementations\nby leveraging an exhaustive callback system (Section 3.2).\nAvalanche RL codebase is comprises 5 main modules: Benchmarks, Training,\nEvaluation, Models, and Logging. We give a brief overview of them in the\nremainder of this section, but we refer the reader to [16] for more details about\nthe general architecture of Avalanche.\nBenchmarks maintains a uniform API for data handling, generating a stream\nof data from one or more datasets, conveniently divided into temporal expe-\nriences; this is the core abstraction over the task stream formalism which is\ndistinctive of CL and it is accessible through a Scenario object. In order to cre-\nate benchmarks more easily, this module provides benchmark generators which\nallow one to specify particular conﬁgurations through a simple API.\nTraining provides all the necessary utilities concerning model training. It in-\ncludes simple and eﬃcient ways of implementing new strategies as well as a set\npre-implemented CL baselines and state-of-the-art algorithms. A Strategy ab-\nstracts a general learning algorithm implementing a training and an evaluation\nloop while consuming experiences from a benchmark. Continual behaviors can\n4\nN. Lucchesi et al.\nbe added when needed through Plugins: they operate latching on the callback\nsystem deﬁned by Strategies and are designed in such a modular way so that\nthey can be easily composed to provide hybrid behaviors.\nEvaluation provides all the utilities and metrics that can help evaluate a CL al-\ngorithm. Here we can ﬁnd pluggable metric monitors such as (Train/Test/Batch)\nAccuracy, RAM, CPU and GPU usage, all designed with the same modularity\nprinciples in mind.\nModels contains several model architectures and pre-trained models that can be\nused for continual learning experiments (similar to torchvision.models), from\nsimple customizable networks to implementation of state-of-the-art models.\nLogging includes advanced logging and plotting features with the purpose of\nvisualizing the metrics of the Evaluation module, such as highly readable output,\nﬁle and TensorBoard support.\n2.1\nNotation\nWe adopt the well renowned notation from [32] for Reinforcement Learning\nrelated formulations while we make use of the formalization introduced in [15]\nregarding Continual Learning.\nIn particular, we refer to the RL problem as consisting of a tuple of ﬁve elements\ncommonly denoted as < S, A, R, P, γ > in the MDP formulation, where S and A\nare sets of states and actions, respectively. R or r() is the reward function,\nwith r(s, a, s′) being the expected immediate reward for transition from state\ns ∈S to s′ ∈S under action a ∈A. P or p() is the transition function deﬁning\nthe dynamics of the environment, with p(s′, r|s, a) denoting the probability of\ntransitioning from s into s′ with scalar reward r under a. Finally, γ represents the\ndiscount factor which weights the importance of immediate and future rewards.\nAn agent follows a policy π, which maps states to action probabilities. In Deep\nRL, learned policies are parameterized function (such as a neural network) which\nwe indicate with πθ.\nWe refer to a Dataset as a collection of samples {xi}N\ni , optionally with labels\n{< xi, yi >}N\ni in the case of supervised learning. We then denote a general task\nto be solved by some agent with τ and deﬁne the data relative to that task with\nDτ.\n3\nAvalanche RL\nCRL applications in Avalanche RL are implemented by modeling the interaction\nbetween core components: the task-stream abstraction (i.e., the continuously\nchanging environment) and the RL strategy (i.e., the agent and its learning\nalgorithm).\nAvalanche RL: a Continual Reinforcement Learning Library\n5\nAvalanche RL implements these two components in the Benchmarks and\nTraining module, respectively. In the remainder of this section, we describe\nthe environment and the implementation of its continual shift in Section 3.1.\nThen, in Section 3.2, we describe the implementation of RL algorithms and\ntheir integration in the Training module. Section 3.3 and 3.4 highlight some\nimportant implementation details and useful features oﬀered by the framework,\nsuch as the automatic parallelization of the RL environment.\n3.1\nBenchmarks: Stream of Environments\nMost continual learning frameworks [15] assume that the stream of data is made\nof static datasets of a ﬁxed size. Instead, in CRL problems the stream consists\nof diﬀerent environments, and samples are obtained through the interaction be-\ntween the agent and the environment.\nTo support streams of environments, Avalanche RL deﬁnes a stream S =\n{e1, e2, ..} as a sequence of experiences ei, where each experience provides access\nto an environment with which the agent can interact to generate state transitions\n(samples) online. Over time, this means that the agent learns by interacting\nwith a stream of environments {E1, E2, ..}, as in Figure 1. In the source code,\nRLExperience is the class which deﬁnes the CRL experience.\nUsing this task-stream abstraction, it is easy to deﬁne CRL benchmarks\nas a set of parallel streams of environments. Notice that each experience may\nbe a small shift, such as a change in the background, as well as a completely\ndiﬀerent tasks, such as a diﬀerent game. Diﬀerent tasks may provide a task label\nwhich can be used by the agent to distinguish among them. The RLScenario is\nthe class responsible for the CRL benchmark’s deﬁnition, and it can be thought\nas a container of streams.\nRL Environments implement a common interface, which is the one of OpenAI\nGym environments. This common interface allows to abstract away the inter-\naction with the environment, decoupling the data generation process from the\ndata sampling and freeing the user from the hassle of manually re-writing the\ndata-fetching loop.\nNew CRL benchmarks can be easily created using the gym benchmark generator,\nwhich allows to deﬁne an RLScenario by providing any sequence of Gym envi-\nronments (including custom ones). We can see an example in Fig. 2, in which\nwe instantiate an RLScenario handling a stream of tasks which gives access to\ntwo randomly sampled environments.\nNote that unlike static datasets, the environment can be used to produce an\nendless amount of data. Therefore, the interaction with the experience must be\nexplicitly limited by some number of steps or episodes rather than epochs, which\nwe can express during the creation of a Strategy as in Section 3.2.\nAs the Atari game suite [3] has become the main benchmark for RL algo-\nrithms in recent years, we also provide a tailored atari benchmark generator\n6\nN. Lucchesi et al.\n(a) Benchmark creation\n(b) Minimal training setup\nFig. 2: Example of Avalanche RL usage. (a) deﬁnes a task stream alternating\ntwo randomly sampled environments for 4 experiences. n parallel envs spec-\niﬁes the number of parallel actors (Section 3.3). The second scenario instead\ncreates a stream of 2 Atari games with pre-processing attached. (b) puts every-\nthing together, instantiating a pre-implemented model (Section 3.4) and creating\nan “A2C agent” which is trained on the stream of games. The agent will perform\n10000 Update steps per-experience while gathering 5 data samples at every Roll-\nout step (Section 3.2). Evaluation will take place with the speciﬁed parameters.\n(Fig. 2) which takes care of adding common pre-processing techniques (e.g. frame\nstacking) as Gym Wrappers around each environment. This allows to minimize\nthe time in between experiments as one can easily reproduce setups such as the\none in [12] (sampling random Atari games to learn in sequence) by simply spec-\nifying a few arguments when creating a scenario. The benchmark interface also\npromotes the pattern of environment wrapping, which is Gym’s intended way of\norganizing data processing methods to favor reproducibility. Reproducibility of\nexperiments in particular is of great importance to Avalanche and one of the\nmain reasons that drove us to propose an end-to-end framework for CRL.\n3.2\nTraining: Reinforcement Learning Strategies\nAvalanche RL provides several learning algorithms (listed at the end of this\nsection) which have been implemented to be highly modular and easily cus-\ntomizable. The framework oﬀers full-access to their internals in order to provide\nﬁne-grained customization options and speciﬁc support for continual learning\ntechniques.\nThere are two main patterns to adapt a learning algorithm: subclassing and\nPlugins (as introduced in Section 2). In particular, Avalanche RL implements\nmost continual learning strategies as Plugins. The modularity of the implemen-\ntation allows to combine many RL strategies with popular CL strategies, such\nas replay buﬀers, regularization methods, and so on. As far as we are aware,\nAvalanche is the only library that allows the seamless composition of diﬀerent\nAvalanche RL: a Continual Reinforcement Learning Library\n7\nlearning algorithms. RL strategies inherit from RLBaseStrategy, a class which\nprovides a common “skeleton” for both on and oﬀ-policy algorithms and ab-\nstracts many of the most repetitive patterns, including environment interaction,\ntracking metrics, CPU-GPU tensor relocation and more. RLBaseStrategy also\nprovides callbacks which can be used by plugins.\nInspired by the open-source framework stable-baseline3 [25] (sb3), RL strate-\ngies are divided into two main steps: rollout collection and update. Unlike sb3,\nwe grouped both on and oﬀ-policy algorithms under this simple workﬂow.\nThe rollout stage abstracts the data gathering process which iterates the follow-\ning steps:\n1. at ∼πθ(st): sample rollout action, to be implemented by the speciﬁc\nalgorithm, returns the action to perform during a rollout step.\n2. play action and observe next state and reward: s’, r, done, info=env.step(at)\nreferring to Gym interface.\n3. store state transition in some data structure: Step. Store multiple Steps in a\nRollout. These data structures are optimized for insertion speed and lazily\ndelay “re-shaping” operations until they are needed by the update phase.\n4. test rollout terminal condition, number of steps or episodes to run.\nThe update step is instead entirely delegated to the speciﬁcs of the algo-\nrithm: it boils down to implementing a method which has access to the rollouts\ncollected at the previous stage and must deﬁne and compute a loss function\nwhich is then used to execute the actual parameters update. To enable the user\nwith ﬁne-grained control over the strategy workﬂow, we added callbacks which\nare executed just before and after the two stages.\nAt a higher level, the workﬂow we described happens within a single experience.\nTo learn from a stream, the process is repeated for each experience in the stream,\na behavior which is implemented by the RLBaseStrategy.\nTo summarize, one can implement a RL algorithm by sub-classing RLBaseStrategy\nand implementing the sample rollout action and update step. For example,\nA2C can be implemented in less than 30 lines of code 8. Alternatively, customiza-\ntion of any algorithm is always possible by implementing a plugin, which allows to\n“inject” additional behavior, or by subclassing any of the available strategies. All\nthe algorithm implementations expose their internals through class attributes,\nso one can for instance access the loss externally (e.g. from plugins) simply with\nstrategy.loss.\nAlong with the release of our framework we provide an implementation of A2C\nand DQN [18], including popular “variants” with target network [19] and Dou-\nbleDQN [9].\n3.3\nParallel Actors Interaction: VectorizedEnv\nSince the data gathering supports any environment exposing the Gym interface,\nwe are also able to automatically parallelize the agent-environment interaction\n8 https://github.com/ContinualAI/avalanche-rl/blob/master/avalanche_rl/\ntraining/strategies/actor_critic.py\n8\nN. Lucchesi et al.\nin a transparent way to the user. This common practice [17,25,6] relies on using\nmultiple Actors, each owning a local copy of the environment in which they\nperform actions, while synchronizing updates on a shared network; varying the\namount of local resources available to each worker we can obtain diﬀerent degrees\nof asynchronicity [7], allowing to scale computations on multiple CPUs.\nTo implement this behavior we leveraged Ray [20], a framework for parallel\nand distributed computing with a focus on AI applications. Ray abstracts away\nthe parallel (and distributed) execution of code, sharing data between master\nand workers by serializing numpy arrays, which, in the case of execution on a\nsingle machine, are written once to shared memory in read-only mode and only\nreferred to by actors.\nThis feature is opaque to the user, as it happens entirely inside a VectorizedEnv:\nthis component wraps a single Gym environment and exposes the same interface,\nwhile under the hood it instantiates a pool of actors and handles results gath-\nering and synchronization, acting as master. The API of our implementation\nwas inspired by the work of sb3, although we opted to use Ray as a backend\ninstead of Python’s multiprocessing library due to distributed setting support.\nRLBaseStrategy takes care of wrapping any environment with a VectorizedEnv,\nso the user can exploit parallel execution by simply specifying the number of\nworkers/environment replicas, as shown in Figure 2.\n3.4\nAdditional Features\nTo complement the features we described in the previous sections we also im-\nplemented a series of utility components which one expects from a serviceable\nframework. Most of the changes listed in this section are not as important when\ntaken singularly but as a whole they contribute signiﬁcantly to Avalanche RL\nfunctionalities and as such they are hereby reported.\n– Models from seminal papers such as [19,9,18,17] have been re-implemented\nin Pytorch and are available in the Models module.\n– Evaluation Metrics: RLBaseStrategy automatically records gathered rewards\nand episode lengths during training, smoothing scalars with a window aver-\nage by default. Additionally, one can record any signiﬁcant value (e.g. loss,\nϵ-greedy’s ϵ) with minimal eﬀort thanks to improved metrics builders.\n– Continual Control Environments: classic control environments provided by\nGym have been wrapped in order to expose hard-coded parameters (e.g. grav-\nity, force..) which can now be modiﬁed to obtain varying conditions. This is\nuseful for rapidly testing out algorithms on well renowned problems.\n– Extended available Plugins, including EWC [12] and a ReplayMemory-\nbased one inspired by works from [27] and [11].\n– Miscellaneous tools such as environment wrappers for easily re-mapping ac-\ntions keys (useful when learning multiple games with a single network) or\nreducing the action set and an additional logger with improved readabil-\nity. Avalanche RL is compatible with Avalanche logging methods, such as\nTensorboard [1].\nAvalanche RL: a Continual Reinforcement Learning Library\n9\n4\nContinual Habitat Lab\nContinual-Habitat-Lab (CHL) is a high-level library for FAIR’s simulator\nHabitat [28]: inspired by Habitat-Lab, we created a library with the goal of\nadding support for continual learning. CHL deﬁnes the abstraction layer needed\nto work with a stream of tasks {τ1, τ2..}, the core of CL systems.\nWe designed the library to be a shallow wrapper on top of Habitat-Sim function-\nalities and API while “steering” its intended usage toward learning applications,\nenforcing the data generation process to be carried out through online interac-\ntion and dropping the need for a pre-computed Dataset of positions altogether.\nWe also revisited the concept of Task to make it simpler and yet give it more\ncontrol over the environment: while the next-state transition function p(s′|s, a)\nis implemented by the dynamics of the simulator (Habitat-Sim), we bundled\nthe reward function r into the task deﬁnition. To deﬁne a Task one must hence\ndeﬁne a reward function r(s, a, s′) →r, a goal test function g(s) →{T, F} and\nan action space A as deﬁned by Gym.\nAs Task is meant to be the main component through which the user can inject\nlogic and behavior to be learned by the agent, we give direct access to the simu-\nlator at speciﬁc times through callbacks (e.g. to change environment condition,\nlighting, add objects..).\nIn order to to natively support CRL a TaskIterator is assigned to the han-\ndling of the stream of tasks, hiding away the logic behind task sampling and\nduration while giving access to the current active task to be used by the envi-\nronment.\nWe leveraged the multitude of 3D scenes datasets compatible with Habitat-Sim\nwith the goal of specifying changing environment conditions, a most important\nfeature to CL. To do so, we bundled the functionalities regarding scene switch in\na sole component named SceneManager. It provides utilities for loading and\nswitching scenes with a few conﬁgurable behaviors: scene swapping can happen\non task change or after a number of episodes or or actions is reached, even amid\na running episode, maintaining current agent conﬁguration and avoiding any\nexpensive simulator re-instantions.\nTo oﬀer a easily conﬁgurable system we re-designed the conﬁguration system\nfrom scratch basing it on the popular OmegaConf library for Python: apart from\nproviding a uniﬁed conﬁguration entry-point which can be created program-\nmatically or from a yaml ﬁle, the system dynamically maps Task and Actions\nparameters to conﬁguration options. This allows the user to change experiments\nconditions by changing class arguments directly from the conﬁguration ﬁle.\nContinual Habitat Lab is integrated with Avalanche RL through a specialized\nbenchmark generator (habitat benchmark generator) that takes care of syn-\nchronizing the stream of tasks deﬁned in the CHL conﬁguration with the one\nserved to a Strategy. It does so by deﬁning an experience each time a task or\n10\nN. Lucchesi et al.\nscene is changed, while serving the same object reference to the Habitat-Sim\nenvironment.\n5\nConclusion and Future Work\nIn this paper, we have presented two novel libraries for Continual Reinforce-\nment Learning: Avalanche RL and Continual Habitat Lab. We believe that\nthese libraries can be helpful for the CRL community by extending and adapt-\ning work from the Continual Learning community on supervised and unsuper-\nvised continual learning (Avalanche) while also integrating a realistic simulator\n(Habitat-Sim) to benchmark CRL algorithms on complex embodied real-life\nscenarios.\nIn particular, Avalanche RL allows users to easily train and evaluate agents\non a continual stream of tasks deﬁned as a sequence of any Gym Environment.\nIt is based on implementing a simple API upon the interaction of RL algorithms\nand task-streams, while oﬀering a ﬁne-grained control over their internals.\nThrough Avalanche researchers can exploit and extend the large amount of work\ndone by the Continual Learning community while beneﬁting from the integration\nof highly modular and easily extensible RL algorithms. The library implements\na large set of highly desirable features, such as parallel environment interaction,\nand provides implementations for popular baselines such as EWC [12], includ-\ning benchmarks, learning strategies and architectures, all of which can be easily\ninstantiated with a single line of code.\nAvalanche RL can improve code reusability, ease-of-use, modularity and repro-\nducibility of experiments, and we strongly believe that the whole CRL commu-\nnity would beneﬁt from a collective eﬀort such as Avalanche RL as a tool to\nspeed-up the research in the ﬁeld.\nHaving the goal of providing a shared and collaborative open-source codebase\nfor CRL applications, Avalanche RL is constantly looking to add and reﬁne\nfunctionalities. In the short term, we plan to implement a broader range of\nstate-of-the art RL algorithms, including (but not limited to) PPO [30], TRPO\n[29] and SAC [8]. Additionally, we are also looking to increment the number of\nCL strategies such as pseudo-rehersal [26,2].\nWe are aiming to keep on expanding the supported simulators targeting a wider\nrange of applications, from robotics to games engines [31] to widen the CRL\nbenchmarks suite. Finally, we are expecting to merge Avalanche RL into Avalanche,\nstriving to provide a single end-to-end framework for all continual learning ap-\nplications.\nAvalanche RL: a Continual Reinforcement Learning Library\n11\nReferences\n1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., , et al.: Ten-\nsorFlow: Large-scale machine learning on heterogeneous systems (2015), https:\n//www.tensorflow.org/, software available from tensorﬂow.org\n2. Atkinson, C., McCane, B., Szymanski, L., Robins, A.V.: Pseudo-rehearsal:\nAchieving deep reinforcement learning without catastrophic forgetting. CoRR\nabs/1812.02464 (2018), http://arxiv.org/abs/1812.02464\n3. Bellemare, M.G., Naddaf, Y., Veness, J., Bowling, M.: The arcade learning en-\nvironment: An evaluation platform for general agents. Journal of Artiﬁcial In-\ntelligence Research Vol. 47, 253–279 (2012). https://doi.org/10.1613/jair.3912,\nhttp://arxiv.org/abs/1207.4708, cite arxiv:1207.4708\n4. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,\nJ., et al.: Openai gym. CoRR abs/1606.01540 (2016), http://arxiv.org/abs/\n1606.01540\n5. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-\nscale hierarchical image database. In: 2009 IEEE conference on computer vision\nand pattern recognition. pp. 248–255. Ieee (2009)\n6. Denoyer, L., la Fuente, A.D., Duong, S., Gaya, J., Kamienny, P., Thompson, D.H.:\nSalina: Sequential learning of agents. CoRR abs/2110.07910 (2021), https://\narxiv.org/abs/2110.07910\n7. Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., et al.:\nImpala: Scalable distributed deep-rl with importance weighted actor-learner archi-\ntectures (2018)\n8. Haarnoja, T., Zhou, A., Abbeel, P., Levine, S.: Soft actor-critic: Oﬀ-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. CoRR\nabs/1801.01290 (2018), http://arxiv.org/abs/1801.01290\n9. van Hasselt, H., Guez, A., Silver, D.: Deep reinforcement learning with double\nq-learning. CoRR abs/1509.06461 (2015), http://arxiv.org/abs/1509.06461\n10. Hook,\nD.W.,\nPorter,\nS.J.,\nHerzog,\nC.:\nDimensions:\nBuilding\ncontext\nfor\nsearch\nand\nevaluation.\nFrontiers\nin\nResearch\nMetrics\nand\nAnalytics\n3,\n23 (2018). https://doi.org/10.3389/frma.2018.00023, https://www.frontiersin.\norg/article/10.3389/frma.2018.00023\n11. Isele, D., Cosgun, A.: Selective experience replay for lifelong learning. CoRR\nabs/1802.10269 (2018), http://arxiv.org/abs/1802.10269\n12. Kirkpatrick, J., Pascanu, R., Rabinowitz, N.C., Veness, J., Desjardins, G., Rusu,\nA.A., et al.: Overcoming catastrophic forgetting in neural networks. CoRR\nabs/1612.00796 (2016), http://arxiv.org/abs/1612.00796\n13. Krizhevsky, A., Nair, V., Hinton, G.: Cifar-10 (canadian institute for advanced\nresearch) http://www.cs.toronto.edu/~kriz/cifar.html\n14. LeCun, Y., Cortes, C.: MNIST handwritten digit database (2010), http://yann.\nlecun.com/exdb/mnist/\n15. Lesort,\nT.,\nLomonaco,\nV.,\nStoian,\nA.,\nMaltoni,\nD.,\nFilliat,\nD.,\nD´ıaz-\nRodr´ıguez, N.: Continual learning for robotics: Deﬁnition, framework, learn-\ning\nstrategies,\nopportunities\nand\nchallenges.\nInformation\nFusion\n58,\n52–\n68 (2020). https://doi.org/https://doi.org/10.1016/j.inﬀus.2019.12.004, https://\nwww.sciencedirect.com/science/article/pii/S1566253519307377\n16. Lomonaco, V., Pellegrini, L., Cossu, A., Carta, A., Graﬃeti, G., Hayes, T.L., et al.:\nAvalanche: an end-to-end library for continual learning (2021)\n12\nN. Lucchesi et al.\n17. Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T., Silver, D.,\nKavukcuoglu, K.: Asynchronous methods for deep reinforcement learning. CoRR\nabs/1602.01783 (2016), http://arxiv.org/abs/1602.01783\n18. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,\nRiedmiller, M.: Playing atari with deep reinforcement learning (2013)\n19. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,\net al.: Human-level control through deep reinforcement learning. Nature 518(7540),\n529–533 (Feb 2015), http://dx.doi.org/10.1038/nature14236\n20. Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., Elibol,\nM., Yang, Z., Paul, W., Jordan, M.I., Stoica, I.: Ray: A distributed framework for\nemerging ai applications (2018)\n21. Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., et al.: Ray:\nA distributed framework for emerging AI applications. CoRR abs/1712.05889\n(2017), http://arxiv.org/abs/1712.05889\n22. Normandin, F., Golemo, F., Ostapenko, O., Rodr´ıguez, P., Riemer, M.D., Hurtado,\nJ., Khetarpal, K., Zhao, D., Lindeborg, R., Lesort, T., Charlin, L., Rish, I., Caccia,\nM.: Sequoia: A software framework to unify continual learning research. CoRR\nabs/2108.01005 (2021), https://arxiv.org/abs/2108.01005\n23. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., et al.:\nPytorch:\nAn\nimperative\nstyle,\nhigh-performance\ndeep\nlearning\nlibrary.\nIn:\nWallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., Gar-\nnett, R. (eds.) Advances in Neural Information Processing Systems 32, pp.\n8024–8035. Curran Associates, Inc. (2019), http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf\n24. Plappert, M.: keras-rl. https://github.com/keras-rl/keras-rl (2016)\n25. Raﬃn, A., Hill, A., Ernestus, M., Gleave, A., Kanervisto, A., Dormann, N.: Stable\nbaselines3. https://github.com/DLR-RM/stable-baselines3 (2019)\n26. Robins, A.V.: Catastrophic forgetting, rehearsal and pseudorehearsal. Connect.\nSci. 7, 123–146 (1995)\n27. Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T.P., Wayne, G.: Experience replay\nfor continual learning. CoRR abs/1811.11682 (2018), http://arxiv.org/abs/\n1811.11682\n28. Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., et al.:\nHabitat: A platform for embodied ai research (2019)\n29. Schulman, J., Levine, S., Moritz, P., Jordan, M.I., Abbeel, P.: Trust region pol-\nicy optimization. CoRR abs/1502.05477 (2015), http://arxiv.org/abs/1502.\n05477\n30. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy\noptimization algorithms (2017)\n31. Schwarz, J., Altman, D., Dudzik, A., Vinyals, O., Teh, Y.W., and, R.P.: Towards\na natural benchmark for continual learning (2018), https://marcpickett.com/\ncl2018/CL-2018_paper_48.pdf\n32. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. The MIT\nPress, second edn. (2018), http://incompleteideas.net/book/the-book-2nd.\nhtml\n33. Wolczyk, M., Zajac, M., Pascanu, R., Kucinski, L., Milos, P.: Continual world: A\nrobotic benchmark for continual reinforcement learning. CoRR abs/2105.10919\n(2021), https://arxiv.org/abs/2105.10919\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV"
  ],
  "published": "2022-02-28",
  "updated": "2022-03-24"
}