{
  "id": "http://arxiv.org/abs/2202.13972v2",
  "title": "The impact of lexical and grammatical processing on generating code from natural language",
  "authors": [
    "Nathanaël Beau",
    "Benoît Crabbé"
  ],
  "abstract": "Considering the seq2seq architecture of TranX for natural language to code\ntranslation, we identify four key components of importance: grammatical\nconstraints, lexical preprocessing, input representations, and copy mechanisms.\nTo study the impact of these components, we use a state-of-the-art architecture\nthat relies on BERT encoder and a grammar-based decoder for which a\nformalization is provided. The paper highlights the importance of the lexical\nsubstitution component in the current natural language to code systems.",
  "text": "The impact of lexical and grammatical processing on generating code\nfrom natural language\nNathanaël Beau1,2 and Benoît Crabbé1\n1 Université de Paris, LLF, CNRS, 75013 Paris, France\n2 onepoint, 29 rue des Sablons, F-75116 Paris, France\nn.beau@groupeonepoint.com\nbenoit.crabbe@u-paris.fr\nAbstract\nConsidering the seq2seq architecture of Yin\nand Neubig (2018) for natural language to\ncode translation, we identify four key compo-\nnents of importance: grammatical constraints,\nlexical preprocessing, input representations,\nand copy mechanisms. To study the impact\nof these components, we use a state-of-the-art\narchitecture that relies on BERT encoder and\na grammar-based decoder for which a formal-\nization is provided. The paper highlights the\nimportance of the lexical substitution compo-\nnent in the current natural language to code\nsystems.\n1\nIntroduction\nTranslating natural language program descriptions\nto actual code is meant to help programmers to ease\nwriting reliable code efﬁciently by means of a set\nof advanced code completion mechanisms.\nThere are mainly two classes of methods for ob-\ntaining code corresponding to a query expressed\nin natural language. The ﬁrst one is code retrieval,\nwhich consists of searching and retrieving an ap-\npropriate code snippet from a code database. The\nsecond one is code generation, where the goal is to\ngenerate code fragments from a natural language\ndescription, generating potentially previously un-\nseen code. In this work, we are interested in Python\ncode generation. Code generation features a mis-\nmatch between an ambiguous and noisy natural\nlanguage input and the structured nature of the gen-\nerated code. Although Python’s vocabulary has a\nﬁnite number of keywords, the set of values that can\nbe assigned to a variable is inﬁnite and constitutes\none of the issues in predicting code corresponding\nto natural language.\nLike many other NLP tasks, current architectures\nfor natural language to code generally take advan-\ntage of pre-trained language models such as BERT\n(Devlin et al., 2019) or GPT (Brown et al., 2020)\nbased on the transformer architecture (Vaswani\net al., 2017). In particular, these architectures are\nused for code generation where parallel data is\nlimited due to the human expertise required for\nalignment. The best results on code generation are\nreached by pretraining seq2seq models on exter-\nnal sources, then by ﬁne-tuning those models on\nsmaller data sets. For instance, Orlanski and Git-\ntens (2021) ﬁne-tune BART (Lewis et al., 2020)\non data pairs of natural language and code and by\ntaking advantage of external informations. Simi-\nlarly, Norouzi et al. (2021) used BERT and a trans-\nformer decoder in a semi-supervised way by taking\nadvantage of a large amount of additional mono-\nlingual data. Another popular method is to train\nlarge language models on code (Austin et al., 2021;\nHendrycks et al., 2021). Notably, GPT-3 has been\nﬁnetuned on a large quantity of data from Github\nto obtain a powerful language model named Codex\n(Chen et al., 2021) that powers Github Copilot, a\ntool to help developers.\nOverall the above mentioned solutions aim to\ntake advantage of large amounts of training data\navailable nowadays, but few of them care about\ngenerating code that is guaranteed to be syntacti-\ncally correct nor well typed. Let us mention some\nexceptions from semantic parsing like Dong and\nLapata (2016); Rabinovich et al. (2017); Yin and\nNeubig (2017) that rely on grammatical constraints\nto ensure that the generated code can be executable.\nIn this work, we study variations around the\nTranX seq2seq architecture (Yin and Neubig, 2018)\nfor translating natural language to code. Rather\nthan generating directly code tokens from natural\nlanguage, the architecture generates an Abstract\nSyntax Tree (AST) constrained by the program-\nming language grammar.\nThe paper reports state of the art results on the\ntask and speciﬁcally introduces:\n• A formalization of the grammar constrained\ncode generator relying on the Earley (1970)\nparser transition system.\narXiv:2202.13972v2  [cs.CL]  16 Mar 2022\n• A study of the impact of key components of\nthe architecture on the performance of the sys-\ntem: we study the impact of the grammatical\ncomponent itself, the impact of the language\nmodel chosen, the impact of variable naming\nand typing and the impact of the input/output\ncopy mechanisms.\nIt is structured as follows. Section 2 formalizes the\nsymbolic transition system used for generating the\ngrammatically correct code, Section 3 describes a\nfamily of variants around the TranX architecture\nthat will be used to study the impact of these varia-\ntions in the experimental part of the paper (Section\n4).\n2\nA transition system for code generation\nAmong the models tested in the paper, some are\ngenerating syntactically constrained code. In the\ncontext of our study, we propose a transition model\nthat meets two objectives: the code generated is\ngrammatically valid in terms of syntax and the\nwhole translation process still reduces to a seq2seq\ntransduction mechanism that allows us to leverage\nstandard machine learning methods.\nTo this end we introduce a transition system for\ncode generation that generates an AST as a se-\nquence of actions. The derivations can then be\ntranslated into ASTs and in actual Python code\nby means of deterministic functions. The set of\nvalid ASTs is a set of trees that are generated by\nan ASDL grammar (Wang et al., 1997). An ASDL\ngrammar is essentially a context free grammar ab-\nstracting away from low level syntactic details of\nthe programming language and aims to ease the se-\nmantic interpretation of the parse trees. To this end\nASDL grammar rules come with additional deco-\nrators called constructors and ﬁeld names (Figure\n1).\nOur transition system generates derivations, or\nsequences of actions, that can be translated to a\nsyntactically correct Python code. We adapt to\ncode generation the transition system of the Ear-\nley parser (Earley, 1970) as formalized in Figure\n2. The generator state is a stack of dotted rules. A\ndotted rule is a rule of the form A →α•Xβ where\nα is a sequence of grammar symbols whose sub-\ntrees are already generated and Xβ is a sequence\nof grammar symbols for which the subtrees are yet\nto be generated. The •X symbol is the dotted sym-\nbol or the next symbol for which the system has to\ngenerate the subtree. The Python ASDL grammar\nincludes rules with star (∗) qualiﬁers allowing zero\nor more occurrences of the starred symbol. The\ntransition system uses an additional set of starred\nactions and a CLOSE action to stop these iterations\n(Figure 2).\nEach PREDICT(C) action starts the generation\nof a new subtree from its parent. The GENERATE\naction adds a new leaf to a tree. The COMPLETE ac-\ntion ﬁnishes the generation of a subtree and contin-\nues the generation process with its parent. The set\nof PREDICT actions is parametrized by the ASDL\nrule constructor (C), thus there are as many predict\nactions as there are constructors in the ASDL gram-\nmar. Constructors are required in order to generate\nthe actual ASTs from the derivations.\nGENERATE(V) actions are actions responsible\nfor generating the terminal or primitive sym-\nbols. The Python ASDL grammar generates ASTs\nwith primitive leaf types (identifier, int,\nstring, constant) that have to be ﬁlled with\nactual values for the AST to be useful. To generate\nactual primitive values the set of generate actions\nis also parametrized by the actual values V for the\nprimitive types. The set of such values is inﬁnite\nand consequently the set of generate actions is also\ninﬁnite.\nNon-Determinism comes from the use of PRE-\nDICT(C), GENERATE(V) and CLOSE rules. By con-\ntrast the application of the COMPLETE action is\nentirely deterministic: once the generator has a\ncompleted dotted rule on the top of its stack, it has\nno other choice than applying the complete rule.\nThe sequential generation process is illustrated\nin Figure 3. Given a start state, at each time step,\nthe generator has to decide which action to perform\naccording to the current state of the stack and up-\ndates the stack accordingly. Once the generator\nreaches the goal state, we collect the list of actions\nperformed (the derivation) in order to build the\nAST that we ﬁnally translate into actual Python\ncode1.\n3\nFactors inﬂuencing code prediction\nAll architectures analyzed in this study are varia-\ntions around a seq2seq architecture. We describe\nthe several variants of this architecture used in this\npaper both on the encoder and decoder side. We\nidentify key factors that have an impact on the\nnatural-language-to-code translation architecture\n1We use the astor library to this end.\nexpr\n=\nBinOp expr left, operator op, expr right\noperator\n=\nAdd\nexpr\n=\nConstant constant value\nexpr\n=\nList expr* elts\nFigure 1: Example of ASDL rules for the Python language. Each rule is built from a set of grammatical symbols\n(in blue), is uniquely identiﬁed by a constructor name (in red) and provides names to its right hand side symbols,\nits ﬁelds (in green). Grammatical symbols are split in nonterminals (like expr) and terminals or primitives (like\nconstant). Grammatical symbols can also be annotated with qualiﬁers (*) that allow for zero or more iterations\nof the symbol.\nAction\nTransition\nCondition\nSTART(C)\n⟨A →•α⟩\nGOAL\n⟨A →α•⟩\nPREDICT(C)\n⟨S|A →α • Bβ⟩\n⇒\n⟨S|A →α • Bβ|B →•γ⟩\n(B →γ ∈rules)\nGENERATE(V)\n⟨S|A →α • tβ⟩\n⇒\n⟨S|A →αt • β⟩\n(t ∈primitives)\nCOMPLETE\n⟨S|A →α • Bβ|B →γ•⟩\n⇒\n⟨S|A →αB • β⟩\nPREDICT∗(C)\n⟨S|A →α • B∗β⟩\n⇒\n⟨S|A →α • B∗β|B →•γ⟩\n(B →γ ∈rules)\nGENERATE∗(V)\n⟨S|A →α • t∗β⟩\n⇒\n⟨S|A →αt•t∗β⟩\n(t ∈primitives)\nCOMPLETE∗\n⟨S|A →α • B∗β|B →γ•⟩\n⇒\n⟨S|A →αB • B∗β⟩\nCLOSE∗\n⟨S|A →α • X∗β⟩\n⇒\n⟨S|A →α • β⟩\nFigure 2: An Earley inspired transition system for generating Abstract Syntactic Trees. The state of the generator\nis a stack of dotted rules whose bottom is S. As in the the Earley parser, the PREDICT rule starts the generation of\na new subtree by pushing a new dotted rule on the stack, the GENERATE rule adds a leaf to the tree by swapping\nthe top of the stack and the COMPLETE rule attaches a generated subtree into its parent by popping the top two\nelements of the stack and pushing an updated dotted rule. To handle * qualiﬁers we add the starred inference rules\nwhere COMPLETE∗and GENERATE∗implement an iteration that stops with the CLOSE∗rule.\nGenerator State (stack)\nAction\n⟨expr →•expr∗⟩\nSTART(List)\n⟨expr →•expr∗|expr →•expr operator expr⟩\nPREDICT∗(BinOp)\n⟨expr →•expr∗|expr →•expr operator expr|expr →•constant⟩\nPREDICT(Constant)\n⟨expr →•expr∗|expr →•expr operator expr|expr →constant•⟩\nGENERATE(7)\n⟨expr →•expr∗|expr →expr • operator expr⟩\nCOMPLETE\n⟨expr →•expr∗|expr →expr • operator expr|expr →•⟩\nPREDICT(Add)\n⟨expr →•expr∗|expr →expr operator • expr⟩\nCOMPLETE\n⟨expr →•expr∗|expr →expr operator • expr|expr →•constant⟩\nPREDICT(Constant)\n⟨expr →•expr∗|expr →expr operator • expr|expr →constant•⟩\nGENERATE(5)\n⟨expr →•expr∗|expr →expr operator expr•⟩\nCOMPLETE\n⟨expr →expr • expr∗⟩\nCOMPLETE∗\n⟨expr →expr • expr∗|expr →•constant⟩\nPREDICT∗(Constant)\n⟨expr →expr • expr∗|expr →constant•⟩\nGENERATE(4)\n⟨expr →expr expr • expr∗⟩\nCOMPLETE∗\n⟨expr →expr expr•⟩\nCLOSE∗\nexpr\n(List)\nexpr:elts\n(Constant)\nconstant:value\n4\nexpr:elts\n(BinOp)\nexpr:right\n(Constant)\nconstant:value\n5\noperator:op\n(Add)\nexpr:left\n(Constant)\nconstant:value\n7\nFigure 3: Example derivation for the generation of the Python list expression [7+5,4]. The derivation starts\nwith expr as axiom symbol and applies transitions until the goal is reached. The list of actions performed is called\nthe generator derivation. Given a generated derivation we can design a straightforward deterministic procedure to\ntranslate it into an AST. The actual Python code is generated from the AST by the astor library.\nand we formalize a family of models that allow to\ntest variations of these factors.\nWe consider a family of models generating\nPython code y from a natural language description\nx, that have the generic form:\np(y|x) =\nY\nt\np(yt|y<t, x)\n(1)\ny is either a sequence of code tokens in case we do\nnot use a grammar, or a sequence of actions from a\nderivation in case we use a grammar. The decoding\nobjective aims to ﬁnd the most-probable hypothe-\nsis among all candidate hypotheses by solving the\nfollowing optimization problem:\nˆy = argmax\ny\np(y|x)\n(2)\nThe family of models varies according to four\nkey qualitative factors that we identify in the TranX\narchitecture. First we describe a substitution proce-\ndure managing variables and lists names in section\n3.1). Second, in section 3.2, we test the architec-\ntural variations for encoding the natural language\nsequence. Third, in section 3.3, we describe vari-\nations related to constraining the generated code\nwith grammatical constraints and architectural vari-\nations that allow to copy symbols from the natural\nlanguage input to the generated code.\n3.1\nSubstitution\nProgramming languages come with a wide range of\nvariable names and constant identiﬁers that make\nthe set of lexical symbols inﬁnite. Rather than\nlearning statistics on a set of ad-hoc symbols, we\nrather normalize variable and constant names with\na pre-processing method, reusing the method of\nYin and Neubig (2018).\nPreprocessing amounts to substitute the actual\nnames of the variables with a normalized set of pre-\ndeﬁned names known to the statistical model. The\nsubstitution step renames all variables both in the\nnatural language and in the code with conventional\nnames such as var_0, var_1, etc. for variables\nand lst_0,lst_1, etc. for lists. A post process-\ning step substitutes back the predicted names with\nthe original variable names in the system output.\nFor example, given the natural language intent:\ncreate list `done` containing permuta-\ntions of each element in list `[a, b,\nc, d]` with variable `x` as tuples\nis transformed into:\ncreate list var_0 containing permuta-\ntions of each element in list lst_0 with\nvariable var_1 as tuples\nThe predicted code such as var_0 = [(el,\nvar_1) for el in [lst_0]]\nis\ntrans-\nformed back into done = [(el, x) for\nel in [a, b, c, d]].\nModels using variable replacement as illustrated\nabove, are identiﬁed with the notation SUBSTITU-\nTION = TRUE in section 4. Implementing this\nheuristic is made easy by the design of the CoNaLa\ndata set where all such names are explicitly quoted\nin the data while for Django we had to detect vari-\nable names by comparing natural language with its\ncorresponding code.\n3.2\nEncoder\nWe switched between a classic bi-LSTM and a\npretrained BERTBASE to encode the input natural\nlanguage {xi, i ∈J1, nK} of n words into a vecto-\nrial representations {h(enc)\ni\n, i ∈J1, nK} which are\nlater used to compute the attention mechanism.\nWe set the BERT factor to TRUE when using it and\nFALSE when using the bi-LSTM.\n3.3\nDecoder\nAt each time step t, the LSTM decoder computes\nits internal hidden state h(dec)\nt\n:\nh(dec)\nt\n= LSTM([et−1 : eat−1], h(dec)\nt−1 )\n(3)\nwhere et−1 is the embedding from the previous\nprediction, eat−1 is the attentional vector.\nWe compute the attentional vector eat as in Lu-\nong et al. (2015) combining the weighted average\nover all the source hidden state ct and the decoder\nhidden state h(dec)\nt\n:\neat = Wa[ct : h(dec)\nt\n]\n(4)\nIt is the attention vector eat which is the key to\ndetermine the next prediction yt.\nWe use several variants of the code generator,\nthat we describe by order of increasing complexity.\nThe basic generator is a feed forward that uses the\nattention vector to generate a code token v from a\nvocabulary V :\np(yt = GENERATE[v]|x, e<t) =\nsoftmax(e⊤\nv · Wg · eat)\n(5)\nFigure 4: Illustration of the seq2seq model with the variables SUBSTITUTION, GRAMMAR, BERT, POINTERNET\nset to TRUE. We describe here the complete process where we predict a derivation sequence composed of grammar\nrules and CLOSE (PREDRULE) or Python variables/built-in (GENERATE). The astor library is used to transform the\nAST constructed with the derivation sequence into Pyton code. In the case where GRAMMAR = FALSE, we only\nhave the GENERATE action which exclusively predicts unconstrained code tokens (as for a classical seq2seq).\nThese models are not constrained by the Python\ngrammar and we identify these models with GRAM-\nMAR = FALSE.\nWe also use a pointer network that may either\ncopy symbols from input to output or generate sym-\nbols from V . Then the probability of generating\nthe symbol v is given by the marginal probability:\np(yt = GENERATE[v]|x, e<t) =\np(gen|x, e<t)p(v|gen, x, e<t)\n+p(copy|x, e<t)p(v|copy, x, e<t)\n(6)\nThe probabilities p(gen|.) and p(copy|.) sum to\n1 and are computed with softmax(W · eat). The\nprobability of generating v from the vocabulary\nV p(v|gen, .) is deﬁned in the same way as (5).\nWe use the pointer net architecture (Vinyals et al.,\n2015) to compute the probability p(v|copy, .) of\ncopying an element from the natural language x.\nModels that use a pointer network are identiﬁed\nwith PN = TRUE, otherwise with PN = FALSE .\nFinally we use a set of models that are con-\nstrained by the Python grammar and that rely on\nthe transition system from section 2. Rather than\ndirectly generating Python code, these models gen-\nerate a derivation whose actions are predicted using\ntwo prediction tasks.\nWhen the generator is in a state where the dot of the\nitem on the top of the stack points on a nonterminal\nsymbol, the PREDRULE is used. This task either\noutputs a PREDICT(C) action or the CLOSE action:\np(yt = PREDRULE[c]|x, e<t) =\nsoftmax(e⊤\nr · Wp · eat)\n(7)\nWhen the generator is in a state where the dot of\nthe item on the top of the stack points on a terminal\nsymbol, the generate task is used. This amounts to\nreuse either equation (5) or equation (6) according\nto the model at hand. Models constrained by the\ngrammar are labelled with GRAMMAR = TRUE.\nRecall that the COMPLETE action of the transition\nsystem is called deterministically (Section 2).\n4\nExperiments\nIn this section we describe the characteristics of the\ndata sets on which we have tested our different se-\ntups and the underlying experimental parameters2.\n4.1\nData sets\nIn this study we use two available data sets, Django\nand CoNaLa, to perform our code generation task.\nThe Django data set provides line-by-line com-\nments with code from the Django web framework.\n2The code of our experiments is public and available at\nhttps://gitlab.com/codegenfact/BertranX\nAbout 70% of the 18805 examples are simple\nPython operation ranging from function declara-\ntions to package imports, and including excep-\ntion handling. Those examples strongly share the\nnatural language structure (e.g. call the function\ncache.close →cache.close()). More than\n26% of the words in the natural language are also\npresent in the code, BLEU score between the natu-\nral language and code is equal to 19.4.\nCoNaLa is made up of 600k NL-code pairs from\nStackOverflow, among which 2879 examples\nhave been been manually cleaned up by developers.\nAll results are reported on the manually curated\nexamples, unless stated otherwise. The natural lan-\nguage descriptions are actual developer queries (e.g.\nDelete an element 0 from a dictionary ‘a‘) and the\nassociated code is diverse and idiomatic (e.g. {i:\na[i] for i in a if (i != 0)}). Com-\npared to Django, the code is much more challeng-\ning to generate. Especially because the number of\nwords shared between the NL and the code is much\nlower (BLEU = 0.32). Also, the code is longer and\nmore complex with an AST depth of 7.1 on average\nagainst 5.1 for Django.\n4.2\nVocabulary generation\nThe vocabulary of natural language and code is\nessential. Usually, this vocabulary is created by\nadding all the words present in the training data set.\nThere are however exceptions that are detailed in\nthis section.\nThe natural language vocabulary relies on a byte\npair encoding tokenizer when BERT = TRUE. As\nexplained in section 3.1, the variable names are\nreplaced with special tokens var_i and lst_i.\nThese new tokens are crucial to our problem, and\nadded to the BERT vocabulary . We can then ﬁne-\ntune BERT with this augmented vocabulary on our\ndata sets.\nFor the decoder part, when GRAMMAR = TRUE,\nthe vocabulary of grammatical actions is ﬁxed,\nwhile the vocabulary of AST leaves has to be built.\nThis associated vocabulary can be composed of\nbuilt-in Python functions, libraries with their asso-\nciated functions or variable names. Its creation is\nconsequently a major milestone in the generation\nprocess.\nTo create this external vocabulary, we proceed as\nin TranX. From the code, we create the derivation\nsequence composed of the action of the grammar\nas well as the primitives. All primitives of the\naction sequences are incorporated into our external\nvocabulary.\n4.3\nSetup\nWhen BERT = FALSE, the size of the representa-\ntions is kept small to prevent overﬁtting. Encoder\nand decoder embedding size is set to 128. The hid-\nden layer size of the encoder and decoder bi-LSTM\nis set to 256 and the resulting attention vector size\nis 300. We have two dropout layers: for embed-\ndings and at the output of the attention. We use\nAdam optimizer with learning rate α = 5.10−3.\nWhen BERT = TRUE, encoder embeddings have\na natural size of 756 with BERT. We therefore\napply a linear transformation to its output to get an\nembedding size equal to 512. The size of LSTM\ndecoder hidden state and attention vector are set to\n512. We regularize only the attentional vector in\nthat case. We use Adam optimizer with learning\nrate α = 5.10−5. In both cases, we use a beam\nsearch size of 15 for decoding.\nEvaluation\nTo compare with previous work, we\nreport the standard evaluation metrics for each data\nset: exact match accuracy and corpus-level BLEU.\nPython\nversion\nAs\nthe\ngrammar\nslightly\nchanges between Python versions, let us mention\nthat all our experiments have been carried out with\nPython 3.7.\n4.4\nAblation study\nFigure 5: Difference between the marginal mean of\neach variable for the TRUE and FALSE conditions.\nTo highlight the contribution of the different fac-\ntors, SUBSTITUTION, BERT, GRAMMAR, PN on the\nDjango and CoNaLa data sets we report a detailed\nstudy of their impact in Table 1.\nSubstitution\nBERT\nGrammar\nPN\nCoNaLa BLEU\nCoNaLa accuracy\nDjango BLEU\nDjango accuracy\nFalse\nFalse\nFalse\nFalse\n21.05 ± 0.81\n0.9 ± 0.42\n42.58 ± 1.54\n26.86 ± 1.15\nTrue\n22.33 ± 0.78\n1.7 ± 0.90\n64.79 ± 1.00\n62.85 ± 1.21\nTrue\nFalse\n20.59 ± 0.74\n2.87 ± 0.48\n43.23 ± 1.62\n30.12 ± 0.63\nTrue\n22.16 ± 1.93\n3.87 ± 1.65\n62.55 ± 1.60\n65.20 ± 0.03\nTrue\nFalse\nFalse\n30.83 ± 4.08\n2 ± 0.94\n53.18 ± 0.87\n30.28 ± 0.26\nTrue\n30.98 ± 1.33\n3.3 ± 1.48\n58.69 ± 1.28\n37.96 ± 0.27\nTrue\nFalse\n25.88 ± 0.94\n3.8 ± 1.96\n47.32 ± 0.50\n29.62 ± 0.33\nFalse\nTrue\n28.43 ± 0.64\n4.4 ± 1.67\n52.55 ± 0.51\n37.38 ± 0.38\nFalse\nFalse\n31.17 ± 0.88\n3.1 ± 1.52\n70.4 ± 0.25\n70.40 ± 0.29\nTrue\nTrue\n32.10 ± 1.06\n3.1 ± 1.24\n70.28 ± 0.38\n70.46 ± 0.37\nTrue\nFalse\n33.36 ± 1.63\n6.37 ± 0.63\n70.82 ± 0.22\n71.3 ± 0.19\nTrue\n32.86 ± 1.75\n5 ± 1.67\n70.62 ± 0.49\n71.47 ± 0.19\nTrue\nFalse\nFalse\n36.43 ± 0.41\n4.5 ± 1.84\n76.97 ± 0.15\n74.58 ± 0.27\nTrue\n36.29 ± 2.27\n5 ± 1.32\n76.62 ± 0.50\n76 ± 0.71\n35.42 ± 1.75∗\n5.2 ± 1.33∗\n-\n-\nTrue\nFalse\n35.04 ± 1.03\n7.3 ± 1.25\n76.20 ± 0.46\n74.88 ± 0.56\nTrue\n37.99 ± 1.85\n7.5 ± 1.12\n76.32 ± 0.59\n75.32 ± 1.54\n39.01 ± 1.08∗\n7.7 ± 1.92∗\n-\n-\nTable 1: Performances with different natural language encoders on the development sets with and without a gram-\nmatical component. The scores reported are the mean and standard deviation resulting from training with 5 differ-\nent seeds. The * refers to the use of 100k CoNaLa mined data in addition to clean examples.\nThe results are analyzed by distinguishing lex-\nical and grammatical aspects and by identifying\nrelations between the different factors. We start by\na comparison of the marginal mean of the BLEU\nscore for each of our variables in both conditions.\nFigure 5 highlights the mean difference between\nthe conditions by contrasting the case where the\nvalue is TRUE with the case where the value is\nFALSE.\nPointer network\nThe pointer network can im-\nprove the results, especially when SUBSTITUTION\n= FALSE. This is because the only way to obtain\nthe name of the variables is to copy them. Com-\nbined with substitution, the pointer network of-\nfers an additional possibility to predict the var_i,\nlst_i which allows to achieve the best results\nwith a BLEU score of 39.01 on CoNaLa and an\nexact match accuracy of 76 on Django.\nSubstitution and Typing\nThe scores are sta-\nbilised and much higher with substitution. We gain\nmore than 9 points of BLEU on CoNaLa (respec-\ntively 20 points on Django) thanks to substitution.\nThe \"weakest\" conﬁguration where all variables\nare FALSE except the substitution gives better re-\nsults than all conﬁgurations where SUBSTITUTION\n= FALSE.\nThe increase in BLEU with substitution can be ex-\nplained in two ways. On the one hand, we remark\nthat the model has difﬁculties to memorize the val-\nues to ﬁll the lists with GENERATE. For example,\nfour tokens of code must be generated to predict\nthe list [a, b, c, d]. Using substitution, the\nmodel can just predict lst_0 which will be re-\nplaced by [a, b, c, d] during postprocessing.\nThis avoids a potential error in the creation of the\nlist and directly gives a valid 4-gram. This con-\ntributes to greatly increase the BLEU, which shows\nthe importance of replacing lists.\nOn CoNaLa,\nBLEU score on the development set drops from an\naverage of 37.99 to an average of 30.66 without list\nreplacement. Besides list replacement, the architec-\nture has also a weakness with respect to variable\ntyping. When using the grammar without substi-\ntution, the results are lower than without grammar.\nThis effect is the result of a type checking failure.\nThe model predicts ill-typed AST structures. For\ninstance it predicts an AST whose corresponding\ncode should be 1.append([6,7]). However\nthe AST library we used prevents from generating\nsuch ill-typed code. The absence of code genera-\ntion in such cases explain the decrease in BLEU\nscore.\nThe use of substitution partially corrects for\nthese typing errors because the substituted sym-\nbols var_i, lst_i are generally more likely to\nbe predicted and are likely to have the right type\nthanks to the mapping.\nGrammatical aspect The transition system\ndoesn’t improve the results on average because\nSystem\nCoNaLa BLEU\nCoNaLa accuracy\nDjango BLEU\nDjango accuracy\n(Yin and Neubig, 2018)\n27.2\n-\n-\n73.7\n(Yin and Neubig, 2018) + mined\n28.1\n-\n-\n-\n(Orlanski and Gittens, 2021) + mined 100k\n30.55\n-\n-\n-\n(Norouzi et al., 2021) + 600k mined\n32.57\n-\n-\n81.03\nOurs BERT + GRAMMAR\n31.6\n4.5\n79.86\n79.77\nOurs BERT + GRAMMAR + 100k mined\n34.20\n5.8\n-\n-\nOurs BERT (tokens)\n30.73\n1.40\n79.81\n79.61\nOurs BERT + 100k mined (tokens)\n32.39\n3.4\n-\n-\nTable 2: Comparisons of the systems trained without external data sources on CoNaLa and Django test sets.\nof the empty predictions when SUBSTITUTION =\nFALSE. The use of the transition system leads to\nbetter results when SUBSTITUTION = TRUE but not\nas drastically as one would have expected. How-\never the real contribution of the grammar associated\nwith substitution is the syntactic validity of the code\nin 100% of the cases, as tested with our architec-\nture obtaining the best results. In scenarios where\nwe do not use the grammar, it is never the case to\nhave an empty output. But then the proportion of\ncode sequences that are actually syntactically valid\nin this setup is 92% on average.\nBERT\nAs expected when using BERT to encode\nthe natural language input we get an improvement\nof about 6 marginal BLEU on CoNaLa (respec-\ntively +3 BLEU on Django). More interestingly,\nthis effect is lower than the one of the substitution\noperation.\nWe conclude that the use of a pre-trained model\nincreases the results but less than substitution, de-\nspite what one might think and it suggests that im-\nproving the management of variable names and\nlists is one of the key elements for improving\nthe system. The contribution of grammatical con-\nstraints in BLEU may seem detrimental but we\ncould see that this is a side effect of typing con-\nstraints in adversarial scenarios. Overall the non-\nconstrained generated code is syntactically incor-\nrect in 8% of the cases.\n4.5\nTest\nWe compare in table 2 our results with other sys-\ntems on CoNaLa and Django test sets. We report\nour best performing models on the development set\nwith and without grammatical constraints. We also\nuse models trained on the full CoNaLa including\nmined examples to get relevant comparisons.\nAmong the other systems Yin and Neubig (2018)\nis the only one that uses grammatical constraints.\nOur architecture differs with the use of a BERT\nencoder whereas Yin and Neubig (2018) use an\nLSTM. The other systems do not use grammati-\ncal constraints but rather try to take advantage of\nadditional data. Orlanski and Gittens (2021) and\nNorouzi et al. (2021) aim to take advantage of the\nCoNaLa mined examples. As these mined exam-\nples are noisy, Orlanski and Gittens (2021) takes\nadvantage of BART (Lewis et al., 2020), a denois-\ning encoder. They also enrich the natural language\ninput with the results of queries from StackOver-\nﬂow by adding the title of the post, its associated\ntags, etc. Norouzi et al. (2021) use BERT as en-\ncoder and a transformer decoder. They apply the\nTarget Autoencoding method introduced by Currey\net al. (2017). During training, the encoder parame-\nters are frozen and the decoder is trained to recon-\nstruct code examples. They use this method on the\nmined examples to take maximal advantage of the\nadditional noisy data.\nWe observe that our grammar based model with\nBERT encoder is state of the art on CoNaLa while\nthe transformer encoder/decoder architecture of\nNorouzi et al. (2021) performs best on Django.\nQuite interestingly the exact match accurracy of\nthese models remain weak on CoNaLa.\n5\nConclusion\nWe formalized a transition system that allows us\nto guarantee the generation of syntactically cor-\nrect code. A detailed study of the components of\nthe seq2seq architecture reveals that the models\nhave difﬁculties at managing accurately variable\nnames and list encodings. The comparison with\nmodels trained on larger noisy data sets reveals that\nour grammatically constrained architecture with-\nout explicit denoising remains competitive. This\nfurther highlights the importance of grammatical\nconstraints and of speciﬁc processes dedicated to\nmanage variables, list naming and typing.\nFinally, we observe that BLEU and exact match,\nused in this paper, although commonly used in the\nliterature, are not ideal metrics especially because\nhigh BLEU scores do not guarantee that the code\nwill be executable. Even exact match is not sati-\nfactory since a single natural language query can\nbe solved by several python programs. In future\nwork, we plan to build extensions to the datasets\nused here with additional test cases assessing the\ncorrection of the generated code. These tests are\nlikely to support more relevant metrics for code\ngeneration evaluation.\nReferences\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and\nCharles Sutton. 2021. Program synthesis with large\nlanguage models. CoRR, abs/2108.07732.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell,\nSandhini Agarwal,\nAriel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harrison Edwards, Yuri Burda, Nicholas\nJoseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott\nGray, Nick Ryder, Mikhail Pavlov, Alethea Power,\nLukasz Kaiser, Mohammad Bavarian, Clemens Win-\nter, Philippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021. Eval-\nuating large language models trained on code. vol-\nume abs/2107.03374.\nAnna Currey, Antonio Valerio Miceli Barone, and Ken-\nneth Heaﬁeld. 2017. Copied monolingual data im-\nproves low-resource neural machine translation. In\nProceedings of the Second Conference on Machine\nTranslation, WMT 2017, Copenhagen, Denmark,\nSeptember 7-8, 2017, pages 148–156. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. pages 4171–4186.\nLi Dong and Mirella Lapata. 2016. Language to log-\nical form with neural attention.\nIn Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics, ACL 2016, August 7-12,\n2016, Berlin, Germany, Volume 1: Long Papers. The\nAssociation for Computer Linguistics.\nJay Earley. 1970. An efﬁcient context-free parsing al-\ngorithm. Commun. ACM, 13(2):94–102.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, and Jacob\nSteinhardt. 2021. Measuring coding challenge com-\npetence with APPS. CoRR, abs/2105.09938.\nMike\nLewis,\nYinhan\nLiu,\nNaman\nGoyal,\nMar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020.\nBART: denoising sequence-to-sequence\npre-training for natural language generation, trans-\nlation, and comprehension.\nIn Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July\n5-10, 2020, pages 7871–7880. Association for Com-\nputational Linguistics.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. pages 1412–1421.\nSajad Norouzi, Keyi Tang, and Yanshuai Cao. 2021.\nCode generation from natural language with less\nprior knowledge and more monolingual data.\nIn\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume\n2: Short Papers), Virtual Event, August 1-6, 2021,\npages 776–785. Association for Computational Lin-\nguistics.\nGabriel Orlanski and Alex Gittens. 2021.\nRead-\ning stackoverﬂow encourages cheating:\nAdding\nquestion text improves extractive code generation.\nCoRR, abs/2106.04447.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017.\nAbstract syntax networks for code gener-\nation and semantic parsing.\nIn Proceedings of\nthe 55th Annual Meeting of the Association for\nComputational Linguistics, ACL 2017, Vancouver,\nCanada, July 30 - August 4, Volume 1: Long Papers,\npages 1139–1149. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. pages 5998–6008.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. pages 2692–2700.\nDaniel C. Wang, Andrew W. Appel, Jeffrey L. Korn,\nand Christopher S. Serra. 1997.\nThe zephyr ab-\nstract syntax description language. In Proceedings\nof the Conference on Domain-Speciﬁc Languages,\nOctober 15-17, 1997, Santa Barbara, California,\nUSA, pages 213–228.\nPengcheng Yin and Graham Neubig. 2017. A syntac-\ntic neural model for general-purpose code genera-\ntion.\nIn Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics,\nACL 2017, Vancouver, Canada, July 30 - August 4,\nVolume 1: Long Papers, pages 440–450. Associa-\ntion for Computational Linguistics.\nPengcheng Yin and Graham Neubig. 2018. TRANX:\nA transition-based neural abstract syntax parser\nfor semantic parsing and code generation.\nIn\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 7–12, Brussels, Belgium. As-\nsociation for Computational Linguistics.\nA\nAdditional Qualitative Examples\nWe present examples of code generated by our best\nmodels with and without grammar.\nSource\ndeclare an array\nGold\nmy_list = []\nGrammar\nx = [0] * 2\nWithout\n[(0) for _ in range\n(10000)]\nRemark\nSource is not precise enough.\nSource\nincrement piece by ﬁrst element of\nelt\nGold\npiece += elt[0]\nGrammar\npiece += elt[1]\nWithout\npiece += elt[1]\nRemark\nFirst element of a list is zero.\nSource\nremove ﬁrst element of text\nGold\ntext = text[1:]\nGrammar\ntext = text[1:]\nWithout\ntext[1:\nRemark\nSyntax mistake for the code with-\nout grammar.\nSource\nget the position of item 1 in\n‘testlist‘\nGold\n[i for i, x in\nenumerate(testlist)\nif x == 1]\nGrammar\n[i for i, v in\nenumerate(testlist)\nif v == 1]\nWithout\ntestlist = [i for i in\ntestlist if i != 1]\nRemark\nGrammar output is not equal to\nGold due to dummy variable.\nSource\nappend a numpy array ‘b‘ to a\nnumpy array ‘a‘\nGold\nnp.vstack((a, b))\nGrammar\na = numpy.array([b,\na])\nWithout\nz = np.array([b]).\nreshape((3, 3))\nRemark\nGold is not accurate with np unde-\nﬁned before. vstack function not\nin the external vocabulary.\nSource\nactivate is a lambda function which\nreturns None for any argument x.\nGold\nactivate = lambda x :\nNone\nGrammar\nactivate = lambda x =\nNone :\nx\nWithout\nactivate = lambda x :\nNone\nRemark\nGood BLEU for grammar output\nwhile the result is not adequate.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-02-28",
  "updated": "2022-03-16"
}