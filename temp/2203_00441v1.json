{
  "id": "http://arxiv.org/abs/2203.00441v1",
  "title": "Bridge the Gap between Supervised and Unsupervised Learning for Fine-Grained Classification",
  "authors": [
    "Jiabao Wang",
    "Yang Li",
    "Xiu-Shen Wei",
    "Hang Li",
    "Zhuang Miao",
    "Rui Zhang"
  ],
  "abstract": "Unsupervised learning technology has caught up with or even surpassed\nsupervised learning technology in general object classification (GOC) and\nperson re-identification (re-ID). However, it is found that the unsupervised\nlearning of fine-grained visual classification (FGVC) is more challenging than\nGOC and person re-ID. In order to bridge the gap between unsupervised and\nsupervised learning for FGVC, we investigate the essential factors (including\nfeature extraction, clustering, and contrastive learning) for the performance\ngap between supervised and unsupervised FGVC. Furthermore, we propose a simple,\neffective, and practical method, termed as UFCL, to alleviate the gap. Three\nkey issues are concerned and improved: First, we introduce a robust and\npowerful backbone, ResNet50-IBN, which has an ability of domain adaptation when\nwe transfer ImageNet pre-trained models to FGVC tasks. Next, we propose to\nintroduce HDBSCAN instead of DBSCAN to do clustering, which can generate better\nclusters for adjacent categories with fewer hyper-parameters. Finally, we\npropose a weighted feature agent and its updating mechanism to do contrastive\nlearning by using the pseudo labels with inevitable noise, which can improve\nthe optimization process of learning the parameters of the network. The\neffectiveness of our UFCL is verified on CUB-200-2011, Oxford-Flowers,\nOxford-Pets, Stanford-Dogs, Stanford-Cars and FGVC-Aircraft datasets. Under the\nunsupervised FGVC setting, we achieve state-of-the-art results, and analyze the\nkey factors and the important parameters to provide a practical guidance.",
  "text": "SUBMITTED TO IEEE TIP\n1\nBridge the Gap between Supervised and\nUnsupervised Learning for Fine-Grained\nClassiﬁcation\nJiabao Wang, Yang Li, Xiu-Shen Wei, Hang Li, Zhuang Miao, and Rui Zhang\nAbstract—Unsupervised learning technology has caught up\nwith or even surpassed supervised learning technology in general\nobject classiﬁcation (GOC) and person re-identiﬁcation (re-ID).\nHowever, it is found that the unsupervised learning of ﬁne-\ngrained visual classiﬁcation (FGVC) is more challenging than\nGOC and person re-ID. In order to bridge the gap between\nunsupervised and supervised learning for FGVC, we investigate\nthe essential factors (including feature extraction, clustering, and\ncontrastive learning) for the performance gap between supervised\nand unsupervised FGVC. Furthermore, we propose a simple,\neffective, and practical method, termed as UFCL, to alleviate\nthe gap. Three key issues are concerned and improved: First,\nwe introduce a robust and powerful backbone, ResNet50-IBN,\nwhich has an ability of domain adaptation when we transfer\nImageNet pre-trained models to FGVC tasks. Next, we propose to\nintroduce HDBSCAN instead of DBSCAN to do clustering, which\ncan generate better clusters for adjacent categories with fewer\nhyper-parameters. Finally, we propose a weighted feature agent\nand its updating mechanism to do contrastive learning by using\nthe pseudo labels with inevitable noise, which can improve the\noptimization process of learning the parameters of the network.\nThe effectiveness of our UFCL is veriﬁed on CUB-200-2011,\nOxford-Flowers, Oxford-Pets, Stanford-Dogs, Stanford-Cars and\nFGVC-Aircraft datasets. Under the unsupervised FGVC setting,\nwe achieve state-of-the-art results, and analyze the key factors\nand the important parameters to provide a practical guidance.\nIndex Terms—Unsupervised learning, image classiﬁcation, ﬁne-\ngrained classiﬁcation, clustering, density clustering.\nI. INTRODUCTION\nF\nIne-grained visual classiﬁcation (FGVC) [1] is a long-\nstanding problem in the ﬁeld of computer vision, aiming\nto classify hundreds of subordinate categories that are under the\nsame basic-level category, e.g., different species of birds [2],\nmodels of cars [3], and aircrafts [4]. It is a more challenging\n• This work has been supported by the Natural Science Foundation of\nJiangsu Province (No. BK20200581, BK20210340), and in part by National\nKey R&D Program of China (2021YFA1001100), the National Natural Science\nFoundation of China under Grant 61806220, the China Postdoctoral Science\nFoundation under Grant 2020M683754 and 2021T140799, CAAI-Huawei\nMindSpore Open Fund, and Beijing Academy of Artiﬁcial Intelligence (BAAI).\n• Jiabao Wang, Yang Li, Hang Li, Zhuang Miao and Rui Zhang are with\nArmy Engineering University of PLA, Nanjing 210007, China. (E-mails:\njiabao 1108@163.com; solarleeon@outlook.com; lihang0003@outlook.com;\nemiao beyond@163.com; 3959966@qq.com). Xiu-Shen Wei is with PCA\nLab, Key Lab of Intelligent Perception and Systems for High-Dimensional\nInformation of Ministry of Education, and Jiangsu Key Lab of Image\nand Video Understanding for Social Security, School of Computer Science\nand Engineering, Nanjing University of Science and Technology, Nanjing\n210094, China. Xiu-Shen Wei is also with State Key Lab. for Novel\nSoftware Technology, Nanjing University, Nanjing 210023, China. (E-mail:\nweixs@njust.edu.cn)\n• Corresponding author: Yang Li\nCifar10\nSTL10\n(a) GOC\n0\n25\n50\n75\n100\nACC (%)\nSupervised\nUnsupervised\nMarket1501\nDukeMTMCReID\n(b) Person Re-ID\n0\n25\n50\n75\n100\nRank-1 (%)\nSupervised\nUnsupervised\nCUB-200-2011\nFGVC-Aircraft\n(c) FGVC\n0\n25\n50\n75\n100\nTop-1 (%)\nSupervised\nUnsupervised\nFigure 1.\n(a) General object classiﬁcation (GOC) and (b) Person re-\nidentiﬁcation (re-ID), (c) Fine-grained visual classiﬁcation (FGVC). In the\nﬁrst row, circles, squares and triangles respectively represent three types of\nexamples, and ellipse represents the distribution of examples from different\ncategories, where FGVC has small difference between sub-categories and large\nvariance within sub-categories. In the second row, CIFAR-10 and STL-10\nresults are from SCAN [5], Market-1501 and DukeMTMC-ReID results are\nfrom ICE [6], CUB-200-2011 and FGVC-Aircraft results are from reproduced\nGroup-Sampling [7] and PMG [8].\nproblem than general object classiﬁcation (GOC) due to the\ninherently subtle inter-class object variations amongst sub-\ncategories (as show in the ﬁrst row of Fig. 1). Besides, it is\nextremely hard even for human beings to recognize hundreds\nof sub-categories.\nEarly works on FGVC mostly attempt to ﬁnd discriminative\nregions with the assistance of manual annotations. For example,\nBranson et al. [9] proposed to obtain the local parts by using\ngroups of key-points to compute multiple warped image regions.\nZhang et al. [10] trained a R-CNN model based on ground\ntruth part annotations, and then performed part detection to\nget discriminative regions. Besides, segmentation models, such\nas PS-CNN [11] and Mask-CNN [12], were also employed\nto locate part regions. However, human annotations based on\nobject parts are difﬁcult to obtain, and can often be error-prone\nresulting in performance degradations.\nTo alleviate this burden, weakly-supervised FGVC tried to\ntraining models with only image-level labels [13], [14], [15],\n[16], [17], [18], [19], [8]. These models are able to locate\nmore discriminative local regions for classiﬁcation with less\nlabel efforts. However, label annotation of training data is\nalso required. What happens if there is no access to ground-\ntruth semantic labels during FGVC training? Can they achieve\nor reach the performance of supervised learning methods?\nUnfortunately, the answer is no. As illustrated in the second row\nof Fig. 1, the unsupervised and supervised methods have the\narXiv:2203.00441v1  [cs.CV]  1 Mar 2022\nSUBMITTED TO IEEE TIP\n2\nsimilar performance for GOC and person re-identiﬁcation (re-\nID), but there is a huge gap between the unsupervised method\nand supervised learning methods for FGVC. This performance\ngap is also an undesirable consequence of slowing down the\nresearch progress in the ﬁeld of unsupervised FGVC.\nBecause of the recent success of self-supervised learning\nmethods in GOC [20], [21], [22], [23], [24] and person re-\nID [25], [26], [6], [7], [27], [28], one natural question arises:\nWhat are the essential factors for the performance gap between\nunsupervised and supervised methods for FGVC? Ideally,\nif we know the key factors of the aforementioned question\nand encourage a model to be robust to these factors during\nunsupervised learning, then we might design a more appropriate\nunsupervised FGVC method compared with the state-of-the-art\nsupervised FGVC methods, which can bridge the gap between\nunsupervised and supervised learning for FGVC. In addition,\nwe argue that studying this topic may also unleash the potentials\nof unsupervised FGVC.\nTo this end, in this paper, we investigate the gap difference\nbetween FGVC, GOC, and person re-ID. We argue that FGVC\nis more challenging due to it contains different identities with\nlarger intra-class variances. Moreover, the performance gap\nbetween unsupervised and supervised FGVC is mainly affected\nby three main factors, including feature extraction, clustering,\nand contrastive learning, which named as Instability Gap.\nFurthermore, we propose a simple, effective, and practical\nmethod, termed as Unsupervised Fine-grained Clustering\nLearning (UFCL), to alleviate the Instability Gap. UFCL\ninvolves robust feature extraction module, stable hierarchical\nclustering module and cluster-level contrastive learning module,\nso that it converges to the optimal results by the end of the\ntraining process. As shown in Fig. 2, our method involves three\nsteps: extracting features via a backbone network, generating\npseudo labels by a clustering algorithm, and updating network\nparameters by contrastive learning. Then, the network is\nupgraded at the end of each epoch. The aforementioned\nframework brings three key issues, and we provide solutions\naccordingly. First, since the initial parameters of feature\nextraction backbone network are usually trained on large-\nscale general datasets, such as ImageNet and JFT300M, which\nhas a big domain gap with unsupervised FGVC datasets, the\nbackbone should have robust domain adaptability. As a result,\nwe introduce a robust and powerful backbone, ResNet50-\nIBN [29], which can achieve comparable improvements as\ndomain adaptation methods when applying it to new domains.\nSecond, as the network is upgrade with pseudo labels, it is\nnecessary to ensure that the false pseudo labels are as few as\npossible. Hence, we propose to adopt the HDBSCAN [30],\n[31] algorithm to do clustering, which can generate better\nclusters for adjacent categories with fewer hyper-parameters\nthan the widely used DBSCAN algorithm. Third, optimizing\na deep network by pseudo labels with inevitable noise may\nalso cause unstable gradients, which degenerates the learning\nability of unsupervised FGVC. Consequently, we introduce\ncluster-level contrastive leaning and propose a new weighted\nupdating strategy to compute a feature agent for each category\nin a batch and update the parameters with the robust gradients\nof feature agents.\nThe effectiveness of our UFCL method is veriﬁed on six\nunsupervised FGVC tasks on the standard vision setting, we\nachieve the state-of-the-art performance with 69.0%, 90.1%,\n79.0%, 58.5%, 33.7%, and 43.3% classiﬁcation Top-1 accuracy\non CUB-200-2011, Oxford-Flowers, Oxford-Pets, Stanford-\nDogs, Stanford-Cars, and FGVC-Aircraft, respectively. In ad-\ndition, we demonstrate the beneﬁts of robust feature extraction\nmodule, stable hierarchical clustering module and cluster-level\ncontrastive learning module on CUB-200-2011 and Oxford-\nFlowers datasets. Comparing with the widely used ResNet50\nbackbone, ResNet50-IBN has 24.1% and 1.9% improvements\non Top-1 accuracy. HDBSCAN increases the Top-1 accuracy\nof 3.7% and 0.5% compared with DBSCAN [32], and the best\nweighted updating strategy can improve the Top-1 accuracy\nover 22.0% and 11.0% comparing with Cluster-Contrast [28].\nThe contributions of this paper are concluded as follows:\n• We investigate the essential factors (i.e., feature extraction,\nclustering, and contrastive learning) for the performance\ngap between unsupervised and supervised FGVC, and we\npropose a simple, effective, and practical method, termed\nas UFCL, to alleviate the gap.\n• Three key issues are concerned and improved. We\nintroduce a robust and powerful backbone, ResNet50-\nIBN [29], which has an ability of domain adaptation\nwhen we transferred the ImageNet pre-trained backbone\nto FGVC task. Next, we propose to introduce HDBSCAN\ninstead of DBSCAN to do clustering, which can generate\nbetter clusters for adjacent categories with fewer hyper-\nparameters. Furthermore, we propose a new weighted\nupdating strategy to update the network supervised by\nthe pseudo labels with inevitable noise, which can obtain\nrobust gradients and improve the optimization process of\nparameter learning.\n• The effectiveness of our UFCL method is veriﬁed on\nsix FGVC datasets. Under the unsupervised FGVC tasks\nsetting, we achieve the state-of-the-art results and analyze\nthe key factors and the important parameters to provide a\npractical guidance.\nII. RELATED WORKS\nOur UFCL belongs to unsupervised learning, which has\naroused extensive interests due to its ability of enabling models\nto be trained by unlabeled data and save expensive annotation\ncost. The kernel idea of our UFCL comes from the combination\nof advantages of clustering [33], [25] and self-supervised\ncontrastive learning [34], [28].\nA. Clustering\nClustering methods [33], [22], [25] employ cluster indexes\nas pseudo labels to train the model by unsupervised learning\nparadigm. There are two types of success applications: the\npre-trained model for general object representation [33], [22]\nand the special model for unsupervised person re-ID [25], [28].\nIn general object representation, DeepCluster [33] is the\nﬁrst unsupervised learning method that alternates between\nclustering the representation by k-means and learning to predict\nSUBMITTED TO IEEE TIP\n3\nFigure 2. Our UFCL includes a feature extraction module, a hierarchical clustering module and a contrastive learning module. The three modules are seamlessly\nintegrated into a cycled framework to learning a robust model in an unsupervised schema.\nthe cluster assignments as pseudo-labels. It can scale to large-\nscale dataset and can be used for pre-training of supervised\nnetworks [35]. Another famous one is SwAV [22], which\nincorporates clustering into a Siamese network, by computing\nthe assignment from one view and predicting it from another\nview. The results of these methods are affected by the clustering\nalgorithm, which may generate false pseudo labels for unlabeled\ndata and supervise network learning in a bad way.\nIn unsupervised person re-ID, the early works are dominated\nby unsupervised domain adaptive learning methods, include\nMMT [26] and SpCL [25]. They ﬁrstly adopted a pre-trained\nImageNet model to ﬁne-tune it on a large-scale supervised\nperson re-ID dataset, and then to ﬁne-tune it on the small-\nscale unsupervised target dataset. They distinguished trusted\nexamples from untrusted examples through DBSCAN clus-\ntering, which has no prior requirements for data distribution\nand the total number of clusters as k-means. However, these\nmethods relied on labeled large-scale source domain datasets,\nand it is difﬁcult and costs a lot to build such datasets\nin practice. Recent works directly conducted unsupervised\nclustering learning on the person re-ID datasets. For example,\nGroup-Sampling [7] carried out sampling learning in the unit\nof group examples, which can cluster similar examples better.\nCluster-Contrast [28] directly updated the class center on the\ntarget domain dataset, and looked for difﬁcult examples in batch\nto improve the learning effect. IICS [27] and ICE [6] used the\na priori distributed within the camera to further improve the\nperformance.\nThe aforementioned two types of clustering methods use k-\nmeans and DBSCAN clustering algorithms to generate pseudo\nlabels respectively. Although DBSCAN has stronger density\nclustering ability and better anti-noise ability than k-means, it\nis easy to cluster two adjacent categories into one cluster in the\nface of FGVC tasks with small inter-class difference and large\nintra-class variance. To alleviate the problem of DBSCAN, we\npropose to introduce HDBSCAN [30], [31] into FGVC tasks\nto better cluster adjacent sub-categories.\nB. Self-supervised Contrastive Learning\nContrastive learning methods [21], [20], [23], [24] currently\nachieve state-of-the-art performance in self-supervised learning.\nContrastive approaches learn the discriminative representation\nby bringing representation of different views of the same image\ncloser, and spreading representations of views from different\nimages apart [36]. The existing methods can be categorized\ninto instance-level methods [21], [20], [24] and cluster-level\nmethods [34], [22], [28].\nInstance-level methods treated views of the same image as\npositive pairs and views of different images in the same batch\n(or memory bank) as negative pairs. For example, SimCLR [21]\ntreated the examples in the batch as the negative examples.\nMoCo [20] adopted a dictionary to implement contrastive\nlearning, where one branch of the Siamese network is updated\nwith momentum strategy. SiaSiam [24] trained the Siamese\nnetwork by stopping gradient back-propagation in one of the\nbranch. However, instance-level methods simply make each\nexample independent and repel to each other, which ignores\nthe cluster structure information in examples.\nCluster-level methods regard examples in the same clusters\nas positive cluster and other examples as negative clusters.\nInterCLR [34] used both InfoNCE loss and MarginNCE loss\nto attract positive examples and repelled negative examples.\nSwAV [22] proposed an online clustering loss to improve the\nability of the network to explore the inter-invariance of clusters.\nCluster-Contrast [28] adopted ClusterNCE loss to compute\nthe batch-hard example with the cluster centers in memory\nbank, which greatly improve the performance in person re-ID.\nHowever, cluster-level methods rely on the clustering results\nin which the batch-hard example learning faces convergence\nproblem in the early training stage.\nThe aforementioned methods carry out self-supervised\ncontrastive learning from different granularity. The cluster-\nlevel methods use the structure information between different\nidentities and are more robust than the instance-level ones, but it\nis also a challenge problem for FGVC to solve the convergence\nin the early training stage caused by the batch-hard example\nlearning strategy. In this paper, based on cluster-level contrastive\nSUBMITTED TO IEEE TIP\n4\nlearning, we propose a weighted feature agent, and robustly\nupdated it through a new centroid-based updating mechanism.\nC. Unsupervised Fine-grained Classiﬁcation\nFGVC faces greater challenges due to the subtle difference\namong sub-categories and large variations of many different\nidentities of the same sub-categories. At present, it is dominated\nby supervised learning methods, such as PA-CNN [37], Cross-\nX [38], PMG [8], CAMF [39]. The existing recent methods [38],\n[39] focused on the attention mechanism to ﬁnd effective object\ndiscriminative parts to improve the effectiveness. However,\nthe small inter-class difference is hard to identify even for\nprofessional experts, and annotating these ﬁne-grained sub-\ncategories requires a lot of labor costs.\nTherefore, researchers try to explore the unsupervised\nﬁne-grained image classiﬁcation. However, the performance\nof existing unsupervised methods reported on ﬁne-grained\nclassiﬁcation datasets is very low. For example, the cluster\nmethod, AND [40], only reaches 14.4% and 32.3% on CUB-\n200-2011 and Stanford-Dogs datasets respectively. Although it\nenables the control and mitigation of the clustering errors\nand their negative propagation, it cannot tackle the non-\ngaussian distribution and the inevitable noise. The unsupervised\ncontrastive learning methods, SimCLR [21] and BYOL [23],\nevaluate the ﬁne-grained classiﬁcation tasks, but the ﬁnal linear\nclassiﬁers needs to be trained with labels. In this paper, we\ndirectly reported the accuracy based on the k-NN classiﬁer,\nwhich does not require training. Besides, based on the priority\nof unsupervised clustering and contrastive learning, in this\npaper, we incorporate both of them into one joint framework\nto iteratively generate pseudo labels and train the model, in a\nrobust schema.\nIII. OUR APPROACH\nThe framework of our proposed UFCL is shown in Fig. 2.\nIt is a learning framework which contains three key modules.\nThe ﬁrst one is feature extraction module, which is a deep\nneural network to extract features directly from images. The\nsecond one is hierarchical clustering module, which is a robust\nalgorithm to cluster the features to assign pseudo labels to the\nimages. And the third one is contrastive learning module, which\nis an updating strategy to learn the parameters of the deep\nneural network by a cluster-level contrastive loss. All the three\nmodules are seamlessly integrated into a cycled framework to\nlearning a robust model in an unsupervised schema. Hence,\ndifferent modules can give feedback to each other in UFCL,\nwhich results in bridging the gap between supervised and\nunsupervised learning for FGVC.\nIdeally, a robust feature extraction module can generate\neffective features, which can be clustered into stable clusters\nthrough a stable clustering algorithm. Pseudo labels with high\naccuracy can be generated from the stable clusters, and with the\nhelp of the contrastive learning module, the parameters of deep\nneural network in feature extraction module can be updated\neffectively. As a result, it is a key task to systematically analyze\nand design these three modules. In the following subsections,\nwe will focus on why and how to design three key modules to\nbridge the gap between supervised and unsupervised learning\nfor FGVC, and present the details of our UFCL framework.\nA. Feature Extraction Module\nOur UFCL contains a feature extraction backbone network\nfrom [29] as a module, which is used to extract robust the\nfeature representation for given images. More speciﬁcally,\nResNet50-IBN [29] is constructed based on ResNet-50 by\nreplacing batch normalization block with IBN block, which\nis composed of Batch Normalization (BN) and Instance\nNormalization (IN), to extract robust features. BN can can\naccelerate training and preserves discriminative features, while\nIN can provide visual and appearance invariance. The built-\nin appearance invariance introduced by IN helps the model\nto generalize from the pre-trained ImageNet domain to the\nFGVC domain, even without using the data from the target\ndomain [29].\nOf course, the backbone is not limited to ResNet50-IBN,\nother CNN-based [41] or Transformer-based [42] backbones\nwith domain adaptation ability, can also be used to substitute\nthe ResNet50-IBN as our backbone. But it is not the focus\nof this paper to study new backbone networks. Hence, we\njust adopted ResNet50-IBN for illustrating the effectiveness of\nour UFCL framework, and leave the study of other candidate\nnetworks for future pursuit.\nAfter all convolutional layers of ResNet50-IBN, a gen-\neralized pooling layer, GEM pooling [43], is introduced to\noffer signiﬁcant performance boost over standard non-trainable\npooling layer. Speciﬁcally, suppose that a feature tensor\nX ∈RW ×H×K extracted for a given image I by the ResNet50-\nIBN backbone, where W, H, and K are the width, height and\nnumber of channels of X respectively. The feature matrix of\nthe k-th channel of X is expressed as X(k) ∈RW ×H, and\nthen a real-value f(k) is generated by doing GEM on X(k):\nf(k) =\n\n\n1\n|X(k)|\nX\nx∈X(k)\nxp(k)\n\n\n1\np(k)\n,\n(1)\nwhere p(k) represents the super-parameter corresponding to\nthe k-th channel, and can be learned in the training process.\nWhen p(k) →∞, it equivalents to GMP. When p(k) = 1,\nit equivalents to GAP. GEM pooling generalizes max and\naverage pooling to improve the feature presentation for the\nnext clustering step. Finally, the feature vector f is obtained by\ndoing the ℓ2 regularization on f gem = [f(1), f(2), ..., f(K)].\nB. Hierarchical Clustering Module\nBased on the features extracted from the feature extraction\nmodule, a hierarchical clustering algorithm, HDBSCAN [30],\n[31], is introduced to cluster the features into many clusters. It\ngenerates a pseudo label for the feature of an image according\nto the cluster to which it belongs.\nAs we known, the density-based clustering algorithm, DB-\nSCAN [32], is widely used for unsupervised learning in person\nre-ID [7], [25], [44], [45], [6], [46], [28]. DBSCAN can\nautomatically produce a number of categories based on the\nSUBMITTED TO IEEE TIP\n5\n(a) Results of DBSCAN\n(b) Results of HDBSCAN\nFigure 3. (a) Two adjacent categories are false clustered together (red region),\n(b) The two adjacent categories are spited as different classes correctly [31]\n(red box).\ndistribution of the examples, and has no prior requirements\nfor data distribution and the total number of clusters. It takes\ntwo parameters, ε and k, where ε represents a distance scale,\nand k is a density threshold expressed in terms of a minimum\nnumber of points. However, the ﬁxed distance scale ε faces\nthe problem of variable density clustering, which DBSCAN\nstruggles with. As shown in Fig. 3(a), for adjacent categories\nwith multiple adjacent examples, DBSCAN force examples of\ntwo adjacent categories into one cluster (red region in ﬁgure)\nif the ε is set to a single ﬁxed value. In FGVC tasks, the\ninter-class difference are small, features of the examples from\nadjacent categories are close to each other, and they is easy to\nbe clustered into one cluster when DBSCAN is adopted.\nTo solve the aforementioned problem, we propose to intro-\nduce HDBSCAN [30], [31] to address variable density cluster-\ning problem. HDBSCAN extends DBSCAN by converting it\ninto a hierarchical clustering algorithm and extracts the stable\nand ﬂat clusterings from a condensed tree. HDBSCAN builds\na hierarchy of DBSCAN for varying ε values, and encourages\nthe algorithm to search the best parameter over all ε values, so\nit can deal with variable density clustering problem. As shown\nin Fig. 3(b), HDBSCAN can split two adjacent categories in\ntwo clusters (red box in ﬁgure) by introducing a notion of\nminimum cluster size, which can ﬁnd the clusters that persist for\nmany values of ε. Besides, HDBSCAN has only one parameter,\nminimum cluster size, to be set, so it is more easy to ﬁnd the\nbest parameter than DBSCAN.\nIn our implementation, HDBSCAN uses the pre-computed\nJaccard distance to do clustering based on the features of all\ntraining examples, and generates a large number of clusters. For\nclustering results, the examples belonging to the same cluster\nare classiﬁed into a same class and generate the same pseudo\nlabel. The pseudo labels of examples among different clusters\nare different. The examples not belonging to any cluster are\nregarded as outliers, and pseudo labels are not assigned for\nthem and they will not take part in the subsequent training\nprocess.\nC. Contrastive Learning Module\nThe goal of contrastive learning module is to learn the\nparameters of the backbone network in contrastive fashion\nwith the assigned pseudo labels. It relies on the images\nand their generated pseudo labels to optimize the parameters\nand make them more suitable for FGVC tasks. State-of-the-\nart unsupervised learning methods in person re-ID compute\nthe contrastive loss between query instances and memory\ndictionary. MMCL [47] computes the loss and updates the\nmemory bank both in the instance level. SpCL [25] and Group-\nSampling [7] computes the loss in cluster level but updates the\nmemory dictionary in the instance level. Due to the number of\ninstances in each cluster is imbalanced, the updating progress\nfor each cluster is inconsistent correspondingly. To solve the\ninconsistency problem, ClusterContrast [28] computes the loss\nand update the dictionary both in the cluster level. It uses the\nfeature of a random instance in the cluster to initialize the\ncluster feature, and select the hardest instance for each person\nidentity in the mini-batch to momentum update the cluster\nfeature. However, it is inevitable to contain noisy labels in the\npseudo labels, so the random instance initialization and hardest\ninstance updating mechanism are easy to mislead the network\noptimization.\nTo solve aforementioned problem, we propose a weighted\nfeature agent for the cluster-level memory bank, and propose\na centroid-based updating mechanism to update the feature\nagent. The feature agent, calculated by weighting all examples\ncoming from the same cluster and momentum updated based\non the mean feature of the same cluster in the mini-batch, is\nrobust to the noisy examples, especially for the unsupervised\nFGVC that relies heavily on the accuracy of the pseudo labels\nto distinguish two categories with subtle variance. It can help\nfeature extraction network learning parameters more effectively.\nIn the training process, the features are extracted by forward\npropagation, and the ClusterNCE loss [28] is calculated based\non the mean feature of each class in mini-batch and the feature\nagent in cluster-level memory bank. The ClusterNCE loss\nfunction is:\nL = −log\nexp(d(¯fk, ck)/τ)\nP\nj exp(d(mj, cj)/τ),\n(2)\nwhere ¯fk denotes the mean feature of the k-th class in each\nmini-batch, ck represents the feature agent of k-th cluster,\nand τ represents the temperature super-parameter. d(¯fk, ck)\nrepresents the Euclidean distance between the mean feature\n¯fk and the feature agent ck of the k-th class. When ¯fk has\na higher similarity to its ground-truth feature agent ck and\ndissimilarity to all other feature agents, the objective loss has\na lower value.\nIn the following subsections, we discuss about the computa-\ntion and updating of feature agent in detail.\n1) Computing Feature Agent: After generating pseudo labels,\na feature agent for a cluster is calculated. The calculation\nmethod is obtained by the following weighted averaging:\nck=\nNk\nX\ni=1\nwi\nkf i\nk.\n(3)\nIn Eq. (3), ck is the feature agent (weighted centroid) of\nk-th cluster, Nk is the number of examples in k-th cluster,\nf i\nk is the feature of i-th example of k-th cluster, and wi\nk is\nthe corresponding weight, which is obtained by the following\nformula:\nSUBMITTED TO IEEE TIP\n6\nwi\nk =\nexp(d(f i\nk, Fk))\nP\nj exp(d(f j\nk, Fk))\n,\n(4)\nIn Eq. (4), Fk is the set of all example features of k-th\ncluster, exp(·) represents the exponential function, and the\ndistance measurement d(f i\nk, Fk) can be calculated by any of\nthe following three calculation methods:\n(a) zero distance:\nd(f i\nk, Fk)=0,\n(5)\nwhere the distance constraint is out of consideration, which\nis called zero distance. This distance makes the weight wi\nk =\n1/Nk.\n(b) min distance:\nd(f i\nk, Fk) = arg\nmin\nf j\nk∈Fk,j̸=i\n{d(f i\nk, f j\nk)},\n(6)\nwhere d(f i\nk, Fk) is the minimum distance from the feature f i\nk to\nall other example features of k-th class is called min distance.\n(c) mean distance:\nd(f i\nk, Fk) = 1\nNk\nNk\nX\nj=0,j̸=i\nd(f i\nk, f j\nk),\n(7)\nwhere d(f i\nk, Fk) represents the average distance from the feature\nf i\nk to all other example features of k-th class, which is called\nmean distance.\n2) Updating Feature Agent: The feature agent is momentum\nupdated with Eq. (8):\nck ←mck + (1 −m)¯fk,\n(8)\nwhere m represents the updating momentum, and ¯fk is\ncalculated as follows:\n¯fk =\n1\n|Bk|\nX\nf∈Bk f ,\n(9)\nwhere Bk represents the set of features belonging to the k-th\nclass in a mini-batch batch, |Bk| represents the number of\nfeature in set Bk, f is an instance feature. Eq. (8) represents\nthat the feature agent uses the mean feature of each class in\nmini-batch for momentum updating. Even if there is a small\namount of noise examples in one cluster, the updating can\navoid the inﬂuence of the noise examples when we use the\nfeature agent, which is initialized by weighting all examples\ncoming from the same cluster and is momentum updated based\non the mean feature of the same cluster in the mini-batch.\nIV. EXPERIMENTS\nA. Datasets, Parameter Setting and Evaluation Criteria\n1) Datasets: We use six widely used ﬁne-grained classiﬁ-\ncation datasets, including CUB-200-20111, Oxford-Flowers2,\nOxford-Pets3, Stanford-Dogs4, Stanford Cars5, FGVC Aircraft6.\n1http://www.vision.caltech.edu/visipedia/CUB-200-2011.html\n2https://www.robots.ox.ac.uk/˜vgg/data/ﬂowers/102/\n3https://www.robots.ox.ac.uk/˜vgg/data/pets/\n4http://vision.stanford.edu/aditya86/ImageNetDogs/\n5https://ai.stanford.edu/˜jkrause/cars/car dataset.html\n6https://www.robots.ox.ac.uk/˜vgg/data/fgvc-aircraft/\nTable I\nDETAILED STATISTICS OF THE DATASETS. ‘#CLASS’: THE NUMBER OF\nCLASSES, ‘#TRAIN’: THE NUMBER OF TRAINING EXAMPLES, ‘#TEST’: THE\nNUMBER OF TESTING EXAMPLES, AND ‘#IMAGES’: THE TOTAL NUMBER OF\nEXAMPLES.\nDatasets\n#Class\n#Train\n#Test\n#Images\nCUB-200-2011\n200\n5994\n5794\n11788\nOxford-Flowers\n102\n2040\n6149\n8189\nOxford-Pets\n37\n3680\n3669\n7349\nStanford-Dogs\n120\n12000\n8580\n20580\nStanford Cars\n196\n8144\n8041\n16158\nFGVC Aircraft\n100\n6667\n3333\n10000\nThe statistical information on the training and testing of the\nsix datasets is shown in Table I.\n2) Parameter Setting: In implementation, the convolution\nstride of the ﬁrst layer in the fourth stage of ResNet50-IBN\nis adjusted to 1, that is, the size of the feature map is not\ndown-sampled. For the output features at the end of the fourth\nstage, GEM pooling [43] is used to merge the features of\neach channel to obtain the features with a dimension of 2048.\nFinally, ℓ2 normalization is performed to obtain the normalized\nfeatures for clustering.\nHDBSCAN is used for clustering, and the parameter min-\ncluster-size is uniformly set to 5. The input image is uniformly\nscaled to 224×224, using data augmentation such as random\nclipping, horizontal ﬂipping and random erasing. The batch-size\nis set to 256 unless special statement. The backbone network\nmodel adopts ResNet50-IBN, initialized with ImageNet pre-\ntrained parameters, and the parameters are updated with Adam\noptimizer. The initial learning rate is 0.00035 and weight\ndecay=5 × 10−4, momentum update parameter m = 0.1, and\nthe total number of iterations is 50.\nTo compare different unsupervised methods for ﬁne-grained\nclassiﬁcation, we reproduce Group-Sampling, Cluster-Contract\nand ICE and transfer them from person re-ID to FGVC task.\nThe reproduction effects of each method on different datasets\nare shown in Table 3. When Group-Sampling is reproduced,\nthe parameter n-group is set to 256, and all other parameters\nadopt the default parameters. Cluster-Contract adopts default\nparameters. ICE sets batch-size to 64, and other are default\nparameters. On Oxford-Flowers and Oxford-Pets datasets, the\nnumber of iterations within each epoch is set to 25 and the\nother datasets are set to 100.\n3) Evaluation Criteria: the widely used evaluation criteria\nfor unsupervised clustering are ACC, NMI and ARI [5], but\nthese criteria are mainly used for k-means, that is, all examples\nhave predicted labels. Different from k-means, DBSCAN does\nnot predict label for all examples, so its criteria needs to be\nmodiﬁed.\nSuppose that N represents the total number of all examples\nand Ω= {m1, ..., mk, ..., mK} represents the division of\nclass clusters, where the number of examples in the k-th\nclass cluster mk is #mk, then PK\nk=1 #mk = M, M ≤N.\nC = {c1, ..., cj, ..., cJ} indicates the truth division, where\nthe number of examples in the j-th class cluster cj is #cj,\nthen PJ\nj=1 #cj = N. Among them, the clustering results of\nHDBSCAN or DBSCAN clustering algorithm will have many\noutliers, which do not belong to any cluster, so the modiﬁed\nSUBMITTED TO IEEE TIP\n7\nindicators are as follows:\n(a) Clustering Criteria (ACC)\nACC = 1\nN\nM\nX\ni=1\nδ(gi, ρ(pi)),\n(10)\nwhere gi is the truth label of the i-th example in the clustering,\npi is the pseudo label corresponding to its prediction, and\nρ(·) is the redistribution mapping function of the best class\nlabel, which can be realized in polynomial time by Hungarian\nalgorithm. δ(a, b) is Dirac delta function which returns 1 if\na = b, and 0 otherwise. Similarly, NMI and ARI are calculated\nin the similar way.\n(b) Classiﬁcation Criteria (Top-1)\nWeighted k-NN classiﬁer (k = 5) in AND [5] is used for\nprediction,\nTop-1 = 1\nN\nN\nX\nc=1\nδ(gc, pc),\n(11)\nwhere gc is the truth label of the c-th example in the clustering\nresult and pc is the predicted pseudo label. The calculation\nmethod is\npc =\nX\ni∈Nk wi · δ(c, ci),\n(12)\nwhere δ(c, ci) means to judge whether the c-th example and\nthe ci-th example are k-nearest neighbors. wi = exp(pi/τ),\nτ = 0.07.\nB. Results and Analysis\n1) Basic Results and Analysis: At present, there is few\nreports on the unsupervised learning for ﬁne-grained classiﬁca-\ntion. Therefore, we directly transfer the existing unsupervised\nlearning method from the domain of person re-ID, and mainly\nmodify the data input and evaluation, without changing the\nalgorithm itself. The speciﬁc comparison methods include\nGroup-Sampling, Cluster-Contrast and ICE, and the three\nmethods all use ResNet50-IBN as the backbone network for\nfair comparison. ICE method does not consider the constraints\nof cameras. The ﬁnal test results are shown in Table II.\nIt can be easily found that our UFCL is the best on the whole\nin terms of classiﬁcation Top-1 and clustering ACC. Our UFCL\nhave the best Top-1 except Stanford-Dogs and best ACC except\nFGVC-Aircraft. Cluster-Contrast is the second best method,\nwhich achieves the second best Top-1 performance on four\ndatasets. ICE has unstable performance. It has good results on\nCUB-200-2011, Stanford-Dogs and Oxford-Flowers, but the\nresults are poor on the other three datasets. Group-Sampling\nis poor generally.\nComparing different datasets, it can be found that Oxford-\nFlowers and Oxford-Pets are relatively easy for classiﬁcation.\nMost methods have high classiﬁcation Top-1 and clustering\nACC on these two datasets, where our UFCL achieves 90.1%\nand 79.0% in Top-1. It is relatively difﬁcult for classiﬁcation\non Stanford-Cars and FGVC-Aircraft, in which our UFCL only\nachieves 33.7% and 43.3% in Top-1. In terms of clustering\nACC, our UFCL has the best result on CUB-200-2011, reaching\n(a)\n(b)\n(c)\n(d)\nFigure 4. Performance curves of different feature extraction networks.\n58.0% in ACC, followed by Oxford-Flowers and Stanford-\nDogs, reaching 39.2% and 31.1% in ACC respectively. It has\nthe worst performance on FGVC-Aircraft, on which different\nmethods are similar ACC. It can be seen that it is difﬁcult to\ndo unsupervised learning on FGVC-Aircraft.\nBased on the aforementioned results, it can be found that\nthe unsupervised learning methods of person re-ID has great\ngaps when they are applied to ﬁne-grained recognition tasks,\nmeanwhile there are also great performance difference on\ndifferent datasets.\nC. Ablation Analysis\n1) Feature Extraction Module: In order to measure the\nfeature extraction capability of different networks, we use\ndifferent feature extraction networks, ResNet-18, ResNet50,\nMobileNetV2, DenseNet121, ResNet50-IBN respectively. The\nexperiments are conducted on CUB-200-2011 and Oxford-\nFlowers, and the results are shown in Table III. Due to the\nlimitation of the CUDA memory, the corresponding batch-size\nis adjusted from 256 to 192 when DenseNet121 is used.\nIt can be found from Table III that ResNet50-IBN has the\nbest performance on CUB-200-2011, reaching 69.0% in Top-1\nand 58.0% in ACC. Secondly, DenseNet121 reached 62.3%\nin Top-1 and 48.6% in ACC. The Top-1 and ACC of ResNet-\n18 were 39.3% and 34.8% respectively, which have 29.7%\nand 23.3% gaps from the best ResNet50-IBN. It can also be\nfound that the light-weighted MobileNetV2 has achieved better\nresults than ResNet50, reaching 55.6% in Top-1 and 46.1% in\nACC. The main reason is that MobileNetV2 is proposed by\nabsorbing the advantage of ResNet architecture and has better\ngeneralization ability.\nAt the same time, the parameters of feature extraction\nnetwork are initialized by the pre-trained model on the\nImageNet dataset, so the initial feature distribution is affected\nby the prior knowledge of ImageNet dataset and has an\nimportant impact on the results of generating pseudo labels\nSUBMITTED TO IEEE TIP\n8\nTable II\nBASIC RESULTS\nMethods\nCUB-200-2011\nOxford-Flowers\nOxford-Pets\nTop-1\nACC\nNMI\nARI\nTop-1\nACC\nNMI\nARI\nTop-1\nACC\nNMI\nARI\n†Group-Sampling [7]\n24.1%\n13.0%\n35.3%\n0.8%\n66.4%\n29.2%\n59.2%\n16.6%\n18.3%\n10.8%\n14.5%\n0.4%\n†Cluster-Contrast [28]\n47.0%\n30.2%\n57.3%\n3.5%\n79.1%\n34.4%\n66.7%\n22.7%\n71.3%\n25.3%\n40.2%\n2.2%\n†ICE [6]\n63.9%\n47.2%\n73.8%\n4.5%\n64.9%\n9.8%\n30.9%\n0.2%\n19.2%\n4.5%\n4.6%\n0.0%\nOur UFCL\n69.0%\n58.0%\n78.6%\n45.3%\n90.1%\n39.2%\n70.7%\n26.5%\n79.0%\n28.1%\n44.8%\n3.6%\nMethods\nStanford-Dogs\nStanford-Cars\nFGVC-Aircraft\nTop-1\nACC\nNMI\nARI\nTop-1\nACC\nNMI\nARI\nTop-1\nACC\nNMI\nARI\n†Group-Sampling [7]\n28.5%\n10.0%\n22.3%\n0.1%\n16.8%\n9.1%\n28.3%\n0.3%\n27.3%\n6.7%\n19.8%\n0.1%\n†Cluster-Contrast [28]\n40.2%\n15.9%\n31.5%\n0.3%\n22.4%\n12.0%\n36.3%\n0.4%\n41.2%\n6.2%\n20.1%\n0.1%\n†ICE [6]\n63.7%\n30.1%\n44.9%\n0.8%\n20.7%\n10.0%\n32.0%\n0.1%\n19.6%\n2.3%\n4.9%\n0.0%\nOur UFCL\n58.5%\n31.1%\n45.8%\n0.8%\n33.7%\n19.4%\n46.5%\n3.3%\n43.3%\n6.0%\n19.0%\n0.1%\n† The reproduced method for FGVC.\nTable III\nRESULTS OF DIFFERENT FEATURE EXTRACTION NETWORKS.\nBackbone\nBatch Size\nCUB-200-2011\nOxford-Flowers\nTop-1\nACC\nNMI\nARI\nTop-1\nACC\nNMI\nARI\nMobileNetV2\n256\n55.6%\n46.1%\n71.4%\n32.2%\n88.4%\n38.5%\n70.2%\n26.3%\nDenseNet121\n192\n62.3%\n48.6%\n72.8%\n28.3%\n90.8%\n39.5%\n71.3%\n27.3%\nResNet18\n256\n39.3%\n34.8%\n64.1%\n24.4%\n81.5%\n36.3%\n67.4%\n23.6%\nResNet50\n256\n44.9%\n37.9%\n65.8%\n25.5%\n88.0%\n37.6%\n69.7%\n25.2%\nResNet50-IBN\n256\n69.0%\n58.0%\n78.6%\n45.3%\n90.1%\n39.2%\n70.7%\n26.5%\nby the ﬁrst clustering. The results of different networks is\nevaluated on CUB-200-2011 and Stanford-Flowers and shown\nin Fig. 4. Fig. 4 shows the curves of Top-1 and ACC in\ntraining process. It can be found that most of networks have\nrelatively good initial classiﬁcation Top-1, but it has a relatively\nlarge decline after the ﬁrst epoch. This is mainly because the\ninitialized parameters need an adaptation process on a new\ndataset. For the clustered ACC, the initial feature distribution\nis relatively scattered, and the value is relatively low. After\nseveral epochs, the performance is also continuously improved.\nIn addition, comparing different feature extraction networks, it\ncan be found that a small-scale network (ResNet18) is not as\ngood as a large-scale network (ResNet50).\n2) Hierarchical Clustering Module: In order to verify the\neffect of HDBSCAN, the widely used DBSCAN algorithm is\ncompared, in which the DBSCAN parameter ε is uniformly\nset to 0.4 and the number of adjacent examples is set to 4. The\nexperimental results are shown in Fig. 5.\nCompared with DBSCAN, HDBSCAN has increased by\n3.7% and 0.5% in Top-1, and 8.6% and 1.4% in ACC on CUB-\n200-2011 and Oxford-Flowers, respectively. It is veriﬁed that\nHDBSCAN produces better clustering results for unsupervised\nlearning. From another viewpoint, the ACC of HDBSCAN\nhas a big gap between CUB-200-2011 and Oxford-Flowers. It\nindicates that HDBSCAN has better clustering results on CUB-\n200-2011 than on Oxford-Flowers. The Top-1 of HDBSCAN\nhas also a big gap between CUB-200-2011 and Oxford-Flowers.\nHowever, it has higher Top-1 on Oxford-Flowers than that\non CUB-200-2011. It indicates that HDBSCAN has better\nclassiﬁcation results on Oxford-Flowers than that on CUB-200-\n2011. Comparing ACC and Top-1, it can be found that the\nclustering effectiveness are not necessarily positively correlated\nwith the classiﬁcation accuracy. In addition, HDBSCAN has\nonly one parameter, which has better applicability.\n(a)\n(b)\nFigure 5. Comparison of DBSCAN and HDBSCAN.\n3) Contrastive Learning Module: The feature agent pro-\nposed in this paper is similar to Cluster-Contrast, which is\nthe feature center of the examples in the same cluster. The\ndifference is that our method is a generalization of that in\nCluster-Contrast. When we use zero distance, it is the same as\nthe method in Cluster-Contrast. At the same time, our method\ndirectly uses examples to update feature agents, unlike Cluster-\nContrast, which uses difﬁcult examples in batch examples\nto update feature agents. In addition, in order to verify the\nadvantages of the updating strategy adopted in this paper, we\nalso compare it with Group-Sampling, which maintains the\nfeature of each example in memory to do updating. In an\nexecution, it updates the feature of a single example. The\nexperimental results are shown in Table IV.\nIt can be found from Table IV that on CUB-200-2011 and\nOxford ﬂowers, the three weighted strategies proposed in this\npaper are signiﬁcantly improved on Top-1 and ACC compared\nwith Cluster-Contrast and Group-Sampling. This improvement\nis mainly caused by the proposed distance calculation strategy.\nAt the same time, there are some difference among the three\ndifferent distance measures. The mean distance is the best one.\nSUBMITTED TO IEEE TIP\n9\nTable IV\nRESULTS OF DIFFERENT UPDATING STRATEGIES.\nMethods\nCUB-200-2011\nOxford-Flowers\nTop-1\nACC\nNMI\nARI\nTop-1\nACC\nNMI\nARI\nGroup-Sampling [7]\n24.1%\n13.0%\n35.3%\n0.8%\n66.4%\n29.2%\n59.2%\n16.6%\nCluster-Contrast [28]\n47.0%\n30.2%\n57.3%\n3.5%\n79.1%\n34.4%\n66.7%\n22.7%\nOur UFCL (zero)\n67.1%\n56.7%\n77.8%\n43.9%\n87.9%\n37.5%\n69.7%\n25.5%\nOur UFCL (min)\n68.2%\n57.0%\n78.6%\n44.7%\n87.5%\n38.1%\n70.2%\n26.2%\nOur UFCL (mean)\n69.0%\n58.0%\n78.6%\n45.3%\n90.1%\n39.2%\n70.7%\n26.5%\n(a)\n(b)\nFigure 6. Results of different super-parameters.\nD. Factor Analysis\nIn this paper, the pooling strategy in feature extraction, the\nhyper-parameter in HDBSCAN and the iterations of a single\nepoch have a great impact on the results. The speciﬁc analysis\nis as follows:\n1) GEM Pooling: To test the effectiveness of GEM, we\ncompare it with the widely used GAP and GMP. On CUB-\n200-2011 and Oxford-Flowers, all experimental conditions and\nparameter settings were the same, and only the pooling method\nwas different. Four methods including GAP, GMP, GAP+GMP\nand GEM were compared. The results are shown in Table V.\nIt can be found that under ResNet-50-IBN, the Top-1 of\nGEM with a parameter reaches 60.0% and 90.1% respectively\non CUB-200-2011 and Oxford-Flowers datasets, and the ACC\nreaches 58.0% and 39.2% respectively. Compared with GAP,\nGEM increased Top-1 by 22.4% and 0.9%, and increased\nACC by 22.4% and 1.5% on CUB-200-2011 and Oxford-\nFlowers respectively. Compared with GMP, it also has a\ncertain improvement. At the same time, the effect is slightly\nbetter than that of GAP+GMP. Therefore, GEM pooling has\nbetter performance and is good at unsupervised ﬁne-grained\nclassiﬁcation.\n2) Super-parameter, min-cluster-size: HDBSCAN has a\nhyper-parameter min-cluster-size. We conducted experiments\nto test the results of 3-11 different values on CUB-200-2011\nand Oxford-Flowers. The results are shown in Fig. 6.\nIt can be found that the number of clusters will decrease\nwith the increase of parameter value. At the same time, the\nclassiﬁcation Top-1 and clustering ACC are better when they\nare close to the actual number of categories. It is difﬁcult\nto set the optimal clustering parameters without any prior\nconditions. However, the experimental results show that on\nCUB-200-2011, when the number of clusters is close to 200,\nboth Top-1 and ACC achieve good results. On Oxford ﬂowers,\nwhen the number of clusters is close to 102, Top-1 and ACC\nare also the best. Therefore, if the number of categories is\nknown in advance, the parameter that makes the number of\nclusters closest to the number of categories can be selected\naccording to the number of categories.\n3) Iterations in Each Epoch: Because each epoch performs\nmany iterations internally to learn a relatively better model\nfor clustering, the number of iterations in a single epoch is a\nkey parameter. Too many iterations will lead to over-ﬁtting,\nand too few iterations will lead to under-ﬁtting. Therefore, an\nappropriate number of iterations will have a great impact on\nperformance. We performed 25, 50, 100 and 200 iterations on\nCUB-200-2011 and Oxford-Flowers respectively. The results\nare shown in Table VI.\nIt can be found from Table VI that for CUB-200-2011, 50\niterations have the best performance. For Oxford-Flowers, 25\niterations performed best. If iterations are less, the learning in\none epoch will be insufﬁcient, and the model is not enough to\nextract effective features to produce good clusters in the next\nclustering. If iterations are more, the model will over-ﬁt the\ntraining data, the model is difﬁcult to have good generalization,\nand the clusters generated in the next clustering will be worse.\nE. Feature Visualization and Analysis\nTo visualize the distribution of the extracted features, we\nembedded the features into a 2D plane by the t-SNE algorithm\non CUB-200-2011 and Oxford-Flowers. In order to show the\ndistribution of the intra-class features, we connect the intra-\nclass features to their feature agent. The features are showed\nin Fig. 7.\nIt can be found from Fig. 7 that at the beginning (epoch\n0), CUB-200-2011 had poor initial distribution compared with\nOxford-Flowers, and most of the examples are scattered on the\nfeature space. after the 10-th epoch, the intra-class examples\nstart to cluster, and ﬁnally move together at the 50-th epoch.\nV. CONCLUSION\nIn this paper, we explored an unsupervised learning method\nof ﬁne-grained image classiﬁcation, and examined each module\non the impact performance in the clustering learning process\nand discover three key modules that contribute to the perfor-\nmance of unsupervised FGVC. For feature extraction module,\nwe found that a strong and powerful backbone can obtain\nbetter performance than weak backbone, the quality of the\ninitial extracted feature distribution has an important impact\non the clustering. It is easy to lead to the deterioration or\nfailure especially when the initial feature distribution is poor.\nFor the clustering module, HDBSCAN can produce stable\nclusters and achieve better performance than DBSCAN with\nSUBMITTED TO IEEE TIP\n10\nTable V\nRESULTS OF DIFFERENT POOLING OPERATIONS.\nPooling\nCUB-200-2011\nOxford-Flowers\nTop-1\nACC\nNMI\nARI\nTop-1\nACC\nNMI\nARI\nGAP\n44.1%\n35.6%\n63.8%\n24.1%\n89.2%\n37.7%\n70.1%\n25.8%\nGMP\n68.2%\n60.1%\n79.9%\n47.8%\n87.5%\n38.2%\n69.4%\n25.3%\nGAP+GMP\n68.4%\n61.6%\n80.7%\n50.0%\n88.6%\n38.2%\n69.9%\n25.7%\nGEM\n69.0%\n58.0%\n78.6%\n45.3%\n90.1%\n39.2%\n70.7%\n26.5%\nTable VI\nRESULTS OF DIFFERENT ITERATION TIMES IN ONE EPOCH\nIterations\nCUB-200-2011\nOxford-Flowers\nTop-1\nACC\nNMI\nARI\nTop-1\nACC\nNMI\nARI\n25\n68.3%\n48.7%\n70.5%\n7.5%\n90.1%\n39.2%\n70.7%\n26.5%\n50\n71.0%\n58.1%\n77.6%\n35.2%\n82.1%\n36.8%\n68.6%\n24.4%\n100\n69.0%\n58.0%\n78.6%\n45.3%\n81.2%\n36.2%\n68.2%\n23.2%\n200\n62.6%\n58.8%\n79.4%\n46.8%\n76.1%\n35.2%\n65.5%\n17.8%\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n(a)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n(b)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n3637\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71 72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162163\n164\n165\n166\n167\n168\n169\n170\n171\n(c)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39 40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n9899\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155156\n157 158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n(d)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n(e)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n(f)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n(g)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n(h)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n(i)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n(j)\nFigure 7. Distribution of features from CUB-200-2011 (ﬁrst line) and Oxford-Flowers (second line). (a)-(e) and (f)-(j) are the clustering results of the 0th, 1st,\n10th, 20th and 50th epochs, respectively. Best viewed on a monitor when zoomed in.\nfewer super-parameters. For the contrastive learning module,\ncluster-level contrastive loss with weighted feature agent can\nlearn the parameters better. In each training epoch, the number\nof iterations is an important factor, and it should be set by\nbalancing over-ﬁtting and under-ﬁtting. We hope these new\nobservations and discussions can challenge some common\nbeliefs and encourage people to rethink the importance of the\nthree key modules in unsupervised FGVC.\nAlthough the proposed method has achieved better results\non multiple datasets, but it still needs further research. For\nexample, the existing unsupervised contrastive learning can\neffectively learn the feature representation of a single example\nwithout clustering the whole training dataset. It is potentially\nlearning method worthy of exploration.\nREFERENCES\n[1] X.-S. Wei, Y.-Z. Song, O. Mac Aodha, J. Wu, Y. Peng, J. Tang, J. Yang,\nand S. Belongie, “Fine-grained image analysis with deep learning: A\nsurvey,” IEEE Transactions on Pattern Analysis and Machine Intelligence,\npp. 1–1, 2021.\n[2] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The Caltech-\nUCSD Birds-200-2011 Dataset,” California Institute of Technology, Tech.\nRep. CNS-TR-2011-001, 2011.\n[3] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3d object representations\nfor ﬁne-grained categorization,” in 2013 IEEE International Conference\non Computer Vision Workshops, ICCV Workshops 2013, Sydney,\nAustralia, December 1-8, 2013.\nIEEE Computer Society, 2013, pp.\n554–561. [Online]. Available: https://doi.org/10.1109/ICCVW.2013.77\n[4] S. Maji, E. Rahtu, J. Kannala, M. B. Blaschko, and A. Vedaldi,\n“Fine-grained visual classiﬁcation of aircraft,” CoRR, vol. abs/1306.5151,\n2013. [Online]. Available: http://arxiv.org/abs/1306.5151\n[5] W. V. Gansbeke, S. Vandenhende, S. Georgoulis, M. Proesmans, and\nL. V. Gool, “SCAN: learning to classify images without labels,” in\nComputer Vision - ECCV 2020 - 16th European Conference, Glasgow,\nUK, August 23-28, 2020, Proceedings, Part X, ser. Lecture Notes in\nComputer Science, A. Vedaldi, H. Bischof, T. Brox, and J. Frahm,\nEds., vol. 12355.\nSpringer, 2020, pp. 268–285. [Online]. Available:\nhttps://doi.org/10.1007/978-3-030-58607-2 16\n[6] H. Chen, B. Lagadec, and F. Br´emond, “ICE: inter-instance contrastive\nencoding\nfor\nunsupervised\nperson\nre-identiﬁcation,”\nCoRR,\nvol.\nabs/2103.16364, 2021. [Online]. Available: https://arxiv.org/abs/2103.\n16364\n[7] X. Han, X. Yu, N. Jiang, G. Li, J. Zhao, Q. Ye, and Z. Han,\n“Group sampling for unsupervised person re-identiﬁcation,” CoRR, vol.\nabs/2107.03024, 2021. [Online]. Available: https://arxiv.org/abs/2107.\n03024\n[8] R. Du, D. Chang, A. K. Bhunia, J. Xie, Z. Ma, Y. Song, and J. Guo,\n“Fine-grained visual classiﬁcation via progressive multi-granularity\ntraining of jigsaw patches,” in Computer Vision - ECCV 2020 - 16th\nEuropean Conference, Glasgow, UK, August 23-28, 2020, Proceedings,\nPart XX, ser. Lecture Notes in Computer Science, A. Vedaldi, H. Bischof,\nT. Brox, and J. Frahm, Eds., vol. 12365.\nSpringer, 2020, pp. 153–168.\nSUBMITTED TO IEEE TIP\n11\n[Online]. Available: https://doi.org/10.1007/978-3-030-58565-5 10\n[9] S. Branson, G. V. Horn, S. J. Belongie, and P. Perona, “Bird species\ncategorization using pose normalized deep convolutional nets,” CoRR, vol.\nabs/1406.2952, 2014. [Online]. Available: http://arxiv.org/abs/1406.2952\n[10] N. Zhang, J. Donahue, R. B. Girshick, and T. Darrell, “Part-based\nr-cnns for ﬁne-grained category detection,” in Computer Vision - ECCV\n2014 - 13th European Conference, Zurich, Switzerland, September\n6-12, 2014, Proceedings, Part I, ser. Lecture Notes in Computer\nScience, D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars,\nEds., vol. 8689.\nSpringer, 2014, pp. 834–849. [Online]. Available:\nhttps://doi.org/10.1007/978-3-319-10590-1 54\n[11] S. Huang, Z. Xu, D. Tao, and Y. Zhang, “Part-stacked CNN for\nﬁne-grained visual categorization,” in 2016 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV,\nUSA, June 27-30, 2016.\nIEEE Computer Society, 2016, pp. 1173–1182.\n[Online]. Available: https://doi.org/10.1109/CVPR.2016.132\n[12] X. Wei, C. Xie, J. Wu, and C. Shen, “Mask-cnn: Localizing parts\nand selecting descriptors for ﬁne-grained bird species categorization,”\nPattern Recognit., vol. 76, pp. 704–714, 2018. [Online]. Available:\nhttps://doi.org/10.1016/j.patcog.2017.10.002\n[13] J.\nFu,\nH.\nZheng,\nand\nT.\nMei,\n“Look\ncloser\nto\nsee\nbetter:\nRecurrent attention convolutional neural network for ﬁne-grained image\nrecognition,” in 2017 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017.\nIEEE Computer Society, 2017, pp. 4476–4484. [Online]. Available:\nhttps://doi.org/10.1109/CVPR.2017.476\n[14] H. Zheng, J. Fu, T. Mei, and J. Luo, “Learning multi-attention\nconvolutional neural network for ﬁne-grained image recognition,” in\nIEEE International Conference on Computer Vision, ICCV 2017,\nVenice, Italy, October 22-29, 2017.\nIEEE Computer Society, 2017, pp.\n5219–5227. [Online]. Available: https://doi.org/10.1109/ICCV.2017.557\n[15] T. Lin, A. RoyChowdhury, and S. Maji, “Bilinear convolutional neural\nnetworks for ﬁne-grained visual recognition,” IEEE Trans. Pattern Anal.\nMach. Intell., vol. 40, no. 6, pp. 1309–1322, 2018. [Online]. Available:\nhttps://doi.org/10.1109/TPAMI.2017.2723400\n[16] Y. Wang, V. I. Morariu, and L. S. Davis, “Learning a discriminative\nﬁlter bank within a CNN for ﬁne-grained recognition,” in 2018 IEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2018,\nSalt Lake City, UT, USA, June 18-22, 2018.\nIEEE Computer Society,\n2018, pp. 4148–4157.\n[17] X. He, Y. Peng, and J. Zhao, “Which and how many regions to gaze:\nFocus discriminative regions for ﬁne-grained visual categorization,”\nInt. J. Comput. Vis., vol. 127, no. 9, pp. 1235–1255, 2019. [Online].\nAvailable: https://doi.org/10.1007/s11263-019-01176-2\n[18] Y. Gao, X. Han, X. Wang, W. Huang, and M. Scott, “Channel\ninteraction networks for ﬁne-grained image categorization,” in The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020,\nThe Thirty-Second Innovative Applications of Artiﬁcial Intelligence\nConference, IAAI 2020, The Tenth AAAI Symposium on Educational\nAdvances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA,\nFebruary 7-12, 2020.\nAAAI Press, 2020, pp. 10 818–10 825. [Online].\nAvailable: https://aaai.org/ojs/index.php/AAAI/article/view/6712\n[19] D.\nChang,\nY.\nDing,\nJ.\nXie,\nA.\nK.\nBhunia,\nX.\nLi,\nZ.\nMa,\nM. Wu, J. Guo, and Y. Song, “The devil is in the channels:\nMutual-channel loss for ﬁne-grained image classiﬁcation,” IEEE Trans.\nImage Process., vol. 29, pp. 4683–4695, 2020. [Online]. Available:\nhttps://doi.org/10.1109/TIP.2020.2973812\n[20] K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick, “Momentum\ncontrast for unsupervised visual representation learning,” in 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\nCVPR 2020, Seattle, WA, USA, June 13-19, 2020.\nComputer\nVision Foundation / IEEE, 2020, pp. 9726–9735. [Online]. Available:\nhttps://doi.org/10.1109/CVPR42600.2020.00975\n[21] T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton, “A simple\nframework for contrastive learning of visual representations,” in\nProceedings of the 37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, ser. Proceedings of Machine\nLearning Research, vol. 119.\nPMLR, 2020, pp. 1597–1607. [Online].\nAvailable: http://proceedings.mlr.press/v119/chen20j.html\n[22] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin,\n“Unsupervised learning of visual features by contrasting cluster assign-\nments,” in Advances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato,\nR. Hadsell, M. Balcan, and H. Lin, Eds., 2020.\n[23] J. Grill, F. Strub, F. Altch´e, C. Tallec, P. H. Richemond, E. Buchatskaya,\nC. Doersch, B. ´A. Pires, Z. Guo, M. G. Azar, B. Piot, K. Kavukcuoglu,\nR. Munos, and M. Valko, “Bootstrap your own latent - A new\napproach to self-supervised learning,” in Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,\nH. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds.,\n2020.\n[24] X. Chen and K. He, “Exploring simple siamese representation learning,”\nin IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2021, virtual, June 19-25, 2021.\nComputer Vision Foundation / IEEE,\n2021, pp. 15 750–15 758.\n[25] Y. Ge, F. Zhu, D. Chen, R. Zhao, and H. Li, “Self-paced contrastive learn-\ning with hybrid memory for domain adaptive object re-id,” in Advances\nin Neural Information Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan,\nand H. Lin, Eds., 2020.\n[26] Y. Ge, D. Chen, and H. Li, “Mutual mean-teaching: Pseudo label\nreﬁnery for unsupervised domain adaptation on person re-identiﬁcation,”\nin 8th International Conference on Learning Representations, ICLR\n2020, Addis Ababa, Ethiopia, April 26-30, 2020.\nOpenReview.net,\n2020. [Online]. Available: https://openreview.net/forum?id=rJlnOhVYPS\n[27] S. Xuan and S. Zhang, “Intra-inter camera similarity for unsupervised\nperson re-identiﬁcation,” in IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021.\nComputer Vision Foundation / IEEE, 2021, pp. 11 926–11 935.\n[Online]. Available: https://openaccess.thecvf.com/content/CVPR2021/\nhtml/Xuan Intra-Inter Camera Similarity for Unsupervised Person\nRe-Identiﬁcation CVPR 2021 paper.html\n[28] Z. Dai, G. Wang, S. Zhu, W. Yuan, and P. Tan, “Cluster contrast for\nunsupervised person re-identiﬁcation,” CoRR, vol. abs/2103.11568, 2021.\n[Online]. Available: https://arxiv.org/abs/2103.11568\n[29] X. Pan, P. Luo, J. Shi, and X. Tang, “Two at once: Enhancing\nlearning and generalization capacities via ibn-net,” in Computer Vision -\nECCV 2018 - 15th European Conference, Munich, Germany, September\n8-14, 2018, Proceedings, Part IV, ser. Lecture Notes in Computer\nScience, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss,\nEds., vol. 11208.\nSpringer, 2018, pp. 484–500. [Online]. Available:\nhttps://doi.org/10.1007/978-3-030-01225-0 29\n[30] R. J. G. B. Campello, D. Moulavi, and J. Sander, “Density-based\nclustering based on hierarchical density estimates,” in Advances in\nKnowledge Discovery and Data Mining, 17th Paciﬁc-Asia Conference,\nPAKDD 2013, Gold Coast, Australia, April 14-17, 2013, Proceedings,\nPart II, ser. Lecture Notes in Computer Science, J. Pei, V. S. Tseng,\nL. Cao, H. Motoda, and G. Xu, Eds., vol. 7819. Springer, 2013, pp. 160–\n172. [Online]. Available: https://doi.org/10.1007/978-3-642-37456-2 14\n[31] R. J. G. B. Campello, D. Moulavi, A. Zimek, and J. Sander,\n“Hierarchical density estimates for data clustering, visualization, and\noutlier detection,” ACM Trans. Knowl. Discov. Data, vol. 10, no. 1, pp.\n5:1–5:51, 2015. [Online]. Available: https://doi.org/10.1145/2733381\n[32] M. Ester, H. Kriegel, J. Sander, and X. Xu, “A density-based algorithm for\ndiscovering clusters in large spatial databases with noise,” in Proceedings\nof the Second International Conference on Knowledge Discovery and\nData Mining (KDD-96), Portland, Oregon, USA, E. Simoudis, J. Han,\nand U. M. Fayyad, Eds.\nAAAI Press, 1996, pp. 226–231. [Online].\nAvailable: http://www.aaai.org/Library/KDD/1996/kdd96-037.php\n[33] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering\nfor unsupervised learning of visual features,” in Computer Vision -\nECCV 2018 - 15th European Conference, Munich, Germany, September\n8-14, 2018, Proceedings, Part XIV, ser. Lecture Notes in Computer\nScience, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss,\nEds., vol. 11218.\nSpringer, 2018, pp. 139–156. [Online]. Available:\nhttps://doi.org/10.1007/978-3-030-01264-9 9\n[34] J. Xie, X. Zhan, Z. Liu, Y. Ong, and C. C. Loy, “Delving into\ninter-image invariance for unsupervised visual representations,” CoRR,\nvol. abs/2008.11702, 2020. [Online]. Available: https://arxiv.org/abs/\n2008.11702\n[35] M. Caron, P. Bojanowski, J. Mairal, and A. Joulin, “Unsupervised\npre-training of image features on non-curated data,” in 2019 IEEE/CVF\nInternational Conference on Computer Vision, ICCV 2019, Seoul, Korea\n(South), October 27 - November 2, 2019.\nIEEE, 2019, pp. 2959–2968.\n[Online]. Available: https://doi.org/10.1109/ICCV.2019.00305\n[36] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, “Unsupervised feature learning\nvia non-parametric instance discrimination,” in 2018 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake\nCity, UT, USA, June 18-22, 2018.\nComputer Vision Foundation / IEEE\nComputer Society, 2018, pp. 3733–3742.\nSUBMITTED TO IEEE TIP\n12\n[37] H. Zheng, J. Fu, Z. Zha, J. Luo, and T. Mei, “Learning rich part\nhierarchies with progressive attention networks for ﬁne-grained image\nrecognition,” IEEE Trans. Image Process., vol. 29, pp. 476–488, 2020.\n[Online]. Available: https://doi.org/10.1109/TIP.2019.2921876\n[38] W. Luo, X. Yang, X. Mo, Y. Lu, L. Davis, J. Li, J. Yang, and\nS. Lim, “Cross-x learning for ﬁne-grained visual categorization,”\nin 2019 IEEE/CVF International Conference on Computer Vision,\nICCV 2019, Seoul, Korea (South), October 27 - November 2,\n2019.\nIEEE, 2019, pp. 8241–8250. [Online]. Available: https:\n//doi.org/10.1109/ICCV.2019.00833\n[39] Z. Miao, X. Zhao, J. Wang, Y. Li, and H. Li, “Complemental attention\nmulti-feature fusion network for ﬁne-grained classiﬁcation,” IEEE Signal\nProcess. Lett., vol. 28, pp. 1983–1987, 2021. [Online]. Available:\nhttps://doi.org/10.1109/LSP.2021.3114622\n[40] J. Huang, Q. Dong, S. Gong, and X. Zhu, “Unsupervised deep\nlearning by neighbourhood discovery,” in Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019, 9-15\nJune 2019, Long Beach, California, USA, ser. Proceedings of\nMachine Learning Research, K. Chaudhuri and R. Salakhutdinov,\nEds., vol. 97.\nPMLR, 2019, pp. 2849–2858. [Online]. Available:\nhttp://proceedings.mlr.press/v97/huang19b.html\n[41] K. Zhou, Y. Yang, A. Cavallaro, and T. Xiang, “Learning generalisable\nomni-scale representations for person re-identiﬁcation,” CoRR, vol.\nabs/1910.06827, 2019. [Online]. Available: http://arxiv.org/abs/1910.\n06827\n[42] T. Xu, W. Chen, P. Wang, F. Wang, H. Li, and R. Jin, “Cdtrans:\nCross-domain transformer for unsupervised domain adaptation,” CoRR,\nvol. abs/2109.06165, 2021. [Online]. Available: https://arxiv.org/abs/\n2109.06165\n[43] F. Radenovic, G. Tolias, and O. Chum, “Fine-tuning CNN image\nretrieval with no human annotation,” IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 41, no. 7, pp. 1655–1668, 2019. [Online]. Available:\nhttps://doi.org/10.1109/TPAMI.2018.2846566\n[44] T. Isobe, D. Li, L. Tian, W. Chen, Y. Shan, and S. Wang,\n“Towards discriminative representation learning for unsupervised person\nre-identiﬁcation,” CoRR, vol. abs/2108.03439, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2108.03439\n[45] Z. Hu, C. Zhu, and G. He, “Hard-sample guided hybrid contrast learning\nfor unsupervised person re-identiﬁcation,” CoRR, vol. abs/2109.12333,\n2021. [Online]. Available: https://arxiv.org/abs/2109.12333\n[46] D. Cheng, J. Zhou, N. Wang, and X. Gao, “Hybrid dynamic\ncontrast and probability distillation for unsupervised person re-\nid,” CoRR, vol. abs/2109.14157, 2021. [Online]. Available: https:\n//arxiv.org/abs/2109.14157\n[47] D. Wang and S. Zhang, “Unsupervised person re-identiﬁcation via multi-\nlabel classiﬁcation,” in 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19,\n2020.\nComputer Vision Foundation / IEEE, 2020, pp. 10 978–10 987.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-03-01",
  "updated": "2022-03-01"
}