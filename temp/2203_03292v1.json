{
  "id": "http://arxiv.org/abs/2203.03292v1",
  "title": "On Credit Assignment in Hierarchical Reinforcement Learning",
  "authors": [
    "Joery A. de Vries",
    "Thomas M. Moerland",
    "Aske Plaat"
  ],
  "abstract": "Hierarchical Reinforcement Learning (HRL) has held longstanding promise to\nadvance reinforcement learning. Yet, it has remained a considerable challenge\nto develop practical algorithms that exhibit some of these promises. To improve\nour fundamental understanding of HRL, we investigate hierarchical credit\nassignment from the perspective of conventional multistep reinforcement\nlearning. We show how e.g., a 1-step `hierarchical backup' can be seen as a\nconventional multistep backup with $n$ skip connections over time connecting\neach subsequent state to the first independent of actions inbetween.\nFurthermore, we find that generalizing hierarchy to multistep return estimation\nmethods requires us to consider how to partition the environment trace, in\norder to construct backup paths. We leverage these insight to develop a new\nhierarchical algorithm Hier$Q_k(\\lambda)$, for which we demonstrate that\nhierarchical credit assignment alone can already boost agent performance (i.e.,\nwhen eliminating generalization or exploration). Altogether, our work yields\nfundamental insight into the nature of hierarchical backups and distinguishes\nthis as an additional basis for reinforcement learning research.",
  "text": "On Credit Assignment in Hierarchical Reinforcement Learning\nJoery A. de Vries1, 2\nThomas M. Moerland2\nAske Plaat2\n1Algorithmics Group , TU Delft , Delft, The Netherlands\n2Leiden Institute of Advanced Computer Science , Leiden University, Leiden, The Netherlands\nAbstract\nHierarchical Reinforcement Learning (HRL) has\nheld longstanding promise to advance reinforce-\nment learning. Yet, it has remained a considerable\nchallenge to develop practical algorithms that ex-\nhibit some of these promises. To improve our fun-\ndamental understanding of HRL, we investigate\nhierarchical credit assignment from the perspective\nof conventional multistep reinforcement learning.\nWe show how e.g., a 1-step ‘hierarchical backup’\ncan be seen as a conventional multistep backup\nwith n skip connections over time connecting each\nsubsequent state to the ﬁrst independent of actions\ninbetween. Furthermore, we ﬁnd that generalizing\nhierarchy to multistep return estimation methods\nrequires us to consider how to partition the envi-\nronment trace, in order to construct backup paths.\nWe leverage these insight to develop a new hierar-\nchical algorithm HierQk(λ), for which we demon-\nstrate that hierarchical credit assignment alone can\nalready boost agent performance (i.e., when elimi-\nnating generalization or exploration). Altogether,\nour work yields fundamental insight into the na-\nture of hierarchical backups and distinguishes this\nas an additional basis for reinforcement learning\nresearch.\n1\nINTRODUCTION\nHierarchical Reinforcement Learning (HRL) is often re-\ngarded as an open frontier in RL for developing more\nsample-efﬁcient control algorithms [Sutton et al., 1999, Di-\netterich, 1998, Bakker and Schmidhuber, 2004, Sutton and\nBarto, 2018, Levy et al., 2018, Kulkarni et al., 2016]. Hierar-\nchy provides innate structure for solving complex problems\nby decomposing tasks into smaller, simpler and recurring,\nsubtasks. In turn, this allows the decision making algorithm\nto reason or plan over temporally distant events instead of\nonly the (arbitrarily granular) environment actions.\nA vast body of literature has documented and demonstrated\npotential beneﬁts of hierarchy. There have also been a few\nimpressive empirical results (e.g., [Jaderberg et al., 2019]).\nDespite all this, it has remained difﬁcult to design and work\nwith hierarchical agents due to the additional challenges\nthat these methods introduce. In essence, in HRL we of-\nten attempt to design algorithms that try to unite multiple,\npossibly non-stationary and unstable, policies into a more\nefﬁcient whole. Research is occasionally hindered by issues\nrelating to training instability, goal misspeciﬁcation, or even\na collapse of the hierarchy to a ﬂat agent [Vezhnevets et al.,\n2017, Sutton et al., 1999, Nachum et al., 2018]. While nu-\nmerous methods effectively patch such encountered issues\nad-hoc, a better fundamental understanding of the learning\ndynamics will be crucial in advancing HRL research.\nThe concept of learning in RL relates to the policy evalua-\ntion problem: the estimation of the expected future return\nfor a particular policy. Numerous successful return estima-\ntion algorithms have been proposed for ﬂat agents, such as\nTD(λ) or Retrace [Sutton and Barto, 2018, Munos et al.,\n2016]. However, it is not directly obvious how these meth-\nods translate to the hierarchical setting. Hierarchical policies\noperate on varying levels of time granularity, and essentially\ncan ‘jump’ over ﬂat actions when backing up rewards.\nWe take as a practical example the hierarchical Q-learning\nalgorithm by Levy et al. [2018] and show how ﬂat reward\nestimation methods can be adapted to hierarchy. We do this\nfor the Tree-Backup operator [Sutton and Barto, 2018] and\nan adaptation of Watkin’s Q(λ) [Watkins and Dayan, 1992],\nwe generalize this into a new algorithm called HierQk(λ)\nwhere k refers to the number of hierarchy levels. Finally, we\nanalyze these methods in environment domains where we\nisolate the beneﬁt of hierarchy to just the credit assignment,\nin order to compare the hierarchical agents to similarly\nformulated ﬂat approaches.\nIn short, we make the following contributions: 1) we sys-\nPreprint. Under Review.\narXiv:2203.03292v1  [cs.LG]  7 Mar 2022\ntematically study hierarchical credit assignment patterns,\nand compare them to ﬂat back-ups, 2) we propose a new\nalgorithm, Hierarchical Q(λ), which integrates these in-\nsights with Tree-Backup, and 3) we empirically compare\nthe performance of this algorithm on a range of tasks, which\nshows hierarchy provides a fundamental performance ben-\neﬁt over ﬂat agents through reward assignment alone (i.e.,\nwhen removing other beneﬁts of hierarchy provided by gen-\neralization, exploration, or transfer).\n2\nRELATED WORK\nThe pursuit for Hierarchical structure in Reinforcement\nLearning agents has long been motivated by both philo-\nsophical considerations and promising results [Wen et al.,\n2020, Dietterich, 1998, Kulkarni et al., 2016, Bakker and\nSchmidhuber, 2004, Pertsch et al., 2020, Nachum et al.,\n2018] and is perhaps most well known under the Options\nframework of Sutton et al. [1999]. Though numerous RL\nalgorithms have been designed that leverage beneﬁts of hier-\narchy, such studies often put more emphasis in getting their\nmethod to work rather than to gain a better understanding\nof the underlying dynamics.\nOur work takes as a running example a recent hierarchical\nQ-learning algorithm by Levy et al. [2018], which illus-\ntrated strong performance in both discrete and continuous\nenvironments coupled with neural networks for function\napproximation. This method is similar to earlier work by\nBakker and Schmidhuber [2004], the difference is the usage\nof neural networks and assuming that each observable state\ncan be a goal state. Though their method can work well, it\nremained unclear what kind of beneﬁts multiple levels of\nhierarchy could bring forth (as also mentioned by some of\nthe reviewers in the OpenReview ICLR-2019 submission of\nLevy et al. [2018]). This unclarity can be directly answered\nthrough our interpretation of hierarchical credit assignment.\nA formulation for hierarchical agents that is similar to the\nmultistep methods we develop in this paper is by Jain and\nPrecup [2018], who extended the option framework to gen-\neral return estimation operators (such as, Retrace [Munos\net al., 2016], or Tree-Backup). They do this using an intra-\noption framework for updating their policies. However, their\nmethod was limited to the fact that individual levels could\nnot train independently of one another, and it remained un-\nclear whether their hierarchical structure actually provided\nany substantial beneﬁts to conventional agents.\n3\nBACKGROUND\nWe consider solving Markov Decision Processes (MDP)\nwhich are 5-tuples M = ⟨S, A, p, R, γ⟩where S and A\nare a set of states and actions; p : S × A →P(S) is\na stationary transition distribution; R maps transitions to\nrewards R : S × A × S →R which we abbreviate as\nRt = R(St, At, St+1); and γ ∈[0, 1] is a discount factor\n[Sutton and Barto, 2018]. Let π : S →P(A) denote a\ncontrol policy from which we can generate experiences1\nin M: τt:T :k = {St+ki, At+ki, Rt+ki+1}T −t\ni=0 , k = 1. Then\nwe seek the optimal policy π∗that maximizes the value\nfunction,\nqπ(s, a) = Eπ,p[Gt:T |St = s, At = a]\n(1)\n= Eπ,p[\nT −t\nX\ni=0\nγiRt+i+1|St = s, At = a],\nwhich, in practice, is estimated from samples Qπ ≈qπ.\nA famous algorithm that learns qπ from data is Q-\nlearning [Watkins and Dayan, 1992]. At any transition\nSt, At, Rt+1, St+1 it computes a biased estimation error\nusing the recursive Bellman property of Qπ,\nδt = Rt+1 + γ max\na\nQ(St+1, a) −Q(St, At).\n(2)\nThis error term is then used to adjust the current estimate\nto qπ using the online update rule Qπ = Qπ + αδ, where\nα ∈(0, 1] is a step-size parameter. Under some technical\nassumptions, Q-learning will converge to π∗in the limit of\ninﬁnite data [Watkins and Dayan, 1992].\n3.1\nMULTISTEP BACKUPS\nThough Q-learning may converge to π∗eventually, this\nmethod converges quite slowly as the estimation error δ\nis considered for 1-step transitions only. More clever algo-\nrithms leverage experience gathered over multiple timesteps,\ne.g., Tree-Backup (TB(n)) generalizes Q-learning by updat-\ning towards\nδt:t+n =\nn−1\nX\nk=0\nδt+k\nk\nY\ni=1\nγπ(At+i|St+i),\n(3)\nwhich essentially computes n single-transition errors δt+k\nand sums them according to their policy probabilities [Sut-\nton and Barto, 2018]. If we were to utilize a greedy target\npolicy π∗, then Tree-Backup simply sums the rewards n\nsteps along the trace for as long as the actions are always\ngreedy w.r.t. Qπ.\nFor online learning, it should be obvious that TB(n) also\nincurs a time-delay for updates as we need to wait for sub-\nsequent transitions. This consequence is inherent to this\ntype of forward view method: at St we are looking n-steps\nahead in time to construct δt:t+n. It is also possible to ex-\npress multistep updates using a backward view where we\n1Notice that we make use of time-slicing for sequences accord-\ning to the Python convention. For example τt ≡τt:T :k means: the\nsequence τ starting at t up until T with jumps of k.\ncast only the current estimation error δt towards our Qπ\nestimates for previous experiences. Consider an eligibility\ntrace z(s, a) ∈[0, 1], ∀(s, a) (by default set to zero), where\nwe keep track of past transitions by updating the recency\nvalues,\nz(s, a) =\n(\n1,\n(s, a) = (St, At)\nγλπ(At|St)z(s, a),\n(s, a) ̸= (St, At)\n(4)\nwhere λ ∈[0, 1] is a decay parameter that governs a bias-\nvariance trade-of. This particular update rule for z(s, a) is\nknown as the replacing trace. We can then update Qπ with,\nQπ(s, a) = Qπ(s, a) + αδtz(s, a),\n∀(s, a)\n(5)\nwhich yields the Tree-Backup(λ) algorithm for general pol-\nicy learning or Watkin’s Q(λ) for learning π∗[Munos et al.,\n2016, Sutton and Barto, 2018].\n4\nHIERARCHICAL Q-LEARNING\nMultistep reward algorithms considerably improve over 1-\nstep methods, yet, policy learning can still become arbitrar-\nily slow. For example, when no rewards are observed or\nwhen Tree-Backup prematurely truncates backups when the\nbehaviour policy diverges from the target policy [Munos\net al., 2016]. Hierarchical Q-learning must handle credit\nassignment differently due to its recursive structure that de-\ncomposes the full MDP task into distinct subtasks — it must\nestimate returns for each task-specialized policy and handle\nmultiple time resolutions.\nEach specialized policy is updated to maximize a pseudo-\nreward for reaching its goal state s ∈S. Thus, instead of\nsearching for one policy that maximizes qπ, we require a set\nof policies Π = {π⟨s1⟩, π⟨s2⟩, . . . , π⟨s|S|⟩} that each max-\nimize their respective value function. We consider binary\npseudo-rewards of the form rt ≡1St, which is also known\nas the Successor Representation pseudo-reward [Dayan,\n1993]. Alternatively, we could write this as a vector of ze-\nros with a one at the index corresponding to state St, i.e.,\nthe Kronecker delta (rt)i = 1Si=St. As a result, we are\nguaranteed to observe a reward (intrinsically) at every state\ntransition. So, when we do observe an environment (ex-\ntrinsic) reward, we can leverage the knowledge contained\nwithin Π to follow this new gradient of interest.\nFrom the structure of Π we can deﬁne policies over policies\n(akin to Options [Sutton et al., 1999]) in order to sample\ngoals to reach, the agent can then stipulate a trajectory of\ngoals that maximize the environment reward (see Figure 1).\nHierarchical Q-learning does this in recursive fashion, as\ndepicted in Figure 1 and Algorithm 1. The method creates\na tree of k goal-sampling policies Πi, i = 0, . . . , k −1 that\neach instruct their lower level2 (direct child node) to achieve\n2For notation, we denote a hierarchy level i with subscripts.\nFor overloaded indices (e.g., time), we use superscripts (i).\nFigure 1: Example structure of the hierarchical task decom-\nposition with k = 3 levels of hierarchy. Given the ﬁnal goal\nST , the top level i = 2 sequentially samples the two sub-\npolicies at i = 1 that effectively stipulate a goal-trajectory\ntowards ST . These sub-policies in turn perform a similar pro-\ncedure, but conditioned on a different sub-goal {ST −2, ST }\nand on a more granular time-scale. Finally, the ﬂat i = 0\npolicies take environment actions to achieve their instructed\ngoal (red arrows).\nAlgorithm 1: Hierarchical policy training procedure.\nInput :Hierarchy size k and horizon Hi for each level i\n1 Initialize πi for each level i = 0, . . . , k −1\n2 Initialize S and Sgoal\n3 while S ̸= Sgoal do\n4\nS ←Recurse (k −1, S, Sgoal)\n5 Function Recurse (i, S, S(i)\ngoal):\n6\nSet counter n ←0\n7\nwhile n < Hi and S ̸= S(j)\ngoal, ∀j, j ≥i do\n8\nSample action A(i) ∼πi(a | S, S(i)\ngoal)\n9\nif i > 0 then\n10\nS′ ←Recurse (i −1, S, A(i))\n11\nelse\n12\nApply A(0), observe transition S′, R\n13\nUpdate Πi, i = 0, . . . , k −1 {e.g., Alg. 2}\n14\nn ←n + 1, S ←S′\n15\nreturn S\nsome state in ﬁnite time (e.g., in Hi steps). In other words,\nfor any πi ∈Πi, its action space is Ai ⊆Πi−1, i > 0,\nwith A0 ≡A, and its episode horizon is Hi. The hierarchy\nprunes branches when nodes exceed their budget Hi or when\na goal state is achieved (at any level). As a result, any level\ni > 0 is semi-Markov with a maximum atomic horizon (the\ntime-span of executed actions) of Ha\ni = Qi−1\nj=0 Hj [Sutton\net al., 1999]. For k = 1 we collapse to a ﬂat agent, making\nthis formulation a recursive generalization to ﬂat RL.\n4.1\nHIERARCHICAL POLICY EVALUATION\nA desirable property of Algorithm 1 is that it allows us to\nupdate all policies (that is for each hierarchy level) after\nevery environment step (line 13). In contrast to conventional\nupdate mechanisms (such as TB(λ)), hierarchical actions do\nnot have to be completed when performing updates. Levy\net al. [2018], showed that we can relabel hierarchical actions\n(in hindsight) as the states that were reached rather than\nthose that were instructed. This philosophy leads to a dense\nand counterfactual learning mechanism in the sense that:\nif the current state had been an instructed goal, then this\nwould have been a valid hierarchical action at previously\nencountered states (and optimal if the current state was a\ngoal state).\nThe general idea is illustrated in Figure 2 in a forward view\nfor (a, b, c) and a backward view in (d). Given some environ-\nment trace, the conventional way of looking at a multistep\n(n-step; Equation 3) backup is to consider n environment\nactions being applied sequentially (Figure 2a). In contrast,\nthe hierarchical action relabelling implies that, counterfac-\ntually, a state St could have been a goal-action at each of\nthe preceding Ha\ni states— i.e., the states that lie within the\nhierarchical policy’s atomic horizon. As an example, for\nHa\ni = 2, a 1-step update implies that the set {St−1, St−2}\ncontains all valid preceding states for the action St.\nHierarchical agents can update on paired state\nevents (St, St+j), ignoring events inbetween.\nWhen we consider hierarchical n-step backups through this\nlens, we can see that the number of backup paths grows\nquite swiftly (as shown for n = 2 in Figure 2c; now, there\nare 4 backup paths of length n). We can actually show\nthat the number of all possible backup paths grows super-\nexponentially with respect to k, n, Hi and t (for more details,\nsee Appendix A). Hence, for online learning, considering all\npossible backup paths will quickly become infeasible, how-\never, most of these paths will show overlap. As illustrated\nby the backward view in Figure 2d, looking back from the\ngoal-tile, it doesn’t make much sense to perform a n = 2\nbackup from ST −1 to ST −2 when ST −2 lies within the Ha\ni\nhorizon, we can jump directly from ST to ST −2.\nFor this reason, we suggest to sparsify the multistep backups\nby only considering those with maximum valid length. To\nview the effect of this sparsiﬁcation in Figure 2c,d, imagine\npruning all white arrows that do not make time-jumps of\nlength Ha\ni (maximum valid time-span). This is somewhat\nheuristic of course, but it makes the return computation\ntractable and incentivizes the hierarchical policies to take\nactions that make maximal use of their action-budget. Natu-\nrally, if we’d consider backup paths of Ha\ni = 1, we would\nend up with a ﬂat backup and effective policy.\n d)\n a)\n b)\n c)\nFigure 2: Comparison of a ﬂat n = 2 backup at i = 0 (a),\na hierarchical n = 1 and Ha\n1 = 2 backup at level i = 1\n(b) and the set of all possible backups for a hierarchical\nn = Ha\n1 = 2 backup (c) along with its backward view (d)\nboth at level i = 1. The backups are all conditioned on,\nand all utilize a shared trace towards, the light-blue, dashed,\ngoal-tile ST . White arrows indicate that the backup paths\nare inferred through partitioning of the trace (multistep)\nwhereas the red arrows indicate states that are reachable for\nthe hierarchy (1-step) starting from the reference state (black\ndots). Heuristically, we could sparsify the white arrows by\nonly considering those with maximum time-span: Ha\ni = 2.\n4.2\nALGORITHM FORMULATION\nDenote k estimates for qπ at each level in the hierarchy as\nQi ⊂R|S|×|Ai|×|S|, i = 0, . . . , k −1. Then, observe that\nfor a hierarchical policy πi ∈Πi, i > 0 at state St we can\nlook forward on the trace to ﬁnd the set of reachable actions\nA(i)\nt\n= {St+j}Ha\ni\nj=1. If we extend the 1-step update from\nEquation 2 to consider the hierarchical time skips and all\nspecialized goal policies contained in Πi, we get:\nδj\nt = rt+j + γt+jEπQi(St+j, ·, g) −Qi(St, A(i)\nt+j−1, g),\n= Gj\nt −Qi(St, A(i)\nt+j−1, g)\n(6)\nwhere A(i)\nt+j−1 ≡St+j, i > 0 and EπQi is short-hand for\nthe expectation of Qi under the target policy π. Note that\nδj\nt ∈R|S| is now a vector of errors for each policy πi ∈Πi,\nthis is made explicit by the goal-vector g. Accordingly, the\nvector rt+j = (1 −γt+j)/γ = 1St+j is the state indicator\nas discussed before and bold γ is a termination function\nthat ends (restarts) episodes when a St-specialized policy\nachieves St. If we then map the set of state-action pairs\n{St}×A(i)\nt\nto the set of corresponding update targets ∆(i)\nt\n=\n{δ1\nt , . . . , δHa\ni\nt\n} using a greedy target policy π∗, we get the\nupdate target for HierQk [Levy et al., 2018].\nHierTBk(n)\nWe can extend the single step update targets in ∆(i)\nt\nto\nmultistep ones through the Tree-Backup operator. To reduce\nAlgorithm 2: Hierarchical Q(λ)/ HierQk(λ)\nInput : Environment trace τt+1 = {Sj, Aj}t+1\nj=0, the\nhierarchy level i, eligibilities Z(i), a target policy π,\nand Q-table Qi(s, a, g)\nParameters : Discount factor γ ∈[0, 1), step-size\nα ∈(0, 1], decay λ ∈[0, 1], and policy reach Ha\n1 Let h ←t mod Ha and tmin ←min(Ha −1, t)\n2 Infer level action A ←A(0) or St+1\n3 G ←rt+1 + γt+1EπQi(St+1, ·, g)\n4 δ ←G −Qi(St−tmin, A, g)\n5 Z(i)\nh (s, a, g) ←λγt−tminπt−tminZ(i)\nh (s, a, g), ∀(s, a)\n6 Qi(s, a, g)\n←Qi(s, a, g) + αδZ(i)\nh (s, a, g), ∀(s, a)\n7 for j = 0, . . . , min(Ha −1, tmin) do\n8\nZ(i)\nh (Stmin−j, A, g) ←1\n9\nQi(Stmin−j, A, g)\n←\n(1 −α)Qi(Stmin−j, A, g) + αG\nclutter, denote h = Ha\ni , then we can write,\nδj\nt:t+nh:h =\nn−1\nX\nk=0\nδj\nt+kh\nk\nY\nl=1\nγt+j+lhπ(A(i)\nt+j+lh|St+j+lh, g)\n=\nn−1\nX\nk=0\nδj\nt+kh\nk\nY\nl=1\nγt+j+lhπt+j+lh,\n(7)\nwith A(i)\nt+j+lh\n≡St+j+(l+1)h, i > 0, as the general\nHierTBk(n) error for each Πi, i = 0, . . . , k −1 (with cor-\nrected h) and for any well-deﬁned target policy π. Our spar-\nsiﬁcation can be observed from the time-jumps towards the\nfarthest allowed state on the trace from each reference state,\ni.e., from St towards St+Ha\ni as indicated by the subscripts\n(see also Figure 2c,d for a visual reference; all white arrows\nwith span < Ha\ni get pruned). Our implementation of Tree-\nBackup performs hierarchical updates with a similar time\ncomplexity as conventional Tree-Backup. This is a drastic\nimprovement over any naive implementation, which would\nbe of exponential time-complexity (see Appendix A.1).\nHierQk(λ)\nLet us ﬁrst deﬁne the eligibility trace as the matrix Z(i) ⊆\n[0, 1]|S|×|Ai|×|S| for each level i = 0, . . . , k −1. So, each\ncolumn in Z(i) tracks an individual eligibility trace (c.f.,\nEquation 4) for every separate policy in Πi. Denote the\nset of valid preceding states S(i)\nt\n= {St−j}Ha\ni\nj=0 with valid\naction A(i)\nt , we can then write the update for Z(i), for each\nelement (s, a) ∈Ω≡S(i)\nt\n× {A(i)\nt }, according to,\nZ(i)(s, a, g) =\n(\n1,\n(s, a) ∈Ω\nλγkπkZ(i)(s, a, g),\nOtherwise\n(8)\nwhere πk contains the transition (state-action) probabilities\nfor each goal, γk terminates achieved state-goals, and k ∈\n{t, t −1, . . . , t −Ha\ni + 1}. Like before (the sparsiﬁcation\nfrom hierarchical Tree-Backup) we can assume that the\npolicy always takes actions with maximal temporal span\nπk =⇒π(A(i)\nt |St+1−Ha\ni , g).\nIt turns out, to get an exact generalization of our version\nof Hierarchical TB(n) to hierarchical TB(λ), we need\nto keep track of Ha\ni disjoint eligibility matrices Z(i)\nh\nfor\nh = 1, . . . , Ha\ni in order to correctly track the sparsiﬁed\nback-up paths for each hierarchy level. We can then cir-\nculate through each eligibility matrix by only utilizing the\neligibility Z(i)\nh , h = t mod Ha\ni at timestep t. This idea is\nillustrated in Algorithm 2, which in the case of a greedy tar-\nget policy π yields a hierarchical generalization of Watkin’s\nQ(λ), which we dub HierQk(λ). In the pseudocode, we\nkeep the 1-step updates separate from the eligibility trace\nupdate initially (line 9; Algorithm 2), seeing as each ele-\nment (s, a) ∈Ωinduces a different error δ even though they\nshare the same returns G. Of course, this is only relevant for\nthe 1-step errors, after which these trailing pairs are simply\nadded to the eligibility trace (line 8; Algorithm 2).\n5\nEMPIRICAL EVALUATION\nFrom our formulation of the hierarchical backup, along with\ntheir algorithmic implementations, we can see how hier-\narchical credit assignment differs from conventional (ﬂat)\ncredit assignment in a number of ways. Most notably, the\nhierarchy induces skip-connections over the environment\ntrace when computing returns. As a result, we don’t need to\naccount for correction terms at each environment transition\n(without decaying, discounting, or truncation), which allows\nus to propagate rewards much further back in time.\nIt is a well known phenomenon that conventional Tree-\nBackup (or Watkin’s Q(λ)) truncates traces too often due\nto possible divergence between behaviour policies and the\ntarget policy [Munos et al., 2016, Peng and Williams, 1996,\nKozuno et al., 2021]. Naturally, this trace truncation is for\ngood reason from a conservative perspective: we would oth-\nerwise be estimating a different policy. A similar reasoning\napplies to the λ parameter for the eligibility trace and the\nparameter n for the multistep return, which both interpo-\nlate between having a more biased estimate of the return\nor one with potentially high variance. When rewards are\npropagated far backwards in time, the resulting update can\nbecome arbitrarily noisy and unstable.\nAll in all, this may raise the question whether a hierarchi-\ncal backup only yields a beneﬁt (or detriment) to the agent\nthrough deeper reward propagation (see also Appendix B for\na direct comparison of a ﬂat and hierarchical agent trained\non the same trace). Hypothetically, we could achieve a simi-\nlar effect of deeper reward propagation by just increasing n\n10x10 Gridworld\n20x20 Gridworld\n4-rooms 5-to-1\n9-rooms 5-to-1\n10x10 Maze\n20x20 Maze\nBenchmark environments.\nFigure 3: Overview of the tested environments. The agent starts out at the blue tile and is rewarded at the green tile.\nor γ, λ for the ﬂat agent (not exactly of course, due to the\naforementioned truncation issue). Thus, we ran experiments\nto analyze the difference in behaviour and performance of\nthese agents over various backup parameters.\nEXPERIMENTAL SETUP\nWe evaluated the Hierarchical Tree-Backup and Q(λ) im-\nplementations from Algorithm 3 and Algorithm 2 over two\nseparate parameter grids on each of the discrete gridworld\nenvironments portrayed in Figure 3 (See also Appendix C\nfor additional experiment details). Each environment was\nchosen to provide either a distinct structure to the state-\naction spaces or to their scale. We opted for a tabular setting\nto eliminate most other beneﬁts posed by hierarchy [Wen\net al., 2020], e.g., to eliminate possible generalization be-\ntween goals or goal-directed exploration. The agents were\ninitialized (with Qi = 0, ∀i) at the blue-tile and could tra-\nverse the environment deterministically with the actions\nA(0) = {↑, ↓, ←, ↑}. Upon reaching the green-tile, the\nagents received a sparse binary reward.\nThe ﬁrst, and largest, parameter grid evaluated our algo-\nrithms over various hierarchy levels k ∈{1, 2, 3, 4}, backup\nsteps n ∈{1, 3, 5, 8}, decay values λ ∈{0, 0.5, 0.8, 1}, and\nbehaviour policies {Πk−1, π0} (only during training). This\nstudy aimed to quantify the marginal performance beneﬁt\nof evaluating and training with additional hierarchy for vari-\nous reward backup depths. The second parameter grid was\nevaluated for k ∈{1, 2, 3}, behaviour policies {Πk−1, π0}\n(only during training), and decay λ = 1, with derived param-\neters γ = (γ0)1/Ha\nk−1 and n = n0/Ha\nk−1. These formulas\nwere intended to adjust the discount and backup parame-\nters such that every hierarchy level k in this ablation study\nsent credits back equally far. For a complete overview of\nall parameters and their descriptions, see Table 7 in the sup-\nplementary material. We utilized ϵ-greedy exploration with\nϵ = 0.25 for all ﬂat policies k = 1 (i = 0) during training\nand ϵ = 0.05 during evaluation. All hierarchical polices\nutilized a fully greedy policy ϵ = 0 (uniform tie-breaking)\nsuch that exploration was mostly handled by the ﬂat level.\nThis choice for hierarchical exploration is well motivated in\nour case seeing as uniform random exploration compounds\non each level, Pr(Random-Policy) = 1 −(1 −ϵ)k. Con-\nsidering the fact that hierarchical policies sample actions\nthat carry over multiple time-steps, uniformly random ex-\nploration would result in erratic behaviour of the effective\nenvironment policy — especially as the dimensionality of\nthe action-space increases. Of course we could have opted\nfor a different exploration policy (e.g., Boltzmann explo-\nration), but this would have more strongly confounded our\nresults due to more efﬁcient hierarchical exploration.\nAll agent conﬁgurations were evaluated over 200 random\nseeds on each environment utilizing a simple alternating\ntrain-test loop for 50 iterations (after which most agents\nhad converged). Training episodes were terminated and re-\nset when the agent exceeded a budget of 105 steps, this\nwas sufﬁciently large to let every agent conﬁguration ob-\nserve a reward in their ﬁrst episode. We aggregated the test-\nperformance, measured in the number of log(steps) needed\nto reach the green-tile starting from the blue-tile, over all\nrepetitions per time-step to produce loss-curves. We chose\nto log-transform the step-counts to stabilize the variance\nof the means. Finally, we averaged over the loss-curves to\nquantify the marginal performance, this could be interpreted\nas an unnormalized area under the mean performance curve.\nTable 1: Example distribution statistics for the trace length\nof the ﬁrst training episode (the ﬁrst observed reward). The\nmean µ can roughly be interpreted as the scale β of an expo-\nnential distribution, Exp(β−1). Additional hierarchy levels\nseem to prolong the ﬁrst episode in the 20 × 20 Gridworld,\nhowever this pattern is not monotonic and seems to reverse\nat k > 2 on the 10 × 10 Maze.\nBackup\nn or λ\nLevels k\nµ ± s−:\n20 × 20\nGridworld\nµ ± s−:\n10 × 10 Maze\nn = 3\nk = 1\n3064 ± 210\n2493 ± 171\nk = 2\n5176 ± 394\n5000 ± 555\nk = 3\n8010 ± 600\n2573 ± 247\nk = 4\n6563 ± 628\n1681 ± 143\nλ = 0.8\nk = 1\n3300 ± 189\n2525 ± 187\nk = 2\n6528 ± 436\n6318 ± 665\nk = 3\n8254 ± 663\n2914 ± 267\nk = 4\n8406 ± 788\n1663 ± 143\n1\n2\n3\n4\n1\n2\n3\n4\n3\n4\n5\n10x10 Gridworld\n1\n2\n3\n4\n1\n2\n3\n4\n3\n4\n5\n4-rooms 5-to-1\n1\n2\n3\n4\n1\n2\n3\n4\n3\n4\n5\n6\n10x10 Maze\n1\n2\n3\n4\n1\n2\n3\n4\n4\n5\n6\n7\n20x20 Gridworld\n1\n2\n3\n4\n1\n2\n3\n4\n4\n5\n6\n7\n8\n= 0.0\nn = 1\n= 0.5\nn = 3\n= 0.8\nn = 5\n= 1.0\nn = 8\n9-rooms 5-to-1\n1\n2\n3\n4\n1\n2\n3\n4\n4\n6\n20x20 Maze\nNumber of Hierarchies\nAverage number of log(steps) per episode\nNumber of steps per episode\nAveraged over first 50 episodes and 200 runs.\nFigure 4: Marginal log-performance of each experiment conﬁguration for each environment from Figure 3 (lower is better).\nAll agents utilized a base-discount of γ = 0.95. The conﬁgurations are split left for the eligibility trace agents (Algorithm 2)\nand right for Tree-Backup agents (Algorithm 3) as indicated by the colored annotations. Shaded regions indicate ½-standard\nerrors of the mean and grey-dashed lines indicate optimal performance per environment.\nEXPERIMENT RESULTS\nThe mean results (with standard errors) for the ﬁrst parame-\nter grid, conditioned on the hierarchical training policy, are\nvisualized fully in Figure 4. Generally speaking, the pattern\nindicates that hierarchy near-monotonically improves upon\nits ﬂat counterpart k = 1 for any parameter setting (in ex-\npected log-score). Though, hierarchy generally did indeed\nimprove upon the ﬂat counterparts, it also exhibited much\nhigher variance; in the ﬁrst number of episodes we found\nthat hierarchy could actually degrade performance. Regu-\nlarly, the hierarchical agents would take a marginally longer\ntime to ﬁnish their ﬁrst few training episodes compared to\nthe ﬂat agents. This is also substantiated by the results in Ta-\nble 1 (and Table 3 in the Supplementary Material). The table\nindicates that generally, additional hierarchy levels prolong\nthe ﬁrst episode. However, this pattern occasionally jumps\nback when the hierarchy level is taken to an extreme, which\nis especially visible on the 10 × 10 Maze environment.\nIt is also noteworthy that the ﬂat λ = 1 agents consis-\ntently performed in a competitive manner to the hierarchical\npolicies, especially on the small environments. The environ-\nments were deterministic after all, and these agents were\ncapable of sending rewards back the farthest of the ﬂat poli-\ncies. As also noted in Table 3, these agents generally spent\nthe least amount of time wandering about in the ﬁrst episode.\nTo analyze the effect of reward backup depth on the quality\nof the resulting policy, it must be noted that the results in\nFigure 4 are confounded by the fact that the training data\nwas generated by entirely different policies for the ﬂat and\nhierarchical agents (as should be evident from Table 3). For\na more fair comparison to the effect of reward backup depth,\nFigure 5 (see also Figures 9,10 in the Supplementary Ma-\nterial) visualizes the mean performance of the agents over\ntime/ training episodes for two of the tested environments\n(chosen for visual clarity). Still, this result shows a similar\npattern as observed in Figure 4: even with appropriately\nbalanced backup depths and training either with or without\nthe hierarchical structure, the positive effect on performance\ndue to hierarchy seems to persist. This result also shows that\nthe test scores for the hierarchical agent trained with the ﬂat\nbehaviour policy improved marginally slower in the large\nGridworld and faster in the small Maze environments. We\nalso see that the ﬂat agent k = 1 (red line) performed worse\nduring evaluation than both hierarchical policies.\n6\nDISCUSSION\nThough our results may hint that hierarchy supersedes con-\nventional, ﬂat, agents, they often exhibited larger variance\nin the ﬁrst few episodes. Also rewards were not always ob-\nserved as swiftly by the hierarchical agents compared to\nthe ﬂat agents. These results are actually an artifact of our\nchoice of policy parameterization, ϵ-greedy for level i = 0\nand fully greedy for i > 0. For the conventional agents, as\nlong as no reward was observed the agent would do an uni-\nk = 1,\n= 0.983,\n= 1.0,b =\n0\nk = 3,\n= 0.857,\n= 1.0,b =\n0\nk = 3,\n= 0.857,\n= 1.0,b =\nk\n1\nk = 1,\n= 0.983,n = 9,b =\n0\nk = 3,\n= 0.857,n = 1,b =\n0\nk = 3,\n= 0.857,n = 1,b =\nk\n1\nAverage number of log(steps)\nNumber of steps per episode. Averaged over 200 runs.\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n3.6\n3.8\n4\n4.2\n4.4\n20x20 Gridworld\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n3\n4\n6\n10x10 Maze\nNumber of Episodes\nFigure 5: Average log(score) of the evaluated agents at each\ntraining episode (lower is better) when credit assignment\ndepth is appropriately balanced for each hierarchy level k\nand when training data is generated with either a ﬂat policy\nb = π0 or with the full hierarchy b = Πk−1. Data for the\nhierarchical agent was generated with two different policies\nto account for the observation that hierarchy could prolong\nepisodes (as noted in Table 1). Shaded regions indicate 1-\nstandard error of the mean.\nform random walk. For the hierarchical agent, this was also\nthe case for i = k −1 (the top-level). However, all levels\nbelow k −1 would swiftly learn to follow their instructed\ngoals as a result of their state-specialization; they became\nexperts in reaching the proposed goals by level i = k −1\neven though this agent was still random. As a result, the\nagent became biased towards the states for which it could\nquickly accumulate knowledge. The top-level could then\nrandomly sample a state, far from the goal-state, and the\nlevels below would naively, skillfully, follow.\nThis issue could of course have been handled through alter-\nnative hierarchical exploration methods, however, for a fair\ncomparison to the conventional agent we chose to simply\ngenerate training data with a ﬂat policy (as shown in Fig-\nure 7 in the Appendix). This again showed that hierarchy\ngenerally improves upon the ﬂat agents, though the effect\nseemed to become less pronounced with additional hier-\narchy levels. In a way, our results should emphasize that\nϵ-based exploration becomes more and more unreliable as\nwe decrease the temporal resolution. To effectively leverage\nthe beneﬁts posed by hierarchy, agents should follow a more\nprincipled way for exploration (c.f., [Kulkarni et al., 2016,\nBellemare et al., 2016, Ecoffet et al., 2021]).\nWhen we again juxtapose the deﬁnitions of ﬂat and hier-\narchical multistep backups, the main distinctions that we\ncan make are: how the rewards are sent back and how the\ntrajectory is partitioned in order to perform updates. As men-\ntioned before, Tree-Backup and its variants compute policy\ncorrection terms over state-transitions to estimate the returns\nfor good reason. The return would otherwise be estimating\na different policy, leading to e.g., a SARSA update [Sutton\nand Barto, 2018]. Anecdotally, initial experiments showed\nthat SARSA (or variants thereof) always performed worse\nthan Q-learning in our experimental setting. Thus, it is in-\nteresting that hierarchy does not degrade performance in a\nsimilar way, but rather improves upon it. Possibly, we could\nview hierarchical structure to make Q-learning (or any other\nconservative return operator) amenable to non-conservative\nupdates — i.e., akin to SARSA or Peng’s Q(λ) [Sutton and\nBarto, 2018, Peng and Williams, 1996, Pertsch et al., 2020].\nOf course, we should not attribute the performance beneﬁt\nof hierarchy merely to the update mechanism, but also to\nits innate structure. The recursive decomposition induces a\nform of ‘stitching effect’ between policies that contain some\nknowledge about the task. However, it is not clear if this\neffect can be measurably distinguished from the hierarchical\nbackups.\nFinally, it must be noted that our description for HierQk(λ)\ndoes not yield a practical algorithm. Memory does not\nscale well in the tabular setting, especially when consid-\nering that we track |S| policies at each level. The same\nreasoning goes for the eligibility traces, which in the worst-\ncase scales proportional to O(Ha\nk−1|S|2|A|), seeing as\nZ ∈R|S|×|A(i)|×|S| for which we keep Ha\nk−1 copies. Our\nimplementation adopts some heuristic optimizations in or-\nder to make our experiments tractable (see Appendix D),\nfurther optimizations such as a LIFO buffer or approxima-\ntions to the eligibilities are left as future work. Our version\nof Hierarchical Tree-Backup should be readily applicable\nto hierarchical Q-learning methods even if e.g., (non-linear)\nfunction approximation is used [Levy et al., 2018].\nCONCLUSION\nThis paper takes a fundamental perspective on how credit\nassignment is performed in Hierarchical Reinforcement\nLearning through a direct comparison to multistep return\nestimation methods [Sutton and Barto, 2018]. We take as\na running example the Hierarchical Q-learning method by\nLevy et al. [2018] to illustrate how such an algorithm may\nconstruct estimations of its returns. Our perspective showed\nthat the number of possible ways to construct update targets\nfor hierarchical policies grows rapidly over time but can\nbe sparsiﬁed by focusing only on a couple of heuristically\nchosen paths. We used this insight to extend the original\n1-step method to more general estimation operators like\nTree-Backup and Q(λ). Our experiments again show how\nhierarchy can yield substantial performance gains over their\nﬂat counterparts, even when strictly isolating the beneﬁt\nposed by credit assignment.\nFinally, we conclude that hierarchy opens up an additional\nbasis for research on return estimators. Canonically, almost\nall developed return estimation algorithms consider the se-\nquential Markov Chain generated by some policy. Our hier-\narchical formulation of Tree-Backup and Q(λ) shows that\nthere exists a family of return estimation methods that also\ntake into account the number of ways to partition traces of\nexperiences rather than purely stepping through them.\nAuthor Contributions\nWork done while J.A. de Vries was at LIACS as part of his\nMaster thesis supervised by T.M. Moerland and A. Plaat.\nReferences\nBram Bakker and Jürgen Schmidhuber. Hierarchical Re-\ninforcement Learning Based on Subgoal Discovery and\nSubpolicy Specialization. In Proceedings of the 8-th Con-\nference on Intelligent Autonomous Systems, IAS-8, pages\n438–445, 2004.\nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski,\nTom Schaul, David Saxton, and Remi Munos. Unify-\ning Count-Based Exploration and Intrinsic Motivation.\narXiv:1606.01868 [cs, stat], November 2016.\nURL\nhttp://arxiv.org/abs/1606.01868.\narXiv:\n1606.01868.\nPeter Dayan. Improving Generalization for Temporal Dif-\nference Learning: The Successor Representation. Neural\nComputation, 5(4):613–624, July 1993. ISSN 0899-7667.\ndoi: 10.1162/neco.1993.5.4.613. Conference Name: Neu-\nral Computation.\nThomas G. Dietterich. The MAXQ Method for Hierarchi-\ncal Reinforcement Learning. In In Proceedings of the\nFifteenth International Conference on Machine Learning,\npages 118–126. Morgan Kaufmann, 1998.\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Ken-\nneth O. Stanley, and Jeff Clune.\nFirst return,\nthen explore.\nNature, 590(7847):580–586, Febru-\nary\n2021.\nISSN\n1476-4687.\ndoi:\n10.1038/\ns41586-020-03157-9. URL https://www.nature.\ncom/articles/s41586-020-03157-9.\nNum-\nber: 7847 Publisher: Nature Publishing Group.\nRonald L. Graham, Donald E. Knuth, and Oren Patashnik.\nConcrete Mathematics: A Foundation for Computer Sci-\nence. Addison-Wesley, Reading, 1989.\nMax Jaderberg, Wojciech M. Czarnecki, Iain Dun-\nning, Luke Marris, Guy Lever, Antonio Garcia Cas-\ntañeda, Charles Beattie, Neil C. Rabinowitz, Ari S.\nMorcos, Avraham Ruderman, Nicolas Sonnerat, Tim\nGreen, Louise Deason, Joel Z. Leibo, David Sil-\nver, Demis Hassabis, Koray Kavukcuoglu, and Thore\nGraepel.\nHuman-level performance in 3D multi-\nplayer games with population-based reinforcement learn-\ning.\nScience, May 2019.\ndoi: 10.1126/science.\naau6249.\nURL https://www.science.org/\ndoi/abs/10.1126/science.aau6249.\nPub-\nlisher: American Association for the Advancement of\nScience.\nAyush Jain and Doina Precup. Eligibility Traces for Options.\nIn Proceedings of the 17th International Conference on\nAutonomous Agents and MultiAgent Systems, AAMAS\n’18, pages 1008–1016, Richland, SC, July 2018. Interna-\ntional Foundation for Autonomous Agents and Multia-\ngent Systems.\nTadashi Kozuno, Yunhao Tang, Mark Rowland, Rémi\nMunos, Steven Kapturowski, Will Dabney, Michal Valko,\nand David Abel. Revisiting peng’s q(λ) for modern re-\ninforcement learning. 2021. URL https://arxiv.\norg/abs/2103.00107v1.\nTejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi,\nand Joshua B. Tenenbaum.\nHierarchical Deep Rein-\nforcement Learning: Integrating Temporal Abstraction\nand Intrinsic Motivation. arXiv:1604.06057 [cs, stat],\nMay 2016. URL http://arxiv.org/abs/1604.\n06057. arXiv: 1604.06057.\nAndrew Levy, George Konidaris, Robert Platt, and Kate\nSaenko. Learning Multi-Level Hierarchies with Hind-\nsight. September 2018. URL https://openreview.\nnet/forum?id=ryzECoAcY7.\nRémi Munos, Tom Stepleton, Anna Harutyunyan, and\nMarc G. Bellemare. Safe and Efﬁcient Off-Policy Rein-\nforcement Learning. arXiv:1606.02647 [cs, stat], Novem-\nber 2016.\nURL http://arxiv.org/abs/1606.\n02647. arXiv: 1606.02647.\nOﬁr Nachum, Shixiang Gu, Honglak Lee, and Sergey\nLevine. Data-Efﬁcient Hierarchical Reinforcement Learn-\ning. arXiv:1805.08296 [cs, stat], October 2018. URL\nhttp://arxiv.org/abs/1805.08296.\narXiv:\n1805.08296.\nJing Peng and Ronald J. Williams. Incremental multi-step\nq-learning. 22(1):283–290, 1996. ISSN 1573-0565. doi:\n10.1007/BF00114731. URL https://doi.org/10.\n1007/BF00114731.\nKarl Pertsch, Oleh Rybkin, Frederik Ebert, Chelsea Finn,\nDinesh Jayaraman, and Sergey Levine. Long-Horizon Vi-\nsual Planning with Goal-Conditioned Hierarchical Predic-\ntors. arXiv:2006.13205 [cs, stat], November 2020. URL\nhttp://arxiv.org/abs/2006.13205.\narXiv:\n2006.13205.\nRichard S. Sutton and Andrew G. Barto.\nRein-\nforcement Learning: An Introduction.\nA Brad-\nford Book, Cambridge, Massachusetts, 2 edition,\nNovember 2018.\nISBN 978-0-262-03924-6.\nURL\nhttp://www.incompleteideas.net/book/\nthe-book-2nd.html.\nRichard S. Sutton, Doina Precup, and Satinder Singh.\nBetween MDPs and semi-MDPs: A framework for\ntemporal abstraction in reinforcement learning. Artiﬁcial\nIntelligence, 112(1):181–211, August 1999.\nISSN\n0004-3702.\ndoi: 10.1016/S0004-3702(99)00052-1.\nURL\nhttps://www.sciencedirect.com/\nscience/article/pii/S0004370299000521.\nAlexander Sasha Vezhnevets, Simon Osindero, Tom Schaul,\nNicolas Heess, Max Jaderberg, David Silver, and Ko-\nray Kavukcuoglu.\nFeUdal Networks for Hierarchi-\ncal Reinforcement Learning.\narXiv:1703.01161 [cs],\nMarch 2017.\nURL http://arxiv.org/abs/\n1703.01161. arXiv: 1703.01161.\nChristopher J. C. H. Watkins and Peter Dayan. Q-learning.\nMachine Learning, 8(3):279–292, May 1992. ISSN 1573-\n0565. doi: 10.1007/BF00992698. URL https://doi.\norg/10.1007/BF00992698.\nZheng Wen, Doina Precup, Morteza Ibrahimi, Andre\nBarreto, Benjamin Van Roy, and Satinder Singh. On\nEfﬁciency in Hierarchical Reinforcement Learning.\nAdvances\nin\nNeural\nInformation\nProcessing\nSys-\ntems, 33, 2020.\nURL https://proceedings.\nneurips.cc/paper/2020/hash/\n4a5cfa9281924139db466a8a19291aff-Abstract.\nhtml.\nXingdong Zuo. mazelab: A customizable framework to\ncreate maze and gridworld environments. https://\ngithub.com/zuoxingdong/mazelab, 2018.\nA\nTHE NUMBER OF BACKUP PATHS\nFor multistep hierarchical methods we need to consider how\nto look forward (or backward) on the trace in order to com-\npute an estimate of the value function. Whereas conventional\nmethods do this step-by-step through time, a hierarchical\nbackup can make time ‘jumps’. The backup path is thus de-\nﬁned by the time-deltas, suppose we have collected a trace\nτt up until time t and write the time-deltas as ∆τ — for ﬂat\nRL this is simply {1}t\nj=0. The hierarchical backup needs to\nconsider how to construct ∆τ such that P\nj(∆τ)j = t, this\nis a special case of an integer partitioning problem.\nIn a forward sense, the problem of computing the number\nof all hierarchical backup paths can also be framed as a\ndice rolling problem. The hierarchical environment horizon\nHa\ni deﬁnes the maximum value that each die can take, i.e.,\nit deﬁnes the range of the time-deltas at any level i. Then,\nthe backup parameter n deﬁnes the number of dice that\nwe have. If we now assume n to be sufﬁciently large, and\nwrite h = Ha\ni for shorthand, then the number of possible\nhierarchical backup paths of depth t is given by,\nαt(h, n) =\nn\nX\nj=0\n(−1)j\n\u0012n\nj\n\u0013\u0012t −hj −1\nn −1\n\u0013\n(9)\nwhere\n\u0000n\nk\n\u0001\nis the binomial coefﬁcient.\nProof. The quantity αt(h, n) is the coefﬁcient of the t-th\nmonomial in the power expansion of rolling n die with\nranges 1 to h, this is a result that follows from the binomial\ntheorem (see e.g., Chapter 7 of the book by Graham et al.\n[1989] for a similar treatment on these types of combinato-\nrial problems). This sequence of coefﬁcients is captured by\nthe ordinary generating function (o.g.f.),\nF(x) = (x1 + x2 + · · · + xh)n,\n(10)\n= xn \u0010Ph−1\nj=0 xj\u0011n\nThis can be rewritten using the closed-form solution to the\nﬁnite geometric series,\nF(x) = xn\n\u00121 −xh\n1 −x\n\u0013n\n= xn(1 −xh)n(1 −x)−n.\nWritten as a binomial series, this yields,\nF(x) = xn\n∞\nX\nj=0\n\u0012n\nj\n\u0013\n(−xh)j\n∞\nX\nk=0\n\u0012−n\nk\n\u0013\n(−x)k\n= xn\n∞\nX\nj=0\n(−1)j\n\u0012n\nj\n\u0013\nxhj\n∞\nX\nk=0\n(−1)k\n\u0012−n\nk\n\u0013\nxk,\nThen see that,\n\u0000−n\nk\n\u0001\n= (−1)k\u0000k+n−1\nk\n\u0001\n, which we can use\nto cancel out the other power of −1 (due to the even power\n2k) to give us,\nF(x) = xn\n∞\nX\nj=0\n(−1)j\n\u0012n\nj\n\u0013\nxhj\n∞\nX\nk=0\n\u0012n + k −1\nk\n\u0013\nxk.\nTo extract the coefﬁcient of the monomial with power t\nfrom this o.g.f., note that that the powers in F(x) being\nxn, xhj, xk should together sum to t. In other words, t =\nn + hj + k. If we then also note that the ﬁrst summation\nin F(x) sums inﬁnitely many zeros after j > n because\n\u0000n\nk\n\u0001\n= 0 for k > n. Then, to get the coefﬁcient αt for the\nt-th power in F(x) we can omit the second summation in\nF(x) and simply substitute for k = t−n−hj, which yields,\nαtxt = xn\nn\nX\nj=0\n(−1)j\n\u0012n\nj\n\u0013\nxhj\n\u0012n + k −1\nk\n\u0013\nxt−n−hj,\n= xt\nn\nX\nj=0\n(−1)j\n\u0012n\nj\n\u0013\u0012n + (t −n −hj) −1\nt −n −hj\n\u0013\n.\nThen applying the identity\n\u0000n\nk\n\u0001\n=\n\u0000 n\nn−k\n\u0001\nand dividing the\nabove expression by xt we get the solution in Equation 9.\nThe quantity from Equation 9 is of course for a ﬁxed backup\ndepth, because h can be varied between 1, . . . , h there are\nmultiple possible backup depths. When we use a smaller\nn when computing the multistep returns, then the backup\npaths get truncated after n steps. Then, it follows trivially\nthat the shortest backup path is bounded by n whereas the\nlongest backup path is bounded by hn. We can write the\ntotal number of possible paths to compute returns on as,\nS =\nnh\nX\nd=n\nαd(h, n).\n(11)\nThis of course yields S = hn. The number of hierarchical\nbackup paths grows at least exponentially.\nWith that said, we should also keep in mind that the atomic\nhorizon h ≡Ha\ni too grows exponentially with increas-\ning levels of hierarchy according to the hierarchical ac-\ntion budget, Ha\ni = Qi−1\nj=0 Hj. For example, a hierarchy\nwith k = 3 levels and a policy budget of Hi = 4 for\nall levels i gives Ha\nk−1 = 16. If we then want to do a\nn = 4 multistep update, then in total there are approxi-\nmately Pnh\nd=n αd(16, 4) = 164 ≈65 · 103 possible updates.\nHence, the number of backup paths actually grows super-\nexponentially w.r.t., n, Hi and k. This is not practical for\nany algorithm.\nA.1\nNUMBER OF TREE-BACKUP PATHS\nFor our formulation of hierarchical Tree-Backup (Section\n4.2), we provide the backup parameter n and horizon h when\ncomputing multistep returns. Thus, to compute all possible\nTree-Backup paths, we can simply fall back to Equation 11.\nHowever, this does not take into account the severe overlap\nbetween backup paths, the truncation of the traces due to the\ntarget policy probabilities, and our proposed sparsiﬁcation.\nIn fact, our sparsiﬁcation of the multistep returns are able\nto reduce the quantity S for any n and h to an upper bound\nof just ST B = nh backup paths. Along with some imple-\nmentation optimizations, this can be implemented in O(n)\ntime which is equivalent in time-complexity to conventional\n(ﬂat) Tree-Backup (as illustrated in Algorithm 3).\nB\nEXAMPLE HIERARCHICAL POLICY\nTo provide a better illustration to the joint effect of hierarchy\non the effective environment policy, in Figure 6 we draw\na k = 2 hierarchical and k = 1 ﬂat policy (both condi-\ntioned on the end-goal ⟨ST ⟩) after one training update with\na shared trace. The decomposition that the hierarchy induces\nwas portrayed earlier in Figure 1, however, that structure\ncentered around the trace whereas Figure 6 illustrates this\nin the actual environment. In the left side of Figure 6, we\nsee that the agent moves from the blue to the green tile in a\nslightly sub-optimal way, it loops before the corridor, and\ntakes a detour before reaching ST . After encountering ST ,\nthe ﬂat and hierarchical policy are both updated in parallel\nwith backup parameters λ = 1.0, γ = 0.95 and Ha\ni = 3.\nThe k = 1 ﬂat policy exhibits a familiar pattern as indicated\nin the right side of Figure 6, its newly updated policy greed-\nily follows along the previously traversed path (greedily\nby omitting the loop before the corridor). In contrast, the\nhierarchy seems to sample a hierarchical action that would\nsend the lower level i = 0 towards the loop-tile before the\ncorridor (the one that the ﬂat agent now omitted). Of course,\nthis possible issue can be avoided by simply greedily re-\nsampling a new goal after each transition. The reason that\nthe hierarchy sends the ﬂat agent to this loop is actually an\nartifact of partitioning the hierarchical backup according\nto the maximal horizon Ha\ni = 3, this can be seen in the\nnumber of colored squares (left-upper aligned in each tile)\nthat point towards the colored circles (right-upper aligned\nin each tile). Each hierarchical action A(1) is repeated on\nthree distinct tiles. The hierarchy provides a beneﬁt to the\nﬂat agent in that it can jump from the left-tile next to ST\nstraight to ST , without the detour that the trace made. Surely,\nthe ﬂat agent would still follow the detour when instructed\nwith ST , but this only shows that the hierarchical agent can\nlook further forwards on the trace during learning.\nThus, when the hierarchy greedily resamples goals when\nbetter ones are encountered, the hierarchical agent funda-\nmentally supersedes the ﬂat agent in this example. The\ngreedy hierarchy omits the loop before the corridor, just like\nthe ﬂat agent. However, the hierarchy can jump over the tile\nleft to ST , straight to ST whereas the ﬂat policy is forced\nto follow its previous tracks. This may indicate that it may\nbe smart for the lower level policy to ‘forget’ slightly in this\nenvironment, such that when ST is instructed that the agent\ndoes not greedily follow its previous trajectory. Naturally,\nthis is closely tied to the tuning of the backup parameters\nγ, λ, n, and Ha\ni and the bias-variance trade off.\nC\nFURTHER EXPERIMENT DETAILS\nThis section enumerates all further experimental details that\ndid not ﬁt in the main narrative. For an overview of all\nhyperparameters and the domain we considered for our\nablations, see Table 2.\nFor all our main experiments we utilized a greedy hierar-\nchy during training ϵ = 0, but we also greedily terminated\nand resampled goals for the lower levels to follow during\nevaluation time. So sub-optimal goals could be sampled\nand followed during training, but were resampled greedily\naccording to Qi during test-time. Furthermore, for all eligi-\nbility trace based agents, we truncated/ cut the traces when\nthe eligibility value would shrink to z < 10−8 to reduce\ncomputation time (see Appendix D for further reference\non our implementation). We also did not allow hierarchical\npolicies to sample a state as goal that they were already\npositioned at πi(St|St, g) = 0, ∀i > 0. If the instructed\ngoal state of a hierarchical policy i was within reach of\nthe agent, than that policy would directly sample the goal\n(if the agent sees the goal, then it can greedily sample the\ngoal). We utilized uniformly random tie breaking in case of\nshared Q values for various state-action pairs, at every level\ni = 0, . . . , k −1.\nAll experiments were run on an AMD Ryzen 9 5900x CPU\nwhich took approximately a 1-2 days of compute when\nrunning multiple ablations in parallel. Our framework was\nbuilt on Python 3.9.7 with Numpy 1.20.3, OpenAI Gym\n0.19.0, and we utilized environment examples from the\npublic MazeLab library by Zuo [2018]. Code for implemen-\ntation and reproduction of our experiments is available at:\n<LINK REMOVED FOR DOUBLE-BLIND>, see also the\nsupplementary material for the experiment data used for\nvisualization.\nD\nIMPLEMENTATION NOTES\nThis section discusses implementation speciﬁc details and\nconsiderations. The next subsection describes important\npoints about our hierarchical Q-learning algorithm and\nHierTBk(n). The subsections afterward discuss our policy\nrepresentation and choice for reward function. We provide\nan additional experimental result/ ablation there to illustrate\nand back our choice for these particular implementations.\nTable 2: Table of parameter values (and ablation set) along with a short description. This table does not display every\ninvolved variable; e.g., the parameterization of the policies or the tie-breaking method in case of shared Q-values.\nParameter\nSet of Values\nDescription\nα\n1\nStep-size/ learning-rate for updating value estimates (Q-tables) during\nlearning (see Section 3).\nHi\n3\nThe number of hierarchical steps before termination of the policy/ prun-\ning this policy-branch. E.g., if policy H0 = 3, then policy π0 is termi-\nnated if it hasn’t reached its goal in 3 steps. This value was kept constant\nat every level i = 0, . . . , k −1 in the hierarchy.\nb : S × A →[0, 1]\n{Πk−1, π0}\nBehaviour policy for generating training data. We used either the full\nhierarchy, or just the ﬂat policy by ﬁxing A(i) = Sgoal, ∀i > 0.\nϵ0\n0.05, 0.25\nExploration probability of the level i = 0 policy during evaluation (0.05)\nand during training (0.25).\nϵi>0\n0.0\nExploration probability of the level i > 0 policies (greedy).\ntmax\n105\nNumber of allowed steps within one training episode, after which the\nagent is reset to the initial state. This value is kept large enough to let the\nagent almost always observe a reward.\nMain Ablation Study\nλ\n{0, 0.5, 0.8, 1.0}\nDecay parameter for the eligibility trace (c.f., Equation 4).\nn\n{1, 3, 5, 8}\nNumber of steps to perform Tree-Backup on for computing multistep\nreturns (c.f., Equation 3).\nγ ≡γ0\n0.95\nBase discount factor for the cumulative returns (c.f. Equation 1).\nk\n{1, 2, 3, 4}\nNumber of hierarchy levels to deﬁne policies over policies (to sample\nstates as goals/ actions).\nBackup Depth Study\nλ\n1.0\nSee Above.\nn\n{9, 3, 1}\nNumber of adjusted Tree-Backup steps for the multistep returns such that\neach hierarchy level k in the ablation study sent rewards back equally\nfar (see deﬁnition of the atomic horizon Ha\ni in Section 4).\nγ\n{0.983, 0.95, 0.857}\nBase discount factors adjusted proportional to the propagation depth of\ncredits/ rewards, these were calculated as (γ0)1/Ha\nk−1 (see Above).\nk\n{1, 2, 3}\nSee Above.\nFigure 6: Greedy policy comparison of a ﬂat (arrows) and hierarchical agent (boxes-to-circles) on an example environment\ntrace with λ = 1.0. The hierarchical agent is instructing the ﬂat agent to take two more environment steps to reach the\ngreen tile (starting from the center in the left room) if the agent were to strictly adhere to every subgoal — i.e., without\nintermediate termination. Also, the ﬂat agent aims to follow its previously traversed path, only eliminating the small loop,\nwhereas the hierarchical policy wants to ‘jump’ over these redundant actions (as shown by the blue boxes-to-circle).\nD.1\nALGORITHM DETAILS\nIn Algorithm 2 one may have noticed that we can send the\nrewards back in a form of ‘semi-backward’ view in order to\nupdate 1-step state-action pairs (line 9). It turns out that for\nall our hierarchical return operators, we don’t need to wait\nfor the entire set ∆(i)\nt\nto be computed before being able to\nupdate Q-estimates. In fact, we can perform the hierarchical\nupdate by only computing G1\nt (Equation 6) and updating\nthe Q-estimates towards this return value for all state-action\npairs in {St−j+1}Ha\ni\nj=1 × {A(i)\nt }. This mechanism is ‘semi-\nbackward’ in the sense that S(i)\nt\n= {St−j+1}Ha\ni\nj=1 contains\nall past states for which A(i)\nt\nis a reachable action. The\nmotivation behind this particular mechanism can also be\nseen by noting that G1\nt ≡Gj\nt−j, ∀j > 0; there is exact\noverlap between returns within a rolling window of states.\nWe also adopted this efﬁcient semi-backward view for our\nHierarchical Tree-Backup implementation, which is shown\nin Algorithm 3. The algorithm is of almost the exact same\nform as conventional Tree-Backup [Sutton and Barto, 2018],\nbut differs in the fact that: at each backup-step we perform\n‘jumps’ of length Ha\ni in time, and given the return G we\nupdate multiple state-action pairs (inferred from the trailing\nstates S(i)\ntn .\nLike conventional Tree-Backup, our algorithm incurs a time-\ndelay proportional to n before being able to compute update\ntargets. This is exacerbated by the hierarchy horizon Ha\ni ,\nwe need to wait nHa\ni time-steps before being able to com-\npute the multistep return. Despite this, we can still compute\nreturns/ update targets at each timestep due to the observed\npseudo-rewards. This allows us to continually update all\ngoal-specialized levels Πi, i = 0, . . . , k −1. As mentioned\nbefore and as illustrated by Algorithm 3, due to our spar-\nsiﬁcation assumption and the overlap between hierarchical\nreturns at different timesteps, G1\nt ≡Gj\nt−j (Equation 6),\nwe only have to backup along a single path to update the\nQ-values of the elements in S(i)\nt\n× {A(i)\nt }.\nEligibility Traces\nAt a ﬁrst glance, it might seem that an eligibility trace would\nmake the extension for 1-step hierarchical updates to multi-\nstep methods even simpler opposed to Tree-Backup. With\neligibility traces we can just update recency values within\nthe trace and cast a 1-step error backwards (see Equation 5).\nHowever, the hierarchical structure actually makes the ex-\ntension to Tree-Backup(λ) quite intricate.\nAs alluded to in the main paper (Section 4.2), there are nu-\nmerous subtleties in extending our version of Tree-Backup\nto a full backward method. Since every level in the hierarchy\nestimates the returns at different temporal resolutions, this\nmeans that we need to track k separate eligibilities — in fact,\nAlgorithm 3: Tree-Backup for Hierarchical Q-learning.\nInput: Environment trace τt+1 = {Sj, Aj}t+1\nj=0, the\nhierarchy level i, a target policy π, and Q-table\nQi(s, a, g)\nParameter: Discount factor γ ∈[0, 1), step-size\nα ∈(0, 1], backup depth n ∈N, and policy reach Ha\n1 tn ←t −Ha(n −1)\n{Sweep n to 1 if t = T }\n2 if tn ≥0 then\n3\nG ←rt+1 + γt+1EπQi(St+1, ·, g)\n4\nfor k = t −Ha + 1 ≥tn with steps of −Ha do\n5\nA ←Sk+Ha if i > 0 else A ←Ak\n6\nG ←rk + γk(πkG + (1 −πk)EπQi(Sk, ·, g))\n7\nA ←S1+tn if i > 0 else A ←Atn\n8\nfor j = 0, 1, . . . , min(Ha −1, tn) do\n9\nQi(Stn−j, A, g) ←\n(1 −α)Qi(Stn−j, A, g) + αG\nany level i can be interpreted to estimate the return up to a\n(γ)1/Ha\ni diluted discount (and/ or decay rate). Moreover, in\norder to correctly update all policies π ∈Πi, at every level\ni, we need to account for the policy correction terms at each\ntransition for each goal-specialized policy. For example, an\naction At can be optimal at St for reaching goal S′ but be\nsuboptimal for reaching another goal S′′. Hence, why we\nargued for utilizing |S| separate eligibility traces at each\nseparate level i.\nAnother interesting implication was that we had to use an-\nother Ha\ni separate eligibility traces for each temporal hori-\nzon. This is an artifact of our heuristic choice of dealing with\nthe exorbitant number of possible backup paths by making\nﬁxed time-jumps of Ha\ni steps. Suppose we applied the up-\ndate in Equation 8 at every environment step, then it is easy\nto see that our sparse structure gets violated: we do not only\nadd new paths to the previously added ones, but we also de-\ncay all previous path exponentially fast due to the repeated\nmultiplication with (γλ). For example, if we added a state-\naction pair of time-length Ha\ni in the previous time-step, and\nin the next time-step we added the state-action pairs for all\ntrailing states, we have a backup-path that connects through\na Ha\ni -time jump to an intermediate 1-step path. From Algo-\nrithm 3 it should be obvious that intermediate time-jumps\nshould always be Ha\ni .\nD.2\nPOLICY REPRESENTATION AND MEMORY\nAs explained in Section 4 each level within the hierarchy\nwas parameterized as Ai ⊆Πi−1, i > 0. The action space\nof the hierarchical levels was the set of goal-specialized\npolices at the level below. This is quite trivial to implement\nsimply by using the full set of polices Ai ≡Πi−1, i > 0\nsuch that the agent can ﬁgure out itself which goals are\nreachable (which actions are viable) through the backed\nup rewards. Seeing as we utilized fully greedy hierarchical\npolicies, after observing the ﬁrst reward, the agent would\nalways sample valid state-actions.\nWe restricted this set of policies by deﬁning A to be a func-\ntion over the power set of the state-space (in terms of goal-\npolicies), Ai : S →P(Πi−1) such that each π ∈Ai(St)\ncould actually reach its goal within Ha\ni steps. In other words,\nwe restricted the action set of each hierarchical policy at\nevery state such that any sampled policy’s goal state lied\nwithin a Ha\ni neighborhood of the current state in terms of\nthe state-space (we utilized l1 neighborhoods). This makes\nsense because any policy that the hierarchical level could\nsample that lied outside this neighborhood would be termi-\nnated prematurely due to the hierarchical action budgets\nHi. Of course, this is an ad-hoc detail, and is not applica-\nble in every domain; this either requires knowledge of the\ntransition function or an assumption on the environment’s\ngeometry. Due to our tabular domain, this assumption was\nnecessary to cut down on the memory usage.\nAnother memory optimization is noting that we do not uti-\nlize all policies in the top-level Πk−1, only the policy condi-\ntioned on the environment reward. This cut down the mem-\nory complexity from O(|S|2 × |Ak−1|) to O(|S| × |Ak−1|)\nfor the top-level. Combined with the restricted action-spaces\nat the lower levels i < k −1, this provided a tractable imple-\nmentation for the larger domains (e.g., 20 × 20 Gridworld).\nD.3\nREWARD FUNCTION AND DISCOUNTING\nA common theme in our main paper was whether hierarchy\nsimply adds depth to the reward estimation similarly to\nﬂat multistep methods. However, we argued that hierarchy\ncreates skip-connections over time whereas conventional\nmethods simply step-through the trace. An important factor\nthat balanced the depth of the return in either case, was\nthe discount factor γ. Due to our choice of binary pseudo-\nrewards, i.e., the Successor Representation [Dayan, 1993],\nafter sufﬁcient time the rewards will get diluted due to the\nexponential discounting. For our experiments/ ablations it\nmay have made sense to utilize another reward function,\nsuch that γ could be set to γ = 1. A simple alternative is\nr′\nt = 1−rt, i.e., a penalizing reward of −1 at each time-step\nand 0 when the goal is observed.\nThough this choice for reward function eliminates one con-\nfounder γ, it introduces another one. The penalizing reward\nhas a similar effect to count-based exploration methods. Dur-\ning training episodes, states will get penalized continuously\nuntil goals are observed, this has the effect that the agent\nwill steer away from frequently visited states if it doesn’t\nobserve rewards for said goals. Our choice of the binary re-\nwards ensured that the agents would stay uniformly random\nuntil actual rewards were observed.\nD.4\nPOLICY AND REWARD ABLATION\nOur choice for the relative policy parameterization and bi-\nnary pseudo-rewards is motivated by our additional abla-\ntion results in Figure 8. These results were generated with\nthe exact same experimental setup as discussed before in\nSection 5. However, here we performed ablations over the\nchoice of reward function (binary Vs. penalizing) and the\npolicy parameterization (restricted Vs. unrestricted) only for\nthe 1-step Hierarchical Q-learning algorithm. In a sense, this\ncan be seen as a direct comparison to the original algorithm\nby Levy et al. [2018] and our adaptation. All-in-all, these\nresults simply strengthen our implementation considerations\nthat we used to generate our main results.\n3 × 100\n4 × 100\n5 × 100\n6 × 100\n4 × 100\n6 × 100\n3 × 100\n4 × 100\n5 × 100\n4 × 100\n5 × 100\n6 × 100\n7 × 100\n8 × 100\n1\n2\n3\n4\n1\n2\n3\n4\n10x10 Gridworld\n1\n2\n3\n4\n1\n2\n3\n4\n4-rooms 5-to-1\n1\n2\n3\n4\n1\n2\n3\n4\nn = 3\nn = 5\nn = 8\n10x10 Maze\n1\n2\n3\n4\n1\n2\n3\n4\n20x20 Gridworld\n1\n2\n3\n4\n1\n2\n3\n4\n= 0.0\nn = 1\n= 0.5\nn = 3\n= 0.8\nn = 5\n= 1.0\nn = 8\n9-rooms 5-to-1\n1\n2\n3\n4\n1\n2\n3\n4\n20x20 Maze\nNumber of Hierarchies\nAverage number of log(steps) per episode\nNumber of steps per episode\nAveraged over first 50 episodes and 200 runs.\nFigure 7: Marginal log-performance of each experiment conﬁguration for each environment from Figure 3 (lower is better).\nThis ﬁgure is identical to the main paper (including the y-scale; c.f., Figure 4), however this data portrays the agent’s\nperformance when training proceeded purely with the ﬂat policy (i.e., by keeping the hierarchy ﬁxed during training:\nA(i) = Sgoal, ∀i > 0) and evaluation with the hierarchical policy.\n2\n3\n2\n3\n2\n3\n2\n3\n102\n103\n104\n10x10 Gridworld\n2\n3\n2\n3\n2\n3\n2\n3\n102\n103\n104\n4-rooms 5-to-1\n2\n3\n2\n3\n2\n3\n2\n3\n102\n103\n104\n10x10 Maze\n2\n3\n2\n3\n2\n3\n2\n3\n102\n103\n104\n20x20 Gridworld\n2\n3\n2\n3\n2\n3\n2\n3\n102\n103\n104\n9-rooms 5-to-1\n2\n3\n2\n3\n2\n3\n2\n3\n102\n103\n104\n20x20 Maze\nHierarchy Levels (k)\nDistribution: Average number of steps\nHierQk( = 0): Number of steps needed for reaching goal.\n, rt + 1 = 1St + 1\n, rt + 1 = 1St + 1\n, rt + 1 = 1\n1St + 1\n, rt + 1 = 1\n1St + 1\nFigure 8: Comparison of our implementation for HierQk(λ = 0) to that of Levy et al. [2018]. The x-axis shows the number\nof hierarchy levels k, split by the reward function and as indicated in the legend. The y-axis shows the aggregate number of\nsteps to reach the environment goal of the greedy policy over the ﬁrst 50 training episodes, the violinplots illustrate their\ndistribution over 200 random seeds.\nTable 3: Table of all ﬁrst episode reward statistics from the main ablation study, i.e., the recorded number of environment\nsteps in the ﬁrst training episode. As in the main paper (c.f., Table 1), the cell values contain the mean and standard errors\nµ ± s−1 rounded to integers. These means can also be interpreted as the scale parameter β for an exponential distribution\nExp(β−1), though this data is best captured by a scaled Poisson (or Gamma). The lowest means are emphasized with bold\nfonts whereas the highest means are emphasized with italics, see also the supplementary material for further visualizations.\nHierQk(λ)\n10x10\nGridworld\n20x20\nGridworld\n4-rooms\n5-to-1\n9-rooms\n5-to-1\n10x10 Maze\n20x20 Maze\nλ = 0\nk = 1\n561 ± 37\n2746 ± 178\n931 ± 56\n2556 ± 142\n2149 ± 173\n9215 ± 599\nk = 2\n834 ± 63\n5188 ± 380\n1135 ± 66\n3949 ± 230\n5418 ± 734\n14312 ± 982\nk = 3\n857 ± 64\n6996 ± 529\n1293 ± 69\n4178 ± 277\n2497 ± 209\n12388 ± 791\nk = 4\n950 ± 71\n6704 ± 548\n1200 ± 70\n4035 ± 238\n1471 ± 127\n10665±688\nλ = 0.5\nk = 1\n626 ± 35\n3168 ± 179\n1024 ± 59\n2541 ± 125\n2797 ± 215\n9080 ± 629\nk = 2\n1043 ± 83\n6014 ± 478\n1222 ± 74\n4136 ± 239\n4805 ± 564\n14028 ± 908\nk = 3\n990 ± 70\n8244 ± 590\n1273 ± 91\n4109 ± 283\n2860 ± 262\n11647 ± 784\nk = 4\n1245 ± 90\n8770 ± 727\n1339 ± 77\n3928 ± 270\n1308 ± 95\n10580 ± 643\nλ = 0.8\nk = 1\n629 ± 36\n3300 ± 189\n941 ± 54\n2792 ± 151\n2525 ± 187\n10384 ± 684\nk = 2\n1130 ± 78\n6528 ± 436\n1308 ± 73\n3717 ± 213\n6318 ± 665\n13167 ± 867\nk = 3\n1134 ± 76\n8254 ± 663\n1400 ± 99\n4655 ± 303\n2914 ± 267\n14865 ± 944\nk = 4\n1315 ± 105\n8406 ± 788\n1331 ± 71\n4492 ± 287\n1663 ± 143\n9854 ± 633\nλ = 1\nk = 1\n562 ± 31\n3023 ± 216\n838 ± 46\n2937 ± 162\n2236 ± 189\n8922 ± 574\nk = 2\n1069 ± 76\n6586 ± 504\n1255 ± 72\n3532 ± 198\n5617 ± 600\n13826 ± 829\nk = 3\n1010 ± 70\n10385 ± 819\n1339 ± 76\n4102 ± 240\n2965 ± 260\n11590 ± 711\nk = 4\n1380 ± 104\n8643 ± 766\n1370 ± 81\n3933 ± 235\n1785 ± 151\n10923 ± 707\nHierTBk(n)\nn = 1\nk = 1\n637 ± 43\n3251 ± 221\n895 ± 52\n2960 ± 190\n2255 ± 155\n9718 ± 651\nk = 2\n859 ± 70\n5518 ± 368\n1196 ± 80\n3568 ± 219\n4464 ± 614\n14190 ± 1040\nk = 3\n959 ± 71\n6417 ± 526\n1297 ± 69\n4658 ± 279\n2469 ± 199\n12969 ± 832\nk = 4\n895 ± 72\n6468 ± 605\n1251 ± 74\n4244 ± 267\n1490 ± 105\n9919 ± 585\nn = 3\nk = 1\n698 ± 44\n3064 ± 210\n972 ± 58\n3183 ± 193\n2493 ± 171\n9028 ± 555\nk = 2\n1069 ± 87\n5176 ± 394\n1174 ± 75\n3627 ± 206\n5000 ± 555\n12388 ± 936\nk = 3\n994 ± 76\n8010 ± 600\n1310 ± 77\n4776 ± 332\n2573 ± 247\n11168 ± 748\nk = 4\n1163 ± 78\n6563 ± 628\n1262 ± 71\n3880 ± 279\n1681 ± 143\n9549 ± 584\nn = 5\nk = 1\n519 ± 33\n3023 ± 186\n946 ± 55\n2873 ± 154\n2514 ± 185\n9440 ± 644\nk = 2\n1050 ± 86\n6077 ± 480\n1090 ± 63\n3723 ± 219\n4097 ± 420\n13469 ± 884\nk = 3\n869 ± 69\n8179 ± 649\n1234 ± 78\n4589 ± 385\n2824 ± 251\n10385 ± 637\nk = 4\n1141 ± 91\n8313 ± 731\n1292 ± 78\n4236 ± 236\n1599 ± 146\n9846 ± 634\nn = 8\nk = 1\n632 ± 36\n3139 ± 191\n1064 ± 61\n2789 ± 161\n2479 ± 193\n8984 ± 597\nk = 2\n968 ± 74\n6094 ± 404\n1131 ± 68\n3591 ± 218\n3707 ± 439\n14032 ± 933\nk = 3\n1030 ± 74\n7691 ± 557\n1271 ± 74\n3891 ± 233\n2900 ± 256\n11853 ± 753\nk = 4\n1140 ± 80\n8691 ± 830\n1341 ± 78\n4310 ± 243\n1551 ± 146\n9077 ± 559\n3 × 100\n4 × 100\n5 × 100\n6 × 100\n4 × 100\n5 × 100\n6 × 100\n7 × 100\n3 × 100\n4 × 100\n6 × 100\n4 × 100\n5 × 100\n6 × 100\n7 × 100\n3 × 100\n4 × 100\n6 × 100\n4 × 100\n6 × 100\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n10x10 Gridworld\nk = 1,\n= 0.983, n = 9,b =\n0\nk = 2,\n= 0.950, n = 3,b =\n0\nk = 2,\n= 0.950, n = 3,b =\nk\n1\nk = 3,\n= 0.857, n = 1,b =\n0\nk = 3,\n= 0.857, n = 1,b =\nk\n1\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n4-rooms 5-to-1\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n10x10 Maze\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20x20 Gridworld\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n9-rooms 5-to-1\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20x20 Maze\nNumber of Training Episodes\nAverage number of log(steps) per episode\nNumber of steps per episode. Averaged over 200 runs.\nFigure 9: Average log(score) of the evaluated hierarchical Tree-Backup agents at each training episode (lower is better)\nwhen credit assignment depth is appropriately balanced for each hierarchy level k. Shaded regions indicate 1-standard error\nof the mean.\n3 × 100\n3.05 × 100\n3.1 × 100\n3.15 × 100\n3.2 × 100\n3.25 × 100\n3.3 × 100\n3.35 × 100\n3.4 × 100\n4 × 100\n5 × 100\n2.8 × 100\n2.9 × 100\n3 × 100\n3.1 × 100\n3.2 × 100\n3.3 × 100\n3.4 × 100\n3.4 × 100\n3.6 × 100\n3.8 × 100\n4 × 100\n4.2 × 100\n4.4 × 100\n2.9 × 100\n3 × 100\n3.1 × 100\n3.2 × 100\n3.3 × 100\n3.4 × 100\n3.5 × 100\n3.6 × 100\n4 × 100\n5 × 100\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n10x10 Gridworld\nk = 1,\n= 0.983,\n= 1.0, b =\n0\nk = 2,\n= 0.950,\n= 1.0, b =\n0\nk = 2,\n= 0.950,\n= 1.0, b =\nk\n1\nk = 3,\n= 0.857,\n= 1.0, b =\n0\nk = 3,\n= 0.857,\n= 1.0, b =\nk\n1\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n4-rooms 5-to-1\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n10x10 Maze\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20x20 Gridworld\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n9-rooms 5-to-1\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20x20 Maze\nNumber of Training Episodes\nAverage number of log(steps) per episode\nNumber of steps per episode. Averaged over 200 runs.\nFigure 10: Average log(score) of the evaluated hierarchical Q(λ) agents at each training episode (lower is better) when\ncredit assignment depth is appropriately balanced for each hierarchy level k. Shaded regions indicate 1-standard error of the\nmean.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-03-07",
  "updated": "2022-03-07"
}