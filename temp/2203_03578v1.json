{
  "id": "http://arxiv.org/abs/2203.03578v1",
  "title": "Unsupervised Quantum Circuit Learning in High Energy Physics",
  "authors": [
    "Andrea Delgado",
    "Kathleen E. Hamilton"
  ],
  "abstract": "Unsupervised training of generative models is a machine learning task that\nhas many applications in scientific computing. In this work we evaluate the\nefficacy of using quantum circuit-based generative models to generate synthetic\ndata of high energy physics processes. We use non-adversarial, gradient-based\ntraining of quantum circuit Born machines to generate joint distributions over\n2 and 3 variables.",
  "text": "Unsupervised Quantum Circuit Learning in High Energy Physicsâˆ—\nAndrea Delgado1, â€  and Kathleen E. Hamilton2, â€¡\n1Physics Division, Oak Ridge National Laboratory, Oak Ridge, TN 37830\n2Computer Science and Engineering Division, Oak Ridge National Laboratory, Oak Ridge, TN 37830\nUnsupervised training of generative models is a machine learning task that has many applications\nin scientiï¬c computing. In this work we evaluate the eï¬ƒcacy of using quantum circuit-based gener-\native models to generate synthetic data of high energy physics processes. We use non-adversarial,\ngradient-based training of quantum circuit Born machines to generate joint distributions over 2 and\n3 variables.\nI.\nINTRODUCTION\nHigh-energy physics seeks to understand matter at the\nmost fundamental level. Vast and complex accelerators\nhave been built to elucidate the dynamical basis of the\nfundamental constituents, the quarks, and the gluons. At\nthese large-scale facilities, high-performance data stor-\nage and processing systems are needed to store, access,\nretrieve, distribute, and process experimental data. Ex-\nperiments like the Compact Muon Solenoid (CMS) at the\nLarge Hadron Collider (LHC) are incredibly complex, in-\nvolving thousands of detector elements that produce raw\nexperimental data rates over a Tb/sec, resulting in the\nannual productions of datasets in the scale of hundreds\nof Terabytes to Petabytes. In addition, the manipulation\nof these complex datasets into summaries suitable for the\nextraction of physics parameters and model comparison\nis a time-consuming and challenging task.\nA crucial element of any analysis workï¬‚ow in particle\nphysics involves simulating the physical processes and\ninteractions at these facilities to develop new theories\nand models, explain experimental data, and character-\nize background. These simulations also allow for study-\ning detector response and plan detector upgrades. The\nsimulation of particle physics interactions is often com-\nputationally intensive, taking up a signiï¬cant fraction of\nthe computational resources available to physicists.\nRecently, alternative methods for detector simulation\nand data analysis tasks have been explored, like machine\nlearning applications and quantum information science\n(QIS). QIS is a rapidly developing ï¬eld focused on un-\nderstanding the analysis, processing, and transmission\nof information using quantum mechanical principles and\nâˆ—This manuscript has been authored by UT-Battelle, LLC, under\nContract No. DE-AC0500OR22725 with the U.S. Department\nof Energy. The United States Government retains and the pub-\nlisher, by accepting the article for publication, acknowledges that\nthe United States Government retains a non-exclusive, paid-up,\nirrevocable, world-wide license to publish or reproduce the pub-\nlished form of this manuscript, or allow others to do so, for the\nUnited States Government purposes. The Department of Energy\nwill provide public access to these results of federally sponsored\nresearch in accordance with the DOE Public Access Plan.\nâ€  delgadoa@ornl.gov\nâ€¡ hamiltonke@ornl.gov\ncomputational techniques. QIS can address the conven-\ntional computing gap associated with HEP-related prob-\nlems, speciï¬cally those computational tasks that chal-\nlenge CPUs and GPUs, such as eï¬ƒcient and accurate\nevent generators. Thus, the relevance of having an ef-\nï¬cient simulation mechanism that can faithfully repro-\nduce particle interactions after a high-energy collision has\nsparked the development of alternative methods.\nOne\nparticular example is the use of generative models such as\ngenerative adversarial networks (GANs) [1], which have\nbeen utilized in HEP as a tool for fast Monte Carlo (MC)\nsimulations [2â€“4], and as a machine learning-enhanced\nmethod for event generation [5â€“8]. Some of these studies\nreport up to ï¬ve orders of magnitude decrease in com-\nputing time. A crucial feature of generative models is\ntheir ability to generate synthetic data by learning from\nactual samples without knowing the underlying physical\nlaws of the original system.\nIn some studies, genera-\ntive models have been shown to overcome the statistical\nlimitations of an input sample in subtraction of negative-\nweight events in samples generated beyond leading-order\nin QCD [5], and to increase the statistics of centrally\nproduced Monte Carlo datasets [7].\nQuantum-assisted\nmodels have also been proposed for Monte Carlo event\ngeneration [9], detector simulation [10, 11], and determin-\ning the parton distribution functions in a proton [12]. In\nthis work, we successfully trained a quantum generative\nmodel to reproduce kinematic distributions of particles\nin pp interactions at the LHC, with high ï¬delity. Quan-\ntum circuit Born machines (QCBM) trained via gradient-\nbased optimization [13â€“15] are examples of circuit-based\nparameterized models that can be trained on near-term\nquantum platforms.\nII.\nMODEL AND LEARNING ALGORITHM\nAlthough generative models trained in adversarial set-\ntings have been proven to be a valuable tool in HEP,\nfor this work, we focus on generative models trained\nwith non-adversarial methods. Expressly, we set up and\ntrain multiple QCBM using data-driven circuit learning\n(DDCL) [13â€“15]. DDCL employs a classical-quantum hy-\nbrid feedback loop, as described in Fig. 1.\nGenerating synthetic data for this HEP application\nconsists of three stages: ï¬rst, we encode our data of of\narXiv:2203.03578v1  [quant-ph]  7 Mar 2022\n2\nFIG. 1: Diagram of the diï¬€erentiable QCBM training\nscheme.\nM observations with N-dimensional, numerical features\n(D = {X(1), . . . , X(m), . . . , X(M)}) into a distribution\nP(x) deï¬ned over ï¬nite length bitstrings (x); second, we\ntrain a parameterized circuit ansatz using DDCL to pre-\npare an approximation of this distribution eP(x); third,\nwe generate synthetic data by decoding bitstrings sam-\npled from eP(x). In the following subsection, we describe\nthese stages in greater detail.\nA.\nData Encoding\nThe kinematic distributions of a particle jet in a high-\nenergy collider experiment such as the LHC can be\nconstructed as a 2-dimensional joint distribution (over\n(pT , m) features) or 3-dimensional joint distributions\n(over (pT , m, Î·) features).\nFor a QCBM constructed\nwith Q total qubits, P(x) is constructed by concatentat-\ning lists of binary bitstrings which encode the marginals\nof individual features in a classical dataset.\nThe N-\ndimensional correlated data features (X(m)) are encoded\nas 2Q-length binary strings (xi).\nIndividual features are encoded in subsets of q = Q/N\nqubits by discretizing the marginal distribution into 2q\nbins and each bin index is converted to a 2q-length bi-\nnary bitstring.\nThe ï¬nal 2Q-length bistrings are con-\nstructed by concatenating the N feature 2q-length binary\nbitstrings and normalizing the amplitudes.\nB.\nQuantum Circuit Model and Training\nThe QCBM is an example of an implicit model for gen-\nerative learning [16] that generates data by measuring\nthe system as a Born machine. A QCBM is a parame-\nterized unitary U(Î˜) that prepares Q-qubits in the state\n|ÏˆÎ˜âŸ©= U(Î˜) |Ïˆ0âŸ©.\nThe initial state |Ïˆ0âŸ©is ï¬xed, and\nin this study, we use: the all zero-state |Ïˆ0âŸ©= |0âŸ©âŠ—Q; a\nproduct state of Q/2 Bell states |Î¦+âŸ©âŠ—Q/2; and a product\nstate of Q/3 GHZ states |Ïˆ0âŸ©= |GHZâŸ©âŠ—Q/3.\nMeasuring |ÏˆÎ˜âŸ©in a ï¬xed basis M 1 requires sampling\nfrom the state with Nshots shots. This deï¬nes a classi-\ncal distribution over the 2Q computational basis states\nePÎ˜(x) that is used in training, and later used to generate\nthe synthetic data e\nX.\n1.\nParameterized Quantum Circuit\nFinding the ideal unitary U(Î˜) is dependent on the\nparameterized quantum circuit (PQC). The PQC design\nplays an essential role in the performance of many varia-\ntional hybrid quantum-classical algorithms [17] by deï¬n-\ning the hypothesis class. For our application, PQCs must\nbe able to model diï¬€erent types of correlations in the in-\nput data. This requires circuits to prepare strongly en-\ntangled quantum states and the ability to explore Hilbert\nspace.\nVariational algorithms have been implemented using\nquantum circuits composed of a network of single and\ntwo-qubit operations, with rotation angles serving as\nvariational parameters.\nThe pattern deï¬ning the net-\nwork of gates is referred to as a unit-cell or circuit block\nthat can be repeated to suit the needs of the application\nor task at hand. Recently, the term Multilayer Quantum\nCircuit (MPQC) was coined to describe this type of vari-\national circuit architecture [18]. In this study, we train\ntwo circuit templates or ansatz. Each circuit is deï¬ned\nusing a Q-qubit register and speciï¬ed by the number of\nlayers d, consisting of a rotation and an entangling com-\nponent. Ansatz 1 has a combined rotation and entangling\ngates in a â€œBrick Layerâ€ or â€œSimpliï¬ed 2-designâ€ archi-\ntecture and has been shown to exhibit important prop-\nerties to study barren plateaus in quantum optimization\nlandscapes [19, 20]. Ansatz 2 was chosen due to the low\ncorrelation displayed between variables and is an exten-\nsion of ansatz employed in benchmarking tasks [14].\nThe diagram for one layer of each template is shown\nin Fig. 2. The rotation gate layers are parameterized by\nthe arbitrary single-qubit rotation gate implemented in\nPennyLane [21], which has 3 rotation angles R(Î˜)(i)\nâ„“\n=\nR(Ï‰, Î¸, Ï†)(i)\nâ„“, where the layer index â„“runs from 0 to d,\nand i is the qubit index.\nWe choose to train two ansatz that can be embed-\nded onto near-term NISQ devices:\neither using a 1D\nchain of qubits (for Anasatz 1), or the â€disconnected\ntreeâ€ conï¬guration (for Ansatz 2).\nFor Ansatz 1 used\nin this study, the number of parameters to optimize\nduring training is Npar(Nâ„“, Q) = 3Nâ„“[2Q âˆ’1(2)] + 3Q\nfor QCBM with odd(even) qubits Q.\nFor Ansatz 2,\nNpar(Nâ„“, Q) = 3(Nâ„“+ 1)Q, regardless of whether Q is\neven or odd.\n1 We use the Z-basis M = Zi âŠ—Â· Â· Â· âŠ—ZQ unless otherwise noted.\n3\nğ‘…(â‹…)\nÃ— ğ‘‘\nğ‘…(â‹…)\nÃ— ğ‘‘\nğ‘…(â‹…)\nğ‘…(â‹…)\nAnsatz 1\nAnsatz 2\nFIG. 2: Diagram for one layer of the circuit templates\nused to construct and train the QCBM. Both ansatz are\nconstructed using a layered pattern of rotation and\nentangling gates. For both ansatz a ï¬nal layer of\nrotation gates is added before measurement.\n2.\nTraining and cost function\nTraining a QCBM via DDCL optimizes Î˜ by minimiz-\ning a loss function L(P, ePÎ˜) such that ePÎ˜(x) â‰ˆP(x).\nWe use the Jensen-Shannon (JS) divergence- a diï¬€eren-\ntiable function that compares two distributions P(x) and\nePÎ˜(x):\nJS(P| ePÎ˜) = 1\n2\nX\nx\n\"\nP log\n\u0012 P\nM\n\u0013\n+ ePÎ˜ log\n ePÎ˜\nM\n!#\n(1)\nwhere M = (P + ePÎ˜)/2. The minimum value of the loss\nfunction JS(P| ePÎ˜) = 0 is achieved when ePÎ˜(x) = P(x).\nThe model parameters Î˜ are optimized using classical\ngradient descent methods so that the loss function is min-\nimized. The gradient of the loss function is computed\nwith respect to the circuit parameters using the parame-\nter shift rule [22]. Each QCBM is trained using the Adam\ngradient-based optimizer [23] available in the PennyLane\nlibrary [21].\nC.\nMeasurement decoding and post-processing\nThe output of a QCBM is ePÎ˜(x):\na classical dis-\ntribution over 2Q-length binary strings.\nTo con-\nvert ePÎ˜(x) to numerical n-dimensional features ( eD =\n{ e\nX(1), . . . , e\nX(m), . . . }), we reverse the steps described in\nSection II A: each binary string 2Q length string is disas-\nsociated into N composite strings each of length 2q, and\na ï¬‚oat value randomly drawn from a uniform distribu-\ntion deï¬ned with the bin edges previously used to map\nthe samples xm into the binary basis to generate P(x).\nIII.\nVALIDATION ON LHC DATASET\nOne of the big computational challenges in HEP is the\nconsiderable computing time required to model the be-\nhavior of subatomic particles both at the vertex and de-\ntector level. In Section II, we introduce the architecture\nof a quantum generative model that aims to provide an\nalternative to traditional Monte Carlo (MC) methods in\nthe context of data augmentation. Thus, to validate the\nproposed model, we consider the simulation of the pro-\nduction of pairs of jets in pp interactions at the LHC.\nThe dataset [6] consists of 10 million di-jet events gener-\nated using MadGraph5 [24] and PYTHIA8 [25], corre-\nsponding to a center of mas energy of 13 TeV and an inte-\ngrated luminosity of about 0.5 fbâˆ’1. The response of the\ndetector was simulated by a DELPHES [26] fast simu-\nlation, using settings that resemble the ATLAS detector.\nAn average of 25 additional soft-QCD pp collisions (pile-\nup) were added to the simulation to mimic the conditions\nof a typical collider event realistically. Jets were recon-\nstructed using the anti-kT [27] algorithm as implemented\nin FastJet [28], with a distance parameter R = 1.0. Se-\nlection cuts on pT and HT were applied, reducing the\nsample size to about 4 million events.\nThe kinematic\ndistributions of the leading jet on the di-jet system are\nused to validate the QCBM models and evaluate its per-\nformance in a real-world application.\nIV.\nRESULTS\nIn this section, we report on the results of training a\nQCBM to prepare the target distribution of the dataset\ndescribed in Section III. The model performance is eval-\nuated by studying the JS divergence value throughout\nthe training and comparing target (MC expectation) and\ngenerated (the output of the trained QCBM model) dis-\ntributions. We explore the encoding of the target dis-\ntributions for 2(3) variable joint distributions into 8(12)\nqubit systems. This scheme allows for a four-qubit en-\ncoding per distribution. Unless noted, each marginal dis-\ntribution is encoded in a target state binned over 24 = 16\nbasis states. We trained the QCBM circuits in the ab-\nsence of noise to obtain a set of optimal parameters Î˜.\nThen, the circuits were deployed using the trained pa-\nrameters on IBM quantum devices to study the eï¬€ect of\nnoise in the loss landscape, reproducing the target dis-\ntribution. Finally, a local parameter tuning scheme was\napplied to improve the performance in the presence of\nnoise.\nA.\nTraining with noiseless qubits\n1.\n2D Distributions\nIn Figure 3, the JS divergence values are plotted as a\nfunction of training step. The circuits were trained using\nNshots = 8192 to prepare a joint 2D distribution corre-\nsponding to the marginal distributions of the leading jet\ntransverse momentum (pT ) and mass, both binned over\nthe 16 basis states corresponding to four qubits. Circuits\nwere constructed using the ansatz conï¬gurations shown\nin Figure 2 and trained to start from either the all-zero\nstate (|Ïˆ0âŸ©= |0âŸ©âŠ—8), or from a product of four Bell states\n4\n(|Î¦+âŸ©âŠ—4). We ï¬xed the number of layers Nlayers = 6 for\nAnsatz 1 (blue) and 12 for Ansatz 2(red). Each circuit\nis trained for 300 steps of Adam with a learning rate\nÎ± = 0.01.\n0\n50\n100\n150\n200\n250\n300\noptimizer step\n10\n2\n10\n1\nJS(P|P )\nAnsatz 1 - |0\nQ\nAnsatz 1 - |\n+\nQ/2\nAnsatz 2 - |0\nQ\nAnsatz 2 - |\n+\nQ/2\nFIG. 3: JS divergence value as a function of training\nstep using Nshots = 8192. Blue(red) points correspond\nto Ansatz 1(2). Circuits were initialized in the all zero\nstate (solid lines) or four Bell states (dashed lines) and\ntrained to learn a 2D joint distribution.\nFrom Figure 3 we can conclude that: Ansatz 2 con-\nverges to a stable JS divergence value much faster than\nAnsatz 1, and the training of the QCBM is not aï¬€ected by\nthe choice of the initial state. Figure 4 displays the dis-\ntributions of samples generated via projective measure-\nments on the qubits in the trained circuits. We observe\nthat the data generated resembles the target distribu-\ntions with high ï¬delity, with a slightly better agreement\nfor data generated by sampling from the QCBM con-\nstructed with Ansatz 2 (red).\n250\n300\n350\n400\n450\n500\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\nFraction of samples\nAnsatz 1\nAnsatz 2\nMC Expectation\n20\n40\n60\n80\n100\n120\n140\n160\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n250\n300\n350\n400\n450\n500\nJet pT [GeV]\n0.5\n1.0\n1.5\nP/P  ratio\n25\n50\n75\n100\n125\n150\nJet mass [GeV]\n0.5\n1.0\n1.5\nFIG. 4: Top: Target and sampled distributions.\nBlue(red) points correspond to Ansatz 1(2). Circuits\nwere initialized in the all-zero state (|Ïˆ0âŸ©= |0âŸ©âŠ—8) and\ntrained to learn a 2D joint distribution. Bottom: Ratio\nof target and sampled distributions with horizontal\nguide lines marking P/ ePÎ˜ = 0.9 and P/ ePÎ˜ = 1.1.\nIn Figure 5, we compare the sampled and target distri-\nbutions obtained when starting the training from either\nan all-zero state (|Ïˆ0âŸ©= |0âŸ©âŠ—8) or from a product of four\nBell states (|Î¦+âŸ©âŠ—4). We deï¬ne a similarity measure to\nperform a systematic comparison of the marginal distri-\nbutions for each feature by computing the mean absolute\nerror (MAE) per bin between all normalized target and\nsampled marginal distributions:\nD(p|ep(Î˜)) =\n1\nN2q\nN\nX\nn\n2q\nX\ni\n\f\f\fp(n)\ni\nâˆ’ep(n)\ni\n(Î˜)\n\f\f\f .\n(2)\n250\n300\n350\n400\n450\n500\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\nFraction of samples\nAnsatz 1 - |0\nQ\nAnsatz 1 - |\n+\nQ/2\nMC Expectation\n20\n40\n60\n80\n100\n120\n140\n160\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n250\n300\n350\n400\n450\n500\nJet pT [GeV]\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\nFraction of samples\nAnsatz 2 - |0\nQ\nAnsatz 2 - |\n+\nQ/2\nMC Expectation\n20\n40\n60\n80\n100\n120\n140\n160\nJet mass [GeV]\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\nFIG. 5: Top: Target and sampled distributions\nconstructed using Ansatz 1. Circuits were initialized in\nthe all-zero state (blue) and four Bell states (black).\nBottom: Target and sampled distributions constructed\nusing Ansatz 2. Circuits were initialized in the all\nzero-state (red) and four Bell states (black).\nFrom both Figure 5 and Table I, we can conclude that\nAnsatz 2, initialized in the all-zero state, reproduces the\nfeature marginals with the highest ï¬delity. On the other\nhand, the diï¬€erence in D(p|ep(Î˜)) for the two initial con-\nï¬gurations considered is negligible, considering a statisti-\ncal error proportional to 1/\np\nNsamples âˆ¼0.0005728, im-\nplying that the training is independent of circuit initial-\nization.\nAnsatz Npar Initial State\nD(p|ep(Î˜))\n1\n276 |Ïˆ0âŸ©= |0âŸ©âŠ—8\n0.007153\n1\n|Ïˆ0âŸ©= |Î¦+âŸ©\nâŠ—4\n0.006767\n2\n312 |Ïˆ0âŸ©= |0âŸ©âŠ—8\n0.005452\n2\n|Ïˆ0âŸ©= |Î¦+âŸ©\nâŠ—4\n0.005696\nTABLE I: D(p|ep(Î˜)) for 8 qubit QCBM.\nAnother important factor in the process of building the\ncircuit to prepare the target state is the number of lay-\ners the template or ansatz in Figure 2 is repeated. This\nchoice will also determine the number of trainable param-\neters in the model. In Figure 6, the JS divergence value is\nplotted as a function of training steps for d = {1, 3, 6, 9}\nlayers in Ansatz 1; and d = {2, 6, 12, 18} layers in Ansatz\n2.\nWe can see from this plot that the number of lay-\ners used to construct Ansatz 2 has little impact on the\n5\nğ½(ğ‘ƒ|ğ‘ƒ%!)\nFIG. 6: Loss as a function of training step using\nNshots = 8192. The diï¬€erent curves represent a diï¬€erent\nnumber of layers d, used to construct the 8 qubit\nQCBM. Top: Ansatz 1 (blue), and Bottom: Ansatz 2\n(red).\nğ½(ğ‘ƒ|ğ‘ƒ%!)\nFIG. 7: Mean loss between the complete dataset and\nthe distribution generated by 8-qubit QCBM models\nQCBM circuits plotted as solid circles in blue(red) for\nAnsatz 1(2). Each QCBM model was trained on partial\ndata. Shaded regions correspond to mean JS divergence\nvalue Â±Ïƒ (one standard deviation).\nminimal JS value reached after the training converged.\nNonetheless, it aï¬€ects how fast the model reaches this\nminimal JS value. On the other hand, the training per-\nformance of Ansatz 1 is highly dependent on the number\nof layers used to construct the circuit. We chose to use\nd = 6(12) to construct our QCBM with Ansatz 1(2) to\nkeep an optimal balance between training time and per-\nformance.\nThis study proposes using quantum generative models\nas a data augmentation tool. The results presented in\nthis section thus far were obtained by training QCBMs\non a target distribution using close to 4 million events,\nwhich is the typical number for any analysis campaign\nin HEP. We also investigated how reducing the training\ndataset size aï¬€ects the trained modelâ€™s ï¬delity to repro-\nduce the target distribution. In Figure 7, the horizontal\naxis represents the fraction of the initial training dataset\nof 4 million events used to train the QCBM. Once the\nmodel is trained, using a fraction of the full dataset, the\nJS divergence metric is evaluated on the target distribu-\ntion generated using the whole dataset. The distribution\nwas obtained by evaluating the QCBM with the trained\nparameters with 8192 shots. The sampling process was\nrepeated 1000 times, and the mean is reported in Figure\n7 as a solid blue(red) dot for Ansatz 1(2). The bands cor-\nrespond the mean JS divergence value Â±Ïƒ (one standard\ndeviation).\n(a) Monte Carlo (Ground Truth)\npT\nmass\npT\n-\n0.2\nmass\n0.2\n-\n(b) Ansatz 1\npT\nmass\n|0âŸ©âŠ—8 |Î¦+âŸ©âŠ—4 |0âŸ©âŠ—8 |Î¦+âŸ©âŠ—4\npT\n-\n0.19\n0.12\nmass\n0.19\n0.12\n-\n(c) Ansatz 2\npT\nmass\n|0âŸ©âŠ—8\n|Î¦+âŸ©âŠ—4\n|0âŸ©âŠ—8\n|Î¦+âŸ©âŠ—4\npT\n-\n-1.0e-3\n-9.1e-3\nmass\n-1.0e-3\n-9.1e-3\n-\nTABLE II: Correlation matrices between jet pT and mass\n(m) variables in the (a) target distribution, and samples\nobtained from the evaluation of the QCBMs constructed\nusing (b) Ansatz 1 and (c) Ansatz 2 in Figure 2 with\nthe trained parameters. Values displayed for initial states\nprepared in the all-zero state (olive) and a product of 4\nBell states (teal).\nFinally, we report on the correlation matrix between\nthe jet pT and mass variables used to construct the target\ndistribution. In IIa, the values associated with the target\ndistribution are displayed in black. If the trained QCBM\nlearned the joint distribution, one would expect to re-\ncover the correlation matrix when evaluating the QCBM\nwith the trained parameters. The results in Table II in-\ndicate that this is true for the QCBM constructed with\nAnsatz 1 (red), but not for the QCBM constructed with\nAnsatz 2. The correlation matrix for the latter case indi-\ncates that there is little correlation between the marginal\ndistributions in the synthetic samples.\n2.\n3D Distributions\nTo understand how the trainability of non-adversarial\ngenerative models scales with the number of quantum\n6\n0\n50\n100\n150\n200\n250\n300\noptimizer step\n10\n1\nJS(P|P )\nAnsatz 1 - 8,192 shots\nAnsatz 1 - 20,000 shots\nAnsatz 2 - 8,192 shots\nAnsatz 2 - 20,000 shots\nFIG. 8: JS divergence as a function of training step\nusing Nshots = 8192 (solid lines) and Nshots = 20, 000\n(triangles). Blue(red) points correspond to Ansatz 1(2).\nCircuits were initialized in the all zero state and trained\nto learn a 3D joint distribution. QCBM were initialized\nin the all-zero state (|Ïˆ0âŸ©= |0âŸ©âŠ—12.\nregisters, we increased the number of qubits in our model\nfrom 8 to 12. This increment translates into a larger num-\nber of basis states (28 = 256 to 212 = 4096). Further-\nmore, the joint probability distribution that we encode\nin the target state is now three-dimensional by including\nan additional marginal distribution associated with the\nâ€forwardnessâ€ of the jet with respect to the beam (jet\nÎ·). In Figure 8, the JS divergence loss is plotted as a\nfunction of training step.\nThe circuits were initialized\nin the all-zero state (|Ïˆ0âŸ©= |0âŸ©âŠ—12) and d = 6(12) to\nconstruct our QCBM with Ansatz 1(2). In this plot, we\ncan also see the eï¬€ect of increasing the number of shots\nduring training, reporting a signiï¬cant diï¬€erence in JS\ndivergence values when increasing Nshots from 8,192 to\n20,000.\nWhen training QCBM with Q = 12 qubits, we also\nused an initial state (|Ïˆ0âŸ©= |GHZâŸ©âŠ—Q/3) where 3-qubit\nsubsets are initialized in a GHZ state. The comparison\nfor the JS divergence value as a function of the training\nstep from the three initial states considered is displayed\nin Figure 9. For Ansatz 1 (blue), the JS value for the\nthree conï¬gurations is very similar for the ï¬rst 100 train-\ning steps. From step 100 on, the QCBM initialized in a\nproduct of Bell states (blue cross) trains faster and con-\nverges to a lower JS value than the QCBMs initialized\nin the all-zero (solid blue) and all-plus (blue star) state.\nFor Ansatz 2, the eï¬€ect of the initial state used in the\npreparation of the circuit has a more negligible eï¬€ect on\nthe minimal JS value the model converges after training.\nNonetheless, the QCBMs prepared from the all-plus state\n(stars) seem to take longer to converge.\nIn Figure 10, we compare the distributions of sam-\nples generated via projective measurements on the cir-\ncuits evaluated on the trained parameters. The circuits\nwere initialized in the all-zero state. By looking at Fig-\nure 10 and Table III, we observe a degraded performance\nin terms of similarity metric (Eq. 2) when compared to\nthe 8-qubit circuit results. The D(p|ep(Î˜)) value increases\nfrom 0.007153 to 0.02226 for Ansatz 1 and from 0.005452\nto 0.0108 for Ansatz 2. In Figure 11, we compare the dis-\nFIG. 9: JS divergence as a function of training step\nusing Nshots = 20, 000. Blue(red) points correspond to\nAnsatz 1(2). Circuits were initialized in the all-zero\nstate (solid lines), a product of Bell states (cross), or\nthe all-plus state (stars), and trained to learn a 3D joint\ndistribution.\n250\n300\n350\n400\n450\n500\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\nFraction of samples\n20\n40\n60\n80\n100\n120\n140\n160\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n2\n1\n0\n1\n2\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nAnsatz 1\nAnsatz 2\nMC Expectation\n250\n300\n350\n400\n450\n500\nJet pT [GeV]\n0.5\n1.0\n1.5\nP/P  ratio\n20\n40\n60\n80\n100\n120\n140\n160\nJet mass [GeV]\n0.5\n1.0\n1.5\n2\n1\n0\n1\n2\nJet  [GeV]\n0.5\n1.0\n1.5\nFIG. 10: Top: Target and sampled distributions.\nBlue(red) points correspond to Ansatz 1(2). Circuits\nwere initialized in the all zero state (|Ïˆ0âŸ©= |0âŸ©âŠ—12) and\ntrained to learn a 3D joint distribution. Bottom: Ratio\nof target and sampled distributions with horizontal\nguide lines marking P/ eP = 0.9 and P/ eP = 1.1.\nAnsatz Npar\nInitial State\nD(p|ep(Î˜))\n1\n432\n|Ïˆ0âŸ©= |0âŸ©âŠ—12\n0.0226\n1\n|Î¦+âŸ©\nâŠ—6\n0.0138\n1\n|Ïˆ0âŸ©= |GHZâŸ©âŠ—4\n0.0187\n2\n468\n|Ïˆ0âŸ©= |0âŸ©âŠ—12\n0.0108\n2\n|Î¦+âŸ©\nâŠ—6\n0.0090\n2\n|Ïˆ0âŸ©= |GHZâŸ©âŠ—4\n0.0106\nTABLE III: Comparison between target and sampled\ndistributions according to Eq. 2.\ntributions generated by QCBMs initialized in the three\ndiï¬€erent initial conï¬gurations. Again, the trained QCBM\nthat prepares the target distribution with the highest ï¬-\ndelity is Ansatz 2 and we see little dependence on the ini-\ntial state for QCBMs prepared using Ansatz 2. Nonethe-\nless, the eï¬€ect of the initial state in QCBMs prepared\nusing Ansatz 1 is now more evident.\n7\n250\n300\n350\n400\n450\n500\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\nFraction of samples\n20\n40\n60\n80\n100\n120\n140\n160\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n2\n1\n0\n1\n2\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nAnsatz 1 - |0\nQ\nAnsatz 1 - |\n+\nQ/2\nAnsatz 1 - | +\nq\nMC Expectation\n250\n300\n350\n400\n450\n500\nJet pT [GeV]\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\nFraction of samples\n20\n40\n60\n80\n100\n120\n140\n160\nJet mass [GeV]\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n2\n1\n0\n1\n2\nJet  [GeV]\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nAnsatz 1 - |0\nQ\nAnsatz 1 - |\n+\nQ/2\nAnsatz 1 - | +\nq\nMC Expectation\nFIG. 11: Top: Target and sampled distributions for\n12-qubit QCBM trained to learn a 3D joint distribution\nusing Ansatz 1. Registers were initialized in the all zero\nstate (solid blue line), a product of Bell states (black\ndashed line), and the all-plus state (gray dashed line).\nBottom: Target and sampled distributions for 12-qubit\nQCBM trained to learn a 3D joint distribution using\nAnsatz 2. Registers were initialized in the all zero state\n(solid red line), a product of Bell states (black dashed\nline), and the all-plus state (gray dashed line).\n(a) Monte Carlo (Ground Truth)\npT\nmass\nÎ·\npT\n-\n0.2\n7.3e-12\nmass\n0.2\n-\n2.7e-11\nÎ·\n7.3e-12 2.7e-11\n-\n(b) Ansatz 1\npT\nmass (m)\nÎ·\n|0âŸ©âŠ—12 |Î¦+âŸ©âŠ—6 |GHZâŸ©âŠ—4 |0âŸ©âŠ—12 |Î¦+âŸ©âŠ—6 |GHZâŸ©âŠ—4 |0âŸ©âŠ—12 |Î¦+âŸ©âŠ—6 |GHZâŸ©âŠ—4\npT\n-\n0.16\n0.16\n0.18\n8.2e-3\n1.2e-3\n-1.3e-3\nm\n0.16\n0.16\n0.18\n-\n-0.014\n4.4e-3\n7.7e-3\nÎ·\n8.2e-3\n1.2e-3\n-1.3e-3\n-0.014\n4.4e-3\n7.7e-3\n-\n(c) Ansatz 2\npT\nmass (m)\nÎ·\n|0âŸ©âŠ—12\n|Î¦+âŸ©âŠ—6 |GHZâŸ©âŠ—4\n|0âŸ©âŠ—12\n|Î¦+âŸ©âŠ—6 |GHZâŸ©âŠ—4\n|0âŸ©âŠ—12\n|Î¦+âŸ©âŠ—6 |GHZâŸ©âŠ—4\npT\n-\n-4.1e-3\n4.6e-3\n0.019\n-3.7e-3\n-9.1e-3\n1.6e-3\nm\n-4.1e-3\n4.6e-3\n0.019\n-\n6.3e-3\n4.9e-3\n2.2e-3\nÎ·\n-3.7e-3\n-9.1e-3\n1.6e-3\n6.3e-3\n4.9e-3\n2.2e-3\n-\nTABLE IV: Correlation matrices between jet pT , mass\n(m), and Î· variables in the (a) target distribution, and\nsamples obtained from the evaluation of the QCBMs con-\nstructed using (b) Ansatz 1 and (c) Ansatz 2 in Figure 2\nwith the trained parameters. Values displayed for initial\nstates prepared in the all-zero state (olive), a product of 6\nBell states (teal), and a product of 4 GHZ states (violet).\nFinally, we report on the correlation matrix between\nthe jet pT , mass and Î· variables used to construct the\ntarget distribution. In Table IV, the values associated\nwith the target distribution are displayed in black. The\nresults in Table IV display a slight discrepancy in the\noriginal correlation matrix (training dataset) and that\nfor the samples generated by sampling Ansatz 1 (blue)\nwith the trained parameters. Again, for the QCBM con-\nstructed with Ansatz 2, the matrix values indicate that\nthere is little correlation between the marginal distribu-\ntions in the synthetic samples.\nB.\nNoisy Training with Layer-wise Coordinate\nDescent\nThe gradient-based training of the QCBM models pre-\nsented in IV A was executed on noiseless (ideal) qubits\nbut was not used for training on noisy hardware. How-\never, the performance of the trained QCBM model on\nnear-term quantum devices will be heavily impacted by\nhardware noise. Qubit initialization, gate noise, and er-\nrors in the circuit measurement step, all result in state\npreparation error that can cause parameterized models\nto converge to maximally mixed states, with an overall\neï¬€ect of ï¬‚attening the loss landscape [29] and manifests\nin a noisy estimation of ePÎ˜(x).\nWhile developing error mitigation methods that can\nbe incorporated into variational training algorithms is\nan open area of research, one approach to noise mitiga-\ntion is to implement the circuit training with hardware\nnoise in order to learn optimized parameters that can\ncompensate for time-independent errors, such as over-\nand under-rotation in single qubit gates. We test the ro-\nbustness of the ï¬nal parameters found in Section IV A\nto hardware noise by ï¬rst executing the trained QCBM\nmodels constructed with Ansatz 2 on superconducting\nqubit devices. The 8-qubit QCBM (276/312 parameters)\nwas executed on the 16-qubit device ibmq guadalupe and\nthe 12-qubit QCBM (468) was executed on the 27-qubit\ndevice ibm cairo, both were accessed through a cloud-\nbased queue.\nIn the absence of error and noise mitigation we instead\nopted to use a localized search over individual parame-\nters. Each parameterized Ansatz (shown in Fig. 2) are\nconstructed with layers of parameterized rotation gates,\nimplemented using an arbitrary unitary gate with 3 ro-\ntational parameters. Then, we used a layer-wise coor-\ndinate descent (LCD) method to optimize each QCBM\nperformance on hardware. The workï¬‚ow is shown in Fig.\n12. No readout error mitigation or other noise mitiga-\ntion methods were used. LCD optimizes the parameters\nof a circuit U(Î˜) by searching the multi-dimensional pa-\nrameter space along linear cuts of ï¬nite width. With N\nqubits in the register, each parameter was swept through\na shift of parameters deï¬ned by Ïµ = âˆ’Ï€/4 and spac-\ning 2Ïµ/n.\nThe targeted backends allowed for a maxi-\nmum number of circuits per batch (B) which deï¬nes the\nspacing n = B/N. For the 8-qubit QCBM trained on\nibmq guadalupe this resulted in a grid spacing of 0.0419.\nFor the 12-qubit QCBM trained on ibm cairo this re-\nsulted in a mesh spacing of 0.0628.\nEach circuit was\nsampled using Nshots = 20000.The rotational parameters\n8\nğ‘…â„“(ğœƒ\"\n#)\nğ’°[\"#$,&]\nğ‘…â„“(ğœƒ$\n#)\nğ’°[(,\")$] \nğ›¿% = {âˆ’ğœ–+ 2 ğœ–\nğ‘›ğ‘—: ğ‘—âˆˆ[0, â€¦ ğ‘›]}\nğš¯\" = min\n7 {â„’8, â„’9, â„’:, â„’;, â€¦ , â„’<Ã—> }\nğ’°[\",&](ğš¯$)\nğ’°(,\")+ (ğš¯)\nğ‘…â„“(ğš¯ğ¢ )\nğ’°[\"#$,&] (ğš¯$)\nğ’°(,\")$ (ğš¯)\nğ‘…'(ğš¯ğ¢(ğŸ)\nQPU\nğ‘…â„“(ğœƒ\"\n# + ğ›¿%)\nğ’°[\"#$,&]\nğ‘…â„“(ğœƒ$\n#)\nğ’°[(,\")$] \nğ‘…â„“(ğœƒ\"\n#)\nğ’°[\"#$,&]\nğ‘…â„“(ğœƒ$\n# + ğ›¿%)\nğ’°[(,\")$] \n(a)\n(b)\n(c)\n(d)\n(e)\nFIG. 12: Layer-wise coordinate descent (LCD) workï¬‚ow. (a) a single rotational layer in a N-qubit QCBM model,\n(b) is composed of rotational gates Râ„“acting on individual qubits. A batch of n Ã— N circuits is constructed by\nsweeping each individual gate over a discrete set of n shifts and executed on a quantum processor (QPU). The loss is\nevaluated for each circuit executed (d), the parameter vector Î˜ is updated by the values which return the minimal\nvalue, and the updated parameter vector is used to start the search over the next rotational layer (e).\nare optimized starting with the gates closes to the mea-\nsurement process. If the noiseless simulation returned an\nexcessively ï¬‚at landscape (variance of the loss was below\n5eâˆ’7) these experiments were not executed on hardware.\nThe top of Figure 13 displays the quartiles of JS loss\nover each iteration of LCD with 8-qubit QCBM circuits.\nThe quartiles are plotted for the QCBM constructed with\nAnsatz 1 and 2 and evaluated with the updated parame-\nters after each iteration on the ibmq guadalupe backend\nin blue and red, respectively. The plot also shows how\nthe JS divergence value degrades as the LCD training pa-\nrameters deviate from those obtained during the noise-\nless training. On the other hand, hardware performance\nis relatively stable. The bottom of Figure 13 shows the\nsampled distributions generated by the evaluation of the\nQCBM with the parameters that yielded the lowest JS\ndivergence value during the LCD training. We observe\na more signiï¬cant discrepancy between the target and\nsampled distributions compared to the results reported\nin Figure 4, where the QCBM is evaluated with the pa-\nrameters obtained during the noiseless training. In Fig-\nure 14, the quartiles of JS loss (top) and the sampled and\ntarget distributions (bottom) are displayed. The QCBMs\nare constructed using Ansatz 2, to prepare a 3D joint dis-\ntribution in a 12-qubit register. The LCD training was\nperformed on the ibm cairo device.\nIn Figure 15 we show the quartiles of JS for three iter-\nations representative of the diï¬€erent stages of the LCD\ntraining (left). During the ï¬rst iterations in the training,\nthe loss landscape is very ï¬‚at, and the JS value is lower\nfor the QCBMs evaluated on the qasm simulator. The\nbox plots on the left correspond to the JS values obtained\nduring the parameter sweep in the range Ïµ âˆˆ[âˆ’Ï€\n4 , Ï€\n4 ] for\na particular rotation gate in a given layer. Then, we ob-\nserve that, for subsequent iterations, the parameter tun-\ning moves the parameters into regions where the perfor-\nmance of the noiseless simulator is degraded, and slowly\nimproving the performance on the quantum device.\nAs we can see from Figures 13 and 14, noisy train-\ning can improve performance, but there are a number of\nobstacles. When the training is initialized with param-\neters pre-trained with noiseless qubits, multiple intera-\ntions may be needed to re-train. The gradual improve-\nments in the loss function may not be robust against large\ndeviations in the device noise. For example, executing\nthe LCD workï¬‚ow over all rotational parameters of the\n12-qubit QCBM was done over multiple days, probably\ncausing the discrete change in the loss between iterations\n31 and 32.\nV.\nQCBM DESIGN SPACE\nParameterized circuit ansatz used for variational algo-\nrithms need to balance expressability with trainability.\nThere are considerable ongoing eï¬€orts in determining the\ncharacteristics of circuits that lead to eï¬€ective training\nand scaling. In this paper we used two diï¬€erent ansatz\ndesigns and multiple initializations for the Q-qubit reg-\nister. In this section we discuss some observations based\n9\n250\n300\n350\n400\n450\n500\nJet pT [GeV]\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\nFraction of samples\n20\n40\n60\n80\n100\n120\n140\n160\nJet mass [GeV]\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nAnsatz 1 - ibmq_guadalupe\nAnsatz 2 - ibmq_guadalupe\nMC Expectation\nFIG. 13: Top: Quartiles of Jensen-Shannon loss over each iteration of LCD with 8 qubit QCBM circuits. Bottom:\nTarget and sampled distributions constructed using Ansatz 1(2) in blue (red). Circuits were initialized in the\nall-zero state and evaluated on ibmq guadalupe with an 8-qubit QCBM. Distributions for the best parameters found\nafter the execution of the LCD workï¬‚ow.\non our results reported in Sections IV A and IV B. The\nQCBM models we trained in this paper used the same pa-\nrameterization (the arbitrary rotation gate implemented\nin PennyLane) and entangling gate operation (the CNOT\ngate). Each feature was encoded into the same number\nof qubits (4) which deï¬ned the size of the overall register:\n8 qubits for 2D distributions, 12 qubits for 3D distribu-\ntions.\nBetween the two ansatz designs, only Ansatz 1 can\ngenerate arbitrary Q-qubit entanglement and ï¬t arbi-\ntrary correlations between 2- or 3-variables, regardless\nof the choice of initial state. However, as shown in Figs.\n3,5,8,11 this ansatz slowly learns. On the other hand,\nAnsatz 2 quickly learns, but only prepares a product\nstate of 4-qubit systems. When the qubit register is ini-\ntialized in the all zero state |0âŸ©âŠ—Q or as a set of Bell states\n|Î¦+âŸ©âŠ—Q, then Ansatz 2 cannot by deï¬nition, model ar-\nbitrary correlations between each 4 qubit subset.\nFor\nQ = 12, if the circuit is initialized with |GHZâŸ©âŠ—4, then\nthere is local entanglement between the qubit subsets.\nYet as reported in Tables II and IV, this is insuï¬ƒcient\nto capture the correlations as seen in the Monte Carlo\ndata. We observe a trade-oï¬€in the modeling capacity of\nAnsatz 1 and Ansatz 2: Ansatz 1 can model the corre-\nlations between variables but has lower ï¬delity in ï¬tting\nmarginal distributions (as quantiï¬ed by Eq. 2 in Tables\nI and III and seen in Figs. 4,10). On the other hand,\nfor Ansatz 2 the generated data fails to capture the cor-\nrelations in the Monte Carlo data, but has high ï¬delity\nin ï¬tting marginal distributions (as reported in Tables\nII,IV). Simply including local correlations in the initial\nstate by using |GHZâŸ©âŠ—4 was insuï¬ƒcient to generate high\ncorrelations between variables pT and mass.\n10\n250\n300\n350\n400\n450\n500\nJet pT [GeV]\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\nFraction of samples\n20\n40\n60\n80\n100\n120\n140\n160\nJet mass [GeV]\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n2\n1\n0\n1\n2\nJet  [GeV]\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nAnsatz 2 - ibmq_guadalupe\nMC Expectation\nFIG. 14: Top: Quartiles of Jensen-Shannon loss over each iteration of LCD implemented in ibm cairo with a\n12-qubit QCBM. Bottom: Target (gray) and sampled distributions constructed using Ansatz 2 in blue (red lines).\nCircuits were initialized in the all-zero state and evaluated on ibm cairo with an 12-qubit QCBM. Distributions for\nthe best parameters found after the execution of the LCD workï¬‚ow.\nVI.\nCONCLUSION\nThe size of the design space associated with parameter-\nized quantum circuit models is large. This work demon-\nstrated the eï¬ƒcacy of non-adversarial unsupervised train-\ning of generative models implemented as parameterized\nquantum circuits.\nWe demonstrate the usefulness of\nthese quantum models in the context of a HEP appli-\ncation and to assist other practitioners, we have used:\nseveral circuit ansatzes found in the quantum computing\nliterature and tested the trainability of QCBM initialized\nwith diï¬€erent quantum states. We quantify the ï¬delity\nof the trained models using: the JS score (also used to\ntrain the models), the MAE of feature marginals, and the\ncorrelation matrices of generated data.\nWe are encouraged by the success of gradient-based\ntraining for 12 qubit QCBM. We show that for two and\nthree correlated variables, both ansatz can minimize the\nloss to the order of âˆ¼10âˆ’2, but whether that corresponds\nto models that can faithfully reproduce the kinematic\ndistributions of a jet in a pp collision typical of the LHC\nexperiment cannot be deduced by the training loss alone.\nOnly Ansatz 1 can ï¬t the correlations between variables\nin the absence of noise, but the individual marginal ï¬ts\nfor Ansatz 1 are lower ï¬delity than Ansatz 2. On the\nother hand, while Ansatz 2 can reproduce the individual\nmarginals with high ï¬delity it cannot by deï¬nition, model\ncorrelations, as we see in Tables II and IV. We report\nthese results to assist other practitioners in the design of\nparameterized ansatz for scientiï¬c applications.\nWe also report on the inï¬‚uence of hardware noise on\nthe QCBM performance. In our study, the QCBM were\ntrained in the absence of noise and certain trained models\nwere deployed on near-term devices. In our results we\nobserve that hardware noise ï¬‚attens out the landscape.\nAdditionally, we observe that the addition of hardware\nnoise does not lead to an increase in correlation between\nthe encoded variables.\nTraining in the presence of hardware noise (e.g. us-\ning local parameter tuning or LCD) can improve perfor-\nmance, however, without robust error mitigation hard-\nware ï¬‚uctuations can undo small improvements in per-\nformance (see Fig. 14). Designing scale-able error mit-\n11\nFIG. 15: Left: Quartiles of Jensen-Shannon loss for three representative iterations in the LCD scheme. Each\niteration right corresponds to a parameter sweep in the range [âˆ’Ï€\n4 , Ï€\n4 ] for a particular rotation gate in a given layer.\nThe plots display the JS values for a 8-qubit QCBM trained on ibmq guadalupe using Ansatz 1 (top) and Ansatz\n2(middle). The bottom plot corresponds to a 12-qubit QCBM constructed using Ansatz 2 and trained on ibm cairo.\n12\nigation methods that fully capture correlations in the\nhardware is an active area of research in quantum com-\nputing.\nFor example, commonly employed methods of\nreadout error mitigation using measurement ï¬delity ma-\ntrices [30, 31]. These methods have an advantage in that\nthe can be incorporated in to variational training work-\nï¬‚ows (either gradient-based optimization or LCD) as a\ndata post-processing step. This motivates the need for\na systematic follow up study of error mitigation eï¬ƒcacy\nin this application, with a focus on balancing overhead\nwith model performance.\nACKNOWLEDGMENTS\nThis work was partially supported by the Quantum\nInformation Science Enabled Discovery (QuantISED) for\nHigh Energy Physics program at ORNL under FWP\nERKAP61.\nThis work was partially supported by the\nLaboratory Directed Research and Development Pro-\ngram of Oak Ridge National Laboratory, managed by\nUT-Battelle, LLC, for the U. S. Department of Energy.\nThis work was partially supported as part of the ASCR\nTestbed Pathï¬nder Program at Oak Ridge National Lab-\noratory under FWP ERKJ332. This work was partially\nsupported as part of the ASCR Fundamental Algorith-\nmic Research for Quantum Computing Program at Oak\nRidge National Laboratory under FWP ERKJ354. This\nresearch used quantum computing system resources of\nthe Oak Ridge Leadership Computing Facility, which\nis a DOE Oï¬ƒce of Science User Facility supported un-\nder Contract DE-AC05-00OR22725. Oak Ridge National\nLaboratory manages access to the IBM Q System as\npart of the IBM Q Network.\nThe authors would like\nto thank Dr. Phil Lotshaw for helpful comments during\nthe manuscript preparation.\n[1] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\nGenerative adversarial nets, in Advances in Neural Infor-\nmation Processing Systems, Vol. 27, edited by Z. Ghahra-\nmani, M. Welling, C. Cortes, N. Lawrence, and K. Q.\nWeinberger (Curran Associates, Inc., 2014).\n[2] M. Paganini, L. de Oliveira, and B. Nachman, Calogan:\nSimulating 3d high energy particle showers in multilayer\nelectromagnetic calorimeters with generative adversarial\nnetworks, Phys. Rev. D 97, 014021 (2018).\n[3] L. de Oliveira, M. Paganini, and B. Nachman, Learn-\ning particle physics by example: Location-aware genera-\ntive adversarial networks for physics synthesis, Comput-\ning and Software for Big Science 1, 10.1007/s41781-017-\n0004-6 (2017).\n[4] P. Musella and F. Pandolï¬, Fast and accurate simu-\nlation of particle detectors using generative adversarial\nnetworks, Computing and Software for Big Science 2,\n10.1007/s41781-018-0015-y (2018).\n[5] A. Butter, T. Plehn, and R. Winterhalder, How to\nGAN LHC events, SciPost Physics 7, 10.21468/scipost-\nphys.7.6.075 (2019).\n[6] R. Di Sipio, M. F. Giannelli, S. K. Haghighat, and\nS. Palazzo, Dijetgan:\na generative-adversarial net-\nwork approach for the simulation of qcd dijet events\nat the lhc, Journal of High Energy Physics 2019,\n10.1007/jhep08(2019)110 (2019).\n[7] B. Hashemi,\nN. Amin,\nK. Datta,\nD. Olivito, and\nM. Pierini, Lhc analysis-speciï¬c datasets with generative\nadversarial networks (2019), arXiv:1901.05282 [hep-ex].\n[8] Y. Alanazi, N. Sato, T. Liu, W. Melnitchouk, P. Am-\nbrozewicz, F. Hauenstein, M. P. Kuchera, E. Pritchard,\nM. Robertson, R. Strauss, L. Velasco, and Y. Li, Simu-\nlation of electron-proton scattering events by a feature-\naugmented and transformed generative adversarial net-\nwork (FAT-GAN), in Proceedings of the Thirtieth Inter-\nnational Joint Conference on Artiï¬cial Intelligence (In-\nternational Joint Conferences on Artiï¬cial Intelligence\nOrganization, 2021).\n[9] C. Bravo-Prieto, J. Baglio, M. C`e, A. Francis, D. M.\nGrabowska, and S. Carrazza, Style-based quantum gener-\native adversarial networks for monte carlo events (2021),\narXiv:2110.06933 [quant-ph].\n[10] S. Y. Chang, S. Herbert, S. Vallecorsa, E. F. Com-\nbarro, and R. Duncan, Dual-parameterized quantum cir-\ncuit GAN model in high energy physics, EPJ Web of\nConferences 251, 03050 (2021).\n[11] S. Y. Chang,\nS. Vallecorsa,\nE. F. Combarro, and\nF. Carminati, Quantum generative adversarial networks\nin a continuous-variable architecture to simulate high en-\nergy physics detectors (2021), arXiv:2101.11132 [quant-\nph].\n[12] A. PÂ´erez-Salinas, J. Cruz-Martinez, A. A. Alhajri, and\nS. Carrazza, Determining the proton content with a\nquantum computer, Physical Review D 103, 034027\n(2021).\n[13] J.-G. Liu and L. Wang, Diï¬€erentiable learning of quan-\ntum circuit born machines, Physical Review A 98, 062324\n(2018).\n[14] K.\nE.\nHamilton,\nE.\nF.\nDumitrescu,\nand\nR.\nC.\nPooser, Generative model benchmarks for superconduct-\ning qubits, Physical Review A 99, 062323 (2019).\n[15] M.\nBenedetti,\nD.\nGarcia-Pintos,\nY.\nNam,\nand\nA. Perdomo-Ortiz, A generative modeling approach for\nbenchmarking and training shallow quantum circuits,\narXiv preprint arXiv:1801.07686 (2018).\n[16] S. Mohamed and B. Lakshminarayanan, Learning in im-\nplicit generative models, arXiv preprint arXiv:1610.03483\n(2016).\n[17] S. Sim, P. D. Johnson, and A. Aspuru-Guzik, Express-\nibility and entangling capability of parameterized quan-\ntum circuits for hybrid quantum-classical algorithms, Ad-\nvanced Quantum Technologies 2, 1900070 (2019).\n[18] Y. Du, M.-H. Hsieh, T. Liu, and D. Tao, Expressive\npower of parametrized quantum circuits, Physical Review\nResearch 2, 10.1103/physrevresearch.2.033125 (2020).\n[19] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Bab-\nbush, and H. Neven, Barren plateaus in quantum neural\nnetwork training landscapes, Nature communications 9,\n4812 (2018).\n13\n[20] M. Cerezo, A. Sone, T. Volkoï¬€, L. Cincio, and P. J.\nColes, Cost function dependent barren plateaus in shal-\nlow parametrized quantum circuits, Nature Communica-\ntions 12, 1 (2021).\n[21] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, M. S.\nAlam, S. Ahmed, J. M. Arrazola, C. Blank, A. Del-\ngado, S. Jahangiri, K. McKiernan, J. J. Meyer, Z. Niu,\nA. SzÂ´ava, and N. Killoran, Pennylane: Automatic dif-\nferentiation of hybrid quantum-classical computations\n(2020), arXiv:1811.04968 [quant-ph].\n[22] M. Schuld, V. Bergholm, C. Gogolin, J. Izaac, and N. Kil-\nloran, Evaluating analytic gradients on quantum hard-\nware, Physical Review A 99, 032331 (2019).\n[23] D. P. Kingma and J. Ba, Adam: A method for stochastic\noptimization, arXiv preprint arXiv:1412.6980 (2014).\n[24] J. Alwall, M. Herquet, F. Maltoni, O. Mattelaer, and\nT. Stelzer, MadGraph 5 : Going Beyond, JHEP 06, 128,\narXiv:1106.0522 [hep-ph].\n[25] T. SjÂ¨ostrand, S. Mrenna, and P. Skands, A brief introduc-\ntion to PYTHIA 8.1, Computer Physics Communications\n178, 852 (2008).\n[26] J. de Favereau, , C. Delaere, P. Demin, A. Giammanco,\nV. LemaË†Ä±tre, A. Mertens, and M. Selvaggi, DELPHES 3:\na modular framework for fast simulation of a generic col-\nlider experiment, Journal of High Energy Physics 2014,\n10.1007/jhep02(2014)057 (2014).\n[27] M. Cacciari, G. P. Salam, and G. Soyez, The anti-ktjet\nclustering algorithm, Journal of High Energy Physics\n2008, 063 (2008).\n[28] M. Cacciari, Fastjet: a code for fast kt clustering, and\nmore (2006), arXiv:hep-ph/0607071 [hep-ph].\n[29] S. Wang, E. Fontana, M. Cerezo, K. Sharma, A. Sone,\nL.\nCincio,\nand\nP.\nJ.\nColes,\nNoise-induced\nbarren\nplateaus\nin\nvariational\nquantum\nalgorithms\n(2020),\narXiv:2007.14384 [quant-ph].\n[30] K. E. Hamilton and R. C. Pooser, Error-mitigated data-\ndriven circuit learning on noisy quantum hardware,\nQuantum Machine Intelligence 2, 10.1007/s42484-020-\n00021-x (2020).\n[31] K. E. Hamilton, T. Kharazi, T. Morris, A. J. McCaskey,\nR. S. Bennink, and R. C. Pooser, Scalable quantum\nprocessor noise characterization, in 2020 IEEE Interna-\ntional Conference on Quantum Computing and Engineer-\ning (QCE) (IEEE, 2020) pp. 430â€“440.\n",
  "categories": [
    "quant-ph",
    "hep-ex"
  ],
  "published": "2022-03-07",
  "updated": "2022-03-07"
}