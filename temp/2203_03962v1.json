{
  "id": "http://arxiv.org/abs/2203.03962v1",
  "title": "Generative Cooperative Learning for Unsupervised Video Anomaly Detection",
  "authors": [
    "Muhammad Zaigham Zaheer",
    "Arif Mahmood",
    "Muhammad Haris Khan",
    "Mattia Segu",
    "Fisher Yu",
    "Seung-Ik Lee"
  ],
  "abstract": "Video anomaly detection is well investigated in weakly-supervised and\none-class classification (OCC) settings. However, unsupervised video anomaly\ndetection methods are quite sparse, likely because anomalies are less frequent\nin occurrence and usually not well-defined, which when coupled with the absence\nof ground truth supervision, could adversely affect the performance of the\nlearning algorithms. This problem is challenging yet rewarding as it can\ncompletely eradicate the costs of obtaining laborious annotations and enable\nsuch systems to be deployed without human intervention. To this end, we propose\na novel unsupervised Generative Cooperative Learning (GCL) approach for video\nanomaly detection that exploits the low frequency of anomalies towards building\na cross-supervision between a generator and a discriminator. In essence, both\nnetworks get trained in a cooperative fashion, thereby allowing unsupervised\nlearning. We conduct extensive experiments on two large-scale video anomaly\ndetection datasets, UCF crime, and ShanghaiTech. Consistent improvement over\nthe existing state-of-the-art unsupervised and OCC methods corroborate the\neffectiveness of our approach.",
  "text": "Generative Cooperative Learning for Unsupervised Video Anomaly Detection\nM. Zaigham Zaheer1,2,3,5, Arif Mahmood4, M. Haris Khan5, Mattia Seg`u3, Fisher Yu3, Seung-Ik Lee1,2\nElectronics and Telecommunications Research Institute1, Univ. of Science and Technology2,\nETH Zurich3, Information Technology Univ.4, Mohamed bin Zayed Univ. of Artiﬁcial Intelligence5\nAbstract\nVideo anomaly detection is well investigated in weakly-\nsupervised and one-class classiﬁcation (OCC) settings.\nHowever, unsupervised video anomaly detection methods\nare quite sparse, likely because anomalies are less frequent\nin occurrence and usually not well-deﬁned, which when\ncoupled with the absence of ground truth supervision, could\nadversely affect the performance of the learning algorithms.\nThis problem is challenging yet rewarding as it can com-\npletely eradicate the costs of obtaining laborious annota-\ntions and enable such systems to be deployed without hu-\nman intervention. To this end, we propose a novel unsuper-\nvised Generative Cooperative Learning (GCL) approach\nfor video anomaly detection that exploits the low frequency\nof anomalies towards building a cross-supervision between\na generator and a discriminator. In essence, both networks\nget trained in a cooperative fashion, thereby allowing unsu-\npervised learning. We conduct extensive experiments on two\nlarge-scale video anomaly detection datasets, UCF crime\nand ShanghaiTech. Consistent improvement over the exist-\ning state-of-the-art unsupervised and OCC methods corrob-\norate the effectiveness of our approach.\n1. Introduction\nIn the real world, learning-based anomaly detection tasks\nare extremely challenging mainly because of the rare oc-\ncurrence of such events. The challenge further exacerbates\nowing to the unconstrained nature of these events. Obtain-\ning sufﬁcient anomaly examples is thus quite cumbersome,\nwhile one may safely assume that an exhaustive set, par-\nticularly required for training fully-supervised models, will\nnever be collected. To make learning tractable, anomalies\nhave often been attributed as signiﬁcant deviations from\nthe normal data. Therefore, a popular approach towards\nanomaly detection is to train a one-class classiﬁer which\nlearns the dominant data representations using only normal\ntraining examples [13, 16, 24, 27, 40, 41, 44, 46, 58, 62, 68]\n(Fig. 1). A noticeable drawback of one-class classiﬁca-\ntion (OCC) based methods is the limited availability of the\n(b) One-class\n(c) Weakly-supervised\n(d) Unsupervised\n(a) Fully Supervised\nAnomalous Frame\nNormal Frame\nA Anomalous Video\n...\n...\n...\n...\nA\nUnlabeled Video\nFigure 1. Different training modes for video anomaly detection:\n(a) Fully supervised mode requires frame-level normal/abnormal\nannotations in the training data.\n(b) One-Class Classiﬁcation\n(OCC) requires only normal training data. (c) Weakly supervised\nmode requires video-level normal/abnormal annotations. (d) Un-\nsupervised mode requires no training data annotations.\nnormal training data, not capturing all the normalcy varia-\ntions [8]. In addition, the OCC approaches are usually un-\nsuitable for complex problems with diverse multiple classes\nand a wide range of dynamic situations often found in video\nsurveillance. In such cases, an unseen normal activity may\ndeviate signiﬁcantly enough from the learned normal repre-\nsentations to be predicted as anomalous, resulting in more\nfalse alarms [13,63,64].\nRecently, weakly supervised anomaly detection methods\nhave gained signiﬁcant popularity [23, 25, 33, 45, 54, 61]\nthat reduce the cost of obtaining manual ﬁne-grained an-\nnotations by employing video-level labels [49, 63–65, 70].\nSpeciﬁcally, a video is labeled as anomalous if some of its\ncontents are anomalous and normal if all of its contents\nare normal, requiring careful manual inspection of the full\nvideo contents. Although such annotations are relatively\ncost-effective, yet remain impractical in many real-world\napplications. There is a plethora of video data, speciﬁcally\nraw footage, that can be leveraged for anomaly detection\ntraining if no annotation cost is incurred. Unfortunately, to\nthe best of our knowledge, there are hardly any notable at-\ntempts in leveraging the unlabelled training data for video\nanomaly detection.\nIn the current work, we explore unsupervised mode for\nvideo anomaly detection that is certainly more challenging\nthan fully, weakly or one-class supervision (Fig. 1). How-\never, it is also more rewarding due to minimal assumptions\nand hence will encourage the development of novel and\nmore practical algorithms. Note that, the term ‘unsuper-\nvised’ in literature often refers to OCC approaches which\nassume all normal training data [10,36,62]. However, it ren-\narXiv:2203.03962v1  [cs.CV]  8 Mar 2022\nders the overall learning problem partially supervised [18].\nIn approaching unsupervised anomaly detection in surveil-\nlance videos, we exploit the simple facts that videos are\ninformation-rich compared to still images and anomalies\nare less frequent than the normal happenings [7,28,50,64],\nand attempt to leverage such domain knowledge in a struc-\ntured manner.\nTo this end, we propose a Generative Cooperative\nLearning (GCL) method which takes unlabelled videos as\ninput and learns to predict frame-level anomaly score pre-\ndictions as output. The proposed GCL comprises two key\ncomponents, a generator and a discriminator, which es-\nsentially get trained in a mutually cooperative manner to-\nwards improving the anomaly detection performance. The\ngenerator not only reconstructs the abundantly available\nnormal representations but also distorts the possible high-\nconﬁdence anomalous representations by using a novel\nnegative learning (NL) approach.\nThe discriminator in-\nstead estimates the probability of an instance to be anoma-\nlous. Since we approach unsupervised anomaly detection,\nwe create pseudo-labels from generator and use these to\ntrain the discriminator and in the following step, we cre-\nate pseudo-labels from the trained version of discriminator\nand then use these to improve the generator. The overall\nsystem is trained in an alternate training fashion where, in\neach iteration, both the generator and the discriminator get\nimproved with mutual cooperation.\nContributions. We propose an anomaly detection approach\ncapable of localizing anomalous events in complex surveil-\nlance scenarios without requiring labelled training data. To\nthe best of our knowledge, our method is the ﬁrst rigorous\nattempt tackling the surveillance videos anomaly detection\nin a fully unsupervised mode. A novel Generative Coopera-\ntive Learning (GCL) framework is proposed that comprises\na generator, a discriminator, and cross-supervision.\nThe\ngenerator network is forced not to reconstruct anomalies by\nusing a novel negative learning approach. Extensive exper-\niments on two large-scale complex anomalous event detec-\ntion datasets, UCF-Crime and ShanghaiTech, show that our\nmethod provides visible gains over the baselines and several\nexisting unsupervised as well as OCC methods.\n2. Related Work\nAnomaly detection is a widely studied problem both in\nthe image [6, 15, 38] and video domain [48, 49, 62, 64, 65].\nWe here introduce different supervision modes for video\nanomaly detection and traditional mutual learning strategies\nAnomaly Detection as One-Class Classiﬁcation (OCC).\nOCC based approaches have found their way in a wide\nrange of anomaly detection problems including, medical di-\nagnosis [56], cyber security [10], surveillance security sys-\ntems [19,28,31,62], and industrial inspection [4]. Some of\nthese approaches use hand-crafted features [2,30,37,53,67],\nwhile others use deep features extracted using pre-trained\nmodels [41, 46].\nWith the advent of generative models,\nmany approaches proposed variants of such networks to\nlearn representations corresponding to normal data [11,34,\n35, 42–44, 59, 60, 62]. OCC based approaches ﬁnd it chal-\nlenging to avoid well-reconstruction of anomalous test in-\nputs. This problem is attributed to the fact that since OCC\napproaches only use normal class data while training, an\nineffective classiﬁer boundary may be achieved which is\nlimited in enclosing normal data while excluding anoma-\nlies [62]. In an attempt to address this limitation, some re-\nsearchers recently proposed pseudo-supervised methods in\nwhich pseudo-anomaly instances are generated using nor-\nmal training data [1,62].\nWeakly Supervised (WS) Anomaly Detection.\nVideo-\nlevel binary annotations are used to train WS classiﬁers\ncapable of predicting frame-level anomaly scores [39, 49,\n51, 63–65, 70].\nVideo-level labels are provided in such\na way that a normal labeled video is completely normal\nwhereas an anomalous labeled video contains both normal\nand anomalous contents without any information about the\ntemporal whereabouts (Fig. 1).\nUnsupervised Anomaly Detection.\nAnomaly detection\nmethods using unlabelled training data are quite sparse in\nliterature. According to the nomenclature shown in Fig. 1,\nmost unsupervised methods in the literature actually fall in\nthe category of OCC. For instance, MVTecAD [4] bench-\nmark ensures the training data to be only normal, there-\nfore its evaluation protocol is OCC and the methods inher-\niting this assumption are also essentially one-class classi-\nﬁers [5, 11]. In contrast to these algorithms, our proposed\nGCL approach is capable of learning from unlabelled train-\ning data without assuming any normalcy. The training data\nin the form of videos conform to several important attributes\nregarding anomaly detection, such as, anomalies are less\nfrequent than normal events and events are often temporally\nconsistent. We derive our motivation from these clues to\ncarry out the training in a completely unsupervised fashion.\nTeacher Student Networks.\nOur proposed GCL shares\nsome similarities with the Teacher Student (TS) frameworks\nfor knowledge distillation [17]. GCL is different from TS\nframework mainly because its aim is not knowledge distil-\nlation. Also our generator generates noisy labels while our\ndiscriminator, being relatively robust to noise, cleans these\nlabels which is not the case in TS framework.\nMutual Learning (ML). The GCL framework also shares\nsimilarities with the ML algorithms [69]. However, the two\ncomponents of the GCL learn different types of informa-\ntion and are trained with cross-supervision in contrast to the\nsupervised learning used by the ML algorithms. Further in\nGCL, the output of each network is passed through a thresh-\nold process to produce pseudo-labels. In ML frameworks,\nthe cohort learns to match the distributions of each member\nFeature Extractor\nDiscriminator (D)\nGenerator (G)\n...\nInput Batches\nUnlabeled Videos\nPseudo-labels\nfrom Generator\nPseudo-labels \nfrom Discriminator\nG Loss\nD Loss\nTraining Data Organization\nV1\nV2\nVn\n…\nGenerative Cooperative Learning Framework\nFigure 2. Proposed Generative Cooperative Learning (GCL) algorithm introduces cross-supervision for training a Generator G and a\nDiscriminator D. The pseudo-labels produced by G are used to compute the D loss and likewise, the pseudo-labels produced by the D are\nutilized to compute the G loss. Both G and D are trained iteratively from unlabelled training data for anomalous events detection.\nwhile in GCL each member tries to learn from the pseudo-\nlabels generated by the other. A mutual learning of a cohort\nin unsupervised mode using unlabelled training data is un-\nexplored yet.\nDual Learning. It is also a related method in which two\nlanguage translation models interactively teach each other\n[14]. However, the external supervision is provided using\npre-trained unconditional language expert models which\ncheck the quality of translations. This way, different models\nhave different learning tasks whereas in our proposed GCL\napproach the learning tasks are identical.\nAnother variant of Cooperative Learning [3] has been\npreviously proposed to learn multiple models jointly for the\nsame task across different domains.\nFor instance, object recognition is formulated by train-\ning a model on RGB images and another model on depth\nimages which then communicate the domain invariant ob-\nject attributes. Whereas, in our GCL approach both models\naddress the same task in the same domain.\n3. Method\nOur proposed Generative Cooperative Learning ap-\nproach for Anomaly Detection (GCL) comprises a feature\nextractor, a generator network, a discriminator network, and\ntwo pseudo-label generators. Fig. 2 shows the overall archi-\ntecture. Each of the components are discussed next.\n3.1. Training Data Organization\nTo minimize the computational complexity and to re-\nduce the training time of GCL, similar to the existing SOTA\n[49,51,63–65,70], we also utilize a deep feature extractor to\nconvert video data into compact features. All input videos\nare arranged as segments, features of which are then ex-\ntracted. Furthermore, these features are randomly arranged\nas batches. In each iteration a randomly selected batch is\nused to train the GCL model (Fig. 2). Formally, given a\ntraining dataset of n videos, every video is partitioned into\nnon-overlapping segments S(i,j) of p frames each, where\ni ∈[1, n] is the video index and j ∈[1, mi] is the seg-\nment index. The segment size p is kept the same across all\ntraining and test videos of a dataset.\nFor each S(i,j), a feature vector f(i,j) ∈Rd is computed\nas f(i,j)=E(S(i,j)) using the feature extractor E(·).\nIn the existing weakly supervised anomaly detection ap-\nproaches, each training iteration is carried out on one or\nmore complete videos [49,70]. Recently, CLAWS Net [64]\nproposed to extract several batches of temporally consistent\nfeatures, each of which was then randomly input to the net-\nwork. Such conﬁguration serves the purpose of minimizing\ncorrelation between consecutive batches. In these existing\napproaches, it is important to maintain temporal order at\nbatch or video level. However, in the proposed GCL ap-\npraoch we randomize the order of input features which re-\nmoves both the intra-batch and inter-batch correlations.\n3.2. Generative Cooperative Learning\nOur proposed Generative Cooperative Learning (GCL)\napproach for anomaly detection consists of a generator\nG which is an autoencoder (AE) and a discriminator D\nwhich is a fully connected (FC) classiﬁer.\nBoth these\nmodels are trained in a cooperative fashion without us-\ning any data annotations.\nMore speciﬁcally, we neither\nuse the normal class annotations as in one class classiﬁ-\ncation (OCC) approaches [11, 36, 52], nor binary annota-\ntions used by the weakly supervised anomaly detection sys-\ntems [49,64,65,70]. As discussed in Section 1, the intuition\nbehind using an AE is that such models can somewhat cap-\nture the overall dominant data trends [11]. On the other\nhand, the FC classiﬁcation network used as a discriminator\nis known to be efﬁcient when provided with supervised, al-\nbeit noisy, training [64]. In order to carry out the training,\nﬁrst pseudo annotations created using G are used to train D.\nIn the next step, pseudo annotations created by using D are\nused to improve G. Thus, each of the two models are trained\nby using the annotations created by the other model, in an\nalternate training fashion. The training conﬁguration aims\nthat the pseudo-labeling is improved over training iterations\nwhich consequently results in an improved overall anomaly\ndetection performance. Particular architecture details and\nseveral design choices are discussed next.\n3.2.1\nGenerator Network\nG takes features as input and produces reconstructions of\nthose features as output. Typically, G is trained by minimiz-\ning the reconstruction loss Lr as:\nLr = 1\nb\nb\nX\nq=1\nLq\nG, Lq\nG = ||f q\ni,j −bf q\ni,j||2,\n(1)\nwhere f q\ni,j is a feature vector that is input to G and bf q\ni,j is\nthe corresponding reconstructed vector, b is the batch size.\n3.2.2\nPseudo Labels from Generator\nIn our proposed collaborative learning, pseudo labels from\nG are created to train D. The labels are created by keep-\ning in view the distribution of the reconstruction loss Lq\nG of\neach instance q over a batch. The main idea is to consider\nfeature vectors resulting in higher loss values as anomalous\nand those generating smaller loss values as normal. In or-\nder to implement this intuition, one may consider using a\nthreshold Lth\nG as:\nlq\nG =\n(\n1,\nif Lq\nG ≥Lth\nG\n0,\notherwise .\n(2)\nWe have followed a simple approach for the Lth\nG selection\nby considering a ﬁxed percentage of the samples having\nmaximum reconstruction error as anomalous. In the Lq\nG\nhistograms we empirically observed a bigger peak towards\nminimum error and a smaller peak towards maximum error.\nDue to the fact that the class boundaries often fall in low\ndensity regions, error histograms are also an effective tool\nfor the selection of appropriate Lth\nG . Analysis of different\nalternates for Lth\nG selection is given in the Supplementary.\n3.2.3\nDiscriminator Network\nThe binary classiﬁcation network used as the discriminator\nD is trained using the pseudo annotations from G by mini-\nReconstruction\nTargets\nAnomalous Pseudo Label\nNormal Pseudo Label\nPseudo Reconstruction Target\nInput Batch\nPseudo-labels\nfrom Discriminator\nD\nG\nFigure 3. Negative learning in GCL: G is constrained to not learn\nthe reconstruction of anomalies using Pseudo Reconstruction Tar-\ngets (PRT). Based on the pseudo-labels produced by D, PRT are\ngenerated for the anomalous inputs while normal targets are used\nfor the normal inputs to guide the training of G.\nmizing the binary cross entropy loss over a batch b as:\nLD = −1\nb\nb\nX\nq=1\nlq\nG lnblq\ni,j + (1 −lq\nG) ln (1 −blq\ni,j),\n(3)\nwhere lq\nG ∈{0, 1} is the pseudo label generated by G and\nblq\ni,j is the output of D when a feature vector f q\ni,j is input.\n3.2.4\nPseudo Labels from Discriminator\nPseudo labels from D are used to improve the reconstruc-\ntion discrimination capability of G. The output bpq\ni,j of D\nis the probability of a feature vector f q\ni,j to be anomalous.\nTherefore, the features obtaining higher probability are con-\nsidered as anomalous by using a threshold mechanism on\nthe output bpq\ni,j of D . The annotations generated by D are\nthen used to ﬁne tune G in the next iteration.\nlq\nD =\n(\n1,\nif bpq\ni,j ≥Lth\nD\n0,\notherwise ,\n(4)\nwhere the threshold Lth\nD is computed the same way as the\nthreshold Lth\nG is computed.\n3.2.5\nNegative Learning of Generator Network\nTraining of G is carried out by using pseudo labels from D\nby employing negative learning (NL). In order to increase\nthe discrimination among reconstructions of normal and\nanomalous inputs, G is encouraged to poorly reconstruct the\nsamples which have anomalous pseudo labels whereas, the\nsamples having normal pseudo labels are aimed to be recon-\nstructed as usual with minimum error.\nSome variants of NL have already been explored in the\nliterature. For instance, Munawar et al. [32] and Astrid et\nal. [1] make the loss negative for a full batch of known\nanomalous inputs. However, this conﬁguration requires a\nprior knowledge of the whole dataset and its labels. In the\nproposed GCL approach, pseudo labels are generated iter-\natively as the training proceeds, therefore it may encounter\nboth normal and anomalous samples in the same batch. In\naddition, instead of making the loss negative, we enforce\nthe abnormal samples to be poorly reconstructed by using\na pseudo reconstruction target. Therefore, as illustrated in\nFig. 3, for each feature vector which is pseudo-labeled as\nanomalous by D, its reconstruction target is replaced by a\ndifferent feature vector. In order to extensively explore this\nconcept, we propose the following different types of pseudo\ntargets: 1) All Ones Target: The original reconstruction\ntarget is replaced by a similar dimensional vector of all 1’s.\n2) Random Normal Target: The original reconstruction\ntarget is replaced by a normal labeled feature vector selected\narbitrarily. 3) Random Gaussian Noise Target: The orig-\ninal reconstruction target is perturbed by adding Gaussian\nnoise. 4) No Negative Learning: No negative learning is\napplied to G. Instead only feature vectors pseudo-labeled as\nnormal are used for the training of G. Extensive analysis of\ndifferent pseudo targets is shown in Fig. 5. We empirically\nobserve that ‘ones’ as pseudo target yields more discrimi-\nnative reconstruction capability, thus better differentiating\nnormal and anomalous inputs. The loss function given by\nEq. (1) is modiﬁed to include negative learning:\nLG = 1\nb\nb\nX\nq=1\n||tq\ni,j −bf q\ni,j||2,\n(5)\nwhere the pseudo target tq is deﬁned as:\ntq\ni,j =\n(\nf q\ni,j,\nif lq\nD = 0\n1 ∈Rd,\nif lq\nD = 1,\n(6)\n3.3. Self-Supervised Pre-training\nThe proposed GCL approach is trained using unlabelled\nvideos by utilizing the cooperation of G and D.\nSince\nanomaly detection is an ill-deﬁned problem, the lack of con-\nstraints may adversely affect the convergence and the sys-\ntem may get stuck in local minima. In order to improve the\nconvergence, we explore to jump-start the training process\nby pre-training both G and D. We empirically observe that\nusing a pre-trained G (based on Eq. (1)) is beneﬁcial for the\noverall stability of the learning system and it also improves\nthe convergence as well as the performance of the system\n(see Section 4).\nAutoencoders are known to capture dominant represen-\ntations of the training data [11, 62]. Despite the fact that\nanomalies are sparse and normal features are abundant in\nthe training data, we experimentally observe that simply uti-\nlizing all training data to pre-train G may not provide an ef-\nfective jump-start. Using the fact that events in videos hap-\npen in temporal sequence and that anomalous frames are\nusually more eventful than the normal ones, we utilize tem-\nporal difference between the consecutive feature vectors as\nan estimator to initially clean the training dataset for the\npre-training of G. That is, a feature vector f t+1\ni,j\nwill only be\nused for pre-training if ||f t+1\ni,j\n−f t\ni,j||2 ≤Dth, where the\nsuperscripts t and t + 1 show the temporal order of features\nin a video and Dth is the threshold. This approach does\nnot guarantee complete removal of anomalous events how-\never, it cleans the data for an effective initialization of the G\nto give a jump-start to the training. Once G is pre-trained,\nit is used to generate psuedo labels which are then used to\npre-train the discriminator. In this step, the role of G is sim-\nilar to a lousy teacher because the generated pseudo-labels\nare quite noisy and the role of D is like an efﬁcient student\nbecause it learns to discriminate normal and anomalous fea-\ntures better even with noisy labels. In the following steps,\nboth pre-trained G and D are plugged into our collaborative\nlearning loop.\n3.4. Anomaly Scoring\nIn order to compute ﬁnal anomaly score at test time, sev-\neral conﬁgurations are possible, i.e., using reconstruction\nerror of G or prediction scores of D. We experimentally\nobserved that G remains relatively lousy while D remains\nefﬁcient across consecutive training iterations. Therefore\nfor simplicity, unless stated otherwise, all results reported\nin this work are computed using the predictions of D.\n4. Experiments\nIn this section, we ﬁrst provide important experimental de-\ntails, then draw comparisons with the existing state-of-the-\nart methods, and ﬁnally study different components of our\nGCL framework.\nDatasets. UCF-Crime (UCFC) dataset contains 13 different\ncategories of real-world anomalous events which were cap-\ntured by CCTV surveillance cameras spanning 128 hours\n[49]. This dataset is complex because of the unconstrained\nbackgrounds. The training split contains 810 abnormal and\n800 normal videos, while the testing split has 140 anoma-\nlous and 150 normal videos. In training split, video-level\nlabels are provided while in test split frame-level binary la-\nbels are provided. In our unsupervised setting, we discard\nthe training-split labels and train the proposed GCL using\nunlabelled training videos.\nShanghaiTech contains staged anomalous events cap-\ntured in a university campus at 13 different locations span-\nning 437 videos. This dataset was originally proposed for\nOCC with only normal videos provided for training. Later,\nZhong et al. [70] reorganized this dataset to facilitate train-\ning of weakly-supervised algorithms. Normal and anoma-\nlous videos were mixed in both the training and the testing\nsplits. The new training split contains 63 anomalous and\n0.1\n0.4\n0.7\n1\nNormal\nAnomalous\n0.1\n0.4\n0.7\n1\nNormal\nAnomalous\n460\n300\n150\n0\n(a) AEAllData\n(b) G from GCLB\n(c) D from GCLB\n0.1\n0.4\n0.7\n1\nNormal\nAnomalous\nFigure 4. Distribution of scores predicted on the test split of UCF-\ncrime dataset by (a) AE trained on all training data, (b) G trained\nin GCLB, and (c) D trained in GCLB. Although G and D are\ntrained cooperatively, D being more robust to noise, demonstrates\nsuperior discrimination between normal and anomalous examples.\n175 normal videos whereas, the new testing split contains\n44 anomalous and 155 normal videos. In order to train our\nproposed GCL, we follow the latter split both for training\nand testing, without using training split video labels.\nEvaluation Measures.\nFollowing the existing methods\n[13, 26, 49, 70], we use area under ROC curve (AUC) for\nevaluation and comparisons. AUC is computed based on\nframe-level annotations of the test videos in both datasets.\nImplementation Details. To demonstrate the concept of\ncooperative learning in its true essence, we select fairly\nsimple architectures, without any bells and whistles, as our\nG and D networks. Architectures of G and D are set as\nFC[2048, 1024, 512, 256, 512, 1024, 2048] and FC[2048,\n512, 32, 1]. We train both networks using RMSprop opti-\nmizer with a learning rate of 0.00002, momentum 0.60, for\n15 epochs on training data with batch size 8192. Thresholds\nfor pseudo-label generation are data driven. For G pseudo-\nlabels Lth\nG\n= µR + σR where µR and σR are the mean\nand the standard deviation of reconstruction error as given\nby Eq. (1) for each batch. For D, Lth\nD = µP + 0.1σP ,\nwhere µP and σP are the mean and standard deviations of\nthe probabilities bpq\ni,j generated by D for each batch. The\nvalue of Dth=0.70 is used in unsupervised pre-training. As\nfeature extractor, we use a popular framework ResNext3d\nproposed by Hara et al. [12] in default mode. Segment size\np for feature extraction is set to 16 non-overlapping frames.\nAll experiments are performed on NVIDIA RTX 2070 with\nIntel Core i7, 8th gen and 16GB RAM. Code will be re-\nleased upon acceptance.\n4.1. Comparisons with State-Of-The-Art (SOTA)\nThe proposed GCL approch is trained in an unsupervised\nfashion without using any video-level or frame-level anno-\ntations. GCL with no pre-training, GCLB, is considered as\nthe baseline. In addition, GCL with pre-training, GCLP T ,\nGCL combined with OCC based pre-trained autoencoder,\nGCLOCC, and GCL weakly-supervised, GCLW S are also\ntrained and evaluated on UCFC and ShanghaiTech datasets.\nAs seen in Table 1, on UCFC dataset, the proposed\nGCLB obtained an overall AUC of 68.17 % which is 11.85\n% higher than the Autoecnoder (AEAllData) trained on\nTable 1.\nPerformance comparison with existing state-of-the-\nart methods on UCF-Crime (UCFC) and ShanghaiTech (STech)\ndatasets. We divide the methods into three categories based on the\nsupervision used in training. Best results are in bold.\nSupervision\nType\nMethod\nFeatures\nUCFC\nAUC%\nSTech\nAUC%\nOne Class\nClassiﬁcation\nSVM [49]\n-\n50.00\n-\nHasan et al. [13]\n-\n50.60\n60.85\nSohrab et al. [47]\n-\n58.50\n-\nLu et al. [26]\n-\n65.51\n68.00\nBODS [52]\nI3D\n68.26\n-\nOGNet** [62]\nResNext\n69.47\n69.90\nGODS [52]\nI3D\n70.46\n-\nTSC [27]\n-\n-\n67.94\nFrame Prediction [24]\n-\n-\n73.40\nMemAE [10]\n-\n-\n71.20\nMNAD [36]\n-\n-\n70.50\nCho et al. [9]\n-\n-\n74.70\nLNTR [1]\n-\n-\n75.97\nRUVAD [55]\n-\n-\n76.67\nBMAN [21]\n-\n-\n76.20\nProposed GCLOCC\nResNext\n74.20\n79.62∗\nWeak\nSupervision\nSultani et al. [49]\nC3D\n75.41\n-\nZhang et al. [66]\nC3D\n78.66\n82.50\nZhu et al. [71]\nC3D\n79.00\n-\nNoise Cleaner [63]\nC3D\n78.27\n84.16\nSRF [65]\nC3D\n79.54\n84.16\nDUAD*** [22]\nC3D\n72.90\n-\nGCN [70]\nC3D\n81.08\n76.44\nGCN [70]\nTSNRGB\n82.12\n84.44\nWu et al. [57]\nI3D\n82.44\n-\nDAM [29]\nI3D\n82.67\n88.22\nCLAWS [64]\nC3D\n83.03\n89.67\nCLAWS** [64]\nResNext\n82.61\n-\nYu et al. [51]\nC3D\n83.28\n91.51\nYu et al. [51]\nI3D\n84.30\n97.27\nPurwantu et al. [39]\nTRN\n85.00\n96.85\nProposed GCLWS\nResNext\n79.84\n86.21\nUnsupervised\nkim et al. ** [20]\nResNext\n52.00\n56.47\nAEAllData\nResNext\n56.32\n62.73\nProposed GCLB\nResNext\n68.17\n72.41\nProposed GCLPT\nC3D\n70.74\n-\nProposed GCLPT\nResNext\n71.04\n78.93\n∗We follow the evaluation protocol of Zhong et al. [70].\n∗∗We implemented the models and computed these scores.\n*** [22] computes scores by taking average over videos.\ncomplete training data including both normal and anoma-\nlous training samples in an unsupervised fashion.\nHis-\ntogram plotted over reconstructions in Fig. 4(a) also pro-\nvides insights that AEAllData is not able to learn discrim-\ninative reconstruction. Also in the GCL, the discrimina-\ntion ability of D (Fig. 4(c)) is much enhanced than G (Fig.\n4(b)). Experiments on kim et al. [20] are conducted on a re-\nimplementation of the method for unlabelled training data.\nGCLP T is the version of proposed GCL with an au-\ntoencoder pre-trained in an unsupervised fashion. In this\nexperiment, an AUC performance of 71.04% is obtained\nwhich is 2.87% better than the baseline GCLB. The two\nmethods are also compared in Fig. 10 using multiple ran-\ndom seed initialization and GCLP T demonstrates consis-\ntent performance gains. Table 1 also shows that the pro-\n0\n25\n50\n75\n100\n125\n150\n175\n200\n225\n250\n275\n300\n325\n350\n48\n55\n62\n69\nOnes NL\nReplace NL\nWithout NL\nGaussian  = 6 NL\nσ = 6 NL\nGaussian  = 1.5 NL\nσ = 6 NL\nTraining Iterations\nAUC %\n(a) Training of G in GCLB\n0\n25\n50\n75\n100\n125\n150\n175\n200\n225\n250\n275\n300\n325\n350\n48\n55\n62\n69\nOnes NL\nReplace NL\nWithout NL\nGaussian  = 6 NL\nσ = 6 NL\nGaussian  = 1.5 NL\nσ = 6 NL\nTraining Iterations\nAUC %\n(b) Training of D in GCLB\nFigure 5. Convergence of G and D in GCL with/without Negative\nLearning (NL). We test different pseudo reconstruction targets in\nNL. Best performance is observed for ‘ones’ NL target.\nposed GCLP T outperforms all existing one-class classiﬁ-\ncation based anomaly detection methods. It is despite the\nfact that while training GCLP T , no labeled supervision is\nused. In contrast, OCC methods use clean normal class for\ntraining which provides extra information compared to our\nunsupervised training based GCL.\nIn another experiment, the autoencoder is pre-trained on\nonly the normal class of the training data, which makes\nthe setting comparable with the one-class classiﬁers. This\nscheme of extra information provided in the form of normal\nclass labels, referred as GCLOCC in Table 1, obtains an im-\nproved performance of 74.20% on UCFC which is signif-\nicantly better than all existing state-of-the-art OCC meth-\nods. It is interesting to note that GCLOCC yields compa-\nrable performance to the approach proposed by Sultani et\nal. [49] which utilizes video-level labels for training.\nAlthough GCL aims at unsupervised cooperative learn-\ning, we also extended it to incorporate weak-supervision.\nThe results for this version are reported as GCLW S in Table\n1. Despite using fairly simple networks of G and D without\nany bells and whistles, GCLW S obtains comparable results\nto several existing weakly-supervised learning methods.\nWe also evaluated our approach on ShanghaiTech\ndataset [28] and the results are compared with the existing\nSOTA methods in Table 1. On this dataset, our proposed\nGCLB obtained 72.41% AUC which is more than 10% bet-\nter than AEAllData showing the effectiveness of the baseline\napproach. GCLP T obtained 78.93% AUC which is 6.5%\nbetter than GCLB demonstrating the importance of unsu-\npervised pre-training for jump-start. Also, although unsu-\npervised, GCLP T outperformed all existing OCC methods.\nThe experiments on ShanghaiTech dataset also demonstrate\nthe effectiveness of the proposed GCLB and GCLP T al-\ngorithms for anomalous events detection using unlabelled\ntraining data.\n4.2. Ablation Study and Analysis\nAnalysis of different components, design choices, qualita-\ntive results and inclusion of supervision are discussed next.\nComponent-wise ablation study. A detailed ablation anal-\nTable 2. Ablation analysis of GCL Algorithm: performance of\ndifferent components with varying supervision levels.\nSupervision\nNegative\nlearning\nUnsup.\npre-training\nAUC%\nOCC\nUnsup.\nAEAllData\n-\n\u0013\n-\n-\n56.32\nAEOCC\n\u0013\n-\n-\n-\n65.76\nAET D\n-\n-\n-\n\u0013\n63.84\nGCLw/oNL\n-\n\u0013\n-\n-\n64.23\nGCLB\n-\n\u0013\n\u0013\n-\n68.17\nGCLP T\n-\n\u0013\n\u0013\n\u0013\n71.04\nGCLOCC\n\u0013\n-\n\u0013\n\u0013\n74.20\nysis of GCL framework with various design choices is re-\nported in Table 2 on the UCFC dataset.\nIt can be seen\nthat an autoencoder trained using all training dataset with-\nout any supervision AEAllData yields a signiﬁcantly low\nperformance of 56.32% compared to the one trained on\nclean normal data in OCC setting AEOCC yielding AUC\nof 65.76%. Training an autoencoder AET D with our pro-\nposed frame temporal difference based unsupervised pre-\nprocessing brings the performance closer to AEOCC, which\ndemonstrates the superiority of our proposed pre-processing\napproach. Using negative learning enhances the overall per-\nformance of GCLB over the counterpart training without\nnegative learning GCLw/oNL by 3.94%. Our complete un-\nsupervised system GCLP T which utilizes negative learning\nand unsupervised pre-training enhances the overall perfor-\nmance to 71.04%. In addition, in GCLOCC adding one-\nclass supervision improves this performance even further by\ndemonstrating an AUC of 74.20%. This also re-validates\nour claim of the overall beneﬁt that OCC may have over\na completely unsupervised setting, making them different\nfrom the unsupervised approaches.\nEvaluating negative learning approaches. Experiments\nare performed with and without Negative Learning (NL)\nwith GCL framework on UCFC dataset. For the case of NL,\nGCLB, the performances of different pseudo targets (dis-\ncussed in section 3.2.5) are also compared in Fig. 5. Three\ndifferent types of pseudo targets are compared: ‘ones’ for\n(a) AEAllData\n(b) AE in GCLw/oNL\n(c) AE in GCLB\nFigure 6. Feature reconstructions, using tSNE, with a) AEAllData,\nb) AE in GCLw/oNL without NL, and c) AE in GCLB with NL us-\ning ‘ones’ pseudo targets. Red and green points represent ground\ntruth anomalous and normal samples, respectively. Using negative\nlearning (NL), most of the anomalous samples get clustered sepa-\nrately from the normal samples, which is the underlying desidera-\ntum of providing pseudo reconstruction targets.\n0\n960\n1920\n2880\n0\n960\n1920\n2880\n0\n960\n1920\n2880\n0\n960\n1920\n2880\n0\n960\n1920\n2880\nAnomaly Scores\n0\n1\nEpoch 0\nEpoch 4\nEpoch 8\nEpoch 12\nEpoch 15\nFigure 7. Evolution of the frame-level anomaly scores in GCLB framework during training. Note that our unsupervised approach success-\nfully produces signiﬁcantly higher scores in the anomalous portions whereas lower scores in the normal portions. Anomaly ground truth\nis shown as red boxes, and the video is Explosion013 from UCF-Crime. Interestingly, the anomaly score stays higher after the anomalous\nground truth is over which is essentially due to aftermath of the explosion that network ﬁgures to be anomalous.\n0\n576\n1152\n0\n2400\n4800\n7200\n0\n1600\n3200\n0\n1600\n3200\n4800\nAnomaly Scores\n0\n1\n(a) Normal915\n(b) Assault006\n(c) Burglary024\n(d) Stealing058\nFigure 8. Anomaly scores by GCLP T are low in normal regions and high in abnormal regions on four different UCFC videos.\nall ones, ‘replace’ with random normal, and ‘gaussian’ with\nµ = 0 and σ = {1.5, 6.0}. Fig. 5 shows that the ‘ones’\npseudo target works better than its counterpart approaches.\nGaussian perturbations with σ = 1.5 demonstrate almost\nidentical trend to the model without any negative learning\nGCLw/oNL, and with σ = 6 the performance improves but\nstill lower than the ‘ones’ performance, GCLB. This could\nbe that the ﬁxed pseudo-target helps consistent learning of\nGCL framework resulting in better discrimination.\nTo further explore the signiﬁcance of NL, we provide\ntSNE visualizations of the reconstructions produced by\nAEAllData, GCLw/oNL AE without NL, and GCLB AE\nwith NL (trained using ‘Ones’ pseudo label) in Fig.\n6.\nAEAllData is trained using all training data without any la-\nbels. Both GCLB AEs with and without NL demonstrate a\nsuperior discrimination over AEAllData. Moreover, in AE\nwith NL (Fig. 6(c)), the anomalous features are forming a\ndistinct cluster which shows that the use of NL with pseudo\nreconstruction target is effective than using no NL option.\nQualitative analysis. A step by step evolution of our GCL\napproach is visualized in Fig. 7. As the training proceeds,\nGCLB learns to predict true anomalous portions within the\nvideo in a completely unsupervised fashion. Fig. 8 shows\n63.98\n68.02\n69.89\n72.17\n68.17\n74.81\n76.4\n79.84\n54\n60\n66\n72\n78\n84\n0\n33\n66\n100\nAUC %\nPercentage of Weak Supervision\nGenerator\nDiscriminator\nFigure 9. Performance evaluation of G and D in weakly supervised\nGCLW S by increasing supervision level from 0 to 100%.\nﬁnal anomaly scores predicted by our GCLP T on four dif-\nferent videos taken from UCFC dataset. In Fig. 8(d), some\nnormal portions are also predicted as anomalous. A visual\ninspection of this video reveals that the beginning and end-\ning frames contain ﬂoating text, which is unusual in the\ntraining data.\nOn convergence.\nWe (empirically) validate the conver-\ngence of both GCLB and GCLP T using multiple (10) ran-\ndom seed initialization in Fig. 10. GCLB and GCLP T ob-\ntain an average AUC of 67.09 ± 0.65 and 70.13 ± 0.52,\nrespectively. GCLP T not only improves the overall perfor-\nmance but also reduces the variation over different seeds,\nthereby demonstrating better convergence.\nOn adding weak-supervision. In a series of experiments\nusing UCFC, weak video-level labels are infused to the\nGCL ranging from 33% to 100%. Fig. 9 demonstrates that\nboth G and D beneﬁt from the added supervision. Notice-\nably, there is a signiﬁcant jump in AUC% upon only pro-\nviding 33% videos with weak labels which demonstrates\nthe fact that even minimal supervision is quite beneﬁcial for\nthe proposed GCL.\nOn training G using its own pseudo-labels. To further un-\nderstand proposed collaborative training, we also explore a\n39\n47\n55\n63\n71\n0\n275\nAUC %\nIterations\n39\n47\n55\n63\n71\n0\n175\nIterations\nGCLB\nGCLPT\nFigure 10. Convergence of both GCLB and GCLP T by initi-\nating training using several random seeds. GCLB and GCLP T\nobtain average AUC of 67.09 ± 0.65 and 70.13 ± 0.52 respec-\ntively. GCLP T not only improves the overall performance but\nalso reduces the variation over different seeds, thereby demonstrat-\ning better convergence.\npossibility of training G using its own pseudo-labels. We\nemploy negative learning to generate labels for training of\nG using the reconstruction error of G itself. Under this con-\nﬁguration, we observed a performance of 62.28% on UCF\ncrime dataset using ResNext3d features. It is better than\n56.32%, the performance of AEAllData (Table 1), however\nnoticeably lower than 71.04%, the performance of our pro-\nposed GCLP T . This demonstrates that the usage of D for\npseudo-labeling is critical due to its robust learning under\nnoisy labels [64, 65]. Since G creates noisy pseudo-labels,\nD being robust to noise effectively cleans these labels en-\nsuring the success of the overall collaborative learning.\nOn using soft labels. In our current conﬁguration, while\nusing pseudo-labels of G to train D, a threshold is applied\nto create binary labels from the reconstruction error (eq.\n(2)). However, it is also possible that we use soft labels in-\nstead of thresholding. Carrying out this experiment on UCF\ncrime dataset using ResNext3d features resulted in a AUC\nof 63.58%. Interestingly, the performance is almost iden-\ntical to that of AET D in Table 2. Intuitively, it is because\nwithout threshold, D simply starts replicating the output of\nG, thereby demonstrating identical performance.\nLimitations. The proposed unsupervised setting enables\nan anomaly detection system to start detecting abnormali-\nties just based on the observed data without any human in-\ntervention. In case there is no abnormal event so far, the\nsystem may consider the rare normal events as abnormal.\nHowever, if a system remains operational for a signiﬁcant\ntime, the probability of having no abnormal event will be\nrather very small.\n5. Conclusion\nWe proposed an unsupervised anomaly detection ap-\nproach (GCL) using unlabeled training videos, which can\nbe deployed without providing any manual annotations.\nGCL shows excellent performance on two public bench-\nmark datasets with varying supervision levels, including\nno-supervision, one class and weak-supervision. Finally,\nwe discussed the limitations of unsupervised settings, i.e.,\nthe assumption of having anomalies in the training dataset.\nHowever, this is more realistic than OCC methods as it is\nnatural to have anomalies in the real-world scenarios.\n6. Acknowledgements\nThis work was supported by the seed-type chal-\nlenge\nresearch\nproject\ngrant\nfunded\nby\nElectronics\nand Telecommunications Research Institute (ETRI) (No.\n21YS2700, Development of learning model and data gener-\nation/augmentation techniques for data efﬁcient deep learn-\ning, 50%) and also supported by ETRI with a grant funded\nby Ulsan Metropolitan City (22AS1600, the development\nof intelligentization technology for the main industry for\nmanufacturing innovation and Human-mobile-space au-\ntonomous collaboration intelligence technology develop-\nment in industrial sites, 50%)\nReferences\n[1] Marcella Astrid, Muhammad Zaigham Zaheer, Jae-Yeong\nLee, and Seung-Ik Lee. Learning not to reconstruct anoma-\nlies. arXiv preprint arXiv:2110.09742, 2021. 2, 4, 6\n[2] Arslan Basharat, Alexei Gritai, and Mubarak Shah. Learning\nobject motion patterns for anomaly detection and improved\nobject detection.\nIn 2008 IEEE Conference on Computer\nVision and Pattern Recognition, pages 1–8. IEEE, 2008. 2\n[3] Tanmay Batra and Devi Parikh. Cooperative learning with\nvisual attributes. arXiv preprint arXiv:1705.05512, 2017. 3\n[4] Paul Bergmann, Michael Fauser, David Sattlegger, and\nCarsten Steger.\nMvtec ad–a comprehensive real-world\ndataset for unsupervised anomaly detection. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 9592–9600, 2019. 2\n[5] Paul Bergmann, Michael Fauser, David Sattlegger, and\nCarsten Steger.\nUninformed students:\nStudent-teacher\nanomaly detection with discriminative latent embeddings. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), June 2020. 2\n[6] Raghavendra Chalapathy and Sanjay Chawla. Deep learn-\ning for anomaly detection:\nA survey.\narXiv preprint\narXiv:1901.03407, 2019. 2\n[7] Antoni Chan and Nuno Vasconcelos.\nUcsd pedestrian\ndataset. IEEE Trans. on Pattern Analysis and Machine In-\ntelligence (TPAMI), 30(5):909–926, 2008. 2\n[8] Varun Chandola, Arindam Banerjee, and Vipin Kumar.\nAnomaly detection: A survey.\nACM computing surveys\n(CSUR), 41(3):1–58, 2009. 1\n[9] MyeongAh Cho, Taeoh Kim, Ig-Jae Kim, and Sangyoun\nLee.\nUnsupervised video anomaly detection via normal-\nizing ﬂows with implicit latent features.\narXiv preprint\narXiv:2010.07524, 2020. 6\n[10] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha,\nMoussa Reda Mansour, Svetha Venkatesh, and Anton\nvan den Hengel. Memorizing normality to detect anomaly:\nMemory-augmented deep autoencoder for unsupervised\nanomaly detection. In Proceedings of the IEEE International\nConference on Computer Vision, pages 1705–1714, 2019. 1,\n2, 6\n[11] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha,\nMoussa Reda Mansour, Svetha Venkatesh, and Anton\nvan den Hengel. Memorizing normality to detect anomaly:\nMemory-augmented deep autoencoder for unsupervised\nanomaly detection. In The IEEE International Conference\non Computer Vision (ICCV), October 2019. 2, 3, 5\n[12] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can\nspatiotemporal 3d cnns retrace the history of 2d cnns and\nimagenet? arXiv preprint, arXiv:1711.09577, 2017. 6\n[13] Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K\nRoy-Chowdhury, and Larry S Davis. Learning temporal reg-\nularity in video sequences. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n733–742, 2016. 1, 6\n[14] Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-\nYan Liu, and Wei-Ying Ma. Dual learning for machine trans-\nlation. Advances in neural information processing systems,\n29:820–828, 2016. 3\n[15] Matth¨aus Heer,\nJanis Postels,\nXiaoran Chen,\nEnder\nKonukoglu, and Shadi Albarqouni. The ood blind spot of\nunsupervised anomaly detection. In Medical Imaging with\nDeep Learning, 2021. 2\n[16] Ryota Hinami, Tao Mei, and Shin’ichi Satoh. Joint detection\nand recounting of abnormal events by learning deep generic\nknowledge. In Proceedings of the IEEE International Con-\nference on Computer Vision, pages 3619–3627, 2017. 1\n[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistill-\ning the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015. 2\n[18] John Taylor Jewell, Vahid Reza Khazaie, and Yalda Mohsen-\nzadeh. Oled: One-class learned encoder-decoder network\nwith adversarial context masking for novelty detection. arXiv\npreprint arXiv:2103.14953, 2021. 2\n[19] Shunsuke Kamijo, Yasuyuki Matsushita, Katsushi Ikeuchi,\nand Masao Sakauchi. Trafﬁc monitoring and accident detec-\ntion at intersections. IEEE transactions on Intelligent trans-\nportation systems, 1(2):108–118, 2000. 2\n[20] Jin-Hwa Kim, Do-Hyeong Kim, Saehoon Yi, and Taehoon\nLee. Semi-orthogonal embedding for efﬁcient unsupervised\nanomaly segmentation.\narXiv preprint arXiv:2105.14737,\n2021. 6\n[21] Sangmin Lee, Hak Gu Kim, and Yong Man Ro.\nBman:\nbidirectional multi-scale aggregation networks for abnormal\nevent detection. IEEE Transactions on Image Processing,\n29:2395–2408, 2019. 6\n[22] Tangqing Li, Zheng Wang, Siying Liu, and Wen-Yan Lin.\nDeep unsupervised anomaly detection. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 3636–3645, 2021. 6\n[23] Daochang Liu, Tingting Jiang, and Yizhou Wang. Complete-\nness modeling and context separation for weakly supervised\ntemporal action localization.\nIn Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 1298–1307, 2019. 1\n[24] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Fu-\nture frame prediction for anomaly detection–a new baseline.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 6536–6545, 2018. 1, 6\n[25] Ziyi Liu, Le Wang, Qilin Zhang, Zhanning Gao, Zhenxing\nNiu, Nanning Zheng, and Gang Hua.\nWeakly supervised\ntemporal action localization through contrast based evalu-\nation networks. In Proceedings of the IEEE International\nConference on Computer Vision, pages 3899–3908, 2019. 1\n[26] Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event detec-\ntion at 150 fps in matlab. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 2720–2727,\n2013. 6\n[27] Weixin Luo, Wen Liu, and Shenghua Gao.\nA revisit of\nsparse coding based anomaly detection in stacked rnn frame-\nwork. In Proceedings of the IEEE International Conference\non Computer Vision, pages 341–349, 2017. 1, 6\n[28] Weixin Luo, Wen Liu, and Shenghua Gao.\nA revisit of\nsparse coding based anomaly detection in stacked rnn frame-\nwork. In Proceedings of the IEEE International Conference\non Computer Vision, pages 341–349, 2017. 2, 7\n[29] Snehashis Majhi, Srijan Das, and Franc¸ois Br´emond. Dam:\nDissimilarity attention module for weakly-supervised video\nanomaly detection. 6\n[30] G´erard Medioni, Isaac Cohen, Franc¸ois Br´emond, Somboon\nHongeng, and Ramakant Nevatia. Event detection and anal-\nysis from video streams. IEEE Transactions on pattern anal-\nysis and machine intelligence, 23(8):873–889, 2001. 2\n[31] Sadegh Mohammadi, Alessandro Perina, Hamed Kiani, and\nVittorio Murino. Angry crowds: Detecting violent events in\nvideos. In European Conference on Computer Vision, pages\n3–18. Springer, 2016. 2\n[32] Asim Munawar, Phongtharin Vinayavekhin, and Giovanni\nDe Magistris. Limiting the reconstruction capability of gen-\nerative neural network using negative learning. In 2017 IEEE\n27th International Workshop on Machine Learning for Sig-\nnal Processing (MLSP), pages 1–6. IEEE, 2017. 4\n[33] Sanath Narayan, Hisham Cholakkal, Fahad Shahbaz Khan,\nand Ling Shao. 3c-net: Category count and center loss for\nweakly-supervised action localization. In Proceedings of the\nIEEE International Conference on Computer Vision, pages\n8679–8687, 2019. 1\n[34] Trong-Nguyen Nguyen and Jean Meunier. Anomaly detec-\ntion in video sequence with appearance-motion correspon-\ndence. In The IEEE International Conference on Computer\nVision (ICCV), October 2019. 2\n[35] Trong Nguyen Nguyen and Jean Meunier.\nHybrid\ndeep network for anomaly detection.\narXiv preprint\narXiv:1908.06347, 2019. 2\n[36] Hyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning\nmemory-guided normality for anomaly detection. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 14372–14381, 2020. 1, 3, 6\n[37] Claudio Piciarelli, Christian Micheloni, and Gian Luca\nForesti. Trajectory-based anomalous event detection. IEEE\nTransactions on Circuits and Systems for video Technology,\n18(11):1544–1554, 2008. 2\n[38] Janis Postels, Hermann Blum, Yannick Str¨umpler, Cesar\nCadena, Roland Siegwart, Luc Van Gool, and Federico\nTombari. The hidden uncertainty in a neural networks ac-\ntivations. arXiv preprint arXiv:2012.03082, 2020. 2\n[39] Didik Purwanto, Yie-Tarng Chen, and Wen-Hsien Fang.\nDance with self-attention: A new look of conditional ran-\ndom ﬁelds on anomaly detection in videos. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 173–183, October 2021. 2, 6\n[40] Mahdyar Ravanbakhsh, Moin Nabi, Hossein Mousavi, Enver\nSangineto, and Nicu Sebe. Plug-and-play cnn for crowd mo-\ntion analysis: An application in abnormal event detection. In\n2018 IEEE Winter Conference on Applications of Computer\nVision (WACV), pages 1689–1698. IEEE, 2018. 1\n[41] Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lu-\ncio Marcenaro, Carlo Regazzoni, and Nicu Sebe. Abnormal\nevent detection in videos using generative adversarial nets.\nIn 2017 IEEE International Conference on Image Process-\ning (ICIP), pages 1577–1581. IEEE, 2017. 1, 2\n[42] Huamin Ren, Weifeng Liu, Søren Ingvor Olsen, Sergio Es-\ncalera, and Thomas B Moeslund. Unsupervised behavior-\nspeciﬁc dictionary learning for abnormal event detection. In\nBMVC, pages 28–1, 2015. 2\n[43] Mohammad Sabokrou, Mahmood Fathy, Guoying Zhao, and\nEhsan Adeli. Deep end-to-end one-class classiﬁer. IEEE\ntransactions on neural networks and learning systems, 2020.\n2\n[44] Mohammad Sabokrou, Mohsen Fayyaz, Mahmood Fathy,\nand Reinhard Klette. Deep-cascade: Cascading 3d deep neu-\nral networks for fast anomaly detection and localization in\ncrowded scenes. IEEE Transactions on Image Processing,\n26(4):1992–2004, 2017. 1, 2\n[45] Zheng Shou, Hang Gao, Lei Zhang, Kazuyuki Miyazawa,\nand Shih-Fu Chang.\nAutoloc: Weakly-supervised tempo-\nral action localization in untrimmed videos. In Proceedings\nof the European Conference on Computer Vision (ECCV),\npages 154–171, 2018. 1\n[46] Sorina Smeureanu, Radu Tudor Ionescu, Marius Popescu,\nand Bogdan Alexe. Deep appearance features for abnormal\nbehavior detection in video. In International Conference on\nImage Analysis and Processing, pages 779–789. Springer,\n2017. 1, 2\n[47] Fahad Sohrab, Jenni Raitoharju, Moncef Gabbouj, and\nAlexandros Iosiﬁdis. Subspace support vector data descrip-\ntion.\nIn 2018 24th International Conference on Pattern\nRecognition (ICPR), pages 722–727. IEEE, 2018. 6\n[48] Jessie James P Suarez and Prospero C Naval Jr. A survey on\ndeep learning techniques for video anomaly detection. arXiv\npreprint arXiv:2009.14146, 2020. 2\n[49] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world\nanomaly detection in surveillance videos.\nIn Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6479–6488, 2018. 1, 2, 3, 5, 6, 7\n[50] Waqas Sultani and Jin Young Choi. Abnormal trafﬁc detec-\ntion using intelligent driver model. In 2010 20th Interna-\ntional Conference on Pattern Recognition, pages 324–327.\nIEEE, 2010. 2\n[51] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh,\nJohan W Verjans, and Gustavo Carneiro. Weakly-supervised\nvideo anomaly detection with robust temporal feature mag-\nnitude learning. arXiv preprint arXiv:2101.10030, 2021. 2,\n3, 6\n[52] Jue Wang and Anoop Cherian. Gods: Generalized one-class\ndiscriminative subspaces for anomaly detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 8201–8211, 2019. 3, 6\n[53] Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg,\nJingbin Wang, James Philbin, Bo Chen, and Ying Wu. Learn-\ning ﬁne-grained image similarity with deep ranking. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 1386–1393, 2014. 2\n[54] Limin Wang, Yuanjun Xiong, Dahua Lin, and Luc Van Gool.\nUntrimmednets for weakly supervised action recognition\nand detection.\nIn Proceedings of the IEEE conference\non Computer Vision and Pattern Recognition, pages 4325–\n4334, 2017. 1\n[55] Xuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao, Ke\nYang, Jian Tang, Jieping Ye, Jingyu Wang, and Qi Qi. Robust\nunsupervised video anomaly detection by multipath frame\nprediction.\nIEEE Transactions on Neural Networks and\nLearning Systems, 2021. 6\n[56] Qi Wei, Yinhao Ren, Rui Hou, Bibo Shi, Joseph Y Lo,\nand Lawrence Carin.\nAnomaly detection for medical im-\nages based on a one-class classiﬁcation. In Medical Imag-\ning 2018: Computer-Aided Diagnosis, volume 10575, page\n105751M. International Society for Optics and Photonics,\n2018. 2\n[57] Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao,\nZhaoyang Wu, and Zhiwei Yang. Not only look, but also\nlisten: Learning multimodal violence detection under weak\nsupervision. In European Conference on Computer Vision,\npages 322–339. Springer, 2020. 6\n[58] Yan Xia, Xudong Cao, Fang Wen, Gang Hua, and Jian\nSun. Learning discriminative reconstructions for unsuper-\nvised outlier removal.\nIn Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 1511–1519,\n2015. 1\n[59] Dan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, and Nicu\nSebe.\nLearning deep representations of appearance and\nmotion for anomalous event detection.\narXiv preprint\narXiv:1510.01553, 2015. 2\n[60] Dan Xu, Yan Yan, Elisa Ricci, and Nicu Sebe. Detecting\nanomalous events in videos by learning deep representations\nof appearance and motion. Computer Vision and Image Un-\nderstanding, 156:117–127, 2017. 2\n[61] Tan Yu, Zhou Ren, Yuncheng Li, Enxu Yan, Ning Xu, and\nJunsong Yuan. Temporal structure mining for weakly super-\nvised action detection. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 5522–5531,\n2019. 1\n[62] Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid,\nand Seung-Ik Lee. Old is gold: Redeﬁning the adversarially\nlearned one-class classiﬁer training paradigm. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 14183–14193, 2020. 1, 2, 5, 6\n[63] Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid,\nArif Mahmood, and Seung-Ik Lee.\nCleaning label noise\nwith clusters for minimally supervised anomaly detection.\nIn Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPRW), 2020. 1, 2, 3, 6\n[64] Muhammad Zaigham Zaheer, Arif Mahmood, Marcella\nAstrid, and Seung-Ik Lee. Claws: Clustering assisted weakly\nsupervised learning with normalcy suppression for anoma-\nlous event detection. In European Conference on Computer\nVision, pages 358–376. Springer, 2020. 1, 2, 3, 4, 6, 9\n[65] Muhammad Zaigham Zaheer, Arif Mahmood, Hochul Shin,\nand Seung-Ik Lee. A self-reasoning framework for anomaly\ndetection using video-level labels. IEEE Signal Processing\nLetters, 27:1705–1709, 2020. 1, 2, 3, 6, 9\n[66] Jiangong Zhang, Laiyun Qing, and Jun Miao. Temporal con-\nvolutional network with complementary inner bag loss for\nweakly supervised anomaly detection.\nIn 2019 IEEE In-\nternational Conference on Image Processing (ICIP), pages\n4030–4034. IEEE, 2019. 6\n[67] Tianzhu Zhang, Hanqing Lu, and Stan Z Li. Learning se-\nmantic scene models by object classiﬁcation and trajectory\nclustering. In 2009 IEEE Conference on Computer Vision\nand Pattern Recognition, pages 1940–1947. IEEE, 2009. 2\n[68] Ying Zhang, Huchuan Lu, Lihe Zhang, Xiang Ruan, and\nShun Sakai.\nVideo anomaly detection based on locality\nsensitive hashing ﬁlters. Pattern Recognition, 59:302–311,\n2016. 1\n[69] Ying Zhang, Tao Xiang, Timothy M Hospedales, and\nHuchuan Lu. Deep mutual learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 4320–4328, 2018. 2\n[70] Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu,\nThomas H Li, and Ge Li. Graph convolutional label noise\ncleaner: Train a plug-and-play action classiﬁer for anomaly\ndetection. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 1237–1246,\n2019. 1, 2, 3, 5, 6\n[71] Yi Zhu and Shawn Newsam.\nMotion-aware feature\nfor improved video anomaly detection.\narXiv preprint\narXiv:1907.10211, 2019. 6\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-03-08",
  "updated": "2022-03-08"
}