{
  "id": "http://arxiv.org/abs/2203.06347v1",
  "title": "Combining Deep Learning with Physics Based Features in Explosion-Earthquake Discrimination",
  "authors": [
    "Qingkai Kong",
    "Ruijia Wang",
    "William R. Walter",
    "Moira Pyle",
    "Keith Koper",
    "Brandon Schmandt"
  ],
  "abstract": "This paper combines the power of deep-learning with the generalizability of\nphysics-based features, to present an advanced method for seismic\ndiscrimination between earthquakes and explosions. The proposed method contains\ntwo branches: a deep learning branch operating directly on seismic waveforms or\nspectrograms, and a second branch operating on physics-based parametric\nfeatures. These features are high-frequency P/S amplitude ratios and the\ndifference between local magnitude (ML) and coda duration magnitude (MC). The\ncombination achieves better generalization performance when applied to new\nregions than models that are developed solely with deep learning. We also\nexamined which parts of the waveform data dominate deep learning decisions\n(i.e., via Grad-CAM). Such visualization provides a window into the black-box\nnature of the machine-learning models and offers new insight into how the deep\nlearning derived models use data to make the decisions.",
  "text": " \n \n1 \nCombining Deep Learning with Physics Based Features in Explosion-Earthquake \nDiscrimination \n \nQingkai Kong1, Ruijia Wang2,3, William R. Walter1, Moira Pyle1, Keith Koper4, Brandon \nSchmandt2 \n \n \n \n \n1Lawrence Livermore National Laboratory \n2University of New Mexico \n3Southern University of Science and Technology \n4University of Utah \n \nCorresponding author: Qingkai Kong (kong11@llnl.gov)  \n \n \n \n \nKey Points: \n• Discrimination of earthquakes and explosions can be enhanced by combining physics-\nbased features with those derived from machine learning \n• Visualizing which parts of the input the deep learning model relies on can provide more \ninsight into the processes underlying decisions \n• The deep learning model focuses on different frequency bands for P and S waves to make \nthe decision \n \n \n \n \n2 \nAbstract \nThis paper combines the power of deep-learning with the generalizability of physics-based \nfeatures, to present an advanced method for seismic discrimination between earthquakes and \nexplosions. The proposed method contains two branches: a deep learning branch operating directly \non seismic waveforms or spectrograms, and a second branch operating on physics-based \nparametric features. These features are high-frequency P/S amplitude ratios and the difference \nbetween local magnitude (ML) and coda duration magnitude (MC). The combination achieves \nbetter generalization performance when applied to new regions than models that are developed \nsolely with deep learning. We also examined which parts of the waveform data dominate deep \nlearning decisions (i.e., via Grad-CAM). Such visualization provides a window into the black-box \nnature of the machine-learning models and offers new insight into how the deep learning derived \nmodels use data to make the decisions.  \n \nPlain Language Summary \nThis paper presents a new method to distinguish earthquakes from explosions using seismic data. \nThe method combines features implicitly defined by a deep learning algorithm with features \nexplicitly defined from physical models of seismic sources and elastic wave propagation. The \ncombination of these two types of features makes our method perform better on new data sets. By \nvisualizing the performance of our combined model, we gain insight into what the deep learning \nmodel relies on to make the decisions.  \n \n1 Introduction \nFrom creating catalogs of tectonic-only events for seismic hazard, to monitoring for nuclear \nexplosions, discrimination between explosions and earthquakes remains an important task in \nseismology. Event characteristics such as focal depth, first motion polarity, efficiency of \ngenerating shear waves, have been found useful in distinguishing explosions from natural \nearthquakes for moderate size seismic events with magnitude larger than ~M3.5 (Bowers & Selby, \n2009; National Research Council, 2012). More recently, with an interest in lowering monitoring \nthresholds, focus has turned to identifying smaller events that are well-recorded only at local \ndistances (less than 250 km). For example, O’Rourke et al., (2016); Pyle & Walter, (2019, 2021) \nand Wang et al. (2020) showed that high-frequency P/S amplitude ratios can potentially be used \nfor small-magnitude seismic discrimination by averaging over many stations at local distances. \nFurthermore, a recently proposed depth discriminant - the difference between local magnitude \n(ML) and coda magnitude (MC) - also shows the ability to separate explosions from deeper, \nnaturally occurring earthquakes (Koper et al., 2021; Voyles et al., 2020). These physics-based \ndiscriminants provide a good understanding of the different characteristics between the two types \nof sources, and generally work very well in different regions and studies. \n \nOn the other hand, recent successful applications of machine learning to various areas in \nseismology (Bergen et al., 2019; Karpatne et al., 2019; Kong et al., 2019) suggest that a data-\ndriven approach might be suitable for source classification problems. There are studies using \nmachine learning models with manually selected features for source type discrimination (Dowla \n \n \n3 \net al., 1990; Kong et al., 2016; Mousavi et al., 2016; Orlic & Loncaric, 2010; Rabin et al., 2016; \nTsvang et al., 1993). These studies are often based on scientist-chosen features and use a machine \nlearning algorithm to find the best classification boundary that best separates the source types. For \nexample, Dowla et al. (1990) obtained a 97% rate of correct discrimination using features extracted \nat different frequencies from Pn, Pg, and Lg spectra with an artificial neural network to separate \nearthquakes from historic nuclear explosions at the former Nevada Test Site recorded on \nbroadband seismic stations operated by Lawrence Livermore National Laboratory. These \napproaches usually work well if the features selected are representative of the different event \nsources, whereas features that are beyond scientists’ awareness may also be missed. There have \nalso been several studies using recently developed deep learning based approaches to distinguish \nexplosions from natural earthquakes (Kim et al., 2020; Kong et al., 2021; Linville et al., 2019; \nMagana-Zook & Ruppert, 2017; Tibi et al., 2019). Linville et al. (2019) used convolutional and \nrecurrent neural networks with spectrograms from seismic sensors as the input to classify \nexplosions and tectonic sources at local distances, with a result of 99% accuracy in terms of the \nsource type discrimination. Although deep learning methods have the advantage of extracting \nuseful patterns from explosion and earthquake waveforms automatically without knowing any of \nthe physics, they have the limitation that the learned features may have no clear physical meaning, \nand therefore may not generalize well in new regions. Even more problematic, they may focus on \nfeatures such as the event location or timing, as opposed to a truly discriminating feature of the \nwaveform. \n \nIn this paper, we propose to combine deep learning and physics-based features in one model for \nsingle-station explosion discrimination. By incorporating the physics-based features, the combined \ndeep learning model improves on the performance compared to the deep learning model alone, \nespecially when it is applied in a new region, i.e., improved transportability of the model. \nFurthermore, to uncover some of the black-box nature of the machine learning model, we also use \nmodel visualization methods to understand what the model learns to make the decision in \nidentifying source type. \n \n2 Data \nFour datasets with local seismic observations of single-fired underground chemical explosions and \nearthquakes are used in this study. We use only valid P/S ratio measurements with values between \n0 to 100 and recorded at stations within 250 km, limiting data to local distances. These studies are \ndescribed in detail in the following cited original references, so only overview information is given \nhere (station and shots distribution maps for the four regions are given in Figure S1).  \n \nThe Source Physics Experiment (SPE) Phase I, conducted between 2011 and 2016, consisted of a \nseries of underground chemical high-explosive detonations in saturated granite of various sizes \nand depths at the Nevada National Security Site. Phase I consisted of five ML 1.2 – 2.1 borehole \nshots (Snelson et al., 2013) and 110 earthquakes occurred with ML 1.0 – 4.4. In total, we assembled \n149 explosion 3-component records and 2,216 earthquake records.  \n \n \n \n4 \nThe Bighorn Arch Seismic Experiment (BASE) (Worthington et al., 2016; Yeck et al., 2014) was \nconducted in 2010 to image the Bighorn Arch in Wyoming. 21 explosive sources (ML 0.7 – 1.7) \nand 19 earthquakes (ML 0.3 – 2.7) were collected in the dataset, translating to a total of 4,394 \nexplosion and 3,297 earthquake 3-component records.  \n \nThe Mount St. Helens magma imaging project (MSH) contains 23 explosive sources (ML 0.9 – \n2.3) and 91 earthquakes (ML 1.5 – 3.3) located within 75 km of Mt. St. Helens during the 2014 – \n2016 iMUSH project (Kiser et al., 2016; Ulberg et al., 2020; Wang et al., 2020). In total, this \ndataset contains 1,652 explosion and 26,751 earthquake 3-component records.  \n \nThe Salton Seismic Imaging Project (SSIP) is an active source seismic survey conducted in 2011 \nto image crustal faults and constrain rifting processes beneath the Salton Sea (Fuis et al., 2017; \nHan et al., 2016), during which 41 shots (ML 0.6 – 2.1) were conducted. The USGS reported 76 \nevents in this region (ML 1 – 3.6) during the same time of the survey, including 6 borehole shots \nmis-labeled as earthquakes, as detailed in Wang et al. (2021). In total, the SSIP dataset has 2,307 \nexplosion and 4,047 earthquake 3-component records.  \n \nWe conduct a number of pre-processing steps to unify the waveforms from different regions. We \nfirst remove the mean and trend from each of the waveforms and apply a taper with a Hanning \nwindow followed by a four-corner bandpass filter from 1 – 20 Hz. We then resample the filtered \nwaveforms to 40 Hz. In the last step, each waveform is cut to 2,000 data points (50 s) with a \nrandom start window (0-5 s) before the origin time. For each earthquake record, we randomly \nselect the start time five times, while for each of the explosion records, 21 randomly selected start \ntimes are sampled. This data augmentation technique leads to a dataset of 173,385 earthquake and \n178,059 explosion records that are roughly comparable for training and testing purposes. The raw \ndata and augmented data distance, magnitude, depth and P/S ratio distributions are shown in \nFigures S2 and S3.  \n \n3 Methods \n3.1 Two-branch model \n3.1.1 Model structures \nThe proposed deep learning model contains two branches (Figure 1) to take advantage of both \ndeep learning and physics-based parameters. It is a single-station-based binary classification \nproblem (i.e., earthquake versus explosion). As shown, the blue dotted box contains the deep \nlearning branch, where we use a convolutional neural network (Goodfellow et al., 2016; LeCun et \nal., 2015) to extract the features automatically from the input 3-component waveforms. The \nfeatures extracted through the CNN (Convolutional Neural Network) layers are flattened into 128 \nfeatures by the FC (Fully Connected) layer. The Physics Parameters Branch measures physics-\nbased parameters that can be fed into this network; in our case, we use the P/S ratio and/or ML – \nMC. Features are extracted from these physics parameters by a FC layer and thus combined with \nthose from the deep-learning branch by a Feature Concatenation layer. The concatenated features \n \n \n5 \nare then passed through another FC layer before the model makes the decision. Dropout layers that \nhave been added after the CNN layers (dropout rate=0.3) and the Feature Concatenation layer \n(dropout rate=0.5) serve as regularization to reduce overfitting. ReLU (Rectified Linear Unit) \nactivation functions are used across the network, except for the last layer, where two neurons with \nsoftmax activation functions (Goodfellow et al., 2016) are used to estimate the probability of the \nwaveforms being earthquake or explosions. More details of the structure are shown in Figure S4.  \n \nIn order to evaluate the effect of adding the physics parameters branch, we compare the \nperformance of the two-branch model with that from a single-branch model. Two single-branch \nmodels are used: (1) the deep learning (CNN) model, which is the blue dotted box, and (2) the \nphysics parameter branch model, which is the green dotted box shown in Figure 1. \n \n3.1.2 Training and testing \nDuring all trainings, we use the SparseCategoricalCrossentropy (Goodfellow et al., 2016) as our \nloss function and Adam as the optimization method (Kingma & Ba, 2017). The learning rate is \ninitialized as 0.001. We also adopt an early stopping mechanism to avoid overfitting, with a \nmaximum of 1,000 training epochs. If the validation accuracy does not change for 30 epochs, we \nstop the training and select the last highest accuracy model for the best trained model.  \n \nWe use ROC (Receiver Operating Characteristic) curves on the testing data to quantify model \nperformance, which jointly consider the true positive and false positive rates (Fawcett, 2006). In \nour case, the explosions are positive cases and earthquakes are negative cases. The Area Under the \nCurve (AUC) is used to measure the quality of the ROC curve, with a value of 1 being the best. \nTo determine how adding the physics parameter branch helps, we adopt two data splitting methods. \nThe first method randomly divides the whole dataset into training and testing, which is the common \napproach that has been used in many studies. The second method reserves one region for testing \nand uses the other three study regions for training, as four datasets are involved in this study. The \nlatter method allows for better evaluation of model transportability with or without the physics \nparameter branch. \n \n3.1.3 P/S ratio and ML-MC \nP/S ratios are calculated from 10 – 18 Hz filtered three-component waveforms using the \ncorresponding phases shown in equation (1) in Wang et al. (2021). SNRs (Signal-to-Noise Ratio) \nfor the three component waveforms are also computed using the predicted P windows and noise \nwindows (i.e., 10 s before the P arrival) for quality control purposes, with a cutoff threshold of \nSNR > 2 to declare a valid P/S ratio of a given event-station pair. More details about the parameters \nand regional velocity models used are described in (Wang et al., 2021). \n \nTo demonstrate that more physics-based features can be added, the “Adding ML-MC” section in \nthe supplementary material provides design and testing for using the P/S ratio and the depth \n \n \n6 \ndiscriminant ML-MC to improve the results slightly. But due to the limited number of ML-MC \nmeasurements from the quality control, we will focus on using P/S ratio in the following sections.   \n \n3.2 Understanding what the model learns \nFor the deep learning branch, the Grad-CAM (Gradient-weighted Class Activation Mapping) is \napplied to understand what features are important to the deep learning branch to make the decision \n(Selvaraju et al., 2017). The basic idea behind this method is the last convolutional layer before \nflattening extracts the feature maps that contain the important features that the model relies on to \nmake the decision. By taking the gradients of the final class score with respect to the feature maps, \nthey provide a good indication of the pixels that are important to the final decision. Some examples \nof the Grad-CAM outputs are shown in the Supplement, where we overlap the derived heatmap on \nthe input waveforms (Figure S8 – S10). In order to understand the role of different frequency \nbands, a deep learning model with spectrograms as inputs is added into the deep learning branch. \nThe model structure is shown in Figure S5 (performance is shown in Figure S6), with Grad-CAM \nexamples shown in Figure S11 – S13. These heatmaps illustrate the importance of the different \nregions on the input image (time series or spectrogram) in influencing the final output to the target \nclass.   \n \nFor P/S ratios in the physics parameters branch, we simply use the error rates (percentage of the \nwrongly estimated waveforms divided by the total waveforms) versus the P/S ratio value bins to \nshow what the model learns.  \n \n4 Results \n \n4.1 Model performance \nWhen the testing data are from the same regions as the training data, using a single deep learning \nbranch model (WF) has very similar results compared to the two-branch models (WF + PS) that \nincluded the P/S ratio branch (Figure 2a): both reach an AUC of 0.99. In contrast, using the P/S \nratio alone (PS) leads to AUC=0.872. Such a difference clearly shows the power of deep learning \nversus a single parameter-based classifier. However, the model performance is significantly \ndecreased when apply to data from a different region. In Figure 2b, where the training and testing \ndata come from different regions, both WF + PS and the WF model performance degrade \nsignificantly on the new data, while the PS only model has a small degradation in performance. \nDue to the addition of the P/S ratio, the WF + PS model performs better than either single-branch \nmodel, showing the benefit of combining the physics-based features and the deep learning \nextracted features.  \n \nWe validate the above observations and ensure they are generic through four sets of tests, where \nall sets of the three regions are used for training and the remaining one is used for testing (Figure \n2c – 2f). The two-branch models achieve the best performance across different regions. Comparing \nthe WF + PS to the WF models, the gaps show the contributions from the physics-based features, \n \n \n7 \ni.e., P/S ratio. Even though the deep learning models have the reputation of automatically \nextracting useful features, the physics-based features still can provide extra information that are \nnot fully captured by deep learning, especially for data from new regions that are not in the training \ndata.  \n \n4.2 Understanding what the model learned \nGrad-CAM is used for understanding the deep learning branch. The calculated Grad-CAM weights \nare normalized to the range of 0 – 1, with higher values indicating greater importance. In order to \nobtain further understanding, we aggregate the weights by grouping them into 20 km distance bins \nand taking the average of the Grad-CAM weights (Figure 3).   \n \nFor the earthquake records (Figure 3 row a), the P and S waves, including the time between the \ntwo arrivals, exhibit higher weights than other parts of the waveform. Interestingly, the importance \nof the weights drops significantly right after the S wave and then slowly increases again for some \nof the later parts of the waveforms. At further distances (i.e. >100 km), the model relies more on \nthe P coda and S wave energy, likely due to the low P wave amplitudes at these large distances. \nThe Grad-CAM weight distribution for the explosion records (Figure 3 row b) suggests heavier \nreliance on the P waves at closer distances, likely due to deficient S wave generation. At farther \ndistances, a small and gentle peak from the later phases slowly takes over as the highest peak.  \n \nThe Grad-CAM on the spectrograms (Figure 3 rows c and d using model in Figure S5) show \nsimilar patterns of the aggregated weights across the time dimension at different distances. For \nearthquake records, the model focuses on the P and S waves with increasing emphasis on S and \ncoda at farther distances, while for explosion records, it mostly focuses on the P wave and the S \ncoda. These figures also show extra information about the frequency content the model finds \nimportant. First, the energy between the P and S arrivals on the spectrograms appears to be less \nimportant than the analogous weights in the time series. Second, the model focuses on different \nfrequency bands for P and S. The average of the five most important energy bands for each phase \nare shown in Figures 4a and 4b across different distances. The model relies on high-frequency P \nwave and low-frequency S wave energy to make the decisions. An apparent decay of the dominant \nfrequency with distance were observed for the earthquake records but not explosions (Figures 4a \nand 4b). We speculate such distance-frequency dependence is associated with earthquake \nmagnitudes, as higher number of small earthquakes with high corner frequencies are recorded at \ncloser distances. The magnitude distribution for the explosion and earthquake records (Figure S18 \nand S19) support our explanation: the magnitude of explosions remains comparable across all \ndistances, while the earthquakes show a positive correlation.  \n \nThe results for understanding the contributions from the P/S ratios in the physics parameters \nbranch are shown in Figure 4c and 4d. The model learns that larger P/S values are associated with \n \n \n8 \nexplosion records, which are reflected by the higher error rates for earthquakes with high P/S ratios \nand lower error rates for explosion records with similar ratios.  \n \n6 Discussion and Conclusions \nBy adding physics-based features to the explosion discrimination problem, the two-branch model \nwe propose here shows promising results for improving deep learning model performance, \nespecially when the model is applied to data from a region different than the training region. One \npossible reason is that deep learning models are highly optimized to the training data, so they \nperform best when the testing data are from the same distribution. On the other hand, the physics-\nbased features provide a more generally applicable basis, even though they may not perform as \nwell as deep learning features on the training data. In combination, adding in physical features \nadvances the performance on the new testing data, i.e., provides features not captured fully by the \ndeep learning model. The combination essentially forces the model to learn from these physics-\nbased features. While this application was to explosion discrimination, we envision a similar \napproach of incorporating physics-based features could be extended to other problems (e.g., noise \nand seismic discriminant in earthquake early warning, landslides classification and so on). \nAdditional work combining physics and data learning is an exciting direction for future work.  \n \nAdding physics-based features does sacrifice the automated nature of the deep learning, however, \nit leverages prior human knowledge about what features are important and broadly applicable. \nFurthermore, the analysis shown here acts as a proof of concept, demanding extra work to \nimplement it to real-world applications. One potential challenge is that P/S ratios may be \nunavailable for some waveforms (e.g., due to SNR thresholds). In this case, one can just rely on \nthe results from the single deep learning branch, which requires a separate deep learning model to \nbe trained on the waveforms. Alternatively, we could add one extra binary feature in the physics-\nparameter branch for each waveform to indicate if a measurement is available. This binary feature \nwould work like a mask; during the training steps, when there are no P/S ratio measurements, the \nmodel would learn to rely only on the features extracted from the deep learning branch.  \n \nThe discrimination performance is also sensitive to training data selection. The machine learning \nmodel tends to perform really well on the same distribution data, especially when the same data \nsource is split for training and testing. As shown in Figures 2a and 2b, the machine learning models \nevaluated on the same distribution data may only reveal a limited picture, and may eventually fail \nin new data cases. This is known as local generalization of the machine learning models, i.e., they \nonly perform well near where we have data points in the training data. Therefore, using a dataset \nfrom a different region can help reveal the limitation of the trained machine learning models. \n \nUnderstanding what the trained model has learned from the waveform data can provide us with \ninsights into how the model will perform in the real-world applications. The learning of the model \n \n \n9 \nfrom the physics-parameter branch aligns with the human knowledge, i.e., that higher P/S ratios \ncorrespond to explosion records. The Grad-CAM method reveals the focus areas of the trained \nmodel on the time series and the spectrograms. The information gained from P and S waves echoes \nseismologists’ experience and supports the effectiveness of the machine-learning model. In \naddition, parts of the coda waves also contribute to the discrimination, especially at farther \ndistances. On the spectrograms, the model weight peaks at different frequencies for different \nphases and event types, encouraging further tests to utilize different or dynamic frequencies for \nP/S ratio calculations (e.g., Tibi, 2021). In addition, P coda seems to be important for the time \nseries, but not for the spectrograms, and it is less clear which parts of the coda matter most. Here, \nwe only focused on preliminary interpretation of the results, and plan further work to develop a \nmore complete understanding.  \n \nWith the proof of concept of combining physics and data learning demonstrated here, we hope the \nproposed methods can be applied on different problems to build more reliable and extensible \napplications to real world issues of importance.  \n \nAcknowledgments \nThis study uses data from the Source Physics Experiments (SPE), and SPE would not have been \npossible without the support of many people from several organizations. The authors wish to \nexpress their gratitude to the National Nuclear Security Administration, Defense Nuclear \nNonproliferation Research and Development (DNN R&D), and the SPE working group, a multi-\ninstitutional and interdisciplinary group of scientists and engineers. The views expressed in the \narticle do not necessarily represent the views of the U.S. Department of Energy or the U.S. \nGovernment. This research was performed in part under the auspices of the U.S. Department of \nEnergy by the LLNL under Contract Number DE-AC52-07NA27344. This is LLNL \nContribution LLNL-JRNL-829223. This research was partially supported by the Air Force \nResearch Lab under contract FA9453-21-2-0024. We also thank the researchers of the datasets \nused in this study, including Source Physics Experiments (SPE), Bighorn Arch Seismic \nExperiment (BASE), Salton Seismic Imaging Project (SSIP), and imaging magma under the St. \nHelens magma (iMush; i.e., MSH). We thank the IRIS DMC (https://www.iris.edu/hq/) for \nhosting seismic data for research. The IRIS DMC is supported by the National Science \nFoundation under Cooperative Support Agreement EAR-1851048. The websites for these \ndatasets are: SPE: https://doi.org/10.7914/SN/CI, “IM”(no DOI), “LB”(no DOI), \nhttps://doi.org/10.7914/SN/NN, https://doi.org/10.7914/SN/SN, \nhttps://doi.org/10.7914/SN/TA, https://doi.org/10.7914/SN/US, \nhttps://doi.org/10.7914/SN/UU, https://doi.org/10.7914/SN/XE_2012; BASE: \nhttps://doi.org/10.7914/SN/IW, https://doi.org/10.7914/SN/IU, \nhttps://doi.org/10.7914/SN/MB, “PB”(no DOI), “RE”(no DOI), \nhttps://doi.org/10.7914/SN/TA, https://doi.org/10.7914/SN/US, \nhttps://doi.org/10.7914/SN/WY, https://doi.org/10.7914/SN/XV_2009, “Z2”(no DOI), \nhttps://doi.org/10.7914/SN/ZH_2010; MSH: SN/CC\" https://doi.org/10.7914/SN/CC, \n \n \n10 \nhttps://doi.org/10.7914/SN/XD_2014, https://doi.org/10.7914/SN/UW; and SSIP: \nhttps://doi.org/10.7914/SN/AZ, https://doi.org/10.7914/SN/CI, \nhttps://doi.org/10.7914/SN/XD_2011. We thank useful discussions from Stephen Myers, \nGiselle Fernández-Godino, and Donald Lucas at the Lawrence Livermore National Laboratory. \nAll the analysis is done in Python and the deep learning framework used here is TensorFlow \n(Abadi et al., 2016), and seismological related analysis used Obspy (Beyreuther et al., 2010; \nKrischer et al., 2015), we thank the awesome Python communities to make everything openly \navailable. \n  \nReferences \nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., et al. (2016). TensorFlow: A \nsystem for large-scale machine learning. In 12th USENIX Symposium on Operating \nSystems Design and Implementation (OSDI 16) (pp. 265–283). Retrieved from \nhttps://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf \nBergen, K. J., Johnson, P. A., Hoop, M. V. de, & Beroza, G. C. (2019). Machine learning for \ndata-driven discovery in solid Earth geoscience. Science, 363(6433). \nhttps://doi.org/10.1126/science.aau0323 \nBeyreuther, M., Barsch, R., Krischer, L., Megies, T., Behr, Y., & Wassermann, J. (2010). ObsPy: \nA Python Toolbox for Seismology. Seismological Research Letters, 81(3), 530–533. \nhttps://doi.org/10.1785/gssrl.81.3.530 \nBowers, D., & Selby, N. D. (2009). Forensic Seismology and the Comprehensive Nuclear-Test-\nBan Treaty. Annual Review of Earth and Planetary Sciences, 37(1), 209–236. \nhttps://doi.org/10.1146/annurev.earth.36.031207.124143 \nDowla, F. U., Taylor, S. R., & Anderson, R. W. (1990). Seismic discrimination with artificial \nneural networks: Preliminary results with regional spectral data. Bulletin of the \nSeismological Society of America, 80(5), 1346–1373. \nhttps://doi.org/10.1785/BSSA0800051346 \n \n \n11 \nFawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–\n874. https://doi.org/10.1016/j.patrec.2005.10.010 \nFuis, G. S., Bauer, K., Goldman, M. R., Ryberg, T., Langenheim, V. E., Scheirer, D. S., et al. \n(2017). Subsurface Geometry of the San Andreas Fault in Southern California: Results \nfrom the Salton Seismic Imaging Project (SSIP) and Strong Ground Motion Expectations. \nBulletin of the Seismological Society of America, ssabull;0120160309v1. \nhttps://doi.org/10.1785/0120160309 \nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MITPress. Retrieved from \nhttps://mitpress.mit.edu/books/deep-learning \nHan, L., Hole, J. A., Stock, J. M., Fuis, G. S., Kell, A., Driscoll, N. W., et al. (2016). Continental \nrupture and the creation of new crust in the Salton Trough rift, Southern California and \nnorthern Mexico: Results from the Salton Seismic Imaging Project. Journal of \nGeophysical Research: Solid Earth, 121(10), 7469–7489. \nhttps://doi.org/10.1002/2016JB013139 \nKarpatne, A., Ebert-Uphoff, I., Ravela, S., Babaie, H. A., & Kumar, V. (2019). Machine \nLearning for the Geosciences: Challenges and Opportunities. IEEE Transactions on \nKnowledge and Data Engineering, 31(08), 1544–1554. \nhttps://doi.org/10.1109/TKDE.2018.2861006 \nKim, S., Lee, K., & You, K. (2020). Seismic Discrimination between Earthquakes and \nExplosions Using Support Vector Machine. Sensors, 20(7), 1879. \nhttps://doi.org/10.3390/s20071879 \nKingma, D. P., & Ba, J. (2017). Adam: A Method for Stochastic Optimization. ArXiv:1412.6980 \n[Cs]. Retrieved from http://arxiv.org/abs/1412.6980 \n \n \n12 \nKiser, E., Palomeras, I., Levander, A., Zelt, C., Harder, S., Schmandt, B., et al. (2016). Magma \nreservoirs from the upper crust to the Moho inferred from high-resolution Vp and Vs \nmodels beneath Mount St. Helens, Washington State, USA. Geology, 44(6), 411–414. \nhttps://doi.org/10.1130/G37591.1 \nKong, Q., Allen, R. M., Schreier, L., & Kwon, Y.-W. (2016). MyShake: A smartphone seismic \nnetwork for earthquake early warning and beyond. Science Advances, 2(2), e1501055. \nhttps://doi.org/10.1126/sciadv.1501055 \nKong, Q., Trugman, D. T., Ross, Z. E., Bianco, M. J., Meade, B. J., & Gerstoft, P. (2019). \nMachine Learning in Seismology: Turning Data into Insights. Seismological Research \nLetters, 90(1), 3–14. https://doi.org/10.1785/0220180259 \nKong, Q., Chiang, A., Aguiar, A. C., Fernández-Godino, M. G., Myers, S. C., & Lucas, D. D. \n(2021). Deep convolutional autoencoders as generic feature extractors in seismological \napplications. Artificial Intelligence in Geosciences, 2, 96–106. \nhttps://doi.org/10.1016/j.aiig.2021.12.002 \nKoper, K. D., Holt, M. M., Voyles, J. R., Burlacu, R., Pyle, M. L., Wang, R., & Schmandt, B. \n(2021). Discrimination of Small Earthquakes and Buried Single‐Fired Chemical \nExplosions at Local Distances (<150 km) in the Western United States from Comparison \nof Local Magnitude (ML) and Coda Duration Magnitude (MC). Bulletin of the \nSeismological Society of America, 111(1), 558–570. https://doi.org/10.1785/0120200188 \nKrischer, L., Megies, T., Barsch, R., Beyreuther, M., Lecocq, T., Caudron, C., & Wassermann, J. \n(2015). ObsPy: a bridge for seismology into the scientific Python ecosystem. \nComputational Science & Discovery, 8(1), 014003. https://doi.org/10.1088/1749-\n4699/8/1/014003 \n \n \n13 \nLeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444. \nhttps://doi.org/10.1038/nature14539 \nLinville, L., Pankow, K., & Draelos, T. (2019). Deep Learning Models Augment Analyst \nDecisions for Event Discrimination. Geophysical Research Letters, 46(7), 3643–3651. \nhttps://doi.org/10.1029/2018GL081119 \nMagana-Zook, S. A., & Ruppert, S. D. (2017). Explosion Monitoring with Machine Learning: A \nLSTM Approach to Seismic Event Discrimination. AGU Fall Meeting Abstracts, 2017, \nS43A-0834. \nMousavi, S. M., Horton, S. P., Langston, C. A., & Samei, B. (2016). Seismic features and \nautomatic discrimination of deep and shallow induced-microearthquakes using neural \nnetwork and logistic regression. Geophysical Journal International, 207(1), 29–46. \nhttps://doi.org/10.1093/gji/ggw258 \nMousavi, S. M., Sheng, Y., Zhu, W., & Beroza, G. C. (2019). STanford EArthquake Dataset \n(STEAD): A Global Data Set of Seismic Signals for AI. IEEE Access, 7, 179464–\n179476. https://doi.org/10.1109/ACCESS.2019.2947848 \nNational Research Council. (2012). The Comprehensive Nuclear Test Ban Treaty: Technical \nIssues for the United States. Washington, DC: The National Academies Press. \nhttps://doi.org/10.17226/12849 \nOrlic, N., & Loncaric, S. (2010). Earthquake—explosion discrimination using genetic algorithm-\nbased boosting approach. Computers & Geosciences, 36(2), 179–185. \nhttps://doi.org/10.1016/j.cageo.2009.05.006 \nO’Rourke, C. T., Baker, G. E., & Sheehan, A. F. (2016). Using P/S Amplitude Ratios for \nSeismic Discrimination at Local DistancesUsing P/S Amplitude Ratios for Seismic \n \n \n14 \nDiscrimination at Local Distances. Bulletin of the Seismological Society of America, \n106(5), 2320–2331. https://doi.org/10.1785/0120160035 \nPyle, M. L., & Walter, W. R. (2019). Investigating the Effectiveness of P/S Amplitude Ratios for \nLocal Distance Event Discrimination. Bulletin of the Seismological Society of America, \n109(3), 1071–1081. https://doi.org/10.1785/0120180256 \nPyle, M. L., & Walter, W. R. (2021). Exploring the Effects of Emplacement Conditions on \nExplosion P/S Ratios across Local to Regional Distances. Seismological Research \nLetters. https://doi.org/10.1785/0220210270 \nRabin, N., Bregman, Y., Lindenbaum, O., Ben-Horin, Y., & Averbuch, A. (2016). Earthquake-\nexplosion discrimination using diffusion maps. Geophysical Journal International, \n207(3), 1484–1492. https://doi.org/10.1093/gji/ggw348 \nSelvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-\nCAM: Visual Explanations from Deep Networks via Gradient-Based Localization. In \n2017 IEEE International Conference on Computer Vision (ICCV) (pp. 618–626). \nhttps://doi.org/10.1109/ICCV.2017.74 \nSnelson, C. M., Abbott, R. E., Broome, S. T., Mellors, R. J., Patton, H. J., Sussman, A. J., et al. \n(2013). Chemical Explosion Experiments to Improve Nuclear Test Monitoring. Eos, \nTransactions American Geophysical Union, 94(27), 237–239. \nhttps://doi.org/10.1002/2013EO270002 \nTibi, R. (2021). Discrimination of Seismic Events (2006–2020) in North Korea Using P/Lg \nAmplitude Ratios from Regional Stations and a Bivariate Discriminant Function. \nSeismological Research Letters, 92(4), 2399–2409. https://doi.org/10.1785/0220200432 \n \n \n15 \nTibi, R., Linville, L., Young, C., & Brogan, R. (2019). Classification of Local Seismic Events in \nthe Utah Region: A Comparison of Amplitude Ratio Methods with a Spectrogram‐Based \nMachine Learning ApproachClassification of Local Seismic Events in the Utah Region. \nBulletin of the Seismological Society of America, 109(6), 2532–2544. \nhttps://doi.org/10.1785/0120190150 \nTsvang, S. L., Pinsky, V. I., & Husebye, E. S. (1993). Enhanced seismic source discrimination \nusing NORESS recordings from Eurasian events. Geophysical Journal International, \n112(1), 1–14. https://doi.org/10.1111/j.1365-246X.1993.tb01432.x \nUlberg, C. W., Creager, K. C., Moran, S. C., Abers, G. A., Thelen, W. A., Levander, A., et al. \n(2020). Local Source Vp and Vs Tomography in the Mount St. Helens Region With the \niMUSH Broadband Array. Geochemistry, Geophysics, Geosystems, 21(3), \ne2019GC008888. https://doi.org/10.1029/2019GC008888 \nVoyles, J. R., Holt, M. M., Hale, J. M., Koper, K. D., Burlacu, R., & Chambers, D. J. A. (2020). \nA New Catalog of Explosion Source Parameters in the Utah Region with Application to \nML–MC‐Based Depth Discrimination at Local Distances. Seismological Research \nLetters, 91(1), 222–236. https://doi.org/10.1785/0220190185 \nWang, R., Schmandt, B., & Kiser, E. (2020). Seismic Discrimination of Controlled Explosions \nand Earthquakes Near Mount St. Helens Using P/S Ratios. Journal of Geophysical \nResearch: Solid Earth, 125(10), e2020JB020338. https://doi.org/10.1029/2020JB020338 \nWang, R., Schmandt, B., Holt, M., & Koper, K. (2021). Advancing Local Distance \nDiscrimination of Explosions and Earthquakes With Joint P/S and ML-MC \nClassification. Geophysical Research Letters, 48(23), e2021GL095721. \nhttps://doi.org/10.1029/2021GL095721 \n \n \n16 \nWorthington, L. L., Miller, K. C., Erslev, E. A., Anderson, M. L., Chamberlain, K. R., Sheehan, \nA. F., et al. (2016). Crustal structure of the Bighorn Mountains region: Precambrian \ninfluence on Laramide shortening and uplift in north-central Wyoming. Tectonics, 35(1), \n208–236. https://doi.org/10.1002/2015TC003840 \nYeck, W. L., Sheehan, A. F., Anderson, M. L., Erslev, E. A., Miller, K. C., & Siddoway, C. S. \n(2014). Structure of the Bighorn Mountain region, Wyoming, from teleseismic receiver \nfunction analysis: Implications for the kinematics of Laramide shortening. Journal of \nGeophysical Research: Solid Earth, 119(9), 7028–7042. \nhttps://doi.org/10.1002/2013JB010769 \n \n \n \n17 \n \n \n \nFigure 1. The two-branch proposed model. The blue dotted box is the deep learning branch which \ntakes in waveforms as the input with convolutional neural network (CNN) layers as the hidden \nlayers, and a fully connected layer (FC) to flatten out the features. The green dotted box contains \nthe physics-based feature branch, which can take in P/S ratio measurements with or without the \nML-MC measurements (adding ML-MC is shown in supplementary materials). Features from these \ntwo branches will be concatenated and pass another FC layer to make a decision. The small texts \non top of the layer block represent the feature maps in CNN or number neurons in FC. 500x1@64 \nrepresents 64 feature maps with dimension 500x1.  \n \n \n \n \n18 \n \nFigure 2. Classification performance metrics. (a) cases where testing data is from the same region \nas training data, i.e., SPE, BASE, and MSH (20% of the total data saved as testing data). (b) cases \nwhere testing data is from a different region, SSIP, rather than the three training regions (c – f) \nThe ROC curve using training data from any of the three regions and testing on the new fourth \nregion for five random initialization with mean (solid lines) and standard deviation (shaded areas). \nThe blue curves show the designed model with deep learning and physics parameters branches. \nThe orange and green curves are the model only with the deep learning or physics parameters \nbranch. The AUC is shown in the legend. WF – Waveform, PS – P/S ratio.  \n \n \n \n19 \n \n \nFigure 3. Average of the normalized Grad-CAM weights for the earthquake and explosion records \nacross different distance bins (bin size 20 km). Rows a and b are weights on the time series. The \nblue thick lines are the average weights and the vertical thin blue lines are the standard deviation \nin the bins. The vertical green and red lines are the average of the estimated P and S arrivals using \nthe same regional velocity models as in the calculation of the P/S ratios. For each panel, the \nhorizontal axis is the time in seconds starting 5s before the arrival of P wave. The vertical axis is \nthe normalized weight. The rows c and d are weights on the spectrograms. Color shows the \nweights, from blue (0) to red (clipped at 0.5). Vertical green and red lines are the average of the \nestimated P and S arrivals using the same regional velocity models as in the calculation of the P/S \nratios. For other distance bins, please refer to Figures S14 – S17. \n \n \n \n20 \n \n \nFigure 4. The average of the top 5 frequency bins across different distances corresponding to \nFigures S16 and S17 and Physics-based branch error visualization. (a) For the earthquake records. \n(b) For the explosion records. The blue lines are for the P wave while the orange lines are for the \nS wave. (c) P/S ratio error histogram for earthquake records with 0.1 step bins. The grey bars are \nall the earthquake records, and the cyan colored bars are the wrongly estimated records in each \nbin. The blue line with dots is the percentage calculated using the cyan bar over grey bar, i.e. the \nerror rate. (d) P/S ratio error histogram for earthquake records with 0.1 step bins. Same as (c), but \nfor the explosion records. \n \nSupplementary Materials for Combining Deep Learning with Physics Extracted \nFeatures in Explosion-Earthquake Discrimination \n \n \nQingkai Kong1, Ruijia Wang2,3, William R. Walter1, Moira Pyle1, Keith Koper4, Brandon \nSchmandt2 \n \n \n \n \n1Lawrence Livermore National Laboratory \n2University of New Mexico \n3Southern University of Science and Technology \n4University of Utah \n \nCorresponding author: Qingkai Kong (kong11@llnl.gov)  \n \n \nThis research was performed in part under the auspices of the U.S. Department of Energy by the \nLLNL under Contract Number DE-AC52-07NA27344. This is LLNL Contribution LLNL-\nJRNL-829223. \n \n \n \n \nFigure S1. Events and station maps for the four experiments used in the study. The red and magenta \nstars are explosion and earthquake events, respectively. The blue triangles show the distribution \nof the stations. (a) Map for the Source Physics Experiment. (b) Map for the Bighorn Arch Seismic \nExperiment. (c) Map for the Mount St. Helens magma imaging experiment. (d) Map for the Salton \nSeismic Imaging Project.  \n \n \nFigure S2. Raw data distribution from the four different regions, i.e., BASE, SPE, MSH and \nSSIP. (a) cumulative distance distribution. (b) cumulative magnitude distribution. (c) cumulative \ndepth distribution. (d) scatter plot for P/S ratios over distance.  \n \nFigure S3. Augmented data distribution from the four different regions, i.e., BASE, SPE, MSH \nand SSIP. Panels are the same as Figure S2. \n \n \nFigure S4. The corresponding model architecture from tensorflow for Figure 1 in the main text. \nThis version shows more information about the structure, such as input/output dimension and the \ndropout layers.  \n \nFigure S5. The model architecture for the proposed model with the additional sub-branch to take \nin spectrograms as the input in the deep learning branch.  \n \n \nFigure S6. Test results corresponding to the model structure shown in Figure S5. Similar to \nFigures 2c – 2f, but added the performance of the model with the spectrogram. SG – \nSpectrogram input, WF – waveforms, PS – P/S ratios. Thus, SG + WF + PS is the two-branch \nmodel with the spectrogram, time series, and P/S ratios inputs. \n \nAdding ML- MC \nML and MC values are all calculated directly from the waveforms. For computing ML, the HH and \nBH component waveforms are converted to a Wood-Anderson seismometer equivalent response. \nAmplitudes are measured without station corrections, and distance corrections are applied. MC is \ncalculated from the measured coda duration using the equations described in  Koper et al. (2021). \nDue to the quality control threshold (more details see Wang et al. 2021), only 59,623 waveforms \nhave corresponding measured ML-MC values. \n \nAs expected, the models combining the deep learning extracted features with these two physics-\nbased features perform the best across different regions, though the differences from the models \nusing only the P/S ratios are negligible. Although ML-Mc model alone shows the poorest \nperformance compared to other models, adding these measurements to some regions improves the \nresults (e.g., SPE and BASE). Note that due to quality control requirements for valid ML- MC \nmeasurements and P/S ratios, the datasets used for the above tests are much smaller than those \nshown in Figure S6. \n \n \nFigure S7. The ROC curve using training data from any of the three regions and testing on the new \nfourth region with ML-MC. WF + PS + ML_MC are the two-branch models with P/S ratios and \nML-Mc values used in the physics-based branch, model structure is shown in Figure 1. \nPS+ML_MC are the single-branch physics-based models with P/S ratio and ML-MC values as the \ninputs. The ML_MC are the models that only use ML-MC values as the input.  \n \nGrad-CAM  \nFollowing the symbols used in Selvaraju et al. (2017), we can first take the gradient of the score \nyc for class c, with respect to the last CNN layer feature maps Ak, where k is the k-th feature map, \nand by globally averaging them across all the pixels, we can define the importance weights ack for \neach feature map in the following equation \n𝛼!\n\" =\n#\n$ ∑∑\n%&!\n%'\"#\n$\n(\n)\n*\n+\n                                                                     (1) \nwhere u and v are the width and height of the feature map, and i and j are the (i, j) pixel on the \nfeature map. The relative importance of each of the feature maps are given by these weights for \nthe target class c before the model makes a decision. Then the class discriminative localization \nmap can be calculated using equation \n𝐿,-./01'2\n\"\n= 𝑅𝑒𝐿𝑈(∑𝛼!\n\"𝐴!\n3\n!\n)                                                (2) \nwhere n is the total number of feature maps in the last convolutional layer, and ReLU (Rectified \nLinear Unit) is an operation that is the same as the activation functions used in the model; \nmathematically it is defined as max(0, x). The reason to apply a ReLU to the linear combination \nof the weighted feature maps is that only the features that have a positive influence on the target \nclass are needed. The derived localization map is a non-negative weighted average of all the feature \nmaps in the same dimension of the last feature map, and it is up-sampled to the input image \nresolution using bi-linear interpolation to form the final heatmap. \n \n \n \nFigure S8. Grad-CAM on time series of a ML1.6 explosion record recorded at 75.3 km. The titles \nshow the estimation, ground truth, and the estimation probability. For example, the top row shows \nthat the model estimates the input as an explosion with probability=1. The bottom row shows the \nearthquake estimation with probability 0.  \n \n \n \nFigure S9. Grad-CAM on a time series recorded at 63.2 km for a ML1.5 and 18.4 km depth \nevent. Legend is the same as Figure S8.  \n \n \nFigure S10. Grad-CAM on a time series recorded at 29.0 km for a ML1.2 and 3.0 km depth \nevent. Legend is the same as Figure S8. \n \nFigure S11. Grad-CAM heatmaps with the inputs. Two examples are shown here as two columns. \nFor each column, the top row shows the spectrogram inputs for the model, with title indicating the \nmagnitude, epicentral distance, and depth of the event, i.e., ML1.4 earthquake with depth 18.8 km \nand ML1.0 explosion with depth 0.0 km. The second and third rows show the Grad-CAM heatmap \nfor the spectrogram input with warmer colors showing more important regions for the model to \nrely on to make the decision. The titles indicate the estimation, ground truth, and the estimation \nprobability. For example, the second row shows the model estimates the input as an explosion with \nprobability 0 for the left panel, and 1 for the right. The third row shows the earthquake estimation \nprobability. The fourth and fifth rows are the same as the second and third rows but for the time \nseries inputs.   \n \n \nFigure S12. Same with S11 but for two different events, i.e., ML2.9 earthquake with 6.8 km \ndepth and ML0.7 explosion with 0.0 km depth.  \n \n \nFigure S13 Same with S11 but for two different events, i.e., ML1.1 earthquake with 2.7 km depth \nand ML1.1 with 4.2 km depth. \n \nFigure S14. Average of the normalized Grad-CAM weights for the earthquake records across \ndifferent distance bins on the time series input data. Distance bins and number of waveforms in \neach bin are shown in the titles. The blue thick lines are the average weights and the vertical thin \nblue lines are the standard deviation in the bins. The vertical green and red lines are the average of \nthe estimated P and S arrivals using the same regional velocity models as in the calculation of the \nP/S ratios. For each panel, the horizontal axis is the time in seconds starting 5s before the arrival \nof P wave. The vertical axis is the normalized weight.  \n \nFigure S15. Average of the normalized Grad-CAM weights for the explosion records across \ndifferent distance bins on the time series input data. Legends are the same as Figure S14.  \n \n \nFigure S16. Average of the normalized Grad-CAM weights for the earthquake records across \ndifferent distance bins on the spectrogram input data. Color shows the weights, x axis shows the \ntime in seconds, and the y axis is the frequency in Hz. Distance bins and number of waveforms in \nthe bins are shown in the titles. The vertical green and red lines are the average of the estimated P \nand S arrivals using the same regional velocity models as in the calculation of the P/S ratios.  \n \n \n \nFigure S17. Average of the normalized Grad-CAM weights for the explosion records across \ndifferent distance bins on the spectrogram input data. Legends are the same as Figure S16.  \n \n \n \nFigure S18. Boxplot of the magnitude distribution for the explosion data from the four regions \nused in training and testing dataset. Orange line shows the median value, and the top and bottom \nlines of the box indicate the first quartile (Q1) and the third quartile (Q3). The bars represent the \nwhiskers extend from the box by 1.5x the inter-quartile range (IQR), i.e., Q1 – 1.5IQR and \nQ3+1.5IQR \n  \n \nFigure S19. Boxplot of the magnitude distribution for the earthquake data from the four regions \nused in training and testing dataset. Orange line shows the median value, and the top and bottom \nlines of the box indicate the first quartile (Q1) and the third quartile (Q3). The bars represent the \nwhiskers extend from the box by 1.5x the inter-quartile range (IQR), i.e., Q1 – 1.5IQR and \nQ3+1.5IQR \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
  "categories": [
    "physics.geo-ph",
    "cs.LG"
  ],
  "published": "2022-03-12",
  "updated": "2022-03-12"
}