{
  "id": "http://arxiv.org/abs/2203.08174v1",
  "title": "Towards understanding deep learning with the natural clustering prior",
  "authors": [
    "Simon Carbonnelle"
  ],
  "abstract": "The prior knowledge (a.k.a. priors) integrated into the design of a machine\nlearning system strongly influences its generalization abilities. In the\nspecific context of deep learning, some of these priors are poorly understood\nas they implicitly emerge from the successful heuristics and tentative\napproximations of biological brains involved in deep learning design. Through\nthe lens of supervised image classification problems, this thesis investigates\nthe implicit integration of a natural clustering prior composed of three\nstatements: (i) natural images exhibit a rich clustered structure, (ii) image\nclasses are composed of multiple clusters and (iii) each cluster contains\nexamples from a single class. The decomposition of classes into multiple\nclusters implies that supervised deep learning systems could benefit from\nunsupervised clustering to define appropriate decision boundaries. Hence, this\nthesis attempts to identify implicit clustering abilities, mechanisms and\nhyperparameters in deep learning systems and evaluate their relevance for\nexplaining the generalization abilities of these systems. We do so through an\nextensive empirical study of the training dynamics as well as the neuron- and\nlayer-level representations of deep neural networks. The resulting collection\nof experiments provides preliminary evidence for the relevance of the natural\nclustering prior for understanding deep learning.",
  "text": "Towards understanding deep learning with the\nnatural clustering prior\nSimon Carbonnelle\nThesis submitted in partial fulﬁllment of the requirements\nfor the degree of Doctor of Philosophy\nDissertation committee:\nProf. Christophe De Vleeschouwer (UCLouvain, co-advisor)\nProf. Marie Van Reybroeck (UCLouvain, co-advisor)\nProf. Tinne Tuytelaars (K.U.Leuven)\nProf. Vincent Fran¸cois-Lavet (VU Amsterdam)\nProf. Benoˆıt Macq (UCLouvain)\nProf. Laurent Jacques (UCLouvain)\nProf. David Bol (UCLouvain, chair)\nContact: simon.carbonnelle.research@gmail.com\nJanuary 2022\narXiv:2203.08174v1  [cs.LG]  15 Mar 2022\nAbstract\nThe prior knowledge (a.k.a. priors) integrated into the design of a machine\nlearning system strongly inﬂuences its generalization abilities. In the speciﬁc\ncontext of deep learning, some of these priors are poorly understood as they im-\nplicitly emerge from the successful heuristics and tentative approximations of bi-\nological brains involved in deep learning design. Through the lens of supervised\nimage classiﬁcation problems, this thesis investigates the implicit integration\nof a natural clustering prior composed of three statements: (i) natural images\nexhibit a rich clustered structure, (ii) image classes are composed of multiple\nclusters and (iii) each cluster contains examples from a single class. The decom-\nposition of classes into multiple clusters implies that supervised deep learning\nsystems could beneﬁt from unsupervised clustering to deﬁne appropriate decision\nboundaries. Hence, this thesis attempts to identify implicit clustering abilities,\nmechanisms and hyperparameters in deep learning systems and evaluate their\nrelevance for explaining the generalization abilities of these systems.\nOur study of implicit clustering abilities exploits hierarchical class labels\nto show that the subclasses (e.g., orchids, poppies, roses, sunﬂowers, tulips)\nassociated to a class (e.g., ﬂowers) are diﬀerentiated in deep neural networks\nthat generalize well, even though only class-level supervision is provided. We\nthen look for clustering mechanisms through the study of neuron-level training\ndynamics in multilayer perceptrons trained on a synthetic dataset with known\nclusters. Our experiments reveal a winner-take-most mechanism: training pro-\ngressively increases the average pre-activation of the most activated clusters of\na class and decreases the average pre-activation of the least activated clusters\nof the same class. Remarkably, this implicit mechanism leads neurons to dif-\nferentiate some clusters from the same class more strongly than clusters from\ndiﬀerent classes. These studies indicate the emergence of a neuron-level training\nprocess that is critical for implicit clustering to occur. We propose to capture\nthe extent by which the neurons of each layer have been eﬀectively “trained”\nduring the global training process through the amount of layer rotation, i.e. the\ncosine distance between the initial and ﬁnal ﬂattened weight vectors of each\nlayer. Equipped with tools to monitor and control the amount of layer rotation\nduring training, we demonstrate that this implicit hyperparameter exhibits a\nconsistent relationship with model generalization and training speed. Moreover,\nwe show that the impact of layer rotation on training seems to explain the eﬀect\nof several explicit hyperparameters such as the learning rate, weight decay, and\nthe use of adaptive gradient methods.\nOverall, our work thus provides a collection of experiments to support the\nrelevance of the natural clustering prior for explaining generalization in deep\nlearning. Additionally, it highlights the potential of using explicit clustering\nalgorithms for training deep neural networks, as this would facilitate the in-\ntegration of natural clustering-related priors into the design of deep learning\nsystems.\nAcknowledgements\nM\ny deepest gratitude goes to all the people I’ve had the chance to live\nwith during these six years. I don’t think they know how much they\nhave shaped my life -and still do. Thank you Sam, Ysa, Clarisse, Ol,\nDenis, Nouch, Oli, Bibou, Sixtine, Math, Ahmad, Faf, Delph, Steph, Radu,\nGilles, Nico, Nina, Lucie, Cl´ement, Th´eo, H´el`ene, Thomas, Arnould, Leia,\nMarie, Mathieu, Paloma, Fabrice, Lilas, Gregor and of course Claire, my love.\nI also want to thank my family, grandparents and friends for their constant sup-\nport and loyalty despite my sometimes unconventional and confusing lifestyle.\nThank you Christophe for your trust all along your supervision of my thesis.\nThank you for the stimulating discussions and for being one of the ﬁrst people\nwith whom I dared to have an argument. Thank you for staying supportive\nalbeit my rather unstable relationship to our shared project.\nGreat thanks to all my colleagues for their precious humour, care, proofreading\nand support.\nThanks to the reddit r/machinelearning community for helping me dive into\nthe ﬁeld of deep learning.\nFinally, I am grateful to the Universit´e catholique de Louvain and the ICTEAM\ninstitute for providing the infrastructure necessary for this thesis. I am also\ngrateful to the Fondation Louvain, the Universit´e catholique de Louvain, the\nFonds National de la Recherche Scientiﬁque (F.R.S.-FNRS) of Belgium and the\nWalloon Region for funding this project.\nContents\n1\nIntroduction and background\n1\n1.1\nDeep learning basics . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.1.1\nNatural data\n. . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.1.2\nLearning from data . . . . . . . . . . . . . . . . . . . . . .\n3\n1.1.3\nGeneralizing to unseen data . . . . . . . . . . . . . . . . .\n7\n1.2\nThe generalization puzzles of deep learning\n. . . . . . . . . . . .\n8\n1.2.1\nWhy do deep neural networks generalize so well? . . . . .\n8\n1.2.2\nWhy do deep neural networks generalize so poorly? . . . .\n10\n1.3\nSolving the puzzles through the study of priors . . . . . . . . . .\n10\n1.3.1\nOn the role of priors in generalization\n. . . . . . . . . . .\n11\n1.3.2\nThe diﬃcult case of deep learning priors . . . . . . . . . .\n12\n1.3.3\nThe natural clustering prior . . . . . . . . . . . . . . . . .\n13\n1.4\nContributions and thesis outline\n. . . . . . . . . . . . . . . . . .\n14\n2\nAn implicit clustering ability\n17\n2.1\nMeasuring intraclass clustering ability\n. . . . . . . . . . . . . . .\n18\n2.1.1\nTerminology and notations\n. . . . . . . . . . . . . . . . .\n19\n2.1.2\nMeasures based on label hierarchies\n. . . . . . . . . . . .\n19\n2.1.3\nMeasures based on variance . . . . . . . . . . . . . . . . .\n21\n2.2\nExperimental methodology\n. . . . . . . . . . . . . . . . . . . . .\n22\n2.2.1\nBuilding a set of models with varying hyperparameters . .\n22\n2.2.2\nEvaluating correlation with generalization . . . . . . . . .\n24\n2.3\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n2.3.1\nThe measures’ relationships with generalization . . . . . .\n25\nCONTENTS\n2.3.2\nInﬂuence of k on the Kendall coeﬃcients . . . . . . . . . .\n25\n2.3.3\nEvolution of the measures across layers\n. . . . . . . . . .\n27\n2.3.4\nEvolution of the measures over the course of training . . .\n28\n2.3.5\nVisualization of subclass extraction in hidden neurons . .\n29\n2.4\nRelated work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n2.5\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n3\nAn implicit clustering mechanism\n33\n3.1\nExperimental setup . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n3.1.1\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n3.1.2\nNeural networks\n. . . . . . . . . . . . . . . . . . . . . . .\n35\n3.1.3\nTraining process\n. . . . . . . . . . . . . . . . . . . . . . .\n35\n3.2\nA winner-take-most mechanism . . . . . . . . . . . . . . . . . . .\n36\n3.3\nTowards understanding the mechanism . . . . . . . . . . . . . . .\n40\n3.3.1\nAn ablation study\n. . . . . . . . . . . . . . . . . . . . . .\n40\n3.3.2\nOn the role of diﬃcult training examples\n. . . . . . . . .\n41\n3.3.3\nOn the role of ReLU . . . . . . . . . . . . . . . . . . . . .\n41\n3.3.4\nA divide-and-conquer strategy\n. . . . . . . . . . . . . . .\n43\n3.3.5\nWhy does the mechanism aﬀect a single class?\n. . . . . .\n43\n3.4\nConnections with standard deep learning settings . . . . . . . . .\n43\n3.4.1\nTraining dynamics w.r.t. example diﬃculty . . . . . . . .\n44\n3.4.2\nThe Coherent Gradient Hypothesis . . . . . . . . . . . . .\n44\n3.4.3\nThe beneﬁts of data augmentation . . . . . . . . . . . . .\n45\n3.4.4\nThe beneﬁts of pre-training . . . . . . . . . . . . . . . . .\n45\n3.4.5\nThe beneﬁts of depth\n. . . . . . . . . . . . . . . . . . . .\n47\n3.4.6\nThe beneﬁts of large learning rates . . . . . . . . . . . . .\n49\n3.4.7\nThe beneﬁts of implicit clustering abilities . . . . . . . . .\n50\n4\nAn implicit clustering hyperparameter\n51\n4.1\nTools for monitoring and controlling layer rotation . . . . . . . .\n52\n4.1.1\nMonitoring layer rotation with layer rotation curves\n. . .\n52\n4.1.2\nControlling layer rotation with Layca\n. . . . . . . . . . .\n53\n4.2\nExperimental setup . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\nCONTENTS\n4.3\nA systematic study of layer rotation conﬁgurations . . . . . . . .\n56\n4.3.1\nLayer rotation rate conﬁgurations\n. . . . . . . . . . . . .\n56\n4.3.2\nLayer rotation’s relationship with generalization\n. . . . .\n57\n4.3.3\nLayer rotation’s relationship with training speed\n. . . . .\n57\n4.4\nA study of layer rotation in standard training settings . . . . . .\n59\n4.4.1\nAnalysis of SGD’s learning rate . . . . . . . . . . . . . . .\n60\n4.4.2\nAnalysis of SGD and weight decay . . . . . . . . . . . . .\n60\n4.4.3\nAnalysis of learning rate warmups\n. . . . . . . . . . . . .\n62\n4.4.4\nAnalysis of adaptive gradient methods . . . . . . . . . . .\n63\n4.5\nRelated work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n4.6\nOn the interpretation of layer rotations\n. . . . . . . . . . . . . .\n67\n5\nDiscussion and perspectives\n69\n5.1\nTowards validating our hypothesis\n. . . . . . . . . . . . . . . . .\n69\n5.2\nA rebirth of clustering algorithms . . . . . . . . . . . . . . . . . .\n71\n5.3\nThe societal impact of deep learning research . . . . . . . . . . .\n71\nConclusion\n73\nBibliography\n75\nPublications\n87\nCONTENTS\nChapter 1\nIntroduction and\nbackground\nD\neep learning has lead to many technological breakthroughs since the\n2010s. It has progressively substituted all other competing techniques\nfor visual object recognition (Krizhevsky, Sutskever, and Hinton, 2012),\nnatural language processing (Young, Hazarika, Poria et al., 2018), speech recog-\nnition (Graves, Mohamed, and Hinton, 2013), playing board and video games\n(Silver, Huang, Maddison et al., 2016), protein structure prediction (Jumper,\nEvans, Pritzel et al., 2021a) and many others. It is integrated in a myriad of\nmodern applications like social network platforms, e-commerce and smartphone\ncameras (LeCun, Bengio, and Hinton, 2015).\nAs the practical applications of deep learning keep ﬂourishing, the realization\nthat we do not really understand why and how deep learning works is growing.\nRenown researchers associate deep learning to “alchemy”, as current practice\ndepends more on beliefs and intuitions than on well-established scientiﬁc facts\n(Rahimi, 2017). Specialized conference workshops are organized to make sense\nof an increasingly large body of observations that escape our understanding (e.g.,\n“Identifying and understanding deep learning phenomena” workshop organized\nduring ICML 2019).\nDeveloping mathematical theories of deep learning has\nbecome an increasingly active area of research (Arora, 2018; Berner, Grohs,\nKutyniok et al., 2021).\nMaking progress on these puzzles has the potential to facilitate the design of\ndeep learning-based systems and widen their range of applications (e.g., safety-\ncritical applications).\nMoreover, deep learning has always been tightly con-\nnected to neuroscience and biological brains (Schmidhuber, 2014; Wang and\nRaj, 2017; Hassabis, Kumaran, and Summerﬁeld, Christopher Botvinick, 2017).\nHence, a better understanding of deep learning has the potential to bring new\ninsights to a long-standing quest in human history: understanding our own\n1\n2\nCHAPTER 1. INTRODUCTION AND BACKGROUND\nminds.\nIn order to dive into this fascinating ﬁeld, we will start by introducing the\nbasics of deep learning and machine learning. The following section provides\na brief and non-technical introduction to these topics tailored for this speciﬁc\nthesis. Many text books are available for the readers looking for a more exhaus-\ntive overview (e.g., Bishop (2006); Murphy (2021); Goodfellow, Bengio, and\nCourville (2016)).\n1.1\nDeep learning basics\nDeep learning is part of the broader ﬁeld of machine learning. Machine learning\nis a class of techniques used to estimate an unknown function f ∗mapping inputs\nx to outputs y. In the context of image classiﬁcation, which is the main focus\nof this work, f ∗maps an image x to a class y (also called label or category)\nreﬂecting the image’s content (e.g., “dog”, “car” or “house”).\nIn order to estimate the function f ∗, the key speciﬁcity of machine learning\nis to make use of knowledge contained in data. This approach reduces the need\nfor knowledge from human experts, which is particularly useful when human ex-\npertise is costly or diﬃcult to formalize (e.g., subjective, intuitive or unconscious\nexpertise). Before discovering how machine learning techniques extract knowl-\nedge from data in Section 1.1.2, let’s clarify what data means in the context of\ndeep learning.\n1.1.1\nNatural data\nWhile deep learning could be applied on any type of data in principle, its popu-\nlarity is mostly due to its performance on natural images, sounds and language.\nIn these cases, deep learning diﬀers from alternative machine learning techniques\nby working directly on raw data, i.e. with minimal pre-processing (LeCun, Ben-\ngio, and Hinton, 2015). As an example, in the context of image classiﬁcation, the\ninput to a deep learning system are typically images as represented by their pixel\nvalues. For an RGB image of size m × n, we have x ∈Rm×n×3. In comparison,\nalternative techniques require human engineered pre-processing algorithms such\nas Histogram of Gradients (Dalal and Triggs, 2005) or Scale Invariant Feature\nTansforms (Lowe, 1999).\nThe scenario by which data are available for a deep learning system can also\nvary. Supervised learning refers to the scenario where inputs x are provided\nwith their associated outputs y. Unsupervised learning refers to the scenario\nwhere only inputs are available1. We also distinguish static data that takes the\nform of a ﬁxed dataset of S examples (xi, yi) for i ∈{1, 2, ..., S} from a stream\n1Reinforcement and self-supervised learning are two other scenarios that require additional\nformalisms which we do not introduce here.\n1.1. DEEP LEARNING BASICS\n3\nof data where examples are provided sequentially during the machine learning\nprocess. This second scenario is often denoted by continuous learning.\nThis thesis focuses on supervised learning applied to image clas-\nsiﬁcation using static datasets. This problem setting has been extensively\nused for deep learning research. In particular, four image classiﬁcation datasets\nbecame the de facto standard for studying deep learning techniques: MNIST\n(LeCun, Bottou, Bengio et al., 1998), CIFAR10, CIFAR100 (Krizhevsky and\nHinton, 2009) and ImageNet (Deng, Dong, Socher et al., 2009). Each of them\ncontains more than 50.000 images with their associated class. Visualizations\nand speciﬁcations of these four datasets are presented in Figure 1.1 and Table\n1.1 respectively.\nFigure 1.1: Examples from three standard datasets used for deep learning re-\nsearch2. The images are not at scale.\nTable 1.1: Speciﬁcations of four standard datasets used for deep learning re-\nsearch.\nDataset\nImage size\n# of samples\n# of classes\nMNIST\n28 × 28\n70.000\n10\nCIFAR-10\n32 × 32 × 3\n60.000\n10\nCIFAR-100\n32 × 32 × 3\n60.000\n100\nImageNet\ne.g., 200 × 200 × 3\n> 1.000.000\n1000\n1.1.2\nLearning from data\nIn order to extract knowledge from data, machine learning techniques need two\ningredients: a hypothesis class and a training algorithm. The hypothesis class is\n2The three dataset visualizations are taken from https://en.wikipedia.org/wiki/MNIST_\ndatabase, https://www.cs.toronto.edu/˜kriz/cifar.html and https://cs.stanford.edu/\npeople/karpathy/cnnembed/ respectively.\n4\nCHAPTER 1. INTRODUCTION AND BACKGROUND\nthe set of functions that are considered as potential estimates of f ∗. The training\nalgorithm is a data-driven procedure to select one estimate ˆf from the hypothesis\nclass. In the case of deep learning, deep neural networks (DNNs) constitute\nthe hypothesis class and stochastic gradient descent (SGD) the most standard\ntraining algorithm. This section brieﬂy describes these two key components.\nDeep neural networks\nThe fundamental building block of deep neural networks are artiﬁcial neurons.\nThe standard artiﬁcial neuron corresponds to the composition of an aﬃne func-\ntion and a non-linear function, as represented graphically in Figure 1.2. Math-\nematically, this corresponds to\nfneuron (x) = h\n \nw0 +\nn\nX\ni=1\nwixi\n!\nwhere xi represents the ith element of the input x, w0, w1, ..., wn are the aﬃne\nfunction’s parameters and h represents the non-linear or activation function.\nAs of today, the most common activation function is the rectiﬁed linear unit or\nReLU (Nair and Hinton, 2010):\nh(x) = max(0, x).\nThe parameters w0, w1, ..., wn (also denoted by weights) are unspeciﬁed, such\nthat any parameter instantiation produces a function that is part of the hypoth-\nesis class. It is the role of the training algorithm to determine the weights to be\nused to estimate f ∗.\nFigure 1.2:\nGraphical representation of an artiﬁcial neuron.\nThe weights\nw0, w1, ..., wn are to be determined by a training algorithm.\nIn order to estimate complex functions, neural networks can be built by\ncombining and connecting multiple artiﬁcial neurons. The most conceptually\n1.1. DEEP LEARNING BASICS\n5\nsimple neural network is the multilayer perceptron (MLP) represented in Figure\n1.3. In this network, the neurons are organized in layers, where the output of\none layer becomes the input of the next. Such layered structure is a key aspect\nof deep neural networks, where deep refers to the relatively many layers they\ncontain.\nFigure 1.3: Graphical representation of a MLP neural network with two hidden\nlayers.\nIn a multilayer perceptron, each neuron is connected to all the inputs of\nits layer (cfr. Figure 1.3). We call such layers fully connected layers. In the\ncontext of images, another connectivity pattern has been very successful: the\nconvolutional layer (LeCun, Bottou, Bengio et al., 1998). Here, the aﬃne func-\ntion becomes a convolution operation, applied on the spatial dimensions of the\ninputs. Each neuron is thus connected to a local neighbourhood of its layer’s\ninputs, akin the local receptive ﬁelds of visual cortices (Hubel and Wiesel, 1962).\nIn addition, the same aﬃne transformation is applied on each neighbourhood,\nsuch that multiple neurons will share the same parameters w0, w1, ...wn. Neu-\nral networks that contain such layers are commonly called convolutional neural\nnetworks (CNNs).\nMany other types of layers have been proposed besides fully connected and\nconvolutional layers. In the context of this thesis, three other layers are regularly\nused: batch normalization (Ioﬀe and Szegedy, 2015), pooling (LeCun, Bottou,\nBengio et al., 1998) and softmax layers. Batch normalization layers are typically\ninserted between the aﬃne and activation functions of a network. They normal-\nize the (pre-)activations of each neuron to have zero mean and unit variance,\nbased on the statistics of a subset of the entire dataset (a batch). Pooling layers\nare applied between convolutional layers to reduce the spatial dimensions of a\nsignal. They do so by aggregating the values of neighbouring pixels (typically\n2 × 2 patches) through mean or max operations. Finally, softmax layers are ap-\nplied at the output of the network to identify the predicted class. It is used as\n6\nCHAPTER 1. INTRODUCTION AND BACKGROUND\na diﬀerentiable alternative to the one-hot argmax operation. For the interested\nreaders, we refer to the original papers and (Goodfellow, Bengio, and Courville,\n2016) for a more extensive description of all these layers.\nThe power of deep neural networks largely comes from their modular struc-\nture which enables a ﬂexible and adaptative design from relatively simple com-\nponents such as layers.\nWith the years, speciﬁc design choices or architec-\ntures gained popularity, amongst which VGG (Simonyan and Zisserman, 2015),\nResNets (He, Zhang, Ren et al., 2016) and Wide ResNet (Zagoruyko and Ko-\nmodakis, 2016). These three architectures will be regularly used in the context\nof this thesis.\nStochastic gradient descent\nOnce a deep neural network has been designed, its parameters or weights still\nneed to be determined by the training algorithm. The optimal parameters are\nthose that minimize the estimation error. But, because we don’t have access\nto the function f ∗to be estimated, we need to use a proxy of the estimation\nerror instead: the loss function. The information we have about f ∗takes the\nform of data. Hence, the loss function is data-driven and typically returns large\nvalues when an estimate ˆf does not match f ∗on the available data and small\nvalues when it does. In the context of image classiﬁcation, the most common\nloss function is categorical cross-entropy (Goodfellow, Bengio, and Courville,\n2016).\nThe optimization algorithm used by deep learning to minimize a loss function\nis stochastic gradient descent (Goodfellow, Bengio, and Courville, 2016). This\nmethod is especially compelling since (i) deep neural networks and categorical\ncross-entropy are diﬀerentiable almost everywhere w.r.t. the weights, (ii) the\nbackpropagation algorithm provides an eﬃcient way to compute the gradient\n(Linnainmaa, 1970; Werbos, 1982; Rumelhart, Hinton, and Williams, 1986) and\n(iii) the loss can be approximated by a random subset (also called batch) of data.\nLet L(xbatch, ybatch) be the average loss of a random batch of data containing N\nsamples (N is also called the batch size). Stochastic gradient descent iteratively\nupdates each weight wi according to the following rule:\nwt+1\ni\n= wt\ni −λt ∂L(xbatch, ybatch)\n∂wi\n,\nwhere λ is the learning rate, which is a parameter that typically evolves during\ntraining according to a pre-determined schedule. Batches are randomly sampled\nfrom the dataset without replacement. We call an epoch the number of iterations\nrequired for all samples to be considered. A single epoch usually doesn’t suﬃce\nfor convergence of the algorithm, and the whole dataset is considered again after\neach epoch.\nDespite the non-convexity of the loss function, it is empirically observed\nthat stochastic gradient descent, provided appropriate tuning of its learning\n1.1. DEEP LEARNING BASICS\n7\nrate parameter, often converges to a global minimum of the loss function (Du,\nLee, Li et al., 2019). Hence, we are able to determine the weights of a deep\nneural network such that it matches the true function f ∗on the data used by\nthe loss function. But what about data that isn’t considered by it?\n1.1.3\nGeneralizing to unseen data\nIntuitively, since the optimization of the weights targets performance on a single\ndataset, there’s a risk that performance decreases when the model is applied on\nother data. The ultimate goal of machine learning techniques is to provide an\nestimate of f ∗that is also accurate on data not considered by the training algo-\nrithm. This ability is called generalization. It is usually measured by computing\nthe loss (or any another measure of error) on a diﬀerent set of examples (the\ntest set) that was created independently using the same data generation process\nas the data used for training (the training set).\nIn addition to this empirical measurement of generalization ability, provid-\ning frameworks to predict or reason about generalization has been an important\nresearch endeavour. The most successful frameworks involve a balance between\nsome notion of capacity (also denoted by complexity, expressive power, richness,\nor ﬂexibility) associated to the hypothesis class and the size of the training set.\nInformally, the capacity of a hypothesis class reﬂects the diversity of functions\nit contains. The larger the capacity, the higher the chance that the hypothesis\nclass contains good approximations of f ∗. However, it also augments the chance\nof containing functions that generalize poorly, i.e. that provide good approxi-\nmations of f ∗on the training set only. This risk gets mitigated by increasing\nthe size of the training set, as the latter then becomes more representative of\nthe data generation process.\nThese intuitions have lead to the bias-variance trade-oﬀ(cfr. Figure 1.4).\nThis is a commonly adopted heuristic that, for a given training set size, pos-\ntulates the existence of an optimal middle ground between too low a capacity\n(denoted by underﬁtting) and too high a capacity (denoted by overﬁtting) (Ge-\nman, Bienenstock, and Doursat, 1992). A more rigorous formalization of these\nintuitions is provided by Vapnik–Chervonenkis theory, which balances VC di-\nmensions (which is a measure of capacity) with the size of the training set to\nbound the diﬀerence between training and test errors (which reﬂects generaliza-\ntion ability) (Vapnik and Chervonenkis, 1968; Vapnik, 1989).\nCapacity-based reasoning can also be useful to think about the role of train-\ning algorithms in generalization. Indeed, even if a hypothesis class has a large\ncapacity (i.e. can represent a lot of diﬀerent functions), the training algorithm\ndoesn’t necessarily search through all functions uniformly. In particular, the al-\ngorithm can be designed to favour certain types of functions, which are expected\nto generalize better. Aspects of the training algorithm which aim to improve\ngeneralization are commonly denoted by regularization. The most classical ex-\nample is L2 regularization, which penalizes functions whose parameters have a\n8\nCHAPTER 1. INTRODUCTION AND BACKGROUND\nlarge Euclidean norm.\nFigure 1.4:\nIllustration of the bias-variance trade-oﬀ, a commonly adopted\nheuristic that, for a given training set size, postulates the existence of an op-\ntimal middle ground between too low a capacity (denoted by underﬁtting) and\ntoo high a capacity (denoted by overﬁtting).\n1.2\nThe generalization puzzles of deep learning\nEven though generalization constitutes the ultimate purpose of machine learn-\ning systems, it largely escapes our understanding in the case of deep learning.\nThe mystery is two-fold. First, deep neural networks generalize remarkably well\nfrom the perspective of classical theoretical frameworks and conventional wis-\ndom (cfr. Section 1.1.3). Second deep neural networks generalize remarkably\npoorly compared to us, humans. This section dives deeper into these two open\nquestions.\n1.2.1\nWhy do deep neural networks generalize so well?\nA remarkable aspect of modern deep neural networks is their gigantic size. State\nof the art models can contain hundreds of layers and millions of parameters (He,\nZhang, Ren et al., 2016; Zagoruyko and Komodakis, 2016). This implies that\nthe capacity of the hypothesis classes used for deep learning are extremely large.\nA typical trend in classical theories and heuristics is that large capacity involves\nthe risk of overﬁtting (cfr. Section 1.1.3). Two pioneering works have shown\nthat deep neural networks mysteriously mitigate this risk.\nFirst, Neyshabur, Tomioka, and Srebro (2015) observed that generalization\nability improves when increasing the amount of neurons (and thus the capacity)\n1.2. THE GENERALIZATION PUZZLES OF DEEP LEARNING\n9\nof single-hidden-layer neural networks, even beyond what is needed to achieve\nzero training error (cfr. Figure 1.5). This contradicts the bias-variance trade-\noﬀ, which states that increasing capacity should ultimately lead to overﬁtting\n(cfr. Figure 1.4). Second, Zhang, Bengio, Hardt et al. (2017) observed that\nstate of the art networks reached perfect training error even when the class\nlabels of their training set are randomized. This implies an ability to memorize\neach example of the training set, and thus an hypothesis class large enough\nto contain many functions that cannot generalize at all. Both works thus show\nthat the generalization abilities of deep neural network do not seem to be aﬀected\nby their enormous capacity. On the contrary, increasing the capacity of deep\nneural networks tends to beneﬁt generalization and is a key component of state\nof the art models.\nFigure 1.5: This experimental result from Neyshabur, Tomioka, and Srebro\n(2015) shows that increasing the amount of neurons H in a one-hidden-layer\nneural network trained on MNIST does not seem to lead to an increase in the test\nerror, even if perfect performance on the training set is already achieved. This\ncontradicts the commonly held belief that increasing capacity should ultimately\nlead to overﬁtting (cfr. Figure 1.4).\nThe large capacity of deep neural networks’ hypothesis classes must thus be\ncompensated by strong regularization mechanisms that steer the training algo-\nrithm towards functions that generalize well. However, both works show that\ntheir observations hold even in the absence of classical regularization techniques.\nIn order to make sense of their experimental results, Neyshabur, Tomioka, and\nSrebro (2015) and Zhang, Bengio, Hardt et al. (2017) thus conjecture the exis-\ntence of an implicit form of regularization originating from stochastic gradient\ndescent. The characterization of this implicit regularization mechanism has be-\ncome a very active, yet unsolved area of research (e.g., Zhang, Bengio, Hardt\net al. (2021); Wu, Zou, Braverman et al. (2021); Smith, Dherin, Barrett et al.\n(2021); Barrett and Dherin (2021); Yun, Krishnan, and Mobahi (2021)).\n10\nCHAPTER 1. INTRODUCTION AND BACKGROUND\n1.2.2\nWhy do deep neural networks generalize so poorly?\nDeep learning is often considered as a potential candidate for human-level ar-\ntiﬁcial intelligence. Hence, it makes sense to compare the performance of deep\nneural networks to humans. While deep neural networks can achieve super-\nhuman performance on speciﬁc datasets (He, Zhang, Ren et al., 2015), their\ngeneralization ability appears to be much worse.\nA ﬁrst line of work demonstrated that fooling deep neural networks into\nwrong and yet conﬁdent image classiﬁcations was relatively easy in an adversar-\nial setting. Szegedy, Zaremba, Sutskeveer et al. (2014); Goodfellow, Shlens, and\nSzegedy (2015) fool networks by adding small perturbations to the inputs that\nare invisible to the human eye, Su, Vargas, and Kouichi (2019) by changing the\nvalue of a single pixel and Nguyen, Yosinski, and Clune (2015) by generating\nimages from scratch that are unrecognizable to humans.\nWhile in the adversarial setting data are manipulated artiﬁcially, a large\nbody of work has shown that natural changes to the data can also dramat-\nically aﬀect a deep neural network’s performance. Torralba and Efros (2011)\nshowed that deep neural networks do not generalize well from one image classiﬁ-\ncation dataset to the other, and Recht, Roelofs, Schmidt et al. (2019); Shankar,\nRoelofs, Mania et al. (2020) observed the same behaviour even when extra care\nis taken to replicate the data generation process. Deep neural networks have\nalso been shown to lack robustness to changes in the background (cfr. Figure\n1.6) (Beery, van Horn, and Perona, 2018), object pose (Alcorn, Li, Gong et al.,\n2019) or texture (Geirhos, Rubisch, Michaelis et al., 2018). Their performance\nalso worsens when small rotations and translations are applied to the image (En-\ngstrom, Tran, Tsipras et al., 2019) as well as corruptions and distortions (Dodge\nand Karam, 2017; Geirhos, Medina Temme, Rauber et al., 2018; Hendrycks and\nDietterich, 2019).\nOverall, there is a growing consensus that deep neural networks are very far\nfrom human-level understanding of natural data. Spurious correlations only oc-\ncurring in speciﬁc datasets seem to play a crucial role in their decisions, leading\nto a lack of robustness to adversarial and natural changes to the data. Over-\ncoming this crucial limitation of deep learning has become a very active and\nyet unsolved area of research (e.g., Arjovsky (2021); Gulrajani and Lopez-Paz\n(2021); Krueger, Caballero, Jacobsen et al. (2021); Nagarajan, Andreassen, and\nNeyshabur (2021)).\n1.3\nSolving the puzzles through the study of pri-\nors\nWhile machine learning leverages data to be less reliant on human expertise,\nthe latter still plays a crucial role. In particular, machine learning practitioners\nspecify the hypothesis class and the training algorithm, which heavily inﬂuence\n1.3. SOLVING THE PUZZLES THROUGH THE STUDY OF PRIORS\n11\n(A) Cow: 0.99, Pasture:\n0.99, Grass: 0.99, No Person:\n0.98, Mammal: 0.98\n(B) No Person: 0.99, Water:\n0.98, Beach: 0.97, Outdoors:\n0.97, Seashore: 0.97\n(C) No Person: 0.97,\nMammal: 0.96, Water:\n0.94, Beach: 0.94, Two: 0.94\nFigure 1.6: This result taken from Beery, van Horn, and Perona (2018) illus-\ntrates the poor generalization abilities of deep neural networks compared to\nhumans. For diﬀerent images of cows, the top ﬁve classes and conﬁdence pro-\nduced by a deep learning system are shown. We observe that the quality of the\npredictions heavily depends on the background. In particular, the cow is better\nrecognized in a ”common” background (Alpine pastures) than in unusual ones\nwhich are probably poorly represented in the training set (e.g., seashore).\na machine learning system’s generalization ability in practice. The practitioners’\nchoices are typically based on some a priori knowledge they possess about the\nfunction f ∗to be estimated (a.k.a. priors). This section explores the role of\npriors in generalization and in deep learning3.\n1.3.1\nOn the role of priors in generalization\nThe No Free Lunch theorem (NFL) states that all machine learning systems\n(even a completely random system that does not depend on data) are equivalent\nin terms of generalization ability in the absence of assumptions or priors on the\nproblem to be solved (Wolpert, 1996; Schaﬀer, 1994). This suggests that the\npriors integrated in a system are key for its performance in a speciﬁc problem\nsetting. Intuitively, the more an algorithm integrates relevant knowledge from\nits designers, the less training data it requires for generalizing well. In particular,\npriors can lead to hypothesis classes and training algorithms which consider a\nmore restricted set of functions while still including good estimations of the\ntarget function f ∗.\nIncluding the role of priors in a general learning theory requires a formalism\n3The notion of prior is very related to the notion of inductive bias.\nWe use priors as\na characteristic of the problem, describing its inherent structure.\nAn inductive bias is a\ncharacteristic of the machine learning system, describing the assumptions it makes on the\nproblems it will be applied on. Generally, one wants the inductive biases to correspond to\npriors that were eﬀectively integrated into the machine learning system’s design. Hence, priors\nand inductive biases are often two sides of the same coin.\n12\nCHAPTER 1. INTRODUCTION AND BACKGROUND\nto represent priors and their relationship with learning problems and algorithms.\nThe bayesian learning framework goes into this direction by expressing priors\nthrough the language of probability theory and making their role in a learning\nsystem more explicit through the use of Bayes’ rule. Another more recent eﬀort\nformalizes the role of priors by incorporating “Teachers” in machine learning\nsystems in addition to data, hypothesis classes and training algorithms (Vapnik\nand Izmailov, 2019). However, these lines of work did not yet lead to theorems\nconnecting priors and generalization in a useful and practical way. In the ab-\nsence of a general theory, one can build theories for speciﬁc problem settings. In\nthis context, assumptions concerning the relevance of priors can be made (and\ntested empirically). A growing body of work argues that studying the priors in-\ntegrated into deep learning systems speciﬁcally is key to solve the generalization\npuzzles we described in Section 1.2 (e.g., Arpit, Jastrzebski, Ballas et al. (2017);\nKawaguchi, Kaelbling, and Bengio (2017); Dauber, Feder, Koren et al. (2020)).\nBut even then, producing a theory of deep learning remains a challenge. Indeed,\nthe priors involved in deep learning appear to be quite diﬃcult to determine and\nformalize.\n1.3.2\nThe diﬃcult case of deep learning priors\nModern deep learning is the result of a relatively long and tedious endeavour.\nIts development started more than 60 years ago and gathered variable amounts\nof popularity over time (cfr. the AI winters). Throughout the process, biolog-\nical brains have been an important source of inspiration (Hassabis, Kumaran,\nand Summerﬁeld, Christopher Botvinick, 2017). From the mathematical for-\nmulation of artiﬁcial neurons (McCulloch and Pitts, 1943) to their learnability\n(Hebb, 1949; Rosenblatt, 1958; Widrow and Hoﬀ, 1960), to convolutional con-\nnectivity patterns (Fukushima, 1980; LeCun, Bottou, Bengio et al., 1998) and\nattention mechanisms (Mnih, Heess, Graves et al., 2014), many foundational\nideas of deep learning are inspired from biological brains. The origin of deep\nlearning’s most successful training algorithm (SGD) provides an exception. In\ncontrast to many alternative training algorithms (e.g., Hebb (1949); Rosenblatt\n(1958)), SGD is not inspired from biological brains, but is rather a very gen-\neral mathematical tool whose use in deep learning stems mostly from a trick\nthat makes it computationally eﬃcient (the backpropagation algorithm, cfr.\nLinnainmaa (1970); Werbos (1982)). SGD’s popularity greatly increased when\nempirical work suggested that it was capable of learning important intermediary\nfeatures automatically (Rumelhart, Hinton, and Williams, 1986). But how this\ncapability emerged from SGD was not explained. Given its empirical successes,\nseveral works attempt to discover how biological brains could in fact implement\nbackpropagation-like algorithms after all (Bengio, Lee, Bornschein et al., 2015;\nLillicrap, Santoro, Marris et al., 2020).\nWhile the above paragraph summarizes a long history in a few sentences\n(we refer to Schmidhuber (2014); Wang and Raj (2017); Lecun (2019) for more\nexhaustive historical perspectives), it reveals that crucial ideas behind deep\n1.3. SOLVING THE PUZZLES THROUGH THE STUDY OF PRIORS\n13\nlearning originate from studies of biological brains and trial and error. Since they\ndo not stem from an understanding of natural data-related problems, they do\nnot provide insights about the priors deep learning takes advantage of. We have\nlittle to no clue as to why deep neural networks and SGD are appropriate choices\nfor natural data problems. Several works provide attempts to characterize the\npriors of deep learning. Today’s most popular priors are the need for distributed\nrepresentations with multiple levels of abstraction (e.g., Rumelhart, Hinton, and\nWilliams (1986); Hinton, Mcclelland, and Rumelhart (1987); Bengio (2009);\nBengio, Courville, and Vincent (2012); LeCun, Bengio, and Hinton (2015)).\nThese priors remain intuitive and are diﬃcult to use in practice to solve deep\nlearning’s puzzles. For example, we are not aware of any formal way to measure\nthe extent by which a deep neural network’s representations are distributed or\ncontain abstraction. Overall, the characterization of deep learning priors is thus\nfar from established and complementary/alternative priors could play a critical\nrole.\n1.3.3\nThe natural clustering prior\nThe natural clustering prior states that natural image datasets exhibit a rich\nclustered structure. This means that natural images can be partitioned into\ndiﬀerent groups (or clusters) such that images inside a group are more similar\nto each other (according to some metric) than to images from other groups.\nWhile this remains a very high-level and quite general description, additional\nstatements can be associated to the natural clustering prior which describe the\nshape of clusters, their relative density, the distance between them or their\nrelationship with class labels. The more precision we can achieve, the more\nhelpful the prior will be.\nPrevious work added the statement that samples\nfrom diﬀerent classes do not belong to the same cluster (Chapelle and Zien,\n2005; Bengio, Courville, and Vincent, 2012). Hence a cluster always contains\nsamples from one unique class. In this thesis, we further state that there\nare many more clusters than classes in standard image classiﬁcation\ndatasets.\nThis implies that a single class is divided into multiple\ndistinct clusters, which we denote by intraclass clusters.\nFigure 1.7 provides a motivation for this prior by identifying intraclass clus-\nters in standard image classiﬁcation datasets. Another argument arises from the\nhierarchical structure of many class labellings. For example, CIFAR100 contains\n20 superclasses (e.g., ﬂowers) which are further divided into 100 subclasses (e.g.,\norchids, poppies, roses, sunﬂowers, tulips). The ImageNet class labels are also\nhierarchically organized with up to 6 levels of abstraction (e.g., digital clock →\nclock →timepiece →measuring instrument →device →artifact). The fact\nthat class labels can be decomposed in multiple subclasses suggests that the\nassociated data can be grouped into multiple intraclass clusters.\nThe presence of intraclass clusters implies that supervised image classiﬁers\nwould beneﬁt from unsupervised clustering and appropriate assumptions on the\n14\nCHAPTER 1. INTRODUCTION AND BACKGROUND\nFigure 1.7: In standard image classiﬁcation datasets, a single class can often be\ndecomposed in multiple groups of similarly looking images, which we interpret as\nintraclass clusters. The occurrence of multiple clusters inside a class constitutes\none of the statements of the natural clustering prior investigated in this thesis.\nclusters’ characteristics. Indeed, whether supervised classiﬁers interpret a set\nof data as one unique or two distinct clusters leads to diﬀerent decision bound-\naries, and thus diﬀerent generalization abilities (cfr. illustration in Figure 1.8).\nThe integration of unsupervised clustering in supervised image classiﬁers was\nalready suggested for non-deep learning approaches (Mansur and Kuno, 2008;\nHoai and Zisserman, 2013). Could unsupervised clustering constitute a prior\nof deep learning systems? Even though no clustering-related components are\nexplicitly programmed into deep neural networks or SGD, these could emerge\nimplicitly. Such a hypothesis is especially compelling since several works con-\njectured the emergence of implicit forms of regularization during deep neural\nnetwork training (cfr. Section 1.2.1).\n1.4\nContributions and thesis outline\nThis thesis evaluates the relevance of the natural clustering prior for under-\nstanding the generalization abilities of deep learning. It does so by identifying\nimplicit clustering in deep learning and studying its relationship with general-\nization. More precisely, we provide a collection of experiments suggesting the\noccurrence of an implicit clustering ability (Chapter 2), an implicit clustering\nmechanism (Chapter 3) and an implicit clustering hyperparameter (Chapter 4)\nin deep learning. Additionally, we show that these clustering phenomena ex-\nhibit a consistent relationship with generalization ability. Our work opens many\npaths of investigation. Hence, we present a discussion and future perspectives\nin Chapter 5.\n1.4. CONTRIBUTIONS AND THESIS OUTLINE\n15\n(a)\n(b)\nFigure 1.8: A simpliﬁed two-dimensional representation of the natural cluster-\ning prior as formulated by this thesis. Points refer to training examples, colors\nto their associated class. The natural clustering prior states that (i) data is\nstructured into clusters, (ii) clusters contain examples from a single class (like\nin (a) and (b)) and (iii) classes are composed of multiple clusters (like in (b)).\nThe latter implies that supervised image classiﬁers would beneﬁt from unsuper-\nvised clustering abilities which make appropriate assumptions on the clusters’\ncharacteristics. For example, A, B and C in ﬁgure (b) constitute ambiguous\ndata structures which could be interpreted as one unique or two distinct clus-\nters by supervised classiﬁers.\nEach interpretation leads to diﬀerent decision\nboundaries, and thus diﬀerent generalization abilities. In this thesis, we exam-\nine whether similar phenomena occur in much higher dimensions when deep\nlearning is applied on natural data.\n16\nCHAPTER 1. INTRODUCTION AND BACKGROUND\nChapter 2\nAn implicit clustering\nability\nThe proposed natural clustering prior suggests that unsupervised clustering abil-\nities could beneﬁt the generalization performance of supervised image classiﬁers\n(cfr. Section 1.3.3). While no clustering mechanisms are explicitly programmed\ninto deep learning, these could emerge implicitly. We show in Figure 2.1 that\ndeep neural networks of suﬃcient depth seem to diﬀerentiate clusters belong-\ning to the same class (i.e. intraclass clusters) in the context of a simple 2D\nclassiﬁcation problem.\nWhen studying standard problem settings, the main challenge resides in\nevaluating a model’s clustering abilities without having access to the underlying\nmechanisms or the clusters’ deﬁnitions.\nHence, our work designs intraclass\nclustering measures based on the following three guiding principles:\n1. Quantify the extent by which a model diﬀerentiates examples or sub-\nclasses1 that belong to the same class, in order to approximately capture\nintraclass clustering;\n2. Identify measures that correlate with generalization, in order to capture\nphenomena that are fundamental to the learning process;\n3. Study multiple measures that oﬀer diﬀerent perspectives in order to reduce\nthe risk that the correlation with generalization is induced by phenomena\nindependent of intraclass clustering.\nBased on these three principles, we provide ﬁve tentative measures of intra-\nclass clustering diﬀering in terms of representation level (black-box vs. neuron\n1Subclasses are available in datasets with hierarchical labellings, where classes (e.g., ﬂow-\ners) are further decomposed into multiple subclasses (e.g., orchids, poppies, roses, sunﬂowers,\ntulips).\n17\n18\nCHAPTER 2. AN IMPLICIT CLUSTERING ABILITY\nvs. layer) and the amount of knowledge about the data’s inherent structure\n(datasets with or without hierarchical labels).\nTo make the link with generalization, we train more than 500 models with\ndiﬀerent generalization abilities by varying 8 standard hyperparameters in a\nprincipled way.\nThe measures’ relationship with generalization is then eval-\nuated qualitatively through visual inspection and quantitatively through the\ngranulated Kendall rank-correlation coeﬃcient introduced by Jiang, Neyshabur,\nMobahi et al. (2020). Both evaluations reveal a tight connection between the\nﬁve proposed measures and generalization ability, providing important evidence\nto support the occurrence and crucial role of implicit clustering abilities in deep\nlearning. Finally, we conduct a series of experiments to provide insights on the\npresumed mechanisms underlying the intraclass clustering abilities which are\nfurther studied in Chapter 3.\nFigure 2.1: Deep neural networks with diﬀerent amount of layers are trained\non a toy problem containing two classes (blue and orange) and two intraclass\nclusters associated to the blue class. For each network depth, 50 models are\ntrained with diﬀerent initializations. The average predictions of these models\nis represented by the heatmaps. They reveal that the diﬀerent models exhibit\ndiﬀerent intraclass clustering abilities. In particular, the linear models do not\ndiﬀerentiate the intraclass clusters at all, while the 3-layer networks’ decision\nboundaries tend to “envelop” the clustered structure. In this chapter, we study\nthe same phenomena in the high dimensions associated to real-world datasets\nand evaluate their relationship with generalization.\n2.1\nMeasuring intraclass clustering ability\nThis section introduces the ﬁve measures of intraclass clustering ability. The\nmeasures diﬀer in terms of representation level (black-box vs. neuron vs. layer)\nand the amount of knowledge about the data’s inherent structure (datasets\nwith or without hierarchical labels). An implementation of the measures based\non Tensorﬂow (Agarwal, Barham, Brevdo et al., 2016) and Keras (Chollet et al.,\n2015) is available at https://github.com/Simoncarbo/Intraclass-clustering-measures.\n2.1. MEASURING INTRACLASS CLUSTERING ABILITY\n19\n2.1.1\nTerminology and notations\nThe letter D denotes the training dataset and I the number of classes in D.\nWe denote the set of examples from class i by Ci with i ∈I = {1, 2, ..., I}.\nIn the case of hierarchical labels, Ci denotes the samples from subclass i and\nSs(i) the samples from the superclass containing subclass i. We denote by N =\n{1, 2, ..., N} and L = {1, 2, ..., L} the indexes of the N neurons and L layers of a\nnetwork respectively. Neurons are considered across all the layers of a network,\nnot a speciﬁc layer. The methodology by which indexes are assigned to neurons\nor layers does not matter. We further denote by meanj∈J and medianj∈J the\nmean and median operations over the index j respectively. Moreover, meank\nj∈J\ncorresponds to the mean of the top-k highest values, over the index j.\nWe call pre-activations (and activations) the values preceding (respectively\nfollowing) the application of the ReLU activation function (Nair and Hinton,\n2010). In our experiments, batch normalization (Ioﬀe and Szegedy, 2015) is\napplied before the ReLU, and pre-activation values are collected after batch\nnormalization. In convolutional layers, a neuron refers to an entire feature map.\nThe spatial dimensions of such a neuron’s (pre-)activations are reduced through\na global max pooling operation before applying our measures.\n2.1.2\nMeasures based on label hierarchies\nThe ﬁrst three measures take advantage of datasets that include a hierarchy of\nlabels. For example, CIFAR100 is organized into 20 superclasses (e.g. ﬂowers)\neach comprising 5 subclasses (e.g. orchids, poppies, roses, sunﬂowers, tulips).\nWe hypothesize that these hierarchical labels reﬂect an inherent structure of the\ndata. In particular, we expect the subclasses to approximately correspond to\ndiﬀerent clusters amongst the samples of a superclass. Hence, measuring the\nextent by which a network diﬀerentiates subclasses when being trained on su-\nperclasses should reﬂect its ability to extract intraclass clusters during training.\nA black-box measure\nThe ﬁrst measure is black-box and is thus not restricted to deep neural net-\nworks. Motivated by the toy experiment presented in Figure 2.1, the measure\nis based on a model’s predictions on the linear interpolation points between two\ntraining examples. We assume the model groups examples into convex clusters.\nIf, for two examples of a given superclass, the predicted probability of the su-\nperclass stays close to 1 along the interpolation points, the network probably\ndid not associate the examples to diﬀerent clusters. On the contrary, a drop in\nthe predicted probability might be reminiscent of a separation of the examples\ninto diﬀerent clusters (like in Figure 2.1). We use these intuitions to quantify\nthe diﬀerentiation of subclasses. The measure evaluates whether the drops in\npredicted superclass probability are smaller when interpolating between exam-\n20\nCHAPTER 2. AN IMPLICIT CLUSTERING ABILITY\nples from the same subclass than when interpolating between examples from\ndiﬀerent subclasses. Let λSs(i),E1×E2 be the average drop in the predicted prob-\nability of superclass Ss(i) when interpolating between examples from subsets E1\nand E2 belonging to Ss(i). The measure is deﬁned as:\nc0 = mediani∈I\nλ Ss(i),Ci×Ci\nλ Ss(i),Ci×(Ss(i)\\Ci)\n(2.1)\nThe median operation is used instead of the mean to aggregate over subclasses,\nas it provided a slightly better correlation with generalization. We suspect this\narises from the outlier behaviour of certain subclasses observed in Section 2.3.5.\nNeuron-level subclass selectivity\nThe second measure quantiﬁes how selective individual neurons are for a given\nsubclass Ci with respect to the other samples of the associated superclass Ss(i).\nHere, strong selectivity means that the subclass Ci can be reliably discriminated\nfrom the other samples of Ss(i) based on the neuron’s pre-activations2. Let µn,E\nand σn,E be the mean and standard deviation of a neuron n’s pre-activation\nvalues taken over the samples of set E. The measure is deﬁned as follows:\nc1 = mediani∈I meank\nn∈N\nµn,Ci −µn,Ss(i)\\Ci\nσn,Ci + σn,Ss(i)\\Ci\n(2.2)\nSince we cannot expect all neurons of a network to be selective for a given\nsubclass, we only consider the top-k most selective neurons. The measure thus\nrelies on k neurons to capture the overall network’s ability to diﬀerentiate each\nsubclass.\nLayer-level Silhouette score\nThe third measure quantiﬁes to what extent the samples of a subclass are close\ntogether relative to the other samples from the associated superclass in the\nspace induced by a layer’s activations.\nIn other words, we measure to what\ndegree diﬀerent subclasses can be associated to diﬀerent clusters in the interme-\ndiate representations of a network. We quantify this by computing the pairwise\ncosine distances3 on the samples of a superclass and applying the Silhouette\nscore (Kaufman and Rousseeuw, 2009) to assess the clustered structure of its\nsubclasses. This score captures the extent by which an example is close (in terms\nof cosine distance) to examples of its subclass compared to examples from other\nsubclasses. Let silhouette(al, Ss(i), Ci) be the mean silhouette score of subclass\n2In other words, we are interested in evaluating whether the linear projection implemented\nby the neuron has been eﬀective in isolating a given subclass.\n3Using cosine distances provided slightly better results than euclidean distances.\n2.1. MEASURING INTRACLASS CLUSTERING ABILITY\n21\nCi based on the activations al of superclass Ss(i) in layer l, the measure is then\ndeﬁned as:\nc2 = mediani∈I meank\nl∈L silhouette(al, Ss(i), Ci)\n(2.3)\n2.1.3\nMeasures based on variance\nTo establish the generality of our results, we also design two measures that\ncan be applied in absence of hierarchical labels. We hypothesize that the dis-\ncrimination of intraclass clusters should be reﬂected by a high variance in the\nrepresentations associated to a class. If all the samples of a class are mapped\nto close-by points in the neuron- or layer-level representations, it is likely that\nthe neuron/layer did not identify intraclass clusters.\nVariance in the neuron-level representations of the data\nThe ﬁrst variance measure is based on standard deviations of a neuron’s pre-\nactivations. If the standard deviation computed over the samples of a class is\nhigh compared to the standard deviation computed over the entire dataset, we\ninfer that the neuron has learned features that diﬀerentiate samples belonging\nto this class. The measure is deﬁned as:\nc3 = meani∈I meank\nn∈N\nσn,Ci\nσn,D\n(2.4)\nA visual representation of the measure is provided in Figure 2.2.\nVariance in the layer-level representations of the data\nThe ﬁfth measure transfers the neuron-level variance approach to layers by com-\nputing the standard deviations over the pairwise cosine distances calculated in\nthe space induced by the layer’s activations. Let Σl,E be the standard devia-\ntion of the pairwise cosine distances between the samples of set E in the space\ninduced by layer l. The measure is deﬁned as:\nc4 = meani∈I meank\nl∈L\nΣl,Ci\nΣl,D\n(2.5)\nTo improve this measure’s correlation with generalization, we found it helpful\nto standardize the representations of diﬀerent neurons. More precisely, we nor-\nmalize each neuron’s pre-activations to have zero mean and unit variance, then\napply a bias and ReLU activation function such that 25% of the samples are\nactivated4. This makes the measure invariant to rescaling and translation of\neach neuron’s preactivations.\n4Activating 25% of the samples was an arbitrary choice that we did not seek to optimize.\n22\nCHAPTER 2. AN IMPLICIT CLUSTERING ABILITY\nFigure 2.2: Our simplest measure (denoted c3) quantiﬁes intraclass clustering\nthrough the ratio of standard deviations σn,Ci and σn,D associated to the class\nCi and the entire dataset D respectively. Intuitively, a high ratio means that the\nneuron relies on features that diﬀerentiate samples from Ci although they belong\nto the same class. Despite its simplicity, our results in Section 2.3 suggest a\nremarkably strong connection between c3 and generalization performance. This\nillustration of measure c3 is based on a neuron from our experimental study,\nand the associated ratio is 2.47.\n2.2\nExperimental methodology\nThe purpose of our experimental endeavour is to assess the relationship between\nthe proposed intraclass clustering measures and generalization performance.\nTo this end, we reproduce the methodology introduced by Jiang, Neyshabur,\nMobahi et al. (2020). First of all, this methodology puts emphasis on the scale\nof the experiments to improve the generality of the observations. Second, it\ntries to go beyond standard measures of correlation, and puts extra care to de-\ntect causal relationships between the measures and generalization performance.\nThis is achieved through a systematic variation of multiple hyperparameters\nwhen building the set of models to be studied, combined with the application\nof principled correlation measures.\n2.2.1\nBuilding a set of models with varying hyperparam-\neters\nOur experiments are conducted on three datasets and two network architectures.\nThe datasets are CIFAR10, CIFAR100 and the coarse version of CIFAR100 with\n20 superclasses (Krizhevsky and Hinton, 2009). The two network architectures\nare Wide ResNets (He, Zhang, Ren et al., 2016; Zagoruyko and Komodakis,\n2016) (applied on CIFAR100 datasets) and VGG variants (Simonyan and Zis-\nserman, 2015) (applied on CIFAR10 dataset).\nBoth architectures use batch\nnormalization layers (Ioﬀe and Szegedy, 2015) since they greatly facilitate the\n2.2. EXPERIMENTAL METHODOLOGY\n23\ntraining procedure.\nIn order to build a set of models with a wide range of generalization perfor-\nmances, we vary hyperparameters that are known to be critical. Since varying\nmultiple hyperparameters improves the identiﬁcation of causal relationships, we\nvary 8 diﬀerent hyperparameters: learning rate, batch size, optimizer (SGD or\nAdam (Kingma and Ba, 2015)), weight decay, dropout rate (Srivastava, Hin-\nton, Krizhevsky et al., 2014), data augmentation, network depth and width.\nA straightforward way to generate hyperparameter conﬁgurations is to specify\nvalues for each hyperparameter independently and then generate all possible\ncombinations.\nHowever, given the amount of hyperparameters, this quickly\nleads to unrealistic amounts of models to be trained.\nTo deal with this, we decided to remove co-variations of hyperparameters\nwhose inﬂuence on training and generalization is suspected to be related. More\nprecisely, we use weight decay only in combination with the highest learning\nrate value, as recent works demonstrated a relation between weight decay and\nlearning rate (van Laarhoven, 2017; Zhang, Wang, Xu et al., 2019). We also\ndon’t combine dropout and data augmentation, as the eﬀect of dropout is dras-\ntically reduced when data augmentation is used.\nFinally, we do not jointly\nincrease width and depth, to avoid very large models that would slow down our\nexperiments.\nThe resulting hyperparameter values are as follows:\n1. (Learning rate, Weight decay): {(0.01, 0.), (0.32, 0.), (0.1, 0.), (0.1, 4 ×\n10−5)}\n2. Batch size: {100, 300}\n3. Optimizer: {SGD, Adam}\n4. (Dropout rate, Data augm.): {(0., true), (0., false), (0.2, false), (0.4, false)}\n5. (Width factor, Depth factor): {(×1., ×1.), (×1.5, ×1.), (×1., ×1.5))}\nWe generate all possible combinations of these hyperparameter values (or pairs\nof values), leading to 192 conﬁgurations.\nSince dropout rates of 0.4 lead to\npoor training performance on VGG variants, only 144 conﬁgurations are used\nin these cases.\nWe train all the models for 250 epochs, and reduce the learning rate by a\nfactor 0.2 at epochs 150, 230, 240. Training stops prematurely if the training\nloss gets smaller than 10−4. Since diﬀerent optimizers may require diﬀerent\nlearning rates for optimal performance (Wilson, Roelofs, Stern et al., 2017), we\ndivide the learning rate by 100 when using Adam to improve its performance\nin our experiments (the same approach is used in Jiang, Neyshabur, Mobahi\net al. (2020)). Overall, all networks reach close to 100% training accuracy, as\nreported by Figure 2.3.\n24\nCHAPTER 2. AN IMPLICIT CLUSTERING ABILITY\nFigure 2.3: Histogram of performances of the set of models used in our experi-\nments.\n2.2.2\nEvaluating correlation with generalization\nJiang, Neyshabur, Mobahi et al. (2020) provides multiple criteria to evaluate the\nrelationship between a measure and generalization. We opted for the granulated\nKendall coeﬃcient for its simplicity and intuitiveness. This coeﬃcient compares\ntwo rankings of the models, respectively provided by (i) the measure of interest\nand (ii) the models’ generalization performances.\nThe Kendall coeﬃcient is\ncomputed across variations of each hyperparameter independently. The average\nover all hyperparameters is then computed for the ﬁnal score. The goal of this\napproach is to better capture causal relationships by not overvaluing measures\nthat correlate with generalization only when speciﬁc hyperparameters are tuned.\nWe compare our intraclass clustering-based measures to sharpness-based\nmeasures. The latter constituted the most promising measure family from the\nlarge-scale study presented in Jiang, Neyshabur, Mobahi et al. (2020). Among\nthe many diﬀerent sharpness measures, we leverage the magnitude-aware ver-\nsions that measure sharpness through random and worst-case perturbations of\nthe weights (denoted by\n1\nσ′ and\n1\nα′ , respectively, in Jiang, Neyshabur, Mobahi\net al. (2020)). We also include the application of these measures with pertur-\nbations applied on kernels only (i.e.\nnot on biases and batch normalization\nweights) with batch normalization layers in batch statistics mode (i.e. not in\ninference mode). We observed that these alternate versions often provided bet-\nter estimations of generalization performance. We denote these measures by\n1\nσ′′\nand\n1\nα′′ .\n2.3. RESULTS\n25\n2.3\nResults\nThis section starts with a thorough evaluation of the relationship between the\nﬁve proposed measures and generalization performance, using the setup de-\nscribed in Section 2.2. Then, it presents a series of experiments to better char-\nacterize intraclass clustering, the phenomenon we expect to be captured by the\nmeasures. These experiments include (i) an analysis of the measures’ evolution\nacross layers and training iterations, (ii) a study of the neuron-level measures’\nsensitivity to k in the mean over top-k operation, as well as (iii) visualizations\nof subclass extraction in individual neurons.\n2.3.1\nThe measures’ relationships with generalization\nWe compute all ﬁve measures on the models trained on the CIFAR100 super-\nclasses, and only the two variance-based measures on the models trained on\nstandard CIFAR100 and CIFAR10 -because they don’t provide subclass infor-\nmation. We set k = 30 for the neuron-level measures, meaning that 30 neurons\nper subclass (for c1) or class (for c3) are used to capture intraclass clustering.\nFor the layer-level measures, we set k = 5 for residual networks and k = 1 for\nVGG networks.\nWe start our evaluation of the measures by visualizing their relationship with\ngeneralization performance in Figure 2.4. We observe a clear correlation across\ndatasets, network architectures and measures. To further support the conclu-\nsions of our visualizations, we evaluate the measures through the granulated\nKendall coeﬃcient (cfr. Section 2.2.2). Tables 2.1, 2.2 and 2.3 present the gran-\nulated Kendall rank-correlation coeﬃcients associated with intraclass clustering\nand sharpness-based measures, for the three dataset-architecture pairs.\nThe Kendall coeﬃcients further conﬁrm the observations in Figure 2.4 by\nrevealing strong correlations between intraclass clustering measures and gen-\neralization performance across all hyperparameters. In terms of overall score,\nintraclass clustering measures surpass the sharpness-based measures variants by\na large margin across all dataset-architecture pairs. On some speciﬁc hyperpa-\nrameters, sharpness-based measures outperform intraclass clustering measures.\nIn particular,\n1\nα′ performs remarkably well when the batch size parameter is\nvaried, which is coherent with previous work (Keskar, Mudigere, Nocedal et al.,\n2017).\n2.3.2\nInﬂuence of k on the Kendall coeﬃcients\nIn our evaluation of the measures in Section 2.3.1, the k parameter, which con-\ntrols the number of highest values considered in the mean over top-k operations,\nwas ﬁxed quite arbitrarily. Figure 2.5 shows how the Kendall coeﬃcient of c1\nand c3 changes with this parameter. We observe a relatively low sensitivity of\n26\nCHAPTER 2. AN IMPLICIT CLUSTERING ABILITY\nFigure 2.4: Visualization of the relationship between the ﬁve proposed intra-\nclass clustering measures and generalization performance, across datasets and\nnetwork architectures. The ﬁve columns correspond to c0, c1, c2, c3 and c4 mea-\nsures respectively. All measures display a tight connection with generalization\nperformance.\nthe measures’ predictive power with respect to k. In particular, in the case of\nResnets trained on CIFAR100 the Kendall coeﬃcient associated with c3 seems to\nstay above 0.7 for any k in the range [1, 900]. The optimal k value changes with\nthe considered dataset and architecture. We leave the study of this dependency\nas a future work.\nObserving the inﬂuence of k also confers insights about the phenomenon\ncaptured by the measures. Figure 2.5 reveals that very small k values work\nremarkably well. Using a single neuron per subclass (k = 1 in Equation 2.2)\nconfers a Kendall coeﬃcient of 0.69 to c1. Using a single neuron per class confers\na Kendall coeﬃcient of 0.78 to c3 in the case of VGGs trained on CIFAR10.\nThese results suggest that individual neurons play a crucial role in the extraction\nof intraclass clusters during training.\nThe fact that the Kendall coeﬃcients\nmonotonically decrease after some k value suggests that the extraction of a\ngiven intraclass cluster takes place in a sub part of the network, indicating some\nform of specialization.\n2.3. RESULTS\n27\nTable 2.1: Kendall coeﬃcients for resnets trained on CIFAR100 superclasses.\nThe higher the coeﬃcient, the stronger the correlation with generalization. Cor-\nrelations are measured across variations of speciﬁc hyperparameters (cfr. 8 ﬁrst\ncolumns) or all of them (cfr. last column).\nlearning\nrate\nbatch\nsize\nweight\ndecay\noptim.\ndropout\nrate\ndata\naugm.\nwidth\ndepth\ntotal\nscore\nIntraclass\nclustering\nc0\n0.57\n0.5\n0.21\n0.27\n0.81\n1.0\n0.25\n0.22\n0.48\nc1\n0.88\n0.31\n0.38\n0.67\n0.96\n1.0\n0.81\n0.69\n0.71\nc2\n0.86\n0.5\n0.67\n0.58\n0.99\n1.0\n0.38\n0.62\n0.7\nc3\n0.88\n0.6\n0.46\n0.62\n0.81\n1.0\n0.81\n0.66\n0.73\nc4\n0.89\n0.69\n0.62\n0.65\n0.86\n1.0\n0.44\n0.69\n0.73\nSharpness\n1 / σ′\n0.81\n0.51\n0.31\n0.69\n0.28\n-0.58\n0.67\n0.61\n0.41\n1 / σ′′\n0.86\n0.58\n0.17\n0.4\n-0.05\n0.42\n0.69\n0.72\n0.47\n1 / α′\n0.88\n0.94\n0.29\n0.26\n0.6\n0.08\n-0.03\n-0.09\n0.37\n1 / α′′\n0.85\n0.8\n0.48\n0.71\n0.16\n-0.08\n0.08\n0.34\n0.42\nTable 2.2: Kendall coeﬃcients for resnets trained on CIFAR100.\nlearning\nrate\nbatch\nsize\nweight\ndecay\noptim.\ndropout\nrate\ndata\naugm.\nwidth\ndepth\ntotal\nscore\nIntraclass\nclustering\nc3\n0.94\n0.65\n0.62\n0.58\n1.0\n1.0\n1.0\n0.78\n0.82\nc4\n0.93\n0.62\n0.62\n0.21\n1.0\n1.0\n0.91\n0.78\n0.76\nSharpness\n1 / σ′\n0.88\n0.68\n0.17\n0.8\n0.4\n-0.62\n0.94\n0.61\n0.48\n1 / σ′′\n0.92\n0.61\n0.12\n0.35\n-0.06\n0.31\n0.94\n0.53\n0.47\n1 / α′\n0.96\n0.96\n0.17\n0.25\n0.54\n0.15\n-0.16\n-0.23\n0.33\n1 / α′′\n0.96\n0.91\n0.42\n0.64\n0.12\n-0.25\n0.17\n0.14\n0.39\nTable 2.3: Kendall coeﬃcients for VGG networks trained on CIFAR10.\nlearning\nrate\nbatch\nsize\nweight\ndecay\noptim.\ndropout\nrate\ndata\naugm.\nwidth\ndepth\ntotal\nscore\nIntraclass\nclustering\nc3\n0.92\n0.83\n0.67\n0.51\n0.92\n1.0\n1.0\n0.88\n0.84\nc4\n0.86\n0.75\n0.33\n0.29\n0.92\n1.0\n0.54\n0.92\n0.7\nSharpness\n1 / σ′\n0.86\n0.62\n-0.25\n0.6\n-0.04\n-0.27\n1.0\n0.85\n0.42\n1 / σ′′\n0.9\n0.67\n0.11\n0.69\n0.92\n0.19\n1.0\n0.94\n0.68\n1 / α′\n0.94\n0.89\n0.61\n0.53\n0.67\n0.77\n0.15\n0.06\n0.58\n1 / α′′\n0.93\n0.67\n0.36\n0.54\n0.69\n-0.02\n0.15\n-0.15\n0.4\n2.3.3\nEvolution of the measures across layers\nWe pursue our experimental endeavour with an analysis of the proposed mea-\nsures’ evolution across layers. For each dataset-architecture pair, we select 64\nmodels which have the same depth hyperparameter value. We then compute the\nfour measures on a layer-level basis (we use the top-5 neurons of each layer for\nthe neuron-level measures) and average the resulting values over the 64 models.\nFigure 2.6 depicts how the average value of each measure evolves across layers\n28\nCHAPTER 2. AN IMPLICIT CLUSTERING ABILITY\nFigure 2.5: Plots showing how the Kendall coeﬃcients of c1 and c3 change\nwith parameter k (cfr. Equations 2.2 and 2.4). The k parameter associated to\nc1 is multiplied by 5 in the plots, to enable comparison with c3 (there are 5\nsubclasses in each of CIFAR100’s superclasses). The total number of neurons\nvaries from 1920 to 2880 in Resnets and from 960 to 1440 in VGGs. The plots\nreveal that generalization performance can be quite accurately estimated using\nthe representations of a surprisingly small set of neurons (k = 1, i.e. a single\nneuron per class, suﬃces in some cases).\nfor the three dataset-architecture pairs.\nWe observe two interesting trends. First, all four measures tend to increase\nwith layer depth. This suggests that intraclass clustering also occurs in the deep-\nest representations of neural networks, and not merely in the ﬁrst layers, which\nare commonly assumed to capture generic or class-independent features. Second,\nthe variance based measures (c3 and c4) decrease drastically in the penultimate\nlayer. We suspect this reﬂects the grouping of samples of a class in tight clusters\nin preparation for the ﬁnal classiﬁcation layer (such behaviour has been studied\nin Kamnitsas, Castro, Le Folgoc et al. (2018); M¨uller, Kornblith, and Hinton\n(2019)). The measures c1 and c2 are robust to this phenomenon as they rely\non relative distances inside a single class, irrespectively of the representations\nof the rest of the dataset.\n2.3.4\nEvolution of the measures over the course of training\nIn this section, we provide a small step towards the understanding of the dy-\nnamics of the phenomenon captured by the measures. We visualize in Figure 2.7\nthe evolution of the measures over the course of training of three Resnet models.\nThe ﬁrst interesting observation comes from the comparison of models with high\nand low generalization performances. It appears that their diﬀerences in terms\nof intraclass clustering measures arise essentially during the early phase of train-\ning. The second observation is that signiﬁcant increases in intraclass clustering\nmeasures systematically coincide with signiﬁcant increases of the training accu-\nracy (in the few ﬁrst epochs and around epoch 150, where the learning rate is\nreduced). This suggests that supervised training could act as a necessary driver\n2.3. RESULTS\n29\nFigure 2.6: Evolution of each measure (after averaging over 64 models) across\nlayers for the three dataset-architecture pairs. The overall increase of the mea-\nsures with layer depth suggests that intraclass clustering occurs even in the\ndeepest representations of neural networks.\nfor intraclass clustering ability, despite not explicitly targeting such behaviour.\n2.3.5\nVisualization of subclass extraction in hidden neu-\nrons\nWe have seen in Section 2.3.2 that the measure c1 reaches a Kendall coeﬃcient\nof 0.69 when considering a single neuron per subclass (k = 1 in Eq. 2.2). Visu-\nalizing the training dynamics in this speciﬁc neuron should enable us to directly\nobserve the phenomenon captured by c1. We study a Resnet model trained\non CIFAR100 superclasses with high generalization performance (82.31% test\naccuracy). For each of the 100 subclasses, we compute the selectivity value and\nthe index of the most selective neuron based on the part of Eq. 2.2 to which\nthe median operation is applied. We then rank the subclasses by their selectiv-\nity value, and display the training dynamics of the neurons associated to the\nsubclasses with maximum and median selectivity values in Figure 2.8.\nThe evolution of the neurons’ preactivation distributions along training re-\nveals that the ’Rocket’ subclass, which has the highest selectivity value, is pro-\ngressively distinguished from its corresponding superclass during training. The\nneuron behaves like it was trained to identify this speciﬁc subclass although no\n30\nCHAPTER 2. AN IMPLICIT CLUSTERING ABILITY\nFigure 2.7: Evolution of the intraclass clustering measures over the course of\ntraining for three models with diﬀerent generalization performances. We ob-\nserve that the diﬀerences between models with high and low generalization per-\nformance arise essentially in the early phase of training.\nsupervision or explicit training mechanisms were implemented to target this be-\nhaviour. The same phenomenon occurs to a lesser extent with the ’Ray’ subclass,\nwhich has the median selectivity value. We observed that very few subclasses\nreached selectivity values as high as the ’Rocket’ subclass (the distribution of\nselectivity values is provided in Figure 2.9). We suspect that the occurrence of\nsuch outliers explain why the median operation outperformed the mean in the\ndeﬁnition of c1 and c2.\nFigure 2.8: Evolution along training of the preactivation distributions associated\nwith the neurons that are the most selective (cfr. Eq. 2.2) for ’Rocket’ and\n’Ray’ subclasses. The neurons behave like they were trained to identify these\nspeciﬁc subclasses although no supervision or explicit training mechanisms were\nimplemented to target this behaviour.\n2.4. RELATED WORK\n31\nFigure 2.9: Distribution of neural subclass selectivity values (cfr. measure c1)\nover the 100 subclasses of CIFAR100. For each subclass, neural subclass selec-\ntivity is computed based on the most selective neuron in the neural network (i.e.\nk = 1). We observe that (i) only a few subclasses reach high selectivity values\nand (ii) the selectivity values vary much across subclasses. We suspect that the\noutliers with exceptionally high selectivity values cause the median operation\nto outperform the mean in the measures c1 and c2.\n2.4\nRelated work\nMany observations made in this chapter are coherent with previous work. In the\ncontext of transfer learning, Huh, Agrawal, and Efros (2016) shows that repre-\nsentations that discriminate ImageNet classes naturally emerge when training\non their corresponding superclasses, suggesting the occurrence of intraclass clus-\ntering. Sections 2.3.2 and 2.3.5 suggest a key role for individual neurons in the\nextraction of intraclass clusters. This is coherent with the large body of work\nthat studied the emergence of interpretable features in the hidden neurons (or\nfeature maps) of deep nets (Zeiler and Fergus, 2014; Simonyan, Vedaldi, and\nZisserman, 2014; Yosinski, Clune, Nguyen et al., 2015; Zhou, Khosla, Lapedriza\net al., 2015; Bau, Zhou, Khosla et al., 2017). In Section 2.3.4, we notice that in-\ntraclass clustering occurs mostly in the early phase of training. Previous works\nhave also highlighted the criticality of this phase of training with respect to reg-\nularization (Golatkar, Achille, and Soatto, 2019), optimization trajectories (Jas-\ntrzebski, Szymczak, Fort et al., 2020; Fort, Dziugaite, Paul et al., 2020), Hessian\neigenspectra (Gur-Ari, Roberts, and Dyer, 2018), training data perturbations\n(Achille, Rovere, and Soatto, 2019) and weight rewinding (Frankle, Dziugaite,\nRoy et al., 2020; Frankle, Schwab, and Morcos, 2020). Morcos, Barrett, Rabi-\nnowitz et al. (2018); Leavitt and Morcos (2020) have shown that class-selective\nneurons are not necessary and might be detrimental for performance. This is\ncoherent with our observation that neurons that diﬀerentiate samples from the\nsame class improve performance.\n32\nCHAPTER 2. AN IMPLICIT CLUSTERING ABILITY\n2.5\nDiscussion\nOur results show that the measures proposed in Section 2.1 (i) correlate with\ngeneralization, (ii) tend to increase with layer depth and (iii) change mostly in\nthe early phase of training. These similarities suggest that the measures capture\none unique phenomenon. Since all measures quantify to what extent a neural\nnetwork diﬀerentiates samples from the same class, the captured phenomenon\npresumably consists in intraclass clustering.\nThis hypothesis is further sup-\nported by the neuron-level visualizations provided in Section 2.3.5. Overall, our\nresults thus provide empirical evidence for this thesis’ hypotheses, i.e. that im-\nplicit clustering abilities emerge during standard deep neural network training\nand improve their generalization abilities.\nHowever, the assessment of the causal nature of the measures’ relationships\nwith clustering and generalization still relies on sophisticated correlation mea-\nsures and informal arguments. Identifying implicit clustering mechanisms in\ndeep learning would further support our hypotheses by strengthening causality.\nInterestingly, our results provide some insights on these presumed mechanisms.\nIn particular, the neuron-level measures correlate with generalization perfor-\nmance as strongly as the layer-level measures in our experiments. As suggested\nby Section 2.3.2, the behaviour of some carefully selected neurons seems to\nquite accurately predict properties of the entire neural network they belong to.\nFigure 2.8 further suggests that individual neurons seem to possess a training\nroutine of their own, targeting the classiﬁcation of a subclass. All these results\nindicate that individual neurons play a crucial role in the presumed clustering\nmechanisms of deep learning. The next chapter of this thesis delves into these\nintriguing phenomena.\nChapter 3\nAn implicit clustering\nmechanism\nChapter 2 studies ﬁve tentative measures of implicit clustering ability in deep\nlearning that appear to strongly correlate with generalization.\nIn order to\nstrengthen the causal relationship between the proposed measures and clus-\ntering, this chapter tries to unveil the clustering mechanisms that presumably\nunderlie these abilities. This requires delving into the inner workings of deep\nneural network training.\nThe main diﬃculty arises from the fact that SGD is a global or end-to-end\noptimization algorithm. Contrary to biologically inspired alternatives, SGD is\nnot based on the repetition of local, neuron-level mechanisms whose behavior is\nmuch simpler to study and understand (e.g. Hebb (1949); Rosenblatt (1958)).\nHowever, several works have suggested that individual neurons exhibit localized\nbehavior during SGD-based training, too. Most notably, a large body of work\nhas observed empirically that interpretable features are captured by the hidden\nneurons (or feature maps) of trained deep neural networks (Zeiler and Fergus,\n2014; Simonyan, Vedaldi, and Zisserman, 2014; Yosinski, Clune, Nguyen et al.,\n2015; Zhou, Khosla, Lapedriza et al., 2015; Bau, Zhou, Khosla et al., 2017;\nCammarata, Carter, Goh et al., 2020). Additionally, as discussed in Section\n2.5, multiple experiments of Chapter 2 indicate that individual neurons play a\ncrucial role in the presumed clustering mechanisms of deep learning.\nThis chapter thus initiates the search for implicit clustering mechanisms\nby studying SGD from the perspective of hidden neurons. More precisely, we\nmonitor both the pre-activations and the partial derivatives of the loss w.r.t the\nactivations in hidden neurons. These two signals are illustrated in Figure 3.1.\nWe study MLP networks trained on a synthetic dataset with known intr-\naclass clusters and on a two-class version of the MNIST dataset obtained by\naggregating the original classes into two superclasses. Our experiments reveal\n33\n34\nCHAPTER 3. AN IMPLICIT CLUSTERING MECHANISM\na behavior similar to the winner-take-most approach of several clustering algo-\nrithms (e.g. Martinetz and Schulten (1991); Fritzke (1997)). Indeed, we observe\nthat the training process progressively increases the average pre-activation of\nthe most activated clusters of a class and decreases the average pre-activation\nof the least activated clusters of the same class in each neuron. Remarkably,\nthis sometimes leads neurons to diﬀerentiate clusters belonging to the same class\nmore strongly than clusters from diﬀerent classes (cfr. Section 3.2). In order\nto solve the classiﬁcation problem, the network thus seems to apply a divide-\nand-conquer strategy, where diﬀerent neurons specialize for the classiﬁcation of\ndiﬀerent clusters of a class.\nTo better understand our observations, we provide an empirical investigation\nof the phenomenon and an intuitive explanation inspired by the Coherent Gra-\ndients Hypothesis introduced by Chatterjee (2020) and the training dynamics\nw.r.t. example diﬃculty studied by Arpit, Jastrzebski, Ballas et al. (2017) (cfr.\nSection 3.3). In order to support the generality of our observations, we further\nshow in Section 3.4 that despite its simplicity, our setup exhibits and provides\ninsights on many phenomena occurring in state-of-the-art models such as the\nregularizing eﬀects of depth, pre-training, data augmentation, large learning\nrates and, importantly, the implicit clustering abilities studied in Chapter 2.\nFigure 3.1: Illustration of an artiﬁcial hidden neuron inside a network. ϕ and\nL denote the activation function and the training loss respectively. The indices\ni, j, t correspond to neuron, input example and training step indices respectively.\nThe symbols pi,j,t and ai,j stand for the neuron’s pre-activation and activation\nvalues respectively. In this paper, we monitor both the neuron’s pre-activation\npi,j,t and the partial derivative of the loss w.r.t. the neuron’s activation ( ∂L\n∂ai,j )t.\n3.1. EXPERIMENTAL SETUP\n35\n3.1\nExperimental setup\n3.1.1\nDatasets\nOur work is mainly based on a synthetic dataset whose clustered structure is\nexactly known. We denote this dataset by SynthClust in the rest of the chapter.\nSynthClust is composed of vectors of 500 elements, such that x ∈R500. Each\ncluster’s centroid is a binary pattern with exactly ﬁve elements set to 1. 30\ncentroids with non-overlapping patterns are generated and split into two classes,\nsuch that each class contains 15 intraclass clusters. For each cluster, 500 training\nexamples and 200 testing examples are generated by adding Gaussian noise with\nzero mean and 0.4 standard deviation on each of the 500 components of the\ncluster’s centroid.\nIn order to improve the generality of our results, Section 3.2 also studies an\nMNIST variant where the ﬁrst and last ﬁve digits are grouped into two distinct\nclasses (class 0: 0 →4, class 1: 5 →9).\nAs in Chapter 2, the ﬁve digits\nof a class are assumed to approximately correspond to ﬁve intraclass clusters.\nThis constitutes an approximation, as Figure 1.7 suggests that single digits are\nthemselves composed of multiple clusters.\n3.1.2\nNeural networks\nWe train MLP networks with a single hidden layer on MNIST and SynthClust,\nwith and without batch normalization respectively. The hidden layer is com-\nposed of 1000 neurons without additive weights (i.e. biases). The output layer\nis composed of one sigmoid neuron associated to the binary cross-entropy loss.\nIn the case of SynthClust, the model can be very simply expressed as follows:\nf (x) = σ (W2 · ReLU (W1 · x) + b2)\nwhere ReLU and σ denote the ReLU and sigmoid functions respectively, and\nW1 and (W2, b2) denote the weights of the hidden and output layer respectively.\nIn Section 3.4, an MLP with multiple hidden layers is trained on SynthClust.\nCompared to the single layer model, this multi-layer network applies batch\nnormalization before each ReLU layer as this stabilizes training. Each hidden\nlayer is also composed of 1000 neurons without biases.\n3.1.3\nTraining process\nWe use Layca (an SGD variant introduced in Chapter 4) for training, as this\ngreatly facilitated the design and the hyperparameter tuning involved in our\nexperiments. This design choice is more extensively discussed and investigated\nin Chapter 4. The SynthClust models are trained in full-batch mode whereas\nthe MNIST models use a batch size of 20000. We use large batch sizes in order\n36\nCHAPTER 3. AN IMPLICIT CLUSTERING MECHANISM\nto avoid the sampling noise inherent to small-batch training. While small batch\nsizes have been considered as a determining factor for generalization in the past\n(Keskar, Mudigere, Nocedal et al., 2017), this view has now been relativized\nby several works (Hoﬀer, Hubara, and Soudry, 2017; Goyal, Dollar, Girshick\net al., 2017; Ginsburg, Gitman, and You, 2018; Geiping, Goldblum, Pope et al.,\n2021). In our context, the use of large batch sizes did not prevent our models\nfrom exhibiting good generalization performances. We trained the models for\n100 and 400 epochs on MNIST and SynthClust respectively, using a learning\nrate of 3−3 which is reduced by a factor of 5 at epochs 85 or 375 respectively.\nThe 5 hidden layer MLP we study in Section 3.4.5 is trained for 1000 epochs,\nwith a reduction of the learning rate by a factor of 5 at epoch 950.\n3.2\nA winner-take-most mechanism\nWe trained the one hidden layer MLPs on SynthClust and MNIST. The resulting\nmodels reach 96.1% and 98% test accuracy respectively. After each training\niteration, we recorded the two neuron-level training signals represented in Figure\n3.1 in 50 hidden neurons and for each training example.\nAfter training, we\nselected 10 hidden neurons amongst the 50 monitored ones for our visualizations.\nThis selection targets the neurons with the strongest inﬂuence on the model’s\npredictions.\nMore precisely, we select the neurons associated to the largest\nweights in the output layer (in absolute value).\nFigures 3.2 and 3.3 display our results for both datasets. The ﬁrst two rows\nof plots represent the evolution of each cluster’s average pre-activation in the\n10 hidden neurons. The curves are colored according to the clusters’ associ-\nated class. We observe that each neuron consistently diﬀerentiates the clusters\nof one class during training according to a winner-take-most mechanism. The\nclusters with larger average pre-activation are pushed towards even larger pre-\nactivations, while the clusters with smaller average pre-activation are pushed\ntowards even smaller pre-activations. Astonishingly, this unsupervised mech-\nanism can be more impactful than the supervised learning process from the\nperspective of a single neuron. Indeed, neurons sometimes diﬀerentiate clusters\nbelonging to the same class more strongly than clusters from diﬀerent classes.\nThe third row represents the histogram of the ﬁnal pre-activations associ-\nated to each class. It is coherent with the ﬁrst two rows: one class consistently\nexhibits a bimodal distribution, reﬂecting the diﬀerentiation of intraclass clus-\nters. The fourth row visualizes the derivative of the loss with respect to each\nneuron’s activation. More precisely, for each example, we compute the average\nsign of the derivative across all steps of the training process. This value tells us\nwhether an increased activation generally beneﬁts (negative average) or penal-\nizes (positive average) the classiﬁcation of a given example. We observe that the\nsign is correlated with the example’s class across the whole training process: the\nexamples of one class should always be pushed towards larger activations (be-\n3.2. A WINNER-TAKE-MOST MECHANISM\n37\ncause their average derivative sign is −1), and the other to smaller activations\n(because their average derivative sign is 1). We further notice that the winner-\ntake-most mechanism always concerns the class with negative derivatives. We\nexplore in the next section why some of the clusters of this class are pushed\ntowards smaller pre-activations despite being associated to negative derivative\nsigns.\n38\nCHAPTER 3. AN IMPLICIT CLUSTERING MECHANISM\nFigure 3.2: We train one hidden layer MLPs on SynthClust and monitor the pre-activations and the derivative of the loss w.r.t.\nthe activations of 10 hidden neurons after each training iteration and for each training example. The ﬁrst two rows represent\nthe evolution of each cluster’s average pre-activation during training. We observe that neurons consistently diﬀerentiate the\nclusters of one class according to a winner-take-most mechanism: training progressively increases the average pre-activation of\nits most activated clusters and decreases the average pre-activation of its least activated clusters. The third row displays the\nresulting pre-activation distributions at the end of training. The fourth row displays a histogram of the sign of the derivative\nof the loss w.r.t. the neuron activation, averaged over all training steps. This value tells us whether an increased activation\ngenerally beneﬁts (negative average) or penalizes (positive average) the classiﬁcation of the example.\nThe results suggest\nthat the examples of one class should always be pushed towards larger activations, and the other towards smaller activations.\nThis seems contradictory to the observed winner-take-most mechanism. We provide insights on this apparent contradiction in\nSection 3.3.\n3.2. A WINNER-TAKE-MOST MECHANISM\n39\nFigure 3.3: The same visualizations as Figure 3.2, but corresponding to training on our two-class version of the MNIST dataset\n(class 0: 0 →4, class 1: 5 →9). We also observe a winner-take-most mechanism that diﬀerentiates digits (i.e. the original\nMNIST classes) which are associated to the same superclass.\n40\nCHAPTER 3. AN IMPLICIT CLUSTERING MECHANISM\n3.3\nTowards understanding the mechanism\nTo better understand why a winner-take-most mechanism emerges in our ex-\nperiments, we start by performing an ablation study that identiﬁes necessary\ningredients for the phenomenon to occur. We then provide intuitions to explain\nthe phenomenon based on diﬃcult training examples and gradient coherence in\nReLU neurons.\n3.3.1\nAn ablation study\nFigure 3.4 shows the average pre-activation of each cluster across training on\nSynthClust for a model without ReLU activation layer (ﬁrst row), with a single\nhidden neuron (second row) and trained on a less noisy dataset1 (third row).\nFor each scenario, we observe that the winner-take-most mechanism does not\noccur. Hidden neurons behave like the output neuron: they classify the data\naccording to the two classes, without consideration for intraclass clusters. This\nresults in a decrease in performance: the models achieve test accuracies ranging\nfrom 80% to 84%. The necessity of ReLU layers, multiple hidden neurons and\nsuﬃcient noise on the training examples gives rise to the intuitions we describe\nin the next sections.\nFigure 3.4: We display the average pre-activation of each cluster across training\nfor a model without ReLU activation layer (ﬁrst row), with a single hidden\nneuron (second row) or trained on a less noisy dataset (third row). For each\nscenario, we observe that the winner-take-most mechanism does not occur.\n1We apply Gaussian noise with a 0.1 standard deviation instead of 0.4 when generating\nthe data.\n3.3. TOWARDS UNDERSTANDING THE MECHANISM\n41\n3.3.2\nOn the role of diﬃcult training examples\nAt the neuron level, a mysterious force pushes some clusters in the same di-\nrection as clusters of the opposite class. This phenomenon starts around the\n6th iteration (cfr. row 2 of Figure 3.2), and concerns the “losing” clusters of\nthe class subject to the winner-take-most mechanism. At ﬁrst sight, this local\nbehavior seems to be contrary to the global objective, which is to diﬀerentiate\nexamples from their opposite class. In particular, the derivatives associated to\nthese clusters are negative (cfr. row 4 of Figure 3.2), aiming for the opposite\ndirection to where they actually go.\nTo make sense of this apparent contradiction, we suggest considering the\nrole of diﬃcult training examples. Some examples can be diﬃcult to classify\nbecause the associated noise (i) decreases their correlation with their associated\nclass and (ii) increases their correlation with the opposite class.\nTherefore,\nthese examples can lead to gradients that are contradictory with the ones of\nregular examples. At the beginning of training, such a contradictory force is\nnegligible, since these examples constitute exceptions. However, once the more\nregular examples start being correctly classiﬁed, the share of diﬃcult examples\nin the total loss increases, potentially surpassing the regular examples’ share.\nThis would lead regular examples to be pushed in a direction opposite to their\nassociated gradient.\nWe observe these exact dynamics in Figure 3.5. We quantify the correlation\nbetween an example and a class as the scalar product between the example\nand the sum of the 15 cluster centroids associated to the class.\nWe divide\ntraining examples into easy and diﬃcult groups depending on whether they\ncorrelate more with their own class or with the opposite class2. We monitor the\ntotal loss associated to each group during training and observe that (i) the loss\nassociated to the diﬃcult group increases during the ﬁrst iterations, indicating\nthe occurrence of contradictory gradients and (ii) the share associated to the\ndiﬃcult group matches the share of the easy group around the 6th iteration,\nwhich corresponds to the appearance of the winner-take-most mechanism (cfr.\nthe 2nd row of Figure 3.2). The role of diﬃcult training examples is further\nsupported by our ablation study (cfr. Section 3.3.1), which shows that reducing\nthe noise during the data generation process, and thus the amount of diﬃcult\ntraining examples, prevents the winner-take-most mechanism from occurring.\n3.3.3\nOn the role of ReLU\nSince the overall gradient at a given training iteration is the sum of the per-\nexample gradients, the directions that are coherent across multiple training\nexamples are reinforced (as highlighted by Chatterjee (2020)). At the neuron\nlevel, the ReLU activation function aﬀects the coherence of gradients in a very\n2This can be interpreted as whether the training examples would be (in-)correctly classiﬁed\nby a linear classiﬁer.\n42\nCHAPTER 3. AN IMPLICIT CLUSTERING MECHANISM\nFigure 3.5: Left. We represent each class by the sum of its 15 associated cluster\ncentroids, and deﬁne the diﬃculty of an example by its scalar product with the\nrepresentation of the opposite class minus its scalar product with its true class’s\nrepresentation. We divide training examples into easy and diﬃcult categories\ndepending on whether the resulting value is negative or positive. Right. Displays\nthe evolution of the easy and diﬃcult groups’ share in the total loss during\ntraining. We notice that (i) the loss associated to the diﬃcult group increases\nduring the ﬁrst iterations, indicating the occurrence of contradictory gradients\nand (ii) the share associated to the diﬃcult group matches the share of the\neasy group around the 6th iteration, which corresponds to the appearance of\nthe winner-take-most mechanism (cfr. the 2nd row of Figure 3.2).\nspeciﬁc way: since the derivative of the ReLU function is zero for negative\ninputs, the training examples that do not activate the neuron (i.e. have negative\npre-activations) do not contribute to the gradient associated to the neuron’s\nweights. Hence, for a given group of examples that share a common pattern,\nonly the examples that activate the neuron reinforce each other.\nIn each neuron of our single hidden layer MLP, the examples associated\nto each cluster share a common pattern in the input signal (by construction of\nSynthClust) and a common pattern in the backpropagated error signal (because\nthey belong to the same class, cfr. row 4 of Figure 3.2). Hence, the number\nof examples that activate the neuron aﬀects the relative share of each cluster\nin the gradient associated to the weights of the neuron. In particular, the clus-\nters with small average pre-activations will tend to be associated with smaller\nshares than clusters with large average pre-activations. This could explain why\nclusters with smaller average pre-activations are the most impacted by the dif-\nﬁcult training examples (cfr. Section 3.3.2), and hence “lose” the competition.\nOn the contrary, clusters with larger average pre-activations inﬂuence a larger\nshare of the gradient associated to the neuron weights and will be less sensi-\ntive to diﬃcult training examples, enabling them to “win” the competition. The\nidea that ReLU layers are key for diﬀerentiating the “winning” clusters from the\n“losing” ones is also coherent with our ablation study of Section 3.3.1, which\nshows that the winner-take-most mechanism does not occur in a linear model\nwithout ReLU activation layers.\n3.4. CONNECTIONS WITH STANDARD DEEP LEARNING SETTINGS43\n3.3.4\nA divide-and-conquer strategy\nFinally, our ablation study of Section 3.3.1 demonstrates that a neural network\nwith a single hidden neuron does not exhibit a winner-take-most mechanism.\nThis makes sense intuitively: the winner-take-most mechanism locally leads\nto misclassiﬁcation of multiple clusters.\nThis local misclassiﬁcation is possi-\nble only if it is counter-balanced by a correct classiﬁcation in other neurons.\nA divide-and-conquer approach, where diﬀerent neurons focus on the classiﬁ-\ncation of diﬀerent clusters, is perfectly compatible with the winner-take-most\nphenomenon. Indeed, the winning clusters are determined by their initial av-\nerage pre-activation, which varies from one neuron to the other because their\nweights are randomly initialized.\n3.3.5\nWhy does the mechanism aﬀect a single class?\nJointly considering the role of diﬃcult training examples, gradient coherence and\ndivide-and-conquer strategies also shines light on the fact that the winner-take-\nmost mechanism only applies to the class associated to negative derivatives, i.e.,\nwhose activations should increase during training. Indeed, for this class, pushing\na cluster in its “opposite” direction simultaneously leads to a deactivation of\nsome of its examples.\nThe cluster’s share in this neuron’s gradients is thus\ndiminished, further promoting the correct classiﬁcation of the associated diﬃcult\ntraining examples in this speciﬁc neuron. On the contrary, pushing clusters of\nthe class associated to positive derivatives in their opposite direction increases\nthe amount of their examples that activate the neuron, promoting the correct\nclassiﬁcation of these clusters in this speciﬁc neuron.\n3.4\nConnections with standard deep learning set-\ntings\nOur study discloses the emergence of a winner-take-most mechanism and pro-\nvides intuitions and experiments to understand it. But these contributions are\nlimited to relatively simple and shallow neural network architectures trained\non a synthetic dataset. It is not clear whether our empirical observations and\nintuitions still hold in standard deep learning settings. In order to support the\ngenerality of our results, we ﬁrst discuss several works that studied diﬃcult\ntraining examples and gradient coherence in standard deep learning settings,\nhighlighting the connections with the intuitions described in Sections 3.3.2 and\n3.3.3. Second, we demonstrate that our simple setup exhibits many phenom-\nena occurring in standard settings and provide empirical evidence that these\nphenomena are reminiscent of winner-take-most mechanisms.\n44\nCHAPTER 3. AN IMPLICIT CLUSTERING MECHANISM\n3.4.1\nTraining dynamics w.r.t. example diﬃculty\nMultiple works have studied how diﬀerent notions of example diﬃculty related\nto the speed at which examples are learned. Arpit, Jastrzebski, Ballas et al.\n(2017) showed across 100 diﬀerent initializations and permutations of the train-\ning data that many examples are consistently classiﬁed (in)correctly after a\nsingle epoch of training. This observation leads them to conjecture that “deep\nlearning learns simple patterns ﬁrst, before memorizing”. Mangalam and Prabhu\n(2019) provides empirical evidence that deep neural networks learn shallow-\nlearnable examples ﬁrst, where shallow-learnable refers to being correctly clas-\nsiﬁed by non-deep learning approaches. Jiang, Zhang, Talwar et al. (2021a)\ncharacterizes examples by their consistency score, deﬁned by their expected ac-\ncuracy as a held-out example given training sets of varying size.\nFigure 10\nof this paper displays the training curves associated to the training examples\ngrouped by consistency score. It reveals that examples with higher scores are\nlearned before those with lower scores. While this aspect is not discussed in\nthe original paper, Figure 10 also reveals that the accuracy of examples with\na low consistency score decreases in the ﬁrst epochs of training. This suggests\nthat the gradients of low-scoring examples are “contradictory” to the ones of\nhigh-scoring examples.\nIn Section 3.3.2, we deﬁne the diﬃculty of training examples by their corre-\nlation with the opposite class relative to their correlation with their true class.\nIn accordance with the observations conducted in standard settings, we observe\nin Figure 3.3.2 that in our simple setup, (i) easy examples are learned before the\ndiﬃcult ones and (ii) the loss of diﬃcult examples increases in the ﬁrst training\niterations, suggesting the presence of contradictory gradients.\n3.4.2\nThe Coherent Gradient Hypothesis\nChatterjee (2020) recently introduced the Coherent Gradient Hypothesis, which\nstates that gradient coherence plays a crucial role in the generalization abilities\nof deep neural networks. Zielinski, Krishnan, and Chatterjee (2020) provides\nmultiple experiments to support this hypothesis in the context of standard deep\nlearning settings involving the ImageNet dataset and ResNet models with 18\nlayers. These works justify the role of gradient coherence with the following\nintuition: because gradients are the sum of per-example gradients, it is stronger\nin the directions where the per-example gradients are more similar. Hence, the\nchanges to the network during training are biased towards those that simultane-\nously beneﬁt multiple examples. They further argue that such bias is beneﬁcial\nfor generalization based on algorithmic stability theory.\nHowever, the previous intuition only holds at the very beginning of training,\nwhen most examples are misclassiﬁed by the model. As we’ve seen in Section\n3.3.2, a small set of diﬃcult training examples strongly inﬂuence the overall\ngradient once regular examples are correctly classiﬁed. Chatterjee and Zielinski\n3.4. CONNECTIONS WITH STANDARD DEEP LEARNING SETTINGS45\n(2020) provide a more extensive analysis of the evolution of gradient coherence\nduring training. They conclude the following: “our experiments provide addi-\ntional evidence for the connection between the alignment of per-example gradi-\nents and generalization. But as our data shows this connection is complicated.”\nWe believe that the winner-take-most mechanisms disclosed by our work and\nthe intuitions described in Section 3.3.3 oﬀer a promising path towards a better\nunderstanding the relationship between gradient coherence and generalization.\n3.4.3\nThe beneﬁts of data augmentation\nObservations in standard settings. Data augmentation is a key component\nof state-of-the-art models (Cubuk, Zoph, Mane et al., 2019). These techniques\nare motivated by the fact that applying plausible transformations on the training\ndata virtually increases the amount of data available for learning. Surprisingly,\ndata augmentation techniques that apply unrealistic transformations, such as\nCutout (DeVries and Taylor, 2017) and Mixup (Zhang, Cisse, Dauphin et al.,\n2018) appear to be quite eﬀective for regularizing deep neural networks as well.\nObservation in our simple setup and connection with the winner-\ntake-most mechanisms. We trained the single layer MLP on a reduced Syn-\nthClust dataset: 1500 examples are randomly selected for training (instead of\n15000). As expected, training on less data resulted in a decreased test accuracy:\nthe model reaches 80.98% accuracy instead of the 96.1% obtained when training\non the complete dataset. We applied Dropout (Srivastava, Hinton, Krizhevsky\net al., 2014) on the inputs of the network to augment the training data. More\nprecisely, we randomly set input components to 0 with a 50% probability. While\nthis transformation is fundamentally diﬀerent from the Gaussian noise inherent\nto the data generation process, Dropout provided a huge gain in terms of test\naccuracy, reaching 88.57%.\nThe ﬁrst row of Figure 3.6 displays the average pre-activation curves of each\ncluster for the model trained without Dropout. The visualization reveals that\ntraining on a reduced training dataset decreases the strength of the winner-take-\nmost mechanism: clusters from the same class are less diﬀerentiated compared\nto clusters from diﬀerent classes.\nInterestingly, this issue gets mitigated by\nthe application of Dropout, as revealed by the second row of Figure 3.6. This\nobservation suggests that data augmentation techniques improve generalization\nby generating diﬃcult training examples, promoting the occurrence of winner-\ntake-most mechanisms (cfr. Section 3.3.2) and improving the model’s clustering\nabilities (cfr. Chapter 2).\n3.4.4\nThe beneﬁts of pre-training\nObservations in standard settings. Pre-training is a long-standing tech-\nnique in deep learning (Hinton, Osindero, and Teh, 2006; Oquab, Bottou, Laptev\net al., 2014; Donahue, Jia, Vinyals et al., 2014). It consists in training a network\n46\nCHAPTER 3. AN IMPLICIT CLUSTERING MECHANISM\nFigure 3.6: The ﬁrs row displays the average pre-activation curves of each cluster\n(cfr.\n1st line of Figure 3.2) when training the single hidden layer MLP on\na reduced SynthClust dataset (1500 training examples instead of 15000). We\nobserve that the strength of the winner-take-most mechanism decreases: clusters\nfrom the same class are less diﬀerentiated compared to clusters from diﬀerent\nclasses. Rows 2 and 3 provide the same visualization when applying Dropout\nto the inputs of the network (row 2) or when pre-training the network on a\ndiﬀerent dataset exhibiting the same clusters and dataset size as the original\nSynthClust dataset but diﬀerent cluster-class associations (row 3). Both lead\nto stronger diﬀerentiation of intraclass clusters and improved test accuracies.\non a related task for which large amounts of data are available and ﬁne-tuning\nthe resulting model on the target task. It can be interpreted as a parameter\ninitialization strategy for SGD training. Surprisingly, researchers observed em-\npirically that pre-training is very eﬀective, even when the pre-training tasks and\ntarget tasks are quite diﬀerent. For example, contrastive learning techniques use\nunsupervised pretext tasks to pre-train supervised image classiﬁcation networks\nand recently gained a lot of popularity (He, Fan, Wu et al., 2020; Zhao, Wu,\nLau et al., 2021).\nObservation in our simple setup and connection with the winner-\ntake-most mechanisms. We pre-trained the single layer MLP on a dataset\ncontaining the same input data as the original SynthClust dataset -and hence\nthe same clusters and number of training examples-, but diﬀerent, randomly\ngenerated cluster-class associations. We then ﬁne-tuned the model on the re-\nduced SynthClust dataset introduced in Section 3.4.3. Despite both classiﬁca-\ntion tasks being diﬀerent, we observe an improvement in test accuracy compared\nto no pre-training: the model reaches an accuracy of 88.13% instead of 80.98%.\nThe study of the cluster’s average pre-activation curves in Figure 3.6 reveals\nthat because both pre-training and target datasets share the same clustered\n3.4. CONNECTIONS WITH STANDARD DEEP LEARNING SETTINGS47\nstructure, the ﬁne-tuning process beneﬁts from the winner-take-most mecha-\nnisms that occurred during pre-training. Indeed, clusters from the same class\nare already strongly diﬀerentiated at initialization.\n3.4.5\nThe beneﬁts of depth\nObservations in standard settings. State-of-the-art deep learning models\ncontain many hidden layers. In the context of image classiﬁcation, the amount\nof layers typically ranges from 16 to more than a hundred (Simonyan and Zisser-\nman, 2015; He, Zhang, Ren et al., 2016). Many works provide results concerning\nthe beneﬁts of depth in terms of expressivity (e.g., Telgarsky (2016); Eldan and\nShamir (2016); Lin, Tegmark, and Rolnick (2017); Liang and Srikant (2017)).\nIts beneﬁts in terms of generalization ability, however, remain unexplained.\nObservation in our simple setup and connection with the winner-\ntake-most mechanisms. We trained an MLP with 5 hidden layers on Synth-\nClust. Despite the simplicity of the SynthClust dataset, using a deeper neural\nnetwork improved the test accuracy (97.68% test accuracy compared to 96.1%).\nIn order to study the impact of depth on the emergence of winner-take-most\nmechanisms, Figure 3.7 displays the average pre-activation curves of each clus-\nter and the histograms of the average derivative signs (cfr. rows 1 and 4 of\nFigure 3.2) in 5 neurons of each hidden layer3.\nThe visualizations reveal that winner-take-most mechanisms occur in the\nﬁrst three layers of the network as well as in some neurons of the 4th layer.\nInterestingly, the mechanism leads a single cluster to be diﬀerentiated from the\nother examples of the dataset in multiple neurons (e.g., neurons 3 and 4 of\nlayer 2). This change in behavior compared to the single layer MLP studied\nin Figure 3.2 could be induced by the fact that derivative signs correlate less\nwith the examples’ class as they propagate through layers (cfr. the histograms\nof average derivative signs in Figure 3.7). The diﬀerentiation of single clusters\nby hidden neurons could improve the network’s clustering abilities, oﬀering an\ninteresting research direction for explaining the beneﬁts of depth in terms of\ngeneralization.\n3The 5 neurons are selected based on the norm of the neurons’ associated weight vector in\nthe next layer (the larger the norm, the better).\n48\nCHAPTER 3. AN IMPLICIT CLUSTERING MECHANISM\nFigure 3.7: Discussed in section 3.4.5\n3.4. CONNECTIONS WITH STANDARD DEEP LEARNING SETTINGS49\nFigure 3.8: Test accuracies and the average pre-activation curves of each cluster\nwhen training an MLP with 5 hidden layers on SynthClust with large (row 1)\nand small (row 2) learning rates. The average pre-activation curves correspond\nto 1 neuron in each layer, selected based on the norm of its outgoing weights.\nWe observe that (i) large learning rates drastically improve generalization in\nour simple setup and (ii) small learning rates induce negligible changes to the\ncluster’s average pre-activations in the ﬁrst layers and thus no winner-take-most\nmechanisms.\n3.4.6\nThe beneﬁts of large learning rates\nObservations in standard settings. The inﬂuence of SGD’s learning rate on\ngeneralization has been highlighted by many works (Jastrzebski, Kenton, Arpit\net al., 2017; Smith and Le, 2017; Smith and Topin, 2017; Hoﬀer, Hubara, and\nSoudry, 2017; Masters and Luschi, 2018). Empirically, we observe that using\nlarger learning rates beneﬁts generalization -as long as convergence remains\npossible.\nObservation in our simple setup and connection with the winner-\ntake-most mechanisms. We trained the 5-hidden-layers MLP on SynthClust\nwith a learning rate reduced by a factor 27. The test accuracy strongly de-\ncreased: the model reaches an accuracy of 84.07% instead of 97.68%. We display\nthe average pre-activation curves corresponding to training with large and small\nlearning rates in Figure 3.8 for 1 neuron in each layer. As usual, the neuron is se-\nlected based on the norm of the associated weights in the next layer. We observe\nthat when using small learning rates, the cluster’s average pre-activations do not\nchange much at all in the ﬁrst layers, leading to the absence of winner-take-most\nmechanisms. On the contrary, training with large learning rates leads to signiﬁ-\ncant changes in the cluster’s average pre-activations, enabling the emergence of\nwinner-take-most mechanisms.\n50\nCHAPTER 3. AN IMPLICIT CLUSTERING MECHANISM\n3.4.7\nThe beneﬁts of implicit clustering abilities\nObservations in standard settings. In Chapter 2, we show that ﬁve tenta-\ntive measures of intraclass clustering correlate with generalization in standard\ndeep learning settings. These correlations occur across variations of 8 standard\nhyperparameters, amongst which data augmentation, depth and the learning\nrate. Two measures (c1 and c3 deﬁned in Section 2.1) are applied at the neuron-\nlevel, capturing the extent by which examples or subclasses from the same class\nare diﬀerentiated in a neuron’s pre-activations.\nObservation in our simple setup and connection with the winner-\ntake-most mechanisms.\nThe winner-take-most mechanism studied in our\nsimple setup leads to the diﬀerentiation of clusters from the same class in a\nneuron’s pre-activations, and is thus closely related to measures c1 and c3. We\nfurther show that data augmentation, depth and learning rate inﬂuence the\nwinner-take-most mechanism and the test accuracies of the studied MLP net-\nworks in Sections 3.4.3, 3.4.5 and 3.4.6. The observations are consistent with our\nexperiments of Chapter 2: the better the clusters of a class are diﬀerentiated,\nthe better the performance on the test set. Hence, our studies of both implicit\nclustering abilities and mechanisms constitute a coherent framework supporting\nthe crucial role of implicit clustering in deep learning systems.\nChapter 4\nAn implicit clustering\nhyperparameter\nDesigning and training deep learning systems requires manual tuning of many\nhyperparameters (network depth, width, learning rate, weight decay, optimizer,\netc.). Hyperparameter tuning is usually based on successful heuristics and in-\ntuitions that practitioners gain with experience. In practice, this still translates\ninto a lot of trial and error, greatly increasing the time and energy consumption\nassociated to the development of deep learning systems.\nThe opaque behaviour of standard hyperparameters becomes less surpris-\ning when one presumes that implicit mechanisms play a role in deep learning.\nIndeed, these mechanisms could be indirectly aﬀected by the explicit hyperpa-\nrameters in potentially complex and coupled ways. Chapter 2 suggests that 8\nstandard hyperparameters indirectly inﬂuence the implicit clustering abilities of\ndeep learning. Hence, the following chapter looks for implicit hyperparameters\nthat control clustering more directly.\nChapters 2 and 3 suggest the emergence of a neuron-level training process\nthat is critical for implicit clustering to occur. However, the training process of\nthe entire network might succeed without fully accomplishing each individual\nneuron’s training, as suggested by our observations in Section 3.4.6.\nHence,\nthe extent by which each neuron has been “trained” potentially constitutes an\nimplicit clustering hyperparameter which we propose to capture through layer\nrotation, i.e., the evolution across training of the cosine distance between each\nlayer’s ﬂattened weight vector and its initialization. Monitoring the rotation of\nweight vectors is motivated by the fact that batch normalization renders the\nscale of a layer’s transformation obsolete. Monitoring training on a per-layer\nbasis is motivated by the works on vanishing and exploding gradients, which\nsuggest that the training dynamics can vary drastically across the layers of\na network (Bengio, Simard, and Frasconi, 1994; Hochreiter, 1998; Glorot and\n51\n52\nCHAPTER 4. AN IMPLICIT CLUSTERING HYPERPARAMETER\nBengio, 2010).\nWe design tools to monitor and control layer rotations and show across a\ndiverse set of experiments that larger layer rotations (and thus presumably more\naccomplished neuron-level training routines) consistently translate into better\ngeneralization. Moreover, we show that the impact of learning rates, weight de-\ncay, learning rate warmups and adaptive gradient methods on generalization and\ntraining speed seems to result from their indirect inﬂuence on layer rotations.\nFinally, we illustrate on a single hidden layer MLP trained on MNIST that layer\nrotation correlates with the degree to which the features of individual neurons\nhave been learned, connecting our results with our initial hypothesis. An imple-\nmentation of this chapter’s tools and experiments based on Tensorﬂow (Agar-\nwal, Barham, Brevdo et al., 2016) and Keras (Chollet et al., 2015) is available\nat https://github.com/ispgroupucl/layer-rotation-tools and https://\ngithub.com/ispgroupucl/layer-rotation-paper-experiments respectively.\n4.1\nTools for monitoring and controlling layer\nrotation\nThis section describes the tools for monitoring and controlling layer rotation\nduring training, such that its relationship with generalization, training speed\nand explicit hyperparameters can be studied in Sections 4.3 and 4.4.\n4.1.1\nMonitoring layer rotation with layer rotation curves\nLayer rotation is deﬁned as the evolution of the cosine distance between each\nlayer’s weight vector and its initialization during training. The cosine distance\nis deﬁned as:\nd(u, v) = 1 −\nu · v\n∥u ∥2∥v ∥2\n.\n(4.1)\nLet wt\nl be the ﬂattened weight tensor of the lth layer at optimization step t (t0\ncorresponding to initialization), then the rotation of layer l at training step t\nis deﬁned as d(wt0\nl , wt\nl)1. In order to visualize the evolution of layer rotation\nduring training, we plot how the cosine distance between each layer’s current\nweight vector and its initialization evolves across training steps. We denote this\nvisualization tool by layer rotation curves hereafter.\n1It is worth noting that our study focuses on weights that multiply the inputs of a layer\n(e.g. kernels of fully connected and convolutional layers). Studying the training of additive\nweights (biases) is left as future work.\n4.1. TOOLS FOR MONITORING AND CONTROLLING LAYER ROTATION53\n4.1.2\nControlling layer rotation with Layca\nThe ability to control layer rotations during training would enable a systematic\nstudy of their relationship with generalization and training speed. Therefore, we\npresent Layca (LAYer-level Controlled Amount of weight rotation), an algorithm\nwhere the layer-wise learning rates directly determine the amount of rotation\nperformed by each layer’s weight vector during each training step (the layer\nrotation rates), in a direction speciﬁed by an optimizer (SGD being the default\nchoice). Inspired by techniques for optimization on manifolds (Absil, Mahony,\nand Sepulchre, 2010), and on spheres in particular, Layca applies layer-wise\northogonal projection and normalization operations on SGD’s updates, as de-\ntailed in Algorithm 1. These operations induce the following simple relationship\nbetween the learning rate ρl(t) of layer l at training step t and the angle θl(t)\nbetween wt\nl and wt−1\nl\n: ρl(t) = tan(θl(t)).\nOur controlling tool is based on a strong assumption: that controlling the\namount of rotation performed during each individual training step (i.e. the layer\nrotation rate) enables control of the cumulative amount of rotation performed\nsince the start of training (i.e. layer rotation). This assumption is not trivial\nsince the aggregated rotation is a priori very dependent on the shape of the loss\nlandscape. For example, for an identical layer rotation rate, the layer rotation\nwill be much smaller if iterates oscillate around a minimum instead of following\na stable downward slope. Our assumption however appeared to be suﬃciently\nvalid, and the control of layer rotation was eﬀective in our experiments.\n54\nCHAPTER 4. AN IMPLICIT CLUSTERING HYPERPARAMETER\nAlgorithm 1 Layca, an algorithm that enables control over the amount of\nweight rotation per step for each layer through its learning rate parameter (cfr.\nSection 4.1.2).\nRequire: o, an optimizer (SGD is the default choice)\nRequire: T, the number of training steps\nL is the number of layers in the network\nfor l = 0 to L −1 do\nRequire: ρl(t), a layer’s learning rate schedule\nRequire: wl\n0, the initial multiplicative weights of layer l\nend for\nfor t = 0 to T do\ns0\nt, ..., sL−1\nt\n= getStep(o, w0\nt , ..., wL−1\nt\n)\n(get the updates of the selected\noptimizer)\nfor l = 0 to L −1 do\nsl\nt ←sl\nt −(sl\nt·wl\nt)wl\nt\nwl\nt·wl\nt\n(project step on space orthogonal to wl\nt)\nsl\nt ←sl\nt∥wl\nt∥2\n∥sl\nt∥2\n(rotation-based normalization)\nwl\nt+1 ←wl\nt + ρl(t)sl\nt\n(perform update)\nwl\nt+1 ←wl\nt+1\n∥wl\n0∥2\n∥wl\nt+1∥2\n(project weights back on sphere)\nend for\nend for\n4.2\nExperimental setup\nThe experiments are conducted on ﬁve diﬀerent tasks which vary in network\narchitecture and dataset complexity, and are further described in Table 4.1.\nTable 4.1: Summary of the tasks used for our experiments2\nName\nArchitecture\nDataset\nC10-CNN1\nVGG-style 25 layers deep CNN\nCIFAR-10\nC100-resnet\nResNet-32\nCIFAR-100\ntiny-CNN\nVGG-style 11 layers deep CNN\nTiny ImageNet\nC10-CNN2\ndeep CNN from torch blog\nCIFAR-10 + data augm.\nC100-WRN\nWide ResNet 28-10 with 0.3 dropout\nCIFAR-100 + data augm.\nWe use the same amount of training epochs and the same learning rate decay\nscheme across layer rotation and explicit hyperparameter conﬁgurations:\n2References: VGG (Simonyan and Zisserman, 2015), ResNet (He, Zhang, Ren et al., 2016),\ntorch blog (Zagoruyko, 2015), Wide ResNet (Zagoruyko and Komodakis, 2016), CIFAR-10\n(Krizhevsky and Hinton, 2009), Tiny ImageNet (Deng, Dong, Socher et al., 2009; CS231N,\n2016). Dropout layers were removed from the torch blog CNN to enable perfect classiﬁcation\non the training set (100% accuracy).\n4.2. EXPERIMENTAL SETUP\n55\n• C10-CNN1: 100 epochs and a reduction of the learning rate by a factor 5\nat epochs 80, 90 and 97\n• C100-resnet: 100 epochs and a reduction of the learning rate by a factor\n10 at epochs 70, 90 and 97\n• tiny-CNN: 80 epochs and a reduction of the learning rate by a factor 5 at\nepoch 70\n• C10-CNN2: 250 epochs and a reduction of the learning rate by a factor 5\nat epochs 100, 170, 220\n• C100-WRN: 250 epochs and a reduction of the learning rate by a factor 5\nat epochs 100, 170, 220\nThe only exceptions are C10-CNN2 and C100-WRN trained with SGD+weight\ndecay and with adaptive methods (cfr.\nSections 4.4.2 and 4.4.4), where the\nlearning rate decay schemes are the ones used in their original implementation\nor in (Wilson, Roelofs, Stern et al., 2017) respectively. Training accuracy is\nclose to optimal in most cases, as revealed by the Tables 4.2, 4.3, 4.4, 4.5 and\n4.6.\nTable 4.2: Train accuracies of models used in Figure 4.1\nα = 0.6\nα = −0.6\nρ(0) = 3−5\nρ(0) = 3−4\nBest\nC10-CNN1\n100%\n99.99%\n100%\n100%\n99.99%\nC100-resnet\n82.09%\n99.54%\n99.87%\n99.99%\n99.75%\ntiny-CNN\n99.98%\n99.95%\n99.97%\n99.97%\n98.91%\nC10-CNN2\n100%\n99.94%\n99.99%\n99.99%\n99.97%\nC100-WRN\n99.88%\n99.91%\n99.97%\n99.99%\n99.96%\nTable 4.3: Train accuracies of models used in Figure 4.3\nlr = 3−4\nlr = 3−3\nlr = 3−2\nlr = 3−1\nlr = 30\nC10-CNN1\n100%\n100%\n100%\n100%\n100%\nC100-resnet\n87.8%\n100%\n100%\n100%\n99.7%\ntiny-CNN\n100%\n100%\n100%\n100%\n100%\nC10-CNN2\n99.8%\n99.9%\n100%\n100%\n83.7%\nC100-WRN\n100%\n100%\n100%\n100%\n57.4%\n56\nCHAPTER 4. AN IMPLICIT CLUSTERING HYPERPARAMETER\nTable 4.4: Train accuracies of models used in Figure 4.5\nC10-CNN1\nC100-resnet\ntiny-CNN\nC10-CNN2\nC100-WRN\nSGD + L2\n100%\n100%\n100%\n100%\n100%\nTable 4.5: Train accuracies of models used in Figure 4.6\nNo warmup\n5 epochs\n10 epochs\n15 epochs\nLayca-No warmup\n96.67%\n99.76%\n99.85%\n99.68%\n99.85%\nTable 4.6: Train accuracies of models used in Figure 4.8\nC10-CNN1\nC100-resnet\ntiny-CNN\nC10-CNN2\nC100-WRN\nAdaptive methods\n100%\n100%\n100%\n100%\n99.9%\nAdaptive + Layca\n100%\n99.7%\n99.2%\n100%\n100%\n4.3\nA systematic study of layer rotation conﬁg-\nurations\nSection 4.1 provides tools to monitor and control layer rotation. The purpose\nof this section is to use these tools to conduct a systematic experimental study\nof layer rotation conﬁgurations. We adopt SGD as default optimizer, but use\nLayca (cfr. Algorithm 1) to vary the relative rotation rates (faster rotation for\nﬁrst layers, last layers, or no prioritization) and the global rotation rate value\n(high or low rate, for all layers).\n4.3.1\nLayer rotation rate conﬁgurations\nLayca enables us to specify layer rotation rate conﬁgurations, i.e. the amount of\nrotation performed by each layer’s weight vector during one optimization step,\nby setting the layer-wise learning rates. To explore the large space of possible\nlayer rotation rate conﬁgurations, our study restricts itself to two directions of\nvariation. First, we vary the initial global learning rate ρ(0), which aﬀects the\nlayer rotation rate of all the layers. During training, the global learning rate\nρ(t) drops following a ﬁxed decay scheme, hence the dependence on t.\nThe\nsecond direction of variation tunes the relative rotation rates between diﬀerent\nlayers. More precisely, we apply static, layer-wise learning rate multipliers that\nexponentially increase/decrease with layer depth (which is typically encountered\nwith exploding/vanishing gradients, cfr. Bengio, Simard, and Frasconi (1994);\nHochreiter (1998); Glorot and Bengio (2010)). The multipliers are parametrized\nby the layer index l (in forward pass ordering) and a parameter α ∈[−1, 1] such\n4.3. A SYSTEMATIC STUDY OF LAYER ROTATION CONFIGURATIONS57\nthat the learning rate of layer l becomes:\nρl(t) =\n(\n(1 −α)5 (L−1−l)\nL−1\nρ(t)\nif\nα > 0\n(1 + α)5\nl\nL−1 ρ(t)\nif\nα ≤0\n(4.2)\nValues of α close to −1 correspond to faster rotation of ﬁrst layers, 0 corresponds\nto uniform rotation rates, and values close to 1 to faster rotation of last layers.\n4.3.2\nLayer rotation’s relationship with generalization\nFigure 4.1 depicts the layer rotation curves (cfr. Section 4.1.1) and the corre-\nsponding test accuracies obtained with diﬀerent layer rotation rate conﬁgura-\ntions. While each conﬁguration achieves ≈100% training accuracy (cfr. Section\n4.2), we observe huge diﬀerences in generalization ability (diﬀerences of up to\n30% test accuracy). These diﬀerences seem to be tightly connected to diﬀer-\nences in layer rotations. In particular, we extract the following rule of thumb\nthat is applicable across the ﬁve considered tasks: the larger the layer rotations,\nthe better the generalization performance. The best performance is consistently\nobtained when nearly all layers reach a cosine distance of 1, which corresponds\nto ﬁnal weights that are orthogonal to their initialization (cfr. ﬁfth column of\nFigure 4.1). This observation would have limited value if many conﬁgurations\n(amongst which the best one) lead to cosine distances of 1. However, we no-\ntice that most conﬁgurations do not. In particular, rotating the layers weights\nvery slightly is suﬃcient for the network to achieve 100% training accuracy (cfr.\nthird column of Figure 4.1).\nWe also observe that layer rotation rates (rotation with respect to the previ-\nous iterate) translate remarkably well into layer rotations (rotation with respect\nto the initialization). For example, the α = 0 setting used in the ﬁfth column\nindeed leads all layers to rotate quasi synchronously. As discussed in Section\n4.1.2, this is not self-evident. Understanding why this happens (and why the\nﬁrst and last layers seem to be less tameable) is an interesting direction of\nresearch resulting from our work.\n4.3.3\nLayer rotation’s relationship with training speed\nWhile generalization is the main focus of our work, we observed through our\nexperiments that layer rotation rates also inﬂuenced the training speed of our\nmodels in a remarkable way. Figure 4.2 depicts the loss curves obtained for\ndiﬀerent values of α and ρ(0) on the ﬁrst three tasks of Table 4.1. It appears\nthat the larger or the more uniform the layer rotation rates are, the higher the\nplateaus in which loss curves get stuck into. The plateaus might be due to a\nform of interference between the diﬀerent neurons’ training processes. The pre-\ncision with which the height of plateaus can be controlled through the α and\nρ(0) parameters is striking and further supports the idea that layer rotation\n58\nCHAPTER 4. AN IMPLICIT CLUSTERING HYPERPARAMETER\nFigure 4.1: Analysis of the layer rotation curves (cfr. Section 4.1.1) and test ac-\ncuracies (η) induced by diﬀerent layer rotation rate conﬁgurations (using Layca\nfor training) on the ﬁve tasks of Table 4.1. The conﬁgurations are parametrized\nby α, that controls which layers have the highest rotation rates (ﬁrst layers for\nα < 0, last layers for α > 0, or no prioritization for α = 0), and ρ(0), the initial\nglobal rotation rate value shared by all layers. ∆η is computed with respect to\nthe best conﬁguration (last column), which corresponds to α = 0 and ρ(0) = 3−3\nfor the ﬁve tasks. This visualization unveils large diﬀerences in generalization\nability across conﬁgurations which seem to follow a simple yet consistent rule of\nthumb: the larger the layer rotation for each layer, the better the generalization\nperformance. Training accuracies are provided in Section 4.2 (≈100% in all\nconﬁgurations).\ncontrols a fundamental yet implicit mechanism in deep learning. Following our\nrule of thumb, this result also suggests that high plateaus are additional indica-\ntors of good generalization performance. This is consistent with the systematic\noccurrence of high plateaus in the loss curves of state of the art networks (e.g.,\nHe, Zhang, Ren et al. (2016); Zagoruyko and Komodakis (2016)).\n4.4. A STUDY OF LAYER ROTATION IN STANDARD TRAINING SETTINGS59\n0.0\n0.5\n1.0\n1.5\n2.0\nTraining loss\nC10-CNN1\nα = -0.0\nα = -0.1\nα = -0.2\nα = -0.3\nα = -0.4\nα = -0.6\nα = 0.0\nα = 0.1\nα = 0.2\nα = 0.3\nα = 0.4\nα = 0.6\nρ(0)=3−3.0\nρ(0)=3−3.2\nρ(0)=3−3.4\nρ(0)=3−3.6\nρ(0)=3−3.8\nρ(0)=3−4.0\n0\n2\n4\nTraining loss\nC100-resnet\n0\n20\n40\n60\n80\n100\nEpoch\n0\n2\n4\nTraining loss\ntiny-CNN\n0\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nEpoch\nFigure 4.2: Loss curves obtained for diﬀerent α and ρ(0) values on the ﬁrst three\ntasks of Table 4.1, using Layca for training. The α and ρ(0) conﬁgurations are\nspeciﬁed for each column in the associated legend. The visualizations unveil\na remarkable phenomenon: the more uniform or the larger the layer rotation\nrates, the higher the plateaus in which the loss gets stuck into. The sudden\ndrops correspond to a reduction of the global learning rate as speciﬁed in 4.2.\n4.4\nA study of layer rotation in standard train-\ning settings\nSection 4.3 uses Layca to study the relation between layer rotations and gen-\neralization or training speed in a controlled setting. This section investigates\nthe layer rotation conﬁgurations that naturally emerge when using SGD, weight\ndecay or adaptive gradient methods for training. First of all, these experiments\nwill provide supplementary evidence for the rule of thumb proposed in Section\n4.3. Second, we’ll see that studying training methods from the perspective of\nlayer rotation can provide useful insights to explain their behaviour.\nThe experiments are performed on the ﬁve tasks of Table 4.1. The learn-\ning rate parameter is tuned independently for each training setting through\ngrid search over 10 logarithmically spaced values (3−7, 3−6, ..., 32), except for\nC10-CNN2 and C100-WRN where learning rates are taken from their original\nimplementations when using SGD + weight decay, and from (Wilson, Roelofs,\nStern et al., 2017) when using adaptive gradient methods for training.\nThe\ntest accuracies obtained in standard settings will often be compared to the best\nresults obtained with Layca, which are provided in the 5th column of Figure\n4.1.\n60\nCHAPTER 4. AN IMPLICIT CLUSTERING HYPERPARAMETER\n4.4.1\nAnalysis of SGD’s learning rate\nThe inﬂuence of SGD’s learning rate on generalization has been highlighted\nby several works (Jastrzebski, Kenton, Arpit et al., 2017; Smith and Le, 2017;\nSmith and Topin, 2017; Hoﬀer, Hubara, and Soudry, 2017; Masters and Luschi,\n2018). The learning rate parameter directly aﬀects layer rotation rates, since it\nchanges the size of the updates. In this section, we verify if the learning rate’s\nimpact on generalization is coherent with our rules of thumb.\nFigure 4.3 displays the layer rotation curves and test accuracies generated\nby diﬀerent learning rate conﬁgurations during vanilla SGD training on the\nﬁve tasks of table 4.1. We observe that test accuracy increases for larger layer\nrotations (consistent with our rule of thumb) until a tipping point where it\nstarts to decrease (inconsistent with our rule of thumb). We show in Figure 4.4\nthat these problematic cases involve extremely abrupt layer rotations that do not\ntranslate in improvements of the training loss. These observations thus highlight\nan important condition for our rules of thumb to hold true: the monitored layer\nrotations should coincide with actual training (i.e. improvements on the loss\nfunction).\nA second interesting observation is that the layer rotation curves obtained\nwith vanilla SGD are far from the ideal scenario disclosed in Section 4.3, where\nthe majority of the layers’ weights reached a cosine distance of 1 from their\ninitialization. In accordance with our rules of thumb, SGD also reaches con-\nsiderably lower test performances than Layca. A more extensive tuning of the\nlearning rate (over 10 logarithmically spaced values) did not help SGD to solve\nits two systematic problems: 1) layer rotations are not uniform and 2) the layers’\nweights stop rotating before reaching a cosine distance of 1.\n4.4.2\nAnalysis of SGD and weight decay\nSeveral papers have recently shown that, in batch normalized networks, the\nregularization eﬀect of weight decay was caused by an increase of the eﬀective\nlearning rate (van Laarhoven, 2017; Hoﬀer, Banner, Golan et al., 2018; Zhang,\nWang, Xu et al., 2019). More generally, reducing the norm of weights increases\nthe amount of rotation induced by a given training step. It is thus interesting to\nsee how weight decay aﬀects layer rotations, and if its impact on generalization\nis coherent with our rule of thumb. Figure 4.5 displays, for the 5 tasks, the\nlayer rotation curves generated by SGD when combined with weight decay (in\nthis case, equivalent to L2-regularization). We observe that weight decay solves\nSGD’s problems ( cfr. Section 4.4.1): all layers’ weights reach a cosine distance\nof 1 from their initialization, and the resulting test performances are on par\nwith the ones obtained with Layca.\nThis experiment not only provides important supplementary evidence for\nour rules of thumb, but also novel insights around weight decay’s regularization\nability in deep nets: weight decay seems to be key for enabling large layer\n4.4. A STUDY OF LAYER ROTATION IN STANDARD TRAINING SETTINGS61\nFigure 4.3: Layer rotation curves and the corresponding test accuracies gener-\nated by vanilla SGD with diﬀerent learning rates. Colour code, axes and ∆η\ncomputation are the same as in Figure 4.1. The inﬂuence of the learning rate\nparameter on generalization is consistent with our rule of thumb (larger layer\nrotations →better generalization), except for cases with abrupt layer rotations.\nWe further show in Figure 4.4 that these abrupt layer rotations do not translate\nin improvements of the loss. Moreover, despite extensive learning rate tuning,\nSGD induces test performances that are signiﬁcantly below Layca’s optimal con-\nﬁguration (cfr. 5th column of Figure 4.1). This is also in accordance with our\nrules of thumb, since SGD does not seem to be able to generate layer rotations\nthat reach a cosine distance of 1.\nrotations (weights reaching a cosine distance of 1 from their initialization) during\nSGD training. Since the same behaviour can be achieved with tools that control\nlayer rotation rates (cfr. Layca), without an extra parameter to tune, our results\ncould potentially lead weight decay to disappear from the standard deep learning\ntoolkit.\n62\nCHAPTER 4. AN IMPLICIT CLUSTERING HYPERPARAMETER\nFigure 4.4: Layer rotation and training curves during the ﬁrst epoch of SGD\ntraining with high learning rates (cfr. Figure 4.3). The visualization reveals\nlarge layer rotations that are sometimes performed in a single iteration. Im-\nportantly, these iterations do not induce improvements in training accuracy,\nwhich probably explains why these conﬁgurations escape the scope of our rule\nof thumb.\nFigure 4.5: Layer rotation curves and the corresponding test accuracies gener-\nated by SGD with weight decay. Colour code, axes and ∆η computation are\nthe same as in Figure 4.1. The application of weight decay during SGD train-\ning enables layer rotations that reach a cosine distance of 1 and leads to test\nperformances comparable to Layca’s optimal conﬁguration (cfr. 5th column of\nFigure 4.1). These results thus provide supplementary evidence for our rule of\nthumb and a new perspective on weight decay regularization.\n4.4.3\nAnalysis of learning rate warmups\nWe’ve seen in Section 4.4.1 that during SGD training, high learning rates could\ngenerate abrupt layer rotations at the very beginning of training that do not\nimprove the training loss. In this section, we investigate if these unstable layer\nrotations could be the reason why learning rate warmups are sometimes neces-\nsary when using high learning rates He, Zhang, Ren et al. (2016); Goyal, Dollar,\nGirshick et al. (2018). For this experiment, we use a deeper network that noto-\n4.4. A STUDY OF LAYER ROTATION IN STANDARD TRAINING SETTINGS63\nriously requires warmups for training: ResNet-110 He, Zhang, Ren et al. (2016).\nThe network is trained on the CIFAR-10 dataset with standard data augmen-\ntation techniques. We use a warmup strategy that starts at a 10 times smaller\nlearning rate and linearly increases to reach the ﬁnal learning rate in a speciﬁed\nnumber of epochs.\nFigure 4.6 displays the layer rotation and training curves when training with\na high learning rate (3−1) and diﬀerent warmup durations (0,5,10 or 15 epochs\nof warmup). We observe that without warmup, SGD generates unstable layer\nrotations and training accuracy doesn’t improve before the 25th epoch. Using\nwarmups brings signiﬁcant improvements: a 75% training accuracy is reached\nafter 25 epochs, with only some instabilities in the training curves -that again\nare synchronized with abrupt layer rotations. Finally, we also use Layca for\ntraining (with a 3−3 learning rate).\nWe observe that it doesn’t suﬀer from\nSGD’s instabilities in terms of layer rotation. Hence, Layca achieves large layer\nrotations and good generalization performance without the need for warmups.\nFigure 4.6: Layer rotation and training curves obtained when using diﬀerent\nwarmup durations (0,5,10 or 15 epochs) during the training of ResNet-110 on\nCIFAR-10 with high learning rates (3−1). The curves are shown for the 25 ﬁrst\nepochs only -out of 200. η is the ﬁnal test accuracy. We observe that SGD\ngenerates unstable layer rotations that translate into a stagnation or a decrease\nof the training accuracy. Using warmups drastically reduces these instabilities.\nLayca doesn’t generate instabilities and reaches high generalization performance\n(and large layer rotations) without the need for warmups.\n4.4.4\nAnalysis of adaptive gradient methods\nThe recent years have seen the rise of adaptive gradient methods in the context\nof machine learning (e.g.\nRMSProp (Tieleman and Hinton, 2012), Adagrad\n64\nCHAPTER 4. AN IMPLICIT CLUSTERING HYPERPARAMETER\n(Duchi, Hazan, and Singer, 2011), Adam (Kingma and Ba, 2015)). The key\nelement distinguishing adaptive gradient methods from their non-adaptative\nequivalents is a parameter-level tuning of the learning rate based on the statis-\ntics of each parameter’s partial derivative. Initially introduced for improving\ntraining speed, (Wilson, Roelofs, Stern et al., 2017) observed that these methods\nalso had a considerable impact on generalization. Since these methods aﬀect\nthe rate at which individual parameters change, they might also inﬂuence the\nrate at which layers change (and thus layer rotations).\nWe ﬁrst observe to what extent the parameter-level learning rates of Adam\nvary across layers. We monitor Adam’s estimate of the second raw moment,\nwhich is used for determining the parameter-level learning rates, when training\non the C10-CNN1 task. The estimate is computed by:\nvt = β2 · vt−1 + (1 −β2) · g2\nt\nwhere gt and vt are vectors containing respectively the gradient and the esti-\nmates of the second raw moment at training step t, and β2 is a parameter spec-\nifying the decay rate of the moment estimate. While our experiment focuses\non Adam, the other adaptive methods (RMSprop, Adagrad) also use statistics\nof the squared gradients to compute parameter-level learning rates. Figure 4.7\ndisplays the 10th, 50th and 90th percentiles of the moment estimations, for each\nlayer separately, as measured at the end of epochs 1, 10 and 50. The conclusion\nis clear: the estimates vary much more across layers than inside layers. This\nsuggests that adaptive gradient methods might have a drastic impact on layer\nrotations.\nFigure 4.7: Adam’s parameter-wise estimates of the second raw moment (un-\ncentered variance) of the gradient during training on C10-CNN1, represented for\neach layer separately through their 10th, 50th and 90th percentiles. The results\nprovide evidence that the parameter-level statistics used by adaptive gradient\nmethods vary mostly between layers and negligibly inside layers.\nAdaptive gradient methods can reach SGD’s generalization ability\nwith Layca\nSince adaptive gradient methods probably aﬀect layer rotations, we will verify\nif their inﬂuence on generalization is coherent with our rule of thumb. Figure\n4.4. A STUDY OF LAYER ROTATION IN STANDARD TRAINING SETTINGS65\n4.8 (1st line) provides the layer rotation curves and test accuracies obtained\nwhen using adaptive gradient methods to train the 5 tasks described in Table\n4.1. We observe an overall worse generalization ability compared to Layca’s\noptimal conﬁguration and small and/or non-uniform layer rotations. We also\nobserve that the layer rotations of adaptive gradient methods are considerably\ndiﬀerent from the ones induced by SGD (cfr. Figure 4.3). For example, adaptive\ngradient methods seem to induce larger rotations of the last layers’ weights,\nwhile SGD usually favors rotation of the ﬁrst layers’ weights.\nCould these\ndiﬀerences explain the impact of parameter-level adaptivity on generalization?\nIn Figure 4.8 (2nd line), we show that when Layca is used on top of adaptive\nmethods (to control layer rotation), adaptive methods can reach test accuracies\non par with SGD + weight decay. Our observations thus suggest that adaptive\ngradient methods’ poor generalization properties are due to their impact on\nlayer rotations. Moreover, the results again provide supplementary evidence for\nour rule of thumb.\nFigure 4.8: Layer rotation curves and the corresponding test accuracies gener-\nated by adaptive gradient methods (RMSProp, Adam, Adagrad, RMSProp+L2\nand Adam+L2 respectively for each task/column) without (1st line) and with\n(2nd line) control of layer rotation with Layca.\nColour code, axes and ∆η\ncomputation are the same as in Figure 4.1.\nIn the ﬁrst line, we observe an\noverall worse generalization ability compared to Layca’s optimal conﬁguration\n(cfr. 5th column of Figure 4.1) -despite extensive learning rate tuning, together\nwith small and/or non-uniform layer rotations (in accordance with our rule of\nthumb). When Layca is used on top of adaptive methods to control layer rota-\ntion (second line), adaptive methods can reach test accuracies on par with SGD\n+ weight decay.\n66\nCHAPTER 4. AN IMPLICIT CLUSTERING HYPERPARAMETER\nSGD can achieve adaptive gradient methods’ training speed with\nLayca\nWe’ve seen that the negative impact of adaptive gradient methods on generaliza-\ntion was largely due to their inﬂuence on layer rotations. Could layer rotations\nalso explain their positive impact on training speed? To test this hypothesis, we\nrecorded the layer rotation rates emerging from training with adaptive gradient\nmethods, and reproduced them during SGD training with the help of Layca. We\nthen observe if this SGD-Layca optimization procedure (that doesn’t perform\nparameter-level adaptivity) could achieve the improved training speed of adap-\ntive gradient methods. Figure 4.9 shows the training curves during training of\nthe 5 tasks of Table 4.1 with adaptive gradient methods, SGD+weight decay and\nSGD-Layca-AdaptCopy (which copies the layer rotation rates of adaptive gradi-\nent methods). While adaptive gradient methods train signiﬁcantly faster than\nSGD+weight decay, we observe that their training curves are nearly indistin-\nguishable from SGD-Layca-AdaptCopy. Our study thus suggests that adaptive\ngradient methods impact on both generalization and training speed is due to\ntheir inﬂuence on layer rotations.\nFigure 4.9: Training curves for the 5 tasks of Table 4.1 with adaptive gradient\nmethods (RMSProp, Adam, Adagrad, RMSProp+L2 and Adam+L2 respec-\ntively for each task/column), SGD+weight decay and SGD-Layca-AdaptCopy.\nDuring training with SGD-Layca-AdaptCopy, Layca is used to reproduce the\nlayer rotations generated by an adaptive gradient method on the same task3.\nWe observe that this training procedure (which doesn’t perform parameter-level\nadaptivity) achieves the same improvements in training speed as adaptive gra-\ndient methods.\n4.5\nRelated work\nThe idea that neurons from diﬀerent layers potentially train at diﬀerent rates\nwas motivated by the works on vanishing and exploding gradients (Bengio,\n3When copying the layer rotations of Adam with SGD-Layca-AdaptCopy we also integrate\nAdam’s momentum scheme.\n4.6. ON THE INTERPRETATION OF LAYER ROTATIONS\n67\nSimard, and Frasconi, 1994; Hochreiter, 1998; Glorot and Bengio, 2010). These\npioneering works revealed that the norm of gradients is aﬀected by its propaga-\ntion through the layers, potentially leading to training diﬃculties. Based on this\nobservation, several other works also designed and studied tools for controlling\ndeep neural network training on a per-layer basis (Yu, Lin, Salakhutdinov et al.,\n2017; Ginsburg, Gitman, and You, 2018; Bernstein, Vahdat, Yue et al., 2020).\nWhile these don’t conduct a systematic study of layer-level training’s relation-\nship with generalization or training speed, they show that layer-level control\nleads to more stable training, reduced hyperparameter tuning and ultimately\nbetter generalization performance. Interestingly, (Liu, Bernstein, Meister et al.,\n2021) recently showed that controlling rotation of weight vectors at the neuron-\nlevel could improve a network’s performance even further. This indicates that\ntraining diﬀerences can also occur amongst the neurons of a layer, and further\nsupports our initial hypothesis which states that emergent neuron-level training\nprocesses play a crucial role in deep neural networks.\n4.6\nOn the interpretation of layer rotations\nThe previous sections of this chapter demonstrate the remarkable consistency\nand explanatory power of layer rotation’s relationship with generalization. This\nsuggests that layer rotation controls fundamental aspects of deep neural network\ntraining. Whether these aspects relate to the clustering abilities and mechanisms\nstudied in Chapters 2 and 3 relies on the hypothesis that layer rotation captures\nthe extent by which hidden neurons have been able to “train” during the net-\nwork’s training process. In this section, we provide an additional experiment to\nsupport the link between all these concepts.\nWe use a toy experiment to visualize how layer rotation aﬀects the features\nlearned by hidden neurons. We train a single hidden layer MLP (with 784 hidden\nneurons) on a reduced MNIST dataset (1000 samples per class, to increase over-\nparameterization). This toy network has the advantage of having intermediate\nfeatures that are easily visualized: the weights associated to hidden neurons live\nin the same space as the input images. Starting from an identical initialization,\nwe train the network with four diﬀerent learning rates using Layca, leading to\nfour diﬀerent layer rotation conﬁgurations that all reach 100% training accuracy\nbut diﬀerent generalization abilities (in accordance with our rule of thumb).\nFigure 4.10 displays the features obtained by the diﬀerent layer rotation con-\nﬁgurations (for 5 randomly selected hidden neurons). This visualization unveils\nan important phenomenon: layer rotation does not seem to aﬀect which\nfeatures are learned, but rather to what extent they are learned dur-\ning the training process. The larger the layer rotation, the more prominent\nthe features -and the less retrievable the initialization. Ultimately, for a layer\nrotation close to 1, the ﬁnal weights of the network got rid of all remnants of the\ninitialization. This experiment thus supports our initial hypothesis: fully accom-\n68\nCHAPTER 4. AN IMPLICIT CLUSTERING HYPERPARAMETER\nplishing emergent neuron-level training processes is not necessary for reaching\n100% training accuracy, but doing so anyway leads to better clustering and\ngeneralization abilities.\nFigure 4.10: A single hidden layer MLP is trained on a reduced MNIST dataset\n(1000 examples per class). Starting from an identical initialization, the network\nis trained with four diﬀerent learning rates using Layca, leading to four diﬀerent\nlayer rotation conﬁgurations that all reach 100% training accuracy but diﬀerent\ngeneralization abilities (in accordance with our rule of thumb). The learned\nintermediate features (associated to 5 randomly selected neurons) are visualized\nfor the diﬀerent layer rotation conﬁgurations. The results suggest that layer\nrotation does not aﬀect which features are learned, but rather to what extent\nthey are learned during the training process.\nChapter 5\nDiscussion and perspectives\nOur work argues that the natural clustering prior plays a key role in deep learn-\ning and generalization. Empirical evidence supporting this hypothesis has been\npresented in Chapters 2,3 and 4. This chapter takes a step back and attempts\nto see the bigger picture behind our work. More precisely, we (i) discuss to what\nextent this work eﬀectively validates our hypothesis (Section 5.1), (ii) predict a\nrebirth of clustering algorithms for training deep neural networks (Section 5.2)\nand (iii) discuss the societal impact of deep learning research (Section 5.3).\n5.1\nTowards validating our hypothesis\nIn order to validate our hypothesis, this work provides a collection of intuitions\nand experiments. In this section, we detail how future work could improve the\ngenerality of our experiments, provide complementary empirical evidence for\nour hypothesis and incorporate mathematical formalisms.\nGeneralizing our experiments\nWe ask machine learning models to be able to generalize to a large variety of\ncontexts. So should the empirical claims that support a theory. The experiments\npresented in this work involve a restricted set of deep learning setups. Using\nother datasets, architectures, training algorithms and hyperparameters could\nlead to diﬀerent or contradicting conclusions. This limitation is particularly\nstrong in Chapter 3, which studies implicit clustering mechanisms in a setup\ninvolving synthetic data and simple neural network architectures. Additionally,\nthe study of large-scale datasets (e.g. ImageNet (Deng, Dong, Socher et al.,\n2009)) and diﬀerent modalities (natural language and sounds) would also greatly\nimprove our work.\n69\n70\nCHAPTER 5. DISCUSSION AND PERSPECTIVES\nProviding complementary empirical evidence\nIdentifying and studying implicit phenomena is a diﬃcult endeavour.\nCon-\nducting many complementary experimental studies are key to (i) oﬀer diﬀerent\nperspectives that will help better characterize the phenomenon and (ii) mitigate\nthe risk of misinterpretation due to unrelated confounding factors.\nIn order to complement our work, a ﬁrst path worth investigating is the\nstudy of densely annotated datasets such as Broden introduced by Bau, Zhou,\nKhosla et al. (2017). Our work makes already use of datasets with two levels of\nclass labels for studying implicit clustering. We interpret the subclasses of such\ndatasets as single clusters, but these could in fact be composed of multiple clus-\nters themselves. Moreover, class labels are attributed to the whole image, while\nthe associated objects/concepts are relevant to only a part of it. Datasets like\nBroden mitigate these two drawbacks by providing many levels of class labels\nas well as pixel-wise annotations. This holds the potential for improved mea-\nsures of clustering abilities and more precise studies of clustering mechanisms\nin natural image datasets.\nAnother path of investigation consists in further evaluating the explanatory\npower of our hypothesis. How well does it explain a variety of phenomena? Our\nwork demonstrates the potential of implicit clustering to explain the beneﬁts\nof pre-training, the coherent gradients hypothesis, neuron interpretability, the\nbeneﬁts of large learning rates, weight decay and others. However, all these ex-\nplanations remain partial, and many other phenomena are not addressed (e.g.\n“double descent” curves (Belkin, Hsu, Ma et al., 2019), the lottery ticket hypoth-\nesis (Frankle and Carbin, 2018) or the beneﬁts of skip connections (Balduzzi,\nFrean, Leary et al., 2017)). Conducting in depth studies of all these phenomena\nunder the light of implicit clustering could further evaluate our hypothesis.\nFormalizing intuitions\nThe amount of equations and mathematical symbols is remarkably low in this\nthesis. Our work is indeed mainly composed of informal arguments and intu-\nitions. Being able to translate our claims into mathematical language would\nhelp increase their precision and falsiﬁability. Moreover, mathematics greatly\nfacilitate the exploration of ideas through deductive reasoning (Wigner, 1960).\nFormalizing the intuitions and results presented in Chapter 3 might be a good\nplace to start, given the simplicity of the associated experimental setup. Ad-\nditionally, Berner, Grohs, Kutyniok et al. (2021) provides a nice overview of\nseveral mathematical studies of deep learning, which could provide inspiration\nfor this diﬃcult endeavour.\n5.2. A REBIRTH OF CLUSTERING ALGORITHMS\n71\n5.2\nA rebirth of clustering algorithms\nBackpropagation, the algorithm behind SGD, has been the most popular al-\ngorithm for training deep neural networks for at least two decades. And for a\ngood reason: it outperforms all other alternatives by a large margin on standard\ndatasets. And yet, there is still a long way to go to reach human-level general-\nization abilities (cfr. Section 1.2.2). This begs the question: should we continue\nbuilding on backpropagation and SGD, or explore novel algorithms? If natural\ndata exhibit a clustered structure, as the natural clustering prior states, aren’t\nclustering algorithms a more natural choice for training deep neural networks?\nCoates and Ng (2012) trains deep neural networks with K-means clustering, but\ntheir method did not match SGD’s performance. Isn’t this contradictory with\nour hypothesis?\nWhile they clearly lack SGD’s capabilities on standard classiﬁcation datasets,\nclustering algorithms have achieved some successes on natural image-related\ntasks in the past (Zoran and Weiss, 2011; Coates, Ng, and Lee, 2011). More-\nover, identifying the right priors appears to be especially crucial for clustering\nalgorithms (Estivill-Castro, 2002). We believe that a better characterization of\nthe natural clustering prior in terms of the shape of clusters, their relative den-\nsity, the distance between them or their relationship with class labels could lead\nto improved clustering algorithms. For example, many clustering algorithms\nassume that all clusters contain approximately the same amount of training\nexamples. However, recent works suggest that the ability of deep neural net-\nworks to “memorize” atypical and poorly represented sub-populations is key for\ntheir performance on natural image classiﬁcation tasks (Feldman, 2020; Jiang,\nZhang, Talwar et al., 2021b). Additionally, many clustering algorithms assume\nspherical cluster shapes while many aspects of natural images vary in speciﬁc,\nanisotropic ways (e.g. scaling, rotation, translation of objects and object parts).\nHence, the current supremacy of SGD over clustering algorithms might not\nbe the ﬁnal picture. Because natural clustering-related priors would be more\neasily integrated into their design, our work predicts a rebirth of clustering\nalgorithms in the coming years, bringing us one step closer to human-level gen-\neralization abilities.\n5.3\nThe societal impact of deep learning research\nHuman societies are complex systems. Understanding how they will react to\nnew technologies is a daunting task. Yet, the well-being of billions of individuals\ncan be at stake. The growing integration of deep learning technologies in the\nindustry raises these diﬃcult questions. Because these were an integral part\nof my own research experience, and encouraged by the NeurIPS conference’s\n72\nCHAPTER 5. DISCUSSION AND PERSPECTIVES\ncall for a “Broader impact” section1, I shortly discuss my personal take on the\nsocietal impact of deep learning research.\nThe ethical concerns around deep learning are numerous.\nDeep learning\ntechnology can be used for autonomous military drones (Shane, Metz, and\nWakabayashi, 2018), generating misinformation automatically (Radford, Wu,\nAmodei et al., 2019), racial discrimination (Kickuchiyo, 2019), mass manipula-\ntion on social networks (Coombe, Curtis, and Orlowski, 2020) and many others.\nMoreover, deep learning confers power to the data owners. The centralization\nof data in big tech companies leads to rising social, economic and political in-\nequalities (Harari, 2018). Of course, many positive opportunities also emerge\nfrom deep learning such as healthcare applications (Panesar, 2019), scientiﬁc\nprogress (Jumper, Evans, Pritzel et al., 2021b) and tackling climate change\n(Rolnick, Donti, Kaack et al., 2019)). But from my very limited perspective, I\nfeel that the negative outcomes largely outnumber the positive ones in terms of\npractical impact.\nAs researchers and engineers, it might be appealing to leave these ethi-\ncal considerations to political institutions. They possess the power to regulate\ntechnologies, and are expected to preserve common good. But reality doesn’t\nnecessarily match expectations. The technological progress could be too fast\nfor ankylosed political systems. Big tech companies could heavily limit a gov-\nernment’s room for manoeuvre when proﬁt is at stake. As Professor Harari\nclaims, many political decisions could in fact be in the hands of the scientists\nand engineers that develop today’s technologies (Harari, 2016).\nWhat are scientists and engineers expected to do then? This remains an open\nquestion. Several brave individuals decide to quit the ﬁeld entirely (e.g., Amadeo\n(2018); Yuan (2020)). Conferences organize workshops around the topic (e.g.,\nChowdhury (2021); Li, Isupova, Haghtalab et al. (2021)). Partnerships with\ncivil society and media organizations are built (e.g. Partnership On AI2). In my\nhumble opinion, throwing deep learning technology into a competitive ecosystem\nis bound to raise inequalities and harm common good. Hence, I believe that\ndesigning tools to facilitate cooperation of citizens around shared goals, at small\nand large scale, has a big potential for positive change. These tools could for\nexample take the form of web applications that facilitate knowledge sharing and\ncollective decision making or that connect people with similar goals. I believe\nthere is a lot of room for improvement around collaborative software design,\nand that this endeavour constitutes a promising path towards a society where\ndeep learning researchers can pursue their quest with a peace of mind.\n1NeurIPS’ call for a “Broader impact” section: In order to provide a balanced perspective,\nauthors are required to include a statement of the potential broader impact of their work,\nincluding its ethical aspects and future societal consequences. Authors should take care to\ndiscuss both positive and negative outcomes.\n2https://partnershiponai.org/\nConclusion\nDeep learning gained a lot of popularity for the plethora of applications it en-\nables.\nBut the open questions and mysteries behind these successes are at\nleast as fascinating. They challenge our understanding of generalization, which\nconstitutes the most fundamental aspect of machine learning. Moreover, they\nexhibit connections with the human brain, one of the universe’s greatest mys-\nteries. In this thesis, we propose a novel path towards a better understanding\nof deep learning.\nOur work builds on the idea that generalization is strongly inﬂuenced by\nthe priors integrated into a learning system’s design.\nWe propose a natural\nclustering prior for supervised image classiﬁcation problems, and study (i) to\nwhat extent this prior is integrated into deep learning systems and (ii) if its\nintegration inﬂuences generalization.\nWe provide a collection of experiments supporting the occurrence of an\nimplicit clustering ability, mechanism and hyperparameter in deep learning.\nMoreover, we demonstrate empirically that these components consistently in-\nﬂuence the generalization abilities of deep neural networks. We further highlight\nmany connections between our observations and previous work on neuron in-\nterpretability, the early phase of training, pre-training, the coherent gradients\nhypothesis and others.\nOverall, our work reveals that the natural clustering prior oﬀers a promis-\ning path towards understanding the generalization abilities of deep learning\nsystems. Additionally, it unveils a path towards new clustering-based training\nalgorithms that could push the generalization abilities of such learning systems\neven further. With these exciting perspectives in mind, we look forward to the\nfuture developments of the fascinating ﬁeld of deep learning.\n73\n74\nCHAPTER 5. DISCUSSION AND PERSPECTIVES\nBibliography\nAbsil, PA, Mahony, R, and Sepulchre, R (2010). Optimization On Manifolds :\nMethods And Applications. In Recent Advances in Optimization and its Ap-\nplications in Engineering, pages 125—-144. Springer. ISBN 9783642125973.\nAchille, A, Rovere, M, and Soatto, S (2019). Critical Learning Periods in Deep\nNeural Networks. In ICLR, pages 1–14. arXiv:arXiv:1711.08856v3.\nAgarwal, A, Barham, P, Brevdo, E, Chen, Z, Citro, C, et al. (2016). TensorFlow\n: Large-Scale Machine Learning on Heterogeneous Distributed Systems. arXiv\npreprint arXiv:1603.04467.\nChollet et al., F (2015). Keras.\nAlcorn, MA, Li, Q, Gong, Z, Wang, C, Mai, L, et al. (2019). Strike (With)\na Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar\nObjects. In CVPR, pages 4845–4854.\nAmadeo, R (2018). A dozen Google employees quit over military drone project.\nArs Technica.\nArjovsky, M (2021). Out of Distribution Generalization in Machine Learning.\nPh.D. thesis. arXiv:2103.02667.\nArora, S (2018). Toward theoretical understanding of deep learning. In ICML\ntutorial.\nArpit,\nD,\nJastrzebski,\nS,\nBallas,\nN,\nKrueger,\nD,\nBengio,\nE,\net\nal.\n(2017).\nA Closer Look at Memorization in Deep Networks.\nIn ICML.\narXiv:arXiv:1706.05394v1.\nBalduzzi, D, Frean, M, Leary, L, Lewis, J, and Ma, Kurt Wan-Duo McWilliams,\nB (2017). The Shattered Gradients Problem: If resnets are the answer, then\nwhat is the question? In ICML. arXiv:arXiv:1702.08591v1.\nBarrett, D and Dherin, B (2021). Implicit Gradient Regularization. In ICLR.\nBau, D, Zhou, B, Khosla, A, Oliva, A, and Torralba, A (2017). Network Dissec-\ntion: Quantifying Interpretability of Deep Visual Representations. In CVPR.\n75\n76\nBIBLIOGRAPHY\nBeery, S, van Horn, G, and Perona, P (2018). Recognition in Terra Incognita.\nIn ECCV. Springer Verlag. arXiv:1807.04975.\nBelkin, M, Hsu, D, Ma, S, and Mandal, S (2019). Reconciling modern machine-\nlearning practice and the classical bias-variance trade-oﬀ. In Proceedings of\nthe National Academy of Sciences. doi: 10.1073/pnas.1903070116.\nBengio, Y (2009). Learning Deep Architectures for AI. Foundations and trends\nin Machine Learning, 2(1):1–127.\nBengio, Y, Courville, A, and Vincent, P (2012).\nRepresentation Learning:\nA Review and New Perspectives. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 35(8):1798–1828. doi: 10.1109/TPAMI.2013.50.\narXiv:1206.5538.\nBengio, Y, Lee, DH, Bornschein, J, Mesnard, T, and Lin, Z (2015). Towards\nBiologically Plausible Deep Learning. arXiv:1502.04156. arXiv:1502.04156.\nBengio, Y, Simard, P, and Frasconi, P (1994). Learning long-term dependencies\nwith gradient descent is diﬃcult.\nIEEE transactions on neural networks,\n5(2):157–166.\nBerner, J, Grohs, P, Kutyniok, G, and Petersen, P (2021). The Modern Math-\nematics of Deep Learning. arXiv:2105.04026. arXiv:arXiv:2105.04026v1.\nBernstein, J, Vahdat, A, Yue, Y, and Liu, MY (2020).\nOn the distance\nbetween two neural networks and the stability of learning.\nIn NeurIPS,\nvolume 2020-Decem. Neural information processing systems foundation.\narXiv:2002.03432.\nBishop, CM (2006). Pattern recognition and machine learning. Springer.\nCammarata, N, Carter, S, Goh, G, Olah, C, Petrov, M, et al. (2020). Thread:\nCircuits. Distill.\nChapelle, O and Zien, A (2005). Semi-supervised classiﬁcation by low density\nseparation. In International workshop on artiﬁcial intelligence and statistics,\npages 57–64.\nChatterjee,\nS\n(2020).\nCoherent\ngradients:\nan\napproach\nto\nunder-\nstanding\ngeneralization\nin\ngradient-based\noptimization.\nIn\nICLR.\narXiv:arXiv:2002.10657v1.\nChatterjee, S and Zielinski, P (2020).\nMaking coherence out of nothing\nat all: measuring the evolution of gradient alignment.\narXiv:2008.01217.\narXiv:arXiv:2008.01217v1.\nChowdhury, T (2021). The moral (and morale) compass: A search for meaning\nin AI research.\nBIBLIOGRAPHY\n77\nCoates, A, Ng, A, and Lee, H (2011). An Analysis of Single-Layer Networks\nin Unsupervised Feature Learning. In Proceedings of the Fourteenth Interna-\ntional Conference on Artiﬁcial Intelligence and Statistics, volume 15, pages\n215–223.\nCoates, A and Ng, AY (2012). Learning Feature Representations with K-means.\nIn Neural Networks: Tricks of the Trade, 2nd edn. Springer.\nCoombe, D, Curtis, V, and Orlowski, J (2020). The Social Dilemma.\nCS231N, S (2016). Tiny ImageNet Visual Recognition Challenge. https://tiny-\nimagenet.herokuapp.com/.\nCubuk, ED, Zoph, B, Mane, D, Vasudevan, V, and Le, QV (2019). AutoAug-\nment: Learning Augmentation Policies from Data. In CVPR, Section 3, pages\n113–123. arXiv:1805.09501.\nDalal, N and Triggs, B (2005). Histograms of oriented gradients for human\ndetection. In CVPR, pages 886–893. doi: 10.1109/CVPR.2005.177¨ı.\nDauber, A, Feder, M, Koren, T, and Livni, R (2020). Can implicit bias explain\ngeneralization? Stochastic convex optimization as a case study. In NeurIPS.\nNeural information processing systems foundation. arXiv:2003.06152.\nDeng, J, Dong, W, Socher, R, Li, LJ, Li, K, et al. (2009). ImageNet: A Large-\nScale Hierarchical Image Database. In CVPR, pages 248–255.\nDeVries, T and Taylor, GW (2017). Improved Regularization of Convolutional\nNeural Networks with Cutout. arXiv:1708.04552. arXiv:1708.04552.\nDodge, S and Karam, L (2017). A Study and Comparison of Human and Deep\nLearning Recognition Performance Under Visual Distortions. 2017 26th In-\nternational Conference on Computer Communications and Networks, ICCCN\n2017. arXiv:1705.02498.\nDonahue, J, Jia, Y, Vinyals, O, Hoﬀman, J, Zhang, N, et al. (2014). DeCAF :\nA Deep Convolutional Activation Feature for Generic Visual Recognition. In\nICML, pages 647–655.\nDu, S, Lee, J, Li, H, Wang, L, and Zhai, X (2019). Gradient Descent Finds\nGlobal Minima of Deep Neural Networks. In ICML, pages 1675–1685. PMLR.\nDuchi, J, Hazan, E, and Singer, Y (2011). Adaptive Subgradient Methods for\nOnline Learning and Stochastic Optimization. Journal of Machine Learning\nResearch, 12(Jul):2121–2159.\nEldan, R and Shamir, O (2016). The Power of Depth for Feedforward Neural\nNetworks. In Annual Conference on Learning Theory, volume 49, pages 907–\n940. PMLR.\n78\nBIBLIOGRAPHY\nEngstrom, L, Tran, B, Tsipras, D, Schmidt, L, and Madry, A (2019). Exploring\nthe Landscape of Spatial Robustness. ICML. arXiv:1712.02779.\nEstivill-Castro, V (2002).\nWhy so many clustering algorithms:\na posi-\ntion paper.\nACM SIGKDD Explorations Newsletter, 4(1):65–75.\ndoi:\n10.1145/568574.568575.\nFeldman, V (2020). Does Learning Require Memorization? A Short Tale about\na Long Tail. In Proceedings of the 52nd Annual ACM SIGACT Symposium on\nTheory of Computing. ACM, New York, NY, USA. doi: 10.1145/3357713.\nFort, S, Dziugaite, GK, Paul, M, Kharaghani, S, Roy, DM, et al. (2020). Deep\nlearning versus kernel learning : an empirical study of loss landscape geometry\nand the time evolution of the Neural Tangent Kernel. In NeurIPS.\nFrankle, J and Carbin, M (2018).\nThe Lottery Ticket Hypothesis: Finding\nSparse, Trainable Neural Networks. In ICLR. International Conference on\nLearning Representations, ICLR. arXiv:1803.03635.\nFrankle,\nJ, Dziugaite,\nGK, Roy,\nDM, and Carbin,\nM (2020).\nLin-\near Mode Connectivity and the Lottery Ticket Hypothesis.\nIn ICML.\narXiv:arXiv:1912.05671v4.\nFrankle, J, Schwab, DJ, and Morcos, AS (2020). The Early Phase of Neural\nNetwork Training. In ICLR. arXiv:arXiv:2002.10365v1.\nFritzke, B (1997). Some competitive learning methods.\nFukushima, K (1980). Neocognitron: A Self-organizing Neural Network Model\nfor a Mechanism of Pattern Recognition Unaﬀected by Shift in Position. Bi-\nological cybernetics, 202(36):193–202.\nGeiping, J, Goldblum, M, Pope, PE, Moeller, M, and Goldstein, T (2021).\nStochastic Training is Not Necessary for Generalization. arXiv:2109.14119.\narXiv:2109.14119.\nGeirhos, R, Medina Temme, CR, Rauber, J, Sch¨utt, HH, Bethge, M, et al.\n(2018). Generalisation in humans and deep neural networks. In NeurIPS.\narXiv:1808.08750v3.\nGeirhos, R, Rubisch, P, Michaelis, C, Bethge, M, Wichmann, FA, et al. (2018).\nImageNet-trained CNNs are biased towards texture; increasing shape bias\nimproves accuracy and robustness. In ICLR 2019. International Conference\non Learning Representations, ICLR. arXiv:1811.12231.\nGeman, S, Bienenstock, E, and Doursat, R (1992). Neural networks and the\nbias/variance dilemma. Neural computation, 4(1):1–58.\nGinsburg, B, Gitman, I, and You, Y (2018). Large Batch Training of Convolu-\ntional Networks with Layer-wise Adaptive Rate Scaling.\nBIBLIOGRAPHY\n79\nGlorot, X and Bengio, Y (2010). Understanding the diﬃculty of training deep\nfeedforward neural networks. In AISTATS, pages 249–256.\nGolatkar, A, Achille, A, and Soatto, S (2019). Time Matters in Regularizing\nDeep Networks : Weight Decay and Data Augmentation Aﬀect Early Learn-\ning. In NeurIPS.\nGoodfellow, I, Bengio, Y, and Courville, A (2016). Deep Learning. Book in\npreparation for MIT Press.\nGoodfellow, IJ, Shlens, J, and Szegedy, C (2015). Explaining and Harnessing\nAdverarial Examples. In ICLR. arXiv:arXiv:1412.6572v3.\nGoyal,\nP, Dollar,\nP, Girshick,\nR, Noordhuis,\nP, Wesolowski,\nL, et al.\n(2017).\nAccurate, Large Minibatch SGD: Training ImageNet in 1 Hour.\narXiv:1706.02677. arXiv:1706.02677.\nGoyal,\nP, Dollar,\nP, Girshick,\nR, Noordhuis,\nP, Wesolowski,\nL, et al.\n(2018).\nAccurate, Large Minibatch SGD: Training ImageNet in 1 Hour.\narXiv:1706.02677. arXiv:arXiv:1706.02677v2.\nGraves, A, Mohamed, Ar, and Hinton, G (2013). Speech recognition with deep\nrecurrent neural networks. IEEE international conference on acoustics, speech\nand signal processing, pages 6645—-6649. arXiv:1303.5778v1.\nGulrajani, I and Lopez-Paz, D (2021). In search of lost domain generalization.\nIn ICLR.\nGur-Ari, G, Roberts, DA, and Dyer, E (2018). Gradient Descent Happens in a\nTiny Subspace. arXiv:1812.04754. arXiv:arXiv:1812.04754v1.\nHarari, YN (2016). Homo Deus: A brief history of tomorrow. Harvill Secker.\nISBN 978-5-906837-92-9.\nHarari, YN (2018). Why Technology Favors Tyranny.\nHassabis, D, Kumaran, D, and Summerﬁeld, Christopher Botvinick, M (2017).\nNeuroscience-inspired artiﬁcial intelligence. Neuron, 95(2):245—-258.\nHe, K, Fan, H, Wu, Y, Xie, S, and Girshick, R (2020). Momentum Contrast for\nUnsupervised Visual Representation Learning. In CVPR, pages 9729–9738.\nHe, K, Zhang, X, Ren, S, and Sun, J (2015).\nDelving Deep into Rectiﬁers:\nSurpassing Human-Level Performance on ImageNet Classiﬁcation. In ICCV.\narXiv:1502.01852v1.\nHe, K, Zhang, X, Ren, S, and Sun, J (2016). Deep Residual Learning for Image\nRecognition. In CVPR, pages 770–778. arXiv:arXiv:1512.03385v1.\nHebb, DO (1949). The organization of behavior.\n80\nBIBLIOGRAPHY\nHendrycks, D and Dietterich, T (2019). Benchmarking Neural Network Ro-\nbustness to Common Corruptions and Perturbations. In ICLR. International\nConference on Learning Representations, ICLR. arXiv:1903.12261.\nHinton, GE, Mcclelland, JL, and Rumelhart, DE (1987). Distributed Repre-\nsentations.\nIn Parallel Distributed Processing: Explorations in the micro-\nstructure of cognition.\nHinton, GE, Osindero, S, and Teh, YW (2006).\nA Fast Learning Algo-\nrithm for Deep Belief Nets.\nNeural Computation, 18(7):1527–1554.\ndoi:\n10.1162/NECO.2006.18.7.1527.\nHoai, M and Zisserman, A (2013). Discriminative sub-categorization. In Pro-\nceedings of the IEEE Computer Society Conference on Computer Vision and\nPattern Recognition, pages 1666–1673. doi: 10.1109/CVPR.2013.218.\nHochreiter, S (1998). The vanishing gradient problem during learning recurrent\nneural nets and problem solutions. IJUFKS, 6(2):1–10.\nHoﬀer, E, Banner, R, Golan, I, and Soudry, D (2018). Norm matters: eﬃcient\nand accurate normalization schemes in deep networks.\narXiv:1803.01814.\narXiv:1803.01814.\nHoﬀer, E, Hubara, I, and Soudry, D (2017). Train longer , generalize better :\nclosing the generalization gap in large batch training of neural networks. In\nNIPS, pages 1729—-1739. arXiv:arXiv:1705.08741v1.\nHubel, DH and Wiesel, TN (1962). Receptive ﬁelds, binocular interaction and\nfunctional architecture in the cat’s visual cortex. The Journal of Physiology,\n160(1):106–154. doi: 10.1113/JPHYSIOL.1962.SP006837.\nHuh, M, Agrawal, P, and Efros, AA (2016). What makes ImageNet good for\ntransfer learning? arXiv:1608.08614. arXiv:arXiv:1608.08614v2.\nIoﬀe, S and Szegedy, C (2015). Batch Normalization: Accelerating Deep Net-\nwork Training by Reducing Internal Covariate Shift. ICML, pages 448—-456.\ndoi: 10.1007/s13398-014-0173-7.2. arXiv:1502.03167.\nJastrzebski,\nS,\nKenton,\nZ,\nArpit,\nD,\nBallas,\nN,\nFischer,\nA,\net\nal.\n(2017).\nThree Factors Inﬂuencing Minima in SGD.\narXiv:1711.04623.\narXiv:arXiv:1711.04623v1.\nJastrzebski, S, Szymczak, M, Fort, S, Arpit, D, Tabor, J, et al. (2020). The\nBreak-Even Point on Optimization Trajectories of Deep Neural Networks. In\nICLR.\nJiang, Y, Neyshabur, B, Mobahi, H, Krishnan, D, and Bengio, S (2020).\nFantastic Generalization Measures and Where to Find Them.\nIn ICLR.\narXiv:arXiv:1912.02178v1.\nBIBLIOGRAPHY\n81\nJiang, Z, Zhang, C, Talwar, K, and Mozer, MC (2021a). Characterizing Struc-\ntural Regularities of Labeled Data in Overparameterized Models. In ICML,\npages 5034–5044. PMLR.\nJiang, Z, Zhang, C, Talwar, K, and Mozer, MC (2021b). Characterizing Struc-\ntural Regularities of Labeled Data in Overparameterized Models. In ICML,\npages 5034–5044. PMLR.\nJumper, J, Evans, R, Pritzel, A, Green, T, Figurnov, M, et al. (2021a). Highly\naccurate protein structure prediction with AlphaFold. Nature.\nJumper, J, Evans, R, Pritzel, A, Green, T, Figurnov, M, et al. (2021b). Highly\naccurate protein structure prediction with AlphaFold. Nature, 596(7873):583–\n589. doi: 10.1038/s41586-021-03819-2.\nKamnitsas, K, Castro, DC, Le Folgoc, L, Ian, W, Ryutaro, T, et al. (2018).\nSemi-Supervised Learning via Compact Latent Space Clustering. In ICML.\narXiv:arXiv:1806.02679v2.\nKaufman, L and Rousseeuw, PJ (2009). Finding groups in data: an introduction\nto cluster analysis. John Wiley & Sons.\nKawaguchi, K, Kaelbling, LP, and Bengio, Y (2017). Generalization in Deep\nLearning. arXiv:1710.05468. arXiv:arXiv:1710.05468v6.\nKeskar, NS, Mudigere, D, Nocedal, J, Smelyanskiy, M, and Tang, PTP (2017).\nOn Large-Batch Training for Deep Learning: Generalization Gap and Sharp\nMinima. In ICLR.\nKickuchiyo (2019).\n[D] Has anyone noticed a lot of ML research into facial\nrecognition of Uyghur people lately?\nKingma, DP and Ba, JL (2015). Adam: A method for stochastic optimization.\nIn ICLR. arXiv:arXiv:1412.6980v9.\nKrizhevsky, A and Hinton, G (2009). Learning Multiple Layers of Features from\nTiny Images. Technical report, University of Toronto.\nKrizhevsky, A, Sutskever, I, and Hinton, GE (2012). ImageNet Classiﬁcation\nwith Deep Convolutional Neural Networks. In NIPS.\nKrueger, D, Caballero, E, Jacobsen, JH, Zhang, A, Binas, J, et al. (2021). Out-\nof-Distribution Generalization via Risk Extrapolation. In ICML.\nvan Laarhoven, T (2017). L2 Regularization versus Batch and Weight Normal-\nization. arXiv:1706.05350. arXiv:1706.05350.\nLeavitt, ML and Morcos, A (2020).\nSelectivity considered harmful:\neval-\nuating the causal impact of class selectivity in DNNs.\narXiv:2003.01262.\narXiv:arXiv:2003.01262v3.\n82\nBIBLIOGRAPHY\nLecun, Y (2019). The Epistemology of Deep Learning. In Talk at the Institute\nfor Advanced Study.\nLeCun, Y, Bengio, Y, and Hinton, G (2015).\nDeep learning.\nNature,\n521(7553):436–444. doi: 10.1038/nature14539. arXiv:arXiv:1312.6184v5.\nLeCun, Y, Bottou, L, Bengio, Y, and Haﬀner, P (1998). Gradient-based learning\napplied to document recognition. Proceedings of the IEEE, 86(11):2278–2323.\ndoi: 10.1109/5.726791. arXiv:1102.0183.\nLi, Y, Isupova, O, Haghtalab, N, White, A, and Granziol, D (2021). The ICML\nDebate: Should AI Research and Development Be Controlled by a Regulatory\nBody or Government Oversight?\nLiang, S and Srikant, R (2017). Why Deep Neural Networks for Function Ap-\nproximation? In ICLR. arXiv:1610.04161.\nLillicrap, TP, Santoro, A, Marris, L, Akerman, CJ, and Hinton, G (2020). Back-\npropagation and the brain. Nature Reviews Neuroscience, 21(6):335–346. doi:\n10.1038/S41583-020-0277-3.\nLin, HW, Tegmark, M, and Rolnick, D (2017).\nWhy does deep and cheap\nlearning work so well?\nJournal of Statistical Physics, 168:1223–1247.\narXiv:arXiv:1608.08225v2.\nLinnainmaa, S (1970). The representation of the cumulative rounding error of\nan algorithm as a Taylor expansion of the local rounding errors. Ph.D. thesis,\nUniv. Helsinki.\nLiu, Y, Bernstein, J, Meister, M, and Yue, Y (2021). Learning by Turning:\nNeural Architecture Aware Optimisation. In ICML, pages 6748–6758. PMLR.\nLowe, DG (1999). Object recognition from local scale-invariant features. In\nICCV, pages 1150–1157.\nMangalam, K and Prabhu, VU (2019). Do deep neural networks learn shallow\nlearnable examples ﬁrst? In Workshop Deep Phenomena, ICML.\nMansur, A and Kuno, Y (2008). Improving Recognition through Object Sub-\ncategorization. In Advances in Visual Computing, pages 851–859. Springer.\nMartinetz, T and Schulten, K (1991). A ”neural-gas” network learns topologies.\nArti ﬁcial Neural Networks.\nMasters, D and Luschi, C (2018). Revisiting Small Batch Training for Deep\nNeural Networks. arXiv:1804.07612. arXiv:arXiv:1804.07612v1.\nMcCulloch, WS and Pitts, W (1943). A logical calculus of the ideas immanent\nin nervous activity. Bulletin of Mathematical Biology, 52(1-2):99–115. doi:\n10.1007/BF02459570.\nBIBLIOGRAPHY\n83\nMnih, V, Heess, N, Graves, A, and Kavukcuoglu, K (2014). Recurrent Models\nof Visual Attention. In NIPS.\nMorcos, AS, Barrett, DGT, Rabinowitz, NC, and Botvinick, M (2018).\nOn the importance of single directions for generalization.\nIn ICLR.\narXiv:arXiv:1803.06959v4.\nM¨uller, R, Kornblith, S, and Hinton, G (2019). When Does Label Smoothing\nHelp ? In NeurIPS. arXiv:arXiv:1906.02629v3.\nMurphy, KP (2021). Probabilistic Machine Learning: An introduction. MIT\nPress.\nNagarajan, V, Andreassen, A, and Neyshabur, B (2021). Understanding the\nfailure modes of out-of-distribution generalization. In ICLR.\nNair, V and Hinton, GE (2010).\nRectiﬁed Linear Units Improve Restricted\nBoltzmann Machines. In ICML, pages 807—-814.\nNeyshabur, B, Tomioka, R, and Srebro, N (2015). In search of the real induc-\ntive bias: On the role of implicit regularization in deep learning. In ICLR.\narXiv:arXiv:1412.6614v4.\nNguyen, A, Yosinski, J, and Clune, J (2015). Deep Neural Networks are Easily\nFooled : High Conﬁdence Predictions for Unrecognizable Images. In CVPR,\npages 427–436. arXiv:arXiv:1412.1897v4.\nOquab, M, Bottou, L, Laptev, I, and Sivic, J (2014). Learning and Transferring\nMid-Level Image Representations using Convolutional Neural Networks. In\nCVPR, pages 1717–1724.\nPanesar, A (2019).\nMachine learning and AI for healthcare.\nApress Media.\nISBN 9781484265369. doi: 10.1007/978-1-4842-6537-6.\nRadford, A, Wu, J, Amodei, D, Amodei, D, Clark, J, et al. (2019).\nBetter\nLanguage Models and Their Implications.\nRahimi, A (2017). Test of Time Award reception. In NIPS.\nRecht, B, Roelofs, R, Schmidt, L, and Shankar, V (2019). Do ImageNet Clas-\nsiﬁers Generalize to ImageNet? In ICML, pages 5389–5400.\nRolnick,\nD, Donti,\nPL, Kaack,\nLH, Kochanski,\nK, Lacoste,\nA, et al.\n(2019). Tackling Climate Change with Machine Learning. arXiv:1906.05433.\narXiv:1906.05433.\nRosenblatt, F (1958). The perceptron: a probabilistic model for information\nstorage and organization in the brain. Psychological review, 65(6):386–408.\nRumelhart, DE, Hinton, GE, and Williams, RJ (1986).\nLearning repre-\nsentations by back-propagating errors.\nNature, 323(6088):533–536.\ndoi:\n10.1038/323533a0.\n84\nBIBLIOGRAPHY\nSchaﬀer, C (1994). A Conservation Law for Generalization Performance. In In-\nternational Conference on Machine Learning, pages 259–265. Morgan Kauf-\nmann. doi: 10.1016/B978-1-55860-335-6.50039-8.\nSchmidhuber,\nJ\n(2014).\nDeep\nLearning\nin\nNeural\nNetworks:\nAn\nOverview.\narXiv\npreprint\narXiv:1404.7828,\npages\n1–66.\ndoi:\n10.1016/j.neunet.2014.09.003. arXiv:arXiv:1404.7828v1.\nShane, S, Metz, C, and Wakabayashi, D (2018).\nHow a Pentagon Contract\nBecame an Identity Crisis for Google.\nShankar, V, Roelofs, R, Mania, H, Fang, A, Recht, B, et al. (2020). Evaluating\nMachine Accuracy on ImageNet. In ICML, pages 8634–8644. PMLR.\nSilver, D, Huang, A, Maddison, CJ, Guez, A, Sifre, L, et al. (2016).\nMas-\ntering the game of Go with deep neural networks and tree search. Nature,\n529(7585):484–489. doi: 10.1038/nature16961.\nSimonyan, K, Vedaldi, A, and Zisserman, A (2014). Deep Inside Convolutional\nNetworks : Visualising Image Classiﬁcation Models and Saliency Maps. ICLR.\narXiv:arXiv:1312.6034v2.\nSimonyan, K and Zisserman, A (2015). Very Deep Convolutional Networks for\nLarge-Scale Image Recognition. In ICLR. arXiv:arXiv:1409.1556v6.\nSmith, LN and Topin, N (2017).\nSuper-Convergence:\nVery Fast Train-\ning of Residual Networks Using Large Learning Rates.\narXiv:1708.07120.\narXiv:arXiv:1708.07120v2.\nSmith, SL, Dherin, B, Barrett, D, and De, S (2021). On the Origin of Implicit\nRegularization in Stochastic Gradient Descent. In ICLR.\nSmith, SL and Le, QV (2017). A bayesian perspective on generalization and\nstochastic gradient descent. In Proceedings of Second workshop on Bayesian\nDeep Learning (NIPS 2017). arXiv:arXiv:1710.06451v3.\nSrivastava, N, Hinton, GE, Krizhevsky, A, Sutskever, I, and Salakhutdinov, R\n(2014). Dropout : A Simple Way to Prevent Neural Networks from Overﬁt-\nting. Journal of Machine Learning Research, 15(1):1929–1958.\nSu, J, Vargas, DV, and Kouichi, S (2019). One pixel attack for fooling deep neu-\nral networks. IEEE Transactions on Evolutionary Computation, 23(5):828–\n841. doi: 10.1109/tevc.2019.2890858. arXiv:1710.08864.\nSzegedy, C, Zaremba, W, Sutskeveer, I, Bruna, J, Erhan, D, et al. (2014). In-\ntriguing properties of neural networks. In ICLR. arXiv:arXiv:1312.6199v4.\nTelgarsky, M (2016). beneﬁts of depth in neural networks. In Annual Conference\non Learning Theory, volume 49, pages 1517–1539. PMLR.\nBIBLIOGRAPHY\n85\nTieleman, T and Hinton, G (2012). Lecture 6.5—RmsProp: Divide the gradient\nby a running average of its recent magnitude. COURSERA: Neural Networks\nfor Machine Learning.\nTorralba, A and Efros, AA (2011).\nUnbiased look at dataset bias.\nIn Pro-\nceedings of the IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition, pages 1521–1528. IEEE Computer Society.\ndoi:\n10.1109/CVPR.2011.5995347.\nVapnik, V (1989). Statistical learning theory. Wiley New York.\nVapnik, V and Izmailov, R (2019).\nRethinking statistical learning theory:\nlearning using statistical invariants. Machine Learning, 108(3):381–423. doi:\n10.1007/s10994-018-5742-0.\nVapnik, VN and Chervonenkis, AY (1968). On the uniform convergence of rela-\ntive frequencies of events to their probabilities. Dokl. Akad. Nauk., 181(4):781.\nWang, H and Raj, B (2017). On the Origin of Deep Learning. arXiv:1702.07800,\npages 1–72. arXiv:arXiv:1702.07800v4.\nWerbos, PJ (1982). Applications of advances in nonlinear sensitivity analysis.\nIn System Modeling and Optimization, pages 762–770. Springer-Verlag. doi:\n10.1007/BFB0006203.\nWidrow, B and Hoﬀ, ME (1960). Adaptive switching circuits. IRE WESCON\nConvention Record, 4:96–104.\nWigner, E (1960). The unreasonable eﬀectiveness of mathematics in the natural\nsciences. Communications in Pure and Applied Mathematics, 13:1–14.\nWilson, AC, Roelofs, R, Stern, M, Srebro, N, and Recht, B (2017). The Marginal\nValue of Adaptive Gradient Methods in Machine Learning. In NIPS, pages\n4151–4161.\nWolpert, DH (1996). The lack of a prior distinctions between learning algorithms\nand the existence of a priori distinctions between learning algorithms. Neural\nComputation, 8:1341–1421.\nWu, J, Zou, D, Braverman, V, and Quanquan Gu (2021). Direction Matters:\nOn the Implicit Bias of Stochastic Gradient Descent with Moderate Learning\nRate. In ICLR.\nYosinski, J, Clune, J, Nguyen, A, Fuchs, T, and Lipson, H (2015).\nUnder-\nstanding Neural Networks Through Deep Visualization. In Deep Learning\nWorkshop at ICML. arXiv:arXiv:1506.06579v1.\nYoung, T, Hazarika, D, Poria, S, and Cambria, E (2018). Recent trends in deep\nlearning based natural language processing. ieee Computational intelligence\nmagazine, 13(3):55–75. doi: 10.1038/nature14539.\n86\nBIBLIOGRAPHY\nYu, AW, Lin, Q, Salakhutdinov, R, and Carbonell, J (2017).\nNormalized\ngradient with adaptive stepsize method for deep neural network training.\narXiv:1707.04822.\nYuan, Y (2020). YOLO Creator Joseph Redmon Stopped CV Research Due to\nEthical Concerns. Medium.\nYun, C, Krishnan, S, and Mobahi, H (2021). A unifying view on implicit bias\nin training linear neural networks. In ICLR.\nZagoruyko,\nS\n(2015).\n92.45%\non\nCIFAR-10\nin\nTorch.\nhttp://torch.ch/blog/2015/07/30/cifar.html.\nZagoruyko, S and Komodakis, N (2016). Wide Residual Networks. In BMVC.\narXiv:arXiv:1605.07146v2.\nZeiler, MD and Fergus, R (2014). Visualizing and Understanding Convolutional\nNetworks. ECCV, pages 818–833. arXiv:arXiv:1311.2901v3.\nZhang, C, Bengio, S, Hardt, M, Recht, B, and Vinyals, O (2017).\nUn-\nderstanding deep learning requires re-thinking generalization.\nIn ICLR.\narXiv:arXiv:1611.03530v1.\nZhang, C, Bengio, S, Hardt, M, Recht, B, and Vinyals, O (2021). Understanding\ndeep learning (still) requires rethinking generalization. Communications of the\nACM, 64(3):107–115. doi: 10.1145/3446776.\nZhang, G, Wang, C, Xu, B, and Grosse, R (2019). Three mechanisms of weight\ndecay regularization. In ICLR.\nZhang, H, Cisse, M, Dauphin, YN, and Lopez-Paz, D (2018). mixup: Beyond\nEmpirical Risk Minimization. In ICLR. International Conference on Learning\nRepresentations, ICLR. arXiv:1710.09412.\nZhao, N, Wu, Z, Lau, RWH, and Lin, S (2021). What makes instance discrimi-\nnation good for transfer learning? In ICLR. arXiv:2006.06606.\nZhou, B, Khosla, A, Lapedriza, A, Oliva, A, and Torralba, A (2015). Object\ndetectors emerge in Deep Scene CNNs. In ICLR. arXiv:arXiv:1412.6856v2.\nZielinski, P, Krishnan, S, and Chatterjee, S (2020). Weak and Strong Gradi-\nent Directions: Explaining Memorization, Generalization, and Hardness of\nExamples at Scale. arXiv:2003.07422. arXiv:2003.07422.\nZoran, D and Weiss, Y (2011). From learning models of natural image patches to\nwhole image restoration. In Proceedings of the IEEE International Conference\non Computer Vision, pages 479–486. doi: 10.1109/ICCV.2011.6126278.\nPublications\nThis thesis wraps up a series of works that have been previously published in\nseveral conference venues:\n• Simon Carbonnelle, C. De Vleeschouwer. Intraclass clustering: an implicit\nlearning ability that regularizes DNNs, ICLR 2021.\n• Simon Carbonnelle, C. De Vleeschouwer.\nExperimental study of the\nneuron-level mechanisms emerging from backpropagation, ESANN 2019.\n• Simon Carbonnelle, C. De Vleeschouwer. Layer rotation: a surprisingly\nsimple indicator of generalization in deep networks, Workshop Deep Phe-\nnomena, ICML 2019.\nMy journey as a PhD student started with a collaboration with Claire Gosse\nand Marie Van Reybroeck, two researchers in speech and language pathology.\nThis collaboration initiated an ongoing research project that studies the inter-\nactions between handwriting, spelling and dyslexia through the use of digital\ntablets and signal processing tools. My work in this context also contributed to\nthe following two publications:\n• C. Gosse, S. Carbonnelle, C. De Vleeschouwer, M. Van Reybroeck. Spec-\nifying the graphic characteristics of words that inﬂuence children’s hand-\nwriting, Reading and Writing, 31 (5), 1181-1207, 2018.\n• C. Gosse, S. Carbonnelle, C. De Vleeschouwer, M. Van Reybroeck. The\ninﬂuence of graphic complexities of words on the handwriting of children\nof 2nd grade, SIG WRITING, 16th international conference of the EARLI\nspecial interest group on writing, Liverpool, 2016.\n87\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-03-15",
  "updated": "2022-03-15"
}