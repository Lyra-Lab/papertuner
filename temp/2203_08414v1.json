{
  "id": "http://arxiv.org/abs/2203.08414v1",
  "title": "Unsupervised Semantic Segmentation by Distilling Feature Correspondences",
  "authors": [
    "Mark Hamilton",
    "Zhoutong Zhang",
    "Bharath Hariharan",
    "Noah Snavely",
    "William T. Freeman"
  ],
  "abstract": "Unsupervised semantic segmentation aims to discover and localize semantically\nmeaningful categories within image corpora without any form of annotation. To\nsolve this task, algorithms must produce features for every pixel that are both\nsemantically meaningful and compact enough to form distinct clusters. Unlike\nprevious works which achieve this with a single end-to-end framework, we\npropose to separate feature learning from cluster compactification.\nEmpirically, we show that current unsupervised feature learning frameworks\nalready generate dense features whose correlations are semantically consistent.\nThis observation motivates us to design STEGO ($\\textbf{S}$elf-supervised\n$\\textbf{T}$ransformer with $\\textbf{E}$nergy-based $\\textbf{G}$raph\n$\\textbf{O}$ptimization), a novel framework that distills unsupervised features\ninto high-quality discrete semantic labels. At the core of STEGO is a novel\ncontrastive loss function that encourages features to form compact clusters\nwhile preserving their relationships across the corpora. STEGO yields a\nsignificant improvement over the prior state of the art, on both the CocoStuff\n($\\textbf{+14 mIoU}$) and Cityscapes ($\\textbf{+9 mIoU}$) semantic segmentation\nchallenges.",
  "text": "Published as a conference paper at ICLR 2022\nUNSUPERVISED SEMANTIC SEGMENTATION\nBY DISTILLING FEATURE CORRESPONDENCES\nMark Hamilton\nMIT, Microsoft\nmarkth@mit.edu\nZhoutong Zhang\nMIT\nBharath Hariharan\nCornell University\nNoah Snavely\nCornell University, Google\nWilliam T. Freeman\nMIT, Google\nABSTRACT\nUnsupervised semantic segmentation aims to discover and localize semantically\nmeaningful categories within image corpora without any form of annotation. To\nsolve this task, algorithms must produce features for every pixel that are both se-\nmantically meaningful and compact enough to form distinct clusters. Unlike pre-\nvious works which achieve this with a single end-to-end framework, we propose\nto separate feature learning from cluster compactiﬁcation. Empirically, we show\nthat current unsupervised feature learning frameworks already generate dense fea-\ntures whose correlations are semantically consistent. This observation motivates\nus to design STEGO (Self-supervised Transformer with Energy-based Graph\nOptimization), a novel framework that distills unsupervised features into high-\nquality discrete semantic labels. At the core of STEGO is a novel contrastive loss\nfunction that encourages features to form compact clusters while preserving their\nrelationships across the corpora. STEGO yields a signiﬁcant improvement over\nthe prior state of the art, on both the CocoStuff (+14 mIoU) and Cityscapes (+9\nmIoU) semantic segmentation challenges.\n1\nINTRODUCTION\nSemantic segmentation is the process of classifying each individual pixel of an image into a known\nontology. Though semantic segmentation models can detect and delineate objects at a much ﬁner\ngranularity than classiﬁcation or object detection systems, these systems are hindered by the dif-\nﬁculties of creating labelled training data. In particular, segmenting an image can take over 100×\nmore effort for a human annotator than classifying or drawing bounding boxes (Zlateski et al., 2018).\nFurthermore, in complex domains such as medicine, biology, or astrophysics, ground-truth segmen-\ntation labels may be unknown, ill-deﬁned, or require considerable domain-expertise to provide (Yu\net al., 2018).\nRecently, several works introduced semantic segmentation systems that could learn from weaker\nforms of labels such as classes, tags, bounding boxes, scribbles, or point annotations (Ren et al.,\n2020; Pan et al., 2021; Liu et al., 2020; Bilen et al.). However, comparatively few works take up\nthe challenge of semantic segmentation without any form of human supervision or motion cues.\nAttempts such as Independent Information Clustering (IIC) (Ji et al., 2019) and PiCIE (Cho et al.,\n2021) aim to learn semantically meaningful features through transformation equivariance, while\nimposing a clustering step to improve the compactness of the learned features.\nIn contrast to these previous methods, we utilize pre-trained features from unsupervised feature\nlearning frameworks and focus on distilling them into a compact and discrete structure while pre-\nserving their relationships across the image corpora. This is motivated by the observation that cor-\nrelations between unsupervised features, such as ones learned by DINO (Caron et al., 2021), are\nalready semantically consistent, both within the same image and across image collections.\nAs a result, we introduce STEGO (Self-supervised Transformer with Energy-based Graph\nOptimization), which is capable of jointly discovering and segmenting objects without human super-\nvision. STEGO distills pretrained unsupervised visual features into semantic clusters using a novel\n1\narXiv:2203.08414v1  [cs.CV]  16 Mar 2022\nPublished as a conference paper at ICLR 2022\nFigure 1: Unsupervised semantic segmentation predictions on the CocoStuff (Caesar et al., 2018)\n27 class segmentation challenge. Our method, STEGO, does not use labels to discover and segment\nconsistent objects. Unlike the prior state of the art, PiCIE (Cho et al., 2021), STEGO’s predictions\nare consistent, detailed, and do not omit key objects.\ncontrastive loss. STEGO dramatically improves over prior art and is a considerable step towards\nclosing the gap with supervised segmentation systems. We include a short video detailing the work\nat https://aka.ms/stego-video. Speciﬁcally, we make the following contributions:\n• Show that unsupervised deep network features have correlation patterns that are largely\nconsistent with true semantic labels.\n• Introduce STEGO, a novel transformer-based architecture for unsupervised semantic seg-\nmentation.\n• Demonstrate that STEGO achieves state of the art performance on both the CocoStuff (+14\nmIoU) and Cityscapes (+9 mIoU) segmentation challenges.\n• Justify STEGO’s design with an ablation study on the CocoStuff dataset.\n2\nRELATED WORK\nSelf-supervised Visual Feature Learning\nLearning meaningful visual features without human\nannotations is a longstanding goal of computer vision. Approaches to this problem often optimize a\nsurrogate task, such as denoising (Vincent et al., 2008), inpainting (Pathak et al., 2016), jigsaw puz-\nzles, colorization (Zhang et al., 2017), rotation prediction (Gidaris et al., 2018), and most recently,\ncontrastive learning over multiple augmentations (Hjelm et al., 2018; Chen et al., 2020a;a;c; Oord\net al., 2018). Contrastive learning approaches, whose performance surpass all other surrogate tasks,\nassume visual features are invariant under a certain set of image augmentation operations. These ap-\nproaches maximize feature similarities between an image and its augmentations, while minimizing\nsimilarity between negative samples, which are usually randomly sampled images. Some notable\nexamples of positive pairs include temporally adjacent images in videos (Oord et al., 2018), image\naugmentations (Chen et al., 2020a;c), and local crops of a single image (Hjelm et al., 2018). Many\nworks highlight the importance of large numbers of negative samples during training. To this end Wu\net al. (2018) propose keeping a memory bank of negative samples and Chen et al. (2020c) propose\nmomentum updates that can efﬁciently simulate large negative batch sizes. Recently some works\nhave aimed to produce spatially dense feature maps as opposed to a single global vector per image.\nIn this vein, VADeR (Pinheiro et al., 2020) contrasts local per-pixel features based on random com-\npositions of image transformations that induce known correspondences among pixels which act as\npositive pairs for contrastive training. Instead of trying to learn visual features and clustering from\nscratch, STEGO treats pretrained self-supervised features as input and is agnostic to the underlying\nfeature extractor. This makes it easy to integrate future advances in self-supervised feature learning\ninto STEGO.\n2\nPublished as a conference paper at ICLR 2022\nUnsupervised Semantic Segmentation\nMany unsupervised semantic segmentation approaches\nuse techniques from self-supervised feature learning. IIC (Ji et al., 2019) maximizes mutual in-\nformation of patch-level cluster assignments between an image and its augmentations. Contrastive\nClustering (Li et al., 2020), and SCAN (Van Gansbeke et al., 2020) improve on IIC’s image cluster-\ning results with supervision from negative samples and nearest neighbors but do not attempt semantic\nsegmentation. PiCIE (Cho et al., 2021) improves on IIC’s semantic segmentation results by using\ninvariance to photometric effects and equivariance to geometric transformations as an inductive bias.\nIn PiCIE, a network minimizes the distance between features under different transformations, where\nthe distance is deﬁned by an in-the-loop k-means clustering process. SegSort (Hwang et al., 2019)\nadopts a different approach. First, SegSort learns good features using superpixels as proxy seg-\nmentation maps, then uses Expectation-Maximization to iteratively reﬁne segments over a spherical\nembedding space. In a similar vein, MaskContrast (Van Gansbeke et al., 2021) achieves promising\nresults on PascalVOC by ﬁrst using an off-the-shelf saliency model to generate a binary mask for\neach image. MaskContrast then contrasts learned features within and across the saliency masks.\nIn contrast, our method focuses reﬁning existing pretrained self-supervised visual features to distill\ntheir correspondence information and encourage cluster formation. This is similar to the work of\nCollins et al. (2018) who show that low rank factorization of deep network features can be useful\nfor unsupervised co-segmentation. We are not aware of any previous work that achieves the goal\nof high-quality, pixel-level unsupervised semantic segmentation on large scale datasets with diverse\nimages.\nVisual Transformers\nConvolutional neural networks (CNNs) have long been state of the art for\nmany computer vision tasks, but the nature of the convolution operator makes it hard to model long-\nrange interactions. To circumvent such shortcomings, Wang et al. (2018); Zhang et al. (2019) use\nself-attention operations within a CNN to model long range interactions. Transformers (Vaswani\net al., 2017), or purely self-attentive networks, have made signiﬁcant progress in NLP and have re-\ncently been used for many computer vision tasks (Dosovitskiy et al., 2020; Touvron et al., 2021;\nRanftl et al., 2021; Caron et al., 2021). Visual Transformers (ViT) (Vaswani et al., 2017) apply\nself-attention mechanisms to image patches and positional embeddings in order to generate features\nand predictions. Several modiﬁcations of ViT have been proposed to improve supervised learning,\nunsupervised learning, multi-scale processing, and dense predictions. In particular, DINO (Caron\net al., 2021) uses a ViT within a self-supervised learning framework that performs self-distillation\nwith exponential moving average updates. Caron et al. (2021) show that DINO’s class-attention can\nproduce localized and semantically meaningful salient object segmentations. Our work shows that\nDINO’s features not only detect salient objects but can be used to extract dense and semantically\nmeaningful correspondences between images. In STEGO, we reﬁne the features of this pre-trained\nbackbone to yield semantic segmentation predictions when clustered. We focus on DINO’s embed-\ndings because of their quality but note that STEGO can work with any deep network features.\n3\nMETHODS\n3.1\nFEATURE CORRESPONDENCES PREDICT CLASS CO-OCCURRENCE\nRecent progress in self-supervised visual feature learning has yielded methods with powerful and\nsemantically relevant features that improve a variety of downstream tasks. Though most works aim\nto generate a single vector for an image, many works show that intermediate dense features are\nsemantically relevant (Hamilton et al., 2021; Collins et al., 2018; Zhou et al., 2016). To use this\ninformation, we focus on the “correlation volume” (Teed & Deng, 2020) between the dense feature\nmaps. For convolutional or transformer architectures, these dense feature maps can be the activation\nmap of a speciﬁc layer. Additionally, the Q, K or V matrices in transformers can also serve as\ncandidate features, though we ﬁnd these attention tensors do not perform as well in practice. More\nformally, let f ∈RCHW , g ∈RCIJ be the feature tensors for two different images where C\nrepresents the channel dimension and (H, W), (I, J) represent spatial dimensions. We form the\nfeature correspondence tensor:\nFhwij :=\nX\nc\nfchw\n|fhw|\ngcij\n|gij|,\n(1)\nwhose entries represent the cosine similarity between the feature at spatial position (h, w) of feature\ntensor f and position (i, j) of feature tensor g. In the special case where f = g these correspon-\n3\nPublished as a conference paper at ICLR 2022\nFigure 2: Feature correspondences from DINO. Correspon-\ndences between the source image (left) and the target images\n(middle and right) are plotted over the target images in the\nrespective color of the source point (crosses in the left im-\nage). Feature correspondences can highlight key aspects of\nshared semantics within a single image (middle) and across\nsimilar images such as KNNs (right)\nFigure 3: Precision recall curves show\nthat\nfeature\nself-correspondences\nstrongly\npredict\ntrue\nlabel\nco-\noccurrence.\nDINO\noutperforms\nMoCoV2 and a CRF kernel, which\nshows its power as an unsupervised\nlearning signal.\ndences measure the similarity between two regions of the same image. We note that this quantity\nappears often as the “cost-volume” within the optical ﬂow literature, and Hamilton et al. (2021) show\nthis acts a higher-order generalization of Class Activation Maps (Zhou et al., 2016) for contrastive\narchitectures and visual search engines. By examining slices of the correspondence tensor, F, at a\ngiven (h, w) we are able to visualize how two images relate according the featurizer. For example,\nFigure 2 shows how three different points from the source image (shown in blue, red, and green) are\nin correspondence with relevant semantic areas within the image and its K-nearest neighbors with\nrespect to the DINO (Caron et al., 2021) as the feature extractor.\nThis feature correspondence tensor not only allows us to visualize image correspondences but is\nstrongly correlated with the true label co-occurrence tensor. In particular, we can form the ground\ntruth label co-occurrence tensor given a pair of ground-truth semantic segmentation labels k ∈\nCHW , l ∈CIJ where C represents the set of possible classes:\nLhwij :=\n\u001a1,\nif lhw = kij\n0,\nif lhw ̸= kij\nBy examining how well the feature correspondences, F, predict the ground-truth label co-\noccurrences, L, we can measure how compatible the features are with the semantic segmentation\nlabels. More speciﬁcally we treat the feature correspondences as a probability logit and compute\nthe average precision when used as a classiﬁer for L. This approach not only acts as a quick diag-\nnostic tool to determine the efﬁcacy of features, but also allows us to compare with other forms of\nsupervision such as the fully connected Conditional Random Field (CRF) (Kr¨ahenb¨uhl & Koltun,\n2011), which uses correspondences between pixels to reﬁne low-resolution label predictions. In\nFigure 3 we plot precision-recall curves for the DINO backbone, the MoCoV2 backbone, the CRF\nKernel, and our trained STEGO architecture. Interestingly, we ﬁnd that DINO is already a spectacu-\nlar predictor of label co-occurrence within the Coco stuff dataset despite never seeing the labels. In\nparticular, DINO recalls 50% of true label co-occurrences with a precision of 90% and signiﬁcantly\noutperforms both MoCoV2 feature correspondences and the CRF kernel. One curious note is that\nour ﬁnal trained model is a better label predictor than the supervisory signal it learns from. We at-\ntribute this to the distillation process discussed in Section 3.2 which ampliﬁes this supervisory signal\nand drives consistency across the entire dataset. Finally, we stress that our comparison to ground\ntruth labels within this section is solely to provide intuition about the quality of feature correspon-\ndences as a supervisory signal. We do not use the ground truth labels to tune any parameters of\nSTEGO.\n3.2\nDISTILLING FEATURE CORRESPONDENCES\nIn Section 3.1 we have shown that feature correspondences have the potential to be a quality learning\nsignal for unsupervised segmentation. In this section we explore how to harness this signal to create\npixel-wise embeddings that, when clustered, yield a quality semantic segmentation. In particular, we\nseek to learn a low-dimensional embedding that “distills” the feature correspondences. To achieve\n4\nPublished as a conference paper at ICLR 2022\nthis aim, we draw inspiration from the CRF which uses an undirected graphical model to reﬁne noisy\nor low-resolution class predictions by aligning them with edges and color-correlated regions in the\noriginal image.\nMore formally, let N : RC′H′W ′ →RCHW represent a deep network backbone, which maps an\nimage x with C′ channels and spatial dimensions (H′, W ′) to a feature tensor f with C channels\nand spatial dimensions (H, W). In this work, we keep this backbone network frozen and focus on\ntraining a light-weight segmentation head S : RCHW →RKHW , that maps our feature space to\na code space of dimension K, where K < C. The goal of S is to learn a nonlinear projection,\nS(f) =: s ∈RKHW , that forms compact clusters and ampliﬁes the correlation patterns of f.\nTo build our loss function let f and g be two feature tensors from a pair of images x, and y and let\ns := S(f) ∈RCHW and t := S(g) ∈RCIJ be their respective segmentation features. Next, using\nEquation 1 we compute a feature correlation tensor F ∈RHW IJ from f and g and a segmentation\ncorrelation tensor S ∈RHW IJ from s and t. Our loss function aims to push the entries of s and t\ntogether if there is a signiﬁcant coupling between two corresponding entries of f and g. As shown\nin Figure 4, we can achieve this with a simple element-wise multiplication of the tensors F and S:\nLsimple−corr(x, y, b) := −\nX\nhwij\n(Fhwij −b)Shwij\n(2)\nWhere b is a hyper-parameter which adds uniform “negative pressure” to the equation to prevent\ncollapse. Minimizing L with respect to S encourages elements of S to be large when elements\nof F −b are positive and small when elements of F −b are negative. More explicitly, because\nthe elements of F and S are cosine similarities, this exerts an attractive or repulsive force on pairs\nof segmentation features with strength proportional to their feature correspondences. We note that\nthe elements of S are not just encouraged to equal the elements of F but rather to push to total\nanti-alignment (−1) or alignment (1) depending on the sign of F −b.\nIn practice, we found that Lsimple−corr is sometimes unstable and does not provide enough learning\nsignal to drive the optimization. Empirically, we found that optimizing the segmentation features\ntowards total anti-alignment when the corresponding features do not correlate leads to instability,\nlikely because this increases co-linearity. Therefore, we optimize weakly-correlated segmentation\nfeatures to be orthogonal instead. This can be efﬁciently achieved by clamping the segmentation\ncorrespondence, S, at 0, which dramatically improved the optimization stability.\nAdditionally, we encountered challenges when balancing the learning signal for small objects which\nhave concentrated correlation patterns. In these cases, Fhwij −b is negative in most locations, and\nthe loss drives the features to diverge instead of aggregate. To make the optimization more balanced,\nwe introduce a Spatial Centering operation on the feature correspondences:\nF SC\nhwij := Fhwij −1\nIJ\nX\ni′j′\nFhwi′j′.\n(3)\nTogether with the zero clamping, our ﬁnal correlation loss is deﬁned as:\nLcorr(x, y, b) := −\nX\nhwij\n(F SC\nhwij −b)max(Shwij, 0).\n(4)\nWe demonstrate the positive effect of both the aforementioned “0-Clamp” and “SC” modiﬁcations\nin the ablation study of Table 2.\n3.3\nSTEGO ARCHITECTURE\nSTEGO uses three instantiations of the correspondence loss of Equation 4 to train a segmentation\nhead to distill feature relationships between an image and itself, its K-Nearest Neighbors (KNNs),\nand random other images. The self and KNN correspondence losses primarily provide positive,\nattractive, signal and random image pairs tend to provide negative, repulsive, signal. We illustrate\nthis and other major architecture components of STEGO in Figure 4.\nSTEGO is made up of a frozen backbone that serves as a source of learning feedback, and as an\ninput to the segmentation head for predicting distilled features. This segmentation head is a simple\n5\nPublished as a conference paper at ICLR 2022\nImage 2\nImage 1\nRandom \nImage Corr.\nFrozen Visual\nBackbone\nFrozen Visual \nBackbone\nSegmentation \nHead\nSegmentation \nHead\nFeature \nCorrespondences\nSegmentation\nCorrespondences\nCorrespondence Distillation Loss\nTensor \nProduct\nTraining\nPrediction\nVis.Trans.\nSelf Corr.\nSeg. Head\nCRF\nCluster\nKNN Corr.\nFigure 4: High-level overview of the STEGO architecture at train and prediction steps. Grey boxes\nrepresent three different instantiations of the main correspondence distillation loss which is used to\ntrain the segmentation head.\nfeed forward network with ReLU activations (Glorot et al., 2011). In contrast to other works, our\nmethod does not re-train or ﬁne-tune the backbone. This makes our method very efﬁcient to train:\nit only takes less than 2 hours on a single NVIDIA V100 GPU card.\nWe ﬁrst use our backbone to extract global image features by global average pooling (GAP) our\nspatial features: GAP(f). We then construct a lookup table of each image’s K-Nearest Neighbors\naccording to cosine similarity in the backbone’s feature space. Each training minibatch consists of a\ncollection of random images x and random nearest neighbors xknn. In our experiments we sample\nxknn randomly from each image’s top 7 KNNs. We also sample random images, xrand, by shufﬂing\nx and ensuring that no image matched with itself. STEGO’s full loss is:\nL = λselfLcorr(x, x, bself) + λknnLcorr(x, xknn, bknn) + λrandLcorr(x, xrand, brand)\n(5)\nWhere the λ’s and the b’s control the balance of the learning signals and the ratio of positive to\nnegative pressure respectively. In practice, we found that a ratio of λself ≈λrand ≈2λknn worked\nwell. The b parameters tended to be dataset and network speciﬁc, but we aimed to keep the system\nin a rough balance between positive and negative forces. More speciﬁcally we tuned the bs to keep\nmean KNN feature similarity at ≈0.3 and mean random similarity at ≈0.0.\nMany images within the CocoStuff and Cityscapes datasets are cluttered with small objects that are\nhard to resolve at a feature resolution of (40, 40). To better handle small objects and maintain fast\ntraining times we ﬁve-crop training images prior to learning KNNs. This not only allows the network\nto look at closer details of the images, but also improves the quality of the KNNs. More speciﬁcally,\nglobal image embeddings are computed for each crop. This allows the network to resolve ﬁner\ndetails and yields ﬁve times as many images to ﬁnd close matching KNNs from. Five-cropping\nimproved both our Cityscapes results and CocoStuff segmentations, and we detail this in Table 2.\nThe ﬁnal components of our architecture are the clustering and CRF reﬁnement step. Due to the\nfeature distillation process, STEGO’s segmentation features tend to form clear clusters. We apply a\ncosine distance based minibatch K-Means algorithm (MacQueen et al., 1967) to extract these clus-\nters and compute concrete class assignments from STEGO’s continuous features. After clustering,\nwe reﬁne these labels with a CRF to improve their spatial resolution further.\n3.4\nRELATION TO POTTS MODELS AND ENERGY-BASED GRAPH OPTIMIZATION\nEquation 4 can be viewed in the context of Potts models or continuous Ising models from statistical\nphysics (Potts, 1952; Baker Jr & Kincaid, 1979). We brieﬂy overview this connection, and point\ninterested readers to Section A.8 for a more detailed discussion. To build the general Ising model, let\nG = (V, w) be a fully connected, weighted, and undirected graph on |V| vertices. In our applications\nwe take V to be the set of pixels in the training dataset. Let w : V × V →R represent an edge\n6\nPublished as a conference paper at ICLR 2022\nTable 1: Comparison of unsupervised segmentation architectures on 27 class CocoStuff validation\nset. STEGO signiﬁcantly outperforms prior art in both unsupervised clustering and linear-probe\nstyle metrics.\nUnsupervised\nLinear Probe\nModel\nAccuracy\nmIoU\nAccuracy\nmIoU\nResNet50 (He et al., 2016)\n24.6\n8.9\n41.3\n10.2\nMoCoV2 (Chen et al., 2020c)\n25.2\n10.4\n44.4\n13.2\nDINO (Caron et al., 2021)\n30.5\n9.6\n66.8\n29.4\nDeep Cluster (Caron et al., 2018)\n19.9\n-\n-\n-\nSIFT (Lowe, 1999)\n20.2\n-\n-\n-\nDoersch et al. (2015)\n23.1\n-\n-\n-\nIsola et al. (2015)\n24.3\n-\n-\n-\nAC (Ouali et al., 2020)\n30.8\n-\n-\n-\nInMARS (Mirsadeghi et al., 2021)\n31.0\n-\n-\n-\nIIC (Ji et al., 2019)\n21.8\n6.7\n44.5\n8.4\nMDC (Cho et al., 2021)\n32.2\n9.8\n48.6\n13.3\nPiCIE (Cho et al., 2021)\n48.1\n13.8\n54.2\n13.9\nPiCIE + H (Cho et al., 2021)\n50.0\n14.4\n54.8\n14.8\nSTEGO (Ours)\n56.9\n28.2\n76.1\n41.0\nweighting function. Let φ : V →C be a vertex valued function mapping into a generic code space C\nsuch as the probability simplex over cluster labels P(L), or the K-dimensional continuous feature\nspace RK. The function φ can be a parameterized neural network, or a simple lookup table that\nassigns a code to each graph node. Finally, we deﬁne a compatibility function µ : C × C →R\nthat measures the cost of comparing two codes. We can now deﬁne the following graph energy\nfunctional:\nE(φ) :=\nX\nvi,vj∈V\nw(vi, vj)µ(φ(vi), φ(vj))\n(6)\nConstructing the Boltzmann Distribution (Hinton, 2002) yields a normalized distribution over the\nfunction space Φ:\np(φ|w, µ) =\nexp(−E(φ))\nR\nΦ exp(−E(φ′))dφ′\n(7)\nIn general, sampling from this probability distribution is difﬁcult because of the often-intractable\nnormalization factor. However, it is easier to compute the maximum likelihood estimate (MLE),\narg maxφ∈Φ p(φ|w, µ). In particular, if Φ is a smoothly parameterized space of functions and φ and\nµ are differentiable functions, one can compute the MLE using stochastic gradient descent (SGD)\nwith highly-optimized automatic differentiation frameworks (Paszke et al., 2019; Abadi et al., 2015).\nIn Section A.8 of the supplement we prove that the ﬁnding the MLE of Equation 7 is equivalent to\nminimizing the loss of Equation 4 when |V | is the set of pixels in our image training set, φ = S ◦N,\nw is the cosine distance between features, and µ is cosine distance. Like STEGO, the CRF is also a\nPotts model, and we use this connection to re-purpose the STEGO loss function to create continuous,\nminibatch, and unsupervised variants of the CRF. We detail this exploration in Section A.9 of the\nSupplement.\n4\nEXPERIMENTS\nWe evaluate STEGO on standard semantic segmentation datasets and compare with current state-of-\nthe-art. We then justify different design choices of STEGO through ablation studies. Additional de-\ntails on datasets, model hyperparameters, hardware, and other implementation details can be found\nin Section A.10 of the Supplement.\n4.1\nEVALUATION DETAILS\nDatasets\nFollowing Cho et al. (2021), we evaluate STEGO on the 27 mid-level classes of the\nCocoStuff class hierarchy and on the 27 classes of Cityscapes. Like prior art, we ﬁrst resize images\nto 320 pixels along the minor axis followed by a (320 × 320) center crops of each validation image.\nWe use mean intersection over union (mIoU) and Accuracy for evaluation metrics. Our CocoStuff\nevaluation setting originated in Ji et al. (2019) and is common in the literature. Our Cityscapes\n7\nPublished as a conference paper at ICLR 2022\nFigure 5: Comparison of ground truth labels (middle row)\nand cluster probe predictions for STEGO (bottom row) for\nimages from the Cityscapes dataset.\nFigure 6: Confusion matrix of STEGO\ncluster probe predictions on CocoStuff.\nClasses after the “vehicle” class are\n“stuff” and classes before are “things”.\nRows are normalized to sum to 1.\nevaluation setting is adopted from Cho et al. (2021). The latter is newer and more challenging, and\nthus fewer baselines are available. Finally we also compare on the Potsdam-3 setting fro Ji et al.\n(2019) in Section A.2 of the Appendix.\nLinear Probe\nThe ﬁrst way we evaluate the quality of the distilled segmentation features is\nthrough transfer learning effectiveness. As in Van Gansbeke et al. (2021); Cho et al. (2021); Chen\net al. (2020b), we train a linear projection from segmentation features to class labels using the cross\nentropy loss. This loss solely evaluates feature quality and is not part of the STEGO training process.\nClustering\nUnlike the linear probe, the clustering step does not have access to ground truth super-\nvised labels. As in prior art, we use a Hungarian matching algorithm to align our unlabeled clusters\nand the ground truth labels for evaluation and visualization purposes. This measures how consistent\nthe predicted semantic segments are with the ground truth labels and is invariant to permutations of\nthe predicted class labels.\n4.2\nRESULTS\nWe summarize our main results on the 27 classes of CocoStuff in Table 1. STEGO signiﬁcantly\noutperforms the prior state of the art, PiCIE, on both linear probe and clustering (Unsupervised)\nmetrics. In particular, STEGO improves by +14 unsupervised mIoU, +6.9 unsupervised accuracy,\n+26 linear probe mIoU, and +21 linear probe accuracy compared to the next best baseline. In Table 3,\nwe ﬁnd a similarly large improvement of +8.7 unsupervised mIoU and +7.7 unsupervised accuracy\non the Cityscapes validation set. These two experiments demonstrate that even though we do not\nﬁne-tune the backbone for these datasets, DINO’s self-supervised weights on ImageNet (Deng et al.,\n2009) are enough to simultaneously solve both settings. STEGO also outperforms simply clustering\nthe features from unmodiﬁed DINO, MoCoV2, and ImageNet supervised ResNet50 backbones. This\ndemonstrates the beneﬁts of training a segmentation head to distill feature correspondences.\nWe show some example segmentations from STEGO and our baseline PiCIE on the CocoStuff\ndataset in Figure 1. We include additional examples and failure cases in Sections A.4 and A.5. We\nnote that STEGO is signiﬁcantly better at resolving ﬁne-grained details within the images such as\nthe legs of horses in the third image from the left column of Figure 1, and the individual birds in the\nright-most column. Though the PiCIE baseline uses a feature pyramid network to output high reso-\nlution predictions, the network does not attune to ﬁne grained details, potentially demonstrating the\nlimitations of the sparse training signal induced by data augmentations alone. In contrast, STEGO’s\npredictions capture small objects and ﬁne details. In part, this can be attributed to DINO backbone’s\nhigher resolution features, the 5-crop training described in 3.3, and the CRF post-processing which\nhelps to align the predictions to image edges. We show qualitative results on the Cityscapes dataset\nin Figure 5. STEGO successfully identiﬁes people, street, sidewalk, cars, and street signs with high\n8\nPublished as a conference paper at ICLR 2022\nTable 2: Architecture ablation study on the CocoStuff\nDataset (27 Classes).\nArch.\n0-Clamp\n5-Crop\nSC\nCRF\nUnsup.\nLinear Probe\nAcc. mIoU Acc.\nmIoU\nMoCoV2 ✓\n48.4\n20.8\n70.7\n26.5\nViT-S\n34.2\n7.3\n54.9\n15.6\nViT-S\n✓\n44.3\n21.3\n70.9\n36.8\nViT-S\n✓✓\n47.6\n23.4\n72.2\n36.8\nViT-S\n✓✓✓\n47.7\n24.0\n72.9\n38.4\nViT-S\n✓✓✓✓48.3\n24.5\n74.4\n38.3\nViT-B\n✓✓✓\n54.8\n26.8\n74.3\n39.5\nViT-B\n✓✓✓✓56.9\n28.2\n76.1\n41.0\nTable 3:\nResults on the Cityscapes\nDataset (27 Classes). STEGO improves\nsigniﬁcantly over all baselines in both ac-\ncuracy and mIoU.\nUnsup.\nModel\nAcc. mIoU\nIIC (Ji et al., 2019)\n47.9\n6.4\nMDC (Cho et al., 2021) 40.7\n7.1\nPiCIE (Cho et al., 2021) 65.5\n12.3\nSTEGO (Ours)\n73.2\n21.0\ndetail and ﬁdelity. We note that prior works did not publish pretrained models or linear probe results\non Cityscapes so we exclude this information from Table 3 and Figure 5.\nTo better understand the predictions and failures of STEGO, we include confusion matrices for\nCocoStuff (Figure 6) and Cityscapes (Figure 11 of the Supplement). Some salient STEGO errors\ninclude confusing the “food” category from the CocoStuff “things”, and the “food” category from\nCocoStuff “stuff”. STEGO also does not properly separate “ceilings” from “walls”, and lacks con-\nsistent segmentations for classes such as “indoor”, “accessory”, “rawmaterial” and “textile”. These\nerrors also draw our attention to the challenges of evaluating unsupervised segmentation methods:\nlabel ontologies can be arbitrary. In these circumstances the divisions between classes are not well\ndeﬁned and it is hard to imagine a system that can segment the results consistently without additional\ninformation. In these regimes, the linear probe provides a more important barometer for quality be-\ncause the limited supervision can help disambiguate these cases. Nevertheless, we feel that there is\nstill considerable progress to be made on the purely unsupervised benchmark, and that even with the\nimprovements of STEGO there is still a measurable performance gap with supervised systems.\n4.3\nABLATION STUDY\nTo understand the impact of STEGO’s architectural components we perform an ablation analysis\non the CocoStuff dataset, and report the results in Table 2. We examine the effect of using several\ndifferent backbones in STEGO including MoCoV2, the ViT-Small, and ViT-Base architectures of\nDINO. We ﬁnd that ViT-Base is the best feature extractor of the group and leads by a signiﬁcant\nmargin both in terms of accuracy and mIoU. We also evaluate the several loss function and architec-\nture decisions described in Section 3.3. In particular, we explore clamping the segmentation feature\ncorrespondence tensor at 0 to prevent the negative pressure from introducing co-linearity (0-Clamp),\nﬁve-cropping the dataset prior to mining KNNs to improve the resolution of the learning signal (5-\nCrop), spatially centering the feature correspondence tensor to improve resolution of small objects\n(SC), and Conditional Random Field post-processing to reﬁne predictions (CRF). We ﬁnd that these\nmodiﬁcations improve both the cluster and linear probe evaluation metrics.\n5\nCONCLUSION\nWe have found that modern self-supervised visual backbones can be reﬁned to yield state of the\nart unsupervised semantic segmentation methods. We have motivated this architecture by show-\ning that correspondences between deep features are directly correlated with ground truth label co-\noccurrence. We take advantage of this strong, yet entirely unsupervised, learning signal by intro-\nducing a novel contrastive loss that “distills” the correspondences between features. Our system,\nSTEGO, produces low rank representations that cluster into accurate semantic segmentation pre-\ndictions. We connect STEGO’s loss to CRF inference by showing it is equivalent to MLE in Potts\nmodels over the entire collection of pixels in our dataset. We show STEGO yields a signiﬁcant\nimprovement over the prior state of the art, on both the CocoStuff (+14 mIoU) and Cityscapes (+9\nmIoU) semantic segmentation challenges. Finally, we justify the architectural decisions of STEGO\nwith an ablation study on the CocoStuff dataset.\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGMENTS\nWe would like to thank Karen Hamilton for proofreading the work and Siddhartha Sen for spon-\nsoring access to the Microsoft Research compute infrastructure. We also thank Jang Hyun Cho for\nhelping us run and evaluate the PiCIE baseline. We thank Kavital Bala, Vincent Sitzmann, Marc\nBosch, Desalegn Delelegn, Cody Champion, and Markus Weimer for their helpful commentary on\nthe work.\nThis material is based upon work supported by the National Science Foundation Graduate Research\nFellowship under Grant No. 2021323067. Any opinion, ﬁndings, and conclusions or recommenda-\ntions expressed in this material are those of the authors(s) and do not necessarily reﬂect the views of\nthe National Science Foundation. This research is based upon work supported in part by the Ofﬁce of\nthe Director of National Intelligence (Intelligence Advanced Research Projects Activity) via 2021-\n20111000006. The views and conclusions contained herein are those of the authors and should not\nbe interpreted as necessarily representing the ofﬁcial policies, either expressed or implied, of ODNI,\nIARPA, or the U S Government. The US Government is authorized to reproduce and distribute\nreprints for governmental purposes notwithstanding any copyright annotation therein. This work is\nsupported by the National Science Foundation under Cooperative Agreement PHY-2019786 (The\nNSF AI Institute for Artiﬁcial Intelligence and Fundamental Interactions, http://iaiﬁ.org/)\nREFERENCES\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\nKudlur, Josh Levenberg, Dan Man´e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,\nMike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-\ncent Vanhoucke, Vijay Vasudevan, Fernanda Vi´egas, Oriol Vinyals, Pete Warden, Martin Watten-\nberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\non heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from\ntensorﬂow.org.\nJiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance segmentation\nwith inter-pixel relations. CoRR, abs/1904.05044, 2019. URL http://arxiv.org/abs/\n1904.05044.\nGeorge A Baker Jr and John M Kincaid. Continuous-spin ising model and λ: φ 4: d ﬁeld theory.\nPhysical Review Letters, 42(22):1431, 1979.\nHakan Bilen, Rodrigo Benenson, and Seong Joon Oh. Eccv 2020 tutorial on weakly-supervised\nlearning in computer vision.\nURL https://github.com/hbilen/wsl-eccv20.\ngithub.io.\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1209–\n1218, 2018.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-\npervised learning of visual features. In Proceedings of the European Conference on Computer\nVision (ECCV), pp. 132–149, 2018.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin.\nEmerging properties in self-supervised vision transformers.\narXiv preprint\narXiv:2104.14294, 2021.\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Se-\nmantic image segmentation with deep convolutional nets and fully connected crfs. arXiv preprint\narXiv:1412.7062, 2014.\nLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous\nconvolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\n10\nPublished as a conference paper at ICLR 2022\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-\nsupervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020a.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020b.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020c.\nJang Hyun Cho, U. Mall, K. Bala, and Bharath Hariharan. Picie: Unsupervised semantic segmenta-\ntion using invariance and equivariance in clustering. ArXiv, abs/2103.17070, 2021.\nEdo Collins, Radhakrishna Achanta, and Sabine Susstrunk. Deep feature factorization for concept\ndiscovery. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 336–352,\n2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\ncontext prediction. In Proceedings of the IEEE international conference on computer vision, pp.\n1422–1430, 2015.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\nWilliam\nFalcon\net\nal.\nPytorch\nlightning.\nGitHub.\nNote:\nhttps://github.com/PyTorchLightning/pytorch-lightning, 3, 2019.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by\npredicting image rotations. arXiv preprint arXiv:1803.07728, 2018.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In\nProceedings of the fourteenth international conference on artiﬁcial intelligence and statistics, pp.\n315–323. JMLR Workshop and Conference Proceedings, 2011.\nMark Hamilton, Scott Lundberg, Lei Zhang, Stephanie Fu, and William T Freeman. Model-agnostic\nexplainability for visual search. arXiv preprint arXiv:2103.00370, 2021.\nYufei Han and Maurizio Filippone. Mini-batch spectral clustering. In 2017 International Joint\nConference on Neural Networks (IJCNN), pp. 3888–3895. IEEE, 2017.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nGeoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural\ncomputation, 14(8):1771–1800, 2002.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. Learning deep representations by mutual information estimation\nand maximization. arXiv preprint arXiv:1808.06670, 2018.\nJyh-Jing Hwang, Stella X. Yu, Jianbo Shi, Maxwell D. Collins, Tien-Ju Yang, Xiao Zhang, and\nLiang-Chieh Chen.\nSegsort: Segmentation by discriminative sorting of segments.\nCoRR,\nabs/1910.06962, 2019. URL http://arxiv.org/abs/1910.06962.\nPhillip Isola, Daniel Zoran, Dilip Krishnan, and Edward H Adelson. Learning visual groups from\nco-occurrences in space and time. arXiv preprint arXiv:1511.06811, 2015.\nMax Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer\nnetworks. arXiv preprint arXiv:1506.02025, 2015.\n11\nPublished as a conference paper at ICLR 2022\nXu Ji, Jo˜ao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised\nimage classiﬁcation and segmentation. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 9865–9874, 2019.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nPhilipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference in fully connected crfs with gaussian\nedge potentials. Advances in neural information processing systems, 24:109–117, 2011.\nJohn Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random ﬁelds: Proba-\nbilistic models for segmenting and labeling sequence data. 2001.\nOmer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. Advances\nin neural information processing systems, 27:2177–2185, 2014.\nYunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive cluster-\ning. arXiv preprint arXiv:2009.09687, 2020.\nYun Liu, Yu-Huan Wu, Peisong Wen, Yujun Shi, Yu Qiu, and Ming-Ming Cheng.\nLeveraging\ninstance-, image- and dataset-level information for weakly supervised instance segmentation.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. doi: 10.1109/TPAMI.\n2020.3023152.\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\nsegmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npp. 3431–3440, 2015.\nDavid G Lowe. Object recognition from local scale-invariant features. In Proceedings of the seventh\nIEEE international conference on computer vision, volume 2, pp. 1150–1157. Ieee, 1999.\nJames MacQueen et al. Some methods for classiﬁcation and analysis of multivariate observations. In\nProceedings of the ﬁfth Berkeley symposium on mathematical statistics and probability, volume 1,\npp. 281–297. Oakland, CA, USA, 1967.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representa-\ntions of words and phrases and their compositionality. arXiv preprint arXiv:1310.4546, 2013.\nS Ehsan Mirsadeghi, Ali Royat, and Hamid Rezatoﬁghi.\nUnsupervised image segmentation by\nmutual information maximization and adversarial regularization. IEEE Robotics and Automation\nLetters, 6(4):6931–6938, 2021.\nAnnamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu,\nand Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. arXiv preprint\narXiv:1707.05005, 2017.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\nYassine Ouali, C´eline Hudelot, and Myriam Tami. Autoregressive unsupervised image segmenta-\ntion. In European Conference on Computer Vision, pp. 142–158. Springer, 2020.\nShun-Yi Pan, Cheng-You Lu, Shih-Po Lee, and Wen-Hsiao Peng. Weakly-supervised image seman-\ntic segmentation using graph convolutional networks. In 2021 IEEE International Conference on\nMultimedia and Expo (ICME), pp. 1–6. IEEE, 2021.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance\ndeep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran\nAssociates, Inc., 2019.\n12\nPublished as a conference paper at ICLR 2022\nDeepak Pathak, Philipp Kr¨ahenb¨uhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context en-\ncoders: Feature learning by inpainting. In CVPR, 2016.\nFabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:\nMachine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011.\nPedro O Pinheiro, Amjad Almahairi, Ryan Y Benmalek, Florian Golemo, and Aaron Courville.\nUnsupervised learning of dense visual representations. arXiv preprint arXiv:2011.05499, 2020.\nRenfrey Burnard Potts. Some generalized order-disorder transformations. In Mathematical pro-\nceedings of the cambridge philosophical society, volume 48, pp. 106–109. Cambridge University\nPress, 1952.\nRen´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.\nArXiv preprint, 2021.\nZhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-Yu Liu, Alexander G Schwing, and Jan Kautz.\nUfo2: A uniﬁed framework towards omni-supervised object detection. In European Conference\non Computer Vision, pp. 288–313. Springer, 2020.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overﬁtting. The journal of machine\nlearning research, 15(1):1929–1958, 2014.\nZachary Teed and Jia Deng. RAFT: recurrent all-pairs ﬁeld transforms for optical ﬂow. CoRR,\nabs/2003.12039, 2020. URL https://arxiv.org/abs/2003.12039.\nMarvin TT Teichmann and Roberto Cipolla. Convolutional crfs for semantic segmentation. arXiv\npreprint arXiv:1805.04777, 2018.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv´e J´egou.\nTraining data-efﬁcient image transformers & distillation through attention.\nIn\nInternational Conference on Machine Learning, pp. 10347–10357. PMLR, 2021.\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc\nVan Gool. Scan: Learning to classify images without labels. In European Conference on Com-\nputer Vision, pp. 268–285. Springer, 2020.\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, and Luc Van Gool.\nUn-\nsupervised semantic segmentation by contrasting object mask proposals.\narxiv preprint\narxiv:2102.06191, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and\ncomposing robust features with denoising autoencoders. In Proceedings of the 25th international\nconference on Machine learning, pp. 1096–1103, 2008.\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794–7803,\n2018.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-\nparametric instance discrimination. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 3733–3742, 2018.\nDonghui Yan, Ling Huang, and Michael I Jordan. Fast approximate spectral clustering. In Pro-\nceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data\nmining, pp. 907–916, 2009.\n13\nPublished as a conference paper at ICLR 2022\nHongshan Yu, Zhengeng Yang, Lei Tan, Yaonan Wang, Wei Sun, Mingui Sun, and Yandong Tang.\nMethods and datasets on semantic segmentation: A review. Neurocomputing, 304:82–103, 2018.\nHan Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena.\nSelf-attention generative\nadversarial networks. In International conference on machine learning, pp. 7354–7363. PMLR,\n2019.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning\nby cross-channel prediction. In CVPR, 2017.\nXuewen Zhang, Selene E Chew, Zhenlin Xu, and Nathan D Cahill. Slic superpixels for efﬁcient\ngraph-based dimensionality reduction of hyperspectral imagery. In Algorithms and Technolo-\ngies for Multispectral, Hyperspectral, and Ultraspectral Imagery XXI, volume 9472, pp. 947209.\nInternational Society for Optics and Photonics, 2015.\nBolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep\nfeatures for discriminative localization.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 2921–2929, 2016.\nAleksandar Zlateski, Ronnachai Jaroensri, Prafull Sharma, and Fr´edo Durand. On the importance\nof label quality for semantic segmentation. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 1479–1487, 2018.\n14\nPublished as a conference paper at ICLR 2022\nA\nAPPENDIX\nA.1\nVIDEO AND CODE\nWe include a short video description of our work at https://aka.ms/stego-video.\nWe also provide training and evaluation code at https://aka.ms/stego-code\nA.2\nADDITIONAL RESULTS ON THE POTSDAM-3 DATASET\nIn addition to our evaluations in Section 4.1 we compare STEGO to prior art on the Potsdam 3-class\naerial image segmentation task presented in Ji et al. (2019). In Table ?? We ﬁnd that STEGO is\nable to achieve +12% accuracy compared to the previous state of the art, IIC. We show example\nqualitative results in Figure 7.\nTable 4: Additional results on the Potsdam-3 aerial image segmentation challenge\nModel\nUnsup. Acc.\nRandom CNN (Ji et al., 2019)\n38.2\nK-Means (Pedregosa et al., 2011)\n45.7\nSIFT (Lowe, 1999)\n38.2\nDoersch et al. (2015)\n49.6\nIsola et al. (2015)\n63.9\nDeep Cluster (Caron et al., 2018)\n41.7\nIIC (Ji et al., 2019)\n65.1\nSTEGO (Ours)\n77.0\nFigure 7: Qualitative comparison of STEGO segmentation results on the Potsdam-3 segmentation\nchallenge.\n15\nPublished as a conference paper at ICLR 2022\nA.3\nADDITIONAL ABLATION STUDY\nIn addition to the ablation study of Table 2, we investigate the effect of each major architectural\ndecision in isolation. We ﬁnd that in most metrics, removing each architectural component hurts\nperformance.\nTable 5: Additional architecture ablation study on the CocoStuff Dataset (27 Classes).\n0-Clamp\n5-Crop\nPointwise\nCRF\nSelf-Loss\nKNN-Loss\nRand-Loss\nUnsupervised Linear Probe\nBackbone\nAcc.\nmIoU\nAcc.\nmIoU\nViT-Small ✓✓✓✓✓✓✓48.3\n24.5\n74.4\n38.3\nMoCoV2\n✓✓✓✓✓✓✓43.1\n19.6\n65.9\n26.0\nViT-Small\n✓✓✓✓✓✓42.8\n10.3\n59.3\n19.3\nViT-Small ✓\n✓✓✓✓✓48.0\n23.1\n73.9\n38.9\nViT-Small ✓✓\n✓✓✓✓50.2\n22.3\n73.7\n37.7\nViT-Small ✓✓✓\n✓✓✓47.7\n24.0\n72.9\n38.4\nViT-Small ✓✓✓✓\n✓✓43.0\n20.2\n73.0\n36.2\nViT-Small ✓✓✓✓✓\n✓47.0\n22.2\n74.0\n37.7\nViT-Small ✓✓✓✓✓✓\n39.8\n12.8\n65.5\n29.9\n16\nPublished as a conference paper at ICLR 2022\nA.4\nADDITIONAL QUALITATIVE RESULTS\nFigure 8: Additional unsupervised semantic segmentation predictions on the CocoStuff 27 class\nsegmentation challenge using STEGO (Ours) and the prior state of the art, PiCIE. Images are not\ncurated.\n17\nPublished as a conference paper at ICLR 2022\nA.5\nFAILURE CASES\nUnsupervised Segmentation is prone to a variety of issues. We include some of the following to\nsegmentations to demonstrate cases where STEGO breaks down. In the ﬁrst column of Figure 9 we\ncan see that STEGO improperly segments ground from trees and backgrounds. In the second column\nwe see that STEGO makes an understandable error and assigns the barn ﬂoor to the “outdoor” class\nand the barn wall to the “building” class. In the third column STEGO misses the boundary between\nwall and ceiling. The fourth column demonstrates the challenge between food (thing) and food\n(stuff) characterization. Interestingly PiCIE makes the same type of error both here, and in the barn\ncase. The last column shows an example of STEGO missing a human in the lower left. In this image\nit is challenging to spot the person, probably because it is grayscale.\nFigure 9: STEGO failure cases.\n18\nPublished as a conference paper at ICLR 2022\nA.6\nFEATURE CORRESPONDENCES PREDICT STEGO’S ERRORS\nSection 3.1 demonstrates how unsupervised feature correspondences serve as an excellent proxy for\nthe true label co-occurrence information. In this section we explore how and where DINO’s feature\ncorrespondences systematically differ from the ground truth labels, and show that these insights\nallow us to directly predict STEGO’s ﬁnal confusion matrix.\nMore speciﬁcally we consider the setting of Section 3.1. Instead of computing precision-recall\ncurves from our feature correspondence scores we can instead threshold these scores, select the\nstrongest couplings between the images, and evaluate whether these couplings are between objects\nof the same class or objects of different classes. In particular, Figure 10 shows a confusion matrix\ncapturing how well DINO feature correspondences between images and their K-Nearest Neighbors\nalign with the ground truth label ontology in the CocoStuff27 dataset. We ﬁnd that that this analysis\npredicts many of the areas where the ﬁnal STEGO architecture fails. In particular, we can see that\nDINO conﬂates the “Food (things)” and “Food (stuff)” and this error also appears in STEGO’s\nconfusion matrix in Figure 12. Likewise both visualizations show confusion between “appliance”\nand “furniture”, “window” and “wall”, and several other common errors.\nThis analysis demonstrates that many of STEGO’s errors originate from the structure of the DINO\nfeatures used to train STEGO as opposed to other aspects of the architecture. However we note that\nthe question of whether whether this is an issue with the DINO features, or due to ambiguities in the\nCocoStuff label ontology is still outstanding. Finally we note that this analysis is able to predict the\nresults of a fully-trained STEGO architecture, and could be used as a way to select better backbones\nwithout having to training STEGO.\nFigure 10: Normalized matrix of predicted label co-occurrences between an Images and KNNs.\nThis analysis shows where our unsupervised supervisory signal, the DINO feature correspondences,\nfails to align with the CocoStuff27 label ontology.\n19\nPublished as a conference paper at ICLR 2022\nA.7\nHIGHER RESOLUTION CONFUSION MATRICES\nFigure 11: Confusion Matrix for Cityscapes predictions\nFigure 12: Confusion Matrix for CocoStuff predictions\n20\nPublished as a conference paper at ICLR 2022\nA.8\nRELATIONSHIP WITH GRAPH ENERGY MINIMIZATION\nIn section 3.4 we brieﬂy mention that STEGO’s feature correlation distillation loss deﬁned in Equa-\ntion 4 can be seen as a particular case of Maximum Likelihood (ML) estimation on a undirected\ngraphical model or Ising model. In this section we demonstrate this connection in greater detail\nusing the formalism deﬁned in 3.4. In particular, we recall the energy for a Potts model:\nE(φ) :=\nX\nvi,vj∈V\nw(vi, vj)µ(φ(vi), φ(vj))\n(8)\nWe then construct the Boltzmann Distribution (Hinton, 2002) yields a normalized distribution over\nthe function space Φ:\np(φ|w, µ) =\nexp(−E(φ))\nR\nΦ exp(−E(φ′))dφ′\n(9)\nIn general, sampling from this probability distribution is difﬁcult because of the often-intractable\nnormalization factor. However, it is easier to compute the maximum likelihood estimate (MLE):\narg max\nφ∈Φ\np(φ|w, µ) = arg max\nφ∈Φ\n1\nZ exp(−E(φ))\n(10)\nWhere Z is the unknown constant normalization factor. Simplifying the right-hand side yields:\narg max\nφ∈Φ\np(φ|w, µ) = arg min\nφ∈Φ\nE(φ) = arg min\nφ∈Φ\nX\nvi,vj∈V\nw(vi, vj)µ(φ(vi), φ(vj))\n(11)\nWe are now in the position to connect this to the STEGO loss function. First, we take our nodes\nV to be the set of all spatial locations across our entire dataset of images. For concreteness we can\nrepresent v ∈V by the tuple (n, h, w) where h, w represent height and width n represents the image\nnumber. We now let φ(vi) be the output of the segmentation head, svi, at the image and spatial\nlocation vi. Using cosine distance, dcos(x, y) = 1 −\nx\n|x|\ny\n|y| as the compatibility function, µ, yields\nthe following:\n= arg min\nS\nX\nvi,vj∈V\n−w(vi, vj) svi\n|svi|\nsvj\n|svj|\n(12)\nWherte the argmin now ranges over the parameters of the segmentation head S. We can now observe\nthat the sum over all pairs vi, vj ∈V can be written as a sum over pairs of images x, y ∈X and\npairs of spatial locations (h, w), (i, j) where we note that (i, j) in this context refers to the spatial\ncoordinates of image y as in 3.1 and not the indices of the vertices.\n= arg min\nS\nX\nx,y∈X\nX\nhwij\n−W(x, y)hwijS(x, y)hwij\n(13)\nWhere we deﬁne S(x, y) to be the segmentation feature correlation tensor for images x, y as deﬁned\nin Section 3.2. Finally letting W(x, y)hwij = Fhwij −b we recover our loss:\narg max\nφ∈Φ\np(φ|w, µ) = arg min\nS\nX\nx,y∈X\nLsimple−corr(x, y, b)\n(14)\nFinally we note that in practice we approximate the minimization using minibatch SGD, and our\ninclusion of KNN and Self-correspondence distillation changes the weight function w, but does not\nchange its functional form.\n21\nPublished as a conference paper at ICLR 2022\nSwitching to the ML formulation of this problem allows us to solve this optimization for φ by\ngradient descent on the parameters of the segmentation head, S, and makes this computationally\ntractable. For large image datasets that can contain millions of high-resolution images, the induced\ngraph can contain billions of image locations. Other graph embedding and clustering approaches\nsuch as Spectral methods require solving for eigenvalues of the graph Laplacian, which can take\nO(|V|3) time (Yan et al., 2009). More recent attempts to accelerate Spectral clustering such as\n(Yan et al., 2009) and (Han & Filippone, 2017) further assume a “Nonparametric” structure on the\nfunction φ, where a separate cluster assignment is learned for each vertex. This assumption of a\n“nonparametric” function φ can be undesirable as one cannot cluster or embed new data without\nrecomputing the entire clustering. In contrast, STEGO’s backbone and segmentation head act as a\nparametric form for the function φ allowing the approach to output predictions for novel images.\nA.9\nCONTINUOUS, UNSUPERVISED, AND MINI-BATCH CRF\nFigure 13: Unsupervised CRF solutions for discrete (middle) and continuous (right) code spaces. In\nthe discrete case we mark the boundaries between classes, in the continuous case we visualize the\ntop 3 dimensions of the code space.\nFully connected Gaussian Conditional Random Fields (CRFs) (Lafferty et al., 2001) are an ex-\ntremely popular addition to semantic segmentation architectures. The CRF has the ability to improve\ninitial predictions of locations, and can “sharpen” predictions to make them consistent with edges\nand areas with consistent color in the original image. CRF post-processing for reﬁning supervised\nand weakly supervised semantic segmentation predictions is ubiquitous in the literature (Lafferty\net al., 2001; Chen et al., 2014; Long et al., 2015; Liu et al., 2020; Ahn et al., 2019). Recently, new\nconnections between CRF message passing and convolutional networks have allowed CRFs to be\nembedded into existing models (Chen et al., 2017; Teichmann & Cipolla, 2018) and trained jointly\nfor better performance. By connecting the STEGO correspondence distillation loss to the energy\nof an undirected model on image pixels we can use the same minibatch MLE strategy to estimate\nother similar graphical models. For example, in the fully connected Gaussian edge potential CRF,\none forms a pairwise potential function potential function for the pixels of a single image:\nwcrf(vi, vj) = a exp\n \n−|pi −pj|2\n2θ2α\n−|Ii −Ij|2\n2θ2\nβ\n!\n+ b exp\n\u0012\n−|pi −pj|2\n2θ2γ\n\u0013\n(15)\nWhere pi represent the pixel coordinates associated with node vi and Ii represents pixel colors as-\nsociated with node vi. The parameters a, b, θα, θβ, θγ are hyperparameters and control the behavior\nof the model. These parameters balance the effect of long- and short-range color similarities against\n22\nPublished as a conference paper at ICLR 2022\nsmoothness. The CRF directly learns a pixel-wise array of probabilistic class assignments over k\nlabels corresponding to the probability simplex code space C = P(l) and a non-parametric clus-\ntering function f. For a compatibility function µ the CRF chooses the Potts Model (Potts, 1952):\nµpotts(φ(vi), φ(vj)) := P(φ(vi) ̸= φ(vj)).\nWith this setting of the weights and compatibility function, we directly recover the binary potentials\nof the fully connected Gaussian edge potential CRF (Kr¨ahenb¨uhl & Koltun, 2011). We can also\nadd the unary potentials which are often the outputs of another model. However, for our analysis\nwe explore the case without unary potentials which yields an “unsupervised” variant of the CRF.\nHowever, without external unary potential terms, the strictly positive similarity kernel encourages\nthe maximum likelihood estimator (MLE) of the graph to be the constant function. To rectify this,\nwe can add small negative constant, −b, to the weight tensor to push unrelated pixels apart. This\nnegative force is the direct analogue of the negative pressure hyper-parameter in STEGO and can\nbe interpreted through the lens of negative sampling (Mikolov et al., 2013). This negative shift\nalso appears in the word2vec and graph2vec embedding techniques (Narayanan et al., 2017; Levy\n& Goldberg, 2014). Our shifted CRF potential encourages natural clusters to form that respect\nthe structure of the potentials that capture similarities in pixel colors and locations. In the discrete\ncase, solutions to this equation resemble superpixel algorithms such as SLIC (Zhang et al., 2015).\nAdditionally lifting this to the continuous code space and provide a natural continuous generalization\nof superpixels and seems to avoid challenging local minima. We illustrate these solutions to just the\nunsupervised CRF potential in Figure 13. Finally, we note that the second term of Equation 15,\nreferred to as the smoothness kernel, matches IIC’s notion of local class consistency. However,\nwe found that adding these CRF terms to the self-correspondence loss of STEGO did not improve\nperformance.\n23\nPublished as a conference paper at ICLR 2022\nA.10\nIMPLEMENTATION DETAILS\nModel\nSTEGO uses the “ViT-Base” architecture of DINO pre-trained on ImageNet. This back-\nbone was trained using self-supervision without access to ground-truth labels. We use the “teacher”\nweights when creating our backbone. We take the ﬁnal layer of spatially varying features and ap-\nply a small amount (p = 0.1) of channel-wise dropout (Srivastava et al., 2014) before using them\nthroughout the architecture during training. Our segmentation head consists of a linear network and\na two-layer ReLU MLP added together and outputs a 70 dimensional vector. We use the Adam\noptimizer (Kingma & Ba, 2014) with a learning rate of 0.0005 and a batch size of 32. To make\nour losses resolution independent we sample 121 random spatial locations in the source and target\nimplementations and use grid sampling (Jaderberg et al., 2015) to sample features from the back-\nbone and segmentation heads. Our cluster probe is trained alongside the STEGO architecture using\na minibatch k-means loss where closeness is measured by cosine distance. Cluster and linear probes\nare trained with separate Adam optimizers using a learning rate of .005\nDatasets\nWe use the training and validation sets of Cocostuff described ﬁrst in Ji et al. (2019) and\nused throughout the literature including in Cho et al. (2021). We note that the validation set used in\nJi et al. (2019) is a subset of the full CocoStuff validation set and we use this validation subset to be\nconsistent with prior benchmarks. We note that using the full validation set does not change results\nsigniﬁcantly. When ﬁve-cropping images we use a target size of (.5h, .5w) for each crop where h, w\nare the original image height and width. Training images are then scaled to have minor axis equal\nto 224 and are then center cropped to (224, 224), validation images are ﬁrst scaled to 320 then are\ncenter cropped to (320, 320). All image resizing uses bilinear interpolation and resizing of target\ntensors for evaluation uses nearest neighbor interpolation.\nCRF\nWe use PyDenseCRF (Kr¨ahenb¨uhl & Koltun, 2011) with 10 iterations with parameters a =\n4, b = 3, θα = 67, θβ = 3, θγ = 1 as written in Section A.9.\nCompute\nAll experiments use PyTorch (Paszke et al., 2019) v1.7 pre-trained models, on an\nUbuntu 16.04 Azure NV24 Virtual Machine with Python 3.6. Experiments use PyTorch Lightning\nfor distributed and multi-gpu training when necessary (Falcon et al., 2019).\nHyperparameters\nWe use the following hyperparameters for our results in Tables 1 and 3:\nTable 6: Hyperparameters used in STEGO\nParameter Cityscapes CocoStuff\nλrand\n0.91\n0.15\nλknn\n0.58\n1.00\nλself\n1.00\n0.10\nbrand\n0.31\n1.00\nbknn\n0.18\n0.20\nbself\n0.46\n0.12\n24\nPublished as a conference paper at ICLR 2022\nA.11\nA HEURISTIC FOR SETTING HYPER-PARAMETERS\nSetting hyperparameters without cross-validation on ground truth data can be difﬁcult and this is\nan outstanding challenges with the STEGO architecture that we hope can be solved in future work.\nNevertheless we have identiﬁed some key intuition to guide manual hyperparameter tuning. More\nspeciﬁcally, we ﬁnd that the most important factor affecting performance is the balance of positive\nand negative forces. Too much negative feedback and vectors will all push apart and clusters will\nnot form well, too much positive feedback and the system will tend towards a small number of\nclusters. To debug this balance, we found it useful to visualize the distribution of feature correspon-\ndence similarities as a function of training step as shown in Figure 14. A balanced system (Orange\ndistribution) will tend towards a bi-modal distribution with peaks at alignment 1 or orthogonality\nat 0. This bi-modal structure is indicative that there is some clustering within images, but that not\neverything is assigned to the same cluster. Pink and blue distributions show too much positive and\nnegative signal respectively. We ﬁnd that given a reasonable balance of the λ’s, this balance can be\nachieved by tuning the bs to achieve the desired balance.\nFigure 14: Distributions of feature correspondences between an image and itself across three dif-\nferent hyper-parameter settings. The orange curve and distribution shows a proper balance between\nattractive and repulsive forces allowing some pairs features to cluster together (the peak at 1) and\nother pairs of features to orthogonalize (the peak at 0)\n25\nPublished as a conference paper at ICLR 2022\nA.12\nA NOTE ON 5-CROP NEAREST NEIGHBORS\nWe found that pre-processing the dataset by 5-cropping images was a simple and effective way to\nimprove the spatial resolution of STEGO and the quality of K-Nearest Neighbors. We consider each\nresulting 5-crop as a separate image when computing KNNs and patches from the same image are\nvalid KNNs. Figure 15 shows the distribution of these self-matches for the CocoStuff dataset. We\nnote that the majority of patches do not have any nearest neighbors from the same image.\nFigure 15: Number of patches from the same image found within each patch’s 7 nearest neighbors\n26\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2022-03-16",
  "updated": "2022-03-16"
}