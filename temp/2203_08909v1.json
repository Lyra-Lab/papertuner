{
  "id": "http://arxiv.org/abs/2203.08909v1",
  "title": "Morphological Processing of Low-Resource Languages: Where We Are and What's Next",
  "authors": [
    "Adam Wiemerslage",
    "Miikka Silfverberg",
    "Changbing Yang",
    "Arya D. McCarthy",
    "Garrett Nicolai",
    "Eliana Colunga",
    "Katharina Kann"
  ],
  "abstract": "Automatic morphological processing can aid downstream natural language\nprocessing applications, especially for low-resource languages, and assist\nlanguage documentation efforts for endangered languages. Having long been\nmultilingual, the field of computational morphology is increasingly moving\ntowards approaches suitable for languages with minimal or no annotated\nresources. First, we survey recent developments in computational morphology\nwith a focus on low-resource languages. Second, we argue that the field is\nready to tackle the logical next challenge: understanding a language's\nmorphology from raw text alone. We perform an empirical study on a truly\nunsupervised version of the paradigm completion task and show that, while\nexisting state-of-the-art models bridged by two newly proposed models we devise\nperform reasonably, there is still much room for improvement. The stakes are\nhigh: solving this task will increase the language coverage of morphological\nresources by a number of magnitudes.",
  "text": "Morphological Processing of Low-Resource Languages:\nWhere We Are and What’s Next\nAdam Wiemerslage♯∗\nMiikka Silfverberg♭∗\nChangbing Yang♭\nArya D. McCarthy♮\nGarrett Nicolai♭\nEliana Colunga♯\nKatharina Kann♯\n♯University of Colorado Boulder\n♭University of British Columbia\n♮Johns Hopkins University\nAbstract\nAutomatic morphological processing can aid\ndownstream natural language processing appli-\ncations, especially for low-resource languages,\nand assist language documentation efforts for\nendangered languages.\nHaving long been\nmultilingual, the ﬁeld of computational mor-\nphology is increasingly moving towards ap-\nproaches suitable for languages with minimal\nor no annotated resources. First, we survey re-\ncent developments in computational morphol-\nogy with a focus on low-resource languages.\nSecond, we argue that the ﬁeld is ready to\ntackle the logical next challenge: understand-\ning a language’s morphology from raw text\nalone.\nWe perform an empirical study on\na truly unsupervised version of the paradigm\ncompletion task and show that, while existing\nstate-of-the-art models bridged by two newly\nproposed models we devise perform reason-\nably, there is still much room for improvement.\nThe stakes are high: solving this task will in-\ncrease the language coverage of morphologi-\ncal resources by a number of magnitudes.\n1\nIntroduction\nAutomatic morphological processing tools have\nthe potential to drastically speed up language docu-\nmentation (Moeller et al., 2020) and thereby help\ncombat the language endangerment crisis (Austin\nand Sallabank, 2011). Explicit morphological in-\nformation also beneﬁts myriad NLP tasks, such\nas parsing (Hohensee and Bender, 2012; Seeker\nand C¸ etino˘glu, 2015), language modeling (Blevins\nand Zettlemoyer, 2019; Park et al., 2021; Hofmann\net al., 2021), and machine translation (Dyer et al.,\n2008; Tamchyna et al., 2017).\nFor low-resource languages, valuable morpho-\nlogical resources are typically small or non-existent.\nOf late, the ﬁeld of computational morphology has\nincreased its efforts to extend the coverage of multi-\nlingual morphological resources (Kirov et al., 2016,\n∗*The ﬁrst two authors contributed equally.\n2018; McCarthy et al., 2020a; Metheniti and Neu-\nmann, 2020). Simultaneously, there has been a\nrevival of minimally supervised and unsupervised\nmodels for morphological tasks, such as segmenta-\ntion (Eskander et al., 2019), inﬂection (Kann et al.,\n2017b), and lemmatization (Bergmanis and Gold-\nwater, 2019). Given the speed of recent develop-\nments, it is important to reﬂect on where we are as\na ﬁeld and what future challenges lie ahead.\nTo this end, we survey recent computational mor-\nphology: we review existing multilingual resources\n(§2) and tasks and systems (§3), with a focus on\nlow-resource languages.\nGiven recent develop-\nments in unsupervised segmentation, low-resource\nmorphological inﬂection, and unsupervised mor-\nphological paradigm completion (Jin et al., 2020;\nErdmann et al., 2020)—which we argue is not fully\nunsupervised—we believe the community is poised\nfor the next logical step: inferring a language’s mor-\nphology purely from raw text.\nIn §4, we formalize a new task:\ntruly un-\nsupervised morphological paradigm completion\n(tUMPC). We then introduce a pipeline with two\nnovel components (§4.3): one model for aligning\nparadigm slots across lexemes and another for pre-\ndicting the slots of observed forms. With these, we\nassess several state-of-the-art models and the inﬂu-\nence of different types of unlabeled corpora within\nthe framework of tUMPC. While existing methods\nleave room for improvement, they perform reason-\nably enough to support our argument that inferring\na language’s morphology from raw text is within\nreach and worthy of community efforts.\nTo summarize, we present the following con-\ntributions: (i) a survey of tasks and systems in\ncomputational morphology with a focus on low-\nresource languages; (ii) models for the tasks of\nparadigm slot alignment and slot prediction, (iii)\na formalization of the task of truly unsupervised\nmorphological paradigm completion and (iv) an\nevaluation of state-of-the-art approaches and differ-\narXiv:2203.08909v1  [cs.CL]  16 Mar 2022\nent corpora within the framework of this task. Our\ncode and data are publicly available.1\n2\nMorphological Resources\nManually created resources are necessary for devel-\noping and evaluating NLP systems. They also serve\nas a basis for research questions in a multilingual\ncontext (Pimentel et al., 2019; Wu et al., 2019).2\nBelow, we review the two largest active multilin-\ngual resources for morphology and a number of\nlanguage-speciﬁc resources.\nBackground and Notation\nThe canonical form\nof a word is called its lemma, and the set of all\nsurface forms of a lemma is referred to as that\nlemma’s paradigm. As is common, we formally\nwrite the paradigm of a lemma ℓas:\nπ(ℓ) =\n\nf(ℓ,⃗tγ)\n\u000b\nγ∈Γ(ℓ) ,\n(1)\nwith f : Σ∗× T →Σ∗deﬁning a mapping from a\ntuple consisting of the lemma and a vector ⃗tγ ∈T\nof morphological features to the corresponding in-\nﬂected form. Σ is an alphabet of discrete symbols:\nthe characters used in the language of lemma ℓ.\nΓ(ℓ) is the set of slots in ℓ’s paradigm.\nUniMorph\nThe\nUniMorph\nproject\n(Sylak-\nGlassman et al., 2015a,b; Kirov et al., 2016) is\na database of triples organized into paradigms,\nwhere each triple represents a word as its lemma ℓ,\nmorpho-syntactic description ⃗tγ, and surface form\nf(ℓ,⃗tγ). An English example triple is:\nmutate\nmutates\nV;3;SG;PRS\nThis structure provides training data for inﬂec-\ntion generation, lemmatization, or paradigm com-\npletion. The most recent version of UniMorph\n(McCarthy et al., 2020a) includes 118 languages\nand 14.8 million triples, with more languages un-\nder development. As it is semi-automatically cre-\nated, issues have been noted—particularly, it is\na convenience sample across languages (Gorman\net al., 2019; Malouf et al., 2020). Still, related\nefforts validate themselves using UniMorph, in-\ncluding Metheniti and Neumann (2020)—another\nWiktionary-derived resource for morphology. Wik-\ninﬂection captures segmentation information (§3.2)\nfrom Wiktionary templates, though the authors note\nsome limits in the morphological tags that are ex-\ntracted to accompany these.\n1https://github.com/Adamits/tUMPC\n2This approach has been criticized by Malouf et al. (2020)\ndue to incompleteness and quality of existing resources.\nUniversal Dependencies\nWhereas UniMorph\ncontains type-level annotations, the Universal De-\npendencies project (UD) is a resource of token-\nlevel annotations. As of writing, the latest release\n(v2.8; Zeman et al., 2021) spans 114 languages,\ntypically semi-automatically extracted from exist-\ning corpora, sometimes with less comprehensive\nannotations (Malaviya et al., 2018). The structure\nis useful for morphological tagging (§3.1) at the\nsentence level (Goldman and Tsarfaty, 2021), and\nseveral languages have parallel text, enabling eval-\nuation of projection-based approaches for morphol-\nogy induction, parsing, and other tasks (Yarowsky\net al., 2001; Rasooli and Collins, 2017).\nMapping between UniMorph and Universal De-\npendencies\nThe UD2 morphological annotations\nborrow several features from UniMorph.3 Conse-\nquently, there is great harmony between the two\nschemas.\nA deterministic mapping (McCarthy\net al., 2018) has shown the synergy; for instance,\nBergmanis and Goldwater (2019) augment a con-\ntextual tagger with UniMorph inﬂection tables.\nLanguage-Speciﬁc Resources\nThroughout the\nyears, many language-speciﬁc morphological re-\nsources have been created. These include corpora\nand treebanks like the morphologically annotated\ncorpus for Emirati Arabic by Khalifa et al. (2018).\nResources also come in the form of morphological\ndatabases, such as CELEX for Dutch, English and\nGerman (Baayen et al., 1996), or morphological\nanalyzers, such as the Paraguayan Guaran´ı analyzer\npresented by Zueva et al. (2020).\nCreation of morphological resources is an ongo-\ning effort which in recent years has increasingly\nfocused on low-resource languages. Several confer-\nences and workshops like LREC (Calzolari et al.,\n2020), SIGMORPHON (Nicolai et al., 2021), Com-\nputEL (Arppe et al., 2021), AmericasNLP (Mager\net al., 2021), PYLO (Klavans, 2018) and FSMNLP\n(Maletti and Constant, 2011) have presented and\ncontinue to present language-speciﬁc tools and\ndatasets for computational morphology.\n3\nWhere We Are: Tasks and Systems\n3.1\nMorphological Tagging\nMorphological tagging is a sequence-labeling task\nsimilar to part-of-speech (POS) tagging.\nAs a\ntoken-level task, it considers words in context.\n3http://universaldependencies.org/v2/\nfeatures.html#comparison-with-unimorph\nGiven a sentence, it consists of assigning to\neach word f(ℓ,⃗tγ) a morphosyntactic description\n(MSD), i.e., a tag representing the morphological\nfeatures ⃗tγ it expresses. For instance, in the sen-\ntence The virus mutates, the word mutates would\nbe assigned the tag V;3;SG;PRS. Morphological\ntagging was featured in the SIGMORPHON 2019\nshared task (McCarthy et al., 2019).\nSystems\nA leading non-neural morphological\ntagger is MARMOT (Mueller et al., 2013), a higher-\norder conditional random ﬁeld (CRF;\nLafferty\net al., 2001) tagger. Of late, LSTM (Hochreiter and\nSchmidhuber, 1997) and Transformer (Vaswani\net al., 2017) models have been used for tagging\n(Heigold et al., 2016, 2017; Nguyen et al., 2021).\nFor low-resource languages, both projection-\nbased approaches (Buys and Botha, 2016) and\ncross-lingual transfer approaches via multitask\ntraining (Cotterell and Heigold, 2017) have been\ndeveloped. 16 systems were submitted to the SIG-\nMORPHON 2019 shared task4 (McCarthy et al.,\n2019), which featured 66 languages. The winning\nteam (Kondratyuk, 2019) built a tagger based on\nmultilingual BERT (Devlin et al., 2019), thus em-\nploying cross-lingual transfer; for other systems,\nwe refer the reader to the shared task overview. The\nlargest multilingual morphological tagging effort\nto date is that by Nicolai et al. (2020) who build\nmorphological analyzers for 1108 languages using\nprojection from a high-resource to a low-resource\nlanguage via the aligned text in the JHU Bible Cor-\npus (McCarthy et al., 2020b).\n3.2\nMorphological Segmentation\nThe goal of morphological segmentation (Gold-\nsmith, 2010) is to split words into their smallest\nmeaning-bearing units: morphemes. We discuss\nboth surface and canonical segmentation here.\n3.2.1\nSurface Segmentation\nDuring surface segmentation, a word is split into\nmorphemes in a way such that the concatenation\nof all parts exactly results in the original word. An\nexample (with “*” marking boundaries) is:\nmutates →mutate * s\nSurface segmentation was the focus of the Morpho\nChallenge from 2005 to 2010 (Kurimo et al., 2010).\n4The task is concerned with joint lemmatization and tag-\nging, but systems can be used for separate tagging as well.\nThe competition featured datasets in Finnish, Turk-\nish, German, English, and Arabic. Additionally,\nsegmentation was a track (alongside morphologi-\ncal analysis and generation) of LowResourceEval-\n2019 (Klyachko et al., 2020), a shared task which\nfeatured four low-resource languages from Rus-\nsia. The shared task overview lists morphological\nresources for other Russian languages.\nSystems\nMany approaches to this task are un-\nsupervised. Harris (1970) identiﬁes morpheme\nboundaries in English based on the frequency of\ncharacters at the end of a word. LINGUISTICA\n(Goldsmith, 2001) ﬁnds sets of stems and sufﬁxes\nthat represent the minimum description length of\nthe data. MORFESSOR (Creutz and Lagus, 2002)\nintroduces a family of probabilistic models for iden-\ntifying morphemes, which have seen wide use, in-\ncluding variations of the original model (Virpioja\net al., 2009; Smit et al., 2014). Lignos et al. (2009)\nlearn rewrite rules that can explain many types\nin the corpus. Poon et al. (2009) apply a CRF\nto unsupervised segmentation by learning parame-\nters with contrastive estimation (Smith and Eisner,\n2005). Incorporating semantic similarity between\nrelated words that form ”chains” has also been\nshown to be effective (Narasimhan et al., 2015).\nMonson et al. (2007) propose a segmentation al-\ngorithm that exposes the properties of partial mor-\nphological paradigms in order to learn segments.\nXu et al. (2018) iteratively reﬁne segments accord-\ning to their distribution across paradigms. They\nﬁlter unreliable paradigms with statistically reli-\nable ones, and induce segments with the proposed\npartial paradigms. Both systems can only model\nsufﬁx concatenation. Xu et al. (2020) follow a\nsimilar strategy, but incorporate language typol-\nogy, expanding beyond sufﬁxes, and outperform\nXu et al. (2018). MorphAGram (Eskander et al.,\n2020) is a publicly available tool for unsupervised\nsegmentation based on adaptor grammars (Johnson\net al., 2007).\nSupervised (Creutz and Lagus, 2005; Ruoko-\nlainen et al., 2013; Cotterell et al., 2015) and semi-\nsupervised systems (Ruokolainen et al., 2014) also\nexist. Non-neural systems are often based on CRFs.\nRuokolainen et al. (2013) focus explicitly on low-\nresource settings and perform experiments on Ara-\nbic, English, Hebrew, Finnish, and Turkish with\ntraining set sizes as small as 100 instances.\nNeural models have also been applied to surface\nsegmentation: Wang et al. (2016) obtain strong re-\nsults with window LSTM neural networks in the\nhigh-resource setting, Seker and Tsarfaty (2020)\nintroduce a pointer network (Vinyals et al., 2015)\nfor segmentation and tagging, and Micher (2017)\npropose a segmental RNN (Kong et al., 2015)\nfor segmentation and tagging of Inuktitut. Kann\net al. (2018b) explore LSTM-based sequence-to-\nsequence (seq2seq) models for segmentation in\ncombination with data augmentation, multitask and\nmultilingual training; they evaluate on datasets they\nintroduce for four low-resource Mexican languages.\nEskander et al. (2019) apply an unsupervised ap-\nproach based on adaptor grammars to the same\nlanguages; it outperforms supervised methods in\nsome cases. Sorokin (2019) show that CNNs out-\nperform RNN-based models on that data as well as\non North S´ami (Gr¨onroos et al., 2019).\nAdditional contributions have been made by\nYarowsky and Wicentowski (2000), Schone and Ju-\nrafsky (2001), and Clark (2001). Linguistically in-\nformed approaches show demonstrable value com-\npared to approaches like BPE; see Church (2020)\nand Hofmann et al. (2021). Still, not all morpholog-\nical phenomena are suited for a segmentation-based\nanalysis, as in fusional morphology that sometimes\nleaves ambiguity as to where a morpheme bound-\nary lies; indeed in some cases there is no consensus\namong linguists as to the proper segmentation of a\nword. Therefore, (especially surface) segmentation\nis not necessarily meaningful for all languages.\n3.2.2\nCanonical Segmentation\nCanonical segmentation is more complex: its aim is\nto jointly split a word into morphemes and to undo\nthe orthographic changes which have occurred dur-\ning word formation. As a result, each word is seg-\nmented into its canonical morphemes. While often\nnot being modeled this way in practice, the task\ncan be seen as the following two-step process:\nmanic →maniaic →mania * ic\nSystems\nThe state-of-the-art pre-neural system\nis the CRF-based model by Cotterell et al. (2016c),\nwhich is jointly trained on segmentation and\nrestoration of orthographic changes. The unsuper-\nvised system of Bergmanis and Goldwater (2017)\nbuilds upon MorphoChains (Narasimhan et al.,\n2015).\nNeural models are typically based on\nseq2seq architectures: Kann et al. (2016) use a\nseq2seq GRU and a feature-based reranker. Like\nCotterell et al. (2016c), they evaluate on German,\nEnglish, and Indonesian. Ruzsics and Samardˇzi´c\n(2017) use a similar system, but add a language\nmodel over canonical segments and do not require\nexternal resources. In addition to German, English,\nand Indonesian, they evaluate on Chintang, a truly\nlow-resource language spoken in Nepal. Wang et al.\n(2019) use a character-level seq2seq model for (sur-\nface and) canonical segmentation in Mongolian.\nMager et al. (2020) show the beneﬁt of copy mech-\nanisms and introduce datasets for two low-resource\nMexican languages. Moeng et al. (2021) show that\nTransformers outperform RNNs for canonical seg-\nmentation in four Nguni languages.\n3.3\nLemmatization, Inﬂection, Reinﬂection\nInﬂection and reinﬂection have recently gained\npopularity in computational morphology by being\nfeatured in yearly SIGMORPHON shared tasks\n(Cotterell et al., 2016b). They are concerned with\ngenerating inﬂected forms f(ℓ,⃗tγ) of a lemma ℓ;\nthe target inﬂected form can be speciﬁed in dif-\nferent ways, depending on the exact task formu-\nlation. While the terms inﬂection and reinﬂection\nare sometimes used synonymously in the literature,\ninﬂection refers to generating a word form from a\ngiven lemma, while reinﬂection refers to genera-\ntion from an arbitrary given form in the paradigm.\nLemmatization is a special case of reinﬂection: in-\nstead of generating an indicated inﬂected form, a\nlemma is produced. As the target form is implicitly\ndetermined by the task deﬁnition, lemmatization\ngenerally does not require tags to indicate which\nform to generate.\n3.3.1\nType-level Versions\nMost commonly, lemmatization, inﬂection and re-\ninﬂection are type-level tasks. The input consists of\nan input form together with the target MSD (which\ncan be omitted for lemmatization). The output is\nthe corresponding inﬂected form, for instance:\nmutated V;3;SG;PRS →mutates\nThe version of reinﬂection featured in the SIG-\nMORPHON 2016 shared task also provides the\nMSD of the source form, but performance improve-\nments are usually minor (Cotterell et al., 2016a).\nSystems\nPre-neural systems for the task include\nthose by Durrett and DeNero (2013) and Nicolai\net al. (2015). These systems align lemmas and in-\nﬂections before extracting character-level transduc-\ntions for training CRF-inspired models. Faruqui\net al. (2016) propose the ﬁrst neural model for mor-\nphological inﬂection, an RNN seq2seq model, but\nfail to outperform prior approaches on some of\nthe datasets they evaluate on. The breakthrough\nfor neural models was the SIGMORPHON 2016\nshared task (Cotterell et al., 2016a), with about\none third of the systems being neural: the winning\nsystem (Kann and Sch¨utze, 2016a,b) used multi-\ntask training by encoding MSDs together with the\ncharacter sequence of the source word. This ap-\nproach has now become the standard for the task,\nand while a multilingual version of the model by\nKann and Sch¨utze (2016a) was submitted to the\nSIGMORPHON 2021 shared task (Pimentel et al.,\n2021; Szolnok et al., 2021), the same multitask\napproach has since been used with other seq2seq\nmodels such as Transformers (Wu et al., 2021).\nEnsembles have been shown to improve perfor-\nmance for inﬂection (Kann and Sch¨utze, 2016a)\nand have been systematically studied for the task\nby Kylli¨ainen and Silfverberg (2019).\nThe SIGMORPHON shared tasks on morpho-\nlogical inﬂection have focused increasingly on\nlow-resource settings. Seq2seq models with hard\nmonotonic attention (Aharoni and Goldberg, 2017),\na copy mechanism (Sharma et al., 2018; Singer\nand Kann, 2020), or both (Makarov et al., 2017;\nMakarov and Clematide, 2018a,b) obtain great re-\nsults for training sets as small as 100 examples.\nCross-lingual transfer via multitask training was\nproposed by Kann et al. (2017b) for GRU seq2seq\nmodels and has later been used with other architec-\ntures, e.g., in the SIGMORPHON 2019 shared task\non cross-lingual transfer (McCarthy et al., 2019).\nAnother approach suitable for low-resource lan-\nguages is data augmentation.\nFor morphologi-\ncal inﬂection, this was suggested by several con-\ntemporaneous works (Kann and Sch¨utze, 2017;\nBergmanis et al., 2017; Silfverberg et al., 2017). In\nthe following years, other augmentation strategies\nhave been developed (Anastasopoulos and Neubig,\n2019). The success of data augmentation is mixed,\nas it is largely dependent on the architecture (Does\nit have to learn how to copy or is there a copy\nmechanism?) as well as on the quality of the origi-\nnal data, which inﬂuences the quality of artiﬁcially\ngenerated examples.\n3.3.2\nToken-level Versions\nThe token-level version of the task is often referred\nto as lemmatization or inﬂection in context. Here\nthe information about which form to generate is\nexplicitly given via a sentence context in which the\ntarget word should be embedded, e.g.:\nmutate – The virus [MASK]. →mutates\nA drawback of this formulation is that typically\nmany different inﬂected forms are possible within\nthe same context: in the given example, mutates\nis the gold solution, but mutated would be equally\ngrammatical. To overcome this, multiple gold so-\nlutions can be provided (Cotterell et al., 2018). It\nmight be impossible to unambiguously deﬁne the\ntarget form for some languages if the speaker’s\nintention is unknown.\nSystems\nLemmatization in context is arguably\neasier than inﬂection or reinﬂection, as the target\nform for generation is implicitly deﬁned. Neu-\nral models for inﬂection are seq2seq architectures:\nBergmanis and Goldwater (2018) propose Lematus,\na character-level LSTM, which they later extend to\nthe low-resource setting by training on labeled data\nin combination with raw text (Bergmanis and Gold-\nwater, 2019). They explore data settings as small\nas 1k types each from UD and UniMorph. Zal-\nmout and Habash (2020) use a similar architecture\nto Lematus but add subword features. Malaviya\net al. (2019) present a joint model for tagging and\nlemmatization and show that joint training beneﬁts\nlow-resource languages. They evaluate on 20 lan-\nguages, using data from UD. The best lemmatizer\nin the SIGMORPHON 2019 shared task (McCarthy\net al., 2019), UDPipe (Straka et al., 2019), is based\non BERT (Devlin et al., 2019).\nInﬂection in context can be tackled by neural\nseq2seq models too. Models typically either see a\ncontext window around the target word (Makarov\nand Clematide, 2018c; Kann et al., 2018a; ´Acs,\n2018) and then are optionally trained via multitask\ntraining (Kementchedjhieva et al., 2018) or predict\nthe MSD of the form to generate as a ﬁrst step (Liu\net al., 2018). Kementchedjhieva et al. (2018) show\nthat a multilingual model can aid low-resource lan-\nguages via cross-lingual transfer.\n3.4\nParadigm Completion\nThe paradigm cell ﬁlling problem (Ackerman et al.,\n2009) – also called supervised paradigm comple-\ntion (Cotterell et al., 2017a) – is yet another inﬂec-\ntion task, but differs from the above ones in that\nthe inﬂected forms for all slots Γ(ℓ) of lemma ℓ’s\nparadigm need to be generated and that the input\ncan consist of one or more forms.\nSystems\nMany approaches for the paradigm cell\nﬁlling problem are effectively systems for mor-\nphological reinﬂection and generate all forms of\na paradigm individually and from a single input\nform, e.g., Silfverberg et al. (2017); Silfverberg\nand Hulden (2018); Moeller et al. (2020). Kann\net al. (2017a) propose a model for multi-source in-\nﬂection, showing that multiple available forms per\nparadigm can be beneﬁcial for generation, but do\nnot evaluate on paradigm completion. Two notable\nexceptions which design approaches explicitly for\nthe paradigm cell ﬁlling problem are Cotterell et al.\n(2017b) and Kann and Sch¨utze (2018). Cotterell\net al. (2017b) rely on the notion of principal parts\n(Finkel and Stump, 2007) to jointly generate all\nforms in the paradigm. Kann and Sch¨utze (2018)\nuse a transductive training approach, ﬁne-tuning on\na paradigm’s input forms before generating missing\ntarget forms. The latter shows good performance\nfor training sets with as few as 10 paradigms.\n3.5\nParadigm Clustering\nParadigm clustering can be seen as a ﬁrst step to-\nwards the unsupervised analysis of a language’s\nmorphology and is typically part of pipelines for\nunsupervised paradigm completion (§3.6). The\ngoal of paradigm clustering is to group all types\nin a corpus into (partial) morphological paradigms.\nFor example, the input The, virus, mutates, after, it,\nhas, mutated should result in the paradigm cluster\n(mutates, mutated) and 5 singleton clusters. Sys-\ntems for the task can be evaluated using best-match\nF1 (BMF1; Wiemerslage et al., 2021).\nSystems\nPerhaps\nthe\nseminal\nwork\nin\ndistributionally-based\nparadigm\nclustering\nis\nthe work of Yarowsky and Wicentowski (2000).\nTheir work predates embedding-based approaches\nwhile leveraging both distributional features of\ncontext and relative frequency, along with early\nstatistical models of inﬂection-to-lemma string\ntransduction.\nFor instance, the work succeeds\nin identifying that the past tense of ‘sing’ is not\n‘singed’ but ‘sang’, based on both the distributional\nsignatures of music vs. ﬁre terms in context, as\nwell as the distribution of observed tense frequency\nratios, where the regular sing:singed pairing can\nalso be rejected given its frequency ratio is several\nstandard deviations off of expectation, while the\nirregular sing:sang pairing occurs at nearly exactly\nthe ratio expected. While contextual information\nhas been incorporated in follow-up works (Schone\nand Jurafsky, 2001) and in recent approaches by\nmeans of word embeddings, we do not see much\nfollow-on use of the frequency ratio features,\nwhich remain ripe for disambiguation of paradigm\nmembers.\nSegmentation approaches like Goldsmith (2001),\ndeveloped to segment words into stems and af-\nﬁxes, can also be used to induce paradigm clus-\nters. Chan (2006) formalizes the notion of a prob-\nabilistic paradigm — modeling conditional proba-\nbilities of sufﬁxes given paradigms and paradigms\ngiven stems. However, they that a segmentation\nis given, and only model regular morphology for\nunambiguous words, or those with a known POS.\nSome segmentation algorithms induce paradigms\nas a byproduct, as in Monson et al. (2007), Xu et al.\n(2018) and Xu et al. (2020). These can also be\nemployed as paradigm clustering systems.\nSeveral systems have been proposed for the SIG-\nMORPHON 2021 shared task (Wiemerslage et al.,\n2021).\nThe best performing system (McCurdy\net al., 2021) segments input types with MorphA-\nGram (Eskander et al., 2020), then groups the re-\nsulting stems into paradigm clusters. Yang et al.\n(2021) learn frequent transformation rules and clus-\nter types together that result from rule application.\n3.6\nUnsupervised Paradigm Completion\nDue to the recent progress on supervised morpho-\nlogical tasks, unsupervised paradigm completion\n(UMPC; or the paradigm discovery problem (El-\nsner et al., 2019)) has recently (re)emerged as a\npromising way to automatically extend morpho-\nlogical resources such as UniMorph to more low-\nresource languages. Similar to the supervised ver-\nsion of the task, the goal is to generate the inﬂected\nforms corresponding to all slots Γ(ℓ) of lemma ℓ’s\nparadigm. However, no morphological annotations\nare given during training. Two independent works\npropose similar unsupervised paradigm completion\nsetups. In Jin et al. (2020), the basis of the SIG-\nMORPHON 2020 shared task (Kann et al., 2020),\nthe input consists of 1) a corpus in a low-resource\nlanguage and 2) a list of lemmas from one POS in\nthat language. In Erdmann et al. (2020), the inputs\nare 1) a corpus and 2) a list of word forms belong-\ning to a single POS. For both, the expected output\nis the paradigms for the words in the provided list.\nAs systems are trained without supervision, they\ncannot output human-readable MSDs and, instead,\nassign uninterpretable slot identiﬁers to generated\nforms. Thus, evaluation against gold standard data\nfrom UniMorph is non-trivial. Jin et al. (2020) pro-\npose to evaluate systems via best-match accuracy\n(BMAcc): the best accuracy among all mappings\nfrom pseudo tags to paradigm slots.\nSystems\nState-of-the-art systems for paradigm\ncompletion follow a pipeline approach similar to\nthat by Jin et al. (2020): 1) based on the given\ninput forms, they detect transformations which\nhappen during inﬂection (and sometimes new lem-\nmas), 2) the paradigm structure is detected based on\nthe transformations, and 3) an inﬂection model is\ntrained to generate missing surface forms. Jin et al.\n(2020) employ the inﬂection model by Makarov\nand Clematide (2018a), while Mager and Kann\n(2020) use the LSTM pointer-generator model from\nSharma et al. (2018), and Singer and Kann (2020)\nimplement a Transformer-based pointer-generator\nmodel. The performance across languages is mixed\n(Kann et al., 2020).\nIs the Task Truly Unsupervised?\nExisting ver-\nsions of the unsupervised paradigm completion\ntask make small concessions to supervision re-\nquirements by providing lists of lemmas or surface\nforms from a single POS. This simpliﬁes a difﬁ-\ncult task, but also makes it less realistic. From the\npoint of view of data availability, this method is not\nlanguage-agnostic, as many languages do not have\nthe required documentation: many of the world’s\nlanguages have fuzzy POS deﬁnitions, and no an-\nnotated POS corpora. From a language learning\nperspective, existing methods are closer to L2 than\nto L1 learning.\nUnder this framing, UMPC requires only dis-\ncovering the set of inﬂection slots for a single\nparadigm, of a single POS that must be known a\npriori. The presence of a word list also allows sys-\ntems to anchor to a privileged form and simpliﬁes\nparadigm clustering to a retrieval task.\n4\nWhat’s Next: Truly Unsupervised\nParadigm Completion\n4.1\nMotivation\nWe introduce a version of UMPC that more strictly\nremoves human intervention. By removing the\ninput lexicon and evaluating more than one POS,\nwe minimize any prior human involvement with\nthe data and better evaluate a system’s ability to\ngeneralize. This means that our only input is a raw\ntext corpus, and it introduces two challenges. 1) We\nmust model the entire training corpus, rather than\na ﬁltered set of words. 2) We must predict which\nslots to generate at test time. We design test sets\nto evaluate these problems, ensuring they include\nparadigms from at least two POS, and prompt for\ninput forms in context, half of which are unseen in\nthe training corpora, so systems can infer the input\nword POS. We refer to this version of the task as\ntruly unsupervised paradigm completion (tUMPC).\n4.2\nData and Languages\nLanguages\nWe select three development lan-\nguages (English, Finnish, and Swedish) and four\ntest languages (German, Greek, Icelandic, and\nRussian). We select our test languages to maxi-\nmize orthographic and typological diversity, given\nthree constraints: (1) a large number of available\nparadigms in UniMorph, (2) two or more POS in\nUniMorph, and (3) no known issues with the Uni-\nMorph data such as large numbers of missing forms.\n(We exclude all paradigms containing multiword\nforms.) We note that this yields a set of test lan-\nguages that are all Indo-European, though it spans\nthree different orthographies.\nRaw Text Corpora\nWe experiment on two cor-\npora: the JHU Bible Corpus (McCarthy et al.,\n2020b) and a child-directed corpus we create by\ndigitizing children’s books.\nWhile many stud-\nies in computational morphology focus on tran-\nscripts of child-directed speech from databases like\nCHILDES (MacWhinney, 2014), child-directed\nbooks are part of parent’s child-directed talk, and\nare thus an important source of language for many\nchildren (Montag et al., 2015). We translate the\nchild-directed corpus into all of our languages from\nEnglish using the Google Translate API following\nDou and Neubig (2021). We tokenize with spaCy.5\nDetails are given in Table A.1.\nTest Data\nOur test data consists of words in con-\ntext from two different corpora – Wikipedia (Ginter\net al., 2017) and JW300 (Agi´c and Vuli´c, 2019) –,\nplus their gold paradigms from UniMorph. A de-\ntailed description of the preparation of the test data\ncan be found in Appendix C.2.\n4.3\nModels\nTo use existing state-of-the-art approaches and to\nevaluate them within the framework of tUMPC,\n5https://spacy.io\nwe tackle the task with a pipeline approach, con-\nducting 4 steps: 1) paradigm clustering, 2) slot\nalignment, 3) slot prediction, and 4) inﬂection gen-\neration. State-of-the-art models exist for Steps 1\nand 4, and we propose systems for Steps 2 and 3\nhere, together with descriptions of those subtasks.\nHyperparameters for all models are in Appendix B.\nParadigm Clustering\nThe ﬁrst step for tUMPC\nis clustering words into paradigms. We compare\n3 paradigm clustering algorithms: McCurdy et al.\n(2021, McC), Xu et al. (2018, Xu), and the baseline\nfrom Wiemerslage et al. (2021, SIG). We modify\nSIG so it does not predict clusters which are sub-\nsets of other clusters, which improves precision.\nFor reference, we provide those systems’ paradigm\nclustering results in Table A.2. In some clustering\nsystems, each type appears in only one paradigm,\nwhich confounds our task for types that can instan-\ntiate more than one POS, and thus more than one\ninﬂectional paradigm, depending on the context.\nSlot Alignment\nSlot alignment is concerned\nwith identifying which words across paradigms\nexpress the same inﬂectional information.\nThe system we propose for the task ﬁrst re-\nmoves all singleton paradigm clusters from the\ninput, as they contain no inﬂection pairs to learn\nfrom, and converts all remaining clusters into ab-\nstract paradigms ci ∈C (Hulden et al., 2014) by\ncomputing the longest common substring (LCS)\nfor each cluster. For example, the LCS of the\n(true) paradigm of walk is walk, and the abstract\nparadigm is X0, X0+ed, X0+ing, X0+s. We ﬁlter\nabstract forms that appear less than β = 50 times.\nNext, we assign a POS tag to each cluster. With\na set of latent tags Z, we deﬁne a Bayesian model:\nP(k, ci) = P(k)\nY\nfj∈ci\nP(fj | k)\n(2)\nP(ci) =\nX\nk∈Z\nP(ci, k)\n(3)\nWe then maximize the likelihood of the paradigm\nclusters ci ∈C with an expectation maximization\nalgorithm (Dempster et al., 1977). The POS assign-\nment for each ci is thus argmaxk(P(k, ci)), and\n|Z| is a hyperparameter which we set to 3.\nWe now have sets Ck. We assign a slot to each\nform in an abstract paradigm, considering one Ck\nat a time. To this end, we compute a fastText (Bo-\njanowski et al., 2017) embedding for each type in\nthe corpus and compute the embedding for an ab-\nstract form a as the average fastText embedding of\nall types whose abstract form is a. We deﬁne the\nsimilarity of two abstract forms a and a′ as\nsim(a, a′) = cos(a, a′) × (1 −J(a, a′)),\n(4)\nwhere cos(a, a′) is the cosine similarity, J is the\nJaccard similarity\nJ(a, a′) = |Ca ∩Ca′|\n|Ca ∪Ca′|,\n(5)\nand Ca is the set of abstract paradigms containing\na. Finally, we apply agglomerative clustering over\nthe abstract forms with (4) as our similarity metric\nand a distance threshold of 0.15.\nSlot Prediction\nGiven a test form f(ℓ,⃗tγ), the\ngoal of slot prediction is to predict the source slot\n⃗tγ and target slots Γ(ℓ). We treat this as a simpliﬁed\nPOS tagging task and use a character-level Trans-\nformer seq2seq model to predict a word’s POS tag\nand source slot. The model is trained on the results\nof the slot alignment step. For every word from the\nraw-text corpus that was assigned a slot, we sample\nup to 5 unique contexts. A given target word is in-\nput with its left and right neighbors; context words\nthat occur fewer than α = 50 times in the training\ndata are replaced with OOV. The outputs are the\nPOS tag and the source slot generated by slot align-\nment. We train our model in FAIRSEQ (Ott et al.,\n2019); hyperparameters are in Appendix B.\nAt test time, the model predicts f(ℓ,⃗tγ) and the\n(pseudo) POS tag. Because the slot alignment step\nassociates each POS tag with a unique set of slots,\nwe can perform a simple lookup to ﬁnd the slots\nthat f(ℓ,⃗tγ) inﬂects for.\nMorphological Inﬂection\nTo generate missing\nforms, we train state-of-the-art inﬂection models\non the results of the slot alignment step and gener-\nate surface forms according to the slot prediction.\nWe experiment with the following three models:\nMakarov and Clematide (2018a, M&C), Wu et al.\n(2021, Wu), and Kann and Sch¨utze (2016b, K&S).\n4.4\nNon-neural Baseline\nWe\ncompare\nagainst\na\nrule-based\nsystem\n(baseline) that heuristically predicts the same set\nof slots for all words, and inﬂects by applying edit\ntrees to input words. A detailed description is in\nAppendix D, together with a comparison between\nbaseline and our proposed POS-based system\njw300\nwiki\nFigure 1: BMAcc for each paradigm clustering system for the POS-based slot aligner; averaged over inﬂectors.\njw300\nwiki\nFigure 2: BMAcc for each inﬂector for the POS-based slot aligner; averaged over paradigm clusters.\nfor slot alignment and slot prediction.\nAs the\nPOS-based system clearly outperforms baseline,\nwe focus the remainder of this paper on the former.\n4.5\nResults and Discussion\nWe present results from all experiments in terms of\nBMAcc (Jin et al., 2020). Overall, tUMPC is dif-\nﬁcult, though the variance in results over different\ncomponents of our pipeline implies that there is a\ngreat deal of room for the community to innovate.\nWe see the lowest scores for our Greek and Ice-\nlandic corpora. These have far fewer tokens than\nGerman and Russian, plus higher type–token ratios,\nwhich likely makes the task more challenging.\nImpact of the Clustering System\nFigure 1\nshows that the choice of paradigm clustering strat-\negy strongly affects our pipeline’s downstream per-\nformance. McC, the best performing clustering sys-\ntem on the paradigm clustering task, frequently\noutperforms the other two strategies. The excep-\ntion to this is Russian, where Xu gives the best\nresults—by a large margin when learning from the\nchild-directed training corpora.\nImpact of the Inﬂection System\nFrom Figure 2\nit is obvious that the choice of inﬂection model\ndoes not have a large effect on downstream results.\nAll three systems we compare are known to be\nextremely competitive on the supervised inﬂection\ntask, so it is reasonable to assume that they ﬁt the\ngenerated training data relatively similarly. Future\nwork can assess how inﬂection generation can best\naccount for the noisy nature of the data in this task,\nakin to Michel and Neubig (2018).\nImpact of the Corpus\nThe consilience of our re-\nsults suggests that the child-directed corpus leads\nto slightly better downstream performance, except\nin German. Notably, the German Bible contains\nfar more tokens and far fewer types than the corre-\nsponding child-directed corpus (Table A.1), which\nmay signiﬁcantly simplify the learning task.\n5\nConclusion\nThanks to strong systems for inﬂection, segmen-\ntation, and paradigm completion, computational\nmorphology is ripe to contribute to the large num-\nber of the world’s languages with very few digital\nresources. We explore this through the novel task\ntUMPC—which presents several challenges. We\nbelieve that truly unsupervised morphology is an\nimportant direction, and it can have a large impact\non language technology for thousands of languages.\nWith the goal of preserving endangered languages,\nwe note that more than half the world’s languages\nhave no writing system (Harmon, 1995). A frontier\nfor this task would process speech as a strategy for\nlanguage documentation in unwritten languages.\nAcknowledgments\nWe thank David Yarowsky, members of NALA,\nand the anonymous reviewers for helpful feedback.\nReferences\nFarrell Ackerman, James P Blevins, and Robert Mal-\nouf. 2009. Parts and wholes: Implicative patterns in\ninﬂectional paradigms. Analogy in grammar: Form\nand acquisition, pages 54–82.\nJudit ´Acs. 2018.\nBME-HAS system for CoNLL–\nSIGMORPHON 2018 shared task: Universal mor-\nphological reinﬂection.\nIn Proceedings of the\nCoNLL–SIGMORPHON 2018 Shared Task:\nUni-\nversal Morphological Reinﬂection, pages 121–126,\nBrussels. Association for Computational Linguis-\ntics.\nˇZeljko Agi´c and Ivan Vuli´c. 2019. JW300: A wide-\ncoverage parallel corpus for low-resource languages.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n3204–3210, Florence, Italy. Association for Compu-\ntational Linguistics.\nRoee Aharoni and Yoav Goldberg. 2017. Morphologi-\ncal inﬂection generation with hard monotonic atten-\ntion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2004–2015, Vancouver,\nCanada. Association for Computational Linguistics.\nAntonios Anastasopoulos and Graham Neubig. 2019.\nPushing the limits of low-resource morphological in-\nﬂection. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n984–996, Hong Kong, China. Association for Com-\nputational Linguistics.\nAntti Arppe, Jeff Good, Atticus Harrigan, Mans\nHulden, Jordan Lachler, Sarah Moeller, Alexis\nPalmer, Miikka Silfverberg, and Lane Schwartz, ed-\nitors. 2021.\nProceedings of the 4th Workshop on\nthe Use of Computational Methods in the Study of\nEndangered Languages Volume 1 (Papers). Associa-\ntion for Computational Linguistics, Online.\nGiusepppe Attardi. 2015.\nWikiextractor.\nhttps://\ngithub.com/attardi/wikiextractor.\nP. Austin and J. Sallabank, editors. 2011. The Cam-\nbridge Handbook of Endangered Languages. Cam-\nbridge Handbooks in Language and Linguistics.\nCambridge University Press.\nR. Harald Baayen, Richard Piepenbrock, and Leon Gu-\nlikers. 1996.\nThe CELEX lexical database (CD-\nROM).\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2015.\nNeural machine translation by\njointly learning to align and translate.\nCoRR,\nabs/1409.0473.\nToms Bergmanis and Sharon Goldwater. 2017. From\nsegmentation to analyses: a probabilistic model for\nunsupervised morphology induction.\nIn Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 1, Long Papers, pages 337–346, Valencia,\nSpain. Association for Computational Linguistics.\nToms Bergmanis and Sharon Goldwater. 2018. Con-\ntext sensitive neural lemmatization with Lematus. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1391–1400, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nToms Bergmanis and Sharon Goldwater. 2019. Data\naugmentation for context-sensitive neural lemmati-\nzation using inﬂection tables and raw text. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 4119–4128,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nToms Bergmanis, Katharina Kann, Hinrich Sch¨utze,\nand Sharon Goldwater. 2017.\nTraining data aug-\nmentation for low-resource morphological inﬂection.\nIn Proceedings of the CoNLL SIGMORPHON 2017\nShared Task: Universal Morphological Reinﬂection,\npages 31–39, Vancouver. Association for Computa-\ntional Linguistics.\nTerra Blevins and Luke Zettlemoyer. 2019. Better char-\nacter language modeling through morphology.\nIn\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1606–\n1613, Florence, Italy. Association for Computational\nLinguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nJan Buys and Jan A. Botha. 2016. Cross-lingual mor-\nphological tagging for low-resource languages. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1954–1964, Berlin, Germany.\nAssociation for Computational Linguistics.\nNicoletta Calzolari, Fr´ed´eric B´echet, Philippe Blache,\nKhalid Choukri, Christopher Cieri, Thierry De-\nclerck, Sara Goggi, Hitoshi Isahara, Bente Mae-\ngaard, Joseph Mariani, et al., editors. 2020. Proceed-\nings of the 12th Language Resources and Evaluation\nConference. European Language Resources Associ-\nation, Marseille, France.\nErwin Chan. 2006. Learning probabilistic paradigms\nfor morphology in a latent class model. In Proceed-\nings of the Eighth Meeting of the ACL Special In-\nterest Group on Computational Phonology and Mor-\nphology at HLT-NAACL 2006, pages 69–78, New\nYork City, USA. Association for Computational Lin-\nguistics.\nGrzegorz Chrupała. 2008.\nTowards a machine-\nlearning architecture for lexical functional grammar\nparsing. Ph.D. thesis, Dublin City University.\nKenneth Ward Church. 2020. Emerging trends: Sub-\nwords, seriously? Nat. Lang. Eng., 26(3):375–382.\nAlexander Clark. 2001.\nLearning morphology with\npair hidden Markov models. In ACL (Companion\nVolume).\nRyan Cotterell and Georg Heigold. 2017.\nCross-\nlingual character-level neural morphological tag-\nging.\nIn Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 748–759, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nG´eraldine Walther, Ekaterina Vylomova, Arya D.\nMcCarthy, Katharina Kann, Sabrina J. Mielke, Gar-\nrett Nicolai, Miikka Silfverberg, et al. 2018. The\nCoNLL–SIGMORPHON 2018 shared task:\nUni-\nversal morphological reinﬂection.\nIn Proceedings\nof the CoNLL–SIGMORPHON 2018 Shared Task:\nUniversal Morphological Reinﬂection, pages 1–27,\nBrussels. Association for Computational Linguis-\ntics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nG´eraldine Walther, Ekaterina Vylomova, Patrick\nXia,\nManaal\nFaruqui,\nSandra\nK¨ubler,\nDavid\nYarowsky, Jason Eisner, and Mans Hulden. 2017a.\nCoNLL-SIGMORPHON 2017 shared task: Univer-\nsal morphological reinﬂection in 52 languages. In\nProceedings of the CoNLL SIGMORPHON 2017\nShared Task: Universal Morphological Reinﬂection,\npages 1–30, Vancouver. Association for Computa-\ntional Linguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nDavid Yarowsky, Jason Eisner, and Mans Hulden.\n2016a.\nThe SIGMORPHON 2016 shared Task—\nMorphological reinﬂection. In Proceedings of the\n14th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphol-\nogy, pages 10–22, Berlin, Germany. Association for\nComputational Linguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nDavid Yarowsky, Jason Eisner, and Mans Hulden.\n2016b.\nThe\nSIGMORPHON\n2016\nshared\ntask—morphological reinﬂection.\nIn Proceed-\nings of the 14th SIGMORPHON Workshop on\nComputational Research in Phonetics, Phonology,\nand Morphology, pages 10–22.\nRyan Cotterell, Thomas M¨uller, Alexander Fraser, and\nHinrich Sch¨utze. 2015. Labeled morphological seg-\nmentation with semi-Markov models. In Proceed-\nings of the Nineteenth Conference on Computational\nNatural Language Learning, pages 164–174, Bei-\njing, China. Association for Computational Linguis-\ntics.\nRyan Cotterell, John Sylak-Glassman, and Christo\nKirov. 2017b. Neural graphical models over strings\nfor principal parts morphological paradigm comple-\ntion. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Volume 2, Short Papers, pages\n759–765, Valencia, Spain. Association for Computa-\ntional Linguistics.\nRyan Cotterell, Tim Vieira, and Hinrich Sch¨utze.\n2016c. A joint model of orthography and morpho-\nlogical segmentation.\nIn Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 664–669, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nMathias Creutz and Krista Lagus. 2002. Unsupervised\ndiscovery of morphemes.\nIn Proceedings of the\nACL-02 Workshop on Morphological and Phonolog-\nical Learning, pages 21–30. Association for Compu-\ntational Linguistics.\nMathias Creutz and Krista Lagus. 2005.\nInducing\nthe morphological lexicon of a natural language\nfrom unannotated text.\nIn Proceedings of the\nInternational and Interdisciplinary Conference on\nAdaptive Knowledge Representation and Reasoning\n(AKRR’05), 106-113, pages 51–59.\nArthur P. Dempster, Nan M. Laird, and Donald B. Ru-\nbin. 1977.\nMaximum likelihood from incomplete\ndata via the EM algorithm. Journal of the Royal Sta-\ntistical Society: Series B (Methodological), 39(1):1–\n22.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nZi-Yi Dou and Graham Neubig. 2021. Word alignment\nby ﬁne-tuning embeddings on parallel corpora. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 2112–2128, Online.\nAssociation for Computational Linguistics.\nGreg Durrett and John DeNero. 2013.\nSupervised\nlearning of complete morphological paradigms. In\nProceedings of the 2013 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1185–1195, Atlanta, Georgia. Association for\nComputational Linguistics.\nChristopher Dyer, Smaranda Muresan, and Philip\nResnik. 2008.\nGeneralizing word lattice transla-\ntion. In Proceedings of ACL-08: HLT, pages 1012–\n1020, Columbus, Ohio. Association for Computa-\ntional Linguistics.\nMicha Elsner,\nAndrea D Sims,\nAlexander Erd-\nmann, Antonio Hernandez, Evan Jaffe, Lifeng Jin,\nMartha Booker Johnson, Shuan Karim, David L\nKing, Luana Lamberti Nunes, et al. 2019. Model-\ning morphological learning, typology, and change:\nWhat can the neural sequence-to-sequence frame-\nwork contribute?\nJournal of Language Modelling,\n7(1):53–98.\nAlexander Erdmann, Micha Elsner, Shijie Wu, Ryan\nCotterell, and Nizar Habash. 2020. The paradigm\ndiscovery problem. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7778–7790, Online. Association\nfor Computational Linguistics.\nRamy Eskander, Francesca Callejas, Elizabeth Nichols,\nJudith Klavans, and Smaranda Muresan. 2020. Mor-\nphAGram, evaluation and framework for unsuper-\nvised morphological segmentation. In Proceedings\nof the 12th Language Resources and Evaluation\nConference, pages 7112–7122, Marseille, France.\nEuropean Language Resources Association.\nRamy Eskander, Judith Klavans, and Smaranda Mure-\nsan. 2019. Unsupervised morphological segmenta-\ntion for low-resource polysynthetic languages.\nIn\nProceedings of the 16th Workshop on Computational\nResearch in Phonetics, Phonology, and Morphol-\nogy, pages 189–195, Florence, Italy. Association for\nComputational Linguistics.\nManaal Faruqui, Yulia Tsvetkov, Graham Neubig, and\nChris Dyer. 2016. Morphological inﬂection genera-\ntion using character sequence to sequence learning.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 634–643, San Diego, California. Association\nfor Computational Linguistics.\nRaphael Finkel and Gregory Stump. 2007.\nPrinci-\npal parts and morphological typology. Morphology,\n17(1):39–75.\nFilip Ginter, Jan Hajiˇc, Juhani Luotolahti, Milan Straka,\nand Daniel Zeman. 2017. CoNLL 2017 shared task\n- automatically annotated raw texts and word embed-\ndings. LINDAT/CLARIAH-CZ digital library at the\nInstitute of Formal and Applied Linguistics ( ´UFAL),\nFaculty of Mathematics and Physics, Charles Uni-\nversity.\nOmer Goldman and Reut Tsarfaty. 2021. Well-deﬁned\nmorphology is sentence-level morphology. In Pro-\nceedings of the 1st Workshop on Multilingual Repre-\nsentation Learning, pages 248–250, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nJohn Goldsmith. 2001. Unsupervised learning of the\nmorphology of a natural language. Computational\nLinguistics, 27(2):153–198.\nJohn A Goldsmith. 2010. Segmentation and morphol-\nogy. The handbook of computational linguistics and\nnatural language processing, 57:364.\nKyle Gorman, Arya D. McCarthy, Ryan Cotterell,\nEkaterina Vylomova, Miikka Silfverberg, and Mag-\ndalena Markowska. 2019.\nWeird inﬂects but OK:\nMaking sense of morphological generation errors.\nIn Proceedings of the 23rd Conference on Computa-\ntional Natural Language Learning (CoNLL), pages\n140–151, Hong Kong, China. Association for Com-\nputational Linguistics.\nStig-Arne Gr¨onroos, S´ami Virpioja, and Mikko Ku-\nrimo. 2019. North S´ami morphological segmenta-\ntion with low-resource semi-supervised sequence la-\nbeling.\nIn Proceedings of the Fifth International\nWorkshop on Computational Linguistics for Uralic\nLanguages, pages 15–26, Tartu, Estonia. Associa-\ntion for Computational Linguistics.\nJan Hajiˇc and Dan Zeman, editors. 2017. Proceedings\nof the CoNLL 2017 Shared Task: Multilingual Pars-\ning from Raw Text to Universal Dependencies. As-\nsociation for Computational Linguistics, Vancouver,\nCanada.\nDavid Harmon. 1995. The status of the world’s lan-\nguages as reported in “Ethnologue”. Southwest Jour-\nnal of Linguistics, 14:1–28.\nZellig S Harris. 1970. Morpheme boundaries within\nwords: Report on a computer test.\nIn Papers in\nStructural and Transformational Linguistics, pages\n68–77. Springer.\nGeorg Heigold, Guenter Neumann, and Josef van\nGenabith. 2016.\nNeural morphological tagging\nfrom characters for morphologically rich languages.\narXiv preprint arXiv:1606.06640.\nGeorg Heigold, Guenter Neumann, and Josef van Gen-\nabith. 2017. An extensive empirical evaluation of\ncharacter-based morphological tagging for 14 lan-\nguages. In Proceedings of the 15th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Volume 1, Long Papers, pages\n505–513, Valencia, Spain. Association for Computa-\ntional Linguistics.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997.\nLong short-term memory.\nNeural computation,\n9(8):1735–1780.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich\nSch¨utze. 2021. Superbizarre is not superb: Deriva-\ntional morphology improves BERT’s interpretation\nof complex words. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3594–3608, Online. Associa-\ntion for Computational Linguistics.\nMatt Hohensee and Emily M. Bender. 2012. Getting\nmore from morphology in multilingual dependency\nparsing. In Proceedings of the 2012 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 315–326, Montr´eal, Canada. Associ-\nation for Computational Linguistics.\nMans Hulden, Markus Forsberg, and Malin Ahlberg.\n2014. Semi-supervised learning of morphological\nparadigms and lexicons. In Proceedings of the 14th\nConference of the European Chapter of the Asso-\nciation for Computational Linguistics, pages 569–\n578, Gothenburg, Sweden. Association for Compu-\ntational Linguistics.\nHuiming Jin, Liwei Cai, Yihui Peng, Chen Xia, Arya\nMcCarthy, and Katharina Kann. 2020.\nUnsuper-\nvised morphological paradigm completion. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6696–\n6707, Online. Association for Computational Lin-\nguistics.\nMark Johnson, Thomas L Grifﬁths, Sharon Goldwa-\nter, et al. 2007.\nAdaptor grammars:\nA frame-\nwork for specifying compositional nonparametric\nbayesian models. Advances in neural information\nprocessing systems, 19:641.\nKatharina Kann, Ryan Cotterell, and Hinrich Sch¨utze.\n2016.\nNeural morphological analysis: Encoding-\ndecoding canonical segments.\nIn Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 961–967, Austin,\nTexas. Association for Computational Linguistics.\nKatharina Kann, Ryan Cotterell, and Hinrich Sch¨utze.\n2017a. Neural multi-source morphological reinﬂec-\ntion. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Volume 1, Long Papers, pages\n514–524, Valencia, Spain. Association for Compu-\ntational Linguistics.\nKatharina Kann, Ryan Cotterell, and Hinrich Sch¨utze.\n2017b.\nOne-shot neural cross-lingual transfer for\nparadigm completion. In Proceedings of the 55th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages\n1993–2003, Vancouver, Canada. Association for\nComputational Linguistics.\nKatharina Kann, Stanislas Lauly, and Kyunghyun\nCho. 2018a.\nThe NYU system for the CoNLL–\nSIGMORPHON 2018 shared task on universal mor-\nphological reinﬂection.\nIn Proceedings of the\nCoNLL–SIGMORPHON 2018 Shared Task: Univer-\nsal Morphological Reinﬂection, pages 58–63, Brus-\nsels. Association for Computational Linguistics.\nKatharina\nKann,\nJesus\nManuel\nMager\nHois,\nIvan Vladimir Meza-Ruiz, and Hinrich Sch¨utze.\n2018b.\nFortiﬁcation of neural morphological\nsegmentation models for polysynthetic minimal-\nresource languages.\nIn Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers),\npages 47–57, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nKatharina Kann, Arya D. McCarthy, Garrett Nico-\nlai, and Mans Hulden. 2020. The SIGMORPHON\n2020 shared task on unsupervised morphological\nparadigm completion. In Proceedings of the 17th\nSIGMORPHON Workshop on Computational Re-\nsearch in Phonetics, Phonology, and Morphology,\npages 51–62, Online. Association for Computational\nLinguistics.\nKatharina Kann and Hinrich Sch¨utze. 2016a.\nMED:\nThe LMU system for the SIGMORPHON 2016\nshared task on morphological reinﬂection. In Pro-\nceedings of the 14th SIGMORPHON Workshop on\nComputational Research in Phonetics, Phonology,\nand Morphology, pages 62–70, Berlin, Germany. As-\nsociation for Computational Linguistics.\nKatharina Kann and Hinrich Sch¨utze. 2016b. Single-\nmodel encoder-decoder with explicit morphological\nrepresentation for reinﬂection. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n555–560, Berlin, Germany. Association for Compu-\ntational Linguistics.\nKatharina Kann and Hinrich Sch¨utze. 2017. Unlabeled\ndata for morphological generation with character-\nbased sequence-to-sequence models.\nIn Proceed-\nings of the First Workshop on Subword and Charac-\nter Level Models in NLP, pages 76–81, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nKatharina Kann and Hinrich Sch¨utze. 2018.\nNeural\ntransductive learning and beyond: Morphological\ngeneration in the minimal-resource setting. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 3254–\n3264, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nYova Kementchedjhieva, Johannes Bjerva, and Is-\nabelle Augenstein. 2018. Copenhagen at CoNLL–\nSIGMORPHON 2018:\nMultilingual inﬂection in\ncontext with explicit morphosyntactic decoding. In\nProceedings of the CoNLL–SIGMORPHON 2018\nShared Task: Universal Morphological Reinﬂection,\npages 93–98, Brussels. Association for Computa-\ntional Linguistics.\nSalam Khalifa, Nizar Habash, Fadhl Eryani, Ossama\nObeid, Dana Abdulrahim, and Meera Al Kaabi.\n2018. A morphologically annotated corpus of Emi-\nrati Arabic. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nChristo Kirov, Ryan Cotterell, John Sylak-Glassman,\nG´eraldine Walther, Ekaterina Vylomova, Patrick\nXia, Manaal Faruqui, Sabrina J. Mielke, Arya Mc-\nCarthy, Sandra K¨ubler, et al. 2018.\nUniMorph\n2.0: Universal Morphology. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC 2018), Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nChristo Kirov, John Sylak-Glassman, Roger Que, and\nDavid Yarowsky. 2016.\nVery-large scale pars-\ning and normalization of Wiktionary morphologi-\ncal paradigms.\nIn Proceedings of the Tenth Inter-\nnational Conference on Language Resources and\nEvaluation (LREC’16), pages 3121–3126, Portoroˇz,\nSlovenia. European Language Resources Associa-\ntion (ELRA).\nJudith L. Klavans, editor. 2018.\nProceedings of the\nWorkshop on Computational Modeling of Polysyn-\nthetic Languages. Association for Computational\nLinguistics, Santa Fe, New Mexico, USA.\nElena\nKlyachko,\nAlexey\nSorokin,\nNatalia\nKrizhanovskaya,\nAndrew\nKrizhanovsky,\nand\nGalina Ryazanskaya. 2020.\nLowResourceEval-\n2019:\na shared task on morphological analysis\nfor\nlow-resource\nlanguages.\narXiv\npreprint\narXiv:2001.11285.\nDan Kondratyuk. 2019. Cross-lingual lemmatization\nand morphology tagging with two-stage multilin-\ngual BERT ﬁne-tuning. In Proceedings of the 16th\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology, pages 12–18, Florence,\nItaly. Association for Computational Linguistics.\nLingpeng Kong, Chris Dyer, and Noah A Smith.\n2015. Segmental recurrent neural networks. arXiv\npreprint arXiv:1511.06018.\nMikko Kurimo, Sami Virpioja, Ville Turunen, and\nKrista Lagus. 2010. Morpho challenge 2005-2010:\nEvaluations and results.\nIn Proceedings of the\n11th Meeting of the ACL Special Interest Group on\nComputational Morphology and Phonology, pages\n87–95, Uppsala, Sweden. Association for Computa-\ntional Linguistics.\nIlmari Kylli¨ainen and Miikka Silfverberg. 2019. En-\nsembles of neural morphological inﬂection models.\nIn Proceedings of the 22nd Nordic Conference on\nComputational Linguistics, pages 304–309, Turku,\nFinland. Link¨oping University Electronic Press.\nJohn Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001.\nConditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data. In ICML.\nConstantine Lignos, Erwin Chan, Mitchell P Marcus,\nand Charles Yang. 2009. A rule-based unsupervised\nmorphology learning framework. In CLEF (Work-\ning Notes).\nLing Liu, Ilamvazhuthy Subbiah, Adam Wiemerslage,\nJonathan Lilley, and Sarah Moeller. 2018. Morpho-\nlogical reinﬂection in context: CU boulder’s submis-\nsion to CoNLL–SIGMORPHON 2018 shared task.\nIn Proceedings of the CoNLL–SIGMORPHON 2018\nShared Task: Universal Morphological Reinﬂection,\npages 86–92, Brussels. Association for Computa-\ntional Linguistics.\nBrian MacWhinney. 2014.\nThe CHILDES project:\nTools for analyzing talk. Psychology Press.\nManuel Mager, ¨Ozlem C¸ etino˘glu, and Katharina Kann.\n2020.\nTackling the low-resource challenge for\ncanonical segmentation. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5237–5250, On-\nline. Association for Computational Linguistics.\nManuel Mager and Katharina Kann. 2020.\nThe\nIMS–CUBoulder system for the SIGMORPHON\n2020 shared task on unsupervised morphological\nparadigm completion. In Proceedings of the 17th\nSIGMORPHON Workshop on Computational Re-\nsearch in Phonetics, Phonology, and Morphology,\npages 99–105, Online. Association for Computa-\ntional Linguistics.\nManuel Mager, Arturo Oncevay, Abteen Ebrahimi,\nJohn Ortega,\nAnnette Rios,\nAngela Fan,\nXi-\nmena Gutierrez-Vasques, Luis Chiruzzo, Gustavo\nGim´enez-Lugo, Ricardo Ramos, et al. 2021. Find-\nings of the AmericasNLP 2021 shared task on open\nmachine translation for indigenous languages of the\nAmericas. In Proceedings of the First Workshop on\nNatural Language Processing for Indigenous Lan-\nguages of the Americas, pages 202–217, Online. As-\nsociation for Computational Linguistics.\nPeter Makarov and Simon Clematide. 2018a.\nImita-\ntion learning for neural morphological string trans-\nduction. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2877–2882, Brussels, Belgium. Association\nfor Computational Linguistics.\nPeter Makarov and Simon Clematide. 2018b.\nNeu-\nral transition-based string transduction for limited-\nresource setting in morphology. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 83–93, Santa Fe, New Mexico,\nUSA. Association for Computational Linguistics.\nPeter Makarov and Simon Clematide. 2018c. UZH at\nCoNLL–SIGMORPHON 2018 shared task on uni-\nversal morphological reinﬂection.\nIn Proceedings\nof the CoNLL–SIGMORPHON 2018 Shared Task:\nUniversal Morphological Reinﬂection, pages 69–75,\nBrussels. Association for Computational Linguis-\ntics.\nPeter Makarov, Tatiana Ruzsics, and Simon Clematide.\n2017.\nAlign and copy: UZH at SIGMORPHON\n2017 shared task for morphological reinﬂection. In\nProceedings of the CoNLL SIGMORPHON 2017\nShared Task: Universal Morphological Reinﬂection,\npages 49–57, Vancouver. Association for Computa-\ntional Linguistics.\nChaitanya Malaviya, Matthew R. Gormley, and Gra-\nham Neubig. 2018. Neural factor graph models for\ncross-lingual morphological tagging.\nIn Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2653–2663, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nChaitanya Malaviya, Shijie Wu, and Ryan Cotterell.\n2019. A simple joint model for improved contextual\nneural lemmatization. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 1517–1528, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nAndreas Maletti and Matthieu Constant. 2011.\nPro-\nceedings of the 9th international workshop on ﬁnite\nstate methods and natural language processing. In\nProceedings of the 9th International Workshop on\nFinite State Methods and Natural Language Process-\ning.\nRobert Malouf, Farrell Ackerman, and Arturs Se-\nmenuks. 2020. Lexical databases for computational\nanalyses: A linguistic perspective. In Proceedings\nof the Society for Computation in Linguistics 2020,\npages 446–456, New York, New York. Association\nfor Computational Linguistics.\nArya D. McCarthy, Christo Kirov, Matteo Grella, Am-\nrit Nidhi, Patrick Xia, Kyle Gorman, Ekaterina Vy-\nlomova, Sabrina J. Mielke, Garrett Nicolai, Miikka\nSilfverberg, et al. 2020a.\nUniMorph 3.0: Univer-\nsal Morphology.\nIn Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n3922–3931, Marseille, France. European Language\nResources Association.\nArya D. McCarthy, Miikka Silfverberg, Ryan Cotterell,\nMans Hulden, and David Yarowsky. 2018. Marrying\nUniversal Dependencies and Universal Morphology.\nIn Proceedings of the Second Workshop on Univer-\nsal Dependencies (UDW 2018), pages 91–101, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nArya D. McCarthy, Ekaterina Vylomova, Shijie Wu,\nChaitanya Malaviya, Lawrence Wolf-Sonkin, Gar-\nrett Nicolai, Christo Kirov, Miikka Silfverberg, Sab-\nrina J. Mielke, Jeffrey Heinz, et al. 2019. The SIG-\nMORPHON 2019 shared task: Morphological anal-\nysis in context and cross-lingual transfer for inﬂec-\ntion. In Proceedings of the 16th Workshop on Com-\nputational Research in Phonetics, Phonology, and\nMorphology, pages 229–244, Florence, Italy. Asso-\nciation for Computational Linguistics.\nArya D. McCarthy, Rachel Wicks, Dylan Lewis, Aaron\nMueller, Winston Wu, Oliver Adams, Garrett Nico-\nlai, Matt Post, and David Yarowsky. 2020b.\nThe\nJohns Hopkins University Bible corpus:\n1600+\ntongues for typological exploration.\nIn Proceed-\nings of the 12th Language Resources and Evaluation\nConference, pages 2884–2892, Marseille, France.\nEuropean Language Resources Association.\nKate McCurdy, Sharon Goldwater, and Adam Lopez.\n2021.\nAdaptor\nGrammars\nfor\nunsupervised\nparadigm clustering. In Proceedings of the 18th SIG-\nMORPHON Workshop on Computational Research\nin Phonetics, Phonology, and Morphology, pages\n82–89, Online. Association for Computational Lin-\nguistics.\nEleni Metheniti and Guenter Neumann. 2020. Wikin-\nﬂection corpus: A (better) multilingual, morpheme-\nannotated inﬂectional corpus.\nIn Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 3905–3912, Marseille, France. Euro-\npean Language Resources Association.\nPaul Michel and Graham Neubig. 2018.\nMTNT: A\ntestbed for machine translation of noisy text. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 543–\n553, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nJeffrey Micher. 2017. Improving coverage of an Inuk-\ntitut morphological analyzer using a segmental re-\ncurrent neural network. In Proceedings of the 2nd\nWorkshop on the Use of Computational Methods in\nthe Study of Endangered Languages, pages 101–106,\nHonolulu. Association for Computational Linguis-\ntics.\nSarah Moeller, Ling Liu, Changbing Yang, Katharina\nKann, and Mans Hulden. 2020. IGT2P: From inter-\nlinear glossed texts to paradigms. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5251–\n5262, Online. Association for Computational Lin-\nguistics.\nTumi Moeng, Sheldon Reay, Aaron Daniels, and Jan\nBuys. 2021.\nCanonical and surface morpholog-\nical segmentation for Nguni languages.\nCoRR,\nabs/2104.00767.\nChristian Monson, Jaime Carbonell, Alon Lavie, and\nLori Levin. 2007.\nParaMor: Finding paradigms\nacross morphology.\nIn Workshop of the Cross-\nLanguage Evaluation Forum for European Lan-\nguages, pages 900–907. Springer.\nJessica L. Montag, Michael N. Jones, and Linda B.\nSmith. 2015.\nThe words children hear:\nPic-\nture books and the statistics for language learning.\nPsychological Science, 26(9):1489–1496.\nPMID:\n26243292.\nThomas Mueller, Helmut Schmid, and Hinrich Sch¨utze.\n2013. Efﬁcient higher-order CRFs for morphologi-\ncal tagging. In Proceedings of the 2013 Conference\non Empirical Methods in Natural Language Process-\ning, pages 322–332, Seattle, Washington, USA. As-\nsociation for Computational Linguistics.\nKarthik Narasimhan, Regina Barzilay, and Tommi\nJaakkola. 2015. An unsupervised method for uncov-\nering morphological chains. Transactions of the As-\nsociation for Computational Linguistics, 3:157–167.\nMinh Van Nguyen, Viet Dac Lai, Amir Pouran\nBen Veyseh, and Thien Huu Nguyen. 2021. Trankit:\nA light-weight transformer-based toolkit for multi-\nlingual natural language processing. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Sys-\ntem Demonstrations, pages 80–90, Online. Associa-\ntion for Computational Linguistics.\nGarrett Nicolai, Colin Cherry, and Grzegorz Kondrak.\n2015. Inﬂection generation as discriminative string\ntransduction.\nIn Proceedings of the 2015 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 922–931, Denver, Col-\norado. Association for Computational Linguistics.\nGarrett Nicolai, Kyle Gorman, and Ryan Cotterell, edi-\ntors. 2021. Proceedings of the 18th SIGMORPHON\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology. Association for Com-\nputational Linguistics, Online.\nGarrett Nicolai, Dylan Lewis, Arya D. McCarthy,\nAaron Mueller, Winston Wu, and David Yarowsky.\n2020.\nFine-grained morphosyntactic analysis and\ngeneration tools for more than one thousand lan-\nguages. In Proceedings of the 12th Language Re-\nsources and Evaluation Conference, pages 3963–\n3972, Marseille, France. European Language Re-\nsources Association.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019.\nfairseq: A fast, extensible\ntoolkit for sequence modeling.\nIn Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nHyunji Hayley Park, Katherine J. Zhang, Coleman Ha-\nley, Kenneth Steimel, Han Liu, and Lane Schwartz.\n2021.\nMorphology Matters: A Multilingual Lan-\nguage Modeling Analysis. Transactions of the Asso-\nciation for Computational Linguistics, 9:261–276.\nTiago Pimentel, Arya D. McCarthy, Damian Blasi,\nBrian Roark, and Ryan Cotterell. 2019.\nMeaning\nto form: Measuring systematicity as information. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1751–\n1764, Florence, Italy. Association for Computational\nLinguistics.\nTiago Pimentel, Maria Ryskina, Sabrina J. Mielke,\nShijie Wu, Eleanor Chodroff, Brian Leonard, Gar-\nrett Nicolai, Yustinus Ghanggo Ate, Salam Khal-\nifa, Nizar Habash, et al. 2021.\nSigmorphon 2021\nshared task on morphological reinﬂection: Gener-\nalization across languages.\nIn Proceedings of the\n18th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 229–259, Online. Association for Computa-\ntional Linguistics.\nHoifung Poon, Colin Cherry, and Kristina Toutanova.\n2009.\nUnsupervised morphological segmentation\nwith log-linear models. In Proceedings of Human\nLanguage Technologies: The 2009 Annual Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics, pages 209–217,\nBoulder, Colorado. Association for Computational\nLinguistics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D. Manning. 2020.\nStanza:\nA\npython natural language processing toolkit for many\nhuman languages. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics:\nSystem Demonstrations, pages 101–\n108, Online. Association for Computational Linguis-\ntics.\nMohammad Sadegh Rasooli and Michael Collins. 2017.\nCross-lingual syntactic transfer with limited re-\nsources. Transactions of the Association for Com-\nputational Linguistics, 5:279–293.\nTeemu Ruokolainen, Oskar Kohonen, Sami Virpioja,\nand Mikko Kurimo. 2013. Supervised morpholog-\nical segmentation in a low-resource learning setting\nusing conditional random ﬁelds. In Proceedings of\nthe Seventeenth Conference on Computational Nat-\nural Language Learning, pages 29–37, Soﬁa, Bul-\ngaria. Association for Computational Linguistics.\nTeemu Ruokolainen, Oskar Kohonen, Sami Virpioja,\nand Mikko Kurimo. 2014. Painless semi-supervised\nmorphological segmentation using conditional ran-\ndom ﬁelds. In Proceedings of the 14th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, volume 2: Short Papers, pages\n84–89, Gothenburg, Sweden. Association for Com-\nputational Linguistics.\nTatyana Ruzsics and Tanja Samardˇzi´c. 2017.\nNeu-\nral sequence-to-sequence learning of internal word\nstructure. In Proceedings of the 21st Conference on\nComputational Natural Language Learning (CoNLL\n2017), pages 184–194, Vancouver, Canada. Associa-\ntion for Computational Linguistics.\nPatrick Schone and Daniel Jurafsky. 2001. Knowledge-\nfree induction of inﬂectional morphologies. In Sec-\nond Meeting of the North American Chapter of the\nAssociation for Computational Linguistics.\nWolfgang Seeker and ¨Ozlem C¸ etino˘glu. 2015. A graph-\nbased lattice dependency parser for joint morpholog-\nical segmentation and syntactic analysis. Transac-\ntions of the Association for Computational Linguis-\ntics, 3:359–373.\nAmit Seker and Reut Tsarfaty. 2020.\nA pointer net-\nwork architecture for joint morphological segmen-\ntation and tagging. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4368–4378, Online. Association for Computational\nLinguistics.\nAbhishek Sharma, Ganesh Katrapati, and Dipti Misra\nSharma. 2018.\nIIT(BHU)–IIITH at CoNLL–\nSIGMORPHON 2018 shared task on universal mor-\nphological reinﬂection.\nIn Proceedings of the\nCoNLL–SIGMORPHON 2018 Shared Task:\nUni-\nversal Morphological Reinﬂection, pages 105–111,\nBrussels. Association for Computational Linguis-\ntics.\nMiikka Silfverberg and Mans Hulden. 2018.\nAn\nencoder-decoder approach to the paradigm cell ﬁll-\ning problem. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2883–2889, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nMiikka Silfverberg, Adam Wiemerslage, Ling Liu, and\nLingshuang Jack Mao. 2017. Data augmentation for\nmorphological reinﬂection.\nIn Proceedings of the\nCoNLL SIGMORPHON 2017 Shared Task: Univer-\nsal Morphological Reinﬂection, pages 90–99, Van-\ncouver. Association for Computational Linguistics.\nAssaf Singer and Katharina Kann. 2020. The NYU-\nCUBoulder systems for SIGMORPHON 2020 task\n0 and task 2.\nIn Proceedings of the 17th SIG-\nMORPHON Workshop on Computational Research\nin Phonetics, Phonology, and Morphology, pages\n90–98, Online. Association for Computational Lin-\nguistics.\nPeter Smit, Sami Virpioja, Stig-Arne Gr¨onroos, Mikko\nKurimo, et al. 2014. Morfessor 2.0: Toolkit for sta-\ntistical morphological segmentation.\nIn The 14th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics (EACL), Gothen-\nburg, Sweden, April 26-30, 2014. Aalto University.\nNoah A. Smith and Jason Eisner. 2005. Contrastive es-\ntimation: Training log-linear models on unlabeled\ndata.\nIn Proceedings of the 43rd Annual Meet-\ning of the Association for Computational Linguis-\ntics (ACL’05), pages 354–362, Ann Arbor, Michi-\ngan. Association for Computational Linguistics.\nAlexey Sorokin. 2019. Convolutional neural networks\nfor low-resource morpheme segmentation: baseline\nor state-of-the-art?\nIn Proceedings of the 16th\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology, pages 154–159, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMilan Straka, Jana Strakov´a, and Jan Hajic. 2019. UD-\nPipe at SIGMORPHON 2019: Contextualized em-\nbeddings, regularization with morphological cate-\ngories, corpora merging. In Proceedings of the 16th\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology, pages 95–103, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nJohn Sylak-Glassman, Christo Kirov, Matt Post, Roger\nQue, and David Yarowsky. 2015a. A universal fea-\nture schema for rich morphological annotation and\nﬁne-grained cross-lingual part-of-speech tagging. In\nSystems and Frameworks for Computational Mor-\nphology, pages 72–93, Cham. Springer International\nPublishing.\nJohn Sylak-Glassman, Christo Kirov, David Yarowsky,\nand Roger Que. 2015b.\nA language-independent\nfeature schema for inﬂectional morphology. In Pro-\nceedings of the 53rd Annual Meeting of the Associ-\nation for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 674–\n680, Beijing, China. Association for Computational\nLinguistics.\nG´abor Szolnok, Botond Barta, Dorina Lakatos, and\nJudit ´Acs. 2021. Bme submission for sigmorphon\n2021 shared task 0. a three step training approach\nwith data augmentation for morphological inﬂec-\ntion.\nIn Proceedings of the 18th SIGMORPHON\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology, pages 268–273, On-\nline. Association for Computational Linguistics.\nAleˇs Tamchyna, Marion Weller-Di Marco, and Alexan-\nder Fraser. 2017.\nModeling target-side inﬂection\nin neural machine translation.\nIn Proceedings of\nthe Second Conference on Machine Translation,\npages 32–42, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. Advances in neural infor-\nmation processing systems, 28.\nSami Virpioja, Oskar Kohonen, and Krista Lagus. 2009.\nUnsupervised morpheme discovery with Allomor-\nfessor. In CLEF (Working Notes).\nLinlin Wang, Zhu Cao, Yu Xia, and Gerard De Melo.\n2016.\nMorphological segmentation with window\nLSTM neural networks. In Thirtieth AAAI Confer-\nence on Artiﬁcial Intelligence.\nWeihua Wang, Rashel Fam, Feilong Bao, Yves Lep-\nage, and Guanglai Gao. 2019. Neural morpholog-\nical segmentation model for Mongolian.\nIn 2019\nInternational Joint Conference on Neural Networks\n(IJCNN), pages 1–7. IEEE.\nAdam Wiemerslage, Arya D. McCarthy, Alexander\nErdmann, Garrett Nicolai, Manex Agirrezabal, Mi-\nikka Silfverberg, Mans Hulden, and Katharina Kann.\n2021. Findings of the SIGMORPHON 2021 shared\ntask on unsupervised morphological paradigm clus-\ntering. In Proceedings of the 18th SIGMORPHON\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology, pages 72–81, Online.\nAssociation for Computational Linguistics.\nShijie Wu, Ryan Cotterell, and Mans Hulden. 2021.\nApplying the transformer to character-level transduc-\ntion. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pages 1901–1907,\nOnline. Association for Computational Linguistics.\nShijie Wu, Ryan Cotterell, and Timothy O’Donnell.\n2019. Morphological irregularity correlates with fre-\nquency. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 5117–5126, Florence, Italy. Association\nfor Computational Linguistics.\nHongzhi Xu, Jordan Kodner, Mitchell Marcus, and\nCharles Yang. 2020.\nModeling morphological ty-\npology for unsupervised learning of language mor-\nphology. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6672–6681, Online. Association for Computa-\ntional Linguistics.\nRuochen Xu, Yiming Yang, Naoki Otani, and Yuexin\nWu. 2018.\nUnsupervised cross-lingual transfer of\nword embedding spaces. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2465–2474, Brussels, Bel-\ngium. Association for Computational Linguistics.\nChangbing Yang, Garrett Nicolai, and Miikka Silfver-\nberg. 2021. Unsupervised paradigm clustering us-\ning transformation rules.\nIn Proceedings of the\n18th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphol-\nogy, pages 98–106, Online. Association for Compu-\ntational Linguistics.\nDavid Yarowsky, Grace Ngai, and Richard Wicen-\ntowski. 2001.\nInducing multilingual text analysis\ntools via robust projection across aligned corpora. In\nProceedings of the First International Conference on\nHuman Language Technology Research.\nDavid Yarowsky and Richard Wicentowski. 2000. Min-\nimally supervised morphological analysis by multi-\nmodal alignment. In Proceedings of the 38th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 207–216, Hong Kong. Association\nfor Computational Linguistics.\nNasser Zalmout and Nizar Habash. 2020.\nUtiliz-\ning subword entities in character-level sequence-to-\nsequence lemmatization models.\nIn Proceedings\nof the 28th International Conference on Compu-\ntational Linguistics, pages 4676–4682, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nDaniel Zeman,\nJoakim Nivre,\nMitchell Abrams,\nElia Ackermann, No¨emi Aepli, Hamid Aghaei,\nˇZeljko\nAgi´c,\nAmir\nAhmadi,\nLars\nAhrenberg,\nChika Kennedy Ajede, et al. 2021.\nUniversal de-\npendencies 2.8. LINDAT/CLARIAH-CZ digital li-\nbrary at the Institute of Formal and Applied Linguis-\ntics ( ´UFAL), Faculty of Mathematics and Physics,\nCharles University.\nAnna Zueva, Anastasia Kuznetsova, and Francis Ty-\ners. 2020. A ﬁnite-state morphological analyser for\nEvenki. In Proceedings of the 12th Language Re-\nsources and Evaluation Conference, pages 2581–\n2589, Marseille, France. European Language Re-\nsources Association.\nA\nRemaining Results from Main Text\nThe statistics of the data used in our experiments\nis given in Table A.1. Paradigm clustering BMF1\nis given in Table A.2. Additionally, BMAcc on the\ntwo test corpora is given in Figure A.1.\nB\nHyperparameters\nB.1\nMorphological Inﬂection\nTraining\nWe train all inﬂection models on the\n(word, source slot, target slot) triples produced by\nthe slot alignment. Each inﬂection system consid-\ners the word as an input form, and the slots as the\ntags. We take the hyperparameters from (Makarov\nand Clematide, 2018a), and (Wu et al., 2021) ex-\nactly for each language. For the LSTM, we train\na single layer bidirectional encoder with embed-\nding size 100, and LSTM hidden size of 100. The\ndecoder is also a single layer LSTM with hidden\nsize 100. We employ a soft-attention mechanism\n(Bahdanau et al., 2015), and optimize with Adam\n(Kingma and Ba, 2014) with a learning rate of\n0.001, and a gradient clip of 1.0. We train for up\nto 30 epochs, and a batch size of 16. We employ a\nsoft attention mechanism (Bahdanau et al., 2015).\nB.2\nSlot Prediction\nThe slot prediction model is a character-level Trans-\nformer encoder-decoder, where both the encoder\nand decoder have 3 layers and 4 attention heads.\nWe optimize with Adam with a learning rate of\n0.0001, and a clip norm of 0.2 for up to 5 epochs.\nC\nAdditional Details Regarding our\nDatasets\nC.1\nStatistics of Our Raw-text Corpora\nWe give dataset statistics in Table A.1, including\ntype–token ratios. Bible sizes vary depending on\nwhether or not the Old Testament is included. In the\ncase of smaller Bibles, we down-sample the child-\ndirected corpus to have a roughly equal number of\ntokens.\nC.2\nTest Set Creation\nWe use lemmas and POS tag annotations to match\nwords from the test corpora with UniMorph entries.\nWe sample sentences from the annotated Wikipedia\ncorpora (Ginter et al., 2017) from the ConLL 2017\nshared task on Multilingual Parsing (Hajiˇc and Ze-\nman, 2017). For Icelandic, which is not included\nin this dataset, we use wikiextractor (Attardi, 2015)\nto get the raw Wikipedia text, and acquire lemma\nand POS annotations with Stanza (Qi et al., 2020).\nWe hypothesize that systems trained on the Bible\ncorpus may not generalize well to the modern lan-\nguage in Wikipedia. We thus additionally sample\ntest sentences from the JW300 corpus, which is\nmore likely to include religious language that re-\nsembles that of the bible. For JW300 we rely on\nthe tokenization provided by the authors, but we\nagain use Stanza for lemma and POS annotations.\nFor a given language and test corpus, we group\ngold paradigms by POS, and whether at least one\nform from the paradigm is attested in both train-\ning corpora. This means we have two categories\nfor each POS: seen, wherein at least one form is at-\ntested in both training corpora, and unseen, wherein\nno forms are attested in either training corpus. We\nsample up to 200 paradigms from each category,\nensuring that each category contributes an equal\nnumber of paradigms to the gold set. Then one\nsurface form for each gold paradigm is sampled at\nrandom, in context, from the test corpus to serve as\ninput to the systems at test time.\nD\nNon-Neural Baseline for tUMPC\nGiven the set of word form clusters c1, ..., ck,\nwhere each cluster ci = {f1, ..., fn} is a collec-\ntion of forms fj. We start by extracting all edit\ntrees t = EditTree(f, f′) (Chrupała, 2008), where\nf and f′ belong to the same cluster. Let Count(t)\nbe the count of tree t across the entire training\nset. Further, let MLen(t) be the total number of\ncharacters which have to match in the input string,\nwhen we apply edit tree t. For example, for an\nedit tree t which maps walking to walks, a suf-\nﬁx ing must match, so MLen(t) = 3. Finally, let\nMStr(t) = u be the string consisting of all inser-\ntions performed by the edit tree. For the given\nexample t, MStr(t) = s\nWhen generating outputs for a given form f,\nwe ﬁrst form the set of all edit trees which can\nbe applied to f. We then order them in the fol-\nlowing way: t > t′ if MLen(t) > MLen(t′), or\nif the precondition lengths are equal, Count(t) >\nCount(t′). We then apply the top-N trees to f\nto generate all remaining forms in the inﬂectional\nparadigm of f. We set N to the 95th percentile\nof paradigm sizes in our input cluster data, not\ncounting singleton paradigms. Each slot labed is\nassigned based on t as MStr(t). Note that this will\ntypically not generate a slot label for the input form\nCorpus\nLanguage\nLines\nTokens\nTypes\nType–Token Ratio\nBible\nGerman\n31102\n813317\n20644\n0.025\nGreek\n7914\n194135\n15541\n0.080\nIcelandic\n7860\n185995\n13050\n0.070\nRussian\n31102\n714828\n43542\n0.061\nChild Directed\nGerman\n26592\n633229\n31384\n0.050\nGreek\n8513\n196344\n18424\n0.090\nIcelandic\n8380\n181687\n17767\n0.101\nRussian\n26592\n586274\n44823\n0.077\nTable A.1: Statistics for raw text corpora used for morphology learning\nBible\nChild-Directed\nSystem\nDEU\nELL\nISL\nRUS\nAverage\nDEU\nELL\nISL\nRUS\nAverage\nMcC\n79.19\n81.91\n81.66\n82.01\n81.19\n87.72\n73.68\n84.65\n86.28\n83.08\nXu\n63.90\n65.14\n67.81\n52.80\n63.91\n70.02\n46.14\n55.22\n63.48\n58.72\nSIG\n46.04\n57.22\n47.24\n45.10\n48.90\n45.69\n47.04\n43.08\n47.80\n45.90\nTable A.2: Paradigm clustering BMF1 scores for a sample of clusters attested in UniMorph.\njw300\nwiki\nFigure A.1: BMAcc for both slot alignment systems on each test corpus, averaged over results for all input clusters.\nThe POS-based system is also averaged over each inﬂection system.\nf. We, therefore, ﬁnd the maximal edit tree t (in\nthe sense that it has maximal precondition length\nand count) which translates one of the generated\nforms f′ back into the original input form f. The\nslot label for form f is then MStr(t).\nA comparison between baseline and our pro-\nposed POS-based system is shown in Figure A.1.\nThe latter outperforms baseline in the majority\nof settings, often by a large margin.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-03-16",
  "updated": "2022-03-16"
}