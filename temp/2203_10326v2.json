{
  "id": "http://arxiv.org/abs/2203.10326v2",
  "title": "Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models",
  "authors": [
    "Ryokan Ri",
    "Yoshimasa Tsuruoka"
  ],
  "abstract": "We investigate what kind of structural knowledge learned in neural network\nencoders is transferable to processing natural language. We design artificial\nlanguages with structural properties that mimic natural language, pretrain\nencoders on the data, and see how much performance the encoder exhibits on\ndownstream tasks in natural language. Our experimental results show that\npretraining with an artificial language with a nesting dependency structure\nprovides some knowledge transferable to natural language. A follow-up probing\nanalysis indicates that its success in the transfer is related to the amount of\nencoded contextual information and what is transferred is the knowledge of\nposition-aware context dependence of language. Our results provide insights\ninto how neural network encoders process human languages and the source of\ncross-lingual transferability of recent multilingual language models.",
  "text": "Pretraining with Artiﬁcial Language:\nStudying Transferable Knowledge in Language Models\nRyokan Ri and Yoshimasa Tsuruoka\nThe University of Tokyo\n7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan\n{li0123,tsuruoka}@logos.t.u-tokyo.ac.jp\nAbstract\nWe investigate what kind of structural knowl-\nedge learned in neural network encoders is\ntransferable to processing natural language.\nWe design artiﬁcial languages with structural\nproperties that mimic natural language, pre-\ntrain encoders on the data, and see how much\nperformance the encoder exhibits on down-\nstream tasks in natural language. Our exper-\nimental results show that pretraining with an\nartiﬁcial language with a nesting dependency\nstructure provides some knowledge transfer-\nable to natural language. A follow-up probing\nanalysis indicates that its success in the trans-\nfer is related to the amount of encoded con-\ntextual information and what is transferred is\nthe knowledge of position-aware context de-\npendence of language. Our results provide in-\nsights into how neural network encoders pro-\ncess human languages and the source of cross-\nlingual transferability of recent multilingual\nlanguage models.\n1\nIntroduction\nPretrained language models (Devlin et al., 2019;\nYang et al., 2019; Raffel et al., 2020) have demon-\nstrated strong empirical performance not only\nwithin a language but also across languages. Lan-\nguage models pretrained with a mix of monolingual\ncorpora, such as multilingual BERT, exhibit a de-\ncent zero-shot cross-lingual transfer capability, i.e.,\na model ﬁne-tuned in a single source language (L1)\ncan solve the task in another language (L2) (Con-\nneau et al., 2020a; Xue et al., 2021). Surprisingly,\nthe transfer happens without lexical overlaps be-\ntween L1 and L2 (Karthikeyan K and Roth, 2020;\nConneau et al., 2020b) or even without joint pre-\ntraining (Artetxe et al., 2020): an encoder only\npretrained on L1 can be transferred to L2 without\nany parameter updates. These results suggest that,\nwhether the encoder is trained on single or multiple\nlanguages, it learns some transferable knowledge\nabout language.\nFigure 1: Transfer from artiﬁcial language to natural\nlanguage. The artiﬁcial language encodes some struc-\ntural properties (e.g., token distributions, dependency\nstructures) and we study how the learning of such prop-\nerties can be transferred to natural language.\nHowever, the characteristics of such transferable\nknowledge are still underexplored. Recent stud-\nies with the probing methodology (Hupkes and\nZuidema, 2018; Conneau et al., 2018) have re-\nvealed that multilingual BERT captures language-\nindependent linguistic structures such as universal\ndependency relations (Chi et al., 2020) and subject-\nhood (Papadimitriou et al., 2021), but it remains\nunknown whether learning such linguistic proper-\nties actually contributes to the performance, and\nwhether there exists more abstract knowledge trans-\nferred across languages.\nIn this study, we try to shed light on these ques-\ntions with the framework of the Test for Inductive\nBias via Language Model Transfer (Papadimitriou\nand Jurafsky, 2020), focusing on designing arti-\nﬁcial languages with natural-language-like struc-\ntural properties (Figure 1). We pretrain encoders\narXiv:2203.10326v2  [cs.CL]  22 Mar 2022\nwith artiﬁcial languages and transfer the encoders\nto natural language tasks with their parameters\nfrozen. This enables us to see how learning the\nspeciﬁc structural properties of the artiﬁcial lan-\nguage affects the downstream performance.\nSpeciﬁcally, we explore whether it is beneﬁcial\nfor the encoder to know the following two char-\nacteristics of natural language: word distributions\nand latent dependency structures. We design arti-\nﬁcial languages that represent such characteristics\nand perform an extensive study with different en-\ncoder architectures (LSTM and Transformer) pre-\ntraining objectives (causal and masked language\nmodelings).\nThe contribution is summarized as follows:\n• We ﬁrst start by complementing the study\nin Papadimitriou and Jurafsky (2020). We\ntrain LSTM and Transformer encoders with\nthe sentence-level causal language modeling\ntask and evaluate the encoders in English. We\nshow that an artiﬁcial language that models\nsimple statistical dependency within a sen-\ntence provides decent transferable knowledge\non natural language modeling. Furthermore,\nwe ﬁnd that the inductive bias of a nesting\nhead-to-tail dependency structure is more use-\nful than a ﬂat one.\n• We then proceed to investigate transfer learn-\ning in masked language modeling (Devlin\net al., 2019), one of the current dominant pre-\ntraining paradigms. We evaluate pretrained\nTransformer encoders with dependency pars-\ning and conﬁrm that the nesting dependency\nstructure is important to learn the structure of\nnatural language.\n• We hypothesize that the transfer performance\nof pretrained encoders is related to the way\nthe encoder preserves the input contextual in-\nformation in the output vectors. We perform a\nprobing experiment and ﬁnd that the artiﬁcial\nlanguage with the nesting dependency struc-\nture trains encoders to encode the information\non adjacent tokens into the output vector of\neach token. We conclude this paper with the\nhypothesis that a part of transferable knowl-\nedge in language models could be explained\nby the knowledge of position-aware context\ndependence of language.\n2\nRelated Work\n2.1\nTransferable Structural Knowledge in\nPretrained Encoders\nMultilingual language models trained with masked\nlanguage modeling objective (Devlin et al., 2019;\nDoddapaneni et al., 2021) have demonstrated a\nsurprisingly strong cross-lingual transfer capability\n(Liu et al., 2020), given the model is only trained\nwith a mix of monolingual corpora. This leads\nto several studies investigating the source of the\ncross-lingual capability of multilingual models.\nAn early common hypothesis was that the mod-\nels take advantage of a common word-piece vo-\ncabulary across languages (Wu and Dredze, 2019;\nPires et al., 2019), which provides cross-lingual\nalignment signals to learn useful multilingual rep-\nresentations. However, this hypothesis has been\nquestioned by recent studies (Karthikeyan K and\nRoth, 2020; Conneau et al., 2020b) which show\nthat shared word-pieces only play a minor role in\nthe performance. These studies suggest that the\nmodel can exploit abstract structures of languages\nto learn shared multilingual representations.\nAnother line of research suggests that the learn-\ning of transferable knowledge happens even in\nmonolingual pretraining.\nArtetxe et al. (2020)\nshowed that a Transformer encoder pretrained only\non L1 exhibits strong cross-lingual transfer perfor-\nmance simply by aligning the L2 embeddings to\nthe encoder. Papadimitriou and Jurafsky (2020)\npretrained LSTM encoders with natural languages\nand non-linguistic data (e.g., code, music, and arti-\nﬁcial data) to demonstrate that the encoders achieve\nreasonable performance in Spanish language mod-\neling. These studies provide additional evidence\nfor the existence of transferable linguistic knowl-\nedge learned in the model.\nThen what is such knowledge? Probing studies\n(Hupkes and Zuidema, 2018; Conneau et al., 2018)\nhave revealed that the model captures language-\nindependent structures such as universal depen-\ndency relations (Chi et al., 2020) and subjecthood\n(Papadimitriou et al., 2021). However, the probing\nmethodology does not answer whether such lin-\nguistic knowledge contributes to the performance\nin cross-lingual transfer.\nIn this study, we shed light on this question\nby studying transfer learning from artiﬁcial lan-\nguage with the Test for Inductive Bias via Lan-\nguage Model Transfer (TILT) (Papadimitriou and\nJurafsky, 2020). This framework enables us to\nassess if abstract features generalizable to L2 (nat-\nural language) are encoded in L1. Here we explic-\nitly design artiﬁcial languages with some structural\nproperties as L1 to investigate their transferability.\n2.2\nStudying Language Models with\nArtiﬁcial Language\nTo study the behavior of language models, sev-\neral studies have employed a speciﬁc type of ar-\ntiﬁcial language: artiﬁcial variants of natural lan-\nguages. A typical experimental framework is as\nfollows: (1) create an artiﬁcial language that dif-\nfers from a natural language in one linguistic prop-\nerty, such as word orders (Sinha et al., 2021b;\nDufter and Schütze, 2020; Sinha et al., 2021a),\nscripts (Karthikeyan K and Roth, 2020; Dufter and\nSchütze, 2020; Conneau et al., 2020b), or morphol-\nogy (Ravfogel et al., 2019); (2) train or evaluate\nthe natural/artiﬁcial language models and compare\nthe performance to analyze the model’s sensitivity\nto the linguistic property.\nHowever, this methodology is limited to study-\ning linguistic properties that are easily editable\nto create artiﬁcial variants and also offers limited\ncontrol over the experiments. To overcome this\nproblem, White and Cotterell (2021) created artiﬁ-\ncial languages by deﬁning their own probabilistic\ncontext-free grammars (PCFG). As the concurrent\nwork, Chiang and yi Lee (2022) trained Trans-\nformer encoders on artiﬁcial data with token de-\npendencies in the sequences and showed that they\nperform reasonably well on the GLUE benchmark\n(Wang et al., 2019). In this research, we design\nartiﬁcial languages with certain structural proper-\nties from scratch to study knowledge transferable\nto natural language.\n3\nApproach\n3.1\nExperimental Framework\nWe ﬁrst describe the experimental framework used\nthroughout this paper, the Test for Inductive Bias\nvia Language Model Transfer (TILT) introduced by\nPapadimitriou and Jurafsky (2020). TILT consists\nof pretraining and transfer steps:\n1. Pretrain an encoder with a pretraining task in\nthe source language (L1). We explore pretrain-\ning with causal language modeling in §4 and\nmasked language modeling in §5.\n2. Transfer the encoder to the target language\n(L2) in a downstream task. As we are inter-\nested in structural prior knowledge learned\nin the encoder, we discard the learned L1\nword embeddings and initialize the embed-\nding layer with the L2 vocabulary. We then\ntrain the model with the encoder parameters\nfrozen and evaluate the task performance.\nTILT reveals how transferrable the computation\ninduced to solve the L1 pretraining task is to pro-\ncessing L2. In this study, we are interested in the\ntransferability of certain types of structures to nat-\nural language, and thus we primarily use hand-\ndesigned artiﬁcial languages with the structural\nproperties as L1 and natural language as L2.\n3.2\nDesigning Artiﬁcial Languages\nArtiﬁcial languages are designed to mimic a certain\nproperty of natural language. After providing a for-\nmal deﬁnition of artiﬁcial language, we introduce\nseveral languages used in this paper.\n3.2.1\nFormulation of Artiﬁcial Language\nA artiﬁcial language refers to a set of a vocabu-\nlary and algorithms to generate sequential data for\npretraining. Each language has a sentence-length\ndistribution plen(l), token vocabulary {w|w ∈V},\nand sentence-sampling function f(l) : l 7→Vl.\nThe training data is generated sentence by sen-\ntence as follows: we ﬁrst sample a sentence length\n(l ∼plen(l)) and then sample a sequence of tokens\nof that length ([w1, ..., wl] ∼f(l)).\nIn this study, the token vocabulary V simply con-\nsists of integers (or integers with a special symbol)\nand is not intended to correspond to a vocabulary\nof any natural language. Also the sentence-length\ndistribution plen(l) is ﬁtted with a baseline dataset\nin each experiment. The focus is how to design the\nsentence-sampling function f(l). This determines\nwhat kind of characteristics we want to encode in\nthe artiﬁcial dataset.\n3.2.2\nModeling Word Distribution\nWords in natural language are distributed in non-\ntrivial fashions. We will study whether prior knowl-\nedge of token distribution facilitates learning from\nnatural language. We ﬁrst present the simplest arti-\nﬁcial language that serves as a baseline.\nUniform language samples each token in a sen-\ntence independently and uniformly. Speciﬁcally,\nthe probability of a token w being sampled is\np(w) = 1\n|V|.\n(1)\nHowever, this deviates from the token distri-\nbution of natural language. Natural language is\nempirically known to follow the Zipf’s law (Zipf,\n1949), i.e., the relation between the frequency of\na word and its rank is given by frequency(w) ∝\nrank(w)−α. The coefﬁcient α is typically around\n1, although the coefﬁcient shows some variation\naccording to the corpus domain (Zanette and Mon-\ntemurro, 2005).\nZipf language captures this property and samples\neach token w from the following probability distri-\nbution assuming α = 1:\np(w) ∝\n1\nrank(w).\n(2)\nThe two languages introduced so far generate to-\nkens in a sentence independently. However, words\nwithin a sentence of natural language are known to\nhave statistical dependencies, i.e., speciﬁc cooccur-\nrence patterns (Church and Hanks, 1989). Consider\nthe sentence “The cat and dog are ﬁghting over\nfood.” The words the and cat would cooccur much\nmore often than by chance because cat (noun) is\ndependent on the (determinant); so would dog and\ncat because they are topically related. The words in\na sentence are usually coherent according to some\nsyntactic and semantic dependencies.\nLog-linear language is designed to capture this\nproperty. Inspired by the log-linear model in Arora\net al. (2016), tokens in a sentence s are drawn from\nthe following probability distribution:\np(w|s) ∝exp(⃗cs · ⃗vw),\n(3)\nwhere ⃗cs is the discourse vector of the sentence and\n⃗vw is the word vector of the token w. Intuitively, we\ncan imagine that the discourse vector represents the\ntopic of the sentence and determines the unigram\ndistribution over the vocabulary (Blei et al., 2003).\nSampling tokens this way, non-trivial cooccurrence\npatterns within sentences emerge in the language.\nWe speculate that pretraining with the Log-linear\nlanguage will endow the model with an inductive\nbias to aggregate the context in a sentence to predict\nthe identity or property of tokens, which is likely\nto beneﬁt natural language processing.\nIn the experiments, the word vectors ⃗vw are ini-\ntialized with the normal distribution, and the dis-\ncourse vector ⃗cs is also drawn from the normal\ndistribution each time we generate a sentence. We\nset the dimension of the word and discourse vec-\ntor to 10 as we empirically ﬁnd that this makes\nthe entire token distribution close to the Zipﬁan\ndistribution.\n3.2.3\nModeling Latent Dependency Structure\nSentences in natural language are known to have la-\ntent structures, which are often described in the\nform of trees (Chomsky, 1957) or dependency\ngraphs (Mel’ˇcuk, 1988). Now we consider how\nto endow the sampled tokens with such structures.\nIn this study, we adopt a dependency-based la-\ntent structure. Words in sentences of natural lan-\nguage often have dependency relations and the exis-\ntence of a certain word can be predictive of another\nword (e.g., the verb am always cooccurs with I). We\nhypothesize that, pretrained on such data, language\nmodels may acquire inductive bias towards ﬁnding\nrelations between tokens in the input, which is pre-\nsumably important in processing natural language.\nInspired by Papadimitriou and Jurafsky (2020),\nwe design algorithms that generate structured sen-\ntences given a set of tokens sampled with any of\nthe strategies described in §3.2.2. The general idea\nis that half of the tokens (heads) in the vocabulary\nare all paired with another half of tokens (tails). A\npair of head and tail can be represented in right and\nleft brackets with the same integer (e.g., “<123”,\n“123>”). The pairs always appear together in a\nsentence and express simple dependency relations.\nAfter determining the sentence length l ∼f(l),\nwe ﬁrst sample l\n2 (rounded to an integer) pairs of\nhead and tail and then arrange them with one of the\nfollowing structures.\nFlat Dependency structure simply arranges the to-\nkens randomly while keeping the right order of\nthe brackets (e.g., [“<5”, “<84”, “5>”, “<123”,\n“123>”, “84>”]). The dependency arcs are al-\nlowed to be crossed and thus often result in a non-\nprojective dependency structure.\nNesting Dependency language, by contrast, does\nnot allow any dependency arcs to be crossed, and\nthe brackets are nested hierarchically (e.g., [“<5”,\n“<84”, “84>”, “5>”, “<123”, “123>”]). The sen-\ntences are generated from the stack-based algo-\nrithm described in Appendix A.\nThese structures are similar to the Parenthe-\nsis languages used to study the inductive bias of\nlanguage models in Papadimitriou and Jurafsky\n(2020). However, our Dependency languages differ\nfrom them in how to represent the head and tail\ntokens. In the Parenthesis language, the head and\ntail are represented with the same token (e.g., [“5”,\n“84”, “84”, “5”, “123”, “123”]), which we argue\ndeviates from the dependency structure in natural\nlanguage, because in natural language, dependency\nrelations usually hold between different words (e.g.,\nI and am). We will show that this difference is in\nfact crucial and draw a different conclusion from\nPapadimitriou and Jurafsky (2020) on the impor-\ntance of the nested structure (§4.2).\n4\nCausal Language Model Pretraining\nwith Artiﬁcial Language\nIn this section, we complement the study of Pa-\npadimitriou and Jurafsky (2020). While they stud-\nied the inductive bias learned in LSTM encoders\nwith some artiﬁcial languages, here we provide\nadditional studies with the newly introduced Log-\nlinear and Dependency artiﬁcial languages, and the\nTransformer encoder.\n4.1\nExperimental Setups\nTask. We study sentence-level causal (left-to-right)\nlanguage modeling (CLM), where the model needs\nto predict the next word given the previous con-\ntext in the sentence. Note that, Papadimitriou and\nJurafsky (2020) experiment with language model-\ning across sentences, but we adopt sentence-level\nmodeling because we would like to focus on the\nlearning of sentence structures here. As we will see\nin §4.2, we observe the same tendency in regard to\nthe effect of artiﬁcial pretraining where we share\nthe setups. The task performance is measured by\nthe average perplexity scores for each token.\nModel.\nWe study two encoder architectures:\nLSTM (Hochreiter and Schmidhuber, 1997) and\nTransformer (Vaswani et al., 2017). These archi-\ntectures are known to exhibit different abilities in\ncapturing the underlying hierarchical structure of\nsequential data (Tran et al., 2018).\nThe size of word embeddings is set to 300. For\nboth LSTM and Transformer encoders, the number\nof layers is set to 3, and the number of parameters\nis conﬁgured to be the same (6.9M parameters) to\nenable a fair comparison between architectures (for\nfurther details, see Appendix B).\nPretraining Data. We generate artiﬁcial corpora\nwith three unstructured languages, which randomly\narrange the tokens sampled from Uniform, Zipf,\nand Log-linear languages, and four structured lan-\nguages which combine the Zipf sampling strategy\nwith the structures of Flat Parenthesis, Nesting\nParenthesis, Flat Dependency, and Nesting Depen-\ndency.\nWe also experiment with natural language cor-\npora. We create training corpora from Wikipedia\ndumps of English, Japanese, and Spanish. The sen-\ntences are tokenized with the Moses tokenizer1\nfor English and Spanish and MeCab2 for Japanese.\nThe sentence lengths of artiﬁcial data were sam-\npled from the empirical distribution of the English\nWikipedia corpus. The size of the vocabulary |V | is\nset to 32,000 for both artiﬁcial and natural corpora,\nand out-of-vocabulary words in natural language\nare replaced with the OOV token. For each corpus,\nwe sample 12.8 M sentences and train the model\nwith one iteration over the corpus.\nEvaluation Data. We evaluate the pretrained en-\ncoders on the Penn Treebank (PTB) corpus (Mar-\ncus et al., 1993) with preprocessing from Mikolov\net al. (2010). Note that, when we train language\nmodels with the pretrained encoders, the parame-\nters of the encoder are not updated and only the\nEnglish word embeddings are learned from scratch\n(optimization details in Appendix B.2).\n4.2\nResults\nWe provide two baseline models trained on the\nL2 training corpus from scratch and trained with\nfrozen random weights in the encoder to compare\nwith pretrained encoders. For each conﬁguration,\nwe pretrain three encoders with different random\nseeds, and for each encoder ﬁne-tuned three mod-\nels, which results in nine models in total. We sum-\nmarize the average scores and standard deviations\nin Figure 2.\nThe Transformer encoder is more ﬂexible\nthan LSTM. We start by discussing overall trends.\nWe observe that the Transformer encoders give\nlower perplexity scores compared to LSTM regard-\nless of pretraining language. This tendency is in\nline with the observations on the surprisingly good\ntransferability or pretrained Transformer encoders\nto other languages (Conneau et al., 2020a), or even\nother modalities (Lu et al., 2021; Reid et al., 2022).\nWe think that this is because Transformer encoders\nare better at aggregating and preserving the context\ninformation at each time step, as we will see in §6,\npresumably because the Transformer architecture\nhas self-attention and residual connections.\n1https://github.com/moses-smt/\nmosesdecoder\n2http://taku910.github.io/mecab/\n(a) Comparison of token distributions.\n(b) Comparison of dependency structures.\n(c) Comparison of natural languages.\nFigure 2: The perplexity scores (the lower the better) on the sentence-level causal language modeling task with the\nEnglish Penn Treebank dataset. The two baselines (From scratch and Random weights) are not pretrained, and the\nothers are the results of pretrained encoders.\nNatural languages are better than the artiﬁ-\ncial languages. As expected, pretraining with nat-\nural languages (English, Spanish and Japanese) pro-\nvides better encoders for language modeling than\nthe artiﬁcial languages both with LSTM and Trans-\nformer. However, the performance differences be-\ntween natural languages seem to be negligible, in-\ndicating that there is not much difference in the\nway the encoders process these different languages,\nconforming with the observation of cross-lingual\ntransferability of pretrained encoders (Artetxe et al.,\n2020).\nThe Uniform and Zipf languages degrade the\nencoders. Looking at the difference among un-\nstructured languages (Figure 2a), Uniform and Zipf\nlanguages give higher perplexities than the Ran-\ndom weights baseline particularly with LSTM. In\nhindsight, it is natural that encoders would be de-\ngraded even from random weights when trained\nwith sequences where tokens are drawn indepen-\ndently from each other because the encoders are\nnot incentivized to use contextual information and\nwill even learn to discard the input information.\nWe will demonstrate this with a follow-up probing\nexperiment in §6.\nThe Log-linear language provides a useful in-\nductive bias to language modeling. On the con-\ntrary, the Log-linear language gives reasonably\nlower perplexities compared to Random weights\n(Figure 2a). This indicates that knowing the exis-\ntence of statistical dependency within a sentence,\nor learning to predict tokens from the cooccurrence\ninformation, is a useful inductive bias even though\nthe cooccurrence statistics is not necessarily in line\nwith L2.\nWe do not observe the importance of the\nnested structure in the Parenthesis languages.\nPapadimitriou and Jurafsky (2020) showed that\nLSTM encoders trained on the Flat Parenthesis and\nNesting Parenthesis structures do not provide a sig-\nniﬁcant difference in perplexity, and concluded that\nsimple non-hierarchical head-dependent-type rela-\ntions are important in LSTM language processing.\nA similar observation can be made in Figure 2b:\nalthough the Nesting Parenthesis exhibits the lower\naverage score, there is no signiﬁcant difference\nbetween Flat Parenthesis and Nesting Parenthesis\n(232.9±30.0 vs. 203.8±7.7, p > 0.01 in Welch’s\nt-test) with the unstable results of Flat Parenthesis.\nAlso, the trend of the average scores is reversed in\nTransformer: the Nesting Parenthesis exhibits the\nhigher average score (212.4 ± 8.8) than Flat Paren-\nthesis (191.9 ± 11.8), which makes it difﬁcult to\ndraw a consistent conclusion from here.\nHowever, the Dependency languages suggest\nthat the nested structure is actually important\nin language modeling. While the Parenthesis lan-\nguage represents dependency relations with two\nidentical tokens (e.g., “4543” and “4543”), our\nDependency language represents relations with two\ndifferent tokens (e.g., “<4543” and “4543>”).\nWe expect that expressing dependency relations\nwith two different tokens is closer to natural lan-\nguage and thus provides more viable insights into\nnatural language. When we compare the scores of\nthe Dependency languages, Nesting Dependency\nprovides the lower and more stable perplexity than\nFlat Dependency with LSTM (175.7 ± 4.3 vs.\n187.2±10.7) and the signiﬁcantly lower score with\nTransformer (160.6±1.6 vs. 175.7±4.3, p > 0.01\nin Welch’s t-test). Overall, Nesting Dependency\nperforms best among other artiﬁcial languages, in-\ndicating our Dependency language is closer to nat-\nural language and the nested structure is useful for\nlanguage modeling.\n5\nMasked Language Model Pretraining\nwith Artiﬁcial Language\nWe proceed to investigate transfer learning from\nartiﬁcial languages in one of the most successful\npretraining paradigms, masked language modeling\n(MLM) (Devlin et al., 2019) to see if we can ob-\nserve similar trends to what we see in the CLM\nexperiment (§4).\n5.1\nExperimental Setups\nPretraining. To allow for fast experimentation, we\ntrain small Transformer encoders. The size of word\nembeddings is set to 300 and the encoders have\nthree layers (further details in Appendix C). The\npretraining datasets are the same as in §4.1.\nDownstream Task. We evaluate the pretrained en-\ncoders with dependency parsing to see if the struc-\ntural knowledge learned with artiﬁcial language is\nbeneﬁcial to predict the structure of natural lan-\nguage. We use the English EWT dataset from\nUniversal Dependencies (UD) v2.8 (Nivre et al.,\n2020)3.\nModel. We adopt the biafﬁne graph-based parser\n(Dozat and Manning, 2017) with the Transformer\nencoder. The input word representations are the\nconcatenation of word embeddings and charac-\nter features computed by a character-level bi-\ndirectional LSTM encoder (Ling et al., 2015). For\n3https://universaldependencies.org/\nFigure 3: The downstream performance on two syn-\ntactic tasks with the English EWT dataset. The two\nbaselines (From scratch and Random weights) are not\npretrained, and the others are the results of encoders\npretrained with masked language modeling.\nthe details on ﬁne-tuning these models, please refer\nto Appendix C.\n5.2\nResults\nWe provide two baseline models trained from\nscratch and trained with random encoder weights.\nFor each pretraining language, we again train three\nencoders and ﬁne-tune three models for each, and\ntake the mean and standard deviation of the nine\nmodels. Figure 3 shows the results.\nThe unstructured languages do not provide\nuseful transferable knowledge for dependency\nparsing. The Uniform, Zipf, and Log-linear en-\ncoders perform comparably to or worse than the\nRandom weights baseline. This is in contrast with\nthe causal language modeling task, where the Log-\nlinear language at least outperforms the Random\nweights baseline (§4.2).\nOn the other hand, learning from structured\nlanguages seems to be important in dependency\nparsing. The Dependency encoders outperform the\nRandom weights baseline, and also we can observe\nthat learning from the nesting structure is more\neffective than the ﬂat structure, and Dependency\nlanguages outperform Parenthesis languages, as\nobserved in the CLM in §4.\n6\nHow much contextual information do\nthe pretrained encoders capture?\nIn the previous sections, we have seen that the en-\ncoders pretrained with different artiﬁcial languages\nexhibit various degrees of transferability to natural\nlanguage. In this section, we try to explain why\npretraining with some artiﬁcial languages is bet-\nter or worse for the transfer to natural language\nfrom the perspective of the amount of contextual\ninformation in the encoder outputs.\nThe intuition is, for example, if a pretrained en-\ncoder has learned to discard the input information,\nwe cannot expect the encoder to perform well when\ntransferred to any tasks. Also, existing studies show\nthat neural language models assign more impor-\ntance to local context when they make predictions\n(Khandelwal et al., 2018; Lai et al., 2020). Can\nwe observe that encoders pretrained with artiﬁcial\nlanguages exhibit similar patterns to natural lan-\nguages regarding how they encode the contextual\ninformation?\n6.1\nExperimental Setups\nWe investigate how much contextual information\ncan be extracted from the outputs of the pretrained\nencoders by setting up a simple probing task. In\nthis task, the encoder is asked to recover the identity\nof the contextual words given the contextualized\nvector of a target word.\nSpeciﬁcally, we ﬁrst randomly generate 100K\nsequences of integers with the length of 15 ∼25\n(close to most frequent sequence lengths in the\npretrained corpus) with the vocabulary size 100 and\nsplit them into training (90K sequences), validation\n(5K) and test (5K) sets.\nThen we simultaneously train several linear clas-\nsiﬁers, each of which predicts the ID of the context\nword at a ﬁxed relative position to the target word\nin the sequence, on top of a frozen pretrained en-\ncoder. For the encoders pretrained with CLM in §4,\nthe target word is the last word in sequences and\nthe classiﬁers predict the words at the positions of\n[-9, -4, -3, -2, -1, 0]; for the encoders pretrained\nwith MLM in §5, the target word is the middle\nword and the classiﬁers predict the words at [-6, -3,\n-2, -1, 0, 1, 2, 3, 6].\nAfter training, we measure the accuracy of pre-\ndicting the words at each position on the test set\nand interpret this as how much information on each\ncontextual word the encoder preserves.\n6.2\nResults\nFigure 4 summarizes the results of the encoders\ntrained in §4 and §5.\nThe amount of the encoded contextual infor-\nmation can explain the transfer performance in\nsome obvious cases. In the experiment of CLM\n(Figure 2a), we observed that the Uniform and Zipf\nencoders tend to perform worse even than Ran-\ndom weights. Figure 4a and 4d demonstrate that\ntheir poor performance is because the encoders\nare trained to discard the input information. The\nUniform and Zipf encoders tend to preserve less\ncontextual information even than Random weights\nbecause capturing the contextual information does\nnot lead to solving the pretraining task in these\nlanguages.\nOn the other hand, if words are predictable from\nthe context, encoders are encouraged to learn to\npreserve the contextual information.\nThe Log-\nlinear encoders trained with CLM encode a de-\ncent amount of the contextual information (Fig-\nure 4a and 4d) and also performed best among the\nunstructured artiﬁcial languages in CLM (Figure\n2a). Moreover, encoders trained with natural lan-\nguages (Figure 4c, 4f and 4i) capture not only the\nlocal context well (at distance 0 ∼2) but also a\nmodest amount of the farther context (at distance\n3 ∼), which is consistent with the existing obser-\nvation that LSTM encoders trained with natural\nlanguage are better at memorizing the inputs than\nones trained with randomly sampled data (Liu et al.,\n2018). In these cases, the downstream performance\nand the amount of the encoded contextual informa-\ntion seem to be correlated.\nHowever, this trend is not as clear when compar-\ning the structured artiﬁcial languages. For exam-\nple, the Nesting Dependency encoders perform the\nbest for the downstream tasks among the structured\nartiﬁcial languages but do not necessarily in the\nprobing task (Figure 4b and 4e).\nThe nesting structure seems to facilitate en-\ncoders to remember the local context with\nMLM. The difference between the Nesting and\nFlat languages is striking in Figure 4f. The Nesting\nencoders are consistently better at capturing the\nlocal contextual information (at positions −2 ∼2)\nthan their ﬂat counterparts, which may explain the\nbetter performance of the Nesting encoders in de-\npendency parsing (Figure 3), given that the local\ncontextual information is particularly important to\npredict the syntactic characteristics of words (Levy\nand Goldberg, 2014; Ri and Tsuruoka, 2020).\n7\nDiscussion and Future Work\nIn this paper, we studied what kind of structural\nproperties in pretraining data is useful to train en-\ncoders for natural language tasks. We have found\n(a) LSTM-CLM.\n(b) LSTM-CLM.\n(c) LSTM-CLM.\n(d) Transformers-CLM.\n(e) Transformers-CLM.\n(f) Transformers-CLM.\n(g) Transformers-MLM.\n(h) Transformers-MLM.\n(i) Transformers-MLM.\nFigure 4: The accuracy of the task of recovering the contextual words from the encoder output of target words.\nthat to achieve decent results, L1 needs at least sta-\ntistical dependency in a sentence (§4), and having\nthe head-to-tail dependency with the nesting struc-\nture is further beneﬁcial (§4 and §5). The probing\nexperiment in §6 suggests that the encoders trained\nwith languages with the above characteristics are\ngood at capturing the positions and identities of the\ncontext words.\nFrom these observations, we suggest a tentative\nanswer to the initial research question: what knowl-\nedge in pretrained encoders are transferred across\ndifferent languages? That is position-aware context\ndependence of language, in other words, “tokens\nin a sequence can be characterized by its neigh-\nbor tokens at speciﬁc positions”.\nWe think that it can explain the success of trans-\nferring the encoder across languages to some extent.\nTo solve natural language tasks, it is often useful\nto characterize words in a sentence by the words\naround them. For example, to understand the se-\nmantics of a sentence, it would be useful to look\nfor the subject by looking for a noun that precedes\nthe word is; to parse a sentence, a word can be iden-\ntiﬁed as a noun because it follows the article the.\nIf the encoder computes the output representation\nof a word in a sentence by aggregating the infor-\nmation from its surrounding words, that should be\na useful inductive bias to solve most NLP tasks\nin any language. Also, it is easy to imagine that\nthe knowledge of position-aware context depen-\ndence gives a reasonable prior for solving sequence\nmodeling problems in other domains, which may\nexplain the success of cross-modality transfer of\nlanguage models (Lu et al., 2021; Reid et al., 2022).\nOf course, we do not expect that the knowledge\nof position-aware context dependence explains ev-\nery aspect of the success of cross-lingual transfer.\nAs future work, we need further investigation for\na more ﬁne-grained view of the transferred knowl-\nedge. Important questions include how much the\nmodel size affects the transferability of the encoder\nor if there is any difference in the knowledge trans-\nferred among different downstream tasks.\nAcknowledgement\nWe thank the anonymous reviewers for their in-\nsightful comments and constructive suggestions to\nimprove the paper.\nReferences\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2016. A Latent Variable Model\nApproach to PMI-based Word Embeddings. Trans-\nactions of the Association for Computational Lin-\nguistics, 4:385–399.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the Cross-lingual Transferability of Mono-\nlingual Representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics.\nDavid M. Blei, A. Ng, and Michael I. Jordan. 2003. La-\ntent Dirichlet Allocation. Journal of Machine Learn-\ning Research, 3:993–1022.\nEthan A. Chi, John Hewitt, and Christopher D. Man-\nning. 2020.\nFinding Universal Grammatical Rela-\ntions in Multilingual BERT. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics.\nCheng-Han Chiang and Hung yi Lee. 2022.\nOn the\nTransferability of Pre-trained Language Models: A\nStudy from Artiﬁcial Datasets. In Proceedings of\nthe Thirty-Sixth AAAI Conference on Artiﬁcial Intel-\nligence.\nNoam Chomsky. 1957.\nSyntactic Structures.\nDe\nGruyter Mouton.\nKenneth Ward Church and Patrick Hanks. 1989. Word\nAssociation Norms, Mutual Information, and Lexi-\ncography. In 27th Annual Meeting of the Associa-\ntion for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties.\nIn\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b.\nEmerg-\ning Cross-lingual Structure in Pretrained Language\nModels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1.\nSumanth Doddapaneni, Gowtham Ramesh, Anoop\nKunchukuttan, Pratyush Kumar, and Mitesh M.\nKhapra. 2021. A Primer on Pretrained Multilingual\nLanguage Models. ArXiv, abs/2107.00676.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep Biafﬁne Attention for Neural Dependency\nParsing. In International Conference on Learning\nRepresentations.\nPhilipp Dufter and Hinrich Schütze. 2020. Identifying\nElements Essential for BERT’s Multilinguality. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nShort-Term Memory. Neural Computation, 9:1735–\n1780.\nDieuwke Hupkes and Willem Zuidema. 2018. Visual-\nisation and ’Diagnostic Classiﬁers’ Reveal how Re-\ncurrent and Recursive Neural Networks Process Hi-\nerarchical Structure (Extended Abstract).\nIn Pro-\nceedings of the Twenty-Seventh International Joint\nConference on Artiﬁcial Intelligence, IJCAI-18.\nStephen Mayhew Karthikeyan K, Zihan Wang and Dan\nRoth. 2020. Cross-Lingual Ability of Multilingual\nBERT: An Empirical Study. In International Con-\nference on Learning Representations.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Juraf-\nsky. 2018. Sharp Nearby, Fuzzy Far Away: How\nNeural Language Models Use Context. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics.\nYi-An Lai, Garima Lalwani, and Yi Zhang. 2020.\nContext Analysis for Pre-trained Masked Language\nModels. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020.\nOmer Levy and Yoav Goldberg. 2014. Dependency-\nBased Word Embeddings.\nIn Proceedings of the\n52nd Annual Meeting of the Association for Compu-\ntational Linguistics.\nWang Ling, Chris Dyer, Alan W Black, Isabel Tran-\ncoso, Ramón Fermandez, Silvio Amir, Luís Marujo,\nand Tiago Luís. 2015. Finding Function in Form:\nCompositional Character Models for Open Vocab-\nulary Word Representation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing.\nNelson F. Liu, Omer Levy, Roy Schwartz, Chenhao\nTan, and Noah A. Smith. 2018.\nLSTMs Exploit\nLinguistic Attributes of Data.\nIn Proceedings of\nThe Third Workshop on Representation Learning for\nNLP.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020.\nMultilingual Denoising\nPre-training for Neural Machine Translation. Trans-\nactions of the Association for Computational Lin-\nguistics, 8:726–742.\nIlya Loshchilov and Frank Hutter. 2019.\nDecoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nKevin Lu, Aditya Grover, P. Abbeel, and Igor Mor-\ndatch. 2021. Pretrained Transformers as Universal\nComputation Engines. ArXiv, abs/2103.05247.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a Large Annotated\nCorpus of English: The Penn Treebank.\nComput.\nLinguistics, 19:313–330.\nIgor Mel’ˇcuk. 1988. Dependency Syntax: Theory and\nPractice. State University Press of New York.\nTomas Mikolov,\nMartin Karaﬁát,\nLukás Burget,\nJan Honza Cernocký, and Sanjeev Khudanpur. 2010.\nRecurrent neural network based language model. In\nProceedings of Annual Conference of the Interna-\ntional Speech Communication Association.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Hajiˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn Evergrowing Multilingual Treebank Collection.\nIn Proceedings of the 12th Language Resources and\nEvaluation Conference.\nIsabel Papadimitriou, Ethan A. Chi, Richard Futrell,\nand Kyle Mahowald. 2021.\nDeep Subjecthood:\nHigher-Order Grammatical Features in Multilingual\nBERT. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume.\nIsabel Papadimitriou and Dan Jurafsky. 2020. Learn-\ning Music Helps You Read: Using Transfer to Study\nLinguistic Structure in Language Models. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow Multilingual is Multilingual BERT?\nIn Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020.\nExploring\nthe Limits of Transfer Learning with a Uniﬁed Text-\nto-Text Transformer. Journal of Machine Learning\nResearch, 21(140):1–67.\nShauli Ravfogel, Yoav Goldberg, and Tal Linzen. 2019.\nStudying the Inductive Biases of RNNs with Syn-\nthetic Variations of Natural Languages. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1.\nMachel Reid, Yutaro Yamada, and Shixiang Shane Gu.\n2022. Can Wikipedia Help Ofﬂine Reinforcement\nLearning? ArXiv, abs/2201.12122.\nRyokan Ri and Yoshimasa Tsuruoka. 2020. Revisiting\nthe Context Window for Cross-lingual Word Embed-\ndings. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021a.\nMasked language modeling and the distributional\nhypothesis: Order word matters pre-training for lit-\ntle. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing.\nKoustuv Sinha, Prasanna Parthasarathi, Joelle Pineau,\nand Adina Williams. 2021b. UnNatural Language\nInference. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing.\nKe Tran, Arianna Bisazza, and Christof Monz. 2018.\nThe Importance of Being Recurrent for Modeling\nHierarchical Structure. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in Neural Information Pro-\ncessing Systems, volume 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\nJennifer C. White and Ryan Cotterell. 2021.\nExam-\nining the Inductive Bias of Neural Language Mod-\nels with Artiﬁcial Languages. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing.\nShijie Wu and Mark Dredze. 2019. Beto, Bentz, Be-\ncas: The Surprising Cross-Lingual Effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mT5: A Massively\nMultilingual Pre-trained Text-to-Text Transformer.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. In Advances in Neural In-\nformation Processing Systems, volume 32.\nDamián H. Zanette and Marcelo A. Montemurro. 2005.\nDynamics of Text Generation with Realistic Zipf’s\nDistribution.\nJournal of Quantitative Linguistics,\n12:29 – 40.\nGeorge Kingsley Zipf. 1949. Human behavior and the\nprinciple of least effort. Addison-Wesley Press.\nAppendix for “Pretraining with Artiﬁcial\nLanguage: Studying Transferable\nKnowledge in Language Models”\nA\nGenerating the Nesting Structure\nIn the Nesting languages introduced in §3.2.3, to-\nkens are ordered in a way that any dependency arcs\nin a sequence are not crossed. This is realized by\nthe stack-based algorithm in Algorithm 1. We set\nthe probability of closing a dependency pair to 0.4\nfollowing Papadimitriou and Jurafsky (2020).\nAlgorithm 1 Generating a sentence from the Nest-\ning Dependency language.\nInput: input_pairs: Stack[(w, w)]]\nOutput: sentence: List[w]\n1: closing_stack = []\n2: while not input_pairs.is_empty() do\n3:\nUniform sampling p ∼[0, 1]\n4:\nif closing_stack.is_empty() or p < 0.4 then\n5:\nhead, tail = input_pairs.pop()\n6:\nsentence.append(head)\n7:\nclosing_stack.push(tail)\n8:\nelse\n9:\ntail = closing_stack.pop()\n10:\nsentence.append(tail)\n11:\nend if\n12: end while\n13: while not closing_stack.is_empty() do\n14:\ntail = closing_stack.pop()\n15:\nsentence.append(tail)\n16: end while\n17: return sentence\nB\nDetails of Causal Language Modeling\nTask\nB.1\nModel conﬁguration\nFor the experiment with causal language modeling\n(§4), we set the number of layers of the LSTM and\nTransformer encoders to 3 and conﬁgure them so\nthat they have the same number of parameters (2.1\nM parameters without the embedding and output\nprojection layers). The details of conﬁguration are\nshown in Table 1 and Table 2.\nThe weights of the output projection layer are\ntied with the word embedding layer (Press and\nWolf, 2017). Note that, to enable this, the LSTM\nencoder has an additional linear layer to project the\nhidden vector (294 dim) to the input size (300 dim),\nwhich the Transformer encoder does not have.\n# of layers\n3\ninput size\n300\nhidden size\n294\nTable 1: Conﬁguration of the LSTM encoder.\n# of layers\n3\nsize\n300\nfeedforward size\n600\n# of attention heads\n4\nTable 2: Conﬁguration of the Transformer encoder.\nB.2\nOptimization\nWe optimize the pretrained models for 10k steps\nwith 12.8 M sentences and the batch size of 128\nusing AdamW (Loshchilov and Hutter, 2019). We\nuse the the Noam Learning rate scheduler described\nin Vaswani et al. (2017) with the warmup steps\nof 4000, and the other hyper-parameter details\nare shown in Table 3. We use the same hyper-\nparameters for ﬁne-tuning with the L2 language.\nName\nValue\nPretraining minimum sentence length\n6\nPretraining maximum sentence length\n60\nDropout\n0.1\nWeight decay\n0.01\nAdam β1\n0.9\nAdam β2\n0.98\nAdam ϵ\n1e-9\nGradient clipping\n0.25\nTable 3: Hyper-parameters for pretraining.\nC\nDetails of Masked Language Modeling\nTask\nC.1\nModel conﬁguration\nFor the experiment with masked language model-\ning (§5), we set the number of layers of the Trans-\nformer encoders to 3. The details of conﬁguration\nare shown in Table 4 (2.1 M parameters without\nthe embedding and output projection layers).\nThe hyper-parameters for the masked language\nmodeling task is shown in Table 5. For optimiza-\ntion, we used the same hyper-parameters as in Ap-\npendix B.2.\n# of layers\n3\nsize\n300\nfeedforward size\n600\n# of attention heads\n4\nTable 4: Model conﬁguration of the Transformer en-\ncoder.\nMask probability for words\n15%\nRandom-word probability for words\n10%\nUnmasked probability for words\n10%\nTable 5: The hyper-parameters for masked language\nmodeling.\nD\nComputing Infrastructure\nWe ran the experiments on a server with a Intel(R)\nXeon(R) CPU E5-2698 v4 @ 2.20GHz CPU and\n10 NVIDIA TITAN Xp GPUs. Each pretraining\nand ﬁnetuning were run with a single GPU.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-03-19",
  "updated": "2022-03-22"
}