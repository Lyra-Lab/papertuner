{
  "id": "http://arxiv.org/abs/2203.10627v2",
  "title": "Enriching Unsupervised User Embedding via Medical Concepts",
  "authors": [
    "Xiaolei Huang",
    "Franck Dernoncourt",
    "Mark Dredze"
  ],
  "abstract": "Clinical notes in Electronic Health Records (EHR) present rich documented\ninformation of patients to inference phenotype for disease diagnosis and study\npatient characteristics for cohort selection. Unsupervised user embedding aims\nto encode patients into fixed-length vectors without human supervisions.\nMedical concepts extracted from the clinical notes contain rich connections\nbetween patients and their clinical categories. However, existing unsupervised\napproaches of user embeddings from clinical notes do not explicitly incorporate\nmedical concepts. In this study, we propose a concept-aware unsupervised user\nembedding that jointly leverages text documents and medical concepts from two\nclinical corpora, MIMIC-III and Diabetes. We evaluate user embeddings on both\nextrinsic and intrinsic tasks, including phenotype classification, in-hospital\nmortality prediction, patient retrieval, and patient relatedness. Experiments\non the two clinical corpora show our approach exceeds unsupervised baselines,\nand incorporating medical concepts can significantly improve the baseline\nperformance.",
  "text": "Proceedings of Machine Learning Research LEAVE UNSET:1–16, 2022\nConference on Health, Inference, and Learning (CHIL) 2022\nEnriching Unsupervised User Embedding via Medical Concepts\nXiaolei Huang\nxiaolei.huang@memphis.edu\nUniversity of Memphis, United States\nFranck Dernoncourt\ndernonco@adobe.com\nAdobe Research, United States\nMark Dredze\nmdredze@cs.jhu.edu\nJohns Hopkins University, United States\nAbstract\nClinical notes in Electronic Health Records\n(EHR) present rich documented information of\npatients to inference phenotype for disease di-\nagnosis and study patient characteristics for co-\nhort selection. Unsupervised user embedding\naims to encode patients into ﬁxed-length vectors\nwithout human supervisions. Medical concepts\nextracted from the clinical notes contain rich\nconnections between patients and their clinical\ncategories. However, existing unsupervised ap-\nproaches of user embeddings from clinical notes\ndo not explicitly incorporate medical concepts.\nIn this study, we propose a concept-aware unsu-\npervised user embedding that jointly leverages\ntext documents and medical concepts from two\nclinical corpora, MIMIC-III and Diabetes. We\nevaluate user embeddings on both extrinsic and\nintrinsic tasks, including phenotype classiﬁca-\ntion, in-hospital mortality prediction, patient\nretrieval, and patient relatedness. Experiments\non the two clinical corpora show our approach\nexceeds unsupervised baselines, and incorporat-\ning medical concepts can signiﬁcantly improve\nthe baseline performance.\nData and Code Availability\nIn this study, we\nexperiment with two clinical datasets that are pub-\nlicly available, Medical Information Mart for Intensive\nCare (MIMIC)-III (Johnson et al., 2016) and Clini-\ncal Trial Cohort Selection (Diabetes) (Stubbs et al.,\n2019).1 The data description and processing details is\nin Section 2. Our experimental codes, supplementary\nanalyses, and data access instructions are at https://\ngithub.com/xiaoleihuang/UserEmb_Explainable.\n1. We denote the cohort selection dataset as Diabetes because\npatients share the same syndrome, diabetes. Diabetes data\nlink:\nhttps://portal.dbmi.hms.harvard.edu/projects/\nn2c2-nlp/, MIMIC-III data link: https://physionet.org/\ncontent/mimiciii/1.4/.\n1. Introduction\nUser embedding aims to learn ﬁx-length vectors for\nusers and maps all user information into a uniﬁed vec-\ntor space (Pan and Ding, 2019; Li and Zhao, 2020; Si\nand Roberts, 2020). Patient representations inferred\nby user embeddings have shown broad applications\nin medical diagnosis (Choi et al., 2016, 2017), hospi-\ntal stays (Wang et al., 2019), and patient readmis-\nsion (Darabi et al., 2020). The supervised learning ap-\nproaches require a large amount of annotated datasets,\nyet obtaining human annotations for clinical notes can\nbe labor-intensive, time-consuming, and expensive.\nAdditionally, International Classiﬁcation of Diseases\n(ICD) codes or other human clinical annotations have\nannotating issues including noisy (Harutyunyan et al.,\n2019), error-prone (Birman-Deych et al., 2005), and\nlow inter-rater agreement (Stubbs et al., 2019). Qual-\nity of data annotations can impact the robustness of\nsupervised embedding models.\nUnsupervised user embedding learns patient rep-\nresentations from clinical notes without human su-\npervision. The embedding aims to capture semantic\nrelations between patients and their clinical records.\nTo represent clinical notes, models derive features from\ntopic model (Blei et al., 2003), paragraph vectors (Le\nand Mikolov, 2014), word2vec embeddings (Mikolov\net al., 2013). Finding hidden structures to distinguish\npatients is a challenge for unsupervised user embed-\ndings to compress patient features from clinical notes.\nUnsupervised approaches (Sushil et al., 2018; Lei et al.,\n2018) utilize autoencoders by feature reconstruction,\nwhile ignoring other structures like medical concepts.\nMedical concepts, which consist of one or more to-\nkens, are basic description units for medical informa-\ntion, such as disease symptom and clinical drug. The\nconcepts contain rich information of medical ontology\nthat owns strong connections with phenotype infer-\n© 2022 X. Huang, F. Dernoncourt & M. Dredze.\narXiv:2203.10627v2  [cs.CL]  29 Mar 2022\nEnriching Unsupervised User Embedding via Medical Concepts\nence. Medical concept embeddings treat concepts as\nspecial tokens or phrases and encode them as word vec-\ntors. The representations are eﬀective in downstream\ntasks of health diagnosis (Choi et al., 2016, 2020) and\nreadmission forecast (Choi et al., 2017; Zhang et al.,\n2020). However, unsupervised user embeddings have\nnot explicitly consider medical concepts, which can\nlose semantic information of clinical conditions.\nIn this study, we propose a concept-aware user em-\nbedding (CAUE) model that jointly handles clinical\nnotes and medical concepts under the multitask learn-\ning framework.\nOur unsupervised method applies\ncontrastive learning (Logeswaran and Lee, 2018; Chen\net al., 2020) and negative sampling to leverages two\nlevels of information sources, patient-document and\npatient-concept. Patient-document task encodes se-\nquential information of patients to guide models recog-\nnizing patients of clinical notes, and patient-concept\ntask treats medical concepts as a medium point to con-\nstrain patients. To model unusually lengthy clinical\ndocuments, we propose a data augmentation approach,\nrandom split, to cut each note into random sizes of\nsnippets. We evaluate on multiple clinical datasets\nwith two extrinsic tasks, phenotype and mortality pre-\ndictions, and two intrinsic tasks, patient relatedness\nand retrieval. Research suggests that intrinsic eval-\nuations of embedding models can reduce parameter\nbiases of extrinsic evaluations for controlling fewer\nhyperparameters (Schnabel et al., 2015). The four\nevaluations show that our proposed approach achieves\nthe best performance overall and highlight the eﬀec-\ntiveness of medical concepts. Our ablation analysis\nshows that integrating medical concepts can generally\nimprove eﬀectiveness of baseline models.\n2. Data\nIn this section, we introduce the two clinical data\n(Diabetes and MIMIC-III) and report data processing\nsteps. We obtain the Diabetes data (Stubbs et al.,\n2019) from Track 1 of the 2018 National NLP Clini-\ncal Challenges (n2c2) shared task, which contains a\ncollection of longitudinal patient records, with 2 to 5\ntext documents per patient. The clinical documents\nsummarize diagnosis results and describe clinical con-\nditions indicating if patients have coronary artery\ndisease. Each patient in the Diabetes data has 13\nselection criteria annotations, which follow Clinical-\nTrials.gov and relate to diabetes and heart disease.\nMIMIC-III is a relational database that collects a large\nset of intensive care unit (ICU) patients from the Beth\nIsrael Deaconess Medical Center between 2001 and\n2012.2 The data includes various patient information\nsuch as demographics, laboratory results, mortality,\nand clinical notes. Clinical notes are unstructured\nnarrative documents describing patients’ ICU stays,\nincluding radiology and discharge summaries. Like Di-\nabetes, each MIMIC-III document has a list of medical\nannotations, International Classiﬁcation of Disease,\n9th Edition (ICD-9). The ICD-9 annotations indicate\nsymptoms and disease types of patients during their\nICU stays. Both data corpora have been de-identiﬁed\nfollowing Health Insurance Portability and Account-\nability Act (HIPAA) standards, and any dates were\ntime-shifted by a random amount.\nWe use MetaMap (Aronson and Lang, 2010) to\nextract medical concepts by the Uniﬁed Medical Lan-\nguage System Metathesaurus (UMLS) (Bodenreider,\n2004).3\nMetaMap extracts medical concepts with\ntheir Concepts Unique Identiﬁer (CUI) linked to the\nUMLS, conﬁdence scores, and corresponding medical\ntypes. To improve accuracy and reduce the ambiguity\nof extracted concepts, we enable the word sense disam-\nbiguation (WSD) module. The WSD module checks if\nconcepts are semantically consistent with surrounding\ntexts and automatically removes ambiguous entities.\nEach concept may have one or more tokens and asso-\nciate with at least one concept type. We lowercase all\nextracted medical concepts and empirically removed\nnon-symptom-related entities, including digits-only\nsequences, language, and temporal concepts.4\nIn this study, we preprocess medical annotations\nand clinical notes for both corpora. For MIMIC-III, we\nfollow previous work (Mullenbach et al., 2018) to keep\ndischarge summary, which merges enough details of a\nsingle admission for each patient. ICD-9 labels for the\nMIMIC-III have over 15K diﬀerent codes, which are\nhighly dimensional and sparse. To reduce sparsity and\nnoise, we follow previous work (Harutyunyan et al.,\n2019) to merge ICD-9 codes according to the Health\nCost and Utilization (HCUP) Clinical Classiﬁcation\nSofware (CCS).5 The predeﬁned HCUP-CCS schema\n2. We access the data at https://physionet.org/content/\nmimiciii/1.4/.\nMIMIC has diﬀerent versions, and we\nexperiment with the stable version, MIMIC-III 1.4. Its\nrelease year is 2016.\n3. While we have alternative toolkits, such as cTAKES (Savova\net al., 2010) and CLAMP toolkit (Soysal et al., 2017),\nhowever, they did not adapt to the new authentication\nsystem of UMLS when this study started.\n4. MetaMap concept types: https://metamap.nlm.nih.gov/\nDocs/SemanticTypes_2018AB.txt\n5. https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.\njsp\n2\nEnriching Unsupervised User Embedding via Medical Concepts\nDataset\nDocument Statistics\nUser Stats\nConcept Stats\nDoc\nVocab\nToken-stats\nUser\nAge\nF\nU-label\nConcept\nType\nDiabetes\n1265\n34592\n2426, 483, 42\n288\n63.13\n0.45\n10\n68938\n89\nMIMIC-III\n54888\n390237\n7522, 1263, 50\n48807\n62.47\n0.44\n276\n10761211\n94\nTable 1: Data summary of Diabetes and MIMIC-III by documents, patients, and concepts. The “Token-stats”\nincludes maximum, median and minimum lengths of processed clinical notes. We present values\nof patient count (User), averaged age (Age), gender ratio of female (F) and male (M), number of\nunique labels (U-label), number of concepts (Concept), and number of unique concept types (Type).\nclusters ICD-9 procedure, billing, and diagnostic codes\ninto mutually exclusive, largely homogeneous disease\ncategories. We follow previous work (Harutyunyan\net al., 2019) to drop any patients who are younger\nthan 18 years old. Two domain experts annotated\neach patient from the Diabetes data by 13 diﬀerent\nclinical criteria. We empirically drop three of them be-\ncause their inter-rater agreements are fewer than 0.5.6\nWe apply the same preprocessing steps for clinical\nnotes. Each patient associates with one or more notes\nand their medical annotations. The temporal shift\nis an attribute of the in-hospital records, MIMIC-III.\nTo reduce temporal impacts and noises, we isolate a\npatient’s records per visit as a separate patient. For\neach note, we lowercase all tokens, tokenize sequences\nby NLTK (Bird and Loper, 2004), remove person and\nhospital names, replace numbers and dates with place-\nholders ([NUM] and [DATE]), and remove repeated\npunctuation. We empirically drop any notes with less\nthan 40 tokens.\nWe summarize data statistics in Table 1. MIMIC-\nIII shows more varied statistics of clinical notes than\nthe Diabetes dataset. For example, the range of token\ncounts for MIMIC-III data is broader than Diabetes\n(50 to 7522 vs. 42 to 2426), and MIMIC-III has a\nlarger vocabulary. Both datasets have approximately\nsimilar demographic distributions of age and gender.\nMIMIC-III data has more complex clinical annota-\ntions, while Diabetes data has a maximum of 10 labels\nfor each patient. Comparing to health data from so-\ncial media, such as Twitter, the clinical corpora have\nmore senior user groups and more variations in doc-\numents, patients, and annotation information. For\nexample, while the Twitter suicide users (Amir et al.,\n2017) only has binary labels, MIMIC-III data has 276\nunique phenotypes.\n6. The agreement rate of ENGLISH, MAKES-DECISIONS,\nand KETO-1YR are 0.46, 0.31, -0.1, respectively. Note that\nthe main reason that causes a negative agreement score of\nKETO-1YR is the skewed distributions of annotations.\n2.1. Privacy and Ethical Considerations\nWe do not release any clinical data associate with\npatient identities due to privacy and ethical consid-\nerations.\nInstead, we have released our code and\nprovided detailed instructions to replicate our anal-\nysis and experiments. To protect user privacy, we\nhave followed corresponding data agreements to en-\nsure the proper data usage and experimented with\nde-identiﬁed data. Our experiments do not store any\npatient data and only use available text documents for\nresearch demonstrations. Except, our experiments use\nanonymized patient ID and clinical notes for training\nand evaluating user embeddings.\n3. Concept-Aware User Embedding\n(CAUE)\nIn this section, we propose an unsupervised concept-\naware user embedding (CAUE) that jointly models\nclinical notes and medical concepts under a multi-\ntask learning framework. Methods (Si et al., 2020) to\ntrain unsupervised user embedding from clinical notes\nprimarily focus on bag-of-words features without ex-\nplicitly capturing semantic information of medical con-\ncepts. (Steinberg et al., 2021) utilizes medical codes of\nclinical records as input features to derive patient rep-\nresentations sequentially, while our focus is to model\nsequential language features, clinical notes. (Dligach\nand Miller, 2018) obtains user embeddings by train-\ning a supervised classiﬁer using concept embeddings.\nHowever, our approach jointly models document and\nconcept embeddings in an unsupervised manner. We\npropose two joint tasks: modeling language sequen-\ntial dependency by neural document encoders (task\n1: patient-document) and leveraging connections be-\ntween medical concepts and patients (task 2: patient-\nconcept).\nWe present the model architecture and\nlearning process in Figure 1.\n3\nEnriching Unsupervised User Embedding via Medical Concepts\nDu\ndu, i: w1, …, wn\nPatient Documents\nDoc Vector\nv(du, i)\nDocument Encoder\nD!𝑢\nd!𝑢, j: w1, …, wn\nNegative Samples of Documents\n·\n·\n1\n0\nDoc Vector\nv(d!u, j)\nUser Vector, v(u)\nCu\ncu, k\nC!𝑢\nc!𝑢, m\nConcept Embedding\n·\n·\nLabel\nLabel\nTask 1:\nPatient-Doc.\nTask 2:\nPatient-Concept\nPatient Concepts\nNegative Samples of Concepts\nConcept Vector\nv(cu, k)\nConcept Vector\nv(c!u, m)\nFigure 1: CAUE illustrations. All tasks center with the the patient, u. The arrows and their colors refer to\nthe input directions and sources respectively. The J is a similarity measurement.\n3.1. Task 1: Patient-Document\nThe ﬁrst task is to enforce models to understand con-\ntextual and sequential information of clinical notes\nso that models can summarize patients. Instead of\ntraining individual tokens, this task aims to model\nconnections between users with documents. Given U\nbe the list of patients, Du be a list of clinical notes\nthat describe the patient u, and we have u ∈U, we\ncan formalize our goal by maximizing the conditional\nprobability of P(Du|u). Let du,i as a clinical docu-\nment (index as i) that describes the patient u clinical\ninformation, and each clinical note is independent with\nother notes in the collection Du, our user-document\ntask aims to maximize the following probability:\nP(Du|u) =\n|Du|\nX\ni=1\nlogP(du,i|u)\n(1)\nTo maximize the probability, we convert the task to a\nprediction task. Given a user, a model predicts a prob-\nabilistic distribution of a user vector over the patient’s\nclinical notes so that the clinical note summarizes its\ntarget patient.\nConsidering the large size of the patients and clini-\ncal notes, we approximate our optimization objectives\nby the negative sampling and contrastive learning.\nThe negative sampling is to randomly choose dif-\nferent documents from the existing corpus besides\nthe target clinical document. The contrastive learn-\ning (Logeswaran and Lee, 2018; Chen et al., 2020)\nis to predict if a sampled document or sentence is a\ncontext of the target. Previous study (Logeswaran\nand Lee, 2018) predicts the context sentence from\nseveral sampled or generated counterfactuals, which\nare stylistically similar to the context sentence but\nexpress diﬀerent meanings. In this study, we treat pa-\ntients as our target and clinical documents as context\nso that learning process will align patient representa-\ntions toward their correct context, clinical documents.\nTo generate counterfactuals, we choose two types of\ninformation: document and token. For the document\nlevel, we sample 3 snippets as counterfactuals by ran-\ndomly choosing documents from the other patients.\nFor the token level, we randomly replace tokens in\nthe original document, where the sampling tokens are\nthe vocabulary of each corpus. Instead of predicting\neach clinical document over all patients, the negative\nsampling enables model predictions over a few sam-\nples. This converts our prediction task into predicting\nif our model can identify if a text document describes\nthe target patient or not. Let V as embeddings for\ndocuments, users and concepts, we can derive a docu-\nment vector v(d) for the document du, a user vector\nv(u) for the patient u, and a concept vector v(c) for\nthe concept c. Then we can treat the task as a binary\nclassiﬁcation problem and minimize loss values by the\nbinary cross-entropy:\nL(u, d) = −log(σ(v(u) · v(du)))\n−log(1 −σ(v(u) · v(d˜u)))\n(2)\n4\nEnriching Unsupervised User Embedding via Medical Concepts\nwhere σ is a non-linear function and d˜u refers to\nnoise documents. We use the sigmoid function as\nour non-linear function to normalize values of the dot\nproduction. The binary classiﬁcation task will predict\nif a sampled clinical document describes correctly the\ntarget user, where one means correct match and 0\nmeans incorrect descriptions.\nWe utilize a document encoder to derive document\nvectors v(d) to learn language dependency across se-\nquential tokens. Let a document d has a sequence of\nwords, d = {w1, w2, ..., wn}. Then we can derive doc-\nument vectors by v(d) = fd(v(w1), ..., v(wn)), where\nfd is the document encoder and v(w) is a word vector.\nFinally, our document encoder can learn sequential\ninformation of each clinical note into a uniﬁed vector,\nv(d).\n3.2. Long Text Modeling via Random Split\nThe clinical notes have a much longer document length\n(e.g, 7522 in our case) comparing to Twitter messages.\nSuch a long sequence prevents eﬀectively training\nneural models and ﬁtting GPU memory because of se-\nquence length limits. For example, the BERT (Devlin\net al., 2019) has a limitation of 512 tokens. We solve\nthe problem by splitting each document longer than\na length limit into multiple snippets with a random\nsize, which is between 200 and 512 tokens. This sim-\nulates clinical settings in real-world that physicians\ncan recognize their patients with partial symptom\ndescriptions.\n3.3. Task 2: Patient-Concept\nThe second task is to enrich user representations by\nextracted concepts from clinical notes. On the one\nhand, concepts are identities of users if the concepts\nco-occur with the users. On the other hand, concept\nentities can connect users who share similar medical\nsymptoms and separate users who do not share similar\ninformation. Given that Cu be a list of extracted\nmedical concepts from the patient’s clinical notes and\nwe have c ∈C, following task 1, we can formalize\nour goal as maximizing the conditional probability of\nP(Cu|u). Let cu,k as a medical concept (index as k)\nthat occurs in the clinical notes of the patient u, our\npatient-document task aims to maximize the following\nprobability:\nP(Cu|u) =\n|Cu|\nX\nk=1\nlogP(cu,k|u)\n(3)\nTo reduce computational complexity, we employ\nnegative sampling to approximate our task goal. The\nnegative sampling can help us convert the task to\na binary prediction task that checks if a sampled\nconcept exists in the patient’s clinical notes.\nIf a\nsampled concept exists in the target patient’s clinical\nnotes, the label is 1; otherwise, the prediction label\nis 0. The classiﬁcation problem minimizes loss values\nby the binary cross-entropy:\nL(u, c) = −log(σ(v(u) · v(cu)))\n−log(1 −σ(v(u) · v(c˜u)))\n(4)\n, where c˜u refers to negative samples of noise docu-\nments. Following the task 1, we use the sigmoid func-\ntion as our non-linear function to normalize values\nof the dot production. For each concept, we sample\nanother 3 noise concept sets which do not co-occur\nwith the target patient.\n3.4. Joint Learning\nWe optimize user embeddings by patient-document,\npatient-concept, and masked language modeling (mlm)\ntasks in parallel. The joint tasks can incorporate doc-\nument and concept information into user representa-\ntions. For the mlm, we follow pre-training tasks of\nthe BERT (Devlin et al., 2019) to randomly masked\nout 15% tokens only if the CAUE models are using\nthe BERT as neural feature extractors. We sum the\ntotal loss value by:\nL = λL(u, c) + (1 −λ)L(u, d) + αLmlm\n(5)\nwhere λ and α are weight factors to balance diﬀerent\ntasks. We empirically set λ as 0.3 and α as 0.03.\n4. Experiments\nIn this section, we present experimental settings of\nour approach and baselines. To keep consistency, we\nﬁx embedding dimensions as 300 for user and concept\nembeddings. We include more implementation details\nin our appendix.\n4.1. CAUE Settings\nWe treat neural models as feature extractors and ex-\nperiment with two types of neural document encoders,\nGated Recurrent Unit (GRU) (Cho et al., 2014) and\n5\nEnriching Unsupervised User Embedding via Medical Concepts\nModels\nPhenotype Prediction\nMortality\nPatient Relatedness\nRetrieval\nDiabetes\nMIMIC-III\nMIMIC-III\nDiabetes\nMIMIC-III\nDiabetes\nMIMIC-III\nBaselines\nword2user\n0.574\n0.321\n0.861\n0.530\n0.809\n0.374\n0.065\ndoc2user\n0.288\n0.235\n0.425\n0.573\n0.253\n0.344\n0.132\ndp2user\n0.416\n0.315\n0.868\n0.542\n0.829\n0.318\n0.113\nsuisil2user\n0.413\n0.274\n0.424\n0.542\n0.830\n0.313\n0.132\nusr2vec\n0.556\n0.158\n0.423\n0.355\n0.252\n0.349\n0.137\nsiamese2user\n0.590\n0.253\n0.750\n0.410\n0.218\n0.360\n0.116\nOurs\nCAUE-GRU\n0.635\n0.369\n0.817\n0.292\n0.205\n0.386\n0.139\nCAUE-BERT\n0.593\n0.339\n0.792\n0.337\n0.235\n0.396\n0.142\nTable 2: Two types of performance evaluations: supervised (phenotype and mortality classiﬁcation) and\nunsupervised (patient relatedness and retrieval). For the patient relatedness, lower scores are better.\nBERT (Devlin et al., 2019). We denote the two types\nof encoders as CAUE-GRU and CAUE-BERT in the\nfollowing. Each type of encoder reads through sequen-\ntial tokens and generates document representations.\nWe set maximum epochs as 15, batch size as 16, and\nvocabulary size as 15,000 for both token and concept.\nCAUE-GRU utilizes one GRU layer to encode\neach document. To incorporate more contextual infor-\nmation, we employ a bi-directional GRU layer. Its out-\nput is a concatenation of bi-directional hidden vectors.\nWe set the dimension of the GRU layer as 300 and\napply a dropout with 0.2 on outputs of the GRU layer.\nWe use a pre-trained word embedding (Zhang et al.,\n2019) to encode tokens into vectors. The embedding\nwas pre-trained on PubMed and MIMIC-III corpora.\nWhile extensive research methods have explored how\nto train concept embeddings (Choi et al., 2016; Zhang\net al., 2020), our study uses the concepts as mediums\nto learn hidden patterns to better separate patients.\nWe induce an initial concept embedding for (multi-\ntoken) concepts by averaging constituent word vectors.\nWe optimize the model using RMSprop (Hinton et al.,\n2012) with a learning rate 1e −4.\nCAUE-BERT encodes document into ﬁxed-length\nvectors via BERT (Devlin et al., 2019). The BERT\nmodel encodes each document by a hierarchical trans-\nformer (Vaswani et al., 2017) layers. We add a fully\nconnected layer on top of the encoded [CLS] token\nfrom the last layer of the BERT model to map 768-\ndimensional vectors into 300 dimensions. To get doc-\nument vectors, we pad input documents into a ﬁxed-\nlength, which is a hyperparameter. As initializing\nthe concept embedding with other pre-trained embed-\ndings does not align with the BERT in the vector\nspace, we treat concepts as short sentences and uti-\nlize the BERT model to obtain concept embeddings.\nWe experimented with multiple pre-trained BERT\nmodels (Peng et al., 2019; Alsentzer et al., 2019; Lee\net al., 2019). However, we did not ﬁnd signiﬁcant\nimprovements with the diﬀerent choices. We adopt\nAdaW (Loshchilov and Hutter, 2019) to optimize\nmodel parameters with a learning rate of 3e −5. We\npad documents with less than 512 tokens to ensure\nevery input document has an equal size.\n4.2. Baselines\nIn this study, we compare our approach with six\nunsupervised user embeddings.\nTo keep fair com-\nparison, we keep the same embedding dimensions,\nrun experiments with three times, and average model\nperformances. We use the same pre-trained embed-\ndings (Zhang et al., 2019; Alsentzer et al., 2019)\nthroughout the baselines.\nword2user represents users by averaging over ag-\ngregated word representations (Benton et al., 2016).\nWe calculate a user representation by averaging embed-\ndings of all tokens in the clinical notes of each patient.\nWe use the pre-trained word embedding (Zhang et al.,\n2019) to encode tokens.\ndoc2user applies paragraph2vec (Le and Mikolov,\n2014) to obtain user vectors. We implemented the\nUser-D-DBOW model, which achieved the best per-\nformance in the previous work (Ding et al., 2018).\nThe model trains the paragraph2vec for 10 epochs\nand left other parameters with default values in the\n6\nEnriching Unsupervised User Embedding via Medical Concepts\nGensim (Rehurek and Sojka, 2010). We append clini-\ncal documents of each patient as a single document.\nThen the User-D-DBOW model can derive a single\nuser vector from the aggregated document.\nsuisil2user combines the doc2user with medical\nconcepts to obtain patient vectors (Sushil et al., 2018).\nWe reuse doc2vec to derive document features and\nfollow the previous work to extract concept features.\nWe ﬁrst derive TF-IDF weighted scores of medical\nconcepts for each patient and train an autoencoder\nto reduce the dimensions of concept features. The\nautoencoder learns user embeddings by compressing\nfeatures inputs into intermediate embeddings and re-\nconstructing them back feature inputs.\nFollowing\nthe previous work, we set the sigmoid as the default\nactivation function. We add a 0.05 noise rate and\ntrain the autoencoder for ten epochs. The document\nvectors concatenate with dimension-reduced concept\nvectors to generate patient representations.\ndp2user generates patient representations by\napplying an autoencoder on extracted topic fea-\ntures (Miotto et al., 2016). We train Latent Dirichlet\nAllocation (LDA) (Blei et al., 2003) on the patient\nlevel and apply the LDA on clinical notes to derive\ntopic features. We set the number of topics as 300,\ntrain the model for 10 epochs, and leave the rest of the\nparameters as their default values in Gensim (Rehurek\nand Sojka, 2010). We train a four-layer autoencoder\nwith a noise score of 0.05 to reduce dimensions of\ntopic features.\nusr2vec trains user embeddings by predicting if\nusers authored sampled tokens (Amir et al., 2017).\nThe predictive goal is to measure if sampled words\nco-occur with a patient in clinical notes in terms of\nclinical notes. To make a fair comparison, instead of\nrandomly initializing word embeddings from scratch,\nwe initialized embedding weights as the same pre-\ntrained embeddings (Zhang et al., 2019) in our ap-\nproach. We train the model for 10 epochs, set the\nnumber of negative samples as three.\nsiamese2user utilizes a Siamese (Mueller and\nThyagarajan, 2016) recurrent architecture (Huang\net al., 2021) that learns patient representations on\nthe document level. The Siamese model measures\nsimilarities between two sentences. We encode each\nclinical note via a bidirectional GRU, which is beyond\nthe token-level usr2vec. For the patient, we apply\na feed-forward network on the patient vectors and\noptimize the model by distances between document\nand patient vectors.\n4.3. Extrinsic 1: Phenotype prediction\nWe train classiﬁers with user embeddings as input fea-\ntures to infer phenotype (ICD-9 codes). Each patient\nhas an encoded set, a one-hot vector, which is a length\nof the unique labels. The task is to evaluate how accu-\nrately a classiﬁer can predict the clinical labels of each\npatient. We use the embedding to encode patients\nand feed patient vectors to the logistic regression (LR)\nclassiﬁer for each user embedding. We optimize the\nLR classiﬁer by Adam (Kingma and Ba, 2014) and\nL2-regularization. We set the learning rate as 0.001\nand the regularization score as 0.01. We use 5-fold\ncross-validation to evaluate classiﬁcation performance.\nEach fold splits the Diabetes and MIMIC-III data\nbased on their patient counts by 80% as a training\nset and 20% as a test set. Finally, we report mean\naverage precision (MAP) for model evaluation.\nWe present our classiﬁcation results in Table 2.\nComparing to the baselines, LR classiﬁers trained\nby our proposed approach (CAUE-GRU) achieve the\nbest performance across the two datasets. The re-\nsults highlight that our method can capture patient\nvariations and augment phenotype inference via the\njoint optimization tasks. The only diﬀerence between\nthe siamese2user and usr2vec methods is whether it\nis on document level. We can ﬁnd that siamese2user\noutperforms usr2vec, which indicates that learning un-\nsupervised user embedding can beneﬁt from sequential\ndependency beyond individual tokens.\n4.4. Extrinsic 2: In-hospital mortality\nprediction\nIn-hospital mortality is a binary classiﬁcation task to\npredict if patients died during in-hospital stay based\non the ﬁrst 48 hours of an ICU stay. We follow the\nbenchmark (Harutyunyan et al., 2019) to generate\ninpatient mortality labels. The mortality annotations\nare only available for the MIMIC-III dataset. We\nuse 5-folds cross-validation to train LR classiﬁers and\nF1-score (macro) to evaluate the performance of mor-\ntality prediction as for the macro is less sensitive to\nimbalanced mortality labels.\nWe present mortality prediction results in Table 2.\nOur approach generally performs well among baselines.\nWe can ﬁnd that our close work usr2vec performs\nworse than siamese2user.\nTraining mechanisms of\nusr2vec is the poor performance on both classiﬁcation\ntasks. The usr2vec predicts relations between words\nand users on Twitter messages, which have much\nshorter documents. The negative sampling space will\n7\nEnriching Unsupervised User Embedding via Medical Concepts\nbe too sparse to ﬁnd strong patterns that separate pa-\ntients. This highlights the unique challenge to derive\nunsupervised user embeddings on clinical settings.\n4.5. Intrinsic 3: patient relatedness\nThe patient relatedness task is to measure seman-\ntic relation or similarity between patients. Data to\nevaluate relatedness originates from lexical semantics.\nSimLex-999 (Hill et al., 2015) provides a list of word\npairs with similarity scores from human annotators.\nSimLex-999 checks if similarities of word vectors are\nclose to human perceptions. Inspired by this, we uti-\nlize an intrinsic evaluation that measures the quality\nof user embeddings by patient relatedness. However,\nunlike word embedding evaluation, there is no human\njudgement for patient-level similarities. Therefore, we\nadapt medical codes to measure patient similarities.\nThe patient relatedness assumes that the similarity\nbetween every two patients should be proportional to\nthe similarity of their clinical labels:\nSimilarity(u1, u2) ∝Similarity(l1, l2)\n(6)\n, where u1 ∈U, u2 ∈U, l1 and l2 are patients’ corre-\nsponding phenotype labels. We use user embeddings\nto encode patients into vectors and encode the la-\nbels into one-hot vectors.\nThe dimension of each\nlabel vector is the number of unique clinical labels.\nFor example, l1 in MIMIC-III data has 276 dimen-\nsions, corresponding to number of unique phenotype\nlabels in Table 1. We use cosine similarity to measure\npatient relatedness between vectors. Mean Squared\nError (MSE) is to measure diﬀerences between a pair\nof patients and a pair of label vectors:\nMSE =\n1\n|U|\n|U|\nX\ni=1\n(Sim(l1, l2) −Sim(u1, u2))2\n(7)\nWe present our evaluation results of patient relat-\nedness in Table 2. Overall our concept-aware user\nembedding (CAUE-GRU) achieves the best perfor-\nmance over baselines. While the suisil2user achieves\nbetter performance than the doc2user on Diabetes, it\nfails on the MIMIC-III. Furthermore, our approach\nyields good performance on the two datasets. The\nresult can highlight that how to encode concept fea-\ntures into user embeddings are critical for improving\nmodel robustness.\n4.6. Intrinsic 4: patient retrieval\nThe retrieval task is to treat each patient as a “query”\nand measure relations between the query and its re-\ntrieved top ranks. We ﬁnd the top 10 most similar\npatients for a query patient by calculating cosine sim-\nilarities between the query patient vector and other\npatient vectors. To measure the eﬀectiveness of user\nembeddings, we need to examine the quality of top-\nranking returns. Intuitively, the top returns should\nshare similar clinical conditions with the query pa-\ntient. However, clinical labels of each patient can be\nmore than the binary labels in previous work (Amir\net al., 2017) or in-hospital mortality. The query aims\nto ﬁnd the best candidates so that similarities be-\ntween the query and the other candidates should be\nclose to 1. Therefore, we adapt jaccard coeﬃcient to\nmeasure similarity between a query patient (u1) and\na retrieved patient (u2) by their clinical labels:\nJaccard(u1, u2) = |l1 ∩l2|\n|l1 ∪l2|\n(8)\nWe choose ten top ranks and evaluate retrieved\ncandidates by the average Jaccard coeﬃcients. We\npresent evaluation results of the retrieval task in Ta-\nble 2. The CAUE-GRU outperforms baselines by a\nlarge margin. The result highlights that modeling\nlanguage sequential information can be beneﬁcial to\nlearn patient clinical conditions for retrieval tasks.\nThe CAUE outperforms the siamese2user, which does\nnot incorporate concepts. Performance improvements\nshow that incorporating medical concepts and lever-\naging document-level relations can signiﬁcantly boost\nmodel performance.\n4.7. Eﬀectiveness of Medical Concepts\nWe now investigate the eﬀectiveness of medical con-\ncepts in more detail. While our approach uses neural\nconcept embeddings, other baselines do not. Our main\ngoal is to examine if medical concepts can generally\nimprove the performance of user representations.\nWe update ﬁve baselines (word2user, doc2user,\ndp2user, usr2vec, and siamese2user) to incorporate\nconcept information into training user embedding\nunder the multitask learning framework.\nFor the\nword2user, the model aggregates all tokens of con-\ncepts, average over the concept vectors, and com-\nbine both token and concept vectors to obtain new\npatient representations. For the doc2user, we treat\ngroups of medical concepts as documents, build a para-\ngraph2vec (Le and Mikolov, 2014) model to convert\n8\nEnriching Unsupervised User Embedding via Medical Concepts\n+ Concept\nPhenotype Prediction\nMortality\nPatient Relatedness\nRetrieval\nDiabetes\nMIMIC-III\nMIMIC-III\nDiabetes\nMIMIC-III\nDiabetes\nMIMIC-III\nword2user\n+7.6% (.044)\n+40.4% (.130)\n+4.8% (.041)\n+3.4% (-.018)\n+1.2% (-.010)\n-0.5% (-.002)\n+24.6% (.016)\ndoc2user\n-43.4% (-.125)\n-32.3% (-.076)\n+2.8% (.012)\n+1.1% (-.006)\n+0.8% (-.02)\n+6.1% (.021)\n-3.0% (-.004)\ndp2user\n+22.6% (.094)\n+52.1% (.164)\n+3.8% (.033)\n+2.0% (-.011)\n+24.9% (-.207)\n+17.6% (.056)\n+18.6% (.021)\nusr2vec\n+9.4% (.052)\n+249.0% (.394)\n+111.1% (.470)\n+6.2% (-.022)\n-63.7% (.444)\n+9.5% (.033)\n-36.5% (-.050)\nsiamese2user\n+7.6% (.045)\n+45.8% (.116)\n+45.8% (.116)\n+28.7% (-.118)\n+6.0% (-.013)\n+7.2% (.026)\n+19.8% (.023)\nAverage\n+.8% (.013)\n+61.84 (.146)\n+33.66% (.134)\n+8.28% (-.035)\n-6.16% (.194)\n+8.0% (.027)\n+4.7 (.001)\nMedian\n+9.4% (.052)\n+45.8% (.116)\n+4.8% (.041)\n+3.4% (-.018)\n+1.2% (-.010)\n+7.2% (.026)\n+18.6% (.021)\nTable 3: Performance gains of user embedding models combining with medical concepts (+Concept) comparing\nto standard non-concept information. We report percentage increases with absolute value increases\nin brackets after incorporating medical concepts. For patient relatedness, because lower is better, a\nnegative value within brackets means a performance gain.\nconcepts into vectors, and concatenate both tokens\nwith the new concept vectors. For the dp2user, we\nbuild a second topic model on medical concepts be-\nsides the ﬁrst topic model over tokens. We then derive\nconcept vectors by the second topic model and train\nan autoencoder to reduce dimensions of the concept-\ntopic features. Finally, we combine the compressed\nconcept and token vectors from the two separated au-\ntoencoders. For the siamese2user, we build a second\nprediction task to compare distances between user\nand concept embeddings. For the usr2vec, we create\ntwo joint predictions, user-token and user-concept pre-\ndictions. Both prediction tasks follow the negative\nsampling process to learn user embeddings. Next, we\ninitialize concept embeddings by averaging vectors of\ntokens within each concept, which gets token vectors\nfrom the pre-trained word embedding (Zhang et al.,\n2019).\nTable 3 shows the percentage improvement and\nabsolute value increases in brackets across four eval-\nuation tasks when incorporating medical concepts\ncompared to baselines without explicitly leveraging\nmedical concepts. Overall, medical concepts generally\naugment the performance of user embeddings with an\naverage absolute improvement of 5.93% on Diabetes\nand 23.51% on MIMIC-III respectively. The medi-\ncal concepts appear to be particularly important for\nCAUE, improving performance on all four evaluation\ntasks with an average increase in performance up to\n20.3%. Our close work usr2vec gains a signiﬁcant im-\nprovement with an average of 40.7%. Comparing the\ndiﬀerent methods for incorporating medical concepts,\non the one hand, our proposed CAUE-GRU works\nthe best on average overall; on the other hand, joint\nlearning of concepts and clinical notes can generally\nimprove eﬀectiveness of the baselines.\n5. Related Work\nIn this work, we focus on learning user embeddings\nfrom clinical notes, while some studies (Lei et al.,\n2018; Wang et al., 2019; Yin et al., 2019; Steinberg\net al., 2021) learn patient representations from tabular\ndata. Yin et al. applies tensor factorization to learn\npatient representations from tabular data, including\ndiagnosis codes and laboratory tests. However, learn-\ning user embeddings from clinical notes is our focus\nin this study, and clinical notes are noisy and unstruc-\ntured data. Several studies have proposed supervised\nuser embeddings for clinical notes (Wu et al., 2020; Si\nand Roberts, 2019). Dligach and Miller learn patient\nvectors by feed-forward neural networks on concept\nembeddings. The model averages concept vectors for\neach patient and predicts ICD-9 codes (Dligach and\nMiller, 2018). Si and Roberts presents a hierarchical\nrecurrent neural network to train embeddings for pa-\ntients. The method trains a model on MIMIC-III with\nphenotype annotations and applies the pre-trained\nmodel on other tasks (Si and Roberts, 2020). However,\nunsupervised user embeddings do not have medical\nannotations to learn hidden structures of patients. Au-\ntoencoder is an encoder-decode neural network that\ntakes patient features as inputs, compresses the fea-\ntures into ﬁxed-length vectors, and reconstructs the\nvectors back to its input features (Miotto et al., 2016;\nSushil et al., 2018). However, the autoencoder-based\napproaches require well-designed feature sets, and the\ndimension ratio between input features and user em-\nbeddings may reduce training eﬀectiveness. Our study\nﬁlls the gap by proposing an unsupervised user em-\nbedding under a multitask framework that enforces\nmodels to recognize patients of clinical notes. The\napproach does not require any human supervision and\ncan learn user embeddings in an end-to-end style.\n9\nEnriching Unsupervised User Embedding via Medical Concepts\nConcept embeddings aim to learn vector rep-\nresentations for medical concepts. Research treats\nmedical concepts as tokens or phrases and encodes\nthem into vectors by contextualized word embed-\ndings (Zhang et al., 2020), medical ontology (Song\net al., 2019) and graphical neural networks (Choi et al.,\n2016, 2020). However, few studies have incorporate\nmedical concepts into learning patient representations\nunder unsupervised settings. Our study jointly lever-\nages clinical notes and medical concepts to learn user\nembeddings for patients. While our study does not\nfocus on concept embeddings, we utilize the concept\nembeddings to enrich patient representations. Recent\nstudies utilized concepts in multimodal machine learn-\ning to align embedding spaces between vision and\nlanguage (Zhang et al., 2021). Our study utilizes con-\ncepts as additional constraints that balance semantic\nrelations between patient, clinical text descriptions,\nand medical concepts.\nMultitask learning simultaneously trains a model\nwith multiple tasks. For example, Si and Roberts\noptimize patient representations by two supervised\ntasks, mortality and length of stay predictions (Si\nand Roberts, 2019). However, this study handles a\ntriangle relation across patient, clinical notes, and\nmedical concepts. We deﬁne the relation predictions\nas tasks to joint optimize patient representations in-\nstead of the downstream tasks. Our approach trains\nuser embeddings from multiple sources and uses med-\nical concepts to enrich patient representations. While\nstudies (Miotto et al., 2016; Sushil et al., 2018) uti-\nlize features from diﬀerent sources, they concatenate\npatient features and train models in a single task.\n6. Conclusion\nIn this study, we have proposed a concept-aware user\nembedding (CAUE) in an unsupervised setting. The\nmodel jointly leverages information of medical con-\ncepts and clinical notes using multitask learning. The\nCAUE is an end-to-end neural model without any\nhuman supervision via contrastive learning and nega-\ntive sampling. Our extrinsic and intrinsic evaluations\ndemonstrate the eﬀectiveness of our approach, which\noutperforms baselines by a large margin. Nonetheless,\nthe medical concept shows its eﬀectiveness in improv-\ning unsupervised baselines in general. We include\nadditional implementation details in the appendix.\nLimitation.\nWhile experiments have demonstrated\nthe eﬀectiveness of our proposed unsupervised ap-\nproach in clinical settings, several limitations must be\nacknowledged to interpret our ﬁndings appropriately.\nFirst, user behaviors can shift over time (Han et al.,\n2020). In this study, we have proposed methods to\nisolate data entries by a patient per hospital visit. The\nstrategy can eﬀectively reduce the temporal impacts\non modeling and evaluation. Second, data sources\nmay hurdle model evaluations due to their diﬀerent\ntypes of clinical annotations. We keep evaluations\nconsistent across the two datasets and conduct intrin-\nsic evaluations to avoid training a large number of\nclassiﬁer parameters.\nInstitutional Review Board (IRB)\nThis study has no human-subject research and only\nuses publicly available and de-identiﬁed data, which\ndoes not need an IRB approval.\nAcknowledgments\nThe authors thank the anonymous reviews for their\ninsightful comments and suggestions. This work was\nsupported in part by a research gift from Adobe Re-\nsearch. The authors would also thank the HPC cluster\nprovided by the University of Memphis.\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott.\nPublicly available clini-\ncal BERT embeddings.\nIn Proceedings of the\n2nd Clinical Natural Language Processing Work-\nshop, pages 72–78, Minneapolis, Minnesota, USA,\nJune 2019. Association for Computational Linguis-\ntics.\ndoi: 10.18653/v1/W19-1909.\nURL https:\n//www.aclweb.org/anthology/W19-1909.\nSilvio Amir, Glen Coppersmith, Paula Carvalho,\nMario J. Silva, and Bryon C. Wallace.\nQuanti-\nfying mental health from social media with neural\nuser embeddings. In Finale Doshi-Velez, Jim Fack-\nler, David Kale, Rajesh Ranganath, Byron Wal-\nlace, and Jenna Wiens, editors, Proceedings of the\n2nd Machine Learning for Healthcare Conference,\nvolume 68 of Proceedings of Machine Learning Re-\nsearch, pages 306–321, Boston, Massachusetts, 18–\n19 Aug 2017. PMLR. URL http://proceedings.\nmlr.press/v68/amir17a.html.\n10\nEnriching Unsupervised User Embedding via Medical Concepts\nAlan R Aronson and Fran¸cois-Michel Lang.\nAn\noverview of MetaMap: historical perspective and\nrecent advances. Journal of the American Med-\nical Informatics Association, 17(3):229–236, 05\n2010. ISSN 1067-5027. doi: 10.1136/jamia.2009.\n002733. URL https://doi.org/10.1136/jamia.\n2009.002733.\nAdrian Benton, Raman Arora, and Mark Dredze.\nLearning multiview embeddings of twitter users.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Vol-\nume 2: Short Papers), pages 14–19, Berlin, Ger-\nmany, August 2016. Association for Computational\nLinguistics.\ndoi:\n10.18653/v1/P16-2003.\nURL\nhttps://www.aclweb.org/anthology/P16-2003.\nSteven Bird and Edward Loper. NLTK: The natural\nlanguage toolkit. In Proceedings of the ACL Inter-\nactive Poster and Demonstration Sessions, pages\n214–217, July 2004. URL https://www.aclweb.\norg/anthology/P04-3031.\nElena Birman-Deych, Amy D Waterman, Yan Yan,\nDavid S Nilasena, Martha J Radford, and Brian F\nGage. Accuracy of icd-9-cm codes for identifying car-\ndiovascular and stroke risk factors. Medical care, 43\n(5):480–485, 2005. URL http://www.jstor.org/\nstable/3768402.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\nLatent dirichlet allocation.\nJournal of Machine\nLearning Research, 3:993–1022, 2003. URL http:\n//www.jmlr.org/papers/v3/blei03a.html.\nOlivier Bodenreider.\nThe Uniﬁed Medical Lan-\nguage System (UMLS): integrating biomedical\nterminology. Nucleic acids research, 32(Database\nissue):D267–D270, jan 2004. ISSN 1362-4962. doi:\n10.1093/nar/gkh061. URL https://pubmed.ncbi.\nnlm.nih.gov/14681409https://www.ncbi.nlm.\nnih.gov/pmc/articles/PMC308795/.\nTing\nChen,\nSimon\nKornblith,\nKevin\nSwersky,\nMohammad\nNorouzi,\nand\nGeoﬀrey\nE\nHin-\nton.\nBig self-supervised models are strong\nsemi-supervised\nlearners.\nIn\nH.\nLarochelle,\nM. Ranzato, R. Hadsell, M. F. Balcan, and\nH. Lin, editors, Advances in Neural Information\nProcessing Systems, volume 33, pages 22243–22255.\nCurran Associates, Inc., 2020.\nURL https:\n//proceedings.neurips.cc/paper/2020/file/\nfcbc95ccdd551da181207c0c1400c655-Paper.\npdf.\nKyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. On the properties of\nneural machine translation: Encoder–decoder ap-\nproaches. In Proceedings of SSST-8, Eighth Work-\nshop on Syntax, Semantics and Structure in Sta-\ntistical Translation, pages 103–111, Doha, Qatar,\nOctober 2014. Association for Computational Lin-\nguistics. doi: 10.3115/v1/W14-4012. URL https:\n//www.aclweb.org/anthology/W14-4012.\nEdward Choi, Mohammad Taha Bahadori, Eliza-\nbeth Searles, Catherine Coﬀey, Michael Thompson,\nJames Bost, Javier Tejedor-Sojo, and Jimeng Sun.\nMulti-layer representation learning for medical con-\ncepts. In Proceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge Discov-\nery and Data Mining, KDD ’16, page 1495–1504,\nNew York, NY, USA, 2016. Association for Com-\nputing Machinery.\nISBN 9781450342322.\ndoi:\n10.1145/2939672.2939823. URL https://doi.org/\n10.1145/2939672.2939823.\nEdward Choi, Mohammad Taha Bahadori, Le Song,\nWalter F. Stewart, and Jimeng Sun. Gram: Graph-\nbased attention model for healthcare representation\nlearning. In Proceedings of the 23rd ACM SIGKDD\nInternational Conference on Knowledge Discov-\nery and Data Mining, KDD ’17, page 787–795,\nNew York, NY, USA, 2017. Association for Com-\nputing Machinery.\nISBN 9781450348874.\ndoi:\n10.1145/3097983.3098126. URL https://doi.org/\n10.1145/3097983.3098126.\nEdward Choi, Zhen Xu, Yujia Li, Michael Dusen-\nberry, Gerardo Flores, Emily Xue, and Andrew Dai.\nLearning the graphical structure of electronic health\nrecords with graph convolutional transformer. In\nProceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 34, pages 606–613, New York,\nUS, Apr. 2020. AAAI. doi: 10.1609/aaai.v34i01.\n5400.\nURL https://ojs.aaai.org/index.php/\nAAAI/article/view/5400.\nS. Darabi, M. Kachuee, S. Fazeli, and M. Sar-\nrafzadeh.\nTaper:\nTime-aware patient ehr rep-\nresentation.\nIEEE Journal of Biomedical and\nHealth Informatics, 24(11):3268–3275, 2020. doi:\n10.1109/JBHI.2020.2984931. URL https://doi.\norg/10.1109/jbhi.2020.2984931.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. BERT: Pre-training of deep\n11\nEnriching Unsupervised User Embedding via Medical Concepts\nbidirectional transformers for language understand-\ning.\nIn Proceedings of the 2019 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, June 2019.\ndoi:\n10.18653/\nv1/N19-1423.\nURL https://www.aclweb.org/\nanthology/N19-1423.\nT. Ding, W. K. Bickel, and S. Pan. Predicting delay\ndiscounting from social media likes with unsuper-\nvised feature learning. In 2018 IEEE/ACM Interna-\ntional Conference on Advances in Social Networks\nAnalysis and Mining, pages 254–257, Barcelona,\nSpain, Aug 2018. IEEE. doi: 10.1109/ASONAM.\n2018.8508277.\nDmitriy Dligach and Timothy Miller. Learning Pa-\ntient Representations from Text. In Proceedings of\nthe Seventh Joint Conference on Lexical and Com-\nputational Semantics, pages 119–123, New Orleans,\nLouisiana, jun 2018. Association for Computational\nLinguistics.\ndoi:\n10.18653/v1/S18-2014.\nURL\nhttps://www.aclweb.org/anthology/S18-2014.\nLei Han, Alessandro Checco, Djellel Difallah, Gian-\nluca Demartini, and Shazia Sadiq. Modelling user\nbehavior dynamics with embeddings. In Proceedings\nof the 29th ACM International Conference on Infor-\nmation; Knowledge Management, CIKM ’20, page\n445–454, New York, NY, USA, 2020. Association\nfor Computing Machinery. ISBN 9781450368599.\ndoi: 10.1145/3340531.3411985. URL https://doi.\norg/10.1145/3340531.3411985.\nHrayr Harutyunyan, Hrant Khachatrian, David C.\nKale, Greg Ver Steeg, and Aram Galstyan. Multi-\ntask learning and benchmarking with clinical time\nseries data. Scientiﬁc Data, 6(1):96, 2019. ISSN\n2052-4463. doi: 10.1038/s41597-019-0103-9. URL\nhttps://doi.org/10.1038/s41597-019-0103-9.\nFelix Hill, Roi Reichart, and Anna Korhonen. SimLex-\n999: Evaluating semantic models with (genuine)\nsimilarity estimation. Computational Linguistics,\n41(4):665–695, December 2015.\ndoi:\n10.1162/\nCOLI a 00237.\nURL https://www.aclweb.org/\nanthology/J15-4004.\nGeoﬀrey\nE.\nHinton,\nNitish\nSrivastava,\nand\nKevin\nSwersky.\nLecture\n6a-\noverview\nof\nmini-batch\ngradient\ndescent,\n2012.\nURL\nhttp://www.cs.toronto.edu/{~}tijmen/\ncsc321/slides/lecture{_}slides{_}lec6.pdf.\nXiaolei Huang, Michael J. Paul, Franck Dernoncourt,\nRobin Burke, and Mark Dredze. User factor adap-\ntation for user embedding via multitask learning.\nIn Proceedings of the Second Workshop on Do-\nmain Adaptation for NLP, pages 172–182, Kyiv,\nUkraine, April 2021. URL https://www.aclweb.\norg/anthology/2021.adaptnlp-1.18.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-\nwei H Lehman, Mengling Feng, Mohammad Ghas-\nsemi, Benjamin Moody, Peter Szolovits, Leo An-\nthony Celi, and Roger G Mark. Mimic-iii, a freely\naccessible critical care database. Scientiﬁc Data,\n3:160035, 2016. doi: 10.1038/sdata.2016.35. URL\nhttps://doi.org/10.1038/sdata.2016.35.\nDiederik P. Kingma and Jimmy Ba. Adam: A Method\nfor Stochastic Optimization. In Proceedings of the\n3rd International Conference on Learning Repre-\nsentations, pages 1–15, San Diego, US, 2014. ICLR.\nURL http://arxiv.org/abs/1412.6980.\nQuoc Le and Tomas Mikolov. Distributed representa-\ntions of sentences and documents. In Eric P. Xing\nand Tony Jebara, editors, Proceedings of Machine\nLearning Research, volume 32, pages 1188–1196,\nBejing, China, 22–24 Jun 2014. PMLR. URL http:\n//proceedings.mlr.press/v32/le14.html.\nJinhyuk\nLee,\nWonjin\nYoon,\nSungdong\nKim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang.\nBioBERT: a pre-trained biomed-\nical language representation model for biomed-\nical text mining.\nBioinformatics, 36(4):1234–\n1240, 09 2019.\nISSN 1367-4803.\ndoi: 10.1093/\nbioinformatics/btz682. URL https://doi.org/10.\n1093/bioinformatics/btz682.\nLiqi Lei, Yangming Zhou, Jie Zhai, Le Zhang, Zhijia\nFang, Ping He, and Ju Gao. An eﬀective patient\nrepresentation learning for time-series prediction\ntasks based on ehrs. In 2018 IEEE International\nConference on Bioinformatics and Biomedicine,\npages 885–892, Madrid, Spain, 2018. IEEE, IEEE.\ndoi: 10.1109/BIBM.2018.8621542.\nURL https:\n//doi.org/10.1109/BIBM.2018.8621542.\nSheng Li and Handong Zhao.\nA survey on repre-\nsentation learning for user modeling.\nIn Chris-\ntian Bessiere, editor, Proceedings of the Twenty-\nNinth International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-20, pages 4997–5003, online,\n7 2020. International Joint Conferences on Arti-\nﬁcial Intelligence Organization.\ndoi:\n10.24963/\n12\nEnriching Unsupervised User Embedding via Medical Concepts\nijcai.2020/695. URL https://doi.org/10.24963/\nijcai.2020/695. Survey track.\nLajanugen Logeswaran and Honglak Lee. An eﬃcient\nframework for learning sentence representations.\nIn International Conference on Learning Repre-\nsentations, 2018. URL https://openreview.net/\nforum?id=rJvJXZb0W.\nIlya Loshchilov and Frank Hutter. Decoupled weight\ndecay regularization. In International Conference\non Learning Representations, 2019. URL https:\n//openreview.net/forum?id=Bkg6RiCqY7.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeﬀrey Dean. Distributed representations\nof words and phrases and their compositionality. In\nProceedings of the 26th International Conference on\nNeural Information Processing Systems - Volume 2,\nNIPS’13, pages 3111–3119, USA, 2013. Curran As-\nsociates Inc. URL http://dl.acm.org/citation.\ncfm?id=2999792.2999959.\nRiccardo Miotto, Li Li, Brian A Kidd, and Joel T\nDudley.\nDeep patient:\nan unsupervised repre-\nsentation to predict the future of patients from\nthe electronic health records.\nScientiﬁc reports,\n6(1):1–10, 2016.\ndoi: 10.1038/srep26094.\nURL\nhttps://doi.org/10.1038/srep26094.\nJonas Mueller and Aditya Thyagarajan.\nSiamese\nrecurrent architectures for learning sentence sim-\nilarity.\nIn Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, volume 30, Mar.\n2016.\nURL https://ojs.aaai.org/index.php/\nAAAI/article/view/10350.\nJames Mullenbach, Sarah Wiegreﬀe, Jon Duke, Ji-\nmeng Sun, and Jacob Eisenstein. Explainable pre-\ndiction of medical codes from clinical text.\nIn\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pages 1101–1111,\nJune 2018.\ndoi:\n10.18653/v1/N18-1100.\nURL\nhttps://www.aclweb.org/anthology/N18-1100.\nShimei Pan and Tao Ding. Social media-based user\nembedding: A literature review. In Proceedings of\nthe Twenty-Eighth International Joint Conference\non Artiﬁcial Intelligence, IJCAI-19, pages 6318–\n6324, Macao, China, 7 2019. International Joint\nConferences on Artiﬁcial Intelligence Organization.\ndoi: 10.24963/ijcai.2019/881. URL https://doi.\norg/10.24963/ijcai.2019/881.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alch´e-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n32, pages 8024–8035. Curran Associates, Inc., 2019.\nURL https://arxiv.org/pdf/1912.01703.pdf.\nFabian Pedregosa, Gael Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron\nWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-\ndre Passos, David Cournapeau, Matthieu Brucher,\nMatthieu Perrot, and ´Edouard Duchesnay. Scikit-\nlearn: Machine learning in Python. Journal of Ma-\nchine Learning Research, 12(Oct):2825–2830, 2011.\nISSN 15324435.\nYifan Peng, Shankai Yan, and Zhiyong Lu. Transfer\nlearning in biomedical natural language process-\ning: An evaluation of BERT and ELMo on ten\nbenchmarking datasets. In Proceedings of the 18th\nBioNLP Workshop and Shared Task, pages 58–65,\nAugust 2019. doi: 10.18653/v1/W19-5006. URL\nhttps://www.aclweb.org/anthology/W19-5006.\nRadim Rehurek and Petr Sojka.\nSoftware Frame-\nwork for Topic Modelling with Large Corpora. In\nProceedings of the LREC 2010 Workshop on New\nChallenges for NLP Frameworks, pages 45–50, Val-\nletta, Malta, may 2010. ELRA. ISBN 2-9517408-\n6-7.\nURL https://radimrehurek.com/gensim/\nlrec2010_final.pdf.\nGuergana K Savova, James J Masanz, Philip V Ogren,\nJiaping Zheng, Sunghwan Sohn, Karin C Kipper-\nSchuler, and Christopher G Chute.\nMayo clini-\ncal Text Analysis and Knowledge Extraction Sys-\ntem (cTAKES): architecture, component evalua-\ntion and applications. Journal of the American\nMedical Informatics Association, 17(5):507–513, 09\n2010. ISSN 1067-5027. doi: 10.1136/jamia.2009.\n001560. URL https://doi.org/10.1136/jamia.\n2009.001560.\n13\nEnriching Unsupervised User Embedding via Medical Concepts\nTobias Schnabel, Igor Labutov, David Mimno, and\nThorsten Joachims. Evaluation methods for unsu-\npervised word embeddings. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing, pages 298–307, Septem-\nber 2015.\ndoi:\n10.18653/v1/D15-1036.\nURL\nhttps://www.aclweb.org/anthology/D15-1036.\nYuqi Si and Kirk Roberts.\nDeep patient repre-\nsentation of clinical notes via multi-task learn-\ning for mortality prediction.\nAMIA Summits\non Translational Science Proceedings, 2019:779,\n2019. URL https://www.ncbi.nlm.nih.gov/pmc/\narticles/PMC6568068/.\nYuqi Si and Kirk Roberts. Patient representation\ntransfer learning from clinical notes based on hi-\nerarchical attention network.\nAMIA Summits\non Translational Science Proceedings, 2020:597,\n2020. URL https://www.ncbi.nlm.nih.gov/pmc/\narticles/PMC7233035/.\nYuqi Si, Jingcheng Du, Zhao Li, Xiaoqian Jiang,\nTimothy Miller, Fei Wang, W. Jim Zheng, and\nKirk Roberts.\nDeep representation learning\nof patient data from electronic health records\n(ehr): A systematic review. Journal of Biomed-\nical Informatics, 115:103671, 2020.\nISSN 1532-\n0464.\ndoi:\nhttps://doi.org/10.1016/j.jbi.2020.\n103671. URL https://www.sciencedirect.com/\nscience/article/pii/S1532046420302999.\nLihong Song, Chin Wang Cheong, Kejing Yin,\nWilliam K. Cheung, Benjamin C. M. Fung, and\nJonathan Poon. Medical concept embedding with\nmultiple ontological representations. In Proceed-\nings of the Twenty-Eighth International Joint Con-\nference on Artiﬁcial Intelligence, IJCAI-19, pages\n4613–4619. International Joint Conferences on Ar-\ntiﬁcial Intelligence Organization, 7 2019.\ndoi:\n10.24963/ijcai.2019/641. URL https://doi.org/\n10.24963/ijcai.2019/641.\nErgin Soysal, Jingqi Wang, Min Jiang, Yonghui\nWu, Serguei Pakhomov, Hongfang Liu, and Hua\nXu.\nCLAMP – a toolkit for eﬃciently building\ncustomized clinical natural language processing\npipelines. Journal of the American Medical Infor-\nmatics Association, 25(3):331–336, 11 2017. ISSN\n1527-974X.\ndoi:\n10.1093/jamia/ocx132.\nURL\nhttps://doi.org/10.1093/jamia/ocx132.\nEthan Steinberg, Ken Jung, Jason A Fries, Conor K\nCorbin, Stephen R Pfohl, and Nigam H Shah. Lan-\nguage models are an eﬀective representation learn-\ning technique for electronic health record data.\nJournal of Biomedical Informatics, 113:103637,\n2021.\nISSN 1532-0464.\ndoi: 10.1016/j.jbi.2020.\n103637. URL https://www.sciencedirect.com/\nscience/article/pii/S1532046420302653.\nAmber Stubbs, Michele Filannino, Ergin Soysal,\nSamuel Henry, and ¨Ozlem Uzuner.\nCohort se-\nlection for clinical trials: n2c2 2018 shared task\ntrack 1.\nJournal of the American Medical In-\nformatics Association, 26(11):1163–1171, 09 2019.\nISSN 1527-974X. doi: 10.1093/jamia/ocz163. URL\nhttps://doi.org/10.1093/jamia/ocz163.\nMadhumita Sushil, Simon ˇSuster, Kim Luyckx,\nand Walter Daelemans.\nPatient representa-\ntion learning and interpretable evaluation us-\ning\nclinical\nnotes.\nJournal\nof\nBiomedical\nInformatics,\n84:103–113,\n2018.\nISSN\n1532-\n0464.\ndoi:\nhttps://doi.org/10.1016/j.jbi.2018.\n06.016. URL https://www.sciencedirect.com/\nscience/article/pii/S1532046418301266.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez,  Lukasz\nKaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing\nsystems, pages 5998–6008, Long Beach, CA, US,\n2017. NIPS.\nYing Wang, Xiao Xu, Tao Jin, Xiang Li, Guotong\nXie, and Jianmin Wang. Inpatient2vec: Medical\nrepresentation learning for inpatients.\nIn 2019\nIEEE International Conference on Bioinformat-\nics and Biomedicine, pages 1113–1117, San Diego,\nCA, USA, 2019. IEEE. doi: 10.1109/BIBM47256.\n2019.8983281. URL https://doi.org/10.1109/\nBIBM47256.2019.8983281.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, October 2020.\nURL https://www.aclweb.org/anthology/2020.\nemnlp-demos.6.\n14\nEnriching Unsupervised User Embedding via Medical Concepts\nStephen Wu, Kirk Roberts, Surabhi Datta, Jingcheng\nDu, Zongcheng Ji, Yuqi Si, Sarvesh Soni, Qiong\nWang, Qiang Wei, Yang Xiang, et al. Deep learn-\ning in clinical natural language processing: a me-\nthodical review. Journal of the American Medical\nInformatics Association, 27(3):457–470, 2020. doi:\n10.1093/jamia/ocz200. URL https://doi.org/10.\n1093/jamia/ocz200.\nKejing Yin, Dong Qian, William K. Cheung, Ben-\njamin C. M. Fung, and Jonathan Poon.\nLearn-\ning phenotypes and dynamic patient representa-\ntions via rnn regularized collective non-negative\ntensor factorization. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 33,\npages 1246–1253, Jul. 2019.\ndoi: 10.1609/aaai.\nv33i01.33011246.\nURL https://ojs.aaai.org/\nindex.php/AAAI/article/view/3920.\nBowen Zhang, Hexiang Hu, Linlu Qiu, Peter Shaw,\nand Fei Sha. Visually grounded concept compo-\nsition. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021, pages 201–\n215, Punta Cana, Dominican Republic, Novem-\nber 2021. Association for Computational Lin-\nguistics. URL https://aclanthology.org/2021.\nfindings-emnlp.20.\nXiao Zhang, Dejing Dou, and Ji Wu.\nLearning\nconceptual-contextual embeddings for medical text.\nIn Proceedings of the AAAI Conference on Artiﬁ-\ncial Intelligence, volume 34, pages 9579–9586, New\nYork, US, Apr. 2020. AAAI. doi: 10.1609/aaai.\nv34i05.6504. URL https://ojs.aaai.org/index.\nphp/AAAI/article/view/6504.\nYijia Zhang, Qingyu Chen, Zhihao Yang, Hongfei Lin,\nand Zhiyong Lu. Biowordvec, improving biomedical\nword embeddings with subword information and\nmesh.\nScientiﬁc data, 6(1):1–9, 2019.\ndoi: 10.\n1038/s41597-019-0055-0. URL https://doi.org/\n10.1038/s41597-019-0055-0.\nAppendix A. Ablation Analysis\nWe compare performance impacts of individual mod-\nules in our proposed approach. The comparison ex-\nperiments follow the previous experimental settings in\nSection 4 and trained models with diﬀerent learning\nrates within [1e −4, 1e −6]. We experiment with the\ntwo base neural encoders, GRU and BERT, in our\nCAUE framework with both the contrastive learn-\ning (CT) and medical concepts (CC). There are four\ncombination types on top of the base neural feature\nextractors (GRU and BERT): 1) none of the compo-\nnents (None), 2) only with CT (+CT), 3) only with\nCC (+CC), 4) and with both optimization compo-\nnents (+CC+CT). We evaluate the four combinations\non the four tasks, phenotype prediction, mortality,\npatient relatedness, and retrieval.\nWe present results in Table 4. Generally, learning\nuser embeddings with CT and CC can achieve the\nbest performance across the evaluation tasks. Both\nCT and CC can improve the embedding performance.\nThis indicates that the two proposed strategies work\ncoherently to improve model eﬀectiveness on extract-\ning patient information from clinical notes and medi-\ncal concepts. And we can also observe that medical\nconcepts generally have more improvements over the\ncontrastive learning. For example, +CC models usu-\nally outperforms +CT models. This indicates that\nmedical concepts may have more indicative informa-\ntion and correlations with the clinical codes.\nAppendix B. Implementation Details\nWe follow previous work (Mullenbach et al., 2018;\nHarutyunyan et al., 2019; Stubbs et al., 2019) to pro-\ncess MIMIC-III data. Harutyunyan et al. provides\nmethods to calculate patient age information, and we\nwrote regular expressions to extract age information\nfrom the Diabetes corpus. We keep clinical records\nof adults (≥18) in both MIMIC-III and Diabetes\ndatasets. The Diabetes data aggregates discharge sum-\nmaries of each patient into one ﬁle, and we split indi-\nvidual discharge summaries by its record separator. To\nkeep consistent, we use discharge summaries of both\ndata. We implement our models and re-implement\nbaselines using PyTorch (Paszke et al., 2019), Hugging-\nface Transformers (Wolf et al., 2020), scikit-learn (Pe-\ndregosa et al., 2011) and NLTK (Bird and Loper,\n2004). For the CAUE-BERT, we experiment with\nmultiple pre-trained BERT models (Peng et al., 2019;\nAlsentzer et al., 2019; Lee et al., 2019). However, we\ndid not ﬁnd signiﬁcant improvements with the diﬀer-\nent choices. Initially, we trained models of Diabetes\nand MIMIC-III datasets on a NVIDIA GeForce RTX\n3090, and conducted evaluation experiments on CPUs.\nFor the baselines, we train user2vec on the GPU and\nother baselines on CPUs. We also experimented dif-\nferent learning rates within [1e−4, 1e−6] and trained\nmodels on the university HPC cluster with NVIDIA\nTesla V100s. We then conducted the ablation analysis\nand reported results of the experiments.\n15\nEnriching Unsupervised User Embedding via Medical Concepts\nCAUE\nPhenotype Prediction\nMortality\nPatient Relatedness\nRetrieval\nDiabetes\nMIMIC-III\nMIMIC-III\nDiabetes\nMIMIC-III\nDiabetes\nMIMIC-III\nGRU\nNone\n0.573\n0.191\n0.658\n0.364\n0.231\n0.359\n0.119\n+CT\n0.587\n0.284\n0.691\n0.334\n0.226\n0.363\n0.106\n+CC\n0.618\n0.348\n0.814\n0.319\n0.207\n0.377\n0.131\n+CC+CT\n0.635\n0.369\n0.817\n0.292\n0.205\n0.386\n0.139\nBERT\nNone\n0.545\n0.197\n0.648\n0.517\n0.262\n0.322\n0.101\n+CT\n0.569\n0.198\n0.660\n0.503\n0.244\n0.351\n0.103\n+CC\n0.586\n0.242\n0.692\n0.419\n0.250\n0.382\n0.126\n+CC+CT\n0.593\n0.339\n0.792\n0.337\n0.235\n0.396\n0.142\nTable 4: Ablation study performance. None indicates no contrastive learning (CT) and medical concept\n(CC). The + means adding the training component. While lower scores for the patient relatedness\nare better, higher scores for the other tasks are better.\nAppendix C. Exploratory Analysis of\nMedical Concepts\nPhenotype inference and patient retrieval are essential\ntasks for diagnosis and knowledge discovery. Learning\npatient representations is critical for building robust\nmodels. Medical concepts composed of one or more\ntokens in various combinations can indicate diverse se-\nmantic meanings than individual tokens. For example,\nseparations of “bid” and “protein” can have diﬀerent\nsemantic meanings than a combination of them, which\nis one type of proteins during cell death receptormedi-\nated apoptosis. Medical concepts can correlate closely\nwith medical annotations. For example, “drinker” in-\ndicates people who drink alcohol, while “heavy drinker”\nidentiﬁed in Diabetes data frequently co-occurs with\nthe cohort annotation, “alcohol-abuse”. This section\naims to test how the extracted concepts in our par-\nticular datasets can contribute to user modeling and\nhow strong the eﬀects are.\nDataset\nFeature\nCoeﬃcient\nDiabetes\nn-gram\n-0.037\nconcept\n0.388*\nMIMIC-III\nn-gram\n0.405*\nconcept\n0.146*\nTable 5: Multivariate linear regression analysis be-\ntween feature and label similarities.\nThe\nvariables are two features n-gram and con-\ncept. * indicates that p-vale < 0.05.\nTo answer the question, we conduct a patient re-\ntrieval task by treating each patient as a query and\nretrieve similar patients. We deﬁne the similarity be-\ntween every two patients by calculating a cosine score\nbetween their phenotype labels. We assume that if\ntwo patients are similar, their phenotype annotations\nshould be similar. Therefore, if document features\ncan accurately describe patients, the feature similar-\nity of two patients should be proportional to their\nannotation similarity.\nWe examine the eﬀects of concept features by a\nmultivariate linear regression to examine relations\nbetween patient annotation and the two features types.\nTo represent patient features, we derive and combine\ntwo feature types, n-gram and concept. We extract\nuni-, bi-, and tri-gram features and concept features\nthat are normalized by TF-IDF. We calculate feature\nand medical annotation (ICD-9 codes) similarities\nbetween every two patients. Then, we can compare\nthe two feature types by the linear regression between\nfeature and medical annotation similarities.\nWe show the regression analysis results in Table 5.\nIn both datasets (Diabetes and MIMIC-III), concept\nfeatures show a signiﬁcant correlation with medical\nannotations, while the n-gram features fail the sig-\nniﬁcance test on the Diabetes data. The failed test\nveriﬁes the null hypothesis that there is no linear\ncorrelation between n-gram feature and medical an-\nnotation. The results also indicate that incorporating\nmedical concepts could help retrieve similar patients.\nConcept features show a positive correlation with the\nmedical annotation similarity. The positive correla-\ntion means that as the annotation similarity of two\npatients increases, the concept feature similarity also\nintends to increase. Those observations suggest there\nis a strong linear connection between medical concept\nand phenotype.\n16\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2022-03-20",
  "updated": "2022-03-29"
}