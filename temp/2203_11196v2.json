{
  "id": "http://arxiv.org/abs/2203.11196v2",
  "title": "Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series",
  "authors": [
    "Martín Solís",
    "Luis-Alexander Calvo-Valverde"
  ],
  "abstract": "Deep Learning and transfer learning models are being used to generate time\nseries forecasts; however, there is scarce evidence about their performance\nprediction that it is more evident for monthly time series. The purpose of this\npaper is to compare Deep Learning models with transfer learning and without\ntransfer learning and other traditional methods used for monthly forecasts to\nanswer three questions about the suitability of Deep Learning and Transfer\nLearning to generate predictions of time series. Time series of M4 and M3\ncompetitions were used for the experiments. The results suggest that deep\nlearning models based on TCN, LSTM, and CNN with transfer learning tend to\nsurpass the performance prediction of other traditional methods. On the other\nhand, TCN and LSTM, trained directly on the target time series, got similar or\nbetter performance than traditional methods for some forecast horizons.",
  "text": " \n \n \nInteligencia Artificial 25(70), 110-125 \ndoi: 10.4114/intartif.vol25iss70pp110-125 \n \nISSN: 1137-3601 (print), 1988-3064 (on-line) \n©IBERAMIA and the authors \nINTELIGENCIA ARTIFICIAL \n \nhttp://journal.iberamia.org/ \n \n \n \n \n \nPerformance of Deep learning models with transfer learning for multiple-step-ahead \nforecasts in monthly time series \n \nMartín Solís[1], Luis-Alexander Calvo-Valverde[2],      \n \n[1] Tecnológico de Costa Rica, Cartago, 159-7050, Costa Rica, marsolis@itcr.ac.cr \n[2] Tecnológico de Costa Rica, Computer Science School, Cartago, 159-7050, Costa Rica, lcalvo@itcr.ac.cr \n \nAbstract Deep learning and transfer learning models are being used to generate time series forecasts; however, \nthere is scarce evidence about their performance prediction mainly for monthly time series. The purpose of this \npaper is to compare deep learning    models with transfer learning and without transfer learning and other traditional \nmethods used for monthly forecasts to answer three questions about the suitability of deep learning and transfer \nlearning to generate predictions of time series. Time series of M4 and M3 competitions were used for the \nexperiments. The results suggest that deep learning models based on tcn, lstm, and cnn with transfer learning tend \nto surpass the performance prediction of other traditional methods. On the other hand, tcn and lstm, trained directly \non the target time series, got similar or better performance than traditional methods for some forecast horizons. \n \nResumen Los modelos de aprendizaje profundo y aprendizaje por transferencia se están utilizando para generar \npronósticos de series temporales; sin embargo, existe escasa evidencia sobre su desempeño predictivo \nprincipalmente en series mensuales. El propósito de este artículo es comparar modelos de aprendizaje profundo con \naprendizaje de transferencia y sin aprendizaje de transferencia y otros métodos tradicionales utilizados para \npronósticos mensuales para responder tres preguntas sobre la idoneidad del aprendizaje por transferencia para \ngenerar predicciones de series temporales. Se utilizaron series temporales de las competiciones M4 y M3 para los \nexperimentos. Los resultados sugieren que los modelos de aprendizaje profundo basados en tcn, lstm y cnn con \naprendizaje por transferencia tienden a superar la predicción de rendimiento de otros métodos tradicionales. Por \notro lado, tcn y lstm, entrenados directamente en la serie temporal objetivo, obtuvieron un rendimiento similar o \nmejor que los métodos tradicionales para algunos horizontes de pronóstico. \n \nKeywords: Deep learning, Time series, Transfer learning, Machine learning, Forescast  \n \n1 Introduction \nAn increasing number of deep learning    models for time series forecasting have been published in conferences \nand journals in recent years [1]. These models have been successful for predictions in different areas [2] and, in \nsome cases, yielded a superior predicted performance than traditional techniques [3]. For example, Zeroual et al [4] \nshowed the promising potential of the deep learning    model in forecasting COVID-19. Mariano-Hernández [5] \nfound that deep learning    models were the best to predict energy consumption in one of the buildings analyzed in \ntheir research. Hewage et al [6] showed that deep learning    models could be better than the classic machine learning \nand statistical approaches to predict weather conditions. \nTransfer learning is a technique used successfully in computer vision and NLP to extract the knowledge from \na source domain and used in learning a model on a target domain [7]. Although transfer learning has not been widely \nused for deep learning    models of time series [8], its application has emerged recently. For example, some authors \nhave used transfer learning to deal with the scarcity of labeled time-series of data in classification cases [e.g 9,10]. \nQi-Qiao et al [8] used transfer learning to make predictions with financial time series extracted from stock markets. \nOtovi et al [11] and Poghosyan et al [12] proved that a pre-trained model on a global time-series database could be \nused with transfer learning to get good predictions of other times series that even come from a different domain.  \n \n \nInteligencia Artificial 70 (2022)  \n \n111 \n \n \nDue to the relevance that deep learning    and transfer learning models have acquired for time series forecasting, \nthere is a need to benchmark them against more traditional methods [2]. Performance comparisons have been made \nusing statistical forecasting methods and classical machine learning methods [13, 14 15, 16]. However, comparisons \nand analyses about the predictive performance between those methods and deep learning    models are scarce. This \nresearch aims to contribute to the aforementioned knowledge gap, answering the following questions. \n \n1. Are the deep learning    models for multiple-step-ahead forecasts in monthly time series more effective in \nterms of performance prediction than traditional methods? \n2. How does the performance of the deep learning    models change according to the forecast horizon \ncompared to the traditional models? \n3. Are there groups of time series where deep learning    methods and the application of transfer learning are \nmore effective? \n \nWe hope that this work provides valuable information about the suitability of deep learning    models and the \nusage of pre-trained models to make predictions with transfer learning on new target time series. The main \ncontributions of this paper are as follows: \n \n1. We answered three questions about the effectiveness of deep learning    models for monthly forecasts not \nresponded yet. \n2. Although there are benchmarking about the time series methods for forecasting, our work is the first, as \nfar as we know, to compare and analyze the performance of the deep learning    models with and without \ntransfer learning with other more traditional machine learning algorithms and statistical methods. \n3. The performance of deep learning models with transfer learning for monthly forecasts is analyzed \nthroughout relevant characteristics of time series. Therefore, this work can be used as a reference point \nabout the suitability of applying deep learning models and transfer learning for monthly forecast tasks. \n2 Related Work \n2.1 \nDeep learning with transfer learning in time series  \nLaptev, Yu, and Rajagopal [17] transfer time-series features across diverse domains and mentioned that there \nis no prior work related to the application of transfer learning for time-series. From 2018 some studies have emerged \nwith the application of transfer learning to deep learning models to generate the forecast for a target time series. \nThose models have been applied to different tasks as time series classification tasks [e.g. 9, 10, 18], time series \nregression tasks [11, 12] and anomy detection in time series [19, 20]. \nIn the case of regression tasks (the scope of this research), the transfer learning has been used in different areas. \nIn finance, Qi-Qiao et al [8] evaluate the effectiveness of transfer learning for stock price prediction using a two-\nlayer neuronal network and two-layer lstm. Xu y Meng [21] developed a novel hybrid transfer learning model for \nenergy consumption forecasting based on time series decomposition. Le et al, [22] were concerned with the \ncomputational time to train several models to predict the energy consumption of the apartments at the same building; \ntherefore, they developed a framework for multiple electric energy consumption forecasting in smart buildings based \non transfer learning. Due to the problem of developing predictive models with limited data for energy load and \npower generation, [23] propose a transfer learning strategy using a convolutional neuronal network. Karb et al, [24] \nwere also concerned about making predictions with limited time series, but in the domain of food sales of new \nproducts. They propose a network-based transfer learning approach for deep neural networks to create effective \npredictive models. \nFor crude oil price forecasting, Xiao et al, [25] generate a hybrid transfer learning-based analog complexing \nmodel (HTLM). Otovic et al [11] analyzed if knowledge transfer between related domains could be more beneficial \nthan knowledge transfer between unrelated domains in classification and regression time-series tasks. They used \ndatasets from diverse areas such as seismic datasets, acoustic signals, medical datasets, and stock-market prices. \nXin and Peng [26] combines autoencoders, convolutional neural networks (AE-CNN), and transfer learning to \ncapture the intrinsic certainty of chaotic time series. The authors of all previous studies demonstrate that the models \nwere better than the baselines. In some cases, the comparison was with other deep learning, and in others, cases \nwere statistical models as arima. The conclusions often highlight the promising of transfer learning for time series.  \n \n \n112 \n \nInteligencia Artificial XX (XXXX) \n \n \n \n2.2 \nPerformance Comparisons between forecast methods \nSome comparisons have recently been made between the performance of various algorithms and methods for \ntime series forecasting. One of the most cited is the work of [16]. Those authors compare the performance of seven \nstatistical models and ten machine learning models using the M3 competition. Due to high computational time, they \napplied deep learning  models such as the lstm and rnn, but without transfer learning and based on very simple \narchitectures. They conclude that traditional statistical methods are more accurate than machine learning ones. \nPapacharalampous et al [15] also extensively compare several stochastic and ML techniques to forecast hydrological \nprocesses. The machine learning models used were: \n Three simple neuronal networks (Single hidden layer, Multilayer, and Perceptron (MLP)). \n Three random forest. \n Three support vector machines. \n \nDifferent to [16], they conclude that the stochastic and ML methods can share a quite similar performance when \nforecasting hydrological time series of small length, but in linear situations, the ML methods are more likely to be \ninferior, while in non-linear situations, the ML methods are more likely to outperform.  \nParmezan et al [14] provide a comparison between popular statistical methods and machine learning models \n(support vector machines (svm), kNN-TSPI, Multilayer and Perceptron (mlp), and lstm), using synthetic and real \ntimes series of different domains. They conclude that SARIMA is the best for deterministic series, kNN-TSPI and \nsarima are the best for Stochastic, and svm was the more stable method for chaotic series. Catal et al [13] also \ncompared machine learning models (linear regression, bayesian regression, neural network regression, decision \nforest regression, and boosted decision tree regression) and statistical models using the Walmart sales dataset. They \nfound that boosted decision tree regression algorithm was the best predictor. \nOn the other hand, Lara-Benitez et al, [27] generate an experimental study comparing the performance of the \nmost popular deep learning architectures for time series prediction using different datasets. In this study, they didn’t \ncompare with other machine learning algorithms or statistical methods nor analyzed the effect of transfer learning \non the performance prediction. lstm, cnn, and tcn were between networks with the best results. \n3 Material and methods \n3.1 \nDatasets \nThe time series used comes from the M3 [39] and M4 competitions [40]. Two sets called A and B, of 1000 time \nseries, were selected randomly from the Monthly M4 set, which contains 48 000 time series, and a third set named \nB_M3 were composed by all-time series of monthly M3 competition. Every time-series taken from the M4 \ncompetition includes the train and test subset. The testing subset of M4 competition for each time series in \nthe A set was used to calculate the models' performance metrics. This subset is made up of 18 months. \nAs the research questions require the analysis of multiple-step-ahead forecasts (forecast horizon) performance \nwith different steps, we pre-process the 18 months to make predictions of one step ahead, three, six, and twelve \nsteps ahead. For example, for three steps ahead, the 18 test months were transformed into sixteen instances of \nprediction (table 1). \n \nTable 1. Testing dataset structure for \noutput, using Three steps ahead \n \ninstance \nMonth \nStep1 \nStep2 \nStep3 \n1 \n1 \n2 \n3 \n2 \n2 \n3 \n4 \n3 \n3 \n4 \n5 \n……………………………………… \n……………………………………… \n……………………………………… \n \n \nInteligencia Artificial 70 (2022)  \n \n113 \n \n \n……………………………………… \n15 \n15 \n16 \n17 \n16 \n16 \n17 \n18 \n \n3.2 \nExperimental procedure \nThe machine learning and deep learning    models were trained using different input sizes for each forecast horizon \n(time steps ahead). For 3, 6, and 12 steps ahead, we applied a window input size of: \n \n the same size as the forecast horizon. This input size was chosen because Shynkevich et al [28] \nfound that the highest prediction performance is achieved when the input window length is \napproximately equal to the horizon forecast. \n 1.25 times of the forecast horizon size. This input size was chosen because Lara-Benitez et al [27] \nfound better performance with 1.25 times the forecast horizon size than larger output horizons. \n 12 months. This input size was chosen to incorporate stational information into the models. \nThe procedure for training and testing each of the methods used is explained below: \n \n3.2.1 \nDeep learning models with transfer learning  \nFigure 1 describes the process. Source set was used to develop the models1 and set A to make the transfer \nlearning and get the predictions of the test sample2. There were two kinds of source datasets: B set (1000 time series \nselected randomly from M4 competition) and B_M3 set (monthly time series of M3 competition). The process \nexecuted was the next. First, the input window size and output horizon size were selected for the pre-processing of \neach time series in every set. Then the data is divided into training, validation, and testing samples. The train and \nvalidation samples of the all-time series in the source set were concatenated to train the models and select the hyper-\nparameters and architectures using Bayesian optimization. The grid of search for the Bayesian optimization and the \nmost frequent solution (between all models generated for each combination of input-output) is in table 2. \n \nEach model has been trained using the Adam optimizer and a stop criterion which consists of stopping after \ntwo epochs without improvement in the validation sample's loss function. The loss function was the mean absolute \npercentage error, and the batch size was equal to one. After the model has been generated, transfer learning is applied \nfor every time series of set A (which is the target dataset).  For the transfer learning we executed the next steps with \neach time series in dataset A: \n \n1. The training sample was used to update the weights of the last layer in the trained models with a learning \nrate of 0.000005. This learning rate is smaller than the one used to build the models. The rest of the \nweights were frozen.  \n2. After two epochs without improvement in the validation sample's loss function, the updating process \nfinishes. \n3. Finally, the predictions were generated with the model updated and performance metrics were computed \nusing the test sample. \n \nWe generated two versions of the deep learning models that were applied with transfer learning. In the first \nversion, we trained the cnn, lstm, and tcn using the B set, and in the second version, we trained these networks using \nthe B_M3 set. The purpose of the two versions was to assess how transfer learning worked when the models had \nbeen trained from different datasets. \n                                                           \n1 Due to the Bayesian optimization process, the network architecture changes between the models generated for the different input-output \ncombinations. In the next repository (https://github.com/martin12cr/DatasetsDeepTransfer.) is the code of the base architectures from which the \nfinal models could change.     \n2 The datasets are available in the next repository  (https://github.com/martin12cr/DatasetsDeepTransfer). The test set for each time series in \ndataset A is composed by the last 18 points.  \n \n \n114 \n \nInteligencia Artificial XX (XXXX) \n \n \n \n \nFigure 1. Procedure to train and test deep learning models with transfer learning \n \n \n \nTable 2. Grid of search for the Bayesian optimization of deep learning models and most frequent solution between \nthe models \ncnn \ntcn \nlstm \n \nsearch grid \nMost \nfrequent \nsolution \n \nsearch grid \nMost frequent \nsolution \n \nsearch grid \nMost \nfrequent \nsolution \n Number of hidden \nlayers between 1 \nand \n2 \n(Batch \nnormalization \nis \napplied after the \nfirst layer) \n \n \nFilters between 12 \nand 132 with a \nstep of 24 \n \n \nKernel \nsize \nbetween 2 and 12 \nwith a step of 2 \n \n2 \n \n \n \n \n \n \n12, \nboth \nlayers)  \n \n \n12 (first) \n2 and 12 \n(second) \n \n \n \nFilters between 12 \nand 132 with a \nstep of 24 \n \n \nKernel \nsize \nbetween 2 and 12 \nwith a step of 2 \n \n \nActivation \nfunction \namong \nlinear, relu, and \ntanh \n \n \nReturn sequences \nTrue or False \n \n12 \n \n \n \n \n12 \n \n \n \nTanh \n \n \n \nFalse \n \n \n \nRecurrent \nunits \nbetween 12 and \n132 with a step of \n24 \n \n \nActivation \nfunction \namong \nlinear, relu, and \ntanh \n \n \nReturn sequences \nTrue or False \n \n \nLearning \nrate \namong \n0.001, \n0.0001,0.00001 \n84 \n \n \n \n \n \nRelu \n \n \n \nTrue \n \n \n0.001 \n \n \nInteligencia Artificial 70 (2022)  \n \n115 \n \n \n \nMax pooling with \na size of 2 or \nwithout \nmax \npooling   \n \nActivation \nfunction \namong \nlinear, relu, and \ntanh \n \nLearning \nrate \namong \n0.001, \n0.0001,0.00001  \n \nWhithout \n \n \n \nTanh \n \n \n \n \n0.0001 \n \nLearning \nrate \namong \n0.001, \n0.0001,0.00001 \n  \n \nDilations between \n[1,2,4,8] \nor \n[1,2,4,8,16] \n \n0.001 \n \n \n \n[1,2,4,8,16] \n \n \n \n3.2.2 \nMachine learning and deep learning without transfer learning \nFigure 2 describes the process. These models were developed with the training and validation samples of set A and \nevaluated with the test sample. The Bayesian optimization was used to select the hyperparameters of the models \nand the architectures of the deep learning    models. The grid of search is in table 2 for deep learning  models, and \nin table 3 for machine Learning models. When the model has been trained for each time series, the predictions and \nperformance metrics were generated using the test dataset.  \n \n \nFigure 2. Procedure to train and test Machine Learning models \n \nTable 3. Grid of search for the Bayesian optimization of machine learning models and most frequent solution \nbetween the models \n \nxgboost \nsupport vector machines \n \nrandom forest \n \nsearch grid \nMost \nfrequent \nsolution \nsearch grid \nMost \nfrequent \nsolution \n \nsearch grid \nMost \nfrequent \nsolution \n \nMax depth between 2 \nand 12 \n \n4 \n \n0.4-0.5 \n \nC between 0.01 \nand 10  \n \n9.97-10 \n \n \nestimators \nbetween 10 and \n250 \n \n110-135 \n \n \n \n \n116 \n \nInteligencia Artificial XX (XXXX) \n \n \n \n \nLearning \nrate \nbetween 0.01 and 1 \n \n \nestimators \nbetween \n10 and 150 \n \n \n \n \n70-79 \n \nGamma between \n0.001 and 0.33 \n \n \nKernel=rvf \n \n0.0001 -\n0.05 \n \n \n \nmax_features \nbetween 1 and \n15] \n \n \nmin sample leaf \nbetween 1 and 8 \n \n \nmas \nsamples \nbetween \n0.70 \nand 0.99 \n \n \n2 \n \n \n \n1 \n \n \n \n0.70-0.75 \n \n \n \n \n3.2.3 \nStatistical models \nThe methods used were auto arima, ets, theta. The training and testing process is similar to figure 2 but without \nBayesian Optimization component, because these statistical methods use other techniques to define the model \nfunction. \n \n3.3 \nPerformance metrics  \nIn order to evaluate the performance of the models, two metrics were estimated using the test sample, mape and \nsMape. We chose these metrics because they allow the performance comparison between time series independently \nof the scale. The time series were normalized only for the deep learning    models, but the metrics were estimated \non the original scale after converting the predictions to the original scale. \n \nMape =  \n1\n𝑛𝑛∑\nቚ\n𝑦𝑦𝑡𝑡−𝑦𝑦ො𝑡𝑡\n𝑦𝑦𝑡𝑡ቚ\n𝑛𝑛\n𝑡𝑡=1\n                        sMape=  \n1\n𝑛𝑛∑\nቚ\n𝑦𝑦𝑡𝑡−𝑦𝑦ො𝑡𝑡\n|𝑦𝑦𝑡𝑡|+|𝑦𝑦ො𝑡𝑡|ቚ\n𝑛𝑛\n𝑡𝑡=1\n \n \n3.4 Software  \nThe models and the experimental procedure were developed in python 3.6. The deep learning models were \ndeveloped and trained with keras, the machine learning models were trained with scikit-learn and the statistical \nmodels were generated with sktime. The bayes_opt library was used for the optimization process of the machine \nlearning models and keras library for the optimization process of the deep learning models. \n \n4 Results \n \n4.1 \n4.1. Best input size \n \nTable 4 shows the relative distribution of the best input size for the all-time series in the target dataset according to \nthe type of model and output horizon. The deep learning models with transfer learning tend to get the best results \nwith an input of 12 than deep learning models without transfer learning and machine learning models, for outputs \nhorizons of 1,3 and 6. For an output of 12, the best option in most time series was an input of 1.25 times the size of \nthe output horizon, in deep learning and machine learning models. These results suggest that the best input depends \non the time series and that there is a tendency for some types of models and output horizons to get the best results \nwith higher inputs.   \n \n \nInteligencia Artificial 70 (2022)  \n \n117 \n \n \n \nTable 4. Relative distribution of the best input size according to output horizon and type of model.  \nOutput \nInput \ndeep learning_M4 \ndeep learning_M3 \ndeep learning sin transfer \nmachine learning \n1 \n3 \n41.9% \n28.8% \n59.3% \n55.1% \n12 \n58.1% \n71.2% \n40.7% \n44.9% \nTotal \n100.0% \n100.0% \n100.0% \n100.0% \n3 \n3 \n32.0% \n19.1% \n35.3% \n31.0% \n4 \n24.2% \n23.2% \n31.8% \n32.9% \n12 \n43.7% \n57.8% \n32.9% \n36.1% \nTotal \n100.0% \n100.0% \n100.0% \n100.0% \n6 \n6 \n25.2% \n25.0% \n34.1% \n42.8% \n8 \n22.9% \n19.6% \n33.7% \n23.1% \n12 \n51.9% \n55.3% \n32.2% \n34.2% \nTotal \n100.0% \n100.0% \n100.0% \n100.0% \n12 \n12 \n45.1% \n38.0% \n52.7% \n44.5% \n15 \n54.9% \n62.0% \n47.3% \n55.5% \nTotal \n100.0% \n100.0% \n100.0% \n100.0% \n \n \n \n \n4.2 \nPerformance by forecast horizon  \nTable 5 shows the performance for each model and forecast horizon, based on mape and sMape. The machine \nlearning models and deep learning    models were trained with different input window sizes for each output horizon; \nhowever, we selected the models with the input size that let to obtain the best performance.  \nAccording to the results, the statistical models have more stable performance through the forecast horizons; \nmeanwhile, the machine learning models show an increment as the forecast horizon increases. The deep learning    \nmodels with transfer learning tend to show the better performance at the mape and sMape in each forecast horizon, \nbut to determine if there are significant statistical differences between models, the CD Diagrams were generated, \nfollowing the procedure of the package autorank from python [29]. \nIn the CD diagram, the vertical lines show the average rank of each model compared to the others. This average \nis calculated based on the performance metric used for the test sample. The horizontal line represents the critical \ndifference for the comparison between models. When the average distance between models is greater than the \ncritical distance, there is a statistically significant difference in the performance using a p-value =0.05. Therefore, \nwhen the horizontal line intersects the vertical lines of the average rankings, there is no difference between models. \n \n \n \nTable 5. Mape and sMape on test by type of model and forecast horizon \n \nMape \nsMape \n \n \n118 \n \nInteligencia Artificial XX (XXXX) \n \n \n \nModel  \nForecast horizon \n1 \n3 \n6 \n12 \narima \n0.199 \n0.192 \n0.195 \n0.206 \nets \n0.167 \n0.17 \n0.169 \n0.169 \ntheta \n0.159 \n0.16 \n0.159 \n0.156 \nrf \n0.136 \n0.13 \n0.154 \n0.238 \nsvm \n0.136 \n0.129 \n0.148 \n0.241 \nxgb \n0.136 \n0.129 \n0.148 \n0.254 \ncnn_wot \n0.138 \n0.155 \n0.179 \n0.219 \nlstm_wot \n0.107 \n0.115 \n0.136 \n0.167 \ntcn_wot \n0.106 \n0.123 \n0.155 \n0.2 \ncnn \n0.093 \n0.112 \n0.123 \n0.14 \nlstm \n0.091 \n0.097 \n0.111 \n0.131 \ntcn \n0.087 \n0.098 \n0.111 \n0.131 \ncnn2 \n0.094 \n0.1 \n0.115 \n0.134 \nlstm2 \n0.09 \n0.101 \n0.114 \n0.133 \ntcn2 \n0.088 \n0.101 \n0.116 \n0.139 \n \nModel \nForecast horizon \n1 \n3 \n6 \n12 \narima \n0.073 \n0.073 \n0.074 \n0.077 \nets \n0.064 \n0.066 \n0.066 \n0.065 \ntheta \n0.062 \n0.063 \n0.063 \n0.062 \nrf \n0.057 \n0.054 \n0.064 \n0.107 \nsvm \n0.057 \n0.054 \n0.062 \n0.107 \nxgb \n0.056 \n0.054 \n0.062 \n0.108 \ncnn_wot \n0.053 \n0.07 \n0.08 \n0.093 \nlstm_wot \n0.045 \n0.05 \n0.057 \n0.069 \ntcn_wot \n0.044 \n0.053 \n0.065 \n0.086 \ncnn \n0.038 \n0.048 \n0.051 \n0.056 \nlstm \n0.037 \n0.04 \n0.045 \n0.052 \ntcn \n0.036 \n0.041 \n0.045 \n0.052 \ncnn2 \n0.039 \n0.042 \n0.047 \n0.053 \nlstm2 \n0.037 \n0.042 \n0.046 \n0.053 \ntcn2 \n0.036 \n0.042 \n0.047 \n0.055 \nrf: Random forest, svm: Support vector machines, xgb: Xgboost, cnn_wot=Convolutional neuronal \nnetwork without transfer learning, lstm_wot: Long short term memory without transfer learning, tcn_wot: \nTemporal convolutional network without transfer learning, cnn: Cnn with transfer learning, lstm: Lstm with \ntransfer learning, tcn: Tcn with transfer learning: Cnn with transfer learning trained with M4 dataset, lstm: \nLstm with transfer learning trained with M4 dataset, tcn: Tcn with transfer learning trained with M4 dataset, \ntcn2: Tcn with transfer learning trained with M3 dataset, lstm2: Lstm with transfer learning trained with \nM3 dataset, cnn2: Cnn with transfer learning trained with M3 dataset.  \n \n \n \nFigure 4 shows the sMape CD diagrams for each forecast horizon, and figure 5  the mape CD diagrams. The results \nare similar in both graphs. Based on the hypothesis test, the best models when the forecast horizons are 1 and 3 \nmonths were the tcn with transfer learning and lstm with transfer learning. The first and second place depends on \nwhether we look at the mape o sMape figure.  According to the statistical test, the best model for a forecast horizon \nof one and three is tcn and the best model for a forecast horizon of six and twelve is lstm. The second position tends \nto belong to lstm1 and lstm2 or tcn1 and tcn2. According to the statistical test, the second position is shared by two \nor more methods in two graphs. For a forecast horizon of six in figure 4, there is no statistical difference between \ntcn and lstm2 and for a forecast horizon of one in figure 4, there is no statistical difference between lstm, tcn2, lstm2, \ncnn, cnn2, and tcn_wot.  Another finding from the statistical test is that the last position tends to belong to cnn_wot \nor arima. \n \nWhen the forecast horizon is larger, the lstm with transfer learning was first, and tcn or lstm trained with M3 \ndataset was second. The CNN and tcn without transfer learning tend to be the worst option for a larger horizon. \nInterestingly, the deep learning    models with transfer learning but trained with a different dataset show good results \nbecause they were located in the second subset of the best models in each forecast horizon. The M3 monthly dataset \nused to train these models is not very different from the monthly M4 dataset however shows some differences; for \nexample, the M3 time series are less forecastable, trended, and linear [30]. This finding suggests that it is possible to \nget good results when the training dataset has differences from the test dataset while these differences are not large.   \nSome deep learning    models without transfer learning were in the third group of best performance when the forecast \nhorizon is less than 12, despite the exposition to overfitting, because of monthly time series with few data points and \nmodels with more parameters. Finally, the machine learning models (not deep learning models) were neither the \nthird-best group nor the worst group. \n \n \n \n \n \n \n \n \nInteligencia Artificial 70 (2022)  \n \n119 \n \n \n \n \n \nF.horizon= 1 \n \n \nF.horizon= 3 \n \nF.horizon= 6 \n \nF.horizon= 12 \n \n \n \n \n \nFigure 4. sMape CD diagrams for each forecast horizon. \n \n \n \nF.horizon= 1 \n \n \nF.horizon= 3 \n \n \n \n120 \n \nInteligencia Artificial XX (XXXX) \n \n \n \nF.horizon= 6 \n \n \n \nF.horizon= 12 \n \n \nFigure 5. Mape CD diagrams for each forecast horizon. \n \n \n4.3 \nPerformance by type of time series \nIn order to analyze the performance of the models by the behavior of the time series, we estimated eight features \nfor each time series, and later we applied the PAM cluster to classify the time series according to their behavior. \nThe features calculated were: \n1. Forecastability measure with entropy [31]: Larger values occur when a time series is difficult to forecast. \n2. Seasonal [31]: Larger values means more seasonal strength \n3. Linearity/nonlinearity [31]: Takes large values when the series is non-linear and around 0 when the time \nseries is linear. \n4. Skewness [31]: negative skew commonly indicates negative asymmetry distribution, positive skew \nindicates positive asymmetry distribution, and values close to zero indicates symmetry distribution \n5. Kurtosis [31]: Lager values means more concentration around the mean \n6. White noise measure with box test [32]. Higher values of chi-square test mean not white noise \n7. Outliers: Proportion of outliers computed using [33] approach \n8. Stationarity was measured with the adf test of stationarity [34]. Higher p values indicate that the series is \nnot stationary \n \nBefore the PAM execution, the features were standardized. The silhouette score and calinski harabasz score \nwere higher with two cluster; however, we decided to generate four to have more cluster diversity. Based on the \nmeans features of each cluster, we determine that the first and second clusters are composed for time series more \nunpredictable (see the entropy and white noise metrics), but in the second group, the time series tend to be more \nnon-linear, asymmetric and with more concentration of data point around the mean. Clusters 3 and 4 are more \npredictable, but in 3, the time series tend to have more seasonal strength, while in group 4 tend to be more stationary. \nAnother difference is that in 4, the time series tend to be more non-linear, with more outliers. At the bottom of table \n2, we include brief names for each cluster based on the previous analysis. \n \nTable 6. Features means by cluster \nFeatures \ncluster1 \ncluster2 \ncluster3 \ncluster4 \nentropy \n0.614 \n0.598 \n0.227 \n0.231 \nseason \n0.634 \n0.214 \n0.653 \n0.252 \nskewness \n0.059 \n0.670 \n0.058 \n0.389 \nkurtosis \n0.159 \n1.704 \n-0.761 \n-0.598 \nnon_linear \n0.338 \n0.666 \n0.235 \n0.676 \nwhite_noise \n732.5 \n793.1 \n3458.0 \n2008.8 \noutliers \n0.009 \n0.015 \n0.007 \n0.034 \nstacionarity \n0.519 \n0.401 \n0.382 \n0.646 \n \n \nInteligencia Artificial 70 (2022)  \n \n121 \n \n \n   Cluster 1: unpredictable -symmetric, Cluster 2: unpredictable-nonlinear  \n   with concentration around mean, Cluster 3: predictable-seasonable \n   Cluster 4: predictable - with stationary and nonlinear tendency \n                                                                                    \nFigure 6 and Figure 7 presents the CD diagrams by series cluster, using sMape and mape, respectively.   The \nmain finding in both figures is that the tcn and lstm with transfer learning were between the two best models.  \n \nAccording to the statistical differences, the best model for the cluster unpredictable -symmetric is tcn using the \nsMape and lstm2 using the mape. For unpredictable-nonlinear the best model is lstm, and in the case of predictable-\nseasonable and predictable - with stationary and nonlinear tendency the best models with both metrics are lstm and \ntcn, respectively. The statistical test algo suggest that the worst method is cnn_wot in five graphs and arima in three \ngraphs. There are several methods that don’t show statistical difference in positions three, four, five and six.  \n  \nAlso, the ranking of these figures shows that the deep learning models trained with a different dataset were \nfrequently between the third and five positions. These results confirm that the performance of the deep learning    \nmodels with transfer learning was between the best in different groups of series, that goes from the more predictable \nto the more unpredictable and noisy time series. The CNN without transfer learning was in the last position, and the \ntcn without transfer learning was not between the best seven models independently of the cluster. The lstm without \ntransfer learning got the best results, but in some clusters like the unpredictable-nonlinear with a concentration \naround mean and predictable - with stationary and nonlinear tendency were surpassed by theta or ets. \n \n \nunpredictable -symmetric \n \nunpredictable-nonlinear with concentration around \nmean \n \npredictable-seasonable \n \npredictable - with stationary and nonlinear \ntendency \nFigure 6. sMape CD diagrams for each cluster of series \n \n \n \n \n \n122 \n \nInteligencia Artificial XX (XXXX) \n \n \n \nunpredictable -symmetric \n \n \nunpredictable-nonlinear with concentration around \nmean \n \n \npredictable-seasonable  \n \n \npredictable - with stationary and nonlinear \ntendency \n \nFigure 7. Mape CD diagrams for each cluster of series \n \n \n5 Conclusions  \nThree research questions were posited in the introduction; they will be answered in this section. The first question \nwas: Are the deep learning    models for multiple-step-ahead forecasts in monthly time series more effective in terms \nof performance prediction than traditional models? \nBased on the mape and sMape metrics, the deep learning models tcn, lstm, and cnn with transfer learning tend to be \nmore effective than some more traditional models such as theta, ets, arima, random forest xgboost, and svm. Those \ndeep learning models were trained with a concatenated dataset of time series, and later, the transfer learning was \napplied to update the weights of the last layer for new time series. Also, the models trained with the M3 dataset \nshow the best performance than traditional methods. The monthly M3 dataset has some similarities with the monthly \nM4 dataset but is not entirely identical because the time series tends to be less forecastable, trended, and linear [30]. \nThis finding suggests that the deep learning    algorithms are a good option or maybe the best option to build forecast \nmodels if we make a dataset with time series that comes from a population that is the same or at least a little different \nfrom the population where our target time series comes from. \nOn the other hand, if we train the tcn, lstm, and cnn directly on the target time series, the results suggest that \nthe lstm and tcn could get similar or better performance than traditional methods, depending on the forecast horizon, \nwhile the cnn is not a good option. It is reasonable that the deep learning models have lower performance being \ntrained directly on the monthly target time series because this kind of series didn’t have a lot of data points, and \ndeep learning models require to estimate several weights, which could cause overfitting. \nThe second question was: How does the performance of the deep learning models change according to the \nforecast horizon compared to the traditional models? \nThe answer is that the statistical models are more stable in the performance if the forecast horizon increases, \nhowever the deep learning    models with transfer learning show a best average ranking position and less mape and \nsMape independently if the forecast horizon is 1, 3, 6, or 12. According to the forecast horizon, the best option is \nlstm or tcn; for example, the results suggest that if the goal is to predict 1 or 3 months, the best option is tcn, but for \nlarger horizons, the best is lstm. The cnn with transfer learning tends to have lower performance on any horizon. \nThe third question was: Are there groups of time series where deep learning    methods and the application of \ntransfer learning are more effective? \n \n \nInteligencia Artificial 70 (2022)  \n \n123 \n \n \nWe classify the time series in four groups: unpredictable -symmetric, unpredictable-nonlinear with a \nconcentration around mean, predictable-seasonable and predictable - with stationary and nonlinear tendency. Our \nresults suggest that the deep learning    models with transfer learning tend to be the best independently of the group; \nhowever, it doesn’t mean that they are always the best option. The best deep learning    model using transfer learning \nbelongs to the tcn or lstm model in the four groups.  \nIn relation with the deep learning    models trained directly on the target time series, the result suggests that the \nlstm tends to have better performance than other traditional methods when the time series is unpredictable -\nsymmetric and predictable-seasonable. In other groups, the ets or theta got better performance. \nFuture research lines \nIn this study, the performance of deep learning    models with transfer learning was evaluated on time series that \nbelong to a similar or slightly different population of the time series used to train the models. However, it is relevant \nto know how the transfer learning models would work as the distance between the features of the training and testing \ntime series are further apart.  \nClusters of time series have been generated to compare the performance of the algorithms by groups; \nhowever, the final goal should be to develop a meta-learning model that determines which algorithm would be the \nbest according to the features of each time series. Some efforts have been made on this research line [e.g, 35, 36, \n37, 38] without incorporating the deep learning models. \nLast, our study didn’t include the performance comparison of the deep learning models with hybrid models that \ncombine different algorithms. Several studies have shown that this kind of models perform well in various topics. \nIn the field of Natural Language Process, there are pre-trained transformer models that have been used \nsuccessfully to fine-tune a model on different tasks. For example, a pre-trained mask language model can be used \nto generate a model for text classification or text summarization. According to [41] “in time series there are limited \nworks on pre-trained transformers and existing studies mainly focus on time series classification”. Therefore, new \nstudies should generate and evaluate the performance of pre-trained transformers for time series forecasting to \ndetermine in what conditions the transformers are more effective than other kinds of models.  Additionally, it is \nrelevant to explore different architectures of transformers not only the vanilla transformer. In [41] the authors found \nthat different alterations to the vanilla transformer could bring good performance in time series forecasting. \n \nAcknowledgements  \nThis work was supported by Tecnológico de Costa Rica. \nReferences \n[1] Sezer, O. B., Gudelek, M. U., & Ozbayoglu, A. M. (2020). Financial time series forecasting  with Deep learning   \n : \nA \nsystematic \nliterature \nreview: \n2005–2019. \nApplied \nSoft \nComputing, \n90, \n106181. \ndoi:10.1016/j.asoc.2020.106181 \n[2] Sharma, A., & Jain, S. K. (2021). Deep learning    Approaches to Time Series Forecasting. R  cent Advances i\nn Time Series Forecasting, 91–97. doi:10.1201/9781003102281-6 \n[3] Gamboa, J. C. B. (2017). Deep learning    for time-series analysis. arXiv preprint  arXiv:1701.01887. \n[4] Zeroual, A., Harrou, F., Dairi, A., & Sun, Y. (2020). Deep learning    methods for forecasting COVID-19 time-\nSeries data: A Comparative study. Chaos, Solitons & Fractals, 140, 110121. doi:10.1016/j.chaos.2020.110121 \n[5] Mariano-Hernández, D., Hernández-Callejo, L., Solís, M., Zorita-Lamadrid, A., Duque-Perez, O., Gonzalez-\nMorales, L., & Santos-García, F. (2021). A Data-Driven Forecasting Strategy to Predict Continuous Hourly \nEnergy Demand in Smart Buildings. Applied Sciences, 11(17), 7886. doi:10.3390/app11177886 \n[6] Hewage, P., Behera, A., Trovati, M., Pereira, E., Ghahremani, M., Palmieri, F., & Liu, Y. (2020). Temporal \nconvolutional neural (TCN) network for an effective weather forecasting using time-series data from the local \nweather station. Soft Computing, 24(21), 16453–16482. doi:10.1007/s00500-020-04954-0 \n[7] Pan, S. J., & Yang, Q. (2009). A survey on transfer learning. IEEE Transactions on knowledge and data \nengineering, 22(10), 1345-1359. \n[8] Qi-Qiao, H., Cheong-Iao, P.,  Yain-Whar, S. (2019, August). Transfer learning for financial time series \nforecasting. In Pacific Rim International Conference on Artificial Intelligence (pp. 24-36). Springer, Cham. \n[9] Li, F., Shirahama, K., Nisar, M. A., Huang, X., & Grzegorzek, M. (2020). Deep Transfer learning for Time \nSeries Data Based on Sensor Modality Classification. Sensors, 20(15), 4271. doi:10.3390/s20154271 \n \n \n124 \n \nInteligencia Artificial XX (XXXX) \n \n \n \n[10] Gupta, P., Malhotra, P., Narwariya, J., Vig, L., & Shroff, G. (2019). Transfer learning for Clinical Time Series \nAnalysis Using Deep Neural Networks. Journal of Healthcare Informatics Research, 4(2), 112–137. \ndoi:10.1007/s41666-019-00062-3 \n[11] Otović, E., Njirjak, M., Jozinović, D., Mauša, G., Michelini, A., & S̆ tajduhar, I. (2022). Intra- domain and \ncross-domain transfer learning for time series data—How transferable are the features? Knowledge-Based \nSystems, 239, 107976. doi:10.1016/j.knosys.2021.107976 \n[12] Poghosyan, A., Harutyunyan, A., Grigoryan, N., Pang, C., Oganesyan, G., Ghazaryan, S., & Hovhannisyan, N. \n(2021). An Enterprise Time Series Forecasting System for Cloud Applications Using Transfer learning. \ndoi:10.20944/preprints202101.0326.v1 \n[13] Catal, C., Ece, K., Arslan, B., & Akbulut, A. (2019). Benchmarking of Regression Algorithms and Time Series \nAnalysis Techniques for Sales Forecasting. Balkan Journal of Electrical and Computer Engineering, 20–26. \ndoi:10.17694/bajece.494920 \n[14] Parmezan, A. R. S., Souza, V. M. A., & Batista, G. E. A. P. A. (2019). Evaluation of statistical  and machine \nlearning models for time series prediction: Identifying the state-of-the-art and the best conditions for the use \nof each model. Information Sciences, 484, 302–337. doi:10.1016/j.ins.2019.01.076 \n[15] Papacharalampous, G., Tyralis, H., & Koutsoyiannis, D. (2019). Comparison of stochastic and machine \nlearning methods for multi-step ahead forecasting of hydrological processes. Stochastic Environmental \nResearch and Risk Assessment, 33(2), 481–514. doi:10.1007/s00477-018-1638-6 \n[16] Makridakis, S., Spiliotis, E., & Assimakopoulos, V. (2018). Statistical and Machine Learning forecasting \nmethods: Concerns and ways forward. PLOS ONE, 13(3), e0194889. doi:10.1371/journal.pone.0194889 \n[17] Laptev, N., Yu, J., & Rajagopal, R. (2018, August). Reconstruction and regression loss for time-series transfer \nlearning. In Proceedings of the Special Interest Group on Knowledge Discovery and Data Mining (SIGKDD) \nand the 4th Workshop on the Mining and LEarning from Time Series (MiLeTS), London, UK (Vol. 20). \n[18] Nguyen, T.-T., & Yoon, S. (2019). A Novel Approach to Short-Term Stock Price Movement Prediction using \nTransfer learning. Applied Sciences, 9(22), 4745. doi:10.3390/app9224745 \n[19] Wen, T., & Keyes, R. (2019). Time series anomaly detection using convolutional neural networks and transfer \nlearning. arXiv preprint arXiv:1905.13628. \n[20] Xiong, P., Zhu, Y., Sun, Z., Cao, Z., Wang, M., Zheng, Y., … Que, Z. (2018). Application of Transfer learning \nin Continuous Time Series for Anomaly Detection in Commercial Aircraft Flight Data. 2018 IEEE \nInternational Conference on Smart Cloud (SmartCloud). doi:10.1109/smartcloud.2018.00011 \n[21] Xu, X., & Meng, Z. (2020). A hybrid transfer learning model for short-term electric load forecasting. Electrical \nEngineering, 102(3), 1371–1381. doi:10.1007/s00202-020-00930-x \n[22] Le, T., Vo, M. T., Kieu, T., Hwang, E., Rho, S., & Baik, S. W. (2020). Multiple Electric Energy Consumption \nForecasting Using a Cluster-Based Strategy for Transfer learning in Smart Building. Sensors, 20(9), 2668. \ndoi:10.3390/s20092668 \n[23] Hooshmand, A., & Sharma, R. (2019). Energy Predictive Models with Limited Data using Transfer learning. \nProceedings \nof \nthe \nTenth \nACM \nInternational \nConference \non \nFuture \nEnergy \nSystems. \ndoi:10.1145/3307772.3328284 \n[24] Karb, T., Kühl, N., Hirt, R., & Glivici-Cotruta, V. (2020). A network-based transfer learning approach to \nimprove sales forecasting of new products. arXiv preprint arXiv:2005.06978. \n[25] Xiao, J., Hu, Y., Xiao, Y., Xu, L., & Wang, S. (2017). A hybrid transfer learning model for crude oil price \nforecasting. Statistics and Its Interface, 10(1), 119–130. doi:10.4310/sii.2017.v10.n1.a11 \n[26] Xin, B., & Peng, W. (2020). Prediction for Chaotic Time Series-Based AE-CNN and Transfer learning. \nComplexity, 2020, 1–9. doi:10.1155/2020/2680480 \n[27] Lara-Benítez, P., Carranza-García, M., & Riquelme, J. C. (2021). An Experimental Review on Deep learning    \nArchitectures for Time Series Forecasting. International Journal of Neural Systems, 31(03), 2130001. \ndoi:10.1142/s0129065721300011 \n \n \nInteligencia Artificial 70 (2022)  \n \n125 \n \n \n[28] Shynkevich, Y., McGinnity, T. M., Coleman, S. A., Belatreche, A., & Li, Y. (2017). Forecasting price \nmovements using technical indicators: Investigating the impact of varying input window length. \nNeurocomputing, 264, 71–88. doi:10.1016/j.neucom.2016.11.095 \n[29] Herbold, S. (2020). Autorank: A Python package for automated ranking of classifiers. Journal of Open Source \nSoftware, 5(48), 2173. doi:10.21105/joss.02173 \n[30] Spiliotis, E., Kouloumos, A., Assimakopoulos, V., & Makridakis, S. (2020). Are forecasting competitions data \nrepresentative \nof \nthe \nreality? \nInternational \nJournal \nof \nForecasting, \n36(1), \n37–53. \ndoi:10.1016/j.ijforecast.2018.12.007 \n[31] Wang, X., Smith, K., & Hyndman, R. (2006). Characteristic-Based Clustering for Time Series Data. Data \nMining and Knowledge Discovery, 13(3), 335–364. doi:10.1007/s10618-005-0039-x \n[32] Box, G. E. P., & Pierce, D. A. (1970). Distribution of Residual Autocorrelations in Autoregressive-Integrated \nMoving Average Time Series Models. Journal of the American Statistical Association, 65(332), 1509–1526. \ndoi:10.1080/01621459.1970.10481180 \n[33] Chen, C., & Liu, L.-M. (1993). Forecasting time series with outliers. Journal of Forecasting, 12(1), 13–35. \ndoi:10.1002/for.3980120103 \n[34] Said, S. E., & Dickey, D. A. (1984). Testing for unit roots in autoregressive-moving average models of \nunknown order. Biometrika, 71(3), 599–607. doi:10.1093/biomet/71.3.599 \n[35] Vaiciukynas, E., Danenas, P., Kontrimas, V., & Butleris, R. (2020). Meta-Learning for Time Series Forecasting \nEnsemble. arXiv preprint arXiv:2011.10545. \n[36] Oreshkin, B. N., Carpov, D., Chapados, N., & Bengio, Y. (2020). Meta-learning framework with applications \nto zero-shot time-series forecasting. arXiv preprint arXiv:2002.02887. \n[37] Li, Y., Zhang, S., Hu, R., & Lu, N. (2021). A meta-learning based distribution system load forecasting model \nselection framework. Applied Energy, 294, 116991. doi:10.1016/j.apenergy.2021.11699 \n[38] Lemke, C., & Gabrys, B. (2010). Meta-learning for time series forecasting and forecast combination. \nNeurocomputing, 73(10-12), 2006–2016. doi:10.1016/j.neucom.2009.09.020. \n[39] Makridakis and Hibon (2000) The M3-competition: results, conclusions and implications. International Journal \nof Forecasting, 16, 451-476. \n[40] M4 Team (2018). M4 competitor’s guide: prizes and rules. See https:// www.m4.unic.ac.cy/wp-\ncontent/uploads/2018/03/M4-CompetitorsGuide.pdf. \n[41] Wen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., & Sun, L. (2022). Transformers in time series: A \nsurvey. arXiv preprint arXiv:2202.07125. \n \n \n \n \n \n \n \n \n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-03-18",
  "updated": "2023-10-11"
}