{
  "id": "http://arxiv.org/abs/2203.11861v1",
  "title": "Supervised and Unsupervised Machine Learning of Structural Phases of Polymers Adsorbed to Nanowires",
  "authors": [
    "Quinn Parker",
    "Dilina Perera",
    "Ying Wai Li",
    "Thomas Vogel"
  ],
  "abstract": "We identify configurational phases and structural transitions in a polymer\nnanotube composite by means of machine learning. We employ various unsupervised\ndimensionality reduction methods, conventional neural networks, as well as the\nconfusion method, an unsupervised neural-network-based approach. We find neural\nnetworks are able to reliably recognize all configurational phases that have\nbeen found previously in experiment and simulation. Furthermore, we locate the\nboundaries between configurational phases in a way that removes human intuition\nor bias. This could be done before only by relying on preconceived, ad-hoc\norder parameters.",
  "text": "Supervised and Unsupervised Machine Learning of Structural Phases\nof Polymers Adsorbed to Nanowires\nQuinn Parker,1, ∗Dilina Perera,2 Ying Wai Li,3 and Thomas Vogel1\n1Department of Physics and Astronomy, University of North Georgia, Dahlonega, GA 30597, U.S.A.\n2Department of Physics, University of Colombo, Colombo 03, Sri Lanka\n3Computer, Computational, and Statistical Sciences Division,\nLos Alamos National Laboratory, Los Alamos, NM 87545, U.S.A.\n(Dated: March 23, 2022)\nWe identify conﬁgurational phases and structural transitions in a polymer nanotube composite\nby means of machine learning. We employ various unsupervised dimensionality reduction methods,\nconventional neural networks, as well as the confusion method, an unsupervised neural-network-\nbased approach. We ﬁnd neural networks are able to reliably recognize all conﬁgurational phases that\nhave been found previously in experiment and simulation. Furthermore, we locate the boundaries\nbetween conﬁgurational phases in a way that removes human intuition or bias. This could be done\nbefore only by relying on preconceived, ad-hoc order parameters.\nI.\nINTRODUCTION\nWe previously studied soft–solid matter nano-com-\nposites, in particular systems composed of ﬂexible poly-\nmers adsorbed at thin nanostrings or tubes [1–3]. Such\nsystems are believed to play an important role in cur-\nrent and future development of high-performance nano-\nmaterials. Carbon nanotubes, for example, have been\nfunctionalized by wrapping them with certain types of\npolymers to serve as biosensors for the detection of\nglucose [4, 5].\nHowever, the successful fabrication of\nsuch materials depends on a variety of parameters. In\nparticular, the wetting behavior of carbon nanotubes\nhas been shown to be one critical parameter for the\ndevelopment of nanocomposites [6].\nWe have previ-\nously developed and employed a coarse-grained model\nto investigate nanoscale wetting and adhesion phenom-\nena using Monte Carlo methods; we identiﬁed various,\nstructurally diﬀerent low-temperature phases including\nglobular polymers simply attached to the nanostring\nand polymers completely wrapping, or coating, the sub-\nstrate [1]. One particular problem that we recognized\nwas the classiﬁcation of structural phases at low tem-\nperatures depending on various model parameters. We\nhave addressed the problem in the past by the ad hoc\nintroduction of order parameters to identify boundaries\nbetween such structural phases. In this paper we show\nhow a less biased approach, based on machine learning,\ncan be deployed. We revisit the earlier introduced con-\nﬁgurational phase diagram, aiming at identifying classes\nof the polymer–wire system and the boundaries between\nthem without any assumptions or other input based on\n∗Current address: School of Physics, Georgia Institute of Tech-\nnology, Atlanta, GA 30332, U.S.A.; Correspondence email ad-\ndress: qparker3@gatech.edu\na human perception of structure. In a more general con-\ntext, such automated structure identiﬁcation can also\nprovide means to recognize system conﬁgurations dur-\ning Monte Carlo sampling.\nThis could prove beneﬁ-\ncial in order to run generalized-ensemble simulations\nwhere diﬀerent structural phases are assigned individual\nweights [7], or simply to collect statistics for individual\nphases during a simulation.\nRecent years have witnessed signiﬁcant advances in\nthe use of machine learning (ML) methods for phase\nclassiﬁcation.\nIn this regard, supervised learning ap-\nproaches [8–14], for which the prior labeling of conﬁg-\nurations is required, as well as unsupervised learning\napproaches [15–20], which work without such prior la-\nbeling, have been attempted. It has been demonstrated,\nfor example, that neural networks (NNs) trained with\nlabeled conﬁgurations can encode information about\nthe ordered and disordered phases in model systems\nby learning the relevant order parameters [8]. In par-\nticular, approaches based on purposefully mislabeling\nconﬁgurations and evaluating the network performance\nhave been developed to detect phase transitions [21].\nSuch a method does not require true labels to be known\nin advance and therefore no prior knowledge about the\nexistence (or lack thereof) of a transition is needed. It\nhas also been demonstrated to work in the presence of\nmultiple transitions [22].\nIn the context of unsupervised learning approaches,\ndimensionality reduction techniques, for example, prin-\ncipal component analysis (PCA), multidimensional scal-\ning (MDS), t-distributed stochastic neighbor embed-\nding (t-SNE), autoencoders, etc., have been found use-\nful in distinguishing ordered and disordered phases [15–\n19, 23]. For systems with clear order parameters, such\nas the Ising model or the XY model, the latent param-\neters or the dominant principal components have been\nshown to directly correlate with the respective order\nparameters [17, 19]. Besides structure recognition, NNs\narXiv:2203.11861v1  [cond-mat.soft]  22 Mar 2022\n2\ncan be trained to predict macroscopic physics quantities\nsuch as the total energy, and microscopic quantities such\nas charge density and magnetization locally for each\natom [24]. NNs are also used to learn interatomic poten-\ntials, for example. They have been trained to generate\neﬀective many-body potentials from ab-initio data [25]\nand were successfully applied to construct precise phase\ndiagrams of water in molecular-dynamics (MD) sim-\nulations over a large range of temperatures and pres-\nsures [26]. Another ML potential, ANI-Al, was trained\nto obtain quantum-level accuracy and has been suc-\ncessfully combined with MD simulations to study shock\nphysics in metals [27]. Training ML surrogate models\nis also becoming a useful technique to bridge diﬀerent\nlength and time scales in computer simulations, see [28],\nfor example.\nFinally, NNs have been applied in the ﬁeld of polymer\nmodel simulations to study transition between coil and\nglobule structures and recognize Mackay–Anti-Mackay\nstructures, for example [29, 30]. Transitions between\nsuch crystalline structures in the solid phase are noto-\nriously hard to simulate [31] and advanced generalized\nensemble methods have been developed to do so in the\npast [7, 32].\nThese studies emphasize the beneﬁt of\nknowing the conformational state of a system during\nthe simulation and ML could contribute valuable infor-\nmation if structures can be reliably recognized. In this\npaper we will provide more evidence that this can in-\ndeed be achieved by employing NNs in the supervised\nrecognition of low-energy conﬁgurations of polymers ab-\nsorbed to a substrate (Sec. III A). Furthermore, we will\nshow how unsupervised ML method can be employed if\nno previous knowledge of structural phases of a model is\navailable beforehand (Sec. III B). Finally, we will deter-\nmine boundaries between phases in the model param-\neter space by training NNs in a conventional way, but\nalso by applying the more recently developed confusion\nmethod (Sec. IV).\nII.\nMODEL AND OBSERVED STRUCTURAL\nPHASES\nTo model the nanotube–polymer composite we use\na coarse-grained bead–spring description for the poly-\nmer [33] and an attractive interaction between the\nmonomers and the one-dimensional, continuous string\nthat is derived from a Lennard-Jones potential [1–3].\nThe latter contains two parameters, the eﬀective thick-\nness of the string, σf, and its attraction strength, ϵf:\nV (r⊥) ∝ϵf\n \n63\n64\nσ12\nf\nr11\n⊥\n−3\n2\nσ6\nf\nr5\n⊥\n!\n(1)\nwhere r⊥is the perpendicular distance between a mono-\nmer and the string. The interaction between nonbonded\nFigure 1.\nExamples of low-temperature conﬁgurations in\nphases B, Gi, Ge, and C (from left to right).\nDiﬀerent\nmonomer colors indicate their distance from the string.\nmonomers is described by a standard Lennard-Jones po-\ntential and there is a weak bending stiﬀness for consec-\nutive monomer–monomer bonds, as often employed in\nbead–spring polymer models [33–35]. For a more de-\ntailed discussion of diﬀerent approaches to model a thin,\ncylindrical substrate, see [36].\nDepending on the parameter set {σf, ϵf} in Eqn. (1)\nlow-energy structures take qualitatively diﬀerent shapes\nthat can be grouped into structural classes or phases. In\nour previous work we distinguished between four such\nphases and labeled them Ge, Gi, C, and B; see Fig. 1 for\nvisualisations. Structures like the ones we ﬁnd in the Gi,\nGe, and C regions have been found and imaged in ex-\nperimental studies before, in particular “clam-shell” (C)\npolymer nanodroplets have been emphasized [6, Fig. 4].\nGlobular conﬁgurations in phase Ge are similar to struc-\ntures seen during dewetting of polymers on the surface\nof carbon nanotubes (CNTs) [6, Fig. 2] while Gi con-\nﬁgurations for large values of ϵf show similarities with\n“barrel-type” nanodroplets [ibid]. Note that pure mono-\nlayer barrel structures (B) can be mapped onto diﬀerent\ntypes of CNTs [37, 38]. In fact, we found that region\nB contains sub-phases with diﬀerent chiralities corre-\nsponding to those found in CNTs [2].1\nIII.\nMACHINE LEARNING OF STRUCTURAL\nPHASES\nIn the following we investigate diﬀerent supervised\nand unsupervised machine learning (ML) methods for\nstructure recognition. In ML one typically desires large\ndatasets to reliably train a robust model. However, in\nthe research presented here the data is intrinsically hard\nto generate since we are analysing states that domi-\nnate canonical ensembles at very low temperatures. We\nuse Wang–Landau (WL) sampling [39] to produce these\nlow-energy conﬁgurations.\nEven though WL reliably\nﬁnds these states, we face the challenge to have to col-\nlect many, very diﬀerent and ideally uncorrelated low-\nenergy conﬁgurations for all parameter values {σf, ϵf}\n1 While those sub-phases should be able to be recognized by\nappropriately trained neural networks, we will not emphasize\nthose any further in this paper.\n3\n(see Eq. 1).\nIn an extreme approach and as a proof\nof concept, we here only record one conﬁguration every\ntime the WL walker explores a low-energy valley and\nthen wait for the walker to move to regions in the phase\nspace corresponding to high temperatures before col-\nlecting data again at low energies. Admittedly, such a\nstrategy is computationally expensive and even though\napplied to generate the dataset analysed in this section,\nit might not be necessary in that extreme way (see a\ndiscussion below in Sec. IV).\nIn all our simulations, a polymer conﬁguration is\nrepresented by the three spatial coordinates of 100\nmonomers. We use these 300 coordinates (either in raw\nformat or preprocessed, see below) as the feature set for\nthe machine learning algorithms. Although it is possible\nto utilize an engineered feature set based on our phys-\nical intuition of the system (for example by including\nmacroscopic physical observables like the radius of gy-\nration, end-to-end distance, energy, etc.), avoiding such\nengineered features leaves the machine learning algo-\nrithm unbiased and free of any preconceived notions.\nA.\nSupervised Learning: Structure Recognition\nwith Neural Networks\nThe neural network (NN) is set up with an input layer\nof 300 neurons, two hidden layers of 50 neurons each and\none output layer of four neurons, see Fig. 2. The dataset\nconsists of about 3000 conﬁgurations or samples for each\npolymer type. Two thirds of the dataset are allocated\nfor training, while the remaining data is used for testing.\nThe rectiﬁed linear unit (ReLU) activation function is\nused for the hidden layers and the softmax function for\nthe output layer. We use the Nadam optimization al-\ngorithm over ninety epochs for training.\nFinally, to\nmitigate overﬁtting we employ L2 kernel regularization.\nThe results of the NN classiﬁcation are shown in Fig. 3\nwhere we plot the confusion matrices and the learning\ncurves.\nWe start by running an analysis with the adsorbed\npolymer conﬁguration types B, C, Ge, Gi (see Fig. 1)\nas part of the dataset. We preprocess the data by spa-\ntially shifting the monomers in the z direction such that\nthe center of mass of each polymer lies on the xy plane.\nFigure 3 (a) shows the training and validation accu-\nracy measured at each epoch during the NN training.\nBoth the training accuracy and the validation accuracy\nrapidly converge to a steady value within 20 epochs. In\naddition, the training and validation curves are quite\nclose to each other and therefore show no noticeable\nsign of overﬁtting. Figure 3 (b) shows the confusion ma-\ntrix obtained for the validation set, normalized by the\nnumber of elements in each class. We see that the oﬀ-\ndiagonal elements are zero, except for a very few mis-\nclassiﬁcations of C-type polymers, yielding an almost\nFigure 2. Schematic of the neural network with 300 input\nneurons, two hidden layers, an four output neurons.\n100% overall validation accuracy.\nSince the neural network was able to reliably identify\nall adsorbed polymer structures we also included high-\ntemperature, random-coil polymer structures (“R”) not\nadsorbed to the string as another type and added an\noutput neuron accordingly. Figures 3 (c) and (d) show\nthe corresponding accuracy curves and the confusion\nmatrix. The training and validation accuracies again\nconverge to 1.0, although the convergence rate is slower\ncompared to the previous case. Again, the curves do not\nshow evidence of noticeable overﬁtting. The slightly in-\ncreased presence of oﬀ-diagonal entries in the “R” row of\nthe confusion matrix indicates a somewhat higher ten-\ndency for random coil conﬁgurations to be misclassiﬁed\nas other polymer types. However, this is expected as\nthose polymers are random conﬁgurations that could, in\nfact, loosely resemble any of the other classes by chance.\nB.\nUnsupervised Learning: Dimensionality\nReduction Methods\nDimensionality reduction methods refer to a class\nof unsupervised machine-learning techniques that map\ndata from an original, high-dimensional space to a\nlower-dimensional space while ideally preserving some\nof the salient properties of the data.\nIn the context\nof thermodynamic phase classiﬁcation, for example,\nsuch low-dimensional representations of the conﬁgura-\ntion space have been used to facilitate the visual iden-\ntiﬁcation of distinct phases [15, 16, 19] and to provide\ninsight into the relationship between important features\nand order parameters of complex systems [17, 19, 30].\nPrincipal component analysis (PCA) [40] is a linear\ndimensionality reduction technique which identiﬁes a\nset of mutually orthogonal unit vectors in a given fea-\nture space. These vectors are ordered according to the\nvariance of the data in the corresponding directions,\nsuch that the ﬁrst unit vector indicates the direction\n4\nFigure 3. Training and validation accuracy (left) and normalized confusion matrices (right) during supervised structure\nrecognition training on the the neural network. Top row: Only the four low-energy phases (B, C, Ge, Gi; where polymers\nare adsorbed on string) are used for training and recognition. Bottom row: Data includes high-temperature, random coil\nconﬁgurations (R; where polymers are desorbed from string).\nof greatest variance in the data. This direction is then\ncalled the ﬁrst principal component, the one with the\nsecond highest variance the second principal compo-\nnent, and so on.\nThe principal components are the\neigenvectors of the covariance matrix of the data, and\nhence can be determined by the eigendecomposition of\nthat matrix or the singular value decomposition of the\ndata matrix. The original conﬁgurations are then pro-\njected into a space spanned by the ﬁrst m principal\ncomponents to obtain the desired lower m-dimensional\nrepresentations. For some spin systems, the principal\ncomponents have been shown to recover the physical\norder parameters for phase transitions [17, 19].\nIn addition to PCA, we apply a number of other non-\nlinear dimensionality reduction methods, namely, mul-\ntidimensional scaling (MDS) [41], t-distributed stochas-\ntic neighbor embedding (t-SNE) [42], Isomap [43], and\ndiﬀusion map [44]. Note that Isomap becomes equiva-\nlent to PCA as the neighborhood size approaches the\nsample size.\nTherefore, we limited the neighborhood\nsize to 20 for this demonstration, but also conﬁrmed\nthat changing this number will not change the qual-\nitative results.\nIn general, such non-linear methods\nidentify lower-dimensional manifolds embedded within\nthe higher-dimensional feature space, in which similar\ndata points are clustered together. Typically, manifold\n5\nlearning methods can capture nonlinear relationships\nwithin the data that cannot be captured through prin-\ncipal component analysis.\n1.\nData Pre-processing\nWhen employing unsupervised learning methods the\ndata typically has to be prepared in some way to ob-\ntain meaningful results. In the raw data the polymer\nis adsorbed at the string at an arbitrary position, while\nthe string is always located at the z-axis in Cartesian\ncoordinates. We here utilize diﬀerent scaling and co-\nordinate transformation methods to potentially make\nthe features of the polymers more comparable for the\nmachine. A common method in machine learning, re-\nferred to as “standard scaling” [45] aims at bringing all\nfeatures (in our case, monomer coordinates) onto the\nsame length scale by subtracting the mean of all data\nfrom each feature and individually scaling each feature\nto unit variance. Two other ways that do not alter the\noverall shape of the polymer are translations along the\nstring such that the z-component of the center of mass\nis zero for all polymers, eliminating arbitrary shifts in\nspatial position, and translations of the overall center of\nmass to the coordinate origin, normalizing the position\nof the polymers across the simulated examples. While\nthe ﬁrst aims at recognizing the general position of the\npolymer with respect to the string, the latter is aimed\nat identifying the internal structure of globular poly-\nmers. Ge and Gi type conﬁgurations, for example, have\na similar surface shape, but diﬀer in relative position to\nthe string and their internal crystalline structure. Fi-\nnally, to help the machine recognize structural rather\nthan size diﬀerences across all polymer types, we scaled\nall polymers with respect to their radius of gyration Rg.\n2.\nResults of unsupervised learning\nIn Fig. 4 we present the two-dimensional representa-\ntions of the conﬁguration space obtained from the di-\nmensionality reduction methods mentioned above. Dif-\nferent columns show results after diﬀerent data prepro-\ncessing steps employed (raw data without preprocesss-\ning, standard scaling, subtracting the z-component of\nthe center of mass, subtracting all three components of\nthe center of mass, and data scaled by Rg after subtract-\ning the center of mass). Even without any preprocessing\n(leftmost column), for example, we observe that PCA\ncan reasonably well distinguish barrel-like (B) confor-\nmations from all others. However, none of the methods\ncan distinguish Gi, Ge, and C conformations without\npreprocessing.\nThis is presumably because the z co-\nordinates of the conﬁgurations have much higher vari-\nance than the x and y coordinates, as the system shows\ntranslational invariance in the z direction.\nThe poor\nperformance of ML algorithms due to diﬀerent features\nhaving diﬀerent scales is a common problem in machine\nlearning. This issue can be alleviated with appropriate\nfeature scaling techniques. Here we ﬁrst test standard\nscaling. As the second column of Fig. 4 shows, this ap-\nproach does improve the performance of the algorithms\n(particularly that of MDS), as a clearer separation of\nGi, Ge, and C conformations can be observed. How-\never, it is important to note that since the scaling is\nperformed independently on individual features, these\ncoordinate transformations lead to non-physical defor-\nmations in the polymer conﬁgurations.\nA more physically intuitive scaling approach is to sub-\ntract the center of mass, which would reduce the vari-\nance of the coordinates due to the drifting of polymers\nin arbitrary directions.\nIn particular, polymers have\nthe freedom to drift along the substrate in z-direction.\nTherefore one would expect noticeable improvements\nin the results just by subtracting the z component of\nthe center of mass alone. As the third column of Fig. 4\nshows, we indeed observe improved performance in most\nalgorithms in terms of separating previously overlap-\nping phases observed in the analysis using raw data.\nThe fourth column shows the results obtained by sub-\ntracting the whole center of mass. For some algorithms\n(particularly MDS), subtracting all three components\nof the center of mass further improves phase separa-\ntion. The rightmost column in Fig. 4 shows the results\nobtained with all coordinates furthermore normalized\nby scaling with the radius of gyration Rg.\nHowever,\nwe observe that Gi, Ge, and C phases are no longer\ndistinguishable. This indicates that the length scale of\npolymers is a particularly important feature for distin-\nguishing diﬀerent polymer states.\nIn summary, we ﬁnd that identifying barrel-type (B)\nconﬁgurations can be accomplished by all methods with\nsuitable preprocessing steps.\nTelling all other struc-\ntures apart is more challenging and no single scheme is\nable to do so alone.2 That said, we note that the MDS\nmethod trained with data preprocessed by subtracting\nall three components of the center of mass seems to give\nthe best, single overall performance, particularly since\nboth B and C conformations are grouped into isolated\nclusters spatially separated from other states. Still, an\noverlap between Gi and Ge phases can be observed in\nthis case since both phases diﬀer mostly by the relative\nlocation with respect to the substrate and not in over-\nall shape. To observe a separation between Gi and Ge\nstructures one would need to use another procedure,\nlike MDS or PCA with a translational normalization\n2 We note that it might be possible to do so with a reduction to\na 3-dimensional space though.\n6\nFigure 4. Two-dimensional projections of the conﬁguration space obtained using various dimensionality reduction techniques,\nnamely, principal component analysis (PCA), multidimensional scaling (MDS), t-distributed stochastic neighbor embedding\n(t-SNE), Isomap, and diﬀusion map. Diﬀerent columns represent diﬀerent data preprocessing techniques applied: (from\nleft to right) raw data without preprocesssing, data standardized by subtracting the mean and scaling to unit variance,\ndata processed by subtracting the z component of the center of mass, data processed by subtracting all three Cartesian\ncomponents of the center of mass, and data scaled by the radius of gyration after subtracting the center of mass.\nalong the z-direction only. The general ﬁnding that no\none scaling approach and reduction-method combina-\ntion can clearly separate all phases present in our sys-\ntem is probably true for other complex polymer systems\nas well.\nDepending on symmetry and speciﬁc struc-\ntures, diﬀerent data preprocessing and scaling methods\nmight always have to be chosen to match all physical\nproperties.\n7\nIV.\nIDENTIFYING STRUCTURAL\nTRANSITIONS WITH NEURAL NETWORKS\nIn this section we study the applicability of neural\nnetworks (NNs) to not only recognize diﬀerent struc-\ntures but to detect transitions points between them.\nWhile we have discussed above the desire to use large\ndatasets for NN training in general, much more train-\ning data is potentially needed for such an endeavor\nsince structural diﬀerences could be much more sub-\ntle between polymers close to each other in parameter\nspace, compared to above (Sec. III) where structures\nare more fundamentally diﬀerent from each other. To\nenrich our datasets we therefore apply a strategy in the\nspirit of oversampling augmentation [46] where we al-\nlow to record up to 100 slightly modiﬁed conﬁgurations\nevery time the WL walker explores a low-energy region.\nAfter reaching that number, the walker has to com-\npletely “warm up” again, that is move to energies en-\ncountered well inside the random-coil phase. To ensure\nthe data in each such batch is not eﬀectively identical\nbut to some degree still uncorrelated we enforce a min-\nimum energy diﬀerence ∆E between two consecutive\nconﬁgurations that are added to the dataset.\nA.\nConventional, supervised approach\nIn previous research we had to rely on human intu-\nition to deﬁne structural classes and suitable observ-\nables or order parameters to ﬁnd the boundaries in pa-\nrameter space between them. The structural phase di-\nagram for low-energy states [1] (see Fig. 5 for a reduced\nversion) was hence developed upon the ad-hoc intro-\nduction of an asymmetry parameter, for example, to\nlocate the crossing from phase Gi to B. Such practice\ninevitably introduces a bias based on the human per-\nception of structure. It is therefore, in principle, hard\nto judge whether or not we identiﬁed the most rele-\nvant structural features. A less biased approach that\ncurrently gets increasing attention is the use of ma-\nchine learning methods to identify crossing or phase\ntransition points between structural or thermodynamic\nphases [8, 29, 47–50]. We here use neural networks that\nwe train with data which can be clearly assigned to dif-\nferent structural classes and have them analyse polymer\nconﬁgurations in regions of the parameter space where\nsuch a classiﬁcation is less deﬁned.\nSpeciﬁcally, we investigate the transition between\nglobular polymers absorbed to the string (Ge) and clam-\nshell structures surrounding the string (C), two of the\nphases that were particularly hard to distinguish with\nthe unsupervised methods discussed above. The NN is\nset up with the same parameters as above (cf. Fig. 2)\nwith the diﬀerence that only two nodes are speciﬁed\nFigure 5. Conﬁgurational phase diagram in model-param-\neter space of low-temperature polymer conﬁgurations ad-\nsorbed to a thin string. Previously determined transition\nregions are shaded in gray.\nWe analyse structures at all\npoints indicated along the diagonal line from λ = {σf, ϵf} =\n{1.0, 1.0} to λ = {2.1, 3.2}.\nfor the output layer.\nThe network is then trained\nwith conﬁgurations at λ = {σf, ϵf} = {1.0, 1.0} and\nλ = {2.1, 3.2}, which clearly belong to the Ge and\nC classes, respectively. We use the such trained net-\nwork to analyse conﬁgurations at ten other parame-\nter values in between those points (see black, diago-\nnal line in Fig. 5) and predict their belonging to either\nclass. When plotting the corresponding probabilities,\nas shown in Fig. 6 (left), we see a “crossing” of the\nprobability curves.\nAs one would expect, the corre-\nsponding error bars are largest around the phase in-\ntersection and decrease towards the outermost points,\nsee Fig. 6 (right). That is, the uncertainty of the net-\nwork in classifying polymer conﬁguration is maximal\naround the transition from one phase to another. We\nassess uncertainties of the trained NN models via diﬀer-\nent methods including cross-validation [51] and query-\nby-committee [52]. In cross-validation, a subset of the\nwhole dataset is held out for testing while the remaining\ndata would be used for training. The process repeats\nwith diﬀerent held-out testing sets, resulting in a group\nof NN models that can be used for the estimation of sta-\ntistical errors.\nWe performed 10-fold cross-validation\nbut it seemed to underestimate the real error of the\nmodel. It could be because the 10 resulting models are\nnot statistically independent: each of them are trained\nusing training datasets that overlap with each other by\n80%. When these highly-correlated models are applied\nto make predictions on out-of-sample data (structures\nbetween λ = {1.0, 1.0} and λ = {2.1, 3.2}), they re-\nsult in a small distribution (variance) around the mean,\nbut the mean prediction might have a high discrepancy\n(bias) compared to the reference. Hence we report er-\nrors from query-by-committee: the whole dataset is di-\n8\nFigure 6. Left: probabilities of predicting C and Ge polymers at diﬀerent points in parameter space after training the\nNN with data at λ = {σf, ϵf} = {1.0, 1.0} and λ = {2.1, 3.2} (outermost points on each side). Right: The corresponding\nstatistical error from diﬀerent, independent predictions. The error is largest around the transition region.\nvided into 10 subsets, within each subset 70% of the\ndata were used for training and 30% of the data were\nused for testing. This results in 10 individually trained\nNN models that are truly independent and not corre-\nlated. The error bars we show in Fig. 6 therefore indi-\ncate the statistical error from multiple runs with NNs\nwhich were individually trained with independent data\nand also analysing diﬀerent datasets. That way we cap-\nture both epistemic and aleatoric uncertainties.\nB.\nUnsupervised: The Confusion Method\nA neural network is inherently a supervised learn-\ning method and requires a dataset with preassigned la-\nbels for the adjustment of weights in the training phase.\nHowever, in certain cases it may be diﬃcult, or even im-\npossible, to know the correct assignment of labels be-\nforehand. For the case of phase classiﬁcation, one can\ncircumvent this issue by identifying a window within\nwhich a given transition occurs and labeling the conﬁgu-\nrations outside this window based on the corresponding\nphase labels, as we did above in Sec. IV A. In particular\nfor ﬁnite systems that do not naturally scale up to the\nthermodynamic limit, though, it can be challenging to\nreliably locate the exact point of transition this way.\nThe confusion method [21] provides an alternative.\nIt not only eliminates the need for prior assignment of\nlabels, but also results in a clearer and more precise es-\ntimate of the transition point. This method as well is a\nneural-network based, but semi-supervised approach for\ndetecting phase transitions and relies on purposeful mis-\nlabeling of the data. Let λ denote a model parameter\nor a thermodynamic observable such as the temperature\nor the average energy. Assume that there exists a crit-\nical point λc at which a transition from a phase X to a\nphase Y occurs. When applying the confusion scheme,\none ﬁrst identiﬁes a window [λa, λb] within which the\ntransition is likely to occur. Then a potential transition\npoint λ′\nc ∈[λa, λb] is proposed, and the label “0” (de-\nnoting phase X) is assigned to all conﬁgurations below\nλ′\nc, and the label “1” (denoting phase Y) to all conﬁg-\nurations above λ′\nc.\nA neural network is then trained\nwith this label assignment and the classiﬁcation accu-\nracy P(λ′\nc) obtained for a test set is recorded.\nThis\nprocess is repeated by systematically varying λ′\nc from\nλa to λb. The resulting curve P(λ′\nc) then yields a char-\nacteristic “W” shape, with the middle peak occurring at\nλ′\nc = λc [21].\nThis W-shaped proﬁle of P(λ′\nc) can be understood\nas follows. For λ′\nc = λa, all conﬁgurations are labeled\n“1”, and the neural network correctly predicts the as-\nsigned label for all samples, achieving 100% accuracy.\nSimilarly, the network performs with 100% accuracy\nfor λ′\nc = λb as all the conﬁgurations are labeled “0”.\nFor λ′\nc = λc, the assigned labels for all samples exactly\nmatch the true phase labels and, in principle, the NN\ncan again achieve perfect accuracy.\nFor other values\nof λ′\nc, the NN sees a discrepancy between the assigned\nlabels and the true phase labels as identiﬁed by the pat-\nterns in data. Due to this confusion, the NN learns to\npredict the majority label. Ultimately this yields the\ncharacteristic W shape of P(λ′\nc).\nNote that in prac-\ntice this shape will likely be distorted due to ﬁnite-size\neﬀects and imperfections in the training process.\nWe apply the confusion method to investigate the\ntransition from the Ge phase to the C phase along the\nsame straight path through the phase diagram as above\nand indicated in Fig. 5.\nFor the neural network we\nadopt a feed-forward architecture with a single hidden\n9\n(1.0;1.0)\n(1.2;1.4)\n(1.4;1.8)\n(1.6;2.2)\n(1.8;2.6)\n(2.0;3.0)\nλ′\nc\n0.92\n0.94\n0.96\n0.98\n1.00\nP(λ′\nc)\nFigure 7. The test accuracy P(λ′\nc) as a function of the pro-\nposed transition point λ′\nc.\nlayer of 40 neurons. We use the same data as above\nand 70% of the conﬁgurations were randomly selected\nfor training while the remaining samples were used for\ntesting. For error estimation, the confusion scheme is\nrepeated 10 times, each time with a diﬀerent random se-\nlection of training and testing samples. Figure 7 shows\nthe test accuracy P(λ′\nc) as a function of the proposed\ntransition point λ′\nc. The curve follows the characteris-\ntic W shape discussed above, conﬁrming the expected\ntransition from the Ge phase to the C phase. The mid-\ndle peak indicates the location of the transition point\nλ′\nc = λc.\nWe emphasize that the evidence for a transition pro-\nvided by the confusion method is more compelling than\nthat provided by the conventional neural network based\napproach discussed above in Sec. IV A. The conven-\ntional approach may falsely indicate the presence of a\ntransition within the window [λa, λb] even when all sam-\nples belong to the same phase, because also in the ab-\nsence of a transition conﬁgurations may undergo slight\nstructural changes as the parameter λ is varied between\n[λa, λb]. More speciﬁcally, one assumes that the tran-\nsition occurs within a sub-window [λ1, λ2], and assigns\nthe label “0” to all conﬁgurations in the interval [λa, λ1],\nand the label “1” to all conﬁgurations in [λ2, λb]. The\nneural network may then detect the gradual structural\nchanges in the conﬁgurations as a function of λ, and\nestablish a decision boundary between λ1 and λ2 such\nthat the predictions are consistent with the assigned\nlabels. Consequently, the curves for the averaged out-\nput neuron values may still cross each other, giving the\nfalse indication of a transition. In the confusion method\nhowever, the W-shape proﬁle is only possible if there are\nabrupt, drastic changes in structure at a certain value\nof λ, reminiscent of a true transition. In the absence of\na transition, the middle peak in P(λ′\nc) curve disappears,\nresulting in a “V” shape [21].\nV.\nSUMMARY\nIn this paper we explore the applicability of various\nmachine learning methods to recognize structures and\nstructural transitions in a model for polymer–nanotube\ncomposites.\nIn particular, we investigate structures\nthat have been observed experimentally where the poly-\nmer is adsorbed at the nanotube. The two main ques-\ntions we address are whether and how we can identify\nthose structure with machine learning and how to locate\nthe transition regions between them.\nFor structure recognition we test various unsuper-\nvised dimensionality reduction methods like principal\ncomponent analysis or multidimensional scaling that we\ncombine with diﬀerent ways to pre-process the data.\nThe advantage of unsupervised methods is that no pre-\nlabelling of structures is required, removing all poten-\ntial human bias in structure classiﬁcation. We ﬁnd that\nwhile structure identiﬁcation in principle is possible, no\nsingle method alone is capable of doing so. We found\nit particularly challenging to have the machine diﬀer-\nentiate between globular structures where the polymer\nis fully wrapped around the substrate or just connects\nto the tube. Aside from the unsupervised methods we\nalso employed neural network methods that do require\npre-labeled input. The network was able to reliably rec-\nognize all polymer structures after suitable training.\nWhile it is probably uncontroversial to introduce dif-\nferent structural phases for polymer–nanotube compos-\nites, ﬁnding the exact boundary between those phases\nremains a challenge since it is in general not obvious\nwhat good order parameters are. We previously intro-\nduced such parameters ad-hoc, but test here if a neural\nnetwork could identify transitions between conﬁgura-\ntional phases, with and without training using conﬁgu-\nrations from the respective phases and without further\nhuman guidance or knowledge of pre-deﬁned order pa-\nrameters. This will be particularly useful since there\nis not a sharp, thermodynamic phase transition. We\nﬁnd that neural-network methods still indicate a tran-\nsition, most notably the confusion method. However,\nsince such structural transitions of ﬁnite systems poten-\ntially happen in diﬀerent steps and over a broader region\nin parameter space, diﬀerent machine learning methods\nor neural networks might pick up diﬀerent steps in this\ntransition at slightly diﬀerent parameter values. In that\nsense, results for the crossing point shown in Figs. 6\nand 7 are not necessarily contradicting, when keeping\nalso in mind that data has to be binned for the con-\nfusion method, leaving a corresponding uncertainty in\nthe exact position of the crossing. That said, we also\nnote that the traditional method of training the network\nwith labeled conﬁgurations from both phases has to be\nused with care since it can potentially detect a crossing\neven if there was no phase transition. The main advan-\n10\ntage of the confusion method here is therefore that it\nprovides evidence for a transition between Ge and C.\nOtherwise, the shape of the detection accuracy graph\nwould be a “V”-shape rather than a “W”-shape.\nOverall, we conﬁrm that deﬁning structural transi-\ntions in our system is reasonable, in principle. We also\nconclude, though, that we might not have been success-\nful initially [1] in ﬁnding the best order parameter for all\ntransitions, in particular for the crossing between \"Ge\"\nand \"C\" structures, as evidenced by the results shown\nin Figs. 5–7. In that sense, the machine learning meth-\nods applied here can be a valuable complement to more\nconventional methods of detecting structural transitions\nused earlier, as they remove the necessity of identifying\nor deﬁning explicit order parameters beforehand and\ntherefore provide a potentially less biased approach to\nstructure recognition and classiﬁcation.\nACKNOWLEDGEMENTS\nAll data was produced on the University of North\nGeorgia’s Pando computing cluster. Y. W. Li acknowl-\nedges support from U.S. Department of Energy, Oﬃce\nof Science, Oﬃce of Basic Energy Sciences, under Award\nNumber DE-SC0022311. LA-UR-21-30102.\n[1] T. Vogel and M. Bachmann, Phys. Rev. Lett. 104,\n198302 (2010).\n[2] T. Vogel and M. Bachmann, Phys. Procedia 4, 161\n(2010).\n[3] T. Vogel and M. Bachmann, Comp. Phys. Commun.\n182, 1928 (2011), Special Edition for Conference on\nComputational Physics, Trondheim, Norway, June 23-\n26, 2010.\n[4] M. Gao, L. Dai,\nand G. Wallace, Electroanalysis 15,\n1089 (2003).\n[5] J. Wang, Electroanalysis 17, 7 (2005).\n[6] M. Q. Tran, J. T. Cabral, M. S. P. Shaﬀer, and A. Bis-\nmarck, Nano Lett. 8, 2744 (2008).\n[7] S. Schnabel, W. Janke,\nand M. Bachmann, J. Comp.\nPhys. 230, 4454 (2011).\n[8] J. Carrasquilla and R. Melko, Nature Phys. 13, 431\n(2017).\n[9] F. Schindler, N. Regnault, and T. Neupert, Phys. Rev.\nB 95, 245134 (2017).\n[10] Y. Zhang and E.-A. Kim, Phys. Rev. Lett. 118, 216401\n(2017).\n[11] K.\nCh’ng,\nJ.\nCarrasquilla,\nR.\nG.\nMelko,\nand\nE. Khatami, Phys. Rev. X 7, 031038 (2017).\n[12] P. Ponte and R. G. Melko, Phys. Rev. B 96, 205146\n(2017).\n[13] C.-D. Li, D.-R. Tan, and F.-J. Jiang, Annals of Physics\n391, 312 (2018).\n[14] P. Zhang, H. Shen, and H. Zhai, Phys. Rev. Lett. 120,\n066401 (2018).\n[15] L. Wang, Phys. Rev. B 94, 195105 (2016).\n[16] C. Wang and H. Zhai, Phys. Rev. B 96, 144432 (2017).\n[17] W. Hu, R. R. P. Singh, and R. T. Scalettar, Phys. Rev.\nE 95, 062122 (2017).\n[18] N. C. Costa, W. Hu, Z. J. Bai, R. T. Scalettar,\nand\nR. R. P. Singh, Phys. Rev. B 96, 195138 (2017).\n[19] S. J. Wetzel, Phys. Rev. E 96, 022140 (2017).\n[20] K. Ch’ng, N. Vazquez, and E. Khatami, Phys. Rev. E\n97, 013306 (2018).\n[21] E. van Nieuwenburg, Y. Liu,\nand S. Huber, Nature\nPhys. 13, 435 (2017).\n[22] M. J. S. Beach, A. Golubeva, and R. G. Melko, Phys.\nRev. B 97, 045207 (2018).\n[23] A. M. Samarakoon, K. Barros, Y. W. Li, M. Eisenbach,\nQ. Zhang, F. Ye, V. Sharma, Z. L. Dun, H. Zhou, S. A.\nGrigera, C. D. Batista,\nand D. A. Tennant, Nature\nCommun. 11, 1 (2020).\n[24] M. Lupo Pasini, Y. W. Li, J. Yin, J. Zhang, K. Barros,\nand M. Eisenbach, J. Phys: Cond. Mat. 33, 084005\n(2020).\n[25] L. Zhang, J. Han, H. Wang, R. Car, and W. E, Phys.\nRev. Lett. 120, 143001 (2018).\n[26] L. Zhang, H. Wang, R. Car,\nand W. E, “The phase\ndiagram of a deep potential water model,”\n(2021),\narXiv:2102.04804 [physics.chem-ph].\n[27] J. S. Smith, B. Nebgen, N. Mathew, J. Chen, N. Lub-\nbers, L. Burakovsky, S. Tretiak, H. A. Nam, T. Ger-\nmann, S. Fensin, and K. Barros, Nature Commun. 12,\n1257 (2021).\n[28] A. Diaw, K. Barros, J. Haack, C. Junghans, B. Keenan,\nY. W. Li, D. Livescu, N. Lubbers, M. McKerns, R. S.\nPavel, D. Rosenberger, I. Sagert, and T. C. Germann,\nPhys. Rev. E 102, 023310 (2020).\n[29] Q. Wei, R. G. Melko, and J. Z. Y. Chen, Phys. Rev. E\n95, 032504 (2017).\n[30] X. Xu, Q. Wei, H. Li, Y. Wang, Y. Chen, and Y. Jiang,\nPhys. Rev. E 99, 043307 (2019).\n[31] S. Schnabel, T. Vogel, M. Bachmann,\nand W. Janke,\nChem. Phys. Lett. 476, 201 (2009).\n[32] S. Schnabel, M. Bachmann,\nand W. Janke, J. Chem.\nPhys. 131, 124904 (2009).\n[33] F. H. Stillinger, T. Head-Gordon, and C. L. Hirshfeld,\nPhys. Rev. E 48, 1469 (1993).\n[34] M. Bachmann, H. Arkin, and W. Janke, Phys. Rev. E\n71, 031906 (2005).\n[35] S. Schnabel, M. Bachmann, and W. Janke, Phys. Rev.\nLett. 98, 048103 (2007).\n[36] T. Vogel, J. Gross, and M. Bachmann, J. Chem. Phys.\n142, 104901 (2015).\n[37] T. Vogel, T. Mutat, J. Adler, and M. Bachmann, Com-\nmun. Comp. Phys. 13, 1245 (2013).\n[38] T. Vogel, T. Mutat, J. Adler, and M. Bachmann, Phys.\nProcedia 15, 87 (2011), Proceedings of the 24th Work-\nshop on Computer Simulation Studies in Condensed\nMatter Physics (CSP2011).\n11\n[39] F. Wang and D. P. Landau, Phys. Rev. Lett. 86, 2050\n(2001).\n[40] K. Pearson, Philos. Mag. 2, 559 (1901).\n[41] J. B. Kruskal, Psychometrika 29, 115 (1964).\n[42] L. van der Maaten and G. Hinton, Journal of Machine\nLearning Research 9, 2579 (2008).\n[43] J. B. Tenenbaum, V. d. Silva,\nand J. C. Langford,\nScience 290, 2319 (2000).\n[44] R. R. Coifman, S. Lafon, A. B. Lee, M. Maggioni,\nB. Nadler, F. Warner,\nand S. W. Zucker, Proc. Natl.\nAcad. Sci. USA 102, 7426 (2005).\n[45] A. Zhen and A. Casari, Feature Engineering for Ma-\nchine Learning:\nPrinciples and Techniques for Data\nScientists (O’Reilly Media, 2018).\n[46] C. Shorten and T. M. Khoshgoftaar, J. Big Data 6, 60\n(2019).\n[47] B. S. Rem, N. Käming, M. Tarnowski, L. Asteria,\nN. Fläschner, C. Becker, K. Sengstock, and C. Weiten-\nberg, Nature Phys. 15, 917 (2019).\n[48] M. Walters, Q. Wei, and J. Z. Y. Chen, Phys. Rev. E\n99, 062701 (2019).\n[49] W. Zhang, J. Liu,\nand T.-C. Wei, Phys. Rev. E 99,\n032142 (2019).\n[50] H. Munoz-Bauza, F. Hamze, and H. G. Katzgraber, J.\nStat. Mech.: Theory Exp. 2020, 073302 (2020).\n[51] I. Goodfellow, Y. Bengio,\nand A. Courville, Deep\nLearning (MIT Press, 2016) http://www.book.org.\n[52] B. Lakshminarayanan, A. Pritzel, and C. Blundell, in\nAdvances in Neural Information Processing Systems 30,\nedited by I. Guyon et al. (Curran Associates, Inc., 2017)\npp. 6402–6413.\n",
  "categories": [
    "cond-mat.soft",
    "cond-mat.dis-nn"
  ],
  "published": "2022-03-22",
  "updated": "2022-03-22"
}