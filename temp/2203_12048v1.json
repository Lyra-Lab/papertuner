{
  "id": "http://arxiv.org/abs/2203.12048v1",
  "title": "Review of Metrics to Measure the Stability, Robustness and Resilience of Reinforcement Learning",
  "authors": [
    "Laura L. Pullum"
  ],
  "abstract": "Reinforcement learning has received significant interest in recent years, due\nprimarily to the successes of deep reinforcement learning at solving many\nchallenging tasks such as playing Chess, Go and online computer games. However,\nwith the increasing focus on reinforcement learning, applications outside of\ngaming and simulated environments require understanding the robustness,\nstability, and resilience of reinforcement learning methods. To this end, we\nconducted a comprehensive literature review to characterize the available\nliterature on these three behaviors as they pertain to reinforcement learning.\nWe classify the quantitative and theoretical approaches used to indicate or\nmeasure robustness, stability, and resilience behaviors. In addition, we\nidentified the action or event to which the quantitative approaches were\nattempting to be stable, robust, or resilient. Finally, we provide a decision\ntree useful for selecting metrics to quantify the behaviors. We believe that\nthis is the first comprehensive review of stability, robustness and resilience\nspecifically geared towards reinforcement learning.",
  "text": "PREPRINT \n \n1 \nReview of Metrics to Measure the Stability, Robustness and \nResilience of Reinforcement Learning \n \nLaura L. Pullum, DSc \nComputer Science and Mathematics Division, Oak Ridge National Laboratory \npullumll@ornl.gov \n \nAbstract \n \nReinforcement learning (RL) has received significant interest in recent years, due \nprimarily to the successes of deep reinforcement learning at solving many challenging \ntasks such as playing Chess, Go and online computer games. However, with the \nincreasing focus on RL, applications outside of gaming and simulated environments \nrequire understanding the robustness, stability and resilience of RL methods. To this \nend, we characterize the available literature on these three behaviors as they pertain to \nRL. We classify the quantification approaches used, determine the objectives of the \ndesired behaviors, and provide a decision tree for selecting metrics to quantify the \nbehaviors. \n \n1.  Introduction \n \nRecent literature on the robustness of machine learning models has focused almost \nentirely on the robustness of deep neural networks for imaging applications. However, at \nthe time of this study, there are no published surveys on robustness of RL. With RL use \nincreasing, especially in control systems contexts, we pursued this review. Included \nalong with robustness are stability and resilience. Stability is included because the term \nhas been used interchangeably with robustness and resilience is included because the \nterm has been used as a state beyond robustness. \n \nRL involves agents which take actions in an environment and experience at a reward for \nthose actions. The agent is to learn a policy that maximizes the cumulative reward. \nFormally, consider an agent operating over time ùë° ‚àà {1, ‚Ä¶ , ùëá}. At time t, the agent is in \nenvironment state st and produces an action ùëé! ‚ààùê¥. The agent then observes a new \nstate st+1 and receive a reward ùëü! ‚ààùëÖ. The set of possible actions A can be discrete or \ncontinuous. The goal of reinforcement learning is to find a policy ùúã(ùëé!|ùë†!) for choosing \nan action in state st to maximize a utility function or (expected return). [252] \n \nPREPRINT \n \n2 \nùêΩ(ùúã) = ùë¨\"!,$!,‚Ä¶ 67 ùõæ!ùëü(ùë†!, ùëé!)\n&\n!'(\n9 \n \nwhere 0 ‚â§ùõæ‚â§1 is a discount factor; ùëé!~ùúã(ùëé!|ùë†!) is drawn from the policy; and \nùë†!)*~ùëÉ(ùë†!)*|ùë†!, ùëé!) is generated by the environmental dynamics. The state value \nfunction \n \nùëâ+(ùë†!) = ùë¨$\",\"\"#$,‚Ä¶ 67 ùõæ,ùëü(ùë†!),, ùëé!),)\n&\n,'(\n9 \n \nis the expected return by policy p from state st. The state action function  \n \nùëÑ+(ùë†!, ùëé!) = ùë¨\"\"#$,$\"#$,‚Ä¶ 67 ùõæ,ùëü(ùë†!)*, ùëé!)*)\n&\n,'(\n9 \n \nis the expected return by policy p after taking action at at state st. [252] \n \nThe objective of this manuscript is to present a systematic review of RL literature to \nidentify metrics to measure the stability, robustness and resilience of RL. We limit RL \nto general reinforcement learning and not specialized RL, such as inverse RL. We \nreviewed papers that attempted to measure or otherwise characterize the stability, \nrobustness and resilience of RL, seeking metrics for these behaviors. \n \nWe searched computer science and technical literature databases for eligible papers, \ncombining RL, the behavior terms and terms related to measuring, metrics and \nquantification. The result comprised 16,015 items, which after removal of duplications \nand extraneous material, a collection of 546 items was established. Through a process of \nelimination described in full in the paper, we reduced the set to 248 papers. We \nsystematically reviewed those 248 papers, and the results are presented in this analysis.  \n \nWe classified the papers by behavior (i.e., stability (n=76), robustness (n=169), and \nresilience (n=3)) and identified the primary domains of application as robotics, network \nsystems, power system control and vehicle/traffic control and navigation. We identified \napproaches to determining or measuring each behavior individually and those across \nbehaviors. The approaches were categorized as quantitative or theoretical and the \nquantitative approaches were further classified as being applied internally (e.g., in \ntraining) or externally (e.g., performance measures on outputs) the model. Metrics, \nPREPRINT \n \n3 \napproaches and objectives were identified for each paper surveyed. The objective \nindicates to what the metric or approach was intended to be stable, robust or resilient. \nWe close by indicating the need to define stability, robustness and resilience behaviors \nfor RL and identify the quantitative and theoretical approaches to achieve measurement \nand determination of these behaviors. \n \nThere is a rich set of domains (i.e., 53 identified in this survey) in which measurement \nof RL stability, robustness and resilience have been conducted. The domains ranged \nfrom robotics and network systems to sheep herding and fish behavior. The most \nfrequently mentioned domains include robotics, general control and network systems, \nwith numerous papers not specifying a domain. Many papers used Gym [254] and other \nenvironments for demonstration. Though the search focused on quantitative \nmeasurement of stability, robustness and resilience, theoretical approaches were \nidentified as well. The quantitative approaches were categorized as internal or external, \ndependent upon where in the model the evaluation was held. Internal measures \nquantified the performance of the training, where external measures quantified the \nultimate performance of the model. \n \nThe goal of this systematic review was to identify metrics to measure the stability, \nrobustness and resilience of RL. To initiate the search for this review, we identified \nkeywords and phrases related to reinforcement learning, the behaviors of interest \n(stability, robustness and resilience) and measurement (shown in Table 1). \n \nTable 1. Keywords and Phrases \nKey Phrase \nBehavior \nMeasurement \nreinforcement \nlearning \nstability \nmetric \nmeasure \nrobust* \nindex \nscore \nresilien* \nquantifier \nindicator \n \n \nWe believe that this is the first comprehensive review of stability, robustness and \nresilience specifically geared towards RL. The remainder of the paper is organized as \nfollows. Section 2 describes the methods used in this systematic review. Section 3 \npresents the results of the review. Section 4 discusses the results of the review and \nintroduces a decision tree for metric selection based on the review. Section 5 provides \nsupplementary information. \n \n \nPREPRINT \n \n4 \n2.  Methods \n \nKeywords salient to RL, system behavior and measurement were identified for the \nresearch topic and are shown in Table 1. The typical search was of the form \n \n<Key Phrase> + <Behavior> + <Measurement> \n \nwith <Key Phrase>, <Behavior> and <Measurement> defined in Table 1.  A specific \nexample is  \n \n‚Äúreinforcement learning‚Äù AND robust* AND (‚Äúmetric‚Äù OR ‚Äúmeasure‚Äù OR ‚Äúindex‚Äù OR \n‚Äúscore‚Äù OR ‚Äúquantifier‚Äù OR ‚Äúindicator‚Äù) \n \nMultiple searches were conducted in bibliographic databases covering the broad areas of \ncomputer science, physical and biological sciences and engineering. See Table 12 for a \nlist of information sources used in this study. No restrictions were placed on the \npublication‚Äôs date or language. Journal articles, books, books in a series, book sections \nor chapters, edited books, theses and dissertations, conference papers, and technical \nreports containing the keywords and phrases were included in the search. The \npublication date of search results returned are bound by the dates of coverage of each \ndatabase and the date on which the search was performed, however all searches were \ncompleted by October 31, 2020. The range of dates for the documents ultimately \nincluded in the review was 2002-2020. \n \nThe databases queried resulted in 16,015 citations being collected. Irrelevant citations \nwere also unwittingly retrieved. We removed extraneous studies resulting in a collection \nof 699 publications. Further, removing duplicate papers resulted in 580 publications. \nCitations for ‚ÄúFull Conference Proceedings‚Äù were removed if relevant paper(s) within \nthe associated conference were otherwise collected, resulting in 546 publications. Further \nrefinement excluded publications that were not on RL, that were not on the searched \nbehavior or those that had no metrics or theoretical content, resulting in 248 \ndocuments. As a result, we systematically reviewed 248 papers and the results are \npresented in this analysis. See Figures 10, 11, and 12 for graphic summaries of the data \nreduction methodology for stability, robustness and resilience behaviors, respectively. \nSee Table 13 for a tabular summary of the data reduction. In addition, see Checklist 1 \nfor the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-\nAnalyses) guidelines used for the evaluation of the papers.  \n \nPREPRINT \n \n5 \nThe 248 papers that made it through the screening process were grouped by the \nbehavior searched, that is Stability, Robustness, and Resilience. We also identified those \npapers on one behavior that mention one or both of the other behaviors. Some papers \nthat mentioned other behaviors did so interchangeably. For instance, stability and \nrobustness were used interchangeably in several papers, which can lead to some of the \nconfusion that exists in the definitions of these behaviors. The primary domains of \napplication were identified and categorized as robotics, network systems, general control \nsystems and Gym [254] and other environments. We also identified those publications \nthat mentioned the RL policy. \n \nThe primary focus of the paper is to identify approaches to determining or measuring \neach behavior. Of course, most of the publications focused on quantitative approaches \nbecause of the search terms used. The ones that use a theoretical approach provide \nadditional insight into the behavior determination problem. The quantitative \napproaches were further classified as being applied internal (e.g., in training) or external \n(e.g., performance measures on outputs) the model. Metrics, approaches and objectives \nwere identified for each paper surveyed (see Figure 1). The objective indicates to what \nthe metric or approach was intended to be stable, robust or resilient. \n \nThere is little agreement on the definitions of stability, robustness and resilience in the \nliterature. In fact, there are few distinct definitions of these behaviors. For this review, \nwe use the following definitions. \n \nStability is a property of the learning algorithm (that is, a small change in the training \nset results in a similar model) and refers to the ranking of the variance of a model [253]. \nFor example, if we use variance of the loss function over all datasets as a performance \nmeasure and test a set of models. The smallest loss indicates the more stable model. \nGiven this definition, stability analysis is the application of sensitivity analysis to \nmachine learning. \n \nRobustness, when used with respect to computer software, refers to an operating \nsystem or other program that performs well not only under ordinary conditions but \nalso under unusual conditions that stress its designers‚Äô assumptions \n(http://www.linfo.org/robust.html). Robustness then is a property of the model and is \nmeasured by the, e.g., loss over all datasets (as opposed to the variance of the loss). \n \nPREPRINT \n \n6 \nThroughout the literature, resilience has been used interchangeably with robustness, \nhowever, it is used most often with production machine learning systems to indicate \nrobustness to different data sets and different data added to the data set. \n \n \n \n \nFigure 1. Categorization and resulting metrics, approaches and objectives \n \n \n3.  Results and Analysis \n \nThe publications were categorized by behavior (see Table 2): Stability (n=76), \nRobustness (n=169), and Resilience (n=3). Papers on one behavior often mention the \nother behaviors, especially Stability and Robustness (see Figure 2). Resilience is \nmentioned in 5 Stability papers and in 11 Robustness papers. Robustness is mentioned \nin 50 Stability papers and in 1 Resilience paper. Stability is mentioned in 104 \nRobustness papers and in all (3) Resilience papers. \n \nTable 2. Citations categorized by behavior \nBehavior \nCitations \nTotal \nStability \n[4-80] \n76 \nRobustness \n[81-250] \n169 \nResilience \n[1-3] \n3 \n \nTotal \n248 \n \nPREPRINT \n \n7 \nGiven the recent explosion of papers on robustness of neural networks to adversarial \nattacks, one might expect it to be a cornerstone of the robustness papers reviewed \nherein. The term ‚Äúadversarial‚Äù is mentioned in a quarter (n=61, N=248) of the papers \nreviewed (see Figure 3). That is, 1 Resilience paper, 56 Robustness papers and 4 \nStability papers mention ‚Äúadversarial‚Äù. Some papers on one behavior used one of the \nother behaviors interchangeably, notably stability and robustness, specifically [91, 93, \n105, 145, 146, 179, 194, 225, and 237] and generally in several other articles. \n \n \n \n \nFigure 2.  Stability, robustness and resilience papers were mentioned in papers on \nother behaviors. For example, Robustness is mentioned in 104 Robustness papers and in \nall 3 Resilience papers.  \n \n \n \nPREPRINT \n \n8 \n \nFigure 3. Number of papers in which ‚Äúadversarial‚Äù is mentioned, categorized by \nbehavior. For example, 56 Robustness papers mention ‚Äúadversarial‚Äù. \n \n \n3.1 \nApplication Domains \n \nThe publications‚Äô application domains are provided in Table 3 and illustrated in Figure \n4. The primary domains were robotics with 16.4% (n=44) of the total citations \n(N=268), followed by network systems and general control (each with 7.8%, n=21), \nwith 9.3% (n=25) using Gym or other environments as their experiment domain. Just \nas many (n=25, 9.3%) did not specify a domain. These top 5 (of 53) domains comprised \nover 50% (52.9%, n=136) of the citations. Most (52.8%, n=28) of the domains (n=53) \nhad a single citation each.  \n \n \n \n \nPREPRINT \n \n9 \nTable 3. Citations categorized by application domain \nDomain \nBehavior(s) Citations \nTotal \nRobotics \nStability, \nRobustness \n[9, 12, 16, 21, 27, 31, 44, 45, 46, \n51, 56, 60, 70, 77, 78],  \n[82, 86, 103, 109, 116, 118, 122, \n125, 128, 133, 134, 140, 142, 149, \n151, 158, 162, 164, 178, 190, 208, \n211, 212, 217, 221, 224, 236, 242, \n249] \n44 \nGym and other \nenvironments \nStability, \nRobustness \n[8], [87, 101, 146, 182, 185, 191, \n192, 195, 199, 200, 208, 109, 213, \n225-229, 231, 233, 240, 245, 246, \n250] \n25 \nGeneral, non-specified \nRobustness \n[132, 136, 145, 150, 171, 173, 174, \n181, 184, 185, 195, 197, 198, 200, \n205, 215, 216, 219, 223, 225, 229, \n240, 247, 248, 250] \n25 \nNetwork Systems \nStability, \nRobustness, \nResilience \n[6, 10, 11, 14, 32, 42, 66, 73, 77],  \n[81, 84, 89, 96, 98, 104, 113, 115, \n127, 203, 214],  \n[3] \n21 \nControl, not otherwise \nnoted \nStability, \nRobustness \n[64, 68, 69, 71, 74, 76, 79], \n[135, 137, 147, 163, 175, 188, 189, \n194, 210, 218, 228, 230, 233, 244] \n21 \nVehicle/Traffic Control \nand Navigation, \nCollision Avoidance \nStability, \nRobustness \n[16, 20, 38, 47, 49, 58, 65], [83, \n100, 110, 111, 114, 123, 166, 177, \n195, 226] \n17 \nGames/Game Systems \nStability, \nRobustness \n[24, 30, 33, 35, 37, 52], [156, 165, \n168, 193, 201, 204, 220, 234, 239, \n250] \n16 \nPower System Control \nStability, \nRobustness \n[4, 10, 15, 22, 23, 36, 43, 48, 50, \n59, 67], [105, 107, 117, 172] \n15 \nImage Analysis \nStability, \nRobustness \n[72], [88, 93, 94, 99, 108, 148, 155, \n235] \n9 \nNonlinear Dynamic \nSystems/Processes \nStability \n[5, 25, 41, 54, 61, 62] \n6 \nContinuous Control \nRobustness \n[157, 191, 231, 237, 245] \n5 \nPREPRINT \n \n10 \nTable 3. Citations categorized by application domain \nDomain \nBehavior(s) Citations \nTotal \nMulti-agent Systems \nStability, \nRobustness \n[18, 29, 52], [241] \n4 \nDomain Agnostic \nStability \n[7, 28, 40] \n3 \nManufacturing Systems \nStability, \nRobustness \n[10, 63], [95] \n3 \nRide-share Dispatching, \nDelivery \nStability, \nRobustness \n[19], [97, 160] \n3 \nMedical - System \nControl, Medication \nLevel/Control \nRobustness \n[112, 144, 187]  \n3 \nAutonomous Systems \nRobustness \n[143, 196, 207] \n3 \nSecurity & Cyber \nDefense, Spammer \nDetection \nRobustness \n[138, 159, 179] \n3 \nUncertain Non-linear \nSystems \nRobustness \n[106, 121] \n2 \nConversation and \nSpeech \nRobustness \n[119, 124] \n2 \nText Analysis, Machine \nTranslation \nStability, \nRobustness \n[13], [85] \n2 \nMulti-armed Bandit \nStability, \nRobustness \n[74], [120]  \n2 \nFeedback Controller \nStability, \nRobustness \n[73], [129] \n2 \nInformation Retrieval \n(search) \nStability, \nRobustness \n[17], [161] \n2 \nEconomics - \nQuantitative \nInvestment, Trading \nSystems, Markets \nRobustness \n[102, 130, 206] \n2 \nVideo Presentation \nQuality \nStability \n[26] \n1 \nBrain-Machine Interface \n(BMI) Controller \nStability \n[34] \n1 \nSkill Acquisition \nStability \n[53] \n1 \nPREPRINT \n \n11 \nTable 3. Citations categorized by application domain \nDomain \nBehavior(s) Citations \nTotal \nTechnical Process \nFeedback \nStability \n[55] \n1 \nLinear Quadratic \nOutput Tracking \nStability \n[39] \n1 \nDecentralized Control \nRobustness \n[90] \n1 \nPolicy-selection Races \nRobustness \n[126] \n1 \nComplex Adaptive \nSystems \nResilience \n[1] \n1 \nEmission Control \nStability \n[57] \n1 \nFlight Simulator \nTraining \nResilience \n[2] \n1 \nMarkov Decision \nProcess \nStability \n[75] \n1 \nCellular \nCommunications \nRobustness \n[167] \n1 \nFish Behavior \nRobustness \n[141] \n1 \nSpace Telescope \nRobustness \n[131] \n1 \nBrain Modeling \nRobustness \n[139] \n1 \nSensors \nRobustness \n[152] \n1 \nUnsupervised Goal \nExploration \nRobustness \n[153] \n1 \nDirected Graph \nEmbedding \nRobustness \n[154] \n1 \nHVAC Control \nRobustness \n[169] \n1 \nTrain \nRobustness \n[170] \n1 \nTeamwork \nRobustness \n[176] \n1 \nCausal Discovery \nRobustness \n[180] \n1 \nTurbo Fan Engines \nRobustness \n[183] \n1 \nSheep Herding \nRobustness \n[186] \n1 \nVan de Pol Oscillator \nRobustness \n[222] \n1 \nSingle Episode Transfer \nRobustness \n[232] \n1 \nMulti-Task Batch \nRobustness \n[238] \n1 \nSpectral Efficiency \nRobustness \n[243] \n1 \n \nTotal Citations \n268 \nThe categories are not mutually exclusive. \nTotal Domains \n53 \nPREPRINT \n \n12 \n \n \n \nFigure 4. Application Domain Categories \n \n \n \n3.2 \nReinforcement Learning Policies \n \nThe types of RL policies mentioned in the articles are provided in Table 4. Most \ndocuments did not identify the policy used. If a behavior is not provided in Table 4 for \na particular policy, that policy was not discussed in the papers on that behavior. Of the \n21 types of policies mentioned, the top 4 ‚Äì Actor-Critic (n=18), Q-learning (n=16), \nProximal Policy Optimization (PPO) (n=8) and Adaptive Critic Design (n=5) comprise \n72.3% of the total citations that included policy (n=65). \n \n \n \n \nPREPRINT \n \n13 \nTable 4. Citations categorized by the reinforcement learning policy used  \nPolicy \nBehavior \nCitations \nSubtotal \nTotal \nActor-Critic \nStability \n[5, 10, 46, 47, 49, 51, 65, \n68, 69, 77, 79] \n11 \n18 \nRobustness [131, 155, 180, 185, 191, \n192, 222] \n7 \nQ-Learning \nStability \n[18, 19, 39, 66, 72-75] \n6 \n16 \nRobustness [92, 94, 107, 109, 113, \n130, 138, 223, 236] \n9 \nResilience \n[3] \n1 \nPPO \nRobustness [143, 147, 149, 158, 161, \n162, 166, 208] \n8 \n8 \nAdaptive Critic \nDesign \nStability \n[43, 54, 61] \n3 \n5 \nRobustness [110, 125] \n2 \nDeep \nDeterministic \nPolicy Gradient \n(DDPG) \nRobustness [162, 167] \n2 \n2 \n4 variational \nModel-based \nPolicy \nOptimization \nRobustness [181] \n1 \n1 \nAdvantage Actor \nCritic (A2C) \nRobustness [143] \n1 \n1 \nAd-hoc On-\ndemand Distance \nVector (AODV) \nRobustness [123] \n1 \n1 \nActive Tracking \nTarget Network \n(ATTN) and \nAnytime Reduced \nValue Iteration \n(ARVI) \nRobustness [177] \n1 \n1 \nConstant \nFeedback to \nControl Policy \nRobustness [106] \n1 \n1 \n \nPREPRINT \n \n14 \nTable 4. Citations categorized by the reinforcement learning policy used  \nPolicy \nBehavior \nCitations \nSubtotal \nTotal \nConstraint-\ncontrolled PPO \n(CPPO) \nRobustness [164] \n1 \n1 \nData-regulated \nActor-Critic \n(DrAC) \nRobustness [171] \n1 \n1 \nDeep \nDeterministic \nPolicy Gradient \nStability \n[70] \n1 \n1 \nGeneralized \nAdvantage \nEstimation \nRobustness [158] \n1 \n1 \nGoal Policy \nRobustness [153] \n1 \n1 \nGradient \nRobustness [179] \n1 \n1 \nGraph-based \nPolicy Learning \nRobustness [176] \n1 \n1 \nLyapunov-based \nActor-Critic \n(CLAC) \nRobustness [183] \n1 \n1 \nMOOSE (MOdel-\nbased Offline \npolicy Search with \nEnsembles) \nRobustness [151] \n1 \n1 \nNatural \nStochastic Policies Robustness [184] \n1 \n1 \nStudent t Policy \nRobustness [146] \n1 \n1 \n  \nTotal Citations including Policy \n65 \n  \nTotal Policies \n21 \n \n \n3.3 \nApproach to Determining or Measuring Behavior \n \nThe publications‚Äô approaches to determining or measuring each behavior are categorized \nas either quantitative or theoretical (Table 5). Most of the publications focused on \nquantitative approaches (n=205, 82.0%), which is understandable given that the search \nPREPRINT \n \n15 \nfocused on quantifying the behaviors. For publications on the stability behavior, there \nwas an almost even split between quantitative (n=42) and theoretical (n=43) \napproaches. However, publications on the robustness behavior were primarily focused on \nquantitative approaches. All (3) resilience publications applied quantitative approaches.  \n \n \nTable 5. Citations categorized by approach to determining or measuring behavior \nApproach \nBehavior \nCitations \nTotal \nQuantitative \nStability \n[4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, \n20, 21, 23, 26, 27, 28, 29, 30, 31, 32, 34, 36, 38, \n42, 43, 45, 50, 51, 53, 58, 59, 66, 70, 72, 73, 75, \n78, 80] \n42 \nRobustness [81-89, 92-104, 106-120, 122-196, 198-201, 203-\n222, 224-227, 229, 231-247, 249-250] \n160 \nResilience \n[1, 2, 3] \n3 \n \n \nTotal Quantitative \n205 \nTheoretical \nStability \n[4, 5, 10, 13, 15, 16, 20, 22, 24, 25, 27, 33, 35, \n36, 37, 39, 40, 41, 43, 44, 46, 47, 48, 49, 52, 54, \n55, 56, 57, 60, 61, 62, 63, 64, 65, 67-69, 71, 74, \n76-79] \n44 \nRobustness \n[90, 91, 97, 105, 116, 121, 135, 145, 150, 165, \n184, 185, 194, 195, 197, 200-202, 205, 206, 215, \n216, 222, 223, 225-230, 233, 239, 240, 248, 250] \n35 \nResilience \n-- \n0 \n \n \nTotal Theoretical \n79 \nIf a document covered both quantitative and theoretical approaches, it was placed in both \ncategories. \n \n3.3.1 Types of Quantitative Approaches \n \nNext, we further categorize the quantitative approaches into whether they are focused \ninternal or external to the model (see Table 6). Internal quantitative approaches \nmeasure aspects within the model, e.g., its training and associated measures such as the \nvalue of rewards over time or the number of episodes until convergence. External \nquantitative approaches measure performance-related aspects of the model, e.g., \nvariations in accuracy or throughput. Most (n=141, 63.2%) of the quantitative \napproaches were categorized as performance-related, or external, measures. Of these, \nmost (n=103) were on robustness, with stability (n=36) next. The 3 papers on resilience \nPREPRINT \n \n16 \nfocused on performance-related quantitative measures. Robustness also led the internal \napproaches (n=68) with stability following (n=14). These are primarily due to the large \nnumber of robustness papers (n=170) and paucity of resilience papers (n=3) overall. Of \nthe robustness papers, 40.0% (n=68) contained internal quantitative measures and \n60.6% contained external quantitative measures. For stability, these values are 18.2% \nand 46.8%, respectively. \n \nTable 6. Quantitative approaches categorized by internal or external measures \nQuantitative \nApproach \nBehavior \nCitations \nTotal \nInternal \nStability \n[7, 8, 9, 11, 13, 19, 20, 27, 29, 30, 43, 72, 75, \n78] \n14 \nRobustness \n[83, 84, 92, 94, 97, 101-103, 109, 114, 122, 130, \n132, 134, 138, 143, 146, 149-151, 155-157, 163-\n164, 166, 174-176, 181-182, 185, 187-189, 191-\n193, 195, 198-200, 204-205, 208-209, 211-213, \n215, 217, 220, 226, 229, 231-239, 241, 244-247, \n250] \n69 \nResilience \n-- \n0 \n \n \nTotal Internal Measures \n83 \nExternal \nStability \n[6, 9, 10, 12, 13, 14, 15, 17, 18, 19, 20, 21, 23, \n26, 27, 28, 31, 32, 34, 36, 38, 42, 45, 50, 51, 53, \n58, 59, 66, 68, 70, 73, 75, 78, 80] \n36 \nRobustness \n[81-83, 85-89, 93, 95-100, 102, 104, 106-120, \n123-129, 131, 133, 135-145, 147-148, 152-154, \n158-162, 165-173, 177-181, 183-184, 186, 190, \n194-196, 201, 203-204, 206-207, 210, 214, 216, \n218-219, 221-222, 224-227, 240, 242-243, 249] \n103 \nResilience \n[1, 2, 3] \n3 \n \n \nTotal Performance-related Measures \n142 \nIf there were both internal and external quantitative approaches in a document, the document \nwas placed in both categories. \n \n \n3.3.2 Types of Internal Quantitative Approaches \n \nLooking at the types of internal quantitative approaches, we see a fairly narrow set of \naspects being considered in the papers (see Table 7). These are metrics specifically made \nPREPRINT \n \n17 \nto measure stability other than by the variance of the output. They essentially measure \nvariation in training performance. The vast majority (n=75, 88.2%) of the internal \nquantitative approaches calculate reward- or score-based metrics. Other types of \ninternal quantitative approaches include 2 each of policy entropy, variations in control \nstrategy approximation weights, and the convergence rate, and 1 each of policy weight, \ncalculation of the Lyapunov stability criteria and calculation of the Wasserstein function \nlower bound. The term convergence, in RL context, refers to the stability of the learning \nprocess (and the underlying model) over time [11]. \n \n \nTable 7. Internal Quantitative approaches categorized by metric \nInternal Quantitative \nMetric \nBehavior \nCitations \nTotal \nReward or Score ‚Äì \nmagnitude, mean/ \nvariance, variation in \naverage reward, time to \nthreshold, episode \nduration \nStability, \nRobustness \n[7, 8, 9, 13, 30, 72, 75, 78] \n[83, 84, 92, 94, 97, 101-103, 109, 114, \n122, 130, 132, 134, 138, 143, 146, \n149-151, 155-157, 163-164, 166, 174-\n176, 181-182, 185, 188-189, 191-193, \n195, 198-200, 204, 208-209, 211-213, \n215, 217, 220, 226, 229, 231-239, 241, \n244-247, 250] \n75 \nPolicy entropy \nStability \n[11, 19] \n2 \nVariations in control \nstrategy approximation \nweights \nStability, \nRobustness \n[20] \n[120] \n2 \nConvergence rate \nStability \n[27, 29] \n2 \nLyapunov stability criteria \ncalculated \nStability \n[43]  \n1 \nPolicy weight \nRobustness [231] \n1 \nRegret \nRobustness [187] \n1 \nWasserstein function \nbounds calculated \nRobustness [205] \n1 \n \n \nTotal \n85 \nIf a document had metrics that fell in more than one category, the document was placed in each \nof the categories. \n \n \nPREPRINT \n \n18 \n3.3.3 Types of External Quantitative Approaches \n  \nThe external or performance-based quantitative approaches to measuring the behaviors \nprimarily (n=39) used deviations or variation in performance-related metrics other than \nprecision, accuracy or recall (see Table 8 and Figure 5). The next category (n=28) of \nquantitative metrics used error, failure and success rates. Statistics on the performance \nof the tracking or estimation error follows with n=23 papers. Papers in the network \ndomain used network-related metrics (n=15) to measure the behavior. Statistics on \nprecision, accuracy and recall (n=12) followed. Five papers used variance in loss or \nregret estimation, 3 papers used game-related performance measures to quantify \nbehavior and 2 papers each used bounds on or the size of the stability region and \nterminal wealth and inventory. Eighteen (18) additional different types of external \nquantitative metrics categories were represented by a single paper each. \n \n3.3.4 Quantitative Approach Objectives \n \nAn additional aspect reviewed was to what action or event were the quantitative \napproaches attempting to be stable, robust or resilient. We call this the <behavior> \nobjective. The <behavior> objective category (see Table 9) with the highest number of \ncitations was geared toward handling changes in the operational environment or a \ndynamic environment or network (n=41). Papers that did not specifically state their \nobjective comprised the next most populous category (n=35). The objective of handling \nuncertainty and disturbances in the environment also contained n=35 papers. The \nremaining objectives included input variation/perturbations (n=20); differences between \ntraining and test or operational environments (n=19); differences or uncertainties in \nmodel parameters (n=16); adversarial attack (n=14); different domains, environments \nor settings (n=8); errors or failures in operational environment (n=5); differences in \ntraining data sets or initializations (n=5); high variability (n=2) and one paper each in \nsystematic pressure, spamming, incomplete data, and unknown control coefficients. \n \n \n \n \n \n \nPREPRINT \n \n19 \nTable 8. External Quantitative approaches categorized by metric \nExternal Quantitative Metric \nBehavior \nCitations \nTotal \nDeviations/variation in other (than \nprecision, accuracy and recall) \nperformance-related metrics \nStability, \n \n \nRobustness, \n \n \n \n \nResilience \n[9, 13, 15, 19, 20, 26, 27, \n36, 38, 45, 50, 51, 58, 59, \n68, 80] \n[85, 95, 97, 106, 107, \n110, 111, 119, 126, 137-\n139, 141, 142, 152, 169, \n178, 179, 181, 183, 184, \n195] \n[2] \n39 \nError and failure rates/success rate \nStability, \nRobustness \n[12] \n[86, 88, 100, 104, 112, \n124, 129, 131, 133, 136, \n140, 143, 153, 158, 160, \n162, 166-168, 186, 204, \n207, 221, 222, 225, 242, \n249]  \n28 \nPerformance of tracking/trajectories \nestimation error; mean absolute \ndeviation, mean square error, mean \nabsolute percentage error, margins \nand magnitude of correlation \ncoefficient \nStability, \nRobustness \n[10, 18, 23, 28, 31] \n[87, 88, 93, 96, 106, 116, \n125, 144, 145, 147, 148, \n158, 173, 186, 201, 224, \n226, 240] \n \n \n23 \nNetwork-related timing/delay, path \nand link metrics, connectivity, \ndelivery ratio, routing loops, path \noptimality, visitation distribution, \nstructural Hamming distance, Small \nbase station-serving ratio, sum-rate \nand 5th percentile rate \nStability,  \nRobustness \n[6, 14, 32, 42, 53, 66, 73] \n[83, 89, 123, 127, 170, \n180, 203, 214]  \n15 \nMean/average and variation in \naccuracy, precision and recall, area \nunder the receiver operating \ncharacteristic (ROC) curve (AUC) \nStability,  \nRobustness,  \n \nResilience \n[17, 34] \n[93, 99, 102, 108, 113, \n154, 159, 161, 168, 227] \n[3] \n12 \nVariance of the estimation of loss, \nregret \nRobustness \n[118, 159, 187, 216, 224]  \n5 \nPREPRINT \n \n20 \nTable 8. External Quantitative approaches categorized by metric \nExternal Quantitative Metric \nBehavior \nCitations \nTotal \nGame-related performance - scores of \ngame playing, percent wins, \nexploitability \nRobustness \n[109, 120, 165] \n3 \nSize of the stability region; bounds \nStability, \nRobustness \n[21] \n[135] \n2 \nTerminal wealth, terminal inventory, \ncost, Sharpe ratio \nRobustness \n[206, 218]  \n2 \nAverage proportion of failed \neavesdropping attempts and of \njammed red-force nodes; Average \nthroughput \n \nRobustness \n \n[81] \n \n1 \nHours of operation with some \nmaintenance \nRobustness \n[82] \n1 \nResponse time, energy consumption, \nand execution time \nRobustness \n[98] \n1 \nSpectral efficiency \nRobustness \n[243] \n1 \nTime headway (sec) \nRobustness \n[210] \n1 \nCovariance analysis as a metric \nRobustness \n[114] \n1 \nMutual information \nRobustness \n[177]  \n1 \nFidelity \nRobustness \n[219] \n1 \nExpected rank and Robust \nMeasurements metric \nRobustness \n[115]  \n1 \nSingular value decomposition (SVD)-\nbased controllability measure \nRobustness \n[117]  \n1 \nTime to find goal/destination \nRobustness \n[128]  \n1 \nNumber of adversarial actions \nrequired to cause error \nResilience \n[1] \n1 \nNormalized Energy Stability Margin \n(NESM) \nStability \n[70] \n1 \nVoltage violation rate, active power \nloss \nRobustness \n[172]  \n1 \nJensen-Shannon divergence (JSD) \nRobustness \n[171]  \n1 \nNumber of successful steps \nRobustness \n[190]  \n1 \nBlood glucose responses, Insulin \nconcentration \nRobustness \n[194]  \n1 \nPREPRINT \n \n21 \nTable 8. External Quantitative approaches categorized by metric \nExternal Quantitative Metric \nBehavior \nCitations \nTotal \nLikelihood (Mahalanobis Distance) \nRobustness \n[196]  \n1 \n \n \nTotal \n147 \nIf a paper contained quantitative approaches belonging to multiple categories, it was placed in \neach of the relevant categories. \n \n \n \nFigure 5. External quantitative metrics \n \n \n \n \n25%\n19%\n16%\n10%\n8%\n5%\n17%\nDeviations/variation in other\nperformance-related metrics\nError and failure rates, success rate\nPerformance of tracking/ estimation\nerror / correlation coefficient\nmagnitude\nNetwork-related timing/delay, path\nand link metrics\nMean/average and variation in\naccuracy, precision and recall, AUC\nVariance of the estimation of loss,\nregret\n1-3 citations each\nPREPRINT \n \n22 \n \nFigure 6. Quantitative <behavior> objectives \n \n3.3.5 Types of Theoretical Approaches \n \nA majority of the types of theoretical approaches in the papers reviewed were based on \nLyapunov theory (n=50, 61.0%) (Table 10). The next highest types of theoretical \napproaches used are convergence to Nash equilibrium (n=10) and value-based \nguarantees such as error and output deviation bounds (n=8). Of the remainder, 3 \npapers used the Wasserstein distance to explore stability, 3 papers proved the methods \nwere doubly robust, 2 papers proved the methods exhibited Lipschitz continuity, and \nstochastic stability theory to prove stability, stability guarantees, policy-based \nguarantees, regret bounds, minimization of the Jacobian on input, and per-episode \nBellman-error regret guarantees/bounds were used by a single paper each to establish \nstability of the RL methods discussed. \n \nPREPRINT \n \n23 \nTable 9. Quantitative <behavior> objectives \n<behavior> Objective \nBehavior(s) Citations \nTotal \nChanges in the operational \nenvironment/dynamic environment or \nnetwork; distribution shift \nStability \n \nRobustness \n \n \n \n \n \nResilience \n[26, 27, 32, 45, 50, 66, \n70, 73] \n[85, 86, 89, 93, 94, 96, \n98, 99, 102, 107, 108, \n111, 112, 114, 117, 118, \n122, 123, 128, 131, 142, \n154, 167, 171, 172, 177, \n187, 194, 207, 214, 220, \n243] \n[3] \n41 \nNot specifically stated \nStability \n \n \nRobustness \n[6, 8, 9, 10, 11, 13, 14, \n15, 17, 18, 20, 21, 23, \n28, 31, 38, 43, 51, 58, \n59, 78] \n[113, 116, 125, 127, 130, \n134, 165, 184, 198, 209, \n216, 227, 240, 247] \n35 \nUncertainty and disturbances in \nenvironment, e.g., noisy sensor data, \nmeasurement noise, distractors, \nnuisances \nRobustness \n[133, 135, 138, 140, 141, \n147, 148, 152, 153, 162, \n169, 173, 175, 178, 186, \n188, 190, 191, 193, 195, \n196, 199, 208, 211, 217, \n218, 219, 222, 224, 225, \n234-236, 244, 249] \n35 \nInput variation/perturbations, \noutliers \nStability \nRobustness \n[7, 12, 19] \n[101, 103, 106, 132, 139, \n143, 146, 149, 157, 158, \n170, 187, 195, 208, 221, \n226, 231] \n20 \nDifferences between training and test \nor operational environments \nStability \nRobustness \n[31, 42] \n[82-84, 87, 95, 100, 109, \n119, 120, 124, 129, 136, \n149, 174, 191, 221, 245] \n19 \n \n \n \nPREPRINT \n \n24 \nTable 9. Quantitative <behavior> objectives \n<behavior> Objective \nBehavior(s) Citations \nTotal \nDifferences/uncertainties in model \n[hyper-]parameters, model error \nStability \nRobustness \n[36, 53, 75] \n[92, 110, 126, 137, 164, \n174, 181, 182, 192, 201, \n225, 237, 241] \n16 \nAdversarial attack \nStability \nRobustness \n \nResilience \n[29, 30]  \n[81, 97, 156, 159, 163, \n166, 204, 206, 210, 212] \n[1, 2] \n14 \nDifferent domains, environments or \nsettings \nStability \nRobustness \n[80] \n[161, 164, 176, 203, 224, \n232, 242] \n8 \nErrors or failures in operational \nenvironment \nRobustness \n[104, 115, 145, 168, 189] \n5 \nDifferences in training data sets or \ninitializations \nStability \nRobustness \n[34] \n[88, 151, 213, 238] \n5 \nHigh variability \nRobustness \n[144, 183] \n2 \nSystematic pressure, e.g., sudden \nsurge of requests \nRobustness \n[160] \n1 \nSpamming \nRobustness \n[179] \n1 \nIncomplete data \nRobustness \n[180] \n1 \nUnknown control coefficients \nStability \n[68] \n1 \n \n \nTotal \n204 \nIf a technique had multiple robustness objectives, it was placed in each of those objectives. \n \n \n \n \n \n \nPREPRINT \n \n25 \nTable 10. Theoretical approaches categorized by specific approach \nTheoretical Approach Behavior \nCitations \nTotal \nLyapunov stability \ntheory \nStability \n[4, 5, 10, 15, 16, 20, 25, 35, 36, 39, \n40, 41, 43, 44, 46, 47, 48, 49, 52, 54, \n55, 56, 57, 60, 61, 62, 64, 65, 67, 69, \n74, 76-79] \n50 \nRobustness [90, 91, 97, 105, 116, 121, 135, 145, \n194, 200, 222, 225, 230, 233, 248] \nConvergence to Nash \nequilibrium \nStability \n[22, 24, 37, 63, 71] \n10 \nRobustness [165, 197, 206, 215, 230, 239] \nValue-based guarantees, \nerror bounds, output \ndeviation bounds \nRobustness [195, 201, 226-228, 230, 233, 250] \n8 \nWasserstein distance \nRobustness [185, 201, 205] \n3 \nProve double robustness \nRobustness [184, 216, 240] \n3 \nProve Lipschitz \ncontinuity \nRobustness [201, 229] \n2 \nStochastic stability \ntheory \nStability \n[33] \n1 \nProve stability \nguarantees \nStability \n[68] \n1 \nPolicy-based guarantees \nRobustness [201] \n1 \nRegret bounds \nRobustness [202] \n1 \nMinimize Jacobian on \ninput \nRobustness [150] \n1 \nPer-episode Bellman-\nerror regret \nguarantees/bounds \nRobustness [223] \n1 \n \n \nTotal \n82 \nIf a paper used multiple theoretical approaches, the was placed in each of those categories. \n \n \nPREPRINT \n \n26 \n \nFigure 7. Theoretical Approaches \n \n \n3.3.6 Theoretical Approach Objectives \n \nWe also reviewed the <behavior> objective for theoretical papers (Table 11). Most \n(n=42, 54.5%) papers on theoretical approaches did not state their objective. Of those \nfew that did, changes or dynamics in the operational environment was the most \nfrequent objective (n=10), followed by differences or uncertainties in model parameters \n(n=7), adversarial attack (n=6), error or failure (n=5), differences between training and \ntest or operational environments (n=2), input variation (n=2), then 1 each for domain \nshifts, different function approximation architectures and differences in quantization \nlevels. \n \n4.  Discussion \n \nOur study was conducted to characterize published means of measuring or determining \nthe stability, robustness or resilience of RL. Out of an initial collection of 16,015 items \n248 papers met inclusion criteria and were systematically reviewed. Approaches to \nmeasuring or determining the behavior were classified as either quantitative or \ntheoretical. Quantitative approaches were further classified as internal or external \ndepending on whether they evaluated the training phase or the test or operational \nphases. For both categories of quantitative approaches, we categorized the metrics used, \nwith internal approaches primarily using the reward or score (and statistics on same) \nand external approaches primarily using variations on performance-related metrics  \n59%\n13%\n10%\n18%\nLyapunov stability theory\nConvergence to Nash\nequilibrium\nValue-based guarantees,\nerror bounds, output\ndeviation bounds\n1-3 citations each\nPREPRINT \n \n27 \nTable 11. Theoretical <behavior> objectives \n<behavior> Objective \nBehavior(s) Citations \nTotal \nNot specifically stated \nStability \n \n \n \nRobustness \n[4, 5, 10, 15, 16, 20, 22, \n24, 25, 33, 37, 39, 41, \n43, 44, 46, 47, 48, 49, \n52, 54, 56, 57, 60, 61, \n62, 63, 64, 65, 67, 74, \n76, 78, 80] \n[90, 105, 116, 121, 165, \n184, 197, 200, 216, 227, \n240] \n42 \nChanges in the operational \nenvironment/dynamic environment, \nenvironment uncertainties, \nenvironment disturbances \nStability \nRobustness \n[35, 79] \n[135, 185, 194, 195, 222, \n225, 228, 248] \n10 \nDifferences/uncertainties in model \nparameters \nStability \nRobustness \n[36, 55, 68] \n[91, 201, 205, 225] \n7 \nAdversarial attack, corruption, \nperturbations \nRobustness \n[150, 206, 215, 223, 230, \n239] \n6 \nError, failure \nStability \nRobustness \n[69, 71] \n[135, 145, 202] \n5 \nDifferences between training and test \nor operational environments \nStability \nRobustness \n[40] \n[215] \n2 \nInput variation or perturbation \nRobustness \n[195, 226] \n2 \nDomain shifts \nRobustness \n[229] \n1 \nFunction approximation architecture \nRobustness \n[233] \n1 \nQuantization level \nRobustness \n[250] \n1 \n \n \nTotal \n77 \nIf a paper used multiple theoretical approaches, it was placed in each of those categories. \n \n \n(though not precision, accuracy or recall). Theoretical approaches were dominated by \nthe use of Lyapunov stability theory. We further characterized the objectives of the \nstability, robustness and resilience behaviors. Quantitative approaches to measuring the \nbehavior focused on the ability to handle differences in the operational environment, \nwhereas the vast majority of theoretical approaches to determining the behavior did not \nspecifically state an objective. However, the objective of the theoretical approaches can \nPREPRINT \n \n28 \nbe implied by the use of Lyapunov stability theory, that is, to prove the stability of the \nsystem. Lyapunov was used regardless of whether the article was on stability or \nrobustness. \n \n \n \nFigure 8. Theoretical <behavior> objectives \n \n \nAs an aid to deciding which metric to use, we developed a decision tree based on the \ninformation obtained in this literature review. It is a collapsible tree so that branches \nare not exposed unless selected and open branches can be closed or collapsed. There are \nseveral levels in the decision tree, starting with the i) behavior (stability, robustness or \nresilience); ii) the domain (see Table 3); iii) a list of quantitative and theoretical \nobjectives (see Tables 9, 11); iv) the next level divides the metrics into external, internal \nand theoretical metrics (see Tables 5, 6); and v) the last level, i.e., the leaves, is the set \nof metrics for that branch of the decision tree (see Tables 7, 8). For example, suppose \nwe want to find a suitable metric to measure robustness of a control system that is \nexpected to face changes in the operational environment. From the metric decision tree \nshown in Figure 14, we see that the first selection is for a robustness metric. This \nselection displays the domains in which robustness metrics were described. Selecting the \nGeneral Control domain reveals 5 quantitative objectives and 4 theoretical objectives, \nincluding the objective ‚ÄúChanges in the Operational Environment‚Äù in both the \nquantitative and theoretical objectives. An external metric found in the literature for \nthis case is ‚Äúblood glucose response‚Äù which is not applicable for this control system. The \nmore appropriate metrics and approaches are Lyapunov stability theory and calculation, \n55%\n13%\n9%\n8%\n6%\n9%\nNot specifically stated\nOperational environment\ndynamics and uncertainties\nDifferences/uncertainties in\nmodel parameters\nAdversarial attack,\ncorruption, perturbations\nError, failure\n1-2 citations each\nPREPRINT \n \n29 \nsize of the stability region and value-based guarantees. One or all of these can be used \nto measure robustness of a general control system to changes in the operational \nenvironment. \n \n \n \n \nFigure 9. Metric Selection Decision Tree \n \n \n5.  Supporting Information \n \nThe databases in Table 12 were readily available to the author through Oak Ridge \nNational Laboratory library subscriptions. Databases selected for this study are \ninternational in scope and ensure we investigate relevant studies that are global and \nPREPRINT \n \n30 \ncover a wide range of application domains, thereby eliminating the risk of bias from the \nauthor subject matter expertise and a Western perspective. However, we are bound by \nthe content of these databases. \n \nMany abstracting and indexing databases have very broad coverage, which results in \nindividual databases duplicating content found in others. Duplicated bibliographic \nrecords were identified and removed. After duplicate entries were removed, the \nremaining citations were reviewed for completeness in terms of information needed to \nobtain the document from a library. If the citation did not include this information \n(e.g., author name(s), article title, and journal name), the missing information was \nobtained from the source database or other online sources and the citation was manually \ncorrected. \n \n \nTable 12.  Information sources used in study \nDatabase \nDates of \nCoverage \nType of Database \narXiv \n1991-present \nPublicly Available / Open Access \nScopus \n1823-present \nSubscription \nWeb of Science \n1900-present \nSubscription \n \n \n \n \nPREPRINT \n \n31 \n \n \nFigure 10.  The PRISMA Flow Diagram for Stability \n \n \n \nPREPRINT \n \n32 \n \n \n \nFigure 11.  The PRISMA Flow Diagram for Robustness \n \n \n \n \nPREPRINT \n \n33 \n \n \n \nFigure 12.  The PRISMA Flow Diagram for Resilience \n \n \nPREPRINT \n \n34 \nTable 13. Data Reduction Summary \n \n \n \n \nPREPRINT \n \n35 \nTable 13.  The PRISMA Checklist [251] \nSection/topic  # Checklist item  \nReported in \nSection \nTITLE  \n \nTitle  \n1 Identify the report as a systematic review, meta-analysis, \nor both.  \nIntroduction \nABSTRACT  \n \n \nStructured \nsummary  \n \n2 \nProvide a structured summary including, as applicable: \nbackground; objectives; data sources; study eligibility \ncriteria, participants, and interventions; study appraisal \nand synthesis methods; results; limitations; conclusions \nand implications of key findings; systematic review \nregistration number.  \nIntroduction \nINTRODUCTION  \n \nRationale  \n3 Describe the rationale for the review in the context of \nwhat is already known.  \nIntroduction \nObjectives  \n4 Provide an explicit statement of questions being \naddressed with reference to participants, interventions, \ncomparisons, outcomes, and study design (PICOS).  \nIntroduction \nMETHODS  \n \nProtocol and \nregistration  \n5 Indicate if a review protocol exists, if and where it can \nbe accessed (e.g., Web address), and, if available, \nprovide registration information including registration \nnumber.  \nIntroduction \nEligibility \ncriteria  \n6 Specify study characteristics (e.g., PICOS, length of \nfollow-up) and report characteristics (e.g., years \nconsidered, language, publication status) used as criteria \nfor eligibility, giving rationale.  \nMethods and \nSupporting \nInformation (SI) \nInformation \nsources  \n7 Describe all information sources (e.g., databases with \ndates of coverage, contact with study authors to identify \nadditional studies) in the search and date last searched.  \nMethods and SI \nSearch  \n8 Present full electronic search strategy for at least one \ndatabase, including any limits used, such that it could be \nrepeated.  \nSI \nStudy selection  \n9 State the process for selecting studies (i.e., screening, \neligibility, included in systematic review, and, if \napplicable, included in the meta-analysis).  \nMethods \n \n \n \nPREPRINT \n \n36 \nTable 13.  The PRISMA Checklist [251] \nSection/topic  # Checklist item  \nReported in \nSection \nMETHODS (Continued) \n \nData collection \nprocess  \n10 Describe method of data extraction from reports (e.g., \npiloted forms, independently, in duplicate) and any \nprocesses for obtaining and confirming data from \ninvestigators.  \nMethods \nRisk of bias in \nindividual \nstudies  \n12 Describe methods used for assessing risk of bias of \nindividual studies (including specification of whether this \nwas done at the study or outcome level), and how this \ninformation is to be used in any data synthesis.  \nMethods and SI \nSummary \nmeasures  \n13 State the principal summary measures (e.g., risk ratio, \ndifference in means).  \nMethods \nSynthesis of \nresults  \n14 Describe the methods of handling data and combining \nresults of studies, if done, including measures of \nconsistency (e.g., I2) for each meta-analysis.  \nMethods  \nRisk of bias \nacross studies  \n15 Specify any assessment of risk of bias that may affect the \ncumulative evidence (e.g., publication bias, selective \nreporting within studies).  \nMethods \nAdditional \nanalyses  \n16 Describe methods of additional analyses (e.g., sensitivity \nor subgroup analyses, meta-regression), if done, \nindicating which were pre-specified.  \nMethods \nRESULTS  \n \nStudy selection  \n17 Give numbers of studies screened, assessed for eligibility, \nand included in the review, with reasons for exclusions at \neach stage, ideally with a flow diagram.  \nResults and SI \nStudy \ncharacteristics  \n18 For each study, present characteristics for which data \nwere extracted (e.g., study size, PICOS, follow-up \nperiod) and provide the citations.  \nResults \nRisk of bias \nwithin studies  \n19 Present data on risk of bias of each study and, if \navailable, any outcome level assessment (see item 12).  \nResults \nResults of \nindividual \nstudies  \n20 For all outcomes considered (benefits or harms), present, \nfor each study: (a) simple summary data for each \nintervention group (b) effect estimates and confidence \nintervals, ideally with a forest plot.  \nResults \nSynthesis of \nresults  \n21 Present results of each meta-analysis done, including \nconfidence intervals and measures of consistency.  \nResults \nRisk of bias \nacross studies  \n22 Present results of any assessment of risk of bias across \nstudies (see Item 15).  \nResults \nPREPRINT \n \n37 \nTable 13.  The PRISMA Checklist [251] \nSection/topic  # Checklist item  \nReported in \nSection \nRESULTS (Continued) \n \nAdditional \nanalysis  \n23 Give results of additional analyses, if done (e.g., \nsensitivity or subgroup analyses, meta-regression [see \nItem 16]).  \nResults \nData items  \n11 List and define all variables for which data were sought \n(e.g., PICOS, funding sources) and any assumptions and \nsimplifications made.  \nMethods \nSection/topic  # Checklist item  \nReported in \nSection \nDISCUSSION  \n \nSummary of \nevidence  \n24 Summarize the main findings including the strength of \nevidence for each main outcome; consider their relevance \nto key groups (e.g., healthcare providers, users, and \npolicy makers).  \nDiscussion \nLimitations  \n25 Discuss limitations at study and outcome level (e.g., risk \nof bias), and at review-level (e.g., incomplete retrieval of \nidentified research, reporting bias).  \nDiscussion \nConclusions  \n26 Provide a general interpretation of the results in the \ncontext of other evidence, and implications for future \nresearch.  \nDiscussion \nFUNDING  \n \nFunding  \n27 Describe sources of funding for the systematic review \nand other support (e.g., supply of data); role of funders \nfor the systematic review.  \nAcknowledgements \n \n6. \nAcknowledgements \n \nThe author would like to acknowledge Rama Vasudevan, PhD of Oak Ridge National \nLaboratory for intellectual discussions and collaborative research on reinforcement \nlearning. The author would also like to thank Nathan Martindale for assistance making \nthe decision tree more readable and thus usable. \n \nThis work was funded initially by the AI Initiative at the Oak Ridge National \nLaboratory and subsequently funded by the US Department of Energy, National \nNuclear Security Administration‚Äôs Office of Defense Nuclear Nonproliferation Research \nand Development (NA-22).  \nPREPRINT \n \n38 \n \nThis manuscript has been authored by UTBattelle, LLC, under contract DE-AC05-\n00OR22725 with the US Department of Energy (DOE). The US government retains and \nthe publisher, by accepting the article for publication, acknowledges that the US \ngovernment retains a nonexclusive, paid-up, irrevocable, worldwide license to publish or \nreproduce the published form of this manuscript, or allow others to do so, for US \ngovernment purposes. DOE will provide public access to these results of federally \nsponsored research in accordance with the DOE Public Access Plan \n(http://energy.gov/downloads/doe-publicaccess-plan). \n \n \nAcronyms \n \nA2C  \nAdvantage Actor Critic \nAODV \nAd-hoc On-demand Distance Vector \nARVI  \nAnytime Reduced Value Iteration \nATTN  \nActive Tracking Target Network \nAUC  \nArea Under the receiver operating characteristic (ROC) Curve (AUC) \nBMI  \nBrain Machine Interface \nCLAC  \nLyapunov-based Actor Critic \nCPPO  \nConstraint-controlled Proximal Policy Optimization \nDB \n \nDatabase \nDDPG  \nDeep Deterministic Policy Gradient \nDOE  \nDepartment of Energy \nDrAC  \nData-regulated Actor-Critic \nHVAC  \nHeating, Ventilation, and Air Conditioning \nJSD \n \nJensen-Shannon Divergence \nMax  \nMaximum \nMin \n \nMinimum \nMOOSE \nModel-based Offline policy Search with Ensembles \nNESM  \nNormalized Energy Stability Margin \nPerf \n \nPerformance \nPPO  \nProximal Policy Optimization \nPRISMA \nPreferred Reporting Items for Systematic Reviews and Meta-Analyses \nRL \n \nReinforcement Learning \nRes \n \nResearch \nRob \n \nRobustness \nROC  \nReceiver Operating Characteristic \nPREPRINT \n \n39 \nsec \n \nseconds \nStab  \nStability \nSVD  \nSingular Value Decomposition \nTAK  \nTitle, Abstract and Keywords \nTheo  \nTheoretical \nUS \n \nUnited States \nWoS  \nWeb of Science \n \n \nReferences \n \nResilience \n \n[1] \nBehzadan, V., and Munir, A. 2018. Adversarial exploitation of emergent \ndynamics in smart cities. Proceedings of the 2018 IEEE International Smart \nCities Conference, doi 10.1109/ISC2.2018.8656789. \n[2] \nEnjalbert, S., and Vanderhaegen, F. 2017. A hybrid reinforced learning system to \nestimate resilience indicators. Engineering Applications of Artificial Intelligence \n64:295-301. \n[3] \nBunyakitanon, M., Vasilakos, X., Nejabati, R., and Simeonidou D. 2020. End-to-\nEnd Performance-Based Autonomous VNF Placement with Adopted \nReinforcement Learning. IEEE Transactions on Cognitive Communications and \nNetworking 6(2):534-547. doi 10.1109/TCCN.2020.2988486. \n \nStability \n \n[4] \nDong, Zhe, X. Huang, Y. Dong, and Z. Zhang. 2020. Multilayer perception based \nreinforcement learning supervisory control of energy systems with application to \na nuclear steam supply system. Journal of Applied Energy 259, Feb. 2020. doi \n10.1016/j.apenergy.2019.114193. \n[5] \nWen, Guoxing, C.L. Philip Chen, Shuzhi Sam Ge, Hongli Yang, and Xiaoguang \nLiu. 2019. Optimized adaptive nonlinear tracking control using actor-critic \nreinforcement learning strategy. IEEE Transactions on Industrial Informatics \n15(9):4969-4977. \nPREPRINT \n \n40 \n[6] \nMuneeswari, B., and M.S.K. Manikandan. 2019. Energy efficient clustering and \nsecure routing using reinforcement learning for three-dimensional mobile ad hoc \nnetworks. IET Communications 13(12):1828-1839. \n[7] \nde Oliveira, Bernardo A.G., Carlos A.P. da S. Martins, Flavia Magalhaes, Luis \nFabricio, and W. Goes. 2019. Difference based metrics for deep reinforcement \nlearning algorithms. IEEE Access 7:159141-159149. \n[8] \nZhang, Kaiqing, Alec Koppel, Hao Zhu, and Tamer Bas. 2019. Policy search in \ninfinite-horizon discounted reinforcement learning: advances through connections \nto non-convex optimization. Proceedings: 53rd Annual Conference on \nInformation Sciences and Systems (CISS), Baltimore, MD, Mar 20-22. \n[9] \nDu, Zhijiang, Wei Wang, Zhiyuan Yan, Wei Dong, and Weidong Wang. 2017. \nVariable admittance control based on fuzzy reinforcement learning for minimally \ninvasive surgery manipulator, Sensors 17(4). \n[10] \nJiang, He, Huaguang Zhang, Yanhong Luo, and Junyi Wang. 2016. Optimal \ntracking control for completely unknown nonlinear discrete-time Markov jump \nsystems using data-based reinforcement learning method. Neurocomputing \n194:176-182. \n[11] \nAbdallah, Sherief. 2009. Why global performance is a poor metric for verifying \nconvergence of multi-agent learning. arXiv:0904.2320v1 [cs.MA] 15 April 2009.  \n[12] \nTalele, Nihar, and Katie Byl. 2019. Mesh-based tools to analyze deep \nreinforcement learning policies for underactuated biped locomotion. \narXiv:1903.12311v2 [cs.RO] 1 November 2019. \n[13] \nTuan, Yi-Lin, Jinzhi Zhang, Yujia Li, and Hung-yi Lee. 2018. Proximal policy \noptimization and its dynamic version for sequence generation. \narXiv:1808.07982v1 [cs.CL] 24 August 2018. \n[14] \nSerhani, Abdellatif, Najib Naja, and Abdellah Jamali. 2020. AQ-Routing: \nmobility-, stability-aware adaptive routing protocol for data routing in MANET‚Äì\nIoT systems. Cluster Computing 23:13‚Äì27. doi 10.1007/s10586-019-02937-x. \n[15] \nDong, Zhe, Xiaojin Huang, Yujie Dong, and Zuoyi Zhang. 2020. Multilayer \nperception based reinforcement learning supervisory control of energy systems \nwith application to a nuclear steam supply system. Applied Energy 259 (2020) \n114193. doi 10.1016/j.apenergy.2019.114193. \n[16] \nZhang, Huaguang, Kun Zhang, Yuliang Cai, and Jian Han. 2019. Adaptive fuzzy \nfault-tolerant tracking control for partially unknown systems with actuator faults \nPREPRINT \n \n41 \nvia integral reinforcement learning method. IEEE Transactions on Fuzzy Systems \n27(10). doi 10.1109/TFUZZ.2019.2893211. \n[17] \nCohen, Daniel, Scott M. Jordan, and W. Bruce Croft. 2019. Learning a Better \nNegative Sampling Policy with Deep Neural Networks for Search. In the 2019 \nACM SIGIR International Conference on the Theory of Information Retrieval \n(ICTIR ‚Äô19), October 2‚Äì5, 2019, Santa Clara, CA, USA. ACM, New York, NY, \nUSA. doi 10.1145/3341981.3344220. \n[18] \nMu, Chaoxu, Qian Zhao, Zhongke Gao, and Changyin Sun. 2019. Q-learning \nsolution for optimal consensus control of discrete-time multiagent systems using \nreinforcement learning. Journal of the Franklin Institute 356:6946‚Äì6967. doi \n10.1016/j.jfranklin.2019.06.0070016-0032. \n[19] \nTang, Xiaocheng, Zhiwei (Tony) Qin, Fan Zhang, Zhaodong Wang, Zhe Xu, \nYintai Ma, Hongtu Zhu, and Jieping Ye. 2019. A deep value-network based \napproach for multi-driver order dispatching. KDD 19, August 4‚Äì8, 2019, \nAnchorage, AK, USA. doi 10.1145/3292500.3330724. \n[20] \nAbouheaf, Mohammed, and Wail Gueaieb. 2019. Model-free adaptive control \napproach using integral reinforcement learning. IEEE International Symposium \non Robotic and Sensors Environments ‚Äì Proceedings. \n[21] \nSeo, Donghyeon, Harin Kim, and Donghan Kim. 2019. Push recovery control for \nhumanoid robot using reinforcement learning. Third IEEE International \nConference on Robotic Computing (IRC). doi 10.1109/IRC.2019.00102. \n[22] \nLv, Y., X. Ren, and J. Na. 2019. Online Nash-optimization tracking control of \nmulti-motor driven load system with simplified RL scheme. ISA Transactions. \ndoi 10.1016/j.isatra.2019.08.025. \n[23] \nTang, Li, Yan-Jun Liu, and C. L. Philip Chen. 2018. Adaptive critic design for \npure-feedback discrete-time MIMO systems preceded by unknown backlashlike \nhysteresis. IEEE Transactions on Neural Networks and Learning Systems. \n29(11), November. doi 10.1109/TNNLS.2018.2805689. \n[24] \nMertikopoulos, Panayotis, and William H. Sandholm. 2018. Riemannian game \ndynamics. Journal of Economic Theory (177):315-364. doi \n10.1016/j.jet.2018.06.002. \n[25] \nLiu, D., and G.-H. Yang. 2018. Model-free adaptive control design for nonlinear \ndiscrete-time processes with reinforcement learning techniques. International \nPREPRINT \n \n42 \nJournal of Systems Science 49(11):2298-2308. doi \n10.1080/00207721.2018.1498557. \n[26] \nBentaleb, Abdelhak, Ali C. Begen, and Roger Zimmermann. 2018. ORL-SDN: \nOnline reinforcement learning for SDN-enabled HTTP adaptive streaming. ACM \nTrans. Multimedia Comput. Commun. Appl. 14(3) Article 71 (August), 28 pages. \ndoi 10.1145/3219752. \n[27] \nHu Y., and B. Si. 2018. A reinforcement learning neural network for robotic \nmanipulator control. Neural Computation 30(7):1983-2004. doi \n10.1162/neco_a_01079. \n[28] \nMei, Y., G.-Z. Tan, Z.-T. Liu, and H. Wu. 2018. Chaotic time series prediction \nbased on brain emotional learning model and self-adaptive genetic algorithm. \nActa Physica Sinica 67(8). doi 10.7498/aps.67.20172104. \n[29] \nHong, Zhang-Wei, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang, and Chun-\nYi Lee. 2018. A deep policy inference Q-network for multi-agent systems. In \nProc. of the 17th International Conference on Autonomous Agents and \nMultiagent Systems (AAMAS 2018), Stockholm, Sweden, July 10‚Äì15, 2018.  \n[30] \nXiong, Yanhai, Haipeng Chen, Mengchen Zhao, and Bo An. 2018. HogRider: \nChampion agent of Microsoft Malmo collaborative AI challenge. The Thirty-\nSecond AAAI Conference on Artificial Intelligence (AAAI-18), pp. 4767-4774. \n[31] \nWu, Weiguo, and Liyang Gao. 2017. Posture self-stabilizer of a biped robot based \non training platform and reinforcement learning. Robotics and Autonomous \nSystems 98:42-55. doi 10.1016/j.robot.2017.09.001. \n[32] \nBoushaba, Mustapha, Abdelhakim Hafid, and Michel Gendreau. 2017. Node \nstability-based routing in wireless mesh networks. Journal of Network and \nComputer Applications 93:1‚Äì12. doi 10.1016/j.jnca.2017.02.010. \n[33] \nChasparis, Georgios C. 2017. Stochastic stability analysis of perturbed learning \nAutomata with constant step-size in strategic-form games. American Control \nConference (ACC), Seattle, WA, 2017, pp. 4607-4612. \n[34] \nPrins, Noeline W., Justin C. Sanchez and Abhishek Prasad. 2017. Feedback for \nreinforcement learning based brain-machine interfaces using confidence metrics. \nJournal of Neural Engineering 14 036016. doi 10.1088/1741-2552/aa6317. \n[35] \nSong, Ruizhuo, Frank L. Lewis, and Qinglai Wei. 2017. Off-policy integral \nreinforcement learning method to solve nonlinear continuous-time multiplayer \nPREPRINT \n \n43 \nnonzero-sum games. IEEE Transactions on Neural Networks and Learning \nSystems 28(3). doi 10.1109/TNNLS.2016.2582849. \n[36] \nYousefian, Reza, Amireza Sahami, and Sukumar Kamalasadan. 2017. Hybrid \ntransient energy function-based real-time optimal wide-area damping controller. \nIEEE Transactions on Industry Applications 53(2). doi \n10.1109/TIA.2016.2624264. \n[37] \nTatari, Farzaneh, Mohammad-Bagher Naghibi-Sistani, and Kyriakos G. \nVamvoudakis. 2015. Distributed learning algorithm for non-linear differential \ngraphical games. Transactions of the Institute of Measurement and Control 1‚Äì10.  \n[38] \nLu, C., J. Huang, and J. Gong. 2016. Reinforcement learning for ramp control: an \nanalysis of learning parameters. Promet ‚Äì Traffic & Transportation 28(4):371-\n381. \n[39] \nVamvoudakis, Kyriakos G. 2016. Optimal trajectory output tracking control with \na Q-learning algorithm. Proceedings of the American Control Conference, pp. \n5752-5757. doi 10.1109/ACC.2016.7526571. \n[40] \nR√™go, Patr√≠cia Helena Moraes, Jo√£o Viana da Fonseca Neto, and Ernesto M. \nFerreira. 2013. Convergence of the standard RLS method and UDUT \nfactorisation of covariance matrix for solving the algebraic Riccati equation of the \nDLQR via heuristic approximate dynamic programming. International Journal of \nSystems Science. doi 10.1080/00207721.2013.844283. \n[41] \nLiu, Derong, and Qinglai Wei. 2014. Policy iteration adaptive dynamic \nprogramming algorithm for discrete-time nonlinear systems. IEEE Transactions \non Neural Networks and Learning Systems 25(3). \n[42] \nAlharbi, Amal, Abdullah Al-Dhalaan, and Miznah Al-Rodhaan. 2014. Q-routing \nin cognitive packet network routing protocol for MANETs. In Proceedings of the \nInternational Conference on Neural Computation Theory and Applications \n(NCTA-2014), pp. 234-243. doi 10.5220/0005082902340243. \n[43] \nYousefian, Reza, and Sukumar Kamalasadan. 2014. An approach for real-time \ntuning of cost functions in optimal system-centric wide area controller based on \nadaptive critic design. IEEE Power and Energy Society General Meeting, \nOctober 2014. doi 10.1109/PESGM.2014.6939224. \n[44] \nDong, Bo, and Yuanchun Li. 2013. Decentralized reinforcement learning robust \noptimal tracking control for time varying constrained reconfigurable modular \nPREPRINT \n \n44 \nrobot based on ACI and ùëÑ-function. Mathematical Problems in Engineering 2013, \nArticle 387817, 16 pages. doi 10.1155/2013/387817. \n[45] \nTeixeira, Carlos, Lino Costa, and Cristina Santos. 2014. Biped locomotion - \nimprovement and adaptation. IEEE International Conference on Autonomous \nRobot Systems and Competitions (ICARSC), May 14-15, Espinho, Portugal.  \n[46] \nLuy, N.T., N.T. Thanh, H.M. Tri. 2014. Reinforcement learning-based intelligent \ntracking control for wheeled mobile robot. Transactions of the Institute of \nMeasurement and Control 36(7):868‚Äì877. doi 10.1177/0142331213509828. \n[47] \nHager, Louw vS., Kenneth R. Uren, and George van Schoor. 2014. Series-Parallel \nApproach to On-line Observer based Neural Control of a Helicopter System. \nProceedings of the 19th World Congress the International Federation of \nAutomatic Control, Cape Town, South Africa. August 24-29, 2014.  \n[48] \nWei, Qinglai, and Derong Liu. 2014. Data-driven neuro-optimal temperature \ncontrol of water-gas shift reaction using stable iterative adaptive dynamic \nprogramming. IEEE Transactions on Industrial Electronics 61(11). \n[49] \nZhao, D., B. Wang, and D. Liu. 2013. A supervised Actor-Critic approach for \nadaptive cruise control. Soft Comput 17:2089‚Äì2099. doi 10.1007/s00500-013-1110-\ny. \n[50] \nKashki, M., M.A. Abido, and Y.L. Abdel-Magid. 2013. Power system dynamic \nstability enhancement using optimum design of PSS and static phase shifter \nbased stabilizer. Arab J Sci Eng 38:637‚Äì650. doi 10.1007/s13369-012-0325-z. \n[51] \nLi, Cai, Robert Lowe, and Tom Ziemke. 2013. Humanoids learning to walk: a \nnatural CPG-actor-critic architecture. Frontiers in Neurorobotics 7, Article 5. doi \n10.3389/fnbot.2013.00005. \n[52] \nVamvoudakis, K.G., F.L. Lewis, and G.R. Hudas. 2012. Multi-agent differential \ngraphical games: Online adaptive learning solution for synchronization with \noptimality. Automatica 48(8):1598-1611. doi 10.1016/j.automatica.2012.05.074. \n[53] \nMoradi, P., M.E. Shiri, A.A. Rad, A. Khadivi, and M. Hasler. 2012. Automatic \nskill acquisition in reinforcement learning using graph centrality measures. \nIntelligent Data Analysis 16(1):113-135. doi 10.3233/IDA-2011-0513. \n[54] \nBhasin, Shubhendu, Nitin Sharma, Parag Patre, and Warren Dixon. 2011. \nAsymptotic tracking by a reinforcement learning-based adaptive critic controller. \nJ Control Theory Appl 9(3):400-409. doi 10.1007/s11768-011-0170-8. \nPREPRINT \n \n45 \n[55] \nHafner, Roland, and Martin Riedmiller. 2011. Reinforcement learning in feedback \ncontrol: Challenges and benchmarks from technical process control. Machine \nLearning 84:137-169. doi 10.1007/s10994-011-5235-x. \n[56] \nLuy, Nguyen Tan. 2012. Reinforcement learning-based tracking control for \nwheeled mobile robot. IEEE International Conference on Systems, Man, and \nCybernetics, October 14-17, 2012, Seoul, Korea. \n[57] \nShih, Peter, Brian C. Kaul, Sarangapani Jagannathan, and James A. Drallmeier. \n2009. Reinforcement-learning-based output-feedback control of nonstrict \nnonlinear discrete-time systems with application to engine emission control. IEEE \nTransactions on Systems, Man, and Cybernetics‚ÄîPart B: Cybernetics 39(5). \n[58] \nBoada, M.J.L., B.L. Boada, A.G. Bab√©, J.A. Calvo Ramos, and V.D. L√≥pez. \n2009. Active roll control using reinforcement learning for a single unit heavy \nvehicle. International Journal of Heavy Vehicle Systems 16(4):412-430. doi \n10.1504/IJHVS.2009.027413. \n[59] \nGuo, L., Y. Zhang, and J.-L. Hu. 2007. Adaptive HVDC supplementary (lamping \ncontroller based on reinforcement learning. Electric Power Automation \nEquipment 27(10):87-91. \n[60] \nLin, C.-K. 2003. A reinforcement learning adaptive fuzzy controller for robots. \nFuzzy Sets and Systems 137(3):339-352. doi 10.1016/S0165-0114(02)00299-3. \n[61] \nJagannathan, S. 2002. Adaptive critic neural network-based controller for \nnonlinear systems. Proceedings of the 2002 IEEE International Symposium on \nIntelligent Control, Vancouver, Canada, October 27-30, 2002. \n[62] \nKaygisiz, Burak H., Aydan M. Erkmen, and Ismet Erkmen. 2002. Smoothing \nstability roughness of fractal boundaries using reinforcement learning. IFAC \nProceedings Volumes 15(1):481-485. \n[63] \nLi, J.N., J.L. Ding, T.Y. Chai, and F.L. Lewis. 2020. Nonzero-Sum Game \nReinforcement Learning for Performance Optimization in Large-Scale Industrial \nProcesses. IEEE Transactions on Cybernetics 50(9):4132-4145. Doi \n10.1109/TCYB.2019.2950262. \n[64] \nZhang, K., H.G. Zhang, Y.L. Cai, and R. Su. 2020. Parallel Optimal Tracking \nControl Schemes for Mode-Dependent Control of Coupled Markov Jump Systems \nvia Integral RL Method. IEEE Transactions on Automation Science and \nEngineering 17(3):1332-1342. Doi 10.1109/TASE.2019.2948431. \nPREPRINT \n \n46 \n[65] \nZhang, Qian, Kui Wu, and Yang Shi. 2020. Route Planning and Power \nManagement for PHEVs With Reinforcement Learning. IEEE Transactions on \nVehicular Technology 69(5):4751-4762. doi 10.1109/TVT.2020.2979623. \n[66] \nSerhani, Abdellatif, Najib Naja, and Abdellah Jamali. 2020. AQ-Routing: \nmobility-, stability-aware adaptive routing protocol for data routing in MANET-\nIoT systems. Cluster Computing 23(1):13-27. Doi 10.1007/s10586-019-02937-x. \n[67] \nDong, Zhe, Xiaojin Huang, Yujie Dong, and Zuoyi Zhang. 2020. Multilayer \nperception based reinforcement learning supervisory control of energy systems \nwith application to a nuclear steam supply system. Applied Energy 259. doi \n10.1016/j.apenergy.2019.114193. \n[68] \nWang, Qingling. 2020. Integral Reinforcement Learning Control for a Class of \nHigh-Order Multivariable Nonlinear Dynamics with Unknown Control \nCoefficients. IEEE Access 8:86223-86229. Doi 10.1109/ACCESS.2020.2993265. \n[69] \nZhang J., Z. Peng, J. Hu, Y. Zhao, R. Luo, B.K. Ghosh. 2020. Internal \nreinforcement adaptive dynamic programming for optimal containment control of \nunknown continuous-time multi-agent systems. Neurocomputing 413:85-95. doi \n10.1016/j.neucom.2020.06.106. \n[70] \nMitriakov, A., P. Papadakis, S.M. Nguyen, and S. Garlatti. 2020. Staircase \ntraversal via reinforcement learning for active reconfiguration of assistive robots. \nProceedings IEEE International Conference on Fuzzy Systems. doi \n10.1109/FUZZ48607.2020.9177581. \n[71] \nLv, Y., X. Ren, and J. Na. 2020. Online Nash-optimization tracking control of \nmulti-motor driven load system with simplified RL scheme. ISA Transactions \n98:251-262. doi 10.1016/j.isatra.2019.08.025. \n[72] \nThornton, C.E., M.A. Kozy, R.M. Buehrer, A.F. Martone, and K.D. Sherbondy. \n2020. Deep Reinforcement Learning Control for Radar Detection and Tracking in \nCongested Spectral Environments. IEEE Transactions on Cognitive \nCommunications and Networking. doi 10.1109/TCCN.2020.3019605. \n[73] \nPrasanna, John Deva, John Aravindhar, P. Sivasankar, and K. Perumal. 2020. \nReinforcement learning based virtual backbone construction in manet using \nconnected dominating sets. Journal of Critical Reviews 7(9):146-152. doi \n10.31838/jcr.07.09.28. \nPREPRINT \n \n47 \n[74] \nPongfai, J., X. Su, H. Zhang, and W. Assawinchaichote. 2020. PID controller \nautotuning design by a deterministic Q-SLP algorithm. IEEE Access 8:50010-\n50021. doi 10.1109/ACCESS.2020.2979810. \n[75] \nHoppe, Sabrina, and Marc Toussaint. 2020. Q graph-bounded Q-learning: \nStabilizing Model-Free O-Policy Deep Reinforcement Learning. \narXiv:2007.07582v1 [cs.LG] 15 Jul 2020. \n[76] \nOsinenko, Pavel, Lukas Beckenbach, Thomas G√∂hrt, and Stefan Streif. 2020. A \nreinforcement learning method with closed-loop stability guarantee. \narXiv:2006.14034v1 [math.OC] 24 Jun 2020. \n[77] \nHan, Minghao, Lixian Zhang, Jun Wang, and Wei Pan. 2020. Actor-Critic \nReinforcement Learning for Control with Stability Guarantee. \narXiv:2004.14288v3 [cs.RO] 15 Jul 2020. \n[78] \nKhader, Shahbaz A., Hang Yin, Pietro Falco and Danica Kragic. 2020. Stability-\nGuaranteed Reinforcement Learning for Contact-rich Manipulation. \narXiv:2004.10886v2 [cs.RO] 27 Sep 2020. \n[79] \nHan, Minghao, Yuan Tian, Lixian Zhang, Jun Wang, and Wei Pan. 2020. H \ninfinity Model-free Reinforcement Learning with Robust Stability Guarantee. \narXiv:1911.02875v3 [cs.LG] 25 Jul 2020. \n[80] \nTessler, Chen, Nadav Merlis, and Shie Mannor. Stabilizing Deep Reinforcement \nLearning with Conservative Updates. arXiv:1910.01062v2 [cs.LG] 9 Feb 2020. \n \nRobustness \n \n[81] \nAbuzainab, Nof, Tugba Erpek, Kemal Davaslioglu, Yalin E. Sagduyu, Yi Shi, \nSharon J. Mackey, Mitesh Patel, Frank Panettieri, Muhammad A. Qureshi, \nVolkan Isler, and Aylin Yener. 2019. QoS and jamming-aware wireless \nnetworking using deep reinforcement learning. arXiv:1910.05766v1 [cs.NI] 13 \nOctober 2019. \n[82] \nAhn, Michael. 2019. ROBEL: Robotics benchmarks for learning with low-cost \nrobots. arXiv:1909.11639v3 [cs.RO] 16 Dec 2019. \n[83] \nDhiman, Vikas, Shurjo Banerjee, Brent Griffin, Jeffrey M Siskind, and Jason J \nCorso. 2019. A critical investigation of deep reinforcement learning for \nnavigation. arXiv:1802.02274v2 [cs.RO] 4 Jan 2019. \nPREPRINT \n \n48 \n[84] \nNaderializadeh, Navid, Jaroslaw Sydir, Meryem Simsek, Hosein Nikopour, and \nShilpa Talwar. 2019. When multiple agents learn to schedule: a distributed radio \nresource management framework. arXiv:1906.08792v1 [cs.LG] 20 Jun 2019. \n[85] \nNguyen, Khanh, Hal Daum√© III, and Jordan Boyd-Graber. 2017. Reinforcement \nlearning for bandit neural machine translation with simulated human feedback. \narXiv:1707.07402v4 [cs.CL] 11 Nov 2017. \n[86] \nTalele, Nihar, and Katie Byl. 2019. Mesh-based tools to analyze deep \nreinforcement learning policies for underactuated biped locomotion. \narXiv:1903.12311v2 [cs.RO] 1 Nov 2019. \n[87] \nTurchetta, Matteo, Andreas Krause, and Sebastian Trimpe. 2019. Robust model-\nfree reinforcement learning with multi-objective Bayesian optimization. \narXiv:1910.13399v1 [cs.RO] 29 Oct 2019. \n[88] \nYuan, Ye, and Kris Kitani. 2019. Ego-pose estimation and forecasting as real-\ntime PD control. arXiv:1906.03173v2 [cs.CV] 4 Aug 2019. \n[89] \nMuneeswari, B., and M.S.K. Manikandan. 2019. Energy efficient clustering and \nsecure routing using reinforcement learning for three-dimensional mobile ad hoc \nnetworks. IET Communications 13(12):1828-1839. doi 10.1049/iet-com.2018.6150. \n[90] \nZhao, B., D. Wang, G. Shi, D.R. Liu, and Y.C. Li. 2018. Decentralized control \nfor large-scale nonlinear systems with unknown mismatched interconnections via \npolicy iteration. IEEE Transactions on Systems Man Cybernetics-Systems 48(10). \n[91] \nZhang, Y., Y. Yang, S.X. Ding, and L.L. Li. 2016. Optimal design of residual-\ndriven dynamic compensator using iterative algorithms with guaranteed \nconvergence. IEEE Transactions on Systems, Man, and Cybernetics: Systems \n46(4). doi 10.1109/TSMC.2015.2450203. \n[92] \nTokic, M. 2010. Adaptive epsilon-greedy exploration in reinforcement learning \nbased on value differences. Lecture Notes in Artificial Intelligence, 33rd Annual \nGerman Conference on Artificial Intelligence, Karlsruhe, Germany, Sep 21-24, \n2010. \n[93] \nXiong, Y., L. Guo, Y. Huang, and L. Chen. 2020. Intelligent thermal control \nstrategy based on reinforcement learning for space telescope. Journal of \nThermophysics and Heat Transfer 34(1):37-44. \n[94] \nIsa-Jara, R.F., G.J. Meschino, and V.L. Ballarin. 2020. A comparative study of \nreinforcement learning algorithms applied to medical image registration. IFMBE \nProceedings, pp. 281-289. \nPREPRINT \n \n49 \n[95] \nGuo, F., X. Zhou, J. Liu, Y. Zhang, D. Li, and H. Zhou. 2019. A reinforcement \nlearning decision model for online process parameters optimization from offline \ndata in injection molding. Applied Soft Computing Journal 85. doi \n10.1016/j.asoc.2019.105828. \n[96] \nLi, S., C. He, M. Liu, Y. Wan, Y. Gu, J. Xie, S. Fu, and K. Lu. 2019. Design and \nimplementation of aerial communication using directional antennas: Learning \ncontrol in unknown communication environments. IET Control Theory and \nApplications 13(17):2906-2916. doi 10.1049/iet-cta.2018.6252. \n[97] \nTang, X., Z. Qin, F. Zhang, Z. Wang, Z. Xu, Y. Ma, H. Zhu, and J. Ye. 2019. A \ndeep value-network based approach for multi-driver order dispatching. \nProceedings of the ACM SIGKDD International Conference on Knowledge \nDiscovery and Data Mining, pp. 1780-1790. doi 10.1145/3292500.3330724. \n[98] \nChowdhury, A., S.A. Raut, and H.S. Narman. 2019. DA-DRLS: Drift adaptive \ndeep reinforcement learning based scheduling for IoT resource management. \nJournal of Network and Computer Applications 138:51-65. doi \n10.1016/j.jnca.2019.04.010. \n[99] \nWang, X., C. Li, L. Yu, L. Han, X. Deng, E. Yang, and P. Ren. 2019. UAV first \nview landmark localization with active reinforcement learning. Pattern \nRecognition Letters 125:549-555. doi 10.1016/j.patrec.2019.03.011. \n[100] L√ºtjens, B., M. Everett, and J.P. How. 2019. Safe reinforcement learning with \nmodel uncertainty estimates. Proceedings - IEEE International Conference on \nRobotics and Automation, pp. 8662-8668. doi 10.1109/ICRA.2019.8793611. \n[101] Balakrishnan, A., and J.V. Deshmukh. 2019. Structured reward functions using \nSTL. Proceedings of the 2019 22nd ACM International Conference on Hybrid \nSystems: Computation and Control, pp. 270-271. doi 10.1145/3302504.3313355. \n[102] Tang, C., W. Zhu, and X. Yu. 2019. Deep hierarchical strategy model for multi-\nsource driven quantitative investment. IEEE Access 7:79331-79336. doi \n10.1109/ACCESS.2019.2923267. \n[103] Cheng, Q., X. Wang, Y. Niu, and L. Shen. 2019. Reusing source task knowledge \nvia transfer approximator in reinforcement transfer learning. Symmetry 11(1). doi \n10.3390/sym11010025. \n[104] Jeon, Y.-S., H. Lee, and N. Lee. 2018. Robust MLSD for wideband SIMO systems \nwith one-bit ADCs: reinforcement-learning approach. Proceedings - 2018 IEEE \nPREPRINT \n \n50 \nInternational Conference on Communications Workshops, ICC Workshops, pp. \n1-6. doi 10.1109/ICCW.2018.8403665. \n[105] Yang, X., and H. He. 2018. Self-learning robust optimal control for continuous-\ntime nonlinear systems with mismatched disturbances. Neural Networks 99:19-30. \ndoi 10.1016/j.neunet.2017.11.022. \n[106] Jiang, H., H. Zhang, Y. Cui, and G. Xiao. 2018. Robust control scheme for a \nclass of uncertain nonlinear systems with completely unknown dynamics using \ndata-driven reinforcement learning method. Neurocomputing 273:68-77. doi \n10.1016/j.neucom.2017.07.058. \n[107] Shayeghi, H., and A. Younesi. 2017. An online Q-learning based multi-agent LFC \nfor a multi-area multi-source power system including distributed energy \nresources. Iranian Journal of Electrical and Electronic Engineering 13(4):385-\n398. doi 10.22068/IJEEE.13.4.385. \n[108] Zhao, D., Y. Ma, Z. Jiang, and Z. Shi. 2017. Multiresolution airport detection via \nhierarchical reinforcement learning saliency model. IEEE Journal of Selected \nTopics in Applied Earth Observations and Remote Sensing 10(6):2855-2866. doi \n10.1109/JSTARS.2017.2669335. \n[109] Tow, A.W., S. Shirazi, J. Leitner., N. S√ºnderhauf, M. Milford, and B. Upcroft. \n2016. A robustness analysis of deep Q networks. Australasian Conference on \nRobotics and Automation, pp. 116-125. \n[110] Hatami, E., and H. Salarieh. 2015. Adaptive critic-based neuro-fuzzy controller \nfor dynamic position of ships. Scientia Iranica 22(1):272-280. \n[111] Xiang, J., and Z. Chen. 2015. Adaptive traffic signal control of bottleneck \nsubzone based on grey qualitative reinforcement learning algorithm. ICPRAM \n2015 - 4th International Conference on Pattern Recognition Applications and \nMethods, Proceedings, 2:295-301. \n[112] Padmanabhan, R., N. Meskin, and W.M. Haddad. 2015. Closed-loop control of \nanesthesia and mean arterial pressure using reinforcement learning. Biomedical \nSignal Processing and Control 22:54-64. doi 10.1016/j.bspc.2015.05.013. \n[113] Bruno, R., A. Masaracchia, and A. Passarella. 2014. Robust adaptive modulation \nand coding (AMC) selection in LTE systems using reinforcement learning. IEEE \nVehicular Technology Conference. doi 10.1109/VTCFall.2014.6966162. \nPREPRINT \n \n51 \n[114] Jamali, N., P. Kormushev, S.R. Ahmadzadeh, and D.G. Caldwell. 2014. \nCovariance analysis as a measure of policy robustness. OCEANS 2014 ‚Äì Taipei. \ndoi 10.1109/OCEANS-TAIPEI.2014.6964339. \n[115] Tati, S., S. Silvestri, T. He, and T.L. Porta. 2014. Robust network tomography \nin the presence of failures. Proceedings - International Conference on Distributed \nComputing Systems, pp. 481-492. doi 10.1109/ICDCS.2014.56. \n[116] Luy, N.T., N.T. Thanh, and H.M. Tri. 2013. Reinforcement learning-based robust \nadaptive tracking control for multi-wheeled mobile robots synchronization with \noptimality. Proceedings of the 2013 IEEE Workshop on Robotic Intelligence in \nInformationally Structured Space, pp. 74-81. doi 10.1109/RiiSS.2013.6607932. \n[117] Kashki, M., M.A. Abido, and Y.L. Abdel-Magid. 2013. Power system dynamic \nstability enhancement using optimum design of PSS and static phase shifter \nbased stabilizer. Arabian Journal for Science and Engineering 38(3):637-650. doi \n10.1007/s13369-012-0325-z. \n[118] Lopes, M., T. Lang, M. Toussaint, and P.-Y. Oudeyer. 2012. Exploration in \nmodel-based reinforcement learning by empirically estimating learning progress. \nAdvances in Neural Information Processing Systems 1:206-214. \n[119] Llorente, M.S., and S.E. Guerrero. 2012. Increasing retrieval quality in \nconversational recommenders. IEEE Transactions on Knowledge and Data \nEngineering 24(10):1876-1888. doi 10.1109/TKDE.2011.116. \n[120] Maes, F., L. Wehenkel, and D. Ernst. 2012. Learning to play K-armed bandit \nproblems. Proceedings of the 4th International Conference on Agents and \nArtificial Intelligence, pp. 74-81. \n[121] Bhasin, S., N. Sharma, P. Patre, and W. Dixon. 2011. Asymptotic tracking by a \nreinforcement learning-based adaptive critic controller. Journal of Control Theory \nand Applications 9(3):400-409. doi 10.1007/s11768-011-0170-8. \n[122] Tjahjadi, A., S. Sendari, S. Mabu, and K. Hirasawa. 2010. Robustness analysis of \ngenetic network programming with reinforcement learning. Proceedings Joint 5th \nInternational Conference on Soft Computing and Intelligent Systems and 11th \nInternational Symposium on Advanced Intelligent Systems, pp. 594-601. \n[123] Kulkarni, S.A., and G.R. Rao. 2010. Vehicular ad hoc network mobility models \napplied for reinforcement learning routing algorithm. Communications in \nComputer and Information Science, pp. 230-240. doi 10.1007/978-3-642-14825-\n5_20. \nPREPRINT \n \n52 \n[124] Molina, C., N.B. Yoma, F. Huenup√°n, C. Garret√≥n, and J. Wuth. 2010. \nMaximum entropy-based reinforcement learning using a confidence measure in \nspeech recognition for telephone speech. IEEE Transactions on Audio, Speech \nand Language Processing 18(5):1041-1052. doi 10.1109/TASL.2009.2032618. \n[125] Luy, N.T., N.D. Thanh, N.T. Thanh, and N.T.P. Ha. 2010. Robust reinforcement \nlearning-based tracking control for wheeled mobile robot. The 2nd International \nConference on Computer and Automation Engineering, pp. 171-176. doi \n10.1109/ICCAE.2010.5451973. \n[126] Heidrich-Meisner, Verena, and Christian Igel. 2009. Hoeffding and Bernstein \nraces for selecting policies in evolutionary direct policy search. Proceedings of the \n26th International Conference on Machine Learning, pp. 401-408. \n[127] Satoh, H. 2008. A nonlinear approach to robust routing based on reinforcement \nlearning with state space compression and adaptive basis construction. IEICE \nTransactions on Fundamentals of Electronics, Communications and Computer \nSciences 7:1733-1740. doi 10.1093/ietfec/e91-a.7.1733. \n[128] Conn, K., and R.A. Peters. 2007. Reinforcement learning with a supervisor for a \nmobile robot in a real-world environment. Proceedings of the 2007 IEEE \nInternational Symposium on Computational Intelligence in Robotics and \nAutomation, pp. 73-78. doi 10.1109/CIRA.2007.382878. \n[129] Wang, X.-S., Y.-H. Cheng, and W. Sun. 2007. A proposal of adaptive PID \ncontroller based on reinforcement learning. Journal of China University of \nMining and Technology 17(1):40-44. doi 10.1016/S1006-1266(07)60009-1. \n[130] Leem, JoonBum, and Ha Young Kim. 2020. Action-specialized expert ensemble \ntrading system with extended discrete action space using deep reinforcement \nlearning. PLOS One 15(7). doi 10.1371/journal.pone.0236178. \n[131] Xiong, Yan, Liang Guo, Yong Huang, and Liheng Chen. 2020. Intelligent \nThermal Control Strategy Based on Reinforcement Learning for Space Telescope. \nJournal of Thermophysics and Heat Transfer 34(1):37-44. doi 10.2514/1.T5774. \n[132] Balakrishnan, Anand, and Jyotirmoy V. Deshmukh. 2019. Structured Reward \nFunctions using STL. Proceedings of the 2019 22nd ACM International \nConference on Hybrid Systems: Computation and Control (HSCC '19), pp. 270-\n271. doi 10.1145/3302504.3313355. \nPREPRINT \n \n53 \n[133] Chen, G., S. Yao, J. Ma, L. Pan, Y. Chen, P. Xu, J. Ji, and X. Chen. 2020. \nDistributed Non-Communicating Multi-Robot Collision Avoidance via Map-\nBased Deep Reinforcement Learning. Sensors 20(17). doi 10.3390/s20174836. \n[134] Sun, C., X. Li, and C. Belta. 2020. Automata Guided Semi-Decentralized Multi-\nAgent Reinforcement Learning. Proceedings of the American Control \nConference, pp. 3900-3905. doi 10.23919/ACC45564.2020.9147704. \n[135] Wang, X., and X. Ye. 2020. Optimal Robust Control of Nonlinear Uncertain \nSystem via Off-Policy Integral Reinforcement Learning. Proceedings of the \nChinese Control Conference, pp. 1928-1933. doi \n10.23919/CCC50068.2020.9189626. \n[136] Yan, Z., J. Ge, Y. Wu, L. Li, and T. Li. 2020. Automatic virtual network \nembedding: A deep reinforcement learning approach with graph convolutional \nnetworks. IEEE Journal on Selected Areas in Communications 38(6):1040-1057. \ndoi 10.1109/JSAC.2020.2986662. \n[137] Alhazmi, K., and S.M. Sarathy. 2020. Continuous Control of Complex Chemical \nReaction Network with Reinforcement Learning. Proceedings of the European \nControl Conference 2020, ECC 2020, pp. 1066-1068. \n[138] Ghasemkhani, A., A. Darvishi, I. Niazazari, A. Darvishi, H. Livani, and L. Yang. \n2020. DeepGrid: Robust Deep Reinforcement Learning-based Contingency \nManagement. Proceedings of the 2020 IEEE Power and Energy Society \nInnovative Smart Grid Technologies Conference, ISGT 2020. doi \n10.1109/ISGT45199.2020.9087633. \n[139] Pitti, A., M. Quoy, C. Lavandier, and S. Boucenna. 2020. Gated spiking neural \nnetwork using Iterative Free-Energy Optimization and rank-order coding for \nstructure learning in memory sequences (INFERNO GATE). Neural Networks \n121:242-258. doi 10.1016/j.neunet.2019.09.023. \n[140] Vecerik, Mel, Jean-Baptiste Regli, Oleg Sushkov, David Barker, Rugile \nPevceviciute, Thomas Roth√∂rl, Christopher Schuster, Raia Hadsell, Lourdes \nAgapito, and Jonathan Scholz. 2020. S3K: Self-Supervised Semantic Keypoints \nfor Robotic Manipulation via Multi-View Consistency. arXiv:2009.14711. \n[141] Jiao, Yusheng, Feng Ling, Sina Heydari, Nicolas Heess, Josh Merel, and Eva \nKanso. 2020. Learning to swim in potential flow. arXiv:2009.14280. \nPREPRINT \n \n54 \n[142] Alm√°si, P√©ter, R√≥bert Moni, and B√°lint Gyires-T√≥th. 2020. Robust \nReinforcement Learning-based Autonomous Driving Agent for Simulation and \nReal World. arXiv:2009.11212. \n[143] Ding, Wenhao, Baiming Chen, Bo Li, Kim Ji Eun, and Ding Zhao. 2020. \nMultimodal Safety-Critical Scenarios Generation for Decision-Making Algorithms \nEvaluation. arXiv:2009.08311. \n[144] Schamberg, Gabe, Marcus Badgeley, and Emery N. Brown. 2020. Controlling \nLevel of Unconsciousness by Titrating Propofol with Deep Reinforcement \nLearning. arXiv:2008.12333. \n[145] Pang, Bo, and Zhong-Ping Jiang. 2020. Robust Reinforcement Learning: A Case \nStudy in Linear Quadratic Regulation. arXiv:2008.11592. \n[146] Kobayashi, Taisuke, and Wendyam Eric Lionel Ilboudo. 2020. t-Soft Update of \nTarget Network for Deep Reinforcement Learning. arXiv:2008.10861. \n[147] Zavoli, Alessandro, and Lorenzo Federici. 2020. Reinforcement Learning for \nLow-Thrust Trajectory Design of Interplanetary Missions. arXiv:2008.08501. \n[148] Limoyo, Oliver, Bryan Chan, Filip Mariƒá, Brandon Wagstaff, Rupam Mahmood, \nand Jonathan Kelly. 2020. Heteroscedastic Uncertainty for Robust Generative \nLatent Dynamics. arXiv:2008.08157. \n[149] Zhao, Wenshuai, Jorge Pe√±a Queralta, Li Qingqing, and Tomi Westerlund. 2020. \nTowards Closing the Sim-to-Real Gap in Collaborative Multi-Robot \nDeep Reinforcement Learning. arXiv:2008.07875. \n[150] Qu, Xinghua, Yew-Soon Ong, Abhishek Gupta, and Zhu Sun. 2020. Defending \nAdversarial Attacks without Adversarial Attacks in Deep Reinforcement \nLearning. arXiv:2008.06199. \n[151] Swazinna, Phillip, Steffen Udluft, and Thomas Runkler. 2020. Overcoming Model \nBias for Robust Offline Deep Reinforcement Learning. arXiv:2008.05533. \n[152] Ahmed, Ibrahim, Hamed Khorasgani, and Gautam Biswas. 2020. Comparison of \nModel Predictive and Reinforcement Learning Methods for Fault Tolerant \nControl. arXiv:2008.04403. \n[153] Kovaƒç, Grgur, Adrien Laversanne-Finot, and Pierre-Yves Oudeyer. 2020. \nGRIMGEP: Learning Progress for Robust Goal Sampling in Visual Deep \nReinforcement Learning. arXiv:2008.04388. \nPREPRINT \n \n55 \n[154] Zhu, Jianxin Li, Hao Peng, Senzhang Wang, Philip S. Yu, and Lifang He. 2020. \nAdversarial Directed Graph Embedding. arXiv:2008.03667. \n[155] Ma, Xiao, Siwei Chen, David Hsu, and Wee Sun Lee. 2020. Contrastive \nVariational Model-Based Reinforcement Learning for Complex Observations. \narXiv:2008.02430. \n[156] Oikarinen, Tuomas, Tsui-Wei Weng, and Luca Daniel. 2020. Robust Deep \nReinforcement Learning through Adversarial Loss. arXiv:2008.01976. \n[157] Vinitsky, Eugene, Yuqing Du, Kanaad Parvate, Kathy Jang, Pieter Abbeel, and \nAlexandre Bayen. 2020. Robust Reinforcement Learning using Adversarial \nPopulations. arXiv:2008.01825. \n[158] Park, Hwangpil, Ri Yu, Yoonsang Lee, Kyungho Lee, and Jehee Lee. 2020. \nUnderstanding the Stability of Deep Control Policies for Biped Locomotion. \narXiv:2007.15242. \n[159] Steverson, Kai, Jonathan Mullin, and Metin Ahiskali. 2020. Adversarial \nRobustness for Machine Learning Cyber Defenses Using Log Data. \narXiv:2007.14983. \n[160] Chen, Xinwei, Tong Wang, Barrett W. Thomas, and Marlin W. Ulmer. 2020. \nSame-Day Delivery with Fairness. arXiv:2007.09541. \n[161] Chen, Xin, Yawen Duan, Zewei Chen, Hang Xu, Zihao Chen, Xiaodan Liang, \nTong Zhang, and Zhenguo Li. 2020. CATCH: Context-based Meta Reinforcement \nLearning for Transferrable Architecture Search. arXiv:2007.09380. \n[162] Zhang, Lin, Hao Xiong, Ou Ma, and Zhaokui Wang. 2020. Multi-robot \nCooperative Object Transportation using Decentralized Deep Reinforcement \nLearning. arXiv:2007.09243. \n[163] Tan, Kai Liang, Yasaman Esfandiari, Xian Yeow Lee, Aakanksha, and Soumik \nSarkar. 2020. Robustifying Reinforcement Learning Agents via Action Space \nAdversarial Training. arXiv:2007.07176. \n[164] Stooke, Adam, Joshua Achiam, and Pieter Abbeel. 2020. Responsive Safety in \nReinforcement Learning by PID Lagrangian Methods. arXiv:2007.03964. \n[165] Abe, Kenshi, and Yusuke Kaneko. 2020. Off-Policy Exploitability-Evaluation and \nEquilibrium-Learning in Two-Player Zero-Sum Markov Games. arXiv:2007.02141. \n[166] Wang, Xiao, Saasha Nair, and Matthias Althoff. 2020. Falsification-Based Robust \nAdversarial Reinforcement Learning. arXiv:2007.00691. \nPREPRINT \n \n56 \n[167] Lee, Heunchul, Maksym Girnyk, and Jaeseong Jeong. 2020. Deep reinforcement \nlearning approach to MIMO precoding problem: Optimality and Robustness. \narXiv:2006.16646. \n[168] Xu, Duo, Mohit Agarwal, Ekansh Gupta, Faramarz Fekri, and Raghupathy \nSivakumar. 2020. Accelerating Reinforcement Learning Agent with EEG-based \nImplicit Human Feedback. arXiv:2006.16498. \n[169] Yu, Liang, Yi Sun, Zhanbo Xu, Chao Shen, Dong Yue, Tao Jiang, and Xiaohong \nGuan. 2020. Multi-Agent Deep Reinforcement Learning for HVAC Control in \nCommercial Buildings. arXiv:2006.14156. \n[170] Gleave, Adam, Michael Dennis, Shane Legg, Stuart Russell, and Jan Leike. 2020. \nQuantifying Differences in Reward Functions. arXiv:2006.13900. \n[171] Raileanu, Roberta, Max Goldstein, Denis Yarats, Ilya Kostrikov, and Rob \nFergus. 2020. Automatic Data Augmentation for Generalization in Deep \nReinforcement Learning. arXiv:2006.12862. \n[172] Liu, Haotian, and Wenchuan Wu. 2020. Online Multi-agent Reinforcement \nLearning for Decentralized Inverter-based Volt-VAR Control. arXiv:2006.12841. \n[173] Zou, Yayi, and Xiaoqi Lu. 2020. Gradient-EM Bayesian Meta-learning. \narXiv:2006.11764. \n[174] Panaganti, Kishan, and Dileep Kalathil. 2020. Model-Free Robust Reinforcement \nLearning with Linear Function Approximation. arXiv:2006.11608. \n[175] Zhang, Amy, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey \nLevine. 2020. Learning Invariant Representations for Reinforcement Learning \nwithout Reconstruction. arXiv:2006.10742. \n[176] Rahman, Arrasy, Niklas Hopner, Filippos Christianos, and Stefano V. Albrecht. \n2020. Open Ad Hoc Teamwork using Graph-based Policy Learning. \narXiv:2006.10412. \n[177] Jeong, Heejin, Hamed Hassani, Manfred Morari, Daniel D. Lee, and George J. \nPappas. 2020. Learning to Track Dynamic Targets in Partially Known \nEnvironments. arXiv:2006.10190. \n[178] Ning, Kun-Peng, and Sheng-Jun Huang. 2020. Reinforcement Learning with \nSupervision from Noisy Demonstrations. arXiv:2006.07808. \n[179] Dou, Yingtong, Guixiang Ma, Philip S. Yu, and Sihong Xie. 2020. Robust \nSpammer Detection by Nash Reinforcement Learning. arXiv:2006.06069. \nPREPRINT \n \n57 \n[180] Huang, Xiaoshui, Fujin Zhu, Lois Holloway, and Ali Haidar. 2020. Causal \nDiscovery from Incomplete Data using An Encoder and Reinforcement Learning. \narXiv:2006.05554. \n[181] Chow, Yinlam, Brandon Cui, MoonKyung Ryu, and Mohammad Ghavamzadeh. \n2020. Variational Model-based Policy Optimization. arXiv:2006.05443. \n[182] Jafferjee, Taher, Ehsan Imani, Erin Talvitie, Martha White, and Micheal \nBowling. 2020. Hallucinating Value: A Pitfall of Dyna-style Planning with \nImperfect Environment Models. arXiv:2006.04363. \n[183] Tian, Yuan, Manuel Arias Chao, Chetan Kulkarni, Kai Goebel, and Olga Fink. \n2020. Real-Time Model Calibration with Deep Reinforcement Learning. \narXiv:2006.04001. \n[184] Kallus, Nathan, and Masatoshi Uehara. 2020. Efficient Evaluation of Natural \nStochastic Policies in Offline Reinforcement Learning. arXiv:2006.03886. \n[185] Hou, Linfang, Liang Pang, Xin Hong, Yanyan Lan, Zhiming Ma, and Dawei Yin. \n2020. Robust Reinforcement Learning with Wasserstein Constraint. \narXiv:2006.00945. \n[186] Zhi, Jixuan, and Jyh-Ming Lien. 2020. Learning to Herd Agents Amongst \nObstacles: Training Robust Shepherding Behaviors using Deep Reinforcement \nLearning. arXiv:2005.09476. \n[187] Chandak, Yash, Georgios Theocharous, Shiv Shankar, Martha White, Sridhar \nMahadevan, and Philip S. Thomas. 2020. Optimizing for the Future in Non-\nStationary MDPs. arXiv:2005.08158. \n[188] Ding, Yiming, Ignasi Clavera, and Pieter Abbeel. 2020. Mutual Information \nMaximization for Robust Plannable Representations. arXiv:2005.08114. \n[189] Totaro, Simone, Ioannis Boukas, Anders Jonsson, and Bertrand Corn√©lusse. 2020. \nLifelong Control of Off-grid Microgrid with Model Based Reinforcement \nLearning. arXiv:2005.08006. \n[190] Xie, Zhaoming, Hung Yu Ling, Nam Hee Kim, and Michiel van de Panne. 2020. \nALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills. \narXiv:2005.04323. \n[191] Singh, Rahul, Qinsheng Zhang, and Yongxin Chen. 2020. Improving Robustness \nvia Risk Averse Distributional Reinforcement Learning. arXiv:2005.00585. \nPREPRINT \n \n58 \n[192] Kostrikov, Ilya, Denis Yarats, and Rob Fergus. 2020. Image Augmentation Is All \nYou Need: Regularizing Deep Reinforcement Learning from Pixels. \narXiv:2004.13649. \n[193] Chen, Jerry Zikun. 2020. Reinforcement Learning Generalization with Surprise \nMinimization. arXiv:2004.12399. \n[194] Ngo, Phuong D., and Fred Godtliebsen. 2020. Data-Driven Robust Control Using \nReinforcement Learning. arXiv:2004.07690. \n[195] Everett, Michael, Bjorn Lutjens, and Jonathan P. How. 2020. Certified \nAdversarial Robustness for Deep Reinforcement Learning. arXiv:2004.06496. \n[196] Koren, Mark, and Mykel J. Kochenderfer. 2020. Adaptive Stress Testing without \nDomain Heuristics using Go-Explore. arXiv:2004.04292. \n[197] Anahtarci, Berkay, Can Deha Kariksiz, and Naci Saldi. 2020. Q-Learning in \nRegularized Mean-field Games. arXiv:2003.12151. \n[198] Lindenberg, Bj√∂rn, Jonas Nordqvist, and Karl-Olof Lindahl. 2020. Distributional \nReinforcement Learning with Ensembles. arXiv:2003.10903.  \n[199] Shen, Qianli, Yan Li, Haoming Jiang, Zhaoran Wang, and Tuo Zhao. 2020. Deep \nReinforcement Learning with Robust and Smooth Policy. arXiv:2003.09534. \n[200] Zhang, Huan, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, \nand Cho-Jui Hsieh. 2020. Robust Deep Reinforcement Learning against \nAdversarial Perturbations on State Observations. arXiv:2003.08938. \n[201] Guo, Xin, Anran Hu, Renyuan Xu, and Junzi Zhang, 2020. A General \nFramework for Learning Mean-Field Games. arXiv:2003.06069. \n[202] Touati, Ahmed, Adrien Ali Taiga, and Marc G. Bellemare. 2020. Zooming for \nEfficient Model-Free Reinforcement Learning in Metric Spaces. arXiv:2003.04069. \n[203] Gao, Shen, Peihao Dong, Zhiwen Pan, and Geoffrey Ye Li. 2020. Reinforcement \nLearning Based Cooperative Coded Caching under Dynamic Popularities in \nUltra-Dense Networks. arXiv:2003.03758. \n[204] Lin, Jieyu, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, and \nNicolas Papernot. 2020. On the Robustness of Cooperative Multi-Agent \nReinforcement Learning. arXiv:2003.03722. \n[205] Derman, Esther, and Shie Mannor. 2020. Distributional Robustness and \nRegularization in Reinforcement Learning. arXiv:2003.02894. \nPREPRINT \n \n59 \n[206] Spooner, Thomas, and Rahul Savani. Robust Market Making via Adversarial \nReinforcement Learning. arXiv:2003.01820. \n[207] Chanc√°n, Marvin, and Michael Milford. 2020. MVP: Unified Motion and Visual \nSelf-Supervised Learning for Large-Scale Robotic Navigation. arXiv:2003.00667. \n[208] Ilboudo, Wendyam Eric Lionel, Taisuke Kobayashi, and Kenji Sugimoto. 2020. \nTAdam: A Robust Stochastic Gradient Optimizer. arXiv:2003.00179. \n[209] Tschantz, Alexander, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. \n2020. Reinforcement Learning through Active Inference. arXiv:2002.12636. \n[210] Kuutti, Sampo, Saber Fallah, and Richard Bowden. 2020. Training Adversarial \nAgents to Exploit Weaknesses in Deep Control Policies. arXiv:2002.12078. \n[211] Nguyen, Ngoc Duy, Thanh Thi Nguyen, and Saeid Nahavandi. 2020. A Visual \nCommunication Map for Multi-Agent Deep Reinforcement Learning. \narXiv:2002.11882. \n[212] Yang, Chao-Han Huck, Jun Qi, Pin-Yu Chen, Yi Ouyang, I-Te Danny Hung, \nChin-Hui Lee, and Xiaoli Ma. 2020. Enhanced Adversarial Strategically-Timed \nAttacks against Deep Reinforcement Learning. arXiv:2002.09027. \n[213] Sun, Tao, Han Shen, Tianyi Chen, and Dongsheng Li. 2020. Adaptive Temporal \nDifference Learning with Linear Function Approximation. arXiv:2002.08537. \n[214] Naderializadeh, Navid, Jaroslaw Sydir, Meryem Simsek, and Hosein Nikopour. \n2020. Resource Management in Wireless Networks via Multi-Agent Deep \nReinforcement Learning. arXiv:2002.06215. \n[215] Kamalaruban, Parameswaran, Yu-Ting Huang, Ya-Ping Hsieh, Paul Rolland, \nCheng Shi, and Volkan Cevher. 2020. Robust Reinforcement Learning via \nAdversarial training with Langevin Dynamics. arXiv:2002.06063. \n[216] Kallus, Nathan, and Masatoshi Uehara. 2020. Statistically Efficient Off-Policy \nPolicy Gradients. arXiv:2002.04014. \n[217] Lee, Gilwoo, Brian Hou, Sanjiban Choudhury, and Siddhartha S. Srinivasa. 2020. \nBayesian Residual Policy Optimization: Scalable Bayesian Reinforcement \nLearning with Clairvoyant Experts. arXiv:2002.03042. \n[218] Pacelli, Vincent, and Anirudha Majumdar. 2020. Learning Task-Driven Control \nPolicies via Information Bottlenecks. arXiv:2002.01428. \n[219] Yao, Jiahao, Marin Bukov, and Lin Lin. 2020. Policy Gradient based Quantum \nApproximate Optimization Algorithm. arXiv:2002.01068. \nPREPRINT \n \n60 \n[220] Nishio, Daichi, Daiki Kuyoshi, Toi Tsuneda, and Satoshi Yamane. 2020. \nDiscriminator Soft Actor Critic without Extrinsic Rewards. arXiv:2001.06808. \n[221] Dai, Tianhong, Kai Arulkumaran, Tamara Gerbert, Samyakh Tukra, Feryal \nBehbahani, and Anil Anthony Bharath. 2020. Analysing Deep Reinforcement \nLearning Agents Trained with Domain Randomisation. arXiv:1912.08324. \n[222] Zhang, Xinglong, Jiahang Liu, Xin Xu, Shuyou Yu, and Hong Chen. 2020. \nLearning-based Predictive Control for Nonlinear Systems with Unknown \nDynamics Subject to Safety Constraints. arXiv:1911.09827. \n[223] Lykouris, Thodoris, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. 2020. \nCorruption robust exploration in episodic reinforcement learning. \narXiv:1911.08689. \n[224] Salter, Sasha, Dushyant Rao, Markus Wulfmeier, Raia Hadsell, and Ingmar \nPosner. 2020. Attention-Privileged Reinforcement Learning. arXiv:1911.08363. \n[225] Han, Minghao. Yuan Tian, Lixian Zhang, Jun Wang, and Wei Pan. 2020. H‚àû \nModel-free Reinforcement Learning with Robust Stability Guarantee. \narXiv:1911.02875. \n[226] L√ºtjens, Bj√∂rn, Michael Everett, and Jonathan P. How. 2020. Certified \nAdversarial Robustness for Deep Reinforcement Learning. arXiv:1910.12908. \n[227] Uehara, Masatoshi, Jiawei Huang, and Nan Jiang. 2020. Minimax Weight and Q-\nFunction Learning for Off-Policy Evaluation. arXiv:1910.12809. \n[228] Li, Shuo, and Osbert Bastani. 2020. Robust Model Predictive Shielding for Safe \nReinforcement Learning with Stochastic Dynamics. arXiv:1910.10885. \n[229] Slaoui, Reda Bahi, William R. Clements, Jakob N. Foerster, and S√©bastien Toth. \n2020. Robust Visual Domain Randomization for Reinforcement Learning. \narXiv:1910.10537. \n[230] Zhang, Kaiqing, Bin Hu, and Tamer Ba≈üar. 2020. Policy Optimization for H2 \nLinear Control with H‚àû Robustness Guarantee: Implicit Regularization and \nGlobal Convergence. arXiv:1910.09496. \n[231] Liu, Zhuang, Xuanlin Li, Bingyi Kang, and Trevor Darrell. 2020. Regularization \nMatters in Policy Optimization. arXiv:1910.09191. \n[232] Yang, Jiachen, Brenden Petersen, Hongyuan Zha, and Daniel Faissol. 2020. \nSingle Episode Policy Transfer in Reinforcement Learning. arXiv:1910.07719. \nPREPRINT \n \n61 \n[233] Chen, Shuhang, Adithya M. Devraj, Fan Lu, Ana Bu≈°iƒá, and Sean P. Meyn. \n2020. Zap Q-Learning with Nonlinear Function Approximation. \narXiv:1910.05405. \n[234] Schwartz, Erez, Guy Tennenholtz, Chen Tessler, and Shie Mannor. 2020. \nLanguage is Power: Representing States Using Natural Language in \nReinforcement Learning. arXiv:1910.02789. \n[235] Yarats, Denis, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and \nRob Fergus. 2020. Improving Sample Efficiency in Model-Free Reinforcement \nLearning from Images. arXiv:1910.01741. \n[236] Kalweit, Gabriel, Maria Huegle, and Joschka Boedecker. 2020. Composite Q-\nlearning: Multi-scale Q-function Decomposition and Separable Optimization. \narXiv:1909.13518. \n[237] Ryu, Moonkyung, Yinlam Chow, Ross Anderson, Christian Tjandraatmadja, and \nCraig Boutilier. 2020. CAQL: Continuous Action Q-Learning. arXiv:1909.12397. \n[238] Li, Jiachen, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Henrik Iskov \nChristensen, and Hao Su. 2020. Multi-task Batch Reinforcement Learning with \nMetric Learning. arXiv:1909.11373. \n[239] Shen, Macheng, and Jonathan P. How. 2020. Robust Opponent Modeling via \nAdversarial Ensemble Reinforcement Learning in Asymmetric Imperfect-\nInformation Games. arXiv:1909.08735. \n[240] Kallus, Nathan, Masatoshi Uehara. 2020. Double Reinforcement Learning for \nEfficient Off-Policy Evaluation in Markov Decision Processes. arXiv:1908.08526. \n[241] Roy, Julien, Paul Barde, F√©lix G. Harvey, Derek Nowrouzezahrai, and \nChristopher Pal. 2020. Promoting Coordination through Policy Regularization in \nMulti-Agent Deep Reinforcement Learning. arXiv:1908.02269. \n[242] Urakami, Yusuke, Alec Hodgkinson, Casey Carlin, Randall Leu, Luca Rigazio, \nand Pieter Abbeel. 2020. DoorGym: A Scalable Door Opening Environment and \nBaseline Agent. arXiv:1908.01887. \n[243] Wang, Qisheng, Keming Feng, Xiao Li, and Shi Jin. 2020. PrecoderNet: Hybrid \nBeamforming for Millimeter Wave Systems with Deep Reinforcement Learning. \narXiv:1907.13266. \n[244] Bogdanovic, Miroslav, Majid Khadiv, Ludovic Righetti. 2020. Learning Variable \nImpedance Control for Contact Sensitive Tasks. arXiv:1907.07500. \nPREPRINT \n \n62 \n[245] Mankowitz, Daniel J., Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas \nAbdolmaleki, Jost Tobias Springenberg, Timothy Mann, Todd Hester, and \nMartin Riedmiller. 2020. Robust Reinforcement Learning for Continuous Control \nwith Model Misspecification. arXiv:1906.07516. \n[246] Li, Alexander C., Carlos Florensa, Ignasi Clavera, and Pieter Abbeel. Sub-policy \nAdaptation for Hierarchical Reinforcement Learning. arXiv:1906.05862. \n[247] Assran, Mahmoud, Joshua Romoff, Nicolas Ballas, Joelle Pineau, and Michael \nRabbat. 2020. Gossip-based Actor-Learner Architectures for Deep Reinforcement \nLearning. arXiv:1906.04585. \n[248] Gravell, Benjamin, Peyman Mohajerin Esfahani, and Tyler Summers. 2020. \nLearning robust control for LQR systems with multiplicative noise via policy \ngradient. arXiv:1905.13547. \n[249] Francis, Anthony, Aleksandra Faust, Hao-Tien Lewis Chiang, Jasmine Hsu, J. \nChase Kew, Marek Fiser, and Tsang-Wei Edward Lee. 2020. Long-Range Indoor \nNavigation with PRM-RL. arXiv:1902.09458. \n[250] Wang, Jingkang, Yang Liu, and Bo Li. 2020. Reinforcement Learning with \nPerturbed Rewards. arXiv:1810.01032. \n \nOther \n \n[251] Moher D, Liberati A, Tetzlaff J, Altman DG, The PRISMA Group. 2009. \nPreferred Reporting Items for Systematic Reviews and Meta-Analyses: The \nPRISMA Statement. PLoS Med 6(7): e1000097. doi \n10.1371/journal.pmed1000097 \n[252] Liu, Y., P. Ramachandran, Q. Liu, and J. Peng, Stein Variational Policy \nGradient. arXiv:1704.02399v1 [cs.LG] 7 April 2017. \n[253] 5. Liang, G., Zhu, X., Zhang, C. An Empirical Study of Bagging Predictors for \nDifferent Learning Algorithms. AAAI, pp. 1802‚Äì1803 (2011) \n[254] Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas \nand Schulman, John and Tang, Jie and Zaremba, Wojciech. Openai gym, \narXiv preprint arXiv:1606.01540, 2016. \n \n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-03-22",
  "updated": "2022-03-22"
}