{
  "id": "http://arxiv.org/abs/2203.12114v2",
  "title": "An Optical Control Environment for Benchmarking Reinforcement Learning Algorithms",
  "authors": [
    "Abulikemu Abuduweili",
    "Changliu Liu"
  ],
  "abstract": "Deep reinforcement learning has the potential to address various scientific\nproblems. In this paper, we implement an optics simulation environment for\nreinforcement learning based controllers. The environment captures the essence\nof nonconvexity, nonlinearity, and time-dependent noise inherent in optical\nsystems, offering a more realistic setting. Subsequently, we provide the\nbenchmark results of several reinforcement learning algorithms on the proposed\nsimulation environment. The experimental findings demonstrate the superiority\nof off-policy reinforcement learning approaches over traditional control\nalgorithms in navigating the intricacies of complex optical control\nenvironments. The code of the paper is available at\nhttps://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking.",
  "text": "An Optical Control Environment for Benchmarking\nReinforcement Learning Algorithms\nAbulikemu Abuduweili\nabulikea@andrew.cmu.edu\nRobotics Institute, Carnegie Mellon University\nChangliu Liu\ncliu6@andrew.cmu.edu\nRobotics Institute, Carnegie Mellon University\nAbstract\nDeep reinforcement learning has the potential to address various scientific problems. In this\npaper, we implement an optics simulation environment for reinforcement learning based\ncontrollers. The environment captures the essence of nonconvexity, nonlinearity, and time-\ndependent noise inherent in optical systems, offering a more realistic setting. Subsequently,\nwe provide the benchmark results of several reinforcement learning algorithms on the pro-\nposed simulation environment. The experimental findings demonstrate the superiority of\noff-policy reinforcement learning approaches over traditional control algorithms in navigat-\ning the intricacies of complex optical control environments.\n1\nIntroduction\nIn recent years, deep reinforcement learning (RL) has been used to solve challenging problems in various\nfields (Sutton & Barto, 2018), including self-driving car (Bansal et al., 2018) and robot control (Zhang et al.,\n2015). Among all applications, deep RL made significant progress in playing games on a superhuman level\n(Mnih et al., 2013; Silver et al., 2014; 2016; Vinyals et al., 2017). Beyond playing games, deep RL has the\npotential to strongly impact the traditional control and automation tasks in the natural science, such as\ncontrol problems in chemistry (Dressler et al., 2018), biology (Izawa et al., 2004), quantum physics (Bukov\net al., 2018), optics and photonics (Genty et al., 2020).\nIn optics and photonics, there are particular potentials for RL methods to drive the next generation of\noptical laser technologies (Genty et al., 2020).\nThat is not only because there are increasing demands\nfor adaptive control and automation (of tuning and control) for optical systems (Baumeister et al., 2018),\nbut also because many phenomena in optics are nonlinear and multidimensional (Shen, 1984), with noise-\nsensitive dynamics that are extremely challenging to model using conventional approaches. RL methods\nare able to control multidimensional environments with nonlinear function approximation (Dai et al., 2018).\nThus, exploring RL controllers becomes increasingly promising in optics and photonics as well as in scientific\nresearch, medicine, and other industries (Genty et al., 2020; Fermann & Hartl, 2013).\nIn the field of optics and photonics, Stochastic Parallel Gradient Descent (SPGD) algorithm with a PID\ncontroller has traditionally been employed to tackle control problems (Cauwenberghs, 1993; Zhou et al., 2009;\nAbuduweili et al., 2020a). These problems typically involve adjusting system parameters, such as the delay\nline of mirrors, with the objective of maximizing a reward, such as optical pulse energy. SPGD is a specific\ncase of the stochastic error descent method (Cauwenberghs, 1993; Dembo & Kailath, 1990), which operates\nbased on a model-free distributed learning mechanism. The algorithm updates the parameters by perturbing\neach individual parameter vector, resulting in a decrease in error or an increase in reward. However, the\napplicability of SPGD is limited to convex or near-convex problems, while many control problems in optics\nexhibit non-convex characteristics. As a result, SPGD struggles to find the global optimum of an optics\ncontrol system unless the initial state of the system is in close proximity to the global optimum. Traditionally,\nexperts would manually tune the initial state of the optical system, followed by the use of SPGD-PID to\n1\narXiv:2203.12114v2  [cs.LG]  1 Oct 2023\ncontrol the adjusted system. Nevertheless, acquiring such expert knowledge becomes increasingly challenging\nas system complexity grows.\nTo enable efficient control and automation in optical systems, researchers have introduced deep reinforcement\nlearning (RL) techniques (T√ºnnermann & Shirakawa, 2019; Sun et al., 2020; Abuduweili et al., 2020b; 2021).\nPrevious studies predominantly focused on implementing Deep Q-Network (DQN) (Mnih et al., 2013) and\nDeep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015) in simple optical control systems, aiming\nto achieve comparable performance to traditional SPGD-PID controllers (T√ºnnermann & Shirakawa, 2019;\nValensise et al., 2021). However, there is a lack of research evaluating a broader range of RL algorithms\nin more complex optical control environments. The exploration and evaluation of RL algorithms in real-\nworld optical systems pose significant challenges due to the high cost and the need for experienced experts to\nimplement multiple optical systems with various configurations. Even for a simple optical system, substantial\nefforts and resources are required to instrument and implement RL algorithms effectively.\nSimulation has been widely utilized in the fields of robotics and autonomous driving since the early stages of\nresearch (Pomerleau, 1998; Bellemare et al., 2013). As the interest and application of learning-based robotics\ncontinue to grow, the role of simulation becomes increasingly crucial in driving research advancements. We\nbelieve that simulation holds equal importance in evaluating RL algorithms for optical control. However, to\nthe best of our knowledge, there is currently no open-source RL environment available for optical control\nsimulation.\nIn this paper, we present OPS (Optical Pulse Stacking), an open and scalable simulator designed for con-\ntrolling typical optical systems. The underlying physics of OPS aligns with various optical applications,\nincluding coherent optical inference (Wetzstein et al., 2020) and linear optical sampling (Dorrer et al., 2003),\nwhich find applications in precise measurement, industrial manufacturing, and scientific research. A typi-\ncal optical pulse stacking system involves the direct and symmetrical stacking of input pulses to multiply\ntheir energy, resulting in stacked output pulses (T√ºnnermann & Shirakawa, 2017; Stark et al., 2017; As-\ntrauskas et al., 2017; Yang et al., 2020). By introducing the OPS optical control simulation environment,\nour objective is to encourage exploration of RL applications in optical control tasks and further investigate\nRL controllers in natural sciences. We utilize OPS to evaluate several important RL algorithms, including\nTwin Delayed Deep Deterministic Policy Gradient (TD3) (Fujimoto et al., 2018), Soft Actor-Critic (SAC)\n(Haarnoja et al., 2018), and Proximal Policy Optimization (PPO) (Schulman et al., 2017). Our findings\nindicate that in a simple optical control environment (nearly convex), the traditional SPGD-PID controller\nperforms admirably. However, in complex environments (non-convex optimization), SPGD-PID falls short,\nand RL-trained policies outperform SPGD-PID. Following the reporting of these RL algorithm results, we\ndiscuss the potential and challenges associated with RL algorithms in real-world optical systems. By provid-\ning the OPS simulation environment and conducting RL algorithm experiments, we aim to facilitate research\non RL applications in optics, benefiting both the machine learning and optics communities. The code of the\npaper is available at https://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking.\n2\nSimulation environment\n2.1\nPhysics of the simulation\nThe optical pulse stacking (OPS), also known as pulse combination, system employs a recursive approach\nto stack optical pulses in the time domain. The dynamics of the OPS are similar to the recurrent neural\nnetworks (RNN) or Wavenet architecture (Oord et al., 2016). We illustrate the dynamics of the OPS in\nRNN style as shown in Fig. 1. In the OPS system, the input consists of a periodic pulse train 1 with a\nrepetition period of T. Assuming the basic function of the first pulse at time step t is denoted as E1 = E(t)\n(a complex function), the subsequent pulses can be described as E2 = E(t + T), E3 = E(t + 2T), and so on.\nThe OPS system recursively imposes time delays to earlier pulses in consecutive pairs. For instance, in the\nfirst stage of OPS, a time-delay controller imposes the delay œÑ1 on pulse 1 to allow it to combine (overlap)\nwith pulse 2. With the appropriate time delay, pulse 1 can be stacked with the next pulse, E2, resulting\n1The periodic pulse train is typically emitted by lasers, where each laser pulse‚Äôs wave function is nearly identical except for\nthe time delay.\n2\nùê∏\"\nùúè\"\nùúè$\nùê∏%\nùê∏$\nùúè\"\nùê∏&\nùê∏'\nùúè\"\nùê∏(\nùê∏)\nùúè\"\nùê∏*\nùê∏\",%\nùê∏$,&\nùúè%\nùê∏',(\nùê∏),*\nùúè%\nùê∏\",%,$,&\nùê∏',(,),*\n1st stage stacking\n2nd stage stacking\n3rd stage stacking\nùê∏;<=\nFigure 1: Illustration of the principle of optical pulse stacking. Only 3-stage pulse stacking was plotted for\nsimplicity.\nin the stacked pulses E1,2 = E(t + œÑ1) + E(t + T). Similarly, pulse 3 can be stacked with pulse 4, creating\nE3,4 = E(t + 2T + œÑ1) + E(t + 3T), and so forth. In the second stage of OPS, an additional time delay,\nœÑ2, is imposed on E1,2 to allow it to stack with E3,4, resulting in E1,2,3,4. This stacking process continues\nin each subsequent stage of the OPS controller, multiplying the pulse energy by a factor of 2N by stacking\n2N pulses, where N time delays (œÑ1, œÑ2, ..., œÑN) are required for control and stabilization. Additional details\nabout the optical pulse stacking are shown in appendix A.1.\n20\n16\n12\n8\n4\n0\n4\n8\n12\n16\n20\ntime delay \n0.2\n0.4\n0.6\n0.8\n1.0\ncombined pulse energy\n(a) One stage OPS (1-d)\n1st stage time delay \n4\n2\n0\n2\n4\n2nd stage time delay \n4\n2\n0\n2\n4\ncombined pulse energy\n0.2\n0.4\n0.6\n0.8\n1.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(b) Two stage OPS (2-d)\nFigure 2: Function of the (a) 1-stage OPS: pulse energy P1(œÑ1) w.r.t. delay line œÑ1. (b) 2-stage OPS: pulse\nenergy P2(œÑ1, œÑ2) w.r.t. delay lines (œÑ1, œÑ2).\n2.2\nControl objective and noise\nThe objective of controlling an OPS system is to maximize the energy of the final stacked (output) pulse\nby adjusting the time delays. We denote the vector of time delays as œÑ = [œÑ1, œÑ2, ¬∑ ¬∑ ¬∑ , œÑN], and Eout(t; œÑ)\nrepresents the final stacked pulse under the time delay configuration œÑ. For an N-stage OPS system, the\nenergy of the final stacked pulse, denoted as PN(œÑ), is computed as the integral of the norm of Eout(t; œÑ) over\ntime. In mathematical terms, we express it as PN(œÑ) =\nR\n|Eout(t; œÑ)|dt. To formulate the objective function\n3\nfor controlling an N-stage OPS system, we aim to find the optimal set of time delays œÑ ‚àóthat maximizes\nPN(œÑ). The objective function can be defined as follows:\narg max\nœÑ\nPN(œÑ) = arg maxœÑ1,œÑ2,...,œÑN PN(œÑ1, œÑ2, ..., œÑN)\n(1)\nWhen ignoring noise, the objective function of the final pulse energy, PN, with respect to the time delays, œÑ,\ncan be derived based on optical coherence. Figure 2(a) depicts the pulse energy function P1(œÑ1) in a 1-stage\nOPS system, showing the relationship between pulse energy and the time delay œÑ1. Similarly, Fig. 2(b)\ndisplays the function surface of P2(œÑ1, œÑ2) in a 2-stage OPS system, illustrating how pulse energy varies with\nthe first and second stage time delays (œÑ1, œÑ2). As evident from the figures, the control objective of the OPS\nsystem is nonlinear and non-convex, even when noise is disregarded. This inherent complexity arises due to\nfactors such as optical periodicity and the nonlinearity of coherent interference. Consequently, achieving the\nglobal optimum or better local optima becomes a challenging task for any control algorithms, particularly\nwhen starting from a random initial state.\nHowever, in practical scenarios, noise cannot be ignored, and the OPS system is highly sensitive to noise.\nThis sensitivity is primarily due to the pulse wavelength being on the order of micrometers (1¬µm = 10‚àí6m).\nEnvironmental noise, including vibrations of optical devices and atmospheric temperature drift, can easily\ncause shifts in the time delays, resulting in changes to the output pulses. As a result, the objective function\nin real-world applications is much more complex than what is depicted in Fig. 2, especially for higher-\nstage OPS systems with higher dimensions. Consequently, achieving the control objective becomes even\nmore challenging in the presence of unknown initial states and unpredictable noise in such noise-sensitive\ncomplex systems (Genty et al., 2020).\nTherefore, model-based controllers face significant difficulties in\nimplementation.\nIn this paper, we primarily focus on model-free reinforcement learning approaches to\naddress these challenges.\nIn this simulation, we incorporate two types of noise: fast noise arising from device vibrations and slow\nnoise caused by temperature drift. The fast noise is modeled as a zero-mean Gaussian random noise with\nvariance œÉ2, following the simulation noise approach outlined in (T√ºnnermann & Shirakawa, 2019). On the\nother hand, the slow noise ¬µt accounts for temperature drift and is represented as a piecewise linear function\n(Ivanova et al., 2021). To capture the combined effect of these two noise sources, we define the overall noise\net as a random process. Specifically, we can express it as follows:\nE [et] = ¬µt,\nVAR [et] = œÉ2\n(2)\n2.3\nReinforcement learning environment\nInteractions with RL agent. An RL agent interacts with the OPS environment in discrete time steps,\nas shown in Fig. 3. At each time step t, the RL agent receives the current state of the OPS environment,\ndenoted as st. Based on this state, the agent selects an action at to be applied to the environment. The action\ncould involve adjusting the time delays œÑ in the OPS system. Once the action is chosen, it is transmitted\nto the OPS environment, which then processes the action and transitions to a new state st+1. The OPS\nenvironment provides feedback to the RL agent in the form of a reward rt. The reward serves as a measure\nof how well the OPS system is achieving the objective of maximizing the final stacked pulse energy. The RL\nagent utilizes the experience tuple (st, at, st+1, rt) to learn and update its policy œÄ(a, s) over time. The goal\nof the agent is to learn a policy that maximizes the expected cumulative reward over the interaction with\nthe OPS environment.\nState space. The state space of the OPS system is a continuous and multidimensional vector space. The\nstate value at time step t, denoted as st, corresponds to the pulse amplitude measurement of the final stacked\npulse, given by st = |Eout(t; œÑ)|. Therefore, st provides a time-domain representation of the final stacked\npulse, offering direct insight into the control performance. In practical implementations, the pulse amplitude\nis typically detected using a photo-detector and subsequently converted into digital time-series signals. In\nour simulation, we have incorporated real-time rendering of the pulse amplitude to facilitate the monitoring\nof the control process.\n4\nOPS Environment\nInitial pulses\nRL agent\nstate ùë†\nùê∏! ùê∏\" ùê∏# ùê∏$\nùê∏!,\"\nùê∏#,$\nùê∏!,\",#,$\n1st stage\ntime delay ùúè!\n2nd stage\ntime delay ùúè\"\nPhoto\ndetection\nreward ùëü\n1st stacked\npulses\n2nd stacked\npulses\naction ùëé\nùúè!\nùúè\"\nFigure 3: Illustration of the interaction between RL agent and OPS environment. Only 2-stage OPS was\nplotted for simplicity.\nAction space. The action space of an N-stage OPS environment is a continuous and N-dimensional vector\nspace. At each time step t, the action at corresponds to an additive time delay value ‚àÜœÑ(t) for the N-stage\nOPS environment: at = ‚àÜœÑ(t) = œÑ(t+1)‚àíœÑ(t). The OPS environment applies the additive time delay value\na(t) to transition to the next state.\nReward. As mentioned in section 2.2, the objective of the OPS controller is maximizing the final stacked\npulse energy PN(œÑ). In our simulation, we use the normalized final pulse energy as the reward value. The\nreward at each time step is defined as:\nr = ‚àí(PN(œÑ) ‚àíPmax)2\n(Pmin ‚àíPmax)2 ,\n(3)\nwhere Pmax is the maximum pulse energy at the global optimum, and Pmin is the minimum pulse energy.\nThe maximum reward 0 achieved when P(œÑ) = Pmax (peak position of Fig. 2(b)) .\nState transition function. The environmental noise has direct impacts on the delay lines, including the\nvibration and temperature-induced shift noise of the delay line devices. Therefore, in the state transition\nprocess, the actual applied delay line value œÑreal(t + 1) is a combination of the action at and the noise et.\nSpecifically, it can be expressed as:\nœÑreal(t + 1) = œÑreal(t) + at + et.\n(4)\nAfter selecting the pulses, the real-time delay œÑreal(t + 1) is imposed on them using delay line devices, which\nintroduce additional time delays for the pulses. The state transition process is governed by the combination\nof the current state, the action taken, and the noise present. The specific form of the state transition follows\nthe principles of coherent light interference (Saleh & Teich, 2019). Let f be an interference observation\nfunction. Then the state transition can be written as:\nst+1 = f(œÑreal(t + 1)) = f(œÑreal(t) + at + et) = f(f ‚àí1(st) + at + et)\n(5)\nIt is important to note that the slow-changing noise term E [et] = ¬µt follows a piecewise linear function that\nchanges slowly over time. During episodic training for RL agents, ¬µt can be treated as a constant value\nwithin an episode. However, the value of ¬µt may vary from one episode to the next. In this case, assuming\nthat ¬µt changes very slowly, the OPS control process can be modeled as a Markov decision process (MDP).\nMode\nInitial state\nNoise\nObjective\nEasy\nnear the\noptimum\ntime-independent:\nd¬µt\ndt ‚â°0\nconvex\nMedium\nrandom\ntime-independent:\nd¬µt\ndt ‚â°0\nnon-convex\nHard\nrandom\ntime-dependent:\nd¬µt\ndt Ã∏‚â°0\nnon-convex\nTable 1: Different difficulty modes on OPS.\nfrom optics_env import OPS_env\nenv = OPS_env(stage=5, mode= \"medium\")\nenv.reset()\ndone = False\nwhile not done:\naction = env.action_space.sample()\nobservation, reward, done, info = env.step(action)\nenv.render()\nFigure 4: Example code of the OPS environment.\n5\nControl difficulty of the environment. We have implemented the OPS environment to support arbitrary\nstages of pulse stacking (N ‚àà1, 2, 3, ...). As the number of stages increases, the control task becomes more\nchallenging. In addition to the customizable number of stages, we have also introduced three difficulty modes\n(easy, medium, and hard) for each stage of OPS, as outlined in table 1. The difficulty mode is determined\nby the initial state of the system and the distribution of noise. These difficulty modes allow for different\nlevels of complexity and challenge in the control task, providing flexibility for evaluating and training control\nalgorithms in various scenarios.\n‚Ä¢ Easy mode. In the easy mode of the OPS environment, the initial state of the system is set to be near\nthe global optimum. This configuration is often encountered in traditional optics control problems where\nexperts fine-tune the initial state to facilitate easier control. As depicted in Fig. 5(a), we provide an\nexample of the initial state for the easy mode in a 3-stage OPS environment. The proximity of the initial\nstate to the global optimum allows the control objective in the easy mode to be considered convex.\n‚Ä¢ Medium mode. In the medium mode of the OPS environment, the initial state of the system is randomly\ndetermined, as illustrated in Fig. 5(b).\nThis random initialization introduces non-convexity into the\ncontrol problem, making it more challenging to solve. However, in the medium mode, the noise present\nin the system is time-independent. We model this noise as a Gaussian distribution, where et follows a\nnormal distribution N(¬µ, œÉ). This setting aligns with classical reinforcement learning and typical Markov\nDecision Process (MDP) settings, where the noise distribution remains the same throughout each episode.\n‚Ä¢ Hard mode. In the hard mode of the OPS environment, similar to the medium mode, the initial state of\nthe system is randomly determined. However, in contrast to the medium mode, the behavior of the noise\nin the hard mode is more complex. The mean value of the noise distribution ¬µt becomes a time-dependent\nvariable that slowly changes over time. This time-dependent noise introduces additional challenges and\ncomplexity to the control problem. In this case, the control problem deviates from a typical Markov\nDecision Process (MDP) setting. By incorporating the noise term into the state definition as ÀÜst = [st; et],\nwe effectively convert the control process into a Partially Observable Markov Decision Process (POMDP).\nThe hard mode is designed to mimic real-world settings more closely. In practical applications, when\ndeploying the trained model in a testing environment, we often encounter temperature drift, which causes\nthe noise distribution of the testing environment to differ from the training environment. Therefore, the\nhard mode simulates the realistic scenario where the noise distribution is not stationary and may vary\nover time, making the control problem more challenging and closer to real-world conditions.\n80\n60\n40\n20\n0\n20\n40\n60\n80\nTime\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIntensity (normalized)\n(a) Initial state: easy mode\n80\n60\n40\n20\n0\n20\n40\n60\n80\nTime\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIntensity (normalized)\n(b) Initial state: medium/hard mode\n80\n60\n40\n20\n0\n20\n40\n60\n80\nTime\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIntensity (normalized)\n(c) Target optimal state\nFigure 5: Rendering examples of the (a) initial state for easy mode, (b) initial state for medium or hard\nmode, (c) global optimal target state in a 2-stage OPS. In the initial state of the easy mode is closer to the\ntarget state. The initial state of medium or hard mode is random.\nAPI & sample usage. The simulation in this study is based on the Nonlinear-Optical-Modeling (Hult,\n2007), which provides the optical and physical fundamentals for the OPS environment. To facilitate in-\ntegration and compatibility with existing frameworks and tools, the OPS environment is designed to be\ncompatible with the widely used OpenAI Gym API (Brockman et al., 2016). To demonstrate the usage of\nthe OPS environment, we provide an example code snippet in Fig. 4.\nFeatures of the OPS environment. We summarize the key features of the OPS environment as follows:\n6\n‚Ä¢ Open-source optical control environment: To the best of our knowledge, this is the first open-sourced RL\nenvironment for optical control problems. The use of open-source licenses enables researchers to inspect\nthe underlying code and modify the environment if necessary to test new research ideas.\n‚Ä¢ Scalable and difficulty-adjustable scientific environment: Unlike many RL environments that are easy to\nsolve, our OPS environment allows flexible adjustment of difficulty. The dimension of the action space\ncan easily scale with the stage number N. Choosing a larger N with the hard mode makes controlling the\nenvironment more challenging. Effective solutions to hard scientific control problems can have a broad\nimpact on various scientific control problems (Genty et al., 2020; Fermann & Hartl, 2013).\n‚Ä¢ Realistic noise: In the hard mode of the OPS environment, we model the noise distribution as a time-\ndependent function. This mirrors a real-world scenario in which a noise-distribution shift occurs between\nthe testing and training environments. Such realistic noise modeling is particularly relevant for noise-\nsensitive systems (Ivanova et al., 2021) and increases the stochasticity of the environment.\n‚Ä¢ Extendable state and structural information: When ¬µt changes very slowly, the OPS control process can\nbe formulated as an MDP. In cases where ¬µt undergoes substantial variations and the noise et varies\nwith time, the inclusion of the noise term within the state definition as ÀÜst = [st; et] seamlessly transforms\nthe control process into a POMDP. Furthermore, we can explore the structural information or physical\nconstraints from the function of the OPS (see Fig. 2) and incorporate it with RL controllers.\n3\nExperiments\n3.1\nExperimental setup\nWe present benchmark results for various control algorithms, including a traditional control algorithm and\nthree state-of-the-art reinforcement learning algorithms:\n‚Ä¢ SPGD (Stochastic Parallel Gradient Descent) algorithm with PID controller: SPGD is a widely used\napproach for controlling optical systems (Cauwenberghs, 1993; Zhou et al., 2009). In SPGD, the objective\ngradient estimation is achieved by applying a random perturbation to the time delay value, denoted as\nŒ¥œÑ. The update formula is œÑ(t+1) = œÑ(t)+Œ∑[PN(œÑ(t)+Œ¥œÑ)‚àíPN(œÑ(t))]Œ¥œÑ, where Œ∑ is the update step-size.\nThe output of the SPGD algorithm is then sent to a PID controller to control the system. In this work,\nwe refer to the SPGD-PID controller as the SPGD controller.\n‚Ä¢ PPO (Proximal Policy Optimization) is an on-policy reinforcement learning algorithm (Schulman et al.,\n2017). It efficiently updates its policy within a trust region by penalizing KL divergence or clipping the\nobjective function.\n‚Ä¢ SAC (Soft Actor-Critic) is an off-policy reinforcement learning algorithm (Haarnoja et al., 2018). It\nlearns two Q-functions and utilizes entropy regularization, where the policy is trained to maximize a\ntrade-off between expected return and entropy.\n‚Ä¢ TD3 (Twin Delayed Deep Deterministic policy gradient) is an off-policy reinforcement learning algorithm\n(Fujimoto et al., 2018). It learns two Q-functions and uses the smaller of the two Q-values to form the\ntargets in the loss functions. Additionally, TD3 adds noise to the target action for exploration.\nWe used the algorithms implemented in stable-baselines-3 (Raffin et al., 2019).\nThe training procedure\nfor an RL agent consists of multiple episodes, and each episode consists of 200 steps. Across each of the\nexperimental configurations, we executed the experiments with 20 different random seeds and present the\nmean results accompanied by their respective standard deviations. A detailed explanation of RL algorithms\nand their associated hyperparameters is presented in appendices B.1 and B.2.\n3.2\nResults on controlling 5-stage OPS\nIn this section, we present the results obtained for the 5-stage OPS system, which involves stacking 32\npulses. We evaluate all four algorithms in three difficulty modes: easy, medium, and hard. It is important\nto note that SPGD is a training-free method, as it relies on a fixed policy. Therefore, we only evaluate the\ntesting performance of SPGD. For the RL algorithms (PPO, TD3, and SAC), we assess both the training\nconvergence and the testing performance of the trained policy.\n7\nThe training curves, depicting the reward per step during training iterations, are shown in Fig. 6(a) for the\neasy mode, Fig. 6(b) for the medium mode, and Fig. 6(c) for the hard mode. From these plots, we observe\nthat TD3 and SAC exhibit similar performance, which is consistently higher than that of PPO across all three\ndifficulty modes. Notably, in the hard mode of the environment, the convergence speed of the algorithms\nslows down. Moreover, the final convergence value decreases as the difficulty of the environment increases.\nFor instance, in the easy mode, TD3 converges to a reward value of ‚àí0.05 within 150,000 steps, while it\ntakes 200,000 steps to converge to a reward value of ‚àí0.15 in the hard mode.\n0\n50000 100000 150000 200000 250000 300000\nIterations\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\nTraining Reward\nTD3\nSAC\nPPO\n(a) Training curve: easy mode\n0\n50000 100000 150000 200000 250000 300000\nIterations\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\nTraining Reward\nTD3\nSAC\nPPO\n(b) Training curve: medium mode\n0\n50000 100000 150000 200000 250000 300000\nIterations\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\nTraining Reward\nTD3\nSAC\nPPO\n(c) Training curve: hard mode\nFigure 6:\nTraining curve for SAC, TD3, and PPO on 5-stage OPS environment for (a) easy mode, (b)\nmedium mode, and (c) hard mode. The dashed region shows the area within the standard deviation.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \nTD3\nSAC\nPPO\nSPGD\n(a) Testing curve: easy mode\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \nTD3\nSAC\nPPO\nSPGD\n(b) Testing curve: medium mode\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \nTD3\nSAC\nPPO\nSPGD\n(c) Testing curve: hard mode\nFigure 7:\nEvaluation of the stacked pulse power PN (normalized) of different policies in the testing envi-\nronment for (a) easy mode, (b) medium mode, and (c) hard mode.\nFollowing the training of RL agents, we proceeded to evaluate the performance of the trained policies in the\ntesting environment. The final pulse energy PN achieved under different iterations is depicted in Fig. 7(a)\nfor the easy mode, Fig. 7(b) for the medium mode, and Fig. 7(c) for the hard mode. We observed that SPGD\nperforms admirably in the easy mode, where the control problem is close to convex. However, its performance\ndeteriorates significantly in the medium and hard modes, which are non-convex control problems.\nThis\ndisparity arises because RL controllers are capable of learning better policies through exploration in non-\nconvex settings. Furthermore, the off-policy RL algorithms (TD3 and SAC) outperform the on-policy RL\nalgorithm (PPO) in our simulation environment. Across all methods, the testing performance in the hard\nmode is lower than that in the medium mode, despite both being non-convex control problems.\nThis\ndiscrepancy can be attributed to the more complex and realistic noise present in the hard mode, which slows\ndown the convergence rate and reduces the final pulse energy in the testing environment.\nThe final pulse energy PN is reported for both the training and testing environments, detailing the perfor-\nmance of the trained policy as presented in table 2. It‚Äôs noteworthy that the training and testing environments\nfor the easy and medium modes exhibit similarity, akin to the classical Atari setup, with performance dis-\ncrepancies primarily arising from inherent randomness. However, the hard mode introduces distinct noise\ncharacteristics between the training and testing environments due to gradual temperature drift. This dis-\ncrepancy results in a performance gap between the two domains.\nAs depicted, in the easy mode, SPGD\nsignificantly outperforms other algorithms, as indicated by Welch‚Äôs t-test with p < 0.05 (Welch, 1947). For\nthe intricate medium and hard modes, TD3 and SAC exhibit superior performance compared to PPO and\n8\nSPGD, with statistical significance according to Welch‚Äôs t-test. Notably, there exist no substantial differ-\nences between TD3 and SAC. The fundamental differentiation between SAC, TD3, and PPO lies in their\nunderlying methodologies. SAC and TD3 operate as off-policy approaches, whereas PPO functions as an\non-policy method. This can potentially be attributed to PPO‚Äôs susceptibility to becoming ensnared in local\noptima due to limited exploration and a tendency to emphasize recent data. On the other hand, SAC and\nTD3 harness historical experiences, drawn from a replay buffer with a capacity of 10000, thus avoiding undue\nreliance on only the most recent data. Our observations align harmoniously with insights derived from a\nbenchmark study (Yu et al., 2020). As elucidated in (Yu et al., 2020), SAC achieves successful task resolu-\ntion across the board, whereas PPO demonstrates adeptness in most tasks while encountering challenges in\nsome cases. The experimental results of different stage OPS environments and discussions can be found in\nappendices B.3 and B.5.\nTable 2: The performance of the implemented algorithms is evaluated based on the Final Pulse Energy PN\n(mean ¬± standard deviation) in 5-stage OPS environments. The results of the best-performing algorithm on\neach task, as well as all algorithms that have performances that are not statistically significantly different\n(Welch‚Äôs t-test with p < 0.05), are highlighted in boldface.\nMode\nEvaluation environment\nSPGD\nPPO\nSAC\nTD3\neasy\ntraining\n0.9919 ¬± 0.0121\n0.8184 ¬± 0.0884\n0.9410 ¬± 0.0572\n0.9552 ¬± 0.0483\ntesting\n0.9909 ¬± 0.0125\n0.8067 ¬± 0.0661\n0.9403 ¬± 0.0531\n0.9572 ¬± 0.0501\nmedium\ntraining\n0.6155 ¬± 0.0271\n0.7591 ¬± 0.0728\n0.9145 ¬± 0.0727\n0.9353 ¬± 0.0451\ntesting\n0.6178 ¬± 0.0263\n0.7519 ¬± 0.0623\n0.9098 ¬± 0.0838\n0.9285 ¬± 0.0437\nhard\ntraining\n0.5321 ¬± 0.0352\n0.6977 ¬± 0.0732\n0.8739 ¬± 0.0775\n0.8724 ¬± 0.0680\ntesting\n0.5132 ¬± 0.0348\n0.6526 ¬± 0.0687\n0.8371 ¬± 0.0804\n0.8488 ¬± 0.0615\n3.3\nRun-time Analysis\nOur experiments were conducted on an Ubuntu 18.04 system, with an Nvidia RTX 2080 Ti (12 GB) GPU,\nIntel Core i9-7900x processors, and 64 GB memory. We present the time usage of distinct RL algorithms\nwithin a 5-stage OPS environment under hard mode, as delineated in table 3. In the table, \"TPS\" signifies\ntime usage per step, measured in seconds. The \"convergence step\" designates the iteration count at which an\nalgorithm attains its peak value, beyond which further iterations yield minimal alterations. During training,\nthe time per step encompasses simulation TPS (denoting OPS simulation time per step) and optimization\nTPS (reflecting the time spent on gradient descent optimization per step). For instance, TD3 involves a\nsimulation cost of 0.0086s per step and an optimization of 0.0175s per step. With approximately 212018\nsteps to convergence, the total training duration amounts to about 5456s. Notably, there exists negligible\ndisparity in training times across various algorithms. In the post-training inference phase, the inference TPS\nrepresents the time taken for action calculation per step. For instance, TD3‚Äôs inference cost is 0.0036s per\nstep, with around 58 steps to achieve high pulse energy, leading to a total control time of approximately 0.71\ns. As evident, TD3 and SAC distinctly outperform PPO in terms of inference time efficiency, showcasing\nremarkable superiority. The GPU memory usage of all algorithms is about 1000 MB.\nTable 3:\nTime usage (simulation, optimization, inference time) of the RL algorithms on 5-stage OPS\nenvironment with hard mode. The time usage per step (TPS) is quantified in seconds. The algorithm with\nthe optimal time efficiency, as well as algorithms that are not significantly different-performed (Welch‚Äôs t-test\nwith p < 0.05), are highlighted in boldface.\nTime\nSimulation\nTraining\nInference\nsimulation TPS\noptimization TPS\nconvergence step\ntotal time\ninference TPS\nconvergence step\ntotal time\nPPO\n0.0086\n0.0274\n142666 ¬± 25390\n5141 ¬± 979\n0.0057\n78 ¬± 10\n1.13 ¬± 0.21\nSAC\n0.0086\n0.0199\n168844 ¬± 24889\n4833 ¬± 740\n0.0046\n67 ¬± 10\n0.89 ¬± 0.18\nTD3\n0.0086\n0.0175\n212018 ¬± 28484\n5456 ¬± 804\n0.0036\n58 ¬± 7\n0.71 ¬± 0.12\n3.4\nComparison of the different settings of OPS environment\nIn this section, we investigate the impact of different modes (easy, medium, hard) and stage numbers (N) in\nan N-stage OPS environment. We evaluate the trained TD3 and SAC policies, as well as SPGD, on different\n9\n1\n2\n3\n4\n5\n6\nStage number in OPS\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTesting pulse energy\nTD3@hard\nSAC@hard\nSPGD@hard\nTD3@medium\nSAC@medium\nSPGD@medium\nTD3@easy\nSAC@easy\nSPGD@easy\n(a) Testing curve (final return PN)\n1\n2\n3\n4\n5\n6\nStage number in OPS\n0\n50000\n100000\n150000\n200000\n250000\n300000\nConvergence step\nTD3@hard\nSAC@hard\nTD3@medium\nSAC@medium\nTD3@easy\nSAC@easy\n(b) Training convergence step\nFigure 8: (a) Final return PN of different stage OPS on testing environment controlled with TD3 or SAC.\n(b) Convergence steps for the training of TD3 and SAC on different stage OPS environments. The dashed\nregion shows the area within the standard deviation.\ntesting environments with varying stage numbers. Figure 8(a) illustrates the final return PN in relation to\nthe stage number N in the N-stage OPS environment, comparing the performance of different algorithms\nacross different modes. From the figure, we draw the following conclusions: (1) For the easy mode, SPGD\noutperforms SAC and TD3, and all methods achieve almost-optimal performance regardless of the stage\nnumber N. (2) Across both the medium and hard modes, the performance of off-policy RL methods (SAC\nand TD3) is comparable, and RL methods are better than SPGD. (3) As the stage number increases in the\nhard and medium modes, the performance of SPGD drops significantly. Moreover, for N ‚â•4, RL methods\n(SAC and TD3) outperform SPGD significantly (under Welch‚Äôs t-test). Figure 8(b) illustrates the training\nconvergence steps for different stage OPS. It can be observed that as the stage number increases, the number\nof steps required for training convergence also increases significantly.\n0\n50000 100000 150000 200000 250000 300000\nIterations\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\nTraining Reward\n2-stage\n3-stage\n4-stage\n5-stage\n(a) Training curve: TD3 with hard mode\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \n2-stage(TD3)\n3-stage(TD3)\n4-stage(TD3)\n5-stage(TD3)\n2-stage(SPGD)\n3-stage(SPGD)\n4-stage(SPGD)\n5-stage(SPGD)\n(b) Testing curve: TD3 with hard mode\nFigure 9: Comparison of the results on hard mode N-stage OPS environment with TD3 algorithms. (a)\nshows the training curve; (b) shows the evaluation of TD3 and SPGD in the testing environment. The\ndashed region shows the area within the standard deviation.\nTo provide a clearer illustration of the impact of stage number N in the OPS environment, we present the\ntraining and testing curves for TD3 and SPGD on the hard mode. Figure 9(a) displays the training curve of\nTD3 on different N-stage OPS systems. It can be observed that as the stage number increases, the training\nconvergence becomes slower. Figure 9(b) showcases the testing curve of SPGD and TD3 on different N-\nstage OPS systems. From the figure, we can draw the following observations: (1) With an increase in stage\nnumber, the final return PN becomes smaller for both TD3 and SPGD, indicating a decrease in performance\nas the system becomes more complex. (2) TD3 consistently outperforms SPGD for stage numbers N ‚â•4.\n10\nThis suggests that RL methods like TD3 are more effective in handling the control challenges posed by OPS\nsystems with a larger number of stages.\n3.5\nTransferring trained policy between different modes\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \nhard->hard\nhard->medium\nhard->easy\n(a) hard trained\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \nmedium->hard\nmedium->medium\nmedium->easy\n(b) medium trained\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \neasy->hard\neasy->medium\neasy->easy\n(c) easy trained\nFigure 10: Demonstration of the transfer performance of the trained policy on (a) hard mode training\nenvironment; (b) medium mode training environment; (c) easy mode training environment. The dashed\nregion shows the area within the standard deviation.\nThe major difference between the simulation and real-world environments is the different noise levels. In\norder to investigate the transferability of trained policies between different noise levels in the OPS envi-\nronment, we conducted a simulated experiment. Our simulation environment incorporates different noise\nlevels depending on the difficulty mode: \"easy\", \"medium\", and \"hard\". We trained policies on the \"hard\"\nmode environment and tested their performance on \"hard\", \"medium\", and \"easy\" mode environments. The\ntransfer results are presented in Fig. 10(a).\nAs can be observed, the trained policy can be successfully\ntransferred to both \"medium\" and \"easy\" mode environments, achieving high performance in terms of pulse\nenergy. Figure 10(b) and Figure 10(c) depict the transfer results of policies trained on the \"medium\" and\n\"easy\" mode environments, respectively. It can be seen that when policies trained on easier environments are\ntransferred to a harder environment, their performance drops. On the contrary, policies trained in harder\nenvironments can be effectively applied to easier environments. Based on these findings, training policies\nin harder simulation environments that introduce more noise and uncertainty can be more useful. This\napproach allows us to explore and develop fast and robust control algorithms that can then be deployed on\nreal-world physical systems.\n4\nDiscussion\n4.1\nReal-world environment and simulation\nDeploying RL algorithms in real-world optics systems poses several challenges, including the need for signal\nconversion, time delays, and manual tuning of optical devices.\nIn our simulation system, we have the\nadvantage of faster control steps and simplified initial alignment. In real-world optics systems, the optical\nsignal needs to be converted to an electrical analog signal using a photo-detector (PD), which is then further\nconverted to a digital signal using an analog-to-digital converter (ADC). Furthermore, the delay line device\nor controller introduces supplementary time overhead to the control step, typically spanning from 0.1 to\n1 second. Nonetheless, within our simulation system, both simulation and control steps can be performed\nin less than 0.01 seconds, as demonstrated in table 3. This facilitates expedited training and evaluation\nprocesses. Furthermore, in real-world OPS systems, manual tuning of the optical devices is required when\nthe optical beams are misaligned, which can be a time-consuming process taking several hours or even days.\nIn contrast, in our simulation system, we can easily reset the environment to achieve the initial alignment,\nsimplifying the setup and reducing the time required for system preparation.\nPrevious endeavors to directly train RL algorithms to real-world OPS systems have encountered obstacles,\nincluding slow training in a real environment, and unstable and non-optimal convergence of RL algorithms.\nConsequently, the sim2real approach, which involves training and evaluating RL algorithms in simulation\nenvironments before deploying them to real-world systems, has garnered considerable interest. Our objective\n11\nis to conduct comprehensive research on RL algorithms within the simulation environment and subsequently\nleverage the sim2real approach to transition these algorithms into real-world applications.\nThe validity of our simulation is partially confirmed in (T√ºnnermann & Shirakawa, 2019; Yang et al., 2020).\nThe experiments detailed in (T√ºnnermann & Shirakawa, 2019) align with the 1-stage OPS environment\ndescribed within our paper. In their study, the authors conducted both simulations and real experiments to\nassess RL algorithms and demonstrated their usefulness in the context of their paper. They also emphasized\nthe value of using simulations for training due to the time and effort required for real-world RL training.\nAlthough our OPS simulation and experimental settings are more intricate than those in (T√ºnnermann &\nShirakawa, 2019), the underlying physics remains consistent, bolstering the reliability and accuracy of our\nsimulation environment.\n4.2\nRL controllers and different simulation modes\nThe experimental results demonstrate that off-policy RL algorithms (TD3 and SAC) outperform traditional\nSPGD controllers in larger N-stage OPS systems, particularly in the challenging hard mode. In the simu-\nlation, the easy mode corresponds to the traditional control approach in which experts manually fine-tune\nthe OPS system to achieve a state close to the global optimum (representing the real-world \"easy\" mode)\nbefore employing SPGD for control. However, the future of optical control lies in automation. The hard\nmode in the simulation reflects a more realistic scenario where direct control of the OPS system is performed\nwithout initial expert tuning. In this context, RL controllers exhibit significant promise for optical systems.\nThis motivates our focus on developing OPS simulation environments, emphasizing the need for fast training\nand noise-robust RL algorithms capable of handling non-stationary noise and non-convex control objectives.\nAdditionally, exploring the nonconvex and periodic nature of OPS objectives holds potential benefits for\nreal-world RL applications, incorporating valuable structural information into the control tasks in optics.\n4.3\nLimitations\nIn our simulation, we model vibration-induced fast noise as Gaussian noise. However, it‚Äôs important to\nacknowledge that vibrations may not consistently conform to a Gaussian distribution. The characteristics\nand features of noise generated by vibrations can vary based on a range of factors. For instance, if vibration\nproperties exhibit non-symmetry or involve non-linear effects, the resulting noise profile might deviate from\nGaussian attributes. Guided by the Central Limit Theorem, we consider Gaussian noise to be a suitable\napproximation for representing fast noise in our simulation. As a result, the principal disparity between\nreal-world experiments and simulations often originates from divergent noise properties.\nIn forthcoming\nresearch, we aim to delve into sim2real transfer and formulate a more realistic gap, taking into account these\nnuanced noise characteristics.\n5\nConclusion\nIn this paper, we present OPS, an open-source simulator for controlling pulse stacking systems using RL\nalgorithms. To the best of our knowledge, this is the first publicly available RL environment specifically\ntailored for optical control problems. We conducted evaluations of SAC, TD3, and PPO within our proposed\nsimulation environment. The experimental results clearly demonstrate that off-policy RL methods outper-\nform traditional SPGD-PID controllers by a substantial margin, especially in challenging environments. By\noffering an optical control simulation environment and providing RL benchmarks, our aim is to encourage\nthe exploration and application of RL techniques in optical control tasks, as well as to facilitate further\nadvancements of RL controllers in the field of natural sciences.\n12\nReferences\nAbulikemu Abuduweili, Bowei Yang, and Zhigang Zhang. Modified stochastic gradient algorithms for con-\ntrolling coherent pulse stacking. In Conference on Lasers and Electro-Optics, pp. STh4P.1, 2020a.\nAbulikemu Abuduweili, Bowei Yang, and Zhigang Zhang. Control of delay lines with reinforcement learning\nfor coherent pulse stacking. In Conference on Lasers and Electro-Optics, pp. JW2F.33, 2020b.\nAbulikemu Abuduweili, Jie Wang, Bowei Yang, Aimin Wang, and Zhigang Zhang. Reinforcement learning\nbased robust control algorithms for coherent pulse stacking. Opt. Express, 29(16):26068‚Äì26081, Aug 2021.\nAlekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient\nmethods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22\n(98):1‚Äì76, 2021.\nIgnas Astrauskas, Edgar Kaksis, Tobias Fl√∂ry, Giedrius Andriukaitis, Audrius Pug≈ælys, Andrius Baltu≈°ka,\nJohn Ruppe, Siyun Chen, Almantas Galvanauskas, and Tadas Balƒçi¬Øunas. High-energy pulse stacking via\nregenerative pulse-burst amplification. Optics letters, 42(11):2201‚Äì2204, 2017.\nMayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet: Learning to drive by imitating the best\nand synthesizing the worst. arXiv preprint arXiv:1812.03079, 2018.\nThomas Baumeister, Steven L Brunton, and J Nathan Kutz. Deep learning and model predictive control for\nself-tuning mode-locked lasers. JOSA B, 35(3):617‚Äì626, 2018.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An\nevaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253‚Äì279, 2013.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech\nZaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\nMarin Bukov, Alexandre GR Day, Dries Sels, Phillip Weinberg, Anatoli Polkovnikov, and Pankaj Mehta.\nReinforcement learning in different phases of quantum control. Physical Review X, 8(3):031086, 2018.\nGert Cauwenberghs. A fast stochastic error-descent algorithm for supervised learning and optimization.\nAdvances in neural information processing systems, 5:244‚Äì251, 1993.\nBo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed: Convergent\nreinforcement learning with nonlinear function approximation. In International Conference on Machine\nLearning, pp. 1125‚Äì1134. PMLR, 2018.\nAmir Dembo and Thomas Kailath. Model-free distributed learning. IEEE Transactions on Neural Networks,\n1(1):58‚Äì70, 1990.\nC Dorrer, DC Kilper, HR Stuart, G Raybon, and MG Raymer. Linear optical sampling. IEEE Photonics\nTechnology Letters, 15(12):1746‚Äì1748, 2003.\nOliver J Dressler, Philip D Howes, Jaebum Choo, and Andrew J deMello. Reinforcement learning for dynamic\nmicrofluidic control. ACS omega, 3(8):10084‚Äì10091, 2018.\nSimon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably efficient q-learning with function\napproximation via distribution shift error checking oracle. arXiv preprint arXiv:1906.06321, 2019.\nYan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement\nlearning for continuous control. In International conference on machine learning, pp. 1329‚Äì1338. PMLR,\n2016.\nGabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning.\narXiv preprint arXiv:1904.12901, 2019.\nMartin E Fermann and Ingmar Hartl. Ultrafast fibre lasers. Nature photonics, 7(11):868‚Äì874, 2013.\n13\nScott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic\nmethods. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on\nMachine Learning, volume 80, pp. 1587‚Äì1596. PMLR, 2018.\nGo√´ry Genty, Lauri Salmela, John M Dudley, Daniel Brunner, Alexey Kokhanovskiy, Sergei Kobtsev, and\nSergei K Turitsyn. Machine learning and applications in ultrafast photonics. Nature Photonics, pp. 1‚Äì11,\n2020.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum\nentropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause (eds.),\nProceedings of the 35th International Conference on Machine Learning, volume 80, pp. 1861‚Äì1870. PMLR,\n10‚Äì15 Jul 2018.\nJohan Hult. A fourth-order runge‚Äìkutta in the interaction picture method for simulating supercontinuum\ngeneration in optical fibers. J. Lightwave Technol., 25(12):3770‚Äì3775, Dec 2007.\nYoanna M Ivanova, Hannah Pallubinsky, Rick Kramer, and Wouter van Marken Lichtenbelt. The influence of\na moderate temperature drift on thermal physiology and perception. Physiology & Behavior, 229:113257,\n2021.\nJun Izawa, Toshiyuki Kondo, and Koji Ito. Biological arm motion through reinforcement learning. Biological\ncybernetics, 91(1):10‚Äì22, 2004.\nMichael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and\nPieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark. arXiv preprint arXiv:2110.15191,\n2021.\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David\nSilver, and Daan Wierstra.\nContinuous control with deep reinforcement learning.\narXiv preprint\narXiv:1509.02971, 2015.\nSobhan Miryoosefi, Kiant√© Brantley, Hal Daum√© III, Miroslav Dud√≠k, and Robert Schapire. Reinforcement\nlearning with convex constraints. arXiv preprint arXiv:1906.09323, 2019.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and\nMartin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal\nKalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.\narXiv preprint arXiv:1609.03499, 2016.\nD Pomerleau. An autonomous land vehicle in a neural network. Advances in Neural Information Processing\nSystems; Morgan Kaufmann Publishers Inc.: Burlington, MA, USA, 1998.\nAntonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dormann.\nStable baselines3. https://github.com/DLR-RM/stable-baselines3, 2019.\nA Ray, J Achiam, and D Amodei. Benchmarking safe exploration in deep reinforcement learning. arxiv.\narXiv preprint arXiv:1910.01708, 7, 2019.\nJohn F Ready. Industrial applications of lasers. Elsevier, 1997.\nBahaa EA Saleh and Malvin Carl Teich. Fundamentals of photonics. john Wiley & sons, 2019.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-\ntion algorithms. arXiv preprint arXiv:1707.06347, 2017.\nYuen-Ron Shen. The principles of nonlinear optics. New York, 1984.\nAnthony E Siegman. Lasers. University science books, 1986.\n14\nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deter-\nministic policy gradient algorithms.\nIn Proceedings of the 31st International Conference on Machine\nLearning, volume 32 of Proceedings of Machine Learning Research, pp. 387‚Äì395. PMLR, 2014.\nURL\nhttp://proceedings.mlr.press/v32/silver14.html.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go\nwith deep neural networks and tree search. Nature, 529(7587):484‚Äì489, 2016.\nHenning Stark, Michael M√ºller, Marco Kienel, Arno Klenke, Jens Limpert, and Andreas T√ºnnermann.\nElectro-optically controlled divided-pulse amplification. Optics express, 25(12):13494‚Äì13503, 2017.\nChang Sun, Eurika Kaiser, Steven L Brunton, and J Nathan Kutz. Deep reinforcement learning for optical\nsystems: A case study of mode-locked lasers. Machine Learning: Science and Technology, 1(4):045013,\n2020.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,\nAbbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al.\nDeepmind control suite.\narXiv preprint\narXiv:1801.00690, 2018.\nHenrik T√ºnnermann and Akira Shirakawa. Delay line coherent pulse stacking. Opt. Lett., 42(23):4829‚Äì4832,\nDec 2017.\nHenrik T√ºnnermann and Akira Shirakawa. Deep reinforcement learning for coherent beam combining ap-\nplications. Opt. Express, 27(17):24223‚Äì24230, Aug 2019.\nCarlo M Valensise, Alessandro Giuseppi, Giulio Cerullo, and Dario Polli. Deep reinforcement learning control\nof white-light continuum generation. Optica, 8(2):239‚Äì242, 2021.\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo,\nAlireza Makhzani, Heinrich K√ºttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new challenge\nfor reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.\nHao-nan Wang, Ning Liu, Yi-yun Zhang, Da-wei Feng, Feng Huang, Dong-sheng Li, and Yi-ming Zhang.\nDeep reinforcement learning: a survey. Frontiers of Information Technology & Electronic Engineering, 21\n(12):1726‚Äì1744, 2020.\nBernard L Welch. The generalization of ‚Äòstudent‚Äôs‚Äôproblem when several different population varlances are\ninvolved. Biometrika, 34(1-2):28‚Äì35, 1947.\nGordon Wetzstein, Aydogan Ozcan, Sylvain Gigan, Shanhui Fan, Dirk Englund, Marin Soljaƒçiƒá, Cornelia\nDenz, David AB Miller, and Demetri Psaltis.\nInference in artificial intelligence with deep optics and\nphotonics. Nature, 588(7836):39‚Äì47, 2020.\nBowei Yang, Guanyu Liu, Abuduweili Abulikemu, Yan Wang, Aimin Wang, and Zhigang Zhang. Coherent\nstacking of 128 pulses from a ghz repetition rate femtosecond yb:fiber laser. In Conference on Lasers and\nElectro-Optics, pp. JW2F.28, 2020.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine.\nMeta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference\non robot learning, pp. 1094‚Äì1100. PMLR, 2020.\nFangyi Zhang, J√ºrgen Leitner, Michael Milford, Ben Upcroft, and Peter Corke. Towards vision-based deep\nreinforcement learning for robotic motion control. arXiv preprint arXiv:1511.03791, 2015.\nPu Zhou, Zejin Liu, Xiaolin Wang, Yanxing Ma, Haotong Ma, Xiaojun Xu, and Shaofeng Guo. Coherent\nbeam combining of fiber amplifiers using stochastic parallel gradient descent algorithm and its application.\nIEEE Journal of Selected Topics in Quantum Electronics, 15(2):248‚Äì256, 2009.\n15\nA\nRelated Works\nA.1\nOptical Pulse Stacking\nHigh-energy lasers find extensive application in scientific research, medical procedures, and various industries\n(Fermann & Hartl, 2013). Nonetheless, the pulse energy of individual lasers often falls short for applications\nsuch as large-scale industrial material processing (Ready, 1997).\nThis limitation arises due to the non-\nlinearities and losses inherent in laser resonators (Siegman, 1986). Overcoming this challenge necessitates\ninnovative approaches to achieve higher energies without being constrained by the limitations of a single\nlaser pulse. Optical (coherent) pulse stacking has emerged as a solution to this predicament (T√ºnnermann\n& Shirakawa, 2017), offering a promising avenue for scaling pulse energy. The nonlinearity and periodic-\nity inherent to light interference, underlie not only OPS but also other optical control challenges. These\ntechniques are integral to precision measurements, industrial manufacturing, and scientific investigations.\nOur simulation, which mirrors these optical control complexities, represents a significant and representative\nenvironment for optical control experimentation.\nFigure 11: Real-world optical pulse stacking system.\nA controller adjusts time delay values to achieve\nmaximum pulse energy.\nThe actual OPS system is depicted in Fig. 11. A controller is responsible for determining the value of each\ntime delay œÑ by measuring the final stacked pulse using a photodetector. Subsequently, these time delay\nvalues are transmitted to each delay line device in order to adjust the positions of the pulses. It should be\nnoted that in Fig. 11, the controller is connected to the electric signal line of the 1st, 2nd, and 3rd delay\nline devices located at the bottom. Conducting real-world OPS control experiments can be both costly and\ntime-consuming due to the complexities involved in the process. To visually illustrate the stacking procedure\nfor combining two pulses, please refer to Supplemental Video 1 2.\nA.2\nReinforcement Learning Benchmarks\nIn tandem with the remarkable achievements of Deep Reinforcement Learning (RL), a plethora of Rein-\nforcement Learning Benchmarks have been explored. These include well-known platforms like the Atari\nArcade Learning Environment (Bellemare et al., 2013), the OpenAI gym (Brockman et al., 2016), and the\nDeepMind Control (DMC) Suite (Tassa et al., 2018). Beyond these general benchmarks, the advancement\nof Reinforcement Learning across diverse applications has spurred the development of specialized bench-\nmarks. Noteworthy examples include Rllab (Duan et al., 2016), tailored for continuous control challenges,\nand Meta-World (Yu et al., 2020), serving as a benchmark for Meta Reinforcement Learning. SafetyGym\n(Ray et al., 2019) evaluates the ability of RL agents to accomplish tasks while adhering to safety constraints,\nwhile URLB (Laskin et al., 2021) assesses the performance of Unsupervised Reinforcement Learning. While\nthese existing benchmarks aptly cater to control problems in the fields of Robotics and Machine Learning,\na notable gap exists in the availability of a benchmark and repository of user-friendly baseline algorithms\nfor scientific control problems or optical control tasks. This significant gap is our primary motivation for\npropelling progress in the realm of RL benchmarks for optical control problems through OPS.\n2https://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking/blob/main/demo/Video1.gif\n16\nB\nAdditional details of Experiments\nB.1\nRL algorithms\nReinforcement Learning (RL) is a field of machine learning focused on how intelligent agents should make\ndecisions in an environment to maximize cumulative rewards (Sutton & Barto, 2018). In our OPS envi-\nronment, RL algorithms learn a policy that aims to maximize the output pulse power by controlling time\ndelay, as detailed in section 2.3. For this study, we conducted a thorough evaluation of three RL algorithms:\nProximal Policy Optimization (PPO), Twin Delayed Deep Deterministic Policy Gradients (TD3), and Soft\nActor-Critic (SAC). We briefly illustrate each algorithm as follows.\n‚Ä¢ Proximal Policy Optimization (PPO) is a popular policy gradient algorithm used in reinforcement\nlearning (Schulman et al., 2017). PPO belongs to the class of on-policy algorithms, meaning it optimizes\nthe policy by collecting data through interactions with the environment in real-time. The key idea behind\nPPO is to ensure that the policy updates are not too large, preventing significant policy changes that\nmight lead to instability or catastrophic forgetting. Specifically, PPO uses a clipped surrogate objective\nfunction to update the policy. The objective aims to maximize the expected reward while also introducing\na constraint that prevents the policy update from deviating too far from the current policy.\n‚Ä¢ Soft Actor-Critic (SAC) is one of the commonly used deep reinforcement learning algorithms intro-\nduced by (Haarnoja et al., 2018). SAC is an off-policy algorithm, which means it can learn from past\nexperiences (replay buffer) and does not rely on the most recent data like on-policy algorithms. Off-policy\nlearning enables SAC to efficiently use data and improve sample efficiency. One of the key features of SAC\nis the emphasis on entropy maximization. The policy is not only optimized to maximize the expected\nreward but also to maximize the entropy of the policy. By maximizing the entropy, SAC encourages\nexploration and avoids premature convergence to suboptimal policies. Another key feature is the soft\nvalue function. SAC uses soft value functions instead of the typical Q-functions used in other algorithms.\nThese soft Q-functions estimate the expected return, but they are \"softened\" by adding an entropy term.\n‚Ä¢ Twin Delayed Deep Deterministic policy gradient (TD3) is a powerful deep reinforcement learning\nalgorithm designed for solving tasks with continuous action spaces (Fujimoto et al., 2018). Similar to\nSAC, TD3 is an off-policy algorithm, which means it could efficiently use data and improve sample\nefficiency. TD3 follows the actor-critic framework. It consists of two neural networks: an actor-network\nand two critic networks. The actor-network learns the policy, mapping states to continuous actions,\nwhile the critic networks estimate the Q-values (expected return) for different state-action pairs. The\nactor network learns a deterministic policy, allowing for easier optimization in continuous action spaces.\nTD3 employs two critic networks to reduce overestimation biases commonly encountered in Q-learning\nalgorithms.\nFor further information on RL algorithms, readers can refer to the comprehensive survey paper by (Wang\net al., 2020).\nB.2\nExperimental setting\nTo optimize the performance of RL algorithms, we conducted a grid search to identify optimal hyperparam-\neters. Each set of hyperparameters was thoroughly evaluated using 5 different random seeds. The search\nprocess involved training on the 5-stage OPS environment with medium difficulty. Detailed information re-\ngarding the hyperparameter ranges and the selected values for TD3, SAC, and PPO can be found in tables 4\nto 6. While we didn‚Äôt explicitly mention the specific hyperparameter values except in tables 4 to 6, we\nmaintained the default settings provided by the stable baseline3 library.\nModel size stands as a pivotal hyperparameter, bearing significance not only on evaluation performance but\nalso exerting influence on training duration. Table 7 presents a juxtaposition of diverse model sizes for the\npolicy network within the TD3 algorithm and the 5-stage OPS environment. In this context, [64,64] signifies\na fully connected network boasting two layers, each with a hidden dimension of 64. Conversely, [256,256]\nrepresents a fully connected network encompassing three layers, with a hidden dimension of 256. Evidently,\nthe network architecture of [256,256] emerges as the most proficient, excelling both in the assessment of\n17\nTable 4: TD3: ranges used during the hyperparameter search and the final selected values.\nHyperparameter\nRange\nBest-selected\nSize of the replay buffer\n{1000,10000,100000}\n10000\nStep of collect transition before training\n{100, 1000, 10000}\n1000\nUnroll Length/n-step\n{1,10, 100}\n100\nTraining epochs per update\n{1,10, 100}\n100\nDiscount factor (Œ≥)\n{0.98, 0.99, 0.999}\n0.98\nNoise type\n{‚Äônormal‚Äô, ‚Äôornstein-uhlenbeck‚Äô, None}\n‚Äônormal‚Äô\nNoise standard value\n{0.1, 0.3, 0.5, 0.7, 0.9}\n0.7\nLearning rate\n{0.0001, 0.0003, 0.001,0.003,0.01}\n0.0003\nPolicy network hidden layer\n{2, 3}\n2\nPolicy network hidden dimension\n{64, 128, 256}\n256\nOptimizer\nAdam\nAdam\nTable 5: SAC: ranges used during the hyperparameter search and the final selected values.\nHyperparameter\nRange\nBest-selected\nSize of the replay buffer\n{1000,10000,100000}\n10000\nStep of collect transition before training\n{100, 1000, 10000}\n1000\nUnroll Length/n-step\n{1,10, 100}\n1\nTraining epochs per update\n{1,10, 100}\n1\nDiscount factor (Œ≥)\n{0.98, 0.99, 0.999}\n0.99\nGeneralized State Dependent Exploration (gSDE)\n{True, False}\nTrue\nSoft update coefficient for \"Polyak update\" (œÑ)\n{0.002,0.005, 0.01, 0.02}\n0.002\nLearning rate\n{0.0001, 0.0003, 0.001,0.003,0.01}\n0.0003\nPolicy network hidden layer\n{ 2, 3}\n2\nPolicy network hidden dimension\n{64, 128, 256}\n256\nOptimizer\nAdam\nAdam\nTable 6: PPO: ranges used during the hyperparameter search and the final selected values.\nHyperparameter\nRange\nBest-selected\nUnroll Length/n-step\n{128,256,512,1024,2048}\n512\nTraining epochs per update\n{1,5,10}\n10\nClipping range\n{0.1,0.2,0.4}\n0.2\nDiscount factor (Œ≥)\n{0.98, 0.99, 0.999}\n0.99\nEntropy Coefficient\n{0, 0.001, 0.01, 0.1}\n0.001\nGAE (Œª)\n{0.90, 0.95, 0.98, 0.99}\n0.95\nValue function coefficient\n{0.1,0.3,0.5,0.7,0.9}\n0.5\nLearning rate\n{0.0001, 0.0003, 0.001,0.003,0.01}\n0.0003\nGradient norm clipping\n{0.1, 0.5, 1.0, 5.0}\n0.5\nPolicy network hidden layer\n{2, 3}\n2\nPolicy network hidden dimension\n{64, 128, 256}\n256\nOptimizer\nAdam\nAdam\n18\nstacked pulse energy (Evaluation performance) and time efficiency (Training Time Usage). Consequently,\nwe opt for the [256,256] architecture as our policy network configuration.\nTable 7: Comparative evaluation of various models within the TD3 algorithm and 5-stage OPS environment,\nwith emphasis on performance, memory usage, and time costs.\nModel size\nEvaluation performance\nMemory Usage\nTraining Time Usage\n[64, 64]\n0.8931 ¬± 0.0873\n933 MB\n‚âà83 min\n[256, 256]\n0.9285¬±0.0437\n989 MB\n‚âà90 min\n[256, 256, 256]\n0.9243 ¬±0.0399\n1030 MB\n‚âà117 min\nB.3\nResults on controlling 4-stage and 6-stage OPS environments\nWe present the training curves (training reward vs. iterations) and testing curves (final pulse energy PN\nvs. testing iterations) for the 4-stage OPS environment in Fig. 12 and for the 6-stage OPS environment\nin Fig. 13. From the figures, it is evident that TD3 and SAC outperform PPO in terms of performance.\nMoreover, comparing Fig. 12 (4-stage) to Fig. 13 (6-stage), it can be observed that as the stage number\nincreases, the training convergence becomes slower and the final return PN becomes smaller, particularly in\nthe medium and hard difficulty modes.\n0\n50000 100000 150000 200000 250000 300000\nIterations\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\nTraining Reward\nTD3\nSAC\nPPO\n(a) Easy mode: train curve\n0\n50000 100000 150000 200000 250000 300000\nIterations\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\nTraining Reward\nTD3\nSAC\nPPO\n(b) Medium mode: train curve\n0\n50000 100000 150000 200000 250000 300000\nIterations\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\nTraining Reward\nTD3\nSAC\nPPO\n(c) Hard mode: train curve\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \nTD3\nSAC\nPPO\n(d) Easy mode: testing curve\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \nTD3\nSAC\nPPO\n(e) Medium mode: testing curve\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \nTD3\nSAC\nPPO\n(f) Hard mode: testing curve\nFigure 12: 4-stage OPS experiments. Training reward was plotted for (a) easy mode, (b) medium mode,\nand (c) hard mode. Evaluation of the stacked pulse power P4 (normalized) of the testing environment was\nplotted for (d) easy mode, (e) medium mode, and (f) hard mode.\nB.4\nRendering the controlling results on OPS environment\nFigure 14 depicts the pulse trains on a 5-stage hard mode OPS system controlled by TD3, starting from a\nrandom initial state. It is evident from the figure that the TD3 algorithm is capable of attaining a maximum\npower within 40 iterations. For a more comprehensive visualization, please refer to supplemental video 2 3.\n3https://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking/blob/main/demo/Video2.gif\n19\n0\n50000 100000 150000 200000 250000 300000\nIterations\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\nTraining Reward\nTD3\nSAC\nPPO\n(a) Easy mode: train curve\n0\n50000 100000 150000 200000 250000 300000\nIterations\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\nTraining Reward\nTD3\nSAC\nPPO\n(b) Medium mode: train curve\n0\n50000 100000 150000 200000 250000 300000\nIterations\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\nTraining Reward\nTD3\nSAC\nPPO\n(c) Hard mode: train curve\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \nTD3\nSAC\nPPO\n(d) Easy mode: testing curve\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \nTD3\nSAC\nPPO\n(e) Medium mode: testing curve\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \nTD3\nSAC\nPPO\n(f) Hard mode: testing curve\nFigure 13: 6-stage OPS experiments. Training reward was plotted for (a) easy mode, (b) medium mode,\nand (c) hard mode. Evaluation of the stacked pulse power P6 (normalized) of the testing environment was\nplotted for (d) easy mode, (e) medium mode, and (f) hard mode.\n300\n200\n100\n0\n100\n200\n300\nTime\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIntensity (normalized)\niteration=0\n(a) Initial state\n300\n200\n100\n0\n100\n200\n300\nTime\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIntensity (normalized)\niteration=10\n(b) After 10 iterations\n300\n200\n100\n0\n100\n200\n300\nTime\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIntensity (normalized)\niteration=20\n(c) After 20 iterations\n300\n200\n100\n0\n100\n200\n300\nTime\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIntensity (normalized)\niteration=30\n(d) After 30 iterations\n300\n200\n100\n0\n100\n200\n300\nTime\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIntensity (normalized)\niteration=40\n(e) After 40 iterations\n300\n200\n100\n0\n100\n200\n300\nTime\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIntensity (normalized)\niteration=50\n(f) After 50 iterations\nFigure 14: Demonstration of the controlling 5-stage OPS hard mode testing environment by TD3 algorithm\nafter training. (a): initial state of pulses; (b) pulse state after 10 control iterations; (c) pulse state after 20\ncontrol iterations; (d) pulse state after 30 control iterations; (e) pulse state after 40 control iterations; (e)\npulse state after 50 control iterations.\n20\nB.5\nDiscussion about experimental results\nIn our experiments, we observed a significant performance advantage of SAC and TD3 over PPO. The\nprimary distinction between SAC, TD3, and PPO lies in their underlying approach - SAC and TD3 are off-\npolicy methods, while PPO is an on-policy method. The fundamental characteristic of on-policy algorithms\nis that both the data used for learning and policy improvement originate from the same policy being updated.\nThis implies that the agent relies heavily on recent experiences to adjust its policy (Sutton & Barto, 2018).\nIn the context of our OPS environment, where numerous local optimal points exist (as depicted in Figure\n2), PPO can become trapped in local optima due to its limited exploration and tendency to focus on recent\ndata, limiting its ability to discover globally optimal policies.\nConversely, SAC and TD3 can leverage\nhistorical experiences (e.g., from a replay buffer of size 10000) and are not constrained to relying solely\non the most recent data.\nThis typically results in more efficient data utilization and potentially better\nconvergence towards optimal solutions compared to on-policy methods. Our observations align with findings\nfrom a benchmark study (Yu et al., 2020). As demonstrated in Figures 6 and 8 of (Yu et al., 2020), SAC\nsuccessfully solves all tasks presented in the paper, while PPO manages to solve most tasks but falls short in\ncertain cases. We acknowledge that PPO might exhibit better performance in specific scenarios. However,\nconsidering the results presented in both our paper and (Yu et al., 2020), it appears that SAC may serve as\na more promising starting point for addressing control problems using RL algorithms.\nB.6\nFeedback delay\nIn control systems, the presence of feedback loops invariably introduces delays due to finite sensing speeds,\nsignal processing, computation of control inputs, and actuation. Specifically, let œÑreal(t) denote the time\ndelay (or state) of a real OPS system at time step t. Consider that activities such as photo-detection, data\nacquisition, analog-to-digital conversion, and action computation require a time interval Œ¥. Consequently, a\nfeedback delay of Œ¥ emerges within our OPS control system. When we execute action at = at(œÑreal(t)), the\nsystem‚Äôs time delay (state) might deviate from œÑreal(t) to œÑreal(t) + D(Œ¥) due to the feedback delay. Here,\nD(Œ¥) accounts for an additive time-delay value attributed to the feedback delay D(¬∑). The the next step‚Äôs\ntime-delay can be expressed as:\nœÑreal(t + 1) = œÑreal(t) + at + D(Œ¥).\n(6)\nIn our simulation, we replicate the feedback delay (or hardware-dependent delay) by injecting free-running\nnoise, including elements like slow noise and fast noise, into the delay lines. This process is elucidated in (4),\nwhich bears a resemblance to the representation in (6). This noise effectively emulates the sequence of events\nseen in real-world experiments, encompassing photo-detection, data preprocessing, and the computation of\nactions.\nThe proposed OPS environment offers versatile noise levels, allowing for the configuration of distinct feedback\ndelays and associated system free-running noise. In this context, we present an exploration involving the\ntraining of RL algorithms within a low feedback delay system, followed by the transfer of the learned policy\nto a system characterized by a higher feedback delay. The training noise within the 5-stage hard-mode OPS\nsystem aligns with the feedback delay Œ¥ and noise et = D(Œ¥) , exemplified by Œ¥ = 10 ms. Subsequently, the\npolicy trained under these conditions is evaluated within systems featuring elevated feedback levels kŒ¥ and\nnoise enew = D(k ¬∑ Œ¥), where k takes on values such as 1, 20, 40, 60, 80, and 100. As depicted in Fig. 15, it\nis evident that, regardless of the specific feedback delay, the trained policy consistently attains convergence\nwithin 70 steps, thereby ensuring a cumulative pulse power exceeding 0.7 (a.u.). However, it is noteworthy\nthat an increase in feedback delay corresponds to a notable decline in performance, particularly concerning\npulse energy.\nC\nPotential Impact and future work\nOur simulation environment offers significant benefits in tackling challenging and realistic reinforcement\nlearning problems. Real-world reinforcement learning problems are often highly challenging due to factors\nsuch as high-dimensionality of control, noisy behaviors, and distribution shift (Dulac-Arnold et al., 2019;\nAgarwal et al., 2021; Du et al., 2019). By selecting a large N-stage number with the hard mode in our\n21\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIterations\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFinal Stacked Pulse Energy \ndelay:1\ndelay:20\ndelay:40\ndelay:60\ndelay:80\ndelay:100\nFigure 15: Subsequent to the training of a TD3 policy within a low feedback delay OPS system characterized\nby Œ¥, we then evaluate the policy‚Äôs performance within systems characterized by higher feedback delays k ¬∑Œ¥.\nsimulation environment, we can create high-dimensional and difficult control scenarios. The hard mode of the\nOPS environment exhibits a distinct noise distribution in the testing environment compared to the training\nenvironment, which mirrors the challenges encountered in real-world reinforcement learning problems.\nIn our simulation, we have access to the objective function of the OPS (ignoring noise), which provides\nvaluable structural information and physical constraints. This enables us to explore additional information\nabout the OPS function and incorporate it into RL algorithms. Rather than focusing on generic noncon-\nvex problems, many real-world scenarios involve specific nonconvex control problems with known objective\nfunctions or physical constraints (Miryoosefi et al., 2019). Exploring the nonconvex and periodic nature of\nthe OPS objective can greatly benefit real-world RL problems that encompass structural information.\nSimilar to our OPS control system, optical control problems in general are influenced by the nonlinearity and\nperiodicity of light interactions. This includes applications such as coherent optical interference (Wetzstein\net al., 2020) and linear optical sampling (Dorrer et al., 2003), which find utility in precise measurement,\nindustrial manufacturing, and scientific research. We consider our simulation environment to be an important\nand representative optical control environment.\nFurthermore, RL methods have the potential to drive\nadvancements in optical laser technologies and the next generation of scientific control technologies (Genty\net al., 2020).\n22\n",
  "categories": [
    "cs.LG",
    "cs.RO",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2022-03-23",
  "updated": "2023-10-01"
}