{
  "id": "http://arxiv.org/abs/2203.13344v1",
  "title": "Linking Emergent and Natural Languages via Corpus Transfer",
  "authors": [
    "Shunyu Yao",
    "Mo Yu",
    "Yang Zhang",
    "Karthik R Narasimhan",
    "Joshua B. Tenenbaum",
    "Chuang Gan"
  ],
  "abstract": "The study of language emergence aims to understand how human languages are\nshaped by perceptual grounding and communicative intent. Computational\napproaches to emergent communication (EC) predominantly consider referential\ngames in limited domains and analyze the learned protocol within the game\nframework. As a result, it remains unclear how the emergent languages from\nthese settings connect to natural languages or provide benefits in real-world\nlanguage processing tasks, where statistical models trained on large text\ncorpora dominate. In this work, we propose a novel way to establish such a link\nby corpus transfer, i.e. pretraining on a corpus of emergent language for\ndownstream natural language tasks, which is in contrast to prior work that\ndirectly transfers speaker and listener parameters. Our approach showcases\nnon-trivial transfer benefits for two different tasks -- language modeling and\nimage captioning. For example, in a low-resource setup (modeling 2 million\nnatural language tokens), pre-training on an emergent language corpus with just\n2 million tokens reduces model perplexity by $24.6\\%$ on average across ten\nnatural languages. We also introduce a novel metric to predict the\ntransferability of an emergent language by translating emergent messages to\nnatural language captions grounded on the same images. We find that our\ntranslation-based metric highly correlates with the downstream performance on\nmodeling natural languages (for instance $\\rho=0.83$ on Hebrew), while\ntopographic similarity, a popular metric in previous work, shows surprisingly\nlow correlation ($\\rho=0.003$), hinting that simple properties like attribute\ndisentanglement from synthetic domains might not capture the full complexities\nof natural language. Our findings also indicate potential benefits of moving\nlanguage emergence forward with natural language resources and models.",
  "text": "Published as a conference paper at ICLR 2022\nLINKING EMERGENT AND NATURAL LANGUAGES VIA\nCORPUS TRANSFER\nShunyu Yao\nPrinceton University\nMo Yu\nWeChat AI\nYang Zhang\nMIT-IBM Watson AI Lab\nKarthik Narasimhan\nPrinceton University\nJoshua B. Tenenbaum\nMIT\nChuang Gan\nMIT-IBM Watson AI Lab\nABSTRACT\nThe study of language emergence aims to understand how human languages are\nshaped by perceptual grounding and communicative intent. Computational ap-\nproaches to emergent communication (EC) predominantly consider referential\ngames in limited domains and analyze the learned protocol within the game frame-\nwork. As a result, it remains unclear how the emergent languages1 from these\nsettings connect to natural languages or provide beneﬁts in real-world language\nprocessing tasks, where statistical models trained on large text corpora dominate.\nIn this work, we propose a novel way to establish such a link by corpus transfer,\ni.e. pretraining on a corpus of emergent language for downstream natural language\ntasks, which is in contrast to prior work that directly transfers speaker and listener\nparameters. Our approach showcases non-trivial transfer beneﬁts for two different\ntasks – language modeling and image captioning. For example, in a low-resource\nsetup (modeling 2 million natural language tokens), pre-training on an emergent\nlanguage corpus with just 2 million tokens reduces model perplexity by 24.6%\non average across ten natural languages. We also introduce a novel metric to\npredict the transferability of an emergent language by translating emergent mes-\nsages to natural language captions grounded on the same images. We ﬁnd that our\ntranslation-based metric highly correlates with the downstream performance on\nmodeling natural languages (for instance ρ = 0.83 on Hebrew), while topographic\nsimilarity, a popular metric in previous work, shows surprisingly low correlation\n(ρ = 0.003), hinting that simple properties like attribute disentanglement from\nsynthetic domains might not capture the full complexities of natural language. Our\nﬁndings also indicate potential beneﬁts of moving language emergence forward\nwith natural language resources and models2.\n1\nINTRODUCTION\nThe recent remarkable progress in NLP (Devlin et al., 2018; Radford et al., 2019; Brown et al.,\n2020) beneﬁts from huge text corpora, on which powerful models learn rich statistical patterns tabula\nrasa and adapt to downstream tasks. But in stark contrast, human language acquisition (De Villiers\net al., 1978) does not start with passive and complex corpora like Wikipedia. Instead, children\nlearn language through a cumulative series of interactions with other people (Bruner, 1985; Barton,\n1994) grounded in the physical and social world (Harnad, 1990; Vogt, 2002; Alomari et al., 2017).\nIncorporating such cognitive insights with recent machine learning advances holds great promises\ntowards more functional (Wittgenstein, 1958; Wagner et al., 2003) and generalizable (Lake et al.,\n2017) language agents.\nThe study of emergent communication (EC) (Cangelosi & Parisi, 2002; Kirby, 2002; Wagner et al.,\n2003; Lazaridou & Baroni, 2020) is one promising direction toward this motivation, where a commu-\n1In this work “emergent language” means generated messages from an EC game speaker. The term also\nrefers to children’s language development in psychology (Bohlmann & Downer, 2016; Zajicek-Farber, 2010).\n2Code at https://github.com/ysymyth/ec-nl and correspondence to shunyuy@princeton.edu. Work partly\ndone during the ﬁrst author’s remote internship at MIT-IBM Watson AI Lab.\n1\narXiv:2203.13344v1  [cs.CL]  24 Mar 2022\nPublished as a conference paper at ICLR 2022\nLooking \ndown on \nfountains\nA view of \nskyline\nA group of \nstudents \nfrom plant\n13 28 \n39 555 \n2 …\n(d) Emergent -> Natural \nTranslation\n49 28 39 555 2 …\n(b) Language Modeling\nCaptioned \nImages\nEmergent \nMessages\n(c) Image Captioning\n✓\n(a) Emergent \nCommunication Game\n…\nListener\n13 28 22 99\n122\nPre-train\nLM\nThese people are\nFine-tune\nfrom\nLM\nPre-train\n13 34 33 4 ..\nIC\nFine-tune\nA view of skyline \nIC\n13 87 \n213 3 \n22 …\n49 28 \n39 2 \n843 …\nSpeaker\nEmergent \nMessage\nSpeaker\nFigure 1: Our framework. (a) We train a referential game on natural images, and use the trained\nspeaker to generate a corpus of emergent messages (bold box) to pre-train for (b) language\nmodeling and (c) image captioning. We also propose to (d) translate emergent language (yellow) into\ncorresponding natural language (blue) with same perceptual grounding to evaluate their closeness.\nnication protocol is shaped through multi-agent interactions with perceptual grounding. A typical\nsetup is the image referential game (Figure 1(a)), where a speaker generates a discrete sequence of\ntokens based on an input image, a listener is challenged to select the input out of distractors based\non the message, and both networks are optimized jointly via game success signals. By studying\nthese games, researchers are interested in the emergence of desirable properties resembling natural\nlanguage, such as game success generalization (Kharitonov & Baroni, 2020; Lazaridou & Baroni,\n2020) and compositionality (Smith et al., 2003; Andreas & Klein, 2017; Kirby et al., 2015; Lazaridou\net al., 2018; Li et al., 2020b). However, these properties are mostly deﬁned and analyzed within each\nindividual game framework. For example, generalization is usually measured as game success with\nnovel inputs, but it remains unknown how the emergent coding can be useful beyond the particular\ngame (e.g. useful for natural language tasks). Moreover, compositionality is often approximated by\nattribute-wise disentanglement in simple environments, which could be drastically different from\nthe structural properties of natural language (Dankers et al., 2021). Such gaps pose difﬁculty in\nobjectively understanding the emergent properties with respect to the full complexities of natural\nlanguage, and assessing the universality of various claims.\nIn this work, we aim to address these issues by linking emergent languages with natural languages.\nConcretely, we investigate if modeling an emergent language provides transferable beneﬁts for down-\nstream natural language tasks — language modeling (Figure 1(b)), image captioning (Figure 1(c)) —\nand if downstream performances in turn allow us to understand and analyze properties of emergent\nlanguage beyond the EC game and toward natural language. The key technique is corpus transfer,\ni.e. by pre-training a model on a corpus of emergent messages produced by a trained emergent speaker,\nand ﬁne-tuning the model on downstream tasks with natural language data. This approach integrates\ninspirations from prior work like Papadimitriou & Jurafsky (2020), which proposes the transfer\nscheme from synthetic to natural corpora, and Li et al. (2020b), which aims to improve few-shot\ntranslation by transferring the trained EC speaker and listener model weights. Through a series of\nexperiments, we ﬁnd that corpus transfer is helpful when the downstream natural language resource\nis limited. For example, in a low-resource setup of modeling two million natural language tokens,\nsuch a transfer scheme reduces the test perplexity by 24.6% on average versus training from scratch,\nacross ten different downstream languages. We also establish the non-triviality of such a transfer\nperformance by comparing to other synthetic and natural source corpora, as well as multiple ablation\nstudies on the EC and downstream transfer setups to help understand the transferability of emergent\nlanguage. Notably, our method of corpus transfer signiﬁcantly outperforms directly transferring\nthe trained emergent speaker model (Li et al., 2020b), demonstrating that modeling the emergent\nlanguage could yield greater usefulness than directly transferring the EC agents.\nWhile our transfer experiments demonstrate that emergent languages can be leveraged and under-\nstood outside the game and towards real-world tasks, the pre-training and ﬁne-tuning pipeline is\ncomputationally expensive as a scalable evaluation scheme for a population of emergent languages.\nInstead, we are interested in developing a simpler metric to predict the transferability of emergent\nlanguage, which pertains to properties of natural language and may challenge metrics deﬁned within\n2\nPublished as a conference paper at ICLR 2022\nthe game framework (e.g. game accuracy). Therefore, we propose a novel metric for emergent\nlanguages based on how easily it can be translated to the corresponding natural language grounded\non the same perceptual input (Figure 1(d)). Concretely, we employ the trained speaker to produce\nemergent messages based on a small set of unseen images with natural language captions, and train\na translation model on paired emergent and natural captions. We show that the translation score\n(such as ROUGE L (Lin, 2004)) can better predict (e.g. Pearson correlation ρ = 0.83 for modeling\nHebrew) the transfer beneﬁts of different emergent languages than two commonly adopted metrics in\nprior literature – game success with novel inputs (ρ = 0.74), and topographic similarity (Brighton\n& Kirby, 2006; Lazaridou et al., 2018) (ρ = 0.003). These results call for a re-thinking of what\nproperties we should evaluate for an emergent language corpus, and how we should evaluate them.\nFor example, game accuracy may be confounded by the listener performance, and topographic\nsimilarity, a metric of attribute disentanglement, can be ﬂawed for measuring the compositional\nstructure of real language (Kirby, 2001; Goldberg, 2015; Steinert-Threlkeld, 2020).\nIn summary, our work takes a step toward bridging the gap between emergent and natural languages.\nWe believe that shifting from synthetic setups and in-game evaluations to leveraging downstream\nnatural language tasks and state-of-the-art language models could potentially lead to more general\ninsights and greater practical impact for EC and NLP research.\n2\nRELATED WORK\nEmergent Communication and Evaluation\nA large body of prior work qualitatively analyzes the\nevolved message structures when the EC games are set up with synthetic environments (Mordatch &\nAbbeel, 2018; Lazaridou et al., 2018) or even with real images (Havrylov & Titov, 2017). Alternative\nevaluations and examinations include ease of teaching (Li & Bowling, 2019) and symbol usage\npurity (Lazaridou et al., 2016). While these schemes align the emergent language with game success\nor different aspects of game input, our work evaluates the emergent language outside the game\nframework, instead via the corpus transfer performance to natural language tasks and the translation\nperformance to natural language.\nLinking Synthetic and Natural Language\nPapadimitriou & Jurafsky (2020) ﬁnd that recurrent\nnetworks pretrained on non-linguistic corpora (e.g. music, code, regular languages) facilitate natural\nlanguage modeling. In an opposite direction, Lu et al. (2021) observe that language pre-training\nimproves Transformer’s performance with several non-language tasks (logical, vision, protein) even\nwhen most weights are frozen, indicating that learning language structure could yield generally useful\ncomputational subroutines. Andreas et al. (2017) propose to translate a differentiable communication\nchannel in deep multi-agent policies to natural language by collecting human-human communication\nin the same games. However, their approach focused on interpreting a continuous channel, while we\nfocus on evaluating emergent messages with discrete structures.\nUse of Emergent Communication beyond games\nThere have been efforts to incorporate commu-\nnication signals and natural language supervision to better communicate (Lowe et al., 2020), avoid\nlanguage drift (Lazaridou et al., 2020), or to improve NLP tasks like vision-language navigation (Fried\net al., 2018), translation (Lee et al., 2018), and image captioning (Havrylov & Titov, 2017). Different\nfrom these setups where the speaker is also grounded on natural language annotations, Li et al.\n(2020b) propose to separate the EC game, where no natural language is involved, and ﬁne-tuning on\na downstream translation task with limited natural language data. Despite being the closest work to\nours, such a model transfer approach is fundamentally different from our corpus transfer in several\nways. First, while Li et al. (2020b) mainly aim to improve downstream tasks, we envision transfer as\na novel means to evaluating and analyzing emergent languages. Also, the emergent language is a\nrepresentation with properties beyond the parameters of speciﬁc EC models (Section 3.1). Last but\nnot least, corpus transfer has several practical advantages over model transfer (Section 4.3).\n3\nBACKGROUND\n3.1\nEMERGENT COMMUNICATION (EC) GAME\nAs shown in Figure 1(a), we consider a typical speaker-listener referential game (Lazaridou et al.,\n2016; 2018; Li & Bowling, 2019) on a set of N image features DI = {I1, · · · , IN}. At each training\n3\nPublished as a conference paper at ICLR 2022\nstep, the speaker takes an input image feature Ii and generates a discrete message Mi ∈[V ]T using\nthe Gumbel-Softmax trick (Jang et al., 2016), where V is the vocabulary size and T is the sequence\nlength limit. For simplicity denote m = Mi so that mt = Mi,t denotes the t-th token of the message\nMi. Then\nhs0 = Ii,\nhst = GRUspk (mt−1, hst−1) (t > 0),\nm0 = [CLS],\nmt = Gumbel-Softmax (MLPspk(hst)) (t > 0).\n(1)\nHere hst denotes speaker hidden states. Next, the listener takes the message m, and tries to guess the\nright image Ii out of a set of K confounding images Ci = {Ij1, · · · , IjK} ⊂DI −{Ii}. To do so, it\nuses another GRU layer to turn the message m into a hidden vector hlT\nhl0 = 0,\nhlt = GRUlsn (mt, hlt−1) (t > 0).\n(2)\nBased on hlT , the listener assigns a score for each candidate image based on inverse square error (Lee\net al., 2018), then selects the image by Softmax sampling across the scores. The speaker and listener\nare jointly optimized by minimizing the cross-entropy loss of image selection:\nscore(I) = ||hlT −MLPlsn(I)||−2\n2 ,\n(3)\np(guess = I) = softmax(score(I))\n(I ∈{Ii} ∪Ci),\n(4)\nLEC = −EIi,CiEMi log p(guess = Ii).\n(5)\nAfter the game is trained, the speaker can be employed to sample a corpus of emergent language\nDM = {M1, · · · , MN} based on input images. Though the emergent language is generated by a\nparticular EC speaker, the representation could be potentially learned and used by new agents, with\nmore general properties beyond the parameters of a particular speaker architecture. Thus, our work\nfocuses on evaluating and analyzing the properties of the emergent language (e.g. corpus transfer)\nrather than the emergent communication models (e.g. model transfer).\n3.2\nEVALUATING EMERGENT LANGUAGE\nA central question in emergent communication is how to evaluate the quality of the emergent corpus\nDM. Two commonly used metrics in prior work are game accuracy and topographic similarity.\nGame Accuracy with Novel Objects\nThe most straightforward metric is the accuracy of playing\nthe referential game EIi,Ci,Mi [1guess=Ii]. Prior work (Cogswell et al., 2019; Li & Bowling, 2019)\nusually considers how well the speaker and listener generalize to novel inputs, and ﬁnds that such a\ngeneralization ability might not correlate with other properties such as compositionality (Chaabouni\net al., 2020; Kharitonov & Baroni, 2020). Note that this metric involves both the speaker and listener,\nwhile the emergent language corpus DM is generated solely by the speaker.\nTopographic Similarity\nThe topographic similarity (Brighton & Kirby, 2006; Lazaridou et al.,\n2018) measures how messages align with the meaning representations. More concretely, let CSij =\n−Ii · Ij/(||Ii||2||Ij||2) be the negative cosine similarity of image features Ii and Ij, and EDij be\nthe Levenshtein distance (Levenshtein et al., 1966) between messages Mi and Mj. The metric is\ndeﬁned as the Spearman rank correlation of the two distance metrics:\nrED,CS = ρR(ED),R(CS) = Ei,j\n\u0002\n(R(EDij) −µR(ED))(R(CSij) −µR(CS))\n\u0003\nσR(ED)σR(CS)\n(6)\nHere R(·) denotes the rank of the element in the list. This metric intuitively measures disentangle-\nment: different attributes of the input should be expressed in different token positions, and each\ntoken only describes one attribute (e.g. “big red cube” and “big blue cube”). Such a property has\nbeen claimed (Lazaridou et al., 2018; Li & Bowling, 2019) as connected to compositionality, a\nkey structural property of natural language. However, this metric is too rigid in its deﬁnition of\ncompositionality, ignoring aspects like argument structure, context or morphology which play a key\nrole in determining the combination of word semantics (Goldberg, 2015).\nWe note that experiments around both these metrics are within the game framework – accuracy is\nbased on game success, while topographic similarity measures how emergent messages align with\ngame input representations. This leaves two questions unsolved: (i) can emergent languages be used\noutside the game? (ii) if so, would these metrics predict the usefulness of emergent languages for\ndownstream tasks? We tackle these two questions in Sections 4 and 5 respectively.\n4\nPublished as a conference paper at ICLR 2022\n4\nEMERGENT CORPUS TRANSFER FOR NATURAL LANGUAGE TASKS\nIn this section, we present initial evidence that a corpus of emergent language DM can beneﬁt two\ntypes of downstream tasks involving natural language: language-only task (language modeling,\nSection 4.1) and vision-language task (image captioning, Section 4.2). Moreover, we show such\na beneﬁt is most signiﬁcant when the natural language resources are limited, which indicates that\nemergent language is potentially valuable for low-resource languages and tasks. We then analyze\nwhat contributes to such a transfer via ablation studies in Section 4.3.\n4.1\nLANGUAGE MODELING\nInspired by Papadimitriou & Jurafsky (2020), we consider the task of natural language modeling, and\nexplore if pretraining a language model on a corpus of emergent messages can reduce the downstream\nperplexity in a low-resource setup (Figure 1(b)), and how emergent language compares to other\nsources of synthetic and even natural language corpora.\nPre-training Corpora\nWe compare three source corpora: (i) a Spanish Wikipedia corpus (es) culled\nto 50,000 vocabulary size using the pre-processing script from Papadimitriou & Jurafsky (2020),\n(ii) a regular language of well-balanced brackets (paren-zipf) with the Zipf unigram distribution\nas es, and (iii) an emergent language corpus (ec). paren-zipf is the best-performing source corpus\nfrom Papadimitriou & Jurafsky (2020) not created by humans (e.g. natural language, music, or code)\nwith the inductive bias of hierarchical structure, and es should set an upper bound of the transfer\nperformance for both paren-zipf and ec. We vary the source corpus size from 2, 5, 10, 15 to 30\nmillion tokens to understand how the transfer results change with respect to the pre-training scale.\nFine-tuning Corpora\nWe scrape Wikipedia corpora of 10 languages to test downstream trans-\nfer: Danish (da, IE-Germanic), Basque (eu, Basque), Japanese (ja, Japanese), Romanian (ro,\nIE-Romance), Finnish (ﬁ, Uralic), Indonesian (id, Austronesian), Kazakh (kk, Turkic), Hebrew (he,\nAfro-Asiatic), Urdu (ur, IE-Indic), and Persian (fa, IE-Iranian). They come from diverse linguistic\nfamilies with different levels of resource richness. Each corpus has 2 million tokens and a vocabulary\nsize of 50,000, and is pre-processed (to remove noise) via the same script to process es.\nImplementation\nWe train the EC game and generate ec based on the Conceptual Captions\ndataset (Sharma et al., 2018), using more than 2.8 million natural images in the wild. Note that we\ndo not use the natural language captions in the dataset for EC game training. We take the 512-dim\npre-trained ResNet-18 (He et al., 2016) features before the classiﬁcation head as input image features\nIi. Other architecture and training details mainly follow Li et al. (2020b), and by default V = 4035,\nT = 15, K = 256. For language modeling, we adopt a Transformer (Vaswani et al., 2017) with 6\ndecoder layers and 6 attention heads, and pre-train on each source corpus for 3,000 steps with batch\nsize 32, input length 1,000, and learning rate 5 × 10−4. For ﬁne-tuning and training from scratch on\ndownstream corpora, the batch size is 8 and learning rate is 10−4. Hyperparameters are chosen after\ngrid search. We report the test perplexity at the best validation loss.\nResults\nAs can be seen from Figure 2, pre-training on ec (blue) signiﬁcantly reduces the perplexity\ncompared with training from scratch (red, dotted line with constant value), with an average reduction\nof 24.6% with merely 2 million pre-training tokens. This is a positive signal that a corpus of\nemergent language could be useful beyond the communication game itself.\nComparing two synthetic pre-training corpora, ec performs better than or comparable to paren-zipf\nthroughout different source sizes and downstream languages, with the exception of Basque and\nFinnish when pre-training size is more than 15 million. Given the strong inductive bias in paren-zipf\n(hierarchical structure and unigram information from real language), and the fact that ec is generated\nby just a single-layer GRU speaker without such explicit bias, such a result hints that ec may possess\nnon-trivial structural properties due to the communication signals and perceptual grounding.\nOn the other hand, the natural language corpus es usually achieve a lower perplexity than both ec\nand paren-zipf as may be expected. Most surprisingly, when pre-training only on 2M tokens, ec\nconsistently beats es in terms of transfer performance, and their performances are comparable on\nmost languages using 5 million source tokens. To offer one plausible explanation, ec could contain\n5\nPublished as a conference paper at ICLR 2022\n200\n250\n300\nPerplexity\nPersian (fa)\n1000\n1200\n1400\nBasque (eu)\n150\n200\n250\nDanish (da)\n200\n250\nRomanian (ro)\n100\n120\n140\nJapanese (ja)\nec\nes\nparen-zipf\nscratch\n2 5\n10\n15\n30\nSource Size (Million)\n150\n200\nPerplexity\nIndonesian (id)\n2 5\n10\n15\n30\nSource Size (Million)\n200\n250\nTurkic (kk)\n2 5\n10\n15\n30\nSource Size (Million)\n400\n500\nHebrew (he)\n2 5\n10\n15\n30\nSource Size (Million)\n150\n200\nUrdu (ur)\n2 5\n10\n15\n30\nSource Size (Million)\n140\n160\n180\n200\nFinnish (fi)\nFigure 2: Test perplexity for language modeling on ten natural languages, when either pre-trained\non a corpus of tokens from EC (ec, blue), Spanish (es, orange), well-balanced brackets (paren-zipf,\ngreen) or without any pre-training (scratch, red, dotted, constant with respect to source sizes).\nMetrics\ncoco (5k)\ncoco (50k)\ncoco (full)\nbase\n+ec\n+nl\nbase\n+ec\n+nl\nbase\n+ec\n+nl\nBLEU4\n14.1\n15.3\n18.5\n26.3\n27.2\n28.2\n35.8\n36.2\n36.0\nROUGE L\n39.8\n41.0\n44.4\n50.0\n50.7\n51.5\n56.9\n57.0\n57.1\nCIDEr\n30.8\n35.7\n48.0\n78.3\n82.8\n88.8\n117.5\n118.1\n118.5\nTable 1: Results on image captioning transfer using three ﬁne-tuning dataset sizes. base, +ec, +nl\ndenote training from scratch and pre-training using EC captions and NL captions, respectively.\nproperties closer to everyday conversations – smaller vocabulary, simpler structures (generated by one-\nlayer GRU) and repetitive (all about image references) – easier for language modeling, compared to\nthe complicated structures and statistical patterns of large text corpora like Wikipedia es. However, the\nrelative simplicity of ec also prevents the transfer from scaling as well as es - comparing pretraining\non 30 versus 2 million tokens, ec only reduces perplexity by 2.4% on average, while es reduces\n23.8%. Such a mixed ﬁnding points to potential in combining the strengths of simpler emergent\ncorpora (easier to acquire and beneﬁt from) and noisy natural language corpora (more structures and\ncomplexities) for more efﬁcient language learning. Such an idea of mixing NLP and EC training has\nbeen useful for some NLP (Fried et al., 2018; Lee et al., 2018) and EC (Lowe et al., 2020; Lazaridou\net al., 2020) tasks. Another important direction would be on how to evolve emergent languages\nwith more complexities that resemble and transfer to natural language, where existing approaches to\nimproving and regularizing EC (Mu & Goodman, 2021; Guo et al., 2019; Li & Bowling, 2019; Dess`ı\net al., 2021; Luna et al., 2020) could be tested through our transfer scheme.\n4.2\nIMAGE CAPTIONING\nThe language modeling transfer experiment leverages the structural properties of the source corpus,\nbut has little to do with the semantics of the source corpus. For example, a regular language like\nparen-zipf has no real meanings, but still reduces downstream perplexity due to its hierarchical\nstructure. Therefore, we also consider transfer to image captioning, a classical vision-language task\nwhere a model has to ground meaning of the words to the images. As shown in Figure 1(c), we\npre-train an image captioning model on unlabeled images to generate EC captions, then ﬁne-tune on\ndownstream annotated images to generate natural language captions. We note that such a transfer\nscheme cannot work for other synthetic corpora such as music, code, or paren-zipf as they are not\ngrounded on perceptual stimuli.\nPre-training Data\nWe use the EC speaker trained on Conceptual Captions to generate an EC\ncaption Mi based on each ResNet-18 image feature Ii in the dataset. As image captioning requires\nﬁne-grained information beyond a global feature, we extract detection features I′\ni for each image via\npre-trained Faster R-CNN (Ren et al., 2015) and ResNet-101 (He et al., 2016), and pre-train an image\ncaptioning model to generate EC captions based on detection features (I′\ni 7→Mi). To set up an upper\n6\nPublished as a conference paper at ICLR 2022\nMethod\nLM (ro)\nLM (he)\nEC pretrain\n198 (4)\n375 (14)\nfrom scratch\n265 (1)\n446 (8)\nrandom speaker\n313 (21)\n575 (24)\nrandom input\n245 (28)\n489 (77)\npermuted EC\n211 (1)\n383 (4)\nmodel transfer (GRU)\n655 (40)\n1105 (6)\ncorpus transfer (GRU)\n553 (53)\n1056 (55)\nTable 2: Ablations on EC game, corpus, and transfer setups.\nStandard deviations in parentheses. The language model is\nan GRU for last two rows and a Transformer for others.\nMetrics\nLM (ro)\nLM (he)\nAccuracy\n0.672\n0.737\nTopographic\n0.030\n0.003\nTranslation\n0.757\n0.829\nTable 3:\nPearson correlations be-\ntween different metrics (validation ac-\ncuracy, topographic similarity, emer-\ngent to natural language translation\nROUGE L) and downstream language\nmodeling performance (negated per-\nplexity).\nbound for transfer performance, we also use the English captions NLi in the Conceptual Captions\ndataset for pre-training (I′\ni 7→NLi).\nFine-tuning Data\nWe use the MS-COCO dataset (Lin et al., 2014) for ﬁne-tuning, which consists\nof around 500,000 pairs of images and English captions. We use the full training set, or a subset with\n5,000 or 50,000 samples to study the transfer beneﬁt when natural language annotation is limited.\nImplementation\nWe use a Transformer with 3 encoder layers and 6 decoder layers as the image\ncaptioning model, and perform pre-training and ﬁne-tuning in a seq2seq framework (Ott et al., 2019).\nNotably, we only transfer the encoder weights, and ﬁne-tune the whole network end-to-end with a\nlearning rate of 3 × 10−4, as we notice transferring the full weights leads to worse performance. We\nreport BLEU4 (Papineni et al., 2002), ROUGE L (Lin, 2004), and CIDEr (Vedantam et al., 2015)\nscores for the test split based on best validation loss.\nResults\nAs shown in Table 1, when ﬁne-tuning on 5,000 or 50,000 samples, pre-training on either\nEC or NL captions both signiﬁcantly improve the image captioning performance. For example,\nwith 50,000 samples, EC pre-training can improve the BLEU-4 score by +0.9 points, while NL\npre-training further improves +1.0 points. The fact that EC pre-training can provide half the beneﬁt\nof NL pre-training is surprising, because the former requires no language annotation - both training\nthe EC games and generating the EC captions can be done on unlabeled images in the wild, while the\nNL pretraining leverages more than 2.8 million English sentences.\nHowever, when the full MS-COCO dataset is used, even pretraining on 2.8 million English captions\nis not signiﬁcantly helpful, possibly because Conceptual Captions collect noisy Internet sentences\nwhile MS-COCO annotates captions with less diversity (Wang et al., 2020) so a model trained from\nscratch on the full dataset can capture most of the output patterns. Other vision-language tasks where\nnatural language annotations are harder or more limited (e.g. instruction following, navigation) would\npotentially beneﬁt more from EC pre-training, where we can turn abundant unlabeled visual stimuli\ninto annotations in the emergent language.\n4.3\nWHAT CONTRIBUTES TO SUCCESSFUL CORPUS TRANSFER?\nWe have shown that emergent languages can provide transfer beneﬁt for natural language tasks, but it\nis still unknown what properties of emergent languages contribute to such a beneﬁt. Hence we ablate\nseveral key aspects of the EC game training and EC corpus generation, and examine the effect via\ntransfer (with 15 million pre-training tokens) to two example languages, Romanian (ro) and Hebrew\n(he). We leave ablation studies on the image captioning experiment in Section A.3.\nCommunication and Perceptual Stimuli in EC Game\nThe emergent language stems from two\nelements in the game that human language acquisition relies on: perceptual grounding (input image\nfeatures) and communication (referential game). For ablation, we ﬁrst consider an untrained GRU\nspeaker with randomized parameters (random speaker), and use it to sample tokens conditioned on\nimage features according to (1). As Table 2 shows, pre-training on such a corpus leads to even worse\n7\nPublished as a conference paper at ICLR 2022\nperformance than training from scratch, because an untrained speaker has highly random generations\nwith no clear structure.\nOn the other hand, to ablate the effect of visual stimuli, we sample a set of N random vectors from\nN(µ(DI), σ(DI)), and replace it for DI as inputs of the EC game (random input). We then use the\ntrained speaker to generate a EC corpus based on the same random vectors. It turns out only one out\nof three game trials develop a game accuracy > 90%, and other two trials cannot learn an accuracy\nbeyond 5%. On Hebrew, the “successful” trial achieves a transfer perplexity of 388, which is still\nworse than EC corpus on visual inputs, while other two trials yield signiﬁcantly worse perplexities\n(573, 509). So it turns out communicating visual stimuli instead of random inputs is more robustly\nsuccessful, and more helpful for transfer even when communication can be successful.\nSentence Structure in EC Corpus\nTo investigate how structured EC sentences are, we indepen-\ndently shufﬂe each EC sentence in DM to form a new-pretraining corpus (permuted EC), and ﬁnd in\nTable 2 that it indeed leads to a consistent worse performance, though better than training from scratch.\nThis indicates the speaker develops a coding scheme with sentence structures beyond bag-of-words.\nModel vs. Corpus Transfer\nLi et al. (2020b) propose to transfer the EC models for a downstream\ntranslation task, whereas we leverage a speaker-generated EC corpus to pre-train with architectures\nthat could be more ﬂexible and powerful, which leads to several signiﬁcant practical advantages.\nFirst, EC model architectures might not apply to many downstream tasks (e.g. image captioning\nwith multiple input features), but corpus transfer could still be readily deployed (Section 4.2). On\nother tasks (e.g. language modeling) where model transfer could be implemented, its performance is\nstill constrained by the simple EC model architectures3. As shown in Table 2, directly ﬁne-tuning\nthe one-layer speaker GRU (model transfer (GRU)) indeed leads to much worse perplexities than\ncorpus transfer with a Transformer (EC pretrain), but more interestingly, is also worse than corpus\ntransfer with the same GRU architecture (corpus transfer (GRU)), possibly due to the beneﬁt of\nself-distillation (Zhang et al., 2019; Allen-Zhu & Li, 2020). In summary, our corpus transfer approach\nbetter utilizes the emergent communication for downstream tasks than model transfer.\n5\nEMERGENT-NATURAL LANGUAGE TRANSLATION AS A METRIC\nSection 4.3 presents initial evidence that the quality of an emergent language correlates with the\ntransfer performance, by showing ablated emergent languages transfer worse. However, directly\nusing transfer as an evaluation scheme for emergent languages has certain deﬁciencies. First, the pre-\ntraining and ﬁne-tuning optimizations are computationally expensive. Second, transfer performance\nacross different downstream tasks might induce inconsistency. Hence we propose a cheaper and\nsimpler evaluation for emergent languages by measuring how easily it translates to the corresponding\nnatural language grounded on the same input, and show that it better correlates with the transfer\nperformance than existing metrics across different tasks.\nMore concretely, we use a small set of captioned images, where image Ii is associated with a natural\nlanguage caption NLi. We also use the trained speaker to generate emergent message Mi based on\neach Ii, and train a translation model to map emergent sentences to natural sentences Mi 7→NLi.\nIntuitively, a higher translation score means the emergent and natural sentences are closer in structure\nand semantics, similar to how French-English translation might be easier than Chinese-English.\nImplementation\nTo estimate the correlation between metrics and transfer performance re-\nquires a population of emergent languages with different speakers.\nThus we choose\n5 EC game setups with varying vocabulary and sequence length limits ((V, T)\n∈\n{(4035, 5), (4035, 15), (4035, 25), (1000, 15), (10000, 25)}), and for each setup we run 4 trials for\n2,000 steps. We consider checkpoints every 200 steps, summing to 5 × 4 × 10 = 200 checkpoints.\nDue to computational limits, we train EC games on a small subset of 50,000 MS-COCO images, and\nuse another subset of 50,000 MS-COCO image-caption pairs to calculate the translation metric. For\ntranslation, we use the same seq2seq Transformer for image captioning, but only train for 2 epochs,\nso that an emergent language more similar to the natural language can perform better with limited\n3We note that unlike most NLP tasks, EC model architectures might not arbitrarily scale up due to optimization\nchallenges with discretization. For example, we have tried 2/3-layer GRUs for EC, but the transfer is worse.\n8\nPublished as a conference paper at ICLR 2022\n0\n200 400 600 800 1000\nTrain steps\n0\n25\n50\n75\n100\n(a) Metric (Val Accuracy)\n0\n200 400 600 800 1000\nTrain steps\n0.0\n0.1\n0.2\n0.3\n(b) Metric (Topographic)\n0\n200 400 600 800 1000\nTrain steps\n0.35\n0.40\n(c) Metric (Ours, ROUGE_L)\n0\n200 400 600 800 1000\nTrain steps\n260\n240\n220\n200\n(d) - Perplexity (ro)\nFigure 3: Different metrics and downstream Romanian perplexities (negated) with respect to training\nsteps, averaged over four trials with vocabulary limit 10000 and sequence length limit 15. Our metric\nbetter correlates with downstream performance across time steps.\ntraining. We use ROUGE L (Lin, 2004) as the translation metric, and ﬁnd other metrics with similar\nresults. For downstream language modeling, we use ImageNet (Deng et al., 2009) to generate a\ncorpus of 15 million tokens and ﬁne-tune on Romanian (ro) and Hebrew (he).\nResults\nWe compare with the two existing metrics, validation accuracy and topographic similarity\n(see Section 3.2). Table 3 shows how different metrics correlate with downstream performance\nacross all checkpoints form all trials4. Surprisingly, we ﬁnd little correlation between topographic\nsimilarity and downstream performance, while our translation metric has a better correlation (ρro =\n0.757, ρhe = 0.829) than validation accuracy (ρro = 0.672, ρhe = 0.737).\nTo better understand the results, we take all 4 trials from a speciﬁc game setup (V = 10000, T = 15)\nand plot in Figure 3 how the three metrics (accuracy, topographic, translation ROUGE L) as well as\nthe downstream Romanian performance change with training steps (similar analysis of more setups\n(vocabulary size, sequence length) in Section B.4). We observe that our metric (Figure 3(c)) aligns\nwith the downstream performance (Figure 3(d)) in that both reach a high level around 300 steps (red\ndash line) and maintains a similar level afterwards. The validation accuracy ((Figure 3(a))) has a\ngenerally similar tendency as these two metrics, with the difference being that it is only around 50%\nat step 300 and far from convergence, possibly due to the underdevelopment of the listener. A similar\nobservation was made in Li et al. (2020b). Such a dependency on the listener renders game success\nunreliable for the evaluating emergent languages produced by the speaker alone.\nOn the other hand, the topographic similarity (Figure 3(b)) peaks as early as step 300, after which the\ndegradation fails to explain the downstream performance. To illustrate the misalignment, consider the\n“best” language with respect to topographic, i.e. a fully disentangled coding scheme (e.g. “red/blue\ncube/sphere”). Such a corpus would essentially have a maximum possible entropy, pre-training on\nwhich should not help with any downstream tasks. Our ﬁnding suggests that simple measurements\nof rigid disentanglement fall short of estimating the compositionality of real language, and we\nmay need to take into account other structural properties of natural language, e.g. stable irregular-\nity (Kirby, 2001), or the role of argument structures, context and morphology (Goldberg, 2015;\nSteinert-Threlkeld, 2020).\n6\nCONCLUSION\nWhile previous research in language emergence tends to investigate different setups and how they\naffect certain metrics, such a paradigm cannot provide robust and valuable insights if the metrics are\nﬂawed. Thus, our work calls for a paradigm shift in how we evaluate and analyze language emergence\n— by stepping from metrics within each EC framework or over-approximate NL properties, and\ninstead directly linking to NL by transfer or translation. Such a paradigm shift enables several brand\nnew and exciting future directions — how to improve or regularize EC through the lens of our metrics,\nhow to push EC to be more useful for NLP5, and how to design more ﬁne-grained metrics for speciﬁc\nNL properties. We believe progress in these directions will further develop a synergy between EC\nand NLP research and bring beneﬁts and insights for both sides.\n4For topographic, we exclude 9 checkpoints with the metric undeﬁned due to zero variance of edit distances.\n5See relevant discussions at the end of Section 4.1 and Section 4.2.\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGEMENTS\nThis work was supported by MIT-IBM Watson AI Lab and its member company Nexplore, ONR\nMURI (N00014-13-1-0333), DARPA Machine Common Sense program, ONR (N00014-18-1-2847)\nand MERL. The information, data, or work presented herein was also funded by the Advanced\nResearch Projects Agency-Energy (ARPA-E), U.S. Department of Energy, under Award Number\nDE-AR0001210. SY and KN also acknowledge support from the National Science Foundation under\nGrant No. 2107048. The views and opinions of authors expressed herein do not necessarily state or\nreﬂect those of the United States Government or any agency thereof.\nETHICS STATEMENT\nOur work is a preliminary step toward aligning the language of machines and humans, which has\npotential positive social impacts in several ways. First, shaping machine communications towards\nthe structure of natural language contributes to human and AI alignment in terms of values and\nunderstanding of the world. Second, emergent languages could yield transfer beneﬁts for low-\nresource languages and tasks, contributing to linguistic diversity and inclusion. We could not think of\nnegative ethics impact for now, as the emergent language is still very different from natural language.\nREPRODUCIBILITY STATEMENT\nOur research in based on public codebases and datasets, with implementation details described\nthroughout the paper. We have conducted reasonable hyperparameter searches and ablation studies\nto make sure experiment conclusions are fair. In the Appendix, we further provide links to all used\ncode and data resources, and additional details including hyperparameter search setups and resource\nrequirements for each experiment. We will clean and release the code upon acceptance.\nREFERENCES\nZeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and\nself-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.\nMuhannad Alomari, Paul Duckworth, David Hogg, and Anthony Cohn. Natural language acquisition\nand grounding for embodied robotic systems. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 31, 2017.\nJacob Andreas and Dan Klein. Analogs of linguistic structure in deep representations. arXiv preprint\narXiv:1707.08139, 2017.\nJacob Andreas, Anca Dragan, and Dan Klein. Translating neuralese. arXiv preprint arXiv:1704.06960,\n2017.\nMichelle E Barton. Input and interaction in language acquisition. Cambridge University Press, 1994.\nNatalie L Bohlmann and Jason T Downer. Self-regulation and task engagement as predictors of\nemergent language and literacy skills. Early Education and Development, 27(1):18–37, 2016.\nHenry Brighton and Simon Kirby. Understanding linguistic evolution by visualizing the emergence\nof topographic mappings. Artiﬁcial life, 12(2):229–242, 2006.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nJerome Bruner. The role of interaction formats in language acquisition. In Language and social\nsituations, pp. 31–46. Springer, 1985.\nAngelo Cangelosi and Domenico Parisi. Computer simulation: A new scientiﬁc approach to the\nstudy of language evolution. In Simulating the evolution of language, pp. 3–28. Springer, 2002.\n10\nPublished as a conference paper at ICLR 2022\nRahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni.\nCompositionality and generalization in emergent languages. arXiv preprint arXiv:2004.09124,\n2020.\nMichael Cogswell, Jiasen Lu, Stefan Lee, Devi Parikh, and Dhruv Batra. Emergence of compositional\nlanguage with deep generational transmission. arXiv preprint arXiv:1904.09067, 2019.\nVerna Dankers, Elia Bruni, and Dieuwke Hupkes. The paradox of the compositionality of natural\nlanguage: a neural machine translation case study. arXiv preprint arXiv:2108.05885, 2021.\nJill G De Villiers, Jill De Villiers, Peter A De Villiers, and Peter A DeVilliers. Language acquisition.\nHarvard University Press, 1978.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nRoberto Dess`ı, Eugene Kharitonov, and Marco Baroni. Interpretable agent communication from\nscratch (with a generic visual processor emerging on the side). arXiv preprint arXiv:2106.04258,\n2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nDaniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency,\nTaylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models\nfor vision-and-language navigation. arXiv preprint arXiv:1806.02724, 2018.\nAdele E Goldberg. Compositionality. In The Routledge handbook of semantics, pp. 435–449.\nRoutledge, 2015.\nShangmin Guo, Yi Ren, Serhii Havrylov, Stella Frank, Ivan Titov, and Kenny Smith. The emergence\nof compositional languages for numeric concepts through iterated learning in neural agents. arXiv\npreprint arXiv:1910.05291, 2019.\nStevan Harnad. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3):335–346,\n1990.\nSerhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to\ncommunicate with sequences of symbols. arXiv preprint arXiv:1705.11192, 2017.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npp. 770–778, 2016.\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144, 2016.\nEugene Kharitonov and Marco Baroni. Emergent language generalization and acquisition speed are\nnot tied to compositionality. arXiv preprint arXiv:2004.03420, 2020.\nSimon Kirby. Spontaneous evolution of linguistic structure-an iterated learning model of the emer-\ngence of regularity and irregularity. IEEE Transactions on Evolutionary Computation, 5(2):\n102–110, 2001.\nSimon Kirby. Natural language from artiﬁcial life. Artiﬁcial life, 8(2):185–215, 2002.\nSimon Kirby, Monica Tamariz, Hannah Cornish, and Kenny Smith. Compression and communication\nin the cultural evolution of linguistic structure. Cognition, 141:87–102, 2015.\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building\nmachines that learn and think like people. Behavioral and brain sciences, 40, 2017.\nAngeliki Lazaridou and Marco Baroni. Emergent multi-agent communication in the deep learning\nera. arXiv preprint arXiv:2006.02419, 2020.\n11\nPublished as a conference paper at ICLR 2022\nAngeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the\nemergence of (natural) language. arXiv preprint arXiv:1612.07182, 2016.\nAngeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of lin-\nguistic communication from referential games with symbolic and pixel input. arXiv preprint\narXiv:1804.03984, 2018.\nAngeliki Lazaridou, Anna Potapenko, and Olivier Tieleman. Multi-agent communication meets\nnatural language: Synergies between functional and structural language learning. arXiv preprint\narXiv:2005.07064, 2020.\nJason Lee, Kyunghyun Cho, Jason Weston, and Douwe Kiela. Emergent translation in multi-agent\ncommunication. In International Conference on Learning Representations, 2018.\nVladimir I Levenshtein et al. Binary codes capable of correcting deletions, insertions, and reversals.\nIn Soviet physics doklady, volume 10, pp. 707–710. Soviet Union, 1966.\nFushan Li and Michael Bowling. Ease-of-teaching and language structure from emergent communi-\ncation. arXiv preprint arXiv:1906.02403, 2019.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision, pp. 121–137. Springer, 2020a.\nYaoyiran Li, Edoardo Maria Ponti, Ivan Vuli´c, and Anna Korhonen. Emergent communication\npretraining for few-shot machine translation. In Proceedings of the 28th International Conference\non Computational Linguistics, pp. 4716–4731, 2020b.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74–81, 2004.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision, pp. 740–755. Springer, 2014.\nRyan Lowe, Abhinav Gupta, Jakob Foerster, Douwe Kiela, and Joelle Pineau. On the interaction\nbetween supervision and self-play in emergent communication. arXiv preprint arXiv:2002.01093,\n2020.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal\ncomputation engines. arXiv preprint arXiv:2103.05247, 2021.\nDiana Rodr´ıguez Luna, Edoardo Maria Ponti, Dieuwke Hupkes, and Elia Bruni. Internal and external\npressures on language emergence: least effort, object constancy and frequency. arXiv preprint\narXiv:2004.03868, 2020.\nIgor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent\npopulations. In Thirty-second AAAI conference on artiﬁcial intelligence, 2018.\nJesse Mu and Noah Goodman.\nEmergent communication of generalizations.\narXiv preprint\narXiv:2106.02668, 2021.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint\narXiv:1904.01038, 2019.\nIsabel Papadimitriou and Dan Jurafsky. Learning music helps you read: Using transfer to study\nlinguistic structure in language models. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pp. 6829–6839, 2020.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics, pp. 311–318, 2002.\n12\nPublished as a conference paper at ICLR 2022\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. Advances in neural information processing systems, 28:\n91–99, 2015.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n2556–2565, 2018.\nKenny Smith, Henry Brighton, and Simon Kirby. Complex systems in language evolution: the\ncultural emergence of compositional structure. Advances in complex systems, 6(04):537–558,\n2003.\nShane Steinert-Threlkeld. Toward the emergence of nontrivial compositionality. Philosophy of\nScience, 87(5):897–909, 2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 4566–4575, 2015.\nPaul Vogt. The physical symbol grounding problem. Cognitive Systems Research, 3(3):429–457,\n2002.\nKyle Wagner, James A Reggia, Juan Uriagereka, and Gerald S Wilkinson. Progress in the simulation\nof emergent communication and language. Adaptive Behavior, 11(1):37–69, 2003.\nZeyu Wang, Berthy Feng, Karthik Narasimhan, and Olga Russakovsky. Towards unique and informa-\ntive captioning of images. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part VII 16, pp. 629–644. Springer, 2020.\nLudwig Wittgenstein. Philosophical investigations. New York: Macmill an Publish, 1958.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers:\nState-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\nMichaela L Zajicek-Farber. The contributions of parenting and postnatal depression on emergent\nlanguage of children in low-income families. Journal of Child and Family Studies, 19(3):257–269,\n2010.\nLinfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your\nown teacher: Improve the performance of convolutional neural networks via self distillation. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3713–3722, 2019.\n13\nPublished as a conference paper at ICLR 2022\nA\nIMPLEMENTATION DETAILS\nA.1\nEMERGENT COMMUNICATION GAME\nWe adapt the public code6 from Li et al. (2020b) and mostly follow their default setups. For training,\nwe use a batch size of 256, and each batch element contains one input images and other 255 distractor\nimages. Since the Conceptual Captions dataset has more than 2.8 million images, random sampling a\nbatch of data is computationally costly. So for each batch, we ﬁrst sample 50,000 images from the\nwhole dataset, then sample input and distractor pairs from this subset. We use an Adam optimizer\nwith learning rate 10−3. We use a soft version of Gumbel-softmax with temperature 1, and have\ntried hard Gumbel-softmax and found it not further helpful for downstream performance. Each game\ntraining only takes less than 12 hours using one GeForce RTX 2080 GPU.\nA.2\nLANGUAGE MODELING\nWe use the public script7 from Papadimitriou & Jurafsky (2020) to pre-process Wikipedia corpora\nof different languages, using the default setup of culling to 50,000 vocabulary size. We hand-pick\ndownstream languages to make sure they represent different linguistic families.\nWe use the language modeling script8 from Huggingface (Wolf et al., 2019) for both pre-training and\nﬁne-tuning.\nWe have tried grid search for the pre-training learning rate (10−3, 5 × 10−4, 10−4) and batch size\n(4, 32), which checkpoint to transfer (1000, 2000, 3000), as well as the ﬁne-tuning learning rate\n(10−4, 5 × 10−5, 10−5) and batch size (8, 32). We ﬁnd that for all three source corpora (es, ec,\nparen-zipf), it works best to pre-train with learning rate 5 × 10−4 and batch size (32), transfer using\nthe checkpoint with 3000 training steps, and ﬁne-tune with learning rate 10−4 and batch size 8. For\ntraining from scratch, we have tried grid search for the learning rate (10−3, 5×10−4, 10−4, 5×10−5)\nand batch size (4, 32), and ﬁnd that learning rate 10−4 and batch size 8 work best for different\ndownstream languages. An pre-training experiment can ﬁnish within one hour using one GeForce\nRTX 3090 GPU, while a ﬁne-tuning or training-from-scratch experiment can ﬁnish within one hour\nusing one GeForce RTX 2080 GPU.\nA.3\nIMAGE CAPTIONING\nWe use the pre-processed detection features9 of Conceptual Captions from the codebase of Li et al.\n(2020a).\nFor both pre-training and ﬁne-tuning, we use a public codebase10 for image captioning based on\nFAIRSEQ (Ott et al., 2019), and mostly follow their default setups. Pre-training on Conceptual\nCaptions takes 8 GeForce RTX 3090 GPU for around two days. Fine-tuning takes 1 GeForce RTX\n2080 GPU for one hour.\nB\nADDITIONAL RESULTS\nB.1\nLANGUAGE UNIGRAMS\nAs shown in Figure 4, the es and paren-zipf corpora have a larger vocabulary size (5000) and a larger\nentropy (6.48). While ec is set with vocabulary limit 4,035, its corpus only uses around 2,500 words\nwith smaller entropy (3.7). The ec corpus with random speaker almost has a large entropy (7.98).\n6https://github.com/cambridgeltl/ECNMT/tree/master/ECPRETRAIN\n7https://github.com/toizzy/tilt-transfer/tree/master/corpora/create_\nwiki_corpus\n8https://github.com/huggingface/transformers/blob/v4.4.2/examples/\nlanguage-modeling/run_clm.py\n9https://github.com/microsoft/Oscar/blob/master/VinVL_DOWNLOAD.md\n10https://github.com/krasserm/fairseq-image-captioning\n14\nPublished as a conference paper at ICLR 2022\n0\n10000\n20000\n30000\n40000\n50000\n4\n6\n8\n10\n12\n14\n16\nes and paren-zipf, entropy=6.48\n0\n500\n1000\n1500\n2000\n2500\n0\n2\n4\n6\n8\n10\n12\n14\nec, entropy=3.7\n0\n1000\n2000\n3000\n4000\n9\n10\n11\n12\n13\n14\n15\nec (random speaker), entropy=7.98\nFigure 4: Unigram distributions of (1) es and paren-zipf, (2) ec, and (3) ec with random speaker.\nFigure 5: The validation CIDEr (Vedantam et al., 2015) score across different ﬁne-tuning epochs,\nwhen using 5,000, 50,000, or the all samples of MS-COCO training samples.\nB.2\nIMAGE CAPTIONING\nWe visualize the ﬁne-tuning process of image captioning experiments in Figure 5. Interestingly,\nwe ﬁnd that under different natural language resource conditions (5,000, 50,000, or all samples in\nthe MS-COCO (Lin et al., 2014) training set) the training progress is different. Speciﬁcally, with\n5,000 samples, EC or NL pre-training and training from scratch ﬁrst learn similarly well, then gaps\ngradually appear with more training epochs. In contrast, when more han 50,0000 samples are used,\nthe gap between pre-training methods and training from scratch is most signiﬁcant when trained\nfor only one epoch, and it starts to diminish with more training epochs. It suggests that even when\ndownstream natural language resources are abundant, pre-training on an EC corpus might still help in\na fast adaption setup.\nWe also perform additional ablation studies to conﬁrm the non-triviality of image captioning results.\nDue to computation limits, we use 450k MS-COCO samples (instead of 3M Conceptual Captions\nsamples used in the main paper) to pre-train (image 7→emergent caption / shufﬂed emergent caption /\nrandom paren-zipf string with the same unigram as emergent captions), and the other 50k MS-COCO\nsamples to ﬁnetune (image 7→English caption). As shown in Table 4, pre-training with shufﬂed\nemergent captions or ungrounded strings leads to worse performance than even no pre-training, while\npre-training with emergent captions still improves the downstream performance. This helps address\nthe importance of having a semantically grounded and structural language for vision-language pre-\ntraining. However, we do note that in the case of MS-COCO image captioning, ﬁne-tuning usually\ndoes the heavy-lifting and pre-training contributes much less than the case of language modeling.\nThat is why our work focuses analysis on the language modeling experiments.\nB.3\nCORRELATIONS OF METRICS AND DOWNSTREAM PERFORMANCE\nFigure 6 plots the 200 data points in terms of their metric values and downstream performance on\nRomanian and Hebrew modeling, respectively. We note that topographic similarity is usually very\n15\nPublished as a conference paper at ICLR 2022\nPre-training language\nCIDEr\nEmergent language\n62.5 (1.4)\nEmergent language (shufﬂed)\n59.2 (0.5)\nParen-zipf\n59.5 (0.2)\nNo pre-training\n60.9 (0.5)\nTable 4: Image captioning results with different pre-training languages or no pre-training.\n0\n20\n40\n60\n80\n100\nacc\n350\n325\n300\n275\n250\n225\n200\n-ppl (ro)\nPearson Correlation: 0.672\n0.30\n0.35\n0.40\n0.45\nROUGE_L\nPearson Correlation: 0.757\n0.0\n0.1\n0.2\n0.3\n0.4\nts\nPearson Correlation: 0.030\n0\n20\n40\n60\n80\n100\nacc\n650\n600\n550\n500\n450\n400\n-ppl (he)\nPearson Correlation: 0.737\n0.30\n0.35\n0.40\n0.45\nROUGE_L\nPearson Correlation: 0.829\n0.0\n0.1\n0.2\n0.3\n0.4\nts\nPearson Correlation: 0.003\nFigure 6: 200 data points with x axis being metrics (validation acuracy, translation ROUGE L,\ntopographic) and y axis being downstream performance (negated perplexity) on Romanian (ro) and\nHebrew (he), respectively.\nlow (less than 0.1), and a higher value does not correlate with better downstream performance. Also,\nwe note that data points around the beginning of training (e.g. 200 steps) have very low accuracy or\ntranslation score, and their downstream performance is worse but signiﬁcantly more varied than the\nrest of data points.\nB.4\nHOW SETUPS (VOCABULARY SIZE, SEQUENCE LENGTH) EFFECT METRICS AND\nDOWNSTREAM PERFORMANCES\nFigure 3 in Section 5 mainly studies how metrics and downstream performance change with respect\nto the training steps, while controlling the other setups. Following a similar logic, in Figure 7 we take\nthose checkpoints with 1000 training steps and sequence lengths being 15 and plot the downstream\nperformances and metrics with respect to the vocabulary size (1000, 4035, 10000), and in Figure 8\nwe take those checkpoints with 1000 training steps and vocabulary size being 4035 and plot the\ndownstream performances and metrics with respect to the sequence length (5, 15, 25).\nFor the vocabulary size setup (Figure 7), we ﬁnd that both validation accuracy and our metric capture\nthat the performance when vocabulary size is 1000 is worse and has signiﬁcantly more variance. In\ncontrast, topographic similarity is highest when vocabulary size is smallest (1000), which disagrees\nwith downstream performances. However, all metrics are higher when vocabulary size is 4035 instead\nof 10000, which is the opposite for downstream performances.\n16\nPublished as a conference paper at ICLR 2022\nFor the sequence length setup (Figure 8), we ﬁnd that our metric captures the fact that a larger\nsequence length leads to better and less varied downstream performance. In contrast, validation\naccuracy with sequence length being 25 is worse than sequence length being 15, and topographic\nsimilarity fails to capture the decreasing trend of performance variance.\nIn summary, we ﬁnd that a larger vocabulary size or sequence length within our setup choices leads to\nbetter downstream performance with less variance, which is best captured by our translation metric.\n1000\n4035\n10000\nvocab\n70\n80\n90\n100\n(a) Metric (Val Accuracy)\n1000\n4035\n10000\nvocab\n0.06\n0.08\n0.10\n0.12\n(b) Metric (Topographic)\n1000\n4035\n10000\nvocab\n0.39\n0.40\n0.41\n0.42\n0.43\n(c) Metric (Ours, ROUGE_L)\n1000\n4035\n10000\nvocab\n225\n220\n215\n210\n205\n200\n(d) - Perplexity (ro)\n1000\n4035\n10000\nvocab\n225\n220\n215\n210\n205\n200\n(e) - Perplexity (he)\nFigure 7: How metrics and downstream performances change with respect to vocabulary size (1000,\n4035, 10000). Sequence length is 15 and training step is 1000.\n5\n15\n25\nseqlen\n75\n80\n85\n90\n95\n100\n(a) Metric (Val Accuracy)\n5\n15\n25\nseqlen\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n(b) Metric (Topographic)\n5\n15\n25\nseqlen\n0.415\n0.420\n0.425\n0.430\n0.435\n0.440\n(c) Metric (Ours, ROUGE_L)\n5\n15\n25\nseqlen\n230\n220\n210\n200\n(d) - Perplexity (ro)\n5\n15\n25\nseqlen\n430\n420\n410\n400\n390\n380\n370\n(e) - Perplexity (he)\nFigure 8: How metrics and downstream performances change with respect to sequence length (5, 15,\n25). Vocabulary size is 4035 and training step is 1000.\nB.5\nQUALITATIVE EXAMPLES\nFigure 9 includes some COCO images (not Conceptual Captions training images for EC) and their\nnatural and emergent captions. Manual check might reveal some patterns, e.g. transportation tools\nmight start with 2340 (a, b, j), indoor scenes might start with two 1777 tokens and indoor home\nscenes might start with even more 1777 tokens (d, e, k, l). Still, we note that our paper intentionally\naims to move away from previous manual-check paradigms in EC papers and tries to establish a more\nstandardized, scalable, and robust quantitative metric to enable more progress.\n17\nPublished as a conference paper at ICLR 2022\n(a) large passenger airplane ﬂy-\ning through the air. [2340, 2403,\n2308, 320, 1329, 2308, 2308,\n1329, 3643, 1512, 4033, 1298,\n1526, 1526, 0]\n(b) delta airplane at the airport on\nthe runway. [2340, 2422, 2403,\n141, 2422, 320, 2340, 2308, 3701,\n110, 3701, 397, 1241, 587, 0]\n(c) green street light in between\ntwo buildings. [1603, 320, 2422,\n1965, 2403, 3098, 319, 1965,\n1307, 1526, 1307, 2389, 1526,\n1223, 0]\n(d) this kitchen has stainless steel\nappliances and granite counter-\ntops. [1777, 1777, 1777, 1777,\n1298, 1777, 3682, 2823, 3701,\n3701, 3643, 4033, 1526, 1526, 0]\n(e) black white and brown cat\nlooking into a toilet bowl. [1777,\n1777, 1777, 1043, 1777, 1043,\n1976, 1777, 2422, 2550, 2762,\n477, 2422, 3715, 0]\n(f) large airplane and a truck next\nto a building. [1777, 2403, 2422,\n2308, 1965, 814, 2308, 397, 1526,\n477, 2389, 1526, 2389, 1526, 0]\n(g) there are bicycles parked along\na stone sidewalk. [1777, 1965,\n2403, 1329, 1329, 1329, 319,\n2277, 1329, 1526, 1526, 3701,\n1526, 2277, 0]\n(h) cat sitting next to a bunch of\nbikes parked next to each other.\n[2422, 1777, 1043, 320, 1329,\n1043, 3643, 3682, 3643, 477,\n3701, 2944, 2277, 2389, 0]\n(i) photo of a sidewalk and yard\nwith a ﬁre hydrant and signs.\n[1603, 2422, 319, 1603, 1043,\n319, 2762, 320, 2277, 2762, 141,\n3701, 2389, 1526, 0]\n(j) man standing on a vehicle with\ntwo large wheels. [2340, 1777,\n1965, 319, 1329, 3682, 1329,\n1329, 3643, 1329, 165, 3643,\n2389, 2389, 0]\n(k) television screen is mounted in\nthe wall of the bathroom. [1777,\n1777, 1777, 1777, 1777, 2762,\n1965, 2550, 3226, 4012, 2823,\n2550, 2655, 4033, 0]\n(l) restaurant that has many ta-\nbles set up . [1777, 1777, 3098,\n320, 320, 1298, 1965, 2823, 3701,\n1298, 1714, 1241, 3701, 4033, 0]\nFigure 9: Images from COCO and their natural and emergent captions.\n18\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2022-03-24",
  "updated": "2022-03-24"
}