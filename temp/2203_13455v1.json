{
  "id": "http://arxiv.org/abs/2203.13455v1",
  "title": "A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training",
  "authors": [
    "Yifei Wang",
    "Yisen Wang",
    "Jiansheng Yang",
    "Zhouchen Lin"
  ],
  "abstract": "Adversarial Training (AT) is known as an effective approach to enhance the\nrobustness of deep neural networks. Recently researchers notice that robust\nmodels with AT have good generative ability and can synthesize realistic\nimages, while the reason behind it is yet under-explored. In this paper, we\ndemystify this phenomenon by developing a unified probabilistic framework,\ncalled Contrastive Energy-based Models (CEM). On the one hand, we provide the\nfirst probabilistic characterization of AT through a unified understanding of\nrobustness and generative ability. On the other hand, our unified framework can\nbe extended to the unsupervised scenario, which interprets unsupervised\ncontrastive learning as an important sampling of CEM. Based on these, we\npropose a principled method to develop adversarial learning and sampling\nmethods. Experiments show that the sampling methods derived from our framework\nimprove the sample quality in both supervised and unsupervised learning.\nNotably, our unsupervised adversarial sampling method achieves an Inception\nscore of 9.61 on CIFAR-10, which is superior to previous energy-based models\nand comparable to state-of-the-art generative models.",
  "text": "Published as a conference paper at ICLR 2022\nA UNIFIED CONTRASTIVE ENERGY-BASED MODEL\nFOR UNDERSTANDING THE GENERATIVE ABILITY OF\nADVERSARIAL TRAINING\nYifei Wang1\nYisen Wang2,3∗\nJiansheng Yang1\nZhouchen Lin2,3,4\n1 School of Mathematical Sciences, Peking University\n2 Key Lab. of Machine Perception (MoE), School of Artiﬁcial Intelligence, Peking University\n3 Institute for Artiﬁcial Intelligence, Peking University\n4 Peng Cheng Laboratory\nABSTRACT\nAdversarial Training (AT) is known as an effective approach to enhance the ro-\nbustness of deep neural networks. Recently researchers notice that robust mod-\nels with AT have good generative ability and can synthesize realistic images,\nwhile the reason behind it is yet under-explored. In this paper, we demystify this\nphenomenon by developing a uniﬁed probabilistic framework, called Contrastive\nEnergy-based Models (CEM). On the one hand, we provide the ﬁrst probabilistic\ncharacterization of AT through a uniﬁed understanding of robustness and gener-\native ability. On the other hand, our uniﬁed framework can be extended to the\nunsupervised scenario, which interprets unsupervised contrastive learning as an\nimportant sampling of CEM. Based on these, we propose a principled method\nto develop adversarial learning and sampling methods. Experiments show that\nthe sampling methods derived from our framework improve the sample quality\nin both supervised and unsupervised learning. Notably, our unsupervised adver-\nsarial sampling method achieves an Inception score of 9.61 on CIFAR-10, which\nis superior to previous energy-based models and comparable to state-of-the-art\ngenerative models.\n1\nINTRODUCTION\nAdversarial Training (AT) is one of the most effective approaches developed so far to improve the\nrobustness of deep neural networks (DNNs) (Madry et al., 2018). AT solves a minimax optimization\nproblem, with the inner maximization generating adversarial examples by maximizing the classiﬁca-\ntion loss, and the outer minimization ﬁnding model parameters by minimizing the loss on adversarial\nexamples generated from the inner maximization (Wang et al., 2019). Recently, researchers have\nnoticed that such robust classiﬁers obtained by AT are able to extract features that are perceptually\naligned with humans (Engstrom et al., 2019). Furthermore, they are able to synthesize realistic im-\nages on par with state-of-the-art generative models (Santurkar et al., 2019). Nevertheless, it is still\na mystery why AT is able to learn more semantically meaningful features and turn classiﬁers into\ngenerators. Besides, AT needs the labeled data {(xi, yi)} for training while canonical deep genera-\ntive models do not, e.g., VAE (Kingma & Welling, 2014) and GAN (Goodfellow et al., 2015) only\nrequire {xi}. Thus, it is worth exploring if it is possible to train a robust model without labeled data.\nSeveral recent works (Jiang et al., 2020; Kim et al., 2020; Ho & Vasconcelos, 2020) have proposed\nunsupervised AT by adversarially attacking the InfoNCE loss (Oord et al., 2018) (a widely used\nobjective in unsupervised contrastive learning), which indeed improves the robustness of contrastive\nencoders. However, a depth investigation and understanding for unsupervised AT is still missing.\nTo address the above issues, in this work, we propose a uniﬁed probabilistic framework, Contrastive\nEnergy-based Models (CEM), that provides a principled understanding on the robustness and the\ngenerative ability of different training paradigms. Speciﬁcally, we make the following contributions:\n• Demystifying adversarial training and sampling. We ﬁrstly propose a probabilistic in-\nterpretation for AT, that is, it is inherently a (biased) maximum likelihood training of the\n∗Corresponding author: Yisen Wang (yisen.wang@pku.edu.cn).\n1\narXiv:2203.13455v1  [cs.LG]  25 Mar 2022\nPublished as a conference paper at ICLR 2022\ncorresponding energy-based model, which explains the generative ability of robust models\nlearned by AT. Inspired by this, we propose some novel sampling algorithms with better\nsample quality than previous methods.\n• A uniﬁed probabilistic framework. Based on the understanding above, we propose Con-\ntrastive Energy-based Model (CEM) that incorporates both supervised and unsupervised\nlearning paradigms. Our CEM provides a uniﬁed probabilistic understanding of previous\nstandard and adversarial training methods in both supervised and unsupervised learning.\n• Principled unsupervised adversarial training and sampling. Speciﬁcally, under our\nproposed CEM framework, we establish the equivalence between the importance sampling\nof CEM and the InfoNCE loss of contrastive learning, which enables us to design principled\nadversarial sampling for unsupervised learning.\nNotably, we show that the sampling methods derived from our framework achieve state-of-the-art\nsample quality (9.61 Inception score) with unsupervised robust models, which is comparable to both\nthe supervised counterparts and other state-of-the-art generative models.\n2\nRELATED WORK\nRobust generative models. Researchers recently notice that features extracted by robust classiﬁers\nare perceptually aligned with humans, while standard classiﬁers are not (Engstrom et al., 2019;\nKaur et al., 2019; Bai et al., 2021). Santurkar et al. (2019) show that we can also generate images\nof high quality with robust classiﬁers by iterative updating from a randomly sampled noise, where\nthe resulting sample quality is comparable to the state-of-the-art generative models like BigGAN\n(Brock et al., 2018).\nContrastive learning. Oord et al. (2018) ﬁrstly propose unsupervised contrastive learning by max-\nimizing a tractable lower bound on mutual information (MI), i.e., the negative InfoNCE loss. How-\never, later works ﬁnd that the lower bounds degrade a lot with a large MI, and the success of these\nmethods cannot be attributed to the properties of MI alone (Poole et al., 2019; Tschannen et al.,\n2020). Our work provides an alternative understanding of unsupervised contrastive learning as im-\nportance sampling of an energy-based model, which also enables us to characterize the limitations of\nexisting methods from a new perspective. In fact, contrastive learning can also be seen as a general\nlearning framework beyond the unsupervised scenarios. For example, SupContrast (Khosla et al.,\n2020) extends contrastive learning to supervised scenarios. Our work further bridges supervised,\nunsupervised and adversarial contrastive learning with a uniﬁed probabilistic framework.\n3\nCEM: A UNIFIED PROBABILISTIC FRAMEWORK\nInspired by previous work that bridges discriminative models with energy-based models (Grathwohl\net al., 2019), in this work, we propose a uniﬁed framework, called Contrastive Energy-based Model\n(CEM), that incorporates both supervised and unsupervised scenarios.\nOur proposed CEM is a special Energy-based Model (EBM) that models the joint distribution\npθ(u, v) over two variables (u, v) with a similarity function fθ(u, v) deﬁned in a contrastive form,\npθ(u, v) = exp(fθ(u, v))\nZ(θ)\n,\n(1)\nwhere Z(θ) =\nR\nexp (fθ(u, v)) dudv is the corresponding partition function. In other words, in\nCEM, a pair of samples (u, v) has higher probability if they are more alike. In particular, it can be\ninstantiated into the two following variants under different learning scenarios.\nParametric CEM. In the supervised scenario, we specify the Parametric CEM (P-CEM) that models\nthe joint distribution pθ(x, y) of data x and label y in the following form,\npθ(x, y) = exp(fθ(x, y))\nZ(θ)\n= exp(gθ(x)⊤wy)\nZ(θ)\n,\n(2)\nwhere gθ : Rn →Rm denotes the encoder, g(x) ∈Rm is the representation of x, and wk ∈Rm\nrefers to the parametric cluster center of the k-th class. Denote the linear classiﬁcation weight as\n2\nPublished as a conference paper at ICLR 2022\nW = [w1, · · · , wK] and the logit vector as h(x) = g(x)⊤W, we can see the equivalence between\nP-CEM and JEM (Grathwohl et al., 2019) as\nfθ(x, y) = gθ(x)⊤wy = hθ(x)[y].\n(3)\nNon-Parametric CEM. In the unsupervised scenario, we do not have access to labels, thus we\ninstead model the joint distribution between two natural samples (x, x′) as\npθ(x, x′) = exp(fθ(x, x′))\nZ(θ)\n= exp\n\u0000gθ(x)⊤gθ(x′)\n\u0001\nZ(θ)\n,\n(4)\nand the corresponding likelihood gradient of this Non-Parametric CEM (NP-CEM) is\n∇θEpd(x,x′) log pθ(x, x′) = Epd(x,x′)∇θfθ(x, x′)−Epθ(ˆx,ˆx′)∇θfθ(ˆx, ˆx′).\n(5)\nIn contrastive to P-CEM that incorporates parametric cluster centers, the joint distribution of NP-\nCEM is directly deﬁned based on the feature-level similarity between the two instances (x, x′). We\ndeﬁne the joint data distribution pd(x, x′) = pd(x)pd(x′|x) through re-parameterization,\nx′ = fθ(t(x)),\nt\nu.a.r.\n∼\nT ,\nx ∼pd(x),\n(6)\nwhere u.a.r. denotes sampling uniformly at random and T refers to a set of predeﬁned data aug-\nmentation operators T = {t : Rn →Rn}. For the ease of exposition, we assume the empirical\ndata distribution pd(x) is uniformly distributed over a ﬁnite (but can be exponentially large) set of\nnatural samples X.\n4\nSUPERVISED SCENARIO: REDISCOVERING ADVERSARIAL TRAINING AS\nMAXIMUM LIKELIHOOD TRAINING\nIn this section, we investigate why robust models have a good generative ability. The objective of\nAT is to solve the following minimax optimization problem:\nmin\nθ Epd(x,y)\n\u0014\nmax\n∥ˆx−x∥p≤ε ℓCE(ˆx, y; θ)\n\u0015\n, where ℓCE(ˆx, y; θ) = −log pθ(y|ˆx).\n(7)\nThe inner maximization problem is to ﬁnd an adversarial example ˆx within the ℓp-norm ε-ball around\nthe natural example x that maximizes the CE loss. While the outer minimization problem is to ﬁnd\nmodel parameters that minimize the loss on the adversarial examples ˆx.\n4.1\nMAXIMIZATION PROCESS\nFor the inner maximization problem, Projected Gradient Descent (PGD) (Madry et al., 2018) is the\ncommonly used method, which generates the adversarial example ˆx by maximizing the CE loss1\n(i.e., minimizing the log conditional probability) starting from ˆx0 = x:\nˆxn+1 = ˆxn + α∇ˆxnℓ(ˆxn, y; θ) = ˆxn −α∇ˆxn log pθ(y|ˆxn)\n= ˆxn + α∇ˆxn\n\"\nlog\nK\nX\nk=1\nexp(fθ(ˆxn, k))\n#\n−α∇ˆxnfθ(ˆxn, y),\n(8)\nwhile the Langevin dynamics for sampling P-CEM starts from random noise ˆx0 = δ and updates\nwith\nˆxn+1 = ˆxn + α∇ˆx log pθ(ˆxn) +\n√\n2α · ε\n(9)\n= ˆxn + α∇ˆxn\n\"\nlog\nK\nX\nk=1\nexp(fθ(ˆxn, k))\n#\n+\n√\n2α · ε.\nEqns. 8 and 9 both have a positive logsumexp gradient (the second term) to push up the marginal\nprobability pθ(ˆx). As for the third term, PGD starts from a data point (x, y) such that it requires\nthe repulsive gradient to be away from the original data point and do the exploration in a local\n1Note that we omit the projection operation and the gradient re-normalization steps.\n3\nPublished as a conference paper at ICLR 2022\nregion. Langevin dynamics instead starts from a random noise and an additive noise ε is injected\nfor exploration.\nComparing PGD and Langevin. Following the above analysis, the maximization process in AT\ncan be seen as a (biased) sampling method that draws samples from the corresponding probabilistic\nmodel pθ(ˆx). Compared to Langevin dynamics, PGD imposes speciﬁc inductive bias for sampling.\nWith the additional repulsive gradient and ε-ball constraint, it explicitly encourages the samples to be\nmisclassiﬁed around the original data points. In practice, adversarial training with such adversarial\nexamples is generally more stable than training JEM with Langevin samples, which indicates that\nPGD attack is a competitive alternative for the negative sampling method for JEM training.\n4.2\nMINIMIZATION PROCESS\nTo begin with, the gradient of the joint log likelihood for P-CEM can be written as follows:\n∇θEpd(x,y) log pθ(x, y)\n=Epd(x,y)∇θfθ(x, y)−Epθ(ˆx,ˆy)∇θfθ(ˆx, ˆy)\n=Epd(x,y)∇θfθ(x, y)−Epθ(ˆx)pθ(ˆy|ˆx)∇θfθ(ˆx, ˆy),\n(10)\nwhere (x, y) ∼pd(x, y) denotes the positive data pair, and (ˆx, ˆy) ∼pθ(ˆx, ˆy) denotes the negative\nsample pair. As discussed above, the adversarial examples ˆx generated by the maximization process\ncan be regarded as negative samples, and ˆy ∼pθ(ˆy|ˆx) denotes the predicted label of ˆx. To see how\nthe maximum likelihood training of P-CEM is related to the minimization process of AT, we add an\ninterpolated adversarial pair (ˆx, y) into Eq. 10 and decompose it as the consistency gradient and the\ncontrastive gradient:\n∇θEpd(x,y) log pθ(x, y) = Epd(x,y) N pθ(ˆx,ˆy) [∇θfθ(x, y)−∇θfθ(ˆx, ˆy)]\n=Epd(x,y) N pθ(ˆx,ˆy)\n\u0002\n∇θfθ(x, y)−∇θfθ(ˆx, y)\n|\n{z\n}\nconsistency gradient\n+ ∇θfθ(ˆx, y)−∇θfθ(ˆx, ˆy)\n|\n{z\n}\ncontrastive gradient\n\u0003\n.\n(11)\nNext, we show that the two parts correspond to two effective mechanisms developed in the adver-\nsarial training literature.\nAT loss. As the two sample pairs in the contrastive gradient share the same input ˆx, we can see that\nthe contrastive gradient can be written equivalently as\nEpd(x,y) N pθ(ˆx,ˆy) [∇θfθ(ˆx, y)−∇θfθ(ˆx, ˆy)]\n=Epd(x,y) N pθ(ˆx)\n\u0002\n∇θfθ(ˆx, y) −Epθ(ˆy|ˆx)∇θfθ(ˆx, ˆy)\n\u0003\n=Epd(x,y) N pθ(ˆx)∇θ log pθ(y|ˆx),\n(12)\nwhich is exactly the negative gradient of the robust CE loss (AT loss) in Eq. 7, in other words,\ngradient ascent with the contrastive gradient is equivalent to gradient descent w.r.t. the AT loss.\nRegularization. As for the consistency gradient, original AT (Madry et al., 2018) simply ignores it.\nIts variant TRADES (Zhang et al., 2019) instead proposes the KL regularization KL(p(·|ˆx)∥p(·|x))\nthat regularizes the consistency of the predicted probabilities on all classes, whose optimum implies\nthat p(·|ˆx) = p(·|x) →fθ(x, y) = fθ(ˆx, y).\nComparing AT and JEM training paradigms. The above analysis indicates that the minimization\nobjective of AT is closely related to the maximum likelihood training of JEM (Grathwohl et al.,\n2019). Compared to JEM that decomposes the joint likelihood into an unconditional model pθ(x)\nand a discriminative model pθ(y|x), the decomposition of AT in Eq. 10 instead stabilizes training by\nintroducing an intermediate adversarial pair (ˆx, y) that bridges the positive pair (x, y) and the nega-\ntive pair (ˆx, ˆy). Besides, it can inject the adversarial robustness bias by regularizing the consistency\ngradient. Together with our analysis on the maximization process, we show that AT is a competitive\nalternative for training JEM (a generative model) with more stable training behaviors. That explains\nwhy robust models with AT are also generative.\n4.3\nPROPOSED SUPERVISED ADVERSARIAL SAMPLING ALGORITHMS\nOur interpretation also inspires principled designs of sampling algorithms for robust classiﬁers.\n4\nPublished as a conference paper at ICLR 2022\nTargeted Attack (TA). Previously, to draw samples from a robust classiﬁer, Santurkar et al. (2019)\nutilize targeted attack that optimizes an random initialized input ˆx0 towards a speciﬁc class ˆy:\nˆxn+1 = ˆxn + α∇xn log pθ(ˆy|ˆxn) = ˆxn + α∇xf(ˆxn, ˆy) −α∇ˆxn\n\"\nlog\nK\nX\nk=1\nexp(fθ(ˆxn, k))\n#\n.\n(13)\nCompared to PGD attack in Eq. 8, while pushing ˆx towards ˆy, TA has a negative logsumexp gradient\nthat decreases the marginal probability pθ(ˆx). This could explain why TA is less powerful for\nadversarial attack and is rarely used for adversarial training.\nConditional Sampling (CS). To overcome the drawback of targeted attack, a natural idea would be\ndropping the negative logsumexp gradient. In fact, we can show that this is equivalent to sampling\nfrom the conditional distribution:\npθ(x|ˆy) = exp(fθ(x, ˆy))\nZx|ˆy(θ)\n, Zx|ˆy(θ) =\nZ\nx\nexp(fθ(x, ˆy))dx,\nand its Langevin dynamics takes the form:\nˆxn+1 = xn + α∇ˆxn log pθ(ˆxn|ˆy) +\n√\n2α · ε = ˆxn + α∇ˆxnfθ(ˆxn, ˆy) +\n√\n2α · ε.\n(14)\nSamples drawn this way essentially follow an approximated model distribution, pθ(ˆx, ˆy) ≈\npd(ˆy)pθ(ˆx|ˆy). Thus, CS can be seen as a debiased targeted attack algorithm.\nReinforced Conditional Sampling (RCS). Inspired by the above analysis, we can design a biased\nsampling method that deliberately injects a positive logsumexp gradient:\nˆxn+1 =ˆxn + α∇ˆxnfθ(ˆxn, ˆy) + α∇ˆxn\n\"\nlog\nK\nX\nk=1\nexp(fθ(ˆxn, k))\n#\n+\n√\n2α · ε.\n(15)\nWith our designed bias, RCS will sample towards the target class ˆy by maximizing pθ(ˆx|ˆy) (with the\nfθ(ˆxn, ˆy) term), and at the same time improve the marginal probability pθ(ˆx) (with the logsumexp\nterm). As shown in our experiment, RCS indeed obtains improved sample quality.\n4.4\nDISCUSSION ON STANDARD TRAINING\nIn the above discussion, we have explained why adversarial training is generative from the perspec-\ntive of CEM. In fact, it can also help characterize why classiﬁers with Standard Training (ST) are\nnot generative (i.e., poor sample quality). A key insight is that if we replace the model distribution\npθ(ˆx) with the data distribution pd(x) in Eq. 10, we have\n∇θEpd(x,y) log pθ(x, y) = Epd(x,y)∇θfθ(x, y)−Epθ(ˆx)pθ(ˆy|ˆx)∇θfθ(ˆx, ˆy)\n≈Epd(x,y)∇θfθ(x, y) −Epd(x)pθ(ˆy|x)∇θfθ(x, ˆy) = ∇θEpd(x,y) log pθ(y|x),\n(16)\nwhich is the negative gradient of the CE loss in Eq. 7. Thus, ST is equivalent to training CEM by\nsimply replacing model-based negative samples ˆx ∼pθ(x) with data samples x ∼pd(x). This\napproximation makes ST computationally efﬁcient with good accuracy on natural data, but signiﬁ-\ncantly limits its robustness on adversarial examples (as model-based negative samples). Similarly,\nbecause ST ignores exploring negative samples while training, standard classiﬁers also fail to gen-\nerate realistic samples.\n5\nEXTENSION OF ADVERSARIAL TRAINING TO UNSUPERVISED SCENARIO\nIn this section, we show that with our uniﬁed framework, we can naturally extend the interpretation\ndeveloped for supervised adversarial training to the unsupervised scenario.\n5.1\nUNDERSTANDING UNSUPERVISED STANDARD TRAINING THROUGH CEM\nInfoNCE. Recently, the following InfoNCE loss is widely adopted for unsupervised contrastive\nlearning of data representations (Oord et al., 2018; Chen et al., 2020; He et al., 2020),\nℓNCE(x, x′, {ˆxj}K\nj=1; θ) = −log\nexp(fθ(x, x′))\nPK\ni=j exp(fθ(x, ˆxj))\n,\n(17)\n5\nPublished as a conference paper at ICLR 2022\nwhere fθ(x, ˆx) = gθ(x)⊤gθ(ˆx) calculates the similarity between the representations of the two\ndata samples, x, x′ are generated by two random augmentations (drawn from T ) of the same data\nexample, and {ˆxj}K\nj=1 denotes K independently drawn negative samples. In practice, one of the\nK negative samples is chosen to be the positive sample x′. Therefore, InfoNCE can be seen as an\ninstance-wise K-class cross entropy loss for non-parametric classiﬁcation.\nPerhaps surprisingly, we show that the InfoNCE loss is equivalent to the importance sampling esti-\nmate of our NP-CEM (Eq. 4) by approximating the negative samples from pθ(x) with data samples\nfrom pd(x), as what we have done in standard supervised training (Section 4.4):\nEpd(x,x′)∇θfθ(x, x′) −Epθ(ˆx,ˆx′)∇θfθ (ˆx, ˆx′)\n=Epd(x,x′)∇θfθ(x, x′) −Epθ(ˆx)pd(ˆx′)\nexp(fθ(ˆx, ˆx′))\nEpd(˜x) exp(fθ(ˆx, ˜x))∇θfθ(ˆx, ˆx′)\n≈Epd(x,x′)∇θfθ(x, x′) −Epd(ˆx)pd(ˆx′)\nexp(fθ(ˆx, ˆx′))\nEpd(˜x) exp(fθ(ˆx, ˜x))∇θfθ(ˆx, ˆx′)\n(18)\n=Epd(x,x′)∇θ log\nexp(fθ(x, x′))\nEpd(ˆx′) exp(fθ(x, ˆx′)) ≈1\nN\nN\nX\ni=1\n∇θ log\nexp(fθ(xi, x′\ni))\nPK\nk=1 exp(fθ(xi, ˆx′\nik))\n,\nwhich is exactly the negative gradient of the InfoNCE loss. In the above analysis, for an empirical\nestimate, we draw N positive pairs (xi, x′\ni) ∼pd(x, x′), and for each anchor xi, we further draw K\nnegative samples {ˆx′\nik} independently from pd(ˆx′).\nRemark. As pθ(ˆx, ˆx′) = pθ(ˆx)pθ(ˆx′|ˆx), the negative phase of NP-CEM is supposed to sample ˆx′\nfrom pθ(ˆx′|ˆx), where samples semantically close to the anchor sample ˆx, a.k.a. hard negative sam-\nples, should have high probabilities. However, InfoNCE adopts a non-informative uniform proposal\npd(ˆx′) for importance sampling, which is very sample inefﬁcient because most samples are useless\n(Kalantidis et al., 2020). This observation motivates us to design more efﬁcient sampling scheme for\ncontrastive learning by mining hard negatives. For example, Robinson et al. (2021) directly replace\nthe plain proposal with ˜pθ(ˆx|ˆx′) = exp(βfθ(ˆx, ˆx′))/Zβ(θ) while keeping the reweighing term.\nFrom the perspective of CEM, the temperature β introduces bias that should be treated carefully. In\nall, CEM provides a principled framework to develop efﬁcient contrastive learning algorithms.\n5.2\nPROPOSED UNSUPERVISED ADVERSARIAL TRAINING\nAT is initially designed for supervised learning, where adversarial examples can be clearly deﬁned\nby misclassiﬁcation. However, it remain unclear what is the right way to do Unsupervised Adver-\nsarial Training (UAT) without access to any labels. Previous works (Jiang et al., 2020; Ho & Vas-\nconcelos, 2020; Kim et al., 2020) have carried out UAT with the adversarial InfoNCE loss, which\nworks well but lacks theoretical justiﬁcation. Our uniﬁed CEM framework offers a principled way\nto generalize adversarial training from supervised to unsupervised scenarios.\nMaximization Process. Sampling from pθ(x) can be more difﬁcult than that in supervised scenarios\nbecause it does not admit a closed form for variable x′. Thus, we perform Langevin dynamics with\nK negative samples {ˆx′\nk} drawn from pd(ˆx′),\nˆxn+1 = ˆxn + α∇ˆxn log pθ(ˆxn) +\n√\n2α · ε\n(19)\n≈ˆxn + α∇ˆxn\n\"\nlog 1\nK\nK\nX\nk=1\npθ(ˆxn, ˆx′\nk)\n#\n+\n√\n2α · ε\n= ˆxn + α∇ˆxn\n\"\nlog\nK\nX\nk=1\nexp(fθ(ˆxn, ˆx′\nk))\n#\n+\n√\n2α · ε.\nWhile the PGD attack of the InfoNCE loss (Eq. 31),\nˆxn+1 = ˆxn + α∇ˆxn log\nexp(fθ(ˆxn, x′))\nPK\nk=1 exp(fθ(ˆxn, ˆx′\nk))\n(20)\n=ˆxn + α∇ˆxn\n\"\nlog\nK\nX\nk=1\nexp(fθ(ˆxn, ˆx′\nk))\n#\n−α∇θfθ(ˆxn, x′),\n6\nPublished as a conference paper at ICLR 2022\nresembles the Langevin dynamics as they both share the positive logsumexp gradient that pushes\nup pθ(ˆx), and differs by a repulse negative gradient −fθ(ˆx, x′) away from the anchor x′, which is\na direct analogy of the PGD attack in supervised learning (Section 4.1). Therefore, we believe that\nthe PGD attack of InfoNCE is a proper way to craft adversarial examples by sampling from pθ(x).\nMinimization Process. Following the same routine in Section 4.2, with the adversarial example\nˆx ∼pθ(ˆx), we can insert an interpolated adversarial pair (ˆx, x′) and decompose the gradient of\nNP-CEM into the consistency gradient and the contrastive gradient,\n∇θEpd(x,x′) log pθ(x, x′) = Epd(x,x′) N pθ(ˆx,ˆx′) [∇θfθ(x, x′)−∇θfθ(ˆx, x′)]\n=Epd(x,x′) N pθ(ˆx,ˆx′)\n\u0002\n∇θfθ(x, x′)−∇θfθ(ˆx, x′)\n|\n{z\n}\nconsistency gradient\n+ ∇θfθ(ˆx, x′)−∇θfθ(ˆx, ˆx′)\n|\n{z\n}\ncontrastive gradient\n\u0003\n.\n(21)\nIn this way, we can directly develop the unsupervised analogy of AT loss and regularization\n(Sec. 4.2). Following Eq. 30, it is easy to see that the contrastive gradient is equivalent to the gradi-\nent of the Adversarial InfoNCE loss utilized in previous work (Jiang et al., 2020; Ho & Vasconcelos,\n2020; Kim et al., 2020) with adversarial example ˆx,\nEpd(x,x′) N pθ(ˆx,ˆx′) [∇θfθ(ˆx, x′)−∇θfθ(ˆx, ˆx′)]\n=Epd(x,x′) N pθ(ˆx)\n\u0014\n∇θfθ(ˆx, x′) −Epθ(ˆx′)pθ(ˆx)\npθ(ˆx|ˆx′)\npθ(ˆx) ∇θfθ(ˆx, ˆx′)\n\u0015\n=Epd(x,x′) N pθ(ˆx)\n\u0014\n∇θfθ(ˆx, x′) −Epθ(ˆx′)\npθ(ˆx|ˆx′)\npθ(ˆx) ∇θfθ(ˆx, ˆx′)\n\u0015\n≈1\nN\nN\nX\ni=1\n∇θ log\nexp(fθ(ˆxi, x′\ni))\nPK\nk=1 exp(fθ(ˆxi, ˆx′\nik))\n,\n(22)\nwhere {ˆx′\nik} denotes the adversarial negative samples from pθ(ˆx′).\n5.3\nPROPOSED UNSUPERVISED ADVERSARIAL SAMPLING\nIn supervised learning, a natural method to draw a sample ˆx from a robust classiﬁer is to maximize\nits conditional probability w.r.t. a given class ˆy ∼pd(ˆy), i.e., maxˆx p(ˆy|ˆx), by targeted attack\n(Santurkar et al., 2019). However, in the unsupervised scenarios, we do not have access to labels,\nand this approach is not applicable anymore. Meanwhile, Langevin dynamics is also not directly\napplicable (Eq. 19) because it requires access to real data samples.\nMaxEnt. Nevertheless, we still ﬁnd an effective algorithm for drawing samples from an unsuper-\nvised robust model. We ﬁrst initialize a batch of N samples B = {xi}N\ni=1 from a prior distribution\np0(x) (e.g., Gaussian). Next, we update the batch jointly by maximizing the empirical estimate of\nentropy, where we simply take the generated samples at B = {xi}N\ni=1 as samples from pθ(x)\nH(pθ) = −Epθ(x) log pθ(x) ≈−1\nN\nN\nX\ni=1\npθ(xi) ≈−1\nN\nN\nX\ni=1\nlog 1\nN\nN\nX\nj=1\nexp(fθ(xi, xj)) + log Z(θ). (23)\nSpeciﬁcally, we update each sample xi by maximizing the empirical entropy (named MaxEnt)\nx′\ni = xi + α∇xiH(pθ) +\n√\n2α · ε ≈xi −α∇xi\nN\nX\ni=1\nlog\nN\nX\nj=1\nexp(fθ(xi, xj)) +\n√\n2α · ε.\n(24)\nAs a result, the generated samples {xi}N\ni=1 are encouraged to distribute uniformly in the feature\nspace with maximum entropy, and thus cover the overall model distribution with diverse semantics.\n6\nEXPERIMENTS\nIn this section, we evaluate the adversarial sampling methods derived from our CEM framework,\nshowing that they can bring signiﬁcant improvement to the sample quality of previous work. Be-\nsides, in Appendix A, we also conduct a range of experiments on adversarial robustness to verify\nour probabilistic understandings of AT. We show adversarial training objectives derived from our\n7\nPublished as a conference paper at ICLR 2022\nTable 1: Inception Scores (IS) and Fr´echet Inception Distance (FID) of different generative models.\nResults marked with ⋆are taken from Shmelkov et al. (2018).\nMethod\nIS (↑)\nFID (↓)\nAuto-regressive\nPixelCNN++⋆(Salimans et al., 2017)\n5.36\n119.5\nGAN-based\nDCGAN⋆(Radford et al., 2016)\n6.69\n35.6\nWGAN-GP (Gulrajani et al., 2017)\n7.86\n36.4\nPresGAN (Dieng et al., 2019)\n-\n52.2\nStyleGAN2-ADA (Karras et al., 2020)\n10.02\n-\nScore-based\nNCSN (Song & Ermon, 2019)\n8.87\n25.32\nDDPM (Ho et al., 2020)\n9.46\n3.17\nNCSN++ (Song et al., 2020)\n9.89\n2.20\nEBM-based\nJEM (Grathwohl et al., 2019)\n8.76\n38.4\nDRL (Gao et al., 2021)\n8.58\n9.60\nAT-based\nTA (Santurkar et al., 2019) (w/ ResNet50)\n7.5\n-\nSupervised CEM (w/ ResNet50)\n9.80\n55.91\nUnsupervised CEM (w/ ResNet18) (ours)\n8.68\n36.4\nUnsupervised CEM (w/ ResNet50) (ours)\n9.61\n40.25\nCEM can indeed signiﬁcantly improve the performance of AT in both supervised and unsupervised\nscenarios, which helps justify our interpretations and our framework.\nModels. For supervised robust models, we adopt the same pretrained ResNet50 checkpoint2 on\nCIFAR-10 as Santurkar et al. (2019) for a fair comparison. The model is adversarially trained with\nℓ2-norm PGD attack with random start, maximal perturbation norm 0.5, step size 0.1 and 7 steps. As\nfor the unsupervised case, we are the ﬁrst to consider sampling from unsupervised robust models.\nWe train ResNet18 and ResNet50 (He et al., 2016) following the setup of an existing unsupervised\nadversarial training method ACL (Jiang et al., 2020). The training attack is kept the same as that of\nthe supervised case for a fair comparison. More details are provided in Appendix.\nSampling protocol. In practice, our adversarial sampling methods take the following general form\nas a mixture of the PGD and Langevin dynamics,\nxn+1 = Π∥xn−x0∥2≤β [xn + αgk + ηεk] , x0 = δ, εk ∼N(x′ero, 1), k = 0, 1, . . . , K,\nwhere gk is the update gradient, εk is the diffusion noise, ΠS is the projector operator, and δ is the\n(conditional) initial seeds drawn from the multivariate normal distribution whose mean and covari-\nance are calculated from the CIFAR-10 test set following Santurkar et al. (2019). We evaluate the\nsample quality quantitatively with Inception Score (IS) (Salimans et al., 2016) and Fr´echet Inception\nDistance (FID) (Heusel et al., 2017). More details can be found in in Appendix C.1.\nComparison with other generative models. In Table 1, we compare the sample quality of adver-\nsarial sampling methods with different kinds of generative models. We analyze the results in terms\nof the following aspects:\n• Our adversarial sampling methods outperform many deep generative models like Pixel-\nCNN++, WGAN-GP and PresGAN, and obtain state-of-the-art Inception scores on par\nwith StyleGAN2 (Karras et al., 2020).\n• Comparing our AT-based methods with previous methods for training EBMs (Grathwohl\net al., 2019; Gao et al., 2021), we see that it obtains state-of-the-art Inception scores among\n2We\ndownload\nthe\ncheckpoint\nfrom\nthe\nrepository\nhttps://github.com/MadryLab/\nrobustness_applications.\n8\nPublished as a conference paper at ICLR 2022\nthe EBM-based methods. Remarkably, our unsupervised CEM with ResNet18 obtains\nboth better IS and FID scores than the original JEM, which adopts WideResNet-28-10\n(Zagoruyko & Komodakis, 2016) with even more parameters.\n• Compared with previous AT-based method (Santurkar et al., 2019), our CEM-based meth-\nods also improve IS by a large margin (even with the unsupervised ResNet18). Remarkably,\nthe supervised and unsupervised methods obtain similar sample quality, and the supervised\nmethods are better (higher) at IS while the unsupervised methods are better (lower) at FID.\n• We obtain similar Inception scores as state-of-the-art score-based models like NCSN++,\nwhile fail to match their FID scores. Nevertheless, a signiﬁcant drawback of these methods\nis that they typically require more than 1,000 steps to draw a sample, while ours only\nrequire less than 50 steps.\nTable 2: Inception Scores (IS) and Fr´echet Incep-\ntion Distance (FID) of different sampling methods\nfor adversarially robust models. Cond: conditional.\nUncond: unconditional.\nTraining\nSampling\nMethod\nIS (↑)\nFID (↓)\nSupervised\nCond\nTA\n9.26\n56.72\nLangevin\n9.65\n63.34\nCS\n9.77\n56.26\nRCS\n9.80\n55.91\nUnsupervised\n(w/ ResNet18)\nUncond\nPGD\n5.35\n74.27\nMaxEnt\n8.24\n41.80\nCond\nPGD\n5.85\n68.54\nMaxEnt\n8.68\n36.44\nUnsupervised\n(w/ ResNet50)\nUncond\nPGD\n5.24\n141.54\nMaxEnt\n9.57\n44.86\nCond\nPGD\n5.37\n137.68\nMaxEnt\n9.61\n40.25\nSeed\nTA\nLangevin\nCS\nPGD\nMaxEnt\nPGD\nMaxEnt\nFigure 1:\nFour groups of random sam-\nples (top to bottom):\ninitial, supervised\n(ResNet50), unsupervised (ResNet18), unsu-\npervised (ResNet50).\nComparison among adversarial sampling methods. In Table 2, we further compare the sample\nquality of different adversarial sampling methods discussed in Sections 4.3 & 5.3. For supervised\nmodels, we can see that indeed TA obtains the lowest IS, while CS can signiﬁcantly reﬁne the sample\nquality, and RCS can further improve the sample quality by the injected bias. For unsupervised\nmodels, we can see that MaxEnt outperforms PGD consistently by a large margin. In particular,\nconditional sampling initialized with class-wise noise can improve a little on the sample quality\ncompared to unconditional sampling. The average visual sample quality in Figure 1 is roughly\nconsistent with the quantitative results.\nThe mismatch between IS and FID. A notable issue of adversarial sampling methods is the mis-\nmatch between the IS and FID scores. For example, in Table 1, DDPM and our unsupervised CEM\n(w/ ResNet50) have similar Inception scores, but the FID of DDPM (2.20) is signiﬁcantly smaller\nthan ours (40.25), a phenomenon also widely observed in previous methods (Santurkar et al., 2019;\nGrathwohl et al., 2019; Song & Ermon, 2019). Through a visual inspection of the samples in Figure\n1, we can see that the samples have realistic global structure, but as for the local structure, we can\nﬁnd some common artifacts, which could be the reason why it has a relatively large distance (FID)\nto the real data.\n7\nCONCLUSION\nIn this paper, we proposed a uniﬁed probabilistic framework, named Contrastive Energy-based\nModel (CEM), which not only explains the generative ability of adversarial training, but also pro-\nvides a uniﬁed perspective of adversarial training and sampling in both supervised and unsupervised\nparadigms. Extensive experiments show that adversarial sampling methods derived from our frame-\nwork indeed demonstrate better sample quality than state-of-the-art methods.\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGEMENT\nYisen Wang is partially supported by the National Natural Science Foundation of China under Grant\n62006153, Project 2020BD006 supported by PKU-Baidu Fund, and Open Research Projects of\nZhejiang Lab (No. 2022RC0AB05). Jiansheng Yang is supported by the National Science Founda-\ntion of China under Grant No. 11961141007. Zhouchen Lin is supported by the NSF China (No.\n61731018), NSFC Tianyuan Fund for Mathematics (No. 12026606), Project 2020BD006 supported\nby PKU-Baidu Fund, and Qualcomm.\nREFERENCES\nYang Bai, Xin Yan, Yong Jiang, Shu-Tao Xia, and Yisen Wang. Clustering effect of adversarial\nrobust models. In NeurIPS, 2021.\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity\nnatural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. ICML, 2020.\nAdji B Dieng, Francisco JR Ruiz, David M Blei, and Michalis K Titsias. Prescribed generative\nadversarial networks. arXiv preprint arXiv:1910.04302, 2019.\nLogan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Alek-\nsander Madry.\nAdversarial robustness as a prior for learned representations.\narXiv preprint\narXiv:1906.00945, 2019.\nRuiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P Kingma. Learning energy-based\nmodels by diffusion recovery likelihood. ICLR, 2021.\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\nexamples. ICLR, 2015.\nWill Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,\nand Kevin Swersky. Your classiﬁer is secretly an energy based model and you should treat it like\none. In ICLR, 2019.\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-\nproved training of Wasserstein GANs. NeurIPS, 2017.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for\nunsupervised visual representation learning. In CVPR, 2020.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGANs trained by a two time-scale update rule converge to a local Nash equilibrium. NeurIPS,\n2017.\nChih-Hui Ho and Nuno Vasconcelos. Contrastive learning with adversarial examples. NeurIPS,\n2020.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS,\n2020.\nZiyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang. Robust pre-training by adversarial\ncontrastive learning. NeurIPS, 2020.\nYannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard\nnegative mixing for contrastive learning. NeurIPS, 2020.\nHarini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint\narXiv:1803.06373, 2018.\n10\nPublished as a conference paper at ICLR 2022\nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training\ngenerative adversarial networks with limited data. NeurIPS, 2020.\nSimran Kaur, Jeremy Cohen, and Zachary C Lipton. Are perceptually-aligned gradients a general\nproperty of robust classiﬁers? arXiv preprint arXiv:1910.08640, 2019.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan.\nSupervised contrastive learning.\narXiv preprint\narXiv:2004.11362, 2020.\nMinseon Kim, Jihoon Tack, and Sung Ju Hwang. Adversarial self-supervised contrastive learning.\nNeurIPS, 2020.\nDiederik P Kingma and Max Welling. Auto-encoding variational Bayes. ICLR, 2014.\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. In ICLR, 2018.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\nBen Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On varia-\ntional bounds of mutual information. ICML, 2019.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. ICLR, 2016.\nJoshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with\nhard negative samples. ICLR, 2021.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training GANs. NeurIPS, 2016.\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the\nPixelCNN with discretized logistic mixture likelihood and other modiﬁcations. arXiv preprint\narXiv:1701.05517, 2017.\nShibani Santurkar, Andrew Ilyas, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander\nMadry. Image synthesis with a single (robust) classiﬁer. In NeurIPS, 2019.\nKonstantin Shmelkov, Cordelia Schmid, and Karteek Alahari. How good is my GAN? ECCV, 2018.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\narXiv preprint arXiv:1907.05600, 2019.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020.\nMichael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual\ninformation maximization for representation learning. ICLR, 2020.\nYisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu.\nOn the\nconvergence and robustness of adversarial training. In ICML, 2019.\nSergey Zagoruyko and Nikos Komodakis.\nWide residual networks.\narXiv preprint\narXiv:1605.07146, 2016.\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.\nTheoretically principled trade-off between robustness and accuracy. ICML, 2019.\n11\nPublished as a conference paper at ICLR 2022\nA\nEVALUATING ADVERSARIAL ROBUSTNESS\nIn Section 6, we have shown that the new adversarial sampling algorithms derived from our CEM\nframework indeed obtain improved sample quality, which helps justiﬁes our interpretation of AT\nfrom a generative aspect. In this section, we further take a complementary way to verify our inter-\npretation by studying its discriminative part. In particular, we develop new variants of AT regular-\nization and verify their effectiveness on improving the adversarial robustness of AT. As CEM has\nboth supervised and unsupervised variants, we develop AT variants for each scenario respectively\nwhile following the same spirit.\nA.1\nSUPERVISED ADVERSARIAL TRAINING\nA.1.1\nPROPOSED METHOD\nIn Section 4.2, we have mentioned that for the consistency gradient, original AT (Madry et al., 2018)\nsimply ignores it. Denoting x and ˆx as the natural and adversarial inputs, the state-of-the-art AT\nvariant, TRADES (Zhang et al., 2019), instead adopts the KL regularization KL(p(·|ˆx)∥p(·|x)) that\nexplicitly regularizes the consistency of the predicted probabilities on all classes, whose optimum\nimplies that p(·|ˆx) = p(·|x) →fθ(x, y) = fθ(ˆx, y).\nInspired by this discussion, alternatively, we can directly regularize the consistency gradient to zero.\nWe achieve this with the following Consistency Regularization (CR) with least square loss:\nLCR(θ) = Epd(x,y) N pθ(x) (fθ(x, y) −fθ(ˆx, y))2 .\n(25)\nWe note the our proposed AT+CR differs to ALP (Kannan et al., 2018) as we only minimizes the\ngap between the logits of the label class fθ(x, y). ALP was shown to be ineffective for improving\nAT, while in the experiments below, we show that our AT+CR objective indeed achieves comparable\n(even slightly better) results as TRADES.\nA.1.2\nEMPIRICAL EVALUATION\nExperimental setup.\nFollowing the conventions, we compare different AT methods with Pre-\nactivated ResNet18 (He et al., 2016) and WideResNet34 (Zagoruyko & Komodakis, 2016) on\nCIFAR-10. The maximum perturbation is bounded by ε = 8/255 under ℓ∞norm. The training\nattack is PGD10 (Madry et al., 2018) with random start and step size ε/4. The test attack is PGD20\nwith random start and step size ε/4. We evaluate both the ﬁnal epoch and the early stopped epoch\nwith the best robust accuracy.\nTable 3: Robustness (accuracy (%) on adversarial attacks) of supervised adversarial training methods\non CIFAR-10. R18: ResNet18. W34: WideResNet34.\nModel\nTraining\nNatural Acc (%)\nAdversarial Acc (%)\nResNet18\nAT (Madry et al., 2018)\n83.7\n52.2\nTRADES (Zhang et al., 2019)\n82.5\n54.3\nAT+CR (ours)\n81.5\n55.2\nWideResNet34\nAT (Madry et al., 2018)\n86.8\n53.6\nTRADES (Zhang et al., 2019)\n83.4\n57.0\nAT+CR (ours)\n86.6\n57.0\nResult analysis. From Table 3, we can see that the AT+CR objective derived from our framework\nindeed enjoy much better robustness than vanilla AT. Compared to the state-of-the-art AT vari-\nant TRADES, we can see that AT+CR is comparable, and sometimes slightly better at robustness.\nWhen the two have similar robustness (for WideResNet34), AT+CR obtains better natural accuracy\nthan TRADES (86.6 v.s. 83.4). These results empirically justify that our interpretation of AT and\nTRADES from a probabilistic perspective.\nA.2\nUNSUPERVISED ADVERSARIAL TRAINING\nSimilarly, we can develop the same regularization technique for unsupervised adversarial training\nthrough a uniﬁed perspective of supervised and unsupervised AT offered by our CEM.\n12\nPublished as a conference paper at ICLR 2022\nA.2.1\nPROPOSED METHOD\nIn Section 5.2, we have developed a principled unsupervised adversarial training routine by an anal-\nogy with the supervised AT (Section 4.2). Besides, we can also consider an unsupervised version of\nthe consistency regularization above, namely, the Unsupervised Consistency Regularization (UCR),\nLUCR(θ) = Epd(x,x′) N pθ(ˆx)∥fθ(x, x′) −fθ(ˆx, x′)∥2,\n(26)\nwhich encourages the consistency between the similarity between the natural pair (x, x′) and the\nadversarial pair (ˆx, x′).\nA.2.2\nEMPIRICAL EVALUATION\nExperimental setup. Among the many variants of contrastive learning, we adopt SimCLR as our\nbaseline method and take the recently proposed ACL (Jiang et al., 2020) as our Unsupervised Ad-\nversarial Training (UAT) following the same default setup as in Section C.1. The training attack\nconﬁguration is kept the same as the supervised case, while after training, we freeze the encoder and\nﬁne-tune a linear classiﬁcation layer (standard training) on top with labeled data for evaluating the\nlearned features. In particular, we evaluate the composed model on the test data with two different\nattack methods, FGSM (Goodfellow et al., 2015) (one-step attack) and PGD20 (multi-step attack),\nboth with ε = 8/255, and report their natural and adversarial accuracy, respectively.\nTable 4: Robustness (accuracy (%) on adversarial attacks) of unsupervised contrastive learning\nmethods on CIFAR-10 with ResNet-18 backbone and two different attack methods: FGSM (Good-\nfellow et al., 2015) and PGD (Madry et al., 2018).\nTraining\nNatural Acc (%)\nAdversarial Acc (%)\nFGSM\nPGD20\nStandard Training (Chen et al., 2020)\n91.5\n25.6\n0.8\nUAT (Jiang et al., 2020)\n66.6\n26.2\n21.4\nUAT+UCR (ours)\n72.0\n30.7\n24.6\nResult analysis. As shown in Table 4, features learned by UAT is indeed more robust than standard\ntraining, e.g., 0.8% to 21.4% under PGD attack. Moreover, with our proposed UCR regularizer\n(Eq. 26), we not only effectively improve both natural accuracy (66.6% to 72.0%), and also improve\nadversarial robustness: 26.2% to 30.7% under FGSM attack and 21.4% to 24.6% under PGD attack.\nThis also helps justify the connection between contrastive learning and our CEM.\nA.3\nCONCLUDING REMARK\nWith the above experiments on adversarial robustness, we empirically verify that our framework\ncould serve as a valid probabilistic understanding of adversarial training and can be used to develop\nnew effective adversarial training objectives.\nB\nOMITTED TECHNICAL DETAILS\nFor a concise presentation, we have omitted several technical details in the main text. Here we\npresent a complete description of the derivation process.\nB.1\nLOG LIKELIHOOD GRADIENT OF EBM\nIn Section 3, we have introduced Energy-based Models (EBM) and the gradient of their log likeli-\nhood. We now show how it can be derived out.\nFor a EBM in the following form,\npθ(x) = exp (−Eθ(x))\nZ(θ)\n,\n(27)\n13\nPublished as a conference paper at ICLR 2022\nthe gradient of the log likelihood can be derived as follows:\n∇θEpd(x) log pθ(x)\n(28)\n= −Epd(x)∇θEθ(x) −∇θ log Z(θ)\n=Epd(x)∇θEθ(x) + ∇θ\nR\nx exp(−Eθ(x))\nZ(θ)\n= −Epd(x)∇θEθ(x) +\nZ\nˆx\nexp(−Eθ(ˆx))\nZ(θ)\n∇θEθ(ˆx)\n= −Epd(x)∇θEθ(x)\n|\n{z\n}\npositive phase\n+ Epθ(ˆx)∇θEθ(ˆx)\n|\n{z\n}\nnegative phase\n.\nB.2\nCONNECTION BETWEEN STANDARD TRAINING AND JEM\nIn Section 4.4, we have claimed that if we replace the model distribution pθ(ˆx) with the data distri-\nbution pd(x) in Eqn. 10, the log likelihood gradient of JEM is equivalent to the negative gradient of\nthe CE loss. Here we give a detailed proof as follows:\n∇θEpd(x,y) log pθ(x, y)\n=Epd(x,y)∇θfθ(x, y)−Epθ(ˆx)pθ(ˆy|ˆx)∇θfθ(ˆx, ˆy)\n≈Epd(x,y)∇θfθ(x, y) −Epd(x)pθ(ˆy|x)∇θfθ(x, ˆy)\n=Epd(x,y)\n\u0002\n∇θfθ(x, y) −Epθ(ˆy|x)∇θfθ(x, ˆy)\n\u0003\n=Epd(x,y)\n\"\n∇θfθ(x, y) −\nK\nX\nk=1\npθ(k|x)∇θfθ(x, k)\n#\n=Epd(x,y)\n\"\n∇θfθ(x, y) −\nK\nX\nk=1\nexp(fθ(x, k))∇θfθ(x, k)\nPK\nj=1 exp(fθ(x, j))\n#\n=Epd(x,y)\n\"\n∇θfθ(x, y) −\nK\nX\nk=1\n∇θ exp(fθ(x, k))\nPK\nj=1 exp(fθ(x, j))\n#\n=Epd(x,y)\n\"\n∇θfθ(x, y) −∇θ\nPK\nk=1 exp(fθ(x, k))\nPK\nj=1 exp(fθ(x, j))\n#\n=Epd(x,y)\n\"\n∇θfθ(x, y) −∇θ log\nK\nX\nk=1\nexp(fθ(x, k))\n#\n=Epd(x,y)∇θ log\nexp(fθ(x, y))\nPK\nk=1 exp(fθ(x, k)\n=∇θEpd(x,y) log pθ(y|x).\n(29)\nB.3\nEQUIVALENCE BETWEEN AT LOSS AND CONTRASTIVE GRADIENT IN SUPERVISED\nLEARNING\nIn Section 4.3, we have claimed that the contrastive gradient equals to the negative gradient of the\nrobust CE loss (AT loss) following the same deduction in Eqn. 29,\nEpd(x,y) N pθ(ˆx,ˆy) [∇θfθ(ˆx, y)−∇θfθ(ˆx, ˆy)]\n=Epd(x,y) N pθ(ˆx)\n\u0002\n∇θfθ(ˆx, y) −Epθ(ˆy|ˆx)∇θfθ(ˆx, ˆy)\n\u0003\n=Epd(x,y)∇θ log\nexp(fθ(ˆx, y))\nPK\nk=1 exp(fθ(ˆx, k)\n=Epd(x,y) N pθ(ˆx)∇θ log pθ(y|ˆx),\n(30)\nwhich is exactly the negative gradient of the canonical AT loss (Madry et al., 2018).\n14\nPublished as a conference paper at ICLR 2022\nB.4\nEQUIVALENCE BETWEEN INFONCE LOSS AND NON-PARAMETRIC CEM\nIn Section 6.1, we have claimed that the the log likelihood gradient of NP-CEM equals to exactly\nthe negative gradient of the InfoNCE loss when we approximate pθ(ˆx) with pd(ˆx). The derivation\nis presented as follows:\nEpd(x,x′)∇θfθ(x, x′) −Epθ(ˆx,ˆx′)∇θfθ\n\u0000ˆx, ˆx′\u0001\n=Epd(x,x′)∇θfθ(x, x′) −Epθ(x)pθ(ˆx|ˆx′)∇θfθ\n\u0000ˆx, ˆx′\u0001\n=Epd(x,x′)∇θfθ(x, x′) −Epθ(x)\npθ(ˆx, ˆx′)\npθ(ˆx′) ∇θfθ\n\u0000ˆx, ˆx′\u0001\n=Epd(x,x′)∇θfθ(x, x′) −Epθ(ˆx)\nZ\nˆx\nexp(fθ(ˆx, ˆx′)\nR\n˜x exp(fθ(ˆx, ˜x))∇θfθ(ˆx, ˆx′)\n=Epd(x,x′)∇θfθ(x, x′) −Epθ(ˆx)\nZ\nˆx\npd(ˆx) exp(fθ(ˆx, ˆx′)\nR\n˜x pd(˜x) exp(fθ(ˆx, ˜x))∇θfθ(ˆx, ˆx′)\n(as pd(ˆx) = pd(˜x) =\n1\n|X|)\n=Epd(x,x′)∇θfθ(x, x′) −Epθ(ˆx)pd(ˆx′)\nexp(fθ(ˆx, ˆx′))\nEpd(˜x) exp(fθ(ˆx, ˜x))∇θfθ(ˆx, ˆx′)\n≈Epd(x,x′)∇θfθ(x, x′) −Epd(ˆx)pd(ˆx′)\nexp(fθ(ˆx, ˆx′))\nEpd(˜x) exp(fθ(ˆx, ˜x))∇θfθ(ˆx, ˆx′)\n(31)\n=Epd(x,x′)∇θfθ(x, x′) −Epd(ˆx)∇θ log Epd(ˆx′) exp(fθ(ˆx, ˆx′))\n=Epd(x,x′)\n\u0002\n∇θfθ(x, x′) −∇θ log Epd(ˆx′) exp(fθ(ˆx, ˆx′))\n\u0003\n(merge pd(x) with pd(ˆx))\n=Epd(x,x′)∇θ log\nexp(fθ(x, x′))\nEpd(ˆx′) exp(fθ(x, ˆx′)) ≈1\nN\nN\nX\ni=1\n∇θ log\nexp(fθ(xi, x′\ni))\nPK\nk=1 exp(fθ(xi, ˆx′\nik))\n,\nwhere (x, x′\ni)’s are positive samples from pd(x, x′) and ˆx′\nik’s are negative samples independently\ndrawn from pd(ˆx′).\nB.5\nEQUIVALENCE BETWEEN ADVERSARIAL INFONCE AND NP-CEM\nIn Section 6.2, we have developed the unsupervised analogy of AT loss and regularization. In\nparticular, we have claimed that contrastive gradient is equivalent to the gradient of the Adversarial\nInfoNCE loss (i.e., the InfoNCE loss of the adversarial example ˆx) utilized in previous work (Jiang\net al., 2020; Ho & Vasconcelos, 2020; Kim et al., 2020). It can be derived following Eqn. 31:\nEpd(x,x′) N pθ(ˆx,ˆx′)\n\u0002\n∇θfθ(ˆx, x′)−∇θfθ(ˆx, ˆx′)\n\u0003\n=Epd(x,x′) N pθ(ˆx)\n\u0002\n∇θfθ(ˆx, x′) −Epθ(ˆx′|ˆx)∇θfθ(ˆx, ˆx′)\n\u0003\n=Epd(x,x′) N pθ(ˆx)\n\u0014\n∇θfθ(ˆx, x′) −Epd(ˆx′)\nexp(fθ(ˆx, ˆx′))\nEpd(˜x) exp(fθ(ˆx, ˆx′))∇θfθ(ˆx, ˆx′)\n\u0015\n=Epd(x,x′) N pθ(ˆx)∇θ log\nexp(fθ(ˆx, x′))\nEpd(ˆx′) exp(fθ(ˆx, ˆx′)\n≈1\nN\nN\nX\ni=1\n∇θ log\nexp(fθ(ˆxi, x′\ni))\nPK\nk=1 exp(fθ(ˆxi, ˆx′\nik))\n,\n(32)\nwhere (xi, x′\ni) are positive samples drawn from pd(x, x′), ˆxi’s are adversarial samples drawn from\npθ(ˆx), and ˆx′\nik’s are negative samples independently drawn from pd(ˆx′).\nC\nMORE DETAILS AND RESULTS ON ADVERSARIAL SAMPLING\nC.1\nDETAILED EXPERIMENTAL SETUP\nSupervised Adversarial Sampling. For supervised robust models, we adopt the same pretrained\nResNet50 checkpoint on CIFAR-10 as Santurkar et al. (2019) 3 for a fair comparison. As described\n3We\ndownload\nthe\ncheckpoint\nfrom\nthe\nrepository\nhttps://github.com/MadryLab/\nrobustness_applications.\n15\nPublished as a conference paper at ICLR 2022\n10\n20\n30\n40\n50\nNumber of sampling steps\n9.76\n9.78\n9.80\n9.82\n9.84\n9.86\n9.88\n9.90\nInception score\n0\n0.001\n0.01\n0.1\n1.0\nNoise ratio\n1.0\n1.5\n2.0\n2.5\n3.0\n. . .\n9.0\n9.5\nInception score\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMaximal\n2 norm\n4\n5\n6\n7\n8\n9\n10\nInception score\nFigure 2: Algorithmic analysis of our proposed supervised adversarial sampling algorithm (RCS).\nLeft: Inception score with increasing sampling steps N. Middle: Inception score with increasing\ndiffusion noise scale. Right: Inception score with increasing ℓ2-norm bound β.\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nNumber of sampling steps\n9.50\n9.55\n9.60\n9.65\n9.70\n9.75\nInception score\n0\n0.001\n0.01\n0.1\n1.0\n3.7\nNoise ratio\n1.0\n1.5\n2.0\n. . .\n7.0\n7.5\n8.0\n8.5\n9.0\n9.5\n10.0\nInception score\n7\n8\n9\n10\n11\n12\n13\n14\nMaximal\n2 norm\n6.0\n6.5\n7.0\n7.5\n8.0\n8.5\n9.0\n9.5\n10.0\nInception score\nFigure 3: Algorithmic analysis of our proposed unsupervised adversarial sampling algorithm (Max-\nEnt). Left: Inception score with increasing sampling steps N. Middle: Inception score with increas-\ning diffusion noise scale. Right: Inception score with increasing ℓ2-norm bound β.\nin Santurkar et al. (2019), the ResNet-50 model is adversarially trained for 350 epochs with learning\nrate 0.01 and batch size 256. The learning rate is dropped by 10 at epoch 150 and 200. The training\nattack is ℓ2-norm PGD attack with random start, maximal perturbation norm 0.5, step size 0.1 and 7\nsteps.\nUnsupervised Adversarial Sampling. As far as we know, we are the ﬁrst to consider sampling\nfrom unsupervised robust models. Therefore, to obtain an unsupervised robust model for sampling,\nwe adopt ACL (Jiang et al., 2020) as the baseline method and follow their default hyper-parameters\n4 to train it. The ofﬁcial implementation of ACL is built upon the SimCLR (Chen et al., 2020)\nframework for contrastive learning, while they choose a speciﬁc hyper-parameter conﬁguration for\nadversarial training. In particular, they choose 512 for batch size and train for 1000 epochs with\nResNet-18 backbone (He et al., 2016). The base learning rate is 1.0, where they use a linear warm\nup strategy for the ﬁrst 10 epochs, and apply cosine annealing scheduler after that. The training\nattack is kept the same as that of the supervised case for a fair comparison. For ResNet-18, we\nadopt the ACL(A2A) setting, which adopts the normal ResNet with only one Batch Normalization\nmodule. Instead, for ResNet-50, we notice that the ACL(A2S) setting yields slightly better results.\nThe ResNet variants in the ACL(A2S) setting contains two BN modules, where we assign natural\nand adversarial examples to different modules. We refer more details to the original paper (Jiang\net al., 2020).\nEvaluation of sample quality. Note that there are four hyper-parameters in our sampling protocol:\nstep size α, ℓ2-ball size β, noise scale η, and iteration steps K, for which we list our choice in Table\n5. We evaluate sample quality quantitatively w.r.t. the Inception Score (IS) (Salimans et al., 2016)\nand Fr´echet Inception Distance (FID) (Heusel et al., 2017) with 50,000 samples, where the standard\nvariation of IS is around 0.1.\nC.2\nADDITIONAL ANALYSIS\nBesides the results presented in the main text, we also conduct more experiments to analyze the\nbehavior of our proposed adversarial sampling algorithms, both quantitatively and qualitatively. We\nconduct a detailed analysis of our proposed sampling algorithms and present the results of supervised\n4Ofﬁcial code: https://github.com/VITA-Group/Adversarial-Contrastive-Learning.\n16\nPublished as a conference paper at ICLR 2022\nTable 5: Sampling hyper-parameters in each scenario.\nScenario\nModel\nα\nβ\nη\nK\nSupervised\nResNet50\n1\n6\n0.01\n20\nUnsupervised\nResNet18\n7\n∞\n0.0\n10\nResNet50\n7\n∞\n0.0\n50\nadversarial sampling (with RCS) in Figure 2 and the results of unsupervised adversarial sampling\n(with MaxEnt) in Figure 3. Note that in both cases, we adopt the ResNet-50 backbone and use the\ndefault hyper-parameters unless speciﬁed.\nC.2.1\nCHAIN LENGTH\nFrom the left panels of Figure 2 and Figure 3, we can see the two adversarial algorithms both have\na sweet spot of sampling steps N (length of the sampling Markov chains) at around 30 to 40 steps,\nbefore and after which the results will be slightly worse.\nC.2.2\nNOISE RATIO\nIn the proper Langevin dynamics, the scale of the noise is determined by the step size, η =\n√\n2α.\nHowever, in practice, this would lead to a catastrophically degraded sample quality as the noise\ntakes over the gradient information. Therefore, following Song & Ermon (2019) and Grathwohl\net al. (2019), we also anneal the noise ratio η to a smaller value for better sample quality. As shown\nin the middle panels of Figure 2 and Figure 3, the optimal noise ratio is around 0.01 for both cases.\nC.2.3\nMAXIMAL NORM\nAn apparent difference of our adversarial sampling algorithms to the canonical Langevin dynamics\nis that ours have a projection step that limits the distance between the samples and the initial seeds.\nIn the right panels of Figure 2 and Figure 3, we show the impact of the scale of the ℓ2-norm ball\nfor the sample quality. We can see that generally speaking, as the ball grows larger, the samples get\nreﬁned. In the supervised case, the sample quality gets slightly worse with a large norm, which does\nnot happen in the unsupervised case.\nC.2.4\nSAMPLING TRAJECTORY\nAside from the qualitative inspection of the proposed sampling algorithms, we also demonstrate\nthe sampling trajectory of our supervised (RCS) and unsupervised (MaxEnt) adversarial sampling\nmethods in Figure 4 & 5. We can see that the samples get gradually reﬁned in terms both low-level\ntextures and high-level semantics.\n17\nPublished as a conference paper at ICLR 2022\nFigure 4: Sampling trajectory (the ﬁrst 20 steps) of our proposed supervised adversarial sampling\nalgorithm (RCS). Each row represents the reﬁnement progress of a single sample.\nFigure 5: Sampling trajectory (the ﬁrst 20 steps) of our proposed unsupervised adversarial sampling\nalgorithm (MaxEnt). Each row represents the reﬁnement progress of a single sample.\n18\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2022-03-25",
  "updated": "2022-03-25"
}