{
  "id": "http://arxiv.org/abs/2203.13699v1",
  "title": "Unsupervised Image Deraining: Optimization Model Driven Deep CNN",
  "authors": [
    "Changfeng Yu",
    "Yi Chang",
    "Yi Li",
    "Xile Zhao",
    "Luxin Yan"
  ],
  "abstract": "The deep convolutional neural network has achieved significant progress for\nsingle image rain streak removal. However, most of the data-driven learning\nmethods are full-supervised or semi-supervised, unexpectedly suffering from\nsignificant performance drops when dealing with real rain. These data-driven\nlearning methods are representative yet generalize poor for real rain. The\nopposite holds true for the model-driven unsupervised optimization methods. To\novercome these problems, we propose a unified unsupervised learning framework\nwhich inherits the generalization and representation merits for real rain\nremoval. Specifically, we first discover a simple yet important domain\nknowledge that directional rain streak is anisotropic while the natural clean\nimage is isotropic, and formulate the structural discrepancy into the energy\nfunction of the optimization model. Consequently, we design an optimization\nmodel-driven deep CNN in which the unsupervised loss function of the\noptimization model is enforced on the proposed network for better\ngeneralization. In addition, the architecture of the network mimics the main\nrole of the optimization models with better feature representation. On one\nhand, we take advantage of the deep network to improve the representation. On\nthe other hand, we utilize the unsupervised loss of the optimization model for\nbetter generalization. Overall, the unsupervised learning framework achieves\ngood generalization and representation: unsupervised training (loss) with only\na few real rainy images (input) and physical meaning network (architecture).\nExtensive experiments on synthetic and real-world rain datasets show the\nsuperiority of the proposed method.",
  "text": "Unsupervised Image Deraining: Optimization Model Driven\nDeep CNN\nChangfeng Yu*\nHuazhong University of Science and\nTechnology\nWuhan, China\nycf@hust.edu.com\nYi Chang*\nPeng Cheng Laboratory\nShenzhen, China\nowuchangyuo@gmail.com\nYi Li\nHuazhong University of Science and\nTechnology\nWuhan, China\nli_yi@hust.edu.cn\nXile Zhao\nUniversity of Electronic Science and\nTechnology of China\nChengdu, China\nxlzhao122003@163.com\nLuxin Yanâ€ \nHuazhong University of Science and\nTechnology\nWuhan, China\nyanluxin@hust.edu.cn\nABSTRACT\nThe deep convolutional neural network has achieved significant\nprogress for single image rain streak removal. However, most of the\ndata-driven learning methods are full-supervised or semi-supervised,\nunexpectedly suffering from significant performance drop when\ndealing with the real rain. These data-driven learning methods are\nrepresentative yet generalize poor for real rain. The opposite holds\ntrue for the model-driven unsupervised optimization methods. To\novercome these problems, we propose a unified unsupervised learn-\ning framework which inherits the generalization and representation\nmerits for real rain removal. Specifically, we first discover a sim-\nple yet important domain knowledge that directional rain streak\nis anisotropic while the natural clean image is isotropic, and for-\nmulate the structural discrepancy into the energy function of the\noptimization model. Consequently, we design an optimization model\ndriven deep CNN in which the unsupervised loss function of the\noptimization model is enforced on the proposed network for better\ngeneralization. In addition, the architecture of the network mimics\nthe main role of the optimization models with better feature rep-\nresentation. On one hand, we take advantage of the deep network\nto improve the representation. On the other hand, we utilize the\nunsupervised loss of the optimization model for better generaliza-\ntion. Overall, the unsupervised learning framework achieves good\ngeneralization and representation: unsupervised training (loss) with\nonly a few real rainy images (input) and physical meaning network\n(architecture). Extensive experiments on synthetic and real-world\nrain datasets show the superiority of the proposed method.\nKEYWORDS\nDeraining, unsupervised learning, optimization model, CNN.\nACM Reference Format:\nChangfeng Yu, Yi Chang, Yi Li, Xile Zhao, and Luxin Yan. 2021. Un-\nsupervised Image Deraining: Optimization Model Driven Deep CNN. In\nProceedings of the 29th ACM Intâ€™l Conference on Multimedia (MM â€™21), Oct.\n20â€“24, 2021, Virtual Event, China. ACM, New York, NY, USA, 9 pages.\nhttps://doi.org/10.1145/3474085.3475441\n*Both authors contributed equally to this research.\nâ€ Corresponding Author.\n1\nINTRODUCTION\nThe single image rain streak removal [4â€“6, 11, 14, 15, 17â€“21, 24,\n28, 31â€“38, 40, 41, 43, 47] has made significant progress in the past\ndecade, which serves as a pre-processing step for subsequent high-\nlevel computer vision tasks such as detection [27] and segmentation\n[46]. Most of the existing learning base CNN methods are full-\nsupervised [6, 37] or semi-supervised [35, 41, 42], which achieve\nsatisfactory performance for the simulated rain streaks. However,\nthe huge gap between the synthetic and real streaks would inevitably\nresult in obvious performance drop. In this work, the goal is to\nhandle the real rain streaks from an unsupervised perspective.\nIn pre-deep learning era, the optimization methods have achieved\nconsiderable progress in rain streaks removal. The main idea of\noptimization model is to formulate deraining task into an image\ndecomposition framework by decoupling the rain streaks and clean\nimage components which lie on two distinguishable subspaces. Thus,\nthe key of optimization model is to construct an energy function\nand manually design hand-crafted priors for each component. The\ndictionary learning [15, 24], low-rank representation [1, 3], Gaussian\nmixture models (GMMs) [21] have been widely explored for rain\nstreaks removal. The optimization-based methods dig deeply into\ndomain knowledge of the rain streaks, such as spatial sparsity [9,\n15], smoothness [49], non-local similarity [3], directionality [1].\nMoreover, they are usually free from the large-scale training datasets,\nso they can generalize well for real rain streaks. However, these hand-\ncrafted priors are typically based on linear transformation, in which\nthe representation ability is limited, especially for highly complex\nand varied rainy scenes. In addition, the optimization procedure is\nusually slow due to the multiple iterations procedure.\nAlthough the optimization-based methods have achieved promis-\ning deraining results, these hand-crafted priors are less robust to\nhandle the rain streaks with diverse distributions, due to the varied\nangle, location, intensity, density, length, width and so on. In recent\nyears, the deep learning based deraining methods [4â€“6, 11, 14, 17â€“\n20, 28, 31â€“38, 40â€“43, 47] have received tremendous success in\nderaining task due to nonlinear representation ability of CNN. The\npowerful representation enables the CNN to implicitly learn different\ncomplex distribution of the rain streaks. Another advantage of the\nCNN methods is the fast inference time once the network is trained.\narXiv:2203.13699v1  [cs.CV]  25 Mar 2022\nModel-Driven Optimization Methods\nData-Driven Learning Methods\n2012\nDictionary Learning\n(Kang et al. TIP)\n2013\nLow-rank, ICCV\n(Chen et al.)\n2015\nDSC, ICCV\n(Luo et al.)\n2016\nGMM, CVPR\n(Li et al.)\nJBO, ICCV\n(Zhu et al.)\nResidual CNN\n(Fu et al. CVPR)\nJORDER, CVPR\n(Yang et al.)\n2017\nDID, CVPR\n(Zhang et al.)\nSPANet, CVPR\n(Wang et al.)\n2018\nRESCAN \n(Li et al. ECCV)\nSyn2Real, CVPR\n(Yasarla et al.)\nSSIR, CVPR \n(Wei et al.)\n2019\nJRGR, CVPR\n(Ye et al.)\n2021\n2020\nRCDNet, CVPR\n(Wang et al.)\n2017\nTLR, ICCV\n(Chang et al.)\nVRGNet, CVPR\n(Wang et al.)\nUnsupervised\nSupervised\nGeneralization\nRepresentation\nHand-crafted \nLinear\nHierarchy \nNonlinear\nReal rain\nGMM\nJORDER-E\nProposed\nFigure 1: The complementary between the optimization and CNN methods. We show the development of typical rain streak removal\nmethods. The unsupervised model driven-optimization methods generalize well yet with only shallow representation. On the contrary,\nthe full/semi-supervised based CNN deraining methods are representative with poor generalization. In this work, we bridge the gap\nbetween the model-driven and data-driven methods within an end-to-end unsupervised learning framework. Below shows the real\nrain removal results for representative optimization-based GMM [21], learning-based JORDER-E [36], and the proposed method.\nThe key components in conventional CNN are to 1) prepare the\ntraining data pairs; 2) design architecture of the network; 3) define\nloss function for the training purpose.\nMost of the existing CNN-based deraining methods are full-\nsupervised, in which they pay most of their attention to the architec-\nture design of the network such as the multi-stage [20, 28, 32, 36, 37],\nmulti-scale [14, 26, 40, 47], attention [11, 17, 33, 47], so as to better\nimprove the representation for the rain streaks. The full-supervised\nderaining methods heavily rely on the paired clean and rainy im-\nage. The existing full-supervised methods usually construct a rain\nsynthesis model to generate the simulated rainy image. However,\nthere exist a huge gap between the real and synthetic rains. That\nis the main reason why the existing CNN methods have been less\ngeneralized for the real rain streaks. The semi-supervised deraining\nmethods [35, 41, 42] could alleviate the generalization issue to some\nextent by introducing the real rainy image as the additional con-\nstraint. However, the problem still exists since these semi-supervised\nmethods also employ the synthetic rainy image.\nOverall, the model-driven optimization methods have good gener-\nalization endowed by the unsupervised loss yet weak representation\n(plane and linear) ability. On the contrary, the data-driven learn-\ning methods have good representation endowed by the hierarchy\nnonlinear transformation yet poor generalization (supervised loss\non synthetic data) ability. This motivates us to inherit the powerful\nrepresentation of the network and also the good generalization of\noptimization methods simultaneously.\nIn this work, we bridge the gap between the model-driven and\ndata-driven methods within an end-to-end unsupervised learning\nframework. Specifically, we first discover a simple yet important\ndomain knowledge that directional rain streak is anisotropic while\nthe natural clean image is isotropic. This motivates us to construct\na simple yet effective unsupervised directional gradient based opti-\nmization model (UDG) in which the rainy image is decomposed as\nthe clean image regularized by isotropic TV and the rain streak con-\nstrained by anisotropic TV. UDG has good generalization yet poor\nrepresentation ability and can be efficiently solved by the alternating\ndirection method of multipliers [22]. To further improve the represen-\ntation of UDG, we design an UDG optimization model driven deep\nCNN (UDGNet). The architecture of the network mimics the main\nrole of the optimization models with better feature representation.\nConsequently, the unsupervised loss of UDG is correspondingly\nenforced on UDGNet. Overall, the proposed method inherits good\ngeneralization and representation from both the optimization and\nCNN. The main contributions are summarized:\nâ€¢ Different from existing full/semi-supervised deraining methods, we\nattempt to solve real rain streaks from an unsupervised perspective.\nWe connect the model-driven and data-driven methods via an unsu-\npervised learning framework with simultaneous generalization and\nrepresentation, which offers a new insight to deraining community.\nâ€¢ We discover the structural discrepancy between the rain streak\nand clean image. Consequently, we construct an unsupervised di-\nrectional gradient based optimization model (UDG) for real rain\nstreaks removal. Furthermore, we propose an optimization model-\ndriven deep CNN (UDGNet) in which we optimize the network\nweights by minimizing the unsupervised loss function UDG.\nâ€¢ The proposed UDGNet can be trained with a few real rainy im-\nages, even one single image. We conduct extensive experiments on\nboth synthetic and real-world datasets, which consistently perform\nsuperior against the state-of-the-art methods.\n2\nRELATED WORK\n2.1\nOptimization-based Deraining\nThe model-based optimization methods formulate the single im-\nage rain streaks removal task as an ill-posed problem, in which\nthe decomposition framework is employed to model the image and\nGravity\nWind\nTrajectory \nRaindrop\n(a)\n(b)\n-30\n-20\n10\nGradient Histogram of Clean Image\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nProbability\nHorizontal\nVertical\n10\n20\n30\n0\n(c)\nHorizontal\nVertical\n-20\n10\nGradient Histogram of Rain Streak\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nProbability\n10\n20\n30\n0\n-30\n(d)\nIsotropic\nAnisotropic\nFigure 2: The illustration of the rain streaks and the statistical discrepancy between the rain streak and clean image. (a) The analysis\nof the physical procedure of the raindrop in real-world space. (b) The visualization of the rain streaks in imaging space. (c) and (d)\nshow the gradient histogram of the clean image and rain streak, respectively.\nrain streaks simultaneously with hand-crafted priors [1, 3, 9, 15, 21,\n24, 49]. The key of the optimization method is to dig the domain-\nknowledge of both the rain streaks and image. In 2012, Kang et al.\n[15] excavated the spatial sparsity of both the image and rain streaks,\nand first introduced the one-dimensional vector-based dictionary\nlearning with morphological component analysis. Later, Luo et al.\n[24] proposed a discriminative sparse coding method by additionally\nforcing the two learned dictionaries with mutual exclusivity. Further,\nthe authors utilized the non-local similarity of the images, and em-\nployed the two-dimensional low-rank matrix recovery [3] to better\npreserve the structure of the images. The directional property of the\nrain streak has been widely utilized. For example, Chang et al. [1]\nproposed a transformed low-rank model for compact rain feature\nrepresentation. Li et al. [21] presented a simple patch-based Gauss-\nian mixture models and can accommodate multiple orientations and\nscales of the rain streaks. In this work, we discover the structural\ndiscrepancy between the rain streak and clean image in the gradient\ndomain, and propose a novel unsupervised directional gradient based\noptimization model for rain streaks removal. Moreover, we extend\nthe proposed optimization model to the deep network by minimiza-\ntion of the unsupervised loss UDG, so as to significantly improve\nthe feature representation for better real rain streaks removal.\n2.2\nLearning-based Deraining\nThe CNN based single image rain streak removal methods can be\nmainly classified into the following categories: full-supervised [4â€“\n6, 11, 14, 17, 18, 20, 28, 31â€“34, 36â€“38, 40, 43, 47], semi-supervised\n[35, 41], and unsupervised [47]. Most existing methods are full-\nsupervised where the clean image and synthetic rain image pair are\nrequired. Fu et al. [6] first introduced the end-to-end residual CNN\nto solve the rain streaks removal problem. Yang et al. [37] jointly de-\ntected and removed the rain in a multi-task network. The multi-stage\nand multi-scale architecture networks [14, 20, 26, 32, 40] have been\nextensively studied for better feature representation. Ren et al. [28]\nproposed a simple yet effective progressive recurrent network with\nrecursive blocks for image deraining. To better generalize the real\nrain streaks, the researchers employed the semi-supervised learning\nparadigm. For example, apart from the supervised loss, Wei et al.\n[35] additionally enforced a parameterized GMM distribution on real\nrain streaks. To get rid of the limitation of the paired synthetic-clean\ntraining data, the unsupervised methods [47] have raised attention.\nThe existing unsupervised methods all take advantage of the Cycle-\nGAN framework [48] to handle the unpaired real rain streaks. In\nthis work, our method starts from the unpaired and unsupervised\nnetwork. To the best of our knowledge, we are the first unsupervised\nnetwork that handles the real rainy image from the loss function\nperspective by utilizing the domain knowledge of the rain streaks.\n2.3\nThe Combination of Optimization and CNN\nThe model-driven optimization methods and the data-driven learning\nmethods are the two main categories restoration methods over the\npast decades. These two methodologies are complementary to each\nother in terms of the generalization, representation, training and\ntesting time. There are many attempts to combine them into a unified\nframework. The most popular way is the plug-and-play strategy [44].\nBenefiting from the variable splitting techniques [22], the discrimi-\nnative CNN can be plugged into model-based restoration methods as\na learnable regularization. Liu et al. [23] exploited a deep layer prior\nunder the maximum-a-posterior framework to recover the intrinsic\nrain structure. Another typical manner is the unfolding [39], which\ndesigns the network with sufficient interpretability by unfolding the\niterative optimization procedure into a deep network architecture.\nWang et al. [32] designed a rain convolutional dictionary RCDNet\nfor image deraining with exact step-by-step corresponding relation-\nship between the network modules and the operators in optimization\nprocedure. In this work, we unify the optimization model and CNN\ninto an end-to-end unsupervised learning network, in which we op-\ntimize the network weights by minimizing an unsupervised loss\nfunction, derived from the anisotropic smoothness knowledge of\nrain streaks. Moreover, the architecture of the model-driven deep\nneural network is interpretable with better feature representation.\n3\nMETHODOLOGY\n3.1\nDiscrepancy Between Image and Rain Layer\nThe key of the unsupervised optimization method is to excavate the\ndomain knowledge of both the clean image and rain streaks, so as to\ndecouple the two components into distinguishable subspaces. The\nsparsity, low-rank, GMM properties have been extensively utilized\nin previous works. In this work, we discover a simple yet important\ndomain knowledge that directional rain streak is anisotropic while\nthe natural clean image is isotropic1. Here, we provide an analysis\nto support our statement from a physical and image statistical view-\npoint. On one hand, the physical shape of the rain is approximately\n1Note that in this work, we relax the isotropic to the horizontal and vertical direction.\nRotation\nÎ¸\nRainy Image\nRotated Image\n...\n...\nRain Streak Estimation\nClean Image Estimation\nSelf-Consistent Loss\nAdversarial Loss\nTotal Variation Loss\nAnisotropic TV Loss\nAngle Regression Loss\nTÎ¸\nGrid Generator\nBilinear\nInterporation\nFigure 3: The architecture of the proposed network. The UDGNet mimics the main operations in optimization procedure. Given the\nrainy image, we first estimate the principal direction angle of the rain streaks and feed it to the simplified spatial transformation\nmodule [13] so as to obtain the regular vertical rain streaks. Then, the rotated image is decomposed into distinguishable image and\nrain streak subspaces, which is realized by the domain knowledge driven unsupervised isotropic and directional anisotropic gradient\nloss. Finally, we reconstruct the rotated rainy image with the self-consistency loss to further improve the feature representation.\nspherical raindrop [7]. The raindrops are affected by both the gravity\nand wind. The gravity ensures that the rain is vertically descending\nand the wind determines the descending angle, as shown in Fig. 2(a).\nThen, the imaging system maps the raindrop of the real world to\nthe image plane. Due to the long exposures of the imaging and fast\nmotion of the raindrops, the visual appearance of the rain in image\nspace is presented as severely motion-blurred rain streaks, as shown\nin Fig. 2(b). That is to say, the directional rain streaks are naturally\nanisotropic. On the contrary, the natural image is typically vertical\nand horizontal isotropic [29].\nWe further statistically demonstrate that the structure discrepancy\nbetween clean image and line-pattern rain streaks. Specifically, we\nfirst calculate the gradient maps (first-order forward difference) along\nboth vertical and horizontal axis, and then calculate the histogram\nof gradient maps (x-axis denotes the gradient bins, y-axis represents\nthe number count) on large-scale datasets, as shown in Fig. 2(c) and\n(d). The isotropic means that properties are the same when measured\nalong axes in different directions. The gradient histograms of the\nimage along different directions including x and y axis are very close\nto each other (isotropic TV), while this property does not hold true\nfor the rain streaks (anisotropic TV). That is to say, the directional\nproperty of the rain streaks mainly increases the gradient variation\nacross the streak line direction while has less influence along the\nstreak line. The structure discrepancy between the clean image and\nthe rain streak is the key to decouple them into two subspaces.\n3.2\nUnsupervised Directional Gradient Model\nNow, the key problem is how to mathematically formulate the struc-\nture discrepancy into the optimization model. As for clean image,\nwe utilize isotropic total variational to depict the isotropic gradient\nsmoothness along both horizontal and vertical dimension. As for\nrain streak, we would like to design an anisotropic directional gra-\ndient constraint: penalize the gradient along the rain streak while\npreserving the gradient across rain streak. This is very reasonable,\nsince the rain streaks exhibit obvious directionality similar to the\nsharp edges. However, the main difficulty is the arbitrary angle of\nrain streaks in different images. To solve this problem, we follow\nthe rotated model in [1], so as to obtain vertical rain streaks:\nğœƒâ—¦Y = X + R,\n(1)\nwhere Y is the observed rainy image, ğœƒis the rotation angle, X is the\nclean background, and R is the rain streaks. The goal of this work is\nto estimate both clean image X and rain streaks R simultaneously\nfrom the given rainy image Y. The general optimization deraining\nmodel can be deduced via the maximum-a-posterior as follow:\nğ‘šğ‘–ğ‘›\nX,R\n1\n2 ||X + R âˆ’ğœƒâ—¦Y||2\nğ¹+ ğœğ‘ƒğ‘¥(X) + ğœ†ğ‘ƒğ‘Ÿ(R),\n(2)\nwhere the first term is the data fidelity, ğ‘ƒğ‘¥and ğ‘ƒğ‘Ÿdenote the prior\nterm on clean image and rain streaks, respectively. According to\nthe analysis above, we choose the conventional isotropic TV for the\nclean image and anisotropic directional TV for the rain streaks. For\nsimplicity of the optimization, we estimate the angle of the ğœƒvia\nTILT [45] in advance, and denote Yğ‘Ÿ= ğœƒâ—¦Y. Thus, the formulation\nof the unsupervised directional gradient image deraining model is:\nğ‘šğ‘–ğ‘›\nX,R\n1\n2 ||X + R âˆ’Yğ‘Ÿ||2\nğ¹+ ğœ||X||TV + ğœ†||R||UTV,\n(3)\nwhere ||X||TV = ||âˆ‡X||1 and ||R||UTV = ||âˆ‡ğ‘¥R||1 + ||âˆ‡ğ‘¦Yğ‘Ÿâˆ’âˆ‡ğ‘¦R||1,\nand âˆ‡= (âˆ‡ğ‘¥; âˆ‡ğ‘¦) denotes the vertical (along the rain streak) and\nhorizontal (across the rain streak) derivative operators, respectively,\nwhere ||Â·||1 denotes the sum of absolute value of the matrix elements.\n1) Rain Streaks Update: given image X, the rain streaks R can\nbe estimated from the following minimization problem:\n^R = arg min\nR\n1\n2 ||X + R âˆ’Yğ‘Ÿ||2\nğ¹+ ğœ†ğ‘¥||âˆ‡ğ‘¥R||1 + ğœ†ğ‘¦||âˆ‡ğ‘¦R âˆ’âˆ‡ğ‘¦Yğ‘Ÿ||1. (4)\nDue to the non-differentiability of the ğ¿1 norm, we introduce the\nADMM [22] to convert the original problem into two easy sub-\nproblems with closed-form solutions. By introducing two auxiliary\nvariables Pğ‘¥= âˆ‡ğ‘¥R and Pğ‘¦= âˆ‡ğ‘¦R âˆ’âˆ‡ğ‘¦Yğ‘Ÿ, the Eq. (4) is equiva-\nlent to following problem:\n{^R, ^Pğ‘¥, ^Pğ‘¦} = arg min\nR,Pğ‘¥,Pğ‘¦\n1\n2 ||X + R âˆ’Yğ‘Ÿ||2\nğ¹+ ğœ†ğ‘¥||Pğ‘¥||1 + ğœ†ğ‘¦||Pğ‘¦||1\n+ ğ›¼\n2 ||Pğ‘¥âˆ’âˆ‡ğ‘¥R âˆ’Jğ‘¥\nğ›¼||2\nğ¹+ ğ›½\n2 ||Pğ‘¦âˆ’(âˆ‡ğ‘¦R âˆ’âˆ‡ğ‘¦Yğ‘Ÿ) âˆ’Jğ‘¦\nğ›½||2\nğ¹.\n(5)\nâ– The R-related subproblem is\n^R = arg min\nR\n1\n2 ||X + R âˆ’Yğ‘Ÿ||2\nğ¹+ ğ›¼\n2 ||Pğ‘¥âˆ’âˆ‡ğ‘¥R âˆ’Jğ‘¥\nğ›¼||2\nğ¹\n+ ğ›½\n2 ||Pğ‘¦âˆ’(âˆ‡ğ‘¦R âˆ’âˆ‡ğ‘¦Yğ‘Ÿ) âˆ’Jğ‘¦\nğ›½||2\nğ¹,\n(6)\nwhich has a close-form solution via 2-D fast Fourier transform (FFT)\nRğ‘˜+1 = F âˆ’1\n \nF\n\u0010\n(Yğ‘Ÿâˆ’Xğ‘˜)+âˆ‡ğ‘‡\nğ‘¥(ğ›¼ğ‘˜Pğ‘˜\nğ‘¥âˆ’Jğ‘˜\nğ‘¥)+âˆ‡ğ‘‡\nğ‘¦(ğ›½ğ‘˜Pğ‘˜\nğ‘¦+ğ›½ğ‘˜âˆ‡ğ‘¦Yğ‘Ÿâˆ’Jğ‘˜\nğ‘¦)\n\u0011\n1+ğ›¼ğ‘˜(F(âˆ‡ğ‘¥))2+ğ›½ğ‘˜(F(âˆ‡ğ‘¦))\n2\n!\n.\n(7)\n36\n38\n40\n35\n37\n39\nUnitary Angle Model\nHybrid Angle Model\nÂ°\nÂ°\nÂ°\n45\n60\n90\nHybrid Angle\nPSNR\nFigure 4: The advantage of rotation for rainy images. The same\nmodel trained on the unitary rain streak angle dataset signifi-\ncantly outperforms that of the hybrid angle.\nâ– The {Pğ‘¥, Pğ‘¦}-related subproblem is\nï£±ï£´ï£´ï£´ï£²\nï£´ï£´ï£´ï£³\n^\nPğ‘¥= arg min\nPğ‘¥\nğœ†ğ‘¥||Pğ‘¥||1 + ğ›¼\n2 ||Pğ‘¥âˆ’âˆ‡ğ‘¥R âˆ’Jğ‘¥\nğ›¼||2\nğ¹\n^\nPğ‘¦= arg min\nPğ‘¦\nğœ†ğ‘¦||Pğ‘¦||1 + ğ›½\n2 ||Pğ‘¦âˆ’(âˆ‡ğ‘¦R âˆ’âˆ‡ğ‘¦Yğ‘Ÿ) âˆ’Jğ‘¦\nğ›½||2\nğ¹.\n(8)\nwhich can be solved efficiently via a soft shrinkage operator:\nï£±ï£´ï£´ï£´ï£´ï£²\nï£´ï£´ï£´ï£´ï£³\nPğ‘˜+1\nğ‘¥\n= ğ‘ â„ğ‘Ÿğ‘–ğ‘›ğ‘˜_L1(âˆ‡ğ‘¥Rğ‘˜+1 + Jğ‘˜\nğ‘¥\nğ›¼ğ‘˜, ğœ†ğ‘¥\nğ›¼ğ‘˜)\nPğ‘˜+1\nğ‘¦\n= ğ‘ â„ğ‘Ÿğ‘–ğ‘›ğ‘˜_L1(âˆ‡ğ‘¦Rğ‘˜+1 âˆ’âˆ‡ğ‘¦Yğ‘Ÿ+\nJğ‘˜\nğ‘¦\nğ›½ğ‘˜, ğœ†ğ‘¦\nğ›½ğ‘˜)\nğ‘ â„ğ‘Ÿğ‘–ğ‘›ğ‘˜_L1(ğ‘Ÿ, ğœ‰) =\nğ‘Ÿ\n|ğ‘Ÿ| âˆ—max(|ğ‘Ÿ| âˆ’ğœ‰, 0).\n(9)\nFinally, the Lagrangian multipliers and penalization parameters\nare updated as follows:\nï£±ï£´ï£´ï£²\nï£´ï£´ï£³\nJğ‘˜+1\nğ‘¥\n= Jğ‘˜ğ‘¥+ ğ›¼ğ‘˜(âˆ‡ğ‘¥Rğ‘˜+1 âˆ’Pğ‘˜+1\nğ‘¥\n)\nJğ‘˜+1\nğ‘¦\n= Jğ‘˜ğ‘¦+ ğ›½ğ‘˜(âˆ‡ğ‘¦Rğ‘˜+1 âˆ’âˆ‡ğ‘¦Yğ‘Ÿâˆ’Pğ‘˜+1\nğ‘¦\n)\n{ğ›¼ğ‘˜+1, ğ›½ğ‘˜+1} = {ğ›¼ğ‘˜, ğ›½ğ‘˜} Â· ğœŒ.\n(10)\n2) Image Update: given rain streak R, the image X can be esti-\nmated from the following minimization problem:\n^X = arg min\nX\n1\n2 ||X + R âˆ’Yğ‘Ÿ||2\nğ¹+ ğœ||âˆ‡X||1.\n(11)\nThe optimization of Eq. (11) is similar to that of Eq. (4). Here, we\ndo not describe the procedure in detail.\n3.3\nThe Optimization Model-driven Deep CNN\nAlthough the UDG could remove most of the rain streaks and well\naccommodate different rainy images, the residual rain streaks and\nover-smooth phenomenon are easily observed, especially for the\ncomplex scenes. The main reason is the limited representation ability\nof the hand-crafted linear transformation of TV prior. In this work,\nwe bridge the gap between the optimization model and the deep\nCNN to overcome this issue.\nFrom the optimization model perspective, we deepen the shallow\noptimization model by introducing deep CNN with powerful rep-\nresentation ability. Specifically, we introduce CNN to approximate\nclean image and rain streaks, and enforce the unsupervised loss\nof optimization model as constraint for the deep CNN. Thus, the\nunsupervised learning framework can retain generalization ability,\nmeanwhile leverage the hierarchy nonlinear representation of CNN.\nFrom the deep learning perspective, we replace the conventional\nsupervised paired constraint by the unsupervised loss of the opti-\nmization model. Thus, we get rid of the paired clean-synthetic labels\nfor supervised training and directly train from the real rainy images.\nThe unsupervised loss endows us the good generalization ability\nwith powerful representation of the network. Moreover, most of the\noptimization methods are time-consuming, since the iteration of the\nsolving large-scale linear systems are required. Thus, the proposed\nmethod can enjoy a fast inference speed of CNN.\nThe Architecture and Loss of UDGNet. Now we introduce con-\ncrete architecture of the proposed network which is used for mini-\nmizing the defined unsupervised loss function. The overall network\narchitecture in Fig. 3 is to mimic the optimization procedure of Eq.\n(2). Specifically, we first learn the rotation angle ğœƒregression module\nwhich repeatedly includes several conv and pooling layers.\nLğœƒ= âˆ¥ğœƒâˆ’F (Y; Wğœƒ)âˆ¥2,\n(12)\nwhere Y is the input rainy image, F (â€¢) is the network transformation\nand Wğœƒis the learnable angle regression parameters, and ğœƒis ground-\ntruth of the rain streaks which can be easily obtained in advance.\nCompared with the 2D image and rain streak, the angle is a single\nscalar, which is much easier to learn and label.\nNext, we feed the learned scalar angle into a modified differen-\ntiable spatial transform module [13], which explicitly allows the\nspatial affine transformation of the input image. The classical STN\n[13] can not control how the transformed feature map is. Compared\nwith the classical STN, we additionally enforce a physical meaning\nrotation angle ğœƒ, so that the rainy image Y with arbitrary direction\ncan be well rectified into the vertical direction Yğ‘Ÿ. We show the\noriginal image Y and the rotated version Yğ‘Ÿ, in Fig. 3. Such a sim-\nple operation would significantly reduce the rain streak removal\ndifficulty by reducing the angle variations of different rain streaks.\nTo show the effectiveness of the rotation, we train on two cases:\nrain streaks with arbitrary directions and rain streaks with only the\nvertical direction. The results are reported in Fig. 4, which is not\nsurprising since the rotation has explicitly eliminated the uncertainty.\nThen, we construct two parallel streams for the image and rain\nstreaks estimation, analog to the two prior terms ğ‘ƒğ‘¥(X) and ğ‘ƒğ‘Ÿ(R)\nin Eq. (2). Each stream corresponds to the alternating minimization\nof Eq. (4) and Eq. (11) in optimization model. Furthermore, we\nadditionally introduce the adversarial loss [8] on the clean image for\nbetter textures preserving. The architecture of the two streams are the\nsame with 32 Resblocks [10]. Thus, instead of directly minimization\nof the clean image and the rain streaks, we learn the parameters Wğ¼\nand Wğ‘…in each stream as follow:\nLğ‘–ğ‘šğ‘ğ‘”ğ‘’= ğœâˆ¥âˆ‡F (Yğ‘Ÿ; Wğ¼)âˆ¥1 + ğœ‡Lğ‘ğ‘‘ğ‘£,\n(13)\nLğ‘Ÿğ‘ğ‘–ğ‘›= ğœ†ğ‘¥âˆ¥âˆ‡ğ‘¥F (Yğ‘Ÿ; Wğ‘…)âˆ¥1 + ğœ†ğ‘¦\n\r\râˆ‡ğ‘¦F (Yğ‘Ÿ; Wğ‘…) âˆ’âˆ‡ğ‘¦Yğ‘Ÿ\n\r\r\n1, (14)\nwhere F (â€¢) is the network transformation, Lğ‘ğ‘‘ğ‘£is the adversar-\nial loss [8]. The first term TV loss in Eq. (13) serves as the local\npixel-level smoothness prior while the second term adversarial loss\nworks as the global image-level statistical prior. The two terms are\ncomplementary to each other, so as to obtain natural and clean image.\nFinally, we enforce the self-consistency constraint by composing\nthe estimated image and rain streaks back to the rotated rainy image,\nwhich is exactly the first data fidelity term in Eq. (2):\nLğ‘ ğ‘’ğ‘™ğ‘“= âˆ¥Yğ‘…âˆ’F (Yğ‘Ÿ; Wğ¼) âˆ’F (Yğ‘Ÿ; Wğ‘…)âˆ¥2.\n(15)\n(f) DDN\n(h) UDG\n(i) UDGNet\n(j) GT\n(a) Rain\n(b) DSC\n(d) PatchGAN\n(e) CycleGAN\n(c) GMM\n(g) JORDER-E\nFigure 5: Visualization of deraining results on Cityscape dataset.\n(a) Rain\n(b) DSC\n(c) GMM\n(d) PatchGAN\n(e) CycleGAN\n(f) DDN\n(g) JORDER-E\n(h) UDG\n(i) UDGNet\n(j) GT\nFigure 6: Visualization of deraining results on Rain1400 dataset.\nTable 1: Quantitative comparison PSNR and SSIM with state-of-the-art methods on synthetic datasets.\nDataset\nIndex\nRain\nOptimization\nFull-supervised\nSemi\nUnsupervised\nDSC\nGMM\nUDG\nDDN\nJORDER-E\nRCDNet\nSSIR\nGAN\nCyclegan\nUDGNet\nCityscape\nPSNR\n26.22\n28.18\n29.11\n26.95\n30.59\n24.76\n28.87\n25.15\n30.09\n31.62\n34.65\nSSIM\n0.7776\n0.8104\n0.8649\n0.9434\n0.9232\n0.8559\n0.8778\n0.8165\n0.9325\n0.9140\n0.9653\nRain1400\nPSNR\n23.68\n26.26\n25.72\n23.35\n28.07\n22.18\n24.69\n25.67\n24.14\n28.32\n29.16\nSSIM\n0.7542\n0.7828\n0.7797\n0.8380\n0.8633\n0.7719\n0.7840\n0.8183\n0.7880\n0.8685\n0.8910\nTable 2: Quantitative comparison NIQE and User study results\nwith state-of-the-art methods on real dataset.\nMethod\nRainy\nDSC\nGMM\nUDG\nNIQE\n5.70\n5.55\n6.06\n5.12\nUser study\n1.00\n2.88\n3.64\n4.53\nMethod\nJORDER-E\nRCDNet\nCycleGAN\nUDGNet\nNIQE\n5.48\n5.62\n6.54\n5.08\nUser study\n4.03\n3.03\n1.14\n5.69\nThus, the overall loss of the proposed network can be interpreted as:\nLğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘™ğ‘™= Lğœƒ+ Lğ‘–ğ‘šğ‘ğ‘”ğ‘’+ Lğ‘Ÿğ‘ğ‘–ğ‘›+ Lğ‘ ğ‘’ğ‘™ğ‘“.\n(16)\nThe deep network explicitly learns the optimization procedure, in\nwhich each module and loss mimic the main operation in optimiza-\ntion. On one hand, the network inherits the unsupervised domain\nknowledge from optimization. Thus, the proposed network general-\nizes well to different rainy images and can be trained and tested on\none single image (without the adversarial loss). Moreover, once the\nnetwork is trained, it only requires a very fast forward pass through\nthe deep network to predict the clean image without further optimiza-\ntion steps. On the other hand, the proposed model is representative\nof the complex scenes endowed by the highly-nonlinear network,\nand the proposed network is interpretable and controllable.\nTable 3: Model size (MB) and time complexity (seconds)\nMethod\nDSC\nGMM\nUDG\nDDN\nModel size\n-\n-\n-\n0.233\nRunning time\n5066\n1443\n7.846\n0.162\nMethod\nJORDER-E\nRCDNet\nCycleGAN\nUDGNet\nModel size\n16.7\n13.1\n11.7\n5.7\nRunning time\n9.123\n22.6\n3.098\n0.129\n4\nEXPERIMENTAL RESULTS\n4.1\nDatasets and Experimental Setting\nDatasets. We evaluate UDGNet on both synthetic and real datasets.\nâ€¢ Cityscape. We simulate cityscapes rain images following the\nscreen blend model [24] with different streak length, width, angle\nand intensity. We follow the cityscapes dataset with 2975 images\nfor training and 500 images for testing.\nâ€¢ Rain1400. We adopt rain1400 [6] as another synthetic dataset,\nwhich contains 1400 rain/clear images pairs. We randomly select\n12600 for training and 1400 pairs for testing.\nâ€¢ RealRain. We collect real world rainy images with large field of\nview from the datasets [2, 35, 37] and Google search. We utilize\n100 real rainy images to train and test the UDGNet.\n(e) JORDER-E\n(f) UDGNet\n(d) SSIR\n(c) CycleGAN\n(b) DSC\n(a) Rainy Image\nFigure 7: Visualization of deraining results on RealRain dataset.\nTable 4: The influence of the estimated angle ğœƒ.\nğœƒ\nEstimated\nGT\nGTÂ±5â—¦\nGTÂ±10â—¦\nGTÂ±20â—¦\nPSNR\n34.65\n34.71\n34.56\n34.26\n33.03\nSSIM\n0.9663\n0.9665\n0.9639\n0.9602\n0.9389\nImplemention Details. The images are trained and tested through\nsliding windows with the size of 128*128. The angle of rain streaks\nis obtained via the TILT [45] in advance for training. We adopt\nAdam [16] as the optimizer with batch size of 8. The initial learning\nrate is set to be 0.001 and decay 0.1 every 30 epochs. We set the\nhyper-parameter ğœ, ğœ‡, ğœ†ğ‘¥, ğœ†ğ‘¦as 0.01, 400, 1.5, 1.0, respectively.\nExperimental Setting. We compare the proposed unsupervised\nUDG and UDGNet with (1) optimization methods DSC [24] and\nGMM [21]; (2) supervised methods DDN [6], JORDER-E [36] and\nRCDNet [32]; (3) semi-supervised methods SSIR [35]; (4) unsuper-\nvised network PatchGAN [12] and CycleGAN [48]. For synthetic\ndata, the full-reference PSNR and SSIM are utilized as the quantita-\ntive evaluation. For real-world images, we employ the non-reference\nNIQE [25] and user studies to quantitatively evaluate the visual\nquality of deraining results. The higher PSNR, SSIM and user study\npoint is and the lower the NIQE is, the better the deraining result\nis. The optimization methods do not need the training dataset. The\nsupervised methods are trained on the defined dataset in the original\npaper and tested on different datasets in our work.\n4.2\nQuantitative and Qualitative Results\nQualitative Results. In Fig. 5-7, we show the visual deraining re-\nsults on both synthetic and real datasets: Cityscape, Rain1400 and\nRealRain. We can observe that there are obvious residual rain streaks\nin optimization-based methods, especially in Fig. 5, because of the\nlimited representation ability of hand-crafted priors for diverse back-\nground and rain streaks. The unsupervised learning-based methods\nPatchGAN and CycleGAN could remove most of the rain streaks\nwith a few residual. However, GAN-based unsupervised methods are\ndifficult to train and easy to collapse, since they heavily rely on the\ndistribution of training dataset. The supervised methods DDN and\nTable 5: The effectiveness analysis of each loss in UDGNet.\nCase\nLğ‘–ğ‘šğ‘”âˆ’ğ‘¡ğ‘£\nLğ‘–ğ‘šğ‘”âˆ’ğ‘ğ‘‘ğ‘£\nLğ‘Ÿğ‘ğ‘–ğ‘›\nLğ‘ ğ‘’ğ‘™ğ‘“\nPSNR\nSSIM\n1\nâˆš\nÃ—\nÃ—\nÃ—\n22.63\n0.8229\n2\nÃ—\nâˆš\nÃ—\nÃ—\n30.09\n0.9325\n3\nâˆš\nâˆš\nÃ—\nÃ—\n30.08\n0.8915\n4\nÃ—\nÃ—\nâˆš\nÃ—\n32.44\n0.9538\n5\nÃ—\nâˆš\nâˆš\nâˆš\n33.01\n0.9681\n6\nâˆš\nÃ—\nâˆš\nâˆš\n34.64\n0.9644\n7\nâˆš\nâˆš\nâˆš\nâˆš\n34.65\n0.9653\nJORDER-E could suppress rain streaks to some extent, while there\nare also some residual rain streaks in the results due to the data dis-\ntribution discrepancy. The optimization model UDG could remove\nmost rain streaks. UDGNet further enhances the performance. On\none hand, the visual rain streaks have been satisfactorily removed\nby the UDGNet with few residuals. On the other hand, compared\nwith the UDG, the unsupervised learning-based UDGNet could well\npreserve the image structures. Compared with other methods, the\nproposed UDGNet could achieve better performance in terms of\nboth rain streaks removal and image texture preserving.\nQuantitative Results. The quantitative results are reported in Table\n1 and 2 in which the best results are in bold. The UDGNet consis-\ntently obtains the best deraining results, which further demonstrates\nthe superior of the proposed method in terms of the performance\nand generalization. We further show model size and time complexity\nof different methods in Table 3. The proposed UDGNet is compu-\ntationally cheap and efficient. The model size of UDGNet is about\n5.7MB, which is significantly smaller than the competing methods.\nFurthermore, we benchmark the running time with an Intel Core\ni7-8700 CPU and an NVIDIA RTX 2080Ti. For an image with size\n1024*2048, the running time of the UDG is 7.8s, which is obviously\nfaster than DSC (5066s) and GMM (1433s). The testing time of\nUDGNet is 0.12s, which is more attractive for practical use.\n4.3\nAblation Study\nHow does Angle Estimation Affect the Performance? The angle\nestimation and rotation module is an important pre-processing part\n(a) Rain\nOversmooth\nOptimal\nResidual\nx\n(b) Î»\nyÎ»\n:\n=1\nx\n(c) Î»\nyÎ»\n:\n=1.5\nx\n(d) Î»\nyÎ»\n:\n=2\nFigure 8: The illustration of how the regularization parameters control the deraining strength.\n(c) DIP\n(d) UDGNet\n(b) DSC\n(a) Rain\nFigure 9: The single image training and inferencing.\nof our UDGNet. In Table 4, we show how the angle estimation in-\nfluences the deraining performance. We can observe that the more\naccurate the angle is, the better the deraining result is, which indi-\ncates that precise angle guidance can indeed improve UDGNet for\nbetter deraining performance. Moreover, the deraining result of the\nestimated angle is very close to that of the provided the oracle (GT)\nangle, which validates the effectiveness of the proposed network.\nWhat is the Effectiveness of Each Loss? To verify the effect of\neach loss in UDGNet, in Table 5, we conduct ablation study of each\nterm on cityscape validation. From cases 1, 2, 3: the adversarial loss\nis more important than the TV loss for image. From case 4: the pro-\nposed directional domain knowledge of rain streak is very effective\nfor rain removal. From case 5, 6, 7: the joint loss with both rain streak\nand clean image could further boost the performance. Moreover, the\nself-consistency loss is also beneficial to the performance.\nHow Can We Control the Deraining Result of UDGNet? The\nloss function of UDGNet is derived from optimization model (UDG),\nin which each term has clear interpretability. Thus the deraining\nstrength can be controlled through hyper-parameters ğœ†ğ‘¥and ğœ†ğ‘¦. As\nshown in Fig. 8, different deraining results are obtained with different\nğœ†ğ‘¥/ğœ†ğ‘¦ratios. As ratio increases, more details are preserved with\nmore streaks left, and vice versa. Thus, we can set different ratios to\nbalance texture preservation and rain streak removal adaptively.\n4.4\nDiscussion\nSingle Image Training and Inference The previous learning based\nmethods consistently need a number of the training samples, in\nwhich the testing performance heavily relies on the training datasets.\nIn this work, our unsupervised learning framework not only utilizes\nexternal dataset but also the internal prior knowledge of singe im-\nage. Therefore, the UDGNet can be trained on both the large scale\n(a) Rainy \n(b) UDGNet \nFigure 10: The limitation of the UDGNet.\ndatasets and one single image. We test the performance of single\nimage training along with the similar deep image prior DIP [30] for\ncomparison in Fig. 9. We can observe that the UDGNet could well\nremove and rain streaks with clear image texture, while the DIP has\nunexpectedly over-smooth the image details.\nLimitation In Fig. 10, we show a real image with rain streaks\n(nearby) and veiling artifacts (distant). We can observe that the rain\nstreaks have been satisfactorily removed while the veiling effects\nstill exist in the result. This is reasonable, since the UDGNet mainly\nutilizes the directional anisotropic characteristic in the loss. In future,\nwe would like to incorporate the domain knowledge of the veiling\nartifacts via the unsupervised loss into the proposed framework.\n5\nCONCLUSION\nIn this work, we aim at the real image rain streak removal, and\npropose a novel optimization model driven deep CNN method for\nunsupervised deraining. Our start point is to bridge the gap between\nthe model-driven optimization method and the data-driven learning\nmethod in terms of the generalization and representation. The key to\nour learning framework is the modelling of the structure discrepancy\nbetween the rain streak and clean image. We formulate this domain\nknowledge into unsupervised direction gradient optimization model,\nand transfer the unsupervised loss function to the deep network, such\nthat the proposed method could simultaneously achieve good gener-\nalization and representation ability. Extensive experimental results\non both the real and synthetic datasets demonstrate the effectiveness\nof the proposed method for real rain streaks removal.\nAcknowledgements.This work was supported by National Natu-\nral Science Foundation of China under Grant No. 61971460, China\nPostdoctoral Science Foundation under Grant 2020M672748, Na-\ntional Postdoctoral Program for Innovative Talents BX20200173 and\nEquipment Pre-Research Foundation under grant No. 6142113200304.\nREFERENCES\n[1] Yi Chang, Luxin Yan, and Sheng Zhong. 2017. Transformed low-rank model for\nline pattern noise removal. In Proc. ICCV. 1726â€“1734.\n[2] Jie Chen, Cheen-Hau Tan, Junhui Hou, Lap-Pui Chau, and He Li. 2018. Robust\nvideo content alignment and compensation for rain removal in a CNN framework.\nIn Proc. CVPR. 6286â€“6295.\n[3] Yi Lei Chen and Chiou Ting Hsu. 2013. A generalized low-rank appearance model\nfor spatio-temporally correlated rain streaks. In Proc. ICCV. 1968â€“1975.\n[4] Sen Deng, Mingqiang Wei, Jun Wang, Yidan Feng, Luming Liang, Haoran Xie,\nFulee Wang, and Meng Wang. 2020. Detail-recovery Image Deraining via Context\nAggregation Networks. In Proc. CVPR. 14560â€“14569.\n[5] Yingjun Du, Jun Xu, Xiantong Zhen, Ming-Ming Cheng, and Ling Shao. 2020.\nConditional Variational Image Deraining. IEEE Trans. Image Process. 29 (2020),\n6288â€“6301.\n[6] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao Ding, and John\nPaisley. 2017. Removing rain from single images via a deep detail network. In\nProc. CVPR. 3855â€“3863.\n[7] Kshitiz Garg and Shree K Nayar. 2007. Vision and rain. Int. J. Comput. Vision 75,\n1 (2007), 3â€“27.\n[8] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-\nFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative\nAdversarial Networks. In Proc. NIPS. 2672â€“2680.\n[9] Shuhang Gu, Deyu Meng, Wangmeng Zuo, and Lei Zhang. 2017. Joint con-\nvolutional analysis and synthesis sparse representation for single image layer\nseparation. In Proc. ICCV. 1708â€“1716.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proc. CVPR. 770â€“778.\n[11] Xiaowei Hu, Chi-Wing Fu, Lei Zhu, and Pheng-Ann Heng. 2019.\nDepth-\nattentional Features for Single-image Rain Removal. In Proc. CVPR. 8022â€“8031.\n[12] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2017. Image-\nto-Image Translation with Conditional Adversarial Networks. In Proc. CVPR.\n1125â€“1134.\n[13] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu.\n2015. Spatial transformer networks. In Proc. ICLR.\n[14] Kui Jiang, Zhongyuan Wang, Peng Yi, Chen Chen, Baojin Huang, Yimin Luo,\nJiayi Ma, and Junjun Jiang. 2020. Multi-Scale Progressive Fusion Network for\nSingle Image Deraining. In Proc. CVPR. 8346â€“8355.\n[15] LiWei Kang, ChiaWen Lin, and YuHsiang Fu. 2012. Automatic single-image-\nbased rain streaks removal via image decomposition. IEEE Trans. Image Process.\n21, 4 (2012), 1742â€“1755.\n[16] Diederik P Kingma and Jimmy Ba. 2015.\nAdam: A Method for Stochastic\nOptimization. In Proc. ICLR.\n[17] Guanbin Li, Xiang He, Wei Zhang, Huiyou Chang, Le Dong, and Liang Lin. 2018.\nNon-locally Enhanced Encoder-Decoder Network for Single Image De-raining. In\nACM. MM. 1056â€“1064.\n[18] Ruoteng Li, Loong-Fah Cheong, and Robby T Tan. 2019. Heavy Rain Image\nRestoration: Integrating Physics Model and Conditional Adversarial Learning. In\nProc. CVPR. 1633â€“1642.\n[19] Siyuan Li, Wenqi Ren, Jiawan Zhang, Jinke Yu, and Xiaojie Guo. 2019. Single\nimage rain removal via a deep decompositionâ€“composition network. Comput. Vis.\nImage Und. 186, 6 (2019), 48â€“57.\n[20] Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, and Hongbin Zha. 2018. Recurrent\nsqueeze-and-excitation context aggregation net for single image deraining. In Proc.\nECCV. 254â€“269.\n[21] Yu Li, Robby T Tan, Xiaojie Guo, Jiangbo Lu, and Michael S Brown. 2016. Rain\nstreak removal using layer priors. In Proc. CVPR. 2736â€“2744.\n[22] Zhouchen Lin, Risheng Liu, and Zhixun Su. 2011. Linearized alternating direction\nmethod with adaptive penalty for low-rank representation. In NIPS. 612â€“620.\n[23] Risheng Liu, Zhiying Jiang, Long Ma, Xin Fan, and Zhongxuan Luo. 2018.\nDeep Layer Prior Optimization for Single Image Rain Streaks Removal. In Proc.\nICASSP. 1408â€“1412.\n[24] Yu Luo, Yong Xu, and Hui Ji. 2015. Removing rain from a single image via\ndiscriminative sparse coding. In Proc. ICCV. 3397â€“3405.\n[25] Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. 2013. Making a â€™Com-\npletely Blindâ€™ Image Quality Analyzer. IEEE Signal Proc. Let. 20, 3 (2013),\n209â€“212.\n[26] Bo Pang, Deming Zhai, Junjun Jiang, and Xianming Liu. 2020. Single Image\nDeraining via Scale-space Invariant Attention Neural Network. In ACM. MM.\n375â€“383.\n[27] Joseph Redmon and Ali Farhadi. 2017. YOLO9000: better, faster, stronger. In\nProc. CVPR. 7263â€“7271.\n[28] Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu, and Deyu Meng. 2019.\nProgressive image deraining networks: a better and simpler baseline. In Proc.\nCVPR. 3937â€“3946.\n[29] Antonio Torralba and Aude Oliva. 2003. Statistics of natural image categories.\nNetwork: computation in neural systems 14, 3 (2003), 391â€“412.\n[30] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. 2018. Deep image prior.\nIn Proc. CVPR. 9446â€“9454.\n[31] Guoqing Wang, Changming Sun, and Arcot Sowmya. 2019. ERL-Net: Entangled\nRepresentation Learning for Single Image De-Raining. In Proc. ICCV. 5644â€“\n5652.\n[32] Hong Wang, Qi Xie, Qian Zhao, and Deyu Meng. 2020. A Model-driven Deep\nNeural Network for Single Image Rain Removal. In Proc. CVPR. 3103â€“3112.\n[33] Tianyu Wang, Xin Yang, Ke Xu, Shaozhe Chen, Qiang Zhang, and Rynson WH\nLau. 2019. Spatial Attentive Single-Image Deraining with a High Quality Real\nRain Dataset. In Proc. CVPR. 12270â€“12279.\n[34] Zheng Wang, Jianwu Li, and Ge Song. 2019. DTDN: Dual-task De-raining\nNetwork. In ACM. MM. 1833â€“1841.\n[35] Wei Wei, Deyu Meng, Qian Zhao, Zongben Xu, and Ying Wu. 2019. Semi-\nSupervised Transfer Learning for Image Rain Removal. In Proc. CVPR. 3877â€“\n3886.\n[36] W. Yang, R. Tan, J. Feng, Z. Guo, S. Yan, and J. Liu. 2019. Joint rain detection\nand removal from a single image with contextualized deep networks. IEEE Trans.\nPattern Anal. Mach. Intell. 42, 6 (2019), 1377â€“1393.\n[37] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and\nShuicheng Yan. 2017.\nDeep joint rain detection and removal from a single\nimage. In Proc. CVPR. 1357â€“1366.\n[38] Wenhan Yang, Robby T. Tan, Shiqi Wang, Yuming Fang, and Jiaying Liu. 2020.\nSingle Image Deraining: From Model-Based to Data-Driven and Beyond. IEEE\nTrans. Pattern Anal. Mach. Intell. (2020).\n[39] Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. 2016. Deep ADMM-Net for\ncompressive sensing MRI. In Proc. NIPS. 10â€“18.\n[40] Rajeev Yasarla and Vishal M. Patel. 2019.\nUncertainty Guided Multi-Scale\nResidual Learning-Using a Cycle Spinning CNN for Single Image De-Raining. In\nProc. CVPR. 8405â€“8414.\n[41] Rajeev Yasarla, Vishwanath A. Sindagi, and Vishal M Patel. 2020. Syn2Real\nTransfer Learning for Image Deraining using Gaussian Processes. In Proc. CVPR.\n2726â€“2736.\n[42] Yuntong Ye, Yi Chang, Hanyu Zhou, and Luxin Yan. 2021. Closing the Loop:\nJoint Rain Generation and Removal via Disentangled Image Translation. In Proc.\nCVPR.\n[43] He Zhang and Vishal M Patel. 2018. Density-aware single image de-raining using\na multi-stream dense network. In Proc. CVPR. 695â€“704.\n[44] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. 2017. Learning deep\nCNN denoiser prior for image restoration. In Proc. CVPR. 3929â€“3938.\n[45] Zhengdong Zhang, Arvind Ganesh, Xiao Liang, and Yi Ma. 2012. TILT: Transform\ninvariant low-rank textures. Int. J. Comput. Vision 99, 1 (2012), 1â€“24.\n[46] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.\n2017. Pyramid scene parsing network. In Proc. CVPR. 2881â€“2890.\n[47] Hongyuan Zhu, Xi Peng, Tianyi Zhou, Songfan Yang, Vijay Chanderasekh, Liyuan\nLi, and Joo-Hwee Lim. 2019. Single Image Rain Removal with Unpaired Infor-\nmation: A Differentiable Programming Perspective. In Proc. AAAI. 9332â€“9339.\n[48] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. Unpaired\nImage-to-Image Translation using Cycle-Consistent Adversarial Networks. In\nProc. CVPR. 2223â€“2232.\n[49] Lei Zhu, ChiWing Fu, Dani Lischinski, and PhengAnn Heng. 2017. Joint bi-layer\noptimization for single-image rain streak removal. In Proc. ICCV. 2526â€“2534.\n",
  "categories": [
    "cs.CV",
    "eess.IV"
  ],
  "published": "2022-03-25",
  "updated": "2022-03-25"
}