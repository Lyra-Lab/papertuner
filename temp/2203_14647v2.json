{
  "id": "http://arxiv.org/abs/2203.14647v2",
  "title": "Automatic Debate Evaluation with Argumentation Semantics and Natural Language Argument Graph Networks",
  "authors": [
    "Ramon Ruiz-Dolz",
    "Stella Heras",
    "Ana García-Fornes"
  ],
  "abstract": "The lack of annotated data on professional argumentation and complete\nargumentative debates has led to the oversimplification and the inability of\napproaching more complex natural language processing tasks. Such is the case of\nthe automatic debate evaluation. In this paper, we propose an original hybrid\nmethod to automatically evaluate argumentative debates. For that purpose, we\ncombine concepts from argumentation theory such as argumentation frameworks and\nsemantics, with Transformer-based architectures and neural graph networks.\nFurthermore, we obtain promising results that lay the basis on an unexplored\nnew instance of the automatic analysis of natural language arguments.",
  "text": "Automatic Debate Evaluation with Argumentation Semantics and Natural\nLanguage Argument Graph Networks\nRamon Ruiz-Dolz1, Stella Heras2, Ana García-Fornes2\n1Centre for Argument Technology, University of Dundee, Dundee DD1 4HN, United Kingdom\n2VRAIN, Universitat Politècnica de València, 46022 València, Spain\nrruizdolz001@dundee.ac.uk, {stehebar,agarcia}@dsic.upv.es\nAbstract\nThe lack of annotated data on professional ar-\ngumentation and complete argumentative de-\nbates has led to the oversimplification and the\ninability of approaching more complex natural\nlanguage processing tasks. Such is the case\nof the automatic evaluation of complete pro-\nfessional argumentative debates. In this paper,\nwe propose an original hybrid method to au-\ntomatically predict the winning stance in this\nkind of debates. For that purpose, we com-\nbine concepts from argumentation theory such\nas argumentation frameworks and semantics,\nwith Transformer-based architectures and neu-\nral graph networks. Furthermore, we obtain\npromising results that lay the basis on an unex-\nplored new instance of the automatic analysis\nof natural language arguments.\n1\nIntroduction\nThe automatic evaluation of argumentative debates\nis a Natural Language Processing (NLP) task that\ncan support judges in debate tournaments, analysts\nof political debates, and even help to understand the\nhuman reasoning used in social media (e.g., Twit-\nter debates) where argumentation may be difficult\nto follow. This task belongs to the computational\nargumentation area of research, a broad, multidis-\nciplinary area of research that has been evolving\nrapidly in the last years (Atkinson et al., 2017).\nClassically, computational argumentation research\nfocused on formal abstract logic and computational\n(i.e., graph) representations of arguments and their\nrelations. In this approach, the evaluation of argu-\nments relied exclusively in logical and topological\nproperties of the argument representations (Alfano\net al., 2021). Furthermore, these techniques have\nbeen thoroughly studied and analysed, but from a\ntheoretical and formal viewpoint considering spe-\ncific cases and configurations instead of large, in-\nformal debates (Verheij, 2005; Caminada, 2006).\nThe significant advances in NLP have enabled\nthe study of new less formal approaches to under-\ntake argumentative analysis tasks (Lawrence and\nReed, 2019; Goffredo et al., 2023). One of the\nmost popular tasks that has gained a lot of popular-\nity in the recent years is argument mining, a task\naimed at finding argumentative elements in natu-\nral language inputs (i.e., argumentative discourse\nsegmentation) (Jo et al., 2019), defining their argu-\nmentative purpose (i.e., argumentative component\nclassification) (Bao et al., 2021), and detecting ar-\ngumentative structures between these elements (i.e.,\nargumentative relation identification) (Ruiz-Dolz\net al., 2021a). Even though most of the NLP re-\nsearch applied to computational argumentation has\nbeen focused in argument mining, other NLP-based\ntasks have also been researched such as the genera-\ntion of natural language arguments (Mitsuda et al.,\n2022), the assesment of the persuasiveness of natu-\nral language arguments (El Baff et al., 2020), and\nthe automatic generation of argument summaries\n(Bar-Haim et al., 2020) among others. However, it\nis possible to observe an important lack of research\naimed at the evaluation of complete argumentative\ndebates approached with NLP-based algorithms.\nFurthermore, most of the existing research in this\ntopic has been contextualised in online debate fo-\nrums, considering only short text arguments and\nmessages, and without a professional human evalu-\nation (Hsiao et al., 2022).\nIn this paper, we propose a hybrid method for\nevaluating complete argumentative debates consid-\nering the lines of reasoning presented by profes-\nsional debaters and taking into account the human\nevaluations provided by an impartial jury. Our\nmethod combines concepts from the classical com-\nputational argumentation theory (i.e., argumenta-\ntion frameworks and semantics), with models and\nalgorithms effectively used in other NLP tasks (i.e.,\nTransformer-based sentence vector representations\nand graph networks). This way, we take a complete\nprofessional debate including all the argumentation\nand rebuttal phases as an input, and predict the win-\narXiv:2203.14647v2  [cs.CL]  21 Jan 2024\nning stance (i.e., in favour or against) for a given\nargumentative topic. For that purpose, we define\nan original computational modelling of complete\nprofessional natural language debates that allows\nus to improve the prediction of the winning stance\nof complete argumentative debates compared to\nmore conventional approaches. We present a com-\nplete comparison of approaches relying exclusively\non NLP algorithms and approaches relying exclu-\nsively on argumentation theory concepts. From\nour findings, we can observe that the hybrid ap-\nproach proposed in this paper is the more adequate\nto tackle a complex NLP challenge such as predict-\ning the winning stance in complete debates.\n2\nRelated Work\nClassically, the computational representation and\nassessment of arguments has been conducted\nthrough argumentation frameworks and argumen-\ntation semantics (Dung, 1995). However, this line\nof research has been focused on abstract argumen-\ntation and formal logic-based argumentative struc-\ntures, and has not been properly extended to the\ninformal natural language representation of human\nargumentation.\nThe automatic assessment of natural language\narguments is a relatively new topic of research that\nhas been addressed from different NLP viewpoints.\nMost of this research has been focused on perform-\ning an individual evaluation of arguments or ar-\ngumentative lines of reasoning (Wachsmuth et al.,\n2017) instead of a global, interactive viewpoint\nwhere complete debates consisting of multiple, con-\nflicting lines of reasoning are analysed. Typically,\nthe automatic evaluation of natural language ar-\nguments has been carried out comparing the con-\nvincingness of pairs of arguments (Gleize et al.,\n2019); analysing user features such as interests\nor personality to predict argument persuasiveness\n(Al Khatib et al., 2020); and analysing natural lan-\nguage features of argumentative text to estimate\nits persuasive effect (El Baff et al., 2020). The\nuse of graph-based approaches to evaluate individ-\nual argument structures has been recently explored\nin (Saveleva et al., 2021). In the same direction,\n(Marro et al., 2022) proposes a framework for eval-\nuating three dimensions of arguments (i.e., cogency,\nrhetoric, and reasonableness) by producing natu-\nral language embeddings from individual argument\nstructures (e.g., claim - premise). However, none of\nthem considers the problem of argument evaluation\nin an argumentative dialogue as in the case of the\ndebates.\nThe global (i.e., debate) approach on the evalu-\nation of natural language arguments was initially\nresearched in (Potash and Rumshisky, 2017) where\nRecurrent Neural Networks were used to evaluate\nnon-professional debates in a corpus of limited size\nand structure. Following this trend, in (Shirafuji\net al., 2019), the authors propose a method based\non the persuasiveness to predict the outcome of\nonline debates using a support vector machine. Re-\ncently, in (Hsiao et al., 2022), the authors present\nan algorithm for predicting the outcome of non-\nprofessional debates of limited length and depth\nin online forums. Furthermore, in the previous\nwork the considered argumentative structures are\nsimple, and the proposed methods depend exclu-\nsively on natural language features. All these works\nhave two main aspects in common: first, they are\nfocused exclusively in online text-based debates,\nwhere information is easy to obtain, but very lim-\nited from an argumentative viewpoint; and second,\nthe debates brought into consideration present short\ninteractions and simpler arguments than the ones\nthat can be found in a professional debate.\nInterestingly, a recent trend in the proposed argu-\nment assessment algorithms from the more theoret-\nical side of the computational argumentation area\nof research also consists of leveraging structural\ninformation of the graph to estimate the acceptabil-\nity of arguments by using neural networks instead\nof classic solvers (Kuhlmann and Thimm, 2019;\nMalmqvist et al., 2020; Craandijk and Bex, 2021).\nHowever, in these cases arguments are treated as\nabstract entities without specific natural language\nbeing attributed to them, making their results more\ndifficult to contextualise in a real situation with\nnatural language arguments.\nThe previously reviewed work evidences that\nfundamental concepts from computational argu-\nmentation theory are typically overseen in the\nargument-related NLP literature, and the used cor-\npora contain debates far removed from the concept\nof a professional debate. Thus, we propose a new\nmethod that combines the advantages of both areas\nof research: formal argumentation theory and NLP.\nThis way, our proposal enables the analysis of com-\nplete professional argumentative debates in both,\nlength and argumentative depth, a task that has not\nbeen addressed in the literature yet.\n3\nData\nIn this paper, we approach the automatic prediction\nof the winning stance in complete natural language\nprofessional debates. For that purpose, we use the\nVivesDebate1 corpus (Ruiz-Dolz et al., 2021b) to\nconduct all the experiments and the validation of\nour proposed method. This corpus contains the\nannotations of the complete lines of reasoning pre-\nsented by the debaters in a debate tournament, in-\nspired by the AIF (Chesnevar et al., 2006) standard,\nand the professional jury evaluations of the qual-\nity of argumentation presented in each debate. It\nis important to emphasise this aspect, as the av-\nerage length of the debates we analysed in this\npaper is 4819 words (30-40 minutes of length),\nand large language models have problems when\nworking with long sequences of natural language\ntext (Beltagy et al., 2020)2. Previously published\ncorpora for the analysis of natural language argu-\nmentation always tended to simplify the annotated\nargumentative reasoning, by only considering in-\ndividual arguments, pairs of arguments, or consid-\nering a small set of arguments, instead of deeper\nand complete lines of argumentative reasoning. For\nexample, in argument mining (e.g., US2016 (Visser\net al., 2020) and QT30 (Hautli-Janisz et al., 2022)),\nargument assessment (e.g., IBM-EviConv (Gleize\net al., 2019)), or natural language argument genera-\ntion/summarisation (e.g., GPR-KB (Orbach et al.,\n2019), DebateSum , (Roush and Balaji, 2020)). Fur-\nthermore, online debates with their crowd-sourced\nevaluations were compiled in (Durmus and Cardie,\n2019), but argumentation was produced in short\nwritten paragraphs, and evaluations were based on\nanonymous votes from the community that did not\nrequire any justification. Therefore, the VivesDe-\nbate corpus is the only identified publicly available\ncorpus that enables the study of the automatic eval-\nuation of natural language professional debates in\ntheir complete form.\nThe VivesDebate corpus contains 29 complete\nargumentative debates (139,756 words) from a uni-\nversity debate tournament in Catalan. Each debate\nis annotated entirely without partitions, and cap-\nturing the complete lines of reasoning presented\nby the debaters. The natural language text is seg-\nmented into Argumentative Discourse Units (7,810\nADUs) (Peldszus and Stede, 2013). Each ADU\n1Available\nonline\nin:\nhttps://doi.org/10.5281/\nzenodo.6531487\n2The Longformer supports sequences of up to 4096 tokens.\ncontains its own text, its stance (i.e., in favour or\nagainst the topic of the debate), the phase of the\ndebate where it has been uttered (i.e., introduction,\nargumentation, and conclusion), and a set of argu-\nmentative relations (i.e., inference, conflict, and\nrephrase) that make possible to capture argumenta-\ntive structures, the sequentiality in the debate, and\nthe existing major lines of reasoning. Additionally,\neach debate has the scores of the jury that indicate\nwhich team has proposed a more solid and stronger\nargumentative reasoning. An in-depth analysis of\nthe corpus structure and statistics can be found in\n(Ruiz-Dolz et al., 2021b).\n4\nMethod\nThe human evaluation of argumentative debates\nis a complex task that involves many different as-\npects such as the thesis solidity, the argumentation\nquality, and other linguistic aspects of the debate\n(e.g., oral fluency, grammatical correctness, etc.).\nIt is possible to observe that both, the logic of argu-\nmentation and the linguistic properties play a ma-\njor role in the evaluation of argumentative debates.\nTherefore, the method proposed in this paper is\ndesigned to capture both aspects of argumentation\nby combining concepts from computational argu-\nmentation theory and NLP. Our method is divided\ninto two different phases: first, (i) determining the\nacceptability of arguments (i.e., their logical valid-\nity) in a debate based on their logical structures\nand relations; and second, (ii) scoring the resulting\nacceptable arguments by analysing aspects of their\nunderlying natural language features to determine\nthe winner of a debate. Figure 1 presents an scheme\nwith the most important phases and elements of the\nproposed method.\nBefore describing both phases of our method, it\nis important to contextualise our proposal within\nthe area of computational argumentation research.\nWe assume that the whole argument analysis of\nnatural language text has already been carried out:\nthe argumentative discourse has been segmented,\nthe argument components have been classified, and\nargument relations have been identified among the\nsegmented argumentative text spans (see (Lenz\net al., 2020)). Thus, a graph structure can be de-\nfined from a given natural language argumenta-\ntive input. As depicted in Figure 1, the Argument\nAnalysis containing the text of the arguments (i.e.,\nnode content), their stance (i.e., node colour), infer-\nence relations (i.e., green edges), conflict relations\nArgument\nAnalysis\nFramework\nEncoding\nArgumentation\nSemantics\nLearning\nSample\nSample Init.\nDebate\n[...]\n[...]\n[...]\nLanguage\nModel\nGraph \nNetwork\n[...]'\n[...]'\n[...]'\nPredictor\n....\n....\nWinner\nLooser\nPhase I\nPhase II\nFigure 1: Structural scheme of the proposed automatic debate evaluation method.\n(i.e., red edges), and rephrase relations (i.e., yellow\nedges) among arguments can be a valid starting\npoint to the proposed method.\n(Lenz et al., 2019)\n4.1\nPhase I: Argument Acceptability\nThe first phase of the proposed method relies on\nconcepts from computational argumentation theory.\nThis phase can be understood as a pre-processing\nstep from the NLP research viewpoint. Thus, the\nmain goal of Phase I is to analyse the argumenta-\ntive information contained in the argument graph,\nand to computationally encode this information\nfocusing on the most relevant aspects for natural\nlanguage argumentation (see Figure 1, Framework\nEncoding and Argumentation Semantics).\nFor that purpose, it is necessary to introduce the\nconcept of an abstract argumentation framework\nand argumentation semantics. Originally proposed\nby Dung in (Dung, 1995), an argumentation frame-\nwork is a graph-based representation of abstract\n(i.e., non-structured) arguments and their attack\nrelations:\nDefinition 1 (Argumentation Framework) An\nArgumentation Framework (AF) is a tuple AF =\n< A, R > where: A is a finite set of arguments,\nand R is the attack relation on A such as A × A\n→R.\nFurthermore, argumentation semantics were pro-\nposed along with the AFs as a set of logical rules\nto determine the acceptability of an abstract argu-\nment or a set of arguments. In this paper, following\none of the most popular notations in argumentation\ntheory, we will refer to these sets as acceptable\nextensions. These semantics rely on two essential\nset properties: conflict-freeness and admissibility.\nThus, we can consider that a set of arguments is\nconflict free if there are not any attacks between\narguments belonging to the same set:\nDefinition 2 (Conflict-free) Let AF = < A, R >\nbe an argumentation framework and Args ⊆A.\nThe set of arguments Args is conflict-free iff ¬∃αi,\nαj ∈Args : (αi, αj) ∈R.\nWe can also consider that a set of arguments is\nadmissible if, in addition of being conflict-free, it\nis able to defend itself from external attacks:\nDefinition 3 (Admissible) Let AF = < A, R >\nbe an argumentation framework and Args ⊆A.\nThe set of arguments Args is admissible iff Args\nis conflict-free, and ∀αi ∈Args, ¬∃αk ∈A :\n(αk, αi) ∈R and (αi, αk) /∈R, or ¬∃αj ∈Args :\n(αj, αk) ∈R\nIn this paper, we compare the behaviour of these\ntwo properties through the use of Naïve (conflict-\nfree) and Preferred (admissible) semantics to com-\npute all the acceptable extensions of arguments\nfrom the AF representations of debates. Naïve se-\nmantics are defined as maximal (w.r.t. set inclusion)\nconflict-free sets of arguments in a given AF. Simi-\nlarly, Preferred semantics are defined as maximal\n(w.r.t. set inclusion) admissible sets of arguments\nin a given AF.\nAt this point, it is important to remark that the\ncriteria of selecting both semantics for our method\nis oriented by the principle of maximality. Since\nacceptable extensions will be used as samples to\ntrain the natural language model in the subsequent\nphase of the proposed method, we selected these se-\nmantics that allow us to obtain the highest number\nof extensions, but keeping the most of the natural\nlanguage information and maximising differences\namong the extensions (i.e., not accepting the sub-\nsets of a given maximal extension, which would\nresult in data redundancy and hamper the learning\nprocess of the model).\nAlgorithm 1 Argumentation Framework Encoding.\n1: function GRAPHTOAF(ArgumentGraph)\n2:\nAG ←ArgumentGraph\n3:\nr ←AG.edges(′conflict′)\n4:\nAG.removeEdges(r)\n5:\ncc ←AG.connected_components()\n6:\nAF ←NewGraph()\n7:\nfor subgraph ∈cc do\n8:\narg ←{}\n9:\nfor node ∈subgraph do\n10:\narg.append(node.Data())\n11:\nend for\n12:\nAF.addNode(arg)\n13:\nend for\n14:\nAF.addEdges(r)\n15:\nreturn AF\n16: end function\nTherefore, we encode the argument graphs, re-\nsulting from a natural language analysis of the de-\nbate, as abstract AFs using the proposed Algorithm\n1. ADUs that follow the same line of reasoning (i.e.,\nrelated with inference or rephrase) are grouped into\nabstract arguments, and the existing conflicts be-\ntween ADUs are represented with the attack re-\nlation of the AF. Then, both Naïve and Preferred\nsemantics are computed on the AF representation\nof the debate. This leads to a finite set of extensions,\neach one of them consisting of a set of acceptable\narguments under the logic rules of computational\nargumentation theory. These extensions will be\nused as learning samples for training and evaluat-\ning the natural language model in the subsequent\nphase of the proposed method.\n4.2\nPhase II: Debate Outcome Estimation\nThe second phase of the method focuses on\nanalysing the natural language arguments con-\ntained in the acceptable extensions, and determin-\ning the winner of a given debate. For that pur-\npose, we use the Graph Network (GN) architecture\ncombined with Transformer-based sentence em-\nbeddings generated from the natural language ar-\nguments contained in the acceptable extensions. A\nGN is a machine learning algorithm aimed at learn-\ning computational representations for graph-based\ndata structures (Battaglia et al., 2018). Therefore,\na GN receives a graph as an input containing ini-\ntialised node features (i.e., v1, . . . , vi ∈V ), edge\ndata (i.e., (e1, r1, s1), . . . , (ek, rk, sk) ∈E, where\ne are the edge features, r is the receiver node, and\ns is the sender node), and global features (i.e., u);\nand updates them according to three learnt update\nϕ and three static aggregation ρ functions:\ne′\nk = ϕe(ek, vrk, vsk, u)\n¯e′\ni = ρe→v(E′\ni)\nv′\ni = ϕv(¯e′\ni, vi, u)\n¯v′ = ρe→u(E′)\nu′ = ϕu(¯e′, ¯v′, u)\n¯v′ = ρv→u(V ′)\n(1)\nThis way, ϕe computes an edge-wise update of\nedge features, ϕv updates the features of the nodes,\nand ϕu is computed at the end, updating global\ngraph features. Finally, ρ functions must be com-\nmutative, and calculate aggregated features, which\nare used in the subsequent update functions.\nThus, the first step in Phase II is to build the\nlearning samples from the previously computed\nextensions of AFs (see Figure 1, Learning Sam-\nple). An extension is a set of logically acceptable\narguments under the principles of conflict-freeness\nand/or admissibility. However, there are no explicit\nrelations between the acceptable arguments, since\nAF representations only consider attacks between\narguments, and the conflict-free principle states\nthat there must be no attacks between arguments\nbelonging to the same extension. Thus, in order to\nstructure the data and make it useful for learning\nlinguistic features for the debate evaluation task,\nwe generate a complete bipartite graph from each\nextension. The two disjoint sets of arguments are\ndetermined by their stance (i.e., one set consisting\nof all the acceptable arguments in favour, and the\nother against), since argumentation semantics al-\nlow to define sets of logically acceptable arguments\nbut do not guarantee that they will have the same\nclaim or a similar stance.\nThe second step consists on initialising all the\nrequired features of the learning samples for the\nGN architecture (see Figure 1, Sample Init.). Thus,\nwe define which features will encode edge, node,\nand global information of the previously processed\nbipartite graph samples. Edges do not contain any\nrelevant natural language information, so we ini-\ntialise edge features identically (similar to previ-\nous research (Craandijk and Bex, 2021)), so that\nnode influence can be stronger when learning edge\nupdate functions. Nodes, however, are a pivotal\naspect of this second phase since they contain all\nthe natural language data. Node features are ini-\ntialised from sentence embedding representations\nof the natural language ADUs contained in each\nnode. Thus, we propose the use of a pre-trained\nlanguage model to generate dense vector represen-\ntations of these ADUs, and initialise the vector\nfeatures for learning the task. Finally, the global\nfeatures of our learning samples encode the proba-\nbility distribution of winning/losing a given debate\n(represented as acceptable extension-based bipar-\ntite graphs), and are a binary label that indicates\nthe winning stance (i.e., 0 for the team in favour\nand 1 for the team against).\nThe final step in the Phase II of our proposed\nmethod is focused on learning the automatic evalua-\ntion of argumentative debates (see Figure 1, Graph\nNetwork). In a classical debate, there are always\ntwo teams/stances: in favour and against some spe-\ncific claim. In this paper, we approach the debate\nevaluation as a binary classification task. There-\nfore, at the end of the proposed method, we model\nthe classification problem as follows:\nˆc = arg max\nc∈C\nP(c|G)\n(2)\nwhere C = [“F”, “A”], depending on the win-\nner of each debate (i.e., in “F”avour or “A”gainst).\nAnd G is a complete bipartite graph generated from\nthe acceptable extensions of the AF pre-processing\ndescribed in the Phase I of our method. We ap-\nproach this probabilistic modelling with three Multi\nLayer Perceptrons (MLP) consisting of two layers\nof 128 hidden units for each of the ϕ update func-\ntions. Since the debate evaluation is an instance of\nthe graph prediction task, it is important to point out\nthat the architecture of the two MLP approaching\nϕe and ϕv are equivalent, and their parameters are\nlearnt from the backpropagation of the MLP archi-\ntecture for ϕu. Finally, the proposed GN model has\na 2-unit linear layer (for binary classification) and\na softmax function (for modelling the probability\ndistribution) on its top.\n5\nExperimental Analysis\n5.1\nExperimental Setup\nAll the experiments and results reported in this pa-\nper have been implemented using Python 3 and\nrun under the following setup.\nThe initial cor-\npus pre-processing and data structuring (i.e., Phase\nI) has been carried out using Pandas (McKinney\net al., 2010) together with NetworkX (Hagberg\net al., 2008) libraries. Argumentation semantics\nhave been implemented considering the NetworkX-\nbased AF graph structures. Regarding Phase II,\nthe language model and the dense sentence vec-\ntor embeddings have been implemented through\nthe Sentence Transformers library (Reimers and\nGurevych, 2019). We used a pre-trained XLM-\nRoBERTa architecture (Reimers and Gurevych,\n2020) able to encode multilingual natural language\ninputs into a 768 dimensional dense vector space\n(i.e., word embedding size). Finally, the Jraph3\nlibrary has been used for the implementation of the\ngraph network architecture, for learning its update\nfunctions (i.e., Equation 1), and for the probabilis-\ntic modelling defined in Equation 2. We used an\nIntel Core i7-9700k computer with an NVIDIA\nRTX 3090 GPU and 32GB of RAM to run all\nour experiments.\nThe code implementation of\nthe proposed method and the subsequent exper-\niments is publicly available in https://github.\ncom/raruidol/ArgumentEvaluation.\nIt is also important to completely define the no-\ntion behind a learning sample in the experimental\nsetup, and how the data pipeline manages all these\nsamples and structures them for training/evaluation.\nIn our proposal, we defined a learning sample as\nan acceptable extension of a given debate. Thus,\ndifferent debates may produce a different number\nof learning samples depending on the argumen-\ntation semantics and/or the argumentation frame-\nwork topology. This way, learning samples can\nbe managed from a debate-wise or an extension-\nwise viewpoint. Even though we used the learning\nsamples individually (i.e., extension-wise) for the\ntraining of the proposed models, we will always\nconsider debate-wise partitions of our data in our\nexperimental setup. This decision has been made\nbecause it would be unfair to consider learning\nsamples belonging to the same debate in both our\ntrain and test data partitions. The reported results\ncould be misleading, and would not properly re-\nflect the strengths and weaknesses of our method.\nTherefore, we used 29 complete debates in our ex-\nperiments (18 in favour, 11 against). To create the\ntrain-test data splits, we assigned 23 debates (80%)\nto the train split and 6 debates (20%) to the test\nsplit.\nTo evaluate and validate our proposal, we have\ncalculated the values of the performance scores\naveraged after 3 sequential runs with a different\n3https://jraph.readthedocs.io/\n(random) initialisation. As for the performance\nscores, we have calculated the precision, the recall,\nand the weighted average F1 score in all of our\nexperiments. We decided to report the weighted av-\nerage of the F1 score due to the existing variability\nof the class distributions on the learning samples\ngenerated from different debates included in the\ntest split from one run to another.\n5.2\nBaselines\nWe defined five baselines to compare the perfor-\nmance of our proposed method and validate our\ncontribution. The selected baselines have been\ndefined to provide a better understanding of the\nbenefits of using transversal approaches to a prob-\nlem such as the prediction of the winning stance in\na complete and long argumentative debate, where\ncomplex natural language and argumentative rela-\ntions are present.\nFor that purpose, we have used a random base-\nline (RB) that assigned randomly a class to each\ndebate as our most basic baseline. Moreover, we\nhave considered two baselines that rely exclusively\non concepts from computational argumentation the-\nory (i.e., semantics) to determine the winner of a\ndebate. These are the Naïve argumentation the-\nory baseline (Naïve-ATB) and the Preferred argu-\nmentation theory baseline (Preferred-ATB), which\ncompute a diferent set of acceptable extensions\nbased on the different admissibility principles, and\nthen calculate the majoritarian set of arguments\ngrouped by stance. Therefore, the winning team\nis decided by having more acceptable arguments\nin the extensions produced by the argumentation\nsemantics. To compare argumentation theory ex-\nclusive approaches with algorithms relying exclu-\nsively in NLP techniques, we have defined two\nmore baselines. The first one is the Longformer\n(Beltagy et al., 2020) architecture, which we have\nfine-tuned to predict the winner of a debate directly\nfrom the text transcripts of the debates. We decided\nto use this model since it represents one of the best\noptions to deal with longer texts in the state-of-the-\nart. We fine-tuned the model4 for 3 epochs on our\ntraining data with a learning rate of 5e-5. As for\nthe second natural language baseline, we have used\nthe Graph Network Baseline (GNB), in which we\nignored the Phase I of our proposed method and we\ntrained the graph network directly on the argument\n4https://huggingface.co/allenai/\nlongformer-base-4096\ngraphs resulting from the argumentative analysis.\n5.3\nOur method\nApart from the baselines, we have experimented\nwith two variants of our proposed method: the\nNaïve-GN and the Preferred-GN. In the Naïve-GN,\nwe applied the Naïve semantics during the Phase\nI, and then trained the graph network using the\nlearning samples generated based on the principle\nof conflict-freedom. Conversely, in the Preferred-\nGN approach, we calculated the Preferred exten-\nsions during Phase I and trained the graph network\nwith the learning samples resulting from applying\nthe principle of admissibility to our argumentation\ngraphs. In the end, we obtained a total number\nof 471 Naïve and 32 Preferred extensions from\nthe 29 debates. The 471 Naïve extensions were\ndistributed as follows: 203 learning samples be-\nlonging to class 0 (i.e., in favour team wins in the\n43.1% of the cases), and 268 samples belonging\nto class 1 (i.e., against team wins in the 56.9% of\nthe cases). On the other hand, the 32 Preferred\nextensions were distributed as follows: 19 learn-\ning samples belonging to class 0, and 13 samples\nbelonging to class 1.\nFrom the variations in data distributions in both\napproaches, it is possible to observe how the admis-\nsibility principle is much more strict from a logical\nperspective than the conflict-free principle, and has\na significant repercussion on the number of accept-\nable extensions (i.e., learning samples) produced.\nTherefore, more learning samples are produced by\nthe Naïve semantics which can be leveraged by the\nPhase II of the proposed method.\n5.4\nResults\nAs we can observe in Table 1, the best results in all\nthe evaluation metrics were obtained by the Naïve-\nGN approach. The Preferred-GN was the second\nbest in terms of precision and recall, but performed\nsimilar to the argumentation theory baselines in\nterms of F1. Argumentation theory baselines, how-\never, performed similar to the random baseline in\nall the metrics. This observation is telling us that\nrelying exclusively on formal logic aspects in infor-\nmal environments such as natural language debates,\nmust not be enough to approach linguistically com-\nplex tasks such as the automatic evaluation of natu-\nral language argumentation. Finally, we have also\nbeen able to observe that both baselines relying\nexclusively on NLP algorithms and techniques per-\nformed worse than the random baseline. The main\nExperiment\nEvaluation Metrics\nModel\nPrecision\nRecall\nWeighted-F1\nRB\n0.59\n0.55\n0.55\nNaïve-ATB\n0.58\n0.46\n0.48\nPreferred-ATB\n0.51\n0.55\n0.53\nLongformer\n0.50\n0.25\n0.33\nGNB\n0.29\n0.44\n0.33\nNaïve-GN\n0.64\n0.65\n0.64\nPreferred-GN\n0.62\n0.61\n0.52\nTable 1: Precision, recall, and weighted-F1 results of\nthe automatic debate evaluation task. The reported re-\nsults have been averaged from 3 randomly initialised\nsequential runs.\ncause of this problem can be probably attributed\nto the lack of data in our domain. State-of-the-\nart NLP algorithms rely on large amounts of data,\nwhich we do not have when it comes to analysing\nand evaluating complete professional debates in\nnatural language.\nWe have also detected some interesting findings\nwhen looking specifically at each group of exper-\nimental baselines. Regarding the ATBs, it is pos-\nsible to observe how, after 3 runs, the Preferred-\nATB is consistently providing better results than the\nNaïve-ATB. This finding makes a lot of sense, since\n(despite their bad general performance) relying on\nthe principle of admissibility is more informative\nfrom the argumentative viewpoint than doing it on\nthe principle of conflict-freedom. A similar be-\nhaviour can be found within the NLP baselines.\nWe can observe that the GNB has a significantly\nbetter recall than the Longformer, meaning that\nthe GNB is doing a better generalisation than the\nLongformer. This behaviour can be attributed to\nthe fact that, apart from the limitations in the size\nof the corpus for relying exclusively on state-of-\nthe-art NLP techniques, learning representations\nfrom graph-structured data is a better idea than just\nusing the whole text (i.e., debate transcripts) as the\nunique input for the models.\n5.5\nError Analysis\nApart from looking at the performance scores, we\nhave also analysed the behaviour of the proposed\nmethod and the baselines from the error perspec-\ntive. Figure 2 depicts the aggregated confusion\nmatrices of the three runs reported in Table 1. To\nF\nA\n0.15\n0.32\n0.22\n0.31\nNaïve-ATB\n0.22\n0.28\n0.17\n0.33\nPreferred-ATB\nF\nA\n0.5\n0\n0.5\n0\nLongformer\n0.11\n0.45\n0.11\n0.33\nGraph Network Baseline\nFavour\nAgainst\nF\nA\n0.22\n0.24\n0.1\n0.44\nNaïve-GN\nFavour\nAgainst\n0.5\n0\n0.39\n0.11\nPreferred-GN\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure 2: Aggregated confusion matrices.\nrepresent the error distributions, we have calculated\nthe percentages after 3 runs of debates classified\nas in Favour (F) or Against (A) winning stances.\nWe can observe how the ATBs present an almost\nuniform distribution of the error, similar to what\ncan be expected of a random baseline. This goes\nin line with the reported results in the previous\nsection. Conversely, the Longformer assigned to\nevery debate the F winning stance (i.e., the majority\nclass) giving more emphasis on the needs for larger\ncorpora to make use of LLMs. Finally, we can ob-\nserve the impact of the differences between the use\nof Naïve and Preferred semantics in our methods\non the error distributions. With the Naïve-GN, the\nmodel managed to correctly identify almost all the\ndebates where A was the winning stance, but had\nsome problems when doing it in Favour. On the\nother hand, the Preferred-GN did the opposite, all\nthe debates where F was the winning stance were\ncorrectly classified, but in the case of A debates,\nthe model miss-classified most of them.\n6\nDiscussion\nIn this paper, we have defined an original hybrid\nmethod to approach the winning stance prediction\nof complete natural language argumentative de-\nbates. For that purpose, we present a new instance\nof the debate assessment task, where argumentative\ndebates and their underlying lines of reasoning are\nconsidered in a comprehensive, undivided manner.\nThe proposed method combines aspects from for-\nmal logic and computational argumentation theory,\nwith NLP and Deep Learning. From the observed\nresults, several conclusions can be drawn. First,\nit has been possible to determine that our method\nperformed better than approaching independently\nthe debate evaluation task from either the argu-\nmentation theory or the NLP viewpoints. Further-\nmore, we have observed in our experiments that\nconflict-free semantics produce a higher number\nof acceptable extensions from each AF compared\nto the admissibility-based semantics. This helped\nto improve the learning process of the task in a\nsimilar way to that achieved by data augmentation\ntechniques. Thus, a better probabilistic models of\nnatural language distributions that are not too con-\nstrained to formal logic and graph topology can be\nlearnt by our model.\nThis paper represents a solid starting point of\nresearch in the evaluation of complete natural lan-\nguage professional debates. As future work, we\nare interested in considering finer-grained features\nfor the evaluation of argumentation such as thesis\nsolidity, argumentation quality, and adaptability.\nWe also plan to extend our method with acoustic\nfeatures, considering aspects such as the intonation\nor the fluency.\nLimitations\nThe present paper describes a new method for the\nautomatic evaluation of complete argumentative\ndebates. However, several limitations are described\nalong the paper. Even though the proposed method\ncan be generalised to any argumentative domain,\nthe reported results are constrained to the domain\nof the corpus used in the experiments to validate\nour proposal. As discussed in Section 3, it has not\nbeen possible to identify any other corpus suitable\nfor approaching the task as presented in this work.\nThus, we are not able to evaluate our approach\nwhen considering different argumentation topics\nor domains. Furthermore, we used different base-\nlines to validate the improvements achieved by our\nproposed method, but it was not possible to use pre-\nvious research as reference. Finally, since we used\nreal debates, the experimental configuration was\nalso highly dependent on the specific debate struc-\ntures. Extending the corpus may help to provide\nmore solid results and explore the performance of\nour proposal when generalising to multiple topics\nand domains.\nAcknowledgements\nThis\nwork\nhas\nbeen\ndeveloped\nthanks\nto\nthe\nfunding\nof\nthe\nfollowing\nprojects:\nGrant\nPID2021-123673OB-C31\nfunded\nby\nMCIN/AEI/10.13039/501100011033,\nGrant\nTED2021-131295B-C32\nfunded\nby\nAEI/10.13039/501100011033/ European Union\nNextGenerationEU/PRTR, Spanish Government\nproject PID2020-113416RB-I00, and the ‘AI for\nCitizen Intelligence Coaching against Disinforma-\ntion (TITAN)’ project, funded by the EU Horizon\n2020 research and innovation programme under\ngrant agreement 101070658, and by UK Research\nand innovation under the UK governments Horizon\nfunding guarantee grant numbers 10040483 and\n10055990.\nReferences\nKhalid Al Khatib, Michael Völske, Shahbaz Syed, Niko-\nlay Kolyada, and Benno Stein. 2020.\nExploiting\npersonal characteristics of debaters for predicting\npersuasiveness. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7067–7072, Online. Association for\nComputational Linguistics.\nGianvincenzo Alfano, Sergio Greco, Francesco Parisi,\nand Irina Trubitsyna. 2021. Argumentation frame-\nworks with strong and weak constraints: Semantics\nand complexity. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 35, pages\n6175–6184.\nKatie Atkinson, Pietro Baroni, Massimiliano Gia-\ncomin, Anthony Hunter, Henry Prakken, Chris Reed,\nGuillermo Simari, Matthias Thimm, and Serena Vil-\nlata. 2017.\nTowards artificial argumentation.\nAI\nmagazine, 38(3):25–36.\nJianzhu Bao, Chuang Fan, Jipeng Wu, Yixue Dang, Ji-\nachen Du, and Ruifeng Xu. 2021. A neural transition-\nbased model for argumentation mining. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 6354–6364, Online.\nAssociation for Computational Linguistics.\nRoy Bar-Haim, Lilach Eden, Roni Friedman, Yoav Kan-\ntor, Dan Lahav, and Noam Slonim. 2020. From ar-\nguments to key points: Towards automatic argument\nsummarization. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4029–4039, Online. Association for\nComputational Linguistics.\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Al-\nvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz\nMalinowski, Andrea Tacchetti, David Raposo, Adam\nSantoro, Ryan Faulkner, et al. 2018. Relational in-\nductive biases, deep learning, and graph networks.\narXiv preprint arXiv:1806.01261.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nMartin Caminada. 2006. On the issue of reinstatement\nin argumentation. In European Workshop on Logics\nin Artificial Intelligence, pages 111–123. Springer.\nCarlos Chesnevar, Sanjay Modgil, Iyad Rahwan, Chris\nReed, Guillermo Simari, Matthew South, Gerard\nVreeswijk, Steven Willmott, et al. 2006. Towards\nan argument interchange format. The knowledge en-\ngineering review, 21(4):293–316.\nDennis Craandijk and Floris Bex. 2021. Deep learning\nfor abstract argumentation semantics. In Proceed-\nings of the Twenty-Ninth International Conference\non International Joint Conferences on Artificial Intel-\nligence, pages 1667–1673.\nPhan Minh Dung. 1995. On the acceptability of ar-\nguments and its fundamental role in nonmonotonic\nreasoning, logic programming and n-person games.\nArtificial Intelligence, 77(2):321–357.\nEsin Durmus and Claire Cardie. 2019. A corpus for\nmodeling user and language effects in argumentation\non online debating. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 602–607, Florence, Italy. Associa-\ntion for Computational Linguistics.\nRoxanne El Baff,\nHenning Wachsmuth,\nKhalid\nAl Khatib, and Benno Stein. 2020. Analyzing the Per-\nsuasive Effect of Style in News Editorial Argumenta-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n3154–3160, Online. Association for Computational\nLinguistics.\nMartin Gleize, Eyal Shnarch, Leshem Choshen, Lena\nDankin, Guy Moshkowich, Ranit Aharonov, and\nNoam Slonim. 2019. Are you convinced? choos-\ning the more convincing evidence with a Siamese\nnetwork. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 967–976, Florence, Italy. Association for Com-\nputational Linguistics.\nPierpaolo Goffredo, Elena Cabrio, Serena Villata,\nShohreh Haddadan, and Jhonatan Torres Sanchez.\n2023. Disputool 2.0: A modular architecture for\nmulti-layer argumentative analysis of political de-\nbates. In Proceedings of the AAAI Conference on Ar-\ntificial Intelligence, volume 37, pages 16431–16433.\nAric Hagberg, Pieter Swart, and Daniel S Chult. 2008.\nExploring network structure, dynamics, and func-\ntion using networkx. Technical report, Los Alamos\nNational Lab.(LANL), Los Alamos, NM (United\nStates).\nAnnette Hautli-Janisz, Zlata Kikteva, Wassiliki Siskou,\nKamila Gorska, Ray Becker, and Chris Reed. 2022.\nQt30: A corpus of argument and conflict in broad-\ncast debate. In Proceedings of the 13th Language\nResources and Evaluation Conference, pages 3291–\n3300. European Language Resources Association\n(ELRA).\nFa-Hsuan Hsiao, An-Zi Yen, Hen-Hsen Huang, and\nHsin-Hsi Chen. 2022. Modeling inter round attack of\nonline debaters for winner prediction. In Proceedings\nof the ACM Web Conference 2022, pages 2860–2869.\nYohan Jo, Jacky Visser, Chris Reed, and Eduard Hovy.\n2019. A cascade model for proposition extraction in\nargumentation. In Proceedings of the 6th Workshop\non Argument Mining, pages 11–24, Florence, Italy.\nAssociation for Computational Linguistics.\nIsabelle Kuhlmann and Matthias Thimm. 2019. Us-\ning graph convolutional networks for approximate\nreasoning with abstract argumentation frameworks:\nA feasibility study.\nIn International Conference\non Scalable Uncertainty Management, pages 24–37.\nSpringer.\nJohn Lawrence and Chris Reed. 2019. Argument min-\ning: A survey. Computational Linguistics, 45(4):765–\n818.\nMirki Lenz, Premtim Sahitaj, Sean Kallenberg, Christo-\npher Coors, Lorik Dumani, Ralf Schenkel, and Ralph\nBergmann. 2020.\nTowards an argument mining\npipeline transforming texts to argument graphs. Com-\nputational Models of Argument: Proceedings of\nCOMMA 2020, 326:263.\nMirko Lenz, Stefan Ollinger, Premtim Sahitaj, and\nRalph Bergmann. 2019. Semantic textual similar-\nity measures for case-based retrieval of argument\ngraphs. In Case-Based Reasoning Research and De-\nvelopment: 27th International Conference, ICCBR\n2019, Otzenhausen, Germany, September 8–12, 2019,\nProceedings 27, pages 219–234. Springer.\nLars Malmqvist, Tommy Yuan, Peter Nightingale, and\nSuresh Manandhar. 2020. Determining the accept-\nability of abstract arguments with graph convolu-\ntional networks. In SAFA@ COMMA, pages 47–56.\nSantiago Marro, Elena Cabrio, and Serena Villata. 2022.\nGraph embeddings for argumentation quality assess-\nment. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pages 4154–4164,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nWes McKinney et al. 2010. Data structures for statisti-\ncal computing in python. In Proceedings of the 9th\nPython in Science Conference, volume 445, pages\n51–56. Austin, TX.\nKoh Mitsuda, Ryuichiro Higashinaka, and Kuniko Saito.\n2022. Combining argumentation structure and lan-\nguage model for generating natural argumentative di-\nalogue. In Proceedings of the 2nd Conference of the\nAsia-Pacific Chapter of the Association for Compu-\ntational Linguistics and the 12th International Joint\nConference on Natural Language Processing, pages\n65–71.\nMatan Orbach, Yonatan Bilu, Ariel Gera, Yoav Kantor,\nLena Dankin, Tamar Lavee, Lili Kotlerman, Shachar\nMirkin, Michal Jacovi, Ranit Aharonov, and Noam\nSlonim. 2019. A dataset of general-purpose rebuttal.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 5591–\n5601, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAndreas Peldszus and Manfred Stede. 2013. From ar-\ngument diagrams to argumentation mining in texts:\nA survey. International Journal of Cognitive Infor-\nmatics and Natural Intelligence (IJCINI), 7(1):1–31.\nPeter Potash and Anna Rumshisky. 2017. Towards de-\nbate automation: a recurrent model for predicting\ndebate winners. In Proceedings of the 2017 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 2465–2475, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nNils Reimers and Iryna Gurevych. 2020.\nMaking\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational\nLinguistics.\nAllen Roush and Arvind Balaji. 2020.\nDebateSum:\nA large-scale argument mining and summarization\ndataset. In Proceedings of the 7th Workshop on Ar-\ngument Mining, pages 1–7, Online. Association for\nComputational Linguistics.\nRamon Ruiz-Dolz, Jose Alemany, Stella M Heras Bar-\nberá, and Ana García-Fornes. 2021a. Transformer-\nbased models for automatic identification of argu-\nment relations: A cross-domain evaluation. IEEE\nIntelligent Systems, 36(6):62–70.\nRamon Ruiz-Dolz, Montserrat Nofre, Mariona Taulé,\nStella Heras, and Ana García-Fornes. 2021b. Vives-\ndebate: A new annotated multilingual corpus of argu-\nmentation in a debate tournament. Applied Sciences,\n11(15):7160.\nEkaterina Saveleva, Volha Petukhova, Marius Mosbach,\nand Dietrich Klakow. 2021. Graph-based argument\nquality assessment. In Proceedings of the Interna-\ntional Conference on Recent Advances in Natural\nLanguage Processing (RANLP 2021), pages 1268–\n1280, Held Online. INCOMA Ltd.\nDaiki Shirafuji, Rafal Rzepka, and Kenji Araki. 2019.\nDebate outcome prediction using automatic persua-\nsiveness evaluation and counterargument relations.\nIn LaCATODA/BtG@ IJCAI, pages 24–29.\nBart Verheij. 2005. Evaluating arguments based on\ntoulmin’s scheme. Argumentation, 19(3):347–371.\nJacky Visser, Barbara Konat, Rory Duthie, Marcin Kos-\nzowy, Katarzyna Budzynska, and Chris Reed. 2020.\nArgumentation in the 2016 us presidential elections:\nannotated corpora of television debates and social me-\ndia reaction. Language Resources and Evaluation,\n54(1):123–154.\nHenning Wachsmuth, Nona Naderi, Yufang Hou,\nYonatan Bilu, Vinodkumar Prabhakaran, Tim Alberd-\ningk Thijm, Graeme Hirst, and Benno Stein. 2017.\nComputational argumentation quality assessment in\nnatural language. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 1, Long Pa-\npers, pages 176–187, Valencia, Spain. Association\nfor Computational Linguistics.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-03-28",
  "updated": "2024-01-21"
}