{
  "id": "http://arxiv.org/abs/2203.15076v1",
  "title": "Neurosymbolic hybrid approach to driver collision warning",
  "authors": [
    "Kyongsik Yun",
    "Thomas Lu",
    "Alexander Huyen",
    "Patrick Hammer",
    "Pei Wang"
  ],
  "abstract": "There are two main algorithmic approaches to autonomous driving systems: (1)\nAn end-to-end system in which a single deep neural network learns to map\nsensory input directly into appropriate warning and driving responses. (2) A\nmediated hybrid recognition system in which a system is created by combining\nindependent modules that detect each semantic feature. While some researchers\nbelieve that deep learning can solve any problem, others believe that a more\nengineered and symbolic approach is needed to cope with complex environments\nwith less data. Deep learning alone has achieved state-of-the-art results in\nmany areas, from complex gameplay to predicting protein structures. In\nparticular, in image classification and recognition, deep learning models have\nachieved accuracies as high as humans. But sometimes it can be very difficult\nto debug if the deep learning model doesn't work. Deep learning models can be\nvulnerable and are very sensitive to changes in data distribution.\nGeneralization can be problematic. It's usually hard to prove why it works or\ndoesn't. Deep learning models can also be vulnerable to adversarial attacks.\nHere, we combine deep learning-based object recognition and tracking with an\nadaptive neurosymbolic network agent, called the Non-Axiomatic Reasoning System\n(NARS), that can adapt to its environment by building concepts based on\nperceptual sequences. We achieved an improved intersection-over-union (IOU)\nobject recognition performance of 0.65 in the adaptive retraining model\ncompared to IOU 0.31 in the COCO data pre-trained model. We improved the object\ndetection limits using RADAR sensors in a simulated environment, and\ndemonstrated the weaving car detection capability by combining deep\nlearning-based object detection and tracking with a neurosymbolic model.",
  "text": "Neurosymbolic hybrid approach to driver collision warning\nKyongsik Yun*a, Thomas Lua, Alexander Huyena, Patrick Hammerb, Pei Wangb\naJet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Dr, Pasadena, CA\n91109, bTemple University, 1801 N Broad St, Philadelphia, PA 19122\nABSTRACT\nThere are two main algorithmic approaches to autonomous driving systems: (1) An end-to-end system in which a single\ndeep neural network learns to map sensory input directly into appropriate warning and driving responses. (2) A mediated\nhybrid recognition system in which a system is created by combining independent modules that detect each semantic\nfeature. While some researchers believe that deep learning can solve any problem, others believe that a more engineered\nand symbolic approach is needed to cope with complex environments with less data. Deep learning alone has achieved\nstate-of-the-art results in many areas, from complex gameplay to predicting protein structures. In particular, in image\nclassification and recognition, deep learning models have achieved accuracies as high as humans. But sometimes it can\nbe very difficult to debug if the deep learning model doesn't work. Deep learning models can be vulnerable and are very\nsensitive to changes in data distribution. Generalization can be problematic. It's usually hard to prove why it works or\ndoesn't. Deep learning models can also be vulnerable to adversarial attacks. Here, we combine deep learning-based\nobject recognition and tracking with an adaptive neurosymbolic network agent, called the Non-Axiomatic Reasoning\nSystem (NARS), that can adapt to its environment by building concepts based on perceptual sequences. We achieved an\nimproved intersection-over-union (IOU) object recognition performance of 0.65 in the adaptive retraining model\ncompared to IOU 0.31 in the COCO data pre-trained model. We improved the object detection limits using RADAR\nsensors in a simulated environment, and demonstrated the weaving car detection capability by combining deep\nlearning-based object detection and tracking with a neurosymbolic model.\nKeywords: deep learning, neurosymbolic model, hybrid deep learning model, non-axiomatic reasoning, driver collision\nwarning\n1. INTRODUCTION\nIn the past few years, deep learning algorithms have achieved state-of-the-art results in many areas including computer\nvision and natural language understanding 1–5. In particular, in image classification and object recognition, deep learning\nmodels have achieved human-level accuracy 6–8. Despite the unprecedented performance of deep learning, there are still\nsome shortcomings. The most important aspect is that deep learning systems lack explainability and reliability when they\ndo not perform well. The complexity of deep learning systems makes it difficult to pinpoint the cause of the problem.\nTherefore, in the autonomous driving industry, major companies, including Tesla (HydraNet), Waymo (ChauffeurNet)\nand Ford/Volkswagen Group (Argo AI), are using modular systems that combine multiple independent modules rather\nthan end-to-end for autonomous driving functions. The \"divide and conquer\" approach helped us identify and fix\nproblems while solving complex problems.\nAnother challenge in deep learning is its high sensitivity to a given data set, which prevents generalization. In\nthe ideal situation where the given training data distribution covers the entire edge case, this should be fine. However, it\nis very difficult to collect sufficient data from the edge cases, especially in the autonomous driving industry, such as\ncrashes, unexpected road conditions, etc.. This is where neurosymbolic hybrid systems can help. Deep learning systems\ncombined with well-defined logic-based systems that establish physical and logical boundary conditions will accelerate\nthe performance of autonomous driving systems even in edge cases. Moreover, a hybrid system would be less\n*kyongsik.yun@jpl.nasa.gov; phone 1 818 354-1468; fax 1 818 393-6752; jpl.nasa.gov\nsusceptible to biased data distribution or adversarial attacks as the system would be controlled and constrained by the\nvalues ​of the logic and the rules of the road, rather than relying solely on the given training data 9,10.\nTo address the limitations of deep learning, we combined deep learning-based object recognition and tracking\nwith an adaptive neurosymbolic network agent called Non-Axiomatic Reasoning System (NARS) 11. The specific topic\nwe were trying to address in this study was driver crash warning, especially for first responders, including emergency\nvehicles and police vehicles. Emergency car accidents in the United States cost $35 billion annually 12. Collision\nfatalities are 4.8 times higher than the national average for first responders. Police officers have twice the rate of car\naccidents per million vehicles they drive than the general public.\nHere, we aim to develop and demonstrate an AI assistant that enables first responders to drive safely to avoid\naccidents in and around high traffic. We implement a hybrid deep learning and neurosymbolic algorithm to provide\nexplainable and trustable autonomy techniques for driver safety recommendations in time-critical first responder\noperations.\n2. CARLA DRIVING SIMULATION ENVIRONMENT\nCARLA is an open source driving simulation software that can simulate various driving environments and crash\nscenarios 13. CARLA supports the development, training and validation of autonomous driving systems. The CARLA\nsystem receives driving scenarios, including user-controlled inputs and predefined multi-vehicle movement scripts, and\noutputs simulated videos and images, as well as various telemetry data. Also in this study, several virtual map creation\ntools were used, including OpenStreetMap 14 for editable world maps, OpenDrive 15 for an open format specification that\ndescribes the logic of road networks, and RoadRunner 16 from MathWorks for designing 3D scenes for automated\ndriving simulations.\nWe designed a CARLA server and client environment, and a neurosymbolic hybrid system (Fig. 1). Each client\nsends user actions and scripted scenario data to the server. And the CARLA server synchronizes with multiple clients to\nsend visible and depth camera streams, dynamic vision sensor camera streams, Global Navigation Satellite System\n(GNSS) telemetry, accelerometer, gyroscope and compass, LIDAR and radar telemetry data to clients. The CARLA\nserver also provides basic signal processing, including collision detection, obstacle detection, semantic LIDAR, and\nsemantic segmentation of objects in the scene.\nFigure 1. CARLA simulation environment, Neurosymbolic Hybrid System, including deep neural network-based perception for object\ndetection and tracking, and Non-Axiomatic Reasoning System (NARS) for collision warning.\nThe CARLA simulator uses a Python development environment to provide an easy interface with external\nmodules and algorithms. Deep learning-based object detection and tracking algorithms have real-time access to\nCARLA's video, LIDAR, and radar data streams, allowing object detection and tracking systems to seamlessly process\nthe data to generate the information needed for the adaptive neurosymbolic system (NARS). Finally, our system provides\ninformation about driver priority and collision warning via NARS (Figure 1).\nWe created various scenarios in the CARLA environment. First responder vehicles can be manually controlled\nas ego vehicles. Non-player character (NPC) vehicles drive, brake and follow waypoints according to a scenario. We set\npass or fail criteria for each scenario to improve the algorithm and avoid collisions. The first scenario was intersection\nnavigation assistance. A first responder vehicle approaches the intersection without slowing down. Another civilian\nvehicle is speeding through the intersection and there is a high risk of colliding with the first responder vehicle. When\nthe first responder vehicle approaches an intersection on the street: NARS searches the crash history database to find\ninformation about past crashes at this intersection as a reference for risk analysis. NARS also checks traffic patterns to\nsee if parameters in the crash history match current traffic patterns. NARS calculates the risk, warns first responders\nahead of potential collisions through intersections, and warns drivers to slow down to a safe speed.\nThe second scenario is stationary vehicle assistance. A first responder vehicle is parked behind civilian vehicles\non the shoulder. If a first responder vehicle attempts to park on the shoulder: NARS searches the crash history database\nto find the crash history of that road as a reference for risk analysis. After matching the time and location to the crash\nrecord, NARS calculates the risk, warns of a potential crash at that location, and advises the driver to move to a safer\nlocation. NARS can provide 360-degree situational awareness for shoulder-parked first responder vehicles. If a nearby\ncivilian vehicle exhibits dangerous behavior, the system will warn the first responder of the risk of a collision in advance.\nCARLA ran on Ubuntu OS 18.04 with AWS (NVIDIA T4 GPU instance). It took roughly 3 hours to plan and\ncreate a new scenario, and 0.5 to an hour to modify an existing one.\n3. DEEP LEARNING-BASED OBJECT DETECTION AND TRACKING\nFor deep learning-based object detection and tracking, the first step is to generate a dataset for training and testing the\nmodel. We generated training data in the CARLA environment. The meaning of generating training data is to label\nobjects with bounding boxes. The first labeling step is to create a bounding box for the vehicle. You can get very\naccurate bounding boxes by pulling information from the CARLA simulation world itself. All vehicles in the CARLA\nenvironment were extracted from each video frame. Then we removed the vehicle that was too far away (200 meters or\nmore). We've also gone through the process of removing vehicles that are obstructing other objects.\nFigure 2. Semantic segmentation of the CARLA simulation environment. We used the segmentation data to localize each object and\nplotted bounding boxes for subsequent deep learning model training. This was done automatically to generate large amounts of\ntraining data in various driving environments..\nBecause CARLA does not provide the same kind of information for non-vehicle actors as vehicles, new data\nextraction methods are required to obtain bounding boxes for all objects other than vehicles. We overlayed the semantic\nsegmentation image provided by CARLA, and used a machine learning clustering algorithm (DBSCAN 17,18) to find each\ncluster of similar color. This allowed us to extract bounding boxes for non-vehicle objects.\nThe objects we target are pedestrians, traffic lights, traffic signs and vehicles. We generated data containing a\ntotal of 4,480 training images and 1,120 test images. Image resolution is important for recognizing distant objects, so we\nsaved the image at the highest possible resolution of 1920 x 1080px. We also organized the training data to be as\ngeneralizable as possible without overfitting the model training by varying weather conditions and times.\nWe collected training data in a CARLA environment for object detection and compared the performance of\neach deep neural network to determine the best model for object detection in our environment. Then, we formatted the\nmodel output for integration with the NARS adaptive neurosymbolic network for alerting first responders. We also\ncompared the accuracy of various CARLA sensors, including vision, radar, LIDAR, and depth cameras for object\ntracking. The CARLA sensor was then integrated with object detection and NARS systems. Finally, the safety of the\ngiven scenario was evaluated.\n4. ADAPTIVE NEUROSYMBOLIC NETWORK\nNon-Axiomatic Reasoning System (NARS) is an intelligent model that can perform learning and reasoning in various\ndomains 11,19. NARS is a system that can draw tentative conclusions from available knowledge and make inferences only\nwith uncertain and incomplete information. NARS can respond quickly even when there is not enough time to\nthoroughly evaluate all possibilities. NARS can not only directly input human logic, but also learn from its own success\nand failure experiences by interacting with the test environment.\nIn this study, NARS takes as input the output of the driving simulator (CARLA) and the output of object\ndetectors and trackers. NARS predicts risk by combining input from multiple forms with background knowledge. A\npredicted risk triggers a warning message to the user. NARS also provides recommendations for avoiding or reducing\npredicted risks. All conclusions generated by the system are attached with confidence and priority values, and the\ndecision logic can be transparently described according to the derivation process.\nEach piece of generative knowledge has a truth value and a priority value, which can be triggered by the\nexample conditions below. 1. A warning is issued when a first responder vehicle enters an intersection at a red light and\nanother vehicle is rapidly approaching from the left or right. 2. If a first responder vehicle is moving, and there is a\npedestrian in front, an alert is issued. 3. If a first responder vehicle is crossing an intersection and another first responder\nvehicle is expected to cross from the left or right, an alert is issued.\nKnowledge can be acquired in several ways. They can be provided directly by human experts, or they can be\nextracted from traffic regulations and laws. We can also acquire knowledge derived from input data or derived from\nother knowledge. The performance of the NARS system improves over time and can adapt to changes in the\nenvironment. NARS receives vehicle type, location, speed and direction information. The knowledge database also\ncollects background knowledge about the missions and notifications to be generated. The output of NARS is the relevant\nnotifications and alerts for first responders. A knowledge base is mission-related knowledge of an intended use case. The\nknowledge base also includes environmental information, including weather conditions, locations of other first\nresponders, and target tasks.\nMoreover, NARS can search the local automobile accident history database and utilize it for risk analysis. It\nwarns of potential collisions in the area and advises drivers. At each stage, the operation of the NARS's experience\nbuffer (judgment, questioning, or goal setting) interacts with the concept of memory according to its rules. Tasks and\nconcepts are selected probabilistically based on their priorities. Factors affecting the priority of an item include its\nquality, past usefulness, and relevance to its current context. NARS processes many concepts in parallel.\nA collection of concepts stored in a NARS-specific data structure is called a \"bag\" and is actively maintained\nand prioritized by the system. Concepts with higher priority are more likely to be selected. Concepts store beliefs (i.e.,\nknowledge of the system) that are used for reasoning. All concepts in NARS are fluid. Its meaning is determined not by\nreferences or definitions, but by experienced relationships with other concepts. The meaning of the concept is changed as\na function of the new experience acquired.\nFigure 3. Non-Axiomatic Reasoning System (NARS) logic example. We define background knowledge and the system learns and\nadapts to new environments and use cases.\n5. TESTING AND EVALUATION\nSeveral object detection models were compared, including CenterNet 20,21, EfficientDet 22,23, and YOLOv4 6. These\nmodels were pre-trained on the COCO17 data set 24. YOLOv4 consistently detected test vehicles with a maximum\ndetection distance of 60.32 m and high confidence of 94% (51.73 m and 45% for CenterNet and 45.38 m and 39% for\nEfficientDet). YOLOv4 was also the first to detect distant objects. CenterNet detected objects approaching 0.46 seconds\nlater than YOLOv4, and EfficientDet detected objects approaching 0.8 seconds later than YOLOv4.\nThen, the YOLOv4 model was further retrained to improve performance on the original COCO17 pretrained\nmodel. Increased maximum object detection distance after retraining from 60.32 m to 88.0 m. We also achieved an\nimproved intersection-over-union (IOU) object recognition performance of 0.65 in the adaptive retraining model\ncompared to IOU 0.31 in the COCO data pre-training model. We also cropped the target areas where vehicles are likely\nto be present. We cropped the image area to 640X640 pixels and used it as the input for the YOLOv4 model, achieving a\nfurther distance detection limit of 135 m.\nOur object detection system can reliably detect objects about 60-80 meters away from the source. At 60 mph,\nthis gives you about three seconds of warning before a crash. In order to increase the warning time before a crash, the\ndetection range should be increased. To this end, additional sensors such as radar, LIDAR, and depth cameras were\napplied and performance improvements were evaluated. As a result of comparing the measured truth distance and the\nradar estimated distance (vertical and horizontal viewing angles of 45 degrees), the errors were 4.7% at 100 m, 6.1% at\n200 m, and 8.3% at 300 m. This is a good reference for applications in a simulation environment, but results may vary in\nreal-world situations. Using LIDAR, we obtained distance estimation errors of 5.2% at 100 meters, 7.7% at 200 meters,\nand 10.3% at 300 meters. We also obtained similar distance estimation error profiles using a depth camera (5.8% for 100\nmeters, 8.2% for 200 meters, and 11.3% for 300 meters).\nIn summary, we found radar achieved a minimum distance error distribution compared to LIDAR or depth\ncameras, so we used radar for object distance calculation and subsequent tracking. Moreover, we can change the angle of\nthe radar field of view, first performing a coarse radar scan of the entire area at a relatively large angle, and then\ndetecting rapidly changing points. It can be used to locate moving objects and narrow the angle of radar scans (fine radar\nscan) for more precise results.\nFigure 4. Weave car detection. We detected a car and tracked the x-coordinate movement frame by frame to determine if the car was\nweaving. We found the weaving from up to 181 meters away.\nWe detected a weaving car as far as 181 meters away (Figure 4). We also compared the vehicle detection\naccuracy based on different distances using radar and LIDAR systems. We found that dynamic radar focusing with\ncoarse and fine radar scan achieved better detection accuracy than LIDAR in all distances from 100 to 300 meters. We\nfound that using radar the maximum distance that we detected using radar was 321 meters.\nTwo scenarios were tested. The first was the risk assessment of the intersection. The system detects approaching\nvehicles and traffic lights, and NARS sends warning signals based on intersection hazards. In the second scenario, the\nfirst responder vehicle is parked on the shoulder of a freeway, and other vehicles approach from behind. The system\ndetected a weaving vehicle approaching from behind and sent an alert 2.7 seconds before the crash (Figure 5).\nFigure 5. Crossroads risk notification (https://giphy.com/gifs/Ljp4ohbFG336R3PHcJ) and weaving car demonstration\n(https://giphy.com/gifs/J9Y2sGnpKo7gGYb96s). Other vehicles and traffic lights were recognized correctly. The blue line is the lane\ndetected behind the first responder vehicle. Red text is a NARS notification that has detected an incoming vehicle hazard.\n6. CONCLUSION\nIn this study, we showed the improved object recognition performance of our proposed adaptive retraining model\ncompared to the performance of the pre-trained model using COCO data. A radar sensor was used to improve the object\ndetection distance limit in a simulated environment, and deep learning-based object detection and tracking was combined\nwith a neurosymbolic model to demonstrate a weaving car detection function.\nThe next step is to create multiple sub-scenarios, increasing the variation of each scenario. The transformation\npoints are: Variety of training data can be generated through variations in map location, vehicle presence and model,\nvehicle location and orientation, weather and lighting, and event timing. We will develop variants in each of these areas,\nand build and test easy, regular, and difficult versions of each of the scenarios. We will also collect more synthetic video\nstreams and images for training and testing. The data sets to collect include vehicles, people, traffic signs, buildings, lane\nmarkings, and lane dividers. Finally, real-world user testing should integrate real-world or virtual-reality driving\nsituations with the head-up display hardware.\nACKNOWLEDGMENT\nThe research was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with\nthe National Aeronautics and Space Administration (80NM0018D0004). The research was funded by the U.S.\nDepartment\nof\nTransportation,\nNational\nHighway Traffic Safety Administration - Vehicle Safety Research\n(DOT-NHTSA) under Task Plan Number 82-106589. We acknowledge the contribution of the system implementation\nby JPL interns - Daniel Lundstrom, Jacques Joubert, Kevin Yu, Kevin Li, Jessica Chen, Vickie Do, Evelyn Chin, Arya\nMevada, Michael Batchev, Peter Isaev, Mina Gabriel, Christian Hahm. We would also like to thank Miami-Dade Police\nDepartment George Perera for providing valuable insight regarding the first response operation.\nREFERENCES\n[1]\nVoulodimos, A., Doulamis, N., Doulamis, A. and Protopapadakis, E., “Deep learning for computer vision: A\nbrief review,” Computational intelligence and neuroscience 2018 (2018).\n[2]\nYun, K., Lu, T. and Huyen, A., “Transforming unstructured voice and text data into insight for paramedic\nemergency service using recurrent and convolutional neural networks,” Pattern Recognition and Tracking XXXI\n11400, 1140006, International Society for Optics and Photonics (2020).\n[3]\nYun, K., “AI for the future of first responders” (2017).\n[4]\nYun, K., “Accelerating autonomy in space using memory-centric architectures for deep neural networks” (2019).\n[5]\nYun, K., Nguyen, L., Nguyen, T., Kim, D., Eldin, S., Huyen, A., Lu, T. and Chow, E., “Small target detection for\nsearch and rescue operations using distributed deep learning and synthetic data generation,” Pattern Recognition\nand Tracking XXX 10995, 1099507, International Society for Optics and Photonics (2019).\n[6]\nBochkovskiy, A., Wang, C.-Y. and Liao, H.-Y. M., “Yolov4: Optimal speed and accuracy of object detection,”\narXiv preprint arXiv:2004.10934 (2020).\n[7]\nYun, K., Huyen, A. and Lu, T., “Deep Neural Networks for Pattern Recognition,” arXiv preprint\narXiv:1809.09645 (2018).\n[8]\nYun, K., Yu, K., Osborne, J., Eldin, S., Nguyen, L., Huyen, A. and Lu, T., “Improved visible to IR image\ntransformation using synthetic data augmentation with cycle-consistent adversarial networks,” presented at\nPattern Recognition and Tracking XXX, 2019, 1099502, International Society for Optics and Photonics.\n[9]\nDang-Nhu, R., “PLANS: Neuro-symbolic program learning from videos,” Advances in Neural Information\nProcessing Systems 33, 22445–22455 (2020).\n[10]\nGarcez, A. d’Avila and Lamb, L. C., “Neurosymbolic AI: the 3rd wave,” arXiv preprint arXiv:2012.05876\n(2020).\n[11]\nHammer, P., Lofthouse, T. and Wang, P., “The OpenNARS implementation of the non-axiomatic reasoning\nsystem,” International conference on artificial general intelligence, 160–170, Springer (2016).\n[12]\nMiller, T. R., Bhattacharya, S., Zaloshnja, E., Taylor, D., Bahar, G. and David, I., “Costs of crashes to\ngovernment, United States, 2008,” Annals of Advances in Automotive Medicine/Annual Scientific Conference\n55, 347, Association for the Advancement of Automotive Medicine (2011).\n[13]\nDosovitskiy, A., Ros, G., Codevilla, F., Lopez, A. and Koltun, V., “CARLA: An open urban driving simulator,”\nConference on robot learning, 1–16, PMLR (2017).\n[14]\nHaklay, M. and Weber, P., “Openstreetmap: User-generated street maps,” IEEE Pervasive computing 7(4), 12–18\n(2008).\n[15]\nDupuis, M., Strobl, M. and Grezlikowski, H., “Opendrive 2010 and beyond–status and future of the de facto\nstandard for the description of road networks,” Proc. of the Driving Simulation Conference Europe, 231–242\n(2010).\n[16]\n“RoadRunner.”, <https://www.mathworks.com/products/roadrunner.html> (8 March 2022 ).\n[17]\nKhan, K., Rehman, S. U., Aziz, K., Fong, S. and Sarasvady, S., “DBSCAN: Past, present and future,” The fifth\ninternational conference on the applications of digital information and web technologies (ICADIWT 2014),\n232–238, IEEE (2014).\n[18]\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P. and Xu, X., “DBSCAN revisited, revisited: why and how you\nshould (still) use DBSCAN,” ACM Transactions on Database Systems (TODS) 42(3), 1–21 (2017).\n[19]\nWang, P., “Non-axiomatic reasoning system (version 4.1),” AAAI/IAAI, 1135–1136 (2000).\n[20]\nDuan, K., Bai, S., Xie, L., Qi, H., Huang, Q. and Tian, Q., “Centernet: Keypoint triplets for object detection,”\nProceedings of the IEEE/CVF international conference on computer vision, 6569–6578 (2019).\n[21]\nXu, Z., Hrustic, E. and Vivet, D., “Centernet heatmap propagation for real-time video object detection,”\nEuropean conference on computer vision, 220–234, Springer (2020).\n[22]\nSong, S., Jing, J., Huang, Y. and Shi, M., “EfficientDet for fabric defect detection based on edge computing,”\nJournal of Engineered Fibers and Fabrics 16, 15589250211008346 (2021).\n[23]\nTan, M., Pang, R. and Le, Q. V., “Efficientdet: Scalable and efficient object detection,” Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, 10781–10790 (2020).\n[24]\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. and Zitnick, C. L., “Microsoft\ncoco: Common objects in context,” European conference on computer vision, 740–755, Springer (2014).\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-03-28",
  "updated": "2022-03-28"
}