{
  "id": "http://arxiv.org/abs/2203.15796v2",
  "title": "Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition",
  "authors": [
    "Junrui Ni",
    "Liming Wang",
    "Heting Gao",
    "Kaizhi Qian",
    "Yang Zhang",
    "Shiyu Chang",
    "Mark Hasegawa-Johnson"
  ],
  "abstract": "An unsupervised text-to-speech synthesis (TTS) system learns to generate\nspeech waveforms corresponding to any written sentence in a language by\nobserving: 1) a collection of untranscribed speech waveforms in that language;\n2) a collection of texts written in that language without access to any\ntranscribed speech. Developing such a system can significantly improve the\navailability of speech technology to languages without a large amount of\nparallel speech and text data. This paper proposes an unsupervised TTS system\nbased on an alignment module that outputs pseudo-text and another synthesis\nmodule that uses pseudo-text for training and real text for inference. Our\nunsupervised system can achieve comparable performance to the supervised system\nin seven languages with about 10-20 hours of speech each. A careful study on\nthe effect of text units and vocoders has also been conducted to better\nunderstand what factors may affect unsupervised TTS performance. The samples\ngenerated by our models can be found at\nhttps://cactuswiththoughts.github.io/UnsupTTS-Demo, and our code can be found\nat https://github.com/lwang114/UnsupTTS.",
  "text": "Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech\nRecognition\nJunrui Ni1∗, Liming Wang1∗, Heting Gao1∗, Kaizhi Qian2, Yang Zhang2, Shiyu Chang3, Mark\nHasegawa-Johnson1\n1University of Illinois at Urbana-Champaign\n2MIT-IBM Watson AI Lab 3University of California, Santa Barbara\n{junruin2,lwang114,hgao17,jhasegaw}@illinois.edu, {kqian, yang.zhang2}@ibm.com,\nchang87@ucsb.edu\nAbstract\nAn unsupervised text-to-speech synthesis (TTS) system learns\nto generate speech waveforms corresponding to any written\nsentence in a language by observing: 1) a collection of un-\ntranscribed speech waveforms in that language; 2) a collec-\ntion of texts written in that language without access to any\ntranscribed speech.\nDeveloping such a system can signiﬁ-\ncantly improve the availability of speech technology to lan-\nguages without a large amount of parallel speech and text data.\nThis paper proposes an unsupervised TTS system based on an\nalignment module that outputs pseudo-text and another syn-\nthesis module that uses pseudo-text for training and real text\nfor inference.\nOur unsupervised system can achieve com-\nparable performance to the supervised system in seven lan-\nguages with about 10-20 hours of speech each. A careful study\non the effect of text units and vocoders has also been con-\nducted to better understand what factors may affect unsuper-\nvised TTS performance. The samples generated by our mod-\nels can be found at https://cactuswiththoughts.\ngithub.io/UnsupTTS-Demo, and our code can be found\nat https://github.com/lwang114/UnsupTTS.\nIndex Terms: speech synthesis, speech recognition, unsuper-\nvised learning\n1. Introduction\nText-to-speech (TTS) synthesis is an essential component of a\nspoken dialogue system.\nWhile capable of generating high-\nﬁdelity, human-like speech for languages such as English and\nMandarin, the existing state-of-the-art TTS systems such as\nTacotron 1&2 [1, 2], Deep Voice 3 [3], FastSpeech [4] and\nTransformer TTS [5] are trained with a large amount of parallel\nspeech and textual data. The reliance on a large amount of tran-\nscribed speech makes such systems impractical for the majority\nof the languages in the world. Training a supervised text-to-\nspeech (TTS) system requires dozens of hours of single-speaker\nhigh-quality recordings [6], but collecting a large amount of\nsingle-speaker, clean, and transcribed speech corpus can be\nquite time-consuming and expensive [7]. A potential way to\nrelax such a requirement is to use non-parallel untranscribed\nspeech and text corpora in the same language. Such corpora\nare much easier to obtain in practice since no human annota-\ntors are required in the data collection process, thanks to the\nabundance of text data on the Internet. Learning to perform\nTTS using non-parallel speech and text, or unsupervised TTS,\nposes unique challenges: ﬁrst, standard supervised training cri-\nteria such as autoregressive mean-squared error are no longer\n*These authors contributed equally to this work\napplicable; further, to learn a latent alignment between the spo-\nken frames and text units, the model now needs to search over\nevery utterance and every transcript in the entire corpus instead\nof limiting the search space within a single utterance-transcript\npair.\nThis paper proposes the ﬁrst model for solving the unsu-\npervised TTS problem. We decompose training the model into\ntwo tasks, learning an alignment module that assigns a sin-\ngle pseudo-transcript to each utterance and learning a synthesis\nmodule that learns from pseudo-text and utterance pairs. The\nalignment module is motivated by the best publicly available\nunsupervised ASR system, wav2vec-U [8], and can generalize\nseamlessly to future updates of the wav2vec-U model. We con-\nduct our unsupervised TTS experiments on seven languages.\nWe further provide an in-depth analysis of the effect of several\ncomponents on unsupervised TTS performance, including the\ngrapheme-to-phoneme (G2P) converter and the vocoder.\n2. Related works\nSeveral recent works have attempted to develop TTS systems\nfor low-resource scenarios. One direction of research is to re-\nplace ground truth phoneme or grapheme labels required for su-\npervised TTS with other units obtained with less or no supervi-\nsion, such as articulatory features [9], or acoustic units discov-\nered by self-supervised speech representation models such as\nvector-quantized variational auto-encoder (VQ-VAE) [10, 11]\nand HuBERT [12, 13, 14]. The unsupervised, textless approach\ncan be applied to any language, including those without any\nwritten form. However, the performance of such a system is\nlimited by the quality of the acoustic units used, which can be\nquite noisy due to the difﬁculty of acoustic unit discovery. To\naddress this limitation, [15, 16, 17, 18] have studied the use\nof other sensory modalities such as images in place of textual\ntranscripts as a weaker form of supervision for conditional gen-\neration of speech, or “TTS without T”, using various attention\nmechanisms over the visual features. Another approach to ad-\ndress this issue is to allow a small amount of transcribed speech\nand train the TTS in a semi-supervised fashion [19, 6]. Speciﬁ-\ncally, [19] leveraged unpaired speech and text data by construct-\ning pseudo-corpora via dual transformation between ASR and\nTTS systems with on-the-ﬂy reﬁnement followed by knowledge\ndistillation, while LRSpeech [6] trained an ASR and a TTS\nsystem that used only several minutes of paired single-speaker,\nhigh-quality speech for TTS, and several hours of low-quality,\nmulti-speaker data for ASR.\nOur approach is motivated by the most recently published\nunsupervised automatic speech recognition (ASR) system [8],\narXiv:2203.15796v2  [eess.AS]  15 Aug 2022\nFigure 1: Network architecture for unsupervised speech synthesis, splitted into an alignment module (left) and a synthesis module\n(right)\nwhich learns to recognize phones by leveraging pre-trained\nspeech representations and an unpaired text corpus.\nOther\nworks on unsupervised ASR typically try to match the empir-\nical prior and posterior distributions of phonemes either using\ncross-entropy [20] or adversarial loss [21].\nUsing powerful\nself-supervised, pre-trained acoustic features such as wav2vec\n2.0 [22] and a generative adversarial network (GAN)-based sys-\ntem, the adversarial approach with self-training achieves com-\nparable performance to its supervised counterpart on large-scale\nspeech datasets for multiple languages [8].\n3. Proposed method\nThe proposed unsupervised TTS system contains two modules:\nan alignment module to obtain pseudo-text for each utterance\nand a synthesis module that trains on pseudo-text. We evaluate\nthe proposed unsupervised TTS system in English, as well as in\nsix other languages (Hungarian, Spanish, Finnish, German, and\nJapanese).\n3.1. Alignment Module\nMotivated by wav2vec-U [8], our alignment module greedily\nproposes a pairing relationship (which we denote as an align-\nment) between real speech utterances and pseudo-transcripts.\nThe alignment module follows a two-step approach:\nGAN\ntraining and self-training. In the GAN training step, a 1-layer\nCNN acts as the generator, which takes the segment representa-\ntions extracted from a pre-trained wav2vec 2.0 model [22] and\noutputs a sequence of distributions over text units, where con-\nsecutive segments with the same argmax value are collapsed.\nThe discriminator, a 3-layer CNN trained against the genera-\ntor, tries to tell which source (real or generated) the input se-\nquence is from.\nThis is achieved by iteratively maximizing\nthe likelihood of the generated phoneme sequence to train the\ngenerator and minimizing the binary cross-entropy loss to train\nthe discriminator. In addition, since GAN training can be very\nunstable, we search over the weights for regularization losses\nsuch as gradient penalty loss, segment smoothness penalty, and\nphoneme diversity loss as described in [8]. We also validate\nthe model with 50-100 transcribed utterances from the corpus\nto ensure convergence instead of using the unsupervised metric\nin [8]. After GAN training, greedy decoding is applied to the\ngenerator’s output over the training set. We then train a hid-\nden Markov model (HMM) with framewise speech representa-\ntions extracted from a wav2vec 2.0 model as input and pseudo-\ntext decoded by the generator as output. Finally, we decode\nthe entire corpus again using the newly-trained HMM to obtain\npseudo-transcripts for the supervised TTS system. Except for\nEnglish, we opt not to further ﬁne-tune a wav2vec 2.0 model\nwith the pseudo-transcripts from the HMM.\n3.2. Synthesis Module\nThe synthesis module uses the proposed alignment pairs from\nthe alignment module and learns to synthesize speech from\npseudo-text. Our synthesis module is motivated by Tacotron\n2 [2] with guided attention loss [23].\nDuring training, we\nperform an unsupervised model selection process by feeding\nthe module with pseudo-transcripts when computing validation\nloss. During evaluation, ground truth transcripts are used as in-\nputs to the synthesis module. Character error rates (CER) and\nword error rates (WER) are used to measure how much linguis-\ntic content is preserved by the synthesis module. We train a\nfully supervised TTS system using real text instead of pseudo-\ntext and calculate the CER and WER on the same subset for\ncomparison. To obtain the CER and the WER on each language,\nwe either directly use a publicly available wav2vec 2.0 speech\nrecognizer (for English) or ﬁne-tune a pre-trained wav2vec 2.0\nmodel on each language individually.\n4. Experiments\n4.1. Unsupervised TTS on English\nWe ﬁrst evaluated the two-stage unsupervised TTS system on\nEnglish. To train the alignment module, we used speech ut-\nterances from the 24-hour single-speaker LJSpeech corpus [24]\nand text samples from the LibriSpeech language modeling cor-\npus [25]. We set aside about 300 utterances for validation and\nabout 500 utterances for testing. We kept the ground truth tran-\nscripts for validation and test sets and used the rest for training\nwithout ground truth transcripts. The speech representations\nwere extracted using a publicly available wav2vec 2.0 Large\nmodel trained on LibriLight [26], and the segment representa-\ntions were built following the pre-processing procedures in [8].\nThe non-parallel text samples used for training, as well as\nthe ground truth transcripts for the validation and test utter-\nances, were converted to phones using a grapheme-to-phoneme\n(G2P) converter [27]. The best weights for the auxiliary penal-\nTable 1: Alignment module results on the LJSpeech dataset us-\ning English wav2vec 2.0 pre-trained features\nLanguage\nDuration (hr)\nUnsup ASR (PER)\nNo ST\nST\nEnglish\n24\n12.37\n3.59\nTable 2: Unsupervised TTS results on the LJSpeech dataset us-\ning English wav2vec 2.0 pre-trained features\nLanguage\nUnsup TTS\nSupervised TTS\nCER\nWER\nCER\nWER\nEnglish\n4.56\n11.95\n3.93\n10.76\nties of the GAN system, i.e., code penalty, gradient penalty, and\nsmoothness weight, c.f. [8], were determined by grid search,\nand we chose the best model based on its PER over the valida-\ntion set after 150k steps with a batch size of 160. GAN train-\ning is sometimes unstable in ways that we could only detect by\nusing 50-100 supervised validation examples, which were the\nonly places during training where we used paired data. The re-\nsults of this stage is shown in Table 1. After determining the\nbest GAN model, its output phone sequence was then reﬁned\nusing a self-training (ST) process [8] as follows. First, we used\nframewise wav2vec 2.0 features after PCA transformation as in-\nput and pseudo phone sequences transcribed by the generator as\ntargets to train a triphone HMM. The triphone output from the\nHMM was decoded into words with an HCLG decoding graph,\nand we further ﬁne-tuned a wav2vec 2.0 Large model using the\npseudo character targets obtained from the above step, under the\nConnectionist Temporal Classiﬁcation (CTC) loss [28]. Both\nsteps were validated with the corresponding pseudo-text for the\nvalidation set. As shown in Table 1, ST reduces the phone error\nrate on the test set by 70% relative and provides very accurate\ntranscripts for the second-stage TTS system. We used the pub-\nlicly available wav2vec-U model in the Fairseq toolkit [29] to\ntrain the GAN and used the Kaldi toolkit [30] to train the tri-\nphone HMM and to build the decoding graph.\nTo train the synthesis module, we used the Tacotron 2\n[2] implementation in ESPnet [31]. The ESPnet implementa-\ntion follows the original Tacotron 2 model, except that another\nguided attention loss [23] was calculated on top of the encoder-\ndecoder attention matrix so that it is not too far from being\ndiagonal. During training, the synthesis module takes pseudo\nphone transcripts as inputs, and outputs 80-dimensional mel-\nspectrograms. These pseudo phone transcripts are converted\nby G2P from the word-level hypotheses generated by the ﬁne-\ntuned wav2vec 2.0 model (in the ﬁnal step of alignment mod-\nule training). The synthesis module was trained for 80 epochs,\nwith the same validation and test splits used for training the\nalignment module.\nDuring validation of the synthesis mod-\nule, we calculated the reconstruction loss based on pseudo-text\ninstead of real text. During testing, we fed the trained syn-\nthesis module with real, phonemicized text transcripts for the\ntest set to obtain mel-spectrograms and synthesized raw audios\nwith HiFi-GAN [32]. We calculated the CERs and raw WERs\nwithout additional language models using a publicly available\nwav2vec 2.0 Large model ﬁne-tuned on LibriSpeech. Table 2\nshows the two error rates on the synthesized test utterances\n512\n1024\n2048\n4096\nHz\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nTime\n512\n1024\n2048\n4096\nHz\nFigure 2: Mel-spectrograms for ground truth (upper) and syn-\nthetic speech by the unsupervised TTS model (lower) for the\nEnglish sentence “in being comparatively modern.”\nusing our proposed unsupervised system (Unsup TTS). Com-\npared with a fully-supervised Tacotron 2 model trained and val-\nidated with real, phonemicized text transcripts, our unsuper-\nvised system only lags behind 0.63% absolute in terms of CER\nand 1.19% absolute in terms of WER. Figure 2 plots the mel-\nspectrogram of a synthetic speech example by our unsupervised\nmodel, which shows that except for the temporal patterns, the\nmel-spectrogram by the unsupervised TTS looks very similar to\nthe ground truth with very little loss of linguistic content.\n4.2. Unsupervised TTS on CSS10 Languages\nTable 3: Unsupervised ASR results on the CSS10 dataset using\nEnglish wav2vec 2.0 pretrained features\nLanguage\nDuration (hr)\nUnsup ASR (CER)\nNo ST\nST\nJapanese\n15\n26.12\n17.81\nHungarian\n10\n25.08\n15.26\nSpanish\n24\n20.80\n14.57\nFinnish\n10\n29.78\n21.00\nGerman\n17\n26.31\n19.47\nDutch\n14\n45.65\n39.24\nTable 4: Unsupervised TTS results on the CSS10 dataset using\nEnglish wav2vec 2.0 pretrained features\nLanguage\nUnsup TTS\nSupervised TTS\nCER\nWER\nCER\nWER\nJapanese\n17.98\n47.81\n17.87\n36.23\nHungarian\n27.78\n76.82\n18.05\n63.14\nSpanish\n23.03\n55.52\n18.19\n36.74\nFinnish\n36.05\n84.46\n22.84\n58.67\nGerman\n17.25\n56.78\n11.28\n40.94\nDutch\n53.01\n89.41\n34.53\n76.71\nWe evaluated our unsupervised TTS system on six addi-\ntional languages: Japanese, Hungarian, Spanish, Finnish, Ger-\nman and Dutch from the CSS10 dataset [33]. The total duration\nof each language is listed in Table 3. The experiments followed\nsimilar steps as the English experiment in Sec 4.1. The align-\nTable 5: The effect of different pretrained vocoders (Grifﬁn-Lim,\nHiFi-GAN) on unsupervised TTS results for LJSpeech and var-\nious languages from CSS10\nLanguage\nGrifﬁn-Lim\nHiFi-GAN\nCER\nWER\nCER\nWER\nEnglish\n5.02\n12.83\n4.56\n11.95\nJapanese\n17.98\n47.81\n20.58\n54.09\nHungarian\n27.78\n76.82\n26.92\n76.60\nSpanish\n23.03\n55.52\n29.41\n68.82\nFinnish\n36.05\n84.46\n37.66\n87.48\nGerman\n17.25\n56.78\n18.45\n59.90\nTable 6: The effect of different text units on unsupervised TTS\nusing Grifﬁn-Lim vocoder\nLanguage\nPhoneme\nGrapheme\nCER\nWER\nCER\nWER\nHungarian\n22.73\n68.80\n27.78\n76.82\nFinnish\n27.58\n67.87\n36.05\n84.46\nDutch\n22.04\n56.85\n53.01\n89.41\nment module extracts speech representations from the same En-\nglish wav2vec 2.0 Large model, followed by GAN training and\nself-training. The results for the alignment module are shown in\nTable 3. There were still a few differences in details in this mul-\ntilingual experiment. Due to resource limits, these multilingual\nexperiments used a potentially easier setting where both the au-\ndio and text were drawn from the same CSS10 dataset with their\npaired relationship broken up, instead of from different datasets\nas for English. We did not convert graphemes into phonemes\nand directly used the characters in each language as text unit.\nWe split the audio and text data into training and validation sets\nwith a ratio of 99 to 1, leaving about 50 to 100 validation ut-\nterances depending on the dataset size. The self-training step\nof the ﬁrst stage only contained a character-based HMM (in-\nstead of a triphone HMM with HCLG decoding) for generating\npseudo-text, and we did not have a second step of ﬁne-tuning\na wav2vec 2.0 model as for English. During the evaluation of\nthe synthesis module, we used a Grifﬁn-Lim vocoder to syn-\nthesize the audios from the generated mel-spectrogram, and the\nresults reported in Table 4 were calculated using audios from\nthe Grifﬁn-Lim vocoder instead of the HiFi-GAN vocoder. We\nswitched to the Grifﬁn-Lim vocoder because we empirically\nfound that it yielded lower error rates on these languages. To\ncalculate CER and raw WER, we ﬁne-tuned a publicly avail-\nable wav2vec 2.0 Base model for each language individually,\nusing paired speech and character-level transcripts from each\nCSS10 corpus.\nThe multilingual results in Table 4 conﬁrm the conclusions\nwe reach in the English experiments. Although the self-training\nstep is simpliﬁed to only a character-based HMM, the self-\ntraining step still greatly reduces the error rates by 25% to 40%\nrelative to all the languages. Compared to the fully-supervised\nTacotron 2 models trained using real text transcripts, the CERs\nof our unsupervised systems differ by only about 9% absolute\non average while requiring only a few paired utterances during\nvalidation. Further, we observe that the gap in WER between\nsupervised and unsupervised TTS systems generally is about\n10-20% absolute for all languages except Finnish, a much larger\ngap than CER. We hypothesize that it may be due to the lack of\na robust language model in the TTS systems, making it harder\nfor the model to preserve word-level information when training\nwith noisy (pseudo-)transcripts. Last but not least, we observe\nthat the performance of the alignment module does not always\nlimit the performance of unsupervised TTS. In the case of Ger-\nman, the TTS trained with pseudo-transcripts achieves a lower\nCER compared to the alignment module alone, which suggests\nthat the TTS has some internal mechanism to correct the noise\nin the pseudo-transcripts.\n4.3. Comparison Between Grifﬁn-Lim and HiFi-GAN\nA comparison between the error rates of using Grifﬁn-Lim and\nHiFi-GAN vocoders is presented in Table 5. We observe that\nthe Grifﬁn-Lim vocoder yields lower CERs and WERs than the\nHiFi-GAN vocoder in all languages except English and Hun-\ngarian, even though informal listening suggests that HiFi-GAN\ngenerates more natural speech with fewer artifacts. We hypoth-\nesize that HiFi-GAN works better for English because it is pre-\ntrained on the English LJSpeech dataset and may not generalize\nvery well when applied to datasets of different languages.\n4.4. Comparison Between Phoneme and Grapheme\nWe trained additional phoneme-based unsupervised TTS mod-\nels in Hungarian, Finnish, and Dutch to study how the text\nunits affect system performance. The training procedure was\nthe same as that described in Sec 4.2, except that for train-\ning the alignment module, we converted the language-speciﬁc\ngraphemes to the phonetic annotations, i.e., the International\nPhonetic Alphabet, using LanguageNet G2Ps [34]. We then use\nthe phonetic outputs from the HMM within the alignment mod-\nule to train the synthesis module. The CERs and WERs are re-\nported in Table 6. The table shows that the phone-based systems\nyield signiﬁcantly lower error rates than the grapheme systems.\nAs graphemes are the smallest functional unit of a writing sys-\ntem, it involves extra complexity on top of the phone systems.\nThus, modeling the grapheme systems is harder than modeling\nthe phone systems, as indicated by its higher error rates. The\ngap between grapheme and phoneme systems is considerably\nsmaller for Hungarian and Finnish than for Dutch. One proba-\nble explanation is that spelling and phonetic transcription is far\nmore regular for the former two languages than for Dutch.\n5. Conclusions\nIn this work, we combined an alignment module and a synthesis\nmodule to build a unsupervised TTS system that trains without\npaired data. The ﬁnal unsupervised TTS system demonstrates\ncompetitive intelligibility in English and a slight degradation\nin intelligibility in six other languages on the level of super-\nvised TTS models. We further show that phonemes work better\nthan graphemes as text units for our systems. In the future, we\nwould like to explore unsupervised TTS with truly non-parallel\ndatasets for languages other than English and ways to improve\nthe stability for the alignment module.\n6. Acknowledgements\nThis work is supported by IBM-UIUC Center for Cognitive\nComputing Systems Research (C3SR). We would like to thank\none anonymous reviewer for insights on Sec 4.4.\n7. References\n[1] Y. Wang, D. S. RJ Skerry-Ryan, Y. Wu, R. J. Weiss, N. Jaitly,\nZ. Yang, Y. Xiao, Z. Chen, S. Bengio et al., “Tacotron: Towards\nend-to-end speech synthesis,” in arXiv, 2017. [Online]. Available:\npreprintarXiv:1703.10135\n[2] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y. Zhang, Y. Wang, R. Skerry-Ryan, R. A. Saurous,\nY. Agiomyrgiannakis, and Y. Wu1, “Natural tts synthesis by con-\nditioning wavenet on mel spectrogram predictions,” in ICASSP,\n2018.\n[3] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan,\nS. Narang, J. Raiman, and J. Miller, “Deep voice 3: 2000-speaker\nneural text-to-speech,” in ICLR, 2018.\n[4] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y.\nLiu, “Fastspeech: Fast, robust and controllable text to speech,” in\nAdvances in Neural Information Processing Systems, 2019.\n[5] N.Li, S.Liu, Y.Liu, S.Zhao, and M.Liu, “Neural speech synthesis\nwith transformer network,” in AAAI, vol. 33, 2019, p. 6706–6713.\n[6] J. Xu, X. Tan, Y. Ren, T. Qin, J. Li, S. Zhao, and T. Liu, “LR-\nSpeech: Extremely low-resource speech synthesis and recogni-\ntion,” in KDD, 2020, pp. 2802–2812.\n[7] K. Park and T. Mulc, “CSS10: A collection of single speaker\nspeech datasets for 10 languages,” Interspeech, 2019.\n[8] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, “Unsupervised\nspeech recognition,” in Neural Information Processing Systems,\n2021.\n[9] P. K. Muthukumar and A. W. Black, “Automatic discovery of a\nphonetic inventory for unwritten languages for statistical speech\nsynthesis,” in ICASSP, 2014, pp. 2594–2598.\n[10] A. H. Liu, T. Tu, H. Lee, and L. Lee, “Towards unsupervised\nspeech recognition and synthesis with quantized speech represen-\ntation learning,” in ICASSP, 2020, pp. 7259–7263.\n[11] H. Zhang and Y. Lin, “Unsupervised learning for sequence-to-\nsequence text-to-speech for low-resource languages,” in Inter-\nspeech, 2020, pp. 3161–3165.\n[12] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak,\nB. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed,\nand E. Dupoux, “On generative spoken language modeling\nfrom raw audio,” in arXiv, 2021. [Online]. Available:\nhttps:\n//arxiv.org/pdf/2102.01192.pdf\n[13] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N.\nHsu, A. Mohamed, and E. Dupoux, “Speech resynthesis from\ndiscrete disentangled self-supervised representations,” in arXiv,\n2021. [Online]. Available: https://arxiv.org/pdf/2104.00355.pdf\n[14] E.\nKharitonov,\nA.\nLee,\nA.\nPolyak,\nY.\nAdi,\nJ.\nCopet,\nK.\nLakhotia,\nT.-A.\nNguyen,\nM.\nRivi`ere,\nA.\nMohamed,\nE. Dupoux, and W.-N. Hsu, “Text-free prosody-aware generative\nspoken language modeling,” in arXiv, 2021. [Online]. Available:\nhttps://arxiv.org/pdf/2109.03264.pdf\n[15] M. Hasegawa-Johnson, A. Black, L. Ondel, O. Scharenborg, and\nF. Ciannella, “Image2speech: Automatically generating audio de-\nscriptions of images,” in ICNLSSP, 2017, p. 1–5.\n[16] X. Wang, S. Feng, J. Zhu, M. Hasegawa-Johnson, and O. Scharen-\nborg, “Show and speak: directly synthesize spoken description of\nimages,” in icassp, 2021.\n[17] W.-N. Hsu, D. Harwath, T. Miller, C. Song, and J. Glass, “Text-\nfree image-to-speech synthesis using learned segmental units,” in\nACL-IJCNLP, 2021, pp. 5284–5300.\n[18] J. Effendi, S. Sakti, and S. Nakamura, “End-to-end image-to-\nspeech generation for untranscribed unknown languages,” IEEE\nAccess, vol. 9, pp. 55 144–55 154, 2021.\n[19] Y. Ren, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, “Almost\nunsupervised text to speech and automatic speech recognition,” in\nICML, 2019, pp. 5410–5419.\n[20] C.-K. Yeh, J. Chen, C. Yu, and D. Yu, “Unsupervised speech\nrecognition via segmental empirical output distribution match-\ning,” in ICLR, 2019.\n[21] K.-Y. Chen, C.-P. Tsai, D.-R. Liu, H.-Y. Lee, and L. shan Lee,\n“Completely unsupervised speech recognition by a generative\nadversarial network harmonized with iteratively reﬁned hidden\nMarkov models,” in Interspeech, 2019.\n[22] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:\nA framework for self-supervised learning of speech representa-\ntions,” in Neural Information Processing Systems, 2020.\n[23] H. Tachibana, K. Uenoyama, and S. Aihara, “Efﬁciently trainable\ntext-to-speech system based on deep convolutional networks with\nguided attention,” in ICASSP, 2018, pp. 4784–4788.\n[24] K. Ito and L. Johnson, “The lj speech dataset,” https://keithito.\ncom/LJ-Speech-Dataset/, 2017.\n[25] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: An ASR corpus based on public domain audio books,”\nin ICASSP, 2015, pp. 5206–5210.\n[26] J. Kahn, M. Rivi`ere, W. Zheng, E. Kharitonov, Q. Xu, P. E.\nMazar´e, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen,\nT. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and\nE. Dupoux, “Libri-light: A benchmark for asr with limited or no\nsupervision,” in ICASSP, 2020, pp. 7669–7673.\n[27] K. Park and J. Kim, “g2pe,” https://github.com/Kyubyong/g2p,\n2019.\n[28] A. Graves, S. Fern´andez, F. J. Gomez, and J. Schmidhuber,\n“Connectionist temporal classiﬁcation: labelling unsegmented se-\nquence data with recurrent neural networks,” in ICML, 2006, pp.\n369–376.\n[29] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grang-\nier, and M. Auli, “fairseq: A fast, extensible toolkit for sequence\nmodeling,” in Proceedings of NAACL-HLT 2019: Demonstra-\ntions, 2019.\n[30] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz,\nJ. Silovsky, G. Stemmer, and K. Vesely, “The kaldi speech recog-\nnition toolkit,” in ASRU, 2011.\n[31] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,\nT. Toda, K. Takeda, Y. Zhang, and X. Tan, “ESPnet-TTS: Uni-\nﬁed, reproducible, and integratable open source end-to-end text-\nto-speech toolkit,” in ICASSP, 2020, pp. 7654–7658.\n[32] J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adversarial\nnetworks for efﬁcient and high ﬁdelity speech synthesis,” in Neu-\nral Information Processing Systems, 2020.\n[33] K. Park and T. Mulc, “Css10: A collection of single speaker\nspeech datasets for 10 languages,” in Interspeech, 2019.\n[34] M. Hasegawa-Johnson, L. Rolston, C. Goudeseune, G.-A. Levow,\nand K. Kirchhoff, “Grapheme-to-phoneme transduction for cross-\nlanguage asr,” in SLSP, 2020, pp. 3–19.\n",
  "categories": [
    "eess.AS",
    "cs.AI"
  ],
  "published": "2022-03-29",
  "updated": "2022-08-15"
}