{
  "id": "http://arxiv.org/abs/2204.01409v1",
  "title": "Safe Controller for Output Feedback Linear Systems using Model-Based Reinforcement Learning",
  "authors": [
    "S M Nahid Mahmud",
    "Moad Abudia",
    "Scott A Nivison",
    "Zachary I. Bell",
    "Rushikesh Kamalapurkar"
  ],
  "abstract": "The objective of this research is to enable safety-critical systems to\nsimultaneously learn and execute optimal control policies in a safe manner to\nachieve complex autonomy. Learning optimal policies via trial and error, i.e.,\ntraditional reinforcement learning, is difficult to implement in\nsafety-critical systems, particularly when task restarts are unavailable. Safe\nmodel-based reinforcement learning techniques based on a barrier transformation\nhave recently been developed to address this problem. However, these methods\nrely on full state feedback, limiting their usability in a real-world\nenvironment. In this work, an output-feedback safe model-based reinforcement\nlearning technique based on a novel barrier-aware dynamic state estimator has\nbeen designed to address this issue. The developed approach facilitates\nsimultaneous learning and execution of safe control policies for\nsafety-critical linear systems. Simulation results indicate that barrier\ntransformation is an effective approach to achieve online reinforcement\nlearning in safety-critical systems using output feedback.",
  "text": "Safe Controller for Output Feedback Linear Systems using Model-Based\nReinforcement Learning\nS M Nahid Mahmud1 Moad Abudia1 Scott A Nivison2 Zachary I. Bell2 Rushikesh Kamalapurkar1\nAbstract\nThe objective of this research is to enable safety-critical systems to simultaneously learn and execute\noptimal control policies in a safe manner to achieve complex autonomy. Learning optimal policies via\ntrial and error, i.e., traditional reinforcement learning, is difﬁcult to implement in safety-critical systems,\nparticularly when task restarts are unavailable. Safe model-based reinforcement learning techniques based\non a barrier transformation have recently been developed to address this problem. However, these methods\nrely on full state feedback, limiting their usability in a real-world environment. In this work, an output-\nfeedback safe model-based reinforcement learning technique based on a novel barrier-aware dynamic state\nestimator has been designed to address this issue. The developed approach facilitates simultaneous learning\nand execution of safe control policies for safety-critical linear systems. Simulation results indicate that\nbarrier transformation is an effective approach to achieve online reinforcement learning in safety-critical\nsystems using output feedback.\nI. INTRODUCTION\nOver the past decade, the topic of safe reinforcement learning has gained a lot of attention in the disciplines of\nrobotics and controls. One of the primary reasons for this is the increase in the expectation of autonomy in safety-\ncritical systems in the real-world tasks. While unmanned autonomous systems have signiﬁcant advantages such\nas repeatability, precision, and lack of physical weariness over their non-autonomous and biological counterparts,\nthey are often costly to construct and restore. To avoid failures during the learning phase, methods that allow the\nunmanned autonomous agents to learn to perform tasks with safety guarantees are needed.\nIn past, Reinforcement learning (RL) has been demonstrated to be an effective approach for synthesizing online\noptimal policies for both known and unknown discrete/continuous-time dynamical systems [1], [2]. However, due\nto sample inefﬁciencies, RL often necessitates a large number of iterations. Model-based reinforcement learning\n(MBRL) techniques can enhance sample efﬁciency in RL [3]–[7]. Generally MBRL techniques guarantee stability,\nnot safety. In recent years, signiﬁcant progress has been made in developing Safe Model-based Reinforcement\n*This research was supported, in part, by the Air Force Research Laboratories under award number FA8651-19-2-0009. Any opinions, ﬁndings,\nor recommendations in this article are those of the author(s), and do not necessarily reﬂect the views of the sponsoring agencies.\n1School of Mechanical and Aerospace Engineering, Oklahoma State University, email: {nahid.mahmud, abudia@okstate.edu,\nrushikesh.kamalapurkar} @okstate.edu.\n2 Air Force Research Laboratories, Florida, USA, email: {scott.nivison, zachary.bell.10} @us.af.mil.\n1\narXiv:2204.01409v1  [eess.SY]  4 Apr 2022\nlearning (SMBRL) techniques to learn safe controllers for different classes of systems [8]–[19]. While Markov\ndecision process (MDP) based SMBRL methods have been available for discrete time systems with ﬁnite state and\naction spaces [8]–[10], synthesizing online controllers for systems in continuous time, under output feedback, while\nguaranteeing stability and safety is still a challenging problem.\nSMBRL techniques that provide provide probabilistic safety guarantees for continuous-time (CT) stochastic\nsystems have recently been studied in results such as [11]–[13]; however, not all applications are conducive to\nprobabilistic safety guarantees. Applications such as manned aviation demand deterministic safety guarantees, and\nas such, online, real-time learning in such systems is challenging. Recently, SMBRL techniques for CT deterministic\nsystems have been studied in results such as [14]–[19]. In [14], a SMBRL method has been developed to synthesis\na real-time safe controller online by incorporating proximity penalty method developed in [20], [21] with the\nframework of control barrier functions. While the control barrier function results in safety guarantees, the existence\nof a smooth value function, in spite of a nonsmooth cost function, needs to be assumed [18].\nThis paper is inspired by the nonlinear coordinate transformation ﬁrst introduced in [22]. Leveraging the results\nof [22], a barrier transformation (BT) to construct an equivalent, unconstrained optimal control problem from a state\nconstrained optimal control problem was introduced in [15]. The unconstrained problem is then solved using an\nadaptive optimal control method under persistence of excitation (PE). To soften the restrictive PE requirement, [16]\nutilized a MBRL formulation to yield a SMBRL technique to synthesize safe controllers. However, the SMBRL\nmethod in [16] requires exact model knowledge; to address this limitation, [18] extended the results in [16] to yield\na SMBRL solution to the online state-constrained optimal feedback control problem under parametric uncertainty\nusing a ﬁltered concurrent learning technique.\nWhile results such as [15]–[18] provide veriﬁed safe feedback controllers, they all rely on full state feedback.\nTo achieve safe learning using output feedback, [19] extended the results in [15]–[18] for nonlinear control-afﬁne\nsystems in Brunovsky canonical form, where the state comprises of the output and its derivatives. This paper\nfocuses on extending the results (OF-SMBRL) in [19] by developing a SMBRL technique for partial observable\nstate-constrained linear systems, not necessarily in the Brunovski canonical form, with a more general output\nequation.\nThe primary challenge in output feedback SMBRL is that the BT preservs neither the linearity, nor the Brunovsky\ncanonical form of the system. As such, observer development in the transformed coordinates is difﬁcult. While state\nestimation can be done in the original coordinates, due to the nature of the BT, small estimation errors in the original\ncoordinates do not translate to small errors in the transformed coordinates. Since the controllers are designed in\ntransformed coordinates, large state estimation errors in the transformed coordinates can yield unexpected results.\nIn this paper, a novel Luenberger-like [23] BT based adaptive observer is designed using Lyapunov methods, and is\nintegrated in a BT-based SMBRL framework, to learn feedback control policies with guaranteed safety and stability\nduring the learning and execution phases.\nIn the following, Section II formalize problem statement. III introduces the BT. Section IV and Section V detail\nthe novel state estimator and an analysis of bounds on the resulting state estimation errors, respectively. Section\nVI describes the novel SMBRL technique for synthesizing feedback control policies in transformed coordinates. In\n2\nSection VII, a Lypaunov-based analysis, in the transformed coordinates, is utilized to establish practical stability of\nthe closed-loop system resulting from the developed SMBRL technique. Guarantees that the safety requirements are\nsatisﬁed in the original coordinates are also established. Simulation results in Section VIII show the performance\nof the developed SMBRL approach.\nII. PROBLEM FORMULATION\nWe consider the following continuous-time linear dynamical system.\n˙x = Ax + Bu,\ny = Cx\n(1)\nwhere x := [x1; . . . ; xn] ∈Rn is the system state, A ∈Rn×n is the transition matrix, B ∈Rn×m is the control\neffectiveness matrix, u ∈Rm is the control input, C ∈Rq×n is output matrix, and y ∈Rq is the measured output.\nTo make the barrier transformation feasible, the following structure is imposed on the problem.\nAssumption 1. The output is comprised of a selection of state variables. That is, every row of the matrix C has\nexactly one element equal to one, and the rest of the elements are zero.\nLet, ˆx := [ˆx1; . . . ; ˆxn] ∈Rn be the estimates of x, where the notation [v; w] is used to denote the vector\n[vT\nwT ]T . The objective is to design an adaptive estimator to estimate the state variables online, using input-\noutput measurements, and to simultaneously estimate and utilize an optimal feedback controller, u such that starting\nfrom a given estimated feasible initial condition x0, the trajectories x(·) decay to the origin and satisfy xi(t) ∈\n(zi, zi), ∀i = 1, . . . , n for some constants zi < 0 < zi. The notation (·)i is used in the rest of the manuscript to\ndenote the ith element of the vector (·), and the notation Io denotes the identity matrix of size o.\nThe following section introduces the barrier transformation and formalizes the connection between trajectories\nof the transformed system and the original system.\nIII. BARRIER TRANSFORMATION\nDeﬁnition 1. Given E < 0 < E , The function b(E ,E ) : R →R, referred to as the barrier function (BF), is deﬁned\nas b(E ,E )(Y ) := log\n\u0010\nE (E −Y )\nE (E −Y )\n\u0011\n.\nThe inverse of the BF on the interval (E , E ) containing the origin is given by b−1\n(E ,E )(Y ) =\n\u0010\nE E\neY −1\nE eY −E\n\u0011\n. In this\npaper, the barrier transformation is a nonlinear coordinate transformation, deﬁned as si := b(zi,zi)(xi),\nand\nxi =\nb−1\n(zi,zi)(si), where si is the ith element of the transformed system state vector, s. Evaluating the derivative of\nthe inverse of the barrier function at si yields\ndb−1\n(zi,zi)(si)\ndsi\n=\n1\nTi(si), where Ti(si) :=\nz2\ni esi−2zizi+z2\ni e−si\nziz2\ni −ziz2\ni\n. Let\nT(s) := [T1(s1); . . . ; Tn(sn)].\nLet z = [z1; . . . ; zn], and z = [z1; . . . ; zn]. In the following, for any vector L , comprised of components of\nx, such that L = [(x)p; . . . ; (x)q], with 1 ≤p ≤q ≤n, the notation b(L ) is used to denote componentwise\napplication of the barrier function with the appropriate limits selected componentwise from the vectors z and z.\nThat is, b(L ) := [b((z)p,(z)p)((x)p); . . . ; b((z)q,(z)q)((x)q)]. Similarly, given any vector L , comprised of components\nof s, such that L = [(s)p, · · · , (s)q]T , with 1 ≤p ≤q ≤n, b−1(L ) := [b−1\n((z)p,(z)p)((s)p); . . . ; b−1\n((z)q,(z)q)((s)q)]T .\n3\nTo transform the dynamics in (1) using the BT, the time derivative of the transformed state, s ∈Rn, can be\ncomputed as\n˙s = T(s) ⊙(Ax + Bu) = F(s) + G(s)u\n(2)\nwhere, ⊙represents Hadamard product [24], F(s) := T(s) ⊙Ab−1(s), G(s) := T(s) ⊙B.\nA. Analysis of trajectories\nIn the following lemma, the trajectories of the original system and the transformed system are shown to be related\nby the barrier transformation provided the trajectories of the transformed system are complete [18].\nLemma 1. If t 7→Φ\n\u0000t, b(x0), ζ\n\u0001\nis a complete Carath´eodory solution to (2), starting from the initial condition\nb(x0), under the feedback policy (s, t) 7→ζ(s, t) and t 7→Λ(t, x0, ξ) is a Carath´eodory solution to (1), starting\nfrom the initial condition x0, under the feedback policy (x, t) 7→ξ(x, t), deﬁned as ξ(x, t) = ζ(b(x), t), then\nt 7→Λ(t, x0, ξ) is complete and Λ(t, x0, ξ) = b−1 \u0000Φ(t, b(x0), ζ)\n\u0001\nfor all t ∈R≥0.\nProof. See [18, Lemma 1].\nIt can be noted that the feedback ξ is well-deﬁned at x only if b(x) is well-deﬁned, which is the case whenever\nx is inside the barrier. As such, the main conclusion of the lemma also implies that Λ(·, x0, ξ) remains inside the\nbarrier. It is thus inferred from Lemma 1 that if the trajectories of (2) are bounded and decay to a neighborhood\nof the origin under a feedback policy (s, t) 7→ζ(s, t), then the feedback policy (x, t) 7→ζ\n\u0000b(x), t\n\u0001\n, when applied\nto the original system in (1), achieves the control objective stated in Section II.\nIn the following section, a state estimator, that allows for rigorous inclusion of state estimation errors in the\nanalysis of the controller, is developed.\nIV. STATE ESTIMATOR\nIn this section, a barrier based adaptive state estimator inspired by Luenberger observer [23] has been designed\nto generate estimates of x. Furthermore, the design is motivated by the need to obtain the error bound in Lemma\n3, and the designed estimator is given by\n˙ˆx = [ ˙ˆx1; · · · ; ˙ˆxn]\n(3)\nwith ˙ˆxi := (Bu)i +\n1\nTi(b(ˆxi))\n\u0012\nAb(ˆx) + L (ym −Cb(ˆx))\n\u0013\ni\n, where, i = [1; · · · ; n], ym = Cb(x), and L ∈Rn×q is\nthe gain matrix.\nTo transform the dynamics in (3) using the BT, the time derivative of the estimated transformed state, ˆs ∈Rn,\ncan be computed as\n˙ˆs = T(ˆs) ⊙(Bˆu) + Aˆs + L (ym −Cb(ˆx))\n= Aˆs + G(ˆs)u + LC˜s.\n(4)\n4\nNote that the relationship b(Cx) = Cb(x), leveraged in the computation above, is only true under Assumption\n1.\nIn the transformed coordinates, the estimator error can be computed as\n˙˜s = F(s) + G(s)u −G(ˆs)ˆu −As + A˜s −LC˜s,\n(5)\nwith ˜s = s −ˆs,\n˙˜s = ˙s −˙ˆs.\nAs detailed in Lemma 2 below, the design of the BT ensures that the trajectories of (1), (2), (3), and (5) are\nlinked by the BT whenever the underlying state trajectories x(·) and s(·) and the initial conditions ˆx0 and ˆs0 are\nlinked by the BT.\nLemma 2. If t 7→Ψ\n\u0000t; b(x(·)), b(ˆx0)\n\u0001\nis a Carath´eodory solution to (5) along the trajectory x(·), starting from the\ninitial condition b(ˆx0), and if t 7→ξ(t; x(·), ˆx0) is a Carath´eodory solution to (3), starting from the initial condition\nˆx0, along the trajectory x(·), then ξ(t; x(·), ˆx0) = b−1\u0000Ψ\n\u0000t; b(x(·)), b(ˆx0)\n\u0001\u0001\nfor all t ∈R≥0.\nProof. See [19, Lemma 2].\nThe following section develops a bound on a Lyapunov-like function of the state estimation errors to be utilized\nin the subsequent stability analysis.\nV. ERROR BOUNDS FOR THE ESTIMATOR\nLemma 3. Let Vse : Rn →R≥0 be a continuously differentiable candidate Lyapunov function deﬁned as Vse(˜s) :=\n˜sT P ˜s where P is a PD matrix and P(A−LC)+(A−LC)T P = −ζ. Provided s, ˆs ∈B(0, χ) for some χ > 0, the\norbital derivative of Vse along the trajectories of ˙˜s can be bounded as ˙Vse ≤−(λmin(ζ) −ϖ2) ∥˜s∥2 +ϖ1∥˜s∥∥s∥+\nϖ3∥˜s∥∥˜Wa∥+ ϖ4∥˜s∥.\nProof. See Appendix A.\nThe following section develops a novel OF-SMBRL technique for synthesizing feedback control policies.\nVI. SAFE MODEL-BASED REINFORCEMENT LEARNING\nIt is immediate from the Lemma 1 that if a feedback controller that stabilizes the transformed system in (2) is\ndesigned, then the same feedback controller, applied to the original system by inverting the BT also achieves the\ncontrol objective stated in Section II. In the following, a controller that practically stabilizes (2) is designed as an\nestimate of a controller that minimizes the inﬁnite horizon cost.\nJ(u(·)) :=\nZ ∞\n0\nc(φ(τ, s0, u(·)), u(τ))dτ,\n(6)\nover the set U of piecewise continuous functions t 7→u(t), subject to (2), where φ(τ, s0, u(·)) denotes the trajectory\nof (2), evaluated at time τ, starting from the state s0, and under the controller u(·). In (6), c(s, u) := Q′(s)+uT Ru,\nwith R ∈Rm×m is a symmetric positive deﬁnite (PD) matrix. For the optimal value function to be a Lyapunov\nfunction for the optimal policy, we are assuming that Q′ is PD. A state penalty function x 7→E(x), given in\n5\nthe original coordinates, can easily be transformed into an equivalent state penalty Q′(s) = E(b−1(s)). Since the\nbarrier function is monotonic and b(0) = 0, if E is positive deﬁnite, then so is Q′. Furthermore, for applications\nwith bounded control inputs, a non-quadratic penalty function similar to [25, Eq. 17] can be incorporated in (6).\nAssuming that an optimal controller exists, let the optimal value function, denoted by V ∗: Rn × Rq →R, be\ndeﬁned as\nV ∗(s) :=\nmin\nu(·)∈U[t,∞)\nZ ∞\nt\nc(φ(τ, s, u[0,τ)(·)), u(·))dτ,\n(7)\nwhere uI and UI are obtained by restricting the domains of u and functions in UI to the interval I ⊆R, respectively.\nAssuming that the optimal value function is continuously differentiable, it can be shown to be the unique PD solution\nof the Hamilton-Jacobi-Bellman (HJB) equation [26, Theorem 1.5]\nmin\nu∈Rq\n\u0010\nVs (F(s) + G(s)u) + Q′(s) + uT Ru\n\u0011\n= 0,\n(8)\nwhere ∇(·) :=\n∂\n∂(·), and V(·) := ∇(·)V . Furthermore, the optimal controller is given by the feedback policy\nu(t) = u∗(φ(t, s, u[0,t))) where u∗: Rn →Rm deﬁned as\nu∗(s) := −1\n2R−1G(s)T (∇sV ∗(s))T .\n(9)\nA. Value function approximation\nConsidering analytical solutions to the HJB problem are often infeasible to compute, particularly for nonlinear\nsystems, parametric approximation methods are utilized to estimate the value function V ∗and the optimal policy\nu∗.\nThe optimal value function can be expressed as\nV ∗(s) = W T σ (s) + ϵ (s) ,\n(10)\nwhere W ∈Rl is an unknown vector of bounded weights, σ : Rn →Rl is a vector of continuously differentiable\nnonlinear activation functions [27, Def. 2.1] such that σ (0) = 0 and ∇sσ (0) = 0, l ∈N is the number of basis\nfunctions, and ϵ : Rn →R is the reconstruction error. Using the universal function approximation property [28,\nTheorem 1.5] of single layer neural networks, it can be deduced that given any compact set1 B (0, χ) ⊂Rn\nand a positive constant ϵ ∈R, there exists a number of basis functions l ∈N, and known positive constants ¯W\nand σ such that ∥W∥≤¯W, sups∈B(0,χ) ∥ϵ (s)∥≤ϵ, sups∈B(0,χ) ∥∇sϵ (s)∥≤ϵ, sups∈B(0,χ) ∥σ (s)∥≤σ, and\nsups∈B(0,χ) ∥∇sσ (s)∥≤σ.\nUsing (8), a representation of the optimal controller using the same basis as the optimal value function can be\nderived as\nu∗(s)=−1\n2R−1GT (s)\n\u0000∇sσT (s) W+∇sϵT (s)\n\u0001\n.\n(11)\n1Note that at this stage, the existence of a compact forward-invariant set that contains trajectories of (2) is not being assumed. The existence\nof such a set can be established from [19, Theorem 1a].\n6\nSince the ideal weights, W, are unknown, an actor-critic technique is utilized in the following to estimate W. Let\nthe NN estimates ˆV : Rn × Rl →R and ˆu : Rn × Rl →Rm be deﬁned as\nˆV\n\u0010\nˆs, ˆWc\n\u0011\n:= ˆW T\nc σ (ˆs) ,\n(12)\nˆu\n\u0010\nˆs, ˆWa\n\u0011\n:= −1\n2R−1GT (ˆs) ∇ˆsσT (ˆs) ˆWa,\n(13)\nwhere the critic weights, ˆWc ∈Rl and actor weights, ˆWa ∈Rl are estimates of the ideal weights, W.\nB. Bellman Error\nSubstituting (12) and (13) into (8) results in a residual term, ˆδ : Rn × Rl × Rl →R, referred to as the Bellman\nerror (BE), deﬁned as\nˆδ(ˆs, ˆWc, ˆWa) := ˆVˆs(ˆs, ˆWc)\n\u0010\nF(ˆs) + G(ˆs)ˆu(ˆs, ˆWa)\n\u0011\n+ Q′(ˆs) + ˆu(ˆs, ˆWa)T Rˆu(ˆs, ˆWa).\n(14)\nTo learn the approximation control policy, online RL algorithms traditionally require a persistence of excitation (PE)\ncondition [6], [29], [30]. It is typically impossible to guarantee PE a priori and verify PE online. While impossible to\nensure a priori, by utilizing the model’s virtual excitation, stability and convergence of online RL can be established\nunder a PE-like condition that can be validated online (by monitoring the minimum eigenvalue of a matrix in the\nsubsequent Assumption 2) [7]. Using the system model, the BE can be evaluated at any arbitrary point in the state\nspace. Virtual excitation can then be implemented by selecting a set of state variables {rk | k = 1, · · · , N}, where\nrk2 ∈Rn, and evaluating the BE at this set of state variables to yield\nˆδk(rk, ˆWc, ˆWa) := ˆVrk(rk, ˆWc)\n\u0010\nF(rk) + G(rk)ˆu(rk, ˆWa)\n\u0011\n+ Q′(rk) + ˆu(rk, ˆWa)T Rˆu(rk, ˆWa).\n(15)\nDeﬁning the actor and critic weight estimation errors as ˜Wc := W −ˆWc and ˜Wa := W −ˆWa and substituting the\nestimates (10) and (11) into (8), and subtracting from (14), the BE that can be expressed in terms of the weight\nestimation errors as3\nˆδ = −ωT ˜Wc + 1\n4\n˜W T\na Gσ ˜Wa + ∆,\n(16)\nwhere ∆:= 1\n2W T ∇ˆsσGR∇ˆsϵT + 1\n4Gϵ−∇ˆsϵF, GR := GR−1GT , Gϵ := ∇ˆsϵGR∇ˆsϵT , Gσ := ∇ˆsσGR−1GT ∇ˆsσT ,\nand ω := ∇ˆsσ\n\u0010\nF + Gˆu\n\u0010\nˆs, ˆWa\n\u0011\u0011\n. Similarly, (15) implies that\nˆδk = −ωT\nk ˜Wc + 1\n4\n˜W T\na Gσk ˜Wa + ∆k,\n(17)\nwhere, ∆k := 1\n2W T ∇rkσkGRk∇rkϵT\nk + 1\n4Gϵk −∇rkϵkFk, Gϵk := ∇rkϵkGRk∇rkϵT\nk ,\nωk := ∇rkσk\n\u0010\nFk + Gkˆu\n\u0010\nˆzk, ˆWa\n\u0011\u0011\n, Gσk := ∇rkσkGkR−1GT\nk ∇rkσT\nk , GRk := GkR−1GT\nk , Fk := F(rk), Gk :=\nG(rk), Hk := H(rk), σk := σ(rk), and ϵk := ϵ(rk).\nNote that sups∈B(0,χ) |∆| ≤daϵ and if rk ∈B (0, χ) then |∆k| ≤daϵk, for some constant da > 0.\n2In this analysis, the extrapolation state variables rk are assumed to be constant, however, the approach can be extended in a straightforward\nmanner to time-varying extrapolation state variables conﬁned to a compact neighborhood of the origin.\n3The dependence of various functions on the state, s, is omitted hereafter for brevity whenever it is clear from the context.\n7\nC. Update laws for Actor and Critic weights\nUsing the extrapolated BEs ˆδk from (15), the weights are updated according to\n˙ˆWc = −kc\nN Γ\nN\nX\nk=1\nωk\nρk\nˆδk,\n(18)\n˙Γ = βΓ −kc\nN Γ\nN\nX\nk=1\nωkωT\nk\nρ2\nk\nΓ,\n(19)\n˙ˆWa=−ka1\n\u0010\nˆWa−ˆWc\n\u0011\n+\nN\nX\nk=1\nkcGT\nσk ˆWaωT\nk\n4Nρk\nˆWc−ka2 ˆWa,\n(20)\nwith Γ (t0) = Γ0, where Γ : R≥t0 →Rl×l is a time-varying least-squares gain matrix, ρk (t) := 1 + γωT\nk (t) ωk (t),\nγ > 0 is a constant positive normalization gain, β > 0 ∈R is a constant forgetting factor, and kc, ka1, ka2 > 0 ∈R\nare constant adaptation gains. The control commands sent to the system are then computed using the actor weights\nas\nu(t) = ˆu\n\u0010\nˆs(t), ˆWa(t)\n\u0011\n,\nt ≥0.\n(21)\nThe Lyapunov function needed to analyze the closed loop system deﬁned by (2), (5), (18), (19), and (20) is\nconstructed using stability properties of (2) under the optimal feedback (9). To that end, the following section\nanalyzes the optimal closed-loop system.\nUsing the assumption that Q′(s) is PD [19, Theorem 1a], and the converse Lyapunov theorem for asymptotic\nstability [31, Theorem 4.17], the existence of a radially unbounded PD function V : Rn →R and a PD function\nW : Rn →R is guaranteed such that\nVs(s)(F(s)+G(s)u∗(s))≤−W(s),\n(22)\nfor all s ∈Rn. The functions V and W are utilized in the following section to analyze the stability of the output\nfeedback approximate optimal controller.\nVII. STABILITY ANALYSIS\nThe following PE-like rank condition is utilized in the stability analysis.\nAssumption 2. There exists a constant c1 > 0 such that the set of points {rk ∈Rn | k = 1, . . . , N} satisﬁes\nc1IL ≤\ninf\nt∈R≥T\n \n1\nN\nN\nX\nk=1\nωk (t) ωT\nk (t)\nρ2\nk (t)\n!\n.\n(23)\nSince ωk is a function of the estimates ˆs and ˆWa, Assumption 2 cannot be guaranteed a priori. However, unlike the\nPE condition, Assumption 2 can be veriﬁed online. Furthermore, since λmin\n\u0010PN\nk=1\nωk(t)ωT\nk (t)\nρ2\nk(t)\n\u0011\nis non-decreasing\nin the number of samples, N, Assumption 2 can be met, heuristically, by increasing the number of samples. It\nis established in [6, Lemma 1] that under Assumption 2 and provided λmin\n\b\nΓ−1\n0\n\t\n> 0, the update law in (19)\nensures that the least squares gain matrix satisﬁes\nΓIL ≤Γ (t) ≤ΓIL,\n(24)\n8\n∀t ∈R≥0 and for some Γ, Γ > 0. Using (2), the orbital derivative of the PD function V introduced in (22), along the\ntrajectories of (2), under the controller u = ˆu\n\u0010\nˆs, ˆWa\n\u0011\n, is given by ˙V\n\u0010\ns, ˜s, ˜Wa\n\u0011\n= Vs (s)\n\u0010\nF (s) + G (s) ˆu\n\u0010\nˆs, ˆWa\n\u0011\u0011\n,\nwhere ˜s := s −ˆs.\nUsing (22) and the facts that G is bounded, the basis functions σ are bounded, and the value function approxima-\ntion error ϵ and its derivative with respect to s, ˆs are bounded on compact sets, the time-derivative can be bounded\nas\n˙V\n\u0010\ns, ˜s, ˜Wa\n\u0011\n≤−W (s) + ι1ϵ + ι2 ∥˜s∥\n\r\r\r ˜Wa\n\r\r\r + ι3\n\r\r\r ˜Wa\n\r\r\r + ι4 ∥˜s∥,\n(25)\nfor all ˆWa ∈Rl, for all s ∈B(0, χ), and for all ˆs ∈B(0, χ), where ι1, · · · , ι4 are positive constants.\nLet Θ\n\u0010\n˜Wc, ˜Wa, t\n\u0011\n:= 1\n2 ˜W T\nc Γ−1 (t) ˜Wc + 1\n2 ˜W T\na ˜Wa. The orbital derivative of Θ along the trajectories of (18) -\n(20) is given by\n˙Θ\n\u0010\n˜Wc, ˜Wa, t\n\u0011\n= ˜W T\nc Γ−1 ˙˜Wc −1\n2\n˜W T\nc Γ−1 ˙ΓΓ−1 ˜Wc + ˜W T\na ˙˜Wa,\n(26)\nwhere ˙˜Wc = −˙ˆWc, and ˙˜Wa = −˙ˆWa.\nProvided the extrapolation state variables are selected such that rk ∈B(0, χ), ∀k = {1, . . . , N}, the orbital derivative\nin (26) can be bounded as\n˙Θ\n\u0010\n˜Wc, ˜Wa, t\n\u0011\n≤−kcc\n\r\r\r ˜Wc\n\r\r\r\n2\n−(ka1 + ka2)\n\r\r\r ˜Wa\n\r\r\r\n2\n+ kcι8ϵ\n\r\r\r ˜Wc\n\r\r\r + kcι5\n\r\r\r ˜Wa\n\r\r\r\n2\n+ (kcι6 + ka1)\n\r\r\r ˜Wc\n\r\r\r\n\r\r\r ˜Wa\n\r\r\r\n+\n\u0000kcι7 + ka2W\n\u0001 \r\r\r ˜Wa\n\r\r\r ,\n(27)\nfor all t ≥0, where ι5, . . . , ι8 are positive constants that are independent of the learning gains, W denotes an upper\nbound on the norm of the ideal weights W, and\nc = inft≥0 λmin\nn\u0010\nβ\n2kc Γ−1 (t) +\n1\n2N\nPN\nk=1\nωkωT\nk\nρk\n\u0011o\n. Assumption 2 and (24) guarantee that c > 0. From (34) we\nget,\n˙Vse ≤−(λmin(ζ) −ϖ2) ∥˜s∥2 + ϖ1∥˜s∥∥s∥+ ϖ3∥˜s∥∥˜Wa∥+ ϖ4∥˜s∥,\n(28)\nfor all ˆWa ∈Rl, for all s ∈B(0, χ), and for all ˆs ∈B(0, χ).\nTheorem 1. Provided Assumptions 1 - 2, and the hypotheses of Lemma 3 hold, the gains are selected large enough\nto ensure that (39) holds and the matrix M +M T , is PD, and the weights ˆWc, Γ, and ˆWa are updated according to\n(18), (19), and (20), respectively, then the estimation errors ˜Wc, ˜Wa, and the trajectories of the transformed system\nin (2), under the controller in (21), are locally uniformly ultimately bounded.\nProof. See Appendix B\nTheorem 1 and Lemma 1-2 shows that the trajectories of the original system in (1), under the controller (21),\nsatisfy the safety constraints and decay to a neighborhood of the origin. It is thus immediate that our developed\ntechnique achieves the control objective stated in Section II.\n9\nFig. 1. Phase portrait of the F-16 aircraft longitudinal dynamical system’s state variables using OF-SMBRL in the original coordinates. The\nboxed area represents the user-selected safe set.\nVIII. SIMULATION\nTo demonstrate the performance of the developed method, the simulation results of the F-16 aircraft longitudinal\ndynamical system as an example is provided.\nConsider the F-16 aircraft longitudinal dynamical system described by (1), where\nA =\n\n\n−1.01887\n0.90506\n−0.00215\n0.82225\n−1.07741\n−0.17555\n0\n0\n−1\n\n, B =\n\n\n0\n0\n1\n\n,\nC =\nh\n1\n0\n0\ni\n.\nThe state x = [x1; x2; x3] needs to satisfy the constraints x1 ∈(z1, z1), x2 ∈(z2, z2), and x3 ∈(z3, z3) where z1\n= z2 = z3 = −0.1, z1 = z2 = z3 = 0.1.\nThe objective is to synthesize the action policy to minimize the inﬁnite horizon cost in (6), with Q′(s) = sT Qs\nwhere Q = 10I2 and R = 1. The basis functions for value function approximation are selected as σ(ˆs) =\n[ˆs1ˆs2; ˆs1ˆs3; ˆs2ˆs3; ˆs2\n1; ˆs2\n2; ˆs2\n3].\nThe initial conditions for the state variables, estimated state variables, and the initial guesses for the weights\nare selected as x(0) = [α0; q0; δe0] = [0.045; 0; 0.0393] where α denotes the angle of attack [rad]; q is the pitch\nrate [rad/sec]; δe is the elevator deﬂection angle [rad]. Let, ˆx(0) = [0.05; 0.05; 0.05], L = [1; 1; 1] Γ(0) = I6,\nˆWa(0) = [1; 1; 1; 1; 1; 1], ˆWc(0) = [1; 1; 1; 1; 1; 1], Kc = 100, Ka1 = 100, Ka2 = 1, β = 0.1, and v = 1. The ideal\nvalues of the actor and the critic weights are unknown. The simulation uses 125 ﬁxed Bellman error extrapolation\npoints in a 0.08×0.08 ×0.08 cube around the origin of the s−coordinate system.\n10\n0\n5\n10\n15\n20\n-0.06\n-0.04\n-0.02\n0\n0.02\nFig. 2. Estimation errors between the original states and the estimated states under selected nominal gains from for the F-16 aircraft longitudinal\ndynamical system\n.\n0\n5\n10\n15\n20\n0\n0.5\n1\nFig. 3. Estimates of the actor and the critic weights under selected nominal gains for the F-16 aircraft longitudinal dynamical system.\nFig.1 indicates that the system state variables, x, remain inside the user-speciﬁed safe set when approaching the\norigin. Fig. 2 shows that the state estimation errors also converge to the zero. As observed from the results in Fig.\n3, the unknown weights for both the actor and critic converge to similar values. The plots from the simulation\nshows the effective performance of the developed method.\nIX. CONCLUSION\nThis paper presents a novel framework that utilizes a new barrier-based adaptive state estimator to yield a\nsafe MBRL based online, approximate optimal controller synthesizing technique for safety-critical linear systems,\nunder output feedback. BT, a transformation method to transform a constrained optimal control problem into an\n11\nunconstrained optimal control problem, facilitates existing MBRL techniques to obtain safe optimal controllers in\nthe original coordinate. The newly designed Luenberger like state estimator enables the SMBRL framework to\nprovide OF-SMBRL controller that guarantees to keep the state of the original system within the safety bounds.\nRegulation of the system state variables to a neighborhood of the origin and convergence of the estimated policy\nto a neighborhood of the optimal policy is established using a Lyapunov-based stability analysis.\nWhile the simulation results are promising, safety violations due to unmodeled uncertainties in the system\ndynamics and/or the environment are possible. Furthermore, simulation study indicates that the technique is sensitive\nto initial guesses of the unknown policy and the unknown value function, as predicted by the local stability result.\nFuture research targeted towards these limitations will pave the way for the use of the barrier transformation\napproach in safety-critical applications such as autonomous driving and manned aviation.\nThe structure of the sensor matrix used in the state estimator limits the use of sensors to those that directly\nmeasure a subset of the state variables. In addition, to address safety, the barrier function can only constrain the\nstate variables of the system to a box. A more generic and adaptive barrier function to relax the limitations is\na subject for future research. The barrier transformation approach depends on knowledge of the dynamics of the\nsystem to assure safety. If a part of the dynamics is not included in the original model or if there are uncertain\nparameters in the model, the the relationship between trajectories of the original dynamics and the transformed\nsystem (Lemma 1) and the relationship between the trajectories of the state estimator in the transformed and the\noriginal coordinates (Lemma 2) fail to hold. Therefore, more study is required to develop safety assurances under\noutput feedback with parametric uncertainties and/or unmodeled dynamics.\nREFERENCES\n[1] R. S. Sutton, A. G. Barto, and R. J. Williams, “Reinforcement learning is direct adaptive optimal control,” IEEE Control Syst. Mag.,\nvol. 12, no. 2, pp. 19–22, 1992.\n[2] K. Doya, “Reinforcement learning in continuous time and space,” Neural Comput., vol. 12, no. 1, pp. 219–245, 2000.\n[3] P. Wawrzy´nski, “Real-time reinforcement learning by sequential actor-critics and experience replay,” Neural Netw., vol. 22, no. 10, pp.\n1484–1497, 2009.\n[4] H. Zhang, L. Cui, X. Zhang, and Y. Luo, “Data-driven robust approximate optimal tracking control for unknown general nonlinear systems\nusing adaptive dynamic programming method,” IEEE Trans. Neural Netw., vol. 22, no. 12, pp. 2226–2236, Dec. 2011.\n[5] R. Kamalapurkar, P. Walters, and W. E. Dixon, “Model-based reinforcement learning for approximate optimal regulation,” in Control\nof Complex Systems: Theory and Applications, K. Vamvoudakis and S. Jagannathan, Eds.\nButterworth-Heinemann, Aug. 2016, pp.\n247–273.\n[6] R. Kamalapurkar, J. A. Rosenfeld, and W. E. Dixon, “Efﬁcient model-based reinforcement learning for approximate online optimal\ncontrol,” Automatica, vol. 74, pp. 247–258, Dec. 2016.\n[7] R. Kamalapurkar, P. Walters, and W. E. Dixon, “Model-based reinforcement learning for approximate optimal regulation,” Automatica,\nvol. 64, pp. 94–104, Feb. 2016.\n[8] R. A. Howard, Dynamic programming and Markov processes.\nCambridge, MA: MIT Press, 1960.\n[9] S. P. Meyn and R. L. Tweedie, “Stability of Markovian processes i: criteria for discrete-time chains,” Adv. Appl. Probab., vol. 24, no. 3,\npp. 542–574, 1992.\n[10] M. Puterman, Markov decision processes: discrete stochastic dynamic programming.\nJohn Wiley & Sons, 2014.\n[11] J. F. Fisac, A. K. Akametalu, M. N. Zeilinger, S. Kaynama, J. Gillula, and C. J. Tomlin, “A general safety framework for learning-based\ncontrol in uncertain robotic systems,” IEEE Trans. Autom. Control, vol. 64, no. 7, pp. 2737–2752, 2018.\n12\n[12] Z. Li, U. Kalabi´c, and T. Chu, “Safe reinforcement learning: Learning with supervision using a constraint-admissible set,” in Proc. Am.\nControl Conf.\nIEEE, 2018, pp. 6390–6395.\n[13] Y. Li, N. Li, H. E. Tseng, A. Girard, D. Filev, and I. Kolmanovsky, “Safe reinforcement learning using robust action governor,”\narXiv:2102.10643, 2021.\n[14] M. H. Cohen and C. Belta, “Approximate optimal control for safety-critical systems with control barrier functions,” in Proc. IEEE Conf.\nDecis. Control, 2020, pp. 2062–2067.\n[15] Y. Yang, K. G. Vamvoudakis, H. Modares, W. He, Y.-X. Yin, and D. Wunsch, “Safety-aware reinforcement learning framework with an\nactor-critic-barrier structure,” in Proc. Am. Control Conf., 2019, pp. 2352–2358.\n[16] M. L. Greene, P. Deptula, S. Nivison, and W. E. Dixon, “Sparse learning-based approximate dynamic programming with barrier constraints,”\nIEEE Control Syst. Lett., vol. 4, no. 3, pp. 743–748, 2020.\n[17] S. M. N. Mahmud, K. Hareland, S. Nivison, Z. I. Bell, and R. Kamalapurkar, “A safety aware model-based reinforcement learning\nframework for systems with uncertainties,” in Proc. Am. Control Conf., New Orleans, LA, USA, May 2021, pp. 1979–1984.\n[18] S. M. N. Mahmud, S. A. Nivison, Z. I. Bell, and R. Kamalapurkar, “Safe model-based reinforcement learning for systems with parametric\nuncertainties,” Frontiers in Robotics and AI, vol. 8, 2021.\n[19] S. M. N. Mahmud, M. Abudia, S. A. Nivison, Z. I. Bell, and R. Kamalapurkar, “Safety aware model-based reinforcement learning for\noptimal control of a class of output-feedback nonlinear systems,” arXiv preprint arXiv:2110.00271, 2021.\n[20] P. Walters, R. Kamalapurkar, and W. E. Dixon, “Approximate optimal online continuous-time path-planner with static obstacle avoidance,”\nin Proc. IEEE Conf. Decis. Control, Osaka, Japan, Dec. 2015, pp. 650–655.\n[21] P. Deptula, H. Chen, R. A. Licitra, J. A. Rosenfeld, and W. E. Dixon, “Approximate optimal motion planning to avoid unknown moving\navoidance regions,” IEEE Transactions on Robotics, vol. 36, no. 2, pp. 414–430, 2020.\n[22] K. Graichen and N. Petit, “Incorporating a class of constraints into the dynamics of optimal control problems,” Optimal Control\nApplications and Methods, vol. 30, no. 6, pp. 537–561, 2009.\n[23] Luenberger, “Observing the state of a linear system,” IEEE Trans. Mil. Electron., vol. 8, pp. 74–80, 1964.\n[24] R. A. Horn, “The hadamard product,” 1990.\n[25] Y. Yang, D.-W. Ding, H. Xiong, Y. Yin, and D. C. Wunsch, “Online barrier-actor-critic learning for h∞control with full-state constraints\nand input saturation,” J. Franklin Inst., vol. 357, no. 6, pp. 3316 – 3344, 2020.\n[26] R. Kamalapurkar, P. Walters, J. A. Rosenfeld, and W. E. Dixon, Reinforcement learning for optimal feedback control: A Lyapunov-based\napproach, ser. Communications and Control Engineering.\nSpringer International Publishing, 2018.\n[27] N. Sadegh, “A perceptron network for functional identiﬁcation and control of nonlinear systems,” IEEE Trans. Neural Netw., vol. 4, no. 6,\npp. 982–988, 1993.\n[28] F. Sauvigny, Partial Differential Equations 1.\nSpringer, 2012.\n[29] H. Modares, F. L. Lewis, and M.-B. Naghibi-Sistani, “Adaptive optimal control of unknown constrained-input systems using policy iteration\nand neural networks,” IEEE Trans. Neural Netw. Learn. Syst., vol. 24, no. 10, pp. 1513–1525, 2013.\n[30] B. Kiumarsi, F. L. Lewis, H. Modares, A. Karimpour, and M.-B. Naghibi-Sistani, “Reinforcement Q-learning for optimal tracking control\nof linear discrete-time systems with unknown dynamics,” Automatica, vol. 50, no. 4, pp. 1167–1175, Apr. 2014.\n[31] H. K. Khalil, Nonlinear systems, 3rd ed.\nUpper Saddle River, NJ: Prentice Hall, 2002.\nAPPENDIX\nA. Proof of Lemma 3\nLemma 3. Let Vse : Rn →R≥0 be a continuously differentiable candidate Lyapunov function deﬁned as Vse(˜s) :=\n˜sT P ˜s where P is a PD matrix and P(A−LC)+(A−LC)T P = −ζ. Provided s, ˆs ∈B(0, χ) for some χ > 0, the\norbital derivative of Vse along the trajectories of ˙˜s can be bounded as ˙Vse ≤−(λmin(ζ) −ϖ2) ∥˜s∥2+ϖ1∥˜s∥∥s∥+\nϖ3∥˜s∥∥˜Wa∥+ ϖ4∥˜s∥.\nProof. The estimator error in the transformed coordinate can be computed as\n˙˜s = F(s) + G(s)ˆu −G(ˆs)ˆu −As + A˜s −LC˜s,\n(29)\n13\nP :=\n\n\nkcι8ϵ\n\u0000kcι7 + ka2W + ι3\n\u0001\nι4 + ϖ4\n\n\nT\n,\nM :=\n\n\nkcc\n0\n0\n−(kcι6 + ka1)\n(ka1 + ka2 −kcι5)\n0\n0\n−(ι2 + ϖ3)\nλmin(ζ) −1\n2ϖ1 −ϖ2\n\n\nT\n.\nThe orbital derivative can be expressed as\n˙Vse = ˜sT P ˙˜s + ˙˜sT P ˜s,\n(30)\nUsing (5), we can rewrite (30) as\n˙Vse = ˜sT P (F(s) + G(s)ˆu −G(ˆs)ˆu −As + A˜s −LC˜s) + (F(s) + G(s)ˆu −G(ˆs)ˆu −As + A˜s −LC˜s)T P ˜s\n(31)\nwhich yields\n˙Vse = ˜sT P(A −LC)˜s + ˜sT PF(s) + ˜sT P ˜G(s, ˆs)ˆu −˜sPAs + ˜sT (A −LC)T P ˜s + (˜sT PF(s))T\n+(˜sT P ˜G(s, ˆs)ˆu)T −(˜sPAs)T .\n(32)\nWe can rewrite (32) as\n˙Vse = ˜sT\n\u0012\nP(A −LC) + (A −LC)T P\n\u0013\n˜s + ˜sT PF(s) +\n\u0000˜sT PF(s)\n\u0001T −˜sPAs −(˜sPAs)T\n−˜sT P ˜G (s, ˆs) ˆu\n\u0010\ns, ˜Wa\n\u0011\n+ ˜sT P ˜G (s, ˆs) ˆu\n\u0010\ns, ˜Wa\n\u0011\n−˜sT P ˜G (s, ˆs) ˆu\n\u0010\nˆs, ˜Wa\n\u0011\n−˜sT P ˜G (s, ˆs) ˆu (s, W)\n+ ˜sT P ˜G (s, ˆs) ˆu (ˆs, W) + ˜sT P ˜G (s, ˆs) ˆu (s, W) −\n\u0010\n˜sT P ˜G (s, ˆs) ˆu(s, ˜Wa)\n\u0011T\n+\n\u0010\n˜sT P ˜G (s, ˆs) ˆu(s, ˜Wa)\n\u0011T\n−\n\u0010\n˜sT P ˜G (s, ˆs) ˆu(ˆs, ˜Wa)\n\u0011T\n−\n\u0010\n˜sT P ˜G (s, ˆs) ˆu (s, W)\n\u0011T\n+\n\u0010\n˜sT P ˜G (s, ˆs) ˆu(ˆs, W)\n\u0011T\n+\n\u0010\n˜sT P ˜G (s, ˆs) ˆu(s, W)\n\u0011T\n.\n(33)\nLet P(A −LC) + (A −LC)T P = −ζ, where ζ is a PD matrix. With P being a PD matrix, we need to pick a\nsatisfactory gain matrix L so that (A −LC) becomes Hurwitz.\nConsidering ∥P∥and ∥W∥as constants, using the Cauchy-Schwarz inequality and the fact that F, G are locally\nLipschitz continuous on B(0, χ), we have\n˙Vse ≤−λmin(ζ)∥˜s∥2 + ϖ1∥˜s∥∥s∥+ ϖ2∥˜s∥∥˜s∥+ ϖ3∥˜s∥∥˜Wa∥+ ϖ4∥˜s∥\n(34)\nwhere ϖ1; · · · ; ϖ4 as constants. From (34), we obtain the desired bound\n˙Vse ≤−(λmin(ζ) −ϖ2) ∥˜s∥2 + ϖ1∥˜s∥∥s∥+ ϖ3∥˜s∥∥˜Wa∥+ ϖ4∥˜s∥.\n(35)\nB. Proof of Theorem 1\nTheorem 1. Provided Assumptions 1 - 2, and the hypotheses of Lemma 3 hold, the gains are selected large enough\nto ensure that (39) holds and the matrix M + M T , is PD, and the weights ˆWc, Γ, and ˆWa are updated according\n14\nto (18), (19), and (20), respectively, then the estimation errors ˜Wc, ˜Wa, and the trajectories of the transformed\nsystem in (2), under the controller in (21), are locally uniformly ultimately bounded.\nProof. The candidate Lyapunov function for the closed-loop system is selected as\nVL (Z, t) := V (s) + Θ\n\u0010\n˜Wc, ˜Wa, t\n\u0011\n+ Vse (˜s) ,\n(36)\nwhere Z :=\nh\nsT\n˜W T\nc\n˜W T\na\n˜sT\niT\n.\nLet C ⊂R2n be a compact set deﬁned as\nC :=\nn\n(s, ˜s) ∈R2n | ∥s∥+ ∥˜s∥\no\n.\nWhenever, (s, ˜s) ∈C, it can be concluded that s, ˆs ∈B(0, χ). As a result, (25), (27), and (34) imply that\nwhenever Z ∈C × R2l, the orbital derivative of the candidate Lyapunov function along the trajectories of (2), (5),\n(18), (19), (20), under the controller (21), can be bounded as\n˙VL (Z, t) ≤−W (s) + ι1ϵ + ι2 ∥˜s∥\n\r\r\r ˜Wa\n\r\r\r + ι3\n\r\r\r ˜Wa\n\r\r\r + ι4 ∥˜s∥−kcc\n\r\r\r ˜Wc\n\r\r\r\n2\n−(ka1 + ka2)\n\r\r\r ˜Wa\n\r\r\r\n2\n+ kcι8ϵ\n\r\r\r ˜Wc\n\r\r\r\n+kcι5\n\r\r\r ˜Wa\n\r\r\r\n2\n+ (kcι6 + ka1)\n\r\r\r ˜Wc\n\r\r\r\n\r\r\r ˜Wa\n\r\r\r +\n\u0000kcι7 + ka2W\n\u0001 \r\r\r ˜Wa\n\r\r\r −\n\u0000λmin(ζ) −ϖ2\n\u0001\n∥˜s∥2\n+\n\u0000ϖ1\n\u0001\n∥˜s∥∥s∥+ ϖ3∥˜s∥∥˜Wa∥+ ϖ4∥˜s∥,\nwhich can be re-expressed as,\n˙VL (Z, t) ≤−W (s) + ι1ϵ + (ι2 + ϖ3) ∥˜s∥\n\r\r\r ˜Wa\n\r\r\r −kcc\n\r\r\r ˜Wc\n\r\r\r\n2\n−(ka1 + ka2 −kcι5)\n\r\r\r ˜Wa\n\r\r\r\n2\n+ kcι8ϵ\n\r\r\r ˜Wc\n\r\r\r\n+ (kcι6 + ka1)\n\r\r\r ˜Wc\n\r\r\r\n\r\r\r ˜Wa\n\r\r\r +\n\u0000kcι7 + ka2W + ι3\n\u0001 \r\r\r ˜Wa\n\r\r\r + (ι4 + ϖ4) ∥˜s∥−\n\u0000λmin(ζ) −ϖ2\n\u0001\n∥˜s∥2 + ϖ1∥˜s∥∥s∥,\nwhich yields using Young’s inequality\n˙VL (Z, t) ≤−W (s) + 1\n2(ϖ1)∥s∥2 + ι1ϵ + (ι2 + ϖ3) ∥˜s∥\n\r\r\r ˜Wa\n\r\r\r + (ι4 + ϖ4) ∥˜s∥−kcc\n\r\r\r ˜Wc\n\r\r\r\n2\n−(ka1 + ka2 −kcι5)\n\r\r\r ˜Wa\n\r\r\r\n2\n+ kcι8ϵ\n\r\r\r ˜Wc\n\r\r\r + (kcι6 + ka1)\n\r\r\r ˜Wc\n\r\r\r\n\r\r\r ˜Wa\n\r\r\r +\n\u0000kcι7 + ka2W + ι3\n\u0001 \r\r\r ˜Wa\n\r\r\r\n−\n\u0000λmin(ζ) −1\n2ϖ1 −ϖ2\n\u0001\n∥˜s∥2,\nwhere z :=\nh\r\r\r ˜Wc\n\r\r\r\n\r\r\r ˜Wa\n\r\r\r\n∥˜s∥\niT\n. Provided the matrix M + M T is PD,\n˙VL (Z, t) ≤−W (s) + 1\n2(ϖ1)∥s∥2 −M ∥z∥2 + P ∥z∥+ ι1ϵ,\nwhere M := λmin\nn\nM+M T\n2\no\n, P = ∥P∥∞. Letting M =: M 1 + M 2, and letting W : R2n+2l →R be deﬁned as\nW (Z) = −W (s) + 1\n2(ϖ1)∥s∥2 −M 1 ∥z∥2, with W(s) > 1\n2(ϖ1)∥s∥2, the orbital derivative can be bounded as\n˙VL (Z, t) ≤−W (Z) ,\n(37)\n∀∥Z∥> 1\n2\n\u0010\nP\nM 2 +\nq\nP\n2\nM 2\n2 + ι2\n1ϵ2\nM 2\n2\n\u0011\n=: µ, ∀Z ∈B (0, ¯χ), for all t ≥0, and some ¯χ such that B(0, ¯χ) ⊆C × R2l.\nUsing the bound in (24) and the fact that the converse Lyapunov function is guaranteed to be time-independent,\nradially unbounded, and PD, [31, Lemma 4.3] can be invoked to conclude that\nv (∥Z∥) ≤VL (Z, t) ≤v (∥Z∥) ,\n(38)\n15\nfor all t ∈R≥0 and for all Z ∈R2n+2l, where v, v : R≥0 →R≥0 are class K functions.\nProvided the learning gains, the domain radii χ and ¯χ, and the basis functions for function approximation are\nselected such that M + M T is PD and\nµ < v−1 (v (0, ¯χ)) ,\n(39)\nthen [31, Theorem 4.18] can be invoked to conclude that Z is locally uniformly ultimately bounded. Since the\nestimates ˆWa approximate the ideal weights W, the policy ˆu approximates the optimal policy u∗.\n16\n",
  "categories": [
    "eess.SY",
    "cs.SY"
  ],
  "published": "2022-04-04",
  "updated": "2022-04-04"
}