{
  "id": "http://arxiv.org/abs/2204.01782v1",
  "title": "The First Principles of Deep Learning and Compression",
  "authors": [
    "Max Ehrlich"
  ],
  "abstract": "The deep learning revolution incited by the 2012 Alexnet paper has been\ntransformative for the field of computer vision. Many problems which were\nseverely limited using classical solutions are now seeing unprecedented\nsuccess. The rapid proliferation of deep learning methods has led to a sharp\nincrease in their use in consumer and embedded applications. One consequence of\nconsumer and embedded applications is lossy multimedia compression which is\nrequired to engineer the efficient storage and transmission of data in these\nreal-world scenarios. As such, there has been increased interest in a deep\nlearning solution for multimedia compression which would allow for higher\ncompression ratios and increased visual quality.\n  The deep learning approach to multimedia compression, so called Learned\nMultimedia Compression, involves computing a compressed representation of an\nimage or video using a deep network for the encoder and the decoder. While\nthese techniques have enjoyed impressive academic success, their industry\nadoption has been essentially non-existent. Classical compression techniques\nlike JPEG and MPEG are too entrenched in modern computing to be easily\nreplaced. This dissertation takes an orthogonal approach and leverages deep\nlearning to improve the compression fidelity of these classical algorithms.\nThis allows the incredible advances in deep learning to be used for multimedia\ncompression without threatening the ubiquity of the classical methods.\n  The key insight of this work is that methods which are motivated by first\nprinciples, i.e., the underlying engineering decisions that were made when the\ncompression algorithms were developed, are more effective than general methods.\nBy encoding prior knowledge into the design of the algorithm, the flexibility,\nperformance, and/or accuracy are improved at the cost of generality...",
  "text": "Abstract\nTitle of Dissertation:\nThe First Principles of Deep Learning and Compression\nMax Ehrlich\nDoctor of Philosophy, 2022\nDissertation Directed By:\nProfessor Abhinav Shrivastava\nDepartment of Computer Science\nProfessor Larry S. Davis\nDepartment of Computer Science\nThe deep learning revolution incited by the 2012 Alexnet paper has been transformative for the ﬁeld of\ncomputer vision. Many problems which were severely limited using classical solutions are now seeing\nunprecedented success. The rapid proliferation of deep learning methods has led to a sharp increase in their\nuse in consumer and embedded applications. One consequence of consumer and embedded applications\nis lossy multimedia compression which is required to engineer the eﬃcient storage and transmission of\ndata in these real-world scenarios. As such, there has been increased interest in a deep learning solution for\nmultimedia compression which would allow for higher compression ratios and increased visual quality.\nThe deep learning approach to multimedia compression, so called Learned Multimedia Compression, involves\ncomputing a compressed representation of an image or video using a deep network for the encoder and\nthe decoder. While these techniques have enjoyed impressive academic success, their industry adoption has\nbeen essentially non-existent. Classical compression techniques like JPEG and MPEG are too entrenched\nin modern computing to be easily replaced. This dissertation takes an orthogonal approach and leverages\ndeep learning to improve the compression ﬁdelity of these classical algorithms. This allows the incredible\nadvances in deep learning to be used for multimedia compression without threatening the ubiquity of the\nclassical methods.\nThe key insight of this work is that methods which are motivated by ﬁrst principles, i.e., the underlying\nengineering decisions that were made when the compression algorithms were developed, are more eﬀective\nthan general methods. By encoding prior knowledge into the design of the algorithm, the ﬂexibility,\nperformance, and/or accuracy are improved at the cost of generality. While this dissertation focuses on\ncompression, the high level idea can be applied to many diﬀerent problems with success.\nFour completed works in this area are reviewed. The ﬁrst work, which is foundational, uniﬁes the disjoint\nmathematical theories of compression and deep learning allowing deep networks to operate on compressed\ndata directly. The second work shows how deep learning can be used to correct information loss in JPEG\ncompression over a wide range of compression quality, a problem that is not readily solvable without a ﬁrst\nprinciples approach. This allows images to be encoded at high compression ratios while still maintaining visual\nﬁdelity. The third work examines how deep learning based inferencing tasks, like classiﬁcation, detection,\nand segmentation, behave in the presence of classical compression and how to mitigate performance loss.\nAs in the previous work, this allows images to be compressed further but this time without accuracy loss\non downstream learning tasks. Finally, these ideas are extended to video compression by developing an\nalgorithm to correct video compression artifacts. By incorporating bitstream metadata and mimicking the\ndecoding process with deep learning, the method produces more accurate results with higher throughput\nthan general methods. This allows deep learning to improve the rate-distortion of classical MPEG codecs and\ncompetes with fully deep learning based codecs but with a much lower barrier-to-entry.\narXiv:2204.01782v1  [eess.IV]  4 Apr 2022\nUniversity of Maryland, College Park\nSubmitted to the Faculty of the Graduate School\nThe First Principles of\nDeep Learning and Compression\nA Dissertation by\nMax Ehrlich\nmaxehr@umiacs.umd.edu\nhttps://maxehr.umiacs.io\nWednesday March 30th 2022\nFor Partial Fulﬁllment of the Requirements of the Degree of\nDoctor of Philosophy in Computer Science\nAdvisors: Professor Abhinav Shrivastava, Professor Larry S. Davis\nDean’s Representative: Professor Wojciech Czaja\nExamining Committee: Professor Ramani Duraiswami, Professor Dinesh Manocha, Dr. Michael A. Isnardi (SRI\nInternational), Professor David A. Forsyth (UIUC)\nUniversity of Maryland, College Park\nDisclaimer\nThis document was created for partial fulﬁllment of the requirements of the degree of Doctor of Philosophy\nin Computer Science and therefore does not purport to impart any knowledge of any particular subject. Any\nknowledge obtained as as result of reading this document is entirely coincidental and should be used at the\nreaders own risk.\nCopyright\nCopyright © 2022 Max Ehrlich\nLicense\nThe First Principles of Deep Learning and Compression is licensed under the Creative Commons Attribution-\nNonCommercial-NoDerivatives 4.0 International license.\nCC BY-NC-ND 4.0 cbnd\nAcknowledgement\nThis document was typeset with the help of KOMA-Script and LATEX using the kaobook class.\nSpecial thanks to\n▶Christian Steinruecken for providing the math coloring function from his dissertation.\n▶My editors: Shishira Maiya, Lillian Huang, Vatsal Agarwal, and Namitha Padmanabhan.\n▶My social media consultant Gowthami Somepalli and her assistant Kamal Gupta.\nThe research presented in this dissertation was partially supported by Facebook AI, Defense Advanced\nResearch Projects Agency (DARPA) MediFor (FA87501620191), DARPA SemaFor (HR001119S0085) and DARPA\nSAIL-ON (W911NF2020009) programs. There is no collaboration between Facebook and DARPA.\nPublisher\nSelf-published electronically in April 2022. https://maxehr.umiacs.io/dissertation.\nTo my wife, Dr. Sujeong Kim\nYou supported me unwaveringly and unconditionally throughout this\nprocess and I am eternally grateful.\nTo my daughter Yena and my son Jaeo\nKnowing you will be the greatest privilege of my life.\nPreface\nM\nultimedia compression is a critical feature of the modern internet (Duggan 2013). Websites like Facebook,\nInstagram, and YouTube have increasingly coalesced around the sharing of images and video. Viewing\nand sharing such media are now prerequisite to modern internet interactions. When comparing media, we\ncan create an approximate hierarchy with each successive level containing an order of magnitude more\ninformation. Text, which comprises a simple linguistic description. Images, which contain a full visualization\nof some scene. And videos, which contain a temporal evolution of the visualization of a scene. Naturally, as\nthe amount of information contained in a particular medium increases, so too does the size of its digital\nrepresentation.\nBecause of modern engineering constraints, it is not feasible to transmit image and video media in their\nnative format (e.g., a 2D or 3D array of samples). As an example: a single frame of a 1080p video in a\nraw format, assuming single byte samples in three colors, would require around 1 byte × 3 channels ×\n1920 × 1080 = 6220800 bytes ≈6MB to represent it natively. Extending this to 30 seconds of video\nat 30 frames per second would require 6220800 bytes × 30s × 30 fps = 5598720000 bytes ≈5GB. We\ncan observe that most videos are longer than 30 seconds and 4k videos are becoming more common.\nTransmitting these media over modern cellular or even wired connections would be quite diﬃcult. A typical\nhome internet connection bandwidth ranges from 10-50 Mbps. For the video example, this would take\n5598720000 bytes × 8 ÷ [10000000, 50000000]bps = [4478, 896]s, i.e., anywhere from 15 minutes to 1.2 hours\nfor this short video. The situation is even worse on cellular connections where LTE upload speeds range from\n2-5 Mbps (Verizon Inc. n.d.) (almost 3 hours for our example in the best case) and most users pay for a ﬁxed\namount of data.\nTo make the modern internet feasible, by reducing transmission and storage cost, we compress these\nmedia. For modern compression codecs, JPEG (G. K. Wallace 1992) can reduce the 6MB image to around\n25kB in size, and H.264 (Richardson 2011) compression: the 5GB video to only a few megabytes depending\non the spatial and temporal entropy. These impressive size reductions are a result of more than just entropy\nreducing operations: they also incur a loss of information. The removed information is designed to be as\nimperceptible as possible and is based on analyses of human visual perception. For images, we remove “high\nspatial frequencies” (Ahmed et al. 1974) or small spatial changes that would not ordinarily be noticed. For\nvideos, we can take a further step to estimate motion between frames and encode only a description of the\nmotion (Le Gall 1991). For modern codecs, the lossy eﬀects are generally not noticed to laymen, and codec\ndevelopment continues to improve visual ﬁdelity and reduce ﬁle sizes year-after-year.\nDespite these amazing advances in compression, there are still problems. In many parts of the world,\nincluding in rural America, many people use metered internet connections (Schmit and Severson 2021).\nUnder these connections they have a ﬁnite amount of data or pay for the data they use similar to most\nmodern cell phone plans. For these people, participation in the modern internet is increasingly diﬃcult.\nNot only are they expected to upload their own media, but they must view others’ media in order to take\npart in the discourse on many websites. For this class of consumer, it is paramount that as minimal data as\npossible be used during any internet transmission, precluding most videos and some images from being\naccessible. The internet is historically unprecedented as both an entertainment medium and a repository of\nhuman knowledge, and beneﬁts from maximal participation. Therefore, in order to reach these people more\neﬀectively, it is critical that further advances in multimedia compression be developed.\nMeanwhile, deep learning has revolutionized modern machine learning (He, Xiangyu Zhang, et al. 2016;\nKrizhevsky, Sutskever, et al. 2012; Tan and Le 2019). In deep learning, a model is trained to take an input in its\nnative representation and learn a nonlinear mapping directly from it. This is in contrast to classical machine\nlearning which depended on engineered features which were extracted prior to mappings being computed.\nBy taking the native input representation, the deep model can be organized into many layers which function\nas their own feature extractors. Instead of engineered features, the best features to solve a given problem are\nlearned jointly with the mapping function. The development of these techniques has enjoyed unprecedented\nsuccess in all areas of machine learning, and these models are being rapidly deployed in consumer settings to\nsolve problems which were once thought to be impossible for computers to solve.\nUnsurprisingly, given the previous discussion, one area of interest for deep learning applications is that of\nmultimedia compression. And also unsurprisingly, deep learning has made amazing contributions here (Ballé\net al. 2016; Prakash et al. 2017; Stock et al. 2020; Theis et al. 2017; G. Toderici, O’Malley, et al. 2015; G. Toderici,\nVincent, et al. 2017). Deep models are able to compress both images and video signiﬁcantly better than\nclassical algorithms and with little loss of quality. Despite this, the classical algorithms stubbornly persist.\nJPEG ﬁles are still ubiquitous and MPEG standards continue to be developed and deployed in consumer\napplication despite the amazing advances of machine learning. These algorithms, and their associated ﬁles,\nare simply too familiar and too ingrained in the code powering the modern internet to be easily replaced.\nNevertheless, there is a plain socioeconomic need, as described previously, for deep learning in compression\nas there is for anything that reduces the size of images and videos.\nIn This Dissertation\nI take an orthogonal approach to multimedia compression in deep learning. In my\napproach, I develop deep learning methods which work with the existing compression algorithms rather\nthan replace them. In this way, our algorithms are easy to integrate into the modern internet as simple pre- or\npost-processing steps on images or videos. These classical compression algorithms are developed with a\nseries of engineering decisions that determine how much and the nature of the information that is lost. I\ncall these decisions “ﬁrst principles” and I develop machine learning algorithms that are explicitly aware of\nthese decisions. I will show that this leads to a signiﬁcant improvement in ﬁdelity and/or ﬂexibility of the\nsolution. These advances have greatly improved the practicality of deploying machine learning solutions to\nsolve compression problems, although their potential applications are widespread.\nOrganization Of This Document\nThis document is organized into three parts. In the ﬁrst part, I will\ndiscuss, brieﬂy, background knowledge that a reader should be equipped with in order to have a full\nunderstanding of this dissertation. The next two parts discuss related works and my own contributions to\nimage and video compression respectively. This dissertation is written in a conversational style, and beyond\nthis preface I will often refer to the reader as “we”. This is indicating the “we”, i.e., the reader and I, are\ndiscovering the knowledge together as the concepts in the dissertation are developed from prior work into\ncompleted topics. I strongly believe in the use of color for guidance. When I believe it will be helpful, I will\nuse color in mathematics and ﬁgures to group related ideas. For complex mathematics speciﬁcally, I ﬁnd this\nto be much clearer than braces alone especially for hinting from early in a derivation which parts of long\nequations are related and will eventually be grouped together or cancelled. When useful for clarifying an\nalgorithm I have included code listings. These listings are written in something approximating python with\npytorch (Paszke et al. 2019) APIs where deep learning is required. These code samples are not guaranteed to\nrun exactly as they are written.\nWhat This Document Is\nThis document is, ﬁrst and foremost, a dissertation. This means that its primary\npurpose is to relay the unique contributions of the author over the course of about ﬁve years of research. The\nastute reader will notice, in the table of contents, section titles which are colored in Plum. These sections\nrepresent the unique contributions of my research program, i.e., papers which were published in the course\nof completing my Ph.D. These section titles are colored in the body of the document as well, so it is always\neasy for the reader to know if they are reading about background work or one of my contributions. Readers\nwill, naturally, ﬁnd these sections are the most detailed and well developed. In each of these chapters, I have\nincluded a dedicated section titled “Limitations and Future Directions”. No scientiﬁc work is perfect and\nmine are no exception. I believe it is important to be up front about these limitations with a candid discussion\nalong with guidance for future researchers in the ﬁeld.\nWhat This Document Is Not\nThis document is not a textbook or survey of multimedia compression\nalgorithms and their relationship with deep learning and readers should manage their expectations as such.\nFor the purposes of imparting a full understanding of this dissertation’s contributions to scientiﬁc discourse,\nthere is a review of elementary concepts of mathematics, machine learning, and compression as well as\nan overview of related works and recent advances in machine learning. If, in the course of reading this\ndissertation, a reader gains any useful knowledge, this is welcome but entirely accidental.\nContents\nAbstract\ni\nPreface\nv\nContents\nvii\nList of Figures\nxiii\nList of Tables\nxv\nList of Code Listings\nxvii\nNotation\nxix\nPreliminaries\n1\n1\nLinear Algebra\n3\n1.1\nScalars, Vectors, and Matrices\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nBases and Finite Dimensional Vector Spaces\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n1.3\nInﬁnite Dimensional Vector Spaces\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.4\nAbstractions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2\nMultilinear Algebra\n9\n2.1\nTensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2\nTensor Products and Einstein Notation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.3\nTensor Spaces\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.4\nLinear Pixel Manipulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3\nHarmonic Analysis\n17\n3.1\nThe Fourier Transform\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.2\nThe Gabor Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3.3\nWavelet Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.3.1\nContinuous and Discrete Wavelet Transforms . . . . . . . . . . . . . . . . . . . . . .\n23\n3.3.2\nHaar Wavelets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n4\nEntropy and Information\n27\n4.1\nShannon Entropy\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.2\nHuﬀman Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n4.3\nArithmetic Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n5\nMachine Learning and Deep Learning\n33\n5.1\nBayesian Decision Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n5.2\nPerceptrons and Multilayer Perceptrons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n5.3\nImage Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n5.3.1\nHistogram of Oriented Gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n5.3.2\nScale-Invariant Feature Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.4\nConvolutional Networks and Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n5.5\nResidual Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.6\nU-Nets\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.7\nGenerative Adversarial Networks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n5.8\nRecap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\nImage Compression\n45\n6\nJPEG Compression\n47\n6.1\nThe JPEG Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n6.1.1\nCompression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n6.1.2\nDecompression\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n6.2\nThe Multilinear JPEG Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n6.3\nOther Image Compression Algorithms\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n7\nJPEG Domain Residual Learning\n55\n7.1\nNew Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n7.1.1\nFrequency-Component Rearrangement . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n7.1.2\nStrided Convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n7.2\nExact Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n7.2.1\nJPEG Domain Convolutions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n7.2.2\nBatch Normalization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n7.2.3\nGlobal Average Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n7.3\nReLU\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n7.4\nRecap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n7.5\nEmpirical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n7.6\nLimitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n8\nImproving JPEG Compression\n71\n8.1\nPixel Domain Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n8.2\nDual-Domain Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n8.3\nSparse-Coding Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n8.4\nSummary and Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n9\nQuantization Guided JPEG Artifact Correction\n77\n9.1\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n9.2\nConvolutional Filter Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n9.3\nPrimitive Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n9.4\nFull Networks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n9.5\nLoss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\n9.6\nEmpirical Evaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n9.6.1\nComparison with Other Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n9.6.2\nGeneralization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n9.6.3\nEquivalent Quality\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n9.6.4\nExploring Convolutional Filter Manifolds\n. . . . . . . . . . . . . . . . . . . . . . . .\n86\n9.6.5\nFrequency Domain Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n9.6.6\nQualitative Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n9.7\nLimitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n10 Task-Targeted Artifact Correction\n93\n10.1 Standard JPEG Compression Mitigation Techniques . . . . . . . . . . . . . . . . . . . . . . .\n93\n10.2 Artifact Correction for Computer Vision Tasks . . . . . . . . . . . . . . . . . . . . . . . . . .\n94\n10.3 Eﬀect of JPEG Compression on Computer Vision Tasks . . . . . . . . . . . . . . . . . . . . .\n96\n10.4 Transferability and Multiple Task Heads\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n10.5 Understanding Model Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n10.6 Limitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\nVideo Compression\n101\n11 Modeling Time Redundancy: MPEG\n103\n11.1\nMotion JPEG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n104\n11.2 Motion Vectors and Error Residuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n105\n11.3 Slices and Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n107\n11.4 Recap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n108\n12 Improving Video Compression\n111\n12.1 Notable Methods for General Video Restoration . . . . . . . . . . . . . . . . . . . . . . . . .\n111\n12.2 Single Frame Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n112\n12.3 Multi-Frame Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n113\n12.4 Summary and Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n114\n13 Metabit: Leveraging Bitstream Metadata\n115\n13.1 Capturing GOP Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n116\n13.2 Motion Vector Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n117\n13.3 Network Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n118\n13.4 Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n119\n13.5 Towards a Better Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n121\n13.6 Empirical Evaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n122\n13.6.1\nRestoration Evaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n123\n13.6.2 Compression Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n124\n13.7 Limitations and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n125\nConcluding Remarks\n127\nAppendix\n131\nA Study on JPEG Compression and Machine Learning\n133\nA.1\nPlots of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n133\nA.2 Tables of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n142\nB\nAdditional Results\n145\nB.1\nQuantization Guided JPEG Artifact Correction . . . . . . . . . . . . . . . . . . . . . . . . . .\n145\nB.2\nTask Targeted Artifact Correction\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n152\nB.3\nMetabit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n159\nC Survey of Fully Deep-Learning Based Compression\n161\nC.1\nImage Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n161\nC.2\nVideo Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n163\nC.3\nLossless Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n164\nBibliography\n167\nTerminology\n181\nAcronyms\n185\nIndex\n187\nFigure Credits\n189\nList of Figures\n2.1\nGrayscale Example Image\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.2\nGrayscale Gaussian Smoothing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3\nColor Example Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.4\nColor Smoothing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.5\nColor Downsampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.6\nBlock Linear Map Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.1\nDiscrete Wavelet Transform\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.3\nWavelet Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.2\nMorlet Wavelet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.4\nDual Tree Complex Wavelet Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.5\nHaar Wavelet\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.6\nDWT Using Haar Wavelets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.1\nThe General Communication System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.2\nHuﬀman Tree Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n4.3\nArithmetic Coding Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n5.1\nSalmon vs Sea bass\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.2\nMultilayer Perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.3\nHoG Features\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.4\nDiﬀerence of Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.5\nConvolutional Neural Network\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n5.6\nU-Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.7\nResidual Block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.8\nGAN Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n6.1\nJPEG Information Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n6.2\nZig-Zag Order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n7.1\nFrequency Component Rearrangement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n7.2\nTransform Domain Global Average Pooling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n7.3\nReLU Approximation Example\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n7.4\nToy Network Architecture\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n7.5\nReLU Approximation Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n7.6\nThroughput Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n9.1\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n9.2\nFCR With Grouped Convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n9.3\nRRDB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n9.4\n8 × 8 stride-8 CFM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\n9.5\nRestoration Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n9.6\nSubnetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n9.7\nGAN Discriminator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n9.8\nQuality Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n9.10\nEquivalent Quality Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n9.9\nIncrease in PSNR\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n9.11\nEquivalent Quality Examples\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n9.12 Embeddings for Diﬀerent CFM Layers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n9.13 CFM Weight Visualization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n9.14\nImages Which Maximally Activate CFM Weights. . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n9.15 Frequency Domain Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n9.16\nQualitative Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\n10.1\nTask-Targeted Artifact Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n94\n10.2 Performance Loss Due to JPEG Compression\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n10.3 Performance Loss with Mitigations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n10.4\nTransfer Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n10.5 Multiple Task Heads\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n10.6\nMaskRCNN TIDE Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n10.7\nMask R-CNN Qualitative Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n10.8\nModel Throughput . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n11.1\nMotion JPEG Comparison\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n104\n11.2\nMotion Vector Grid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n105\n11.3\nMotion Vector Arrows\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n105\n11.4\nMotion Compensation and Error Residuals\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n106\n11.5\nRate Control Comparison\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n107\n11.6\nSlicing Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n107\n13.1\nCapturing GOP Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n116\n13.2 Motion Vector Alignment\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n117\n13.3 Motion Vectors vs Optical Flow\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n117\n13.4\nMetabit System Overview\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n118\n13.5 LR Block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n118\n13.6 Metabit Critic Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n121\n13.7\nFPS vs Params . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n123\n13.8\nRate-Distortion Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n124\n13.9 Learned Compression Throughput Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . .\n124\n13.10 Metabit Restoration Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n125\n13.11 Metabit Comparison\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n126\nA.1\nOverall Classiﬁcation Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n133\nA.2\nClassiﬁcation Results: MobileNetV2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n133\nA.3\nClassiﬁcation Results: VGG-19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n134\nA.4\nClassiﬁcation Results: InceptionV3\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n134\nA.5\nClassiﬁcation Results: ResNeXt 50 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n134\nA.6\nClassiﬁcation Results: ResNeXt 101\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n135\nA.7\nClassiﬁcation Results: ResNet 18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n135\nA.8\nClassiﬁcation Results: ResNet 50 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n135\nA.9\nClassiﬁcation Results: ResNet 101 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n136\nA.10 Classiﬁcation Results: EﬃcientNet B3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n136\nA.11 Overall Detection and Instance Segmentation Results . . . . . . . . . . . . . . . . . . . . . . . .\n137\nA.12 Detection Results: FastRCNN\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n137\nA.13 Detection Results: FasterRCNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n137\nA.14 Detection Results: RetinaNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n138\nA.15 Instance Segmentation Results Results: MaskRCNN\n. . . . . . . . . . . . . . . . . . . . . . . .\n138\nA.16 Overall Semantic Segmentation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n139\nA.17 Semantic Segmentation Results: HRNetV2 + C1 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n139\nA.18 Semantic Segmentation Results: MobileNetV2 + C1 . . . . . . . . . . . . . . . . . . . . . . . . .\n139\nA.19 Semantic Segmentation Results: ResNet 18 + PPM . . . . . . . . . . . . . . . . . . . . . . . . . .\n140\nA.20 Semantic Segmentation Results: Resnet50 + UPerNet . . . . . . . . . . . . . . . . . . . . . . . .\n140\nA.21 Semantic Segmentation Results: ResNet 50 + PPM\n. . . . . . . . . . . . . . . . . . . . . . . . .\n140\nA.22 Semantic Segmentation Results: ResNet 101 + UPerNet . . . . . . . . . . . . . . . . . . . . . . .\n141\nA.23 Semantic Segmentation Results: ResNet 101 + PPM . . . . . . . . . . . . . . . . . . . . . . . . .\n141\nB.1\nEquivalent quality visualizations. For each image we show the input JPEG, the JPEG with\nequivalent SSIM to our model output, and our model output. . . . . . . . . . . . . . . . . . . .\n145\nB.2\nFrequency domain results 1/4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n146\nB.3\nFrequency domain results 2/4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n146\nB.4\nFrequency domain results 3/4.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n147\nB.5\nFrequency domain results 4/4.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n147\nB.6\nModel interpolation results 1/4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n148\nB.7\nModel interpolation results 2/4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n148\nB.8\nModel interpolation results 3/4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n149\nB.9\nModel interpolation results 4/4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n149\nB.10 Qualitative results 1/4. Live-1 images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n150\nB.11\nQualitative results 2/4. Live-1 images.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n150\nB.12 Qualitative results 3/4. Live-1 images.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n151\nB.13 Qualitative results 4/4. ICB images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n151\nB.14 Fine Tuned Model Comparison\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n152\nB.15 Oﬀ-the-Shelf Artifact Correction Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n153\nB.16 Task-Targeted Artifact Correction Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n153\nB.17 FasterRCNN TIDE Plots. Left: quality 10, Middle: quality 50, Right: quality 100. . . . . . . . . .\n154\nB.18 MaskRCNN TIDE Plots. Left: quality 10, Middle: quality 50, Right: quality 100. . . . . . . . . .\n154\nB.19 MobileNetV2, Ground Truth: “Pembroke, Pembroke Welsh corgi”\n. . . . . . . . . . . . . . . .\n155\nB.20 FasterRCNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n156\nB.21 MaskRCNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n157\nB.22 HRNetV2 + C1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n158\nB.23 Dark Region . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n159\nB.24 Crowd\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n159\nB.25 Texture Restoration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n159\nB.26 Compression Artifacts Mistaken for Texture . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n160\nB.27 Motion Blur\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n160\nB.28 Artiﬁcial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n160\nList of Tables\n7.1\nModel Conversion Accuracies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n8.1\nSummary of JPEG Artifact Correction Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n9.1\nQGAC Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n12.1 Summary of Video Compression Reduction Techniques . . . . . . . . . . . . . . . . . . . . . . .\n113\n13.1 Metabit HEVC Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n123\n13.2 Metabit AVC Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n123\n13.3 Metabit GAN Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n123\nA.1 Results for classiﬁcation models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n142\nA.2 Results for detection models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n142\nA.3 Results for segmentation models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n143\nA.4 Reference results (results with no compression). . . . . . . . . . . . . . . . . . . . . . . . . . . .\n143\nList of Code Listings\n4.1\nBuilding a Huﬀman Tree. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n7.1\nExploding Convolutions (Naive) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n7.2 Exploding Convolutions (Fast)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n7.3 Transform Domain Batch Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\nNotation\n풖푖\nA vector (i.e., a member of a Vector space) indexed by 푖.\n풗푗\nA co-vector (i.e., a member of the dual of a Vector space) indexed by 푗.\n·\nScalar multiplication.\n훿푖\n푗\nThe Kronecker delta which is 1 when 푖= 푗and zero otherwise.\n∧\nThe logical “and” operation which is true if both arguments are true and false otherwise.\n⟨풗, 풖⟩\nThe inner product of 풗and 풖.\n∨\nThe logical “or” operation which is false if both arguments are false and true otherwise.\n∇푥푓(풙) The gradient of the scalar valued function of a vector 푓() with respect to 푥.\n⊙\nThe Hadamard (element-wise) product.\n⊗\nThe tensor product.\n★\nThe discrete, valid convolution or cross-correlation operator.\n×\nThe Cartesian product of two sets: the set of pairs of elements with one component from each set.\nSometimes used to construct a vector (as in ℝ푛). Also used to denote the size (of an image or block,\ne.g., 8 × 8 pixel blocks.)\n푎\nA scalar (i.e., a member of a Field).\n퐹\nA ﬁeld of numbers.\n푔푖푗\nA contravariant metric tensor.\n푔푖푗\nA covariant metric tensor.\n퐻(푥)\nThe Heaviside function which is 1 when 푥> 0 and 0 otherwise.\n푖\nThe imaginary number such that 푖2 = −1.\n푀\nA matrix in the classical sense, a 2D array of numbers, an order-2 tensor with unspeciﬁed type\n(type-(2, 0), (1, 1), or (0, 2)).\n푆\nA set, i.e., a collection of elements with no duplicates.\n푈푎푏푐...\n푖푗푘...\nA type-(m, n) Tensor indexed by 푎, 푏, 푐, . . . and 푖, 푗, 푘, . . ., a member of the Tensor space generated by\n푉⊗푉⊗. . . ⊗푉∗⊗푉∗.\n푉\nA vector space.\n푉∗\nA co-vector space, the dual of 푉.\nPreliminaries\nLinear Algebra 1\n1.1 Scalars, Vectors, and Matri-\nces . . . . . . . . . . . . . . . . 3\n1.2 Bases and Finite Dimen-\nsional Vector Spaces . . . . . 6\n1.3 Inﬁnite Dimensional Vector\nSpaces . . . . . . . . . . . . . .\n7\n1.4 Abstractions . . . . . . . . . . 8\nT\no begin the dissertation, we brieﬂy review the fundamental ideas of\nlinear algebra. These concepts are extremely important for modeling\nin the high dimensional spaces used by deep learning, and indeed\ndeﬁning what a high dimensional space actually is and how it behaves.\nGeneralizations of linear algebra, which we will cover in the next chapter,\nhave a special relationship with the dissertation outside of this general\nimportance: we will use these ideas to represent JPEG compression.\nLinear algebra also forms the basis of harmonic analysis which is central\nto lossy image compression.\nWarning\nIf you are familiar with the algebraic deﬁnitions of linear algebra, this\nchapter may seem somewhat hand-wavy. It is intended as a general\nintroduction and we will generalize it later.\n1.1 Scalars, Vectors, and Matrices\nAll concepts in mathematics relate back to the foundational idea of the\nnumber. For our purposes, we will call a single number a scalar. Scalars\nwill be denoted as a lower case letter in regular font: 푎.\nWe can “stack” several scalars in rows or columns to create vectors.\nWhen we wish to call attention to a vector, we will use lowercase bold\nfont. For example\n풃=\n\n푏0\n푏1\n...\n푏푛\n\n(1.1)\nis a vector made by stacking the 푛scalars in a column.\n풄=\n\u0002\n푐0\n푐1\n· · ·\n푐푛\n\u0003\n(1.2)\nis also a vector made by stacking 푛scalars in a row. We will call 푛the\ndimension of the vector. Note that in general 풃≠풄. We call 풃a vector\nand 풄a co-vector. The distinction will become important later. For now,\nwe deﬁne the transpose operation on a vector which transforms a vector\ninto a co-vector or a co-vector into a vector\n풄푇=\n\n푐0\n푐1\n...\n푐푛\n\n(1.3)\n4\n1 Linear Algebra\nGiven a scalar and a vector, we can multiply them to produce a new\nvector\n풅= 푎풄\n(1.4)\n=\n\u0002\n푎푐0\n푎푐1\n· · ·\n푎푐푛\n\u0003\n(1.5)\nwhere each component of 풄was multiplied by 푎, thus scaling the vector\nby 푎hence the name scalar. We can also add vectors by summing their\ncomponents to produce another vector. Given a set of 푚vectors 푉of\ndimension 푛.\n풆=\nX\n풗∈푉\n푣=\n(1.6)\n=\n\u0002P\n풗∈푉푣0\nP\n풗∈푉푣1\n· · ·\nP\n풗∈푉푣푛\n\u0003\n(1.7)\nWe can now combine these operations to create one of the most\nfundamental ideas of linear algebra: the linear combination. A linear\ncombination is the sum of the product of some number of vectors and\nscalars and therefore produces a new vector. Let 푆be a set of 푚scalars\n품=\n푚\nX\n푖=0\n푠푖풗풊\n(1.8)\nfor 푠푖∈푆and 풗풊∈푉\nGiven two vectors, we can multiply them by computing their inner\nproduct which produces a scalar\n푓= ⟨풃, 풄⟩=\n푛\nX\n푖=0\n푏푖푐푖\n(1.9)\nWe deﬁne the 푙2 norm of a vector as\n||풃||2 =\np\n⟨풃, 풃⟩\n(1.10)\nWe call any vector 풖such that ||풖||2 = 1 a unit vector, noting that we\nnormalize a vector by computing the vector\n풃\n||풃||2 . Any two vectors 풗and\n풘such that\n⟨풗, 풘⟩= 0\n(1.11)\nare said to be perpendicular or orthogonal to each other.\nThe formula for the 푙2 norm\n||풃||2 =\ns\n푛\nX\n푖=0\n푏2\n푖\n(1.12)\nimplies a general formulation for an 푙푛norm\n||풃||푛=\n \n푛\nX\n푖=0\n푏푛\n푖\n! 1\n푛\n(1.13)\n1.1 Scalars, Vectors, and Matrices\n5\n1: We will use upper indices much more\nfrequently than powers in this disserta-\ntion so it is advised to get familiar with\nthe notation now. It will be left entirely\nto context to determine which we mean.\nAnother useful norm is the 푙1 norm\n||풃||1 =\n\f\f\f\f\f\n푛\nX\n푖=0\n푏푖\n\f\f\f\f\f\n(1.14)\nTaking the limit as 푛→∞gives the 푙∞norm\n||풃||∞= max(푏푖)\n(1.15)\nWe can create two dimensional arrays of scalars which we call matrices\nand which we denote with upper case normal font: 퐴.\n퐴=\n\n푎11\n푎12\n· · ·\n푎1푛\n푎21\n푎22\n· · ·\n푎2푛\n...\n...\n...\n...\n푎푚1\n푎푚2\n· · ·\n푎푚푛\n\n(1.16)\n퐴is said to be 푚× 푛dimensional.\nWe multiply a matrix and a vector by taking the linear combination\nof each element of the vector with the corresponding column of the\nmatrix\n풉= 퐴풃=\n푛\nX\n푖=1\n푏푖푨풊\n(1.17)\nnote that 푏푖is the 푖th element of 풃which is a scalar and 푨풊is the 푖th\ncolumn of 퐴which is a vector. Note also that the result 풉is a vector of\ndimension 푚. This equation implies that the number of columns in 퐴\nmust match the number of rows in 풃. We can extend this to matrix-matrix\nproducts, given an 푛× 푚matrix 퐵\n퐶= 퐴퐵\n(1.18)\n=\n\n⟨(퐴1)푇, 퐵1⟩\n⟨(퐴1)푇, 퐵2⟩\n· · ·\n⟨(퐴1)푇, 퐵푚⟩\n⟨(퐴2)푇, 퐵1⟩\n⟨(퐴2)푇, 퐵2⟩\n· · ·\n⟨(퐴2)푇, 퐵푚⟩\n...\n...\n...\n...\n⟨(퐴푛)푇, 퐵1⟩\n⟨(퐴푛)푇, 퐵2⟩\n· · ·\n⟨(퐴푛)푇, 퐵푚⟩\n\n(1.19)\nwhere 퐴푗denotes the 푗th row1 from 퐴. In other words, each entry in 퐶is\nthe inner product of the corresponding row of 퐴with the corresponding\ncolumn of 퐵. This construct implies a particular identity matrix\n퐼=\n\n1\n0\n· · ·\n0\n0\n1\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\n1\n\n(1.20)\nMatrices are important for representing linear maps on vectors. A linear\nmap is any map which preserves vector addition and scalar multiplication.\nWe can essentially “store” the coeﬃcients of linear maps in matrices\nand use the action of matrix-vector multiplication to apply the map to a\nvector.\n6\n1 Linear Algebra\n1.2 Bases and Finite Dimensional Vector Spaces\nGiven a set of vectors 퐵, we deﬁne the Span of 퐵as the set of all linear\ncombinations of the vectors in 퐵. Formally, given a set of scalars 푆\nspan(퐵) =\n(\n|푉|\nX\n푖=1\n휆푖b푖\n\f\f\f\f\fb푖∈퐵, 휆푖∈푆\n)\n(1.21)\nGiven some arbitrary set of vectors 푉, we may wish to ﬁnd 퐵, i.e., a subset\nof vectors that spans 푉. If all elements of 퐵are linearly independent, we\nsay that 퐵is a basis of 푉. The basis allows us to express any element of 푉\nin terms of scalars of elements of 퐵and, in eﬀect, deﬁnes 푉.\nThere may be many bases for the same set of vectors, so we may wish\nto change the basis and we may wish to deﬁne a particular basis as\ncanonical. For example, consider the vector space ℝ3. We often choose\nthe following basis\n풆0 =\n\n1\n0\n0\n\n풆1 =\n\n0\n1\n0\n\n풆2 =\n\n0\n0\n1\n\n(1.22)\nThis canonical basis is desirable because the vectors are all orthonormal,\ni.e., they are all orthogonal to each other and have magnitude of 1 which\nmeans there is no “rotation” or “scaling” of the coordinates. Moreover, this\nbasis makes it extremely easy to express vectors in a familiar component\nnotation. The vector\n풗=\n\n1\n2\n3\n\n(1.23)\nis only expressed as such because we chose this canonical basis (implicitly)\nand deﬁned 풗as\n풗= 1풆0 + 2풆1 + 3풆2\n(1.24)\nIf we wish to change the basis, we ﬁrst write the coordinates of the new\nbasis (퐵1) vectors in the old basis (퐵0) and then stack these vectors into a\nmatrix 퐴. We can then multiply any vector 풗0 written in terms of basis\n퐵0 by 퐴to obtain the coordinates in terms of basis 퐵1.\n풗1 = 퐴풗0\n(1.25)\nWe make the following notes about bases for 푉\n1. 푉has a basis\n2. All bases of 푉have the same cardinality which is the dimension\nof 푉. If we write 풗∈푉in coordinates and count the number of\ncoordinates, that count will be the same as the number of basis\nvectors\nNote the implication of the last property, we can count the number of\nelements in the dimension of V, therefore, V is a ﬁnite dimensional vector\nspace.\n1.3 Inﬁnite Dimensional Vector Spaces\n7\n1.3 Inﬁnite Dimensional Vector Spaces\nInﬁnite dimensional vector spaces will play an important role in the\nlater analysis of compression, although the results of this analysis will\neventually be discretized for use on a computer. In principle, inﬁnite\ndimensional vector spaces behave in much the same way as ﬁnite dimen-\nsional ones. While a full treatment of this topic is beyond the scope of the\ndissertation, we will make some deﬁnitions in this chapter which will be\nexpanded upon later.\nAssume that 풇and 품are members of an inﬁnite dimensional vector\nspace 푉. We can think about components of 풇and 품as being indexed\nby any real number instead of a ﬁnite number of natural numbers.\nFor example we might have 풇(2) = 4 for the second component and\n풇(−12.5) = −156.25 for the negative-twelve-point-ﬁve-th component. In\nother words, 풇and 품are functions, and these functions are vectors in a\nvector space.\nWith that established, our next goal should be to produce a basis\nfor these functions. After all, being able to express a function as the\ncoeﬃcients of some basis should have myriad uses especially if we do\nnot know exactly the form of the function we wish to express. We will\ndevelop this basis later in the dissertation but for now we can deﬁne\ntwo important concepts: orthogonality of functions and normality of\nfunctions.\nRecall that two vectors were said to be orthogonal If they point at\nright angles to each other, i.e., their inner product is zero. To determine\northogonality we need an inner product for functions. We make the\nfollowing deﬁnition\n\n푓(푥), 푔(푥)\n\u000b\n=\n∫∞\n−∞\n푓(푥)푔(푥)푑푥\n(1.26)\nwhich is exactly the same as the inner product formula in ﬁnite dimensions\nwith the sum expanded to an integral. As usual, if\n\n푓(푥), 푔(푥)\n\u000b\n= 0 then\nthe functions are orthogonal.\nNext we need to deﬁne normality of a function. Recall that a vector\nwas said to be normal if its length is 1. So we need a way of deﬁning the\n“length” of a function. We make the following deﬁnition\n∥푓(푥)∥2 =\ns∫∞\n−∞\n푓2(푥)푑푥\n(1.27)\nOnce again, this is the same as the discrete formula using only an integral\nand if ∥푓(푥)∥= 1 then the vector is a normal vector.\nGiven the tools to determine if a set of functions are orthonormal we\ncan now develop what is essentially a canonical basis for functions. This\ndiscussion will be continued in Chapter 3 (Harmonic Analysis).\n8\n1 Linear Algebra\n1.4 Abstractions\nWhile the geometric interpretations provide useful intuitions, there is a\nlimit to how far we can take them mathematically. We conclude by brieﬂy\nintroducing the abstract forms of the ideas in this chapter.\nA ﬁeld 퐹is a set on which addition and multiplication are deﬁned.\nSpeciﬁcally we deﬁne\n+ : 퐹× 퐹→퐹\n(1.28)\n· : 퐹× 퐹→퐹\n(1.29)\nand stipulate that if they meet the following criteria for 푎, 푏, 푐∈퐹:\nAssociativity addition and multiplication are associative: 푎+ (푏+ 푐) =\n(푎+ 푏) + 푐and 푎· (푏· 푐) = (푎· 푏) · 푐\nCommutivity addition and multiplication are commutative: 푎+푏= 푏+푎\nand 푎· 푏= 푏· 푎\nIdentity Two diﬀerent elements 0 and 1 exist that satisfy additive and\nmultiplicative identity respectively: 푎+ 0 = 푎, 푎· 1 = 푎\nInverse There exists an additive inverse −푎and a multiplicative inverse\n푎−1 such that 푎+ (−푎) = 0 and 푎· 푎−1 = 1\nDistributivity Multiplication and addition distribute according to 푎·\n(푏+ 푐) = (푎· 푏) + (푎· 푐)\nthen 퐹is a ﬁeld.\nWe deﬁne a vector space 푉over the ﬁeld 퐹in a similar way. We have\ntwo operations\n+ : 푉× 푉→푉\n(1.30)\n· : 퐹× 푉→푉\n(1.31)\nand we call 푉a vector space, elements of 푉vectors, and elements of 퐹\nscalars if for 풖, 풗, 풘∈푉and 푎, 푏∈퐹,\nAssociativity addition is associative: 풖+ (풗+ 풘) = (풖+ 풗) + 풘\nCommutivity addition is commutative: 풖+ 풗= 풗+ 풖\nIdentity and Inverse two elements 0 and −풗exist such that 풗+ 0 = 풗\nand 풗+ (−풗) = 0\nCompatibility of Multiplication Scalar and ﬁeld multiplication are com-\npatible: 푎· (푏· 풗) = (푎· 푏) · 풗\nScalar Multiplication Identity Multiplication with the scalar identity:\n1 · 풗= 풗\nDistributivity Scalar multiplication is distributive with respect to both\nvector and ﬁeld addition: 푎· (풖+ 풗) = 푎· 풖+ 푎· 풗and (푎+ 푏) · 풗=\n푎· 풗+ 푏· 풗\nNote that we made no mention of coordinates or numbers, we only\ndeﬁned sets and operations along with their behavior.\nWith these deﬁnitions we can form linear combinations for 풘, 풗0 . . . 풗푵∈\n푉and 푎0 . . . 푎푁∈퐹\n푤=\n푁\nX\n푖=0\n푎푖· 풗풊\n(1.32)\n1: One important thing to note at this\npoint is that we can only tweak one argu-\nment at a time, we cannot, for example,\ncompute 퐵(푣0 + 푣1, 푢0 + 푢1) and expect\na linear result.\nMultilinear Algebra 2\n2.1 Tensors . . . . . . . . . . . . . 10\n2.2 Tensor Products and Ein-\nstein Notation . . . . . . . . . 10\n2.3 Tensor Spaces . . . . . . . . . 12\n2.4 Linear Pixel Manipulations 12\nT\nhe previous chapter developed vectors and matrices where vectors\nare a primary “mathematical object” in a high-dimensional space\nand a matrix represents a map which can transform that object. In a\nsense, this discussion feels unﬁnished. We had scalars which were zero\ndimensional, vectors which were one dimensional, and matrices which\nwere two dimensional. Why stop there?\nIn this chapter we develop the extremely high level ideas of multilinear\nalgebra which generalizes linear algebra to higher dimensional objects.\nThis is a large and complex topic which we only need a small piece of\nfor understanding this dissertation, in fact this entire chapter may be\ncloser to the ﬁrst lecture in a semester long graduate course. This chapter\nwill immediately obsolete the matrix and vector notation we introduced\nin the previous chapter for reasons which will be explained in the ﬁrst\nsection.\nThe primary goal of multilinear algebra is to study multilinear maps.\nRecall that linear maps are maps which preserve vector addition and\nscalar multiplication. More formally, we call 퐴: 푉→푉a linear map on\nthe vector space 푉over the ﬁeld 퐹if, for 푣, 푢∈푉and 푎∈퐹\n▶퐴(푣+ 푢) = 퐴(푣) + 퐴(푢)\n▶퐴(푎· 푣) = 푎· 퐴(푣)\nA bilinear map is an extension of this concept to two arguments where\nthe map is linear in each argument. We call 퐵: 푉× 푉→푉a bilinear\nmap with vector space 푉and ﬁeld 퐹for 푣0, 푣1, 푢1, 푢1 ∈푉and 푎∈퐹if\n▶퐵(푣0+푣1, 푢0) = 퐵(푣0, 푢0)+퐵(푣1, 푢0) and 퐵(푣0, 푢0+푢1) = 퐵(푣0, 푢0)+\n퐵(푣0, 푢1)\n▶퐵(푎· 푣0, 푢0) = 퐵(푣0, 푎· 푢0) = 푎· 퐵(푣0, 푢0)\nContinuing this until its natural end, we call a multilinear map a function\nof multiple arguments which is linear in each one1. 푀: 푉×푉×· · ·×푉→\n푉is a multilinear map with 푣0···푁, 푢∈푉and 푎∈퐹if\n▶푀(푣0+푢, 푣1, · · · , 푣푁) = 푀(푣0, 푣1, · · · , 푣푁)+푀(푢, 푣1, · · · , 푣푁) and\n푀(푣0, 푣1+푢, · · · , 푣푁) = 푀(푣0, 푣1, · · · , 푣푁)+푀(푣0, 푢, · · · , 푣푁), etc.\n▶푀(푎· 푣0, 푣1, · · · , 푣푁) = 푀(푣0, 푎· 푣1, · · · , 푣푁) · · · 푀(푣0, 푣1, · · · , 푎·\n푣푁) = 푎· 푀(푣1, 푣1, · · · , 푣푁)\nWe will represent multilinear maps using higher order objects called\ntensors. Perhaps surprisingly, the practical use of these concepts in the\ndissertation will still only be linear or bilinear maps, however, we leverage\nmultilinear algebra by working on tensor inputs and outputs which serve\nas a natural representation for images vs the vectors that are traditionally\nused in these maps.\n10\n2 Multilinear Algebra\n2.1 Tensors\nTraditionally in computer science we think of tensors as multidimen-\nsional arrays of numbers. Despite the protests of many physicists and\nmathematicians, this is a perfectly reasonable deﬁnition of a tensor. For\nexample, we might have a 3- or 4- or 5D array of numbers and call\nthis a tensor. In the mathematical sense, a tensor is a representation\nof a multilinear map. We will denote tensors and tensor spaces with\nuppercase math font: 푇.\nRecall the concepts of vectors and co-vectors. We will refer to vector\nspaces as 푉and co-vector spaces as 푉∗. It is important to keep in mind\nthat although these spaces are related they are not the same. These vectors\nand co-vectors will, in a sense, be the primitives that we use to construct\ntensors. We will index vectors using upper indices and co-vectors using\nlower indices.\nAll tensors have a type, which is the primary way we will refer to\nthem. Some texts refer to a tensor rank, we do not use this convention\nbecause it is ambiguous. Rank has other meanings in linear algebra and\ntensor rank does not explain the composition of the tensor in terms\nof vectors and co-vectors. If we absolutely have to refer to the sum of\nthe number of vector and co-vector spaces we will call this the order\nof the tensor, though this situation will be extremely rare. We will say\nthat vectors are type-(1, 0) tensors and co-vectors are type-(0, 1) tensors.\nMatrices can then be type-(2,0) tensors, type-(1, 1) tensors, or type-(0, 2)\ntensors. The distinction between type is important. Since matrices and\nvectors now have concrete deﬁnitions as tensors, this obsoletes our earlier\nnotation which drew a distinction between them. From this point on, all\nnon-scalars will be written in tensor notation.\n2.2 Tensor Products and Einstein Notation\nWe construct arbitrary tensors using products of vectors and co-vectors.\nTo do this we deﬁne the tensor product of two tensors. We will build up\nto this by revisiting some concepts from linear algebra. Given two vectors\n푣, 푢in some vector space 푉on a ﬁeld 퐹, we deﬁned the inner product\nas\n푁\nX\n푖=0\n푣푖푢푖= 푎\n(2.1)\nwhere 푎∈퐹. Given a matrix (a type-(1, 1) tensor) 푀we can compute the\nmatrix-vector product as\n푁\nX\n푖=0\n푀푖푥푖= 푤\n(2.2)\nfor 푤∈푉. Similarly we compute the matrix-matrix product given another\nmatrix 푁as\n푁\nX\n푖=0\n푀푖푁푖= 푂\n(2.3)\n2.2 Tensor Products and Einstein Notation\n11\nEinstein, “Die grundlage der allge-\nmeinen relativitätstheorie”\nThese expressions can by simpliﬁed using Einstein notation (Einstein\n1923). In Einstein notation, repeated indices that appear as upper and\nlower indices are assumed to be summed out, allowing us to remove the\nsummations from the previous equations. For example, the matrix-matrix\nproduct is now simply\n푀푗\n푖푁푖\n푘= 푂푗\n푘\n(2.4)\nwhere the non-summed indices are added in for clarity. This is extremely\nimportant when working with general tensors because the expressions\nare quite verbose with summation notation. We will make heavy use of\nEinstein notation in this dissertation so it is important to understand it\nnow.\nGiven two arbitrary tensors we can now deﬁne the generic tensor\nproduct\n푇⊗푈= 푇푢0,푢1,··· ,푢푁\n푙0,푙1,··· ,푙푁\n푈\n푢′\n0,푢′\n1,··· ,푢′\n푁\n푙′\n0,푙′\n1··· ,푙′\n푁\n= 푉\n푢0,··· ,푢푁,푢′\n0··· ,푢′\n푁\n푙0,··· ,푙푁,푙′\n0,··· ,푙′\n푁\n(2.5)\nOf course we are free to form other useful products for tensors. For\nexample, given a type-(2, 3) tensor 푃and a type-(4, 2) tensor 푄we could\ncompute the type-(4, 3) tensor 푅as\n푃푘푚푙\n푖푗\n푄푖푗\n푎푏푐푑= 푅푘푚푙\n푎푏푐푑\n(2.6)\nwhere we have summed out the 푖, 푗indices.\nTo construct a tensor from vectors and co-vectors we can use this tensor\nproduct. Consider the vectors 푢, 푣∈푉and the co-vectors 푝, 푞, 푟∈푉∗.\nWe can construct a type-(3, 2) tensor from these by computing\n푢푖푣푗푝푘푞푙푟푚= 푇푖푗\n푘푙푚\n(2.7)\nIn many situations it will be useful for us to raise or lower indices\n(sometimes called index juggling). In other words, given a tensor 푇푖푗\nwe may want to construct 푇푖\n푗or 푇푖푗. These tensors are related to 푇푖푗but\nthey are not the same. We can accomplish this by multiplying 푇by\nthe covariant or contravariant metric tensor which relates to vector and\nco-vector spaces. These tensors are deﬁned such that\n푔푖푘푔푘푗= 훿푖\n푗\n(2.8)\nwhere 훿is the Kronecker delta\n훿푖\n푗=\n(\n0\n푖≠푗\n1\n푖= 푗\n(2.9)\na generalization of the identity matrix from linear algebra, 푔푖푗is the\ncontravariant metric (for converting co-vectors to vectors) and 푔푖푗is the\ncovariant metric (for converting vectors to co-vectors).\nFor various reasons we will consider a general derivation of the metric\ntensors to be beyond the scope of this dissertation, and in fact we will\nalways be using tensors deﬁned with respect to the canonical basis which\n12\n2 Multilinear Algebra\nFigure 2.1: Grayscale Example Image.\nhas a metric of 훿. This means we can freely raise and lower indices\nwithout considering the metric.\n2.3 Tensor Spaces\nIf we needed to start with vectors every time we wanted to build a tensor\nit would quickly become unsustainable. Instead, we need a way to refer\nto tensor spaces, or sets of tensors. This is sometimes referred to as the\nintrinsic deﬁnition of a tensor. We again use the tensor product but this\ntime we use vector and co-vector spaces. Recalling the type-(2, 3) tensor\n푇which we constructed from vectors and co-vectors, we can deﬁne 푇\ndirectly as\n푇∈푉⊗푉⊗푉∗⊗푉∗⊗푉∗\n(2.10)\nin other words, 푉⊗푉⊗푉∗⊗푉∗⊗푉∗deﬁnes a space of tensors. This\nspace contains all tensors which can be constructed by the tensor product\nof 푉twice and 푉∗three times. In other words, all tensors which can\nbe built from an equation like Equation 2.7 but with any 푢, 푣∈푉and\n푝, 푞, 푟∈푉∗.\nFor a generic tensor T, we say that it is of type-(p, q) for\n푇∈푉⊗· · · ⊗푉\n|        {z        }\n푝times\n⊗푉∗⊗· · · ⊗푉∗\n|          {z          }\n푞times\n(2.11)\nThis will be the primary convention that we use to deﬁne tensors in the\nrest of this dissertation. Note that this mimics some of the deﬁnitions from\nSection 1.4 (Abstractions) in that we no longer have need of coordinates,\nwe only deal with arbitrary vector spaces, co-vector spaces, and the\ntensor products of their members which is why this is called the intrinsic\ndeﬁnition. We close by noting that although we have only used 푉and 푉∗,\nin general, the vector spaces deﬁning a tensor can be diﬀerent provided\nthat the spaces are deﬁned over the same ﬁeld.\n2.4 Linear Pixel Manipulations\nWith the boring theory out of the way we can look at an interesting practi-\ncal application of tensors: linear pixel manipulations. By representing an\nimage as a tensor we can compute many complex transformations of the\nimage using other tensors. Some of these are not traditionally thought\nof as being “linear” when we restrict our thinking to two-dimensional\nmatrices as linear maps that transform images through matrix multipli-\ncation. Instead of thinking of images as “collections of vectors” we treat\nthe image as one object: a higher order tensor and then we consequently\ndeﬁne the linear map on this object in even higher dimensions.\nMore formally, we will deal with planar images. The image may have\nany number of channels but it always has two spatial dimensions. So a\ngrayscale image would be a type-(0, 2) tensor. A traditional color image\nwould be a type-(0, 3) tensor. In most cases, even for color images it will\n2.4 Linear Pixel Manipulations\n13\nFigure\n2.2:\nGrayscale\nGaussian\nSmoothing.\nFigure 2.3: Color Example Image.\nsuﬃce to deﬁne linear maps as type-(2, 2) tensors which transform the\nspatial dimensions while preserving the channel dimension.\nWe begin with a simple example. Consider the example image in\nFigure 2.1. We can represent this grayscale image as a type-(0, 2) tensor\n퐼∈퐻∗⊗푊∗. One simple linear manipulation we can perform on this\nimage is Gaussian smoothing in a 3 × 3 window. We can represent this\nlinear map as a type-(2, 2) tensor\n퐺: 퐻∗⊗푊∗→퐻∗⊗푊∗\n(2.12)\n퐺∈퐻⊗푊⊗퐻∗⊗푊∗\n(2.13)\n퐺푖푗\n푢푣=\n\n\n0.5\n푖= 푢∧푗= 푣\n0.125\n푖= 푢∧(푗= 푣−1 ∨푗= 푣+ 1)\n0.125\n(푖= 푢−1 ∨푖= 푢+ 1) ∧푗= 푣\n0\notherwise\n(2.14)\nFrom the ﬁrst equation, we can see that 퐺is a linear map on type-(0, 2)\ntensors that transforms them into type-(0, 2) tensors. From the second\nequation we see that 퐺is a type-(2, 2) tensor (this is a consequence of\nthe ﬁrst equation). The third equation deﬁnes the form of 퐺for arbitrary\nindices 푖, 푗, 푢, 푣. In this case, 푖, 푗index the input pixel and 푢, 푣index the\noutput pixel, the value stored at the index is the coeﬃcient of the pixel.\nSo that is 0.5 when the indices are equal and 0.125 for any neighboring\npixels, all other pixel have a zero coeﬃcient. We apply this linear map by\ncomputing\n퐼′\n푢푣= 퐺푖푗\n푢푣퐼푖푗\n(2.15)\nThe result of this computation is show in Figure 2.2.\nNext we can consider a color image. The color version of the example\nimages is shown in Figure 2.3. Converting this color image to grayscale is\na linear manipulation. We represent the color image as 퐼∈푃∗⊗퐻∗⊗푊∗.\nWe then deﬁne the following linear map\n푌: 푃∗⊗퐻∗⊗푊∗→퐻∗⊗푊∗\n(2.16)\n푌∈푃⊗퐻⊗푊⊗퐻∗⊗푊∗\n(2.17)\n푌푝푖푗\n푢푣=\n\n\n0.299\n푝= 0\n0.587\n푝= 1\n0.114\n푝= 2\n(2.18)\nwhich comes directly from the grayscale conversion equation\n푌= 0.299푅+ 0.587퐺+ 0.114퐵\n(2.19)\nFigure 2.4: Color Smoothing.\nWe apply this map as\n퐼′\n푢푣= 푌푝푖푗\n푢푣퐼푝푖푗\n(2.20)\nIf we apply this map to the example image we get the same image as\nFigure 2.1. Interestingly, we can apply 퐺to this color image as well and it\n14\n2 Multilinear Algebra\nFigure 2.5: Color Downsampling.\nwill perform correct smoothing on the color image (Figure 2.4). In this\ncase we would be computing\n퐼′\n푝푢푣= 퐺푖푗\n푢푣퐼푝푖푗\n(2.21)\nand since 퐺∈퐻⊗푊⊗퐻∗⊗푊∗, the channel dimension of 퐼, 푃∗, is\npreserved.\nLet’s try something more interesting: resampling. We can deﬁne nearest\nneighbor up- and downsampling as linear maps. This works for both\ncolor and grayscale images, the computation is the same and the tensor\nwill be type-(2, 2). For downsampling by a factor of 2 we deﬁne the\nfollowing linear map\n퐻: 퐻∗⊗푊∗→퐻′∗⊗푊′∗\n(2.22)\n퐻∈퐻⊗푊⊗퐻′∗⊗푊′∗\n(2.23)\n퐻푖푗\n푢푣=\n(\n1\n푖= 2푢∧푗= 2푣\n0\notherwise\n(2.24)\nwhere 퐻′ and 푊′ are vector spaces with half the dimension of 퐻and 푊.\nWe can deﬁne upsampling in a similar way. For upsampling by a factor\nof 2 we deﬁne the following linear map\n퐷: 퐻′∗⊗푊′∗→퐻∗⊗푊∗\n(2.25)\n퐻∈퐻′ ⊗푊′ ⊗퐻∗⊗푊∗\n(2.26)\n퐻푖푗\n푢푣=\n(\n1\n푖= ⌊푢/2⌋∧푗= ⌊푣/2⌋\n0\notherwise\n(2.27)\nWe apply these maps by computing\n퐼′\n푢푣= 퐻푖푗\n푢푣퐼푖푗\n(2.28)\n퐼푢푣= 퐷푖푗\n푢푣퐼′\n푖푗\n(2.29)\nfor grayscale images and\n퐼′\n푝푢푣= 퐻푖푗\n푢푣퐼푝푖푗\n(2.30)\n퐼푝푢푣= 퐷푖푗\n푢푣퐼′\n푝푖푗\n(2.31)\nfor color images. The result for the color image is shown in Figure 2.5.\nTaking this further, we can deﬁne any convolution or cross-correlation\nusing tensors. This is reasonable since we know that convolutions are\nlinear operations although we do not always see them written out as\nlinear maps. We consider a general convolution kernel 퐾with any shape.\nWe will denote the shape of the kernel as the tuple 푆= (푠0, 푠1). Then we\n2.4 Linear Pixel Manipulations\n15\n2: For example if we wanted even 8 ×\n8 blocks we might write these as ℝ8∗\nalthough I do not like this notation\ndeﬁne the following linear map\n퐶: 퐻∗⊗푊∗→퐻∗⊗푊∗\n(2.32)\n퐶∈퐻⊗푊⊗퐻∗⊗푊∗\n(2.33)\n퐶푖푗\n푢푣=\n(\n퐾푢−푖+푠0,푣−푗+푠1\n푢−푠0 ≤푖≤푢+ 푠0 ∧푣−푠1 ≤푗≤푣+ 푠1\n0\notherwise\n(2.34)\nnote that this does not consider a mapping between channels like we\nwould use in a convolutional network (this is simple enough to add in\nthough). We apply this to grayscale or color images as\n퐼′\n푢푣= 퐶푖푗\n푢푣퐼푖푗\n(2.35)\n퐼′\n푝푢푣= 퐶푖푗\n푢푣퐼푝푖푗\n(2.36)\nAs a taste of what’s to come, let’s try something more surprising. It\nmay sound surprising but breaking an image into evenly sized blocks\nis a linear operation, and we can derive a tensor which represents this\nmap. We will ﬁrst deﬁne two new co-vector spaces, the block dimensions\n푀∗and 푁∗2. We will also deﬁne the spaces 푋∗and 푌∗with dimension\nequal to the dimension of 퐻∗and 푊∗divided by the block size (i.e., the\nnumber of blocks that can ﬁt in the image). Then we can deﬁne a type-(2,\n4) tensor, the linear map\n퐵: 퐻∗⊗푊∗→푋∗⊗푌∗⊗푀∗⊗푁∗\n(2.37)\n퐵∈퐻⊗푊⊗푋∗⊗푌∗⊗푀∗⊗푁∗\n(2.38)\n퐵푖푗\n푥푦푚푛=\n(\n1\npixel ℎ, 푤belongs in block 푥, 푦at oﬀset 푚, 푛\n0\notherwise\n(2.39)\nwhich may seem like kind of a let down but this is the canonical form\nwe will use later in the dissertation. A more satisfying and programmer-\noriented deﬁnition might be\n퐵푖푗\n푥푦푚푛=\n(\n1\n푥· dim(푀) + 푚= 푖∧푦· dim(푁) + 푛= 푗\n0\notherwise\n(2.40)\nWe apply the map as\n퐼′\n푝푥푦푚푛= 퐵푖푗\n푥푦푚푛퐼푝푖푗\n(2.41)\nFigure 2.6: Block Linear Map Example.\nThe blocks are arranged spatially but\nnote that in tensor form there are sepa-\nrate indices for the block position and\nthe 2D oﬀset into each block.\nSince this one might be a little confusing, consider a concrete example\nwith the example image in Figure 2.1. This is a 16 × 16 image and we\nwant to break it into 8 × 8 blocks, so there will be four total blocks\nin a 2 × 2 grid (Figure 2.6). In this case, dim(푀) = dim(푁) = 8 and\ndim(푋) = dim(푌) = 2. So after applying 퐵to the 16×16 image we would\nget a tensor of shape 2 × 2 × 8 × 8 giving the spatial arrangement of the\n8 × 8 blocks.\n16\n2 Multilinear Algebra\nWhile this was a fun exercise the actual practical application of this\nidea is fairly limited since the tensors must be on the order of the image\nsize. A critical component of the dissertation is that we can actually\nrepresent all of JPEG as a linear map. This is extremely powerful because\nlinear maps are well studied phenomena, so expressing something as\ncomplex as JPEG as a single linear map gives us myriad tools for further\nanalysis and manipulation.\n1: It is interesting to note that although\nthis result is one of the most inﬂuential\nresults in all of engineering, it was given\nnegative reviews at the time Fourier pub-\nlished it.\nHarmonic Analysis 3\n3.1\nThe Fourier Transform . . 18\n3.2\nThe Gabor Transform . . . 21\n3.3\nWavelet Transforms . . . . 23\n3.3.1 Continuous and Discrete\nWavelet Transforms . . . . 23\n3.3.2 Haar Wavelets . . . . . . . . 25\nH\narmonic analysis is an invaluable tool for mathematics and engi-\nneering that enables some of the most important technologies in\nexistence today. In Section 1.3 (Inﬁnite Dimensional Vector Spaces) we\ntouched brieﬂy on the concept of inﬁnite dimensional vector spaces\nand we noted that the vector space of functions of real variables is one\nsuch space. In this chapter we expand upon this idea and introduce the\nFourier transform and harmonic analysis. The ideas we present in this\nchapter will be fundamental guiding principles behind image and video\ncompression.\nFourier was interested in solving the heat equation which describes\nthe temperature of an ideal length of wire over space and time. The\nequation deﬁnes a function 푢(푡, 푥) as a partial diﬀerential equation with\nconditions:\n휕\n휕푡푢(푡, 푥) = 휕2\n휕푥2 푢(푡, 푥)\n(3.1)\n푢(0, 푥) = 푓(푥)\n(3.2)\n푢(푡, 0) = 0\n(3.3)\n푢(푡, 1) = 0\n(3.4)\nfor 푡≥0 and 푥∈[0, 1].\nOf critical importance to us is the second equation which relates the\nform of 푢(푡, 푥) at time 푡= 0 to some arbitrary function of space. Fourier\nshowed that, since the other conditions of the heat equation yield a\nharmonic function, one composed of simple waves, 푓(푥) must be able to\nbe decomposed as such 1. While we will not go into the full derivation\nthat Fourier used, or even touch on the modern understanding of the\ntransform, we will show how this implies an orthonormal basis for\nfunctions which allows us to express them as a sum of coeﬃcients of\nsimple waves.\nNote\nThere are many diﬀerent ways to think about the fourier transform.\nFourier was thinking in terms of the heat equations, many people like\nto envision a “machine” that isolates frequencies. I prefer the model\nwhich is motivated by linear algebra and that is what I discuss in\nthis chapter although all views on the subject are equally correct and\ninteresting.\n18\n3 Harmonic Analysis\n3.1 The Fourier Transform\nRecall our deﬁnitions for the 푙2 norm and inner product of functions\n∥푓(푥)∥2 =\ns∫∞\n−∞\n푓2(푥) 푑푥\n(3.5)\n⟨푓(푥), 푔(푥)⟩=\n∫∞\n−∞\n푓(푥)푔(푥) 푑푥\n(3.6)\ngiven these tools we can try to ﬁnd something resembling a canonical\nbasis for functions. We would like a canonical basis to be a set of functions\nthat is orthonormal, i.e., a set of functions which are all of unit length\nand which are all orthogonal to each other.\nConsider the functions sin(푥) and cos(푥). We can show easily that\nthese functions are orthogonal to each other by solving\n⟨sin(푥), cos(푥)⟩=\n∫∞\n−∞\nsin(푥) cos(푥) 푑푥\n(3.7)\nWe start by restricting the domain to [−휋, 휋] since the sine and cosine\nfunctions are periodic.\n=\n∫휋\n−휋\nsin(푥) cos(푥) 푑푥\n(3.8)\nThen we use substitution to solve the integral. Let 푢= cos(푥) and\n푑푢= −푠푖푛(푥) 푑푥\n∫휋\n−휋\nsin(푥) cos(푥) 푑푥=\n∫\n푢−푑푢\n(3.9)\n= −\n∫\n푢푑푢\n(3.10)\n= −푢2\n2 + 퐶\n(3.11)\nsubstituting and evaluating the result gives\n−푢2\n2 + 퐶= −cos2(푥)\n2\n+ 퐶\n(3.12)\n−cos2(푥)\n2\n+ 퐶\n\f\f\f\f\n휋\n−휋\n(3.13)\n= −cos2(−휋)\n2\n+ cos2(휋)\n2\n= 0\n(3.14)\nso sine and cosine are indeed orthogonal. To check if they are normal we\ncompute\n∫휋\n−휋\ncos2(푥) 푑푥\n(3.15)\n∫휋\n−휋\nsin2(푥) 푑푥\n(3.16)\n3.1 The Fourier Transform\n19\nWe can solve the ﬁrst integral with the trigonometric identity\ncos2(푥) = 1 + cos(2푥)\n2\n(3.17)\nsubstituting gives\n∫휋\n−휋\n1 + cos(2푥)\n2\n푑푥\n(3.18)\n= 1\n2\n∫휋\n−휋\n1 + cos(2푥) 푑푥\n(3.19)\n= 1\n2\n\u0012∫휋\n−휋\n푑푥+\n∫휋\n−휋\ncos(2푥) 푑푥\n\u0013\n(3.20)\n= 1\n2\n\u0012\n푥+ sin(2푥)\n2\n\u0013\f\f\f\f\n휋\n−휋\n(3.21)\n= 푥\n2 + sin 2푥\n4\n\f\f\f\f\n휋\n−휋\n(3.22)\n= 휋\n2 + sin 2휋\n4\n−−휋\n2 −sin −2휋\n4\n(3.23)\n= 휋+ sin 2휋\n4\n−sin −2휋\n4\n(3.24)\n= 휋\n(3.25)\nWe get the same result for sine, so the functions are not normal but they\ncan be easily made normal by dividing by 휋. Therefore, sine and cosine\nseem like ideal candidates provided we can produce an inﬁnite set from\nthese two.\nIn order to have a basis for the inﬁnite dimensional space of functions\nwe need an inﬁnitely large set of basis vectors. Without further elaboration,\nthe Fourier transform deﬁnes this set as\n{sin(−2휋푥휁), cos(−2휋푥휁)|휁∈ℝ}\n(3.26)\nor simply\n{푒−2휋푖푥휁|휁∈ℝ}\n(3.27)\nNote that this is an uncountable inﬁnite set of vectors, which is what\nwe needed, and we call 휁the frequency. The actual integral transform is\nthen\n퐹(휁) =\n∫∞\n−∞\n푓(푥)푒−2휋푖푥휁푑푥\n(3.28)\nNote that, as we described for the norm and inner product of functions,\nthis is simply generalizing the expression for a linear combination of a\nﬁnite dimensional vector with its basis vectors.\nAs useful as this result is, it is not readily applicable to computation as\nis the case with many concepts dealing with inﬁnity. We can, however,\ndeﬁne the Discrete Fourier Transform (DFT) as the following type-(1, 1)\n20\n3 Harmonic Analysis\nJain and fast Karhunen, “Loeve Trans-\nform for a class of random processes”\nKekre and Solanki, “Comparative perfor-\nmance of various trigonometric unitary\ntransforms for transform image coding”\nAhmed et al., “Discrete cosine transform”\ntensor\n퐹∈ℂ푁⊗ℂ푁∗\n(3.29)\n퐹푚푛=\n1\n√\n푁\n푒−2휋푖푚푛\n푁\n(3.30)\n퐹is a linear map 퐹: ℂ푁→ℂ푁acting on complex vectors of dimension\n푁. Note that 퐹is symmetric, i.e., 퐹푖\n푗= 퐹푗\n푖. For practical applications, this\nmatrix multiply would be prohibitively expensive so we use the fast\nFourier transform to recursively memoize the transform result reducing\nthe number of computations to 푂(푁log(푁)). We do not describe this\nalgorithm in detail here.\nThere are some other transforms which are related to the DFT and are\nuseful. Speciﬁcally a major downside to the DFT is the dependence on\ncomplex numbers. For many discrete applications, real numbers would\nwork ﬁne. This motivates the Discrete Sine Transform (DST) (Jain and\nfast Karhunen 1976; Kekre and Solanki 1978) and the Discrete Cosine\nTransform (DCT) (Ahmed et al. 1974).\nThese transforms can be thought of as taking only the imaginary (sine)\nor real (cosine) part of the DFT. We can get away with this on discrete\nsamples by assuming that the signal, outside of the region we sampled,\nis an odd or even function. We are free to do this since we do not care at\nall about what the function actually looks like outside where we sampled\nso it does not need to be accurate.\nFor our purposes, the DCT will play an outsize role since it is central\nto our later discussion of JPEG. The DST will come up brieﬂy in video\ncoding, however. The DCT can be deﬁned diﬀerently depending on how\nboundary conditions are handled. We will not detail all of these, but the\ntwo important ones for us are the DCT-II, which we will call “the DCT“,\nand is deﬁned in two dimensions as\n퐷푖\n푗=\n1\n√\n2푁\n퐶(푖)퐶(푗)\n푁\nX\n푥=1\n푁\nX\n푦=1\ncos\n\u0014(2푥+ 1)푖휋\n2푁\n\u0015\ncos\n\u0014(2푦+ 1)푗휋\n2푁\n\u0015\n(3.31)\n퐶(푢) =\n(\n1\n√\n2\n푢= 0\n1\n푢≠0\n(3.32)\nand the DCT-III, which we will call “the inverse DCT”, and is deﬁned in\ntwo dimensions as\n(퐷−1)푥\n푦=\n1\n√\n2푁\n푁\nX\n푖=1\n푁\nX\n푗=1\n퐶(푖)퐶(푗) cos\n\u0014(2푥+ 1)푖휋\n2푁\n\u0015\ncos\n\u0014(2푦+ 1)푗휋\n2푁\n\u0015\n(3.33)\nIn both cases, 퐶(푢) is a scale factor which makes the transform or-\nthonormal. As in the DFT, these are both linear maps, this time with\n퐷: ℝ푁→ℝ푁and 퐷−1 : ℝ푁→ℝ푁and are type-(1, 1) tensors.\nWe note here an important theorem which will be useful for us later in\nthe dissertation\n3.2 The Gabor Transform\n21\nTheorem 3.1.1 (The DCT Least Squares Approximation Theorem)\nGiven a set of 푁samples of a signal 푋, let 푌be the DCT coeﬃcients of 푋.\nThen for 1 ≤푚≤푁the approximation of 푋given by\n푝푚(푡) =\n1\n√\n푁\n푦0 +\nr\n2\n푁\n푚\nX\n푘=1\n푦푘cos\n\u0012 푘(2푡+ 1)휋\n2푁\n\u0013\n(3.34)\nminimizes the least-squared error\n푒푚=\n푁\nX\n푖=1\n(푝푚(푖) −푥푖)2\n(3.35)\nProof. First consider that since Equation 3.34 represents the Discrete\nCosine Transform, which is a Linear map, we can write rewrite it as\n퐷푇\n푚푦= 푥\n(3.36)\nwhere 퐷푚is formed from the ﬁrst 푚rows of the DCT matrix, 푦is a\nrow vector of the DCT coeﬃcients, and 푥is a row vector of the original\nsamples.\nTo solve for the least squares solution, we use the the normal equations,\nthat is we solve\n퐷푚퐷푇\n푚푦= 퐷푚푥\n(3.37)\nand since the DCT is an orthonormal transformation, the rows of 퐷푚are\northonormal, so 퐷푚퐷푇\n푚= 퐼. Therefore\n푦= 퐷푚푥\n(3.38)\nSince there is no contradiction, the least squares solution must use the\nﬁrst 푚DCT coeﬃcients.\n■\nA related transform to the “trigonometric” transforms is the Hadamard\ntransform or Walsh-Hadamard transform. The Hadamard transform\ndeﬁnes the transformation matrix recursively as\n퐻0 = 1\n(3.39)\n퐻푚=\n\u0014\n퐻푚−1\n퐻푚−1\n퐻푚−1\n−퐻푚−1\n\u0015\n(3.40)\nThe obvious advantage of this transform is that it contains only −1 and 1\nentries, so it can be computed quite eﬃciently without even multiplication\noperations (only sign changes are needed).\n3.2 The Gabor Transform\nWhile the Fourier transform is useful for telling us what frequencies\nmake up a given signal, it cannot tell us when those frequencies occur.\nIt considers all the samples we have and tells us which frequencies\nexplain all the samples. In some cases, it would be useful to know\n22\n3 Harmonic Analysis\nFigure 3.1: Discrete Wavelet Transform.\nThe DWT repeats the sampling process\nrecursively on the low frequency band.\n2: in contrast to the Fourier result which\nis amplitude vs frequency with no time\ncomponent\nboth which frequencies occur and where they occur. For example, if we\nare examining seismic data, it may be important to know when high\nfrequency vibrations occurred to predict the time of a future earthquake.\nWith a Fourier transform, we would only know that there were high\nfrequency vibrations.\nWe can accomplish this in a naive way with a Short-Time Fourier\nTransform (STFT). The high level idea is extremely simple. The input\nsignal is broken up into smaller blocks of time and the Fourier transform\nis computed on each block separately. Then, for each block of time we\ncan see which frequencies are available, and we can adjust the block size\nto increase the time resolution.\nThe Gabor transform is an interesting twist on this idea. Instead of a\nhard window, we use a soft window by convolving the Fourier transform\nwith a Gaussian kernel. In a continuous representation this is\n퐺(휏, 휁) =\n∫∞\n−∞\n푓(푡)푒−휋(푡−휏)2푒−2휋푖푡휁푑푡\n(3.41)\nyielding amplitude results with time oﬀsets 휏as well as frequencies 휁\n2. While this yields a smooth windowed response in time, it still suﬀers\nfrom what we call the uncertainty principle which all STFTs are subject\nto. That is, the larger the time window, the worse the localization is,\nand the smaller the time window, the more constrained we are in the\nfrequencies we can represent. Put another way, time-resolution and\nfrequency-resolution are inverses: can only have one and not both.\nTo see this result, consider the DFT matrix given in Equation 3.30. This\nmatrix has a ﬁnite number of frequencies that it can represent because of\nthe discrete representation. The high frequency represents each sample\nin a single period. If we restrict the size of the DFT to windows, as in\nthe Gabor transform, we reduce the size of this matrix and therefore we\nreduce the number of frequencies we can represent. Conversely, if we\nallow the size of the window to increase without bound, so as to get the\nbest frequency resolution, we will eventually end up with a window size\nthat is the length of the original signal and therefore is equivalent to\nthe standard Fourier transform that has no temporal component at all.\nAs we will see in the next section, this uncertainty principle extends to\nmore sophisticated methods and is a fundamental limitation of harmonic\nanalysis.\n3.3 Wavelet Transforms\n23\nFigure 3.3: Wavelet Uncertainty. The\nlow frequency wavelet has poor time res-\nolution, in other words, we cannot tell as\nexactly the time where that frequency oc-\ncured as we can with the high frequency\nwavelets. Image credit: wikipedia.\n3.3 Wavelet Transforms\nFigure 3.2: Morlet Wavelet. The Morlet\nwavelet illustrates the high amplitude in\nthe center of the wave with decreasing\namplitude moving to the sides. Image\ncredit: Wikipedia.\nWavelet transforms extend the concept of the STFT to what, at the time\nof writing, can be considered its natural end. Instead of using sine and\ncosine bases, the wavelet transform deﬁnes other functions which have\n\"ﬁnite support\". In other words, they have a high amplitude at time 푡= 0\nwith the amplitude gradually decreasing as 푡moves away from 0 (this\nis shown in Figure 3.2 with the Morlet wavelet). As in the STFT, this\nmeasures a local response to the wavelet. Then, as in the Gabor transform,\nwe can slide the wavelet around by shifting it along the input signal to\ncompute local responses at diﬀerent times.\nThe key improvement of wavelet transforms is that they include a term\nwhich controls the frequency of the wave. This allows for a full bank\nof frequencies to be computed at each time representing the response\nof the signal to wavelets of increasing frequency. Note that because of\nthe uncertainty principal, this generates a tree-like structure. For a given\ntime 푡, there may be multiple high frequency wavelet responses for a\nsingle low frequency wavelet (Figure 3.3). As in the last section, the more\nprecisely we wish to describe the constituent frequencies in a signal\nthe less precisely we can localize them in time. Unlike the last section,\nhowever, we can still localize the high frequencies well even if we cannot\nlocalize the low frequencies, with a STFT, our localization capability is\ndeﬁned entirely by the block size (or Gaussian standard deviation for the\nGabor transform). Since we examine the same signal at multiple scales,\nor resolutions, we call this multiresolution analysis.\nFormally, we deﬁne a mother wavelet 휓(푡) which we can then shift\nand scale as desired. This yields a basis for the space of functions, just as\nwith the fourier transform, given by the following set\n푊=\n\u001a\n휓휁,푠(푡)\n\f\f\f\f 휁∈ℝ, 푠∈ℝ, 휓휁,푠(푡) =\n1\n√\n휁\n휓\n\u0012\n푡−푠\n휁\n\u0013\u001b\n(3.42)\nwhere 휁determines the frequency (or scale) of the wavelet and 푠deter-\nmines the shift. We then compute the integral transform\n푇(휁, 푠) =\n∫∞\n−∞\n푓(푡)휓휁,푠(푡) 푑푡\n(3.43)\nfor a function of time (a signal) 푓(푡). Just as with the Fourier transform\nthis is simply a linear combination of the signal with each of the basis\nentries, but we have generalized from the Fourier basis (푒−2휋푖푡휁) to the\nmore general 휓(푓, 푠). In the rest of this section, we will discuss how to\napply the wavelet transform to discrete signals and how certain important\nwavelets are deﬁned.\n3.3.1 Continuous and Discrete Wavelet Transforms\nAs with the Fourier transform, in order to use these tools on real signals,\nwe must discretize them for execution on a computer. There are several\nways we can do this, the ﬁrst one we will discuss is the Continuous Wavelet\nTransform (CWT) which, despite the name, is not exactly continuous. To\n24\n3 Harmonic Analysis\nSelesnick et al., “The dual-tree complex\nwavelet transform”\ndeﬁne this we simply assume that the signal 푓(푡) is ﬁnite and discretely\nsampled, and we rewrite the integral of Equation 3.43 as a sum\n푇(휁, 푠) =\nX\n푘\n푓푘휓휁,푠(푘)\n(3.44)\nthen we stipulate that the wavelet function have ﬁnite support, in other\nwords, we assume that it is zero outside of a certain range so we can\nrepresent it with a ﬁnite number of samples. We can then deﬁne the\nwavelet transform using convolution. We deﬁne the kernel\ne휓휁,푡= 1\n휁휓\n\u0012\n휁푇−푡\n휁\n\u0013\n(3.45)\nfor a wavelet with support 푇and we compute\n푇푚\n푠푡= 푓푡★e휓휁,푚+휁푇\n(3.46)\nThe Discrete Wavelet Transform (DWT) takes this idea further. The idea\nis that instead of dealing with the wavelet basis change equations directly,\nwe can simply express the transform as a series of high pass/low pass\nﬁlters which coarsely discretize the scale. We ﬁrst construct convolution\nkernels for a high pass and low pass ﬁlter 푔and ℎand compute the\nconvolutions\n푦low = 푓★푔\n(3.47)\n푦high = 푓★ℎ\n(3.48)\nBy deﬁnition, these ﬁlters pass half the frequencies they are given as\ninput. Therefore, by the Nyquist Sampling Theorem, we can also discard\nhalf the samples of each result without losing information. We represent\nthis with a downsampling by two operation (↓)\n푦low = ( 푓★푔) ↓2\n(3.49)\n푦high = ( 푓★ℎ) ↓2\n(3.50)\nThis process is repeated recursively on 푦low while 푦high is retained as an\noutput. This yields a tree structure (Figure 3.1).\nWe brieﬂy mention a newer technique here, the Dual Tree Complex\nWavelet Transform (DTCWT) (Selesnick et al. 2005). This is a complex\nwavelet transform which is inspired by real cosine and imaginary sine\ncomponents of the Fourier transform. The main advantage of this trans-\nform is shift invariance, i.e., a shift in the input signal yields the same\ntransform coeﬃcients. While the theory of the DTCWT is quite involved,\nthe algorithm is simple assuming suitable wavelets exist. As in the DWT,\nhigh and low pass ﬁlters are applied with the results decimated, only\nthis time there are two wavelets producing two trees (Figure 3.4). The\nresults of one tree are treated as the real part of a complex output and\nthe results of the other tree are used as the imaginary part.\nAll of these methods require suitable deﬁnitions of the 휓(푡) function.\nWhile the natural instinct is to choose orthogonal wavelets, biorthogonal\nwavelets, which relax the orthogonal constraint as long as the transform is\nstill invertible, have also been shown to work well and have more ﬂexibility\nin their design. Note that the deﬁnition of a basis does not require\n3.3 Wavelet Transforms\n25\nFigure 3.4: Dual Tree Complex Wavelet\nTransform. The DTCWT is computed in\nthe same way as the DWT but with two\ntrees.\nFigure 3.5: Haar Wavelet. Frequency in-\ncreases vertically, time increase to the\nright.\northogonality. Common choices for 휓(푡) include the Haar wavelets\n(discussed next), the Morlet wavelet which is related to the Gabor\ntransform, and the Daubechies wavelets among others. While most\ntasks will work ﬁne with the simplistic Haar wavelets, knowing the\nproperties of each wavelet to pick the ideal one for a given task can make\na diﬀerence.\n3.3.2 Haar Wavelets\nThe Haar wavelet is one of the most simple and popular choices for 휓(푡).\nIt is deﬁned as\n휓(푡) =\n\n\n1\n0 ≤푡≤1\n2\n−1\n1\n2 ≤푡≤1\n0\notherwise\n(3.51)\nThe Haar wavelet transform is simple to implement and computationally\neﬃcient leading to its widespread use. The wavelets have compact support\nand are orthogonal making the Haar transform eﬀective for conducting\nlocalized frequency analysis, in fact they were the ﬁrst attempt at a basis\nfor multiresolution analysis. The 1D Haar wavelet is plotted in Figure\n3.5 for three frequencies and several shifts per frequency. Note that the\ntime axis (horizontal) spans from 0 to 1. The Haar wavelet has very\ncompact support, outside the support region, which naturally shrinks\nwith increasing frequency, the value of the wavelet is zero, so any samples\noutside the considered region contribute no information to the frequency\nresponse.\nIn the 1D transform the wavelet was measuring diﬀerences along the\ntime axis to measure the frequency response. In 2D, we must consider\ndiﬀerences on two axes including the diagonal (both axes simultaneously).\nFigure 3.6 shows an example of this for a single level DWT. Note that\neach of the four frequency bands, called LL, HL, LH, HH, are stored at\nhalf the width and height leading to the 4 × 4 arrangement on the left\nhand side. In this case, the top-left is the LL band, the top right is the LH\nband, the bottom left is the HL band, and the bottom right is the HH\nband. Note the diﬀerent features that each band responds to: the HL and\n26\n3 Harmonic Analysis\nFigure 3.6: DWT Using Haar Wavelets.\nThe left image is the single level DWT of\nthe right image. Note that each ﬁltered\nimage is stored at half the resolution in\nthe width and height so each of the four\nﬁltered images can be arranged in the\nsame shape as the original image.\nDaubechies, Ten lectures on wavelets\nBruna and Mallat, “Invariant scattering\nconvolution networks”\nP. Liu et al., “Multi-level wavelet-CNN\nfor image restoration”\nX. Zhao et al., “Wavelet-Attention CNN\nfor image classiﬁcation”\n3: among many others including cur-\nrently unpublished work.\nLH bands respond to horizontal and vertical structures respectively and\nthe HH band respond to diagonal structures.\nWhile the Haar transform’s simplicity and eﬀectiveness allow for\nwidespread use there may be more suitable wavelets for a given task.\nThe Daubechies wavelets (Daubechies 1992) in particular have come\ninto common use as they were designed based on the analysis of Ingrid\nDaubachies who made numerous contributions to multiresolution anal-\nysis. For example Daubachies showed that if the number of vanishing\nmoments is 푁, then the support of the wavelet is at least 2푁−1. Vanishing\nmoments, which relate the wavelet to a polynomial, can be of critical\nimportance in choosing a wavelet if there is some understanding of the\nfunction to be analysed. Generally, a wavelet with 푁vanishing moments\nis orthogonal to a polynomial of degree 푁−1.\nIn this section we covered only the most basic ideas of multiresolution\nanalysis as it does not factor into the work of this dissertation. However,\nthe wavelet transform, which was a critical part of the last decade of\nsignal processing, is now making its way rapidly into deep learning\napplications (Bruna and Mallat 2013; P. Liu et al. 2018; X. Zhao et al. 2022)\n3 so knowledge of these techniques will rapidly become important for\nthe computer vision researchers.\nShannon, “A mathematical theory of\ncommunication”\n1: Note that I am choosing these words\ncarefully. We are deciding to measure\ninformation in this way and developing\na ﬁeld around that decision rather than\nmeasuring some natural property of the\nworld like a physicist might.\nEntropy and Information 4\n4.1 Shannon Entropy . . . . . . . 27\n4.2 Huﬀman Coding . . . . . . . 29\n4.3 Arithmetic Coding . . . . . . 30\nI\nnformation theory marked a major advancement in the understanding\nof communication. Claude Shannon’s 1943 paper “A Mathematical\nTheory of Communication” was rare in that it both introduced the ﬁeld\nof information theory and then systematically solved all major problems\nwithin it, essentially an entire ﬁeld in one paper. Importantly for us,\nShannon’s formulations for measuring the information contained in a\nmessage gave rise to lossless compression algorithms which are still used\nto this day. In this chapter, we review the high level ideas of information\ntheory, speciﬁcally entropy, and how these ideas were used to develop\ncompression algorithms.\nThe overall goal of information theory (Shannon 1948) is to measure the\namount of information contained in a signal. The signal can be discrete\n(e.g., words) or continuous (e.g., television, sound, etc.). Shannon was\nresponding to a recent development in communication: modulation.\nThese techniques were rudimentary lossy compression methods which\nintroduced noise into the messages in exchange for reducing their size\n(similar to JPEG and MPEG as we will see later). Exactly how much noise\nwas introduced and the limits of the system with respect to how much\nnoise would make the message unintelligible was a mystery. As expected\nthis was preventing the full and eﬀective use of these technologies, since\noperators would either introduce too much distortion and be left with\nan unintelligible message or introduce too little noise and be faced with\ntransmission delay.\n4.1 Shannon Entropy\nMathematically, we are free to make any choice to deﬁne a measure of\ninformation. In other words, any monotonic function of the number of\npossible messages since all are equally likely. However, Shannon chooses\nto deﬁne information on a log scale since it has some useful properties\n▶Many practical properties vary with the logarithm. For example,\ntwo wires have double the bandwidth of one wire.\n▶It makes the math considerably easier since logarithms have nice\nproperties around addition, multiplication, diﬀerentiation, etc.\ntherefore, we deﬁne1 the “amount of information” 퐼as\n퐼∝log(푀)\n(4.1)\nfor some message 푀. For logarithm base 2, we will call the unit of\ninformation \"bits\". Since this is our measure of information, we can also\nmeasure the information capacity of a channel as\n퐶= lim\n푡→∞\nlog(푁(푡))\n푡\n(4.2)\nwhere 푁(푡) messages can be transmitted in time 푡.\n28\n4 Entropy and Information\nFigure 4.1: The General Communica-\ntion System. One of Shannon’s most im-\nportant contributions was the idea that\nany communication system can be di-\nvided into parts and developed sepa-\nrately. Image credit: Claude Shannon (Shan-\nnon 1948).\nINFORMATION\nSOURCE\nMESSAGE\nTRANSMITTER\nSIGNAL\nRECEIVED\nSIGNAL\nRECEIVER\nMESSAGE\nDESTINATION\nNOISE\nSOURCE\nBefore we continue, however, we touch on one of Shannon’s most\ninﬂuential contributions. That is the general deﬁnition of a communication\nsystem, given in Figure 4.1. Shannon showed that any communication\nsystem consists of the same fundamental parts. Even systems such as\ntelegraphy and color television which seem very diﬀerent from each\nother are fundamentally the same. This model drives much of Shannon’s\nanalysis of information content.\nSince the communication system must be designed to support any\npossible message, we must take a probabilistic approach to describing\nthe generation of messages by the information source. In other words, for\na discrete communication system, the information source will generate\nmessages by producing discrete symbols one at a time. The generation\nof a given symbol is determined based on the past symbols and we can\ntherefore compute a probability for each symbol.\nAs an example of this consider the English language. Given a set of\nletters: \"FIRE BA\" we can say that the letter \"D\" is highly likely to be the\nnext letter. This is a Markov process and while incredibly complicated to\nproduce for real scenarios, Markov modeling would allow us to produce\nprobabilities for each symbol. The important point here is that since we\nare fairly certain about \"D\", a \"D\" being generated has low information\nand therefore requires less space to transmit. Something unexpected\nlike an \"X\" would have high information content. So we can represent\nexpected or frequent results with fewer bits.\nAnother example, assume I wish to communicate the weather in Seattle,\nand I know that there is a 100% chance of rain in Seattle. This information\ncan be transmitted with zero bits, since there is no need to communicate\nanything. Suppose that I instead wish to communicate the weather in\nCollege Park where it rains roughly 50% of the time, then I would require\nthe same amount of bits to transmit raining or sunny.\nSo now we have established an intuitive idea of the information content\nof a message. That is, we are measuring how “expected” or “surprising”\nor “random” a message appears. Given a set of symbols with probabilities\n푝푖for the 푖th symbol, we deﬁne the entropy 퐻as\n퐻= −\n푁\nX\n푖=1\n푝푖log 푝푖\n(4.3)\nThis measure has some important properties\n4.2 Huﬀman Coding\n29\nHuﬀman, “A method for the construction\nof minimum-redundancy codes”\n1\n0\nP=1\nA\nP=0.4\n11\n10\nP=0.6\nB\nP=0.35\n110\n111\nP=0.25\nC\nP=0.2\nD\nP=0.05\nFigure 4.2: Huﬀman Tree Example. The\nfollowing tree structure assigned the\nsmallest length sequence to the most\nprobable symbol and the longest length\nsequence to the least probable.\n▶퐻= 0 if and only if all of the 푝푖are zero except for one, in other\nwords, there is only one symbol and it always occurs (like the\nSeattle example). This means there is no entropy.\n▶퐻is maximized when all 푝푖are the same ( 1\n푁), since this is the most\nuncertain situation (like in the College Park example).\nAt this point we have developed information theory to the barest\nminimum extent in order to deﬁne entropy of a discrete channel. We\nare not taking into account noise or continuous signals, all of which are\ndiscussed at length in Shannon’s paper along with much more thorough\nderivations. We have already touched on the idea that low entropy\nsymbols can be represented with fewer bits. In the next two sections\nwe will develop algorithms for computing these representations. These\nmethods are examples of lossless compression where all information in the\noriginal message is preserved.\n4.2 Huﬀman Coding\nHuﬀman coding (Huﬀman 1952) is a method for producing optimal length\ncodes for symbols based on their probability of occurrence. It was the\nﬁrst method for ﬁnding optimal codes (Shannon presented a method\nwhich was not guaranteed to be optimal) and it is still in heavy use at the\ntime of writing by image and video codecs 70 years after its invention.\nHuﬀman coding requires a set of symbols and their probabilities of\noccurrence as input. Then, given a message as a sequence of symbols, the\nalgorithm produces the minimum length code that uniquely conveys the\nmessage. This requires assigning the shortest codes to the most probable\nsymbols and the longest codes to the least probable symbols.\nWe do this using a binary tree. Start with a leaf node for each symbol that\nstores the probability of that symbol and insert them into a priority queue.\nThen, at each step, remove the two nodes with the lowest probability\nand merge them into an internal node with probability equal to the sum\nof the probabilities of these nodes. Then insert this new node into the\npriority queue and repeat until the queue has only one node on it. This\nnode is the root of the tree. The process is a simple greedy algorithm.\nApproximate code is given in Listing 4.1.\nListing 4.1: Building a Huﬀman Tree.\n1 def build_tree(symbols: List[Tuple(float, str)]) -> Node:\n2\nleaves = [(s[0], Node(s[0], s[1], None, None)) for s in\nsymbols]\n3\np = heapq.heapify(leaves)\n4\n5\nwhile len(p) > 1:\n6\nl = heapq.heappop(p)\n7\nr = heapq.heappop(p)\n8\n9\nn = Node(l.probability + r.probability, None, l, r)\n10\nheapq.heappush((n.probability, n))\n11\n12\nreturn p[0]\nTo encode, for each symbol traverse the tree from the root tracking the\nseries of left and right child’s used in the traversal. Add a 0 to the symbol\n30\n4 Entropy and Information\nFigure 4.3: Arithmetic Coding Example.\nUsing the same alphabet and probabili-\nties as the last section, we encode ABD\ninto the range [0.29, 0.3).\nA\nEncode: ABD\nB\nC\nD\n0\n1\n0.4\n0.75\n0.95\nB\nC\nD\n0.4\nB\nA\nC\nD\n0.16\n0.3 0.38\nB\nC\nD\nA\nC\nD\n0.16\n0.3\nA B\n0\nCD\nfor a left and a 1 for a right. When the correct leaf node is reached, the\nresulting string of 0s and 1s encodes the symbol. To decode, simply read\neach bit at a time and traverse the tree (right or left) based on the bit\nvalue. When a leaf node is encountered, emit that symbol and return to\nthe root of the tree.\nLet’s consider a simple example. Suppose we are given a simple four\nletter alphabet with symbols 푀= {퐴, 퐵, 퐶, 퐷}. These four symbols\nare known to occur with probabilities 푃= {푝퐴= 0.4, 푝퐵= 0.35, 푝퐶=\n0.2, 푝퐷= 0.05}. Since we have four symbols, the default encoding would\nbe 2 bits per symbol, {퐴= 00, 퐵= 01, 퐶= 10, 퐷= 11}. However\ncomputing the entropy of the set 푃gives\n퐻(푃) = −\nX\n푝∈푃\n푝log 푝\n(4.4)\n= −0.4 log(0.4) −0.35 log(0.35) −0.2 log(0.2) −0.05 log(0.05)\n(4.5)\n= 0.529 + 0.530 + 0.464 + 0.216\n(4.6)\n= 1.74\n(4.7)\nso approximately 1.74 bits, meaning that the default encoding of 2 bits\nwastes 0.26 bits per symbol on average.\nWe construct a Huﬀman tree for the above set in Figure 4.2. This gives\nthe following variable length codes {퐴= 0, 퐵= 10, 퐶= 110, 퐷= 111}\nobtained by traversing the tree for each symbol. Note that although there\nare some symbols which now require 3 bits to encode, these are the least\nprobable symbols and the most probable symbol, 퐴, requires only 1 bit.\nIf we compute the average size of a symbol with these codes we actually\nhave 1.85 bits/symbol on average so we are still above the limit in terms\nof entropy. This is because symbols cannot occupy a fraction of a bit.\n4.3 Arithmetic Coding\nAlthough Huﬀman codes were optimal in terms of the number of bits\nto encode single symbols, we saw that Huﬀman coding was not able to\nreach the theoretical minimum number of bits deﬁned by the entropy of\nthe set. By computing an encoding for an entire message rather than one\nsymbol at a time we can overcome this limitation. This is the motivation\nbehind arithmetic coding, which stores an entire message into an arbitrary\nnumber 푞such that 0 ≤푞< 1.\nOnce again the algorithm is given a set of symbols and their probabili-\nties. The encoder starts with the interval [0, 1) and divides the interval\ninto sub-intervals for each symbol. The algorithm picks the interval which\ncorresponds to the current symbol and proceeds to the symbol. When\n4.3 Arithmetic Coding\n31\n2: Speciﬁcally, enough bits such that any\nfraction beginning with the transmitted\nnumber falls into the desired interval.\nall symbols are consumed, the resulting interval uniquely identiﬁes the\nmessage, and since the intervals are unique we only need to transmit a\nsingle element of the ﬁnal interval to identify the message2. To decode\nwe can follow the same process, but this time we are given the number 푞.\nAt each step we construct the same intervals and simply check which one\nthe given number falls into, emitting that symbol at each step. This does\nrequire either a special terminating symbol or a known message length\nto stop. The algorithm is shockingly simple and highly eﬀective.\nAn example encoding is shown in Figure 4.3. In that example, we\nencode the message “ABD” following the same alphabet and probabilities\nwe used for the Huﬀman coding example. We start by dividing [0, 1) into\nproportional parts for each symbol, we ﬁnd that the ﬁrst symbol is 퐴so\nwe choose the interval from [0, 0.4). Next we divide that interval into\nproportional parts and since the next symbol is 퐵, we choose [0.16, 0.3)\nsince 0.16 = 0.4 × 0.4 and 0.3 = 0.16 + (0.4 × 0.35). The ﬁnal symbol is 퐷\nso we choose the interval from [0.29, 0.3) and transmit (arbitrarily) 0.295.\nAgain, decoding follows a similar process. We are given the number\n0.295 as input and we divide up the interval [0, 1), ﬁnding that this falls\ninto [0, 0.4), we emit 퐴. Then we ﬁnd that 0.295 falls into [0.16, 0.3) and\nwe emit 퐵. Finally, we ﬁnd that 0.295 falls into [0.29, 0.3) and we emit 퐷,\nhaving decoded the message “ABD”.\nWhile it may seem remarkable that a message can be transmitted in\na single number, the algorithm does have faults. Again, the message\nmust ﬁt into a discrete number of bits, which can reduce the eﬃciency\ncompared to the theoretical maximum. Furthermore, we are assuming\nthat we have an accurate probability model of the symbol frequencies.\nThis may not be possible to obtain exactly, and in fact, we may not even\nwant global symbol probabilities. Since we are encoding a message, the\nmost eﬃcient encoding of that message would model the probabilities\nof symbols in that message only (e.g., 1\n3 for 퐴, 퐵, 퐷and 0 for 퐶in our\nexample). However, this requires transmitting the probability model\nwhich may remove any gains in eﬃciency from the coding. In general,\nthese are still open problems and while we can obtain “optimal” codes\nwith respect to some speciﬁc deﬁnition of optimal, the theoretical entropy\nlimit that Shannon’s work gives us remains elusive.\nMachine Learning and Deep\nLearning 5\n5.1\nBayesian Decision Theory 33\n5.2\nPerceptrons and Multi-\nlayer Perceptrons . . . . . . 35\n5.3\nImage Features . . . . . . . 37\n5.3.1 Histogram of Oriented\nGradients . . . . . . . . . . . 37\n5.3.2 Scale-Invariant Feature\nTransform . . . . . . . . . . 38\n5.4\nConvolutional Networks\nand Deep Learning . . . . 39\n5.5\nResidual Networks\n. . . . 40\n5.6\nU-Nets\n. . . . . . . . . . . . 41\n5.7\nGenerative Adversarial\nNetworks . . . . . . . . . . . 42\n5.8\nRecap . . . . . . . . . . . . . 43\nM\nachine learning is rapidly revolutionizing the way that people\ninteract with computers. This is largely driven by the explosive\nproliferation of Convolutional Neural Networks (CNNs) (LeCun, Boser,\net al. 1990) since they were shown to be computationally viable for large\nproblems in 2012 (Krizhevsky, Sutskever, et al. 2012). Although machine\nlearning seems commonplace today, this was not the case ten years ago\n(at the time of writing) and there were many who believed that machine\nlearning would never achieve widespread success.\nWhile this dissertation is centered on compression as an application,\nit is ﬁrst and foremost a contribution to machine learning for computer\nvision. In this chapter, we develop a high-level understanding of machine\nlearning concepts which relate to the rest of the dissertation. This discus-\nsion is grounded in Bayesian decision theory which is often overlooked in\nmachine learning discourse. Otherwise, the focus is on computer vision\nmethods rather than general methods.\nNote\nSome of the material in this chapter is based on the book Pattern Clas-\nsiﬁcation (Hart et al. 2000) which I strongly recommend to interested\nreaders for more in-depth information.\n5.1 Bayesian Decision Theory\nBayesian decision theory tells us the best possible decision we can make\nabout data even if we know exactly the underlying generating distribu-\ntions. In a sense this can be thought of as a best case scenario because in\nreal life we do not know the underlying distributions so we must either\napproximate them or approximate decision criteria directly. The classic\nexample of this proceeds as follows. Dockworker Dave is observing ﬁsh\nas they are unloaded from boats. His task is to sort the ﬁsh into bins, one\nfor sea bass which we will denote as 푐푏and one for salmon which we will\ndenote as 푐푠. The ﬁsh come out of the boat randomly. In the absence of\nany other information (such as identifying markers), how can he develop\na strategy to sort them with minimal errors?\nLet’s give Dave some knowledge to help. Since the ﬁsh are coming oﬀ\nthe boat in a random order, we must describe the occurrence of each ﬁsh\nprobabilistically. Assume that Dave knows how many ﬁsh were caught\nof each type, then he knows the prior probability 푃(푐푏) and 푃(푐푠). For\nexample if 푃(푐푏) = 0.7 and 푃(푐푠) = 0.3 then Dave should classify all of the\nﬁsh as bass and he will have 70% accuracy. Of course this will entail him\ndumping all ﬁsh into the bass bin which is a bit odd considering that he\nknows there are two types of ﬁsh. Nevertheless this strategy will attain\nthe lowest error given what Dave knows.\n34\n5 Machine Learning and Deep Learning\nFigure 5.1: Salmon vs Sea bass. Top:\nSalmon, Bottom: Sea bass. The two ﬁsh\nhave diﬀerent colors.\nWe can give Dave some more information to help him. Dave’s daughter\nWendy studies ﬁsh and she informs him that the color can be used to\ndiﬀerentiate bass from salmon although it is not a perfect indicator (see\nFigure 5.1). In this case we would say that there is a continuous random\nvariable 푥which yields conditional probabilities 푃(푥|푐푏) which is the\nprobability of each color value for sea bass and 푃(푥|푐푠) for salmon. We\ncall this the likelihood of the color given the type of ﬁsh and we will call\nthe color of the ﬁsh a feature.\nHow does Dave use this information? In order to make a decision\ngiven color, we want to compute 푃(푐푠|푥) and 푃(푐푏|푥), which we call\nposterior probability, and take the larger probability, but we only have\n푃(푐푏), 푃(푐푠), 푃(푥|푐푏), 푃(푥|푐푠). We also know that there is a joint distribu-\ntion for each class 푃(푐푠,푏, 푥) which is the probability of a ﬁsh being class\n푠or 푏and having color 푥that relates these quantities. From probability\ntheory, we can write this in terms of the conditional\n푃(푐푠,푏, 푥) = 푃(푐푠,푏|푥)푝(푥) = 푃(푥|푐푠,푏)푃(푐푠,푏)\n(5.1)\nthis is the deﬁnition of conditional probability. Rearranging to group the\nquantities that we know gives\n푃(푐푠,푏|푥) = 푃(푥|푐푠,푏)푃(푐푠,푏)\n푃(푥)\n(5.2)\nwhich is known as Bayes rule. This allows us to compute the class\nprobability given some measurement as long as we have the known\nlikelihood and prior probabilities. We have another unknown term, the\nevidence term, in Equation 5.2, 푃(푥), which is the probability of any ﬁsh\nhaving the measured color: in general we do not need this term. The\nBayes decision rule is\n푐=\n(\n푐푠\n푃(푐푠|푥) > 푃(푐푏|푥)\n푐푏\n푃(푐푏|푥) > 푃(푐푠|푥)\n(5.3)\nexpanding one of these inequalities gives\n푃(푥|푐푠)푃(푐푠)\n푃(푥)\n> 푃(푥|푐푏)푃(푐푏)\n푃(푥)\n(5.4)\n= 푃(푥|푐푠)푃(푐푠)\n\b\b\n\b\n푃(푥)\n> 푃(푥|푐푏)푃(푐푏)\n\b\b\n\b\n푃(푥)\n(5.5)\n= 푃(푥|푐푠)푃(푐푠) > 푃(푥|푐푏)푃(푐푏)\n(5.6)\nin terms of only known quantities. This is good because the evidence\nterm is often hard to measure.\nSo now Dave can use his knowledge of the prior probabilities and\nWendy’s color probabilities and multiply them to produce the probability\nof sea bass or salmon, binning the ﬁsh based on whichever is more\nprobable. This seems like a perfectly reasonable idea, but what kinds of\nerrors will Dave make? Let’s compute the probability of Dave’s error\n푃(error|푥) =\n(\n푃(푐푏|푥)\n푐= 푐푠\n푃(푐푠|푥)\n푐= 푐푏\n(5.7)\n5.2 Perceptrons and Multilayer Perceptrons\n35\nRosenblatt, The perceptron, a perceiving and\nrecognizing automaton Project Para\nIn other words, the error rate will be the probability of the other class.\nTo compute the average error rate, we marginalize 푥from the joint\ndistribution\n푃(error) =\n∫∞\n−∞\n푃(error, 푥) 푑푥\n(5.8)\n=\n∫∞\n−∞\n푃(error|푥)푃(푥) 푑푥\n(5.9)\nWe cannot control, or really even measure, 푃(푥) but we can control\n푃(error|푥) by making it as small as possible. And the only way to\naccomplish that is by picking the higher probability for 푃(푐푠,푏|푥) as\nour classiﬁcation choice, thus proving the optimality of the Bayesian\ndecision.\nSo now we have a way of making the best possible classiﬁcation\ndecisions. Given prior probabilities of the diﬀerent classes and likelihoods\nof each feature given each class, we can then compute the posterior\nprobabilities and pick the higher one. This guarantees the minimum\nerror: we cannot achieve lower error than this. However we now have a\nnew problem: how do we produce these probabilities for real problems?\nIn general, we can not, and we will have to approximate the distributions\nleading to an even high error rate. In this sense, the Bayesian decision\ncan be thought of as a theoretical lower limit for the error rate. Even\nif we know everything, because of the probabilistic nature of decision\nproblems, we will not make the right choice for every input.\nThis sets up a theoretical dichotomy. Do we approximate the underlying\nprior and likelihood distributions which generated the data and then\nmake Bayesian decisions based on our observations? Or instead can\nwe simply compute the boundary between the posterior distributions\nas a function of the observation that makes a decision directly? Either\nway, these two questions are the entire purpose of machine learning.\nGiven some data, sampled from unknown distributions, how do we\ncompute approximations which match the true distributions or decision\nboundaries as closely as possible.\n5.2 Perceptrons and Multilayer Perceptrons\nOne simple way of learning decision boundaries is the perceptron (Rosen-\nblatt 1957). The perceptron deﬁnes a simple linear model for making a\nbinary decision between two classes (although it can be extended to more\ncomplex scenarios). Given a vector of weights 풘, and an input feature\nvector 풙, the perceptron makes the following decision\n푓(풙) =\n(\n1\n⟨풘, 풙⟩> 0\n0\notherwise\n(5.10)\nor simply\n푓(풙) = 퐻(⟨풘, 풙⟩)\n(5.11)\n36\n5 Machine Learning and Deep Learning\nFigure 5.2: Multilayer Perceptron. The\nmultilayer perceptron organizes groups\nof perceptrons into layers separated by\nnon-linearities. In this case each circle\nrepresents a perceptron. The ﬁrst and last\nlayers are termed the input and output\nlayers respectively; any layers in between\nare termed hidden layers. Image credit:\nwikipedia.\nLeCun, Boser, et al., “Handwritten digit\nrecognition with a back-propagation net-\nwork”\nwhere 퐻() is the Heaviside function, for classes 1 and 0. The decision\nboundary in this case is a linear function of 풙. The task then is to compute\na suitable 풘given some data.\nStarting from a randomly initialized 풘0 and some set of training data\n풙푖with labels 푦푖the learning algorithm ﬁrst computes the decision on\n풙푖.\nb푦푖= 푓(풙풊) = ⟨풘0, 풙⟩\n(5.12)\nwhich may be incorrect. The algorithm then updates the weights as\n풘1 = 풘0 + (푦푖−b푦푖)풙푖\n(5.13)\nThis process is repeated for all pairs (풙풊, 푦푖) until some predeﬁned\nstopping criterion is met. In the case that all 풙푖are linearly separable with\nrespect to 푦푖, this stopping criterion may be convergence, but this is\nalmost never the case in real life.\nTo model real scenarios, a more complex model is needed: one that can\nmodel non-linear relationships. We can extend the perceptron to model\nthese more complex scenarios by building a Multilayer Perceptron (MLP)\n(MLP). The MLP stacks layers of perceptrons separated by non-linearity\n(Figure 5.2). More formally, for layer weights 푊푙(a matrix), input 풙, and\nnonlinearity 휎(), a MLP can be deﬁned as\n푓(풙) = 푊푁휎(. . . 휎(푊1휎(푊0풙)))\n(5.14)\nfor an MLP with 푁layers. We call the ﬁrst layer (weights 푊0) the input\nlayer, the last layer (weights 푊푁) the output layer, and the intermediate\nlayers (weights 푊1, . . . , 푊푁−1) the hidden layers. In practice we will also\ndeﬁne a loss function 푙() which takes the network output and the true\nclassiﬁcation and tell use how wrong it was. Importantly this function\nneeds to be scalar valued\n푒(푊) = 푙(푦, 푓((푥);푊))\n(5.15)\ndescribing the error for some set of weights 푊.\nTraining this model requires some tricks. We use an algorithm called\nbackpropagation (LeCun, Boser, et al. 1990). If we observe the form of 푙(),\nwe can see that it is a scalar valued function of a vector. This means that\nwe can compute the gradient of the output with respect to the input\n∇푊푙(푦, 푓((푥);푊)) =\n\n휕\n휕푤0\n00 푙(푦, 푓((푥);푊))\n휕\n휕푤0\n10 푙(푦, 푓((푥);푊))\n...\n휕\n휕푤푙\n푖푗푙(푦, 푓((푥);푊))\n...\n휕\n휕푤퐿\n푀푁푙(푦, 푓((푥);푊))\n\n(5.16)\nfor 퐿layers and weights of size 푀푁, which tells us “in what direction\nand by how much” we would need to change the network in order to\nclassify 풙correctly. We can compute these quantities using the chain\n5.3 Image Features\n37\nDalal and Triggs, “Histograms of ori-\nented gradients for human detection”\nrule. For each layer we compute the Jacobian 휕푊퐿\n푊퐿−1 (since these are vector\nvalued functions) with respect to the previous layer and continue until\nwe have diﬀerentiated every layer.\n∇푊0푙= ∇푊푁푙⊙\n휕푊푁\n휕푊푁−1 ⊙휕푊푁−1\n휕푊푁−2 · · · 휕푊1\n휕푊0\n(5.17)\nwhich gives updates for the weights in each layer.\n5.3 Image Features\nIn order to apply any of these models to images, we need some way\nof representing images as the input vectors 푥to the functions in the\nprevious section. While we could simply ﬂatten the images into vectors,\nthis may cause issues with the learning process. Small perturbations\nof the input pixels can cause large changes in their actual values. Also\npixels themselves can vary considerably in appearance yet still represent\nthe same class. These issues impact the separability of the problem, and\ncreate extremely complex decision boundaries that are diﬃcult if not\nimpossible to model without arbitrarily deep networks.\nA more successful strategy would be to compute some higher order\nrepresentation of the images which we can show is more meaningful.\nAlthough we may explore ideas like extracting numerical shape descrip-\ntions or color conversions, there are some abstract representations which\nhave been shown to be eﬀective. We will explore two of these in this\nsection: Histogram of Oriented Gradients (HOG) and the Scale-Invariant\nFeature Transform (SIFT). Both of these techniques transform an image\ninto a series of vectors, which we call features, that can then be input to a\nmachine learning model.\n5.3.1 Histogram of Oriented Gradients\nThe Histogram of Oriented Gradients (Dalal and Triggs 2005) captures\nshape and orientation of objects using a local descriptor. Often the image\nwill be contrast normalized in blocks before the histogram of gradients is\ncomputed on each pixel in small cells. The descriptor for each cell is the\nconcatenation of the histograms for all of the pixels in the cell.\nTo compute the gradient of an image, it can simply be convolved with\na gradient kernel. There are many such kernels but\nℎ=\n\u0002\n−1\n0\n1\u0003\n(5.18)\n푣=\n\n−1\n0\n1\n\n(5.19)\nare the popular choices for computing horizontal and vertical gradients\nrespectively. After the gradient is computed it can be binned per cell\n(usually 8 × 8 pixel cells) to compute the histogram. These histogram\ncells are then normalized with respect to larger blocks (usually 16 × 16\nblocks) to further increase invariance to image transformations. This\ngives a descriptor for each cell which can be input to any classiﬁer. For\n38\n5 Machine Learning and Deep Learning\n1: We do not cover SVMs here\nFigure 5.3: HoG Features. The left shows\nan example image and the right shows\nHoG features which classify as “per-\nson”. The HoG features are shown as\nthe weighted orientation based on the\nhistogram of the cell and classiﬁcation\nconﬁdence. Image credit: Dalal and Triggs\n2005.\nLowe, “Object recognition from local\nscale-invariant features”\nScale\n(first\noctave)\nScale\n(next\noctave)\nGaussian\nDifference of\nGaussian (DOG)\n. . .\nFigure 5.4: Diﬀerence of Gaussians.\nThe diﬀerence of Gaussian’s scale space\ncomputes gaussian blurs of increasing\nstrength to the input image. The blurred\nimages are then subtracted from each\nother. Points which survive this process\nare scale invariant. Image credit: Lowe 1999.\nexample, Dalal and Triggs used the HoG feature for pedestrian detection\nan SVM classiﬁer1.\nThe result at the time was quite impressive and HoG features came into\nwidespread use. HoG features are “dense” in the sense that every block in\nthe image is covered in some sense which means that the model is given\na strong prior on the local shapes present in the image. This can be seen\nvisually in Figure 5.3. In the ﬁgure we see a man in the example image.\nThe HoG features visualized on the right show outlines of the important\nshapes in each region. This visualization is produced by drawing tangent\nlines for each orientation in the histogram and weighting the lines by the\nhistogram values on each cell. The lines are then further weighted by\nthe SVM conﬁdence to show which lines are contributing to the human\nclassiﬁcation. We see strong responses on the feet, shoulders, and head\nmeaning that the model considers these unique identiﬁers of people that\nare not present in other objects.\n5.3.2 Scale-Invariant Feature Transform\nOne of the most popular and powerful image features is the Scale-\nInvariant Feature Transform (Lowe 1999). Like HoG features, SIFT features\ncapture a local description of shape using orientation. Unlike HoG, the\nprimary purpose of SIFT was to ﬁnd scale-invariant keypoints which are\nunique locations that appear the same under scale changes. These points\ncan be used for object matching. Since the points should be rotation and\nscale invariant, a query object should be able to be located even if it is\nsubject to complex deformations.\nTo compute the scale space, SIFT uses a diﬀerence of Gaussian’s (DoG).\nThe image is computed at diﬀerent scales by Gaussian blurring the\nimage successively, then the diﬀerence between neighboring blurred\nimages is taken (Figure 5.4). When this is done over many scales, any\npoints which survive for the entire stack of DoG images are considered\nscale-invariant since they are clearly localized across scales. These points\nare pixel localized by applying non-maximum suppression and then\nsub-pixel localized by computing a second order Taylor expansion on\nthe pixel which can produce a zero point in between pixel boundaries.\nFor each keypoint, the rotation invariant descriptor is computed. The\ngradient magnitude 푚(푥, 푦) and orientation 표(푥, 푦) are computed as\n푚(푥, 푦) =\nq\n(푃푥+1,푦−푃푥−1,푦)2 + (푃푥,푦+1 −푃푥,푦−1)2\n(5.20)\n표(푥, 푦) = arctan\n푃푥,푦+1 −푃푥,푦−1\n푃푥+1,푦−푃푥−1,푦\n(5.21)\nfor image 푃. This is computed in a 3 × 3 neighborhood around the\nkeypoint and then a histogram is computed. The orientation with the\nhighest bin is assigned to the keypoint. To further improve the invariance,\nthese descriptors are compiled in a 4× 4 grid into an 8 bin histogram. The\nresulting 128 dimensional descriptor resulting from the concatenation of\nthese histograms is assigned as the keypoint descriptor. This descriptor\ncan be normalized to improve invariance to lighting changes.\n5.4 Convolutional Networks and Deep Learning\n39\nFigure 5.5: Convolutional Neural Net-\nwork. Diagram shows the LeNet-5 ar-\nchitecture. The model takes pixels as in-\nput and computes feature maps using\nsuccessive convolutions, non-linearities,\nand subsampling layers. For classiﬁca-\ntion, the network terminates in a MLP.\nThis allows the classiﬁer and the feature\nextractors to be trained jointly using back-\npropagation. Image credit: LeCun, Bottou,\net al. 1998\nLeCun, Boser, et al., “Handwritten digit\nrecognition with a back-propagation net-\nwork”\nKrizhevsky, Sutskever, et al., “Imagenet\nclassiﬁcation with deep convolutional\nneural networks”\nSIFT features were the de facto standard in image features for many\nyears. During the end of the classical/feature based learning era, there\nwas a particular shift towards dense SIFT features. This step simply\nforgoes the keypoint detection steps and computes a descriptor for each\npixel. This is useful for tasks like semantic segmentation that require\nper-pixel labels but it can also be used as a rotation invariant base for\nmore general tasks.\n5.4 Convolutional Networks and Deep\nLearning\nFeature engineering is a complex process. The two algorithms we de-\nscribed in the last section are non-trivial to understand, much less to\ndevelop on one’s own. Furthermore, it is not clear if a given feature\nis suitable to any particular task, we only have vague motivation and\nintuition to guide us. The fundamental contribution of deep learning\nwas that the best features for a given problem can be learned along with\nthe classiﬁer using only pixels as input. This replaced the tedious feature\nengineering process with something much more powerful and much\nsimpler to develop.\nDeep learning is powered by the CNN (LeCun, Boser, et al. 1990).\nThese ideas had been around for some time but it was not until Alexnet\n(Krizhevsky, Sutskever, et al. 2012) that deep enough and complex\nenough networks were shown to be computationally viable with a\nGPU implementation. This quickly revolutionized machine learning\nwith entire scientiﬁc careers dedicated to feature engineering becoming\nobsolete in a short time frame.\nThe CNN itself is not particularly complex. It is a MLP with the matrix\nmultiplications replaced with convolutions, formally\n푓(풙) = 푊푁★휎(. . . 휎(푊1 ★휎(푊0 ★풙)))\n(5.22)\nThe advantage of this is that the weights can be small kernels, usually\n3 × 3 instead of the large matrices required to process an image with a\nMLP (these matrices would need to be the same width and height as\nthe image). This already made CNNs much more eﬃcient than MLPs\neven without the GPU implementation. Furthermore, many seemingly\ncomplex image transformations can be computed with convolutions\nwhich is why we say that the convolutional network computes learned\nfeature representations. These non-linear feature extractors replace the\nhand designed feature extractors of classical machine learning.\n40\n5 Machine Learning and Deep Learning\nLeCun, Bottou, et al., “Gradient-based\nlearning applied to document recogni-\ntion”\nHe, Xiangyu Zhang, et al., “Deep residual\nlearning for image recognition”\nSimonyan and Zisserman, “Very deep\nconvolutional networks for large-scale\nimage recognition”\nOne of the more inﬂuential and yet simple architectures is shown in\nFigure 5.5, LeNet-5 (LeCun, Bottou, et al. 1998). Many CNN variants can be\ndescribed by the components in that ﬁgure. The convolutional layers are\npaired with non-linearity and subsampling layers. The subsampling layers\nare usually some kind of pooling (max pooling or average pooling) which\nhelps aggregate feature information spatially. The actual classiﬁcation\ndecision is made using a MLP once the feature maps have been reduced\nto a suﬃciently small and abstract representation. For non-linearity\ncurrently ReLU is the most popular choice which I like to deﬁne in terms\nof the Heaviside function\n푅(푥) = 퐻(푥)푥\n(5.23)\nbut which most people like to write as\n푅(푥) =\n(\n푥\n푥> 0\n0\n푥≤0\n(5.24)\nWhy CNNs work as well as they do remains somewhat of a mystery,\nbut like much of machine learning, we can get an idea using intuition.\nAs we have already discussed, the hand designed features of classical\nmachine learning may not have been the best for a given task. The learned\nfeatures of a convolutional network are likely more suited since they are\ncustomized to the task. Images are a discrete sampling of a 2D signal,\nand nearby pixels are often highly correlated or anti-correlated (in terms\nof edges). CNNs can pick up on these correlations because they use\na translation-invariant learned convolution which is moved across the\nimage spatially in a sliding window. Finally, since convolutional networks\nare highly eﬃcient, they can be made deeper and wider to learn more\ncomplex mappings.\nIn this dissertation we will be exclusively exploring convolutional\nneural network architectures. While there have been some major ad-\nvancements to CNNs which we touch on in the rest of this chapter and\nthroughout the dissertation, it is worth noting that the CNNs of today\nare largely the same as those used by the pioneers of deep learning.\n5.5 Residual Networks\nResidual networks (He, Xiangyu Zhang, et al. 2016) were a major ad-\nvancement in the design of convolutional networks. Instead of learning a\nmapping 푦= 푓(푥) like a traditional network, the residual network deﬁnes\na mapping 푦= 푓(푥)+푥. This, along with some other notable architectural\nchanges, makes the residual network highly eﬀective. The precise reason\nwhy this helps so much is still debated however it likely makes “gradient\nﬂow” easier (gradient ﬂow was also explored by the VGG (Simonyan and\nZisserman 2014) and Inception Szegedy, W. Liu, et al. 2015 architectures).\nExamining Equation 5.17, we can spot a potential problem. As the depth\nof the network increases, carrying the gradient from the loss through\nall Jacobians to the earliest layer may be diﬃcult. The gradient tends to\nshrink as we move backwards through the layers, we call this problem\nvanishing gradient. Residual learning is likely solving this problem by\n5.6 U-Nets\n41\nFigure 5.6: U-Net. U-Nets arrange convo-\nlutional layers in a U-shape of decreasing\nand then increasing size. Skip connec-\ntions allow for better gradient ﬂow to\nearly layers. Image credit: Ronneberger et\nal. 2015.\nFigure 5.7: Residual Block. Each resid-\nual block consists of two convolutions\nwith a ReLU non-linearity and a batch\nnormalization layer. Image credit: He, Xi-\nangyu Zhang, et al. 2016.\nIoﬀe and Szegedy, “Batch normalization:\nAccelerating deep network training by\nreducing internal covariate shift”\nRonneberger et al., “U-net: Convolutional\nnetworks for biomedical image segmen-\ntation”\nallowing a shortcut connection around some of the convolution layers\nwhich carries a stronger gradient signal to the early layers.\nThe actual design of the network is based on a so called “residual\nblock” pictured in Figure 5.7. Each block has two weight layers with\nbatch normalization (Ioﬀe and Szegedy 2015) separated by a ReLU non-\nlinearity. The addition of batch normalization is thought to simplify the\nlearning process even further for the weight layers by removing lower\norder statsitics of mean and variance. For each batch, the layer tracks the\nrunning mean 휇and variance 휎2 and computes\nBN(푥) = 훾푥−휇\n휎\n+ 훽\n(5.25)\nfor learned 훾and 훽. The block includes the hallmark residual connection\nshort-circuiting the two weight layers.\nThe entire network architecture stacks the residual blocks using strided\nconvolutions to perform learned downsampling instead of using pooling.\nThe network terminates with a “global average pooling” layer which\nperforms spatial averaging over each channel of the output to produce a\nsmall vector suitable for input to a MLP like prior network designs.\n5.6 U-Nets\nIt is worth noting that we are, of course, not limited to only classiﬁcation\nproblems. The U-Net (Ronneberger et al. 2015) architecture is suitable for\nproblems which require a spatial output like image-to-image translation\nand semantic segmentation. In this dissertation, we will almost exclusively\nbe dealing with image-to-image problems although the architectures\nwe discuss later will diﬀer greatly from the U-Nets. Similar to residual\nnetworks, U-Nets were a major advancement in these spatial tasks. And\nalso like residual networks, the major contribution was likely in gradient\nﬂow.\nU-Nets deﬁne the network in two distinct parts: the encoder and the\ndecoder, the schematic is shown in Figure 5.6. The encoder is much like a\n42\n5 Machine Learning and Deep Learning\nIsola et al., “Image-to-image translation\nwith conditional adversarial networks”\nGoodfellow et al., “Generative adversar-\nial nets”\n2: Although this is an abuse of the term,\ntechnically an autoencoder should gen-\nerate the exact image it is given as input\nand nothing else.\nLeCun, “The MNIST database of hand-\nwritten digits”\nGenerator\nDiscriminator\nFake Image\nReal Image\nNoise\nReal\nFake\nFigure 5.8: GAN Procedure. The gener-\nator creates an image from random noise\nand provides it to the discriminator along\nwith real images. The discriminator must\nidentify which images are real and which\nare fake.\nHeusel et al., “Gans trained by a two time-\nscale update rule converge to a local nash\nequilibrium”\nNash, “Equilibrium points in n-person\ngames”\nNash, “Non-cooperative games”\ntraditional convolutional network. There are alternating convolutions and\nnon-linearities with downsampling. The decoder is the reverse process,\ntaking the compact representation from the encoder and using upsam-\npling operations to compute a result which has the same dimensions as\nthe input image. The major design feature of this is the skip connections.\nThese connections take feature maps from the encoder and concatenate\nthem with the feature representations of the same size in the decoder\nwhich allows a strong gradient signal to ﬂow to the early layers avoiding\nthe vanishing gradient problem.\nThe U-Net was revolutionary at the time for its results on the extremely\ndiﬃcult semantic segmentation problem. However, the U-Net would\nquickly become widely used for any spatial task, and is still used quite\nfrequently. Pix2Pix (Isola et al. 2017) for example was based entirely on\nthe U-Net. While U-Nets were highly inﬂuential on all image-to-image\nproblems, we will employ very diﬀerent architectures later in the disser-\ntation, and indeed very few works in compression actually use U-Nets.\nThis is because there are other ways to deal with vanishing gradients (like\nresidual blocks and their derivatives) and the downsampling operations\nin U-Nets tend to remove ﬁne details which we want to preserve in\nrestoration tasks.\n5.7 Generative Adversarial Networks\nGenerative Adversarial Networks (GANs) (Goodfellow et al. 2014) will be\nrelied upon heavily in the methods we detail in the dissertation. GANs\nwere a truly revolutionary moment in the generation of images using\nCNNs. Prior error based methods, called autoencoders2, produced very\npoor results even for simple datasets like MNIST (LeCun 1998).\nThe many variants of the GANs would change this dramatically\nusing an ingenious and fairly simple idea. The GAN methods sets up\nan adversarial game with two networks. One network, the generator,\ngenerates images, and another, the discriminator, tries to identify which\nimages are real and which are fake. The generator is rewarded for fooling\nthe discriminator into classifying its images as real and penalized for\ngetting caught. Conversely, the discriminator is rewarded for correctly\nidentifying fake images and penalized for incorrectly classifying them.\nTraining (theoretically) ends when the two networks achieve a Nash\nequilibrium (Heusel et al. 2017; Nash 1950, 1951). This procedure is shown\nin Figure 5.8.\nWe train this pair of networks using standard cross entropy classiﬁca-\ntion loss. The only diﬀerence is that we reverse the labels when training\nthe generator since we want it to fool the discriminator. This is sometimes\ncall the minimax loss. Given real samples 푥, noise vectors 푧, discriminator\n퐷(), and generator 퐺(), we deﬁne the loss\n푙(푥, 푧) = log(퐷(푥)) + log(1 −퐷(퐺(푧)))\n(5.26)\nand we train the discriminator to maximize 푙() while training the generator\n5.8 Recap\n43\nto minimize 푙(). In other words\nmin\n퐺max\n퐷E푥∈real[log(퐷(푥))] + E푧∈noise[log(1 −log(퐷(퐺(푍))))]\n(5.27)\nAs these two networks play their game over the course of training,\nthe discriminator will start to identify more and more fake images. The\nincreasing loss on the generator will cause it to generate more realistic\nimages. Since identifying fake images is relatively easy for a CNN, by\nthe end of training, the generator will be producing extremely realistic\nimages in order to continue to fool the discriminator. In practice the Nash\nequilibrium is hard to achieve and we simply stop training GANs after a\ncertain number of steps. GANs also chronically diverge since it is hard\nfor the GAN to recover from a situation where the discriminator has a\nlarge advantage over the generator.\n5.8 Recap\nTo recap, we have reviewed machine learning from the ground up. We\nbuilt the ideas of machine learning on a foundation of how to make\ndecisions in the presence of perfect information. We then developed\nthe perceptron and its extension, the multilayer perceptron which is the\nprogenitor of modern deep learning. We discussed hand engineered\nfeatures and why they were necessary and ﬁnally developed deep learn-\ning as a replacement for these features. We then reviewed some of the\nmost important ideas of deep learning including convolutional networks,\nresidual learning, U-Nets, and GANs. This concludes the foundational\nknowledge which is required to fully understand the original research\ndeveloped in the remainder of this dissertation.\nImage Compression\nFigure 6.1: JPEG Information Loss. This\nimage suﬀers from extreme degradations\ncaused by JPEG compression. Zoom in\non this image, it probably has fewer de-\ntails than you think it does.\nIndependant JPEG Group, libjpeg\nJPEG Compression 6\n6.1\nThe JPEG Algorithm\n. . . 47\n6.1.1 Compression . . . . . . . . . 48\n6.1.2 Decompression . . . . . . . 49\n6.2\nThe Multilinear JPEG\nRepresentation . . . . . . . 50\n6.3\nOther Image Compression\nAlgorithms . . . . . . . . . . 52\nJ\nPEG has been a driving force for internet media since its standardization\nin 1992 (G. K. Wallace 1992). The principal idea in JPEG compression\nis to identify which details of an image are the least likely to be noticed if\nthey are missing. These details can then be replaced with lower entropy\nversions. By removing information, there is a signiﬁcant size reduction\nover methods which perform entropy coding alone. This is called lossy\ncompression, since information is lost in the encoding process.\nThe lost information is, in general, not recoverable. Usually this is\nnot a major issue, as the JPEG algorithm was designed to remove un-\nnoticed details. However, there are situations where the information\nloss is noticeable in the form of unpleasant artifacts (Figure 6.1). This\nis particularly true when a JPEG image is saved multiple times, which\ncauses repeated application of the lossy process. A signiﬁcant portion of\nthe dissertation is devoted to using machine learning to approximate the\nlost information.\nA common source of consumer confusion with JPEG is in the name\nitself. JPEG refers to three things simultaneously\nThe JPEG Algorithm The algorithm for compressing images.\nJPEG Files The disk ﬁle format for storing JPEG compressed data and its\nassociated metadata. This is actually either a JPEG File Interchange\nFormat (JFIF) ﬁle or an Exchangeable Image File Format (EXIF) ﬁle.\nThe Joint Photographic Experts Group The working group that main-\ntains the JPEG standard.\nThis chapter is devoted to giving the reader an understanding of JPEG\ncompression which is suﬃcient to motivate the ﬁrst-principles that we\nuse in developing the algorithms later in the dissertation. We will review\nthe function of JPEG compression and decompression step-by-step and\nwe will discuss the extremely important view of JPEG as a linear map.\nWe will also brieﬂy discuss other image compression algorithms.\n6.1 The JPEG Algorithm\nWe now present the JPEG algorithm step-by-step. Where the standard is\nambiguous we defer to the Independent JPEG Group’s libjpeg software\n(Independant JPEG Group n.d.). This software is widely considered\nstandard in the industry, although there are other implementations of\nJPEG. We start by describing the compression process and then conclude\nwith the decompression process, which is largely the inverse. Throughout\nthe description we will place emphasis on which parts of the standard\nare motivated by human perception and which steps involve loss of\ninformation.\n48\n6 JPEG Compression\nInternational Telecommunication Union,\nStudio encoding parameters of digital televi-\nsion for standard 4:3 and wide-screen 16:9\naspect ratios\n1: This is an archaic and confusing nota-\ntion but unfortunately it is still used.\n6.1.1 Compression\nJPEG compression starts with an RGB image usually in interlaced (RGB24)\nformat. This image is then converted to the YCbCr planar format, however,\nthis is not the more common ITU-R BT.601 (International Telecommuni-\ncation Union 2011) format, which produces values in [16, 235] for 푌and\n[16, 240] for 퐶푏, 퐶푟. Instead, this format uses the full range of byte values\n([0, 255]). The color conversion uses the following three equations\n푌= 2.99푅+ 0.587퐵+ 0.114퐺\n(6.1)\n퐶푏= 128 −0.168736푅−0.331264퐵+ 0.5퐺\n(6.2)\n퐶푟= 128 + 0.5푅−0.418688퐵−0.081312퐺\n(6.3)\nThis color conversion is designed to better represent human perception\nof the image which treats changes in luminance (the Y channel) with more\nweight than chrominance (the 퐶푏and 퐶푟channels). Therefore, the 퐶푏and\n퐶푟channels can have more information removed with less of an eﬀect on\nthe overall image.\nOne operation in particular which removes additional information\nfrom the color channels is chroma subsampling. Chroma subsampling\ndescribes a 4×2 block of pixels and is represented as a triple, e.g., 4 : 2 : 0.\nThe 4 represents the number of luma samples per row. The 2 represents\nthe number of chroma samples in the 1st row. The 0 represents the\nnumber of chroma samples which change in the second row. So in this\nexample, there are 4 luma samples in each row, 2 chroma samples in the\nﬁrst row, and none of them change in the second row, meaning that the\nchroma channels should be stored at half the width and height of the\nluma channel. Another example is 4 : 2 : 2 which indicates that the 2\nchroma samples in the ﬁrst row both change in the second row, so the\nchroma channels are stored with half the width but the same height as\nthe luma channel1.\nBefore we remove information we need to pad the image. JPEG is\nbased on 8 × 8 blocks so at the least the image needs to be padded to a\nmultiple of 8 in the width and height. If chroma subsampling is used,\nthis needs to be taken into account during padding and the image may\nneed to be padded to a multiple of 16 or more in the width, height, or\nboth. This deﬁnes the Minimum Coded Unit (MCU), i.e., the minimum\nsize block which can be encoded using the given settings. The padding in\nthis case is always done on the bottom and right edges of the image and\nrepeats the ﬁnal sample as the padding value. With the image padded,\nthe chroma channels can be subsampled.\nNext comes the main feature of the JPEG algorithm, the DCT on non-\noverlapping 8 × 8 blocks. Before computing this, the pixels are centered\nby subtracting 128. The DCT is applied using\n퐷푖푗= 1\n4퐶(푖)퐶(푗)\n7\nX\n푥=0\n7\nX\n푦=0\n푃푥푦cos\n\u0014(2푥+ 1)푖휋\n16\n\u0015\ncos\n\u0014(2푦+ 1)푗휋\n16\n\u0015\n(6.4)\n퐶(푢) =\n(\n1\n√\n2\n푢= 0\n1\n푢≠0\n(6.5)\n6.1 The JPEG Algorithm\n49\n0\n1\n5\n6\n 14\n 15\n 27\n 28\n2\n4\n7\n 13\n 16\n 26\n 29\n 42\n3\n8\n 12\n 17\n 25\n 30\n 41\n 43\n9\n 11\n 18\n 24\n 31\n 40\n 44\n 53\n10\n 19\n 23\n 32\n 39\n 45\n 52\n 54\n20\n 22\n 33\n 38\n 46\n 51\n 55\n 60\n21\n 34\n 37\n 47\n 50\n 56\n 59\n 61\n35\n 36\n 48\n 49\n 57\n 58\n 62\n 63\nFigure 6.2: Zig-Zag Order. This ordering\nis intended to put low frequencies in the\nbeginning and high frequencies at the\nend.\nHuﬀman, “A method for the construction\nof minimum-redundancy codes”\nRissanen and Langdon, “Arithmetic cod-\ning”\nfor 8×8 block of pixels 푃. This accomplishes two goals. First, it concentrates\nthe energy of each block into the top left corner. Second, it serves as a\nfrequency transform which allows us to remove frequencies which we\nbelieve viewers will be less likely to notice.\nThe DCT coeﬃcients are then quantized by dividing by a quantization\nmatrix. This is an 8× 8 matrix of coeﬃcients which reduce the magnitude\nof the DCT coeﬃcients. Since humans tend not to notice missing high\nspatial frequencies, the quantization matrices generally target these.\nHowever, most encoders compute the quantization matrix from a scalar\nquality factor which is easier for users to comprehend, and as this quality\ndecreases, the quantization matrix removes lower and lower frequencies.\nAfter quantization, the result is truncated to an integer. This removes\ninformation in the fractional part and permits the result to be stored in an\ninteger which takes up less space. In a sense this is the ﬁrst “compression”\noperation. The entire operation is given by\n푌′\n푖푗=\n\u0016\n푌푖푗\n(푄푦)푖푗\n\u0019\n(6.6)\n(퐶′\n푏)푖푗=\n\u0016 (퐶푏)푖푗\n(푄푐)푖푗\n\u0019\n(6.7)\n(퐶′\n푟)푖푗=\n\u0016 (퐶푟)푖푗\n(푄푐)푖푗\n\u0019\n(6.8)\nfor luminance quantization matrix 푄푦and chrominance quantization\nmatrix 푄푐. The color channels are often quantized more coarsely as human\nvision is less sensitive to color data. Note that since we truncate, any\nfractional part after division is irrevocably lost, the resulting coeﬃcient\ncan only be approximated from the integer part. Any coeﬃcient which is\nless than zero after division is set to zero and cannot be recovered even\napproximately. Other than chroma subsampling, this is the only source\nof loss in JPEG compression. In order to decode the image, 푄푦and 푄푐\nare both stored in the JPEG ﬁle.\nThese quantized coeﬃcients are then vectorized in a zig-zag order\n(Figure 6.2) which is designed to put low frequencies in the beginning of\nthe 64 dimensional vectors and high frequencies at the end. This is because\nthe next step is to run-length code this vector. Since the quantization\nprocess was more likely to zero out high frequency coeﬃcients, this\nconcentrates the zeros at the end of the vector and leads to more eﬀective\nrun-length coding. This is the second “compression” operation.\nThe ﬁnal run-length coded vectors are then entropy coded. This\ncan use either Huﬀman coding (Huﬀman 1952) or arithmetic coding\n(Rissanen and Langdon 1979). With a signiﬁcant amount of redundant\nor unnoticeable information removed, these entropy coding operations\nare extremely eﬃcient and yield a signiﬁcant space reduction over the\nuncompressed image.\n6.1.2 Decompression\nThe decompression algorithm is largely the reverse operation. After\nundoing entropy coding we have the quantized coeﬃcients. These are\n50\n6 JPEG Compression\nSmith, “Fast software processing of mo-\ntion JPEG video”\nChang, “Video Compositing in the DCT\ndomain”\nNatarajan and Vasudev, “A fast approxi-\nmate algorithm for scaling down digital\nimages in the DCT domain”\nShen and Sethi, “Inner-block operations\non compressed images”\nSmith and Rowe, “Algorithms for manip-\nulating compressed images”\nelement-wise multiplied by the quantization matrices to compute the\napproximated coeﬃcients\nb푌푖푗= 푌′\n푖,푗(푄푦)푖푗\n(6.9)\n(c\n퐶푏)푖푗= (퐶′\n푏)푖,푗(푄푐)푖푗\n(6.10)\n(c\n퐶푟)푖푗= (퐶푟)′\n푖푗(푄푐)푖푗\n(6.11)\nWe can then compute the inverse DCT of the approximated coeﬃ-\ncients\n푃푥푦= 1\n4\n7\nX\n푖=0\n7\nX\n푗=0\n퐶(푖)퐶(푗)b퐷푖푗cos\n\u0014(2푥+ 1)푖휋\n16\n\u0015\ncos\n\u0014(2푦+ 1)푗휋\n16\n\u0015\n(6.12)\nand uncenter the spatial domain result by adding 128. The color channels\nare interpolated to remove chroma subsampling. We then remove any\npadding that was added, and convert the image back to the RGB24 color\nspace\n푅= 푌+ 1.402(퐶푟−128)\n(6.13)\n퐺= 푌−0.344136(퐶푏−128) −0.714136(퐶푟−128)\n(6.14)\n퐵= 푌+ 1.772(퐶푏−128)\n(6.15)\nand the image is ready for display.\nThere are three important things to take away from this discussion.\nFirst, other than chroma subsampling, which is optional, the only lossy\noperation is the truncation during the quantization step. This is a fairly\nsimple operation considering the DCT coeﬃcients but it creates complex\npatterns in the spatial domain. Next, the blocks are non-overlapping,\nso for each block there is no dependence on pixels outside of the block.\nFinally, each pixel in the block depends on all of the coeﬃcients in the\nblock. Conversely, each coeﬃcient in the block also depends on all of the\npixels in the block. We will exploit this property later in the dissertation.\n6.2 The Multilinear JPEG Representation\nIn what is perhaps a surprising result, the steps of the JPEG transform are\neasily linearizable (Smith 1994), a property that was explored signiﬁcantly\nin the 1990s (Chang 1992; Natarajan and Vasudev 1995; Shen and Sethi\n1995; Smith and Rowe 1993). Indeed, outside of entropy coding, the only\nnon-linear step in compression is the truncation that occurs during quan-\ntization, and all the steps of decompression are linear. Furthermore, when\nwe process JPEG images, we are either dealing with the decompression\nprocess, or we are in full control over the compression process and it\nis therefore our choice if and when we truncate. We would only need\nto do this if we were saving the result as a JPEG. We now develop the\nsteps of the JPEG algorithm into linear maps and compose them into\na single linear map that models compression and a single linear map\nwhich models decompression.\nWithout loss of generality, consider a single channel (grayscale) image.\nWe model this image as the type-(0, 2) tensor 퐼∈퐻∗⊗푊∗. Note\n6.2 The Multilinear JPEG Representation\n51\n2: Note that we’re doing things slightly\nout of order but ultimately the order\ndoes not matter here and doing it this\nway simpliﬁes the form of the next tensor\nthat although we are essentially dealing with real numbers, we have\nintentionally left 퐻∗and 푊∗as arbitrary co-vector spaces because there is\nno reason to deﬁne them concretely for our purposes. We will, however,\nmake the stipulation that they are deﬁned with respect to a standard\northonormal basis so that we can freely convert between the co-vector and\nvector spaces without the use of a metric tensor. Note that the following\nequations are written in Einstein notation; see Chapter 2 (Multilinear\nAlgebra) if this is unfamiliar.\nOur ﬁrst task is to break this image into 8 × 8 blocks. We deﬁne the\nlinear map\n퐵: 퐻∗⊗푊∗→푋∗⊗푌∗⊗푀∗⊗푁∗\n(6.16)\n퐵∈퐻⊗푊⊗푋∗⊗푌∗⊗푀∗⊗푁∗\n(6.17)\n퐵ℎ푤\n푥푦푚푛=\n(\n1\npixel ℎ, 푤belongs in block 푥, 푦at oﬀset 푚, 푛\n0\notherwise\n(6.18)\nwhere 퐵is a type-(2, 4) tensor deﬁning a linear map on type-(0, 2) tensors.\nThe result of this map will be a tensor with 8 × 8 blocks indexed by 푥, 푦\nand 2D oﬀsets for each block indexed by 푚, 푛. Although this deﬁnition is\nfairly abstract, it can be computed fairly easily using modular arithmetic,\nalthough it does need to be recomputed for each image.\nNext we compute the DCT of each block,. We deﬁne the following\nlinear map\n퐷: 푀∗⊗푁∗→퐴∗⊗퐵∗\n(6.19)\n퐷∈푀⊗푁⊗퐴∗⊗퐵∗\n(6.20)\n퐷푚푛\n훼훽= 1\n4퐶(훼)퐶(훽) cos\n\u0012(2푚+ 1)훼휋\n16\n\u0013\ncos\n\u0012(2푛+ 1)훽휋\n16\n\u0013\n(6.21)\n퐶(푢) =\n(\n1\n√\n2\n푢= 0\n1\n푢≠0\n(6.22)\nThe equation for 퐷should look familiar by now. 퐷is a type-(2, 2) tensor\ndeﬁning a linear map on type-(0, 2) tensors. The 푚, 푛block oﬀset indices\nin the input tensor will index spatial frequency after applying this map.\nNext we linearize the coeﬃcients 2. We deﬁne the following linear\nmap\n푍: 퐴∗⊗퐵∗→Γ∗\n(6.23)\n푍∈퐴⊗퐵⊗Γ∗\n(6.24)\n푍훼훽\n훾=\n(\n1\n훼, 훽is at 훾under zigzag ordering\n0\notherwise\n(6.25)\nThis is a type-(2, 1) tensor deﬁning a linear map on type-(0, 2) tensors. It\nﬂattens the 8 × 8 blocks into 64 dimensional vectors. In other words, the\n훼, 훽indices indicate which indexed spatial frequency will be indexed\nwith a single 푘after applying this transformation. This tensor depends\non the zigzag ordering and can simply be hard coded.\nFinally, we divide by the quantization matrix. We still need to scale the\ncoeﬃcients even though we are not rounding them. We deﬁne the linear\n52\n6 JPEG Compression\nBoutell, PNG (Portable Network Graphics)\nSpeciﬁcation Version 1.0\nmap\n푆: Γ∗→퐾∗\n(6.26)\n푆∈Γ ⊗퐾∗\n(6.27)\n푆훾\n푘= 1\n푞푘\n(6.28)\nwhere 푞푘is the 푘th entry in the quantization matrix for the JPEG image.\nThis is a type-(1, 1) tensor deﬁning a linear map on co-vectors.\nIn order to deﬁne both compression and decompression, we need only\none more linear map, scaling by the quantization matrix\ne푆: 퐾∗→Γ∗\n(6.29)\ne푆∈퐾⊗Γ∗\n(6.30)\ne푆푘\n훾= 푞푘\n(6.31)\nfor the same deﬁnition of 푞푘as above.\nWe now have the fairly simple task of assembling these steps into\nsingle tensors. We say this is simple because all of the operations are\nlinear maps and therefore are readily composable. We deﬁne\n퐽: 퐻∗⊗푊∗→푋∗⊗푌∗⊗퐾∗\n(6.32)\n퐽∈퐻⊗푊⊗푋∗⊗푌∗⊗퐾∗\n(6.33)\n퐽ℎ푤\n푥푦푘= 퐵ℎ푤\n푥푦푚푛퐷푚푛\n훼훽푍훼훽\n훾푆훾\n푘\n(6.34)\nfor compression and\ne퐽: 푋∗⊗푌∗⊗퐾∗→퐻∗⊗푊∗\n(6.35)\ne퐽∈푋⊗푌⊗퐾⊗퐻∗⊗푊∗\n(6.36)\ne퐽푥푦푘\nℎ푤= 퐵푥푦푚푛\nℎ푤\n퐷훼훽\n푚푛푍훾\n훼훽e푆푘\n훾\n(6.37)\nfor decompression.\nIt is diﬃcult to express how powerful this result is and how easily it\nis achieved using rudimentary concepts from multilinear algebra. What\nseems like a fairly complex algorithm, and indeed is, when thought of as\nan operation on a matrix, reduces to a simple linear map when we model\nthe inputs and intermediate steps as tensors. Equipped with this linear\nmap, we can and will model complex phenomena on compressed JPEG\ndata directly without needing to decompress it.\n6.3 Other Image Compression Algorithms\nThe astute reader will have noticed early on that we, quite conﬁdently,\nare in a part labeled “image compression” and yet we are only discussing\nJPEG. There are other image compression algorithms, so a natural ques-\ntion is “why are we not discussing those?”\nFor myriad reasons, there are really no interesting problems to study\nfor other compression algorithms. PNG (Boutell 1997), for example, is\nwidely used. But this is lossless compression, so there are no artifacts to\n6.3 Other Image Compression Algorithms\n53\nCompuServe Inc, Graphics Interchange For-\nmat\nBellard, Better Portable Graphics\nMPEG, Requirements for still image coding\nusing HEVC\nSkodras et al., “The jpeg 2000 still image\ncompression standard”\nSwartz, Understanding digital cinema: a\nprofessional handbook\novercome. GIF (CompuServe Inc 1987) is also lossless, although many\nGIF services quantize colors into a palette to save more space, which is a\npotential problem that could be interesting to work on. More modern\nformats are based on video compression and while they are lossy, they\nare simply unused. BPG (Bellard 2018) is the most promising of these, and\ntherefore the least used, but there is also HEIC/HEIF (MPEG 2013) which\nis currently being unsuccessfully pushed by Apple on the iPhone.\nProbably the most interesting of these algorithms is JPEG 2000 (Skodras\net al. 2001), which is lossy and widely used in digital cinema (Swartz\n2004), although it was completely ignored by consumers. This codec is\ninteresting because it would require us to update our theory to take into\naccount the discrete wavelet transform that JPEG 2000 uses in place of\nthe DCT. However, the use of this transform imposes a major practical\nproblem as well: JPEG 2000 images look good even at low bitrates because\nthe wavelet transform is so eﬀective, so they may not require correction.\nInstead, we will focus our energy where it can have the most impact:\nby exclusively studying JPEG. Even at the time of writing, 30 years after\nstandardization, JPEG is the most commonly used image ﬁle format. It is\neasy to use, familiar to consumers, and has become the backbone of the\ninternet, making it incredibly resilient to any challenger, no matter how\nmuch better the compression or quality of the images. At the same time,\nJPEG does suﬀer from some extreme artifacts in many conditions. It is\nthis combination of visible quality loss and widespread use that makes\nJPEG ideal for further study.\n1: We frame this discussion in terms of\nclassiﬁcation but it applies equally well\nto any problem type\n2: i.e., a Δ function such that for all 푖,\n|퐻(푖) −Δ(퐻)(DCT(푖))| = 0\nIoﬀe and Szegedy, “Batch normalization:\nAccelerating deep network training by\nreducing internal covariate shift”\nJPEG Domain Residual Learning 7\n7.1\nNew Architectures . . . . . 56\n7.1.1 Frequency-Component\nRearrangement . . . . . . . 56\n7.1.2 Strided Convolutions . . . 57\n7.2\nExact Operations . . . . . . 57\n7.2.1 JPEG Domain Convolu-\ntions . . . . . . . . . . . . . . 57\n7.2.2 Batch Normalization\n. . . 60\n7.2.3 Global Average Pooling . . 63\n7.3\nReLU\n. . . . . . . . . . . . . 63\n7.4\nRecap . . . . . . . . . . . . . 66\n7.5\nEmpirical Analysis . . . . . 66\n7.6\nLimitations and Future\nDirections . . . . . . . . . . 68\nN\now we develop a general method for performing residual net-\nwork (He, Xiangyu Zhang, et al. 2016, Section 5.5 (Residual Net-\nworks)) learning and inference on JPEG data directly, i.e., without the\nneed for decompressing the images. This method was published sepa-\nrately in the proceedings of the International Conference on Computer\nVision (Ehrlich and Davis 2019).\nWarning\nThis chapter is extremely math-heavy and dry. It is strongly recom-\nmended to review the background math outlined in the ﬁrst chapters\nof the dissertation for a complete understanding of the material. This\nchapter may serve as a powerful sleep aid: do not operate heavy\nmachinery while reading this chapter.\nCompared to processing data in the pixel domain, directly working\non JPEG data has several advantages. First, we observe that most images\non the internet are compressed, including deep learning datasets such\nas ImageNet (Jia Deng et al. 2009). Next we observe that JPEG images,\nbeing compressed, are naturally smaller and more sparse than their un-\ncompressed counterparts. These are all desirable properties for memory-\nand compute- hungry deep learning algorithms.\nThe primary goal of the method presented in this section is to be\nas close as possible to the pixel domain result. In other words, given a\nlearned pixel domain mapping 퐻: 퐼→ℝ푐mapping images to class\nprobabilities for 푐classes1, we want to deﬁne a mapping Δ(퐻) such that\n|퐻(퐼푛) −Δ(퐻)(DCT(퐼푛))| is minimized for any 퐼푛∈퐼. We can accomplish\nthis goal analytically and we will develop the theory in the coming\nsections, including a discussion of why it is (likely) not possible to\ngenerate a mathematically exact Δ(퐻) function2 and what guarantees\nare available on the deviation.\nRecall that a residual network requires several components to operate\nConvolution The primary learned linear mapping between feature maps\nat each layer. Each “residual block” contains two of these operations.\nBatch Normalization Produces normalized features for the convolu-\ntions; this is thought to ease the learning process by removing\nunnecessary statistics from the input features which can be repre-\nsented exactly (Ioﬀe and Szegedy 2015).\nGlobal Average Pooling An innovation of the ResNet. When the convo-\nlution layers are exhausted, the features are averaged channel-wise\nto produce a vector suitable for input to a fully connected layer\nReLU The non-linearity of the “residual block”, this allows the network\nto learn complex mapping.\nOur task will be to derive transform domain versions of each of these\noperations.\n56\n7 JPEG Domain Residual Learning\nGueguen et al., “Faster neural networks\nstraight from jpeg”\nLo and Hang, “Exploring semantic seg-\nmentation on the dct representation”\nFirst Principles\n▶JPEG is easily linearized, convolutions are linear, composing\nthem expressed a learned convolution exactly in the JPEG\ndomain.\n▶Other components of the residual network can be expressed\nanalytically in the JPEG domain.\n▶ReLU can be approximated with a bilinear map.\n7.1 New Architectures\nBefore discussing the proposed technique, we will ﬁrst make a detour\nto review two popular methods of JPEG and DCT deep learning. These\nmethods are all new architectures which enable eﬀective processing in the\ntransform domain but which do not attempt to replicate any pixel domain\nresult. These methods have some advantages over both the method\npresented in the rest of this chapter and when compared to pixel-domain\nnetworks. For example, both methods show good task accuracy with\nfaster processing. There are some notable disadvantages, however. In\nparticular, these methods are not suitable for situations when a pixel\ndomain network already exists and its results need to be replicated on\nJPEGs.\nThese ideas were inspired by the “do nothing” approach published\nin both NeurIPS and an ICLR workshop (Gueguen et al. 2018). In this\napproach, the transform coeﬃcients are passed into a mostly unmodiﬁed\nResNet for classiﬁcation. The authors postulate that with the higher-\nlevel representation of the DCT, fewer layers are required to achieve\nsimilar accuracy and therefore the network will be faster. Indeed the\nauthors show this is true empirically. However, despite the intuition, this\npaper’s evaluation leaves much to be desired, and it is unclear what the\ncontribution of the DCT is to the result. Meanwhile, it is well known in\nJPEG artifact correction literature (discussed later in the dissertation),\nwhere “dual-domain” methods are commonplace, that providing DCT\ncoeﬃcients to a network is not successful without considerable eﬀort, a\nresult seeming at odds with Gueguen et al..\nInstead, the following methods are inspired by unique attributes of\nDCT coeﬃcients. Namely: that each coeﬃcient is a function of all pixels\nin a block, that a block of pixels is only correctly represented by all DCT\ncoeﬃcients, and that the DCT coeﬃcients are orthogonal and arranged in\na grid simply for convenience. This last point is critical. One of the reasons\nthat small convolutional kernels work well on pixels is that nearby pixels\nare usually correlated in some way, so translation invariant features are\nreadily learned. If that convolution is instead applied to coeﬃcients, this\nis then a mapping on arbitrary orthogonal measurements which are\nintentionally decorrelated, leaving little hope for success.\n7.1.1 Frequency-Component Rearrangement\nThis is treated by Lo et al. (Lo and Hang 2019) by simply rearranging\nthe frequencies into the channel dimension before processing (Figure\n7.2 Exact Operations\n57\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13\n14\n15\n16 17 18 19 20 21\n22\n23\n24 25 26 27 28 19\n30\n31\n32 33 34 35 36 37\n38\n39\n40 41 42 43 44 45\n46\n47\n48 49 50 51 52 53\n54\n55\n56 57 58 59 60 61\n62\n63\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13\n14\n15\n16 17 18 19 20 21\n22\n23\n24 25 26 27 28 19\n30\n31\n32 33 34 35 36 37\n38\n39\n40 41 42 43 44 45\n46\n47\n48 49 50 51 52 53\n54\n55\n56 57 58 59 60 61\n62\n63\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13\n14\n15\n16 17 18 19 20 21\n22\n23\n24 25 26 27 28 19\n30\n31\n32 33 34 35 36 37\n38\n39\n40 41 42 43 44 45\n46\n47\n48 49 50 51 52 53\n54\n55\n56 57 58 59 60 61\n62\n63\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13\n14\n15\n16 17 18 19 20 21\n22\n23\n24 25 26 27 28 19\n30\n31\n32 33 34 35 36 37\n38\n39\n40 41 42 43 44 45\n46\n47\n48 49 50 51 52 53\n54\n55\n56 57 58 59 60 61\n62\n63\n0\n0\n0\n0\n2\n2\n2\n2\n63\n63\n63\n63\n1\n1\n1\n1\n...\nInput: 1 channel 16  16\n×\nOutput: 64 channel 2  2\n×\nFigure 7.1: Frequency component rear-\nrangement.\nDeguerre et al., “Fast object detection in\ncompressed jpeg images”\n7.1), yielding a feature map which is 1\n8th the width and height and with\n64 channels (per input channel). Note how this allows a convolution to\ncapture the information contained in the DCT. Since the convolution\noperation used in deep learning maps all channels in the input to each\nchannel in the output, every coeﬃcient plays a role in the resulting map\nand therefore complete block information is captured. They call this\nmethod Frequency-Component Rearrangement (FCR)\nLo et al. use their network for semantic segmentation of road features\nquite successfully. At the time of publication, their method was both fast\nand accurate.\n7.1.2 Strided Convolutions\nA similar solution proposed by Deguere et al. (Deguerre et al. 2019) uses\nstrided convolutions instead of FCR. Speciﬁcally, this method uses an\n8 × 8 stride-8 convolution such that each DCT block is processed in\nisolation. Note again how this makes good use of the coeﬃcients: the 8×8\nconvolution ensures that every coeﬃcient plays a role in the resulting\nmapping, and the stride-8 ensures that there is no leakage of information\nacross blocks. Once these “block representations” are computed, the\nresulting feature map is again 1\n8th the width and height (now with a\nvariable number of features) . Deguere et al. use this method for object\ndetection in the DCT domain and again performed admirably at the time\nof publication.\n7.2 Exact Operations\nIn the previous section we discussed novel architectures that equip CNNs\nwith the ability to process data in the transform domain. While this is\nuseful and important, it requires training a new CNN from scratch and\nhas no particular relationship to the underlying pixels that the CNN is\nprocessing. Since CNNs were designed to process pixel domain data,\nand the DCT is a transform of pixel data, a natural question is whether a\nmethod can be formulated that is capable of processing transform domain\ndata and which has some mathematical guarantee or relationship to the\nunderlying pixel domain model. We now develop just such a method.\n7.2.1 JPEG Domain Convolutions\nRecall from Section 6.2 (The Multilinear JPEG Representation) that the\nJPEG transform can be linearized and written as linear maps on tensor\ninputs and that this analysis yields the following linear maps:\n퐽: 퐻∗⊗푊∗→푋∗⊗푌∗⊗퐾∗\n(7.1)\nfor compression of an image represented by 퐼∈퐻∗⊗푊∗to transform\ncoeﬃcients 퐹∈푋∗⊗푌∗⊗퐾∗, and\ne퐽: 푋∗⊗푌∗⊗퐾∗→퐻∗⊗푊∗\n(7.2)\n(7.3)\n58\n7 JPEG Domain Residual Learning\n3: Much in the same way that linear func-\ntions 푓(푥) = 5푥and 푔(푥) = 2푥can be\ncombined into (푓◦푔)(푥) = 10푥which\nhas only a single multiply vs the two mul-\ntiplies of separately applying 푔and then\n푓\n4: Although other sizes and shapes are\npossible\nto decompress. We proceed by considering only single channel images.\nWe will add in channels and batch dimensions later since they have no\nbearing on the derivation.\nWe know that convolutions are linear maps, therefore, deriving a JPEG\ndomain convolution is fairly simple. Assume that 퐶: 퐻∗⊗푊∗→퐻∗⊗푊∗\nis a linear map representing an arbitrary convolution. This convolution\nwould be applied to an image 퐼in the pixel domain by computing\n퐼′\nℎ′푤′ = 퐶ℎ푤\nℎ′푤′퐼ℎ푤\n(7.4)\nGiven transform coeﬃcients 퐹∈푋∗⊗푌∗⊗퐾∗for 퐼, we can derive 퐼as\n퐼ℎ푤= e퐽푥푦푘\nℎ푤퐹푥푦푘\n(7.5)\nSimilarly, we can derive transform coeﬃcients 퐹′ for 퐼′ by applying 퐽\n퐹′\n푥′푦′푘′ = 퐽ℎ′푤′\n푥′푦′푘′퐼′\nℎ′푤′\n(7.6)\nSubstituting these two expressions yields\n퐼′\nℎ′푤′ = 퐶ℎ푤\nℎ′푤′e퐽푥푦푘\nℎ푤퐹푥푦푘\n(7.7)\n퐹′\n푥′푦′푘′ = 퐽ℎ′푤′\n푥′푦′푘′퐶ℎ푤\nℎ′푤′e퐽푥푦푘\nℎ푤퐹푥푦푘\n(7.8)\nAnd we make the following deﬁnition\n퐹′\n푥′푦′푘′ =\nh\n퐽ℎ′푤′\n푥′푦′푘′퐶ℎ푤\nℎ′푤′e퐽푥푦푘\nℎ푤\ni\n퐹푥푦푘\n(7.9)\nΞ푥푦푘\n푥′푦′푘′ = 퐽ℎ′푤′\n푥′푦′푘′퐶ℎ푤\nℎ′푤′e퐽푥푦푘\nℎ푤\n(7.10)\ngiving a simple expression for computing Ξ : 푋∗⊗푌∗⊗퐾∗→푋∗⊗푌∗⊗퐾∗,\na convolution in the compressed domain, given a convolution in the pixel\ndomain. It is important to note that this is not a simple notational trick.\nBecause 퐽, 퐶, and e퐽are linear maps, the resulting Ξ performs all three\noperations in a single step and is signiﬁcantly faster than performing\nthem separately3.\nWith the mathematics satisﬁed, we now turn to the development of an\neﬃcient algorithm for computing Ξ. After all, the convolution 퐶is usually\nrepresented as a simple 3×3 matrix of numbers4. However our derivation\nis expressed in terms of an dim(퐻) × dim(푊) × dim(퐻) × dim(푊) (2,\n2)-tensor.\nOne way to understand 퐶is as a look-up table of coeﬃcients. For\nexample, if we index 퐶as C[5, 7], we are given a tensor of coeﬃcients\nfor every pixel in the input representing its contribution to the (5, 7) pixel\nin the output. Naturally, many of these coeﬃcients are 0. In fact, the\nonly non-zero pixels are those from (2, 4) to (8, 10). Similarly, if we index\n퐶as C[:, :, 5, 7] we can see the contribution of pixel (5, 7) in the\ninput to every output pixel (which again is mostly zero). This implies a\nnaive algorithm: Exploding Convolutions (Listing 7.1) where the entire\n(2,2)-tensor is iterated and the correct coeﬃcients are copied from the\nconvolution kernel. The resulting map is then composed with 퐽ande퐽to\nproduce the transform domain map.\n7.2 Exact Operations\n59\nPaszke et al., “PyTorch: An Imperative\nStyle, High-Performance Deep Learning\nLibrary”\n5: Note that the deﬁnition of 퐶has\nchanged slightly and is now kernel\nListing 7.1: Exploding Convolutions\n(Naive)\n1 def explode_convolution(shape: Tuple[int, int], conv: Tensor, J:\nTensor, J_tilde: Tensor) -> Tensor:\n2\nsize = (conv.shape[0] // 2, conv.shape[1] // 2)\n3\nshape = (shape[0] + size[0] * 2, shape[1] + size[1] * 2)\n4\n5\nc = torch.zeros((shape[0], shape[1], shape[0], shape[1]))\n6\nfor i in range(shape[0]):\n7\nfor j in range(shape[1]):\n8\nfor u in range(shape[0]):\n9\nfor v in range(shape[1]):\n10\nhrange = (u - size[0], u + size[0])\n11\nvrange = (v - size[1], v + size[1])\n12\n13\nif hrange[0] <= i <= hrange[1] and vrange[0]\n<= j <= vrange[1]:\n14\nx = u - i + size[0]\n15\ny = v - j + size[1]\n16\nc[i, j, u, v] = conv[x, y]\n17\n18\nxi = torch.einsum(\"h’w’x’y’k’,hwh’w’,xykhw->xykx’y’k’\", [J,c,\nJ_tilde])\n19 return xi\nAlthough this algorithm is simple, it comes with some notable dis-\nadvantages. First, it is slow. Iterating over the entire (2,2)-tensor is time\nconsuming even for a small image. Second, it is diﬃcult to parallelize\nwithout domain knowledge of low-level programming. In other words,\na CUDA kernel (or similar construct) would need to be produced to\neﬃciently implement this algorithm. A better algorithm would be readily\nand eﬃciently programmed in a high level deep learning library like\nPyTorch (Paszke et al. 2019).\nExamine the tensore퐽and note that\ne퐽∈푋⊗푌⊗퐾⊗퐻∗⊗푊∗\n(7.11)\nRecall that our model of single channel images uses 퐼∈퐻∗⊗푊∗, therefore,\nthe last two dimensions ofe퐽are a single channel image and we can model\ne퐽as a batch of single channel images by reshaping it to fold 푋, 푌, 퐾into\na single dimension 푁, giving\nb퐽∈푁⊗퐻∗⊗푊∗\n(7.12)\nWe are then free to convolveb퐽with the kernel5 퐶:\nb퐶= 퐶★b퐽\n(7.13)\nand then reshape b퐶giving\ne퐶∈푋⊗푌⊗퐾⊗퐻∗⊗푊∗\n(7.14)\nNote that the shape of e퐶and e퐽are the same, all we have done here is\ncompose the convolution kernel 퐶into the decompression operatione퐽.\n60\n7 JPEG Domain Residual Learning\nChetlur et al., “cudnn: Eﬃcient primi-\ntives for deep learning”\nDaniel, Gray, et al., “opt_einsum-a\npython package for optimizing contrac-\ntion order for einsum-like expressions”\nIoﬀe and Szegedy, “Batch normalization:\nAccelerating deep network training by\nreducing internal covariate shift”\nNext, we compose e퐶and 퐽\nΞ푥푦푘\n푥′푦′푘′ = e퐶푥푦푘\nℎ푤퐽ℎ푤\n푥′푦′푘′\n(7.15)\nto compute Ξ.\nListing 7.2: Exploding Convolutions\n(Fast)\n1 def explode_convolution(J_tilde: Tensor, J: Tensor, C: Tensor) ->\nTensor:\n2\nJ_hat = J_tilde.flatten(0, 2)\n3\nc_hat = torch.nn.functional.conv2d(J_hat, C)\n4\nc_tilde = c_tilde.view_as(J_tilde)\n5\nxi = torch.einsum(\"xykhw,x’y’k’hw->xykx’y’k’\",[C_tilde, J])\n6\nreturn xi\nThis algorithm (Listing 7.2) is simple to code in machine learning\nlibraries. Here, it takes up only six lines of code and involves no loops.\nFurthermore, since this algorithm depends only on reshaping, convo-\nlution, and einsum, it can take advantage of the built-in optimizations\nthat these libraries include resulting from years of research into these\nalgorithms (Chetlur et al. 2014; Daniel, Gray, et al. 2018). It is also worth\nnoting that autograd algorithms used by these libraries will work as\nexpected for this algorithm, i.e., it is straightforward to optimize 퐶with\nrespect to some objective when Ξ is used to transform the input feature\nmaps.\nExtending this to batches of multi-channel images is straightforward.\nFirst, we deﬁne the convolution 퐶as 퐶: 푃∗⊗퐻∗⊗푊∗→푃′∗⊗퐻∗⊗푊∗\nadding the input and output plane dimensions 푃, 푃′ and noting that 퐶\nlacks any batch dimension since the same operation is applied to each\nimage in the batch. Next, we simply deﬁne Ξ as\nΞ푝푥푦푘\n푝′푥′푦′푘′ = 퐽ℎ′푤′\n푥′푦′푘′퐶푝ℎ푤\n푝′ℎ′푤′e퐽푥푦푘\nℎ푤\n(7.16)\nwhere the 퐽,e퐽tensors have not changed. This simply adds the plane\ndimensions 푃, 푃′ to to Ξ. This map is applied to transform coeﬃcients\n퐹∈푁∗⊗푃∗⊗푋∗⊗푌∗⊗퐾∗as\n퐹′푛푝′푥′푦′푘′ = Ξ푝푥푦푘\n푝′푥′푦′푘′퐹푛푝푥푦푘\n(7.17)\nwhere the batch dimension 푁is preserved. With the exception of some\nextra indices, this does not change the algorithm in Listing 7.2.\n7.2.2 Batch Normalization\nBatch normalization (Ioﬀe and Szegedy 2015) is a commonly used tech-\nnique which ensures each layer receives normalized feature maps. For\na single channel feature map 퐼∈퐻∗⊗푊∗batch normalization uses\nthe sample mean E[퐼] and variance Var[퐼] along with learnable aﬃne\nparameters 훾, 훽. These parameters are then applied as\nBN(퐼) = 훾퐼−E[퐼]\np\nVar[퐼]\n+ 훽\n(7.18)\nThe batch statistics are used to update running statistics which are applied\nat inference time instead of the sample statistics. This equation has a\n7.2 Exact Operations\n61\n6: If there are multiple coeﬃcient blocks\n(as is common) their means will need to\nbe combined.\nsimple closed-form expression in the transform domain.\nWe start with the mean and variance. Recall from Section 3.1 (The\nFourier Transform) the deﬁnition of the 2D Discrete Cosine Transform\nover 푁× 푁blocks\n퐷(푖, 푗) =\n1\n√\n2푁\n퐶(푖)퐶(푗)\n푁−1\nX\n푥=0\n푁−1\nX\n푦=0\n퐼(푥, 푦) cos\n\u0012(2푥+ 1)푖휋\n2푁\n\u0013\ncos\n\u0012(2푦+ 1)푗휋\n2푁\n\u0013\n(7.19)\n퐶(푘) =\n(\n1\n√\n2\n푘= 1\n1\n푘≠0\n(7.20)\nLet us compute an expression for the (0, 0) coeﬃcient\n퐷(0, 0) =\n1\n√\n2푁\n퐶(0)퐶(0)\n푁−1\nX\n푥=0\n푁−1\nX\n푦=0\n퐼(푥, 푦) cos\n\u0012(2푥+ 1)0휋\n2푁\n\u0013\ncos\n\u0012(2푦+ 1)0휋\n2푁\n\u0013\n(7.21)\n=\n1\n2\n√\n2푁\n푁−1\nX\n푥=0\n푁−1\nX\n푦=0\n퐼(푥, 푦) cos(0) cos(0)\n(7.22)\n=\n1\n2\n√\n2푁\n푁−1\nX\n푥=0\n푁−1\nX\n푦=0\n퐼(푥, 푦)\n(7.23)\nWe further assume 8 × 8 blocks as used by JPEG\n=\n1\n2\n√\n2 · 8\n7\nX\n푥=0\n7\nX\n푦=0\n퐼(푥, 푦)\n(7.24)\n= 1\n8\n7\nX\n푥=0\n7\nX\n푦=0\n퐼(푥, 푦)\n(7.25)\n(7.26)\nSince\nE[퐼] = 1\n64\n7\nX\n푥=0\n7\nX\n푦=0\n퐼(푥, 푦)\n(7.27)\nwe have\nE[퐼] = 1\n8퐷(0, 0)\n(7.28)\nyielding a simple expression for the sample mean of a block given DCT\ncoeﬃcients. Note that this is extremely eﬃcient compared to computing\nthe mean on the feature maps directly: it requires one read operation\nand one multiply operation per block vs 64 reads, 63 sums, and one\nmultiply 6.\nTo compute the variance we use the following theorem\n62\n7 JPEG Domain Residual Learning\nTheorem 7.2.1 (The DCT Mean-Variance Theorem) Given a set of samples\nof a signal 푋such that 퐸[푋] = 0, let 푌be the DCT coeﬃcients of 푋. Then\nVar[푋] = E[푌2]\n(7.29)\nProof. Start by considering Var[푋], we write this as\nVar[푋] = E[푋2] −E[푋]2\n(7.30)\nWe are given E[푋] = 0, so we simplify this to\nVar[푋] = E[푋2]\n(7.31)\nNext, we use the DCT linear map 퐷: 푀∗⊗푁∗→퐴∗⊗퐵∗where the\nvector spaces 푀and 푁indicate the block dimensions and 퐴, 퐵indicate\nspatial frequencies. Then:\n푋푚푛= 퐷훼훽\n푚푛푌훼훽\n(7.32)\nand\n퐸[푋2\n푚푛] = 퐸[(퐷훼훽\n푚푛푌훼훽)2]\n(7.33)\nExpanding the squared term gives\n퐸[푋푚푛푋푚푛] = 퐸[퐷훼훽\n푚푛푌훼훽퐷훼훽\n푚푛푌훼훽]\n(7.34)\nAnd expanding the expectation gives\n1\n|푀||푁| 푋푚푛푋푚푛=\n1\n|퐴||퐵| 퐷훼훽\n푚푛푌훼훽퐷훼훽\n푚푛푌훼훽\n(7.35)\nNote that\n1\n|푀||푁| =\n1\n|퐴||퐵| so we cancel giving\n푋푚푛푋푚푛= 푌훼훽퐷훼훽\n푚푛푌훼훽퐷훼훽\n푚푛\n(7.36)\nRearranging the right-hand side gives\n푋푚푛푋푚푛= 퐷훼훽\n푚푛퐷훼훽\n푚푛푌훼훽푌훼훽\n(7.37)\nSince the tensors 퐷are deﬁned with respect to a standard orthonormal\nbasis, we can freely raise and lower their indices (their metric tensor is\nidentity). Lowering 훼, 훽and raising 푚, 푛on one of the 퐷tensors gives:\n푋푚푛푋푚푛= 퐷훼훽\n푚푛퐷푚푛\n훼훽푌훼훽푌훼훽\n(7.38)\nSince 퐷훼훽\n푚푛퐷푚푛\n훼훽= 1 we have\n푋푚푛푋푚푛= 푌훼훽푌훼훽\n(7.39)\n= 푋2\n푚푛= 푌2\n훼훽\n(7.40)\nSubstituting gives\nVar[푋] = E[푋2] = 퐸[푌2]\n(7.41)\n■\n7.3 ReLU\n63\n7: Depending on the batch norm imple-\nmentation, it may be necessary to apply\nBessel correction to the variance compu-\ntation as well.\nDCT Coefficients\nGlobal Average Pooling Vector\nFigure 7.2: Illustration of transform do-\nmain global average pooling.\nFukushima and Miyake, “Neocognitron:\nA self-organizing neural network model\nfor a mechanism of visual pattern recog-\nnition”\nNair and Geoﬀrey E Hinton, “Rectiﬁed\nlinear units improve restricted boltz-\nmann machines”\nTherefore, it is suﬃcient to compute the mean of the squared DCT\ncoeﬃcients to get the variance of the underlying pixels. This is no faster\nor slower than the pixel domain algorithm.\nNext, we move on to the aﬃne parameters 훾, 훽. Applying 훾is easy:\nsince the transform we are using is linear, multiplying by a scalar can\nhappen before or after the transform, i.e.,\n퐽(훾퐼) = 훾퐽(퐼)\n(7.42)\nso we can simply multiply the transform coeﬃcients by 훾. Applying 훽is\nalso straightforward, since adding the scalar 훽would raise the mean by 훽,\nwe can add 훽to only the (0,0) coeﬃcient. This yields a simple closed-form\nalgorithm for computing batch normalization.\nListing 7.3: Transform Domain Batch\nNorm\n1 def batch_norm(F: Tensor, gamma: float, beta: float) -> Tensor:\n2\nmu = F[0, 0]\n3\nF[0, 0] = 0\n4\nvar = torch.mean(F**2)\n5\n6\nF *= gamma / torch.sqrt(var)\n7\nF[0, 0] = beta\n8\nreturn F\nNote that the algorithm in Listing 7.3 assumes each sample is a single\n8 × 8 block. If this is not the case, then the algorithm can be easily\nadjusted to compute combined mean and variance over several blocks\n(and multiple channels)7.\n7.2.3 Global Average Pooling\nGlobal average pooling reduces feature maps to a single scalar per\nchannel. In other words, spatial information is averaged \"globally\". Given\nthe discussion in the previous section, this is extremely simple to compute\nin the transform domain. As the (0, 0) coeﬃcient is proportional to the\nmean of each block, we can simply read oﬀthese coeﬃcients and scale\nthem to produce the global average pooling vector (Figure 7.2). This is\nsigniﬁcantly faster than the pixel domain algorithm. Note that this is\nexactly the result that the pixel domain algorithm would have generated,\nso from this point forward we no longer need to worry about operations\nin the transform domain (i.e., the fully-connected layers do not need\nmodiﬁcation).\n7.3 ReLU\nHaving deﬁned the exact operations, we now turn to a missing and\ncritical component of residual networks: ReLU (Fukushima and Miyake\n1982; Nair and Geoﬀrey E Hinton 2010). Note that we have dedicated an\nentire section to what is a relatively simple operation in the pixel domain.\nReLU is deﬁned as\n푅(푥) =\n(\n푥\n푥≥0\n0\n푥< 0\n(7.43)\n64\n7 JPEG Domain Residual Learning\n8: This is true for other piecewise func-\ntion intervals as well. The technique de-\nscribed here is general.\nThe previous section made use of mathematical properties of the JPEG\ntransform in order to derive closed form solutions for transform domain\noperations. Since ReLU is necessarily non-linear, we will have no such luck\nwith that approach. In fact, not only is ReLU non-linear, it is piecewise\nlinear depending on the pixel domain value, information which we do\nnot have access to in the transform domain. Instead, we will develop an\napproximation technique for ReLU that works in the transform domain\nand is tunable giving an accuracy-speed trade-oﬀ.\nWe compute this approximation by partially decoding each block of\ncoeﬃcients. This is still fast since only a subset of coeﬃcients are required\nand since the result of the approximation is in the pixel domain we can\nfreely compute ReLU on it. Recall the DCT Least Squares Approximation\nTheorem proven in Section 3.1 (The Fourier Transform).\nTheorem 7.3.1 (The DCT Least Squares Approximation Theorem)\nGiven a set of 푁samples of a signal 푋let 푌be the DCT coeﬃcients of 푋.\nThen for 1 ≤푚≤푁the approximation of 푋given by\n푝푚(푡) =\n1\n√\n푁\n푦0 +\nr\n2\n푁\n푚\nX\n푘=1\n푦푘cos\n\u0012 푘(2푡+ 1)휋\n2푁\n\u0013\n(7.44)\nminimizes the least-squared error\n푒푚=\n푁\nX\n푖=1\n(푝푚(푖) −푥푖)2\n(7.45)\nTheorem 7.3.1 guides us in choosing the lowest 푚frequencies when we\ndecode (rather than some arbitrary set) in order to constrain the error of\nthe approximation. For a 2D DCT, we use all frequencies (푖, 푗) such that\n푖+ 푗≤푚yielding 15 frequencies. The threshold 푚is freely tunable to\nthe problem and we will examine its eﬀect later.\nAlthough we now have a reasonable algorithm for computing ReLU\nfrom transform coeﬃcients, we are left with two major problems. The\nﬁrst is that although our approximation was motivated by a least-squares\nminimization, it is not guaranteed to reproduce any of the original\nsamples. Since ReLU preserves positive samples (only zeroing negative\nsamples) it would be nice if at least those were preserved. The second is\nthat our network expects transform coeﬃcients as input but the ReLU we\nhave computed is in the spatial domain. It would be expensive to have to\nconvert the result back to transform coeﬃcients before continuing our\ncomputation.\nConsider for a moment the nature of our ﬁrst problem. Suppose we\nhave a sample with value 0.7. After taking the DCT and computing the\nleast-squares approximation with a subset of coeﬃcients, the value of\nthis sample is changed to 0.5. We can observe that although the least-\nsquares approximation is incorrect, it is still positive. In other words, the\nreconstruction has not changed the sign of the sample so it will not be\nzeroed by ReLU. The more coeﬃcients we use the more likely it is that\nthese reconstructions are sign-preserving8 since the high frequencies\ncontribute less to the accuracy of the result (otherwise they would not\nbe a least-squares minimization). In this sense we can observe that it is\neasier to preserve the sign than the exact pixel value.\n7.3 ReLU\n65\nOriginal\nTrue ReLU\nNaive\nASM\nFigure 7.3: ReLU Approximation Exam-\nple. Green pixels are negative, red pix-\nels are positive, blue pixels are exactly\nzero. The top-left shows the original im-\nage. The top-right is the true ReLU. The\nbottom-left shows a naive approximation\nusing only the least squares approxima-\ntion. Note that while negative pixels are\nzeroed, very few positive pixels have\nthe correct value and there are mask er-\nrors resulting from the approximation.\nThe bottom-right image shows the ASM\ntechnique. Note that while there are still\nmask errors, positive pixel values are\npreserved.\nTherefore, rather than compute ReLU on this approximation, we can\ninstead compute a mask and apply that mask. We reformulate ReLU as\nfollows\n푅(푥) = 퐻(푥)푥\n(7.46)\n퐻(푥) =\n(\n1\n푥≥0\n0\n푥< 0\n(7.47)\nwhere 퐻(푥) is the Heaviside step function which we treat as a mask. If\nwe compute 퐻(푝푚) on the approximation 푝푚, and multiply the result by\nthe original samples 푥, we will have masked the negative samples while\npreserving the positive ones. We call this technique Approximated Spatial\nMasking (ASM). See Figure 7.3 for a visual example of this algorithm.\nThe only problem left to solve is that our original samples are in the\ntransform domain and the mask is in the pixel domain. To simplify the\nfollowing discussion, we consider only DCT blocks here (extending to\nthe full transform is trivial). We can solve this using our multilinear\nmodel of the JPEG transform. Given transform coeﬃcients 퐹∈퐴∗⊗퐵∗, a\nspatial domain mask 퐺∈푀∗⊗푁∗, and the masked result 퐹′ ∈퐴∗⊗퐵∗,\nconsider the steps such an algorithm would perform\n1. Take the inverse DCT of 퐹to give 퐼∈푀∗⊗푁∗\n2. Pixelwise multiply the mask 퐺and 퐼to give 퐼′\n3. Take the DCT of 퐼′ to give the masked result 퐹′\nAll of these steps are linear or bilinear\n퐼푚푛= 퐷훼훽\n푚푛퐹훼훽\n(7.48)\n퐼′\n푚푛= 퐺푚푛퐼푚푛\n(7.49)\n퐹′\n훼′훽′ = 퐷푚푛\n훼′훽′퐼′\n푚푛\n(7.50)\nSubstituting, we have\n퐹′\n훼′훽′ = 퐷푚푛\n훼′훽′퐺푚푛퐼푚푛\n(7.51)\n= 퐷푚푛\n훼′훽′퐺푚푛퐷훼훽\n푚푛퐹훼훽\n(7.52)\n= 퐺푚푛퐷푚푛\n훼′훽′퐷훼훽\n푚푛퐹훼훽\n(7.53)\nAnd we make the following deﬁnition (after raising some indices to\npreserve dimensions)\n퐹′\n훼′훽′ = 퐺푚푛\nh\n퐷푚푛\n훼′훽′퐷훼훽푚푛i\n퐹훼훽\n(7.54)\nΨ훼훽푚푛\n훼′훽′\n= 퐷푚푛\n훼′훽′퐷훼훽푚푛\n(7.55)\ngiving the bilinear map Ψ : 푀∗⊗푁∗× 퐴∗⊗퐵∗→퐴∗⊗퐵∗. This map\ncan be computed once and reused. We can use this map along with our\napproximate mask and original DCT coeﬃcients to produce a highly\naccurate ReLU approximation with few coeﬃcients.\n66\n7 JPEG Domain Residual Learning\n7.4 Recap\nBefore continuing to empirical concerns, we brieﬂy recap the theoretical\ndiscussion in the previous sections. Residual networks consist of four\nbasic operations: Convolution, Batch Normalization, Global Average\nPooling, and ReLU.\nIn Section 7.2.1 (JPEG Domain Convolutions) we found that JPEG\ndomain convolutions can be expressed as\nΞ푝푥푦푘\n푝′푥′푦′푘′ = e퐽ℎ′푤′\n푥′푦′푘′퐶푝ℎ푤\n푝′ℎ′푤′퐽푥푦푘\nℎ푤\n(7.56)\nand in Listing 7.2 we developed a fast algorithm for computing this.\nIn Section 7.2.2 (Batch Normalization) we developed a closed form\nsolution for JPEG domain batch normalization. We found that\nE[퐼] = 1\n8퐷(0, 0)\n(7.57)\nVar[퐼] = E[퐷2] iﬀ퐷(0, 0) = 0\n(7.58)\nand that we can apply 훽by adding it to 퐷(0, 0) and we can apply 훾as we\nwould to a spatial domain input (by multiplying it by each coeﬃcient).\nIn Section 7.2.3 (Global Average Pooling) we found that global average\npooling in the JPEG domain is as simple as computing 1\n8퐷(0, 0) from\neach channel. We also noted that since this is equivalent to the spatial\ndomain mean, there is no need to derive the fully-connected layers.\nFinally, in Section 7.3 (ReLU) we developed an approximation technique\nfor ReLU where we use a subset of coeﬃcients to decode each block and\ncompute and approximate 퐻(푥) on each block where 퐻() is the Heaviside\nstep function producing a mask 퐺푚푛. Then we apply this mask to the\noriginal coeﬃcients 퐹훼훽using\n퐹′\n훼′훽′ = 퐺푚푛Ψ훼훽푚푛\n훼′훽′ 퐹훼훽\n(7.59)\nThis concluded our theoretical derivations.\nModel Conversion\nOne important thing to note is that at no time did we stipulate that\nthe convolution weights or batch norm aﬃne parameters need to be\nlearned from scratch. Indeed, this method can take any such values,\nrandom or learned, and produce JPEG domain operations. Therefore,\nwe can use the method to convert pre-trained models to operate in the\nJPEG domain. This idea has some powerful implications and we will\nexamine it’s trade-oﬀs in the empirical analysis.\n7.5 Empirical Analysis\nWe now turn out attention to an empirical evaluation of the algorithm.\nAfter all, the discussion in the previous sections was highly theoretical\n7.5 Empirical Analysis\n67\nInput: T X 1 X 32 X 32\nRes Block 1: 16 Filters, No \nDownsampling\nOutput: (T X 16 X 32 X 32)\nRes Block 2: 32 Filters, \nDownsampling\nOutput: T X 32 X 16 X 16\nRes Block 3: 64 Filters, \nDownsampling\nOutput: T X 64 X 8 X 8 \n(single JPEG block)\nGlobal Average Pooling\nOutput: T X 64\nFully Connected: 64 to \n10/100\nOutput: T X 10/100\nFigure 7.4: Toy Network Architecture.\nNote that by the ﬁnal ResBlock, the image\nis reduced to 8×8 which is a single block\nof coeﬃcients. This simpliﬁes the global\naverage pooling layer.\nLeCun, “The MNIST database of hand-\nwritten digits”\nKrizhevsky, Geoﬀrey Hinton, et al.,\n“Learning multiple layers of features\nfrom tiny images”\n9: MNIST inputs are zero padded with\ntwo pixels on each side\nTable 7.1: Model Conversion Accura-\ncies. Note that the deviation is small\nbetween the spatial domain and JPEG\ndomain network.\nDataset\nSpatial\nJPEG\nDeviation\nMNIST\n0.988\n0.988\n2.999e-06\nCIFAR-10\n0.725\n0.725\n9e-06\nCIFAR-100\n0.385\n0.385\n1e-06\n10: Assuming the same number of fre-\nquencies are used for training and infer-\nence.\nand altogether divorced from practical concerns. A natural question at\nthis point is: “How well does this actually work?”\nWe will start by creating a toy network. This small network will be\nused in the experiments in this section to evaluate and benchmark the\ntechnique. This toy architecture consists of three residual blocks followed\nby global average pooling and a single fully connected layer. Although\nthis is a simple architecture, it will more than suﬃce for our benchmarks\nof MNIST (LeCun 1998) and CIFAR 10/100 (Krizhevsky, Geoﬀrey Hinton,\net al. 2009). The inputs will always be 32 × 32 images to ensure an even\nnumber of JPEG blocks9. We consider two versions of this network, one\nwhich processes images in the spatial domain (i.e., a traditional ResNet)\nand one which we have applied algorithm on to allow it to process JPEG\ntransform coeﬃcients.\nFor those unconvinced by mathematics (or maybe suspicious of the\nability to implement the math in PyTorch), we ﬁrst examine whether our\nderivations were correct at all. This is straightforward: we simply use an\nexact ReLU, taking all 15 frequencies for the JPEGiﬁed version of the toy\nnetwork. For more meaningful accuracies, the network is trained until\nconvergence in the pixel domain and the weights are then converted.\nSince our other operations are supposed to be \"exact\", this should yield\nthe same accuracy as a pixel domain network to within some small\nﬂoating point error, which is conﬁrmed by the result in Table 7.1.\nNext we examine the accuracy of the ReLU approximation. Since this\nis not a true ReLU, we expect there to be some eﬀect on overall network\naccuracy when fewer frequencies are used. However, it is still a non-\nlinearity which should enable the network to learn eﬀective mappings.\nWe consider ReLU accuracy from three perspectives\nAbsolute Error How accurate is our ASM approximation compared\nwith an naive approximation?\nConversion Error If we convert pre-trained weights, how much does\nfrequency eﬀect the ﬁnal accuracy result?\nTraining Error If we train a network from scratch using the ReLU ap-\nproximation, how much does frequency aﬀect the ﬁnal accuracy\nresult?10\nWe show results to this eﬀect in Figure 7.5. The left graph shows the\nabsolute error of the ReLU approximation. For this experiment, 10 million\n8 × 8 blocks are generated by upsampling random 4 × 4 pixel blocks. We\nthen measure RMSE between the true block and the approximated block.\nNote that compared to the naive approximation, the ASM method we\ndeveloped has lower error throughout and the error drops faster. In the\nmiddle graph, we show model conversion error. We train 100 models\n68\n7 JPEG Domain Residual Learning\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 1\n 3\n 5\n 7\n 9\n 11\n 13\n 15\nAverage RMSE\nNumber of Spatial Frequencies\nAPX\nASM (ours)\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1\n 3\n 5\n 7\n 9\n 11\n 13\n 15\nAverage Accuracy (%)\nNumber of Spatial Frequencies\nAPX MNIST\nAPX CIFAR10\nAPX CIFAR100\nASM MNIST\nASM CIFAR10\nASM CIFAR100\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1\n 3\n 5\n 7\n 9\n 11\n 13\n 15\nAverage Accuracy (%)\nNumber of Spatial Frequencies\nAPX MNIST\nAPX CIFAR10\nAPX CIFAR100\nASM MNIST\nASM CIFAR10\nASM CIFAR100\nFigure 7.5: ReLU Approximation Accuracy. Left: RMSE error. Middle: Model accuracy after model conversion. Right: Model accuracy\nwhen re-training from scratch. Note that APX denotes he naive ReLU approximation. Dotted lines represent spatial domain accuracy.\n 0\n 5\n 10\n 15\n 20\nMNIST\nCIFAR10\nCIFAR100\nThroughput (Images/Sec)\nJPEG Training\nJPEG Testing\nSpatial Training\nSpatial Testing\nFigure 7.6: Throughput Comparison.\nWe compare JPEG domain and spatial\ndomain training and inference.\nfrom random weights in the pixel domain and then apply our algorithm\nto convert the weights, and measure the resulting classiﬁcation accuracy.\nAgain we see that the ASM method has better performance. In the ﬁnal\ngraph, we train networks from random weights using our JPEG domain\nalgorithm. Interestingly, this performs signiﬁcantly better than model\nconversion indicating that the weights have learned to adapt to the ReLU\napproximation.\nThe ﬁnal result we show is throughput. In general, the method devel-\noped here should be fast if for no other reason than the JPEG images do\nnot need to be decompressed before being processed. In Figure 7.6 we\ncompare throughput for training and testing in the JPEG domain vs in\nthe spatial domain. As expected, inference is signiﬁcantly faster in the\nJPEG domain. Curiously, however, training is only slightly faster. This\nis caused by the more complex update rule for autograd to compute\nthrough the ReLU approximation and the JPEG domain conversion for\nthe convolutions.\n7.6 Limitations and Future Directions\nThe astute reader will have noticed by now a major limitation with this\nwork: memory usage. Recall that compressed domain convolutions are\nformed by convolving the kernel 퐶, a dim(푃) × dim(푃′) × 3 × 3 matrix,\nwith the JPEG decompression tensore퐽∈푋⊗푌⊗퐾⊗퐻∗⊗푊∗and then\napplying the JPEG compression tensor 퐽∈푋∗⊗푌∗⊗퐾∗⊗퐻⊗푊. This\nyields a the type (3, 3) tensor Ξ ∈푃′ ⊗푋⊗푌⊗퐾⊗푃⊗푋∗⊗푌∗⊗퐾∗.\nObserve the size of this tensor. For an image of size dim(퐻) × dim(푊)\nit is in 푂((dim(퐻) × dim(푊))2). In other words we have taken a small\nconstant size weight and expanded it to be on the order of the image\nsize squared. This is perhaps the primary direction for future work. The\nmassive size of this tensor entirely prevents the method from being useful\nfor anything beyond the toy network and small image datasets presented\nin the previous section. While a constant size kernel could be created\nusing tiling (each convolution depends on at most the blocks one outside\nof the “currently processed” block), this would still be signiﬁcantly larger\nthan the small kernel used by spatial domain networks. By restricting the\nconvolution to a single block, an dim(푃′) × dim(푃) × 8 × 8 × 8 × 8 kernel\ncould be created with an approximate result which would signiﬁcantly\n7.6 Limitations and Future Directions\n69\nimprove the situation. It is left to future work to determine the practicality\nof these ideas and what their eﬀect on network accuracy is.\nOur ReLU formulation is currently an approximation. As we studied in\nthe previous section, this approximation does impact the overall network\naccuracy even when retraining. It would be nice if an exact ReLU could\nbe formulated to avoid this issue. It is currently unknown if this is\npossible.\nWhile on the topic of ReLU, software support for our method is\ncurrently quite lacking. In essence, many of our memory and speed\nsavings come from the sparse nature of JPEG compressed data. Zero\nelements could take up no memory and contribute no operations to\nthe compute graph, but this depends on adequate software support for\nsparse operations which is currently missing from libraries like PyTorch.\nSpeciﬁcally, support for sparse einsum would need to be added. This\nis perhaps the low-hanging fruit that would immediately reduce the\nmemory footprint while further increasing the speed the algorithm.\nFoi et al., “Pointwise shape-adaptive\nDCT for high-quality deblocking of com-\npressed color images “”\nS. Yang et al., “Blocking artifact free in-\nverse discrete cosine transform”\nTran et al., “The generalized lapped\nbiorthogonal transform”\nDong, Y. Deng, et al., “Compression arti-\nfacts reduction by a deep convolutional\nnetwork”\nYu et al., “Deep convolution networks for\ncompression artifacts reduction”\nDong, Loy, et al., “Learning a deep\nconvolutional network for image super-\nresolution”\nImproving JPEG Compression 8\n8.1 Pixel Domain Techniques . 71\n8.2 Dual-Domain Techniques . 73\n8.3 Sparse-Coding Methods . . 75\n8.4 Summary and Open Prob-\nlems . . . . . . . . . . . . . . . 75\nW\nith a good understanding of JPEG compression and how it relates\nto deep learning, we turn to a survey of methods which improve\nJPEG compression. These methods are essentially specializations of image\nenhancement. So sister problems in this domain are super-resolution,\ndenoising, deraining, etc. Notably, we will not be considering new deep\nlearning based codecs which are beyond the scope this this dissertation.\nThese methods are reviewed brieﬂy in Appendix C, however. We focus\non historical methods which made signiﬁcant advancements in the\nunderstanding of JPEG artifact correction and present them roughly in\npublication order although they are grouped into sections by their high\nlevel ideas.\nBefore discussing the deep learning techniques we ﬁrst mention two\nclassical methods for correction of JPEG artifacts. The ﬁrst method uses\na “pointwise shape-adaptive DCT” (SA-DCT) (Foi et al. n.d.). The SA-\nDCT can be thought of as a generalization of the block DCT used by\nJPEG to account for block sizes of varying shape. Foi et al. model JPEG\ncompression artifacts as Gaussian noise with zero mean and compute 휎2\nusing an empirically developed formula on the quantization matrix. For\neach point in the image, the technique computes a DCT kernel that best\nﬁts the underlying data (hence shape-adaptive). This ﬁlter is then used\nto estimate the Gaussian noise term for enhancement. The next method\n(S. Yang et al. 2000) use a generalized lapped biorthogonal transform\n(GLBT) (Tran et al. 1998). In this technique, the JPEG DCT coeﬃcients\nare modeled as an intermediate output of the GLBT and the remaining\nﬁlters in the method are designed to remove blocking artifacts. Prior to\ndeep learning, these techniques were the most successful at removing\nJPEG artifacts.\nWarning\nThis chapter is mostly a history lesson. Skip to the last section if you\nwant a TL;DR.\n8.1 Pixel Domain Techniques\nWe begin our discussion with the straightforward “pixel domain” tech-\nniques. These networks function as traditional convolutional networks.\nThey use pixels and input and output either the corrected network or its\nresidual. The ﬁrst such technique was the ARCNN (Dong, Y. Deng, et al.\n2015) later followed up by Fast ARCNN (Yu et al. 2016). These networks\nfollowed a traditional encoder-decoder architecture and are based oﬀof\nthe contemporary SRCNN (Dong, Loy, et al. 2014).\nARCNN is tiny by modern standards with four convolutional layers.\nThe ﬁrst is a 9 × 9 layer with 64 channels, next a 7 × 7 with 32 channels,\nthen a 1 × 1 with 16 channels and ﬁnally a 5 × 5 decoder with 1 channel\n72\n8 Improving JPEG Compression\nRonneberger et al., “U-net: Convolutional\nnetworks for biomedical image segmen-\ntation”\nSvoboda et al., “Compression artifacts\nremoval using convolutional neural net-\nworks”\nCavigelli et al., “CAS-CNN: A deep con-\nvolutional neural network for image com-\npression artifact suppression”\nP. Liu et al., “Multi-level wavelet-CNN\nfor image restoration”\nHonggang Chen et al., “DPW-SDNet:\nDual pixel-wavelet domain deep CNNs\nfor soft decoding of JPEG-compressed\nimages”\n(for grayscale only). The authors of ARCNN claim that each layer is\ndesigned for a speciﬁc purpose but there is no deep supervision on the\nlayers and they are trained end-to-end so it is unlikely that they learn a\nparticular task.\nFast ARCNN changes this architecture to an “hourglass” shape, essen-\ntially a U-Net (Ronneberger et al. 2015) without skip connections which\nwas common at the time. The architecture uses strided convolutions\nfor the downsampling operations. Since the size of the feature maps is\nreduced, the architecture processes images faster, hence the name. This\ndoes reduce the overall reconstruction accuracy, however.\nThe L4/L8 networks (Svoboda et al. 2016) introduce two major new\nideas to artifact correction. The ﬁrst is the idea of residual learning, where\nthe network is encouraged to learn only the diﬀerence between the input\nimage and the true reconstruction. In other words, the reconstructed\nimage 푋푟is expressed as\n푋푟= 푋푐+ 푓(푋푐)\n(8.1)\nfor compressed image 푋푐and learned network 푓(). The second contri-\nbution is that of an edge preserving loss. The authors rightly observe\nthat prior networks, due to their regression only losses, have blurry\nedges. They solve this by using Sobel ﬁlters to compute the partial ﬁrst\nderivatives of the reconstructed image and computing loss on these\nﬁltered images which focuses the network on edge reconstructions. As\nexpected the L4/L8 architectures have four and eight layers respectively\nand otherwise do not diﬀer signiﬁcantly from ARCNN.\nCAS-CNN (Cavigelli et al. 2017) build on the previous idea by employ-\ning a signiﬁcantly more complex architecture. This architecture contains\nskip connections, not unlike a U-Net, and upgrades the traditional re-\ngression loss to use multiple scales. These scales are computed using\ndeep supervision of the downsampled feature maps and make a fairly\nsigniﬁcant improvement to the overall accuracy. This is likely helped by\nthe skip connections in the U-Net architecture.\nWe now jump to the MWCNN (P. Liu et al. 2018) which is a major\ndiﬀerence in architecture. MWCNN is a fascinating method for general\nimage restoration which was applied directly to JPEG artifacts at the time\nof publication (along with other problems). The key idea is to replace the\npooling layers in a traditional CNN with a discrete wavelet transform.\nRecall that a discrete wavelet transform computes band-pass ﬁlters which\nrestrict each output to half the frequency range of the input. By the\nnyquist sampling theorem, we can then discard half the samples without\nlosing any information. MWCNN exploits this by using the DWT in place\nof a pooling operation, stacking the resulting four frequency sub-bands\nin the channel dimension without any signiﬁcant loss of information. The\noriginal image can then be reconstructed by using the inverse wavelet\ntransform on the feature maps after traditional convolutional layers.\nOtherwise the architecture resembles U-Net. The use of this clever signal\nprocessing trick allows MWCNN to achieve remarkable results on a\nnumber of restoration tasks including JPEG artifact correction.\nHonorable mention at this point goes to DPW-SDNet (Honggang Chen\net al. 2018). This could be considered a dual-domain method although we\n8.2 Dual-Domain Techniques\n73\nB. Zheng, R. Sun, et al., “S-Net: a scalable\nconvolutional neural network for JPEG\ncompression artifact reduction”\nGalteri et al., “Deep generative adversar-\nial compression artifact removal”\nGalteri et al., “Deep universal genera-\ntive adversarial compression artifact re-\nmoval”\n1: Which, in my opinion, is completely\nacceptable.\nY. Zhang et al., “Residual dense network\nfor image restoration”\nX. Wang, Yu, et al., “Esrgan: Enhanced\nsuper-resolution generative adversarial\nnetworks”\nHe, Xiangyu Zhang, et al., “Deep residual\nlearning for image recognition”\nX. Liu et al., “Data-driven sparsity-based\nrestoration of JPEG-compressed images\nin dual transform-pixel domain”\nGuo and Chao, “Building dual-domain\nrepresentations for compression artifacts\nreduction”\ntake a somewhat stricter deﬁnition of domain so instead we list it here\nwith MWCNN. The main contribution of DPW-SDNet was to include\ntwo networks, one which processes the image in the pixel domain and\nanother which processes it after a single level DWT.\nAnother method from 2018, S-Net (B. Zheng, R. Sun, et al. 2018),\nintroduces a scalable network. This is based on the apt observation that\nmore quantization requires a deeper network and “more work” to restore.\nTheir architecture is, therefore, scalable either based on the amount of\ndegradation applied to the image or constraints on the compute budget\nof the hardware. This was an important contribution toward the practical\nuse of artifact correction and remains an under-explored idea.\nTwo works by Galteri et al. (Galteri et al. 2017, 2019) introduce GANs to\nthe problem of artifact correction. As we observed in the discussion of\nL4/L8 and CAS-CNN, regression losses produce a blurry result. This\nis both because of the CNN’s inherent bias towards error minimization,\nsomething which is easiest to accomplish with a low-frequency recon-\nstruction, and because of JPEG’s tendency to destroy high frequency\ndetails in the ﬁrst place. Although L4/L8/CAS-CNN make progress on\nthis problem with specialized losses, they had obvious limitations which\nGalteri et al. overcome with a GAN loss. This generates signiﬁcantly more\nrealistic reconstructions, although there is no attempt at an “accurate” re-\nconstruction with good numerical results1. The 2019 version of this work\neven includes a rudimentary attempt at a “universal” architecture which\ncan operate independently of quality setting although it accomplishes\nthis with an ensemble.\nThe ﬁnal technique we discuss in this section is RDN (Y. Zhang et al.\n2020). This represents a departure from the more traditional U-Net style\nnetworks we have been discussing. Instead, RDN is based on ESRGAN\n(X. Wang, Yu, et al. 2018) and its RRDB layers. These layers are an\nenhanced version of the traditional residual layer (He, Xiangyu Zhang,\net al. 2016) with more residual connections. Just as these layers were a\nhuge improvement for super-resolution, they are a huge improvement\nfor artifact correction.\n8.2 Dual-Domain Techniques\nDual domain techniques are the result of an attempt to inject some low\nlevel JPEG data into the learning process. The high level idea is to process\nthe input in both the spatial (pixel) domain and the frequency (DCT)\ndomain. This is done with two separate networks and their result is fused.\nThis way if there is some information that either domain does not capture,\nit can potentially be exploited by the other domain. The technique was\nintroduced with a sparse-coding method (X. Liu et al. 2015) that we will\nexamine in the next section.\nOn the deep learning side the idea is ﬁrst addressed with DDCN (Guo\nand Chao 2016). The idea is very straightforward. There are two separate\nencoders, one for the pixel domain and one for the DCT domain. The\noutput of both networks is processed by a third aggregation network\nwhich decodes to a residual that is added to the input image.\n74\n8 Improving JPEG Compression\nXiaoshuai Zhang et al., “DMCNN: Dual-\ndomain multi-scale convolutional neu-\nral network for compression artifacts re-\nmoval”\nB. Zheng, Y. Chen, et al., “Implicit dual-\ndomain convolutional network for robust\ncolor image compression artifact reduc-\ntion”\nJin et al., “Dual-stream Multi-path Recur-\nsive Residual Network for JPEG Image\nCompression Artifacts Reduction”\nDMCNN (Xiaoshuai Zhang et al. 2018) extends this idea in two ways.\nThe ﬁrst is with a multiscale loss on the pixel branch as we saw in L4/L8.\nThe next is with a DCT Rectiﬁer which constrains the magnitude of the\nDCT residual based on the possible values that the true coeﬃcients could\ntake. Recall the formula for quantization\n푌′\n푖푗=\n\u0016\n푌푖푗\n(푄푦)푖푗\n\u0019\n(8.2)\nshown here for the 푌channel only. The approximated coeﬃcient is then\nb푌푖푗=\n\u0016\n푌푖푗\n(푄푦)푖푗\n\u0019\n(푄푦)푖푗\n(8.3)\nDividing by (푄푦)푖푗gives\nb푌푖푗\n(푄푦)푖푗\n=\n\u0016\n푌푖푗\n(푄푦)푖푗\n\u0019\n(8.4)\nWe can now expand this as an inequality since the rounded value must\nrange from [−1\n2 , 1\n2] around the rounding result\nb푌푖푗\n(푄푦)푖푗\n−1\n2 ≤\n푌푖푗\n(푄푦)푖푗\n≤\nb푌푖푗\n(푄푦)푖푗\n+ 1\n2\n(8.5)\nmultiplying by (푄푦)푖푗yields our desired constraint on 푌푖푗\nb푌푖푗−\n(푄푦)푖푗\n2\n≤푌푖푗≤b푌푖푗+\n(푄푦)푖푗\n2\n(8.6)\nSince the artifact correction network is trying to compute 푌푖푗from b푌푖푗,\nthis constraint helps reduce the space of possible solutions.\nThe next major innovation in dual-domain methods is IDCN (B. Zheng,\nY. Chen, et al. 2019). The major advantage of IDCN is that it is designed for\ncolor images and uses “variance maps” to account for the diﬀerences in\nstatistics between the channels. Their dual-domain formulation is also of\ninterest. They introduce a dual-domain layer which is “implicit”, similar\nto our result in Section 6.2 (The Multilinear JPEG Representation), the\nDCT transform can be composed such that the DCT result and the pixel\nresult are computed simultaneously.\nFinally Jin et al. (Jin et al. 2020) extend the dual domain concept\nto process frequency bands in diﬀerent paths. This is based on two\nobservations: ﬁrstly, some artifacts are restricted to particular frequency\nbands and secondly, as we have said many times, accurate high-frequency\nreconstructions are diﬃcult. By separating out the frequency bands for\nseparate processing, the network is able to focus on restoring those\nparticular frequencies as well as freeing up model capacity for artifacts\nwhich occur only in the considered frequency bands of the branch.\n8.3 Sparse-Coding Methods\n75\nTable 8.1: Summary of JPEG Artifact Correction Methods. The methods are all listed with their technique (CNN or Sparse Coding)\nand whether they incorporate dual domain information or not. This table is not exhaustive. Methods are sorted by year.\nYear\nMethod\nCitation\nTechnique\nDual Domain\nNote\n2015\nARCNN\nDong, Loy, et al. 2014\nCNN\n×\nData driven sparsity ...\nX. Liu et al. 2015\nSparse Coding\n✓\n2016\nL4/L8\nSvoboda et al. 2016\nCNN\n×\nDDCN\nGuo and Chao 2016\nCNN\n✓\nD3\nZhangyang Wang et al. 2016\nSparse Coding\n✓\n2017\nCAS-CNN\nCavigelli et al. 2017\nCNN\n×\nDeep Genarative ...\nGalteri et al. 2017\nCNN\n×\nGAN, Color\n2018\nMWCNN\nP. Liu et al. 2018\nCNN\n×\nUses DWT instead of pooling\nDPW-SDNet\nHonggang Chen et al. 2018\nCNN\n×\nDual wavelet and pixel domain\nS-Net\nB. Zheng, R. Sun, et al. 2018\nCNN\n×\nScalable\nDMCNN\nXiaoshuai Zhang et al. 2018\nCNN\n✓\nDCT Rectiﬁer\n2019\nDeep Generative ...\nGalteri et al. 2019\nCNN\n×\nGAN, Universal with ensemble, Color\nIDCN\nB. Zheng, Y. Chen, et al. 2019\nCNN\n✓\nImplicit DCT Layer, Color\nDCSC\nFu et al. 2019\nSparse Coding\n×\nUses CNN Features\n2020\nRDN\nY. Zhang et al. 2020\nCNN\n×\nUses RRDB\nDual stream multi path ...\nJin et al. 2020\nCNN\n✓\n2: I do not believe that an “over com-\nplete basis” is an actual concept in linear\nalgebra. I assume the developers of this\nmethod are referring to a frame\nX. Liu et al., “Data-driven sparsity-based\nrestoration of JPEG-compressed images\nin dual transform-pixel domain”\nZhangyang Wang et al., “D3: Deep dual-\ndomain based fast restoration of jpeg-\ncompressed images”\nFu et al., “Jpeg artifacts reduction via\ndeep convolutional sparse coding”\n8.3 Sparse-Coding Methods\nSparse coding is a dictionary learning method. A series of representative\nexamples are learned which (we hope) form an “over complete” basis for\nour solution space 2. Because the input is no longer uniquely determined\nby the basis, we also try to enforce sparsity such that the members of the\nbasis are as sparse as possible. We do not cover sparse coding in more\ndetail in this dissertation.\nSparse coding was introduced to artifact correction by Li et al. (X. Liu\net al. 2015) where they also introduced dual domain learning. The idea is\nstraightforward: learn sparse codes in pixel space and DCT space and\nfuse the results.\nD3 (Zhangyang Wang et al. 2016) makes an interesting extension to\nLi et al.. They formulate the problem in a “feed forward” manner. In\nother words, sparse coding is used ﬁrst on the DCT coeﬃcients and then\nthe result of that is fed into anther sparse coding module in the pixel\ndomain. Both stages are supervised with loss functions similar to neural\nnetworks.\nThe ﬁnal sparse coding method we consider, DCSC (Fu et al. 2019) is\npixel domain only. However, they incorporate a simple convolutional\nnetwork into their architecture such that the sparse codes are computed\non CNN features. This gives a sort of “best case” scenario where the\npowerful convolutional features can be exploited by the sparse coding\nmethod. As a bonus, their method uses a single model for all quality\nsettings, although they do not train in the general case and only target\nqualities 10 and 20.\n8.4 Summary and Open Problems\nWe summarize all the methods discussed in this chapter in Table 8.1.\nThere are some interesting things we can take away from this discussion.\nFor example, it seems that dual-domain methods work well and they are\ncontinually revisited. Deeper networks have also naturally been successful\nbut the switch to RRDB layers by RDN was particularly interesting. More\n76\n8 Improving JPEG Compression\nGalteri et al., “Deep universal genera-\ntive adversarial compression artifact re-\nmoval”\nFu et al., “Jpeg artifacts reduction via\ndeep convolutional sparse coding”\nJiang et al., “Towards ﬂexible blind JPEG\nartifacts removal”\nKim, Soh, and Cho, “AGARNet: adap-\ntively gated JPEG compression artifacts\nremoval network for a wide range qual-\nity factor”\nKim, Soh, Park, et al., “A pseudo-blind\nconvolutional neural network for the re-\nduction of compression artifacts”\nZini et al., “Deep Residual Autoencoder\nfor quality independent JPEG restora-\ntion”\ncomplex techniques like wavelet based or sparse coding based methods\nare underutilized and may be more complex than is needed with the\nadvances of vanilla neural networks.\nOne noteworthy takeaway is that while there are pixel domain tech-\nniques and dual-domain techniques, there is not a single DCT domain\nonly technique. A careful examination of ablation studies in the dual-\ndomain papers explains this: their DCT branches do not perform well\non their own. Somehow, the DCT branch is capturing new information\nthat the pixel branch does not, but not enough to carry out restoration\non its own. This is likely caused by the DCT being a set of coeﬃcients\nfor orthogonal basis functions rather than a single correlated signal like\npixels. We will consider this an open problem as we move into the next\nsection.\nAlso somewhat surprising is that although many methods recognized\nthat JPEG artifact correction struggles to restore high frequencies with\nregression losses, only one author thought to use a GAN for correction.\nThis is at least partially because of the community’s incessant focus on\nbenchmark results as the criterion for publication. GAN restoration does\nnot perform well on the benchmarks. We consider this an open problem\nas well.\nAnother oddity: very few of the methods explicitly treat color images.\nThis is odd on its own but even more so when we consider that JPEG ex-\nplicitly handles chrominance diﬀerently than luminance by compressing\nit more aggressively and downsampling it. Also, there is spatial corre-\nlation between luminance and chrominance, which could and should\nbe exploited in the reconstruction. Only the works of Galteri et al. and\nIDCN explicitly handle color data. This is another open problem.\nFinally, and crucially, there are very few “universal” or “quality blinded”\ntechniques. In fact, the only ones discussed in this section were Galteri et\nal. (Galteri et al. 2019) and DCSC (Fu et al. 2019). All other networks in this\nsection trained a diﬀerent network for each quality setting they consider, a\npractice which is not sustainable in real deployments. Although solutions\nto this problem have been cropping up at the time of writing (Jiang et al.\n2021; Kim, Soh, and Cho 2020; Kim, Soh, Park, et al. 2019; Zini et al. 2019)\nwe again consider this to be an open problem. In the next chapter, we\nwill develop a method that addresses these problems.\nEhrlich, Davis, Lim, et al., “Quantization\nguided jpeg artifact correction”\nQuantization Guided JPEG\nArtifact Correction 9\n9.1\nOverview . . . . . . . . . . . 78\n9.2\nConvolutional Filter\nManifolds . . . . . . . . . . 79\n9.3\nPrimitive Layers . . . . . . 80\n9.4\nFull Networks . . . . . . . . 81\n9.5\nLoss Functions . . . . . . . 82\n9.6\nEmpirical Evaluation . . . 84\n9.6.1 Comparison with Other\nMethods . . . . . . . . . . . 85\n9.6.2 Generalization\n. . . . . . . 85\n9.6.3 Equivalent Quality . . . . . 86\n9.6.4 Exploring Convolutional\nFilter Manifolds . . . . . . . 86\n9.6.5 Frequency Domain Results 89\n9.6.6 Qualitative Results . . . . . 89\n9.7\nLimitations and Future\nDirections . . . . . . . . . . 92\nI\nn the previous chapter we discussed several methods for using deep\nnetworks to improve JPEG compression. These techniques augment\nJPEG with signiﬁcantly better rate-distortion while allowing users to still\nproduce and share their familiar JPEG ﬁles1\n1: With the added beneﬁt that users with-\nout special software can still view the\nﬁles albeit at lower quality.\n. These networks, however,\ncome with three major disadvantages that have so far made them purely\nacademic successes.\nFirst and foremost, these methods are so called “quality aware” meth-\nods in which all training and testing data is compressed at a single quality\nlevel. This yields a single model per JPEG quality, which is undesirable\nfor several reasons. Recall that quality is an integer in [0, 100], thus po-\ntentially requiring 101 diﬀerent models to be trained. Although it is likely\nthat the models may generalize to nearby qualities (we will examine this\nsomewhat in Section 9.6.2 (Generalization)), at the very least this still\nrequires the training and deployment of more than one model, something\nwhich is still considered expensive for most institutions. Furthermore,\nwhen these models are deployed, they will be given arbitrary JPEG ﬁles\nto correct and the JFIF ﬁle format does not store quality leaving a real\nsystem with no reliable method to choose a model. This problem could\nbe solved with an auxiliary model that regresses image to quality (Galteri\net al. 2019)\nGalteri et al., “Deep universal genera-\ntive adversarial compression artifact re-\nmoval”\nbut this still requires training and deploying an ensemble and\nnow an additional model to pick the quality.\nThe next, and perhaps more peculiar, problem with these methods\nis that they are grayscale only. In other words these models only work\non the luminance channel of the compressed images. While this does\nalign well with human perception, humans can certainly perceive color\ndegradations (see Figure 9.1 to perceive this yourself). There is an implicit\nassumption that luminance models could be applied channel-wise to\nYCbCr or RGB images however we ﬁnd that this does not hold well in\npractice as we show in Section 9.6.1 (Comparison with Other Methods).\nLastly these methods are hyper-focused on error metrics. While this has\nproven to be a reliable way to improve rate-distortion, it generally does\nnot translate to improved perceptual quality producing blurry edges and\nan overall lack of texture. To improve perceptual quality, more complex\ntechniques are required.\nIn this chapter we develop a technique which addresses all three of\nthese major problems. Our method leverages low-level JPEG information\nto condition a single network on quantization data stored in the JFIF\nﬁle, allowing one network to achieve good results on a wide range of\nqualities. Our network treats color channels as ﬁrst class citizens and\ntakes concrete steps to correct them eﬀectively, keeping in mind that\nJPEG compression treats color and luminance diﬀerently applying more\ncompression to the color channels. Finally we develop texture restoring\nand GAN losses that are designed to produce a visually pleasing result\nespecially at low qualities. This method was published separately in the\nproceedings of the European Conference on Computer Vision (Ehrlich,\nDavis, Lim, et al. 2020).\n78\n9 Quantization Guided JPEG Artifact Correction\nFirst Principles\n▶Conditioning the network on the quantization matrix allows it\nto correct at many diﬀerent qualities using information available\nto a real system\n▶Explicitly modeling color degradation improves performance\non color images\n▶Formulating DCT domain regression allows the network to\nleverage quantization data more eﬀectively\n▶GAN loss functions for high frequency restoration\n9.1 Overview\nThe method we develop in this chapter consists of several parts, all of\nwhich operate together to produce the ﬁnal result. We will develop this\nmethod from the bottom up, starting with the individual building blocks\nof the method and then describing how they are connected. At a high\nlevel, our network operates in several stages, these are illustrated in\nFigure 9.1.\nOur network ﬁrst corrects the luminance (Y) channel of the image. The\nluminance channel has less aggressive compression applied to it and\nserves as a base for further correction. Our network then moves on to\ncorrecting the color channels. As these channels are further compressed,\nthey lack ﬁne detail and structure that may have been present in the\nluminance channel especially after correction. Therefore, we provide the\ncorrected luminance channel along with the degraded color channel to\nthe color correction network to give it additional information.\nThroughout the network, we condition carefully selected layers on the\nJPEG quantization matrix. Recall that this 8 × 8 matrix describes how\nmuch rounding was applied to each DCT coeﬃcient. Because this is\ndirectly describing a phenomenon in the frequency domain, our entire\nnetwork processes the DCT coeﬃcients of the input only: no pixels are\nused, and the network produces DCT coeﬃcients as output. This is in\nstark contrast to other methods which use only pixel or both pixels and\ncoeﬃcients and depends on new developments in DCT domain networks.\nWe use methods described in Section 7.1 (New Architectures) to correctly\nprocess these data. Before these methods were developed, DCT domain\nnetworks had objectively inferior performance to pixel and dual-domain\nnetworks.\nOur training likewise proceeds in stages. After training the network to\nproduce only luminance coeﬃcients using regression, we then add the\ncolor network in and train it again using regression. This way, the color\nnetwork is always getting a high quality luminance result to condition\nits own correction on. After the luminance and chrominance networks\nare trained, we then ﬁne-tune the entire network using GAN and texture\nlosses. This adds signiﬁcant detail to the result while preventing it from\nquickly deviating and diverging.\n9.2 Convolutional Filter Manifolds\n79\nY Channel \nCorrection\nColor Channel \nCorrection\nGAN\nFigure 9.1: Overview. The network ﬁrst restores the Y channel, then the color channels, then applies GAN correction.\nKang et al., “Crowd counting by adapting\nconvolutional neural networks with side\ninformation”\nMildenhall et al., “Burst denoising with\nkernel prediction networks”\n9.2 Convolutional Filter Manifolds\nOne potential limitation of traditional convolutional networks is that they\nlearn only a single mapping from input features (퐹푖) to output features,\nin other words\nℎ(퐹푖) = 휎(푊★퐹푖)\n(9.1)\nfor non-linearity 휎and learned weight 푊. While this is suﬃcient for\nmany use cases, it can be limiting in others.\nSpeciﬁcally in our case, we would like to specialize the learned ﬁlters\nfor diﬀerent quantization matrices, in other words, the learned weight\n푊should be a function of the quantization matrix 푄. One simple way to\ndo this is to tile 푄to match the shape of 퐹and concatenate the two\nℎ(퐹푖, 푄) = 휎(푊★[퐹푖푄]])\n(9.2)\nhowever, this yields a linear mapping between 퐹푖and 푄limits the learned\nrelationship between 퐹푖and 푄.\nInstead, we can use a ﬁlter manifold (Kang et al. 2016), sometimes called\na kernel predictor (Mildenhall et al. 2018). The goal of the ﬁlter manifold\nis to predict a convolutional kernel given a scalar side input, i.e.,\nℎ(퐹푖, 푠) = 휎(푊(푠) ★퐹푖)\n(9.3)\nfor 푠∈ℝ, so now the weight 푊is a non-linear function of 푠. Kang et al.\nchoose a small MLP for 푊\nℎ(퐹푖, 푠) = 휎(푊(푠) ★퐹푖)\n(9.4)\n푊(푠) = 휎(푊2(휎(푊1푠)))\n(9.5)\nallowing the network to learn a non-linear relationship between the side\ndata and 퐹푖along with the learned mapping between 퐹푖and the network\noutput.\nHowever, our side input 푄is not a scalar, it is an 8 × 8 matrix. Using a\nMLP for this input would be computationally expensive, so we propose\na simple extension, termed convolutional ﬁlter manifolds(CFM), to replace\n푊() with a small convolutional network. We additionally learn a bias\n80\n9 Quantization Guided JPEG Artifact Correction\nX. Wang, Yu, et al., “Esrgan: Enhanced\nsuper-resolution generative adversarial\nnetworks”\nMaas et al., “Rectiﬁer nonlinearities im-\nprove neural network acoustic models”\nHe, Xiangyu Zhang, et al., “Delving deep\ninto rectiﬁers: Surpassing human-level\nperformance on imagenet classiﬁcation”\n0\n0\n0\n0\n1\n1\n1\n1\n2\n2\n2\n2\n63\n63\n63\n63\nG0\nG1\nG2\nG63\n...\n0\n0\n0\n0\n1\n1\n1\n1\n2\n2\n2\n2\n63\n63\n63\n63\nChannel / Frequency\nFigure 9.2: FCR With Grouped Convo-\nlutions. Each frequency component is\nprocessed in isolation with its own convo-\nlution weights. We implement this using\na grouped convolution with 64 groups.\nterm along with the weight\nℎ(퐹푖, 푄) = 휎(푊(푄) ★퐹푖+ 푏(푄))\n(9.6)\n푏(푄) = 푊푏★퐹푞(푄)\n(9.7)\n푊(푄) = 푊푤★퐹푞(푄)\n(9.8)\n퐹푞(푄) = 휎(푊2 ★(휎(푊1 ★푄)))\n(9.9)\nThis formulation allows us to learn parameterized weights represent-\ning the complex relationship between the JPEG DCT features and the\nquantization matrix and can be thought of as generating a “quantiza-\ntion invariant” representation for the network to operate on. This is the\nprimary contribution which allows the network to model degradations\nfrom many diﬀerent quality levels. In Section 9.3 (Primitive Layers) we\nwill describe primitive layers which make use of this formulation and\nin Section 9.4 (Full Networks) we will describe where these layers are\nplaced in the overall network structure in order to maximize their eﬀec-\ntiveness. In Section 9.6.4 (Exploring Convolutional Filter Manifolds), we\nwill explore some interesting properties of these layers.\n9.3 Primitive Layers\nThe network we develop in this chapter is dependent on several “primitive\nlayers” or basic operations which we will use to build the network. In this\nsection, we describe them in detail. The ﬁrst is the Residual-in-Residual\nDense Block (RRDB) layer (X. Wang, Yu, et al. 2018) ﬁrst developed for\nsuper-resolution. This layer consists of three “Dense Blocks” in a residual\nsequence. Each of these “Dense Blocks” consists of ﬁve convolution-relu\nlayers with skip connections between each layer forming an enhanced\nversion of the standard residual block. See Figure 9.3 for a schematic\ndepiction of this layer. We make only one change to the RRDB used in\nESRGAN by replacing the Leaky ReLU (Maas et al. 2013) with Parametric\nReLU (He, Xiangyu Zhang, et al. 2015).\nIn Section 7.1 (New Architectures) we discussed recent advances in\nconvolutional networks that can take advantage of the unique char-\nacteristics of DCT coeﬃcients. We employ both of these layers in our\nnetwork. The ﬁrst is frequency-component rearrangement where the\nDCT coeﬃcients for each block are arranged in the channel dimension\nyielding 64 channels and 1\n8th the width and height of the input. We take\nthe additional step of using grouped convolutions with 64 groups to\nensure that each frequency is processed in isolation. See Figure 7.1 for the\nfrequency rearrangement and Figure 9.2 for an illustration of the grouped\nconvolution. We insert these layers into the RRDB described above. This\nparadigm allows our network to focus on enhancing individual frequency\nbands more eﬀectively.\nHowever, many frequency bands are entirely zeroed out by the com-\npression process. Completely relying on the grouped convolution would\nbe destined for failure because if a frequency band is set to zero, no\namount of convolutional layers can change its value (it will either remain\nzero or be set to the layer biases). Therefore, we need a layer which is also\ncapable of looking at multiple frequency bands, and for this we choose\nthe 8 × 8 stride-8 layer. This layer produces a representation of each DCT\n9.4 Full Networks\n81\nDense Block\nDense Block\nDense Block\nDense Block\nConv\nPReLU\nConv\nPReLU\nConv\nPReLU\nConv\nPReLU\nConv\nFigure 9.3: RRDB Layer shown with in-\nput feature map 퐹푖and output feature\nmap 퐹표. Note that we change the original\nRRDB layer by adding PReLU layers.\nHochreiter et al., Gradient ﬂow in recur-\nrent nets: the diﬃculty of learning long-term\ndependencies\nblock by considering all the frequency bands in the block at once. Since\nthe stride is set to 8, the representation does not include information\nfrom nearby blocks. Information from nearby blocks is incorporated by\nprocessing the block representations with RRDB layers. Since these layers\nare considering the DCT coeﬃcients of the entire block, we make the\nadditional step to use CFMs instead of regular convolutions to equip the\nlayers with quantization information, thus generating the “quantization\ninvariant” block representation. This is shown in Figure 9.4. For our\napplications, the input to the CFM is the 8 × 8 quantization matrix. This\nis processed with a convolutional network to produce the weight and\nbias. Note that the weight layer has in_channels × out_channels chan-\nnels and is a transposed 8 × 8 convolution. The result is reshaped to an\nout_channels × in_channels × 8 × 8 convolution kernel.\n9.4 Full Networks\nWith the primitive layers deﬁned, we now show how to build those layers\ninto the networks and subnetworks our method uses for correction of\nJPEG artifacts. Recall that our method ﬁrst corrects the grayscale channel\nand then uses that result to aid correction of the chroma channels.\nTherefore we start by describing the grayscale correction network. After\nthat we will describe the color correction network.\nThe grayscale correction network, shown in Figure 9.5 left, consists of\nfour subnetworks which work in series to produce the ﬁnal correction:\ntwo blocknets, a frequencynet, and a fusion network which we describe\nnext. The blocknet (Figure 9.6 left) uses the 8 × 8 stride-8 CFM layers\ndescribed in the previous section. It computes block representations\nand then processes the representations with stacked RRDB layers before\ndecoding the block representations with a transposed CFM layer. Between\nthe two blocknets we place a frequencynet (Figure 9.6 middle). This uses\nthe FCR grouped convolutions to enhance frequency bands in isolation.\nThe frequencies are ﬁrst rearranged before being processed with RRDB\nlayers. The result is then rearranged to restore the frequencies to the\nspatial dimensions. The intermediate results from all of the subnetworks\nare then passed to a fusion layer (Figure 9.6 right). The primary purpose\nof this is to strengthen the gradient received by the early layers which\nwould be prone to gradient vanishing otherwise (Hochreiter et al. 2001).\n82\n9 Quantization Guided JPEG Artifact Correction\nFigure 9.4: 8×8 stride-8 CFM. Note that\nthe numbers in parenthesis denote the\nnumber of channels. The CFM layer com-\nputes a weight and bias from the quanti-\nzation matrix using a small convolutional\nnetwork. The result of this network is re-\nshaped as either a weight or bias.\nCFM \n channels\nCFM\n \nchannels Conv \n (64 channels)\nPReLU\nConv \n (64 channels)\nPReLU\nTransposed Conv \n(\n channels)\n Conv \n\n (\n channels)\nInput\nFeatures\n8\n8\n1\n1\n2: Note that this is a visual improvement\non its own, however it is nothing com-\npared to the result from the GAN and\ntexture losses.\nThe color correction network (Figure 9.5 right) borrows the main\nideas from the blocknet in the grayscale correction network. We assume\nthat inputs are 4:2:0 chroma subsampled, which means they must be\nupsampled by a factor of two in each dimension to match the grayscale\nresolution. We use the block representation of the color channels and use\na 4 × 4 stride-2 layer to do the upsampling. The result is concatenated\nchannelwise with the block representation of the restored Y-channel\nbefore being processed further and ﬁnally decoded. In both the grayscale\nand color networks, we treat network outputs as residuals which are\nadded to the degraded input coeﬃcients.\n9.5 Loss Functions\nA well documented problem with image-to-image translation is that of\na blurry result. Intuitively, since the network is told to optimize 푙1 or 푙2\ndistance between the input and output, the easiest way to accomplish its\ngoal is to produce a sort of “averaging”. The human perception of this\naveraging is as a low-frequency image which lacks ﬁne details. This is\nexacerbated by compression which intentionally removes high frequency\ndetails. In this sense, a simple error based loss function is, in essence,\nasking the network to solve the wrong problem. What we really want the\nnetwork to do is restore high frequencies.\nNevertheless, an error based loss is useful for correcting hard block\nboundaries that JPEG creates as well as for preventing divergence with\nmore complex losses. Therefore, we pre-train the grayscale and color\nnetworks using 푙1 and Structural Similarity (SSIM) (Zhou Wang et al.\n2004)\nZhou Wang et al., “Image quality assess-\nment: from error visibility to structural\nsimilarity”\nlosses to ensure that they start from a reasonable location when we\nﬁne-tune with the more interesting loss functions2. We denote this loss\nfunction as\nL푅(푋푢, 푋푟) = ||푋푢−푋푟||1 −휆SSIM(푋푢, 푋푟)\n(9.10)\nfor restored image 푋푟and uncompressed image (i.e., a version of 푋푟\nwhich was never compressed) 푋푢and 휆is a balancing hyperparameter.\n9.5 Loss Functions\n83\nDegraded Y-Channel\nQuantization\nMatrix\nBlockNet\nFrequencyNet\nBlockNet\nFusion\nOutput\nResidual\nDegraded\nY-Channel\nRestored\nCoefficients\nDegraded {Cb/Cr}-\nChannel\n stride-8 32 channel CFM\nPReLU\n stride-8 32 channel CFM\nPReLU\nRestored Y\nChannel\nY Channel Quantization\nMatrix\nRRDB (32 Channels)\nTransposed \n stride-2 32\nChannels\nPReLU\nChannelwise Concatenation\nRRDB (32 Channels)\nTransposed \n stride-8 32\nchannel CFM\nPReLU\nOutput Residual\nRestored\nCoefficients\nColor\nQuantization\nMatrix\nDegraded {Cb/Cr}-\nChannel\nFigure 9.5: Restoration Networks. Left: Y-Channel Network. Right: Color Channel Network. Note the skip connections around each of\nthe subnetworks in the Y-Channel Network which promotes gradient ﬂow to these early layers.\n stride-8 256 channel\nCFM\nPReLU\nRRDB, 256 Channels\nTransposed \n stride-8 32\nchannel CFM\nPReLU\nInput Feature Map\nQuantization Matrix\nOutput Feature Map\nInput Feature Map\nFCR\nConv \n 256 Channels, 64\nGroups\nPReLU\nRRDB, 256 Channels, 64\nGroups\nConv \n 256 Channels, 64\nGroups\nPReLU\nFCR\nOutput Feature Map\nBlockNet  Output\nFrequencyNet Output\nBlockNet  Output\nChannelwise Concatenation\nFCR\nConv \n 256 Channels, 64\nGroups\nPReLU\nConv \n 256 Channels, 64\nGroups\nPReLU\nConv \n 64 Channels, 64\nGroups\nFCR\nOutput Coefficient\nResidual\nFigure 9.6: Subnetworks. Left: BlockNet, Center: FrequencyNet, Right: Fusion.\nGoodfellow et al., “Generative adversar-\nial nets”\nJolicoeur-Martineau, “The relativistic dis-\ncriminator: a key element missing from\nstandard GAN”\nWith the color and grayscale networks trained for regression we now\nmove on to GAN (Goodfellow et al. 2014) and texture losses. GANs\nwere originally introduced purely for generating realistic images. The\nalgorithm pits a generator network against discriminator network where\nthe generator’s goal is to produce an image which is realistic enough to\nfool the discriminator and the discriminator’s goal is to discover which\nimages were generated by the generator. In this way, the two networks\nare adversaries in a game and by rewarding them for doing well, the\ngenerator can create more realistic images. For our purposes, we use a\nGAN to hallucinate plausible high frequency details, edges, and textures\nonto the compressed images.\nFor this we employ the relativistic average GAN loss (Jolicoeur-\nMartineau 2018). This loss function tweaks the original GAN deﬁnition to\nencourage the generator to produce images which appear “more realistic\nthan the average fake data” and is generally more stable than a vanilla\nGAN. For our purposes, we redeﬁne “fake” as the restored image 푋푟and\n84\n9 Quantization Guided JPEG Artifact Correction\nRadford et al., “Unsupervised represen-\ntation learning with deep convolutional\ngenerative adversarial networks”\nMiyato et al., “Spectral normalization for\ngenerative adversarial networks”\n stride-8 64 channel CFM\nLeakyReLU\nCompressed\nYCbCr Channel\nConv \n stride-8 196\nChannels\nLeakyReLU\nRestored or\nUncompressed YCbCr\nY and Cb/Cr\nQuantization\nChannelwise Concatenation\nConv \n stride-8 128\nChannels (spectral norm)\nLeakyReLU\nConv \n stride-8 128\nChannels (spectral norm)\nLeakyReLU\nConv \n stride-8 128\nChannels (spectral norm)\nLeakyReLU\nConv \n stride-8 128\nChannels (spectral norm)\nLeakyReLU\nOutput Per-Block\nDecisions\nFigure 9.7: GAN Discriminator. Note\nthat the discriminator makes decisions\nfor each JPEG block.\nJohnson et al., “Perceptual losses for real-\ntime style transfer and super-resolution”\nJia Deng et al., “Imagenet: A large-scale\nhierarchical image database”\nSimonyan and Zisserman, “Very deep\nconvolutional networks for large-scale\nimage recognition”\nBell et al., “Material recognition in\nthe wild with the materials in context\ndatabase”\nD. P. Kingma and Ba, “Adam: A method\nfor stochastic optimization”\nPaszke et al., “PyTorch: An Imperative\nStyle, High-Performance Deep Learning\nLibrary”\n“real” as the uncompressed image 푋푢. We then deﬁne the loss as\nLRA(푋푢, 푋푟) = log(퐿(푋푢)) −log(1 −퐿(푋푟)) (9.11)\n퐿(푥) =\n(\n휎(퐷(푥) −E푥푟∈Restored[퐷(푥푟)])\n푥is uncompressed\n휎(퐷(푥) −E푥푢∈Uncompressed[퐷(푥푢)])\n푥is restored\n(9.12)\nFor discriminator 퐷() and sigmoid 휎(). We base the discriminator 퐷() on\nDCGAN (Radford et al. 2015), its architecture is shown in Figure 9.7. All\nconvolutional layers use spectral normalization (Miyato et al. 2018). Note\nthat we provide both the compressed as well as the uncompressed/re-\nstored version of the image and discriminator decisions are made on a\nper-JPEG block basis.\nWhile the GAN is a useful tool for generating realistic corrections, the\ngeneral notion of real or fake only provides so much information. In\npractice, GAN losses for image-to-image translation are often coupled\nwith “perceptual losses” (Johnson et al. 2016). More speciﬁcally, these\nlosses use an ImageNet (Jia Deng et al. 2009) trained VGG network\n(Simonyan and Zisserman 2014). The intuition is that this auxiliary\nnetwork measures a semantic similarity between the input image and\nthe desired target since this network was trained for classiﬁcation. By\nencouraging semantic similarity, a more realistic result can be achieved\nsince the images appear to fall into the same class.\nWhile this is useful for general image-to-image translation we ﬁnd an\nalternative approach is more useful for compression. Since compression\ndestroys high frequency details, like textures, the more these details\ncan be recovered or suﬃciently hallucinated, the more realistic the\nreconstruction. Therefore, we use a VGG network trained on the MINC\n(Bell et al. 2015) dataset for material classiﬁcation. The main idea here\nis that if a restored and uncompressed image have similar logits for a\nmaterial classiﬁcation task, they would likely be classiﬁed as the same\nmaterial and therefore have realistic textures. We denote this loss function\nas\nLt(푋푢, 푋푟) = ||MINC5,3(푋푢) −MINC5,3(푋푟)||1\n(9.13)\nwhere MINC5,3 indicates layer 5 convolution 4 from the MINC trained\nVGG.\nThis yields the complete GAN loss\nLGAN(푋푢, 푋푟) = L푡(푋푢, 푋푟) + 훾LRA(푋푢, 푋푟) + 휈||푋푢−푋푟||1\n(9.14)\nfor balancing hyperparameters 훾, 휈. Note that the 푙1 loss makes another\nappearance here to prevent the GAN from diverging.\n9.6 Empirical Evaluation\nNo artifact correction work is complete without an empirical evaluation,\nand with the algorithm now developed, we are in a position to perform\none. For this evaluation we train the network using the Adam (D. P.\nKingma and Ba 2014) optimizer using a batch of 32 256 × 256 patches,\nthe network is implemented in PyTorch (Paszke et al. 2019). All DCT\n9.6 Empirical Evaluation\n85\nTable 9.1: QGAC Quantitative Results. Format is PSNR (dB)↑/ PSNR-B (dB) ↑/ SSIM ↑with the best result shown in bold. This table is\nprovided for those dedicated enough to read the small font. These numerical results are unimportant; what is important is the qualitative\nresults that follow.\nDataset\nQuality\nJPEG\nARCNN (Dong, Y. Deng, et al. 2015)\nMWCNN (P. Liu et al. 2018)\nIDCN (B. Zheng, Y. Chen, et al. 2019)\nDMCNN (Xiaoshuai Zhang et al. 2018)\nQGAC (Ours)\nLive-1\n10\n25.60 / 23.53 / 0.755\n26.66 / 26.54 / 0.792\n27.21 / 27.02 / 0.805\n27.62 / 27.32 / 0.816\n27.18 / 27.03 / 0.810\n27.65 / 27.40 / 0.819\n20\n27.96 / 25.77 / 0.837\n28.97 / 28.65 / 0.860\n29.54 / 29.23 / 0.873\n30.01 / 29.49 / 0.881\n29.45 / 29.08 / 0.874\n29.92 / 29.51 / 0.882\n30\n29.25 / 27.10 / 0.872\n30.29 / 29.97 / 0.891\n30.82 / 30.45 / 0.901\n-\n-\n31.21 / 30.71 / 0.908\nBSDS500\n10\n25.72 / 23.44 / 0.748\n26.83 / 26.65 / 0.783\n27.18 / 26.93 / 0.794\n27.61 / 27.22 / 0.805\n27.16 / 26.95 / 0.799\n27.69 / 27.36 / 0.810\n20\n28.01 / 25.57 / 0.833\n29.00 / 28.53 / 0.853\n29.45 / 28.96 / 0.866\n29.90 / 29.20 / 0.873\n29.35 / 28.84 / 0.866\n29.89 / 29.29 / 0.876\n30\n29.31 / 26.85 / 0.869\n30.31 / 29.85 / 0.887\n30.71 / 30.09 / 0.895\n-\n-\n31.15 / 30.37 / 0.903\nICB\n10\n29.31 / 28.07 / 0.749\n30.06 / 30.38 / 0.744\n30.76 / 31.21 / 0.779\n31.71 / 32.02 / 0.809\n30.85 / 31.31 / 0.796\n32.11 / 32.47 / 0.815\n20\n31.84 / 30.63 / 0.804\n32.24 / 32.53 / 0.778\n32.79 / 33.32 / 0.812\n33.99 / 34.37 / 0.838\n32.77 / 33.26 / 0.830\n34.23 / 34.67 / 0.845\n30\n33.02 / 31.87 / 0.830\n33.31 / 33.72 / 0.807\n34.11 / 34.69 / 0.845\n-\n-\n35.20 / 35.67 / 0.860\nIndependant JPEG Group, libjpeg\nLoshchilov and Hutter, “Sgdr: Stochastic\ngradient descent with warm restarts”\nAgustsson and Timofte, “Ntire 2017 chal-\nlenge on single image super-resolution:\nDataset and study”\nSheikh et al., “A statistical evaluation of\nrecent full reference image quality assess-\nment algorithms”\nFoi et al., “Pointwise shape-adaptive\nDCT for high-quality deblocking of com-\npressed color images “”\nRawzor, Image Compression Benchmark\nHeusel et al., “Gans trained by a two time-\nscale update rule converge to a local nash\nequilibrium”\nB. Zheng, Y. Chen, et al., “Implicit dual-\ndomain convolutional network for robust\ncolor image compression artifact reduc-\ntion”\ncoeﬃcients are normalized using per-channel and per-frequency mean\nand standard deviations. quantization matrices are normalized to [0, 1]\nand use the “baseline” setting in libjpeg (Independant JPEG Group\nn.d.).\nThe training proceeds in stages, as described previously. First the Y\nchannel network is trained using L푅(Equation 9.10) for 400,000 batches\nwith the learning rate starting at 10−3 and decaying by a factor of 2 every\n100,000 batches. We set 휆= 0.05. Then we freeze the Y channel weights\nand train the color network using L푅(Equation 9.10) for 100,000 batches\nwith the learning rate decaying from 10−3 to 10−6 using cosine annealing\n(Loshchilov and Hutter 2016).\nWith the network fully trained for regression we then ﬁne-tune end-to-\nend using LGAN (Equation 9.14). The network is again trained for 100,000\niterations using cosine annealing this time with the learning rate starting\nat 10−4 and ending at 10−6. We set 훾= 5 × 10−3 and 휈= 10−2.\nFor training data we use DIV2k and Flickr2k (Agustsson and Timofte\n2017) which contain 900 and 2650 images respectively. We pre-extract 30\n256 × 256 patches from each image and compress them using quality in\n[10, 100] in steps of 10 for a total training set size of 1,065,000 patches. We\nevaluate the method using the Live1 (Sheikh et al. 2006), Classic-5 (Foi\net al. n.d.), and ICB (Rawzor n.d.) datasets. To be consistent with prior\nworks, we report PSNR, PSNR-B, and SSIM as metrics for the regression\nnetwork. For the GAN we report FID score (Heusel et al. 2017).\n9.6.1 Comparison with Other Methods\nWe start by comparing our method to others in the form of a large boring\ntable in Table 9.1. Note that this uses the regression weights only. Note\nthat of the compared methods, only IDCN has native handling of color\ninformation and all of the methods are quality dependent with a diﬀerent\nmodel for each quality. Ours, in contrast, is only a single model.\n9.6.2 Generalization\nIn the development of this method we place emphasis on a single network\ngeneralizing to multiple JPEG qualities. This raises an interesting question:\n“can other models generalize to diﬀerent qualities?” In general the answer\nis “no” and we demonstrate this using IDCN (B. Zheng, Y. Chen, et al.\n2019) with an example image compressed at quality 50. Since IDCT\nprovides only quality 10 and 20 models, we test both of those models\non this image. The result is shown in Figure 9.8. The quality 10 model\n86\n9 Quantization Guided JPEG Artifact Correction\nOriginal\nJPEG (Q=50)\nIDCN (Q=10)\nIDCN (Q=20)\nQGAC\nFigure 9.8: Quality Generalization. Note that both the IDCN quality 10 and 20 models appear to oversmooth the quality 50 JPEG.\n0\n20\n40\n60\n80\n10\n20\n30\n40\n50\nEquivalent Quality\nInput Quality\n20\n22\n24\n26\n28\n30\n10\n20\n30\n40\n50\nSpace Saved (kB)\nInput Quality\nFigure 9.10: Equivalent Quality Plots.\nTop: space savings on average. Bottom:\nequivalent quality on average.\noversmoothes this image and it appears worse than the JPEG it was\nsupposed to correct. The quality 20 model looks better, but QGAC’s\nsingle model looks the best as it was able to adapt its weights to the\nquality 50 JPEG by processing the quantization data. As this experiment\nshows, it is important for prior works to select the correct model for the\nJPEG.\n 0\n 1\n 2\n 3\n10 20 30 40 50 60 70 80 90 100\nIncrease in PSNR (dB)\nQuality\nLive-1\n BSDS500\nICB\nFigure 9.9: Increase in PSNR. Shown\nfor color datasets on all JPEG quality\nsettings. Note the steep dropoﬀat high\nqualities.\nIn fact, since our method is not restricted in quality, we can show\nhow it generalizes in an even more compelling way: by testing on all\nJPEG quality settings. We show this in the graph in Figure 9.9. Note\nthat for most quality settings, the increase is fairly stable, only at quality\n90 and above does a steep dropoﬀoccur. For these qualities, however,\nthe degradation is hardly noticeable and artifact correction is likely not\nnecessary.\n9.6.3 Equivalent Quality\nOne important application of artifact correction is to improve compression\nﬁdelity. In other words, rather than replacing the entire JPEG codec with\na compression algorithm based on deep learning, we can simply use\nmore aggressive JPEG settings and use artifact correction to make the\nresult presentable. This is much more likely to succeed in the short term\ndue to the technical debt surrounding JPEG.\nWe explore this phenomenon using “equivalent quality”, i.e., given a\nJPEG which is compressed at some low quality and then corrected, what\nhigher quality would we have had to compress the image at in order to\nmatch the restored error? And how much space did we save by using the\nsmaller JPEG image? We start with an example in Figure 9.11.\nNote that our model is equivalent to almost doubling the quality of\nthe JPEG, allowing us to save a signiﬁcant amount of space. Next we\ncompute the equivalent quality over the entire Live1 dataset and plot it\nalong with the space savings in kB. We do this for qualities 10-50. These\nplots are shown in Figure 9.10.\n9.6.4 Exploring Convolutional Filter Manifolds\nOf the various proposals in this work, one of the most intriguing is the\nConvolutional Filter Manifold (CFM). In this section we explore their\nproperties empirically. First we can visualize the CFM weights. We do\nthis by pulling out three channels of one of the CFM layers. Since we can\n9.6 Empirical Evaluation\n87\nInput\nQuality 30\nEquivalent Quality JPEG\nQuality 58\nReconstruction\n46.8kB Saved (47.9%)\nFigure 9.11: Equivalent Quality Examples. Taking three images compressed at random qualities, we correct them and then ﬁnd the\ncompression quality that matches the corrected result in terms of error.\n 0\n 20\n 40\n 60\n 80\n 100\nFigure 9.12: Embeddings for Diﬀerent\nCFM Layers. 3 channels are taken from\neach embedding, color shows JPEG qual-\nity setting that produced the input quan-\ntization matrix. Circled points indicate\nquantization matrices that were seen dur-\ning training.\nGeoﬀrey E Hinton and Roweis, “Stochas-\ntic neighbor embedding”\nadapt these weights, we generate quantization matrices for qualities 10,\n50, and 100 and produce CFM weights. This is shown with a heatmap in\nFigure 9.13.\nWe see some interesting behavior in this ﬁgure. The kernels in each\nrow appear diﬀerent, since these are diﬀerent channels we hope that they\nare diﬀerent as they should capture diﬀerent information. In contrast,\nthe kernels in each column appear to be similar, but scaled versions with\nthe magnitude decreasing as quality increases. This makes sense because\nhigh quality images should need less correction.\nWe can take this visualization further by ﬁnding the ﬁlters which\nmaximally activate each of these weights. This will tell us which patterns\neach ﬁlter responds to. We do this by taking a noise image and optimizing\nit to maximize the output of the ﬁlter. In other words, we treat the\nvisualization as a parameter and use stochastic gradient ascent with the\nobjective being the magnitude of the weight we wish to visualize. This\nresult is shown in Figure 9.14.\nWe again see some interesting behavior. Clear patterns of JPEG artifacts\nare visible in these images. As in Figure 9.13, the columns seem to capture\ndiﬀerent types of artifacts, with the ﬁrst column capturing local block\nartifacts, the second capturing larger block artifacts, and the the third\ncapturing some ringing artifacts. As we descend each column, we see\nsimilar artifacts reducing in strength until the quality 100 ﬁlter which\nleaves the images mostly unchanged.\nFinally, we can examine the manifold property of the CFM. We show\nthis in Figure 9.12 where we have taken the three kernels from three\ndiﬀerent CFM layers for all possible quantization matrices (0-100). We\nthen compute a t-SNE embedding (Geoﬀrey E Hinton and Roweis 2002)\nto two dimensions and plot the kernels. What we see is a smooth manifold\nthrough the space of quantization matrices. By coloring each point by\nthe quality level used to generate it, we can see that the kernels generate\n88\n9 Quantization Guided JPEG Artifact Correction\nFigure 9.13: CFM Weight Visualization.\nHorizontal axis shows diﬀerent channels\nof the weight, vertical axis shows quality.\nQuality levels shown are Top: 10, Middle:\n50, Bottom: 100. These are simply the\nheatmapped 8 × 8 kernels of the CFM\nlayer.\nChannel\nQuality\nFigure 9.14: Images Which Maximally\nActivate CFM Weights. Horizontal axis\nshows diﬀerent channels from the\nweight, vertical axis shows quality. Qual-\nity levels shown are Top: 10, Middle: 50,\nBottom: 100.\nChannel\nQuality\n9.6 Empirical Evaluation\n89\n3: We use the deﬁnition of frequency 푚\nsuch that 푖+ 푗= 푚for a 2D frequency\n(푖, 푗) for simplicity.\nan order on the space. We can also see that each channel corresponds to\na diﬀerent manifold.\n9.6.5 Frequency Domain Results\nNext we analyze the constituent frequencies of compressed and restored\nimages. One of the claims we made when developing the method is that\nGAN training produced more realistic high frequency reconstructions.\nIndeed, by examining Figure 9.15 we can see that, compared with JPEG\nand regression reconstruction, the GAN result has signiﬁcantly more\nactivity in the high frequencies. We show this both with heatmaps\nof the Y channel coeﬃcients and by plotting the “probability” 3 with\nwhich a given frequency is non-zero on a bar chart, for four examples.\nExamining the frequency chart, we can see that in real images, even the\nhighest frequency components have some probability of being non-zero.\nThis probability is signiﬁcantly reduced by JPEG compression and the\nregression result does little to correct it. The GAN result on the other\nhand has high frequency responses that are signiﬁcantly higher, at least\nas likely to be non-zero as the original images. So in this sense at least,\nthe GAN loss was successful.\n9.6.6 Qualitative Results\nIn this section we simply show some qualitative outputs of the model,\nthese results are shown in Figure 9.16. Observe that the degraded images\nsuﬀer from extreme banding caused by the quantization process. Our\nreconstructions are able to eﬀectively mitigate this banding, along with\nother complex ringing and blocking artifacts. More qualitative results\nare given in Appendix B.\n90\n9 Quantization Guided JPEG Artifact Correction\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nProbability\nFrequency\nOriginal\nJPEG\nRegression\nGAN\nOriginal\nPlot\nDCT\nJPEG Q=10\nRegression\nGAN\nFigure 9.15: Frequency Domain Results. Note how the GAN reconstruction generates signiﬁcantly more high frequency content than\nthe regression reconstruction. Also note how few high frequencies are in the compressed image. We show only one example here, please\nsee more examples in Appendix B.\n9.6 Empirical Evaluation\n91\nJPEG Q=10\nReconstruction\nOriginal\nFigure 9.16: Qualitative Results. The compressed images at quality 10 are compared to our reconstructions and the originals.\n92\n9 Quantization Guided JPEG Artifact Correction\nKim, Soh, and Cho, “AGARNet: adap-\ntively gated JPEG compression artifacts\nremoval network for a wide range quality\nfactor”\nJiang et al., “Towards ﬂexible blind JPEG\nartifacts removal”\n9.7 Limitations and Future Directions\nAlthough this work represents a major step forward in the usability of\nJPEG artifact correction methods, there are still some major problems to\nbe solved. First and foremost is the double compression problem. Because\nQGAC parameterizes itself only on the quantization matrix of the ﬁle it is\ncorrecting, it has no way of knowing if this image was recompressed. For\nexample a real-life company which will not be named directly and with\na complex image processing pipeline decompresses and recompresses\neach JPEG it receives multiple times. Realizing that this would lead to\nsigniﬁcant degradation, this company recompresses its images at quality\n100, mitigating most quality loss. However, QGAC will treat this as a\nquality 100 JPEG and perform essentially no restoration on it: it knows no\nbetter. In eﬀect the image processing pipeline has lied to QGAC about the\nnature of the compression. This was partially addressed by AGARNET\n(Kim, Soh, and Cho 2020) which allows for a spatially varying “Q-map”,\nessentially per-pixel quality, to be used as an auxiliary input, however\ngenerating the Q-map is not straightforward.\nThen there is the related problem: the JPEG degraded image may\nnot be stored as a JPEG at all. It is fairly common to transcode JPEG\nﬁles to PNGs where they can be stored without further degradation.\nQGAC, of course, cannot operate on PNGs because they do not contain\nquantization information. This was addressed by FBCNN (Jiang et al.\n2021) which trains a network to predict quality level from pixels (along\nwith the restored output) thus implicitly parameterizing the network on\nquality and allowing it to take any kind of image as input.\nThere is the longstanding problem of high frequency reconstructions.\nIt seems that there are currently two paradigms in restoration: low-\nfrequency but accurate reconstructions and high frequency but inaccurate\nreconstructions (e.g. GAN reconstruction which looks nice but has little\nrelationship to the ground truth image). A “holy grail” of reconstruction\nwork would be allow accurate reconstructions in the high frequencies.\nThis is partially addressed later in the dissertation with the scale-space\nloss of Metabit.\nAnother important direction to consider for practical usage of artifact\ncorrection is runtime, memory, and hardware concerns. The end goal is\nto put these methods into the hands of users who may be on smartphones\nor laptops but little attention has been paid to this so far. Current tech-\nniques often require datacenter machines with powerful, often multiple,\nGPUs in order to run in a timely manner (or at all). More attention\nto practical, eﬃcient formulations and to quantized integer models or\nspecialized hardware is important to the widespread dissemination of\nthis technology.\nEhrlich, Davis, Lim, et al., “Analyzing and\nMitigating JPEG Compression Defects in\nDeep Learning”\nTask-Targeted Artifact\nCorrection 10\n10.1 Standard JPEG Compres-\nsion Mitigation Tech-\nniques . . . . . . . . . . . . . 93\n10.2 Artifact Correction for\nComputer Vision Tasks . . 94\n10.3 Eﬀect of JPEG Compres-\nsion on Computer Vision\nTasks . . . . . . . . . . . . . . 96\n10.4 Transferability and Multi-\nple Task Heads\n. . . . . . . 97\n10.5 Understanding Model\nErrors\n. . . . . . . . . . . . . 98\n10.6 Limitations and Future\nDirections . . . . . . . . . . . 99\nT\nhus far we have considered artifact correction as a tool for presenting\nattractive images to a user. In other words, where a compressed\nimage contains certain artifacts, we want to suppress those artifacts so\nthat the user can view something closer to the uncompressed image. We\nnoted that this was a diﬃcult task to accomplish for some time because\nartifact correction methods were trained on a “per-quality” basis with a\ndiﬀerent model for each quality and we proceeded to develop a method\nfor correction of JPEG artifacts that is “quality-blind”, i.e., only a single\nmodel is trained for all JPEG qualities.\nWe now consider a slightly diﬀerent question: what if the images are\nintended for machine consumption and not human consumption? How\ndoes this change the problem, if at all, and how do machine learning\nalgorithms respond to JPEG compression? In this contribution of the\ndissertation, we develop a ﬂexible method of overcoming the accuracy\nloss caused by JPEG compression on common computer vision models.\nThis includes both a study of how JPEG compression aﬀects these models\nand the examination of diﬀerent methods for mitigation of the accuracy\nloss. This method was published separately in the MELEX workshop\nin the proceedings of the International Conference on Computer Vision\n(Ehrlich, Davis, Lim, et al. 2021).\nThe method presented in this chapter trains an artifact correction\nnetwork to target a speciﬁc computer vision task. This has signiﬁcant\nadvantages over oﬀ-the-shelf techniques which we examine in Section\n10.4 (Transferability and Multiple Task Heads). Namely, the method is\ntransferable between models. In other words, once trained to assist a\nparticular model, it is general enough to assist other models. Similarly, it\ncan be trained to assist multiple tasks simultaneously without a signiﬁcant\npenalty on its eﬀectiveness. We call this method Task-Targeted Artifact\nCorrection (TTAC).\nFirst Principles\n▶JPEG degrades task performance. Leveraging explicit JPEG\ncorrection can mitigate the problem\n▶Supervise the JPEG correction method using diﬀerences be-\ntween the uncompressed and corrected images\n▶Task trained correction networks are generalizable to many\ndownstream tasks\n10.1 Standard JPEG Compression Mitigation\nTechniques\nBefore moving on we brieﬂy review other techniques which are commonly\nthought to mitigate JPEG artifacts.\n94\n10 Task-Targeted Artifact Correction\nS. Zheng et al., “Improving the robust-\nness of deep neural networks via stability\ntraining”\nArtifact\nCorrection\n(Trainable Weights )\nTask \n(Fixed Weights)\nJPEG\nError\nDegraded \nPrediction\nOriginal\nPrediction\nFigure 10.1: Task-Targeted Artifact Cor-\nrection. The logit diﬀerence from the\ntask network between clean and artifact-\ncorrected versions of the same image is\nused to train the artifact correction net-\nwork.\nSupervised Fine-Tuning/Data Augmentation\nThe simplest possible\nscheme, JPEG compressed inputs are mixed in during training as a form\nof data augmentation. The goal here is to train the network to expect\nJPEG compressed inputs and map them correctly. While this idea works,\noften well, it has several disadvantages. The ﬁrst is that it sacriﬁces\naccuracy on clean images. So the result of the network is no longer “at a\ntheoretical maximum” because it has, in some sense, expended capacity\nmodeling JPEG compressed inputs. Additionally, this method requires\nground-truth labels which can be expensive to obtain.\nOﬀ-the-Shelf Artifact Correction\nAnother exceedingly simple method:\nsimply apply an artifact correction network to JPEG compressed inputs.\nSince the artifact correction method is reducing error with respect to the\nclean image, intuition states that this should help the performance of a\ndownstream task (and indeed it does). Moreover, this technique could\nbe employed practically with the development of QGAC which does\nnot require knowledge of the JPEG quality. This technique also has the\nadvantage of not requiring any training at all and indeed keeping the\nclean accuracy intact is a selling point of the method. However, this is an\n“all-or-nothing” approach in that there is no way to tune it when it does\nnot work.\nStability Training\nThis technique (S. Zheng et al. 2016) is more inter-\nesting than the last two ideas and involves logit matching between the\nnetwork output on clean and perturbed (in this case JPEG compressed)\nimages. In this case, the stability loss is deﬁned as\nLstability(푥, 푥′) = || 푓(푥) −푓(푥′)||2\n(10.1)\nwhere 푓(푥) is some neural network and 푥′ is the perturbed version of 푥.\nThis objective is then minimized along with the primary task objective\nduring training. While this technique does encourage robustness and\nis self-supervised it inherits several drawbacks from the supervised\nmethod. The task network now needs to expend capacity to model the\ncompressed mapping and performance on clean images is sacriﬁced.\n10.2 Artifact Correction for Computer Vision\nTasks\nThe algorithm we propose in this chapter targets an artifact correction\nnetwork to a particular task. In all cases we will use QGAC from Chapter 9\n(Quantization Guided JPEG Artifact Correction) for the artifact correction\nnetwork. Starting from pre-trained weights, we ﬁne-tune the artifact\ncorrection network using logit error from the task network between clean\ninputs and compressed inputs.\nFormally, given a task 푡(), and artifact correction network 푞(), we\nminimize\nL휃(퐵) = ||푡(푏) −푡(푞(JPEG푞(퐵); 휃))||1\n(10.2)\n10.2 Artifact Correction for Computer Vision Tasks\n95\n10\n20\n30\n40\n50\nQuality\n0\n2\n4\n6\n8\n10\n12\n14\nAccuracy Loss (%)\nEfficientNet B3\nInceptionV3\nMobileNetV2\nResNet-101\nResNet-18\nResNet-50\nResNeXt-101\nResNeXt-50\nVGG-19\n10\n20\n30\n40\n50\nQuality\n0\n2\n4\n6\n8\n10\n12\n14\n16\nmAP Loss\nFasterRCNN\nFastRCNN\nMaskRCNN\nRetinaNet\n10\n20\n30\n40\n50\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nmIoU Loss\nHRNetV2 + C1\nMobileNetV2 (dilated) + C1 (ds)\nResNet101 + UPerNet\nResNet101 (dilated) + PPM\nResNet18 (dilated) + PPM\nResNet50 + UPerNet\nResNet50 (dilated) + PPM\nFigure 10.2: Performance Loss Due to JPEG Compression separated by task. Left: Classiﬁcation, Middle: Detection, Right: Segmentation.\nThe plots show all models from a single task with no mitigation applied. For segmentation tasks, the format of the model name is\nEncoder Model + Decoder Model and “ds” indicates that the model was trained with deep supervision. Note that methods which use a\nPyramid Pooling Module (PPM) decoder always use deep supervision.\n10\n20\n30\n40\n50\nQuality\n0\n2\n4\n6\n8\n10\n12\n14\nAccuracy Loss (%)\nEfficientNet B3\nInceptionV3\nMobileNetV2\nResNet-101\nResNet-18\nResNet-50\nResNeXt-101\nResNeXt-50\nVGG-19\n10\n20\n30\n40\n50\nQuality\n0\n2\n4\n6\n8\n10\n12\n14\n16\nmAP Loss\nFasterRCNN\nFastRCNN\nMaskRCNN\nRetinaNet\n10\n20\n30\n40\n50\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nmIoU Loss\nHRNetV2 + C1\nMobileNetV2 (dilated) + C1 (ds)\nResNet101 + UPerNet\nResNet101 (dilated) + PPM\nResNet18 (dilated) + PPM\nResNet50 + UPerNet\nResNet50 (dilated) + PPM\nFigure 10.3: Performance Loss with Mitigations ••• Circle: No Mitigation, +++ Cross: Oﬀ-the-Shelf Artifact Correction, ♦Diamond:\nTask-Targeted Artifact Correction, ■Square: Supervised Fine-Tuning. The models in this ﬁgure correspond to those shown in Figure 10.2.\nwhere JPEG푞denotes JPEG compression at quality 푞. Note that the pa-\nrameters, 휃, that we optimize belong to 푞. The task network is unchanged\nduring this process. See Figure 10.1 for a visual depiction of this process.\nWhile the intuition behind this process is simple there are several\ndetails that need to be accounted for. First consider that we are not\ntraining the artifact correction network based on any decision by the\ntask network, e.g., classiﬁcation or detection. Instead, we are matching\nthe actual logit values. These are vectors of real numbers and are much\nﬁner grained than the actual decision which may be binary. In eﬀect\nwe are rewarding the artifact correction network for inducing the same\nperception of an input image in the task network. Note that since there\nis no hard decision required for training the method is entirely self-\nsupervised. Only the logit values, which are independent of any ground\ntruth, are considered during the training process.\nThis diﬀers from stability training in several key ways. First, it does not\nmodify the task network, so performance on clean images is unchanged\nand the task network is free to expend its entire capacity learning the\nrelationship between clean data and the output. Next, since the correction\ntask is given to an auxiliary network, this network can be reused for\nother tasks. As we examine in Section 10.4 (Transferability and Multiple\nTask Heads), this works surprisingly well allowing the artifact correction\nnetwork to be trained using a lightweight task and reused for more\ncomplex tasks. To summarize, task-targeted artifact correction takes the\nadvantages of all prior techniques with none of the disadvantages and\nadds in transferability as a bonus.\n96\n10 Task-Targeted Artifact Correction\n10\n20\n30\n40\n50\nQuality\n0\n1\n2\n3\n4\n5\nAccuracy Loss (%)\nFine-Tuned\nTask-Targeted Artifact Correction\nMobileNetV2 Transfer\nResNet-18 Transfer\n10\n20\n30\n40\n50\nQuality\n0\n1\n2\n3\n4\n5\n6\nmAP Loss\nFine-Tuned\nTask-Targeted Artifact Correction\nMobileNetV2 Transfer\nResNet-18 Transfer\n10\n20\n30\n40\n50\nQuality\n1\n2\n3\n4\n5\n6\n7\nmIoU Loss\nFine-Tuned\nTask-Targeted Artifact Correction\nMobileNetV2 Transfer\nResNet-18 Transfer\nFigure 10.4: Transfer Results. Left: ResNet 101 (classiﬁcation), Middle: Faster R-CNN (detection), Right: HRNetV2 encoder with C1\ndecoder (semantic segmentation). In all plots, we add an evaluation using artifact correction weights that were trained on ResNet-18 and\nMobileNetV2, our lightest weight models. Note that “Fine-Tuned” and “Task-Targeted Artifact Correction” methods are both trained\nusing their respective task network directly e.g. in (a) they use a ResNet 101. - - dashed lines indicate results shown in Section 10.3\n10\n20\n30\n40\n50\nQuality\n1\n0\n1\n2\n3\n4\n5\nAccuracy Loss (%)\nFine-Tuned\nTask-Targeted Artifact Correction\nMultihead (2 Model)\nMultihead (3 Model)\n10\n20\n30\n40\n50\nQuality\n0\n1\n2\n3\n4\n5\n6\nmAP Loss\nFine-Tuned\nTask-Targeted Artifact Correction\nMultihead (2 Model)\nMultihead (3 Model)\n10\n20\n30\n40\n50\nQuality\n1\n2\n3\n4\n5\n6\nmIoU Loss\nFine-Tuned\nTask-Targeted Artifact Correction\nMultihead (3 Model)\nFigure 10.5: Multiple Task Heads. Left: ResNet 50 (classiﬁcation), Middle: Faster R-CNN (detection), Right: HRNetV2 encoder with C1\ndecoder (semantic segmentation). In all plots, we add an evaluation using artifact correction weights that were trained using multiple\ntask networks. For the two task setup, we used ResNet-50 and FasterRCNN. For the three task setup, we used ResNet-50, FasterRCNN,\nand HRNetV2 + C1. Note that HRNetV2 + C1 has no two-task multihead model. - - dashed lines indicate results shown in Section 10.3.\n1: We only show [10, 50] in this section\nas these are the most interesting results.\nLoshchilov and Hutter, “Sgdr: Stochastic\ngradient descent with warm restarts”\n10.3 Eﬀect of JPEG Compression on Computer\nVision Tasks\nA legitimate question at this point is “how much does JPEG actually\naﬀect computer vision tasks?”. We can answer this with a study, the\nconclusions of which are summarized in this section. The full results are\nrelegated to Appendix A.\nFor this study, we compressed images using quality in [10, 90] in steps\nof 101 using the test sets of the respective models we are evaluating. The\ninput images are compressed, then restored, then they are transformed\naccording to the requirements of the target model (e.g., cropping to 224 ×\n224). We evaluate supervised ﬁne-tuning, oﬀ-the-shelf artifact correction,\nand task-targeted artifact correction, we do not evaluate stability training.\nFor methods requiring ﬁne-tuning, we train for 200 epochs varying the\nlearning rate from 10−3 to 10−6 using cosine annealing (Loshchilov and\nHutter 2016). We compare all mitigation methods to a baseline of “doing\nnothing”, i.e., accepting JPEG inputs with no modiﬁcation. We evaluate\nthe following tasks, datasets, and models:\nClassiﬁcation using ImageNet (Jia Deng et al. 2009) with MobileNetV2\n(Sandler et al. 2018), ResNet 18, 50, and 101 (He, Xiangyu Zhang,\net al. 2016), ResNeXt 50 and 101 (Xie et al. 2017), VGG-19 (Simonyan\nand Zisserman 2014), InceptionV3 (Szegedy, Vanhoucke, et al. 2016),\nand EﬃcientNet B3 (Tan and Le 2019)\nDetection using MS-COCO (Lin, Maire, et al. 2014) with Fast R-CNN\n(Girshick 2015), Faster R-CNN (Ren et al. 2016), and RetinaNet (Lin,\nGoyal, et al. 2017)\n10.4 Transferability and Multiple Task Heads\n97\nFigure 10.6: MaskRCNN TIDE Plots.\nFrom left to right the model was evalu-\nated at quality 10, 50, 100 with no mitiga-\ntions. Note that the bulk of the errors are\nmissed detections at low quality. As qual-\nity increases, more objects are detected\nbut they are not localized correctly.\nInstance Segmentation again using MS-COCO with Mask-RCNN (He,\nGkioxari, et al. 2017)\nSemantic Segmentation using ADE-20k (Zhou et al. 2016, 2017) with\nencoding models MobileNetV2 (Sandler et al. 2018), ResNet 18, 50,\n101 (He, Xiangyu Zhang, et al. 2016), and HRNet (K. Sun et al. 2019)\nand decoders C1 (H. Zhao et al. 2017), PSPNet (Zhou et al. 2017),\nand UPerNet (Xiao et al. 2018).\nIn Figure 10.2 we see the result of these models for varying JPEG quality.\nAll of the models face a steep penalty for the lowest quality settings\nwhich gradually abates as quality increases. This ﬁnding is intuitive and\nconﬁrms our need for JPEG mitigation techniques. We follow this up\nwith summary plots of the mitigation study in Figure 10.3. We can see\nsome interesting behavior here, mainly that the diﬀerent mitigations do\nnot behave the same on diﬀerent tasks. In particular, tasks that are very\nlocalization-heavy like segmentation do not beneﬁt from supervised ﬁne-\ntuning as much. These tasks are greatly aided by task-targeted artifact\ncorrection, however.\n10.4 Transferability and Multiple Task Heads\nOne of the most intriguing properties of task-targeted artifact correction\nis the potential for transferability. Since the original task network is not\nchanged in any way, and only the artifact correction network is ﬁne-tuned,\nthere is no reason that we cannot use the outputs of the artifact correction\nnetwork for other tasks entirely. This opens up a range of new potential\ndeployment scenarios. For example, a TTAC model could be targeted\nto MobileNetV2 (Sandler et al. 2018), which is fast and lightweight to\ntrain, and then used for a much heavier semantic segmentation network\nwhich would have been impossible to train without signiﬁcant compute\npower.\nOf course this only works if the TTAC models can generalize. We\nexamine this in Figure 10.4. For each plot, we take the supervised ﬁne-\ntuning and task-targeted artifact correction results from Figure 10.3.\nThese are shown in dashed lines. We compare this with a target-targeted\nartifact correction network which was trained with MobileNetV2 (green)\nand ResNet-18. As the plots show, the transfer works quite well even\nto diﬀerent tasks. In the right hand plot, for example, the new results\nare almost indistinguishable from the task-targeted network which was\nﬁne tuned for segmentation and performs better than ﬁne-tuning the\nsegmentation network itself.\n98\n10 Task-Targeted Artifact Correction\nJPEG Q=10\nTask-Targeted Artifact Correction\nGround Truth\nFigure 10.7: Mask R-CNN Qualitative Result. Input was compressed at quality 10. This compares the JPEG result to TTAC and the\nground truth.\nHe, Gkioxari, et al., “Mask r-cnn”\nBolya et al., “TIDE: A General Toolbox\nfor Identifying Object Detection Errors”\nIt is also worth noting that there is no reason a TTAC model needs to\nbe trained with only one downstream task target. We can use as many\ndownstream tasks as we have compute power for. We examine this in\nFigure 10.5. In these plots we have added results from a TTAC model with\n2 heads (classiﬁcation and detection) and a TTAC model with 3 heads\n(classiﬁcation, detection, and segmentation). Not only does this work\nperfectly well, but in many cases the additional model heads improved\nthe generalizability of the TTAC model, leading to improved results.\n10.5 Understanding Model Errors\nSo far we have looked at model error in a very aggregate view. In\nother words, we are looking at overall accuracy and how it changes\nwith increasing compression. In this section, using Mask R-CNN (He,\nGkioxari, et al. 2017) as a representative example, we examine the errors\nmade by the model in more detail.\nWe start by using TIDE (Bolya et al. 2020) to compute a breakdown of\nthe exact errors that the network is making with increasing compression.\nThese plots are shown in Figure 10.6 with no mitigations applied. The\ntrend here is interesting. For low quality, the bulk of the errors are\ncaused by missed detections. However, as the quality increases and the\nmissed detections decrease, the localization error actually increases as\nthe newly detected objects are not properly localized. This is sensible\nbecause it suggests that once enough information is present in the image\nfor objects to be identiﬁable, the “spread” induced by the missing high\nfrequency basis functions causes the exact boundaries of the objects to\nbe obscured.\nWe can view this qualitatively as well. Figure 10.7 shows the result for\na JPEG compressed at quality 10 both with and without TTAC (as well as\nthe ground truth). In the uncorrected model, we can observe a signiﬁcant\nnumber of missed detections as well as minor localization errors on\nthe orange, although overall the orange is localized quite well given the\nsigniﬁcant blocking artifacts present on the boundaries. The TTAC output\nis also informative. Not only does the image appear signiﬁcantly high\nquality (keep in mind it was the same as the left image before artifact\ncorrection) but there are also far fewer missed detections. What remains\nare some localization errors particularly on the bowl.\n10.6 Limitations and Future Directions\n99\nMobileNetV2\nResNet-18\nResNet-50\nResNet-101\nResNeXt-50\nResNeXt-101\nVGG-19\nInceptionV3\nEfficientNet B3\nFasterRCNN\nFastRCNN\nRetinaNet\nMaskRCNN\nHRNetV2 + C1\nMobileNetV2 + C1\nResNet18 + PPM\nResNet50 + UPerNet\nResNet50 + PPM\nResNet101 + UPerNet\nResNet101 + PPM\n0\n2\n4\n6\n8\n10\n12\n14\nThroughput (fps)\nSupervised Fine Tuning (Training)\nSupervised Fine Tuning (Validation)\nTask-Targeted Artifact Correction (Training)\nTask-Targeted Artifact Correction (Validation)\nFigure\n10.8:\nModel\nThroughput.\nThroughput comparing TTAC FPS for\ntraining and inference. TTAC incurs a\nnon-negligible throughput impact.\n10.6 Limitations and Future Directions\nThere are two major limiting factors to TTAC in its current incarnation:\nspeed and ﬁdelity. Since TTAC requires placing an artifact correction net-\nwork, speciﬁcally a QGAC network, before any task processing happens, it\ncan severely limit performance. As long as this happens at the datacenter\nlevel, the impact is likely minimal but it is still a legitimate concern as\nGPU resources are still highly valuable and are currently required for\nartifact correction networks. This could be addressed by more eﬃcient\nformulations for artifact correction or bespoke TTAC architectures that\nare intended to be lightweight.\nNext, although TTAC has some marked advantages over data augmen-\ntation techniques, it currently struggles to outperform them in all cases.\nWe expect that this can be addressed with deeper supervision on the\ntask networks (i.e., matching more than just the ﬁnal logits) however\nthis is currently an open problem. There may be deep changes required\nto the scheduling for generation of suitable training data that would be\nrequired to see a clear numerical advantage.\nFinally, the scope of TTAC is still somewhat restricted. Although\nJPEG artifacts are arguably the most important and prevalent type of\ndegradation applied to images, we believe that TTAC is an invaluable\ntool for general degradations. This could mean corruptions like noise,\nmasking, rotations, etc. or something more mundane like resampling.\nVideo Compression\nMarpe et al., “The H. 264/MPEG4 ad-\nvanced video coding standard and its\napplications”\nSullivan et al., “Overview of the high ef-\nﬁciency video coding (HEVC) standard”\nBitmovin, Video Developer Report 2019\nModeling Time Redundancy:\nMPEG 11\n11.1 Motion JPEG . . . . . . . . 104\n11.2 Motion Vectors and Error\nResiduals . . . . . . . . . . 105\n11.3 Slices and Quantization . 107\n11.4 Recap . . . . . . . . . . . . . 108\nH\naving discussed image compression at length, we now move on\nto video compression. When considering uncompressed images,\nwe modeled them as samples of a continuous 2D signal. We now allow\nthose samples to vary over time to create a sort of “ﬂip-book”. Light\nintensity is captured in discrete steps in space to create “frames”, and\nthen, multiple frames are captured in discrete steps in time to create the\nvideo. By sampling frames at a suﬃciently high frame rate, there is an\nillusion of smooth motion.\nNaturally this signiﬁcantly increases the size of the representation.\nSince each frame in the video is the size of a single image, videos increase\nin size quickly with increasing framerate and time. Since we classiﬁed\nimages as big enough to warrant compression, videos also certainly need\nto be compressed. In fact, timely transmission of even short videos would\nbe impossible without compression.\nIn this chapter we cover, at a high level, the ﬁrst principles of video\ncompression. There are many diﬀerent video “codecs”, or, diﬀerent\nalgorithms for compressing videos. Although most of the concepts we\ndiscuss here are applicable to all modern codecs in some form, when\nwe need speciﬁc details, we will defer to MPEG and speciﬁcally, the\nAVC standard (Marpe et al. 2006). Readers may be familiar with AVC by\nother names like H.264 or MPEG4 part 10. We standardize on the AVC\nterminology to easily diﬀerentiate between HEVC/H.265 (Sullivan et al.\n2012) and to align better with the naming of AOM codecs (like VP9, AV1,\netc.). We focus on AVC because it is widely used (Bitmovin 2019) and\nmany of its key ideas are used as the foundation for continuing codec\ndevelopment.\nAs we will see, the important insight which makes video compression\npossible is that we can exploit time redundancy in the signal and remove\ninformation across time. This is in addition to the spatial manipulations\nwe are to used in JPEG, and the eﬀect is synergistic. In other words,\nby exploiting temporal redundancy, we can remove additional spatial\ninformation which we would have needed to store if we only had a single\nimage.\nThe dependence on the temporal dimension will create the need for\nthree frames types:\nIntra Frames or I-frames which are frames that can be decoded with-\nout information from any other frame, i.e., there is no temporal\ndependency.\nPredicted Frames or P-frames are frames that requires at least one previ-\nous frame to decode We are said to predict the current frame based\non the previous frame and any hints stored with the current frame.\nBipredicted Frames or B-frames are frames that requires at least one\nprevious and future frame to decode. These frames are beyond the\nscope of this dissertation.\n104\n11 Modeling Time Redundancy: MPEG\nFigure 11.1: Motion JPEG Comparison. Left: Motion JPEG frame, Center: AVC frame, Right: Original frame. The motion JPEG video\nwas larger and poorer quality than the AVC frame, motivating compression in the time dimension as well as the spatial dimension.\n1: Although historically MPEG-1 was\ntechnically\nstandardized\nﬁrst,\ncore\nMPEG-1 technology was based on work\nfrom the JPEG (committee).\nThese frames together referred to as a Group of Pictures, i.e., an I-frame\nand its associated P- or B- frames is a group of pictures.\n11.1 Motion JPEG\nBefore we begin the discussion of “true” video codecs, it is worth\ndiscussing an obvious solution: Motion JPEG. Motion JPEG can be thought\nof as a successor to MPEG 1. The idea is incredibly simple: each frame is\ncompressed separately as a JPEG and stored in a ﬁle along with some kind\nof frame rate speciﬁcation. This information is all that is needed to decode\nand play the video. Note that although Motion JPEG enjoys widespread\nuse because of its simplicity, there is actually no standard which deﬁnes it\nand diﬀerent software libraries will have diﬀerent methods of specifying\nmetadata.\nAs a quick example of this in action, we can take a raw 240frame, 24fps,\n1080p video and try compressing with motion JPEG and with ﬀmpeg\ndefaults for AVC. The original video in this case is 746,496,000 bytes\n(1920 × 1080 = 2, 073, 600 bytes for luminance plane, 4:2:0 subsampling\ngives 2, 073, 600/4+2, 073, 600/4 = 1, 036, 800 bytes for the chrominance\nplanes so 3, 110, 400 bytes per frame times 240 frames = 746, 496, 000) or\nabout 747MB. Pretty large for 10 seconds of 1080p video.\nThe Motion JPEG ﬁle generated from this video is 12.7MB, an impres-\nsive 62x compression ratio. This can be though of as a naive “limit” on\nhow much compression is attainable with out considering the temporal\ndimension. The AVC ﬁle on the other hand is only 7.2MB, a 103x com-\npression ratio. This is not the end of the story, however, as the AVC ﬁle is\nalmost indistinguishable from the original frame, yet the Motion JPEG\nframes have signiﬁcant blocking artifacts from compression (Figure 11.1).\nThis example motivates our desire to study compression in the temporal\ndimension: the AVC frames are both high quality and smaller (in ﬁle size)\nthan the Motion JPEG frames.\n11.2 Motion Vectors and Error Residuals\n105\n  :  Current Frame\n  :  Reference Frame\nFigure 11.2: Motion Vector Grid. The\ngrid is deﬁned based on the frame that\nis currently being decoded. Each motion\nvector indicates where in the previous\nframe a block of pixel moved from. The\nnumbers on the grid cells indicate the\nmotion strength.\n2: Barring large cuts or scene changes\nFigure 11.3: Motion Vector Arrows.\nEach vector is positioned in the center\nof the block, the arrow points to the po-\nsition that block came from in the refer-\nence frame. Image credit: Big Buck Bunny\n(Goedegebure et al. 2008).\n3: reference frame, width, height, refer-\nence x, reference y, current x, current y,\nmotion strength, and ﬂags\n11.2 Motion Vectors and Error Residuals\nWe will “measure” temporal information by modeling motion between\nneighboring frames. After modeling the motion, we can warp and subtract\nthe frames giving a “residual”. The motion modeling is designed to be\ncompact and simple while still capturing some complex motions. Since at\na high enough framerate, inter-frame motion is small2, the frames should\nshare a signiﬁcant amount of information after accounting for motion.\nAny additional information is stored in the residual, which is generally\nlow entropy.\nWe call frames which are constructed from motion and residual\ninformation Predicted Frames (P-frames) since they must be predicted\nfrom a previous frame. We will call the frame which is being decoded\nthe current frame and the previously decoded frame the reference frame.\nMPEG standards also deﬁne a Bipredicted Frame (B-Frame) which is\npredicted from a previous and future frame. This takes advantage of\nthe presentation timestamp and decoding timestamp features of video\ncontainers to store frames out-of-order and with a complex dependency\ngraph. We will consider further discussion of B-frame and multi-reference\ndecoding to be beyond the scope of this dissertation.\nMotion information is stored in the form of motion vectors. These vectors\nare computed by breaking the current frame into a regular grid and,\nfor each grid cell, measuring where in the reference frame that grid cell\nmoved from. For some cells there may be no motion and for others there\nmay be large motions. For AVC, our example codec, the cells themselves\ncan be 8 × 8, 16 × 8, 8 × 16, or 16 × 16 pixels. The blocks are stored as nine\n16-bit integers3. So this operation alone turns, at worst, 192 byte pixel\nblocks into 144 byte motions.\nIn Figure 11.2, we show an example of the grid structure from a real\nvideo. Note that the grid is deﬁned from the current frame which is\nbroken into a clean regular grid. On the reference frame, these blocks\nmay overlap. There are some grid cells missing from the current frame,\nfor these blocks no motion was detected, so they are skipped. We can\nalso visualize the vectors themselves as in Figure 11.3. For each block in\nthe image, we draw a vector starting from the center of that block and\nterminating at the position in the reference frame that the block came\nfrom.\n106\n11 Modeling Time Redundancy: MPEG\nFigure 11.4: Motion Compensation and\nError Residuals. Left: motion compen-\nsated frame, Right: error residual. Note\nthe block artifacts in the motion com-\npensated frame. corrections to these are\nstored in the error residual along with\nsmall edges. Note that the error residual\nis mostly zeros and therefore much eas-\nier to compress. To produce the residual,\nwe subtracted the motion compensated\nframe (left) from the true current frame\n(Figure 11.2 right).\n4: The MPEG standards only deﬁne the\ndecoder. It is up to the encoder to pro-\nduce a standards-compliant bitstream\nhowever it wants.\nThe motion vectors are used in a process called motion compensation.\nThis process simply copies blocks of pixels from their position in the\nreference frame to their position in the current frame, pasting over any\ncontent that was there previously. The resulting motion compensated\nframe represents a coarse warping of the reference frame to match the\ncurrent frame. Of course there are still errors since the motion is only\ncomputed on blocks, so it is not usually a perfect representation of the\ncurrent frame. See the left side of Figure 11.4 for an example of this. The\nvectors are computed using a process called motion estimation. We do not\ncover this process here as it is not standardized4.\nIn order to correct errors in the motion compensated frame, the encoder\nstores an error residual. One thing that is immediately noticeable in Figure\n11.2 and Figure 11.3 is that not all blocks move. Indeed in both there are\nmany blocks which are stationary. This means that the pixels in those\nblocks are exactly the same in the reference and current frames and\ntherefore if we subtract the two frames those blocks will be ﬁlled with\nzeros. Motion compensation takes this a step further to try to match\nmoving objects as closely as possible as well as stationary regions; this\nincreases the likelihood of generating zero blocks. These zero blocks are\nextremely low entropy and aid in the compression process.\nTo compute this residual, we ﬁrst compute the motion compensated\ncurrent frame from the reference frame then subtract the true frame,\nyielding everything that was not accurately modeled by the motion\nestimation process. An example is shown on the right side of Figure 11.4.\nIn eﬀect we have told the decoder to reuse information it already had\nabout those blocks without needing to store them. Small errors are then\naccumulated on edges and in rapidly moving objects which make up\nthe bulk of the size of the compressed residual. Note also that the above\ndiscussion is appearance based. An object moving in the physical world\nmay very well generate blocks in the frames that do not appear to move,\nlike in the center regions of the parachute in Figure 11.2. This information\ncan still be freely used to ﬁll in the current frame even though it is not an\naccurate reﬂection of the real-life parachute.\nTo summarize: the encoder stores per-block motion. This motion is\nthen combined with a low entropy residual and a previous frame to\nproduce the current frame. This yields direct savings in that storing\nmotion information is more compact than storing pixels, and indirect\n11.3 Slices and Quantization\n107\nCBR (5.4Mbps)\nCQP (25)\nCRF (23)\n9.4MB (5.4Mbps)\n9.5MB (5.4Mbps)\n9.2MB (5.3Mbps)\nFigure 11.5: Rate Control Comparison. The three rate control methods are tested targeting the same ﬁle size. Note the diﬀerent artifacts\nproduced by each method despite similar ﬁle sizes. Also note that for this video, CQP 26 undershoots the target to produces 6.6MB ﬁle\nbut CQP 25 overshoots at 7.5MB. CBR and CRF are very close in ﬁle size.\nBackground Slice\nHigh Motion Slice\nLow Motion Slice\nFigure 11.6: Slicing Example. The image\nhas been broken up into three slices by\nthe encoder: a background region, a high\nmotion region, and a low motion region.\nsavings because the error residual is much more compressible than the\noriginal pixels.\n11.3 Slices and Quantization\nIn addition to motion modeling, the AVC standard makes some notable\nchanges to the way frames are structured. Also, similar to JPEG, AVC\nallows the use of quantization for rate control, although this is exposed\nto the user in several diﬀerent ways. Unlike the last section, these ideas\nare applicable to both P-frames and I-frames.\nThe biggest departure from JPEG is the concept of slices. A slice is a\nregion of the frame made up of a whole number of macroblocks (usually\n16 × 16 pixel blocks). Prediction (motion compensation) is only possible\nwithin a single slice. In the simplest case, the entire frame is one slice,\nbut the general idea allows the encoder to break up the image into more\nmeaningful groups. For example, the encoder might have two slices: a\nhigh detail slice with minimal quantization and a low detail slice with\nhigher quantization. It may have high and low motion slices. Even more\nintriguing is the concept of I- and P-slices. This means that a single frame\ncan contain intra information and predicted information rather than\nthe encoder making a blanket decision for the entire frame which may\nbe sub-optimal. For example, highly detailed regions with low motion\nmay be better stored as an I-slice but a high motion region may be more\neﬃciently stored as a P-slice. An example of this is shown in Figure 11.6.\nIn JPEG rate control was implemented by choosing a scalar quality in\n[0, 100] and mapping that scalar to a quantization matrix. For videos we\nhave several options; these are compared visually in Figure 11.5. The most\nsimilar is Constant Quantization Parameter (CQP). This is essentially the\n108\n11 Modeling Time Redundancy: MPEG\nsame idea but on a scale of [0, 51] with 51 being the worst quality, this\nnumber is used to derive a quantization matrix for the coeﬃcient blocks.\nThis type of rate control is not generally used because it is extremely\nsimplistic to apply the same amount of quantization to every frame\nand generally produces sub-optimal results in both size and perceptual\nquality.\nInstead the more common Constant Rate Factor (CRF) is used. This\nis also a number in [0, 51] with 0 being truly lossless encoding (no\nquantization) and 51 being the worst quality. This method is tuned to\nhold perceptual quality constant and takes into account inter-frame\nmotion and frame rate. Still objects will be less aggressively quantized\nand moving objects more so following a similar argument for removing\nhigh spatial frequencies: fast moving object are harder to perceive with\ndetail.\nThe ﬁnal rate control method, which is also quite common, is Constant\nBitrate (CBR). In CBR encoding, the only thing the encoder is trying to\noptimize is the bitrate of the video, it should be as close as possible to\na speciﬁed target without going over it. This is useful for maxing out\na connection with a known bandwidth where it is desirable to get the\nmaximum quality that the connection can support without dropping\nframes. However, there is no way for the encoder to know a priori how to\nperform CBR encoding so it will regularly over- or undershoot the target\nunless two pass encoding is used.\n11.4 Recap\nWe have now covered the high level ideas by which video codecs compress\ntemporal data. The encoder computes and stores coarse motion between\nframes and use that motion to compute an error residual containing\nanything in the frames which cannot be modeled with motion from a\nprevious frame. Internal to each frame, the encoder is free to slice the\nframe and make per-slice decisions. Rate control is accomplished with the\nhelp of a target bitrate, a user deﬁned CRF, or a user deﬁned quantization\nparameter.\nThe spatial domain compression is similar to JPEG. Pixels are trans-\nformed into the frequency domain using a 4 × 4 DCT, DST, or Hadamard\ntransform (the encoder is free to choose this) and, depending on the\nrate control mechanism, a QP is computed or is given to the encoder.\nThese QPs map directly to a standard set of quantization matrices for the\nblocks. Note that the QP is allowed to vary spatially so diﬀerent blocks\ncan receive diﬀerent amounts of compression, as opposed to JPEG where\none quantization matrix is used for every block in the image.\nThis yields a surprisingly straightforward compression algorithm.\nGiven a set of frames, partition the frames into GOPs. This is usually\na ﬁxed number of frames per GOP but it can be based on the content.\nEncode the ﬁrst frame of each GOP as an intra frame by quantizing the\ntransform coeﬃcients of its pixels. For each subsequent frame in the\nGOP, compute motion vectors from the previous frame, compute the\nmotion compensated frame, and take the diﬀerence between the current\nframe and the motion compensated frame. Encode the predicted frame\n11.4 Recap\n109\nInternational Telecommunication Union,\nAdvanced video coding for generic audiovi-\nsual services\n5: This is not the same as an MP4 ﬁle, for\nexample, but the MP4 ﬁle will contain\nAVC video streams.\nby storing the motion vectors and quantized coeﬃcients of the error\nresidual. Entropy code the frames. The decoder simply needs to loop\nthrough each frame and either decode it directly if it is an I-frame or\nwarp the currently displayed frame using the motion vectors and add\nthe error residual if it is a P-frame.\nUnsurprisingly, an actual video codec is much more complex than this\nand is full of small details which make a large diﬀerence to the overall\ncoding eﬃciency. As of this writing, the current revision of the AVC\nstandard (International Telecommunication Union 2021) was released on\nAugust 2021 and is 844 pages long. Aside from the core decompression al-\ngorithm it includes instructions for storing the resulting data in a stream5,\ndeﬁnitions of constants and other hard coded mappings, algorithms\nfor scalable streams, algorithms for compressing multi-view/depth/3D\nvideos, etc.. Although covering such details is beyond the scope of this\ndissertation, the high level intuition we developed in this chapter will\nbe enough to guide us in the following chapters as we explore ways to\nimprove video coding eﬃciency using deep learning.\nSajjadi et al., “Frame-Recurrent Video\nSuper-Resolution”\nImproving Video Compression 12\n12.1 Notable Methods for Gen-\neral Video Restoration . . 111\n12.2 Single Frame Methods . . 112\n12.3 Multi-Frame Methods . . 113\n12.4 Summary and Open\nProblems . . . . . . . . . . 114\nF\nollowing the same model as Chapter 8 (Improving JPEG Compres-\nsion), we now survey techniques for improving video compression\nusing deep learning. Compared with JPEG compression, this chapter\nwill seem quite short. Video methods are still somewhat of a novelty in\ndeep learning literature as of the time of writing, and the subﬁeld of\nvideo compression reduction is particularly nascent. One major diﬀerence\nbetween JPEG and video compression is the inclusion of the in-loop\ndeblocking ﬁlter. Although some JPEG software does use rudimentary\nheuristic deblocking, all modern video codecs include reasonably eﬀec-\ntive deblocking ﬁlters. However, these ﬁlters are not perfect and often\nthere are still visible artifacts, although this ﬁlter does complicate the\ntask for the network quite a bit.\nThe ﬁeld can be divided into two disjoint sets. The ﬁrst set of methods,\nsingle frame methods, consider only a single video frame at a time. They\nmay act diﬀerently on diﬀerent types of frames (e.g., I- or P-frames)\nbut they never consider information from any previous or future frame.\nMulti-frame methods in contrast do consider several frames. While\ngeneral restoration techniques have largely moved from sliding-window\nto recurrent methods thanks to the signiﬁcant eﬃciency improvements\nof the recurrent models, video compression reduction methods still use\nsliding windows.\nBefore reviewing these methods we will ﬁrst discuss four important\nworks in general video restoration. These are methods which are so\ninﬂuential that they have had an outsize eﬀect on the entire restoration\nﬁeld and thus should be considered critical background knowledge. We\nwill then proceed to study only video compression reduction methods.\nWarning\nGet ready for another, albeit shorter, history lesson\n12.1 Notable Methods for General Video\nRestoration\nMore general video restoration methods have a long history particularly\nin super-resolution. For our purposes we will only examine works of\noutsize importance such that they have highly inﬂuenced followup works\nin video compression reduction. These are all methods which combine\ninformation from multiple frames, a distinguishing feature of video\nrestoration vs image restoration.\nThe ﬁrst work we discuss is FRVSR (Sajjadi et al. 2018). FRVSR is notable\nfor its highly eﬃcient recurrent formulation. This came at a time when\nvideo restoration was either single frame or was using sliding windows.\nSliding windows should oﬀer better performance but are signiﬁcantly\n112\n12 Improving Video Compression\nXue et al., “Video Enhancement with\nTask-Oriented Flow”\nX. Wang, Chan, et al., “Edvr: Video\nrestoration with enhanced deformable\nconvolutional networks”\nLi\net\nal.,\n“COMISR:\nCompression-\nInformed Video Super-Resolution”\nT. Wang et al., “A novel deep learning-\nbased method of improving coding eﬃ-\nciency from the decoder-end for HEVC”\nDong, Loy, et al., “Learning a deep\nconvolutional network for image super-\nresolution”\nRen Yang, Xu, T. Liu, et al., “Enhancing\nquality for HEVC compressed videos”\nmore resource intensive to compute. The lightweight recurrent formu-\nlation was a signiﬁcant step towards the practical application of these\ntechniques.\nThe next year ToFlow (Xue et al. 2019) was published. A common idea\nin video restoration methods is that consecutive frames, or their features,\nmust be aligned in order to make proper use of the extra information,\nand this is commonly done with optical ﬂow. The key insight of ToFlow\nis that that optical ﬂow can be learned using a network which is trained\nas part of the restoration process, customizing the optical ﬂow to the task.\nAnother major contribution of this paper is the Vimeo90k dataset which\nis widely used in video restoration literature.\nIn the same year came the next major innovation in video enhance-\nment: EDVR (X. Wang, Chan, et al. 2019). The main contribution of\nEDVR was the replacement of explicit motion compensation with optical\nﬂow with implicit motion compensation using deformable convolutions.\nThe deformable convolutions allow the feature extraction network to\nautomatically capture information which is spatially oﬀset in frames to\naccount for motion. This should be a faster and more ﬂexible method\nthan optical ﬂow; however, the 20M parameter model and seven frame\nsliding window was highly impractical.\nFinally we discuss the COMISR (Li et al. 2021) method. COMISR is a\nrecurrent super-resolution method with a speciﬁc focus on compressed\nvideo. Li et al. rightly observe that real videos are compressed and yet\nprior restoration work does not take this into account. Their method is\ntrained and tested on compressed videos and includes a novel Laplacian\nloss which is designed to restore high frequency details.\n12.2 Single Frame Methods\nAs early as 2017, single frame methods were presented for video com-\npression reduction. These methods are quite simple and only account\nfor information in the frame that is currently being restored. Unlike the\ngeneral restoration methods presented in the previous section there is no\ndependence on additional information from prior or future frames.\nDCAD (T. Wang et al. 2017) proposed a simple method for restoring\nsingle frames of HEVC compressed video. The method bears a striking\nresemblance to ARCNN (Dong, Loy, et al. 2014). The method uses a stack\nof convolutional layers. The main point of comparison is the built in\ndeblocking ﬁlter which they show an improvement over.\nThe QE-CNN (Ren Yang, Xu, T. Liu, et al. 2018) method presented the\nnext year was designed to take compression into account explicitly. This\nis done using two networks, QE-CNN-I for I-frames and QE-CNN-P\nfor P-frames. Interestingly, for P-frames, the method applies both the -I\nnetwork and the -P network as HEVC encoding may contain intra- and\ninter- slices in one P-frame. Note that these networks still only consider a\nsingle frame at a time even though separate networks are used for the I-\nand P-frames; there is no shared hidden state or window.\n12.3 Multi-Frame Methods\n113\nTable 12.1: Summary of Video Compression Reduction Techniques. Methods are listed with their use of multiple frames and their\nmethod for motion compensation in publication order.\nYear\nName\nCitation\nMulti-frame\nMotion Compensation\nNote\n2017\nDCAD\nT. Wang et al. 2017\n×\n-\n2018\nQE-CNN\nRen Yang, Xu, T. Liu, et al. 2018\n×\n-\nSeparate I- and P-frames networks\nMFQE\nRen Yang, Xu, Zulin Wang, et al. 2018\n✓\nExplicit\nPQFs (SVM)\n2020\nSTDF\nJianing Deng et al. 2020\n✓\nImplicit\n2021\nMFQE 2.0\nXing et al. 2021\n✓\nExplicit\nPQFs (BiLSTM)\nPTSQE\nDing et al. 2021\n✓\nImplicit\n3D Convolution\nRFDA\nM. Zhao et al. 2021\n✓\nImplicit\nCross-window recurrent\nRen Yang, Xu, Zulin Wang, et al., “Multi-\nframe Quality Enhancement for Com-\npressed Video”\nXing et al., “MFQE 2.0: A New Approach\nfor Multi-frame Quality Enhancement\non Compressed Video”\nSchuster and Paliwal, “Bidirectional re-\ncurrent neural networks”\nJianing Deng et al., “Spatio-Temporal De-\nformable Convolution for Compressed\nVideo Quality Enhancement”\nDing et al., “Patch-Wise Spatial-Temporal\nQuality Enhancement for HEVC Com-\npressed Video”\nX. Wang, Yu, et al., “Esrgan: Enhanced\nsuper-resolution generative adversarial\nnetworks”\nM. Zhao et al., “Recursive Fusion and De-\nformable Spatiotemporal Attention for\nVideo Compression Artifact Reduction”\n12.3 Multi-Frame Methods\nIn 2018, MFQE (Ren Yang, Xu, Zulin Wang, et al. 2018), the ﬁrst multi-\nframe video compression reduction network, was developed. In addition\nto the seven frame sliding window with optical ﬂow alignment, this\nmethod introduced the concept of peak quality frames (PQFs). PQFs are\nframes which naturally have a higher quality than their surrounding\nframes and therefore have more information to extract. MQFE leverages\nthese frames by extracting feature from them separately and using those\nfeatures to guide the restoration of the nearby non-PQFs. They identify\nthe PQFs by manually labeling frames by PSNR and then training an\nSVM. At test time, the SVM identiﬁes PQFs, then the PQF features are\nextracted, and ﬁnally the entire sequence is restored in a sliding window.\nThe followup work, MFQE V2 (Xing et al. 2021) replaces the SVM with a\nBi-LSTM (Schuster and Paliwal 1997) which is more accurate. In addition\nto these ideas the MFQE paper also introduces the dataset which is used\nfor all follow-up works.\nSTDF (Jianing Deng et al. 2020) is the next major advancement. This\nmethod takes the key idea from EDVR - deformable convolutions - and\napplies it to compression artifact reduction. The network consists of oﬀset\nprediction, deformable feature extraction, and quality enhancement\nmodules.\nThe following year, PTSQE (Ding et al. 2021), a patch-based method,\nwas introduced. The key idea is to use separate networks for capturing\nspatial and temporal information of a single patch and to use attention\nmethods to fuse the two. PTSQE also takes the step of incorporating\nthe residual dense blocks from ESRGAN (X. Wang, Yu, et al. 2018). The\nimplicit motion compensation is also done using 3D convolutions instead\nof the traditional deformable convolution.\nFinally, RFDA (M. Zhao et al. 2021), another recent model, uses a\nrecursive fusion method to artiﬁcially increase the temporal window\nsize. The method is built on the STDF method, and uses STDF whole-\nsale as a subnetwork. From the STDF output, the method outputs a\nhidden state which is used in a downstream network to hold additional\ninformation from prior sliding windows. In this way, the STDF model\nis essentially accessing additional temporal information. This is almost\na recurrent method in its function although the STDF method is still\nsliding window.\n114\n12 Improving Video Compression\n12.4 Summary and Open Problems\nThe methods discussed in this section are summarized in Table 12.1. At\na high level, the summary is that multi-frame methods are preferred\nto single frame methods (because of their increased performance), and\nimplicit motion compensation is preferred to explicit because it is faster\nto compute.\nGiven these high level ideas, there are some outstanding problems\nwe can identify. First, it is interesting to note that the implicit motion\ncompensation performs as well if not better than explicit (optical ﬂow\nbased) motion compensation. This implies that ﬁne grained alignment\nmay not be completely necessary for enhancement, or at least for video\ncompression reduction. This ﬁnding was at least partially conﬁrmed by\nToFlow which showed that a task guided ﬂow is better than a “perfect”\nﬂow.\nOn a related note, although QE-CNN was aware of the underlying\nmetadata of frame type, there is otherwise a lack of use of bitstream\nmetadata. This is surprising since the bitstream information often contains\nuseful cues for how information was removed from the original frames.\nAdditionally, coarse motion information is present in the metadata which,\nas we already discussed, may be good enough for feature alignment. In\nthe next chapter, we will develop a video compression reduction method\nwhich addresses these issues.\nEhrlich, Barker, et al., “Leveraging Bit-\nstream Metadata for Fast and Accurate\nVideo Compression Correction”\nRen Yang, Xu, Zulin Wang, et al., “Multi-\nframe Quality Enhancement for Com-\npressed Video”\nJianing Deng et al., “Spatio-Temporal De-\nformable Convolution for Compressed\nVideo Quality Enhancement”\nXing et al., “MFQE 2.0: A New Approach\nfor Multi-frame Quality Enhancement\non Compressed Video”\nSchuster and Paliwal, “Bidirectional re-\ncurrent neural networks”\nXue et al., “Video Enhancement with\nTask-Oriented Flow”\nMetabit: Leveraging Bitstream\nMetadata 13\n13.1\nCapturing GOP Struc-\nture . . . . . . . . . . . . . 116\n13.2\nMotion Vector Align-\nment . . . . . . . . . . . . 117\n13.3\nNetwork Architecture . 118\n13.4\nLoss Functions . . . . . . 119\n13.5\nTowards a Better Bench-\nmark . . . . . . . . . . . . 121\n13.6\nEmpirical Evaluation . . 122\n13.6.1 Restoration Evaluation .\n123\n13.6.2 Compression Evaluation 124\n13.7\nLimitations and Future\nWork . . . . . . . . . . . . 125\nU\nntil this point we have reviewed the basic principles of video\ncompression including how to achieve compression over time by\nremoving redundant motion information. We have also reviewed several\nmethods for using deep learning to restore quality to compressed video\nframes. In order for video compression to function, i.e., in order to\nsuccessfully decompress a compressed bitstream, we need additional\ninformation beyond simply transform coeﬃcients. This information, such\nas QP values, GOP structure, and motion vectors among others, give a\nvery strong prior for how the encoder has compressed the video stream\nand what information has been removed that should be restored.\nWe now turn our attention to developing a deep learning method which\nexploits this data to improve its reconstruction. This contribution of the\ndissertation is currently under submission for separate publishing and is\navailable as a pre-print (Ehrlich, Barker, et al. 2022). If we closely examine\nthe direction of prior works, there are some whispers of this idea. For\nexample, MFQE (Ren Yang, Xu, Zulin Wang, et al. 2018) contributed the\nidea of “peak quality frames” which were high quality frames that could\nbe used to restore nearby (in time) low quality frames. STDF (Jianing\nDeng et al. 2020) does away with expensive motion compensation to rely\non deformable convolutions.\nHowever, both of these methods leave something to be desired, specif-\nically by relying on outside computation for what is already stored by\nthe encoder. While the concept of peak quality frames seems somewhat\nabstract, after all how can we predict the existence of such frames, their\nexistence is grounded in ﬁrst principles. These are I-frames. The encoder\ninserts them intentionally to create frames with high information content\nwhich improves the decoding ﬁdelity. Recall that MFQE 1.0 scans the\nentire sequence to determine peak quality frames using an SVM and\nMFQE 2.0 (Xing et al. 2021) does the same using a Bi-LSTM (Schuster and\nPaliwal 1997). These are computationally expensive algorithms which\nare essentially computing the I- and P-frame structure of the GOP, some-\nthing which we can readily extract from a bitstream with no additional\ncomputation.\nThe MFQE family of networks also rely on optical ﬂow to align nearby\nframes. While there are many methods for computing optical ﬂow, they\nall vary in their speed and accuracy, although perfectly accurate optical\nﬂow may not be necessary in the ﬁrst place (Xue et al. 2019). The major\ncontribution of STDF was to move away from explicit motion estimation\nby using deformable convolutions to learn an implicit motion estimation.\nThis is desirable because it reduces the computational burden of the\nalgorithm: the deformable convolutions model both motion and mapping\nsimultaneously. However, we can do better than both explicit and implicit\nmotion estimation; we can do no motion estimation at all. Of course we\nstill wish to align nearby frames and for this we can extract motion vector\nfrom the bitstream. This gives a coarse motion compensation which we\nshow is not only good enough for accurate reconstruction but, taken with\n116\n13 Metabit: Leveraging Bitstream Metadata\nGOP Representation\nGOP\n...\nI\nP\nP\n...\nChannelwise Concatenation\n...\nFigure 13.1: Capturing GOP Structure.\nThe GOP representation is computed\nfrom wide I-frame feature extractors and\nnarrow P-frame feature extractors.\nour other contributions, outperforms both MFQE and STDF as well as\ntheir later follow-up works.\nThe common theme among the contributions of our method is that we\nare removing things which were computed explicitly by prior algorithms\nand replacing them with things that are computed by the encoder and\nstored in the video. We view these computations as redundant. By\nreducing these redundant computations we are left with extra compute\ntime per frame that we can re-invest in additional model parameters\nleading to an improved result. We take the additional step of moving\naway from the sliding window paradigm, where a block of seven frames\nproduces a single frame output, and instead use a block based approach\nwhere all seven frames are produced in a single forward pass. The result\nof these eﬃciency improvements is a network which has almost twice the\nparameters of STDF and yet runs the same or faster than it depending on\ninput resolution. It also outperforms STDF by a wide margin for many\ncompression settings.\nFirst Principles\n▶Architecture captures GOP structure\n▶Explicit I- and P-frame representations based on expected infor-\nmation content\n▶Alignment using motion vectors\n▶High frequency restoration using targeted loss functions\n13.1 Capturing GOP Structure\nOne of the primary contributions of this work is the way in which\nour network takes into account GOP structure. Recall from Chapter 11\n(Modeling Time Redundancy: MPEG) that (in the MPEG standards)\nframes can be either I-,P-,or B-frames where I-frames are “intra frames”\nwhich can be reconstructed using only information in the frame itself,\nP-frames are “predicted frames” which require some previous frame\nto reconstruct, and B-frames are “bipredicted frames” which require\na previous and future frame to reconstruct. The goal of using these\ndiﬀerent frames types is to rely more on information which is stored\nin other frames that would be redundant to store again. These frames\nare organized into a group-of-pictures (GOP) which is an I-frame and\nits associated P-/B- frames. Without loss of generality we only consider\nP-frames in the following discussion.\nSince the predicted frames intentionally do not store information\nwhich is stored in previous frames, we can observe that they contain\nless information and, due to prediction errors, generally have lower\nperceptual quality than their associated I-frame. When other models\nprocess video frames in a sliding window, they do not take this into\naccount in any meaningful way and so the same network which processes\nI-frames is used to process P-frames.\nWe can view this as wasting compute resources. Since the bulk of\nthe information is stored in the I-frame, we can process that with a\nwide representation. We can then use a narrower, and therefore faster\n13.2 Motion Vector Alignment\n117\nAligning I-Frame to P-Frames\nAligning P-Frames to I-Frame\nApply Reversed\nFrame 3 Motion\nVectors\nApply Reversed\nFrame 2 Motion\nVectors\nAligned to I-Frame\nApply Frame 3 Motion\nVectors\nApply Frame 2 Motion\nVectors\nRestored I-Frame\nFigure 13.2: Motion Vector Alignment.\nP-frames are warped backwards to the I-\nframe during feature extract. The I-frame\nis warped forwards to align to the P-\nframes during frame generation.\nXue et al., “Video Enhancement with\nTask-Oriented Flow”\nX. Wang, Chan, et al., “Edvr: Video\nrestoration with enhanced deformable\nconvolutional networks”\nXing et al., “MFQE 2.0: A New Approach\nfor Multi-frame Quality Enhancement\non Compressed Video”\nRen Yang, Xu, Zulin Wang, et al., “Multi-\nframe Quality Enhancement for Com-\npressed Video”\nJianing Deng et al., “Spatio-Temporal De-\nformable Convolution for Compressed\nVideo Quality Enhancement”\nto compute, representation for the P-frame to extract the additional\ninformation the P-frame contains. This is shown in Figure 13.1. Note that\nit is important to match the depth of the extraction networks so that the\nreceptive ﬁelds are aligned. We view the resulting GOP representation\nas capturing the available information in the entire sequence and use it\nto reconstruct each frame in the GOP after warping.\nThis is a major gain in eﬃciency since the faster network is used\nfor most frames in each sequence. Further, we will expend signiﬁcant\nresources reconstructing the I-frame, which was already higher quality\nas it contains the most information in the frame. We will then use this\nrestored I-frame as a base to compute the restored P-frames again using\na lighter network. In other words, the GOP structure is encoded into our\nreconstruction algorithm in all stages.\n13.2 Motion Vector Alignment\nFigure 13.3: Motion Vectors vs Optical\nFlow. The motion vectors resemble a\ncoarse or downsampled version of the\noptical ﬂow. Optical ﬂow was computed\nwith RAFT (Teed and Jia Deng 2020)\nIn multiframe restoration problems, it is extremely common to align\nnearby frames, or features extracted from nearby frames, to ensure\nthat various scene details are overlapping (see ToFlow (Xue et al. 2019)\nand EDVR (X. Wang, Chan, et al. 2019) among others). Conceptually,\nthis should make the restoration task easier for the network since the\nadditional information of nearby frames is in the correct location, ready\nto be exploited for additional reconstruction accuracy. The removal video\ncompression defects is no diﬀerent, and as discussed in the opening to the\nchapter, this is generally accomplished explicitly with optical ﬂow as in\nMQFE and related networks (Xing et al. 2021; Ren Yang, Xu, Zulin Wang,\net al. 2018) with STDF using deformable convolutions for an implicit\nalignment (Jianing Deng et al. 2020).\nWhile it is useful to compute high quality alignments between frames,\nit may not be necessary (this is discussed at length in ToFlow). Assuming\nthat the constraint of high quality alignment can be relaxed, we have\na convenient tool at our disposal: motion vectors which are compared\n118\n13 Metabit: Leveraging Bitstream Metadata\nInputs\nI-Frame Generation\nNetwork\nI-Frame Representation Network\n10 LR Blocks\n...\n3x3x64\n10 LR Blocks\n...\n3x3x16\nI-Frame Representation\n(160 channels)\n10 LR Blocks\n3x3x64\n...\n1 Low Quality I-Frame Pixels\n6 Low Quality P-Frame Pixels\n6 P-Frame Motion Vectors\n1 Restored I-Frame\n...\nAlign to I-Frame\nP-Frame Generation\nNetwork\nP-Frame Representation \nHigh Quality I-Frame\n6 Warped High Quality I-Frames\n6 P-Frame Motion Vectors\n6 Low Quality P-Frame Pixels\nAlign to P-Frames\nP-Frame Representation\n (6 channels)\n...\n10 LR Blocks\n3x3x64\n6 Restored P-Frames\nFinal Outputs\n...\nFigure 13.4: MetaBit System Overview. I-Frames are shown in Blue and P-Frames are shown in Pink. Our network takes an input\n(Orange) in the form of a low-quality Group-of-Pictures and ﬁrst performs multi-frame correction on the I-Frame. The resulting\nhigh-quality I-Frame is used to guide correction of the low-quality P-Frames. The ﬁnal output of our network (Yellow) is the entire\nhigh-quality Group-of-Pictures.\n1: where “well” is measured in terms of\nreconstruction accuracy.\nX. Wang, Yu, et al., “Esrgan: Enhanced\nsuper-resolution generative adversarial\nnetworks”\nIoﬀe and Szegedy, “Batch normalization:\nAccelerating deep network training by\nreducing internal covariate shift”\nwith optical ﬂow in Figure 13.3. The motion vectors relate nearby frames\nat the block level; for most resolutions the blocks are ﬁne enough and\nthe motion accurate enough that warping frames using motion vectors\ninstead of optical ﬂow works well 1 The major advantage of using motion\nvectors is that they require no computation to produce since they are\nstored in the video bitstream. Compared with optical ﬂow, they require\nno more computation to apply.\nWe will use the motion vector to align each P-frame to the I-frames\nduring feature extract, and then to align the restored I-frame to each\nP-frame during frame generation. This is illustrated in Figure 13.2. Since\nmotion vector measures motion from the previous frame, we must reverse\nand warp each frame in sequence, e.g., frame 3 is warped by frame 2’s\nmotion vectors; the result is warped by frame 1’s motion vectors, etc..\nDuring frame generation we carry out the inverse process by warping the\nrestored I-frames by each of the P-frame’s motion vectors in sequence.\n13.3 Network Architecture\nLightweight Restoration Block\nTwo Conv, 3x3xN,\nLeakyReLU\nChannel Attention\nInput\nOutput\nFigure 13.5: LR Block. The Lightweight\nRestoration block modiﬁes the residual\nblock to follow recent best practices in\ndeep learning and image-to-image trans-\nlation.\nWe are now in a position to develop a complete network architecture\nusing the ideas in the previous two sections. Although we will develop\na concrete architecture in this section, the high level idea to leverage\nspeciﬁc bitstream metadata can actually be applied to many diﬀerent\narchitectures. First we need to develop a basic block to build the rest of\nour network with. We would like to base this on residual blocks (Section\n5.5 (Residual Networks)) which are known to be eﬀective at many tasks;\nhowever, residual blocks by themselves do not follow best practices for\nimage-to-image problems. Conversely, the RRDB layer ((X. Wang, Yu,\net al. 2018)) works well but is computationally ineﬃcient. For videos, we\nrequire something which is lightweight and eﬀective.\nWe make the following modiﬁcations (Figure 13.5) to the residual block\nwhich we call a lightweight restoration (LR) block. First, we remove batch\nnormalization (Ioﬀe and Szegedy 2015) which is known to perform poorly\n13.4 Loss Functions\n119\nX. Wang, Yu, et al., “Esrgan: Enhanced\nsuper-resolution generative adversarial\nnetworks”\nVaswani et al., “Attention is all you need”\nQ. Wang et al., “ECA-Net: eﬃcient chan-\nnel attention for deep convolutional neu-\nral networks, 2020 IEEE”\nCharbonnier et al., “Two deterministic\nhalf-quadratic regularization algorithms\nfor computed imaging”\nin image-to-image translation scenarios (X. Wang, Yu, et al. 2018). Next,\nwe replace the ReLU layers with LeakyReLU. Finally, we add channel\nattention (Vaswani et al. 2017; Q. Wang et al. 2020) following recent best\npractices in deep learning methodologies. To these residual blocks, we\nadd our accounting for GOP structure and our motion vector alignment\nblocks. An overview of the Metabit system is shown in Figure 13.4.\nThe network is divided into several stages. The network takes a 7-\nframe GOP with no B-frames as input. In the ﬁrst stage, the I- and P-\nframe representations are computed using separate feature extractors.\nAs discussed in Section 13.1 (Capturing GOP Structure), the I-frame\nrepresentation is 64 dimensional while the P-frame representation is 16\ndimensional. Each of the P-frames is warped using motion vectors as in\nSection 13.2 (Motion Vector Alignment) to align them to the I-frame. Given\na 7 frame GOP this gives a ﬁnal representation of 160 dimensions. This\nrepresentation is then used as input to the I-frame generation network\nwhich produces the high quality I-frame. This high quality I-frame is\nthen warped 6 times to generate 6 copies each aligned to the individual\nP-frames. Then, the aligned I-frame is concatenated with the low quality\nP-frames and the P-frame generation network generates the 6 high quality\nP-frames. This gives the ﬁnal output: the high quality GOP consisting of\n7 frames.\nNote that this is quite diﬀerent from the sliding window or even\nrecurrent approaches used in other video restoration work. In sliding\nwindow, a new representation would be computed for every frame\nconsisting of three preceding and three succeeding frames. In a recurrent\nformulation, an accumulated hidden state is used to condition the current\nframe on past frames. In contrast, the method we developed in this chapter\ncompacts information for a block of frames into a compact representation\nand then projects that information forward in time such that each frame\nhas some past and some future information. This mimics the process that\nthe video decoder performs when it decodes a bitstream. The information\nis discarded when a new I-frame is encountered.\n13.4 Loss Functions\nAs we discussed in Chapter 9 (Quantization Guided JPEG Artifact Cor-\nrection), restoration problems which are based purely on regression\nsuﬀer from blurring and lack low-frequency content. Video compres-\nsion reduction is no exception to this, and in fact, just like with JPEG\ncompression, video compression speciﬁcally removes high frequency\ncontent. Unlike other restoration problems, however, all prior work in\nvideo compression reduction uses either 푙2 loss or Charbonnier loss\n(Charbonnier et al. 1994), both of which are simple error penalties. We can\nintroduce more complex loss functions to overcome this. For “standard“\nregression, we will depend on the 푙1 loss as usual (for the uncompressed\nframes 푋푢and the reconstructed frames 푋푟)\nL1(푋푢, 푋푟) = ||푋푢−푋푟||1\n(13.1)\nIn Chapter 9 (Quantization Guided JPEG Artifact Correction), we were\nlacking a method for accurate high frequency reconstruction. We can\n120\n13 Metabit: Leveraging Bitstream Metadata\naddress this here, with partial success, by using a loss based on the\nDiﬀerence of Gaussians (DoG) scale space. The diﬀerence of Gaussians\nconstructs a scale space by convolving an image with Gaussian blur\nkernels of diﬀering standard deviations. These function as bandpass\nﬁlters which capture image content at diﬀerent frequency bands. The\nprocess is repeated on downsampled versions of the image to capture\ninformation at diﬀerent scales.\nWe can employ this as a loss function by separating out the diﬀerent\nfrequency bands at diﬀerent scales and computing their 푙1 error as\nseparate loss terms. This eﬀectively weights each frequency band in the\nsame way rather than the decreasing magnitude we see using an overall\n푙1 loss. As such, the network is rewarded for accurately reconstructing\nhigh frequencies.\nFor the uncompressed and reconstructed frames we compute four\ndiﬀerent scales:\n푆푢= {푋푢, 푋푢2, 푋푢4, 푋푢8}\n(13.2)\n푆푟= {푋푢, 푋푟2, 푋푟4, 푋푟8}\n(13.3)\nwhere each entry 푋푢푠or 푋푟푠is obtained by downsampling 푋푢by a factor\nof 푠. We then compute the diﬀerence of Gaussians by convolving with a\n5 × 5 2D Gaussian kernel:\n퐺(휎)푖푗=\n1\n2휋휎2 푒−푖2+푗2\n2휎2\n(13.4)\nfor kernel oﬀsets 푖, 푗(note that these range from [-2,2]). Then, for each\nscale 푠we compute the four ﬁltered images\n푋푢푠,휎= 퐺(휎) ★푋푢푠\n(13.5)\n푋푟푠,휎= 퐺(휎) ★푋푟푠\n(13.6)\nfor 휎∈{1.1, 2.2, 3.3, 4.4}. We then compute the diﬀerences between the\npairs\n푋푢푠,1 = 푋푢푠,2.2 −푋푢푠,1.1\n(13.7)\n푋푢푠,2 = 푋푢푠,3.3 −푋푢푠,2.2\n(13.8)\n푋푢푠,3 = 푋푢푠,4.4 −푋푢푠,3.3\n(13.9)\n푋푟푠,1 = 푋푟푠,2.2 −푋푟푠,1.1\n(13.10)\n푋푟푠,2 = 푋푟푠,3.3 −푋푟푠,2.2\n(13.11)\n푋푟푠,3 = 푋푟푠,4.4 −푋푟푠,3.3\n(13.12)\nto yield the per-scale frequency bands. Finally, we compute the loss\nLDoG(푋푢, 푋푟) =\nX\n푠∈{1,2,4,8}\n3\nX\n푏=1\n||푋푢푠,푏−푋푟푠,푏||1\n(13.13)\nAs in Chapter 9 (Quantization Guided JPEG Artifact Correction),\nhowever, we ﬁnd that even this enhanced regression loss is not suﬃcient\nto generate realistic reconstructions although it does help. Instead, we\nagain turn to the GAN and Texture losses. The texture loss, repeated\n13.5 Towards a Better Benchmark\n121\nBell et al., “Material recognition in\nthe wild with the materials in context\ndatabase”\nSimonyan and Zisserman, “Very deep\nconvolutional networks for large-scale\nimage recognition”\nArjovsky et al., “Wasserstein generative\nadversarial networks”\nRadford et al., “Unsupervised represen-\ntation learning with deep convolutional\ngenerative adversarial networks”\nMiyato et al., “Spectral normalization for\ngenerative adversarial networks”\nChu et al., “Learning temporal coher-\nence via self-supervision for GAN-based\nvideo generation”\n2: Note that the critic itself has a diﬀerent\nloss function which we do not show here,\nsee Arjovsky et al. 2017 for details.\nWasserstein GAN Critic \nRestored/Target Frames\nCompressed Frames\nConcatenate \n...\nFigure 13.6: Metabit Critic Architec-\nture. For the entire seven frame sequence,\nthe compressed and restored frames are\nconcatenated following (Chu et al. 2020).\nThe resulting 48 channel input is reduced\nto a single scalar using a series of spectral-\nnormed convolutions, batch norm, and\nReLU layers.\nRen Yang, Xu, Zulin Wang, et al., “Multi-\nframe Quality Enhancement for Com-\npressed Video”\nhere, is discussed at length in Section 9.5 (Loss Functions).\nL푡(푋푢, 푋푟) = ||MINC5,3(푋푢) −MINC5,3(푋푟)||1\n(13.14)\nwhere MINC5,3 denotes the output of layer 5 convolution 3 in a MINC\n(Bell et al. 2015) trained VGG (Simonyan and Zisserman 2014) network.\nFor the GAN loss we will use a Wassertein GAN formulation (Arjovsky\net al. 2017). In a Wassertein GAN, the discriminator network is replaced\nwith a critic which rates examples on a [-1, 1] scale with -1 being fake\nand 1 being real. This critic then makes a soft decision about the realness\nor fakeness of a sequence rather than a hard decision, which, along\nwith some gradient clipping, makes it more stable and less sensitive to\nhyperparameter choices.\nAs in Chapter 9 (Quantization Guided JPEG Artifact Correction) our\ncritic architecture is based on DCGAN (Radford et al. 2015) with spectral\nnormalized convolutions (Miyato et al. 2018) except that we modify\nthe critic procedure to introduce temporal consistency following the\nprocedure from TeCoGAN (Chu et al. 2020). The modiﬁcation is relatively\nsimple: we use the compressed and restored/uncompressed sequence\nas input with the frames stacked in the channel dimension. The means\nthe critic is now considering the entire sequence instead of individual\nframes and therefore is incentivised to produce similar reconstructions\nover the sequence. The architecture is shown in Figure 13.6. This yields\nthe GAN loss\nL푤(푋푢, 푋푟) = −푑(푋푢, 푋푟)\n(13.15)\nfor critic 푑() 2.\nThese are combined into two composite loss functions: one for regres-\nsion only results\nL푅(푋푢, 푋푟) = 휶[L1(푋푢, 푋푟) LDoG(푋푢, 푋푟)]푇\n(13.16)\nfor balancing hyperparameters 휶∈ℝ2 and a loss for qualitative results\nL퐺(푋푢, 푋푟) = 휷[L1(푋푢, 푋푟) LDoG(푋푢, 푋푟) L푊(푋푢, 푋푟) L푡(푋푢, 푋푟)]푇\n(13.17)\nfor balancing hyperparameters 휷∈ℝ4.\n13.5 Towards a Better Benchmark\nAll video compression reduction work is tested on one primary dataset:\nthe MFQE (Ren Yang, Xu, Zulin Wang, et al. 2018) dataset. This dataset\nconsists of a large training set of diverse sequences and an eighteen video\ntest set. The test set contains diverse real-world scenes and a variety\nof resolutions. The videos are stored in raw (YUV) format. Overall,\nthis dataset is satisfactory for the purposes of evaluating compression\nreduction.\nThe problem, however, comes in how the dataset is used. Prior works\nused only HEVC (H.265) compression with constant QP values in\n122\n13 Metabit: Leveraging Bitstream Metadata\nBitmovin, Video Developer Report 2019\n3: Although this has likely decreased\nsince 2019 it would not be by much.\nTomar, “Converting video formats with\nFFmpeg”\nRen Yang, Xu, Zulin Wang, et al., “Multi-\nframe Quality Enhancement for Com-\npressed Video”\nXue et al., “Video Enhancement with\nTask-Oriented Flow”\nPaszke et al., “PyTorch: An Imperative\nStyle, High-Performance Deep Learning\nLibrary”\nD. P. Kingma and Ba, “Adam: A method\nfor stochastic optimization”\nArjovsky et al., “Wasserstein generative\nadversarial networks”\nR. Zhang et al., “The unreasonable eﬀec-\ntiveness of deep features as a perceptual\nmetric”\n{27, 32, 37, 42} or some subset of these depending on the paper. While\nnumerical results should not generally be our primary concern when\nevaluating any proposed method, these compression settings do leave\nmuch to be desired. Firstly, HEVC compression incurs less degradation\nthan other commonly encountered codecs. Although HEVC would no\nlonger be considered “new”, it is also much less frequently used with\nalmost no browser support. Additionally, the constant QP compression\nmethod is simply not used in real videos and is mostly included as a\ndebugging tool. The degradations that it causes are much simpler to\nmodel than CRF or CBR which are used frequently in real videos. See\nSection 11.3 (Slices and Quantization) for a deeper discussion of these\nterms.\nInstead we propose to use AVC (H.264) compression for evaluation\nand we use CRF instead of CQP. This is a much better representation\nof real world video than the previous benchmark as AVC compression\naccounts for nearly 91% of internet videos as of 2019 (Bitmovin 2019) 3.\nWe choose CRF values in {25, 35, 40, 50} ranging from relatively little\ncompression at 25 (this is the default for ﬀmpeg (Tomar 2006)) to 50\nwhich is only one less than the maximum. To reiterate, our goal with this\nbenchmark is to ensure that compression reduction algorithms face tests\nwhich accurately represent videos in the real world. In the next section\nwe will see that MFQE fails to converge for any CRF setting as it produces\nsigniﬁcantly more complex degradations than CQP, thus justifying our\nconcern.\n13.6 Empirical Evaluation\nWith the method suﬃciently developed we can empirically evaluate its\nperformance. In all cases we train our network using the MFQE training\nsplit (Ren Yang, Xu, Zulin Wang, et al. 2018) which consists of 108 variable\nlength sequences. To this we add a randomly selected one third of the\nVimeo90k dataset (Xue et al. 2019) which is approximately 30,000 7-frame\nsequences. We randomly crop 256 × 256 patches (and 7 frames from the\nMFQE examples) and apply random vertical and horizontal ﬂipping\nduring training. For H.264 bechmarks we encode using one I-frame and\nsix P-frames with CRF encoding as discussed in the previous section. For\nH.265 benchmarks we comply with prior works and use CQP encoding.\nWe train a separate model for each QP or CRF setting. All evaluations\nare conducted on the MQFE test split of 18 variable length sequences.\nThe network is implemented in PyTorch (Paszke et al. 2019) and\noptimized using the Adam (D. P. Kingma and Ba 2014) optimizer for 200\nepochs with the learning rate set to 10−4. For quantitative experiments\nwe train using the regression loss (Equation 13.16) with 훼= [1.0 1.0].\nFor qualitative results we ﬁne tune using the GAN loss (Equation 13.17)\nfor an additional 200 epochs with a learning rate of 10−5 and 훽=\n[0.01 0.01 0.005 1]. As recommended we use the RMSProp optimizer for\nWassertein GAN training (Arjovsky et al. 2017).\nFor numerical results we report the change in PSNR and the change in\nLPIPS (R. Zhang et al. 2018). For compared works we only compute LPIPS\nif there is a published model or training code. We ﬁnd no usable trend in\n13.6 Empirical Evaluation\n123\nMethod\nHEVC CQP\n27\n32\n37\n42\nMFQE 2.0 (Xing et al. 2021)\n0.49 / -\n0.52 / -\n0.56 / -\n0.59 / -\nPSTQE (Ding et al. 2021)\n0.63 / -\n0.67 / -\n0.69 / -\n0.69 / -\nSTDF-R3 (Jianing Deng et al. 2020)\n0.72 / 0.025\n0.86 / 0.027\n0.83 / 0.033\n- / -\nRFDA (M. Zhao et al. 2021)\n0.82 / -\n0.87 / -\n0.91 / -\n0.82 / -\nMetaBit (Ehrlich, Barker, et al. 2022)\n1.17 / 0.025\n0.99 / 0.023\n0.91 / 0.029\n0.82 / -\nTable 13.1: Metabit HEVC Results. Re-\nported as ΔPSNR (dB) ↑/ ΔLPIPS ↓.\nMethod\nAVC CRF\n25\n35\n40\n50\nSTDF-R1 (Jianing Deng et al. 2020)\n0.741 / 0.034\n0.862 / 0.032\n0.814 / 0.030\n0.632 / 0.013\nSTDF-R3 (Jianing Deng et al. 2020)\n0.784 / 0.035\n0.846 / 0.032\n0.882 / 0.029\n0.817 / 0.011\nMetaBit (Ehrlich, Barker, et al. 2022)\n1.085 / 0.024\n1.137 / 0.014\n1.113 / 0.005\n0.887 / -0.016\nTable 13.2: Metabit AVC Results. Re-\nported as ΔPSNR (dB) ↑/ ΔLPIPS ↓.\nZhou Wang et al., “Image quality assess-\nment: from error visibility to structural\nsimilarity”\nHeusel et al., “Gans trained by a two time-\nscale update rule converge to a local nash\nequilibrium”\nDing et al., “Patch-Wise Spatial-Temporal\nQuality Enhancement for HEVC Com-\npressed Video”\nM. Zhao et al., “Recursive Fusion and De-\nformable Spatiotemporal Attention for\nVideo Compression Artifact Reduction”\nTable 13.3: Metabit GAN Numerical\nResults. Reported as FID ↓/ LPIPS ↓\nMethod\nH.264 CRF\n40\n50\nAVC\n67.07 / 0.259\n152.19 / 0.498\nRegression\n80.67 / 0.265\n154.42 / 0.482\nGAN\n37.78 / 0.191\n95.26 / 0.368\n0\n1\n2\nParameters (M)\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nFPS\nMQFE 1.0\nMQFE 2.0\nSTDF-R1\nSTDF-R3L\nMetabit\nFigure 13.7: FPS vs Params. The size\nof each point indicates the increase in\nPSNR.\nGoedegebure et al., Big Buck Bunny\nSSIM (Zhou Wang et al. 2004) so we do not report that although some\nother works do. For consistency with prior work we report metrics on\nthe Y channel only although we would like to see this practice end soon.\nTo evaluate the GAN, besides providing qualitative results we report FID\n(Heusel et al. 2017) and LPIPS.\n13.6.1 Restoration Evaluation\nWe start with the boring numerical results in Table 13.1 for HEVC and\nTable 13.2 for AVC. We can see that the Metabit architecture we developed\nhas a signiﬁcantly better reconstruction result in most cases. The only\nexceptions are in the high QPs for HEVC where our method ties the next\nmost recent work. For AVC, the results are signiﬁcantly better indicating\nthat the more complex CRF degradation is handled well by our method,\nin other words, the extra parameterization is able to better model complex\ndegradations. Note that MFQE failed to converge entirely for CRF training.\nNote that PTSQE (Ding et al. 2021) does not provide public code and\nRFDA (M. Zhao et al. 2021) does not provide usable training code so we\nwere unable to fully evaluate these methods.\nTo evaluate the GAN we report FID and LPIPS in Table 13.3. Note\nthat this compares the degraded AVC input with our model trained for\nregression vs GAN on extreme CRF settings (40, 50), we do not bother\ncomparing to other works. As expected, the GAN generates signiﬁcantly\nmore realistic results.\nA more important numerical result for our purposes is throughput,\nspeciﬁcally when compared with the number of model parameters. Our\nformulation is designed to be highly eﬃcient while permitting a large\nnumber of parameters, traits which are important for timely and accurate\nvideo restoration. This result is shown graphically in Figure 13.7. We see\nthat our method is as fast as STDF with about double the parameters\nand a higher PSNR. In other words, our method is able to better utilize\nthe extra parameters without slowing down thanks to the eﬃciency\nimprovements we made by leveraging the bitstream metadata.\nWith the numerical results out of the way we can look at how the\nrestoration functions on real images, with an example shown in Figure\n13.10 from the short ﬁlm Big Buck Bunny (Goedegebure et al. 2008). In this\nexample we can see that despite the heavy compression (ACV CRF 40),\nthe multiframe GAN restoration is able to produce a striking resemblance\n124\n13 Metabit: Leveraging Bitstream Metadata\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nRate (bpp)\n33\n34\n35\n36\n37\n38\nDistortion (dB)\nAVC\nWu et al. (2018)\nDVC (2019)\nSTAT-SSF-SP (2020)\nHLVC (2020)\nScale-Space (2020)\nLiu et al. (2020)\nNeRV (2021)\nAVC + MetaBit (Ours)\nFigure 13.8: Rate-Distortion Compari-\nson. Using Metabit with AVC compres-\nsion performs better for low bitrates than\nfully deep learning codecs. Distortion is\nmeasured using PSNR.\nWu et al.\nDVC\nLiu et al.\nNeRV\nMetabit + AVC\n0\n10\n20\n30\n40\n50\nFPS\nFigure\n13.9:\nLearned\nCompression\nThroughput\nComparison.\nOrange\nshows\nencoding\ntime,\nblue\nshows\ndecoding time. Current fully deep\nlearning based codecs are quite slow.\nOne major advantage of using video\nrestoration is that encoding is quite fast.\nDecoding is on par with other methods\nexcept NeRV, although NeRV has a\nmuch slower encoding time.\nMercat et al., “UVG dataset: 50/120fps\n4K sequences for video codec analysis\nand development”\nto the original image. Textures are accurately reconstructed on the grass\nand tree and yet the smooth sky region is preserved, i.e., the network\nhas not hallucinated textures where thy should not be. This is even more\nremarkable considering that there was no artiﬁcial training data.\nTo compare the eﬀect of GAN restoration we show crops in Figure 13.11.\nThe diﬀerence here is quite pronounced. Although STDF and Metabit\nusing regression are able to improve the visual quality of the images,\nthe GAN restoration is signiﬁcantly more realistic in terms of overall\nsharpness and texture. This is particularly noticeable on the trees in the\ntop row.\n13.6.2 Compression Evaluation\nAs in Chapter 9 (Quantization Guided JPEG Artifact Correction) one of\nthe principal applications of restoration is as a method to better compress\nmedia. For our case, we can make a direct comparison to other fully deep\nlearning based compression codecs where we compare quite favorably\nboth in terms of latency and rate-distoration.\nThe rate-distortion result is shown in Figure 13.8 along with with a\nnumber of recent deep learning based compression algorithms. We use\nthe UVG dataset (Mercat et al. 2020) for this task, which is a widely used\ndataset for compression evaluation. We compare to Wu et al. 2018, Lu\net al. 2019, J. Liu et al. 2020, Hao Chen et al. 2021, Ruihan Yang et al. 2020,\nRen Yang, Mentzer, et al. 2020, and Agustsson, Minnen, et al. 2020. Note\nthat we use a model trained for regression only in this case.\nOne potential problem we encounter here is that deep learning based\ncompression literature compares with “low-delay P” mode compression.\nThis is a compression setting which saves additional space by using\nonly a single I-frame for the entire sequence with the rest of the frames\nencoded as P-frames. This setting saves additional space over placing\nI-frame periodically at the cost of quality and is often used in streaming\nscenarios where low-latency is critical. In order to compare fairly, we\nmodify our restoration procedure slightly to accommodate this. Since\nthe ﬁrst group of seven frames always includes an I-frame, this group is\nrestored following our standard procedure. We then cache the restored\nseventh P-frame and instead of reading seven more frames we read\nsix. These six frames are all P-frames and we use the cached restored\nP-frame in place of an I-frame. There is no retraining of the network\ninvolved in this process so the results are likely lower than they could be\nif we trained for this scenario; however this likely improves cross-block\ntemporal consistency by reusing information from the previous block.\nFor low bitrates (i.e., the bitrates which matter), simply using AVC\ncompression with Metabit restoration outperforms deep learning codecs.\nThis is to be expected since Metabit is an objective improvement on\nAVC which many deep learning methods still struggle to beat in general\ncases. The advantage of this comes in its ease of use (almost all modern\nhardware can decode an AVC compressed video) and its speed. Encoding\ntimes are in the hundreds of frames-per-second on commodity CPU\nhardware. Decoding time even including Metabit is also faster.\n13.7 Limitations and Future Work\n125\nAVC CRF 40\nReconstruction\nOriginal\nFigure 13.10: Metabit Restoration Example. Crop from 1920 × 1080 ﬁlm “Big Buck Bunny”. Left: compressed, Middle: restored, Right:\nOriginal. This artiﬁcial scene is restored accurately despite a lack of artiﬁcial training data. Note the grass and tree textures, sharp edges,\nremoval of blocking on the ﬂower, and preservation of the smooth sky region. The video was compressed using AVC CRF 40.\nIn Figure 13.9 we compare the throughput of a subset of these methods\nto Metabit with AVC compression. Metabit is on par with other methods\nhere, and actually outperforms all but NeRV. The major disadvantage of\nNeRV, however, is the extremely long encoding time. Since NeRV is an\nimplicit representation a new network is learned for each video taking\non the order of hours to encode one video. We simply skip encoding time\nfor NeRV in the plot.\nThe major takeaway from this discussion is that video compression\nreduction, speciﬁcally the Metabit method we developed in this chapter,\nis eﬃcient and eﬀective at generating accurately restored video frames\nfrom low bitrate video. Since it only depends on commodity codecs,\nour method can be encoded quickly and easily and decoded by anyone\nwithout special hardware. Those with special hardware can use our\nmethod to achieve more visually pleasing results. This method provides\na promising avenue for deploying deep learning in compression in the\nnear term.\n13.7 Limitations and Future Work\nThe work as presented in this chapter has a few limitation all of which\nwe believe are solvable with relatively minor changes. The network\narchitecture currently depends on a ﬁxed GOP size of seven frames.\nThis would likely not work with real videos which may use a variable\nlength GOP for several reasons. This is readily solvable by projecting a\nvariable size GOP representation to a ﬁxed size one using an adaptive\npooling layer following by a projection. It remains to be investigated if\nthis mapping is as eﬀective as using a ﬁxed GOP.\n126\n13 Metabit: Leveraging Bitstream Metadata\nCompressed (CRF 40)\nSTDF\nMetaBit (Regression)\nMetaBit (GAN)\nTarget\nFigure 13.11: Metabit Comparison. Crops from “Big Buck Bunny“” and “Traﬃc” from the MFQE dataset. We compare our GAN\nrestoration to regression restoration and the STDF method.\nChu et al., “Learning temporal coher-\nence via self-supervision for GAN-based\nvideo generation”\nSimilarly, since each GOP is treated as a separate block, and restored\nseparately, there are temporal consistency issues across GOP blocks. Note\nthat within a single GOP the frames are quite consistent thanks to the\nTeCoGAN (Chu et al. 2020) formulation which we use. It is only across\nGOP boundaries that consistency issues arise. This is noticeable in the\nrestored output as a sort of ﬂickering or noise pattern that appears. This\nis likely solvable by keeping a compact hidden state from the previous\nGOP to compose with the current GOP processing.\nContinuing in this line of thought, there are issues which arise in a\nstreaming scenario which make an architecture like this wholly unsuitable.\nIn streaming scenarios, the video is often encoded using “low-delay P\nmode”. In this mode, there is a single I-frame at the beginning of the GOP\nfollowing by only P-frames, in other words, there is only a single GOP\nin the entire video. For this we actually have a partial solution which is\nto simply use the previously restored ﬁnal frame (restored frame 7) in\nplace of the I-frame for the next GOP, skipping the I-frame restoration\nstep entirely. This does lead to a loss of visual quality especially if the\nnetwork was not trained to perform this kind of restoration.\nAnother issue in streaming situations is latency and buﬀering. For\nreal-time applications this currently precludes restoration technology,\nbut in our case in particular the 7 frame GOP needs to be buﬀered before\nrestoration can occur. This may be a limiting factor in some scenarios.\nThe only way around this is to reduce the number of buﬀered frames or\nmove to a fully recurrent solution.\nFinally, this method suﬀers from the same “quality aware” problem\nthat was prevalent in JPEG artifact correction. For each CRF setting we\nhave to train a diﬀerent model. Unlike JPEG, however, CRF, and more\nimportantly the derived QP values, are stored in the video ﬁle. In this\nway it should be easy to create a parameterized network which is aware\nof the QPs that each frame was compressed with.\nConcluding Remarks\n129\nY\nou have made it to the end of the dissertation, a journey developing the ﬁrst principles of deep learning\nand classical compression from the preliminaries through to the published research of the author. With\nthe body of the dissertation behind us we can recap where we’ve been and where we’re going. To reiterate,\nthe overall goal of my dissertation was to present, explicitly, an approach that follows the ﬁrst principles\nof the compression problems. This is motivated by engineering as much as by science, and I have shown\nthat incorporating engineering principles, both into the methodologies, e.g., considering how compression\nalgorithms were developed, and the philosophy of the research, e.g., approaching a scientiﬁc problem as an\nengineer and a scientist simultaneously, can be successful. While this is not an approach unique to myself, I\nﬁnd that it is rarely stated out loud. And yet, the engineers that developed the compression algorithms we all\nuse on a daily basis were extremely smart, so is it not logical to follow in their example when studying deep\nlearning? I hope that the implications of this thought extend far beyond this document.\nIn Chapter 7 (JPEG Domain Residual Learning) we developed a method for performing deep learning in\nthe JPEG domain. This method operates on coeﬃcients directly and requires no decompression. The goal of\nthe method was to produce a result which is as close as possible to the pixel domain result. In this work\nwe leveraged the ﬁrst principles of JPEG compression by linearizing the JPEG transform and composing\nit with our pixel domain convolutions. We also leveraged this to produce closed form derivations of batch\nnormalization and average pooling.However, we could not do this for ReLU and so we used an approximation\ntechnique. The primary issue with this work is that it uses substantially more memory to store feature maps\nand convolutions.\nIn Chapter 9 (Quantization Guided JPEG Artifact Correction) we improved JPEG compression using deep\nlearning. We noted that there were several issues preventing the widespread success of prior work in this\nﬁeld. Prior works were quality dependent models which trained a unique model for each JPEG quality\nfactor. They did not handle color images, and they were focused only on regression. We solved each of these\nproblems again leveraging ﬁrst principles. By conditioning our network on the JPEG quantization matrix\nand processing DCT coeﬃcients instead of pixels, we were able to encode quality information using a single\nnetwork. By explicitly handling chroma subsampling and the additional quantization that color channels\nare subjected to, we improved results on color images. Finally, we incorporated GAN and texture losses to\nimprove the visual result over a regression-only solution. While this method was highly successful, it had a\ndistinct disadvantage when quantization data is either incorrect, as in multiple compression, or not available\nat all as in transcoding.\nThis was followed up in Chapter 10 (Task-Targeted Artifact Correction) which extended the previous\nmethod to optimize the correction for machine consumption. This is somewhat of a novelty in artifact\ncorrection which is traditionally for humans only. By incorporating a loss based on a downstream task, we\nare able to greatly improve the performance of that task on JPEG compressed inputs, often outperforming\ndata augmentation techniques that retrain the task with JPEG images. Our method had the added beneﬁt\nthat a network trained for one downstream task would also work well for other downstream tasks with no\nre-training required. The main drawback of this method is that it has increased running time (for multiple\nnetworks) and that it does not always out perform data augmentation.\nFinally, in Chapter 13 (Metabit: Leveraging Bitstream Metadata), we forayed into video compression by\ndeveloping a correction based method for improving AVC and HEVC compression. We noted that prior\nworks expended signiﬁcant resources computing results which were already stored explicitly in the video\nbitstreams, such as high quality frame locations (I-frames) and motion data. We leveraged this data and used\nour increased compute budget to more than double the number of parameters in our network with almost no\nimpact on throughput. We also incorporated scale-space loss along with the GAN and texture losses from the\nJPEG work, in order to improve high frequency reconstructions. While this method outperforms prior works\nand fully deep-learning based codecs at low birates, it does struggle with high bitrate reconstructions and it\ncurrently requires a unique model for each video compression setting. The method would also struggle to\nrun in real time on any consumer hardware.\nSo where do we go from here? Aside from the multitude of related problems to work on in compression,\nfrom things as simple as improving the results to as tangential as data privacy, the number one focus for the\nnext decade of compression and deep learning is going to be on making these techniques practical. One of the\n130\nprimary goals in writing this document is to instruct practitioners, e.g., professional engineers, developers,\nstudents, etc., that while these techniques show extreme promise, actually getting them into the hands of\nconsumers requires considerable eﬀort.\nFocusing solely on the techniques which improve compression performance, which would be of direct use\nto consumers who lack broadband internet, these methods currently require signiﬁcant compute resources\non the end user’s side. This could be shifted to the data center side, i.e., a hybrid technique which extracts\nsome deep representation during transmission. This could also be addressed simply by developing faster and\nlower-memory algorithms or by leveraging customized hardware. While the latter sounds like an expensive\nsolution, consider that video decoders are almost exclusively implemented in hardware on modern processors\nboth for desktop/laptop machines and mobile phones. This is also starting to happen for deep learning,\ne.g., the Google Tensor chips and edge TPUs, and the Apple A14 chips. In any case, I believe consumer\napplications for this technology are no more than two years oﬀat the time of writing, and within the decade\nfor fully deep learning based compression. The ﬁeld of compression as a whole is progressing as fast as ever\nand it is an exciting time to be involved.\nThis is all in the midst of the global pandemic. In a world which was just beginning to address the inequality\nin internet access, suddenly in late 2019, we were forced to confront this issue as work and school became\nprimarily remote. Remote work and school means communication with video and images which means\ncompression. Those who did not have a strong internet connection were simply left behind as there were no\nsuitable alternatives, and it remains to be seen what the long term ramiﬁcations of this will be. Now in early\n2022, the world is quickly moving on from pandemic life. Yet it is important not to forget this lesson.\nBetter compression has the ability to help people right now.\nAuthor’s Note\nI invite the readers to now visit the appendices where they will ﬁnd material which is just as interesting,\nand yet not directly related to, the dissertation proper. In particular we will review some additional\nqualitative results and brieﬂy cover fully deep-learning based compression algorithms. Thank you for\nreading my dissertation!\nMax Ehrlich\nAppendix\nA\nStudy on JPEG Compression and Machine Learning\nThis appendix reproduces the full plots and tables of the results of the study on JPEG compression and\ndeep learning (Ehrlich, Davis, Lim, et al. 2021). See Chapter 10 (Task-Targeted Artifact Correction) for more\ndetails. These plots are for informational purposes only.\nA.1 Plots of Results\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\n10\n12\n14\nAccuracy Loss (%)\nMobileNetV2\nResNet-18\nResNet-50\nResNet-101\nResNeXt-50\nResNeXt-101\nVGG-19\nInceptionV3\nEfficientNet B3\nFigure A.1: Overall Classiﬁcation Results\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\n10\n12\n14\nAccuracy Loss (%)\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.2: Classiﬁcation Results: MobileNetV2\n134\nA Study on JPEG Compression and Machine Learning\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nAccuracy Loss (%)\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.3: Classiﬁcation Results: VGG-19\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\nAccuracy Loss (%)\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.4: Classiﬁcation Results: InceptionV3\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\n10\nAccuracy Loss (%)\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.5: Classiﬁcation Results: ResNeXt 50\nA.1 Plots of Results\n135\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\n10\nAccuracy Loss (%)\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.6: Classiﬁcation Results: ResNeXt 101\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\n10\nAccuracy Loss (%)\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.7: Classiﬁcation Results: ResNet 18\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\n10\n12\nAccuracy Loss (%)\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.8: Classiﬁcation Results: ResNet 50\n136\nA Study on JPEG Compression and Machine Learning\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\n10\nAccuracy Loss (%)\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.9: Classiﬁcation Results: ResNet 101\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\nAccuracy Loss (%)\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.10: Classiﬁcation Results: EﬃcientNet B3\nA.1 Plots of Results\n137\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\n10\n12\n14\nmAP Loss\nFasterRCNN\nFastRCNN\nRetinaNet\nMaskRCNN\nFigure A.11: Overall Detection and Instance Segmentation Results\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nmAP Loss\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.12: Detection Results: FastRCNN\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nmAP Loss\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.13: Detection Results: FasterRCNN\n138\nA Study on JPEG Compression and Machine Learning\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nmAP Loss\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.14: Detection Results: RetinaNet\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nmAP Loss\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.15: Detection Results: MaskRCNN\nA.1 Plots of Results\n139\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\n10\n12\n14\n16\nmIoU Loss\nHRNetV2 + C1\nMobileNetV2 (dilated) + C1 (ds)\nResNet18 (dilated) + PPM\nResNet50 + UPerNet\nResNet50 (dilated) + PPM\nResNet101 + UPerNet\nResNet101 (dilated) + PPM\nFigure A.16: Overall Semantic Segmentation Results\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nmIoU Loss\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.17: Semantic Segmentation Results: HRNetV2 + C1\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nmIoU Loss\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.18: Semantic Segmentation Results: MobileNetV2 + C1\n140\nA Study on JPEG Compression and Machine Learning\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nmIoU Loss\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.19: Semantic Segmentation Results: ResNet 18 + PPM\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nmIoU Loss\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.20: Semantic Segmentation Results: Resnet50 + UPerNet\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nmIoU Loss\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.21: Semantic Segmentation Results: ResNet 50 + PPM\nA.1 Plots of Results\n141\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nmIoU Loss\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.22: Semantic Segmentation Results: ResNet 101 + UPerNet\n10\n20\n30\n40\n50\n60\n70\n80\n90\nQuality\n0\n2\n4\n6\n8\n10\n12\n14\nmIoU Loss\nNone\nOff-the-Shelf Artifact Correction\nFine-Tuned\nTask-Targeted Artifact Correction\nFigure A.23: Semantic Segmentation Results: ResNet 101 + PPM\n142\nA Study on JPEG Compression and Machine Learning\nA.2 Tables of Results\nModel\nMetric\nReference\nMitigation\nQ=10\nQ=20\nQ=30\nQ=40\nQ=50\nQ=60\nQ=70\nQ=80\nQ=90\nEﬃcientNet B3\nTop-1 Accuracy\n83.98\nSupervised Fine-Tuning\n79.78\n81.84\n82.47\n82.68\n82.78\n82.75\n82.83\n82.85\n82.83\nNone\n77.24\n81.11\n81.95\n82.52\n82.67\n82.91\n83.10\n83.37\n83.75\nOﬀ-the-Shelf Artifact Correction\n75.92\n80.02\n81.47\n82.12\n82.44\n82.71\n82.94\n83.23\n83.70\nTask-Targeted Artifact Correction\n81.03\n82.71\n83.21\n83.53\n83.64\n83.71\n83.73\n83.80\n83.76\nInceptionV3\nTop-1 Accuracy\n77.33\nSupervised Fine-Tuning\n75.11\n77.25\n77.77\n77.89\n78.13\n78.13\n78.24\n78.26\n78.32\nNone\n69.38\n74.15\n75.44\n75.98\n76.38\n76.69\n76.95\n77.14\n77.30\nOﬀ-the-Shelf Artifact Correction\n71.21\n75.04\n76.09\n76.42\n76.68\n76.79\n76.97\n77.06\n77.13\nTask-Targeted Artifact Correction\n73.65\n75.89\n76.53\n76.82\n76.93\n76.99\n77.09\n77.15\n77.10\nMobileNetV2\nTop-1 Accuracy\n70.72\nSupervised Fine-Tuning\n65.65\n69.21\n69.92\n70.20\n70.37\n70.53\n70.50\n70.55\n70.54\nNone\n57.23\n65.55\n67.87\n68.95\n69.47\n69.98\n70.24\n70.60\n70.86\nOﬀ-the-Shelf Artifact Correction\n57.33\n65.25\n67.76\n68.93\n69.60\n70.07\n70.40\n70.71\n70.58\nTask-Targeted Artifact Correction\n64.64\n68.63\n69.71\n70.18\n70.32\n70.44\n70.50\n70.52\n70.34\nResNet-101\nTop-1 Accuracy\n76.91\nSupervised Fine-Tuning\n74.63\n76.50\n77.07\n77.20\n77.27\n77.29\n77.43\n77.44\n77.53\nNone\n66.12\n73.00\n74.65\n75.39\n75.83\n76.29\n76.51\n76.79\n76.96\nOﬀ-the-Shelf Artifact Correction\n67.91\n73.64\n75.09\n75.84\n76.23\n76.52\n76.56\n76.80\n76.74\nTask-Targeted Artifact Correction\n72.99\n75.53\n76.30\n76.60\n76.59\n76.72\n76.70\n76.72\n76.59\nResNet-18\nTop-1 Accuracy\n68.84\nSupervised Fine-Tuning\n65.49\n68.46\n69.07\n69.16\n69.36\n69.33\n69.38\n69.53\n69.49\nNone\n57.62\n65.26\n67.07\n67.68\n68.08\n68.30\n68.61\n68.84\n68.92\nOﬀ-the-Shelf Artifact Correction\n61.19\n66.39\n67.87\n68.39\n68.61\n68.77\n68.97\n68.99\n68.90\nTask-Targeted Artifact Correction\n63.83\n67.06\n68.04\n68.24\n68.35\n68.48\n68.52\n68.60\n68.50\nResNet-50\nTop-1 Accuracy\n75.31\nSupervised Fine-Tuning\n73.18\n75.46\n76.02\n76.24\n76.36\n76.42\n76.52\n76.52\n76.55\nNone\n63.43\n71.20\n73.23\n74.10\n74.43\n74.63\n75.01\n75.09\n75.34\nOﬀ-the-Shelf Artifact Correction\n66.90\n72.45\n73.95\n74.60\n74.93\n75.18\n75.26\n75.42\n75.30\nTask-Targeted Artifact Correction\n70.48\n73.56\n74.39\n74.81\n74.94\n75.00\n74.98\n74.98\n74.89\nResNeXt-101\nTop-1 Accuracy\n78.81\nSupervised Fine-Tuning\n75.60\n78.00\n78.50\n78.71\n78.86\n78.97\n79.01\n78.98\n79.06\nNone\n68.83\n74.84\n76.39\n77.05\n77.60\n78.00\n78.16\n78.56\n78.75\nOﬀ-the-Shelf Artifact Correction\n71.19\n75.88\n77.14\n77.80\n78.15\n78.30\n78.57\n78.66\n78.61\nTask-Targeted Artifact Correction\n74.73\n77.33\n78.08\n78.29\n78.55\n78.62\n78.68\n78.73\n78.68\nResNeXt-50\nTop-1 Accuracy\n76.99\nSupervised Fine-Tuning\n74.21\n76.23\n76.79\n77.01\n77.08\n77.18\n77.16\n77.30\n77.17\nNone\n66.96\n73.21\n74.85\n75.62\n76.07\n76.37\n76.63\n76.88\n77.06\nOﬀ-the-Shelf Artifact Correction\n68.05\n73.56\n75.11\n75.95\n76.38\n76.59\n76.71\n76.99\n76.90\nTask-Targeted Artifact Correction\n72.22\n75.45\n76.09\n76.62\n76.86\n76.83\n76.85\n76.99\n76.81\nVGG-19\nTop-1 Accuracy\n73.44\nSupervised Fine-Tuning\n69.50\n72.66\n73.29\n73.74\n73.83\n73.85\n73.95\n74.14\n74.11\nNone\n59.27\n68.08\n70.49\n71.53\n71.99\n72.42\n72.80\n73.24\n73.46\nOﬀ-the-Shelf Artifact Correction\n61.93\n68.79\n70.82\n71.83\n72.50\n72.94\n73.13\n73.40\n73.44\nTask-Targeted Artifact Correction\n67.50\n71.32\n72.33\n72.76\n73.03\n73.16\n73.50\n73.48\n73.44\nTable A.1: Results for classiﬁcation models.\nModel\nMetric\nReference\nMitigation\nQ=10\nQ=20\nQ=30\nQ=40\nQ=50\nQ=60\nQ=70\nQ=80\nQ=90\nFasterRCNN\nmAP\n35.37\nSupervised Fine-Tuning\n29.09\n33.34\n34.72\n35.08\n35.49\n35.82\n35.96\n36.06\n36.17\nNone\n20.35\n30.03\n32.59\n33.43\n34.04\n34.31\n34.73\n34.93\n35.25\nOﬀ-the-Shelf Artifact Correction\n28.45\n31.86\n33.10\n33.85\n34.05\n34.47\n34.70\n34.77\n34.71\nTask-Targeted Artifact Correction\n31.43\n33.85\n34.29\n34.81\n34.81\n34.97\n35.01\n34.88\n34.81\nFastRCNN\nmAP\n34.02\nSupervised Fine-Tuning\n28.01\n31.94\n33.08\n33.56\n33.88\n34.17\n34.42\n34.44\n34.66\nNone\n19.99\n29.04\n31.22\n32.19\n32.65\n33.00\n33.34\n33.40\n33.80\nOﬀ-the-Shelf Artifact Correction\n27.62\n30.91\n32.04\n32.56\n32.78\n33.18\n33.28\n33.48\n33.44\nTask-Targeted Artifact Correction\n30.11\n32.31\n33.07\n33.31\n33.39\n33.53\n33.69\n33.68\n33.59\nMaskRCNN\nmAP\n32.84\nSupervised Fine-Tuning\n26.32\n30.48\n31.79\n32.21\n32.55\n32.83\n33.11\n33.20\n33.32\nNone\n18.35\n27.58\n29.83\n30.80\n31.32\n31.62\n32.02\n32.29\n32.62\nOﬀ-the-Shelf Artifact Correction\n25.82\n29.35\n30.67\n31.32\n31.59\n31.85\n32.03\n32.24\n32.16\nTask-Targeted Artifact Correction\n28.48\n30.85\n31.71\n32.00\n32.19\n32.24\n32.35\n32.43\n32.26\nRetinaNet\nmAP\n33.57\nSupervised Fine-Tuning\n27.64\n31.97\n33.03\n33.50\n33.80\n34.12\n34.30\n34.33\n34.40\nNone\n18.76\n28.23\n30.63\n31.59\n32.27\n32.57\n32.88\n33.02\n33.42\nOﬀ-the-Shelf Artifact Correction\n26.74\n29.90\n31.24\n31.87\n32.19\n32.60\n32.86\n33.02\n32.93\nTask-Targeted Artifact Correction\n29.66\n31.86\n32.73\n32.97\n32.98\n33.13\n33.24\n33.23\n33.09\nTable A.2: Results for detection models.\nA.2 Tables of Results\n143\nModel\nMetric\nReference\nMitigation\nQ=10\nQ=20\nQ=30\nQ=40\nQ=50\nQ=60\nQ=70\nQ=80\nQ=90\nHRNetV2 + C1\nmIoU\n40.59\nSupervised Fine-Tuning\n34.76\n37.35\n38.74\n38.78\n39.27\n39.75\n39.98\n39.86\n39.96\nNone\n24.95\n35.16\n38.03\n38.52\n39.02\n40.09\n40.50\n40.41\n40.54\nOﬀ-the-Shelf Artifact Correction\n32.30\n36.54\n38.40\n38.52\n40.08\n40.44\n40.46\n40.22\n40.60\nTask-Targeted Artifact Correction\n34.14\n37.61\n39.23\n39.24\n39.92\n40.53\n40.62\n40.39\n40.55\nMobileNetV2 (dilated) + C1 (ds)\nmIoU\n29.52\nSupervised Fine-Tuning\n19.07\n22.37\n23.43\n23.62\n23.60\n24.15\n24.44\n24.37\n24.46\nNone\n13.92\n24.03\n27.13\n27.75\n27.73\n28.86\n29.37\n29.35\n29.43\nOﬀ-the-Shelf Artifact Correction\n21.17\n25.27\n27.31\n27.16\n29.14\n29.32\n29.26\n29.06\n29.54\nTask-Targeted Artifact Correction\n24.74\n27.37\n28.44\n28.33\n29.19\n29.56\n29.54\n29.38\n29.52\nResNet101 + UPerNet\nmIoU\n41.08\nSupervised Fine-Tuning\n35.32\n37.41\n38.27\n38.28\n38.55\n38.59\n38.72\n38.58\n38.70\nNone\n26.14\n36.70\n39.45\n39.81\n39.55\n40.47\n40.98\n40.97\n41.07\nOﬀ-the-Shelf Artifact Correction\n33.90\n37.39\n39.12\n39.38\n40.32\n40.58\n40.78\n40.79\n41.04\nTask-Targeted Artifact Correction\n35.82\n38.67\n39.96\n39.98\n40.22\n40.79\n40.97\n40.91\n41.00\nResNet101 (dilated) + PPM\nmIoU\n40.26\nSupervised Fine-Tuning\n31.86\n35.45\n36.73\n36.94\n36.91\n37.33\n37.67\n37.55\n37.65\nNone\n25.68\n35.19\n37.76\n38.43\n38.24\n39.27\n40.03\n40.17\n40.21\nOﬀ-the-Shelf Artifact Correction\n31.44\n35.86\n38.01\n38.26\n39.54\n39.73\n39.94\n40.06\n40.22\nTask-Targeted Artifact Correction\n33.99\n37.63\n39.04\n39.11\n39.38\n39.73\n40.07\n40.11\n40.10\nResNet18 (dilated) + PPM\nmIoU\n36.65\nSupervised Fine-Tuning\n29.84\n32.33\n33.08\n33.01\n33.38\n33.61\n33.50\n33.29\n33.33\nNone\n21.16\n31.99\n34.72\n35.36\n35.41\n36.16\n36.56\n36.60\n36.59\nOﬀ-the-Shelf Artifact Correction\n28.64\n32.59\n34.56\n34.53\n35.96\n36.21\n36.29\n36.25\n36.64\nTask-Targeted Artifact Correction\n31.69\n34.55\n35.80\n35.80\n36.12\n36.50\n36.66\n36.54\n36.60\nResNet50 + UPerNet\nmIoU\n39.21\nSupervised Fine-Tuning\n32.88\n35.11\n35.94\n35.90\n36.41\n36.58\n36.63\n36.49\n36.55\nNone\n24.29\n34.78\n37.34\n37.71\n37.70\n38.57\n39.12\n39.13\n39.16\nOﬀ-the-Shelf Artifact Correction\n31.83\n35.52\n37.20\n37.26\n38.44\n38.67\n38.87\n38.86\n39.12\nTask-Targeted Artifact Correction\n34.36\n36.94\n38.17\n38.07\n38.55\n38.93\n39.14\n39.06\n39.09\nResNet50 (dilated) + PPM\nmIoU\n38.91\nSupervised Fine-Tuning\n32.26\n35.33\n36.04\n36.04\n36.53\n36.75\n36.93\n36.71\n36.92\nNone\n23.05\n33.95\n36.66\n37.07\n37.40\n38.58\n38.93\n38.70\n38.86\nOﬀ-the-Shelf Artifact Correction\n28.36\n32.69\n35.24\n35.31\n37.74\n38.04\n38.18\n38.13\n38.73\nTask-Targeted Artifact Correction\n31.92\n35.43\n37.04\n36.92\n38.05\n38.69\n38.79\n38.52\n38.74\nTable A.3: Results for segmentation models.\nModel\nValue\nImageNet Classiﬁcation, Metric: Top-1 Accuracy\nResNet 18\n68.84\nResNet 50\n75.31\nResNet 101\n76.91\nResNeXt 50\n76.99\nResNeXt 101\n78.81\nVGG 19\n73.44\nMobileNetV2\n70.72\nInceptionV3\n77.33\nEﬃcientNet B3\n83.98\nCOCO Object Detection and Instance Segmentation, Metric: mAP\nFastRCNN\n34.02\nFasterRCNN\n35.38\nRetinaNet\n33.57\nMaskRCNN\n32.84\nADE20k Semantic Segmentation, Metric: mIoU\nHRNetV2 + C1\n40.59\nMobileNetV2 (dilated) + C1\n29.52\nResNet 18 (dilated) + PPM\n36.65\nResNet 50 (dilated) + PPM\n38.91\nResNet 101\n41.08\nResNet 101 (dilated) + PPM\n40.26\nTable A.4: Reference results (results with no compression).\nB\nAdditional Results\nIn this appendix we examine more interesting outputs from various methods discussed in the body of the\ndissertation. These are mostly qualitative results. While these images are not critical to understanding the\nmethods, everyone likes looking at pictures!\nWarning\nThe results presented here are intended to be reproductions from the published papers, so there may be\nsome repeats from the body of the dissertation.\nB.1 Quantization Guided JPEG Artifact Correction\nThese results are from the method presented in Chapter 9 (Quantization Guided JPEG Artifact Correction).\nWe ﬁrst show more equivalent quality examples next, recall that equivalent quality performs restoration\non an image then uses SSIM to ﬁnd the matching JPEG quality to the restored image which can give an\nindication of how much space is saved by using QGAC.\nInput\nQuality 30\nEquivalent Quality JPEG\nQuality 58\nReconstruction\n46.8kB Saved (47.9%)\nInput\nQuality 50\nEquivalent Quality JPEG\nQuality 85\nReconstruction\n29.5kB Saved (43.6%)\nInput\nQuality 40\nEquivalent Quality JPEG\nQuality 78\nReconstruction\n25.0kB Saved (42.7%)\nFigure B.1: Equivalent quality visualizations. For each image we show the input JPEG, the JPEG with equivalent SSIM to our model\noutput, and our model output.\nNext we show the full frequency domain results. Recall that these results show the frequency domain content\nof the images comparing JPEG compression, regression restoration, and GAN restoration.\n146\nB Additional Results\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nProbability\nFrequency\nOriginal\nJPEG\nRegression\nGAN\nOriginal\nPlot\nDCT\nJPEG Q=10\nRegression\nGAN\nFigure B.2: Frequency domain results 1/4\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nProbability\nFrequency\nOriginal\nJPEG\nRegression\nGAN\nOriginal\nPlot\nDCT\nJPEG Q=10\nRegression\nGAN\nFigure B.3: Frequency domain results 2/4\nB.1 Quantization Guided JPEG Artifact Correction\n147\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nProbability\nFrequency\nOriginal\nJPEG\nRegression\nGAN\nOriginal\nPlot\nDCT\nJPEG Q=10\nRegression\nGAN\nFigure B.4: Frequency domain results 3/4.\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nProbability\nFrequency\nOriginal\nJPEG\nRegression\nGAN\nOriginal\nPlot\nDCT\nJPEG Q=10\nRegression\nGAN\nFigure B.5: Frequency domain results 4/4.\n148\nB Additional Results\nOne way to reduce any artifacts caused by divergent GAN training is to use model interpolation (X. Wang,\nYu, et al. 2018). Model interpolation simply takes the regression weights 푊푅and the GAN weights 푊퐺along\nwith a scalar 훼and computes new model parameters\n푊퐼= (1 −훼)푊푅+ 훼푊퐺\n(B.1)\nWe show close up views of diﬀerent textured regions for diﬀerent choices of 훼.\nRegression\nGAN\nFigure B.6: Model interpolation results 1/4\nRegression\nGAN\nFigure B.7: Model interpolation results 2/4\nB.1 Quantization Guided JPEG Artifact Correction\n149\nRegression\nGAN\nFigure B.8: Model interpolation results 3/4\nRegression\nGAN\nFigure B.9: Model interpolation results 4/4\nWe close with purely qualitative results. These are for quality 10 as in Chapter 9 (Quantization Guided JPEG\nArtifact Correction) and quality 20 which was not shown there to save space.\n150\nB Additional Results\nJPEG Q=10\nJPEG Q=20\nReconstruction\nReconstruction\nOriginal\nOriginal\nFigure B.10: Qualitative results 1/4. Live-1 images.\nJPEG Q=10\nJPEG Q=20\nReconstruction\nReconstruction\nOriginal\nOriginal\nFigure B.11: Qualitative results 2/4. Live-1 images.\nB.1 Quantization Guided JPEG Artifact Correction\n151\nJPEG Q=10\nJPEG Q=20\nReconstruction\nReconstruction\nOriginal\nOriginal\nFigure B.12: Qualitative results 3/4. Live-1 images.\nJPEG Q=10\nJPEG Q=20\nReconstruction\nReconstruction\nOriginal\nOriginal\nFigure B.13: Qualitative results 4/4. ICB images.\n152\nB Additional Results\nB.2 Task Targeted Artifact Correction\nThese results are from the method of Chapter 10 (Task-Targeted Artifact Correction). We start with visualiza-\ntions of model errors, ﬁrst using GradCam (Selvaraju et al. 2017). This shows how the model focus is impacted\nby JPEG compression and how it can be corrected using the various mitigation techniques we studied. The\nﬁgures show some interesting behavior. In terms of localization, the JPEG compressed input actually does\nwell, and the localization is in fact more accurate than the original model with an uncompressed input. The\nproblem with the JPEG compressed input seems to be with the gradient, which is extremely noisy. Mitigation\nseems to help with this, with the supervised method providing the cleanest gradient although there is a loss\nof localization accuracy.\nFine-tuned model gradient\nOriginal model gradient \nOriginal model gradient with\noriginal input image \nFine-tuned model CAM\nOriginal model CAM\nOriginal model CAM with\noriginal input image \nFigure B.14: Fine Tuned Model Comparison\nB.2 Task Targeted Artifact Correction\n153\nFine-tuned model gradient\nOriginal model gradient \nOriginal model gradient with\noriginal input image \nFine-tuned model CAM\nOriginal model CAM\nOriginal model CAM with\noriginal input image \nFigure B.15: Oﬀ-the-Shelf Artifact Correction Comparison\nFine-tuned model gradient\nOriginal model gradient \nOriginal model gradient with\noriginal input image \nFine-tuned model CAM\nOriginal model CAM\nOriginal model CAM with\noriginal input image \nFigure B.16: Task-Targeted Artifact Correction Comparison\nFor visualizing detection results we provide plots generated using TIDE (Bolya et al. 2020). We show these for\nFasterRCNN (Ren et al. 2016) and MaskRCNN (He, Gkioxari, et al. 2017). The results show a signiﬁcant number\nof missed detection for low quality inputs. This is overtaken by localization errors as quality increases.\n154\nB Additional Results\nFigure B.17: FasterRCNN TIDE Plots. Left: quality 10, Middle: quality 50, Right: quality 100.\nFigure B.18: MaskRCNN TIDE Plots. Left: quality 10, Middle: quality 50, Right: quality 100.\nWe close the section with qualitative results including visualizations of the results where appropriate.\nB.2 Task Targeted Artifact Correction\n155\nJPEG Q=10, Prediction: Norwich terrier'', Fine-\nTuned Prediction: Pembroke, Pembroke Welsh\ncorgi'' \nOff-the-shelf Artifact Correction, Prediction:\n\"basenji\"\nTask-Targeted Artifact Correction, Prediction:\n\"Pembroke, Pembroke Welsh corgi\"\nOriginal, Prediction: \"Pembroke, Pembroke Welsh\ncorgi\"\nFigure B.19: MobileNetV2, Ground Truth: “Pembroke, Pembroke Welsh corgi”\n156\nB Additional Results\nJPEG Q=10\nOff-the-Shelf Artifact Correction\nTask-Targeted Artifact Correction\nSupervised Fine-Tuning\nOriginal\nGround Truth\nFigure B.20: FasterRCNN\nB.2 Task Targeted Artifact Correction\n157\nJPEG Q=10\nOff-the-Shelf Artifact Correction\nTask-Targeted Artifact Correction\nSupervised Fine-Tuning\nOriginal\nGround Truth\nFigure B.21: MaskRCNN\n158\nB Additional Results\nJPEG Q=10\nPrediction\nGround Truth\nJPEG Q=10\nPrediction\nGround Truth\nJPEG Q=10\nPrediction\nGround Truth\nJPEG Q=10\nFine-tuning Prediction\nGround Truth\nFigure B.22: HRNetV2 + C1\nB.3 Metabit\n159\nB.3 Metabit\nThe results in this section are from the method of Chapter 13 (Metabit: Leveraging Bitstream Metadata).\nThese are purely qualitative results but they do highlight speciﬁc successes and failures of the method.\nFigure B.23: Dark Region. Crop from 2560 × 1600 “People on Street”. The dark region, is poorly preserved by compression. Our GAN\nrestoration struggles to cope with the massive information loss in this region.\nFigure B.24: Crowd. Crop from 2560 × 1600 “People on Street”. The image shows an extremely dense crowd. Despite the chaotic nature,\nour GAN is able to produce a good restoration although there is detail missing.\nFigure B.25: Texture Restoration. Crop from 1920 × 1080 “Cactus”. The texture on the background is destroyed by compression. Our\nGAN reconstructs a reasonable approximation to the true texture.\n160\nB Additional Results\nFigure B.26: Compression Artifacts Mistaken for Texture. Crop from 1920 × 1080 “Cactus”. The compressed image exhibits strong\nchroma subsampling artifacts (lower right corner). These are mistaken by the GAN is a texture and restored as such.\nFigure B.27: Motion Blur. Crop from 1920 × 1080 “Cactus”. The tiger exhibits high motion which presents itself in the target frame as\nmotion blur. This blur is destroyed by compression and is not able to be restored by the GAN loss. The GAN loss is also “rewarded” for\nsharp edges which would make reconstructing blurry objects diﬃcult. As an aside, note the additional detail on the background objects\nin the GAN image when compared to the compressed image.\nFigure B.28: Artiﬁcial. Crop from 1920 × 1080 “Big Buck Bunny”. This artiﬁcial scene is restored accurately despite a lack of artiﬁcial\ntraining data. Note the grass and tree textures, sharp edges, removal of blocking on the ﬂower, and preservation of the smooth sky\nregion.\nC\nSurvey of Fully Deep-Learning Based Compression\nAlthough fully deep-learning based compression methods are generally considered out of the scope of\nthis dissertation, there is general interest in these technologies and they are certainly related to the work\npresented in the body of the document. Therefore, in this appendix, we conduct a brief survey of the major\npoints of image and video compression that depends entirely on deep learning to produce the encodings.\nWhile deep learning based compression shows extreme promise, it is still a very academic problem. Models\ncurrently require expensive hardware to train and to compress new media in a timely manner. This also\nleads to high memory usage. In general, important compression concepts like rate control are still largely\nmissing.\nIn terms of objective performance, the most recent methods at the time of writing are on par with classical\ncompression on some benchmarks. This is not always easy to evaluate, however, as methods depending on\ngeneration like GANs (Goodfellow et al. 2014) often do not produce meaningful rate-distortion curves in the\ntraditional sense.\nIn a rare personal opinion, based on my observation of the state of the art, I believe that machine learning,\nwherever it may end up, is the future of compression. Within the decade (i.e., before 2030) we will begin to\nsee machine learning techniques used in consumer application. In contrast, the techniques presented in the\nbody of the dissertation will likely be seen in consumer application in the next one or two years. There are\ncurrently a number of companies competing for deep-learning based compression market share, e.g., Google\nand Wave One. While these companies are delivering important research contributions it is unlikely that\ntheir proprietary solutions will win out in the long term given the compression community’s reliance on\nstandardization. Although Google was able to gain traction in classical compression with its VP codecs, even\nthese were eventually standardized into the Alliance for Open Media (AOM) and development continued\nwith the AV codecs. Notable standardization eﬀorts include JPEG-AI and MPEG-AI which are much more\nlikely to see success, meaning that any new players in this ﬁeld would do well to work with the standards\nbodies.\nC.1 Image Compression\nWe start with image compression. The goal of these models is to train a CNN to encode pixels into a feature\nvector with another CNN trained simultaneously to decode the feature vectors to an image, essentially a\nfancy autoencoder. The feature vectors are quantized and losslessly compressed before “transmission”, or in\nthis case, evaluating their size in bytes. The networks will be trained to minimize both the size of the feature\nvectors when stored on disk and the error of the reconstruction. There are three obvious problems here which\ndrive the works we will consider in this section\n▶The size on disk is not diﬀerentiable and therefore not suitable for use in a loss function.\n▶Classical compression algorithms incorporate rate-control to make their use more ﬂexible. It is not\ntrivial to incorporate such a side parameter into a CNN.\n▶Minimizing the error term does not necessarily produce a visually pleasing result.\nLikely the ﬁrst modern work in image compression with deep learning was the work by G. Toderici,\nO’Malley, et al. 2015 for thumbnails with a follow up for full resolution images (G. Toderici, Vincent, et al. 2017).\nTodereci’s work is based on recurrent networks speciﬁcally Long-Short-Term Memory networks (LSTMs).\nThe output of the LSTM at time 푡is subtracted from the input and this residual used as the input to the LSTM\nat time 푡+ 1 after starting the process with the input patch generating a ﬁxed length code for a given bitrate\nsetting. The network is only trained to minimize the 푙2 error. Considering how early these architectures were\n162\nC Survey of Fully Deep-Learning Based Compression\ndeveloped they have some nice properties, including reasonable results compared to JPEG and a rudimentary\nattempt at variable rate encoding.\nNext, Theis et al. 2017 proposed compressive autoencoders to generate a compressed representation. The\nidea is to produce a deep encoding of the input image which is then quantized for transmission and decoded\nby another deep network. The objective can be written as\n−log(푄( 푓(푥))) + 훽푑(푥, 푔(푓(푥)))\n(C.1)\nwhere 푄() is the quantization function, 푓() is the encoder, 푔() is the decoder, and 푑() is a measure of\ndistortion (i.e., error). The left term here is measuring the size of the representation (number of bits) and\nthe right term is measuring the error. Of course this objective cannot be minimized directly since 푄() is\nnot diﬀerentiable. To get around this they deﬁne a diﬀerentiable approximation to the rounding step. By\nadjusting this approximation, they are able to produce a much more accurate variable rate encoder, although\nthe empirical results show that training for a single rate naturally works better.\nAlso in 2017, Agustsson, Mentzer, et al. 2017 developed “soft-to-hard vector quantization”. They start with\nthe same problem of Theis et al. 2017, that the quantization step is not diﬀerentiable. They solve the problem\nby using a soft assignment of the features to symbols, i.e., instead of a hard rounding they compute\n휙(푧) = softmax(−휎(∥푧−푐1∥2, . . . , ∥푧−푐푛∥2))\n(C.2)\nfor 푛symbols 푐. This equation is fully diﬀerentiable. However, this alone would be a poor approximation so\nduring training the “hardness” is “annealed” from some initial condition to inﬁnity which produces a more\nand more accurate approximation of the hard assignment which is used at test time. This allows the network\nto quickly converge on the easier soft solution in early training while increasing the problem diﬃculty to\nmatch the real scenario in late training.\nBallé et al. 2016 propose yet another solution to this problem. Their solution is motivated by Shannon’s\ninformation theory and, although somewhat questionable in the motivation, has become a staple technique\nfor approximating quantization. Bellé et al. observe that the discrete quantization processes is essentially\nintroducing noise into the signal which is output by the deep encoder. Of course, entropy of a noisy channel\nis something that Shannon studied quite extensively (Shannon 1948). Therefore, the solution is to simply add\nGaussian noise to the signal which is a simple and diﬀerentiable process. Of course the issue with this is that\nGaussian noise is very diﬀerent in appearance from quantization noise and CNNs are very sensitive to the\nactual appearance even if the entropy analysis is the same (entropy is essentially giving an aggregate view of\nthe information loss). Nevertheless the method does work well.\nMentzer, Agustsson, et al. 2018 speciﬁcally focus on designing a method for variable rate encoding.\nAlthough this was a feature of prior works, their primary focus was on overcoming the non-diﬀerentiable\nquantization. Mentzer et al. use both the soft-to-hard technique (Agustsson, Mentzer, et al. 2017) and the\ncompressive autoencoders technique (Theis et al. 2017) to deal with quantization. To model the rate term in\nthe loss, Mentzer et al. treat the feature vectors as a conditional distribution, i.e.,\n푃(푧) =\n푁\nY\n푖=1\n푃(푧푖|푧푖−1, . . . 푧1)\n(C.3)\nin raster order. So each feature vector is considered to have its own probability which is conditioned on all\nprevious features. They then model 푃(푧) and the conditional distributions using another deep network (which\nis diﬀerentiable). Speciﬁcally they use a 3D convolution since this is eﬃcient and respects the “causality\nconstraint.” In other words, the previous feature vectors cause the current feature vector since they are\nconditional distributions. This formulation for 푃(푧) allows them to compute an approximation for the entropy\nand therefore the rate which they use as a loss term.\nIn something of a departure from prior works, Agustsson, Tschannen, et al. 2019 formulate a compression\nalgorithm based on GANs (Goodfellow et al. 2014). The advantage here is that these algorithms can produce\nstriking, faithful, images in extreme settings. In this case the distortion term is replaced with a GAN loss\ninstead of the traditional 푙2 loss of prior works. This is based on the correct observation that 푙2 loss does not\ncapture human perception well. Although the paper oﬀers limited further insight into the mathematics of\nC.2 Video Compression\n163\ndeep learning based compression systems, the imagery their method produces is truly remarkable often\noutperforming JPEG by a wide margin while saving considerably more space.\nThis work is continued by Mentzer, G. D. Toderici, et al. 2020 and they make a number of advancements\nover Agustsson, Tschannen, et al. 2019. Where Aggustsson et al. showed visually realistic results there were\nmajor deviations from the true outputs. The preservation of the output seemed to be more semantic than\nvisual which makes sense given that the GAN training uses deep networks classifying real/fake. Mentzer et al.\nby comparison is extremely faithful to the original images often deviating in ways that are indistinguishable\nto the human eye. Aggustsson et al. is also quite limited in the size of the input that it can accept (an eﬃciency\nconcern), whereas the Mentzer et al. formulation works eﬃciently on sizes up to 2000 × 2000 which is a\nrespectable size for a modern image. An interesting avenue of analysis in this work is in the eﬀectiveness, or\nlack thereof, of metrics. After conducting an extensive user study, they found that no metric was adequate for\nmatching the human’s responses. This is not at all surprising.\nAlthough we end in 2020 image compression continues to be an active area of research, although it remains\nto be seen which works of 2021 will emerge as the most inﬂuential. In the interest of space, we conclude\nthe discussion of image compression here. Although the advance of Mentzer, G. D. Toderici, et al. 2020 was\nextremely promising, there is still no deep learning algorithm that is suitable for deployment in a consumer\napplication. This is partly an eﬃciency concern but it is also a ﬂexibility concern. JPEG was extremely well\nthought out to work for the widest range of situations which is part of the reason it has persisted for 30 years.\nDeep learning methods are only just scratching the surface of this kind of long term thinking.\nC.2 Video Compression\nWe now turn to video compression. Similar to the previous section, the goal will be to train encoder and\ndecoder CNNs with some kind of quantization of the encoded feature vectors. Unlike the last section,\nhowever, we now have a temporal component in everything we do. In addition to the challenges of image\ncompression, dealing with the temporal component is a problem by itself. Some methods will treat the\ntime component as independent, essentially image compression with diﬀerent features over time. Some will\nattempt to incorporate the temporal component into the prediction itself either in a recurrent or motion based\nsolution. Still others will use an implicit representation, essentially over-ﬁtting a network for each video.\nWe start with the method of Wu et al. 2018. The key insight is that “keyframes” can be deﬁned which are\nthen encoded using oﬀ-the-shelf image encoders. Then, the intermediate frames are produced using image\ninterpolation networks that take the two keyframes as input and produce the intermediate frames. Naturally,\nthe longer the interval between the keyframes the higher the error of the predictions. While the method\ncertainly works, it does not clearly outperform H.264 in the same way that image compression algorithms\nwere clearly outperforming JPEG. This was still a major advancement, however, since prior to this no one had\ntried to produce a video codec using deep learning. By focusing on keyframe compression and interpolation,\nthe method is eﬃcient which was a major concern with video compression.\nNext, DVC (Lu et al. 2019) proposes an end-to-end technique which encodes motion and residual information\nfor predicted frames. This is intentionally designed to mimic the classical compression loop which stores\nintra-frames and then predicts intermediate frames using motion warping and low-entropy error residuals.\nIn this case, each component is modeled separately with a CNN. The method has several moving parts\nMotion Estimation which uses a task-speciﬁc optical ﬂow network to produce per-pixel motion. This motion\nis then encoded using another CNN for compression and quantized. The decoder performs the inverse\nprocess to produce the ﬂows\nMotion Compensation Also uses a deep network. First the decoded optical ﬂow is used to warp the reference\nimage, then the reference image, warped images, and optical ﬂow are all used as input to another deep\nnetwork to predict the true frame.\nTransform The residual between the predicted and true frame is taken and encoded using yet another CNN\nto produce the quantized encoding. This is similar to image compression techniques.\nOverall the method is fairly complex and heavy consisting of several convolutional networks. While all of this\ndoes pay oﬀin terms of the overall result compared with Wu et al. 2018, the actual codec itself struggles to\nmatch H.265. Furthermore, per-pixel ﬂow is likely wasteful, at least we know that classical video codecs do\n164\nC Survey of Fully Deep-Learning Based Compression\nnot make use of dense motion information. That being said the end-to-end nature and the idea of replacing\neach part of a traditional video encoder with a CNN are major advances to the state-of-the-art.\nIn 2020, we ﬁnally had a technique capable of outperforming H.265. The method of J. Liu et al. 2020\nproposed a simple but eﬀective technique. Use a standard deep learning based image compression algorithm\nto generate initial codes for each frame. Then perform internal learning to generate an “optimal” code for that\nframe and use a conditional entropy model to produce a ﬁnal code for the current frame that is conditioned\non the previous frame. Note that the internal learning method is actually learning a small CNN just for that\nparticular frame, so the encoding time is increased but the decoding is still fast since the decoder only needs\nto perform inference on the resulting network to obtain the code. The conditional entropy model also helps\nthe encoder reuse information from prior frames to reduce the ﬁnal code length. The contribution is very\nstraightforward. Aside from these two ideas there are no special formulations (this is a good thing). The\nresult is impressive, with results that consistently outperform the classical codecs for higher bitrates. The\nmethod does struggle at low bitrates, however.\nContinuing with internal learning is NeRV (Hao Chen et al. 2021). This method is entirely an internal\nlearning technique, which means that for each video, the compression process is to train a neural network\nwhich predicts only that video (over-ﬁtting it) and then the neural network weights are compressed using\na model compression technique like pruning. To decode, the transmitted model weights are used in an\ninferencing pass to retrieve the frames. In particular, NeRV proposes a frame-based implicit representation vs\nthe pixel based approach in something like SIREN (Sitzmann et al. 2020). What this means practically is the\nNeRV takes a time 푡as input and produces the frame of the video at time 푡instead of taking the triple 푥, 푦, 푡\nand producing the pixel at position 푥, 푦at time 푡. Not only is the NeRV formulation simpler for the network\nto learn (leading to better results) it is also signiﬁcantly faster, requiring 푇forward passes to produce a video\nof length 푇instead of 퐻× 푊× 푇forward passes. While the overall idea here is interesting the results do\nleave something to be desired, as NeRV struggles to outperform even H.264. Furthermore, although the\nnetwork is small making decoding time fast, encoding (i.e., training the network) is on the order of hours.\nWe close the section with ELF-VC (Rippel et al. 2021), a method which is groundbreaking both in its results\nand in its methodical design. The approach is fast, provides a well motivated method for I- and P-frame\nencoding with deep networks, supports variable rate encoding, and compares well to other classical and deep\nlearning codecs. For the I-frame model, standard image deep learning compression is used. For P-frames,\nthe method is more interesting. Motion is predicted using a ﬂow model as in Lu et al. 2019 and the residual\nand ﬂows are both stored. The decoder uses a prior frame as an initial estimate of the warped frame before\nincorporating the ﬂow vectors and residual. Variable rate encoding is achieved using a level map where the\nrate-distortion curve is discretized in levels. The level is tiled spatially and used as input to the encoder and\ndecoder with the loss encouraging the network to hit the speciﬁed bitrate target. This provides a simple way\nto tune the bitrate. In terms of results the ELF-VC method largely outperforms other work on all benchmarks\nwith the exception of AV1, the latest in classical compression.\nAlthough ELF-VC hits on a number of ideas that would be required for a commercial video codec, that\ngoal is still very far oﬀ. With optimizations ELF-VC can decode a 1080p video at 18fps, which is not fast\nenough, and requires a GPU with a large amount of memory. As new methods are developed which are\nmore eﬃcient (this needs to be a continuing focus, however) and hardware speed increases, the likelihood of\ndeep learning compression ﬁnding its way to consumer applications increases. These reasons contribute to\nthe 10 year estimate.\nC.3 Lossless Techniques\nThe previous two section dealt exclusively with lossy compression. We expect that the networks will remove\ninformation from images during encoding, and even if they do not the quantization process will. But we can\nalso use machine learning for lossless compression. This takes a number of forms which we discuss in this\nsection. Importantly this is a fairly interesting use case and could potentially see practical application sooner\nthan the lossy methods although it would be in niche scenarios. For example, these techniques have uses in\nlossless transcoding of classically compressed images. This particular application is important because it\nwould allow large datacenters, which have the resources to run deep learning models at scale, to save on\nstorage costs by transcoding images and videos to a smaller deep learning based ﬁle. The media are then\nC.3 Lossless Techniques\n165\ntranscoded back to their consumer format before being transmitted so that the consumer does not need special\nhardware or software to view the media. When we discussed entropy coding in Chapter 4 (Entropy and\nInformation), we noted that entropy coders work by assigning shorter codes to probable symbols and longer\ncodes to improbable symbols. In order to work well the encoder needs an accurate probability distribution\nwhich is diﬃcult to come up with particularly for image data. The techniques in this section are primarily\nfocused on learning such distributions.\nPixelRNNs (Van den Oord et al. 2016; Van Oord et al. 2016) are generative models that predict each pixel in an\nimage as a discrete conditional distribution. This has an advantage over other generative methods like GANs\n(Goodfellow et al. 2014) because the model predicts the distribution explicitly instead of simply producing\nsamples from the distribution. In standard fashion, each pixel is treated as a distribution conditioned on all\nprevious pixels\n푝(푧) =\n푁\nY\n푖=푖\n푝(푧푖|푧푖−1, . . . , 푧1)\n(C.4)\nEach pixel can then be generated by sampling from the learned distribution pixel by pixel. So how is this\nrelevant to compression? With the likelihood of each pixel, we can use these distributions to produce\nprobabilities for entropy coders (Huﬀman 1952; Rissanen and Langdon 1979).\nInteger discrete ﬂows(IDFs) (Berg et al. 2020; Hoogeboom et al. 2019) are similar in spirit. The idea again is\nto learn an explicit distribution of the image data and produce a latent code from the distribution with the\nadvantage of much faster sampling. IDFs in particular are designed to overcome an explicit problem with\nﬂows in general: that they assume a continuous random variable. Images are discrete random variables so\nquantization of the resulting model to ﬁt the discrete distribution may introduce loss. By formulating an\ninteger discrete ﬂow, the authors can provably reproduce exactly the given input from a code. The ﬂow itself\nis based on a change of variables formula\n푃푋(푥) = 푃푧(푓(푥))\n\f\f\f\f\n휕푧\n휕푥\n\f\f\f\f\n(C.5)\nThe ﬂow is then reformulated to be in integer form where the Jacobian is one. The method was extended\nmore recently in iVPF (S. Zhang et al. 2021) which used volume preserving ﬂows instead of integer discrete\nﬂows (they are quite similar in operation however). In either case, the learned ﬂow can then be used directly\nas a probability distribution for entropy coding.\nWhile Bits-back encoding (Frey 1998; Frey and Geoﬀrey E Hinton 1996; Hinton and van Camp n.d.; C. S.\nWallace 1990) has been around for some time, the Bits-Back ANS (Townsend, Tom Bird, et al. 2019) method\nwas the ﬁrst algorithm using neural networks for the learning component and which was shown to be\neﬃcient on large datasets. Without going into too much detail, the idea of bits-back encoding is to assume\nthat the given symbol 푠has some latent variable 푦associated with it and that we have a way of measuring\n푝(푦), 푝(푠|푦), and 푝(푦|푠). Bits-back encoding allows us to leverage this knowledge of the latent distribution to\nstore 푠with fewer bits. Bits-back ANS uses a variational autoencoder (VAE) for the latent model. Bit-swap\n(F. Kingma et al. 2019) and Hilloc (Townsend, Thomas Bird, et al. 2019) extend this with hierarchical latent\nvariables, and LBB (Ho et al. 2019) merges ﬂows with bits-back encoding.\nWe close with a very diﬀerent approach, Mentzer, Gool, et al. 2020. This method actually leverages lossy\ncompression in order to improve the lossless compression rate. The idea is to start with BPG (Bellard 2018)\nand use a network to predict an optimal quantization parameter controlling how aggressive BPG should\nbehave. BPG of course loses information so the residual of the true frame and the compressed frame is taken\nand another network predicts the probability of the residual given the input image. The residual is then\nencoded using an entropy coder with the learned distribution. Since the encoded residual is stored with the\nBPG compressed image, there is no information loss.\nOverall, this ﬁeld is full of interesting and practical ideas. Although somewhat niche in their application,\nthese are highly developed techniques that could already be useful engineering applications. However, these\nideas are by deﬁnition not suitable for consumers as they are really one part of a more complex whole and\ntheir performance can not match lossy algorithms. Their use is more suited to specialized applications in\nmedical imaging, datacenters, or remote sensing where loss of data many not be acceptable.\nBibliography\nAgustsson, Eirikur, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and\nLuc V Gool\n2017 “Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations”, in\nAdvances in Neural Information Processing Systems, ed. by I. Guyon, U. V. Luxburg, S. Bengio, H.\nWallach, R. Fergus, S. Vishwanathan, and R. Garnett, Curran Associates, Inc., vol. 30. (Cited on\npage 162.)\nAgustsson, Eirikur, David Minnen, Nick Johnston, Johannes Balle, Sung Jin Hwang, and George Toderici\n2020 “Scale-space ﬂow for end-to-end optimized video compression”, in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 8503-8512. (Cited on page 124.)\nAgustsson, Eirikur and Radu Timofte\n2017 “Ntire 2017 challenge on single image super-resolution: Dataset and study”, in Proceedings of the\nIEEE conference on computer vision and pattern recognition workshops, pp. 126-135. (Cited on page 85.)\nAgustsson, Eirikur, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool\n2019 “Generative adversarial networks for extreme learned image compression”, in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 221-231. (Cited on pages 162, 163.)\nAhmed, Nasir, T_ Natarajan, and Kamisetty R Rao\n1974\n“Discrete cosine transform”, IEEE transactions on Computers, 100, 1, pp. 90-93. (Cited on pages v, 20.)\nArjovsky, Martin, Soumith Chintala, and Léon Bottou\n2017\n“Wasserstein generative adversarial networks”, in International conference on machine learning, PMLR,\npp. 214-223. (Cited on pages 121, 122.)\nBallé, Johannes, Valero Laparra, and Eero P Simoncelli\n2016 “End-to-end optimized image compression”, arXiv preprint arXiv:1611.01704. (Cited on pages vi,\n162.)\nBell, Sean, Paul Upchurch, Noah Snavely, and Kavita Bala\n2015\n“Material recognition in the wild with the materials in context database”, in Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 3479-3487. (Cited on pages 84, 121.)\nBellard, Fabrice\n2018 Better Portable Graphics, https://bellard.org/bpg/. (Cited on pages 53, 165.)\nBerg, Rianne van den, Alexey A Gritsenko, Mostafa Dehghani, Casper Kaae Sønderby, and Tim Salimans\n2020 “Idf++: Analyzing and improving integer discrete ﬂows for lossless compression”, arXiv preprint\narXiv:2006.12459. (Cited on page 165.)\nBitmovin\n2019 Video Developer Report 2019, https://go.bitmovin.com/video-developer-report-2019. (Cited\non pages 103, 122.)\nBolya, Daniel, Sean Foley, James Hays, and Judy Hoﬀman\n2020 “TIDE: A General Toolbox for Identifying Object Detection Errors”, in ECCV. (Cited on pages 98,\n153.)\nBoutell, Thomas\n1997\nPNG (Portable Network Graphics) Speciﬁcation Version 1.0, RFC 2083, doi: 10.17487/RFC2083, https:\n//www.rfc-editor.org/info/rfc2083. (Cited on page 52.)\nBruna, Joan and Stéphane Mallat\n2013 “Invariant scattering convolution networks”, IEEE transactions on pattern analysis and machine\nintelligence, 35, 8, pp. 1872-1886. (Cited on page 26.)\nCavigelli, Lukas, Pascal Hager, and Luca Benini\n2017\n“CAS-CNN: A deep convolutional neural network for image compression artifact suppression”, in\n2017 International Joint Conference on Neural Networks (ĲCNN), IEEE, pp. 752-759. (Cited on pages 72,\n75.)\nChang, S-F\n1992 “Video Compositing in the DCT domain”, in IEEE Workshop on Visual Signal Processing and\nCommunications, Raleigh, NC, Sep. 1992. (Cited on page 50.)\nCharbonnier, Pierre, Laure Blanc-Feraud, Gilles Aubert, and Michel Barlaud\n1994\n“Two deterministic half-quadratic regularization algorithms for computed imaging”, in Proceedings\nof 1st International Conference on Image Processing, IEEE, vol. 2, pp. 168-172. (Cited on page 119.)\nChen, Hao, Bo He, Hanyu Wang, Yixuan Ren, Ser-Nam Lim, and Abhinav Shrivastava\n2021 “NeRV: Neural Representations for Videos”, arXiv preprint arXiv:2110.13903. (Cited on pages 124,\n164.)\nChen, Honggang, Xiaohai He, Linbo Qing, Shuhua Xiong, and Truong Q Nguyen\n2018 “DPW-SDNet: Dual pixel-wavelet domain deep CNNs for soft decoding of JPEG-compressed\nimages”, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,\npp. 711-720. (Cited on pages 72, 75.)\nChetlur, Sharan, CliﬀWoolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and\nEvan Shelhamer\n2014\n“cudnn: Eﬃcient primitives for deep learning”, arXiv preprint arXiv:1410.0759. (Cited on page 60.)\nChu, Mengyu, You Xie, Jonas Mayer, Laura Leal-Taixé, and Nils Thuerey\n2020\n“Learning temporal coherence via self-supervision for GAN-based video generation”, ACM Transac-\ntions on Graphics (TOG), 39, 4, pp. 75-1. (Cited on pages 121, 126.)\nDalal, Navneet and Bill Triggs\n2005\n“Histograms of oriented gradients for human detection”, in 2005 IEEE computer society conference on\ncomputer vision and pattern recognition (CVPR’05), Ieee, vol. 1, pp. 886-893. (Cited on pages 37, 38,\n189.)\nDaniel, G, Johnnie Gray, et al.\n2018 “opt_einsum-a python package for optimizing contraction order for einsum-like expressions”,\nJournal of Open Source Software, 3, 26, p. 753. (Cited on page 60.)\nDaubechies, Ingrid\n1992 Ten lectures on wavelets, SIAM. (Cited on page 26.)\nDeguerre, Benjamin, Clément Chatelain, and Gilles Gasso\n2019 “Fast object detection in compressed jpeg images”, in 2019 ieee intelligent transportation systems\nconference (itsc), IEEE, pp. 333-338. (Cited on page 57.)\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei\n2009 “Imagenet: A large-scale hierarchical image database”, in 2009 IEEE conference on computer vision\nand pattern recognition, Ieee, pp. 248-255. (Cited on pages 55, 84, 96.)\nDeng, Jianing, Li Wang, Shiliang Pu, and Cheng Zhuo\n2020 “Spatio-Temporal Deformable Convolution for Compressed Video Quality Enhancement”, Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence, 34, 0707 (Apr. 2020), pp. 10696-10703, doi:\n10.1609/aaai.v34i07.6697. (Cited on pages 113, 115, 117, 123.)\nDing, Qing, Liquan Shen, Liangwei Yu, Hao Yang, and Mai Xu\n2021\n“Patch-Wise Spatial-Temporal Quality Enhancement for HEVC Compressed Video”, IEEE Transac-\ntions on Image Processing, 30, pp. 6459-6472. (Cited on pages 113, 123.)\nDong, Chao, Yubin Deng, Chen Change Loy, and Xiaoou Tang\n2015 “Compression artifacts reduction by a deep convolutional network”, in Proceedings of the IEEE\nInternational Conference on Computer Vision, pp. 576-584. (Cited on pages 71, 85.)\nDong, Chao, Chen Change Loy, Kaiming He, and Xiaoou Tang\n2014 “Learning a deep convolutional network for image super-resolution”, in European conference on\ncomputer vision, Springer, pp. 184-199. (Cited on pages 71, 75, 112.)\nDuggan, Maeve\n2013\n“Photo and video sharing grow online”, Pew research internet project. (Cited on page v.)\nEhrlich, Max, Jon Barker, Namitha Padmanabhan, Larry Davis, Andrew Tao, Bryan Catanzaro, and Abhinav\nShrivastava\n2022 “Leveraging Bitstream Metadata for Fast and Accurate Video Compression Correction”, arXiv\npreprint arXiv:2202.00011. (Cited on pages 115, 123.)\nEhrlich, Max and Larry Davis\n2019\n“Deep residual learning in the jpeg transform domain”, in Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 3484-3493. (Cited on page 55.)\nEhrlich, Max, Larry Davis, Ser-Nam Lim, and Abhinav Shrivastava\n2020\n“Quantization guided jpeg artifact correction”, in European Conference on Computer Vision, Springer,\npp. 293-309. (Cited on page 77.)\n2021 “Analyzing and Mitigating JPEG Compression Defects in Deep Learning”, in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 2357-2367. (Cited on pages 93, 133.)\nEinstein, Albert\n1923\n“Die grundlage der allgemeinen relativitätstheorie”, in Das Relativitätsprinzip, Springer, pp. 81-124.\n(Cited on page 11.)\nFoi, A, V Katkovnik, and K Egiazarian\nn.d.\n“Pointwise shape-adaptive DCT for high-quality deblocking of compressed color images “”, in Proc.\n14th Eur. Signal Process. Conf., EUSIPCO 2006. (Cited on pages 71, 85.)\nFrey, Brendan J\n1998 Bayesian networks for pattern classiﬁcation, data compression, and channel coding. Citeseer. (Cited on\npage 165.)\nFrey, Brendan J and Geoﬀrey E Hinton\n1996\n“Free energy coding”, in Proceedings of Data Compression Conference-DCC’96, IEEE, pp. 73-81. (Cited\non page 165.)\nFu, Xueyang, Zheng-Jun Zha, Feng Wu, Xinghao Ding, and John Paisley\n2019 “Jpeg artifacts reduction via deep convolutional sparse coding”, in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 2501-2510. (Cited on pages 75, 76.)\nFukushima, Kunihiko and Sei Miyake\n1982\n“Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recogni-\ntion”, in Competition and cooperation in neural nets, Springer, pp. 267-285. (Cited on page 63.)\nGalteri, Leonardo, Lorenzo Seidenari, Marco Bertini, and Alberto Del Bimbo\n2017\n“Deep generative adversarial compression artifact removal”, in Proceedings of the IEEE International\nConference on Computer Vision, pp. 4826-4835. (Cited on pages 73, 75.)\nGalteri, Leonardo, Lorenzo Seidenari, Marco Bertini, and Alberto Del Bimbo\n2019 “Deep universal generative adversarial compression artifact removal”, IEEE Transactions on Multi-\nmedia. (Cited on pages 73, 75-77.)\nCompuServe Inc\n1987 Graphics Interchange Format, Standard. (Cited on page 53.)\nGirshick, Ross\n2015\n“Fast r-cnn”, in Proceedings of the IEEE international conference on computer vision, pp. 1440-1448. (Cited\non page 96.)\nGoedegebure, Sacha, Andy Goralczyk, Enrico Valenza, Nathan Vegdahl, William Reynish, Brecht van Lommel,\nCampbell Barton, Jan Morgenstern, and Ton Roosendaal\n2008 Big Buck Bunny, https://peach.blender.org/. (Cited on pages 105, 123, 189.)\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio\n2014\n“Generative adversarial nets”, Advances in neural information processing systems, 27. (Cited on pages 42,\n83, 161, 162, 165.)\nGueguen, Lionel, Alex Sergeev, Ben Kadlec, Rosanne Liu, and Jason Yosinski\n2018 “Faster neural networks straight from jpeg”, Advances in Neural Information Processing Systems, 31.\n(Cited on page 56.)\nGuo, Jun and Hongyang Chao\n2016\n“Building dual-domain representations for compression artifacts reduction”, in European Conference\non Computer Vision, Springer, pp. 628-644. (Cited on pages 73, 75.)\nHart, Peter E, David G Stork, and Richard O Duda\n2000 Pattern classiﬁcation, Wiley Hoboken. (Cited on page 33.)\nHe, Kaiming, Georgia Gkioxari, Piotr Dollár, and Ross Girshick\n2017 “Mask r-cnn”, in Proceedings of the IEEE international conference on computer vision, pp. 2961-2969.\n(Cited on pages 97, 98, 153.)\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun\n2015\n“Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation”, in\nProceedings of the IEEE international conference on computer vision, pp. 1026-1034. (Cited on page 80.)\n2016 “Deep residual learning for image recognition”, in Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 770-778. (Cited on pages v, 40, 41, 55, 73, 96, 97, 189.)\nHeusel, Martin, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter\n2017 “Gans trained by a two time-scale update rule converge to a local nash equilibrium”, Advances in\nneural information processing systems, 30. (Cited on pages 42, 85, 123.)\nHinton, GE and Drew van Camp\nn.d. “Keeping neural networks simple by minimising the description length of weights. 1993”, in\nProceedings of COLT-93, pp. 5-13. (Cited on page 165.)\nHinton, Geoﬀrey E and Sam Roweis\n2002 “Stochastic neighbor embedding”, Advances in neural information processing systems, 15. (Cited on\npage 87.)\nHo, Jonathan, Evan Lohn, and Pieter Abbeel\n2019 “Compression with ﬂows via local bits-back coding”, Advances in Neural Information Processing\nSystems, 32. (Cited on page 165.)\nHochreiter, Sepp, Yoshua Bengio, Paolo Frasconi, Jürgen Schmidhuber, et al.\n2001 Gradient ﬂow in recurrent nets: the diﬃculty of learning long-term dependencies. (Cited on page 81.)\nHoogeboom, Emiel, Jorn Peters, Rianne Van Den Berg, and Max Welling\n2019\n“Integer discrete ﬂows and lossless compression”, Advances in Neural Information Processing Systems,\n32. (Cited on page 165.)\nHuﬀman, David A\n1952 “A method for the construction of minimum-redundancy codes”, Proceedings of the IRE, 40, 9,\npp. 1098-1101. (Cited on pages 29, 49, 165.)\nIndependant JPEG Group\nn.d. libjpeg, http://libjpeg.sourceforge.net. (Cited on pages 47, 85.)\nIoﬀe, Sergey and Christian Szegedy\n2015\n“Batch normalization: Accelerating deep network training by reducing internal covariate shift”, in\nInternational conference on machine learning, PMLR, pp. 448-456. (Cited on pages 41, 55, 60, 118.)\nIsola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros\n2017 “Image-to-image translation with conditional adversarial networks”, in Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 1125-1134. (Cited on page 42.)\nInternational Telecommunication Union\n2011\nStudio encoding parameters of digital television for standard 4:3 and wide-screen 16:9 aspect ratios, Standard,\nGeneva, CH. (Cited on page 48.)\n2021 Advanced video coding for generic audiovisual services, Standard, Geneva, CH. (Cited on page 109.)\nJain, AK and A fast Karhunen\n1976\n“Loeve Transform for a class of random processes”, IEEE Trans. Comm, 24, pp. 1023-1029. (Cited on\npage 20.)\nJiang, Jiaxi, Kai Zhang, and Radu Timofte\n2021\n“Towards ﬂexible blind JPEG artifacts removal”, in Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 4997-5006. (Cited on pages 76, 92.)\nJin, Zhi, Muhammad Zafar Iqbal, Wenbin Zou, Xia Li, and Eckehard Steinbach\n2020 “Dual-stream Multi-path Recursive Residual Network for JPEG Image Compression Artifacts\nReduction”, IEEE Transactions on Circuits and Systems for Video Technology. (Cited on pages 74, 75.)\nJohnson, Justin, Alexandre Alahi, and Li Fei-Fei\n2016 “Perceptual losses for real-time style transfer and super-resolution”, in European conference on\ncomputer vision, Springer, pp. 694-711. (Cited on page 84.)\nJolicoeur-Martineau, Alexia\n2018\n“The relativistic discriminator: a key element missing from standard GAN”, in International Conference\non Learning Representations. (Cited on page 83.)\nKang, Di, Debarun Dhar, and Antoni B Chan\n2016\n“Crowd counting by adapting convolutional neural networks with side information”, arXiv preprint\narXiv:1611.06748. (Cited on page 79.)\nKekre, HB and JK Solanki\n1978 “Comparative performance of various trigonometric unitary transforms for transform image\ncoding”, International Journal of Electronics Theoretical and Experimental, 44, 3, pp. 305-315. (Cited on\npage 20.)\nKim, Yoonsik, Jae Woong Soh, and Nam Ik Cho\n2020\n“AGARNet: adaptively gated JPEG compression artifacts removal network for a wide range quality\nfactor”, IEEE Access, 8, pp. 20160-20170. (Cited on pages 76, 92.)\nKim, Yoonsik, Jae Woong Soh, Jaewoo Park, Byeongyong Ahn, Hyun-Seung Lee, Young-Su Moon, and\nNam Ik Cho\n2019 “A pseudo-blind convolutional neural network for the reduction of compression artifacts”, IEEE\nTransactions on Circuits and Systems for Video Technology, 30, 4, pp. 1121-1135. (Cited on page 76.)\nKingma, Diederik P and Jimmy Ba\n2014 “Adam: A method for stochastic optimization”, arXiv preprint arXiv:1412.6980. (Cited on pages 84,\n122.)\nKingma, Friso, Pieter Abbeel, and Jonathan Ho\n2019\n“Bit-swap: Recursive bits-back coding for lossless compression with hierarchical latent variables”,\nin International Conference on Machine Learning, PMLR, pp. 3408-3417. (Cited on page 165.)\nKrizhevsky, Alex, Geoﬀrey Hinton, et al.\n2009\n“Learning multiple layers of features from tiny images”. (Cited on page 67.)\nKrizhevsky, Alex, Ilya Sutskever, and Geoﬀrey E Hinton\n2012 “Imagenet classiﬁcation with deep convolutional neural networks”, Advances in neural information\nprocessing systems, 25, pp. 1097-1105. (Cited on pages v, 33, 39.)\nLe Gall, Didier\n1991\n“MPEG: A video compression standard for multimedia applications”, Communications of the ACM,\n34, 4, pp. 46-58. (Cited on page v.)\nLeCun, Yann\n1998\n“The MNIST database of handwritten digits”, http://yann. lecun. com/exdb/mnist/. (Cited on pages 42,\n67.)\nLeCun, Yann, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard,\nand Lawrence D Jackel\n1990\n“Handwritten digit recognition with a back-propagation network”, in Advances in neural information\nprocessing systems, pp. 396-404. (Cited on pages 33, 36, 39.)\nLeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haﬀner\n1998\n“Gradient-based learning applied to document recognition”, Proceedings of the IEEE, 86, 11, pp. 2278-\n2324. (Cited on pages 39, 40, 189.)\nLi, Yinxiao, Pengchong Jin, Feng Yang, Ce Liu, Ming-Hsuan Yang, and Peyman Milanfar\n2021\n“COMISR: Compression-Informed Video Super-Resolution”, in Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pp. 2543-2552. (Cited on page 112.)\nLin, Tsung-Yi, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár\n2017\n“Focal loss for dense object detection”, in Proceedings of the IEEE international conference on computer\nvision, pp. 2980-2988. (Cited on page 96.)\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and\nC Lawrence Zitnick\n2014 “Microsoft coco: Common objects in context”, in European conference on computer vision, Springer,\npp. 740-755. (Cited on page 96.)\nLiu, Jerry, Shenlong Wang, Wei-Chiu Ma, Meet Shah, Rui Hu, Pranaab Dhawan, and Raquel Urtasun\n2020\n“Conditional entropy coding for eﬃcient video compression”, in Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16, Springer, pp. 453-468.\n(Cited on pages 124, 164.)\nLiu, Pengju, Hongzhi Zhang, Kai Zhang, Liang Lin, and Wangmeng Zuo\n2018\n“Multi-level wavelet-CNN for image restoration”, in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition Workshops, pp. 773-782. (Cited on pages 26, 72, 75, 85.)\nLiu, Xianming, Xiaolin Wu, Jiantao Zhou, and Debin Zhao\n2015 “Data-driven sparsity-based restoration of JPEG-compressed images in dual transform-pixel\ndomain”, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5171-\n5178. (Cited on pages 73, 75.)\nLo, Shao-Yuan and Hsueh-Ming Hang\n2019\n“Exploring semantic segmentation on the dct representation”, in Proceedings of the ACM Multimedia\nAsia, pp. 1-6. (Cited on page 56.)\nLoshchilov, Ilya and Frank Hutter\n2016 “Sgdr: Stochastic gradient descent with warm restarts”, arXiv preprint arXiv:1608.03983. (Cited on\npages 85, 96.)\nLowe, David G\n1999\n“Object recognition from local scale-invariant features”, in Proceedings of the seventh IEEE international\nconference on computer vision, Ieee, vol. 2, pp. 1150-1157. (Cited on pages 38, 189.)\nLu, Guo, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, and Zhiyong Gao\n2019\n“Dvc: An end-to-end deep video compression framework”, in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 11006-11015. (Cited on pages 124, 163, 164.)\nMaas, Andrew L, Awni Y Hannun, Andrew Y Ng, et al.\n2013\n“Rectiﬁer nonlinearities improve neural network acoustic models”, in Proc. icml, 1, Citeseer, vol. 30,\np. 3. (Cited on page 80.)\nMarpe, Detlev, Thomas Wiegand, and Gary J Sullivan\n2006\n“The H. 264/MPEG4 advanced video coding standard and its applications”, IEEE communications\nmagazine, 44, 8, pp. 134-143. (Cited on page 103.)\nMentzer, Fabian, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool\n2018\n“Conditional probability models for deep image compression”, in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 4394-4402. (Cited on page 162.)\nMentzer, Fabian, Luc Van Gool, and Michael Tschannen\n2020 “Learning better lossless compression using lossy compression”, in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 6638-6647. (Cited on page 165.)\nMentzer, Fabian, George D Toderici, Michael Tschannen, and Eirikur Agustsson\n2020\n“High-ﬁdelity generative image compression”, Advances in Neural Information Processing Systems, 33,\npp. 11913-11924. (Cited on page 163.)\nMercat, Alexandre, Marko Viitanen, and Jarno Vanne\n2020\n“UVG dataset: 50/120fps 4K sequences for video codec analysis and development”, in Proceedings\nof the 11th ACM Multimedia Systems Conference, pp. 297-302. (Cited on page 124.)\nMildenhall, Ben, Jonathan T Barron, Jiawen Chen, Dillon Sharlet, Ren Ng, and Robert Carroll\n2018\n“Burst denoising with kernel prediction networks”, in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 2502-2510. (Cited on page 79.)\nMiyato, Takeru, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida\n2018\n“Spectral normalization for generative adversarial networks”, arXiv preprint arXiv:1802.05957. (Cited\non pages 84, 121.)\nMPEG\n2013 Requirements for still image coding using HEVC, Standard, Vienna, AT. (Cited on page 53.)\nNair, Vinod and Geoﬀrey E Hinton\n2010\n“Rectiﬁed linear units improve restricted boltzmann machines”, in Icml. (Cited on page 63.)\nNash, John F\n1950\n“Equilibrium points in n-person games”, Proceedings of the national academy of sciences, 36, 1, pp. 48-49.\n(Cited on page 42.)\n1951\n“Non-cooperative games”, Annals of mathematics, pp. 286-295. (Cited on page 42.)\nNatarajan, Balas K and Bhaskaran Vasudev\n1995 “A fast approximate algorithm for scaling down digital images in the DCT domain”, in Image\nProcessing, 1995. Proceedings., International Conference on, IEEE, vol. 2, pp. 241-243. (Cited on page 50.)\nPaszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,\nZachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala\n2019 “PyTorch: An Imperative Style, High-Performance Deep Learning Library”, in Advances in Neural\nInformation Processing Systems 32, ed. by H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc,\nE. Fox, and R. Garnett, Curran Associates, Inc., pp. 8024-8035. (Cited on pages vi, 59, 84, 122.)\nPrakash, Aaditya, Nick Moran, Solomon Garber, Antonella DiLillo, and James Storer\n2017\n“Semantic perceptual image compression using deep convolution networks”, in 2017 Data Compres-\nsion Conference (DCC), IEEE, pp. 250-259. (Cited on page vi.)\nRadford, Alec, Luke Metz, and Soumith Chintala\n2015\n“Unsupervised representation learning with deep convolutional generative adversarial networks”,\narXiv preprint arXiv:1511.06434. (Cited on pages 84, 121.)\nRawzor\nn.d. Image Compression Benchmark, http://imagecompression.info/. (Cited on page 85.)\nRen, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun\n2016\n“Faster r-cnn: Towards real-time object detection with region proposal networks”, IEEE transactions\non pattern analysis and machine intelligence, 39, 6, pp. 1137-1149. (Cited on pages 96, 153.)\nRichardson, Iain E\n2011 The H. 264 advanced video compression standard, John Wiley & Sons. (Cited on page v.)\nRippel, Oren, Alexander G Anderson, Kedar Tatwawadi, Sanjay Nair, Craig Lytle, and Lubomir Bourdev\n2021 “Elf-vc: Eﬃcient learned ﬂexible-rate video coding”, in Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 14479-14488. (Cited on page 164.)\nRissanen, Jorma and Glen G Langdon\n1979\n“Arithmetic coding”, IBM Journal of research and development, 23, 2, pp. 149-162. (Cited on pages 49,\n165.)\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox\n2015\n“U-net: Convolutional networks for biomedical image segmentation”, in International Conference on\nMedical image computing and computer-assisted intervention, Springer, pp. 234-241. (Cited on pages 41,\n72, 189.)\nRosenblatt, Frank\n1957 The perceptron, a perceiving and recognizing automaton Project Para, Cornell Aeronautical Laboratory.\n(Cited on page 35.)\nSajjadi, Mehdi S. M., Raviteja Vemulapalli, and Matthew Brown\n2018 “Frame-Recurrent Video Super-Resolution”, in The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR). (Cited on page 111.)\nSandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen\n2018 “Mobilenetv2: Inverted residuals and linear bottlenecks”, in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 4510-4520. (Cited on pages 96, 97.)\nSchmit, Todd M and Roberta M Severson\n2021 “Exploring the feasibility of rural broadband cooperatives in the United States: The new New\nDeal?”, Telecommunications Policy, 45, 4, p. 102114. (Cited on page v.)\nSchuster, Mike and Kuldip K Paliwal\n1997\n“Bidirectional recurrent neural networks”, IEEE transactions on Signal Processing, 45, 11, pp. 2673-2681.\n(Cited on pages 113, 115.)\nSelesnick, Ivan W, Richard G Baraniuk, and Nick C Kingsbury\n2005\n“The dual-tree complex wavelet transform”, IEEE signal processing magazine, 22, 6, pp. 123-151. (Cited\non page 24.)\nSelvaraju, Ramprasaath R, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and\nDhruv Batra\n2017\n“Grad-cam: Visual explanations from deep networks via gradient-based localization”, in Proceedings\nof the IEEE international conference on computer vision, pp. 618-626. (Cited on page 152.)\nShannon, Claude Elwood\n1948 “A mathematical theory of communication”, The Bell system technical journal, 27, 3, pp. 379-423.\n(Cited on pages 27, 28, 162, 189.)\nSheikh, Hamid R, Muhammad F Sabir, and Alan C Bovik\n2006 “A statistical evaluation of recent full reference image quality assessment algorithms”, IEEE\nTransactions on image processing, 15, 11, pp. 3440-3451. (Cited on page 85.)\nShen, Bo and Ishwar K Sethi\n1995 “Inner-block operations on compressed images”, in Proceedings of the third ACM international\nconference on Multimedia, ACM, pp. 489-498. (Cited on page 50.)\nSimonyan, Karen and Andrew Zisserman\n2014\n“Very deep convolutional networks for large-scale image recognition”, arXiv preprint arXiv:1409.1556.\n(Cited on pages 40, 84, 96, 121.)\nSitzmann, Vincent, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein\n2020\n“Implicit neural representations with periodic activation functions”, Advances in Neural Information\nProcessing Systems, 33, pp. 7462-7473. (Cited on page 164.)\nSkodras, Athanassios, Charilaos Christopoulos, and Touradj Ebrahimi\n2001\n“The jpeg 2000 still image compression standard”, IEEE Signal processing magazine, 18, 5, pp. 36-58.\n(Cited on page 53.)\nSmith, Brian C\n1994 “Fast software processing of motion JPEG video”, in Proceedings of the second ACM international\nconference on Multimedia, ACM, pp. 77-88. (Cited on page 50.)\nSmith, Brian C and Lawrence A Rowe\n1993\n“Algorithms for manipulating compressed images”, IEEE Computer Graphics and Applications, 13, 5,\npp. 34-42. (Cited on page 50.)\nStock, Pierre, Armand Joulin, Rémi Gribonval, Benjamin Graham, and Hervé Jégou\n2020 “And the Bit Goes Down: Revisiting the Quantization of Neural Networks”, in ICLR 2020-Eighth\nInternational Conference on Learning Representations, pp. 1-11. (Cited on page vi.)\nSullivan, Gary J, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand\n2012\n“Overview of the high eﬃciency video coding (HEVC) standard”, IEEE Transactions on circuits and\nsystems for video technology, 22, 12, pp. 1649-1668. (Cited on page 103.)\nSun, Ke, Yang Zhao, Borui Jiang, Tianheng Cheng, Bin Xiao, Dong Liu, Yadong Mu, Xinggang Wang, Wenyu\nLiu, and Jingdong Wang\n2019 “High-resolution representations for labeling pixels and regions”, arXiv preprint arXiv:1904.04514.\n(Cited on page 97.)\nSvoboda, Pavel, Michal Hradis, David Barina, and Pavel Zemcik\n2016 “Compression\nartifacts\nremoval\nusing\nconvolutional\nneural\nnetworks”,\narXiv\npreprint\narXiv:1605.00366. (Cited on pages 72, 75.)\nSwartz, Charles S\n2004 Understanding digital cinema: a professional handbook, Routledge. (Cited on page 53.)\nSzegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,\nVincent Vanhoucke, and Andrew Rabinovich\n2015\n“Going deeper with convolutions”, in Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 1-9. (Cited on page 40.)\nSzegedy, Christian, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and Zbigniew Wojna\n2016\n“Rethinking the inception architecture for computer vision”, in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 2818-2826. (Cited on page 96.)\nTan, Mingxing and Quoc V Le\n2019 “Eﬃcientnet: Rethinking model scaling for convolutional neural networks”, arXiv preprint\narXiv:1905.11946. (Cited on pages v, 96.)\nTeed, Zachary and Jia Deng\n2020\n“Raft: Recurrent all-pairs ﬁeld transforms for optical ﬂow”, in European conference on computer vision,\nSpringer, pp. 402-419. (Cited on page 117.)\nTheis, Lucas, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszár\n2017\n“Lossy image compression with compressive autoencoders”, arXiv preprint arXiv:1703.00395. (Cited\non pages vi, 162.)\nToderici, George, Sean M O’Malley, Sung Jin Hwang, Damien Vincent, David Minnen, Shumeet Baluja,\nMichele Covell, and Rahul Sukthankar\n2015\n“Variable rate image compression with recurrent neural networks”, arXiv preprint arXiv:1511.06085.\n(Cited on pages vi, 161.)\nToderici, George, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and Michele\nCovell\n2017 “Full resolution image compression with recurrent neural networks”, in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 5306-5314. (Cited on pages vi, 161.)\nTomar, Suramya\n2006\n“Converting video formats with FFmpeg”, Linux Journal, 2006, 146, p. 10. (Cited on page 122.)\nTownsend, James, Thomas Bird, Julius Kunze, and David Barber\n2019 “Hilloc: Lossless image compression with hierarchical latent variable models”, arXiv preprint\narXiv:1912.09953. (Cited on page 165.)\nTownsend, James, Tom Bird, and David Barber\n2019 “Practical lossless compression with latent variables using bits back coding”, arXiv preprint\narXiv:1901.04866. (Cited on page 165.)\nTran, Trac D, Ricardo De Queiroz, and Truong Q Nguyen\n1998 “The generalized lapped biorthogonal transform”, in Proceedings of the 1998 IEEE International\nConference on Acoustics, Speech and Signal Processing, ICASSP’98 (Cat. No. 98CH36181), IEEE, vol. 3,\npp. 1441-1444. (Cited on page 71.)\nVan den Oord, Aaron, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al.\n2016 “Conditional image generation with pixelcnn decoders”, Advances in neural information processing\nsystems, 29. (Cited on page 165.)\nVan Oord, Aaron, Nal Kalchbrenner, and Koray Kavukcuoglu\n2016\n“Pixel recurrent neural networks”, in International conference on machine learning, PMLR, pp. 1747-1756.\n(Cited on page 165.)\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin\n2017\n“Attention is all you need”, in Advances in neural information processing systems, pp. 5998-6008. (Cited\non page 119.)\nVerizon Inc.\nn.d. 4G LTE Speeds vs. Your Home Network, https://www.verizon.com/articles/4g-lte-speeds-vs-\nyour-home-network/. (Cited on page v.)\nWallace, Chris S\n1990\n“Classiﬁcation by minimum-message-length inference”, in International Conference on Computing and\nInformation, Springer, pp. 72-81. (Cited on page 165.)\nWallace, Gregory K\n1992 “The JPEG still picture compression standard”, IEEE transactions on consumer electronics, 38, 1,\npp. xviii-xxxiv. (Cited on pages v, 47.)\nWang, Qilong, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu\n2020\n“ECA-Net: eﬃcient channel attention for deep convolutional neural networks, 2020 IEEE”, in CVF\nConference on Computer Vision and Pattern Recognition (CVPR). IEEE. (Cited on page 119.)\nWang, Tingting, Mingjin Chen, and Hongyang Chao\n2017 “A novel deep learning-based method of improving coding eﬃciency from the decoder-end for\nHEVC”, in 2017 Data Compression Conference (DCC), IEEE, pp. 410-419. (Cited on pages 112, 113.)\nWang, Xintao, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy\n2019 “Edvr: Video restoration with enhanced deformable convolutional networks”, in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 0–0. (Cited on\npages 112, 117.)\nWang, Xintao, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy\n2018\n“Esrgan: Enhanced super-resolution generative adversarial networks”, in Proceedings of the European\nConference on Computer Vision (ECCV), pp. 0–0. (Cited on pages 73, 80, 113, 118, 119, 148.)\nWang, Zhangyang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, and Thomas S Huang\n2016 “D3: Deep dual-domain based fast restoration of jpeg-compressed images”, in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pp. 2764-2772. (Cited on page 75.)\nWang, Zhou, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli\n2004\n“Image quality assessment: from error visibility to structural similarity”, IEEE transactions on image\nprocessing, 13, 4, pp. 600-612. (Cited on pages 82, 123.)\nWu, Chao-Yuan, Nayan Singhal, and Philipp Krahenbuhl\n2018 “Video compression through image interpolation”, in Proceedings of the European Conference on\nComputer Vision (ECCV), pp. 416-431. (Cited on pages 124, 163.)\nXiao, Tete, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun\n2018 “Uniﬁed perceptual parsing for scene understanding”, in Proceedings of the European Conference on\nComputer Vision (ECCV), pp. 418-434. (Cited on page 97.)\nXie, Saining, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He\n2017\n“Aggregated residual transformations for deep neural networks”, in Proceedings of the IEEE conference\non computer vision and pattern recognition, pp. 1492-1500. (Cited on page 96.)\nXing, Qunliang, Zhenyu Guan, Mai Xu, Ren Yang, Tie Liu, and Zulin Wang\n2021 “MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on Compressed Video”,\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 43, 3 (Mar. 2021), arXiv: 1902.09707,\npp. 949-963, doi: 10.1109/TPAMI.2019.2944806. (Cited on pages 113, 115, 117, 123.)\nXue, Tianfan, Baian Chen, Jiajun Wu, Donglai Wei, and William T. Freeman\n2019\n“Video Enhancement with Task-Oriented Flow”, International Journal of Computer Vision, 127, 8 (Aug.\n2019), arXiv: 1711.09078, pp. 1106-1125, doi: 10.1007/s11263-018-01144-2. (Cited on pages 112, 115,\n117, 122.)\nYang, Ren, Fabian Mentzer, Luc Van Gool, and Radu Timofte\n2020\n“Learning for video compression with hierarchical quality and recurrent enhancement”, in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6628-6637. (Cited on\npage 124.)\nYang, Ren, Mai Xu, Tie Liu, Zulin Wang, and Zhenyu Guan\n2018 “Enhancing quality for HEVC compressed videos”, IEEE Transactions on Circuits and Systems for\nVideo Technology, 29, 7, pp. 2039-2054. (Cited on pages 112, 113.)\nYang, Ren, Mai Xu, Zulin Wang, and Tianyi Li\n2018 “Multi-frame Quality Enhancement for Compressed Video”, in 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, IEEE, pp. 6664-6673, doi: 10.1109/CVPR.2018.00697.\n(Cited on pages 113, 115, 117, 121, 122.)\nYang, Ruihan, Yibo Yang, Joseph Marino, and Stephan Mandt\n2020 “Hierarchical\nautoregressive\nmodeling\nfor\nneural\nvideo\ncompression”,\narXiv\npreprint\narXiv:2010.10258. (Cited on page 124.)\nYang, Seungjoon, Surin Kittitornkun, Yu-Hen Hu, Truong Q Nguyen, and Damon L Tull\n2000\n“Blocking artifact free inverse discrete cosine transform”, in Proceedings 2000 International Conference\non Image Processing (Cat. No. 00CH37101), IEEE, vol. 3, pp. 869-872. (Cited on page 71.)\nYu, Ke, Chao Dong, Chen Change Loy, and Xiaoou Tang\n2016 “Deep convolution networks for compression artifacts reduction”, arXiv preprint arXiv:1608.02778.\n(Cited on page 71.)\nZhang, Richard, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang\n2018\n“The unreasonable eﬀectiveness of deep features as a perceptual metric”, in Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 586-595. (Cited on page 122.)\nZhang, Shifeng, Chen Zhang, Ning Kang, and Zhenguo Li\n2021\n“ivpf: Numerical invertible volume preserving ﬂow for eﬃcient lossless compression”, in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 620-629. (Cited on page 165.)\nZhang, Xiaoshuai, Wenhan Yang, Yueyu Hu, and Jiaying Liu\n2018 “DMCNN: Dual-domain multi-scale convolutional neural network for compression artifacts re-\nmoval”, in 2018 25th IEEE International Conference on Image Processing (ICIP), IEEE, pp. 390-394.\n(Cited on pages 74, 75, 85.)\nZhang, Yulun, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu\n2020 “Residual dense network for image restoration”, IEEE Transactions on Pattern Analysis and Machine\nIntelligence. (Cited on pages 73, 75.)\nZhao, Hengshuang, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia\n2017\n“Pyramid scene parsing network”, in Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 2881-2890. (Cited on page 97.)\nZhao, Minyi, Yi Xu, and Shuigeng Zhou\n2021 “Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact\nReduction”, in Proceedings of the 29th ACM International Conference on Multimedia, pp. 5646-5654.\n(Cited on pages 113, 123.)\nZhao, Xiangyu, Peng Huang, and Xiangbo Shu\n2022\n“Wavelet-Attention CNN for image classiﬁcation”, Multimedia Systems, pp. 1-10. (Cited on page 26.)\nZheng, Bolun, Yaowu Chen, Xiang Tian, Fan Zhou, and Xuesong Liu\n2019\n“Implicit dual-domain convolutional network for robust color image compression artifact reduction”,\nIEEE Transactions on Circuits and Systems for Video Technology. (Cited on pages 74, 75, 85.)\nZheng, Bolun, Rui Sun, Xiang Tian, and Yaowu Chen\n2018\n“S-Net: a scalable convolutional neural network for JPEG compression artifact reduction”, Journal of\nElectronic Imaging, 27, 4, p. 043037. (Cited on pages 73, 75.)\nZheng, Stephan, Yang Song, Thomas Leung, and Ian Goodfellow\n2016\n“Improving the robustness of deep neural networks via stability training”, in Proceedings of the ieee\nconference on computer vision and pattern recognition, pp. 4480-4488. (Cited on page 94.)\nZhou, Bolei, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba\n2016 “Semantic understanding of scenes through the ade20k dataset”, arXiv preprint arXiv:1608.05442.\n(Cited on page 97.)\n2017\n“Scene Parsing through ADE20K Dataset”, in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition. (Cited on page 97.)\nZini, Simone, Simone Bianco, and Raimondo Schettini\n2019 “Deep Residual Autoencoder for quality independent JPEG restoration”, arXiv preprint\narXiv:1903.06117. (Cited on page 76.)\nTerminology\nB\nbasis A set of vectors which is linearly independent and spans a vector space.. 6, 7, 18\nBayesian decision theory A method for making optimal decisions given perfect probability distributions\ndescribing possible events.. 33\nC\nchroma subsampling The process for storing chrominance channels at a smaller resolution since human\nvision is less sensitive to changes in color information.. 48, 50\nchrominance The color or hue of light captured by a sensor.. 48\ncommunication system A system for conveying some message from one party to another. Consists of an\ninformation source, a transmitter, a signal, a source of noise corrupting the signal, a receiver, and a\ndestination.. 28\ncompression Any operation which reduces the size in bits of a computational object.. vi\nconvolution The correlation of a signal and a kernel as the kernel is shifted across the signal.. 14, 24\nconvolutional ﬁlter manifold A modiﬁcation of the ﬁlter manifold which uses a spatial input.. 79\ncross-correlation See convolution (although they are technically diﬀerent).. 14\nD\ndecision boundary The manifold in space separating classiﬁcation decisions.. 35, 37\ndeep learning A machine learning technique that learns many layers of features jointly with a task objective..\nv, vi, 39, 55, 71\ndissertation a paricular type of write only document.. vi, 11, 33\nE\nentropy The amount of information in a message, the amount of randomness in a system, the minimum\nnumber of bits required to encode a message.. 27, 47\nerror residual The diﬀerence between a motion compensated frame and the true frame.. 106\nevidence The probability of an observation.. 34\nF\nfeature An abstract or higher order representation of an image or a part of an image that is more suitable for\ninput to a machine learning algorithm.. 37\nﬁlter manifold A method for learning adaptable convolution kernels from a scalar input.. 79\nﬁrst principles The underlying engineering decisions which motivate an algorithm.. vi, 129\nFourier transform An integral transform deﬁning an orthogonal basis for functions.. 19, 21–24\nG\nGabor transform A special case of the STFT which uses a Gaussian ﬁlter to window the transform.. 22, 23\ngradient For a scalar valued function of a vector: the vector of partial derivatives of each component of the\ninput with respect to the scalar output.. 36\nH\nHadamard transform An approximation of the discrete cosine transform consisting of only 1s and -1s.. 21,\n108\nHuﬀman coding A method for producing optimal length codes for single symbols given the probabilities\nthat each symbol will occur.. 29\nI\nimage A discrete 2D signal giving a sample value at integer positions (푥, 푦), the sample may be a scalar\n(grayscale) or a vector (color).. 12, 15, 17\nimage-to-image The machine learning problem which takes an image as input and produces an image as\noutput.. 41\ninterlaced A method for storing color images which stores color information in sequence, e.g., each pixel\ncould consist of 24 bits with 8 bits for res, green, and blue.. 48\nJ\nJacobian For a tensor valued function of a tensor: the tensor of partial derivatives of each component of the\ninput with respect to each component of the output.. 37, 40\nJPEG The Joint Photographic Experts Group, often referring to an image ﬁle or compression algorithm.. v,\nvi, 20, 27, 47, 52, 55, 56, 64, 71, 77, 80, 103\nL\nlikelihood The probability of an observation given that some event occurs.. 34\nlinear combination A series of scalar multiplication and vector addition.. 4, 5, 8, 19\nlinear map A mapping which preserves scalar multiplication and vector addition.. 9, 14, 15, 20\nlinearly separable A decision which is able to be made using only the relative position with respect to a\nhyperplane.. 36\nlossless compression A compression operation which preserves all information in the original signal.. 29\nlossy compression A compression operation which removes information from the signal to save space.. 27,\n47\nluminance The brightness (“quantity” of light) captured by a sensor.. 48, 49, 77\nM\nmacroblocks Pixel blocks in a frame which are larger than the transform block size.. 107\nmetric tensor A tensor which relates a vector space and a co-vector space.. 11\nmotion compensation The process by which a video codec warps frames using estimated motion.. 106\nmotion estimation The process by which a video encoder measures block motion.. 106\nMotion JPEG A video codec which stores each frame as a JPEG.. 104\nmotion vectors Vector specifying the motion of video blocks.. 105\nMPEG The Motion Picture Experts Group, often referring to a compression algorithm.. vi, 27\nmultilinear map A mapping which is linear in exactly one argument.. 9, 10\nmultiresolution analysis See wavelet transform.. 23\nN\nNash equilibrium The state of a game where no player can obtain an advantage over any other player.. 42\nNyquist Sampling Theorem A signal with a maximum frequency 휁푚can be represented exactly by discrete\nsamples with a sampling rate of at least 2휁푚.. 24\nP\nplanar A method for storing color images which stores color information separately, e.g., the image may\nconsist of all the red pixels followed by all the blue pixels, etc... 48\nposterior probability The probability of an event occurring given an observation.. 34\nprior probability The probability of an event occurring in the absence of any other information.. 33\nR\nrate control Any method for tuning the bitrate of an image or video.. 107, 108\nS\nsemantic segmentation The machine learning problem which takes an image as input and produces a\nclassiﬁcation label for each pixel.. 41\nslices Regions of a video frame consisting of a whole number of macroblocks.. 107\nSVM Support Vector Machine, a linear model which separates examples using the maximum margin\nhyperplane.. 38\nT\ntransform domain A catch-all term for DCT coeﬃcients, quantized JPEG data, or any other transformation\nof pixel data.. 56, 60, 64\nV\nvideo A discrete 3D signal giving a sample value at integer positions (푥, 푦, 푡), the sample may be a scalar\n(grayscale) or a vector (color).. 17\nW\nwavelet A wave-like function with ﬁnite support.. 23, 24\nwavelet transform An integral transform using a set of wavelets as the basis, allows for multi-resolution\nand localization of frqeuencies in time.. 23, 24, 26\nAcronyms\nC\nCNN Convolutional Neural Network. 33, 39, 40, 42, 43, 57\nCWT Continuous Wavelet Transform. 23\nD\nDCT Discrete Cosine Transform. 20, 48–50, 56, 57, 78, 80, 81, 108\nDFT Discrete Fourier Transform. 19, 20, 22\nDST Discrete Sine Transform. 20, 108\nDTCWT Dual Tree Complex Wavelet Transform. 24\nDWT Discrete Wavelet Transform. 24\nE\nEXIF Exchangeable Image File Format. 47\nF\nFCR Frequency-Component Rearrangement. 57\nG\nGAN Generative Adversarial Network. 42, 43\nJ\nJFIF JPEG File Interchange Format. 47, 77\nM\nMCU Minimum Coded Unit. 48\nMLP Multilayer Perceptron. 36, 39, 41, 79\nR\nRRDB Residual-in-Residual Dense Block. 80, 81, 118\nS\nSTFT Short-Time Fourier Transform. 22, 23\nIndex\napproximated spatial masking,\n65\narcnn, 71\nB-Frame, 105\nbackpropagation, 36\nbasis, 6\nbatch normalization, 60\nBayes decision rule, 34\nBayes rule, 34\nbayesian decision theory, 33\nbiorthogonal, 24\nbipredcted frame, 103\nbipredicted frame, 116\ncanonical basis, 18\nchange of basis, 6\nchroma subsampling, 48\nco-vector, 3, 10\ncodec, 103\ncompression, v\nconstant bitrate, 108\nconstant quantization\nparameter, 107\nconstant rate factor, 108\ncontinuous wavelet transform,\n23\nconvolution, 14\nconvolutional ﬁlter manifold,\n79\nconvolutional neural network,\n39\nconvolutional neural\nnetworks, 33, 57\ncross-correlation, 14\nDaubechies wavelet, 25, 26\nDCT, 56\ndct, 48–50\ndecision boundary, 35\ndeep learning, v, 39\ndiﬀerence of gaussians, 38,\n120\ndiscrete cosine transform,\n20\ndiscrete fourier transform,\n19\ndiscrete sine transform, 20\ndiscrete wavelet transform,\n24\ndiscriminator, 42\ndissertation, vi\ndual domain, 73\ndual tree complex wavelet\ntransform, 24\nentropy, 27\nerror residual, 106\nEXIF, 47\nfeature, 34\nﬁeld, 8\nﬁlter manifold, 79\nﬁnite dimensional vector\nspace, 6\nﬁnite support, 24\nﬁrst principles, vi\nfourier transform, 17\nframe, 103\nfrequency component\nrearrangement, 57,\n80\ngeneralize lapped\nbiorthogonal\ntransform, 71\ngenerative adversarial\nnetwork, 83\ngenerative adversarial\nnetworks, 42\ngenerator, 42\ngroup of pictures, 104, 116\nHaar wavelet, 25\nHadamard transform, 21\nharmonic analysis, 17\nheat equation, 17\nHeaviside function, 36\nhistogram of oriented\ngradients, 37\nidentity matrix, 5\nin-loop ﬁlter, 111\nindex juggling, 11\ninformation theory, 27\ninner product, 4\ninteger discrete ﬂows, 165\nintegral transform, 19\nintra frame, 103, 116\nintra slice, 107\nJFIF, 47\nJPEG, 47, 52, 55, 56, 71\nkernel predictor, 79\nkeypoint, 38\nkingma2013auto, 165\nl2 norm, 4\nlevel map, 164\nlinear algebra, 3\nlinear combination, 4, 8\nlinear map, 47\nlinear pixel manipulations,\n12\nlong-short-term memory,\n161\nlossless compression, 27, 164\nlossy compression, 47, 164\nlow-delay p mode, 124, 126\nmachine learning, 33\nmacroblocks, 107\nmatrix, 5, 9\nmetric tensor, 11\nminimum coded unit, 48\nMorlet wavelet, 25\nmother wavelet, 23\nmotion compensation, 106\nmotion estimation, 106\nmotion vector, 105\nmultilayer perceptron, 36,\n79\nmultiresolution analysis, 23\nnormalize, 4\nnyquist sampling theorem,\n24\northogonal, 4\northonormal, 6, 18\nP-frame, 105\npeak quality frame, 113\npeak quality frames, 115\nperceptron, 35\nposterior probabilities, 34\npredicted frame, 103, 116\npredicted slice, 107\nprior probability, 33\nquantization, 164\nquantization matrix, 49\nrate control, 107\nresidual block, 41, 80\nresidual learning, 55\nresidual networks, 40\nresidual-in-residual dense\nblock, 80\nrun-length code, 49\nscalar, 3, 8\nscale space, 120\nscale-invariant, 38\nscale-invariant feature\ntransform, 37, 38\nshape-adaptive discrete cosine\ntransform, 71\nshort-time fourier transform,\n22\nskip connections, 42\nslice, 107\nspan, 6\nsparse coding, 75\nstructural similarity, 82\ntask-targeted artifact\ncorrection, 93\ntensor, 9\ntranscode, 164\ntransform domain, 56, 57, 61\ntranspose, 3\nu-net, 41\nvanishing gradient, 40\nvanishing moments, 26\nvector, 3, 8–10\nvector space, 8\nvideo compression reduction,\n111\nwavelet, 23\nwavelet transforms, 23\nFigure Credits\nUnless listed here, ﬁgures are either generated by the author, in the public domain. The original authors of\nthese works do not endorse any changes made for this document.\nFigure 3.2 on page 23 Wikipedia. User JonMcloone\nhttps://commons.wikimedia.org/wiki/File:MorletWaveletMathematica.svg.\nCC-BY-SA 3.0.\nRemoved axes.\nFigure 3.3 on page 23 Wikipedia. User JonMcloone\nhttps://commons.wikimedia.org/wiki/File:MorletWaveletMathematica.svg.\nCC-BY-SA 3.0.\nRemoved axes, added scaled version to show hierarchy.\nFigure 3.5 on page 25 Wikipedia. User Omegatron\nhttps://commons.wikimedia.org/wiki/File:Haar_wavelet.svg.\nCC-BY-SA 3.0.\nRemoved axes, added scaled version to show hierarchy.\nFigure 4.1 on page 28 Shannon, “A mathematical theory of communication”\nFigure 5.2 on page 36 Wikipedia. User Glosser.ca\nhttps://commons.wikimedia.org/wiki/File:Colored_neural_network.svg.\nCC-BY-SA 3.0.\nFigure 5.3 on page 38 Dalal and Triggs, “Histograms of oriented gradients for human detection”\nFigure 5.4 on page 38 Lowe, “Object recognition from local scale-invariant features”\nFigure 5.5 on page 39 LeCun, Bottou, et al., “Gradient-based learning applied to document recognition”\nFigure 5.6 on page 41 Ronneberger et al., “U-net: Convolutional networks for biomedical image segmenta-\ntion”\nFigure 5.7 on page 41 He, Xiangyu Zhang, et al., “Deep residual learning for image recognition”\nFigure 11.3 on page 105 Big Buck Bunny. Goedegebure et al., Big Buck Bunny\nhttps://peach.blender.org/\nCC-BY-SA 3.0.\nMotion vector arrows added to frame.\n",
  "categories": [
    "eess.IV",
    "cs.CV",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2022-04-04",
  "updated": "2022-04-04"
}