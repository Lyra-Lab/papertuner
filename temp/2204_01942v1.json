{
  "id": "http://arxiv.org/abs/2204.01942v1",
  "title": "Fault-Tolerant Deep Learning: A Hierarchical Perspective",
  "authors": [
    "Cheng Liu",
    "Zhen Gao",
    "Siting Liu",
    "Xuefei Ning",
    "Huawei Li",
    "Xiaowei Li"
  ],
  "abstract": "With the rapid advancements of deep learning in the past decade, it can be\nforeseen that deep learning will be continuously deployed in more and more\nsafety-critical applications such as autonomous driving and robotics. In this\ncontext, reliability turns out to be critical to the deployment of deep\nlearning in these applications and gradually becomes a first-class citizen\namong the major design metrics like performance and energy efficiency.\nNevertheless, the back-box deep learning models combined with the diverse\nunderlying hardware faults make resilient deep learning extremely challenging.\nIn this special session, we conduct a comprehensive survey of fault-tolerant\ndeep learning design approaches with a hierarchical perspective and investigate\nthese approaches from model layer, architecture layer, circuit layer, and cross\nlayer respectively.",
  "text": "Special Session - Fault-Tolerant Deep Learning: A\nHierarchical Perspective\nCheng Liu ∗, Zhen Gao †, Siting Liu ‡¶, Xuefei Ning §, Huawei Li ∗∥and Xiaowei Li ∗\n∗SKLCA, Institute of Computing Technology, Chinese Academy of Sciences\n† School of Electrical and Information Engineering, Tianjin University\n‡ School of Information Science and Technology, ShanghaiTech University\n§ Department of Electronic Engineering, Tsinghua University\n¶ Shanghai Engineering Research Center of Energy Efﬁcient and Custom AI IC\n∥Peng Cheng Laboratory, Shen Zhen\nAbstract—With the rapid advancements of deep learning in\nthe past decade, it can be foreseen that deep learning will be\ncontinuously deployed in more and more safety-critical applica-\ntions such as autonomous driving and robotics. In this context,\nreliability turns out to be critical to the deployment of deep\nlearning in these applications and gradually becomes a ﬁrst-\nclass citizen among the major design metrics like performance\nand energy efﬁciency. Nevertheless, the back-box deep learning\nmodels combined with the diverse underlying hardware faults\nmake resilient deep learning extremely challenging. In this special\nsession, we conduct a comprehensive survey of fault-tolerant deep\nlearning design approaches with a hierarchical perspective and\ninvestigate these approaches from model layer, architecture layer,\ncircuit layer, and cross layer respectively.\nI. INTRODUCTION\nDeep learning has been demonstrated to be successful in\na plethora of applications including computer vision [1] [2]\nand natural language processing [3], and is gaining increasing\nattention of researchers from a broad disciplines such as\nengineering [4], biology [5], and chemistry [6]. It can be\nexpected that deep learning will be applied in more and more\nsafety-critical applications like autonomous driving, avionics,\nand robotics [7] [8], which typically require highly reliable\nprocessing to avoid catastrophic consequences. There have\nbeen intensive efforts devoted to enhance the robustness of\ndeep learning against various perturbations like adversarial\nnoise and natural noise in deep learning community to ensure\nsafety deployment of deep learning [9] [10] [11]. In contrast,\nthe inﬂuence of hardware faults in silicon-based computing\nfabrics such as deep learning accelerators, FPGAs, and GPUs\nthat sustain efﬁcient deep learning processing are generally\noverlooked.\nDeep learning models especially neural networks are known\nto be fault-tolerant inherently mainly because of the widely\nutilized activation functions, pooling layers, and the ranking-\nbased outputs that are usually insensitive to computing varia-\ntions. Many prior work explored the inherent fault tolerance\nof neural networks for the sake of higher energy efﬁciency,\nperformance, and memory footprint with approaches like\nvoltage scaling [12] [13], DRAM refresh scaling [14], and\nlow-bit-width quantization [15] [16]. However, the unique\nfault-tolerant feature does not guarantee fault tolerance against\nhardware faults and even results in substantial accuracy vari-\nation across the different fault conﬁgurations according to the\ninvestigation in [17] [18] [19], which essentially aggravates\nthe uncertainty of the deep learning processing and hinders the\ndeployment of deep learning in safety-critical applications.\nPrior approaches that are proposed to enhance the robust-\nness of neural networks (NN) against the adversarial noise\nor natural noise can also guide fault-tolerant neural network\nprocessing against hardware faults in certain extent [20] [10],\nbut the effectiveness can be limited because of the distinct\nmechanisms of inﬂuence on neural network processing. The\nperturbations caused by adversarial noise and natural noise\nonly affect inputs of neural network processing while weights\nand neurons are generally intact. In contrast, hardware faults\nmostly pose more varied inﬂuence on neural network process-\ning [17] [18] [21]. Speciﬁcally, faults in the on-chip buffers\nof the computing fabrics can affect not only neural network\ninputs but also weights and intermediate features of neural\nnetworks. Moreover, hardware faults can also be located at\ncomputing logic of neural network processing engines and\ncorrupt the computing of neural network processing directly.\nAs a result, it is usually more difﬁcult to characterize the\ninﬂuence of hardware faults on neural network processing and\nthey may pose distinct inﬂuence on neural network processing\n[22] [23] [24] [25]. Thereby, systematic fault-tolerant design\napproaches against various hardware faults remain highly\ndemanded for the deployment of neural networks in safety-\ncritical applications.\nIn order to mitigate the inﬂuence of hardware faults on\nneural network processing, a number of approaches from\nvarious angles have been proposed. Relevant surveys [26]\n[27] [28] [29] mainly focus on fault-tolerant training ap-\nproaches and many recent fault-tolerant approaches based\non architectural design and circuit design are not classiﬁed\nor included. In this work, we investigate the fault-tolerant\ndeep learning approaches from a hierarchical perspective that\nmatches the general deep learning processing stacks all the\nway from high-level models to low-level circuits. With a top-\ndown perspective, we have existing fault-tolerant deep learning\ndesign approaches divided into model layer, architecture layer,\nand circuit layer respectively. In addition, we observe that\narXiv:2204.01942v1  [cs.AR]  5 Apr 2022\nmany approaches may cover multiple layers at the same time\nand brieﬂy introduce the cross-layer approaches as well. Model\nlayer fault-tolerant design approaches typically explore the\ninherent fault tolerance and redundancy in neural network\nmodels and have the models desensitized to computing varia-\ntions and input variations induced by various hardware faults.\nArchitecture layer fault-tolerant approaches mitigate hardware\nfaults with specialized neural network accelerator architectures\nsuch as online recomputing [17] and runtime voting [30].\nCircuit layer fault-tolerant approaches focus on low-level\ncircuit designs such as error-tolerant encoding, ﬁne-grained\nmodular redundancy [31], and stochastic circuits [32]. Cross-\nlayer fault-tolerant approaches usually combine fault-tolerant\napproaches from different layers in a uniﬁed framework to\nmake best use of the different approaches and achieve more\neffective protection [19].\nThe organization of this paper can be summarized as\nfollows. Section II mainly introduces fault-tolerant model\ndesign approaches against hardware faults. Section III focuses\non fault-tolerant architectural design approaches particularly\nfor deep learning accelerators. Section IV mainly covers the\ncircuit-based fault-tolerant design approaches with an em-\nphasis on approximate computing and stochastic computing.\nSection V brieﬂy introduces the cross-layer design approaches.\nSection VI concludes this paper.\nII. MODEL-LAYER FAULT TOLERANCE\nA. Related Work\nThis section introduces the model-level fault tolerance\ntechniques for NN applications, which lie at the top of\nthe hierarchy. One can presume that low-level techniques\n(i.e., the architecture-level and circuit-level techniques) can\nprovide better reliability guarantees since they could directly\nhandle more types of actual hardware faults. Nevertheless,\npurely relying on low-level fault tolerance techniques for fault-\ntolerant deep learning can be prohibitively costly, especially\nconsidering the ongoing trend of scaling up the NN model\ncapacities. Therefore, there exists a vast literature on exploiting\nthe application-level or model-level characteristics of NNs to\nfacilitate more economical fault-tolerant DL.\nThe key characteristic utilized by all model-level techniques\nis the NNs’ inherent redundancy and tolerance for faults. In\nother words, most NNs have only a fraction of neurons (sen-\nsitivity neurons), whose computational faults induce severe\nfunctional errors. Thus, all model-level techniques could be\nseen as designed revolving around neuron sensitivity. And we\nconclude them into three types, sensitivity analysis methods\nto assess neuron sensitivity, training strategies to alleviate or\ncompensate for neuron sensitivities, and model architecture\ndesigns to eliminate or decrease the number of sensitive\nneurons.\n1) Sensitivity Analysis: To exploit the neural network (NN)\napplications’ characteristics for a more economical fault-\ntolerant DL solution, one should ﬁrst understand the behavior\nof NN models with computational faults by conducting sen-\nsitivity analysis. Vialatte and Leduc-Primeau [33] analyze the\nlayer-wise sensitivity of NN models under two fault models.\nF. Libano et al. [34] conduct layer-wise sensitivity analysis,\nand then propose to only triplicate the vulnerable layers, and\nthus reduce the triple modular redundancy (TMR) overhead for\nprotecting an NN model. Christoph Schorn et al. [35] propose\na bit-ﬂip resilience evaluation metric, and conduct sensitivity\nanalysis of each individual neuron. The authors further reﬁne\ntheir analysis model in [36]. Guanpeng Li et al. [37] ﬁnd that\nthe impacts and propagation of computational faults in an NN\ncomputation system depend on the hardware data path, the\nmodel topology, and the type of layers. These methods analyze\nthe sensitivity of existing NN models at different granularities,\nand many of them also propose to exploit the analysis results\nto reduce the hardware overhead for reliability.\n2) Training Strategy: Fault-tolerant training is one of the\ncommonest techniques to enhance the fault tolerance capa-\nbility of an NN model. In order to alleviate the inﬂuences\ninduced by faults, many prior studies [38] [39] [40] [41] [42]\n[43] [44] establish random weights or feature fault models,\nand inject faults accordingly during training. In this way, the\nNN models can learn to tolerate these types of faults. In other\nwords, the sensitivities of neurons are alleviated such that their\ncomputational faults no longer lead to functional errors.\nInstead\nof\nstatistically\ninjecting\nfaults\ninto\ntraining,\nChristoph Schorn et al. [36] propose a training strategy using\nanalytical sensitivity analysis to adjust neuron sensitivities.\nThis work argues that achieving a homogeneous resilience\ndistribution inside the DNN can help obviate the need for\nspecial protection of critical parts. Accordingly, this work\nproposes an explicit weight rescaling technique to equalize\nthe sensitivity metrics of different channels in one layer, and\nconducts the rescaling and ﬁnetuning processes iteratively.\nOther regularization techniques have also been proposed.\nAs neural network outputs are usually more sensitive to large\nmagnitude weights [45], weight decay that limits the mag-\nnitude of weights is proposed to improve the fault tolerance\n[46][47]. Different from the training approaches with direct\nfault injection, Chi-Sing Leung et al. [48] proposed a new\nobjective function with an additional regularization term to\nminimize the training set errors and obtain fault-tolerant radial\nbasis function (RBF) networks. It targets both weight fault and\nmultiplicative weight noise.\nAnother type of studies [49][50][51][19] design retraining\nstrategies to compensate for the performance loss caused by\ndefunct neurons. These methods retrain the NN model to\nrestore the model performance, after the online detection of\nthe actual faults and variations.\n3) Model Architecture Design: As the output neurons di-\nrectly inﬂuence the output, Tao Liu et al. [52] propose to use\nerror-correcting output codes (ECOC) [53] to tolerate vari-\nations and SAFs. Speciﬁcally, they replace the conventional\nsoftmax with a collaborative logistic classiﬁer that leverages\nasymmetric binary classiﬁcation coupled with an optimized\nvariable-length decode-free ECOC.\nChing-Tai Chiu et al. [54] propose to add additional hidden\nnodes to avoid model accuracy loss and repeatedly remove\nNetwork Architecture\n(Rollout)\nRNN\nController\nSample\nCandidate\nNetwork\nSuper-net\nEvaluator\nReward\nObjective\nLoss\nAccuracy\nFaulty\nAccuracy\nClean Cross \nEntropy\nFaulty Cross \nEntropy\nFTT-NAS Framework\n(b)\nDiscovered \nArchitecture\nFault-Tolerant Training\nFault-Tolerant \nTraining \nFault-Tolerant\nNN Model\n(c)\nApplication-level Statistical Fault Model\n(a)\nRRAM-based PIM Accelerator\n!!\nDevice Fault 1:\nStuck-At-Fault (SA0, SA1)\n……\n- +\n- +\n- +\n- +\n……\n……\n!!\n!\"\n!!\n!!\n!!\nRRAM\ng&,(\n!$% = #\n&\n!'& ⋅%&% ,\n%&% = −\n(&%\n((\nw ~ 1,-./0123(5); 7$, 7*)\nw ~ 9,-./0123(5); 7$, 7*)\nDevice Fault 2:\nResistance Variation\n\"!\"#$%\n\"\nw ~ :;%-7<=%/>?=<@/> 5); A\nw ~ B=(?=<@/> 5); A\nREG\n!!$\nREG\n!!%\n!!&\nREG\n……\nMAC\nREG\n!!$\nREG\n!!%\n!!&\nREG\n……\nMAC\nSRAM bank\nFeature Buffer\n…\nSRAM bank\nSRAM bank\nWeight Buffer\n…\nSRAM bank\nProcess Elements\nREG\n!!$\nREG\n!!%\n!!&\nREG\n……\nMAC\nSEU in Feature Buffer\nSEU in Weight Buffer\ny ~ -C3(D); <()\nw ~ -C3(5); <()\nASIC CNN Accelerator\nLogic gate-based Computing Unit\ne.g., 1-bit Full Adder\n\"\n#\n$'\n%\n$#\n\"\n#\n$'\n0\n1\nSEU at Gates\nf ~ -CC(E); <(+ F+)\nFPGA CNN Accelerator\nLUT-based Computing Unit\ne.g., Adder with Carry Chain\n#\"\n$\"#$\n%\"#$\nLUT\n$\"\n%\"\nCarry \ngates\nLUT\nCarry \ngates\n#\"#$\n&\"\n&\"#$\n…\nSEU in LUTs\nf ~ -CC(E); <(F+F,(.))\nFig. 1: The FTT-NAS workﬂow: (a) Establish algorithm-level fault models. (b) Parameter-sharing neural architecture search.\n(c) Fault-tolerant training of discovered architectures.\nnodes that do not signiﬁcantly affect the network output. A.\nAhmadi et al. [55] proposed to add a spare neuron which can\nbe reconﬁgured to compare with any neuron in the model.\nIt can be used for both fault detection and correction, but\nit is limited to spatial neural network architecture and can\nonly be used to recover from single faults. FTSET [56]\nuses simulation to analyze the sensitivity and replicates the\ncritical neurons. Christoph Schorn et al. [36] conduct some\nsimple manual architecture modiﬁcations to eliminate sensitive\nneurons identiﬁed by analytical analysis.\nApart from these manual architecture designs and modiﬁ-\ncations, a recent work [57] proposes FTT-NAS, which adopts\nthe neural architecture search (NAS) technique to automate the\nprocess of ﬁnding a more fault-tolerant model architecture. We\ngive a case study on FTT-NAS as follows.\nB. Case Study: Automated Architecture Search for Fault Tol-\nerance\n1) The FTT-NAS Workﬂow: FTT-NAS aims at improving\nthe NN model’s algorithmic fault tolerance from the archi-\ntectural perspective. The overall workﬂow is shown in Fig. 1.\nFirstly, in order to evaluate the fault tolerance of an NN model\nefﬁciently, FTT-NAS abstracts the algorithm-level fault model\nfor injection-based evaluation. Speciﬁcally, the authors analyze\nthe convolution computations on different types of NN accel-\nerators, and conclude two representative weight and feature\nfault models: the MAC-i.i.d Bit-Bias (MiBB) feature fault\nmodel, and the arbitrary-distributed Stuck-at-Fault (adSAF)\nweight fault model. The MiBB feature fault model abstracts\nthe faulty effects of the feature map caused by random bit-\nﬂips in FPGA LUTs constructing the adder tree. And the\nadSAF weight fault model corresponds to the stuck-at faults\noccurring in the memristor cells of the RRAM crossbar. We\nshow the examples of injecting these two types of faults into\nthe convolution computation in Fig. 2.\nSecondly, according to preliminary experiments on what\ntypes of architectural decisions inﬂuence the fault tolerance\ncapability, FTT-NAS designs a large search space containing\nabout 1025 architectures. Then, FTT-NAS employs parameter-\nsharing NAS [58] to search for fault-tolerant architecture in\nthis search space. Speciﬁcally, FTT-NAS constructs an over-\nparameterized super network that contains the parameters\nneeded to evaluate all architectures in the search space, and an\nRNN-based controller to sample architecture from the search\nspace. The weights of controller are updated using the reward\nevaluated using the super network. And FTT-NAS inject faults\naccording to the previously established fault model into the\n12\n14\n21\n-6\n20\n2\n1\n16\n6\n3\n4\n7\n9\n4\n3\n-1\n2\n3\n12\n-1\n8\n5\n15\n9\n23\nSEU\nweight\nfeature\n=\n=\n528\n347\n336\n327\n500\n351\n340\n311\noutput\n(\" = 8, & = 0)\n12\n14\n21\n-2\n20\n10\n1\n16\n6\n3\n0\n7\n9\n4\n3\n-1\n(a)\n12\n14\n21\n-6\n20\n2\n1\n16\n6\n3\n4\n7\n9\n4\n3\n-1\n2\n3\n12\n0\n8\n5\n15\n9\n23\n2\n3\n12\n0\n127\n5\n15\n9\n23\nSAF-1\nweight\nfeature\n=\n=\n528\n347\n336\n327\n786\n468\n699\n806\noutput\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\nSAF-0\nPositive\nNegative\n(\" = 6, & = 0)\n(b)\nFig. 2: Example of injecting faults. (a) Feature faults under the\niBB fault model (soft errors in FPGA LUTs). Random bit-ﬂip\nin the adder tree LUTs caused a power-2 bias to be added\nor subtracted from the feature value. (b) Weight faults under\nthe adSAF fault model (SAF errors in RRAM cells). SAF in\nthe memristor cells caused the weight value to be stuck at the\nboundary of the quantization representation range. Note that\nthe usual NN to RRAM crossbar mapping scheme uses two\ncrossbars to separately store positive and negative weights.\ntraining and evaluation process of the super network. Finally,\nFTT-NAS derives a ﬁnal architecture using the controller, and\ntrains it with fault-tolerant training.\n2) Sample Results: FTT-NAS compares the fault tolerance\nof its discovered architectures with baseline architectures, in-\ncluding ResNet, VGG, MobileNet. For example, Fig. 3 shows\nthe reliability comparison between the baseline architectures\nand the W-FTT-Net architecture discovered under the 8bit-\nadSAF fault model. During the test time, the reliability is\nevaluated using three different types of weight fault models,\nincluding the 8bit-adSAF model, the 1bit-adSAF model, and\nthe iBF model. As we can see, W-FTT-Net outperforms\nbaseline architectures consistently at different noise levels\nunder three different types of weight fault models.\nIII. ARCHITECTURE-LAYER FAULT TOLERANCE\nDespite the inherent fault tolerance of neural networks,\nit is insufﬁcient to protect the neural network processing\nwith only model layer design because of the vast fault con-\nﬁguration space that can hardly be fully considered during\nthe model design stage. Moreover, model layer fault-tolerant\ndesign approaches typically require time-consuming training,\nwhich inhibits the runtime fault recovery. Moreover, training\nusually relies on application data which may not be always\naccessible during the deployment stage. Thereby, fault-tolerant\narchitectural designs which can potentially mitigate hardware\nfaults with much less limitation on neural network models and\nhigh-level applications are investigated recently.\nA. Related Work\nWhile convolution in neural networks can be viewed as\nmatrix-matrix multiplication, some of the fault-tolerant archi-\ntectural designs are inspired by fault-tolerant matrix-matrix\nmultiplication methods such as algorithm-based fault tolerance\n(ABFT) proposed in [59]. The basic idea is to add sum\nof each row/column of the input matrices to the original\ninput matrices such that the sum of output matrices in each\nrow/column can be obtained from both the extended matrix-\nmatrix multiplication and accumulation of elements in the\noriginal output matrix. In this case, checksum of the results\nfrom different approaches can be performed to detect errors\nin the matrix-matrix multiplication. At the same time, single\nbit error can also be recovered based on the checksum on\nboth row and column accumulation. E. Ozen et al. [60] took\nadvantage of the regular computing pattern of convolution\nneural networks and applied the ABFT technique to protect\nneural network accelerators against soft errors. Kai Zhao et al.\n[61] explored the various data ﬂows of ABFT technique for\nconvolution operations to obtain the fault detection/correction\ncapability comprehensively, which can also be utilized to\nguide fault-tolerant neural network accelerator designs. While\nthe overhead of the naive ABFT is non-trivial, Dionysios\nFilippas et al. [62] proposed a lightweight ABFT implemen-\ntation, ConvGuard, which predicts the output checksum of\nconvolution implicitly by accumulating only the pixels at\nthe border of the dropped input features. Thibaut Marty et\nal. [63] proposed to utilize the ABFT technique to mitigate\ntiming errors induced by overclocking of the neural network\naccelerators on FPGAs. Their experiments reveal that the\nproposed ABFT design poses negligible area overhead, enables\naggressive overclocking of the neural network accelerators,\nand achieves up to 60% throughput improvement of the overall\nneural network processing.\nUnlike the ABFT-based architectural designs that are gener-\nally independent with the speciﬁc deployed neural networks,\nmany approaches also explore the features of neural networks\nand develop corresponding fault-tolerant architectural design\nto achieve more effective protection. Christoph Schorn et al.\n[35] investigated the importance variations of neurons in the\nmodels and proposed an heterogeneous computing array that\nprovides two different levels of fault tolerance. In this case,\nimportant neurons are allocated to highly resilient computing\narray partitions while less important neurons are allocated to\nthe rest of computing array, which ensures resilient neural\nnetwork processing with much less hardware overhead. Jeff\nZhang et al. [21] [19] proposed to add a zero bypass data path\nto processing elements (PEs) in neural network accelerators\nand the bypass will be enabled when the corresponding PEs are\nfaulty. Although zero bypassing typically has less yet predicted\n(a)\n(b)\n(c)\nFig. 3: Accuracy curves under different weight fault models. (a) 8bit-adSAF model (caused by SAFs in 8-bit RRAM memristor\ncells). (b) 1bit-adSAF model (caused by SAFs in 1-bit RRAM memristor cells). (c) iBF model (caused by random bit ﬂips in\nthe weight buffer).\ninﬂuence on the neural network processing compared to values\nwith random faults, it may still cause substantial accuracy loss.\nTo address the problem, the authors perform retraining to adapt\nto each speciﬁc fault conﬁguration, which essentially alters the\nimportance of neural network weights or neurons to suit the\nfaulty computing array. Navid Khoshavi et al. [64] proposed\nan online fault assessment paradigm to delineate the most\nvulnerable parts of neural networks. On top of the assessment,\nthey provided corresponding hardening strategy to accomplish\noptimized neural network accelerator designs against transient\nerrors with resource constraints.\nIn addition, there are also conventional fault-tolerant com-\nputing mechanisms closely combined with neural network\naccelerator architectures. Zhen Gao et al. [30] introduced\nensemble learning to fault-tolerant neural network processing\nfor the ﬁrst time and combined it with redundancy design. The\nbasic idea is to have a group of redundant base neural network\nmodels implemented in parallel and equipped the different\nimplementations with a score comparison voter on FPGAs\nsuch that the majority of the soft error induced prediction\nerrors can be mitigated with negligible hardware overhead. The\ndifferent base models are all lightweight compared to the large\noriginal model, so the overhead is much smaller compared\nto conventional triple modular design (TMR) on the original\nmodel. Meanwhile, the ensemble model that is combined on\ntop of the lightweight models can also achieve competitive\naccuracy compared to large-scale models. Cheng Liu et al. [17]\n[18] proposed to apply conventional recomputing mechanism\nto fault-tolerant neural network accelerator designs with a\nhybrid computing architecture. The basic idea is to have\nadditional computing units seated along with a classical neural\nnetwork accelerator to recompute the operations mapped to\nthe faulty PEs. Since the additional computing units have\neach faulty operations processed in parallel independently, the\nrecomputing fabric can be utilized to ﬁx faulty neural network\naccelerators with arbitrary distribution of the faulty PEs.\nIn summary, there have been a number of fault-tolerant\nneural network accelerator designs proposed from distinct\narchitectural angles. They differ in terms of transparency to\nneural network models, target hardware fault types, hardware\noverhead, and performance penalty. There is no determined\nanswer for all the fault-tolerant requirements and the brief\nsurvey can be utilized to guide the optimized fault-tolerant\narchitecture selection.\nB. Case Study: Hybrid Computing Architecture for Deep\nLearning Accelerators\nIn this sub section, we will take the hybrid computing\narchitecture (HyCA) proposed in [17] as a case study and\nillustrate how it can be utilized as a general architecture for\nfault-tolerant neural network processing.\n1) HyCA Architecture: Figure. 4 presents an overview of\nHyCA for fault-tolerant neural network processing. It has\na dot-product processing unit (DPPU) seated along with a\nclassical 2-D computing computing array, to recompute all the\noperations mapped to the faulty PEs in arbitrary locations of\nthe 2-D computing array. While the 2-D computing array has\neach PE to calculate the different output features sequentially\ngiven the output stationary data ﬂow [65] and the DPPU has\nall the PEs to compute a single output features in parallel.\nTo make sure that the normal 2-D array processing will\nnot be affected by the DPPU recomputing, DPPU cannot read\nthe required weights and input features aligned in channel\ndimension if it starts the recomputing at the same time with\nthe 2-D computing array. To that end, we have the input\nfeatures and weights buffered in an input register ﬁle (IRF)\nand a weight register ﬁle (WRF) respectively while they are\nread for the 2-D computing array processing. Meanwhile, we\nhave the recomputing delayed until there are sufﬁcient inputs\nand weights ready for the recomputing. Accordingly, the delay\nmust be larger than or equal to the number of weights required\nby DPPU data consumption in a single cycle to ensure DPPU\ncan be fully utilized. As the DPPU may recompute operations\non any PE in the 2-D computing array, the delay also needs\nto be larger than or equal to Col when the last column of the\nPEs obtain the weights passed from the ﬁrst column of PEs.\n0 1 2\n31\n2\n0\n1\n1\n2\n2\n  \n  \nCol index\nRow index\nFPT\nWRF\nInput Buffer\nWeight Buffer\nOutput Buffer\n \n \n \n \n \nAGU\nfaulty PE\nnormal PE\n0 1 2\n \n \n \n \n \n31\nswap\nwrite-addr\nweight to DPPU\nDPPU\n0\n1\n2\n31\ninput to DPPU\n \n \nG1\nG2\nG8\nweight\ninput\nORF\n \nPE\nPE\nPE\nPE\nPE\nPE\nPE\nPE\nPE\nPE\nPE\nPE\nPE\nPE\nPE\nPE\n \n \n \n \n0\n1\n31\n  \nIRF\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n0\n1\n31\n  \n2\nPE\nPE\nFault \nDetection\n0\n1\n31\n  \n  \n  \n  \n  \nread-addr to WRF\ninput\nweight\noutput\naddr\nread-addr to IRF\nFig. 4: Overview of of a DLA with Hybrid Computing\nArchitecture. The components highlighted with blue are added\nto the conventional DLA to tolerate faulty PEs in arbitrary\nlocations of the 2-D computing array.\nNote that Col refers to the column size of the 2-D computing\narray.\nIn this work, we organize IRF and WRF in Ping-Pong\nmanner to ensure that the 2-D computing array can continue\nthe normal dataﬂow without any stall during the DPPU recom-\nputing. As the DPPU conducts the output feature calculation\nin parallel, DPPU can always ﬁnish the recomputing of the\noperations mapped to the faulty PEs before the Ping-Pong\nregister ﬁles swap with each other when the DPPU size does\nnot exceed the number of the faulty PEs. Note that DPPU size\nrefers to the number of multipliers in DPPU. Since the peak\ncomputing power of DPPU equals to that of the 2-D computing\narray when conﬁgured with the same number of PEs, DPPU\nsize is comparable to the 2-D computing array size and can\nalso be used to represent its computing power. This also\nexplains why DPPU can always ﬁnish the recomputing tasks\nbefore new weights and inputs are ready when DPPU size is\nlarger than the number of faulty PEs in the 2-D computing\narray.\nIn addition, we have a fault PE table (FPT) to record the\ncoordinates of the faulty PEs in the 2-D computing array which\ncan be usually obtained with a power-on self-test procedure.\nWith the coordinates of faulty PEs, an address generation unit\n(AGU) is used to generate the read addresses and instruct the\nDPPU to read the right input features and weights from the\nregister ﬁles. Moreover, AGU also determines the addresses to\nthe output buffer for the overlapped writes of the recomputed\noutput features. Similar to the IRF and WRF, there is also a\nPing-Pong register ﬁle called ORF for the DPPU outputs and\nit is utilized to pipeline the DPPU recomputing and the write\nfrom DPPU to the output buffer.\n2) Experiments: In this experiment, we have two different\nfault distribution models including the random distribution\nmodel and the clustered distribution model implemented. For\nthe random distribution model, the faults are randomly dis-\ntributed across the entire computing array. For the clustered\ndistribution model which is usually used to characterize the\nmanufacture defects, the faults are more likely to be close to\neach other and the model proposed in [66] is applied in this\nwork. Meanwhile, we notice that the inﬂuence of hardware\nfaults is related with the fault distribution, so we generate\n10000 conﬁgurations randomly for each fault injection rate and\naverage the evaluation in the experiments. Since we mainly\nfocus on the reliability of the regular 2-D computing array in\na deep learning accelerator, we use PE error rate (PER) as the\nfault injection metric similar to the work in [21] and [67]. We\nevaluate the hard error rate in a large scale ranging from 0%\nto 6%.\nTo evaluate the reliability of the DLAs, we propose two\nmetrics that can be applied for different applications. One\nof them is the fully functional probability and it shows the\nprobability that the DLA can be fully functional without any\nperformance penalty. It is preferred by the mission-critical\napplications that do not allow any performance degradation nor\nmodel modiﬁcation because any system modiﬁcation may re-\nquire expensive and lengthy safety evaluation and certiﬁcation.\nThe experiment is shown in Figure. 5. It shows that HyCA\noutperforms the three classical redundancy approaches and the\nadvantage gets enlarged under the clustered fault distribution.\nThe main reason is that each redundant PE in row redundancy\n(RR), column redundancy (CR) and diagonal redundancy (DR)\ncan only be utilized to replace a single faulty PE in a row, a\ncolumn, and a row-column pair respectively. When multiple\nfaults occur in the same protected region, these redundancy\napproaches fail to recover the faulty 2-D computing array\nand the design will not be fully functional. Unlike these\nclassical redundancy approaches, HyCA allows arbitrary faulty\ndistribution and can perfectly repair the computing array as\nlong as the number of faulty PEs in the 2-D computing array\ndoes not exceed the DPPU size. Thereby, the fully functional\nprobability of HyCA is not sensitive to the fault distribution\nmodels.\nThe other metric is the normalized remaining computing\npower and it refers to the percentage of the remaining comput-\ning array size over the original 2-D computing array size. This\nmetric is particularly important for the non-critical applications\nthat do not require fully functional accelerators and allow the\naccelerators to be degraded, because the remaining computing\narray size determines the theoretical computing power and\naffects the performance of the deployed neural network models\ndirectly. Figur. 6 reveals the computing power comparison of\nthe different redundancy approaches. It can be observed that\nHyCA shows signiﬁcantly higher computing power under all\nthe different PER setups and the advantage also enlarges with\n CR  \n \n RR  \n \n DR  \n \n HyCA \n0%\n20%\n40%\n60%\n80%\n100%\n0%\n2%\n4%\n6%\nFully functional \nprobability\nPER\n0%\n20%\n40%\n60%\n80%\n100%\n0%\n2%\n4%\n6%\nFully functional \nprobability\nPER\n(a)Random fault distribution model (b) Clustered fault distribution model \nFig. 5: Fully functional probability of DLAs with different\nredundancy approaches.\n CR  \n \n RR  \n \n DR  \n \n HyCA \n0%\n20%\n40%\n60%\n80%\n100%\n0%\n2%\n4%\n6%\nNormalized computing \npower\nPER\n0%\n20%\n40%\n60%\n80%\n100%\n0%\n2%\n4%\n6%\nNormalized computing \npower\nPER\n(a)Random fault distribution model (b)Clustered fault distribution model \nFig. 6: Normalized computing power of DLAs with different\nredundancy approaches.\nthe increase of the PER. This is mainly brought by the fault\nrecovery ﬂexibility of the HyCA that allows the DPPU to\nselect the most critical faulty PEs to repair when the redundant\nfaulty PEs are insufﬁcient. In contrast, each redundant PE can\nonly repair a limited subset of the faulty PEs for the RR, CR\nand DR. There is little space left to optimize the faulty PE\nmitigation order. Thereby, the remaining computing power of\nRR, CR, and DR is much lower.\n3) Summary: Prior redundancy design approaches for the\nregular computing array such as RR and CR greatly reduce\nthe hardware overhead compared to the classical TMR ap-\nproaches, but they are rather sensitive to the fault distribution\nand fail to work especially when the faults are unevenly\ndistributed. HyCA has a DPPU to recompute all the operations\nmapped to the faulty PEs in the 2-D computing array. When\nthe number of faulty PEs in the 2-D computing array is\nless than the DPPU size, HyCA can fully recover the 2-D\ncomputing array despite the fault distribution. Even when the\nfault error rate further increases, DPPU can still be used to\nrepair the most critical PEs ﬁrst to ensure a large available\ncomputing array and minimize the performance penalty.\nIV. CIRCUIT-LAYER FAULT TOLERANCE\nHamid Reza Mahdiani et al. [31] proposed to take advantage\nof the inherent fault tolerance in neural network applications\nby using relaxed fault-tolerant processing elements for neural\nnetwork processing. Instead of conducting PE-level TMR\nprotection, this approach enables selective ripple-carry adder\ncell TMR protection for the PEs in neural network accelerators\nbased on the affected bit position of the outputs. Basically,\ncells that affect the higher output bits will be protected with\nhigher priority and the exact protection strategy depends on the\ncorresponding fault tolerance level requirements of the appli-\ncations. The experiment reveals that this approach shows much\nless chip area and shorter critical path compared to fully TMR\nprotection under the same protection level. However, compared\nto the computing datapath, memory access is more energy-\nconsuming [68]. In [69], a framework, “Minerva”, is developed\nto optimize a neural network accelerator and the energy due\nto SRAM is reduced by scaling its supply voltage. The power\nconsumption reduces quadratically with the supply voltage\nwhile the bit error rate increases exponentially. For a minimal\naccuracy reduction caused by the bit errors, Razor SRAMs\nand bit/word masking are employed to detect and correct the\nerrors respectively. Consequently, a fault probability of more\nthan 10−3 can be tolerated and 2.7× power saving is achieved.\nSimilarly, Lita Yang et al. [70] explored the fault tolerance of\nbinary neural networks and took advantage of the fault toler-\nance to reduce the SRAM voltage in a convolutional neural\nnetwork (CNN) processor. The experiment reveals signiﬁcant\nenergy savings with limited accuracy degradation, though it\nmay vary across the network topologies and classiﬁcation\ntasks. Juan Antonio Clemente et al. [71] developed a fault-\ntolerant Hopﬁled neural network (FT-HNN) on FPGAs by\ninserting additional connection to obtain hidden accumulation\nstates differently and then had them voted for fault tolerance.\nSince the different accumulations share many partial results,\nthe proposed FT-HNN requires much less hardware overhead\ncompared to the baseline TMR implementation (HNN+TMR),\nbut FT-HNN still shows comparable standard errors and con-\nvergence over HNN+TMR under the same SEU setup.\nEmerging computing paradigm, such as approximate com-\nputing and stochastic computing, can also exploit the fault-\ntolerant feature of many applications, so that higher per-\nformance and energy efﬁciency are obtained with limited\naccuracy loss.\nA. Approximate computing circuits\nApproximate computing has widely been investigated on\nvarious levels of a computing system, from the programmable\nlanguage and algorithm down to the circuit. On circuit level,\ncommonly used approximation techniques include the voltage\noverscaling (VOS) [72], implementing a complex arithmetic\noperation based on a simpliﬁcation of its mathematical rep-\nresentation [73], and modiﬁcation of the classical (accurate)\nlogic function of an arithmetic circuit [74]. As the most\nsimple and convenient approach, VOS reduces the power\ndissipation of computing without the need of circuit mod-\niﬁcation. However, it may result in uncertain errors in the\nmore signiﬁcant bits of arithmetic operations, which can\ndamage the accuracy of the entire system. BY modifying\nthe classical design of an arithmetic circuit, the hardware\noverhead lowered with deterministic errors; thus, the errors\ncan be characterized and/or compensated in the following\ncomputations. Considering deep learning applications, due to\nthe recurrent reﬁnement of learning algorithms, some errors\nof arithmetic circuits can be recovered at the system level. As\na result, approximate arithmetic circuits have been potential\nchoices for implementing a hardware-efﬁcient deep learning\nalgorithm.\n[75] proposes a precision self-adaptive approximate addition\nunit in the design of a binarized weight network proces-\nsor for keyword-spotting (KWS). Compared to the accurate\ncomputing mode, the approximate addition reduces the power\nconsumption by 1.3×. In addition, approximate multiplication\nis utilized in the required mel-scale frequency cepstral coef-\nﬁcients module. Finally, the power dissipation of the KWS\nsystem is reduced by 1.4× with a less than 0.5% recognition\naccuracy loss.\nIn [76], approximate arithmetic circuit designs including\nadders, multipliers and dividers based on various approxima-\ntion methodologies are introduced. These designs are com-\nprehensively evaluated by different metrics with respect to\ntheir accuracy and hardware efﬁciency. Moreover, explorations\nare performed to reveal the relationship between the statistical\nerror metric of approximate arithmetic circuits and the accu-\nracy of image processing and deep neural network applications\nusing them. Speciﬁcally, by using various approximate adders\nand multipliers, several image processing algorithms and the\nface detection and alignment implemented by using a multi-\ntask CNN are performed. The simulation results show that\nfor simple operations without many serial computations, such\nas a sum of product in an image ﬁltering, the approximate\narithmetic circuits with smaller mean relative error distances\n(MREDs) generally leads to higher quality. The MRED is\ndeﬁned as\nMRED =\nPN\ni=1 | ˆxi−xi\nxi\n|\nN.\n(1)\nˆxi represents the approximate result and xi the real value.\nIn deep learning, as complex computations such as multiple\nof consecutive matrix multiplications, except for MRED, error\nbias is of great importance. The simulation results show\nthat approximate adders and multipliers with small error bias\ngeneral result in low degradation in the accuracy of face\ndetection and alignment. The error bias is given by\nError bias =\nPN\ni=1 (ˆxi −xi)\nN\n.\n(2)\nThe simulation results also show that, with a same bit width,\ndeep learning application is more sensitive to the errors of\nadders than those of multipliers. Another interesting conclu-\nsion from [76] is that, by using approximate arithmetic circuits,\ndeep learning applications can achieve beneﬁts in both energy-\nefﬁciency and accuracy.\nB. Stochastic computing circuits\nStochastic computing (SC) is an alternative computing\nparadigm that produces unbiased estimate of the actual results,\ni.e., the error bias is 0. In SC, numbers are encoded by streams\n0\n5\n10\n15\nnth order polynomial circuit\n100\n101\n102\n103\n104\n105\nApproximate transistor count\nStochastic circuits\n8-bit binary circuits\nFig. 7: Transistor count implementing Bernstein polynomials\nwith different order, stochastic computing vs. 8-bit binary\ncircuits.\nof random 0s and 1s and the probability of 1 in this sequence is\nused to represent a number. For example, “10011100” can be\nused to represent 0.5 in the unipolar representation, where the\nencoded number equals to the probability of 1 in the sequence.\nUsing SC, the complexity of the arithmetic circuits can be\ngreatly reduced. For example, an AND gate implements a\nmultiplier in the unipolar representation in SC. The output\nof an AND gate is 1 only when both the inputs are 1s.\nSo the probability of the AND gate generating a 1 is the\nproduct of the probabilities of 1s of the two input sequences\ngiven that they are independently generated. Nevertheless, in\nconventional binary circuits, it typically takes hundreds of\ngates to build a multiplier. Fig. 7 shows the transistor count\nof core circuits computing Bernstein polynomials using SC\ncompared to the conventional 8-bit binary counterpart. The SC\ncircuits are synthesized by the method proposed in [77]. The\nresults show that stochastic computing circuit can achieve one\nhundredth of the hardware cost of its 8-bit binary counterpart.\nHowever, the peripheral supporting circuits would take a\nlarge portion of an SC system since the stochastic bit stream is\ntypically generated by the costly stochastic number generators\n(SNGs) and later converted back to binary numbers by proba-\nbility estimators, as shown in 8. In the unipolar representation,\nthe stochastic bit stream can be generated by comparing the\nnumber to be encoded, x, with a uniformly distributed random\nnumber. If x is larger, a ‘1’ is generated; otherwise, a ‘0’ is\nproduced. So the probability generating a ‘1’ equals to x. The\nﬁnal output can be estimated by counting the number of 1’s\nin the output bit stream and divide it by the sequence length.\nIn [32], the SC accelerators are studied from an architectural\nperspective and the conversion cost can be mitigated by\nsharing the components of the conversion units across the\nSC circuits. It is also found that a higher computation-to-\nconversion ratio indicates a lower conversion overhead and\nthus a higher energy efﬁciency gain. This is often the case\nin compute-intensive tasks such as convolution, which is the\nmajor operations in a CNN. For a 3times3 Gaussian ﬁlter\nBinary\nNumbers\nStochastic Number\nGenerators (SNGs)\nStochastic\nComputing\nCircuits\nProbability\nEstimators\n00101011......\nStochastic\nBit Streams\n10001001......\nStochastic\nBit Streams\nBinary\nNumbers\nFig. 8: A typical stochastic computing system contains\nstochastic number generators (SNGs), stochastic computing\ncircuits and probability estimators.\n10110100\n01110111\n00110100\n10110100\n11110111\n10110100\n(0.1000)2\n(0.1100)2\n(0.0110)2\nX\n(0.1000)2\n(0.0100)2\n(0.0010)2\nX\n(a)\n(b)\nFig. 9: A typical stochastic computing system contains\nstochastic number generators (SNGs), stochastic computing\ncircuits and probability estimators.\nand 5times5 general convolution, the conversion overhead can\nbe shared by 6.6 to 18.2 arithmetic operations by exploiting\ndata locality and reuse. Eventually, the SC circuits achieve\nhigher energy efﬁciency than their binary counterparts at a bit\nprecision of 8 or lower with limited accuracy degradation of\nwithin 1% for an SVM classiﬁcation task.\nWhen considering the inherit fault-tolerant nature of SC,\nlarger energy savings can be achieved by voltage scaling. This\nstems from the unique coding method of SC, i.e., numbers are\nencoded as equal-weighted long bit streams and each bit only\naccounts for 1/L of its value when the total sequence length\nis L. Therefore, one or few bit ﬂip error in the bit stream\ndoes not affect the ﬁnal results very much. On the contrary, in\nthe binary systems, the numbers mostly use positional coding,\ni.e., signiﬁcant bits usually have larger weights. Thus, a bit\nﬂip error on the most signiﬁcant bit can lead to a large error,\nas compared in Fig. 9.\nDue to this fault-tolerance feature, voltage scaling can be\napplied to SC circuits to further reduce the energy cost.\nHowever, when the supply voltage is beyond its critical point,\ntiming violations and bit ﬂips may occur. [32] shows that on a\nfabricated ASIC prototype, the SC circuits can tolerate these\nerrors and operate at a supply voltage as low as 0.55 V while\nproducing satisfying results, whereas the binary circuits fail at\n0.8 V under the same working condition. This can bring an\nextra 3.3times energy improvement for the SC circuits.\nIn the context of conventional binary system, binarized neu-\nral networks (BNNs) can also exploit bit-wise computations\nto perform multiplications and additions using XNOR gates\nand bit-counting respectively [78]. However, the real values\nare deterministically binarized through a comparison with 0.\nAs a result, a bit ﬂip can change the results signiﬁcantly. [79]\ncompares the fault-tolerance under different bit-ﬂip rates of\nSC-based neural networks (SCNNs) and BNNs. The results\na[1]\na[0]\na[0]\nb[1]\nb[1]\nb[0]\nout[1]\nout[0]\na[1]\na[0]\nb[1]\nb[0]\nc[1]\nc[0]\nd[1]\nd[0]\nout[1]\nout[0]\nDescending\norder\nsorting\nStochastic\ntenary\nmultiplier\nAscending\norder\nsorting\n0\n1\n1\n0\n0\n1\n0\n1\nTenary\nactivation\nfunction\n1\n-1\nStochastic tenary\naccumulate-and-activate\nusing bitonic sorting\nFig. 10: SC circuit implement the function in a TNN. The\nmultiplier is customized to ﬁt the proposed encoding scheme.\nThe accumulate-activate is implemented using a bitonic sorting\nnetwork circuit.\nshow that at different bit-ﬂip rate levels, the recognition\naccuracy of SCNNs is always higher than that of the BNNs for\nthe MNIST dataset. The accuracy of SCNNs falls below 97%\nwhen the bit-ﬂip rate is higher than 10% while the number\nis at most 5% for the BNNs when both the activations and\nweights are affected by the bit-ﬂip errors. Conventional fault-\ntolerant schemes such as modular redundancy can be applied\nto BNNs to reduce its vulnerability to noise. However, the\nrecognition accuracy of BNNs still cannot match that of an\nSCNN for the MNIST dataset. But when tested on a CNN\nfor the CIFAR10 dataset, BNNs with modular redundancy\noutperform the SCNN, but at the cost of extra hardware\nresources. On the other hand, the long latency is a drawback\nof SCNNs due to the requirement for a long stochastic bit\nstream to perform computations with acceptable accuracy. It\nalso increase the energy consumption of the SC circuits.\nThis can be solved by combining SC with low-precision\nNNs. For example, in [80], a ternary NN (TNN) is imple-\nmented by SC using sorting network circuits as the basic\nprocessing units while the fault tolerance feature is maintained.\nBesides, at least 2.8times energy efﬁciency improvement is\nobtained compared to its binary counterpart. In TNNs, all the\nweights and activations are ternarized to {0, +1, −1} and they\nare encoded as {10/01, 11, 00} respectively, with a sequence\nlength of only 2 bits. The complex multiply-accumulate-\nactivate function of each TNN layer (4 input activations and 4\nweights) then can be implemented by less than 100 gates. The\nSC multiplier is tailored for the proposed stochastic encoding\nscheme and the accumulate-activate is fused and implemented\nby a bitonic sorting network, as shown in Fig. 10. The sorting\nnetwork ﬁrst place all the 1’s on the top and 0’s at the bottom.\nThen the output is decided by the number of 1’s and 0’s in\nthe input bits. When there are more 1’s than 0’s, the output is\n“11”, indicating a ‘+1’; when there are more 0’s, the output\nis “00”, indicating a ‘-1’; otherwise, the number of 1’s and\n0’s is equal, the output is “10”, indicting a ‘0’. This exactly\nimplements the tenary activation function.\nThe proposed design is tested on MNIST digit recognition\napplication for its performance, hardware efﬁciency and fault-\ntolerance. For the 4 × 4 × 1 convolutional layer, the SC-based\nTNN accelerator can reach 833 TOPS/W, which is 24.5×\nof the conventional SC design. The recognition accuracy\nwithout any bit-ﬂip error is 97.35% for the proposed SC\ndesign. To evaluate its fault-tolerance, the SRAM read error\nand multiply-and-add calculation error are considered. When\nthe calculation bit error rate is 10%, the proposed design\nmaintain a high recognition accuracy of 94% while that of\na binary and conventional SC design is lower than 80%. An\nSRAM read error harms the accuracy more than the calculation\nerror, but the fault-tolerance of the SC-based TNN design still\noutperforms the other two counterparts signiﬁcantly.\nC. Summary\nA deep neural network model can be error-resilience,\ntherefore, inexact computation results can be tolerated and\n“inexact” circuit design schemes, such as voltage scaling\nand approximate computing, can be used to improve the\nperformance and energy efﬁciency. On the other hand, alter-\nnative computing paradigms such as SC uses different number\nencoding system, providing more fault-tolerance, especially\nagainst bit-ﬂip error, while dramatically reducing the hardware\ncost. The long latency used to be a major drawback for SC\ndue to the long sequences required for producing acceptable\naccuracy. It can be potentially solved by circuit-algorithm\ncodesign with careful optimizations.\nV. CROSS-LAYER FAULT TOLERANCE\nIn this work, we mainly investigate the fault-tolerant neural\nnetwork design techniques with a layer-wise manner and the\nrelevant techniques in each layer are illustrated separately in\nprior sections. However, we still want to emphasize that many\ncross-layer fault-tolerant approaches have been explored to\nmake best use of the fault-tolerant techniques from different\nlayers for optimized design trade-offs in terms of performance,\nhardware overhead, and reliability. Hence, we will brieﬂy\nintroduce the cross-layer fault-tolerant approaches with a few\ntypical examples in this section. Jeff Jun Zhang et al. [21]\n[19] proposed to mitigate permanent faults in neural network\naccelerators with cross-layer optimizations. At architecture\nlayer, a constant bypass is added to each PE in neural\nnetwork accelerators. On top of the architecture, retraining,\na typical model-layer fault-tolerant technique, is applied for\neach speciﬁc fault conﬁguration to recover the model accuracy\nsigniﬁcantly. The authors in [81] proposed a dynamic per-\nlayer voltage underscaling circuit on top of a classical neural\nnetwork accelerator such that the accelerator can operate\nat optimized voltage in each layer of the neural network\nprocessing. On top of the circuits, runtime pruning architec-\nture like zero-skip [82] [69] is integrated for higher energy\nefﬁciency. Sung Kim et al. [83] proposed to combine adaptive\nneural network training and weight memory voltage scaling\nto achieve energy-efﬁcient neural network processing. Similar\ncross-layer optimizations that utilize voltage scaling and fault-\naware training or high-level fault correction are also applied\nin many different scenarios [14] [84] [85] [63]. In summary,\ncross-layer fault-tolerant approaches show promising results\nin generally and it can be expected many of the fault-tolerant\ntechniques surveyed in prior sections can also be potentially\ncombined and optimized for more effective protection against\nhardware faults.\nVI. CONCLUSION\nIn this paper, we reviewed the techniques for fault-tolerant\ndeep learning against perturbations caused by hardware faults\nin the underlying silicon-based computing engines especially\ndeep learning accelerators. The review is generally conduced\nin a top-down manner and investigates the fault-tolerant ap-\nproaches from model layer, architecture layer, and circuit layer\nrespectively. Meanwhile, cross-layer approaches that combine\nfault-tolerant techniques in multiple layers are also brieﬂy in-\ntroduced. While fault-tolerant deep learning needs to consider\nnot only the fault tolerance but also many other metrics includ-\ning performance, accuracy, and hardware overhead at the same\ntime, cross-layer approaches that can leverage advantages\nof the fault-tolerant techniques from different layers can be\npotentially beneﬁcial and some of prior work also conﬁrms the\ngreat advantages. Nevertheless, cross-layer approaches require\nsynergistic efforts of researchers from AI domain, architectural\ndomain, and reliability domain.\nACKNOWLEDGMENT\nThe authors would like to thank the support from National\nKey Research and Development Program of China under\nGrant No.2020YFB1600201 and National Natural Science\nFoundation of China (NSFC) under Grant No.62174162, No.\n62171313, and No.61902375.\nREFERENCES\n[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” Advances in neural informa-\ntion processing systems, vol. 25, 2012.\n[2] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\nonce: Uniﬁed, real-time object detection,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2016, pp. 779–\n788.\n[3] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in\ndeep learning based natural language processing,” ieee Computational\nintelligenCe magazine, vol. 13, no. 3, pp. 55–75, 2018.\n[4] Y. Wang, X. Ding, R. Liu, and Y. Shao, “Conditionsensenet: A deep\ninterpolatory convnet for bearing intelligent diagnosis under variational\nworking conditions,” IEEE Transactions on Industrial Informatics, 2021.\n[5] T. Ching, D. S. Himmelstein, B. K. Beaulieu-Jones, A. A. Kalinin, B. T.\nDo, G. P. Way, E. Ferrero, P.-M. Agapow, M. Zietz, M. M. Hoffman\net al., “Opportunities and obstacles for deep learning in biology and\nmedicine,” Journal of The Royal Society Interface, vol. 15, no. 141, p.\n20170387, 2018.\n[6] A. C. Mater and M. L. Coote, “Deep learning in chemistry,” Journal\nof chemical information and modeling, vol. 59, no. 6, pp. 2545–2559,\n2019.\n[7] M. Fink, Y. Liu, A. Engstle, and S.-A. Schneider, “Deep Learning-Based\nMulti-scale Multi-object Detection and Classiﬁcation for Autonomous\nDriving,” in Fahrerassistenzsysteme 2018. Springer, 2019, pp. 233–242.\n[8] M. Tzelepi and A. Tefas, “Human Crowd Detection for Drone Flight\nSafety Using Convolutional Neural Networks,” in 2017 25th European\nSignal Processing Conference (EUSIPCO).\nIEEE, 2017, pp. 743–747.\n[9] A. Biondi, F. Nesti, G. Cicero, D. Casini, and G. Buttazzo, “A safe,\nsecure, and predictable software architecture for deep learning in safety-\ncritical systems,” IEEE Embedded Systems Letters, vol. 12, no. 3, pp.\n78–82, 2019.\n[10] S. Tang, R. Gong, Y. Wang, A. Liu, J. Wang, X. Chen, F. Yu,\nX. Liu, D. Song, A. Yuille et al., “Robustart: Benchmarking robust-\nness on architecture design and training techniques,” arXiv preprint\narXiv:2109.05211, 2021.\n[11] M. Rabe, S. Milz, and P. Mader, “Development methodologies for\nsafety critical machine learning applications in the automotive domain:\nA survey,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 129–141.\n[12] X. Xue, H. Huang, C. Liu, Y. Wang, T. Luo, and L. Zhang, “Wino-\ngrad convolution: A perspective from fault tolerance,” arXiv preprint\narXiv:2202.08675, 2022.\n[13] R. Paul, S. Sarkar, S. Sau, K. Chakraborty, S. Roy, and A. Chakrabarti,\n“Voltage scaling for partitioned systolic array in a reconﬁgurable plat-\nform,” arXiv preprint arXiv:2102.06888, 2021.\n[14] F. Tu, W. Wu, S. Yin, L. Liu, and S. Wei, “Rana: Towards efﬁcient\nneural acceleration with refresh-optimized embedded dram,” in 2018\nACM/IEEE 45th Annual International Symposium on Computer Archi-\ntecture (ISCA).\nIEEE, 2018, pp. 340–352.\n[15] X. Zhao, Y. Wang, X. Cai, C. Liu, and L. Zhang, “Linear symmetric\nquantization of neural networks for low-precision integer hardware,” in\nInternational Conference on Learning Representations, 2020. [Online].\nAvailable: https://openreview.net/forum?id=H1lBj2VFPS\n[16] X. Zhao, Y. Wang, C. Liu, C. Shi, K. Tu, and L. Zhang, “Bitpruner:\nNetwork pruning for bit-serial accelerators,” in 2020 57th ACM/IEEE\nDesign Automation Conference (DAC).\nIEEE, 2020, pp. 1–6.\n[17] C. Liu, C. Chu, D. Xu, Y. Wang, Q. Wang, H. Li, X. Li, and K.-T.\nCheng, “Hyca: A hybrid computing architecture for fault tolerant deep\nlearning,” IEEE Transactions on Computer-Aided Design of Integrated\nCircuits and Systems, 2021.\n[18] D. Xu, C. Chu, Q. Wang, C. Liu, Y. Wang, L. Zhang, H. Liang, and\nK.-T. Cheng, “A hybrid computing architecture for fault-tolerant deep\nlearning accelerators,” in 2020 IEEE 38th International Conference on\nComputer Design (ICCD).\nIEEE, 2020, pp. 478–485.\n[19] J. J. Zhang, K. Basu, and S. Garg, “Fault-tolerant systolic array based\naccelerators for deep neural network execution,” IEEE Design & Test,\nvol. 36, no. 5, pp. 44–53, 2019.\n[20] A. Serban, E. Poll, and J. Visser, “Adversarial examples on object recog-\nnition: A comprehensive survey,” ACM Computing Surveys (CSUR),\nvol. 53, no. 3, pp. 1–38, 2020.\n[21] J. J. Zhang, T. Gu, K. Basu, and S. Garg, “Analyzing and mitigating\nthe impact of permanent faults on a systolic array based neural network\naccelerator,” in 2018 IEEE 36th VLSI Test Symposium (VTS), 2018, pp.\n1–6.\n[22] D. Xu, Z. Zhu, C. Liu, Y. Wang, H. Li, L. Zhang, and K.-T. Cheng,\n“Persistent fault analysis of neural networks on fpga-based acceleration\nsystem,” in 2020 IEEE 31st International Conference on Application-\nspeciﬁc Systems, Architectures and Processors (ASAP).\nIEEE, 2020,\npp. 85–92.\n[23] D. Xu, Z. Zhu, C. Liu, Y. Wang, S. Zhao, L. Zhang, H. Liang, H. Li, and\nK.-T. Cheng, “Reliability evaluation and analysis of fpga-based neural\nnetwork acceleration system,” IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems, vol. 29, no. 3, pp. 472–484, 2021.\n[24] Y. He, P. Balaprakash, and Y. Li, “Fidelity: Efﬁcient resilience anal-\nysis framework for deep learning accelerators,” in 2020 53rd Annual\nIEEE/ACM International Symposium on Microarchitecture (MICRO).\nIEEE, 2020, pp. 270–281.\n[25] B. Salami, O. S. Unsal, and A. C. Kestelman, “On the resilience of\nRTL NN accelerators: Fault characterization and mitigation,” in 2018\n30th International Symposium on Computer Architecture and High\nPerformance Computing (SBAC-PAD).\nIEEE, 2018, pp. 322–329.\n[26] C. Torres-Huitzil and B. Girau, “Fault and error tolerance in neural\nnetworks: A review,” IEEE Access, vol. 5, pp. 17 322–17 341, 2017.\n[27] M. Shaﬁque, M. Naseer, T. Theocharides, C. Kyrkou, O. Mutlu,\nL. Orosa, and J. Choi, “Robust machine learning systems: Challenges,\ncurrent trends, perspectives, and the road ahead,” IEEE Design & Test,\nvol. 37, no. 2, pp. 30–57, 2020.\n[28] N. Khoshavi, C. Broyles, and Y. Bi, “A survey on impact of transient\nfaults on bnn inference accelerators,” arXiv preprint arXiv:2004.05915,\n2020.\n[29] S. Mittal, “A Survey on Modeling and Improving Reliability of DNN\nAlgorithms and Accelerators,” Journal of Systems Architecture, vol. 104,\np. 101689, 2020.\n[30] Z. Gao, H. Zhang, Y. Yao, J. Xiao, S. Zeng, G. Ge, Y. Wang, A. Ullah,\nand P. Reviriego, “Soft error tolerant convolutional neural networks on\nfpgas with ensemble learning,” IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems, 2022.\n[31] H. R. Mahdiani, S. M. Fakhraie, and C. Lucas, “Relaxed fault-tolerant\nhardware implementation of neural networks in the presence of multiple\ntransient errors,” IEEE transactions on neural networks and learning\nsystems, vol. 23, no. 8, pp. 1215–1228, 2012.\n[32] V. T. Lee, A. Alaghi, R. Pamula, V. S. Sathe, L. Ceze, and M. Os-\nkin, “Architecture considerations for stochastic computing accelerators,”\nIEEE Transactions on Computer-Aided Design of Integrated Circuits\nand Systems, vol. 37, no. 11, pp. 2277–2289, 2018.\n[33] J.-C. Vialatte and F. Leduc-Primeau, “A study of deep learning robust-\nness against computation failures,” arXiv:1704.05396, 2017.\n[34] F. Libano, B. Wilson, J. Anderson, M. Wirthlin, C. Cazzaniga, C. Frost,\nand P. Rech, “Selective hardening for neural networks in fpgas,” IEEE\nTransactions on Nuclear Science, vol. 66, no. 1, pp. 216–222, 2018.\n[35] C. Schorn, A. Guntoro, and G. Ascheid, “Accurate neuron resilience\nprediction for a ﬂexible reliability management in neural network\naccelerators,” in 2018 Design, Automation & Test in Europe Conference\n& Exhibition (DATE).\nIEEE, 2018, pp. 979–984.\n[36] ——, “An efﬁcient bit-ﬂip resilience optimization method for deep neu-\nral networks,” in 2019 Design, Automation & Test in Europe Conference\n& Exhibition (DATE).\nIEEE, 2019, pp. 1507–1512.\n[37] G. Li, S. K. S. Hari, M. Sullivan, T. Tsai, K. Pattabiraman, J. Emer, and\nS. W. Keckler, “Understanding error propagation in deep learning neural\nnetwork (dnn) accelerators and applications,” in Proceedings of the\nInternational Conference for High Performance Computing, Networking,\nStorage and Analysis, 2017, pp. 1–12.\n[38] C.-S. Leung and J. P.-F. Sum, “Rbf networks under the concurrent fault\nsituation,” IEEE transactions on neural networks and learning systems,\nvol. 23, no. 7, pp. 1148–1155, 2012.\n[39] S. U. Ahmed, M. Shahjahan, and K. Murase, “Injecting chaos in\nfeedforward neural networks,” Neural processing letters, vol. 34, no. 1,\npp. 87–100, 2011.\n[40] A. P. Piotrowski, P. M. Rowinski, and J. J. Napiorkowski, “Comparison\nof evolutionary computation techniques for noise injected neural network\ntraining to estimate longitudinal dispersion coefﬁcients in rivers,” Expert\nSystems with Applications, vol. 39, no. 1, pp. 1354–1361, 2012.\n[41] T. Cho, K. Katahira, K. Okanoya, and M. Okada, “Node perturbation\nlearning without noiseless baseline,” Neural networks, vol. 24, no. 3,\npp. 267–272, 2011.\n[42] O. Osoba and B. Kosko, “Noise-enhanced clustering and competitive\nlearning algorithms,” Neural Networks, vol. 37, pp. 132–140, 2013.\n[43] G. B. Hacene, F. Leduc-Primeau, A. B. Soussia, V. Gripon, and\nF. Gagnon, “Training modern deep neural networks for memory-fault\nrobustness,” IEEE International Symposium on Circuits and Systems\n(ISCAS), pp. 1–5, 2019.\n[44] Z. He, J. Lin, R. Ewetz, J. Yuan, and D. Fan, “Noise injection adaption:\nEnd-to-end reram crossbar non-ideal effect adaption for neural network\nmapping,” in 2019 56th ACM/IEEE Design Automation Conference\n(DAC).\nACM, 2019.\n[45] J. L. Bernier, J. Ortega, E. Ros, I. Rojas, and A. Prieto, “A quantitative\nstudy of fault tolerance, noise immunity, and generalization ability of\nmlps,” Neural Computation, vol. 12, no. 12, pp. 2941–2964, 2000.\n[46] Y. Huang, “Advances in artiﬁcial neural networks–methodological de-\nvelopment and application,” Algorithms, vol. 2, no. 3, pp. 973–1007,\n2009.\n[47] S. Cavalieri and O. Mirabella, “A novel learning algorithm which\nimproves the partial fault tolerance of multilayer neural networks,”\nNeural Networks, vol. 12, no. 1, pp. 91–106, 1999.\n[48] C.-S. Leung, W. Y. Wan, and R. Feng, “A regularizer approach for\nRBF networks under the concurrent weight failure situation,” IEEE\ntransactions on neural networks and learning systems, vol. 28, no. 6,\npp. 1360–1372, 2016.\n[49] C. Liu, M. Hu, J. P. Strachan, and H. Li, “Rescuing memristor-based\nneuromorphic design with high defects,” in 2017 54th ACM/EDAC/IEEE\nDesign Automation Conference (DAC).\nIEEE, 2017, pp. 1–6.\n[50] L. Xia, M. Liu, X. Ning, K. Chakrabarty, and Y. Wang, “Fault-tolerant\ntraining with on-line fault detection for rram-based neural computing\nsystems,” in Proceedings of the 54th Annual Design Automation Con-\nference 2017, 2017, pp. 1–6.\n[51] D. Xu, K. Xing, C. Liu, Y. Wang, Y. Dai, L. Cheng, H. Li, and L. Zhang,\n“Resilient neural network training for accelerators with computing\nerrors,” in 2019 IEEE 30th International Conference on Application-\nspeciﬁc Systems, Architectures and Processors (ASAP), vol. 2160-052X,\n2019, pp. 99–102.\n[52] T. Liu, W. Wen, L. Jiang, Y. Wang, C. Yang, and G. Quan, “A fault-\ntolerant neural network architecture,” in 2019 56th ACM/IEEE Design\nAutomation Conference (DAC).\nIEEE, 2019, pp. 1–6.\n[53] T. G. Dietterich and G. Bakiri, “Solving multiclass learning problems via\nerror-correcting output codes,” Journal of artiﬁcial intelligence research,\nvol. 2, pp. 263–286, 1994.\n[54] C.-T. Chin, K. Mehrotra, C. Mohan, and S. Rankat, “Training techniques\nto obtain fault-tolerant neural networks,” in Proceedings of IEEE 24th\nInternational Symposium on Fault- Tolerant Computing, 1994, pp. 360–\n369.\n[55] A. Ahmadi, M. Sargolzaie, S. Fakhraie, C. Lucas, and S. Vakili, “A\nlow-cost fault-tolerant approach for hardware implementation of artiﬁ-\ncial neural networks,” in 2009 International Conference on Computer\nEngineering and Technology, vol. 2, 2009, pp. 93–97.\n[56] F. Morgado-Dias, R. Borralho, and P. Fontes, “Ftset-a software tool\nfor fault tolerance evaluation and improvement,” Neural Computing and\nApplications, vol. 19, pp. 701–712, 07 2010.\n[57] X. Ning, G. Ge, W. Li, Z. Zhu, Y. Zheng, X. Chen, Z. Gao, Y. Wang,\nand H. Yang, “Ftt-nas: Discovering fault-tolerant convolutional neural\narchitecture,” ACM Transactions on Design Automation of Electronic\nSystems, vol. 26, no. 6, aug 2021.\n[58] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, “Efﬁcient neural\narchitecture search via parameter sharing,” in International Conference\non Machine Learning (ICML), 2018.\n[59] K.-H. Huang and J. A. Abraham, “Algorithm-based fault tolerance for\nmatrix operations,” IEEE transactions on computers, vol. 100, no. 6, pp.\n518–528, 1984.\n[60] E. Ozen and A. Orailoglu, “Sanity-check: Boosting the reliability of\nsafety-critical deep neural network applications,” in 2019 IEEE 28th\nAsian Test Symposium (ATS).\nIEEE, 2019, pp. 7–75.\n[61] K. Zhao, S. Di, S. Li, X. Liang, Y. Zhai, J. Chen, K. Ouyang,\nF. Cappello, and Z. Chen, “Ft-cnn: Algorithm-based fault tolerance\nfor convolutional neural networks,” IEEE Transactions on Parallel and\nDistributed Systems, vol. 32, no. 7, pp. 1677–1689, 2020.\n[62] D. Filippas, N. Margomenos, N. Mitianoudis, C. Nicopoulos, and\nG. Dimitrakopoulos, “Low-cost online convolution checksum checker,”\nIEEE Transactions on Very Large Scale Integration (VLSI) Systems,\n2021.\n[63] T. Marty, T. Yuki, and S. Derrien, “Safe overclocking for cnn accel-\nerators through algorithm-level error detection,” IEEE Transactions on\nComputer-Aided Design of Integrated Circuits and Systems, vol. 39,\nno. 12, pp. 4777–4790, 2020.\n[64] N. Khoshavi, A. Roohi, C. Broyles, S. Sargolzaei, Y. Bi, and D. Z. Pan,\n“Shieldenn: Online accelerated framework for fault-tolerant deep neural\nnetwork architectures,” in 2020 57th ACM/IEEE Design Automation\nConference (DAC).\nIEEE, 2020, pp. 1–6.\n[65] Y.-H. Chen, J. Emer, and V. Sze, “Eyeriss: A Spatial Architecture\nfor Energy-Efﬁcient Dataﬂow for Convolutional Neural Networks,” in\n2016 ACM/IEEE 43rd Annual International Symposium on Computer\nArchitecture (ISCA).\nIEEE, 2016, pp. 367–379.\n[66] F. J. Meyer and D. K. Pradhan, “Modeling Defect Spatial Distribution,”\nIEEE Transactions on Computers, vol. 38, no. 4, pp. 538–546, 1989.\n[67] J. Qian, Z. Zhou, T. Gu, L. Zhao, and L. Chang, “Optimal Reconﬁgura-\ntion of High-performance VLSI Subarrays with Network Flow,” IEEE\nTransactions on Parallel and Distributed Systems, vol. 27, no. 12, pp.\n3575–3587, 2016.\n[68] M. Horowitz, “1.1 computing’s energy problem (and what we can do\nabout it),” in 2014 IEEE International Solid-State Circuits Conference\nDigest of Technical Papers (ISSCC), 2014, pp. 10–14.\n[69] B. Reagen, P. Whatmough, R. Adolf, S. Rama, H. Lee, S. K. Lee,\nJ. M. Hern´andez-Lobato, G.-Y. Wei, and D. Brooks, “Minerva: Enabling\nlow-power, highly-accurate deep neural network accelerators,” in 2016\nACM/IEEE 43rd Annual International Symposium on Computer Archi-\ntecture (ISCA), 2016, pp. 267–278.\n[70] L. Yang, D. Bankman, B. Moons, M. Verhelst, and B. Murmann, “Bit\nerror tolerance of a cifar-10 binarized convolutional neural network\nprocessor,” in 2018 IEEE International Symposium on Circuits and\nSystems (ISCAS).\nIEEE, 2018, pp. 1–5.\n[71] J. A. Clemente, W. Mansour, R. Ayoubi, F. Serrano, H. Mecha, H. Ziade,\nW. El Falou, and R. Velazco, “Hardware implementation of a fault-\ntolerant hopﬁeld neural network on fpgas,” Neurocomputing, vol. 171,\npp. 1606–1609, 2016.\n[72] J. Chen and J. Hu, “Energy-efﬁcient digital signal processing via voltage-\noverscaling-based residue number system,” IEEE Transactions on Very\nLarge Scale Integration (VLSI) Systems, vol. 21, no. 7, pp. 1322–1332,\n2013.\n[73] C. Chen, W. Qian, M. Imani, X. Yin, and C. Zhuo, “PAM: a piecewise-\nlinearly-approximated ﬂoating-point multiplier with unbiasedness and\nconﬁgurability,” IEEE Transactions on Computers, pp. 1–15, 2022.\n[74] H. Jiang, L. liu, F. Lombardi, and J. Han, “Low-power unsigned divider\nand square root circuit designs using adaptive approximation,” IEEE\nTransactions on Computers, vol. 68, no. 11, pp. 1635–1646, 2019.\n[75] B. Liu, H. Cai, Z. Wang, Y. Sun, Z. Shen, W. Zhu, Y. Li, Y. Gong, W. Ge,\nJ. Yang, and L. Shi, “A 22nm, 10.8µW/15.1µW dual computing modes\nhigh power-performance-area efﬁciency domained background noise\naware keyword- spotting processor,” IEEE Transactions on Circuits and\nSystems I: Regular Papers, vol. 67, no. 12, pp. 4733–4746, 2020.\n[76] H. Jiang, F. J. H. Santiago, H. Mo, L. Liu, and J. Han, “Approximate\narithmetic circuits: A survey, characterization, and recent applications,”\nProceedings of the IEEE, vol. 108, no. 12, pp. 2108–2135, 2020.\n[77] W. Qian, X. Li, M. D. Riedel, K. Bazargan, and D. J. Lilja, “An\narchitecture for fault-tolerant computation with stochastic logic,” IEEE\nTransactions on Computers, vol. 60, no. 1, pp. 93–105, 2011.\n[78] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, “Xnor-net:\nImagenet classiﬁcation using binary convolutional neural networks,” in\nEuropean conference on computer vision. Springer, 2016, pp. 525–542.\n[79] A. Ardakani, A. Ardakani, and W. J. Gross, “Fault-tolerance of bina-\nrized and stochastic computing-based neural networks,” in 2021 IEEE\nWorkshop on Signal Processing Systems (SiPS), 2021, pp. 52–57.\n[80] Y. Zhang, S. Lin, R. Wang, Y. Wang, Y. Wang, W. Qian, and R. Huang,\n“When sorting network meets parallel bitstreams: A fault-tolerant paral-\nlel ternary neural network accelerator based on stochastic computing,” in\n2020 Design, Automation Test in Europe Conference Exhibition (DATE),\n2020, pp. 1287–1290.\n[81] J. Zhang, K. Rangineni, Z. Ghodsi, and S. Garg, “Thundervolt: enabling\naggressive voltage underscaling and timing error resilience for energy\nefﬁcient deep learning accelerators,” in Proceedings of the 55th Annual\nDesign Automation Conference, 2018, pp. 1–6.\n[82] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and\nA. Moshovos, “Cnvlutin: Ineffectual-neuron-free deep neural network\ncomputing,” ACM SIGARCH Computer Architecture News, vol. 44,\nno. 3, pp. 1–13, 2016.\n[83] S. Kim, P. Howe, T. Moreau, A. Alaghi, L. Ceze, and V. Sathe,\n“Matic: Learning around errors for efﬁcient low-voltage neural network\naccelerators,” in 2018 Design, Automation & Test in Europe Conference\n& Exhibition (DATE).\nIEEE, 2018, pp. 1–6.\n[84] Y. Wang, J. Deng, Y. Fang, H. Li, and X. Li, “Resilience-aware fre-\nquency tuning for neural-network-based approximate computing chips,”\nIEEE Transactions on Very Large Scale Integration (VLSI) Systems,\nvol. 25, no. 10, pp. 2736–2748, 2017.\n[85] L. Li, D. Xu, K. Xing, C. Liu, Y. Wang, H. Li, and X. Li, “Squeezing\nthe last mhz for cnn acceleration on fpgas,” in 2019 IEEE International\nTest Conference in Asia (ITC-Asia).\nIEEE, 2019, pp. 151–156.\n",
  "categories": [
    "cs.AR",
    "cs.AI",
    "cs.LG",
    "B.2.3; B.8.1"
  ],
  "published": "2022-04-05",
  "updated": "2022-04-05"
}