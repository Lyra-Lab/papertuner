{
  "id": "http://arxiv.org/abs/2204.02380v1",
  "title": "CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations",
  "authors": [
    "Leonard Salewski",
    "A. Sophia Koepke",
    "Hendrik P. A. Lensch",
    "Zeynep Akata"
  ],
  "abstract": "Providing explanations in the context of Visual Question Answering (VQA)\npresents a fundamental problem in machine learning. To obtain detailed insights\ninto the process of generating natural language explanations for VQA, we\nintroduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with\nnatural language explanations. For each image-question pair in the CLEVR\ndataset, CLEVR-X contains multiple structured textual explanations which are\nderived from the original scene graphs. By construction, the CLEVR-X\nexplanations are correct and describe the reasoning and visual information that\nis necessary to answer a given question. We conducted a user study to confirm\nthat the ground-truth explanations in our proposed dataset are indeed complete\nand relevant. We present baseline results for generating natural language\nexplanations in the context of VQA using two state-of-the-art frameworks on the\nCLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation\ngeneration quality for different question and answer types. Additionally, we\nstudy the influence of using different numbers of ground-truth explanations on\nthe convergence of natural language generation (NLG) metrics. The CLEVR-X\ndataset is publicly available at\n\\url{https://explainableml.github.io/CLEVR-X/}.",
  "text": "CLEVR-X: A Visual Reasoning Dataset for Natural\nLanguage Explanations\nLeonard Salewski1[0000−0001−8531−3011], A. Sophia Koepke1[0000−0002−5807−0576],\nHendrik P. A. Lensch1[0000−0003−3616−8668], and\nZeynep Akata1,2,3[0000−0002−1432−7747]\n1 University of T¨ubingen, T¨ubingen, Germany\n2 MPI for Informatics, Saarbr¨ucken, Germany\n3 MPI for Intelligent Systems, T¨ubingen, Germany\n{leonard.salewski, a-sophia.koepke, hendrik.lensch,\nzeynep.akata}@uni-tuebingen.de\nAbstract. Providing explanations in the context of Visual Question Answering\n(VQA) presents a fundamental problem in machine learning. To obtain detailed\ninsights into the process of generating natural language explanations for VQA, we\nintroduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with\nnatural language explanations. For each image-question pair in the CLEVR dataset,\nCLEVR-X contains multiple structured textual explanations which are derived\nfrom the original scene graphs. By construction, the CLEVR-X explanations are\ncorrect and describe the reasoning and visual information that is necessary to\nanswer a given question. We conducted a user study to conﬁrm that the ground-\ntruth explanations in our proposed dataset are indeed complete and relevant.\nWe present baseline results for generating natural language explanations in the\ncontext of VQA using two state-of-the-art frameworks on the CLEVR-X dataset.\nFurthermore, we provide a detailed analysis of the explanation generation quality\nfor different question and answer types. Additionally, we study the inﬂuence of\nusing different numbers of ground-truth explanations on the convergence of natural\nlanguage generation (NLG) metrics. The CLEVR-X dataset is publicly available\nat https://explainableml.github.io/CLEVR-X/.\nKeywords: Visual Question Answering · Natural Language Explanations.\n1\nIntroduction\nExplanations for automatic decisions form a crucial step towards increasing transparency\nand human trust in deep learning systems. In this work, we focus on natural language\nexplanations in the context of vision-language tasks.\nIn particular, we consider the vision-language task of Visual Question Answering\n(VQA) which consists of answering a question about an image. This requires multiple\nskills, such as visual perception, text understanding, and cross-modal reasoning in the\nA version of the contribution has been accepted for publication, after peer review but is not the\nVersion of Record and does not reﬂect post-acceptance improvements, or any corrections. The\nVersion of Record will be available online at: https://doi.org/10.1007/978-3-031-04083-2 5.\narXiv:2204.02380v1  [cs.CV]  5 Apr 2022\n2\nL. Salewski et al.\nVQA-X\nQuestion: Does this scene\nlook like it could be from the\nearly 1950s?\nAnswer | Explanation:\nYes | The photo is in black and\nwhite and the cars are all clas-\nsic designs from the 1950s\ne-SNLI-VE\nHypothesis: A woman is hold-\ning a child.\nAnswer | Explanation:\nEntailment | If a woman holds\na child she is holding a child.\nCLEVR-X\nQuestion: There is a purple\nmetallic ball; what number of\ncyan objects are right of it?\nAnswer | Explanation:\n1 | There is a cyan cylinder\nwhich is on the right side of\nthe purple metallic ball.\nFig. 1: Comparing examples from the VQA-X (left), e-SNLI-VE (middle), and CLEVR-\nX (right) datasets. The explanation in VQA-X requires prior knowledge (about cars from\nthe 1950s), e-SNLI-VE argues with a tautology, and our CLEVR-X only uses abstract\nvisual reasoning.\nvisual and language domains. A natural language explanation for a given answer allows\na better understanding of the reasoning process for answering the question and adds\ntransparency. However, it is challenging to formulate what comprises a good textual\nexplanation in the context of VQA involving natural images.\nExplanation datasets commonly used in the context of VQA, such as the VQA-X\ndataset [26] or the e-SNLI-VE dataset [13,29] for visual entailment, contain explanations\nof widely varying quality since they are generated by humans. The ground-truth expla-\nnations in VQA-X and e-SNLI-VE can range from statements that merely describe an\nimage to explaining the reasoning about the question and image involving prior informa-\ntion, such as common knowledge. One example for a ground-truth explanation in VQA-X\nthat requires prior knowledge about car designs from the 1950s can be seen in Fig. 1. The\ne-SNLI-VE dataset contains numerous explanation samples which consist of repeated\nstatements (“x because x”). Since existing explanation datasets for vision-language tasks\ncontain immensely varied explanations, it is challenging to perform a structured analysis\nof strengths and weaknesses of existing explanation generation methods.\nIn order to ﬁll this gap, we propose the novel, diagnostic CLEVR-X dataset for visual\nreasoning with natural language explanations. It extends the synthetic CLEVR [27]\ndataset through the addition of structured natural language explanations for each question-\nimage pair. An example for our proposed CLEVR-X dataset is shown in Fig. 1. The\nsynthetic nature of the CLEVR-X dataset results in several advantages over datasets\nthat use human explanations. Since the explanations are synthetically constructed from\nthe underlying scene graph, the explanations are correct and do not require auxiliary\nprior knowledge. The synthetic textual explanations do not suffer from errors that get\nintroduced with human explanations. Nevertheless, the explanations in the CLEVR-X\ndataset are human parsable as demonstrated in the human user study that we conducted.\nFurthermore, the explanations contain all the information that is necessary to answer a\nCLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\n3\ngiven question about an image without seeing the image. This means that the explanations\nare complete with respect to the question about the image.\nThe CLEVR-X dataset allows for detailed diagnostics of natural language expla-\nnation generation methods in the context of VQA. For instance, it contains a wider\nrange of question types than other related datasets. We provide baseline performances\non the CLEVR-X dataset using recent frameworks for natural language explanations in\nthe context of VQA. Those frameworks are jointly trained to answer the question and\nprovide a textual explanation. Since the question family, question complexity (number of\nreasoning steps required), and the answer type (binary, counting, attributes) is known for\neach question and answer, the results can be analyzed and split according to these groups.\nIn particular, the challenging counting problem [48], which is not well-represented in the\nVQA-X dataset, can be studied in detail on CLEVR-X. Furthermore, our dataset contains\nmultiple ground-truth explanations for each image-question pair. These capture a large\nportion of the space of correct explanations which allows for a thorough analysis of the\ninﬂuence of the number of ground-truth explanations used on the evaluation metrics. Our\napproach of constructing textual explanations from a scene graph yields a great resource\nwhich could be extended to other datasets that are based on scene graphs, such as the\nCLEVR-CoGenT dataset.\nTo summarize, we make the following four contributions: (1) We introduce the\nCLEVR-X dataset with natural language explanations for Visual Question Answering;\n(2) We conﬁrm that the CLEVR-X dataset consists of correct explanations that contain\nsufﬁcient relevant information to answer a posed question by conducting a user study;\n(3) We provide baseline performances with two state-of-the-art methods that were\nproposed for generating textual explanations in the context of VQA; (4) We use the\nCLEVR-X dataset for a detailed analysis of the explanation generation performance for\ndifferent subsets of the dataset and to better understand the metrics used for evaluation.\n2\nRelated work\nIn this section, we discuss several themes in the literature that relate to our work, namely\nVisual Question Answering, Natural language explanations (for vision-language tasks),\nand the CLEVR dataset.\nVisual Question Answering (VQA). The VQA [5] task has been addressed by several\nworks that apply attention mechanisms to text and image features [56,55,60,45,16].\nHowever, recent works observed that the question-answer bias in common VQA datasets\ncan be exploited in order to answer questions without leveraging any visual informa-\ntion [1,2,27,59]. This has been further investigated in more controlled dataset settings,\nsuch as the CLEVR [27], VQA-CP [2], and GQA [25] datasets. In addition to a controlled\ndataset setting, our proposed CLEVR-X dataset contains natural language explanations\nthat enable a more detailed analysis of the reasoning in the context of VQA.\nNatural language explanations. Decisions made by neural networks can be visually\nexplained with visual attribution that is determined by introspecting trained networks\nand their features [46,57,43,8,58], by using input perturbations [42,14,15], or by training\na probabilistic feature attribution model along with a task-speciﬁc CNN [30]. Comple-\nmentary to visual explanations methods that tend to not help users distinguish between\n4\nL. Salewski et al.\ncorrect and incorrect predictions [32], natural language explanations have been investi-\ngated for a variety of tasks, such as ﬁne-grained visual object classiﬁcation [21,20], or\nself-driving car models [31]. The requirement to ground language explanations in the\ninput image can prevent shortcuts, such as relying on dataset statistics or referring to\ninstance attributes that are not present in the image. For a comprehensive overview of\nresearch on explainability and interpretability, we refer to recent surveys [7,10,17].\nNatural language explanations for vision-language tasks. Multiple datasets for natu-\nral language explanations in the context of vision-language tasks have been proposed,\nsuch as the VQA-X [26], VQA-E [35], and e-SNLI-VE datasets [29]. VQA-X [26]\naugments a small subset of the VQA v2 [18] dataset for the Visual Question Answering\ntask with human explanations. Similarly, the VQA-E dataset [35] extends the VQA v2\ndataset by sourcing explanations from image captions. However, the VQA-E explana-\ntions resemble image descriptions and do not provide satisfactory justiﬁcations whenever\nprior knowledge is required [35]. The e-SNLI-VE [29,13] dataset combines human\nexplanations from e-SNLI [11] and the image-sentence pairs for the Visual Entail-\nment task from SNLI-VE [54]. In contrast to the VQA-E, VQA-X, and e-SNLI-VE\ndatasets which consist of human explanations or image captions, our proposed dataset\ncontains systematically constructed explanations derived from the associated scene\ngraphs. Recently, several works have aimed at generating natural language explanations\nfor vision-language tasks [26,53,52,38,40,29]. In particular, we use the PJ-X [26] and\nFM [53] frameworks to obtain baseline results on our proposed CLEVR-X dataset.\nThe CLEVR dataset. The CLEVR dataset [27] was proposed as a diagnostic dataset\nto inspect the visual reasoning of VQA models. Multiple frameworks have been pro-\nposed to address the CLEVR task [24,41,23,28,47,44]. To add explainability, the XNM\nmodel [44] adopts the scene graph as an inductive bias which enables the visualization of\nthe reasoning based on the attention on the nodes of the graph. There have been numerous\ndataset extensions for the CLEVR dataset, for instance to measure the generalization\ncapabilities of models pre-trained on CLEVR (CLOSURE [51]), to evaluate object\ndetection and segmentation (CLEVR-Ref+ [37]), or to benchmark visual dialog models\n(CLEVR dialog [34]). The Compositional Reasoning Under Uncertainty (CURI) bench-\nmark uses the CLEVR renderer to construct a test bed for compositional and relational\nlearning under uncertainty [49]. [22] provide an extensive survey of further experimental\ndiagnostic benchmarks for analyzing explainable machine learning frameworks along\nwith proposing the KandinskyPATTERNS benchmark that contains synthetic images\nwith simple 2-dimensional objects. It can be used for testing the quality of explana-\ntions and concept learning. Additionally, [6] proposed the CLEVR-XAI-simple and\nCLEVR-XAI-complex datasets which provide ground-truth segmentation information\nfor heatmap-based visual explanations. Our CLEVR-X augments the existing CLEVR\ndataset with explanations, but in contrast to (heatmap-based) visual explanations, we\nfocus on natural language explanations.\n3\nThe CLEVR-X dataset\nIn this section, we introduce the CLEVR-X dataset that consists of natural language\nexplanations in the context of VQA. The CLEVR-X dataset extends the CLEVR dataset\nCLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\n5\nwith 3.6 million natural language explanations for 850k question-image pairs. In Sec-\ntion 3.1, we brieﬂy describe the CLEVR dataset, which forms the base for our proposed\ndataset. Next, we present an overview of the CLEVR-X dataset by describing how\nthe natural language explanations were obtained in Section 3.2, and by providing a\ncomprehensive analysis of the CLEVR-X dataset in Section 3.3. Finally, in Section 3.4,\nwe present results for a user study on the CLEVR-X dataset.\n3.1\nThe CLEVR dataset\nThe CLEVR dataset consists of images with corresponding full scene graph annotations\nwhich contain information about all objects in a given scene (as nodes in the graph) along\nwith spatial relationships for all object pairs. The synthetic images in the CLEVR dataset\ncontain three to ten (at least partially visible) objects in each scene, where each object\nhas the four distinct properties size, color, material, and shape. There are three\nshapes (box, sphere, cylinder), eight colors (gray, red, blue, green, brown,\npurple, cyan, yellow), two sizes (large, small), and two materials (rubber,\nmetallic). This allows for up to 96 different combinations of properties.\nThere are a total of 90 different question families in the dataset which are grouped\ninto 9 different question types. Each type contains questions from between 5 and 28\nquestion families. In the following, we describe the 9 question types in more detail.\nHop questions: The zero hop, one hop, two hop, and three hop question types contain\nup to three relational reasoning steps, e.g. “What color is the cube to the left of the ball?”\nis a one hop question.\nCompare and relate questions: The compare integer, same relate, and comparison\nquestion types require the understanding and comparison of multiple objects in a scene.\nQuestions of the compare integer type compare counts corresponding to two independent\nclauses (e.g. “Are there more cubes than red balls?”). Same relate questions reason about\nobjects that have the same attribute as another previously speciﬁed object (e.g. “What\nis the color of the cube that has the same size as the ball?”). In contrast, comparison\nquestion types compare the attributes of two objects (e.g. “Is the color of the cube\nthe same as the ball?”).\nSingle and/or questions: Single or questions identify objects that satisfy an exclusive\ndisjunction condition (e.g. “How many objects are either red or blue?”). Similarly, sin-\ngle and questions apply multiple relations and ﬁlters to ﬁnd an object that satisﬁes all\nconditions (e.g. “How many objects are red and to the left of the cube.”).\nEach CLEVR question can be represented by a corresponding functional program\nand its natural language realization. A functional program is composed of basic functions\nthat resemble elementary visual reasoning operations, such as ﬁltering objects by one or\nmore properties, spatially relating objects to each other, or querying object properties.\nFurthermore, logical operations like and and or, as well as counting operations like count,\nless, more, and equal are used to build complex questions. Executing the functional\nprogram associated with the question against the scene graph yields the correct answer\nto the question. We can distinguish between three different answer types: Binary answers\n6\nL. Salewski et al.\nQuestion: What number of other\nobjects are there of the same\nmaterial as the tiny thing?\nScene graph\nProgram\nParameters\nTracing result\nfilter unique <size> \n<tiny>\n<obj1>:\nsame <attribute>\n<material>\n<obj2> : \ncount\n<verb2> :  are \nExplanation:\nThere are a large yellow metallic cube and cylinder \nthat have the same material as the tiny sphere.\nTemplate: \nThere <verb2> a <obj2> {that, which} have the \n{same, identical} <attribute> as the <obj1>.\nImage\nTracing the functional program\nExplanation generation\nFig. 2: CLEVR-X dataset generation: Generating a natural language explanation for a\nsample from the CLEVR dataset. Based on the question, the functional program for\nanswering the question is executed on the scene graph and traced. A language template\nis used to cast the gathered information into a natural language explanation.\n(yes or no), counting answers (integers from 0 to 10), and attribute answers (any of\nthe possible values of shape, color, size, or material).\n3.2\nDataset generation\nHere, we describe the process for generating natural language explanations for the\nCLEVR-X dataset. In contrast to image captions, the CLEVR-X explanations only\ndescribe image elements that are relevant to a speciﬁc input question. The explanation\ngeneration process for a given question-image pair is illustrated in Fig. 2. It consists of\nthree steps: Tracing the functional program, relevance ﬁltering (not shown in the ﬁgure),\nand explanation generation. In the following, we will describe those steps in detail.\nTracing the functional program. Given a question-image pair from the CLEVR dataset,\nwe trace the execution of the functional program (that corresponds to the question) on the\nscene graph (which is associated with the image). The generation of the CLEVR dataset\nuses the same step to obtain a question-answer pair. When executing the basic functions\nthat comprise the functional program, we record their outputs in order to collect all the\ninformation required for explaining a ground-truth answer.\nIn particular, we trace the ﬁlter, relate and same-property functions and record the\nreturned objects and their properties, such as, for instance, shape or size. As a result,\nthe tracing omits objects in the scene that are not relevant for the question. As we are\naiming for complete explanations for all question types, each explanation has to mention\nall the objects that were needed to answer the question, i.e. all the evidence that was\nobtained during tracing. For example, for counting questions, all objects that match\nthe ﬁlter function preceding the counting step are recorded during tracing. For and\nquestions, we merge the tracing results of the preceding functions which results in short\nand readable explanations. In summary, the tracing produces a complete and correct\nunderstanding of the objects and relevant properties which contributed to an answer.\nRelevance ﬁltering. To keep the explanation at a reasonable length, we ﬁlter the ob-\nject attributes that are mentioned in the explanation according to their relevance. For\nexample, the color of an object is not relevant for a given question that asks about\nthe material of said object. We deem all properties that were listed in the question\nto be relevant. This makes it easier to recognize the same referenced object in both the\nCLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\n7\nquestion and explanation. As the shape property also serves as a noun in CLEVR, our\nexplanations always mention the shape to avoid using generic shape descriptions like\n“object” or “thing”. We distinguish between objects which are used to build the question\n(e.g. “[. . . ] that is left of the cube?”) and those that are the subject of the posed question\n(e.g. “What color is the sphere that is left of the cube?”). For the former, we do not\nmention any additional properties, and for the latter, we mention the queried property\n(e.g. color) for question types yielding attribute answers.\nExplanation generation. To obtain the ﬁnal natural language explanations, each ques-\ntion type is equipped with one or more natural language templates with variations in\nterms of the wording used. Each template contains placeholders which are ﬁlled with the\noutput of the previous steps, i.e. the tracing of the functional program and subsequent\nﬁltering for relevance. As mentioned above, our explanations use the same property\ndescriptions that appeared in the question. This is done to ensure that the wording of\nthe explanation is consistent with the given question, e.g. for the question “Is there a\nsmall object?” we generate the explanation “Yes there is a small cube.”1. We randomly\nsample synonyms for describing the properties of objects that do not appear in the\nquestion. If multiple objects are mentioned in the explanation, we randomize their order.\nIf the tracing step returned an empty set, e.g. if no object exists that matches the given\nﬁltering function for an existence or counting question, we state that no relevant object\nis contained in the scene (e.g. “There is no red cube.”).\nIn order to decrease the overall sentence length and to increase the readability,\nwe aggregate repetitive descriptions (e.g. “There is a red cube and a red cube”) using\nnumerals (e.g. “There are two red cubes.”). In addition, if a function of the functional\nprogram merely restricts the output set of a preceding function, we only mention the\noutputs of the later function. For instance, if a same-color function yields a large\nand a small cube, and a subsequent filter-large function restricts the output to\nonly the large cube, we do not mention the output of same-color, as the output of the\nfollowing filter-large causes natural language redundancies2.\nThe selection of different language templates, random sampling of synonyms and\nrandomization of the object order (if possible) results in multiple different explanations.\nWe uniformly sample up to 10 different explanations per question for our dataset.\nDataset split. We provide explanations for the CLEVR training and validation sets,\nskipping only a negligible subset (less than 0.04h) of questions due to malformed\nquestion programs from the CLEVR dataset, e.g. due to disjoint parts of their abstract\nsyntax trees. In total, this affected 25 CLEVR training and 4 validation questions.\nAs the scene graphs and question functional programs are not publicly available for\nthe CLEVR test set, we use the original CLEVR validation subset as the CLEVR-X test\nset. 20% of the CLEVR training set serve as the CLEVR-X validation set. We perform\n1 The explanation could have used the synonym “box” instead of “cube”. In contrast, “tiny” and\n“small” are also synonyms in CLEVR, but the explanation would not have been consistent with\nthe question which used “small”.\n2 E.g. for the question: “How many large objects have the same color as the cube?”, we do not\ngenerate the explanation “There are a small and a large cube that have the same color as the red\ncylinder of which only the large cube is large.” but instead only write “There is a large cube\nthat has the same color as the red cylinder.”\n8\nL. Salewski et al.\nTable 1: Statistics of the CLEVR-X dataset compared to the VQA-X, and e-SNLI-VE\ndatasets. We show the total number of images, questions, and explanations, vocabulary\nsize, and the average number of explanations per question, the average number of words\nper explanation, and the average number of words per question. Not all subset values\nadd up to the Total values since some subsets have overlaps (e.g. for the vocabulary).\nDataset\nSubset\nTotal #\nAverage #\nImages Questions Explanations Vocabulary Explanations Expl. Words Quest. Words\nVQA-X\nTrain\n24,876\n29,549\n31,536\n9,423\n1.07\n10.55\n7.50\nVal\n1,431\n1,459\n4,377\n3,373\n3.00\n10.88\n7.56\nTest\n1,921\n1,921\n5,904\n3,703\n3.07\n10.93\n7.31\nTotal\n28,180\n32,886\n41,817\n10,315\n1.48\n10.64\n7.49\ne-SNLI-VE\nTrain\n29,779\n401,672\n401,672\n36,778\n1.00\n13.62\n8.23\nVal\n1,000\n14,339\n14,339\n8,311\n1.00\n14.67\n8.10\nTest\n998\n14,712\n14,712\n8,334\n1.00\n14.59\n8.20\nTotal\n31,777\n430,723\n430,723\n38,208\n1.00\n13.69\n8.23\nCLEVR-X\nTrain\n56,000\n559,969\n2,401,275\n96\n4.29\n21.52\n21.61\nVal\n14,000\n139,995\n599,711\n96\n4.28\n21.54\n21.62\nTest\n15,000\n149,984\n644,151\n96\n4.29\n21.54\n21.62\nTotal\n85,000\n849,948\n3,645,137\n96\n4.29\n21.53\n21.61\nthis split on the image-level to avoid any overlap between images in the CLEVR-X\ntraining and validation sets. Furthermore, we veriﬁed that the relative proportion of\nsamples from each question and answer type in the CLEVR-X training and validation\nsets is similar, such that there are no biases towards speciﬁc question or answer types.\nCode for generating the CLEVR-X dataset and the dataset itself are publicly available\nat https://github.com/ExplainableML/CLEVR-X.\n3.3\nDataset analysis\nWe compare the CLEVR-X dataset to the related VQA-X and e-SNLI-VE datasets in\nTable 1. Similar to CLEVR-X, VQA-X contains natural language explanations for the\nVQA task. However, different to the natural images and human explanations in VQA-\nX, CLEVR-X consists of synthetic images and explanations. The e-SNLI-VE dataset\nprovides explanations for the visual entailment (VE) task. VE consists of classifying an\ninput image-hypothesis pair into entailment / neutral / contradiction categories.\nThe CLEVR-X dataset is signiﬁcantly larger than the VQA-X and e-SNLI-VE\ndatasets in terms of the number of images, questions, and explanations. In contrast to\nthe two other datasets, CLEVR-X provides (on average) multiple explanations for each\nquestion-image pair in the train set. Additionally, the average number of words per\nexplanation is also higher. Since the explanations are built to explain each component\nmentioned in the question, long questions require longer explanations than short ques-\ntions. Nevertheless, by design, there are no unnecessary redundancies. The explanation\nlength in CLEVR-X is very strongly correlated with the length of the corresponding ques-\nCLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\n9\n10\n20\n30\n40\n50\nWords\n0k\n10k\n20k\n30k\n40k\n50k\n60k\nCount\nAverage explanation length\nQuestion types\ncompare integer\nsingle and\ncomparison\nzero hop\nthree hop\ntwo hop\nsame relate\none hop\nsingle or\n0\n10\n20\n30\n40\n50\nWords\n0k\n10k\n20k\n30k\n40k\n50k\n60k\nCount\nAverage explanation length\nDataset\nVQA-X\nCLEVR-X\ne-SNLI-VE\nFig. 3: Stacked histogram of the average explanation lengths measured in words for the\n9 question types for the CLEVR-X training set (left). Explanation length distribution\nfor the CLEVR-X, VQA-X, and e-SNLI-VE training sets (right). The long tail of the\ne-SNLI-VE distribution (125 words) was cropped out for better readability.\ntion (Spearman’s correlation coefﬁcient between the number of words in the explanations\nand questions is 0.89).\nFigure 3 (left) shows the explanation length distribution in the CLEVR-X dataset for\nthe 9 question types. The shortest explanation consists of 7 words, and the longest one has\n53 words. On average, the explanations contain 21.53 words. In Fig. 3 (right) and Table 1,\nwe can observe that explanations in CLEVR-X tend to be longer than the explanations\nin the VQA-X dataset. Furthermore, VQA-X has signiﬁcantly fewer samples overall\nthan the CLEVR-X dataset. The e-SNLI-VE dataset also contains longer explanations\n(that are up to 125 words long), but the CLEVR-X dataset is signiﬁcantly larger than the\ne-SNLI-VE dataset. However, due to the synthetic nature and limited domain of CLEVR,\nthe vocabulary of CLEVR-X is very small with only 96 different words. Unfortunately,\nVQA-X and e-SNLI-VE contain spelling errors, resulting in multiple versions of the\nsame words. Models trained on CLEVR-X circumvent those aforementioned challenges\nand can purely focus on visual reasoning and explanations for the same. Therefore,\nNatural Language Generation (NLG) metrics applied to CLEVR-X indeed capture the\nfactual correctness and completeness of an explanation.\n3.4\nUser study on explanation completeness and relevance\nIn this section, we describe our user study for evaluating the completeness and relevance\nof the generated ground-truth explanations in the CLEVR-X dataset. We wanted to\nverify whether humans are successfully able to parse the synthetically generated textual\nexplanations and to select complete and relevant explanations. While this is obvious\nfor easier explanations like “There is a blue sphere.”, it is less trivial for more complex\nexplanations such as “There are two red cylinders in front of the green cube that is to the\nright of the tiny ball.” Thus, strong human performance in the user study indicates that\nthe sentences are parsable by humans.\nWe performed our user study using Amazon Mechanical Turk (MTurk). It consisted\nof two types of Human Intelligence Tasks (HITs). Each HIT was made up of (1) An\nexplanation of the task; (2) A non-trivial example, where the correct answers are already\n10\nL. Salewski et al.\nFig. 4: Two examples from our user study to evaluate the completeness (left) and rele-\nvance (right) of natural language explanations in the CLEVR-X dataset.\nselected; (3) A CAPTCHA [3] to verify that the user is human; (4) The problem deﬁnition\nconsisting of a question and an image; (5) A user qualiﬁcation step, for which the user\nhas to correctly answer a question about an image. This ensures that the user is able\nto answer the question in the ﬁrst place, a necessary condition to participate in our\nuser study; (6) Two explanations from which the user needs to choose one. Example\nscreenshots of the user interface for the user study are shown in Fig. 4.\nFor the two different HIT types, we randomly sampled 100 explanations from each\nof the 9 question types, resulting in a total of 1800 samples for the completeness and\nrelevance tasks. For each task sample, we requested 3 different MTurk workers based in\nthe US (with high acceptance rate of > 95% and over 5000 accepted HITs). A total of 78\nworkers participated in the completeness HITs. They took on average 144.83 seconds per\nHIT. The relevance task was carried out by 101 workers which took on average 120.46\nseconds per HIT. In total, 134 people participated in our user study. In the following,\nwe describe our ﬁndings regarding the completeness and relevance of the CLEVR-X\nexplanations in more detail.\nExplanation completeness. In the ﬁrst part of the user study, we evaluated whether\nhuman users are able to determine if the ground-truth explanations in the CLEVR-X\ndataset are complete (and also correct). We presented the MTurk workers with an image,\na question, and two explanations. As can be seen in Fig. 4 (left), a user had to ﬁrst\nselect the correct answer (yes) before deciding which of the two given explanations was\ncomplete. By design, one of the explanations presented to the user was the complete\nCLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\n11\nTable 2: Results for the user study evaluating the accuracy for the completeness and\nrelevance tasks for the 9 question types in the CLEVR-X dataset.\nZero\nhop\nOne\nhop\nTwo\nhop\nThree\nhop\nSame\nrelate\nCompari-\nson\nCompare\ninteger\nSingle\nor\nSingle\nand\nAll\nCompleteness\n100.00\n98.00\n98.67\n94.00\n100.00\n83.67\n77.00\n84.00\n94.33\n92.19\nRelevance\n99.67\n99.00\n95.67\n89.00\n95.67\n87.33\n83.67\n90.67\n92.00\n92.52\none from the CLEVR-X dataset and the other one was a modiﬁed version for which\nat least one necessary object had been removed. As simply deleting an object from\na textual explanation could lead to grammar errors, we re-generated the explanations\nafter removing objects from the tracing results. This resulted in incomplete, albeit\ngrammatically correct, explanations.\nTo evaluate the ability to determine the completeness of explanations, we measured\nthe accuracy of selecting the complete explanation. The human participants obtained\nan average accuracy of 92.19%, conﬁrming that complete explanations which mention\nall objects necessary to answer a given question were preferred over incomplete ones.\nThe performance was weaker for complex question types, such as compare-integer and\ncomparison with accuracies of only 77.00% and 83.67% respectively, compared to the\neasier zero-hop and one-hop questions with accuracies of 100% and 98.00% respectively.\nAdditionally, there were huge variations in performance across different partici-\npants of the completeness study (Fig. 5 (top left)), with the majority performing very\nwell (>97% answering accuracy) for most question types. For the compare-integer,\ncomparison and single or question types, some workers exhibited a much weaker perfor-\nmance with answering accuracies as low as 0%. The average turnaround time shown in\nFig. 5 (bottom left) conﬁrms that easier question types required less time to be solved\nthan more complex question types, such as three hop and compare integer questions.\nSimilar to the performance, the work time varied greatly between different users.\nExplanation relevance. In the second part of our user study, we analyzed if humans\nare able to identify explanations which are relevant for a given image. For a given\nquestion-image pair, the users had to ﬁrst select the correct answer. Furthermore, they\nwere provided with a correct explanation and another randomly chosen explanation from\nthe same question family (that did not match the image). The task consisted of selecting\nthe correct explanation that matched the image and question content. Explanation 1 in\nthe example user interface shown in Fig. 4 (right) was the relevant one, since Explanation\n2 does not match the question and image.\nThe participants of our user study were able to determine which explanation matched\nthe given question-image example with an average accuracy of 92.52%. Again, the\nperformance for complex question types was weaker than for easier questions. The\ndifﬁculty of the question inﬂuences the accuracy of detecting the relevant explanation,\nsince this task ﬁrst requires understanding the question. Furthermore, complex questions\ntend to be correlated with complex scenes that contain many objects which makes\nthe user’s task more challenging. The accuracy for three-hop questions was 89.00%\ncompared to 99.67% for zero-hop questions. For compare-integer and comparison\n12\nL. Salewski et al.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAnswering accuracy\nCompleteness\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelevance\nzero hop\none hop\ntwo hop\nthree hop\ncompare integer\nsame relate\ncomparison\nsingle or\nsingle and\nQuestion type\n101\n102\n103\nWorkTimeInSeconds\nzero hop\none hop\ntwo hop\nthree hop\ncompare integer\nsame relate\ncomparison\nsingle or\nsingle and\nQuestion type\n101\n102\n103\nFig. 5: Average answering accuracies for each worker (top) and average work time\n(bottom) for the user study (left: completeness, right: relevance). The boxes indicate the\nmean as well as lower and upper quartiles, the lines extend 1.5 interquartile ranges of\nthe lower and upper quartile. All other values are plotted as diamonds.\nquestions, the users obtained accuracies of 83.67% and 87.33% respectively, which is\nsigniﬁcantly lower than the overall average accuracy.\nWe analyzed the answering accuracy per worker in Fig. 5 (top right). The perfor-\nmance varies greatly between workers, with the majority performing very well (>90%\nanswering accuracy) for most question types. Some workers showed much weaker per-\nformance with answering accuracies as low as 0% (e.g. for compare-integer and single\nor questions). Furthermore, the distribution of work time for the relevance task is shown\nin Fig. 5 (bottom right). The turnaround times for each worker exhibit greater variation\non the completeness task (bottom left) compared to the relevance task (bottom right).\nThis might be due to the nature of the different tasks. For the completeness task, the users\nneed to check if the explanation contains all the elements that are necessary to answer\nthe given question. The relevance task, on the other hand, can be solved by detecting a\nsingle non-relevant object to discard the wrong explanation.\nOur user study conﬁrmed that humans are able to parse the synthetically generated\nnatural language explanations in the CLEVR-X dataset. Furthermore, the results have\nshown that users prefer complete and relevant explanations in our dataset over corrupted\nsamples.\nCLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\n13\n4\nExperiments\nWe describe the experimental setup for establishing baselines on our proposed CLEVR-X\ndataset in Section 4.1. In Section 4.2, we present quantitative results on the CLEVR-X\ndataset. Additionally, we analyze the generated explanations for the CLEVR-X dataset\nin relation to the question and answer types in Section 4.3. Furthermore, we study the\nbehavior of the NLG metrics when using different numbers of ground-truth explanations\nfor testing in Section 4.4. Finally, we present qualitative explanation generation results\non the CLEVR-X dataset in Section 4.5.\n4.1\nExperimental setup\nIn this section, we provide details about the datasets and models used to establish\nbaselines for our CLEVR-X dataset and about their training details. Furthermore, we\nexplain the metrics for evaluating the explanation generation performance.\nDatasets. In the following, we summarize the datasets that were used for our experiments.\nIn addition to providing baseline results on CLEVR-X, we also report experimental\nresults on the VQA-X and e-SNLI-VE datasets. Details about our proposed CLEVR-X\ndataset can be found in Section 3. The VQA-X dataset [26] is a subset of the VQA v2\ndataset with a single human-generated textual explanation per question-image pair in\nthe training set and 3 explanations for each sample in the validation and test sets. The\ne-SNLI-VE dataset [13,29] is a large-scale dataset with natural language explanations\nfor the visual entailment task.\nMethods. We used multiple frameworks to provide baselines on our proposed CLEVR-X\ndataset. For the random words baseline, we sample random word sequences of length\nw for the answer and explanation words for each test sample. The full vocabulary\ncorresponding to a given dataset is used as the sampling pool, and w denotes the\naverage number of words forming an answer and explanation in a given dataset. For the\nrandom explanations baseline, we randomly sample an answer-explanation pair from\nthe training set and use this as the prediction. The explanations from this baseline are\nwell-formed sentences. However, the answers and explanations most likely do not match\nthe question or the image. For the random-words and random-explanations baselines,\nwe report the NLG metrics for all samples in the test set (instead of only considering the\ncorrectly answered samples, since the random sampling of the answer does not inﬂuence\nthe explanation). The Pointing and Justiﬁcation model PJ-X [26] provides text-based\npost-hoc justiﬁcations for the VQA task. It combines a modiﬁed MCB [16] framework,\npre-trained on the VQA v2 dataset, with a visual pointing and textual justiﬁcation\nmodule. The Faithful Multimodal (FM) model [53] aims at grounding parts of generated\nexplanations in the input image to provide explanations that are faithful to the input image.\nIt is based on the Up-Down VQA model [4]. In addition, FM contains an explanation\nmodule which enforces consistency between the predicted answer, explanation and the\nattention of the VQA model. The implementations for the PJ-X and FM models are\nbased on those provided by the authors of [29].\nImplementation and training details. We extracted 14×14×1024 grid features for the\nimages in the CLEVR-X dataset using a ResNet-101 [19], pre-trained on ImageNet [12].\n14\nL. Salewski et al.\nThese grid features served as inputs to the FM [53] and PJ-X [26] frameworks. The\nCLEVR-X explanations are lower case and punctuation is removed from the sentences.\nWe selected the best model on the CLEVR-X validation set based on the highest mean\nof the four NLG metrics, where explanations for incorrect answers were set to an empty\nstring. This metric accounts for the answering performance as well as for the explanation\nquality. The ﬁnal models were evaluated on the CLEVR-X test set. For PJ-X, our best\nmodel was trained for 52 epochs, using the Adam optimizer [33] with a learning rate of\n0.0002 and a batch size of 256. We did not use gradient clipping for PJ-X. Our strongest\nFM model was trained for 30 epochs, using the Adam optimizer with a learning rate of\n0.0002, a batch size of 128, and gradient clipping of 0.1. All other hyperparameters were\ntaken from [26,53].\nEvaluation metrics. To evaluate the quality of the generated explanations, we use the\nstandard natural language generation metrics BLEU [39], METEOR [9], ROUGE-L [36]\nand CIDEr [50]. By design, there is no correct explanation that can justify a wrong\nanswer. We follow [29] and report the quality of the generated explanations for the subset\nof correctly answered questions.\n4.2\nEvaluating explanations generated by state-of-the-art methods\nIn this section, we present quantitative results for generating explanations for the CLEVR-\nX dataset (Table 3). The random words baseline exhibits weak explanation performance\nfor all NLG metrics on CLEVR-X. Additionally, the random answering accuracy is\nvery low at 3.6%. The results are similar on VQA-X and e-SNLI-VE. The random\nexplanations baseline achieves stronger explanation results on all three datasets, but is\nstill signiﬁcantly worse than the trained models. This conﬁrms that, even with a medium-\nsized answer space (28 options) and a small vocabulary (96 words), it is not possible to\nachieve good scores on our dataset using a trivial approach.\nWe observed that the PJ-X model yields a signiﬁcantly stronger performance on\nCLEVR-X in terms of the NLG metrics for the generated explanations compared to\nthe FM model, with METEOR scores of 58.9 and 52.5 for PJ-X and FM respectively.\nAcross all explanation metrics, the scores on the VQA-X and e-SNLI-VE datasets are\nin a lower range than those on CLEVR-X. For PJ-X, we obtain a CIDEr score of 639.8\non CLEVR-X and 82.7 and 72.5 on VQA-X and e-SNLI-VE. This can be attributed to\nthe smaller vocabulary and longer sentences, which allow n-gram based metrics (e.g.\nBLEU) to match parts of sentences more easily.\nIn contrast to the explanation generation performance, the FM model is better at\nanswering questions than PJ-X on CLEVR-X with an answering accuracy of 80.3% for\nFM compared to 63.0% for PJ-X. Compared to recent models tuned to the CLEVR task,\nthe answering performances of PJ-X and FM do not seem very strong. However, the PJ-X\nbackbone MCB [16] (which is crucial for the answering performance) preceded the pub-\nlication of the CLEVR dataset. A version of the MCB backbone (CNN+LSTM+MCB in\nthe CLEVR publication [27]) achieved an answering accuracy of 51.4% on CLEVR [27],\nwhereas PJ-X is able to correctly answer 63% of the questions. The strongest model\ndiscussed in the initial CLEVR publication (CNN+LSTM+SA in [27]) achieved an\nanswering accuracy of 68.5%.\nCLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\n15\nTable 3: Explanation generation results on the CLEVR-X, VQA-X, and e-SNLI-VE\ntest sets using BLEU-4 (B4), METEOR (M), ROUGE-L (RL), CIDEr (C), and answer\naccuracy (Acc). Higher is better for all reported metrics. For the random baselines, Acc\ncorresponds to 100/# answers for CLEVR-X and e-SNLI-VE, and to the VQA answer score\nfor VQA-X. (Rnd. words: random words, Rnd. expl: Random explanations)\nModel\nCLEVR-X\nVQA-X\ne-SNLI-VE\nB4\nM\nRL\nC\nAcc\nB4\nM\nRL\nC\nAcc B4\nM\nRL\nC\nAcc\nRnd. words 0.0\n8.4 11.4\n5.9\n3.6\n0.0\n1.2\n0.7\n0.1\n0.1 0.0 0.3\n0.0\n0.0 33.3\nRnd. expl\n10.9 16.6 35.3 30.4\n3.6\n0.9\n6.5 18.4 21.6 0.2 0.4 5.4\n9.9\n2.6 33.3\nFM [53]\n78.8 52.5 85.8 566.8 80.3 23.1 20.4 47.1 87.0 75.5 8.2 15.6 29.9 83.6 58.5\nPJ-X [26]\n87.4 58.9 93.4 639.8 63.0 22.7 19.7 46.0 82.7 76.4 7.3 14.7 28.6 72.5 69.2\n4.3\nAnalyzing results on CLEVR-X by question and answer types\nIn Fig. 6 (left and middle), we present the performance for PJ-X on CLEVR-X for the 9\nquestion and 3 answer types. The explanation results for samples which require counting\nabilities (counting answers) are lower than those for attribute answers (57.3 vs. 63.3).\nThis is in line with prior ﬁndings that VQA models struggle with counting problems [48].\nThe explanation quality for binary questions is even lower with a METEOR score of only\n55.6. The generated explanations are of higher quality for easier question types; zero-hop\nquestions yield a METEOR score of 64.9 compared to 62.1 for three-hop questions. It\ncan also be seen that single-or questions are harder to explain than single-and questions.\nThese trends can be observed across all NLG explanation metrics.\n4.4\nInﬂuence of using different numbers of ground-truth explanations\nIn this section, we study the inﬂuence of using multiple ground-truth explanations for\nevaluation on the behavior of the NLG metrics. This gives insights about whether the\nmetrics can correctly rate a model’s performance with a limited number of ground-\ntruth explanations. We set an upper bound k on the number of explanations used and\nrandomly sample k explanations if a test sample has more than k explanations for\nk ∈{1, 2, . . . , 10}. Figure 6 (right) shows the NLG metrics (normalized with the\nmaximum value for each metric on the test set for all ground-truth explanations) for the\nPJ-X model depending on the average number of ground-truth references used on the\ntest set.\nOut of the four metrics, BLEU-4 converges the slowest, requiring close to 3 ground-\ntruth explanations to obtain a relative metric value of 95%. Hence, BLEU-4 might not\nbe able to reliably predict the explanation quality on the e-SNLI-VE dataset which has\nonly one explanation for each test sample. CIDEr converges faster than ROUGE and\nMETEOR, and achieves 95.7% of its ﬁnal value with only one ground-truth explanation.\nThis could be caused by the fact, that CIDEr utilizes a tf-idf weighting scheme for\ndifferent words, which is built from all reference sentences in the subset that the metric is\n16\nL. Salewski et al.\nzero hop\none hop\ntwo hop\nthree hop\nsame relate\ncompare integer\ncomparison\nsingle and\nsingle or\nQuestion type\n0\n10\n20\n30\n40\n50\n60\nMETEOR\nattribute\nbinary\ncounting\nAnswer type\noverall\n1\n2\n3\n4\nAverage number of\n ground-truth explanations\n80\n85\n90\n95\n100\nRelative metric value [%]\nMetric\nMETEOR\nROUGE-L\nCIDEr\nBLEU-4\nFig. 6: Explanation generation results for PJ-X on the CLEVR-X test set according to\nquestion (left) and answer (middle) types compared to the overall explanation quality.\nEasier types yield higher METEOR scores. NLG metrics using different numbers of\nground-truth explanations on the CLEVR-X test set (right). CIDEr converges faster than\nthe other NLG metrics.\ncomputed on. This allows CIDEr to be more sensitive to important words (e.g. attributes\nand shapes) and to give less weight, for instance, to stopwords, such as “the”. The VQA-\nX and e-SNLI-VE datasets contain much lower average numbers of explanations for\neach dataset sample (1.4 and 1.0). Since there could be many more possible explanations\nfor samples in those datasets that describe different aspects than those mentioned in the\nground truth, automated metric may not be able to correctly judge a prediction even if it\nis correct and faithful w.r.t. to the image and question.\n4.5\nQualitative explanation generation results\nWe show examples for explanations generated with the PJ-X framework on CLEVR-X in\nFig. 7. As can be seen across the three examples presented, PJ-X generates high-quality\nexplanations which closely match the ground-truth explanations.\nIn the left-most example in Fig. 7, we can observe slight variations in grammar\nwhen comparing the generated explanation to the ground-truth explanation. However,\nthe content of the generated explanation corresponds to the ground truth. Furthermore,\nsome predicted explanations differ from the ground-truth explanation in the use of\nanother synonym for a predicted attribute. For instance, in the middle example in Fig. 7,\nthe ground-truth explanation describes the size of the cylinder as “small”, whereas the\npredicted explanation uses the equivalent attribute “tiny”. In contrast to other datasets, the\nset of ground-truth explanations for each sample in CLEVR-X contains these variations.\nTherefore, the automated NLG metrics do not decrease when such variations are found\nin the predictions. For the ﬁrst and second example, PJ-X obtains the highest possible\nexplanation score (100.0) in terms of the BLEU-4, METEOR, and ROUGE-L metrics.\nWe show a failure case where PJ-X predicted the wrong answer in Fig. 7 (right). The\ngenerated answer-explanation pair shows that the predicted explanation is consistent\nCLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\n17\nQuestion: How many tiny red\nthings are the same material as\nthe big sphere?\nGT Answer | Explanation:\n1 | The tiny red metal block\nhas the same material as a big\nsphere.\nPred. Answer | Expl.\n1 | There is the tiny red metal\nblock which has the identical\nmaterial as a big sphere.\nB4 / M / RL / C:\n100.0 / 100.0 / 100.0 / 744.0\nQuestion: The cylinder has\nwhat size?\nGT Answer | Explanation:\nSmall | The cylinder is small.\nPred. Answer | Expl.\nSmall | The cylinder is tiny.\nB4 / M / RL / C:\n100.0 / 100.0 / 100.0 / 462.4\nQuestion: Are there any small\nmatte cubes?\nGT Answer | Explanation:\nNo | There are no small matte\ncubes.\nPred. Answer | Expl.\nYes | There is a small matte\ncube.\nB4 / M / RL / C:\n0.0 / 76.9 / 57.1 / 157.1\nFig. 7: Examples for answers and explanations generated with the PJ-X framework on\nthe CLEVR-X dataset, showing correct answer predictions (left, middle) and a failure\ncase (right). The NLG metrics obtained with the explanations for the correctly predicted\nanswers are high compared to those for the explanation corresponding to the wrong\nanswer prediction.\nwith the wrong answer prediction and does not match the input question-image pair. The\nNLG metrics for this case are signiﬁcantly weaker with a BLEU-4 score of 0.0, as there\nare no matching 4-grams between the prediction and the ground truth.\n5\nConclusion\nWe introduced the novel CLEVR-X dataset which contains natural language explanations\nfor the VQA task on the CLEVR dataset. Our user study conﬁrms that the explanations\nin the CLEVR-X dataset are complete and match the questions and images. Furthermore,\nwe have provided baseline performances using the PJ-X and FM frameworks on the\nCLEVR-X dataset. The structured nature of our proposed dataset allowed the detailed\nevaluation of the explanation generation quality according to answer and question types.\nWe observed that the generated explanations were of higher quality for easier answer\nand question categories. One of our ﬁndings is, that explanations for counting problems\nare worse than for other answer types, suggesting that further research into this direction\nis needed. Additionally, we ﬁnd that the four NLG metrics used to evaluate the quality\nof the generated explanations exhibit different convergence patterns depending on the\nnumber of available ground-truth references.\nSince this work only considered two natural language generation methods for VQA\nas baselines, the natural next step will be the benchmarking and closer investigation\n18\nL. Salewski et al.\nof additional recent frameworks for textual explanations in the context of VQA on the\nCLEVR-X dataset. We hope that our proposed CLEVR-X benchmark will facilitate\nfurther research to improve the generation of natural language explanations in the context\nof vision-language tasks.\n6\nAcknowledgements\nThe authors thank the Amazon Mechanical Turk workers that participated in the user\nstudy. This work was supported by the DFG – EXC number 2064/1 – project number\n390727645, by the DFG: SFB 1233, Robust Vision: Inference Principles and Neural\nMechanisms - project number: 276693517, by the ERC (853489 - DEXIM), and by the\nBMBF (FKZ: 01IS18039A). The authors thank the International Max Planck Research\nSchool for Intelligent Systems (IMPRS-IS) for supporting Leonard Salewski.\nReferences\n1. Agrawal, A., Batra, D., Parikh, D.: Analyzing the behavior of visual question answering\nmodels. In: EMNLP. pp. 1955–1960. Association for Computational Linguistics (2016) 3\n2. Agrawal, A., Batra, D., Parikh, D., Kembhavi, A.: Don’t just assume; look and answer:\nOvercoming priors for visual question answering. In: CVPR. pp. 4971–4980 (2018) 3\n3. von Ahn, L., Blum, M., Hopper, N., Langford, J.: Captcha: Using hard ai problems for security.\nIn: EUROCRYPT (2003) 10\n4. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up\nand top-down attention for image captioning and visual question answering. In: CVPR. pp.\n6077–6086 (2018) 13\n5. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh, D.: Vqa:\nVisual question answering. In: ICCV. pp. 2425–2433 (2015) 3\n6. Arras, L., Osman, A., Samek, W.: Clevr-xai: A benchmark dataset for the ground truth\nevaluation of neural network explanations. Information Fusion 81, 14–40 (2022) 4\n7. Arrieta, A.B., D´ıaz-Rodr´ıguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garc´ıa,\nS., Gil-L´opez, S., Molina, D., Benjamins, R., et al.: Explainable artiﬁcial intelligence (xai):\nConcepts, taxonomies, opportunities and challenges toward responsible ai. Information Fusion\n58, 82–115 (2020) 4\n8. Bach, S., Binder, A., Montavon, G., Klauschen, F., M¨uller, K.R., Samek, W.: On pixel-wise\nexplanations for non-linear classiﬁer decisions by layer-wise relevance propagation. PloS one\n10(7), e0130140 (2015) 3\n9. Banerjee, S., Lavie, A.: METEOR: An automatic metric for MT evaluation with improved\ncorrelation with human judgments. In: ACL Workshop. pp. 65–72 (2005) 14\n10. Brundage, M., Avin, S., Wang, J., Belﬁeld, H., Krueger, G., Hadﬁeld, G., Khlaaf, H., Yang, J.,\nToner, H., Fong, R., et al.: Toward trustworthy ai development: mechanisms for supporting\nveriﬁable claims. arXiv preprint arXiv:2004.07213 (2020) 4\n11. Camburu, O.M., Rockt¨aschel, T., Lukasiewicz, T., Blunsom, P.: e-snli: Natural language\ninference with natural language explanations. In: NeurIPS (2018) 4\n12. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical\nimage database. In: CVPR. pp. 248–255 (2009) 13\n13. Do, V., Camburu, O.M., Akata, Z., Lukasiewicz, T.: e-snli-ve-2.0: Corrected visual-textual\nentailment with natural language explanations. arXiv preprint arXiv:2004.03744 (2020) 2, 4,\n13\nCLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\n19\n14. Fong, R.C., Patrick, M., Vedaldi, A.: Understanding deep networks via extremal perturbations\nand smooth masks. In: ICCV. pp. 2950–2958 (2019) 3\n15. Fong, R.C., Vedaldi, A.: Interpretable explanations of black boxes by meaningful perturbation.\nIn: ICCV. pp. 3429–3437 (2017) 3\n16. Fukui, A., Park, D.H., Yang, D., Rohrbach, A., Darrell, T., Rohrbach, M.: Multimodal compact\nbilinear pooling for visual question answering and visual grounding. In: EMNLP. pp. 457–468\n(2016) 3, 13, 14\n17. Gilpin, L.H., Bau, D., Yuan, B.Z., Bajwa, A., Specter, M., Kagal, L.: Explaining explanations:\nAn overview of interpretability of machine learning. In: IEEE DSAA. pp. 80–89 (2018) 4\n18. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in VQA Matter:\nElevating the Role of Image Understanding in Visual Question Answering. In: CVPR. pp.\n6904–6913 (2017) 4\n19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR.\npp. 770–778 (2016) 13\n20. Hendricks, L.A., Hu, R., Darrell, T., Akata, Z.: Generating counterfactual explanations with\nnatural language. arXiv preprint arXiv:1806.09809 (2018) 4\n21. Hendricks, L.A., Hu, R., Darrell, T., Akata, Z.: Grounding visual explanations. In: ECCV. pp.\n264–279 (2018) 4\n22. Holzinger, A., Saranti, A., Mueller, H.: Kandinskypatterns - an experimental exploration\nenvironment for pattern analysis and machine intelligence. arXiv preprint arXiv:2103.00519\n(2021) 4\n23. Hudson, D., Manning, C.D.: Learning by abstraction: The neural state machine. In: NeurIPS\n(2019) 4\n24. Hudson, D.A., Manning, C.D.: Compositional attention networks for machine reasoning. In:\nICLR (2018) 4\n25. Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In: CVPR. pp. 6693–6702 (2019) 3\n26. Huk Park, D., Anne Hendricks, L., Akata, Z., Rohrbach, A., Schiele, B., Darrell, T., Rohrbach,\nM.: Multimodal explanations: Justifying decisions and pointing to the evidence. In: CVPR.\npp. 8779–8788 (2018) 2, 4, 13, 14, 15\n27. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C.L., Girshick, R.B.:\nClevr: A diagnostic dataset for compositional language and elementary visual reasoning. In:\nCVPR. pp. 2901–2910 (2017) 2, 3, 4, 14\n28. Johnson, J., Hariharan, B., Van Der Maaten, L., Hoffman, J., Fei-Fei, L., Lawrence Zitnick, C.,\nGirshick, R.: Inferring and executing programs for visual reasoning. In: ICCV. pp. 2989–2998\n(2017) 4\n29. Kayser, M., Camburu, O.M., Salewski, L., Emde, C., Do, V., Akata, Z., Lukasiewicz, T.: e-vil:\nA dataset and benchmark for natural language explanations in vision-language tasks. In: ICCV.\npp. 1244–1254 (2021) 2, 4, 13, 14\n30. Kim, J.M., Choe, J., Akata, Z., Oh, S.J.: Keep calm and improve visual feature attribution. In:\nICCV. pp. 8350–8360 (2021) 3\n31. Kim, J., Rohrbach, A., Darrell, T., Canny, J., Akata, Z.: Textual explanations for self-driving\nvehicles. In: ECCV. pp. 563–578 (2018) 4\n32. Kim, S.S., Meister, N., Ramaswamy, V.V., Fong, R., Russakovsky, O.: Hive: Evaluating the\nhuman interpretability of visual explanations. arXiv preprint arXiv:2112.03184 (2021) 4\n33. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR (2015) 14\n34. Kottur, S., Moura, J.M., Parikh, D., Batra, D., Rohrbach, M.: CLEVR-dialog: A diagnostic\ndataset for multi-round reasoning in visual dialog. In: NAACL. pp. 582–595 (2019) 4\n35. Li, Q., Tao, Q., Joty, S., Cai, J., Luo, J.: Vqa-e: Explaining, elaborating, and enhancing your\nanswers for visual questions. In: ECCV. pp. 552–567 (2018) 4\n20\nL. Salewski et al.\n36. Lin, C.Y.: ROUGE: A package for automatic evaluation of summaries. In: ACL. pp. 74–81\n(2004) 14\n37. Liu, R., Liu, C., Bai, Y., Yuille, A.L.: Clevr-ref+: Diagnosing visual reasoning with referring\nexpressions. In: CVPR. pp. 4185–4194 (2019) 4\n38. Marasovi´c, A., Bhagavatula, C., Park, J.s., Le Bras, R., Smith, N.A., Choi, Y.: Natural language\nrationales with full-stack visual reasoning: From pixels to semantic frames to commonsense\ngraphs. In: EMNLP. pp. 2810–2829 (2020) 4\n39. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of\nmachine translation. In: ACL. pp. 311–318 (2002) 14\n40. Patro, B., Patel, S., Namboodiri, V.: Robust explanations for visual question answering. In:\nWACV. pp. 1577–1586 (2020) 4\n41. Perez, E., Strub, F., De Vries, H., Dumoulin, V., Courville, A.: FiLM: Visual Reasoning with\na General Conditioning Layer. In: AAAI. vol. 32 (2018) 4\n42. Petsiuk, V., Das, A., Saenko, K.: Rise: Randomized input sampling for explanation of black-\nbox models. In: BMVC. p. 151 (2018) 3\n43. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-cam: Visual\nexplanations from deep networks via gradient-based localization. In: ICCV. pp. 618–626\n(2017) 3\n44. Shi, J., Zhang, H., Li, J.: Explainable and Explicit Visual Reasoning Over Scene Graphs. In:\nCVPR. pp. 8376–8384 (2019) 4\n45. Shih, K.J., Singh, S., Hoiem, D.: Where to look: Focus regions for visual question answering.\nIn: CVPR. pp. 4613–4621 (2016) 3\n46. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising\nimage classiﬁcation models and saliency maps. In: ICLR Workshop (2014) 3\n47. Suarez, J., Johnson, J., Li, F.F.: Ddrprog: A clevr differentiable dynamic reasoning programmer.\narXiv preprint arXiv:1803.11361 (2018) 4\n48. Trott, A., Xiong, C., Socher, R.: Interpretable counting for visual question answering. In:\nICLR (2018) 3, 15\n49. Vedantam, R., Szlam, A., Nickel, M., Morcos, A., Lake, B.M.: CURI: A benchmark for\nproductive concept learning under uncertainty. In: ICML. pp. 10519–10529 (2021) 4\n50. Vedantam, R., Zitnick, C.L., Parikh, D.: Cider: Consensus-based image description evaluation.\nIn: CVPR. pp. 4566–4575 (2015) 14\n51. de Vries, H., Bahdanau, D., Murty, S., Courville, A.C., Beaudoin, P.: CLOSURE: assessing\nsystematic generalization of CLEVR models. In: NeurIPS Workshop (2019) 4\n52. Wu, J., Chen, L., Mooney, R.: Improving vqa and its explanations by comparing competing\nexplanations. In: AAAI Workshop (2021) 4\n53. Wu, J., Mooney, R.: Faithful Multimodal Explanation for Visual Question Answering. In:\nACL Workshop. pp. 103–112 (2019) 4, 13, 14, 15\n54. Xie, N., Lai, F., Doran, D., Kadav, A.: Visual entailment: A novel task for ﬁne-grained image\nunderstanding. arXiv preprint arXiv:1901.06706 (2019) 4\n55. Xu, H., Saenko, K.: Ask, attend and answer: Exploring question-guided spatial attention for\nvisual question answering. In: ECCV. pp. 451–466 (2016) 3\n56. Yang, Z., He, X., Gao, J., Deng, L., Smola, A.: Stacked attention networks for image question\nanswering. In: CVPR. pp. 21–29 (2016) 3\n57. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: ECCV.\npp. 818–833 (2014) 3\n58. Zhang, J., Bargal, S.A., Lin, Z., Brandt, J., Shen, X., Sclaroff, S.: Top-down neural attention\nby excitation backprop. International Journal of Computer Vision 126(10), 1084–1102 (2018)\n3\n59. Zhang, P., Goyal, Y., Summers-Stay, D., Batra, D., Parikh, D.: Yin and yang: Balancing and\nanswering binary visual questions. In: CVPR. pp. 5014–5022 (2016) 3\nCLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\n21\n60. Zhu, Y., Groth, O., Bernstein, M., Fei-Fei, L.: Visual7w: Grounded question answering in\nimages. In: CVPR. pp. 4995–5004 (2016) 3\n",
  "categories": [
    "cs.CV",
    "cs.CL"
  ],
  "published": "2022-04-05",
  "updated": "2022-04-05"
}