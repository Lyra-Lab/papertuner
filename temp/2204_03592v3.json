{
  "id": "http://arxiv.org/abs/2204.03592v3",
  "title": "Testing the limits of natural language models for predicting human language judgments",
  "authors": [
    "Tal Golan",
    "Matthew Siegelman",
    "Nikolaus Kriegeskorte",
    "Christopher Baldassano"
  ],
  "abstract": "Neural network language models can serve as computational hypotheses about\nhow humans process language. We compared the model-human consistency of diverse\nlanguage models using a novel experimental approach: controversial sentence\npairs. For each controversial sentence pair, two language models disagree about\nwhich sentence is more likely to occur in natural text. Considering nine\nlanguage models (including n-gram, recurrent neural networks, and transformer\nmodels), we created hundreds of such controversial sentence pairs by either\nselecting sentences from a corpus or synthetically optimizing sentence pairs to\nbe highly controversial. Human subjects then provided judgments indicating for\neach pair which of the two sentences is more likely. Controversial sentence\npairs proved highly effective at revealing model failures and identifying\nmodels that aligned most closely with human judgments. The most\nhuman-consistent model tested was GPT-2, although experiments also revealed\nsignificant shortcomings of its alignment with human perception.",
  "text": "Testing the limits of natural language models for\npredicting human language judgments\nTal Golan1,2∗†, Matthew Siegelman3∗,\nNikolaus Kriegeskorte1,3,4,5, Christopher Baldassano3\n1Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, USA\n2Department of Cognitive and Brain Sciences, Ben-Gurion University of the Negev, Be’er-Sheva, Israel\n3Department of Psychology, Columbia University, New York, NY, USA\n4Department of Neuroscience, Columbia University, New York, NY, USA\n5Department of Electrical Engineering, Columbia University, New York, NY, USA\n∗The first two authors contributed equally to this work.\n†To whom correspondence should be addressed; E-mail: golan.neuro@bgu.ac.il\nNeural network language models appear to be increasingly aligned with how hu-\nmans process and generate language, but identifying their weaknesses through ad-\nversarial examples is challenging due to the discrete nature of language and the\ncomplexity of human language perception. We bypass these limitations by turning\nthe models against each other. We generate controversial sentence pairs for which\ntwo language models disagree about which sentence is more likely to occur. Con-\nsidering nine language models (including n-gram, recurrent neural networks, and\ntransformers), we created hundreds of controversial sentence pairs through syn-\nthetic optimization or by selecting sentences from a corpus. Controversial sentence\npairs proved highly effective at revealing model failures and identifying models that\naligned most closely with human judgments of which sentence is more likely. The\nmost human-consistent model tested was GPT-2, although experiments also revealed\nsignificant shortcomings of its alignment with human perception.\nKeywords— Language Models, Human Acceptability Judgments, Controversial Stimuli, Adversarial Attacks\nin NLP\n1\nIntroduction\nNeural network language models are not only key tools in natural language processing (NLP) but are also draw-\ning an increasing scientific interest as potential models of human language-processing. Ranging from recurrent\nneural networks [1, 2] to transformers [3–7], each of these language models (explicitly or implicitly) defines\na probability distribution over strings of words, predicting which sequences are likely to occur in natural lan-\nguage. There is substantial evidence from measures such as reading times [8], functional MRI [9], scalp EEG\n1\narXiv:2204.03592v3  [cs.CL]  12 Sep 2023\n[10], and intracranial ECoG [11] that humans are sensitive to the relative probabilities of words and sentences as\ncaptured by language models, even among sentences that are grammatically correct and semantically meaning-\nful. Furthermore, model-derived sentence probabilities can also predict human graded acceptability judgments\n[12, 13]. These successes, however, have not yet addressed two central questions of interest: (1) Which of the\nmodels is best-aligned with human language processing? (2) How close is the best-aligned model to the goal\nof fully capturing human judgments?\nA predominant approach for evaluating language models is to use a set of standardized benchmarks such\nas those in the General Language Understanding Evaluation (GLUE) [14], or its successor, SuperGLUE [15].\nThough instrumental in evaluating the utility of language models for downstream NLP tasks, these benchmarks\nprove insufficient for comparing such models as candidate explanations of human language-processing. Many\ncomponents of these benchmarks do not aim to measure human alignment but rather the usefulness of the mod-\nels’ language representation when tuned to a specific downstream task. Some benchmarks challenge language\nmodels more directly by comparing the probabilities they assign to grammatical and ungrammatical sentences\n(e.g., BLiMP [16]). However, since such benchmarks are driven by theoretical linguistic considerations, they\nmight fail to detect novel, unexpected ways in which language models may diverge from human language un-\nderstanding. Lastly, an additional practical concern is that the rapid pace of NLP research has led to quick\nsaturation of these kinds of static benchmarks, making it difficult to distinguish between models [17].\nOne proposed solution to these issues is the use of dynamic human-in-the-loop benchmarks in which peo-\nple actively stress-test models with an evolving set of tests. However, this approach faces the major obstacle\nthat “finding interesting examples is rapidly becoming a less trivial task” [17]. We propose to complement\nhuman-curated benchmarks with model-driven evaluation. Guided by model predictions rather than experi-\nmenter intuitions, we would like to identify particularly informative test sentences, where different models\nmake divergent predictions. This approach of running experiments mathematically optimized to “put in jeop-\nardy” particular models belongs to a long-standing scientific philosophy of design optimization [18]. We can\nfind these critical sentences in large corpora of natural language or synthesize novel test sentences that reveal\nhow different models generalize beyond their training distributions.\nWe propose here a systematic, model-driven approach for comparing language models in terms of their\nconsistency with human judgments. We generate controversial sentence pairs: pairs of sentences designed\nsuch that two language models strongly disagree about which sentence is more likely to occur. In each of these\nsentence pairs, one model assigns a higher probability to the first sentence than the second sentence, while the\nother model prefers the second sentence to the first. We then collect human judgments of which sentence in\neach pair is more probable to settle this dispute between the two models.\nThis approach builds on previous work on controversial images for models of visual classification [19].\nThat work relied on absolute judgments of a single stimulus, which are appropriate for classification responses.\nHowever, asking the participants to rate each sentence’s probability on an absolute scale is complicated by\nbetween-trial context effects common in magnitude estimation tasks [20–22], which have been shown to impact\njudgments like acceptability [23]. A binary forced-choice behavioral task presenting the participants with a\nchoice between two sentences in each trial, the approach we used here, minimizes the role of between-trial\ncontext effects by setting an explicit local context within each trial. Such an approach has been previously\nused for measuring sentence acceptability [24] and provides substantially more statistical power compared to\ndesigns in which acceptability ratings are provided for single sentences [25].\nOur experiments demonstrate that (1) it is possible to procedurally generate controversial sentence pairs for\nall common classes of language models, either by selecting pairs of sentences from a corpus or by iteratively\nmodifying natural sentences to yield controversial predictions; (2) the resulting controversial sentence pairs\nenable efficient model comparison between models that otherwise are seemingly equivalent in their human\n2\nconsistency; and (3) all current NLP model classes incorrectly assign high probability to some non-natural\nsentences (one can modify a natural sentence such that its model probability does not decrease but human\nobservers reject the sentence as unnatural). This framework for model comparison and model testing can give\nus new insight into the classes of models that best align with human language perception and suggest directions\nfor future model development.\n2\nResults\nWe acquired judgments from 100 native English speakers tested online. In each experimental trial, the partic-\nipants were asked to judge which of two sentences they would be “more likely to encounter in the world, as\neither speech or written text”, and provided a rating of their confidence in their answer on a 3-point scale (see\nExtended Data Fig. 1 for a trial example). The experiment was designed to compare nine different language\nmodels (Supplementary Section 6.1): probability models based on corpus frequencies of 2-word and 3-word\nsequences (2-grams and 3-grams) and a range of neural network models comprising a recurrent neural network\n(RNN), a long short-term memory network (LSTM), and five transformer models (BERT, RoBERTa, XLM,\nELECTRA, and GPT-2).\n2.1\nEfficient model comparison using natural controversial pairs\nAs a baseline, we randomly sampled and paired 8-word sentences from a corpus of Reddit comments. How-\never, as shown in Fig. 1a, these sentences fail to uncover meaningful differences between the models. For\neach sentence pair, all models tend to prefer the same sentence (Extended Data Fig. 2), and therefore perform\nsimilarly in predicting human preference ratings (see Supplementary Section 7.1).\nInstead, we can use an optimization procedure (Supplementary Section 6.2) to search for controversial\nsentence pairs, in which one language model assigns a high probability (above the median probability for\nnatural sentences) only to sentence 1 and a second language model assigns a high probability only to sentence\n2; see examples in Table 1. Measuring each model’s accuracy in predicting human choices for sentence pairs\nin which it was one of the two targeted models indicated many significant differences in terms of model-\nhuman alignment (Fig. 1b), with GPT-2 and RoBERTa showing the best human consistency and 2-gram the\nworst. We can also compare each model pair separately (using only the stimuli targeting that model pair),\nyielding a similar pattern of pairwise dominance (Extended Data Fig. 3a). All models except GPT-2, RoBERTa,\nand ELECTRA performed significantly below our lower bound on the noise ceiling (the accuracy obtained\nby predicting each participant’s responses from the other participants’ responses), indicating a misalignment\nbetween these models’ predictions and human judgments which was only revealed when using controversial\nsentence pairs.\n3\n0\n25%\n50%\n75%\n100%\nhuman-choice prediction accuracy\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\n0\n50\n100\nGPT-2\np(sentence) percentile\n0\n20\n40\n60\n80\n100\nRoBERTa\np(sentence) percentile\n0\n25%\n50%\n75%\n100%\nhuman-choice prediction accuracy\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\n0\n50\n100\nGPT-2\np(sentence) percentile\n0\n20\n40\n60\n80\n100\nRoBERTa\np(sentence) percentile\na Randomly sampled natural-sentence pairs\nb Controversial natural-sentence pairs\nFigure 1: Model comparison using natural sentences. (a) (Left) Percentile-transformed sentence probabili-\nties for GPT-2 and RoBERTa (defined relative to all sentences used in the experiment) for randomly-sampled\npairs of natural sentences. Each pair of connected dots depicts one sentence pair. The two models are highly\ncongruent in their rankings of sentences within a pair (lines have upward slope). (Right) Accuracy of model\npredictions of human choices, measured as the proportion of trials in which the same sentence was preferred\nby both the model and the human participant. Each dot depicts the prediction accuracy of one candidate model\naveraged across a group of 10 participants presented with a unique set of trials. The colored bars depict grand-\naverages across all 100 participants. The gray bar is the noise ceiling whose left and right edges are lower\nand upper bounds on the grand-average performance an ideal model would achieve (based on the consistency\nacross human subjects). There were no significant differences in model performance on the randomly sam-\npled natural sentences. (b) (Left) Controversial natural-sentence pairs were selected such that the models’\nsentence probability ranks were incongruent (lines have downward slope). (Right) Controversial sentence pairs\nenable efficient model comparison, revealing that BERT, XLM, LSTM, RNN and the n-gram models perform\nsignificantly below the noise ceiling (asterisks indicate significance—two-sided Wilcoxon signed-rank test,\ncontrolling the false discovery rate for nine comparisons at q < .05). On the right of the plot, each closed circle\nindicates a model significantly dominating alternative models indicated by open circles (two-sided Wilcoxon\nsigned-rank test, controlling the false discovery rate for all 36 model pairs at q < .05). GPT-2 outperforms all\nmodels except RoBERTA at predicting human judgments.\n4\nsentence\nlog probability (model 1)\nlog probability (model 2)\n# human choices\nn1: Rust is generally caused by salt and sand.\nlog p(n1|GPT-2) =−50.72\nlog p(n1|ELECTRA) =−38.54\n10\nn2: Where is Vernon Roche when you need him.\nlog p(n2|GPT-2) =−32.26\nlog p(n2|ELECTRA) =−58.26\n0\nn1: Excellent draw and an overall great smoking experience.\nlog p(n1|RoBERTa) =−67.78\nlog p(n1|GPT-2) =−36.76\n10\nn2: I should be higher and tied to inflation.\nlog p(n2|RoBERTa) =−54.61\nlog p(n2|GPT-2) =−50.31\n0\nn1: You may try and ask on their forum.\nlog p(n1|ELECTRA) =−51.44\nlog p(n1|LSTM) =−44.24\n10\nn2: I love how they look like octopus tentacles.\nlog p(n2|ELECTRA) =−35.51\nlog p(n2|LSTM) =−66.66\n0\nn1: Grow up and quit whining about minor inconveniences.\nlog p(n1|BERT) =−82.74\nlog p(n1|GPT-2) =−35.66\n10\nn2: The extra a is the correct Sanskrit pronunciation.\nlog p(n2|BERT) =−51.06\nlog p(n2|GPT-2) =−51.10\n0\nn1: I like my password manager for this reason.\nlog p(n1|XLM) =−68.93\nlog p(n1|RoBERTa) =−49.61\n10\nn2: Kind of like clan of the cave bear.\nlog p(n2|XLM) =−44.24\nlog p(n2|RoBERTa) =−67.00\n0\nn1: We have raised a Generation of Computer geeks.\nlog p(n1|LSTM) =−66.41\nlog p(n1|ELECTRA) =−36.57\n10\nn2: I mean when the refs are being sketchy.\nlog p(n2|LSTM) =−42.04\nlog p(n2|ELECTRA) =−52.28\n0\nn1: This is getting ridiculous and ruining the hobby.\nlog p(n1|RNN) =−100.65\nlog p(n1|LSTM) =−43.50\n10\nn2: I think the boys and invincible are better.\nlog p(n2|RNN) =−45.16\nlog p(n2|LSTM) =−59.00\n0\nn1: Then attach them with the supplied wood screws.\nlog p(n1|3-gram) =−119.09\nlog p(n1|GPT-2) =−34.84\n10\nn2: Sounds like you were used both a dog.\nlog p(n2|3-gram) =−92.07\nlog p(n2|GPT-2) =−52.84\n0\nn1: Cream cheese with ham and onions on crackers.\nlog p(n1|2-gram) =−131.99\nlog p(n1|RoBERTa) =−54.62\n10\nn2: I may have to parallel process that drinking.\nlog p(n2|2-gram) =−109.46\nlog p(n2|RoBERTa) =−70.69\n0\nTable 1: Examples of controversial natural-sentence pairs that maximally contributed to each model’s\nprediction error. For each model (double row, “model 1”), the table shows results for two sentences on which\nthe model failed severely. In each case, the failing model 1 prefers sentence n2 (higher log probability bolded),\nwhile the model it was pitted against (“model 2”) and all 10 human subjects presented with that sentence pair\nprefer sentence n1. (When more than one sentence pair induced an equal maximal error in a model, the example\nincluded in the table was chosen at random.)\n2.2\nGreater model disentanglement with synthetic sentence pairs\nSelecting controversial natural-sentence pairs may provide greater power than randomly sampling natural-\nsentence pairs, but this search procedure considers a very limited part of the space of possible sentence pairs.\nInstead, we can iteratively replace words in a natural sentence to drive different models to make opposing\npredictions, forming synthetic controversial sentences that may lay outside any natural language corpora, as il-\nlustrated in Fig. 2 (see Methods, “Generating synthetic controversial sentence pairs” for full details). Examples\nof controversial synthetic-sentence pairs that maximally contributed to the models’ prediction error appear in\nTable 2.\nWe evaluated how well each model predicted the human sentence choices in all of the controversial synthetic-\nsentence pairs in which the model was one of the two models targeted (Fig. 3a). This evaluation of model-\nhuman alignment resulted in an even greater separation between the models’ prediction accuracies than was\nobtained when using controversial natural-sentence pairs, pushing the weaker models (RNN, 3-gram, and 2-\ngram) far below the 50% chance accuracy level. GPT-2, RoBERTa, and ELECTRA were found to be signifi-\ncantly more accurate than the alternative models (BERT, XLM, LSTM, RNN, 3-gram, and 2-gram) in predicting\nthe human responses to these trials (with similar results when comparing model pair separately, see Extended\nData Fig. 3b). All of the models except for GPT-2 were found to be significantly below the lower bound on the\nnoise ceiling, demonstrating misalignment with human judgments.\n5\n0\n20\n40\n60\n80\n100\nGPT-2\np(sentence) percentile\n0\n20\n40\n60\n80\n100\nELECTRA\np(sentence) percentile\nNothing has\na world of\nexcitement\nand joys.\nDiddy has a wealth of experience\nwith grappling.\nLuke has a ton of experience\nwith winning.\na\n0\n20\n40\n60\n80\n100\nRoBERTa\np(sentence) percentile\n0\n20\n40\n60\n80\n100\n3-gram\np(sentence) percentile\nYou have to realize\nis that noise again.\nI wait to see how\nit shakes out.\nI need to see how\nthis played out.\nb\nFigure 2: Synthesizing controversial sentence pairs. The small open dots denote 500 randomly sampled\nnatural sentences. The big open dot denotes the natural sentence used for initializing the controversial sentence\noptimization, and the closed dots are the resulting synthetic sentences. (a) In this example, we start with the\nrandomly sampled natural sentence “Luke has a ton of experience with winning”. If we adjust this sentence to\nminimize its probability according to GPT-2 (while keeping the sentence at least as likely as the natural sentence\naccording to ELECTRA), we obtain the synthetic sentence “Nothing has a world of excitement and joys”. By\nrepeating this procedure while switching the roles of the models, we generate the synthetic sentence “Diddy\nhas a wealth of experience with grappling”, which decreases ELECTRA’s probability while slightly increasing\nGPT-2’s. (b) In this example, we start with the randomly sampled natural sentence “I need to see how this\nplayed out”. If we adjust this sentence to minimize its probability according to RoBERTa (while keeping the\nsentence at least as likely as the natural sentence according to 3-gram), we obtain the synthetic sentence “You\nhave to realize is that noise again”. If we instead decrease only 3-gram’s probability, we generate the synthetic\nsentence “I wait to see how it shakes out”.\n2.3\nPairs of natural and synthetic sentences uncover blindspots\nLast, we considered trials in which the participants were asked to choose between a natural sentence and one of\nthe synthetic sentences which was generated from that natural sentence. If the language model is fully aligned\nwith human judgments, we would expect humans to agree with the model, and select the synthetic sentence\nat least as much as the natural sentence. In reality, human participants showed a systematic preference for the\nnatural sentences over their synthetic counterparts (Fig. 3b), even when the synthetic sentences were formed\nsuch that the stronger models (i.e., GPT-2, RoBERTA, or ELECTRA) favored them over the natural sentences;\nsee Extended Data Table 1 for examples. Evaluating natural sentence preference separately for each model-\npairing (Extended Data Fig. 4), we find that these imperfections can be uncovered even when pairing a strong\nmodel with a relatively weak model (such that the strong model “accepts” the synthetic sentence and the weak\nmodel rejects it).\n6\n0\n25%\n50%\n75%\n100%\nhuman-choice prediction accuracy\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\n0\n50\n100\nGPT-2\np(sentence) percentile\n0\n20\n40\n60\n80\n100\nRoBERTa\np(sentence) percentile\n0\n25%\n50%\n75%\n100%\nhuman-choice prediction accuracy\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\n0\n50\n100\nGPT-2\np(sentence) percentile\n0\n20\n40\n60\n80\n100\nRoBERTa\np(sentence) percentile\na Synthetic controversial sentence pairs\nb Synthetic vs. natural sentences\nnatural sentence\nsynthetic sentence\nFigure 3: Model comparison using synthetic sentences. (a) (Left) Percentile-transformed sentence probabil-\nities for GPT-2 and RoBERTa for controversial synthetic-sentence pairs. Each pair of connected dots depict\none sentence pair. (Right) Model prediction accuracy, measured as the proportion of trials in which the same\nsentence was preferred by both the model and the human participant. GPT-2, RoBERTa and ELECTRA sig-\nnificantly outperformed the other models (two-sided Wilcoxon signed-rank test, controlling the false discovery\nrate for all 36 model comparisons at q < .05). All of the models except for GPT-2 were found to perform below\nthe noise ceiling (gray) of predicting each participant’s choices from the majority votes of the other participants\n(asterisks indicate significance—two-sided Wilcoxon signed-rank test, controlling the false discovery rate for\nnine comparisons at q < .05). (b) (Left) Each connected triplet of dots depicts a natural sentence and its derived\nsynthetic sentences, optimized to decrease the probability only under GPT-2 (left dots in a triplet) or only under\nRoBERTa (bottom dots in a triplet). (Right) Each model was evaluated across all of the synthetic-natural sen-\ntence pairs for which it was targeted to keep the synthetic sentence at least as probable as the natural sentence\n(see Extended Data Fig. 6 for the complementary data binning). This evaluation yielded a below-chance pre-\ndiction accuracy for all of the models, which was also significantly below the lower bound on the noise ceiling.\nThis indicates that, although the models assessed that these synthetic sentences were at least as probable as the\noriginal natural sentence, humans disagreed and showed a systematic preference for the natural sentence. See\nFig. 1’s caption for details on the visualization conventions used in this figure.\n7\nsentence\nlog probability (model 1)\nlog probability (model 2)\n# human choices\ns1: You can reach his stories on an instant.\nlog p(s1|GPT-2) =−64.92\nlog p(s1|RoBERTa) =−59.98\n10\ns2: Anybody can behead a rattles an an antelope.\nlog p(s2|GPT-2) =−40.45\nlog p(s2|RoBERTa) =−90.87\n0\ns1: However they will still compare you to others.\nlog p(s1|RoBERTa) =−53.40\nlog p(s1|GPT-2) =−31.59\n10\ns2: Why people who only give themselves to others.\nlog p(s2|RoBERTa) =−48.66\nlog p(s2|GPT-2) =−47.13\n0\ns1: He healed faster than any professional sports player.\nlog p(s1|ELECTRA) =−48.77\nlog p(s1|BERT) =−50.21\n10\ns2: One gets less than a single soccer team.\nlog p(s2|ELECTRA) =−38.25\nlog p(s2|BERT) =−59.09\n0\ns1: That is the narrative we have been sold.\nlog p(s1|BERT) =−56.14\nlog p(s1|GPT-2) =−26.31\n10\ns2: This is the week you have been dying.\nlog p(s2|BERT) =−50.66\nlog p(s2|GPT-2) =−39.50\n0\ns1: The resilience is made stronger by early adversity.\nlog p(s1|XLM) =−62.95\nlog p(s1|RoBERTa) =−54.34\n10\ns2: Every thing is made alive by infinite Ness.\nlog p(s2|XLM) =−42.95\nlog p(s2|RoBERTa) =−75.72\n0\ns1: President Trump threatens to storm the White House.\nlog p(s1|LSTM) =−58.78\nlog p(s1|RoBERTa) =−41.67\n10\ns2: West Surrey refused to form the White House.\nlog p(s2|LSTM) =−40.35\nlog p(s2|RoBERTa) =−67.32\n0\ns1: Las beans taste best with a mustard sauce.\nlog p(s1|RNN) =−131.62\nlog p(s1|RoBERTa) =−60.58\n10\ns2: Roughly lanes being alive in a statement ratings.\nlog p(s2|RNN) =−49.31\nlog p(s2|RoBERTa) =−99.90\n0\ns1: You are constantly seeing people play the multi.\nlog p(s1|3-gram) =−107.16\nlog p(s1|ELECTRA) =−44.79\n10\ns2: This will probably the happiest contradicts the hypocrite.\nlog p(s2|3-gram) =−91.59\nlog p(s2|ELECTRA) =−75.83\n0\ns1: A buyer can own a genuine product also.\nlog p(s1|2-gram) =−127.35\nlog p(s1|ELECTRA) =−40.21\n10\ns2: One versed in circumference of highschool I rambled.\nlog p(s2|2-gram) =−113.73\nlog p(s2|ELECTRA) =−92.61\n0\nTable 2: Examples of controversial synthetic-sentence pairs that maximally contributed to each model’s\nprediction error. For each model (double row, “model 1”), the table shows results for two sentences on which\nthe model failed severely. In each case, the failing model 1 prefers sentence s2 (higher log probability bolded),\nwhile the model it was pitted against (“model 2”) and all 10 human subjects presented with that sentence pair\nprefer sentence s1. (When more than one sentence pair induced an equal maximal error in a model, the example\nincluded in the table was chosen at random.)\n8\n1.0\n0.5\n0.0\n0.5\n1.0\nordinal correlation between human ratings and models'\nsentence pair probability log-ratio (signed-rank cosine similarity)\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\nFigure 4: Ordinal correlation of the models’ sentence probability log-ratios and human Likert ratings.\nFor each sentence pair, model prediction was quantified by log p(s1|m)\np(s2|m). This log-ratio was correlated with the\nLikert ratings of each particular participant, using signed-rank cosine similarity (see Methods). This analysis,\ntaking all trials and human confidence level into account, indicates that GPT-2 performed best in predicting\nhuman sentence probability judgments. However, its predictions are still significantly misaligned with the\nhuman choices. See Fig. 1’s caption for details on the visualization convention.\n2.4\nEvaluating the entire dataset reveals a hierarchy of models\nRather than evaluating each model’s prediction accuracy with respect to the particular sentence pairs that were\nformed to compare this model to alternative models, we can maximize our statistical power by computing the\naverage prediction accuracy for each model with respect to all of the experimental trials we collected. Fur-\nthermore, rather than binarizing the human and model judgments, here we measure the ordinal correspondence\nbetween the graded human choices (taking confidence into account) and the log ratio of the sentence proba-\nbilities assigned by each candidate model. Using this more sensitive benchmark (Fig. 4), we found GPT-2 to\nbe the most human-aligned, followed by RoBERTa; then ELECTRA; BERT; XLM and LSTM; and the RNN,\n3-gram, and 2-gram models. However, all of the models (including GPT-2) were found to be significantly less\naccurate than the lower bound on the noise ceiling.\nOne possible reason for the poorer performance of the bidirectional transformers (RoBERTa, ELECTRA,\nBERT, and XLM) compared to the unidirectional transformer (GPT-2) is that computing sentence probabilities\nin these models is complex, and the probability estimator we developed (see Methods, “Evaluating sentence\nprobabilities in transformer models”) could be non-optimal; Indeed, the popular pseudo-log-likelihood (PLL)\napproach yields slightly higher accuracy for randomly sampled natural-sentence pairs (Extended Data Fig. 5a).\nAnd yet, when we directly compared our estimator to PLL by means of generating and administrating new\nsynthetic controversial sentences, our estimator was found to be markedly better aligned to human judgments\n(Extended Data Fig. 5b and Extended Data Table 2).\nFinally, a control analysis employing probability measures normalized by token count revealed that such\nnormalization had minimal influence on the observed differences among models (Supplementary Section 7.2\nand Supplementary Fig. S1).\n9\n3\nDiscussion\nIn this study, we probed the ability of language models to predict human relative sentence probability judgments\nusing controversial sentence pairs, selected or synthesized so that two models disagreed about which sentence\nwas more probable. We found that (1) GPT-2 (a unidirectional transformer model trained on predicting up-\ncoming tokens) and RoBERTa (a bidirectional transformer trained on a held-out token prediction task) were the\nmost predictive of human judgments on controversial natural-sentence pairs (Fig. 1b); (2) GPT-2, RoBERTa,\nand ELECTRA (a bidirectional transformer trained on detecting corrupted tokens) were the most predictive of\nhuman judgments on pairs of sentences synthesized to maximize controversiality (Fig. 3a); and (3) GPT-2 was\nthe most human-consistent model when considering the entire behavioral dataset we collected (Fig. 4). And yet,\nall of the models, including GPT-2, exhibited behavior inconsistent with human judgments; using an alternative\nmodel as a counterforce, we could corrupt natural sentences such that their probability under a model did not\ndecrease, but humans tended to reject the corrupted sentence as unlikely (Fig. 3b).\n3.1\nImplications for computational psycholinguistic modeling\nUnlike convolutional neural networks, whose architectural design principles are roughly inspired by biological\nvision [26], the design of current neural network language models is largely uninformed by psycholinguistics\nand neuroscience. And yet, there is an ongoing effort to adopt and adapt neural network language models\nto serve as computational hypotheses of how humans process language, making use of a variety of different\narchitectures, training corpora, and training tasks [11, 27–35]. We found that recurrent neural networks make\nmarkedly human-inconsistent predictions once pitted against transformer-based neural networks. This find-\ning coincides with recent evidence that transformers also outperform recurrent networks for predicting neural\nresponses as measured by ECoG or fMRI [11, 32], as well as with evidence from model-based prediction\nof human reading speed [33, 36] and N400 amplitude [36, 37]. Among the transformers, GPT-2, RoBERTa,\nand ELECTRA showed the best performance. These models are trained to optimize only word-level pre-\ndiction tasks, as opposed to BERT and XLM which are additionally trained on next-sentence prediction and\ncross-lingual tasks, respectively (and have the same architecture as RoBERTa). This suggests that local word\nprediction provides better alignment with human language comprehension.\nDespite the agreement between our results and previous work in terms of model ranking, the significant\nfailure of GPT-2 in predicting the human responses to natural versus synthetic controversial pairs (Fig. 3b)\ndemonstrates that GPT-2 does not fully emulate the computations employed in human processing of even short\nsentences. This outcome is in some ways unsurprising, given that GPT-2 (like all of the other models we con-\nsidered) is an off-the-shelf machine learning model that was not designed with human psycholinguistic and\nphysiological details in mind. And yet, the considerable human inconsistency we observed seems to stand in\nstark contrast with the recent report of GPT-2 explaining about 100 percent of the explainable variance in fMRI\nand ECoG responses to natural sentences [32]. Part of this discrepancy could be explained by the fact that\nSchrimpf and colleagues [32] mapped GPT-2 hidden-layer activations to brain data by means of regularized\nlinear regression, which can identify a subspace within GPT-2’s language representation that is well-aligned\nwith brain responses even if GPT-2’s overall sentence probabilities are not human-like. More importantly,\nwhen language models are evaluated with natural language, strong statistical models might capitalize on fea-\ntures in the data that are distinct from, but highly correlated with, features that are meaningful to humans.\nTherefore, a model that performs well on typical sentences might employ computational mechanisms that are\nvery distinct from the brain’s, which will only be revealed by testing the model in a more challenging domain.\nNote that even the simplest model we considered—a 2-gram frequency table—actually performed quite well\n10\non predicting human judgments for randomly-sampled natural sentences, and its deficiencies only became ob-\nvious when challenged by controversial sentence pairs. We predict that there will be substantial discrepancies\nbetween neural representations and current language models when using stimuli that intentionally stress-test\nthis relationship, using our proposed sentence-level controversiality approach or complementary ideas such as\nmaximizing controversial transition probabilities between consecutive words [38].\nUsing controversial sentences can be seen as a generalization test of language models: can models predict\nwhat kinds of changes to a natural sentence will lead to humans rejecting the sentence as improbable? Humans\nare sometimes capable of comprehending language with atypical constructions (e.g. in cases when pragmatic\njudgments can be made about a speaker’s intentions from environmental and linguistic context [39]), but none of\nthe models we tested were fully able to predict which syntactic or semantic perturbations would be accepted or\nrejected by humans. One possibility is that stronger next-word prediction models, using different architectures,\nlearning rules, or training data, might close the gap between models and humans. Alternatively, it might be\nthat optimizing for other linguistic tasks, or even non-linguistic task demands (in particular, representing the\nexternal world, the self, and other agents) will turn out to be critical for achieving human-like natural language\nprocessing [40].\n3.2\nControversial sentence pairs as adversarial attacks\nMachine vision models are highly susceptible to adversarial examples [41, 42]. Such adversarial examples are\ntypically generated by choosing a correctly classified natural image and then searching for a minuscule (and\ntherefore human-imperceptible) image perturbation that would change the targeted model’s classification. The\nprospect that similar covert model failure modes may exist also for language models has motivated proposed\ngeneralizations of adversarial methods to textual inputs [43]. However, imperceptible perturbations cannot be\napplied to written text: any modified word or character is humanly perceptible. Prior work on adversarial\nexamples for language models have instead relied on heuristic constraints aiming to limit the change in the\nmeaning of the text, such as flipping a character [44, 45], changing number or gender [46], or replacing words\nwith synonyms [47–49]. However, since these heuristics are only rough approximations of human language\nprocessing, many of these methods fail to preserve semantic meaning [50]. Interactive (“human-in-the-loop”)\nadversarial approaches allow human subjects to repeatedly alter model inputs such that it confuses target models\nbut not secondary participants [17, 51], but these approaches are inherently slow and costly and are limited by\nmental models the human subjects form about the evaluated language models.\nBy contrast, testing language models on controversial sentence pairs does not require approximating or\nquerying a human ground truth during optimization—the objective of controversiality is independent of cor-\nrectness. Instead, by designing inputs to elicit conflicting predictions among the models and assessing human\nresponses to these inputs only once the optimization loop has terminated, we capitalize on the simple fact that\nif two models disagree with respect to an input, at least one of the models must be making an incorrect predic-\ntion. Pitting language models against other language models also can be conducted by other approaches such\nas “red-teaming”, where an alternative language model is used as a generator of potential adversarial examples\nfor a targeted model and a classifier is used to filter the generated examples such that the output they induce\nin the targeted model is indeed incorrect [52]. Our approach shares the underlying principle that an alternative\nlanguage model can drive a more powerful test than handcrafted heuristics, but here the models have symmetric\nroles (there are no “attacking” and “attacked” models) and we can optimize stimuli directly without relying on\nfiltering.\n11\n3.3\nLimitations and future directions\nWhile our results demonstrate that using controversial stimuli can identify subtle differences in language mod-\nels’ alignment with human judgments, our study was limited in a number of ways. Our stimuli were all 8-word\nEnglish sentences, limiting our ability to make cognitively meaningful claims that apply to language use glob-\nally. 8-word sentences are long enough to include common syntactic constructions and convey meaningful\nideas but may not effectively probe long-distance syntactic dependencies [53]. Future work may introduce\nadditional sentence lengths and languages, as well as (potentially adaptive) controversial sentence optimization\nprocedures that consider large sets of candidate models, allowing for greater model coverage than our sim-\npler pairwise approach. Future work may also complement the model-comparative experimental design with\nprocedures designed to identify potential failure modes common to all models.\nA more substantial limitation of the current study is that, like any comparison of pre-trained neural networks\nas potential models of human cognition, there could be multiple reasons (i.e., training data, architecture, training\ntasks, learning rules) why particular models are better aligned with human judgments. For example, as we\ndid not systematically control the training corpora used for training the models, it is possible that some of\nthe observed differences are due to differences in the training sets rather than model architecture. Therefore,\nwhile our results expose failed model predictions, they do not readily answer why these failed predictions arise.\nFuture experiments could compare custom-trained or systematically manipulated models, which reflect specific\nhypotheses about human language processing. In Extended Data Fig. 5, we demonstrate the power of using\nsynthetic controversial stimuli to conduct sensitive comparisons between models with subtle differences in how\nsentence probabilities are calculated.\nIt is important to note that our analyses considered human relative probability judgments as reflecting\na scalar measure of acceptability. We made this assumption in order to bring the language models (which\nassign a probability measure to each sentence) and the human participants onto a common footing. However,\nit is possible that different types of sentence pairs engage different human cognitive processes. For pairs of\nsynthetic sentences, both sentences may be unacceptable in different ways (e.g. exhibit different kinds of\ngrammatical violations), requiring a judgment that weighs the relative importance of multiple dimensions [54]\nand could therefore produce inconsistent rankings across participants or across trials [55]. By contrast, asking\nparticipants to compare a natural and a synthetic sentence (Fig. 3b, Extended Data Table 1) may be more\nanalogous to previous work measuring human acceptability judgments for sentence pairs [24]. Nonetheless, it\nis worth noting that for all of the controversial conditions, the noise ceiling was significantly above the models’\nprediction accuracy, indicating non-random human preferences unexplained by current models that should be\naccounted for by future models, which may have to be more complex and capture multiple processes.\nFinally, the use of synthetic controversial sentences can be extended beyond probability judgments. A\nsufficiently strong language model may enable constraining the experimental design search-space to particular\nsentence distributions (e.g., movie reviews or medical questions). Given such a constrained space, we may\nbe able to search for well-formed sentences that elicit contradictory predictions in alternative domain-specific\nmodels (e.g., sentiment classifiers or question-answering models). However, as indicated by our results, the\ntask of capturing distributions of well-formed sentences is less trivial than it seems.\n12\n4\nMethods\n4.1\nLanguage models\nWe tested nine models from three distinct classes: n-gram models, recurrent neural networks, and transformers.\nThe n-gram models were trained with open source code from the Natural Language Toolkit [56], the recurrent\nneural networks were trained with architectures and optimization procedures available in PyTorch [57], and\nthe transformers were implemented with the open-source repository HuggingFace [58]. For full details see\nSupplementary Section 6.1.\n4.2\nEvaluating sentence probabilities in transformer models\nWe then sought to compute the probability of arbitrary sentences under each of the models described above.\nThe term “sentence” is used in this context in its broadest sense—a sequence of English words, not necessarily\nrestricted to grammatical English sentences. Unlike some classification tasks in which valid model predictions\nmay be expected only for grammatical sentences (e.g., sentiment analysis), the sentence probability comparison\ntask is defined over the entire domain of eight-word sequences.\nFor the set of unidirectional models, evaluating sentence probabilities was performed simply by summing\nthe log probabilities of each successive token in the sentence from left to right, given all the previous tokens.\nFor bidirectional models, this process was not as straightforward. One challenge is that transformer model\nprobabilities do not necessarily reflect a coherent joint probability; the summed log sentence probability result-\ning from adding words in one order (e.g. left to right) does not necessarily equal the probability resulting from a\ndifferent order (e.g. right to left). Here we developed a novel formulation of bidirectional sentence probabilities\nin which we considered all permutations of serial word positions as possible construction orders (analogous to\nthe random word visitation order used to sample serial reproduction chains, [59]). In practice, we observed that\nthe distribution of log probabilities resulting from different permutations tends to center tightly around a mean\nvalue (for example, for RoBERTa evaluated with natural sentences, the average coefficient of variation was\napproximately 0.059). Therefore in order to efficiently calculate bidirectional sentence probability, we evaluate\n100 different random permutations and define the overall sentence log probability as the mean log probability\ncalculated from each permutation. Specifically, we initialized an eight-word sentence with all tokens replaced\nwith the “mask” token used in place of to-be-predicted words during model training. We selected a random\npermutation P of positions 1 through 8, and started by computing the probability of the word at first of these\npositions P1 given the other seven “mask” tokens. We then replaced the “mask” at position P1 with the actual\nword at this position and computed the probability of the word at P2 given the other six “mask” tokens and the\nword at P1. This process was repeated until all “mask” tokens had been filled by the corresponding word.\nA secondary challenge in evaluating sentence probabilities in bidirectional transformer models stems from\nthe fact that these models use word-piece tokenizers (as opposed to whole words), and that these tokenizers are\ndifferent for different models. For example, one tokenizer might include the word “beehive” as a single token,\nwhile others strive for a smaller library of unique tokens by evaluating “beehive” as the two tokens “bee” and\n“hive”. The model probability of a multi-token word—similar to the probability of a multi-word sentence—\nmay depend on the order in which the chain rule is applied. Therefore, all unique permutations of token order\nfor each multi-token word were also evaluated within their respective “masks”. For example, the probability of\nthe word “beehive” would be evaluated as follows:\n13\nlog p(w = beehive) =0.5\n\u0000log p(w1 = bee | w2 = MASK) + log p(w2 = hive | w1 = bee)\n\u0001\n+0.5\n\u0000log p(w2 = hive | w1 = MASK) + log p(w1 = bee | w2 = hive)\n\u0001\n(1)\nThis procedure aimed to yield a more fair estimate of the conditional probabilities of word-piece tokens\nand therefore the overall probabilities of multi-token words by 1) ensuring that the word-piece tokens were\nevaluated within the same context of surrounding words and masks, and 2) eliminating the bias of evaluating\nthe word-piece tokens in any one particular order in models which were trained to predict bidirectionally.\nOne more procedure was applied in order to ensure that all models were computing a probability distribution\nover sentences with exactly 8 words. When evaluating the conditional probability of a masked word in models\nwith word-piece tokenizers, we normalized the model probabilities to ensure that only single words were being\nconsidered, rather than splitting the masked tokens into multiple words. At each evaluation step, each token was\nrestricted to come from one of four normalized distributions: i) single-mask words were restricted to be tokens\nwith appended white space, ii) masks at the beginning of a word were restricted to be tokens with preceding\nwhite space (in models with preceding white space, e.g. BERT), iii) masks at the end of words were restricted\nto be tokens with trailing white space (in models with trailing white space, e.g. XLM), and iv) masks in the\nmiddle of words were restricted to tokens with no appended white space.\n4.3\nAssessing potential token count effects on sentence probabilities\nNote that, because tokenization schemes varied across models, the number of tokens in a sentence could dif-\nfer for different models. These alternative tokenizations can be conceived of as different factorizations of the\nmodeled language distribution, changing how a sentence’s log probability is additively partitioned across the\nconditional probability chain (but not affecting its overall probability) [60]. Had we attempted to normalize\nacross models by dividing the log probability by the number of tokens, as is often done when aligning model\npredictions to human acceptability ratings [12, 13], our probabilities would have become strongly tokenization-\ndependent [60]. To empirically confirm that tokenization differences were not driving our results, we sta-\ntistically compared the token counts of each model’s preferred synthetic sentences with the token counts of\ntheir non-preferred counterparts. While we found significant differences for some of the models, there was\nno systematic association between token count and model sentence preferences (Supplementary Table S1). In\nparticular, lower sentence probabilities were not systematically confounded by higher token counts.\n4.4\nDefining a shared vocabulary\nTo facilitate the sampling, selection, and synthesis of sentences that could be evaluated by all of the candidate\nmodels, we defined a shared vocabulary of 29,157 unique words. Defining this vocabulary was necessary in\norder to unify the space of possible sentences between the transformer models (which can evaluate any input\ndue to their word-piece tokenizers) and the neural network and n-gram models (which include whole words as\ntokens), and to ensure we only included words that were sufficiently prevalent in the training corpora for all\nmodels. The vocabulary consisted of the words in the subtlex database [61], after removing words that occurred\nfewer than 300 times in the 300M word corpus (see Supplementary Section 6.1) used to train the n-gram and\nrecurrent neural network models (i.e., with frequencies lower than one in a million).\n14\n4.5\nSampling of natural sentences\nNatural sentences were sampled from the same four text sources used to construct the training corpus for the\nn-gram and recurrent neural network models, while ensuring that there was no overlap between training and\ntesting sentences. Sentences were filtered to include only those with eight distinct words and no punctuation\naside from periods, exclamation points, or question marks at the end of a sentence. Then, all eight-word\nsentences were further filtered to include only the words included in the shared vocabulary and to exclude those\nincluded in a predetermined list of inappropriate words and phrases. To identify controversial pairs of natural\nsentences, we used integer linear programming to search for sentences that had above-median probability in\none model and minimum probability rank in another model (see Supplementary Section 6.2).\n4.6\nGenerating synthetic controversial sentence pairs\nFor each pair of models, we synthesized 100 sentence triplets. Each triplet was initialized with a natural\nsentence n (sampled from Reddit). The words in sentence n were iteratively modified to generate a synthetic\nsentence with reduced probability according to the first model but not according to the second model. This\nprocess was repeated to generate another synthetic sentence from n, in which the roles of the two models\nwere reversed. Conceptually, this approach resembles Maximum Differentiation (MAD) competition [62],\nintroduced to compare models of image quality assessment. Each synthetic sentence was generated as a solution\nfor a constrained minimization problem:\ns∗= argmin\ns\nlog p(s | mreject)\nsubject to log p(s | maccept) ≥log p(n | maccept)\n(2)\nmreject denotes the model targeted to assign reduced sentence probability to the synthetic sentence compared\nto the natural sentence, and maccept denotes the model targeted to maintain a synthetic sentence probability\ngreater or equal to that of the natural sentence. For one synthetic sentence, one model served as maccept and\nthe other model served as mreject, and for the other synthetic sentence the model roles were flipped.\nAt each optimization iteration, we selected one of the eight words pseudorandomly (so that all eight posi-\ntions would be sampled N times before any position would be sampled N + 1 times) and searched the shared\nvocabulary for the replacement word that would minimize the log p(s | mreject) under the constraint. We ex-\ncluded potential replacement words that already appeared in the sentence, except for a list of 42 determiners and\nprepositions such as “the”, “a”, or “with”, which were allowed to repeat. The sentence optimization procedure\nwas concluded once eight replacement attempts (i.e., words for which no loss-reducing replacement has been\nfound) have failed in a row.\n4.7\nWord-level search for bidirectional models\nFor models for which the evaluation of log p(s | m) is computationally cheap (2-gram, 3-gram, LSTM, and\nthe RNN), we directly evaluated the log-probability of the 29,157 sentences resulting from each of the 29,157\npossible word replacements. When such probability vectors were available for both models, we simply chose\nthe replacement minimizing the loss. For GPT-2, whose evaluation is slower, we evaluated sentence probabil-\nities only for word replacements for which the new word had a conditional log-probability (given the previous\nwords in the sentence) of no less than −10; in rare cases when this threshold yielded fewer than 10 candidate\nwords, we reduced the threshold in steps of 5 until there were at least 10 words above the threshold. For the\n15\nbi-directional models (BERT, RoBERTa, XLM, and ELECTRA), for which the evaluation of log p(s | m) is\ncostly even for a single sentence, we used a heuristic to prioritize which replacements to evaluate.\nSince bi-directional models are trained as masked language models, they readily provide word-level com-\npletion probabilities. These word-level log-probabilities typically have positive but imperfect correlation with\nthe log-probabilities of the sentences resulting from each potential completion. We hence formed a simple\nlinear regression-based estimate of log p(s{i} ←w | m), the log-probability of the sentence s with word w\nassigned at position i, predicting it from log p(s{i} = w | m, s{i} ←mask), the completion log-probability\nof word w at position i, given the sentence with the i-th word masked:\nlog ˆp(s{i} ←w | m) = β1 log p(s{i} = w | m, s{i} ←mask) + β0\n(3)\nThis regression model was estimated from scratch for each word-level search. When a word was first\nselected for being replaced, the log-probability of two sentences was evaluated: the sentence resulting from\nsubstituting the existing word with the word with the highest completion probability and the sentence resulting\nfrom substituting the existing word with the word with the lowest completion probability. These two word-\nsentence log-probability pairs, as well as the word-sentence log-probability pair pertaining to the current word,\nwere used to fit the regression line. The regression prediction, together with the sentence probability for the\nother model (either the exact probability, or approximate probability if the other model was also bi-directional)\nwas used to predict log p(s | mreject) for each of the 29,157 potential replacements. We then evaluated the\ntrue (non-approximate) sentence probabilities of the replacement word with the minimal predicted probability.\nIf this word indeed reduced the sentence probability, it was chosen to serve as the replacement and the word-\nlevel search was terminated (i.e., proceeding to search a replacement for another word in the sentence). If it\ndid not reduce the probability, the regression model (Eq. 3) was updated with the new observation, and the\nnext replacement expected to minimize the sentence probability was evaluated. This word-level search was\nterminated after five sentence evaluations that did not reduce the loss.\n4.8\nSelecting the best triplets from the optimized sentences\nSince the discrete hill-climbing procedure described above is highly local, the degree to which this succeeded in\nproducing highly-controversial pairs varied depending on the starting sentence n. We found that typically, nat-\nural sentences with lower than average log-probability gave rise to synthetic sentences with greater controver-\nsiality. To better represent the distribution of natural sentences while still choosing the best (most controversial)\ntriplets for human testing, we used stratified selection.\nFirst, we quantified the controversiality of each triplet as\ncm1,m2(n, s1, s2) = log p(n | m1)\np(s1 | m1) + log p(n | m2)\np(s2 | m2),\n(4)\nwhere s1 is the sentence generated to reduce the probability in model m1 and s2 is the sentence generated to\nreduce the probability in model m2.\nWe employed integer programming to choose the 10 most controversial triplets from the 100 triplets opti-\nmized for each model pair (maximizing the total controversiality across the selected triplets), while ensuring\nthat for each model, there was exactly one natural sentence in each decile of the natural sentences probability\ndistribution. The selected 10 synthetic triplets were then used to form 30 unique experimental trials per model\npair, comparing the natural sentence with one synthetic sentence, comparing the natural sentence with the other\nsynthetic sentence, and comparing the two synthetic sentences.\n16\n4.9\nDesign of the human experiment\nOur experimental procedures were approved by the Columbia University Institutional Review Board (protocol\nnumber IRB-AAAS0252) and were performed in accordance with the approved protocol. All participants\nprovided informed consent prior. We presented the controversial sentence pairs selected and synthesized by\nthe language models to 100 native English-speaking, US-based participants (55 male) recruited from Prolific\n(www.prolific.co), and paid each participant $5.95. The average participant age was 34.08 ± 12.32. The\nsubjects were divided into 10 groups, and each ten-subject group was presented with a unique set of stimuli.\nEach stimulus set contained exactly one sentence pair from every possible combination of model pairs and\nthe four main experimental conditions: selected controversial sentence pairs; natural vs. synthetic pair, where\none model served as maccept and the other as mreject; a natural vs. synthetic pair with the reverse model\nrole assignments; and directly pairing the two synthetic sentences. These model-pair-condition combinations\naccounted for 144 (36×4) trials of the task. In addition to these trials, each stimulus set also included nine trials\nconsisting of sentence pairs randomly sampled from the database of eight-word sentences (and not already\nincluded in any of the other conditions). All subjects also viewed 12 control trials consisting of a randomly\nselected natural sentence and the same natural sentence with the words scrambled in a random order. The order\nof trials within each stimulus set as well as the left-right screen position of sentences in each sentence pair\nwere randomized for all participants. While each sentence triplet produced by the optimization procedure (see\nsubsection “Generating synthetic controversial sentence pairs”) gave rise to three trials, these were allocated\nsuch that no subject viewed the same sentence twice.\nOn each trial of the task, participants were asked to make a binary decision about which of the two sentences\nthey considered more probable (for the full set of instructions given to participants, see Supplementary Fig. S2).\nIn addition, they were asked to indicate one of three levels of confidence in their decision: somewhat confident,\nconfident, or very confident. The trials were not timed, but a 90-minute time limit was enforced for the whole\nexperiment. A progress bar at the bottom of the screen indicated to participants how many trials they had\ncompleted and had remaining to complete.\nWe rejected the data of 21 participants who failed to choose the original, unshuffled sentence in at least\n11 of the 12 control trials, and acquired data from 21 alternative participants instead, all of whom passed this\ndata-quality threshold. In general, we observed high agreement in sentence preferences among our participants,\nthough the level of agreement varied across conditions. There was complete or near-complete agreement (at\nleast 9/10 participants with the same binary sentence preference) in 52.2% of trials for randomly-sampled\nnatural-sentence pairs, 36.6% of trials for controversial natural-sentence pairs, 67.6% of trials for natural-\nsynthetic pairs, and 60.0% of trials for synthetic-synthetic pairs (versus a chance rate of 1.1%, assuming a\nbinomial distribution with p = 0.5).\n4.10\nEvaluation of model-human consistency\nTo measure the alignment on each trial between model judgments and human judgments, we binarized both\nmeasures; we determined which of the two sentences was assigned with a higher probability by the model,\nregardless of the magnitude of the probability difference, and which of the two sentences was favored by the\nsubject, regardless of the reported confidence level. When both the subject and the model chose the same\nsentence, the trial was considered as correctly predicted by that model. This correctness measure was averaged\nacross sentence pairs and across the 10 participants who viewed the same set of trials. For the lower bound on\nthe noise ceiling, we predicted each subject’s choices from a majority vote of the nine other subjects who were\npresented with the same trials. For the upper bound (i.e., the highest possible accuracy attainable on this data\n17\nsample), we included the subject themselves in this majority vote-based prediction.\nSince each of the 10 participant groups viewed a unique trial set, these groups provided 10 independent\nreplications of the experiment. Models were compared to each other and to the lower bound of the noise\nceiling by a Wilcoxon signed-rank test using these 10 independent accuracy outcomes as paired samples. For\neach analysis, the false discovery rate across multiple comparisons was controlled by the Benjamini-Hochberg\nprocedure [63].\nIn Fig. 4, we instead measure model-human consistency in a more continuous way, comparing the sentence\nprobability ratio in a model to the graded Likert ratings provided by humans; see Supplementary Section 6.3\nfor full details.\n4.11\nSelecting trials for model evaluation\nAll of the randomly sampled natural-sentence pairs (Fig. 1a) were evaluated for each of the candidate models.\nControversial sentence pairs (either natural, Fig. 1b or synthetic, Fig. 3) were included in a model’s evaluation\nset only if they were formed to target that model specifically. The overall summary analysis (Fig. 4) evaluated\nall models on all available sentence pairs.\n4.12\nComparison to pseudo-log-likelihood acceptability measures\nWang & Cho [64] proposed an alternative approach for computing sentence probabilities in bidirectional\n(BERT-like) models, using a pseudo-log-likelihood measure that simply sums the log-probability of each token\nconditioned on all of the other tokens in the sentence. While this measure does not reflect a true probability\ndistribution [65], it is positively correlated with human acceptability judgments for several bidirectional models\n[13, 66]. To directly compare this existing approach to our novel method for computing probabilities, we again\nused the method of controversial sentence pairs to identify the approach most aligned with human judgments.\nFor each bidirectional model (BERT, RoBERTa, and ELECTRA), we created two copies of the model, each us-\ning a different approach for computing sentence probabilities. We synthesized 40 sentence pairs to maximally\ndifferentiate between the two copies of each model, with each copy assigning a higher probability to a different\nsentence in the pair. Subsequently, we tested 30 human participants, presenting each participant with all 120\nsentence pairs. Model-human consistency was quantified as in the main experiment.\n4.13\nData and code availability\nThe experimental stimuli, detailed behavioral testing results, sentence optimization code, and code for repro-\nducing all analyses and figures are available at github.com/dpmlab/contstimlang [67].\nAcknowledgments\nThis material is based upon work partially supported by the National Science Foundation under Grant No.\n1948004 to NK. This publication was made possible with the support of the Charles H. Revson Foundation to\nTG. The statements made and views expressed, however, are solely the responsibility of the authors.\n18\nAuthor Contributions\nT.G., M.S., N.K., and C.B. designed the study. M.S. implemented the computational models and T.G. imple-\nmented the sentence pair optimization procedures. M.S. conducted the behavioral experiments. T.G. and M.S.\nanalyzed the experiments’ results. T.G., M.S., N.K., and C.B. wrote the paper.\nCompeting Interests\nThe authors declare no competing interests.\nReferences\n1.\nRumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by back-propagating errors.\nNature 323, 533–536. doi:10.1038/323533a0 (1986).\n2.\nHochreiter, S. & Schmidhuber, J. Long Short-Term Memory. Neural Computation 9, 1735–1780. doi:10.\n1162/neco.1997.9.8.1735 (1997).\n3.\nDevlin, J., Chang, M., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers\nfor Language Understanding in Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies (Minneapolis, MN, USA,\n2019), 4171–4186. doi:10.18653/v1/n19-1423.\n4.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. & Stoyanov,\nV. RoBERTa: A Robustly Optimized BERT Pretraining Approach. Preprint at https://arxiv.org/\nabs/1907.11692 (2019).\n5.\nConneau, A. & Lample, G. Cross-lingual Language Model Pretraining in Advances in Neural Information\nProcessing Systems 32 (Vancouver, BC, Canada, 2019). URL: proceedings.neurips.cc/paper/\n2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf.\n6.\nClark, K., Luong, M., Le, Q. V. & Manning, C. D. ELECTRA: Pre-training Text Encoders as Discrimina-\ntors Rather Than Generators in 8th International Conference on Learning Representations, ICLR 2020\n(Online, 2020). URL: openreview.net/forum?id=r1xMH1BtvB.\n7.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are un-\nsupervised multitask learners 2019. URL: cdn.openai.com/better- language- models/\nlanguage_models_are_unsupervised_multitask_learners.pdf.\n8.\nGoodkind, A. & Bicknell, K. Predictive power of word surprisal for reading times is a linear function of\nlanguage model quality in Proceedings of the 8th Workshop on Cognitive Modeling and Computational\nLinguistics (CMCL 2018) (Salt Lake City, Utah, 2018), 10–18. doi:10.18653/v1/W18-0102.\n9.\nShain, C., Blank, I. A., van Schijndel, M., Schuler, W. & Fedorenko, E. fMRI reveals language-specific\npredictive coding during naturalistic sentence comprehension. Neuropsychologia 138, 107307. doi:10.\n1016/j.neuropsychologia.2019.107307 (2020).\n10.\nBroderick, M. P., Anderson, A. J., Di Liberto, G. M., Crosse, M. J. & Lalor, E. C. Electrophysiologi-\ncal correlates of semantic dissimilarity reflect the comprehension of natural, narrative speech. Current\nBiology 28, 803–809. doi:10.1016/j.cub.2018.01.080 (2018).\n19\n11.\nGoldstein, A., Zada, Z., Buchnik, E., Schain, M., Price, A., Aubrey, B., Nastase, S. A., Feder, A., Emanuel,\nD., Cohen, A., Jansen, A., Gazula, H., Choe, G., Rao, A., Kim, C., Casto, C., Fanda, L., Doyle, W.,\nFriedman, D., Dugan, P., Melloni, L., Reichart, R., Devore, S., Flinker, A., Hasenfratz, L., Levy, O.,\nHassidim, A., Brenner, M., Matias, Y., Norman, K. A., Devinsky, O. & Hasson, U. Shared computational\nprinciples for language processing in humans and deep language models. Nature Neuroscience 25, 369–\n380. doi:10.1038/s41593-022-01026-4 (2022).\n12.\nLau, J. H., Clark, A. & Lappin, S. Grammaticality, Acceptability, and Probability: A Probabilistic View\nof Linguistic Knowledge. Cognitive Science 41, 1202–1241. doi:10.1111/cogs.12414 (2017).\n13.\nLau, J. H., Armendariz, C., Lappin, S., Purver, M. & Shu, C. How Furiously Can Colorless Green Ideas\nSleep? Sentence Acceptability in Context. Transactions of the Association for Computational Linguistics\n8, 296–310. doi:10.1162/tacl_a_00315 (2020).\n14.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O. & Bowman, S. R. GLUE: A Multi-Task Benchmark\nand Analysis Platform for Natural Language Understanding in 7th International Conference on Learning\nRepresentations, ICLR 2019, (New Orleans, LA, USA, 2019). URL: openreview.net/forum?id=\nrJ4km2R5t7.\n15.\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O. & Bowman, S. Su-\nperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems in Advances\nin Neural Information Processing Systems 32 (Vancouver, BC, Canada, 2019). URL: proceedings.\nneurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf.\n16.\nWarstadt, A., Parrish, A., Liu, H., Mohananey, A., Peng, W., Wang, S.-F. & Bowman, S. R. BLiMP: The\nBenchmark of Linguistic Minimal Pairs for English. Transactions of the Association for Computational\nLinguistics 8, 377–392. doi:10.1162/tacl_a_00321 (2020).\n17.\nKiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu, Z., Vidgen, B., Prasad, G., Singh, A., Ring-\nshia, P., Ma, Z., Thrush, T., Riedel, S., Waseem, Z., Stenetorp, P., Jia, R., Bansal, M., Potts, C. & Williams,\nA. Dynabench: Rethinking Benchmarking in NLP in Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies\n(Online, 2021), 4110–4124. doi:10.18653/v1/2021.naacl-main.324.\n18.\nBox, G. E. & Hill, W. J. Discrimination Among Mechanistic Models. Technometrics 9, 57–71. doi:10.\n1080/00401706.1967.10490441 (1967).\n19.\nGolan, T., Raju, P. C. & Kriegeskorte, N. Controversial stimuli: Pitting neural networks against each other\nas models of human cognition. Proceedings of the National Academy of Sciences 117, 29330–29337.\ndoi:10.1073/pnas.1912334117 (2020).\n20.\nCross, D. V. Sequential dependencies and regression in psychophysical judgments. Perception & Psy-\nchophysics 14, 547–552. doi:10.3758/BF03211196 (1973).\n21.\nFoley, H. J., Cross, D. V. & O’reilly, J. A. Pervasiveness and magnitude of context effects: Evidence\nfor the relativity of absolute magnitude estimation. Perception & Psychophysics 48, 551–558. doi:10.\n3758/BF03211601 (1990).\n22.\nPetzschner, F. H., Glasauer, S. & Stephan, K. E. A Bayesian perspective on magnitude estimation. Trends\nin Cognitive Sciences 19, 285–293. doi:10.1016/j.tics.2015.03.002 (2015).\n23.\nGreenbaum, S. Contextual Influence on Acceptability Judgments. Linguistics 15. doi:10.1515/ling.\n1977.15.187.5 (1977).\n20\n24.\nSch¨utze, C. T. & Sprouse, J. in (eds Podesva, R. J. & Sharma, D.) 27–50 (Cambridge University Press,\nCambridge, 2014). doi:10.1017/CBO9781139013734.004.\n25.\nSprouse, J. & Almeida, D. Design sensitivity and statistical power in acceptability judgment experiments.\nGlossa 2, 14. doi:10.5334/gjgl.236 (2017).\n26.\nLindsay, G. W. Convolutional Neural Networks as a Model of the Visual System: Past, Present, and\nFuture. Journal of Cognitive Neuroscience 33, 2017–2031. doi:10.1162/jocn_a_01544 (2021).\n27.\nWehbe, L., Vaswani, A., Knight, K. & Mitchell, T. Aligning context-based statistical models of language\nwith brain activity during reading in Proceedings of the 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) (Doha, Qatar, 2014), 233–243. doi:10.3115/v1/D14-1030.\n28.\nToneva, M. & Wehbe, L. Interpreting and improving natural-language processing (in machines) with\nnatural language-processing (in the brain) in Advances in Neural Information Processing Systems 32\n(Vancouver, BC, Canada, 2019). URL: proceedings . neurips . cc / paper / 2019 / file /\n749a8e6c231831ef7756db230b4359c8-Paper.pdf.\n29.\nHeilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P. & De Lange, F. P. A hierarchy of linguistic\npredictions during natural language comprehension. Proceedings of the National Academy of Sciences\n119, e2201968119. doi:10.1073/pnas.2201968119 (2022).\n30.\nJain, S., Vo, V., Mahto, S., LeBel, A., Turek, J. S. & Huth, A. Interpretable multi-timescale models for\npredicting fMRI responses to continuous natural speech in Advances in Neural Information Processing\nSystems 33 (Online, 2020), 13738–13749. URL: proceedings.neurips.cc/paper_files/\npaper/2020/file/9e9a30b74c49d07d8150c8c83b1ccf07-Paper.pdf.\n31.\nLyu, B., Marslen-Wilson, W. D., Fang, Y. & Tyler, L. K. Finding structure in time: Humans, machines,\nand language. bioRxiv. Preprint at https://www.biorxiv.org/content/10.1101/2021.\n10.25.465687v2 (2021).\n32.\nSchrimpf, M., Blank, I. A., Tuckute, G., Kauf, C., Hosseini, E. A., Kanwisher, N., Tenenbaum, J. B. &\nFedorenko, E. The neural architecture of language: Integrative modeling converges on predictive pro-\ncessing. Proceedings of the National Academy of Sciences 118, e2105646118. doi:10.1073/pnas.\n2105646118 (2021).\n33.\nWilcox, E., Vani, P. & Levy, R. A Targeted Assessment of Incremental Processing in Neural Language\nModels and Humans in Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers) (Online, 2021), 939–952. doi:10.18653/v1/2021.acl-long.76.\n34.\nCaucheteux, C. & King, J.-R. Brains and algorithms partially converge in natural language processing.\nCommunications Biology 5, 134. doi:10.1038/s42003-022-03036-1 (2022).\n35.\nArehalli, S., Dillon, B. & Linzen, T. Syntactic Surprisal From Neural Models Predicts, But Underesti-\nmates, Human Processing Difficulty From Syntactic Ambiguities in Proceedings of the 26th Conference\non Computational Natural Language Learning (CoNLL) (Abu Dhabi, United Arab Emirates (Hybrid),\n2022), 301–313. doi:10.18653/v1/2022.conll-1.20.\n36.\nMerkx, D. & Frank, S. L. Human Sentence Processing: Recurrence or Attention? Proceedings of the\nWorkshop on Cognitive Modeling and Computational Linguistics. doi:10.18653/v1/2021.cmcl-\n1.2. URL: dx.doi.org/10.18653/v1/2021.cmcl-1.2 (2021).\n21\n37.\nMichaelov, J. A., Bardolph, M. D., Coulson, S. & Bergen, B. K. Different kinds of cognitive plausibility:\nwhy are transformers better than RNNs at predicting N400 amplitude? in Proceedings of the Annual Meet-\ning of the Cognitive Science Society 43 (2021). URL: escholarship.org/uc/item/9z06m20f.\n38.\nRakocevic, L. I. Synthesizing controversial sentences for testing the brain-predictivity of language models\nhttps://hdl.handle.net/1721.1/130713. PhD thesis (Massachusetts Institute of Technol-\nogy, 2021).\n39.\nGoodman, N. D. & Frank, M. C. Pragmatic Language Interpretation as Probabilistic Inference. Trends in\nCognitive Sciences 20, 818–829. doi:10.1016/j.tics.2016.08.005 (2016).\n40.\nHowell, S. R., Jankowicz, D. & Becker, S. A model of grounded language acquisition: Sensorimotor\nfeatures improve lexical and grammatical learning. Journal of Memory and Language 53, 258–276.\ndoi:https://doi.org/10.1016/j.jml.2005.03.002 (2005).\n41.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. & Fergus, R. Intriguing\nproperties of neural networks Preprint at http://arxiv.org/abs/1312.6199. 2013.\n42.\nGoodfellow, I. J., Shlens, J. & Szegedy, C. Explaining and Harnessing Adversarial Examples in 3rd\nInternational Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings (2015). URL: arxiv.org/abs/1412.6572.\n43.\nZhang, W. E., Sheng, Q. Z., Alhazmi, A. & Li, C. Adversarial attacks on deep-learning models in natural\nlanguage processing: A survey. ACM Transactions on Intelligent Systems and Technology (TIST) 11, 1–\n41. doi:10.1145/3374217 (2020).\n44.\nLiang, B., Li, H., Su, M., Bian, P., Li, X. & Shi, W. Deep Text Classification Can be Fooled in Proceedings\nof the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18 (Stockholm,\nSweden, 2018), 4208–4215. doi:10.24963/ijcai.2018/585.\n45.\nEbrahimi, J., Rao, A., Lowd, D. & Dou, D. HotFlip: White-Box Adversarial Examples for Text Classifica-\ntion in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n2: Short Papers) (Melbourne, Australia, 2018), 31–36. doi:10.18653/v1/P18-2006.\n46.\nAbdou, M., Ravishankar, V., Barrett, M., Belinkov, Y., Elliott, D. & Søgaard, A. The Sensitivity of Lan-\nguage Models and Humans to Winograd Schema Perturbations in Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics (Online, 2020), 7590–7604. doi:10.18653/v1/\n2020.acl-main.679.\n47.\nAlzantot, M., Sharma, Y., Elgohary, A., Ho, B.-J., Srivastava, M. & Chang, K.-W. Generating Natural\nLanguage Adversarial Examples in Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing (Brussels, Belgium, 2018), 2890–2896. doi:10.18653/v1/D18-1316.\n48.\nRibeiro, M. T., Singh, S. & Guestrin, C. Semantically Equivalent Adversarial Rules for Debugging NLP\nmodels in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) (Melbourne, Australia, 2018), 856–865. doi:10.18653/v1/P18-1079.\n49.\nRen, S., Deng, Y., He, K. & Che, W. Generating Natural Language Adversarial Examples through Prob-\nability Weighted Word Saliency in Proceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics (Florence, Italy, 2019), 1085–1097. doi:10.18653/v1/P19-1103.\n50.\nMorris, J., Lifland, E., Lanchantin, J., Ji, Y. & Qi, Y. Reevaluating Adversarial Examples in Natural\nLanguage in Findings of the Association for Computational Linguistics: EMNLP 2020 (Online, 2020),\n3829–3839. doi:10.18653/v1/2020.findings-emnlp.341.\n22\n51.\nWallace, E., Rodriguez, P., Feng, S., Yamada, I. & Boyd-Graber, J. Trick Me If You Can: Human-in-the-\nLoop Generation of Adversarial Examples for Question Answering. Transactions of the Association for\nComputational Linguistics 7, 387–401. doi:10.1162/tacl_a_00279 (2019).\n52.\nPerez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A., McAleese, N. & Irving, G.\nRed Teaming Language Models with Language Models in Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing (Abu Dhabi, United Arab Emirates, 2022), 3419–3448.\ndoi:10.18653/v1/2022.emnlp-main.225.\n53.\nGibson, E. Linguistic complexity: locality of syntactic dependencies. Cognition 68, 1–76. doi:10.1016/\nS0010-0277(98)00034-1 (1998).\n54.\nWatt, W. C. The indiscreteness with which impenetrables are penetrated. Lingua 37, 95–128. doi:10.\n1016/0024-3841(75)90046-7 (1975).\n55.\nSch¨utze, C. T. The empirical base of linguistics. Grammaticality judgments and linguistic methodology\nClassics in Linguistics 2. doi:10.17169/langsci.b89.100 (Language Science Press, Berlin,\n2016).\n56.\nBird, S., Klein, E. & Loper, E. Natural language processing with Python: analyzing text with the natural\nlanguage toolkit (”O’Reilly Media, Inc.”, Sebastopol, CA, USA, 2009).\n57.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,\nN., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy,\nS., Steiner, B., Fang, L., Bai, J. & Chintala, S. PyTorch: An Imperative Style, High-Performance Deep\nLearning Library in Advances in Neural Information Processing Systems 32 (Vancouver, BC, Canada,\n2019), 8024–8035. URL: papers.neurips.cc/paper/9015-pytorch-an-imperative-\nstyle-high-performance-deep-learning-library.pdf.\n58.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Fun-\ntowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger,\nS., Drame, M., Lhoest, Q. & Rush, A. M. Transformers: State-of-the-Art Natural Language Processing\nin Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations (Online, 2020), 38–45. doi:10.18653/v1/2020.emnlp-demos.6.\n59.\nYamakoshi, T., Griffiths, T. & Hawkins, R. Probing BERT’s priors with serial reproduction chains in\nFindings of the Association for Computational Linguistics: ACL 2022 (Dublin, Ireland, 2022), 3977–\n3992. doi:10.18653/v1/2022.findings-acl.314.\n60.\nChestnut, S. Perplexity Accessed: 2022-09-23. 2019. URL: drive.google.com/uc?export=\ndownload&id=1gSNfGQ6LPxlNctMVwUKrQpUA7OLZ83PW.\n61.\nVan Heuven, W. J. B., Mandera, P., Keuleers, E. & Brysbaert, M. Subtlex-UK: A New and Improved Word\nFrequency Database for British English. Quarterly Journal of Experimental Psychology 67, 1176–1190.\ndoi:10.1080/17470218.2013.850521 (2014).\n62.\nWang, Z. & Simoncelli, E. P. Maximum differentiation (MAD) competition: A methodology for compar-\ning computational models of perceptual quantities. Journal of Vision 8, 8–8. doi:10.1167/8.12.8\n(2008).\n63.\nBenjamini, Y. & Hochberg, Y. Controlling the False Discovery Rate: A Practical and Powerful Approach\nto Multiple Testing. Journal of the Royal Statistical Society: Series B (Methodological) 57, 289–300.\ndoi:10.1111/j.2517-6161.1995.tb02031.x (1995).\n23\n64.\nWang, A. & Cho, K. BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language\nModel in Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language\nGeneration (Minneapolis, Minnesota, 2019), 30–36. doi:10.18653/v1/W19-2304.\n65.\nCho, K. BERT has a Mouth and must Speak, but it is not an MRF kyunghyuncho.me/bert-has-\na-mouth-and-must-speak-but-it-is-not-an-mrf/. Accessed: 2022-09-28. 2019.\n66.\nSalazar, J., Liang, D., Nguyen, T. Q. & Kirchhoff, K. Masked Language Model Scoring in Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics (Online, 2020), 2699–2712.\ndoi:10.18653/v1/2020.acl-main.240.\n67.\nGolan, T., Siegelman, M., Kriegeskorte, N. & Baldassano, C. Code and data for ”Testing the limits of\nnatural language models for predicting human language judgments” version 1.2.2. 2023. doi:10.5281/\nzenodo.8147166.\n68.\nShannon, C. E. A mathematical theory of communication. The Bell System Technical Journal 27, 379–\n423. doi:10.1002/j.1538-7305.1948.tb01338.x (1948).\n69.\nIrvine, A., Langfus, J. & Callison-Burch, C. The American Local News Corpus in Proceedings of the Ninth\nInternational Conference on Language Resources and Evaluation (LREC’14) (Reykjavik, Iceland, 2014),\n1305–1308. URL: www.lrec-conf.org/proceedings/lrec2014/pdf/914_Paper.pdf.\n70.\nKneser, R. & Ney, H. Improved backing-off for m-gram language modeling in 1995 international confer-\nence on acoustics, speech, and signal processing 1 (1995), 181–184. doi:10.1109/ICASSP.1995.\n479394.\n71.\nGurobi Optimization, LLC. Gurobi Optimizer Reference Manual 2021. URL: www.gurobi.com.\n72.\nWoodbury, M. A. Rank Correlation when There are Equal Variates. The Annals of Mathematical Statistics\n11, 358–362. URL: www.jstor.org/stable/2235684 (1940).\n73.\nSch¨utt, H. H., Kipnis, A. D., Diedrichsen, J. & Kriegeskorte, N. Statistical inference on representational\ngeometries. eLife 12 (eds Serences, J. T. & Behrens, T. E.) e82566. doi:10.7554/eLife.82566\n(2023).\n74.\nNili, H., Wingfield, C., Walther, A., Su, L., Marslen-Wilson, W. & Kriegeskorte, N. A Toolbox for Rep-\nresentational Similarity Analysis. PLOS Computational Biology 10, 1–11. doi:10.1371/journal.\npcbi.1003553 (2014).\n75.\nPennington, J., Socher, R. & Manning, C. GloVe: Global Vectors for Word Representation in Proceedings\nof the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Doha, Qatar,\n2014), 1532–1543. doi:10.3115/v1/D14-1162.\n76.\nFrank, S. L. & Willems, R. M. Word predictability and semantic similarity show distinct patterns of\nbrain activity during language comprehension. Language, Cognition and Neuroscience 32, 1192–1203.\ndoi:10.1080/23273798.2017.1323109 (2017).\n5\nExtended Data\n24\nExtended Data Figure 1: An example of one experimental trial, as presented to the participants. The participant must\nchoose one sentence while providing their confidence rating on a 3-point scale.\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\nmodel 2\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\nmodel 1\n0.93 0.88 0.86 0.83 0.87 0.76 0.82 0.78\n0.93\n0.90 0.83 0.88 0.89 0.80 0.84 0.82\n0.88 0.90\n0.87 0.84 0.90 0.83 0.90 0.86\n0.86 0.83 0.87\n0.84 0.83 0.77 0.83 0.77\n0.83 0.88 0.84 0.84\n0.86 0.81 0.79 0.77\n0.87 0.89 0.90 0.83 0.86\n0.87 0.91 0.84\n0.76 0.80 0.83 0.77 0.81 0.87\n0.84 0.89\n0.82 0.84 0.90 0.83 0.79 0.91 0.84\n0.91\n0.78 0.82 0.86 0.77 0.77 0.84 0.89 0.91\n0.00\n0.25\n0.50\n0.75\n1.00\nbetween model agreement rate\n(proportion of sentence pairs)\nExtended Data Figure 2: Between-model agreement rate\non the probability ranking of the 90 randomly sampled\nand paired natural sentence pairs evaluated in the exper-\niment. Each cell represents the proportion of sentence pairs\nfor which two models make congruent probability ranking\n(i.e., both models assign a higher probability to sentence 1,\nor both models assign a higher probability to sentence 2).\n25\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\nmodel 2\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\nmodel 1\n0.51 0.53 0.78 0.83 0.66 0.84 0.79 0.88\n0.49\n0.60 0.65 0.78 0.60 0.80 0.76 0.86\n0.47 0.40\n0.63 0.75 0.57 0.79 0.64 0.59\n0.22 0.35 0.37\n0.58 0.30 0.44 0.60 0.54\n0.17 0.22 0.25 0.42\n0.47 0.49 0.42 0.55\n0.34 0.40 0.43 0.70 0.53\n0.84 0.62 0.70\n0.16 0.20 0.21 0.56 0.51 0.16\n0.49 0.62\n0.21 0.24 0.36 0.40 0.58 0.38 0.51\n0.63\n0.12 0.14 0.41 0.46 0.45 0.30 0.38 0.37\n0.00\n0.25\n0.50\n0.75\n1.00\nhuman choice aligned with model 1\n(proportion of trials)\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\nmodel 2\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\nmodel 1\n0.55 0.61 0.79 0.84 0.85 0.86 0.76 0.92\n0.45\n0.36 0.60 0.89 0.81 0.96 0.94 0.95\n0.39 0.64\n0.49 0.88 0.89 0.95 0.98 0.95\n0.21 0.40 0.51\n0.71 0.73 0.86 0.78 0.90\n0.16 0.11 0.12 0.29\n0.51 0.70 0.72 0.91\n0.15 0.19 0.11 0.27 0.49\n0.79 0.65 0.71\n0.14 0.04 0.05 0.14 0.30 0.21\n0.32 0.55\n0.24 0.06 0.02 0.22 0.28 0.35 0.68\n0.67\n0.08 0.05 0.05 0.10 0.09 0.29 0.45 0.33\n0.00\n0.25\n0.50\n0.75\n1.00\nhuman choice aligned with model 1\n(proportion of trials)\na Natural controversial sentences\nb Synthetic controversial sentences\nExtended Data Figure 3: Pairwise model comparison of model-human consistency. For each pair of models (represented\nas one cell in the matrices above), the only trials considered were those in which the stimuli were either selected (a) or\nsynthesized (b) to contrast the predictions of the two models. For these trials, the two models always made controversial\npredictions (i.e., one sentence is preferred by the first model and the other sentence is preferred by the second model). The\nmatrices above depict the proportion of trials in which the binarized human judgments aligned with the row model (“model\n1”). For example, GPT-2 (top-row) was always more aligned (green hues) with the human choices than its rival models. In\ncontrast, 2-gram (bottom-row) was always less aligned (purple hues) with the human choices than its rival models.\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\nmodels assigned as mreject\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\nmodels assigned as maccept\n0.21 0.24 0.43 0.23 0.29 0.36 0.17 0.22\n0.21\n0.16 0.29 0.51 0.29 0.23 0.18 0.39\n0.17 0.20\n0.27 0.34 0.27 0.40 0.27 0.20\n0.14 0.18 0.27\n0.44 0.23 0.25 0.33 0.20\n0.07 0.07 0.11 0.16\n0.17 0.13 0.08 0.22\n0.03 0.01 0.07 0.04 0.17\n0.06 0.16 0.04\n0.05 0.03 0.05 0.03 0.03 0.01\n0.04 0.06\n0.04 0.03 0.02 0.08 0.05 0.04 0.06\n0.04\n0.01 0.02 0.01 0.02 0.04 0.01 0.07 0.04\n0.00\n0.25\n0.50\n0.75\n1.00\nhumans choice aligned with maccept\n(proportion of trials)\nExtended Data Figure 4:\nPairwise model analysis of human re-\nsponse for natural vs. synthetic sentence pairs. In each optimiza-\ntion condition, a synthetic sentence s was formed by modifying a nat-\nural sentence n so the synthetic sentence would be “rejected” by one\nmodel (mreject, columns), minimizing p(s | mreject), and would be\n“accepted” by another model (maccept, rows), satisfying the constraint\np(s | maccept) ≥p(n | maccept). Each cell above summarizes model-\nhuman agreement in trials resulting from one such optimization condi-\ntion. The color of each cell denotes the proportion of trials in which\nhumans judged a synthetic sentence to be more likely than its natural\ncounterpart and hence aligned with maccept. For example, the top-right\ncell depicts human judgments for sentence pairs formed to minimize\nthe probability assigned to the synthetic sentence by the simple 2-gram\nmodel while ensuring that GPT-2 would judge the synthetic sentence\nto be at least as likely as the natural sentence; humans favored the syn-\nthetic sentence in only 22 out the 100 sentence pairs in this condition.\n26\n1.0\n0.5\n0.0\n0.5\n1.0\nordinal correlation between human ratings and models'\nsentence pair probability log-ratio (signed-rank cosine similarity)\nRoBERTa\nRoBERTa (PLL)\nELECTRA\nELECTRA (PLL)\nBERT\nBERT (PLL)\na Randomly sampled natural-sentence pairs\n1.0\n0.5\n0.0\n0.5\n1.0\nordinal correlation between human ratings and models'\nsentence pair probability log-ratio (signed-rank cosine similarity)\nRoBERTa\nRoBERTa (PLL)\nELECTRA\nELECTRA (PLL)\nBERT\nBERT (PLL)\nb Synthetic controversial sentence pairs\nExtended Data Figure 5: Human consistency of bidirectional transformers: approximate log-likelihood versus pseudo-\nlog-likelihood (PLL). Each dot in the plots above depicts the ordinal correlation between the judgments of one participant\nand the predictions of one model. (a) The performance of BERT, RoBERTa, and ELECTRA in predicting the human judg-\nments of randomly sampled natural sentence pairs in the main experiment, using two different likelihood measures: our novel\napproximate likelihood method (i.e., averaging multiple conditional probability chains, see Methods) and pseudo-likelihood\n(PLL, summating the probability of each word given all of the other words [64]). For each model, we statistically com-\npared the two likelihood measures to each other and to the noise ceiling using a two-sided Wilcoxon signed-rank test across\nthe participants. False discovery rate was controlled at q < 0.05 for the 9 comparisons. When predicting human pref-\nerences of natural sentences, the pseudo-log-likelihood measure is at least as accurate as our proposed approximate\nlog-likelihood measure. (b) Results from a follow-up experiment, in which we synthesized synthetic sentence pairs for each\nof the model pairs, pitting the two alternative likelihood measures against each other. Statistical testing was conducted in the\nsame fashion as in panel a. These results indicate that for each of the three bidirectional language models, the approximate\nlog-likelihood measure is considerably and significantly (q < 0.05) more human-consistent than the pseudo-likelihood mea-\nsure. Synthetic controversial sentence pairs uncover a dramatic failure mode of the pseudo-log-likelihood measure,\nwhich remains covert when the evaluation is limited to randomly-sampled natural sentences. See Extended Data Table\n2 for synthetic sentence pair examples.\n27\n0\n25%\n50%\n75%\n100%\nhuman-choice prediction accuracy\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\nExtended Data Figure 6: Model prediction accuracy for pairs of\nnatural and synthetic sentences, evaluating each model across\nall of the sentence pairs in which it was targeted to rate the syn-\nthetic sentence to be less probable than the natural sentence.\nThe data binning applied here is complementary to the one used in\nFig. 3b, where each model was evaluated across all of the sentence\npairs in which it was targeted to rate the synthetic sentence to be\nat least as probable as the natural sentence. Unlike Fig. 3b, where\nall of the models performed poorly, here no models were found to\nbe significantly below the lower bound on the noise ceiling; typ-\nically, when a sentence was optimized to decrease its probability\nunder any model (despite the sentence probability not decreasing\nunder a second model), humans agreed that the sentence became\nless probable.\nsentence\nlog probability (model 1)\nlog probability (model 2)\n# human choices\nn: I always cover for him and make excuses.\nlog p(n|GPT-2) =−36.46\nlog p(n|2-gram) =−106.95\n10\ns: We either wish for it or ourselves do.\nlog p(s |GPT-2) =−36.15\nlog p(s |2-gram) =−122.28\n0\nn: This is why I will never understand boys.\nlog p(n|RoBERTa) =−46.88\nlog p(n|2-gram) =−103.11\n10\ns: This is why I will never kiss boys.\nlog p(s |RoBERTa) =−46.75\nlog p(s |2-gram) =−107.91\n0\nn: One of the ones I did required it.\nlog p(n|ELECTRA) =−35.97\nlog p(n|LSTM) =−40.89\n10\ns: Many of the years I did done so.\nlog p(s |ELECTRA) =−35.77\nlog p(s |LSTM) =−46.25\n0\nn: There were no guns in the Bronze Age.\nlog p(n|BERT) =−48.48\nlog p(n|ELECTRA) =−30.40\n10\ns: There is rich finds from the Bronze Age.\nlog p(s |BERT) =−48.46\nlog p(s |ELECTRA) =−44.34\n0\nn: You did a great job on cleaning them.\nlog p(n|XLM) =−40.38\nlog p(n|RNN) =−43.47\n10\ns: She did a great job at do me.\nlog p(s |XLM) =−39.89\nlog p(s |RNN) =−61.03\n0\nn: This logic has always seemed flawed to me.\nlog p(n|LSTM) =−39.77\nlog p(n|RNN) =−45.92\n10\ns: His cell has always seemed instinctively to me.\nlog p(s |LSTM) =−38.89\nlog p(s |RNN) =−62.81\n0\ns: Stand near the cafe and sip your coffee.\nlog p(s |RNN) =−65.55\nlog p(s |ELECTRA) =−34.46\n10\nn: Sit at the front and break your neck.\nlog p(n|RNN) =−44.18\nlog p(n|ELECTRA) =−34.65\n0\nn: Most of my jobs have been like this.\nlog p(n|3-gram) =−80.72\nlog p(n|LSTM) =−35.07\n10\ns: One of my boyfriend have been like this.\nlog p(s |3-gram) =−80.63\nlog p(s |LSTM) =−41.44\n0\nn: They even mentioned that I offer white flowers.\nlog p(n|2-gram) =−113.38\nlog p(n|BERT) =−62.81\n10\ns: But even fancied that would logically contradictory philosophies.\nlog p(s |2-gram) =−113.24\nlog p(s |BERT) =−117.98\n0\nExtended Data Table 1: Examples of pairs of synthetic and natural sentences that maximally contributed to each\nmodel’s prediction error. For each model (double row, “model 1”), the table shows results for two sentences on which the\nmodel failed severely. In each case, the failing model 1 prefers synthetic sentence s (higher log probability bolded), while the\nmodel it was pitted against (“model 2”) and all 10 human subjects presented with that sentence pair prefer natural sentence\nn. (When more than one sentence pair induced an equal maximal error in a model, the example included in the table was\nchosen at random.)\n28\nsentence\npseudo-log-likelihood (PLL)\napproximate log probability\n# human choices\ns1: I found so many in things and called.\nlog p(s1|BERT (PLL)) =−55.14\nlog p(s1|BERT) =−55.89\n30\ns2: Khrushchev schizophrenic so far\ndisproportionately goldfish fished alone.\nlog p(s2|BERT (PLL)) =−22.84\nlog p(s2|BERT) =−162.31\n0\ns1: Figures out if you are on the lead.\nlog p(s1|BERT (PLL)) =−38.11\nlog p(s1|BERT) =−51.27\n30\ns2: Neighbours unsatisfactory indistinguishable\nmisinterpreting schizophrenic on homecoming\ncheerleading.\nlog p(s2|BERT (PLL)) =−16.43\nlog p(s2|BERT) =−258.91\n0\ns1: I just say this and not the point.\nlog p(s1|ELECTRA (PLL)) =−34.41\nlog p(s1|ELECTRA) =−33.80\n30\ns2: Glastonbury reliably mobilize disenfranchised\nhomosexuals underestimate unhealthy skeptics.\nlog p(s2|ELECTRA (PLL)) =−11.81\nlog p(s2|ELECTRA) =−162.62\n0\ns1: And diplomacy is more people to the place.\nlog p(s1|ELECTRA (PLL)) =−62.81\nlog p(s1|ELECTRA) =−47.33\n30\ns2: Brezhnev ingenuity disembarking Acapulco\nmethamphetamine arthropods unaccompanied\nKhrushchev.\nlog p(s2|ELECTRA (PLL)) =−34.00\nlog p(s2|ELECTRA) =−230.97\n0\ns1: Sometimes what looks and feels real to you.\nlog p(s1|RoBERTa (PLL)) =−36.58\nlog p(s1|RoBERTa) =−51.61\n30\ns2: Buying something breathes or crawls\naesthetically to decorate.\nlog p(s2|RoBERTa (PLL)) =−9.78\nlog p(s2|RoBERTa) =−110.27\n0\ns1: In most other high priority packages were affected.\nlog p(s1|RoBERTa (PLL)) =−71.13\nlog p(s1|RoBERTa) =−61.60\n30\ns2: Stravinsky cupboard nanny contented burglar\nbabysitting unsupervised bathtub.\nlog p(s2|RoBERTa (PLL)) =−21.86\nlog p(s2|RoBERTa) =−164.70\n0\nExtended Data Table 2: Examples of controversial synthetic-sentence pairs that maximally contributed to the pre-\ndiction error of bidirectional transformers using pseudo-log-likelihood (PLL). For each bidirectional model, the table\ndisplays two sentence pairs on which the model failed severely when its prediction was based on pseudo-log-likelihood\n(PLL) estimates [64]. In each of these sentence pairs, the PLL estimate favors sentence s2 (higher PLL bolded), while the\napproximate log-likelihood estimate and most of the human subjects presented with that sentence pair preferred sentence\ns1. (When more than one sentence pair induced an equal maximal error in a model, the example included in the table was\nchosen at random.) Sentences with long, multi-token words (e.g., “methamphetamine”) have high PLL estimates since\neach of their tokens is well predicted by the others tokens. And yet, the entire sentence is improbable according to\nhuman judgments and approximate log-probability estimates based on proper conditional probability chains.\n29\n6\nSupplementary Methods\n6.1\nLanguage models\nN-gram models. N-gram models [68], the simplest language model class, are trained by counting the\nnumber of occurrences of all unique phrases of length N words in large text corpora. N-gram models make\npredictions about upcoming words by using empirical conditional probabilities in the training corpus. We\ntested both 2-gram and 3-gram variants. In 2-gram models, all unique two-word phrases are counted, and\neach upcoming word probability (probability of w2 conditioned on previous word w1) is determined by\ndividing the count of 2-gram w1, w2 by the count of unigram (word) w1. In 3-gram models, all unique three-\nword phrases are counted, and upcoming word probabilities (probability of w3 conditioned on previous\nwords w1 and w2) are determined by dividing the count of 3-gram w1, w2, w3 by the count of 2-gram\nw1, w2. In both such models, sentence probabilities can be computed as the product of all unidirectional\nword transition probabilities in a given sentence. We trained both the 2-gram and 3-gram models on a large\ncorpus composed of text from four sources: 1. public comments from the social media website Reddit\n(reddit.com) acquired using the public API at pushshift.io, 2. articles from Wikipedia, 3. English\nbooks and poetry available for free at Project Gutenberg (gutenberg.org), and 4. articles compiled in\nthe American Local News Corpus [69]. The n-gram probability estimates were regularized by means of\nKneser-Ney smoothing [70].\nRecurrent neural network models. We also tested two recurrent neural network models, including a\nsimple recurrent neural network (RNN) [1] and a more complex long short-term memory recurrent neural\nnetwork (LSTM) [2]. We trained both of these models on a next word prediction task using the same corpus\nused to train the n-gram models. Both the RNN and LSTM had a 256-feature embedding size and a 512-\nfeature hidden state size, and were trained over 100 independent batches of text for 50 epochs with a learning\nrate of .002. Both models’ training sets were tokenized into individual words and consisted of a vocabulary\nof 94,607 unique tokens.\nTransformer models. Similar to RNNs, transformers are designed to make predictions about sequential\ninputs. However, transformers do not use a recurrent architecture, and have a number of more complex\narchitectural features. For example, unlike the fixed token embeddings in classic RNNs, transformers utilize\ncontext-dependent embeddings that vary depending on a token’s position. Most transformers also contain\nmultiple attention heads in each layer of the model, which can help direct the model to relevant tokens in\ncomplex ways. We tested five models with varying architectures and training procedures, including BERT\n[3], RoBERTa [4], XLM [5], ELECTRA [6], and GPT-2 [7].\n• We used the large version of BERT (bi-directonal encoder representations from transformers), con-\ntaining 24 encoding layers, 1024 hidden units in the feedforward network element of the model, and\n16 attention heads. BERT is a bi-directional model trained to perform two different tasks: 1. a masked\nlanguage modeling (MLM) task, in which 15 percent of tokens are replaced with a special [MASK]\ntoken and BERT must predict the masked word, and 2. next sentence prediction (NSP), in which\nBERT aims to predict the upcoming sentence in the training corpus given the current sentence.\n• RoBERTa is also a bi-directional model that uses the same architecture as BERT. However, RoBERTa\nwas trained on exclusively the masked word prediction task (and not next sentence prediction), and\nused a different optimization procedure (including longer training on a larger dataset). This makes\nempirical comparisons between BERT and RoBERTa particularly interesting, because they differ only\nin training procedure and not architecture.\n30\n• XLM is a cross-lingual bi-directional model which, too, shares BERT’s original architecture. XLM is\ntrained on three different tasks: 1. the same MLM task used in both BERT and RoBERTa, 2. a causal\nlanguage modeling task where upcoming words are predicted from left to right, and 3. a translation\nmodeling task. On this task, each training example consists of the same text in two languages, and the\nmodel performs a masked language modeling task using context from one language to predict tokens\nof another. Such a task can help the XLM model become robust to idiosyncrasies of one particular\nlanguage that may not convey much linguistic information.\n• The ELECTRA model uses a training approach that involves two transformer models: a generator\nand a discriminator. While the generator performs a masked language modeling task similar to other\ntransformers, the discriminator simultaneously tries to figure out which masked tokens were replaced\nby the generator. This task may be more efficient than pure masked token prediction, because it uses\ninformation from all input tokens rather than only the masked subset.\n• GPT-2, the second iteration of GPT OpenAI’s GPT model, is the only unidirectional transformer\nmodel that we tested. We used the pretrained GPT-2-xl version, with 48 encoding layers and 25\nattention heads in each layer. Because GPT-2 is unidirectional it was trained only on the causal\nlanguage modeling task, in which tokens are predicted from left to right.\n6.2\nSelection of controversial natural-sentence pairs\nWe evaluated 231,725 eight-word sentences sampled from Reddit. Reddit comments were scraped from\nacross the entire website and all unique eight-word sentences were saved. These sentences were subse-\nquently filtered to exclude blatant spelling errors, inappropriate language, and individual words that were\nnot included in the corpus used to train the n-gram and recurrent neural network models in our experiment.\nWe estimated log p(s | m) for each natural sentence s and each model m as described above. We\nthen rank-transformed the sentence probabilities separately for each model, assigning the fractional rank\nr(s | m) = 0 to the least probable sentence according to model m and r(s | m) = 1 to the most probable\none. This step eliminated differences between models in terms of probability calibration.\nNext, we aimed to filter this corpus for controversial sentences. To prune the candidate sentences, we\neliminated any sentence s for which no pair of models m1, m2 held (r(s | m1) < 0.5) and (r(s | m2) ≥0.5),\nwhere r(s | m1) is the fractional rank assigned for sentence s by model m. This step ensured that all of the\nremaining sentences had a below-median probability according to one model and above-median probability\naccording to another, for at least one pair of models. We also excluded sentences in which any word (except\nfor prepositions) appeared more than once. After this pruning, 85,749 candidate sentences remained, from\nwhich\n\u000085749\n2\n\u0001\n≈3.67 × 109 possible sentence pairs can be formed.\nWe aimed to select 360 controversial sentence pairs, devoting 10 sentence pairs to each of the 36\nmodels pairs. First, we defined two 360-long integer vectors m1 and m2, specifying for each of the\n360 yet unselected sentence pairs which model pair they contrast. We then selected 360 sentence pairs\n\u0000s1\n1, s2\n1\n\u0001\n,\n\u0000s1\n2, s2\n2\n\u0001\n...,\n\u0000s1\n360, s2\n360\n\u0001\nby solving the following minimization problem:\n{(s1\nj\n∗, s2\nj\n∗) | j = 1, 2, ..360} = argmin\ns1,s2\nX\nj\n\u0000r(s1\nj|m1\nj) + r(s2\nj|m2\nj)\n\u0001\n(5)\nsubject to\n∀jr(s1\nj|m2\nj) ≥0.5\n(5a)\n∀jr(s2\nj|m1\nj) ≥0.5\n(5b)\nAll 720 sentences are unique.\n(5c)\n31\nTo achieve this, we used integer linear programming (ILP) as implemented by Gurobi [71]. We repre-\nsented sentence allocation as a sparse binary tensor S of dimensions 85,749 × 360 × 2 (sentences, trials,\npair members) and the fractional sentence probabilities ranks as a matrix R of dimensions 85,749 × 9 (sen-\ntences, models). This enabled us to express and solve the selection problem in Eq. 5 as a standard ILP\nproblem:\nS∗= argmin\nS\nX\ni,j\nSi,j,1Ri,m1\nj + Si,j,2Ri,m2\nj\n(6)\nsubject to\nSi,j,1Ri,m2\nj ≥0.5\n(6a)\nSi,j,2Ri,m1\nj ≥0.5\n(6b)\n∀i\nX\nj,k\nSi,j,k ≤1 (each sentence i is used only once in the experiment)\n(6c)\n∀j\nP\ni Si,j,1 = 1\n∀j\nP\ni Si,j,2 = 1\n)\n(each trial j is allocated exactly one sentence pair)\n(6d)\nS is binary\n(6e)\n6.3\nEvaluation of model-human consistency: Correlating model log-probability\nratios to human Likert ratings\nFor every model m and experimental trial i, we evaluated the log probability ratio for the trial’s two sen-\ntences:\nLR(s1\ni , s2\ni | m) = log p(s2\ni | m)\np(s1\ni | m)\n(7)\nThe human Likert ratings were recoded to be symmetrical around zero, mapping the six ratings appear-\ning in Extended Data Fig. 1 to (−2.5, −1.5, −0.5, +0.5, +1.5, +2.5). We then sought to correlate the model\nlog-ratios and with the zero-centered human Likert ratings, quantifying how well the model log-ratios were\nassociated with human sentence-likeliness judgments. To allow for an ordinal (not necessarily linear) asso-\nciation between the log-ratios and Likert ratings, we rank-transformed both measures (ranking within each\nmodel or each human) while retaining the sign of the values.\nFor each participant h:\nr(s1\ni , s2\ni | h) = sign(y0(s1\ni , s2\ni | h)) · R\n\u0000 \f\fy0(s1\ni , s2\ni | h)\n\f\f \u0001\n,\n(8)\nwhere y0(s1\ni , s2\ni | h)) is the zero-centered Likert rating provided by subject h for trial i and R(·) is rank\ntransform using random tie-breaking.\nFor each model m:\nr(s1\ni , s2\ni | m) = sign(LR(s1\ni , s2\ni | m)) · R\n\u0000 \f\fLR(s1\ni , s2\ni | m)\n\f\f \u0001\n,\n(9)\nA valid correlation measure of the model ranks and human ranks must be invariant to whether one\nsentence was presented on the left (s1) and the other on the right (s2), or vice versa. Changing the sentence\norder within a trial would flip the signs of both the log-ratio and the zero-centered Likert rating. Therefore,\nthe required correlation measure must be invariant to such coordinated sign flips, but not to flipping the sign\nof just one of the measures. Since cosine similarity maintains such invariance, we introduced signed-rank\n32\ncosine similarity, an ordinal analog of cosine similarity, substituting the raw data points for signed ranks (as\ndefined in Eq. 8-9):\nSCSR =\nP\ni r(s1\ni , s2\ni | m)r(s1\ni , s2\ni | h)\nqP\ni r(s1\ni , s2\ni | m)2\nqP\ni r(s1\ni , s2\ni | h)2\n.\n(10)\nTo eliminate the noise contributed by random tie-breaking, we used a closed-form expression of the\nexpected value of Eq. 10 over different random tie-breaking draws:\nE(SCSR) =\nP\ni E\n\u0000r(s1\ni , s2\ni | m)\n\u0001\nE\n\u0000r(s1\ni , s2\ni | h)\n\u0001\npPn\nk=1 k2pPn\nk=1 k2\n=\nP\ni ¯r(s1\ni , s2\ni | m)¯r(s1\ni , s2\ni | h)\nPn\nk=1 k2\n,\n(11)\nwhere ¯r(·) denotes signed rank with average-rank assigned to ties instead of random tie-breaking, and n\ndenotes the number of evaluated sentence pairs. The expected value of the product in the numerator is\nequal to the product of expected values of the factors since the random tie-breaking within each factor is\nindependent. The vector norms (the factors in the denominator) are constant since given no zero ratings,\neach signed-rank rating vector always includes one of each rank 1 to n (where n is the number of sentence\npairs considered), and the signs are eliminated by squaring. This derivation follows a classical result for\nSpearman’s ρ [72] (see [73], appendix C, for a modern treatment). We empirically confirmed that averaging\nSCSR as defined in Eq. 10 across a large number of random tie-breaking draws converges to E(SCSR) as\ndefined in Eq. 11. This latter expression (whose computation requires no actual random tie-breaking) was\nused to quantify the correlation between each participant and model.\nFor each participant, the lower bound on the noise ceiling was calculated by replacing the model-derived\npredictions with an across-participants average of the nine other participants’ signed-rank rating vectors. The\nlower bound plotted in main text Fig. 4 is an across-subject average of this estimate. An upper bound on the\nnoise ceiling was calculated as a dot product between the participant’s expected signed-rank rating vector\n(¯r/\npP k2) and a normalized, across-participants average of the expected signed-rank rating vectors of all\n10 participants.\nInference was conducted in the same fashion as that employed for the binarized judgments (Wilcoxon\nsigned-rank tests across the 10 subject groups, controlling for false discovery rate).\n7\nSupplementary Results\n7.1\nRandomly sampled natural-sentence pairs fail to adjudicate among mod-\nels\nAs a baseline, we created 90 pairs of natural sentence pairs by randomly sampling from a corpus of 8-word\nsentences appearing on Reddit. Evaluating the sentence probabilities assigned to the sentences by the dif-\nferent models, we found that models tended to agree on which of the two sentences was more probable\n(Extended Data Fig. 2). The between-model agreement rate ranged from 75.6% of the sentence pairs for\nGPT-2 vs. RNN to 93.3% for GPT-2 vs. RoBERTa, with an average agreement between models of 84.5%.\nMain text Fig. 1a (left-hand panel) provides a detailed graphical depiction of the relationship between sen-\ntence probability ranks for one model pair (GPT-2 and RoBERTa).\nWe divided these 90 pairs into 10 sets of nine sentences and presented each set to a separate group of 10\nsubjects. To evaluate model-human alignment, we computed the proportion of trials where the model and the\nparticipant agreed on which sentence was more probable. All of the nine language models performed above\n33\nchance (50% accuracy) in predicting the human choices for the randomly sampled natural sentence pairs\n(main text Fig. 1a, right-hand panel). Since we presented each group of 10 participants with a unique set of\nsentence pairs, we could statistically test between-model differences while accounting for both participants\nand sentence pairs as random factors by means of a simple two-sided Wilcoxon signed-rank test conducted\nacross the 10 participant groups. For the set of randomly sampled natural-sentence pairs, this test yielded\nno significant prediction accuracy differences between the candidate models (controlling for false discovery\nrate for all 36 model pairs at q < .05). This result is unsurprising considering the high level of between-\nmodel agreement on the sentence probability ranking within each of these sentence pairs.\nTo obtain an estimate of the noise ceiling [74] (i.e., the best possible prediction accuracy for this dataset),\nwe predicted each participant’s choices by the majority vote of the nine other participants who were pre-\nsented with the same trials. This measurement provided a lower bound on the noise ceiling. Including the\nparticipant’s own choice in the prediction yields an upper bound, since no set of predictions can be more\nhuman-aligned on average given the between-subject variability. For the randomly sampled natural sen-\ntences, none of the models were found to be significantly less accurate than the lower bound on the noise\nceiling (controlling the false discovery rate for all nine models at q < .05). In other words, the 900 trials\nof randomly sampled and paired natural sentences provided no statistical evidence that any of the language\nmodels are human-inconsistent.\n7.2\nComparing the accuracy of unnormalized and normalized sentence prob-\nability estimates\nPrevious studies (e.g., [13]) have found that normalizing language model sentence probability estimates by\nthe sentences’ token counts can result in greater alignment with human acceptability judgments. While we\ndeliberately used unnormalized sentence log probability when designing the experiment, we evaluated the\nprediction accuracy of each model under such normalizations through a control analysis.\nRather than predicting human judgments based only on the relative log probabilities of the two sen-\ntences, we instead used cross-validated logistic regression to predict human judgments using a combination\nof unnormalized log probability differences (“LP”) and two measures from Lau and colleagues [13] that\nincorporate information about the token counts in each sentence. The “MeanLP” measure normalized each\nsentence’s log probability by its token count T, whereas the “PenLP” measure divided each sentence’s log\nprobability by a dampened version of its token count,\n\u0000(5 + T)/(5 + 1)\n\u00010.8. For models trained on whole\nwords (LSTM, RNN, 3-gram, and 2-gram), we used character count instead of token count.\nFor each language model m, we fitted a separate logistic regression to predict the individual binarized\nsentence choices across the entire main experiment dataset by weighing the three predictors “LP,” “MeanLP,”\nand “PenLP.” We did not include an intercept due to the symmetry of the prediction task (the presentation\nof sentences as sentence 1 or 2 was randomized). We cross-validated the logistic regression’s accuracy by\nleaving out one sentence pair at a time, using data from all conditions of the experiment.\nTaking token count into consideration led to minor improvements in the prediction accuracy of most\nmodels (an average improvement of 0.95%), but this adjustment did not change the hierarchy of the models\nin terms of their human consistency (Supplementary Fig. S1). We hypothesize that the greater disparities\nbetween unnormalized and normalized probability measures, as observed by Lau and colleagues [13] com-\npared to those found in our study, may be attributed to their experiment involving sentences of markedly\ndifferent lengths.\n34\n7.3\nModels differ in their sensitivity to low-level linguistic features\nWhile the controversial sentences presented in this study were synthesized without consideration for partic-\nular linguistic features, we performed a post hoc analysis to explore the contribution of different features\nto model and human preferences (Supplementary Fig. S3). For each controversial synthetic sentence pair,\nwe computed the average log-transformed word frequency for each sentence (extracted from the publicly\navailable subtlex database [61]). We also computed the average pairwise correlation between semantic\nGloVe vector representations [75] of all eight words, based on neuroimaging research showing that there\nare specific neural signatures evoked by dissimilarity in semantic vectors [10, 76]. We performed paired\nsample t-tests across sentence pairs between the linguistic feature preferences for models vs. humans, and\nfound that GPT-2, LSTM, RNN, 3-gram, and 2-gram models were significantly more likely (vs. humans)\nto prefer sentences with low GloVe correlations, while ELECTRA was significantly more likely to prefer\nhigh GloVe correlations (controlling the false discovery rate for all nine models at q < .05). For word fre-\nquency, the RNN, 3-gram, and 2-gram models were significantly biased (vs. humans) to prefer sentences\nwith low-frequency words, while ELECTRA and XLM showed a significant bias for high-frequency words.\nThese results indicate that even strong models like GPT-2 and ELECTRA can exhibit subtle misalignments\nwith humans in their response to simple linguistic features, when evaluated on sentences synthesized to be\ncontroversial.\n35\n8\nSupplementary Figures\n0\n25%\n50%\n75%\n100%\nhuman-choice prediction accuracy\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\na\n0\n25%\n50%\n75%\n100%\nhuman-choice prediction accuracy\nGPT-2\nRoBERTa\nELECTRA\nBERT\nXLM\nLSTM\nRNN\n3-gram\n2-gram\nb\nUnnormalized sentence probability estimates\nNormalized sentence probability estimates\nSupplementary Fig. S1: The predictivity of normalized and unnormalized log-probability measures.\n(a) Predicting human judgments from all conditions using only unnormalized log probability differences\n(equivalent to Fig. 4 in the main text, except using binarized accuracy as a dependent measure). (b) Binarized\naccuracy of the logistic regression optimally combining LP, MeanLP, and PenLP for each language model.\nRelative model performance is nearly identical in these two analyses, indicating that tokenization differences\nacross models did not play a large confounding role in our main results.\n36\nSupplementary Fig. S2: The task instructions provided to the participants at the beginning of the experimen-\ntal session.\n37\na\nb\nSupplementary Fig. S3: Linguistic feature values for synthetic sentence pairs. (a) GloVe correlation\nvalues of the preferred and rejected sentence for each synthetic sentence pair. Each panel depicts preferences\nfor both humans (red) and a specific model (black), for sentence pairs that this model was involved in\nsynthesizing. Black sub-panel outlines indicate significant differences between the preferences of models\nand humans on that particular set of sentence pairs, according to a paired sample t-test (controlling for false\ndiscovery rate across all nine models at q < .05). (b) Same as (a), but for average log-transformed word\nfrequency.\n38\n9\nSupplementary Tables\nmodel\naccepted sentence\nhas more tokens\nequal\ntoken-counts\nrejected sentence\nhas more tokens\np-value\nGPT-2\n24\n13\n3\n<0.0001\nRoBERTa\n6\n18\n16\n0.0656\nELECTRA\n12\n21\n7\n0.3593\nBERT\n4\n8\n28\n<0.0001\nXLM\n2\n16\n22\n<0.0001\nSupplementary Table S1: Token count control analysis. For each transformer model, we considered syn-\nthetic controversial sentence pairs where the other targeted model was also a transformer (a total of 40\nsentence pairs per model). For each such pair, we evaluated the token count of the synthetic sentence to\nwhich the model assigned a higher probability (“accepted sentence”) and the token count of the synthetic\nsentence to which the model assigned a lower probability (“rejected sentence”). For each model, this table\npresents the number of sentence pairs in which the accepted sentence had a higher token count, both sen-\ntences had an equal number of tokens, and the rejected sentence had a higher token count. We compared the\nprevalence of higher token counts in accepted and rejected sentences using a binomial test (H0 : p = 0.5)\ncontrolled for False Discovery Rate across five comparisons.\nGPT-2 assigned significantly more tokens to accepted sentences, whereas BERT and XLM assigned sig-\nnificantly more tokens to rejected sentences. For RoBeRTa and ELECTRA, no significant difference was\nfound. Note that since the controversial sentences are driven by relative model response properties, a sig-\nnificant difference for a particular model does not necessarily indicate that token count biases the model’s\nsentence probability estimates. For example, GPT-2’s apparent preference for sentences with a greater token\ncount might reflect biases of the alternative models pitted against GPT-2. These models might prefer shorter\nsentences that exhibit undetected grammatical or semantic violations over longer but felicitous sentences.\nOverall, these results indicate that while certain models’ probability estimates might be biased by to-\nkenization, lower sentence probabilities were not systematically confounded by higher token counts.\n39\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "q-bio.NC"
  ],
  "published": "2022-04-07",
  "updated": "2023-09-12"
}