{
  "id": "http://arxiv.org/abs/2204.03905v2",
  "title": "BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model",
  "authors": [
    "Hongyi Yuan",
    "Zheng Yuan",
    "Ruyi Gan",
    "Jiaxing Zhang",
    "Yutao Xie",
    "Sheng Yu"
  ],
  "abstract": "Pretrained language models have served as important backbones for natural\nlanguage processing. Recently, in-domain pretraining has been shown to benefit\nvarious domain-specific downstream tasks. In the biomedical domain, natural\nlanguage generation (NLG) tasks are of critical importance, while understudied.\nApproaching natural language understanding (NLU) tasks as NLG achieves\nsatisfying performance in the general domain through constrained language\ngeneration or language prompting. We emphasize the lack of in-domain generative\nlanguage models and the unsystematic generative downstream benchmarks in the\nbiomedical domain, hindering the development of the research community. In this\nwork, we introduce the generative language model BioBART that adapts BART to\nthe biomedical domain. We collate various biomedical language generation tasks\nincluding dialogue, summarization, entity linking, and named entity\nrecognition. BioBART pretrained on PubMed abstracts has enhanced performance\ncompared to BART and set strong baselines on several tasks. Furthermore, we\nconduct ablation studies on the pretraining tasks for BioBART and find that\nsentence permutation has negative effects on downstream tasks.",
  "text": "BioBART: Pretraining and Evaluation of\nA Biomedical Generative Language Model\nHongyi Yuan1 ∗Zheng Yuan1 ∗Ruyi Gan2 Jiaxing Zhang2 Yutao Xie2 Sheng Yu1 †\n1Tsinghua University\n2International Digital Economy Academy\n{yuanhy20,yuanz17}@mails.tsinghua.edu.cn\n{ganruyi,zhangjiaxing,xieyutao}@idea.edu.cn\nsyu@tsinghua.edu.cn\nAbstract\nPretrained language models have served as im-\nportant backbones for natural language pro-\ncessing.\nRecently, in-domain pretraining\nhas been shown to beneﬁt various domain-\nspeciﬁc downstream tasks. In the biomedical\ndomain, natural language generation (NLG)\ntasks are of critical importance, while under-\nstudied.\nApproaching natural language un-\nderstanding (NLU) tasks as NLG achieves\nsatisfying performance in the general do-\nmain through constrained language generation\nor language prompting.\nWe emphasize the\nlack of in-domain generative language models\nand the unsystematic generative downstream\nbenchmarks in the biomedical domain, hinder-\ning the development of the research commu-\nnity. In this work, we introduce the generative\nlanguage model BioBART that adapts BART\nto the biomedical domain. We collate various\nbiomedical language generation tasks includ-\ning dialogue, summarization, entity linking,\nand named entity recognition. BioBART pre-\ntrained on PubMed abstracts has enhanced per-\nformance compared to BART and set strong\nbaselines on several tasks. Furthermore, we\nconduct ablation studies on the pretraining\ntasks for BioBART and ﬁnd that sentence per-\nmutation has negative effects on downstream\ntasks.\n1\nIntroduction\nSince the advent of ELMo (Peters et al., 2018) and\nBERT (Devlin et al., 2019), the new pretrain-then-\nﬁnetune paradigm has brought great performance\nimprovement and dominated the methodology re-\nsearch of the natural language processing (NLP)\nﬁeld. Previous research has illustrated that pre-\ntraining language models on the domain-speciﬁc\ncorpora can improve the model performance on\ndomain-speciﬁc tasks further (Gururangan et al.,\n2020). With the large-scale publicly accessible\n∗Contributed equally.\n† Corresponded author.\ncorpora from PubMed, researchers have already\nproposed biomedical domain pretrained language\nmodels such as BioBERT (Lee et al., 2020) and\nPubMedBERT (Gu et al., 2022) to aid the later\nresearch.\nNatural language generation (NLG) tasks such\nas dialogue system (Chao et al., 2017) and ques-\ntion answering (Jin et al., 2022) are of critical im-\nportance for the biomedical artiﬁcial intelligence\nresearch, and there is also a trend to approach nat-\nural language understanding as NLG tasks in the\ngeneral domain (Sun et al., 2021; Yan et al., 2021).\nFor example, an entity retrieval task can be solved\nby constrained natural language generation (Cao\net al., 2021). However, there exist two gaps in\nthe research of the biomedical NLG. On the one\nhand, the architectures of the biomedical pretrained\nlanguage models are almost all encoder-only trans-\nformers. Such architecture is incapable of generat-\ning natural languages auto-regressively. A decoder\nis necessary for language generation (Liu and La-\npata, 2019). On the other hand, there are very\nfew in-domain generative language models for bio-\nmedicine (Phan et al., 2021). Models pretrained\non biomedical corpora may further enhance the\nperformance of current biomedical NLG methods.\nTo bridge the gaps mentioned above, we propose\na biomedical auto-regressive generative language\nmodel, BioBART, pretrained on the biomedical\ncorpora. In our work, we adopt BART (Bidirec-\ntional and Auto-Regressive Transformers), a gen-\nerative pretrained language model which achieves\nstate-of-the-art results on different NLG tasks in\nthe general domain (Lewis et al., 2020a). We con-\ntinuously pretrain BART on PubMed abstracts to\nachieve biomedical domain adaption only using the\ntext-inﬁlling task. We also collate and evaluate Bio-\nBART on the existing biomedical NLG tasks. The\nin-domain BioBART outperforms BART model\nand sets strong baselines for several NLG tasks.\nThe main contributions of our work are summa-\narXiv:2204.03905v2  [cs.CL]  22 Apr 2022\nrized as follows1:\n1. In aid of the research concerning the biomedi-\ncal NLG tasks, we collate existing biomedical\nNLG tasks along with corresponding data and\nexperimental settings. The archived biomedi-\ncal tasks will be released.\n2. We further analyze the inﬂuence of the\npretraining task of sentence permutation in\nBART, and we ﬁnd it brings degradation on\nthe biomedical NLG tasks.\n3. We evaluate our BioBART models on various\nNLG tasks and demonstrate the superb perfor-\nmance over BART. We will release the codes\nand weights to help reproduce our results.\n2\nRelated Work\n2.1\nAuto-regressive Language Model\nMost of the prestigious language models such\nas BERT, RoBERTa (Liu et al., 2019) are auto-\nencoding transformers. The encoder-only archi-\ntecture prevents the direct implementation of the\nseq2seq language generation. Several generative\nauto-regressive language models are proposed to\nmitigate the problem.\nThe serial GPT models\n(Radford and Narasimhan, 2018; Radford et al.,\n2019; Brown et al., 2020) adopt the decoder-only\ntransformer architecture which is a left-to-right lan-\nguage model. They pretrain the models by auto-\nregressively predicting the upcoming word of sen-\ntences. UniLM1 (Dong et al., 2019) and UniLM2\n(Bao et al., 2020) implement attention masks to\nthe transformer encoder to achieve unidirectional\nlanguage modeling. They pretrain their models\nwith a mixture of masked language modeling and\nauto-regressive language generation. T5 (Raffel\net al., 2020) and BART (Lewis et al., 2020a) ap-\nply the full transformer architecture, the encoder is\nused for input sequence encoding and the decoder\nis used for language generation. T5 and BART are\nboth pretrained by denoising the corrupted corpora.\nSuch models achieve many state-of-the-art results\non various NLG tasks and some NLU tasks.\n2.2\nBiomedical Domain Pretraining\nExisting work has shown that pretraining the lan-\nguage models on the domain-speciﬁc corpora can\n1Our codes and pretrained checkpoints can be found at\nhttps://github.com/GanjinZero/BioBART.\nbring better model transferability on the corre-\nsponding downstream tasks (Gururangan et al.,\n2020).\nThere are endeavors to adapt language\nmodels to the speciﬁc domain. BioBERT (Lee\net al., 2020) pretrained BERT model using biomed-\nical corpora from PubMed abstracts and PubMed\nCentral (PMC) full-text articles. BlueBERT (Peng\net al., 2020) and clinicalBERT (Alsentzer et al.,\n2019) add electronic medical record (EMR) cor-\npora from MIMIC-III (Johnson et al., 2016) to\nthe pretraining data. Instead of continuous train-\ning from the general BERT checkpoint, SciBERT\n(Beltagy et al., 2019) and PubMedBERT (Gu et al.,\n2022) are trained from scratch using scientiﬁc pa-\npers from Semantic Scholar (Ammar et al., 2018)\nand PubMed articles respectively.\n(Shin et al.,\n2020) releases BioMegatron, a larger-size BERT-\nstyle language model pretrained on PubMed ab-\nstracts, PMC and MIMIC-III. The aforementioned\nwork all use the model architecture of BERT. Other\nresearchers are exploring different language mod-\nels.\nBioELMo (Jin et al., 2019) is pretrained on\nbiomedical corpora based on stacked bidirectional\nLSTM language model ELMo (Peters et al., 2018).\nBioELECTRA (Kanakarajan et al., 2021) applies\nan adversarial training scheme consisting of a dis-\ncriminator and a generator. They use PubMed ab-\nstracts and PMC articles as in-domain pretraining\ncorpora. BioMed-RoBERTa (Gururangan et al.,\n2020) is initialized from RoBERTa (Liu et al.,\n2019), with additional training on the scientiﬁc pa-\npers from Semantic Scholar. Bio-lm (Lewis et al.,\n2020b) is pretrained on data from PubMed, PMC,\nand MIMIC-III based on the RoBERTa model. Ke-\nBioLM (Yuan et al., 2021) uses Entity as Experts\n(Févry et al., 2020) model to inject biomedical en-\ntity knowledge into the language model, starting\nfrom the weights of PubMedBERT. Coder (Yuan\net al., 2022b) and SapBERT (Liu et al., 2021) take\nadvantage of the synonyms resource from biomed-\nical knowledge base UMLS (Bodenreider, 2004)\nand enhance the model with entity knowledge by\ncontrastive pretraining.\nDue to the nature of model architecture, encoder-\nonly language models have limited performance on\nthe NLG tasks, such as summarization and question\nanswering. In recent research, SciFive (Phan et al.,\n2021) is proposed for biomedical NLP tasks. Sci-\nFive is pretrained on PubMed abstracts and PMC\narticles based on T5 architecture. While T5 is avail-\nable for NLG tasks, SciFive is focused on evaluat-\ning NLU tasks. Compared to SciFive, we choose\nto use BART as our model backbone and evalu-\nate more on NLG tasks to leverage the power of\ndecoders.\n2.3\nBiomedical Natural Language\nGeneration\nIn the biomedical domain, most of the NLP tasks\nare natural language understanding (NLU) tasks.\nThere are well-archived benchmarks for the evalua-\ntion of biomedical NLU, such as BLUE (Gu et al.,\n2022) and CBLUE (Zhang et al., 2021). NLG tasks\nare relatively less studied. (Ju et al., 2020) collects\nthe patients and doctors’ dialogues and forms a\nbenchmark for Covid-19 related dialogue system.\n(Ben Abacha et al., 2021) is an annual biomedical\nNLP competition containing NLG tasks such as\nmedical question (or answer) summarization and\nﬁgure captions.\nMoreover, with the success of GPT-3, there is a\nnovel trend that uniﬁes all the NLP tasks as NLG\ntasks (McCann et al., 2018; Brown et al., 2020).\nThe traditional NLU tasks can be approached by\nconstrained language generation. Much attention\nis paid on the NLG methods recently.\nIn the\nbiomedical domain, entities are of primary concern.\nGENRE (Cao et al., 2021), Yuan et al. (2022a) and\nBARTNER (Yan et al., 2021) reach the new state-\nof-the-art by auto-regressive language model on\nentity linking and named entity recognition tasks.\nSuch methods can be adapted to the biomedical\ndomain.\n3\nBiomedical Domain Pretraining\nBART is a sequence-to-sequence model with a\nbi-directional encoder and a left-to-right auto-\nregressive decoder. The model architecture is con-\nsistent with the Transformers (Vaswani et al., 2017)\nexcept for changing the ReLU activation functions\nto GeLUs (Hendrycks and Gimpel, 2016). BART\nis pretrained by denoising the corrupted input doc-\numents. The work ablates ﬁve different types of\ncorruption noise: text masking, text deletion, text\ninﬁlling, sentence permutation, and document ro-\ntation. As a result, the pretraining documents are\ncorrupted in two ways: 1) Text Inﬁlling: For each\ndocument, a number of token spans are sampled,\nand each sample span is replaced with a single\nmask token. 2) Sentence Permutation: A docu-\nment is split into sentences and sentences are shuf-\nﬂed in random orders. The pretraining objective\nis to minimize the negative log-likelihood of the\noriginal documents.\nPrior work has shown that continuous-pretrained\nmodels can get competitive results compared with\nthose trained from scratch (Gu et al., 2022). In\nour work, we continuously pretrain BART on the\nbiomedical domain corpora. We revisit the methods\nto corrupt input texts. BART keeps the sentence\npermutation noise because of the signiﬁcant perfor-\nmance gain on the summarization task, although\nthis noise may lead to slight degradation on other\ntasks. We run further ablation studies on various\nbiomedical NLG tasks. We show that the model\npretrained without sentence permutation has better\nperformance. Further details are listed in Section\n5.5. Therefore we only implement the text inﬁlling\ntask to corrupt input texts for pretraining BioBART.\n4\nGenerative Downstream Task\nIn this section, we introduce the generative down-\nstream tasks in the biomedical domain. We will\nconduct experiments on these tasks to illustrate the\nperformance of the domain-speciﬁc BioBART.\n4.1\nDialogue System\nA medical dialogue system aims to imitate the hu-\nman doctor to communicate with human patients in\na natural way. Based on the BART-style model, the\npatients’ primitive descriptions and dialogue histo-\nries are used as inputs to the model, then the model\nauto-regressively generates the replies as outputs.\nThe task is trained and evaluated in a sequence-to-\nsequence fashion.\n4.2\nAbstractive Summarization\nSummarization is a classical NLP task.\nIt is\nimportant for healthcare to concisely summarize\nknowledge-rich biomedical documents.\nTech-\nnically, there are abstractive and extractive ap-\nproaches to generate better summaries. With the\nhelp of large pretrained language models, abstrac-\ntive summarization methods outperform extractive\nmethods in summary diversity and conciseness\n(Zhang et al., 2020a; Dou et al., 2021). The ab-\nstractive summarization is naturally an NLG task.\nWe follow the BART (Lewis et al., 2020a) work\nand evaluate our BioBART on the biomedical sum-\nmarization tasks in the same fashion. The input\ndocuments are encoded by the model encoder and\nthe summaries are generated by the decoder auto-\nregressively.\n4.3\nEntity Linking\nEntity linking is a task that maps entity mentions in\ntexts to its standard entity concepts. Traditional en-\ntity linking methods use language models to encode\nentity concepts from knowledge bases(e.g. UMLS)\nand mentions into the same dense space and disam-\nbiguate mentions by vector similarity. The large\nmemory footprint requirements and difﬁcult model\ntraining hinder the development of such methods.\nCao et al. (2021) proposes GENRE which uses\ngenerative language models to disambiguate en-\ntity mentions by auto-regressively generating the\nstandard concept names conditioned on the inputs.\n(Yuan et al., 2022a) achieves state-of-the-art entity\nlinking performance on various biomedical entity\nlinking datasets by generative methods. We include\nthis leading-edge method to show the superior per-\nformance of BioBART.\n4.4\nNamed Entity Recognition\nNamed entity recognition (NER) is a critical task\nin the biomedical NLP community which extracts\nbiomedical-related entities from texts. Nested and\ndiscontinuous entities widely exist in biomedical\npapers and EMR due to the multi-granularity se-\nmantic meanings and complex syntax structures\n(Yuan et al., 2020). Well-used sequential labelling\nframework in NER (Lample et al., 2016) is not\ndirectly ﬁtted for nested and discontinuous NER\n(Finkel and Manning, 2009). Yan et al. (2021)\npropose BARTNER to model nested and discontin-\nuous NER into seq2seq task by inputting sentences\nand outputting entities with their entity types one\nby one. The generative approach of BARTNER\nachieves state-of-the-art performance on nested and\ndiscontinuous NER datasets, and we will use it to\nevaluate our proposed BioBART can further en-\nhance the performance.\n5\nExperiments\n5.1\nPretraining\nPretraining\nCorpora\nThere\nare\ntwo\nmain\nsources of biomedical corpora: PubMed abstracts,\nPMC articles. In the prior work (Gu et al., 2022),\ntraining on both corpora surprisingly leads to a\nslight degradation in performance compared to\nsolely training on PubMed abstracts. Therefore, we\nonly use PubMed abstracts as the pretraining cor-\npora. The corpora contain about 41 GB of biomed-\nical research paper abstracts on PubMed.\nPretraining Setup\nWe continuously pretrain\nboth large and base versions of BART for 120k\nsteps with a batch size of 2560. We use the same\nvocabulary as BART to tokenize the texts. Al-\nthough the input length limitation of BART is 1024,\nthe tokenized PubMed abstracts rarely exceed 512.\nTherefore, for the sake of training efﬁciency, we\ntruncate all the input texts to 512 maximum length.\nWe mask 30% of the input tokens and the masked\nspan length is determined by sampling from a Pois-\nson distribution (λ = 3) as used in BART. We use\na learning rate scheduler of 0.02 warm-up ratio\nand linear decay. The learning rate is set to 1e-4.\nWe train the base version of BioBART on 2 DGX\nwith 16 40GB A100 GPUs for about 100 hours and\nthe large version of BioBART on the same devices\nfor 168 hours with the help of the open-resource\nframework DeepSpeed (Rajbhandari et al., 2020).\n5.2\nDataset for Downstream Task\n5.2.1\nDialogue System\nCovidDialog\n(Ju et al., 2020) Concerning the\nwidespread Coronavirus disease 2019 (COVID-19)\npandemic, the CovidDialog dataset is proposed to\nfacilitate the development of dialogue system pro-\nviding COVID-related consultations to people. The\ndataset is collected from online healthcare forums.\nIt contains 603 consultations about COVID-19 and\nother related pneumonia, having 1232 utterances in\ntotal. Each consultation starts with a description re-\nlated to patients’ medical conditions, then followed\nthe conversation between a doctor and a patient.\n5.2.2\nAbstractive Summarization\niCliniq, HealthCareMagic\nBoth datasets are\nextracted from MedDialog (Zeng et al., 2020)\ndataset, collected from the online healthcare plat-\nform. iCliniq contains 31,062 samples and Health-\nCareMagic contains 226,405 samples. Each sam-\nple is comprised of a summary and corresponding\ndialogues between a patient and a doctor. Health-\nCareMagic’s summaries are more abstractive and\nare written in a formal style, unlike iCliniq’s\npatient-written summaries. We follow the previous\nwork (Mrini et al., 2021) for training, developing,\nand testing data separations of both datasets.\nMeQSum\n(Ben Abacha and Demner-Fushman,\n2019) The dataset is created for better medical ques-\ntion summarization because the original patients’\nTask\nDataset\nTrain\nDev\nTest\nDataset\nTrain\nDev\nTest\nMetric\nDialogue\nCovidDialog\n490\n63\n61\nRouge,BERTscore,\nBLEU\nSummarization\nMeQSum\n500\n-\n500\nMEDIQA-ANS\n38,166\n174\n552\nRouge, BERTscore\niCliniq\n24,851\n3,105\n3,108\nMEDIQA-QS\n1,000\n50\n100\nHealthCareMagic\n181,122\n22,641\n22,642\nMEDIQA-MAS\n1,104\n50\n80\nEntity Linking\nMedMentions\n122,241\n40,884\n40,157\nNCBI\n5,784\n787\n960\nRecall@1,@5\nBC5CDR\n9,285\n9,515\n9,654\nCOMETA\n13,489\n2,176\n4,350\nAskAPatients\n16,826\n1,663\n1,712\nNER\nShARe13\n5,146\n669\n5,333\nShARe14\n10,380\n771\n7,922\nEntity-level F1 score\nCADEC\n4,430\n898\n990\nGENIA\n50,509\n-\n5,506\nTable 1: The statistics of the datasets for biomedical generative tasks. The counts for NER are entity counts.\nCovid19-Dialogue\nModel\nRouge-1\nRouge-2\nRouge-L\nBLEU\nBERTscore\nBART BASE\n27.24\n12.31\n25.66\n10.36\n0.852\nBioBART BASE\n28.14\n12.77\n26.32\n11.40\n0.849\nBART LARGE\n29.02\n12.08\n26.93\n10.96\n0.852\nBioBART LARGE\n28.81\n13.79\n26.96\n12.05\n0.850\nState-of-the-art\n-\n-\n-\n7.60\n-\nSource\n-\n-\n-\n(Zhou et al., 2021)\n-\nTable 2: The main results on Dialogue System task.\nquestions are verbose, causing difﬁculty for the\nquestion-answering system. The dataset contains\n1000 patients’ health questions selected from a col-\nlection distributed by the U.S. National Library of\nMedicine (Kilicoglu et al., 2018). Each question is\nannotated with a question summarization by medi-\ncal experts.\nMEDIQA-ANS\n(Savery et al., 2020) When feel-\ning discomfort, people may turn to the internet for\nthe answers to their medical questions. The raw\nsearching result may be obscure for even medical\nexperts. The dataset is proposed to emphasize the\nneed for a medical answer summarization system\nin aid of better understanding biomedical materials.\nIt consists of 156 health questions, corresponding\nanswers to these questions, and expert-created sum-\nmaries (both abstractive and extractive) of these\nanswers. Following the paper, we use BioASQ\n(Tsatsaronis et al., 2015) to construct training data,\nMedInfo (Abacha et al., 2019) for validation, and\nthe whole MEDIQA-ANS dataset for testing.\nMEDIQA-QS, MEDIQA-MAS\nBoth datasets\nare derived from the MEDIQA 2021 Tasks\n(Ben Abacha et al., 2021). MEDIQA-QS dataset\naims to incentivize the development of new sum-\nmarization approaches that address speciﬁcally the\nchallenges of long and complex health questions.\nThe dataset provides the validation and test sets,\nand MeQSum dataset is used as the training set.\nMEDIQA-MAS aims to prompt research that si-\nmultaneously aggregates and summarize the differ-\nent relevant answers to a medical question. This\ndataset provides the validation and test sets, and\nMEDIQA-ANS dataset comprises the training set.\n5.2.3\nEntity Linking\nMedMentions\n(Mohan and Li, 2019) MedMen-\ntions is a large-scale biomedical entity recognition\ndataset. The commonly used St21pv subset con-\ntains 4,392 PubMed abstracts, and over 350,000\nmentions are linked to concepts of 21 selected se-\nmantic types in UMLS (Bodenreider, 2004).\nBC5CDR\n(Li et al., 2016) BC5CDR is a bench-\nmark for biomedical entity linking. 1500 PubMed\narticle abstracts are annotated with 4409 chemicals,\n5818 diseases entities, and 3116 chemical-disease\ninteractions. MeSH ontology, a subset of UMLS\nis used to annotate entities. We follow most recent\nwork (Angell et al., 2021; Varma et al., 2021) for\ndata pre-processing.\nNCBI\n(Do˘gan et al., 2014) The dataset is built\nfrom 793 PubMed abstracts. It consists of 6892\nannotated disease mentions of 790 unique disease\nconcepts. The annotators label all the mentions to\nconcepts in MEDIC ontology (Davis et al., 2012).\nMEDIC is a medical dictionary that merges the\ndiseases concepts, synonyms, and deﬁnitions in\nMeSH and OMIM and is composed of 9700 unique\ndiseases. We follow BioSyn (Sung et al., 2020) to\nprocess data and construct dataset splits.\niCliniq\nHealthCareMagic\nMEDIQA-QS\nModel\nRouge-1/2/L\nBERTscore\nRouge-1/2/L\nBERTscore\nRouge-1/2/L\nBERTscore\nBART BASE\n61.43/48.68/59.71\n0.941\n46.81/26.19/44.34\n0.918\n28.82/10.99/26.99\n0.896\nBioBART BASE\n61.07/48.47/59.42\n0.941\n46.67/26.03/44.11\n0.918\n30.12/11.28/27.44\n0.898\nBART LARGE\n59.87/47.01/58.12\n0.938\n47.24/26.54/44.68\n0.919\n29.97/10.64/28.41\n0.901\nBioBART LARGE\n60.32/47.98/58.69\n0.940\n46.54/26.14/44.23\n0.919\n31.97/12.39/29.70\n0.903\nState-of-the-art\n62.3/48.7/58.5\n-\n46.9/24.8/43.2\n-\n35.14/16.08/31.31\n-\nSource\n(Mrini et al., 2021)\n(Mrini et al., 2021)\n(Ben Abacha et al., 2021)\nMEDIQA-MAS\nMEDIQA-ANS(Pages)\nMeQSum\nModel\nRouge-1/2/L\nBERTscore\nRouge-1/2/L\nBERTscore\nRouge-1/2/L\nBERTscore\nBART BASE\n31.63/9.98/27.85\n0.859\n19.10/6.77/16.90\n0.851\n52.93/35.79/50.46\n0.927\nBioBART BASE\n32.90/11.28/29.26\n0.861\n18.97/7.46/16.77\n0.850\n53.75/36.50/51.27\n0.929\nBART LARGE\n29.32/9.00/26.14\n0.857\n21.52/9.31/19.15\n0.853\n53.68/36.80/51.05\n0.928\nBioBART LARGE\n30.60/10.37/27.04\n0.861\n21.58/9.34/19.18\n0.857\n55.61/38.11/53.15\n0.933\nState-of-the-art\n32.15/16.21/19.10\n-\n23.07/ 5.41/15.35\n-\n54.5/37.9/50.2\n-\nSource\n(Ben Abacha et al., 2021)\n(Laskar et al., 2021)\n(Mrini et al., 2021)\nTable 3: The main results on Summarization tasks.\nMedMentions\nBC5CDR\nNCBI\nCOMETA\nAAP\nModel\nRecall@1/@5\nRecall@1/@5\nRecall@1/@5\nRecall@1/@5\nRecall@1/@5\nBART BASE\n69.77/84.59\n91.56/94.89\n88.54/95.31\n78.34/87.40\n86.37/94.29\nBioBART BASE\n71.15/86.22\n93.01/95.59\n89.27/95.31\n79.63/88.64\n87.51/94.92\nBART LARGE\n71.49/84.95\n92.48/95.26\n90.21/95.52\n80.70/88.65\n88.79/96.59\nBioBART LARGE\n71.78/85.42\n93.26/95.74\n89.90/95.63\n81.77/88.87\n89.40/95.76\nState-of-the-art\n74.6/ -\n91.9/ -\n92.4/ -\n80.1/ -\n89.0/ -\nSource\n(Varma et al., 2021)\n(Varma et al., 2021)\n(Lai et al., 2021)\n(Lai et al., 2021)\n(Liu et al., 2021)\nTable 4: The main results on Entity Linking tasks.\nShARe13\nShARe14\nCADEC\nGENIA\nModel\nF1\nF1\nF1\nF1\nBART BASE\n76.63\n77.87\n68.37\n78.06\nBioBART BASE\n78.78\n79.17\n68.39\n78.43\nBART LARGE\n79.69\n80.34\n70.64\n78.93\nBioBART LARGE\n80.75\n80.41\n70.53\n79.93\nState-of-the-art\n82.52\n81.75\n73.21\n81.39\nSource\n(Li et al., 2021)\nTable 5: The main result on NER tasks.\nCOMETA\n(Basaldella et al., 2020) COMETA\nis derived from the online publicly available and\nanonymous health discussion on Reddit. It consists\nof 20k English biomedical entity mentions expert-\nannotated with concepts from SNOMED CT. We\nuse the “stratiﬁed (general)” split and follow the\ntraining and evaluation procedures of SapBert (Liu\net al., 2021) and ResCNN (Lai et al., 2021).\nAskAPatient\n(Limsopatham and Collier, 2016)\nIt contains 8,662 phrases from social media. Each\nphrase can be mapped to one of the 1,036 medical\nconcepts from SNOMED-CT and AMT (the Aus-\ntralian Medicines Terminology). The samples in\nAskAPatient do not include contextual information.\nWe follow Sung et al. (2020) and Limsopatham and\nCollier (2016) for data pre-processing and apply\nthe 10-fold evaluation protocol.\n5.2.4\nNamed Entity Recognition\nShARe13,\nShARe14,\nCADEC\nThese three\ndatasets annotate discontinuous adverse drug\nevents entities. The main difference is the anno-\ntated data of ShARe tasks (Pradhan et al., 2013;\nMowery et al., 2014) comes from MIMIC-II, and\nCADEC (Karimi et al., 2015) comes from social\nmedia. There is only one entity type for these\ndatasets. We follow Yan et al. (2021) for dataset\npreprocess.\nGENIA\n(Kim et al., 2003) GENIA annotates\n2000 MEDLINE abstracts with biological entities.\nEntities can be nested with others. We follow (Lin\net al., 2019) to combine ﬁne-grained entity types\ninto 5 coarse-grained entity types and to construct\ndataset splits.\nAll the aforementioned datasets are in English.\nThe statistical overview of the aforementioned\ndatasets is listed in Table 1.\n5.3\nFine-tuning details\nDialogue\nWe use BioBART as the dialogue sys-\ntem model. The dialogue history is fed into the en-\ncoder and the decoder generates the response auto-\nregressively. We apply the negative log-likelihood\nfunction as the training objective with respect to\nthe reference dialogue response. We ﬁne-tune the\nmodel with learning rate 5e-5 for the base version\nand 1e-5 for the large version for 20 epochs. We\nrun evaluations on the validation set at the end of\neach epoch and use the checkpoint with the best\nvalidation performance for testing. During infer-\nence, we use beam search of size 5 to sample re-\nsponses from the model’s outputs. We use Rouge-\n1/2/L (Lin, 2004), BLEU (Papineni et al., 2002)\nand BERTscore (Zhang et al., 2020b) as our evalu-\nation metrics. RoBERTa-large (Liu et al., 2019) is\nused as scorer in BERTscore.\nSummarization\nSimilarly, for summarization,\nthe encoder takes the documents as input, and the\ndecoder generates the corresponding summariza-\ntions. We minimize the log-likelihood objective\nto ﬁne-tune the model and apply beam search for\ninference. Across different summarization datasets,\nthe beam size is set to 5 and we use no length\npenalty. We ﬁne-tune the model with learning rate\n5e-5 for the base version and 1e-5 for the large\nversion for 6 epochs. We run evaluations on the\nvalidation set at the end of each epoch and use the\ncheckpoint with the best validation performance\nfor testing. We apply the commonly used Rouge-\n1/2/L and BERTscore for evaluation metrics. The\nlarge version of RoBERTa is used as the scorer in\nBERTscore.\nEntity Linking\nWe follow the method and exper-\nimental settings in Yuan et al. (2022a) to implement\nthe generative model for biomedical entity linking\ntasks. We do not apply knowledge-base guided\npre-training proposed in Yuan et al. (2022a). The\ndocuments with the positions of mentions marked\nare fed into the encoder and the decoder outputs\nthe corresponding synonyms in the knowledge base\ndirectly. We use the top1 and top5 recall (Recall@1\nand Recall@5) as the evaluation metrics.\nNER\nWe use BARTNER (Yan et al., 2021) as\nour model. The target type for BARTNER is word\n(i.e. output ﬁrst BPE of each word in entities). We\nuse the parameters selected by Yan et al. (2021) for\nall pretrained models and ﬁne-tune for 30 epochs.\nEntity-level F1 is used as the metric.\n5.4\nMain Result\nIn this section, we present the base and large ver-\nsion of BioBART on various generation tasks. We\ncompare our in-domain BioBART with BART to\nillustrate the effectiveness of domain adaption. We\nalso compare with the existing state-of-the-art re-\nsults on each dataset to shed light on the superior\nperformance of BioBART. The experimental re-\nsults are shown in Table 2-5. The best and the\nsecond-best scores are highlighted with bold num-\nbers and underlines respectively.\nDialogue\nWe evaluate biomedical dialogue re-\nsponse generation on CovidDialog. For both base\nand large version, BioBART shows improvement\non the automatic metric Rouge. The large Bio-\nBART outperforms BART by 1.71 on Rouge-2 and\n0.03 on Rouge-L . Our evaluations surpasses the\ncurrent state-of-the-art on BLEU score by 4.45.\nSummarization\nWe present broad experimen-\ntal results on biomedical summarization datasets.\nFrom Table 3, BioBART has competitive or even\nsuperior performance on the task.\nExcept for\niCliniq and HealthCareMagic, we see consistent\nimprovement on different datasets for both sizes of\nBioBART. For MeQSum, BioBART large exceeds\nBART large for 1.93/1.31/2.1 on Rouge-1/2/L and\neven outperforms the current state-of-the-art. The\npossible reason that biomedical in-domain pretrain-\ning fails on iCliniq and HealthCareMagic is that\nboth datasets are built upon a clinical corpus. There\nstill exists a domain-shifting problem for BioBART\npretrained on biomedical scientiﬁc articles from\nPubMed.\nOn dialogue and summarization tasks, there are\nminor changes in BERTscore for different models.\nThis is possible because the metric is calculated\nby other pretranined language models. The im-\nplemented RoBERTa may suffer from biomedical\ndomain-shifting and cannot quantify the model per-\nformance accurately.\nEntity Linking\nThe results on biomedical en-\ntity linking tasks are shown in Table 4. For all\nthe tasks, models ﬁnetuned based on BioBART\nhave better performance. On AAP, BC5CDR, and\nCOMETA, our results outperform the current dis-\ncriminative state-of-the-art methods by 0.4, 1.67,\nand 1.36 points of Recall@1 respectively.\nNER\nThe performance improvement of Bio-\nBART on ShARe13, ShARe14, and GENIA is sig-\nniﬁcant, while the increase on CADEC is mediocre.\nFor the large models, BioBART improves entity-\nlevel F1 scores for 1.06 and 1 on ShARe13 and\nGENIA datasets. There are promising results for\ngenerative biomedical NER methods, while the gap\nCovidDialogue\nMeQSum\nMEDIQA-MAS\nRouge-2/L\nBLEU\nRouge-2/L\nBERTscore\nRouge-2/L\nBERTscore\nBART BASE\n12.31/25.66\n10.36\n35.79/50.46\n0.927\n9.98/27.85\n0.859\nw/ TI & SP\n10.90/25.46\n10.46\n34.93/49.28\n0.926\n9.04/27.43\n0.859\nw/ TI\n11.81/25.79\n12.79\n37.14/51.71\n0.929\n10.66/27.65\n0.862\nMedMentions\nCOMETA\nShARe13\nCADEC\nRecall@1\nRecall@5\nRecall@1\nRecall@5\nF1\nF1\nBART BASE\n69.77\n84.59\n78.34\n87.40\n76.63\n68.37\nw/ TI & SP\n70.76\n85.69\n79.31\n87.93\n77.01\n67.98\nw/ TI\n70.82\n85.68\n79.43\n88.28\n78.03\n69.22\nTable 6: Ablation study for pretraining tasks. TI is short for Text Inﬁlling and SP is short for Sentence Permutation.\nContext(Document)\nBART Response(Summary)\nBioBART Response(Summary)\nDescription: I have type 1 diabetes that I try\nhard to control, but i struggle with. Are my\nchances high for poor outcomes with the virus?\nI’m in such a panic about it.\nIn brief: diabetes. Your risk of complications\nwith covid-19 is very low. If you are well con-\ntrolled your risk should not be elevated. If your\ndiabetes is uncontrolled you should not have a\nparticularly high risk.\nDiabetes and Covid-19. If you are a type 1 dia-\nbetic, you are at high risk for poor outcomes\nwith the virus. Your risk is very high. You\nshould be aware of your risk factors and moni-\ntor your glycemic control closely.\nSUBJECT: treatment for dry mouth caused by\nnecessary medicine MESSAGE: My provider\ncan’t help (I asked.) I am intolerant of all the\n\"sugar alcohols\" such as maltilol, ...\nWhat are the treatments for dry mouth caused\nby sugar alcohols?\nWhat are the treatments for dry mouth?\nAre there any new treatments for orthostatic\ntremor?\n...\nDeep brain stimulation is some-\ntimes done when drugs cannot control a severe,\ndisabling tremor. Sometimes essential tremors\nor tremors due to Parkinson disease or another\ndisorder require such treatment. ... There is\nno cure for most forms of tremor, but there are\ntreatments to help manage symptoms. ...\nThere is no cure for tremor, but there are treat-\nments to help manage the symptoms.\nThere is no cure for tremor, but there are treat-\nments to help manage the symptoms. The most\ncommon ...\nhelp relieve symptoms.\nDeep\nbrain stimulation is sometimes done when\ndrugs cannot control a severe, disabling tremor.\nSometimes essential tremors or tremors due to\nParkinson disease or another disorder require\nsuch treatment.\nTable 7: Example dialogue and summaries from the ﬁne-tuned BART and BioBART model excepts of showing the\nreferences. The key information and differences are highlighted with colors.\nwith the current state-of-the-art NER method (Li\net al., 2021) is still salient.\n5.5\nAblation Study on Pretraining Task\nIn this section, we test on pretraining with or with-\nout the sentence permutation task. We pretrain\nBART base following the same pretraining settings\nexcept for reducing the training step to 40k for efﬁ-\nciency. We ﬁne-tuned the pretrained models on the\ndownstream tasks. The ablation results are shown\nin Table 6.\nFrom the result, it is illustrated that the model\npretrained on isolated text inﬁlling task performs\nthe best. The sentence permutation task down-\ngrades the model’s performance even for generative\nsummarization and dialogue system tasks.\n5.6\nGenerated example\nHere we demonstrate BioBART’s performance\nqualitatively. In Table 7, we present three gen-\nerative examples on CovidDialog, MeQSum, and\nMEDIQA-ANS respectively. In the ﬁrst example,\nwe can see that BART generates an erroneous in-\nstruction of the inﬂuence of diabetes. BioBART\ninjected with domain knowledge can correctly give\nthe response. In the second, BART misunderstands\nthe document where sugar alcohol is not the cause\nof dry mouth. BioBART generates an accurate\nand concise summary. In the ﬁnal example, the\nMEDIQA-ANS document is rather long and BART\nfails to extract complete information (colored in\nred). From the examples, we can conclude that\nBioBART has improvements on biomedical com-\nmon sense and documents understanding.\n6\nConclusions\nIn this work, we pretrain the biomedical domain\ngenerative language model BioBART. We also\ncollect various publicly available benchmarks for\nbiomedical generative tasks to prompt future re-\nsearch. Our experimental results show that con-\ntinuous pretraining on PubMed abstracts helps the\nmodel with domain adaption. BioBART shows\ngreat improvements on different benchmarks and\nachieves competitive or superior results over the\ncurrent state-of-the-art methods. We also release\nour pretraining and ﬁne-tuning codes to facilitate\nfuture research for reproducibility.\nWe will explore pretraining generative language\nmodels 1) on in-domain vocabularies and from\nscratch, 2) and with clinical corpora such as EMRs\nin MIMIC-III (Johnson et al., 2016) or PMC-\nPatients (Zhao et al., 2022) in the future studies.\nAcknowledgements\nWe appreciate three anonymous reviewers for help-\nful comments. This work was supported by the Na-\ntional Natural Science Foundation of China (Grant\nNo. 12171270), and the Natural Science Founda-\ntion of Beijing Municipality (Grant No. Z190024).\nReferences\nAsma Ben Abacha, Yassine Mrabet, Mark E. Sharp,\nTravis R. Goodwin, Sonya E. Shooshan, and Dina\nDemner-Fushman. 2019. Bridging the gap between\nconsumers’ medication questions and trusted an-\nswers. Studies in health technology and informatics,\n264:25–29.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal bert embeddings. In Proceedings of the 2nd Clin-\nical Natural Language Processing Workshop, pages\n72–78.\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu A. Ha, Rodney Michael Kinney, Sebas-\ntian Kohlmeier, Kyle Lo, Tyler C. Murray, Hsu-\nHan Ooi, Matthew E. Peters, Joanna L. Power, Sam\nSkjonsberg, Lucy Lu Wang, Christopher Wilhelm,\nZheng Yuan, Madeleine van Zuylen, and Oren Et-\nzioni. 2018. Construction of the literature graph in\nsemantic scholar. In NAACL.\nRico Angell, Nicholas Monath, Sunil Mohan, Nishant\nYadav, and Andrew McCallum. 2021.\nClustering-\nbased inference for biomedical entity linking.\nIn\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2598–2608, Online. Association for Compu-\ntational Linguistics.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-\nmasked language models for uniﬁed language model\npre-training.\nIn International Conference on Ma-\nchine Learning, pages 642–652. PMLR.\nMarco Basaldella, Fangyu Liu, Ehsan Shareghi, and\nNigel Collier. 2020. COMETA: A corpus for med-\nical entity linking in the social media. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n3122–3137, Online. Association for Computational\nLinguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientiﬁc text. In\nEMNLP.\nAsma Ben Abacha and Dina Demner-Fushman. 2019.\nOn the summarization of consumer health questions.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28th - August 2.\nAsma Ben Abacha, Yassine Mrabet, Yuhao Zhang,\nChaitanya Shivade,\nCurtis Langlotz,\nand Dina\nDemner-Fushman. 2021. Overview of the MEDIQA\n2021 shared task on summarization in the medical\ndomain. In Proceedings of the 20th Workshop on\nBiomedical Language Processing, pages 74–85, On-\nline. Association for Computational Linguistics.\nOlivier Bodenreider. 2004.\nThe uniﬁed medical lan-\nguage system (umls):\nintegrating biomedical ter-\nminology.\nNucleic acids research, 32 Database\nissue:D267–70.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell,\nSandhini Agarwal,\nAriel Herbert-Voss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh,\nDaniel M. Ziegler,\nJeff Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. ArXiv, abs/2005.14165.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn International Conference on Learning Represen-\ntations.\nHsiao-Tuan Chao, Lucy Liu, and Hugo J Bellen. 2017.\nBuilding dialogues between clinical and biomed-\nical research through cross-species collaborations.\nIn Seminars in cell & developmental biology, vol-\nume 70, pages 49–57. Elsevier.\nAllan Peter Davis, Thomas C Wiegers, Michael C\nRosenstein, and Carolyn J Mattingly. 2012. Medic:\na practical disease vocabulary used at the compara-\ntive toxicogenomics database. Database, 2012.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1–10.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019.\nUniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems, volume 32. Curran Asso-\nciates, Inc.\nZi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao\nJiang, and Graham Neubig. 2021. GSum: A gen-\neral framework for guided neural abstractive summa-\nrization. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4830–4842, Online. Association for\nComputational Linguistics.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4937–4951, Online. Associa-\ntion for Computational Linguistics.\nJenny Rose Finkel and Christopher D Manning. 2009.\nNested named entity recognition. In Proceedings of\nthe 2009 conference on empirical methods in natural\nlanguage processing, pages 141–150.\nYuxian Gu, Robert Tinn, Hao Cheng, Michael R.\nLucas,\nNaoto Usuyama,\nXiaodong Liu,\nTris-\ntan Naumann, Jianfeng Gao, and Hoifung Poon.\n2022.\nDomain-speciﬁc language model pretrain-\ning for biomedical natural language processing.\nACM Transactions on Computing for Healthcare\n(HEALTH), 3:1 – 23.\nSuchin\nGururangan,\nAna\nMarasovi´c,\nSwabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks.\nIn\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nDan Hendrycks and Kevin Gimpel. 2016.\nGaus-\nsian error linear units (gelus).\narXiv preprint\narXiv:1606.08415.\nQiao Jin, Bhuwan Dhingra, William Cohen, and\nXinghua Lu. 2019. Probing biomedical embeddings\nfrom language models. In Proceedings of the 3rd\nWorkshop on Evaluating Vector Space Representa-\ntions for NLP, pages 82–89.\nQiao Jin, Zheng Yuan, Guangzhi Xiong, Qianlan Yu,\nHuaiyuan Ying, Chuanqi Tan, Mosha Chen, Song-\nfang Huang, Xiaozhong Liu, and Sheng Yu. 2022.\nBiomedical question answering: A survey of ap-\nproaches and challenges.\nACM Comput. Surv.,\n55(2).\nAlistair E. W. Johnson, Tom J. Pollard, Lu Shen,\nLi wei H. Lehman,\nMengling Feng,\nMoham-\nmad Mahdi Ghassemi, Benjamin Moody, Peter\nSzolovits, Leo Anthony Celi, and Roger G. Mark.\n2016.\nMimic-iii, a freely accessible critical care\ndatabase. Scientiﬁc Data, 3.\nZeqian Ju, Subrato Chakravorty, Xuehai He, Shu Chen,\nXingyi Yang, and Pengtao Xie. 2020.\nCoviddi-\nalog:\nMedical dialogue datasets about covid-19.\nhttps://github.com/UCSD-AI4H/COVID-Dialogue.\nKamal\nRaj\nKanakarajan,\nBhuvana\nKundumani,\nand Malaikannan Sankarasubbu. 2021.\nBio-\nelectra:pretrained biomedical text encoder using\ndiscriminators. In BIONLP.\nSarvnaz Karimi, Alejandro Metke-Jimenez, Madonna\nKemp, and Chen Wang. 2015. Cadec: A corpus of\nadverse drug event annotations. Journal of biomedi-\ncal informatics, 55:73–81.\nHalil Kilicoglu, Asma Ben Abacha, Yassine Mrabet,\nSonya E. Shooshan, Laritza M. Rodriguez, Kate\nMasterton, and Dina Demner-Fushman. 2018. Se-\nmantic annotation of consumer health questions.\nBMC Bioinformatics, 19.\nJ-D Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi\nTsujii. 2003. Genia corpus—a semantically anno-\ntated corpus for bio-textmining.\nBioinformatics,\n19(suppl_1):i180–i182.\nTuan Lai, Heng Ji, and ChengXiang Zhai. 2021. BERT\nmight be overkill: A tiny but effective biomedical\nentity linker based on residual convolutional neural\nnetworks. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021, pages 1631–\n1639, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 260–270, San Diego, California. Association\nfor Computational Linguistics.\nMd Tahmid Rahman Laskar, Enamul Hoque, and\nJimmy Xiangji Huang. 2021.\nDomain adapta-\ntion with pre-trained transformers for query focused\nabstractive text summarization.\narXiv preprint\narXiv:2112.11670.\nJinhyuk\nLee,\nWonjin\nYoon,\nSungdong\nKim,\nDonghyeon Kim,\nSunkyu Kim,\nChan Ho So,\nand Jaewoo Kang. 2020.\nBiobert: a pre-trained\nbiomedical\nlanguage\nrepresentation\nmodel\nfor\nbiomedical text mining. Bioinformatics, 36:1234 –\n1240.\nMike\nLewis,\nYinhan\nLiu,\nNaman\nGoyal,\nMar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin\nStoyanov. 2020b.\nPretrained language models for\nbiomedical and clinical tasks: Understanding and\nextending the state-of-the-art.\nIn Proceedings of\nthe 3rd Clinical Natural Language Processing Work-\nshop, pages 146–157, Online. Association for Com-\nputational Linguistics.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nJingye Li, Hao Fei, Jiang Liu, Shengqiong Wu, Meis-\nhan Zhang, Chong Teng, Donghong Ji, and Fei\nLi. 2021.\nUniﬁed named entity recognition as\nword-word relation classiﬁcation.\narXiv preprint\narXiv:2112.10070.\nNut Limsopatham and Nigel Collier. 2016. Normalis-\ning medical concepts in social media texts by learn-\ning semantic representation. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1014–1023, Berlin, Germany. Association for Com-\nputational Linguistics.\nChin-Yew Lin. 2004.\nROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nHongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun.\n2019. Sequence-to-nuggets: Nested entity mention\ndetection via anchor-region networks. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 5182–5192,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2021. Self-alignment\npretraining for biomedical entity representations. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4228–4238.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders.\nIn Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language de-\ncathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nSunil Mohan and Donghui Li. 2019. Medmentions: A\nlarge biomedical corpus annotated with {umls} con-\ncepts. In Automated Knowledge Base Construction\n(AKBC).\nDanielle L Mowery, Sumithra Velupillai, Brett R South,\nLee Christensen, David Martinez, Liadh Kelly, Lor-\nraine Goeuriot, Noemie Elhadad, Sameer Pradhan,\nGuergana Savova, et al. 2014. Task 2: Share/clef\nehealth evaluation lab 2014.\nIn Proceedings of\nCLEF 2014.\nKhalil Mrini, Franck Dernoncourt, Seunghyun Yoon,\nTrung Bui, Walter Chang, Emilia Farcas, and Ndapa\nNakashole. 2021. A gradually soft multi-task and\ndata-augmented approach to medical question under-\nstanding. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 1505–1515, Online. Association for Computa-\ntional Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting on Association for Computa-\ntional Linguistics, ACL ’02, page 311–318, USA.\nAssociation for Computational Linguistics.\nYifan Peng, Qingyu Chen, and Zhiyong Lu. 2020. An\nempirical study of multi-task learning on bert for\nbiomedical text mining. In BIONLP.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL.\nLong Phan, James T. Anibal, Hieu Tran, Shaurya\nChanana, Erol Bahadroglu, Alec Peltekian, and Gré-\ngoire Altan-Bonnet. 2021.\nSciﬁve: a text-to-text\ntransformer model for biomedical literature. ArXiv,\nabs/2106.03598.\nSameer Pradhan, Noemie Elhadad, Brett R South,\nDavid Martinez, Lee M Christensen, Amy Vogel,\nHanna Suominen, Wendy W Chapman, and Guer-\ngana K Savova. 2013. Task 1: Share/clef ehealth\nevaluation lab 2013.\nIn CLEF (Working Notes),\npages 212–31.\nAlec Radford and Karthik Narasimhan. 2018.\nIm-\nproving language understanding by generative pre-\ntraining.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020.\nExploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020.\nZero: Memory optimiza-\ntions toward training trillion parameter models. In\nProceedings of the International Conference for\nHigh Performance Computing, Networking, Storage\nand Analysis, SC ’20. IEEE Press.\nMax E. Savery, Asma Ben Abacha, Soumya Gayen,\nand Dina Demner-Fushman. 2020. Question-driven\nsummarization of answers to consumer health ques-\ntions. Scientiﬁc Data, 7.\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina,\nRaul Puri, Mostofa Patwary, Mohammad Shoeybi,\nand Raghav Mani. 2020.\nBioMegatron:\nLarger\nbiomedical domain language model.\nIn Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4700–4706, Online. Association for Computational\nLinguistics.\nTianxiang Sun, Xiangyang Liu, Xipeng Qiu, and\nXuanjing\nHuang.\n2021.\nParadigm\nshift\nin\nnatural language processing.\narXiv preprint\narXiv:2109.12575.\nMujeen Sung, Hwisang Jeon, Jinhyuk Lee, and Jaewoo\nKang. 2020. Biomedical entity representations with\nsynonym marginalization. In ACL.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R. Alvers, Dirk Weissenborn, Anastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, Yannis Almirantis, John Pavlopoulos, Nico-\nlas Baskiotis, Patrick Gallinari, Thierry Artières,\nAxel-Cyrille Ngonga Ngomo, Norman Heino, Éric\nGaussier, Liliana Barrio-Alvers, Michael Schroeder,\nIon Androutsopoulos, and Georgios Paliouras. 2015.\nAn overview of the bioasq large-scale biomedical\nsemantic indexing and question answering competi-\ntion. BMC Bioinformatics, 16.\nMaya Varma, Laurel Orr, Sen Wu, Megan Leszczynski,\nXiao Ling, and Christopher Ré. 2021. Cross-domain\ndata integration for named entity disambiguation in\nbiomedical text. In Findings of the Association for\nComputational Linguistics: EMNLP 2021, pages\n4566–4575, Punta Cana, Dominican Republic. As-\nsociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nHang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng\nZhang, and Xipeng Qiu. 2021. A uniﬁed generative\nframework for various NER subtasks. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 5808–5822,\nOnline. Association for Computational Linguistics.\nHongyi Yuan, Zheng Yuan, and Sheng Yu. 2022a. Gen-\nerative biomedical entity disambiguation via knowl-\nedge base-guided pre-training and synonyms-aware\nﬁne-tuning. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies.\nZheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang,\nand Fei Huang. 2021.\nImproving biomedical\npretrained language models with knowledge.\nIn\nBIONLP.\nZheng Yuan, Yuanhao Liu, Qiuyang Yin, Boyao Li, Xi-\naobin Feng, Guoming Zhang, and Sheng Yu. 2020.\nUnsupervised multi-granular chinese word segmen-\ntation and term discovery via graph partition. Jour-\nnal of Biomedical Informatics, 110:103542.\nZheng Yuan, Zhengyun Zhao, Haixia Sun, Jiao Li, Fei\nWang, and Sheng Yu. 2022b. Coder: Knowledge-\ninfused cross-lingual medical term embedding for\nterm normalization.\nJournal of Biomedical Infor-\nmatics, page 103983.\nGuangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang,\nSicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi\nZeng, Xiangyu Dong, Ruoyu Zhang, Hongchao\nFang, Penghui Zhu, Shu Chen, and Pengtao Xie.\n2020.\nMedDialog: Large-scale medical dialogue\ndatasets.\nIn Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 9241–9250, Online. Associa-\ntion for Computational Linguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020a. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages\n11328–11339. PMLR.\nNingyu Zhang, Mosha Chen, Zhen Bi, Xiaozhuan\nLiang, Lei Li, Xin Shang, Kangping Yin, Chuanqi\nTan, Jian Xu, Fei Huang, et al. 2021. Cblue: A chi-\nnese biomedical language understanding evaluation\nbenchmark. arXiv preprint arXiv:2106.08087.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020b.\nBertscore:\nEvaluating text generation with bert.\nIn Interna-\ntional Conference on Learning Representations.\nZhengyun Zhao, Qiao Jin, and Sheng Yu. 2022. Pmc-\npatients: A large-scale dataset of patient notes and\nrelations extracted from case reports in pubmed cen-\ntral. arXiv preprint arXiv:2202.13876.\nMeng Zhou, Zechen Li, Bowen Tan, Guangtao Zeng,\nWenmian Yang, Xuehai He, Zeqian Ju, Subrato\nChakravorty,\nShu Chen,\nXingyi Yang,\nYichen\nZhang, Qingyang Wu, Zhou Yu, Kun Xu, Eric Xing,\nand Pengtao Xie. 2021. On the generation of med-\nical dialogs for COVID-19. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers), pages 886–896, Online. As-\nsociation for Computational Linguistics.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-04-08",
  "updated": "2022-04-22"
}