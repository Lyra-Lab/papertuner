{
  "id": "http://arxiv.org/abs/2204.05462v1",
  "title": "Out-Of-Distribution Detection In Unsupervised Continual Learning",
  "authors": [
    "Jiangpeng He",
    "Fengqing Zhu"
  ],
  "abstract": "Unsupervised continual learning aims to learn new tasks incrementally without\nrequiring human annotations. However, most existing methods, especially those\ntargeted on image classification, only work in a simplified scenario by\nassuming all new data belong to new tasks, which is not realistic if the class\nlabels are not provided. Therefore, to perform unsupervised continual learning\nin real life applications, an out-of-distribution detector is required at\nbeginning to identify whether each new data corresponds to a new task or\nalready learned tasks, which still remains under-explored yet. In this work, we\nformulate the problem for Out-of-distribution Detection in Unsupervised\nContinual Learning (OOD-UCL) with the corresponding evaluation protocol. In\naddition, we propose a novel OOD detection method by correcting the output bias\nat first and then enhancing the output confidence for in-distribution data\nbased on task discriminativeness, which can be applied directly without\nmodifying the learning procedures and objectives of continual learning. Our\nmethod is evaluated on CIFAR-100 dataset by following the proposed evaluation\nprotocol and we show improved performance compared with existing OOD detection\nmethods under the unsupervised continual learning scenario.",
  "text": "Out-Of-Distribution Detection In Unsupervised Continual Learning\nJiangpeng He\nhe416@purdue.edu\nFengqing Zhu\nzhu0@purdue.edu\nElmore Family School of Electrical and Computer Engineering, Purdue University, USA\nAbstract\nUnsupervised continual learning aims to learn new tasks\nincrementally without requiring human annotations. How-\never, most existing methods, especially those targeted on im-\nage classiﬁcation, only work in a simpliﬁed scenario by as-\nsuming all new data belong to new tasks, which is not realis-\ntic if the class labels are not provided. Therefore, to perform\nunsupervised continual learning in real life applications, an\nout-of-distribution detector is required at beginning to iden-\ntify whether each new data corresponds to a new task or\nalready learned tasks, which still remains under-explored\nyet. In this work, we formulate the problem for Out-of-\ndistribution Detection in Unsupervised Continual Learning\n(OOD-UCL) with the corresponding evaluation protocol.\nIn addition, we propose a novel OOD detection method by\ncorrecting the output bias at ﬁrst and then enhancing the\noutput conﬁdence for in-distribution data based on task dis-\ncriminativeness, which can be applied directly without mod-\nifying the learning procedures and objectives of continual\nlearning. Our method is evaluated on CIFAR-100 dataset\nby following the proposed evaluation protocol and we show\nimproved performance compared with existing OOD de-\ntection methods under the unsupervised continual learning\nscenario.\n1. Introduction\nUnsupervised continual learning is an emerging future\nlearning system, capable of learning new tasks incremen-\ntally from unlabeled data. It requires neither static datasets\nnor human annotations compared with supervised ofﬂine\nlearning. Existing methods study this problem under the\nassumption that all new data belongs to new tasks. We ar-\ngue that if human annotation is not available as common in\nunsupervised scenario, we cannot know whether the unla-\nbeled new data belongs to new or learned tasks. For exam-\nple, an image-based mobile food recognition system should\nbe able to distinguish new and learned food images ﬁrst in-\nstead of blindly treating all of them as new food classes to\nperform unsupervised continual learning for update. There-\nfore, in order to make unsupervised continual learning work\nin practical problems, an out-of-distribution (OOD) detec-\ntor should be required at the beginning of each incremental\nlearning step to identify whether each data belongs to new\nor already learned tasks. However, the problem of OOD de-\ntection in continual learning still remains under-explored,\ni.e. none of the existing OOD detection methods target for\ncontinual learning.\nThe goal of OOD detection for image classiﬁcation is\nto detect novel classes data.\nHowever, it becomes more\nchallenging under continual learning scenario due to (1) the\ntraining data of learned tasks becomes unavailable; (2) we\nalso need to address catastrophic forgetting problem [20].\nMost existing methods cannot be applied here because they\neither require all training data for already learned tasks to\ntrain an OOD detector [14, 26, 29], or they need to mod-\nify the training procedure and objectives [9, 21, 30], which\nmay sacriﬁce the classiﬁcation accuracy. Therefore, we fo-\ncus on “post-hoc” methods [32] that can be directly applied\non any trained classiﬁcation models to perform OOD detec-\ntion based on the output conﬁdence, which has been widely\nadopted in real-world environments to avoid the need to ac-\ncess training data.\nThe central idea of “post-hoc” methods to perform OOD\ndetection is to assign in-distribution (ID) data with higher\nconﬁdence value Confin than the OOD data Confout based\non the output vector where the conﬁdence Conf is deﬁned\nas the maximum of softmax output [10, 16] or the energy\nscore [17]. The detection performance greatly depends on\nthe difference value of output conﬁdence between ID and\nOOD data Dc = Confin −Confout where higher Dc in-\ndicates better discrimination.\nHowever, there exists two\nmajor issues in continual learning scenario that can lead to\nthe decrease of Dc including (1) the biased output value to-\nwards new classes as revealed in [31, 34]; (2) the decrease\nof output conﬁdence compared with ofﬂine learning due to\nthe objective of improving generalization ability to mitigate\ncatastrophic forgetting [15, 33]. Both issues can result in\nperformance degradation for existing “post-hoc” methods.\nIn this work, we ﬁrst formulate the OOD detection in\narXiv:2204.05462v1  [cs.CV]  12 Apr 2022\nunsupervised continual learning scenario denoted as OOD-\nUCL and introduce the corresponding evaluation protocol.\nThen, we propose a novel OOD detection method that can\naddress both issues mentioned above to achieve improved\nperformance in unsupervised continual learning scenario.\nThe main contributions are summarized as following.\n• To the best of our knowledge, we are the ﬁrst to for-\nmulate the problem and evaluation protocol for out-of-\ndistribution detection in unsupervised continual learn-\ning (OOD-UCL), which remains under-explored.\n• A novel method is introduced for OOD detection by\ncorrecting output bias and enhancing output conﬁ-\ndence difference based on task discriminativeness.\n• We conduct extensive experiments on the CIFAR-\n100 [13] dataset to show the effectiveness of each com-\nponent of our proposed method compared to existing\nworks under OOD-UCL scenario.\n2. Related Work\nWe focus on image classiﬁcation problem and we review\nthe existing methods that are related to our work including\n(1) unsupervised continual learning; (2) OOD detection.\n2.1. Unsupervised Continual Learning\nCompared with supervised case, unsupervised continual\nlearning has not received much attention [19]. Stojanov et\nal. [28] introduced an unsupervised object learning envi-\nronment to learn a sequence of single-class exposures. In\naddition, CURL [23] and STAM [27] are proposed for task-\nfree unsupervised continual learning where task boundary\nis not given. Based on existing supervised protocol [24],\nthe most recent work [5] proposed to use pseudo labels ob-\ntained based on cluster assignments to perform continual\nlearning and show promising results on several benchmark\ndatasets in unsupervised scenario. However, they only as-\nsume a simpliﬁed scenario where all the new data belong to\nnew classes, which rarely happens in real life applications\nwhen the class labels are not available. Therefore, an OOD\ndetector that can work under unsupervised continual learn-\ning scenario becomes indispensable.\n2.2. Out-of-distribution Detection\nAs illustrated in Section 1, we focus on image classiﬁ-\ncation based OOD detection and analyze this problem in\ncontinual learning scenario where the training objective is\nmore challenging. Therefore, we target on methods that\ncan be applied to any trained classiﬁcation model without\nmodifying the training procedure, which is called “post-\nhoc” methods [32]. Existing “post-hoc” methods are origi-\nnated from [10], which directly uses the maximum softmax\nprobability as the conﬁdence score to discriminate ID and\n𝒉𝑲\n𝒉𝑲\"𝟏\nmodel\nOOD Detection\nunsupervised\ncontinual learning\nlearned classes (ID)\nnew classes (OOD)\n𝑺𝟏,…,𝑲\n𝑺𝑲%𝟏\n𝑺𝟏,…,𝑲\n𝑺𝑲%𝟐\n𝑺𝑲%𝟏\n𝑫𝑲%𝟏\nFigure 1. Formulation of out-of-distribution detection in unsuper-\nvised continual learning (OOD-UCL). hK refers to the updated\nincremental models after learning T K. DK and SK denote the\ncorresponding training and testing splits for task K, respectively.\nOOD data. Then ODIN [16] applies temperature scaling\nand input perturbation to amplify the conﬁdence difference\nDc between ID and OOD data where a large temperature\ntransforms the softmax score back to the logit space. Built\non these insights, recent work [17] proposed to use energy\nscore as output conﬁdence for OOD detection, which maps\nthe output to a scalar through a convenient log-sum-exp op-\nerator. However, none of the existing “post-hoc” methods\nconsider the two issues in continual learning scenario as il-\nlustrated in Section 1, resulting in performance degradation.\n3. Problem Formulation\nThe objective is to perform OOD detection in continual\nlearning scenario to discriminate unlabeled learned tasks\ndata (as ID) and new task data (as OOD), which can be\nthen incorporated into any existing unsupervised contin-\nual learning methods to apply in real life applications. We\nformulate the out-of-distribution in unsupervised continual\nlearning (OOD-UCL) problem based on the existing unsu-\npervised class-incremental learning protocol [5] to evalu-\nate the OOD detection performance before each incremen-\ntal learning step. Speciﬁcally, the continual learning for\nimage classiﬁcation problem T can be expressed as learn-\ning a sequence of N tasks {T 1, ..., T N} corresponding\nto (N −1) incremental learning steps where the learning\nof the ﬁrst task T 1 is not included.\nEach task contains\nM non-overlapped classes, which is known as incremen-\ntal step size. Let {D1, ..., DN} denote the training data and\n{S1, ..., SN} denote the testing data for each task, we for-\nmulate the OOD-UCL with the following properties.\nProperty 1: The OOD detection is performed at be-\nginning of the learning step for each new task T K where\nK ∈{2, ...N}. The test data belonging to learned tasks\nSi, i ∈{1, ...K −1} is regarded as ID data and the test data\nbelonging to the current incremental step SK is regarded as\nthe OOD data. Figure 1 illustrates the evaluation protocol,\nwhere we perform total (N −1) times OOD detection for\ncontinually learning a sequence of N tasks {T 1, ..., T N}.\nProperty 2: The training data allowed for OOD de-\ntection before learning T K is restricted to (1) the train-\ning set of DK−1 and (2) the stored exemplars belonging\nto {T 1, ..., T K−2} if applicable. This restricts the usage of\nmost existing methods [14,26,29] which requires all train-\n𝒙\n𝒉𝒌\n∶\n𝑜!\n𝑜\"\n𝑜#\n𝑜$\n∶\n#𝑜!\n#𝑜\"\n#𝑜#\n#𝑜$\nBias \nCorrection\nConfidence\nEnhancement\nOOD\nID\nConfidence\nScore\n𝑫𝑪\nTask 1\nTask 2\n∶\nTask K\nMeasure task\ndiscriminativeness\nFigure 2. The overview of our proposed method where x refers\nto input data and hK denotes the continual model after learning\ntask T K. We ﬁrst correct the bias of output O to obtain ˆO and\nthen perform conﬁdence enhancement to further increase the con-\nﬁdence difference Dc to improve OOD detection performance.\ning data for learned classes to train an OOD detector.\nEvaluation metrics: In OOD detection, each test data\nis assigned with a conﬁdence score where samples below\nthe pre-deﬁned conﬁdence threshold are considered as OOD\ndata. By regarding the ID data as positive and OOD data as\nnegative, we can obtain a series of true positives rate (TPR)\nand false positive rate (FPR) by varying the thresholds. One\nof the commonly used metrics for OOD detection is FPR95,\nwhich measures the FPR when the TPR is 0.95 and lower\nvalue indicates better detection performance. Besides, we\ncan also calculate the area under receiver operating charac-\nteristic curve (AUROC [2]) based on FPR and TPR as well\nas the area under the precision-recall curve (AUPR [25]).\nFor both AUROC and AUPR, a higher value indicates bet-\nter detection performance.\n4. Our Method\nIn this section, we introduce a novel “post-hoc” OOD\ndetection method with the goal of improving the perfor-\nmance under unsupervised continual learning scenario, i.e.\nincrease the conﬁdence difference Dc between ID and OOD\ndata for better discrimination. The overview of the proposed\nmethod is shown in Figure 2, which can be directly applied\nwithout requiring any change to the existing classiﬁcation\nmodels. There are two main steps including bias correc-\ntion and conﬁdence enhancement where we ﬁrst correct\nthe biased output value and then enhance the conﬁdence\ndifference Dc based on task discriminativeness, which are\ndescribed in Section 4.1 and Section 4.2, respectively.\n4.1. Bias Correction\nOutput bias towards new classes is a widely recognized\nissue [31,34] caused by the lack of training data for learned\ntasks during continual learning. This results in the increase\nof the output value towards the biased classes for both ID\nand OOD data, therefore decreases the conﬁdence differ-\nence Dc, i.e.\nthe degradation of OOD detection perfor-\nmance. Motivated by WA [34] which shows the existence\nof biased weights in the FC classiﬁer, we propose to per-\nform bias correction by normalizing output logits based on\nthe norm of weight vectors in the classiﬁer corresponding\nto each learned class. Speciﬁcally, we denote the weight\nparameters in the classiﬁer as P ∈Rd×C where d is the\ndimension of extracted feature of each input sample and C\nrefers to the total number of classes seen so far. The weight\nnorm of P corresponds to each learned class is calculated\nas\n|W i| = L2(P 1,i, P 2,i, ...P d,i), i ∈{1, 2, ...C}\n(1)\nwhere L2() denotes the l2 normalization and P j,k refers\nto the element of jth row and kth column in P.\nLet\nO = {o1, o2, ..., oC} denote the output from the classiﬁer,\nwe normalize it through\nˆoi = oi/|W i|, i ∈{1, 2, ...C}\n(2)\nwhere ˆoi refers to the corrected output for class i.\nOur\nweight-based normalization generates the corrected output\nby efﬁciently mitigating the bias effect from the classiﬁer.\n4.2. Conﬁdence Enhancement\nThe learning objective also changes in continual learn-\ning scenario.\nBesides learning new tasks, we also need\nto maintain the learned knowledge.\nAs shown in [22],\nhigher conﬁdent output can decrease the model’s general-\nization ability, which leads to catastrophic forgetting. Most\nexisting continual learning methods address this problem\nby adding regularization to restrict the change of parame-\nters [1, 3, 4, 12, 15, 24] when learning new tasks, which de-\ncrease the output conﬁdence for both ID and OOD data,\nresulting in the decrease of conﬁdence difference Dc. Our\ngoal is to increase Dc to achieve better detection perfor-\nmance.\nOur proposed conﬁdence enhancement method\nis motivated by the most recent work [6, 7], which show\nthat the continual learning model is able to maintain the\ndiscriminativeness within each learned task.\nIdeally, an\nID data should be more conﬁdent and task-discriminative\nthan OOD data. Therefore, after correcting the biased out-\nput, we apply softmax on ˆO = {ˆo1, ˆo2, ..., ˆoC} to ob-\ntain ˆS = {ˆs1, ˆs2, ..., ˆsC}. We extract the maximum value\nas ˆSmax = max( ˆS) and its corresponding task index\nImax = argmaxi=1,2...K( ˆS) where K denotes the total\nnumber of tasks {T 1, ..., T K} learned so far.\nThe soft-\nmax output value for task TImax is extracted from ˆS as\nˆSImax = {ˆs1\nImax, ˆs2\nImax, ...ˆsM\nImax} where M refers to the\nnumber of classes in each task, i.e. the incremental step\nsize. We then measure the discriminativeness based on en-\ntropy as in Equation 3 where lower entropy H indicates\nmore discriminative.\nHImax =\nM\nX\ni=1\nˆsi\nImax × logM(ˆsi\nImax)\n(3)\nFinally, we calculate the conﬁdence score as\nConf =\nˆSmax\nHImax + ϵ\n(4)\n(a)\n(b)\n(c)\nFigure 3. Results on CIFAR-100 with step size (a) 5 (b) 10 and (c) 20. The numerator and denominator of x-axis refers to the number of\nlearned classes and new added classes, which are regarded as in-distribution and out-of-distribution data, respectively.\nMethods\nStep size 5\nStep size 10\nStep size 20\nAUROC↑\nAUPR↑\nFPR95↓\nAUROC↑\nAUPR↑\nFPR95 ↓\nAUROC↑\nAUPR↑\nFPR95↓\nMSP [10]\n0.679\n0.947\n0.855\n0.685\n0.899\n0.873\n0.681\n0.834\n0.877\nODIN [16]\n0.723\n0.950\n0.810\n0.715\n0.909\n0.831\n0.715\n0.858\n0.839\nEnergy Score [17]\n0.707\n0.950\n0.824\n0.714\n0.907\n0.837\n0.706\n0.853\n0.844\nOurs (w/o BC)\n0.712\n0.951\n0.823\n0.719\n0.912\n0.845\n0.706\n0.851\n0.842\nOurs (w/o CE)\n0.708\n0.947\n0.836\n0.713\n0.907\n0.851\n0.699\n0.844\n0.854\nOurs\n0.754\n0.959\n0.793\n0.736\n0.915\n0.824\n0.729\n0.874\n0.814\nTable 1. Average AUROC, AUPR and FPR95 on CIFAR-100 with step size 5, 10 and 20. BC and CE denotes bias correction step and\nconﬁdence enhancement step, respectively. Best results are marked in bold.\nwhere ϵ = 0.00001 is used for regularization. Test samples\nassigned with larger score is regarded as ID data.\n5. Experimental Results\nIn this section, we show the effectiveness of our OOD\ndetection method by applying the baseline in [5] to perform\nunsupervised continual learning. The experimental results\nby incorporating other continual learning method [5,12] are\navailable in the supplementary Section 8. We follow the\nproposed evaluation protocol by comparing the OOD de-\ntection results with existing “post-hoc” methods including\nMSP [10], ODIN [16] and Energy Score [17]. We run\neach experiment 5 times and report the average results. The\nimplementation detail of all existing methods can be found\nin the supplementary Section 7.\nWe use the CIFAR-100 [13] dataset and divide the 100\nclasses into splits of 20, 10 and 5 tasks with corresponding\nincremental step size 5, 10 and 20, respectively. Following\nthe protocol in Section 3, we perform OOD detection at the\nbeginning of each new task except the ﬁrst one.\n5.1. Results on CIFAR-100\nTable 1 shows the average OOD detection results on\nCIFAR-100 in terms of AUROC, AUPR and FPR95 as in-\ntroduced in Section 3.\nWe observe signiﬁcant improve-\nments for OOD detection in unsupervised continual learn-\ning scenario compared with existing “post-hoc” methods.\nBesides, we also include ours (w/o BC) and ours (w/o CE)\nfor ablation study where BC and CE denote bias correc-\ntion and conﬁdence enhancement steps as illustrated in Sec-\ntion 4. Note that the MSP [10] can be regarded as ours (w/o\nBC and CE). Thus, both BC and CE improves the detection\nperformance compared with MSP and our method including\nboth steps achieve the best performance. In addition, the\nAUROC on CIFAR-100 for each incremental step is shown\nin Figure 3. Our method outperforms existing approaches\nat each step especially with larger margins for smaller step\nsize, as both output bias and conﬁdence decrease problems\nbecome more severe due to the increasing number of incre-\nmental learning steps.\n6. Conclusion\nIn this work, we ﬁrst formulate the problem of out-\nof-distribution detection in unsupervised continual learn-\ning (OOD-UCL) and introduce the corresponding evalua-\ntion protocol. Then a novel OOD detection method is pro-\nposed by correcting output bias and enhancing conﬁdence\ndifference between ID and OOD data. Our experimental\nresults on CIFAR-100 show promising improvements com-\npared with existing methods for various step sizes.\nFor future work, instead of splitting the dataset with non-\noverlapped classes, we will focus on unsupervised contin-\nual learning in a more realistic scenario where each new\ntask may contain both new classes and learned classes data.\nTherefore, a more efﬁcient method that can perform contin-\nual learning based on the output of OOD detection is needed\nfor real life applications.\nReferences\n[1] Francisco M. Castro, Manuel J. Marin-Jimenez, Nicolas\nGuil, Cordelia Schmid, and Karteek Alahari. End-to-end in-\ncremental learning. Proceedings of the European Conference\non Computer Vision, September 2018. 3\n[2] Jesse Davis and Mark Goadrich. The relationship between\nprecision-recall and roc curves. Proceedings of the 23rd in-\nternational conference on Machine learning, pages 233–240,\n2006. 3\n[3] Jiangpeng He, Runyu Mao, Zeman Shao, and Fengqing Zhu.\nIncremental learning in online scenario. Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 13926–13935, 2020. 3\n[4] Jiangpeng He and Fengqing Zhu. Online continual learning\nfor visual food classiﬁcation. Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision Workshops,\npages 2337–2346, October 2021. 3\n[5] Jiangpeng He and Fengqing Zhu. Unsupervised continual\nlearning via pseudo labels. arXiv preprint arXiv:2104.07164,\n2021. 2, 4, 7, 8\n[6] Jiangpeng He and Fengqing Zhu. Exemplar-free online con-\ntinual learning. arXiv preprint arXiv:2202.05491, 2022. 3\n[7] Jiangpeng He and Fengqing Zhu. Online continual learning\nvia candidates voting. Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision (WACV),\npages 3154–3163, January 2022. 3\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016. 7\n[9] Matthias Hein, Maksym Andriushchenko, and Julian Bitter-\nwolf. Why relu networks yield high-conﬁdence predictions\nfar away from the training data and how to mitigate the prob-\nlem. Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 41–50, 2019. 1\n[10] Dan Hendrycks and Kevin Gimpel. A baseline for detecting\nmisclassiﬁed and out-of-distribution examples in neural net-\nworks. Proceedings of International Conference on Learning\nRepresentations, 2017. 1, 2, 4, 7\n[11] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distill-\ning the knowledge in a neural network. Proceedings of the\nNIPS Deep Learning and Representation Learning Work-\nshop, 2015. 7\n[12] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and\nDahua Lin. Learning a uniﬁed classiﬁer incrementally via\nrebalancing. Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 831–839, 2019.\n3, 4, 8\n[13] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 2, 4, 7\n[14] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A\nsimple uniﬁed framework for detecting out-of-distribution\nsamples and adversarial attacks. Advances in neural infor-\nmation processing systems, 31, 2018. 1, 2\n[15] Zhizhong Li and Derek Hoiem. Learning without forgetting.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 40(12):2935–2947, 2017. 1, 3\n[16] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the re-\nliability of out-of-distribution image detection in neural net-\nworks. Proceedings of International Conference on Learning\nRepresentations, 2018. 1, 2, 4, 7\n[17] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li.\nEnergy-based out-of-distribution detection.\nAdvances in\nNeural Information Processing Systems, 2020. 1, 2, 4, 7\n[18] Stuart Lloyd. Least squares quantization in pcm. IEEE trans-\nactions on information theory, 28(2):129–137, 1982. 7\n[19] Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel\nMenta, Andrew D Bagdanov, and Joost van de Weijer. Class-\nincremental learning: survey and performance evaluation.\narXiv preprint arXiv:2010.15277, 2020. 2\n[20] Michael McCloskey and Neal J Cohen. Catastrophic inter-\nference in connectionist networks: The sequential learning\nproblem. In Psychology of Learning and Motivation, vol-\nume 24, pages 109–165. Elsevier, 1989. 1\n[21] Jay Nandy, Wynne Hsu, and Mong Li Lee. Towards maxi-\nmizing the representation gap between in-domain & out-of-\ndistribution examples. Advances in Neural Information Pro-\ncessing Systems, 33:9239–9250, 2020. 1\n[22] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz\nKaiser, and Geoffrey Hinton. Regularizing neural networks\nby penalizing conﬁdent output distributions. arXiv preprint\narXiv:1701.06548, 2017. 3\n[23] Dushyant Rao, Francesco Visin, Andrei A Rusu, Yee Whye\nTeh, Razvan Pascanu, and Raia Hadsell.\nContinual\nunsupervised representation learning.\narXiv preprint\narXiv:1910.14481, 2019. 2\n[24] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg\nSperl, and Christoph H. Lampert. iCaRL: Incremental clas-\nsiﬁer and representation learning. Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\nJuly 2017. 2, 3, 7\n[25] Takaya Saito and Marc Rehmsmeier. The precision-recall\nplot is more informative than the roc plot when evaluat-\ning binary classiﬁers on imbalanced datasets.\nPloS one,\n10(3):e0118432, 2015. 3\n[26] Chandramouli Shama Sastry and Sageev Oore.\nDetecting\nout-of-distribution examples with gram matrices. Interna-\ntional Conference on Machine Learning, pages 8491–8501,\n2020. 1, 2\n[27] James Smith, Seth Baer, Cameron Taylor, and Constantine\nDovrolis. Unsupervised progressive learning and the stam\narchitecture. arXiv preprint arXiv:1904.02021, 2019. 2\n[28] Stefan Stojanov, Samarth Mishra, Ngoc Anh Thai, Nikhil\nDhanda, Ahmad Humayun, Chen Yu, Linda B Smith, and\nJames M Rehg. Incremental object learning from contiguous\nviews. Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 8777–8786, 2019. 2\n[29] Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and\nYarin Gal. Uncertainty estimation using a single deep de-\nterministic neural network. International conference on ma-\nchine learning, pages 9690–9700, 2020. 1, 2\n[30] Yezhen Wang, Bo Li, Tong Che, Kaiyang Zhou, Ziwei Liu,\nand Dongsheng Li.\nEnergy-based open-world uncertainty\nmodeling for conﬁdence calibration.\nProceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9302–9311, 2021. 1\n[31] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,\nZicheng Liu, Yandong Guo, and Yun Fu. Large scale in-\ncremental learning. Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, June 2019. 1, 3\n[32] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu.\nGeneralized out-of-distribution detection: A survey. arXiv\npreprint arXiv:2110.11334, 2021. 1, 2\n[33] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi\nFeng. Revisiting knowledge distillation via label smoothing\nregularization.\nProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3903–\n3911, 2020. 1\n[34] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-\nTao Xia. Maintaining discrimination and fairness in class\nincremental learning. Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 13208–\n13217, 2020. 1, 3\nMethods\nStep size 5\nStep size 10\nStep size 20\nAUROC↑\nAUPR↑\nFPR95↓\nAUROC↑\nAUPR↑\nFPR95↓\nAUROC↑\nAUPR↑\nFPR95↓\nbaseline\n0.754\n0.959\n0.793\n0.736\n0.915\n0.824\n0.729\n0.874\n0.814\nLUCIR + Pseudo Labels\n0.762\n0.963\n0.786\n0.742\n0.921\n0.819\n0.734\n0.879\n0.806\nTable 2. Average AUROC, AUPR and FPR95 on CIFAR-100 with step size 5, 10 and 20.\nFigure 4. Overview of the baseline solution to learn the new\ntask i. h refers to the model in different steps and m denotes the\nnumber of learned classes so far after task i −1. Firstly, the hi−1\n(except the last fully connected layer) is applied to extract fea-\nture embeddings used for K-means clustering where the number\n1, 2, 3 denote the corresponding cluster assignments. In step 2,\nthe pseudo labels are obtained as 1+m, 2+m, 3+m respectively.\nFinally in step 3, the unlabeled data with pseudo label is used to-\ngether for continual learning without requiring human annotations.\n7. Implementation Detail\nIn this section, we provide the detail for methods im-\nplemented in experimental parts including (1) the method\nto perform unsupervised continual learning and (2) exist-\ning “post-hoc” OOD detection methods, which will be il-\nlustrated in Section 7.1 and Section 7.2, respectively.\n7.1. Unsupervised Continual Learning\nIn this work, we apply the baseline method proposed\nin [5] to perform unsupervised continual learning as shown\nin Figure 4, which includes three main steps: (1) Apply\nK-means clustering [18] on extracted features for all new\ntask data using lower layers of the continual learning model\nupdated from last incremental step. (2) Obtain the pseudo\nlabels based on the cluster assignments. (3) Perform unsu-\npervised continual learning and maintain the learned knowl-\nedge by using exemplar set [24] and knowledge distillation\nloss [11].\nIn our experimental part, we follow the same setting to\nuse ResNet-32 [8] for CIFAR-100 [13] as the backbone net-\nwork. The batch size of 128 with SGD optimizer, the initial\nlearning rate is 0.1. We train 120 epochs for each incremen-\ntal step and the learning rate is decreased by 1/10 for every\n30 epochs. Exemplar size is set as 2, 000.\n7.2. OOD detection\nIn our experiments, three existing ”post-hoc” OOD\ndetection methods are used for comparisons including\nMSP [10], ODIN [16] and Energy Score [17].\nMSP directly applies the trained classiﬁcation network\nto uses the maximum of softmax probability as the con-\nﬁdence score to discriminate between in-distribution and\nout-of-distribution data, which is regarded a strong base-\nline. Speciﬁcally, for each input data x, we obtain the soft-\nmax output using trained classiﬁcation model Fc. The con-\nﬁdence score is calculated as\nConf = Max(Softmax(Fc(x)))\nODIN further improves the performance by introduc-\ning the temperature scaling and input data pre-processing.\nSpeciﬁcally, they proposed to calculate softmax probability\nusing output value scaled by temperature T > 1 and use the\nmaximum as conﬁdence score.\nBesides,\nto\nincrease\nthe\ndifference\nbetween\nin-\ndistribution and out-of-distribution data, they pre-process\neach input data by adding small perturbation\n˜x = x −ϵsign(−∇xlog(Fc(x)\nT\n))\nThen, the ﬁnal conﬁdence score is given by\nConf = Max(Softmax(Fc(˜x)\nT\n))\nIn our experiment, we use ϵ = 0.001 and T = 1, 000.\nEnergy Score is proposed to replace the maximum of\nsoftmax probability as the conﬁdence score. Speciﬁcally,\nconsidering the dimension of output logits Fc(x) is K. The\nenergy function is used to calculate the conﬁdence score,\nwhich is deﬁned as\nConf = E(x) = −T × log(\nK\nX\ni\neFc(x)/T )\nwhere we use T = 1, 000 as temperature scaling.\n8. Additional Experimental Results\nFor all experiments shown in the paper, we apply the\nbaseline in [5] to perform unsupervised continual learning\nas illustrated in 7.1. In this section, we will demonstrate that\nour proposed OOD detection method can work with other\nexisting continual learning methods to achieve higher per-\nformance. Following [5], we apply LUCIR [12] + Pseudo\nLabels to perform unsupervised continual learning and the\nresults are shown in Table 2.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-04-12",
  "updated": "2022-04-12"
}