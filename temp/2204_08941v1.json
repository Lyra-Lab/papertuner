{
  "id": "http://arxiv.org/abs/2204.08941v1",
  "title": "CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex",
  "authors": [
    "Immanuel Trummer"
  ],
  "abstract": "CodexDB is an SQL processing engine whose internals can be customized via\nnatural language instructions. CodexDB is based on OpenAI's GPT-3 Codex model\nwhich translates text into code. It is a framework on top of GPT-3 Codex that\ndecomposes complex SQL queries into a series of simple processing steps,\ndescribed in natural language. Processing steps are enriched with user-provided\ninstructions and descriptions of database properties. Codex translates the\nresulting text into query processing code. An early prototype of CodexDB is\nable to generate correct code for a majority of queries of the WikiSQL\nbenchmark and can be customized in various ways.",
  "text": "CodexDB: Generating Code for Processing\nSQL Queries using GPT-3 Codex\nImmanuel Trummer\nCornell University\nIthaca, NY\nitrummer@cornell.edu\nABSTRACT\nCodexDB is an SQL processing engine whose internals can be cus-\ntomized via natural language instructions. CodexDB is based on\nOpenAI’s GPT-3 Codex model which translates text into code. It is\na framework on top of GPT-3 Codex that decomposes complex SQL\nqueries into a series of simple processing steps, described in natural\nlanguage. Processing steps are enriched with user-provided instruc-\ntions and descriptions of database properties. Codex translates the\nresulting text into query processing code. An early prototype of\nCodexDB is able to generate correct code for a majority of queries\nof the WikiSQL benchmark and can be customized in various ways.\nArtifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/itrummer/CodexDB.\n1\nINTRODUCTION\nModifying a database management system is hard. Systems such as\nPostgres feature millions of code lines. Understanding and chang-\ning that code requires expert knowledge in databases on top of\nadvanced coding skills. This prevents all but the most experienced\ndevelopers from creating customized versions.\nThis paper presents the vision behind CodexDB, a novel database\nmanagement system that can be customized without expert devel-\noper skills. Users specify natural language instructions, along with\ntheir queries, which influences code generated for query processing.\nThe enabling technology for this system is OpenAI’s GPT-3 Codex\nmodel. Codex is a large neural network, currently available via a\nprivate beta test, that translates natural language instructions into\ncode. This paper presents first experimental results and an outlook\non future steps.\nThe range of applications is vast. To name just a few, consider\nthe following use cases.\nExample 1.1. A developer wants to benchmark different data\nprocessing frameworks (e.g., Pandas and Vaex in Python or Table-\nsaw and Morpheus in Java) on a specific SQL workload and hard-\nware platform. Traditionally, doing so requires either modifying\nan existing database management system or writing query-specific\ncode from scratch. With CodexDB, that developer specifies queries,\ntogether with natural language instruction such as “Use pandas\nlibrary”. While CodexDB may not succeed at generating code for\neach workload query, obtaining performance results for a subset\ncan guide future development efforts. Also, generated code can be\nmanually validated and reused in case of recurrent queries.\nExample 1.2. A novice database user wants to gain a deeper\nunderstanding of how database management systems work. To\nthat purpose, the user would like to generate customized output\nafter each processing step (e.g., summarizing steps performed and\nshowing a small sample of intermediate results). Integrating such\nchanges into traditional systems is beyond the user’s capabilities.\nWith CodexDB, the user specifies natural language queries (which,\ninternally, are translated into SQL), together with a natural language\ndescription of desired per-step output.\nCodexDB accepts queries, together with natural language in-\nstructions, as input. These instructions customize the way in which\nqueries are executed. CodexDB generates code to process queries\nwhile complying with additional instructions. A first option is to\nsubmit queries and instructions directly to GPT-3 for code genera-\ntion. We will see in Section 4 that this approach does not work.\nInstead, CodexDB adapts techniques from classical query plan-\nning. It decomposes complex SQL queries into sequences of simple\nprocessing steps. In contrast to prior work, those steps are for-\nmulated in natural language using corresponding text templates.\nFinally, automatically generated plan steps are interleaved with\nuser-provided instructions. The resulting text is enriched with in-\nformation about the database schema and physical layout. The\nfinal text is submitted to GPT-3 Codex (as a so-called “prompt”).\nUsing this approach as a starting point, CodexDB generates code\nfor sample queries in a training step. The resulting code samples\ncan be integrated into prompts generated at run time to increase\nthe chances of success. An early prototype of CodexDB generates\ncorrect code in a majority of cases for a popular text-to-SQL bench-\nmark. Also, it is able to customize generated code using simple\ninstructions, inspired by the use cases outlined before.\nIn summary, the original scientific contributions in this paper\nare the following:\n• The paper presents the vision behind CodexDB, an analytical\nSQL engine that can be customized via natural language\ninstructions.\n• The paper discusses first experimental results, based on an\nearly prototype of CodexDB.\n• The paper outlines next steps and future research.\nThe remainder of this paper is organized as follows. Section 2\ndiscusses recent progress in natural language processing and com-\npares CodexDB to prior work. Section 3 describes the architecture\nof the first prototype. Section 4 reports first experimental results in\nmultiple scenarios. Section 5 discusses next steps and concludes.\n2\nBACKGROUND AND RELATED WORK\nCodexDB is enabled by recent advances in the domain of natural\nlanguage processing. Those advances have been fuelled by two key\nideas: a novel neural network architecture, the Transformer [23],\narXiv:2204.08941v1  [cs.DB]  19 Apr 2022\nImmanuel Trummer\nand new training paradigms, implementing the idea of transfer\nlearning [15]. The Transformer is nowadays the dominant archi-\ntecture in the domain of language processing [26]. Among other\nadvantages, it lends itself better to parallelization than prior meth-\nods. This has, in part, enabled the creation of very large, pre-trained\nlanguage models. Such models are pre-trained on tasks for which\nlarge amounts of training data are easily available, e.g. predicting\nthe next word in text snippets. While pre-training is very expen-\nsive, the resulting models can be easily specialized for new tasks\nvia different methods. Fine-tuning describes a process in which\npre-trained models are used as a starting point for further train-\ning on more specialized tasks (reducing the amount of training\nsamples and computational overheads by orders of magnitude via\npre-training [4]). Until recently, fine-tuning has been the primary\nmethod of exploiting pre-trained language models. The latest gen-\neration of pre-trained models, most notably OpenAI’s Generative\nPre-Trained Transformer (GPT) version 3, unlocks new possibilities.\nIt turns out that sufficiently large models can oftentimes solve new\ntasks without specialized training (“zero-shot learning”), based on\ninputs describing the task in natural language alone [1]. Precision\nincreases if the input integrates few (i.e., typically less than ten)\nexamples pairing tasks of the same type with solutions (“few-shot\nlearning”). This is the method currently used by CodexDB. The\nfinal development that enabled this paper is the emergence of the\nCodex variant of GPT-3 [2, 14]. The primary difference between\nGPT-3 Codex and the original GPT-3 model lies in the data used for\npre-training. GPT-3 Codex is trained on code and technical docu-\nmentation. This results in a model whose primary use case is the\ntranslation of natural language commands into code.\nCodexDB connects to prior work on natural language interfaces\nin the database community [9, 16, 25]. So far, the focus was on\n“democratizing access to data”, i.e. enabling lay users to work with\ndatabase systems. CodexDB goes one step further by “democratiz-\ning” the design of database system internals. The goal is to enable\nlay users to change system behavior to a degree that goes beyond\nthe configuration scope of traditional database systems (as well as\nmaking such changes easier for more advanced users).\nCodexDB relates to prior work exploiting machine learning [6,\n7, 13] and specifically Transformers [20, 21] in the context of data-\nbase systems. It connects broadly to prior work using GPT-3 for\nprogram synthesis [5, 11, 12]. It differs by its focus on customizable\nSQL query processing. Prior work on code generation for query\nprocessing [8, 24] cannot integrate natural language instructions.\n3\nSYSTEM OVERVIEW\nFigure 1 shows an overview of CodexDB. Users enter a query as well\nas natural language instructions, influencing the code generated\nfor query processing. The query is formulated either in SQL or in\nnatural language. In the latter case, the query is first translated into\nan SQL query via text-to-SQL methods [10, 18, 28].\nThe SQL query and natural language instructions form the input\nto the query planner. This planner differs from prior query planners\nby its output format. As the plan is translated into code by GPT-3 in\nthe following steps, the plan is formulated as a sequence of natural\nlanguage steps. User-provided natural language instructions are\nincluded as steps in such plans.\nCodexDB\nText-to-SQL (Optional)\nNatural Language Planner\nQuery\nInstructions\nPrompt Generator\nCode Generator (GPT-3)\nExecution Engine\nVerification\nCode Samples\nDB Catalog\nResult\nFigure 1: Overview of CodexDB prototype.\nTable 1: Text templates used during planning.\nPattern\nText Template\nX = Y\nCheck if T(X) equals T(Y)\nfrom X where Y\nFilter T(X) using T(Y)\nX as Y\nT(X) (aka. T(Y))\nselect X from Y\nCreate table with columns T(X) from T(Y)\nMore precisely, the planner treats the nodes in the query tree in\npost-order. Each node is translated into a processing step, formu-\nlated in natural language. To do so, the planner uses text templates\nthat are associated with specific node types. Table 1 shows ex-\nample templates (T(X) and T(Y) denote the text representation of\nexpressions X and Y respectively). As plan steps are numbered, inter-\nmediate results are referred to via the number of the step generating\nthem. The last step in the plan instructs GPT-3 to write the query\nresult into a file at a specified location.\nCurrently, CodexDB allows users to specify two types of instruc-\ntions: instructions that refer to the plan execution as a whole (e.g.,\ninstructions on which libraries to use for processing) as well as\ninstructions that are executed after each step (e.g., instructions\ndetermining customized logging output). Instructions of the former\ntype are pre-pended to the template-based processing steps (i.e.,\nthey become the first plan step) while instructions of the latter type\nare inserted after each processing step (i.e., the number of plan\nsteps doubles). Note that, currently, the planner does not perform\nany cost-based or heuristic optimization.\nCode generation is initiated by submitting a prompt to GPT-3\nfor completion. This prompt represents the start of a program that\nGPT-3 Codex tries to finish. The prompt integrates details about the\ndata in the database, extracted from the database catalog, including\nthe names of tables and their columns, as well as a path to the\ncorresponding files. This description is generated using a simple\ntext template with placeholders for column and table names. Also,\nCodexDB: Generating Code for Processing\nSQL Queries using GPT-3 Codex\n\"\"\"\nTable Data with columns 'Player','No_','Nationality','Position',\n'Years_in_Toronto','School_Club_Team', stored in 'Data.csv'.\nProcessing steps:\n1. Load data for table Data.\n2. Print progress updates.\n3. Check if 'Player' equals 'dell curry'.\n4. Print progress updates.\n5. Filter results of Step 1 using results of Step 3.\n6. Print progress updates.\n7. Create table with columns 'Years_in_Toronto'\n(aka. result ) from results of Step 5.\n8. Print progress updates.\n9. Write results of Step 7 to file 'result.csv' (with header).\n10. Print progress updates.\n\"\"\"\nFigure 2: Example prompt for code generation integrating a\ndescription of the database (blue), processing steps (black),\nand natural language instructions (red).\ndata = pd . read_csv ( ' Data . csv ' )\nprint ( ' Loaded Data ' )\ni s _ d e l l _ c u r r y\n= data [ ' Player ' ] ==\n' d e l l\ncurry '\nprint ( ' Checked\ni f\nPlayer\nequals\nd e l l\ncurry ' )\nd e l l _ c u r r y _ d a t a = data [ i s _ d e l l _ c u r r y ]\nprint ( ' F i l t e r e d\ndata\nf o r\nd e l l\ncurry ' )\nr e s u l t\n= d e l l _ c u r r y _ d a t a [ ' N a t i o n a l i t y ' ]\nprint ( ' Created\nt a b l e\nwith\nN a t i o n a l i t y\ncolumn ' )\nr e s u l t . to_csv ( ' r e s u l t . csv ' ,\nheader=True )\nprint ( ' Wrote to\nf i l e\nr e s u l t . csv ' )\nFigure 3: Code generated by GPT-3 in response to prompt\nfrom Figure 2 (code was shortened for readability by remov-\ning empty lines and comments).\nthe prompt integrates the aforementioned plan steps. The prompt is\npassed on to GPT-3 which answers with a piece of code. CodexDB\ntries to execute the code, and to read the generated query result. If\nthe code does not execute (or if it does not generate a result file),\nCodexDB executes up to a configurable number of retries. With\neach retry, the “temperature” (a Codex parameter determining the\ndegree of randomness in code generation) is increased to enable\nnew solutions. If successful, the result is returned to the user.\nCodexDB can be used in a zero-shot setting (i.e., it generates\ncode with instructions not seen before). Alternatively, it executes a\ntraining phase before run time with fixed instructions. The purpose\nof training is to generate a library of code samples, generated us-\ning the target instructions. During training, CodexDB uses sample\nqueries for which the query result is known. It retries code gen-\neration until the query result matches the known one (or until it\nreaches the maximal number of retries). At run time, a specified\nnumber of samples is randomly selected from that library and in-\ncluded into the prompt. Having examples in the prompt (“few-shot\nlearning”) increases the success probability, as shown in Section 4.\nExample 3.1. Consider the query Select \"Years_in_Toronto\"\nas Result from Data where \"Player\" = 'dell curry' from the\nWikiSQL benchmark. Assume a user enters this query, together with\nthe per-step instructions “Print progress updates”. Figure 2 shows\nthe prompt for this query, interleaving automatically generated\nprocessing steps with user instructions and providing context on the\ndatabase schema. Figure 3 shows the generated code. It processes\nthe query and writes the result into a file. While doing so, it prints\nout progress updates summarizing steps performed.\n4\nEXPERIMENTS\nThe goal of the experiments is threefold. First, to verify that CodexDB\ngenerates correct code in most cases. Second, to evaluate the degree\nto which code can be customized via natural language instructions.\nThird, to compare CodexDB to other baselines. Section 4.1 discusses\nthe experimental setup while Section 4.2 reports results.\n4.1\nSetup\nAll experiments are executed on an AWS EC2 instance of type\nt2.xlarge with 16 GB of RAM, four virtual CPUs, and 800 GB of\nEBS storage. The instance uses Amazon’s Deep Learning AMI\n(Version 53) and runs Ubuntu 18.04. CodexDB is implemented in\nPython 3 and accesses OpenAI’s GPT-3 Codex model via OpenAI’s\nPython API. The experiments use the “Cushman” and “Davinci”\nversions of Codex with an estimated parameter count of 6.7 billion\nand 175 billion parameters respectively [1, 19]. The generated code\nis in Python, the language both models are most capable in [14].\nThe following experiments compare CodexDB to baselines that\ntry translating natural language queries directly to code. This is\nthe most direct method of using GPT-3, making the comparison\ninteresting. Doing so requires a text-to-SQL benchmark that fea-\ntures natural language questions, along with corresponding queries.\nWe consider a subset of the WikiSQL benchmark [28], a popular\nbenchmark featuring over 80,000 queries with examples. The ex-\nperiments only consider up to the first hundred queries as treating\nall queries is prohibitively expensive1. The data on which queries\noperate is stored in the .csv format.\nThe experiments evaluating CodexDB focus on the key step of\ntranslating an SQL query into code, possibly with additional natural\nlanguage instructions. Translating natural language questions into\nSQL queries is a well studied problem. Corresponding results for\nthe WikiSQL benchmark are available [28] with recent methods\nachieving a precision of over 90 % [27]. We consider a test case\n(characterized by a natural language query with associated data) as\n“solved” if the generated program is executable and generates the\ncorrect result. This proxy for correctness is often used to evaluate\nnatural language query interfaces [17, 28]. A subset of generated\nprograms was manually validated as well. Unless noted otherwise,\nCodexDB retries generating a program once if the first generated\nprogram is not executable. If the first program executes but gener-\nates an incorrect result, the corresponding test case is not solved.\nCodexDB uses a temperature of zero for the first try and increases\nthe temperature (determining the degree of randomization during\ncode generation) by an amount determined by the formula 0.5/𝑁\nwhere 𝑁is the maximal number of allowed tries (typically two).\n1At the time of writing, OpenAI Codex is only available to beta testers and access is\nsubject to a rate limit of 20 requests per minute.\nImmanuel Trummer\nCushman\nDavinci\n0\n10\n20\n0\n0\n0\n0\n11\n22\nGPT-3 Codex variant\nNr. Solved\nQuestion Prompt\nQuery Prompt\nCodexDB Prompt\nFigure 4: Number of test cases solved out of 100 with-\nout prior training (“zero-shot”) for different models and\nprompts.\n1\n2\n3\n4\n5\n6\n7\n8\n9 10\n0\n20\n40\nNumber of tries\nNr. Solved\n-\nUse pandas\nUse vaex\nUse datatable\nPrint “Done.”\nPrint results\nPrint progress\nFigure 5: Number of test cases out of 50 solved during train-\ning for different instructions as function of the number of\ntries.\nTo test customization, we consider six natural language instruc-\ntions. Three of them focus on processing methods by instruct-\ning CodexDB to use specific libraries: “Use pandas library”, “Use\nvaex library”, and “Use datatable library”. The other three instruct\nCodexDB to generate specific logging output after each process-\ning step: “Print ’Done.’ ”, “Print intermediate results”, and “Print\nprogress updates”. The first three instructions are added once as\nfirst plan step. The last three are added after each step of the initial\nplan. Note that the following figures and tables abbreviate those\ninstructions slightly (e.g., in the figure legends).\n4.2\nResults\nFigure 4 reports results of an experiment comparing different prompt\ngeneration methods (on 100 queries from the WikiSQL test set).\n“CodexDB Prompt” refers to prompts generated by CodexDB (in-\ntegrating, in particular, natural language query plans). “Question\nPrompt” and “Query Prompt” integrate the same description of\nthe data source as CodexDB (i.e., table and column names) but\nreplace the natural language query plan by the natural language\nquestion or the correct SQL query respectively. Clearly, the prompts\nof CodexDB, enriched by query plans, are necessary to generate\ncorrect code. The Davinci model (which features most parameters)\nsolves significantly more test cases than the Cushman version. On\nthe other side, average generation times (seven seconds versus two\nseconds) are higher for Davinci.\nFigure 4 reports a success rate of 22% without prior training.\nLanguage models are often fine-tuned to increase performance for\nspecific tasks. This option is not yet available for the Codex series of\nGPT-3. Instead, we consider few-shot scenarios [1] in the following.\nHere, examples with solutions are integrated as part of the prompt.\n0\n2\n4\n0\n20\n40\n60\n80\n100\nNr. samples\nNr. Solved\nCushman Model\n0\n2\n4\nNr. samples\nDavinci Model\nQuestion Prompt\nQuery Prompt\nCodexDB Prompt\nFigure 6: Number of test cases solved out of 100 as a function\nof the number of training samples in prompt (“few-shot”).\nFigure 5 reports the results of a preparation run, using 50 queries\nfrom the WikiSQL training set and the Davinci model. As training\nis executed before run time, up to ten tries are allowed. Further-\nmore, it is assumed that solutions for training samples are available,\nallowing to stop code generation only if the execution result is\ncorrect (as opposed to using the first executable code). Figure 6\nreports solved test cases as a function of the (maximal) number of\ntries. Different lines are associated with additional natural language\ninstructions (“-” designates no additional instructions). Training\ntook between 1510 seconds (when instructed to use the pandas\nlibrary) and 8,300 seconds (with instructions “print intermediate\nresults”). Given enough tries and results to compare to, CodexDB\nsolves 80% of test cases without additional instructions.\nFigure 6 reports number of test cases solved (out of 100 queries\nfrom the WikiSQL test set, i.e. no overlap with pre-generated sam-\nples) as a function of the number of samples included in the prompt.\nIt compares the previously introduced prompt styles. Clearly, per-\nformance improves significantly (e.g., from around 20 to around\n80% for Davinci) when adding samples. Adding samples decreases\nthe gap between CodexDB’s and other prompts. Still, the CodexDB\nprompt performs best except for four samples and the Cushman\nmodel. The reason is the slightly longer prompts of CodexDB (fea-\nturing query plans) that exceed the maximum input size for the\nCushman model for 67 test cases. Davinci supports larger inputs\nand does not suffer from this problem. Unless noted otherwise, the\nremaining experiments use two samples and the Davinci model\n(the configuration leading to maximal performance in Figure 6).\nWe test customization by adding the instructions described in\nSection 4.1. Figure 7 reports the number of test cases solved with\ndifferent instructions. In most cases, adding more instructions tends\nto decrease success ratio for the largest model. Interestingly, the\nimpact varies across instructions. In particular, asking CodexDB to\nuse the pandas library slightly increases performance. This seems\nreasonable as the pandas library is popular (i.e., the training set of\nGPT-3 Codex likely includes various example codes) and supports\noperations similar to SQL operators. Manual analysis of the first 20\nprograms generating the correct result shows that they are indeed\ncorrect. Table 2 reports statistics on the size of generated code\n(and on the size of the corresponding SQL queries), measured in\ncharacters. The average size of code generated by CodexDB is larger\nby up to one order of magnitude, compared to SQL queries. This\nillustrates the difficulty of the task. Adding instructions on logging\nincreases code size (due to print statements after processing steps).\nCodexDB: Generating Code for Processing\nSQL Queries using GPT-3 Codex\nTable 2: Code length in characters for different languages\nand instructions (only considering executable programs).\nLanguage\nInstructions\nMin\nMedian\nMax\nSQL\n-\n42\n77\n227\nPython\n-\n276\n545\n1110\nUse pandas library\n284\n438.5\n782\nUse vaex library\n355\n637\n1018\nUse datatable library\n307\n437\n848\nPrint “Done.”\n390\n724\n1388\nPrint intermediate results\n458\n875\n1734\nPrint progress updates\n585\n836\n1458\n60\n65\n70\n75\n6769\n74\n60\n64\n69\n72\nNr. Solved\nCushman-Codex Model\n60\n70\n80\n7981\n76\n58\n6971\n77\nNr. Solved\nDavinci-Codex Model\n-\nUse pandas\nUse vaex\nUse datatable\nPrint “Done.”\nPrint results\nPrint progress\nFigure 7: Number of test cases solved out of 100 for different\nnatural language instructions.\ncsv\npandas\nvaex\ndatatable\n0\n50\n100\n34\n66\n0\n0\n0\n100\n0\n0\n16\n61\n100\n0\n3\n15\n0\n100\nImported libraries\nNr. Programs\nDavinci-Codex Model\n-\nUse pandas\nUse vaex\nUse datatable\nFigure 8: Number of generated programs out of 100 import-\ning specific libraries for library-related instructions.\nSo far, we discussed correctness. Next, we examine whether\nadditional instructions are reflected in the generated programs.\n0\n1\n2\n3\nExecution Time (s)\nScaling Factor=1,000\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n50\n100\nExecution Time (s)\nScaling Factor=1,000,000\n-\nUse pandas\nUse vaex\nUse datatable\nDBMS\nFigure 9: Execution time of programs generated by CodexDB\nwith different instructions and of one traditional DBMS.\nTable 3: Total run time for queries solved by all baselines.\nBaseline\nTime (s)\nSF: 1K\nSF: 1M\nCodexDB: -\n3\n196\nCodexDB: Use pandas library\n3\n119\nCodexDB: Use vaex library\n12\n240\nCodexDB: Use datatable library\n2\n51\nDBMS\n1\n368\nFigure 8 reports the number of generated programs (out of 100)\nthat import certain libraries. Without specific instructions, 34%\nof generated programs import the “csv” library while 66% import\npandas. Incorporating instructions to use pandas, vaex, or datatable\ninto the prompt ensures that each generated programs imports the\nassociated library. In some cases, in particular for vaex, programs\nimport multiple libraries (both, csv and pandas). Manual inspection\nof the generated code reveals that some of those programs contain\nredundancy (e.g., by importing data using vaex, then transforming\ninto pandas data frames). While this subset of programs formally\nsatisfies the instructions (they import, i.e. “use”, the corresponding\nlibrary), they do not entirely reflect its spirit.\nFigure 9 reports execution time measurements for programs gen-\nerated with different instructions for the ten first queries. Missing\nbars indicate that no correct program was generated for the corre-\nsponding query. The data sets of the WikiSQL benchmark are too\nsmall for meaningful performance measurements. Hence, data were\nscaled by factor 1,000 and by factor 1,000,000 (by simply duplicating\nrows). The resulting data sets have an average size of 1.2 GB and\n15 million rows. Table 3 reports total execution time for all of the\naforementioned queries for which correct programs were gener-\nated for all possible instructions. Clearly, instructing CodexDB to\nImmanuel Trummer\nany\n“Done.”\nstring\nvariable\n50\n100\n2\n0\n0\n0\n100\n100\n100\n0\n100\n0\n25\n100\n100\n0\n100\n13\nPrint statements\nNr. Programs\nDavinci-Codex Model\n-\nPrint “Done.”\nPrint results\nPrint progress\nFigure 10: Number of generated programs containing spe-\ncific types of print statements for output-related instruc-\ntions.\nuse different libraries has significant impact on performance. This\nindicates that the generated code is fundamentally different. Finally,\ntime measurements are provided for a traditional, widely used,\ndatabase management system. To ensure a fair comparison, time\nmeasurements include time for loading data from disk, processing\nthe query, and writing the result back to disk (the generated code\nimplements the same tasks). While performance is not the primary\ngoal of CodexDB, the generated code is reasonably efficient.\nFigure 10 refers to logging-related instructions. The figure shows\nhow many out of 100 generated programs contain certain types of\nprint commands, distinguished by the operand. The figure considers\npresence of any print commands, commands printing out “Done.”,\ncommands printing hard-coded strings, and commands printing\nout variables. Without further instructions, only 2% of generated\nprograms contain any print statements. This ratio increases to 100%\nfor any of the logging-related instructions. Instructing CodexDB to\nprint “Done.” after each step is reflected by the presence of corre-\nsponding print commands in each program. Instructing CodexDB\nto print intermediate results ensures that each generated program\nprints out variables. Requiring progress updates leads to programs\nprinting out hard-coded strings in all (100%) and printing out vari-\nables in some (13%) cases. Note that this instruction leaves room\nfor interpretation (as the form of progress updates is not specified).\nManual inspection reveals that most generated code includes print\ncommands after each step, outlining the action performed at a high\nlevel of abstraction. Figure 3 from Section 3 shows a corresponding\nexample.\n5\nCONCLUSION AND OUTLOOK\nCodexDB blurs the line between user and developer. It enables\nfar-ranging customization via natural language commands. Ex-\nperiments with a first prototype are promising but also hint at\nsignificant potential for improvements.\nFirst, CodexDB generates correct code in most but not in all\ncases (up to 81% of queries are solved, depending on scenario and\nmodel). A success rate of 100% is illusory for any kind of natural\nlanguage interfaces. Still, the newest generation of text-to-SQL\nmethods achieves a precision of more than 90% on the same bench-\nmark. Hence, increasing the precision of CodexDB will be a primary\nresearch goal in the near term.\nSecond, customizing code via natural language instructions works\nbut sometimes in unexpected ways. For instance, given instructions\nto use specific libraries, CodexDB always imports (“uses”) them\nindeed. However, in a minority of cases, imported libraries are\nnot ultimately “used” for processing query steps. This motivates\nstronger mechanisms allowing users to enforce a specific interpre-\ntation of their natural language input.\nThe natural language query planner does not yet use cost-based\noptimization. This is acceptable for the simple queries of the Wik-\niSQL benchmark. To handle complex queries with many joins, fu-\nture versions will integrate optimization according to cost models\n(e.g., based on the number of tuples processed) that have been\nshown to work quite well across different physical operator im-\nplementations [3]. Alternatively, machine learning can be used to\noptimize generated query plans for specific workloads [13, 22].\nREFERENCES\n[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-\npher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language models are few-shot learners. Advances in\nNeural Information Processing Systems 2020-Decem (2020). arXiv:2005.14165\n[2] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brock-\nman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-\ntios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shan-\ntanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike,\nJosh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight,\nMiles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario\nAmodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu-\nating Large Language Models Trained on Code. http://arxiv.org/abs/2107.03374\n(2021). arXiv:2107.03374 http://arxiv.org/abs/2107.03374\n[3] Andrey Gubichev, Peter Boncz, Alfons Kemper, and Thomas Neumann. 2015.\nHow good are query optimizers, really? PVLDB 9, 3 (2015), 204–215.\n[4] Neil Houlsby, Andrei Giurgiu, Stanisraw Jastrzçbski, Bruna Morrone, Quentin\nde Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. 36th International Conference on\nMachine Learning, ICML 2019 2019-June (2019), 4944–4953. arXiv:1902.00751\n[5] Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh\nParthasarathy, Sriram Rajamani, and Rahul Sharma. 2021. Jigsaw: Large Language\nModels meet Program Synthesis. Vol. 1. Association for Computing Machinery.\n1–12 pages. arXiv:2112.02969 http://arxiv.org/abs/2112.02969\n[6] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, and\nAlfons Kemper. 2018. Learned cardinalities: estimating correlated joins with deep\nlearning. In CIDR. arXiv:1809.00677 http://arxiv.org/abs/1809.00677\n[7] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2017.\nThe Case for Learned Index Structures. 1 (2017), 1–30. https://doi.org/10.1145/\n2348283.2348367 arXiv:1712.01208\n[8] Konstantinos Krikellas, Stratis D. Viglas, and Marcelo Cintra. 2010. Generating\ncode for holistic query evaluation. In ICDE. IEEE, 613–624. https://doi.org/10.\n1109/ICDE.2010.5447892\n[9] Fei Li and HV Jagadish. 2014. NaLIR: an interactive natural language interface\nfor querying relational databases. SIGMOD (2014), 709–712. https://doi.org/10.\n1145/2588555.2594519\n[10] Fei Li and HV Jagadish. 2016. Understanding natural language queries over\nrelational databases. SIGMOD Record 45, 1 (2016), 6–13.\n[11] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,\nand Rémi Leblond. 2022. Competition-Level Code Generation with AlphaCode.\nDeepMind Technical Report (2022), 1–73.\n[12] Pietro Liguori, Erfan Al-Hossami, Domenico Cotroneo, Roberto Natella, Bo-\njan Cukic, and Samira Shaikh. 2022. Can We Generate Shellcodes via Natural\nLanguage? An Empirical Study. arXiv:2202.03755v1 (2022). arXiv:2202.03755\nhttp://arxiv.org/abs/2202.03755\nCodexDB: Generating Code for Processing\nSQL Queries using GPT-3 Codex\n[13] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2018. Neo: A Learned\nquery optimizer. PVLDB 12, 11 (2018), 1705–1718.\nhttps://doi.org/10.14778/\n3342263.3342644 arXiv:1904.03711\n[14] OpenAI. 2021. https://openai.com/blog/openai-codex/.\n[15] Sebastian Ruder, Matthew E Peters, Swabha Swayamdipta, and Thomas Wolf.\n2019. Transfer Learning in Natural Language Processing. In ACL: Tutorials.\n15–18.\n[16] Diptikalyan Saha, Avrilia Floratou, Karthik Sankaranarayanan, Umar Farooq\nMinhas, Ashish R Mittal, and Fatma Ozcan. 2016. ATHENA: An ontology-driven\nsystem for natural language querying over relational data stores. VLDB 9, 12\n(2016), 1209–1220.\n[17] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. PICARD:\nParsing Incrementally for Constrained Auto-Regressive Decoding from Language\nModels. (2021), 9895–9901. https://doi.org/10.18653/v1/2021.emnlp-main.779\narXiv:2109.05093\n[18] Jaydeep Sen, Chuan Lei, Abdul Quamar, Fatma Özcan, Vasilis Efthymiou, Ayushi\nDalmia, Greg Stager, Ashish Mittal, Diptikalyan Saha, and Karthik Sankara-\nnarayanan. 2020. ATHENA++: natural language querying for complex nested\nSQL queries. Proceedings of the VLDB Endowment 13, 12 (2020), 2747–2759.\nhttps://doi.org/10.14778/3407790.3407858\n[19] Richard Shin and Benjamin Van Durme. 2021. Evaluating the Text-to-SQL Capa-\nbilities of Large Language Models. arXiv preprint arXiv:2112.08696 (2021).\n[20] Sahaana Suri, Ihab Ilyas, Christopher Re, and Theodoros Rekatsinas. 2021.\nEmber : No-Code Context Enrichment via similarity-based keyless joins.\narXiv:2106.01501v1 (2021). arXiv:arXiv:2106.01501v1\n[21] Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam\nMadden, and Mourad Ouzzani. 2021. Rpt: Relational pre-trained transformer is\nalmost all you need towards democratizing data preparation. In Proceedings of the\nVLDB Endowment, Vol. 14. 1254–1261. https://doi.org/10.14778/3457390.3457391\narXiv:2012.02469\n[22] Immanuel Trummer, Junxiong Wang, Deepak Maram, Samuel Moseley, Saehan\nJo, and Joseph Antonakakis. 2019. SkinnerDB: regret-bounded query evaluation\nvia reinforcement learning. In SIGMOD. 1039–1050.\n[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Processing Systems 2017-Decem, Nips\n(2017), 5999–6009. arXiv:1706.03762\n[24] Skye Wanderman-Milne and Nong Li. 2014. Runtime Code Generation in Cloud-\nera Impala. IEEE Data Engineering Bulletin 37, 1 (2014), 31–37. http://dblp.uni-\ntrier.de/db/journals/debu/debu37.html#Wanderman-MilneL14\n[25] Nathaniel Weir, Andrew Crotty, Alex Galakatos, Amir Ilkhechi, Shekar Ra-\nmaswamy, Rohin Bhushan, Ugur Cetintemel, Prasetya Utama, Nadja Geisler,\nBenjamin Hättasch, Steffen Eger, and Carsten Binnig. 2019. DBPal: Weak Su-\npervision for Learning a Natural Language Interface to Databases. (2019), 1–4.\narXiv:1909.06182 http://arxiv.org/abs/1909.06182\n[26] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language\nProcessing. In EMNLP. 38–45. https://doi.org/10.18653/v1/2020.emnlp-demos.6\narXiv:arXiv:1910.03771v5\n[27] Kuan Xuan, Yongbo Wang, Yongliang Wang, Zujie Wen, and Yang Dong. 2021.\nSeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising. (2021).\narXiv:2105.07911 http://arxiv.org/abs/2105.07911\n[28] Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2SQL: Generating\nStructured Queries from Natural Language using Reinforcement Learning. (2017),\n1–12. arXiv:1709.00103 http://arxiv.org/abs/1709.00103\n",
  "categories": [
    "cs.DB",
    "cs.CL",
    "cs.LG",
    "H.2.4"
  ],
  "published": "2022-04-19",
  "updated": "2022-04-19"
}