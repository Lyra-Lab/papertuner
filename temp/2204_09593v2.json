{
  "id": "http://arxiv.org/abs/2204.09593v2",
  "title": "COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks",
  "authors": [
    "Fangyi Zhu",
    "See-Kiong Ng",
    "Stéphane Bressan"
  ],
  "abstract": "Vision outlooker improves the performance of vision transformers, which\nimplements a self-attention mechanism by adding an outlook attention, a form of\nlocal attention.\n  In natural language processing, as has been the case in computer vision and\nother domains, transformer-based models constitute the state-of-the-art for\nmost processing tasks. In this domain, too, many authors have argued and\ndemonstrated the importance of local context.\n  We present an outlook attention mechanism, COOL, for natural language\nprocessing. COOL, added on top of the self-attention layers of a\ntransformer-based model, encodes local syntactic context considering word\nproximity and more pair-wise constraints than dynamic convolution used by\nexisting approaches.\n  A comparative empirical performance evaluation of an implementation of COOL\nwith different transformer-based models confirms the opportunity for\nimprovement over a baseline using the original models alone for various natural\nlanguage processing tasks, including question answering. The proposed approach\nachieves competitive performance with existing state-of-the-art methods on some\ntasks.",
  "text": "COOL, a Context Outlooker, and its Application to Question Answering\nand other Natural Language Processing Tasks\nFangyi Zhu1 , See-Kiong Ng1 and St´ephane Bressan1\n1National University of Singapore\nzhufy1030@gmail.com,{seekiong, steph}@nus.edu.sg\nAbstract\nVision outlooker improves the performance of\nvision transformers, which implements a self-\nattention mechanism by adding an outlook atten-\ntion, a form of local attention.\nIn natural language processing, as has been the case\nin computer vision and other domains, transformer-\nbased models constitute the state-of-the-art for\nmost processing tasks. In this domain, too, many\nauthors have argued and demonstrated the impor-\ntance of local context.\nWe present an outlook attention mechanism,\nCOOL, for natural language processing. COOL,\nadded on top of the self-attention layers of a\ntransformer-based model, encodes local syntactic\ncontext considering word proximity and more pair-\nwise constraints than dynamic convolution used by\nexisting approaches.\nA comparative empirical performance evaluation\nof an implementation of COOL with different\ntransformer-based models conﬁrms the opportunity\nfor improvement over a baseline using the origi-\nnal models alone for various natural language pro-\ncessing tasks, including question answering. The\nproposed approach achieves competitive perfor-\nmance with existing state-of-the-art methods on\nsome tasks.\n1\nIntroduction\nTransformer neural networks, based solely on the self-\nattention mechanism [Vaswani et al., 2017], have been shown\nto be remarkably successful in handling various tasks in\nnatural language processing (NLP) [Lan et al., 2019; He\net al., 2021] and other domains [Dosovitskiy et al., 2021;\nSun et al., 2019; Dong et al., 2018]. Self-attention, in a nut-\nshell, considers every token in the input sequence when en-\ncoding a current token. This strategy allows the learning and\ncapture of cross-passage long-range dependencies.\nThe interrogation naturally arises of whether self-attention\nis detrimental to local information and of how, if the case,\nthis ﬂaw can be remedied. Several authors have considered\nand evaluated various ways of incorporating local context in-\nformation back into a self-attention mechanism for various\nFigure 1: Different modes of integration of global and local encod-\ning.\ntasks in various domains [Wu et al., 2019; Gulati et al., 2020;\nYuan et al., 2021; Xiao et al., 2021].\nRecently, Yuan et al. [2021] proposed Vision Outlooker\n(VOLO) for visual recognition. VOLO successfully improves\nthe performance of vision transformers by adding a light-\nweight outlook attention mechanism, a form of local atten-\ntion.\nWe present and evaluate the outlook attention mechanism\nfor NLP, named Context Outlooker (COOL). COOL imple-\nments a context outlook attention mechanism that encodes\nthe local syntactic context considering word proximity and\ncan be inserted into most transformer-based models. Com-\npared to the operations for local augmentation used by exist-\ning works, outlook attention considers more pair-wise con-\nstraints as it processes multiple local windows containing an\nanchor token. We consider three possible modes for the com-\nposition of global and local encoding, in which the global en-\ncoding precedes, Global-to-Local, follows, Local-to-Global,\nor is parallel to, Global-and-Local, the local encoding.\nComparative empirical performance evaluation of COOL\nwith different transformer-based approaches conﬁrms the op-\nportunity for improvement over a baseline using the original\nmodels for eleven NLP tasks, including question answering,\nquestion generation, sentiment analysis, etc.\narXiv:2204.09593v2  [cs.CL]  15 May 2023\n2\nRelated Work\nTransformer-based\nlanguage\nmodels.\nVaswani\net\nal. [2017] ﬁrst proposed the Transformer architecture, based\nsolely on an attention mechanism, for neural machine trans-\nlation (NMT). Since Bidirectional Encoder Representation\nfrom Transformer (BERT) [Devlin et al., 2019] outstandingly\ntackled eleven NLP tasks, ﬁne-tuning a transformer-based\nlanguage model has become the mainstream approach for\nmost NLP tasks. The transformer-based structure has been\nexplored for other domains as well and achieved competitive\nperformance with classical convolutional neural network-\n(CNN-) or recurrent neural network- (RNN-) based models,\nsuch as Vision Transformer (ViT) for computer vision\n(CV) [Dosovitskiy et al., 2021], Bidirectional Encoder\nRepresentations from Transformers for Sequential Recom-\nmendation (BERT4Rec) for recommendation system [Sun\net al., 2019], and Speech-Transformer for automatic speech\nrecognition (ASR) [Dong et al., 2018].\nThe self-attention mechanism, an operation for cross-\npassage encoding derived from [Cheng et al., 2016], is the\ncrucial module in a transformer-based architecture.\nIt re-\nlates all the elements in a sequence to capture long-range\ndependencies [Bahdanau et al., 2014].\nNonetheless, there\nare two drawbacks of long-range encoding: the computation\nis expensive, especially for long sequences, and the cross-\npassage encoding might dilute the local information within\nthe surrounding context.\nTo address these issues, recent\nworks from different domains proposed to exploit an adaptive\nspan [Hofst¨atter et al., 2020; Yang et al., 2018; Sukhbaatar et\nal., 2019], or augment local encoding, e.g., [Wu et al., 2019;\nZhang et al., 2020; Jiang et al., 2020] for NLP, [Liu et al.,\n2021b; Yuan et al., 2021] for CV, and [Gulati et al., 2020] for\nASR.\nLocal attention.\nThe dilution of local information in a\nstandard self-attention mechanism for NLP and the beneﬁt\nof its reinstatement was ﬁrst discussed for NMT by Wu et\nal. [2019]. They presented the dynamic convolution to re-\nplace global encoding with the self-attention layers. They\ndemonstrated that only encoding the local context could out-\nperform executing a global encoding via self-attention.\nSeveral works followed that tried to integrate both local,\nleveraging the dynamic convolution technique, and global en-\ncoding, leveraging the self-attention mechanism. Zhang et\nal. [2020] presented a novel module where a dynamic convo-\nlution and a self-attention mechanism were placed in parallel\nfor NMT. Jiang et al. [2020] further connected a self-attention\nmechanism and a dynamic convolution in series to form a\nmixed-attention mechanism.\nRecently, in the area of CV, Yuan et al. [2021] and Xiao et\nal. [2021] demonstrated that a signiﬁcant factor limiting\nthe performance of the transformer-based fashion for vi-\nsual tasks was the low efﬁcacy of the self-attention mech-\nanism in encoding the local features as well.\nMoreover,\nYuan et al. [2021] proposed VOLO for efﬁcient pixel to-\nken aggregation via an outlook attention mechanism, with\nwhich a transformer-based model could outperform the clas-\nsical CNN-based models on visual recognition and segmen-\ntation. Compared to the standard dynamic convolution tech-\nnique, handling a single constraint within the current window\nonly, the outlook attention mechanism deals with more pair-\nwise constraints for the current token encoding. Speciﬁcally,\nit considers all local windows containing the current token.\n3\nMethodology\nWe present COOL, an outlook attention mechanism and ar-\nchitecture, that helps emphasise local information in NLP us-\ning a transformer-based model.\nFor the sake of clarity, we present the proposed model for\na speciﬁc task of extractive question answering.\n3.1\nArchitecture\nYuan et al. [2021] argued the importance of local informa-\ntion for visual tasks and proposed VOLO to overcome the\nweakening of local information in representations produced\nby the standard self-attention mechanism. The outlook atten-\ntion mechanism, the primary component of VOLO, dynam-\nically ﬁne-tunes pixel token features produced by the self-\nattention mechanism considering local information from sur-\nrounding pixel tokens.\nWe present to generalise this idea into NLP as the context\noutlooker (COOL). The overall architecture has three major\nmodules, global encoding by a generic transformer-based en-\ncoder, local encoding via the proposed context outlooker, and\noutput layers for the downstream tasks. The output layers for\nan extractive question answering task are for answer predic-\ntion.\nThere are three different modes for integrating global and\nlocal encoding illustrated in Figure 1,\n• Global-to-Local (G2L): ﬁrst encodes the question and\npassage by the global encoder. Then, the yielded global\nembeddings go through the context outlooker to enrich\nthe local context information. Finally, the start and end\npositions of the answer are predicted in the output layers.\n• Local-to-Global (L2G): switches the order of global en-\ncoding and local augmentation in the ﬁrst structure. In\ncontrast to G2L, L2G ﬁrst conducts local encoding, fol-\nlowed by global encoding.\n• Global-and-Local (G&L): executes global and local en-\ncoding parallelly and fuses the integrated global and lo-\ncal embeddings with several linear layers. Finally, pre-\ndict the answer locations.\nNext, we elaborate the workﬂow in light of the ﬁrst struc-\nture of G2L in Figure 2.\n3.2\nGlobal Encoder\nGiven a question q =\n\b\nq1, ..., qM\t\nand a passage p =\n\b\np1, ..., pN\t\n, M, N denotes the total number of tokens in the\nquestion and passage. The global encoder, as a backbone of\nour proposed system, can be the encoder in any transformer-\nbased model, e.g., BERT [Devlin et al., 2019], ALBERT [Lan\net al., 2019], and RoBERTa [Zhuang et al., 2021] with an\nencoder-only architecture, or ProphetNet [Qi et al., 2020]\nwith an encoder-decoder architecture.\nhg = Global([q; p])),\n(1)\nMultiplication\nTransformer\nConvolutional\nBlock\nLinear Layers\nLinear Layers\n…\nValue Matrice\n…\nOutlook Attentions\nLinear Layers\nUnfold\nUnfold\nFold\nReshape\n…\nSoftmax\nContext Outlook Layer X N\nLinear Layers\nLinear Layers\n...\nT1\nT2\nT3\nT4\nT5\nT6\n...\n(a) Self-attention Mechanism\nwindow\n...\nT1\nT2\nT3\nT4\nT5\nT6\n...\n(b) Convolution or Dynamic Convolution\nwindow\n...\nT1\nT2\nT3\nT4\nT5\nT6\n...\n(c) Outlook Attention\nwindow1\nwindow2\nwindow3\nContext Outlooker\nGlobal Encoder\nOutput Layers\nQuestion\nPassage\nStart \nPosition\nEnd \nPosition\nContext Outlooker Block\nFigure 2: Diagram of our proposed system with a G2L structure. There are three major components: ﬁrst, global encoding by a transformer-\nbased encoder; second, local augmentation by the context outlooker; third, answer prediction by the output layers. Unfold and Fold correspond\nto unfold and fold operations in PyTorch.\nwhere hg ∈RL×H is the obtained global representations, L\nis the max length of the input sequence, and H is the size of\nhidden units in the last layer of the global encoder.\n3.3\nContext Outlooker\nContext outlooker, the pivotal component in our proposed ap-\nproach, consists of a convolutional block and a context out-\nlook block.\nConvolutional Block\nThe convolution operation is effective and stable for capturing\nlocal information [Kalchbrenner et al., 2014]. Several works\nhave proved that inserting convolutional layers can help trans-\nformers in applications to different domains [Wu et al., 2019;\nXiao et al., 2021; Karpov et al., 2020; Liu et al., 2021a;\nGulati et al., 2020]. We thus try to insert a convolutional\nblock for local augment, and further compare the convolution\ntechnique to our proposed module.\nWe stack the convolutional block, consisting of several\nconvolutional layers, on top of the global encoder,\nhc = ReLU(ConvBlock(hg)),\n(2)\nhc =\n\u0002\nh1\nc, ..., hd\nc, ...hD\nc\n\u0003\n,\n(3)\nˆhc = Concat(AdaptivePool(hc)).\n(4)\nIn Eq. (3), hd\nc is the output of the d-th convolutional layer with\nReLU non-linear activation operation [Glorot et al., 2011],\nand D is the number of convolutional layers in the block. The\ndimension of hd\nc is Ld × F d. Ld depends on the kernel size\nand padding size of the convolutional layer, and F d is the\nnumber of ﬁlters in the convolutional layer. To align different\nLd to original length L, we perform adaptive pooling after\neach convolutional layer interpreted in Eq.(4). Finally, the\noutputs from all convolutional layers are concatenated along\nthe second dimension to produce ˆhc ∈RL×F ,\nF =\nX\n1≤d<D\nF d.\n(5)\n0Unfold extracts sliding local blocks from a batched input tensor.\nFold, the reverse operation of unfold, combines an array of sliding\nlocal blocks into the original large containing tensor.\nContext Outlook Block\nThe context outlook block, consisting of several context out-\nlook layers, follows by the convolutional block.\nIn analogy with the feature map of an image, each row in\nthe representation matrix ˆhc is the representation of a token.\nTo maintain the integrality of representation of each token,\nwe adapt the local window size in the visual outlook attention\nmechanism to K × F to handle text tokens instead of K × K\nfor processing image pixel tokens.\nSimilar to the computation of Query, Key, Value matrices\nin the self-attention mechanism, the outlook attention mecha-\nnism ﬁrst produces value matrix v ∈RL×F via a linear layer\nwith ˆhc from previous convolutional block,\nv = Wv ˆhc + bv,\n(6)\nwhere Wv and bv separately denote the parameters and bias\nneeded to be learned in the linear layer.\nHence, for the i-th token, the value matrices of its neigh-\nbour tokens in a local window with the size of K × F are,\nv∆i = vi+r−⌊K\n2 ⌋,\n0 ≤r < K,\n(7)\nv∆i ∈RK×F , and ⌊·⌋is the round down formula.\nIn visual outlook attention, Yuan et al. [2021] proposed to\nproduce the aggregating weights directly from the features of\nthe centric token to dispense the expensive dot product com-\nputation in self-attention, as each spatial location in an im-\nage feature map can represent its close neighbours. Likewise,\nthe embedding of a token in a sentence is able to express its\nneighbour tokens as well. Therefore, following their work,\nwe calculate attention matrix a ∈RL×A with ˆhc via another\nlinear layer,\na = Wa ˆhc + ba,\n(8)\nhere A = K × K × F. Wa and ba are learned parame-\nters and bias. Then, the attention weights within different\nlocal windows are obtained by reshaping a from RL×A to\nRL×K×(K×F ) , a∆i ∈R1×1×(K×F ) represents the obtained\nattention weights of i-th token within a local window.\nTherewith, normalise the attention weights in each local\nwindow with a softmax function and aggregate the neigh-\nbours within the window with the acquired attention weights,\nˆv∆i = softmax(a∆i)v∆i.\n(9)\nAfterward, sum up the weighted values at the same position\nfrom different windows,\n˜vi =\nX\n0≤r<K\nˆvi\n∆i+r−⌊K\n2 ⌋,\n(10)\nwhere ˜vi is the acquired representation for i-th token with rich\nlocal context information, and ˜v is the representation matrice\nfor the whole sequence.\nFinally, conduct a residual operation [He et al., 2016] that\nsum the encoded ˜v with ˆhc, and go through several linear lay-\ners for further representation learning, followed by perform-\ning another residual operation,\nhf = ˜v + ˆhc,\n(11)\nˆhf = Wfhf + bf,\n(12)\nh\n′\nf = ˆhf + hf.\n(13)\n3.4\nOutput Layers\nWe stack linear layers to perform answer prediction with the\nobtained h\n′\nf,\nys = softmax(Wsh\n′\nf + bs),\n(14)\nye = softmax(Weh\n′\nf + be),\n(15)\nWs, We and bs, be are parameters and bias needed to be\nlearned in the linear layers. ys, ye are the probability distri-\nbutions for the start position and the end position.\nThe training objective is to minimize the sum of the nega-\ntive log probabilities at the ground truth start position as and\nend position ae,\nL = −log(yas\ns + yae\ne ).\n(16)\n4\nExperiments\nWe evaluate the proposed system for extractive question\nanswering on the Stanford Question Answering Dataset\n(SQuAD), i.e., SQuAD 2.0 [Rajpurkar et al., 2018] and\nSQuAD 1.1 [Rajpurkar et al., 2016], with no additional train-\ning data. We use the standard metrics for question answering,\nF1 measure and exact match (EM).\nWe conduct extensive experiments with other NLP tasks\nand other language to evaluate our proposed approach’s ver-\nsatility and signiﬁcance.\n4.1\nSetup\nWe leverage the encoder in BERT-base1, ALBERT-base2,\nRoBERTa-base3, and ProphetNet4 for text generation as the\nglobal encoder and keep their default settings. The max se-\nquence length in our experiments is set to 384.\n1bert-base-cased: https://huggingface.co/bert-base-cased\n2albert-base-v2: https://huggingface.co/albert-base-v2\n3roberta-base-squad2: https://huggingface.co/roberta-base\n4https://github.com/microsoft/ProphetNet\nArchitecture\nOutlook Layers\nF1\nEM\nBERT\n-\n75.41\n71.78\nG2L w/o Conv.\n3\n76.79\n73.36\nG2L w/ Conv.\n2\n77.11\n74.02\nL2G w/o Conv.\n3\n74.80\n72.02\nL2G w/ Conv.\n2\n50.06\n49.98\nG&L w/o Conv.\n3\n75.58\n72.78\nG&L w/ Conv.\n2\n75.98\n72.86\nTable 1: SQuAD 2.0 Dev results of different integration modes.\nIn the convolutional block, we stack multiple convolutional\nlayers to augment local information within different n-grams.\nWe stack 3 × (H + 4), 4 × (H + 4), and 5 × (H + 4) con-\nvolutional layers with 100 ﬁlters each. H is the dimension of\nthe input representations, for instance, H is 768 while using\nBERT-base as global encoder. 2-padding is used to maintain\nthe sequence length.\nIn the context outlook block, the kernel size of context out-\nlook layers is 3 × 300, depending on the number of ﬁlters\nin the convolutional layers. While without a convolutional\nblock, it is set to 3 × H according to the global encoder.\nDuring training, we use AdamW [Loshchilov and Hutter,\n2019] optimizer. We set a learning rate to 2e-5 for the global\nencoder, and 1e-4 for other layers. We train the model with a\nmini-batch size of 48 for about 5 epochs. The main codes are\nuploaded as supplementary materials.\n4.2\nExtractive Question Answering\nChoice of architecture.\nWe ﬁrst compare the three modes\nof global and local encoding integration: G2L, L2G, and\nG&L.\nTable 1 reports the results of the respective architectures\nwith BERT as a baseline. The table shows the most effec-\ntive mode is G2L. G&L achieves a minor improvement over\nthe baseline, yet not exceeding that of the ﬁrst structure. A\npossible reason is it’s difﬁcult to precisely capture the rela-\ntionships between global and local representations. We ﬁnd\nL2G architecture does not work well. The local encoding\noperation perturbs the following global encoding procedure,\nparticularly while integrating the convolution operations with\ndifferent kernel sizes. Speciﬁcally, directly combining the in-\nformation within different n-grams confuses to the model. In\nsummary, it should better not replace the input to the pre-\ntrained model. We nevertheless acknowledge that early con-\nvolution might beneﬁt a scenario as discussed in [Xiao et al.,\n2021] for visual tasks. In the following experiments, we adopt\nthe G2L mode.\nOverall results.\nTable 2 shows the comparisons of COOL\nwith and without the convolutional block combined with\nBERT, ALBERT, RoBERTa and the corresponding baselines\nfor SQuAD 2.0 and SQuAD 1.1. It can be viewed that COOL\nwith BERT achieves an improvement of 2.24 and 1.7 points\nfor EM and F1, respectively, over the original BERT. COOL\nwith ALBERT adds 0.88 EM points and 0.86 F1 points.\nCOOL with RoBERTa outperforms the original in terms of\nboth EM and F1 as well. SQuAD 1.1 does not contain unan-\nswerable questions and is thus less challenging than SQuAD\nModel\nF1\nEM\nSQuAD 2.0\nBERT\n75.41\n71.78\nCOOL(BERT) w/o Conv.\n76.79\n73.36\nCOOL(BERT) w/ Conv.\n77.11\n74.02\nALBERT\n78.63\n75.33\nCOOL(ALBERT) w/o Conv.\n79.23\n75.95\nCOOL(ALBERT) w/ Conv.\n79.49\n76.21\nRoBERTa\n82.91\n79.87\nCOOL(RoBERTa) w/o Conv.\n83.39\n80.12\nCOOL(RoBERTa) w/ Conv.\n83.61\n80.33\nSQuAD 1.1\nBERT\n88.33\n80.76\nCOOL(BERT) w/o Conv.\n88.38\n81.00\nCOOL(BERT) w/ Conv.\n88.44\n80.91\nALBERT\n87.81\n79.50\nCOOL(ALBERT) w/o Conv.\n87.81\n79.93\nCOOL(ALBERT) w/ Conv.\n87.90\n80.15\nRoBERTa\n90.40\n83.00\nCOOL(RoBERTa) w/o Conv.\n90.65\n83.36\nCOOL(RoBERTa) w/ Conv.\n90.66\n83.42\nTable 2: SQuAD Dev results for extractive question answering.\nModel\nF1\nEM\nBERT\n75.41\n71.78\nBERT-Deeper(3 layers)\n75.28\n71.81\nBERT-Deeper(5 layers)\n74.94\n71.28\nCOOL(BERT) w/o Conv.\n76.79\n73.36\nCOOL(BERT) w/ Conv.\n77.11\n74.02\nTable 3: Comparisons with deeper models.\nModel\nF1\nEM\nBERT\n75.41\n71.78\nBERT + Convolution\n75.82\n72.81\nBERT + Dynamic Convolution\n75.57\n71.19\nBERT + Visual Outlook Attention\n75.89\n72.94\nBERT + Context Outlook Attention\n76.79\n73.36\nTable 4: Comparisons with other techniques for local augmentation.\n2.0. Nevertheless, models inserted COOL improve across the\nboard over the original models. These results conﬁrm the\nbeneﬁt of an appropriate consideration of local information\nin a transformer-based approach for question answering that\nCOOL is an effective approach.\nThe comparison of the results for COOL with and with-\nout the convolutional block, further shows that COOL with\nthe convolutional block generally outperforms COOL with-\nout the convolution block, except for the case of SQuAD\n1.1 where COOL(BERT) without the convolutional block\nmarginally outperforms COOL(BERT) with the convolu-\ntional block by 0.09 point. In the following ablation study,\nwe further explore and compare the impacts of the convolu-\ntion techniques and the outlook attention.\nAblation study.\nTo verify these improvements are con-\ntributed by COOL rather than more trainable parameters, we\nFigure 3: Results with different numbers of context outlook layers.\nﬁrst compare the performance with a deeper BERT contain-\ning more self-attention layers in Table 3. We added 3 or 5\nmore self-attention layers on top of the original BERT. It can\nbe observed that direct stacking layers does not gain an im-\nprovement over the base model, even bringing adverse ef-\nfects. It further conﬁrms that COOL brings these improve-\nments and it can remedy the deﬁciencies of the self-attention\nmechanism while encoding the local information.\nWe further compare the outlook attention with other tech-\nniques for local information enhancement in Table 4. We sep-\narately implement BERT with standard convolution and dy-\nnamic convolution [Wu et al., 2019; Jiang et al., 2020]. The\ntable reports the COOL(BERT) results without the convolu-\ntional block in the interest of fairness. We can see that the\nmodels with outlook attention outperform those with standard\nor dynamic convolution.\nWe also compare the results between the original visual\noutlook attention and context outlook attention to demon-\nstrate that the changes to the visual outlook attention to apply-\ning to NLP are relevant and effective. The results are collated\nin Table 4 as well. It can be viewed that outlook attention,\nwhether implemented as the visual outlook or improved into a\nmechanism dedicated to NLP, brings an improvement. Thus,\nthe modiﬁed module is indeed better suited to NLP tasks.\nThe results of COOL with different numbers of context\noutlook layers are reported in Figure 3. All variants with\nCOOL outperform the baseline. It achieves peak performance\nwith two context outlook layers with a convolutional block.\nIllustrative examples.\nWe ﬁrst present and discuss illustra-\ntive examples of the situations in which models with COOL\npredict the correct or better answers while the baseline mod-\nels do not and reverse. The examples indicate the question\n(Q), the paragraph or the signiﬁcant excerpt thereof (P), the\nanswers by the baseline models, and the answers by COOL\nwith the corresponding baseline models.\nThe parts of the\nparagraph relevant to the discussion are highlighted in blue\nand correct answers in green.\nIn Example 1, the baseline models fail to predict the\ncorrect answer. After inserting COOL, COOL(BERT) and\nCOOL(RoBERTa) precisely recognise the relationship be-\ntween ‘USC’ and ‘Trojans’ and thus give the correct answer.\nAlthough COOL(ALBERT) still fails, it attempts to recog-\nnise the relationship between the state ‘UCLA’ and the team’s\nname ‘Bruins’. We further visualize the self-attention weights\nin Figure 4. We can observe the most important tokens to\n(a) BERT\n(b) RoBERTa\nFigure 4: Visualization of weights yielded by the self-attention\nmechanism.\n‘USC’ are itself, ‘UCLA’, ‘both’, in that order. For ‘UCLA’,\nthe important tokens are itself, ‘USC’, ‘the’, ‘Bruins’, ‘and’,\nand ‘both’. Likewise, the important words to ‘Bruins’ are\nitself, ‘both’, ‘and’, ‘Trojan’, and to ‘Trojan’ are itself, its\nsubtoken ‘##s’, and ‘Bruins’. It explains that the standard\nself-attention operation via similarity score calculation can\nnot distinguish these tokens and understand their relation-\nships. It thus predicts a long span as the answer. Whereas\naugmenting the local context information by COOL helps to\naddress this disadvantage and beneﬁts understanding the re-\nlations between close neighbours.\nExample 1.\nQ: What is the name of the team from USC?\nP: The UCLA Bruins and the USC Trojans both ﬁeld teams...\nBERT: UCLA Bruins and the\nUSC Trojans\nALBERT: UCLA Bruins\nRoBERTa: USC Trojans\nCOOL(BERT): Trojans\nCOOL(ALBERT): Bruins\nCOOL(RoBERTa): Trojans\nMoreover, we showcase interesting Example 2, the consid-\neration of the local context allows COOL to capture logical\nrelations where the baseline models fail. It is reasonable as\nmost logical formulas are within local content.\nExample 2.\nQ: What is an expression that can be used to illustrate the suspected\ninequality of complexity classes?\nP: Many known complexity classes are suspected to be unequal,...\nFor instance P ⊆NP ⊆PP ⊆PSPACE, but it is possible that P =\nPSPACE.\nBERT: 〈no answer〉\nALBERT: P = PSPACE\nRoBERTa: P = PSPACE\nCOOL(BERT): P ⊆NP ⊆PP ⊆\nPSPACE, but it is possible that P\n= PSPACE.\nCOOL(ALBERT): P ⊆NP ⊆PP\n⊆PSPACE,\nCOOL(RoBERTa): P ⊆NP ⊆\nPP ⊆PSPACE\nThere are, unfortunately, situations in which too much lo-\ncal attention can be misleading and detrimental, as illustrated\nby Examples 3. We target to address this challenge of balanc-\ning the global and local encoding in future work.\nExample 3.\nQ: What was the Anglo-Norman language’s ﬁnal form?\nP: The Anglo-Norman language was eventually absorbed into the\nAnglo-Saxon language of their subjects (see Old English) and in-\nﬂuenced it, helping (along with the Norse language of the earlier\nAnglo-Norse settlers and the Latin used by the church) in the devel-\nopment of Middle English. It in turn evolved into Modern English.\nModel\nB@4\nM\nR\nProphetNet\n23.91\n26.60\n52.26\nERNIE-GEN (beam=1)\n24.03\n26.31\n52.36\nERNIE-GEN (beam=5)\n25.40\n26.92\n52.84\nCOOL(ProphetNet) (beam=1)\n24.33\n26.24\n52.53\nCOOL(ProphetNet) (beam=5)\n25.93\n27.14\n52.86\nTable 5: SQuAD 1.1 Dev results for question generation.\nModel\nMNLI\nQNLI\nMRPC\nRTE\nSTS-B\nBERT\n84.3\n90.6\n88.0\n67.8\n88.9\nCOOL(BERT)\n84.3\n91.3\n91.9\n71.9\n89.6\nRoBERTa\n87.6\n92.8\n90.2\n78.7\n91.2\nCOOL(RoBERTa)\n87.8\n93.0\n93.3\n81.3\n90.6\nTable 6: GLUE Test results. F1 scores are reported for MRPC.\nSpearman correlations are reported for STS-B. Accuracy scores are\nreported for the other tasks.\nBERT: Modern English\nALBERT: 〈no answer〉\nRoBERTa: Modern English\nCOOL(BERT): 〈no answer〉\nCOOL(ALBERT): 〈no answer〉\nCOOL(RoBERTa): Modern En-\nglish\n4.3\nOther Natural Language Processing Tasks\nWe seek to conﬁrm the versatility of COOL and its effec-\ntiveness in tackling more NLP tasks. We compare the base-\nline model and a COOL-augmented version, noted COOL\n(〈base〉), for question generation, multiple-choice question\nanswering, natural language inference, sentiment analysis,\nnamed-entity recognition, and part-of-speech tagging with\nthe corresponding widely used datasets.\nQuestion generation.\nQuestion generation (QG) targets\ngenerating a question given a passage and an answer [Qi et\nal., 2020]. We evaluate the model with SQuAD 1.1 and adopt\nthe commonly used metrics, BLEU@4 (B@4) [Papineni et\nal., 2002], METEOR (M) [Banerjee and Lavie, 2005], and\nROUGE-L (R) [Lin, 2004]. ERNIE-GEN [Xiao et al., 2020]\nand ProphetNet [Qi et al., 2020] achieve the state-of-the-art\nfor QG. We insert COOL into ProphetNet, then compare it\nwith the original ProphetNet and with ERNIE-GEN. The re-\nsults of the different methods are compared in Table 5. The\noriginal ProphetNet has a BLEU@4 score of 23.91 and a\nMETEOR score of 26.60. With COOL, the BLEU@4 and\nMETEOR reach 25.93 and 27.14, signiﬁcantly outperform-\ning both the original ProphetNET and ERNIE-GEN.\nNatural language inference.\nNatural language inference\n(NLI), also termed natural language understanding (NLU),\ntargets to determine whether two input sequences entail or\ncontradict each other [Wang et al., 2018]. The General Lan-\nguage Understanding Evaluation benchmark (GLUE), com-\nposed of 9 datasets for different tasks, is widely used for\nNLI. We evaluate the proposed approach on the Multi-Genre\nNatural Language Inference Corpus (MNLI) [Williams et\nal., 2018], the Question-answering NLI dataset (QNLI) [Ra-\njpurkar et al., 2016], the Microsoft Research Paraphrase\nCorpus (MRPC) [Dolan and Brockett, 2005], the Recog-\nnizing Textual Entailment dataset (RTE) [Bentivogli et al.,\nModel\nSWAG\nNER\nPOS\nSST\nIMDB\nBERT\n79.7\n90.8\n93.4\n92.5\n93.4\nCOOL(BERT)\n79.9\n91.4\n93.7\n92.2\n93.6\nRoBERTa\n83.3\n91.9\n92.8\n94.8\n91.3\nCOOL(RoBERTa)\n83.4\n95.3\n93.5\n95.2\n91.2\nTable 7: SWAG Test accuracy results, CoNLL-2003 NER and POS\nF1 results, and SST and IMDB accuracy results.\n2009], and the Semantic Textual Similarity Benchmark (STS-\nB) [Cer et al., 2017].\nThe results are presented in Ta-\nble 6. COOL(BERT) and COOL(RoBERTa) always outper-\nform the originals, excluding on the MNLI dataset. More-\nover, COOL(BERT) and COOL(RoBERTa) achieve signiﬁ-\ncant improvements on the MRPC and RTE datasets.\nMultiple-choice\nquestion\nanswering.\nMultiple-choice\nquestion answering (MCQA) is a type of retrieval-based\nquestion answering that selects an answer to a question by\nranking the given choices. The Situations With Adversarial\nGenerations (SWAG) dataset contains 113k sentence-pair\nexamples\nthat\nevaluate\ngrounded\ncommonsense\ninfer-\nence [Zellers et al., 2018].\nIt targets to choose the most\nplausible continuation among four choices. The results of the\ncomparisons of the baselines with models with COOL are\nreported in Table 7. According to the results, COOL module\nis able to improve this task.\nNamed-entity recognition and part-of-speech tagging.\nNamed-entity recognition (NER) locates and classiﬁes named\nentities in a sentence into pre-deﬁned categories. It extracts,\nfor instance, names of locations and people from the text\nand places them under certain categories, such as organiza-\ntion and person [Yamada et al., 2020]. Part-of-speech (POS)\ntagging assigns to each word in the text its part of speech,\ne.g., noun, verb, adjective, and other grammatical categories\nsuch as tense [Tian and Lo, 2015]. NER and POS tagging\nare basic NLP tasks. We evaluate the model performance on\nthe CoNLL-2003 dataset [Tjong Kim Sang and De Meulder,\n2003]. The comparative results are reported in Table 7. We\ncan observe that COOL beneﬁts the two tasks as well.\nSentiment analysis.\nSentiment analysis (SA) determines\nwhether a sentence is positive, negative, or neutral.\nThe\ncommon datasets are the large movie review dataset\n(IMDB) [Maas et al., 2011] and the Stanford Sentiment Tree-\nbank (SST-2) from GLUE. The comparisons are also pre-\nsented in Table 7. For this task, the results are mixed, suggest-\ning the possible lesser importance of the ﬁne-grained element\nbrought by involving the local context.\n4.4\nOther Language\nTo verify the proposed context outlooker is generalisable for\nother languages. We try to apply it to Malay, an important\nlow-resource language in Southeast Asia. We evaluate the\napproach on Malay SQuAD 2.0 which is translated from En-\nglish SQuAD 2.0 by Husein5. Compared with the original\nBERT-Bahasa6 in Table 8, the proposed approach also brings\n5https://github.com/huseinzol05/Malay-Dataset\n6https://github.com/huseinzol05/malaya\nModel\nEM\nF1\nBERT-Bahasa\n57.17\n61.49\nCOOL(BERT)\n58.41\n63.24\nTable 8: Malay SQuAD 2.0 Dev results for Malay extractive ques-\ntion answering.\nModel\nF1\nEM\nBERT\n75.41\n71.78\n+ Local Graph (RGCN)\n74.83\n71.52\n+ Local Graph (GAT)\n75.79\n72.90\n+ Local Graph (HGT)\n75.96\n72.67\nTable 9: Comparisons with GNNs on SQuAD 2.0.\nimprovement to the Malay language. It further proves the ef-\nfectiveness and generalisability of the proposed module.\n4.5\nComparison with augmentation by a local\ngraph\nWe consider whether we can perform local augmentation by\nbuilding a local graph on top of the fully connected graph pro-\ncessed by the self-attention mechanism. Thus, we try to build\na local graph with the outputs from a backbone where only\nclose neighbours within the predeﬁned range are connected\nand adopt graph neural networks (GNNs) to update the ver-\ntex embeddings. In Table 9, we report the preliminary results\nby utilising different GNNs, i.e., relational graph convolution\nnetwork (RGCN) [Michael Sejr et al., 2018], graph attention\nnetwork (GAT) [Veliˇckovi´c et al., 2017], and heterogeneous\ngraph transformer (HGT) [Hu et al., 2020]. The model with\nRGCN underperforms the baseline as it treats all neighbours\nequally, which might weaken the discrimination of differ-\nent neighbours. The models with GAT or HGT, considering\nweighted effects from different neighbours, outperform the\nbaselines that prove the impact of local augmentation. Nev-\nertheless, the results do not exceed the model with a context\noutlooker. We consider the reason is the GNNs handle the\ndirect neighbours only. However, the context outlooker con-\nsiders more constraints from indirect neighbours.\n5\nConclusion\nWe presented COOL, an outlook attention mechanism\nfor natural language processing that can be added to a\ntransformer-based network for any natural language process-\ning task.\nA comparative empirical performance evaluation of an im-\nplementation of COOL with different models conﬁrms that\nCOOL, in its Global-to-Local mode, yields a practical perfor-\nmance improvement over a baseline using the original model\nalone for question answering. The performance and examples\nsuggest that COOL is indeed able to acquire a more expres-\nsive representation of local context information.\nWe also presented empirical results for other natural lan-\nguage processing tasks: question generation, multiple-choice\nquestion answering, natural language inference, sentiment\nanalysis, named-entity recognition, and part-of-speech tag-\nging not only illustrate its portability and versatility of COOL\nover different models and for different tasks but also conﬁrms\nthe positive contribution of COOL to the effectiveness of the\nresolution of these tasks and other languages.\nAcknowledgments\nThis research/project is supported by the National Research\nFoundation, Singapore under its Industry Alignment Fund –\nPre-positioning (IAF-PP) Funding Initiative. Any opinions,\nﬁndings and conclusions or recommendations expressed in\nthis material are those of the author(s) and do not reﬂect the\nviews of National Research Foundation, Singapore.\nReferences\n[Bahdanau et al., 2014] Dzmitry\nBahdanau,\nKyunghyun\nCho, and Yoshua Bengio.\nNeural machine translation\nby jointly learning to align and translate. arXiv preprint\narXiv:1409.0473, 2014.\n[Banerjee and Lavie, 2005] Satanjeev Banerjee and Alon\nLavie. METEOR: An automatic metric for MT evaluation\nwith improved correlation with human judgments. In ACL\nWorkshop on Intrinsic and Extrinsic Evaluation Measures\nfor Machine Translation and/or Summarization, 2005.\n[Bentivogli et al., 2009] Luisa Bentivogli, Peter Clark, Ido\nDagan, and Danilo Giampiccolo. The ﬁfth PASCAL rec-\nognizing textual entailment challenge. In TAC, 2009.\n[Cer et al., 2017] Daniel Cer, Mona Diab, Eneko Agirre,\nInigo Lopez-Gazpio, and Lucia Specia. Semantic textual\nsimilarity-multilingual and cross-lingual focused evalua-\ntion. arXiv preprint arXiv:1708.00055, 2017.\n[Cheng et al., 2016] Jianpeng Cheng, Li Dong, and Mirella\nLapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understand-\ning. In the Conference of the North American Chapter of\nthe Association for Computational Linguistics, 2019.\n[Dolan and Brockett, 2005] William B. Dolan and Chris\nBrockett. Automatically constructing a corpus of senten-\ntial paraphrases. In the Workshop on Paraphrasing, 2005.\n[Dong et al., 2018] Linhao Dong, Shuang Xu, and Bo Xu.\nSpeech-transformer:\nA\nno-recurrence\nsequence-to-\nsequence model for speech recognition. In the Conference\non Acoustics, Speech and Signal Processing, 2018.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In The Ninth Inter-\nnational Conference on Learning Representations, 2021.\n[Glorot et al., 2011] Xavier Glorot, Antoine Bordes, and\nYoshua Bengio.\nDeep sparse rectiﬁer neural networks.\nIn the Conference on Artiﬁcial Intelligence and Statistics,\n2011.\n[Gulati et al., 2020] Anmol Gulati,\nJames Qin,\nChung-\nCheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han,\nShibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruom-\ning Pang.\nConformer:\nConvolution-augmented trans-\nformer for speech recognition. In Proc. Interspeech, 2020.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, 2016.\n[He et al., 2021] Pengcheng He, Xiaodong Liu, Jianfeng\nGao,\nand Weizhu Chen.\nDeBERTa:\nDecoding-\nenhanced BERT with disentangled attention.\nArXiv,\nabs/2006.03654, 2021.\n[Hofst¨atter et al., 2020] Sebastian Hofst¨atter, Hamed Za-\nmani, Bhaskar Mitra, Nick Craswell, and Allan Hanbury.\nLocal self-attention over long text for efﬁcient document\nretrieval.\nIn the 43rd International Conference on Re-\nsearch and Development in Information Retrieval, 2020.\n[Hu et al., 2020] Ziniu Hu, Yuxiao Dong, Kuansan Wang,\nand Yizhou Sun. Heterogeneous graph transformer. In\nProceedings of The Web Conference, 2020.\n[Jiang et al., 2020] Zi-Hang Jiang,\nWeihao Yu,\nDaquan\nZhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan.\nConvBERT: Improving BERT with span-based dynamic\nconvolution. In Advances in Neural Information Process-\ning Systems, 2020.\n[Kalchbrenner et al., 2014] Nal\nKalchbrenner,\nEdward\nGrefenstette, and Phil Blunsom. A convolutional neural\nnetwork for modelling sentences. In the Association for\nComputational Linguistics, 2014.\n[Karpov et al., 2020] Pavel Karpov, Guillaume Godin, and\nIgor V Tetko.\nTransformer-CNN: Swiss knife for qsar\nmodeling and interpretation. Journal of Cheminformatics,\n2020.\n[Lan et al., 2019] Zhenzhong Lan, Mingda Chen, Sebas-\ntian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut.\nAlbert:\nA lite BERT for self-supervised\nlearning of language representations.\narXiv preprint\narXiv:1909.11942, 2019.\n[Lin, 2004] Chin-Yew Lin.\nROUGE: A package for auto-\nmatic evaluation of summaries.\nIn Text Summarization\nBranches Out, 2004.\n[Liu et al., 2021a] Yun Liu, Guolei Sun, Yu Qiu, Le Zhang,\nAjad Chhatkuli, and Luc Van Gool.\nTransformer\nin convolutional neural networks.\narXiv preprint\narXiv:2106.03180, 2021.\n[Liu et al., 2021b] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. arXiv preprint arXiv:2103.14030, 2021.\n[Loshchilov and Hutter, 2019] Ilya Loshchilov and Frank\nHutter.\nDecoupled weight decay regularization.\nIn the\nConference on Learning Representations, 2019.\n[Maas et al., 2011] Andrew L. Maas, Raymond E. Daly, Pe-\nter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In the\nAssociation for Computational Linguistics, 2011.\n[Michael Sejr et al., 2018] Schlichtkrull Michael Sejr, Kipf\nThomas N., Bloem Peter, Berg Rianne Van Den, Titov\nIvan, and Welling Max.\nModeling relational data with\ngraph convolutional networks. In The Semantic Web - 15th\nInternational Conference, 2018.\n[Papineni et al., 2002] Kishore Papineni,\nSalim Roukos,\nTodd Ward, and Wei-Jing Zhu. BLEU: a method for auto-\nmatic evaluation of machine translation. In the Association\nfor Computational Linguistics, 2002.\n[Qi et al., 2020] Weizhen Qi, Yu Yan, Yeyun Gong, Dayi-\nheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and\nMing Zhou.\nProphetNet: Predicting future n-gram for\nsequence-to-Sequence Pre-training. In Findings of the As-\nsociation for Computational Linguistics, 2020.\n[Rajpurkar et al., 2016] Pranav Rajpurkar, Jian Zhang, Kon-\nstantin Lopyrev, and Percy Liang.\nSQuAD: 100,000+\nquestions for machine comprehension of text. In the Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, 2016.\n[Rajpurkar et al., 2018] Pranav Rajpurkar, Robin Jia, and\nPercy Liang. Know what you don’t know: Unanswerable\nquestions for SQuAD. In the Association for Computa-\ntional Linguistics, 2018.\n[Sukhbaatar et al., 2019] Sainbayar\nSukhbaatar,\nEdouard\nGrave, Piotr Bojanowski, and Armand Joulin. Adaptive\nattention span in transformers. In the Association for Com-\nputational Linguistics, 2019.\n[Sun et al., 2019] Fei Sun, Jun Liu, Jian Wu, Changhua Pei,\nXiao Lin, Wenwu Ou, and Peng Jiang. BERT4Rec: Se-\nquential recommendation with bidirectional encoder rep-\nresentations from transformer. In the Conference on Infor-\nmation and Knowledge Management, 2019.\n[Tian and Lo, 2015] Yuan Tian and David Lo. A compara-\ntive study on the effectiveness of part-of-speech tagging\ntechniques on bug reports. In the Conference on Software\nAnalysis, Evolution, and Reengineering, 2015.\n[Tjong Kim Sang and De Meulder, 2003] Erik\nF.\nTjong\nKim Sang and Fien De Meulder.\nIntroduction to the\nCoNLL-2003 shared task: Language-independent named\nentity recognition. In the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL, 2003.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing sys-\ntems, 2017.\n[Veliˇckovi´c et al., 2017] Petar Veliˇckovi´c, Guillem Cucurull,\nArantxa Casanova, Adriana Romero, Pietro Lio, and\nYoshua Bengio. Graph attention networks. arXiv preprint\narXiv:1710.10903, 2017.\n[Wang et al., 2018] Alex Wang, Amanpreet Singh, Julian\nMichael, Felix Hill, Omer Levy, and Samuel Bowman.\nGLUE: A multi-task benchmark and analysis platform for\nnatural language understanding. In the EMNLP Workshop\nBlackboxNLP, 2018.\n[Williams et al., 2018] Adina Williams, Nikita Nangia, and\nSamuel Bowman. A broad-coverage challenge corpus for\nsentence understanding through inference. In the Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics, 2018.\n[Wu et al., 2019] Felix Wu, Angela Fan, Alexei Baevski,\nYann N. Dauphin, and Michael Auli. Pay less attention\nwith lightweight and dynamic convolutions. In 7th Inter-\nnational Conference on Learning Representations, 2019.\n[Xiao et al., 2020] Dongling Xiao, Han Zhang, Yukun Li,\nYu Sun, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE-\nGEN: An enhanced multi-ﬂow pre-training and ﬁne-tuning\nframework for natural language generation. arXiv preprint\narXiv:2001.11314, 2020.\n[Xiao et al., 2021] Tete Xiao, Mannat Singh, Eric Mintun,\nTrevor Darrell, Piotr Doll´ar, and Ross Girshick.\nEarly\nconvolutions help transformers see better. arXiv preprint\narXiv:2106.14881, 2021.\n[Yamada et al., 2020] Ikuya Yamada, Akari Asai, Hiroyuki\nShindo, Hideaki Takeda, and Yuji Matsumoto.\nLUKE:\nDeep contextualized entity representations with entity-\naware self-attention.\nIn the Conference on Empirical\nMethods in Natural Language Processing, 2020.\n[Yang et al., 2018] Baosong Yang, Zhaopeng Tu, Derek F.\nWong, Fandong Meng, Lidia S. Chao, and Tong Zhang.\nModeling localness for self-attention networks.\nIn the\nConference on Empirical Methods in Natural Language\nProcessing, 2018.\n[Yuan et al., 2021] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi\nFeng, and Shuicheng Yan. VOLO: Vision outlooker for vi-\nsual recognition. arXiv preprint arXiv:2106.13112, 2021.\n[Zellers et al., 2018] Rowan Zellers, Yonatan Bisk, Roy\nSchwartz, and Yejin Choi. SWAG: A large-scale adver-\nsarial dataset for grounded commonsense inference. In the\nConference on Empirical Methods in Natural Language\nProcessing, 2018.\n[Zhang et al., 2020] Zhebin Zhang, Sai Wu, Gang Chen, and\nDawei Jiang. Self-attention and dynamic convolution hy-\nbrid model for neural machine translation. In IEEE Inter-\nnational Conference on Knowledge Graph (ICKG), 2020.\n[Zhuang et al., 2021] Liu Zhuang, Lin Wayne, Shi Ya, and\nZhao Jun. A robustly optimized BERT pre-training ap-\nproach with post-training. In the 20th Chinese National\nConference on Computational Linguistics, 2021.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2022-04-01",
  "updated": "2023-05-15"
}