{
  "id": "http://arxiv.org/abs/2204.09931v2",
  "title": "Learning to Purification for Unsupervised Person Re-identification",
  "authors": [
    "Long Lan",
    "Xiao Teng",
    "Jing Zhang",
    "Xiang Zhang",
    "Dacheng Tao"
  ],
  "abstract": "Unsupervised person re-identification is a challenging and promising task in\ncomputer vision. Nowadays unsupervised person re-identification methods have\nachieved great progress by training with pseudo labels. However, how to purify\nfeature and label noise is less explicitly studied in the unsupervised manner.\nTo purify the feature, we take into account two types of additional features\nfrom different local views to enrich the feature representation. The proposed\nmulti-view features are carefully integrated into our cluster contrast learning\nto leverage more discriminative cues that the global feature easily ignored and\nbiased. To purify the label noise, we propose to take advantage of the\nknowledge of teacher model in an offline scheme. Specifically, we first train a\nteacher model from noisy pseudo labels, and then use the teacher model to guide\nthe learning of our student model. In our setting, the student model could\nconverge fast with the supervision of the teacher model thus reduce the\ninterference of noisy labels as the teacher model greatly suffered. After\ncarefully handling the noise and bias in the feature learning, our purification\nmodules are proven to be very effective for unsupervised person\nre-identification. Extensive experiments on three popular person\nre-identification datasets demonstrate the superiority of our method.\nEspecially, our approach achieves a state-of-the-art accuracy 85.8\\% @mAP and\n94.5\\% @Rank-1 on the challenging Market-1501 benchmark with ResNet-50 under\nthe fully unsupervised setting. The code will be released.",
  "text": "1\nLearning to Puriﬁcation for Unsupervised Person\nRe-identiﬁcation\nLong Lan∗, Member, IEEE, Xiao Teng∗, Jing Zhang, Member, IEEE, Xiang Zhang, Member, IEEE,\nand Dacheng Tao, Fellow, IEEE\nAbstract—Unsupervised person re-identiﬁcation is a challeng-\ning and promising task in computer vision. Nowadays unsu-\npervised person re-identiﬁcation methods have achieved great\nprogress by training with pseudo labels. However, how to purify\nfeature and label noise is less explicitly studied in the unsuper-\nvised manner. To purify the feature, we take into account two\ntypes of additional features from different local views to enrich\nthe feature representation. The proposed multi-view features\nare carefully integrated into our cluster contrast learning to\nleverage more discriminative cues that the global feature easily\nignored and biased. To purify the label noise, we propose to take\nadvantage of the knowledge of teacher model in an ofﬂine scheme.\nSpeciﬁcally, we ﬁrst train a teacher model from noisy pseudo\nlabels, and then use the teacher model to guide the learning of our\nstudent model. In our setting, the student model could converge\nfast with the supervision of the teacher model thus reduce the\ninterference of noisy labels as the teacher model greatly suffered.\nAfter carefully handling the noise and bias in the feature learning,\nour puriﬁcation modules are proven to be very effective for\nunsupervised person re-identiﬁcation. Extensive experiments on\nthree popular person re-identiﬁcation datasets demonstrate the\nsuperiority of our method. Especially, our approach achieves a\nstate-of-the-art accuracy 85.8% @mAP and 94.5% @Rank-1 on\nthe challenging Market-1501 benchmark with ResNet-50 under\nthe fully unsupervised setting. The code will be released.\nIndex Terms—clustering puriﬁcation, knowledge distillation,\nunsupervised person ReID.\nI. INTRODUCTION\nP\nERSON re-identiﬁcation (ReID) aims to retrieve the same\nperson under different camera views. It has attracted\nwidespread attentions in the computer vision community due\nto its great potential in real world applications [1]. Although\ngreat performance has been achieved in the supervised person\nReID setting, the demand of human annotation heavily limits\nthe application. To make it more scalable in the real world, the\ntask of unsupervised person ReID has been raised and attracted\nincreasing more attention as it requires no human annotation.\nUnsupervised person ReID mainly includes two categories,\nunsupervised domain adaptation (UDA) person ReID and\npurely unsupervised learning (USL) person ReID [2]. The\nUDA methods aim to learn from the annotated source dataset\nand transfer the knowledge to the unlabeled target dataset [3]–\n[5]. They usually adopt the two-stage training strategy. At ﬁrst\nthe model is pre-trained on the labeled source dataset, then\nthe unlabeled target dataset is utilized to ﬁnetune the model.\nUnlike existing general unsupervised domain adaptation set-\nting where the source domain and target domain share the\nsame label space [6]–[8], UDA person ReID usually assumes\nthere are no interactions between source and target domains\nin the label space, thus compared with existing unsupervised\ndomain adaptation setting [9]–[11], UDA person ReID is more\nchallenging. Similar to other general unsupervised domain\nadaptation methods, UDA ReID methods are also proposed\nbased on the assumption that the discrepancy between source\ndomain and target domain is not signiﬁcant, and the perfor-\nmance of these methods will drop signiﬁcantly when the gap\nbetween source and target domains is large.\nTo further relax the dependency on labeled source dataset,\nthe USL methods directly learn from the unlabeled target\ndataset, which require no annotation information from other\ndomains [12]–[15]. Thus, compared with the UDA person\nReID, the USL person ReID is more scalable. Nowadays\nstate-of-the-art USL person ReID methods have achieved great\nprogress by training the model with the pseudo labels gener-\nated by clustering algorithm [2], [16], [17]. These methods\nhold the assumption that the images of the same person\nshare higher similarity in the feature space, thus will be more\nlikely to be collected in the same cluster. Generally, these\nmethods can be regarded as two-stage training schemes, ﬁrstly\na clustering algorithm is applied to divide the features of\nimages into different clusters, and assign pseudo labels to\ndifferent clusters accordingly. Then the model is trained with\ngenerated pseudo labels. These two stages are conducted in\nan iterative scheme as they can promote each other in the\nwhole training process. Based on this training framework,\nmemory-based contrastive learning methods have achieved the\nstate-of-the-art performance nowadays by taking advantage of\ncontrastive learning with image features stored in the memory\nbank [2], [16], [17].\nAlthough the above ReID methods have achieved great\nprogress in recent years, the gap between supervised and\nunsupervised methods is still large. After carefully analysed\nthe reason behind the phenomenon, we think the learning\nprocess of unsupervised person ReID is mainly inﬂuenced\nby the feature bias and label noise due to limited global\nfeature representation power and lack of accurate predicted\nlabels. As the aim of global feature learning is to capture\nthe most salient clues of appearance to represent identities of\ndifferent pedestrians, some non-salient but important detailed\nlocal cues can be easily ignored due to the limited scales and\nless diversities of the training dataset, which makes global\nfeatures hard to distinguish from similar inter-class persons\n[18]. As a result, images with different identities but similar\nsalient clues of appearance could be easily merged to the same\ncluster, which will make the learned feature representation\nbiased. On the other hand, since the model is trained with\narXiv:2204.09931v2  [cs.CV]  22 Jun 2022\n2\n(a) Cluster contrast learning\n(b) Cluster contrast learning with puriﬁcation modules\nFig. 1: Comparison of cluster contrast learning with/without puriﬁcation modules. (a) Cluster contrast learning trains the model\nsolely with cluster centers of global feature representations. (b) Besides the global level cluster contrast learning, feature and\nlabel noise puriﬁcation modules are also applied. The former takes into account the features from two local views to purify the\nbias of the global feature involved. Meanwhile, the latter aims to purify the label noise by taking advantage of the knowledge\nof teacher model in an ofﬂine scheme.\npseudo labels generated by clustering algorithm, it will suffer\nfrom severe label noise during the whole convergence process\nas it is initialized with parameters pre-trained on ImageNet\ndataset, which has the signiﬁcant discrepancy with person\nReID datasets. To relieve the above problems, we propose the\nfeature and label noise puriﬁcation modules for unsupervised\nperson ReID, as shown in Fig. 1.\nSpeciﬁcally, our method mainly includes two modules, the\nfeature puriﬁcation (FP) module and label noise puriﬁcation\n(LP) module. The former takes into account the features\nfrom two local views to enrich the feature representation and\npurify the inherent feature bias of the global feature involved.\nMeanwhile, the latter aims to purify the label noise by taking\nadvantage of the knowledge of teacher model in an ofﬂine\nscheme. As the noise will be inevitably introduced in the\nclustering process, the model will suffer from the label noise\nin the whole training process. Based on the phenomenon that\nthe trained model is more accurate than the initialized model,\nintuitively the knowledge of the trained model can be utilized\nas the guidance to help the student model relieve the inﬂuence\nof noise in the training process. Our contributions can be\nconcluded in the following:\n• We propose a feature puriﬁcation module which care-\nfully integrate the multi-view features in our cluster\ncontrast learning framework and is proven to be effective\nin handling the bias of the global feature easily involved.\n• We further propose a label noise puriﬁcation module\nwhich aims to relieve the label noise introduced by clus-\ntering procedure by taking advantage of the knowledge\nof teacher model. To our knowledge, this is the ﬁrst work\nto apply ofﬂine knowledge distillation for unsupervised\nperson ReID, and we ﬁnd it is very effective for such\ntask.\n• Extensive experiments are conducted on three popular\nperson ReID benchmarks. The results show our method\nsigniﬁcantly outperforms existing state-of-the-art unsu-\npervised person ReID methods. Specially, our method\noutperforms the state-of-the-art method [17] by 3.2%,\n3.4% and 6.2% in terms of mAP on Market-1501,\nDukeMTMC-ReID, and MSMT17 datasets.\nII. RELATED WORK\nIn this section, we introduce the most related work from\nthree perspectives: 1) Unsupervised person ReID, which in-\ncludes unsupervised domain adaptation (UDA) person ReID\nand purely unsupervised learning (USL) person ReID; 2) Part-\nbased person ReID, which takes advantage of local parts of the\nperson to get more discriminative feature representations; and\n3) Knowledge distillation, which includes some techniques of\nknowledge distillation in different areas.\nA. Unsupervised Person ReID\nUnsupervised person ReID can be summarized into two\ncategories, unsupervised domain adaptation (UDA) person\nReID and purely unsupervised learning (USL) person ReID.\nThe former aims to learn from the annotated source dataset\nand transfer the knowledge from the source domain to the\nunlabeled target domain [2]–[5]. While the latter directly\ntrains on the unlabeled target dataset without any labeled data\n[2], [16], [17]. To make full use of unlabeled target dataset,\nunsupervised person ReID methods usually apply existing\nclustering algorithms, such as Kmeans [19] and DBSCAN\n[20] to generate pseudo labels for each sample in the target\ndomain. Then the generated pseudo labels and the unlabeled\ndataset are used together to train the model in an iterative\nscheme [21]. To further improve the quality of pseudo labels,\nmany variants of pseudo label generation methods have been\nproposed. BUC [12] proposed a bottom-up clustering frame-\nwork by exploiting the intrinsic diversity among identities and\nsimilarity within each identity to learn more discriminative\nfeature representations. GLT [22] proposed a Group-aware\nLabel Transfer algorithm that facilitates the online interaction\nand mutual promotion of the pseudo labels prediction and\nfeature learning. To avoid the label noise accumulation in a\n3\nsingle model setting, MMT [5] reﬁnes the noisy pseudo labels\nby optimizing two neural networks under the joint supervisions\nof off-line reﬁned hard pseudo labels and on-line reﬁned soft\npseudo labels.\nThe above methods can be applied to both UDA ReID and\nUSL ReID. However, different from USL ReID, UDA ReID\nalso has the auxiliary labeled source dataset. Thus the key\nof UDA ReID methods is how to take advantage of labeled\nsource dataset to improve the performance of the model on\nunlabeled target dataset. These methods usually work based\non the assumption that the discrepancy between the source\ndomain and the target domain is not signiﬁcant and apply\ntransfer learning techniques to tackle such problem. To further\nmitigate the gap between source and target domains, some\ndomain-translation-based methods are proposed, which aim\nto take advantage of generative adversarial networks (GANs)\n[23] to translate the source-domain images to have target-\ndomain styles while preserving their original IDs [24]–[26].\nThen the translated source dataset and unlabeled target dataset\ncan be used together to train the model with some semi-\nsupervised techniques. However, such methods highly depend\non the performance of GANs as the quality of the translated\ndataset has a signiﬁcant effect on the trained model. Instead\nof directly using GANs to translate the style of the source\ndataset to augment the target dataset, some works aim to\nregard the unsupervised person ReID problem as the multi-\nlabel classiﬁcation problem by using labeled source dataset as\nreference persons [27]–[29]. These methods aim to learn the\nsoft multi-label for each target-domain image by comparing\nthem to the known auxiliary reference persons in the source\ndomain.\nIn this work we focus on USL ReID and our work is\nestablished on memory-based contrastive learning frameworks,\nwhich are the state-of-the-arts for unsupervised person ReID.\nSPCL [16] proposed a self paced method which gradually\ncreate more reliable clusters to reﬁne the hybrid memory and\nlearning targets. To solve the problem of inconsistency in\nthe memory updating process, CCL [17] proposed a novel\ncluster contrast learning framework which was built on a\ncluster-level cluster memory dictionary and achieved great\nperformance. In this work, to purify the feature and label noise\nfor unsupervised person ReID, we also take into account local\nviews and the knowledge of the teacher model. Thus we also\ndiscuss some works related to these techniques in the below.\nB. Part-based Person ReID\nMost deep learning-based person ReID approaches take\nadvantage of only the global feature of the person, which turns\nout to be sensitive to the missing key parts. To relieve the issue,\nrecently many works focused on leveraging part discriminative\nfeature representations. These works aim to make use of local\nparts to make more accurate retrieval. Part-based person ReID\ncan be divided into three categories. In the ﬁrst category, the\nprior knowledge like poses or body landmarks are required\nto be estimated to locate the accurate parts of the person.\nHowever, the performance of such approaches heavily rely on\nthe accuracy of the pose or landmarks estimation models [30],\n[31]. The second category utilized the attention mechanism\nto adaptively locate the high activation in the feature map.\nBut the selected regions lack of semantic interpretation [32].\nThe third category directly utilizes the predeﬁned strips as it\nassumes the person is vertically aligned. Compared with the\nﬁrst category it is more scalable as it requires no extra pre-\ntrained models, thus it is widely used in the person ReID and\nachieved great improvements in recent years. Speciﬁcally, PCB\n[33] conducts uniform partition on the conv-layer for learning\npart-level discriminative fetaures. MGN [18] and RMGL [34]\nutilize multi-Granularity parts to get a more accurate feature\nrepresentation. These methods are all proposed for supervised\nperson ReID. Similar to our work, SSG [35] is also proposed\nfor unsupervised person ReID and utilizes three sets of local\nand global features to represent persons. However, SSG gener-\nates pseudo labels by applying clustering algorithm on each set\non them, which causes a huge cost of extra time computation.\nFurthermore, SSG updates the network with triplet loss and\ninstance-level features, which may neglect high-level semantic\nmeanings. Compared with SSG, our method uses the cluster\ncenter memory to capture the relation between local/global\nfeatures and their own cluster centers, and our local and global\nbranches share the same pseudo label set jointly generated by\nthem to relieve the inherent feature bias in the single-scale\nglobal feature representations.\nC. Knowledge Distillation\nThe aim of knowledge distillation is to transfer the knowl-\nedge from the network to another. The original idea of\nknowledge distillation is to compress the knowledge from\nthe teacher network to a smaller student network. Recently,\nmore works have focused on self-knowledge distillation, which\nkeeps the structure of the teacher and student network the\nsame [10], [36]–[38]. These methods usually directly use\nthe outputs of the teacher whose structure is the same as\nthe student. Speciﬁcally, a simple but effective baseline was\nproposed for few shot learning in [39] by minimizing the\nloss where the target is the distribution of class probabili-\nties induced by the teacher model. CS-KD [36] proposed a\nnew regularization technique, which matches the distribution\npredicted between different samples of the same class. SSD\n[38] proposed a effective multi-stage training scheme for long-\ntailed recognition, which utilized the output of the teacher\nto generate soft label for the student. Similar to our work, a\nprobability distillation module is proposed in [40] which aims\nto align the probability distribution between the network and\nthe teacher network updated by Exponential Moving Average\n(EMA) method. However, the teacher network updated in the\nonline scheme is still limited in the feature representation and\nsuffers from severe label noise [9]. In the work, we aim to\ntransfer the knowledge from the teacher to the student model in\na multi-view and ofﬂine scheme directly in the feature space,\nand we ﬁnd it is effective for unsupervised person ReID.\nIII. METHOD\nA. Cluster Contrast Learning Framework\nLet X = {x1, x2, . . . , xN} denotes the unlabeled training\nset which contains N instances. F = {f1, f2, . . . , fN} denotes\n4\nFig. 2: Feature puriﬁcation module. Besides the global feature, centers of local features are also maintained as independent\ncluster memory dictionaries. The DBSCAN clustering algorithm is applied in the fused pairwise similarity matrix to generate\nthe consistent pseudo labels. The shared pseudo labels guide the local/global features to initialize their respective memory\ncluster representations.\nthe corresponding feature maps extracted from the training set\nwith the encoder fθ, which can be described as fi = fθ (xi).\nU = {u1, u2, . . . , uN} denotes the feature vectors got from\nthe feature maps after the pooling operation. uq is the cor-\nresponding feature vector of the query instance q extracted\nwith encoder fθ. Φ = {φ1, φ2, . . . , φC} denotes C cluster\nrepresentations in the training. Note that the number of the\ncluster C can vary according to clustering results.\nMemory-based cluster contrast learning frameworks have\nachieved the state-of-the-art performance by taking ad-\nvantage of memory mechanism and contrastive learning\n[16], [17]. Speciﬁcally, these methods utilize Kmeans [19]\nor DBSCAN [20] to generate pseudo labels for unla-\nbeled samples. Thus a pseudo labeled dataset X′\n=\n{(x1, y1) , (x2, y2) , . . . , (xN, yN ′)} can be obtained, where\nyi ∈{1, . . . , C} is the pseudo label generated for the i −th\nsample and N ′ is the number of the labeled samples in the\ndataset. Then contrastive learning and memory mechanism can\nbe applied on the pseudo labeled dataset. Among existing\nmemory-based cluster contrast learning frameworks, cluster\ncontrast learning [17] has achieved impressive performance by\nimplementing contrastive learning on the cluster-level cluster\nmemory dictionaries as following:\nL = −log\nexp (uq · φ+/τ)\nPC\nk=0 exp (uq · φk/τ)\n,\n(1)\nwhere uq is the feature vector of the query sample. φk is\nthe centroid feature vector representing the k −th cluster\nstored in the memory, which is initialized by the average\nfeature vector of samples in the k −th cluster and φ+ is\nthe centroid feature vector of the cluster the query sample\nbelongs to. C is the number of the cluster. τ is the temperature\nhyper-parameter. Then the centroid feature vector stored in the\nmemory dictionary sets can be updated in the following way:\nφk = mφk + (1 −m)uq,\n(2)\nwhere m is the momentum updating factor and k is the index\nof the cluster query sample belongs to.\nOur unsupervised person re-identiﬁcation is implemented\nin the framework of the cluster contrast learning. To achieve\nthe goal of puriﬁcation in the unsupervised person re-\nidentiﬁcation. We design two functional components, namely\nFP module and LP module, as shown in Fig. 2 and Fig.\n3, respectively. The former takes into account the features\nfrom two local views to enrich the feature representation and\npurify the inherent feature bias of the global feature confront.\nMeanwhile, the latter aims to purify the label noise by taking\nadvantage of the knowledge of teacher model in an ofﬂine\nscheme.\nB. Feature Puriﬁcation Module\nAlthough most works only utilize the global feature map\nfor the unsupervised person ReID [3]–[5], [17], the inherent\nfeature bias of global feature may hinder the learning of\nthe model for differing different persons as they tend to\ncapture the most salient clues of the appearance while ignoring\nsome detailed local cues. From this view, we propose the FP\nmodule, which aims to take advantage of extra local views\nto enhance the feature learning process by encouraging the\nmodel to discover more discriminative local cues in the feature\nrepresentation. The procedure of the module is shown in Fig.\n2. To clearly describe the proposed FP module, we divide this\nmodule into three sub-modules as follows.\n1) Multi-view features generation process: Given an un-\nlabeled training set X\n= {x1, x2, . . . , xN}, where N is\nthe number of the samples in the dataset. We can get the\n5\nFig. 3: Label noise puriﬁcation module. We ﬁxed the teacher model to learn the student model. The ClusterNCE loss and L2\nloss are applied to update the student model.\ncorresponding feature maps F = {f1, f2, . . . , fN} with the\nencoder fθ. Then we split feature maps in F into two parts\nhorizontally, which are denoted as F up = {f up\n1 , f up\n2 , . . . , f up\nN }\nand F dw =\n\b\nf dw\n1 , f dw\n2 , . . . , f dw\nN\n\t\nrespectively. To get the\nfeature vectors from them, Generalized-Mean (GEM) pooling\noperations are applied on these feature branches independently.\nAs a result, we can get three sets of feature vectors respec-\ntively.\n\n\n\n\n\nU gb =\nn\nugb\n1 , ugb\n2 , . . . , ugb\nN\no\nU up = {uup\n1 , uup\n2 , . . . , uup\nN }\nU dw =\n\b\nudw\n1 , udw\n2 , . . . , udw\nN\n\t ,\n(3)\nwhere U gb, U up and U dw are three sets of feature vectors\nrespectively. Compared with the global feature representations,\nfeature vectors from these local views can introduce more\ndetailed and complementary information about the person.\nNote that feature maps of introduced two local views are\ndirectly split from the global feature maps, thus the generation\nof these local feature vectors bring no extra computation\nburden to the model.\n2) Pseudo labels generation process: After getting the\nthree sets of feature vectors in equation 3, following SPCL\n[16] and CCL [17], we also apply DBSCAN [20] clustering\nalgorithm on these feature vectors to generate pseudo labels.\nUnlike these works which only utilize global features, we aim\nto generate pseudo labels by taking advantage of both global\nand local features. Our motivation is that as global features\ntend to capture the most salient cues, some non-salient but\nimportant detailed local cues can be easily ignored due to\nlimited scales and less diversities of the training dataset. Thus\nimages with different identities but similar appearance could\nbe easily merged to the same cluster if we only utilize global\nfeatures in the pseudo labels generation process. To mitigate\nsuch issue, we capture pairwise relations of samples by taking\ninto consideration more detailed information. Speciﬁcally,\nwith global and local feature vector sets U gb, U up and U dw,\nthe pairwise distance matrix of the dataset can be calculated\nindependently, which are denoted as Dgb, Dup and Ddw. Then\na re-weighted pairwise distance matrix can be achieved using\nthe following function:\nD = (1 −2λ1) Dgb + λ1Dup + λ1Ddw,\n(4)\nwhere D is the re-weighted pairwise distance matrix, λ1\nis the balancing factor. Then the pseudo labels ˜Y can be\ngenerated by DBSCAN clustering algorithm with matrix D.\nIn this way, we can get a pseudo labeled dataset X′ =\n{(x1, y1) , (x2, y2) , . . . , (xN, yN′)}, where N ′ is the number\nof the pseudo labeled dataset. Note that N ′ is smaller than\nthe number of samples in the original dataset N due to the\nexistence of outliers in the clustering process.\n3) Multi-view cluster centers generation process: Follow-\ning CCL [17], we also implement contrastive learning on\nthe cluster-level memory dictionaries to avoid the problem of\ninconsistency in the memory updating process. Speciﬁcally, as\nthe pseudo labeled dataset is obtained, then cluster centroids in\nthe memory are initialized by the corresponding mean feature\nvectors and the pseudo labels as following:\nφk =\n1\n|Ck|\nX\ni∈Ck\nui,\n(5)\nwhere Ck denotes the k−th cluster |·| denotes the number of\nthe instances in the corresponding cluster and ui is the feature\nvector of the i −th sample. To mitigate the limited repre-\nsentation of global features, we also maintain features from\nlocal views to promote the model to discover more detailed\ninformation in the learning process. As shown in Fig. 2, these\nthree branches calculate the cluster center according to Eq. (5)\nindependently with their own feature vectors and the shared\npseudo label set ˜Y . Thus, we can get three sets of cluster\ncentroid representations as following:\n\n\n\n\n\nΦgb =\nn\nφgb\n1 , φgb\n2 , . . . , φgb\nC\no\nΦup = {φup\n1 , φup\n2 , . . . , φup\nC }\nΦdw =\n\b\nφdw\n1 , φdw\n2 , . . . , φdw\nC\n\t ,\n(6)\nwhere C is the number of the clusters, as these three branches\nshare the same pseudo labels, thus the number of clusters in\n6\nthese three banches are the same. φgb\ni , φup\ni\nand φdw\ni\nare the i−\nth cluster centers in these three branches. Note that the pseudo\nlabels generation operation is applied before each epoch, thus\nit may change in the training process according to clustering\nresult before each epoch.\nC. Label Noise Puriﬁcation Module\nThe training process of state-of-the-art unsupervised person\nReID methods can be regarded as two stages. First, pseudo\nlabels are generated by dividing the dataset into diverse\nclusters, then the model is trained with the pseudo labels.\nThese two stages are conducted in an iterative scheme [16],\n[17]. However, the noise will be inevitably introduced in the\nconvergence process as the model initialized with ImageNet\npre-trained ResNet-50 [41] performs poorly on these person\nReID datasets at the beginning, which may accumulate label\nnoise during the training process. To relieve the issue, we\npropose the LP module, which aims to utilize the knowledge\nof the teacher to help the student model relieve the inﬂuence\nof the label noise. For a fair comparison, we take the model\ntrained on the same dataset as the teacher model and the new\nImageNet pre-trained initialized model as the student model,\nthus the structures of these two models are the same and\nit requires no extra information. Our proposed LP module\nworks based on the assumption that although the structure of\nthe student model has no superiority over the teacher model,\nthe trained teacher model performs better than the initialized\nstudent model on this task. Thus the teacher model can provide\nmore accurate pseudo labels at the beginning and guide the\nstudent model to reduce the inference of noisy labels in the\ntraining phase through knowledge distillation.\nSpeciﬁcally, the teacher model is trained with cluster con-\ntrast learning and the proposed FP module with the following\nobjective:\n\n\n\n\n\n\n\n\n\n\n\nLgb\nq = −log\nexp(ugb\nq ·φgb\n+ /τ)\nPC\nk=0 exp(ugb\nq ·φgb\nk /τ)\nLup\nq\n= −log\nexp(uup\nq ·φup\n+ /τ)\nPC\nk=0 exp(uup\nq ·φup\nk /τ)\nLdw\nq\n= −log\nexp(udw\nq\n·φdw\n+ /τ)\nPC\nk=0 exp(udw\nq\n·φdw\nk /τ)\n,\n(7)\nwhere u∗\nq is the feature vector of the query instance q from\nthe corresponding view. φ∗\nk is the centroid feature vector\nrepresenting the k−th cluster stored in the memory. φ∗\n+ is the\ncentroid feature vector representing the cluster query instance\nq belongs to stored in the memory. τ is the temperature hyper-\nparameter and C is the number of the cluster. Different from\nSPCL [16] and CCL [17] which only use global features in\nthe training process, we also maintain feature representations\nfrom local views in the training phase to promote the model\nto discover more detailed information as following:\nLstage1 = (1 −λ2) Lgb\nq + λ2(Lup\nq + Ldw\nq ),\n(8)\nwhere λ2 is the loss weight to balance the importance between\nglobal and local features and more details about the training\nprocess of teacher model can refer to Sec. III-D1.\nWhen the trained teacher model is prepared, then LP module\ncan be applied on the student model. This module includes\ntwo parts, the warm up part and the knowledge distillation\npart. More details about these parts can refer to Sec. III-D2.\nAs the initialized student model performs poorly in the person\nReID, the generated pseudo labels will contain numerous label\nnoise in the early training period, thus may cause the feature\nrepresentation biased. To tackle the issue, in the warm up\npart, we directly utilize the trained teacher model to generate\npseudo labels and use its feature vectors to initialize the\ncluster center representations as in Eq. (6). Then the student\nis directly trained with the pseudo labels and ﬁxed cluster\ncenter representations generated by the teacher model for a\nshort period. In this way, the student model can learn the\nknowledge directly from the teacher model in a fast way to\ngenerate more accurate pseudo labels in the early period of\nthe training phase.\nIn the remaining training phase, given the pseudo labeled\ndataset and memory center dictionaries as described in Sec.\nIII-B, the student model computes the objective function with\nmulti-view knowledge distillation as following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLgb\nStu = Lgb\nq + µ\n\f\f\f\f\nugb\nq\n∥ugb\nq ∥−\n˜ugb\nq\n∥˜ugb\nq ∥\n\f\f\f\f\n2\n2\nLup\nStu = Lup\nq + µ\n\f\f\f\nuup\nq\n∥uup\nq ∥−\n˜uup\nq\n∥˜uup\nq ∥\n\f\f\f\n2\n2\nLdw\nStu = Ldw\nq\n+ µ\n\f\f\f\f\nudw\nq\n∥udw\nq ∥−\n˜udw\nq\n∥˜udw\nq ∥\n\f\f\f\f\n2\n2\n,\n(9)\nwhere Lgb\nStu, Lup\nStu and Ldw\nStu are the objective functions of\nthree branches of the student model. Lgb\nq , Lup\nq\nand Ldw\nq\nare\nthe ClusterNCE loss presented in Eq. (7). µ is the balancing\nfactor. {ugb\nq , uup\nq , udw\nq } and {˜ugb\nq , ˜uup\nq , ˜udw\nq } are the three\nfeature vectors of query q in the student model and teacher\nmodel respectively. Therefore, the ﬁnal objective function of\nthe student model is as following:\nLstage2 = (1 −λ2) Lgb\nStu + λ2(Lup\nStu + Ldw\nStu),\n(10)\nwhere λ2 is the balancing factor, which is the same as in\nEq. (8). Then the cluster feature representations stored in the\nmemory dictionary sets are updated similar with the teacher\nmodel in the following way:\n\n\n\nφgb\nk = mφgb\nk + (1 −m)ugb\nq\nφup\nk = mφup\nk2 + (1 −m)uup\nq\nφdw\nk\n= mφdw\nk3 + (1 −m)udw\nq\n,\n(11)\nwhere m is the momentum updating factor. k is the index of\nthe cluster query belongs to, which is the same in these three\nbranches as they share the same pseudo label set. The details\nof the training procedure of the student model are described\nin Sec. III-D2. Note that the pseudo label generation process\nand training process are conducted iteratively until the model\nconverges. In the test phase, we only adopt the global feature\nbranch for computation efﬁciency.\nD. Training Process\n1) Training process of the teacher model: The detailed\ntraining process of the teacher model is shown in Algorithm 1.\nGiven the unlabeled dataset X and the encoder fθ initialized\nwith parameters of ResNet-50 pretrained on ImageNet [41].\n7\nAlgorithm 1: Training process of the teacher model\nRequire: Unlabeled training data X\nRequire: Initialize the encoder fθ with\nImageNet-pretrained ResNet-50\nRequire: Temperature hyper-parameter τ for Eq. (7)\nRequire: Balancing factors λ1 and λ2 for Eq. (4)\nand Eq. (8)\nRequire: Momentum updating factor m for Eq. (11)\nfor n in [1,num epochs] do\nExtract feature vector sets\n\b\nU gb, U up, U dw\t\nfrom X by fθ;\nClustering\n\b\nU gb, U up, U dw\t\ninto C clusters\nwith Eq. (4) and DBSCAN;\nInitialize three memory dictionaries individually\nwith Eq. (5) ;\nfor i in [1,num iterations] do\nSample P × K query images from X;\nCompute objective function with Eq. (8) ;\nUpdate cluster feature representations with\nEq. (11);\nend\nend\nFor each epoch, we can get the corresponding feature map set\nF gb with the encoder fθ. Then we split feature maps F gb into\ntwo parts horizontally, which are denoted as F up and F dw\nrespectively. Then GEM pooling is applied to get the corre-\nsponding feature vector sets U gb, U up and U dw, respectively.\nTo get the pseudo labels ˜Y for samples in dataset X, Eq. (4)\nand DBSCAN algorithm are applied. Then, Eq. (5) is used\nto initialize memory dictionaries individually for these three\nbranches. When the pseudo labels and memory dictionaries\nare prepared, we start to train the model. Speciﬁcally, in each\niteration, we ﬁrstly sample P × K query images from X to\nupdate parameters of the model according to Eq. (8), and then\nupdate features stored in memory dictionaries as Eq. (11).\n2) Training process of the student model: The detailed\ntraining process of the student model is shown in Algorithm 2.\nBesides the unlabeled dataset X and the encoder fθ initialized\nwith parameters of ResNet-50 pretrained on ImageNet, the\nteacher model trained following Algorithm 1 is also required.\nThe training process of the student model includes two parts,\nthe warm-up part and the knowledge distillation part.\nIn the warm-up part, we directly utilize the trained teacher\nmodel to encode the dataset X into feature map set F gb,\nand then use horizontally split operation and GEM pooling\nto obtain corresponding feature vector sets U gb, U up and\nU dw. Then Eq. (4) and DBSCAN algorithm are applied to\nget the pseudo labels and initialized memory dictionaries for\neach branch as described in the training process of the teacher\nmodel. Then P × K query images are sampled from X to\nupdate parameters of the model according to Eq. (8) without\nupdating the features stored in memory dictionaries. Note\nthat in this part we aim to use ﬁxed cluster centers stored\nin memory dictionaries from the teacher model to help the\nstudent model directly learn in a fast way to avoid label\nAlgorithm 2: Training process of the student model\nRequire: Unlabeled training data X\nRequire: Initialize the encoder fθ with\nImageNet-pretrained ResNet-50\nRequire: The teacher encoder ˜fθ trained on the\nunlabeled training data X using Algorithm\n1\nRequire: Balancing factors µ for Eq. (9)\n// warm up period\nExtract feature vector sets\n\b\nU gb, U up, U dw\t\nfrom\nX by ˜fθ;\nClustering\n\b\nU gb, U up, U dw\t\ninto C clusters with\nEq. (4) and DBSCAN;\nInitialize three memory dictionaries individually\nwith Eq. (5) ;\nfor i in [1, num iterations × 2] do\nSample P × K query images from X;\nCompute objective function with Eq. (8) ;\nend\n// knowledge distillation period\nfor n in [1,num epochs] do\nExtract feature vector sets\n\b\nU gb, U up, U dw\t\nfrom X by fθ;\nClustering\n\b\nU gb, U up, U dw\t\ninto C clusters\nwith Eq. (4) and DBSCAN;\nInitialize three memory dictionaries individually\nwith Eq. (5) ;\nfor i in [1,num iterations] do\nSample P × K query images from X;\nCompute objective function with Eq. (10) ;\nUpdate cluster feature representations with\nm and Eq. (11);\nend\nend\nnoise accumulation in the early period. Then in the knowledge\ndistillation part, for each epoch the the training procedure of\nthe student model is the same as the teacher model except\nthat we update parameters of the student model according\nto Eq. (10) which contains the regularization of knowledge\ndistillation from a global-local manner.\nIV. EXPERIMENT\nA. Datasets and Evaluation Protocol\nWe conduct experiments on three public person Re-ID\nbenchmarks, including Market-1501 [42], DukeMTMC-reID\n[43] and MSMT17 [25]. Market-1501 dataset contains 32,668\nimages of 1,501 IDs captured by 6 different cameras.\nDukeMTMC-reID dataset is another large-scale person ReID\ndataset, which contains 36,441 images of 702 IDs captured by\n8 different cameras. While MSMT17 dataset contains 126,441\nimages of 1,041 IDs captured by 15 different cameras. These\ndatasets are widely used in the person ReID tasks and the\ndetails of these three datasets are summarized in Table I.\nFollowing existing person ReID works [16], [17], [40], [42],\nwe also adopt mean average precision (mAP) and Cumulated\n8\nTABLE I: Statistics of three person ReID datasets used in our\nexperiments.\nDatasets\nCameras\nTraining\nTesting\nIDs\nImages\nQuery\nGallery\nMarket-1501 [42]\n6\n751\n12,936\n3,368\n19,732\nDukeMTMC-reID [43]\n8\n702\n16,522\n2,228\n17,661\nMSMT17 [25]\n15\n1,041\n32,621\n11,659\n51,027\nMatching Characteristics (CMC) as the evaluation metrics, and\nwe report Top-1, Top-5, and Top-10 of the CMC evaluation\nmetric in the paper. For fair comparisons, we don’t adopt any\npost processing techniques in the evaluation period. As the\nsetting of other purely unsupervised ReID works, we don’t\nuse any labeled data or other source domain datasets in the\ntraining process.\nB. Implementations Details\nWe use the Resnet-50 [41] initialized with the parameters\npre-trained on the ImageNet [56] as the backbone encoder.\nFollowing existing cluster contrast framework [17], we remove\nall sub-module layers after layer-4 and add GEM pooling fol-\nlowed by batch normalization layer [57] and L2-normalization\nlayer. During training, we use the DBSCAN [20] as clustering\nalgorithm to generate pseudo labels at the beginning of each\nepoch. During test phase, we only adopt the feature vector of\nthe ﬁrst global feature branch for computation efﬁciency.\nFor training, each mini-batch contains 256 images of 16\npseudo person identities, which are resized as 256 × 128. For\ninput images, random horizontal ﬂipping, padding, random\ncropping, and random erasing [58] are applied. To train our\nmodel, Adam optimizer with weight decay 5e-4 is adopted.\nWe set the initial learning rate as 3.5e-4, and reduce it every\n20 epochs for a total of 50 epochs. The balancing factor λ1\nin Eq. (4) is set to 0.2 while the balancing factor λ2 in Eq.\n(8) is set to 0.15. The balancing factor µ in Eq. (9) is set\nto 1. For DBSCAN clustering algorithm, the minimal number\nof neighbours is set to 4 and the maximum distance d is set\nto 0.6 for Market1501 and DukeMTMC-reID while 0.7 for\nMSMT17.\nC. Comparison with State-of-the-Arts\nWe compare our proposed method with the state-of-the-art\nunsupervised person ReID methods, including UDA person\nReID and fully unsupervised person ReID. The result is shown\nin Table II. We ﬁrst list the state-of-the-art UDA methods,\nincluding ECN [44], MMCL [28], JVTC [45], DG-Net++ [46],\nMMT [5], DCML [47], MEB [48], SPCL [16] and HCD [49].\nAlthough these methods leverage the knowledge of the source\ndomain, our proposed method outperforms all of them on these\nthree datasets. The reason is probably that the gap between\nsource and target domains is large and it is hard to transfer\nthe knowledge from source domain to the target domain.\nCompared with the state-of-the-art fully unsupervised per-\nson ReID methods, our proposed method also achieves better\nperformance. These methods include BUC [12], SSL [13],\nJVTC [45], MMCL [28], HCT [14], CycAs [50], GCL [51],\nSPCL [16], HCD [49], ICE [52], CCL [17], MCL [53],\nHDCPD [40], PPLR [54] and ISE [55]. Speciﬁcally, as\nshown in Table II, our proposed method achieves 85.8/94.5\nin mAP/rank-1 accuracy on Market-1501 and 76.2/86.7 in\nmAP/rank-1 on DukeMTMC-reID. On the MSMT17, our\nmethod achieves 39.5 in mAP and 67.9 in rank-1 accuracy.\nThese results validate the superiority of our proposed method.\nAs our proposed puriﬁcation modules are established on\nthe framework of CCL [17], our method outperforms CCL\nby 3.2%, 3.4% and 6.2% in terms of mAP on Market-\n1501, DukeMTMC-ReID and MSMT17 datasets, respectively.\nCompared with Market-1501 and DukeMTMC-ReID, more\ngains can be achieved on the MSMT17 dataset. The reason\nis probably that more noise exist in MSMT17 dataset as it is\nmore challenging compared with other datasets.\nD. Ablation Studies\nIn this section, we study the effectiveness of different\ncomponents and hyper-parameters in our proposed method. As\nour work is implemented based on the CCL [17], the hyper-\nparameters introduced in our method include the balancing\nfactors λ1, λ2 and µ. The other hyper-parameters follow the\nsetting of CCL.\n1) Different combinations of the components:\nAs our\nmethod is combined with two different puriﬁcation modules,\nwe conduct experiments on the three person ReID datasets\ndescribed in Sec. IV-A versus different combinations of dif-\nferent modules. As our work is implemented based on CCL,\nwe take CCL as baseline and our proposed modules include\nFP module and LP module. As shown in Table III, the ﬁrst line\nmeans the result of CCL on different person ReID datasets.\nCompared with previous methods, CCL can achieve a good\nperformance by taking advantage of contrastive learning and\ncluster center memory, but it is still limited by the feature\nbias and label noise as mentioned in the paper. The second\nline is the result of the combination of CCL and our proposed\nFP module, compared with the ﬁrst line we can ﬁnd that our\nproposed FP module can improve the baseline by 1.6%, 1.2%\nand 1.0% in terms of mAP on Market-1501, DukeMTMC-\nReID and MSMT17 datasets. The third line is the result of the\ncombination of CCL and our proposed LP module, compared\nwith the ﬁrst line, the improvement of 1.9%, 1.5% and 3.4%\nin terms of mAP can be achieved on these three datasets.\nThe last line denotes the result of the combination of CCL\nand our proposed two puriﬁcation modules. Compared with\nthe ﬁrst line, the improvement of 3.0%, 2.7% and 5.3% in\nterms of mAP can be achieved on these datasets. The result\nshows that our proposed two puriﬁcation modules can work in\na mutual beneﬁt way and the baseline with these two modules\ncan achieve the best performance. Furthermore, compare the\nthird line with the ﬁrst line we can also ﬁnd that the LP module\nis more effective on MSMT17 than the other two datasets.\nThe reason is probably that compared with Market-1501 and\nDukeMTMC-reID, the MSMT17 dataset is more challenging\nwhich contains more occluded images. Thus the LP module\ncan mitigate the severe side effect of label noise introduced in\nthe clustering process.\n9\nTABLE II: Experimental results of our proposed method and state-of-the-art methods on Market-1501, DukeMTMC-reID, and\nMSMT17. The top three results are marked as red, blue and green, respectively.\nMethod\nReference\nMarket-1501\nDukeMTMC-reID\nMSMT17\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nUnsupervised Domain Adaption\nECN [44]\nCVPR’19\n43.0\n75.1\n87.6\n91.6\n40.4\n63.3\n75.8\n80.4\n10.2\n30.2\n41.5\n46.8\nMMCL [28]\nCVPR’20\n60.4\n84.4\n92.8\n95.0\n51.4\n72.4\n82.9\n85.0\n16.2\n43.6\n54.3\n58.9\nJVTC [45]\nECCV’20\n61.1\n83.8\n93.0\n95.2\n56.2\n75.0\n85.1\n88.2\n20.3\n45.4\n58.4\n64.3\nDG-Net++ [46]\nECCV’20\n61.7\n82.1\n90.2\n92.7\n63.8\n78.9\n87.8\n90.4\n22.1\n48.8\n60.9\n65.9\nMMT [5]\nICLR’20\n71.2\n87.7\n94.9\n96.9\n65.1\n78.0\n88.8\n92.5\n23.3\n50.1\n63.9\n69.8\nDCML [47]\nECCV’20\n72.6\n87.9\n95.0\n96.7\n63.3\n79.1\n87.2\n89.4\n-\n-\n-\n-\nMEB [48]\nECCV’20\n76.0\n89.9\n96.0\n97.5\n66.1\n79.6\n88.3\n92.2\n-\n-\n-\n-\nSPCL [16]\nNeurIPS’20\n76.7\n90.3\n96.2\n97.7\n68.8\n82.9\n90.1\n92.5\n26.8\n53.7\n65.0\n69.8\nHCD [49]\nICCV’21\n80.2\n91.4\n-\n-\n71.2\n83.1\n-\n-\n29.3\n56.1\n-\n-\nFully Unsupervised\nBUC [12]\nAAAI’19\n29.6\n61.9\n73.5\n78.2\n22.1\n40.4\n52.5\n58.2\n-\n-\n-\n-\nSSL [13]\nCVPR’20\n37.8\n71.7\n83.8\n87.4\n28.6\n52.5\n63.5\n68.9\n-\n-\n-\n-\nJVTC [45]\nECCV’20\n41.8\n72.9\n84.2\n88.7\n42.2\n67.6\n78.0\n81.6\n15.1\n39.0\n50.9\n56.8\nMMCL [28]\nCVPR’20\n45.5\n80.3\n89.4\n92.3\n40.2\n65.2\n75.9\n80.0\n11.2\n35.4\n44.8\n49.8\nHCT [14]\nCVPR’20\n56.4\n80.0\n91.6\n95.2\n50.7\n69.6\n83.4\n87.4\n-\n-\n-\n-\nCycAs [50]\nECCV’20\n64.8\n84.8\n-\n-\n60.1\n77.9\n-\n-\n26.7\n50.1\n-\n-\nGCL [51]\nCVPR’21\n66.8\n87.3\n93.5\n95.5\n62.8\n82.9\n87.1\n88.5\n21.3\n45.7\n58.6\n64.5\nSPCL [16]\nNeurIPS’20\n73.1\n88.1\n95.1\n97.0\n65.3\n81.2\n90.3\n92.2\n19.1\n42.3\n55.6\n61.2\nHCD [49]\nICCV’21\n78.1\n91.1\n96.4\n97.7\n65.6\n79.8\n88.6\n91.6\n26.9\n53.7\n65.3\n70.2\nICE [52]\nICCV’21\n79.5\n92.0\n97.0\n98.1\n67.2\n81.3\n90.1\n93.0\n29.8\n59.0\n71.7\n77.0\nCCL [17]\nArxiv’21\n82.6\n93.0\n97.0\n98.1\n72.8\n85.7\n92.0\n93.5\n33.3\n63.3\n73.7\n77.8\nMCL [53]\nArxiv’21\n82.9\n92.7\n97.6\n98.7\n-\n-\n-\n-\n38.2\n66.5\n75.2\n79.7\nHDCPD [40]\nTIP’22\n84.5\n93.5\n97.6\n98.6\n73.5\n85.4\n92.2\n94.5\n24.6\n50.2\n61.4\n65.7\nPPLR [54]\nCVPR’22\n81.5\n92.8\n97.1\n98.1\n-\n-\n-\n-\n31.4\n61.1\n73.4\n77.8\nISE [55]\nCVPR’22\n85.3\n94.3\n98.0\n98.8\n-\n-\n-\n-\n37.0\n67.6\n77.5\n81.0\nOurs\n-\n85.8\n94.5\n97.8\n98.7\n76.2\n86.7\n93.0\n94.3\n39.5\n67.9\n78.0\n81.6\nTABLE III: Ablation study on Market-1501, DukeMTMC-\nreID, and MSMT17.\nMethod\nMarket-1501\nDukeMTMC-reID\nMSMT17\nmAP\nR1\nmAP\nR1\nmAP\nR1\nBaseline\n82.8\n92.7\n73.5\n85.3\n34.2\n64.2\nBaseline\n+FP\n84.4\n93.5\n74.7\n86.2\n35.2\n64.5\nBaseline\n+LP\n84.7\n93.6\n75.0\n86.5\n37.6\n67.6\nBaseline\n+FP+LP\n85.8\n94.5\n76.2\n86.7\n39.5\n67.9\n2) Balancing factors: As the aim of our proposed extra\ntwo branches is to encourage the model to explore more\ndiscriminative local cues, the weights of these branches play an\nimportant role in our experiment. Fig. 4 and Fig. 5 report the\nresult versus different values of balancing factors λ1 and λ2\nin Eq. (4) and Eq. (8) on Market-1501 and MSMT17 datasets,\nrespectively. Results in Fig. 4 (a) and Fig. 5 (a) show that the\nteacher model can achieve the bast results on Market-1501 and\nMSMT17 with λ1 set to 0.2 and 0.15, respectively. Compared\nwith λ1 = 0, the above settings lead to better performance,\nwhich veriﬁes the necessity of introducing local views into the\npseudo label generation. To balance between different datasets,\nwe set λ1 to 0.15 by default. From Fig. 4 (b) and Fig. 5 (b),\nFig. 4: Impact of hyper-parameter λ1 and λ2 of the teacher\nmodel on Market-1501. In (a) λ2 is ﬁxed to 0.15 while in (b)\nλ1 is ﬁxed to 0.2.\nwe can see that when λ2 is set to 0.2, the teacher model can\nachieve the best results on both datasets. This setting leads to\nbetter performance compared with λ2 = 0, which means the\nmodel can learn more discriminative feature representations\nwith local features involved in the training process.\nHyper-parameter µ in Eq. (9) is another important balancing\nfactor, which determines the weight of the guidance of teacher\nin the overall training process. If µ is too small, then the\n10\nFig. 5: Impact of hyper-parameter λ1 and λ2 of the teacher\nmodel on MSMT17. In (a) λ2 is ﬁxed to 0.15 while in (b) λ1\nis ﬁxed to 0.2.\nstudent model will learn without enough guidance from the\nteacher model. On the other hand, if µ is too large, the student\nmodel will be forced to mimic the teacher model, which limits\nthe generalization of the learned feature representations. Table\nIV and Table V show the results under different values of µ\non Market-1501 and MSMT17 datasets. As can be bseen, the\nmodel can achieve the best performance on both Market-1501\nand MSMT17 datasets with µ set to 0.5 and 1.0, respectively.\nTo balance between different datasets, we set µ to 1.0 in our\nexperiments for all datasets.\nTABLE IV: Impact of hyper-parameter µ on Market-1501.\nµ\nMarket-1501\nmAP\nR1\nR5\nR10\n0.5\n86.1\n94.3\n98.0\n98.7\n1.0\n85.8\n94.5\n97.8\n98.7\n1.5\n85.8\n93.9\n97.4\n98.4\n2.0\n85.8\n94.0\n97.5\n98.5\n2.5\n85.9\n94.4\n97.8\n98.3\n3.0\n85.8\n94.3\n97.7\n98.4\nTABLE V: Impact of hyper-parameter µ on MSMT17.\nµ\nMSMT17\nmAP\nR1\nR5\nR10\n0.5\n39.3\n67.5\n77.6\n81.4\n1.0\n39.5\n67.9\n78.0\n81.6\n1.5\n38.8\n67.4\n77.2\n81.1\n2.0\n38.7\n67.6\n77.2\n80.8\n2.5\n37.9\n66.8\n76.5\n80.2\n3.0\n37.5\n66.7\n76.4\n79.9\n3) Inﬂuence of knowledge distillation: To investigate the\ninﬂuence of knowledge distillation, we show the test accuracy\nof the baseline with/without knowledge distillation in each\nepoch. Due to the lack of ground truth labels, the model has\nto be trained with the pseudo labels generated by clustering\nalgorithm. In this way, the noise will be inevitably introduced\nin the convergence process as the model initialized with Ima-\ngeNet pre-trained ResNet-50 performs poorly on these person\nReID datasets. As shown in Fig. 6 and Fig. 7, the student\nmodel with the ofﬂine knowledge distillation converges faster\n0\n10\n20\n30\n40\n50\nepoch\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nmAP(%)\nbaseline without knowledge distillation\nbaseline with knowledge distillation\nFig. 6: Accuracy of our model with/without knowledge distil-\nlation during training on Market-1501.\n0\n10\n20\n30\n40\n50\nepoch\n0\n5\n10\n15\n20\n25\n30\n35\n40\nmAP(%)\nbaseline without knowledge distillation\nbaseline with knowledge distillation\nFig. 7: Accuracy of our model with/without knowledge distil-\nlation during training on MSMT17.\nthan its counterpart without knowledge distillation, since it\nmitigates the interference of noisy labels.\nTABLE VI: Results of different methods with IBN-ResNet-50\nbackbone on Market-1501, DukeMTMC-reID and MSMT17.\ndataset\nmethod\nmAP\nR1\nR5\nR10\nMarket-1501\nSPCL [16]\n73.8\n88.4\n95.3\n97.3\nCCL [17]\n84.1\n93.2\n97.6\n98.2\nOurs\n86.9\n94.4\n97.7\n98.5\nDukeMTMC-reID\nSPCL [16]\n66.7\n82.1\n90.0\n92.4\nCCL [17]\n74.2\n85.8\n92.1\n94.2\nOurs\n76.8\n88.2\n92.9\n94.7\nMSMT17\nSPCL [16]\n24.0\n48.9\n61.8\n67.1\nCCL [17]\n41.1\n69.1\n79.3\n83.1\nOurs\n47.9\n74.5\n83.8\n86.7\n4) Compared with IBN-ResNet-50 and ResNet-50 back-\nbones: As Instance Normalization (IN) can learn features\nthat are invariant to appearance changes, while Batch Nor-\nmalization (BN) is essential for preserving content related\ninformation, IBN-Net [59] can achieve better performance by\nintegrating Instance Normalization and Batch Normalization.\nThus, IBN-ResNet-50 can be regarded as a stronger baseline\nby replacing the BN in ResNet-50 with IBN. Comparing the\nresults in Table VI and Table II, we can ﬁnd that our proposed\n11\nFig. 8: T-SNE visualization of the learned features on a subset of Market-1501 training set (100 identities, 1,747 images).\nPoints of the same color represent features of the same identity. We show detailed images of some points in the ﬁgure. The\nred circle contains some hard negative samples, which share similar appearance but have different identities, while the green\ncircle contains hard positive samples, which have the same identity but quite different appearance. Compared with CCL, our\nmethod is more discriminative for the hard negative samples while have more compact features for the hard positive samples.\nFig. 9: Top 6 retrieval results of some hard queries on Market-1501 dataset. Note that the green/red boxes denote true/false\nretrieval results, respectively.\n12\nmethod can achieve better performance with the IBN-ResNet-\n50 backbone than the ResNet-50 backbone.\n5) Qualitative analysis of visualization: To further under-\nstand the discrimination ability of our method, we utilize t-\nSNE [60] to visualize the features learned by the baseline\nand our method. As shown in Fig. 8, features of the same\nidentity are usually clustered together in both CCL and our\nproposed method, which veriﬁes the effectiveness of CCL\nand our method. More speciﬁcally, we show some detailed\nimages of some points in the ﬁgure. The red circle contains\nsome hard negative samples, which share similar appearance\nbut have different identities, while the green circle contains\nhard positive samples, which have the same identity but quite\ndifferent appearance. We can see that CCL cannot effectively\ndistinguish these hard negative samples, i.e., they are close\nto each other in the embedding space and easily result in\nwrong clusters in the clustering process, while our method can\ndistinguish them effectively, i.e., being more discriminative\nfor the hard negative samples. On the other hand, for those\nhard positive samples, our method can produce more compact\nfeatures than CCL.\nWe also present some retrieval examples with top 6 retrieved\nimages in Fig. 9. Our puriﬁcation modules can greatly improve\nthe performance of the baseline CCL. In the ﬁrst two rows\nof Fig. 9, the baseline gets some false results for the query\ndue to the high similarity between different persons in terms\nof gender, clothes, etc., except the hair style. However, the\nbaseline with our proposed puriﬁcation modules can ﬁnd the\ntrue retrieval results, and the reason is that our proposed FP\nmodule helps the baseline learn a more discriminative feature\nrepresentation by capturing more detailed local cues, i.e., the\nhair style. Similarly, in the third row, the baseline with the\nproposed puriﬁcation modules gets more accurate retrieval\nresults by extracting more information about the logo of the\nshirt. In the last row, the baseline could be mislead by the\nsimilarity between the backpack and the blurry handbag. As a\nresult, these images may be easily merged to the same cluster\nin the early period of pseudo label generation process and the\nmodel could be biased due to the noise accumulation. But our\nmethod can deliver true retrieval results, and the reason is that\nour proposed LP module can guide the student model to learn\na more robust feature representation by taking advantage of\nthe trained teacher model.\nV. CONCLUSION\nIn the paper we propose the puriﬁcation method for un-\nsupervised person ReID. Two novel puriﬁcation modules are\ndevised. Speciﬁcally, the feature puriﬁcation module takes into\naccount the features from two local views to enrich the feature\nrepresentation to purify the inherent feature bias of the global\nfeature involved. The label noise puriﬁcation module helps\npurify the label noise by taking advantage of the knowledge\nof teacher model in an ofﬂine scheme. Extensive experiments\non three challenging person ReID datasets demonstrate the\nsuperiority of our method over state-of-the-art methods.\nREFERENCES\n[1] J. Zhang and D. Tao, “Empowering things with intelligence: a survey\nof the progress, challenges, and opportunities in artiﬁcial intelligence of\nthings,” IEEE Internet of Things Journal, vol. 8, no. 10, pp. 7789–7817,\n2020.\n[2] Z. Hu, C. Zhu, and G. He, “Hard-sample guided hybrid contrast\nlearning for unsupervised person re-identiﬁcation,” arXiv preprint\narXiv:2109.12333, 2021.\n[3] D. Kumar, P. Siva, P. Marchwica, and A. Wong, “Unsupervised domain\nadaptation in person re-id via k-reciprocal clustering and large-scale\nheterogeneous environment synthesis,” in Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, 2020, pp. 2645–\n2654.\n[4] Y. Ge, F. Zhu, R. Zhao, and H. Li, “Structured domain adaptation\nwith online relation regularization for unsupervised person re-id,” arXiv\npreprint arXiv:2003.06650, 2020.\n[5] Y. Ge, D. Chen, and H. Li, “Mutual mean-teaching: Pseudo label\nreﬁnery for unsupervised domain adaptation on person re-identiﬁcation,”\nin International Conference on Learning Representations, 2019.\n[6] G. Wei, C. Lan, W. Zeng, and Z. Chen, “Metaalign: Coordinating domain\nalignment and classiﬁcation for unsupervised domain adaptation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 16 643–16 653.\n[7] N. Xiao and L. Zhang, “Dynamic weighted learning for unsupervised\ndomain adaptation,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2021, pp. 15 242–15 251.\n[8] J. Na, H. Jung, H. J. Chang, and W. Hwang, “Fixbi: Bridging domain\nspaces for unsupervised domain adaptation,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2021, pp. 1094–1103.\n[9] Q. Zhang, J. Zhang, W. Liu, and D. Tao, “Category anchor-guided\nunsupervised domain adaptation for semantic segmentation,” Advances\nin Neural Information Processing Systems, vol. 32, 2019.\n[10] L. Gao, J. Zhang, L. Zhang, and D. Tao, “Dsp: Dual soft-paste for\nunsupervised domain adaptive semantic segmentation,” in Proceedings\nof the 29th ACM International Conference on Multimedia, 2021, pp.\n2825–2833.\n[11] W. Wang, Y. Cao, J. Zhang, F. He, Z.-J. Zha, Y. Wen, and D. Tao,\n“Exploring sequence feature alignment for domain adaptive detection\ntransformers,” in Proceedings of the 29th ACM International Conference\non Multimedia, 2021, pp. 1730–1738.\n[12] Y. Lin, X. Dong, L. Zheng, Y. Yan, and Y. Yang, “A bottom-up clustering\napproach to unsupervised person re-identiﬁcation,” in Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, vol. 33, no. 01, 2019, pp.\n8738–8745.\n[13] Y. Lin, L. Xie, Y. Wu, C. Yan, and Q. Tian, “Unsupervised person\nre-identiﬁcation via softened similarity learning,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 3390–3399.\n[14] K. Zeng, M. Ning, Y. Wang, and Y. Guo, “Hierarchical clustering with\nhard-batch triplet loss for person re-identiﬁcation,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 13 657–13 665.\n[15] ——, “Energy clustering for unsupervised person re-identiﬁcation,”\nImage and Vision Computing, vol. 98, p. 103913, 2020.\n[16] Y. Ge, F. Zhu, D. Chen, R. Zhao et al., “Self-paced contrastive learning\nwith hybrid memory for domain adaptive object re-id,” Advances in\nNeural Information Processing Systems, vol. 33, pp. 11 309–11 321,\n2020.\n[17] Z. Dai, G. Wang, W. Yuan, S. Zhu, and P. Tan, “Cluster contrast for\nunsupervised person re-identiﬁcation,” arXiv preprint arXiv:2103.11568,\n2021.\n[18] G. Wang, Y. Yuan, X. Chen, J. Li, and X. Zhou, “Learning discriminative\nfeatures with multiple granularities for person re-identiﬁcation,” in\nProceedings of the 26th ACM international conference on Multimedia,\n2018, pp. 274–282.\n[19] J. MacQueen et al., “Some methods for classiﬁcation and analysis of\nmultivariate observations,” in Proceedings of the ﬁfth Berkeley sympo-\nsium on mathematical statistics and probability, vol. 1, no. 14. Oakland,\nCA, USA, 1967, pp. 281–297.\n[20] M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., “A density-based\nalgorithm for discovering clusters in large spatial databases with noise.”\nin kdd, vol. 96, no. 34, 1996, pp. 226–231.\n[21] H. Fan, L. Zheng, C. Yan, and Y. Yang, “Unsupervised person re-\nidentiﬁcation: Clustering and ﬁne-tuning,” ACM Transactions on Multi-\n13\nmedia Computing, Communications, and Applications (TOMM), vol. 14,\nno. 4, pp. 1–18, 2018.\n[22] K. Zheng, W. Liu, L. He, T. Mei, J. Luo, and Z.-J. Zha, “Group-\naware label transfer for domain adaptive person re-identiﬁcation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 5310–5319.\n[23] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”\nAdvances in neural information processing systems, vol. 27, 2014.\n[24] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and J. Jiao, “Image-\nimage domain adaptation with preserved self-similarity and domain-\ndissimilarity for person re-identiﬁcation,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2018, pp. 994–\n1003.\n[25] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person transfer gan to bridge\ndomain gap for person re-identiﬁcation,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2018, pp. 79–\n88.\n[26] Y. Ge, F. Zhu, D. Chen, R. Zhao, X. Wang, and H. Li, “Structured\ndomain adaptation with online relation regularization for unsupervised\nperson re-id,” IEEE Transactions on Neural Networks and Learning\nSystems, 2022.\n[27] H.-X. Yu, W.-S. Zheng, A. Wu, X. Guo, S. Gong, and J.-H. Lai,\n“Unsupervised person re-identiﬁcation by soft multilabel learning,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 2148–2157.\n[28] D. Wang and S. Zhang, “Unsupervised person re-identiﬁcation via multi-\nlabel classiﬁcation,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 10 981–10 990.\n[29] Q. Li, X. Peng, Y. Qiao, and Q. Hao, “Unsupervised person re-\nidentiﬁcation with multi-label learning guided self-paced clustering,”\nPattern Recognition, p. 108521, 2022.\n[30] J. Zhang, Z. Chen, and D. Tao, “Towards high performance human\nkeypoint detection,” International Journal of Computer Vision, vol. 129,\nno. 9, pp. 2639–2662, 2021.\n[31] Y. Xu, J. Zhang, Q. Zhang, and D. Tao, “Vitpose: Simple vision\ntransformer baselines for human pose estimation,” arXiv preprint\narXiv:2204.12484, 2022.\n[32] Y. Fu, Y. Wei, Y. Zhou, H. Shi, G. Huang, X. Wang, Z. Yao, and\nT. Huang, “Horizontal pyramid matching for person re-identiﬁcation,”\nin Proceedings of the AAAI conference on artiﬁcial intelligence, vol. 33,\nno. 01, 2019, pp. 8295–8302.\n[33] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, “Beyond part models:\nPerson retrieval with reﬁned part pooling (and a strong convolutional\nbaseline),” in Proceedings of the European conference on computer\nvision (ECCV), 2018, pp. 480–496.\n[34] G. Wang, Y. Yuan, J. Li, S. Ge, and X. Zhou, “Receptive multi-\ngranularity representation for person re-identiﬁcation,” IEEE Transac-\ntions on Image Processing, vol. 29, pp. 6096–6109, 2020.\n[35] Y. Fu, Y. Wei, G. Wang, Y. Zhou, H. Shi, and T. S. Huang, “Self-\nsimilarity grouping: A simple unsupervised cross domain adaptation\napproach for person re-identiﬁcation,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2019, pp. 6112–6121.\n[36] S. Yun, J. Park, K. Lee, and J. Shin, “Regularizing class-wise predictions\nvia self-knowledge distillation,” in Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, 2020, pp. 13 876–\n13 885.\n[37] K. Kim, B. Ji, D. Yoon, and S. Hwang, “Self-knowledge distillation\nwith progressive reﬁnement of targets,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2021, pp. 6567–6576.\n[38] T. Li, L. Wang, and G. Wu, “Self supervision to distillation for long-\ntailed visual recognition,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2021, pp. 630–639.\n[39] Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, and P. Isola, “Rethink-\ning few-shot image classiﬁcation: a good embedding is all you need?”\nin Computer Vision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XIV 16.\nSpringer, 2020,\npp. 266–282.\n[40] D. Cheng, J. Zhou, N. Wang, and X. Gao, “Hybrid dynamic contrast\nand probability distillation for unsupervised person re-id,” IEEE Trans-\nactions on Image Processing, 2022.\n[41] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[42] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable\nperson re-identiﬁcation: A benchmark,” in Proceedings of the IEEE\ninternational conference on computer vision, 2015, pp. 1116–1124.\n[43] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, “Performance\nmeasures and a data set for multi-target, multi-camera tracking,” in\nEuropean conference on computer vision.\nSpringer, 2016, pp. 17–35.\n[44] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y. Yang, “Invariance matters:\nExemplar memory for domain adaptive person re-identiﬁcation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 598–607.\n[45] J. Li and S. Zhang, “Joint visual and temporal consistency for unsuper-\nvised domain adaptive person re-identiﬁcation,” in European Conference\non Computer Vision.\nSpringer, 2020, pp. 483–499.\n[46] Y. Zou, X. Yang, Z. Yu, B. V. Kumar, and J. Kautz, “Joint disentangling\nand adaptation for cross-domain person re-identiﬁcation,” in Computer\nVision–ECCV 2020: 16th European Conference, Glasgow, UK, August\n23–28, 2020, Proceedings, Part II 16.\nSpringer, 2020, pp. 87–104.\n[47] G. Chen, Y. Lu, J. Lu, and J. Zhou, “Deep credible metric learning for\nunsupervised domain adaptation person re-identiﬁcation,” in Computer\nVision–ECCV 2020: 16th European Conference, Glasgow, UK, August\n23–28, 2020, Proceedings, Part VIII 16.\nSpringer, 2020, pp. 643–659.\n[48] Y. Zhai, Q. Ye, S. Lu, M. Jia, R. Ji, and Y. Tian, “Multiple expert\nbrainstorming for domain adaptive person re-identiﬁcation,” in Computer\nVision–ECCV 2020: 16th European Conference, Glasgow, UK, August\n23–28, 2020, Proceedings, Part VII 16.\nSpringer, 2020, pp. 594–611.\n[49] Y. Zheng, S. Tang, G. Teng, Y. Ge, K. Liu, J. Qin, D. Qi, and D. Chen,\n“Online pseudo label generation by hierarchical cluster dynamics for\nadaptive person re-identiﬁcation,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2021, pp. 8371–8381.\n[50] Z. Wang, J. Zhang, L. Zheng, Y. Liu, Y. Sun, Y. Li, and S. Wang,\n“Cycas: Self-supervised cycle association for learning re-identiﬁable\ndescriptions,” in Computer Vision–ECCV 2020: 16th European Con-\nference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16.\nSpringer, 2020, pp. 72–88.\n[51] H. Chen, Y. Wang, B. Lagadec, A. Dantcheva, and F. Bremond,\n“Joint generative and contrastive learning for unsupervised person re-\nidentiﬁcation,” in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2021, pp. 2004–2013.\n[52] H. Chen, B. Lagadec, and F. Bremond, “Ice: Inter-instance contrastive\nencoding for unsupervised person re-identiﬁcation,” arXiv preprint\narXiv:2103.16364, 2021.\n[53] X. Jin, T. He, Z. Yin, X. Shen, T. Liu, X. Wang, J. Huang, X.-S. Hua, and\nZ. Chen, “Meta clustering learning for large-scale unsupervised person\nre-identiﬁcation,” arXiv preprint arXiv:2111.10032, 2021.\n[54] Y. Cho, W. J. Kim, S. Hong, and S.-E. Yoon, “Part-based pseudo label\nreﬁnement for unsupervised person re-identiﬁcation,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2022, pp. 7308–7318.\n[55] X. Zhang, D. Li, Z. Wang, J. Wang, E. Ding, J. Q. Shi, Z. Zhang,\nand J. Wang, “Implicit sample extension for unsupervised person re-\nidentiﬁcation,” in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2022, pp. 7369–7378.\n[56] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in 2009 IEEE conference on\ncomputer vision and pattern recognition.\nIeee, 2009, pp. 248–255.\n[57] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” in International\nconference on machine learning.\nPMLR, 2015, pp. 448–456.\n[58] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, “Random erasing\ndata augmentation,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 34, no. 07, 2020, pp. 13 001–13 008.\n[59] X. Pan, P. Luo, J. Shi, and X. Tang, “Two at once: Enhancing learning\nand generalization capacities via ibn-net,” in Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), 2018, pp. 464–479.\n[60] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.” Journal\nof machine learning research, vol. 9, no. 11, 2008.\n",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "published": "2022-04-21",
  "updated": "2022-06-22"
}