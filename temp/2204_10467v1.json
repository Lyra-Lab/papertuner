{
  "id": "http://arxiv.org/abs/2204.10467v1",
  "title": "Neural Contrastive Clustering: Fully Unsupervised Bias Reduction for Sentiment Classification",
  "authors": [
    "Jared Mowery"
  ],
  "abstract": "Background: Neural networks produce biased classification results due to\ncorrelation bias (they learn correlations between their inputs and outputs to\nclassify samples, even when those correlations do not represent\ncause-and-effect relationships).\n  Objective: This study introduces a fully unsupervised method of mitigating\ncorrelation bias, demonstrated with sentiment classification on COVID-19 social\nmedia data.\n  Methods: Correlation bias in sentiment classification often arises in\nconversations about controversial topics. Therefore, this study uses\nadversarial learning to contrast clusters based on sentiment classification\nlabels, with clusters produced by unsupervised topic modeling. This discourages\nthe neural network from learning topic-related features that produce biased\nclassification results.\n  Results: Compared to a baseline classifier, neural contrastive clustering\napproximately doubles accuracy on bias-prone sentences for human-labeled\nCOVID-19 social media data, without adversely affecting the classifier's\noverall F1 score. Despite being a fully unsupervised approach, neural\ncontrastive clustering achieves a larger improvement in accuracy on bias-prone\nsentences than a supervised masking approach.\n  Conclusions: Neural contrastive clustering reduces correlation bias in\nsentiment text classification. Further research is needed to explore\ngeneralizing this technique to other neural network architectures and\napplication domains.",
  "text": "Neural Contrastive Clustering: Fully Unsupervised Bias Reduction for \nSentiment Classification\nJared Mowery\nAbstract\nBackground: Neural networks produce biased classification results due to correlation bias (they \nlearn correlations between their inputs and outputs to classify samples, even when those \ncorrelations do not represent cause-and-effect relationships).\nObjective: This study introduces a fully unsupervised method of mitigating correlation bias, \ndemonstrated with sentiment classification on COVID-19 social media data. \nMethods: Correlation bias in sentiment classification often arises in conversations about \ncontroversial topics. Therefore, this study uses adversarial learning to contrast clusters based on \nsentiment classification labels, with clusters produced by unsupervised topic modeling. This \ndiscourages the neural network from learning topic-related features that produce biased \nclassification results. \nResults: Compared to a baseline classifier, neural contrastive clustering approximately doubles \naccuracy on bias-prone sentences for human-labeled COVID-19 social media data, without \nadversely affecting the classifier’s overall F1 score. Despite being a fully unsupervised approach,\nneural contrastive clustering achieves a larger improvement in accuracy on bias-prone sentences \nthan a supervised masking approach.\nConclusions: Neural contrastive clustering reduces correlation bias in sentiment text \nclassification. Further research is needed to explore generalizing this technique to other neural \nnetwork architectures and application domains. \nKeywords: natural language processing, social media, bias, debiasing, adversarial representation\nlearning\nCorrespondence: jared.mowery@gmail.com\n1\nIntroduction\nCrises such as the COVID-19 pandemic can create rapidly emerging and unpredictable sources \nof bias in sentiment classification, and effectively mitigating these biases requires an automatic \nand fully unsupervised approach. Currently, most bias mitigation approaches are only effective \nin supervised or weakly supervised settings, and in both of these settings, the approaches are \ntypically limited to mitigating demographics-related sources of bias, such as gender bias. \nUnknown, unpredictable sources of bias which are specific to a new subject, like the COVID-19 \npandemic, are harder to mitigate since it is difficult to anticipate which sources of bias will \nemerge, and it is time-consuming to produce supervised data to retrain classifiers to address each\nnew source of bias. This study addresses these challenges by providing a fully unsupervised \napproach for mitigating correlation bias, which occurs when classifiers learn to perform \nclassification tasks using correlations between words and emotions, even when those correlations\ndo not represent expressions of emotion. For example, words like “hydroxychloroquine”, \n“virus”, and “mandate” may not normally correlate with an emotion in most situations, but \nduring the COVID-19 pandemic, these words become frequently correlated with expressions of \nanger in online discussions, even though the words themselves convey no emotion. Without \ncorrelation bias mitigation, the classifier learns these correlations, and consequently becomes \nbiased to interpret sentences containing these words as expressing anger.\nA recent survey [1] of bias mitigation approaches, which described multiple types of bias and the\nsupervised bias mitigation algorithms that have been developed so far, highlighted the need for \nunsupervised bias mitigation algorithms. The survey authors found three unsupervised bias \nmitigation studies, but none of the three studies were designed to mitigate correlation bias, \nalthough one may indirectly reduce correlation bias. \nThis paper introduces a fully unsupervised bias mitigation algorithm for correlation bias, and \ndemonstrates its effectiveness in sentiment classification for COVID-19 social media posts. \nThe remainder of this section will provide a brief overview of bias and discrimination in neural \nnetworks and bias mitigation research. Since unsupervised techniques for bias mitigation are rare\nand have only been developed for certain types of bias, this overview will also include other \ntechniques which may be adapted to unsupervised bias mitigation. This includes adversarial \nlearning in supervised bias mitigation techniques, weakly supervised techniques, and several \ncomputer vision studies. \nBias and discrimination in neural networks raise numerous cross-disciplinary questions involving\nlegal, ethical, and societal factors [2]. Bias exists in many natural language processing \ntechnologies, including fundamental technologies for sentiment classification: word embeddings,\nlanguage models, and the sentiment classifiers themselves. Word embeddings have been proven \nto contain human-recognizable bias by demonstrating a statistically significant correlation \nbetween implicit association tests used by psychologists and cosine similarity distances between \nbaskets of relevant words in the word embedding [3], and a similar result holds for image \nclassification [4]. A survey of bias in natural language generation reveals considerable \nchallenges [5], and language models have specifically been shown to exhibit bias in expressed \nsentiment when generating text based on country names, occupations, and genders [6].  \nSystematic race and gender bias has been detected in an analysis of over 200 sentiment analysis \nsystems [7]. The evidence for bias in sentiment classification systems raises significant risks for \n2\nreal world applications, especially when the results may be used to inform policy makers or in \nautomated processing systems that could exhibit discrimination. \nTo mitigate this bias, supervised and weakly supervised techniques have been developed which \nleverage lists of protected attributes, correlated attributes or word lists to mitigate bias. \nSupervised techniques have been developed to mitigate bias when protected attributes are known\nand data labeled with those attributes is available, especially when the protected attributes are \ndemographic, such as age, gender, and race [8]. Adversarial representation learning has been \napplied with known protected attributes in conjunction with Kullback-Leibler divergences \nbetween output distributions and desired, unbiased distributions to minimize bias in the network \n(e.g. in computer vision [9]), or in adversarial learning approaches in which part of a classifier \nlearns to perform the intended classification task while another part “unlearns” how to predict the\nprotected attribute (e.g. [10] [11] [12] [13] [14]). In cases where protected attributes are \nunavailable but correlate with available attributes, such as detected dialect correlating with race \n[15] or race correlating with ZIP codes, the correlated attributes can be used as proxies for the \nprotected attributes [16] [17]. \nFor some protected attributes, weakly supervised techniques have been developed to alleviate the\nneed for labeled data. These approaches often rely on word lists that refer to a protected attribute,\nsuch as gendered words or references to race. Examples include reducing gender bias in word \nembeddings by augmenting the text data with versions in which gendered words are swapped \n[18], minimizing the differences between gendered words on a subspace representing gender bias\nthrough adversarial learning [19], introducing a loss term to minimize a projection of an encoder-\ntrained word embedding onto a subspace that encodes gender [20], and leveraging pairs of word \nlists that indirectly define biases [21]. Word lists that have been manually labeled to indicate \nwhether each word is relevant to a classification task or bias-prone can also help reduce bias. \nThese lists can be used to reduce the bias neural networks learn from their training data via \nobjective functions that encourage learning features from relevant words and discourage learning\nfeatures related to bias-prone words [22], or to mask input words for a trained neural network to \nprevent it from using bias-prone words in its sentiment classification decisions [23]. \nGenerating lists of protected attributes, correlated attributes or word lists is labor intensive, so \nsome semi-automated techniques have been developed to reduce the labor required to generate \nthese lists of relevant or bias-prone tokens (e.g. words, URLs, hashtags, names), as well as help \nresearchers explore types of bias. For example, Ferrer et al. [24] [25] use a pair of hand-chosen \nsets of words representing a type of bias (e.g. lists of male and female gendered words to \nrepresent gender bias) to define a type of bias to explore. They automatically identify candidate \nwords that may be expressions of bias related to those word lists by calculating the centroid of \neach word list in a word embedding, and estimating the bias-propensity of each remaining word \nbased on its frequency of occurrence in the corpus and proximity to either centroid. Each word is\nscored for positive or negative sentiment using pre-trained algorithms, and words belonging to \nparts of speech that are unlikely to exhibit bias (e.g. articles and proper nouns) are ignored. \nFinally, clusters of words that exhibit strong negative sentiment and a close proximity to either \ncentroid can be clustered and categorized using a semantic analysis system to give an end user \nclues about the nature of the bias in a given corpus of text, such as showing that many of the \ncandidate gender biased terms used by a Reddit community are associated with the semantic \ncategories of “Relationship: Intimate/Sexual” and “Power, Organizing”. Approaches like these \nreduce human annotation labor by requiring only the initial word lists to estimate the likelihood \n3\nthat other words may reflect bias. \nThe previously mentioned bias mitigation survey that highlighted a lack of unsupervised \nalgorithms [1] cited three papers covering two types of unsupervised algorithms. The first type of\nunsupervised algorithm compensates for bias arising from the distribution of the training data: \nsparse portions of the input distribution may correspond to underrepresented groups and achieve \nlower classification accuracy compared to dense portions of the input space, so these algorithms \nuse distributionally robust optimization [26] [27] or adversarial weighting of samples [28] to \nensure fairer accuracy across the input space. The second type of unsupervised algorithm \ndevelops disentangled feature representations for a computer vision task and demonstrates a \ncorrelation between using the disentangled representations and fairness [29]. Disentangling \nfeature representations may produce features that are less likely to conflate protected attributes \nand features relevant to the classification task. This could increase the odds that classifiers focus \non the relevant features, and consequently indirectly address correlation bias. However, a more \ndirect approach is still needed to mitigate correlation bias, since the disentangled features for \nprotected attributes will often correlate with the classification labels, and consequently be \nlearned by classifiers. Future research could explore combining disentangled features with \ncorrelation bias mitigation to improve the degree of correlation bias mitigation. \nThe algorithm most closely related to this study is a fully unsupervised, hybrid statistical and \nmachine learning technique that discovers bias-prone tokens and mitigates bias in sentiment \nclassifiers, without retraining the classifier [30]. It identifies bias-prone words by contrasting \nclusters of sentences labeled as expressing anger or negative sentiment against clusters of \nsentences produced via unsupervised topic modeling. Since many bias-prone words pertain to \ncontroversial topics, words that are more strongly associated with topics are disproportionately \nlikely to be bias-prone (e.g. “hydroxychloroquine”), while words that are more strongly \nassociated with the emotion or sentiment labels (e.g. “hate”) are more likely to be expressions of \nemotion or sentiment. The algorithm uses a pre-trained word embedding to improve accuracy in \nits classification of words as bias-prone by recognizing that bias-prone words’ nearest neighbors \nin the embedding space should not be expressions of emotion or sentiment, and vice versa. The \nalgorithm’s identification of bias-prone words is sufficiently accurate that automatically masking\nthem in existing sentiment and emotion classifiers’ inputs mitigates bias, without the algorithm \nrequiring a specification of types of bias to mitigate, protected attributes, or hand-chosen word \nlists. The algorithm presented in this study also uses unsupervised topic modeling to achieve \nunsupervised bias mitigation, but takes a different approach: this study uses adversarial learning \nto debias the neural network itself and eliminate the need for masking, rather than masking \ninputs to an already trained neural network. \nWhile the scope of this paper is limited to correlation bias, the lack of unsupervised algorithms \nfor mitigating any forms of bias makes existing unsupervised algorithms noteworthy, in addition \nto the unsupervised correlation bias mitigation algorithm described in the previous paragraph.\nMethods\nThis study presents a fully unsupervised algorithm for mitigating correlation bias via adversarial \ntraining and unsupervised topic modeling. Since topic modeling relies on word embeddings, \nwhich in turn reflect human biases [3], a classifier can be debiased by “unlearning” topic \nclassification while learning to classify sentiment. However, while [30] treated the sentiment \n4\nclassifier as a black box and masked its inputs to achieve a reduction in bias, this study \nincorporates topic classification into the training process via adversarial learning. As bias \nmitigation research continues, in theory, de-biasing the network itself should outperform \nmasking, since masking ignores the context of words, while the network can unlearn bias in a \ncontext-aware manner and also unlearn bias from words indirectly associated with the bias-prone\nwords. \nThe remainder of this section will describe preparing the data (Data Preparation) and the \nadversarial learning algorithm (Adversarial Learning). \nData Preparation\nReddit social media discussion forum data was gathered to produce training, development, and \ntest data sets using a two step query process. The first step queried for posts containing COVID-\n19 specific terms, such as “COVID-19” and “coronavirus”. The second step segmented those \nposts into sentences and kept only sentences matching a much broader query for pandemic \nsubtopics (e.g. “vaccine” or “CDC”). This two-step process is designed to gather a wide diversity\nof data: the first step ensures posts are relevant to COVID-19, while the second step ensures \nsentences are included across a wide range of pandemic subtopics. If only the query terms from \nthe first step were used, the resulting data set would be too narrowly focused on words like \n“COVID-19”, which would not be representative of online discussions and could produce an \nartificially simple sentiment classification problem. \nMany sentiment analysis studies use weakly supervised query terms (e.g. emojis or hashtags) or \nexplicit sentiment-bearing terms (e.g. “angry”) to gather posts that contain a higher likelihood of \nexpressing sentiment. While this approach reduces the number of sentences that need to be \nannotated to obtain a reasonable number of sentences expressing anger, it would jeopardize the \nvalidity of this study since it can also introduce biases into the training data: the data set would \ninclude an artificially narrow or homogeneous subset of expressions of sentiment, and those \nexpressions of sentiment could be disproportionately overt or susceptible to bias. Therefore, no \nsentiment-related terms were used in either querying step.\nThe posts were gathered from December 2019 through November 2021. A random sample of \n20,043 sentences were then annotated for whether they (1) express anger, and (2) could easily be \nmisinterpreted as an expression of anger by a biased annotator. For brevity, bias-prone \nsentences will be used to refer to the sentences that could be labeled as expressing anger by a \nbiased annotator. Anger was chosen due to its prevalence in the data and the overall rarity of \nexpressions of sentiment in the data (due to not using sentiment-specific query terms). The \nannotation process yielded 263 bias-prone sentences (1.3%) and 3,477 angry sentences (17.3%). \nHowever, note that the 1.3% prevalence of bias-prone sentences is probably an underestimate, \nsince it reflects sentences with relatively straightforward biased interpretations, and that the \nmisclassification rate in sentiment classifiers due to correlation bias is likely to be much higher. \nThe annotations were split into training (60%), development (20%), and test (20%) data sets. \nNon-angry sentences were deleted from the training data set at random to equalize the number of\nangry and non-angry sentences, reducing the number of sentences from 12,025 to 4,172, split \nevenly between angry and non-angry labels. \nTo compare the algorithm in this study to an idealized case of masking bias-prone tokens (i.e. \n5\nreplacing those tokens with “it” so they do not influence the sentiment classifier’s decisions), the \nannotator also labeled the 10,000 tokens that occur most frequently in the corpus to indicate \nwhether they represent expressions of sentiment, and whether they belong to topics that can be \nthe subject of angry discussions (and, therefore, likely sources of correlation bias). The annotator\nlabeled 373 as expressions of sentiment and 1,029 as potentially bias-prone. \nFinally, the sentences were given topic labels using BERTopic [31], a topic modeling algorithm. \nTo reduce the topic labels to a binary classification problem, a sentence was labeled as having a \nrecognizable topic if the maximum-scoring topic from BERTopic was greater than 0.25 (on a \nzero to one scale), and that score was greater than a score BERTopic provides for a \nmiscellaneous topic that indicates the sentence had no recognizable topic. BERTopic was \ninitialized to find 50 topics using 113,438 unlabeled sentences from across the December 2019 to\nNovember 2021 time period, and then used to label the annotated data with topics. The 50 topics \nincluded the miscellaneous topic and provided a reasonable variety of topics. \nAdversarial Learning\nThis study uses a three-part neural network architecture consisting of a shared component and \ntwo classification heads. The shared component includes the pre-trained RoBERTa language \nmodel from TweetEval [32] coupled with a maximum pooling layer or a Gated Recurrent Unit \n(GRU) [33] recurrent neural network layer followed by a maximum pooling layer. Both \nclassification heads use a pair of fully connected layers, with one classification head dedicated to\nsentiment classification and the other to topic classification. Dropout regularization [34] is used \nin all three components of the neural network. \nThe adversarial learning approach trains the shared component and sentiment classification head \nto maximize sentiment classification accuracy. Meanwhile, it trains the topic classification head \nto maximize topic classification accuracy and trains the shared component to minimize topic \nclassification accuracy, via inverted topic labels. To formalize this concept, define the trainable \nparameter sets θ for the shared component (language model and GRU), ϕ for the topic \nclassification head, and ψ for the sentiment classification head. Define the training data sets as Ds\nfor sentiment classification, Dt for topic classification, and D-t for “anti-topic” classification, in \nwhich the positive and negative class labels in Dt have been swapped. Next, let x denote the \ninput, z the encoded latent representation (the output of the shared component), \n^ys  the \npredicted sentiment labels, \n^yt  the predicted topic labels, and \n^y−t  the predicted anti-topic \nlabels. Letting L represent the binary cross-entropy loss function, the neural network’s objective \nfunction is:\nmaxθ maxϕ maxψ L ^y s(Ds;θ ,ψ )+L ^yt(Dt;ϕ)+L ^y−t(D−t;θ )\n \nDefine the encoders and decoders as f θ :x→z  for the encoder, f ψ : z→^y s  for the sentiment \ndecoder, f ϕ :z→^yt  for the topic decoder, and f θ :x→z  for the anti-topic decoder. Now the \nthree-step optimization procedure for the objective function can be written as:\n1. Sentiment training: train fθ and fψ on Ds and yt while holding ϕ fixed\n2. Topic training: train fϕ on Dt while holding θ and ψ fixed\n3. Anti-topic training: train fθ (the encoder) on D-t while holding ϕ and ψ fixed\n6\nThe data sets Dt and D-t use the same samples for each batch of training data and the order of the \nsamples is jointly randomized for each training epoch, so steps 2 and 3 will always perform \nregular and adversarial topic classification updates on the same sample sentences. The weight \nupdates for step 2 are completed prior to any calculations for step 3, to avoid creating potential \ndiscontinuous weight updates at the junction between the shared model components and the topic\nclassification head. Ds uses its own samples for each training epoch and the order is randomized \nindependently of Dt and D-t, to permit the sentiment and topic data sets to be independently \nbalanced and to reduce the risk of introducing instabilities.\nThe training procedure also includes an optional loss coefficient for the topic and anti-topic loss \nfunctions. The loss coefficient controls the relative importance of topic/anti-topic and sentiment \nclassification accuracy (the sentiment loss function’s coefficient is always 1). The topic loss \ncoefficient can be used to control the relative importance the algorithm places on debiasing \nversus sentiment classification accuracy, and consequently, on which features the classifier \nlearns based on how strongly each feature correlates with sentiment labels versus topic labels. In \ncurrent adversarial debiasing, the protected attributes are known and assumed to be the complete \nset of protected attributes, so it is intuitive to completely eliminate the classifier’s ability to \npredict the protected attributes. However, in this study, the protected attributes are unknown, and\nonly hypothesized to correlate with topics. Therefore, it is not necessarily desirable to eliminate \nall topic-related features from the latent representation z, and the topic coefficient provides a \nmechanism to determine the extent to which topic-related features are removed from z. \nThe topic training data, Dt, consists of 4,172 sentences that were sampled from the annotated \ntraining data, with a 50/50 split between sentences containing topics and sentences belonging to \nthe miscellaneous topic (i.e. the sentence had no recognizable topic). The number was chosen to \nexactly equal the size of the sentiment training data. However, to potentially reduce the risk of \ninstability during adversarial training, the subset of annotated sentences used to produce the topic\ndata set was chosen independently from the 12,025 sentiment training sentences (i.e. prior to \nbalancing the number of angry and non-angry sentences), which should reduce the risk of \nmultiple, significant weight updates affecting the same set of parameters during batch training. \nThe sentences for the miscellaneous topic were chosen at random.\nSince correlation bias arises from correlations between contentious topics and emotion labels, the\nbias mitigation effectiveness of the topic and anti-topic training can be increased by ensuring the \ntopic training data set includes the topics which most frequently co-occur with expressions of \nanger. The remainder of this section describes the sampling strategy used to select the 2,086 \nsentences with recognizable topics for Dt. \nTo define a metric for the topic and anger co-occurrence frequency, let each topic t in the \nannotated training data, excluding the miscellaneous topic, consist of |t| sentences, and let the set \nof training data sentences labeled with anger be A. Now define the lower bound of a 95% \nconfidence interval for the probability that a sentence s in t is labeled with anger as:\n~P(s∈A∣s∈t)=Wilson(|t∩A|,|t|)\nwhere Wilson is the lower bound of the Wilson confidence interval with continuity correction \n[35]. Using the lower bound of a confidence interval accounts for sample size, so that topics with\n7\nvery few sentences do not spuriously appear to be highly correlated with anger. Define the \nexpected number of sentences in t expressing anger as:\nnt=~P(s∈A∣s∈t)⋅|t|\nNext, normalize the set of expected values across all topics T by dividing by their sum:\n¯nt= nt\n∑\ni∈T\nni\nFinally, the number of sentences to sample for each topic t is 2,086⋅¯nt . This ensures Dt \ncontains sentences representative of the topics that most frequently overlap with expressions of \nanger, consequently improving the effectiveness of the adversarial training procedure in \nmitigating correlation bias. \nResults\nThis section quantifies the efficacy of the unsupervised correlation bias mitigation method by \ntesting eight main classifier variations for their overall F1 scores and for the fraction of bias-\nprone sentences that they correctly mark as not expressing anger. The eight classifier variations \nare defined by three parameters: whether the classifier is regular (R) or debiased (D), whether \nsupervised masking is applied to the test data to help reduce bias, and whether the classifier \nincludes the GRU layer or only a maximum pooling layer. Masking was implemented by \nreplacing bias-prone tokens with “it”, using the annotated list of 1,029 bias-prone tokens. This \nrepresents an idealized masking approach, since the annotations were produced by hand rather \nthan an unsupervised algorithm. \nThe results demonstrate the efficacy of unsupervised correlation bias mitigation (Table 1). The \nfirst four rows represent the typical, unmasked versions of the classifiers, with the debiased \nversion improving the bias accuracy by 0.34 (without GRU) or 0.29 (with GRU) points \ncompared to the regular classifier. The next four rows show that debiasing outperforms masking, \nand debiasing alone outperforms a combination of debiasing and masking.\nDue to the small number of hand-annotated examples of bias-prone sentences, the results were \naveraged across epochs 3 through 9, inclusive. All classifiers had converged by epoch 3, and \nnone exhibited signs of over-fitting through epoch 9, so averaging over those 7 epochs helps to \nreduce variance in the test results due to fluctuations in the neural network’s trainable weights. In\naddition, for the less computationally expensive non-GRU classifier, the test was repeated five \ntimes and the results were averaged.\n8\nTable 1: Comparison of regular (R) and debiased (D) classifiers, with and without Gated \nRecurrent Unit (GRU) layers and masking, for 4,009 test sentences. Results include the overall \nF1 score on the test data and the accuracy on a subset of 263 bias-prone sentences. Without \nmasking, debiasing improves the bias accuracy by 0.34 points without a GRU and by 0.29 points\nwith a GRU. Debiasing outperforms both supervised masking alone and a combination of \nmasking and debiasing. \nClassifier GRU\nMask\nF1\nBias Accuracy\nR\nN\nN\n0.61\n0.33\nR\nY\nN\n0.6\n0.31\nD\nN\nN\n0.59\n0.67\nD\nY\nN\n0.63\n0.6\nR\nN\nY\n0.59\n0.42\nR\nY\nY\n0.59\n0.43\nD\nN\nY\n0.58\n0.57\nD\nY\nY\n0.6\n0.54\nDiscussion\nThe results demonstrate that accuracy on bias-prone sentences can be approximately doubled for \nsentiment classification, even using a fully unsupervised approach. Unsupervised debiasing also \noutperformed an idealized debiasing approach using hand-annotated word lists to mask words. \nFurther research is needed to explore variations on this approach that may improve its \neffectiveness, to develop heuristics or algorithmic methods for selecting the correct topic loss \ncoefficient (all experiments used loss coefficients of 1), and to generalize this approach to other \napplication domains. \nIn terms of generalization, this study relies on a simple premise: correlation versus causation can \nbe approximated by contrasting the utility of features for the intended classification task against \ntheir utility in classifying samples according to an unsupervised clustering algorithm. While this \ncould be applicable to a wide variety of classification problems, it may be especially well-suited \nto natural language processing. Languages evolved to permit (reasonably) clear communication, \nwhich makes ambiguity undesirable. This encourages words to have relatively few semantically \ndistinct meanings, especially in context and for their most commonly used meanings. As a result,\ndiscovering that a contextualized word or phrase has a topic-related meaning makes it unlikely \nthat it will have a sentiment-related meaning, and vice versa. In application domains outside \nnatural language processing, this property may be weaker, resulting in the debiasing technique \nproducing weaker results. Future research could explore combining the algorithm presented in \nthis paper with disentangling features to improve accuracy. \nIn terms of limitations, the same simple premise may sometimes cause a classifier to unlearn \nuseful features, by mistaking them for features that only correlate with classification labels. This \ncannot be directly addressed until machine learning algorithms are capable of symbol grounding \n(learning what input features mean beyond a latent semantic sense) and reasoning over plausible \ncause-and-effect relationships. In the meantime, future research could investigate using the topic \nloss coefficients to control the trade-off between retaining useful features and rejecting features \n9\nthat only correlate with the classification task. Extensions to the selection and weighting of \ntopics could also be explored, including replacing the existing binary topic classification heads \nwith multilabel heads to better account for individual variation in topics, especially when certain \ntopics may be especially related to the classification problem. For example, references to death \nand destruction will usually correlate with both negative sentiment and violent topics, leading to \nbias. In contrast, for discussions of fictional death and destruction, such as murder mysteries and \nvideo games, certain expressions of death and destruction may be expressions of positive \nsentiments, leading to the opposite bias. In such cases, it may be necessary to choose the topics \nand topic weights used as a basis for debiasing carefully, depending on whether the subject \nmatter for the classifier is known in advance or if the goal is to build a general purpose classifier \n(e.g. by including or excluding entertainment related topics). Topic modeling over time may also\nbe an interesting direction for future research, since sources of bias often change. For example, \n“hydroxychloroquine” may have initially been correlated with positive sentiment before \nswitching to a correlation with negative sentiment. More broadly, this study implements a \nheuristic for distinguishing correlation from causation, and future research may yield improved \nheuristics.  \nThe results revealed that combining unsupervised debiasing with masking actually yielded \npoorer accuracy on bias-prone sentences than debiasing alone. This may indicate debiasing \ndiscourages the classifier from learning obvious, bias-prone terms during training. Since those \nsame terms will often be in the set of masked terms, debiasing may reduce the effectiveness of \nmasking. Exploring masking techniques that identify more subtle bias-prone features (e.g. terms \nor combinations of terms) may still yield an improvement when combined with debiasing, and \nprovide a method for measuring how well the adversarial training procedure identifies and \nremoves these more subtle bias-prone features. \nFinally, the neural network architecture may significantly impact the effectiveness of the \ndebiasing technique. For example, some network architectures use latent feature representations \nwith less representational power, such as using a lower dimensionality z or using a separator \ntoken’s embedding representation instead of maximum pooling. These architectures may \ncomingle relevant features with bias-prone features to an extent that the debiasing algorithm \ncannot unlearn the bias-prone features while still achieving reasonable classification accuracy. \nConclusion\nNeural contrastive clustering is a fully unsupervised approach that approximately doubles \naccuracy in sentiment classification of bias-prone sentences for COVID-19 data, and \noutperforms supervised masking. Further research could explore improvements and \ngeneralizations of the neural contrastive clustering technique, as well as investigating its links to \nother unsupervised bias reduction methods for other types of bias, such as distributionally robust \noptimization for input distribution bias. Combining neural contrastive clustering with \ndisentangled feature representations developed for computer vision tasks may also improve its \neffectiveness. \n10\nReferences\n1. Wan M, Zha D, Liu N and Zou N: Modeling Techniques for Machine Learning Fairness: A \nSurvey. arXiv preprint arXiv:2111.03015. 2021.\n2. Ferrer X, van Nuenen T, Such JM, Coté M and Criado N: Bias and Discrimination in AI: a \ncross-disciplinary perspective. IEEE Technology and Society Magazine. 2021;40:72-80.\n3. Caliskan A, Bryson JJ and Narayanan A: Semantics derived automatically from language \ncorpora contain human-like biases. Science. 2017;356:183-186.\n4. Steed R and Caliskan A: Image Representations Learned With Unsupervised Pre-Training \nContain Human-like Biases. 2021:701–713. Available from: \nhttps://doi.org/10.1145/3442188.3445932 DOI: 10.1145/3442188.3445932\n5. Sheng E, Chang K-W, Natarajan P and Peng N: Societal Biases in Language Generation: \nProgress and Challenges. 2021:4275-4293. Available from: https://aclanthology.org/2021.acl-\nlong.330 DOI: 10.18653/v1/2021.acl-long.330\n6. Huang P-S, Zhang H, Jiang R, Stanforth R, Welbl J, Rae J, Maini V, Yogatama D and Kohli \nP: Reducing sentiment bias in language models via counterfactual evaluation. arXiv preprint \narXiv:1911.03064. 2019.\n7. Kiritchenko S and Mohammad S: Examining Gender and Race Bias in Two Hundred \nSentiment Analysis Systems. Proceedings of the Seventh Joint Conference on Lexical and \nComputational Semantics. 2018:43-53. Available from: https://aclanthology.org/S18-2005 DOI: \n10.18653/v1/S18-2005\n8. Caton S and Haas C: Fairness in Machine Learning: A Survey. 2020. Available from: \nhttps://arxiv.org/abs/2010.04053\n9. Alvi M, Zisserman A and Nellåker C: Turning a blind eye: Explicit removal of biases and \nvariation from deep neural network embeddings. 2018:0-0.\n10. Kim B, Kim H, Kim K, Kim S and Kim J: Learning Not to Learn: Training Deep Neural \nNetworks With Biased Data. 2019.\n11. Zhang BH, Lemoine B and Mitchell M: Mitigating Unwanted Biases with Adversarial \nLearning. Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. 2018. \nAvailable from: http://dx.doi.org/10.1145/3278721.3278779 DOI: 10.1145/3278721.3278779\n12. Edwards H and Storkey A: Censoring representations with an adversary. arXiv preprint \narXiv:1511.05897. 2015.\n13. Madras D, Creager E, Pitassi T and Zemel R: Learning Adversarially Fair and Transferable \nRepresentations. Proceedings of the 35th International Conference on Machine Learning. \n2018;80:3384-3393. Available from: http://proceedings.mlr.press/v80/madras18a.html\n11\n14. Knobbout M: Adversarial Learned Fair Representations using Dampening and Stacking. \narXiv preprint arXiv:2203.08637. 2022.\n15. Ball-Burack A, Lee MSA, Cobbe J and Singh J: Differential Tweetment: Mitigating Racial \nDialect Bias in Harmful Tweet Detection. 2021:116–128. Available from: \nhttps://doi.org/10.1145/3442188.3445875 DOI: 10.1145/3442188.3445875\n16. Gupta M, Cotter A, Fard MM and Wang S: Proxy fairness. arXiv preprint arXiv:1806.11212.\n2018.\n17. Datta A, Fredrikson M, Ko G, Mardziel P and Sen S: Proxy non-discrimination in data-\ndriven systems. arXiv preprint arXiv:1707.08120. 2017.\n18. Maudslay RH, Gonen H, Cotterell R and Teufel S: It's All in the Name: Mitigating Gender \nBias with Name-Based Counterfactual Data Substitution. CoRR. 2019;abs/1909.00871. \nAvailable from: http://arxiv.org/abs/1909.00871\n19. Sweeney C and Najafian M: Reducing Sentiment Polarity for Demographic Attributes in \nWord Embeddings Using Adversarial Learning. Proceedings of the 2020 Conference on \nFairness, Accountability, and Transparency. 2020:359–368. Available from: \nhttps://doi.org/10.1145/3351095.3372837 DOI: 10.1145/3351095.3372837\n20. Bordia S and Bowman SR: Identifying and Reducing Gender Bias in Word-Level Language \nModels. 2019:7-15. Available from: https://aclanthology.org/N19-3002 DOI: 10.18653/v1/N19-\n3002\n21. Lauscher A, Glavaš G, Ponzetto SP and Vulić I: A general framework for implicit and \nexplicit debiasing of distributional word vector spaces. 2020;34:8131-8138.\n22. Liu F and Avci B: Incorporating Priors with Feature Attribution on Text Classification. \nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics. \n2019:6274-6283. Available from: https://aclanthology.org/P19-1631 DOI: 10.18653/v1/P19-\n1631\n23. Ghili S, Kazemi E and Karbasi A: Eliminating Latent Discrimination: Train Then Mask. \nProceedings of the AAAI Conference on Artificial Intelligence. 2019;33:3672-3680. Available \nfrom: https://ojs.aaai.org/index.php/AAAI/article/view/4251 DOI: \n10.1609/aaai.v33i01.33013672\n24. Ferrer X, van Nuenen T, Such JM and Criado N: Discovering and Categorising Language \nBiases in Reddit. Proceedings of the International AAAI Conference on Web and Social Media. \n2021;15:140-151. Available from: https://ojs.aaai.org/index.php/ICWSM/article/view/18048\n25. Aran XF, Van Nuenen T, Criado N and Such J: Discovering and Interpreting Biased \nConcepts in Online Communities. IEEE Transactions on Knowledge and Data Engineering. \n2021.\n12\n26. Hashimoto T, Srivastava M, Namkoong H and Liang P: Fairness Without Demographics in \nRepeated Loss Minimization. Proceedings of the 35th International Conference on Machine \nLearning. 2018;80:1929-1938. Available from: \nhttps://proceedings.mlr.press/v80/hashimoto18a.html\n27. Sagawa S, Koh PW, Hashimoto TB and Liang P: Distributionally robust neural networks for \ngroup shifts: On the importance of regularization for worst-case generalization. arXiv preprint \narXiv:1911.08731. 2019.\n28. Lahoti P, Beutel A, Chen J, Lee K, Prost F, Thain N, Wang X and Chi E: Fairness without \nDemographics through Adversarially Reweighted Learning. Advances in Neural Information \nProcessing Systems. 2020;33:728-740. Available from: \nhttps://proceedings.neurips.cc/paper/2020/file/07fc15c9d169ee48573edd749d25945d-Paper.pdf\n29. Locatello F, Abbati G, Rainforth T, Bauer S, Schölkopf B and Bachem O: On the Fairness of\nDisentangled Representations. 2019;32. Available from: \nhttps://proceedings.neurips.cc/paper/2019/file/1b486d7a5189ebe8d8c46afc64b0d1b4-Paper.pdf\n30. Mowery J: Contrastive Clustering: Toward Unsupervised Bias Reduction for Emotion and \nSentiment Classification. arXiv preprint. 2021. Available from: http://arxiv.org/abs/2111.07448\n31. Grootendorst M: BERTopic: Leveraging BERT and c-TF-IDF to create easily interpretable \ntopics. 2020. Available from: https://doi.org/10.5281/zenodo.4381785 DOI: \n10.5281/zenodo.4381785\n32. Barbieri F, Camacho-Collados J, Espinosa Anke L and Neves L: TweetEval: Unified \nBenchmark and Comparative Evaluation for Tweet Classification. Findings of the Association \nfor Computational Linguistics: EMNLP 2020. 2020:1644-1650. Available from: \nhttps://aclanthology.org/2020.findings-emnlp.148 DOI: 10.18653/v1/2020.findings-emnlp.148\n33. Cho K, Van Merriënboer B, Bahdanau D and Bengio Y: On the properties of neural machine \ntranslation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259. 2014.\n34. Hinton GE, Srivastava N, Krizhevsky A, Sutskever I and Salakhutdinov RR: Improving \nneural networks by preventing co-adaptation of feature detectors. 2012. Available from: \nhttps://arxiv.org/abs/1207.0580 DOI: 10.48550/ARXIV.1207.0580\n35. Newcombe RG: Two-sided confidence intervals for the single proportion: comparison of \nseven methods. Statistics in medicine. 1998;17:857-872.\n13\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2022-04-22",
  "updated": "2022-04-22"
}