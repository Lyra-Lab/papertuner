{
  "id": "http://arxiv.org/abs/2204.11432v1",
  "title": "Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers",
  "authors": [
    "Tsung-Wei Ke",
    "Jyh-Jing Hwang",
    "Yunhui Guo",
    "Xudong Wang",
    "Stella X. Yu"
  ],
  "abstract": "Unsupervised semantic segmentation aims to discover groupings within and\nacross images that capture object and view-invariance of a category without\nexternal supervision. Grouping naturally has levels of granularity, creating\nambiguity in unsupervised segmentation. Existing methods avoid this ambiguity\nand treat it as a factor outside modeling, whereas we embrace it and desire\nhierarchical grouping consistency for unsupervised segmentation.\n  We approach unsupervised segmentation as a pixel-wise feature learning\nproblem. Our idea is that a good representation shall reveal not just a\nparticular level of grouping, but any level of grouping in a consistent and\npredictable manner. We enforce spatial consistency of grouping and bootstrap\nfeature learning with co-segmentation among multiple views of the same image,\nand enforce semantic consistency across the grouping hierarchy with clustering\ntransformers between coarse- and fine-grained features.\n  We deliver the first data-driven unsupervised hierarchical semantic\nsegmentation method called Hierarchical Segment Grouping (HSG). Capturing\nvisual similarity and statistical co-occurrences, HSG also outperforms existing\nunsupervised segmentation methods by a large margin on five major object- and\nscene-centric benchmarks. Our code is publicly available at\nhttps://github.com/twke18/HSG .",
  "text": "Unsupervised Hierarchical Semantic Segmentation with\nMultiview Cosegmentation and Clustering Transformers\nTsung-Wei Ke\nJyh-Jing Hwang\nYunhui Guo\nXudong Wang\nStella X. Yu\nUC Berkeley / ICSI\nAbstract\nUnsupervised semantic segmentation aims to discover\ngroupings within and across images that capture object-\nand view-invariance of a category without external supervi-\nsion. Grouping naturally has levels of granularity, creating\nambiguity in unsupervised segmentation. Existing methods\navoid this ambiguity and treat it as a factor outside model-\ning, whereas we embrace it and desire hierarchical group-\ning consistency for unsupervised segmentation.\nWe approach unsupervised segmentation as a pixel-wise\nfeature learning problem. Our idea is that a good represen-\ntation shall reveal not just a particular level of grouping, but\nany level of grouping in a consistent and predictable man-\nner. We enforce spatial consistency of grouping and boot-\nstrap feature learning with co-segmentation among multiple\nviews of the same image, and enforce semantic consistency\nacross the grouping hierarchy with clustering transformers\nbetween coarse- and fine-grained features.\nWe deliver the first data-driven unsupervised hierarchi-\ncal semantic segmentation method called Hierarchical Seg-\nment Grouping (HSG). Capturing visual similarity and sta-\ntistical co-occurrences, HSG also outperforms existing un-\nsupervised segmentation methods by a large margin on five\nmajor object- and scene-centric benchmarks.\n1. Introduction\nSemantic segmentation requires figuring out the seman-\ntic category for each pixel in an image. Learning such a seg-\nmenter from unlabeled data is particularly challenging, as\nneither pixel groupings nor semantic categories are known.\nIf pixel groupings are known, semantic segmentation is\nreduced to an unsupervised image (segment) recognition\nproblem, to which contrast learning methods [9, 20, 59, 62]\ncould apply, on computed segments instead of images.\nIf semantic categories are known, semantic segmentation\nis reduced to a weakly supervised segmentation problem\nwith coarse annotations of image-level tags; pixel labeling\ncan be predicted from image classifiers [32, 34].\nimage\nRevisit [56]\nSegSort [26]\nOur HSG\nFigure 1. We develop an unsupervised semantic segmentation\nmethod by embracing the ambiguity of grouping granularity and\ndesiring hierarchical grouping consistency for unsupervised seg-\nmentation. Top: We formulate it as a pixel-wise feature learning\nproblem, such that a good feature must be able to best reveal any\nlevel of grouping in a consistent and predictable manner. We boot-\nstrap feature learning from multiview cosegmentation and enforce\ngrouping consistency with clustering transformers. Bottom: Our\nmethod can not only deliver hierarchical semantic segmentation,\nbut also outperform the state-of-the-art unsupervised segmentation\nmethods by a large margin. Shown are sample Cityscapes results.\nThe fundamental task of unsupervised semantic segmen-\ntation is grouping, not semantics in terms of naming, which\nis unimportant other than the convenience of tagging seg-\nments in the same or different groups. The challenge of\nunsupervised semantic segmentation is to discover group-\nings within and across images that capture object- and view-\ninvariance of a category without external supervision, so\nthat (Fig. 1): 1) A baby’s face and body are parts of a whole\narXiv:2204.11432v1  [cs.CV]  25 Apr 2022\nin the same image; 2) The whole baby is separated from\nthe rest of the image; 3) A baby instance is more similar to\nanother baby instance than to a cat instance, despite their\ndifferent poses, illuminations, and backgrounds.\nSeveral representative approaches have been proposed\nfor tackling this challenge under different assumptions.\n• Visual similarity: SegSort [26] first partitions each im-\nage into segments based on contour cues and then by\nsegment-wise contrastive learning discovers clusters of\nvisually similar segments. However, semantics by visual\nsimilarity is far too restrictive: A semantic whole is of-\nten made up of visually dissimilar parts. Parts of body\nsuch as head and torso look very different; it is not their\nvisual similarity but their spatial adjacency and statistical\nco-occurrence that bind them together.\n• Spatial stability: IIC [29] maximizes the mutual infor-\nmation between clusterings from two views of the same\nimage related by a known spatial transformation, enforc-\ning stable clustering while assuming that a fixed number\nof clusters are equally likely within an image. It works\nbest for coarse and balanced texture segmentation and has\nmajor trouble scaling up with the scene complexity.\n• Image-wise feature learning: [56, 60] train representa-\ntions on object-centric datasets with multiscale cropping\nto sharpen the representation within the image. These\nmethods do not work well on scene-centric datasets where\nan image has more than one dominant semantic class.\nGrouping as well as semantics naturally have different\nlevels of granularity: A hand is an articulated configuration\nof a palm and five fingers, likewise a person of a head, a\ntorso, two arms, and two legs. Such an inherent grouping\nhierarchy poses a major challenge: Which level should an\nunsupervised segmentation method target at and what is the\nbasis for such a determination? Existing methods avoid this\nambiguity and treat it as either a factor outside the segmen-\ntation modeling, or an aspect of secondary concern.\nOur key insight is that the inherent hierarchical organi-\nzation of visual scenes is not a nuisance for scene parsing,\nbut a universal property that we can exploit and desire for\nunsupervised segmentation. This idea has previously led\nto a general image segmenter that handles texture and il-\nlusory contours through edges entirely without any explicit\ncharacterization of texture or curvilinearity [65]. We now\nadvance the concept to data-driven representation learning:\nA good representation shall reveal not just a particular level\nof grouping, but any level of grouping in a consistent and\npredictable manner across different levels of granularity.\nWe approach unsupervised semantic segmentation as an\nunsupervised pixel-wise feature learning problem. Our ob-\njective is to best produce a consistent hierarchical segmen-\ntation for each image in the entire dataset based entirely on\nhierarchical clusterings in the feature space (Fig. 1). Specif-\nically, given the pixel-wise feature, we perform hierarchical\ngroupings within and across images and their transformed\nversions (i.e.,views). In turn, groupings at each level impose\na desire on how the feature should be improved to maximize\nthe discrimination among different groups.\nOur model has two novel technical components:\n1)\nMultiview cosegmentation is to not only enforce spatial\nconsistency between segmentations across views, but also\nbootstrap feature learning from visual similarity and co-\noccurrences in a simpler clean setting; 2) Clustering trans-\nformers are used to enforce semantic consistency across\ndifferent levels of the feature grouping hierarchy.\nTo summarize, our work makes three contributions.\n1. We deliver the first unsupervised hierarchical seman-\ntic segmentation method that can produce parts and\nwholes in a data-driven manner from an arbitrary collec-\ntion of images, whether they come from object-centric\nor scene-centric datasets.\n2. We are the first to embrace the ambiguity of grouping\ngranularity and exploit the inherent grouping hierarchy\nof visual scenes to learn a pixel-wise feature representa-\ntion for unsupervised segmentation. It can thus discover\nsemantics based on not only visual similarity but also\nstatistical co-occurrences.\n3. We outperform existing unsupervised (hierarchical)\nsemantic segmentation methods by a large margin on\nnot only object-centric but also scene-centric datasets.\n2. Related Work\nImage segmentation refers to the task of partitioning\nan image into visually coherent regions.\nTraditional ap-\nproaches often consist of two steps: extracting local features\nand clustering them based on different criteria, e.g., mode-\nfinding [3, 10], or graph partitioning [16, 42, 52, 66, 67].\nHierarchical image segmentation has been supervisedly\nlearned from how humans perceive the organization of an\nimage [2]: While each individual segmentation targets a\nparticular level of grouping, the collection of individual seg-\nmentations present the perceptual hierarchy statistically.\nA typical choice for representing a hierarchical segmen-\ntation is contours: They are first detected to sharply localize\nregion boundaries [25, 63] and can then be removed one by\none to reveal coarser segmentations (OWT-UCM [2]).\nSuch models are trained on individual ground-truth seg-\nmentations, hoping that coarse and fine-grained organiza-\ntion would emerge automatically from common and rare\ncontour occurrences respectively in the training data.\nIn contrast, our model is trained on multi-level segmen-\ntations unsupervisedly discovered by feature clustering, and\nit also operates directly on segments instead of contours.\nSemantic segmentation refers to the task of partitioning\nan image into regions of different semantic classes. Most\ndeep learning models treat segmentation as a spatial exten-\nsion of image recognition and formulate it as a pixel-wise\nclassification problem. They are often based on Fully Con-\nvolutional Networks [7, 36, 40], incorporating information\nfrom multiple scales [8, 18, 22–24, 31–33, 35, 45, 53, 64].\nSegSort [26] does not formulate segmentation as pixel-\nwise labeling, but pixel-segment contrastive learning that\noperates directly on segments delineated by contours. It\nlearns pixel-wise features in a non-parametric way, with or\nwithout segmentation supervision. SPML [32] extends it to\nunify segmentation with various forms of weak supervision:\nimage-level tags, bounding boxes, scribbles, or points.\nUnsupervised semantic segmentation has been modeled\nby non-parametric methods using statistical features and\ngraphical models [39, 49, 54]. For example, [49] proposes\nto discover region boundaries by mining the statistical dif-\nferences of matched patches in coarsely aligned images.\nThere are roughly three lines of recent unsupervised se-\nmantic segmentation methods. 1) One way is to increase\nthe location sensitivity of the feature learned from im-\nages [9, 20, 59, 62], by either adding an additional con-\ntrastive loss between pixels based on feature correspon-\ndences across views [60], or using stronger augmentation\nand constrained cropping [51, 56]. 2) A pixel-level feature\nencoder can be learned directly by maximizing discrimina-\ntion between pixels based on either contour-induced seg-\nments [26] or region hierarchies [68] derived from OWT-\nUCM [2]. Segmentation is indicated by pixel feature sim-\nilarity and semantic labels can be inferred from retrieved\nnearest neighbours in a labeled set. 3) A pixel-wise cluster\npredictor can be directly learned by maximizing the mu-\ntual information between cluster predictions on augmented\nviews of the same instance at corresponding pixels [29, 47].\nOur model advances pixel-wise feature learning methods\n[26, 32, 69]: It contrasts features based on feature-induced\nhierarchical groupings themselves, and most strikingly, di-\nrectly outputs consistent hierarchical segmentations.\n3. Hierarchical Segment Grouping (HSG)\nWe approach unsupervised semantic segmentation as an\nunsupervised pixel-wise feature learning problem (Fig. 2).\nThe basic idea is that, once every pixel is transformed into\na point in the feature space, image segmentation becomes a\npoint clustering problem.\nSemantic segmentation and feature clustering form a pair\nof dual processes: 1) Clustering of feature X defines seg-\nmentation G in each image: Pixels with features in the same\n(different) clusters belong to the same (different) semantic\nregions. This idea is used to co-segment similar images\ngiven handcrafted features [30, 37, 48]. 2) Segmentation\nG defines the similarity of feature X: A pixel should be\nmapped close to its own segment group and far from other\nsegment groups in the feature space. This idea is used to\nFigure 2. Method overview. We aim to learn a CNN that maps\neach pixel to a point in the feature space V such that successively\nderived cluster features X0, X1, X2 produce good and consistent\nhierarchical pixel groupings Ge, G1, G2. Their consistency is en-\nforced through clustering transformers Cl+1\nl\n, which dictates how\nfeature clusters at level l map to feature clusters at level l+1. Note\nthat G0 results from clusters of V , and Ge from OWT-UCM edges.\nPl is the probabilistic version of Gl, and Gl the winner-take-all\nbinary version of Pl; P0 ∼G0. For l≥0, Pl+1 results from prop-\nagating Pl by Cl+1\nl\n. Groupings Ge, G1, G2 in turn impose desired\nfeature similarity and drive feature learning. We co-segment mul-\ntiple views of the same image to capture spatial consistency, visual\nsimilarity, statistical co-occurences, and semantic hierarchies.\nlearn the pairwise feature similarity [44] and pixel-wise fea-\nture [26, 32] given segmentations.\nOur key insight is that a good representation shall re-\nveal not just a particular level of grouping – as past co-\nsegmentation methods have explored, but any level of\ngrouping in a consistent and predictable manner. If we em-\nbrace the ambiguity of grouping granularity that all previ-\nous methods have avoided and desire the consistency of hi-\nerarchical semantic segmentation on the pixel-wise feature,\nwe address not only the shortcoming of cosegmentation, but\nalso provide a joint feature-segmentation learning solution.\nSpecifically, while there is no supervision available for\neither feature X or segmentation G, we can desire that: 1)\neach segmentation separates features well and 2) the coarser\nsegmentation defined by next-level feature clusters simply\nmerges the current finer segmentation. These strong con-\nstraints guide the feature learning towards quality hierar-\nchical segmentations, thereby better capturing semantics.\nOur model has two components: 1) multiview coseg-\nmention to robustify feature clustering against spatial trans-\nformation and appearance variations of visual scenes, and 2)\nclustering transformers to enforce consistent semantic seg-\nmentations across different levels of the feature grouping\nhierarchy. Both are necessary for mapping pixel features\nto segmentations, which in turn impose desired pairwise at-\ntraction and repulsion on the pixel features.\nIn the following, we first introduce our contrastive fea-\nture learning loss given any grouping G, and then describe\nhow we obtain three kinds of groupings within and across\nimages, and how we evaluate their goodness of grouping\nand enforce their consistency.\n3.1. Pixel-Segment Contrastive Feature Learning\nWe learn a pixel-wise feature extraction function f as a\nconvolutional neural network (CNN) with parameters θ. It\ntransforms image I to its pixel-wise feature V . Let vvvi be\nthe unit-length feature vector at pixel i of image I:\nvv\\p m b {v} _i \n=vf_i( I ;\\theta ), \\quad \\|\\pmb {v}_i\\| = 1.\n(1)\nSuppose that I is partitioned into segments (Fig. 3). Let uuus\nbe the feature vector for segment s, defined as the (length-\nnormalized) average pixel feature within the segment:\n  \\p m b {u }_s \\ p r op to \\tex t {\nmean}\\ l eft (\\pmb {v}_i: i \\text { in segment } s\\right ), \\quad \\|\\pmb {u}_s\\| = 1\n(2)\nConsider a batch of images and their pixel groupings\n{(I, G)}. We want to learn the right feature mapper f such\nthat all the pixels form distinctive clusters in the feature\nspace, each corresponding to a different semantic group.\nWe follow [26, 32] to formulate desired feature-wise at-\ntraction and repulsion not between pixels, but between pix-\nels and segments. Such contrastive learning across granular-\nity levels reduces computation, improves balance between\nattraction and repulsion, and is more effective [59].\nOur contrastive feature learning loss to minimize is:\n  \\los\ns\nF\n (G)\n\\\n!=\\!\n\\\nsum  _{i\n} \\!-\\\n!\n\\\nlog \n\\\n! \\ frac\n {\\su\nm\n \\\nlimi\nt\ns _ {s \\\ni n G_\ni^+}\\! \\exp {\\frac {\\pmb {v}_{i}^\\top \\pmb {u}_{s}}{T}}} {\\sum \\limits _{s \\in G_i^+}\\! \\exp {\\frac {\\pmb {v}_{i}^\\top \\pmb {u}_{s}}{T}} + \\sum \\limits _{s \\in G_i^-}\\! \\exp {\\frac {\\pmb {v}_{i}^\\top \\pmb {u}_{s}}{T}}} \\label {eqn:lossF}\n(3)\nwhere T is a temperature hyper-parameter that controls the\nconcentration level of the feature distribution. Ideally, vvvi\nshould be attracted to segments in the positive set G+\ni and\nrepelled by segments in the negative set G−\ni .\nOur batch of images consists of several augmented views\nof some training instances. For pixel i in a particular view of\nimage I, G+\ni includes segments of the same semantic group\nin any view of image I except i’s own segment, in order\nto achieve within-instance invariance, whereas G−\ni includes\nsegments of different semantic groups in any view of I, and\nsegments of training instances other than I, in order to max-\nimize between-instance discrimination [26, 62].\n3.2. Consistent Segmentations by View & Hierarchy\nFrom pixel feature V , we compute feature grouping G0\nand cluster feature X0. Our initial pixel grouping Ge is\nbased on OWT-UCM edges detected in the image. Next-\nlevel cluster feature Xl+1 and grouping Gl+1 are predicted\nfrom Gl with ensured consistency. We use three levels for\nthe sake of illustration (Fig. 3), but our procedure can be\nrepeated for more (coarser) levels.\nimage\nOWT-UCM\nfeature ↓\n↓clustering\ncoherent region Ge\nfine G1\ncoarse G2\nFigure 3. We co-segment multiple views (Column 1) of the same\nimage by OWT-UCM edges (Ge, Column 2) or by feature cluster-\ning at fine and coarse levels (G1, G2, Columns 3-4). White lines\nmark the segments derived from pixel feature clustering and OWT-\nUCM edges.\nThe color of feature points (pixels) mark group-\ning in the feature space (segmentation in the image) consistently\nacross rows in the same column, per spatial transformations be-\ntween views. G2’s coarse segmentations simply merge G1’s fine\nsegmentations, their consistency enforced by our clustering trans-\nformers. Minimizing Lf(Ge), Lf(G1), Lf(G2) ensures respec-\ntively that our learned feature is grounded in low-level coherence,\nyet with view invariance, and capable of capturing semantics at\nmultiple levels and producing hierarchical segmentations.\nBase cluster feature X0 and grouping G0, Ge. We seg-\nment each view of I by clustering pixel features, resulting in\nbase grouping G0 and cluster (centroid) feature X0 (Fig. 2).\nDuring training but not testing, we segment image I into\na fixed number of coherent regions according to its OWT-\nUCM edges [14], based on which we split each G0 region\nto obtain edge-conforming segments [26] marked by white\nlines in Fig. 3. For training, we obtain pixel grouping Ge\nby inferring the coherent region segmentation according to\nhow each view is spatially transformed from I.\nMinimizing Lf(Ge) encourages the feature to be similar\nnot only for different pixels of similar appearances in the\nimage, but also for corresponding pixels of different appear-\nances across views of I. The former grounds the feature f\nat respecting low-level appearance coherence, whereas the\nlatter develops view invariance in the feature.\nNext-level cluster feature Xl+1 and grouping Gl+1. Now\nwe have grouping G0 in the feature space of V , and for each\ncluster, we obtain its centroid feature in X0. We model how\ncluster feature Xl maps to cluster feature Xl+1, which cor-\nresponds to how segmentation at level l maps to segmenta-\ntion at level l + 1 in the image.\nWe adopt a probabilistic framework, where any feature\npoint xxx has a (soft assignment) probability belonging to a\ngroup determined by its cluster centroid. Let Pl(a) be the\nprobability of xxx in group a at level l:\n  P_{ l }(a) = \\ t ext {Prob}(G_l=a \\,|\\,\\pmb {x}).\n(4)\nTo ensure that feature points in the same group remain to-\ngether at the next level, we introduce group transition prob-\nability Cl+1\nl\n(a, b), the transition probability from group a\nat level l to group b at level l+1:\n  C_\n{\nl}^ {l + 1}(a,b) &  =  \\t e xt {Prob}(G_{l+1}=b \\,|\\, G_{l}=a).\n(5)\nPer the Bayesian rule, we have:\n  P_{l+ 1\n}\n(\nb) & =  \\su\nm\n _{ a} P_l(a) \\cdot C_{l}^{l+1}(a,b). \\label {eqn:hrchy_bayesian}\n(6)\nWriting Pl as a row vector, we can derive the soft group\nassignment Pl+1 for cluster feature X0 at level l+1:\n  P_ { l+ 1 } = \nP\n_ {l }  \\\nt i me\ns  C _ l ^ {l+1\n}\n = P_0\\times C_0^1 \\times C_1^2 \\times \\cdots \\times C_{l}^{l+1}.\n(7)\nClustering Transformers. Cl+1\nl\nis defined on multiview\ncosegmentation of each instance. We learn a function, in\nterms of a transformer [5], to naturally capture feature group\ntransitions for all the training instances. It enables more\nconsistent grouping compared to non-parametric clustering\nmethods such as KMeans, NCut [58], and FINCH [50].\nOur clustering transformer from level l to l + 1 maps\ngroup centroid feature Xl to the next-level group centroid\nfeature Xl+1, and simultaneously outputs the group transi-\ntion probability Cl+1\nl\n(Fig. 4).\nConsistent feature groupings.\nAt level l = 0, P0 has bi-\nnary values, indicating hard grouping G0. For next level\nl, we compute Pl+1 by propagating Pl with our cluster-\ning transformer Cl+1\nl\n, which also outputs Xl+1. We obtain\nGl+1 by binarizing Pl+1 with winner-take-all. By decreas-\ning the number of groups as l increases, we obtain consis-\ntent fine to coarse segmentations G1, G2 (Fig. 2).\nMinimizing Lf(G1) and Lf(G2) encourages the feature\nf to capture semantics at multiple levels and produce con-\nsistent hierarchical segmentations (Fig. 3).\n3.3. Goodness of Grouping\nWhile clustering transformers ensure grouping consis-\ntency across levels, we still need to drive feature learning\ntowards good segmentations. We follow [55] and super-\nvise our transformer with modularity maximization [46]\nand collapse regularization. The former seeks a partition\nthat results higher (lower) in-cluster (out-cluster) similar-\nity than the total expectation, whereas the latter encourages\nFigure 4. Our clustering transformer enforces grouping consis-\ntency across levels by mapping feature Xl to Xl+1 with fea-\nture transition Cl+1\nl\n. Xl+1 and Cl+1\nl\nare learned simultaneously.\nShown here for level l=0 in Fig. 2, the transformer encoder takes\ninputs Xl and outputs contextualized feature Yl. The transformer\ndecoder takes learnable inputs from query embeddings Ql+1, and\noutputs Xl+1 and additionally projected feature Zl+1. The transi-\ntion is predicted as: Cl+1\nl\n= softmax\n\u0010\n1\n√mY ⊤\nl Zl+1\n\u0011\n; m is the\nfeature dimension.\nStatistical feature mapping: Calculate Yl’s\nmean and std, transform them by fc layers, and add to Ql+1 for\ninstance adaptation.\npartitions of equal sizes. We additionally maximize the sep-\naration between cluster centroids.\nWe first build a sparsified graph based on pairwise fea-\nture similarity for X0. Let e be the number of edges in this\ngraph, nl the number of centroids in Xl, A the n0×n0 con-\nnection matrix for edges, D the n0×1 degree vector of A,\nMl the n0×nl soft assignment matrix where each row is Pl\nfor a centroid of X0, and zzzl,k the normalized k-th feature of\nZl in Fig. 4. Our goodness of grouping loss is:\n  \\\nl\noss\nG \n&\\ !=\\!\\su m\n _{l \\\nge 1} \\un\nd\ner\nb\nrace {\\f rac {-1}{2\ne\n}\\t\nex t {trac e}\n(\nM_\nl\n^\\top (A \\!-\\!\\frac {1}\n{ 2\ne}\n \nD\nD ^\\t\nop ) M_l\n)}_{\\text \n{\nm aximize \nmodularity\n}\n} \n\\\n!+\\! \\un derbrace  {\\frac {\\sqrt {n_l}}{n_0} \\|1^\\top M_l \\|_F\\!-\\! 1}_{ \\text {collapse regularization}} \\nonumber \\\\ &\\!+\\! \\underbrace {\\frac {1}{n_l}\\sum _k -\\log \\frac {\\exp (\\pmb {z}_{l,k}^\\top \\pmb {z}_{l,k})} {\\sum _j\\exp (\\pmb {z}_{l,k}^\\top \\pmb {z}_{l,j})} }_{\\text {maximize centroid separation}}\n(8)\n3.4. Model Overview: Training and Testing\nOur model (Fig. 5) is trained with the contrastive feature\nlearning losses given edge-based grouping Ge and multi-\nFigure 5. Our model consists of two essential components: 1) multiview cosegmentation and 2) hierarchical grouping. We first produces\npixel-wise feature V , from which we cluster to get base cluster feature X0 and grouping G0. Each G0 region is split w.r.t coherent\nregions derived by OWT-UCM procedure, which is marked by the white lines. We create three groupings–Ge, G1 and G2 in multiview\ncosegmentation fashion. We obtain Ge by inferring the coherent region segmentation according to how each view is spatially transformed\nfrom the original image. Starting with input X0 of an image and its augmented views, we conduct feature clustering to merge G0 into G1,\nand then, G1 into G2. Based on Ge, G1 and G2, we formulate a pixel-to-segment contrastive loss for each grouping. Our HSG learns to\ngenerate discriminative representations and consistent hierarchical segmentations for the input images.\nlevel feature-based grouping Gl, and the goodness of group-\ning loss, weighted by λE, λF , and λG respectively:\n  \\m a thcal {L } (f\n)\n = \n\\lambd a  _E \\lossF (G_e) + \\lambda _F \\sum _{l\\ge 1} \\lossF (G_l) + \\lambda _G L_g.\n(9)\nFor testing, the same pipeline with the pixel feature CNN\nand clustering transformers predicts hierarchical segmenta-\ntions {Gl}. To benchmark segmentation performance given\na labeled set, We follow [26] and predict the labels using k-\nnearest neighbor search for each segment feature.\n4. Experiments\nWe benchmark our model on two tasks: unsupervised se-\nmantic segmentation and hierarchical image segmentation,\nthe first on five major object- and scene-centric datasets and\nthe second on Pascal VOC. We conduct ablation study to\nunderstand the contributions of our model components.\nWe adopt FCN-ResNet50 as the common backbone ar-\nchitecture. The FCN head consists of 1 × 1 convolution,\nBatchNorm, ReLU, and 1 × 1 convolution. Specifically,\nwe follow DeepLabv3 [8] to set up the dilation and strides\nin ResNet50. We set Multi Grid to (1, 2, 4) in res5. The\noutput stride is set to 16 and 8 during training and testing.\nWe do not use any pre-trained models, but train our models\nfrom scratch on each dataset. Ground-truth annotations are\nnot for training but only for testing and evaluation’s sake.\nPascal VOC 2012 [15] is a generic semantic segmentation\ndataset of 20 object category and a background class. It con-\nsists of 1, 464 and 1, 449 images for training and validation.\nWe follow [7] to augment the training data with additional\nannotations [19], resulting in 10, 582 training images. Fol-\nlowing [56], we do not train but only inference on VOC.\nMSCOCO [38] is a complex scene parsing dataset with 80\nobject categories. Objects are embedded in more complex\nscenes, with more objects per image than Pascal (7.3 vs.\n2.3). Following [56, 60], we use train2017 split (118, 287\nimages) for training and test on the VOC validation set.\nCityscapes [11] is an urban street scene parsing dataset,\nwith 19 stuff and object categories. Unlike MSCOCO and\nVOC where classes are split by scene context, Cityscapes\ncontains similar street scenes covering almost all 19 cate-\ngories. The train/test split is 2, 975/500.\nKITTI-STEP [61] is a video dataset for urban scene un-\nderstanding, instance detection and object tracking. It has\npixel-wise labels of the same 19 categories as Cityscapes.\nThere are 12 and 9 video sequences for training and valida-\ntion, or 5, 027 and 2, 981 frames.\nCOCO-stuff [4] is a scene texture segmentation dataset, a\nsubset of MSCOCO. As [29, 47], we use 15 coarse stuff\ncategories and reduce the dataset to 52K images with at least\n75% stuff pixels. The train/test split is 49, 629/2, 175.\nPotsdam [17] is a dataset for aerial scene parsing. The raw\n6000×6000 image is divided into 8550 RGBIR 200×200\npatches. There are 6 categories (roads, cars, vegetation,\ntrees, buildings, clutter). The train/test split is 7, 695/855.\nTraining set\nMSCOCO\nCityscapes\nKITTI-STEP\nValidation set\nVOC\nCityscapes\nKITTI-STEP\nMethod\nmIoU\nAcc.\nmIoU\nAcc.\nmIoU\nAcc.\nMoco [20]\n28.1\n-\n15.3\n69.5\n13.7\n60.3\nDenseCL [60]\n35.1\n-\n12.7\n64.2\n9.3\n47.6\nRevisit [56]\n35.1\n-\n17.1\n71.7\n17.0\n65.0\nSegSort [26]\n11.7\n75.1\n24.6\n81.9\n19.2\n69.8\nOur HSG\n41.9\n85.7\n32.5\n86.0\n21.7\n73.8\nTable 1.\nOur method delivers better performance on different\ntypes of datasets. The results are reported on VOC, KITTI-STEP\nand Cityscapes val set, using IoU and pixel accuracy metrics. In\nVOC, object categories are separated according to image scenes.\nIn Cityscapes and KITTI-STEP, images all come from urban street\nscene and thus contain mostly the same set of categories. Instance-\ndiscrimination methods apply image-wise contrastive loss, and\nlearn less optimally on Cityscapes and KITTI-STEP, as image\nscenes are similar. Our HSG instead learns to discriminate regions\nat different scales and performs well on both types of datasets.\nCOCO-stuff\nPotsdam\nMethod\nmIoU\nAcc.\nmIoU\nAcc.\nDeepCluster 2018 [6]\n-\n19.9\n-\n29.2\nDoersch 2015 [13]\n-\n23.1\n-\n37.2\nIsola 2016 [28]\n-\n24.3\n-\n44.9\nIIC [29]\n-\n27.7\n-\n45.4\nAC [47]\n-\n30.8\n-\n49.3\nSegSort [26]\n16.4\n49.9\n35.0\n59.0\nOur HSG\n23.8\n57.6\n43.8\n67.4\nTable 2. Our method outperforms baselines on both stuff region\nand aerial scene parsing datasets.\nThe results are reported on\nCOCO-stuff and Potsdam test set, using IoU and pixel accuracy\nmetrics. We evaluate our model using nearest neighbor search.\nOur HSG achieves superior performance.\nλE\nλG\nλF\nsingle-view\nmulti-view\n✓\n-\n-\n13.0\n40.9\n✓\n✓\n-\n13.8\n41.7\n✓\n✓\n✓\n14.0\n41.9\nTable 3. Regularizing with our goodness of grouping loss and\npixel-to-segment contrastive losses improves learned features. The\nresults are reported over VOC val set, using IoU metric. Our re-\nsulted pixel features encode better semantic information.\nMethod\nKMeans\nNCut [58]\nFINCH [50]\nOur Transfomer\nmIoU\n41.2\n41.3\n40.6\n41.9\nTable 4. Our hierarchical clustering transformer follows semantics\ncloser than other non-parametric clustering algorithms. The results\nare reported on VOC val set with IoU metric. Our learned repre-\nsentations achieve better unsupervised semantic segmentation.\nimage\nSegSort\nHSG\nground truth\nFigure 6. Our framework performs better on different types of\ndatasets. From top to bottom every three rows are visual results\nfrom VOC, Cityscapes and KITTI-STEP dataset. The results are\npredicted via segment retrievals. Our pixel-wise features encode\nmore precise semantic information than baselines.\nResults on unsupervised semantic segmentation.\nAll\nthe models are trained from scratch and evaluated by IoU\nand pixel accuracy. For VOC, we follow baselines [56] to\ntrain on MSCOCO. Table. 1 shows that our method outper-\nforms baselines by 6.8%, 7.9% and 2.5% in mIoU on VOC,\nCityscapes, and KITTI-STEP validation sets respectively.\nNote that methods relying on image-wise instance dis-\ncrimination do not work well on Cityscapes and KITTI-\nSTEP. Both datasets have urban street scenes with similar\ncategories in each image. Our method can still discover se-\nmantics by discriminating regions among these images.\nFor texture segmentation on COCO-stuff and Potsdam,\nimage\n12 regions\n6 regions\n3 regions\nFigure 7. Our clustering transformers capture semantics at dif-\nferent levels of granularity. Top: We compare to other clustering\nalgorithms on VOC val set, using Normalized Foreground Cov-\nerings as metric. We exclude background regions for evaluation.\nOur HSG overlaps with ground truths more accurately. Bottom:\nWe present visual results to compare our hierarchical segmentation\n(top row) with SE [14]-OWT-UCM procedure (bottom row). We\nalso show the detected edges at the leftmost figure in the bottom\nrow. Each image is segmented into 12, 6, 3 regions. Our method\nreveals low-to-high level of semantics more consistently.\nTab. 2 shows that our method achieves huge gains, +26.8%\nand +18.1% over IIC [29] and AC [47] respectively.\nResults on hierarchical segmentation. We benchmark hi-\nerarchical segmentation with respect to ground-truth seg-\nmentation.\nWe evaluate the overlapping of regions be-\ntween predicted segmentations and ground truth within each\nimage, known as Segmentation Covering [2].\nHowever,\nsuch a metric scores performance with the number of pix-\nels within each segment, and is thus easily biased towards\nlarge regions. For object-centric dataset VOC, a trivial all-\nforeground mask would rank high by the Covering metric.\nWe propose a Normalized Foreground Covering metric,\nby focusing on the foreground region and the overlap ratio\ninstead of the overlap pixel count. To measure the average\nforeground region overlap ratio of a ground-truth segmen-\ntation S by a predicted segmentation S′, we define:\n  \\mathrm {NF Coveri\nn\ng}(S'\n \n\\!\\ri\nght\narrow\n \\ !  S_\n{f g }) \\!=\\! \\frac {1}{|S_{fg}|} \\sum \\limits _{R \\in S_{fg}} \\max \\limits _{R' \\in S'} \\frac {|R \\cap R'|}{|R \\cup R'|}\n(10)\nwhere Sfg denotes the set of ground-truth foreground re-\ngions. Given a hierarchical segmentation, we report NF-\nCovering at each level in the hierarchy. Fig. 7 shows that\nour clustering transformers produce segmentations better\naligned with the ground-truth foreground at every level.\nVisualization.\nFig. 6 shows sample semantic segmen-\ntations on VOC (trained on MSCOCO), Cityscapes and\nKITTI-STEP. Compared to SegSort [26], our method re-\ntrieves same-category segments more accurately. For larger\nobjects or stuff categories, such as airplane or road, our\nresults are more consistent within the region. Our segmen-\ntations are also better at respecting object boundaries.\nWe also compare our hierarchical segmentations with\nSE [14]-OWT-UCM, an alternative based entirely on low-\nlevel cues. Fig. 7 bottom shows that, when partitioning an\nimage into 12, 6 and 3 regions, our segmentations follow\nthe semantic hierarchy more closely.\nAblation study. Tab. 3 shows that our model improves con-\nsistently by adding the feature learning loss based on hier-\narchical groupings and the goodness of grouping loss. It\nalso shows that multiview cosegmentation significantly im-\nproves the performance over a single image.\nTab. 4 shows that our clustering transformers provide\nbetter regularization with hierarchical groupings than alter-\nnative non-parametric clustering methods.\nSummary. We deliver the first unsupervised hierarchical\nsemantic segmentation method based on multiview coseg-\nmentation and clustering transformers. Our unsupervised\nsegmentation outperforms baselines on major object- and\nscene-centric benchmarks, and our hierarchical segmenta-\ntion discovers semantics far more accurately.\nAcknowledgements.\nThis work was supported, in part,\nby Berkeley Deep Drive, Berkeley AI Research Commons\nwith Facebook, NSF 2131111, and a Bosch research gift.\nReferences\n[1] Rıza Alp G¨uler, Natalia Neverova, and Iasonas Kokkinos.\nDensepose: Dense human pose estimation in the wild. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 7297–7306, 2018. 12, 14\n[2] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-\ntendra Malik. Contour detection and hierarchical image seg-\nmentation. IEEE transactions on pattern analysis and ma-\nchine intelligence, 33(5):898–916, 2010. 2, 3, 8, 17\n[3] Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh, and\nSuvrit Sra.\nClustering on the unit hypersphere using von\nmises-fisher distributions. Journal of Machine Learning Re-\nsearch, 6(Sep):1345–1382, 2005. 2\n[4] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-\nstuff: Thing and stuff classes in context.\nIn Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1209–1218, 2018. 6\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213–229. Springer, 2020. 5,\n14\n[6] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. Deep clustering for unsupervised learning\nof visual features. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), pages 132–149, 2018. 7\n[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokki-\nnos, Kevin Murphy, and Alan L Yuille.\nDeeplab:\nSe-\nmantic image segmentation with deep convolutional nets,\natrous convolution, and fully connected crfs. arXiv preprint\narXiv:1606.00915, 2016. 3, 6\n[8] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for seman-\ntic image segmentation. arXiv preprint arXiv:1706.05587,\n2017. 3, 6\n[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning, pages 1597–1607. PMLR, 2020. 1, 3\n[10] Dorin Comaniciu and Peter Meer. Mean shift: A robust ap-\nproach toward feature space analysis. PAMI, 2002. 2\n[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld,\nMarkus Enzweiler,\nRodrigo Benenson,\nUwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene understanding. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 3213–3223, 2016. 6\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 13, 17\n[13] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 1422–1430, 2015. 7\n[14] Piotr Doll´ar and C Lawrence Zitnick. Fast edge detection us-\ning structured forests. IEEE transactions on pattern analysis\nand machine intelligence, 37(8):1558–1570, 2014. 4, 8, 17\n[15] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. IJCV, 2010. 6\n[16] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient\ngraph-based image segmentation. IJCV, 2004. 2\n[17] Markus Gerke. Use of the stair vision library within the isprs\n2d semantic labeling benchmark. 2014. 6\n[18] Stephen Gould, Richard Fulton, and Daphne Koller. Decom-\nposing a scene into geometric and semantically consistent\nregions. In ICCV, 2009. 3\n[19] Bharath Hariharan, Pablo Arbel´aez, Lubomir Bourdev,\nSubhransu Maji, and Jitendra Malik. Semantic contours from\ninverse detectors. In 2011 International Conference on Com-\nputer Vision, pages 991–998. IEEE, 2011. 6\n[20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9729–9738, 2020. 1, 3, 7\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 17\n[22] Xuming He, Richard S Zemel, and MA Carreira-Perpindn.\nMultiscale conditional random fields for image labeling. In\nCVPR, 2004. 3\n[23] Jyh-Jing Hwang, Tsung-Wei Ke, Jianbo Shi, and Stella X\nYu. Adversarial structure matching for structured prediction\ntasks. In CVPR, 2019.\n[24] Jyh-Jing Hwang, Tsung-Wei Ke, and Stella X Yu. Contex-\ntual image parsing via panoptic segment sorting. In Multi-\nmedia Understanding with Less Labeling on Multimedia Un-\nderstanding with Less Labeling, pages 27–36. 2021. 3\n[25] Jyh-Jing Hwang and Tyng-Luh Liu. Pixel-wise deep learn-\ning for contour detection. arXiv preprint arXiv:1504.01989,\n2015. 2\n[26] Jyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D\nCollins, Tien-Ju Yang, Xiao Zhang, and Liang-Chieh Chen.\nSegsort:\nSegmentation by discriminative sorting of seg-\nments. In ICCV, 2019. 1, 2, 3, 4, 6, 7, 8, 13, 14, 16, 17\n[27] Phillip Isola, Daniel Zoran, Dilip Krishnan, and Edward H\nAdelson. Crisp boundary detection using pointwise mutual\ninformation. In European Conference on Computer Vision,\npages 799–814. Springer, 2014. 17\n[28] Phillip Isola, Daniel Zoran, Dilip Krishnan, and Edward H\nAdelson.\nLearning visual groups from co-occurrences in\nspace and time. arXiv preprint arXiv:1511.06811, 2015. 7\n[29] Xu Ji, Jo˜ao F Henriques, and Andrea Vedaldi.\nInvariant\ninformation clustering for unsupervised image classification\nand segmentation. In Proceedings of the IEEE International\nConference on Computer Vision, pages 9865–9874, 2019. 2,\n3, 6, 7, 8\n[30] Armand Joulin, Francis Bach, and Jean Ponce. Discrimina-\ntive clustering for image co-segmentation. In CVPR, pages\n1943–1950. IEEE, 2010. 3\n[31] Tsung-Wei Ke, Jyh-Jing Hwang, Ziwei Liu, and Stella X.\nYu. Adaptive affinity fields for semantic segmentation. In\nECCV, 2018. 3\n[32] Tsung-Wei Ke, Jyh-Jing Hwang, and Stella X Yu. Universal\nweakly supervised segmentation by pixel-to-segment con-\ntrastive learning. In International Conference on Learning\nRepresentations, 2021. 1, 3, 4\n[33] Pushmeet Kohli, Philip HS Torr, et al. Robust higher order\npotentials for enforcing label consistency. IJCV, 82(3):302–\n324, 2009. 3\n[34] Alexander Kolesnikov and Christoph H Lampert. Seed, ex-\npand and constrain: Three principles for weakly-supervised\nimage segmentation. In European Conference on Computer\nVision, pages 695–711. Springer, 2016. 1\n[35] Lubor Ladicky, Christopher Russell, Pushmeet Kohli, and\nPhilip HS Torr. Associative hierarchical crfs for object class\nimage segmentation. In ICCV, 2009. 3\n[36] Yann LeCun, Bernhard Boser, John S Denker, Donnie\nHenderson, Richard E Howard, Wayne Hubbard, and\nLawrence D Jackel. Backpropagation applied to handwrit-\nten zip code recognition. Neural Computation, 1989. 3\n[37] Yong Jae Lee and Kristen Grauman. Collect-cut: Segmenta-\ntion with top-down cues discovered in multi-object images.\nIn 2010 IEEE Computer Society Conference on Computer\nVision and Pattern Recognition, pages 3185–3192. IEEE,\n2010. 3\n[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision, pages 740–755.\nSpringer, 2014. 6\n[39] Ce Liu, Jenny Yuen, and Antonio Torralba. Nonparametric\nscene parsing via label transfer. PAMI, 2011. 3\n[40] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation.\nIn\nCVPR, 2015. 3\n[41] Laurens van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-sne.\nJournal of Machine Learning Research,\n2008. 13, 16\n[42] Jitendra Malik, Serge Belongie, Thomas Leung, and Jianbo\nShi. Contour and texture analysis for image segmentation.\nIJCV, 2001. 2\n[43] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database\nof human segmented natural images and its application to\nevaluating segmentation algorithms and measuring ecologi-\ncal statistics. In Proc. 8th Int’l Conf. Computer Vision, vol-\nume 2, pages 416–423, July 2001. 17\n[44] Marina Meila and Jianbo Shi. Learning segmentation by ran-\ndom walks. In Advances in Neural Information Processing\nSystems 13, 2000. 3\n[45] Mohammadreza Mostajabi, Payman Yadollahpour, and Gre-\ngory Shakhnarovich.\nFeedforward semantic segmentation\nwith zoom-out features. In CVPR, 2015. 3\n[46] Mark EJ Newman.\nFinding community structure in net-\nworks using the eigenvectors of matrices. Physical review\nE, 74(3):036104, 2006. 5\n[47] Yassine Ouali, Celine Hudelot, and Myriam Tami. Autore-\ngressive unsupervised image segmentation. In Proceedings\nof the European Conference on Computer Vision (ECCV),\nAugust 2020. 3, 6, 7, 8\n[48] Carsten Rother, Tom Minka, Andrew Blake, and Vladimir\nKolmogorov. Cosegmentation of image pairs by histogram\nmatching-incorporating a global constraint into mrfs.\nIn\n2006 IEEE Computer Society Conference on Computer Vi-\nsion and Pattern Recognition (CVPR’06), volume 1, pages\n993–1000. IEEE, 2006. 3\n[49] Bryan Russell, Alyosha Efros, Josef Sivic, Bill Freeman, and\nAndrew Zisserman. Segmenting scenes by matching image\ncomposites. In NIPS, 2009. 3\n[50] Saquib Sarfraz, Vivek Sharma, and Rainer Stiefelhagen. Effi-\ncient parameter-free clustering using first neighbor relations.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 8934–8943, 2019. 5, 7\n[51] Ramprasaath R Selvaraju, Karan Desai, Justin Johnson, and\nNikhil Naik. Casting your model: Learning to localize im-\nproves self-supervised representations.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11058–11067, 2021. 3\n[52] Jianbo Shi and Jitendra Malik. Normalized cuts and image\nsegmentation. IEEE Transactions on pattern analysis and\nmachine intelligence, 22(8):888–905, 2000. 2\n[53] Jamie Shotton, John Winn, Carsten Rother, and Antonio Cri-\nminisi. Textonboost for image understanding: Multi-class\nobject recognition and segmentation by jointly modeling tex-\nture, layout, and context. IJCV, 2009. 3\n[54] Joseph Tighe and Svetlana Lazebnik. Superparsing: scalable\nnonparametric image parsing with superpixels.\nIn ECCV,\n2010. 3\n[55] Anton Tsitsulin, John Palowitch, Bryan Perozzi, and Em-\nmanuel M¨uller.\nGraph clustering with graph neural net-\nworks. arXiv preprint arXiv:2006.16904, 2020. 5\n[56] Wouter Van Gansbeke, Simon Vandenhende, Stamatios\nGeorgoulis, and Luc Van Gool.\nRevisiting contrastive\nmethods for unsupervised learning of visual representations.\narXiv preprint arXiv:2106.05967, 2021. 1, 2, 3, 6, 7\n[57] Wouter Van Gansbeke, Simon Vandenhende, Stamatios\nGeorgoulis, and Luc Van Gool.\nUnsupervised semantic\nsegmentation by contrasting object mask proposals. arXiv\npreprint arXiv:2102.06191, 2021. 13, 14\n[58] Ulrike Von Luxburg. A tutorial on spectral clustering. Statis-\ntics and computing, 17(4):395–416, 2007. 5, 7\n[59] Xudong Wang, Ziwei Liu, and Stella X Yu. Unsupervised\nfeature learning by cross-level instance-group discrimina-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 12586–12595,\n2021. 1, 3, 4\n[60] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong,\nand Lei Li. Dense contrastive learning for self-supervised\nvisual pre-training. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3024–3033, 2021. 2, 3, 6, 7\n[61] Mark Weber, Jun Xie, Maxwell Collins, Yukun Zhu,\nPaul Voigtlaender, Hartwig Adam, Bradley Green, An-\ndreas Geiger, Bastian Leibe, Daniel Cremers, et al. Step:\nSegmenting and tracking every pixel.\narXiv preprint\narXiv:2102.11859, 2021. 6\n[62] Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Un-\nsupervised feature learning via non-parametric instance-level\ndiscrimination. arXiv preprint arXiv:1805.01978, 2018. 1,\n3, 4\n[63] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection. In ICCV, 2015. 2\n[64] Jian Yao, Sanja Fidler, and Raquel Urtasun. Describing the\nscene as a whole: Joint object detection, scene classification\nand semantic segmentation. In CVPR, 2012. 3\n[65] Stella X. Yu. Segmentation induced by scale invariance. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2005. 2\n[66] Stella X. Yu and Jianbo Shi. Multiclass spectral clustering.\nIn ICCV, 2003. 2\n[67] Stella X Yu and Jianbo Shi.\nSegmentation given partial\ngrouping constraints. PAMI, 2004. 2\n[68] Xiao Zhang and Michael Maire. Self-supervised visual rep-\nresentation learning from hierarchical grouping. Advances in\nNeural Information Processing Systems, 33, 2020. 3, 17\n[69] Zhenli Zhang, Xiangyu Zhang, Chao Peng, Dazhi Cheng,\nand Jian Sun. Exfuse: Enhancing feature fusion for semantic\nsegmentation. In ECCV, 2018. 3\n[70] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nCVPR, 2017. 13, 17\nUnsupervised Hierarchical Semantic Segmentation with\nMultiview Cosegmentation and Clustering Transformers\nTsung-Wei Ke\nJyh-Jing Hwang\nYunhui Guo\nXudong Wang\nStella X. Yu\nUC Berkeley / ICSI\n5. Supplementary\nWe propose the first unsupervised hierarchical seman-\ntic segmentation method. Our core idea has two folds: 1)\nimposing grouping hierarchy in the feature space with clus-\ntering transformers and 2) enforcing spatial grouping con-\nsistency with multiview cosegmentation. We demonstrate\nstate-of-the-art performance on unsupervised semantic seg-\nmentation and hierarchical segmentation. In this supple-\nmentary, we include more details on the following aspects:\n• We present the visual results of the attention maps\nfrom our clustering transformers in 5.1.\n• We present the qualitative results of hierarchical se-\nmantic segmentation on DensePose dataset in 5.2.\n• We present the visual results of contextual information\nencoded in our learned feature mappings in 5.3.\n• We showcase unsupervised semantic segmentation on\nCOCO-stuff and Potsdam datasets in 5.4.\n• We\npresent\nper-category\nresults\non\nVOC\nwith\nImageNet-trained models in 5.5.\n• We present inference latency of our clustering trans-\nformers in 5.6\n• We describe more details of our clustering transform-\ners in 5.7.\n• We describe the details of our experimental settings\nand hyper-parameters in 5.8\n• We detail the experimental settings on VOC using\nImageNet-trained models in 5.9.\n5.1. Visual Results on Attention Maps from Decoder\nWe visualize the multi-head attention maps in the de-\ncoder of our clustering transformer. Such attention maps\ncorrespond to the correlation among cluster centroids and\ninput segments. As shown in Fig. 8, we observe that each\ncluster attends to their cluster members, e.g. face and hair\nof the head region. Interestingly, we also see these clusters\ncorrelate better with image segments that carry more similar\nsemantic meanings. For example, the ‘head’ cluster attends\nmore to body parts than background regions. Such corre-\nlation information implies the next-level groupings: ‘head’\nwill be grouped with ‘torso’ instead of ‘background’.\n5.2. Hierarchical Semantic Segmentation\nWe show that hierarchical segmentation is needed to pick\nout semantics at different levels of granularity. On Dense-\nPose [1] dataset, we process ground-truth labels at two lev-\nels of semantics: person and body parts. Body-part-level\nlabels include head, torso, upper and lower limb regions.\nAs shown in the top of Fig. 9, coarse (person-level) and\nfine (body-part-level) labels are best picked out with higher\nand lower levels of segmentations. The evaluation metric is\nbased on F-score of region matching.\nWe next demonstrate the efficacy of our hierarchical\nclustering transformer. As shown in the bottom of Fig. 9,\nour predicted segmentations outperform others by large\nmargin at every level in the hierarchy on both the fine and\ncoarse sets of semantic labels.\nHierarchical consistency\nprovides regularizations which help us to obtain better seg-\nmentation results at any granularity.\n5.3. Visual Results on Contextual Retrievals\nWe reveal the encoding of visual context in our learned\nfeature representations. We first conduct hierarchical seg-\nmentation using our clustering transformers to partition an\nimage into fine and coarse regions. We then compute the\nunit-length average feature within each region and perform\nnearest neighbor search among the training dataset. Fig. 10\nshows nearest neighbor retrievals at coarse (cyan) and fine\n(red) segmentations. The query and retrieved segments are\ngenerated at same level of partitioning. Strikingly, the fea-\nture representations at each level of grouping correlate with\nmultiple levels of semantic meanings such as baseball play-\ners and their body parts.\nWe next demonstrate the contextual information of co-\noccurring objects encoded in our feature representations.\nWe visualize the length-normalized average features of the\n12\nFigure 8. The multi-head attention maps reveal the fine-to-coarse semantic relationships among image segments. From left to right: input\nimage, our feature-induced segmentation, attention maps in the decoder of our clustering transformers. We use a clustering transformer to\npartition each image into 8 clusters, and show the attention map (colored in viridis color maps) of each cluster to all the image segments.\nWe observe these clusters correlate better with image segments that carry more similar semantic meanings, e.g., the ‘head’ cluster attends\nmore to body parts than background regions. Such correlation information implies the next-level groupings: ‘head’ will be grouped with\n‘torso’ instead of ‘background’\nMethod\naero\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nmIoU\nMaskContrast [57]\n76.2\n26.9\n70.2\n49.6\n56.1\n80.3\n66.8\n66.8\n10.6\n55.1\n17.5\n65.2\n51.8\n59.7\n58.8\n23.1\n73.5\n24.9\n70.9\n38.9\n53.9\nSegSort [26]\n71.3\n26.4\n70.7\n56.1\n51.9\n78.2\n68.5\n72.1\n12.7\n47.2\n36.4\n65.3\n45.5\n61.6\n63.7\n29.3\n60.0\n30.1\n70.3\n59.4\n55.5\nOur HSG\n75.1\n32.2\n76.9\n60.4\n63.9\n81.7\n75.5\n82.0\n18.5\n48.7\n51.2\n71.5\n55.0\n69.4\n71.0\n39.8\n66.8\n33.3\n72.3\n59.6\n61.7\nvs. baseline\n-1.1\n+5.3\n+6.2\n+4.3\n+7.8\n+1.4\n+7.0\n+ 9.9\n+5.8\n+1.5\n+14.8\n+6.2\n+3.2\n+7.8\n+7.3\n+10.6\n-6.7\n+3.2\n+1.4\n+0.2\n+6.2\nTable 5. Our method outperforms SegSort [26] and MaskContrast [57] among most categories on VOC val dataset. We adopt PSPNet [70]-\nResNet50 as our backbone CNN. All models are supervisedly pre-trained on ImageNet [12]. Per-category performance is evaluated\nusing IoU metric. We use the officially trained model and released code for inference MaskContrast. Our HSG demonstrates superior\nperformance for semantic segmentation.\nNo Hierarchy\nHSG\nKMeans\nNCut\nFINCH\nms\n120\n158\n165\n170\n381\nTable 6. Our method imposes less runtime overhead than other\nhierarchical clustering methods. All methods are conducted on a\n640×640 image, which is hierarchically partitioned into 25, 16, 9\nand 4 segments. While major latency comes from the pixel em-\nbedding network, HSG is still 17% faster than KMeans.\n‘person’ category region on Pascal VOC 2012 dataset us-\ning tSNE [41]. We represent each ‘person’ feature with\nthe co-occurring object categories, and observe that features\nin the similar semantic context are clustered. As shown in\nDataset\nB.S\nC.S\nL.R\nW.D\nEpochs\nλE\nλG\nλF\nMSCOCO\n128\n224\n0.1\n0.0001\n380\n1.0\n0.0\n0.0\n48\n448\n0.008\n0.0001\n8\n1.0\n1.0\n0.1\nCityscapes\n32\n448\n0.1\n0.0001\n400\n1.0\n0.2\n0.1\nKITTI-STEP\n48\n448\n0.1\n0.0001\n400\n1.0\n0.2\n0.1\nCOCO-stuff\n8\n336\n0.003\n0.0005\n5\n1.0\n0.2\n0.1\nPotsdam\n8\n200\n0.003\n0.0005\n30\n1.0\n0.2\n0.1\nTable 7. Hyper-parameters for training on different datasets. Gray\ncolored background indicates pre-training settings. B.S, C.S, L.R,\nW.D denote batch size, crop size, learning rate and weight decay.\nFig. 11, we observe clusters of similar co-occurring object\ncategories, such as a person riding a horse (in cerise) or a\n(a)\n(b)\n(c)\nFigure 9.\nOur hierarchical segmentations outperform others,\nwhich better pick out semantics at different levels of granularity.\nOn DensePose [1], ground-truth labels are processed at two levels\nof semantics: person and body parts. a) F-score of region match-\ning among ground truths and our hierarchical segmentations, b)\nF-score on fine (body-part-level) labels and c) F-score on coarse\n(person-level) labels. Hierarchical segmentation is needed to cap-\nture semantics across different granularity and our HSG outper-\nforms others by large margin at every level.\nbike (in green), etc.\n5.4. Visual Results on Semantic Segmentation\nWe show some visual results of semantic segmentation\non COCO-stuff and Potsdam in Fig. 12. Compared to Seg-\nSort [26], our results are more accurate and consistent. Our\npredicted segmentations also preserve boundaries more pre-\ncisely than the baseline.\n5.5. Unsupervised Semantic Segmentation\nIn the main paper, we perform unsupervised seman-\ntic segmentation by training from scratch on each dataset.\nHere, we carry out experiments by following the settings\nused in SegSort [26] and MaskContrast [57]. We summa-\nrize the quantitative results of unsupervised semantic seg-\nmentation using ImageNet-trained models on VOC in Ta-\nble. 5 according to IoU metric. We demonstrate the effi-\ncacy of our method, which outperforms SegSort [26] and\nMaskContrast [57] consistently among most categories by\nlarge margin. Most strikingly, our method is able to capture\ncategories with complex structures better than baselines,\ne.g., chair (+5.8%), table (+14.8%) and plant (+10.6%).\n5.6. Inference Latency on Clustering Transformer\nWe present the inference latency of different hierarchical\nclustering methods. We test on a 640 × 640 image, which\nis hierarchically partitioned into 25, 16, 9 and 4 segments.\nWe iterate KMeans and NCut for 30 times. As shown in\nTable 6, our HSG imposes less runtime overhead than other\nclustering methods. While major latency comes from the\nbackbone CNN, HSG is still 17% faster than KMeans.\n5.7. Hierarchical Clustering Transformer\nWe mostly follow [5] to implement the transformer. The\ndetailed architecture of the clustering transformer is pre-\nsented in Fig. 13. The (l + 1)th-level transformer takes Xl\nas inputs and forwards to the encoder. The encoder con-\ntextually updates Xl to Yl based on the pairwise correlation\ninformation of Xl. Meanwhile, the decoder takes a set of\nquery embeddings Ql+1 as inputs and outputs the next-level\ncluster centroids. Ql+1 can be considered as the initial rep-\nresentations of next-level clusters. As the clusterings should\nadapt with input statistics, we calculate the ‘mean’ and ‘std’\nof Yl, followed by fc layers, and sum them with Ql+1 before\ninputting to the decoder. The decoder contextually updates\nQl+1 to the next-level cluster centroids Xl+1, which be-\ncome the inputs to the next-level transformer. To calculate\nthe clustering assignment, we do not use Xl+1 but Zl+1,\nwhich shares the decoder layers with Xl+1 but transformed\nby a separate fc layer. The soft clustering assignments are\ncalculated as: Cl+1\nl\n= softmax(\n1\n√mY ⊤\nl Zl+1); m is the fea-\nture dimension.\nIn particular, we replace LayerNorm with BatchNorm.\nWe set number of heads to 4 in the attention module, and\nuse 2 encoder (decoder) layers in each encoder (decoder)\nmodule. We set drop out rate to 0.1 during training. For\nquery embeddings Ql at level l, we randomly initiate and\nupdate them thru SGD.\nIn the clustering loss, the affinity matrix A among base\nlevel feature X0 is required to compute the modularity max-\nimization loss.\nWe construct A as a k-nearest neighbor\n(sparse) graph using the similarity of X0, where the entry\nvalue is set to 1. A is a binarized affinity matrix of a sparsi-\nfied graph. For MSCOCO/VOC/COCO-stuff/Potsdam, we\nset k to 2 within an image and its augmented views, respec-\ntively. In such a manner, we encourage segment groupings\nacross views. On Cityscapes/KITTI-STEP, cropped patches\nfrom each image instance are less likely to overlap. Without\nenforcing groupings across views, we search top 4 nearest\nneighbors among views (k = 4).\n5.8. Hyper-Parameters and Experimental Setup\nWe next describe the same set of hyper-parameters\nshared across different datasets, and summarize the differ-\nent settings in Table. 7.\nFor all the experiments, we set the dimension of output\nembeddings to 128, temperature T to\n1\n16. We apply step-\nwise decay learning rate policy, with which learning rate is\ndecayed by 32%, 56% and 75% of total training epochs.\nWe obtain base-level grouping G0 by iterating spherical\nKMeans algorithm over pixel-wise feature V for 15 steps\nFigure 10. Sample retrieval results in MSCOCO for two images, baseball (Rows 1-3) and wii sport (Rows 4-6), based on our CNN features.\nColumn 1 shows a query segment and Columns 2-5 are its nearest neighbour retrievals at the same level of the hierarchy. Segments at a\ncoarser / finer level are shown in cyan (Rows 1,4) / red (Rows 2-3, 5-6). Coarser segment retrievals show that our feature learned from\nhierarchical groupings are reflective of the visual scene layout (For example, Row 1 all has the 3-person baseball pitching configuration\ndespite drastic appearance variations), whereas finer segment retrievals show that our learned feature is precise at characterizing both the\nsegment itself and the visual context around it (For example, the feature of the query segment (legs) in Row 3 is indicative of the pitcher\npose on the baseball field). Such a holistic yet discriminative feature representation is discovered in a pure data-driven fashion without any\nsemantic supervision.\nand partition each cropped input to 4 × 4 segments. During\ntraining not testing, G0 is then refined by coherent regions\nFigure 11. Our visual representations encode contextual information of co-occurring objects. We visualize the average feature of person\ncategory region on Pascal VOC 2012 dataset using tSNE [41]. We use the feature mappings extracted with models trained from scratch\non MSCOCO. We represent each person category region with the co-occurring object categories, and observe that features in the similar\nsemantic context are clustered.\ngenerated from the OWT-UCM procedure. For G1 and G2,\nwe set n1 and n2 to 8 and 4. The whole framework is opti-\nmized using SGD. Notably, we only adopt rescaling, crop-\nping, horizontal flipping, color jittering, gray-scale conver-\nsion, and Gaussian blurring for data augmentation. All the\nother different settings are presented in Table. 7. For fair\ncomparison with corresponding baselines, we apply differ-\nent settings for training on MSCOCO and COCO-stuff.\nParticularly, for MSCOCO, we adopt a two-stage learn-\ning strategy. We first train the model with smaller crop size\n(224 × 224) and larger batch size (128), then fine-tune with\nlarger crop size (448×448) and smaller batch size (48). The\nmodels are trained and fine-tuned for 380 and 8 epochs. We\ndo not use spherical KMeans to generate image overseg-\nmentation in the first stage of training.\nFor inference, we only use single-scale image. For unsu-\npervised semantic segmentation, we follow [26] to conduct\nnearest neighbor search to predict the semantic segmenta-\ntion. We apply spherical KMeans algorithm over V to de-\nrive pixel grouping G0 and base cluster feature X0. We\nsearch nearest neighbors using X0 from the whole train-\ning dataset. We set n0–the number of centroids in G0, to\n6 × 6, 12 × 24 and 6 × 12 on Pascal/COCO-stuff/Potsdam,\nCityscapes and KITTI-STEP dataset. On Cityscapes and\nimage\nSegSort\nHSG\nground truth\nFigure 12. Our framework delivers better semantic segmentation\non textural and aerial region parsing datasets. From top to bottom\nevery three rows are visual results from COCO-stuff and Potsdam.\nThe results are predicted via segment retrievals. Our results are\nmore consistent and accurate within each category. Additionally,\nour segmentation predictions are better aligned with the bound-\naries. Our pixel-wise features encode more precise semantic in-\nformation than baselines.\nKITTI-STEP, we train the baselines with officially released\ncode and test with our inference procedure. Otherwise, we\nreport the numbers according to their papers.\nWe follow [26] and adopt the UCM-OWT procedure [2]\nto generate coherent region segmentations from contours.\nFor MSCOCO and COCO-stuff, we follow [68] to detect\nedges by SE [14]. The detector is first pre-trained on BSDS\nFigure 13.\nOur clustering transformer enforces grouping con-\nsistency across levels by mapping feature Xl to Xl+1 with fea-\nture transition Cl+1\nl\n. Xl+1 and Cl+1\nl\nare learned simultaneously.\nShown here for level l=0 in Fig. 2, the transformer encoder takes\ninputs Xl and outputs contextualized feature Yl. The transformer\ndecoder takes learnable inputs from query embeddings Ql+1, and\noutputs Xl+1 and additionally projected feature Zl+1. The transi-\ntion is predicted as: Cl+1\nl\n= softmax\n\u0010\n1\n√mY ⊤\nl Zl+1\n\u0011\n; m is the\nfeature dimension.\nStatistical feature mapping: Calculate Yl’s\nmean and std, transform them by fc layers, and add to Ql+1 for\ninstance adaptation.\ndataset [43] with ground-truth edge labels. We start with\nthreshold as 0.25 to binarize the UCM, followed by OWT-\nUCM to generate the segmentations. We gradually increase\nthe threshold until the number of regions is smaller than 48.\nFor Cityscapes/KITTI-STEP and Potsdam, we use PMI [27]\nto predict edges. The detector only considers co-occurring\nstatistics among paired colors, and does not require any\nground-truth label. The initial threshold is 0.05 and 0.5,\nwhich is increased until the number of regions is smaller\nthan 1024 and 128.\n5.9. Unsupervised Semantic Segmentation with\nImageNet-trained Models\nWe use PSPNet [70] based on ResNet50 [21] as back-\nbone CNN. The ResNet model is supervisedly pre-trained\non ImageNet [12] and fine-tuned on Pascal VOC 2012. We\nfollow mostly the same hyper-parameters, except that, we\nset batch size to 16, crop size to 448, base learning rate to\n0.002 and weight decay to 0.0005. The models are fine-\ntuned for 15 epochs. We set λE, λG and λF to 1.0, 1.0 and\n0.1, respectively.\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2022-04-25",
  "updated": "2022-04-25"
}