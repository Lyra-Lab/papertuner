{
  "id": "http://arxiv.org/abs/2204.11827v1",
  "title": "Task-Induced Representation Learning",
  "authors": [
    "Jun Yamada",
    "Karl Pertsch",
    "Anisha Gunjal",
    "Joseph J. Lim"
  ],
  "abstract": "In this work, we evaluate the effectiveness of representation learning\napproaches for decision making in visually complex environments. Representation\nlearning is essential for effective reinforcement learning (RL) from\nhigh-dimensional inputs. Unsupervised representation learning approaches based\non reconstruction, prediction or contrastive learning have shown substantial\nlearning efficiency gains. Yet, they have mostly been evaluated in clean\nlaboratory or simulated settings. In contrast, real environments are visually\ncomplex and contain substantial amounts of clutter and distractors.\nUnsupervised representations will learn to model such distractors, potentially\nimpairing the agent's learning efficiency. In contrast, an alternative class of\napproaches, which we call task-induced representation learning, leverages task\ninformation such as rewards or demonstrations from prior tasks to focus on\ntask-relevant parts of the scene and ignore distractors. We investigate the\neffectiveness of unsupervised and task-induced representation learning\napproaches on four visually complex environments, from Distracting DMControl to\nthe CARLA driving simulator. For both, RL and imitation learning, we find that\nrepresentation learning generally improves sample efficiency on unseen tasks\neven in visually complex scenes and that task-induced representations can\ndouble learning efficiency compared to unsupervised alternatives. Code is\navailable at https://clvrai.com/tarp.",
  "text": "Published as a conference paper at ICLR 2022\nTASK-INDUCED REPRESENTATION LEARNING\nJun Yamada1∗, Karl Pertsch2, Anisha Gunjal2, Joseph J. Lim3∗4†\n1 University of Oxford, 2 University of Southern California,\n3 Korea Advanced Institute of Science and Technology, 4 Naver AI Lab\nABSTRACT\nIn this work, we evaluate the effectiveness of representation learning approaches\nfor decision making in visually complex environments. Representation learning\nis essential for effective reinforcement learning (RL) from high-dimensional in-\nputs. Unsupervised representation learning approaches based on reconstruction,\nprediction or contrastive learning have shown substantial learning efﬁciency gains.\nYet, they have mostly been evaluated in clean laboratory or simulated settings. In\ncontrast, real environments are visually complex and contain substantial amounts\nof clutter and distractors. Unsupervised representations will learn to model such\ndistractors, potentially impairing the agent’s learning efﬁciency. In contrast, an\nalternative class of approaches, which we call task-induced representation learning,\nleverages task information such as rewards or demonstrations from prior tasks\nto focus on task-relevant parts of the scene and ignore distractors. We investi-\ngate the effectiveness of unsupervised and task-induced representation learning\napproaches on four visually complex environments, from Distracting DMControl\nto the CARLA driving simulator. For both, RL and imitation learning, we ﬁnd\nthat representation learning generally improves sample efﬁciency on unseen tasks\neven in visually complex scenes and that task-induced representations can double\nlearning efﬁciency compared to unsupervised alternatives. 1\n1\nINTRODUCTION\nThe ability to compress sensory inputs into a compact representation is crucial for effective decision\nmaking. Deep reinforcement learning (RL) has enabled the learning of such representations via\nend-to-end policy training (Mnih et al., 2015; Levine et al., 2016). However, learning representations\nfrom reward feedback requires many environment interactions. Instead, unsupervised objectives for\nrepresentation learning (Lange and Riedmiller, 2010; Finn et al., 2016; Hafner et al., 2019; Oord\net al., 2018; Chen et al., 2020) directly maximize lower bounds on the mutual information between\nthe sensory inputs and the learned representation. Empirically, such unsupervised objectives can\naccelerate the training of deep RL agents substantially (Laskin et al., 2020; Stooke et al., 2021; Yang\nand Nachum, 2021). However, prior works have evaluated such representation learning approaches in\nclean laboratory or simulated settings (Finn et al., 2016; Lee et al., 2020; Laskin et al., 2020) where\nmost of the perceived information is important for the task at hand. In contrast, realistic environments\nfeature lots of task-irrelevant detail, such as clutter or objects moving in the background. This has the\npotential to impair the performance of unsupervised representations for RL since they aim to model\nall information in the input equally.\nThe goal of this paper is to evaluate the effectiveness of representation learning approaches for\ndecision making in such more realistic, visually complex environments with distractors and clutter.\nWe evaluate two classes of representation learning approaches: (1) unsupervised representation\nlearning based on reconstruction, prediction or contrastive objectives, and (2) approaches that\nleverage task information from prior tasks to shape the representation, which we will refer to as\ntask-induced representation learning approaches. Such task information can often be available at\nno extra cost, e.g., in the form of prior task rewards if the data has been collected from RL training\nruns. Task-induced representations can leverage this task information to determine which features\nare important in visually complex scenes, and which are distractors that can be ignored. We focus\n∗Work done while at USC\n† AI Advisor at NAVER AI Lab\n1Project website: clvrai.com/tarp\n1\narXiv:2204.11827v1  [cs.LG]  25 Apr 2022\nPublished as a conference paper at ICLR 2022\nTask-Induced  \nRepresentation Learning\nPrior Task Experience w/ Task Information (rewards, demos…)\nDownstream \nPolicyπ\n{s, a, r1}\n{s, ademo}\n{s, a, r2}\n{s, ademo}\nTask 1\nTask 2\nTask 3\nTask 4\nUnsupervised  \nRepresentation Learning\nVisually Complex Scene\nPre-trained \nRepresentation\nTarget Task\nT\nFigure 1: Overview of our pipeline for evaluating representation learning in visually complex\nscenes: given a multi-task dataset of prior experience we pre-train representations using unsupervised\nobjectives, such as prediction and contrastive learning, or task-induced approaches, which leverage\ntask-information from prior tasks to learn to focus on task-relevant aspects of the scene. We then\nevaluate the efﬁciency of the pre-trained representations for learning unseen tasks.\nour evaluation on a practical representation learning setting, shown in Figure 1: given a large ofﬂine\ndataset of experience collected across multiple prior tasks, we perform ofﬂine representation learning\nwith different approaches. Then, we transfer the pre-trained representations for training a policy on a\nnew task and compare efﬁciency to training the policy from scratch.\nTo evaluate the effectiveness of different representation learning objectives, we compare common\nunsupervised approaches based on reconstruction, prediction and contrastive learning to four task-\ninduced representation learning methods. We perform experiments in four visually complex envi-\nronments, ranging from scenes with distracting background videos in the Distracting DMControl\nbench (Stone et al., 2021) to realistic distractors such as clouds, weather and urban scenery in the\nCARLA driving simulator (Dosovitskiy et al., 2017). Across RL and imitation learning experiments\nwe ﬁnd that pre-trained representations accelerate downstream task learning, even in visually complex\nenvironments. Additionally, in our comparisons task-induced representation learning approaches\nachieve up to double the learning efﬁciency of unsupervised approaches by learning to ignore\ndistractors and task-irrelevant visual details.\nIn summary, the contributions of our work are threefold: (1) we formalize the class of task-induced\nrepresentation learning approaches which leverage task-information from prior tasks for shaping\nthe learned representations, (2) we empirically compare widely used unsupervised representation\nlearning approaches and task-induced representation learning methods across four visually complex\nenvironments with numerous distractors, showing that task-induced representations can lead to\nmore effective learning and (3) through analysis experiments we develop a set of best practices for\ntask-induced representation learning.\n2\nRELATED WORK\nDeep RL can train “end-to-end” policies that directly map raw visual inputs to action com-\nmands (Mnih et al., 2015; Kalashnikov et al., 2018). To improve data efﬁciency in deep RL from high-\ndimensional observations, a number of recent works have explored using data augmentation (Yarats\net al., 2021; Laskin et al., 2021) and unsupervised representation learning techniques during RL\ntraining. The latter can be categorized into (1) reconstruction (Lange and Riedmiller, 2010; Finn\net al., 2016), (2) prediction (Hafner et al., 2019; Lee et al., 2020), and (3) contrastive learning (Laskin\net al., 2020; Stooke et al., 2021; Zhan et al., 2020; Yang and Nachum, 2021) approaches. While these\nworks have shown improved sample efﬁciency, fundamentally they are unsupervised and therefore\ncannot decide which information in the input is relevant or irrelevant to a certain task. We show that\nthis leads to deteriorating performance in visually complex environments such as autonomous driving\nscenarios with lots of non-relevant information in the input observations.\nAnother line of work has used task information to shape representations that learn to ignore distractors.\nSpeciﬁcally, Fu et al. (2021) predict future rewards to learn representations that focus only on task-\n2\nPublished as a conference paper at ICLR 2022\nrelevant aspects of a scene. Zhang et al. (2021) leverage rewards as part of a bisimulation objective\nto learn representations without reconstruction. In this work, we provide a unifying framework for\nsuch task-induced representation learning approaches that encompasses the objectives of Fu et al.\n(2021) and Zhang et al. (2021) among others, and perform a systematic comparison to unsupervised\nrepresentations in visually complex environments.\nClosest to our work is the work of Yang and Nachum (2021), which evaluates unsupervised and task-\ninduced representation learning objectives for pre-training in multiple environments from the D4RL\nbenchmark (Fu et al., 2020). Their work shows that representation pre-training can substantially\nimprove downstream learning efﬁciency. While Yang and Nachum (2021)’s evaluation is performed\nin visually clean OpenAI gym environments (Brockman et al., 2016) without distractors, we focus\nour evaluation on more realistic, visually complex environments with distractors, which can pose\nadditional challenges, especially for unsupervised representation learning approaches.\n3\nPROBLEM FORMULATION\nOur goal is to efﬁciently learn a policy π that solves a target task Ttarget in an MDP deﬁned by a tuple\n(S, A, T , R, ρ, γ) of states, actions, transition probabilities, rewards, initial state distribution, and\ndiscount factor. The policy is trained to maximize the discounted return Eat∼π\n\u0002 PT −1\nt=0 γtR(st, at)\n\u0003\n.\nWe do not assume access to the underlying state s of the MDP, but instead our policy receives a\nhigh-dimensional observation x ∈X in every step, e.g., an image observation. To improve training\nefﬁciency, we aim to learn an encoder φ(x) which maps the input observation to a low-dimensional\nstate representation that is input to the policy π(φ(x)). To learn this representation, we assume\naccess to a dataset of past interactions D = {D1, . . . , DN} collected across T1:N ∈T tasks, with\nper-task datasets Di = {xt, at, (rt), ...} of state-action trajectories and optional reward annotation.\nThe set of training tasks does not include the target task Ttarget /∈T1:N. We assume that the training\ndata contains task information for prior tasks, e.g., in the form of step-wise reward annotations rt or\nby consisting of task demonstrations. Such task information can often be available at no extra cost,\ne.g., if the data was collected from prior training runs (Fu et al., 2020; Çaglar Gülçehre et al., 2020)\nor human teleoperation (Mandlekar et al., 2018; Cabi et al., 2019).\nWe follow Stooke et al. (2021) and test all representation learning approaches in two phases: we ﬁrst\ntrain the representation encoder φ(x) from the ofﬂine dataset D, then transfer its parameters to the\npolicy π(φ(x)) and optimize it on the downstream task, either freezing or ﬁnetuning the encoder\nparameters. This allows us to efﬁciently reuse data collected across prior tasks.\n3.1\nUNSUPERVISED REPRESENTATION LEARNING\nUnsupervised representation learning approaches aim to learn the low-dimensional representation\nφ(x), by maximizing lower bounds on the mutual information maxφ I\n\u0000x, φ(x)\n\u0001\n.\nGenerative Representation Learning.\nMaximizes the mutual information via generative model-\ning, either of the current state (reconstruction) or of future states (prediction). A decoder D(φ(x))\nis trained by maximizing a lower bound on the data likelihood p(x) (Kingma and Welling, 2014;\nRezende et al., 2014; Denton and Fergus, 2018; Hafner et al., 2019).\nContrastive Representation Learning.\nAvoids training complex generative models by using con-\ntrastive objectives for maximizing the mutual information (Oord et al., 2018; Laskin et al., 2020;\nStooke et al., 2021). During training, mini-batches of positive and negative data pairs are assembled,\nwhere positive pairs can for example constitute consecutive frames of the same trajectory while\nnegative pairs mix frames across trajectories. The representation is then trained with the InfoNCE\nobjective (Gutmann and Hyvärinen, 2010; Oord et al., 2018), which is a lower bound on I\n\u0000x, φ(x)\n\u0001\n.\n4\nTASK-INDUCED REPRESENTATION LEARNING\nAn alternative class of representation learning approaches uses task information from prior tasks\nas a ﬁlter for which aspects of the environment are interesting, and which others do not need to be\n3\nPublished as a conference paper at ICLR 2022\nϕ(x)\nh1\nBC(a|ϕ(x))\nh2\nBC(a|ϕ(x))\nhN\nBC(a|ϕ(x))\nh1\nv(V|ϕ(x))\nh2\nv(V|ϕ(x))\nhN\nv (V|ϕ(x))\nh1\nπ(a|ϕ(x))\nQ1(ϕ(x), h1\nπ(ϕ(x)))\nCQL\nN x\nTARP-V\nTARP-CQL\nTARP-BC\nh1\nbisim(z|ϕ(x))\nP(zt+1| ¯zt, at)\n˜r1(z)\nTARP-Bisim\nBisim\nFigure 2: Instantiations of our task-induced representation learning framework. Left to right:\nRepresentation learning via multi-task value prediction (TARP-V), via multi-task ofﬂine RL (TARP-\nCQL), via bisimulation (TARP-Bisim) and via multi-task imitation learning (right, TARP-BC).\nmodeled. This addresses a fundamental problem of unsupervised representation learning approaches:\nby maximizing the mutual information between observations and representation they are trained to\nmodel every bit of information equally and thus can struggle in visually complex environments with\nlots of irrelevant details.\nFormally, the state of an environment S can be divided into task-relevant and task-irrelevant or\nnuisance components S = {Si\ntask, Si\nn}. The superscript i indicates that this division is task-dependent,\nsince some components of the state space are relevant for a particular task Ti but irrelevant for others.\nIn visually complex environments, we can assume that |Stask| ≪|S|, i.e., that the task-relevant part of\nthe state space is much smaller than all state information in the input observations (Zhaoping, 2006).\nWhen choosing how much of this input information to model, end-to-end policy learning and\nunsupervised representation learning, form two ends of a spectrum. End-to-end policy learning only\nmodels Si\ntask for its training task, which is efﬁcient, but only allows transfer from task i to task j\nif Sj\ntask ⊆Si\ntask, i.e., if the task-relevant components of the target task are a subset of those of the\ntraining task. In contrast, unsupervised representation learning models the full S = {Stask, Sn},\nwhich allows for ﬂexible transfer since Sj\ntask ⊆S ∀Tj ∈T , but training can be inefﬁcient since the\nlearned representation contains many nuisance variables.\nIn this work, we formalize the family of task-induced representation learning (TARP) approaches,\nwhich aim to combine the best of both worlds. Similar to end-to-end policy learning, task-induced\nrepresentation learning approaches use task information to learn compact, task-relevant represen-\ntations. Yet, by combining the task information from a wide range of prior tasks they learn a\nrepresentation that combines the task-relevant components of all tasks in the training dataset D:\nSTARP = S1\ntask ∪· · · ∪SN\ntask. Thus, such a representation allows for transfer to a wide range of unseen\ntasks for which Starget\ntask\n⊆STARP. Yet, the representation ﬁlters a large part of the state space which is\nnot relevant for any of the training tasks, allowing sample efﬁcient learning on the target task.\n4.1\nAPPROACHES FOR TASK-INDUCED REPRESENTATION LEARNING\nWe compare multiple approaches for task-induced representation learning which can leverage different\nforms of task supervision. We focus on reward annotations and task demonstrations; but, future\nwork can extend the TARP framework to other forms of task supervision like language commands or\nhuman preferences. All instantiations of TARP train a shared encoder network φ(x) with separate\ntask-supervision heads (see Figure 2).\nValue Prediction (TARP-V).\nWe can leverage reward annotations as task supervision, similar to\nFu et al. (2021), by estimating the future discounted return of the data collection policy. Intuitively, a\nrepresentation that allows estimation of the value of a state needs to include all task-relevant aspects of\nthis state. We introduce separate value prediction heads hi\nv for each task i and train the representation\nφ(x) by minimizing the error in the predicted discounted return:\nLTARP-V =\nN\nX\ni=1\nE(xt,rt)∼Di\n\u0012\nhi\nv(φ(xt)) −\nT\nX\nt′=t\nγt′−trt′\n\u00132\n.\n(1)\nOfﬂine RL (TARP-CQL).\nAlternatively, we can learn a representation by training a policy to\ndirectly maximize the discounted reward of the training tasks. Since we aim to use ofﬂine training\n4\nPublished as a conference paper at ICLR 2022\n(a) Distracting Control\n(d) CARLA\n(b) ViZDoom\n(c) Distracting MetaWorld\nFigure 3: Environments with high visual complexity and substantial task-irrelevant detail for testing\nthe learned representations. (a) Distracting DMControl (Stone et al., 2021) with randomly overlayed\nvideos, (b) ViZDoom (Wydmuch et al., 2018) with diverse textures and enemy appearances, (c) Dis-\ntracting MetaWorld (Yu et al., 2020) with randomly overlayed videos, and (d) CARLA (Dosovitskiy\net al., 2017) with realistic outdoor driving scenes and weather simulation.\ndata only, we leverage recently proposed methods for ofﬂine RL (Levine et al., 2020; Kumar et al.,\n2020) and introduce separate policy heads hi\nπ and critics Qi for each task. Following Kumar et al.\n(2020), we train the policy to maximize the entropy-augmented future return:\nLTARP-CQL = −\nN\nX\ni=1\nEx∼Di\n\u0012\nQi(x, hi\nπ(φ(x))) + αH\n\u0000hi\nπ(φ(x))\n\u0001\u0013\n.\n(2)\nHere H(·) denotes the entropy of the policy’s output distribution and α is a learned weighting\nfactor (Kumar et al., 2020; Haarnoja et al., 2018b).\nBisimulation (TARP-Bisim).\nWe use a bisimulation objective for reward-induced representation\nlearning (Larsen and Skou, 1991; Ferns et al., 2011). It groups states based on their “behavioral\nsimilarity”, measured as their expected future returns under arbitrary action sequences. Speciﬁcally,\nwe build on the approach of Zhang et al. (2021) that leverages deep neural networks to estimate the\nbisimulation distance, and modify it for multi-task learning by adding per-task embedding heads\nzi = hi\nbisim\n\u0000φ(x)\n\u0001\n. The representation learning objective is:\nLTARP-Bisim =\nN\nX\ni=1\nE(xj,aj,rj)\n(xk,ak,rk)∼Di\n\u0012\n|zi\nj−zi\nk|−|rj−rk|−γW2\n\u0000P(·|¯zi\nj,t, aj,t), P(·|¯zi\nk,t, ak,t)\n\u0001\u00132\n(3)\nHere P(·|z, a) is a learned latent transition model and W2 refers to the 2-Wasserstein metric which\nwe can compute in closed form for Gaussian transition models. Following Zhang et al. (2021) we use\na target encoder updated with the moving average of the encoder’s weights for producing ¯z and we\nadd an auxiliary reward prediction objective with per-task reward predictors ˜ri\nbisim(z).\nImitation Learning (TARP-BC).\nWe can also train task-induced representations from data without\nreward annotation by directly imitating the data collection policy, thus learning to represent all\nelements of the state space that were important for the policy’s decision making. We choose behavioral\ncloning (BC, Pomerleau (1989)) for imitation learning since it is easily applicable especially in the\nofﬂine setting. We introduce N separate imitation policy heads hi\nBC and minimize the negative\nlog-likelihood of the training data’s actions:\nLTARP-BC = −\nN\nX\ni=1\nE(x,a)∼Di\n\u0012\nlog hi\nBC(a|φ(x))\n\u0013\n(4)\n5\nEXPERIMENTS\n5.1\nEXPERIMENTAL SETUP\nWe compare the performance of different representation learning approaches in four visually complex\nenvironments (see Figure 3). We will brieﬂy describe each environment; for more details on\nenvironment setup and data collection, see appendix, Section A.\n5\nPublished as a conference paper at ICLR 2022\nDistracting DMControl.\nWe use the environment of Stone et al. (2021), in which the visual\ncomplexity of the standard DMControl “Walker” task (Tassa et al., 2018) is increased by overlaying\nrandomly sampled natural videos from the DAVIS 2017 dataset (Pont-Tuset et al., 2017). We train\non data collected from standing, forward and backward walking policies and test on a downstream\nrunning task. An efﬁcient representation should focus on modeling the agent while ignoring irrelevant\ninformation from the background video.\nViZDoom.\nA simulator for the ego-shooter game Doom (Wydmuch et al., 2018). We use pre-\ntrained models from Dosovitskiy and Koltun (2017) for data collection and vary their objectives to\nget diverse behaviors such as maximizing the number of collected medi-packs or minimizing the loss\nof health points. Learned representations should focus on important aspects such as the location of\nenemies or medi-packs, while ignoring irrelevant details such as the texture of walls or appearance\nfeatures of medi-packs etc. We test on the full “battle” task from Dosovitskiy and Koltun (2017).\nDistracting MetaWorld.\nBased on the MetaWorld multi-task robotic manipulation environ-\nment (Yu et al., 2020). We increase the visual complexity by overlaying the background with\nthe same natural videos used in the distracting DMControl, but deﬁne a much larger set of 15 training\nand 6 testing tasks, which require manipulation of diverse objects with a Sawyer robot arm. All\nobjects manipulated in the testing tasks are present in at least one of the training tasks. During\nevaluation, we report the average performance across the six testing tasks and normalize each task’s\nperformance with the score of the best-performing policy.\nAutonomous Driving.\nWe simulate realistic ﬁrst-person driving scenarios using the CARLA\nsimulator (Dosovitskiy et al., 2017). We collect training data from intersection crossings as well as\nright and left turns using pre-trained policies and test on a long-range point-to-point driving task that\nrequires navigating an unseen part of the environment. For efﬁcient learning, representations need\nto model relevant driving information such as position and velocity of other cars, while ignoring\nirrelevant details such as trees, shadows or the color and make of cars in the scene.\nWe compare the different instantiations of the task-induced representation learning framework from\nSection 4.1 to common unsupervised representation learning approaches:\n• Reconstruction: We train a beta-VAE (Higgins et al., 2017) or a stochastic video prediction\nmodel (“Pred-O”) on the training data and transfers the encoder.\n• Contrastive Learning: We use the contrastive learning objective from Stooke et al. (2021)\nfor pre-training (“ATC”).\nWe also compare to an approach that combines video prediction with reward prediction for pre-\ntraining (“Pred-R+O”), similar to Hafner et al. (2019), thus combining elements form unsupervised\nand task-induced representation learning. Additionally, we report performance of a policy transfer\nbaseline, which pre-trains a policy on D using BC and then ﬁnetunes the full policy on the downstream\ntask as an alternative to representation transfer2. For environments that provide a low-dimensional\nstate (DMControl, MetaWorld), we further report results for an oracle baseline.\nAfter pre-training, we transfer the frozen encoder weights to the target task policy, which we train with\nsoft actor-critic (SAC, Haarnoja et al. (2018a)) on continuous control tasks (distracting DMControl,\ndistracting MetaWorld, CARLA), and with PPO (Schulman et al., 2017) on discrete action tasks\n(ViZDoom).3 For more implementation details and hyperparameters, see Appendix D.\n5.2\nDOWNSTREAM TASK LEARNING EFFICIENCY\nWe report downstream task learning curves for task-induced and unsupervised representation learning\nmethods as well as direct policy transfer in Figure 4. The low performance of the policy transfer\nbaseline (purple) shows that in most tested environments the downstream task requires signiﬁcantly\ndifferent behaviors than those observed in the pre-training data, a scenario in which transferring\n2We also tried pre-training policies on each of the individual task datasets D1, . . . , DN but found the\nﬁnetuning performance of the policies trained on the full dataset D to be superior.\n3We report results with ﬁnetuning of the pre-trained encoder in appendix, Section C, and ﬁnd no substantial\ndifference to the setting with frozen encoder.\n6\nPublished as a conference paper at ICLR 2022\nScratch\nPred-O\nVAE\nReconstruction\nATC\nContrastive\nTARP-V\nTARP-CQL\nTARP-BC\nTask-Induced Representation\n(b) ViZDoom \n(d) CARLA\n(a) Distracting DMControl\nOracle\nPred-R+O\nPolicy Transfer\nTARP-Bisim\nReconstruction+Task-Induced\n(c) Distracting MetaWorld\nFigure 4: Performance of transferred representations on unseen target tasks. The task-induced\nrepresentations (blue) lead to higher learning efﬁciency than the fully unsupervised representations\nor direct policy transfer and achieve comparable performance to the Oracle baseline. All results\naveraged across three seeds. See Figure 9 for per-task MetaWorld performances.\nrepresentations can be beneﬁcial over directly transferring behaviors4. All reconstruction-based\napproaches (green) struggle with the complexity of the tested scenes, especially the method that\nattempts to predict the scene dynamics (Pred-O). We ﬁnd that adding reward prediction to the ob-\njective improves performance (red, Pred-O+R). Yet, downstream learning is still slow, since the\nreconstruction objective leads to task-irrelevant information being modeled in the learned representa-\ntion. The non-reconstructive contrastive approach ATC (brown) achieves stronger results, particularly\nin VizDoom which features less visual distractors, but its performance deteriorates substantially in\nenvironments with more task-irrelevant details, i.e. distracted DMControl, MetaWorld and CARLA.\nOverall, we ﬁnd that task-induced representations enable more sample efﬁcient learning. Among the\nTARP instantiations, we ﬁnd that TARP-V and TARP-Bisim representations can lead to lower transfer\nperformance, since they rely on the expressiveness of the reward function: if future rewards can be\npredicted without modeling all task-relevant features, the representations will not capture them. In\ncontrast, TARP-CQL and TARP-BC learn representations via policy learning, which can enable more\nefﬁcient transfer to downstream policy learning problems. On the distracting DMControl task we ﬁnd\nthat TARP representations even outperform representations trained with direct supervision through\na handcrafted oracle state representation, since they can learn to represent concepts like joint body\nparts of the walker during pre-training, while the oracle needs to learn these during downstream RL.\n5.3\nPROBING TASK-INDUCED REPRESENTATIONS\nTo better understand TARP’s improved learning efﬁciency, we visualize what information is captured\nin the representation in Figure 5: we compare input saliency maps for representations learned with\ntask-induced and unsupervised objectives. Saliency maps visualize the average gradient magnitude\nfor each input pixel with respect to the output φ(x) and thus capture the contribution of each part\nof the input to the representation. We ﬁnd that task-induced representations focus on the important\naspects of the scene, such as the walker agent in distracting DMControl and other cars in CARLA. In\ncontrast, the unsupervised approaches have high saliency values for scattered parts of the input and\noften represent task-irrelevant aspects such as changing background videos, buildings and trees, since\nthey cannot differentiate task-relevant and irrelevant information.\nFor quantitative analysis, we train probing networks on top of the learned representations in distracting\nDMControl. We test whether (1) task-relevant information is modeled by predicting oracle joint\nstates and (2) whether task-irrelevant information is ignored by classifying the ID of the used\nbackground video. The more irrelevant background information is captured in the representation, the\nbetter the probing network will be at classifying the video. The results show that probing networks\ntrained with task-induced representations more accurately predict the task-relevant state information\n(Figure 6a) while successfully ﬁltering information about the background video and thus obtaining\nlower background classiﬁcation accuracy (Figure 6b).\n4The policy transfer baseline achieves good performance in the distracting DMControl environment since it\ncan reuse behaviors from the walk-forward training task for the target run-forward task.\n7\nPublished as a conference paper at ICLR 2022\nATC\nVAE\nTARP\nOriginal\nDistracting DMControl\nATC\nVAE\nTARP\nOriginal\nCARLA\nFigure 5: Visualization of the learned representations. Left to right: saliency maps for represen-\ntations learned with task-induced representation learning (TARP-BC) and the highest-performing\ncomparisons for reconstruction-based (VAE) and reconstruction-free (ATC) representation learn-\ning. Left: Distracting DMControl environment. Right: CARLA environment. Only task-induced\nrepresentations can ignore distracting information and focus on the important aspects of the scene.\nTARP-BC TARP-V\nVAE\nATC\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\nState Prediction Error\n(a) State prediction\nTARP-BC TARP-V\nVAE\nATC\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nBackground \n Classification Accuracy\n(b) Background classiﬁcation\n0.00\n0.05\n0.10\n0.15\n0.20\nEnvironment steps (1M)\n0\n50\n100\n150\n200\n250\n300\nEpisode Reward\nScratch\nPred-O\nVAE\nPred-R+O\nATC\nTARP-V\nTARP-CQL\nTARP-BC\nTARP-Bisim\n(c) Transfer for imitation learning\nFigure 6: (a) Task-induced representations (TARP-BC/V) allow for more accurate prediction of the\ntask-relevant joint states. Unsupervised approaches aim to model all information in the input – thus\nprobing networks can still learn to predict state information, but struggle to achieve comparably low\nerror rates. (b) Task-induced representations successfully ﬁlter task-irrelevant background information\nand thus cannot conﬁdently classify the background video, while unsupervised approaches fail to ﬁlter\nthe irrelevant information and thus achieve perfect classiﬁcation scores. (c) Transfer performance for\nIL in distracting DMControl. Task-induced representations achieve superior sample efﬁciency.\n5.4\nTRANSFERRING REPRESENTATIONS FOR IMITATION LEARNING\nWe test whether the conclusions from the model-free RL experiments above equally apply when\ninstead performing imitation learning (IL) on the downstream task. We train policies with the pre-\ntrained representations from Section 5.2 by Soft-Q Imitation Learning (SQIL, Reddy et al. (2020)) on\nthe distracting DMControl target running task. In Figure 6c we show that task-induced representations\nalso improve downstream performance in visually complex environments for IL and allow for more\nefﬁcient imitation, since they model only task-relevant information. Again, TARP-BC and TARP-\nCQL lead to the best learning efﬁciency. This shows that the beneﬁt of modeling task-relevant aspects\nis not constrained to RL, but the same representations can be used to accelerate IL.\n5.5\nDATA ANALYSIS EXPERIMENTS\nIn the previous sections, we found that task-induced representations can improve learning efﬁciency\nover common unsupervised representation learning approaches in visually complex environments.\nThese task-induced representation learning approaches are designed to leverage data from a variety\nof sources, such as prior training runs or human teleoperation. Thus, analyzing what characteristics\nof this training data lead to successful transfer is key to their practical use – our goal in this section is\nto derive a set of best practices when collecting datasets for task-induced representation learning.\n8\nPublished as a conference paper at ICLR 2022\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nEnvironment steps (1M)\n0.0\n0.2\n0.4\n0.6\nNormalized Episode Reward\n0 Tasks (Scratch)\n3 Tasks\n5 Tasks\n10 Tasks\n17 Tasks\nFigure 7: Transfer performance vs.\nnumber of pre-training tasks. Col-\nlecting training data from a larger\nnumber of tasks is more important\nfor transfer performance than hav-\ning lots of data for few tasks.\nMany tasks vs. lots of data per task.\nWhen collecting large\ndatasets there is a trade-off between collecting a lot of data for\nfew tasks vs. collecting less data each for a larger set of tasks.\nTo analyze the effects of this trade-off on TARP, we train TARP-\nBC on data from a varying number of pre-training tasks in\ndistracting MetaWorld. When training on data from fewer tasks,\nwe collect more data on each of these tasks, to ensure that the\nsize of the training datasets is constant across all experiments.\nThe results in Figure 7 show: training on fewer data from a\nlarger number of tasks is beneﬁcial over training on lots of\ndata from few tasks. Intuitively, since downstream tasks can\nleverage the union of task-relevant components of all training\ntasks, having a diverse set of tasks is more important for transfer\nthan having lots of data for few tasks.\nExpert\nSuboptimal\nRandom\n50\n100\n150\n200\n250\n300\n350\nEpisode Reward\nSAC\nTARP-CQL\nTARP-BC\nTARP-V\nFigure 8:\nRobustness to pre-\ntraining data optimality. TARP ap-\nproaches can learn good representa-\ntions that allow for effective trans-\nfer even from sub-optimal data.\nMore data vs. optimal data.\nAnother trade-off in data col-\nlection exists between the amount of data and its optimality: a\nmethod that can learn from sub-optimal trajectory data is able to\nleverage much larger and more diverse datasets. Since training\non diverse data is important for successful transfer (see above)\nrobustness to sub-optimal training data is an important feature\nof any representation learning approach. We test the robustness\nof task-induced representation learning approaches by training\non sub-optimal trajectory data, collected from only partially\ntrained and completely random policies, in distracting DMCon-\ntrol. The downstream RL performance comparison in Figure 8\nshows that TARP approaches can learn strong representations\nfrom low-performance trajectory data and even when trained\nfrom random data, performance does not decrease compared\nto a SAC baseline trained from scratch. Intuitively, task-induced representation learning does not\npre-train a model on “how to act” but merely “what to pay attention to”. We ﬁnd that TARP-CQL’s\nperformance can even increase slightly with the suboptimal data, which we attribute to its increased\nstate coverage. Thus we conclude that task-induced representation learning is robust to the optimality\nof the pre-training data and collecting larger, diverse datasets is more important than collecting\noptimal data.\nMulti-Source Task Supervision.\nWhen collecting large datasets, it can be beneﬁcial to pool data\nfrom multiple sources, e.g. data obtained from prior training runs or via human teleoperation. In\nSection 4 we introduced different instantiations of the TARP framework that are able to leverage\ndifferent forms of task supervision. Here, we test whether we can train a single representation\nusing multiple sources of task supervision simultaneously. In particular, we train “TARP-V+BC”\nmodels that assume reward annotations for only some of the tasks in the pre-training dataset and\ndemonstration data for the remaining tasks (for more details on data collection, see Section B). We\ncompare downstream learning efﬁciency on distracting DMControl and CARLA in Figure 10 We\nﬁnd that the combined-source model trained from the heterogeneous dataset achieves comparable or\nsuperior performance to all single-source models, showing that practitioners should collect diverse\ndatasets, even if they have heterogeneous sources of task-supervision.\n6\nCONCLUSION\nIn this work, we investigate the effectiveness of representation learning approaches for transfer in\nvisually complex scenes. We deﬁne the family of task-induced representation learning approaches,\nthat leverage task-information from prior tasks to only model task-relevant aspects of a scene\nwhile ignoring distractors. We compare task-induced representations to common unsupervised\nrepresentation learning approaches across four visually complex environments with substantial\ndistractors and ﬁnd that they lead to improved sample efﬁciency on downstream tasks compared to\nunsupervised approaches. Future work should investigate approaches for incorporating other sources\nof task information such as language commands to learn task-induced representation.\n9\nPublished as a conference paper at ICLR 2022\n7\nREPRODUCIBILITY STATEMENT\nWe provide a detailed description of all used environments, the procedures for ofﬂine dataset collec-\ntion, and descriptions of the downstream evaluation tasks in appendix, Section A. Furthermore, in\nappendix, Section D we list all hyperparameters used for the pre-training phase (i.e. task-induced\nand unsupervised representation learning) as well as for training of the RL policies. We open-\nsource our codebase with example commands to reproduce our results on the project website:\nclvrai.com/tarp.\n8\nACKNOWLEDGEMENT\nWe thank our colleagues from the CLVR lab at USC for the valuable discussions that considerably\nhelped to shape this work.\nREFERENCES\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\nSerkan Cabi, Sergio Gomez Colmenarejo, Alexander Novikov, Ksenia Konyushkova, Scott Reed,\nRae Jeong, Konrad Zolna, Yusuf Aytar, David Budden, Mel Vecerik, Oleg Sushkov, David Barker,\nJonathan Scholz, Misha Denil, Nando de Freitas, and Ziyu Wang. Scaling data-driven robotics\nwith reward sketching and batch reinforcement learning. RSS, 2019.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning,\n2020.\nE. Denton and R. Fergus. Stochastic video generation with a learned prior. In ICML, 2018.\nAlexey Dosovitskiy and Vladlen Koltun. Learning to act by predicting the future. ICLR, 2017.\nAlexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA:\nAn open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning,\npages 1–16, 2017.\nNorm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous markov\ndecision processes. SIAM Journal on Computing, 40(6):1662–1714, 2011.\nChelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep\nspatial autoencoders for visuomotor learning. In Proceedings of IEEE International Conference on\nRobotics and Automation, 2016.\nJustin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\nXiang Fu, Ge Yang, Pulkit Agrawal, and Tommi Jaakkola. Learning task informed abstractions.\nIn Proceedings of the 38th International Conference on Machine Learning, Proceedings of Ma-\nchine Learning Research. PMLR, 2021. URL http://proceedings.mlr.press/v139/\nfu21b.html.\nMichael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings of the Thirteenth International Conference on\nArtiﬁcial Intelligence and Statistics, pages 297–304. JMLR Workshop and Conference Proceedings,\n2010.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. ICML, 2018a.\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash\nKumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al.\nSoft actor-critic algorithms and\napplications. arXiv preprint arXiv:1812.05905, 2018b.\n10\nPublished as a conference paper at ICLR 2022\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James\nDavidson. Learning latent dynamics for planning from pixels. ICML, 2019.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a\nconstrained variational framework. In ICLR, 2017.\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre\nQuillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep\nreinforcement learning for vision-based robotic manipulation. In Conference on Robot Learning,\npages 651–673, 2018.\nDiederik P Kingma and Max Welling. Auto-encoding variational Bayes. In ICLR, 2014.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine\nreinforcement learning. NeurIPS, 2020.\nSascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning.\nIn The 2010 International Joint Conference on Neural Networks (IJCNN), 2010.\nKim G Larsen and Arne Skou. Bisimulation through probabilistic testing. Information and computa-\ntion, 94(1):1–28, 1991.\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations\nfor reinforcement learning. In International Conference on Machine Learning, 2020.\nMichael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas.\nReinforcement learning with augmented data. In Proceedings of Neural Information Processing\nSystems (NeurIPS), 2021.\nAlex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:\nDeep reinforcement learning with a latent variable model. NeurIPS, 2020.\nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep\nvisuomotor policies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tutorial,\nreview, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\nAjay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian\nGao, John Emmons, Anchit Gupta, Emre Orbay, Silvio Savarese, and Li Fei-Fei. Roboturk: A\ncrowdsourcing platform for robotic skill learning through imitation. In CoRL, 2018.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,\nShane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.\nNature, 518:529–533, 02 2015.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\nDean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Proceedings of\nNeural Information Processing Systems (NeurIPS), pages 305–313, 1989.\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and\nLuc Van Gool.\nThe 2017 davis challenge on video object segmentation.\narXiv preprint\narXiv:1704.00675, 2017.\nSiddharth Reddy, Anca D. Dragan, and Sergey Levine. {SQIL}: Imitation learning via reinforcement\nlearning with sparse rewards. In International Conference on Learning Representations, 2020.\nURL https://openreview.net/forum?id=S1xKd24twB.\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and\napproximate inference in deep generative models. In ICML, 2014.\n11\nPublished as a conference paper at ICLR 2022\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nAustin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski. The distracting control suite –\na challenging benchmark for reinforcement learning from pixels. arXiv preprint arXiv:2101.02722,\n2021.\nAdam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning\nfrom reinforcement learning. ICML, 2021.\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,\nAbbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint\narXiv:1801.00690, 2018.\nMarcus Loo Vergara. Accelerating training of deep reinforcement learning-based autonomous driving\nagents through comparative study of agent and environment designs. Master’s thesis, Oct 2019.\nURL https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/2625841.\nMarek Wydmuch, Michał Kempka, and Wojciech Ja´skowski. Vizdoom competitions: Playing doom\nfrom pixels. IEEE Transactions on Games, 2018.\nMengjiao Yang and Oﬁr Nachum. Representation matters: Ofﬂine pretraining for sequential decision\nmaking. arXiv preprint arXiv:2102.05815, 2021.\nDenis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing\ndeep reinforcement learning from pixels. In ICLR, 2021.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey\nLevine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\nIn Conference on Robot Learning (CoRL), 2020.\nAlbert Zhan, Ruihan Zhao, Lerrel Pinto, Pieter Abbeel, and Michael Laskin. A framework for\nefﬁcient robotic manipulation. arXiv preprint arXiv:2012.07975, 2020.\nAmy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant\nrepresentations for reinforcement learning without reconstruction. ICLR, 2021.\nLi Zhaoping. Theoretical understanding of the early visual processes by data compression and data\nselection. Network: Computation in Neural Systems, 2006.\nÇaglar Gülçehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio Gómez Colmenarejo,\nKonrad Zolna, Rishabh Agarwal, Josh Merel, Daniel J. Mankowitz, Cosmin Paduraru, Gabriel\nDulac-Arnold, Jerry Li, Mohammad Norouzi, Matthew Hoffman, Nicolas Heess, and Nando\nde Freitas.\nRl unplugged: A suite of benchmarks for ofﬂine reinforcement learning.\nIn\nNeurIPS, 2020.\nURL https://proceedings.neurips.cc/paper/2020/hash/\n51200d29d1fc15f5a71c1dab4bb54f7c-Abstract.html.\n12\nPublished as a conference paper at ICLR 2022\nA\nENVIRONMENTS\nA.1\nDISTRACTING DMCONTROL\nFollowing Stone et al. (2021), we increase the visual complexity of the DMControl “walker”\ntask (Tassa et al., 2018) by overlaying randomly sampled videos from the DAVIS 2017 dataset (Pont-\nTuset et al., 2017) in the background. In our experiments we use expert policies to collect ofﬂine\ndatasets for the pre-training tasks of standing, forward walking, and backward walking respectively.\nTo collect these datasets, we pre-train polices with SAC(Haarnoja et al., 2018a) in the state space\nand collect rollouts of the visual observation. We test our representation on the downstream task of\n“running”.\nRewards: We use reward functions provided by DMControl “walker” task (Tassa et al., 2018). For\nthe backward walking task, we invert the sign of the walking speed in the “forward” task deﬁned in\nDMControl, so that the agent gets higher reward when it moves backwards instead of forwards.\nA.2\nVIZDOOM\nOur experiments use the “D3 battle”environment provided by Wydmuch et al. (2018) where the\nagent’s objective is to defend against enemies while collecting medi-packs and ammunition. For\ncollection of the pre-training task dataset, we use the pre-trained models provided by Dosovitskiy\nand Koltun (2017) and vary the reward weighting parameters (described below) to produce a diverse\nset of behaviors.\nRewards: A reward function is deﬁned by a linear combination of three measurements (ammunition,\nhealth, and frags) with their corresponding coefﬁcients.\nRViZDoom = cammo · xamm\nt\n−xamm\nt−1\n7.5\n+ chealth · xhealth\nt\n−xhealth\nt−1\n30.0\n+ cfrags · xfrags\nt\n−xfrags\nt−1\n1.0\n.\n(5)\nwhere xamm\nt\nis a measurement of ammunition, xhealth\nt\nis the health of the agent, and xfrags\nt\nis the\nnumber of frags at timestep t. The set of coefﬁcients for ammunition, health and frags are represented\nas (cammo, chealth, cfrags). We use the coefﬁcients of (0, 0, 1), (0, 1, 0), and (1, 1, −1) for collecting\nthe pre-training data, and (0.5, 0.5, 1.0) for the target task.\nA.3\nDISTRACTING METAWORLD\nWe increase the visual complexity of MetaWorld environment (Yu et al., 2020) by overlaying randomly\nsampled videos from the DAVIS 2017 dataset (Pont-Tuset et al., 2017), similar to Stone et al. (2021).\nFor data collection, we pre-train polices with SAC(Haarnoja et al., 2018a) using low-dimensional state\ninformation and collect rollouts of the visual observations. All objects manipulated in the downstream\ntasks are present in at least one of the training tasks. We deﬁne 15 tasks for data collection:\nbutton press\nbutton press topdown\nbutton press topdown wall\nplate slide\nplate slide back\nplate slide side\nhandle pull\nhandle pull side\nhandle press\ndoor open\ndoor lock\ncoffee button\ncoffee push\npush\npush back\nWe evaluated transfer performance on 6 downstream tasks:\nbutton press wall\nplate slide back side\nhandle press side\ndoor unlock\ncoffee pull\npush wall\nWe report the average performance across the six downstream tasks and normalize each task’s\nperformance with the episode reward of the best-performing policy. Furthermore, we show the\nperformance of each downstream task in Figure 9.\nRewards: We use reward functions provided by Metaworld (Yu et al., 2020).\n13\nPublished as a conference paper at ICLR 2022\nA.4\nCARLA\nWe use the map of “Town05” from the CARLA environment (Dosovitskiy et al., 2017) for our\nexperiments. At the beginning of each episode, we randomly spawn 300 vehicles and 200 pedestrians.\nThe initial location of the agent is randomly sampled from a task set containing multiple start and\ngoal locations. We collect pre-training datasets for the tasks of intersection crossing, taking a right\nturn, and taking a left turn using pre-trained policies. Then, we test on a long-range curvy road\npoint-to-point driving task that requires navigating an unseen part of the environment. We pre-train\nthe policies with SAC (Haarnoja et al., 2018a) using segmentation masks of the environment as the\ninput, and then use the learnt policies to collect rollouts of visual observations for the datasets.\nRewards: We modify a reward function used in Vergara (2019) for all of the tasks. The reward\nfunction consists of terms for speed, centering on a road, angle of the agent, and collision.\nRspeed =\nv\nvmin\n· 1v≤vmin + (1.0 −\nv −vtarget\nvmax −vtarget\n) · 1v≥vmax + 1.0 · 1vmin≤v≤vmax.\nRcentering = max(1.0 −dcenter\ndmax\n, 0).\nRangle = max(1.0 −|\nr\n(rmax ·\nπ\n180)|, 0).\nRCARLA = Rspeed + Rcentering + Rangle −10−4 · collision_intensity.\n(6)\nwhere v is velocity of the agent, dcenter is distance between the center of the road and the agent, r is\nangle of the agent. We use constant values of vmin = 15.0, vmax = 30.0, vtarget = 25.0, dmax = 3,\nand rmax = 20.\nB\nDATA COLLECTION FOR MULTI-SOURCE TASK SUPERVISION\nWe detail the composition of the pre-training dataset for the experiment in which we train TARP from\nheterogeneous data sources. For the distracting DMControl environment, the dataset is composed of\ndemonstrations for the “forward walking“ and “backward walking“ tasks, and a dataset with reward\nannotations for the “stand“ task. For the CARLA environment, the task-induced representations\nare trained on a dataset composed of demonstrations for the “intersection crossing” task, and data\nannotated with rewards for the “right turn” and “left turn” tasks. For both environments, the datasets\nare collected with the procedures described in appendix, Section A.\nC\nFINETUNING LEARNED REPRESENTATIONS\nIn our experimental evaluation in Section 5 we held the parameters of the pre-trained encoder ﬁxed\nto cleanly evaluate the quality of the pre-trained representation. However, in practice pre-trained\nrepresentations are often ﬁnetuned on the target task using target task rewards. Prior work on\nrepresentation learning found mixed results when ﬁnetuning the pre-trained representations (Yang\nand Nachum, 2021). Thus, in this section we experimentally compare the different representation\nlearning approaches on the Distracting DMControl, ViZDoom, and CARLA environment while\nﬁnetuning the learned representations on the target task. As illustrated in Figure 11(a), we ﬁnd that\ntask-induced representations show improved sample efﬁciency and better performance even in the\nﬁne-tuning setting, analogous to the results in Section 5.2.\n14\nPublished as a conference paper at ICLR 2022\nScratch\nPred-O\nVAE\nReconstruction\nATC\nContrastive\nTARP-V\nTARP-CQL\nTARP-BC\nTask-Induced Representation\n(e) Coffee pull \n(d) Door unlock\nOracle\nPred-R+O\nTARP-Bisim\n(f) Push wall\n(a) Button press wall\n(b) Plate slide back side \n(c) Handle press side\nFigure 9: Transfer performance for each downstream task in the distracting MetaWorld environment.\nThe task-induced representations generally show better performance than unsupervised representa-\ntions and achieve comparable performance to the oracle baseline.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nEnvironment steps (1M)\n0\n50\n100\n150\n200\n250\n300\nEpisode Reward\nTARP-V\nTARP-BC\nTARP-V+BC\n(a) Distracting DMControl\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nEnvironment steps (1M)\n0\n50\n100\n150\n200\nEpisode Reward\nTARP-V\nTARP-BC\nTARP-V+BC\n(b) CARLA\nFigure 10: Transfer performance for single source and multiple source task-induced representation.\nThe task-induced representation with multiple supervision sources performs slightly better or as good\nas the single source representation. This shows that task-induced representations can be effectively\ntrained from datasets with heterogeneous forms of task supervision.\n15\nPublished as a conference paper at ICLR 2022\nScratch\nTARP-V\nTARP-CQL\nTARP-BC\nTask-Induced Representation\nTARP-Bisim\n(a) Distracting DMControl\n(b) ViZDoom\nPred-O\nVAE\nReconstruction\nATC\nContrastive\nPred-R+O\nReconstruction+Task-Induced\n(c) CARLA\nFigure 11: Performance of transferred representations with ﬁnetuning on unseen target tasks on\ndistracting DMControl, ViZDoom, and CARLA environments. The task-induced representations\n(blue) show better sample efﬁciency and performance compared to the representations learned with\nunsupervised objectives.\nD\nHYPERPARAMETERS\nD.1\nHYPERPARAMETERS FOR RL\nTable 1: Common SAC hyperparameter\nParameter\nValue\nStacked frames\n3\nOptimizer\nAdam\nLearning rate\n3e-4\nDiscount factor (γ)\n0.99\nLatent dimension\n256\nConvolution ﬁlters\n[8, 16, 32, 64]\nConvolution strides\n[2, 2, 2, 2]\nConvolution ﬁlter size\n3\nHidden Units (MLP)\n[1024]\nNonlinearity\nReLU\nTarget smoothing coefﬁcient (τ)\n0.005\nTarget entropy\n−dim(A)\nTable 2: Distracting DMControl and MetaWorld SAC hyperparameter\nParameter\nValue\nObservation Rendering\n(64, 64), RGB\nInitial steps\n5 × 103\nAction repeat\n2\nReplay buffer size\n105\nMinibatch size\n256\nTarget update interval\n1\nActor update interval\n1\nInitial temperature\n1\n16\nPublished as a conference paper at ICLR 2022\nTable 3: CARLA SAC hyperparameter\nParameter\nValue\nObservation Rendering\n(128, 128), RGB\nInitial steps\n3 × 103\nAction repeat\n1\nReplay buffer size\n105\nMinibatch size\n128\nTarget update interval\n2\nActor update interval\n2\nInitial temperature\n0.1\nTable 4: ViZDoom PPO hyperparameter\nParameter\nValue\nObservation rendering\n(64, 64) Grey\nStacked frames\n4\nAction repeat\n1\nOptimizer\nAdam\nLearning rate\n3e-4\nPPO epoch\n10\nBuffer size\n2048\nConvolution ﬁlters\n[8, 16, 32, 64]\nConvolution ﬁlter sizes\n[2, 2, 2, 2]\nHidden units (MLP)\n[256]\nGeneralized advantage estimation λ\n0.95\nEntropy bonus coefﬁcient\n4e-3\nDiscount factor (γ)\n0.99\nMinibatch size\n256\nNonlinearity\nReLU\nTable 5: Hyperparameters for CQL\nEnvironment\nTrade-off factor α for Q-values\nNumber of action samples\nDistracting DMControl\n3.\n1\nViZDoom\n1.\n1\nCARLA\n3.\n1\nD.2\nHYPERPARAMETERS FOR PRE-TRAINING\nIn TARP-BC, we use recurrent neural networks to predict a sequence of actions over prediction\nhorizon T to learn task-induced representations only in CARLA (see Table 7).\nTable 6: Common model parameters\nParameter\nValue\nBatch size\n128\nHidden units (MLP)\n[256]\nLearning rate\n1e-4\n17\nPublished as a conference paper at ICLR 2022\nTable 7: Hyperparameters for TARP-BC\nEnvironment\nLSTM hidden units\nPrediction horizon\nDistracting DMControl\n—\n-\nViZDoom\n—\n-\nDistracting MetaWorld\n—\n-\nCARLA\n512\n8\nTable 8: Hyperparameters for TARP-V\nEnvironment\nDiscount rate\nAll environments\n0.4\nTable 9: Hyperparameters for TARP-Bisim\nEnvironment\nReward predictive loss weight\nBisimulation loss weight T\nDistracting DMControl\n1.\n0.1\nViZDoom\n1.\n10.\nDistracting MetaWorld\n1.\n0.1\nCARLA\n1.\n0.01\nTable 10: Hyperparameters for VAE\nEnvironment\nβ constraint\nDistracting DMControl\n100.\nViZDoom\n100.\nDistracting MetaWorld\n100.\nCARLA\n10.\nTable 11: Hyperparameters for Pred-S and Pred-R+S\nEnvironment\nβ constraint\nPrediction horizon T\nDistracting DMControl\n50.\n6\nViZDoom\n10.\n6\nDistracting MetaWorld\n50.\n6\nCARLA\n5.\n8\nTable 12: Hyperparameters for ATC\nEnvironment\nRandom shift probability\nTemporal shift\nAll environments\n1.\n3\n18\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2022-04-25",
  "updated": "2022-04-25"
}