{
  "id": "http://arxiv.org/abs/2205.01757v1",
  "title": "XLTime: A Cross-Lingual Knowledge Transfer Framework for Temporal Expression Extraction",
  "authors": [
    "Yuwei Cao",
    "William Groves",
    "Tanay Kumar Saha",
    "Joel R. Tetreault",
    "Alex Jaimes",
    "Hao Peng",
    "Philip S. Yu"
  ],
  "abstract": "Temporal Expression Extraction (TEE) is essential for understanding time in\nnatural language. It has applications in Natural Language Processing (NLP)\ntasks such as question answering, information retrieval, and causal inference.\nTo date, work in this area has mostly focused on English as there is a scarcity\nof labeled data for other languages. We propose XLTime, a novel framework for\nmultilingual TEE. XLTime works on top of pre-trained language models and\nleverages multi-task learning to prompt cross-language knowledge transfer both\nfrom English and within the non-English languages. XLTime alleviates problems\ncaused by a shortage of data in the target language. We apply XLTime with\ndifferent language models and show that it outperforms the previous automatic\nSOTA methods on French, Spanish, Portuguese, and Basque, by large margins.\nXLTime also closes the gap considerably on the handcrafted HeidelTime method.",
  "text": "XLTime: A Cross-Lingual Knowledge Transfer Framework for Temporal\nExpression Extraction\nYuwei Cao1, William Groves2, Tanay Kumar Saha3, Joel R. Tetreault2,\nAlex Jaimes2, Hao Peng4, Philip S. Yu1\n1Computer Science Department, University of Illinois Chicago\n2Dataminr Inc. 3Walmart Global Tech\n4BDBC, Beihang University\n{ycao43,psyu}@uic.edu, {wgroves,jtetreault,ajaimes}@dataminr.com\ntanaykumar.saha@walmart.com, penghao@buaa.edu.cn\nAbstract\nTemporal Expression Extraction (TEE) is es-\nsential for understanding time in natural lan-\nguage. It has applications in Natural Language\nProcessing (NLP) tasks such as question an-\nswering, information retrieval, and causal in-\nference. To date, work in this area has mostly\nfocused on English as there is a scarcity of la-\nbeled data for other languages. We propose\nXLTime, a novel framework for multilingual\nTEE. XLTime works on top of pre-trained lan-\nguage models and leverages multi-task learn-\ning to prompt cross-language knowledge trans-\nfer both from English and within the non-\nEnglish languages.\nXLTime alleviates prob-\nlems caused by a shortage of data in the tar-\nget language. We apply XLTime with differ-\nent language models and show that it outper-\nforms the previous automatic SOTA methods\non French, Spanish, Portuguese, and Basque,\nby large margins. XLTime also closes the gap\nconsiderably on the handcrafted HeidelTime\nmethod.\n1\nIntroduction\nTemporal Expression Extraction (TEE) refers to the\ndetection of temporal expressions (such as dates,\ndurations, etc., as shown in Table 1). It is an im-\nportant NLP task (UzZaman et al., 2013) and has\ndownstream applications in question answering\n(Choi et al., 2018), information retrieval (Mitra\net al., 2018), and causal inference (Feder et al.,\n2021). Most TEE methods work on English and\nare rule-based (Strötgen and Gertz, 2013; Zhong\net al., 2017). Deep learning-based methods (Chen\net al., 2019; Lange et al., 2020) are less common\nand report results on par with or inferior to the\nrule-based SOTAs.\nMoreover, methods that work on other languages\nare rare, because of the scarcity of annotated data.\nWe ﬁnd that that there is considerable room for\nimproving TEE, especially for low-resource lan-\nguages.\nFor example, the previous SOTA per-\nIn the last three months\n|\n{z\n}\nDuration\n, net revenue rose 4.3%\nto $525.8 million from $504.2 million last year\n| {z }\nDate\n.\nThe ofﬁcial news agency, which gives the daily\n|{z}\nSet\ntally of inspections, updated on Friday evening\n|\n{z\n}\nTime\n.\nTable 1: Temporal expressions of different types (See\nAppendix A for the deﬁnitions of the types).\nformance on the English TE3 dataset (UzZaman\net al., 2013) is around 0.90 in F1, while that on\nthe Basque TEE benchmark (Altuna et al., 2016)\nis merely 0.47. Recent deep learning methods,\nwhich have shown gains for many tasks, are un-\nderexplored for this important area of NLP.\nDeveloping an approach that can learn using\nthe existing limited amount of training data is cru-\ncial for this ﬁeld because of the effort required\nto develop high-quality rules for each language.\nThus we propose a cross-lingual knowledge trans-\nfer framework for multilingual TEE, namely, XL-\nTime. We base our framework on pre-trained multi-\nlingual models (Devlin et al., 2019; Conneau et al.,\n2020). We then use Multi-Task Learning (MTL)\n(Liu et al., 2019a) to prompt knowledge transfer\nboth from English and within the low-resource\nlanguages. For this, we design primary and sec-\nondary tasks. The primary task leverages the ex-\nisting, annotated TEE data of the other languages.\nIt transfers explicit knowledge that tells the forms\nof the temporal expressions in a source language.\nThe secondary task maps the annotated source lan-\nguage TEE data samples to the target language\nusing machine-translation tools, such as Google\nTranslate, and acquires sentence-level labels (of\nthe presence of one or more time expressions) from\nthe original token-level labels. It constructs train-\ning data in a weakly-supervised manner. The sec-\nondary task transfers implicit knowledge by teach-\narXiv:2205.01757v1  [cs.CL]  3 May 2022\ning the model to detect the presence of temporal\nexpressions in text from the target language.\nContributions. 1) We propose XLTime, which\nprompts cross-lingual knowledge transfer using\nMTL to address multilingual TEE. 2) We show\nthat XLTime outperforms the previous automatic\nSOTA methods by large margins on four languages\nincluding French (FR), Spanish (ES), Portuguese\n(PT), and Basque (EU), which are “low-resource”\nfor the TEE task. 3) We show that XLTime also\napproaches the performance of the heavily hand-\ncrafted HeidelTime (Strötgen and Gertz, 2013), and\nXLTime even outperforms it on two languages (Por-\ntuguese and Basque). We make our code and data\npublicly available.1\n2\nRelated Work\nWhile TEE is an important problem in NLP, there\nis relatively little work in the area, and most of\nthis work focuses on English. Prior art can be\ndivided into two classes: rule/pattern-based and\ndeep learning approaches. In the ﬁrst class, Heidel-\nTime (Strötgen and Gertz, 2013) is the top perform-\ning approach to date, and covers over a dozen lan-\nguages. It is driven by a collection of ﬁnely-tuned\nrules. The approach was later extended to more lan-\nguages with HeidelTime-auto (Strötgen and Gertz,\n2015), which leverages language-independent pro-\ncessing and rules. Other approaches include Syn-\nTime (Zhong et al., 2017), which is based on heuris-\ntic rules, and SUTIME (Chang and Manning, 2012)\nand PTime (Ding et al., 2019), which leverages pat-\ntern learning.\nFor the second class, Laparra et al. (2018) pro-\nposes a model based on RNNs. Chen et al. (2019)\nuses BERT with a linear classiﬁer. Lange et al.\n(2020) inputs mBERT embeddings to a BiLSTM\nwith a CRF layer and outperforms HeidelTime-auto\non four languages. However, the reported perfor-\nmances of the deep learning-based methods are\ninferior to the rule-based ones, which is, in part,\ndue to the complexity of the problem and training\ndata paucity. In our work, we propose a new model\nwhich outperforms prior deep learning methods but\nalso closes the gap considerably on HeidelTime,\ndespite the data issues.\nIn addition, we are aware that applying label pro-\njection methods (Jain et al., 2019) can be a straight-\nforward way to address the data scarcity in non-\nEnglish TEE. TMP (Jain et al., 2019), originally\n1https://github.com/YuweiCao-UIC/XLTime\nproposed for cross-lingual named entity recogni-\ntion (NER) (Lample et al., 2016), projects English\ndata in IOB (Inside Outside Beginning) tagging\nformat (Ramshaw and Marcus, 1999) to that of\nthe other languages using machine translation, or-\nthographic, and phonetic similarity packages. We\nshow that the proposed XLTime, speciﬁcally de-\nsigned to transfer temporal knowledge between\nlanguages, outperforms TMP by large margins.\n3\nProposed Method\nWe formalize TEE as a sequence labeling task, sim-\nilar to NER (Lample et al., 2016). The architecture\nis shown in Figure 1.\n3.1\nPre-trained Multilingual Backbone\nXLTime adopts SOTA multilingual models, i.e.,\nmBERT (Devlin et al., 2019) and XLMR (Conneau\net al., 2020) as the backbone. The pre-trained back-\nbone contains lexicon and Transformer encoder\nlayers as shown in Figure 1(a). The backbone al-\nlows XLTime to acquire semantic and syntactic\nknowledge of various languages. The backbone is\nshared by the MTL tasks introduced in Section 3.2.\n3.2\nMTL-based Cross-Lingual Knowledge\nTransfer\nXLTime transfers knowledge from multiple source\nlanguages to the low-resource target language. The\nsource languages include English and others for\nwhich TEE training data is available. We design\nprimary and secondary tasks on top of the back-\nbone to prompt explicit and implicit knowledge\ntransfer. The primary task transfers knowledge that\nexplicitly encodes the forms of the temporal expres-\nsions in a source language. It is formalized as se-\nquence labeling and directly leverages the training\ndata of the source language to train the backbone\nalong with the primary task classiﬁer, shown in\nFigure 1 (a). The primary task minimizes Lsl:\nLsl = −\nb\nX\ni=1\nmi\nX\nj=1\n1(yij, c)log(softmax(W · x)),\n(1)\nwhere b is the total number of input sequences and\nmi is the length of the ith sequence. x ∈Rd,\noutput by the backbone, is the embedding of the\njth token in the ith sequence. d is its dimension.\nc = argmax(W · x) and yij are the predicted and\nground-truth labels of the token. W ∈R|c|×d is the\nparameter of the primary task classiﬁer. |c| is the\n3ULPDU\\\u0003WDVN\u0003\u0010\u0003(1\u0015)5\u001d\u0003\n\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u001d\u00036L[\u0003GHDWKV\u0003ZHUH\u0003UHSRUWHG\u0003LQ\u0003\u0003\u0003\u0003WKH\u0003\u0003\u0003\u0003\u0003\u0003\u0003ODVW\u0003\u0003\u0003\u0003ZHHN\u0011\n\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u001d\u0003\u00032\u0003\u0003\u0003\u0003\u0003\u00032\u0003\u0003\u0003\u0003\u0003\u0003\u00032\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u00032\u0003\u0003\u0003\u0003\u0003\u0003\u00032\u0003%\u0010'DWH\u0003,\u0010'DWH\u0003,\u0010'DWH\n\u0003\u0003\u0003\u0003\u0003\u001d\u0003,W¶V\u0003SRVVLEOH\u0003WKHUH\u0003ZLOO\u0003EH\u0003PRUH\u0011\n\u0003\u0003\u0003\u0003\u0003\u001d\u0003\u00032\u0003\u0003\u0003\u0003\u0003\u00032\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u00032\u0003\u0003\u0003\u0003\u0003\u00032\u0003\u0003\u00032\u0003\u0003\u0003\u00032\n3ULPDU\\\u0003WDVN\u0003\u0010\u0003(6\u0015)5\u001d\u0003\n\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u001d\u0003\u00030D\\R\u0003\u0003\u0003\\\u0003\u0003\u0003-XQLR\u0011\n\u0003\u0003\u0003\u0003\u0003\u001d\u0003%\u0010'DWH\u00032\u0003%\u0010'DWH\n6HFRQGDU\\\u0003WDVN\u0003\u0010\u0003(1\u0015)5\u001d\n\u0003\u0003\u0003\u0003\u0003\u001d\u00036L[\u0003GpFqV\u0003RQW\u0003pWp\u0003VLJQDOpV\u0003OD\u0003VHPDLQH\u0003GHUQLqUH\u0011\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u001d\u0003\u0014\n\u0003\u0003\u0003\u0003\u0003\u001d\u0003\u0003,O\u0003HVW\u0003SRVVLEOH\u0003TX\nLO\u0003\\\u0003HQ\u0003DLW\u0003SOXV\u0011\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u001d\u0003\u0013\n6HFRQGDU\\\u0003WDVN\u0003\u0010\u0003(6\u0015)5\u001d\n\u0003\u0003\u0003\u0003\u0003\u001d\u0003PDL\u0003HW\u0003MXLQ\u0011\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u001d\u0003\u0014\n\u0003\u0003\u0003\u0003\u0003\n7UDQVODWH\n7UDQVODWH\n/H[LFRQ\u0003(QFRGHU\n7UDQVIRUPHU\u0003/D\\HUV\n>&/6@\n3ULPDU\\\u0003WDVN\u0003FODVVLILHU\n\u000b6HTXHQFH\u0003/DEHOLQJ\f\n6HFRQGDU\\\u0003WDVN\u0003FODVVLILHU\n\u000b%LQDU\\\u0003&ODVVLILFDWLRQ\f\n(QJOLVK\n\u000bVRXUFH\f\n6SDQLVK\n\u000bVRXUFH\f\n)UHQFK\n\u000bWDUJHW\f\n7UDQVODWH\n3ULPDU\\\u0003WDVN\u0003\u0010\u0003(1\u0015)5\n3ULPDU\\\u0003WDVN\u0003\u0010\u0003(6\u0015)5\n6HFRQGDU\\\u0003WDVN\u0003\u0010\u0003(1\u0015)5\n6HFRQGDU\\\u0003WDVN\u0003\u0010\u0003(6\u0015)5\n\u000bE\f\u0003\u00036DPSOH\u0003,QSXW\u0003DQG\u0003ODEHO\n\u000bD\f\u0003\u0003$UFKLWHFWXUH\u0003RI\u0003WKH\u0003;/7LPH\u0003)UDPHZRUN\n7DVN\u0010W\\SH\u0010VSHFLILF\u0003\nFODVVLILHUV\n6KDUHG\u0003SUH\u0010WUDLQHG\u0003\nEDFNERQH\nFigure 1: The architecture and sample training input of the proposed XLTime framework (best viewed in color). (a)\nshows how XLTime transfers knowledge from English (EN) and Spanish (ES) to French (FR) through the primary\nand the secondary tasks. (b) presents sample input of the tasks.\ntotal number of unique ground-truth labels. 1(, ) is\n1 if its two arguments are equal and 0 otherwise.\nThe secondary task implicitly reveals how the\ntemporal expressions would be expressed in the\ntarget language. We translate the sequences in the\nsource language training data into the target lan-\nguage using Google Translate (we observe similar\nresults with AWS Translate). The secondary task is\nformalized as binary classiﬁcation, where the input\nsamples are the translated sequences and the labels\nare sentence-level indicators of whether or not the\nsequences contain temporal expressions (which can\nbe easily inferred from the original labels). This\ntask tunes the model to learn the characteristics\nof temporal expressions in the target language in\nan implicit manner. It is weakly-supervised and\nrequires no token-level labeling. It trains the back-\nbone and the secondary task classiﬁer by minimiz-\ning Lbc:\nLbc = −\nb\nX\ni=1\n1(y′\ni, c′)log(softmax(W′ · x′)),\n(2)\nwhere x′ ∈Rd is the sequence embedding output\nby the [CLS] of the backbone. W′ ∈R2×d is the\nparameter matrix of the secondary task classiﬁer.\nc′ = argmax(W′·x′) and y′\ni are the predicted and\ntrue sequence labels of the ith sequence. We train\nXLTime concurrently on the primary and secondary\ntasks (further details found in Appendix B).\nAn Illustrative Example. In Figure 1, Primary\ntask - EN2FR and Secondary task - EN2FR transfer\nknowledge from English to French. Primary task -\nEN2FR reveals the exact forms of English temporal\nexpressions using token-level labels (Y11 and Y12).\nSecondary task - EN2FR takes the French trans-\nlations (X41 and X42) of X11 and X12 as input.\nY41 and Y42 indicate whether the sequences con-\ntain temporal expressions or not (can be inferred\nfrom Y11 and Y12). Secondary task - EN2FR pro-\nvides indirect knowledge about French temporal\nexpressions. Similarly, Primary task - ES2FR and\nSecondary task - ES2FR transfer from Spanish to\nFrench.\n4\nExperiments\nThis section evaluates the proposed XLTime frame-\nwork. Section 4.1 introduces the datasets, models\nevaluated, metrics, and experimental settings. Sec-\ntion 4.2 quantitatively shows how XLTime allevi-\nates data scarcity and prompts TEE performances.\nSection 4.3 studies the effect of transferring knowl-\nedge from other languages in addition to English.\nWe also qualitatively show how XLTime transfers\nknowledge to the target languages in an error anal-\nysis in Appendix E.\n4.1\nExperimental Setup\nDatasets. We use the English (EN), French (FR),\nSpanish (ES), Portuguese (PT), and Basque (EU)\nTEE benchmark datasets. Table 2 shows dataset\nstatistics. For each target language, we split its\ndataset with 10% for validation and 90% for test.\nFor each source language (applicable to XLTime),\nwe use the whole dataset for training.\nBaselines. We evaluate against rule-based, deep\nlearning-based, and entity projection-based meth-\nods. We compare to the handcrafted HeidelTime\n(Strötgen and Gertz, 2013) and its automatically\nextended version, HeidelTime-auto (Strötgen and\nGertz, 2015).\nWe also compare to deep learn-\ning methods: BiLSTM+CRF (Lange et al., 2020),\nmBERT, base and large versions of XLMR. In ad-\nTable 2: The statistics of the datasets.\nLang\nDataset\nDomain\n#Docs\n#Exprs\n#Dates\n#Times\n#Durations\n#Sets\nFR\nBittar et al. (2011)\nNews\n108\n425\n227\n130\n52\n16\nES\nUzZaman et al. (2013)\nNews\n175\n1, 094\n749\n57\n251\n37\nPT\nCosta and Branco (2012)\nNews\n182\n1, 227\n998\n41\n176\n12\nEU\nAltuna et al. (2016)\nNews\n91\n847\n662\n22\n151\n12\nTE3 (UzZaman et al., 2013)\nNews\n276\n1, 830\n1, 471\n34\n291\n34\nEN\nWikiwars (Mazur and Dale, 2010)\nNarrative\n22\n2, 634\n2, 634\n0\n0\n0\nTweets (Zhong et al., 2017)\nUtterance\n942\n1, 128\n717\n173\n200\n38\nModel\nFR\nES\nPT\nEU\nAutomatic Baseline Models\nHeidelTime-auto\n0.55\n0.42\n0.50\n0.17\nBiLSTM+CRF\n0.64\n0.62\n0.64\n0.47\nmBERT\n0.63\n0.62\n0.66\n0.65\nXLMR-base\n0.69\n0.54\n0.63\n0.46\nXLMR-large\n0.75\n0.72\n0.75\n0.70\nProjection Method\nTMP-mBERT\n0.56\n0.23\n0.66\n/\nTMP-XLMRbase\n0.55\n0.23\n0.64\n/\nTMP-XLMRlarge\n0.56\n0.24\n0.65\n/\nTransfer from EN (Ours)\nXLTime-mBERT\n0.73\n0.71\n0.67\n0.76\nXLTime-XLMRbase\n0.78\n0.66\n0.68\n0.71\nXLTime-XLMRlarge\n0.76\n0.72\n0.77\n0.78\nTransfer from EN and others (Ours)\nXLTime-mBERT\n0.80\n0.77\n0.80\n0.77\nXLTime-XLMRbase\n0.82\n0.72\n0.73\n0.79\nXLTime-XLMRlarge\n0.84\n0.75\n0.84\n0.79\nHandcrafted Method\nHeidelTime\n0.86\n0.86\n0.60\n/\nTable 3: Results for Multilingual TEE (Metric: F1).\ndition, we compare to TMP (Jain et al., 2019), a\ncross-lingual label projection method which relies\non machine translation as well as orthographic and\nphonetic similarity packages (unavailable for EU).\nOur Approaches. We test several variants of our\nproposed model, which can be broken into two\nclasses: 1) Cross-lingual transfer from EN. We\napply XLTime on mBERT, base and large versions\nof XLMR and use EN as the only source language.\n2) Cross-lingual transfer from EN and others. We\ntransfer from other languages in addition to EN.\nEvaluation Metrics. We report F1in strict match\n(UzZaman et al., 2013), i.e., all its tokens must\nbe correctly recognized for an expression to be\ncounted as correctly extracted.\nWe follow the setting in prior work of evaluating\n“without type” and report the results without con-\nsidering the types of the temporal expressions (e.g.,\nfor ‘see you tomorrow’, a prediction such as ‘O O\nB-Duration’ would be counted as correct, though\nthe proper labeling would be ‘O O B-Date’).2\n2We do note that the temporal expression ﬁeld should\nExperimental Setting. We set d, the embedding\ndimension, to be consistent with the pre-trained\nmultilingual backbone’s dimension (768 for the\nbase version language models and 1024 for large\nversions). We use AdamW (Loshchilov and Hut-\nter, 2019) with a learning rate of 7e−6 and warm-\nup proportion of 0.1. We train the models for 50\nepochs and use the best model as indicated by\nthe validation set for prediction. All datasets are\ntransformed into IOB2 format to ﬁt the sequence\nlabeling setting. All the deep learning methods\nare trained on English TEE datasets, validated and\nevaluated on low-resource languages. For BiL-\nSTM+CRF, we use the hyperparameters as sug-\ngested in the original paper (Lange et al., 2020).\nFor TMP, we use it to project the English dataset to\nthe target languages, take the projected data to train\nthe language models, then validate and evaluate on\nthe target languages. We perform a grid search over\n{0.05, 0.1, 0.15, 0.25, 0.5} to tune δ, the similarity\nscore threshold of TMP, and present the best per-\nformance. We repeat all experiments for 5 times\nand report the mean result. All experiments are\nconducted on a 64 core Intel Xeon CPU E5-2680\nv4@2.40GHz with 512GB RAM and 1×NVIDIA\nTesla P100-PICE GPU.\n4.2\nMultilingual TEE\nWe evaluate XLTime on multilingual TEE (see Ta-\nble 3 and Appendix D). We observe: 1) XLTime-\nXLMRlarge outperforms the strongest automatic\nbaseline by up to 9% in F1 on all languages. It even\noutperforms the handcrafted HeidelTime method\nby a sizable margin (24% in F1) in PT. 2) Ap-\nplying XLTime improves upon the vanilla lan-\nguage models, even when transferring knowledge\nultimately evaluate on the more complex task of identifying\ntemporal expressions as well as their types. This is in the\nspirit of the annotations and is in line with other sequence\nlabeling tasks, such as NER. Therefore, we also experiment\nwith the “with type” setting and show results in Appendix C.\nIn both settings, the observations made in Sections 4.2 and 4.3\nhold and XLTime outperforms the previous SOTAs by large\nmargins.\nTarget Language\nFR\nES\nSource Language(s)\nEN\nEN, EU\nEN, PT\nEN, ES\nEN\nEN, EU\nEN, PT\nEN, FR\nXLTime-mBERT\n0.73\n0.76\n0.72\n0.80\n0.71\n0.72\n0.72\n0.77\nXLTime-XLMRbase\n0.78\n0.76\n0.78\n0.82\n0.66\n0.68\n0.71\n0.72\nXLTime-XLMRlarge\n0.76\n0.81\n0.80\n0.84\n0.72\n0.72\n0.75\n0.73\nTarget Language\nPT\nEU\nSource Language(s)\nEN\nEN, FR\nEN, ES\nEN, EU\nEN\nEN, PT\nEN, ES\nEN, FR\nXLTime-mBERT\n0.67\n0.80\n0.70\n0.80\n0.76\n0.73\n0.75\n0.77\nXLTime-XLMRbase\n0.68\n0.73\n0.63\n0.56\n0.71\n0.74\n0.75\n0.79\nXLTime-XLMRlarge\n0.77\n0.82\n0.84\n0.74\n0.78\n0.79\n0.79\n0.77\nTable 4: Low-resource language TEE with additional source languages (F1 scores). The blue cells are expected to, while the\nunderlined cells actually outperform (by ≥4%) using EN as the only source language.\nonly from EN. E.g., XLTime-XLMRbase outper-\nforms XLMR-base by 13%, 22%, 8%, and 54%\nin F1 on FR, ES, PT, and EU. 3) Introducing ad-\nditional source languages to XLTime further im-\nproves the performance: the F1 improves by up to\n19%, 11%, and 11% for XLTime-mBERT, XLTime-\nXLMRbase, and XLTime-XLMRlarge.\n4) Hei-\ndelTime is a very hard baseline to beat given the\ntime and care that went into developing language-\nspeciﬁc rules. However, XLTime approaches its\nperformance for FR and ES, outperforms it for\nPT, and makes predictions for EU (where Heidel-\nTime has no rules). Note the previous automatic\nSOTA, XLMR-large, also outperforms HeidelTime\nfor PT, but not as signiﬁcantly. This shows that\nthe automatic methods are increasingly promis-\ning for the non-English TEE task. 5) XLTime-\nXLMRlarge improves upon XLMR-large by a large\nmargin (11% in F1) in EU. For FR, ES, and PT,\nthe improvements are smaller. This may because\nXLMR-large, compared to mBERT and XLMR-\nbase, is already very knowledgeable (especially\nin FR, ES, and PT, which are more common than\nEU). Therefore, applying XLTime may not provide\nmuch improvement (in contrast, applying XLTime\non mBERT and XLMR-base dramatically boosts\nF1 by 8-54%). 6) TMP performs poorly probably\nbecause the falsely projected entities can mislead\nthe language models. Speciﬁcally, the token-by-\ntoken machine translation and matching process\nof TMP does not work well for temporal entities,\nespecially when the target language TEs contain\ndeﬁnite articles, prepositions, etc., that do not have\nexplicit matches in the source language. E.g., EN\nTE ‘yesterday morning’ can be correctly map to FR\nTE ‘hier matin’ (’yesterday’ to ‘hier’ and ‘morning’\nto ‘Matin’) but not to EU TE ‘ayer por la mañana’\n(’yesterday’ to ‘ayer’ and ‘morning’ to ‘Mañana’,\nleaving ‘por’ and ‘la’ unmatched).\n4.3\nTransfer Knowledge from Additional\nLanguages\nWe also study the effect of transferring additional\nknowledge from a low-resource language in addi-\ntion to English, see Table 4 and Appendix D. Our\nassumption is that similar languages (FR, ES, and\nPT) would help each other (one exception is PT, as\nthe published dataset is EN text translated to PT\nand we, therefore, don’t expect machine translation\nto provide additional knowledge). We observe: 1)\nIn most cases, transferring additional knowledge\nfrom similar languages (blue cells) does dramati-\ncally improve performance (underlined cells), with\nF1 increasing by up to 13%. 2) In some rare cases,\nnegative knowlege transfer (Wu et al., 2020) oc-\ncurs as adding source languages hurts performance\n(e.g., EN, ES →PT scores lower than EN →PT\nfor XLTime-XLMRbase). We hypothesize this is\nrelated to the quality of the datasets and plan to\naddress this in the future.\n5\nConclusion\nWe propose XLTime for multilingual language\nTEE in low-resource scenarios. It is based on lan-\nguage models and leverages MTL to prompt cross-\nlanguage knowledge transfer. It greatly alleviates\nthe problems caused by the shortage in training\ndata and shows results superior to the previous au-\ntomatic SOTA methods on four languages. It also\napproaches the performance of a highly engineered\nrule-based system.\nAcknowledgements\nThis work is supported in part by NSF under grants\nIII-1763325, III-1909323, III-2106758, SaTC-\n1930941 and the S&T Program of Hebei through\ngrant 21340301D. For any correspondence, please\nrefer to Hao Peng.\nReferences\nBegoña\nAltuna,\nMaría\nJesús\nAranzabe,\nand\nArantza Díaz de Ilarraza. 2016.\nAdapting timeml\nto basque: Event annotation.\nIn Proceedings of\nCICLing 2016, pages 565–577. Springer.\nAndré Bittar, Pascal Amsili, Pascal Denis, and Lau-\nrence Danlos. 2011.\nFrench timebank:\nan iso-\ntimeml annotated reference corpus. In Proceedings\nof ACL-HLT 2011, pages 130–134.\nAngel X Chang and Christopher D Manning. 2012. Su-\ntime: A library for recognizing and normalizing time\nexpressions. In Lrec, volume 3735, page 3740.\nSanxing Chen, Guoxin Wang, and Börje Karlsson.\n2019. Exploring word representations on time ex-\npression recognition. Technical report, Tech. rep.,\nMicrosoft Research Asia.\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-\ntau Yih, Yejin Choi, Percy Liang, and Luke Zettle-\nmoyer. 2018. Quac: Question answering in context.\nIn Proceedings of EMNLP 2021.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of ACL 2020, pages 8440–8451.\nFrancisco Costa and António Branco. 2012.\nTime-\nbankpt: A timeml annotated corpus of portuguese.\nIn LREC, volume 12, pages 3727–3734.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nIn Proceedings of NAACL-HLT 2019, pages\n4171–4186.\nWentao Ding, Guanji Gao, Linfeng Shi, and Yuzhong\nQu. 2019. A pattern-based approach to recognizing\ntime expressions. In Proceedings of AAAI 2019, vol-\nume 33, pages 6335–6342.\nAmir Feder, Katherine A. Keith, Emaad Manzoor, Reid\nPryzant, Dhanya Sridhar, Zach Wood-Doughty, Ja-\ncob Eisenstein, Justin Grimmer, Roi Reichart, Mar-\ngaret E. Roberts, Brandon M. Stewart, Victor Veitch,\nand Diyi Yang. 2021. Causal inference in natural\nlanguage processing: Estimation, prediction, inter-\npretation and beyond.\nAlankar Jain, Bhargavi Paranjape, and Zachary C. Lip-\nton. 2019.\nEntity projection via machine transla-\ntion for cross-lingual NER. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1083–1092, Hong Kong,\nChina. Association for Computational Linguistics.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of NAACL-HLT 2016, pages 260–\n270.\nLukas Lange, Anastasiia Iurshina, Heike Adel, and Jan-\nnik Strötgen. 2020. Adversarial alignment of multi-\nlingual models for extracting temporal expressions\nfrom text. In Proceedings of Workshop on Represen-\ntation Learning for NLP at ACL 2020, pages 103–\n109.\nEgoitz Laparra, Dongfang Xu, and Steven Bethard.\n2018.\nFrom characters to time intervals:\nNew\nparadigms for evaluation and neural parsing of time\nnormalizations. Transactions of the Association for\nComputational Linguistics, 6:343–356.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019a. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof ACL 2019, pages 4487–4496.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019.\nDecoupled\nweight decay regularization.\nIn Proceedings of\nICLR 2019.\nPawel Mazur and Robert Dale. 2010. Wikiwars: A new\ncorpus for research on temporal expressions. In Pro-\nceedings of EMNLP 2010, pages 913–922.\nBhaskar Mitra, Nick Craswell, et al. 2018. An intro-\nduction to neural information retrieval. Now Foun-\ndations and Trends.\nJames Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-\nrent Romary. 2010.\nIso-timeml: An international\nstandard for semantic annotation.\nIn LREC, vol-\nume 10, pages 394–397.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21:1–67.\nLance A Ramshaw and Mitchell P Marcus. 1999. Text\nchunking using transformation-based learning.\nIn\nNatural language processing using very large cor-\npora, pages 157–176. Springer.\nJannik Strötgen and Michael Gertz. 2013. Multilingual\nand cross-domain temporal tagging. Language Re-\nsources and Evaluation, 47(2):269–298.\nJannik Strötgen and Michael Gertz. 2015. A baseline\ntemporal tagger for all languages. In Proceedings of\nEMNLP 2015, pages 541–547.\nNaushad UzZaman, Hector Llorens, Leon Derczyn-\nski, James Allen, Marc Verhagen, and James Puste-\njovsky. 2013.\nSemeval-2013 task 1: Tempeval-3:\nEvaluating time expressions, events, and temporal\nrelations.\nIn Second Joint Conference on Lexical\nand Computational Semantics, Volume 2: Proceed-\nings of SemEval 2013, pages 1–9.\nSen Wu, Hongyang R Zhang, and Christopher Ré.\n2020.\nUnderstanding and improving information\ntransfer in multi-task learning.\nIn Proceedings of\nICLR 2020.\nXiaoshi Zhong, Aixin Sun, and Erik Cambria. 2017.\nTime expression analysis and recognition using syn-\ntactic token types and general heuristic rules. In Pro-\nceedings of ACL 2017, pages 420–429.\nA\nTypes of the Temporal Expressions\nAccording to ISO-TimeML (Pustejovsky et al.,\n2010), the TEE dataset annotation guideline, there\nare four types of temporal expressions, i.e., Date,\nTime, Duration, and Set. Date refers to a calendar\ndate, generally of a day or a larger temporal unit;\nTime refers to a time of the day and the granularity\nof which is smaller than a day; Duration refers to\nthe expressions that explicitly describe some period\nof time; Set refers to a set of regularly recurring\ntimes (Pustejovsky et al., 2010).\nAlgorithm 1: Training XLTime\n1 // Initialize model.\n2 Load the parameters from a pre-trained\nmultilingual model.\n3 Initialize W and W′ randomly.\n4 // Prepare task data.\n5 for t in {primary, secondary} do\n6\nSplit the data of task t into\nmini-batches Bt\n7 B = Bprimary ∪Bsecondary\n8 for e in 1, ..., epoch do\n9\nRandomly shufﬂe B\n10\n// bt is a mini-batch of task t\n11\nfor bt in B do\n12\nif t is a primary task then\n13\nLsl = Equation 1\n14\nelse\n15\nLbc = Equation 2\n16\nCompute gradient and update\nmodel parameters\nB\nThe Training Procedure\nWe adopt mini-batch-based stochastic gradient de-\nscent (SGD) to train XLTime, as shown in Algo-\nrithm 1. To concurrently train on the primary and\nsecondary tasks, we split the training data of both\ntasks into mini-batches and randomly take one\nmini-batch at each step. We then calculate loss\nusing that mini-batch and update the parameters\nof the shared backbone as well as the task-type-\nspeciﬁc classiﬁer. The classiﬁer of the other task\ntype is unaffected.\nC\nFull Results for Low-resource\nLanguage TEE\nTable 7 shows the full results for low-resource lan-\nguage TEE with/without considering the types of\nthe temporal expressions. Note that the superiority\nof our proposed XLTime over the previous auto-\nmatic SOTA still holds.\nD\nFull Results for Low-resource\nLanguage TEE with Additional\nSource Languages\nTables 8 and 9 show the full results for low-resource\nlanguage TEE with additional source languages.\nE\nComparative Error Analysis\nThis section qualitatively shows how the proposed\nXLTime framework transfers knowledge to the tar-\nget language. Speciﬁcally, we show how the errors\nmade by the vanilla multilingual models can be\nﬁxed by applying XLTime. We also show how ap-\nplying XLTime on other languages in addition to\nEnglish would help ﬁx more errors.\nWe compare mBERT and XLTime-mBERT\n(transfer from EN) on FR TEE. Table 5 summarizes\ncases where mBERT fails while XLTime-mBERT\ngives correct predictions. We can tell that XLTime-\nmBERT learns ‘hier (yesterday)’, which is not un-\nderstood by the mBERT model. XLTime-mBERT\nalso learns to recognize vague time spans such as\n‘désormais (from now on)’ and ‘longtemps (long\ntime)’, which are missed by the mBERT model.\nMoreover, compared to mBERT, XLTime-mBERT\nunderstands FR grammar better, as it recognizes\nthe roles of deﬁnite articles and adjectives, such as\n‘le (the)’ and ‘prochain (next)’, in TEs. In a word,\nthe proposed XLTime framework helps connect the\nconcepts in EN to the corresponding ones in FR.\nTo show how applying XLTime on extra source\nlanguages would help ﬁx more errors, we compare\nXLTime-mBERT (transfer from EN) and XLTime-\nmBERT (transfer from EN and ES) on FR TEE.\nTable 6 summarizes the TEs that the former fails\nwhile the latter gives correct predictions.\nWe\ncan tell that by leveraging ES as an additional\nsource language, XLTime-mBERT better masters\nFR grammar. Speciﬁcally, it learns to recognize\ndeﬁnite articles and prepositions that share similar\n(e.g., ‘le/los’) or identical (e.g., ‘de’ and ‘en’) forms\nin ES and FR. It can also better distinguish TEs of\ndifferent types (e.g., it learns that ‘quelques jours\nTable 5: mBERT vs. XLTime-mBERT (transfer from EN) frequent (count ≥10) errors.\nError Desc.\nFR TEs\nEN translations\nmBERT results (wrong)\nXLTime results (correct)\ncounts\nfail to recognize\n‘hier (yesterday)’\nhier soir\nlast night\nO B-TIME\nB-TIME I-TIME\n30\nhier\nyesterday\nO\nB-DATE\nfail to recognize\nvague time span\ndésormais\nfrom now on\nO\nB-DATE\n6\nlongtemps\nlong time\nO\nB-DURATION\ntoute l’année\nall year\nO O\nB-SET I-SET\nfail to recognize\ndeﬁnite articles\nand adjectives\nle 3 août\nAugust 3\nO B-DATE I-DATE\nB-DATE I-DATE I-DATE\n10\nla nuit\nthe night\nO O\nB-TIME I-TIME\nlundi prochain\nnext Monday\nB-DATE O\nB-DATE I-DATE\nTable 6: XLTime-mBERT (transfer from EN) vs. XLTime-mBERT (transfer from EN and ES) frequent (count ≥8) errors.\nError Desc.\nFR TEs\nEN translations\nEN results (wrong)\nEN and ES results (correct)\ncounts\nfail to recognize\ndeﬁnite articles\nand prepositions\nen été\nin summer\nB-DATE I-DATE\nO B-DATE\n20\nle 13 février\nFebruary 13\nO B-DATE I-DATE\nB-DATE I-DATE I-DATE\nde dimanche\nof Sunday\nB-DATE I-DATE\nO B-DATE\nwrong token\ntypes\nmardi\nTuesday\nB-TIME\nB-DATE\n18\nquelques jours\nA few days\nB-DATE I-DATE\nB-DURATION I-DURATION\nrecognized extra\nTEs\nquotidiens\ndaily\nB-SET\nO\n8\nla saison\nthe season\nB-DATE I-DATE\nO O\n(a few days)’ is a Duration, instead of a Date).\nOne interesting fact is, when transferring solely\nfrom EN, the model recognizes some extra TEs\nthat are not in the ground truth of the FR dataset.\nThis is because of an inconsistency in data label-\ning: ‘daily’ is considered as a Set in the EN dataset,\nwhile its counterpart, ‘quotidiens’ is overlooked in\nthe FR dataset. The proposed XLTime framework\neliminates the needs of manually labeling multiple\ndatasets and therefore, can be applied to minimize\ndata label inconsistency.\nF\nLanguage Models on English TEE\nIn our early experiments, we reexamine the lan-\nguage models on English TEE. This section\npresents the results.\nF.1\nExperimental Setup\nWe study BERT (Devlin et al., 2019) and XLMR\n(Conneau et al., 2020) variants, RoBERTa (Liu\net al., 2019b) and T5 Encoder (Raffel et al., 2019).\nWe compare them to rule-based methods including\nHeidelTime (Strötgen and Gertz, 2013), SynTime\n(Zhong et al., 2017), and PTime (Ding et al., 2019),\nwhich report SOTA performances on Wikiwars,\nTE3, and Tweets, respectively. We experiment on\nboth settings, i.e., “with type\" and “without type\",\nand report F1, precision, and recall in strict match\n(UzZaman et al., 2013). We use the data splits\nfollowing Ding et al. (2019) and the experimental\nsettings introduced in Section 4.1.\nF.2\nEvaluation Results\nTables 10, 11, and 12 show the results. We observe:\n1) When ignoring the types, the language models\nare inferior to SynTime on TE3, on par with or\nbetter than the rule-based methods on Wikiwars\nand Tweets. 2) When considering the types, the\nlanguage models outperform the previous SOTAs\nby 11-22%, 18-21%, and 30-41% in F1 on TE3,\nWikiwars, and Tweets datasets.\nTable 7: Multilingual TEE results (w/ type | w/o type).\nw/ type\nFR\nES\nPT\nEU\nF1\nPr.\nRe.\nF1\nPr.\nRe.\nF1\nPr.\nRe.\nF1\nPr.\nRe.\nAutomatic Baseline Models\nHeidelTime-auto\n0.53\n0.63\n0.46\n0.41\n0.56\n0.32\n0.49\n0.66\n0.39\n0.15\n0.60\n0.09\nBiLSTM+CRF\n0.58\n0.64\n0.51\n0.56\n0.61\n0.51\n0.58\n0.59\n0.58\n0.44\n0.54\n0.37\nmBERT\n0.56\n0.61\n0.51\n0.56\n0.62\n0.51\n0.60\n0.56\n0.64\n0.59\n0.64\n0.55\nXLMR-base\n0.64\n0.69\n0.59\n0.51\n0.58\n0.46\n0.59\n0.59\n0.59\n0.43\n0.60\n0.34\nXLMR-large\n0.69\n0.70\n0.68\n0.68\n0.71\n0.66\n0.71\n0.69\n0.73\n0.66\n0.70\n0.63\nProjection Method\nTMP-mBERT\n0.50\n0.56\n0.45\n0.23\n0.59\n0.14\n0.60\n0.57\n0.64\n/\n/\n/\nTMP-XLMRbase\n0.50\n0.60\n0.43\n0.23\n0.57\n0.14\n0.61\n0.58\n0.64\n/\n/\n/\nTMP-XLMRlarge\n0.52\n0.61\n0.46\n0.24\n0.59\n0.15\n0.61\n0.58\n0.63\n/\n/\n/\nTransfer from EN (Ours)\nXLTime-mBERT\n0.62\n0.62\n0.62\n0.65\n0.70\n0.61\n0.61\n0.58\n0.66\n0.68\n0.72\n0.65\nXLTime-XLMRbase\n0.67\n0.67\n0.68\n0.60\n0.63\n0.58\n0.64\n0.62\n0.66\n0.64\n0.68\n0.60\nXLTime-XLMRlarge\n0.71\n0.74\n0.68\n0.70\n0.76\n0.65\n0.74\n0.71\n0.78\n0.72\n0.79\n0.66\nTransfer from EN and others (Ours)\nXLTime-mBERT\n0.71\n0.69\n0.73\n0.68\n0.69\n0.66\n0.73\n0.70\n0.76\n0.68\n0.72\n0.65\nXLTime-XLMRbase\n0.70\n0.67\n0.74\n0.65\n0.69\n0.62\n0.66\n0.64\n0.68\n0.70\n0.76\n0.65\nXLTime-XLMRlarge\n0.75\n0.72\n0.78\n0.70\n0.76\n0.65\n0.81\n0.79\n0.84\n0.74\n0.79\n0.69\nHandcrafted Method\nHeidelTime\n0.80\n0.81\n0.79\n0.85\n0.90\n0.80\n0.57\n0.60\n0.53\n/\n/\n/\nw/o type\nFR\nES\nPT\nEU\nF1\nPr.\nRe.\nF1\nPr.\nRe.\nF1\nPr.\nRe.\nF1\nPr.\nRe.\nAutomatic Baseline Models\nHeidelTime-auto\n0.55\n0.65\n0.47\n0.42\n0.58\n0.33\n0.50\n0.67\n0.39\n0.17\n0.66\n0.10\nBiLSTM+CRF\n0.64\n0.73\n0.57\n0.62\n0.68\n0.56\n0.64\n0.66\n0.63\n0.47\n0.58\n0.40\nmBERT\n0.63\n0.70\n0.58\n0.62\n0.69\n0.56\n0.66\n0.63\n0.69\n0.65\n0.71\n0.60\nXLMR-base\n0.69\n0.75\n0.64\n0.54\n0.61\n0.48\n0.63\n0.64\n0.62\n0.46\n0.64\n0.36\nXLMR-large\n0.75\n0.78\n0.73\n0.72\n0.75\n0.69\n0.75\n0.74\n0.76\n0.70\n0.74\n0.67\nProjection Method\nTMP-mBERT\n0.56\n0.63\n0.50\n0.23\n0.59\n0.14\n0.66\n0.64\n0.69\n/\n/\n/\nTMP-XLMRbase\n0.55\n0.67\n0.47\n0.23\n0.57\n0.14\n0.64\n0.61\n0.67\n/\n/\n/\nTMP-XLMRlarge\n0.56\n0.66\n0.50\n0.24\n0.59\n0.15\n0.65\n0.61\n0.68\n/\n/\n/\nTransfer from EN (Ours)\nXLTime-mBERT\n0.73\n0.73\n0.72\n0.71\n0.77\n0.66\n0.67\n0.64\n0.71\n0.76\n0.81\n0.71\nXLTime-XLMRbase\n0.78\n0.79\n0.78\n0.66\n0.70\n0.63\n0.68\n0.67\n0.70\n0.71\n0.76\n0.66\nXLTime-XLMRlarge\n0.76\n0.79\n0.73\n0.72\n0.79\n0.67\n0.77\n0.74\n0.81\n0.78\n0.85\n0.71\nTransfer from EN and others (Ours)\nXLTime-mBERT\n0.80\n0.77\n0.82\n0.77\n0.79\n0.74\n0.80\n0.77\n0.83\n0.77\n0.82\n0.72\nXLTime-XLMRbase\n0.82\n0.79\n0.86\n0.72\n0.78\n0.68\n0.73\n0.72\n0.75\n0.79\n0.86\n0.73\nXLTime-XLMRlarge\n0.84\n0.82\n0.86\n0.75\n0.79\n0.71\n0.84\n0.82\n0.87\n0.79\n0.84\n0.74\nHandcrafted Method\nHeidelTime\n0.86\n0.87\n0.85\n0.86\n0.91\n0.81\n0.60\n0.64\n0.57\n/\n/\n/\nTable 8: Low-resource language TEE with additional source languages (F1, precision, and recall scores w/ type). The blue cells\nare expected to, while the underlined cells actually outperform (by ≥3%) using EN as the only source language.\nF1\nTarget Language\nFR\nES\nSource Language(s)\nEN\nEN, EU\nEN, PT\nEN, ES\nEN\nEN, EU\nEN, PT\nEN, FR\nXLTime-mBERT\n0.62\n0.61\n0.61\n0.71\n0.65\n0.66\n0.65\n0.68\nXLTime-XLMRbase\n0.67\n0.67\n0.66\n0.70\n0.60\n0.61\n0.64\n0.65\nXLTime-XLMRlarge\n0.71\n0.73\n0.73\n0.75\n0.70\n0.68\n0.69\n0.68\nTarget Language\nPT\nEU\nSource Language(s)\nEN\nEN, FR\nEN, ES\nEN, EU\nEN\nEN, PT\nEN, ES\nEN, FR\nXLTime-mBERT\n0.61\n0.72\n0.59\n0.73\n0.68\n0.66\n0.66\n0.68\nXLTime-XLMRbase\n0.64\n0.66\n0.55\n0.52\n0.64\n0.66\n0.66\n0.70\nXLTime-XLMRlarge\n0.74\n0.79\n0.81\n0.71\n0.72\n0.71\n0.74\n0.72\nPrecision\nTarget Language\nFR\nES\nSource Language(s)\nEN\nEN, EU\nEN, PT\nEN, ES\nEN\nEN, EU\nEN, PT\nEN, FR\nXLTime-mBERT\n0.62\n0.59\n0.62\n0.69\n0.70\n0.69\n0.71\n0.69\nXLTime-XLMRbase\n0.67\n0.66\n0.67\n0.67\n0.63\n0.64\n0.67\n0.69\nXLTime-XLMRlarge\n0.74\n0.72\n0.76\n0.72\n0.76\n0.65\n0.73\n0.68\nTarget Language\nPT\nEU\nSource Language(s)\nEN\nEN, FR\nEN, ES\nEN, EU\nEN\nEN, PT\nEN, ES\nEN, FR\nXLTime-mBERT\n0.58\n0.68\n0.56\n0.70\n0.72\n0.70\n0.69\n0.72\nXLTime-XLMRbase\n0.62\n0.64\n0.51\n0.49\n0.68\n0.73\n0.69\n0.76\nXLTime-XLMRlarge\n0.71\n0.75\n0.79\n0.68\n0.79\n0.75\n0.79\n0.79\nRecall\nTarget Language\nFR\nES\nSource Language(s)\nEN\nEN, EU\nEN, PT\nEN, ES\nEN\nEN, EU\nEN, PT\nEN, FR\nXLTime-mBERT\n0.62\n0.62\n0.59\n0.73\n0.61\n0.64\n0.60\n0.66\nXLTime-XLMRbase\n0.68\n0.67\n0.64\n0.74\n0.58\n0.59\n0.61\n0.62\nXLTime-XLMRlarge\n0.68\n0.73\n0.71\n0.78\n0.65\n0.71\n0.65\n0.67\nTarget Language\nPT\nEU\nSource Language(s)\nEN\nEN, FR\nEN, ES\nEN, EU\nEN\nEN, PT\nEN, ES\nEN, FR\nXLTime-mBERT\n0.66\n0.75\n0.62\n0.76\n0.65\n0.63\n0.64\n0.64\nXLTime-XLMRbase\n0.66\n0.68\n0.60\n0.55\n0.60\n0.60\n0.63\n0.65\nXLTime-XLMRlarge\n0.78\n0.83\n0.84\n0.74\n0.66\n0.67\n0.69\n0.67\nTable 9: Low-resource language TEE with additional source languages (precision and recall scores w/o type). The blue cells\nare expected to, while the underlined cells actually outperform (by ≥4%) using EN as the only source language.\nPrecision\nTarget Language\nFR\nES\nSource Language(s)\nEN\nEN, EU\nEN, PT\nEN, ES\nEN\nEN, EU\nEN, PT\nEN, FR\nXLTime-mBERT\n0.73\n0.76\n0.76\n0.77\n0.77\n0.76\n0.79\n0.79\nXLTime-XLMRbase\n0.79\n0.77\n0.81\n0.79\n0.70\n0.72\n0.75\n0.78\nXLTime-XLMRlarge\n0.79\n0.81\n0.84\n0.82\n0.79\n0.70\n0.79\n0.74\nTarget Language\nPT\nEU\nSource Language(s)\nEN\nEN, FR\nEN, ES\nEN, EU\nEN\nEN, PT\nEN, ES\nEN, FR\nXLTime-mBERT\n0.64\n0.77\n0.67\n0.77\n0.81\n0.78\n0.79\n0.82\nXLTime-XLMRbase\n0.67\n0.72\n0.60\n0.54\n0.76\n0.82\n0.79\n0.86\nXLTime-XLMRlarge\n0.74\n0.79\n0.82\n0.72\n0.85\n0.85\n0.84\n0.84\nRecall\nTarget Language\nFR\nES\nSource Language(s)\nEN\nEN, EU\nEN, PT\nEN, ES\nEN\nEN, EU\nEN, PT\nEN, FR\nXLTime-mBERT\n0.72\n0.77\n0.69\n0.82\n0.66\n0.69\n0.66\n0.74\nXLTime-XLMRbase\n0.78\n0.76\n0.75\n0.86\n0.63\n0.64\n0.68\n0.68\nXLTime-XLMRlarge\n0.73\n0.81\n0.77\n0.86\n0.67\n0.75\n0.71\n0.72\nTarget Language\nPT\nEU\nSource Language(s)\nEN\nEN, FR\nEN, ES\nEN, EU\nEN\nEN, PT\nEN, ES\nEN, FR\nXLTime-mBERT\n0.71\n0.83\n0.74\n0.83\n0.71\n0.69\n0.70\n0.72\nXLTime-XLMRbase\n0.70\n0.75\n0.66\n0.59\n0.66\n0.67\n0.70\n0.73\nXLTime-XLMRlarge\n0.81\n0.87\n0.87\n0.77\n0.71\n0.74\n0.74\n0.71\nTable 10: Supervised English TEE on TE3 (w/ type | w/o\ntype).\nF1\nPr.\nRe.\nRule-based Models\nHeidelTime\n0.77| 0.81\n0.80| 0.84\n0.75| 0.79\nSynTime\n0.65| 0.92\n0.65| 0.91\n0.66| 0.93\nPTime\n0.67| 0.85\n0.68| 0.88\n0.65| 0.83\nLanguage Models\nBERT-base\n0.76| 0.82\n0.78| 0.85\n0.74| 0.80\nBERT-large\n0.79| 0.83\n0.77| 0.82\n0.80| 0.84\nmBERT\n0.79| 0.84\n0.80| 0.86\n0.77| 0.82\nRoBERTa\n0.78| 0.84\n0.79| 0.86\n0.77| 0.82\nXLMR-base\n0.79| 0.81\n0.80| 0.82\n0.77| 0.81\nXLMR-large\n0.78| 0.81\n0.78| 0.82\n0.78| 0.81\nT5Encoder\n0.79| 0.82\n0.82| 0.85\n0.78| 0.80\nTable 11: Supervised English TEE on Wikiwars (w/ type |\nw/o type).\nF1\nPr.\nRe.\nRule-based Models\nHeidelTime\n0.80| 0.85\n0.86| 0.92\n0.75| 0.80\nSynTime\n0.79| 0.79\n0.79| 0.79\n0.79| 0.79\nPTime\n0.86| 0.86\n0.87| 0.87\n0.86| 0.86\nLanguage Models\nBERT-base\n0.94| 0.94\n0.95| 0.95\n0.94| 0.94\nBERT-large\n0.95| 0.95\n0.94| 0.94\n0.96| 0.96\nmBERT\n0.97| 0.97\n0.96| 0.96\n0.97| 0.97\nRoBERTa\n0.95| 0.95\n0.94| 0.94\n0.97| 0.97\nXLMR-base\n0.97| 0.97\n0.95| 0.95\n0.98| 0.98\nXLMR-large\n0.96| 0.96\n0.94| 0.94\n0.97| 0.97\nT5Encoder\n0.96| 0.96\n0.95| 0.95\n0.97| 0.97\nTable 12: Supervised English TEE on Tweets (w/ type | w/o\ntype).\nF1\nPr.\nRe.\nRule-based Models\nHeidelTime\n0.80| 0.80\n0.90| 0.90\n0.72| 0.72\nSynTime\n0.63| 0.92\n0.62| 0.91\n0.65| 0.95\nPTime\n0.66| 0.95\n0.65| 0.94\n0.67| 0.96\nLanguage Models\nBERT-base\n0.92| 0.94\n0.90| 0.93\n0.93| 0.95\nBERT-large\n0.86| 0.92\n0.84| 0.92\n0.88| 0.92\nmBERT\n0.87| 0.91\n0.85| 0.88\n0.90| 0.94\nRoBERTa\n0.91| 0.95\n0.89| 0.93\n0.94| 0.97\nXLMR-base\n0.90| 0.94\n0.87| 0.92\n0.93| 0.97\nXLMR-large\n0.93| 0.95\n0.91| 0.93\n0.95| 0.96\nT5Encoder\n0.87| 0.93\n0.84| 0.91\n0.91| 0.95\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2022-05-03",
  "updated": "2022-05-03"
}