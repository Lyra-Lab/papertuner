{
  "id": "http://arxiv.org/abs/2205.03504v1",
  "title": "Reinforcement Learning Approach to Estimation in Linear Systems",
  "authors": [
    "Minyue Fu"
  ],
  "abstract": "This paper addresses two important estimation problems for linear systems,\nnamely system identification and model-free state estimation. Our focus is on\nARMAX models with unknown parameters. We first provide a reinforcement learning\nalgorithm for system identification with guaranteed consistency. This algorithm\nis then used to provide a novel solution to model-free state estimation. These\nresults are then applied to solving the model-free LQG control problem in the\nreinforcement learning setting.",
  "text": "arXiv:2205.03504v1  [eess.SY]  6 May 2022\n1\nReinforcement Learning Approach to Estimation in\nLinear Systems\nMinyue Fu1\nAbstract—This paper addresses two important estimation\nproblems for linear systems, namely system identiﬁcation and\nmodel-free state estimation. Our focus is on ARMAX models with\nunknown parameters. We ﬁrst provide a reinforcement learning\nalgorithm for system identiﬁcation with guaranteed consistency.\nThis algorithm is then used to provide a novel solution to model-\nfree state estimation. These results are then applied to solving the\nmodel-free LQG control problem in the reinforcement learning\nsetting.\nIndex Terms—Reinforcement learning, system identiﬁcation,\nmodel-free state estimation, model-free control design.\nI. INTRODUCTION\nIt is well known that system identiﬁcation and state estima-\ntion are closely related learning problems for dynamic systems,\nwith a rich history of research and rich set of methodologies;\nsee, e.g., classical monographs [1], [2] for the former and [3],\n[4] for the latter. The task of system identiﬁcation is to estimate\nthe system parameters, whereas that of state estimation is to\nprovide an estimate of the state for a given system model.\nThe main motivation for this paper is to understand how to\ndo state estimation without a system model. A simple approach\nis, of course, to estimate the system parameters ﬁrst and then\nuse them to estimate the state. But this approach is not suitable\nfor on-line model-free state estimation where the estimates\nneed to be updated recursively (or iteratively) along with the\noutput measurement samples. That is, an online estimation\nalgorithm is preferred. The second motivation for this paper\nis to know whether these estimation problems can be studied\nin the framework of reinforcement learning [5], [6].\nThe system under study is the classical Auto-Regressive\nMoving-Average eXogenous (ARMAX) model with known\norders but unknown parameters. We ﬁrst consider the online\nsystem identiﬁcation problem formulated in the reinforcement\nlearning framework, and the objective is to provide a recursive\n(or iterative) estimate of the system parameters along with the\nupdate of the output measurement. By blending the tools of\ninstrumental variables and bootstrapping, we provide a new\nrecursive learning algorithm that globally optimises a cost\nfunction in the reinforcement learning setting and provides\na convergent and consistent parameter estimate in the system\nidentiﬁcation setting at the same time. We then extend this\nalgorithm to solve the model-free state estimation problem\nunder a similar reinforcement learning setting and give an\nasymptotically optimal state estimate in the Kalman ﬁltering\nsense. The reinforcement learning algorithms for system iden-\ntiﬁcation and state estimation will then be used to solve the\n1School of Electrical Engineering and Computing, The University of\nNewcastle, University Drive, Callaghan, 2308, NSW, Australia.\nE-mail: minyue.fu@newcastle.edu.au.\nclassical linear quadratic Gaussian (LQG) control problem for\nan ARMAX model with unknown parameters. The solution\nis a reinforcement learning algorithm for model-free LQG\ncontrol.\nThe contributions of the paper are summarised below:\n• Reformulation and reinterpretation of the classical system\nidentiﬁcation tools (least-squares, instrumental variables,\nbootstrapping...) in the framework of reinforcement learn-\ning;\n• New recursive parameter estimation algorithm for system\nidentiﬁcation with consistency;\n• Reinforcement learning algorithm for model-free state\nestimation;\n• Application to model-free linear quadratic Gaussian\n(LQG) control.\nII. PROBLEM STATEMENTS\nA. System Model\nIn this paper, we consider a system with the following\nstationary ARMAX model [1]:\nyk + a1yk−1 + . . . + anykn\n= b1uk−1 + . . . bmuk−m + wk + c1wk−1 + . . . cpwk−p, (1)\nwhere uk is the exogenous input, yk is the measured output,\nwk is the process noise, n, m, p are the parameter dimensions\n(orders) which are assumed to be known, ai, bi, ci are system\nparameters which are constant but unknown. The process noise\nis assumed to be Gaussian white noise with zero mean and\nvariance σ2 which is also unknown. The exogenous input is\nknown and assumed to be stationary and independent of the\nprocess noise. The system parameter vector will be denoted\nby θ⋆= [a1 . . . an b1 . . . bm c1 . . . cp]T . The time index k is\nallowed to range from −∞to +∞.\nDenoting the delay operator by z−1, the system model (1)\ncan be rewritten as\na(z)yk = b(z)uk + c(z)wk,\n(2)\nwhere a(z) = 1 + a1z−1 + . . . + anz−n, b(z) = b1z−1 +\n. . . bmz−m and c(z) = 1 + c1z−1 + . . . cpz−p. It is further\nassumed that c(z) is stable (i.e., with all their zeros strictly\ninside the unit circle) and that a(z), b(z) and c(z) do not have\na common factor.\n2\nLemma 1: Under the assumption that n ≥m and n ≥p,\nthe observable-canonical state-space realisation of (2) is given\nby\nxk+1 = Axk + B1uk + B2wk\n=\n\n\n0\n. . .\n0\n−an\n1\n...\n...\n−an−1\n...\n0\n...\n0\n. . .\n1\n−a1\n\n\nxk +\n\n\n0\nbm\n...\nb1\n\nuk +\n\n\n˜cn\n˜cn−1\n...\n˜c1\n\nwk\nyk = Cxk + wk = [0 . . . 0 1]xk + wk,\n(3)\nwhere xk is the state of the system, and ˜ci = ci −ai, i =\n1, 2, . . . , n with the extended cn = . . . = cp+1 = 0.\nSee Appendix A for proof.\nB. Reinforcement Learning\nReinforcement learning (RL) is an iconic tool in machine\nlearning with huge success in applications [5] and has deep\nconnections with the control theory [6]. Consider a system\nxk+1 = f(xk, uk, wk)\nzk = g(xk, uk, wk),\n(4)\nwhere xk is the state, uk is the control input, wk is the process\nnoise, zk is the output known as the cost, f(·) and g(·) are\nunknown functions. Under the assumption that both the state\nand output are measurable, the aim of RL is to design an\noptimal control law uk = π(xk) such that the following total\ncost Jk is minimised:\nJk = E[\n∞\nX\nt=0\nγtzk+t],\n(5)\nwhere 0 < γ < 1 is a forgetting factor. In the standard RL\nterminology, control is called action, control law is called\npolicy, forgetting factor is called the discount factor, −zk is\ncalled the reward, −Jk is called the value function, and (5) is\nequivalent to maximise the value function.\nTo get around of the difﬁculty with unknown f(·) and\ng(·) and unknown structure of feasible policy π(·), total cost\nand policy are parameterised as Jθ\nk and πθ(·) with some\n(high-dimensional) parameter vector θ. These functions are\nthen approximated using neural networks and an iterative\nalgorithm is applied to tune θ, based on the available xk\nand zk sequences (from simulations and/or experiments) such\nthat Jθ\nk is minimised. It is worth noting that apart from\nsome simple cases, RL represents a learning paradigm rather\nthan a guarantee for optimal policies. Important cases where\nthe optimal policy is guaranteed include 1) Markov decision\nprocess (MDP) with ﬁnite numbers of states and actions [5];\n2) linear quadratic regulation (LQR) for state feedback control\nof linear systems [7].\nTwo types of iterative algorithms are most commonly used\nin RL: policy iteration (PI) and value iteration (VI). PI aims\nto improve the policy after each iteration whereas VI focuses\non improving the value function. The key difference between\nPI and VI is the following: In PI, a single policy (known as\non-policy) is used in every time step, whereas in VI, different\npolicies (known as off-policy) can be used in different time\nsteps. This is illustrated in Fig. 1 below. This seemingly\nsubtle difference has a profound inﬂuence on the efﬁciency\nand effectiveness of the algorithm. Namely, in PI, a complete\nevaluation of a new policy needs to be performed before the\nnext iteration, whereas in VI, past value functions evaluated\nbased on old policies can be used in evaluating the new policy,\nmaking VI a much more popular choice in RL.\nPolicy Iteration: Same policy π(i) is used throughout\nValue Iteration: Current and past policies are mixed\n. . .\n✲\nk\nπ(i)\n✲\nk + 1\nπ(i)\n✲\nk + 2\nπ(i)\n✲\nk + 3\nπ(i)\n. . .\n. . .\n✲\nk\nπ(i)\n✲\nk + 1\nπ(i−1)\n✲\nk + 2\nπ(i−2)\n✲\nk + 3\nπ(i−3)\n. . .\nFig. 1. Illustration of Policy Iteration and Value Iteration\nC. RL Formulation of System Identiﬁcation\nWe now formulate the system identiﬁcation problem as a\nreinforcement learning problem. Let\nˆyk = π(y<k, u<k),\n(6)\nbe a (one-step-ahead) predictor of yk, where y<k\n=\n[yk−1, yk−2, . . .] and u<k is similarly deﬁned. The prediction\nerror is given by\nek = yk −ˆyk.\n(7)\nThe total cost is deﬁned to be\nJk = E[\n∞\nX\nt=0\nγte2\nk−t]\n(8)\nfor some discount factor 0 < γ < 1. Notice that this sequence\ngoes backwards in time, and that the initial state is not present\nbecause the sequence of yk starts from k = −∞. The RL\nproblem is to ﬁnd the optimal policy (i.e., predictor) π such\nthat Jk is minimised. We will show later that this formulation\ncoincides with the classical system identiﬁcation problem.\nD. RL Formulation of Optimal State Estimation\nState estimation without a system model has a unique difﬁ-\nculty due to inﬁnite choices of state coordinates. Therefore,\nthe state estimation problem not only needs to provide an\noptimal state estimate, but also to specify the system structure.\nMathematically, we need to determine the following model:\nπ :\nˆxk+1 = ˆf(ˆxk, yk, uk)\nˆyk = ˆg(ˆxk)\n(9)\nwhere ˆxk represents the estimated state, ˆf(·) and ˆg are the un-\nknown functions (i.e., structure and parameters). Collectively,\nˆf(·) and ˆg(·) constitute the policy to be optimised. A “simple”\nchoice for the estimated state is ˆxk = col[y<k, u<k], but this\nis not desirable because its dimension is inﬁnite. It is natural\nthat we want ˆxk to have a ﬁxed ﬁnite dimension.\n3\nThe RL formulation for model-free state estimation is to\nﬁnd the optimal π in (9) such that the total cost Jk in (8) is\nminimised. Again, we will show later that this formulation is\nconsistent with the classical Kalman ﬁltering problem.\nIII. SYSTEM IDENTIFICATION\nThis section solves the RL problem for system identiﬁca-\ntion.\nWe ﬁrst make a simple observation that the discount factor\ndoes not play any role and that the problem formulation (8)\ncan be simpliﬁed.\nLemma 2: For any given (stationary) policy π in (6), the\ntotal cost Jk in (8) can be simpliﬁed to\nJk =\n1\n1 −γ E[e2\nk].\n(10)\nProof: The result follows from the stationarity of the\nsystem model (1) and that of the policy. That is, E[e2\nk] is\nindependent of k. Hence, Jk = E[e2\nk] P∞\nt=0 γt, giving (10).\nThe result above indicates that we effectively minimise the\nsquared prediction error, for which the following holds.\nLemma 3: Suppose, for any k, wk is independent of uk−i\nfor any i = 1, . . . , m. Then, the optimal policy π⋆of (6) that\nminimises E[Jk] is given by\nˆyk = −a1yk−1 −. . . −anyk−n + b1uk−1 + . . . + bmuk−m\n+ c1ek−1 + . . . + cpek−p\n(11)\nwith ek−i = yk−i −ˆyk−i deﬁned recursively. The correspond-\ning minimum is given by\nmin\nπ E[Jk] =\n1\n1 −γ σ2.\n(12)\nProof: Firstly, it is obvious from (1) and (6)-(7) that ek\ncan be rewritten as\nek = wk + ˜ek\nwhere ˜ek a function of u<k, y<k and w<k, hence independent\nof wk. It is clear that E[e2\nk] ≥E[w2\nk] = σ2. By taking ˆyk as\nin (11), we get\nek + c1ek−1 + . . . cpek−p = wk + c1wk−1 + . . . cpwk−p,\ni.e., ek and wk have the same power spectrum, hence E[e2\nk] =\nE[w2\nk] = σ2, conﬁrming the optimality of (11). Finally, (12)\nis obtained by using Lemma 2. (We note that the result for\nminimum E[e2\nk] is consistent with [1].)\nWith Lemma 3, we can take the policy structure to be\nˆyk(θ) = −ˆa1yk−1 −. . . −ˆanyk−n+ ˆb1uk−1+ . . . + ˆbmuk−m\n+ ˆc1ek−1(θ) + . . . + ˆcpek−p(θ)\n(13)\nek(θ) = yk −ˆyk(θ),\n(14)\nwith θ = [ˆa1 . . . ˆan ˆb1 . . . ˆbm ˆc1 . . . ˆcp]T .\nThe most popular method for ARMAX estimation is the\nso-called pseudo-linear regression (PLR) method [1], [2].\nDeﬁning the pseudo-linear regressor as\nϕk(θ) =[−yk−1 . . . −yk−n uk−1 . . . uk−m\nek−1(θ) . . . ek−p(θ)]T ,\n(15)\nthen\nˆyk(θ) = ϕT\nk (θ)θ;\nek(θ) = yk −ϕT\nk (θ)θ.\n(16)\nThe PRL estimate of θ is computed by solving\nE[ϕk(θ)(yk −ϕT\nk (θ)θ)] = 0.\n(17)\nThis is typically done recursively (known as bootstrapping\nmethod in the system identiﬁcation literature): Starting from\nsome initial estimate θ(0), then for each i = 1, 2, . . ., solve\nθ(i) using\nE[ϕk(θ(i−1))(yk −ϕT\nk (θ(i−1))θ(i))] = 0,\n(18)\nwhich is a repeated least-squares problem.\nThe PRL method is also often combined with the instru-\nmental variable method, where the ﬁrst term ϕk(θ) in (17) is\nreplaced with an instrumental variable (vector) ζk(θ) which is\ndesigned to be uncorrelated with ek(θ).\nThe convergence properties of the bootstrapping and the\ninstrumental variable method depend on many factors; see [2],\n[1] for detailed analysis. We emphasise two key observations:\n1) Global convergence to the optimal solution is not always\nguaranteed;\n2) The bootstrapping method above is a form of policy\niteration in the viewpoint of RL.\nWe will see below that by using a value iteration method in\ncombination with the instrumental variable method, a globally\nconvergent algorithm can be derived for ARMAX estimation.\nWe will ﬁrst study off-line identiﬁcation before giving an on-\nline algorithm.\nA. Off-line Identiﬁcation of MA Models\nWe ﬁrst consider the case of MA models, as this is the\nstumbling block in system identiﬁcation, causing the regressor\n(15) to depend on θ. The MA model is given by\nyk = wk + c1wk−1 + . . . cpwk−p\n(19)\nwith the assumption that c(z) = 1 + c1z−1 + . . . + cpz−p is\nstrictly stable. Also, θ = [ˆc1 . . . ˆcp]T in this case.\nIdentiﬁcation of MA models can be traced back at least\nto [8], [9]. But earlier methods all require solving difﬁcult\nnonlinear equations. In [1] (p. 337), a two-step, non-iterative\nmethod is provided: Step 1 estimates a high-order AR model to\napproximate the MA model; Step 2 uses the prediction errors\n(known as innovations) from the AR model as an estimate\nof the past process noise and estimate the MA parameters\nusing the least-squares method. This method requires heavy\ncomputation for the ﬁrst step due to the use of a high-order AR\nmodel and gives only an approximate solution. Alternatively,\nthe PRL method can be used to reduce complexity, but there\nis no theoretical guarantee for an optimal solution.\nHere we introduce a new algorithm based on an VI method\nin RL. That is, we generalise the bootstrapping method (18)\nby allowing the PLR to depend on multiple past estimates\nof θ, as illustrated in Fig. 1. We ﬁrst consider an off-line\niterative algorithm before extending it to on-line learning.\n4\nWe start estimating θ from k = 0. Denote by θ(k), k ≥0\nthe k-th estimate. We revise (13)-(14) to the following:\nˆyk(θ) =ˆc1ek−1(θ(k−1)) + . . . + ˆcpek−p(θ(k−p))\n(20)\nek(θ) = yk −ˆyk(θ).\n(21)\nThat is, ek−i(θ) is replaced with ek−i(θ(k−i)). With some\nabuse of notation, the latter will denoted by ek if not confusing.\nSuppose yk is measured for all k\n<\n0. Due to the\nstationarity of (19), we can compute all the autocorrelations\nry(i) = E[ykyk−i] using the available measurements prior\nto k\n= 0. Our off-line iterative algorithm assumes that\nry(i), i = 0, 1, . . . , p, are available and produces a sequence\nof θ(k), k ≥0, such that θ(k) →θ⋆as k →∞.\nInitialise e−1 = . . . = e−p = 0 and θ(−1) = . . . = θ(−p) =\n0. For k = 0, 1, . . ., solve θ(k) from\nmin\nθ\nE[e2\nk] = E[(yk −ˆc1ek−1 −. . . −ˆcpek−p)2]\n(22)\nand construct the resulting ek using θ(k). Differentiating the\nabove results in the orthogonality condition:\nE[ekek−i] = 0, i = 1, . . . , p.\n(23)\nAs we will show later that the orthogonality condition holds\nrecursively, i.e., E[ek−jek−j−i] = 0 for all j > 0 and i > 0\nas well. This implies that (23) can be simpliﬁed to\nE[(yk −ˆciek−i)ek−i] = 0,\ngiving the simple solution for ˆci as\nc(k)\ni\n=\n\u001a ρk(i)/E[e2\nk−i]\nif E[e2\nk−i] > 0\n0\notherwise\n,\n(24)\nwhere ρk(i) = E[ykek−i].\nThe resulting E[e2\nk] is given by\nE[e2\nk] = ry(0) −(c(k)\n1 )2E[e2\nk−1] −. . . (c(k)\np )2E[e2\nk−p].\n(25)\nNote that, by construction, ek explicitly depends on θ(k) but\nimplicitly depends on θ(k−1), θ(k−2), . . . because of ek−1, . . .,\nhence the method above is an VI method in the RL framework.\nThe computation of (24) involves ρk(i), which can also be\neasily updated. Indeed, for i = 1, . . . , p,\nρk(i) =E[ykek−i]\n=E[yk(yk−i −c(k−i)\n1\nek−i−1 −. . . −c(k−i)\np\nek−i−p)]\n=ry(i) −c(k−i)\n1\nρk(i + 1) −. . . −c(k−i)\np\nρk(i + p).\nFrom (19), ρk(i) = E[ykek−j] = 0 for j > p. Therefore,\n\n\n1\nc(k−1)\n1\n. . .\nc(k−1)\np−1\n0\n1\nc(k−2)\n1\n...\n...\n...\n1\nc(k−p+1)\n1\n0\n. . .\n0\n1\n\n\n\n\nρk(1)\nρk(2)\n...\nρk(p)\n\n=\n\n\nry(1)\nry(2)\n...\nry(p)\n\n(26)\nwhich can be easily computed due to the triangular structure.\nWe have the following result for convergence.\nTheorem 1: The VI method above has two properties:\n• (Orthogonality:) E[ekek−i] = 0 for all k ≥i and i > 0;\n• (Convergence and Consistency:) θ(k) →θ⋆as k →∞.\nProof: Take any k ≥0 and i > 0. The orthogonality\ncondition for i ≤p was given in (23). Now consider i = p+1,\nE[ekek−i] = E[(yk −c(k)\n1 ek−1 −. . . −c(k)\np ek−p)ek−i]\nThe ﬁrst term E[ykek−i] = 0 due to i > p. Thus, E[ekek−i] =\n0 because E[ek−jek−i] = 0 for all j = 1, . . . , p due to i =\np + 1. This process can be repeated for i = p + 2, p + 3, . . ..\nHence, the orthogonality condition holds for all i > 0.\nTo show convergence and consistency, we note that the\nsequences {e0, e1, . . . ek−1} and {y0, y1, . . . , yk−1} form a\nlinear invertible mapping. Let π⋆\nk be the optimal function in\n(6), linear or nonlinear, such that E[(yk −ˆyk)2] is minimised.\nDue to the assumption that wk is a Gaussian white noise, it is\nwell-known [3] that the optimal π⋆\nk is a linear mapping. Due\nto the invertibility above, the optimal ˆyk can be represented\nas the following linear mapping:\nˆyk = δ1ek−1 + . . . δpek−p + δp+1ek−p−1 + . . . + δke0\nand the optimal δi can be solved by minimising E[(yk −ˆyk)2].\nDue to the orthogonality peroperty of ek and the fact that yk\nis orthogonal to ek−i for i > p, it is easy to see that δi = 0\nfor any i > p and, for any i = 1, 2, . . ., p, δi are the same as\nˆci in (24). That is, the optimal ˆyk is given by\nˆyk = ˆc1ek−1 + . . . + ˆcpek−p\n(27)\nwith [ˆc1 . . . ˆcp] = θ(k).\nOn the other hand, it is also well known [3] that, as k →∞,\nthe stability of c(z) and stationarity of (22) implies that the\noptimal ˆyk is such that\nek = yk −ˆyk →c−1(z)yk = wk\nThat is, c(z)ek →yk, i.e.,\nek →−c1ek−1 −. . . −cpek−p + yk\nˆyk →c1ek−1 + . . . cpek−p\nComparing this to (27), we see that θ(k) →θ⋆as k →∞.\nB. Off-line Identiﬁcation of ARMAX Model\nNow let us return to the ARMAX model (1). Using the\npolicy structure (13)-(17), the task is to solve θ to minimise\nE[e2\nk(θ)] = E[(yk −ϕk(θ)T θ)2].\n(28)\nHowever, the coupling between the ARX part and MA part\nof the model makes it difﬁcult to minimise (28) directly. To get\naround this difﬁculty, we can ﬁrst use a classical instrumental\nvariable method in system identiﬁcation to estimate the ARX\npart and then the proposed VI method to estimate the MA\nmodel [1], [2].\nDeﬁne the instrumental variable (vector) as\nζk = F(z)[−yk−p−1 . . . −yk−p−n uk−1 . . . uk−m]T ,\n(29)\n5\nwhere F(z) is a causal linear ﬁlter with both F(z) and F −1(z)\nbeing stable. In particular, we can take F(z) = 1. Using (1)\nand properties of uk and wk, we get\nE[ζk(yk + a1yk−1 + . . . + anyk−n\n−b1uk−1 −. . . −bmuk−m)] = 0.\n(30)\nThis gives (n + m) linear equations:\nR˜θ = r\n(31)\nwith ˜θ = col{a, b}, R = E[ζk ˜ϕT\nk ], r = E[ζkyk] and ˜ϕk =\n[−yk−1 . . . −yk−n uk−1 . . . uk−m]T . This allows us to\nsolve a and b under the mild persistent excitation condition of\nnonsingular R [1], [2].\nFor the case of ARMA models (with b = 0), if F(z) = I,\nthen ζk = [−yk−p−1 . . . −yk−p−n]T and (31) reduces to\nRya = ry\n(32)\nwith Ry = E[ζkζT\nk+p] and ry = E[ζkyk], and we have the\nfollowing result.\nProposition 1: For the case of an ARMA model, Ry is\nnonsingular if an ̸= 0 and c(z)/a(z) is a minimal realisation\n(i.e., a(z) is not degenerate in its order and there is no zero-\npole cancellation between c(z) and a(z)).\nProof: See Appendix B.\nAfter the ARX part of the model is identiﬁed, we deﬁne\n˜yk = yk −˜ϕT\nk ˜θ.\n(33)\nThen the new MA model\n˜yk = wk + c1wk−1 + . . . cpwk−p\n(34)\ncan be identiﬁed by the value iteration method for MA models.\nC. On-line Identiﬁcation of ARMAX Models\nIn order to obtain an on-line identiﬁcation method for\nARMAX models, we need to convert the instrumental variable\nmethod for the ARX part into a recursive algorithm and\ncombine it with a recursive algorithm of the value iteration\nmethod for the MA model.\nWe do the conversion for the ARX part ﬁrst. At each time\ninstant k = 1, 2, . . ., we replace (31) with\nR(k)˜θ(k) = r(k),\n(35)\nby approximating expectations with empirical averages, i.e.,\nR(k) =\n1\nk + 1\nk\nX\nt=0\nζt ˜ϕT\nt =\nk\nk + 1R(k−1) +\n1\nk + 1ζk ˜ϕT\nk ,\n(36)\nr(k) =\n1\nk + 1\nk\nX\nt=0\nζtyt =\nk\nk + 1r(k−1) +\n1\nk + 1ζkyk.\n(37)\nDenoting P (k) = (R(k))−1, it is standard [1] to obtain the\nrecursive solution to (35) as below.\nProposition 2: The solution to (35) has the following\nrecursion for k ≥1:\n˜θ(k) = ˜θ(k−1) + 1\nk P (k−1)ζkγ−1\nk (yk −˜ϕT\nk ˜θ(k−1)),\n(38)\nP (k) =\n\u0014\nI −1\nkP (k−1)ζkγ−1\nk\n˜ϕT\nk\n\u0015 k + 1\nk\nP (k−1)\n(39)\nwhere\nγk = 1 + 1\nk ˜ϕT\nk P (k−1)ζk\n(40)\nMoreover, ˜θ(k)\n→\n˜θ⋆(the true value of col{a, b}) and\nR(k) →R as k →∞with probability 1, provided that R\nis nonsingular.\nProof: The recursion (39) is obtained by applying the\nwell-known matrix inversion lemma to (36). Then, (38) is\nobtained by applying (37) and (39) to solving (35). Also,\nR(k) →R and r(k) →r (with probability 1) owing to the\nstationarity of the system, and ˜θ(k) →˜θ⋆(with probability 1)\nbecause R is nonsingular.\nNext, we convert the MA part. First, we revise ˜yk to\n˜yk = yk −˜ϕT\nk ˜θ(k),\n(41)\nwhich converges to (33) as k →∞.\nSecondly, we replace ry(i), i = 0, 1, . . . , p, with empirical\naverages, i.e.,\nr(k)\ny (i) =\n1\nk + 1\nk\nX\nt=0\n˜yt˜yt−i\n=\nk\nk + 1r(k−1)\ny\n(i) +\n1\nk + 1 ˜yk˜yk−i.\n(42)\nAgain, r(k)\ny (i) →ry(i) as k →∞.\nThirdly, using r(k)\ny (i) above, we modify (26) to\n\n\n1\nc(k−1)\n1\n. . .\nc(k−1)\np−1\n0\n1\nc(k−2)\n1\n...\n...\n...\n1\nc(k−p+1)\n1\n0\n. . .\n0\n1\n\n\n\n\nρk(1)\nρk(2)\n...\nρk(p)]\n\n=\n\n\nr(k)\ny (1)\nr(k)\ny (2)\n...\nr(k)\ny (p)\n\n\n(43)\nFinally, we replace E[e2\nk] with ǫ2\nk and modify (24)-(25) as\nc(k)\ni\n=\n\u001a ρk(i)/ǫ2\nk−i\nif ǫ2\nk−i > 0\n0\notherwise\n, i = 1, 2, . . . , p,\n(44)\nǫ2\nk = r(k)\ny (0) −(c(k)\n1 )2ǫ2\nk−1 −. . . (c(k)\np )2ǫ2\nk−p;\n(45)\nThe resulting on-line algorithm is summarised below.\nWe have the following result.\nTheorem 2: Under the persistent excitation condition R > 0,\nAlgorithm 1 has the following properties as k →∞:\n• ǫ2\nk →E[e2\nk] with probability 1;\n• θ(k) →θ⋆with probability 1.\nProof: The proof follows directly from Theorem 1, Propo-\nsition 2, and r(k)\ny (i) →ry(i) with probability 1 for all i.\n6\nAlgorithm 1 (On-line Identiﬁcation for ARMAX Models)\n• Initialisation:\n– Set ǫ2\n−i = 0, i = 1, 2, . . . p −1 and ǫ2\n0 = y2\n0;\n– Set r(0)\ny (i) = 0, i = 1, 2, . . . , p and r(0)\ny (0) = y2\n0;\n– Set col{˜θ(0), c(0)\n1 , . . . , c(0)\np } = 0;\n– Set P (0) = p0I for any (large) p0 > 0.\n• Main loop: At iteration k = 1, 2, · · · ,\n– Compute ˜θ(k) and P (k) using (38)-(39);\n– Compute r(k)\ny (i), i = 0, 1, . . ., p, using (42);\n– Compute ρk(i), i = 1, 2, . . . , p, using (43);\n– Compute c(k)\ni\n, i = 1, 2, . . ., p, using (44);\n– Compute ǫ2\nk using (45);\nIV. MODEL-BASED STATE ESTIMATION\nA. Optimal State Estimation for a Known Model\nConsider the following state-space model:\nxk+1 = Axk + B1uk + B2wk\nyk = Cxk + vk,\n(46)\nwhere xk is the state, uk is the known input, wk is the process\nnoise, vk is the measurement noise, {(wk, vk)} is zero-mean\nGaussian noise with\nE\n\u001a\u0014 wk\nvk\n\u0015\n[wT\nl vT\nl ]\n\u001b\n=\n\u0014 Q\nS\nST\nR\n\u0015\nδkl,\nk, l ∈R.\n(47)\nWhen the system model is known, the steady-state Kalman\nﬁlter of (46) is given by [3]\nˆxk+1 = Aˆxk + B1uk + L(yk −Cˆxk)\n(48)\nwith the optimal observer gain L given by [3] (Section 5.4)\nL = (AΣCT + B2S)(CΣCT + R)−1\n(49)\nΣ = AΣAT −(AΣCT + B2S)(CΣCT + R)−1\n· (AΣCT + B2S)T + B2QBT\n2 .\n(50)\nIn the above, Σ = E[(xk −ˆxk)(xk −ˆxk)T ] is the steady-\nstate state estimation error covariance, and (49) is an algebraic\nRiccati equation (ARE).\nB. Pitfall for Model-Free State Estimation\nExample 1: Consider the scalar sequence {yk}:\nyk = wk−1 + wk + µk = (1 + z−1)wk + µk\n(51)\nwhich has the following state-space realisation:\nxk+1 = wk\nyk = xk + wk + µk\n(52)\nwhere wk and µk are independent zero-mean Gaussian white\nnoises with variance equal to 1. Comparing with (46)-(47),\nwe verify that vk = wk + µk, A = 1, B1 = 0, B2 = 1, C =\n1, Q = 1, R = 2, S = 1. The state estimator (48) becomes\nˆxk+1 = L(yk −ˆxk).\n(53)\nSolving (50) gives\nΣ = Σ −(Σ + 1)2(Σ + 2)−1 + 1\nresulting in Σ = (\n√\n5 −1)/2 ≈0.618 and L ≈0.618.\nOn the other hand, the spectrum of yk in (51) is\nSy = (1 + z−1)(1 + z) + 1 = α(1 + α−1z−1)(1 + α−1z)\nwith α = 0.5(3+\n√\n5). Now consider an alternative state-space\nrealisation:\nxk+1 = wk\nyk = α−1xk + wk\n(54)\nwith wk being a zero-mean Gaussian white noise with variance\nof α. For (54), the counterpart of (Q, R, S) is given by Q =\nR = S = α. The optimal state estimator is given by\nˆxk+1 = L(yk −ˆxk).\n(55)\nSolving (50) for this estimator gives L = 1, and the corre-\nsponding steady-state state estimation error covariance Σ = 0.\nThat is, in steady state, xk can be perfectly predicted by y<k!\nSince the noises wk and µk are not directly measurable, the\nstate-space representation (46) is indistinguishable from (54).\nWe see from this example that different state-space reali-\nsations can result in vastly different state estimation results,\nwhich is a unique feature for the state estimation problem\nwhen the state-space model is not speciﬁed!\nC. State-Space Realisation for ARMAX Models\nMotivated by Example 1, we see that different state-space\nrealisations may result in vastly different state estimation\nerrors. Here, we present a state-space realisation that has zero\noptimal state estimation error in steady state.\nOur chosen state-space realisation for (1) is (3), for which\nwe have the following result.\nTheorem 3: Suppose the system model (2) is such that 1)\nn ≥m and n ≥p; and 2) c(z) is stable. Then, the optimal\nstate estimator (48) for the state-space realisation (3) has the\nobserver gain\nL = B2 = [˜cn . . . ˜c1]T\n(56)\nand its associated state estimation error is zero with probability\n1 in steady state, i.e., Σ = 0.\nProof: Comparing the realisation (3) with (46)-(47), it\nis clear that Q = S = R = E[wkwT\nk ] = σ2 for (3). Using\nthe state estimator in (48) and deﬁning the estimation error\nεk = xk −ˆxk and its covariance Σk = E[εkεT\nk ], we have\nεk+1 = (A −LC)εk + (B2 −L)wk\nΣk+1 = (A −LC)Σk(A −LC)T + σ2(B2 −L)(B2 −L)T .\nIt is clear that if L = B2 then,\nΣk+1 = (A −B2C)Σk(A −B2C)T .\n(57)\n7\nFrom Lemma 1, we have\nA −B2C =\n\n\n0\n. . .\n0\n−cn\n1\n...\n...\n−cn−1\n...\n0\n...\n0\n. . .\n1\n−c1\n\n\nIt is easy to verify that\ndet(I −(A −B2C)z−1) = c(z),\n(58)\nwhich is assumed to be stable. Hence, A −B2C is stable. It\nfollows from (57) that Σk →0 as k →∞. That is, the steady-\nstate estimation error covariance Σ = 0, which is obviously\noptimal. Hence, L = B2 is the optimal observer gain.\nD. Model-free State Estimation\nThe result in Theorem 3 shows that with an appropriate\nchoice of the state-space realisation, perfect state estimation\ncan be achieved asymptotically. However, this result requires\nknown parameters for the system. We now show how to\nachieve something similar without a known model.\nUsing Algorithm 1, we can build the one-step-ahead pre-\ndiction ˆyk of yk and the prediction error ek as\nˆyk = −a(k)\n1 yk−1 −. . . −a(k)\nn yk−n + b(k)\n1 uk−1 + . . .\n+ b(k)\nm uk−m + c(k)\n1 ek−1 + . . . + c(k)\np ek−p\nek =yk −ˆyk\n(59)\nfor k ≥0, with yk = 0, uk = 0, ek = 0 for all k < 0.\nFollowing Lemma 1, its state-space realisation is given by\nˆxk+1 = A(k)ˆxk + B(k)\n1 uk + B(k)\n2 ek\n=\n\n\n0\n. . .\n0\n−a(k)\nn\n1\n...\n...\n−a(k)\nn−1\n...\n0\n...\n0\n. . .\n1\n−a(k)\n1\n\n\nˆxk +\n\n\n0\nb(k)\nm\n...\nb(k)\n1\n\nuk +\n\n\n˜c(k)\nn\n˜c(k)\nn−1\n...\n˜c(k)\n1\n\n\nek\nyk = Cˆxk + ek = [0 . . . 0 1]ˆxk + ek.\n(60)\nTheorem 4: Under the same conditions as in Theorem 3,\nthe state-space realisation (60) approaches the optimal state\nestimator of (3) asymptotically. That is, deﬁning the state\nestimation error εk = xk −ˆxk between the states of (3) and\n(60), then the estimation error covariance\nΣk = E[εkεT\nk ] →0 as k →∞.\n(61)\nProof: From (3) and (60), the estimation error dynamics\nis given by\nεk+1 = Axk −A(k)ˆxk + (B1 −B(k)\n1 )uk + B2wk −B(k)\n2 ek.\nAs k →∞, the above approaches\nεk+1 →Aεk + B2(wk −ek)\n= Aεk + B2(wk −yk + Cˆxk)\n= Aεk −B2Cεk\n= (A −B2C)εk.\nFrom (58), the above is stable, hence Σk →0 as k →∞.\nV. MODEL-FREE LQG CONTROL FOR ARMAX SYSTEMS\nIn this section, we apply the model-free state estimation\nresults in the previous section to LQG control. We ﬁrst give a\nresult for model-based LQG control, then derive an algorithm\nfor model-free LQG control.\nA. Model-based LQG Control\nConsider the system model (3) and the value function\nVπ(xk) = E[\nX\nt=k\nγt−k(xT\nt Qxt + uT\nt Rut)]\n(62)\nwith discount factor 0 < γ < 1, Q ≥0 and R > 0. The\nobjective is to design a stationary control policy ut = π(y<t)\nto minimise Vπ(xk). This is a generalisation of the determin-\nistic LQR problem studied in [7] where the noise wk void and\nthe state xk is available. The discount factor is necessary to\nensure the boundedness of the value function.\nWe have the following result.\nProposition 3: Consider the system (3) and the value\nfunction (62). The optimal control policy is given by\nuk = u⋆\nk = Kˆxk,\n(63)\nwhere ˆxk is the optimal estimate of xk based on y<k and K\nis given by\nK = (B1PBT\n1 + γ−1R)−1BT\n1 PA\n(64)\nwith P being the solution to the discrete-time algebraic Riccati\nequation (DARE):\nP = Q + γ{AT PA −AT PB1(BT\n1 PB1 + γ−1R)−1BT\n1 PA},\n(65)\nand the optimal value function in steady state is given by\nV⋆(xk) = xT\nk Pxk + γσ2\n1 −γ BT\n2 PB2.\n(66)\n(See Appendix for proof.)\nB. Model-free LQG Control\nWe now solve the model-free LQG control problem.\nFor any control policy π, deﬁne the Q-function [7] as\nfollows:\nQπ(xk, uk) = xT\nk Qxk + uT\nk Ruk + γE[Vπ(xk+1)]\n(67)\nFor the optimal policy π⋆, using the optimal value function in\nsteady state (66), we get\nQ⋆(xk, uk)\n=xT\nk Qxk + uT\nk Ruk + γE[V⋆(xk+1)]\n=xT\nk Qxk + uT\nk Ruk + γ2σ2\n1 −γ BT\n2 PB2\n+ γE[(Axk + B1uk + B2wk)T P(Axk + B1uk + B2wk)]\n=xT\nk Qxk + uT\nk Ruk + γ(Axk + B1uk)T P(Axk + B1uk)\n+ γ2σ2\n1 −γ BT\n2 PB2 + γσ2BT\n2 PB2\n=[xT\nk uT\nk ]\n\u0014 H11\nH12\nHT\n12\nH22\n\u0015 \u0014 xk\nuk\n\u0015\n+ γσ2\n1 −γ BT\n2 PB2.\n8\nwhere H11 = Q + γAT PA, H12 = γAT PB1 and H22 =\nγBT\n1 PB1 + R.\nThen, minimising Q⋆(xk, uk) with respect to uk yields the\noptimal u⋆\nk and Q⋆in steady state:\nuk = −H−1\n22 H21E[xk|y<k] = Kˆxk = u⋆\nk\n(68)\nQ⋆(xk, u⋆\nk) = xT\nk (H11 −H12H−1\n22 HT\n12)xk + γσ2\n1 −γ BT\n2 PB2\n= xT\nk Pxk + γσ2\n1 −γ BT\n2 PB2\n(69)\nby using (64)-(65).\nUsing the proposed on-line identiﬁcation algorithm (Algo-\nrithm 1) and the model-free state estimation result (Theo-\nrem 4), we can use the following recursion for approximating\nP and K:\nPk+1 = Q + γ{(A(k))T PkA(k) −(A(k))T PkB(k)\n1\n· ((B(k)\n1 )T PkB(k)\n1 + γ−1R)−1(B(k)\n1 )T PkA(k)}, (70)\nwith any P0 > 0, and\nKk = (B(k)\n1 P(B(k)\n1 )T + γ−1R)−1(B(k)\n1 )T PkA(k).\n(71)\nWe have the following result on model-free LQG control.\nTheorem 5: Consider the system (3) and the value function\n(62). Let the model-free LQG control policy πk be\nuk = Kkˆxk\n(72)\nwith Kk as in (70)-(72) and ˆxk as in (60). Then, we have\nπk →π⋆(the optimal policy), i.e., Kk →K, Pk →P,\nuk →u⋆\nk, Vπk(xk) →V⋆(xk) for all xk, as k →∞.\nProof: We ﬁrst claim that Pk > 0 for all k ≥0 and\nlimk→∞Pk = P if (70) is modiﬁed to\nPk+1 = Q + γ{AT PkA\n−AT PkB1(BT\n1 PkB1 + γ−1R)−1BT\n1 PkA}.\n(73)\nTo see the claim above, we note that the above iteration can\nbe rewritten (using the matrix inversion lemma) as\nPk+1 = Q + γAT (P −1\nk\n+ γB1R−1BT\n1 )−1A.\nThis immediately leads to Pk ≥0 for all k ≥0. In fact,\nit is known [3] that, under the assumption that (A, B1) is\ncontrollable and P0 > 0, Pk\n> 0 for all k ≥0 and\nlimk→∞Pk = P. Note that (A, B1) is indeed controllable\nbecause (3) is a minimal realisation. Hence, the claimed\nproperty holds.\nNext, from Proposition 3, we have A(k) →A, B(k)\n1\n→B1\nand B(k)\n2\n→B2 as k →∞. It is easy to see that as k →∞,\n(70) converges to (73), hence limk→∞Pk = P still holds for\n(70), which further implies Kk →K, hence, πk →π⋆.\nVI. CONCLUSION\nModel-free state estimation is a challenging problem due\nto the fact that both the system model is unknown and the\nmeasurement contains partial state and noises. By reformu-\nlating the classical system identiﬁcation as a reinforcement\nlearning problem and incorporating the classical tools of\ninstrumental variables and bootstrapping, we have provided\na value-iteration based reinforcement learning algorithm for\nsystem identiﬁcation of an ARMAX system with guaranteed\nconsistency. This algorithm is then used in solving the model-\nfree state estimation problem for an ARMAX system, and\na reinforcement learning solution has been obtained. These\nresults have also been applied to solving the model-free LQG\nproblem for an ARMAX system.\nThe key to our model-free state estimation solution is to\nuse the observable-canonical realisation, which leads to the\noptimal state estimation by the driving the state estimation\nerror covariance to zero. How to generalise this observation\nto more general systems, linear or nonlinear, will be crucial to\nmore general solutions to model-free state estimation. This\nwill alleviate a stumbling block to reinforcement learning\napplications where only measurement of partial state with\nnoise is available.\nAPPENDIX A: PROOF OF LEMMA 1\nProof: Extend bn = . . . = bm+1 = 0. From (3), we get\nzxk,1 + anxk,n = bnuk + ˜cnwk\n−xk,1 + zxk,2 + an−1xk,n = bn−1uk + ˜cn−1wk\n−xk,2 + zxk,3 + an−2xk,n = bn−2uk + ˜cn−2wk\n. . . . . .\n−xk,n−1 + (z + a1)xk,n = b1uk + ˜c1wk\n(74)\nMultiplying the ﬁrst row above by z−1 and adding it the\nsecond row, we get\nzxk,2 + (an−1 + anz−1)xk,n\n=(bn−1 + bnz−1)uk + (˜cn−1 + ˜cnz−1)wk\nAgain, multiplying this row by z−1 and adding it to the third\nrow in (74), we get\nzxk,3 + (an−2 + an−1z−1 + anz−2)xk,n\n=(bn−2 + bn−1z−1 + bnz−2)uk\n+ (˜cn−2 + ˜cn−1z−1 + ˜cnz−2)wk\nRepeating this until the ﬁnal row of (74), we get\n(z + a1 + a2z−1 + . . . anzn−1)xk,n\n=(b1 + b2z−1 + . . . bnzn−1)uk\n+ (˜c1 + ˜c2z−1 + . . . + ˜cnzn−1)wk\nMultiplying the above by z−1 again, we get\na(z)xk,n = b(z)uk + ˜c(z)wk\nwhere ˜c(z) = ˜c1z−1 + . . . + ˜cnz−n. It follows from (3) that\na(z)yk = a(z)xk,n + a(z)wk\n= b(z)uk + (a(z) + ˜c(z))wk\n= b(z)uk + c(z)wk.\nHence (3) is a state-space realisation of (2).\n9\nAPPENDIX B: PROOF OF PROPOSITION 1\nProof: We prove by contradiction. Suppose Ry is rank\ndeﬁcient. Then, there exists some vector v = [v1 . . . vn]T ̸= 0\nsuch that Ryv = 0. We ﬁrst consider the case the ﬁrst element\nof v, v1 ̸= 0 and take v1 = 1 without loss of generality.\nIt is easy to verify that the (i + 1)-th row of Ry, i =\n0, 1, . . . , n −1 is given by\nRy,i = [ry(p + i) . . . ry(p + i −n + 1)].\nThen, Ry,iv = 0 for all i = 0, 1, . . . , n −1.\nFor any i ≥1, it holds that\nry(p + i) = E[yk−p−iyk]\n= E[yk−p−i(−a1yk−1 −. . . −anyk−n\n+ wk + c1wk−1 + . . . cpwk−p)]\n= E[yk−p−i(−a1yk−1 −. . . −anyk−n)]\n= −[an . . . a1][ry(p + i −n) . . . ry(p + i −1)]T .\nIt follows that\n[ry(p + n) . . . ry(p + 1)] = −[an . . . a1]Ry,\ngiving Ry,n = [ry(p+n) . . . ry(p+1)]v = 0. That is, we have\nextended Ry by one row at the bottom and still maintains its\nrank deﬁciency. The above process can be repeated indeﬁnitely\nto give the result that\nRy,i = [ry(p + i) . . . ry(p + i −n + 1)]v = 0, ∀i ≥0.\n(75)\nDenoting V (z) = v1 + v2z−1 + . . . vnz−(n−1) and the one-\nsided Z-transform of ry(k) as\nˆry(z) = ry(0) + ry(1)z−1 + ry(2)z−2 + . . .\nThen, using (75) and Z-transform properties, we get\nV (z)ˆry(z) = D(z) = d0 + d1z−1 + dp−1z−p\nwhere d0, . . . dp−1 depend on ry(0), . . . , ry(p −1). It follows\nthat the spectrum of yk is given by\nSy(z) =\n∞\nX\nk=−∞\nry(|k|)z−k\n= ˆry(z) + ˆry(z−1) −ry(0)\n= D(z)\nV (z) + D(z−1)\nV (z−1) −ry(0).\nBut from (1) (without uk), the spectrum should be given by\nSy(z) = c(z)c(z−1)\na(z)a(z−1)σ2\nThese two expressions have a clear mismatch of the order\nin the denominator because a(z) is n-th order and V (z) is\n(n −1)-th order). This contradiction implies that Ry can not\nbe rank deﬁcient.\nA similar proof works if v1 = . . . vj−1 = 0 for j > 1 but\nvj ̸= 0. But the details are omitted.\nAPPENDIX C: PROOF OF PROPOSITION 3\nLemma 4: Consider the system\nxk+1 = Axk + Bwk\n(76)\nwith stable A and wk ∼N(0, σ2), and the value function\nV (xk) = E[\n∞\nX\nt=k\nγt−kxT\nt Qxt]\n(77)\nwith Q ≥0. Then,\nV (xk) = xT\nk Pxk + γσ2\n1 −γ BT PB\n(78)\nwith\nP =\n∞\nX\nk=0\nγk(Ak)T QAk = Q + γAT PA.\n(79)\nProof: It is straightforward to verify that\nV (xk)\n= xT\nk Qxk + E[\n∞\nX\nt=k+1\nγt−kxT\nt Qxt]\n= xT\nk Qxk + γE[\n∞\nX\nt=k\nγt−kxT\nt+1Qxt+1]\n= xT\nk Qxk + γE[\n∞\nX\nt=k\nγt−k(Axt + Bwt)T Q(Axt + Bwt)]\n= xT\nk Qxk + γ\n∞\nX\nt=k\nγt−kBT QBσ2\n+ γE[\n∞\nX\nt=k\nγt−kxT\nt (AT QA)xt]\n= xT\nk Qxk + γσ2\n1 −γ BT QB + γ ˜V (xk)\nwhere\n˜V (xk) = E[\n∞\nX\nt=k\nγt−kxT\nt ˜Qxt]\nwith ˜Q = AT QA. Do the above repeatedly, we get\nV (xk) =xT\nk Qxk + γxT\nk AT QAxk + γ2xT\nk (A2)T QA2xk + . . .\n+ γσ2\n1 −γ (BT QB + γBT AT QAB + . . .)\n= xT Pxk + γσ2\n1 −γ BT PB\nwith P given by the ﬁrst part of (79). The second part of (79)\nis then easily veriﬁed and the convergence of the sum in (79)\nis guaranteed by the stability of A.\nNow we are ready to prove Proposition 3.\nProof: We ﬁrst consider the state feedback case where xk\nis available. This is an inﬁnite-horizon linear quadratic control\nproblem with Gaussian noise. It is well known [10] that the\noptimal control policy π⋆is given by uk = Kxk for some\n10\nstabilising K, i.e., ˜A = A+B1K is stable. Invoking Lemma 4,\nthe corresponding value function is given by\nV⋆(xk) = xT\nk Pxk +\nσ2\n1 −γ BT\n2 PB2\nwith\nP = Q + γ( ˜AT P ˜A) = Q + γ((A + B1K)T P(A + B1K)).\nIt is clear that the value function is minimised when P is\nminimised by K. It is a well-known in optimal control [10]\nthat the optimal P is given by the DARE (65) with the optimal\nK is given by (64).\nWhen the state is not available, the well-known separation\nprinciple holds [10] which says that that the optimal control\npolicy is given by (63) with ˆxk being the optimal state\nestimate. We see from Theorem 3 that the optimal state\nestimate ˆxk has zero estimation error covariance in steady\nstate. Hence, (66) holds.\nREFERENCES\n[1] L. Ljung. System Identiﬁcation: Theory for the User, Prentice Hall, 2nd\nEdition,1999.\n[2] T. S¨oderstr¨om and P. G. Stoica, Instrumental Variable Methods for\nSystem Identiﬁcation, Springer-Verlag, 1983.\n[3] B. D. O. Anderson and J. Moore, Optimal Filtering, Prentice Hall, 1979.\n[4] G. C. Goodwin and K. S. Sin, Adaptive ﬁltering prediction and control,\nPrentice Hall, 1984.\n[5] D. Sliver, Introduction of Reinforcement Learning with David Sil-\nver, Lecture Series, DeepMind 2015. (https://deepmind.com/learning-\nresources/-introduction-reinforcement-learning-david-silver)\n[6] D. Bertsekas, Reinforcement Learning and Optimal Control, Athena\nScientiﬁc, 2019.\n[7] F. Lewis and D. Vrabie, “Reinforcement learning and adaptive dynamic\nprogramming for feedback control,” IEEE Circuits and Systems Maga-\nzine, 9(3):32-50, 2009.\n[8] Efﬁcient estimators of parameters in moving-average models, Biomet-\nrica, 46:306-316, 1959.\n[9] A. M. Walker, Large-sample estimation of parameters for moving-\naverage models, Biometrica, 48:343-357, 1961.\n[10] B. D. O. Anderson and J. Moore, Optimal Control: Linear Quadratic\nMethods, Prentice Hall 1971.\n",
  "categories": [
    "eess.SY",
    "cs.SY"
  ],
  "published": "2022-05-06",
  "updated": "2022-05-06"
}