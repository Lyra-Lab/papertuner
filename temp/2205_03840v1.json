{
  "id": "http://arxiv.org/abs/2205.03840v1",
  "title": "Fully Automated Binary Pattern Extraction For Finger Vein Identification using Double Optimization Stages-Based Unsupervised Learning Approach",
  "authors": [
    "Ali Salah Hameed",
    "Adil Al-Azzawi"
  ],
  "abstract": "Today, finger vein identification is gaining popularity as a potential\nbiometric identification framework solution. Machine learning-based\nunsupervised, supervised, and deep learning algorithms have had a significant\ninfluence on finger vein detection and recognition at the moment. Deep\nlearning, on the other hand, necessitates a large number of training datasets\nthat must be manually produced and labeled. In this research, we offer a\ncompletely automated unsupervised learning strategy for training dataset\ncreation. Our method is intended to extract and build a decent binary mask\ntraining dataset completely automated. In this technique, two optimization\nsteps are devised and employed. The initial stage of optimization is to create\na completely automated unsupervised image clustering based on finger vein image\nlocalization. Worldwide finger vein pattern orientation estimation is employed\nin the second optimization to optimize the retrieved finger vein lines.\nFinally, the proposed system achieves 99.6 - percent pattern extraction\naccuracy, which is significantly higher than other common unsupervised learning\nmethods like k-means and Fuzzy C-Means (FCM).",
  "text": " \nFully Automated Binary Pattern Extraction For Finger \nVein Identification using Double Optimization \nStages-Based Unsupervised Learning Approach \nAli Salah Hameed1, Adil Al-Azzawi2 \nComputer Science Department \nCollege of Science, University of Diyala \nDiyala, Baqubah, Iraq \nscicompms2118@uodiyala.edu.iq1 ,  adil_alazzawi@uodiyala.edu.iq2 \n \nAbstractâ€”Today, finger vein identification is gaining \npopularity as a potential biometric identification framework \nsolution. Machine learning-based unsupervised, supervised, and \ndeep learning algorithms have had a significant influence on \nfinger vein detection and recognition at the moment. Deep \nlearning, on the other hand, necessitates a large number of \ntraining datasets that must be manually produced and labeled. In \nthis research, we offer a completely automated unsupervised \nlearning strategy for training dataset creation. Our method is \nintended to extract and build a decent binary mask training \ndataset \ncompletely \nautomated. \nIn \nthis \ntechnique, \ntwo \noptimization steps are devised and employed. The initial stage of \noptimization is to create a completely automated unsupervised \nimage clustering based on finger vein image localization. \nWorldwide finger vein pattern orientation estimation is \nemployed in the second optimization to optimize the retrieved \nfinger vein lines. Finally, the proposed system achieves 99.6 - \npercent pattern extraction accuracy, which is significantly higher \nthan other common unsupervised learning methods like k-means \nand Fuzzy C-Means (FCM). \nKeywordsâ€” Clustering Algorithms; Unsupervised Learning; ğ‘²-\nmean; FCM; Finger Vein Identification. \n \nI. \nINTRODUCTION \nBiometric technology has gotten a lot of attention from the \npublic in recent years in which a remarkable increasing need \nfor biometric system safety and accuracy was observed [1]. \nFingerprints, palm prints, iris, gait, facial characteristics, voice \nrecognition, signature, heartbeat, and palm vein have all been \nsuggested as biometric identification methods. However, the \nmajority of these biometric identification methods have a \nnumber of flaws, including susceptibility, lighting, facial \nexpression, position, and occlusion. Instead, a more reliable, \nsafe, and convenient biometric technique, the finger vein, has \nlately been deployed [2].  \nWhen it comes to finger vein recognition, convolutional \nneural networks (CNNs) have become increasingly popular in \nthe field of image recognition as deep learning technology has \nadvanced and the pixel-level label information obtained by \ntraditional texture extraction techniques has been used to train \nCNNs [3]. A template matching strategy is used to classify the \nimages of finger veins based on the feature data. Finger vein \npatterns were extracted using CNN and then restored using \nFully Convolutional Network (FCN) according to Qin et al.'s \n[4]. \nHowever, designing a completely automated finger-vein \ndetection system in the absence of labeled data continues to be \na difficult undertaking to do. Aside from that, in the most \nrecent suggested systems, the entire data set (the entire finger \nvein image) has been utilized in order to train the model, which \nresults in some biasing in terms of the data and environmental \nsimilarities, such as skin color, backdrop, and other aspects. On \nthe other hand, the finger vein lines should be regarded as the \nprimary pure data because they are the primary pattern upon \nwhich it is built. This work proposes a completely automated, \nunsupervised learning strategy for the automatic production of \ntraining finger vein line data. Our technique eliminates the \nchallenges associated with manually labeling training datasets \nby employing a fully automated way to build them based on \ncertain complex algorithms and procedures [5]. \nII. \nRELATED WORKS \nFinger vein recognition systems were of interest to several \nresearchers. This is a list of some of the most recent \npublications that are related to the subject of this paper [6] [7]. \nDSP platform-enabled finger vein-recognition system was \ndeveloped by Zhi Liu et al. [8] can authorize an individual in \naround 0.08 seconds and has an Accuracy of 0.07 % on a 100-\nsubject dataset. Ajay Kumar et al. [9] developed a fully \nautomated and extensive method for comparing finger sample \nimages that use the subsoil properties of the finger, i.e., finger \ntexturing and finger-vein images. It can derive feature \ninformation through a variety of approaches, such as the \nGabor filter, matching filter, repeating line tracing, and \nmaximal curve extraction. Zhi Liu et al. [10] Wonseok Song et \nal. introduced a mean curvature-based finger vein verification \nmethod that treats the vein picture as a geo-metric form and \ndiscovers \nvalley-like \nstructures \nwith \nnegative \nmean \ncurvatures. The matching of vein patterns using matched pixel \nratio demonstrates that, while keeping minimal complexity, \nthe suggested technique achieves 0.25 percent EER through \nveins appearing as dark shadow lines in the acquired picture as \nhemoglobin occurs densely in blood vessels [8] [9] [10].  \n \n \nIII. \nBACKGROUND THEORY \nUnsupervised learning relies heavily on the idea of \nclustering. The main goal of this research is to find patterns and \nstructures in an unstructured dataset. Clustering or grouping \ndataset that are most similar with each other is the major \nobjective. For instance, some clustering adopts the distance \nmajority method to select the proper cluster for each data point, \njust like any other data point without the label. K-means \nclustering is clarified in Algorithm (1) shown below [11] [12]. \nAlgorithm (1): Unsupervised Clustering Algorithm \n1) Cluster centroids should be set up. Âµ1, Âµ1, . . . , Âµğ‘˜âˆˆâ„ğ‘› \nWith ğ¾=5, where ğ¾ is the number of clusters. \n2) Repeating \n3) For each single pixels   ğ‘ƒ(ğ‘–)   do (1) \n \nP ğ‘–=  minğ‘–ğ‘šğ‘¢ğ‘š\nğ‘—\nâ€–ğ‘¥(ğ‘–) âˆ’ğœ‡ğ‘—â€–\n2  \n \n (1) \n4) On a per-cluster basis (ğ‘—)  do (2) \nğœ‡ğ‘—=\nâˆ‘\n1{ ğ‘ƒ (1)=1}ğ‘¥(ğ‘–)\nğ‘›\nğ‘–=1\nâˆ‘\n1{ ğ‘ƒ (1)=ğ‘—}\nğ‘›\nğ‘–=1\n \n \n(2) \n5) Loop (ğ‘—) is completed // [end of for]. \n6) Loop (ğ‘–) is completed // [end of for]. \n7) End \n \nIt is also possible to utilize ğ¾-Means to locate a potentially \nharmful region in a photograph. Cluster mean is used to \nidentify a set of clusters (ğ‘ƒj) [11] [12]. \n \nIV. \nTHE METHODOLOGY AND SYSTEM DESIGN \nThe methodology of the proposed approach includes two \nsteps of optimization for a completely automated finger vein \npattern extraction based on unsupervised learning, as shown in \nFig.1. The initial level of optimization involves creating a \ncompletely automated, unsupervised finger vein image \nlocalization utilizing an efficient image clustering technique. \nThe second optimization stage involves creating a Global \nPattern Optimization (GPO) based on the extraction, \nindication, and optimization of finger vein lines.  \nThe suggested system is divided into three stages, the first \nof which is per-processing. At this stage, certain preliminary \nimage processing procedures are used to increase the image's \ncontrast and mid-range intensity stretching (optimization). The \nsecond level is completely automatic, unsupervised image \nlocalization. \nAt \nthis \nlevel, \na \ncompletely \nautomated \nunsupervised image clustering technique is used to overcome \nthe major shortcomings of typical unsupervised learning \napproaches (such as the ğ¾-means). Our solution covers the \ndifficulties \nof \nclustering \nrandom \ninitialization, \ncluster \nassignment, and time consumption. In the final step, global \nfinger vein lines are extracted using global pattern \noptimization. The GPO projection is used to correct the finger \nvein lines that have been recovered. \n \n \nFig. 1. Fully automated Finger vein pattern extraction based on double \noptmization stages.  \nA. Pre-processing stage (Stage of preparation):  \nThe preprocessing stage (Stage of preparation) major \nprocedures revolve around adjusting the image intensity level. \nThe basic notion behind this is that defining the number of \ndesirable clusters is the most challenging stage in building a \nfully automated unsupervised learning technique. Clustering \nstabilization, on the other hand, is a challenge that our design \nconsidered and resolved. Concerning the first issue, identifying \nthe relevant number of clusters, we projected the original \nintensity level, which is normally between [0] and [255] in the \nmajority of cases. Because the image intensity has been \nstandardized to be between [0] and [1], we will only have 10 \ndifferences. Then, we employed mid-range intensity stretching \nto reduce the intensity to five levels rather than ten. In this \nscenario, we are very certain that there are only 5 total clusters \nin the domain which is utilized to compute the cluster number. \nApplying pre-processing depends on the following steps:  \n1. Preparing Normalized Intensity (Image normalization): \nThe image normalization procedure is carried out using a \ntechnique called localized image normalization, which is \nbased on estimating the local mean and variance utilizing \nthe intensity of the vein in the middle of the fingers. \nEquation (3) depicts the fundamental idea of localized \nimage normalizing of finger vein images [13]. \nğ‘¥â€²(ğ‘–, ğ‘—) = âˆšğœğ‘™(â„(ğ‘–,ğ‘—)âˆ’ğ‘¥Ì…ğ‘™)2\nğœ\n \n             (3) \nWhere  ğ‘¥Ì…   is the average of pixel intensities and ğœ is used \nto calculate the standard deviation. [13]. \n \n2. Image Smoothing and Noise Removal for Finger Veins \n(Image Enhancing): The technique of modifying an image \nsuch that the output is better suited than the original is \nknown as a finger vein image enhancement. In this work, \nwe employ the filter called Wiener used for smoothing and \nremoving noise from finger vein images. The inverse \nfiltering and noise smoothing procedures effectively reduce \n \na total mean square error. Equation (4) denotes the Wiener \nfilter numerical representation [9] [10]. \nFor the original image and for the added noise, the power \nspectrums are  ğ‘†ğ‘¥ğ‘¥(ğ‘“1, ğ‘“2)  and SÎ·Î· ( ğ‘“1, ğ‘“2 ) respectively. \nğ»(ğ‘“1, ğ‘“2)Is the blurring filter. Fig.2 (c) shows the result of \napplying Wiener filtering to the noise reduction process [14] \n[15]. \n3. Image Adjustment by mid-range stretching: Image \ncorrection is a popular pre-processing technique for \nimproving images. Equation (5) is the fundamental image \nadjustment equation [16]. \nğ‘‹ğ‘–ğ‘—=\nğ¿ğ‘œğ‘¢ğ‘¡+(ğ»ğ‘œğ‘¢ğ‘¡âˆ’ğ¿ğ‘œğ‘¢ğ‘¡)Ã—( ğ‘‹ğ‘–ğ‘—âˆ’ğ¿ğ‘–ğ‘›)\n(ğ»ğ‘–ğ‘›âˆ’ğ¿ğ‘–ğ‘›)\n  \n      (5) \nThere are two parameters in the output image: the lower \nand higher bounds of intensity level,   ğ¿ğ‘œğ‘¢ğ‘¡  and  ğ»ğ‘œğ‘¢ğ‘¡ . ğ¿ğ‘–ğ‘›  \nAnd ğ»ğ‘–ğ‘› represent the input images lowest and greatest \nintensity levels, respectively. The major objective of this step is \nto \nexecute \nimage \nintensity \nlevel \nquantification \nafter \nnormalization the finger vein image. If we use the image after \nit has been adjusted, then the intensity levels will range from \n[0] to [1], with [0] referring to black and [1] referring to white. \nStarting with the premise of equalizing image intensity levels \nto be the initial cluster number to start with, appropriate cluster \nsize can be determined. In our dataset, we have an adjustment \nfactor of [0.2-0.6]. For example, where 0.2 signifies the lowest \nintensity level and 0.6 indicates the highest. As we can see, \nthere are five intensity levels ranging from 0.2 to 0.6, so we'll \nuse that as our beginning cluster number. Fig .3 (b) Illustrates \nthe outcome. \nB. Finger Vein Localizing Stage (Stage of localization): \nThere have been numerous kinds of clustering algorithms \ndeveloped, and they may be summarized as follows: Methods \nof partitioning, hierarchical, model, density-based, and grid-\nbased approaches. Traditional clustering methods such as ğ¾-\nmeans and FCM are employed, as well as our own intensity-\nbased clustering methodology. An intensity distribution model \nis used in the clustering method, ğ‘ƒ(ğ‘– ;  ğ‘‘), which describes the \nrelationship between the intensity difference value ğ‘‘ and \nsigned difference intensity values. For instance, let {ğ¼1, ğ¼2 â€¦ ğ¼ğ‘›} \nrepresents a collection of images taken with the same modality \nand \nanatomical \nstructure \non \ndifferent \npeople, \nwhile  \n{ğ‘¥(1), ğ‘¥(2) â€¦ ğ‘¥(ğ¿)}  denotes the number of    \"clusters\"    formed  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nfrom all  ğ¿  pixels in an image, with the number determined by \nthe size of the intensity interval between the pixels. There are \nfour initial cluster levels if the adjusted intensity range is set \nbetween [0.2, 0.8] with an increment value of 0.15. Therefore, \nthe cluster ranges are going to [0.2-0.35], [0.35-0.5], [0.5-\n0.65], and [0.65-0.8] for Cluster 1, Cluster 2, Cluster 3, and \nCluster 4 respectively. In this case, ğ‘¥(ğ‘–)  is a real intensity value \nin a specified range, where1 <= ğ‘–<= ğ¿. Let {ğœƒ1, ğœƒ2 â€¦ ğœƒğ¾} be \nthe set of the average intensity values of ğ¾ clusters. Let ğ‘ˆğ‘— be \nthe index of the cluster whose center (ğœƒğ‘—) is closest to ğ‘¥(ğ‘–).The \ncluster \nassignment \nof \nall \npixels \n {âŒ©ğ‘¥(1), ğ‘ˆğ‘—âŒª, âŒ©ğ‘¥(2), ğ‘ˆ2âŒª, â€¦ , âŒ©ğ‘¥(n), ğ‘ˆğ‘›âŒª}  is repeatedly updated \nbased on the average intensity (ğœƒğ‘—) of clusters. \nAs stated in (6), the centers ( ğœƒğ‘—)  of the ğ¾ clusters are \nestablished as equally dispersed intervals across the intensity \nrange with an equal step size as given below: \nğ‘†ğ‘¡ğ‘’ğ‘ğ‘ ğ‘–ğ‘§ğ‘’= ğ¼ğ‘…ğ‘ğ‘›ğ‘”ğ‘’\nğ¾Ã— 0.1 Ã— 0.1 \n(6) \nWhere ğ¼ğ‘…ğ‘ğ‘›ğ‘”ğ‘’ denotes to the contrast between the two \nimages (greatest and lowest intensity levels). \n In Algorithm (2), the proposed optimization method is \nillustrated. Instead of using the standard [0-255] band of 256 \nintensity levels, this method uses the [0-1] range from 10 \nintensity levels, subsequently reducing the number of degrees \nto 5.  \n \nAlgorithm (2): Unsupervised Clustering Optimization \n1. Convert Image from 2ğ· to 1ğ·ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„ \nğ¿ğ‘’ğ‘›ğ‘”ğ‘¡â„= ğ‘Ÿğ‘œğ‘¤Ã— ğ‘ğ‘œğ‘™  \n \n(7) \n2. While configuring the parameters, use the previously \nchosen intensity images modification level to establish the \ncluster count. \n3. Create vectors for the initialized cluster. \nğ¼ğ‘›ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ğ‘–ğ‘§ğ‘’ğ‘‘= (1ğ·ğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ ğ‘–ğ‘‘)1Ã—ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„          (8) \n4. The initial value of the cluster centers may be determined \nby using  the equations below : \nğ‘€ğ‘ğ‘¥ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’= max ğ‘–ğ‘šğ‘¢ğ‘š(ğ‘”ğ‘Ÿğ‘ğ‘¦_ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™)                   (9) \nğ‘€ğ‘–ğ‘›ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’= minimum(ğ‘”ğ‘Ÿğ‘ğ‘¦_ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™) \n            (10) \nğ‘†ğ‘ğ‘ğ‘›= ğ‘€ğ‘ğ‘¥ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’âˆ’ğ‘€ğ‘–ğ‘›ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’                     (11) \nğ‘†ğ‘¡ğ‘ğ‘–ğ‘Ÿğ‘£=\nğ‘†ğ‘ğ‘ğ‘›\nğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘‘ \n                                    (12) \nğ‘Šğ‘’ğ‘–ğ‘›ğ‘’ğ‘Ÿ(ğ‘“1, ğ‘“2) =\nğ»(ğ‘“1, ğ‘“2) Ã—  ğ‘†ğ‘¥ğ‘¥(ğ‘“1, ğ‘“2)\n|ğ»(ğ‘“1, ğ‘“2)|2 ğ‘†ğ‘¥ğ‘¥(ğ‘“1, ğ‘“2) +  ğ‘†ğœ‚ğœ‚(ğ‘“1, ğ‘“2)  \n(4) \n \nFig. 3. Using Image Adjusment, a mid-range stretch is achieved (a) original \nimage, (b) corrected (adjusted)  image. \n(a) \n(b) \n \nFig. 2.  Results of the preprocessing stage for the finger vein (a) original \nfinger image, (b) outcome of the image normalization, (c) the outcome of \nimage enhancement and noise reduction.  \n(a) \n(b) \n(c) \n \n \n5. ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘œğ‘‘ğ‘   Âµ1, Âµ1, . . . , Âµğ‘˜âˆˆâ„ğ‘›must be Initialized \nusing(13). \nğ‘–ğ‘›ğ‘ğ‘Ÿğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘£= ğ‘†ğ‘¡ğ‘ğ‘–ğ‘Ÿğ‘£     \n        (13) \n6. Begin For ğ‘–=1 to ğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘‘ do \n  Set up the cluster centers depending on: \nğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘(ğ‘–) = ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘£  \n              (14) \nğ‘–ğ‘›ğ‘ğ‘Ÿğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘–(ğ‘–) = ğ‘†ğ‘ğ‘ğ‘›ğ‘£+ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘£(ğ‘–âˆ’1)  \n(15) \n7. The starting value of the means must be supplied. \nğ‘šğ‘’ğ‘ğ‘› ( ğ‘– ) =  2                                      (16) \n8. Assemble  ğ‘ˆğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘–= 0. \n9. Loop (ğ‘–) is completed // [end of for]. \n10. Repeating \n11. Begin For loop ğ‘– from the start to the finish (image length ) \nthen you must do  \n12. Begin For ğ‘— loop  with relation clustered id then you must \ndo \nğ‘¡ğ‘’ğ‘šğ‘â†1ğ·ğ‘–ğ‘š[ğ‘–]                                (17) \n      ğ‘‘ğ‘–ğ‘“(ğ‘–) â‰”abs â€–ğ‘¡ğ‘’ğ‘šğ‘(ğ‘–) âˆ’ğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘—â€–      (18) \n13. Loop (ğ‘—) is completed // [end of for]. \nğ‘¦(ğ‘–) â‰”agr min\nğ‘—\nâ€–ğ‘‘ğ‘–ğ‘“(ğ‘–)â€–                           (19) \n14. If the tested pixel is the lowest Index y(i) Inside the Cluster \nthen this pixel should be assigned to the cluster. \n15. Ending if \n16. Loop (ğ‘–) is completed // [end of for]. \n17. Determine \nhow \nmany \nof \neach \ntype \nhave \nbeen \nevaluated ğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ¼ğ‘‘. \n18. Updating, the mean values \nğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘—= ğœ‡ğ‘—                                      (20) \nğœ‡ğ‘—â‰”\nâˆ‘\n1{ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ(1)=ğ‘–}\nğ‘˜\nğ‘–=1\nâˆ‘\n1{ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(1)=ğ‘—}\nğ‘˜\nğ‘–=1\n                               (21) \n19. Updates will continue until the coverage is complete. \n20. End \n \nC. Pattren Extraction of Finger Vein(Extraction of Pattern \n): Two stages are presented and implemented in this \nstage. The global pattern of the finger vein line was \ncalculated in the first step, and the globalized finger vein \npattern orientation was estimated and retrieved in the \nsecond step. Finally, the globalized pattern orientation \nand frequency estimates are used to optimize and then \ndraw the specific vein patterns on the fingers. Applying \nFinger Vein Pattren Extraction depends on the following \nsteps: \n1. Estimation of the Orientation of Finger Vein Patterns on a \nGlobal Scale: After normalizing the images in the previous \nstep, we can now go to the next step. This step involves \ncalculating the direction of the specific finger vein \ncharacteristics. Finger vein images are represented by the \norientation image, which reflects their properties. Ridges \nand furrows invariant coordinates are specified by the \nlocalized feature orientation. Algorithm (3) displays the \nmajor phases of the technique of estimating the feature \norientation. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n2. Globalized Finger Vein Pattern Frequency Estimation: \nThe normalized or adjusted image is utilized to calculate  \nthe feature patterns frequencies of the image of a finger \nvein, which is dependent based on the predicted direction  \nof the isolated feature. Vein lines can be detected in this \nscenario as an indication shape (waveform) oriented in \nthe usual direction of the local ridge direction. Assume \nthat ğ’¢ represents the normalized finger vein images and  \nğ’ª  represents the local ridge (vein lines) orientation \nproduced in the previous phase. Then, to estimate the \nfrequency of the localized image, we   need use \nAlgorithm (4). \n3. Extraction of Finger Vein Lines from a Specific Area: \nThe second optimization step, which is the basis for the \nfinger vein pattern filtration model, is called the \noptimization phase. Algorithm (5) describes how to \nconnect and optimize a finger vein line at a certain \nlocation. \n \nFig. 4. On the basis of many universal clustering techniques, Finger \nVein Localization (a), (b), (c), and (d) are the orginal image, \n(e), (f), (g), and (h) are the  k-means, FCM, Otsu's double \nThresholding, and our proposed clustering, respectively, are the \nclustering results. (i), (j), (k), and (l) There are localization \nfindings for k-means , FCM Otsu's double Thresholding as well \nas our  proposed algortihm . \n \nAlgorithm (3): Globalized Pattern Orientation \nInputs:   ğ’¢: Finger vein images that have been \nnormalized. \nOutput:   ğ’ª: The position of the local ridges. \n1. Images were divided into  ğ‘¤Ã— ğ‘¤ blocks from the \nnormalized image  ğ’¢ \n2. Determine the image variations that employ the \nSopel Method in different positions, ğ‘¥âˆ’ğ‘ğ‘¥ğ‘–ğ‘  \nğºğ‘Ÿğ‘ğ‘‘ğ‘¥(ğ‘–, ğ‘—)  and ğ‘¦âˆ’ğ‘ğ‘¥ğ‘–ğ‘  ğºğ‘Ÿğ‘ğ‘‘ğ‘¦(ğ‘–, ğ‘—)  for every \npixels in the normalized image. The following \nformula is used to determine the variations \nmagnitude: \n|ğº(ğ‘–, ğ‘—)| = âˆšğºğ‘Ÿğ‘ğ‘‘ğ‘¥(ğ‘–, ğ‘—)2 + ğºğ‘Ÿğ‘ğ‘‘ğ‘¦(ğ‘–, ğ‘—)2            (22) \n3. The estimated magnitude was determined by: \n|ğº| = |ğºğ‘Ÿğ‘ğ‘‘ğ‘¥(ğ‘–, ğ‘—)| + |ğºğ‘Ÿğ‘ğ‘‘ğ‘¦(ğ‘–, ğ‘—)|            (23) \n4. Using the ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ (ğ‘–, ğ‘—) as a reference point, calculate \nthe localized orientation of each single block. \nğ“¥ğ’†ğ’™ ( ğ’Š, ğ’‹ ) = âˆ‘\nâˆ‘\n(2ğºğ‘Ÿğ‘ğ‘‘ğ‘¥(ğ‘š, ğ‘›)2ğºğ‘Ÿğ‘ğ‘‘ğ‘¦(ğ‘š, ğ‘›))\nğ‘—+ğ‘¤\n2\nğ‘–âˆ’ğ‘¤\n2\nğ‘–+ğ‘¤\n2\nğ‘¢=ğ‘–âˆ’ğ‘¤\n2\n    (24) \nğ“¥ğ’†ğ’š ( ğ’Š,ğ’‹) = âˆ‘\nâˆ‘\n(ğºğ‘Ÿğ‘ğ‘‘ğ‘¥\n2(ğ‘š, ğ‘›)âˆ’ğºğ‘Ÿğ‘ğ‘‘ğ‘¦\n2(ğ‘š, ğ‘›))\nğ‘—+ğ‘¤\n2\nğ‘–âˆ’ğ‘¤\n2\nğ‘–+ğ‘¤\n2\nğ‘¢=ğ‘–âˆ’ğ‘¤\n2\n     (25) \nğ´ğ‘›ğ‘”ğ‘™ğ‘’(ğ‘–, ğ‘—) =\n1\n2 ğ‘¡ğ‘ğ‘›âˆ’1 (\nğ’±ğ‘’ğ‘¦(ğ‘–,ğ‘—)\nğ’±ğ‘’ğ‘¥(ğ‘–,ğ‘—))                (26) \n Where Angle(j, j)  the direction of each block's local \nridge   is represented by the Least Square Estimation. \n5. Use the following formula to turn the 2D image into \na continuous vector field: \nâ‚¬ğ‘¥(ğ‘, ğ‘) = cos (2ğ´ğ‘›ğ‘”ğ‘™ğ‘’(ğ‘–, ğ‘—))                   (27) \nâ‚¬ğ‘¦(ğ‘, ğ‘) = sin (2ğ´ğ‘›ğ‘”ğ‘™ğ‘’(ğ‘–, ğ‘—))                    (28) \n6. Compute the direction of localized ridges for each \nğ‘ğ‘œğ‘–ğ‘›ğ‘¡(ğ‘¥, ğ‘¦) using: \nğ’ª(ğ‘¥, ğ‘¦) =\n1\n2 ğ‘¡ğ‘ğ‘›(\nâ‚¬ğ‘¥â€² (ğ‘,ğ‘)\nâ‚¬ğ‘¦â€² (ğ‘,ğ‘))                            (29) \n7. End \nAlgorithm (4): Globalized Pattern Frequency Estimation \nInputs:  ğ’¢: The position of the local ridges. \nOutput: Î©: The assessment of the frequency of ridges. \n1. The normalized image ğ’¢ was divided into a series of \nblocks with the dimensionsw Ã— w (16 Ã— 16). \n2. For every block which is located in the center of the \nğ‘ğ‘–ğ‘¥ğ‘’ğ‘™(i, j) do \n3. Calculate the size of the locally oriented window ğ‘™Ã—\nğ‘¤, making use of window dimensions(32 Ã— 16). \n4. For every block which is located in the center of the \nğ‘ğ‘–ğ‘¥ğ‘’ğ‘™(ğ‘–, ğ‘—) do \n5. Create a 1D vector and use it to compute the x-\nsignature values. \nğ‘‹[ğ‘˜] =\n1\nğ‘¤âˆ‘\nğ’¢(ğ‘¢, ğ‘£), ğ‘˜= 0,1, â€¦ , ğ‘™âˆ’1\nğ‘¤âˆ’1\nğ‘‘=0\n       (30) \nğ‘ˆ= ğ‘–+ (ğ‘‘âˆ’\nğ‘¤\n2) ğ‘ğ‘œğ‘ ğ’ª(ğ‘–, ğ‘—) + (ğ‘˜âˆ’\nğ‘™\n2) ğ‘ ğ‘–ğ‘›ğ’ª(ğ‘–, ğ‘—)   (31) \nğ‘‰= ğ‘—+ (ğ‘‘âˆ’\nğ‘¤\n2) ğ‘ğ‘œğ‘ ğ’ª(ğ‘–, ğ‘—) + (ğ‘˜âˆ’\nğ‘™\n2) ğ‘ ğ‘–ğ‘›ğ’ª(ğ‘–, ğ‘—)   (32) \n6. Ending nested loop / [end of for]. \n7. Ending main loop / [end of for]. \n8. Estimate the frequency ridge from the 1D vector of \nthe x-signature by allowing ğ’¯(ğ‘–, ğ‘—) be the averaged \npixels in the x-signature between two peaks. After \nthat, the frequency Î©(ğ‘–, ğ‘—) calculated using (33). \nÎ©(ğ‘–, ğ‘—) =\n1\nğ’¯(ğ‘–,ğ‘—)                            (33) \n9. End \n \nAlgorithm (5) Localized Finger Vein Pattern Extraction \n       Inputs: \nğ¼ğ‘šğ‘” : Image of a Finger Vein that has been localized \nğ‘šğ‘ğ‘ ğ‘˜â„ğ‘–ğ‘”â„ğ‘¡ : The mask's overall height \nğ‘šğ‘ğ‘ ğ‘˜ğ‘¤ğ‘–ğ‘‘ğ‘¡â„: The mask's overall width \n      Output:  \nğ‘Ÿğ‘’ğ‘”ğ‘–ğ‘œğ‘›: The finger vein area is shown with a binary mask. \n1. Extraction of the image size is required for the \ninitialization. \n2. By inspecting the bottom half of the supplied finger \nvein image, Equation (34) can be utilized to establish \nthe beginning location. \nmask = Zeros(maskhight, maskwidth)             (34) \n3. Create the pretreatment filtering mask for use in post-\nprocessing. \nğ‘šğ‘ğ‘ ğ‘˜= âˆ‘\nğ‘šğ‘ğ‘ ğ‘˜\nâ„ğ‘–ğ‘”â„ğ‘¡/2\nğ‘–=1\n  = âˆ’ 1                 (35) \nğ‘šğ‘ğ‘ ğ‘˜= âˆ‘\nğ‘šğ‘ğ‘ ğ‘˜\nğ‘›\nğ‘–=â„ğ‘–ğ‘”â„ğ‘¡\n2\n =  1                    (36) \n4. By adjusting the mask size x and y, you may create \nthe kernels of the filter depending on the value of \nsigma you've chosen. \nğ‘˜ğ‘’ğ‘Ÿğ‘›ğ‘’ğ‘™=\n1\n2Ã—ğœ‹Ã—ğœğ‘’âˆ’ğ‘¥2+ğ‘¦2\n2Ã—ğœ2                     (37) \n5. Carry out the actual image filtering operations \nğ¼ğ‘šğ‘ğ‘”ğ‘’ğ‘“ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘‘= ğ¶ğ‘œğ‘›ğ‘£(ğ‘–ğ‘šğ‘ğ‘”ğ‘’, ğ‘˜ğ‘’ğ‘Ÿğ‘›ğ‘’ğ‘™)    (38) \n6. Use the greatest curvature technique to determine \nimage curvatures. \n7. Extricate the ğ‘¥, ğ‘¦, ğ‘§ coordinates from the coordinate \nsystem. \n8. Track and find the lines in the image of the veins on \nthe finger. \n9. For each block size ğ‘›Ã— ğ‘› check the global \norientation-based Algorithm (3). \n10. Use the line srt to close the image from the ğ‘›Ã— ğ‘› \nblock that is extracted in the previous step and with ğœƒ \ndegree that is also extracted from the block size in \nAlgorithm (3). \n11. The binary image should be cleaned up of any \nextraneous pixels. \n12. End \n \nFig. 5. (a), (b), and (c) represents the outcomes of the \nglobalized finger vein estimate stage for the localized finger \nfeature extraction. \n \n \n \n \n \n \nFig.6. depicts the final optimization results, which are \nbased on the GPO Estimation algorithm. For each block, we \nemployed the same structural line and degree. Once the small \nobject pixels in the digital finger vein line image have been \neliminated; they are removed using the binary image closure \napproach. \nV. \nEXPERMENTAL RESULTS \nA. Dataset \nThe finger vein database SDUMLA-HMT was used in this \ninvestigation. It was created by Wuhan University's Joint Lab \nfor Smart Technology and Machine Intelligence. The database \nincludes multiple images with three type index, middle and \nring for left and right hands of an individual. Each of these \nfingers has its data collected six times, resulting in six images \nof the finger veins [17]. \nB. Evaluation Criteria \nPreprocessing, clustering, and finger vein, binary line \nextraction play a role in the performance evaluation of our \nsuggested system. Our proposed system preprocessing \narchitecture is initially evaluated utilizing the most often \nemployed image preprocessing measures, such as peak signal \nto noise ratio (P-S-N-R) (39) , mean square error  (M-S-E) \n(40) and signal to noise ratio (S-N-R) (41) , [18] \n[19][20][21][22][23]. \n \n   ğ‘ƒğ‘†ğ‘ğ‘…= 10 Ã— ğ‘™ğ‘œğ‘”10 (ğ‘€ğ´ğ‘‹ğ¼\n2\nğ‘€ğ‘†ğ¸)                            (39) \n \nHere, ğ‘€ğ´ğ‘‹ğ¼ is the image's highest potential pixel value, \nwhile ğ‘€ğ‘†ğ¸ is the image's average squared error. \nğ‘€ğ‘†ğ¸=\n1\nğ‘›Ã—ğ‘šâˆ‘\nâˆ‘\n[ğ‘‚( ğ‘– , ğ‘—) âˆ’ğ¸( ğ‘– , ğ‘—)]2\nğ‘šâˆ’1\nğ‘—=0\nğ‘›âˆ’1\nğ‘–=0\n   (40) \nWhere (ğ‘‚(ğ‘–, ğ‘—)) is the observed value, and (ğ¸(ğ‘–, ğ‘—)) is the   \npredicted value. \nğ‘†ğ‘ğ‘…= 10 Ã— ğ‘™ğ‘œğ‘”10 (\nğ‘ƒğ‘ ğ‘–ğ‘”ğ‘›ğ‘ğ‘™\nğ‘ƒğ‘›ğ‘œğ‘–ğ‘ ğ‘’)                   (41) \nğ‘ƒğ‘ ğ‘–ğ‘”ğ‘›ğ‘ğ‘™ Is the image's average value, while ğ‘ƒğ‘›ğ‘œğ‘–ğ‘ ğ‘’ is the normal \nimage division. \nWe also outperform current clustering methods including \nk-means, FCM, Otsu's local threshold, and the approach. The \ntime required to perform each clustering algorithm is \ncalculated. A number of factors are taken into consideration \nwhen determining how well the finger vein pattern extraction \nperforms: \n1. Accuracy (Recognition Rate): How many correct \nrecognition judgments are made compared to the total \nnumber \nof \ntries? \nAs \nexpressed \nin \n(42) \n[18] \n[19][20][21][22][23]. \nğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦=\nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ‘‡ğ‘âˆ—100                     (42) \n2. Recall (Sensitivity): The number of relevant predictions \ndivided by the number of relevant findings, As provided \nin (43) [18] [19][20][21][22][23]. \nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™=\nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ‘‡ğ‘                                (43) \n3. Precision: Negative detection rate is defined as the \npercentage of negative predictions that are really correct. \nAs provided in (44) [18] [19][20][21][22][23]. \nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=\nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘                            (44) \n4. F1-Measurement: it's a harmonic mean between \nprecision and sensitivity, As seen in (45) [18] \n[19][20][21][22][23]. \nğ¹1 âˆ’ğ‘€ğ‘’ğ‘ğ‘ ğ‘¢ğ‘Ÿğ‘’ğ‘šğ‘’ğ‘›ğ‘¡= 2 Ã—\n2ğ‘‡ğ‘ƒ\n2ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ+ğ¹ğ‘                (45) \n \nC. Performance Results \nThere are measures for pre-processing images such as: \nThe ratio of signal to noise, Mean or Average Square error and \nSignal-to-Noise Ratio at the Peak or Maximum that are shown \nin TABLE  I. \nFig.6. Localized image Pattern Extraction results (a) Globalized \nimage feature orientation estimation, (b) Finger vein line \nextraction based curvatures model, (c) One block of the \norientation estimation, (d) Image closing using line str \nfrom the n Ã— n block that is extracted in the previous step \nand with Î¸ degree. \n(a) \n(b) \n(c) \nFig.5. Globalized Pattren Orientation Estimation Result (1) The \nOriginal image, (3) Globalized image feature orientation \nestimation. \n \n \nTABLE I .  PREPROCESSING RESULT FOR ORGINAL AND \nENHANCED IMAGES. \nOriginal Images Result \nPreprocessed  Images Result \nP- S-N-R \nM-S-E \nS-N-R \nP- S-N-R \nM-S-E \nS-N-R \n63.3667 \n0.05505 \n0.91529 \n66.9896 \n0.01978 \n0.96670 \nUsing an unsupervised clustering approach, the average time \nfor the finger vein image localization step is presented in \nTable II. By an average of 77.53 seconds, our optimization \nmethod is significantly quicker than traditional k-means and \nthe FCM. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTABLE II .  TIME SPENT ON AVERAGE CLUSTERING IMAGES OF \nFINGER VEINS \nAlgorithm name \n Spending time \nTraditional K-means  \n16.01820 s \nFuzzy C-Means Clustering  \n2.83290 s \nOtsuâ€™s Threshold \n0.00156 s \nOur Optimization Clustering \n0.82967 s \n \nRate of Recognition, Precision, Sensitivities and Specificity \nare used to evaluate the performance of the localization and \ngrouping \nstage. \nThe \nformula \nfor \ncalculating \nthese \nmeasurements are given in (42), (43), (44), and (45), and Our \nproposed Performance Results are shown in Table III. \nTABLE III  .  THE IMPLICATIONS OF OUR PROPOSED ACTIVITIES \nAlgorithm name \nAccuracy \nPrecession \nRecall \nF1-measue \nk-mean. \n22.67913 \n77.33376 \n19.90343 \n30.4792 \nFuzzy C-Means. \n19.97815 \n66.36706 \n21.02692 \n26.7769 \nOtsuâ€™s Threshold. \n76.52724 \n91.60966 \n46.11347 \n61.14073 \nOur approach. \n99.56007 \n90.65569 \n55.44011 \n67.74641 \n \nTable III shows that our suggested method (Finger vein \nbinary pattern extraction) achieved a better accuracy (99.7 \npercent) compared to the other unsupervised clustering \nalgorithms. \nOn \naverage, \nour \noptimized \nfinger \nvein \nidentification based on binary pattern extraction has improved \nthe accuracy by 23% compared with the second heist accuracy \nthat has been achieved by the k-means. Moreover, comparing \nwith the other metrics, our approach has achieved better \nprecession, Recall, and F1-measue by (91%), (55.4%), and \n(67.7%) respectively. \nVI. \nCONCLUSION \nThere are two key benefits to the proposed technique of \ncompletely automated binary pattern creation for the detection \nof finger vein images based on double optimization stages: The \nfirst one that in the term of using the deep learning approach \nwe donâ€™t need to manually generate our training dataset which \nimproved the lack of the training dataset arability. Second, our \napproach has also shown a very clear finger vein pattern that is \npurely shows identical identification cases for the human finger \nvein image matching. Based on the optimized unsupervised \nimage clustering approach, the proposed system produces a \nfully unsupervised image clustering and solves the issues that \nother algorithms such as k-means, FCM, Outâ€™s thresholding are \nfacing such as choosing the decent number of clustering, \ncluster initialization, and automated desired cluster selection. \nFinally, the proposed system shows a higher accuracy (99.6%) \nthat was achieved by our optimized algorithm, while the \nsecond highest score is (76.5), which was achieved by Outâ€™s \nthresholding algorithm. \nREFRENCESES \n[1] Mayyadah R. Mahmood, Adnan M. Abdulazeez, \"A Comparative Study \nof a New Hand Recognition Model Based on Line of Features and Other \nTechniques\",  International Conference of Reliable Information and \nCommunication Technology. Springer, Cham. 420-432, 2017. \n[2] Unar, J. A., Woo Chaw Seng, and Almas Abbasi, \"A review of \nbiometric technology along with trends and prospects\", Pattern \nrecognition 47 (8): 2673-2688, 2014. \n[3] Lu Yang, Gongping Yang, Yilong Yin, and Xiaoming Xi.,\"Finger Vein \nRecognition \nWith \nAnatomy \nStructure \nAnalysis.\" \nIEEE \nTRANSACTIONS 28, (8), 2018. \n[4] Yang, J., and J. Wang, \"Finger-Vein Image Restoration Considering \nSkin Layer Structure.\" In Proceedings of the International Conference \non Hand-Based Biometrics. Hong Kong, China.2017. \n[5] Yang, J., and B. Zhang, \"Scattering Removal for Finger-Vein Image \nEnhancement.\" In Proceedings of the International Conference on Hand-\nBased Biometrics,. Hong Kong, China, 2011. \n[6] Zhang, J., and J. Yang, \"Finger-Vein Image Enhancement Based on \nCombination of Gray-Level Grouping and Circular Gabor Filter.\" In \nFig.7. Outcomes of localized image optimization using several \nuniversised procedure utilized (a), (d), (g), and (j) the orginal \nimage, (b), (e), (h), and (k) the finger vein line extraction based \non repeated line tracking method. (c), (f), (i), and (l) the image \nline extraction based on (GPO)\n \nProceedings of the International Conference on Information Engineering \nand Computer Science, Wuhan, . China, 2009. \n[7] Yu, C.-B., Zhang, D.-M., Li, H.-B., Zhang, F.,\"Finger-Vein Image \nEnhancement Based on Muti-Threshold Fuzzy Algorithm.\" In \nProceedings of the International Congress on Image and Signal \nProcessing, . Tianjin, China, 2009. \n[8]  Liu, Zhi & Song, Shangling. (2012). An Embedded Real-Time Finger-\nVein Recognition System for Mobile Devices. Consumer Electronics, \nIEEE Transactions on. 58. 522-527. 10.1109/TCE.2012.6227456. \n \n[9] Kumar, Ajay & Zhou, Yingbo. (2011). Human Identification Using \nFinger Images. IEEE transactions on image processing : a publication of \nthe \nIEEE \nSignal \nProcessing \nSociety. \n21. \n2228-44. \n10.1109/TIP.2011.2171697. \n[10]   Song, Wonseok & Kim, Taejeong & Kim, Hee Chan & Choi, Joon & \nKong, Hyoun-Joong & Lee, Seung-Rae. (2011). A finger-vein \nverification system using mean curvature. Pattern Recognition Letters. \n32. 1541-1547. 10.1016/j.patrec.2011.04.021. \n[11] Diyar Qader Zeebaree, Habibollah Haron, Adnan Mohsin Abdulazeez \nand Subhi R. M. Zeebaree,â€ Combination of K-means clustering with \nGenetic Algorithm: A reviewâ€, International Journal of Applied \nEngineering Research ISSN 0973-4562 Volume 12, Number 24 (2017) \npp. 14238-14245. \n[12] Jian-Wel Liu, and Lei Guo,\"Selection of Initial Parameters of K-means \nClustering Algorthim for MRI Brain Image Segmentaion.\"IEEE \nInternational Conference on Machine Learning and Cyberntics 12-15. \n[13] Wikipedia. \nn.d. \nNormalization \n(image \nprocessing). \nhttps://en.wikipedia.org/wiki/Normalization_(image_processing). \n[14] Woods, Gonzalez &. n.d. Digital Image Processing . 2016. \n[15] Yusoff, R. and Ramli, A.R.,\"Hardware efficient vein enhancement and \nfeature extraction method.\", Science & Technology Research Institute \nfor Defrence 14, 2017. \n[16] Chung, Moo K., \"Gaussian Kernel Smoothingâ€, 2012. \n[17] Yilong Yin, Lili Liu, and Xiwei Sun. n.d. \"SDUMLA-HMT: A \nMultimodal Biometric Database.\" School of Computer Science and \nTechnology, Shandong University. 2017. \n[18] D. Al-Jumeily, A. Al-Azzawi, A. Mahdi  and  J. Hind. 2017. \"A Robust    \nSpatially Invariant Model for Latent Fingerprint Authentication \nApproach.\" 2017 10th International Conference on Developments in \neSystems Engineering (DeSE). Paris. \n[19] N. Miura, A. Nagasaka, T. Miyatake.,\"Extraction of finger-vein patterns \nusing maximum curvature points in image profiles.\" IEICE Trans. Inf. \nSyst., E90-D (8): 1185â€“1194, 2007. \n[20] B. Huang, Y. Dai, R. Li, D. Tang, W. Li.,\"Finger-vein authentication \nbased on wide line detector and pattern normalization.\" 20th Int. Conf. \nPattern Recognit. (ICPR), Istanbul, Turkey, 2010. \n[21] Zhou, A. Kumar and Y.,\"Human identification using finger images.\" \nIEEE Transction Image Processing 21 (4): 2228â€“2244, 2012. \n[22] W. Song, T. Kim, H. C. Kim, J. H. Choi, H.-J. Kong, and S.-R. Lee.. \"A \nfinger-vein verification system using mean curvature.\" Pattern \nRecognition Letter 32 (11): 1541â€“1547. 2011 \n[23] Jian-Wel Liu, and Lei Guo,\"Selection of Initial Parameters of K-means \nClustering Algorthim for MRI Brain Image Segmentaion.\"IEEE \nInternational Conference on Machine Learning and Cyberntics 12-15.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "published": "2022-05-08",
  "updated": "2022-05-08"
}