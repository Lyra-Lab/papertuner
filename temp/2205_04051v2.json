{
  "id": "http://arxiv.org/abs/2205.04051v2",
  "title": "Unsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks",
  "authors": [
    "Zakaria Patel",
    "Ejaaz Merali",
    "Sebastian J. Wetzel"
  ],
  "abstract": "We introduce an unsupervised machine learning method based on Siamese Neural\nNetworks (SNN) to detect phase boundaries. This method is applied to\nMonte-Carlo simulations of Ising-type systems and Rydberg atom arrays. In both\ncases the SNN reveals phase boundaries consistent with prior research. The\ncombination of leveraging the power of feed-forward neural networks,\nunsupervised learning and the ability to learn about multiple phases without\nknowing about their existence provides a powerful method to explore new and\nunknown phases of matter.",
  "text": "Unsupervised Learning of Rydberg Atom Array\nPhase Diagram with Siamese Neural Networks\nZakaria Patel\nDepartment of Engineering Physics, McMaster University, Hamilton, Ontario L8S\n4L8, Canada\nEjaaz Merali\nDepartment of Physics and Astronomy, University of Waterloo, Waterloo, Ontario\nN2L 3G1, Canada\nPerimeter Institute for Theoretical Physics, Waterloo, Ontario N2L 2Y5, Canada\nSebastian J. Wetzel\nPerimeter Institute for Theoretical Physics, Waterloo, Ontario N2L 2Y5, Canada\nAbstract.\nWe introduce an unsupervised machine learning method based on Siamese\nNeural Networks (SNN) to detect phase boundaries. This method is applied to Monte-\nCarlo simulations of Ising-type systems and Rydberg atom arrays. In both cases the\nSNN reveals phase boundaries consistent with prior research.\nThe combination of\nleveraging the power of feed-forward neural networks, unsupervised learning and the\nability to learn about multiple phases without knowing about their existence provides\na powerful method to explore new and unknown phases of matter.\nKeywords: Artiﬁcial Neural Networks, Phase Transitions, Ising Model, Rydberg Array\narXiv:2205.04051v2  [physics.comp-ph]  19 May 2022\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks2\n1. Introduction\nMachine learning (ML) algorithms enable computers to learn from experience and\ngeneralize their gained knowledge to previously unknown settings. It is perhaps the\nmost transformative technology of the early 21th century.\nThe ability to recognize\nobjects in images [1] or translate languages [2] without being explicitly programmed for\nthis task, highlights the enormous potential of machine learning.\nIn recent years the physical sciences have adopted machine learning based\nalgorithms to explore complex questions. Many methods have been designed to solve\nproblems beyond the scope of data science, and have now the potential to revolutionize\nphysics.\nThe most prominent examples of promising tasks that have been tackled\ninclude ﬁnding phase transitions [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], reconstructing\nor simulating quantum systems [14, 15, 16, 17, 18, 19, 20] and rediscovering physical\nconcepts [21, 22, 23, 24, 25, 26, 27, 28, 29, 30]. All these advances are summarized in\nreview articles directed at diﬀerent audiences. A didactical review to the most modern\ntechniques can be found in [31], a review article focused on the applications of machine\nlearning to examine quantum matter [32] and a broad overview across diﬀerent physical\ndisciplines is summarized in [33].\nThe current manuscript contributes to the development of methods that automatize\nthe calculation of phase diagrams with little to no human prior knowledge about the\nnature of the underlying phases. The subﬁeld of automated phase recognition can be\nsubdivided into two categories: supervised and unsupervised phase recognition.\nIn the ﬁrst case, the operating scientists are aware of the of the possible phases\nand have a rough estimate of where these phases are positioned in the phase diagram;\nhowever, they are unsure about the exact location of the phases and the transitions\nbetween them.\nSupervised learning of phase transitions can be based on diﬀerent\nmachine learning algorithms.\nIt was initially introduced using convolutional neural\nnetworks [3], which are to this day the most powerful and robust tools to learn\naccurate physical phase boundaries.\nThere are hybrid methods that build upon\npurposely mislabelling phase classes [4], methods that are built upon support vector\nmachines [24], and other powerful frameworks [7, 34, 35, 36, 37, 38, 39].\nThese\nmethods have demonstrated success across a wide range of physical systems, from simple\nspin lattices, over strongly correlated quantum systems, up to lattice gauge theories\n[8, 10, 11, 13, 23, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50].\nThe second category contains unsupervised phase recognition algorithms. These\nalgorithms are useful when the researcher who is employing these tools is unaware of the\nunderlying phase structure, meaning they do not know about the existence or location\nof certain phases and thus cannot supply this information to the machine learning\nalgorithm. The simplest unsupervised phase recognition scheme is based on principal\ncomponent analysis [5] and the most widely used unsupervised scheme that leverages\nthe power of artiﬁcial neural networks is based on variational autoencoders [6]. These\nmethods have been examined, enhanced [9, 51, 52, 53] and successfully applied to many\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks3\nsystems in physics and materials science [54, 55, 56, 26, 57, 58, 59, 60, 61, 62, 63].\nCompared to supervised algorithms, unsupervised methods usually have the drawback\nof lacking accuracy in determining phase boundaries [6], or restricting the kinds of order\nparameters that can be learned [5].\nWe introduce an unsupervised machine learning method to discover phase\ntransitions based on Siamese neural networks (SNN). Siamese networks were initially\nintroduced for ﬁngerprint and signature identiﬁcation [64, 65]. Instead of predicting a\ncertain class, Siamese networks predict if two inputs belong to the same class. Hence,\nthese networks can be used for multi- or inﬁnite class classiﬁcation by comparison to\nanchor data points whose label is known. Although Siamese networks are very powerful,\nthey have experienced little use in the physical sciences. So far Siamese networks have\nbeen employed to discover symmetry invariants and conserved quantities [25]. While\nSiamese neural networks are supervised machine learning algorithms, our proposed phase\nrecognition method is unsupervised, in the sense that it does not require any phase\ninformation. This apparent contradiction is reconciled in Section 3.4.\nWhile we initially present our phase recognition method using the example of two\nstacked Ising models exhibiting four diﬀerent phases, we demonstrate the power of this\nmethod by examining the phase diagram of the Rydberg atom array. Rydberg atom\narrays are a powerful platform for experimental realizations of quantum many-body\nphenomena [66, 67, 68]. Neutral atoms are typically arranged via optical tweezers to\nconstruct various physical lattices at varying interaction strengths which give rise to rich\nphase diagrams [69]. Such systems have already been examined with the help of machine\nlearning algorithms.\nThe phase diagram has been revealed by a combined eﬀort of\nunsupervised and supervised methods [70]. Experimental states have been reconstructed\nusing neural network based tomography [71].\nGround states have been calculated\n[72] and simulated measurement data has been used for pre-training variational wave\nfunctions [73].\nThe paper is structured in the following order: we ﬁrst introduce the models we\nare examining with our new Siamese network based framework. These models include\na stacked Ising model and the Rydberg atom array. Subsequently, we describe how we\nprepare the input data using Monte Carlo simulations. We describe how Siamese neural\nnetworks are constructed and trained, and develop our framework to do unsupervised\nlearning of phase transitions. Then we apply our method to both models and present the\nresults in the form of the predicted underlying phase structure. Finally, we summarize\nour ﬁndings and place our method into the broader context of machine learning tools\nfor phase recognition.\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks4\n2. Models\n2.1. Stacked Ising Model\nThe Ising model on the square lattice is a simple, well studied, and exactly solvable\nmodel from statistical physics that exhibits a phase transition. Thus, it provides the\nideal starting point for benchmarking the performance of a phase recognition algorithm.\nIts Hamiltonian is\nH(S) = −J ∑\n<ij>nn\nsisj + h∑\ni\nsi,\n(1)\na function of spin conﬁgurations S = (s1,...,sn).\nIn the following, we focus on the\nferromagnetic Ising model J = 1 and set the external ﬁeld h = 0.\nSNN phase recognition is an algorithm that is able to detect multiple phases in an\nunsupervised manner. Hence, we trivially combine two Ising models by overlaying them\non the same lattice where we can tune each temperature T, ˜T, or in other words the\nratios J/kBT and ˜J/kB ˜T, independently. The combined Hamiltonian\nH(S) = −J ∑\n<ij>nn\nsisj −˜J ∑\n<ij>nn\n˜si˜sj\n(2)\nacts on lattices containing two spins per site, S = ((s1, ˜s1),...,(sn, ˜sn)). Since there is no\ninteraction between them, phase transitions trivially occur at lines (T, ˜T) ≈( ⋅,2.269)\nand (T, ˜T) ≈(2.269, ⋅) in the phase diagram.\nFigure 1: Combining the Ising lattices involves the simple stacking shown in this ﬁgure. The red\nsites indicate spin-up, and the grey sites correspond to spin-down. The two lattice conﬁgurations are\nindependently sampled at temperatures T and ˜T to produce a single stacked conﬁguration at (T, ˜T).\n2.2. Rydberg Array\nA common Hamiltonian that can be implemented by Rydberg arrays has a form similar\nto that of a Transverse Field Ising Model, meaning it is sign-problem free and thus\namenable to simulation on classical computers. The Hamiltonian acts on a collection\nof atoms which individually act like 2-level systems, having a ground-state ∣g⟩≡∣0⟩\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks5\nand an excited, so-called Rydberg state ∣r⟩≡∣1⟩.\nThe atoms are subject to a long-\nrange interaction which is described by a van der Waals (vdW) interaction of the form\nVij ∼∣ri −rj∣−6 that penalizes atoms that are simultaneously in the Rydberg state [74].\nAdditionally, the atoms are subject to coherent laser ﬁelds: a detuning δ which acts like\na chemical potential, driving atoms into their Rydberg states, and a Rabi oscillation\nwith frequency Ωwhich excites ground-state atoms and de-excites atoms in Rydberg\nstates.\nH = ∑\ni<j\nVijninj −δ ∑\ni\nni + Ω\n2 ∑\ni\nσx\ni\n(3)\nwhere ni = ∣1⟩⟨1∣i is the occupation/number operator, and σx\ni = ∣1⟩⟨0∣i + ∣0⟩⟨1∣i.\nThe\ninteraction strength is typically parametrized in terms of a Rydberg blockade radius Rb,\nwhich describes an eﬀective radius within which two simultaneous Rydberg excitations\nare heavily penalized: Vij = Ω(Rb/∣ri −rj∣)6 [75, 76].\n3. Methods\n3.1. Monte-Carlo Simulation\nThe well-known single-spin-ﬂip Metropolis algorithm is used to generate importance\nsampled Monte Carlo conﬁgurations of the Ising model on a square lattice of size 20×20\nwith periodic boundary conditions [77]. After initializing a random lattice, we evolve\nthe simulation for 7168 MC steps between drawing samples. It is important to note that\nneural networks can pick up on any residual correlations, thus relying on conventional\nauto-correlation measures to determine the independence of lattice conﬁgurations is not\nenough. We produce 92 independent conﬁgurations at each of 100 temperatures ranging\nfrom Tmin = 1.53 to Tmax = 3.28. This naturally translates to 92 × 92 = 8464 samples for\nthe stacked Ising model at each temperature pair (T, ˜T).\nFor the Rydberg system, we make use of a recent Quantum Monte Carlo method\n[76] to generate occupation basis samples of the Rydberg Hamiltonian.\nThe QMC\nsimulation is based on a power-iteration scheme which projects out the ground-state of\nthe Hamiltonian:\n∣E0⟩≈(C −H)M ∣+⟩⊗N ,\n(4)\nwhere ∣+⟩is the positive eigenstate of σx, N is the number of lattice sites, C is a\nconstant energy shift used to cure the sign-problem emerging from the diagonal part of\nthe Hamiltonian, and M is called the projection length. We perform our simulations on a\n16×16 square lattice with open boundaries at various parameter values. Unlike previous\nDMRG-based studies[75, 69] we do not impose a truncation on the vdW interaction. We\ntake our projection length M to be 100,000 which we found was more than enough to\naccurately converge to the ground-state over the parameter sets which were simulated.\nFor our simulations we ﬁxed Ω= 1 and performed scans over Rb ∈{1.0,1.1,...,1.8} and\nδ ∈{0.5,0.6,...,2.9}.\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks6\nTo generate the occupation basis data for the SNNs, we ﬁrst perform 100,000 Monte\nCarlo update steps to allow the chain to reach equilibrium. We then record one sample\nevery 10,000 steps in order to eliminate any possible autocorrelation between successive\nsamples.\nEach Monte Carlo step consists of a diagonal update step, followed by a\ncluster update step in which all possible line-clusters are built deterministically and\nﬂipped independently according to a Metropolis condition; see [76] for further details.\nAdditionally, at each point in parameter space (Rb,δ) we run 3 independent Markov\nchains. The chains are allowed to evolve until each has generated 400 samples, giving a\ntotal of 1200 independent samples for each parameter pair.\n3.2. Siamese Neural Networks\nFigure 2:\nThe architecture of the convolutional subnetwork f in the SNN F.\nThe input Si is\nﬁrst passed through 2 convolutional layers, followed by an average pooling to capture a quantity\ncharacterized by an average. The result is ﬂattened in order to feed it to a dense layer. This dense\nlayer is then connected to an embedding layer.\nArtiﬁcial neural networks are directed graphs that have the ability to learn an\napproximation to any smooth function f(x) = y given suﬃciently many parameters. A\nneural network is built by successively applying matrix multiplications characterized by\nweights wL\nij that are oﬀset by biases bL\ni . (i,j are neuron indices in diﬀerent layers L).\nBetween subsequent matrix multiplications there is a non-linear activation function,\ncommon choices of which are sigmoid or rectiﬁed linear units.\nA neural network is\ntrained by applying it to a data set and optimizing the network parameters to minimize\na certain objective function using gradient descent.\nSiamese neural networks (SNN) were introduced to solve an inﬁnite class\nclassiﬁcation problem as it occurs in ﬁnger print recognition or signature veriﬁcation\n[64, 65]. Instead of assigning a class label to a data instance, the SNN compares two\ndata points and determines their similarity. A solution to the inﬁnite class classiﬁcation\nproblem is obtained by calculating the predicted similarity between labelled anchor data\npoints and a new unlabelled data instance.\nA SNN F(Si,Sj) (Fig. 3) consists of two identical sub-networks f (Fig. 2) which\nproject an input pair into a latent embedding space. The similarity of two inputs is\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks7\ndetermined based on the distance in embedding space.\nF(Si,Sj) = d(f(Si),f(Sj))\n(5)\nPossible distance metrics d must be chosen according to the problem at hand, in our case\nwe chose the average squared Euclidean distance on the unit sphere which is equivalent\nto the cosine distance.\nd(f1,f2) = ∑N\ni=0(f1i −f2i)2\nN\n,\n(6)\nHere f1i,f2i denotes diﬀerent components in an N dimensional embedding space.\nInstead of training the SNN on paired training data, an eﬀective way to train Siamese\nneural networks is through minimizing contrastive loss functions involving triplets\n(Sa,S+,S−) of data points\nL = max(d(f(Sa),f(S+)) −d(f(Sa),f(S−)) + α,0),\n(7)\nwhere the hyperparameter α is chosen such that the neural network is prevented\nfrom learning trivial embeddings.\nThe intuition behind α is its interpretation as a\nmargin to encourage separation between the anchor and the negative embedding in\nthe embedding space [78]. Minimizing L requires minimizing the distance between the\nanchor and positive sample d(f(Sa),f(S+)), while maximizing the distance between the\nanchor and negative sample d(f(Sa),f(S−). max(⋅,0) prevents the neural network from\nlearning runaway embeddings f(Si)i →∞.\n3.3. Model Architecture\nThe explicit model architecture for f Fig. 2 depends on the underlying data set. In\nthe case of the Ising model, f consists of two 2-D convolutional layers, both with stride\n(1,1), a kernel size of (3,3), and with 6 and 10 ﬁlters, respectively. Each layer is fed\ninto a ReLU activation function. The resulting image dimensions are (16,16). This\nis followed by a (16,16) average pooling layer. The output is ﬂattened and fed to a\ndense layer with 10 neurons. Subsequently, we feed this output to the embedding layer,\nwhich also contains 10 neurons and a sigmoid activation function. The embedding is\nnormalized to unit length under euclidean norm.\nThe model architecture for examining the Rydberg system is similar to the Ising\nmodel. In this case, we begin with two 2-D convolutional layers, both with stride (1,\n1), a kernel size of (3, 3), and with 6 and 4 ﬁlters, respectively. The resulting image\ndimensions are (12, 12). As before, we subsequently apply an average pooling layer, but\nthis time of dimension (12, 12). The remainder of the architecture is identical to before.\nWe use the Adam optimizer to train our neural network. Furthermore, our training\nprocedure involves the early stopping callback. This technique involves training as long\nthe loss is decreasing. If the loss is not decreasing for m epochs, training is stopped. The\nvalue of m is known as the patience. We set the maximum number of epochs to 150,\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks8\nFigure 3: A Siamese neural network (SNN) F takes a pair of input samples (Si,Sj) and predicts their\nsimilarity d(f(Si),f(Sj)) The training architecture eﬀectively mimics two SNNs acting on a triplet\n(Sa,S+,S−) of samples. Sa represents the anchor input, S+ the positive sample from the same class as\nthe anchor, and S−the negative sample from a diﬀerent class. The shared convolutional subnetworks f\nmap their inputs into an embedding space. The SNN is trained to minimize distances d(f(Sa),f(S+)),\nwhile maximizing distances d(f(Sa),f(S−)). During the inference stage, we discard one branch of this\nnetwork to obtain a pairwise comparison between the anchor and a new unknown sample.\nwhich is enough to allow the callback to decide when to terminate the training process.\nWe observe that small values of patience, around m = 1..3 and m = 6 are suﬃcient for\ntraining on the Ising model and the Rydberg system, respectively. Additionally, we use\na learning rate of αlr = 0.001 for the Ising model, and αlr = 0.0005 for the Rydberg\nsystem. Another hyperparameter is the margin for the contrastive loss, where we use\nα = 0.4. We employ the TensorFlow and Keras libraries to implement the network,\ntraining, and callbacks.\n3.4. Phase Boundaries from Siamese Networks\n3.4.1. Supervised Learning of Phase Transitions:\nSince the proposal to use supervised\nmachine learning algorithms for calculating phase diagrams, the most powerful method\nstill remains using feed-forward neural networks for the binary classiﬁcation of Monte-\nCarlo samples [3]. In this case a neural network is trained on conﬁgurations from known\nparts of the phase diagram labelled by their phase. By denoting the phases with binary\nlabels y ∈[0,1], a neural network f is trained to predict the phase of a conﬁguration S\nf(S) = { Phase A\nPhase B .\n(8)\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks9\nAfter training, this neural network is then applied to samples from unknown parts\nof the phase diagram. Since these networks intrinsically learn the underlying physical\nfeatures characterizing the phases like order parameters and other thermodynamic\nquantities [23], the predictions of the neural networks ﬂip from one label to the other at\nthe position of the phase transition.\nThe principle that guides us through the development of an unsupervised Siamese\nnetwork-based scheme for phase recognition is to leverage the power of neural networks\nfor phase classiﬁcation in the supervised setting. This is done by reformulating the task\nof predicting phases by a neural network. A Siamese neural network takes a pair of\ninput conﬁgurations and predicts if they are similar or diﬀerent with respect to a metric\nimposed by the objective function during training.\nF(S1,S2) = { same Phase\ndiﬀerent Phase\n(9)\nIn this formulation a SNN can be used as a supervised algorithm for multi-phase\nrecognition. In order to do unsupervised learning of phases with SNNs the training\ndata cannot be supplied with phase labels. In order to enable a SNN to learn from\nunlabelled data we need to understand how data aﬀects the gradients while training\nneural networks.\n3.4.2. Gradient Manipulation:\nTo develop an unsupervised framework, it is important\nto understand how gradients update the neural network. Let us discuss this at the\nexample of a general neural network h trained in a supervised setting to minimize the\nmean square error loss function on a labelled data set (X,Y ). The discussion can be\nextended to any network in this manuscript. The eﬀect of a single training example\n(x,y) ∈(X,Y ) on the loss function is\nL(h(x),y) = (h(x) −y)2\n(10)\nThe neural network switches its prediction at the decision boundary\nh(x){ < 0.5\nclass A\n> 0.5\nclass B\n(11)\nNeural networks are trained using backpropagation of gradients. Let us focus on the\ngradient signal on an example weight w out of the millions of parameters characterizing\na neural network. Let us further assume we have two identical training conﬁgurations\nx with opposite labels labels y ∈[0,1]. Each update step invloves the product of the\nlearning rate η and the inverse of the derivative of the loss function with respect to w:\nwnew = −η ∂wL(h(x),y)\n(12)\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks10\nThe update depends on the label y:\n∂wL = (h(x) −y)2∣f(x)=0.5= { ∂wh(x)\ny = 0\n−∂wh(x)\ny = 1\n(13)\nThus, by supplying the neural network with two similar training samples, but\nopposite labels in the same training step, their eﬀect on the weights of the neural\nnetworks would approximately cancel each other out. While this combined training\nsignal forces the neural network to be more uncertain h(x) →0.5, it will never change\nthe prediction itself.\n3.4.3. Unsupervised Learning of Phase Transitions:\nIn each update step the Siamese\nnetwork is trained on triplets (Sa,S+,S−), where Sa is called anchor, S+ the positive\ncomparison, and S−the negative comparison. In an unsupervised setting we do not\nhave the true phase labels at hand. However, we know that two samples from the exact\nsame point on the phase diagram must have the same phase. Thus, we create training\nbatches where Sa and S+ are from the same coordinates in the phase diagram, while S−\nis sampled randomly from anywhere in the phase diagram.\nBuilding on the discussion of gradient signals: If S+ and S−stem from the same\nphase the training signals should approximately cancel each other out, such that the\nremaining noise is subleading compared to the signal that is obtained when S+ and S−\nare from diﬀerent phases. In a physical context, the noise might stem from thermal\nﬂuctuations, and the leading signal from relevant thermodynamical quantities.\nSince there is still no comprehensive theory on neural network training dynamics,\nthe above discussion lacks in mathematical rigor. Hence, in line with all other machine\nlearning based phase recognition methods, the only way to convince ourselves of the\ncapabilities of the proposed method is an empirical study by applying the method to\nphysical systems.\n4. Results\nFor the purpose of calculating phase diagrams, we train SNNs as outlined in the previous\nparagraphs for both the stacked Ising model and the Rydberg atom array. We create\ntraining triplets (Sa,S+,S−) containing a randomly sampled anchor conﬁguration Sa,\na positive conﬁguration S+ sampled from the same point in the phase diagram and a\nnegative conﬁguration S−sampled from any other point in the phase diagram. After\nhaving successfully trained a SNN it is employed to perform pairwise comparisons on\nconﬁgurations along a speciﬁed one-dimensional slice through the phase diagram.\n4.1. Adjacency Comparisons\nIn this scheme, a conﬁguration corresponding to a point in the phase diagram is\ncompared to its neighbors within a certain distance. At the example of a single Ising\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks11\nFigure 4: (a) The horizontal and vertical slices extend through the lattice, while the diagonal slice\npierces through the center of the lattice. (b) Within each slice, we compare conﬁgurations at indices\nn and n + k in order to (c) evaluate their dissimilarity. The temperature value at which the peak is\nsituated in (c) corresponds to the predicted phase transition temperature.\nmodel, this means a conﬁguration from temperature Tn is compared to a conﬁguration\nat Tn+k, where n is the temperature list index and k is a constant shift. The value of\nk can be treated as a hyperparameter. A comparison between conﬁgurations at Tn and\nTn+k is assigned a temperature of 1\n2(Tn + Tn+k).\nOur similarity measure of choice is the normalized cosine dissimilarity scaled to a\nrange [0,1], where 1 corresponds to the empirically found maximal dissimilarity and 0\nto the empirically determined minimal similarity. The cosine dissimilarity is related to\nthe cosine distance via 1 −cos(θ), where\ncos(θ) =\nf(Sn) ⋅f(Sn+k)\n∥f(Sn)∥∥f(Sn+k)∥\n(14)\nThe cosine distance is equivalent to the euclidean distance when acting on the SNN\nembedding space normalized to the unit sphere (∥f(Si)∥2 = 1):\n∥f(Sn) −f(Sn+k)∥2 = 2 −2 cos(θ)\n(15)\nSince the neural network f is designed to output positive values f(Sn)i ∈[0,1], the\ncosine dissimilarity takes on its minimum 1 −cos(θ) = 0 for similar embeddings and its\nmaximum 1 −cos(θ) = 1 if the embeddings are dissimilar.\nThe result of scanning across a phase transition is depicted schematically in\nFig. 4(c). The highest peak will indicate the SNN prediction of the phase transition.\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks12\nFigure 5: We use k = 1 to perform adjacency comparisons across the phase diagram. The blue shading\nrepresents the standard deviation over an ensemble of 5 runs. We do not see signiﬁcant variability in\nthe prediction of the phase transition. These particular slices are taken at (a) (⋅, ˜T = 2.767) and (b)\n(T = 2.767,⋅). The peaks of both slices predict Tc ≈2.352.\n4.2. Ising Model\nWe examine the results of applying SNN unsupervised phase recognition to the 20 × 20\nstacked Ising model. Analytically, the phase boundaries of the stacked Ising model in the\nthermodynamic limit are (T, ˜T) = (2.269,⋅) and (T, ˜T) = (⋅,2.269). The phase transition\ntemperature is prone to ﬁnite size eﬀects as the lattice becomes smaller. If the phase\ntransition would be calculated using the magnetization, ﬁnite size eﬀects would distort\nthe phase boundaries to (T, ˜T) ≈(2.36,⋅) and (T, ˜T) ≈(⋅,2.36); however, it is important\nto note that the quantities a neural network learns might diﬀer from the magnetization\nand thus experience diﬀerent ﬁnite size scalings [55].\nIn order to calculate the stacked Ising model phase boundaries, we choose to perform\nvertical and horizontal scans across the phase diagram, where each scan is repeated\nﬁve times and the standard deviation of this ensemble is displayed as uncertainty.\nExemplarily, the results of two of these scans can be found in Fig. 5. The location\nof these scans is depicted in (c). (a) reveals the phase transition from (ferromagnetic,\nparamagnetic) to (paramagnetic, paramagnetic) (FP to PP), while (b) displays the\nphase transition from PF to PP. Collecting 21 vertical and 21 horizontal scans reveals\nthe phase boundaries of the stacked Ising Model, as seen in Fig. 6. By comparing the\nhorizontal (green dotted line) and vertical scans (red dotted line) with the Ising model\nphase boundaries in the thermodynamic limit (black dashed lines) one can observe a\nclear diﬀerence. The SNN predicts critical temperatures of Tc ≈2.352, consistent with\nthe ﬁnite size correction of the magnetization which indicates a phase transition at\nTc ≈2.36.\nIn order to reveal the power of our SNN based phase recognition scheme, we have to\nscan across diagonal slices within the phase diagram that contain more than one phase\ntransition. For this purpose we scan diagonally across the phase diagram as depicted\nin Fig. 7.\nThe two diagonal scans across the lattice reveal that this technique can\nidentify more than one phase transition. The ﬁrst diagonal scan is performed along a\nline through the center of the lattice. In this case, we see a single phase transition, as\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks13\nFigure 6: The phase diagram of the stacked Ising Model contains four regions. Both of the sublattices\nof the stacked model can be in either the paramagnetic (P) or ferromagnetic (F) phase. We perform\nadjacency comparisons as in Fig. 5(a), (b) for each point in the dotted lines indicating the SNN estimates\nfor the phase transitions. The deviations from the true phase transitions are quantitatively consistent\nwith ﬁnite size eﬀects on the magnetization.\nthe network scans directly through the intersection of the vertical and horizontal phase\nlines. A closer examination of the eﬀect of changing k can be found in Fig. D1. The\nprediction yields a phase boundary at (T, ˜T)c = (2.28,2.28).\nThe second diagonal scan we perform is shifted, such that it crosses both the vertical\nand horizontal phase transition. In this case, the true phase boundaries are cut by\nthe shifted diagonal slice at (T, ˜T)c = (1.827,2.269) and (T, ˜T)c = (2.269,2.711). The\nnetwork is able to capture both transitions at (T, ˜T)c ≈(1.857,2.299) and (T, ˜T)c ≈\n(2.281,2.723). A closer examination is found in Fig. D2.\nFurthermore, the embeddings shed some light on how the neural network encodes\nthe aforesaid phase transitions.\nIn Fig. 8 we see embeddings which encode phase\ninformation for the diagonal slice.\nThese embeddings clearly separate between the\ntwo underlying phases. Embedding 1 only spikes in the PP phase, while embedding\n10 activates at FF regimes. A switch between both embeddings occurs at the phase\ntransition. Other embeddings can be found in Fig. C1. In the case of a single embedding,\nwe may have noise contained in the same embedding neuron where the phase behaviour is\ncaptured. With additional embeddings, the noise may be relegated to other embedding\nneurons, isolating the phase transition information.\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks14\nFigure 7: We perform a two diﬀerent diagonal scans across phase diagram of the stacked Ising Model.\nThe adjacency comparison plot (a) depicts the result from a scan across the diagonal (blue line in (c)),\nwhile (b) depicts the same for the shifted diagonal (red line in (c)). In (c), the diagonal (blue) line\nonly crosses the phase boundaries once in the center (FF to PP), and we correctly observe a single\ncorresponding peak in (a). Likewise, the shifted diagonal line (red) crosses two phase boundaries (from\nFF to FP, then FP to PP), resulting in two corresponding peaks.\nFigure 8: Latent space embeddings of the SNN are applied to conﬁgurations of the stacked Ising\nmodel. In particular, we choose our conﬁgurations from the diagonal slice to reveal how the SNN\nencodes phase information. Of the 10 embedding neurons, some encode the phase information of the\ndiagonal slice. Embedding neurons 1 and 10 are such neurons, distinguishing between the FF and PP\nphases in the diagonal slice. (a) and (b) depict a separation of activity between the aforesaid regions.\n(c) counts the number of instances in which a point is above the halfway line in the y-axis of (a) and\n(b). There is a separation in these histograms between the two phase regions, FF and PP. According to\n(c), embedding 1 (blue) has a preference for encoding information in the PP region, while embedding\n10 (green) encodes the FF region.\nThe embedding space of the shifted diagonal scan is depicted in Fig. 9.\nWe\nobserve three crucial behaviours in the embeddings.\nEach of the three histograms\nshow maximum activity in three diﬀerent regions. Embedding 2 encodes the FF phase,\nembedding 4 the PP phase, and embedding 7 encodes conﬁgurations from the FP phase.\n4.3. Rydberg Array\nThe Rydberg atom array phase diagram is not as well studied as the Ising model, so we\nneed to compare our SNN phase boundaries to recent papers [69, 75] and evaluate the\norder parameters on the QMC data ourselves.\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks15\nFigure 9: Latent space embeddings of the SNN are applied to conﬁgurations of the stacked Ising model\nbelonging to the shifted diagonal slice, which contains two phase transitions. The embeddings display\nstructures associated with each phase. While (a) appears to encode the FP phase, its corresponding\nhistogram reveals that it encodes the FF phase. The histogram accounts for the density of points\nbeyond the halfway line in (a), indicating that many points are clustered together in the FF phase.\n(c) similarly shows the encoding of the FP phase, while (b) shows the encoding of the PP phase. The\ncorresponding histograms are colour-coded in (d).\nIn order to identify the approximate phase boundaries we compute predictors for\nthe phases of interest. Each phase corresponds to various peaks in the absolute value of\nthe Fourier transform (FT) of the one-point function:\nn(k) = n(kx,ky) = ∣1\n√\nN ∑\nj\nnj exp(ik ⋅rj)∣\n(16)\nwhere nj is the Rydberg occupation of the jth site. Furthermore, we symmetrize the\nFT by averaging over permutations of the momentum axes:\nF(kx,ky) = 1\n2 [n(kx,ky) + n(ky,kx)]\n(17)\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks16\nThe peaks occur for the checkerboard phase at (π,π), for the striated phase at (π,0) and\n(π,π), and for the star phase at (π,0), and (π/2,π)[75]. We compute, for each point in\nparameter space (Rb,δ), the symmetrized FT at these speciﬁc momenta averaged over\nthe full 1200 sample data set given to the SNNs.\n4.3.1. Testing\nFigure 10:\nWe apply the SNN phase recognition method to the Rydberg array in the region\nRb ∈{1.0,...,1.8} and δ ∈{0.5,...,2.9}.\nWe scan along each horizontal slice by choosing a ﬁxed\nRb, and traversing along all δ ∈{0.5,...,2.9}. We choose k = 1 to perform adjacency comparisons\nwithin these scans. The dissimilarity peaks when conﬁgurations at δi and δi+1 are of diﬀerent phases.\nThe Rydberg atom array phase diagram is examined in regimes where the\ncheckerboard, striated and star phases are present. Thus, guided by [69] we focus the\nregion Rb ∈{1.0,...,1.8} and δ ∈{0.5,...,2.9}. The training of the SNN on Rydberg\nQMC data is performed similar to the Ising model on each horizontal and vertical\nslice. Inference is performed by adjacency comparisons with step-size k = 1. Because\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks17\nthe Rydberg phase diagram is coarser than the Ising model phase diagram (25 points\nbetween δ = 0.5 to δ = 2.90 for the Rydberg array, compared to 100 points between\nT = 1.53 and T = 3.28 for the Ising model), there is no reason to use k > 1.\nFigure 11: Phase transition lines are superposed on the Rydberg phase diagram. These lines result\nfrom connecting the peaks from Fig. 10 and Fig. C4 in the smoothest possible way. Doing so results in\nthree distinct phase boundaries. In (a)-(c) we overlay these phase lines on the diagram generated via\nour QMC simulation, whereas (d)-(f) use the phase diagram produced by [69].\nAll horizontal and vertical scans depicted in Fig. 10 and Fig. C4, respectively, are\nthe result of an ensemble average over 5 diﬀerent runs per slice. The error bars represent\nthe standard deviation between runs. The vertical lines in each plot in Fig. 10 represent\nthe approximate phase transition at each Rb based on [75].\nOur observations diﬀer\nfrom their result in the sense that we consistently observe two phase transitions in all\nslices (although some of these phase transition peaks are weak, as shown in Fig. 10).\nHowever, we also conﬁrm that in all cases the SNN predicts a phase transition that\ncoincides with the results from [75]. There is only a slight deviation at Rb = 1.4 in the\nhorizontal scanning direction. Furthermore, the results in Fig. C4 reveal the striated\nphase in between the star and checkerboard phases, which is also consistent with [75].\nIn ﬁgures Fig. 11 (a)-(c) we compare the SNN predictions with the results from\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks18\nFigure 12:\nSNN phase recognition method applied to the Rydberg array in the region Rb ∈\n{1.0,...,1.8} and δ ∈{0.5,...,2.9}. (a) Raw results of SNN scans along horizontal and vertical slices\ncollected from Fig. 10 and Fig. C4. Strong signals correspond to clearly visible phase transition signals,\nwhile weak signals are only considered phase transition indicators if they can be consistently identiﬁed\nin neighboring slices. (b) Regions in the phase diagram are separated by connecting markers in (a)\nas smoothly as possible, they are labelled according to the Fourier Modes characterizing the phases in\nFig. 11(a)-(c).\nevaluating order parameters on our QMC data. For each the checkerboard phase, the\nstar phase, and the striated phase we create separate plots. The QMC data is chosen as\nthe background, while in the foreground we connect phase transition signals from the\nSNN in the smoothest way possible in the form of blue, red, orange, pink and yellow\nlines. We ﬁrst observe a perfect agreement of the checkerboard phase transitions seen in\n(a). The striated and star phase in ﬁgures (b) and (c) are very elusive; however, the SNN\ntends to capture clearer phase information than what is suggested by the evaluation of\nthe order parameters. The red line captures the striated phase for Rb > 1.3. There are\ntwo distinct diﬀerences between SNN and QMC order parameter results: There is no\nQMC order parameter signal to explain the orange line. Further, the red line continues\nwell within the checkerboard phase, where none of the three order parameters signal a\nphase transition.\nWe also compare our results to those obtained by [69] in Fig. 11 (d)-(f). Again\nthe SNN prediction of the checkerboard phase transition is in good agreement with the\nDMRG results. Further, the red line for 1.3 < Rb < 1.6 is similar to their striated phase\nboundary. The pink and yellow lines also bound the striated phase. In addition, our\norange line corresponds very well to the DMRG star phase. The DMRG star phase is\nmuch larger than the QMC star phase. However, the SNN results should mimic what\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks19\nis present in the QMC data.\nThis contradiction might be resolved by two diﬀerent\nexplanations: 1) the neural network is able to extract features which are a stronger\nindicator for the star phase than Fourier modes. 2) there is another phase present in\nthe QMC data that is responsible for the orange phase boundary, [75] suggests possible\ncandidates in form of rhombic, banded or staggered order.\nFigure 13: Latent space embeddings of the SNN are applied to conﬁgurations of the Rydberg system\nbelonging to the horizontal slice along Rb = 1.20 in the checkerboard region. Of the 10 embedding\nneurons, some encode phase information along Rb = 1.20. Embeddings 1, 2 and 8 are such neurons,\nrevealing a phase transition near δ = 1.20, which is marked by the black dashed line.\nThe phase\ninformation is markedly partitioned by this line, as can be seen in (a) and the corresponding red\nembedding in (d). This phase transition corresponds to the ﬁrst peak in Fig. 10(c). The second peak\nin Fig. 10(c) is located near Rb = 1.70. Embedding 8 in (c) and the blue histogram in (d) reveal a phase\ntransition near δ = 1.70. (b) shows the encoding of an intermediate phase, with the corresponding\nhistogram in (d) coloured green.\nThe full independent SNN prediction of the Rydberg Array phase diagram is\nconstructed in Fig. 12, where (a) depicts the raw peaks of the horizontal and vertical\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks20\nscans in Fig. 10 and Fig. C4, respectively.\nThe vertical scan slices between δ ∈\n{0.50,...,1.00} do not reveal any explicit phase transition, as expected, and are therefore\nnot included in the phase diagram. Fig. 12(b) is constructed by connecting markers as\nsmoothly as possible. This results in 7 phase regions, labelled according to Fig. 11.\nFig. 13 provides insight into how the SNN encodes the phase information in the\nRydberg system. Certain embeddings correspond to very speciﬁc regions in the phase\ndiagram. Embeddings 1 and 2 separate in the vicinity of the ﬁrst phase transition, and\nwhere embeddings 2 and 8 separate, the SNN indicates a second phase transition.\nIt is surprising that the SNN is able to reveal clearer phase information from the\nQMC data than what a direct evaluation of order parameters might indicate. The reason\nfor this might be that the neural network is able to calculate other thermodynamic\nquantities which are relevant to describe the underlying physics, such as energies\nor susceptibilities.\nFurther, the neural network might be able to deal better with\ndomain boundaries. This observation encourages us to predict that neural networks\nmight be able to reveal phases where conventional order parameter evaluations on MC\nconﬁgurations have trouble doing so. In that sense the SNN predicts the occurrence of 2\nphases that are not evident in the QMC order parameter analysis, the blue and orange\nregions in Fig. 12. These ﬁndings guide us to examine these regions closer with a keen\neye on revealing previously unknown physics.\n5. Conclusion\nWe have introduced a Siamese neural network (SNN) based method to detect phase\nboundaries in an unsupervised manner.\nThis method does not require any physical\nknowledge about the nature or existence of the underlying phases. SNN based phase\ndetection shares the power of a) feed forward neural networks, which have been the\nmost powerful machine learning algorithm to be applied to reveal physical phases,\nand b) certain unsupervised methods which can learn multiple phases without knowing\nabout their existence. This method is shown to reproduce phase diagrams when trained\non Monte-Carlo conﬁgurations of the corresponding physical system. In our case we\nintroduced the method at the example of a model consisting of two stacked Ising Models\nexhibiting a phase diagram of four phases. Further, we used this method to calculate the\nphase diagram of a Rydberg atom array which is to the most extent consistent with prior\nresults, and shows additional signatures of unknown and coexistence phases. Futher, in\nsome regimes the SNN tends to be better at picking up phase information than order\nparameters applied to QMC data. As typical for neural network based phase recognition\nschemes, we do not have insight on what features a neural network is learning in order\nto calculate the phase boundaries. These quantities have been shown to be related to\norder parameters and other physically relevant quantities. Explicitly revealing them is\ndiﬃcult, but possible using methods like [23, 79].\nWith this work we have contributed to the zoo of machine learning methods for\nphase diagrams.\nWhile it remains to be seen if one of these methods will reveal a\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks21\ncompletely new unknown phase, we believe SNN based phase detection has the features\nof a top contender with its ability to detect multiple phases in an unsupervised manner.\n6. Acknowledgements\nWe thank Roger Melko for helpful discussions. We thank the National Research Council\nof Canada for their partnership with Perimeter on the PIQuIL. Research at Perimeter\nInstitute is supported in part by the Government of Canada through the Department of\nInnovation, Science and Economic Development Canada and by the Province of Ontario\nthrough the Ministry of Colleges and Universities.\nAppendix A. Normalized Cosine Dissimilarity\nIn this section we display the calculation of the cosine dissimilarity in Fig. A1.\nFigure A1: (a) We begin with the standard cosine similarity plot. (b) We then compute 1 −cos(θ)\nto obtain the cosine dissimilarity. (c) We normalize to guarantee that the plot ranges from 0 to 1.\nAppendix B. Training\nAppendix B.1. General Training Dynamics\nWe observe that training is prone to getting stuck in local minima. For this reason, we\nmay rerun the training phase several times for certain slices. In particular, the diagonal\nslices (both the shifted diagonal and the direct diagonal) are rerun if their adjacency\ncomparisons exhibit unexpected/sub-optimal behaviour. One may use the loss curve\nto identify such instances. For instance, one run may exhibit a single phase transition,\nwhile the other may exhibit 2. We opt to keep the result with the lower loss. This\nscenario is depicted in Fig. B1. The single peak result in (c) corresponds to a higher\nloss. We identify this as a local minimum. The red curve in (a) corresponds to the plot\n(b). While this run also gets stuck in a local minimum, it is able to escape upon further\ntraining. As such, we keep (b) as our result.\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks22\nFigure B1: We train on Rydberg array conﬁgurations belonging to the slice Rb = 1.50, employing\nearly stopping. (a) The blue loss curve reveals that a single peak, shown in (c), occurs when training\nfalls prey to local minima. Training ceases at around 12 epochs in this case due to the early stopping\ncallback. Furthermore, we observe that the red loss curve succeeds in escaping this minimum, and\nconverges to a lower loss. This lower loss corresponds to dual peaks, shown in (b).\nAppendix B.2. Ising Model - Training on the Diagonal Slice\nThe diagonal slice is shown in Fig. 4(a) and Fig. 7(c). Of the 92 Ising conﬁgurations\nproduced at each temperature, 50 are used in the training set, and 42 in the testing\nset. Then, we use these to produce 100 stacked conﬁgurations at random for both the\ntraining and testing sets. For example, we choose two random conﬁgurations out of the\nset of 50 for the training set, and overlay them to produce a single stacked conﬁguration.\nAppendix B.3. Ising Model - Training on the Shifted Diagonal Slice\nThe shifted diagonal slice is shown in Fig. 7(c). Here, we shift the slice by 25 temperature\npoints vertically in order to cross two phase transitions. Of the 92 Ising conﬁgurations\nproduced at each temperature, 70 are used in the training set, and 22 in the testing\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks23\nset. Once again, we use these to produce stacked conﬁgurations at random for both the\ntraining and testing sets. However, this time we produce 3500 stacked conﬁgurations\nfor the training set, and 500 for the testing set. Because the diagonal slice does not\ncontain many points from any of the three phases it crosses, we increase the amount of\ntraining data to compensate.\nAppendix B.4. Ising Model - Training on the Horizontal/Vertical Slices\nOf the 92 Ising conﬁgurations produced at each temperature, 42 are used in the training\nset, and 50 in the testing set. As with the diagonal slice, we then use these to produce\n100 stacked conﬁgurations at random for both the training and testing sets.\nAppendix B.5. Training the Rydberg System\nTraining on the Rydberg system is performed similar to the Ising model, i.e. slice-wise.\nWe train over Rb ∈{1.00,...,1.80} and δ ∈{0.5,...,2.9}, with 1000 conﬁgurations per\nδ-step. 800 are used for training, and the remaining 200 for testing.\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks24\nAppendix C. Additional Embeddings\nThis section presents the remaining embeddings for the Ising Model’s diagonal and\nshifted-diagonal scans, as well as the Rb = 1.20 horizontal scan.\nAppendix C.1. Ising Model - Diagonal Embeddings\nFigure C1: Latent space embeddings of the SNN are applied to conﬁgurations of the stacked Ising\nModel belonging to the diagonal slice. This slice contains a single phase transition between FF and\nPP, as depicted in Fig. 7(c). Some neurons belonging to these embeddings encode phase information,\nalbeit redundantly. For instance, (j) and (l) both encode the PP phase, but almost identically. Other\nembeddings appear unique, but still encode information about the same phase transition. (j) and (m)\nencode the PP and FF phases, respectively, but the both provide insight into the same phase transition\ndepicted by the black dashed line.\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks25\nAppendix C.2. Ising Model - Shifted Diagonal Embeddings\nFigure C2: Latent space embeddings of the SNN are applied to conﬁgurations of the stacked Ising\nModel belonging to the shifted diagonal slice. This slice contains three regions - FF, PF, and PP, as\ndepicted in (a), (b). Some neurons belonging to these embeddings encode phase information, albeit\nredundantly, while other neurons do not appear to encode anything at all. For instance, (e) and (g)\nboth encode the FF phase, but almost identically. On the other hand, (a), (c), (i), and (k) do not\nencode any phase information.\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks26\nAppendix C.3. Rydberg System\nFigure C3: Latent space embeddings of the SNN are applied to conﬁgurations of the Rydberg system\nbelonging to the horizontal slice along Rb = 1.20 in the checkerboard region. Most neurons encode\nredundant information. For instance, (a) and (c) encode information related to the same phase, while\n(c) and (f) exhibit identical phase structures. We also observe that embedding 9 in (m) encodes no\nphase information.\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks27\nFigure C4:\nWe apply the SNN phase recognition method to the Rydberg array in the region\nRb ∈{1.0,...,1.8} and δ ∈{0.5,...,2.9}.\nWe scan along each vertical slice by choosing a ﬁxed δ,\nand traversing along all Rb ∈{1.00,...,1.80}. We choose k = 1 to perform adjacency comparisons\nwithin these scans. The dissimilarity peaks when conﬁgurations at Rb,i and Rb,i+1 are of diﬀerent\nphases.\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks28\nAppendix D. Additional Adjacency Comparisons\nThis section contains further examines the eﬀect of the hyperparameter k on the\nadjacency comparison.\nFigure D1: We scan across the conﬁgurations of the stacked Ising Model belonging to the diagonal\nslice (d). The adjacency comparison plots (a)-(c) depict the results of this scan using three diﬀerent\nvalues of k. The diagonal line in (d) only crosses the phase boundaries once in the center (FF to PP),\nand we correctly observe a single corresponding peak in (a)-(c). Higher values of k appear to smooth\nout noise. The trajectory of (a) is interrupted by a hump between T = 2.4...2.7, which gradually lessens\nthrough to (c).\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks29\nFigure D2: We scan across the conﬁgurations of the stacked Ising Model belonging to the shifted\ndiagonal slice (d). The adjacency comparison plots (a)-(c) depict the results of this scan using three\ndiﬀerent values of k. The diagonal line in (d) only crosses the phase boundaries once in the center (FF\nto PP), and we correctly observe a single corresponding peak in (a)-(c). Higher values of k appear to\nsmooth out noise. For instance, the FP phase of (a) is characterized by noise, which gradually lessens\nthrough to (c).\n[1] Krizhevsky A, Sutskever I and Hinton G E 2012 Advances in neural information processing systems\n25\n[2] Goldberg Y 2016 Journal of Artiﬁcial Intelligence Research 57 345–420\n[3] Carrasquilla J and Melko R G 2017 Nature Physics 13 431–434\n[4] van Nieuwenburg E P L, Liu Y H and Huber S D 2017 Nature Physics 13 435–439\n[5] Wang L 2016 Physical Review B 94\n[6] Wetzel S J 2017 Physical Review E 96\n[7] Zhang Y and Kim E A 2017 Physical Review Letters 118\n[8] Schindler F, Regnault N and Neupert T 2017 Physical Review B 95\n[9] Hu W, Singh R R P and Scalettar R T 2017 Physical Review E 95\n[10] Ohtsuki T and Ohtsuki T 2017 Journal of the Physical Society of Japan 86 044708\n[11] Broecker P, Carrasquilla J, Melko R G and Trebst S 2017 Scientiﬁc Reports 7\n[12] Deng D L, Li X and Sarma S D 2017 Physical Review B 96\n[13] Ch’ng K, Carrasquilla J, Melko R G and Khatami E 2017 Physical Review X 7\n[14] Torlai G and Melko R G 2016 Physical Review B 94\n[15] Carleo G and Troyer M 2017 Science 355 602–606\n[16] Inack E M, Santoro G E, Dell’Anna L and Pilati S 2018 Phys. Rev. B 98(23) 235145 URL\nhttps://link.aps.org/doi/10.1103/PhysRevB.98.235145\n[17] Hibat-Allah M, Ganahl M, Hayward L E, Melko R G and Carrasquilla J 2020 (Preprint http:\n//arxiv.org/abs/2002.02973v2)\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks30\n[18] Carrasquilla J, Luo D, P´erez F, Milsted A, Clark B K, Volkovs M and Aolita L 2019 (Preprint\nhttp://arxiv.org/abs/1912.11052v1)\n[19] Ferrari F, Becca F and Carrasquilla J 2019 Physical Review B 100\n[20] Sharir O, Levine Y, Wies N, Carleo G and Shashua A 2020 Physical Review Letters 124\n[21] Schmidt M and Lipson H 2009 Science 324 81–85\n[22] Iten R, Metger T, Wilming H, del Rio L and Renner R 2020 Physical Review Letters 124\n[23] Wetzel S J and Scherzer M 2017 Physical Review B 96\n[24] Ponte P and Melko R G 2017 Physical Review B 96\n[25] Wetzel S J, Melko R G, Scott J, Panju M and Ganesh V 2020 Physical Review Research 2 033499\n[26] Greitemann J, Liu K and Pollet L 2019 Phys. Rev. B 99(6) 060404 URL https://link.aps.org/\ndoi/10.1103/PhysRevB.99.060404\n[27] ichi Mototake Y 2019 (Preprint http://arxiv.org/abs/2001.00111v1)\n[28] Udrescu S M and Tegmark M 2019 (Preprint http://arxiv.org/abs/1905.11481v1)\n[29] Krenn M, Hochrainer A, Lahiri M and Zeilinger A 2017 Physical review letters 118 080401\n[30] Krenn M, Pollice R, Guo S Y, Aldeghi M, Cervera-Lierta A, Friederich P, Gomes G d P, H¨ase F,\nJinich A, Nigam A et al. 2022 arXiv preprint arXiv:2204.01467\n[31] Dawid A, Arnold J, Requena B, Gresch A, P lodzie´n M, Donatella K, Nicoli K, Stornati P, Koch\nR, B¨uttner M et al. 2022 arXiv preprint arXiv:2204.04198\n[32] Carrasquilla J 2020 Advances in Physics: X 5 1797528\n[33] Carleo G, Cirac I, Cranmer K, Daudet L, Schuld M, Tishby N, Vogt-Maranto L and Zdeborov´a L\n2019 Reviews of Modern Physics 91 045002\n[34] Huembeli P, Dauphin A and Wittek P 2018 Physical Review B 97 134109\n[35] Arnold J and Sch¨afer F 2022 arXiv preprint arXiv:2203.06084\n[36] van Nieuwenburg E, Bairey E and Refael G 2018 Physical Review B 98 060301\n[37] Hsu Y T, Li X, Deng D L and Sarma S D 2018 Physical Review Letters 121 245701\n[38] Vargas-Hern´andez R A, Sous J, Berciu M and Krems R V 2018 Physical review letters 121 255702\n[39] Bachtis D, Aarts G and Lucini B 2020 Physical Review E 102 033303\n[40] Beach M J, Golubeva A and Melko R G 2018 Physical Review B 97 045207\n[41] Zhang W, Liu J and Wei T C 2019 Physical Review E 99 032142\n[42] Suchsland P and Wessel S 2018 Phys. Rev. B 97(17) 174435 URL https://link.aps.org/doi/\n10.1103/PhysRevB.97.174435\n[43] Kim D and Kim D H 2018 Physical Review E 98 022138\n[44] Lian W, Wang S T, Lu S, Huang Y, Wang F, Yuan X, Zhang W, Ouyang X, Wang X, Huang X\net al. 2019 Physical review letters 122 210503\n[45] Dong X Y, Pollmann F, Zhang X F et al. 2019 Physical Review B 99 121104\n[46] Giannetti C, Lucini B and Vadacchino D 2019 Nuclear Physics B 944 114639\n[47] Ohtsuki T and Mano T 2020 Journal of the Physical Society of Japan 89 022001\n[48] Casert C, Vieijra T, Nys J and Ryckebusch J 2019 Physical Review E 99 023304\n[49] Zhang R, Wei B, Zhang D, Zhu J J and Chang K 2019 Physical Review B 99 094427\n[50] Singh V K and Han J H 2019 Physical Review B 99 174426\n[51] Arnold J, Sch¨afer F, ˇZonda M and Lode A U 2021 Physical Review Research 3 033052\n[52] Liu Y H and Van Nieuwenburg E P 2018 Physical review letters 120 176401\n[53] Huembeli P, Dauphin A, Wittek P and Gogolin C 2019 Physical review B 99 104106\n[54] K¨aming N, Dawid A, Kottmann K, Lewenstein M, Sengstock K, Dauphin A and Weitenberg C\n2021 Machine Learning: Science and Technology 2 035037\n[55] Alexandrou C, Athenodorou A, Chrysostomou C and Paul S 2020 The European Physical Journal\nB 93 1–15\n[56] Yin J, Pei Z and Gao M C 2021 Nature Computational Science 1 686–693\n[57] Greplova E, Valenti A, Boschung G, Sch¨afer F, L¨orch N and Huber S D 2020 New Journal of\nPhysics 22 045003\n[58] Kharkov Y A, Sotskov V, Karazeev A, Kiktenko E O and Fedorov A K 2020 Physical Review B\nUnsupervised Learning of Rydberg Atom Array Phase Diagram with Siamese Neural Networks31\n101 064406\n[59] Ch’ng K, Vazquez N and Khatami E 2018 Physical Review E 97 013306\n[60] Wang C and Zhai H 2017 Physical Review B 96 144432\n[61] Kottmann K, Huembeli P, Lewenstein M and Ac´ın A 2020 Physical Review Letters 125 170603\n[62] Jadrich R, Lindquist B and Truskett T 2018 The Journal of chemical physics 149 194109\n[63] Che Y, Gneiting C, Liu T and Nori F 2020 Physical Review B 102 134213\n[64] Bromley J, Guyon I, LeCun Y, S¨ackinger E and Shah R 1993 Advances in neural information\nprocessing systems 6 737–744\n[65] Baldi P and Chauvin Y 1993 neural computation 5 402–418\n[66] Henriet L, Beguin L, Signoles A, Lahaye T, Browaeys A, Reymond G O and Jurczak C 2020\nQuantum 4 327 ISSN 2521-327X URL https://doi.org/10.22331/q-2020-09-21-327\n[67] Browaeys A and Lahaye T 2020 Nature Physics 16 132–142 URL https://doi.org/10.1038/\ns41567-019-0733-z\n[68] Kalinowski M, Samajdar R, Melko R G, Lukin M D, Sachdev S and Choi S 2021 arXiv preprint\narXiv:2112.10790\n[69] Ebadi S, Wang T T, Levine H, Keesling A, Semeghini G, Omran A, Bluvstein D, Samajdar R,\nPichler H, Ho W W, Choi S, Sachdev S, Greiner M, Vuleti´c V and Lukin M D 2021 Nature 595\n227–232 ISSN 1476-4687 URL http://dx.doi.org/10.1038/s41586-021-03582-4\n[70] Miles C, Samajdar R, Ebadi S, Wang T T, Pichler H, Sachdev S, Lukin M D, Greiner M, Weinberger\nK Q and Kim E A 2021 arXiv preprint arXiv:2112.10789\n[71] Torlai G, Timar B, Van Nieuwenburg E P, Levine H, Omran A, Keesling A, Bernien H, Greiner\nM, Vuleti´c V, Lukin M D et al. 2019 Physical review letters 123 230504\n[72] Carrasquilla J and Torlai G 2021 PRX Quantum 2 040201\n[73] Czischek S, Moss M S, Radzihovsky M, Merali E and Melko R G 2022 arXiv preprint\narXiv:2203.04988\n[74] Browaeys A, Barredo D and Lahaye T 2016 Journal of Physics B: Atomic, Molecular and Optical\nPhysics 49 152001 URL https://doi.org/10.1088/0953-4075/49/15/152001\n[75] Samajdar R, Ho W W, Pichler H, Lukin M D and Sachdev S 2020 Physical Review Letters 124\nISSN 1079-7114 URL http://dx.doi.org/10.1103/physrevlett.124.103601\n[76] Merali E, De Vlugt I J and Melko R G 2021 arXiv preprint arXiv:2107.00766\n[77] Newman M E J and Barkema G T 1999 Monte Carlo methods in statistical physics (Clarendon\nPress)\n[78] SchroﬀF, Kalenichenko D and Philbin J 2015 CoRR abs/1503.03832 (Preprint 1503.03832)\nURL http://arxiv.org/abs/1503.03832\n[79] Miles C, Bohrdt A, Wu R, Chiu C, Xu M, Ji G, Greiner M, Weinberger K Q, Demler E and Kim\nE A 2021 Nature Communications 12 1–7\n",
  "categories": [
    "physics.comp-ph",
    "cond-mat.quant-gas",
    "cs.LG",
    "quant-ph"
  ],
  "published": "2022-05-09",
  "updated": "2022-05-19"
}