{
  "id": "http://arxiv.org/abs/2205.06571v1",
  "title": "Convergence Analysis of Deep Residual Networks",
  "authors": [
    "Wentao Huang",
    "Haizhang Zhang"
  ],
  "abstract": "Various powerful deep neural network architectures have made great\ncontribution to the exciting successes of deep learning in the past two\ndecades. Among them, deep Residual Networks (ResNets) are of particular\nimportance because they demonstrated great usefulness in computer vision by\nwinning the first place in many deep learning competitions. Also, ResNets were\nthe first class of neural networks in the development history of deep learning\nthat are really deep. It is of mathematical interest and practical meaning to\nunderstand the convergence of deep ResNets. We aim at characterizing the\nconvergence of deep ResNets as the depth tends to infinity in terms of the\nparameters of the networks. Toward this purpose, we first give a matrix-vector\ndescription of general deep neural networks with shortcut connections and\nformulate an explicit expression for the networks by using the notions of\nactivation domains and activation matrices. The convergence is then reduced to\nthe convergence of two series involving infinite products of non-square\nmatrices. By studying the two series, we establish a sufficient condition for\npointwise convergence of ResNets. Our result is able to give justification for\nthe design of ResNets. We also conduct experiments on benchmark machine\nlearning data to verify our results.",
  "text": "arXiv:2205.06571v1  [cs.LG]  13 May 2022\nConvergence Analysis of Deep Residual Networks\nWentao Huang∗\nand\nHaizhang Zhang†\nAbstract\nVarious powerful deep neural network architectures have made great contribution to the exciting\nsuccesses of deep learning in the past two decades. Among them, deep Residual Networks (ResNets)\nare of particular importance because they demonstrated great usefulness in computer vision by\nwinning the ﬁrst place in many deep learning competitions. Also, ResNets were the ﬁrst class\nof neural networks in the development history of deep learning that are really deep.\nIt is of\nmathematical interest and practical meaning to understand the convergence of deep ResNets. We\naim at characterizing the convergence of deep ResNets as the depth tends to inﬁnity in terms of\nthe parameters of the networks. Toward this purpose, we ﬁrst give a matrix-vector description of\ngeneral deep neural networks with shortcut connections and formulate an explicit expression for\nthe networks by using the notions of activation domains and activation matrices. The convergence\nis then reduced to the convergence of two series involving inﬁnite products of non-square matrices.\nBy studying the two series, we establish a suﬃcient condition for pointwise convergence of ResNets.\nOur result is able to give justiﬁcation for the design of ResNets. We also conduct experiments on\nbenchmark machine learning data to verify our results.\nKeywords: deep learning, deep residual networks, ReLU networks, convolution neural networks,\nResNets, convergence\n1\nIntroduction\nIn the past two decades, people have witnessed a series of major breakthroughs of artiﬁcial intelligence\non a wide range of machine learning problems including face recognition, speech recognition, game\nintelligence, natural language processing, and autonomous navigation, [13, 21]. The breakthroughs\nare brought mainly by deep learning in which there are four major ingredients that contribute to the\nsuccesses. The ﬁrst three of them are the availability of vast amounts of training data, recent dramatic\nimprovements in computing and storage power from computer engineering, and eﬃcient numerical\nalgorithms such as the Stochastic Gradient Decent (SGD) algorithms, Adaptive Boosting (AdaBoost)\nalgorithms, and the Expectation-Maximization algorithm (EM), etc. The last ingredient, which is\ngenerally considered to the most important one, is a class of deep neural network architectures, such\nas Convolutional Neural Networks (CNN), LongShort Time Memory (LSTM) networks, Recurrent\nNeural Networks (RNN), Generative Adversarial Networks (GAN), Deep Belief Networks (DBN),\nAlexNet [2], VGG-Net [19], GoogleLeNet/Inception [5], and Residual Networks (ResNet) [16, 17].\n∗School\nof\nMathematics\n(Zhuhai),\nSun\nYat-sen\nUniversity,\nZhuhai,\nP.R.\nChina.\nE-mail\naddress:\nhuangwt55@mail2.sysu.edu.cn.\n†School of Mathematics (Zhuhai), Sun Yat-sen University, Zhuhai, P.R. China. E-mail address: zhhaizh2@sysu.edu.cn.\nSupported in part by National Natural Science Foundation of China under grant 11971490, and by Natural Science\nFoundation of Guangdong Province under grant 2018A030313841. Corresponding author.\n1\n2\nStimulated by the great achievements of deep learning, many mathematicians have been fascinated\nby DNNs, which is viewed as a nonlinear representation system of functions. Most mathematical\nstudies of DNNs are focused on their approximation and expressive powers in representing diﬀerent\nclasses of functions, [1, 6, 7, 8, 9, 24, 25, 27, 29, 30, 31, 33, 37, 40]. We refer readers to the two\nrecent surveys [7, 9] for a detailed introduction and discussion. Two pieces of work most related to\nour current study are [35] and [36], which ﬁrst investigated the convergence of DNNs in terms of the\nweight matrices and bias vectors. Speciﬁcally, a suﬃcient condition for pointwise convergence was\nestablished for general DNNs with a ﬁxed width and for CNNs with increasing width in [35] and [36],\nrespectively.\nOur current study targets at the Residual Network (ResNet). ResNet becomes well-known when\nit won the ﬁrst place in the ILSVRC 2015 classiﬁcation competition, in which it achieved a 3.57%\ntop-5 error on the ImageNet test set. It was also the ﬁrst DNN architecture that is really deep. It\ncontained 152 layers while the champions of ImageNet classiﬁcation competition in 2013 and 2014,\nGoogleNet and VGG-Net contained 22 and 19 layers, respectively. ResNet also won the ﬁrst place on\nImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC\n& COCO 2015 competitions. ResNet hence deserves special attention for these reasons. Indeed, there\nhave been many researches on the generalization ability, approximation power, and optimization of\nResNets [18, 15, 3, 41, 12, 23, 22, 28, 39] with the hope of increasing the interpretability of ResNets.\nFor instance, [18] partially justiﬁed the advantages of deep ResNets over deep FFNets in generalization\nabilities by comparing the kernel of deep ResNets with that of deep FFNets. It was proved in [22]\nthat a very deep ResNet with the ReLU activation function and stacked modules having one neuron\nper hidden layer can uniformly approximate any Lebesgue integrable function. And a generalization\nbound of ResNets was obtained in [15], which guarantees the performance of ResNet on unseen data.\nOur series of studies [35, 36] and the current one are devoted to the convergence of DNNs as\nthe depth of networks tends to inﬁnity. Convergence of a linear expansion of functions such as the\nFourier and wavelet series has always been a fundamental problem in pure and applied mathematics.\nUnderstanding the convergence for the nonlinear system of DNNs is helpful to reveal the mathematical\nmystery of deep learning and will be useful in guiding the design of more DNN architectures. We have\nmentioned that [35, 36] considered the convergence of a general DNN with a ﬁxed width and CNN\nwith increasing width, respectively. Our study is diﬀerent and valuable in the following aspects:\n1. ResNets are special in containing residual blocks and shortcut connections. These two structures\nwere not investigated in [35, 36].\n2. The two pieces of work [35, 36] also studied DNNs with a single channel only, while ResNets\nand many other useful DNN architectures have multiple channels. Multiple channels will bring\nmajor mathematical diﬃculties in formulating an explicit expression of the function determined\nby a ResNet.\n3. Convergence of ResNets is of particular interest and can justify the original idea in designing the\nResNet [16, 17]. In some applications, the ResNet designed contained more than 1000 layers.\nThe ability of ResNets in accommodating so many layers lies in the design of ResNet that for\na network with so many layers to approximate a function of real interest, the function in deep\nlayers must be close to the identity mapping [17]. The result on convergence of ResNets to be\nestablished in this paper will be able to justify this design.\n4. We will carry out experiments based on well-known machine leaning data to verify the results\nof the paper.\n3\nThe rest of this paper is organized as follows. In Section 2, we shall review the deﬁnition and\nnotation of convolution with zero padding and deﬁne the notation of vectorized version of convolutions.\nIn Section 3, we shall describe the matrix form of ResNets with vectorized version of convolution in\nSection 2, which expresses a ResNet as a special form of DNNs with shortcut connections. In Section\n4, we shall deﬁne the notion of convergence of DNNs with shortcut connections when new layers are\npaved to the existing network so that the depth is increasing to inﬁnity. Then by introducing the\nnotions of activation domains and activation matrices, we derive an explicit expression of deep ReLU\nnetwork with shortcut connections. With the expression, we connect the convergence problem with the\nexistence of two limits involving inﬁnite products of matrices. Suﬃcient conditions for convergence of\nsuch inﬁnite products of matrices are established in Section 5. As consequence, a suﬃcient condition\nfor pointwise convergence of DNNs with shortcut connections is obtained in the same section. Based on\nthose results, we shall establish convenient suﬃcient conditions for convergence of ResNets in Section\n6. Finally, we shall justify our theoretical results with numerical experiments on well-known machine\nlearning data.\n2\nConvolution with Zero Padding\nBefore reviewing the structure of ResNets, we ﬁrst explore the matrix form of the convolution operation\nwith zero padding which will be used to formulate a matrix form of ResNets in the next section.\n2.1\nConvolution of Vectors with Zero Padding in Single Channel\nWe will ﬁrst examine the operation of convolution of vectors with zero padding in single channle.\nLet f ∈N and assume w := (w0, w1, · · · , w2f) ∈R2f+1 is ﬁlter mask with size 2f + 1 and\nx := (x1, x2, · · · , xd) ∈Rd.\nThen the convolution with zero padding of x ∗w outputs a vector\ny := (y1, y2, · · · , yd) ∈Rd, which preserves the dimension of x, deﬁned by\nyi =\nmin(d−i,f)\nX\nj=max(1−i,−f)\nxi+jwf−j, 1 ≤i ≤d.\nIt is convenient to express the convolution x ∗w as multiplication of x with a matrix corresponding\nto w. To this end, we deﬁne a Toeplitz type matrix T(w) ∈Rd×d by\nT(w)i,j :=\n(\nwf−j+i, −f ≤j −i ≤f\n0, otherwise\nand rewrite\ny = x ∗w = T(w) Vec (x).\nHere, Vec (x) is a vectorized version of x deﬁned as follows.\nDeﬁnition 2.1 (The vec operator) Assume x := (x1, x2, · · · , xn) ∈Rm×n and xj :=\n(x1,j, · · · , xm,j)T ∈Rm, 1 ≤j ≤n. Then Vec (x) ∈Rmn is deﬁned by\nVec (x) :=\n\n\nx1\nx2\n...\nxn\n\n.\n4\nWe shall assume f + 1 ≤d since the size of the ﬁlter mask is much less than the size of data in\nreal applications. Under this assumption, the matrix T(w) can be expressed as\nT(w) :=\n\n\nwf\nwf−1\n· · ·\nw0\n0\n· · ·\n0\nwf+1\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nw2f\n...\n...\n...\n...\n...\nw0\n0\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n0\n· · ·\n· · ·\nw2f\n· · ·\n· · ·\nwf\n\n\n∈Rd×d.\n(2.1)\nNote that the entries of T(w) along each of the diagonal and sub-diagonals are constant.\n2.2\nConvolution of Matrices with Zero Padding in Single Channel\nWe shall further formulate convolution of matrices with zero padding in single channel.\nAssume w := (wT\n0 , wT\n1 , · · · , wT\n2f)T ∈R(2f+1)×(2f+1), where wi := (wi,0, wi,1, · · · , wi,2f) ∈R2f+1, 0 ≤\ni ≤2f + 1, and x := (xT\n1 , xT\n2 , · · · , xT\nd )T ∈Rd×d, where xi := (xi,1, xi,2, · · · , xi,d) ∈Rd, 1 ≤i ≤d.\nThen the convolution x ∗w with zero padding outputs a matrix y := (yT\n1 , yT\n2 , · · · , yT\nd )T ∈Rd×d with\nyi := (yi,1, yi,2, · · · , yi,d) ∈Rd, 1 ≤i ≤d, deﬁned by\nyi,j =\nmin(d−i,f)\nX\nk1=max(1−i,−f)\nmin(d−j,f)\nX\nk2=max(1−j,−f)\nxi+k1,j+k2wf−k1,f−k2, 1 ≤i, j ≤d.\nNotice that Vec (xT ) = (x1, x2, · · · , xd)T ∈Rd2 and Vec (yT ) = (y1, y2, · · · , yd)T ∈Rd2 are column\nvectors and thus we could rewrite x ∗w in a matrix-vector multiplication form by\nVec (yT ) = Vec ((x ∗w)T )\n= T(w) Vec (xT )\n=\n\n\nT(wf)\nT(wf−1)\n· · ·\nT(w0)\n0\n· · ·\n0\nT(wf+1)\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nT(w2f)\n...\n...\n...\n...\n...\nT(w0)\n0\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n0\n· · ·\n· · ·\nT(w2f)\n· · ·\n· · ·\nT(wf)\n\n\nVec (xT ).\n(2.2)\nHere, the matrix T(w) ∈Rd2×d2 is a Toeplitz type matrix with block Toeplitz structure and T(wi) ∈\nRd×d are deﬁned as in (2.1).\n5\n2.3\nConvolution of Matrices with Zero Padding in Multiple Channels\nWith the above preparations, we could now give the matrix-vector multiplication form of covolution\nwith zero padding for matrices in multiple channels.\nLet cin and cout denote the number of input and output channels, respectively.\nAssume x :=\n(x1, · · · , xcin) ∈Rd×d×cin with each xi ∈Rd×d, w := (w1, · · · , wcout) where wi := (wi,1, wi,2, · · · , wi,cin) ∈\nR(2f+1)×(2f+1)×cin, 1 ≤i ≤cout with each wi,j ∈R(2f+1)×(2f+1), and y := (y1, y2, · · · , ycout) ∈\nRd×d×cout with each yi ∈Rd×d. For convolution of matrices in multiple channels, we deﬁne\nVec (yT\ni ) =\ncin\nX\nj=1\nT(wi,j) Vec (xT\nj ), 1 ≤i ≤cout\nand\nVec (x) := ( Vec (xT\n1 )T , Vec (xT\n2 )T , · · · , Vec (xT\ncin)T )T ∈Rd2cin,\nVec (y) := ( Vec (yT\n1 )T , Vec (yT\n2 )T , · · · , Vec (yT\ncout)T )T ∈Rd2cout.\nThen the matrix-vector multiplication form of convolution of matrices with zero padding in multiple\nchannels is given as follows:\nVec (y) = W Vec (x),\nwhere W = T(w) =\n\n\nT(w1,1)\nT(w1,2)\n· · ·\nT(w1,cin)\nT(w2,1)\nT(w2,2)\n· · ·\nT(w2,cin)\n...\n...\n...\n...\nT(wcout,1)\nT(wcout,2)\n· · ·\nT(wcout,cin)\n\n.\n(2.3)\nNotice that W ∈Rd2cout×d2cin is a block matrix and every block in W is deﬁned by (2.2).\nIn conclusion, we have formulated a matrix form for convolution of matrices with zero padding in\nmulti-channels in ResNets. It will be helpful for us to derive an explicit expression for the function\ndetermined by a ResNet.\n3\nDeep Residual Networks\nIn order to study the convergence of ResNets, we consider in this section a matrix form of ResNets.\nLet us start with the pure convolution structure with shortcut connection of a ReLU neural network.\nLet σ denote the ReLU activation function\nσ(x) := max(x, 0), x ∈R.\nA ResNet as proposed in [16] with n building blocks may be illustrated as follows:\nx ∈[0, 1]d×d×cin\nws,bs\n−−−−→\nσ\nx(0)\nR(1)\nc(1),f(1),q1\n−−−−−−−→x(1) −→· · · −→\nR(n)\nc(n),f(n),qn\n−−−−−−−−→x(n)\nAp\n−−−−→\nWo,bo y ∈Rcout.\ninput\nsampling\nresidual blocks\noutput\n(3.1)\nWe shall explain the above structure in details. In the sampling layer, it holds\nx(0) = σ(x ∗ws + bs),\n(3.2)\n6\nwhere ws and bs denote the ﬁlter mask and bias vector of the sampling layer, respectively. And in\nthe output layer, it holds\nVec (y) = Wo Ap ( Vec (x(n))) + bo,\n(3.3)\nwhere Wo and bo are the weight matrix and bias vector of the output layer, and Ap denotes an\nglobal average pooling operation. It has been revealed in [26] that max-pooling is not necessary in an\nall-convolutional residual network. Thus we shall only consider the global average pooling.\nThe most complicate structure lies in the residual blocks x(k−1) →x(k), 1 ≤k ≤n. It is represented\nby a nonlinear operator\nx(k) = R(k)\nc(k),f (k),qk(x(k−1)),\n(3.4)\nwhere c(k) := (c(k)\n0 , · · · , c(k)\nqk ) ∈Nqk+1\n+\n, f (k) := (f (k)\n1 , · · · , f (k)\nqk ) ∈Nqk\n+ , and qk ∈N denote the numbers of\nchannels at each layer, the sizes of ﬁlter masks, and the depth of k-th residual block, respectively. Note\nthat in most applications, cin ≤c(k)\nm for all 1 ≤k ≤n, 1 ≤m ≤qn. We shall make this assumption\nthroughout the paper. Also, c(k)\nqk = c(k)\n0\nso that the dimensions are match in vector additions. The\nnonlinear operator R(k)\nc(k),f (k),qk may be illustrated by\nx(k−1) = x(k−1)\n0\nw(k)\n1\n,b(k)\n1\n−−−−−−→\nσ\nx(k−1)\n1\nw(k)\n2\n,b(k)\n2\n−−−−−−→\nσ\nx(k−1)\n2\n−→· · · −→\nw(k)\nqk ,b(k)\nqk ,+x(k−1)\n−−−−−−−−−−−→\nσ\nx(k−1)\nqk\n= x(k)\ninput\n1st layer\n2nd layer\nqk-th layer\n(3.5)\nand mathematically written as\nx(k−1)\ni\n:= σ(x(k−1)\ni−1\n∗w(k)\ni\n+ b(k)\ni\n) ∈Rd×d×c(k)\ni , 1 ≤i ≤qk −1,\nx(k) = x(k−1)\nqk\n:= σ(x(k−1)\nqk−1 ∗w(k)\nqk + x(k−1) + b(k)\ni\n) ∈Rd×d×c(k)\nqk .\n(3.6)\nIn the above, w(k)\ni\n:= (w(k)\ni,1 , w(k)\ni,2 , · · · , w(k)\ni,c(k)\ni\n) ∈R(2f(k)\ni\n+1)×(2f(k)\ni\n+1)×c(k)\ni\nand b(k)\ni\n:= (b(k)\ni,1 Ed, · · · ,\nb(k)\ni,c(k)\ni\nEd), where Ed denotes the d × d all-ones matrix and b(k)\ni,j ∈R (1 ≤j ≤c(k)\ni\n), are the ﬁlter mask\nand the bias vector at the i-th layer of the k-th residual block, respectively.\nBy the matrix languages introduced in Section 2, we can express all the convolution operations\nabove as matrix-vector multiplications. Let ex := Vec (x) ∈Rd2cin, ex(k) := Vec (x(k)) ∈Rd2ck\n0, and\ney := Vec (y) = y ∈Rd2cout. Equations (3.2)-(3.4), and (3.4) can then be rewritten as\nWs := T(ws),\nebs := Vec (bs),\nex(0) := σ(Wsex + ebs),\ney := Wfc Ap (ex(n)) + bfc,\nex(k) := N (k)\nc(k),f (k),qk(ex(k−1)), 1 ≤k ≤n,\n(3.7)\nand\nW(k)\ni\n:= T(w(k)\ni\n),\neb(k)\ni\n:= Vec (b(k)\ni\n),\nex(k−1)\ni\n:= σ(W(k)\ni\nex(k−1)\ni−1\n+ eb(k)\ni\n) ∈Rd2c(k)\ni , 1 ≤i ≤qk −1,\nex(k) = N (k)\nc(k),f (k),qk(ex(k−1)) = ex(k−1)\nqk\n= σ(W(k)\nqk ex(k−1)\nqk−1 + ex(k−1) + eb(k)\ni\n) ∈Rd2c(k)\nqk .\n(3.8)\nWith the above matrix forms, conditions on the ﬁlter masks and bias vectors that ensure the con-\nvergence of network (3.1) as the number of residual blocks n tends to inﬁnity, can then be reformulated\n7\nas conditions on the weight matrices and bias vectors. Convergence of deep ReLU neural networks\nand deep ReLU CNNs was recently studied in [35, 36]. The matrix form for ResNets diﬀers from\nthose for DNNs and CNNs considered there in that the ResNets have shortcut connections while the\nDNNs in [35] or CNNs in [36] do not. Also, the ResNets we consider have multiple-channels while the\nnetworks in [35, 36] both have a single channel. These diﬀerences cause major diﬃculties that will be\nclear as we proceed with the analysis in the next sections. As a consequences, the results in [35, 36]\ncould not be directly applied to the current setting.\n4\nDeep Neural Networks with Shortcut connections\nSince a ResNet (3.1) is a special deep ReLU neural network with shortcut connections, we shall ﬁrst\nstudy convergence of general fully connected feed-forward neural networks with shortcut connections.\nThe result obtained will then be applied to establish the convergence of ResNets. Toward this purpose,\nwe shall describe general fully connected neural networks with shortcut connections and formulate their\nconvergence as convergence of inﬁnite products of non-square matrices.\nLet din, dout, dres be the dimension of the input space, output space, and the domain of residual\nblocks, respectively. We shall always assume din ≤dres as this is the case in real applications. The\nweight matrix Ws and the bias vector bs of the sampling layer satisfy Ws ∈Rdres×din and bs ∈Rdres.\nFor each 1 ≤k ≤n, let W(k) := (W(k)\n1 , · · · , W(k)\nqk ) and b(k) := (b(k)\n1 , · · · , b(k)\nqk ) denote the weight\nmatrices and bias vectors in the k-th residual layer, and c(k) = (c(k)\n0 , · · · , c(k)\nqk ) ∈Nqk+1\n+\ndenotes the\ndimensions of outputs in the k-th residual layer. That is, b(k)\ni\n∈Rc(k)\ni\nfor 1 ≤i ≤qk, 1 ≤k ≤n, and\nW(k)\ni\n∈Rc(k)\ni\n×c(k)\ni−1 for 1 ≤i ≤qk, 1 ≤k ≤n. Note that c(k)\n0\n= c(k)\nqk = dres for 1 ≤i ≤qk, 1 ≤k ≤n.\nThe weight matrix Wo and the bias vector bo of the output layer satisfy Wo ∈Rdout×dres and\nbo ∈Rdout.\nAnd N (k)\nc(k),qk denotes the nonlinear operator determined by the k-th residual blocks,\n1 ≤k ≤n.\nThe structure of such a deep neural network with shortcut connections and the ReLU activation\nfunction σ is illustrated as follows:\nx ∈[0, 1]din\nWs,bs\n−−−−→\nσ\nx(0)\nN (1)\nc(1),q1\n−−−−−→x(1) −→· · · −→\nN (n)\nc(n),qn\n−−−−−→x(n) Wo,bo\n−−−−→y ∈Rdout.\ninput\nsampling\nresidual blocks\noutput\n(4.1)\nwhere the kth residual block is illustrated by\nx(k−1) ∈Rdres\nW(k)\n1\n,b(k)\n1\n−−−−−−→\nσ\nx(k−1)\n1\n−→· · · −→\nW(k)\nqk ,b(k)\nqk ,+x(k−1)\n−−−−−−−−−−−−→\nσ\nx(k) ∈Rdres, 1 ≤k ≤n.\ninput\n1st layer\nqk-th layer\n(4.2)\nHere\nx(0) := σ(Wsx + bs), y := Wox(n) + bo,\nx(k) := N (k)\nc(k),qk(x(k−1)), 1 ≤k ≤n,\n(4.3)\nand N (k)\nc(k),qk is given by\nx(k−1)\n0\n:= x(k−1), x(k−1)\ni\n:= σ(W(k)\ni\nx(k−1)\ni−1\n+ b(k)\ni\n), 1 ≤i ≤qk −1,\nx(k) = N (k)\nc(k),qk(x(k−1)) := σ(W(k)\nqk x(k−1)\nqk−1 + x(k−1) + b(k)\nqk−1).\n(4.4)\n8\nNote that the sampling layer could be treated as a single layer residual block with the weight matrix\nW(0)\n1\n:=\n\u0002\nWs\n0\n\u0003\n−Idres ∈Rdres×dres, where Idres denotes the dres × dres identity matrix and bias\nvector b(0)\n1\n:= bs ∈Rdres. For convenience, we may adjust the input x to x(−1) :=\n\u0014x\n0\n\u0015\n∈Rdres, and\nrewrite equations (4.3) and (4.4) as:\nx(k) = N (k)\nc(k),qk(x(k−1)), 0 ≤k ≤n\n(4.5)\nand\nx(−1)\n0\n:= x(−1), x(k−1)\n0\n:= x(k−1),\nx(k−1)\ni\n:= σ(W(k)\ni\nx(k−1)\ni−1\n+ b(k)\ni\n), 1 ≤i ≤qk −1,\nx(k) := σ(W(k)\nqk x(k−1)\nqk−1 + x(k−1) + b(k)\nqk )\n0 ≤k ≤n,\n(4.6)\nwhere q0 = 1, c(0) := (c(0)\n0 , c(0)\n1 ) with c(0)\n0\n= din, c(0)\n1\n= dres, W(0) := (W(0)\n1 ), and b(0) := (b(0)\n1 ).\nTo formulate an expression for the continuous function determined by the deep neural network (4.1)\nwith shortcut connections, we recall the compact notation for consecutive compositions of functions\nwhich was used in [35].\nDeﬁnition 4.1 (Consecutive composition) Let f1, f2, · · · , fn be a ﬁnite sequence of functions\nsuch that the range of fi is contained in the domain of fi+1, 1 ≤i ≤n−1, the consecutive composition\nof {fi}n\ni=1 is deﬁned to be a function\nn\nK\ni=1\nfi := fn ◦fn−1 ◦· · · ◦f2 ◦f1,\nwhose domain is that of f1. For convenience, we also make the convention (Jn\ni=1 fi)(x) = x if n < 1.\nUsing the above notation, equation (4.5) and (4.6) may be rewritten as\nx(k) =\n \nk\nK\nm=0\nN (m)\nc(m),qm\n!\n(x(−1)), 0 ≤k ≤n,\n(4.7)\nand\nx(k−1)\ni\n=\n \ni\nK\nm′=1\nσ(W(k)\nm′ · +b(k)\nm′ )\n!\n(x(k−1)\n0\n), 0 ≤i ≤qk −1,\nx(k) = σ(W(k)\nqk x(k−1)\nqk−1 + x(k−1) + b(k)\nqk−1),\n0 ≤k ≤n.\n(4.8)\nAnd we have\nN (k)\nc(k),qk(x(k−1)) :=\n \nσ\n \nW(k)\nqk\n qk\nK\ni=1\nσ(W(k)\ni\n· +b(k)\ni\n)\n!\n+ · + b(k)\nqk\n!!\n(x(k−1)), 0 ≤k ≤n.\n(4.9)\nWe are concerned with the convergence of the above functions determined by the deep neural network\nas n tends to inﬁnity. Since the output layer is a linear function of x(n) and it will not aﬀect the\n9\nconvergence of the network. We are hence concerned with the convergence of the deep neural work\ndeﬁned by\nNc,q,n(x) :=\n n\nK\nk=0\nN (k)\nc(k),qk\n!\n(x), x ∈[0, 1]dres\n(4.10)\nas n tends to inﬁnity. Note that here c := (c(0), c(1), · · · , c(n)), q := (q0, q1, · · · , qn) and Nc,q,n is a\nfunction from [0, 1]dres to Rdres.\nWe introduce an algebraic formulation of a deep ReLU network with shortcut connections by\nadapting the notions of activation domains and activation matrices introduced in [35].\nFor each\nm ∈N+, we deﬁne the set of activation matrices by\nDm := { diag (a1, · · · , am) : ai ∈{0, 1}, 1 ≤i ≤m}.\nThe support of an activation matrix J ∈Dm is deﬁned by\nsuppJ := {k : Jk,k = 1, 1 ≤k ≤m}.\nDeﬁnition 4.2 (Activation domains of one layer network) For a weight matrix W ∈Rd×d\n′\nand\na bias vector b ∈Rd, the activation domain of σ(Wx + b) with respect to a diagonal matrix J ∈Dm\nis\nDJ,W,b := {x ∈Rd\n′\n: (Wx + b)j > 0, for j ∈supp J and (Wx + b)j ≤0 for j /∈suppJ}.\nWe need to extend the deﬁnition of activation domains of neural network with one and multiple\nresidual blocks.\nDeﬁnition 4.3 (Activation domains of one-residual-block network) For\nW(0) := (W(0)\n1 , · · · , W(0)\nq0 ) ∈\nq0\nY\nm=1\nRc(0)\nm ×c(0)\nm−1,\nand\nb(0) := (b(0)\n1 , · · · , b(0)\nqn ) ∈\nq0\nY\nm=1\nRc(0)\nm ,\nthe activation domain of\nN (0)\nc(0),q0 = σ(W(0)\nq0 (\nq0−1\nK\nm=1\nσ(W(0)\nm · +b(0)\nm )) + · + b(0)\nq0 )\nwith respect to J(0) := (J(0)\n1 , · · · , J(0)\nq0 ) ∈Qq0\nm=1 Dc(0)\nm is deﬁned recursively by\nD(0)\n¯J(0)\n1 , ¯\nW(0)\n1 ,¯b(0)\n1\n= DJ(0)\n1\n,W(0)\n1\n,b(0)\n1\n∩[0, 1]dres\nand for 2 ≤m ≤q0 −1\nD(0)\n¯J(0)\nm , ¯\nW(0)\nm ,¯b(0)\nm =\n(\nx ∈D(0)\n¯J(0)\nm−1, ¯\nW(0)\nm−1,¯b(0)\nm−1\n: (\nm−1\nK\nm′=1\nσ(W(0)\nm′ · +b(0)\nm′ ))(x) ∈DJ(0)\nm ,W(0)\nm ,b(0)\nm\n)\n,\nD(0)\n¯J(0)\nq0 , ¯\nW(0)\nq0 ,¯b(0)\nq0\n=\n(\nx ∈D(0)\n¯J(0)\nq0−1, ¯\nW(0)\nq0−1,¯b(0)\nq0−1\n: (W(0)\nq0 (\nq0−1\nK\nm′=1\nσ(W(0)\nm′ · +b(0)\nm′ )) + ·)(x) ∈DJ(0)\nq0 ,Ires,b(0)\nq0\n)\n.\n10\nHere,\n¯\nW(0)\nm := (W(0)\n1 , · · · , W(0)\nm ), ¯b(0)\nm := (b(0)\n1 , · · · , b(0)\nm ), ¯J(0)\nm := (J(0)\n1 , · · · , J(0)\nm ),\n1 ≤m ≤q0.\nFor convenience, we denote D(0)\nJ(0),W(0),b(0) := D(0)\n¯J(0)\nq0 , ¯\nW(0)\nq0 ,¯b(0)\nq0\n.\nWith the above deﬁnitions, we could now introduce the deﬁnition of the activation domain of a\nmulti-residual-block network. This deﬁnition is a nontrivial extension of that in [35].\nDeﬁnition 4.4 (Activation domains of a multi-residual-block network) For\n¯\nWn := (W(0), · · · , W(n)) ∈\nn\nY\nk=0\nqk\nY\nm=1\nRc(k)\nm ×c(k)\nm−1,\nand\n¯bn := (b(0), · · · , b(n)) ∈\nn\nY\nk=0\nqk\nY\nm=1\nRc(k)\nm ,\nthe activation domain of\nNc,q,n =\nn\nK\nk=0\nN (k)\nc(k),qk\nwith respect to ¯Jn := (J(0), · · · , J(n)) ∈Qn\nk=0\nQqn\nm=1 Dc(k)\nm is deﬁned recursively by\nD¯J0, ¯\nW0,¯b0 = D(0)\nJ(0),W(0),b(0), D¯Jn, ¯\nWn,¯bn = D(n)\nJ(n),W(n),b(n),\nwhere\nD(n)\n¯J(n)\n1\n, ¯\nW(n)\n1\n,¯b(n)\n1\n=\n(\nx ∈D¯Jn−1, ¯\nWn−1,¯bn−1 : (\nn−1\nK\nk=0\nN (k)\nc(k),qk)(x) ∈DJ(n)\n1\n,W(n)\n1\n,b(n)\n1\n)\nD(n)\n¯J(n)\nm , ¯\nW(n)\nm ,¯b(n)\nm =\n\u001a\nx ∈D(n)\n¯J(n)\nm−1, ¯\nW(n)\nm−1,¯b(n)\nm−1\n:\n((\nm−1\nK\nm′=1\nσ(W(n)\nm′ · +b(n)\nm′ )) ◦(\nn−1\nK\nk=0\nN (k)\nc(k),qk))(x) ∈DJ(n)\nm ,W(n)\nm ,b(n)\nm\n\u001b\n, 2 ≤m ≤qn −1\nD(n)\n¯J(n)\nqn , ¯\nW(n)\nqn ,¯b(n)\nqn\n=\n\u001a\nx ∈D(n)\n¯J(n)\nqn−1, ¯\nW(n)\nqn−1,¯b(n)\nqn−1\n:\n((W(n)\nqn (\nqn−1\nK\nm′=1\nσ(W(n)\nm′ · +b(n)\nm′ )) + ·) ◦(\nn−1\nK\nk=0\nN (k)\nc(k),qk))(x) ∈DJ(n)\nqn ,Ires,b(n)\nqn .\n\u001b\nFor each non-negative number n, the activation domains\nD¯Jn, ¯\nWn,¯bn,\nfor\n¯Jn := (J(0), · · · , J(n)) ∈\nn\nY\nk=0\nqn\nY\nm=1\nDc(k)\nm ,\nform a partition of the unit cube [0, 1]dres. By using these activation domains, we are able to write\ndown an explicit expression of the ReLU network with shortcut connections Nc,q,n with applications\nof the ReLU activation function replaced by multiplications with the activation matrices. To this end,\nwe write\nn\nY\ni=0\nWi = WnWn−1 · · · W0.\n11\nFor n, k ∈N, we also adopt the following convention that\nn\nY\ni=k\nWi = WnWn−1 · · · Wk, for n ≥k, and\nn\nY\ni=k\nWi = Idres, for n < k.\nTheorem 4.5 It holds for each x ∈D¯Jn, ¯\nWn,¯bn, ¯Jn := (J(0), · · · , J(n)) ∈Qn\nk=0\nQqk\nm=1 Ddres that\nNc,q,n(x) =\n n\nK\nk=0\nN (k)\nc(k),qk\n!\n(x) =\n\" n\nY\nk=0\n qk\nY\nm=1\nJ(k)\nm W(k)\nm + J(k)\nqk\n!#\nx\n+\nn\nX\nk=0\nqk\nX\nm=1\n\"\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm .\n(4.11)\nProof: We proof by induction on n. When n = 0\nNc,q,0(x) = N (0)\nc(0),qk(x) =\n \nσ\n \nW(0)\nq0\n q0−1\nK\nm=1\nσ(W(0)\nm · +b(0)\nm )\n!\n+ · + b(0)\nq0\n!!\n(x).\nLet x ∈D¯J0, ¯\nW0,¯b0. By Theorem 3.4 in [35],\n q0−1\nK\nm=1\nσ(W(0)\nm · +b(0)\nm )\n!\n(x) =\nq0−1\nY\nm=1\nJ(0)\nm W(0)\nm x +\nq0−1\nX\nm=1\n \nq0−1\nY\nm′=m+1\nJ(0)\nm′ W(0)\nm′\n!\nJ(0)\nm b(0)\nm\nand thus for x ∈D¯J0, ¯\nW0,¯b0,\nNc,q,0(x) = J(0)\nq0\n \nW(0)\nq0\n q0−1\nY\nm=1\nJ(0)\nm W(0)\nm x +\nq0−1\nX\nm=1\n \nq0−1\nY\nm′=m+1\nJ(0)\nm′ W(0)\nm′\n!\nJ(0)\nm b(0)\nm\n!\n+ x + b(0)\nq0\n!\n=\n q0\nY\nm=1\nJ(0)\nm W(0)\nm + J(0)\nq0\n!\nx +\nq0\nX\nm=1\n \nq0\nY\nm′=m+1\nJ(0)\nm′ W(0)\nm′\n!\nJ(0)\nm b(0)\nm .\n(4.12)\nThe result is hence true when n = 0. Suppose that (4.11) holds for n −1. Now Let x ∈D¯Jn, ¯\nWn,¯bn.\nThen\nNc,q,n(x) = N (n)\nc(n),qn\n  n−1\nK\nk=0\nN (k)\nc(k),qk\n!\n(x)\n!\n= N (n)\nc(n),qn\n \"n−1\nY\nk=0\n qk\nY\nm=1\nJ(k)\nm W(k)\nm + J(k)\nqk\n!#\nx\n+\nn−1\nX\nk=0\nqk\nX\nm=1\n\"\nn−1\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm\n!\n.\n12\nBy deﬁnition (4.4), we get by (4.12) and induction that for x ∈D¯Jn, ¯\nWn,¯bn,\nNc,q,n(x) = N (n)\nc(n),qn\n  n−1\nK\nk=0\nN (k)\nc(k),qk\n!\n(x)\n!\n=\n qn\nY\nm=1\nJ(n)\nm W(n)\nm + J(n)\nqn\n!   n−1\nK\nk=0\nN (k)\nc(k),qk\n!\n(x)\n!\n+\nqn\nX\nm=1\n \nqn\nY\nm′=m+1\nJ(n)\nm′ W(n)\nm′\n!\nJ(n)\nm b(n)\nm\n=\n qn\nY\nm=1\nJ(n)\nm W(n)\nm + J(n)\nqn\n!  \"n−1\nY\nk=0\n qk\nY\nm=1\nJ(k)\nm W(k)\nm + J(k)\nqk\n!#\nx\n+\nn−1\nX\nk=0\nqk\nX\nm=1\n\"\nn−1\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm\n!\n+\nqn\nX\nm=1\n \nqn\nY\nm′=m+1\nJ(n)\nm′ W(n)\nm′\n!\nJ(n)\nm b(n)\nm\n=\n\" n\nY\nk=0\n qk\nY\nm=1\nJ(k)\nm W(k)\nm + J(k)\nqk\n!#\nx +\nqn\nX\nm=1\n \nqn\nY\nm′=m+1\nJ(n)\nm′ W(n)\nm′\n!\nJ(n)\nm b(n)\nm\n+\nn−1\nX\nk=0\nqk\nX\nm=1\n\"\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm\n=\n\" n\nY\nk=0\n qk\nY\nm=1\nJ(k)\nm W(k)\nm + J(k)\nqk\n!#\nx\n+\nn\nX\nk=0\nqk\nX\nm=1\n\"\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm\nwhich proves (4.5).\n✷\nFor convenience, we denote\nAn :=\nn\nY\nk=0\n qk\nY\nm=1\nJ(k)\nm W(k)\nm + J(k)\nqk\n!\n(4.13)\nand\nBn :=\nn\nX\nk=0\nqk\nX\nm=1\n\"\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm .\n(4.14)\nWe reach the main result of this section, which is a direct consequence of the above theorem.\nTheorem 4.6 Let q := {qn}∞\nn=1 ⊆N+, c := {c(n)}∞\nn=0 with c(n) := (c(n)\n0 , · · · , c(n)\nqn ) ∈Nqn\n+ , W :=\n{W(n)}∞\nn=0 with W(n) ∈Qqn\nm=1 Rc(n)\nm ×c(n)\nm−1 be the weight matrices, and b(n) := {b(n)}∞\nn=0 with b(n) ∈\nQqn\nm=1 Rc(n)\nm be the bias vectors. If for all J = {J(n)}∞\nn=1 with J(n) = (J(n)\n1\n, · · · , J(n)\nqn ), the two limits\nlim\nn→∞An = lim\nn→∞\nn\nY\nk=0\n qk\nY\nm=1\nJ(k)\nm W(k)\nm + J(k)\nqk\n!\n(4.15)\n13\nand\nlim\nn→∞Bn = lim\nn→∞\nn\nX\nk=0\nqk\nX\nm=1\n\"\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm\n(4.16)\nboth exist, then the sequence of neural networks {Nc,q,n}∞\nn=1 converges pointwise on [0, 1]dres.\nTheorem 4.6 provides a basis to study the convergence of deep ReLU neural networks with shortcut\nconnections. It reduces the problem to the existence of two limits involving inﬁnite product of non-\nsquare matrices. Note that these two limits are much more complicated than those in [35, 36] in that\nthey contain double products of matrices. Also the matrices are non-square. We shall spend the next\nsection in studying the inﬁnite product of non-square matrices in the two limits.\n5\nConvergence of DNNs with Shortcut connections\nBy Theorem 4.6, existence of two limits (4.15) and (4.16) serves as a suﬃcient condition to ensure\npointwise convergence of deep ReLU neural networks with shortcut connections. In particular, con-\nvergence of the inﬁnite product of matrices\nlim\nn→∞\nn\nY\nk=0\n qk\nY\nm=1\nJ(k)\nm W(k)\nm + J(k)\nqk\n!\n, for any J(n)\nm\n∈Dres,\n(5.1)\nappears in both of the limits. We hence ﬁrst study this important issue in this section.\nLet ∥· ∥be a norm on Rm satisfying\n∥a∥≤∥b∥whenever |ai| ≤|bi|, 1 ≤i ≤m, for a = (a1, a2, . . . , am), b = (b1, b2, . . . , bm) ∈Rm. (5.2)\nWe then deﬁne its induced matrix norm on Rn×m, also denoted by ∥· ∥, by\n∥A∥=\nsup\nx∈Rm,x̸=0\n∥Ax∥\n∥x∥,\nfor A ∈Rn×m.\nClearly, it holds\n∥AB∥≤∥A∥∥B∥for all matrices A, B\n(5.3)\nand\n∥J(k)\nm ∥≤1 for each J(k)\nm ∈Ddres.\n(5.4)\nNorms satisfying the above properties include the ∥· ∥p norms, 1 ≤p ≤+∞.\nTheorem 5.1 Let q = (qn)∞\nn=0 with ∥q∥∞< +∞and qn ∈N+ for n ∈N, c := {c(n)}∞\nn=0 with\nc(n) := (c(n)\n0 , · · · , c(n)\nqn ) ∈Nqn\n+ and c(n)\n0\n= c(n)\nqn = dres for n ∈N, and W := {W(n)}∞\nn=0 with W(n) :=\n(W(n)\n1 , · · · , W(n)\nqn ) ∈Qqn\nm=1 Rc(n)\nm ×c(n)\nm−1. If\n∞\nX\nn=0\nqn\nY\nm=1\n∥W(n)\nm ∥< +∞,\n(5.5)\nthen the inﬁnite product (5.1) converges for all J(n)\nm\n∈Ddres, m = 1, · · · , qn, n ∈N.\n14\nProof: We ﬁrst write\nn\nY\nk=0\n qk\nY\nm=1\nJ(k)\nm W(k)\nm + J(k)\nqk\n!\n=\nn\nY\nk=0\nJ(k)\nqk\n \nW(k)\nqk\nqk−1\nY\nm=1\nJ(k)\nm W(k)\nm + Idres\n!\n.\nBy Theorem 4.3 in [35], a suﬃcient condition for convergence of limit (5.1) is\n∞\nX\nk=0\n\r\r\r\rW(k)\nqk\nqk−1\nY\nm=1\nJ(k)\nm W(k)\nm\n\r\r\r\r < +∞.\nNow assume condition (5.5). As ∥q∥∞< +∞, we have by (5.3) and (5.4) that\n∞\nX\nn=0\n\r\r\r\rW(n)\nqn\nqn−1\nY\nm=1\nJ(n)\nm W(n)\nm\n\r\r\r\r ≤\n∞\nX\nn=0\nqn\nY\nm=1\n∥W(n)\nm ∥< +∞,\nwhich completes the proof.\n✷\nWe next deal with the second limit (4.16).\nTheorem 5.2 Let q = (qn)∞\nn=0 with ∥q∥∞< +∞and qn ∈N+ for n ∈N, c := {c(n)}∞\nn=0\nwith c(n) := (c(n)\n0 , · · · , c(n)\nqn ) ∈Nqn\n+ and c(n)\n0\n= c(n)\nqn\n= dres for n ∈N, W := {W(n)}∞\nn=0 with\nW(n) := (W(n)\n1 , · · · , W(n)\nqn ) ∈Qqn\nm=1 Rc(n)\nm ×c(n)\nm−1, and b := {b(n)}∞\nn=0 with b(n) := (b(n)\n1 , · · · , b(n)\nqn ) ∈\nQqn\nm=1 Rc(n)\nm . If\n∞\nX\nk=0\nqk\nX\nm=1\n \nqk\nY\nm′=m+1\n∥W(k)\nm′ ∥\n!\n∥b(k)\nm ∥< +∞,\n(5.6)\n∞\nY\nk=n\n qk\nY\nm=1\nJ(k)\nm W(k)\nm + J(k)\nqk\n!\nconverges for every n ∈N,\n(5.7)\nand there exists a positive constant C1 such that\nn\nY\nk=n′\n qk\nY\nm=1\n∥W(k)\nm ∥+ 1\n!\n≤C1 for all n, n′ ∈N, n′ ≤n\n(5.8)\nthen the limit (4.16) exists.\nProof: It suﬃces to show that\nBn =\nn\nX\nk=0\nqk\nX\nm=1\n\"\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm\nforms a Cauchy sequence in Rdres. By condition (5.6), we could assume that there exists a positive\nconstant C2 such that\nn\nX\nk=0\nqk\nX\nm=1\n \nqk\nY\nm′=m+1\n∥W(k)\nm′ ∥\n!\n∥b(k)\nm ∥≤C2 for every n ∈N.\n(5.9)\n15\nWe deﬁne dn,n′,n′′ and en,n′′ for n′′ < n, n′′ < n′ and n, n′, n′′ ∈N as follows:\ndn,n′,n′′ :=\nn′′\nX\nk=0\nqk\nX\nm=1\n\"\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!\n−\nn′\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm\n(5.10)\nand\nen,n′′ :=\nn\nX\nk=n′′+1\nqk\nX\nm=1\n\"\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm .\n(5.11)\nLet ǫ > 0 be arbitrary. By condition (5.6), there exists a positive integer n′′ such that\nn\nX\nk=n′′+1\nqk\nX\nm=1\n \nqk\nY\nm′=m+1\n∥W(k)\nm′ ∥\n!\n∥b(k)\nm ∥<\nǫ\n3C1\n, ∀n > n′′.\n(5.12)\nThus, by the triangle inequality, (5.3), (5.4), (5.8), (5.11) and (5.12), we have\n∥en,n′′∥=\n\r\r\r\r\r\nn\nX\nk=n′′+1\nqk\nX\nm=1\n\"\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm\n\r\r\r\r\r\n≤\nn\nX\nk=n′′+1\nqk\nX\nm=1\n\"\nn\nY\nk′=k+1\n qk′\nY\nm′=1\n∥W(k′)\nm′ ∥+ 1\n!#  \nqk\nY\nm′=m+1\n∥W(k)\nm′ ∥\n!\n∥b(k)\nm ∥\n≤C1\nn\nX\nk=n′′+1\nqk\nX\nm=1\n \nqk\nY\nm′=m+1\n∥W(k)\nm′ ∥\n!\n∥b(k)\nm ∥\n≤ǫ\n3.\n(5.13)\nBy (5.7), for big enough n, n′ > n′′, it holds that\n\r\r\r\r\r\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!\n−\nn′\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!\r\r\r\r\r <\nǫ\n3C2\n, ∀k ≤n′.\n(5.14)\n16\nThus, by the triangle inequality, (5.3), (5.4), (5.9), (5.10) and (5.14), we have\n∥dn,n′,n′′∥=\n\r\r\r\r\r\nn′′\nX\nk=0\nqk\nX\nm=1\n\"\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!\n−\nn′\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!#  \nqk\nY\nm′=m+1\nJ(k)\nm′ W(k)\nm′\n!\nJ(k)\nm b(k)\nm\n\r\r\r\r\r\n≤\nn′′\nX\nk=0\nqk\nX\nm=1\n\r\r\r\r\r\nn\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!\n−\nn′\nY\nk′=k+1\n qk′\nY\nm′=1\nJ(k′)\nm′ W(k′)\nm′ + J(k′)\nqk′\n!\r\r\r\r\r\n \nqk\nY\nm′=m+1\n∥W(k)\nm′ ∥\n!\n∥b(k)\nm ∥\n≤\nn′′\nX\nk=0\nqk\nX\nm=1\nǫ\n3C2\n \nqk\nY\nm′=m+1\n∥W(k)\nm′ ∥\n!\n∥b(k)\nm ∥\n≤\nǫ\n3C2\nn′′\nX\nk=0\nqk\nX\nm=1\n \nqk\nY\nm′=m+1\n∥W(k)\nm′ ∥\n!\n∥b(k)\nm ∥\n≤ǫ\n3.\n(5.15)\nSince ∥Bn −Bn′∥= ∥en,n′′ + dn,n′,n′′ −en′,n′′∥, by the triangle inequality, (5.13) and (5.15) we have\n∥Bn −Bn′∥= ∥en,n′′ + dn,n′,n′′ −en′,n′′∥≤∥en,n′′∥+ ∥dn,n′,n′′∥+ ∥en′,n′′∥< ǫ.\n(5.16)\nThis shows Bn is a Cauchy sequence and thus it converges.\n✷\nWe now apply Theorem 5.1 and Theorem 5.2 to establish the convergence of DNNs Nc,q,n with\nShortcut connections.\nTheorem 5.3 Let q = (qn)∞\nn=0 with ∥q∥∞< +∞and qn ∈N+ for n ∈N, c := {c(n)}∞\nn=0\nwith c(n) := (c(n)\n0 , · · · , c(n)\nqn ) ∈Nqn\n+ and c(n)\n0\n= c(n)\nqn\n= dres for n ∈N, W := {W(n)}∞\nn=0 with\nW(n) := (W(n)\n1 , · · · , W(n)\nqn ) ∈Qqn\nm=1 Rc(n)\nm ×c(n)\nm−1, and b := {b(n)}∞\nn=0 with b(n) := (b(n)\n1 , · · · , b(n)\nqn ) ∈\nQqn\nm=1 Rc(n)\nm . If\n∞\nX\nk=0\nqk\nY\nm=1\n∥W(k)\nm ∥< +∞\n(5.17)\nand\n∞\nX\nk=0\nqk\nX\nm=1\n \nqk\nY\nm′=m+1\n∥W(k)\nm′ ∥\n!\n∥b(k)\nm ∥< +∞\n(5.18)\nthen Nc,q,n converges pointwise on [0, 1]dres.\nProof: By (5.17), we could assume\n∞\nX\nk=0\nqk\nY\nm=1\n∥W(k)\nm ∥= C1 < +∞.\n17\nFurthermore, we can verify that for all n, n′ ∈N, n′ ≤n,\nn\nY\nk=n′\n qk\nY\nm=1\n∥W(k)\nm ∥+ 1\n!\n≤\nn\nY\nk=n′\nexp\n qk\nY\nm=1\n∥W(k)\nm ∥\n!\n≤exp\n \nn\nX\nk=n′\nqk\nY\nm=1\n∥W(k)\nm ∥\n!\n≤exp(C1).\nThus, by Theorem 5.1 and 5.2, limits (4.15) and (4.16) exists for all J(n)\nm\n∈Ddres. Therefore, Nc,q,n\nconverges pointwise on [0, 1]dres.\n✷\nSince W(0) = (W(0)\n1 ) with W(0)\n1\n=\n\u0002\nWs\n0\n\u0003\n, b(0) = (b(0)\n1 ) with b(0)\n1\n= bs, and the network (4.1)\nwithout the linear output part is equivalent to (Nc,q,n ◦I) (x) for x ∈[0, 1]din where I(x) :=\n\u0014Iin\n0\n\u0015\nx\nand\n\u0014Iin\n0\n\u0015\n∈Rdres×din, we reach the main theorem of the section.\nTheorem 5.4 Let q = (qn)∞\nn=0 with q0 = 1, qn ∈N+ for n ∈N, and ∥q∥∞< +∞, c := {c(n)}∞\nn=0\nwith c(n) := (c(n)\n0 , · · · , c(n)\nqn ) ∈Nqn\n+ and c(n)\n0\n= c(n)\nqn = dres for n ∈N, W := {W(n)}∞\nn=0 with W(n) :=\n(W(n)\n1 , · · · , W(n)\nqn ) ∈Qqn\nm=1 Rc(n)\nm ×c(n)\nm−1 and W(0) = (W(0)\n1 ) with W(0)\n1\n=\n\u0002\nWs\n0\n\u0003\n, and b := {b(n)}∞\nn=0\nwith b(n) := (b(n)\n1 , · · · , b(n)\nqn ) ∈Qqn\nm=1 Rc(n)\nm . If\n∞\nX\nk=0\nqk\nY\nm=1\n∥W(k)\nm ∥< +∞\n(5.19)\nand\n∞\nX\nk=0\n \nqk\nY\nm′=m+1\n∥W(k)\nm′ ∥\n!\nqk\nX\nm=1\n∥b(k)\nm ∥< +∞\n(5.20)\nthen network (4.1) converges pointwise on [0, 1]din.\n6\nConvergence of Deep Residual Networks\nWe now return to the main topic of this paper, which is to establish the convergence of deep residual\nnetworks. We will apply the result on convergence of deep neural network with shortcut connections\nin Section 5 to deep ResNets.\nWe shall work with the matrix norm induced by the ℓp vector norm. Recall the Riesz-Thorin\ninterpolation theorem (see, [11], page 200) that for any matrix A and p ∈[1, +∞]\n∥A∥p ≤∥A∥\n1\np\n1 ∥A∥\n1−1\np\n∞\n.\n(6.1)\nLet us ﬁrst use this interpolation theorem to study the relationship between the norm of ﬁlter masks\nand the norm of the matrices in (2.3) associated with the ﬁlter masks.\nLemma 6.1 Let w := (w1, · · · , wcout) with wi := (wi,1, · · · , wi,cin) ∈R(2f+1)×(2f+1)×cin and wi,j :=\n(wi,j,i′,j′)2f+1,2f+1\ni′=1,j′=1\nfor 1 ≤i ≤cout denote the ﬁlter masks for multi-channel convolution, W := T(w)\nbe deﬁned as in (2.3), and ∥· ∥p be ℓp-norm on matrices. It holds that\n∥W∥p ≤\n\nmax\n1≤j≤cin\ncout\nX\ni=1\n2f+1\nX\ni′=1\n2f+1\nX\nj′=1\n|wi,j,i′,j′|\n\n\n1\np \nmax\n1≤i≤cout\ncin\nX\nj=1\n2f+1\nX\ni′=1\n2f+1\nX\nj′=1\n|wi,j,i′,j′|\n\n\n1−1\np\n(6.2)\n18\nProof: By the Riesz-Thorin interpolation theorem (6.1), we have\n∥W∥p ≤∥W∥\n1\np\n1 ∥W∥\n1−1\np\n∞\n.\n(6.3)\nSince\nW =\n\n\nT(w1,1)\nT(w1,2)\n· · ·\nT(w1,cin)\nT(w2,1)\nT(w2,2)\n· · ·\nT(w2,cin)\n...\n...\n...\n...\nT(wcout,1)\nT(wcout,2)\n· · ·\nT(wcout,cin)\n\n,\nwe obtain\n∥W∥1 ≤\nmax\n1≤j≤cin\ncout\nX\ni=1\n∥T(wi,j)∥1\n(6.4)\nand\n∥W∥∞≤\nmax\n1≤i≤cout\ncin\nX\nj=1\n∥T(wi,j)∥∞.\n(6.5)\nMoreover, by (2.1) and (2.2), we have for i = 1, · · · , cout and j = 1, · · · , cin,\n∥T(wi,j)∥1 ≤\n2f+1\nX\ni′=1\n2f+1\nX\nj′=1\n|wi,j,i′,j′|\n(6.6)\nand\n∥T(wi,j)∥∞≤\n2f+1\nX\ni′=1\n2f+1\nX\nj′=1\n|wi,j,i′,j′|.\n(6.7)\nCombining (6.3), (6.4), (6.5), (6.6) and (6.7), we get\n∥W∥p ≤∥W∥\n1\np\n1 ∥W∥\n1−1\np\n∞\n≤\n \nmax\n1≤j≤cin\ncout\nX\ni=1\n∥T(wi,j)∥1\n! 1\np\n\nmax\n1≤i≤cout\ncin\nX\nj=1\n∥T(wi,j)∥∞\n\n\n1−1\np\n≤\n\nmax\n1≤j≤cin\ncout\nX\ni=1\n2f+1\nX\ni′=1\n2f+1\nX\nj′=1\n|wi,j,i′,j′|\n\n\n1\np \nmax\n1≤i≤cout\ncin\nX\nj=1\n2f+1\nX\ni′=1\n2f+1\nX\nj′=1\n|wi,j,i′,j′|\n\n\n1−1\np\nand prove the lemma.\n✷\nWe could now apply Theorem 5.4 with Lemma 6.1 to obtain convergence of ResNets.\nTheorem 6.2 Let\n• q = (qn)∞\nn=0 with q0 = 1, qn ∈N+ for n ∈N, and ∥q∥∞< +∞,\n• c := {c(n)}∞\nn=0 with c(n) := (c(n)\n0 , · · · , c(n)\nqn ) ∈Nqn\n+ and supn∈N ∥c(n)∥∞< +∞,\n• f := (f (n))∞\nn=0 with f (n) := (f (n)\n1\n, · · · , f (n)\nqn ) ∈Nqn\n+ , and supn∈N ∥f (n)∥∞< +∞,\n19\n• w := {w(n)}∞\nn=0 with w(n) := (w(n)\n1 , · · · , w(n)\nqn ) ∈Qqn\nm=1 R(2f(n)\nm +1)×(2f(n)\nm +1)×c(n)\nm ×c(n)\nm−1, and\n• w(n)\nm := (w(n)\nm,i,j)\nc(n)\nm ,c(n)\nm−1\ni=1,j=1 , w(n)\nm,i,j := (w(n)\ni,j,i′,j′)2f(n)\nm +1,2f(n)\nm +1\ni′=1,j′=1\n, w(0) = (w(0)\n1 ) with w(0)\n1\n= ws,\n• b := {b(n)}∞\nn=0 with b(n) := (b(n)\n1 , · · · , b(n)\nqn ) ∈Qqn\nm=1 Rd×d×c(n)\nm and b(n)\nm := (b(n)\nm,1Ed, · · · , b(n)\nm,c(n)\nm Ed).\nIf\n∞\nX\nn=0\nqn\nY\nm=1\n\n\nmax\n1≤j≤c(n)\nm−1\nc(n)\nm\nX\ni=1\n2f+1\nX\ni′=1\n2f+1\nX\nj′=1\n|wi,j,i′,j′|\n\n\n1\np \n\nmax\n1≤i≤c(n)\nm\nc(n)\nm−1\nX\nj=1\n2f+1\nX\ni′=1\n2f+1\nX\nj′=1\n|wi,j,i′,j′|\n\n\n\n1−1\np\n< +∞\n(6.8)\nand\n∞\nX\nn=0\nqn\nX\nm=1\nP (n)\nm\n\n\nc(n)\nm\nX\ni=1\n|b(n)\nm |p\n\n\n1\np\n< +∞\n(6.9)\nwhere\nP (n)\nm\n=\nqn\nY\nm′=m+1\n\n\n\n\n\nmax\n1≤j≤c(n)\nm′−1\nc(n)\nm′\nX\ni=1\n2f+1\nX\ni′=1\n2f+1\nX\nj′=1\n|wi,j,i′,j′|\n\n\n\n1\np \n\nmax\n1≤i≤c(n)\nm′\nc(n)\nm′−1\nX\nj=1\n2f+1\nX\ni′=1\n2f+1\nX\nj′=1\n|wi,j,i′,j′|\n\n\n\n1−1\np \n,\n(6.10)\nthen network (3.1) converges pointwise on [0, 1]d×d×cin.\nProof: Note that network (3.1) is equivalent to (4.1) where the weight matrices associated with\nthe ﬁlter masks and the bias vectors are given by W := {W(n)}∞\nn=0 with W(n) := (W(n)\n1 , · · · , W(n)\nqn ),\nW(n)\nm = T(w(n)\nm ) ∈Rd2c(n)\nm ×d2c(n)\nm−1 and Ws = T(ws), and eb := {eb(n)}∞\nn=0 with eb(n) := (eb(n)\n1 , · · · , eb(n)\nqn ),\neb(n)\nm = Vec (b(n)\nm ) ∈Rd2c(n)\nm and ebs = Vec (bs). Also, by Lemma 6.1 and condition (6.8), (5.19) holds\nunder the assumptions of this theorem. Furthermore, supn∈N ∥c(n)∥∞< +∞and (6.9) ensure that\n(5.20) holds true as well. Thus, network (3.1) converges pointwise on [0, 1]d×d×cin.\n✷\nFrom the theoretical results above, one sees that if the conditions (6.8) and (6.9) hold, the network\nwill learn the identity maps as the depth tends to inﬁnity, which veriﬁes the design spirit of ResNets\n[16].\n7\nNumerical Experiments\nIn this section, we shall conduct experiments with deep ResNets on standard image classiﬁcation\nbenchmarks CIFAR10 to verify our theory with the numerical results. To be speciﬁc, we shall train\na suﬃcient deep ResNet on the benchmark data until an over 90% accuracy is achieved.\nIn this\nsituation, the network is considered to be convergent. We then compute the partial sums in the series\nin the suﬃcient conditions (6.8) and (6.9). If the partial sums are bounded as the depth of the ResNet\nincreases then the theoretical result Theorem 6.2 on the convergence of the ResNets is veriﬁed.\n20\nWe now described our experiments. Our architecture is outlined in table 7.1. Each residual block\nhas the form\nReLU\n \nx + w4\n3\nK\nm=1\nReLU (wm ∗· + bm) (x) + b4\n!\n,\nwhere wm, bm, i = 1, · · · , 4 are ﬁlter masks and bias vectors with dimensions determined by the shape\nof ﬁlter masks and the number of channels. All the convolutions in our network are with stride 1,\nand zero padding are used in all convolutions except for the global average pooling in the end of our\nnetwork. Note that our transformation is dimensionality-preserving, and the dimensions are always\nmatched between the head and the tail of residual blocks.\nWe implemented and trained our model with PyTorch framework, using a momentum optimizer\nwith momentum 0.9, and batch size 128. The technique of weight decayness is not involved in the\ntraining process of our model. The initial learning rate is 0.05, which drops by a factor 10 at 60,\n90 and 120 epochs. The model reaches peak performance at around 30k steps for CIFAR10, which\ntakes about 20h on a single NVIDIA GTX3090 GPU. Our code can be easily derived from an open\nsource implementation1 by adjusting the residual components and model architecture.\nAll of the\ninitial parameters in our code are generated by the default methods in PyTorch.\nThe major diﬀerence between our model and the standard models in [16] is that no max pooling\nor batch normalization is involved in our model and thus we add biases in our model. Though the\nstructure and techniques we used in our model are quite simple, we still obtain 91% accuracy on the\nbenchmark of CIFAR10.\nTable 7.1: Architecture for CIFAR10 (241 convolutions, 6.4M parameters)\nweight dimensions\nbias dimensions\ndescription\n3×3×3×256\n256\n1 standard convolution\n\n\n1 × 1 × 256 × 64\n3 × 3 × 64 × 64\n3 × 3 × 64 × 64\n1 × 1 × 64 × 256\n\n\n\n\n64\n64\n64\n256\n\n\n60 residual blocks\n-\n-\n32×32 global average pooling\n256×classes\nclasses\nlinear map\nAs is shown by ﬁgure 7, the suﬃcient conditions in Theorem 6.2 are indeed satisﬁed.\n1https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n21\nFigure 7.1: Convergence plot of best model for CIFAR10\n0\n20\n40\n60\nresidual blocks\n0\n2\n4\n6\n8\ncondition 1\n1e6\np=inf\np=1\np=2\n0\n20\n40\n60\nresidual blocks\n0\n1\n2\n3\n4\ncondition 2\n1e6\np=inf\np=1\np=2\nFigure 7.2: Numerical results for two suﬃcient conditions\n22\nReferences\n[1] B. Adcock and N. Dexter, The gap between theory and practice in function approximation with deep neural\nnetworks, SIAM J. Math. Data Sci. 3 (2021), no. 2, 624–655.\n[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classiﬁcation with deep convolutional neural\nnetworks, Adv. Neural Inf. Process. Syst. 25 (2012).\n[3] Z. Allen-Zhu, and Y. Li, What can resnet learn eﬃciently, going beyond kernels?, Adv. Neural Inf. Process.\nSyst. 32 (2019).\n[4] M. Artzrouni, On the convergence of inﬁnite products of matrices, Linear Algebra Appl. 74 (1986), 11–21.\n[5] S. Christian, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-\nnovich, Going deeper with convolutions, 2015 IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2015, 1-9.\n[6] I. Daubechies, Ten Lectures on Wavelets, SIAM, Philadelphia, 1992.\n[7] R. DeVore, B. Hanin, and G. Petrova, Neural network approximation, arXiv: 2012.14501v1, 2020.\n[8] W. E and Q. Wang, Exponential convergence of the deep neural network approximation for analytic\nfunctions, Sci. China Math. 61 (2018), no. 10, 1733-1740.\n[9] D. Elbr¨achter, D. Perekrestenko, P. Grohs, and H. B¨olcskei, Deep neural network approximation theory,\narXiv:1901.02220.\n[10] G. Folland, Fourier analysis and its applications, Vol. 4. American Mathematical Soc., 2009.\n[11] G. Folland, Real Analysis: Modern Techniques and Their Applications, John Wiley & Sons, 1999.\n[12] S. Frei, Y. Cao, and Q. Gu, Algorithm-dependent generalization bounds for overparameterized deep residual\nnetworks, Adv. Neural Inf. Process. Syst. 32 (2019).\n[13] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning, MIT Press, Cambridge, 2016.\n[14] I. Daubechies, R. DeVore, S. Foucart, B. Hanin, and G. Petrova, Nonlinear approximation and (deep)\nReLU networks, arXiv: 1905.02199, 2019.\n[15] F. He, T. Liu, and D. Tao, Why ResNet Works? Residuals Generalize, IEEE Trans. Neural Netw. Learn.\nSyst. 31, no. 12 (2020): 5349-5362.\n[16] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2016, 770–778.\n[17] K. He, X. Zhang, S. Ren, and J. Sun, Identity mappings in deep residual networks, In: B. Leibe, J. Matas,\nN. Sebe, M. Welling (eds) Computer Vision ¨C ECCV 2016, Lecture Notes in Computer Science, vol. 9908,\nSpringer, Cham.\n[18] K. Huang, Y. Wang, M. Tao, and T. Zhao, Why Do Deep Residual Networks Generalize Better than Deep\nFeedforward Networks?—A Neural Tangent Kernel Perspective, Adv. Neural Inf. Process. Syst. 33 (2020):\n2698-2709.\n[19] S. Karen, and A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv\npreprint arXiv:1409.1556 (2014).\n[20] P. D. Lax, Functional Analysis, Wiley-Interscience, New York, 2002.\n[21] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, Nature 521 (2015), no. 7553, 436-444, 2015.\n[22] H. Lin, and S. Jegelka, Resnet with one-neuron hidden layers is a universal approximator, Adv. Neural Inf.\nProcess. Syst. 31 (2018).\n23\n[23] Y. Lu, C. Ma, Y. Lu, J. Lu, and L. Ying, A mean ﬁeld analysis of deep resnet and beyond: Towards\nprovably optimization via overparameterization from depth, arXiv:2003.05508, 2020.\n[24] H. Montanelli and Q. Du, Deep ReLU networks lessen the curse of dimensionality, arXiv:1712.08688, 2017.\n[25] H. Montanelli and H. Yang, Error bounds for deep ReLU networks using the Kolmogorov-Arnold super-\nposition theorem, Neural Networks 129 (2020), 1–6.\n[26] H. Moritz and T. Ma, Identity Matters in Deep Learning, arXiv:1611.04231, 2017.\n[27] T. Poggio, H. Mhaskar, L. Rosasco, B. Miranda, and Q. Liao, Why and when can deep-but not shallow-\nnetworks avoid the curse of dimensionality: A review, Internat. J. Automat. Comput. 14 (2017), 503–519.\n[28] T. Qin, K. Wu, and D. Xiu, Data driven governing equations approximation using deep neural networks,\nJ. Comput. Phys. 395 (2019): 620-635.\n[29] Z. Shen, H. Yang, and S. Zhang, Deep network approximation characterized by number of neurons, Com-\nmun. Comput. Phys. 28 (2020), no. 5, 1768–1811.\n[30] Z. Shen, H. Yang, and S. Zhang, Deep network with approximation error being reciprocal of width to power\nof square root of depth, Neural Comput. 33 (2021), no. 4, 1005–1036.\n[31] Z. Shen, H. Yang, and S. Zhang, Optimal approximation rate of ReLU networks in terms of width and\ndepth, arXiv:2103.00502, 2021.\n[32] E. Stein and R. Shakarchi, Fourier Analysis. An introduction, Princeton University Press, Princeton, NJ,\n2003.\n[33] Y. Wang, A mathematical introduction to generative adversarial nets (GAN), arXiv:2009.00169, 2020.\n[34] J. H. M. Wedderburn, Lectures on Matrices, Dover, New York, 1964.\n[35] Y. Xu and H. Zhang, Convergence of Deep ReLU Networks, arXiv:2107.12530, 2021.\n[36] Y. Xu and H. Zhang, Convergence of Deep Convolutional Neural Networks, arXiv:2109.13542, 2021.\n[37] D. Yarotsky, Error bounds for approximations with deep relu networks, Neural Networks 94 (2017), 103–\n114.\n[38] T. Zaslavsky, Facing up to arrangements: face-count formulas for partitions of space by hyperplanes, Mem.\nAmer. Math. Soc. 1 (1975), issue 1, no. 154.\n[39] H. Zhang, X. Gao, J. Unterman, and T. Arodz, Approximation capabilities of neural odes and invertible\nresidual networks, ICML, 2020.\n[40] D.X. Zhou, Universality of deep convolutional neural networks, Appl. Comput. Harmon. Anal. 48 (2020),\nno. 2, 787–794.\n[41] D. Zou, P. M. Long, and Q. Gu, On the global convergence of training deep linear ResNets,\narXiv:2003.01094, 2020.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "math.FA"
  ],
  "published": "2022-05-13",
  "updated": "2022-05-13"
}