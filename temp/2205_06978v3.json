{
  "id": "http://arxiv.org/abs/2205.06978v3",
  "title": "Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing",
  "authors": [
    "Yang Ni",
    "Danny Abraham",
    "Mariam Issa",
    "Yeseong Kim",
    "Pietro Mercati",
    "Mohsen Imani"
  ],
  "abstract": "Reinforcement Learning (RL) has opened up new opportunities to enhance\nexisting smart systems that generally include a complex decision-making\nprocess. However, modern RL algorithms, e.g., Deep Q-Networks (DQN), are based\non deep neural networks, resulting in high computational costs. In this paper,\nwe propose QHD, an off-policy value-based Hyperdimensional Reinforcement\nLearning, that mimics brain properties toward robust and real-time learning.\nQHD relies on a lightweight brain-inspired model to learn an optimal policy in\nan unknown environment. On both desktop and power-limited embedded platforms,\nQHD achieves significantly better overall efficiency than DQN while providing\nhigher or comparable rewards. QHD is also suitable for highly-efficient\nreinforcement learning with great potential for online and real-time learning.\nOur solution supports a small experience replay batch size that provides 12.3\ntimes speedup compared to DQN while ensuring minimal quality loss. Our\nevaluation shows QHD capability for real-time learning, providing 34.6 times\nspeedup and significantly better quality of learning than DQN.",
  "text": "Eﬀicient Oﬀ-Policy Reinforcement Learning via Brain-Inspired\nComputing\nYang Ni\nyni3@uci.edu\nUniversity of California, Irvine\nIrvine, California, USA\nDanny Abraham\ndannya1@uci.edu\nUniversity of California, Irvine\nIrvine, California, USA\nMariam Issa\nmariamai@uci.edu\nUniversity of California, Irvine\nIrvine, California, USA\nYeseong Kim\nyeseongkim@dgist.ac.kr\nDaegu Gyeongbuk Institute of\nScience and Technology\nDaegu, Republic of Korea\nPietro Mercati\npietro.mercati@intel.com\nIntel Labs\nHilsboro, Oregon, USA\nMohsen Imani\nm.imani@uci.edu\nUniversity of California, Irvine\nIrvine, California, USA\nABSTRACT\nReinforcement Learning (RL) has opened up new opportunities\nto enhance existing smart systems that generally include a com-\nplex decision-making process. However, modern RL algorithms,\ne.g., Deep Q-Networks (DQN), are based on deep neural networks,\nresulting in high computational costs. In this paper, we propose\nQHD, an oﬀ-policy value-based Hyperdimensional Reinforcement\nLearning, that mimics brain properties toward robust and real-\ntime learning. QHD relies on a lightweight brain-inspired model\nto learn an optimal policy in an unknown environment. On both\ndesktop and power-limited embedded platforms, QHD achieves\nsigniﬁcantly better overall eﬃciency than DQN while providing\nhigher or comparable rewards. QHD is also suitable for highly-\neﬃcient reinforcement learning with great potential for online and\nreal-time learning. Our solution supports a small experience replay\nbatch size that provides 12.3× speedup compared to DQN while en-\nsuring minimal quality loss. Our evaluation shows QHD capability\nfor real-time learning, providing 34.6× speedup and signiﬁcantly\nbetter quality of learning than DQN.\nCCS CONCEPTS\n• Computing methodologies →Machine learning; Intelligent\nagents; • Computer systems organization →Embedded systems.\nKEYWORDS\nHyperdimensional Computing, Brain-inspired Computing\nACM Reference Format:\nYang Ni, Danny Abraham, Mariam Issa, Yeseong Kim, Pietro Mercati, and Mohsen\nImani. 2023. Eﬃcient Oﬀ-Policy Reinforcement Learning via Brain-Inspired\nComputing. In Proceedings of the Great Lakes Symposium on VLSI 2023\n(GLSVLSI ’23), June 5–7, 2023, Knoxville, TN, USA. ACM, New York, NY, USA,\n5 pages. https://doi.org/10.1145/3583781.3590298\nThis work is under a Creative Commons Attribution-\nNonCommercial-NoDerivs International 4.0 License.\nGLSVLSI ’23, June 5–7, 2023, Knoxville, TN, USA\n© 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0125-2/23/06.\nhttps://doi.org/10.1145/3583781.3590298\n1\nINTRODUCTION\nSmart systems and services generally reside in a highly-dynamic\nyet unknown environment and require intelligent algorithms to\nmake optimal decisions with little prior knowledge. In recent years,\nReinforcement Learning (RL) has opened up new opportunities to\nsolve a wide range of complex predictions and decision-making\ntasks that were previously out of reach for a machine [1]. Compared\nto supervised and unsupervised learning methods, RL does not\nhave direct access to labeled training data. Learning through agent-\nenvironment interaction makes RL appealing to dynamic control\nand automated system optimization such as smart transportation\nand smart grid, where the optimal policy is hard to deﬁne and is\nconstantly changing with its environment [1, 2].\nRL methods are generally categorized into policy-based and\nvalue-based RL. The policy-based method directly parameterizes\nthe policy and optimizes it via on-policy model training. On the\nother hand, value-based RL supports oﬀ-policy training, i.e., all past\ninteractions can be used toward learning. Therefore, value-based\nRL is much more sample-eﬃcient. As one of the most popular value-\nbased methods, Deep Q-Networks (DQN) exploits DNN to learn an\napproximation of the Q-value for every pair of actions and state.\nRecently, there have been active developments for various DQN\napplications such as playing computer games [3], Genomics [4], and\nsmart city [2, 5]. DQN is capable of learning complex tasks without\nmodeling the environment, but its power comes at a price, i.e., the\nhuge computation cost and long learning time. This makes it only\nsuitable for powerful computers in the cloud. However, oﬄoading\nRL to the cloud not only leads to extra communication overhead\nbut also causes security and privacy concerns.\nTherefore, we redesign the RL algorithm by exploiting the brain-\ninspired highly-eﬃcient HyperDimensional Computing (HDC) [6].\nHDC is motivated by how human brains process diﬀerent kinds of\ninputs, i.e., brains express information using a vast number of neu-\nrons. The information is then processed and memorized in a holistic\nand high-dimensional way. For inputs in the lower-dimensional\nspace, HDC encodes them to vectors of several thousand dimen-\nsions, i.e., hypervectors. The learning process is based on highly-\nparallelizable operations of hypervectors. HDC has been applied as\na lightweight machine learning solution to multiple applications\nwhere it is capable of achieving comparable accuracy to DNN with\nsigniﬁcantly higher eﬃciency [7, 8].\nGLSVLSI ’23, June 5–7, 2023, Knoxville, TN, USA\nYang Ni, Danny Abraham, Mariam Issa, Yeseong Kim, Pietro Mercati, and Mohsen Imani\nHowever, current HDC solutions mainly focus on traditional\nclassiﬁcation and clustering. In contrast, in this paper, we propose\nQHD, a value-based Hyperdimensional Reinforcement Learning\nalgorithm with oﬀ-policy training, which mimics brain properties\ntowards robust and real-time learning. The main contributions of\nthe paper are listed as follows:\n• To the best of our knowledge, QHD is the ﬁrst oﬀ-policy value-\nbased hyperdimensional RL algorithm targeting discrete action\nspace. QHD relies on lightweight HDC models to learn an optimal\npolicy in an unknown environment. Our algorithm maps state-\naction space into high-dimensional space for eﬃcient decision-\nmaking via novel brain-inspired HDC encoding and self-learning.\n• Thanks to the brain-like hyperdimensional operations, QHD can\nutilize even a small amount of available training data. It thereby\nsupports a much smaller training batch and experience buﬀer\nthan DQN while still providing high-quality results.\n• We compare our QHD accuracy and eﬃciency with the DQN\nalgorithm for multiple dynamic control tasks. Our evaluation\nshows that QHD achieves signiﬁcantly better overall eﬃciency\nthan DQN, especially on the power-limited embedded platform,\ne.g., up to about 15× speedup. For real-time learning, QHD pro-\nvides 34.6× speedup and signiﬁcantly better quality.\n2\nRELATED WORK\nReinforcement Learning: In recent years, RL algorithms have\nobtained dramatically more attention because of the advancement\nin deep learning. For example, DQN greatly expands the application\nof RL to ﬁelds like computer games [3, 9], transportation optimiza-\ntion [2, 10], and health care [11, 12]. In [10], researchers focus on\ndriver dispatch optimization within online ride-sharing services.\nThey use DQN to learn a policy for matching available drivers and\nusers to maximize the success rate while minimizing the wait time.\nAll works above utilize DNN to handle complex agent-environment\ninteractions, so they are computationally intensive with insuﬃcient\neﬃciency. In contrast, we propose a brain-inspired reinforcement\nlearning solution with inherent eﬃciency and robustness.\nHyperdimensional Computing: Prior HDC works mainly pro-\nvide solutions to classiﬁcation and cognitive tasks, such as graph\nreasoning [13], bio-signal processing [7, 8], speech recognition [14],\nneuromorphic sensing [15] and multi-sensor signal classiﬁcation [16].\nIn these highlighted applications, HDC has outperformed state-of-\nthe-art learning solutions, e.g., support vector machines [14] and\nneural networks [16]. Recent orthogonal work proposes an HDC-\nbased policy-based RL speciﬁcally for continuous control tasks [17].\nHowever, this work does not provide support for RL tasks with\ndiscrete action space. In addition, as mentioned in Section 1, policy-\nbased RL methods are less sample-eﬃcient due to the lack of oﬀ-\npolicy training. Unlike all prior works, this paper is the ﬁrst eﬀort\nfocusing on hyperdimensional oﬀ-policy value-based RL.\n3\nQHD: HYPERDIMENSIONAL Q-LEARNING\n3.1\nOverview\nFig. 1 shows an overview of QHD supporting hyperdimensional\nreinforcement learning. In our RL task, there are two components\n(Agent and Environment) and three variables (Action, State, and\nReward). Fig. 1(a) exploits a Cartpole example to illustrate these\nAgent\nθ\nv2\nv1\nEnvironment\nx\n0\nAction\nReward\nActions: \n(Left, Right)\nStates:\n(x,v1,v2,θ)\n(a)\n(b)\nState \n𝑆𝑡\nActions \n𝐴𝑡\nHDC \nEncoder\nHDC-based \nRegression\nQ-values for \nDifferent Actions\nAction with \nthe Highest \nQ-value\nHD-based Q-learning (QHD)\nMemory\nState \nHypervector\nFigure 1: Overview of QHD reinforcement learning.\nρ1\nS1\nPosition HVs\nAssociated HVs\nρ2\nS2\nρn\nSn\n*\nState\nS1\nS2\nSn\nEncoded State\nComplex ρs: 𝑒𝑖θ\nθ ∈[−𝜋, +𝜋]\nFigure 2: HDC encoding with complex-valued position hy-\npervectors (HVs).\ncomponents and variables. As shown in Fig. 1, the interaction be-\ntween the agent and environment forms a loop in which the action\ntaken based on the current state leads to the next state and reward.\nThe trajectory of each episode is saved in local memory for later\nlearning. In Fig. 1(b), we provide an overview of QHD algorithm\nguiding the agent in the decision-making process.\n3.2\nQHD Hyperdimensional Encoding\nQHD starts by mapping the current state vector from the original\nto high-dimensional space, i.e., hypervector encoding. Notice that\nwe can create a large number of near-orthogonal hypervectors\nthrough random sampling, i.e., their dot product ®휌1 · ®휌2 ≈0. Our\nsolution encodes inputs using hypervectors with random exponen-\ntial elements, e.g., ®휌1 belongs to {푒푖휃: 휃∈[−휋, 휋]}퐷. 퐷is the\ndimensionality of these hypervectors.\nAs Fig. 2 shows, through well-deﬁned hypervector operations,\nthe original information is evenly distributed across all hypervec-\ntor elements, i.e., a holographic representation. The advantage of\nbeing holographic is that we can accumulate information by sim-\nply combining two hypervectors. The complex-valued position\nhypervectors used in this paper enable the encoder to capture the\ncorrelation between input features in ﬁner granularity.\nNext, we deﬁne the following HDC mathematics that manipu-\nlates input information in high-dimensional space:\nContinuous Binding: The goal of binding is to associate items\nin hyperspace. Assuming we have an 푛-element state vector at\ntime step 푡in the RL tasks: 푆푡= {푠1,푠2, . . . ,푠푛}. Our encoding gen-\nerates a random exponential hypervector for each state element\n{ ®휌1, ®휌2, · · · , ®휌푛} and then associates state elements with these hy-\npervectors: ®S푡= ®휌푠1\n1\n∗®휌푠2\n2\n∗· · · ∗®휌푠푛\n푛. We deﬁne ®휌푠푘\n푘\nto be the\ncomponent-wise exponential of ®휌푘.\nBundling: This operation stands for component-wise addition\nof hypervectors. The bundling operation is the core of memo-\nrization for HDC models, in which the information from multi-\nple hypervectors is saved into one single hypervector. The bun-\ndled hypervector is similar to every component hypervector, i.e.,\n( ®휌1 + ®휌2) · ®휌1 >> 0. Thus, we represent sets using the bundling\nEﬀicient Oﬀ-Policy Reinforcement Learning via Brain-Inspired Computing\nGLSVLSI ’23, June 5–7, 2023, Knoxville, TN, USA\noperation. Bundling of several encoded states results in a model\nhypervector: ®\nM = ®S1 + ®S2 + · · · + ®S푚. In Section 3.3, we leverage\na weighted bundling to represent it in high-dimensional space, i.e.,\n®\nM = 훼1 ®S1 +훼2 ®S2 +· · ·+훼푚®S푚. The 훼s are learned via HDC-based\nregression.\nHypervector Similarity: Our HDC encoder aims at preserving\nthe distance relationship among inputs. It maps similar input state\nvectors to similar locations in the high-dimensional space, i.e., the\nsimilarity between encoded hypervectors is close to 1. To verify this,\nwe deﬁne a hypervector similarity metric:훿( ®휌푥· ®휌푦) = ®휌푥· ®휌†푦/퐷,\nwhere ®휌†푦is the element-wise conjugate. Then, assuming 푥≈푦,\nwe have: ®휌푥· ®휌†푦/퐷= 퐷−1 Í\n푑푒푖휃푑(푦−푥) ≈1.\n3.3\nQHD Hyperdimensional Regression\nWe develop a regression model based on hyperdimensional math-\nematics. Our regression consists of multiple model hypervectors\n{ ®\nM1, ®\nM2, . . . , ®\nM푛}, where 푛is the size of the action space. For\nevaluation of each action at time step 푡, we only select one of the\nmodel hypervectors ®\nM퐴that corresponds to a certain action 퐴, and\nthe regression is operated on the current-step state hypervector\n®S푡. These model hypervectors are initialized to all zero elements\nand have the same dimensionality as the encoded state hypervec-\ntor, i.e., ®\nM퐴∈{0}D. In regression, the true value is given by the\nideal Q-function, and we use hyperdimensional regression to ap-\nproximately calculate the Q-value. We explain the ground truth for\nQ-value in Section 3.4 when we introduce QHD. On the other hand,\nthe approximated Q-value for action 퐴equals the real component\nin the dot product between the model hypervector and the encoded\nstate hypervector: 푞푝푟푒푑= 푟푒푎푙( ®\nM퐴· ®S †/퐷), where ®S † is a con-\njugate of the encoded query with complex-valued elements (recall\nSection 3.2). As for the regression model update, we use the error\nbetween 푞푝푟푒푑and 푞푡푟푢푒(ground truth). We either add or subtract\na portion of the state hypervector to the model, weighted by the\nregression error: ®\nM퐴= ®\nM퐴+ 훽(푞푡푟푢푒−푞푝푟푒푑) × ®S. This equation\nensures that the model gets updated more aggressively for higher\nprediction error rates (푞푡푟푢푒−푞푝푟푒푑≫0). The lightweight opera-\ntions in our regression design, such as component-wise addition,\ncontribute to the fast learning process for QHD.\n3.4\nHyperdimensional Value-based\nReinforcement Learning\nIn this section, we present the details for our QHD, a hyperdimen-\nsional Q-learning algorithm. We start our introduction with how\nagents with QHD make decisions at each time step. In QHD, we\nuse a greedy policy that prefers actions with higher Q-values. How-\never, it is crucial to balance the exploration of the environment\nand the exploitation of the learned model. We combine a random\nexploration strategy with the greedy policy, i.e., 휖-decay policy.\nAssuming the action space A and time step 푡:\n퐴푡=\n\u001arandom action 퐴∈A,\nwith probability 휖\n푎푟푔푚푎푥퐴∈A푄(푆푡,퐴),\nwith probability 1 −휖\n(1)\nThe probability of selecting random actions will gradually drop\nafter the agent explores and learns for several episodes. In experi-\nments, we use a rate of changing 휖-decay less than 1; this ensures\nthat QHD agents start to trust their learned model more while\ngradually lessening the importance of exploration. In the equa-\ntion above, 푄(푆푡,퐴) is a hyperdimensional regression model that\nreturns approximated Q-values for input action-state pairs. Once\nan action 퐴푡is chosen by QHD, the agent interacts with the envi-\nronment. We then obtain the new state 푆푡+1 for the agent and the\nfeedback reward 푅푡from the environment. At the next time step\n푡+ 1, QHD selects another action 퐴푡+1. This chain of actions and\nfeedbacks form a trajectory or an episode until some termination\nconditions are met. To train an RL algorithm, these episodes or past\nexperiences are usually saved to local memory as training samples.\nMore speciﬁcally, we save a tuple of four elements for each step in\nthe experience replay buﬀer: (푆푡,퐴푡, 푅푡,푆푡+1).\nIn DQN, the RL training process and parameter update are based\non DNN back-propagation, while the training in QHD utilizes more\neﬃcient hypervector operations. The regression model in QHD is\ntrained at the end of each time step after saving current information\nto the replay buﬀer. As in the DQN training process, we apply a\nstrategy called experience replay. The experience replays in QHD\nsamples a training batch and uses it for the regression model update.\nThe training batch includes multiple tuples of past experiences.\nNow assume we sample a one-step experience tuple from past\ntrajectories to train our QHD, i.e., (푆푡,퐴푡, 푅푡,푆푡+1). In Section 3.3,\nwe introduce the regression model update based on the approxima-\ntion error. We ﬁrst encode the input state 푆푡to the hypervector ®S푡\nand the predicted value 푞푡_푝푟푒푑is simply calculated as:\n푞푡_푝푟푒푑= Q(푆푡,퐴푡) = 푟푒푎푙( ®\nM퐴푡· ®S †\n푡/퐷)\n(2)\nFor regression training, we need a ground truth 푞푡_푡푟푢푒. We\ncannot directly obtain the true Q-value because RL is not typical\nsupervised learning. For time step 푡, the feedback is the one-step\nreward 푅푡while the Q-value is the expectation of accumulated\nrewards. The method to connect these two values is called the\nBellman Equation or Dynamic Programming Equation [18]. Since\nmost RL tasks can be viewed as a Markov Decision Process (MDP),\nthe Bellman equation gives a recursive expression for the Q-value\nat step 푡, the expected sum of current rewards, and the Q-value\nfor step 푡+ 1. To learn an optimal Q-function, we use the Bellman\noptimality equation as shown below:\n푞푡_푡푟푢푒= 푅푡+ 훾푚푎푥퐴Q′(푆푡+1,퐴)\n(3)\nRecall that our objective in QHD is to achieve optimal policy\nand maximize the accumulated rewards within one episode. The\nBellman optimality equation states that to achieve optimal results\nfor the whole task, we need to optimize each sub-task. Thus, the true\nvalue푞푡_푡푟푢푒is the sum of푅푡and the max next-step Q-value. Instead\nof using model Q to calculate the maximum next-step Q-value, we\nuse a delayed model Q′ which gets updated periodically using\nparameters in Q. This method is called Double Q-learning [19]; it\nstabilizes the learning process and avoids the overestimation of\nQ-value caused by the maximization in the Bellman equation. We\nalso include a reward decay term 훾that adjusts the eﬀect of future\nrewards on the current step Q-value.\nAfter obtaining the predicted Q-value and true Q-value, we per-\nform regression model updates. We update the model corresponding\nto the action taken, using the regression error 푞푡_푡푟푢푒−푞푡_푝푟푒푑and\nthe encoded state hypervector. The learning rate is 훽.\n®\nM퐴푡+1 = ®\nM퐴푡+ 훽(푞푡_푡푟푢푒−푞푡_푝푟푒푑) × ®S푡\n(4)\nGLSVLSI ’23, June 5–7, 2023, Knoxville, TN, USA\nYang Ni, Danny Abraham, Mariam Issa, Yeseong Kim, Pietro Mercati, and Mohsen Imani\n0\n200\n400\n600\n800\nQHD\nDQN\n0\n40\n80\n120\nQHD\nDQN\n0\n10\n20\n30\n40\nQHD\nDQN\n-200\n-150\n-100\n-50\n0\nQHD\nDQN\n0\n100\n200\n300\n400\nQHD\nDQN\n0\n200\n400\n600\n800\n1000\nQHD\nDQN\nRewards\nQHD\nDQN\nEpisode\nRuntime (s)\nEpisode\nRuntime (s)\nRewards\n(a) Cartpole\n(b) Acrobot\n160\n180\n200\n220\n240\nQHD\nDQN\n0\n200\n400\n600\n800\n1000\nQHD\nDQN\n0\n2000\n4000\n6000\nQHD\nDQN\nRuntime (s)\nEpisode\nRewards\n(c) LunarLander\nFigure 3: Final reward and goal-achieved runtime for DQN and QHD.\nRewards\nRewards\n(a)\n(b)\nEpisode\nRuntime (s)\nQHD\nDQN\n0\n200\n400\n600\n800\n0\n50\n100\n150\n200\n0\n200\n400\n600\n800\n0\n100\n200\n300\n400\nFigure 4: QHD and DQN Cartpole rewards comparison: (a) is\nplot with episode index and (b) is plot with runtime as x-axis.\nRewards\nRewards\n(a)\n(b)\n-600\n-500\n-400\n-300\n-200\n-100\n0\n0\n100 200 300 400 500\n-600\n-500\n-400\n-300\n-200\n-100\n0\n0\n200 400 600 800 1000\nEpisode\nRuntime (s)\nQHD\nDQN\nFigure 5: QHD and DQN Acrobot rewards comparison with\nboth episode index and runtime index.\n4\nEXPERIMENTAL RESULT\n4.1\nExperiment Settings\nWe implement our QHD algorithm using Python on both desktop\n(Intel Core-i7 10700 with 65W TDP) and embedded hardware plat-\nforms (RaspberryPi 4 with 6W TDP). We validate the functionality\nof QHD with multiple control tasks in the OpenAI Gym [9]. For\ncomparison, we use the DQN algorithm for the same tasks in our\nevaluation. In the following subsections, we compare these two\nmethods’ learning performance and eﬃciency in all tasks.\nThe regression model we used in QHD has dimensionality 퐷=\n6000 unless stated otherwise. This dimensionality setting provides\nus with a balance between learning quality and runtime cost; a\nlarger dimensionality will generally lead to higher rewards achieved\nin the RL tasks with the cost of larger computation. The DQN is\npowered by a neural network with two hidden layers. The ﬁrst\nlayer has 128 neurons, and the second one has 256; except in the\nLunarLander task, where we use 64 neurons for the ﬁrst layer and\n128 for the second layer. The experience replay is enabled for model\ntraining in both methods, and we assume nearly unlimited replay\nbuﬀer capacity for rewards and runtime comparison. We select\ndiﬀerent parameters for sampling training batches to ensure the\nbest learning quality for both methods. The QHD training batch\nsize is 4 for Acrobot/Cartpole and 10 for LunarLander, and the DQN\ntraining batch size is 64 for all tasks. Rewards and runtime results\nfor both methods are averaged over multiple trials.\nTask\nAcrobot\nCartpole\nLunarLander\nAlgorithm\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDesktop CPU\n푇푔표푎푙(s)\n815\n82\n36\n11\n5379\n2740\nRaspberryPi\n푇푔표푎푙(s)\n7041\n476\n435\n72\n51532\n14483\nScale Ratio\n8.6\n5.8\n12.3\n6.8\n9.6\n5.3\nTable 1: QHD and DQN goal-achieved runtime (푇푔표푎푙) com-\nparison on both desktop and embedded platforms\n4.2\nRL Rewards & Runtime comparison\nFig. 3 compares the performance of DQN and QHD over three\npopular OpenAI control tasks. As shown in Fig. 3(a) and 4, QHD\nachieves signiﬁcantly higher ﬁnal rewards in Cartpole compared\nto DQN. Within 200 episodes, QHD provides an averaged episodic\nreward over 660, which is nearly 4× higher than DQN. In the early\nepisodes, this may lead to smaller rewards; but after the warm-up,\nQHD can quickly learn from accumulated experience and surpass\nDQN. Notice that considering the total execution time (shown in\nFig. 4b), QHD is signiﬁcantly faster than DQN and reaches higher\nrewards within the same amount of time. We also present the result\nfor Acrobot in Fig. 3(b) and 5. Our QHD provides notably better\nlearning eﬃciency compared to DQN, e.g., about 10× faster in terms\nof runtime and 2× fewer number of episodes. For LunarLander, we\ncompare the RL performance and runtime in Fig. 3(c). It shows that\ncompared to DQN, our QHD achieves the goal nearly 400 episodes\nearlier than DQN and about 2600 seconds (2 times) faster in runtime.\nIn Table 1, we collect the results for the implementation of QHD\nand DQN on embedded CPU. We show that the eﬃciency beneﬁt\nof our algorithm remains signiﬁcant in a power-limited environ-\nment. Thanks to the lightweight QHD learning process and the\nhypervector representation, our algorithm scales better than the\ndeep network structure in DQN, i.e., a constantly smaller runtime\nscale ratio. In addition, the speedup brought by QHD is about 15×\nin Acrobot, 6× in Cartpole, and 3.55× in LunarLander.\n4.3\nEvaluate the eﬀect of training batch size\nBoth QHD and DQN rely on experience replay, and since the ex-\nperience replay buﬀer is ideally inﬁnite, we need to sample the\ntraining dataset from the large replay buﬀer with a preset batch\nsize. This parameter is rather crucial because it controls how much\npast experience is available for the agent to learn from, thereby\ndeeply inﬂuencing the learning quality. A larger batch size pre-\nvents the agent from forgetting past experiences while bringing\ngreater costs. Our QHD, on the other hand, aims to fully utilize the\nprovided training samples at each step. In Fig. 6, we explore the\nEﬀicient Oﬀ-Policy Reinforcement Learning via Brain-Inspired Computing\nGLSVLSI ’23, June 5–7, 2023, Knoxville, TN, USA\n-600\n-500\n-400\n-300\n-200\n-100\n0\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\n2\n4\n10\n15\n20\n25\n30\n0\n400\n800\n1200\n1600\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\n2\n4\n10\n15\n20\n25\n30\n(a) Acrobot Average Rewards\n(b) Acrobot Total Runtime (s)\nBatch \nSize\nBatch \nSize\nTotal runtime\nGoal achieved\nGoal: -120\nFigure 6: Explore the eﬀect of batch size.\neﬀect of replay batch size on both methods. Fig. 6a compares the\naverage rewards for the last 100 episodes, and it is clear that our\nQHD performs signiﬁcantly better. For example, when the batch\nsize is 2, QHD can still achieve the goal with an average of -102.9\nrewards. On the other hand, DQN performs poorly with a reward of\n-480.9. This means that DQN does not eﬃciently utilize the limited\navailable training samples.\nApart from better performance, our QHD also provides higher\neﬃciency. In Fig. 6(b), we provide both the QHD runtime for 500\nepisodes and when the goal is achieved. For DQN, only total runtime\nis provided because DQN cannot achieve the goal with small batch\nsizes from 2 to 30. Our QHD is constantly faster: with the batch size\nof 2 (15), our QHD is about 6.5× (1.7×) faster than DQN in terms of\ntotal runtime. Focusing on the actual runtime when achieving the\ntarget, QHD shows an even larger improvement, e.g., the speedup\nis about 12.3× (2.6×) with a batch size of 2 (30).\n4.4\nQHD vs. DQN with Limited Size of\nExperience Replay Buﬀer\nIn the above sections, we set the RL experience replay to have\ninﬁnite capacity, i.e., the agent has access to all previous experiences\nduring the training. However, in practical implementations, the\nmemory available for experience replay is limited due to energy and\nspace budgets. Thus, in this section, we evaluate the performance\nof our QHD with a tighter cap on the maximum replay buﬀer size\nand compare it to the DQN results.\nIn Fig. 7(a), we present the average reward achieved by both\nmethods under diﬀerent buﬀer sizes. The reward is averaged over\nthe last 100 episodes. When collecting these results, we ﬁx the\ntraining batch size; the batch size is 4 for QHD and 64 for DQN.\nThe ﬁgure shows that DQN performs poorly when the buﬀer size\nis 64 and 128, with an average reward of -500. However, our QHD\ncan reach that goal even with a buﬀer size as large as its batch size.\nThese results show that QHD can perform RL tasks with online\nlearning, i.e., a tiny replay buﬀer.\nWe also take one step further to explore the QHD capability\nof real-time learning. We set both the batch and buﬀer sizes to 1,\nwhich means the agent will learn based on only the current sample.\nWe use DQN with a 256 buﬀer size and 64 batch size as an online-\nlearning comparison. As shown in Fig. 7(b), with a larger buﬀer and\nbatch size, DQN achieves signiﬁcantly lower rewards (-345.4). For a\n500-episode training, our QHD achieves average rewards of -113.7\nusing 83 seconds, which leads to a 34.6× speedup in total runtime.\n5\nCONCLUSION\nWe propose a novel lightweight value-based oﬀ-policy RL algorithm\nbased on brain-inspired HDC. QHD utilizes HDC for high-quality\n-600\n-500\n-400\n-300\n-200\n-100\n0\nQHD\nQHD\nQHD\nQHD\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\n4\n8\n16 32\n64\n128\n256\n512\n1k\n-400\n-300\n-200\n-100\n0\nQHD\nDQN\n0\n500\n1000\n1500\n2000\n2500\n3000\nQHD\nDQN\n(a) Learn with limited memory size\n(b) QHD Real-time learning\nRewards\nRewards\nMem \nSize\nTotal Runtime (s)\nGoal: -120\nFigure 7: QHD learning eﬃciency with tiny replay buﬀer.\nQ-value approximation and self-learning agent training. Our evalu-\nation of several tasks shows that QHD provides signiﬁcantly better\neﬃciency and learning quality than DQN.\nACKNOWLEDGEMENTS\nThis work was supported in part by National Science Foundation\n#2127780, Semiconductor Research Corporation (SRC), Department\nof the Navy, Oﬃce of Naval Research, grant #N00014-21-1-2225 and\n#N00014-22-1-2067, Air Force Oﬃce of Scientiﬁc Research, grant\n#FA9550-22-1-0253, Institute of Information & communications\nTechnology Planning & Evaluation (IITP) grant funded by the Korea\ngovernment(MSIT) (No.2022-0-00991, 1T-1C DRAM Array Based\nHigh-Bandwidth, Ultra-High Eﬃciency Processing-in-Memory Ac-\ncelerator), and a generous gift from Cisco.\nREFERENCES\n[1] M. Botvinick et al., “Reinforcement learning, fast and slow,” Trends in cognitive\nsciences, vol. 23, no. 5, pp. 408–422, 2019.\n[2] T. Qian et al., “Deep reinforcement learning for ev charging navigation by coor-\ndinating smart grid and intelligent transportation system,” IEEE transactions on\nsmart grid, vol. 11, no. 2, pp. 1714–1723, 2019.\n[3] V. Mnih et al., “Playing atari with deep reinforcement learning,” arXiv preprint\narXiv:1312.5602, 2013.\n[4] M. Imani et al., “Partially-observed discrete dynamical systems,” in ACC.\nIEEE,\n2021, pp. 310–315.\n[5] Y. He et al., “Software-deﬁned networks with mobile edge computing and caching\nfor smart cities: A big data deep reinforcement learning approach,” IEEE Commu-\nnications Magazine, vol. 55, no. 12, pp. 31–37, 2017.\n[6] P. Kanerva, “Hyperdimensional computing: An introduction to computing in\ndistributed representation with high-dimensional random vectors,” Cognitive\ncomputation, vol. 1, no. 2, pp. 139–159, 2009.\n[7] A. Moin et al., “A wearable biosensing system with in-sensor adaptive machine\nlearning for hand gesture recognition,” Nature Electronics, 2021.\n[8] A. Rahimi et al., “Hyperdimensional computing for blind and one-shot classiﬁca-\ntion of eeg error-related potentials,” Mobile Networks and Applications, vol. 25,\nno. 5, pp. 1958–1969, 2020.\n[9] G. Brockman et al., “Openai gym,” arXiv preprint arXiv:1606.01540, 2016.\n[10] J. Ke et al., “Optimizing online matching for ride-sourcing services with multi-\nagent deep reinforcement learning,” arXiv preprint arXiv:1902.06228, 2019.\n[11] H.-H. Tseng et al., “Deep reinforcement learning for automated radiation adapta-\ntion in lung cancer,” Medical physics, vol. 44, no. 12, pp. 6690–6705, 2017.\n[12] F.-C. Ghesu et al., “Multi-scale deep reinforcement learning for real-time 3d-\nlandmark detection in ct scans,” IEEE TPAMI, vol. 41, no. 1, pp. 176–189, 2017.\n[13] P. Poduval et al., “Graphd: Graph-based hyperdimensional memorization for\nbrain-like cognitive learning,” Frontiers in Neuroscience, p. 5, 2022.\n[14] A. Hernandez-Cane et al., “Onlinehd: Robust, eﬃcient, and single-pass online\nlearning using hyperdimensional system,” in DATE.\nIEEE, 2021, pp. 56–61.\n[15] Z. Zou et al., “Eventhd: Robust and eﬃcient hyperdimensional learning with\nneuromorphic sensor,” Frontiers in Neuroscience, vol. 16, 2022.\n[16] Y. Kim et al., “Eﬃcient human activity recognition using hyperdimensional\ncomputing,” in IOT, 2018, pp. 1–6.\n[17] Y. Ni et al., “Hdpg: hyperdimensional policy-based reinforcement learning for\ncontinuous control,” in Proceedings of the 59th ACM/IEEE Design Automation\nConference, 2022, pp. 1141–1146.\n[18] R. Bellman, “On the theory of dynamic programming,” Proceedings of the National\nAcademy of Sciences of the United States of America, vol. 38, no. 8, p. 716, 1952.\n[19] H. Hasselt, “Double q-learning,” Advances in neural information processing systems,\nvol. 23, pp. 2613–2621, 2010.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NE"
  ],
  "published": "2022-05-14",
  "updated": "2023-06-21"
}