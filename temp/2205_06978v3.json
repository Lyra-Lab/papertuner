{
  "id": "http://arxiv.org/abs/2205.06978v3",
  "title": "Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing",
  "authors": [
    "Yang Ni",
    "Danny Abraham",
    "Mariam Issa",
    "Yeseong Kim",
    "Pietro Mercati",
    "Mohsen Imani"
  ],
  "abstract": "Reinforcement Learning (RL) has opened up new opportunities to enhance\nexisting smart systems that generally include a complex decision-making\nprocess. However, modern RL algorithms, e.g., Deep Q-Networks (DQN), are based\non deep neural networks, resulting in high computational costs. In this paper,\nwe propose QHD, an off-policy value-based Hyperdimensional Reinforcement\nLearning, that mimics brain properties toward robust and real-time learning.\nQHD relies on a lightweight brain-inspired model to learn an optimal policy in\nan unknown environment. On both desktop and power-limited embedded platforms,\nQHD achieves significantly better overall efficiency than DQN while providing\nhigher or comparable rewards. QHD is also suitable for highly-efficient\nreinforcement learning with great potential for online and real-time learning.\nOur solution supports a small experience replay batch size that provides 12.3\ntimes speedup compared to DQN while ensuring minimal quality loss. Our\nevaluation shows QHD capability for real-time learning, providing 34.6 times\nspeedup and significantly better quality of learning than DQN.",
  "text": "Eï¬€icient Oï¬€-Policy Reinforcement Learning via Brain-Inspired\nComputing\nYang Ni\nyni3@uci.edu\nUniversity of California, Irvine\nIrvine, California, USA\nDanny Abraham\ndannya1@uci.edu\nUniversity of California, Irvine\nIrvine, California, USA\nMariam Issa\nmariamai@uci.edu\nUniversity of California, Irvine\nIrvine, California, USA\nYeseong Kim\nyeseongkim@dgist.ac.kr\nDaegu Gyeongbuk Institute of\nScience and Technology\nDaegu, Republic of Korea\nPietro Mercati\npietro.mercati@intel.com\nIntel Labs\nHilsboro, Oregon, USA\nMohsen Imani\nm.imani@uci.edu\nUniversity of California, Irvine\nIrvine, California, USA\nABSTRACT\nReinforcement Learning (RL) has opened up new opportunities\nto enhance existing smart systems that generally include a com-\nplex decision-making process. However, modern RL algorithms,\ne.g., Deep Q-Networks (DQN), are based on deep neural networks,\nresulting in high computational costs. In this paper, we propose\nQHD, an oï¬€-policy value-based Hyperdimensional Reinforcement\nLearning, that mimics brain properties toward robust and real-\ntime learning. QHD relies on a lightweight brain-inspired model\nto learn an optimal policy in an unknown environment. On both\ndesktop and power-limited embedded platforms, QHD achieves\nsigniï¬cantly better overall eï¬ƒciency than DQN while providing\nhigher or comparable rewards. QHD is also suitable for highly-\neï¬ƒcient reinforcement learning with great potential for online and\nreal-time learning. Our solution supports a small experience replay\nbatch size that provides 12.3Ã— speedup compared to DQN while en-\nsuring minimal quality loss. Our evaluation shows QHD capability\nfor real-time learning, providing 34.6Ã— speedup and signiï¬cantly\nbetter quality of learning than DQN.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Machine learning; Intelligent\nagents; â€¢ Computer systems organization â†’Embedded systems.\nKEYWORDS\nHyperdimensional Computing, Brain-inspired Computing\nACM Reference Format:\nYang Ni, Danny Abraham, Mariam Issa, Yeseong Kim, Pietro Mercati, and Mohsen\nImani. 2023. Eï¬ƒcient Oï¬€-Policy Reinforcement Learning via Brain-Inspired\nComputing. In Proceedings of the Great Lakes Symposium on VLSI 2023\n(GLSVLSI â€™23), June 5â€“7, 2023, Knoxville, TN, USA. ACM, New York, NY, USA,\n5 pages. https://doi.org/10.1145/3583781.3590298\nThis work is under a Creative Commons Attribution-\nNonCommercial-NoDerivs International 4.0 License.\nGLSVLSI â€™23, June 5â€“7, 2023, Knoxville, TN, USA\nÂ© 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0125-2/23/06.\nhttps://doi.org/10.1145/3583781.3590298\n1\nINTRODUCTION\nSmart systems and services generally reside in a highly-dynamic\nyet unknown environment and require intelligent algorithms to\nmake optimal decisions with little prior knowledge. In recent years,\nReinforcement Learning (RL) has opened up new opportunities to\nsolve a wide range of complex predictions and decision-making\ntasks that were previously out of reach for a machine [1]. Compared\nto supervised and unsupervised learning methods, RL does not\nhave direct access to labeled training data. Learning through agent-\nenvironment interaction makes RL appealing to dynamic control\nand automated system optimization such as smart transportation\nand smart grid, where the optimal policy is hard to deï¬ne and is\nconstantly changing with its environment [1, 2].\nRL methods are generally categorized into policy-based and\nvalue-based RL. The policy-based method directly parameterizes\nthe policy and optimizes it via on-policy model training. On the\nother hand, value-based RL supports oï¬€-policy training, i.e., all past\ninteractions can be used toward learning. Therefore, value-based\nRL is much more sample-eï¬ƒcient. As one of the most popular value-\nbased methods, Deep Q-Networks (DQN) exploits DNN to learn an\napproximation of the Q-value for every pair of actions and state.\nRecently, there have been active developments for various DQN\napplications such as playing computer games [3], Genomics [4], and\nsmart city [2, 5]. DQN is capable of learning complex tasks without\nmodeling the environment, but its power comes at a price, i.e., the\nhuge computation cost and long learning time. This makes it only\nsuitable for powerful computers in the cloud. However, oï¬„oading\nRL to the cloud not only leads to extra communication overhead\nbut also causes security and privacy concerns.\nTherefore, we redesign the RL algorithm by exploiting the brain-\ninspired highly-eï¬ƒcient HyperDimensional Computing (HDC) [6].\nHDC is motivated by how human brains process diï¬€erent kinds of\ninputs, i.e., brains express information using a vast number of neu-\nrons. The information is then processed and memorized in a holistic\nand high-dimensional way. For inputs in the lower-dimensional\nspace, HDC encodes them to vectors of several thousand dimen-\nsions, i.e., hypervectors. The learning process is based on highly-\nparallelizable operations of hypervectors. HDC has been applied as\na lightweight machine learning solution to multiple applications\nwhere it is capable of achieving comparable accuracy to DNN with\nsigniï¬cantly higher eï¬ƒciency [7, 8].\nGLSVLSI â€™23, June 5â€“7, 2023, Knoxville, TN, USA\nYang Ni, Danny Abraham, Mariam Issa, Yeseong Kim, Pietro Mercati, and Mohsen Imani\nHowever, current HDC solutions mainly focus on traditional\nclassiï¬cation and clustering. In contrast, in this paper, we propose\nQHD, a value-based Hyperdimensional Reinforcement Learning\nalgorithm with oï¬€-policy training, which mimics brain properties\ntowards robust and real-time learning. The main contributions of\nthe paper are listed as follows:\nâ€¢ To the best of our knowledge, QHD is the ï¬rst oï¬€-policy value-\nbased hyperdimensional RL algorithm targeting discrete action\nspace. QHD relies on lightweight HDC models to learn an optimal\npolicy in an unknown environment. Our algorithm maps state-\naction space into high-dimensional space for eï¬ƒcient decision-\nmaking via novel brain-inspired HDC encoding and self-learning.\nâ€¢ Thanks to the brain-like hyperdimensional operations, QHD can\nutilize even a small amount of available training data. It thereby\nsupports a much smaller training batch and experience buï¬€er\nthan DQN while still providing high-quality results.\nâ€¢ We compare our QHD accuracy and eï¬ƒciency with the DQN\nalgorithm for multiple dynamic control tasks. Our evaluation\nshows that QHD achieves signiï¬cantly better overall eï¬ƒciency\nthan DQN, especially on the power-limited embedded platform,\ne.g., up to about 15Ã— speedup. For real-time learning, QHD pro-\nvides 34.6Ã— speedup and signiï¬cantly better quality.\n2\nRELATED WORK\nReinforcement Learning: In recent years, RL algorithms have\nobtained dramatically more attention because of the advancement\nin deep learning. For example, DQN greatly expands the application\nof RL to ï¬elds like computer games [3, 9], transportation optimiza-\ntion [2, 10], and health care [11, 12]. In [10], researchers focus on\ndriver dispatch optimization within online ride-sharing services.\nThey use DQN to learn a policy for matching available drivers and\nusers to maximize the success rate while minimizing the wait time.\nAll works above utilize DNN to handle complex agent-environment\ninteractions, so they are computationally intensive with insuï¬ƒcient\neï¬ƒciency. In contrast, we propose a brain-inspired reinforcement\nlearning solution with inherent eï¬ƒciency and robustness.\nHyperdimensional Computing: Prior HDC works mainly pro-\nvide solutions to classiï¬cation and cognitive tasks, such as graph\nreasoning [13], bio-signal processing [7, 8], speech recognition [14],\nneuromorphic sensing [15] and multi-sensor signal classiï¬cation [16].\nIn these highlighted applications, HDC has outperformed state-of-\nthe-art learning solutions, e.g., support vector machines [14] and\nneural networks [16]. Recent orthogonal work proposes an HDC-\nbased policy-based RL speciï¬cally for continuous control tasks [17].\nHowever, this work does not provide support for RL tasks with\ndiscrete action space. In addition, as mentioned in Section 1, policy-\nbased RL methods are less sample-eï¬ƒcient due to the lack of oï¬€-\npolicy training. Unlike all prior works, this paper is the ï¬rst eï¬€ort\nfocusing on hyperdimensional oï¬€-policy value-based RL.\n3\nQHD: HYPERDIMENSIONAL Q-LEARNING\n3.1\nOverview\nFig. 1 shows an overview of QHD supporting hyperdimensional\nreinforcement learning. In our RL task, there are two components\n(Agent and Environment) and three variables (Action, State, and\nReward). Fig. 1(a) exploits a Cartpole example to illustrate these\nAgent\nÎ¸\nv2\nv1\nEnvironment\nx\n0\nAction\nReward\nActions: \n(Left, Right)\nStates:\n(x,v1,v2,Î¸)\n(a)\n(b)\nState \nğ‘†ğ‘¡\nActions \nğ´ğ‘¡\nHDC \nEncoder\nHDC-based \nRegression\nQ-values for \nDifferent Actions\nAction with \nthe Highest \nQ-value\nHD-based Q-learning (QHD)\nMemory\nState \nHypervector\nFigure 1: Overview of QHD reinforcement learning.\nÏ1\nS1\nPosition HVs\nAssociated HVs\nÏ2\nS2\nÏn\nSn\n*\nState\nS1\nS2\nSn\nEncoded State\nComplex Ïs: ğ‘’ğ‘–Î¸\nÎ¸ âˆˆ[âˆ’ğœ‹, +ğœ‹]\nFigure 2: HDC encoding with complex-valued position hy-\npervectors (HVs).\ncomponents and variables. As shown in Fig. 1, the interaction be-\ntween the agent and environment forms a loop in which the action\ntaken based on the current state leads to the next state and reward.\nThe trajectory of each episode is saved in local memory for later\nlearning. In Fig. 1(b), we provide an overview of QHD algorithm\nguiding the agent in the decision-making process.\n3.2\nQHD Hyperdimensional Encoding\nQHD starts by mapping the current state vector from the original\nto high-dimensional space, i.e., hypervector encoding. Notice that\nwe can create a large number of near-orthogonal hypervectors\nthrough random sampling, i.e., their dot product Â®íœŒ1 Â· Â®íœŒ2 â‰ˆ0. Our\nsolution encodes inputs using hypervectors with random exponen-\ntial elements, e.g., Â®íœŒ1 belongs to {í‘’í‘–íœƒ: íœƒâˆˆ[âˆ’íœ‹, íœ‹]}í·. í·is the\ndimensionality of these hypervectors.\nAs Fig. 2 shows, through well-deï¬ned hypervector operations,\nthe original information is evenly distributed across all hypervec-\ntor elements, i.e., a holographic representation. The advantage of\nbeing holographic is that we can accumulate information by sim-\nply combining two hypervectors. The complex-valued position\nhypervectors used in this paper enable the encoder to capture the\ncorrelation between input features in ï¬ner granularity.\nNext, we deï¬ne the following HDC mathematics that manipu-\nlates input information in high-dimensional space:\nContinuous Binding: The goal of binding is to associate items\nin hyperspace. Assuming we have an í‘›-element state vector at\ntime step í‘¡in the RL tasks: í‘†í‘¡= {í‘ 1,í‘ 2, . . . ,í‘ í‘›}. Our encoding gen-\nerates a random exponential hypervector for each state element\n{ Â®íœŒ1, Â®íœŒ2, Â· Â· Â· , Â®íœŒí‘›} and then associates state elements with these hy-\npervectors: Â®Sí‘¡= Â®íœŒí‘ 1\n1\nâˆ—Â®íœŒí‘ 2\n2\nâˆ—Â· Â· Â· âˆ—Â®íœŒí‘ í‘›\ní‘›. We deï¬ne Â®íœŒí‘ í‘˜\ní‘˜\nto be the\ncomponent-wise exponential of Â®íœŒí‘˜.\nBundling: This operation stands for component-wise addition\nof hypervectors. The bundling operation is the core of memo-\nrization for HDC models, in which the information from multi-\nple hypervectors is saved into one single hypervector. The bun-\ndled hypervector is similar to every component hypervector, i.e.,\n( Â®íœŒ1 + Â®íœŒ2) Â· Â®íœŒ1 >> 0. Thus, we represent sets using the bundling\nEï¬€icient Oï¬€-Policy Reinforcement Learning via Brain-Inspired Computing\nGLSVLSI â€™23, June 5â€“7, 2023, Knoxville, TN, USA\noperation. Bundling of several encoded states results in a model\nhypervector: Â®\nM = Â®S1 + Â®S2 + Â· Â· Â· + Â®Sí‘š. In Section 3.3, we leverage\na weighted bundling to represent it in high-dimensional space, i.e.,\nÂ®\nM = í›¼1 Â®S1 +í›¼2 Â®S2 +Â· Â· Â·+í›¼í‘šÂ®Sí‘š. The í›¼s are learned via HDC-based\nregression.\nHypervector Similarity: Our HDC encoder aims at preserving\nthe distance relationship among inputs. It maps similar input state\nvectors to similar locations in the high-dimensional space, i.e., the\nsimilarity between encoded hypervectors is close to 1. To verify this,\nwe deï¬ne a hypervector similarity metric:í›¿( Â®íœŒí‘¥Â· Â®íœŒí‘¦) = Â®íœŒí‘¥Â· Â®íœŒâ€ í‘¦/í·,\nwhere Â®íœŒâ€ í‘¦is the element-wise conjugate. Then, assuming í‘¥â‰ˆí‘¦,\nwe have: Â®íœŒí‘¥Â· Â®íœŒâ€ í‘¦/í·= í·âˆ’1 Ã\ní‘‘í‘’í‘–íœƒí‘‘(í‘¦âˆ’í‘¥) â‰ˆ1.\n3.3\nQHD Hyperdimensional Regression\nWe develop a regression model based on hyperdimensional math-\nematics. Our regression consists of multiple model hypervectors\n{ Â®\nM1, Â®\nM2, . . . , Â®\nMí‘›}, where í‘›is the size of the action space. For\nevaluation of each action at time step í‘¡, we only select one of the\nmodel hypervectors Â®\nMí´that corresponds to a certain action í´, and\nthe regression is operated on the current-step state hypervector\nÂ®Sí‘¡. These model hypervectors are initialized to all zero elements\nand have the same dimensionality as the encoded state hypervec-\ntor, i.e., Â®\nMí´âˆˆ{0}D. In regression, the true value is given by the\nideal Q-function, and we use hyperdimensional regression to ap-\nproximately calculate the Q-value. We explain the ground truth for\nQ-value in Section 3.4 when we introduce QHD. On the other hand,\nthe approximated Q-value for action í´equals the real component\nin the dot product between the model hypervector and the encoded\nstate hypervector: í‘í‘í‘Ÿí‘’í‘‘= í‘Ÿí‘’í‘í‘™( Â®\nMí´Â· Â®S â€ /í·), where Â®S â€  is a con-\njugate of the encoded query with complex-valued elements (recall\nSection 3.2). As for the regression model update, we use the error\nbetween í‘í‘í‘Ÿí‘’í‘‘and í‘í‘¡í‘Ÿí‘¢í‘’(ground truth). We either add or subtract\na portion of the state hypervector to the model, weighted by the\nregression error: Â®\nMí´= Â®\nMí´+ í›½(í‘í‘¡í‘Ÿí‘¢í‘’âˆ’í‘í‘í‘Ÿí‘’í‘‘) Ã— Â®S. This equation\nensures that the model gets updated more aggressively for higher\nprediction error rates (í‘í‘¡í‘Ÿí‘¢í‘’âˆ’í‘í‘í‘Ÿí‘’í‘‘â‰«0). The lightweight opera-\ntions in our regression design, such as component-wise addition,\ncontribute to the fast learning process for QHD.\n3.4\nHyperdimensional Value-based\nReinforcement Learning\nIn this section, we present the details for our QHD, a hyperdimen-\nsional Q-learning algorithm. We start our introduction with how\nagents with QHD make decisions at each time step. In QHD, we\nuse a greedy policy that prefers actions with higher Q-values. How-\never, it is crucial to balance the exploration of the environment\nand the exploitation of the learned model. We combine a random\nexploration strategy with the greedy policy, i.e., íœ–-decay policy.\nAssuming the action space A and time step í‘¡:\ní´í‘¡=\n\u001arandom action í´âˆˆA,\nwith probability íœ–\ní‘í‘Ÿí‘”í‘ší‘í‘¥í´âˆˆAí‘„(í‘†í‘¡,í´),\nwith probability 1 âˆ’íœ–\n(1)\nThe probability of selecting random actions will gradually drop\nafter the agent explores and learns for several episodes. In experi-\nments, we use a rate of changing íœ–-decay less than 1; this ensures\nthat QHD agents start to trust their learned model more while\ngradually lessening the importance of exploration. In the equa-\ntion above, í‘„(í‘†í‘¡,í´) is a hyperdimensional regression model that\nreturns approximated Q-values for input action-state pairs. Once\nan action í´í‘¡is chosen by QHD, the agent interacts with the envi-\nronment. We then obtain the new state í‘†í‘¡+1 for the agent and the\nfeedback reward í‘…í‘¡from the environment. At the next time step\ní‘¡+ 1, QHD selects another action í´í‘¡+1. This chain of actions and\nfeedbacks form a trajectory or an episode until some termination\nconditions are met. To train an RL algorithm, these episodes or past\nexperiences are usually saved to local memory as training samples.\nMore speciï¬cally, we save a tuple of four elements for each step in\nthe experience replay buï¬€er: (í‘†í‘¡,í´í‘¡, í‘…í‘¡,í‘†í‘¡+1).\nIn DQN, the RL training process and parameter update are based\non DNN back-propagation, while the training in QHD utilizes more\neï¬ƒcient hypervector operations. The regression model in QHD is\ntrained at the end of each time step after saving current information\nto the replay buï¬€er. As in the DQN training process, we apply a\nstrategy called experience replay. The experience replays in QHD\nsamples a training batch and uses it for the regression model update.\nThe training batch includes multiple tuples of past experiences.\nNow assume we sample a one-step experience tuple from past\ntrajectories to train our QHD, i.e., (í‘†í‘¡,í´í‘¡, í‘…í‘¡,í‘†í‘¡+1). In Section 3.3,\nwe introduce the regression model update based on the approxima-\ntion error. We ï¬rst encode the input state í‘†í‘¡to the hypervector Â®Sí‘¡\nand the predicted value í‘í‘¡_í‘í‘Ÿí‘’í‘‘is simply calculated as:\ní‘í‘¡_í‘í‘Ÿí‘’í‘‘= Q(í‘†í‘¡,í´í‘¡) = í‘Ÿí‘’í‘í‘™( Â®\nMí´í‘¡Â· Â®S â€ \ní‘¡/í·)\n(2)\nFor regression training, we need a ground truth í‘í‘¡_í‘¡í‘Ÿí‘¢í‘’. We\ncannot directly obtain the true Q-value because RL is not typical\nsupervised learning. For time step í‘¡, the feedback is the one-step\nreward í‘…í‘¡while the Q-value is the expectation of accumulated\nrewards. The method to connect these two values is called the\nBellman Equation or Dynamic Programming Equation [18]. Since\nmost RL tasks can be viewed as a Markov Decision Process (MDP),\nthe Bellman equation gives a recursive expression for the Q-value\nat step í‘¡, the expected sum of current rewards, and the Q-value\nfor step í‘¡+ 1. To learn an optimal Q-function, we use the Bellman\noptimality equation as shown below:\ní‘í‘¡_í‘¡í‘Ÿí‘¢í‘’= í‘…í‘¡+ í›¾í‘ší‘í‘¥í´Qâ€²(í‘†í‘¡+1,í´)\n(3)\nRecall that our objective in QHD is to achieve optimal policy\nand maximize the accumulated rewards within one episode. The\nBellman optimality equation states that to achieve optimal results\nfor the whole task, we need to optimize each sub-task. Thus, the true\nvalueí‘í‘¡_í‘¡í‘Ÿí‘¢í‘’is the sum ofí‘…í‘¡and the max next-step Q-value. Instead\nof using model Q to calculate the maximum next-step Q-value, we\nuse a delayed model Qâ€² which gets updated periodically using\nparameters in Q. This method is called Double Q-learning [19]; it\nstabilizes the learning process and avoids the overestimation of\nQ-value caused by the maximization in the Bellman equation. We\nalso include a reward decay term í›¾that adjusts the eï¬€ect of future\nrewards on the current step Q-value.\nAfter obtaining the predicted Q-value and true Q-value, we per-\nform regression model updates. We update the model corresponding\nto the action taken, using the regression error í‘í‘¡_í‘¡í‘Ÿí‘¢í‘’âˆ’í‘í‘¡_í‘í‘Ÿí‘’í‘‘and\nthe encoded state hypervector. The learning rate is í›½.\nÂ®\nMí´í‘¡+1 = Â®\nMí´í‘¡+ í›½(í‘í‘¡_í‘¡í‘Ÿí‘¢í‘’âˆ’í‘í‘¡_í‘í‘Ÿí‘’í‘‘) Ã— Â®Sí‘¡\n(4)\nGLSVLSI â€™23, June 5â€“7, 2023, Knoxville, TN, USA\nYang Ni, Danny Abraham, Mariam Issa, Yeseong Kim, Pietro Mercati, and Mohsen Imani\n0\n200\n400\n600\n800\nQHD\nDQN\n0\n40\n80\n120\nQHD\nDQN\n0\n10\n20\n30\n40\nQHD\nDQN\n-200\n-150\n-100\n-50\n0\nQHD\nDQN\n0\n100\n200\n300\n400\nQHD\nDQN\n0\n200\n400\n600\n800\n1000\nQHD\nDQN\nRewards\nQHD\nDQN\nEpisode\nRuntime (s)\nEpisode\nRuntime (s)\nRewards\n(a) Cartpole\n(b) Acrobot\n160\n180\n200\n220\n240\nQHD\nDQN\n0\n200\n400\n600\n800\n1000\nQHD\nDQN\n0\n2000\n4000\n6000\nQHD\nDQN\nRuntime (s)\nEpisode\nRewards\n(c) LunarLander\nFigure 3: Final reward and goal-achieved runtime for DQN and QHD.\nRewards\nRewards\n(a)\n(b)\nEpisode\nRuntime (s)\nQHD\nDQN\n0\n200\n400\n600\n800\n0\n50\n100\n150\n200\n0\n200\n400\n600\n800\n0\n100\n200\n300\n400\nFigure 4: QHD and DQN Cartpole rewards comparison: (a) is\nplot with episode index and (b) is plot with runtime as x-axis.\nRewards\nRewards\n(a)\n(b)\n-600\n-500\n-400\n-300\n-200\n-100\n0\n0\n100 200 300 400 500\n-600\n-500\n-400\n-300\n-200\n-100\n0\n0\n200 400 600 800 1000\nEpisode\nRuntime (s)\nQHD\nDQN\nFigure 5: QHD and DQN Acrobot rewards comparison with\nboth episode index and runtime index.\n4\nEXPERIMENTAL RESULT\n4.1\nExperiment Settings\nWe implement our QHD algorithm using Python on both desktop\n(Intel Core-i7 10700 with 65W TDP) and embedded hardware plat-\nforms (RaspberryPi 4 with 6W TDP). We validate the functionality\nof QHD with multiple control tasks in the OpenAI Gym [9]. For\ncomparison, we use the DQN algorithm for the same tasks in our\nevaluation. In the following subsections, we compare these two\nmethodsâ€™ learning performance and eï¬ƒciency in all tasks.\nThe regression model we used in QHD has dimensionality í·=\n6000 unless stated otherwise. This dimensionality setting provides\nus with a balance between learning quality and runtime cost; a\nlarger dimensionality will generally lead to higher rewards achieved\nin the RL tasks with the cost of larger computation. The DQN is\npowered by a neural network with two hidden layers. The ï¬rst\nlayer has 128 neurons, and the second one has 256; except in the\nLunarLander task, where we use 64 neurons for the ï¬rst layer and\n128 for the second layer. The experience replay is enabled for model\ntraining in both methods, and we assume nearly unlimited replay\nbuï¬€er capacity for rewards and runtime comparison. We select\ndiï¬€erent parameters for sampling training batches to ensure the\nbest learning quality for both methods. The QHD training batch\nsize is 4 for Acrobot/Cartpole and 10 for LunarLander, and the DQN\ntraining batch size is 64 for all tasks. Rewards and runtime results\nfor both methods are averaged over multiple trials.\nTask\nAcrobot\nCartpole\nLunarLander\nAlgorithm\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDesktop CPU\ní‘‡í‘”í‘œí‘í‘™(s)\n815\n82\n36\n11\n5379\n2740\nRaspberryPi\ní‘‡í‘”í‘œí‘í‘™(s)\n7041\n476\n435\n72\n51532\n14483\nScale Ratio\n8.6\n5.8\n12.3\n6.8\n9.6\n5.3\nTable 1: QHD and DQN goal-achieved runtime (í‘‡í‘”í‘œí‘í‘™) com-\nparison on both desktop and embedded platforms\n4.2\nRL Rewards & Runtime comparison\nFig. 3 compares the performance of DQN and QHD over three\npopular OpenAI control tasks. As shown in Fig. 3(a) and 4, QHD\nachieves signiï¬cantly higher ï¬nal rewards in Cartpole compared\nto DQN. Within 200 episodes, QHD provides an averaged episodic\nreward over 660, which is nearly 4Ã— higher than DQN. In the early\nepisodes, this may lead to smaller rewards; but after the warm-up,\nQHD can quickly learn from accumulated experience and surpass\nDQN. Notice that considering the total execution time (shown in\nFig. 4b), QHD is signiï¬cantly faster than DQN and reaches higher\nrewards within the same amount of time. We also present the result\nfor Acrobot in Fig. 3(b) and 5. Our QHD provides notably better\nlearning eï¬ƒciency compared to DQN, e.g., about 10Ã— faster in terms\nof runtime and 2Ã— fewer number of episodes. For LunarLander, we\ncompare the RL performance and runtime in Fig. 3(c). It shows that\ncompared to DQN, our QHD achieves the goal nearly 400 episodes\nearlier than DQN and about 2600 seconds (2 times) faster in runtime.\nIn Table 1, we collect the results for the implementation of QHD\nand DQN on embedded CPU. We show that the eï¬ƒciency beneï¬t\nof our algorithm remains signiï¬cant in a power-limited environ-\nment. Thanks to the lightweight QHD learning process and the\nhypervector representation, our algorithm scales better than the\ndeep network structure in DQN, i.e., a constantly smaller runtime\nscale ratio. In addition, the speedup brought by QHD is about 15Ã—\nin Acrobot, 6Ã— in Cartpole, and 3.55Ã— in LunarLander.\n4.3\nEvaluate the eï¬€ect of training batch size\nBoth QHD and DQN rely on experience replay, and since the ex-\nperience replay buï¬€er is ideally inï¬nite, we need to sample the\ntraining dataset from the large replay buï¬€er with a preset batch\nsize. This parameter is rather crucial because it controls how much\npast experience is available for the agent to learn from, thereby\ndeeply inï¬‚uencing the learning quality. A larger batch size pre-\nvents the agent from forgetting past experiences while bringing\ngreater costs. Our QHD, on the other hand, aims to fully utilize the\nprovided training samples at each step. In Fig. 6, we explore the\nEï¬€icient Oï¬€-Policy Reinforcement Learning via Brain-Inspired Computing\nGLSVLSI â€™23, June 5â€“7, 2023, Knoxville, TN, USA\n-600\n-500\n-400\n-300\n-200\n-100\n0\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\n2\n4\n10\n15\n20\n25\n30\n0\n400\n800\n1200\n1600\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\n2\n4\n10\n15\n20\n25\n30\n(a) Acrobot Average Rewards\n(b) Acrobot Total Runtime (s)\nBatch \nSize\nBatch \nSize\nTotal runtime\nGoal achieved\nGoal: -120\nFigure 6: Explore the eï¬€ect of batch size.\neï¬€ect of replay batch size on both methods. Fig. 6a compares the\naverage rewards for the last 100 episodes, and it is clear that our\nQHD performs signiï¬cantly better. For example, when the batch\nsize is 2, QHD can still achieve the goal with an average of -102.9\nrewards. On the other hand, DQN performs poorly with a reward of\n-480.9. This means that DQN does not eï¬ƒciently utilize the limited\navailable training samples.\nApart from better performance, our QHD also provides higher\neï¬ƒciency. In Fig. 6(b), we provide both the QHD runtime for 500\nepisodes and when the goal is achieved. For DQN, only total runtime\nis provided because DQN cannot achieve the goal with small batch\nsizes from 2 to 30. Our QHD is constantly faster: with the batch size\nof 2 (15), our QHD is about 6.5Ã— (1.7Ã—) faster than DQN in terms of\ntotal runtime. Focusing on the actual runtime when achieving the\ntarget, QHD shows an even larger improvement, e.g., the speedup\nis about 12.3Ã— (2.6Ã—) with a batch size of 2 (30).\n4.4\nQHD vs. DQN with Limited Size of\nExperience Replay Buï¬€er\nIn the above sections, we set the RL experience replay to have\ninï¬nite capacity, i.e., the agent has access to all previous experiences\nduring the training. However, in practical implementations, the\nmemory available for experience replay is limited due to energy and\nspace budgets. Thus, in this section, we evaluate the performance\nof our QHD with a tighter cap on the maximum replay buï¬€er size\nand compare it to the DQN results.\nIn Fig. 7(a), we present the average reward achieved by both\nmethods under diï¬€erent buï¬€er sizes. The reward is averaged over\nthe last 100 episodes. When collecting these results, we ï¬x the\ntraining batch size; the batch size is 4 for QHD and 64 for DQN.\nThe ï¬gure shows that DQN performs poorly when the buï¬€er size\nis 64 and 128, with an average reward of -500. However, our QHD\ncan reach that goal even with a buï¬€er size as large as its batch size.\nThese results show that QHD can perform RL tasks with online\nlearning, i.e., a tiny replay buï¬€er.\nWe also take one step further to explore the QHD capability\nof real-time learning. We set both the batch and buï¬€er sizes to 1,\nwhich means the agent will learn based on only the current sample.\nWe use DQN with a 256 buï¬€er size and 64 batch size as an online-\nlearning comparison. As shown in Fig. 7(b), with a larger buï¬€er and\nbatch size, DQN achieves signiï¬cantly lower rewards (-345.4). For a\n500-episode training, our QHD achieves average rewards of -113.7\nusing 83 seconds, which leads to a 34.6Ã— speedup in total runtime.\n5\nCONCLUSION\nWe propose a novel lightweight value-based oï¬€-policy RL algorithm\nbased on brain-inspired HDC. QHD utilizes HDC for high-quality\n-600\n-500\n-400\n-300\n-200\n-100\n0\nQHD\nQHD\nQHD\nQHD\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\nQHD\nDQN\n4\n8\n16 32\n64\n128\n256\n512\n1k\n-400\n-300\n-200\n-100\n0\nQHD\nDQN\n0\n500\n1000\n1500\n2000\n2500\n3000\nQHD\nDQN\n(a) Learn with limited memory size\n(b) QHD Real-time learning\nRewards\nRewards\nMem \nSize\nTotal Runtime (s)\nGoal: -120\nFigure 7: QHD learning eï¬ƒciency with tiny replay buï¬€er.\nQ-value approximation and self-learning agent training. Our evalu-\nation of several tasks shows that QHD provides signiï¬cantly better\neï¬ƒciency and learning quality than DQN.\nACKNOWLEDGEMENTS\nThis work was supported in part by National Science Foundation\n#2127780, Semiconductor Research Corporation (SRC), Department\nof the Navy, Oï¬ƒce of Naval Research, grant #N00014-21-1-2225 and\n#N00014-22-1-2067, Air Force Oï¬ƒce of Scientiï¬c Research, grant\n#FA9550-22-1-0253, Institute of Information & communications\nTechnology Planning & Evaluation (IITP) grant funded by the Korea\ngovernment(MSIT) (No.2022-0-00991, 1T-1C DRAM Array Based\nHigh-Bandwidth, Ultra-High Eï¬ƒciency Processing-in-Memory Ac-\ncelerator), and a generous gift from Cisco.\nREFERENCES\n[1] M. Botvinick et al., â€œReinforcement learning, fast and slow,â€ Trends in cognitive\nsciences, vol. 23, no. 5, pp. 408â€“422, 2019.\n[2] T. Qian et al., â€œDeep reinforcement learning for ev charging navigation by coor-\ndinating smart grid and intelligent transportation system,â€ IEEE transactions on\nsmart grid, vol. 11, no. 2, pp. 1714â€“1723, 2019.\n[3] V. Mnih et al., â€œPlaying atari with deep reinforcement learning,â€ arXiv preprint\narXiv:1312.5602, 2013.\n[4] M. Imani et al., â€œPartially-observed discrete dynamical systems,â€ in ACC.\nIEEE,\n2021, pp. 310â€“315.\n[5] Y. He et al., â€œSoftware-deï¬ned networks with mobile edge computing and caching\nfor smart cities: A big data deep reinforcement learning approach,â€ IEEE Commu-\nnications Magazine, vol. 55, no. 12, pp. 31â€“37, 2017.\n[6] P. Kanerva, â€œHyperdimensional computing: An introduction to computing in\ndistributed representation with high-dimensional random vectors,â€ Cognitive\ncomputation, vol. 1, no. 2, pp. 139â€“159, 2009.\n[7] A. Moin et al., â€œA wearable biosensing system with in-sensor adaptive machine\nlearning for hand gesture recognition,â€ Nature Electronics, 2021.\n[8] A. Rahimi et al., â€œHyperdimensional computing for blind and one-shot classiï¬ca-\ntion of eeg error-related potentials,â€ Mobile Networks and Applications, vol. 25,\nno. 5, pp. 1958â€“1969, 2020.\n[9] G. Brockman et al., â€œOpenai gym,â€ arXiv preprint arXiv:1606.01540, 2016.\n[10] J. Ke et al., â€œOptimizing online matching for ride-sourcing services with multi-\nagent deep reinforcement learning,â€ arXiv preprint arXiv:1902.06228, 2019.\n[11] H.-H. Tseng et al., â€œDeep reinforcement learning for automated radiation adapta-\ntion in lung cancer,â€ Medical physics, vol. 44, no. 12, pp. 6690â€“6705, 2017.\n[12] F.-C. Ghesu et al., â€œMulti-scale deep reinforcement learning for real-time 3d-\nlandmark detection in ct scans,â€ IEEE TPAMI, vol. 41, no. 1, pp. 176â€“189, 2017.\n[13] P. Poduval et al., â€œGraphd: Graph-based hyperdimensional memorization for\nbrain-like cognitive learning,â€ Frontiers in Neuroscience, p. 5, 2022.\n[14] A. Hernandez-Cane et al., â€œOnlinehd: Robust, eï¬ƒcient, and single-pass online\nlearning using hyperdimensional system,â€ in DATE.\nIEEE, 2021, pp. 56â€“61.\n[15] Z. Zou et al., â€œEventhd: Robust and eï¬ƒcient hyperdimensional learning with\nneuromorphic sensor,â€ Frontiers in Neuroscience, vol. 16, 2022.\n[16] Y. Kim et al., â€œEï¬ƒcient human activity recognition using hyperdimensional\ncomputing,â€ in IOT, 2018, pp. 1â€“6.\n[17] Y. Ni et al., â€œHdpg: hyperdimensional policy-based reinforcement learning for\ncontinuous control,â€ in Proceedings of the 59th ACM/IEEE Design Automation\nConference, 2022, pp. 1141â€“1146.\n[18] R. Bellman, â€œOn the theory of dynamic programming,â€ Proceedings of the National\nAcademy of Sciences of the United States of America, vol. 38, no. 8, p. 716, 1952.\n[19] H. Hasselt, â€œDouble q-learning,â€ Advances in neural information processing systems,\nvol. 23, pp. 2613â€“2621, 2010.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NE"
  ],
  "published": "2022-05-14",
  "updated": "2023-06-21"
}