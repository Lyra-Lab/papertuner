{
  "id": "http://arxiv.org/abs/2205.07015v3",
  "title": "Cliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments",
  "authors": [
    "Ryan Sullivan",
    "J. K. Terry",
    "Benjamin Black",
    "John P. Dickerson"
  ],
  "abstract": "Visualizing optimization landscapes has led to many fundamental insights in\nnumeric optimization, and novel improvements to optimization techniques.\nHowever, visualizations of the objective that reinforcement learning optimizes\n(the \"reward surface\") have only ever been generated for a small number of\nnarrow contexts. This work presents reward surfaces and related visualizations\nof 27 of the most widely used reinforcement learning environments in Gym for\nthe first time. We also explore reward surfaces in the policy gradient\ndirection and show for the first time that many popular reinforcement learning\nenvironments have frequent \"cliffs\" (sudden large drops in expected return). We\ndemonstrate that A2C often \"dives off\" these cliffs into low reward regions of\nthe parameter space while PPO avoids them, confirming a popular intuition for\nPPO's improved performance over previous methods. We additionally introduce a\nhighly extensible library that allows researchers to easily generate these\nvisualizations in the future. Our findings provide new intuition to explain the\nsuccesses and failures of modern RL methods, and our visualizations concretely\ncharacterize several failure modes of reinforcement learning agents in novel\nways.",
  "text": "Cliff Diving: Exploring Reward Surfaces in Reinforcement Learning\nEnvironments\nRyan Sullivan * 1 2 J K Terry * 1 2 Benjamin Black * 1 2 John P. Dickerson 2\nAbstract\nVisualizing optimization landscapes has led to\nmany fundamental insights in numeric optimiza-\ntion, and novel improvements to optimization\ntechniques. However, visualizations of the ob-\njective that reinforcement learning optimizes (the\n“reward surface”) have only ever been generated\nfor a small number of narrow contexts. This work\npresents reward surfaces and related visualiza-\ntions of 27 of the most widely used reinforce-\nment learning environments in Gym for the ﬁrst\ntime. We also explore reward surfaces in the pol-\nicy gradient direction and show for the ﬁrst time\nthat many popular reinforcement learning environ-\nments have frequent “cliffs” (sudden large drops\nin expected return). We demonstrate that A2C\noften “dives off” these cliffs into low reward re-\ngions of the parameter space while PPO avoids\nthem, conﬁrming a popular intuition for PPO’s im-\nproved performance over previous methods. We\nadditionally introduce a highly extensible library\nthat allows researchers to easily generate these\nvisualizations in the future. Our ﬁndings provide\nnew intuition to explain the successes and failures\nof modern RL methods, and our visualizations\nconcretely characterize several failure modes of\nreinforcement learning agents in novel ways.\n1. Introduction\nReinforcement learning typically attempts to optimize the\nexpected discounted return of an agent’s policy. Policy gra-\ndient methods represent the policy with a neural network,\nand learn this policy by approximating the gradient of the\nreinforcement learning objective over policy network param-\neters. This means that the beneﬁts and challenges of deep\n*Equal contribution\n1Swarm Labs 2Department of Computer\nScience, University of Maryland, College Park. Correspondence\nto: Ryan Sullivan <rsulli@umd.edu>.\nProceedings of the 39 th International Conference on Machine\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-\nright 2022 by the author(s).\nFigure 1. Reward surface for the Solaris Atari environment.\nlearning apply to policy gradient methods. However, unlike\nmost deep learning tasks, reinforcement learning is notori-\nously unstable, and agents often experience sharp drops in\nreward during training. Studying the reward surface and how\nRL algorithms interact with it is critical to understanding\nthe successes and failures of deep reinforcement learning.\nA “reward surface” is the high dimensional surface of the\nreinforcement learning objective (the cumulative expected\nreward when following a given policy in an environment)\nover the policy network parameter space. Reward surfaces\nhave been used to study speciﬁc questions in limited con-\ntexts (Ilyas et al., 2020) but have never been generated for a\nwide variety of environments. Loss landscapes have been\nused in computer vision to understand the effect that resid-\nual connections have on computer vision tasks (Li et al.,\n2018), and we expect that visualizations of reward surfaces\ncould be similarly valuable for understanding techniques\nin reinforcement learning. As a ﬁrst step toward this goal,\nwe produce visualizations of the reward surfaces for 27 of\nthe most popular reinforcement learning environments, and\nidentify new patterns in these surfaces for the ﬁrst time.\nWe see common traits of environments that are generally\nperceived to be \"easy\" for reinforcement learning to solve,\nand visual explanations of failure modes in sparse reward\nenvironments.\narXiv:2205.07015v3  [cs.LG]  21 Sep 2022\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nWe additionally conduct a series of novel visualizations\nof the reward surface, ﬁnding evidence of steep “cliffs” in\nreward surfaces plotted in the policy gradient direction of\nnumerous environments. These are directions where the\nreturns are constant or increase for a short distance, then\ndrop sharply. Reinforcement learning notoriously suffers\nfrom instability, and these cliffs may explain the sudden\nperformance drops that agents experience during training.\nOur plots offer conclusive visual evidence that these cliff\nexist, as well as methods to study them further. We show\nthat the speciﬁc cliffs present in our visualizations impact\nlearning performance in some situations, and by comparing\nthe performance of PPO (Schulman et al., 2017) and A2C\n(Mnih et al., 2016) on these cliffs, we provide an explanation\nfor PPO’s improved efﬁcacy over previous methods.\nFinally, to accelerate future research on reward surfaces and\nthe impact of cliffs in reinforcement learning, we release\nthe code for this paper as a comprehensive, modular, and\neasy to use library for researchers to plot reward surfaces\nand other visualizations used in this paper.\n2. Background and Related Work\n2.1. Loss Landscapes\nLi et al. (2018) developed a ﬁlter-normalization, a technique\nfor plotting 3d visualizations of loss landscapes, the surface\ngenerated by a loss function over neural network param-\neters. They were able to demonstrate that the sharpness\nof loss landscapes plotted using ﬁlter-normalized random\ndirections correlated with neural network generalization er-\nror. They create their plots by choosing perturbations d\nof trained parameters that give an informative view of the\nneural network’s local behavior. However, uniform ran-\ndom perturbations are known to be misleading in neural\nnetwork analysis, because neural networks with ReLU ac-\ntivations have scale invariant weights (Li et al., 2018). To\nmitigate this problem, they propose ﬁlter-normalized ran-\ndom directions. They represent the neural network as a\nvector θ indexed by layer i and ﬁlter (not ﬁlter weight) j.1\nThen, they sample a random Gaussian direction d, and scale\neach ﬁlter of this random direction to match the magnitude\nof the neural network parameters in the corresponding ﬁlter,\nby applying the following formula.\ndi,j =\ndi,j\n∥di,j∥∥θi,j∥\nLi et al. (2018) used this method to demonstrate that skip\nconnections can limit the increase in non-convexity as net-\nwork depth increases. Their work was originally applied to\nimage classiﬁcation networks, but we adapt these techniques\n1Note that this method also works for fully connected layers,\nwhich are equivalent to a convolutional layer with a 1x1 output\nfeature map.\nto visualize reward surfaces for policy networks.\n2.2. Reinforcement Learning\nDeep reinforcement learning aims to optimize a policy π to\nmaximize the expected return over neural network parame-\nters θ. This objective can be written as J(πθ) = Eτ∼πθR(τ)\nwhere R(τ) = Pn\nt=0 γtrt. Here τ is a trajectory, rt is the\nreward at time step t, and γ is the discount factor and we\nsum the rewards across the entire episode of n time steps.\nNota & Thomas (2020) showed that the discounted pol-\nicy gradient is not the gradient over the surface of average\ndiscounted rewards, and is in fact not the gradient of any\nsurface. To avoid this issue and make interpretation easier,\nwe plot the undiscounted reward surface where γ = 1.\n2.3. Policy Gradient Methods\nPolicy\ngradient\nmethods\nestimate\nthe\ngradient\nof\nthe policy network and use gradient ascent to in-\ncrease the probability of selecting actions that lead to\nhigh rewards.\nThe gradient of the objective J is\n∇θJ(πθ) = Eτ∼πθ\nh PT\nt=0 ∇θ log πθ(at|st)Aπθ(st|at)\ni\nwhere Aπθ(st|at) is the advantage function for the current\npolicy πθ.\n2.4. Reward surfaces\nThe “reward surface” is the reinforcement learning objec-\ntive function J(πθ). Reward surfaces were ﬁrst visualized\nby Ilyas et al. (2020) to characterize problems with policy\ngradient estimates. The authors plotted a policy gradient es-\ntimate vs a uniform random direction, showing via visually\nstriking examples that low sample estimates of the policy\ngradient rarely guide the policy in a better direction than a\nrandom direction. Note that this work did not make use of\nﬁlter-normalized directions.\nBekci & Gümü¸s (2020) then used loss landscapes from Li\net al. (2018) to study Soft Actor-Critic agents (Haarnoja\net al., 2018) trained on an inventory optimization task.\nThey visualize the impact of policy stochasticity and ac-\ntion smoothing on the curvature of the loss landscapes for 4\nMuJoCo environments.\nLater, Ota et al. (2021) used loss landscapes from Li et al.\n(2018) directly to compare shallow neural networks for\nreinforcement learning to deeper neural networks, showing\nthat deep neural networks perform poorly because their\nloss landscape has more complex curvature. They used\nthis visual insight to develop methods that can train deeper\nnetworks for reinforcement learning tasks. This work plots\nthe loss function of SAC agents which includes an entropy-\nregularization term (Haarnoja et al., 2018), while we directly\nplot the reinforcement learning objective.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nThis paper is different from each of these previous works in\nthe breadth of environments explored, and in that we are the\nﬁrst to use ﬁlter-normalized directions from Li et al. (2018)\nto visualize reward surfaces rather than loss landscapes.\nWe additionally introduce novel experiments and results\ninspired by the ﬁndings in our initial reward surface plots.\n2.5. Proximal Policy Optimization\nProximal Policy Optimization (PPO) (Schulman et al., 2017)\nis a derivative method of Trust Region Policy Optimization\n(TRPO) (Schulman et al., 2015) intended to be easier to im-\nplement and more hyperparameter invariant. Over the past\n4 years, PPO has become a “default” method for many deep\nRL practitioners and has been used in numerous high proﬁle\nworks (Berner et al., 2019; Mirhoseini et al., 2021). TRPO\nand PPO claim to offer enhanced empirical performance\nover previous methods by preventing dramatic changes in\nthe policy via trust regions and ratio clipping respectively.\nRatio clipping is conceptually a useful heuristic, however\nwe are not aware of any work demonstrating why it results\nin empirically good performance across so many domains.\nInstability during training is common in deep reinforcement\nlearning, and agents often experience large drops in reward\nthat can take long to recover from. One intuition for the\nutility of PPO’s gradient and ratio clipping that has been\nidly discussed in the community is that they prevent agents\nfrom taking gradient steps that result in this collapsing per-\nformance (Hui, 2018). More precisely, the intuition is that\nby preventing large changes to the policy, it avoids gradient\nsteps that move the policy into regions of the parameter\nspace with poor rewards and uninformative gradients. To\nthe best of our knowledge, this intuition was ﬁrst described\nin a widely circulated medium article Hui (2018) released\nroughly 1 year after the original PPO paper. We are aware\nof no prior work in the academic literature which directly\nsupport this intuition, and the TRPO and PPO papers do not\ndirectly allude to it. We discover evidence of these cliffs in\nplots of the policy gradient direction, and perform experi-\nments to conﬁrm that they negatively impact learning. As a\nresult of its empirically good performance PPO has become\nthe de-facto policy gradient method, so we chose it as the\nagent in our reward surface experiments.\n3. Environment Selection\nIn exploring these reward surfaces, we sought to cover many\nwidely used benchmark environments. We generated plots\nfor all “Classic Control“ and “MuJoCo” environments in\nGym (Brockman et al., 2016) and for many popular Atari en-\nvironments from the Arcade Learning Environment (Belle-\nmare et al., 2013). Video games with graphical observations\nand discrete actions are a common benchmark for the ﬁeld,\nand Atari games are the most popular games for this purpose.\nThe MuJoCo environments are high quality physics simu-\nlations used for prominent robotics work with continuous\nactions and vector observations. The Classic Control games\nrepresent some of the early toy environments in reinforce-\nment learning, and continue to be used as a standard set of\n\"easy\" environments. We generate surfaces for all classic\ncontrol and MuJoCo environments, but we only generate\nsurfaces for a representative selection of 12 Atari environ-\nments instead of all 59 Atari environments in Gym for the\nsake of brevity. To make sure we explore diverse reward\nschemes, we speciﬁcally picked six sparse reward environ-\nments (Montezuma’s Revenge, Pitfall!, Solaris, Private Eye,\nFreeway, Venture), three popular dense reward environments\n(Bank Heist, Q*Bert, Ms. Pac-Man), and three popular easy\nexploration environments (Breakout, Pong and Space In-\nvaders), according to the standard taxonomy by Bellemare\net al. (2016).\n4. Initial Explorations of Reward Surfaces\n4.1. Methodology\nIn this work, we adapt the techniques from Li et al. (2018) to\nvisualize the reward surfaces of PPO agents in reinforcement\nlearning environments. As in that paper, we deal with the\nhigh-dimensional domain of the reward surface by focusing\nour analysis around points in the policy space visited during\ntraining. Given a training checkpoint with parameters θ, we\nare interested in understanding the local surface of rewards\ngenerated by the policy represented by parameters θ + d for\nsmall perturbations d.\nTo visualize this local region in 3 dimensions we plot the\nempirical expected episodic return on the z axis against\nindependently sampled ﬁlter-normalized random directions\non the x and y axes.2 The plots are additionally scaled\nmanually to highlight features of interest, so note the marks\non the axes which indicate those manual scales. The scale,\nresolution, and number of environment steps used in each\nenvironment are listed in Table 2.\nA reward surface is dependent on the chosen learning al-\ngorithm, network architecture, hyperparameters, and ran-\ndom seed, so for these experiments we chose to plot the\nreward surface of PPO agents using the tuned hyperparam-\neters found in RL Zoo 3 (Rafﬁn, 2020). However, reward\nsurfaces for a given environment are extremely visually\nsimilar across these variables, which we discuss in subsub-\nsection 4.2.2. To understand what challenges RL algorithms\nface towards the end of training after sufﬁcient exploration\nhas occurred, we chose the best checkpoint during train-\ning, evaluated on 25 episodes, with a preference for later\ncheckpoints when evaluations showed equal rewards. The\n2Note that because this is a high-dimensional space, these\ndirections are orthogonal with high probability.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nFigure 2. Reward surfaces for CartPole-v1, Breakout, Hopper-v2, Montezuma’s Revenge\nbest checkpoint was typically found during the last 10% of\ntraining.\n4.2. Results\nA sample of the visualizations of the reward surfaces can be\nseen in Figure 2. The full set of reward surface plots can be\nfound in Appendix A. We additionally do a small number\nof experiments on network architecture, which we include\nin the appendix as a curiosity Appendix G. In the following\nsections we present our key ﬁndings and discuss the validity\nof our reward surface visualizations.\n4.2.1. FINDINGS IN PLOTS\nOne obvious observation is that the size and shape of the\nmaximizers present in the reward surfaces roughly correlates\nwith the perceived difﬁculty of the environment. Though\nthere is some recent work attempting to quantify the dif-\nﬁculty of RL environments, it is still a challenging and\nunsolved problem (Oller et al., 2020; Furuta et al., 2021).\nHowever, researchers have used these benchmarks for years,\nand we now have some intuition for identifying environ-\nments that are easier for modern methods to solve. The\nClassic Control environments were designed as simple toy\nenvironments, and modern methods easily ﬁnd strong poli-\ncies for them. The Atari environments have been taxono-\nmized by Bellemare et al. (2016) to indicate their relative\ndifﬁculty. Breakout, Pong, and Space Invaders are listed as\n\"Easy Exploration\" games. Among the \"Hard Exploration\"\ngames Bank Heist, Ms. Pacman, and Q*bert have dense\nrewards, while Freeway, Montezuma’s Revenge, Pitall!, Pri-\nvate Eye, Solaris, and Venture have a more challenging\nsparse reward structure. The MuJoCo environments have\nnot been ofﬁcially categorized according to difﬁculty to our\nknowledge.\nUsing these rough categories as a guide, we see that the\nClassic Control environments shown in Figure 5, certainly\nthe easiest set that we study, have large maximizers that\noften span the entire domain of the plot. In the Atari en-\nvironments shown in Figure 7, we see that the \"Human\nOptimal\" (Easy Exploration) and \"Dense Reward\" (Hard\nExploration) environments have large smooth maximizers\nrelative the the chaotic landscapes seen in the \"Sparse Re-\nward\" environments. This ﬁnding complements those of Li\net al. (2018) who found that the sharpness of loss landscapes\nusing ﬁlter-normalized directions correlated with general-\nization error. This observation also raises the possibility of\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nFigure 3. Gradient heat maps for Ant, Hopper, and InvertedDoublePendulum. The heat map for Hopper and InvertedDoublePendulum\nshows a cliff-like gradient direction which falls off sharply compared to the ﬁlter-normalized direction.\nfuture work creating a metric based on reward surfaces to\ngauge the difﬁculty of RL environments.\nWe also observe some interesting characteristics of individ-\nual environments. In Figure 5, MountainCarContinuous\nhas a strangely spiky reward surface for a Classic Control\nenvironment. This may be a result of the uniquely low\ntraining time required to solve it. Using the hyperparam-\neters from RL Zoo 3 we train this agent for only 20,000\nsteps, while the next lowest training time in RL Zoo 3 is\n200,000 timesteps. Looking at Figure 6, Humanoid Standup\nhas fairly spiky reward surface, suggesting that it’s reward\nfunction is sensitive to small perturbations in the policy.\nWhile many maximizers in these plots appear as a single\nhill or peak in an otherwise ﬂat region, some of the surfaces\nhave uniquely identiﬁable curvature. Hopper has a large\nsemi-circular curve in its reward surface. Cartpole, Moun-\ntainCar, and InvertedPendulum have large plateaus at their\npeak reward.\nThe sparse reward Atari environments in Figure 7 are par-\nticularly interesting to examine. Note that each point in the\nsparse Atari reward surfaces were evaluated with either 1\nor 2 million environment steps, to limit the standard error\nfor each estimate. We see that Freeway’s reward surface\nhas large ﬂat regions of varying heights surrounding a sin-\ngle smooth maximizer. Montezuma’s Revenge and Venture\nhave short noisy maximizers. The agents for the sparse\nenvironments (except Freeway) perform poorly, so note that\neven the maximizers in these plots represent weak poli-\ncies. As a result of this, we see that the reward surfaces for\nMontezuma’s Revenge, Solaris, and Venture show nearby\nmaximizers that the agent was unable to ﬁnd during training.\nPrivate Eye has a large region of zero reward and a far away\nmaximum with much higher rewards. Finally, as its name\nsuggests, the reward surface for Pitfall! is mostly ﬂat and\nmarred by several deep pits.\nThe idea that sparse rewards structures lead to ﬂat regions\nin a reward surface is intuitive – in environments where\nrewards are issued solely in infrequent goal states, large yet\nprecise policy improvements may be required to experience\nvariations in rewards. Despite the intuitive nature of this ob-\nservation, we’re unaware of this being visually documented\nin the literature. We also ﬁnd that in regions where the\nreward surfaces are not ﬂat, they are often extremely noisy.\nThis is supported by plots of the surface of standard devia-\ntions at each point, which we include in Appendix B. We\nsee in these plots that the standard deviation is often signiﬁ-\ncantly larger than the average reward in the reward surfaces\nfor Montezuma’s Revenge, Private Eye, and Venture, unlike\nany of the other environments we study. These plots seem\nto highlight different failure modes of sparse environments\n– either the surface is ﬂat when the agent experiences no\nvariation in rewards, or the surface is spiky when rewards\nare sparse and noisy.\n4.2.2. PLOT REPRODUCIBILITY AND STANDARD ERROR\nTo demonstrate the consistency of these experiments across\nmultiple random seeds, we repeated our reward surface plots\n18 times for Acrobot, HalfCheetah, Breakout, and Mon-\ntezuma’s Revenge. For each trial, we trained and evaluated\na new agent with a new random seed. We can see from the\nplots in Appendix C that the reward surfaces are extremely\nvisually similar for a particular environment, indicating that\ntraining appears to converge to visually similar regions of\nthe reward landscape, and that the characteristics of these\nplots are consistent across multiple seeds.\nWe evaluated for at least 200,000 time steps (1 or 2 million\nsteps for the sparse reward Atari games) at each point to\nensure that the standard error for these estimates is small.\nWe record the standard error of each plot in Appendix D.\n5. Discovering Cliffs in the Gradient Direction\nFilter normalized random directions provide a broad sense\nof the local reward landscape, and are useful for analysis\nnear the end of training, but they do not represent the di-\nrection of training because the sampled random directions\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nFigure 4. Gradient line search plots for Pendulum, Ant, and Pong. The line plot for Ant shows several checkpoints that exhibit cliff-like\nproperties.\nare likely orthogonal to the gradient direction. To better\nunderstand the optimization characteristics of these environ-\nments, we performed similar experiments using the policy\ngradient direction. We use a high quality estimate of the\npolicy gradient computed over 1,000,000 environment steps\nas in Ilyas et al. (2020). In many of these plots, we ﬁnd\nevidence of “cliffs” in the reward surface. These are regions\nof the reward surface in which rewards sharply decrease\nafter a short distance. We discuss the difﬁculty of precisely\ndeﬁning a sharp decrease in Appendix I and explain the\nheuristic criteria used to identify cliffs in Appendix H.\nOne difﬁculty of plotting the gradient direction is that the\ngradient magnitudes vary drastically for different environ-\nments and at different points during training (McCandlish\net al., 2018). Additionally, any maximum in a reward sur-\nface can be made to look like a sharp cliff by using a large\nenough gradient scale, or like a large plateau by using a\nsmaller gradient scale. To provide a fair comparison of the\ngradient direction’s sharpness, we normalize the gradient\ndirection by dividing each component by it’s L2 norm.\n5.1. Gradient Directions vs. Filter-Normalized\nRandom Directions\nWe show the differences between ﬁlter normalized random\ndirections and the gradient direction by creating plots of the\nreward surface using the gradient direction on the x axis\nand a random ﬁlter normalized direction on the y axis. Due\nto the frequent sharp changes in reward that we observe in\nthese plots, 3d surface plots can become partially obscured\nby peaks, so we choose to plot these surfaces as heat maps\ninstead. A sample of the heat maps can be seen in Figure 3\nand the full set of heat maps for Classic Control and MuJoCo\nenvironments can be found in Appendix E.\n5.1.1. PRELIMINARY OBSERVATIONS\nThe most striking observation is that rewards in the gra-\ndient direction often change much more rapidly than the\nrewards in random directions. We see a sample of these\ngradient heat maps in Figure 3. In the gradient heat maps\nfor Hopper and InvertedDoublePendulum, rewards in the\ngradient direction seem to form cliffs and drop off rapidly\nafter a short distance. The plot for InvertedDoublePendulum\nis particularly interesting. It is possible to argue that the\ndifferent normalization schemes that we use for random and\ngradient directions make these plots falsely appear to have\ncliffs. However, the rewards for InvertedDoublePendulum\ndrop much more quickly in the gradient direction than in\nthe negative gradient direction. Due to these potential con-\ncerns about normalization, in the next section we directly\nvisualize the gradient directions across multiple training\ncheckpoints for each environment to ﬁnd more convincing\nevidence of cliffs.\n5.2. Visualizing Rewards in the Gradient Direction\nDuring Training\n5.2.1. METHODOLOGY\nIn order to study the gradient direction’s reward surface over\nthe course of training, we plot a 1-dimensional projection\nof the rewards along the gradient direction for a series of\ncheckpoints taken at uniform training step increments. A\nsample of these can be seen in Figure 4 and the full set\nof plots can be found in Appendix F Since the training\ncheckpoints are relatively far apart from one another, the plot\nis somewhat discontinuous. However, we selected uniformly\ndistributed checkpoints across the entire span of training,\nso they should be representative of all points visited during\ntraining. We sample 20 points along a distance of 0.4 in\nthe normalized gradient direction, and another 10 points\nbetween the ﬁrst and second sample point. This results in a\nsingle high resolution segment at the start of each plot that\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nallows us to more easily identify cliffs early in the plots. The\nAtari environments Montezuma’s Revenge, Pitfall!, Private\nEye, and Solaris were plotted with 1,000,000 environment\nsteps per sample point to limit evaluation noise, while the\nremaining environments used 200,000 time steps.\n5.2.2. OBSERVATIONS\nWe ﬁnd that many of the same observations here as we do\nin the original reward surfaces. The gradient directions for\neasy, dense reward environments tend to point toward better\nrewards, and sparse reward environments often have ﬂat\ntrajectories in the gradient direction. Some of the sparse\nenvironments also have extremely noisy gradient directions,\nwith wide swings in reward. This typically occurs in envi-\nronments where the agent performs poorly. For example,\nwe see in Freeway, a sparse reward environment where our\nagent ﬁnds a nearly optimal policy, that the gradient line\nplot looks very similar to that of dense reward Atari envi-\nronments like Pong. However, we also note some unique\nproperties of the gradient direction. In some plots, for exam-\nple in Pong, we see “cliffs” in the reward surface where the\nreward brieﬂy increases, then sharply decreases. We ﬁnd\nthat these cliffs occur occasionally in almost every environ-\nment.\n6. Cliffs Impact Policy Gradient Training\nWe are clearly able to see cliffs in our line plots, but we\nneeded to conﬁrm that these cliff-like gradient directions\nare not simply a visualization artifact and that they affect the\nperformance of agents. We hypothesize that methods which\napproximate the policy gradient will occasionally step too\nfar and fall off of these cliffs, thereby performing worse\non cliff-like checkpoints than normal checkpoints. This\nhypothesis follows from the intuition that motivated PPO\nand TRPO. To test our hypothesis, we use the line plots to\nidentify trained checkpoints where the true policy gradient\npoints towards a cliff, and compare the performance of A2C\non these cliffs versus less cliff-like checkpoints. We ﬁnd that\nA2C performs signiﬁcantly worse on cliff checkpoints than\nnon-cliff checkpoints. We further hypothesize that PPO,\nwhich uses ratio clipping to avoid signiﬁcant changes to\nits policy and better hyperparameters, will perform better\nthan A2C on cliff checkpoints. We run the same experiment\nusing PPO, and compare the results in Table 1.\n6.1. Methodology\nTo evaluate performance on cliffs relative to baseline perfor-\nmance on standard checkpoints, we ﬁrst select 12 cliff and\n12 non-cliff checkpoints from our gradient line search data\nusing a heuristic explained in Appendix H.\nFor both A2C and PPO we evaluate the percent change in\nreward that results from training for 2048 environment steps\nat a particular checkpoint. For each checkpoint, we perform\n10 trials in which we evaluate the starting performance of\nthe agent over 1000 episodes, the agent takes a few gradient\nsteps, and then we evaluate the resulting policy over 1000\nepisodes as well. We calculate the percent change in reward,\nand average this value over every checkpoint in each set.\nThis produces an average change in reward for the set of\ncliff checkpoints, and the set of non-cliff checkpoints, which\nwe list in Table 1.\nWe also want to ensure that the method takes a step in the\ndirection that we are studying, and steps far enough to reach\nthe cliff. As such, we try increasing both the learning rate\n(LR) and the number of steps per parallel environment per\ntraining update (N steps). The remaining hyperparameters\nare the optimal hyperparameters from RL Zoo 3 (Rafﬁn,\n2020) as in previous experiments. We use these hyperpa-\nrameters to validate the existence of cliffs and demonstrate\nthat cliffs can have a negative effect on training, whether or\nnot it commonly occurs in practice.\n6.2. Results\nWe see the results of our experiments with several hyperpa-\nrameter sets in Table 1. We ﬁnd that on cliff checkpoints,\nA2C’s gradient step consistently results in a decrease in re-\nwards, while on non-cliff checkpoints it increases expected\nreturn. On the other hand, PPO sees nearly the same percent\nchange in performance across cliff and non-cliff checkpoints.\nFrom these results, we conﬁrm that the cliffs present in our\ngradient line searches can have a real effect on optimization,\nand are not simply a visualization artifact. We also show\nthat for all tested hyperparameters, PPO is affected by cliffs\nless than A2C. Only one of our experiments shows a minor\ndecrease in rewards for PPO, while all of them show a larger\ndecrease for A2C. Engstrom et al. (2020) found that PPO’s\nperformance could largely be attributed to hyperparameter\nimprovements and implementation tricks, so we leave a thor-\nough investigation of the exact components that cause PPO\nto be less affected by cliffs as future work.\n7. Library\nWe developed an extensive software library for plotting the\nreward surfaces of reinforcement learning agents to produce\nthis work and encourage future research using these visual-\nizations. The library includes code for training agents using\nall of the options available in Stable Baselines 3 (Rafﬁn\net al., 2019) and hyperparameters from RL Zoo 3 (Rafﬁn,\n2020). We provide algorithms for estimating the gradient\nand hessian of policy networks along with code for evalu-\nating the rewards or discounted returns of trained agents.\nThe entire code base supports the use of arbitrary directions\nfor investigation, and speciﬁcally provides tools for using\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nCliff\nNon-Cliff\nN steps = 128\nLR = 0.000001\nPPO\n0.03%\n0.03%\nA2C\n-0.3%\n0.2%\nLR = 0.0001\nPPO\n-0.01%\n-0.03%\nA2C\n-4%\n2.3%\nLR = 0.01\nPPO\n0.0%\n0.4%\nA2C\n-0.3%\n2.0%\nLR = 0.5\nPPO\n0.0%\n0.0%\nA2C\n-8.5%\n1.6%\nN steps = 2048\nLR = 0.000001\nPPO\n-0.1%\n-0.4%\nA2C\n-0.5%\n0.2%\nLR = 0.0001\nPPO\n-0.2%\n-0.1%\nA2C\n-7.0%\n4.4%\nLR = 0.01\nPPO\n0.1%\n0.1%\nA2C\n-3.9%\n2.9%\nLR = 0.5\nPPO\n0.0%\n0.3%\nA2C\n-3.6%\n-0.3%\nTable 1. Table of A2C and PPO’s average percent change in reward after taking a few gradient steps on cliff and non-cliff checkpoints for\nvarious sets of hyperparameters. These results are averaged among 10 trials each evaluated for 1000 episodes.\nﬁlter-normalized and policy gradient directions. We include\nroutines for creating 3d plots, line plots, heat maps, and gifs\nof reward surfaces. Finally, all experiments in the library are\nparallelized across multiple environments, and scripts are\nincluded for generating reward surfaces on SLURM clusters.\nThe library is well organized and documented, and it can be\nfound at https://github.com/RyanNavillus/reward-surfaces.\n8. Discussion\nIn this work we introduce valuable new methods for study-\ning deep reinforcement learning, and use them to discover\nnew results about the optimization characteristics of pop-\nular RL environments. Reward surfaces provide a useful\noverview of the reward structure of an environment. Loss\nlandscapes have already been used in debugging tools for\ncomputer vision tasks (Bain et al., 2021), and we hope that\nreward surfaces could be similarly useful in debugging RL\nsystems. In particular, the reward surfaces for sparse en-\nvironments allow us to see large regions of ﬂat rewards,\nand extreme evaluation noise at individual points. Gradi-\nent methods cannot optimize ﬂat surfaces, so solutions to\nthis problem are constrained to either modifying the reward\nstructure (e.g. curiosity or bonus-based exploration meth-\nods (Pathak et al., 2017; Burda et al., 2019)) or condensing\nthe action space such that simple exploration methods are\ntractable (e.g. DIAYN and related work (Eysenbach et al.,\n2019; Sharma et al., 2020)).\nAll this suggests many interesting opportunities for future\nwork, either visualizing the effects of bonus-based explo-\nration methods on the reward surface, or quantiﬁcation of\nreward sparsity. Additionally, our gradient line searches\nvisualize optimization characteristics of the environment,\nand could allow us to select gradient methods more suited to\nreinforcement learning. We see a few worthwhile research\ndirections using these techniques.\nOur gradient line searches show evidence of cliffs in most\npopular RL environments. Interestingly, although these\ncliffs appear in almost every environment occasionally, the\nmost extreme examples occur in relatively easy environ-\nments like CartPole and Inverted Double Pendulum, while\nthe harder Atari environments are characterized by mostly\nﬂat and noisy gradient directions. Despite the apparent dif-\nference, these noisy spikes can also be considered small\ncliffs on the scale of an individual gradient step. Our ex-\nperiments demonstrate that the extreme cliffs can have an\nimpact on training over a few gradient steps, but we suspect\nthat smaller cliffs have a slower, degrading effect over the\ncourse of training. Future work could attempt to directly\nstudy the degree to which cliffs affect training in practice.\nOur experiments comparing PPO and A2C provide an em-\npirical justiﬁcation for why PPO is so effective. We ﬁnd\nthat PPO performs signiﬁcantly better than A2C on check-\npoints with steep cliffs in the gradient direction’s reward\nsurface. A deeper understanding of why existing methods\nwork will allow us to develop stronger algorithms in the\nfuture. Previous work has focused on the hyperparameters\nand algorithmic advances that contribute to PPO’s strong\nperformance (Engstrom et al., 2020), but we believe that the\nspeciﬁc situations where PPO outperforms previous meth-\nods warrants further investigation.\nFinally, this work focuses on policy networks, but the tools\nthat we introduce could be applied to study Q networks or\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nvalue networks. It would also be interesting to see reward\nsurfaces for multiagent algorithms, such as independent\nlearning or parameter sharing (Gupta et al., 2017).\n9. Conclusion\nThis work is the ﬁrst to use ﬁlter-normalized directions to\nvisualize reward surfaces for a large collection of popular\nreinforcement learning environments. We are also the ﬁrst\nto ﬁnd visual evidence of the cliffs that inspired TRPO and\nPPO, and we perform experiments demonstrating their neg-\native impact on policy gradient methods. This offers new\npotential insights into why deep RL works, and why rein-\nforcement learning is seemingly so challenging when com-\npared to other areas of deep learning. To accelerate future\nworks in this ﬁeld we created an extensive, well-documented\nlibrary for plotting reward surfaces. We thoroughly outline\nlimitations this work has in Appendix I. We hope that this\nwork inspires future research on the speciﬁc optimization\nchallenges that reinforcement learning faces, and that it\nenables new studies using reward surfaces.\n10. Acknowledgements\nWe would like to thank Joseph Suarez, Costa Huang, and\nour reviewers for their helpful comments and writing sug-\ngestions.\nReferences\nBain, R., Tokarev, M., Kothari, H., and Damineni, R. Loss-\nplot: A better way to visualize loss landscapes. arXiv\npreprint arXiv:2111.15133, 2021.\nBekci, R. Y. and Gümü¸s, M. Visualizing the loss landscape\nof actor critic methods with applications in inventory\noptimization. arXiv preprint arXiv:2009.02391, 2020.\nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T.,\nSaxton, D., and Munos, R. Unifying count-based ex-\nploration and intrinsic motivation. Advances in neural\ninformation processing systems, 29:1471–1479, 2016.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.\nThe arcade learning environment: An evaluation plat-\nform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013.\nBerner, C., Brockman, G., Chan, B., Cheung, V., D˛ebiak, P.,\nDennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,\nC., et al. Dota 2 with large scale deep reinforcement\nlearning. arXiv preprint arXiv:1912.06680, 2019.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. Openai gym.\narXiv preprint arXiv:1606.01540, 2016.\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. Explo-\nration by random network distillation. In International\nConference on Learning Representations, 2019.\nEngstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos,\nF., Rudolph, L., and Madry, A. Implementation matters\nin deep rl: A case study on ppo and trpo. In International\nConference on Learning Representations, 2020.\nEysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity\nis all you need: Learning skills without a reward function.\nIn 7th International Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net, 2019.\nFuruta, H., Matsushima, T., Kozuno, T., Matsuo, Y., Levine,\nS., Nachum, O., and Gu, S. S. Policy information capac-\nity: Information-theoretic measure for task complexity in\ndeep reinforcement learning. In International Conference\non Machine Learning, 2021.\nGupta, J. K., Egorov, M., and Kochenderfer, M. Cooperative\nmulti-agent control using deep reinforcement learning. In\nSukthankar, G. and Rodriguez-Aguilar, J. A. (eds.), Inter-\nnational Conference on Autonomous Agents and MultiA-\ngent Systems (AAMAS), pp. 66–83, 2017.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft\nactor-critic: Off-policy maximum entropy deep reinforce-\nment learning with a stochastic actor. In International\nconference on machine learning, pp. 1861–1870. PMLR,\n2018.\nHui,\nJ.\nRl\n-\nproximal\npolicy\noptimization\n(ppo)\nexplained,\nDec\n2018.\nURL\nhttps:\n//jonathan-hui.medium.com/rl-proximal-\npolicy-optimization-ppo-explained-\n77f014ec3f12.\nIlyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos,\nF., Rudolph, L., and Madry, A. A closer look at deep pol-\nicy gradients. In International Conference on Learning\nRepresentations, 2020.\nLi, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.\nVisualizing the loss landscape of neural nets. In Neural\nInformation Processing Systems (NeurIPS), 2018.\nMcCandlish, S., Kaplan, J., Amodei, D., and Team, O. D.\nAn empirical model of large-batch training.\narXiv\npreprint arXiv:1812.06162, 2018.\nMirhoseini, A., Goldie, A., Yazgan, M., Jiang, J. W.,\nSonghori, E. M., Wang, S., Lee, Y.-J., Johnson, E., Pathak,\nO., Nazi, A., Pak, J., Tong, A., Srinivasa, K., Hang, W.,\nTuncer, E., Le, Q. V., Laudon, J., Ho, R., Carpenter, R.,\nand Dean, J. A graph placement methodology for fast\nchip design. Nature, 594 7862:207–212, 2021.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational conference on machine learning, pp. 1928–\n1937. PMLR, 2016.\nNota, C. and Thomas, P. S. Is the policy gradient a gradient?\nIn Proceedings of the 19th International Conference on\nAutonomous Agents and MultiAgent Systems, AAMAS\n’20, pp. 939–947, Richland, SC, 2020. International Foun-\ndation for Autonomous Agents and Multiagent Systems.\nISBN 9781450375184.\nOller, D., Glasmachers, T., and Cuccu, G. Analyzing re-\ninforcement learning benchmarks with random weight\nguessing. In AAMAS, 2020.\nOta, K., Jha, D. K., and Kanezaki, A. Training larger net-\nworks for deep reinforcement learning, 2021.\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T.\nCuriosity-driven exploration by self-supervised predic-\ntion. In International conference on machine learning,\npp. 2778–2787. PMLR, 2017.\nRafﬁn, A. Rl baselines3 zoo. https://github.com/\nDLR-RM/rl-baselines3-zoo, 2020.\nRafﬁn,\nA.,\nHill,\nA.,\nErnestus,\nM.,\nGleave,\nA.,\nKanervisto, A., and Dormann, N.\nStable base-\nlines3. https://github.com/DLR-RM/stable-\nbaselines3, 2019.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,\nP. Trust region policy optimization. In International\nconference on machine learning, pp. 1889–1897. PMLR,\n2015.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nSharma, A., Gu, S., Levine, S., Kumar, V., and Hausman,\nK. Dynamics-aware unsupervised discovery of skills. In\nInternational Conference on Learning Representations,\n2020.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nA. All Reward Surfaces\nA.1. Classic Control\nFigure 5. Reward surfaces for 5 Classic Control environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nA.2. MuJoCo\nFigure 6. Reward surfaces for 10 MuJoCo environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nA.3. Atari\nFigure 7. Reward surfaces for 12 Atari environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nA.4. Reward Surface Plot Options\nEnvironment\nGrid Range\nGrid Samples\nEnvironment Steps\nAcrobot\n[-3, 3]\n31 x 31\n200,000\nCartPole\n[-3, 3]\n31 x 31\n200,000\nMountainCar\n[-3, 3]\n31 x 31\n200,000\nMountainCarContinuous\n[-3, 3]\n31 x 31\n200,000\nPendulum\n[-3, 3]\n31 x 31\n200,000\nAnt\n[-1, 1]\n31 x 31\n200,000\nHalfCheetah\n[-1, 1]\n31 x 31\n200,000\nHopper\n[-1, 1]\n31 x 31\n200,000\nHumanoid\n[-1, 1]\n31 x 31\n500,000\nHumanoidStandup\n[-3, 3]\n61 x 61\n500,000\nInvertedDoublePendulum\n[-1, 1]\n31 x 31\n200,000\nInvertedPendulum\n[-3, 3]\n61 x 61\n200,000\nReacher\n[-3, 3]\n91 x 91\n200,000\nSwimmer\n[-2, 2]\n31 x 31\n500,000\nWalker2d\n[-1, 1]\n31 x 31\n200,000\nBreakout\n[-1, 1]\n31 x 31\n200,000\nPong\n[-1, 1]\n31 x 31\n200,000\nSpaceInvaders\n[-1, 1]\n31 x 31\n200,000\nBankHeist\n[-1, 1]\n31 x 31\n200,000\nMsPacman\n[-1, 1]\n31 x 31\n200,000\nQ*bert\n[-1, 1]\n31 x 31\n200,000\nFreeway\n[-3, 3]\n61 x 61\n1,000,000\nMontezuma’s Revenge\n[-1, 1]\n31 x 31\n200,000\nPitfall!\n[-3, 3]\n61 x 61\n1,000,000\nPrivate Eye\n[-3, 3]\n61 x 61\n1,000,000\nSolaris\n[-3, 3]\n91 x 91\n2,000,000\nVenture\n[-1, 1]\n31 x 31\n200,000\nTable 2. Settings used to generate reward surfaces for each environment. These settings were manually chosen to highlight interesting\nfeatures of the reward surface and to reduce standard error in the plots.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nB. Standard Deviation Plots\nB.1. Classic Control\nFigure 8. Standard deviation surfaces for 5 Classic Control environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nB.2. MuJoCo\nFigure 9. Standard deviation surfaces for 10 MuJoCo environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nB.3. Atari\nFigure 10. Standard deviation surfaces for 12 Atari environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nB.4. Standard Deviation and Error Relative to Rewards\nEnvironment\nAverage Standard Deviation / Mean\nAverage Standard Error / Mean\nAcrobot\n30.58%\n0.77%\nCartPole\n37.51%\n0.67%\nMountainCar\n14.30%\n0.37%\nMountainCarContinuous\n21.33%\n0.77%\nPendulum\n6.82%\n0.22%\nAnt\n39.45%\n2.53%\nHalfCheetah\n108.68%\n7.76%\nHopper\n26.81%\n0.82%\nHumanoid\n21.08%\n0.26%\nHumanoidStandup\n11.65%\n0.52%\nInvertedDoublePendulum\n42.23%\n0.74%\nInvertedPendulum\n31.88%\n0.45%\nReacher\n13.23%\n0.21%\nSwimmer\n139.79%\n9.88%\nWalker2d\n100.01%\n2.11%\nBreakout\n119.43%\n21.76%\nPong\n15.80%\n2.09%\nSpaceInvaders\n50.02%\n2.99%\nBankHeist\n98.40%\n13.65%\nMsPacman\n52.12%\n2.83%\nQ*bert\n184.20%\n8.82%\nFreeway\n18.00%\n0.81%\nMontezuma’s Revenge\n559.64%\n65.11%\nPitfall!\n180.45%\n8.56%\nPrivate Eye\n448.63%\n23.26%\nSolaris\n166.94%\n21.49%\nVenture\n742.24%\n31.78%\nTable 3. Standard deviation and Standard error as a percentage of the rewards for each point estimate in the reward surface for each\nenvironment. We see that aside from some of the sparse Atari environments, the standard error of our reward surfaces is fairly low on\naverage.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nC. Reproducibility\nFigure 11. 18 training and plotting runs for the Classic Control Acrobot-v1 environment.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nFigure 12. 18 training and plotting runs for the MuJoCo HalfCheetah-v2 environment.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nFigure 13. 18 training and plotting runs for the Atari Breakout environment.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nFigure 14. 18 training and plotting runs for the Atari Montezuma’s Revenge environment.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nD. Standard Error Plots\nD.1. Classic Control\nFigure 15. Standard error surfaces for 5 Classic Control environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nD.2. MuJoCo\nFigure 16. Standard error surfaces for 10 MuJoCo environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nD.3. Atari\nFigure 17. Standard error surfaces for 12 Atari environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nE. All Gradient Heat Maps\nE.1. Classic Control\nFigure 18. Policy gradient heat maps for 5 Classic Control environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nE.2. MuJoCo\nFigure 19. Policy gradient heat maps for 10 MuJoCo environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nF. All Gradient Line Plots\nF.1. Classic Control\nFigure 20. Policy gradient line search plots for 5 Classic Control environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nF.2. MuJoCo\nFigure 21. Policy gradient line search plots for 10 MuJoCo environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nF.3. Atari\nFigure 22. Policy gradient line search plots for 12 Atari environments.\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nG. Network Architecture Experiments\nWe produce plots of the InvertedDoublePendulum-v2, Swimmer-v2, and Walker2D-v2 environments with different network\narchitectures to show the effect that increasing policy network depth has on a reward surface. We test actor and critic\nnetworks with 2, 4, 6, 8, 12, and 16 shared layers of 128 nodes. In each environment, as the network depth increases, either\nthe maximizer present in the reward surface becomes sharper, or the maximum reward in the plot decreases. Often we see\nboth occur. This is expected from the ﬁndings in (Li et al., 2018) that when ﬁlter-normalization is used, generalization error\ndecreases as the sharpness of the loss landscape increases. We hope that this serves as a useful preliminary result for those\nstudying network architectures in reinforcement learning, and leave further investigation as future work. For an investigation\nof loss landscapes for different RL architectures, see Ota et al. (2021).\nFigure 23. Inverted Double Pendulum environment with different number of layers in policy network. Top: 2, 4, and 6 layer networks.\nBottom: 8, 12, and 16 layer networks\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nFigure 24. Swimmer environment with different number of layers in policy network. Top: 2, 4, and 6 layer networks. Bottom: 8, 12, and\n16 layer networks\nFigure 25. Walker2d environment with different number of layers in policy network. Top: 2, 4, and 6 layer networks. Bottom: 8, 12, and\n16 layer networks\nCliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments\nH. Cliff Checkpoint Selection\nFor cliffs, we select checkpoints that exhibit a decrease in returns of at least 50% in the ﬁrst, high-resolution section of the\nline search (that is, within a normalized gradient magnitude of 0.04). By selecting points closer to the initial, unperturbed\nweights, we choose cliffs that the optimization method is likely to reach. Although we do our best to evaluate the gradient\nline searches with enough environment steps to reduce their standard error, it is not computationally feasible to completely\naccount for the high variance of returns in sparse reward environments. To mitigate this, the decrease in returns that identify\na cliff must also be at least 25% of the global reward range across all checkpoints in the line plot. This ensures that we\nselect cliffs that are signiﬁcant in the environment’s overall reward scale, and not likely to be caused by evaluation noise.\nWe select non-cliff checkpoints arbitrarily from the remaining checkpoints that do not meet these criteria.\nI. Limitations\nThe scale of axes in our plots is a ubiquitous concern in this paper. Fortunately for the 3d reward surface plots, ﬁlter-\nnormalized random directions have been well studied, and we ﬁnd that many of the original ﬁndings from the loss landscapes\npaper hold. The sharpness of the minimizer or maximizer in loss landscapes and reward surfaces respectively correlate with\nthe difﬁculty of the task.\nThe correct scale for plots that use the policy gradient direction are less obvious. In general, a continuous, sharp change in\nrewards can be made to look gentle by zooming into the slope, and any continuous change in rewards can appear to occur\ninstantly if you zoom out enough. Gradient directions are calculated from network parameters, and therefore have their\nown implicit normalization, so we cannot apply ﬁlter-normalization. We choose to normalize these directions to have the\nsame magnitude, and assume that the gradient’s implicit normalization will highlight important features. While it is possible\nthat our gradient line searches miss some cliffs, or make some checkpoints incorrectly appear as cliffs, the results of our\nexperiments with A2C appear to conﬁrm that more often than not, the cliffs in our line searches are real.\nOur comparison of PPO and A2C on cliff checkpoints is a preliminary result and does not identify which component of PPO\nmakes it more robust to cliffs than A2C. Previous work has highlighted that much of the improved performance of PPO can\nbe attributed to implementation details rather than algorithmic improvements like ratio clipping (Engstrom et al., 2020). We\nhope that future work may narrow down the speciﬁc component in PPO that allows it to perform well on cliffs.\nWe also do not use only optimal hyperparameters in our experiments comparing A2C and PPO on cliff vs. non-cliff\ncheckpoints. This allows us to verify the existence of cliffs, but does not provide clear indication as to how impactful they\nare on agents with well-tuned hyperparameters. Naturally, optimized hyperparameters should not allow for the sharp drops\nin reward that we expect to see on the cliff checkpoints. We hope that future work will explore how often cliffs affect\ntraining in practice.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-05-14",
  "updated": "2022-09-21"
}