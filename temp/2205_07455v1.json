{
  "id": "http://arxiv.org/abs/2205.07455v1",
  "title": "Reasoning about Procedures with Natural Language Processing: A Tutorial",
  "authors": [
    "Li Zhang"
  ],
  "abstract": "This tutorial provides a comprehensive and in-depth view of the research on\nprocedures, primarily in Natural Language Processing. A procedure is a sequence\nof steps intended to achieve some goal. Understanding procedures in natural\nlanguage has a long history, with recent breakthroughs made possible by\nadvances in technology. First, we discuss established approaches to collect\nprocedures, by human annotation or extraction from web resources. Then, we\nexamine different angles from which procedures can be reasoned about, as well\nas ways to represent them. Finally, we enumerate scenarios where procedural\nknowledge can be applied to the real world.",
  "text": "Reasoning about Procedures with\nNatural Language Processing: A Tutorial\nLi Zhang\nUniversity of Pennsylvania\nzharry@seas.upenn.edu\nAbstract\nThis tutorial provides a comprehensive and in-depth view of the research on proce-\ndures, primarily in Natural Language Processing. A procedure is a sequence of steps\nintended to achieve some goal. Understanding procedures in natural language has a\nlong history, with recent breakthroughs made possible by advances in technology. First,\nwe discuss established approaches to collect procedures, by human annotation or extrac-\ntion from web resources. Then, we examine diﬀerent angles from which procedures can\nbe reasoned about, as well as ways to represent them. Finally, we enumerate scenarios\nwhere procedural knowledge can be applied to the real world.\n1\nIntroduction\nNatural language is all about events. This is a not wild claim according to Oxford Languages1,\nwhich deﬁnes an event as a thing that happens. When people use natural language, it almost\nalways describes something that happens, or events.\nEvents can be grand, like “glacier\nmovement”, “establishment of an empire”, or “world economic crisis”; they can also be tiny,\nlike “moving one’s ﬁngers”, “breath”, or “alarm clock ringing.” All events, great and small,\nare a fundamental unit of how we describe and perceive the world.\nIn Natural Language Processing (NLP), the study of events is a trendy ﬁeld. From 2012 to\n2021, the yearly number of papers published on ACL Anthology2 (the primary peer-reviewed\npaper reserve for NLP) with substring “event” in their titles have tripled, from 53 to 154.\nIn comparison, the yearly number of papers with “translat(e)” in their titles has remained\nstable, from 400 to 595. What gives? The importance of events in NLP has been ampliﬁed\nby two factors. First, with the advance of other technologies, such as speech processing,\ncomputer vision and computing hardware, more and more real life applications beneﬁt from\nsome knowledge about events. For example, a virtual assistant beneﬁts from knowing the\nintentions of the user’s requests, an advertisement distribution system beneﬁts from modeling\nuser proﬁles based on users’ activities, ans so on. Second, with the advance of NLP tools\nlike large language models, analyzing constructs as abstract as events has become a lot more\ntractable. Most past NLP work has focused on “what the texts say”, but now we can attend\n1https://languages.oup.com/google-dictionary-en/\n2https://aclanthology.org/\n1\narXiv:2205.07455v1  [cs.CL]  16 May 2022\nFigure 1: Number of published papers in NLP venues containing keywords of diﬀerent topics.\nto “what is happenning”[Chen et al., 2021]. This is very exciting, because we have moved\none step towards machine understanding of meaning.\nAll events are not equal.\nOne special type of event is a procedure3.\nA procedure\nis a compound event, like “learn about NLP”, which can be broken down into multiple\nevents, like “read papers”, “take classes”, “attend seminars”, etc. What really distinguishes\na procedure from some generic event is that every procedure must consist of some goal\n(or intent, motivation), and some steps to achieve this goal. As a counterexample, if an\nindividual considers three random things they habitually do in the morning, e.g., “get out of\nbed”, “play tennis”, and “smoke cigaretts”, these are unlikely to form a meaningful procedure,\nbecause a logical goal that these events share is missing. In contrast, “arrive at the airport”,\n“go through securities”, and “wait at the boarding gate” are likely part of the procedure\n“taking a ﬂight”. Similarly, events without a sentient perpetrator are unlikely to form a\nprocedure (e.g., “rain heavily”, “time elapse“), since they arguably do not have any goal.\nWhy do researchers care about procedures? The unique roles of events in a procedure,\nnamely, goals and steps, lead to fascinating interactions among the events. The unique chal-\nlenges and opportunities brought by procedures are manifold compared to ordinary events.\nFor example, a whole class of common sense knowledge is almost exclusive to procedures.\nSay, a person “jumps up and down”. There is no telling what they want to achieve. But if\nthis person also “does push-ups” and “stretch arms”, it is then safe to assume that they’re\n“doing some sort of exercise.” Such ability to reason about goal not only is interesting in\nitself with regard to the intelligence of machines, but also has many downstream applications.\nI should also mention that the NLP community has been so zealous towards common sense\nreasoning, such that papers with ”common” and ”sense” in their titles grew from 3 in 2012\nto 66 in 2021. However, studying procedures has so much more to oﬀer, as we will see more\nin this tutorial.\nProcedures come in many forms, the most common of which is probably natural language\n3The term “procedure”, like many other NLP terms, is overloaded in diﬀerent disciplines. We follow our\nown precise deﬁnition throughout this tutorial.\n2\ninstructions, such as recipes4, manual for assembly, how-to guide, navigation lines, etc.\nAn instruction can be seen as a procedure that should be carried out instead of one that is\ncarried out by someone, just like a recipe provides guidance but is open to interpretation.\nThe majority of past work on procedures has focused on instructions, for good reasons. First,\ninstructions are simple and structured in scope, wordings, presentation, etc., compared to,\nsay, a detailed diary of someone who actually carries out a procedure. Second and more\nimportant, instructions can be easily found, on the web, from the books, and so on. In\naddition, algorithms, programs, execution plans for machines can also be seen as procedures,\nsince they do contain steps for a predetermined goal. More loosely, so are scientiﬁc processes,\nanimal behavior, etc. However, let’s not attend to them in this tutorial as they are not\nhuman activities. Our focus is indeed instructions, but let’s keep our sight within procedures\nin general, since much learning can be shared.\nCan you deal with procedural events the same way you would with general events? The\nshort answer is you could, but you would miss a lot of information. Take the example of\nthe temporal relation among events, which asks the question of which event should happen\nearlier. For some general events, this can be obvious: Christmas-eve predates Christmas in\nany calendar year. For procedural events, things get ambiguous. Should you ﬁrst “turn oﬀ\nthe lights” and then “close the door”? That depends on your goal: you should only if you’re\n“leaving the house”, but not “entering the house”! It becomes evident that the temporal\nrelation between procedural events is usually conditioned on the goal. Also note that in this\nexample, you physically cannot ﬂip these two events, unless you have a ﬁst-sized hole on your\ndoor and you have an arm of several meters. However, this is not always the case. If you’re\n“meeting with your professor”, you should ﬁrst “make an appointment” and then “go to the\nprofessor’s oﬃce”. You could theoretically ﬂip the steps, but your professor would probably\nnot be happy. In fact, procedural events require speciﬁc methodologies in their retrieval,\nprocessing, reasoning, and applications, of which we will see more in the tutorial.\nTLDR: events are one of the most important things to study in natural language; pro-\ncedures are one of the most interesting events; instructions are the most common form of\nprocedures. In this tutorial, you will comprehensively learn about procedures from the point\nof view of NLP. That is, you will ﬁrst see how past research has tackled procedures, where\nto ﬁnd data, how can we reason about them, and what use they have in real life.\n2\nA Brief History of Procedures\nResearch on procedures has largely been driven by available techniques and applications of\ninterest. Hence, it is beneﬁcial to review eﬀorts on studying procedures chronologically to\nunderstand the change of interest and methodology over time.\nThe earliest work on procedures in NLP is probably [Miller, 1976], which is unavailable\ntoday. The earliest available work was [Momouchi, 1980], which aimed to convert a proce-\ndure to a comprehensive ﬂowchart. The work formally deﬁned a procedure as “a sequence\nof actions (steps) intended to achieve a goal.” It primarily focused on knowledge acqui-\nsition, which means to extract various features from procedures.\nA usual byproduct of\nsuch extraction is some procedural representation. Many such features are still receiving\nmuch attention today, such as pre- and post-conditions, temporal duration and relation, etc.\n4Recipes are the most studied procedures in related work by a large margin.\n3\nKnowledge acquisition is a heavy focus in this tutorial, as it is the gateway to reasoning\nabout procedures; such knowledge can be applied to myriad of scenarios.\nThe procedures studied here were from recipes, a common kind of instructional texts that\nwe will later see used in many works.\nProcedural understanding is closely related to planning in artiﬁcial intelligence [Schank,\n1977]. Speciﬁcally, a substantial body at that time focused on natural language generation\nfrom plans [Mellish and Evans, 1989, Wahlster et al., 1993]. It was not surprise that these\nworks had found instructional texts tractable [Kosseim and Lapalme, 1994, Paris et al.,\n1995], which are structured and limited in scope. However, these works hardly attempted to\nlearn from procedures, until later when [Paris et al., 2002] created a human-in-the-loop tool\nfor procedural knowledge acquisition. Note that such knowledge extracted from procedural\ntexts can be transformed to plans (the reverse of “natural language generation from plans”\nmentioned above), which is a substantial body of research [MacMahon et al., 2006, Branavan\net al., 2009, Chen and Mooney, 2011, Artzi and Zettlemoyer, 2013, Kiddon et al., 2015]. We\nwill go into more details later in this tutorial.\nOne primary use of such procedural language generation is to answer how-to questions,\nthe second most sought-after type of queries on the internet at that time [de Rijke et al.,\n2005]. To that end, there were an array of eﬀorts to identify instructional texts on the\nweb [Takechi et al., 2003], automatically generate them [Paris et al., 2005], converting them\nto executables [Gil et al., 2011, Fritz and Gil, 2011], study their linguistic idiosyncrasies\n[Kosseim and Lapalme, 2000, Bielsa and Donnell, 2002, Aouladomar, 2005, Gil, 2015], and\nextract components such as titles [Delpech and Saint-Dizier, 2008].\nProcedural knowledge acquisition and representation continued to thrive [Lau et al., 2009,\nAddis and Borrajo, 2011]. The next outstanding work on procedural knowledge acquisition\nwould be [Zhang et al., 2012], which proposed a standard representation of procedures.\nCompared to [Momouchi, 1980], the proposed representation also stressed upon goals, steps,\npre- and post-conditions, but was much simpler and set the basis of procedural representation\nin subsequent research. [Maeta et al., 2015, Kiddon et al., 2015] took a diﬀerent approach,\nand focused on the relations among entities in procedures. Note that the entity-relation\nview has a long history [Chen, 1976] and is a center piece in information extraction in NLP\n[Doddington et al., 2004, Ellis et al., 2014]. We will see more about representing procedures\nlater in the tutorial.\nJust want kind of procedures are on the web?\nOne most used source is wikiHow5\n(previously eHow6), a website of how-to instructions for many tasks. As the structure and\nwriting style are consistent, wikiHow has been leveraged by NLP researchers since its incep-\ntion [Perkowitz et al., 2004, Addis and Borrajo, 2011, Pareti et al., 2014a]. In recent years,\nwikiHow has grown massively in size (now more than 110k articles), diversity (19 languages,\nhundreds of categories) and quality (editorial process7).\nAs a result, it has fueled much\nresearch on procedures from various aspects [Pareti, 2018], such as linking actions [Pareti\net al., 2014b, Chernov et al., 2016, Lin et al., 2020, Donatelli et al., 2021, Zhou et al., 2022],\nwhat-if reasoning [Tandon et al., 2019, Rajagopal et al., 2020], entity tracking [Tandon et al.,\n2020], next-event prediction [Nguyen et al., 2017, Zellers et al., 2019, Zhang et al., 2020a],\nintent reasoning [Dalvi et al., 2019], goal-step reasoning [Zhou et al., 2019b, Park and Mota-\nhari Nezhad, 2018, Zhang et al., 2020b, Yang et al., 2021], procedure generation [Sakaguchi\n5wikihow.com\n6https://www.wikihow.com/wikiHow:History-of-wikiHow\n7https://www.wikihow.com/wikiHow:Delivering-a-Trustworthy-Experience\n4\nFigure 2: An illustration of an article in wikiHow. Typically, an article contains a goal and\nsome methods, each containing some steps.\net al., 2021, Lyu et al., 2021], simulation [Puig et al., 2018], summarization [Koupaee and\nWang, 2018, Ladhak et al., 2020] and so on. Those are just some works that use procedures\nfrom wikiHow, and there are plenty that use other data sources, that we will examine later.\nIndeed, as more work has explored procedures [Mujtaba and Mahapatra, 2019], research\nagendas become more diverse. With the advance of large language models [Devlin et al.,\n2019, Brown et al., 2020a], researchers no longer ﬁxate on formally representing procedural\nknowledge, but directly use procedural texts for an array of downstream tasks, as enumerated\nabove. It is truly a great time to study procedures.\n3\nSource of Procedures\nAny work that studies procedures must ﬁrst obtain them. There are two prominent sources:\nweb resources and human annotation.\n3.1\nWeb Resources\nExtracting procedures from some resources such as cookbooks has to be one of the most\nintuitive ways of obtaining procedural data. Nowadays, most such instructions can be found\nonline in the web.\nAs mentioned before, wikiHow is one prominent resource with many\ngeneral how-to instructions. Let’s look at a case study to extract procedural information\nfrom wikiHow from one of my own work [Zhang et al., 2020b].\nScrape.\nFirst, we need to scrape the entire wikiHow website using some web-crawler to\nget the HTML for each wikiHow page. To that end we can use a Python package called\nBeautiful Soup. To traverse all wikiHow pages, we can use breadth-ﬁrst search to expand\nevery link found in each page whose URL contains wikihow.com. After getting the HTML\nfor each wikiHow page, we ﬁlter ones corresponding to actual articles using some common\ncharacteristics, such as speciﬁc formatting of the title, sections, etc.\nExtract.\nWe must ﬁrst ascertain what information we would need for a procedure. Mini-\nmally, a procedure might have a goal and some steps. The title of a wikiHow article naturally\n5\nFigure 3: An example of how we can extract a procedrue from a wikiHow article.\nsuﬃces as the goal. Alternatively, many wikiHow articles also have methods or parts, the\ntitle of which could also be a more ﬁne-grained goal. Each step in a wikiHow article contains:\n• a headline in bold, which is always the ﬁrst sentence,\n• details, which are subsequent sentences,\n• additional information, which is optionally listed in a bullet points.\nSee an example in Figure 2. One might decide what to include as goal and steps in a procedure\ndepends on their use cases. For now, let’s take a minimalist approach and treat the title as\nthe goal, and the headlines as the steps (Figure 3). Using some HTML parser like Beautiful\nSoup, we can extract them easily.\nThe data we have at this point are noisy. For example, some HTML or texts might be\nmalformed. It is thus importantly to manually check some obtained procedures to conﬁrm\ntheir quality. Alternatively, you can also use the processed wikiHow procedural data we\nreleased8.\n3.2\nHuman Annotation\nBefore instructions became widely available on the web, human annotation has been a viable\noption to obtain procedural data. Usually, this is done by crowdsourcing, where many people\ncontribute to annotating the data. Compared to using web resources, crowdsourcing costs\nmoney and requires careful design. One primary way of getting human annotations is via\nAmazon Mechanical Turk (mTurk)9, as exempliﬁed by [Lau et al., 2009]. In essence, mTurk\nis a marketplace where researchers pay people (namely, the crowd) to annotate data. To\ncollect procedures, they ask crowd workers to write down a task and provide its instructions.\nIn their case, they collected a total of 43 procedures with 300 steps, costing only 10 US\ndollars!10\n8https://github.com/zharry29/wikihow-goal-step\n9https://www.mturk.com/\n10It is important to pay annotators fairly. By a rough comparison, in 2009 the US minimal wage is $7.25.\nTo earn it, an annotator needs to annotate 29 procedures per hour, or 1 per 2 minutes.\n6\nFigure 4: The formal representation of a procedure, and an example instantiating of the\ncomponents from [Zhang et al., 2012].\nWith a reasonable budget, crowdsourcing can result in a great deal of annotations. More-\nover, it oﬀers much freedom with regard to the type of annotations we want. For example,\n[Regneri et al., 2010, Li et al., 2012, Wanzare et al., 2016] each collected data via crowd-\nsourcing with diﬀerent topics and sizes.\n4\nRepresentation of Procedures\nAt this point we have a collection of procedures, each containing a goal and some steps written\nin texts. At this simplest form, a procedure is be represented as a tree of depth 1, with the\ngoal as the root and the step as leaves. The edges thus represent “goal-step” relations. The\nevents are nodes and represented as plain texts. Such a procedural representation doesn’t\ntell us or the models much, even compared to the ancestral work [Momouchi, 1980] who\nconverted procedures to comprehensive ﬂowcharts. Fortunately, there are much that can be\ndone to represent procedures.\n4.1\nStructure vs. Texts\nBefore we dive into more sophisticated representation of procedures, it’s worth brieﬂy covering\nwhy we want to extract structured knowledge into knowledgebases, databases, or schemata\n[Shapiro and Iw´anska, 2000], and whether doing so is necessary at all. Why aren’t we satisﬁed\nof representing procedures as natural language sentences? The most obvious reason is that\nfor a long time, machines simply have been better at dealing with symbolic data than textual\ndata. Assume you want to devise a software that can tell you the ingredients of a dish, and\nwhere you can buy them. If you are blessed with a knowledgebase with the ingredients of all\ndishes and the location of all ingredients, you can trivially accomplish the task. Even if some\ninformation is missing, you can use machine learning techniques such as K-nearest-neighbors\nor even neuro-symbolic methods to infer the missing information. However, if all you have is\n7\na collection of sentences describing similar information about the ingredients and locations,\nsuch as “I just bought two bottles of milk at a grocery store yesterday,” machines struggle\nat leveraging this information without ﬁrst extracting some structured knowledge. This is\nstrong evidence that structure representation of knowledge, e.g., procedures, is beneﬁcial.\nNo wonder much eﬀort has been put into representing all kinds of knowledge using a uniﬁed\nstructure, such as the Linked Data Cloud11 devised by the semantic web community.\nOn the ﬂip-side, structured representation can be brittle and inﬂexible. The knowledge-\nbase can have many potential ﬂaws. It might only cover a subset of information. It might\ncontain partial information12. It might contain errors. All these issues can easily render\nmachine inference faulty or impossible.\nA surprising contender to structured knowledge representation is to not “represent” at all,\nbut instead maintain plain textual representation. In recent years, these natural language\nsentences can be eﬀectively leveraged by large language models (LLMs) to make eﬀective\ninferences. We will see more later that LLMs such as GPT-3 [Brown et al., 2020b] or T5\n[Raﬀel et al., 2020] actually thrives in the void of structure of the data.\nThe key takeaway here is that structured representation of procedures is a valid, but not\nthe only approach to reasoning about them. Obviously, the quality of such representation\ndirectly inﬂuences how well models can do. Let’s then check out a couple of ways to represent\nprocedures.\n4.2\nStructured Representation of Procedures\n[Zhang et al., 2012] proposed a reasonable graph representation of procedures. See Figure 4\nfor an illustration. Speciﬁcally, a procedure consists of some steps, and of course, a goal.\nEach step consists of an action verb and an actee. It may also have temporal, spatial or\nquantitative information. A step is activated by some pre-condition, and completes with\nsome post-condition. A step may also require some instruments, and may fulﬁll some purpose\n(sub-goal). Such formulation covers most of the important factors of a procedure.\nTwo major problems ensue. First of all, it is challenging to accurate extract all such\ninformation from a textual representation of a procedure, such as a recipe from a cookbook.\nThe authors of [Zhang et al., 2012] employ rule-based systems such as parsers to extract the\ncomponents. However, even were they to use the state-of-the-art neural models as of 2022,\nthe extracted information might not be complete, since some information can be implicit.\nFor example, in the action “add salt to water and bring to a broil”, knowing “bring what to\na broil” requires co-reference reasoning. Further, features like pre- and post-conditions are\neven trickier, and remains an open question to this day, that I also actively work on. Without\na means to accurately extract such information, it is challenging to apply the representation\nto downstream tasks such as question answering for a wide variety of procedures.\n[Kiddon et al., 2015] provides another representation of procedures, focusing on recipes.\nCompared to the previous representation, their focus shifts to entities and relations in proce-\ndures. Naturally, this representation is a boon for answering questions about these entities,\nsuch as “where is the bacon” or “what is mixed with the ﬂour”. However, it hinges on the\nrelatively regular phrasings from recipes, and may not handle other types of procedures well.\n11https://lod-cloud.net/\n12For example, a knowledgebase can have an entry for “milk” but none for “oat milk,” and thus cannot\ndeal with the latter at all despite its high similarity to the former.\n8\nFigure 5: An example recipe and its representation from [Kiddon et al., 2015], denoting the\nactions, ingredients, and their relations to one another.\n4.3\nTextual Representation of Procedures\nRepresenting procedures as texts is quite trivial, and we have seen how we can achieve this.\nWith the advance of large language models, diverse expressions of procedures in free-form\ntexts can also be directly input into the models.\nFree from the bottleneck of extracting\ninformation to form a structured representation, researchers can harness the power of these\nlanguage models to learn from procedures and apply learned knowledge to downstream tasks\nmore ﬂexibly. For the recent years, this has been the dominant approach. In the next section,\nwe will go with the ﬂow and see many examples of working with only textual representation.\nJust because language models work excellently with textual representation does not mean\nthat structured representations are in vain. They still remain a formidable force for proce-\ndural reasoning, especially coupled with quickly advancing neural-symbolic methods [Huang\net al., 2021]. Moreover, structured representations are a step towards language grounding,\nwhich is crucial to robots executing instructions [Puig et al., 2018, Huang et al., 2022, Ahn\net al., 2022]. This area is of course related to procedures and an extremely important front\nof artiﬁcial intelligence, but we will not go into details in this tutorial.\n5\nLearning from Procedures\nThere are many world knowledge encapsulated in procedures, referred to as procedural knowl-\nedge. Learning it not only helps with downstream applications, but also sheds light on model’s\ncapabilities. In this section, we will see how models can learn some common knowledge from\nprocedures.\n5.1\nGoal-Step Relations\nAt the core of a procedures are goals and steps. In practice, however, we might not have\nthe luxury of having all of them at hand. There are many cases where we need to infer the\ngoal from steps. For example, sociologists might be interested in people’s motivation of their\nactions; advertisers might want to know what people want from their browsing activities;\n9\nFigure 6: Examples from the goal-step reasoning tasks from [Zhang et al., 2020b].\ndialog systems might need to ﬁgure out the intent from utterances about user activities.\nConversely, inferring steps from a goal is equally useful. For example, a household assistant\nmight need to help user ﬁgure out how to accomplish tasks. Inferring goals or steps hinges\non understanding the goal-step relation between events. Moreover, the temporal relation\nbetween two steps is equally important. While temporal relations among general events has\nbeen well studied [Zhou et al., 2019a, Han et al., 2019, Vashishtha et al., 2020, Ma et al.,\n2021], the ordering of steps additionally depends on the goal. For example, “go to the oﬃce”\nand “buy food” in general do not have a habitual order, but if both events are step from the\nprocedure with the goal of “have an oﬃce potluck”, it becomes clear that “buy food” should\nhappen ﬁrst. Such is an illustrative example of procedural events’ idiosyncrasy.\nApart from the practical uses, the knowledge of goals and steps is a part of human\ncommon-sense and might be a critical part of artiﬁcial general intelligence. Being able to\nreason about goals and steps also has strong implication towards other NLP tasks such as\nquestion answering, machine reading comprehension, etc. Let’s see how we can learn the\ngoal-step relations in wikiHow following one of our own work [Zhang et al., 2020b] (examples\nshown in Figure 6).\n5.1.1\nGoal-Step Relation\nThe simplest way to formulate this task is that given two events, a model needs to predict\nwhether one can be the step while another can be the goal, logically. In previous sections, we\nhave extracted hundreds of thousands of procedures from wikiHow, each containing a goal\nand some steps. A pair of such a goal and a step then forms a positive example, while a pair\nof some goal and a step from another procedure likely forms a negative example. However,\ndirectly judging the goal-step relation might be subject to ambiguity. For example, is “play\nguitar” a logical goal of “stretch your arms”? It really can go both ways, since you realistically\ncan get cramps if you practice guitar for too long and your arms get sore. However, if we\nconsider another potential goal like “debate”, then the step “stretch your arms” is far more\nunlikely. For this reason, we set up the task comparatively, using a multiple choice setting.\nNamely, given a goal and a couple of potential steps, a model needs to choose the most likely\nstep. Vice versa for given a step and some goals. Consider this example of inferring the step\nfrom a goal.\n10\nWhat is the most likely step of the goal “prevent viruses”?\nA. Wash hands. B. Clap hands. C. Wash a cat. D. Sleep early.\nThe answer is unambiguously A, as B and C are irrelevant, and while D does help with\nboosting one’s immunisation system, it’s less direct than A.\nNow that we formulate the step-inference and goal-inference tasks as multiple choice, an\ninteresting and important consideration is how we should sample negative examples. Note\nthat sampling positive examples is trivial as we can just take a goal and a step from a wikiHow\nprocedure. Think about a multiple choice exam. If the “wrong options” are too obvious, the\nexam becomes trivial. Conversely, all of us must have seen some frustrating “bad questions”\nwhere there are multiple correct answers, which should not occur. In our case, given a step,\nhow do we sample some non-goals that are distracting enough for the models? One way is to\nuse semantic similarity search, which helps us ﬁnd sentences that have similar meaning. Let’s\nconsider the step “store your receipts in a binder” that comes from the wikiHow procedure\nwith the goal “organize receipt”. We then need to ﬁnd a couple of other goals similar to\n“organize receipt”, but they cannot be the goal of “store your receipts in a binder”. If we\nsearch for “organize receipt” using a search engine on wikiHow, we can ﬁnd “write a receipt”,\n“print a uber receipt”, “create a donation receipt”, etc. All those are favorable, since they\neach cannot be a logical goal of “store your receipts in a binder”. To implement such a search\nengine, a typical way is to use Elasticsearch with the classical BM25 search algorithm, while\nindexing the entire textual information from a wikiHow website.\nAn alternative to using search engines is to use a sentence embedding approach. First,\nwe use a sentence encoder such as sBERT [Reimers and Gurevych, 2019] to encode each step\nand each goal from all wikiHow procedures as vector representations. Then, considering the\ngoal inference task, given a step, its corresponding goal, and this goal’s embedding, we can\nsearch for K goals whose embeddings are the closest to the embedding of the given goal, by a\ndistance metric such as L2 or cosine distance. Though there are millions of goals and steps in\nwikiHow, we can rely on a fast K-nearest-neighbor search algorithm such as FAISS [Johnson\net al., 2019] for eﬃciency.\nAt this point, we have found some highly distracting negative examples, but we still have\nno guarantee that these negative examples are indeed negative. For the previous example\nof the goal “organize receipt”, one of our negative examples could be “organize receipt for\na small business”, which can certainly be the goal of “store your receipts in a binder”. If\nwe include this case in a multiple-choice example, it would adversely aﬀect our evaluation\nsince the model predictions would have false negatives. There are no easy ways to eliminate\nthese bad distractors. One idea is to check for lexical overlap, and exclude negative examples\nthat are almost identical to the positive example. Another idea is to place more weights on\nthe verb or the object of a goal or a step while performing similarity search. None of these\nmethods are perfectly reliable.\nFortunately, the noise induced by bad negative examples is not deal-breaking, as we can\nconstruct millions of examples from wikiHow, and most of them are good. Hence, models\ncan usually still learn a lot despite some occasional noise. For evaluation, we can be a bit\nmore cautious and run a human validation. For this we can resort to crowdsourcing using\nAmazon Mechanical Turk. All we need to do is to have people answer our multiple choice\nquestions, and only keep those that most crowd workers answer correctly. In fact, having a\nlarge but noisy training set and a small but clean evaluation set is typical in NLP tasks.\nWe now have a dataset of multiple choice questions for step and goal inference, but\n11\nwe’re not done yet. We have to look out for dataset artifacts, which are spurious statistical\ncorrelations that spawn during our data creation process. There are many kind of artifacts,\nbut one notorious kind common in multiple-choice questions comes from negative sampling.\nWork such as [Zellers et al., 2019] has shown that language models can sometimes achieve high\nperformance in multiple-choice questions by only looking at the choices but not the question.\nThis is reminiscent of some badly designed exam questions, where all correct answers are the\nthird one or the longest one. Note that large neural networks work with manifold so that\nthey can pick up complex cues that we human do not intend nor notice, and thus achieve\ndeceptively high performance. Our dataset is no exception. If we mask out the given goal\nor step but only provide the choices, our best model can achieve signiﬁcantly over chance\naccuracy. To deal with this common issue of cues among choices, for each example, we further\nrandomly re-assign a choice as the correct answer. For instance, consider the example with\nthe goal of “stay healthy” and 4 candidate steps “work out”, “work long hours”, “try out\nlocal restaurants”, and “eat a lot of food”. The correct answer is apparently “work out”. To\navoid models picking up cues, we then randomly decide that “try out local restaurants” is\nthe correct step, while switching the given goal to its corresponding one, “visit a new city”.\nThis way, if we again mask out the given goal or step but only provide the choices, all models\nare destined to perform no better than randomly guessing, since all choices have an equal\nchance of being correct.\nWhat about the performance? For humans, my collaborator and I each did 100 questions\nfor goal inference and step inference, and we have around 97% accuracy. In contrast, state-\nof-the-art language models like RoBERTa [Liu et al., 2019] can achieve about 85%. Since\nit’s a 4-choose-1 format, the chance performance is 25%. This shows that models can quite\nreliably learn procedural knowledge of goals and steps from our data, which is a good news.\n5.1.2\nStep-Step Temporal Relation\nAnother interesting relation of goals and steps is the step-step temporal relation, namely the\norder in which steps happen in a procedure. Namely, given a goal and two of its steps, a\nmodel needs to choose the one that should happen earlier. Consider this example.\nFor the goal “wash silverware”, which step should happen ﬁrst?\nA. Rinse the silverware. B. Dry the silverware.\nThe answer is unambiguously A. Instead of multiple-choice, the task of step ordering should\nbe formatted as binary classiﬁcation, since both of the steps and the goal need to be accessible\nto models. In this case, negative sampling is not an issue, since we can just ﬂip the two steps\nto form an example with an opposite label.\nFor humans, we again did 100 questions for goal inference and step inference, and we\nagain have around 97% accuracy. In contrast, state-of-the-art language models can achieve\nabout 80%, with a chance performance of 25%.\n5.2\nState Tracking\nWhile events are at the core of natural language, entities are at the core of events. Entities are\njust things involved in events and they can move around, change form, or remain constant.\nIn procedures and especially instructions, entities are even more crucial, because here the\nlanguage is usually concise, and every entity mentioned likely plays some important role. For\n12\nFigure 7: An example of the data annotation in ProPara. Each ﬁlled row shows the existence\nand location of participants between each step (“?” denotes “unknown”, “-” denotes “does\nnot exist”). For example in state0, water is located at the soil.\nexample, to drink bottled water, we can buy a bottle from the vending machine, unscrew\nthe cap, and take a sip. Every entity here is indispensable. Without the vending machine,\nwe would have to procure the water somewhere else; the bottle contains water, and has a\ncap that must be removed prior to drinking. Hence, the knowledge of what happens to these\nentities, referred to as their states, is an important knowledge to learn. Such knowledge has\nplenty of practical uses. For example, it can be used to answer questions about the world\n(e.g., where is the cap when I’m drinking the water?), or even provide concrete guidance to\nrobots (e.g., if a water-drinking robot knows that the cap is not on the bottle while drinking\nand that a bottle containing water should be capped, it can infer that it should re-cap the\nbottle). There are a handful of existing work on state tracking [Long et al., 2016, Bosselut\net al., 2017, Mysore et al., 2019]. We will go over some latest ones. We will focus on the task\ndeﬁnition and data instead of methods here.\n5.2.1\nExistence and Location in Scientiﬁc Processes\nLet’s ﬁrst look at ProPara [Dalvi et al., 2018], one of the earliest attempts of state tracking\nin procedures. The authors of this work focus on a speciﬁc kind of procedures: scientiﬁc\nprocesses (e.g., photosynthesis), unlike the rest of this tutorial which focuses on instructions.\nThey track entity state changes for two attributes: existence and location. Namely, after\neach step, they want to know if any entity of interest is created or destroyed, and where it is\nif it exists. See Figure 7 for an example.\nTo create this dataset, they ﬁrst come up with a list of prompts of target scientiﬁc\nprocesses, and then ask crowd workers to write down the steps of these procedures. Next,\nanother group of annotators are asked to write down the entities that have state changes\nand mark the steps after which they are created or destroyed. Finally, a diﬀerent group of\nannotators write down the location of these entities after each step. Thus, a dataset of 81,345\nannotations over 488 paragraphs about 183 processes is created.\nThe ProPara dataset is great for tracking states. However, it only study two attributes.\n13\nFigure 8: An example of the data annotation in OpenPI. The input comprises a query and\na context (past sentences before this step in the paragraph). The output is a set of pre- and\npost-conditions.\nThere are many attributes other than existence and location that have state changes to be\ntracked (e.g., temperature, openness, color, etc.). Moreover, its domain is limited to scientiﬁc\nprocesses.\n5.2.2\nOpen State Tracking in Instructions\nThe next work we’re going to look at, OpenPI [Tandon et al., 2020], addresses the previous\nissues by expanding the attribute set to an open vocabulary. Namely, in addition to existence\nand location, other attributes like temperature, openness, and color are all fair game. Also,\nOpenPI gets its data from wikiHow, falling into the line of work on instructions which cover\na wide variety of domains.\nSimilar to Propara, OpenPI also uses crowdsourcing to collect data. The authors ﬁrst\nselect a suitable subset of wikiHow articles, and then ask crowd workers to write down a\ncouple of state changes for each step. See an example in Figure 8. A model trained on this\ndataset can then also predict some state changes given a step and its contexts.\nThis is great, because now a model knows about the entities in a procedure beyond their\nexistence and location. However, there is no guarantee that the state changes written down by\nthe annotators or predicted by the models are complete. For example, during the procedure\n“fry ﬁsh”, once you “add the ﬁsh to the boiling oil”, many things will happen and many\nstates will change: the ﬁsh will become hard and brown, a crust will form on the surface, the\noil temperature will temporarily decrease, there will be a sizzling sound, and so much more.\n14\nLimited by the training data, a model trained on OpenPI is unlikely to predict all the state\nchanges. This will cause the model to fail when a query is made for a state that the model\ndoes not predict.\n5.2.3\nQuestion Answering of State Tracking\nThe current project of myself and my colleagues takes state tracking in procedures to the\nnext level: what if a model needs to know every state change possible in a procedure?\nApparently, it is impossible to annotate all state changes. However, we might be able to\nleverage the state-of-the-art language models’ strong few-shot learning ability to ﬁgure out\nthese state changes on the ﬂy. For instance, given the ﬁrst example in Figure 8, after the\nstep “apply insecticide to peonies” a natural question might be “are the peonies harmful to\nhuman body?”\nWe manually create such a dataset consisting of questions and answers regarding entity\nstate changes, and our preliminary experiments show that a language model trained on\nOpenPI is unable to answer almost all questions, due to its limit on the predicted state\nchanges.\nSimilarly, strong language models such as T5 also fails almost completely.\nIn\ncontrast, few-shot learners such as GPT3 is able to answer questions regarding entity state\nchanges reasonably. We are currently investigating ways to improve the model, as well as\nother formulation of questions.\n6\nApplications of Procedures\nProcedures are so ubiquitous in life that knowledge about them can be applied in many\nscenarios. For example, the technique of reasoning about goals can be used in advertisement\nto understand people’s wants and needs. The technique of reasoning about steps can be used\nto help users ﬁgure out how to do tasks. Moreover, holistically understanding procedures has\neven more implications. Stories and scripts can be broken down into procedures to be better\nunderstood. Schema of procedures can enable comparison and clustering of procedures.\nIn this section, we will explore some highly practical applications of procedures.\n6.1\nSuggesting Steps\nAs humans, we are familiar with some but not all procedures. For example, as a PhD student\nmyself, I know “how to carry out a research project” all too well, whereas I struggle at “how\nto ﬂy an airplane,” unlike some of my friends who have had aviation training. It would be\nnice if someone can show me the steps of tasks that I don’t know how to do. Without such\nsomeone, my best bet is to look up the tasks online. One major issue of doing so is that\nthere might not be instructions of my task. Moreover, I might require some personalization\nof the task (e.g., “ﬂy an airplane while being disabled”). In such cases, it would be helpful if\na model can generate steps given a goal, even if it has never seen the goal or the task does\nnot previously exist. Let’s see how we can train such a model following [Lyu et al., 2021].\nLet’s ﬁrst formally deﬁne our task. Given a goal (e.g., “write a song”), the model should\noutput a sequence of reasonable steps (e.g., “ﬁnd inspiration”, “determine a feel”, “choose a\nchord progression”, etc.) that may or may not be ordered. As we will soon see, this task is\nno easy feat. To do it well, we need holistic understanding of procedures.\n15\nWe have some data at our disposal. From wikiHow, we have more than 100k procedures\nthat allow us to perform supervised learning. After splitting them into training and testing\nsets Let’s consider two intuitive approaches for modeling.\nLanguage Model Generation.\nNow is the age for using large language model’s generation\nability to tackle a variety of tasks. Our task is no exception. Very simply, we can format\neach procedure as a sequence of the goal and the steps, such as:\nSear a steak [SEP] Heat the pan [SEP] Pat dry the steak [SEP] ...\nUsing a generation model such as T5 or GPT-3, we can then use all the procedures in the\ntraining set to ﬁnetune the model. During inference, given a goal, the ﬁnetuned model would\nbe able to output the steps in a similar format.\nHere is an example of some generated steps for “icing an injured ankle” using T5:\n1. Hang the ankle up in a large, open space.\n2. Wrap the injured ankle up in a heated towel.\n3. Put the ice pack against your leg for 15-20 minutes.\n4. Elevate your legs above the level of irritation.\n5. Take an over-the-counter pain reliever.\n6. See a doctor if 2-3 days persist or worsen.\n7. Keep your foot elevated to avoid further swelling.\n8. Use an orthopedic shoes to trim your ankle.\n9. Wear the right shoes and footware.\nAre these suggested steps any good? At a quick glance, they may look coherent, but at\na closer look, almost all steps are problematic and are no way to really treat your injured\nankle. Here are some issues that are in fact representative of language model generation’s\ncommon failures.\n1. Illogical actions. In 1, you don’t need a large, open space to hang your ankle. In 8, you\nprobably don’t want to make things worse by “trimming” your ankle.\n2. Irrelevant steps. In 2, heated towel has nothing to do with icing. In 4, it would be\nstrange to refer to an injury as “irritation”.\n3. Grammatical issues. In 6 and 8, the grammar is incorrect.\nWhile end-to-end language models are very straightforward to use, the ways that they can\nbe improved are limited. Naturally, we may adjust the language model’s hyperparameters or\nconﬁgurations such as temperature. We may also try diﬀerent data format, or experiment\nwith data distillation, augmentation, or curriculum learning. However, I myself don’t see\nthese tweaks bring about large improvements. Most likely, some insights into the task of\nsuggesting steps is need to handle it better.\nInferring and then Ordering Steps.\nInstead of the end-to-end approach above, we could\nalso reuse our model that infers steps given a goal, and the one that orders steps given a\ngoal. Recall that the step inference model is a transformer (such as RoBERTa) that takes in\ntwo events and calculates a probability of a goal-step relation. To suggest steps given a goal,\n16\nwe can then use this model to calculate the pairwise probability between the goal and every\nsingle possible event, and keep the events with the highest probability of actually being the\nstep of the given goal. While we can’t possibly get all events, we can use all the steps from\nall articles in wikiHow as a candidate pool. Using this approach, we have already eliminated\nissue 1 and 3 of language model generation, given that wikiHow steps are grammatical and\nsensical.\nSuppose we now have the pairwise goal-step probability between the given goal and each\nwikiHow step. We can then choose top K steps with the highest probability, where K is a\ndesign choice and depends on how long we want the suggestion to be.\nNext, we can order these K steps by running our step ordering model, a transformer that\ntakes in two steps and a goal and calculates the probability of the ﬁrst step preceding the\nsecond. To use this model to order or rank more than 2 steps, we can ﬁrst order every pair of\nsteps, and then poll the number of times a step precedes any other step. This is reminiscent\nof a sports tournament where every two players or teams play, and the ranking is determined\nby the number of wins each player or team ends up with.\nBefore looking at some examples of model generation, let’s think about the pros and cons\nof this approach. On the bright side, as mentioned before, since all steps come from wikiHow\nwhich has an editorial process, most of them will be logical and well-formed. However, there\nare also many possible failures. First, the step inference model is not perfect, and some non-\nsteps, perhaps irrelevant ones, might be mistakenly ranked high. Even if the step inference\nmodel were perfect, the top-ranked steps are likely to contain duplicates. Consider the goal\n“choose a laptop”, and a reasonable step “decide on a budget.” It is obvious that there could\nbe many steps in wikiHow about deciding on budgets from articles of buying diﬀerent things.\nThese steps might also have diﬀerent wordings, such as “choose a reasonable budget”, “set\na budget”, “decide on how much you want to spend”, etc. A perfect step inference model is\nbound to score all of these steps high, so that after keeping the top K, we will end up with\nmany steps with the same meaning.\nTo ameliorate this issue, we can leverage a paraphrase detection module which was not\nused in [Lyu et al., 2021]. There are many options. Simply, we can use a sentence embedding\nmodel such as sBERT ﬁnetuned on some paraphrase corpus to calculate the embedding for\neach candidate step. Then, we can use algorithms such as K-means to cluster the steps by\ntheir meaning, before we choose one step from each cluster. Alternatively, we can also use\na dedicated paraphrase detection model such as [Nighojkar and Licato, 2021] to calculate\npairwise similarity among the candidate steps.\nWe have looked at possible failures during the phrase of inferring steps, and will now\nlook at those during step ordering.\nJust like step inference, the step ordering model is\nimperfect and would make erroneous predictions. Moreover, such errors can propagate via\nthe tournament ranking algorithm. For example, given 3 candidate steps A, B, and C, if\nthe step ordering model predicts that A precedes B, B precedes C, and C precedes A, an\nuninformed tie-breaking mechanism (such as randomly choosing one) needs to be invoked.\nOne glaring weakness of our current temporal ordering approach is that we rely on ag-\ngregating pairwise predictions. A more reasonable alternative may be to directly train a\ngeneration model to re-order the entire sequence of steps. To the best of my knowledge, this\nis only attempted by one piece of work [Byrne et al., 2021], which learns a denoising autoen-\ncoder with event sequences that have been shuﬄed. A minor diﬀerence between their work\nand our task is that their model is designed for narrative events which do not necessarily\nhave a goal, like procedural events.\n17\nLet’s look at an example set of steps predicted by our approach, given the same goal as\nabove “ice an injured ankle”.\n1. Place an ice pack on the injured ankle for 15-20 minutes.\n2. Rest the injured ankle as much as possible for 48 hours.\n3. Compress the injured ankle for 48 hours to prevent swelling.\n4. Elevate the injured ankle above your heart to decrease any swelling.\n5. Go to the emergency room if the injured ankle cannot bear weight.\n6. Apply a compression bandage to a sprained ankle.\n7. Consult with a doctor if the ankle does not improve after 2-3 days.\n8. Ask a friend to grab your ankles from the bottom.\nLooks pretty good! Here are some common failures.\n1. Irrelevant steps. Step 8 may be irrelevant, but we don’t have enough information.\n2. Duplicated steps. In this case, we do not observe much duplication within the steps.\n3. Wrong ordering. Step 6 should happen as early as step 3, while step 5 should be the\nvery ﬁrst thing to do.\nUntil now, we have looked at some models to suggest steps given a goal and some of their\nsample output but we haven’t discussed how we can systematically evaluate the suggested\nsteps.\nSimilar to many language generation tasks, faithful automatic evaluation is hard\nto come by. The key issue is that there is no single right answer, and no straightforward\nway to compute the similarity between steps suggested by a model and the reference steps.\nAccording to [Lyu et al., 2021], common metrics such as BERTScore [Zhang et al., 2019]\nhave almost no correlation with human judgements.\nFor now, we’re better oﬀrelying on human judgement, designing which is non-trivial\neither.\nFor procedures that most of us know about, we can perform human judgement\nwithout a reference set of steps. One systematic way to score a sequence of suggested step\nis to edit the steps, by either adding, removing, or re-ordering steps. The number of such\nedit operations is called edit distance. Intuitively, well-suggested steps need few edits, thus\nhaving a small edit distance, and vice versa. Note that such evaluation process has its share\nof shortcomings. For example, it is quite subjective to add steps, as people may expect\ndiﬀerent level of details or granularity from procedures.\nAs a ﬁnal remark, suggesting steps given a goal is an important application of procedural\nknowledge. While current models can do this to some extent, there is still a large room of\nimprovement.\n6.2\nFinding Sub-Steps\nA procedure consists of a goal and some steps. In fact, each step can itself represent another\nprocedure. For example, the procedure “make a movie” may have a step “buy a camera”,\nwhich in turn can be the goal of another procedure consisting of steps such as “set a budget”,\n“decide use case”, “read reviews”, etc. By this logic, procedures are hierarchical, and steps\nmay have sub-steps. This is also the case for general, narrative events, and such hierarchical\nrelations have been studied [Bisk et al., 2019] in some but not much work.\n18\nFigure 9: Example of procedural hierarchies in wikiHow.\nIn the previous section we touched on the issue of event granularity in procedures. When\nit comes to procedures, the level of details in the steps vary greatly. To teach an adept iPhone\nuser how to “ﬁx a frozen iPhone X”, you may just say “force restart it”, resulting in a 1-step\nprocedure. For general audience, you may say “press the volume up botton”, “press the\nvolume down botton”, and “hold the volume up botton”, resulting in a 3-step procedure. To\nteach someone who has never used electronics, you might need to further explain how to hold\nbuttons and where they are. Having control over the granularity of procedures is important\nin many ways. Not only does it give us control over the level of details in instructions, but\nit also inﬂuences some downstream tasks as we will see later. Such granularity can be easily\naccessed if we have a hierarchy of procedures, where abstractions are higher and details are\nlower in the hierarchy.\nBuilding a procedural hierarchy is equivalent to recursively ﬁnding sub-steps for steps,\nwhich is in turn equivalent to linking steps to procedures. In wikiHow, there exists some\nhyperlinks from a step to another article. For example, in “How to Make a YouTube Video”,\nthe step “Record content that is on your computer monitor” is linked to another wikiHow\narticle “How to Record Your Computer Screen”. Unfortunately, such hyperlinks are very\nscarce.\nIf all steps were to have such links when applicable, we can then ﬁnd sub-steps\ntrivially, and thus a procedural hierarchy can be constructed. In fact, linking contents in\nwikiHow has been attempted in previous years [Pareti et al., 2014a, Chernov et al., 2016].\nLet’s see how we can use state-of-the-art NLP techniques to tackle this problem in one of my\nown work [Zhou et al., 2022].\nOur task is to link all steps in wikiHow to some new article or procedure when possible.\nSometimes, a step can be too speciﬁc (e.g., “whisk together two eggs and a cup of heavy\ncream”) or too low-level (e.g., “breathe”) to have sub-steps, in which case we mark this\nstep as unlinkable. For now, let’s focus on linking whole steps in wikiHow, though it’s also\nreasonable to link a part of the step, such as individual actions.\nOur labeled data comes from wikiHow’s existing hyperlinks, which are provided by editors\nand high-quality in general. We split this dataset into training and testing splits. Now,\ngiven a step, how do we ﬁnd a corresponding wikiHow article? This is a typical information\n19\nretrieval or search problem, for which sentence embeddings and semantic similarity are typical\nsolutions. For now, let’s only try matching each step with the goals of the articles, ignoring\nother information in the articles. We have two modeling options.\n1. Unsupervised. We can directly use a pretrained sentence embedding model such as\nsBERT to convert each wikiHow step and goal into a vector representation. Then, we\ncan use a fast K-nearest-neighbor (KNN) algorithm such as FAISS for ﬁnd the closest\ngoal of each step.\n2. Supervised. We can train a network to take in a step and a goal, embed them using\nsBERT, and predict whether they should be linked. In our training data of hyperlinks,\neach training instance is a step and the goal of the article that the step is linked to.\nThis pair of step and goal should have the same meaning.\nThese two methods each have their pros and cons.\nThe unsupervised method is fast\nthanks to the eﬃcient KNN search, but fails to leverage our training data, leading to lower\naccuracy. The supervised method is slow because each step-goal pair needs to be computed,\nbut has better performance by learning from step-goal pairs that have the same meaning.\nFortunately we don’t have to choose. It is conventional in information retrieval tasks to\nsequentially perform these two methods in a retrieve-then-rerank pipeline. First, we use\nthe unsupervised method to narrow down the candidate goals. Then, we use the supervised\nmethod to re-score each pair of the step and a candidate goal, and re-rank the goals.\nYou can see a demo of the resulting procedural hierarchy at https://wikihow-hierarchy.\ngithub.io/.\n6.3\nGuiding Dialogues\nProcedural knowledge has seen use in commercial voice assistants such as Amazon Alexa.\nAt the time of writing, if you ask Alexa how to do something, it will guide you through a\ncorresponding wikiHow article or a Whole Foods Market recipe13 step by step. Navigating\nthrough the steps itself is straightforward, but almost any other interesting interaction require\nprocedural knowledge. For example, if a user asks for clariﬁcation of a step, then the system\nmust know what article to redirect the user to given the contexts (Section 6.2). If a user asks\na question during the procedure, knowledge about entity states might be needed to provide an\nanswer (Section 5.2). If a user run into issues such as not having a certain tool or ingredient,\nthe system needs to invoke what-if reasoning. More futuristically, if a user’s desired task does\nnot have a good set of instructions available, the system can suggest some reasonable steps\n(Section 6.1). In fact, our team at the University of Pennsylvania participated in the Alexa\nPrize TaskBot Challenge14 from 2021 to 2022, and worked on most of the above aspects.\nWhat we have touched upon are just a tip of the iceberg of what procedural knowledge\ncan be used to do in real life. I can’t prove this claim to you at this time, because most of\nthese potential applications remain ongoing or unexplored work. For example, procedural\nknowledge can be used to generate texts, such as a story about going through a lawsuit, or\na ﬁctional narrative about scaling a castle wall. I’ll happily leave all those to future work.\n13https://www.wholefoodsmarket.com/recipes\n14https://www.amazon.science/alexa-prize\n20\n7\nProcedure and Script\n[Schank, 1977] introduced the concept of script as a way to unify knowledge of events,\nreconciling distinctive conventions from psychology, cognitive science, and of course, artiﬁcial\nintelligence. From the perspective of AI, script learning is an ambitious journey towards\nartiﬁcial general intelligence, as it attempts to reduce arbitrarily complex human activities\ninto low-level, symbolic representations that machines can conceivably understand. Such\nan attempt was tremendously diﬃcult, as there are myriad of exceptions and edge cases\nthat any intelligent system must account for. Thus, script learning culminated in popularity\naround that time, but was thwarted by the limitation of technology. In recent years with\nthe advances in technology, it has been steadily picking up traction again. Procedures and\nscripts share a lot of common grounds, since they are both semantic constructs that describe\nevents. In this section, we will juxtapose the two concepts and gain some interesting insights.\nAccording to Schank,\nA script is a structure that describes appropriate sequences of events in a partic-\nular context.\nFor example, the following is a script.\nJohn went to a restaurant. He asked the waitress for coq au vin. He paid the\ncheck and left.\nThe following is not a script.\nJohn was walking on the street. He thought of cabbages. He picked up a shoe\nhorn.\nAn astute reader can immediately notice how a script is similar to a procedure. The only\ndiﬀerence is the emphasis on the “appropriateness” and the “context”, and the lack of the\nemphasis on the “goal” as in a procedure. Indeed, a procedure can be seen as a special kind\nof script: a much simpler one at that. In fact, Schank deﬁned a special type of script, called\ninstrumental script:\nThe crucial diﬀerences between instrumental and situational scripts are with re-\nspect to the number of actors, and the overall intention or goal of the script.\nThis looks just like procedures15. Conversely, other types of scripts, such as situational\nscript containing sequences of actions in certain scenarios, are much more full-ﬂedged, with\nmany properties that are absent in procedures. Note again that the term “procedure” dis-\ncussed here mainly points to what has been studied in NLP (e.g., natural language instruc-\ntions).\nLet’s compare some features of a script and a procedure.\nA script can have multiple explicit actors.\nA procedure usually has only one\nimplicit actor.\nA script can involve more than one person, interacting with each other\nin an arbitrarily complex fashion. Each person has diﬀerent viewpoints, and thus diﬀerent\nscripts from diﬀerent perspectives. However, most procedures, especially those in the form\n15There are still some diﬀerences. For example, Schank claims that instrumental scripts have “very rigid\nordering”, and “each action must be done.” While these are sometimes true, we have seen that steps in a\nprocedure often have ﬂexible local ordering, and varying degree of importance.\n21\nof instructions (e.g., recipes, how-to guide) do not have any explicit actor. The actor of the\nsteps is implied to be the reader or the person following these instructions. Nonetheless, it is\nimportant to recall that instructions are merely a primary focus for research on procedures,\nand procedures do not necessarily appear in the form of instructions. Conceivable, there\ncould be a procedure where two person collaborates to achieve some goal. In such case,\ninter-actor interactions present in scripts can also be present in procedures.\nA script can meaningfully contain multiple procedures.\nFirst, it is crucial to note\nthat both scripts and procedures can be hierarchical. Namely, a script or procedure (e.g., eat\nat a restaurant) can contain other scripts or procedures (e.g., order food, paying the check).\nSince a procedure (as deﬁned in this tutorial) is centered around one goal, it is logical to\nassert that its sub-procedures are thus centered around its sub-goals. However, a script can\ninvolve no goal, one goal, or multiple goals. These goals can belong to diﬀerent actors, one\nactor but at diﬀerent time stage, or one actor simultaneously. Naturally, the events in the\nscript that serve any particular goal would form a procedure. Thus, a script can meaningfully\ncontain multiple procedures. The reverse can also be true, though usually less meaningful.\nFor example, the procedure of “getting a college degree” might have a step “take classes”,\nwhich could be represented by a script with all the complexities such as multiple actors,\nsub-scripts, etc. However, since one would only care about events in this script that serves\nthe goal “take classes” or its sub-goals, it is much convenient to view the step “take classes”\nas another procedure rather than a full-ﬂedged script.\nA script is subject to interferences and distractions, while a procedure usually\nis not.\nOne of the most important distinction between scripts and procedures is that\nscripts often interact and intertwine with each other, while procedures often do not, barring\nhierarchical relations. For example, during a script of “working at a computer at home”,\nwe inattentive mortals can easily be distracted by another script “watching some YouTube\nvideos”, or be interfered by another script “attend to the ringing doorbell”. When such\nsidetracking happens, script learning becomes extremely complex. What will the actor do?\nDo they switch to the other script and just forfeit the current script? Do they deal with the\nother script and return? Are there inﬂuences or damages to the current script caused by\nthe sidetracking? How will the actor deal with that? All these are beyond our scope, since\nworking with procedures we opt to mostly not deal with them. For example, instructions\nsuch as recipes rarely talk about what happens if things go wrong, or if you suddenly decide\nto cook another dish. They are locked in onto the task at hand. However, interferences and\ndistractions deﬁnitely can happen in procedures. For example, what would happen if you\ndon’t have certain ingredients? Your produce goes bad? Your house catch ﬁre (god forbid)?\nThis research question remains largely unexplored.\nThe key takeaway here is that procedures are a speciﬁc type of scripts, with signiﬁcantly\nlower complexity and variation. Scripts are challenging to learn; procedures are relatively\nsimpler.\n8\nRecipes\nJust like procedures are simpler scripts, recipes are simpler procedures. A strand of work on\nprocedures focuses exclusively on recipes [Tasse and Smith, 2008, Hamada et al., 2000, Mori\net al., 2014, Kiddon et al., 2015, Yamakata et al., 2016, Zhou et al., 2018, Bisk et al., 2019, Lin\net al., 2020, Donatelli et al., 2021], for good reasons. A special subset of instructions, recipes\n22\nare abundant and well-structured. They also uses regular, clear, and concise language. They\nhave a small scope of topics (i.e., food) and involved actions and entities. In fact, much\nprevious work on recipes touch on the same aspects as we have seen in this tutorial regarding\ngeneral procedures. An interesting question is to what extent the data, methods, and ﬁndings\nof these pieces of work apply to general procedures. In this section, we will examine several\nmore inﬂuential pieces of work.\n[Mori et al., 2014] tries to represent recipes with a ﬂow graph, comparable to what we\nhave seen for general procedures in Section 4. The authors of this work obtain recipes from\na recipe site, while one can of course obtain other instructions from sites such as wikiHow.\nThey represent recipes as a graph, where vertices are food, tools, or actions. Naturally, this\ncan be generalized to any entities and events in general procedures. The edges stand for\nrelations between entities, such as equality, subset, application, etc. None of those are really\nspeciﬁc to recipes, and can likely apply to any procedure.\n[Bisk et al., 2019] collects a dataset with hierarchical cooking actions with diﬀerent gran-\nularity. Concretely, the authors ask crowd worker to explain captions from cooking videos in\nsimple terms, resulting in lower-level sub-steps. The same methodology can be used for gen-\neral procedures using captions from datasets such as HowTo100M [Miech et al., 2019]. Then,\nthey propose a cloze task by having models predict some omitted actions. This naturally\napplies to general procedures too.\n[Lin et al., 2020] and [Donatelli et al., 2021] align related or equivalent actions between\ntwo dish recipes of the same dish. Similar to the cases above, the authors’ choice of working\non recipes instead of general procedures is motivated the existing datasets. Their method of\nalignment also only assumes linguistic features of instructional language, and not just recipes.\nWithout scrutinizing every single work on recipes, it does seem that most if not all of\nthese pieces of work can be easily generalized to general procedures, especially instructions\nof non-cooking tasks. However, whether this is the case still requires investigation. Recipes\nmay oversimplify things. For example, in a recipe there is only one person involved (i.e., the\nchef), while in other procedures there might be several (e.g., call the hotel representative to\nask for availability). Having multiple agents in a procedure might require knowledge about\ninteraction in diﬀerent scenarios. Also, most entities in recipes are either ingredients or tools,\nwhile in general procedures they might vary a lot more (e.g., locales, institutions, etc.). The\nﬂow and expressions in recipes are usually more straightforward, while in other procedures\nthere might be more complexity (e.g., conditionals, explanations, background information,\netc.). In summary, I believe that future work should attempt consider general procedures\nincluding but not limited to recipes, given the abundant existing web resources.\n9\nRecap\nIn this tutorial, we have gone through research on reasoning about procedures in the NLP\ncommunity.\nWe started by appreciating this trendy line of research and brieﬂy learning\nits history. Next, we saw that procedural data used to come from human annotation, but\nnow can also be obtained to-scale from web resources. We then looked at some eﬀorts to\nrepresent procedures. At the heart of each of these representations is granular knowledge\nregarding components of procedures. We also studied some work on learning such knowledge.\nThen, we examined some practical applications of procedures, with said knowledge equipped.\nFinally, we compare procedures with scripts, a superset of procedures, and recipes, a subset of\n23\nprocedures, both of which have also been receiving great attention from the NLP community.\nReferences\n[Addis and Borrajo, 2011] Addis, A. and Borrajo, D. (2011). From unstructured web knowledge to plan\ndescriptions. In Information Retrieval and Mining in Distributed Environments.\n[Ahn et al., 2022] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakr-\nishnan, K., Hausman, K., Herzog, A., et al. (2022). Do as i can, not as i say: Grounding language in\nrobotic aﬀordances. arXiv preprint arXiv:2204.01691.\n[Aouladomar, 2005] Aouladomar, F. (2005). A preliminary analysis of the discursive and rhetorical structure\nof procedural texts. In Symposium on the Exploration and Modeling of Meaning. Citeseer.\n[Artzi and Zettlemoyer, 2013] Artzi, Y. and Zettlemoyer, L. (2013). Weakly supervised learning of semantic\nparsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics,\n1:49–62.\n[Bielsa and Donnell, 2002] Bielsa, S. and Donnell, M. (2002). Semantic functions in instructional texts: A\ncomparison betw een english and spanish. In Proceedings of the 2nd International Contrastive Linguistics\nConference, pages 723–732.\n[Bisk et al., 2019] Bisk, Y., Buys, J., Pichotta, K., and Choi, Y. (2019). Benchmarking hierarchical script\nknowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages\n4077–4085, Minneapolis, Minnesota. Association for Computational Linguistics.\n[Bosselut et al., 2017] Bosselut, A., Levy, O., Holtzman, A., Ennis, C., Fox, D., and Choi, Y. (2017). Simu-\nlating action dynamics with neural process networks. CoRR, abs/1711.05313.\n[Branavan et al., 2009] Branavan, S., Chen, H., Zettlemoyer, L., and Barzilay, R. (2009). Reinforcement\nlearning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual\nMeeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the\nAFNLP, pages 82–90, Suntec, Singapore. Association for Computational Linguistics.\n[Brown et al., 2020a] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakan-\ntan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T.,\nChild, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,\nS., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020a).\nLanguage models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and\nLin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran\nAssociates, Inc.\n[Brown et al., 2020b] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakan-\ntan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020b). Language models are few-shot learners. Advances\nin neural information processing systems, 33:1877–1901.\n[Byrne et al., 2021] Byrne, B., Krishnamoorthi, K., Ganesh, S., and Kale, M. (2021). TicketTalk: Toward\nhuman-level performance with end-to-end, transaction-based dialog systems. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint Con-\nference on Natural Language Processing (Volume 1: Long Papers), pages 671–680, Online. Association for\nComputational Linguistics.\n[Chen and Mooney, 2011] Chen, D. L. and Mooney, R. J. (2011). Learning to interpret natural language nav-\nigation instructions from observations. In Proceedings of the Twenty-Fifth AAAI Conference on Artiﬁcial\nIntelligence, AAAI’11, page 859–865. AAAI Press.\n[Chen et al., 2021] Chen, M., Zhang, H., Ning, Q., Li, M., Ji, H., McKeown, K., and Roth, D. (2021). Event-\ncentric natural language processing. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language Processing:\nTutorial Abstracts, pages 6–14, Online. Association for Computational Linguistics.\n[Chen, 1976] Chen, P. P.-S. (1976). The entity-relationship model—toward a uniﬁed view of data. ACM\nTrans. Database Syst., 1(1):9–36.\n[Chernov et al., 2016] Chernov, A., Lagos, N., Gall´e, M., and S´andor, A. (2016). Enriching how-to guides\nby linking actionable phrases. In Proceedings of the 25th International Conference Companion on World\nWide Web, WWW ’16 Companion, page 939–944, Republic and Canton of Geneva, CHE. International\nWorld Wide Web Conferences Steering Committee.\n24\n[Dalvi et al., 2018] Dalvi, B., Huang, L., Tandon, N., Yih, W.-t., and Clark, P. (2018).\nTracking state\nchanges in procedural text: a challenge dataset and models for process paragraph comprehension. In\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1595–1604, New Orleans,\nLouisiana. Association for Computational Linguistics.\n[Dalvi et al., 2019] Dalvi, B., Tandon, N., Bosselut, A., Yih, W.-t., and Clark, P. (2019). Everything happens\nfor a reason: Discovering the purpose of actions in procedural text. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pages 4496–4505, Hong Kong, China. Association for\nComputational Linguistics.\n[de Rijke et al., 2005] de Rijke, M., Bunt, H., Geertzen, J., and Thijsse, E. (2005). Question answering:\nWhat’s next?\n[Delpech and Saint-Dizier, 2008] Delpech, E. and Saint-Dizier, P. (2008).\nInvestigating the structure of\nprocedural texts for answering how-to questions. In Proceedings of the Sixth International Conference\non Language Resources and Evaluation (LREC’08), Marrakech, Morocco. European Language Resources\nAssociation (ELRA).\n[Devlin et al., 2019] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of\ndeep bidirectional transformers for language understanding.\nIn Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n[Doddington et al., 2004] Doddington, G., Mitchell, A., Przybocki, M., Ramshaw, L., Strassel, S., and\nWeischedel, R. (2004). The automatic content extraction (ACE) program – tasks, data, and evaluation.\nIn Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04),\nLisbon, Portugal. European Language Resources Association (ELRA).\n[Donatelli et al., 2021] Donatelli, L., Schmidt, T., Biswas, D., K¨ohn, A., Zhai, F., and Koller, A. (2021).\nAligning actions across recipe graphs. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 6930–6942, Online and Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\n[Ellis et al., 2014] Ellis, J., Getman, J., and Strassel, S. M. (2014). Overview of linguistic resources for the\ntac kbp 2014 evaluations: Planning, execution, and results. In Proceedings of TAC KBP 2014 Workshop,\nNational Institute of Standards and Technology, pages 17–18.\n[Fritz and Gil, 2011] Fritz, C. and Gil, Y. (2011). A formal framework for combining natural instruction and\ndemonstration for end-user programming. In Proceedings of the 16th international conference on intelligent\nuser interfaces, pages 237–246.\n[Gil, 2015] Gil, Y. (2015). Human tutorial instruction in the raw. ACM Trans. Interact. Intell. Syst., 5(1).\n[Gil et al., 2011] Gil, Y., Ratnakar, V., and Frtiz, C. (2011). Tellme: Learning procedures from tutorial\ninstruction. In Proceedings of the 16th international conference on intelligent user interfaces, pages 227–\n236.\n[Hamada et al., 2000] Hamada, R., Ide, I., Sakai, S., and Tanaka, H. (2000). Structural analysis of cooking\npreparation steps in japanese.\nIn Proceedings of the Fifth International Workshop on on Information\nRetrieval with Asian Languages, IRAL ’00, page 157–164, New York, NY, USA. Association for Computing\nMachinery.\n[Han et al., 2019] Han, R., Ning, Q., and Peng, N. (2019). Joint event and temporal relation extraction with\nshared representations and structured prediction. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 434–444, Hong Kong, China. Association for Computational Lin-\nguistics.\n[Huang et al., 2021] Huang, J., Li, Z., Chen, B., Samel, K., Naik, M., Song, L., and Si, X. (2021). Scallop:\nFrom probabilistic deductive databases to scalable diﬀerentiable reasoning. In Ranzato, M., Beygelzimer,\nA., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information Processing\nSystems, volume 34, pages 25134–25145. Curran Associates, Inc.\n[Huang et al., 2022] Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. (2022). Language models as zero-\nshot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207.\n[Johnson et al., 2019] Johnson, J., Douze, M., and J´egou, H. (2019). Billion-scale similarity search with\nGPUs. IEEE Transactions on Big Data, 7(3):535–547.\n[Kiddon et al., 2015] Kiddon, C., Ponnuraj, G. T., Zettlemoyer, L., and Choi, Y. (2015). Mise en place:\nUnsupervised interpretation of instructional recipes. In Proceedings of the 2015 Conference on Empirical\n25\nMethods in Natural Language Processing, pages 982–992, Lisbon, Portugal. Association for Computational\nLinguistics.\n[Kosseim and Lapalme, 1994] Kosseim, L. and Lapalme, G. (1994). Content and rhetorical status selection in\ninstructional texts. In Proceedings of the Seventh International Workshop on Natural Language Generation.\n[Kosseim and Lapalme, 2000] Kosseim, L. and Lapalme, G. (2000). Choosing rhetorical structures to plan\ninstructional texts. Computational Intelligence, 16(3):408–445.\n[Koupaee and Wang, 2018] Koupaee, M. and Wang, W. Y. (2018). Wikihow: A large scale text summariza-\ntion dataset. CoRR, abs/1810.09305.\n[Ladhak et al., 2020] Ladhak, F., Durmus, E., Cardie, C., and McKeown, K. (2020). WikiLingua: A new\nbenchmark dataset for cross-lingual abstractive summarization. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pages 4034–4048, Online. Association for Computational Linguistics.\n[Lau et al., 2009] Lau, T. A., Drews, C., and Nichols, J. (2009). Interpreting written how-to instructions. In\nIJCAI, pages 1433–1438.\n[Li et al., 2012] Li, B., Lee-Urban, S., Appling, D. S., and Riedl, M. O. (2012). Crowdsourcing narrative\nintelligence. Advances in Cognitive systems, 2(1).\n[Lin et al., 2020] Lin, A., Rao, S., Celikyilmaz, A., Nouri, E., Brockett, C., Dey, D., and Dolan, B. (2020).\nA recipe for creating multimodal aligned datasets for sequential tasks. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational Linguistics, pages 4871–4884, Online. Association for\nComputational Linguistics.\n[Liu et al., 2019] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer,\nL., and Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692.\n[Long et al., 2016] Long, R., Pasupat, P., and Liang, P. (2016). Simpler context-dependent logical forms\nvia model projections. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1456–1465, Berlin, Germany. Association for Computational\nLinguistics.\n[Lyu et al., 2021] Lyu, Q., Zhang, L., and Callison-Burch, C. (2021).\nGoal-oriented script construction.\nIn Proceedings of the 14th International Conference on Natural Language Generation, pages 184–200,\nAberdeen, Scotland, UK. Association for Computational Linguistics.\n[Ma et al., 2021] Ma, M. D., Sun, J., Yang, M., Huang, K.-H., Wen, N., Singh, S., Han, R., and Peng, N.\n(2021). EventPlus: A temporal event understanding pipeline. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies:\nDemonstrations, pages 56–65, Online. Association for Computational Linguistics.\n[MacMahon et al., 2006] MacMahon, M., Stankiewicz, B., and Kuipers, B. (2006). Walk the talk: Connecting\nlanguage, knowledge, and action in route instructions. Def, 2(6):4.\n[Maeta et al., 2015] Maeta, H., Sasada, T., and Mori, S. (2015). A framework for procedural text understand-\ning. In Proceedings of the 14th International Conference on Parsing Technologies, pages 50–60, Bilbao,\nSpain. Association for Computational Linguistics.\n[Mellish and Evans, 1989] Mellish, C. and Evans, R. (1989). Natural language generation from plans. Com-\nputational Linguistics, 15(4):233–249.\n[Miech et al., 2019] Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev, I., and Sivic, J. (2019).\nHowto100m: Learning a text-video embedding by watching hundred million narrated video clips.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 2630–2640.\n[Miller, 1976] Miller, L. (1976). Natural language procedures: Guides for programming language design. In\nInternational Ergonomics Association Meeting, University of Maryland, volume 1, page 76.\n[Momouchi, 1980] Momouchi, Y. (1980). Control structures for actions in procedural texts and PT-chart.\nIn COLING 1980 Volume 1: The 8th International Conference on Computational Linguistics.\n[Mori et al., 2014] Mori, S., Maeta, H., Yamakata, Y., and Sasada, T. (2014).\nFlow graph corpus from\nrecipe texts. In Proceedings of the Ninth International Conference on Language Resources and Evaluation\n(LREC’14), pages 2370–2377, Reykjavik, Iceland. European Language Resources Association (ELRA).\n[Mujtaba and Mahapatra, 2019] Mujtaba, D. and Mahapatra, N. (2019). Recent trends in natural language\nunderstanding for procedural knowledge. In 2019 International Conference on Computational Science and\nComputational Intelligence (CSCI), pages 420–424.\n[Mysore et al., 2019] Mysore, S., Jensen, Z., Kim, E., Huang, K., Chang, H.-S., Strubell, E., Flanigan, J.,\nMcCallum, A., and Olivetti, E. (2019). The materials science procedural text corpus: Annotating materials\nsynthesis procedures with shallow semantic structures. In Proceedings of the 13th Linguistic Annotation\nWorkshop, pages 56–64, Florence, Italy. Association for Computational Linguistics.\n26\n[Nguyen et al., 2017] Nguyen, D. Q., Nguyen, D. Q., Chu, C. X., Thater, S., and Pinkal, M. (2017). Sequence\nto sequence learning for event prediction. In Proceedings of the Eighth International Joint Conference on\nNatural Language Processing (Volume 2: Short Papers), pages 37–42, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\n[Nighojkar and Licato, 2021] Nighojkar, A. and Licato, J. (2021).\nImproving paraphrase detection with\nthe adversarial paraphrasing task.\nIn Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7106–7116, Online. Association for Computational Linguistics.\n[Pareti, 2018] Pareti, P. (2018). Representation and execution of human know-how on the web.\n[Pareti et al., 2014a] Pareti, P., Klein, E., and Barker, A. (2014a). A semantic web of know-how: Linked data\nfor community-centric tasks. In Proceedings of the 23rd International Conference on World Wide Web,\nWWW ’14 Companion, page 1011–1016, New York, NY, USA. Association for Computing Machinery.\n[Pareti et al., 2014b] Pareti, P., Testu, B., Ichise, R., Klein, E., and Barker, A. (2014b). Integrating know-\nhow into the linked data cloud. In International Conference on Knowledge Engineering and Knowledge\nManagement, pages 385–396. Springer.\n[Paris et al., 2005] Paris, C., Colineau, N., Shijian, L., and Linden, K. V. (2005). Automatically generating\neﬀective online help.\n[Paris et al., 2002] Paris, C., Linden, K. V., and Lu, S. (2002). Automated knowledge acquisition for in-\nstructional text generation. In Proceedings of the 20th Annual International Conference on Computer\nDocumentation, SIGDOC ’02, page 142–151, New York, NY, USA. Association for Computing Machinery.\n[Paris et al., 1995] Paris, C., Vander Linden, K., Fischer, M., Hartley, A., Pemberton, L., Power, R., and\nScott, D. (1995). A support tool for writing multilingual instructions. In International Joint Conference\non Artiﬁcial Intelligence, volume 14, pages 1398–1404. LAWRENCE ERLBAUM ASSOCIATES LTD.\n[Park and Motahari Nezhad, 2018] Park, H. and Motahari Nezhad, H. R. (2018). Learning procedures from\ntext: Codifying how-to procedures in deep neural networks. In Companion Proceedings of the The Web\nConference 2018, pages 351–358.\n[Perkowitz et al., 2004] Perkowitz, M., Philipose, M., Fishkin, K., and Patterson, D. J. (2004).\nMining\nmodels of human activities from the web. In Proceedings of the 13th International Conference on World\nWide Web, WWW ’04, page 573–582, New York, NY, USA. Association for Computing Machinery.\n[Puig et al., 2018] Puig, X., Ra, K., Boben, M., Li, J., Wang, T., Fidler, S., and Torralba, A. (2018).\nVirtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 8494–8502.\n[Raﬀel et al., 2020] Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W.,\nand Liu, P. J. (2020). Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nJournal of Machine Learning Research, 21(140):1–67.\n[Rajagopal et al., 2020] Rajagopal, D., Tandon, N., Clark, P., Dalvi, B., and Hovy, E. (2020). What-if I ask\nyou to explain: Explaining the eﬀects of perturbations in procedural text. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages 3345–3355, Online. Association for Computational\nLinguistics.\n[Regneri et al., 2010] Regneri, M., Koller, A., and Pinkal, M. (2010). Learning script knowledge with web\nexperiments. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,\npages 979–988, Uppsala, Sweden. Association for Computational Linguistics.\n[Reimers and Gurevych, 2019] Reimers, N. and Gurevych, I. (2019). Sentence-BERT: Sentence embeddings\nusing Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 3982–3992, Hong Kong, China. Association for Computational Linguistics.\n[Sakaguchi et al., 2021] Sakaguchi, K., Bhagavatula, C., Le Bras, R., Tandon, N., Clark, P., and Choi, Y.\n(2021).\nproScript: Partially ordered scripts generation.\nIn Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 2138–2149, Punta Cana, Dominican Republic. Association for\nComputational Linguistics.\n[Schank, 1977] Schank, R. C. (1977).\nScripts, plans, goals, and understanding : an inquiry into human\nknowledge structures /. L. Erlbaum Associates ;, Hillsdale, N.J. :.\n[Shapiro and Iw´anska, 2000] Shapiro, S. C. and Iw´anska, L. M. (2000). Natural language processing and\nknowledge representation: language for knowledge and knowledge for language. Citeseer.\n[Takechi et al., 2003] Takechi, M., Tokunaga, T., Matsumoto, Y., and Tanaka, H. (2003). Feature selection\nin categorizing procedural expressions. In Proceedings of the Sixth International Workshop on Information\nRetrieval with Asian Languages, pages 49–56, Sapporo, Japan. Association for Computational Linguistics.\n27\n[Tandon et al., 2019] Tandon, N., Dalvi, B., Sakaguchi, K., Clark, P., and Bosselut, A. (2019). WIQA: A\ndataset for “what if...” reasoning over procedural text. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 6076–6085, Hong Kong, China. Association for Computational Lin-\nguistics.\n[Tandon et al., 2020] Tandon, N., Sakaguchi, K., Dalvi, B., Rajagopal, D., Clark, P., Guerquin, M., Richard-\nson, K., and Hovy, E. (2020). A dataset for tracking entities in open domain procedural text. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6408–6417,\nOnline. Association for Computational Linguistics.\n[Tasse and Smith, 2008] Tasse, D. and Smith, N. A. (2008). Sour cream: Toward semantic processing of\nrecipes. Carnegie Mellon University, Pittsburgh, Tech. Rep. CMU-LTI-08-005.\n[Vashishtha et al., 2020] Vashishtha, S., Poliak, A., Lal, Y. K., Van Durme, B., and White, A. S. (2020).\nTemporal reasoning in natural language inference.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 4070–4078, Online. Association for Computational Linguistics.\n[Wahlster et al., 1993] Wahlster, W., Andr´e, E., Finkler, W., Proﬁtlich, H.-J., and Rist, T. (1993). Plan-\nbased integration of natural language and graphics generation. Artiﬁcial intelligence, 63(1-2):387–427.\n[Wanzare et al., 2016] Wanzare, L. D. A., Zarcone, A., Thater, S., and Pinkal, M. (2016). A crowdsourced\ndatabase of event sequence descriptions for the acquisition of high-quality script knowledge. In Proceedings\nof the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 3494–\n3501, Portoroˇz, Slovenia. European Language Resources Association (ELRA).\n[Yamakata et al., 2016] Yamakata, Y., Imahori, S., Maeta, H., and Mori, S. (2016). A method for extracting\nmajor workﬂow composed of ingredients, tools, and actions from cooking procedural text. In 2016 IEEE\nInternational Conference on Multimedia Expo Workshops (ICMEW), pages 1–6.\n[Yang et al., 2021] Yang, Y., Panagopoulou, A., Lyu, Q., Zhang, L., Yatskar, M., and Callison-Burch, C.\n(2021). Visual goal-step inference using wikiHow. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 2167–2179, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\n[Zellers et al., 2019] Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019). HellaSwag: Can\na machine really ﬁnish your sentence? In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4791–4800, Florence, Italy. Association for Computational Linguistics.\n[Zhang et al., 2020a] Zhang, H., Chen, M., Wang, H., Song, Y., and Roth, D. (2020a). Analogous process\nstructure induction for sub-event sequence prediction. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 1541–1550, Online. Association for Computa-\ntional Linguistics.\n[Zhang et al., 2020b] Zhang, L., Lyu, Q., and Callison-Burch, C. (2020b). Reasoning about goals, steps, and\ntemporal ordering with WikiHow. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4630–4639, Online. Association for Computational Linguistics.\n[Zhang et al., 2019] Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. (2019). Bertscore:\nEvaluating text generation with bert. arXiv preprint arXiv:1904.09675.\n[Zhang et al., 2012] Zhang, Z., Webster, P., Uren, V., Varga, A., and Ciravegna, F. (2012). Automatically\nextracting procedural knowledge from instructional texts using natural language processing. In Proceedings\nof the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 520–527,\nIstanbul, Turkey. European Language Resources Association (ELRA).\n[Zhou et al., 2019a] Zhou, B., Khashabi, D., Ning, Q., and Roth, D. (2019a). “going on a vacation” takes\nlonger than “going for a walk”: A study of temporal commonsense understanding. In Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pages 3363–3369, Hong Kong, China.\nAssociation for Computational Linguistics.\n[Zhou et al., 2018] Zhou, L., Xu, C., and Corso, J. J. (2018). Towards automatic learning of procedures from\nweb instructional videos. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.\n[Zhou et al., 2022] Zhou, S., Zhang, L., Yang, Y., Lyu, Q., Yin, P., Callison-Burch, C., and Neubig, G.\n(2022). Show me more details: Discovering hierarchies of procedures from semi-structured web data. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), Dublin, Ireland. Association for Computational Linguistics.\n[Zhou et al., 2019b] Zhou, Y., Shah, J., and Schockaert, S. (2019b). Learning household task knowledge from\nWikiHow descriptions. In Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5), pages\n50–56, Macau, China. Association for Computational Linguistics.\n28\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-05-16",
  "updated": "2022-05-16"
}