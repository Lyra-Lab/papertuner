{
  "id": "http://arxiv.org/abs/2205.07634v1",
  "title": "A Precis of Language Models are not Models of Language",
  "authors": [
    "Csaba Veres"
  ],
  "abstract": "Natural Language Processing is one of the leading application areas in the\ncurrent resurgence of Artificial Intelligence, spearheaded by Artificial Neural\nNetworks. We show that despite their many successes at performing linguistic\ntasks, Large Neural Language Models are ill-suited as comprehensive models of\nnatural language. The wider implication is that, in spite of the often\noverbearing optimism about AI, modern neural models do not represent a\nrevolution in our understanding of cognition.",
  "text": "arXiv:2205.07634v1  [cs.CL]  16 May 2022\nA Pr´ecis of Language Models are not Models\nof Language\nCsaba Veres\nDepartment of Information Science and Media Studies,\nUniversity of Bergen, Bergen, Norway.\nCorresponding author(s). E-mail(s): csaba.veres@uib.no;\nNatural Language Processing is one of the leading application areas\nin the current resurgence of Artiﬁcial Intelligence, spearheaded\nby Artiﬁcial Neural Networks. We show that despite their many\nsuccesses at performing linguistic tasks, Large Neural Language\nModels are ill suited as comprehensive models of natural language.\nThe wider implication is that, in spite of the often overbearing\noptimism about ”AI”, modern neural models do not represent a\nrevolution in our understanding of cognition.\nHigh level programming languages for digital computers, and theories of\nnatural language have a curious historical connection. John W. Backus who\nled the Applied Science Division of IBM’s Programming Research Group1\ntook inspiration from Noam Chomsky’s work on phrase structure grammars\nand conceived a meta-language that could specify the syntax of computer\nlanguages that were easier for programmers to write than assembler languages.\nThe meta language later became known as Backus-Naur form (BNF), so called\npartly because it was originally co-developed by Peter Naur in a 1963 IBM\nreport on the ALGOL 60 programming language”2. The BNF is a notation for\ncontext free grammars consisting of productions over terminal and nonterminal\nsymbols, which deﬁnes the grammar of programming languages required for\nwriting compilers and interpreters [1].\n1https://betanews.com/2007/03/20/john-w-backus-1924-2007/\n2https://www.masswerk.at/algol60/report.htm\n1\n2\nLanguage Models are not Models of Language\nNatural language is of course diﬀerent from programming languages in\nmany ways, not the least of which is that the grammar of programming lan-\nguages is perfectly known, whereas the role of generative grammar in natural\nlanguage is merely a hypothesis. Chomsky characterised Language as a set of\nsentences (potentially inﬁnite) constructed out of a ﬁnite set of elements fol-\nlowing the rules of a grammar. The role of Linguistics as a science, then, is\nto discover grammars that are able to distinguish legal productions which are\npart of the Language from ill formed strings that are not [2]. When a string\nof words is deemed unacceptable by a native speaker then this is the result,\nby hypothesis, of a violation of grammatical constraints. Similarly, the set of\nwritten statements in programming languages are productions of the gram-\nmar deﬁned for the language. When a programmer writes code which does not\ncompile or execute, then it is likely that they have violated the grammar and\nthe compiler is unable to parse the code [1].\nThe claim that grammar has a central role in Natural Language has\nbeen questioned as a result of the success of Transformer based neural Lan-\nguage Models (LMs) [3], which have acquired signiﬁcant competence in various\nnatural language tasks, including judgement of grammatical acceptability [4].\nNeural LMs are based on traditional statistical n-gram language models\nwhich are joint probability distributions over sequences of words, or alterna-\ntively, functions that return a probability measure over strings drawn from\nsome vocabulary [5]. More informally, language models can refer to ”any sys-\ntem trained only on the task of string prediction” [6] (p. 5185). Large neural\nLMs advance n-gram models by learning probability functions for sequences\nof real valued, continuous vector representations of words rather than the\ndiscrete words themselves. Continuous representations are eﬀective at gener-\nalising across novel contexts, resulting in better performance across a range\nof tasks [7]. Manning [8] describes several ways in which Deep Learning mod-\nels can challenge traditional grammar based approaches in the theoretical\nunderstanding of Language.\nBengio et. al. [9] went further in arguing that continuous representations in\nDeep Learning models fundamentally diﬀerentiate neural LMs from traditional\nsymbolic systems such as grammar because they enable computations based\non non-linear transformations between the representing vectors themselves.\nAs an example, ”If Tuesday and Thursday are represented by very similar\nvectors, they will have very similar causal eﬀects on other vectors of neural\nactivity.” [9] (p.59). In a Classical symbolic system there is no inherent similar-\nity between the two symbols ”Tuesday” and ”Thursday”, and ”similar causal\neﬀects” must be prescribed by explicit axioms (see [10] for a deep dicussion on\nthe fundamental diﬀerences between symbolic and distributed architectures.).\nLarge neural LMs are therefore a fundamental challenge to rule based theories\nbecause they obviate the need for explicit rules.\nPinker and Prince [11] designated neural approaches which eschew tradi-\ntional rules as eliminative connectionism. In eliminative (neural) systems it\nis impossible to ﬁnd a principled mapping between the components of the\nLATEX\nLanguage Models are not Models of Language\n3\ndistributed (vector) processing model and the steps involved in a symbol-\nprocessing theory. Note that neural systems are not necessarily eliminative.\nImplementational connectionism is a class of systems where the computations\ncarried out by collections of neurons are isomorphic to the structures and\nsymbol manipulations of a symbolic system. For example, recurrent neural\nnetworks with long short-term memory have been shown to learn very simple\ncontext free and context sensitive languages. Thus, the language with sentences\nof the form anbn can be learned with gate units acting as counters that can\nkeep track of the number of terminal strings in simple sequences [12]. Crucially,\nan implementational system could be fully compatible with a symbol based\ngrammatical theory, and a network architecture that can induce grammati-\ncal principles would have minimal impact on our understanding of language.\nPinker and Prince argued that language is a ”crucial test case” for eliminative\nconnectionism because so much of our understanding of language is bound up\nin the symbolic paradigm. In this commentary we argue that neural models of\nprogramming languages can provide an even more crucial test since we know\nthat computer code is governed completely by the symbolic paradigm.\nDeep Learning neural networks have been shown to generate computer\ncode. For example OpenAI Codex3, an experimental API which powers GitHub\nCopilot4 and based on the GPT-3 language model ﬁne tuned on publicly avail-\nable computer code, can generate Python code from short textual docstrings\n[13]. Listing 1 shows a simple example problem from the evaluation set (includ-\ning function signature, docstring, body, and several unit tests) together with\na correct and an incorrect solution for the (simple) function is prime.\n1\ndef is_prime (n):\n2\n\"\"\" Return\ntrue if a given\nnumber\nis prime , and\n3\nfalse\notherwise.\n4\n>>> is_prime (6) False\n5\n>>> is_prime (101)\nTrue\n6\n>>> is_prime (11) True\n7\n>>> is_prime (13441)\nTrue\n8\n>>> is_prime (61) True\n9\n>>> is_prime (4) False\n10\n>>> is_prime (1) False\n11\n\"\"\"\n12\n###\nCOMPLETION 6 (CORRECT ): ###\n13\nprime = True\n14\nif n == 1:\n15\nreturn\nFalse\n16\nfor i in range (2, n):\n17\nif n % i == 0:\n18\nprime = False\n19\nreturn\nprime\n20\n###\nCOMPLETION 7 (WRONG ): ###\n21\nfor i in range (2, n):\n22\nif n % i == 0:\n23\nreturn\nFalse\n24\nreturn\nTrue\nListing 1: Example problem speciﬁcation with a correct and an incorrect\ncompletion\n3https://openai.com/blog/openai-codex/\n4https://copilot.github.com/\n4\nLanguage Models are not Models of Language\nCodex generated functionally correct code on up to 60% of the problems.\nHowever, the solutions were syntactically correct in over 90% of cases, showing\nthat errors were predominantly semantic [13]. Often the proposed solution\nsolved only a part of the problem as in this example, where the incorrect\nalgorithm fails to consider the boundary condition where n = 1.\nAustin et al. [14] constructed a slightly more diﬃcult dataset, the Mostly\nBasic Programming Problems (MBPP) which were used to test BERT-style\ntransformer models [3] with parameter counts ranging from 244 million to\n137 billion. The smallest models produced syntactically correct Python code\napproximately 80% of the time, increasing to over 90% for the larger models.\nLMs wich produce computer code bring into sharp focus the nature of\nexplanation in neural models. In order to generate code, one possibility is that\nnetworks learn the grammar of the language(s) they are exposed to. There is\nsome support for this in evidence of syntactic information in natural language\nword representations [15]. However this evidence is far short of an argument\nthat language rules are learned. More importantly, even if this were eventually\nshown to be the case, the conclusion would be that LMs are implementational\nafter all, and their theoretical interest would focus on their ability to learn rules\nwithout explicit instruction. Such models can not provide more insight into the\nnatural phenomena than we already have. In the case of computer languages\nthey provide no principled reason for why some strings are syntactically valid\nand some are not. In reality this is determined entirely by the grammar.\nThe second possibility is that LMs are simply learning sophisticated sta-\ntistical properties of their training data and extrapolate based on the learned\nmodel [16]. On this view the success of LM architectures in generating com-\nputer code shows just how well they are able to extrapolate, being able to\nmimic the productions of a formal system without knowledge of its rules. In\nthe absence of arguments to the contrary there is no reason to think that their\nperformance on natural language tasks is any diﬀerent. That is, large language\nmodels are simply extrapolating from their training data and have nothing to\nsay about the claim that natural language is governed by a grammar.\nPinker and Prince argued that the connectionist models of the time failed\nto deliver a ”radical restructuring of cognitive theory” ([11], p.78) because\nthey did not adequately model the relevant linguistic phenomena. We argue\nthat modern neural models similarly fail, but from the opposite perspective.\nIn becoming universal mimics that can imitate the behaviour of clearly rule\ndriven processes, they become uninformative about the true nature of the phe-\nnomena they are ”parroting” [17]. Enormous amounts of training data and\nadvances in compute power have made the modern incarnation of artiﬁcial\nneural networks tremendously capable in solving certain problems that pre-\nviously required human-like intelligence, but just like their predecessors, they\nhave failed to deliver a revolution in our understanding of human cognition.\nLATEX\nLanguage Models are not Models of Language\n5\nReferences\n[1] Aho, A.V., Lam, M.S., Sethi, R., Ullman, J.D.: Compilers: Princi-\nples, Techniques, and Tools (2nd Edition). Addison-Wesley Longman\nPublishing Co., Inc., USA (2006)\n[2] Chomsky, N.: Syntactic Structures. Mouton & Co., The Hague (1957)\n[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA.N., Kaiser, L., Polosukhin, I.: Attention Is All You Need (2017)\n[4] Warstadt, A., Singh, A., Bowman, S.R.: Neural network acceptability\njudgments. arXiv preprint arXiv:1805.12471 (2018)\n[5] Manning, C.D., Raghavan, P., Sch¨utze, H.: Introduction to Informa-\ntion Retrieval. Cambridge University Press, Cambridge, UK (2008).\nhttp://nlp.stanford.edu/IR-book/information-retrieval-book.html\n[6] Bender, E.M., Koller, A.: Climbing towards NLU: On Meaning, Form, and\nUnderstanding in the Age of Data. Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, 5185–5198 (2020).\nhttps://doi.org/10.18653/v1/2020.acl-main.463\n[7] Bengio, Y., Ducharme, R., Vincent, P., Jauvin, C.: A Neural Probabilistic\nLanguage Model. Journal of Machine Learning Research 3, 1137–1155\n(2003)\n[8] Manning, C.D.: Computational Linguistics and Deep Learning. Compu-\ntational Linguistics 41(4), 701–707 (2015). https://doi.org/10.1162/coli\na 00239\n[9] Bengio, Y., Lecun, Y., Hinton, G.: Deep learning for AI. Communications\nof the ACM 64(7), 58–65 (2021). https://doi.org/10.1145/3448250\n[10] Fodor,\nJ.A.,\nPylyshyn, Z.W.:\nConnectionism\nand\ncognitive\narchi-\ntecture:\nA\ncritical\nanalysis.\nCognition\n28(1-2),\n3–71\n(1988).\nhttps://doi.org/10.1016/0010-0277(88)90031-5\n[11] Pinker, S., Prince, A.: On language and connectionism: Analysis of a\nparallel distributed processing model of language acquisition. Cognition\n28(1-2), 73–193 (1988). https://doi.org/10.1016/0010-0277(88)90032-7\n[12] Gers,\nF.A.,\nSchmidhuber,\nE.:\nLstm\nrecurrent\nnetworks\nlearn\nsimple\ncontext-free\nand\ncontext-sensitive\nlanguages.\nIEEE\nTransactions\non\nNeural\nNetworks\n12(6),\n1333–1340\n(2001).\nhttps://doi.org/10.1109/72.963769\n[13] Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H.P., Kaplan,\n6\nLanguage Models are not Models of Language\nJ., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri,\nR., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan,\nB., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M.,\nWinter, C., Tillet, P., Such, F.P., Cummings, D., Plappert, M., Chantzis,\nF., Barnes, E., Herbert-Voss, A., Guss, W.H., Nichol, A., Paino, A., Tezak,\nN., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C.,\nCarr, A.N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A.,\nKnight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew,\nB., Amodei, D., McCandlish, S., Sutskever, I., Zaremba, W.: Evaluating\nlarge language models trained on code (2021) arXiv:2107.03374 [cs.LG]\n[14] Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D.,\nJiang, E., Cai, C., Terry, M., Le, Q., Sutton, C.: Program Synthesis with\nLarge Language Models. arXiv (2021) 2108.07732\n[15] Hewitt, J., Manning, C.D.: A structural probe for ﬁnding syntax in\nword representations. In: Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pp. 4129–4138. Association for Computational Linguistics,\nMinneapolis, Minnesota (2019). https://doi.org/10.18653/v1/N19-1419.\nhttps://aclanthology.org/N19-1419\n[16] Balestriero,\nR.,\nPesenti,\nJ.,\nLeCun,\nY.:\nLearning\nin\nHigh\nDimension\nAlways\nAmounts\nto\nExtrapolation.\narXiv\n(2021).\nhttps://doi.org/10.48550/ARXIV.2110.09485.\nhttps://arxiv.org/abs/2110.09485\n[17] Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dan-\ngers of stochastic parrots: Can language models be too big? In: Proceed-\nings of the 2021 ACM Conference on Fairness, Accountability, and Trans-\nparency. FAccT ’21, pp. 610–623. Association for Computing Machinery,\nNew York, NY, USA (2021). https://doi.org/10.1145/3442188.3445922.\nhttps://doi.org/10.1145/3442188.3445922\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-05-16",
  "updated": "2022-05-16"
}