{
  "id": "http://arxiv.org/abs/2205.07811v1",
  "title": "Natural Language Specifications in Proof Assistants",
  "authors": [
    "Colin S. Gordon",
    "Sergey Matskevich"
  ],
  "abstract": "Interactive proof assistants are computer programs carefully constructed to\ncheck a human-designed proof of a mathematical claim with high confidence in\nthe implementation. However, this only validates truth of a formal claim, which\nmay have been mistranslated from a claim made in natural language. This is\nespecially problematic when using proof assistants to formally verify the\ncorrectness of software with respect to a natural language specification. The\ntranslation from informal to formal remains a challenging, time-consuming\nprocess that is difficult to audit for correctness. This paper argues that it\nis possible to build support for natural language specifications within\nexisting proof assistants, in a way that complements the principles used to\nestablish trust and auditability in proof assistants themselves.",
  "text": "Natural Language Speciﬁcations in Proof\nAssistants\nColin S. Gordon ! Ï \u0012\nDrexel University, USA\nSergey Matskevich !\nDrexel University, USA\nAbstract\nInteractive proof assistants are computer programs carefully constructed to check a human-designed\nproof of a mathematical claim with high conﬁdence in the implementation. However, this only\nvalidates truth of a formal claim, which may have been mistranslated from a claim made in natural\nlanguage. This is especially problematic when using proof assistants to formally verify the correctness\nof software with respect to a natural language speciﬁcation. The translation from informal to formal\nremains a challenging, time-consuming process that is diﬃcult to audit for correctness. This paper\nargues that it is possible to build support for natural language speciﬁcations within existing proof\nassistants, in a way that complements the principles used to establish trust and auditability in proof\nassistants themselves.\n2012 ACM Subject Classiﬁcation Software and its engineering →Speciﬁcation languages\nKeywords and phrases Natural language, formal speciﬁcation, categorial grammar, computational\nlinguistics\n1\nIntroduction\nProof assistants can establish very high conﬁdence in the correctness of formal proofs,\nboth because of their rigorous checking and attention to producing independently auditable\nevidence that the arument is correct [73, 76]. But one of the unavoidable points of trust for\neven a carefully-implemented proof assistant is the speciﬁcations themselves: proving the\nwrong theorem is of limited use. And only those who can read both formal and informal\nspeciﬁcations can even consider whether this has occurred. This is particularly crucial\nfor software veriﬁcation: software speciﬁcations typically originate in natural language,\nand any accompanying formal speciﬁcation comes afterwards — which increasingly occurs\nfor compilers [54], operating systems [45], and other high-value software. Currently, the\nonly bridge between the formal and informal speciﬁcations is the humans who perform the\ntranslation. There is no independently checkable record of this translation aside from the\npossibility of comments or notes by the translators — themselves largely in informal (though\nlikely careful) natural language. And simply being familiar with both the speciﬁcation\nlanguage and the intended speciﬁcation is insuﬃcient by itself to bridge this gap [26, 25]:\nrelating the two is a separate skill that is independently challenging to develop.\nIdeally, it would be possible to give natural language speciﬁcations directly to the proof\nassistant, for example:\nGoal \"addone is monotone \".\nRobust support for such speciﬁcations could enable signiﬁcant improvements in requirements\ntracing (machine checked mappings from natural language to formal results), including\nfor artifact evaluation; education, where it could help students check their understanding\nof how either mathematics or program speciﬁcations are formalized logically; and even\ncommunication with non-technical clients who might wish to have some conﬁdence that a\nformalization they do not themselves fully understand is correct. On the last point, Wing’s\narXiv:2205.07811v1  [cs.PL]  16 May 2022\n2\nNatural Language Speciﬁcations in Proof Assistants\nclassic paper introducing formal methods [95] posits that customers may read the formal\nspeciﬁcations produced from informal requirements, but this is only possible if the client can\nmake sense of the formal speciﬁcation itself. Machine-checked relationships between natural-\nlanguage and formalized expressions of software properties can help bridge this knowledge\ngap by connecting formal properties to natural language a reader with less background in a\nspeciﬁc formal logic could understand.\nBy the end of this paper, we will have developed a way to accept the very similar\nGoal spec \"addone is monotone \".\n(where spec returns the logical form of the natural language utterance in quotes). We envision\nsuch a system can eventually be used for the purposes above, to generate formal claims about\nmathematics or programs veriﬁed in a proof assistant, whether speciﬁed in a proof assistant’s\nown logic, or indirectly through a foundational program logic [4] built inside a proof assistant.\nWhile these generated speciﬁcations may not match the expert human formalizer’s choices\n(often inﬂuenced by setting up deﬁnitions in a particular way to simplify proofs), they should\nbe implied by the speciﬁcation used to conduct the primary veriﬁcation activities, oﬀering\nan additional, optional level of assurance.\nThis is not a job for machine-learning-centric natural language processing, which is\nincompatible with the goals of using a proof assistant for formal veriﬁcation. inappropriate\nfor foundational veriﬁcation. There is no guarantee a learned translation is sensible, and if a\ntranslation from natural to formal language ends up being surprising, few machine learning\napproaches produce an auditable trail of evidence for why that translation is believed correct\n(in the eyes of the trained model), and no way to precisely ﬁx misunderstandings of speciﬁc\nwords. Meanwhile, proof certiﬁcates play a central role in the design of trustworthy proof\nassistants [76, 31] and foundational program veriﬁcation [4]. Moreover, as proof assistants\nare often used to formalize properties of new mathematics or new programs, often using new\nterminology, there will often be a lack of training data for mapping natural language to a\nformal property. Later, we point out additional ways that the needs of trusted formalization\nof natural language speciﬁcations run afoul of many of machine learning’s known limitations,\nwhile requiring few of its advantages. (We also point out limited ways machine learning can\nplay a role in optimizing the techniques we employ.)\nFortunately the ﬁeld of linguistics predates machine learning. Formalizing categorial\ngrammar [2, 7, 52, 87] carefully in a proof assistant oﬀers a path to a natural, auditable\nway to bridge the gap between formal and natural-language speciﬁcation. In this paper\nwe show a prototype demonstrating that it is possible parse a string containing a natural\nlanguage speciﬁcation into a semantic representation that can be used directly in proofs\nwithin a proof assistant (i.e., a proposition in Coq’s logic) in a principled way using Coq’s\ntypeclasses [86], and argue that this approach is modular and can extend to sophisticated\nnatural language. A partial adaptation to the Lean theorem prover suggests that with Lean’s\nalgorithmic improvements [85] this can be made eﬃcient enough for interactive use. We\nalso analyze how the trusted computing base is aﬀected when considering trust of a formal\nveriﬁcation up to the natural language speciﬁcation.\nThis paper establishes that the theoretical core is readily within reach. But the robust\nrealization of these ideas posited above will require signiﬁcant eﬀorts to develop a broad\ncore vocabulary to act as a starting point for extensions (we outline how such extensions\nwould work) and eventually a robust library of domain-speciﬁc language support. It will\nrequire time and collective eﬀort to collect precise natural language descriptions of partial\nspeciﬁcations to guide development and evaluation of such a system. And it raises potential\nopportunities for fruitful collaboration with linguists on building systems that simultaneously\nC. S. Gordon and S. Matskevich\n3\nbeneﬁt consumers of proof assistants while oﬀering a live proving ground for work on semantic\nparsing.\n2\nBackground & Motivation\nThis section provides a condensed (and therefore somewhat biased) background in natural\nlanguage processing and categorial grammar, from a programming languages point of view.\nCategorial grammar is a body of work concerned with the use of techniques and ideas\nfrom logic to relate the syntax of natural language with the meaning of language in a\nway independent of syntax. The core idea is to build a sort of type theory where base\ntypes correspond to grammatical categories (hence categorial), from which more complex\ngrammatical categories can be deﬁned. A set of inference rules is then used to deﬁne,\nsimultaneously, how grammatical categories combine into larger sentence fragments and\nhow those smaller fragments’ meanings (logical forms) are combined into larger meanings.\nThis process bottoms-out at a lexicon, giving for each word its grammatical roles (types)\nand associated denotations. Thus categorial grammar is a system of simulatenously parsing\nnatural language from strings and assigning denotational semantics — a process traditionally\nreferred to as semantic parsing in the computational linguistics literature.\nMost prominent in the linguistics community are combinatory categorial grammars\n(CCGs) [87], though also relevant to our goals are the categorial type logics (CTLs1). Work\non CCGs epmhasizes appropriate constructs for linguistic ends, while CTLs hew close to\nLambek’s view [52] of categorial grammars as substructural logics for linguistics. While\nthese reﬂect very diﬀerent philosophical and practical aims, for our present purposes the\ndistinction is immaterial: it is widely held, and in some cases formalized [47, 30], that rules\nused in CCGs (including the variant with the most sophisticated linguistic treatments [6])\ncorrespond to theorems in particular CTLs [63]. In this work we use only principles common\nto both CCGs and CTLs.\nAll categorial grammars parse by combining sentence fragments based on their grammatical\ntypes. These types include both atomic primitives (such as noun phrases) as well as more\ncomplex types, namely so-called slash-types that indicate a predicate argument structure\n(which are used to model, for example, most classes of verbs). Oversimplifying slightly,\ncategorial grammars treat parsing as logical deduction in a residuated non-commutative\nlinear logic.2 This is essentially a family of linear logics without the structural rule for freely\ncommuting the order of assumptions, thus modeling sensitivity to word order, and picking up\nas a consequence two forms of implication corresponding to whether an implication expects\nits argument to the left or to the right.3 The model for the logic is a sequence of words, and\ntypes correspond to the grammatical role of a sentence fragment.\nA/B (A over B) is the grammatical type for a fragment that, when given a B to its right,\nforms an A. A\\B (A under B) is the grammatical type for a fragment that, when given an\nA to its left, forms a B. In both cases, the argument is “under” the slash, and the result is\n“above” it.4) These are called slash types. The grammars include rules to combine adjacent\n1 Occasionally also called type-logical grammars (TLGs).\n2 Technically only CTLs [67] take this as an epistemilogical commitment, while CCGs [87]) are agnostic,\ninheriting such a relation via Baldridge and Kruijﬀ’s work [6].\n3 It is the presence of the ability to commute assumptions arbitrarily that allows a single implication to\nsuﬃce in standard logics.\n4 We follow CTL notation rather than CCG notation (which always puts results to the left) as users of\nproof assistants tend to be familiar with a range of logics, so the CCG syntax would likely confuse users\n4\nNatural Language Speciﬁcations in Proof Assistants\n\\-Elim\nΓ ⊢A ⇒a\n∆⊢A\\B ⇒f\nΓ, ∆⊢B ⇒(f a)\n/-Elim\nΓ ⊢A/B ⇒f\n∆⊢B ⇒a\nΓ, ∆⊢A ⇒(f a)\n/-Comp\nΓ ⊢A/B ⇒e\n∆⊢B/C ⇒f\nΓ, ∆⊢A/C ⇒(e ◦f)\n\\-Comp\nΓ ⊢A\\B ⇒e\n∆⊢B\\C ⇒f\nΓ, ∆⊢A\\C ⇒(f ◦e)\nReassoc\n(Γ, ∆), Υ ⊢A ⇒e\nΓ, (∆, Υ) ⊢A ⇒e\nShift\nΓ ⊢A\\(B/C) ⇒f\nΓ ⊢(A\\B)/C ⇒(λr, l. f l r)\nFigure 1 A selection of rules used in this paper, all derivable in CTLs and CCGs.\nparts of a sentence. The elimination rules are the ﬁrst two in Figure 1. The judgment\nΓ ⊢C ⇒e is read as claiming the sequence of words Γ can be combined to form a sentence\nfragment of grammatical type C, whose underlying semantic form — logical form — is given\nby e. e is a term drawn from the logical language being used to represent sentence meaning,\ntypically a simply-typed lambda calculus in keeping with Montague [60, 59, 61, 71], though in\nour work we follow the alternative approach [11, 89, 17, 79] of targeting a dependently-typed\ncalculus.\nA lexicon gives the grammatical role and semantics for individual words, providing the\nstarting point for combining fragments. Categorial grammars push all knowledge speciﬁc to\na particular human language into the lexicon, in categorizing how individual words are used.\nThis allows the core principles to be reused across languages, which has been put to use in\nbuilding wide-coverage lexicons for a variety of natural languages [34, 32, 3, 1, 65].\nTogether, these allow ﬁlling in choices for the metavariables in the rules above, which\ntogether permit derivations like\n“four” ⊢NP ⇒4\n“is” ⊢(NP\\S)/ADJ ⇒λp. λx. p x “even” ⊢ADJ ⇒even\n“is even” ⊢NP\\S ⇒(λx. even x)\n/-Elim\n“four is even” ⊢S ⇒even 4\n\\-Elim\nEach grammatical type C corresponds to a particular type in the underlying lambda calculus\nand the underlying semantic type is determined by a systematic translation from the syntactic\ngrammatical type. Borrowing more notation from logic (where this idea is known as a Tarski-\nstyle universe [56]), we write ⌊C⌋for C’s semantic type. Both slash types correspond to\nfunction types in the lambda calculus: ⌊A/B⌋= ⌊B\\A⌋= B →A. An invariant of the\njudgment Γ ⊢C ⇒e is that in the underlying logic, e always has type ⌊C⌋. This invariant\nexplains why it is correct for “four” to have semantics 4 while “is even” has a function as its\nsemantics. In proof assistants based on type theory, like Coq, the set of grammatical types\ncan be given as a datatype declaration, and the interpretation function as a function from\ngrammatical types to proof assistant types.\nFigure 1 includes a selection of additional rules, each of which is either an axiom or\nderived rule in CCGs, and a theorem in the Lambek calculus. Thus we do not commit to\none approach over the other at this time.\nalready familiar with the Lambek calculus and related systems. This notational choice is orthogonal to\nthe choice of which rules to employ.\nC. S. Gordon and S. Matskevich\n5\nThese few rules are enough to formalize a small fragment of English, and demonstrate the\npossibility of interpreting natural language speciﬁcations within a proof assistant, in a well-\nfounded and extensible way. Our initial choice of rules is limited, but not fundamentally so.\nCCGs recognize mildly context-sensitive languages [39, 48], which are believed to cover the\nfull range of grammatical constructions in any natural language [88]. All rules of CCGs are\nencodable in the way we describe, as are the rules of Turing-complete CTLs [15]. Ultimately\nthe question of which rules are required is an empirical one, considering both linguistic\nconstructions and the complexity of recognizing these grammars using typeclass search. For\nnow we are adding additional CCG rules as needed.\n3\nExploring Categorial Grammars for Coq Speciﬁcations\nThis section describes a model of a very small fragment of English for describing simple\nmathematics. Our goal is not to present a polished and complete natural language fragment\nsuitable for a wide range of speciﬁcations. Producing such a result is a very long-term goal,\nelaborated in Section 4. The purpose of this section then is to emphasize:\n1. that existing work on linguistic semantics covers many of the grammatical aspects of\nthe language constructions we use when speaking or writing about formal claims, and is\nsuﬃciently ﬂexible to admit extensions for grammar unique to mathematical prose;\n2. that existing proof assistants (by example Coq, but by association similar systems like\nLean) oﬀer an environment ready to implement linguistic semantics in a way directly\nintegrated into the use of speciﬁcations in proof assistants; and\n3. extensions of this idea are well worth investigating.\nWe propose using Coq’s typeclass support [86] to perform semantic parsing from natural\nlanguage5 to Coq’s speciﬁcation logic, or alternatively (see Section 4.4) embedded logics.\nThis approach naturally supports an open-ended lexicon, which is essential to modularly\nextending the words handled by semantic parsing (Section 4). Moreover, it has the advantage\nover external tools that it works within existing proof assistants today, with no need to\ninvolve an external toolchain for the translation. This ensures the extended lexicon can grow\nin tandem with a formal development, with organization chosen by developers rather than\ndictated by an external tool, and with Coq automatically checking validity of the lexicon\nextensions at the same time it checks validity of the rest of the formalization.\nMost work on categorial grammars lumps all entities — frogs, people, books — into a\nsingle semantic type. But for logical forms to talk about entities in Coq’s logic — which\ndistinguishes natural numbers, rings, monoids, and so on with diﬀerent types — adjustments\nmust be made. Noun phrases and related syntactic categories must be parameterized by the\nsemantic (Coq) type of entity they concern. Adapting categorial grammar to refer to objects\nin Coq’s logic — a dependently-typed lambda calculus known as the Calculus of Inductive\nConstructions (CIC) [72] requires making the base grammatical categories multi-sorted,\ndistinguishing nouns with diﬀerent semantic types. Traditional categorial grammars assume\nthe logic used for logical forms has a sort E for “entities”, which diﬀerent kinds distinguished\nby predicates: e.g., three : E and Mary : E, but number(Mary) = false. This approach is\nrooted in the assumption that ﬁrst order logic is an appropriate semantic model for sentence\nmeaning. That assumption is plausible for many general circumstances, and useful for\n5 In this paper, English, but in principle any other natural language with thorough treatments in categorial\ngrammar [34, 33, 32, 1, 3, 65, 58].\n6\nNatural Language Speciﬁcations in Proof Assistants\nInductive\nCat : Type :=\n| S (* Sentence/proposition *)\n| NP : forall {x:Type}, Cat\n| rSlash : Cat -> Cat -> Cat (* A/B *)\n| lSlash : Cat -> Cat -> Cat (* A\\B *)\n| ADJ : forall {x:Type}, Cat\n| CN : forall {A:Type}, Cat.\nFixpoint\ninterp (c : Cat) : Type :=\nmatch c with\n| S => Prop (* Coq ’s type of propositions *)\n| @NP t => t\n| rSlash a b => interp b -> interp a\n| lSlash a b => interp a -> interp b\n| @ADJ t => t -> Prop\n| @CN t => unit\nend.\nFigure 2 Core grammatical categories as a Coq type Cat and the mapping ⌊−⌋from grammatical\ntypes to semantic types as interp.\nmodeling ﬁgurative speech, but is clearly incompatible with making natural language claims\nabout mathematical objects deﬁned in intuitionistic type theory.\nWe follow linguistically-motivated work using intuitionistic type theories like Coq’s for\nexploring possible logical forms [89, 78, 79, 16, 11, 10, 80], in using an alternative model\nwhere many grammatical categories are indexed by the underlying semantic type to which\nthey refer.\nCoq’s logic is expressive enough to give the set of grammatical types as a datatype Cat,\nand to give the interpretation of those types into semantic types as a recursive function\ninterp within Coq, as shown in Figure 2.\nGrammatical types (categories) Cat (categories) include the aforementioned slash types,\nsentences (S); noun phrases (NP A) denoting objects of Coq type A; adjectives (ADJ A)\ndenoting predicates over such objects; and common nouns (CN) denoting unit types, but\nimposing constraints on the semantic type indices of other phrases in the context of a sentence,\nused in cases where a sentence must refer to a common class of objects (i.e., a type), such\nas “natural numbers” or “rings.” . As mentioned previously, the slash types correspond to\nfunction types (the direction is relevant only in the grammar, not the semantics), sentences\nare modeled by Coq propositions (the type of logical claims), noun phrases of Coq type t\ncorrespond to elements of t, and similarly adjectives correspond to predicates on such types\n(i.e., elements of t -> Prop). These are modeled by the Coq function interp, which maps\ngrammatical types to other Coq types. Common nouns denote as the unit type. This may\nseem odd, but common nouns essentially serve as a means to force certain type variables to\nbe the ones corresponding to a speciﬁc word.6\nWe use Coq’s notation facilities (essentially, macros) to write the slash types as A // B for\nA/B, and A \\\\ B for A\\B. We also use these macros to deﬁne more interesting grammatical\n6 In traditional semantics in ﬁrst-order logic, common nouns often denote a kind of predicate to guard\nquantiﬁcations.\nC. S. Gordon and S. Matskevich\n7\ncategories.7 For example, a quantiﬁer over a certain common noun type is given as: Quant A ≡\n(S/(NPA\\S)/CNA).That is, a quantiﬁer discussing a Coq type A looks ﬁrst to its right for\na common noun (which constrains the type A to the one named by the natural language\ncommon noun). Then the result of that looks to its right for a sentence fragment expecting\na noun phrase to its left. The latter sentence fragment is essentially a predicate.\n3.1\nCombination Rules\nParsing natural language speciﬁcations requires automatically applying rules like /-Elim\nto combine sentence fragments. Rather than modifying Coq, we can use existing trusted8\nfunctionality to do this for us: typeclasses [86]. These are a mechanism for parameterizing\nfunction deﬁnitions by a set of (often derivable) operations. Coq permits declaring a typeclass\n(roughly, an interface), and declaring implementations associated with certain types. The\nimplementations may be parameterized by implementations for other types (such as deﬁning\nan ordering on pairs in terms of orderings for each component of the pair). When a function\nis called that relies on a set of operations, Coq attempts to use higher-order uniﬁcation\nto construct an appropriate implementation. It is possible to encode the rules of a system\nlike a CTL or CCG into typeclasses. Each judgment signature corresponds to a typeclass,\nand each rule corresponds to an instance (implementation) of the typeclass. We deﬁne the\njudgement form Γ ⊢T ⇒e as:\nClass\nSynth (l : list word) (cat : Cat) :=\n{ denote : interp cat }.\nIf an instance of Synth l C exists, it comes with an operation denote that produces a Coq\nvalue of type interp C — ⌊C⌋. Because e is viewed as an output we would like to query, it is\ndeﬁned as a member of the typeclass, rather than as an additional index. When deriving a\nformal speciﬁcation for sentence s, we will arrange for the typeclass machinery to locate an\ninstance of Synth s S — checking that s is a grammatically valid sentence — and request\nits term denotation when necessary. Translating a speciﬁcation given by the list of words\nw corresponds to parsing w as a sentence: ﬁnding an instance of Synth w S. We deﬁne an\ninstance for each rule to encode, such as this one corresponding to \\-Elim, which applies the\nlogical form of the functional to the logical form of the argument:\nInstance\nSynthLApp {Γ ∆A B}\n(L:Synth Γ A)(R:Synth ∆(A \\\\ B)) :\nSynth (Γ ++ ∆) c2 :=\n{denote := (denote R) (denote L)}.\nand for leftward composition (\\-Comp):\nInstance\nLComp:{Γ ∆A B C}\n(L:Synth Γ (A\\\\B))(R:Synth ∆(B\\\\C))\n: Synth (Γ ++ ∆) (A \\\\ C) :=\n{denote := fun x1 => (denote R) (denote L x1)}.\nThe remaining rules of Figure 1 can also be encoded in this way.\n7 We could use Coq deﬁnitions as well, but macros work better with the uniﬁcation in the next section.\n8 Oﬃcially, typeclasses are not part of Coq’s trusted computing base, as they elaborate to record\noperations before being passed to the core proof checking apparatus. In practice, they mediate which\nterms are passed to the core, so calling them untrusted would be a misnomer.\n8\nNatural Language Speciﬁcations in Proof Assistants\nIn addition to exploiting the proof assistant’s built-in search for parsing, the use of\ntypeclasses means the set of rules is extensible. As mentioned, the core CCG rules are\nalready known to be expressive enough to cover any known linguistic construction (the\ntechnical term is that CCGs are mildly context-sensitive [39, 48]). However:\nAdditional derived rules could be added either to accelerate proof search for recurring\nintricate constructions\nThe rules can be used to oﬀer generalized constructions for words with many roles. This\nis particularly valuable for linguistic constructs likc coordination (Section 3.2.2).\nAs discussed momentarily, it allows easy extension of the lexicon with new words without\nmodifying a ﬁxed database.\n3.2\nLexicon\nThe lexicon is encoded via another typeclass which assigns grammatical types and logical\nforms to individual words rather than series of words. Coq permits declaring multiple\ninstances for the same word (e.g., if a word has multiple meanings of diﬀerent grammatical\ntypes), giving essentially a free variant of intersection types [64] without the coherence issues\ndescribed by Carpenter [14] (only one deﬁnition will be chosen per appearance of the word).\nWe represent our lexicon with another type class, and tie it into the Synth typeclass:\nClass\nlexicon (w : word) (cat : Cat) := { denotation : interp cat }.\nInstance\nSynthLex {w cat}‘( lexicon w cat) : Synth [w] cat :=\n{ denote := denotation }.\nThus, a dictionary for our approach consists of a set of instance declarations for lexicon:\nInstance\nfourlex : lexicon \"four\" NP := { denotation := 4 }.\nInstance\nnoun_is_adj_sentence {A:Type} :\nlexicon \"is\" (@NP A \\\\ (S // @ADJ A)) :=\n{ denotation n p := p n }.\nInstance\nnoun_is_noun_sentence {A:Type} :\nlexicon \"is\" (@NP A \\\\ (S // @NP A)) :=\n{ denotation n a := n = a }.\nHere we have deﬁned two diﬀerent meanings for “is” allowing it to be used to apply an\nadjective, or to denote equality. The diﬀerence between the two, beyond their denotation is\nthe grammatical types: both expect a noun phrase to the left, and some other word to the\nright: an adjective in the ﬁrst case, or another noun in the second. Note that in both cases,\nthe adjective or noun phrase must match the type of underlying Coq object the the left-side\nnoun phrase refers to: the argument n is in both cases a variable of type interp (@NP A)=A\nbecause that is the argument of the outermost slash type, while the second argument in each\nentry corresponds to the interpretation of the second slash type’s argument (a predicate\nor an additional term, respectively). Coupled with a development-speciﬁc bit of lexicon to\nname a particular Coq object of interest:\nInstance\naddone_lex : lexicon \"addone\" NP :=\n{ denotation := addone (* λx. x + 1 *) }.\nthis approach permits giving correct denotations to both:\nJaddone is monotoneK ≡monotone(addone)\nJaddone given 3 is 4K ≡(addone 3) = 4\nC. S. Gordon and S. Matskevich\n9\n3.2.1\nQuantiﬁers\nEarlier we mentioned quantiﬁers over A can be given grammatical type\nQuant A ≡(S/(NPA\\S)/CNA)\nThus, a quantiﬁer looks to its right ﬁrst for a common noun (corresponding to the word\nidentifying the Coq type to quantify over), and after that is combined, the result looks\nfurther to the right for a sentence fragment expecting such a thing to its left. Then adding\nappropriate lexicon entries for “every”:\nInstance\nforall_lex {A:Type} : lexicon \"every\" (Quant A) :=\n{ denotation := fun _ P => (forall (x:A), P x) }.\nand another for the common noun “natural” (number) allows correctly parsing sentences like\nJevery natural is evenK ≡∀(n : nat). (even n)\n(Recall, we must still be able to state claims that are false.) The common noun contributes\nnothing directly to the denotation, but constrains the quantiﬁer to work with noun phrases\nreferring to natural numbers.\n3.2.2\nCoordination\nOne aspect of natural language which is the source of some interest is that the words “and”\nand “or” (or their equivalents in other languages) can often be used to combine sentences\nfragments of widely varying grammatical types. For example, in “four is even and positive”\nthe word “and” conjoins two adjectives: “even” and “positive.” Yet in the sentence “four is\neven and is positive” it conjoins two phrases of grammatical type NPnat\\S (“is even” and “is\npositive”).\nWe can directly adopt a solution from the compuational linguistics literature [14], and\nformalize that “and” and “or” apply to any semantic type that is a function into (a function\ninto. . . ) the type Prop of propositions. We deﬁne an additional typeclass to recognize such\n“Prop-like” grammatical types inductively, starting with the grammatical types S and ADJ,\nand inductively including slash types whose result type is also “Prop-like”, which deﬁne an\noperation to lift boolean semantics through repeated functions. We then add a polymorphic\nlexicon entry for each of “and” and “or” which assigns them any “Prop-like” type.\nThus in a sentence like “four is even and is positive” the two conjuncts are recognized\nas Prop-like (their underlying semantic type is nat →Prop), and the operations of the\ntypeclass recognizing this automatically lift a binary operation on Prop to a binary operation\non predicates. For “and” this lifts logical conjunction to λP. λQ. λx. P x ∧Q x, which is\nexactly what is needed — the grammar rules will apply this function to the semantics of\nthe even and positive predicates, and ﬁnally 4. Disjunction is handled similarly, and this\ngeneralizes to arbitrarily complex slash types whose ﬁnal semantic result is Prop.\n3.3\nUsing Speciﬁcations\nWe couple these with an additional typeclass Semantics s C which is deﬁned when the string\ns splits into a list of words w (string splitting is implemented with another typeclass) and there\nexists an instance of Synth w C. Then we deﬁne a function from strings to their denotations:\nDefinition\nspec (s:string) ‘{sem:Semantics s S} := sdenote\nsem.\n10\nNatural Language Speciﬁcations in Proof Assistants\nWhen invoked with a string s, Coq will search for an instance of Semantics s S — a parse\nof the string as a complete sentence. The logical form of a sentence has Coq type Prop (a\nproposition, or logical claim, to be proven).\nThus we may translate a range of speciﬁcations given an appropriate lexicon, including\nthose below (sugared into math notation for space and readability):\nJaddone is monotoneK = ∀x, y : N. x ≤y ⇒addone x ≤addone y\nJevery natural is non-negativeK = ∀n : N. n ≥0\nJevery natural is non-negative and some natural is evenK = (∀n : N. n ≥0) ∧(∃n : N. even n)\nBecause Coq’s logic allows proof goals to be computed, spec can be used to declare proof\ngoals:\nGoal spec \"addone is monotone \".\n> spec \"addone is monotone\"\nsimpl. (* simplify\nthe goal *)\n> forall x y : nat , x <= y -> addone x <= addone y\nIf Coq cannot ﬁnd a Semantics instance for a speciﬁcation, the user sees an error; because\ninstance search reuses existing proof search functionality, a skilled user could manually debug\nthe failure.\n3.4\nPredicativity\nA careful reader will notice that Section 3 deﬁnes a Type whose constructors quantify over\nType. Our current use cases produce elements of Prop, so we could simply deﬁne Cat has\nhaving sort Prop. However, there are many reasons to conduct proofs avoiding Prop even\nin speciﬁcations (e.g., homotopy type theory [9]). Making all of the typeclass deﬁnitions,\ninstances, and goals universe-polymorphic (Polymorphic) allows us to remain predicative.\n3.5\nPerformance\nThe performance of semantic parsing from natural language into formal speciﬁcations depends\non both the underlying typeclass resolution procedure, as well as the space of derivations\nthat must be explored during parsing. Our tokenization code for splitting strings runs\nin linear time because there is at most one typeclass instance that can apply for each\ncharacter of a string (i.e., depending on whether or not the character is a space). So the rules\nwhich determine the search space are primarily the structural rules encoded in the Synth\ntypeclass instances, and the lexicon instances. Since most words have only one or a very\nsmall number of grammatical roles (in general, not just in our small prototype [33, 34, 32]),\nlexicon ambiguity will not be a major driver of search costs. Instead, most costs come from\nexploring structural rule applications, particularly the rules for associativity and shifting\nof left and right slash type nesting, each of which may send search oﬀinto a dead end.\nTo limit ambiguity, our typeclass for coordinators imposes an upper bound on how many\ntimes boolean operations can be lifted, and in Coq we set the typeclass resolution to use a\ndepth limit of 15 on instance resolution. This reﬂects that by and large, categorial grammar\nderivations tend to be shallow and wide.\nAs a coarse measure of base eﬃciency, under these conditions parsing “every natural is\nnon-negative and some natural is even” takes approximately 26 seconds9 on a 4-core Intel\n9 Measured by setting Semantics \"every natural is non-negative and some natural is even\" S\nas the proof goal, and using time typeclasses eauto to time the search.\nC. S. Gordon and S. Matskevich\n11\nCore i5 (with 16GB of RAM, but this is CPU-bound). Faster would be ideal, but this is at\nleast suitable for using one ﬁle to cache the parses, which can be compiled infrequently, with\nother ﬁles pulling in the relationships as needed. Inspection of the search trace conﬁrms that\nmost search time is spent in dead-ends related to diﬀerent combinations of associativity and\nshifting rules.\nTo investigate whether alternative typeclass resolution algorithms might have an impact,\nwe ported most of our machinery — all except string tokenization10 — to version 4 of\nthe Lean theorem prover [68]. This version uses a new typeclass resolution algorithm [85]\ndesigned speciﬁcally to accelerate complex typeclass resolution problems like the one our\nwork presents. Working with pre-tokenized inputs, compiling the entire formalization and\nparsing multiple examples, including the one above, consistently takes less than 3.5 seconds.\nMore complex sentences of course may require more time and investment in optimization, as\nthe simplest categorial grammars model the context-free languages (which have worst-case\ncubic time parsing), while the most complete mildly-context-sensitive classes of categorial\ngrammars have O(n6) worst-case parsing cost [39], though in practice the common case can\nbe made quite fast [19, 18, 20].\n4\nModularity and Extension: Growing a Lexicon, Handling More\nLogics\nThe previous section described only a small fragment of English suitable for formalizing\nmathematical claims. Categorial grammars are what linguistic semanticists call lexicalized\ngrammar formalisms. Unlike phrase-structure grammars (e.g., context-free grammars) which\nbuild in an explicit classiﬁcation of grammatical phrase types, lexicalized grammars use a\nsmall set of general rules (like those in Figure 1), and then rely on the lexicon to give the\nprecise grammatical types of every word. The availability of slash types (directed function\ntypes) aﬀords signiﬁcant ﬂexibility, and extensions to attach modalities to the slashes [6, 63]\nallow further constraints capturing the subtleties of natural language to be captured solely\nby giving precise grammatical types (and semantics) to individual words.\n4.1\nManaging Words\nAdding new words to a categorial grammar lexicon is conceptually as simple as adding the\nword, particular grammatical type, and associated denotation to the database. This makes\nit easy to extend a system with new concepts (e.g., new algebraic structures); lexicon entries\nto deal with concepts deﬁned in a proof assistant library can be distributed as a part of\nthat library. Conversely, if a word or particular usage of a word is found to be confusing to\nhumans, leading to ambiguity, or otherwise problematic, it can be removed from the lexicon\nwhile aﬀecting only inputs that use that word in that way (i.e., the problematic ones).\nIn practice the situation will be more complex, but we expect most extension to require\nlittle, if any, special linguistic knowledge. Assuming a robust core lexicon (Section 4.3), it is\nlikely that most extensions will be additions of words with simpler categories. Experiments\non a large standard-English lexicon showed [33] that when training on most of lexicon, the\nunseen words in a held-out test set were primarily nouns (35.1%) or transformations of\nnouns (e.g., adjectives, at 29.1%). These are the simplest categories to provide semantics\n10 String tokenization takes negligible time in Coq for reasons outlined above, and does not directly\ntranslate because Lean uses a diﬀerent datatype for strings.\n12\nNatural Language Speciﬁcations in Proof Assistants\nfor (types, objects, and predicates), strongly suggesting that proof assistant users with no\nspecial linguistics background could make most extensions themselves. Similar experiments\nfor a wide-coverage lexicon of German [32] show over half of unknown words to be nouns,\nsuggesting this feasibility extends beyond just English.\n4.2\nSupporting Additional Grammatical Constructions\nFormalization of signiﬁcant fragments of language much deal with more subtle constructions\nthat what we have described so far can handle. However, what we have described thus far is\nessentially read directly out of the literature on linguistic semantics. Linguists have spent many\ndecades building out knowledge of how to handle more sophisticated uses of quantiﬁcation [88,\n62] (“every,” “some,” “most”), resolving pronoun references [36], discontinuity [66] (where a\nword is far from a word it modiﬁes), and much more [14, 67].\n4.3\nA Full-Featured Core Lexicon\nHow large should a base lexicon with reasonably wide coverage be? The largest lexicon for a\ncategorial grammar is Hockenmaier and Steedman’s CCGBank [32, 34], which models the\nusage of language in a particular sample of the Wall Street Journal. It contains roughly 75K\nunique words, and some of the most common words have dozens of grammatical categories, or\nmore (the English word “as” is overloaded with 130 distinct — though related — grammatical\ntypes). This has motivated work on learning lexicons with semantics [41, 5, 51, 50], as well as\nwork on learning more compact lexicons that automatically capture standard word variations\n(e.g., automatically generalizing singular deﬁnitions to work for plural forms) [55, 93, 49].\nThese works all focus on cases where CCGs parse sentences into a variant of ﬁrst-order logic,\nbut should in principle generalize to targeting richer logics like the Calculus of Inductive\nConstructions underlying Coq and Lean. While the need to scale to large lexicons draws us\nback to a kind of machine learning, it draws us back to a kind with an eminently auditable\nresults, producing lexicon entries which have well-deﬁned individual meaning, which can be\nmanually adjusted or removed if necessary.\nAny initial broad-coverage lexicon for technical prose will need to be manually constructed\n(including input to a future learning algorithm). However, since technical prose about math\nand code is still a particular stylistic use of a standard natural language, it mostly reuses\nwords in the same grammatical role — and therefore, same categorial grammar type — as\nnon-specialist grammars. This means we can bootstrap an initial lexicon for English by\nreusing grammatical types from existing categorial grammar lexicons for English [34, 35],\nand similarly German [32], Hindi [3], Japanese [58], and other languages [1].\nThis means initial eﬀorts can focus mostly on deﬁning semantics for existing grammar-only\nlexicon entries, rather than starting from scratch. And for many of the words appearing in\nspeciﬁcations, particularly quantiﬁers (“every”, “all”, etc.), determiners (“a”, “the”, etc.), and\nprepositions (“in”, “of”, etc.), the semantics are typically very simple (quantiﬁer semantics\nlook like the examples in earlier sections; prepositions are typically identity functions,\nfunctioning similarly to linguistic phantom types [27] / units of measure [42] for other words\nto locate parameters.)\nCareful readers or prior students of linguistics may have wondered when matters of verb\ntense, noun case and number, grammatical gender,11 etc. would arise. In full linguistic\n11 Which does not exist in English, but does in German, French, and so on\nC. S. Gordon and S. Matskevich\n13\ntreatments, these are reﬂected in additional parameters to some grammatical categories. So\nfor example, in our setting a noun phrase would be parameterized not only by the underlying\nreferent type, but also by the case, number and so on; lexicon entries would then carry these\nthrough appropriately (making it possible to for example, require the direct object of a verb\nto be in the accusative case rather than nominative). We have omitted such a treatment\nhere partly because it would obscure the key ideas while adding little value, partly because\nmany of these distinctions are less important for our examples in English (which has fewer\nsyntactic case distinctions than other languages), and partly because some aspects (like\ntense) may make sense only for speciﬁc embedded speciﬁcation logics.\n4.4\nBeyond CIC\nWhile the framing in this paper has focused on generating speciﬁcations which in Coq\nand Lean have type Prop, this is not required. Categorial grammars require only that their\ntop-level semantic truth value type have the structure of a Heyting Algebra [53]: a type with\nbinary operators for standard logical operators.\nOur Coq and Lean formalizations in fact make this generalization: the core machinery\nis polymorphic over an arbitrary choice of Heyting Algebra, with a lexicon split between\nentries polymorphic over the Heyting Algebra being targeted (e.g., “or” and “and”) and\nwords speciﬁc to a given Heyting Algebra.\nThis means the core idea applies not only to specs of type Prop, but that this machinery\ncan be readily retargeted to any logic formalized within the proof assistant, such as LTL [75],\nCTL [21], or the BI-algebras underlying separation logics like Iris [40].\nThis is not itself a new observation, as we discuss in related work (Section 7), as others\nhave instantiated categorial grammars to generate, for example, CTL [23], before. However,\nthese prior applications have targeted only speciﬁc use cases, while this setting permits reusing\nmany lexicon entries across many logics, which should help in retargeting this machinery to\nnew applications and future logics which may be formalized within a proof assistant.\n5\nTrust and Auditing\nOne of the essential criteria for an LCF-style proof assistant is the production of an\nindependently-checkable proof certiﬁcate. While we have proposed using Coq’s typeclass\nmachinery to automatically parse and denote, and the typeclass resolution itself is typically\nnot viewed as part of the trusted computing base (TCB), it does eﬀectively produce a proof\ncertiﬁcate. The typeclass machinery explicitly constructs a an instance of the typeclass —\nan element of the corresponding record type — and passes it to spec. So Coq’s kernel sees\n(eﬀectively) a categorial grammar proof, constructed via typeclass instances rather than\nconstructors of an inductive data type. This explicit term persists into the proof certiﬁcates\nCoq already produces, and could be identiﬁed by an independent proof checker that wished\nto also validate the natural language interpretation. For example, the textual representation\nof the term witnessing that “four is even” parses to even 4 is:\nbridgeStringWords (* The\ntypeclass\ninstance to tokenize\nthen\nparse *)\n(split1\nNotSpace4 (* Start of tokenization *)\n(split2\nNotSpace6\nNotSpace6\n(split4 ’ NotSpace6\nNotSpace6\nNotSpace6\nNotSpace6 )))\n(SynthLApp (SynthLex\nfourlex) (* Parse\ntree of tokenized\nstring *)\n(SynthRApp (SynthShift (SynthLex\nnoun_is_adj_sentence ))\n(SynthLex\neven_lex )))\n14\nNatural Language Speciﬁcations in Proof Assistants\nwhich encodes both the tokenization (split1, etc.) and the constructed parse tree (with\nSynthLex calls referring to individual lexicon instances).\nWe can think of several ways a user might accidentally or maliciously risk confusing an\nindependent checker: All but one can easily be detected by a checker aware of the categorial\ngrammar speciﬁcation typeclasses. The ﬁnal possibility amounts to changing the speciﬁcation\nin the proof certiﬁcate.\nA user may redeﬁne or extend our core instances (for Synth) to produce a diﬀerent\ndenotation. A certiﬁcate checker would already ensure these are type-correct. An natural-\nlanguage-speciﬁcation-aware extension could check that the Synth instances correspond to\nthe desired rules. Or to better support some of the extensibility arguments made earlier,\nthe Synth typeclass could be modiﬁed to also carry a justiﬁcation of its conclusions in a\nmore general substructural logic [47, 30], which would amount to requiring extensions to\ncarry conservativity proofs.\nA user may extend the lexicon with additional words or additional grammatical roles\nfor a given word, introducing ambiguity into the parsing. Checking for ambiguity is\nrelatively straightforward: ignoring indexing by Coq types, equivalence of grammatical\ntypes is decidable, and a checker could conservatively require that any lexicon entries with\nthe same index-erased grammatical types have clearly-distinct indices. An independent\nchecker could verify the absence of ambiguity in the lexicon, or alternatively surface the\nuse of any ambiguity in a parsing derivation for human inspection.\nA user could also manipulate the lexicon, for example redeﬁning “monotone” to denote as\nλf, True. This is arguably a form of modifying the speciﬁcation by changing deﬁnitions,\nrather than sneaking a broken proof past a certiﬁcate checker. It is analagous to changing\na deﬁnition of a property veriﬁed by a proof — a working proof with the wrong deﬁnition\nis wrong, but this leaves behind evidence of the incorrect deﬁnition.\nThese possible forms of attack highlight the main sources of trust added when considering\nnatural language speciﬁcations in the approach we describe: the grammatical rules for\ncombining phrases, well-formedness of the lexicon, and the deﬁnitions of words in the lexicon.\nA minor point of trust is the requirement that lexicon entries at least give semantics\nwhose Coq type is consistent with the grammatical type at hand. In our prototype this is\nexpressed via the requirement that the type of a word’s denotation is given by applying the\ninterp function to the grammatical type. This is checked automatically by encoding this\nrequirement in Coq’s types, and enforced by any proof certiﬁcate checker for Coq’s logic.\nThis might seem trivial, but it is worth noting because other implementations of categorial\ngrammars often do not check types. In early experiments we encountered type-related bugs\nin NLTK’s CCG implementation, and our attempt to directly reproduce an existing use of\nCCGs for temporal logic speciﬁcations [23] failed not because of our prototype’s limitations,\nbut because we encountered cases where the published lexicon entries were inconsistent\nwith the stated grammatical types (e.g., giving a word a grammatical type indicating two\narguments, but a logical form which only accepted one). It is possible these were merely\ntypographical errors, as is often found by any mechanization based on a published paper\nrather than a code artifact [44], but in either event the type system detected the incorrect\nentries when we attempted to enter them directly from the paper.\n6\nA Limited Role for Machine Learning\nAs discussed earlier, the need for predictability, auditability, and modular lexicons play to\nthe strengths of categorial grammar. These also correspond to established weaknesses of\nC. S. Gordon and S. Matskevich\n15\ncommon statistical and neural approaches to natural language processing, which often handle\nequivalent expressions in surprisingly incompatible ways (despite intriguing ongoing work\nto alleviate this [96]), provide no interpretable justiﬁcation for linguistic choices, and are\ninherently non-modular (one cannot simply retroﬁt a small collection of additional words\ninto a large model). Beyond this, neural approaches appear to systematically struggle to\ndeal consistently and accurately with boolean operations (especially negation) [91, 24, 70],\nwhich are often critical to formal speciﬁcations.\nOne of the primary advantages of neural and statistical models for natural language\nprocessing is that natural language is constantly evolving, with new expressions and terms\nbeing invented regularly, and neural and statistical models can often handle unseen words\nsomewhat reasonably by mimicking known usage patterns of other words, despite lacking\nany ground notion of meaning [12, 57]. However, this advantage has little role to play for\nformal speciﬁcations, both because of the hightened certainty requirements, but because\nnew terms often carry very speciﬁc meanings, and also because the natural language used\nto describe formal properties tends to evolve more conservatively in order to avoid human\nmisunderstandings.\nThe biggest general advantage of machine learning techniques is their ability to process\nlarge amounts of data with reduced human eﬀort. Historically, large databanks of grammatical\ntypes and semantics for English [34], German [32], Hindi [3], Japanese [58], French [65], and\nother languages [1] have been rooted in enormous human eﬀorts to manually label data,\neither directly or via translation from a corpus manually annotated for another grammar\nformalism. This has inspired a range of techniques for learning a lexicon for semantic parsing\nfrom a small set of initial examples [51, 5, 50, 49]. These techniques could in principle be\nused to rapidly expand an initially-hand-crafted core lexicon, or eventually to learn a domain-\nor program-speciﬁc lexicon to be added to a base lexicon after auditing. Currently all of these\ntechniques target learning logical forms in ﬁrst-order logic, and would require adaptation to\ndeal with the indexed grammars required to target a proof assistant’s logic.\nOne way machine learning could play a major role in this endeavour would be analagous\nto one of its major roles in traditional semantic parsing, which is in learning an optimal\nsearch strategy over derivations from a large corpus of complete derivations [20, 18, 19]. Such\na statistical model over likely parse structures can be used to dramatically accelerate parsing\nby using a model to choose priorities for proof search, thereby avoiding more dead-ends.\nUsing machine learning in this narrow way could lead to substantial performance gains\nwithout compromising the categorial grammar properties relevant for formal speciﬁcations:\na successful parse still yields a full derivation. In principle it should be possible to learn\nhow to assign rule priorities and/or customize generate custom (derived) rules. However,\na prerequisite for training such models is a large corpus of complete parses, which at least\ninitially would need to be obtained through regular uniﬁcation alone.\n7\nRelated Work\nBoth categorial grammars of the form we work with and the use of dependent type theories\nfor natural language semantics have long histories [52, 92, 89], and we are hardly the ﬁrst to\npropose reducing the gap between formal or semi-formal speciﬁcations and natural language.\nOur proposal diﬀers from the former primarily in that we argue for employing these not\nfor the study of linguistics, but for the application of linguistics research to build a system\nfor integrating natural language descriptions into the main intended use of proof assistants,\nincluding cases where the proof assistant is used to construct proofs in an embedded logic.\n16\nNatural Language Speciﬁcations in Proof Assistants\nOur proposal diﬀers from most work on the latter primarily in our focus on employing actual\nlinguistic models of grammar and meaning to extract intent from natural language, rather\nthan using a range of shallow (though often eﬀective) heuristics, in order to aﬀord a higher\ndegree of freedom in expressing expectations. We oﬀer more details on these relationships\nbelow.\nOthers have used type theories like Coq’s for logical forms [89, 78, 79, 16, 11], broadly\nmaking the argument that variants of dependent type theory oﬀer a range of appealing\noptions for modeling natural language semantics that ﬁx some percieved deﬁciencies in the\nuse of a lambda calculus over ﬁrst-order logic formulas, but consistently focused on using\nthis as a means to study linguistics. The notion of indexing some grammatical categories by\nthe type of a referent in such an underlying type theory comes from Ranta’s work [80] on\nstudying the linguistics of mathematical statements. Ranta [79], Kokke [46] and Kiselyov [43]\nhave formalized variants of categorial grammar with semantics in proof assistants, but\nonly as object logics of study in order to prove properties of those systems, rather than as\nworking parsers integrated with other uses of proof assistants. This leaves much to explore in\nintegrating categorial grammar with various forms of type-theoretical language semantics [17],\nsome of which coincide with common speciﬁcation patterns.\nOthers have worked towards using categorial grammars and related techniques to translate\nnatural language into formal speciﬁcations in a variety of other logics. Dzifcak et al. [23]\nused CCG to translate natural language speciﬁcations to CTL∗, though as mentioned in\nSection 4.4 their semantics contain semantic type errors which are caught by working within\na proof assistant that enforces consistency between grammatical and semantic types. Seki et\nal. [84, 83] is the earliest approach we are aware of, using an alternative grammar formalism\n(HPSG [77]) to translate natural langauge to ﬁrst-order logic. Each of these approaches\ntargets only a single logic, and assumes the translation is divorced from any particular use of\nformal speciﬁcations.\nThere have also been notable attempts to translate formal speciﬁcations into English,\nsuch as the support in the KeY theorem prover [38], and a predecessor system [13] that\ndirectly uses categorial grammar in reverse for natural language generation [81] (another\nestablished use for categorial grammars beyond the semantic parsing we focus on).\nThere are also other approaches to bringing rigorous formalization closer to natural\nlanguage, without attempting to capture natural language grammar in a systematic way.\nIsabelle/HOL’s [69] Isar proof language [94] attempts to make proofs themselves more\nreadable using proof manipulation commands resembling English. This is an example of\nwhat is known as controlled natural language [28], a pattern of system development where\nthe input language is a heavily restricted fragment of natural language, usually (though not\nalways) simple enough to enable fully automatic processing, usually by heavily restricting\nboth grammatical constructions and vocabulary. This includes examples like Cramer et\nal. [22], who have worked on heavily restricted subsets of natural language that address both\nthe speciﬁcation of lemmas and their proofs, but like Isar do not attempt to capture any\ngeneral natural language structure, and techniques which focus only on stating formulas in\nnatural language [29]. By contrast, our proposed approach (1) reuses existing proof assistant\nmachinery (typeclasses [86, 85]) rather than requiring specialized support, and (2) aims\nto (eventually) permit almost arbitrary natural language grammar once an adequate base\nlexicon is developed (which can then be directly extended by individual proof developments).\nC. S. Gordon and S. Matskevich\n17\n8\nLooking Forward\nWe have presented evidence that it is plausible to support natural language speciﬁcations\nin current proof assistants by exploiting existing typeclass machinery, with no additional\ntooling required. Carried further, this could be useful in many ways. It can reduce the\ngap between informal and formal speciﬁcations, reducing (though not eliminating) trust\nin the manual formalization of requirements. Potentially non-experts in veriﬁcation could\nunderstand some theorem statements, gaining conﬁdence that a veriﬁcation result matched\ntheir understanding of desired properties. And this could be used in educational contexts to\nhelp students learn or check informal-to-formal translations.\nOf course, the details matter as well, and it will take time to realize a prototype that is\nbroadly useful. First and foremost, a rich lexicon is required. As explained earlier, at least\nthe initial lexicon will need to be manually constructed (borrowing grammatical categories\nfrom existing lexicons, and ﬁlling in the semantics) before it would be fruitful to adapt\ntechniques for learning lexicons. Guiding this eﬀort would require a substantial collection\nof examples of natural-language descriptions of formal claims, both for prioritizing lexicon\ngrowth and for validation that the approach is growing to encompass real direct descriptions\nof claims. Despite the now-enormous body of formalized proofs of program properties and\nmathematical results, early eﬀorts in this direction have revealed this is less trivial than\nit seems. Even popular and classic texts introducing formal speciﬁcation like Software\nFoundations [74] and classic texts like Type Theory and Functional Programming [90] have\nremarkably few crisp natural language statements matching a speciﬁc formal statement,\ninstead discussing various needs at length in order to motivate eventual details of the ﬁnal\nformalization (which is sensible for expository texts). Reynolds’ classic paper introducing\nseparation logic [82] contains no English-language description of any full invariant involving\nseparating conjunction. We do not necessarily require exemplars of full descriptions in a\nsingle sentence; it is common for one speciﬁcation to imply multiple high-level properties,\nand we envision one style of use for natural language speciﬁcations to be checking that a\ngiven veriﬁed result implies multiple natural language claims which each cover part of the\ndesired results.\nIt is possible that small diﬀerences will be required between standard natural language\ngrammars and those used by this approach, arising from distinctions important to proof\nassistants but irrelevant to colloquial language. This is already the case, as mentioned, with\nthe indexing of some grammatical categories with the semantic types of referents, following\nRanta’s early work on formalizing mathematical prose [80]. This direction oﬀers opportunities\nto collaborate with linguists working in syntax and compositional semantics [8, 37, 88]. Such\ncollaborations could both help with possible novel linguistic features of “semi-formal” natural\nlanguage, and oﬀers a setting for applying classical linguistic techniques in a domain where\nthey provide unique value.\nA great deal of work lies ahead, but the potential beneﬁts seem to more than justify\nfurther exploration in this direction.\nReferences\n1\nLasha Abzianidze, Johannes Bjerva, Kilian Evang, Hessel Haagsma, Rik van Noord, Pierre\nLudmann, Duc-Duy Nguyen, and Johan Bos.\nThe parallel meaning bank:\nTowards a\nmultilingual corpus of translations annotated with compositional meaning representations. In\nEACL, 2017.\n18\nNatural Language Speciﬁcations in Proof Assistants\n2\nKazimierz Ajdukiewicz. Die syntaktische konnexität. studia philosophica, 1: 1–27. reprinted\nin storrs mccall, ed., polish logic 1920–1939, 207–231, 1935.\n3\nBharat Ram Ambati, Tejaswini Deoskar, and Mark Steedman. Hindi ccgbank: A ccg treebank\nfrom the hindi dependency treebank. Language Resources and Evaluation, 52(1):67–100, 2018.\n4\nAndrew W Appel. Foundational proof-carrying code. In Proceedings of the 16th Annual IEEE\nSymposium on Logic in Computer Science, LICS 2001, pages 247–256. IEEE, 2001.\n5\nYoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping\ninstructions to actions. Transactions of the Association for Computational Linguistics, 1:49–62,\n2013.\n6\nJason Baldridge and Geert-Jan M. Kruijﬀ. Multi-modal combinatory categorial grammar. In\nProceedings of the Tenth Conference on European Chapter of the Association for Computational\nLinguistics - Volume 1, EACL ’03, pages 211–218, Stroudsburg, PA, USA, 2003. Association\nfor Computational Linguistics. doi:10.3115/1067807.1067836.\n7\nYehoshua Bar-Hillel.\nA quasi-arithmetical notation for syntactic description.\nLanguage,\n29(1):47–58, 1953.\n8\nChris Barker and Pauline Jacobson, editors. Direct compositionality. Oxford University Press,\n2007.\n9\nAndrej Bauer, Jason Gross, Peter LeFanu Lumsdaine, Michael Shulman, Matthieu Sozeau, and\nBas Spitters. The hott library: a formalization of homotopy type theory in coq. In Proceedings\nof the 6th ACM SIGPLAN Conference on Certiﬁed Programs and Proofs, pages 164–172, 2017.\n10\nDaisuke Bekki.\nDependent type semantics: An introduction.\nIn Logic and Interactive\nRationality (LIRA) Yearbook 2012, Volume 1, pages 277–300. 2012.\n11\nDaisuke Bekki.\nRepresenting anaphora with dependent types.\nIn Logical Aspects of\nComputational Linguistics - 8th International Conference, LACL 2014, Toulouse, France,\nJune 18-20, 2014. Proceedings, pages 14–29, 2014. doi:10.1007/978-3-662-43742-1\\_2.\n12\nEmily M Bender and Alexander Koller. Climbing towards nlu: On meaning, form, and\nunderstanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 5185–5198, 2020.\n13\nDavid A Burke and Kristofer Johannisson. Translating formal software speciﬁcations to natural\nlanguage. In International Conference on Logical Aspects of Computational Linguistics, pages\n51–66. Springer, 2005.\n14\nBob Carpenter. Type-logical semantics. MIT press, 1997.\n15\nBob Carpenter. The turing-completeness of multimodal categorial grammars. JFAK: Essays\ndedicated to Johan van Benthem on the occasion of his 50th birthday. Institute for Logic,\nLanguage, and Computation, University of Amsterdam. Available on CD-ROM at http://turing.\nwins. uva. nl, 1999.\n16\nStergios Chatzikyriakidis and Zhaohui Luo. Natural language inference in coq. Journal of\nLogic, Language, and Information, 23, 2014.\n17\nStergios Chatzikyriakidis, Zhaohui Luo, et al. Modern perspectives in type-theoretical semantics,\nvolume 98. Springer, 2017.\n18\nStephen Clark and James R. Curran. Log-linear models for wide-coverage ccg parsing. In\nProceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,\nConference on Empirical Methods on Natural Language Processing ’03, pages 97–104,\nStroudsburg, PA, USA, 2003. Association for Computational Linguistics.\n19\nStephen Clark and James R. Curran. Wide-coverage eﬃcient statistical parsing with ccg and\nlog-linear models. Computational Linguistics, 33(4):493–552, December 2007.\n20\nStephen Clark, Julia Hockenmaier, and Mark Steedman. Building deep dependency structures\nwith a wide-coverage ccg parser. In Proceedings of the 40th Annual Meeting on Association for\nComputational Linguistics, ACL ’02, pages 327–334, Stroudsburg, PA, USA, 2002. Association\nfor Computational Linguistics. URL: http://dx.doi.org/10.3115/1073083.1073138, doi:\n10.3115/1073083.1073138.\nC. S. Gordon and S. Matskevich\n19\n21\nEdmund M. Clarke, E. Allen Emerson, and A. Prasad Sistla. Automatic Veriﬁcation of\nFinite-state Concurrent Systems Using Temporal Logic Speciﬁcations. ACM Transactions on\nProgramming Languages and Systems (TOPLAS), 8(2):244–263, 1986.\n22\nMarcos Cramer, Bernhard Fisseni, Peter Koepke, Daniel Kühlwein, Bernhard Schröder, and\nJip Veldman. The naproche project controlled natural language proof checking of mathematical\ntexts. In International Workshop on Controlled Natural Language, pages 170–186. Springer,\n2009.\n23\nJuraj Dzifcak, Matthias Scheutz, Chitta Baral, and Paul Schermerhorn. What to do and how to\ndo it: Translating natural language directives into temporal and dynamic logic representation\nfor goal management and action execution. In 2009 IEEE International Conference on Robotics\nand Automation, pages 4163–4168. IEEE, 2009.\n24\nAllyson Ettinger. What bert is not: Lessons from a new suite of psycholinguistic diagnostics\nfor language models. Transactions of the Association for Computational Linguistics, 8:34–48,\n2020.\n25\nKate Finney. Mathematical notation in formal speciﬁcation: Too diﬃcult for the masses?\nIEEE Transactions on Software Engineering, 22(2):158–159, 1996.\n26\nKate M Finney and Alex M Fedorec. An empirical study of speciﬁcation readability. In\nTeaching and Learning Formal Methods. Academic Press, 1996.\n27\nMatthew Fluet and Riccardo Pucella. Phantom types and subtyping. Journal of Functional\nProgramming, 16(6):751–791, 2006.\n28\nNorbert E Fuchs. Controlled natural language. In Workshop on Controlled Natural Language,\nCNL. Springer, 2009.\n29\nNorbert E Fuchs, Uta Schwertel, and Sunna Torge. Controlled natural language can replace\nﬁrst-order logic. In 14th IEEE International Conference on Automated Software Engineering,\npages 295–298. IEEE, 1999.\n30\nJager Gerhard et al. Anaphora and type logical grammar, volume 24. Springer Science &\nBusiness Media, 2005.\n31\nMike Gordon. From lcf to hol: a short history. In Proof, Language, and Interaction: Essays in\nHonour of Robin Milner, pages 169–186. 2000.\n32\nJulia Hockenmaier. Creating a ccgbank and a wide-coverage ccg lexicon for german. In\nProceedings of the 21st International Conference on Computational Linguistics and the 44th\nannual meeting of the Association for Computational Linguistics, pages 505–512. Association\nfor Computational Linguistics, 2006.\n33\nJulia Hockenmaier and Mark Steedman. Ccgbank: User’s manual. Technical report, 2005.\n34\nJulia Hockenmaier and Mark Steedman. Ccgbank: a corpus of ccg derivations and dependency\nstructures extracted from the penn treebank. Computational Linguistics, 33(3):355–396, 2007.\n35\nMatthew Honnibal, James R Curran, and Johan Bos. Rebanking ccgbank for improved np\ninterpretation. In Proceedings of the 48th annual meeting of the association for computational\nlinguistics, pages 207–215, 2010.\n36\nPauline Jacobson. Towards a variable-free semantics. Linguistics and philosophy, 22(2):117–185,\n1999.\n37\nPauline I Jacobson.\nCompositional semantics: An introduction to the syntax/semantics\ninterface. Oxford University Press, 2014.\n38\nKristofer Johannisson. Natural language speciﬁcations. In Veriﬁcation of Object-Oriented\nSoftware. The KeY Approach, pages 317–333. Springer, 2007.\n39\nAravind K. Joshi, David J. Weir, and K. Vijay-Shanker. The convergence of mildly context-\nsensitive grammar formalisms. Technical Report MS-CIS-90-01, University of Pennsylvania\n(Philadelphia, PA US), Philadelphia, 1990. URL: http://opac.inria.fr/record=b1042789.\n40\nRalf Jung, Robbert Krebbers, Jacques-Henri Jourdan, Aleš Bizjak, Lars Birkedal, and Derek\nDreyer. Iris from the ground up: A modular foundation for higher-order concurrent separation\nlogic. Journal of Functional Programming, 28, 2018.\n20\nNatural Language Speciﬁcations in Proof Assistants\n41\nMakoto Kanazawa. Learnable classes of categorial grammars. CSLI Publications, Stanford\nUniversity, 1995.\n42\nAndrew Kennedy. Dimension types. In European Symposium on Programming, pages 348–362.\nSpringer, 1994.\n43\nOleg Kiselyov. Applicative abstract categorial grammars in full swing. In JSAI International\nSymposium on Artiﬁcial Intelligence, pages 66–78. Springer, 2015.\n44\nCasey Klein, John Clements, Christos Dimoulas, Carl Eastlund, Matthias Felleisen, Matthew\nFlatt, Jay A McCarthy, Jon Rafkind, Sam Tobin-Hochstadt, and Robert Bruce Findler. Run\nyour research: on the eﬀectiveness of lightweight mechanization. In Proceedings of the 39th\nannual ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages\n285–296, 2012.\n45\nGerwin Klein, June Andronick, Kevin Elphinstone, Toby Murray, Thomas Sewell, Rafal\nKolanski, and Gernot Heiser. Comprehensive formal veriﬁcation of an os microkernel. ACM\nTrans. Comput. Syst., 32(1):2:1–2:70, February 2014. URL: http://doi.acm.org/10.1145/\n2560537, doi:10.1145/2560537.\n46\nWen Kokke. Formalising type-logical grammar in Agda. In 1st Workshop on Type Theory and\nLexical Semantics, 2015.\n47\nGeert-Jan M. Kruijﬀand Jason Baldridge. Relating categorial type logics and ccg through\nsimulation, 2000. Unpublished manuscript. URL: https://citeseerx.ist.psu.edu/viewdoc/\nsummary?doi=10.1.1.27.4152.\n48\nMarco Kuhlmann, Alexander Koller, and Giorgio Satta. Lexicalization and generative power\nin CCG. Computational Linguistics, 41(2):187–219, 2015.\n49\nTom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman.\nLexical\ngeneralization in ccg grammar induction for semantic parsing. In Proceedings of the Conference\non Empirical Methods in Natural Language Processing, Conference on Empirical Methods on\nNatural Language Processing ’11, pages 1512–1523, Stroudsburg, PA, USA, 2011. Association\nfor Computational Linguistics.\n50\nTom Kwiatkowski, Luke S. Zettlemoyer, Sharon Goldwater, and Mark Steedman. Inducing\nprobabilistic ccg grammars from logical form with higher-order uniﬁcation. In Conference on\nEmpirical Methods on Natural Language Processing, pages 1223–1233. ACL, 2010.\n51\nM. Collins L. Zettlemoyer. Learning to map sentences to logical form: Structured classiﬁcation\nwith probabilistic categorial grammars. Proceedings of the Conference on Uncertainty in\nArtiﬁcial Intelligence, 2005.\n52\nJoachim Lambek. The mathematics of sentence structure. The American Mathematical\nMonthly, 65(3):154–170, 1958.\n53\nJoachim Lambek. Categorial and categorical grammars. In Categorial grammars and natural\nlanguage structures, pages 297–317. Springer, 1988.\n54\nXavier Leroy.\nA formally veriﬁed compiler back-end.\nJournal of Automated Reasoning,\n43(4):363–446, 2009.\n55\nMike Lewis and Mark Steedman. Improved ccg parsing with semi-supervised supertagging.\nTransactions of the Association for Computational Linguistics, pages 327–338, 2014. URL:\nhttps://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/388.\n56\nPer Martin-Löf and Giovanni Sambin. Intuitionistic type theory, volume 9. Bibliopolis Naples,\n1984.\n57\nWilliam Merrill, Yoav Goldberg, Roy Schwartz, and Noah A Smith. Provable limitations of\nacquiring meaning from ungrounded form: What will future language models understand?\narXiv preprint arXiv:2104.10809, 2021. To Appear in Transactions of the ACL.\n58\nKoji Mineshima, Ribeka Tanaka, Pascual Martínez-Gómez, Yusuke Miyao, and Daisuke\nBekki. Building compositional semantics and higher-order inference system for a wide-coverage\njapanese ccg parser. In EMNLP, 2016.\n59\nRichard Montague. English as a formal language. In Bruno Visentini, editor, Linguaggi nella\nsocieta e nella tecnica, pages 188–221. Edizioni di Communita, 1970.\nC. S. Gordon and S. Matskevich\n21\n60\nRichard Montague. Universal grammar. Theoria, 36(3):373–398, 1970.\n61\nRichard Montague. The proper treatment of quantiﬁcation in ordinary english. In Approaches\nto natural language, pages 221–242. Springer, 1973.\n62\nMichael Moortgat.\nGeneralized quantiﬁers and discontinuous type constructors.\nIn\nDiscontinuous Constituency, volume 6 of NATURAL LANGUAGE PROCESSING, pages\n181–208. Mouton de Gruyter, 1996.\n63\nMichael Moortgat. Multimodal linguistic inference. Journal of Logic, Language and Information,\n5(3):349–385, Oct 1996. doi:10.1007/BF00159344.\n64\nMichael Moortgat. Constants of grammatical reasoning. In Constraints and resources in\nnatural language syntax and semantics, pages 195–219. 1999.\n65\nRichard Moot. A type-logical treebank for french. Journal of Language Modelling, 3(1):229–264,\n2015.\n66\nGlyn Morrill. Discontinuity in categorial grammar. Linguistics and Philosophy, 18(2):175–219,\n1995.\n67\nGlyn V Morrill. Type logical grammar: Categorial logic of signs. Springer Science & Business\nMedia, 2012.\n68\nLeonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming\nlanguage. In International Conference on Automated Deduction, pages 625–635. Springer,\n2021.\n69\nTobias Nipkow, Lawrence C Paulson, and Markus Wenzel. Isabelle/HOL: a proof assistant for\nhigher-order logic, volume 2283. Springer Science & Business Media, 2002.\n70\nLalchand Pandia and Allyson Ettinger. Sorting through the noise: Testing robustness of\ninformation processing in pre-trained language models. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing, pages 1583–1596, Online and Punta\nCana, Dominican Republic, November 2021. Association for Computational Linguistics. URL:\nhttps://aclanthology.org/2021.emnlp-main.119.\n71\nBarbara H Partee and Herman LW Hendriks. Montague grammar. In Handbook of logic and\nlanguage, pages 5–91. Elsevier, 1997.\n72\nChristine Paulin-Mohring. Inductive deﬁnitions in the system coq rules and properties. In\nInternational Conference on Typed Lambda Calculi and Applications, pages 328–345. Springer,\n1993.\n73\nLawrence C Paulson. Logic and computation: interactive proof with Cambridge LCF, volume 2.\nCambridge University Press, 1990.\n74\nBenjamin C. Pierce, Chris Casinghino, Marco Gaboardi, Michael Greenberg, Catalin Hritcu,\nVilhelm Sjoberg, and Brent Yorgey. Software Foundations. 2011–2016. URL: http://cis.\nupenn.edu/~bcpierce/sf/current/index.html.\n75\nAmir Pnueli. The Temporal Logic of Programs. In FOCS. IEEE, 1977.\n76\nRobert Pollack. How to believe a machine-checked proof. In Twenty Five Years of Constructive\nType Theory, pages 205–220. Oxford University Press, 1998.\n77\nCarl Pollard and Ivan A Sag. Head-driven phrase structure grammar. University of Chicago\nPress, 1994.\n78\nAarne Ranta. Intuitionistic categorial grammar. Linguistics and Philosophy, 14(2):203–239,\n1991.\n79\nAarne Ranta. Type-theoretical Grammar. Oxford University Press, Inc., New York, NY, USA,\n1994.\n80\nAarne Ranta. Context-relative syntactic categories and the formalization of mathematical text.\nIn International Workshop on Types for Proofs and Programs, pages 231–248. Springer, 1995.\n81\nAarne Ranta. Grammatical framework. Journal of Functional Programming, 14(2):145–189,\n2004.\n82\nJohn C Reynolds. Separation logic: A logic for shared mutable data structures. In Proceedings\nof the 17th Annual IEEE Symposium on Logic in Computer Science, LICS 2002, pages 55–74.\nIEEE, 2002.\n22\nNatural Language Speciﬁcations in Proof Assistants\n83\nHiroyuki Seki, Tadao Kasami, Eiji Nabika, and Takashi Matsumura. A method for translating\nnatural language program speciﬁcations into algebraic speciﬁcations. Systems and computers\nin Japan, 23(11):1–16, 1992.\n84\nHiroyuki Seki, Eiji Nabika, Takashi Matsumura, Yujii Sugiyama, Mamoru Fujii, Koji Torii,\nand Tadao Kasami. A processing system for programming speciﬁcations in a natural language.\nIn [1988] Proceedings of the Twenty-First Annual Hawaii International Conference on System\nSciences. Volume II: Software track, volume 2, pages 754–763. IEEE, 1988.\n85\nDaniel Selsam, Sebastian Ullrich, and Leonardo de Moura. Tabled typeclass resolution. arXiv\npreprint arXiv:2001.04301, 2020.\n86\nMatthieu Sozeau and Nicolas Oury. First-class type classes. In International Conference on\nTheorem Proving in Higher Order Logics, pages 278–293. Springer, 2008.\n87\nMark Steedman. The Syntactic Process. The MIT Press, 2001.\n88\nMark Steedman. Taking scope: The natural semantics of quantiﬁers. Mit Press, 2012.\n89\nGöran Sundholm. Proof theory and meaning. In Handbook of philosophical logic, pages 471–506.\nSpringer, 1986.\n90\nSimon Thompson. Type Theory and Functional Programming. Addison-Wesley, 1999. URL:\nhttp://www.cs.kent.ac.uk/people/staff/sjt/TTFP/.\n91\nAaron Traylor, Roman Feiman, and Ellie Pavlick. And does not mean or: Using formal\nlanguages to study language models’ representations. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics, 2021.\n92\nJohan van Benthem. Categorial grammar and type theory. Journal of Philosophical Logic,\n19(2):115–168, 1990. URL: http://www.jstor.org/stable/30226424.\n93\nAdrienne Wang, Tom Kwiatkowski, and Luke Zettlemoyer.\nMorpho-syntactic lexical\ngeneralization for ccg semantic parsing. In Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 1284–1295, 2014.\n94\nMarkus Wenzel. Isar—a generic interpretative approach to readable formal proof documents.\nIn International Conference on Theorem Proving in Higher Order Logics, pages 167–183.\nSpringer, 1999.\n95\nJeannette M Wing. A speciﬁer’s introduction to formal methods. Computer, 23(9):8–22, 1990.\n96\nYuhao Zhang, Aws Albarghouthi, and Loris D’Antoni. Certiﬁed robustness to programmable\ntransformations in LSTMs. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 1068–1083, Online and Punta Cana, Dominican Republic,\nNovember 2021. Association for Computational Linguistics. URL: https://aclanthology.\norg/2021.emnlp-main.82.\n",
  "categories": [
    "cs.PL",
    "cs.CL"
  ],
  "published": "2022-05-16",
  "updated": "2022-05-16"
}