{
  "id": "http://arxiv.org/abs/2205.08034v1",
  "title": "DeepSim: A Reinforcement Learning Environment Build Toolkit for ROS and Gazebo",
  "authors": [
    "Woong Gyu La",
    "Lingjie Kong",
    "Sunil Muralidhara",
    "Pratik Nichat"
  ],
  "abstract": "We propose DeepSim, a reinforcement learning environment build toolkit for\nROS and Gazebo. It allows machine learning or reinforcement learning\nresearchers to access the robotics domain and create complex and challenging\ncustom tasks in ROS and Gazebo simulation environments. This toolkit provides\nbuilding blocks of advanced features such as collision detection, behaviour\ncontrol, domain randomization, spawner, and many more. DeepSim is designed to\nreduce the boundary between robotics and machine learning communities by\nproviding Python interface. In this paper, we discuss the components and design\ndecisions of DeepSim Toolkit.",
  "text": "DeepSim: A Reinforcement Learning Environment\nBuild Toolkit for ROS and Gazebo\nWoong Gyu La\nLingjie Kong\nSunil Muralidhara\nPratik Nichat\nAmazon Web Services\n{woong,lingjik,murasuni,pnichat} @ amazon.com\nAbstract\nWe propose DeepSim 1, a reinforcement learning environment build toolkit for\nROS and Gazebo. It allows machine learning or reinforcement learning researchers\nto access the robotics domain and create complex and challenging custom tasks in\nROS and Gazebo simulation environments. This toolkit provides building blocks\nof advanced features such as collision detection, behaviour control, domain ran-\ndomization, spawner, and many more. DeepSim is designed to reduce the boundary\nbetween robotics and machine learning communities by providing Python interface.\nIn this paper, we discuss the components and design decisions of DeepSim Toolkit.\n1\nIntroduction\nRecently, reinforcement learning (RL) research has made multiple breakthroughs using simulation\nplatforms such as Arcade Learning Environment [1], VizDoom [2], MuJoCo [3], Malmo [4], and\nmany others. However, these platforms started to fall behind in meeting the current demand as\nbenchmarks on these platforms are becoming less differentiable. Researchers are looking for more\npractical and challenging environments to push the boundary of RL research.\nIn robotics communities, ROS [5] and Gazebo [6] are used as foundation layers to build robotic\ncontrol frameworks in both simulation and real world applications. For a simple robot simulation,\nthe current Gazebo and ROS stack satisfy the researchers’ needs. However, since the Gazebo\nfunctionalities exposed to ROS are very primitive, building more complex tasks in Gazebo requires a\ndevelopment of C++ extensions. Considering machine learning (ML) communities are more familiar\nwith Python, the requirement of the C++ extension development becomes a barrier to them. With the\nsupport of advanced features similar to Gazebo C++ plugin extension through a Python interface, it\nallows ML or RL researchers more approachable to Gazebo and ROS foundation work.\nTo bridge robotics and ML communities, we present DeepSim, a reinforcement learning environment\nbuild toolkit for ROS and Gazebo. It provides the building blocks of advanced features such as\ncollision detection, behaviour controls, domain randomization, spawner, and many more to build\na complex and challenging reinforcement learning environment in ROS and Gazebo simulation\nenvironments with Python language.\n2\nRelated Work\nIn this section, we describe the related work of DeepSim. Robotics communities grew with Gazebo\nand ROS platform. DeepSim framework is built to bring ML communities to the robotics domain to\nsolve the complex and practical problems with ML. It allows the ML researchers to create new tasks\nin Gazebo and ROS platforms with minimum efforts.\n1https://github.com/aws-deepracer/deepsim\narXiv:2205.08034v1  [cs.LG]  17 May 2022\nDeepSim Toolkit\nROS\nDeepSim Gazebo Plugin\nDeepSim Framework\nDeepSimEnvs\nDeepSim Behaviour Tree\nGazebo\nUDE\nRL Policy Trainer\nDeepSim Core\nTrackers\nBehaviour\nColliders\nSpawners\nVisual Effects\nDomain\nRandomizer\nFigure 1: DeepSim Toolkit’s high-level component architecture demonstrating the integration with\nexternal tools such as Gazebo and Uniﬁed Distributed Environment (UDE).\n2.1\nGazebo\nGazebo [6] is an open-source 3D simulator for robotics applications. Gazebo was initially a compo-\nnent of the Player Project from 2004 to 2011. In 2012, Open Source Robotics Foundation (OSRF)\nstarted supporting Gazebo as an independent project.\nGazebo integrates physics engines such as Open Dynamics Engine (ODE) [7], Bullet [8], Simbody\n[9], and Dynamic Animation and Robotics Toolkit (DART) [10]. A physical model described by\nSimulation Description Format (SDF) or Uniﬁed Robotic Description Format (URDF) ﬁle using\nXML format can be loaded by each of these physics engines. Gazebo also utilizes OGRE as its\nrendering engine to render robot and environment models, and to process the visual sensors.\nGazebo provides various types of sensors such as camera, lidar, and many others that simulate the\nexisting physical sensors. Instantiated sensors listen to world state changes from a physics simulator\nand output the results. Also, Gazebo allows users to come up with their own world, model, sensor,\nsystem, visual, and GUI plugins by implementing C++ Gazebo extensions. This capability enables\nusers to extend the simulator further into more complex scenarios.\n2.2\nROS\nRobot Operating System (ROS) [5] is an open-source software framework for robot software de-\nvelopment maintained by OSRF. ROS is a widely used middleware by the robotics researchers to\nleverage the communication between different modules in a robot and between different robots, and\nto maximize the re-usability of robotics code from simulation to the physical devices.\nROS allows to run different device’s modules as a node and provide multiple different types of com-\nmunication layers between the nodes such as service, publisher-subscriber, and action communication\nmodels to satisfy different purposes. This allows robotics developers to encapsulate, package, re-use\neach of the modules independently. Furthermore, it allows each module to be used in both simulation\nand physical device without any modiﬁcation.\n3\nDeepSim Toolkit\nIn this section, we describe the design of our proposed toolkit. DeepSim (Figure 1) consists of 4\ndifferent components, DeepSimEnvs, Gazebo Plugin, DeepSim Framework, and Behaviour Tree to\nassist in building complex tasks in the ROS and Gazebo platform with Python. Each component is\nexplained in more detail in the following sections.\n3.1\nDeepSim Gazebo Plugin\nOSRF provides a bridge between Gazebo and ROS with gazebo_ros plugin package. However,\ngazebo_ros has a critical limitation that it can only process single retrieval or modiﬁcation request\nof one model or link state at a time due to the communication using service model [5] over TCP.\nTherefore, in order to get or set multiple model or link states, the time linearly increases with the\nnumber of get and set requests due to the network overhead and the nature of synchronous process in\ngazebo_ros plugin. DeepSim Gazebo Plugin overcomes this limitation by introducing multi-state set\n2\nDeepSim Framework\nBehaviour\nTransform\nTrackers\n\n- GetModelStateTracker\n- GetLinkStateTracker\n- GetVisualTracker\n- SetModelStateTracker\n- SetLinkStateTracker\n- SetVisualTracker\nGazebo\nROS\nGazebo Model\nDeepSim Gazebo\nPlugin\nFigure 2: Illustration of data ﬂow from Gazebo model to behaviour’s transform.\nor get services through a single request (see Appendix A for the usage comparison). DeepSim Gazebo\nPlugin is designed to process an arbitrary number of model states or link states in a single request.\nThis design minimizes the network overhead regardless of the number of state change requests.\nAs illustrated in Figure 2, DeepSim Gazebo Plugin component provides the data communication\nbetween DeepSim Framework and Gazebo in primitive form by using ROS communication layer.\nThe component provides the information from Gazebo through both service and topic model. It\npublishes all model, link, and visual states’ information periodically through publisher-subscriber\ntopics [5]. Lastly, all model, link, and visual states’ modiﬁcation can be requested through service\nrequest.\n3.2\nDeepSim Framework\nDeepSim Gazebo Plugin supports retrieval and modiﬁcation of simulation raw state information such\nas model, link, and visual state data. However, in many cases, solely simulation state information\nretrieval and modiﬁcation are not sufﬁcient to create a challenging task with a complex simulation\nenvironment setting. In order to create more complex tasks, it often requires functionalities including\nbut not limited to ﬁne tuned object movement control, dynamic spawning of objects, waypoint\nnavigation, and segmentation map. From simple primitive state data retrieved with DeepSim Gazebo\nPlugin, DeepSim Framework provides advanced high-level features such as behaviour control,\ncollision detection, spawners, domain randomization and many more to support these needs.\n3.2.1\nDeepSim Core\nDeepSim Core contains basic game mathematics modules and primitive Gazebo entity modules to\nprovide a foundation for advanced game engine features. It can also be used for any custom usage\nby the users. DeepSim Core provides frequently used mathematics such as vector, quaternion, and\neuler. For object management and Gazebo synchronization, link_state, model_state, point, pose,\ntwist, color, material, and visual modules are provided. Some of the advanced game mathematics\nsuch as ray, plane, and frustum are provided to support advanced game features like ray-casting,\nculling, or creating segmentation map. Lastly, DeepSim Framework provides lerp and lerp_angle for\nquick linear interpolation functionalities, which are often used in camera or model movement control.\n3.2.2\nTrackers\nThe tracker is an object that is registered under the tracker manager. Each tracker gets invoked per\nGazebo simulation time-step update. Tracker manager subscribes to clock topic to get a callback\ninvoked on every simulation time-step update. During the callback, the tracker manager invokes\nupdate_tracker method to every tracker that it manages. As illustrated in Figure 3, the callback is\ncalled based on the level of priority group, HIGH, NORMAL, and LOW. The trackers in the HIGH\npriority group are called initially, then the trackers with the NORMAL priority group, and the trackers\nwith the LOW priority group lastly.\nThe tracker manager is registered with default getter and setter trackers for link, model, and visual\nstates in Gazebo. Getter trackers retrieve all information from Gazebo at the time of update_tracker.\nSetter trackers collect all set requests for models, links, and visuals from previous update till the next\nupdate_tracker call, and make a single request to Gazebo simulator through DeepSim Gazebo Plugin.\n3\nTrackerManager\nHigh Priority\n\n- GetModelStateTracker\n- GetLinkStateTracker\n- GetVisualTracker\nNormal Priority\n\n- BehaviourManager\n- VisualEffectManager\nLow Priority\n\n- SetModelStateTracker\n- SetLinkStateTracker\n- SetVisualMaterialTracker\n- SetVisualVisibleTracker\n- SetVisualTransparencyTracker\n/clock\nGazebo\nDeepSim Update Loop\nBehaviourManager\nBehaviour\nBehaviour\nBehaviour\nBehaviour\nfixed_update\nFigure 3: DeepSim Framework’s update loop start-\ning from Gazebo’s simulation clock tick update.\nGazebo\nDeepSim Gazebo Plugin\nROS\nDeepSim Framework\nDeepSimEnv\nUDE\nRL Policy Trainer\nRL Task\nDeepSim Behaviour Tree\nFigure 4: Complete DeepSim component\nstack illustrated from Gazebo simulator to\nagent policy trainer.\nGetter trackers, which synchronize all the latest information in Gazebo to DeepSim Framework, are\npart of the HIGH priority group. Setter trackers, which will synchronize back all the information\nmodiﬁed back to Gazebo at the end of the update loop, have LOW priority. Consequently, the update\nloop starts by synchronizing all the information from Gazebo simulator, and all the updates made\nduring the current update cycle will be synchronized back to Gazebo at the end of the update loop.\n3.2.3\nBehaviour\nBehaviour represents a basic object entity (or model in Gazebo term) in DeepSim Toolkit. For each of\nbehaviour objects, a transform is instantiated to maintain and synchronize the properties with Gazebo\nsimulator through getter and setter trackers. All states, updated in Gazebo simulator, are automatically\nsynchronized to transform, and any new properties conﬁgured with transform are synchronized back\nto Gazebo simulator as shown in Figure 2. The behaviour also takes in a spawner to allow the user\nto control the life-cycle of the object through behaviour module. All behaviours are automatically\nmanaged by behaviour_manager. The behaviour_manager is implemented in singleton pattern to\nprovide the access to behaviour objects in any place in the code.\nThe behaviour must provide the name and the tag. The name and the tag can be used to retrieve\nthe behaviour object from behaviour_manager. This is especially useful when the user tries to\noperate on multiple entities with the same tag. Lastly, behaviour supports two types of update,\nupdate and ﬁxed_update. The update is either invoked manually or through the environment step,\nbut ﬁxed_update is invoked every Gazebo simulation time-step as shown in Figure 3. Therefore,\nthe behaviour controls can be updated with update method, and the update that requires smooth\ninterpolation such as model or camera movement, can be updated using ﬁxed_update.\n3.2.4\nColliders\nThe collisions are automatically processed by Gazebo simulation through an integrated physics\nengine, but such information is not accessible unless the user creates a custom Gazebo plugin to\nretrieve such information. Compared to the actual collision simulation, the access to manual collision\ndetection functionality is often useful to create complex tasks in the simulation environment such\nas checking model collision with way-points to track the progress, offtrack detection, or creating\nsegmentation map. DeepSim Framework provides frequently used 2D and 3D collider types for\n4\nDeepSim Behaviour Tree\nComposite\nParallel Selector\nSelector\nRandom Selector\nSequence\nParallel Sequence\nRandom Sequence\nDecorator\nCondition\nLimit\nRepeater\nInverter\nSucceeder\nUntil Fail\nRunningIsFailure RunningIsSuccess\nFailureIsSuccess\nFailureIsRunning\nSuccessIsRunning SuccessIsFailure\nBT Behaviour\nSuccess\nFailure\nRunning\nFigure 5: The behaviour tree nodes provided by DeepSim Behaviour Tree component.\ngeneral use. For 2D, rectangle, circle, and geometry colliders are supported. Moreover, box and\nsphere colliders are supported in 3D.\nColliders can be attached to a transform to track and automatically update its pose. Colliders also\nallow to set pose offset to provide more complex pose conﬁguration. Colliders provide three main\ntest methods, intersects, contains, and raycast. The intersects returns the ﬂag of whether the collider\nintersects with a given collider or point. The contains returns the ﬂag of whether the collider contains\na given collider or point. Lastly, raycast provides hit information with a given ray object. 2D colliders\nare computed by using Shapely [11], and 3D colliders are computed by employing the mathematics\nmodules from DeepSim Core.\n3.2.5\nSpawners\nDeepSim Framework requires a spawner implementation for each of behaviour types to control the\nlife-cycle of relevant models in Gazebo simulator. The spawner must implement spawn and delete\nfunctions to create and delete the model in the Gazebo simulator respectively. DeepSim Framework\nalso provides two helper modules for the spawner — GazeboXMLLoader and GazeboModelSpanwer.\nGazeboXMLLoader parses and loads SDF or URDF ﬁles in XML or Xacro format. Further Gazebo-\nModelSpanwer provides a simple interface to spawn and delete SDF or URDF models in Gazebo\nsimulator.\n3.2.6\nVisual Effects and Domain Randomizers\nDeepSim Framework provides a feature to manage visual effects. All visual effect implementations\nare automatically managed by the effect manager. When an effect is attached, it is automatically\nadded to the effect manager, and the effect manager is responsible to update each of the effect\nimplementations until the effects are detached. Effect manager is implemented as a tracker for its\ncallback function to be invoked every Gazebo simulation time-step update. Per update, the effect\nmanager will propagate the update call to every effect implementation that it manages. DeepSim\nFramework provides two out of the box visual effects — blink and invisible. The user can further\nextend the abstract effect class to create their own custom visual effects.\nDomain randomizer feature is supported by DeepSim Framework to apply domain randomization to\nthe model that is spawned in Gazebo simulator. DeepSim Framework supports two types of domain\nrandomizer: model visual randomizer and light randomizer. Model visual randomizer allows to\nrandomize the color in different levels such as model, link, and visual. Also, it supports the color\nrange to choose from, the number of units to randomize for each randomization step, and the ﬁlters to\napply randomization. The light randomizer also supports the range of color to choose with attenuation\nranges, constant, linear, and quadratic values. DeepSim Framework allows the user to extend and\ncreate their own custom randomizer by subclassing an abstract randomizer interface.\n3.3\nDeepSim Behaviour Tree\nBehaviour tree is popularly used in the game industry to model the behaviour of an autonomous agent\nor virtual entity in simulations or games [12,13]. Behaviour tree overcomes many limitations from\nFinite State Machine (FSM) approach in terms of maintainability, scalability, and re-usability. While\n5\nbehaviour tree is popularly used in the game industry, not many behaviour tree implementations\nare available in Python language. Based on our knowledge, there are no such behaviour tree\nimplementations in Python language that are light-weighted and extendable.\nDeepSim Behaviour Tree is provided as a component of DeepSim Toolkit to support advanced\nbehaviour designing for the agents or object entities. The component is very light-weighted and\nthe nodes can be extended to create new reusable behaviour nodes. As DeepSim Behaviour Tree\nis a stand-alone component, which does not depend on any other DeepSim Toolkit’s components,\nDeepSim Behaviour Tree can be solely installed in any Python environment without other DeepSim\nToolkit’s components.\nComparable to other behaviour tree implementations, DeepSim Behaviour Tree also traverses and\nexecutes the nodes from the root with tick method call. For every tick operation, it returns one of\nfour status — SUCCESS, FAILURE, RUNNING, and INVALID, which indicates the execution was\nsuccessful, failed, running, and invalid respectively. BT Behaviour represents a base node of the\nbehaviour tree, and all behaviour tree nodes must be derived from BT Behaviour. As illustrated in\nFigure 5, DeepSim Behaviour Tree also provides some useful extensions for each of the behaviour\ntree node types — Leaf, Decorator, and Composite. For further custom behaviour outside of what is\nprovided, users can extend DeepSim Behaviour Tree nodes according to their needs.\n3.3.1\nLeaf and Decorator\nLeaf node is a type of behaviour tree nodes without any child. It executes and returns its status back\nto its parent node. The leaf node often represents an entity’s action or its status check. DeepSim\nBehaviour Tree provides three leaf nodes out of the box — Success, Failure, and Running. As their\nname states, on execution, the nodes just return the status that match with their names, where Success\nalways returns SUCCESS, Failure always returns FAILURE, and lastly Running always returns\nRUNNING.\nDecorator node is a node that only has a single child node. It is often used to manipulate the return\nstatus of its child node, or control the behaviour of its child node. DeepSim Behaviour Tree provides\na handful of pre-deﬁned decorator nodes such as Condition, Limit, Repeater, Inverter, Succeeder,\nUntilFail, and status manipulators. Condition returns SUCCESS if the child node returns the targeted\nstatus otherwise returns FAILURE. Limit only allows it to execute its child node only up to the deﬁned\nnumber of ticks and returns the status from the child node unless the limit is reached. Repeater repeats\nits child node execution up to the input number. Inverter inverts the status of the child node returned,\nSUCCESS to FAILURE and FAILURE to SUCCESS. Succeeder always returns SUCCESS regardless\nof its child’s status. UntilFail executes its child node until the child node returns a FAILURE. Lastly,\nstatus manipulators such as RunningIsFailure, RunningIsSuccess, FailureIsSuccess, FailureIsRunning,\nSuccessIsRunning, and SuccessIsFailure, manipulate and propagate upward the status of its child\naccording to the mapping deﬁned in its node name.\n3.3.2\nComposite\nComposite node is a node that contains more than one node to execute. DeepSim Behaviour Tree\nsupports default composite types — selector and sequence. Selector will execute its children in\norder and will return SUCCESS on ﬁrst SUCCESS from its child’s execution. If all of its children\nfail, then it returns a FAILURE statue. Sequence will execute its children in order and will only\nreturn SUCCESS when all its children return SUCCESS. If any of its child nodes fails, then it returns\nFAILURE status. On top of these default composite node types, DeepSim Behaviour Tree also\nprovides parallel sequence, parallel selector, random sequence, and random selector for variation of\nbehaviour control.\nParallel sequence executes all child nodes in parallel, and if all child nodes succeed then it stops with\na SUCCESS status. If any child node fails, then it stops with a FAILURE status. Parallel selector also\nexecutes all child nodes in parallel, and if any child node succeeds, then it stops with a SUCCESS\nstatus. If all child nodes fail, then it stops with a FAILURE status. Random selector and sequence\noperate similarly to original selector and sequence, but the order of the child nodes’ execution are\nshufﬂed for each new execution.\n6\nFigure 6: The average time of get and set model state synchronization request for both DeepSim\nGazebo Plugin and OSRF gazebo_ros Plugin\n3.4\nDeepSimEnvs\nMost of the components in DeepSim Toolkit are focused on quickly building new environment\ndynamics and creating complex tasks. Meanwhile, DeepSimEnvs component is focused on providing\na quick adaptation to release newly created environments as reinforcement learning environments.\nThe environment developers can implement two simple interfaces, agent and area, to make their\nenvironment to be released as a Uniﬁed Distributed Environment (UDE) [14] compatible environment\n(see Appendix B for the environment usage). The complete DeepSim component stack diagram from\nGazebo to agent policy trainer is illustrated in Figure 4.\n4\nExperiments\nWe evaluated our two synchronization methods, get and set, between the environment application\nand Gazebo over ROS using DeepSim Toolkit Gazebo Plugin and OSRF gazebo_ros plugin. As\ndiscussed in Section 3.1, OSRF gazebo_ros plugin can only process single object get and set state per\nservice request. Our DeepSim Gazebo Plugin overcomes this limitation by supporting get and set for\nmultiple object states through a single service request.\nWe measured the time performance of single scene synchronization where the scene contains multiple\nobjects. We evaluated the performance of synchronization by incrementing ten objects up to one\nhundred objects to understand the impact. For each of the synchronization calls, all objects in the\nscene are retrieved through get method, and all objects positions are updated through set method. We\nmeasured the total time of get or set methods during each of scene synchronization.\nIn Figure 6, we present the average time of 5,000 synchronization requests using DeepSim Gazebo\nPlugin and OSRF gazebo_ros Plugin. The solid curves correspond to the average time and the shaded\nregions correspond to the standard deviation of the request time over 5,000 synchronization requests.\nDeepSim Gazebo Plugin provides a consistent average time regardless of number of objects in the\nscene, while gazebo_ros plugin’s time increases signiﬁcantly with respect to the number of objects in\nthe scene. This shows that the DeepSim Gazebo Plugin can efﬁciently synchronize the objects in the\nscene with the minimum network latency over OSRF gazebo_ros plugin.\n5\nConclusion\nIn this paper, we described the design of the DeepSim Toolkit to build a new complex and challenging\ntask in ROS and Gazebo with Python. We have designed DeepSim Toolkit to help on-board ML\nresearchers to robotics platforms, and allow them to create more practical, complex, and challenging\ntasks such as autonomous driving or robotic manipulation to further expand the robotics research\nwith ML and RL. We hope this toolkit brings new initiatives for both robotics and machine learning\nresearch\n7\nReferences\n[1] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An\nevaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,\nJun 2013.\n[2] Marek Wydmuch, Michał Kempka, and Wojciech Ja´skowski. Vizdoom competitions: Playing\ndoom from pixels. IEEE Transactions on Games, 2018.\n[3] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages\n5026–5033, 2012.\n[4] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for\nartiﬁcial intelligence experimentation. In Proceedings of the Twenty-Fifth International Joint\nConference on Artiﬁcial Intelligence, IJCAI’16, page 4246–4247. AAAI Press, 2016.\n[5] Morgan Quigley, Josh Faust, Tully Foote, Jeremy Leibs, et al. Ros: an open-source robot\noperating system.\n[6] Nathan Koenig and Andrew Howard. Design and use paradigms for gazebo, an open-source\nmulti-robot simulator. In 2004 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS)(IEEE Cat. No. 04CH37566), volume 3, pages 2149–2154. IEEE, 2004.\n[7] Russell Smith et al. Open dynamics engine. 2005.\n[8] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,\nrobotics and machine learning. http://pybullet.org, 2016–2021.\n[9] Michael A Sherman, Ajay Seth, and Scott L Delp. Simbody: multibody dynamics for biomedical\nresearch. Procedia Iutam, 2:241–261, 2011.\n[10] Jeongseok Lee, Michael X Grey, Sehoon Ha, Tobias Kunz, Sumit Jain, Yuting Ye, Siddhartha S\nSrinivasa, Mike Stilman, and C Karen Liu. Dart: Dynamic animation and robotics toolkit.\nJournal of Open Source Software, 3(22):500, 2018.\n[11] Sean Gillies et al. Shapely: manipulation and analysis of geometric objects, 2007–.\n[12] Michele Colledanchise and Petter Ögren. Behavior trees in robotics and ai. Jul 2018.\n[13] Michele Colledanchise and Petter Ögren. How behavior trees modularize hybrid control systems\nand generalize sequential behavior compositions, the subsumption architecture, and decision\ntrees. IEEE Transactions on robotics, 33(2):372–389, 2016.\n[14] Woong Gyu La, Sunil Muralidhara, Lingjie Kong, and Pratik Nichat. Uniﬁed distributed\nenvironment, 2022.\n8\nA\ngazebo_ros vs DeepSim Gazebo Plugin\ngazebo_ros plugin provides functionality to get and set model state as shown in the code below.\nHowever, it only allows single retrieval or modiﬁcation request of one model or link state at a time.\nimport\nrospy\nfrom\ngazebo_msgs.msg import (\nModelState ,\nPose , Quaternion , Twist\n)\nfrom\ngazebo_msgs.srv import\nSetModelState , GetModelState\n# set and get ros\nservice\nset_model_state = rospy.ServiceProxy(\n\"/gazebo/set_model_state \",\nSetModelState)\nget_model_state = rospy.ServiceProxy(\n\"/gazebo/get_model_state \",\nGetModelState)\n# single set for agent0\nstate_msg = ModelState(\"agent0\",\nPose(Position(1, 2, 3), Quaternion ()),\nTwist ())\nset_model_state(state_msg)\n# another\nset for agent1\nstate_msg = ModelState(\"agent1\",\nPose(Position(4, 5, 6), Quaternion ()),\nTwist ())\nset_model_state(state_msg)\n# get agent0 and agent1\nstate\nagent0_state = get_model_state (\"agent0\", \"\")\nagent1_state = get_model_state (\"agent1\", \"\")\nDeepSim Gazebo Plugin overcomes this limitation by introducing multi-state set and get services\nthrough a single request.\nfrom\ndeepsim\nimport (\nGetModelStateTracker , SetModelStateTracker ,\nModelState ,\nPose , Position , Quaternion , Twist\n)\n# set\nmultiple\nmodel\nstates\nthrough\nsingle\nservice\nrequest\nmodel_states = [ModelState(\"agent0\",\nPose(Position(1, 2, 3), Quaternion ()),\nTwist (),\nModelState(\"agent1\",\nPose(Position(4, 5, 6), Quaternion ()),\nTwist ()]\nSetModelStateTracker .get_instance (). set_model_states (model_states)\n# get\nmultiple\nmodel\nstates\nthrough\nsingle\nservice\nrequest\nmodel_names = [\"agent0\", \"agent1\"]\nmodel_states = \\\nGetModelStateTracker .get_instance (). get_model_states (model_names)\n9\nB\nEnvironment Usage\nDevelopers can use DeepSim Toolkit to build complex tasks by implementing AbstractAgent and\nAreaInterface. The area implementation can be passed in as an argument to create an environment.\nThis environment provides reset and step interface for reinforcement learning training and evaluation.\nfrom\ndeepsim_envs\nimport\nAbstractAgent , AreaInterface\n# Define\nrobot\nagent\nbehaviour.\nclass\nRobotAgent(AbstractAgent):\ndef\n__init__(self , name):\nself._name = name\n...\ndef\nget_next_state (self):\n# return\ncurrent\nobservation of the agent.\n...\ndef\non_action_received (self , action):\n# act upon\nreceived\naction\ncommand.\n...\n# Define\nrobot\narea\nlogic.\nclass\nRobotArea(AreaInterface ):\ndef\n__init__(self):\nself._agents = [RobotAgent(\"agent0\"), RobotAgent(\"agent1\")]\ndef\nget_agents(self):\nreturn\nself._agents\ndef\nget_info(self):\n# return\narea info.\n...\ndef reset(self):\n# reset\narea.\n...\ndef\nobservation_space (self):\n# return\nagents ’ observation\nspaces\n...\ndef\naction_space(self):\n# return\nagents ’ action\nspaces\n...\nfrom\ndeepsim_envs\nimport\nEnvironment\n# Instantiate\nUDE\ncompatible\nenvironment.\nrobot_area = RobotArea ()\nenv = Environment(area=robot_area)\n# env\nprovides a unified\ninterface\nfor reset , and step.\nenv.reset ()\nfor _ in range(100):\n# sample\nrandom\nactions\nfor agent0 and agent1\naction_dict = {\"agent0\": env.action_space[\"agent0\"].sample (),\n\"agent1\": env. action_space [\"agent1\"].sample ()}\n# submit\nagents ’ next\nactions\nand\nretrieve\nobservation , reward ,\n# done , and action of all agents\nalong\nwith\nenvironment\n# information.\nstate , reward , done , action , info = env.step(action_dict)\n10\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2022-05-17",
  "updated": "2022-05-17"
}