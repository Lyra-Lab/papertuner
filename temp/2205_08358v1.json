{
  "id": "http://arxiv.org/abs/2205.08358v1",
  "title": "Perturbation of Deep Autoencoder Weights for Model Compression and Classification of Tabular Data",
  "authors": [
    "Manar Samad",
    "Sakib Abrar"
  ],
  "abstract": "Fully connected deep neural networks (DNN) often include redundant weights\nleading to overfitting and high memory requirements. Additionally, the\nperformance of DNN is often challenged by traditional machine learning models\nin tabular data classification. In this paper, we propose periodical\nperturbations (prune and regrow) of DNN weights, especially at the\nself-supervised pre-training stage of deep autoencoders. The proposed weight\nperturbation strategy outperforms dropout learning in four out of six tabular\ndata sets in downstream classification tasks. The L1 or L2 regularization of\nweights at the same pretraining stage results in inferior classification\nperformance compared to dropout or our weight perturbation routine. Unlike\ndropout learning, the proposed weight perturbation routine additionally\nachieves 15% to 40% sparsity across six tabular data sets for the compression\nof deep pretrained models. Our experiments reveal that a pretrained deep\nautoencoder with weight perturbation or dropout can outperform traditional\nmachine learning in tabular data classification when fully connected DNN fails\nmiserably. However, traditional machine learning models appear superior to any\ndeep models when a tabular data set contains uncorrelated variables. Therefore,\nthe success of deep models can be attributed to the inevitable presence of\ncorrelated variables in real-world data sets.",
  "text": "PERTURBATION OF DEEP AUTOENCODER WEIGHTS FOR\nMODEL COMPRESSION AND CLASSIFICATION OF TABULAR\nDATA\nManar D. Samad and Sakib Abrar\nDepartment of Computer Science\nTennessee State University\nNashville, TN, USA\nmsamad@tnstate.edu\nMay 18, 2022\nABSTRACT\nFully connected deep neural networks (DNN) often include redundant weights leading to overﬁt-\nting and high memory requirements. Additionally, the performance of DNN is often challenged by\ntraditional machine learning models in tabular data classiﬁcation. In this paper, we propose periodi-\ncal perturbations (prune and regrow) of DNN weights, especially at the self-supervised pre-training\nstage of deep autoencoders. The proposed weight perturbation strategy outperforms dropout learn-\ning in four out of six tabular data sets in downstream classiﬁcation tasks. The L1 or L2 regularization\nof weights at the same pretraining stage results in inferior classiﬁcation performance compared to\ndropout or our weight perturbation routine. Unlike dropout learning, the proposed weight pertur-\nbation routine additionally achieves 15% to 40% sparsity across six tabular data sets for the com-\npression of deep pretrained models. Our experiments reveal that a pretrained deep autoencoder with\nweight perturbation or dropout can outperform traditional machine learning in tabular data classi-\nﬁcation when fully connected DNN fails miserably. However, traditional machine learning models\nappear superior to any deep models when a tabular data set contains uncorrelated variables. There-\nfore, the success of deep models can be attributed to the inevitable presence of correlated variables\nin real-world data sets.\nKeywords weight pruning, autoencoder, tabular data, model compression, sparse learning\n1\nINTRODUCTION\nThe recent advancement in artiﬁcial intelligence is primarily attributed to the deep learning of image, video, and audio\ndata [1]. The majority of deep learning studies propose different variants of convolutional neural networks (CNNs)\nto demonstrate image classiﬁcation performance. The prevalence of CNN is because of the convolution operation\nwithin neural networks that can extract highly discriminating patterns in data with spatial regularity (e.g., images,\naudio). Furthermore, the hierarchical patterns in images are sequentially captured in multiple layers of CNN, where\nthe ﬁnal convolutional layer yields the most semantic features for image classiﬁcation. These achievements of CNN\novershadow the performance of deep learning on data without such spatial regularity or hierarchical patterns. The\ndata mining literature broadly categorizes data into three types: 1) imaging data, 2) text data, and 3) tabular data\n[2, 3]. Tabular data often do not contain hierarchical patterns or spatial or temporal regularity to effectively leverage\nthe feature extraction mechanisms of popular convolutional or recurrent neural networks (RNN) [4, 5]. In general,\ntabular data sets have received much less attention in the deep learning literature than imaging or text data. There\nare studies reporting the superiority of traditional ensemble learning methods, such as gradient boosting trees, over\ndeep learning approaches in mining tabular data sets [6, 7]. One example of tabular data is electronic medical records\n(EMR), a structured collection of patients’ vitals and lab measurements. Our experience with large EMR data sets\nreveals that traditional ensembles of decision tree classiﬁers outperform FC-DNN in all-cause mortality prediction [8].\narXiv:2205.08358v1  [cs.LG]  17 May 2022\nA PREPRINT - MAY 18, 2022\nThis suboptimal performance of FC-DNN is burdened with high computational cost, lack of model explainability, and\nlarge memory requirements, which deter many domain researchers (e.g., health scientists) from effectively utilizing\nthe true strengths of deep learning.\nThe suboptimal performance of deep learning on tabular data may be due to 1) poor feature representations and\nstatistical assumptions on variables, 2) overﬁtting due to ﬂexibility of deep learning with limited sample size, and 3)\nredundancy in deep architecture. First, an FC-DNN can be pretrained as a self-supervised autoencoder to enhance the\nfeature representation without involving classiﬁcation labels. The pre-trained model is then ﬁne-tuned to perform a\ntarget classiﬁcation task. This transfer learning approach is known to be comparatively more effective than training\nFC-DNN from scratch. Second, the overﬁtting problem of neural networks is tackled using dropout learning and\nweight regularization [9, 10]. Third, the redundancy in fully connected deep models is lessened via numerous weight\npruning techniques, creating a sparse model. A uniﬁed framework that combines the objectives of feature learning,\nmodel regularization, and compression to augment the performance of deep learning of tabular data can be valuable.\nThis paper proposes an algorithm to periodically perturb weights of deep autoencoders to simultaneously yield com-\npressed pretrained models and improved feature representation for downstream classiﬁcation tasks. The proposed\nperturbation routine involves periodic pruning and growing of weights at the time of self-supervised model pretrain-\ning. It is noteworthy that standard weight pruning methods are performed monotonically, conceding some model\naccuracy loss at the expense of introducing model compression. In contrast, we hypothesize that the perturbation of\nselective weights rather than randomly dropping neurons (dropout) or pruning weights can improve the downstream\nclassiﬁcation performance of deep models.\nThe remainder of the article is organized as follows. Section II provides a literature review on autoencoders, dropout\nlearning, and weight pruning with a mathematical realization of autoencoder-based learning. Section III discusses the\nproposed algorithm, experimental setup, and model evaluation criteria. Section IV provides the ﬁndings following the\nproposed hypotheses and experimentation. Section V highlights the major ﬁndings with explanations and limitations\nof our work. The conclusions of this study are summarized in Section VI.\n2\nLiterature review\nA general trend in the deep learning literature is to build deeper and wider learning networks with thousands to millions\nof trainable parameters for achieving state-of-the-art performance. Therefore, some model accuracy can be gained at\nthe cost of substantial computational and memory overheads at both the training and testing phases. Conversely,\na deeper model is known to be more prone to overﬁtting. It is argued that most of these computing and memory\nrequirements are redundant, which hints at the need for optimal network architectures [11, 12]. In a medical image\nclassiﬁcation study using deep CNN, only eight out of 256 (3%) image ﬁlters at the ﬁnal convolutional layer have\nyielded non-zero ﬁlter responses [13]. In other words, more than 96% of the trained ﬁlters at the ﬁnal feature extraction\nlayer appear redundant in this particular scenario, consuming substantial CPU cycles and memory spaces. Therefore,\nreducing model complexity is imperative to 1) alleviate the notorious overﬁtting problem with deep learning and 2)\nintroduce model compression for resource-restricted computing platforms. The reduction in model complexity can be\nachieved by dropping the redundant neurons (nodes) or the connection between neurons (weights).\n2.1\nDropping of neurons\nThe dropout of neurons is commonly used to mitigate overﬁtting at the time of training a deep model [14]. In dropout,\na percentage of the total neurons are deactivated to zero to discontinue their contributions to the next layer in training\nneural networks. The random dropping of neurons at each training epoch improves the model’s generalizability when\ntested on unseen test data. However, the random selection of neurons does not explain the importance of individual\nnodes to justify their inclusions or exclusions. In other words, the selection of neurons is not informed by any state or\nbehavior of the network. Conversely, Hou et al. rate individual channels (ﬁlter responses) at each layer of a deep CNN\nusing channel activation responses. This rating is used to identify and drop a subset of channels in the next layer [15].\nShen et al. have proposed a continuous dropout method that uses probability values instead of binary decisions to\ndeactivate the neurons [16]. Salehinjad and Valaee implement dropout by learning a set of binary pruning state vectors\nusing an energy loss function. Their dropout achieves more than 50% model compression at the loss of less than 5%\nof top-1 classiﬁcation accuracy [17]. Pham et al. have introduced ’AutoDropout’ to automate the process of dropout\nat all channels and layers of deep CNN [18]. AutoDropout learns the best dropout pattern to train a target network.\nIn another study, Kolbeinsson et al. perform tensor factorization of tensor regression layers of CNN on which they\napply dropout for image classiﬁcation tasks [19]. These studies suggest the general trend that proposed new variants\nof dropout learning are tested on image classiﬁcation tasks using deep CNN.\n2\nA PREPRINT - MAY 18, 2022\n2.2\nDropping of weights\nThe weights of deep neural networks are pruned to disconnect redundant connections between two neurons after\ntraining (static), or during training (dynamic) [20, 21].Weights are commonly pruned based on their magnitudes,\nwhich is known as magnitude-based pruning. However, determining an optimal threshold for the magnitude is an open\nproblem. Li et al. have proposed an optimization algorithm to determine the threshold on weight magnitude for layer-\nwise weight pruning [22]. Han et al. prune the weights of an already trained network and then ﬁnetune the remaining\nweights [23]. In contrast to magnitude-based pruning, Park et al. have proposed a score-based pruning method,\nconsidering the relationship between two consecutive layers in CNN and transforming pruning into an optimization\nproblem [24]. All these pruning methods are aimed at pruning only or sparsifying deep neural networks to enable their\nusage in resource restricted platforms.\nA more recent approach is to prune and regrow weights to gain back some of the lost model complexity due to pruning.\nGuo et al. term this as ’dynamic network surgery’ that prunes parameters during training and splices some of those\npruned weights back to retain model accuracy [25]. Lin et al. calculate the weight gradient on pruned weights, which\nis then used to update the weights in back propagation [26]. The feedback from the weight gradient prevents the\npremature elimination of weights in image classiﬁcation tasks. He et al. periodically prune image ﬁlters following\neach training epoch but allow for updating the ﬁlters subsequently [27]. Bellec et al. propose a deep rewiring method\nto periodically prune and regrow the weights during training [28]. In contrast to zero initialization of pruned weights,\nMocanu et al. prune weights at the end of each epoch but initialize them randomly to regrow subsequently [29].\nWe identify several limitations of existing studies. First, all the proposed weight pruning, pruning-and-growing meth-\nods are applied to CNN models and image ﬁlters using benchmark imaging datasets, such as MNIST, CIFAR, and\nImageNet. The terms ’DNN’ and ’CNN’ are often used interchangeably in the literature. There is little or no research\non how these methods perform on fully connected deep neural networks with tabular datasets. Second, all pruning\nmethods are reported and expected to incur some accuracy loss due to model compression. Third, Hookers et al. have\nrecently demonstrated that a pruned neural network model is more sensitive to a subset of samples and vulnerable to\nadversarial attacks [30].\n2.3\nDeep autoencoders\nDeep autoencoders are multi-layer neural networks designed to encode input data into a latent feature representation by\nself-supervised learning. The self-supervised objective function aims to decode the original input data from the latent\nfeature representation. Therefore, it can serve multiple purposes in machine learning. First, the model can be used\nto reconstruct the actual data from corrupt data inputs [31]. Second, the latent feature representation of autoencoders\nmay capture the data manifold in low dimensions for subsequent classiﬁcation or clustering tasks. Third, the self-\nsupervised mechanism of autoencoders alleviates the need for data labels for feature extraction because data labels are\noften expensive to acquire. Fourth, autoencoders can pretrain a DNN in a self-supervised way using a large data set\nwithout data labels. The pre-trained model can be ﬁne-tuned for a given classiﬁcation task with limited data labels.\nTherefore, an autoencoder enables a more effective transfer of knowledge and weight initialization than training a\ndeep model from scratch. Similar to other machine learning models, autoencoders are mainly studied with image data,\nsuch as image reconstruction[32, 33], and image denoising[34, 31]. Autoencoders are proposed in studies on natural\nlanguage generation, translation, and comprehension using text data sets [35].\n2.4\nContributions\nIn reference to the literature review, the contributions of this paper are as follows. First, this study is one of the ﬁrst\nto extensively evaluate the effect of weight pruning on deep autoencoders in learning tabular data sets. Second, a new\nweight perturbation routine is proposed that periodically performs weight pruning and regrowing at the deep model’s\npretraining stage. Third, the weight perturbation routine is modeled to simultaneously achieve model compression\nand improved feature learning instead of trading one for the other. Fourth, weight perturbation is compared against\npopular regularization of neurons and weights (e.g., dropout, weight regularization) using high-dimensional tabular\ndata sets from six research domains. Fifth, we compare the classiﬁcation performances between deep (with and without\nperturbation) and traditional machine learning to explain when deep learning of tabular data sets fails.\n3\nBackground review\nAutoencoders use a self-supervised mechanism to ﬁrst encode (encoder) input data (X∈ℜd) into a new feature em-\nbedding (Z∈ℜm) where d < m and d > m denote over-complete and under-complete cases, respectively. The new\n3\nA PREPRINT - MAY 18, 2022\nfeature embedding is then decoded back to an estimate ( ˆX) as close as the original input via a decoder network. The\nencoder and decoder have a set of trainable parameters θ ∈{Wθ, bθ} and Φ ∈{WΦ, bΦ}, respectively. The encoder\n(Z) and decoder ( ˆX) outputs can be formulated as below.\nZ = f(θ, X)\n(1)\nˆX = g(Φ, f(θ, X))\n(2)\nThe self-supervised objective function of autoencoders updates θ and Φ to minimize the sum of squared differences\nbetween input data (X) and the decoded output ( ˆX) across all N training samples as below.\nξ = argmin\nθ,Φ\nN\nX\ni=1\n||Xi −ˆ\nXi||2\n2.\n(3)\nThe gradient of Equation (3) with respect to θ is:\nδξ\nδθ = −2\nN\nX\ni=1\n(Xi −ˆ\nXi)δg(Φ, Zi)\nδθ\n.\n(4)\nHere, f(.) and g(.) denote sigmoid activation functions for the encoder and decoder networks, respectively.\nZ = σ(WθX + bθ)\n(5)\ng(Φ, Z) = σ(WΦZ + bΦ)\n(6)\nThe derivative of a sigmoid function is\nσ′ = σ(1 −σ).\n(7)\nApplying this derivative solution to Equations 5 and 6, we get\nδZ\nδθ\n=\nZ(1 −Z)X,\n(8)\nδg\nδθ\n=\ng(1 −g)(WΦ\nδZ\nδθ + Z).\n(9)\nThe trainable parameter θ is updated using a learning rate η as follows.\nθ′ = θ + η δξ\nδθ.\n(10)\nThe error gradient can be summarized as follows.\nδξ\nδθ = −2\nN\nX\ni=1\n(Xi −ˆ\nXi) ˆ\nXi(1 −ˆ\nXi)(WΦ\nδZi\nδθ + Zi)\n(11)\nTherefore, an autoencoder does not involve data labels for weight updates. It can learn directly from the unlabeled\ndata to preserve d-dimensional input data in an m-dimensional space. In the absence of target labels, autoencoders\nyield a non-linear dimensionality reduction and a suitable initialization of the trainable parameters (pretraining) for\nsubsequent classiﬁcation tasks.\n4\nMethodology\nThis section discusses the proposed weight perturbation method, three deep neural networks used for experimentation,\nand the experimental steps and model evaluation procedures.\n4.1\nPeriodic perturbation of weights\nWe propose a weight pruning-and-regrowing routine during model training and term it weight perturbation. There are\nseveral differences between our method and those that exist in the literature [27, 28, 29]. First, our weight perturbation\nroutine performs periodic pruning of weights at N>10 epoch intervals instead of every epoch to allow sufﬁcient time\nto regrow some of the pruned weights. Second, the pruned weights are initialized by zero for regrowing instead of\nrandom initialization. Third, weights are perturbed only several times over the entire period of training instead of\n4\nA PREPRINT - MAY 18, 2022\n® \nBinary Mask \nGeneration \nWr \nMt-1 \nM1 i \n\u0003 0 \nMt = Mt 0 Mt-1 \n•0 0 \nt = t + 1 \nYes \n@ \nWinit \nCD \n• W't\n•\nBack Prop\n(Update W) \n@No \n0 \nWeight \n0.5 \n0.3 \n1.7 \n17 \n70 \n2.4 \n-1.3\n0.3 \n2.3 \n237 \n-0.7\n3.9 \n53.7 \n4.3 \n1.4 \n23.1 \n5.3 \n3.1 \n-4.1\n0.7 \nMask \n0 \n0 \n0 \n1 \n1 \n0 \n0 \n0 \n0 \n1 \n0 \n0 \n1 \n0 \n0 \n1 \n0 \n0 \n0 \n0 \nFigure 1: Sequential steps demonstrating periodic weight perturbation at N-epoch intervals. The mask contains zero\nin positions where the weights are to be pruned. After N epochs of training, a new mask (Mt) is generated (5) from\nthe regrown weight values and then cumulatively included in the previous mask (Mt−1) by an element-wise product\n(6). The updated mask is used to prune the weights before the next back propagation (7).\nLowest \nloss\n20\n30\n40\n50\n60\n70\n80\n90\n100\nEpochs\n1\n0\n0.5\nFirst \npertubartion\nAt \nperturbation\nAfter \nperturbation\nThird \nperturbation\n10\nFigure 2: Illustration of weight perturbation schedule on an autoencoder training loss curve.\ncontinuous pruning-and-regrowing. Last but not least, our weight perturbation method is proposed for feed forward\ndeep neural networks learning tabular data rather than image ﬁlters of convolutional neural networks.\nLet W be the trained weight matrix updated via back propagation for N epochs. After N epochs, we obtain a binary\nmask for the ﬁrst time (t=1) by selecting positions in the weight matrices based on the τ% threshold set on weight\nmagnitudes as below.\nMt =\n\u001a0,\nif min(W) ∗τ < W < max(W) ∗τ\n1\notherwise\nThe mask matrix contains ’zero’ in positions where the weight values are less than τ% of the maximum weight value\nand greater than τ% of the minimum weight value. Zero entries in the mask are used to prune the corresponding weight\nvalues using the Hadamard product as W’ = W⊙M. For example, if the maximum and minimum weight values are +10\nand -10, respectively, a τ =5% range is deﬁned by ±0.5. Any small weight values within the range of ±0.5 are pruned\nusing the binary mask. In the second perturbation time (t=2), following another N epochs of training and regrowing of\nthe previously pruned weights, a new weight mask is obtained Mt based on the new range of values deﬁned by τ%. A\nHadamard product between the previous mask (Mt−1) and the newly obtained mask (Mt) is obtained as Mt = Mt⊙\nMt−1. Therefore, the mask matrix cumulatively stores the pruning positions at each N-epoch interval of training. This\nperiodic masking (pruning) and regrowing procedure iteratively continues as shown in Figure 1. The percentage of\nweights with zero values at a given time point of training is used to measure model compression or sparsity.\n5\nA PREPRINT - MAY 18, 2022\n(a) Basic DNN\n(b) Basic Deep Autoencoder\n(c) Stacked Deep Autoencoder\nFigure 3: Three feed forward deep neural network architectures used in this study.\n4.2\nWeight perturbation schedule and setting\nThe best weight perturbation schedule is selected from our preliminary work [36]. Our preliminary work on a single\nlayer autoencoder reveals that periodically perturbing the weights of low magnitude several times can improve the\ndata reconstruction loss. In this paper, we study the effect of proposed perturbation routine on pretraining a three-\nlayer autoencoder (deep). This paper also sheds light on how such weight perturbations perform in transfer learning,\nespecially when the pretrained autoencoder model is ﬁne-tuned for a downstream classiﬁcation task with tabular\ndata. We extensively analyze the transfer learning performance of pretrained models obtained at different perturbation\npoints, as shown in Figure 2. A pretrained encoder is taken from one of these time points for subsequent ﬁnetuning in\na downstream classiﬁcation task. A general notion is that the lowest point in the reconstruction loss is the best model\nto be chosen for downstream learning tasks.\n4.3\nDeep model training\nWe examine the proposed weight perturbation routine on three deep model: 1) a basic fully connected deep neural\nnetwork (DNN), 2) a basic deep autoencoder (basic DAE), and 3) a stacked deep autoencoder (SDAE) [37]. Figure 3\nillustrates the three deep models, their architectures, and training mechanism. All three models have three hidden\nlayers to comply with the deﬁnition of deep learning. The stacked autoencoder is formed by stacking three separately\npre-trained autoencoders. In a stacked autoencoder, the ﬁrst autoencoder hidden layer is trained using input data. The\nlatent response of the ﬁrst autoencoder is used to train the second autoencoder, and so on. The DNN model is trained\nfrom scratch and then tested on a set apart data set. Basic DAE and SDAE are ﬁrst pretrained using the training data\nfold without any data labels. The pretrained models are then ﬁnetuned with the same training data fold with labels.\nFinally, the ﬁnetuned model is tested on a set aside test data fold.\n4.4\nExperimental cases\nThe proposed weight perturbation scheme is evaluated in ﬁve experimental cases: 1) no perturbation (baseline), 2)\npretrained model at the lowest reconstruction loss with perturbation (lowest loss), 3) pretrained model at perturbation\n(right after a perturbation or pruning), 4) after perturbation (regrowing the weights after multiple perturbation points),\nand 5) only 20% dropout. We investigate if the proposed perturbation method (case 2-4) improves feature learning over\nthe baseline model with no perturbation (case 1) or dropout only (case 5). The performances of these experimental\ncases are evaluated in downstream classiﬁcation tasks using three deep neural network models.\n4.5\nModel evaluation\nWe evaluate the experimental cases on three feed forward neural networks using six domain tabular data sets. The\nmodels are primarily analyzed in three steps: 1) pretraining with a training data set without labels, 2) ﬁnetuning with\nthe training data set and corresponding labels, and 3) testing on a separate test data set. The train-and-test procedure\n6\nA PREPRINT - MAY 18, 2022\nData set\nSample size\nDimension\nClassiﬁcation\nDomain\nArcene\n200\n10001\nBinary\nDiagnostic\nGene\n801\n16384\nMulticlass (5)\nGene sequence\nGisette\n7000\n5001\nBinary\nHandwritten digit\nMadelon\n2600\n501\nBinary\nSynthetic data\nParkinson\n756\n754\nBinary\nSpeech data\nMalware\n5653\n1087\nBinary\nCyber-security\nTable 1: A summary of the six tabular data sets used to evaluate the performance of proposed weight perturbation\nmethod.\nis performed using a ﬁve-fold cross validation scheme where four folds are used to train (pre-train and ﬁne-tune)\nthe model, and the remaining fold is used to obtain the test accuracy on the ﬁnetuned model. The average F1-score\nacross ﬁve test data folds is used as the model performance metric. The F1-score accounts for false positives and false\nnegatives along with true positives, which is a robust metric for reporting classiﬁcation performance.\n5\nResults\nAll experiments are implemented and executed on a Dell Precision 5820 workstation running Ubuntu 20.04 with 64GB\nRAM and an Nvidia GeForce RTX 3080 GPU with 10GB memory. The algorithms and model evaluation steps are\nimplemented in Python. The deep learning models are developed using PyTorch. The ﬁndings of the experiments are\ndiscussed below.\n5.1\nTabular data sets\nSix tabular data sets are obtained from the University of California at Irvine (UCI) machine learning data repository\nto train and test the models with or without the perturbation routine [38]. Table 1 shows the summary statistics of the\ntabular data sets used in this paper. Four out of ﬁve data sets have binary classiﬁcation labels. The gene data set has\nmulticlass labels (ﬁve). The Madelon data set is synthetically generated for the NIPS 2003 feature selection challenge.\nFor the malware data set, the class labels are two malware types: Virus Total (VT) and VxHeaven. The gisette data\nset contains handwritten characters ’4’ and ’9’. The pixels of these images are vectorized and converted to tabular\nformat. We select tabular data sets with relatively higher dimensionality (>500) than what is common in the literature.\nWe assume that the sequential layers of deep neural networks are appropriate for learning features by reducing data\ndimensionality at each layer. No data set contains missing values, nominal variables, and ordinal variables. We encode\nthe categorical class labels to numerical labels using label encoding for all data sets.\n5.2\nData processing and deep model setup\nWe standardize data using the mean and standard deviation of individual variables obtained from the training data\nfolds. Therefore, no information about the test data is leaked to the training data folds. For all experimental cases and\ndata sets, a common deep architecture is adopted using three hidden layers with 256, 128, and 64 neurons. The batch\nsize is 64 for all data sets. For optimizing the weight update, Adam optimizer is used with a learning rate of 1e−3\nwithout any weight decay. The exclusion of weight decay is because it introduces weight regularization, which may\nbias the experimental results. We use the rectiﬁed linear unit (reLu) activation function for all hidden layers. However,\nfor the output classiﬁcation layer, sigmoid and softmax are used in binary and multiclass classiﬁcations, respectively.\nIn model pretraining, the loss function is mean squared error loss (MSEloss). The ﬁnetuning step uses binary cross-\nentropy loss (BCELoss) for all data sets except the gene data set. A cross-entropy loss is chosen for the Gene data set\nbecause it has ﬁve target classes. In the dropout experiment, the dropout rate is set to 20%. Self-supervised pretraining\nand supervised ﬁne-tuning are done separately for 100 epochs for DAE and SDAE. However, only supervised training\nis performed with the basic DNN model.\n5.3\nDeep model pretraining\nFigure 4 compares representative training loss curves for different models and experimental scenarios. The Figure 4(a)\nshows that the basic autoencoder pretraining loss (self-supervised) curve appears much lower than the basic DNN\ntraining loss (supervised). In Figure 4(b), the pretraining loss curve, due to dropout, stands above that of the baseline\n(no perturbation, no dropout) and perturbation cases. The effect of perturbation on the loss curve is not discernible\n7\nA PREPRINT - MAY 18, 2022\n0\n20\n40\n60\n80\n100\nEpochs\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\nLoss\nBasic DNN Training\nBasic AE Pretraining\n(a) Basic DNN and AE\n0\n20\n40\n60\n80\n100\nEpochs\n0.6\n0.7\n0.8\n0.9\n1.0\nLoss\nNo perturbation\nPerturb Only\nDropout only\n(b) Perturbation versus Dropout\n0\n20\n40\n60\n80\n100\nEpochs\n0.6\n0.7\n0.8\n0.9\n1.0\nLoss\nNo perturbation\nPerturb 5%\nPerturb 20%\n(c) Effect of τ%\n0\n20\n40\n60\n80\n100\nEpochs\n0\n5\n10\n15\n20\n25\n30\nLoss\nStacked AE 1 Pretraining\nStacked AE 2 Pretraining\nStacked AE 3 Pretraining\n(d) Deep Stacked AE\nFigure 4: Comparative illustrations of loss curves of different deep models and perturbation routines. Perturb 5%\ndonotes a τ value of 5%.\nmay be because the threshold on weight magnitude τ is set to 5% in this study. Figure 4(c) shows clearer effects of a\nlarger τ (=20%), where the loss curve spikes up compared to the curve without perturbation at the time of perturbation\n(Epochs 30, 60, and 90). Figure 4(d) shows pretraining loss curves for three separately trained autoencoders before\nstacking in deep stacked autoencoder. The shade represents the standard deviation of the loss across ﬁve folds. In a\nstacked autoencoder, the ﬁrst encoding unit shows faster learning than the one used at upper encoding units.\n5.4\nData set speciﬁc trends\nEach data set is unique in sample size, dimensionality, sparsity, and distribution. Table 2 highlights data set speciﬁc\nF1-scores due to different model regularization schemes. For the acrene data set, the baseline model (basic DNN, basic\nAE, and stacked AE) performance is generally improved by weight perturbation. The dropout learning substantially\nimproves the performance of basic DNN. However, the performance gain due to dropout learning is on par with weight\nperturbation when using the autoencoder models. The highest F1-score (84.98 (5.7)%) is obtained using a weight\nperturbed stacked autoencoder at the lowest pretraining loss, whereas dropout learning yields (84.5 (6.1)%). The\nstacked autoencoder, with the aid of weight perturbation or dropout learning, outperforms the Gradient Boosting Tree\n(GBT) classiﬁer performance (F1-score: 82.8 (6.5)%). Otherwise, traditional machine learning (GBT) outperforms all\nthree deep learning models at baseline. For the gene sequence data set, the pretraining of baseline deep autoencoders\nyields a large improvement in accuracy over basic DNN at baseline. The performance of basic AE with dropout\nlearning alone slightly drops from the baseline and then further improves in joint dropout and weight perturbation\nlearning. The highest F1-score (99.75 (0.3)%) is obtained using a weight perturbed stacked autoencoder at the lowest\npretraining loss when compared to the dropout learning (F1-score 99.63 (0.3)%). Except for the basic DNN, all\nautoencoder-based deep models outperform the GBT model (F1-score 99.2 (0.4)%). For the Gisette data set, the basic\nDNN never outperforms traditional machine learning model (GBT) performance. The autoencoder-based models\ninvariably outperform the GBT classiﬁer. The highest F1-score is achieved by the basic autoencoder when taken at\nthe time of perturbation (at perturbation, F1-score 98.54(0.3)%) for ﬁnetuning. This best score slightly outperforms\nthe one achieved by dropout learning (98.41(0.2)%). The Madelon data set is unlike the other ﬁve data sets because it\nis synthetically generated under some statistical assumptions. The GBT classiﬁer (F1-score 81.6(1.9)%) substantially\n8\nA PREPRINT - MAY 18, 2022\nData set\nBasic DNN\nBasic DAE\nStacked DAE\nGBT\n% of weights pruned\nBaseline\n57.90 (11.7)\n80.48 (5.7)\n81.93 (6.9)\nLowest loss\n57.90 (11.7)\n81.51 (4.4)\n84.98 (5.7)\nAt perturbation\n58.47 (12.0)\n81.47 (7.3)\n83.03 (4.6)\nAfter perturbation\n57.90 (11.7)\n79.47 (5.2)\n83.95 (5.6)\nArcene (200 x 10001)\nDropout only\n77.28 (7.2)\n82.99 (6.9)\n84.50 (6.1)\n82.8 (6.5)\n15.47 (3.7)\nBaseline\n85.33 (16.9)\n99.62 (0.5)\n99.62 (0.5)\nLowest loss\n85.33 (16.9)\n99.37 (0.6)\n99.75 (0.3)\nAt perturbation\n85.22 (16.8)\n99.62 (0.5)\n99.25 (0.7)\nAfter perturbation\n85.22 (16.8)\n99.25 (0.6)\n99.50 (0.5)\nGene (801 x 16384)\nDropout only\n99.62 (0.5)\n99.37 (0.6)\n99.63 (0.3)\n99.2 (0.4)\n23.03 (9.7)\nBaseline\n71.57 (31.9)\n98.49 (0.2)\n98.21 (0.1)\nLowest loss\n71.57 (31.9)\n98.51 (0.2)\n98.30 (0.2)\nAt perturbation\n71.54 (31.9)\n98.54 (0.3)\n98.16 (0.1)\nAfter perturbation\n71.62 (32.0)\n98.39 (0.2)\n98.17 (0.2)\nGisette (7000 x 5001)\nDropout only\n97.07 (0.8)\n98.41 (0.2)\n98.43 (0.2)\n97.6 (0.5)\n18.19 (0.7)\nBaseline\n46.71 (9.0)\n57.11 (1.5)\n55.67 (1.2)\nLowest loss\n46.73 (9.1)\n57.71 (2.4)\n55.89 (1.8)\nAt perturbation\n46.24 (9.2)\n58.12 (2.2)\n55.39 (1.6)\nAfter perturbation\n46.01 (8.9)\n57.78 (2.7)\n56.08 (2.2)\nMadelon (2600 x 501)\nDropout only\n50.49 (7.2)\n64.97 (0.8)\n56.90 (1.5)\n81.6 (1.9)\n20.09 (0.6)\nBaseline\n57.56 (37.1)\n88.17 (1.6)\n88.13 (2.8)\nLowest loss\n58.54 (37.9)\n87.89 (2.5)\n87.98 (2.0)\nAt perturbation\n58.52 (37.9)\n87.39 (2.2)\n87.54 (3.1)\nAfter perturbation\n57.26 (36.9)\n88.06 (1.8)\n87.78 (1.9)\nParkinson’s (756 x 754)\nDropout only\n72.14 (29.0)\n88.98 (2.8)\n88.01 (2.5)\n87.4 (1.4)\n17.82 (0.7)\nBaseline\n68.19 (30.6)\n94.37 (0.5)\n94.30 (7.0)\nLowest loss\n68.22 (30.6)\n94.36 (0.7)\n94.69 (1.0)\nAt perturbation\n68.12 (30.6)\n93.43 (1.3)\n94.67 (0.7)\nAfter perturbation\n68.47 (30.8)\n94.34 (0.4)\n94.69 (0.3)\nMalware (5653 x 1087)\nDropout only\n93.33 (0.7)\n94.55 (0.6)\n94.32 (0.6)\n96.6 (4.9)\n41.28 (1.9)\nTable 2: Average F1-scores across ﬁve cross-validation folds comparing different model perturbation scenarios. Basic\nDNN = Deep Neural Network and DAE = Deep AutoEncoder. The pretraining is performed 1) with no perturbation or\ndropout (Baseline), 2) at the lowest autoencoder reconstruction loss point during perturbation (lowest loss), 3) at the\nsecond perturbation point (at perturbation), 4) at epoch 100: regrowing after the ﬁnal perturbation at epoch 90 (after\nperturbation). GBT = gradient boosting tree classiﬁer model. The last column shows the percentage of total weights\npruned by the best pretraining scenario.\noutperforms any other deep learning models, considering all perturbation and dropout schemes (64.97(0.8)%). This\nis an example where deep learning fails miserably without leaving an alternative to traditional machine learning. For\nbasic DNN and basic AE, the dropout learning signiﬁcantly improves the accuracy over the baseline models. Any\nweight perturbed models with the lowest loss slightly outperform their baseline counterparts. However, the best F1-\nscores are achieved by dropout learning (F1-score 64.97 (0.8)%) using a basic pretrained AE model. In the Parkinson’s\ndata set, the performance of weight perturbation slightly drops compared with the baseline autoencoder models. The\nsame is observed for dropout learning with the stacked autoencoder model. The baseline autoencoders (F1-score\n88.17(1.6)%) outperform the GBT classiﬁer (F1-score 87.4(1.4)%). The best F1-score is achieved by dropout learning\nwith a basic autoencoder (F1-score 88.98(2.8) %). Like the Madelon data set, none of the deep learning scenarios\noutperform the GBT classiﬁer (F1-score 96.6(4.9 %) for the malware data set. The perturbation of weights improves\nF1-score with the stacked autoencoder model, whereas dropout improves all baseline model performances. The best\nperformance is achieved after weight perturbing a stacked autoencoder (F1-score 94.69(0.3)%), which still falls below\nthe accuracy achieved by a traditional machine learning classiﬁer.\n5.5\nModel speciﬁc trends\nTable 2 reveals that self-supervised pretraining of a deep neural network (autoencoders) signiﬁcantly improves its\ndownstream classiﬁcation performance over a basic deep classiﬁer model (basic DNN) without pretraining. This\nobservation on baseline models is consistent across all domain data sets. However, the contribution of the proposed\nweight perturbation varies across the deep models. The pruning step in perturbation compresses the model, and the\nregrowing step recovers some of the pruned weights to improve the pretraining of model. The basic autoencoder and\nstacked autonencoder models equally share the best performance counts (three/three) across six data sets. However, the\nbaseline basic DNN model fails to outperform the machine learning model (GBT) on all six datasets. In contrast, with\n9\nA PREPRINT - MAY 18, 2022\n1\n6\n11\n16\n21\n26\n31\nPerturbation Index\n12\n14\n16\n18\n20\n22\n24\n% of Weights Perturbed\n(a) Arcene\n1\n6\n11\n16\n21\n26\n31\nPerturbation Index\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n% of Weights Perturbed\n(b) Gene\n1\n6\n11\n16\n21\n26\n31\nPerturbation Index\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\n37.5\n% of Weights Perturbed\n(c) Gisette\n1\n6\n11\n16\n21\n26\n31\nPerturbation Index\n14\n16\n18\n20\n22\n24\n26\n28\n% of Weights Perturbed\n(d) Madelon\n1\n6\n11\n16\n21\n26\n31\nPerturbation Index\n30\n35\n40\n45\n50\n55\n% of Weights Perturbed\n(e) Malware\n1\n6\n11\n16\n21\n26\n31\nPerturbation Index\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n% of Weights Perturbed\n(f) Parkinson’s\nFigure 5: Cumulative growth in the percentage of weights perturbed. The pretraining is continued for 1000 epochs to\nperturb the weights 33 times at 30-epoch intervals.\nthe aid of weight perturbation or dropout, deep autoencoder models outperform GBT (traditional machine learning)\non four out of six data sets.\n5.6\nModel compression by weight perturbation\nIn addition to the performance gain in downstream classiﬁcation, the pretrained model achieves a sparse weight rep-\nresentation. Table 2 shows the percentage of weights pruned in the pretrained models that yield the best F1-scores\nfor individual data sets after ﬁnetuning. The perturbation prunes 15% to 40% of the total weights in the pretrained\nmodel, resulting in signiﬁcant model compression for storing and sharing such models to be used in future ﬁnetuning\nor supervised classiﬁcation. Low standard deviations in the average pruning number suggest a stable selection of\nweights for pruning across the ﬁve data folds. The cumulative weight mask generation (Mt) via weight perturbation\nis observed by pretraining a deep model for 1000 epochs. Figure 5 shows that after increasing with a larger slope, the\npercentage of weight pruned by the weight mask tends to slow down over subsequent training epochs.\n6\nDiscussion\nThis paper proposes a method to periodically perturb weights of deep neural networks during training to simultane-\nously improve feature learning and model compression. The ﬁndings of the paper can be summarized as follows.\nFirst, the pretrained model at the lowest reconstruction loss point with weight perturbation improves downstream clas-\nsiﬁcation performance. Second, our proposed weight perturbation achieves 15% to 40% compression in pretrained\nmodels across six different tabular data sets. Third, instead of conceding a loss in accuracy, as seen in conventional\nweight pruning, our sparse pretrained deep model improves downstream classiﬁcation performance. Fourth, the pro-\nposed weight perturbation method outperforms dropout learning in four out of six tabular data sets and is on par with\ndropout in the other two datasets. Therefore, weight perturbation may help alleviate some overﬁtting and introduce\nmodel compression. Fifth, a traditional machine learning model outperforms basic DNN models in tabular data classi-\nﬁcation. In contrast, pretrained autoencoders with weight perturbation can outperform a traditional machine learning\nmodel on similar data.\n6.1\nEffects of regrowing after pruning\nConventional weight pruning methods permanently set some weight values to zero. In weight perturbation, the pruned\nweights are set to zero and allowed to regrow to recover some of the lost weights. The regrowth of weights after\n10\nA PREPRINT - MAY 18, 2022\n5\n10\n15\n20\n25\n30\nThreshold % on Weight Magnitude\n0\n20\n40\n60\n80\nPercentage\nF1 Score without Perturbation\nF1 Score with Perturbation\n% of Weights Perturbed\n(a) Arcene\n5\n10\n15\n20\n25\n30\nThreshold % on Weight Magnitude\n0\n20\n40\n60\n80\n100\nPercentage\nF1 Score without Perturbation\nF1 Score with Perturbation\n% of Weights Perturbed\n(b) Gene\n5\n10\n15\n20\n25\n30\nThreshold % on Weight Magnitude\n0\n20\n40\n60\n80\n100\nPercentage\nF1 Score without Perturbation\nF1 Score with Perturbation\n% of Weights Perturbed\n(c) Gisette\n5\n10\n15\n20\n25\n30\nThreshold % on Weight Magnitude\n0\n20\n40\n60\n80\nPercentage\nF1 Score without Perturbation\nF1 Score with Perturbation\n% of Weights Perturbed\n(d) Madelon\n5\n10\n15\n20\n25\n30\nThreshold % on Weight Magnitude\n0\n20\n40\n60\n80\n100\nPercentage\nF1 Score without Perturbation\nF1 Score with Perturbation\n% of Weights Perturbed\n(e) Malware\n5\n10\n15\n20\n25\n30\nThreshold % on Weight Magnitude\n0\n20\n40\n60\n80\nPercentage\nF1 Score without Perturbation\nF1 Score with Perturbation\n% of Weights Perturbed\n(f) Parkinson’s\nFigure 6: The effect of perturbation parameter (τ) on perturbing weights (% of weights perturbed) in deep model\npretraining and on downstream classiﬁcation performance. Examples are shown for all six tabular data sets used in\nthis study.\npruning can improve the learning trajectory of the pretrained model over time (1000 epochs) [36]. Without regrowing\nthe pruned weights, pruning alone yields a negative effect by increasing the reconstruction loss. However, the effect\nof weight pruning with τ = 5% is not evident in our loss curves. This observation may be due to our deeper networks\nwith a large number of trainable parameters and a limited period of training (100 epochs) (Figure 4 (b)). The regrowth\nof weights adjusts the loss curve, after retaining a subset of the sparse weights.\n6.2\nPerturbation versus dropout and regularization\nDropout is a random process of regularizing neurons during model training. It prevents the contribution of a set of\nrandomly selected neurons to weight updates in the training phase. However, all neuronal responses are taken into\naccount in the test phase. Therefore, the pretrained model obtained following dropout learning is not sparse. Despite\nthe regrowing of weights, a signiﬁcant portion of the weights remains zero, introducing a decent sparsity in the trained\nmodel. It is noteworthy that the F1-scores from perturbed models are compared with those obtained following L1 and\nL2 weight regularization during training. Weight regularization during the self-supervised pretraining phase yields\nworse F1-scores in subsequent classiﬁcation tasks than dropout. It may be because weight regularization shrinks the\npretrained weights, aiming to minimize the data reconstruction loss. The weight shrinkage may be too aggressive to\nfavorably start and converge the pretrained model in supervised ﬁnetuning. For a fair comparison, dropout, weight\nperturbation, and weight regularization are performed during the pretraining phase only.\n6.3\nDeep learning performance depends on data\nWe explain the performance of deep models and traditional machine learning. One striking difference appears in the\nclassiﬁcation of the Madelon data set. Here, traditional machine learning (F1-score 81.6 (1.9)) substantially outper-\nforms the best deep learning setup (F1-score 64.97 (0.8)). The correlation matrix of the Madelon data set suggests\nthat the variables are highly uncorrelated with zero non-diagonal entries. This unusual result is because the Madelon\ndata set is synthetically generated, unlike real-world datasets that inevitably contain correlated variables. Therefore,\ndatasets with uncorrelated variables are poor candidates for deep learning. The high performance of deep models with\ngene sequence data (F1-score 99.75 (0.3)) explains why these models are so successful with sequential and correlated\ndata (images, text).\n11\nA PREPRINT - MAY 18, 2022\n6.4\nAblation study\nThis study uses a default value of 5% for the τ parameter to limit the model compression. A higher value of τ prunes\na larger number of weights, which may hurt the downstream classiﬁcation accuracy. This section investigates the\neffect of varying τ on model compression and downstream classiﬁcation performance. Figure 6 reveals the percentage\nof weights perturbed and F1-scores at downstream classiﬁcation due to varying the perturbation parameter (τ). As\nexpected, increasing the τ value almost linearly increases the percentage of weights perturbed to introduce sparsity in\nthe pretrained model. In the case of acrene and madelon data sets, a perturbation with τ=25% substantially improves\nthe downstream classiﬁcation F1-scores compared to the baseline model without such perturbation. In all other data\nsets, the downstream classiﬁcation F1-scores remain stable, even after increasing the percentage of weight perturbation\nby increasing the value of τ during the pretraining. This observation can be explained by the notion that the weight\npruning step is followed by the weight regrowing step to recover some of the lost weights by estimating the weight\ngradient on the pruned weights.\n6.5\nLimitations\nSimilar to any studies, this work has several limitations. The proposed weight perturbation does not have any positive\neffects on improving the performance of basic DNN models. Dropout still appears to be a powerful approach with ba-\nsic DNN and datasets with uncorrelated variables. The improvement in F1-scores due to perturbation is between 0.2%\nto 3% across different data sets. This improvement may increase depending on the deep model, data set, and more\nimportantly, the perturbation parameter τ as seen in the ablation study. Despite this limitation, weight perturbation\nadditionally achieves 15% to 40% compression due to sparsity in the pretrained model. The proposed model compres-\nsion is deﬁned for the pretrained model, not at the ﬁnetuning or supervised classiﬁcation stage. The general notion\nis that a generic pretrained model is stored and then ﬁnetuned to be repurposed for a target application. Therefore,\nsparsity in pretrained models enables compression for model storage and sharing.\n7\nConclusions\nThis paper proposes a periodic weight perturbation routine while self-supervised pretraining of a deep neural network\nmodel. The proposed perturbation method can achieve both pretrained model compression and improved downstream\nclassiﬁcation accuracy across a variety of tabular data sets. This improvement often outperforms dropout learning\nand weight regularization when applied at the pretraining stage. A growing step followed by the pruning step can\nhelp correct some weights and learning trajectory, improving the downstream classiﬁcation performance. Therefore,\npruning or regularizing deep models at the self-supervised pretraining stage may be more effective than performing\nsimilar operations at the supervised training or ﬁnetuning step. Additionally, deep learning can perform very poorly in\nclassifying tabular data sets with uncorrelated variables, whereas traditional machine learning achieves substantially\nhigh accuracy.\nAcknowledgements\nResearch reported in this publication was supported by the National Library Of Medicine of the National Institutes\nof Health under Award Number R15LM013569. The content is solely the responsibility of the authors and does not\nnecessarily represent the ofﬁcial views of the National Institutes of Health.\nReferences\n[1] M Alam, M D Samad, L Vidyaratne, A Glandon, and K M Iftekharuddin. Survey on Deep Neural Networks in\nSpeech and Vision Systems. Neurocomputing, 417:302–321, 2020.\n[2] Qingchen Zhang, Laurence T Yang, Zhikui Chen, and Peng Li. A survey on deep learning for big data. Informa-\ntion Fusion, 42:146–157, 2018.\n[3] Jonas Mueller, Xingjian Shi, and Alexander Smola. Faster, simpler, more accurate. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2020.\n[4] Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. Subtab: Subsetting features of tabular data for self-\nsupervised representation learning. Advances in Neural Information Processing Systems, 34, 2021.\n12\nA PREPRINT - MAY 18, 2022\n[5] Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C Bayan Bruss, and Tom Goldstein.\nSaint:\nImproved neural networks for tabular data via row attention and contrastive pre-training.\narXiv preprint\narXiv:2106.01342, 2021.\n[6] Sergei Popov, Stanislav Morozov, and Artem Babenko. Neural oblivious decision ensembles for deep learning\non tabular data. arXiv preprint arXiv:1909.06312, 2019.\n[7] Julian Hatwell, Mohamed Medhat Gaber, and R Azad. Chirps: Explaining random forest classiﬁcation. Artiﬁcial\nIntelligence Review, 53(8):5747–5788, 2020.\n[8] Manar D. Samad, Alvaro Ulloa, Gregory J. Wehner, Linyuan Jing, Dustin Hartzel, Christopher W. Good, Brent A.\nWilliams, Christopher M. Haggerty, and Brandon K. Fornwalt. Predicting Survival From Large Echocardiogra-\nphy and Electronic Health Record Datasets. JACC: Cardiovascular Imaging, 12(4):681–689, jun 2018.\n[9] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a\nsimple way to prevent neural networks from overﬁtting. The journal of machine learning research, 15(1):1929–\n1958, 2014.\n[10] Alvin Poernomo and Dae-Ki Kang. Biased dropout and crossmap dropout: learning towards effective dropout\nregularization in convolutional neural network. Neural networks, 104:60–67, 2018.\n[11] Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana. Towards compact and robust deep neural networks.\narXiv preprint arXiv:1906.06110, 2019.\n[12] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efﬁcient\nconvolutional networks through network slimming. In Proceedings of the IEEE international conference on\ncomputer vision, pages 2736–2744, 2017.\n[13] Raisul Areﬁn, Manar D Samad, Furkan A Akyelken, and Arash Davanian. Non-transfer deep learning of optical\ncoherence tomography for post-hoc explanation of macular disease classiﬁcation. In 2021 IEEE 9th International\nConference on Healthcare Informatics (ICHI), pages 48–52. IEEE, 2021.\n[14] Sanghun Lee and Chulhee Lee. Revisiting spatial dropout for regularizing convolutional neural networks. Mul-\ntimedia Tools and Applications, 79(45):34195–34207, 2020.\n[15] Saihui Hou and Zilei Wang. Weighted channel dropout for regularization of deep convolutional neural network.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 8425–8432, 2019.\n[16] Xu Shen, Xinmei Tian, Tongliang Liu, Fang Xu, and Dacheng Tao. Continuous Dropout. IEEE Transactions on\nNeural Networks and Learning Systems, 29(9):3926–3937, sep 2018.\n[17] Hojjat Salehinejad and Shahrokh Valaee. Edropout: Energy-based dropout and pruning of deep neural networks.\nIEEE Transactions on Neural Networks and Learning Systems, 2021.\n[18] Hieu Pham and Quoc Le. Autodropout: Learning dropout patterns to regularize deep networks. Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, 35(11):9351–9359, May 2021.\n[19] Arinbj¨orn Kolbeinsson, Jean Kossaiﬁ, Yannis Panagakis, Adrian Bulat, Animashree Anandkumar, Ioanna\nTzoulaki, and Paul M Matthews. Tensor dropout for robust learning. IEEE Journal of Selected Topics in Signal\nProcessing, 15(3):630–640, 2021.\n[20] Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. Pruning and quantization for deep\nneural network acceleration: A survey. Neurocomputing, 461:370–403, 2021.\n[21] Xinrui Jiang, Nannan Wang, Jingwei Xin, Xiaobo Xia, Xi Yang, and Xinbo Gao. Learning lightweight super-\nresolution networks with weight pruning. Neural Networks, 144:21–32, 2021.\n[22] Guiying Li, Chao Qian, Chunhui Jiang, Xiaofen Lu, and Ke Tang. Optimization based layer-wise magnitude-\nbased pruning for dnn compression. In IJCAI, pages 2383–2389, 2018.\n[23] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efﬁcient neural\nnetwork. Advances in neural information processing systems, 28, 2015.\n[24] Sejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin. Lookahead: a far-sighted alternative of magnitude-based\npruning. arXiv preprint arXiv:2002.04809, 2020.\n13\nA PREPRINT - MAY 18, 2022\n[25] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efﬁcient dnns. Advances in neural\ninformation processing systems, 29, 2016.\n[26] Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning with\nfeedback. In International Conference on Learning Representations, 2020.\n[27] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft ﬁlter pruning for accelerating deep\nconvolutional neural networks. In Proceedings of the Twenty-Seventh International Joint Conference on Artiﬁcial\nIntelligence, pages 2234–2240. IJCAI, 2018.\n[28] Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training very sparse\ndeep networks. arXiv preprint arXiv:1711.05136, 2017.\n[29] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio\nLiotta. Scalable training of artiﬁcial neural networks with adaptive sparse connectivity inspired by network\nscience. Nature communications, 9(1):1–12, 2018.\n[30] Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. What do compressed deep\nneural networks forget? arXiv preprint arXiv:1911.05248, 2019.\n[31] G¨okcen Eraslan, Lukas M Simon, Maria Mircea, Nikola S Mueller, and Fabian J Theis. Single-cell rna-seq\ndenoising using a deep count autoencoder. Nature communications, 10(1):1–14, 2019.\n[32] Nanjun Li, Faliang Chang, and Chunsheng Liu. Human-related anomalous event detection via spatial-temporal\ngraph convolutional autoencoder with embedded long short-term memory network. Neurocomputing, 2021.\n[33] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael J Black. Generating 3d faces using convolutional\nmesh autoencoders. In Proceedings of the European Conference on Computer Vision (ECCV), pages 704–720,\n2018.\n[34] Zhong Zheng, Zijun Zhang, Long Wang, and Xiong Luo. Denoising temporal convolutional recurrent autoen-\ncoders for time series classiﬁcation. Information Sciences, 588:159–173, 2022.\n[35] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin\nStoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7871–7880, 2020.\n[36] Manar D. Samad, Rahim Hossain, and Khan M. Iftekharuddin. Dynamic perturbation of weights for improved\ndata reconstruction in unsupervised learning.\nIn 2021 International Joint Conference on Neural Networks\n(IJCNN), pages 1–7, 2021.\n[37] Peicheng Zhou, Junwei Han, Gong Cheng, and Baochang Zhang. Learning compact and discriminative stacked\nautoencoder for hyperspectral image classiﬁcation. IEEE Transactions on Geoscience and Remote Sensing,\n57(7):4823–4833, jul 2019.\n[38] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.\n14\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NE"
  ],
  "published": "2022-05-17",
  "updated": "2022-05-17"
}