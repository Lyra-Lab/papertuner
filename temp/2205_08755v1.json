{
  "id": "http://arxiv.org/abs/2205.08755v1",
  "title": "Persian Natural Language Inference: A Meta-learning approach",
  "authors": [
    "Heydar Soudani",
    "Mohammad Hassan Mojab",
    "Hamid Beigy"
  ],
  "abstract": "Incorporating information from other languages can improve the results of\ntasks in low-resource languages. A powerful method of building functional\nnatural language processing systems for low-resource languages is to combine\nmultilingual pre-trained representations with cross-lingual transfer learning.\nIn general, however, shared representations are learned separately, either\nacross tasks or across languages. This paper proposes a meta-learning approach\nfor inferring natural language in Persian. Alternately, meta-learning uses\ndifferent task information (such as QA in Persian) or other language\ninformation (such as natural language inference in English). Also, we\ninvestigate the role of task augmentation strategy for forming additional\nhigh-quality tasks. We evaluate the proposed method using four languages and an\nauxiliary task. Compared to the baseline approach, the proposed model\nconsistently outperforms it, improving accuracy by roughly six percent. We also\nexamine the effect of finding appropriate initial parameters using zero-shot\nevaluation and CCA similarity.",
  "text": "Persian Natural Language Inference: A Meta-learning approach\nHeydar Soudani∗\nSharif University of Technology\nheydars@ce.sharif.edu\nMohammad Hassan Mojab∗\nSharif University of Technology\nmhmojab@ce.sharif.edu\nHamid Beigy\nSharif University of Technology\nbeigy@sharif.edu\nAbstract\nIncorporating information from other lan-\nguages can improve the results of tasks in\nlow-resource languages. A powerful method\nof building functional natural language pro-\ncessing systems for low-resource languages\nis to combine multilingual pre-trained rep-\nresentations with cross-lingual transfer learn-\ning.\nIn general, however, shared represen-\ntations are learned separately, either across\ntasks or across languages.\nThis paper pro-\nposes a meta-learning approach for inferring\nnatural language in Persian. Alternately, meta-\nlearning uses different task information (such\nas QA in Persian) or other language informa-\ntion (such as natural language inference in En-\nglish). Also, we investigate the role of task\naugmentation strategy for forming additional\nhigh-quality tasks. We evaluate the proposed\nmethod using four languages and an auxil-\niary task. Compared to the baseline approach,\nthe proposed model consistently outperforms\nit, improving accuracy by roughly six percent.\nWe also examine the effect of ﬁnding appro-\npriate initial parameters using zero-shot evalu-\nation and CCA similarity.\n1\nIntroduction\nIn natural language processing (NLP), the goal is\nto improve models for the processing and produc-\ntion of human languages. As part of NLP, several\ntasks are deﬁned, each covering a different level\nof natural language understanding. Meanwhile,\nnatural language inference (NLI) is considered an\nappropriate and rigorous measure of language com-\nprehension. This task requires recognizing the con-\nsequences of natural language sentences, which in-\ndicates how well it understands the language (Mac-\nCartney, 2009).\nNLI aims to determine the inferential relation-\nship between a premise p and a hypothesis h. The\nproblem involves a three-class classiﬁcation in\n∗Equal contribution\nwhich every pair (p, h) falls into one of three cat-\negories: entailment, contradiction, and neutral. If\nthe hypothesis can be inferred from the premise,\npair (p, h) will be assigned to the entailment class.\nFor a hypothesis that contradicts the premise, pair\n(p, h) will be assigned to the contradiction and neu-\ntral otherwise (Amirkhani et al., 2020).\nThe Persian language lacks sufﬁcient linguistic\nresources when it comes to natural language un-\nderstanding. The lack of data can be addressed by\ncollecting annotated data, but this process is both\ntime-consuming and expensive (Nooralahzadeh\net al., 2020). FarsTail (Amirkhani et al., 2020)\nis currently available for Persian, which is cre-\nated using the same method as SciTail (Khot\net al., 2018). It contains 10,367 samples. Also,\nParsiNLU (Khashabi et al., 2021) is created for\nhigh-level tasks in Persian, and for NLI, it con-\nsists of 2700 samples. As it turns out, this amount\nof data is too small compared with resource-\nrich languages (such as English, which has only\n550,000 samples in the SNLI (Bowman et al., 2015)\ndataset).\nResearchers have tried to solve the data scarcity\nproblem by using cross-language methods. Recent\nwork on cross-lingual learning has mainly focused\non transfer between languages already covered\nby pre-trained representations (Wu and Dredze,\n2019). Nonetheless, these techniques do not read-\nily transfer to low-resource languages in which (1)\nlarge monolingual corpora are unavailable for pre-\ntraining, and (2) sufﬁcient labeled data is lacking\nfor ﬁne-tuning downstream tasks (Xia et al., 2021).\nThe results of experimental studies for Per-\nsian using different embedding methods includ-\ning word2vec (Mikolov et al., 2013), fastText (Bo-\njanowski et al., 2017), ELMo (Peters et al., 2018),\nand BERT (Devlin et al., 2019) and various mod-\nels, such as, DecompAtt (Parikh et al., 2016),\nESIM (Chen et al., 2016), HBMP (Talman et al.,\n2019), and ULMFiT (Howard and Ruder, 2018) is\narXiv:2205.08755v1  [cs.CL]  18 May 2022\nreported in FarsTail (Amirkhani et al., 2020). Al-\nthough this cross-lingual information sharing has\nenabled success in various natural language pro-\ncessing tasks, it raises the question of how we can\nachieve more effective collaborative learning be-\ntween languages or even between different tasks.\nRecently, meta-learning has shown to be effec-\ntive for a variety of machine learning tasks, includ-\ning NLP (Koch et al., 2015; Ravi and Larochelle,\n2016; Qian and Yu, 2019). This paper uses a meta-\nlearning-based method for learning parameters in\nthe joint space of tasks and languages. Auxiliary\nlanguages include English, Spanish, French, and\nGerman, while QA is the auxiliary task.\nAlternatively, an essential prerequisite for the\nsuccessful application of meta-learning is a task\ndistribution from which a large number of tasks\ncan be sampled to train the meta-learner. How-\never, in NLP, datasets are usually considered as\ntasks (Nooralahzadeh et al., 2020; Qi and Du,\n2020). There are two main problems with treat-\ning entire datasets as tasks. The ﬁrst problem is\noverﬁtting, in which a meta-learner is overﬁtted\nto a small number of training tasks since there is\nonly a small number of supervised datasets for each\nNLP problem. A second concern is that the het-\nerogeneity of NLP datasets may result in learn-\ning episodes that lead to memorization overﬁtting,\nwhere a meta-learner ignores the support set and\nfails to adapt (Murty et al., 2021). To improve the\nquality and quantity of tasks, we use the DReCa\n(Murty et al., 2021) approach as our data augmen-\ntation strategy.\nIn this paper, we employ meta-learning algo-\nrithms to enhance the Persian NLI task. Our models\nare evaluated on the FarsTail dataset. Experimental\nresults show that we push Persian NLI accuracy\nforward by more than 6% and zero-shot accuracy\nby about 4%, setting a new state-of-the-art result\nfor this task. In summary, the main contributions\nof our research are:\n• We have enabled effective parameter sharing\nacross multiple languages and tasks by provid-\ning a meta-learning approach. To the best of\nthe authors’ knowledge, this is the ﬁrst study\nof the interaction between several languages\nand tasks at different levels of abstraction to\nsolve a high-level problem in the Persian lan-\nguage. The evaluation results are based on\nthe FarsTail dataset as a reference dataset in\nthe Persian language. The datasets available\nin the XTREME benchmark (Hu et al., 2020)\nhave also been used for auxiliary languages\nand tasks.\n• We examine a metadata augmentation strategy\nnamed DReCa (Murty et al., 2021) that takes\nas input a set of tasks (entire datasets). We\nthen decompose them to approximate some\nof the latent reasoning categories underlying\nthese datasets, such as various syntactic con-\nstructs within a dataset, or semantic categories\nsuch as quantiﬁers and negation.\n• We also evaluate the trained model in zero-\nshot mode, which means that the target lan-\nguage (Persian) data is never used during the\ntraining process. This test indicates the gener-\nality of the model.\nThe rest of the paper is arranged as follows: Sec-\ntion 2 brieﬂy describes related work. Section 3\nintroduces our method, and in section 4, we ex-\nplain the details of the experimental setup. Section\n5 presents practical results. The results analysis\nand some justiﬁcation are described in section 6.\nWe conclude the paper and summarize future direc-\ntions in section 7.\n2\nRelated Work\nIn this section, we brieﬂy outline related work in\nthree areas. The ﬁrst area is models based on cross-\nlingual algorithms. In the second area, we highlight\nmethods based on meta-learning. Finally, we sum-\nmarize existing data augmentation strategies.\n2.1\nModels based on Cross-lingual\nCross-lingual learning is a method for transfer-\nring knowledge from one natural language to an-\nother (Pikuliak et al., 2021). Pre-trained models\nare one of the most widely used examples of cross-\nlingual learning. Since these models have achieved\ngood results, so Wu and Dredze (2019) explored\nthe broader cross-lingual potential of mBERT (mul-\ntilingual BERT) as a zero-shot language transfer\nmodel with ﬁve NLP tasks including NLI, covering\na total of 39 languages. Also, Wang et al. (2019)\nprovides a comprehensive study of the contribution\nof different components in mBERT to its cross-\nlingual ability. In addition, it examines the impact\nof the linguistic properties of the languages, the ar-\nchitecture of the model, and the learning objectives.\nConneau and Lample (2019) proposed two meth-\nods for learning cross-lingual language models, one\nusing monolingual data and the other using paral-\nlel data and a new cross-lingual language model\nobjective. Singh et al. (2019) introduced a cross-\nlingual data augmentation method that substitutes\npart of the input text with its translation in another\nlanguage.\nHuertas-Tato et al. (2021) designed a new archi-\ntecture called Siamese Inter-Lingual Transformer\n(SILT), to align multilingual embeddings for NLI\nefﬁciently. The paper points out that transformer\nmodels are unable to generalize to other domains\nand have problems with multilingual and inter-\nlinguistic scenarios. A new network has been de-\nveloped to overcome these weaknesses by combin-\ning three parts: a multilingual transformer as pre-\ntrained embedding, an alignment matrix to com-\npute the similarity between two sentences, and a\nmulti-head self-attention block to interpret input\nstrings.\nDespite the advances that Cross-lingual methods\nhave made, building NLP systems in these settings\nis challenging for several reasons. First, the tar-\nget language does not contain sufﬁcient annotated\ndata for effective ﬁne-tuning. Secondly, pre-trained\nmultilingual representations are not directly trans-\nferable due to language disparities (Xia et al., 2021).\nIn contrast to these methods, we consider setting\nup training models simultaneously on multiple lan-\nguages and tasks.\n2.2\nMeta-learning\nMeta-learning addresses the problem of learning\nto learn. By examining many learning problems,\na meta-learner learns a model (Liu et al., 2020).\nSpeciﬁcally, the meta-learner uses a meta training\nset MS = {(Ss\ni, Ts\ni)}Ns\ni=1 , where (Ss\ni, Ts\ni) are the\ntraining (support) and test (query) set of the ith\nlearning problem and Ns is the number of learn-\ning problems used for training; and a meta test set\nMT =\n\b\u0000St\ni, Tt\ni\n\u0001\tNt\ni=1 , where\n\u0000St\ni, Tt\ni\n\u0001\nare the sup-\nport and query set of the ith test learning problem,\nwhile Nt is the number of learning problems used\nfor the test. Given MS, the meta-learner learns\nhow to map a pair (S, T) into an algorithm that\nleverages S to optimally solve T.\nDue to the lack of well-deﬁned task distribution,\nmeta-learning has not yet succeeded in NLP, lead-\ning to attempts that treat datasets as tasks. An ad\nhoc task distribution causes problems with quantity\nand quality. Murty et al. (2021) provide a way to\nbreak down heterogeneous tasks such as datasets\ninto a set of appropriate subtasks. With this method,\ndata is transferred to the feature space using a pre-\ntrained model. They use k-means to decompose\ndata into k clusters and create tasks by combining\nthese clusters.\nRecently, however, the combination of cross-\nlingual techniques in meta-learning frameworks\nhas also been extensively studied. To train a model\nfor low-resource languages on NLI and QA tasks,\nNooralahzadeh et al. (2020) uses the MAML al-\ngorithm and auxiliary languages. van der Heij-\nden et al. (2021) study the text documents classi-\nﬁcation problem in monolingual and multilingual\nmodes, using different algorithms such as, Pro-\ntotypical Networks (Snell et al., 2017), MAML\n(Finn et al., 2017), Reptile (Nichol et al., 2018),\nand ProtoMAML (Triantaﬁllou et al., 2019). Also,\nTarunesh et al. (2021) examine the interaction be-\ntween different languages and tasks to learn an\nappropriate common feature space.\nAdditionally, transfer-learning can be helpful for\nlow-resource languages. Xia et al. (2021) introduce\na meta-learning-based framework called MetaXL\nfor extremely low-resource languages. MetaXL\nlearns an intelligent representational conversion\nfrom several auxiliary languages to the target lan-\nguage, bringing the feature space of these lan-\nguages closer together for more efﬁcient conver-\nsion. The main idea is to use a Representation\nTransformation network between the main model\nlayers which are trained only with target language.\nTo the best of our knowledge, this paper is the\nﬁrst attempt to study meta-learning for solving the\nNLI problem in the Persian language. Also, we\nare pioneers in using task-language pairs as meta-\nlearning tasks in the Persian language.\n2.3\nTask Augmentation\nMachine learning algorithms usually assume that\nthe train and test data have the same distribution. In\ncontrast, the meta-learning framework treats tasks\nas training examples and trains a model to adapt to\nall of them. Meta-learning also assumes that the\ntraining and new tasks are drawn from the same\ndistribution of tasks p(τ). In NLP, datasets are\ntypically treated as tasks, and meta-learners are\nthen overﬁtting their adaptation mechanisms. NLP\ndatasets are highly heterogeneous, which causes\nmany learning episodes to have the poor transfer\nbetween their support and query sets, which dis-\nsuades meta-learners from adapting (Murty et al.,\n2021).\nTo deal with overﬁtting challenges, Yin et al.\n(2019) propose a meta-regularizer to mitigate mem-\norization overﬁtting, but don’t study learner over-\nﬁtting. Rajendran et al. (2020) study task augmen-\ntation for mitigating meta-learners overﬁtting in\nthe context of few-shot label adaptation. SMLMT\nmethod (Bansal et al., 2020) creates new self-\nsupervised tasks that improve meta-overﬁtting, but\nthis does not directly address the dataset-as-tasks\nproblem. In contrast, the DReCa method (Murty\net al., 2021) addresses the dataset-as-tasks prob-\nlem and focuses on using clustering as a way to\nsubdivide and ﬁx tasks that already exist. In this pa-\nper, we use DReCa as a task augmentation strategy\nfor our method since it mitigates meta-overﬁtting\nwithout any additional unlabeled data.\nNLI\nQA\nFA\nFarsTail (10.3K)\nPersianQA (9K)\nEN\nXNLI (392k)\n—\nES\ntr. XNLI (392k)\n—\nDE\ntr. XNLI (392k)\n—\nFR\ntr. XNLI (392k)\n—\nTable 1: Overview of datasets from a variety of sources.\nFor the NLI task, we use the XNLI dataset for English,\nand its translated versions (tr.) for Spanish(ES), Ger-\nman(DE), and French(FR) provided in XTREME. For\neach dataset, the number of training instances is also\nmentioned.\n3\nThe Proposed Methodology\nIn our setting, ﬁrstly, we prepare a set of task-\nlanguage pairs to provide meta-learning tasks. Af-\nterward, in each episode, we sample some tasks and\nfeed them to the meta-learner. In the rest of this\nsection, we describe the proposed task sampling\nstrategy, the proposed meta-learning algorithm, and\nthe proposed task augmentation strategy.\n3.1\nThe Proposed Task Sampling Strategy\nIn meta-learning, task selection has a profound im-\npact on model performance. For this reason, we cre-\nate a queue of tasks ﬁrst. We can create this queue\nusing different scenarios such as selecting lan-\nguages for a target task (Gu et al., 2018), selecting\ntasks for a target language (Dou et al., 2019), and\npicking from various auxiliary languages and auxil-\niary tasks. In the meta-training section, we sample\nsome tasks from the queue. Formally, the queue’s\ntasks are represented by D. We need to sample\ntasks from M, which is a Multinomial distribution\nover PD(i)s. Thus, we investigate temperature-\nbased heuristic sampling (Aharoni et al., 2019),\nwhich deﬁnes the probability of any dataset as a\nfunction of its size as,\nPD(i) = q1/τ\ni\n/\n n\nX\nk=1\nq1/τ\nk\n!\n(1)\nwhere PD(i) is the probability of sampling the ith\ntask, qi is the size of ith task, and τ is the temper-\nature parameter. With τ = 1, tasks are randomly\nsampled proportionately to their dataset sizes, and\nwith τ →∞, they follow a uniform distribution.\n3.2\nThe Proposed Meta-learning Algorithms\nMeta-learning is the process of building a model\nthat can solve a new task with only a few labeled\nexamples by training on a variety of tasks with rich\nannotations. The key idea is to train the model’s\ninitial parameters such that the model has maximal\nperformance on a new task after the parameters\nhave been updated through zero or a couple of gra-\ndient steps (Yin, 2020). MAML (model-agnostic\nmeta-learning) (Finn et al., 2017) is one of the most\nsigniﬁcant algorithms. We describe one episode of\nthe MAML algorithm in Appendix A.1. MAML\nis quite difﬁcult to train, since there are two levels\nof training. Therefore, we use the following two\noptimization-based and metric-based meta-learning\nalgorithms in this work.\nReptile (Nichol et al., 2018) is a ﬁrst-order\noptimization-based algorithm that moves weights\ntoward a manifold of the weighted averages of\ntask-speciﬁc parameters θ(m)\ni\n. It samples training\ntasks from p(T ) : τ1, · · · , τi, · · · , τn. For each\ntraining task, it generates an episode that just con-\ntains the support set data. For training task τi,\nlet’s assume the original parameters θ have gone\nthrough m steps of updating and become θ(m)\ni\n(i.e.,\nθ(m)\ni\n= AdamW (Lτi, θ, m) (2)), then Reptile up-\ndates θ as follows (Yin, 2020):\nθ ←θ + β\n1\n|{T }|\nX\nτi∼M\n\u0010\nθ(m)\ni\n−θ\n\u0011\n(3)\nPrototypical Networks (Snell et al., 2017) is a\nmetric-based meta-learning algorithm. Prototypi-\ncal networks learn a metric space in which classiﬁ-\ncation can be performed by computing distances to\nprototype representations of each class. In general,\nthey are composed of an embedding network fθ and\na distance function d (x1, x2). Using the following\nequation, the embedding network encodes the sup-\nport set samples Sc and computes prototypes µc\nper class based on the mean sample encodings for\nthat class.\nµc =\n1\n|Sc|\nX\n(xi,yi)∈Sc\nfθ (xi) .\n(4)\nA Prototypical network classiﬁes a new sample\naccording to the following rule.\np(y = c | x) =\nexp (−d (fθ(x), µc))\nP\nc′∈C exp (−d (fθ(x), µc′))\n(5)\nThus, we deﬁne the distance-based cross entropy\n(DCE) loss as follows:\nLoss(DCE) = −log P (y = c | x)\n(6)\nTo ensure that the feature space is robust to noise,\nwe also use the Cross Entropy (CE) loss (more\ndetails can be found in Appendix A.3.1).\n3.3\nThe Task Augmentation Strategy\nFirst, we use dataset-as-tasks strategy that is the\nmost common method for selecting tasks for meta-\nlearning in NLP applications. Next, we employ\nDReCa to form additional high quality tasks. The\ngoal of DReCa is to take a heterogeneous task\n(such as a dataset) and produce a decomposed\nset of tasks. Given a training task T tr\ni , DReCa\nﬁrst groups examples by their labels, and then em-\nbeds examples within each group with an embed-\nding function EMBED(.). Concretely, for each\nN-way classiﬁcation task T tr\ni , it forms groups\ngi\nl = {(EMBED (xi) , yi) | yi = l}. Then, it pro-\nceeds to reﬁne each label group into K clusters\nvia k-means clustering to break down T tr\ni\ninto\ngroups\n\b\nCj \u0000gi\nl\n\u0001\tK\nj=1 for l = 1, 2, . . . , N. These\ncluster groups can be used to produce KN potential\nDReCa tasks. Each task is obtained by choosing\none of K clusters for each of the N label groups,\nand taking their union.\n4\nExperimental Setup\n4.1\nDatasets\nWe use FarsTail (Amirkhani et al., 2020) for the tar-\nget dataset. FarsTail is the only large-scale Persian\ncorpus for the NLI task, with 10,367 samples. The\nsamples are generated from 3,539 multiple-choice\nquestions with the least amount of annotators’ in-\nterventions or selected from natural sentences that\nalready exist independently in the wild, similarly\nto the SciTail dataset (Khot et al., 2018).\nWe also use XTREME (Hu et al., 2020) as an\nauxiliary dataset. XTREME is a multilingual multi-\ntask benchmark consisting of classiﬁcation, struc-\ntured prediction, QA, and retrieval tasks. We use\nthis benchmark to prepare NLI data for auxiliary\nlanguages. Note that, large-scale datasets for NLI\nwere only available in English. However, the au-\nthors of XTREME developed a custom-built trans-\nlation system to get translated datasets for NLI.\nFurthermore, we consider the QA as an auxiliary\ntask. Therefore, we use PersianQA (Ayoubi and\nDavoodeh, 2021) which is a Persian reading com-\nprehension dataset for QA, containing over 9000\nentries. Table 1 summarizes the employed dataset\nspeciﬁcations.\n4.2\nBaselines\nOn the FarsTail dataset, Amirkhani et al. (2020)\npresent results of various traditional and deep\nlearning-based methods.\nAccording to the re-\nsults of this paper, the highest test accuracy is\nobtained by using a translation-based approach,\ni.e., Translate-Source with fastText embeddings. In\nTranslate-Source, the Persian-translated MultiNLI\ntraining set is combined with FarsTail training\ndata for training an ESIM model. Furthermore,\nFarsTail’s authors reported mBERT ﬁne-tuning re-\nsults in FarsTail webpage1. Therefore, we use these\nresults as baselines.\n4.3\nImplementation Details\nIn this study, we aim to compare the effects of meta-\nlearning algorithms on classiﬁcation accuracy with\nthose of ﬁne-tuning and non-episodic algorithms.\nTo make a fair comparison, we ﬁrst ﬁne-tune our\npre-trained models using training data of the aux-\niliary task in a non-episodic approach. Afterward,\nwe ﬁne-tune the obtained model using the training\ndata of the target task. In this approach, we use\nmBERT (Devlin et al., 2019), and XLM-R (Con-\nneau et al., 2020), which are known as the state-of-\nthe-art multilingual pre-trained models, and Pars-\nBERT (Farahani et al., 2021) as a monolingual\ntransformer-based model for the Persian language.\nIn the meta-learning approach, we use the XLM-\nR model with output layers tailored for each task\nand train it with Reptile and Prototypical algo-\nrithms. To select the hyperparameters of the Rep-\n1https://github.com/dml-qom/FarsTail\ntile algorithm, we utilize the experiments done in\nTarunesh et al. (2021). Appendix A.2 provides fur-\nther details. The Prototypical algorithm is used\nonly in cross-lingual experiments, and we use Eu-\nclidean distance as its distance function. The auxil-\niary languages are arranged in two scenarios. In the\nﬁrst scenario, support and query set data are gener-\nated from auxiliary languages, while in the second\nscenario, the query set is drawn from both auxil-\niary and target languages. Detailed information is\nprovided in Appendix A.3.2.\nFurthermore, we ﬁne-tune the obtained models\non Persian training data using the following two\nmethods. The ﬁrst method is non-episodic, which\ninvolves ﬁne-tuning models in batches. The sec-\nond method is episodic, in which episodes are con-\nstructed ﬁrst, and then the models are ﬁne-tuned\naccording to the algorithm used.\n5\nResults\nThe meta-learning model is tested on different com-\nbinations and conﬁgurations of the auxiliary tasks.\nThe accuracy results of the Reptile algorithm are\npresented in Table 2. In addition to the zero-shot\nand ﬁne-tuning results, we report the accuracy of\nanother scenario. In this scenario, training data of\nthe target language is placed in the meta-training\nstage along with other auxiliary tasks and cooperate\nin a training process. Consequently, this scenario\ndoes not involve ﬁne-tuning phase. The results\nof the mentioned scenario are shown in the last\ncolumn of Table 2.\nTable 3 shows the accuracy scores using the Pro-\ntotypical Network. In the ﬁrst section of this table,\nwe generate both support and query sets from Per-\nsian language data, without using auxiliary tasks.\nIn the second section of this table, the results of the\nﬁrst multi-lingual scenario (where both the support\nand query sets are generated from auxiliary lan-\nguages data) are reported in rows 5 to 12. In rows\n13 to 16, we show the results of the second multi-\nlingual scenario (where the support set is drawn\nfrom auxiliary language data and the query set is\ndrawn from both auxiliary and Persian language\ndata). Lastly, we added the DReCa strategy and\npresented the results in rows 17 to 20.\nAdditionally, we conducted zero-shot evalua-\ntions of both algorithms. Zero-shot results are pre-\nsented in the ﬁrst accuracy column of Tables 2 and\n3. The confusion matrices of the best-performing\nmodels for both Reptile and Prototypical algo-\nrithms are also depicted in Appendix A.4.\n6\nDiscussion and Analysis\nTable 2 shows that the multi-lingual models are\nalways better than the multi-task models, due to\nthe fact that tasks like NLI (which require deeper\nsemantic representations) are more likely to beneﬁt\nfrom combining data from different languages. We\nfound that our meta-learned models perform bet-\nter than baselines and non-episodic models. The\nreason is that the goal of standard meta-learning\nis to ﬁnd a model that generalizes well to a new\ntarget task. In addition, we compared two different\nmeta-learning algorithms to evaluate their superi-\nority in this paper. From Tables 2 and 3, we can\nsee that Prototypical performed better than Rep-\ntile. It is because Prototypical networks use class\nrepresentations instead of example representations.\nTherefore, it ﬁnds a suitable representation for each\nclass during the meta-train stage.\nAs part of another experiment, we combined\ndata from the target language with data from other\nauxiliary tasks for meta-training. Based on the re-\nsults of these experiments (last column of Table 2\nfor Reptile and rows 13 to 16 of Table 3 for Proto-\ntypical), the model’s accuracy has decreased. This\nis due to the fact that target language data is small\nwhen compared with auxiliary language data. So,\nunbalanced training data confuses the training pro-\ncess and decreases the model’s accuracy. In any\ncase, the cooperation of the target language during\nthe training process is a great idea for future work.\nAs indicated in the last two columns of Table 3,\nepisodic ﬁne-tuning is signiﬁcantly superior to non-\nepisodic ﬁne-tuning. It demonstrates that episodic\ntraining is effective even on single language data\nand creates a generality in the level of training and\ntest data.\nWe examined the proximity between the fea-\nture spaces of the auxiliary languages and the tar-\nget language quantitatively and qualitatively. At\nﬁrst, we collect representations of the auxiliary\nand target languages from non-episodic, Reptile,\nand Prototypical models. In Fig. 1, we present 2-\ncomponent PCA visualization for comparison. We\nalso evaluated the models using a distance metric\ncommonly used in vision and NLP tasks (Hutten-\nlocher et al., 1993; Dubuisson and Jain, 1994; Pa-\ntra et al., 2019; Xia et al., 2021). Informally, the\nHausdorff distance measures the distance between\ndata representations of auxiliary languages and the\nRow\nModel\nShot\nAux. Tasks\nZero-shot\nNon-episodic f.t.\nAdd NLI-fa in m.t.\nBaselines\n1\nTranslate-Source∗\n—\n—\n—\n78.13\n—\n2\nmBERT∗\n—\n—\n—\n83.38\n—\nNon-episodic approach\n3\nParsBERT\n—\n—\n—\n74.64\n—\n4\nmBERT\n—\n—\n—\n81.95\n—\n5\n—\nNLI-en\n56.53\n81.38\n—\n6\n—\nNLI-(en, es, de, fr)\n67.88\n82.34\n—\n7\nXLM-R\n—\n—\n—\n81.97\n—\n8\n—\nNLI-en\n69.49\n86.55\n—\n9\n—\nNLI-(en, es, de, fr)\n69.09\n84.69\n—\nMeta-learning approach\n10\nXLM-R\n1\nNLI-en\n64.19\n84.31\n83.37\n11\n4\n70.96\n87.17\n86.00\n12\n8\n70.70\n87.11\n86.65\n13\n16\n71.03\n87.43\n86.52\n14\n1\nNLI-(en, es, de, fr)\n65.17\n85.21\n83.91\n15\n4\n72.27\n87.57\n85.74\n16\n8\n71.61\n88.35\n88.22\n17\n16\n71.22\n88.02\n87.76\n18\n1\nQA-fa\n34.18\n81.48\n81.58\n19\n4\n34.18\n81.38\n84.96\n20\n8\n33.79\n82.14\n83.59\n21\n16\n34.18\n82.23\n84.70\n22\n1\nNLI-en, QA-fa\n46.42\n83.53\n85.16\n23\n4\n66.02\n86.52\n86.26\n24\n8\n64.26\n86.98\n86.46\n25\n16\n46.88\n86.52\n86.13\nTable 2: Average test accuracy of the Reptile algorithm with baselines and non-episodic approach results on the\nPersian NLI task. The ﬁrst accuracy column shows results before ﬁne-tuning on the Persian NLI train-set (called\nzero-shot). In the second accuracy column, we provided results after ﬁne-tuning (f.t.) on the Persian NLI train-sets.\nThe last accuracy column reports results of using the Persian NLI train-set in the meta-training phase (m.t.). The\ndata with ∗comes from FarsTail’s paper and webpage.\nRow\nModel\nShot\nSupport\nQuery\nZero-shot\nNon-episodic f.t.\nEpisodic f.t.\nWithout auxiliary tasks\n1\nXLM-R\n1\nNLI-fa\nNLI-fa\n—\n70.38\n79.30\n2\n4\n—\n81.97\n85.22\n3\n8\n—\n83.98\n84.64\n4\n16\n—\n85.29\n85.74\nWith auxiliary tasks\n5\nXLM-R\n1\nNLI-en\nNLI-en\n68.10\n84.83\n86.07\n6\n4\n70.57\n86.72\n87.50\n7\n8\n70.77\n86.72\n87.37\n8\n16\n73.18\n87.76\n88.54\n9\n1\nNLI-(en, es, de, fr)\nNLI-(en, es, de, fr)\n69.15\n85.01\n85.97\n10\n4\n70.25\n86.78\n87.63\n11\n8\n71.09\n88.48\n89.39\n12\n16\n72.20\n88.15\n88.28\n13\n1\nNLI-(en, es, de, fr)\nNLI-(en, es, de, fr, fa)\n—\n84.15\n85.12\n14\n4\n—\n86.33\n86.78\n15\n8\n—\n86.33\n86.46\n16\n16\n—\n86.78\n87.24\n17\nXLM-R+\nDReCa\n8\nNLI-en\nNLI-en\n70.44\n87.96\n88.87\n18\n16\n71.94\n87.24\n88.74\n19\n8\nNLI-(en, es, de, fr)\nNLI-(en, es, de, fr)\n71.16\n87.74\n88.48\n20\n16\n71.61\n87.30\n88.22\nTable 3: Average test accuracy on the Persian NLI task using Prototypical algorithm with and without auxiliary\ntasks. The last accuracy column reports results after episodic ﬁne-tuning (f.t.) on the Persian NLI train-set.\nFigure 1: PCA visualization of non-episodic, Reptile, and Prototypical models to examine the closeness of the\nauxiliary and target languages feature spaces.\ntarget language. Given a set of representations\nof the auxiliary language S = {s1, s2, . . . , sm}\nand a set of representations of the target language\nT = {t1, t2, . . . , tm} we compute the Hausdorff\ndistance as follows:\nmax\n\u001a\nmax\ns∈S min\nt∈T d(s, t), max\nt∈T min\ns∈S d(s, t)\n\u001b\n(7)\nwhere cosine distance is used as the inner distance,\ni.e.,\nd(s, t) ≜1 −cos(s, t)\n(8)\nCompared to the non-episodic method, we ob-\nserve a drastic drop of Hausdorff distance from\n0.18 to 0.05 for Prototypical and also, we see a\nminor decline of Hausdorff distance from 0.18 to\n0.13 for Reptile. Both qualitative visualization and\nquantitative metrics conﬁrm that meta-learning ap-\nproaches bring the distributions of auxiliary and\ntarget language data closer together, thus increas-\ning the accuracy on the target language.\nThe advantage of meta-learning methods is that\nthey obtain the appropriate initial parameters for\nthe target language, as mentioned. The zero-shot\ntest is used as a criterion to evaluate this point,\nand it shows that meta-learning-based models are\nmore accurate than other methods. The generality\nof the initial parameters can also be assessed via\ncanonical correlation analysis (CCA) (Raghu et al.,\n2017; Morcos et al., 2018). Using this criterion,\nwe compare the output of each layer before and\nafter ﬁne-tuning, and the results are presented in\nFig. 2. The meta-learning models have a higher\nCCA similarity, which indicates the model obtained\nmore general parameters before ﬁne-tuning.\nIn the next experiment, we apply the DReCa\nstrategy and train the model with the Prototypi-\ncal algorithm. According to Table 3, some results\nhave improved, while others have remained the\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0.6\n0.7\n0.8\n0.9\n1\nlayer\nCCA similarity\nReptile nli-(en,fr,es,de)\nReptile nli-(en)\nPrototypical nli-(en,fr,es,de)\nPrototypical nli-(en)\nNon-episodic XLM-R nli-(en)\nNon-episodic XLM-R nli-(en,fr,es,de)\nNon-episodic mBERT nli-(en,fr,es,de)\nFigure 2: CCA similarity for each transformer layer.\nWe calculate the similarity before and after ﬁne-tuning\non the FarsTail training data.\nsame. It illustrates that task augmentation in meta-\nalgorithms affects the model’s accuracy. However,\ndeﬁning the appropriate task augmentation strategy\nstill needs research.\n7\nConclusion\nWe present effective use of meta-learning to ben-\neﬁt from other tasks or languages. We advanta-\ngeously leverage this approach to improve NLI in\nPersian as a low-resource language. We found that\nour meta-learning model outperformed competi-\ntive baseline models. In response to the concept of\ntreating entire datasets as tasks, we use DReCa as\na general-purpose task augmenting approach. Fi-\nnally, zero-shot evaluations illustrate the generality\nof the results obtained by meta-learning. This work\nwill be extended to other cross-lingual NLP tasks\nin Persian in the future. Furthermore, we would\nlike to use a self-supervised approach to provide a\nuseful starting point for parameters.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies.\nAssociation for Computational Linguistics.\nHossein Amirkhani, Mohammad Azari Jafari, Azadeh\nAmirak,\nZohreh\nPourjafari,\nSoroush\nFaridan\nJahromi, and Zeinab Kouhkan. 2020. FarsTail: A\npersian natural language inference dataset.\narXiv\npreprint arXiv:2009.08820.\nSajjad Ayoubi and Mohammad Yasin Davoodeh. 2021.\nPersianqa: a dataset for persian question answering.\nTrapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai,\nand Andrew McCallum. 2020.\nSelf-supervised\nmeta-learning for few-shot natural language classiﬁ-\ncation tasks. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP). Association for Computational Lin-\nguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nQian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2016. Enhanced LSTM\nfor natural language inference.\narXiv preprint\narXiv:1609.06038.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics. Association\nfor Computational Linguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining.\nAdvances in\nNeural Information Processing Systems.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nZi-Yi Dou, Keyi Yu, and Antonios Anastasopoulos.\n2019.\nInvestigating meta-learning algorithms for\nlow-resource natural language understanding tasks.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP). Associa-\ntion for Computational Linguistics.\nMarie-Pierre Dubuisson and Anil K. Jain. 1994.\nA\nmodiﬁed hausdorff distance for object matching. In\nProceedings of 12th International Conference on\nPattern Recognition. IEEE.\nMehrdad\nFarahani,\nMohammad\nGharachorloo,\nMarzieh Farahani,\nand Mohammad Manthouri.\n2021.\nParsBERT: Transformer-based model for\npersian language understanding.\nNeural Process.\nLett.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In International Conference on Ma-\nchine Learning. PMLR.\nJiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li,\nand Kyunghyun Cho. 2018. Meta-learning for low-\nresource neural machine translation. In Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing. Association for Com-\nputational Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics. Association\nfor Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalization. In International Conference on Machine\nLearning. PMLR.\nJavier Huertas-Tato, Alejandro Martín, and David\nCamacho. 2021.\nSML: a new semantic em-\nbedding alignment transformer for efﬁcient cross-\nlingual natural language inference. arXiv preprint\narXiv:2103.09635.\nDaniel P Huttenlocher, Gregory A. Klanderman, and\nWilliam J Rucklidge. 1993. Comparing images us-\ning the hausdorff distance. IEEE Transactions on\npattern analysis and machine intelligence.\nDaniel Khashabi, Arman Cohan, Siamak Shakeri,\nPedram Hosseini,\nPouya Pezeshkpour,\nMalihe\nAlikhani, Moin Aminnaseri, Marzieh Bitaab, Faeze\nBrahman,\nSarik\nGhazarian,\nMozhdeh\nGheini,\nArman Kabiri, Rabeeh Karimi Mahabagdi, Omid\nMemarrast,\nAhmadreza\nMosallanezhad,\nErfan\nNoury, Shahab Raji, Mohammad Sadegh Rasooli,\nSepideh Sadeghi, Erfan Sadeqi Azer, Niloofar Saﬁ\nSamghabadi,\nMahsa\nShafaei,\nSaber\nSheybani,\nAli Tazarv, and Yadollah Yaghoobzadeh. 2021.\nParsiNLU: A Suite of Language Understanding\nChallenges for Persian.\nTransactions of the\nAssociation for Computational Linguistics.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nSciTaiL: A textual entailment dataset from science\nquestion answering. In Thirty-Second AAAI Confer-\nence on Artiﬁcial Intelligence.\nGregory Koch, Richard Zemel, Ruslan Salakhutdinov,\net al. 2015. Siamese neural networks for one-shot\nimage recognition.\nIn ICML deep learning work-\nshop.\nBo Liu, Hao Kang, Haoxiang Li, Gang Hua, and Nuno\nVasconcelos. 2020. Few-shot open-set recognition\nusing meta-learning.\nIn 2020 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition.\nIlya Loshchilov and Frank Hutter. 2018. Fixing weight\ndecay regularization in adam.\nBill MacCartney. 2009.\nNatural language inference.\nStanford University.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems.\nAri Morcos, Maithra Raghu, and Samy Bengio. 2018.\nInsights on representational similarity in neural net-\nworks with canonical correlation.\nIn Advances in\nNeural Information Processing Systems 31. Curran\nAssociates, Inc.\nShikhar Murty, Tatsunori B. Hashimoto, and Christo-\npher Manning. 2021. DReCa: A general task aug-\nmentation strategy for few-shot natural language in-\nference. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies.\nAlex Nichol, Joshua Achiam, and John Schulman.\n2018.\nOn ﬁrst-order meta-learning algorithms.\narXiv preprint arXiv:1803.02999.\nFarhad Nooralahzadeh, Giannis Bekoulis, Johannes\nBjerva, and Isabelle Augenstein. 2020.\nZero-shot\ncross-lingual transfer with meta learning.\nIn Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP). As-\nsociation for Computational Linguistics.\nAnkur Parikh, Oscar Täckström, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing. Association for\nComputational Linguistics.\nBarun Patra, Joel Ruben Antony Moniz, Sarthak Garg,\nMatthew R. Gormley, and Graham Neubig. 2019.\nBilingual lexicon induction with semi-supervision in\nnon-isometric embedding spaces.\nIn Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics. Association for Compu-\ntational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations.\nIn Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies. Association for Computational\nLinguistics.\nMatúš Pikuliak, Marián Šimko, and Mária Bieliková.\n2021. Cross-lingual learning for text processing: A\nsurvey. Expert Systems with Applications.\nKunxun Qi and Jianfeng Du. 2020. Translation-based\nmatching adversarial network for cross-lingual natu-\nral language inference. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence.\nKun Qian and Zhou Yu. 2019. Domain adaptive dialog\ngeneration via meta learning. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics. Association for Computational\nLinguistics.\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. 2017. Svcca: Singular vec-\ntor canonical correlation analysis for deep learning\ndynamics and interpretability. In Advances in Neu-\nral Information Processing Systems 30. Curran As-\nsociates, Inc.\nJanarthanan Rajendran, Alexander Irpan, and Eric Jang.\n2020.\nMeta-learning requires meta-augmentation.\nIn Advances in Neural Information Processing Sys-\ntems. Curran Associates, Inc.\nSachin Ravi and Hugo Larochelle. 2016. Optimization\nas a model for few-shot learning.\nIn 5th Interna-\ntional Conference on Learning Representations.\nJasdeep Singh, Bryan McCann, Nitish Shirish Keskar,\nCaiming Xiong, and Richard Socher. 2019. XLDA:\ncross-lingual data augmentation for natural language\ninference and question answering.\narXiv preprint\narXiv:1905.11471.\nJake Snell, Kevin Swersky, and Richard Zemel. 2017.\nPrototypical networks for few-shot learning. In Pro-\nceedings of the 31st International Conference on\nNeural Information Processing Systems.\nAarne Talman, Anssi Yli-Jyrä, and Jörg Tiedemann.\n2019. Sentence embeddings in nli with iterative re-\nﬁnement encoders. Natural Language Engineering.\nIshan Tarunesh, Sushil Khyalia, Vishwajeet Kumar,\nGanesh Ramakrishnan, and Preethi Jyothi. 2021.\nMeta-learning for effective multi-task and multilin-\ngual modelling. In Proceedings of the 16th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics: Main Volume. Associa-\ntion for Computational Linguistics.\nEleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pas-\ncal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin,\nCarles Gelada, Kevin Swersky, Pierre-Antoine Man-\nzagol, et al. 2019.\nMeta-dataset:\nA dataset of\ndatasets for learning to learn from few examples.\narXiv preprint arXiv:1903.03096.\nNiels van der Heijden, Helen Yannakoudakis, Pushkar\nMishra, and Ekaterina Shutova. 2021. Multilingual\nand cross-lingual document classiﬁcation: A meta-\nlearning approach. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume. Asso-\nciation for Computational Linguistics.\nZihan Wang, Stephen Mayhew, Dan Roth, et al. 2019.\nCross-lingual ability of multilingual bert: An empir-\nical study. arXiv preprint arXiv:1912.07840.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP). Associa-\ntion for Computational Linguistics.\nMengzhou Xia, Guoqing Zheng, Subhabrata Mukher-\njee,\nMilad\nShokouhi,\nGraham\nNeubig,\nand\nAhmed Hassan Awadallah. 2021.\nMetaXL: Meta\nrepresentation\ntransformation\nfor\nlow-resource\ncross-lingual learning. In Proceedings of the 2021\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies. Association for\nComputational Linguistics.\nMingzhang Yin, George Tucker, Mingyuan Zhou,\nSergey Levine, and Chelsea Finn. 2019.\nMeta-\nlearning without memorization.\narXiv preprint\narXiv:1912.03820.\nWenpeng Yin. 2020. Meta-learning for few-shot natu-\nral language processing: A survey. arXiv preprint\narXiv:2007.09604.\nA\nAppendix\nA.1\nMAML Description\nMAML is one of the most popular meta-learning\nalgorithms and it has proven its effectiveness in\nvarious ﬁelds (e.g., computer vision). MAML is\nable to ﬁnd good initialization parameter values\nand adapt to new tasks quickly. This algorithm can\nbe performed in one episode by following these\nsteps:\n• Make a copy of the model with its initial pa-\nrameters θ.\n• Use the training set Dtrain\ni\nto train the model\nas\nˆθ = θ −α∇θLi\n\u0000θ, Dtrain\ni\n\u0001\n(9)\n• Apply the model with the updated parameters\nˆθ to the validation set Dval\ni\n.\n• Use the loss on the validation set to update the\ninitial parameters θ\nθ = θ −β∇θ\nX\ni\nLi\n\u0010\nˆθ, Dval\ni\n\u0011\n(10)\nA.2\nHyperparameters\nModels are implemented using the PyTorch2 frame-\nwork. ParsBERT, mBERT and XLM-R implemen-\ntations are taken from the HuggingFace library 3.\nIn our experiments, we used the AdamW opti-\nmizer (Loshchilov and Hutter, 2018) with learning\nrate 1e-5 to perform the inner loop of the Reptile\nalgorithm (2), which is known as meta-step. The\nhyperparameters for the Reptile algorithm are listed\nin Table 4.\nHyperparameter\nValue\nepochs\n2\nnumber of iterations\n20000\nsequence length (for NLI)\n128\nsequence length (for QA)\n384\ndropout\n0.1\noptimizer\nAdamW\nlearning rate\n1e-5\nupdate steps (m)\n3\nnumber of class per episode (way)\n2\nqueue length\n4\ntemperature parameter (τ)\n1\nTable 4: Hyperparameters for the Reptile algorithm\nThe hyperparameters for the Prototypical algo-\nrithm are also shown in Table 5. Some parame-\nters are calculated based on a grid search, such as\nDistance Cross-Entropy (DCE) and Cross-Entropy\n(CE) coefﬁcients, and others are chosen similar to\nthe Reptile algorithm.\nThe number of iterations parameter varies ac-\ncording to the value of the shot, and is chosen to\nensure that all instances in the dataset appear at\nleast once in each epoch.\n2https://pytorch.org/\n3https://huggingface.co/\nHyperparameter\nValue\nepochs\n2\nnumber of iterations\n20000\nsequence length (for NLI)\n128\ndropout\n0.1\noptimizer\nAdamW\nlearning rate\n1e-5\nnumber of class per episode (way)\n3\nDCE coefﬁcient (λ1)\n1.0\nCE coefﬁcient (λ2)\n1.0\nTable 5: Hyperparameters for the Prototypical algo-\nrithm\nA.3\nPrototypical Networks\nA.3.1\nLoss Function\nAs we mentioned in section 2.2, the primary loss\nfunction of the Prototypical algorithm is DCE.\nSince a prototype consists of distribution informa-\ntion from instances associated with it, the choice of\nthese instances may introduce noise in the learned\nrepresentation if the neural network is trained only\nby using the DCE loss. We use CE loss in addition\nto the DCE loss to make the feature space robust\nto noise. As a whole, we train the model with a\ncombination of DCE loss and CE loss given by the\nfollowing equation.\nLoss(overall) = λ1 Loss(DCE) + λ2 Loss(CE)\n(11)\nA.3.2\nScenarios\nWe considered two scenarios for making the\nepisodes. In the ﬁrst scenario, the model is trained\nonly on auxiliary languages, then ﬁne-tuned using\nthe target language. Therefore, only auxiliary lan-\nguages are used to generate support and query sets.\nAn episode of the ﬁrst scenario is shown in Table 6.\nIn the second scenario, in addition to auxiliary\nlanguages, we also used the target language for\ntraining. So, the support set is constructed from\nauxiliary language data and the query set is gener-\nated from both auxiliary and Persian language data.\nTable 7 shows an episode of the second scenario.\nA.4\nConfusion Matrices\nThe confusion matrices for the top-performing\nmodels (8-shot with four auxiliary languages) is de-\npicted in Fig. 3 showing the success of this method\nin improving the accuracy in all classes specially\nthe neutral class.\nExample\nCategory\nSupport set (or Query set)\nIn the midst of this amazing amalgam of cultures is a passion for continuity\n⇒A passion for continuity is not the most important of these cultures\nneutral\nThe river plays a central role in all visits to Paris.\n⇒The river is central to all vacations to Paris\nentailment\nFor the moment, he sought refuge in retreat, and left the room precipitately.\n⇒He stayed put and sat on the ﬂoor.\ncontradiction\nTable 6: Example for a 3-way 1-shot episode in the ﬁrst scenario. In this example we select support set and query\nset samples from English dataset. As support and query sets are generated similarly, only one set is shown in this\ntable.\nExample\nCategory\nSupport set\nRecuerda que una vez mencionó que su padre era médico?\n⇒Ella mencionó que su padre era médico hace mucho tiempo\nneutral\nDies ist etwas anderes als eine Cantina-Leuchte\n⇒Dies ist sicherlich keine Cantina-Leuchte\nentailment\nEnsuite, il enfonce un tube respiratoire dans la gorge du patient mort.\n⇒Le patient vit toujours.\ncontradiction\nEnglish Translation\nYou remember her once mentioning that her father was a doctor?\n⇒She mentioned her father being a doctor a long time ago.\nneutral\nThis is something other than a cantina ﬁxture.\n⇒This is certainly not a cantina ﬁxture.\nentailment\nNext he shoves a breathing tube down the dead patient ’s throat .\n⇒The patient is still alive.\ncontradiction\nQuery set\nUne pièce qualiﬁe Frank Lloyd Wright de terrible ingénieur.\n⇒Piece a également déclaré que Wright était un bien meilleur concepteur.\nneutral\nSus rápidos oídos captaron el sonido del tren que se acercaba.\n⇒Escuchó que el tren se acercaba rápidamente.\nentailment\n.از قرن دوازدهم به بعد ارقام عربی برای نخستین بار در ایتالیا کاربرد یافت\nفرانسه اولین کشوری بود که از ارقام عربی استفاده کرد.⇒contradiction\nEnglish Translation\nA piece calls Frank Lloyd Wright an awful engineer.\n⇒Piece also stated Wright was a much better designer.\nneutral\nHer quick ears caught the sound of the approaching train.\n⇒She heard the train approaching fast.\nentailment\nFrom the twelfth century onwards, Arabic numerals were ﬁrst used in Italy.\n⇒France was the ﬁrst country to use Arabic numerals.\ncontradiction\nTable 7: Example for a 3-way 1-shot episode in the second scenario. In this example, the support set samples\nare selected from French, Spanish, and German datasets, respectively, and the query set samples are selected from\nFrench, Spanish, and Persian datasets, respectively.\ncontradiction\nentailment\nneutral\ncontradiction\nentailment\nneutral\n229\n146\n56\n34\n454\n22\n133\n45\n347\nTrue label\nBefore f.t. (Zero-shot test)\ncontradiction\nentailment\nneutral\ncontradiction\nentailment\nneutral\n426\n51\n24\n45\n451\n14\n32\n13\n480\nAfter f.t.\ncontradiction\nentailment\nneutral\ncontradiction\nentailment\nneutral\n415\n53\n33\n39\n458\n13\n29\n14\n482\nUsing NLI-Fa in m.t.\ncontradiction\nentailment\nneutral\ncontradiction\nentailment\nneutral\n324\n126\n51\n45\n445\n20\n155\n47\n323\nBefore f.t. (Zero-shot test)\ncontradiction\nentailment\nneutral\ncontradiction\nentailment\nneutral\n426\n52\n23\n46\n455\n9\n35\n12\n478\nPredicted label\nAfter non-episodic f.t.\ncontradiction\nentailment\nneutral\ncontradiction\nentailment\nneutral\n428\n45\n28\n48\n452\n10\n22\n10\n493\nAfter episodic f.t.\n0\n100\n200\n300\n400\n500\nFigure 3: Confusion matrices of the best-obtained model (8-shot with four auxiliary languages) in both meta-\nlearning algorithms on the FarsTail test set. (Top): Reptile algorithm results. (Bottom): Prototypical algorithm\nresults.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-05-18",
  "updated": "2022-05-18"
}