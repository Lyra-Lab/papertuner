{
  "id": "http://arxiv.org/abs/2205.09550v1",
  "title": "Data Valuation for Offline Reinforcement Learning",
  "authors": [
    "Amir Abolfazli",
    "Gregory Palmer",
    "Daniel Kudenko"
  ],
  "abstract": "The success of deep reinforcement learning (DRL) hinges on the availability\nof training data, which is typically obtained via a large number of environment\ninteractions. In many real-world scenarios, costs and risks are associated with\ngathering these data. The field of offline reinforcement learning addresses\nthese issues through outsourcing the collection of data to a domain expert or a\ncarefully monitored program and subsequently searching for a batch-constrained\noptimal policy. With the emergence of data markets, an alternative to\nconstructing a dataset in-house is to purchase external data. However, while\nstate-of-the-art offline reinforcement learning approaches have shown a lot of\npromise, they currently rely on carefully constructed datasets that are well\naligned with the intended target domains. This raises questions regarding the\ntransferability and robustness of an offline reinforcement learning agent\ntrained on externally acquired data. In this paper, we empirically evaluate the\nability of the current state-of-the-art offline reinforcement learning\napproaches to coping with the source-target domain mismatch within two MuJoCo\nenvironments, finding that current state-of-the-art offline reinforcement\nlearning algorithms underperform in the target domain. To address this, we\npropose data valuation for offline reinforcement learning (DVORL), which allows\nus to identify relevant and high-quality transitions, improving the performance\nand transferability of policies learned by offline reinforcement learning\nalgorithms. The results show that our method outperforms offline reinforcement\nlearning baselines on two MuJoCo environments.",
  "text": "Data Valuation for Ofﬂine Reinforcement Learning\nAmir Abolfazli, Gregory Palmer and Daniel Kudenko\nAbstract— The success of deep reinforcement learning (DRL)\nhinges on the availability of training data, which is typically\nobtained via a large number of environment interactions. In\nmany real-world scenarios, costs and risks are associated with\ngathering these data. The ﬁeld of ofﬂine reinforcement learning\naddresses these issues through outsourcing the collection of\ndata to a domain expert or a carefully monitored program\nand subsequently searching for a batch-constrained optimal\npolicy. With the emergence of data markets, an alternative to\nconstructing a dataset in-house is to purchase external data.\nHowever, while state-of-the-art ofﬂine reinforcement learning\napproaches have shown a lot of promise, they currently rely\non carefully constructed datasets that are well aligned with\nthe intended target domains. This raises questions regarding\nthe transferability and robustness of an ofﬂine reinforcement\nlearning agent trained on externally acquired data. In this\npaper, we empirically evaluate the ability of the current\nstate-of-the-art ofﬂine reinforcement learning approaches to\ncoping with the source-target domain mismatch within two\nMuJoCo environments, ﬁnding that current state-of-the-art\nofﬂine reinforcement learning algorithms underperform in the\ntarget domain. To address this, we propose data valuation for\nofﬂine reinforcement learning (DVORL), which allows us to\nidentify relevant and high-quality transitions, improving the\nperformance and transferability of policies learned by ofﬂine\nreinforcement learning algorithms. The results show that our\nmethod outperforms ofﬂine reinforcement learning baselines on\ntwo MuJoCo environments.\nI. INTRODUCTION\nOfﬂine Reinforcement Learning (RL) – also known as\nbatch-constrained RL – is a class of RL methods that requires\nthe agent to learn from a static dataset of pre-collected\nexperiences without further environment interaction [1]. This\nlearning paradigm disentangles exploration from exploita-\ntion, lending itself to the tasks in which exploration for\ncollecting data is costly, time-consuming, or risky [2], [3].\nBy taking advantage of pre-collected datasets, ofﬂine RL can\nmitigate the technical concerns associated with online data\ncollection, and has potential beneﬁts for a number of real\nenvironments, such as human-robot collaboration [4].\nOfﬂine reinforcement learning outsources the collection\nof data to a domain expert and subsequently searches for a\nbatch-constrained optimal policy. However, this task is chal-\nlenging, as ofﬂine RL methods suffer from the extrapolation\nerror [3], [5]. This pathology occurs when ofﬂine deep RL\nmethods are trained under one distribution but evaluated on a\ndifferent one. More speciﬁcally, value functions implemented\nby a function approximator have a tendency to predict\nunrealistic values for unseen state-action pairs for standard\noff-policy deep RL algorithms (e. g., DQN and DDPG). This\nL3S\nResearch\nCenter,\nLeibniz\nUniversity\nHannover,\nGermany\n{abolfazli, gpalmer, kudenko}@l3s.de\nraises the need for approaches that restrict the action space,\nforcing the agent to learn a behavior that is close to on-policy\nwith respect to a subset of the given source data [3].\nFor ofﬂine RL methods that are designed to mitigate the\nextrapolation error [3], [5], [6], there remains the challenge\nthat external data (e. g., purchased from data markets) may\nnot be well aligned with the intended target domain. There-\nfore, the learned policy induces a different visited state-action\ndistribution that results in a degradation in the performance\nof the ofﬂine RL agent.\nIn recent years there have been a number of efforts within\nthe paradigm of supervised learning for overcoming the\nsource-target domain mismatch problem via valuating data,\nincluding data Shapley [7] and data valuation using rein-\nforcement learning (DVRL) [8]. Such methods have shown\npromising results on several application scenarios such as\ndata-value quantiﬁcation, corrupted sample discovery, robust\nlearning with noisy labels, and domain adaptation [8]. This\nraises the question: data valuation improve the transferability\nand robustness of an ofﬂine RL agent trained on externally\nacquired data?\nTo investigate the above question, we propose a data\nvaluation approach that selects a subset of samples in the\nsource dataset that are relevant to the target task. Our main\ncontributions can be summarized as follows:\n• Inspired by DVRL [8], we propose Data Valuation for\nOfﬂine Reinforcement Learning (DVORL) that for a\ngiven ofﬂine RL method, a ﬁxed source dataset, and\na small target dataset, identiﬁes those samples of the\nsource buffer that are relevant to the target task.\n• We contribute a benchmark on two Gym MuJoCo\ndomains (Hopper and Walker2d) for which parameteri-\nzations (friction and mass of torso) for the target domain\nare different from those of the source domain.\n• We show that the state-of-the-art ofﬂine RL methods fail\nto generalize to different target domain conﬁgurations.\n• We show how our data valuation approach can improve\nthe generalizability of the RL agent to the target domain\nby outperforming the existing state-of-the-art methods\non all considered target domain conﬁgurations.\nThe rest of this paper is organized as follows. Section II\ngives the background on (ofﬂine) RL. In Section III, we for-\nmally deﬁne the source-target domain mismatch problem for\nofﬂine RL, and provide a motivating example in Section IV.\nSection V gives an overview of related work. In Section VI,\nwe introduce our DVORL framework. Section VII describes\nour experiment setup. We discuss our results in Section VIII,\nand in Section IX, we discuss our main ﬁndings. Finally in\nSection X we conclude with suggestions for future work.\narXiv:2205.09550v1  [cs.LG]  19 May 2022\nII. BACKGROUND\nA. Reinforcement Learning\nThe RL problem is typically modeled by a Markov deci-\nsion process (MDP), formulated as a tuple (X, U, p, r, γ),\nwith a state space X, an action space U, and transition\ndynamics p (x′ | x, u). At each discrete time step the agent\nperforms an action u ∈U in a state x ∈X, and transitions\nto a new state x′ ∈X based on the transition dynamics\np (x′ | x, u), and receives a reward r (x, u, x′). The goal\nof the agent is to maximize the expectation of the sum\nof discounted rewards, also known as the return Rt =\nP∞\ni=t+1 γir (xi, ui, xi+1), which weighs future rewards with\nrespect to the discount factor γ ∈[0, 1), determining the\neffective horizon. The agent makes decisions via a policy\nπ : X →P(U), which maps a given state x to a probability\ndistribution over the action space U. For a given policy π, the\nvalue function is deﬁned as the expected return of an agent,\nstarting from state x, performing action u, and following\nthe policy Qπ(s, a) = Eπ [Rt | x, u]. The state-action value\nfunction can be computed through the Bellman equation of\nthe Q function:\nQπ(x, u) = Es′∼p [r (x, u, x′) + γEu′∼πQπ(x′, u′)] .\n(1)\nGiven Qπ, the optimal policy π∗= maxu Q∗(x, u), can be\nobtained by greedy selection over the optimal value function\nQ∗(x, u) = maxπ Qπ(x, u). For environments confronting\nagents with the curse of dimensionality the value can be es-\ntimated with a differentiable function approximator Qθ(x, u),\nwith parameters θ.\nIn this work, we focus on continuous control problems,\nwhere, given a parameterized policy πϑ our objective is to\nﬁnd an optimal policy π∗\nϑ, with respect to the parameters ϑ,\nwhich maximizes the expected return from the start distri-\nbution J(ϑ) = Exi∼pπ,ui∼π[R0] [9]. The policy parameters\nϑ can be updated by taking the gradient of the expected\nreturn ∇ϑJ(ϑ). A popular approach to optimizing the policy\nis to use actor-critic methods, where the actor (policy) can\nbe updated through the deterministic policy gradient algo-\nrithm [10]: ∇ϑJ(ϑ) = Ex∼pπ[∇uQπ\nθ (x, u)|u,π(x)∇ϑπϑ(x)],\nin which the value function Qπ\nθ (x, u) is the critic.\nB. Ofﬂine Reinforcement Learning\nStandard off-policy deep RL algorithms such as deep Q-\nlearning (DQN) [11] and deep deterministic policy gradient\n(DDPG) [9] are applicable in batch RL as they are based\non more fundamental batch RL algorithms [12]. However,\nthey suffer from a phenomenon, known as extrapolation\nerror, which occurs when there is a mismatch between the\ngiven ﬁxed batch of data and true state-action visitation\nof the current policy [3]. This is problematic as incorrect\nvalues of state-action pairs, which are not contained in the\nbatch of data, are propagated through temporal difference\nupdates of most off-policy algorithms [13], resulting in poor\nperformance of the model [14]. Below we give an overview\nof approaches designed to address the extrapolation error,\nwhich will serve as our baselines.\nBCQ. Batch-Constrained deep Q-learning [3] is an ofﬂine\nRL method for continuous control that restricts the action\nspace, thereby eliminating actions that are unlikely to be\nselected by the behavioral policy πb and therefore rarely\nobserved in the batch. Given a dataset of N transitions\nD = {xt, ut, rt+1, xt+1}N\nt=0, collected by a behavior pol-\nicy πb, BCQ consists of four parameterized networks: a\ngenerative model Gω : X →U, parameterized with ω, a\nperturbation model ξφ(x, u, Φ), parameterized with φ, and\ntwo Q-networks Qϑ1(x, u), Qϑ2(x, u), parameterized with\nϑ1 and ϑ2, respectively. The generative model Gω selects\nthe most likely action given the state with respect to the\ndata in the batch. Since modeling the distribution of data\nin the high dimensional continuous control environments\nis not straightforward, a variational autoencoder (VAE) is\nused to approximate it. The policy is deﬁned by sampling n\nactions from Gω(x) and selecting the highest valued action\naccording to a Q-network as it is easier to sample from\nπb(u | x) than modeling πb(u | x) in a continuous action\nspace. The perturbation model ξφ(x, u, Φ), parameterized\nwith φ, models the distribution of data in the batch and is a\nresidual added to the sampled actions in the range [−Φ, Φ].\nThis model is trained with the DDPG [10] and can be thought\nof as a behavioral cloning model. Since the perturbation\nmodel together with the sampling can be considered as a\nhierarchical policy, BCQ can also be considered an actor-\ncritic method.\nCQL. To prevent the training policy from overestimating\nthe Q-values, Conservative Q-Learning (CQL) [6] utilizes a\npenalized empircal RL objective. More precisely, CQL opti-\nmizes the value function not only to minimize the temporal\ndifference error based on the interactions seen on the dataset\nbut also minimizes the value of actions that the currently\ntrained policy takes, while at the same time maximizing the\nvalue of actions taken by the behavioral policy during data\ngeneration. This results in a conservative Q-function.\nTD3+BC. Twin Delayed Deep Deterministic (TD3) policy\ngradient with Behavior Cloning (BC) is a model-free algo-\nrithm that does not explicitly learn a model of the behavioral\npolicy, while trains a policy to mimic the behavior policy\nfrom the data [15]. It directly penalizes Euclidean distance\nto the actions that were recorded in the dataset.\nC. Data Valuation\nThe training samples that machine learning (ML) mod-\nels are trained with are not all equally valuable [16]. A\nsample can be considered low-quality due to noisy input\nvalues, a noisy label, or the source-target domain mismatch\nproblem. Removing low-quality samples has been shown\nto increase the performance of ML models [7], [8]. The\ntask of quantifying the quality of individual datum to the\noverall performance is referred to as data valuation. In\nsupervised learning, data valuation is formally deﬁned as\nfollows. Given a source (training) dataset Ds = {(xi, yi)}N\ni=1\nand a target (test) dataset DT\n=\n\b\u0000xT\nj , yT\nj\n\u0001\tM\nj=1 where\nx ∈X is a d-dimensional feature vector, and y ∈Y is\na corresponding label, the goal is to ﬁnd a subset D∗=\n{(xk, yk) | (xk, yk) ∈DS}K\nk=1 of the source dataset DS that\nmaximizes the performance of the trained model on the target\ndataset DT [7], [17].\nIII. PROBLEM DESCRIPTION\nIn this section, we formally deﬁne the data valuation\nproblem for ofﬂine RL.\nWe\nassume\nthe\navailability\nof\na\nsource\ndataset\nDS = {(xS\ni , uS\ni , x′\ni\nS, rS\ni , eS\ni )}N\ni=1 ∼PS and a target dataset\nDT = {(xT\ni , uT\ni , x′\ni\nT , rT\ni , eT\ni )}M\ni=1 ∼PT , where x ∈Rm is\na state; u ∈Rn is the action that the agent performs at\nthe state x; r ∈R is the reward that the agent gets by\nperforming the action u in the state x; x′ ∈Rm is the state\nthat the agent transitions to (i.e. next state); and e ∈{0, 1}\nindicates whether the x′ is a terminal state. We assume that\nthe target dataset DT is much smaller than the the source\ndataset DS, therefore N ≫M. Furthermore, the source\ndistribution PS can be different from the target distribution\nPT , confronting our learner with the source-target domain\nmismatch problem. As in supervised learning, our goal is\nto ﬁnd a (sub)set D∗⊆DS, and seek a batch-constrained\npolicy π, that when trained on D∗can generalize to the target\ndomain used to construct DT . Therefore, we have a transfer\nlearning problem.\nTo formally deﬁne transfer learning for ofﬂine RL, we\ndraw on the formulation from Zhu et al. [18]. Let ΨS =\n{ΨS | ΨS ∈ΨS} be a set of source domains and ΨT be\na target domain, where each domain corresponds to an\nMDP. Therefore, the MDPs in the source domain ΨS and\ntarget domain ΨT are deﬁned as (XS, US, pS, rS, γS) and\n(XT , UT , pT , rT , γT ), respectively. We assume prior knowl-\nedge DS provided by the set of source domains ΨS and\naccessible to the target domain ΨT .\nBy leveraging the prior information DS from the source\ndomain ΨS as well as information DT provided by ΨT ,\ntransfer learning aims to learn an optimal policy π∗for the\ntarget domain ΨT , such that:\nπ∗= arg max\nπ\nEx∼XT ,u∼π\n\u0002\nQπ\nΨT (x, u)\n\u0003\n,\n(2)\nwhere π = φ (DS ∼ΨS, DT ∼ΨT ) : XT\n→UT is a\nfunction mapping the states to actions for the target domain\nΨT , learned based on information from both DT and DS.\nThe source and target domains can have distinct state\nspaces, but their action spaces have to be the same and their\ntransition function and reward function have to be similar as\nthey share internal dynamics. We focus on policy transfers\nwhere XS = XT , US = UT , rS = rT , but pS ̸= pT .\nThe source and target domains can have distinct transition\nfunctions because a change in environment parameters (e. g.,\nmass of torso and friction) results in a different probability\nfunction p, which itself is conditioned on (x, u, x′), where\nx, u, and x′ denote state, action, and next state, respectively.\nHowever, since the target dataset is very small, compared\nto the source dataset (N\n≫M), state-action transition\n(x, u, x′) is too restrictive. Thus, we also consider the case\n(x) in which only the change in the distribution of the state\nspace is taken into account.\nIV. MOTIVATING EXAMPLE\nFor our motivating example, we consider two scenarios\ninvolving a cobot, depicted in Figure 1. In the source domain\nthe cobot is performing pick-and-place task while the target\ndomain confronts the cobot with a sorting task. Clearly,\na learning agent trained on the source task will perform\npoorly on the target task. However, our hypothesis is that\ndata valuation can help us identify samples that are relevant\nfor both tasks. For instance, both tasks have pick-up and\nplace actions in common. Therefore, the goal is to ﬁnd the\nrelevant subset of the source dataset that allows the agent to\nlearn a policy that generalizes to the target domain.\nV. RELATED WORK\nThe RL literature contains numerous techniques for deal-\ning with the source-target domain mismatch problem. Note-\nworthy contributions here include: EPOPT [19], which is\na combination of policy transfer through source domain\nensemble and learning from limited demonstrations for the\nfast adaptation to the target domain; UP-OSI [20] trains\nrobust agent policies using a large number of synthetic\ndemonstrations from a simulator to deal with environments\nwith unknown dynamics; CAD2RL [21] learns latent repre-\nsentations from observations in the source domain that are\ngenerally applicable to the target domain; DARLA [22], a\nzero-shot transfer learning method that learns disentangled\nrepresentations which are robust against domain shifts; and\nSAFER [23], which accelerates policy learning on complex\ncontrol tasks by considering safety constraints.\nMeanwhile, the literature on off-policy RL includes prin-\ncipled experience replay memory sampling techniques. Pri-\noritized Experience Replay (PER) [24] (e.g., [25], [26],\n[27]) attempts to sample transitions that contribute the most\ntoward learning. prioritized replay with weighted importance\nsampling can improve BCQ. However, the majority of the\nwork to date on ofﬂine RL is focused on preventing the\ntraining policy from being too disjoint with the behavioral\npolicy [3], [6], [28]. To increase the generalization capacity\nof ofﬂine RL methods, Kostrikov et al. [29] propose in-\nsample Q-learning (IQL), which approximates the policy\nimprovement step by considering the state value function as\na random variable with some randomness determined by the\naction, and then taking a state-conditional expectile of this\nrandom variable to estimate the value of the best actions in\nthat state.\nIn contrast to the ofﬂine RL methods listed above, our\nwork focuses on valuating the suitability of state transition\ntuples for a given target domain. Here we draw inspiration\nfrom the literature on data valuation for supervised learning.\nGhorbani et al. in [30] propose the distributional Shapley,\nwhich is a framework in which the value of a point is\ndeﬁned in the context of underlying data distribution. The\nreformulation of the data Shapley value as a distributional\nquantity reduces the dependence on the speciﬁc draw of data\nas the valuation function does not depend on a ﬁxed data\nset. Ghorbani and Zou propose the Neuron Shapley frame-\nwork [31] to quantify how individual neurons contribute to\nthe prediction and performance of a deep neural network\n(DNN). However, the limitation of Shapely based methods\nis that it’s computationally expensive or even impossible to\nquantify the contribution of each individual sample to the\noverall performance of the model, in particular, for complex\nmodels such as DNNs [8].\nWang et al. [32] propose a minimax game-based transfer\nlearning technique for selective transfer learning that con-\nsists of a selector, a discriminator, and a transfer learning\nmodule, playing a minimax game to ﬁnd useful source data.\nYoon et al. [8] introduce a framework for data valuation in\nsupervised learning tasks, making use of RL to determine\nhow likely each training sample is used in the training of\nthe predictor model. This method integrates data valuation\ninto the training procedure of the predictor model, making\nthe predictor and data value estimator able to improve each\nother’s performance.\nDespite the success of the existing works on data valua-\ntion, they are only applicable to the supervised learning tasks\nin which the availability of labels is assumed. Therefore, they\nare not directly applicable to the RL setting where there is no\nground-truth for the transitions. This emphasizes a need for\na data valuation method that is applicable to the RL setting.\nVI. PROPOSED METHOD\nDVORL consists of two learnable functions: (1) a data\nvalue estimator (DVE) model vφ and (2) an ofﬂine reinforce-\nment learning model. Inspired by DVRL [8], we adapt the\nREINFORCE algorithm [33] and use it as the DVE. We use\na DNN for the DVE. The goal is to ﬁnd the parameters φ∗of\nthe DNN so that the network returns the optimal probability\ndistribution over the sample space.\nThe DVE model vφ : X × U × X ′ × R × E →[0, 1]\nis optimized to output data values corresponding to the\nrelevance of training samples to the target task. We formulate\nthe corresponding optimization problem as:\nmaxφ J (πφ) = E (xS,uS,x\n′S)∼P S\n(xT ,uT ,x\n′T )∼P T\nh\nrφ((xS, uS, x\n′S), (xT , uT , x\n′T ))\ni\n,\n(3)\nwhere\nrφ =\n1\nDKL(P(xS,uS,x′S) ∥P(xT ,uT ,x′T ))\n(4)\ncorresponds to the reciprocal of the KL divergence be-\ntween the batch of source dataset and target dataset.\nTherefore, the objective of the network is to assign\nhigh probabilities to samples whose reward function value\nrφ((xS, uS, x\n′S), (xT , uT , x\n′T )) is high.\nTraining. As shown in Algorithm 1 (lines 4 to 10), all the\nsamples of source buffer DS are divided into batches and\neach batch DS\n′ is given as input to the DVE (with shared\nparameters across the batch). The KL divergence between\nthe distribution of the state-action transition (x, u, x\n′) of the\ngiven batch and that of the small target buffer DT is calcu-\nlated and used as the reward signal rsig for training the DVE.\nLet w = vφ(xS\ni , uS\ni , x′\ni\nS, rS\ni , eS\ni ) denote the probability that\nthe sample i of the source buffer is used for training the\nofﬂine reinforcement learning model.\nOur adapted version of REINFORCE algorithm, has the\nfollowing object function for the policy πφ:\nJ (πφ) = E (xS,uS,x\n′S)∼P S\n(xT ,uT ,x\n′T )∼P T\nw∼πφ(DS\n′,·)\nh\nrφ((xS, uS, x\n′S), (xT , uT , x\n′T ))\ni\n=\nZ\nP T \u0010\n(xS, uS, x\n′S)\n\u0011\nX\nw∈[0,1]N\nπφ(DS\n′, w)\n·\nh\nrφ((xS, uS, x\n′S), (xT , uT , x\n′T ))\ni\nd\n\u0010\n(xS, uS, x\n′S)\n\u0011\n.\nIn the above equation, πφ(DS\n′, w) is the probability that\nthe selection probability vector w occurs. In this way, the\npolicy directly uses the values output by the DVE. This is\ndifferent from the DVRL [8], which uses a binary selection\nvector s = (s1, . . . , sBs) where sBs denotes the batch size,\nsi ∈{0, 1}, and P (si = 1) = wi. Thus, in our training,\nthe DVE has no control over exploration and just provides\nweightings for the given samples and is tuned accordingly.\nIt should be noted that we use the whole input information\nof the source buffer (i.e., (x, u, x′, r, e)) for calculating\nthe data values (i.e., DS\n′ in policy πφ(DS\n′, w)); however,\nwe only use the information of the state-action transition\n(x, u, x′) for calculating the reward signal, used for updating\nthe DVE, that is consistent with our formulation of transfer\nlearning where pS ̸= pT as the transition probability function\nis conditioned on the state-action transition (x, u, x′).\nWe calculate the gradient of the above objective function\nin the following.\n∇φJ (πφ) =\nZ\nP T \u0010\n(xS, uS, x\n′S)\n\u0011\nX\nw∈[0,1]N\n∇φπφ(DS\n′, w)\n·\nh\nrφ((xS, uS, x\n′S), (xT , uT , x\n′T )))\ni\nd\n\u0010\n(xS, uS, x\n′S)\n\u0011\n=\nZ\nP T \u0010\n(xS, uS, x\n′S)\n\u0011\n\n\nX\nx∈[0,1]N\n∇φπφ(DS\n′, w)\nπφ(DS\n′, w)\n· πφ(DS\n′, w)\nh\nrφ((xS, uS, x\n′S), (xT , uT , x\n′T ))\ni\n\nd\n\u0010\n(xS, uS, x\n′S)\n\u0011\n=\nZ\nP T \u0010\n(xS, uS, x\n′S)\n\u0011\n\n\nX\nw∈[0,1]N\n∇φ log\n\u0000πφ(DS\n′, w)\n\u0001\n· πφ(DS\n′, w)\nh\nrφ((xS, uS, x\n′S), (xT , uT , x\n′T ))\ni\n\nd\n\u0010\n(xS, uS, x\n′S)\n\u0011\n=\nE\n(xS,uS,x\n′S)∼P S\n(xT ,uT ,x\n′T )∼P T\nw∼πφ(DS,·)\nh\nrφ((xS, uS, x\n′S), (xT , uT , x\n′T ))\ni\n· ∇φ log\n\u0000πφ(DS\n′, w)\n\u0001\n.\nTo enhance the stability of the DVE, we use the moving\naverage rrolling of the previous signal rewards with the\nwindow size ω as the baseline. The baseline reduces the\nvariance of the gradient estimates [34].\nInference. As shown in Algorithm 1 (lines 11 to 18), after\nall the samples of the source buffer are used for training\nthe DVE, the fully-trained DVE is used for outputting the\ndata values of the original source buffer. The samples whose\ncorresponding data values are lower than the selection thresh-\nold ϵ are ﬁltered out and the remaining subset of samples\nform the new source buffer D∗\nS that is relevant to the target\ndomain:\nD∗\nS =\nn\u0010\nxS\ni , uS\ni , x′\ni\nS, rS\ni , eS\ni\n\u0011\n∈DS | i = 1, . . . , N; wi ≥ϵ\no\n.\n(5)\nFinally, the buffer D∗\nS is given to an ofﬂine RL model for\ntraining.\nAlgorithm 1 Data Valuation for Ofﬂine RL\n1: Input: Fixed source and target buffers DS and DT ; mini-\nbatch size Bs > 0; learning rate α; moving average\nwindow size ω; selection threshold ϵ.\n2: Output: A subset of source buffer D∗\nS which is relevant\nto target domain.\n3: Initialize: D∗\nS ←∅; DVE parameters φ; rφ ←0.\n4: for batch Bj in DS do\n5:\nwj = vφ(Bj) = vφ(xj, uj, x′\nj, rj, ej)\n6:\nrφ ←\n1\nDKL(P(xS ,uS ,s′S ) ∥P(xT ,uT ,s′T ))\n7:\nrsig = rφ −rrolling\n8:\nφ ←φ −α [rsig] ∇φ log πφ (Bj, wj)\n9:\nrrolling ←ω−1\nω rrolling + 1\nωrφ\n10: end for\n11: for batch Bk in DS do\n12:\nwk = vφ(Bk) = vφ(xk, uk, x′\nk, rk, ek)\n13:\nfor sample i in Bk do\n14:\nif wi ≥ϵ then\n15:\nD∗\nS = D∗\nS ∪(xi, ui, x′\ni, ri, ei)\n16:\nend if\n17:\nend for\n18: end for\n19: return D∗\nS\nVII. EXPERIMENTS\nA. Baselines\nIn this work, we use the following ofﬂine RL methods\ndiscussed in Section II as baselines: (Vanilla) BCQ, CQL and\nTD3+BC. We also evaluated the performance of BEAR [5]\non our considered domains. However, as found by [6], we\nfound CQL and TD3+BC outperformed BEAR, and therefore\nfocused our evaluation on the above three methods and our\nDVORL.\nWe use the (Vanilla) BCQ as the base model in our\nDVORL, and we refer to it as Data Valuation based BCQ\n(DVBCQ). The reason for using the Vanilla BCQ is that it is\nthe most commonly used ofﬂine RL algorithm, and we also\nintend to show how the selection of relevant transitions can\nhelp a base model, underperforming other methods in most\ncases, outperform the state-of-the-art methods in terms of\ntransferability of learned policy to different target conﬁgura-\ntions. We consider two versions of DVBCQ: i) DVBCQ (x)\nusing information of states, and ii) DVBCQ (x, u, x\n′) using\ninformation of state-action transition, for calculating the\nsimilarity between source and target buffers.\nB. Gathering samples for the replay buffer\nThe DVORL agent learns from a dataset collected by a do-\nmain expert. In our experiments, for each domain and domain\nparametrization, we trained a DDPG agent for one million\niterations and used the fully-trained agent for generating the\nbuffers with the size one million and ten thousand for the\nsource and target, respectively.\nC. Domains\nIn this work, we use the following two MuJoCo domains:\n• Hopper-v3: The Hopper is a simulated monopod robot\nwith 4 body links including the torso, upper leg, lower\nleg, and foot, together with 3 actuated joints. This\ndomain has an 11-dimensional state space including\njoint angles and joint velocities and a 3-dimensional\naction space corresponding to torques at the joints. The\ngoal is to make the hopper hop forward as fast as\npossible.\n• Walker2d-v3: The Walker is a simulated bipedal robot\nconsisting of 7 body links including to two legs and\na torso, along with 6 actuated joints. This domain\nhas a 17-dimensional state space including joint angles\nand joint velocities and a 4-dimensional action space\ncorresponding to joint torques. The goal is to make the\nrobot walk forward as fast as possible.\nTABLE I\nDOMAIN CONFIGURATIONS.\nDomain\nSource Conﬁg\nTarget Conﬁg\nName\nHopper-v3\nF: 2.0\nT: 0.05\nF: 2.0\nT: 0.05\nHopper-Source\nF: 2.5\nT: 0.075\nHopper-Target1\nF: 3.0\nT: 0.075\nHopper-Target2\nWalker2d-v3\nF: 0.9\nT: 0.05\nF: 0.9\nT: 0.05\nWalker2d-Source\nF: 1.125\nT: 0.075\nWalker2d-Target1\nF: 1.35\nT: 0.075\nWalker2d-Target2\nD. Source and target domain settings\nFor our experiments, we shall distinguish between source\nand target domains. The source domain is the one within\nwhich the samples are gathered by a fully-trained DDPG.\nThe target domain is the domain within which the DVORL\nagent is to be deployed. To study the extent to which DVBCQ\ncan cope with modiﬁed domain conﬁgurations, we consider\ntwo scenarios with respect to the source and target domains:\n1) Identical Source and Target Domains: Domain con-\nﬁguration for gathering samples and training DVORL\nagent remain the same. This is the simplest setting\nwhere the DDPG agent gathers samples in an envi-\nronment with a domain parameterization identical to\nthe domain within which the DVORL agent will be\ndeployed. For this setting, we consider two datasets\n“Hopper-Source” and “Walker-Source”.\n2) Transfer Learning: Samples are gathered from a\nsource domain with a parameterization that differs from\nPick-and-Place Task\nSorting Task\nData\nValue\nEstimator\nOﬄine RL\nAgent\nBatches\nSource Replay Buﬀer\nTarget Replay Buﬀer\nFiltering\ndata\nvalues\nreward\nMeasuring the diﬀerence \nbetween                    of \neach batch and target dataset\nusing KL divergence\nFig. 1.\nIllustration of the DVORL framework. (1) A batch of source buffer samples is given as input to the data value estimator (DVE). (2) KL Divergence\nbetween the distribution of the state-action transitions of the given batch and the distribution of the state-action transitions of target buffer (whose transition\nitems are collected by a domain expert on the target domain) is calculated and (3) used as the reward signal for updating the DVE. (4) After all the samples\nof the source buffer are used for training the DVE, (5) the fully trained DVE is used for outputting the data values of the source buffer (6) that are ﬁltered\nout by removing those values being lower than the selection threshold. (7) This results in a subset of the source buffer that is relevant to the target domain\nand is used for training the given base ofﬂine RL algorithm.\nthe target domain. More precisely, the target domain will\nhave different mass of torso and friction coefﬁcients\ncompared with the source domain. For this setting,\nwe consider four datasets “Hopper-Target1”, “Hopper-\nTarget2”, “Walker2d-Target1”, and “Walker2d-Target2”.\nAll the considered source and target domain conﬁgurations\nare presented in Table I.\nE. Parameter Tuning\nFor all the competitors, we used the default parameters\nvalues reported in the corresponding papers. Hyperparame-\nters of DVORL are selected by grid search. Since we used the\nBCQ in our DVORL method, we report the used parameter\nvalues of Data Valuation based BCQ (DVBCQ), listed in\nTable II. The parameters values of the baseline (Vanilla) BCQ\nare the same as those of the base agent in our DVBCQ.\nTABLE II\nPARAMETERS VALUES OF OUR DVBCQ MODEL.\nParameter\nValue\nDescription\ndve batch size\n200\nbatch size for DVE\ndve hidden layers\n[128, 128]\nNumber of nodes in hidden layers\nmoving average window size\n20\nWindow size for the moving average\nselection threshold\n0.1\nselection threshold\nmini batch size\n100\nBatch size for ofﬂine RL model\ndiscount\n0.9\nDiscount factor\ntau\n0.005\nTarget network update rate of BCQ\nlambda\n0.75\nWeighting for clipped double Q-learning\nphi\n0.05\nMax perturbation parameter for BCQ\nF. Implementation\nOur implementation of the DVORL builds on OpenAI\ngym’s [35] control environments with the MuJoCo [36]\nphysics simulator.\nVIII. RESULTS\nA. Evaluation of ofﬂine RL methods on source and target\ndomains\nFigure 2 shows the performance of BCQ, CQL, TD3+BC,\nDVBCQ (x) and DVBCQ (x, u, x\n′) on the source domain\nand two different target domain conﬁgurations described in\nSection VII-D. The models are trained for 1M iterations,\nand the results are averaged over ten runs on target domain\nenvironments with randomly-chosen seeds.\nIdentical source-domain: For DVBCQ and other baselines,\nwe report the average return achieved by the best policy with\nrespect to checkpoints saved throughout the run. For the iden-\ntical source-domain setting, both DVBCQ (x) and DVBCQ\n(x, u, x\n′) signiﬁcantly outperform all baselines on Hopper\nenvironment, and their performance is superior to BCQ\nand CQL, while underperforming TD3+BC on Walker2d\nenvironment.\nTransfer learning: For transfer learning setting, DVBCQ\n(x) outperforms both target domains whose conﬁgurations\n(mass of torso and friction) differ from those of the source\ndomain, on both Hopper and Walker2d environments. How-\never, DVBCQ (x, u, x\n′) underperforms CQL on Hopper\nenvironment and TD3+BC on Walker environment but it has\ncompetitive performance compared with other baselines.\nIt should be noted that the there is a signiﬁcant difference\nbetween the performance of DVBCQ and its base model\nBCQ.\nB. Removing high/low value transitions from source dataset\nFigure 3 shows the performance of BCQ models trained\non the source dataset with different selection thresholds\nand evaluated on a different target domain conﬁguration\n(Walker2d-Target2), where all the models are trained for\n200K iterations, and a ﬁxed seed is used for the evaluation\nenvironment. We consider ﬁve thresholds (0.0, 0.1, ..., 0.4)\nfor excluding the high/low-value data samples of the source\ndataset. In addition, we report the average return of the best\npolicy (with respect to checkpoints saved throughout the run)\nlearned for each point.\nAs shown in Figure 3, removing low-value samples from\nthe source dataset can help the RL agent learn only those\ntransitions relevant to the target domain conﬁguration and\ntherefore achieve better performance on the target domain\nFig. 2.\nPerformance of BCQ, CQL, TD3+BC, and DVBCQ on the source domain and two different target domain conﬁgurations (described in Section VII-\nD), where the models are trained for 1M iterations, and the results are averaged over ten runs on target domain environments with randomly-chosen seeds.\nFor DVBCQ and other baselines, we report the performance achieved by the best policy with respect to checkpoints saved throughout the run.\n(green line). On the other hand, removing high-value samples\nfrom the source dataset signiﬁcantly deteriorates the RL\nagent’s performance (red line).\nThe ﬁndings in Figure 3 support the opening hypothesis\nthat excluding high-value samples worsens the performance\nof the ofﬂine RL methods.\nFig. 3.\nPerformance of BCQ models trained on the source dataset with\ndifferent selection thresholds and evaluated on a different target domain\nconﬁguration (Walker2d-Target2), where all the models are trained for\n200.000 iterations, and a ﬁxed seed is used for the evaluation environment.\nExcluding high-value samples (red line) aggravates the performance of the\nofﬂine RL methods. However, excluding low-value samples (green line) does\nnot deteriorate the performance as much as that of the high-value samples.\nIX. DISCUSSION & FUTURE WORK\nOur results suggest that DVORL can improve the ofﬂine\nreinforcement learning methods on both identical source-\ntarget and transfer learning settings. In addition, our method\nhelps the ofﬂine RL methods achieve signiﬁcantly higher per-\nformance with fewer iterations, making them more efﬁcient.\nFurthermore, our method can identify the relevant samples of\nthe source domain to different target domain conﬁgurations.\nThis is of high importance and has many use cases, such as\nlearning from an externally acquired dataset and safe RL.\nIt should be noted that our goal is not to show that our\nproposed method outperforms all the state-of-the-art ofﬂine\nRL methods on both source and target domains, but to show\nthat the data valuation for the ofﬂine reinforcement learning\n(DVORL) framework can improve the performance of the\nbaseline algorithms.\nFor future work, we aim to examine whether the size of\ntarget buffer plays a role in the performance of DVORL. We\nintend conduct some experiments on real-world domains and\ncompare our results to other data valuation methods like Data\nShapely. Moreover, we plan to improve our reward function\nby taking into account dynamics of the model.\nWe also aim to investigate the extent to which DVORL\ncan identify the safe transitions within a safe reinforcement\nlearning setting. We also plan to apply the idea of transition\nvaluation to the safe multi-agent reinforcement learning [37],\nwhere different data value estimators are optimized for the\ncorresponding agent with respect to the tasks that they need\nto perform. In addition, we aim to incorporate a mechanism\nfor auto-tuning the selection threshold into the training as the\noptimal value for this parameter may vary from one domain\nconﬁguration to another one.\nX. CONCLUSION\nIn this work, we proposed a data valuation framework that\nselects a subset of samples in the source dataset that are\nrelevant to the target task. The data values are estimated using\na deep neural network, trained using reinforcement learning\nwith a reward that corresponds to the similarity between the\ndistribution of the state-action transition of the given data\nand the target dataset. We show that DVORL outperforms\nbaselines on different target domain conﬁgurations and has\na competitive performance on the source domain in which\nthe reinforcement learning agent is trained. We ﬁnd that\nour method can identify relevant and high-quality transitions\nand improve the performance and transferability of policy\nlearned by ofﬂine RL algorithms. Moreover, we contributed\na benchmark on two Gym MuJoCo domains (Hopper and\nWalker2d) for which domain conﬁgurations (friction and\nmass of torso) for the target domain differ from those of\nthe source domain.\nACKNOWLEDGMENT\nThe authors gratefully acknowledge, that the proposed\nresearch is a result of the research project “IIP-Ecosphere”,\ngranted by the German Federal Ministry for Economics and\nClimate Action (BMWK) via funding code 01MK20006A.\nREFERENCES\n[1] S. Lange, T. Gabel, and M. Riedmiller, “Batch reinforcement learning,”\nin Reinforcement learning.\nSpringer, 2012, pp. 45–73.\n[2] D. Isele, A. Nakhaei, and K. Fujimura, “Safe reinforcement learning\non autonomous vehicles,” in 2018 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS).\nIEEE, 2018, pp. 1–6.\n[3] S. Fujimoto, D. Meger, and D. Precup, “Off-policy deep reinforcement\nlearning without exploration,” in Proceedings of the 36th International\nConference on Machine Learning, ser. Proceedings of Machine\nLearning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97.\nPMLR,\n09–15\nJun\n2019,\npp.\n2052–2062.\n[Online].\nAvailable:\nhttp://proceedings.mlr.press/v97/fujimoto19a.html\n[4] C. Breazeal and A. L. Thomaz, “Learning from human teachers with\nsocially guided exploration,” in 2008 IEEE International Conference\non Robotics and Automation.\nIEEE, 2008, pp. 3539–3544.\n[5] A. Kumar, J. Fu, G. Tucker, and S. Levine, “Stabilizing off-\npolicy q-learning via bootstrapping error reduction,” arXiv preprint\narXiv:1906.00949, 2019.\n[6] A. Kumar, A. Zhou, G. Tucker, and S. Levine, “Conservative q-\nlearning for ofﬂine reinforcement learning,” Advances in Neural In-\nformation Processing Systems, vol. 33, pp. 1179–1191, 2020.\n[7] A. Ghorbani and J. Zou, “Data shapley: Equitable valuation of data for\nmachine learning,” in International Conference on Machine Learning.\nPMLR, 2019, pp. 2242–2251.\n[8] J. Yoon, S. Arik, and T. Pﬁster, “Data valuation using reinforcement\nlearning,” in International Conference on Machine Learning. PMLR,\n2020, pp. 10 842–10 851.\n[9] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforce-\nment learning,” arXiv preprint arXiv:1509.02971, 2015.\n[10] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Ried-\nmiller, “Deterministic policy gradient algorithms,” in International\nconference on machine learning.\nPMLR, 2014, pp. 387–395.\n[11] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,”\nnature, vol. 518, no. 7540, pp. 529–533, 2015.\n[12] S. Fujimoto, E. Conti, M. Ghavamzadeh, and J. Pineau, “Bench-\nmarking batch deep reinforcement learning algorithms,” arXiv preprint\narXiv:1910.01708, 2019.\n[13] R. S. Sutton, “Learning to predict by the methods of temporal\ndifferences,” Machine learning, vol. 3, no. 1, pp. 9–44, 1988.\n[14] S. Thrun and A. Schwartz, “Issues in using function approximation for\nreinforcement learning,” in Proceedings of the Fourth Connectionist\nModels Summer School.\nHillsdale, NJ, 1993, pp. 255–263.\n[15] S. Fujimoto and S. S. Gu, “A minimalist approach to ofﬂine reinforce-\nment learning,” Advances in Neural Information Processing Systems,\nvol. 34, 2021.\n[16] A. Lapedriza, H. Pirsiavash, Z. Bylinskii, and A. Torralba, “Are all\ntraining examples equally valuable?” arXiv preprint arXiv:1311.6510,\n2013.\n[17] S. Durga, R. Iyer, G. Ramakrishnan, and A. De, “Training data\nsubset selection for regression with controlled generalization error,”\nin International Conference on Machine Learning.\nPMLR, 2021, pp.\n9202–9212.\n[18] Z. Zhu, K. Lin, and J. Zhou, “Transfer learning in deep reinforcement\nlearning: A survey,” arXiv preprint arXiv:2009.07888, 2020.\n[19] A. Rajeswaran, S. Ghotra, B. Ravindran, and S. Levine, “Epopt:\nLearning robust neural network policies using model ensembles,”\narXiv preprint arXiv:1610.01283, 2016.\n[20] W. Yu, J. Tan, C. K. Liu, and G. Turk, “Preparing for the unknown:\nLearning a universal policy with online system identiﬁcation,” arXiv\npreprint arXiv:1702.02453, 2017.\n[21] F. Sadeghi and S. Levine, “CAD2RL: Real single-image ﬂight without\na single real image,” arXiv preprint arXiv:1611.04201, 2016.\n[22] I. Higgins, A. Pal, A. Rusu, L. Matthey, C. Burgess, A. Pritzel,\nM. Botvinick, C. Blundell, and A. Lerchner, “Darla: Improving zero-\nshot transfer in reinforcement learning,” in International Conference\non Machine Learning.\nPMLR, 2017, pp. 1480–1490.\n[23] D. Slack, Y. Chow, B. Dai, and N. Wichers, “Safer: Data-efﬁcient\nand safe reinforcement learning via skill acquisition,” arXiv preprint\narXiv:2202.04849, 2022.\n[24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experi-\nence replay,” arXiv preprint arXiv:1511.05952, 2015.\n[25] Y. Hou, L. Liu, Q. Wei, X. Xu, and C. Chen, “A novel ddpg\nmethod with prioritized experience replay,” in 2017 IEEE international\nconference on systems, man, and cybernetics (SMC).\nIEEE, 2017,\npp. 316–321.\n[26] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. van\nHasselt, and D. Silver, “Distributed prioritized experience replay,” in\nInternational Conference on Learning Representations, 2018.\n[27] C. Kang, C. Rong, W. Ren, F. Huo, and P. Liu, “Deep deterministic\npolicy gradient based on double network prioritized experience replay,”\nIEEE Access, vol. 9, pp. 60 296–60 308, 2021.\n[28] R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims, “Morel:\nModel-based ofﬂine reinforcement learning,” Advances in neural in-\nformation processing systems, vol. 33, pp. 21 810–21 823, 2020.\n[29] I. Kostrikov, A. Nair, and S. Levine, “Ofﬂine reinforcement learning\nwith implicit q-learning,” in International Conference on Learning\nRepresentations, 2022.\n[30] A. Ghorbani, M. Kim, and J. Zou, “A distributional framework for\ndata valuation,” in International Conference on Machine Learning.\nPMLR, 2020, pp. 3535–3544.\n[31] A. Ghorbani and J. Y. Zou, “Neuron shapley: Discovering the respon-\nsible neurons,” Advances in Neural Information Processing Systems,\nvol. 33, pp. 5922–5932, 2020.\n[32] B. Wang, M. Qiu, X. Wang, Y. Li, Y. Gong, X. Zeng, J. Huang,\nB. Zheng, D. Cai, and J. Zhou, “A minimax game for instance based\nselective transfer learning,” in Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining,\n2019, pp. 34–43.\n[33] R. J. Williams, “Simple statistical gradient-following algorithms for\nconnectionist reinforcement learning,” Machine learning, vol. 8, no. 3,\npp. 229–256, 1992.\n[34] E. Greensmith, P. L. Bartlett, and J. Baxter, “Variance reduction\ntechniques for gradient estimates in reinforcement learning.” Journal\nof Machine Learning Research, vol. 5, no. 9, 2004.\n[35] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schul-\nman, J. Tang, and W. Zaremba, “Openai gym,” arXiv preprint\narXiv:1606.01540, 2016.\n[36] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for\nmodel-based control,” in 2012 IEEE/RSJ international conference on\nintelligent robots and systems.\nIEEE, 2012, pp. 5026–5033.\n[37] I. ElSayed-Aly, S. Bharadwaj, C. Amato, R. Ehlers, U. Topcu, and\nL. Feng, “Safe multi-agent reinforcement learning via shielding,” in\nProceedings of the 20th International Conference on Autonomous\nAgents and MultiAgent Systems, 2021, pp. 483–491.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-05-19",
  "updated": "2022-05-19"
}