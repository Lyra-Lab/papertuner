{
  "id": "http://arxiv.org/abs/2205.11509v1",
  "title": "Information Propagation by Composited Labels in Natural Language Processing",
  "authors": [
    "Takeshi Inagaki"
  ],
  "abstract": "In natural language processing (NLP), labeling on regions of text, such as\nwords, sentences and paragraphs, is a basic task. In this paper, label is\ndefined as map between mention of entity in a region on text and context of\nentity in a broader region on text containing the mention. This definition\nnaturally introduces linkage of entities induced from inclusion relation of\nregions, and connected entities form a graph representing information flow\ndefined by map. It also enables calculation of information loss through map\nusing entropy, and entropy lost is regarded as distance between two entities\nover a path on graph.",
  "text": "Information Propagation by Composited Labels in\nNatural Language Processing\nTakeshi Inagaki\nIBM Japan, Tokyo\nAbstract—In natural language processing (NLP), labeling on\nregions of text, such as words, sentences and paragraphs, is a\nbasic task. In this paper, label is deﬁned as map between mention\nof entity in a region on text and context of entity in a broader\nregion on text containing the mention. This deﬁnition naturally\nintroduces linkage of entities induced from inclusion relation\nof regions, and connected entities form a graph representing\ninformation ﬂow deﬁned by map. It also enables calculation of\ninformation loss through map using entropy, and entropy lost is\nregarded as distance between two entities over a path on graph.\nIndex Terms—NLP; entity extraction; classiﬁcation problem;\nknowledge graph;\nI. INTRODUCTION\nNLP is one of active area machine learning is applied and\nmuch progress made in recent years. One goal of NLP is\nextracting knowledge from text documents and represents it in\nformat with explicit structure processable by computers. As a\ndata format of knowledge, RDF and OWL [1] on top of it is an\nexample. It is an attempt to represent knowledge in a universal\nschema, and there are implementations in service such as\nDBPedia [3] [4]. Knowledge represented in graph structure\nwith RDF schema can be retrieved by a graph query language,\nSPARQL [2]. More generally, this kind of approaches are\ncalled Knowledge Graph [5]. Representation of data in min-\nimal pieces (triples) ensures ﬂexibility of reconstructing data\nin a form user needs at query time. However, its data schema,\nsuch as predicates, is to be predeﬁned before processing data,\nand forming of a graph is relying on matching of subjects and\nobjects as keys to join triples. Informational source for forming\na graph is still an open question, and NLP technologies are\nexpected to extract that information from descriptions in text\ndocuments. In this paper, we focus on geometric information\nin text document as a source of graph structure.\nOutput data from NLP is simply labels on text. A label\nis a name representing category of things described by text.\nA basic task in NLP is to recognize patterns in sequence of\nwords, and classify them in categories according to meaning\nthey imply. Labeling of small region of text which represents\nexisting object is called entity extraction. This task determines\nlabel of region and boundary of region at same time. A task\nlabeling broader region on text is called as classiﬁcation. Many\nstudies are on going to improve quality of labeling to replicate\nhuman’s understanding on text documents by recognizing\ncomplex patterns in a sequence of words.\nIn this paper, we study information carried by labels de-\nlivered by NLP. As mentioned above, labels are bound to\nregions on text, and structure of a document is designed to\nrepresent structure of knowledge. By that, if we have a method\nto capture document structure information effectively, it will\nhelp to reconstruct knowledge from NLP outputs. For this\npurpose, we employ notion of map, it immediately requires\ndeﬁnitions of source set, target set and direction of map.\nEquivalent class in source set is naturally comes in discussion,\nand it makes notion of information entropy clear for labeling\nby NLP models.\nII. DEFINITION OF LABELING\nIn NLP tasks, we consider labeling on various size of\nregions on text documents. For example, if a text contains\na word Bear, and if we know he is a dog, we label this\nregion of text with a label dog. If Buddy is also a dog, we\nlabel this appearance of the word with the label dog too. How\ncan we know they are dogs? The word Bear nor Buddy\nitself does not provide enough information to determine the\nlabel, instead, we need to read text surrounding those words to\nunderstand what they mean. When we use machine learning\nto detect expressions of names of dogs in text documents,\nit recognizes patterns in surrounding text. One example of\nmachine learning algorithm is BERT [6] [7] which recognizes\ncomplex patterns within ﬁxed size regions on text. We employ\nterminology of NLP, we refer regions on text such as Bear and\nBuddy as mentions, meaning of them implied by their context\nin text are called entities, dog is a name of entity label in this\nexample. For typical type of entities called named entities,\nsuch as person, organization and location, their mentions\nare small regions on text. However, if we consider entities\nwith more complex context such as regal statements, their\nmentions will be longer than person names, and to understand\ntheir implication, readers should refer broader regions on text\nincluding those mentions.\nThis situation can be summarized as below. On a document\ntext, there is a mention M which is a region A on text, its\ncontext is implied by broader region B on text which contains\nA. We can associate this region B to entity E of mention M,\nas\nA →M,\nB →E,\nA ⊂B.\n(1)\nTo associate mentions and entities each other, we formulate the\nrelation as a map. There are two possibilities, we can consider\narXiv:2205.11509v1  [cs.CL]  23 May 2022\na map from mention to entity, or a map from entity to mention.\nTo make the map well-deﬁned, for a given element of source\nset, an element of target set should be determined uniquely.\nFor mentions and entities, there are both of two cases, 1) a\nsingle mention is associated to multiple entities, or 2) multiple\nmentions are associated to a single entity. It means we need\nto consider both directions of maps.\nA map of mention M to entity E happens typically when\nwe consider an entity is a super-class of classes described by\nmentions. In this case, an entity represents common features\nof mentions, we deﬁne it as\nf : M →E.\n(2)\nFor a given mention, a super-class is to be uniquely determined\nby a map. Note that this does not prevent a class to have\nhas multiple super-classes, but they need to be expressed by\nmultiple maps with different labels. With a single label, an\nentity is associated to multiple mentions, so it can not be\ninterpreted as mapping from entity to mention, it should be\nformulated as a map from mention to entity, for example\ndog\nclass\n−−−→animal,\ncat\nclass\n−−−→animal,\n(3)\nwhere animal is the entity for mentions dog and cat.\nOn the other hand, mapping from entity to mention is\nregarded as description of property of a class, as\nf ′ : E →M.\n(4)\nIn this case, a single mention can be referred from multiple\nentities, for example, color of two bags can be described by\na single mention red, as\nbag1\ncolor\n−−−→red,\nbag2\ncolor\n−−−→red.\n(5)\nAn entity bag1 can be attributed by multiple maps to mentions,\nsuch as\nbag1\ncolor\n−−−→red,\nbag1\nsize\n−−→small,\nbag1\nshape\n−−−−→square.\n(6)\nNote that this direction of map deﬁnes direction of informa-\ntion propagation. When x is mapped to y by map f, x gains\ninformation of y. Information ﬂow is inverted direction of\nmap. This observation is validated in interpretation of mapping\nas super-class and property. Information of a super-class is\napplicable to its sub-classes, however, information of sub-\nclasses is not applicable for super-class. Similarly, information\nof property is applied to its classes. This behavior is consistent\nwith our identiﬁcation of labeling as map.\nHowever, distinction between super-class or property is\narbitrary. So, we will just refer direction of map only without\nmaking distinction between super-class or property. We denote\nmap from mention to entity, in other words, small region to\nlarge region containing it, forward. Denote map from entity\nto mention, from large region to small region, backward.\nDiscussion in this paper is symmetric on both forward and\nbackward direction map. We can apply observation made on\nmap of a direction to map of other direction.\nFig. 1.\nBlack circles represent mentions in text, and white circles\nare entities represented by mentions. On left hand size, multiple\nmentions are related to one entity, map can be deﬁned from mentions\nto entities. We call this mapping forward type. On right hand size,\nthere are multiple entities relating to a mention. The map should be\nfrom entities to mentions. We call this mapping backward type.\nThere are relations containing both direction of maps. For\nexample, we assume there are two types of bag, bag1 and\nbag2. The color of both bags is black. It is expressed in text\nby a mention black and a label color associates it to two\nentities of bag1 and bag2 from description in text. In other\nplace on text, it is described as bags of bag2 type are owned\nby a woman and a man, a label owning associates them to\nan entity bag2. From this graph, we can know color of a bag\na woman owning is black.\nFig. 2. A black circle at top of left hand side represents mention of\ncolor of two types of bags as black, two black circles below represent\nmentions of owners of the same type of bags. A white circle upper\nside represents type bag1 and a circle at lower represents bag2. A\nbag2 is either of source and target of map.\nWe observed there are two directions of map, and multiple\nelements of source are mapped in same element of target.\nThis causes loss of information. Actually, NLP is a task of\nclassiﬁcation. It categorizes elements into smaller number of\ngroups by ignoring details making distinction among them.\nThis information loss is a fundamental nature of NLP. We\nwill address measurement of lost information later.\nIII. COMPOSITE MAP AND NLP LABELS\nNLP task labels various size of regions on text, words, sen-\ntences, paragraphs and documents. Small regions are contained\nin a larger region, and an entity of larger region contains\ninformation from small regions in it. So, it is natural to\nconsider a chain of inclusion relations of regions.\nBy deﬁning label as map, we can convert this problem to\na problem of composite map of f and g. Consider two maps,\nf maps mention of region A to an entity E represented by\nregion B which contains region A. Region B is regard as a\nmention M ′, and it is mapped by g to an entity E′ represented\nby region C which contains B, as\nf : M →E,\ng : M ′ →E′,\n(7)\nwith inclusion relations\n(A ∼M) ⊂(B ∼E ∼M ′) ⊂(C ∼E′ ∼M ′′).\n(8)\nWe can naturally consider of composite map as\ng ◦f : M →E′.\n(9)\nTo simplify notation we identify mention M at step n to entity\nE at n −1\nMn ≃En−1.\n(10)\nWe obtain a sequence of mappings in more general form, as\nEn−1\nfn−1\n−−−→En\nfn\n−→En+1.\n(11)\nAn example of a sequence of forward mappings is a\ncategorization problem. A dog and a cat mentioned in text\nare categorized in mammal in a paragraph, and mammal\nand bird are categorized in animal in a section. This sequence\ncan be captured by composite maps, as\ndog →mammal →animal,\ncat →mammal →animal,\ncrow →bird →animal.\n(12)\nSimilarly, text can describe a sequence of backward map-\npings. Map color and size associate mentions in text red\nand small to an abstract entity red.small. In a broader\nsection, they are aggregated with shape which maps an entity\nred.small to an entity red.small.square, as\nred.small.square →red.small →red,\nred.small.square →red.small →small,\nred.small.square →square.\n(13)\nWe observed NLP label can be regarded as map between\nmention and entity. By considering regions containing men-\ntions and representing entities, we naturally need to consider\nsequences of mappings. These sequences form a graph by\nidentiﬁcation of source and target sets of mappings. This graph\nis regarded as representing ﬂow of information. In next section,\nwe will investigate propagation of information by map.\nIV. ENTROPY AND DISTANCE\nOur deﬁnition of label as map between mention and entity\nnaturally induces information of inclusion relation of regions\non text into sequence of map which is regarded as composite\nmap. We can say two entities in different sections in a\ndocument are connected, if an entity represented by a mention\nof large text region containing both sections has maps to two\nentities in each section. Length of a path connection two\nentities becomes long if two mentions of entities appear distant\nlocations in a document. It is expected this length indicates\ndistance of two entities in semantic space, it means how distant\ntwo entities are corresponding each other.\nTo make this observation more concrete, we consider in-\nformation loss by composite map corresponding to a path\nconnecting two entities. Mapping process drops some of\ninformation from source set. We start from a simplest case\nX\nf−→Y.\n(14)\nBy map f, two or more of elements in source set X is mapped\nto a same element in target set Y . We denote a set of equivalent\nelements as\n∼f= {a, b ∈X|f(a) = f(b)}.\n(15)\nWe can deﬁne a quotient set C as below\nCf = X/ ∼f .\n(16)\nWe introduce entropy S as cardinality of set C\nSf = ln |Cf|,\n∆Sf = ln |X| −ln |Cf|.\n(17)\nEntropy is a notion to convert number of states which needs\nmultiplicative operation to calculate number of states of a\ncombined system to additive operation. ∆Sf is information\nloss by the map f. It can be regarded as distance between\nX and Y . This distance or entropy loss caused by map\nis accumulated in composite map. We can restore number\nof states just by exponentiation of entropy which can be\ninterpreted as probability\np = e−∆Sf = |Cf|\n|X| ≤1.\n(18)\nAs a next step, we consider composite map\nX\nf−→Y\ng−→Z.\n(19)\nA source set of second map g is image of f denoted as Im f.\nIt is isomorphic to Cf just deﬁned above\nCf ≃Im f.\n(20)\nSame as the case of single map, we deﬁne a quotient set Cg\nand information loss by composite map g ◦f, as\nCg = Im f/ ∼g,\n∆Sg◦f = ln |X| −ln |Cg|.\n(21)\nNext, we consider different type of informational connec-\ntion, where entities are attributed by multiple properties. We\nsuppose we know value of a property, and we would like to\nknow value of other property associated on same entities with\nknowledge extracted from text by using NLP models. Labels\nare expected to support this reasoning. How can we capture\nrelations among mentions or entities from analysis of map?\nFig. 3.\nBlack circles represent mentions in text, and white circles\nare entities represented by mentions. We consider backward map\nrepresenting properties of entity. Two backward mappings f from\ncenter to left, g from center to right are considered. Source sets of\nentities of map do not need to coincide on both sides.\nIn this case, elements of a set of entities Z are mapped to\nmention X by f, and are mapped to Y by g, as\nX\nf←−Z\ng−→Y.\n(22)\nWe would like to know degree of intersection of both map\non a set Z. We need to work on quotient sets Cf and Cg to\ncount number of colliding elements. We regard two elements\nin quotient sets are intersected if elements in two equivalent\ngroups A ∈Cf and B ∈Cg are identical or completely\nincluded in one of them, A ⊆B. By denoting number of\nA ∈Cf intersecting with ∃B ∈Cg as |Cf−→\n∩Cg|, we can\ndeﬁne entropy from this intersection number, as\nCf = Z/ ∼f,\nCg = Z/ ∼g,\nSf −1◦g = ln |Cf−→\n∩Cg|,\n∆Sf −1◦g = ln |Cf| −ln |Cf−→\n∩Cg|.\n(23)\nwhere e−∆Sf−1◦g measures ratio of property f propagated to\nproperty g. If it is 1, it indicates all values of property g are\ndetermined by property f. This provides a systematic way to\nmeasure dependency of properties each other. In an edge case,\na set of products is attributed by size, color and price. In case\nprice is decided just by size, does not depend on color, we see\nthat intersection number between map color and map price is\nzero or Sf −1◦g = −∞. It indicates termination of information\nin this connection.\nTo validate this observation with a simple example, we\nconsider three sets of target mentions for three types of map,\nas\nIm fcolor = (red, black, blue),\nIm gsize = (small, large),\nIm hprice = (expensive, inexpensive).\n(24)\nAs a ﬁrst example case, we assume existence of rules con-\nstraining mappings, as\ng(x) = small ⇒h(x) = inexpensive,\ng(x) = large ⇒h(x) = expensive.\n(25)\nExistence of rules restrict possible combination of properties,\nand we can determine price from other properties size and\ncolor. Quotient sets by equivalent classes of three types of\nmap are\nCf = ({red.large, red.small},\n{back.large, black.small},\n{blue.large, blue.small}),\nCg = ({red.large, black.large, blue.large},\n{red.small, black.small, blue.small}),\nCh = ({red.large, black.large, blue.large},\n{red.small, black.small, blue.small}).\n(26)\nRules constrain quotient sets, and intersection numbers among\nthem are,\n|Cf| = 3, |Cf−→\n∩Ch| = 0,\n|Cg| = 2, |Cg−→\n∩Ch| = 2.\n(27)\nThis encodes information of rules, in other words, we can\nrestore rules by analyzing intersections on quotient sets with\nlabeled data delivered from text. In this particular case, it\nindicates no informational link between color and price. On\nthe other hand, entire information is propagated between size\nand price.\nWe consider another example governed by different rules,\nwhere red one is exceptionally expensive regardless of size\nnor color, as\ng(x) = small ∧f(x) ̸= red ⇒h(x) = inexpensive,\ng(x) = large ∨f(x) = red ⇒h(x) = expensive.\n(28)\nThis time, quotient set Ch is modiﬁed, as\nCh = ({red.small, red.large, black.large, blue.large},\n{black.small, blue.small})\n(29)\nWe obtain intersection number, as\n|Cf| = 3, |Cf−→\n∩Ch| = 1,\n|Cg| = 2, |Cg−→\n∩Ch| = 1.\n(30)\nIn this case, we cannot determine the price from neither of\ncolor nor size. To obtain price, we need to consider a set\nCf ⊗Cg of which each element is a combination of both\nelements. It consists of every combination of two properties\nsuch as red.large. With this, we obtain links from color⊗size\nto price, as\n|Cf ⊗Cg| = 6, |(Cf ⊗Cg)−→\n∩Ch| = 6.\n(31)\nFinally, we make a remark on entropy of each element in\nintersecting quotient sets. It has a form of Shannon entropy of\ninformation\nSentity =\n1\n|Cf−→\n∩Cg|\nln |Cf−→\n∩Cg|.\n(32)\nThis entropy is regarded as measuring amount of information\ncarried by a property value of f against a value of g. It can\nbe used for evaluation of a relevancy score of property g for\nf. In ﬁrst example of relation between size and price, size’s\nscore against price is\n1\n2 ln 2 = 0.35... In second example,\nrelation between color ⊗size and price, a combination of\ncolor and size’s score against price is\n1\n6 ln 6 = 0.30... A\nproperty price in ﬁrst case carries more information of size\nthan of a combination of color and size in second case.\nV. CONCLUSION\nIn this paper, we examined new framework to deﬁne NLP\nlabeling in mathematical language. Labels from a graph of\nwhich structure is induced from document structure, and it\nis possible to measure distance between two labeled entities\nalong on a path in the graph. Deductive reasoning by algorithm\nis one of expected application.\nREFERENCES\n[1] Deborah L. McGuinness, Frank van Harmelen ”OWL Web Ontol-\nogy Language Overview, W3C Recommendation 10 February 2004”\nhttps://www.w3.org/TR/owl-features/\n[2] The W3C SPARQL Working Group ”SPARQL 1.1 Overview, W3C\nRecommendation 21 March 2013” https://www.w3.org/TR/2013/REC-\nsparql11-overview-20130321/\n[3] DBPedia ofﬁcal site https://www.dbpedia.org/\n[4] Christian Bizer, Jens Lehmann, Georgi Kobilarov, Soren Auer, Christian\nBecker, Richard Cyganiak, Sebastian Hellmann ”DBpedia - A crystal-\nlization point for the Web of Data” Web Semantics: Science, Services\nand Agents on the World Wide Web. 7 (3): 154–165.\n[5] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d’Amato, Ger-\nard de Melo, Claudio Gutierrez, Jos´e Emilio Labra Gayo, Sabrina Kir-\nrane, Sebastian Neumaier, Axel Polleres, Roberto Navigli, Axel-Cyrille\nNgonga Ngomo, Sabbir M. Rashid, Anisa Rula, Lukas Schmelzeisen,\nJuan Sequeda, Steffen Staab, Antoine Zimmermann ”Knowledge\nGraphs” ACM Computing Surveys. 54 (4): 1–37. arXiv:2003.02320\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n”BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding” arXiv:1810.04805\n[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin ”Attention Is\nAll You Need” arXiv:1706.03762\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2022-05-23",
  "updated": "2022-05-23"
}