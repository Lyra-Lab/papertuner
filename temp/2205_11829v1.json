{
  "id": "http://arxiv.org/abs/2205.11829v1",
  "title": "Unsupervised Difference Learning for Noisy Rigid Image Alignment",
  "authors": [
    "Yu-Xuan Chen",
    "Dagan Feng",
    "Hong-Bin Shen"
  ],
  "abstract": "Rigid image alignment is a fundamental task in computer vision, while the\ntraditional algorithms are either too sensitive to noise or time-consuming.\nRecent unsupervised image alignment methods developed based on spatial\ntransformer networks show an improved performance on clean images but will not\nachieve satisfactory performance on noisy images due to its heavy reliance on\npixel value comparations. To handle such challenging applications, we report a\nnew unsupervised difference learning (UDL) strategy and apply it to rigid image\nalignment. UDL exploits the quantitative properties of regression tasks and\nconverts the original unsupervised problem to pseudo supervised problem. Under\nthe new UDL-based image alignment pipeline, rotation can be accurately\nestimated on both clean and noisy images and translations can then be easily\nsolved. Experimental results on both nature and cryo-EM images demonstrate the\nefficacy of our UDL-based unsupervised rigid image alignment method.",
  "text": "Unsupervised Difference Learning for Noisy\nRigid Image Alignment\nYu-Xuan Chen1, Dagan Feng2, and Hong-Bin Shen1∗\n1 Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong\nUniversity, and Key Laboratory of System Control and Information Processing,\nMinistry of Education of China, Shanghai, 200240, China\n{yxchen,hbshen}@sjtu.edu.cn\n2 School of Computer Science, University of Sydney, Sydney, 2006, Australia\ndagan.feng@sydney.edu.au\nAbstract. Rigid image alignment is a fundamental task in computer\nvision, while the traditional algorithms are either too sensitive to noise\nor time-consuming. Recent unsupervised image alignment methods de-\nveloped based on spatial transformer networks show an improved per-\nformance on clean images but will not achieve satisfactory performance\non noisy images due to its heavy reliance on pixel value comparations.\nTo handle such challenging applications, we report a new unsupervised\ndifference learning (UDL) strategy and apply it to rigid image alignment.\nUDL exploits the quantitative properties of regression tasks and converts\nthe original unsupervised problem to pseudo supervised problem. Under\nthe new UDL-based image alignment pipeline, rotation can be accurately\nestimated on both clean and noisy images and translations can then be\neasily solved. Experimental results on both nature and cryo-EM images\ndemonstrate the efficacy of our UDL-based unsupervised rigid image\nalignment method.\nKeywords: Rigid Image Alignment, Unsupervised Learning, Difference\nLearning, Regression\n1\nIntroduction\nImage alignment which helps establishing pixel-to-pixel correspondence between\ntwo images is a fundamental task in computer vision and can be widely ap-\nplied to tasks like image clustering [23], 3D image reconstruction [2], semantic\nsegmentation [30], and etc. Finding the global optimal parameters of transfor-\nmation model is an essential and challenging part of image alignment. Tradi-\ntionally, image alignment was performed in non-trainable approaches in which\neither hand-crafted features or pixel space were used to find the global optimal\nparameters of transformation model. In recent years, due to the fast develop-\nment of deep neural networks (DNN), deep end-to-end image alignment methods\nhave been proposed for both supervised image alignment [7] and unsupervised\nimage alignment [24]. Compared to traditional methods, deep end-to-end image\n2\nPreprint\nalignment could process a large amount of data in a short time and improve the\nperformance of image alignment by better feature learning on certain datasets.\nIn supervised image alignment, transformation parameters are directly re-\ngressed using DNN with ground truth (GT) transformation parameters required\nfor learning. Although supervised learning achieves promising results, GT la-\nbeling is prohibitively expensive and time-consuming in real world applications,\nwhich would greatly limit its practical use scope. In [24], end-to-end unsuper-\nvised image alignment is achieved by combing DNN with spatial transformer\nnetworks (STN) [15], where DNN is used to regress transformation parameters\nand STN is used to create a pixel-to-pixel loss function between source image\nand target image. Though such unsupervised strategy achieves promising re-\nsults on high quality images, it is not robust to noise. Several methods [38,18]\nare proposed to improve [24] and make it applicable to wider situations by either\ncomparing features instead of input images or designing more complicated loss\nfunctions. However, since the loss function of STN-based unsupervised learning\nis essentially relying on the comparations of pixel values which will be dominated\nby noise in low signal-to-noise-ratio (SNR) cases, the STN-based unsupervised\nstrategy is still not very robust to noisy image alignment applications.\nTo tackle image alignment in noisy situations, we propose a new unsupervised\ndifference learning strategy UDL and design unsupervised rigid image alignment\nalgorithm based on it in conjunction with network architectures. Our UDL-based\nimage alignment method is designed for rigid image transformation containing\nonly in-plane rotation and translations, which is essential in image alignment and\ncommon in certain cases like medical images registration [33] and cryo-electron\nmicroscopy (cryo-EM) single particle image alignment [2]. Unlike STN-based un-\nsupervised learning methods which rely on pixel values to guide network learning,\nUDL strategy use the difference value between network outputs to create pseudo\nlabels, so that unsupervised learning can be achieved using the same architecture\nas supervised learning. Utilizing such pseudo labels to train DNN, our unsuper-\nvised learning strategy can be generalized to multiple challenging situations just\nlike the supervised learning pipelines. In particular, UDL strategy can provide\nan unsupervised learning solution to noisy rigid image alignment.\nWe demonstrate the simplicity and effectiveness of UDL strategy by applying\nit to synthetic MS-COCO dataset [20], synthetic cryo-EM single particle dataset\nand real-world cryo-EM single particle dataset. Experimental results show that\nwe achieve competitive results as STN-based unsupervised learning methods on\nclean images, better results than supervised learning method on noisy images\nand better results than traditional Fourier-based method on cryo-EM images,\nsuggesting the UDL-based rigid image alignment method works for both clean\nand noisy images. To summarize, our main contributions are:\n- We propose a new unsupervised difference learning strategy UDL, which\nconverts unsupervised learning to supervised learning utilizing the difference\nof network outputs to generate pseudo labels.\nPreprint\n3\n- We apply UDL to noisy rigid image alignment and demonstrate the perfor-\nmance of our UDL-based rigid image alignment method on both nature and\ncryo-EM datasets.\n- We suggest that UDL can also be extended to other regression tasks.\n2\nRelated Work\n2.1\nTraditional Image Alignment\nTraditional image alignment methods can be roughly divided into two cat-\negories [39], feature-based methods and area-based methods. Featured-based\nmethods calculate transformation parameters by detecting and matching local\nfeatures. After matched feature pairs obtained by methods such as SIFT [21]\nand ORB [31], transformation parameters can be estimated by algorithms such\nas RANSAC [9]. Area-based methods directly use image similarity like cross-\ncorrelation (CC) and sequential similarity detection algorithm (SSDA) [1] to\nfind the best matching position, and the progress could be speeded up by Fourier\ntransform [5]. Although featured-based methods usually outperform area-based\nmethods, their performance heavily depends on the quality of hand-crafted fea-\ntures and input images. In noisy images such as cryo-EM images where SNR is\ntypically lower than 0.1, local feature detection would be dominated by noise\nand thus the performance of featured-based methods would degrade. Though\narea-based methods would be more robust in such noisy cases, they are usually\nof intensive computational cost, and their image similarity calculations are also\nsubject to noise.\nIn the typical cryo-EM single particle noisy image alignment, algorithms in\nsoftware such as XMIPP [29] and SPIDER [10] use iterative strategy to reduce\nalignment error caused by noise. Though iterative strategies help on noisy sit-\nuations, it is generally computationally expensive. EMAF [6] uses hand-crafted\nfeatures on Fourier space to directly estimate transformation, but its practical\nuse is limited due to its Gaussian noise assumption.\n2.2\nSupervised Image Alignment\nBased on the powerful feature learning ability of convolutional neural networks\n(CNN), deep learning has been applied to image alignment firstly in [7], which\nbuilds a VGG like architecture [32] and uses four corner displacements to describe\ntransformation model. This method achieves better results than traditional im-\nage alignment methods on homography estimation. To improve the architecture\nof [7], feature correlation and matching layer are introduced in [27] to help learn-\ning correspondence between input images so that more accurate transformation\nparameters can be estimated. Zeng et al. [36] uses a U-Net like architecture [28]\nto estimate the pixel-to-pixel bijection and then transformation parameters are\ncalculated using direct linear transformation (DLT) [11] and RANSAC. Although\nsupervised methods achieve promising results on both clean and noisy images,\nlack of the ground truth labels limits their use in the real world applications.\n4\nPreprint\n2.3\nUnsupervised Image Alignment\nBased on differentiable image warping proposed in STN, unsupervised image\nalignment proposed in [24] adopts four corner displacements to describe homog-\nraphy transformation model and uses STN to warp source image so that the\npixel-to-pixel density error can be calculated and minimized. Such unsupervised\nstrategy achieve comparable results as [7] on synthetic MS-COCO dataset, but\nwill face challenges when dealing with large lamination change or noisy images.\nTo make [24] more robust, Zhang et al. [38] applies pixel-to-pixel minimization\nto features instead of images and maximizes feature difference of two input im-\nages to avoid trivial solutions. However, this method is designed for small scale\nalignment and can be difficult on big changes. Koguciuk et al. [18] introduces\nbidirectional implicit homography estimation (biHomE) loss to decouple trans-\nformation parameters estimation from the representation learning and make the\napproach more robust. With the advancement of Transformers [35] in natural\nlanguage processing (NLP), the Vision Transformer (ViT) [8] attracts much at-\ntention in computer vision field. In [4], ViT is applied to medical image alignment\nfor deformation field estimation and then STN is used to minimize pixel-to-pixel\ndensity error between warped source image and target image. Nevertheless, since\nthe loss function of STN-based unsupervised learning essentially relies on pixel\nvalues, its performance on noisy situations has much space to improve.\n3\nMethods\n3.1\nUnsupervised Difference Learning Strategy UDL\nIn general, when GT labels are available, supervised learning is more robust than\nunsupervised learning and can adapt for more situations. In supervised learn-\ning, network outputs are compared to GT labels to guide loss backpropagation.\nHowever, since GT labels are not always available in real world applications, su-\npervised problems often become unsupervised problems. On such unsupervised\ncases, we find that differences between network outputs are interpretable for\nsome regression tasks, which means output differences can be used as pseudo\nlabels to guide learning. Based on this intuitive idea, we propose an unsuper-\nvised difference learning strategy UDL in this paper for certain regression tasks\nincluding rigid image alignment.\nDenote two inputs to network as x1 and x2, their corresponding outputs as\ny1 and y2, their corresponding GT labels as ˆy1 and ˆy2, and the network itself as\nFθ, where θ denotes network parameters; then the forward progress of network\ncan be represented as y1 = Fθ(x1) and y2 = Fθ(x2).\nIf there exist function G and function H which satisfy G( ˆy1, ˆy2) = H(x1, x2),\nthen pseudo labels can be created for loss function and the corresponding su-\npervised training with pseudo labels can be carried on even though the original\nGT labels are unknown; that is:\n \\l\na\nbel {eq:1} \\min _{ \\ theta  } ||G(F_{\\theta }(x_1),F_{\\theta }(x_2))-H(x_1,x_2)||\n(1)\nPreprint\n5\nIn the most ideal case, Eq. 1 is equal to supervised training with GTs so that\ncorrect outputs can be obtained from well-trained network by pseudo labels. In\nsuch case, Eq. 1 would be equal to:\n \\l\na\nbel {eq: 2 } \\min _{\\theta } ||F_{\\theta }(x_1)-\\hat {y_1}||\n(2)\nHowever, although the convergence of Eq. 2 would always lead to the conver-\ngence of Eq. 1, the inverse progress does not always hold. Ignoring the training\nprogress and assuming network will always converge once GT labels are given,\nfrom the perspective of equation solving, there is one unknown variable y1 (i.e.\nFθ(x1)) to solve in supervised training problem and one equation y1 = ˆy1 given\nin format of loss function. Whereas in unsupervised training problem, no equa-\ntion is provided but there is one unknown variable y1 to solve. In supervised\ntraining with pseudo labels, there are two unknown variables y1 and y2 to solve\nbut only one equation Eq. 1 is provided, which leads to an indeterminate equa-\ntion whose solution space consists of infinite solutions. By designing Eq. 1, the\noriginal unsolvable problem is converted to a solvable problem with infinite pos-\nsible solutions, but only one of them is the correct solution of target problem\nEq. 2.\nIn this way, the theoretical convergence of the designed pseudo supervised\ntraining could not be guaranteed due to the training progress could converge\non any solution in solution space instead of the desired one. However, since\nthe solutions are heavily subject to the form of Eq. 1 and thus subject to the\nformulation of function G and H, by properly designing these two functions,\nsolutions in solution space of Eq. 2 could be obtained in a trackable form, which\nthen leads to the convergence of pseudo supervised training.\nBased on this idea, function G is designed as the difference value between\nFθ(x1) and Fθ(x2). For instance, difference value could be Fθ(x1) −Fθ(x2) in\nsimple numeric tasks, transformation between two pairs of images in image align-\nment tasks and bounding box movements in object recognition tasks. Such design\nof function G would make Eq. 2 be in similar form of the derivative of Eq. 1,\nand leads to simple form of solutions (i.e. the desired solution plus a fixed bias).\nThen the training convergence can be kept at the most degree. However, even\nif function G is designed in such simple form, function H is hard to formulate\nfor any two sampled inputs. Therefore, to better formulate the relationship be-\ntween two inputs, we generate input x2 from input x1 by manually designing\ndisturbance to create output differences.\nDenote x2 as Dτ(x1) where D represents the manually designed disturbance\nfunction and τ represents the scale of output difference caused by disturbance;\nthen Eq. 1 becomes:\n \\l\na\nbel {eq:3} \\min _{\\the t a } | |G(F_{\\theta }(x_1),F_{\\theta }(D_{\\tau }(x_1)))-H(x_1,D_{\\tau }(x_1))||\n(3)\nIn Eq. 3, function G should calculate output difference as close as possible to\nthe manually designed difference τ and thus leads to convergence of pseudo\n6\nPreprint\nsupervised training. Then function G and H are:\n \\ label { eq:4,5} G :  \nF_{\n\\ t het a }(x_ 1 ), F_{\\theta }(D_{\\tau }(x_1) \\rightarrow \\tau \\\\ H: x_1, D_{\\tau }(x_1) \\rightarrow \\tau\n(5)\nAs mentioned before, even though the convergence of pseudo supervised\ntraining can be achieved, network output y1 could be different from GT la-\nbel ˆy1 due to infinite solutions of indeterminate equation. Denote the difference\nfunction as T(Fθ(x1)) −T(Fθ(x2)), where T could be simply identity function\nor in other formats to convert difference function to subtractive format; then\nnetwork outputs would be T(Fθ(x)) + C. Such progress is just like recovering a\nfunction from its derivative. If disturbance is well-designed, C would be a con-\nstant value and easy to determine on any input in dataset with only one GT\nrequired. Though detailed design of disturbance depends on specific tasks, the\ncore idea is to minimize shared elements in original inputs and disturbed inputs\nso that C is less likely to generate from the shared elements of network inputs.\nThe specific disturbance of UDL for rigid image alignment would be discussed\nin the following section.\n3.2\nUnsupervised Image Alignment Based on UDL\nAs mentioned before, we focus on rigid image alignment which contains only in-\nplane rotation and translations. Network input is a pair of images and network\noutputs are their relative transformation parameters which could be in formats\nlike two corner displacements (which are enough to cover rigid transformation),\ntransformation matrix or simply one rotation angle and two translations. Based\non the proposed unsupervised difference learning strategy UDL in Sect. 3.1, the\nunsupervised rigid image alignment pipeline is shown in Fig. 1.\nAs shown in Fig. 1, input image I1 is transformed into disturbed image I′\n1 by\nparameter τ1; input image I2 is transformed into disturbed image I′\n2 by parame-\nter τ2; network outputs for image pairs (I1, I2) and (I′\n1, I′\n2) are correspondingly P\nand P ′. Disturbances are designed as rotation and translations on source images.\nFor any output format, if quantitative transformation relationship is going to\nestablish between two images, the output must be converted to transformation\nmatrix which projects every pixel in source image to target image. In Fig. 1, if\nimage I1 is likely to transform into image I′\n2, there are two paths, i.e. I1 →I2 →\nI′\n2 and I1 →I′\n1 →I′\n2. Denote transformation matrices for τ1, τ2, P and P ′ are\ncorrespondingly M1, M2, M and M ′, and use image to represent all its pixels;\nthen quantitative transformation relationship can be established as:\n \\\nl a bel {e\nq:\n6 }  I'_2=M_2MI_1, \\hspace {1cm} I'_2=M'M_1I_1\n(6)\nwhich leads to:\n \\l a b el {eq:7} M_2M=M'M_1\n(7)\nPreprint\n7\nFig. 1. Unsupervised difference learning strategy UDL for rigid image alignment us-\ning symbols defined in Sect. 3.1. In functions G and H, the minus sign represents\ntransformation parameters that one image needs to transform into another image.\nThen the loss function can be designed as:\n \\ l abel { e q:8} L_M=||M_2M-M'M_1||\n(8)\nFor rigid transformation, transformation matrix is of certain format:\n \\\nl\na\nbel  {eq : 9}\n M=\\ b egi n  {\nb\nm\na\nt\nrix} \\cos \\alpha & \\sin \\alpha & \\Delta x \\\\ -\\sin \\alpha & \\cos \\alpha & \\Delta y \\\\ 0 & 0 & 1 \\end {bmatrix}\n(9)\nwhere α is rotation angle, ∆x is translation in X axis and ∆y is translation\nin Y axis. By matrix multiplication, Eq. 8 leads to an indeterminate equation\nsystem of 4 independent equations and 6 unknown variables. If output formats\nlike corner displacements are used, sin α and cos α would be considered as two\nparameters and even more unknown variables would be produced. Such a se-\nvere imbalance of equations and variables makes Eq. 8 hard to converge on\ntarget solution. The form of multivariate linear equations generated from ma-\ntrix multiplication also implies differences between solutions in solution space\nof indeterminate equation system cannot be simply described in a subtractive\nformat like T(Fθ(x)) + C in Sect. 3.1.\nBased on the above considerations, we adopt a strategy of separating rotation\nand translation, in which we use UDL to estimate only the rotation and then\napply Fourier alignment to re-rotated images to estimate translations. Since only\nrotation is estimated in UDL, network can simply output rotation angle between\ntwo images. Then Eq. 7 and Eq. 8 become:\n  \\ l a be l  { e q : 10 }  \\ a lp\nha _\n2+ \\ alph a  =\\ a lph a  '+\\alpha _1 \\Rightarrow \\alpha -\\alpha '=\\alpha _1-\\alpha _2 \\\\ \\label {eq:11} L_{\\alpha }=||(\\alpha -\\alpha ')-(\\alpha _1-\\alpha _2)||\n(11)\n8\nPreprint\nwhere α1, α2, α and α′ are the corresponding rotation angles of M1, M2, M and\nM ′ and also the correspondingly values of P1, P2, P and P ′. Then function G\nwould be G = P −P ′ = (α −α′) and function H would be H = Hα1(I1, I′\n1) −\nHα2(I2, I′\n2) = α1 −α2, where I′\n1, I′\n2 are generated from I1, I2 by rotation angle\nα1, α2 and random translations in a pre-defined range.\nSince rotation angle is an infinite continuous periodic signal and we expect\nits value locates in period [0, r]([0◦, 360◦] in implementation), Eq. 11 is modified\nas follows by considering the signal period and range penalty:\n \\ label  {eq:12 } L = L_r((\\ a lpha - \\alpha ')\\%r,(\\alpha _1-\\alpha _2)\\%r)+P_r(\\alpha )+P_r(\\alpha ')\n(12)\nwith\n  \\la be l\n {eq:13} L _ r( a,b )=\\lef t \\{ \\ b egi\nn {array} { l}  \\m in (\\m in ( a - r,b\n),\\m\nin (a , b)), a  \\g e q r/2 \\ \\  \\min (\\min (a+r,b),\\min (a,b)), a < r/2 \\end {array}\\right . \\\\ \\label {eq:14} P_r(a)=\\max (0,-a)+\\max (0,r-a)\n(14)\nwhere Lr(a, b) is a function considering period affection with the premise that a,b\nare both in range [0, r], and Pr(a) is penalty function to force network outputs\nbeing in range [0, r].\nIn our implementation, both two input images are disturbed to reduce shared\nelements in original inputs and disturbed inputs. After network training, network\noutputs α + C where C is a constant value and can be determined on any pair\nof input images. After rotation angle estimation, translations can be simply\nand quickly estimated by Fourier correlation [5], which is robust to noise for\ntranslation only image alignment.\n3.3\nNetwork Architecture\nOur network architecture is built upon CNN and the overall architecture is\nshown in Fig. 2. The network takes in two images I1, I2 as input and outputs\ntheir relative rotation angle α12. The entire network architecture consists of\nmask prediction module m(·), feature extraction module fe(·), feature matching\nmodule fm(·) and regression module r(·). All modules except for regression\nmodule are replicated in a siamese configuration so that two input images pass\nthrough two identical networks with shared parameters.\nMask prediction module. We adopt mask prediction module proposed in [38]\nto automatically learn the positions of significant areas in input images. Espe-\ncially in cryo-EM single particle images, particles are located near the centers\nof images and surrounded by meaningless background, where transformation\nrelationship only holds between particles instead of images. Mask prediction\nmodule would help the network focus on particles and learn features containing\nricher transformation information. Mask prediction module consists of four con-\nvolutional blocks with the last one using Sigmoid activate function instead of\nReLU. Mask prediction module takes in image I ∈R1×h×w and outputs mask\nM ∈R1×h×w.\nPreprint\n9\nFig. 2. The overall architecture of our rigid image alignment network using the unsu-\npervised difference learning strategy UDL.\nFeature extraction module. Feature extraction module consists of four residual\nblocks [12] to learn deep features for image alignment. Unlike architecture in\n[7,38,18] where an entire ResNet or VGG is used for feature extraction, we use\nseveral residual blocks cropped at middle stage to extract features so that there\nexists enough spatial information for the following feature matching module.\nFeature extraction takes in image with mask M × I ∈R1×h×w and outputs\nfeature F ∈R256×(h/16)×(w/16).\nFeature matching module. We adopt feature matching layer proposed in [27]\nto utilize pairwise descriptor similarities and their spatial locations to estimate\ntransformation parameters. Feature matching layer is essentially a normalized\ncross-correlation function in which all channels of one feature location is com-\npared to all locations of another feature to generate correlation map. The feature\nmatching layer takes in two features F ∈Rc×h×w as input and outputs their\ncorrelation map C ∈R(h×w)×h×w. Siamese structure proposed in [37] is adopted\nto explore more spatial correlation information and two following convolutional\nblocks after the feature matching module are used to help further enrich spatial\nfeatures.\nRegression module. After feature matching, two correlation maps are flat-\ntened into one dimensional array, concatenated together and feed into fully con-\nnected layers. Regression module consists of three fully connected layers and\noutputs the relative rotation angle between I2 and I1.\n4\nExperimental Results\n4.1\nDatasets and Implementation Details\nSince collecting data for image alignment with ground truth is hard, based on\nsimilar pipeline of benchmark dataset generation in [7,38,18], we generate syn-\nthetic MS-COCO dataset for rigid alignment evaluation. Each image in MS-\nCOCO dataset is rotated with random rotation angle ranging in [0◦, 360◦] and\n10\nPreprint\nshifted with random translation pixels ranging in [0, 10] for both horizontal and\nvertical directions (i.e. left or right, up or down). Then two image patches of\nsize 128 × 128 are extracted at the same position of both original image and\ntransformed image to generate image alignment patch pairs. If noise is required,\nGaussian noise is added to each image patch to make its SNR being certain\nvalue (0.5, 0.1 and 0.05 in implementation) or salt-and-pepper noise is added to\neach image patch with certain proportion for both salt noise and pepper noise\n(0.1, 0.2, 0.3 in implementation). There are about 115k image pairs for train-\ning and 5k image pairs for evaluation. Image patch is converted to grayscale as\n[7,24,38,18]. The data preparation process is shown in Fig. 3.\nFig. 3. Process for generating synthetic MS-COCO dataset. (a) apply rotation and\ntranslations to source image and extract patches on two images at the same position.\n(b) patches from source image. (c) patches from transformed image. From left to right\nin (b) and (c): clean, Gaussian SNR 0.5, Gaussian SNR 0.1, Gaussian SNR 0.05, salt-\nand-pepper proportion 0.1, salt-and-pepper proportion 0.2, salt-and-pepper proportion\n0.3.\nIn cryo-EM datasets, transformation relationship only holds on part of the\nentire image (i.e. particles) and SNR of single particle images is typically lower\nthan 0.1, making the alignment task challenging. Although the format of real-\nworld cryo-EM noise is complicated, as depicted in [35], it is generally considered\nas Gaussian distribution. Such assumption is widely used in the field [3,16,26]\nand also adopted in this paper. As shown in Fig. 4, we use datasets from EM-\nPIAR [14] and real-world dataset GroEL [22] to generate three cluster centers\nas source images by existing clustering method CL2D [34]. Then particle in each\nsource image is extracted to generate synthetic datasets of 50k images of size\n128 × 128 with random surrounded background, random rotation angle ranging\nin [0◦, 360◦], random translation pixels ranging in [0, 10], and Gaussian noise to\nmake SNR of each image being 0.1. In addition to synthetic cryo-EM datasets\nwith GT labels to quantify performance of alignment methods, we also use real-\nPreprint\n11\nworld cryo-EM dataset GroEL [22] to validate our method. Since no GT label is\navailable for real-world cryo-EM datasets, we align every image to a given refer-\nence image and calculate similarity between aligned image and reference image\nas a similarity score, as better alignment method will yield higher similarity.\nFig. 4. From left to right in the figure in each line: GroEL, EMPIAR-10034, EMPIAR-\n10065. (a) three cluster centers as source image for synthetic cryo-EM datasets. (b)\nclean images generated with random background. (c) Gaussian noisy images generated\nfrom clean images of SNR 0.1.\nIn implementation, our network is trained with batch size 64 by Adam\noptimizer [17] with parameters lr = 5.0 × 10−4, β1 = 0.9, β2 = 0.999 and\neps = 1.0 × 10−8. For every 12k iterations, learning rate lr is reduced by 20%.\nConvolutional block consists of one 2D convolutional layer of kernel size 3 × 3, a\nbatch normalization layer [13] and an activation layer ReLU or Sigmoid. Resid-\nual block consists of three 2D convolutional layers of kernel size 1 × 1, 3 × 3 and\n1 × 1 with the first two followed by batch normalization layer and ReLU layer,\nand the last one only followed by batch normalization layer. Shortcut consists of\na 2D convolutional layer of kernel size 1×1 and a batch normalization layer. All\nmaxpooling layers use down-sampling factor 2. The regression module consists\nof three fully connected layers of output sizes 2000, 2000 and 1. All methods\nincluding STN-based unsupervised method, our UDL-based method and super-\nvised method use the same architecture with only the last fully connected layer\nreplaced in STN-based unsupervised method to output two corner displacements\n(i.e. 4 parameters). We implement all code in PyTorch [25] and release the source\ncode for public use.\n4.2\nResults on Synthetic MS-COCO Dataset\nWe perform experiments on clean, Gaussian noisy and salt-and-pepper noisy\ndatasets to compare STN-based unsupervised method, our UDL-based unsu-\npervised method and supervised method. All methods use 115k pair of image\npatches for training and 5k pair of image patches for testing, where training\nset and test set were pre-separated by MS-COCO dataset and patches of little\n12\nPreprint\nFig. 5. From left to right in the figures in each line: original image, reference image,\nresults of STN-based method, results of our UDL-based method, results of supervised\nmethod. (a) Clean dataset. (b) Gaussian SNR 0.1 noisy dataset. (c) Salt-and-pepper\nproportion 0.2 noisy dataset.\ntextures are discarded. As depicted in Sect. 3.2, our UDL-based method out-\nputs only the rotation angle. Since translations are determined by traditional\nFourier alignment once correct rotation is given, to concentrate on evaluating\nunsupervised methods, we simply report rotation angle error of each method.\nSupervised method and our UDL-based method directly output rotation angle\nand STN-based method outputs two corner displacements which is then con-\nverted to rotation angle for comparation. Results of rotation angle error in test\nset are shown in Table 1 and sampled output images are shown in Fig. 5.\nTable 1. Rotation angle error for synthetic MS-COCO datasets (in degree).\nSupervised\nSTN-based\nUDL-based\nClean\n3.87\n0.96\n2.57\nGaussian SNR 0.5\n6.78\nN/A1\n4.29\nGaussian SNR 0.1\n9.23\nN/A1\n5.69\nGaussian SNR 0.05\n12.27\nN/A1\n6.49\nSalt-and-pepper proportion 0.1\n5.72\nN/A1\n3.38\nSalt-and-pepper proportion 0.2\n5.47\nN/A1\n4.92\nSalt-and-pepper proportion 0.3\n6.10\nN/A1\n12.10\n1 Not converge in our experiments.\nAs shown in Table 1, STN-based unsupervised method outperforms the other\ntwo methods on clean dataset but does not converge in either Gaussian noisy\ndataset or salt-and-pepper noisy dataset in our experiments. Its heavy reliance on\npixel value comparations greatly improves its accuracy on clean dataset where\npixel value comparations are reliable but may also cause its failure on noisy\nPreprint\n13\ndatasets where pixel value comparations are dominated by noise. In such noisy\ncases, supervised method shows its robustness though its performance degrades\non test set due to overfitting. Our proposed UDL-based unsupervised method\nhas comparable or even better performance than supervised method on most\ndatasets, and suffers less from overfitting due to unsupervised training. Such\nresults demonstrate the correctness and effectiveness of our UDL strategy and\nUDL-based unsupervised rigid image alignment method.\n4.3\nResults on Cryo-EM Dataset\nTo better explore the usage of our UDL-based unsupervised rigid image align-\nment method, we conduct experiments on both synthetic and real-world cryo-\nEM single particle images to evaluate performance. As depicted in [6], Fourier\ndomain makes it easier to distinguish particle from noise and background in\ncryo-EM images compared to spatial domain, which also keeps exactly the same\nrotation relationship as spatial domain. Therefore, though keeping the network\narchitecture and the difference learning strategy UDL unchanged, we replace\ninputs images to network with their Fourier spectrums to improve the train-\ning convergence. As illustrated in Sect. 4.1, we use GroEL, EMPIAR-10034,\nEMPIAR-10065 to generate three synthetic cryo-EM single particle datasets for\nquantitative comparison. We compare it to traditional Fourier-based method [6]\nand show results in Table 2.\nTable 2. Rotation angle error for synthetic cryo-EM datasets (in degree).\nGroEL\nEMPIAR-10034\nEMPIAR-10065\nFourier-based\n1.75\n6.12\n2.99\nUDL-based\n0.84\n3.27\n2.33\nAs can be seen, we achieve more accurate rotation estimation than tradi-\ntional Fourier-based method on synthetic cryo-EM datasets. For the real GroEL\ndataset, due to it only has 4694 images which is not enough for network train-\ning, we pre-train the network on synthetic GroEL dataset for 20 epochs and\nthen initialize the network with pre-trained model to train on the real-world\nGroEL dataset. Since real GroEL dataset has no GTs, we align every image to a\ngiven reference image and then calculate their similarity using correntropy [34].\nWe also generate the average image of aligned images as shown in Fig. 6. The\naverage similarity of aligned dataset is used to evaluate the performance of\nmethod, with ours’ being 0.965 and the traditional one’s being 0.961. Our pro-\nposed UDL-based unsupervised method achieves better results than traditional\nFourier-based methods on both synthetic and real-world cryo-EM single particle\ndatasets.\n14\nPreprint\nFig. 6. From left to right in the figure: reference image, average aligned image of our\nUDL-based method, average aligned image of traditional Fourier-based method.\n5\nDiscussions\nFrom the perspective of equation solving, unknown variables (network outputs)\nand equations (loss functions using GTs) are equal in supervised learning prob-\nlems. Whereas in unsupervised learning problems, naturally no equation is pro-\nvided but unknown variables are required to be solved. Therefore, various unsu-\npervised learning methods are proposed to create equations utilizing properties\nof specific tasks [24,19]. However, such specifically designed unsupervised strat-\negy would be hard to generalize or transfer to other tasks.\nIn this paper, we propose a new difference learning strategy UDL which con-\nverts unsupervised learning to pseudo supervised learning. Though proposed to\nsolve rigid image alignment in noisy cases, UDL essentially exploits the quantita-\ntive properties of network outputs in regression problem and thus could be gen-\neralized to certain regression problems. Unlike unsupervised learning methods\ndesigned for specific tasks which could create equal or even more equations than\nunknown variables (like pixel comparations in STN-based image alignment),\nUDL converts original problem containing no equation but unknown variables to\nproblem containing equations and more unknown variables. In this way, though\nUDL essentially could not guarantee the convergence of training, it provides\npossible solutions containing the correct one. Then correct solutions could be\nobtained by adding restrictions or formulizing the difference functions, which\ndepends on specific tasks. As discussed in Sect. 3.2, we use rotation angle as\nnetwork output in rigid image alignment so that all solutions could be described\nby the correct solution plus a fixed bias. Though we cannot indicate a suitable\ndifference function for each regression task due to lack of domain-specific knowl-\nedge, we hope that the UDL strategy could serve as a feasible method to help\nestablish unsupervised methods for more regression tasks.\n6\nConclusions\nIn this paper, we have proposed a new unsupervised difference learning strat-\negy UDL for certain regression tasks and developed an unsupervised rigid image\nalignment method aimed at noisy cases based on it. UDL presents a new unsu-\npervised learning strategy to convert unsupervised regression problem to pseudo\nsupervised problem and its convergence can be achieved through designing differ-\nPreprint\n15\nence functions based on specific tasks. It is expected that the new UDL pipeline\nwill be extended to more applications in regression tasks.\nReferences\n1. Barnea, D.I., Silverman, H.F.: A class of algorithms for fast digital im-\nage registration. IEEE transactions on Computers 100(2), 179–186 (1972).\nhttps://doi.org/10.1109/TC.1972.5008923\n2. Bendory,\nT.,\nBartesaghi,\nA.,\nSinger,\nA.:\nSingle-particle\ncryo-electron\nmicroscopy:\nMathematical\ntheory,\ncomputational\nchallenges,\nand\nop-\nportunities.\nIEEE\nsignal\nprocessing\nmagazine\n37(2),\n58–76\n(2020).\nhttps://doi.org/10.1109/MSP.2019.2957822\n3. Bhamre, T., Zhang, T., Singer, A.: Denoising and covariance estimation of sin-\ngle particle cryo-em images. Journal of structural biology 195(1), 72–81 (2016).\nhttps://doi.org/10.1016/j.jsb.2016.04.013\n4. Chen, J., He, Y., Frey, E.C., Li, Y., Du, Y.: Vit-v-net: Vision transformer for unsu-\npervised volumetric medical image registration. arXiv preprint arXiv:2104.06468\n(2021)\n5. Chen, Q.s., Defrise, M., Deconinck, F.: Symmetric phase-only matched filtering\nof fourier-mellin transforms for image registration and recognition. IEEE Trans-\nactions on pattern analysis and machine intelligence 16(12), 1156–1168 (1994).\nhttps://doi.org/10.1109/34.387491\n6. Chen,\nY.X.,\nXie,\nR.,\nYang,\nY.,\nHe,\nL.,\nFeng,\nD.,\nShen,\nH.B.:\nFast\ncryo-em\nimage\nalignment\nalgorithm\nusing\npower\nspectrum\nfeatures.\nJour-\nnal\nof\nChemical\nInformation\nand\nModeling\n61(9),\n4795–4806\n(2021).\nhttps://doi.org/10.1021/acs.jcim.1c00745\n7. DeTone, D., Malisiewicz, T., Rabinovich, A.: Deep image homography estimation.\narXiv preprint arXiv:1606.03798 (2016)\n8. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n9. Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model\nfitting with applications to image analysis and automated cartography. Communi-\ncations of the ACM 24(6), 381–395 (1981). https://doi.org/10.1145/358669.358692\n10. Frank, J., Radermacher, M., Penczek, P., Zhu, J., Li, Y., Ladjadj, M., Leith,\nA.: Spider and web: processing and visualization of images in 3d electron mi-\ncroscopy and related fields. Journal of structural biology 116(1), 190–199 (1996).\nhttps://doi.org/10.1006/jsbi.1996.0030\n11. Hartley, R., Zisserman, A.: Multiple view geometry in computer vision. Cambridge\nuniversity press (2003)\n12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 770–778 (2016)\n13. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In: International conference on machine learning.\npp. 448–456. PMLR (2015)\n14. Iudin, A., Korir, P.K., Salavert-Torres, J., Kleywegt, G.J., Patwardhan, A.: Em-\npiar: a public archive for raw electron microscopy image data. Nature methods\n13(5), 387–388 (2016). https://doi.org/10.1038/nmeth.3806\n16\nPreprint\n15. Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial transformer networks.\nAdvances in neural information processing systems 28 (2015)\n16. Joyeux, L., Penczek, P.A.: Efficiency of 2d alignment methods. Ultramicroscopy\n92(2), 33–46 (2002). https://doi.org/10.1016/S0304-3991(01)00154-1\n17. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n18. Koguciuk, D., Arani, E., Zonooz, B.: Perceptual loss for robust unsupervised ho-\nmography estimation. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 4274–4283 (2021)\n19. Krull, A., Buchholz, T.O., Jug, F.: Noise2void-learning denoising from single noisy\nimages. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. pp. 2129–2137 (2019)\n20. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,\nZitnick, C.L.: Microsoft coco: Common objects in context. In: European conference\non computer vision. pp. 740–755. Springer (2014). https://doi.org/10.1007/978-3-\n319-10602-1 48\n21. Lowe,\nD.G.:\nDistinctive\nimage\nfeatures\nfrom\nscale-invariant\nkey-\npoints.\nInternational\njournal\nof\ncomputer\nvision\n60(2),\n91–110\n(2004).\nhttps://doi.org/10.1023/B:VISI.0000029664.99615.94\n22. Ludtke, S.J., Chen, D.H., Song, J.L., Chuang, D.T., Chiu, W.: Seeing groel at 6 ˚a\nresolution by single particle electron cryomicroscopy. Structure 12(7), 1129–1136\n(2004). https://doi.org/10.1016/j.str.2004.05.006\n23. Monnier, T., Groueix, T., Aubry, M.: Deep transformation-invariant clustering.\nAdvances in Neural Information Processing Systems 33, 7945–7955 (2020)\n24. Nguyen, T., Chen, S.W., Shivakumar, S.S., Taylor, C.J., Kumar, V.: Un-\nsupervised\ndeep\nhomography:\nA\nfast\nand\nrobust\nhomography\nestimation\nmodel.\nIEEE\nRobotics\nand\nAutomation\nLetters\n3(3),\n2346–2353\n(2018).\nhttps://doi.org/10.1109/LRA.2018.2809549\n25. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\nT., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-\nperformance deep learning library. Advances in neural information processing sys-\ntems 32 (2019)\n26. Punjani, A., Zhang, H., Fleet, D.J.: Non-uniform refinement: adaptive regular-\nization improves single-particle cryo-em reconstruction. Nature methods 17(12),\n1214–1221 (2020). https://doi.org/10.1038/s41592-020-00990-8\n27. Rocco, I., Arandjelovic, R., Sivic, J.: Convolutional neural network architecture for\ngeometric matching. In: Proceedings of the IEEE conference on computer vision\nand pattern recognition. pp. 6148–6157 (2017)\n28. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for\nbiomedical image segmentation. In: International Conference on Medical im-\nage computing and computer-assisted intervention. pp. 234–241. Springer (2015).\nhttps://doi.org/10.1007/978-3-319-24574-4 28\n29. De la Rosa-Trev´ın, J., Ot´on, J., Marabini, R., Zald´ıvar, A., Vargas, J., Carazo,\nJ., Sorzano, C.: Xmipp 3.0: an improved software suite for image processing\nin electron microscopy. Journal of structural biology 184(2), 321–328 (2013).\nhttps://doi.org/10.1016/j.jsb.2013.09.015\n30. Rubinstein, M., Joulin, A., Kopf, J., Liu, C.: Unsupervised joint object discovery\nand segmentation in internet images. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. pp. 1939–1946 (2013)\nPreprint\n17\n31. Rublee, E., Rabaud, V., Konolige, K., Bradski, G.: Orb: An efficient alternative to\nsift or surf. In: 2011 International conference on computer vision. pp. 2564–2571.\nIeee (2011). https://doi.org/10.1109/ICCV.2011.6126544\n32. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556 (2014)\n33. Sloan, J.M., Goatman, K.A., Siebert, J.P.: Learning rigid image registration-\nutilizing convolutional neural networks for medical image registration (2018).\nhttps://doi.org/10.5220/0006543700890099\n34. Sorzano, C.O.S., Bilbao-Castro, J., Shkolnisky, Y., Alcorlo, M., Melero, R.,\nCaffarena-Fern´andez, G., Li, M., Xu, G., Marabini, R., Carazo, J.: A clus-\ntering approach to multireference alignment of single-particle projections in\nelectron microscopy. Journal of structural biology 171(2), 197–206 (2010).\nhttps://doi.org/10.1016/j.jsb.2010.03.011\n35. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30 (2017)\n36. Zeng, R., Denman, S., Sridharan, S., Fookes, C.: Rethinking planar homography\nestimation using perspective fields. In: Asian Conference on Computer Vision. pp.\n571–586. Springer (2018). https://doi.org/10.1007/978-3-030-20876-9 36\n37. Zeng, X., Xu, M.: Gum-net: Unsupervised geometric matching for fast and ac-\ncurate 3d subtomogram image alignment and averaging. In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. pp. 4073–4084\n(2020)\n38. Zhang, J., Wang, C., Liu, S., Jia, L., Ye, N., Wang, J., Zhou, J., Sun, J.: Content-\naware unsupervised deep homography estimation. In: European Conference on\nComputer Vision. pp. 653–669. Springer (2020). https://doi.org/10.1007/978-3-\n030-58452-8 38\n39. Zitova, B., Flusser, J.: Image registration methods: a survey. Image and vision com-\nputing 21(11), 977–1000 (2003). https://doi.org/10.1016/S0262-8856(03)00137-9\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-05-24",
  "updated": "2022-05-24"
}