{
  "id": "http://arxiv.org/abs/2205.12752v1",
  "title": "NECA: Network-Embedded Deep Representation Learning for Categorical Data",
  "authors": [
    "Xiaonan Gao",
    "Sen Wu",
    "Wenjun Zhou"
  ],
  "abstract": "We propose NECA, a deep representation learning method for categorical data.\nBuilt upon the foundations of network embedding and deep unsupervised\nrepresentation learning, NECA deeply embeds the intrinsic relationship among\nattribute values and explicitly expresses data objects with numeric vector\nrepresentations. Designed specifically for categorical data, NECA can support\nimportant downstream data mining tasks, such as clustering. Extensive\nexperimental analysis demonstrated the effectiveness of NECA.",
  "text": "NECA: NETWORK-EMBEDDED DEEP REPRESENTATION\nLEARNING FOR CATEGORICAL DATA\nXiaonan Gao\nUniversity of Science and Technology Beijing\ngaoxiaonan0001@163.com\nSen Wu\nUniversity of Science and Technology Beijing\nwusen@manage.ustb.edu.cn\nWenjun Zhou\nUniversity of Tennessee Knoxville\nwzhou4@utk.edu\nMay 26, 2022\nABSTRACT\nWe propose NECA, a deep representation learning method for categorical data. Built upon the\nfoundations of network embedding and deep unsupervised representation learning, NECA deeply\nembeds the intrinsic relationship among attribute values and explicitly expresses data objects with\nnumeric vector representations. Designed speciﬁcally for categorical data, NECA can support im-\nportant downstream data mining tasks, such as clustering. Extensive experimental analysis demon-\nstrated the effectiveness of NECA.\nKeywords unsupervised method, categorical attributes, network embedding\n1\nIntroduction\nCategorical attribute datasets (CADs), deﬁned as datasets where attributes are all categorical, are an important type of\nreal-world data. CADs are collected at an increasing scale from many application domains, such as healthcare, human\nresources, and the Web. More broadly speaking, many existing numerical and mixed-type featured datasets can also\nbe transformed into CADs. Therefore, general purpose methods are needed to provide a unifying framework of CADs\nto support their mining and analysis.\nCategorical attributes have the known characteristics of values being enumerated, non-differentiable, and undeﬁned\nalgebraic operations. As a result, CADs have seen signiﬁcant modeling challenges, since the proximity among data\nobjects is hard to measure. Many of the well-known data mining methods rely on effectively measuring the proximity\namong instances. For data with solely numerical attributes, calculating proximity is straight-forward, for example,\nby using the Euclidean distance. However, measuring the proximity among CADs has often been done using ad-\nhoc, heuristic methods. A systematic extension of CAD representation and proximity measurement has been an\nunderstudied area.\nAn example CAD in a talent recruitment context can be found in Table 1, where each record represents a job applicant,\nand the attributes (“Gender,” “Specialty,” and “Position”) are all categorical.1 To cluster job applicants with categorical\nattributes, a commonly used technique is to code the proximity between two instances by each categorical variable into\n1 if they are the same and 0 otherwise. Take the “Position” attribute as an example, with domain knowledge, humans\ncan understand that, compared with a “Lawyer,” a “Programmer” should have a closer proximity with a “Technician.”\nExiting practice of coarse encoding missed the opportunity of discovering valuable information hidden in CADs,\nresulting in less accurate results. How to fully mine the actual relationship between the categorical attribute values,\nespecially in an unsupervised way, has become a pressing need.\n1“Name” serves as an identiﬁer variable (i.e., unique per record) in this toy example and is not used for modeling.\narXiv:2205.12752v1  [cs.LG]  25 May 2022\nA PREPRINT - MAY 26, 2022\nTable 1: An example CAD used in talent analytics\nName\nGender\nSpecialty\nPosition\nJohn\nM\nEngineering\nProgrammer\nTony\nM\nScience\nAnalyst\nAlisa\nF\nLiberal Arts\nLawyer\nBen\nM\nEngineering\nProgrammer\nAbby\nF\nLiberal Arts\nMarketing\nJames\nM\nEngineering\nTechnician\nExisting related works on categorical data representation learning mainly include three categories: direct encoding-\nbased methods, similarity-based methods, and embedding-based methods. Direct encoding-based methods only take\ninto account the consistency and frequency relationship among categorical attribute values, ignore the latent semantic\nrelationship. Similarity-based methods output the similarity matrix among the categorical data objects instead of an\nexplicit representation of each data object. Thus, these methods can only be used for a subset of data mining techniques\n(i.e., those can use a similarity matrix as input). Embedding-based methods can overcome the limitations of the above\ntwo kinds to methods, but often relies on an external representation learning procedure, leading to susceptibility to the\nchoice of those algorithms and their parameter conﬁgurations. Additionally, these embedding-based methods often\nrequire external information (e.g., labels).\nTo develop a systematic way of CAD representation learning, this paper studies the unsupervised representation learn-\ning method of CAD, focusing on solving the aforementioned limitations in existing related works. We leverage the\nnetwork embedding to solve the problem of deep representation learning of CAD, and propose a network embed-\nding based categorical data deep representation learning method called NEtwork-based CAtegorical representation\nlearning (NECA). First, we convert the structured CAD in the form of a 2-dimensional table into a network form by\nconstructing a weighted heterogeneous network. The attribute values are represented as the network nodes, and the\nrelationships between the attribute values are expressed as edges, the strength of the relationship is reﬂected by the\nedge weight. Then, the relationship between the categorical attribute values is embedded into the deep numerical rep-\nresentation of node through network embedding. Finally, we can get the deep numerical representation of a categorical\ndata object by concatenating the learned representations of attribute values describing the data object. In this way, the\nactual meaning relationship hidden in CADs can be reﬂected in the learned representation while the aforementioned\nlimitations are not encountered. Extensive experiments on UCI datasets show that our NECA can effectively learn the\nimplicit correlation between different attribute values in the original CAD, and accurately represent it in the numerical\nrepresentation, which conforms to the real data distribution and can obtain better performance of downstream mining\ntask, like clustering.\nThe main contributions of this paper can be summarized as follows:\n• We introduce deep learning related theories into the research on unsupervised mining of structured CAD, and\ninnovatively use network embedding to solve the categorical data representation learning problem.\n• A categorical network construction method is presented, which introduces all the useful information hidden\nin CADs into a weighted heterogeneous network.\n• The intra-categorical attribute relationship and inter-categorical attribute relationship can be explored simul-\ntaneously through network embedding, which reveal the actual meaning hidden in real-world.\n• NECA is a unsupervised learning model, which does not rely on any external information and will not be\naffected by other modelling processes.\nThe remainder of this paper is organized as follows. Section 2 provides the problem formulation and an overview of\nNECA. Subsequently, details of the proposed NECA framework are discussed in two separate sections. In Section 3,\nwe present details about the heterogeneous network construction among intra- and inter-categorical attribute relation-\nships. In Section 4, we present the deep representation learning steps, which fuse the information learned from the\nprevious step. Section 5 summarizes the NECA algorithm. We present experimental results in Section 6 and discuss\nrelated work in Section 7. Finally, we draw conclusions in Section 8.\n2\nProblem Deﬁnition and Overview\nThis section ﬁrst provides preliminary information and the problem deﬁnition, and then offers an overview of our\nproposed solution.\n2\nA PREPRINT - MAY 26, 2022\n2.1\nPreliminaries and Problem Deﬁnition\nWe ﬁrst clarify a few basic concepts. A categorical attribute is one that can take values from a ﬁnite, discrete set known\nas its domain, where each value represents a category instead of a quantity. For instance, the domain of “Gender” could\nbe written as dom(Gender) = {M, F, other}. The attribute “Gender” can only take 1 of the 3 possible values in\nits domain. Numerical summaries such as means and percentiles are not applicable. In this case, we can say that\n“Gender” is a categorical attribute.\nNote that although categorical attributes can be further divided into nominal and ordinal attributes, in this paper, we\nlimit our scope to nominal variables only. An ordinal variable will be treated the same way as nominal variables by\nignoring the ordering information among attribute values. Similarly, numerical attributes may be preprocessed into\ncategorical versions. This makes our method more widely applicable as it does not require knowing the ordering\ninformation among attribute values. Formally, we have the following deﬁnition of a categorical attribute dataset.\nDeﬁnition 1 (CAD) A categorical attribute dataset (CAD) is a set of n data objects X = {x1, x2, . . . , xn}, where\neach data object is measured by a set of m categorical attributes C = {C1, C2, . . . , Cm}. We can write xi =\n(xi1, xi2, . . . , xim) as the i-th data object, where xij ∈dom(Cj), ∀i = 1, 2, . . . , n, ∀j = 1, 2, . . . , m.\nGiven a CAD, without a pre-deﬁned distance or similarity measure, it is impossible to quantify the proximity among\ncategorical data objects. Widely used data mining techniques, such as K-means clustering and principal component\nanalysis, therefore, cannot be readily applied to CADs. In this study, we propose a systematic way to learn numerical\nrepresentations of categorical data objects, such that the intrinsic relationships among the data objects are well captured\nto support downstream data mining.\nWith the goal of supporting downstream data mining tasks, the technical challenge of this representation learning\nproblem is to output the CAD’s numerical representation from the original categorical form. In essence, the task is to\nmine the implicit relationship among data objects in the CAD, to express all the useful information into the learned\nnumerical representation.\n2.2\nNECA: An Overview\nTo learn the effective numerical representations for CADs, in this paper, we propose a deep numerical representation\nlearning model for categorical data based on network embedding, called NECA. The basic idea of NECA is illustrated\nin Fig. 1 with a stylized example, which consists of two phases. In the ﬁrst phase, we construct a weighted heteroge-\nneous network to represent the two kinds of relationships among the categorical attribute values in CAD, one is the\nrelationship of values between different attributes, the other is the relationship within the same attribute. The attribute\nvalues are treated as nodes, and their relationships are expressed in the weighted edges. Details of this phase can be\nfound in Section 3. In the second phase, we learn the deep numerical representation of each attribute value based on the\nconstructed network through network embedding. In this way, the learned representation contains the potential useful\ninformation hidden in the original CAD. Along this line, we can get the deep numerical representations of categorical\ndata objects by concatenating the learned representations of attribute values. More details of this phase are discussed\nin Section 4.\n3\nNetwork Construction\nTo derive the deep numerical representations of CAD that capture the implicit relationship between attribute values in\nthe original feature space, we ﬁrst construct a weighted heterogeneous network using the attribute values as the nodes\nand their relationships as the weighted edges.\nDeﬁnition 2 (CAV Node) Suppose that categorical attribute Cj has domain dom(Cj) = {c1\nj, c2\nj, . . . , c|cj|\nj\n}, 1 ≤j ≤\nm. In other words, the categorical attribute Cj has |cj| unique values. When attribute Cj taking the k-th unique value,\nck\nj , we say that Cj = ck\nj makes a categorical attribute value (CAV) node, 1 ≤k ≤|cj|. We write the node as ck\nj for\nshort.\nDeﬁnition 3 (CAV Node Set) For a given CAD X, the CAV node set consists of all possible CAV nodes. Namely,\nV = {ck\nj |j = 1, 2, . . . , m; k = 1, 2, . . . , |cj|}.\n(1)\nThe total number of CAV nodes is |V | = P\nj |cj|.\n3\nA PREPRINT - MAY 26, 2022\nM \nF \nEngineering \nLiberal Arts \nScience \nGender\nSpecialty\nProgrammer \nTechnician\nLawyer\nAnalyst\nPosition\nMarketing\nWeighted Heterogeneous Network Based on CAD\nNetwork Embedding\nM \nF \nEngineering \nScience \nLiberal Arts \nProgrammer \nAnalyst\nLawyer\nMarketing\nTechnician\nDeep Numerical Representation of Categorical Attribute Values\nJohn\nAlisa\nAbby\nTony\nBen\nJames\nDeep Numerical Representation of Categorical Data Objects\nConcatenate\nFigure 1: The basic idea of NECA.\nOur goal is to construct the edges and formulate their weights to express relationship among the CAVs, which in turn\ncan support developing the embedding among CAD objects.\nWe consider two types of relationships: IntEr-Categorical Attribute Relationship (IECAR) and IntrA-Categorical At-\ntribute Relationship (IACAR). An intuitive example is shown in Fig. 2. The two green boxes in Fig. 2 demonstrate\ninter-attribute relationship between attribute-value pairs. In this example dataset, the gender of all instances whose spe-\ncialty is Engineering are Male, indicating that the relationship between “Engineering (Specialty)” and “M (Gender)” is\nstronger than between “Engineering (Specialty)” and “F (Gender).” In addition, “Engineering (Specialty)” and “Pro-\ngrammer (Position)” frequently co-occur, indicating that these two CAVs are closely related. For the intra-attribute\nrelationship within the same attribute, as labeled in red boxes in Fig. 2 for illustration, examines the relationship be-\ntween different values within the same attribute. The frequency of ”M (Gender)” is higher than that of ”F (Gender)”,\nindicating that the when ”F (Gender)” appears, more rare information will be provided. Similarly, in the relationship\n”Science (Specialty) - Liberal Arts (Specialty)”, there is more information in ”Science (Specialty)” than ”Liberal Arts\n(Specialty)”.\nFigure 2: Example CAD to demonstrate attribute-value relationship between and within attributes.\n4\nA PREPRINT - MAY 26, 2022\n(a) Gender vs. Specialty\n(b) Gender vs. Position\n(c) Specialty vs. Position\nFigure 3: Network diagrams illustrating IECAR.\nWe will construct the weighted heterogeneous network based on the IECAR and IACAR relationships respectively.\n3.1\nIECAR Network Construction\nBased on the CAD example in Fig. 2, Fig. 3 shows a schematic diagram of the network constructed based on the\nIECAR relationship. If two attribute values have a co-occurrence relationship in the dataset, they are connected by\nan edge, otherwise the two attribute values are boundlessly connected. The weight on an edge can be calculated by\nEqn. (2) and Eqn. (3).\nDeﬁnition 4 (IECAR Weights) The IECAR weight on edge e(cl\nj, cl′\nj′) is calculated as\nwinter(cl\nj, cl′\nj′) =\nexp{f(cl\nj, cl′\nj′)}\nP\ne(cl\nj,cl′\nj′)∈Einter exp{f(cl\nj, cl′\nj′)},\n(2)\nwhere\nf(cl\nj, cl′\nj′) =\n\f\f\f{xi|xij = cl\nj ∧xij′ = cl′\nj′}\n\f\f\f\n(3)\nrepresents the number of co-occurrences of attribute values cl\nj and cl′\nj′ in the dataset X.\nThe IECAR weights winter(cl\nj, cl′\nj′) represents the weight of the edge between cl\nj and cl′\nj′, which is normalized with\nthe softmax function [1].\nDeﬁnition 5 (IECAR Network) Given a CAD X, the weighted network constructed using IECAR can be expressed\nas\nGinter = (V, Einter),\n(4)\nwhere V is the CAV node set of X (see Def. 3) and Einter are the set of edges, whose weights are speciﬁed in Def. 4.\nThis weight calculation method can reﬂect the relationship between values belonging to different attributes in the\nnetwork. The greater the number of co-occurrences, the closer the relationship between attribute values should be, and\nthe greater the weight of the corresponding edge.\n3.2\nIACAR Network Construction\nThis subsection describes how we operationalize the network construction based on IACAR.\nAccording to the CAD example in Fig. 2, Fig. 4 shows a schematic diagram of the network based on the relationship\nwithin the categorical attribute. The value nodes in the attributes “Gender“, “Specialty” and “Position” are connected\nin pairs, represented by solid lines. In addition, to ensure the connectivity of the network, each node is randomly\nconnected to a value node in other attribute, which is represented by a dotted line. In order not to affect the accuracy\nof the relationship mining within the categorical attributes, these randomly added edges are given smaller weights.\n5\nA PREPRINT - MAY 26, 2022\nFigure 4: Network diagram illustrating IACAR.\nDeﬁnition 6 (IACAR Weights) The IACAR weight on edge e(cl\nj, cl′\nj′) is calculated as\nwintra(cl\nj, cl′\nj′) =\nexp{h(cl\nj, cl′\nj′)}\nP\ne(cl\nj,cl′\nj′)∈Eintra exp{h(cl\nj, cl′\nj′)}\n(5)\nwhere\nh(cl\nj, cl′\nj′) =\n\u001a\nn\ng(cl\nj)+g(cl′\nj ),\nif j = j′;\nβ,\notherwise.\n(6)\nwhere n is the number of instances in the CAD X, g(cl\nj) = |{xi|xij = cl\nj, 1 ≤i ≤n}| represents the number of\ninstances in X whose attribute Cj takes the value cl\nj, and β is a small positive coefﬁcient to ensure connectivity among\nthe CAV nodes.\nDeﬁnition 7 (IACAR Network) Given a CAD X, the weighted network constructed using IACAR can be expressed\nas\nGintra = (V, Eintra),\n(7)\nwhere V is the CAV node set of X and Eintra consists of the edges among the CAVs, whose IACAR weights are\nspeciﬁed in Def. 6.\n4\nRepresentation Learning\nBased on the weighted heterogeneous networks constructed for the CAD, as described in the previous section, we learn\nthe deep numerical representations that best capture the inter-attribute and intra-attribute relationships in the CAD.\n4.1\nWithin-Network Relationship Learning\nThe IACAR and IECAR learning steps follow a similar process. The main difference between the two learning tasks\nis that they are learned within the IACAR and IECAR networks, respectively.\nTake IECAR learning as an example, for a given CAV cl\nj, mining its relationship with other values in the different\ncategorical attributes is based on Ginter = (V, Einter). The basic idea is to explore the inﬂuence of neighbor nodes\nconnected to the target node cl\nj through Einter, and quantify this inﬂuence as the neighbor node’s contribution to the\ntarget node. The general process is as follows. First, we identify the neighbor node set for each CAV node. Then,\nbased on the relationships between each node (called the target node) and its neighbor nodes, we use the attention\nmechanism [2] to learn the contribution of the neighbor nodes on the target node. In this way, we can obtain the\nrepresentation of the target node within the IECAR network, which can reﬂect the IECAR relationship. Learning the\nrepresentation based on IACAR network is the same.\nDeﬁnition 8 (Neighbor Node Set) In a network G, for any given node v, its neighbor nodes make a neighbor node\nset, represented as ΩG(v) = {v′|weight(v, v′) > 0}.\n6\nA PREPRINT - MAY 26, 2022\nGiven the above deﬁnition, while learning IACAR relationship, the neighbor node set on the IACAR network is\nΩintra(v). Similarly, the neighbor node set on the IECAR network is Ωinter(v).\nTake IACAR network for example, the relationship between the target node cl\nj and its neighbor nodes Ωintra(cl\nj) is\nexplored using the attention mechanism. First, we map the nodes into the same feature space:\nhl\nj = W1 · cl\nj,\n(8)\nwhere W1 is used to map the node representing attribute value into the latent feature space, and hl\nj is the node cl\nj’s\nrepresentation in that space. After projecting both the target node cl\nj and its neighbor nodes cl′\nj ∈Ωintra(cl\nj) into the\nsame feature space, we can calculate the contribution of cl′\nj to cl\nj as:\nδ(cl\nj, cl′\nj ) = LeakyReLU(rintra · [hl\nj||hl′\nj ]),\n(9)\nwhere rintra and W1 are parameters to learn, the || operator concatenates the projected representations, LeakyReLU\n[3] is the activation function, which is used to introduce nonlinear factors and enhance the learning ability of the\nnetwork.\nUsing Eqn. (9) we can compute the contribution of all neighbor nodes in Ωintra(cl\nj) to target node cl\nj. Then, we use\nthe softmax function [1] to normalize them and obtain the weight as Eqn. (10).\nDeﬁnition 9 (Neighbor Node Weight) Suppose that the neighbor nodes of a target node cl\nj is cl′\nj ∈Ω(cl\nj). The\nimpact of the neighbor node cl′\nj on target node cl\nj can be calculated as\nα(cl\nj, cl′\nj ) =\nexp{δ(cl\nj, cl′\nj )}\nP\ncl′\nj ∈Ω(cl\nj) exp{δ(cl\nj, cl′\nj )},\n(10)\nwhere δ(cl\nj, cl′\nj ) can be found in Eqn. (9).\nIt is worth mentioning that the weights obtained using Eqn. (9) and Eqn. (10) are only dependent on the features of\nthe target node and its neighbor nodes themselves. Additionally, the mutual inﬂuence between any two nodes are\nasymmetric, which indicates that the weight from a neighbor node cl′\nj to the target node cl\nj is not always equal to\nthe weight from the target node cl\nj to the neighbor node cl′\nj . The computing result depends on the ordering in the ||\noperator and the normalization.\nStill take IACAR network as an example, after obtaining the weights of all neighbor nodes cl′\nj ∈Ωintra(cl\nj) on the\ntarget node cl\nj, we can compute the target node’s representation as follows:\nal\nj = σ\n\n\n\nX\ncl′\nj ∈Ωintra(cl\nj)\nα(cl\nj, cl′\nj ) · hl′\nj\n\n\n,\n(11)\nwhere σ is the activation function, which we speciﬁed as the ELU function [1].\nEqn. (11) indicates that the learned representation based on the intra-attribute relationship of the target node can\ncapture its neighbor nodes’ inherent characteristics and their contribution on the target node. In other words, al\nj can\nfully reﬂect the relationship between the target node and its neighbor nodes.\nSince Eqn. (9), Eqn. (10), and Eqn. (11) are all based on Eqn. (8) to characterize potential features of nodes, if the\nlearned feature space is not suitable for the IACAR relationship exploration, all subsequent calculation processes will\nbe affected. To solve this problem, the multi-head attention mechanism [2] is used to repeat the above operation\nK times, which means that the IACAR relationship can be learned in K different feature spaces, and K numerical\nrepresentations of each CAV is obtained in each space. Finally, the ﬁnal representation of the target node is calculated\nthrough the concatenation operation based on the K spaces:\nal\nj = ||K\nk=1{a(k)l\nj},\n(12)\nwhere a(k)l\nj is the IACAR representation of target node cl\nj learned in the kth space, which is calculated using Eqn. (11).\nSimilarly, from the IECAR network, we can obtain\nel\nj = ||K\nk=1{e(k)l\nj},\n(13)\n7\nA PREPRINT - MAY 26, 2022\nwhere e(k)l\nj is the IECAR representation of cl\nj in the kth space based on the IECAR network.\nConsequently, this subsection learns the deep numerical representations of nodes by mining the IACAR and IECAR\nrelationships in CAD. Speciﬁcally, with attention mechanisms, in K different feature spaces, the numerical represen-\ntation of target node is learned considering the relationship from its neighbors respectively, and the ﬁnal representation\nis obtained by concatenation operation.\n4.2\nBetween-Network Fusion\nAfter mining the IECAR and IACAR relationships, the two representations should be merged to learn the ﬁnal numer-\nical representation of CAV. The basic idea is to use the attention mechanism to learn the contribution of the IECAR\nand IACAR relationships on the CAV respectively, and then calculate the ﬁnal representation of the CAV based on the\nquantiﬁed contribution weights.\nFirst, we measure the importance of the IECAR and IACAR for the overall CAD. Based on the weighted heterogeneous\nnetwork G = (V, E), where E = {Einter, Eintra}, the attention mechanism is used to learn the importance of IECAR\nand IACAR for each CAV node. Then, by calculating the average values, we obtain the importance of IECAR and\nIACAR for the CAD X as follows:\nγinter =\n1\n|V |\nX\ncl\nj∈V\ns · tanh(W2 · el\nj + b),\n(14)\nγintra =\n1\n|V |\nX\ncl\nj∈V\ns · tanh(W2 · al\nj + b),\n(15)\nwhere γinter and γintra are calculated based on all CAVs reﬂecting the importance of IECAR and IACAR relationship\non the whole CAD dataset X, s, W2, and b are parameters to be learned. W2 and b will be used to map the IECAR\nand IACAR representations into the same feature space. s is used to guide the learning of the importance score.\nSubsequently, we normalize γinter and γintra as:\nβinter =\nexp{γinter}\nexp{γinter} + exp{γintra},\n(16)\nβintra =\nexp{γintra}\nexp{γinter} + exp{γintra},\n(17)\nwhere βinter and βintra respectively reﬂect the contribution weights of IECAR and IACAR on the CAD X.\nTo this end, we can fuse the two types of relationships to obtain the ﬁnal deep numerical representation of the target\nCAV cl\nj as:\nf l\nj = βinter · el\nj + βintra · al\nj.\n(18)\nBased on the numerical representations learned for CAVs, we can obtain the deep numerical representation of the\nCAD by concatenating the CAV representations [2].\nx′\ni =\n||\nxj\ni =cl\nj,1≤j≤m\nf l\nj.\n(19)\n4.3\nLearning Loss\nFor training the network, we hold the assumption that nodes with similar neighbors are similar to each other. Take a\nCAV node cl\nj and its neighbor node cl′\nj′ ∈Ωinter(cl\nj) for example, we ﬁrst deﬁne the impacting strength of cl′\nj′ on cl\nj\nas :\np(cl′\nj′|cl\nj) =\nwinter(cl\nj, cl′\nj′)\nP\ncl′′\nj′ ∈Ωinter(cl\nj)\nwinter(cl\nj, cl′′\nj′ ),\n(20)\nwhere the numerator winter(cl\nj, cl′\nj′) is the weight of the edge connecting between the target node cl\nj and its neighbor\nnode cl′\nj′ in the IECAR network, and the denominator is the sum of the edge weights between all neighbor nodes\nconnecting to the target node. To preserve the proximity, the similarity of the learned numerical representations for cl\nj\nand cl′\nj′ should be close to p(cl′\nj′|cl\nj).\n8\nA PREPRINT - MAY 26, 2022\nDeﬁnition 10 (NECA Learning Loss) Given the learned numerical representations of all CAVs, the NECA learning\nloss can be computed as\nLoss(W) = −\n1\n|Einter|\nX\ne(cl\nj,cl′\nj′)∈Einter\n˜l(cj\nj, cl′\nj′),\n(21)\nwhere\n˜l(cj\nj, cl′\nj′)\n=\np(cj′\nj′|cl\nj) log G(f l\nj, f l′\nj′) +\n(1 −p(cj′\nj′|cl\nj)) log(1 −G(f l\nj, f l′\nj′))\nand G(f l\nj, f l′\nj′) refers to the Gaussian kernel similarity between f l\nj and f l′\nj′.\nBy minimizing the loss and updating parameters using Adam [4], we can learn the deep numerical representations of\nCAVs, which contain the potential valuable information hidden in CAD.\n5\nThe NECA Algorithm\nNECA transforms the original CAD into a weighted heterogeneous network, and uses the idea of network embedding\nto fully mine the relationship between the CAVs in the dataset, including IECAR and IACAR. The learned represen-\ntations of CAVs are concatenated to produce the deep numerical representations of the CAD. This section summarizes\nthe detailed steps of our NECA algorithm.\n5.1\nAlgorithm Description\nThe speciﬁc steps of NECA is summarized in Algorithm 1.\nAlgorithm 1: NEtwork-based CAtegorical representation learning (NECA)\nInput : X: An input CAD as deﬁned in Def. 1.\nOutput: X′ = {x′\n1, x′\n2, . . . , x′\nn}: numerical representations\nParam : K: The number of heads in the multi-head attention mechanism (default: 8);\n// Initialize the IECAR and IACAR networks\n1 G ←ConstrucHetNet(X);\n// Learn numerical representations of CAVs\n2 while the training stop condition is not satisﬁed do\n// Within-network relationship learning\n3\nA′ ←DeepLearnCAV (Gintra);\n4\nE′ ←DeepLearnCAV (Ginter);\n// Between-network fusion\n5\nCalculate the contribution weights of IECAR and IACAR relationship on X: βinter, βintra;\n6\nFuse A′ and E′ to obtain the deep numerical representation F = {f 1\n1 , f 2\n1 , . . . , f |cm|\nm\n} of all CAVs;\n7\nCalculate the loss function: Loss;\n8\nUpdate the parameters in NECA;\n9 end\n// Learn numerical representations of CAD\n10 Obtain the deep numerical representation x′\ni of data object xi, by concatenating the learned representations of\nCAVs\n11 return X′\nNECA ﬁrst constructs a weighted heterogeneous network based on the IECAR and IACAR relationships, where CAVs\nmake the nodes, and the relationships between different CAV nodes make the edges, whose weights are to be formu-\nlated (Line 1).\nThen, using the idea of network embedding, we learn the deep numerical representation of each node in the network\n(Lines 2 to 9). This representation can reﬂect the topological relationship in the network, and mine the relationship\nbetween the categorical attributes and the relationship within the attributes, which means that the learned representation\nof a CAV contains all the relations related to other CAVs. By concatenating the learned representations of CAVs\ncontained in a certain data object, the deep numerical representation of the data object can be obtained (Line 10).\nSpeciﬁcally, the training stop condition can be that the epoch is larger than a speciﬁed value, or the loss is convergent.\n9\nA PREPRINT - MAY 26, 2022\nProcedure ConstrucHetNet(X)\n// Create the node set of network\n1 V ←CAV s;\n// Create the IECAR network\n2 for cl\nj and cl′\nj′ ∈V , j ̸= j′ do\n3\nif there is xi ∈X, xj\ni = cl\nj and xj′\ni = cl′\nj′ then\n4\nAdd edge einter(cl\nj, cl′\nj′);\n5\nCalculate edge weight winter(cl\nj, cl′\nj′);\n6\nend\n7 end\n8 Normalize the weights in IECAR network;\n// Create the IACAR network\n9 for cl\nj and cl′\nj ∈V , l ̸= l′ do\n10\nAdd edge eintra(cl\nj, cl′\nj );\n11\nCalculate edge weight wintra(cl\nj, cl′\nj );\n12 end\n13 for cl\nj ∈V do\n14\nRandomly select a node from other attribute: cl′\nj′, j′ ̸= j;\n15\nAdd edge eintra(cl\nj, cl′\nj′) and calculate wintra(cl\nj, cl′\nj′);\n16 end\n17 Normalize the weights in IACAR network;\n18 return G = {V, E}, E = {Einter, Eintra}\nProcedure DeepLearnCAV(G)\n1 Initialize the numerical representations of all nodes cl\nj, 1 ≤j ≤m, 1 ≤l ≤|cj|, through One-hot encoding;\n// Construct the K heads\n2 for cl\nj ∈V do\n3\nfor k ∈{1, . . . , K} do\n4\nFind out the neighbor node set of cl\nj in the G: Ω(cl\nj) ∈{Ωinter(cl\nj), Ωintra(cl\nj)};\n5\nfor cl′\nj ∈Ω(cl\nj) do\n6\nCalculate the contribution of cl′\nj to cl\nj: α(cl\nj, cl′\nj );\n7\nend\n8\nCalculate the numerical representation v(k)l\nj ∈{a(k)l\nj, e(k)l\nj} of cl\nj in this head;\n9\nend\n10\nObtain the numerical representation of cl\nj: vl\nj = ||K\nk=1{v(k)l\nj};\n11 end\n12 V ′ = {vl\nj, 1 ≤j ≤m, 1 ≤l ≤|cj|};\n13 return V ′\n5.2\nCharacteristics of NECA\nThe NECA algorithm has the following characteristics.\n(1) The deep numerical representation learning process of CAD is only based on the dataset, and does not need to use\nexternal information such as class labels. It belongs to unsupervised mining and is suitable for the common unlabeled\nCAD in real-world.\n(2) The IECAR and IACAR relationships between CAVs can be fully mined and expressed in the learned deep numer-\nical representation, based on the idea of network embedding.\n(3 The learned representation is independent to other learning procedures and not impacted by the parameter settings\nof other learning tasks.\n10\nA PREPRINT - MAY 26, 2022\n(4) To the best of our knowledge, this is the ﬁrst time to learn the numerical representations of CAD using the basic\nidea of network embedding, which provides a new way to solve the representation learning problem of CAD.\n6\nExperimental Results\nIn this section, we ﬁrst introduce the experimental setup and then present and discuss the results.\n6.1\nExperimental Setup\nThis subsection provides the basic information of our experimental setup, including datasets, experimental design,\nparameter setting, platform and implementation.\nDatasets. We use 11 publicly available CADs from the UCI Machine Learning Repository [5] for experimental\nanalysis. Summary statistics of each dataset are shown in Table 2. The datasets cover a wide range of sample sizes\n(#Cases), dimensionality (#Attr.), and number of subgroups (#Classes). In their original version, there were missing\nvalues in the BC, DE, MA, MU, and PT datasets, which were imputed using attribute-speciﬁc modes.\nTable 2: Experimental Datasets\nName\n#Cases\n#Attr.\n#Classes\nBreast Cancer (BC)\n286\n9\n2\nCar Evaluation (CE)\n1,728\n6\n4\nDermatology (DE)\n366\n35\n6\nLymphography (LY)\n148\n18\n4\nMammographic (MA)\n961\n5\n2\nMushroom (MU)\n5,644\n22\n2\nPrimary Tumor (PT)\n339\n17\n17\nSoybean Small (SB)\n47\n35\n4\nSpect Heart Train (SH)\n80\n22\n2\nWisconsin (WI)\n699\n9\n2\nZoo (ZO)\n101\n16\n7\nBenchmark Methods. Our benchmark methods include representation learning methods and network embedding\nmethods. The representation learning methods are\n• Frequency-based (FQ) encoding [6];\n• One-hot (OH) encoding [7];\n• Coupled data embedding (CDE) [8].\nIn speciﬁc, FQ and OH are the commonly used categorical embedding models. CDE is the newly proposed cate-\ngorical data representation learning algorithm in recent years, whose superiority over other models has been proved.\nTherefore, we believe that the experiments based on these baselines can validate the effectiveness of NECA.\nThe network embedding benchmarks are based on DeepWalk [9] and Node2vec [10]. Since these methods are designed\nfor homogeneous networks, we adapt them for the IACAR and IECAR networds, respectively.\n• The DeepWalk embedding based on IACAR (DWa);\n• The DeepWalk embedding based on IECAR (DWe);\n• The Node2vec embedding based on IACAR (NVa);\n• The Node2vec embedding based on IECAR (NVe).\nParameter Setting. The number of heads in the multi-attention mechanism is set as 8. Regarding the benchmark\nmethods, there is no need to set parameters in OH and FQ. The CDE needs to set parameter α, which is used to\ndetermine the number of clusters, and parameter β, which is used for dimension reduction. In our experiments, we set\nα = 10, β = 10−10 following the default parameters in the original publication [8].\nPlatform and Implementation. The experiments are conducted on a Windows Server 64-bit system (4-CPU, each\nwith 2.5GHz with Quad-Core, and 16G main memory). All algorithms are implemented in Python.\n11\nA PREPRINT - MAY 26, 2022\nTable 3: Internal Validation Results by CH\nFQ\nOH\nCDE\nDWa\nDWe\nNVa\nNVe\nNECA\nBC\n7.59\n6.95\n9.63\n10.3\n5.12\n6.72\n8.74\n9.16\nCE\n30.3\n32.0\n18.7\n39.9\n10.9\n17.9\n26.6\n46.8\nDE\n30.5\n33.0\n77.23\n35.0\n73.5\n31.8\n73.1\n45.2\nLY\n6.31\n5.98\n7.41\n7.84\n5.18\n5.58\n6.05\n8.66\nMA\n81.4\n126.\n247.\n103.\n105.\n204.\n100.\n44.9\nMU\n761.\n885.\n1269.\n982.\n675.\n1040.\n978.\n1396.\nPT\n4.99\n5.03\n5.94\n4.79\n4.33\n4.78\n5.63\n5.03\nSB\n14.5\n16.1\n36.4\n14.5\n17.0\n21.4\n20.2\n60.4\nSH\n6.91\n6.98\n6.99\n6.76\n6.72\n5.76\n6.38\n7.53\nWI\n217.\n164.\n460.\n160.\n210.\n160.\n188.\n263.\nZO\n31.1\n31.5\n36.8\n28.0\n33.1\n30.6\n40.8\n55.6\nTable 4: Internal Validation Results by S\nFQ\nOH\nCDE\nDWa\nDWe\nNVa\nNVe\nNECA\nBC\n0.04\n0.04\n0.06\n0.06\n0.04\n0.04\n0.06\n0.05\nCE\n-0.01\n-0.00\n-0.09\n-0.05\n0.01\n-0.10\n0.00\n-0.01\nDE\n0.08\n0.10\n0.19\n0.11\n0.19\n0.09\n0.18\n0.09\nLY\n0.06\n0.06\n0.06\n0.06\n0.03\n0.05\n0.03\n0.07\nMA\n0.08\n0.12\n0.21\n0.10\n0.12\n0.18\n0.11\n0.05\nMU\n0.14\n0.15\n0.21\n0.18\n0.13\n0.17\n0.17\n0.23\nPT\n-0.09\n-0.08\n-0.08\n-0.21\n-0.20\n-0.15\n-0.19\n-0.09\nSB\n0.26\n0.29\n0.44\n0.26\n0.30\n0.32\n0.31\n0.47\nSH\n0.09\n0.09\n0.09\n0.09\n0.09\n0.08\n0.09\n0.10\nWI\n0.29\n0.25\n0.42\n0.25\n0.27\n0.24\n0.29\n0.28\nZO\n0.38\n0.37\n0.41\n0.33\n0.36\n0.38\n0.42\n0.46\n6.2\nResults\nIn this subsection, we directly verify the accuracy of the NECA algorithm and compare its performance against bench-\nmark models.\nThe internal cluster validity indices are used to quantitatively evaluate the effectiveness of the learned numerical\nrepresentations, by measuring the compactness of intra-cluster and separation of inter-clusters. If two data objects\nbelong to the same class, their learned representations should be similar, otherwise, the learned representations are\nexpected to be dissimilar.\nCalinski-Harabasz index (CH) [11] and Silhouette index (S) [12], are used to evaluate the representation learning\nperformance in this paper. Speciﬁcally, the CH index is calculated as follows:\nCH(π) =\n1\nT −1\nPT\ni=1 |Ci|d2(ci, c)\n1\nn−T\nPT\nj=1\nP|Cj|\ng=1 d2(xg\ncj, cj)\n,\n(22)\nwhere n is the number of data objects, T is the number of classes, |Ci| is the number of data objects in the i-th class,\nci is the centroid of the i-th class, c is the global center of the entire dataset, xg\ncj is the g-th data object in the j-th class.\nThe numerator of Eqn. (22) is used to measure the degree of separation between classes, and the denominator is used\nto measure the closeness within classes. Greater CH value indicates better performance.\nAdditionally, the S index is calculated as follows:\nS(π) = 1\nT\nT\nX\ni=1\n \n1\n|Ci|\nCi\nX\ng=1\nb(xg\ncj) −a(xg\ncj)\nmax{b(xg\ncj), a(xg\ncj)}\n!\n,\n(23)\nwhere a(xg\nCi) and b(xg\nCi) are used to respectively reﬂect the compactness of intra-cluster and separation of inter-\nclusters of the data object xg\nCi [12]. The larger the S value, the more accurate the representation learning, which\n12\nA PREPRINT - MAY 26, 2022\nmeans the learned numerical representations of two data objects within the same class are adjacent and those belong\nto different classes are nonadjacent.\nThe CH and S indices of different representation learning models on the 11 datasets are reported in Table 3 and Table 4,\nrespectively. Each comparative models are conducted ﬁve times on their optimal parameter conﬁgurations. And we\nreport the best result of each model on each dataset, where the best performance is shown in bold, and the next best is\nshown in italics and underlined.\nTable 3 indicates that the deep representations learned by NECA show the superior performance on the CH index to\nbenchmark models on six datasets, and the second best on one dataset. This indicates that, when evaluated with the\nCH index, our proposed NECA algorithm can effectively capture the potential useful information hidden in CADs, and\nexpress it in the numerical representations. Additionally, CDE had the best performance on the four datasets, which\nmeans tha CDE is a fairly competitive model, only inferior to our proposed NECA.\nTable 4 shows the evaluation results on the S index. It should be noted that both CDE and our proposed NECA perform\nwell, they produce the optimal learning results on ﬁve datasets, respectively, which means from the perspective of S,\nthe capability of NECA and CDE to learn the numerical representation of categorical data is close, and our NECA\ncan effectively handle the situations that CDE cannot deal with. Additionally, the performance of other comparative\nbaselines is relatively inferior.\nIn summary, compared with the existing categorical data representation learning models and network embedding\nmodels not specially used for categorical data representation learning, the NECA algorithm proposed in this paper can\nlearn the most accurate deep numerical representations of CAD, which contain the useful information hidden in the\noriginal dataset, and are in line with the actual class distribution of the dataset.\n7\nRelated Work\nThe existing related studies can be divided into two subgroups: direct encoding methods and embedding-based meth-\nods.\nDirect encoding methods for CAD refers to the numerical symbolization of categorical attribute values. There are three\ncommonly used direct encoding methods: numerical encoding, frequency-based encoding, and one-hot encoding.\nNumerical encoding assigns a different number to each unique category [13]. The exact numbering scale can be\napplication dependent. For example, one can choose {0, 1, 2}, or {10, 20, and 25}, as long as a different number is\nused to represent a different attribute value. This method creates numerical representation for CAD in a subjective\nmanner. It requires domain insights and experimentation to ﬁnd the most suitable numbering scale. Frequency-based\nencoding assigns a numerical value to a categorical attribute value based on the inverse of its frequency in the dataset\n[6]. For example, suppose that there are 100 records of job candidates data, in which ”Engineering (Specialty)”\nappeared 40 times, then the frequency-based encoding for ”Engineering (Specialty)” is designated as log(100/40).\nThis type of encoding assigns smaller values for very common categories and larger values for rare categories, which\nis consistent with the information theory [14]. Frequency-based encoding can be used for both ordinal and nominal\nattributes, but since it only considers frequency, it does not well differentiate categories with similar frequencies or\nreﬂect the actual relationship between different categories. One-hot encoding is the most commonly used method for\nencoding CAD. It converts a categorical attribute into a set of binary attributes, where each corresponds to one possible\ncategory [7]. For any given data object, the categorical attribute then becomes a vector where only one entry (which\ncorresponds to the binary attribute that represents that category) is 1, and all other entries are 0. Although one-hot\nencoding is highly effective and ﬂexible for automatic encoding, it ignores the relationship between the attribute values\nand causes sparse problem.\nThe embedding-based methods refer to incorporating a categorical data object into a low-dimensional continuous\nspace to obtain a numerical representation that is similar to numerical data. There are several methods proposed\nin recent literatures. CDE [8] is a superior embedding-based method for categorical data representation learning.\nIt uses metric learning [15] to dig into the relationship between CAVs, and introduces an clustering algorithm in\nthe representation learning process. The CDE method was further improved into CDE++ [16], which uses mutual\ninformation and marginal entropy to obtain the relationship between CAVs. It proposes a hybrid clustering strategy\nto capture the values of different types of categorical attributes clusters, then an automatic encoder (autoEncoder)\n[1] is employed to obtain a dense low-dimensional vector representation. The UNTIE method [17] is a shallow\nmodel, using frequency to represent the relationship between different CAVs, and the co-occurrence relationship is\nreﬂected by conditional probabilities, then the K-means algorithm is used to mine the heterogeneity between these\nrelationships and obtain the embedding-based representations of CAD. Existing works on embedding-based methods\nconsider the relationship between the categorical attributes and CAVs, which has advantages compared with the direct\n13\nA PREPRINT - MAY 26, 2022\nencoding methods. However, they have the following limitations. Firstly, the clustering process is introduced into the\nrepresentation learning, and the learning result is affected by the selection of clustering algorithms and the clustering\nparameters. Secondly, the representation learning process requires the use of external information, such as data labels,\nwhich is not suitable for unlabeled data that is often faced in real-world.\n8\nConclusion\nIn this paper, we propose a novel categorical data representation learning algorithm, NECA, a deep representation\nlearning method for categorical data based on network embedding. Built upon the foundations of network embedding\nand deep unsupervised representation learning, NECA ﬁrstly constructs a weighted heterogeneous network based\non the inter-categorical attribute relationship and intra-categorical attribute relationship. Next, the deep numerical\nrepresentation of each categorical attribute value is learned by leveraging the basic idea of network embedding. Finally,\nthe numerical representation of categorical data object can be produced through the concatenation operation. In this\nway, NECA deeply embeds the intrinsic relationships among categorical attribute values, and can explicitly express\ncategorical data objects with the learned numerical representations, which contain the potential useful information\nhidden in the original dataset. Extensive experiments on 11 UCI datasets demonstrate that NECA outperforms several\nstate-of-the-art benchmark models.\nReferences\n[1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. Massachusetts:The MIT Press, 2016.\n[2] Davide Bacciu, Federico Errica, Alessio Micheli, and Marco Podda. A gentle introduction to deep learning for\ngraphs. Neural Networks, 129:203–221, 2019.\n[3] M.V. Sowmya Lakshmi, P. Lalitha Saisreeja, L. Chandana, P. Mounika, and Prabu. U. A leakyrelu based effective\nbrain mri segmentation using u-net. 2021 5th International Conference on Trends in Electronics and Informatics\n(ICOEI), pages 1251–1256, 2021.\n[4] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015.\n[5] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.\n[6] Akiko Aizawa. An information theoretic perspective of TF-IDF measures. Information Processing & Manage-\nment, 39(1):45–65, 2003.\n[7] Weinan Zhang, Tianming Du, and Jun Wang. Deep learning over multi-ﬁeld categorical data: A case study on\nuser response prediction. In European Conference on Information Retrieval, pages 45–57, 2016.\n[8] Songlei Jian, Guansong Pang, Longbing Cao, Kai Lu, and Hang Gao. Cure: Flexible categorical data represen-\ntation by hierarchical coupling learning. IEEE Transactions on Knowledge and Data Engineering, 14(8):1–14,\n2018.\n[9] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. Pro-\nceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 2014.\n[10] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. Proceedings of the 22nd\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.\n[11] Tadeusz Cali´nski and J Harabasz. A dendrite method for cluster analysis. Communications in Statistics, 3(1):1–\n27, 1974.\n[12] Peter J. Rousseeuw. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal\nof Computational and Applied Mathematics, 20:53–65, 1987.\n[13] Jiawei Han, Micheline Kamber, and Jian Pei. 3 - data preprocessing. In Jiawei Han, Micheline Kamber, and\nJian Pei, editors, Data Mining Concepts and Techniques, The Morgan Kaufmann Series in Data Management\nSystems, pages 83–124. Morgan Kaufmann, Boston, third edition edition, 2012.\n[14] James V. Stone. Information theory: a tutorial introduction. 2015.\n[15] Xinxing Xu, Wen Li, and Dong Xu. Distance metric learning using privileged information for face veriﬁcation\nand person re identiﬁcation. IEEE transactions on neural networks and learning systems, 26(12):3150–3162,\n2015.\n[16] Bin Dong, Songlei Jian, and Ke Zuo. Cde++: Learning categorical data embedding by enhancing heterogeneous\nfeature value coupling relationships. Entropy, 22(4):391, 2020.\n14\nA PREPRINT - MAY 26, 2022\n[17] Chengzhang Zhu, Longbing Cao, and Jianping Yin. Unsupervised heterogeneous coupling learning for categor-\nical representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(8):1–16, 2015.\n15\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-05-25",
  "updated": "2022-05-25"
}