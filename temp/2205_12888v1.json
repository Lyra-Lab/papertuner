{
  "id": "http://arxiv.org/abs/2205.12888v1",
  "title": "Robust Reinforcement Learning on Graphs for Logistics optimization",
  "authors": [
    "Zangir Iklassov",
    "Dmitrii Medvedev"
  ],
  "abstract": "Logistics optimization nowadays is becoming one of the hottest areas in the\nAI community. In the past year, significant advancements in the domain were\nachieved by representing the problem in a form of graph. Another promising area\nof research was to apply reinforcement learning algorithms to the above task.\nIn our work, we made advantage of using both approaches and apply reinforcement\nlearning on a graph. To do that, we have analyzed the most recent results in\nboth fields and selected SOTA algorithms both from graph neural networks and\nreinforcement learning. Then, we combined selected models on the problem of\nAMOD systems optimization for the transportation network of New York city. Our\nteam compared three algorithms - GAT, Pro-CNN and PTDNet - to bring to the fore\nthe important nodes on a graph representation. Finally, we achieved SOTA\nresults on AMOD systems optimization problem employing PTDNet with GNN and\ntraining them in reinforcement fashion.\n  Keywords: Graph Neural Network (GNN), Logistics optimization, Reinforcement\nLearning",
  "text": "Robust Reinforcement Learning on Graphs for\nLogistics optimization\nZangir Iklassov\nMBZUAI\nPhD in Machine Learning\n20020082@mbzuai.ac.ae\nDmitrii Medvedev\nMBZUAI\nMSc in Machine Learning\n20020035@mbzuai.ac.ae\nAbstract\nLogistics optimization nowadays is becoming one of the hottest areas in the AI\ncommunity. In the past year, signiﬁcant advancements in the domain were achieved\nby representing the problem in a form of graph. Another promising area of research\nwas to apply reinforcement learning algorithms to the above task. In our work,\nwe made advantage of using both approaches and apply reinforcement learning\non a graph. To do that, we have analyzed the most recent results in both ﬁelds\nand selected SOTA algorithms both from graph neural networks and reinforcement\nlearning. Then, we combined selected models on the problem of AMOD systems\noptimization for the transportation network of New York city. Our team compared\nthree algorithms - GAT, Pro-CNN and PTDNet - to bring to the fore the important\nnodes on a graph representation. Finally, we achieved SOTA results on AMOD\nsystems optimization problem employing PTDNet with GNN and training them in\nreinforcement fashion.\nKeywords: Graph Neural Network (GNN), Logistics optimization, Reinforcement\nLearning\n1\nIntroduction\nFor a long time, the problem of logistics optimization has used classical brute force algorithms or\nMPC. These algorithms are inefﬁcient in time, but accurate. According to the research of McKinsey\n[8], logistics is one of the industrial areas in which machine learning is expected to make one of the\ngreatest progresses and can save huge costs in the next decade. For this, machine learning algorithms\nshall become more accurate than classical algorithms, while being already more time-efﬁcient, since\nthey solve the problem in polynomial time, in contrast to exponential time of classical algorithms.\nOne of the potential directions in this ﬁeld is reinforcement learning. It has already shown striking\nresults for strategic tasks such as chess, Go, Atari [6] [7].\nSince logistics is also a strategic optimization problem, the potential for using reinforcement learning\nfor it is quite promising. To do this, RL should show robust results on different settings, on data of\ndifferent types (homogeneous and heterogeneous) and different sizes (city, state, global scale). We\nconcentrate on the second problem, where it is necessary to develop an algorithm which can build\na logistic plan of a high quality independently of a given scale. Since the problem can be perfectly\nrepresented in the form of a graph, this shall also allow using the same model on the data of different\nscale, we are aiming to use a combination of RL and GNN to verify the robustness of the model on\nlogistics (AMOD) problem. For now, our team has already analyzed the existing literature on this\ntopic, reimplemented SOTA models on different datasets and checked their robustness on different\ngraph sizes of New York transport environment.\nWe are planning to use such SOTA models as GAT, Pro-GNN and PTDNet to improve data size\nrobustness of RL on AMOD problem. To the best of our knowledge, there was no research conducted\nbefore in this direction.\narXiv:2205.12888v1  [cs.LG]  25 May 2022\n2\nBackground\nReinforcement learning is one of the methods of Machine Learning in which the model does not\nhave any information about the system but has an option to interact with it and learn the policy from\nthis experience. To formalize the reinforcement learning process we shall refer to Markov Decision\nProcess (MDP) M which can be described as a function of a space S of system’s observed states\ns ∈S, actions a ∈A available to the agent in that space, conditional probability P(st+1|st, at) of\nbeing in state st+1 upon taking action at at the state st, initial distribution d0 (starting point), reward\nr to identify the goodness of the steps which agent takes, and a discount factor 0 < γ ≤1:\nM = (S, A, P, d0, r, γ)\nThe ultimate goal of reinforcement learning is to derive the distribution of optimal actions to take in\neach state, also called policy π(at|st). In order to do that, the agent interacts with MDP following\ninitial policy which is to be adjusted by the cycle: observing states st, taking actions at at that state\nand being rewarded or punished by rt depending on the newly observed state st+1.\nGraph Neural Networks (GNNs)are special type of neural network which directly works with a graph\nstructure. Graph is a data structure G which consists of a set of vertices (nodes) V and edges E\nconnecting them:\nG = (V, E)\nIt is often very convenient to represent one’s data as a graph structure when performing an image\nanalysis task. This conveniences is directly related to graph’s property of permutation invariance,\nmeaning that an order of vertices is not affecting the output when performing calculations on them.\nPermutation invariance dramatically decreases the number of training examples required to generalize\nthe model. However for traditional machine learning techniques one ﬁrst need to represent graph-\nstructured data into numerical vectors or other types of data structures, and such representation\nmay lead to partial loss of data. GNN allows to work with graph data directly avoiding the above\nmentioned inconvenience. The core idea of GNN is based on a propagation of information on a\ngraph. The latter is processed by a set of modules which are interconnected in accordance with the\ngraph’s edges and also linked with the vertices. While training, these modules update their states and\nexchange information until they reach an equilibrium. The mechanism to propagate information is\nlimited to make sure the state of equilibrium exists. The output of GNN is calculated based on the\nstate of the module at each vertex.\nOn of the types of GNN is Graph Convolutional Network (GCNs) used for image classiﬁcation.\nGCNs use the following function to propagate information on a graph:\nX′ = f(X, A) = σ( ˆD−1\n2 ˆA ˆD−1\n2 XW) , ˆA = A + I\nWhere I is an identity matrix, ˆD is a diagonal node degree matrix of ˆA, σ is a non-linear activation\nfunction and W is a matrix of weights.\n3\nLiterature Review\nIn the ﬁeld of logistics optimization, the reinforcement learning approach is steadily gaining popularity\namong other machine learning techniques for its promising results. In 2017 Jian Wen et al. in [9]\ntackled rebalancing needs of AMOD systems. The team employed RL to be able to manage on large\nsystems where data might be not fully available. As the result, the computational speed was 2.5 times\nfaster versus classical models used in logistics while the performance of the model demonstrated\nnear-optimal solution behavior. As the result, RL applications was considered a success for the task\nproviding beneﬁts for both users and operators.\nIn [1] Daniele Gammelli et al. presented the ﬁrst work that combined reinforcement learning and\ngraph neural network on AMOD systems. They represented the transportation network as graph with\nareas of the city as vertices and connectivity between those areas as edges. RL algorithm was applied\non a graph to manage the rebalancing of AMOD systems. The team showed that GNN provides the\n2\nbasis for reinforcement learning agent to outperform classical models in transferability, generalization\nand ability to scale when solving logistics optimization task. Some of the achieved results of the\nwork are shown in Table 1. We can see that the algorithm proposed in [1] demonstrated close-to-\noptimal performance and generate more than 36% cost savings versus learning-based approaches of\nalgorithms with classical architectures.\nTable 1: SYSTEM PERFORMANCE ON NEW YORK 4 × 4 NETWORK\nModel\nReward\nDemand\nRebalancing\n(%Dev. MPC-tri-level)\nServed\nCost ($)\nED\n30,746 (-10.7%)\n8,770\n7,990\nCQL\n30,496 (-11.4%)\n8,736\n8,284\nA2C-MLP\n30,664 (-10.9%)\n8,773\n7,920\nA2C-CNN\n30,443 (-11.5%)\n8,904\n8,775\nA2C-GNN (Daniele Gammelli et al.)\n33,886 (-1.6%)\n8,772\n5,038\nMPC-tri-level\n34,416 (0%)\n8,865\n4,647\nMPC-standard\n35,356 (2.7%)\n8,968\n4,296\nA2C-GNN-0Shot\n33,397 (-3.0%)\n8,628\n4,743\nThe most recent work in the ﬁeld of reinforcement learning for logistics solutions is done by Yimo\net al. in [10]. The paper provides comprehensive overview of RL methods and classiﬁes previous\nstudies in the ﬁeld. Among the strengths, the team highlights the ability of RL to learn from historical\ndata for prediction purposes, as well as provide forecasting and optimisation approaches for stochastic\ntasks in logistics. At the same time reinforcement learning faces issues when dealing with complex\nmulti-agent systems, however this area of logistics does not concern MOD, rather facilities settings\netc.\n4\nMethodology\nFor sparse graphs A2C-GNN shows SOTA results for Autonomous Mobility-on-Demand problem.\nDeviation from MPC solution tend to reach zero value for small grid dimensions [1]. This means that\nfor a graph with small number of connections for each node, A2C-GNN reaches the ideal NP-hard\nsolution faster than other algorithms. However, the same study shows that as grid dimensions increase,\nthe difference with the ideal solution grows. That is, the less sparse the input graph, the worse the\nresults of the A2C-GNN algorithm we have. This trend will not allow the algorithm to be used in the\nindustry for large scale problems. It is important to make an algorithm robust to increase in graph\ncomplexity.\nThe ideal solution to the AMOD problem is a set of vehicles and assigned routes which is NP-hard\ntask that can be ideally solved by MPC-tri-level algorithm. For each vehicle-node pair we choose only\none node at a time as a partial route, while other nodes are meaningless at this iteration. However,\nwhen we use GNN algorithm we average information from all connected nodes. And the more\nconnected nodes we have, the less meaningful becomes an information from every separate node,\nwhich makes it more difﬁcult to ﬁnd an ideal solution. We assume this is the reason why we are\nmoving away from the ideal solution when we have more complex input graph. And to solve the\nproblem, we should focus only on the most important nodes and consider only information coming\nfrom them, making averaged embeddings more meaningful for solving an NP-hard problem.\nTo do this, we will consider three methods that allow us to emphasize the importance of certain nodes\nand zero out the information of others. The ﬁrst algorithm is well-known Graph attention network\n[2], which ﬁnds contextual weights for each node, thus changing the contribution of each node to\nthe calculation of the new embedding. The second algorithm is Pro-GNN [3] which optimizes the\nadjacency matrix making it more sparse and low rank for more robust GNN computations. The third\nalgorithm is PTDNet [4] which uses two neural networks, one to select the sparser subgraph and the\nother one, GNN, to process the subgraph making the ﬁnal prediction.\n4.1\nGAT\nGNN recomputes feature vector of each node h given the formula:\n3\n⃗h′\ni = σ\n\n1\nK\nK\nX\nk=1\nX\nj∈Ni\nαk\nijWk⃗hj\n\n\nWhere a is an attention value which measures ‘importance’ of each adjacent node for a new feature\nvector, and can be expressed using the following three formulas:\neij = a\n\u0010\nW⃗hi, W⃗hj\n\u0011\nαij = softmaxj (eij) =\nexp (eij)\nP\nk∈Ni exp (eik)\nαij =\nexp\n\u0010\nLeakyReLU\n\u0010−→a T h\nW⃗hi∥W⃗hj\ni\u0011\u0011\nP\nk∈Ni exp\n\u0010\nLeakyReLU\n\u0010−→a T\nh\nW⃗hi∥W⃗hk\ni\u0011\u0011\nHere W is a weight matrix to be trained and both i and j represent indices of two different nodes.\nAttention weight a is in range [0, 1] so the higher it is, the more important corresponding adjacent\nnode becomes.\n4.2\nPro-GNN\nmin\nθ\nLGNN (θ, A, X, yL) =\nX\nvi∈VL\nℓ(fθ(X, A)i, yi)\nBy default, we optimize the above function, where function f is an output of GNN model with\nadjacency matrix A, feature input X and weight matrix W to be trained.\nfθ(X, A) = softmax\n\u0010\nˆAσ\n\u0010\nˆAXW1\n\u0011\nW2\n\u0011\nFrom now on we assume that A is over complicated, it is having inﬂated rank for efﬁcient GNN use,\nand we shall replace it with another matrix S which we get from:\narg min\nS∈S\nL0 = ∥A −S∥2\nF + α∥S∥1 + β∥S∥∗, s.t., S = S⊤\nHere we initialize S = A, then we optimize S by making it sparser and downgrade the rank by\nminimizing its l1 and nuclear norms, while preserving it being symmetric and keeping it close to\ninitial A as much as possible.\n4.3\nPTDNet\nFinding output Y , given graph G, can be formulated in terms of several subgraphs g:\nP(Y | G) ≈\nX\ng∈SG\nP(Y | g)P(g | G)\nThen, we can approximate second part using two different neural networks:\nX\ng∈SG\nP(Y | g)P(g | G) ≈\nX\ng∈SG\nQθ(Y | g)Qφ(g | G)\nFirst network aims to select certain edges from initial adjacency matrix A forming subgraph g.\nSecond network, given subgraph g, predicts output Y . Two networks can be trained together, given\nG and Y ; the only thing to consider is to make ﬁrst part differentiable. For this purpose we will use\n4\nGumbel-Softmax function [5] that helps us to generate differentiable samples from a set of all the\nedges.\nAll three algorithms choose certain nodes over others emphasizing the importance of the formers,\nwhich can lead to more representative feature vectors of the nodes in terms of ﬁnding optimal routes\nand combinations for AMOD problem. It is possible to combine the above methods and study whether\nwe can sum-up their effects, and this is to be one of the plans for our future research. However, in\nthis work we will try ﬁrstly to check separate effects of the those methods in order to understand the\ncontribution of each algorithm to the AMOD problem.\n5\nResults\nThe environment used for both training and testing is the Manhattan district represented in k x k grids\nmap, where each grid is one of the New York City stations. Firstly, we create a graph of these grids\nand connect adjacent stations. Secondly, we run a simulation in which a daily demand for commute\nis synthesized. The model has a number of vehicles at its disposal which it has to assign to meet\na demand and assign routes in order to maximize daily income. We can represent Manhattan in a\ndifferent number of stations as a k x k grid. The larger k we have, the more complex graph we will\nbuild.\nOn the input, the model receives a graph in which nodes are represented as stations and edges are\nconnections between those stations. Each edge stores the information about the cost of moving\nfrom one node to another and the number of passengers who want to move this path. Each node\nstores information about the number of vehicles available in that station. The matrix of edges A and\nmatrix of features X are fed to the Graph neural network that has one layer of graph convolution\nwith ReLU activation function followed by 3 fully-connected layers with 32 neurons each. Actor and\nCritic networks have the same architecture with the only difference in output layer. Actor produces\nactions represented as a percentage distribution of vehicles to be rebalanced for each node at a given\ntime step, that is predicting value in the range [0, 1] for each station. Critic aggregates information\nfrom all nodes using sum-pool function and predicts one value for the whole graph, where reward is\nrepresented as a proﬁt at each time step.\nFor GAT network we replaced GCN layer with Graph Attention layer, for Pro-GNN network we added\noptimization of adjacency matrix described in Section 4.2. Finally, for PTDNet we built additional\nsampling network that consists of several MLP layers and for every edge predicts probability of\nleaving it in A, changing A that will be used then in GCN.\nTo train the models we employed PyTorch library in Python and built RL model. The model was\ntrained using the Adam optimizer with 0.003 learning rate and 0.97 discount factor, for 16,000\nepisodes. To evaluate performance, we used demand, expressing it in total amount of executed trips,\ncost, expressing it in the expenses for all movements, and deviation from MPC-tri-level, which is an\nideal solution for this problem, but unusable for a large real-life values of k.\nTable 2: Performance on New York 4 × 4 Network\nModel\nReward\nDemand\nRebalancing\n(%Dev. MPC-standard)\nServed\nCost ($)\nA2C-GNN\n33,384 (-3%)\n8,699\n5,036\nA2C-GAT\n33,280 (-3,3%)\n8,569\n5,049\nA2C-Pro-GNN\n33,246 (-3,4%)\n8,664\n5,018\nA2C-PTDNet\n33,418 (-2,9%)\n8,709\n5,031\nThe tables 1, 2 show the performance of all models for 4x4 and 20x20 grids’ environments. In the\nﬁrst case, the differences between the models are insigniﬁcant, all performance metrices vary around\nthe same values. However, in the second case, the difference rose sharply, with all three new models\nshowing a better result than the baseline and the best result belongs to A2C-PTDNet with the smallest\ndeviation of 30.1%, the largest demand and the smallest cost of 42,291 and 27,019 values respectively.\nThe larger the value of k we choose, the higher the difference between the models’ performance we\nhave. It can be seen in Figure 1.\n5\nTable 3: Performance on New York 20 × 20 Network\nModel\nReward\nDemand\nRebalancing\n(%Dev. MPC-standard)\nServed\nCost ($)\nA2C-GNN\n131,701 (39)\n39,825\n28,151\nA2C-GAT\n130,288 (37.6)\n39,235\n28,219\nA2C-Pro-GNN\n134,152 (31.7)\n41,694\n27,324\nA2C-PTDNet\n138,8365 (30.1)\n42,291\n27,019\nFigure 1: Performance of models trained on single granularity (4 × 4).\n6\nDiscussion\nThe results show an improvement in model’s performance. However, the reason for this may\nbe that the model, in all three cases, has become more complex in comparison with the base\nmodel as a whole and not more robust to the size of the setting in particular. In addition to the\nabove, the negative relationship between the quality of prediction and the size of the setting still\nremained quite high. For the ﬁnal solution of this problem, this dependence should be close to zero.\nHowever, we have shown that focusing on ‘most important’ nodes during calculations can improve\nthe performance. Moreover, discrete selection of nodes works better than their weighting or adjacency\nmatrix continuous transformation. Thus, indeed, an increase in the average number of connected\nnodes for large graphs negatively affects learning, due to the greater blurring of information received\nfrom each node. In the future research, it can be effective to focus on a discrete selection of nodes for\nnetwork performance improvement, as well as to test other techniques of node selection and nodes\ncombinations, and to test the models on other logistics tasks besides AMOD.\n7\nConclusion\nWe got new SOTA results on the New York dataset of AMOD problem using PTDNet with GNN\narchitecture trained in reinforcement fashion. However, it is necessary to further improve the result\nin order to obtain a robust result no matter of setting size and the closest possible solution to MPC-\ntri-level (ideal). In this case, the model will be beneﬁcial for use in the industry, and can become in\ndemand and save large costs for logistics tasks. In general, this direction is potentially beneﬁcial and\ninteresting from both theoretical and practical points of view.\n6\nReferences\n[1] Gammelli, D., Yang, K., Harrison, J., Rodrigues, F., Pereira, F. C., & Pavone, M. (2021). Graph\nNeural Network Reinforcement Learning for Autonomous Mobility-on-Demand Systems. arXiv\npreprint arXiv:2104.11434.\n[2] Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., & Bengio, Y. (2017). Graph\nattention networks. arXiv preprint arXiv:1710.10903.\n[3] Jin, W., Ma, Y., Liu, X., Tang, X., Wang, S., & Tang, J. (2020, August). Graph structure learning\nfor robust graph neural networks. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining (pp. 66-74).\n[4] Luo, D., Cheng, W., Yu, W., Zong, B., Ni, J., Chen, H., & Zhang, X. (2021, March). Learning to\ndrop: Robust graph neural network via topological denoising. In Proceedings of the 14th ACM\nInternational Conference on Web Search and Data Mining (pp. 779-787).\n[5] Jang, E., Gu, S., & Poole, B. (2016). Categorical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144.\n[6] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M.\n(2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.\n[7] Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., ... & Hassabis, D.\n(2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through\nself-play. Science, 362(6419), 1140-1144.\n[8] Baer, T., & Kamalnath, V. (2017). Controlling machine-learning algorithms and their biases.\nMcKinsey Insights.\n[9] Wen, J., Zhao, J., & Jaillet, P. (2017, October). Rebalancing shared mobility-on-demand systems:\nA reinforcement learning approach. In 2017 IEEE 20th International Conference on Intelligent\nTransportation Systems (ITSC) (pp. 220-225). Ieee.\n[10] Yan, Y., Chow, A. H., Ho, C. P., Kuo, Y. H., Wu, Q., & Ying, C. (2021). Reinforcement\nLearning for Logistics and Supply Chain Management: Methodologies, State of the Art, and\nFuture Opportunities. State of the Art, and Future Opportunities (October 4, 2021).\n7\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-05-25",
  "updated": "2022-05-25"
}