{
  "id": "http://arxiv.org/abs/2205.13528v3",
  "title": "SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning",
  "authors": [
    "Marco Bagatella",
    "Sammy Christen",
    "Otmar Hilliges"
  ],
  "abstract": "Efficient exploration is a crucial challenge in deep reinforcement learning.\nSeveral methods, such as behavioral priors, are able to leverage offline data\nin order to efficiently accelerate reinforcement learning on complex tasks.\nHowever, if the task at hand deviates excessively from the demonstrated task,\nthe effectiveness of such methods is limited. In our work, we propose to learn\nfeatures from offline data that are shared by a more diverse range of tasks,\nsuch as correlation between actions and directedness. Therefore, we introduce\nstate-free priors, which directly model temporal consistency in demonstrated\ntrajectories, and are capable of driving exploration in complex tasks, even\nwhen trained on data collected on simpler tasks. Furthermore, we introduce a\nnovel integration scheme for action priors in off-policy reinforcement learning\nby dynamically sampling actions from a probabilistic mixture of policy and\naction prior. We compare our approach against strong baselines and provide\nempirical evidence that it can accelerate reinforcement learning in\nlong-horizon continuous control tasks under sparse reward settings.",
  "text": "SFP: State-free Priors for Exploration\nin Oﬀ-Policy Reinforcement Learning\nMarco Bagatella\nmarco.bagatella@inf.ethz.ch\nDepartment of Computer Science\nETH Zürich\nSammy Christen\nsammy.christen@inf.ethz.ch\nDepartment of Computer Science\nETH Zürich\nOtmar Hilliges\notmar.hilliges@inf.ethz.ch\nDepartment of Computer Science\nETH Zürich\nAbstract\nEﬃcient exploration is a crucial challenge in deep reinforcement learning. Several methods,\nsuch as behavioral priors, are able to leverage oﬄine data in order to eﬃciently accelerate\nreinforcement learning on complex tasks. However, if the task at hand deviates excessively\nfrom the demonstrated task, the eﬀectiveness of such methods is limited. In our work, we\npropose to learn features from oﬄine data that are shared by a more diverse range of tasks,\nsuch as correlation between actions and directedness. Therefore, we introduce state-free\npriors, which directly model temporal consistency in demonstrated trajectories, and are\ncapable of driving exploration in complex tasks, even when trained on data collected on\nsimpler tasks. Furthermore, we introduce a novel integration scheme for action priors in oﬀ-\npolicy reinforcement learning by dynamically sampling actions from a probabilistic mixture\nof policy and action prior. We compare our approach against strong baselines and provide\nempirical evidence that it can accelerate reinforcement learning in long-horizon continuous\ncontrol tasks under sparse reward settings.\n1\nIntroduction\nExploration is a fundamental issue in reinforcement learning (RL): in order for an agent to maximize its\nreward signal, it needs to adequately cover its state space and observe the outcome of its actions. This\nbecomes increasingly diﬃcult when dealing with large, continuous state and action spaces, as is the case in\nmany real world applications. Despite a large and fruitful body of research on exploration (Bellemare et al.,\n2016; Osband et al., 2016; Tang et al., 2017; Osband et al., 2018; Azizzadenesheli et al., 2018; Burda et al.,\n2019; Dabney et al., 2021; Ecoﬀet et al., 2021; Vulin et al., 2021), most general-purpose algorithms remain\nbased on ϵ-greedy exploration (Mnih et al., 2015) or entropy-regularized Gaussian policies (Haarnoja et al.,\n2018). In the absence of an informative reward signal, both methods rely on uniformly sampling actions\nfrom the action space, independently of the history of the agent. Unfortunately, in sparse reward settings,\nachieving positive returns by uncorrelated exploration becomes exponentially less likely as the horizon length\nincreases (Dabney et al., 2021).\nA promising approach to achieve eﬃcient exploration is that of using a behavioral prior to guide the policy\n(Pertsch et al., 2020; Tirumala et al., 2020; Singh et al., 2021).\nTypically, this is learned from expert\ntrajectories as a state-conditional action distribution. Behavioral priors are able to foster directed exploration\n1\narXiv:2205.13528v3  [cs.LG]  31 Aug 2022\nFigure 1: SFP: A state-free prior is trained on weakly informative trajectories, such as reaching random goals in\na reaching task (top left). Actions are then sampled from a dynamic mixture between the state-free prior and the\npolicy in downstream learning of more complex tasks. Our method works with both vector-based and image-based\nstate inputs.\n(Singh et al., 2021), by assuming a strong similarity between the agent and expert tasks. However, an agent\nshould ideally be able to produce eﬃcient explorative behaviors across a broad range of diverse tasks, even\nif they are rather diﬀerent from those demonstrated (Parisi et al., 2021). Similarly, it should be possible to\nleverage information collected on simpler tasks to enable learning in more complex scenarios (Florensa et al.,\n2017; Gehring et al., 2021).\nLet us, for instance, consider a surveying robot that was trained from scratch in a simple room, and needs to\nbe redeployed to a complex construction site: even if observations in the second environments are drastically\ndiﬀerent from those previously received (e.g.\ndue to diﬀerent lighting conditions and terrain patterns),\nrelevant information could still be recovered from past experiences. We argue that such information often\nincludes correlation and directedness in the robot’s behavior. In particular, these characteristics of the robot\nbehavior are not necessarily related in their entirety to the state space it operates in.\nIn this paper, we thus propose to focus on the temporal structure of demonstrations rather than on task-\nspeciﬁc strategies. We ﬁnd that this choice enables knowledge transfer from simple oﬄine trajectories to\na diverse family of more complex tasks. Our method, which we dub State-free Priors for exploration in\nOﬀ-policy Reinforcement Learning (SFP), introduces state-free priors as state-independent non-Markovian\naction distributions ¯π(at|at−1\n0\n), modeling promising actions conditioned on past actions. We ﬁnd that this\nclass of priors is suﬃcient for capturing desirable properties for exploration, such as directedness and temporal\ncorrelation. This information is often learnable from few expert trajectories collected in simple environments\n(e.g. reaching uniformly sampled end-eﬀector positions for a robotic arm, see Figure 1). While we assume\nthat such trajectories display qualities such as correlation and directedness, and in particular reﬂect good\nexploration strategies for the task at hand, we do not require them to explicitly encode state-action de-\npendencies speciﬁc to downstream tasks. State-free priors can then be used to guide exploration in more\ncomplex tasks, in which observed states are not guaranteed to belong to the expert state distribution, and\nbehavioral priors are, as a result, inherently limited.\nFurthermore, we propose a principled manner of integrating action priors into the Soft Actor Critic framework\n(Haarnoja et al., 2018). In downstream learning of more complex tasks, our method samples actions from\na dynamic mixture between the policy and state-free prior. Crucially, we derive an update rule for learning\nthe mixing coeﬃcient by directly optimizing a (max-entropy) RL objective. Our dynamic integration allows\nthe policy to receive strong guidance when needed, while crucially retaining the ability to explore diverse\nactions when the prior is no longer beneﬁcial.\nIn our experiments, we ﬁrst analyze the choice of conditioning variables for action priors by measuring their\neﬀectiveness in accelerating downstream RL. We then focus on state-free priors and verify their capability\nto produce correlated and directed behavior. Most importantly, we provide empirical evidence that SFP can\nleverage weakly informative trajectories to accelerate learning. For this purpose, our method is evaluated\n2\nagainst several baselines in complex long-horizon control tasks with sparse rewards. Finally, we present\nfurther applications of SFP, including leveraging oﬄine state-based trajectories to (a) accelerate visual RL\nor (b) guide exploration when the task is corrupted by biased observations.\nOur contributions can be organized as follows:\n1. We propose learnable non-Markovian action priors conditioned on past actions.\nWe show that\nsampling from these priors produces directed and correlated trajectories.\n2. We introduce a principled manner of integrating action priors into the Soft Actor Critic framework\n(Haarnoja et al., 2018).\n3. We show how state-free priors can be learned from few expert trajectories on simple tasks (e.g.\nreaching) and used to improve exploration eﬃciency in more complex, unseen tasks (e.g. opening a\nwindow), even across fundamentally diﬀerent settings (e.g., from non-visual to visual RL).\nAfter discussing our method’s novelty and related literature in Section 2, we introduce our setting in Section\n3. The method is described in Section 4, while empirical evidence of its eﬀectiveness is reported in Section\n5. Finally, Section 6 contains a brief closing discussion of our work. Code and video are available on our\nproject page1.\n2\nRelated Work\nWhile we limit this section to essential topics, further discussion can be found in Appendix I.\nTemporally-Extended Exploration\nSeveral works have attempted to directly address the inability of\ntraditional methods, such as ϵ-greedy or uniform action sampling (Lillicrap et al., 2016; Haarnoja et al.,\n2018), to produce correlated trajectories. A recent study (Dabney et al., 2021) highlights this issue and\nshows how repeating random actions for multiple steps is suﬃcient to signiﬁcantly accelerate Rainbow\n(Hessel et al., 2018) on the Atari benchmark (Bellemare et al., 2013). Similarly, Amin et al. (2021) propose\na non-learned policy inspired by the theory of freely-rotating chains in polymer physics to collect initial\nexplorative trajectories in continuous control tasks. Both methods pinpoint a fundamental issue, but rely\non scripted policies which are hand-crafted for a particular family of environments. On the other hand, our\nprior is learned, and does not require engineering an explorer, which can be unfeasible for complex tasks.\nA similar approach was previously proposed by Bogdanovic & Righetti (2019), who also leverage simple\ntrajectories to accelerate learning in new tasks by designing a learned exploration model (LEP) conditioned\non a sequence of recent states. Our method further explores this direction by removing the assumption of\na shared state space between tasks, and enables knowledge transfer to a wider range of tasks. Moreover,\ncompared to previous works, our integration of priors into oﬀ-policy RL is not limited to initial trajectories\nand dynamically adjusts the likelihood of sampling actions from the prior.\nHierarchical Reinforcement Learning\nAnother approach to tackle exploration-hard tasks is to rely on a\nhierarchical decomposition into diﬀerent levels of temporal and functional abstraction (Parr & Russell, 1998;\nDietterich, 2000; Sutton et al., 1999; Dayan & Hinton, 2000). For instance, tasks can be decomposed into high\nlevel planning and a set of low-level policies, often referred to as skills (Konidaris & Barto, 2007; Eysenbach\net al., 2019) or options (Sutton et al., 1999; Bacon et al., 2017). This eﬀectively reduces the planning horizon\nand allows eﬃcient solving of complex tasks (Bacon et al., 2017; Vezhnevets et al., 2017; Nachum et al., 2018;\nLevy et al., 2019; Christen et al., 2021). In general, such approaches only target temporal correlation within\nskills, as each skill is generally selected independently from the last one. Incidentally, our method is not\ndesigned to achieve temporal abstraction, but can be interpreted in a hierarchical framework (Schäfer et al.,\n2021) in which a high-level criterion (the mixing function) governs a probabilistic choice between an explorer\n(state-free prior) and an exploiter (policy). Our method’s application is also inherently related to the works\nof Florensa et al. (2017); Gehring et al. (2021), which propose to extract skills on simple training tasks and\n1https://eth-ait.github.io/sfp/\n3\ndeploy them in more complex scenarios. In contrast with these methods, training a state-free prior does not\nrequire access to the training environment, but only to oﬄine trajectories.\nBehavioral Priors\nBehavioral priors generally represent state-conditional action distributions, modeling\npromising actions for the current state of the environment. Such priors can be learned jointly with the\npolicy in the context of KL-regularized RL (Tirumala et al., 2019; 2020). An important line of work deals\nwith information asymmetry (Galashov et al., 2019; Salter et al., 2022), which consists in restricting the\ninformation available to the prior. As a result, learned priors can more easily generalize to diﬀerent settings,\nor be trained jointly to accelerate policy learning. In this context, Tirumala et al. (2020) brieﬂy mention the\npossibility to condition behavioral priors exclusively on a vector of previous actions. Our work can be seen\nas a broad empirical exploration in this direction, coupled with an integration scheme that is better suited to\nthis class of priors. A second approach consists in learning behavioral priors from expert policies on related\ntasks. This is the case for several works (Peng et al., 2019; Pertsch et al., 2020; 2021; Ajay et al., 2021)\nwhich adopt a Gaussian behavioral prior in a latent skill-space. In particular, Pertsch et al. (2020) report\nthat a prior is crucial to guiding a high-level actor in an HRL framework. An important contribution to the\nﬁeld is made by PARROT (Singh et al., 2021), which focuses on a visual setup and introduces a ﬂow-based\ntransformation of the action space to allow arbitrarily complex prior distributions. We extend this idea\nto prior action distributions that are not conditioned on the current state or a part thereof, but rather on\npast actions, and are therefore non-Markovian. Moreover, we propose a novel, more ﬂexible integration of\nthe prior distribution into the learning algorithm. Finally, we overcome the reliance on a strong similarity\nbetween states observed by the expert and the agent.\n3\nBackground\n3.1\nSetting\nReinforcement learning (RL) is the problem that an agent faces when learning to interact with a dynamic\nenvironment. Albeit with a slightly diﬀerent deﬁnition, we formalize the environment as a goal-conditioned\nMarkov Decision Process (gc-MDP) (Nasiriany et al., 2019), that consists of a 6-tuple (S, A, G, R, T , γ),\nwhere S is the state space, A is the action space, G ⊆S is the goal space, R : S × G →R is a scalar\nreward function, T : S × A →Π(S) a probabilistic transition function that maps state-action pairs to\ndistributions over S and, ﬁnally, γ is a discount factor. Assuming goals to be drawn from a distribution pG,\nthe objective of an RL agent can then be expressed over a time horizon T as ﬁnding a probabilistic policy\nπ⋆= arg maxπ Eg∼pG\nPt=T\nt=0 γtR(st, g), with st ∼T (st−1, at−1) and at−1 ∼π(st−1, g). In order to simplify\nnotation, from this point on, we will implicitly include the goal into the state at each time step: st ←(st, g).\nWe focus on long-horizon control problems with continuous state and action spaces and sparse rewards,\ni.e., non-zero only after task completion. Although our method can be generally applied to oﬀ-policy RL\nmethods, we build upon Soft Actor Critic (Haarnoja et al., 2018), due to its wide adoption in these settings.\nFinally, in contrast with several behavioral prior approaches (Galashov et al., 2019; Pertsch et al., 2020;\nSingh et al., 2021), we adopt a more general and challenging setting. First, we do not assume that prior\ninformation on the structure of the state space is available. Second, while we also assume access to a collection\nof trajectories Dexp = {(si\n0, ai\n0, si\n1, ai\n1, . . . , si\nN, ai\nN)}L\ni=0, we do not require trajectories to be collected on a task\nthat is similar to the one at hand. From this point on, we refer to the task used for collecting data as the\ntraining task, and to the task the RL agent has to complete as the downstream task.\n3.2\nBehavioral Priors\nA behavioral prior ¯π(a|s) (Pertsch et al., 2020; Singh et al., 2021) is a state-conditional probability distri-\nbution over the action space (cf. Figure 2a). Behavioral priors can be trained to assign high probability to\nuseful actions with respect to the current state, and hence be used to accelerate RL. A behavioral prior can\nonly guide the policy eﬀectively as long as prior information on the structure of the state space is available\n(Galashov et al., 2019), or the prior has been trained on data collected on a closely related task (Pertsch\net al., 2020; Singh et al., 2021).\n4\nIn our settings, the structure of observations is unknown and expert trajectories Dexp are weakly informative\nor collected on simple tasks. As a consequence, the distribution of training states might not match the\ndistribution of states produced by downstream tasks: behavioral priors will then be evaluated on out-of-\ndistribution samples and their performance will degrade drastically, as shown in Section 5.1.\n4\nMethod\nOur method relies on the integration of a learned state-free prior into an oﬀ-policy RL algorithm. Thus, we\nﬁrst deﬁne and discuss the class of action priors of interest, and later describe how they can be integrated\ninto existing oﬀ-policy algorithms.\n4.1\nState-free Priors\nWithin the settings outlined in Section 3.1, it is still possible to extract and transfer knowledge from the\nexpert dataset Dexp to the agent. In this case, conditioning an action prior on the current state might be\nboth insuﬃcient and counterproductive, as a state-conditional action prior would receive out-of-distribution\nsamples as inputs, due to the distribution shift between states from the training and downstream task. For\nthis reason, we shift our focus towards modeling the temporal correlation of expert trajectories.\nIn order to recover this information, we thus propose to learn a state-free prior. A state-free prior is a\nnon-Markovian, state-independent action prior, representing a probability distribution over the action space\nthat is conditioned on past actions: ¯π(at|at−1\n0\n) (see Figure 2b, c). By renouncing the informativeness of\nstates with respect to expert actions, state-free priors retain the ability to model temporal correlation, which\ncrucially enables knowledge transfer to tasks with a fundamentally diﬀerent state distribution with respect\nto demonstrations. This is indeed a viable strategy in a hard-exploration setting, when no prior information\nis available on the state space and no reward is observed: in this case, extracting any information from the\nstate remains challenging. Our empirical evidence suggests that the simplest form of state-free priors, i.e.,\n¯π(at|at−1) (see Figure 2c) is surprisingly competitive with variants conditioned on multiple past actions (cf.\nSection 5.1) and is suﬃcient to capture non-trivial temporal relations.\nIndependently of the conditioning variables, state-free priors can conveniently be modeled as conditional\ngenerative models and trained through empirical risk minimization.\nFor the purpose of this paper, we\nchoose to use the conditional variant of the Real Non Value Preserving Flow (Dinh et al., 2017; Ardizzone\net al., 2019), which is well suited for Euclidean action spaces (Singh et al., 2021). Moreover, our integration\nin the SAC framework allows arbitrarily complex prior distributions, which Real NVP Flows are in principle\nable to capture.\nIn the context of Real NVP Flows, training samples are actions a ∈A, paired with conditioning variables\nat−1\n0\n, thus the learned mapping is a = fθ(z; at−1\n0\n), with z ∼N. Since fθ is invertible, it is possible to\nanalytically compute the likelihood of a single training pair (at, at−1\n0\n) and maximize its expected value\nthrough standard gradient-based optimization techniques. An empirical justiﬁcation of this choice is found\nin Appendix B, while implementation details are reported in Appendix K. For a complete introduction to\nReal NVP Flows, we refer the reader to Dinh et al. (2017).\nFigure 2: Graphical models of diﬀerent priors: from left\nto right, a behavioral prior, a general state-free prior and\na single-step state-free prior.\nSolid arrows represent the\nenvironment’s transition function T , while dashed arrow\nindicate conditional modeling.\nOne ﬁnal concern regards the nature of the data\nfor training the state-free prior. The main require-\nments for the training data are two: (1) the task\non which the data is collected needs to share the\nsame action space of the downstream task and (2)\nthe training trajectories should display the desired\nqualities of correlation and directness. In general,\nwe adopt weakly informative expert trajectories gen-\nerated by achieving random goals in simple environ-\nments.\nSuch simple trajectories can be produced\nfrom scratch using standard RL or generated by a\nscripted policy. We remark that, in contrast with\n5\nexisting approaches (Pertsch et al., 2020; Singh et al., 2021), this framework poses very weak requirements\non the similarity between the task used for data collection and the target task. As we show in Section\n5.4, this allows our method to transfer knowledge across diﬀerent tasks and settings, such as from a simple\nreaching task with access to the true state of the system to a window-closing task in a visual RL setting.\nLimitations of our method with respect to training data are discussed in Section 6.\n4.2\nSoft Actor Critic with SFP\nThe main challenge introduced by state-free priors stems from their non-Markovianity, which renders existing\nintegrations schemes unsuitable Singh et al. (2021); Tirumala et al. (2020) (see Section 5.2 for an empirical\nevaluation and a broader discussion).\nFor this reason, we introduce a novel method to integrate action\npriors in an oﬀ-policy RL framework. While our method can be formally derived for Markovian priors, it\ncan also be generalized to non-Markovian action distributions by allowing further bias in Q-value estimates.\nIn the following derivations, we will consider a general state-dependent non-Markovian prior ¯π(at|Ht) with\nHt = (st\n0, at−1\n0\n), but the integration remains applicable to state-free or behavioral priors alike (for briefness,\nwe will interchangeably use the compact notation ¯π(at)). The key strategy consists of sampling actions from\na dynamically weighted mixture between the policy and a ﬁxed prior distribution. We demonstrate it as an\nintegration into the SAC framework.\nAlgorithm 1 SAC with SFP\n1: Train action prior ¯π(at|Ht)\n2: Initialize parameters θ, φ, ω\n3: for each iteration do\n4:\nObserve s0\n5:\nInitialize history H0 = (s0)\n6:\nfor each environment step do\n7:\nλt = Λω(st)\n8:\nat ∼˜π = (1 −λt)π(at|st) + λt¯π(at|Ht)\n9:\nst+1 ∼T (st, at)\n10:\nHt+1 = Ht ∪(at, st+1)\n11:\nend for\n12:\nfor each gradient step do\n13:\nUpdate θ, φ, ω\n14:\nend for\n15: end for\nSAC is an oﬀ-policy actor critic algorithm that trains a\nstochastic policy πφ and state-action value function esti-\nmator Qθ in an oﬀ-policy way; its objective is designed to\npursue rewards while maximizing the entropy of its policy.\nWhen prior knowledge on the structure of the environment\nor task is available, simply sampling actions from a maxi-\nmal entropy policy πφ may not be optimal. On the other\nhand, blindly sampling from a ﬁxed action prior ¯π prevents\nexploitation of reward signals as well as any behavior which\nis not encoded in the prior. Ideally, it is desirable to con-\ntrol the degree to which actions are sampled from the prior.\nWe propose to achieve this in a natural way by sampling\nactions from a mixture ˜π between the policy πφ and the\nprior ¯π:\nat ∼˜π(·|st) = (1 −λt)πφ(·|st) + λt¯π(·),\n(1)\nwhere the mixing parameter λt is bounded (0 ≤λt ≤1)\nand computed dynamically at each step t.\nIn principle, it is desirable to bias the mixture ˜π towards the prior ¯π or the policy π, depending on which\nof the two is more likely to reach the goal.\nInterestingly, if the mixing weight is computed through a\nparameterized function of states λt = Λω(st), this behavior can be naturally recovered by maximizing the\nmaximum entropy objective with respect to the mixture ˜π:\narg max\nπφ,Λω\nE\nτ∼˜π\n\u0014 ∞\nX\nt=0\nγt\n\u0012\nR(st, at) + αH(πφ(·|st))\n\u0013\u0015\n.\n(2)\nWe note that, while this expectation is computed over trajectories sampled from the mixture ˜π, the entropy\nterm H(πφ(·|st)) is computed on the policy πφ alone, as it is not desirable to incentivize exploration of states\naccording to the prior ¯π’s entropy.\nThrough straightforward derivations (see Appendix A), it is possible to derive learning objectives for the\nQ-estimator Q˜π\nθ and for the policy πφ, as well as for a mixing function Λω. Given a distribution D of observed\ntransitions, the loss functions for the Q-estimator can be deﬁned as:\nJQ˜π\nθ =\nE\n(s,a,s′)∼D\n\u0014\u0000Q˜π\nθ (s, a) −yt(s, a, s′)\n\u00012\n\u0015\n,\n(3)\n6\nwhere the target for the Q-value is computed as\nyt(s, a, s′) = R(s, a) + γ\n\u0012\nQ˜π\nθ (s′, ˜a′) −α log πφ(a′|s′)\n\u0013\nwith ˜a′ ∼˜π(·|s′), a′ ∼πφ(·|s′).\n(4)\nOn the other hand, the policy πφ and the mixing function Λω can be trained to maximize the value function\n(Haarnoja et al., 2018) for the mixture ˜π. The value function can be decomposed in expectations over actions\nsampled from the prior ¯π and the policy πφ as derived in Appendix A:\nV ˜π(s) = λ\nE\n¯a∼¯π(·)\n\u0014\nQ˜π\nθ (s, ¯a)\n\u0015\n+ (1 −λ)\nE\na∼πφ(·|s)\n\u0014\nQ˜π\nθ (s, a)\n\u0015\n−α\nE\na∼πφ(·|s)\n\u0014\nlog(πφ(a|s))\n\u0015\n,\n(5)\nwhere λ = Λω(s) is the mixing weight computed on the current state s. This decomposed value function can\nbe maximized by minimizing the two following losses with respect to the policy’s and the mixing function’s\nparameters:\nJπφ = −\nE\n(s)∼D\n\u0014\u00001 −Λω(s)\n\u0001\u0000Q˜π\nθ (s, a) −α log πφ(a|s)\n\u0001\u0015\nwith a ∼πφ(·|s),\n(6)\nJΛω = −\nE\n(s)∼D\n\u0014\nΛω(s)\n\u0000Q˜π\nθ (s, ¯a) −Q˜π\nθ (s, a)\n\u0001\u0015\nwith ¯a ∼¯π(·), a ∼πφ(·|s).\n(7)\nThe three objectives can be empirically estimated and minimized through standard procedures, as reported\nin Haarnoja et al. (2018) and in Appendix A. We further note that, as expected, Equation 7 encourages\nhigh mixing weights λt = Λω(st) in case Q-values for actions sampled from the prior ¯π are higher than those\nfor actions sampled from the policy πφ, and vice versa (see Appendix F for an empirical analysis on the\nevolution of λ during training). We remark that the resulting mechanism is similar in spirit to the Q-ﬁlter\npresented in Nair et al. (2018), which is however originally only applied as a binary signal for weighting a\nbehavioral cloning loss.\nAlgorithm 1 summarizes (in blue) the necessary modiﬁcations for integrating the action prior into the SAC\nframework. Namely, actions are sampled from a mixture (line 8) weighted according to the output of a\nmixing function (line 7). Finally, the history of the agent needs to be initialized (line 5) and updated at each\nstep (line 10). Update rules for θ, φ and ω (line 13) are computed by minimizing the objectives in Equations\n6, 3 and 7.\n5\nExperiments\nWe evaluate our method in a series of experiments to empirically validate our contributions.\nFirst, in\nSection 5.1, we compare the eﬀectiveness of diﬀerent action priors to justify the choice of state-independent\nconditioning. Then, in Section 5.2 we evaluate our integration against existing ones in the context of state-\nfree priors. In Section 5.3, we verify that sampling from a state-free prior can produce correlated and state-\ncovering behavior, without the need to hand-craft an exploration policy. Next, we show how our method\ncan improve exploration eﬃciency in unseen long-horizon tasks by comparing against various baselines in\nstate-based RL (Section 5.4). Finally, we present a proof of concept of diﬀerent applications for state-free\npriors, i.e., in visual RL and when dealing with biased observations (Section 5.5). Ablations for our method\nand baselines are provided in Appendix B.\n7\n...\nFigure 3: Non-exhaustive overview of environments used in our experimental validation.\nBaselines\nWe now present a brief introduction to the baselines we consider, while implementation details\nare provided in Appendix K.3.\n• SAC: vanilla Soft Actor Critic (Haarnoja et al., 2018).\n• SAC+BC: SAC with warm-started policy through behavior cloning.\n• SAC-PolyRL: SAC with locally self-avoiding walks (Amin et al., 2021).\n• PARROT-state: ﬂow-based behavioral prior enforced through a transformation of the action space\n(Singh et al., 2021). We benchmark a state-based variant in non-visual settings. We found other\nmethods based on behavioral priors to perform similarly, while being arguably more complex (see\nAppendix H for an additional baseline).\nEnvironments\nWe evaluate our method on three types of domains, namely robotic manipulation, maze\nnavigation and robotic locomotion:\n• meta-world:\nrobot manipulation tasks from the MT10 benchmark in the publicly available\nmeta-world suite (Yu et al., 2020).\n• point-maze: qualitatively diﬀerent 2D maze structures, navigated by a point-like agent Pitis et al.\n(2020)\n• gym-mujoco: widely adopted continuous control environments for locomotion of simulated robots\nBrockman et al. (2016); Todorov et al. (2012).\nThe ﬁrst two suites deﬁne diverse tasks that share similar underlying dynamics (e.g. navigating mazes with\ndiﬀerent structures, or interacting with diﬀerent objects), and are therefore used to benchmark the ability\nto transfer knowledge from oﬄine trajectories to complex downstream tasks (Section 5.1, 5.3, 5.4). In this\ncase, oﬄine trajectories are collected in simple training tasks that involve reaching uniformly sampled goals\nin an empty environment (reach for meta-world and room for point-maze). RL agents are then trained and\nevaluated on a wider range of downstream tasks, involving object manipulation, such as opening a window,\nor navigation in more complex mazes. While our main results focus on priors trained on weakly-informative\ndata, we note that the performance of state-free priors with respect to behavioral priors is dependent on the\nchoice of training task. We further elaborate on this topic by providing an additional empirical analysis on\na more complex, object-interaction training task in Appendix D.\nOn the other hand, gym-mujoco environments do not oﬀer a straightforward way to deﬁne tasks, and are\ntherefore used to evaluate a proof-of-concept experiment on biased observations (Section 5.5).\nMore details can be found in Appendix J, while visual examples of environments are provided in Figure 3.\nMetrics and Training Data\nOur main metric for Sections 5.1, 5.2, 5.4, 5.5 is cumulative returns per test\nepisode. For each task, we average this metric over 10 random seeds (excluding gym-mujoco experiments\nand visual RL experiments, which are respectively limited to 5 and 2 seeds for computational reasons).\nUnless stated explicitly, when aggregating results for diﬀerent tasks (e.g. in plots marked as downstream),\nwe perform a simple mean aggregation due to its statistical eﬃciency, as per-task scores are not normalized,\nand in the same range for all tasks. Alternatively, we report results for a more robust aggregation scheme\n(i.e. IQM (Agarwal et al., 2021)) in Appendix M. Uncertainty is quantiﬁed through 95% stratiﬁed basic\nbootstrap conﬁdence intervals, as suggested in Agarwal et al. (2021). Across the experimental section, all\npriors are trained on 4000 trajectories of 500 steps each. An ablation on the amount of training data is\nreported in Appendix C. Further details can be found in Appendix K.1.\n8\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n200\n400\nTest Return\nmeta-world (training)\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n200\n400\nTest Return\nmeta-world (downstream)\nFigure 4: Comparison of downstream performance of action priors with diﬀerent conditioning variables, reported\non the training task (left) and averaged over downstream tasks (right).\n5.1\nConditioning Variables for Action Priors\nWe now empirically show how the eﬀectiveness of an action prior depends on the conditioning variables\nand on the similarity between the training and the downstream tasks. For this purpose, we train several\nvariants of ﬂow-based action priors on oﬄine reaching trajectories and integrate them into a SAC learner\nas described in Section 4. In particular, we compare state-free priors conditioned on action sequences of\ndiﬀerent lengths (1, 2, 5, 10), non-Markovian priors conditioned on the previous state-action pair and a\nbehavioral prior (conditioned on the state alone). In Figure 4 we report learning curves for the training task\n(reach), as well as averaged learning curves over all downstream tasks in meta-world. We observe that all\npriors are capable of guiding downstream RL as long as the task at hand matches the training task. In this\ncase, state-conditional priors achieve slightly faster exploration by leveraging state-related and task-speciﬁc\ninformation. However, we ﬁnd that including the state in the conditioning variables (as done in ¯π(at|st) and\n¯π(at|st, at−1)) can jeopardize the ability to transfer knowledge to a diﬀerent task. We hypothesize that this\nis due to a mismatch between the state distribution used for training the prior and that generated by the\ndownstream task.\nOn the other hand, state-free priors are not conditioned on states by design, prove to be a capable alternative\nacross both settings and are able to transfer knowledge to unseen tasks. While conditioning on longer action\nsequences can improve performance, we note that single-action-conditional models ¯π(at|at−1) are suﬃcient\nfor capturing non-trivial temporal dependencies within our settings. Hence, they will be the focus of the\nremaining experiments.\n5.2\nIntegration for State-free Priors\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n100\n200\n300\nTest Return\nmeta-world (downstream)\nFigure 5: Performance of diﬀerent integration\nschemes: our mixture-based integration com-\npares favorably against existing methods (ﬂow-\nbased and KL-regularization).\nAfter showing a motivating application for state-free priors,\nwe now argue the importance of our proposed mixture-based\nintegration scheme for priors in oﬄine reinforcement learning\nalgorithms. Two common existing integration schemes for be-\nhavioral priors rely on regularizing the policy with a KL-term\n(Tirumala et al., 2020; Pertsch et al., 2020), or on a ﬂow-based\ntransformation of the action space (Singh et al., 2021). Both\nmethods strongly build on the assumption that the prior is\nonly conditioned on the current state st, or a subset thereof.\nIn the case of ﬂow-based action space warping, the prior is ef-\nfectively integrated in the environment dynamics: as a result,\nif the prior is conditioned on previous actions at−1\n0\nor states\nst−1\n0\n, the environment loses its Markov property, and station-\nary policies are no longer guaranteed to be optimal. On the\nother hand, penalizing the KL-divergence between the policy\nand a non-Markovian prior encourages the stationary policy to\n9\nFigure 6: A qualitative comparison of sampled exploration trajectories in a robotic reaching task and in an empty\n2D maze. Our method achieves directed behavior while covering most of the state space. SAC fails to cover the full\nstate space, while SAC-PolyRL fails to reach distant areas consistently.\nmatch the distribution of a potentially non-stationary prior, which is in general an ill-posed task. Instead,\nour mixture-based integration only suﬀers from biased learning objectives in the case of a non-Markovian\naction prior (see Appendix A). Empirically, we found this to be a mild limitation, as our integration achieves\nsensibly better performance compared to the two baselines in downstream learning, as reported in Figure\n5, where experimental settings are the same as those described in Section 5.1, and details can be found in\nAppendix K.3.\n5.3\nCorrelation and State Coverage\nIn this experiment, we show how a one-step state-independent state-free prior ¯π(at|at−1) can generate corre-\nlated and directed behavior, which leads to a more complete coverage of the state space during exploration.\nTo this end, we sample 20 random trajectories of 500 steps each with our method and two relevant baselines\nin the room and reach environments.\nAs shown in Figure 6, the state-free prior produces directed behavior which covers most of the state space.\nAs expected, uniform sampling (which approximates SAC’s exploration) fails to reach the boundaries of the\nenvironment; SAC-PolyRL is capable of producing correlated and directed behaviors, but only after careful\ndesign and tuning. This qualitative assessment is consistent with the quantitative evaluation in Table 1,\nwhich reports state space coverage and radius of gyration squared (Amin et al., 2021) (see Appendix K.1 for\ndetails).\n5.4\nTransfer Learning\nOur main results are obtained by comparing our method against several baselines in downstream learning\ntasks with a vectorial state space, as presented in Figure 7.\nWe report learning curves averaged across\ndownstream tasks, as well as on training tasks for reference.\nResults for each task are disentangled in\nAppendix L.\nTable 1: State coverage metrics for our method and baselines. SFP’s trajectories are locally directed and cover the\nstate space well in both environments.\nU 2\ng\n% Coverage\nreach\nSAC\n0.006±0.001\n0.137±0.01\nSAC-PolyRL\n0.025±0.001\n0.272±0.01\nSFP (ours)\n0.053±0.009\n0.357±0.02\nroom\nSAC\n0.005±0.001\n0.333±0.02\nSAC-PolyRL\n0.026±0.002\n0.880±0.04\nSFP (ours)\n0.054±0.008\n0.963±0.04\n10\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nmeta-world (training)\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\npoint-maze (training)\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n100\n200\n300\nTest Return\nmeta-world (downstream)\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n100\n200\n300\n400\n500\nTest Return\npoint-maze (downstream)\nFigure 7: Accelerating downstream RL. While other methods are mostly competitive on training tasks (reach and\nroom), SFP is better suited for accelerating RL in unseen tasks. Results are averaged over 10 seeds per task.\nAs expected, we observe that the performance of behavioral priors depends on the similarity of between the\ntask at hand and expert demonstrations. This is the case for PARROT-state, which instantly solves the\ntraining tasks (reach and room), as its behavioral prior already represents a strong policy. On tasks which\nare signiﬁcantly diﬀerent from the training tasks, PARROT-state is however unable to guide the policy, as it\nreceives out-of-distribution states, resulting in low average performance. On the other hand, SFP is largely\nmore capable of transferring knowledge to unseen tasks, while rapidly catching up with PARROT-state in\nthe training tasks.\nOther baselines are in general less eﬀective across the benchmarks: Vanilla SAC only makes progress on\neasier tasks, which can be achieved even with weak exploration.\nSAC-PolyRL is able to produce good explorative trajectories through its hand-crafted policy and improves\non SAC, but fails to explore after its initial phase and does not solve harder tasks. As previously reported\nby Singh et al. (2021), initializing SAC through behavioral cloning (SAC+BC) can help guide exploration\nwithout constraining the policy, but it fails to generalize across tasks and is regularly outperformed by\nstronger methods.\n5.5\nAdditional Applications\nWe ﬁnally provide proofs of concept for possible applications of SFP. Per-task results are available in Ap-\npendix L.\nVisual RL\nState-free priors allow the state space of the downstream task to be deﬁned arbitrarily. Hence,\nthey also allow transfer to the visual RL setting, which avoids reliance on a low-dimensional vectorized state\nspace and is purely based on RGB observations. To this end, we compare SFP with Vanilla SAC and with\nPARROT in its original, visual setup, i.e., by conditioning its behavioral prior on images. We remark that,\n0\n2\n4\nSteps\n×105\n0\n100\n200\nTest Return\nmeta-world (downstream)\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\npoint-maze (downstream)\nFigure 8: Performance on downstream learning in\nvisual settings, averaged per suite.\n0\n0.1 1.0 10.0\nBias\n100%\n80%\n60%\n40%\n20%\nSparsity %\nPARROT-state\n0\n250\n500\n750\n1000\nFigure 9: Mean performance on gym-mujoco envi-\nronments for diﬀerent sparsity thresholds and bias\nnorms.\n11\nin this case, PARROT needs access to additional data (i.e., RGB observations) compared to SFP. We report\nthe results in Figure 8. In general, we observe a performance drop for all methods compared to non-visual\nsettings, which we hypothesize is due to the inherent challenge of learning in high-dimensional state spaces\n(Laskin et al., 2020). Nonetheless, our ﬁndings remain generally valid in visual settings, as SFP retains most\nof its exploration capability and, despite failing to make progress on a majority of visual tasks, consistently\noutperforms the baselines. Per-task results are reported in Appendix L.\nBiased Downstream Learning\nA ﬁnal potential application for state-free priors is that of guiding ex-\nploration when downstream learning is corrupted by biased observations. This scenario could model a form\nof sim-to-real transfer, in which training trajectories were cheaply collected in simulation, but the physical\ntestbed was miscalibrated, resulting in biased observations. We provide experimental results in this setup\nbased on gym-mujoco environments (Ant-v2, HalfCheetah-v2, Swimmer-v2, Hopper-v2, Walker2d-v2).\nTraining trajectories are collected by a SAC agent that was fully trained on dense rewards. In downstream\nlearning, the reward is instead sparse (i.e. only positive when the displacement of the CoM from its initial\nposition surpasses a sparsity threshold ¯x), and signiﬁcant exploration is required. Moreover, the observations\nin downstream learning are corrupted by a ﬁxed additive bias s = s+b. Figure 9 compares test performance\nof downstream agents when guided by a behavioral prior (PARROT-state) or by a state-free prior (SFP),\nafter 500k steps and averaged across environments. We observe that behavioral priors are capable of extract-\ning a near-optimal policy from the training data, which transfers perfectly to the sparsiﬁed environments if\nthe observations are uncorrupted. However, as bias increases, their performance sharply drops. On the other\nhand, state-free priors are less eﬀective in unbiased conditions, but their performance is generally retained as\nbias increases. More details on this experiment are presented in Appendix J.3, and an ablation introducing\na transformation on the action space is reported in Appendix E.\n6\nDiscussion and Conclusion\nLimitations\nWhile state-free priors are able to generalize to a broader set of tasks with respect to be-\nhavioral priors, they are data-driven, and as such will only reconstruct behavior that appears in the oﬄine\ndataset of trajectories they are trained on. For instance, when training on demonstrations for reach, state-\nfree priors will not encode any grasping strategy. While such behavior can still be recovered by the policy π,\nthere is no incentive to do so. As a result, successfully transferring to tasks that require additional strategies,\nsuch as grasping (e.g., pick-place) remains an open challenge. Secondly, as shown in Figure 7, behavioral\npriors remain superior when the downstream state distribution is not distant from the state distribution\nobserved in the training task. As state-free priors do not model state-action relationships, they can only\nprovide weaker guidance in this privileged case. This is in particular true when priors are trained on expert\ntrajectories for complex manipulation tasks, as shown in Appendix D. For this reason, a strategy to combine\nbeneﬁts from behavioral and state-free priors would represent an interesting direction for the future.\nConclusion\nIn this paper we proposed a method for improving exploration eﬃciency in oﬀ-policy reinforce-\nment learning. In particular, we introduced state-free priors, a family of non-Markovian, state-independent\nand ﬂow-based action priors, and proposed a principled manner for integration into an oﬀ-policy reinforce-\nment learning framework. While leveraging a relatively simple idea, state-free priors represent a powerful\nmethod for exploration. As shown by signiﬁcant empirical evidence, they can massively accelerate learning\nacross a diverse set of tasks, while solely requiring a modest amount of oﬄine, weakly informative trajecto-\nries.\nBroader Impact Statement\nOur main contribution revolves on accelerating and enabling reinforcement learning in environments with a\nstrong exploration component. As a consequence, we believe that concerns with respect to our method are\nfor the most part aligned with general RL research. For instance, improved sample eﬃciency could on one\nhand accelerate the process of automation, which might have a negative impact on societal equality, and on\nthe other hand democratize access to powerful RL methods by lowering the amount of required resources.\nDue to the general nature of our method, we believe that we do not introduce fundamentally new risks.\n12\nAcknowledgments\nWe are grateful to Núria Armengol Urpí, Emre Aksan and others for providing constructive feedback on this\nwork. We also acknowledge the anonymous reviewers for their important contributions towards improving\nthe manuscript.\nReferences\nJoshua Achiam. Spinning up in deep reinforcement learning. 2018. URL https://github.com/openai/\nspinningup.\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep\nreinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing\nSystems, 34, 2021.\nAnurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Oﬁr Nachum. OPAL: oﬄine primitive\ndiscovery for accelerating oﬄine reinforcement learning.\nIn 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\nSusan Amin, Maziar Gomrokchi, Hossein Aboutalebi, Harsh Satija, and Doina Precup. Locally persistent\nexploration in continuous control tasks with sparse rewards.\nIn Proceedings of the 38th International\nConference on Machine Learning, volume 139, pp. 275–285, 2021.\nLynton Ardizzone, Jakob Kruse, Carsten Rother, and Ullrich Köthe.\nAnalyzing inverse problems with\ninvertible neural networks. In 7th International Conference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019.\nKamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar.\nEﬃcient exploration through\nbayesian deep q-networks. In 2018 Information Theory and Applications Workshop (ITA), pp. 1–9. IEEE,\n2018.\nPierre-Luc Bacon, Jean Harb, and Doina Precup. The Option-Critic architecture. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 31, 2017.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying\ncount-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems,\npp. 1471–1479, 2016.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An\nevaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47, 2013.\nMiroslav Bogdanovic and Ludovic Righetti. Learning to explore in motion and interaction tasks. In Proceed-\nings 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2686–2692.\nIEEE, 2019.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech\nZaremba. Openai gym, 2016.\nYuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network distilla-\ntion. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019.\nSammy Christen, Stefan Stevšić, and Otmar Hilliges. Guided deep reinforcement learning of control policies\nfor dexterous human-robot interaction. In 2019 International Conference on Robotics and Automation\n(ICRA), pp. 2161–2167, 2019.\nSammy Christen, Lukas Jendele, Emre Aksan, and Otmar Hilliges.\nLearning functionally decomposed\nhierarchies for continuous control tasks with path planning. IEEE Robotics and Automation Letters, 6(2):\n3623–3630, 2021.\n13\nWill Dabney, Georg Ostrovski, and André Barreto.\nTemporally-extended ϵ-greedy exploration.\nIn 9th\nInternational Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, 2021.\nPeter Dayan and Geoﬀrey Hinton. Feudal reinforcement learning. Advances in Neural Information Processing\nSystems, 09 2000.\nThomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decomposition.\nJournal of artiﬁcial intelligence research, 13:227–303, 2000.\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th Inter-\nnational Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017.\nAdrien Ecoﬀet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and JeﬀClune. First return, then explore.\nNature, 590(7847):580–586, 2021.\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning\nskills without a reward function. In 7th International Conference on Learning Representations, ICLR\n2019, New Orleans, LA, USA, May 6-9, 2019.\nCarlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical reinforcement\nlearning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April\n24-26, 2017.\nAlexandre Galashov, Siddhant M. Jayakumar, Leonard Hasenclever, Dhruva Tirumala, Jonathan Schwarz,\nGuillaume Desjardins, Wojciech M. Czarnecki, Yee Whye Teh, Razvan Pascanu, and Nicolas Heess. In-\nformation asymmetry in kl-regularized RL. In 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nJonas Gehring, Gabriel Synnaeve, Andreas Krause, and Nicolas Usunier. Hierarchical skills for eﬃcient\nexploration. In Advances in Neural Information Processing Systems, 2021.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-policy maximum\nentropy deep reinforcement learning with a stochastic actor. International Conference on Machine Learning\n(ICML), 2018.\nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan,\nBilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement\nlearning. In Thirty-second AAAI Conference on Artiﬁcial Intelligence, 2018.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015.\nGeorge Konidaris and Andrew Barto. Building portable options: Skill transfer in reinforcement learning. In\nProceedings of the 20th International Joint Conference on Artiﬁcal Intelligence, IJCAI’07, pp. 895–900,\n2007.\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for\nreinforcement learning. Proceedings of the 37th International Conference on Machine Learning, Vienna,\nAustria, PMLR 119, 2020.\nAndrew Levy, George Dimitri Konidaris, Robert Platt Jr., and Kate Saenko. Learning multi-level hierarchies\nwith hindsight. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019.\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David\nSilver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua Bengio and\nYann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan,\nPuerto Rico, May 2-4, 2016.\n14\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner.\nIn 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April\n30 - May 3, 2018.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through\ndeep reinforcement learning. nature, 518(7540):529–533, 2015.\nOﬁr Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-eﬃcient Hierarchical Reinforcement\nLearning. In Advances in Neural Information Processing Systems, pp. 3303–3313, 2018.\nAshvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel.\nOvercoming\nexploration in reinforcement learning with demonstrations. In 2018 IEEE international conference on\nrobotics and automation (ICRA), pp. 6292–6299. IEEE, 2018.\nSoroush Nasiriany, Vitchyr Pong, Steven Lin, and Sergey Levine. Planning with goal-conditioned policies.\nIn Advances in Neural Information Processing Systems, volume 32, 2019.\nOpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,\nAlex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry\nTworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik’s\ncube with a robot hand, 2019.\nIan Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped\ndqn. Advances in neural information processing systems, 29:4026–4034, 2016.\nIan Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement learning.\narXiv preprint arXiv:1806.03335, 2018.\nSimone Parisi, Victoria Dean, Deepak Pathak, and Abhinav Gupta.\nInteresting object, curious agent:\nLearning task-agnostic exploration. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan\n(eds.), Advances in Neural Information Processing Systems, 2021.\nRonald Parr and Stuart Russell. Reinforcement learning with hierarchies of machines. In Proceedings of the\n1997 Conference on Advances in Neural Information Processing Systems 10, NIPS ’97, pp. 1043–1049,\n1998.\nXue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine. Mcp: Learning compos-\nable hierarchical control with multiplicative compositional policies. In Advances in Neural Information\nProcessing Systems, volume 32, 2019.\nKarl Pertsch, Youngwoon Lee, and Joseph J. Lim. Accelerating reinforcement learning with learned skill\npriors. In Conference on Robot Learning (CoRL), 2020.\nKarl Pertsch, Youngwoon Lee, Yue Wu, and Joseph J. Lim. Demonstration-guided reinforcement learning\nwith learned skills, 2021.\nSilviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba. Maximum entropy gain exploration\nfor long horizon multi-goal reinforcement learning. In International Conference on Machine Learning, pp.\n7750–7761, 2020.\nAravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov,\nand Sergey Levine. Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and\nDemonstrations. In Proceedings of Robotics: Science and Systems (RSS), 2018.\nKate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Eﬃcient oﬀ-policy meta-\nreinforcement learning via probabilistic context variables. In Proceedings of the 36th International Con-\nference on Machine Learning, Proceedings of Machine Learning Research, 2019.\n15\nSasha Salter, Kristian Hartikainen, Walter Goodwin, and Ingmar Posner. Priors, hierarchy, and information\nasymmetry for skill transfer in reinforcement learning. CoRR, 2022.\nStefan Schaal. Learning from demonstration. Advances in Neural Information Processing Systems, 1997.\nLukas Schäfer, Filippos Christianos, Josiah Hanna, and Stefano V Albrecht. Decoupling exploration and\nexploitation in reinforcement learning. arXiv preprint arXiv:2107.08966, 2021.\nJürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the\nmeta-meta-... hook. PhD thesis, Technische Universität München, 1987.\nAvi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Parrot: Data-driven\nbehavioral priors for reinforcement learning. In 9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nKihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep condi-\ntional generative models. Advances in neural information processing systems, 28:3483–3491, 2015.\nMasashi Sugiyama, Ichiro Takeuchi, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and Daisuke\nOkanohara. Conditional density estimation via least-squares density ratio estimation. In Proceedings of\nthe Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, pp. 781–788, 2010.\nRichard S. Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A Framework for\nTemporal Abstraction in Reinforcement Learning. Artif. Intell., 112:181–211, 1999.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip\nDe Turck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement\nlearning. In 31st Conference on Neural Information Processing Systems (NIPS), volume 30, pp. 1–18,\n2017.\nDhruva Tirumala, Hyeonwoo Noh, Alexandre Galashov, Leonard Hasenclever, Arun Ahuja, Greg Wayne,\nRazvan Pascanu, Yee Whye Teh, and Nicolas Heess. Exploiting hierarchy for learning and transfer in\nkl-regularized rl. arXiv preprint arXiv:1903.07438, 2019.\nDhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan Pascanu, Jonathan\nSchwarz, Guillaume Desjardins, Wojciech Marian Czarnecki, Arun Ahuja, Yee Whye Teh, and Nicolas\nHeess. Behavior priors for eﬃcient reinforcement learning, 2020.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. 2012\nIEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033, 2012.\nAlexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver,\nand Koray Kavukcuoglu. FeUdal Networks for Hierarchical Reinforcement Learning. In Proceedings of the\n34th International Conference on Machine Learning, pp. 3540–3549, 2017.\nN. Vulin, S. Christen, S. Stevšić, and O. Hilliges. Improved learning of robot manipulation tasks via tactile\nintrinsic motivation. IEEE Robotics and Automation Letters, 6(2):2194–2201, 2021.\nJane X. Wang, Zeb Kurth-Nelson, Hubert Soyer, Joel Z. Leibo, Dhruva Tirumala, Rémi Munos, Charles\nBlundell, Dharshan Kumaran, and Matt M. Botvinick. Learning to reinforcement learn. In CogSci, 2017.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine.\nMeta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference\non Robot Learning (CoRL), pp. 1094–1100, 2020.\n16\nA\nObjective Derivation\nThe purpose of this section is that of describing how learning objectives for the policy π, the Q-estimator\nQ˜π and the mixing function Λ can be derived from a max-entropy objective; while in practice each function\nis respectively parameterized by φ, θ and ω, in order to ease notation we will only explicitly mention these\nparameters when introducing empirical loss estimates. The objective is deﬁned as\nE\nτ∼˜π\n\u0014 ∞\nX\nt=0\n\u0000γtR(st, at) + αH(π(·|st))\n\u0001\u0015\n,\n(8)\nwhere the expectation is computed over trajectories generated by the mixture ˜π = (1 −λt)π + λt¯π, ¯π is a\nﬁxed action prior and λt = Λ(st) is the mixing weight. The optimization of Equation 8 can be performed\nby modeling the action prior ¯π(·) as a Markovian distribution over the action space A; we note that non-\nMarkovian action priors do not necessarily satisfy Equation 10 and therefore can introduce bias in Q-value\ntargets (Equation 12). The steps of these derivations follow those reported in Haarnoja et al. (2018), as our\nmethod is designed to ﬁt within the proposed framework. First, let us formally introduce the Q-function for\nthe mixture ˜π:\nQ˜π(s, a) = E\nτ∼˜π\n\u0014 ∞\nX\nt=0\n\u0000γtR(st, at)\n\u0001\n+ α\n∞\nX\nt=1\n\u0000γtH(π(·|st))\n\u0001\n|s0 = s, a0 = a\n\u0015\n.\n(9)\nWe can then formulate the Bellman Equation and explicitly unravel the entropy term:\nQ˜π(s, a) =\nE\ns′∼T (·|s,a)\n˜a′∼˜π(·|s′)\n\u0014\nR(s, a) + γ\n\u0000Q˜π(s′, ˜a′) + αH(π(·|s′))\n\u0001\u0015\n=\nE\ns′∼T (·|s,a)\n˜a′∼˜π(·|s′)\n\u0014\nR(s, a) + γ\n\u0000Q˜π(s′, ˜a′) + α\nE\na′∼π(·|s′)[−log(π(a′|s′))]\n\u0001\u0015\n=\nE\ns′∼T (·|s,a)\n˜a′∼˜π(·|s′)\na′∼π(·|s′)\n\u0014\nR(s, a) + γ\n\u0000Q˜π(s′, ˜a′) −α log(π(a′|s′))\n\u0001\u0015\n.\n(10)\nThe right-hand side of Equation 10 can be estimated via Monte Carlo sampling and used as a target for\nQ-estimates, which can then be trained by minimizing a standard MSE loss. In practice, as is done for SAC,\ntwo separate parameterized Q-function estimators (Q˜π\nθi)i=1,2 are used to prevent overestimating Q-values.\nAdditionally, we also adopt target networks (Q˜π\nθtarget,i)i=1,2, updated via Polyak averaging. As a result, when\nsampling a batch B from a replay buﬀer, the empirical estimates for Q-losses are:\nˆJQθi =\n1\n|B|\nX\n(s,a,r,s′,d)∈B\n\u0012\nQ˜π\nθi(s, a) −ˆyt(s′, r, d)\n!2\n,\n(11)\nwhere the target for the Q-value is computed as\nˆyt(s′, r, d) = r + γ(1 −d)\n\u0012\nmin\ni=1,2 Q˜π\nθtarget,i(s′, ˜a′) −α log πφ(a′|s′)\n\u0001\u0013\nwith ˜a′ ∼˜π(·|s′), a′ ∼πφ(·|s′).\n(12)\nand r, d stand for the reward and done signal, respectively.\nAs the prior ¯π is ﬁxed, optimizing the mixture policy ˜π = λ¯π + (1 −λ)π only involves the optimization of\nπ and λ. The policy π can be trained to minimize the KL-divergence with the soft-max of the Q-function,\nor alternatively to maximize the value function (Haarnoja et al., 2018). Our method relies on the second\noption and trains both π and Λ to maximize the value function V ˜π(s), which can be formulated as follows:\nV ˜π(s) =\nE\n˜a∼˜π(·|s)\na∼π(·|s)\n\u0014\nQ˜π(s, ˜a) −α log(π(a|s))\n\u0015\n.\n(13)\n17\nInterestingly, the value function V ˜π can be decomposed as\nV ˜π(s) =\nE\n˜a∼˜π(·|s)\n\u0014\nQ˜π(s, ˜a)\n\u0015\n−α\nE\na∼π(·|s)\n\u0014\nlog(π(a|s))\n\u0015\n=\n\u0012 Z\n˜a∈A\nQ˜π(s, ˜a)˜π(˜a|s)\n\u0013\n−α\nE\na∼π(·|s)\n\u0014\nlog(π(a|s))\n\u0015\n=\n\u0012 Z\n˜a∈A\nQ˜π(s, ˜a)\n\u0000λ¯π(˜a) + (1 −λ)π(˜a|s)\n\u0001\u0013\n−α\nE\na∼π(·|s)\n\u0014\nlog(π(a|s))\n\u0015\n= λ\n\u0012 Z\n¯a∈A\nQ˜π(s, ¯a)¯π(¯a)\n\u0013\n+ (1 −λ)\n\u0012 Z\na∈A\nQ˜π(s, a)π(a|s)\n\u0013\n−α\nE\na∼π(·|s)\n\u0014\nlog(π(a|s))\n\u0015\n= λ\nE\n¯a∼¯π(·)\n\u0014\nQ˜π(s, ¯a)\n\u0015\n+ (1 −λ)\nE\na∼π(·|s)\n\u0014\nQ˜π(s, a)\n\u0015\n−α\nE\na∼π(·|s)\n\u0014\nlog(π(a|s))\n\u0015\n,\n(14)\nwhere λ = Λ(s) is the mixing weight computed on the current state. When diﬀerentiating this objective\nwith respect to the parameters φ of the policy network π, the ﬁrst term does not contribute to gradients,\nresulting in\nmax\nφ\nV ˜π(s) = max\nφ\nE\na∼πφ(·|s)\n\u0014\n(1 −λ)Q˜π\nθ (s, a) −α log(πφ(a|s))\n\u0015\n.\n(15)\nComputing the expectation over actions can then be circumvented by using the reparameterization trick,\nwhich enables expressing actions as aφ(ξ, s), where ξ ∼N is sampled from a standard Gaussian. In practice,\nconsidering the two Q-networks, the policy network can be optimized by minimizing the empirical loss\nestimated over a batch B:\nˆJπφ =\n1\n|B|\nX\n(s)∈B\n\u0014\n(1 −Λω(s))\n\u0000min\ni=1,2 Q˜π\nθi(s, πφ(s)) −α log(πφ(a|s))\n\u0001\u0015\nwith a ∼πφ(·|s),\n(16)\nwhich can be minimized via standard ﬁrst-order optimization techniques.\nThe last learning objective to recover is that for the parameterized mixing function Λω. Once again, we\nobtain this by maximizing the value function V ˜π(s). Starting from Equation 14, we can now drop the ﬁnal\nterm, which does not depend on ω:\nmax\nω\nV ˜π(s) = max\nω\nΛω(s)\nE\n¯a∼¯π(·)\n\u0014\nQ˜π\nθ (s, ¯a)\n\u0015\n+ (1 −Λω(s))\nE\na∼πφ(·|s)\n\u0014\nQ˜π\nθ (s, a)\n\u0015\n= max\nω\nΛω(s)\nE\n¯a∼¯π(·)\n\u0014\nQ˜π\nθ (s, ¯a)\n\u0015\n+\nE\na∼πφ(·|s)\n\u0014\nQ˜π\nθ (s, a)\n\u0015\n−Λω(s)\nE\na∼πφ(·|s)\n\u0014\nQ˜π\nθ (s, a)\n\u0015\n= max\nω\nΛω(s)\nE\n¯a∼¯π(·)\n\u0014\nQ˜π\nθ (s, ¯a)\n\u0015\n−Λω(s)\nE\na∼πφ(·|s)\n\u0014\nQ˜π\nθ (s, a)\n\u0015\n= max\nω\nΛω(s)\nE\n¯a∼¯π(·|s)\na∼πφ(·)\n\u0014\nQ˜π\nθ (s, ¯a) −Q˜π\nθ (s, a)\n\u0015\n.\n(17)\nIn practice, the loss estimate for training the mixing network Λω can be computed over a batch B as\nˆJΛω =\n1\n|B|\nX\n(s)∈B\n\u0014\nΛω(s)\n\u0000\u0014\nmin\ni=1,2 Q˜π\nθi(s, ¯a) −min\ni=1,2 Q˜π\nθi(s, a)\n\u0001\u0015\nwith ¯a ∼¯π(·), a ∼πφ(·|s).\n(18)\n18\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n200\n400\nTest Return\nmeta-world (downstream)\nFigure 10: Performance on downstream meta-world tasks when\nmodeling a one-step state-free prior through various generative\nmodels.\nFigure 11: Performance on downstream\nmeta-world tasks when training on fewer\noﬄine demonstrations, or when they are\ncorrupted by uniform noise.\nThe gradient of this objective with respect to the mixing weight λ = Λ(s) is intuitively the diﬀerence in Q-\nvalues between actions drawn from the prior and policy. As training proceeds, and the policy π improves, the\ngradient turns negative and encourages lower mixing weights, gradually abandoning the prior ¯π’s guidance.\nB\nModel Ablation\nWe now set out to provide empirical backing for an important design choice. Our policy mixing approach\ngrants freedom in choosing generative models capable of describing complex distributions, as computing\ndistance metrics to the prior is not required. We compare the performance of Real NVP Flows with a Con-\nditional VAE (Sohn et al., 2015), a non-parametric conditional Least Squares Density Estimator (Sugiyama\net al., 2010), and an MLP modeling a deterministic prior. As shown in Figure 10, we found ﬂow-based\nmodeling to be both competitive and practical.\nC\nData Ablation\nFigure 11 includes data quantity ablations (with 75%, 50% and 25% of training data), and data quality\nablations (by substituting 25%, 50% and 75% of the training data with uniformly sampled actions). In the\nlatter case, we observe a sharp drop in ﬁnal performance (averaged over downstream meta-world tasks),\nwhile we ﬁnd SFP to remain viable in low-data regimes.\nD\nTraining Task Ablation\nThe eﬀectiveness of a learned prior depends on the nature of both training, and downstream tasks.\nIn\nthe context of this paper, we focus on simple training tasks, which can be solved easily by existing RL\nalgorithms (e.g. reach from meta-world). In this section, however, we instead evaluate several prior designs\nwhen training on expert trajectories from a more complex and informative task (i.e. pick-place). We set\nout to verify whether (a) state-free priors can fail to exploit high-quality expert training trajectories due\nto their lack of expressiveness, and (b) how state-free priors compare to behavioral priors on unseen tasks\nwhose state distribution still matches the expert state distribution (i.e. push).\nOur experimental comparison includes SFP (¯π(at|at−1)), PARROT-state, as well as two variants of SFP\nthat involve priors conditioned on the current state (¯π(at|st)), or on the current state and previous action\n(¯π(at|st, at−1)). Figure 12 gathers the performance of all methods in the training task (pick-place, left),\naveraged over all downstream meta-world tasks (middle), and on a downstream task which is unseen, but\nremains in-distribution of the training task (push, right).\nWe observe that only state-conditioned approaches are able to make progress on the training task, as\n¯π(at|at−1) lacks suﬃcient expressiveness for guiding exploration, since its state-free prior cannot encode\n19\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nmeta-world (training)\n0\n2\n4\nSteps\n×105\n0\n200\nTest Return\nmeta-world (downstream)\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\npush\nFigure 12: When trained on a task involving manipulation, state-conditioned priors can succeed in complex tasks\n(the training task pick-place on the left, and an unseen in-distribution task push on the right), while ¯π(at|at−1)\nfails. However, when averaging returns over all downstream tasks (middle), ¯π(at|at−1) can partially close the gap.\nclosed-loop interaction with objects. Similarly, when evaluating on an unseen in-distribution task requir-\ning manipulation (push), we observe that behavioral priors are, as expected, able to learn the task, while\n¯π(at|at−1) fails to ﬁnd a solution. However, when performance is averaged over all downstream tasks, we ﬁnd\nthat the robustness to domain shifts compensates the lack of expressiveness, and ¯π(at|at−1)’s performance\nis roughly comparable with other methods.\nWithin state-conditional approaches, we note that the two methods adopting our integration scheme can\ndynamically manage the frequency to which the prior is sampled, and can ignore the prior when it suggests\nsuboptimal solutions. As a consequence, their performance on downstream tasks is, on average, improved.\nPARROT-state, on the other hand, relies on a hard integration of the prior, and is the most sample eﬃcient\nmethod for in-distribution tasks, but suﬀers largely when dealing with out-of-distribution downstream tasks.\nFinally, we observe that priors conditioned on action-state pairs ¯π(at|st, at−1) are underperforming with\nrespect to purely behavioral priors ¯π(at|st) in this setting.\nConsidering the results reported in Section\n5.1, which show an inverted pattern, we note that conditioning the prior on action-state pairs allows it to\nextract conditional dependencies on both the state space and the action space. Such priors are not robust\nto distribution shifts in the state space, but may also learn to focus more on their action inputs. As a\nconsequence, we hypothesize that their average performance generally falls between that of purely state-free,\nand purely behavioral priors.\nE\nAblation on Biased Downstream Learning\nThe experiment presented in Section 5.5 introduces a potential application of state-free priors in case\nthe downstream task’s state space is corrupted by bias.\nWe now study the setting in which the state\nspace of training and downstream tasks matches, but the action space is disrupted through a transforma-\ntion. For simplicity, we consider a permutation of the action vector, and in particular an inversion (i.e.\naction=action[::-1]), which occurs before the action is executed in the environment. This transformation\n20%\n40%\n60%\n80%\n100%\nSparsity %\n0\n1000\nTest Return\nFigure 13: SFP (red) and PARROT-state (orange) performance aggregated over mujoco tasks. Dotted bars represent\nperformance on the unmodiﬁed training task, and undotted bars represent performance on the training task, when\nintroducing a disruptive transformation of the action space.\n20\n0\n2\n4\nSteps\n×105\n0\n100\n200\n300\nTest Return\n0.0\n0.5\n1.0\nMixing Weight\nmeta-world (downstream)\nFigure 14:\nEvolution of mixing weight λ and\ntest performance during training,\naveraged over\nmeta-world tasks.\nFigure 15: PSD of trajectories sampled from the\noﬄine dataset, from SFP’s prior and from a uniform\nrandom actor.\nis chosen because, contrary to adding noise to the state space, it would not aﬀect the performance of a naive\nRL algorithm (e.g. SAC), while potentially impacting the eﬀectiveness of learned priors.\nIn Figure 13 we report ﬁnal returns for SFP and PARROT-state averaged over the ﬁve mujoco environments\nused in Section 5.5. We evaluate diﬀerent degrees of reward sparsity (as explained in Section 5.5), and for\neach one we compare the performance of SFP and PARROT-state on the training task, as well as on a\ncorrupted version of the training task. We note that the added transformation results in a comparable drop\nin performance for both methods. In the case of SFP, this can be partially traced back to the fact that the\nprior may be conditioned on out-of-distribution actions. Most importantly, for both SFP and PARROT-\nstate, this transformation disrupts the actions suggested by the prior, as the trajectories it was trained on\ncannot be reproduced in the environment.\nF\nStudy on Learned Mixing Weights\nIn Figure 14, we plot the mixing weight λ during the course of training against returns, averaged over the\ndownstream meta-world tasks. As formally described in Section 4, the mixing weight generally decreases as\nreturns increase; in particular, we observe λ →0 for solved tasks and λ →1 for unsolved tasks. We found\nthis trend to be consistent for most states visited. Per-task plots are reported in Appendix L. We also note\nthat the initial trajectory is inﬂuenced by the initialization discussed in Appendix K.4.\nG\nStudy on Temporal Correlation\nIn order to better quantify the concept of temporally correlated exploration, we plot the PSD (averaged over\naction dimensions) of action sequences sampled from the training set, from our learned prior, as well as those\nobtained by sampling from a uniform distribution on room. In Figure 15 we note prominent low frequencies\nfor both the training data and SFP trajectories.\nH\nAdditional Baselines\nIn this section we report results for an additional baseline, namely SPIRL (Pertsch et al., 2020). SPIRL is\na hierarchical method that combines skill learning with a high-level behavioral prior, which is integrated in\na SAC agent via KL-regularization. We evaluate this method on the meta-world suite in the same settings\nreported in Section 5.4, namely by training all priors on reaching trajectories for random goals, and then\nreporting performance on the training task, as well as averaged over all downstream tasks. We also report\nthe performance of SFP and PARROT-state from Section 5.4 as a reference.\n21\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n200\n400\nTest Return\nmeta-world (training)\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n100\n200\n300\nTest Return\nmeta-world (downstream)\nFigure 16: Comparison of SPIRL with SFP, as well as with another method based on behavioral priors (PARROT-\nstate), reported on the training task (left) and averaged over downstream tasks (right). The setting is the same as\nfor Figure 7.\nFigure 16 highlights that, similarly to PARROT-state, SPIRL can instantly solve the training task, but\nsuﬀers when transferred to unseen tasks with out-of-distribution states. We note that the lower performance\nin the trained task can be traced back to the hierarchical nature of SPIRL: its low level skill decoder is\ntrained on reaching trajectories, which do not include static behavior. As a consequence, the agent has no\naccess to a skill that keeps the gripper still, while reach requires the agent to hold the gripper in its goal\nposition for several steps. Finally, we note that, while this skill-based approach needs further assumption on\nthe training data to ensure learning all necessary skills, the soft integration of the behavioral prior could in\nprinciple allow the agent to mitigate issues related to out-of-distribution states.\nThis experiments relies on the oﬃcial implementation (Pertsch et al., 2020), and adopts the hyperparameter\nset provided for Frankakitchen experiments. Further tuning of the target divergence parameter did not lead\nto signiﬁcant improvements in performance.\nI\nAdditional Related Works\nMeta-RL and Task Inference\nThe idea of quickly adapting to unseen tasks is a fundamental concept\nin meta-reinforcement learning (Schmidhuber, 1987; Wang et al., 2017). While SFP has a similar goal, it\ncrucially relies on training a policy from scratch for downstream learning and is not designed for zero-shot\nadaptation. A line of research in meta-RL, referred to as context-based (Wang et al., 2017; Mishra et al., 2018;\nRakelly et al., 2019), performs task inference in order to identify the current task and quickly extrapolate\nhow to maximize returns.\nA task representation can in practice be extracted from recent experiences.\nInterestingly, state-free priors can in principle handle the same type of input, i.e. sequences of recent action-\nstate pairs. However, instead of producing an explicit representation of the current task, state-free priors\ndirectly model an action distribution.\nLearning from Demonstrations\nSFP is aligned with existing methods that rely on demonstrations\nto accelerate RL on complex tasks (Nair et al., 2018; Rajeswaran et al., 2018; Christen et al., 2019). In\nprinciple, the cost of acquiring few expert trajectories can be small compared to the signiﬁcant engineering\neﬀort required in their absence (OpenAI et al., 2019). In most cases, such trajectories are required to be\nnear-optimal and collected in the same environment (Schaal, 1997) or from the same distribution of tasks\n(Singh et al., 2021). In our case, expert trajectories can be weakly informative , unlabeled and collected\non a signiﬁcantly diﬀerent task, as we focus on reconstructing correlated exploration trajectories instead of\nexploitative behavior.\n22\nJ\nEnvironment Details\nJ.1\nRobot Manipulation\nWe rely on the meta-world suite (Yu et al., 2020) for robot manipulation experiments.\nIt consists of\na simulated 7 DoF Sawyer arm, implemented in the MuJoCo physics engine (Todorov et al., 2012). By\ndefault, states are represented as 36-dimensional vectors across all tasks. The state contains the 3D location\nand aperture of the gripper, the 3D location and quaternion of one object (e.g. door or window), measured\nfor the current and previous time step. Goals are represented as the desired 3D location of the end eﬀector\nor object, according to the task. Actions are 4-dimensional vectors containing a 3D movement and a 1D\ncontrol over the aperture of the eﬀector.\nWe additionally render 64x64 RGB images as observations for the visual setup in Section 5.5, using the\ncamera angle corner (as can be seen in Figure 3). Instead of the originally proposed dense reward, we\nadopt a binary sparse reward which is non-zero only upon task completion. For more details, we refer to the\noriginal implementation (Yu et al., 2020).\nJ.2\nMaze Navigation\nOur point-maze environments are adapted from Pitis et al. (2020). The state consists of the 2D position of\nthe agent, which can be actuated via a velocity-controller through 2D actions. The goal space matches the\nstate spaces in dimensions and representation. The reward signal is 1 when the Euclidean distance to the\ngoal is lesser than 1.2 units, else it is 0. We experiment with two diﬀerent layouts (renderings can be found\nin Figure 3):\n• room is a large, 29×29 square room. The starting position is at the center, and the goal is initialized\nrandomly in one of the 4 corners at each reset. Trajectories for training the priors are obtained from\nthis environment. Only for Figure 6, the size was increased to 81×81, to allow and better visualize\nlong trajectories.\n• corridor is a larger u-shaped corridor with three parts of lengths 60, 3 and 60 respectively, assembled\nat 90° clockwise rotations. The structure of the corridor can therefore be contained in a 60 × 3\nrectangle. Starting and goal position are ﬁxed and located at opposite ends of the corridor.\n• maze is a maze presenting intersections and dead ends. Starting and goal positions are ﬁxed, and\nthe optimal policy requires 66 steps.\nFor further details, we refer directly to the published codebase (URL in Footnote 1).\nJ.3\nContinuous Control\nDue to their prevalence as a benchmark, we use gym-mujoco environments to evaluate SFP and baselines in\nthe presence of biased observations. We evaluate methods across 5 popular environments, namely Ant-v2,\nHalfCheetah-v2, Hopper-v2, Swimmer-v2 and Walker2d-v2 (see Figure 17). Additionally, we parameterize\neach environment with two variables, namely the bias b and the sparsity parameter ¯x.\nObservations in\ndownstream learning are corrupted as s ←s + b, and rewards are sparsiﬁed as rt = 1x≥¯x where x is the L2\ndistance between the initial and current position of the CoM. In order to aggregate data across environments,\nwe empirically ﬁnd an environment-speciﬁc maximum sparsity threshold ¯xmax for which SFP fails to reach\nmeaningful rewards. For each environment, we then evaluate all methods on a set of 6 sparsity coeﬃcients\nscaled linearly from 0 to ¯xmax, that are referred to as percentages in Figure 9 and 23. The maximum sparsity\nthresholds ¯xmax is 12 for HalfCheetah-v2, 2 for Ant-v2, Hopper-v2 and Swimmer-v2 and 1 for Walker-v2.\n23\nSwimmer-v2\nHalfCheetah-v2\nHopper-v2\nAnt-v2\nWalker2d-v2\nFigure 17: Overview of the diﬀerent gym-mujoco environments presented in Section 5.5.\nK\nImplementation Details\nK.1\nMetrics\nAll metrics reported in this paper (e.g. cumulative returns per episode) are averaged over 10 random seeds\n(excluding gym-mujoco experiments and visual RL experiments, which are respectively limited to 5 and 2\nseeds for computational reasons); mean learning curves are smoothed over 5 steps and shaded areas represent\nstandard deviation across seeds.\nWe rely on two metrics for measuring the quality of exploration trajectories in Section 5.3:\n• %Coverage divides the reachable state space in n cubic buckets (n = 1000 for reach and n = 100\nfor room) and reports the ratio between the number of buckets visited by a set of 20 trajectories of\nlength 500 and the total number of buckets.\n• Radius of Gyration Squared measures the spread in visited states, averaged over all trajectories,\nand is adapted from Amin et al. (2021). Given a set T of n trajectories, the metric can be computed\nas:\nU 2\ng (T) = 1\nδn\nX\nτ∈T\n1\n|τ| −1\nX\ns∈τ\nd2(s, ¯τ),\nwhere a trajectory τ is modeled as a sequence of states (si)|τ|−1\n0\n, d2(·, ·) measures the Euclidean\ndistance and ¯τ =\n1\n|τ|\nP\ns∈τ s.\nWe additionally normalize the metric by δ, which measures the\ndiagonal of the box containing reachable states.\nK.2\nData Collection\nTraining tasks for point-maze and meta-world are relatively simple, and allow collecting demonstrations\nthrough a scripted policy. Since these environments do not include obstacles and allow direct control over the\nagent position (in 2D for room and in 3D for reach), scripted policies simply receive the 2D/3D positions of\nthe agent and of its goal, and output a distance vector. This vector is then corrupted with isotropic Gaussian\nnoise and scaled to ﬁt within action limits. A simple implementation of scripted policies is available as part\nof our codebase (see URL in Footnote 1). For each training environment (reach or room), we collect 4000\ntrajectories of 500 steps each. Goals for the scripted policy are sampled uniformly from the reachable state\nspace. On goal achievement, a new goal is sampled, once again uniformly. In the context of gym-mujoco\nexperiments, 1000 trajectories of 500 steps each are instead collected by fully-trained SAC Haarnoja et al.\n(2018) agents. The same expert datasets are then used for training state-free priors, behavioral priors or\nbehavioral cloning.\nK.3\nBaselines\nSAC\nWe build upon the implementation provided by SpinningUp (Achiam, 2018), which is reported to be\nroughly on-par with the best results achieved on gym-mujoco (Brockman et al., 2016). For simplicity, we\ndo not use automatic entropy tuning and keep α constant during learning. Nonetheless, our method could\neasily tune α dynamically at the expense of increased complexity, as is described in Haarnoja et al. (2018).\n24\nTable 2: Hyperparameters for SAC.\nHyperparameter\nValue\nEpochs\n125\nSteps Per Epoch\n4e3\nSteps of Initial Exploration\n1e4\nSteps Before Training\n1e3\nEnvironment Steps per Iteration\n50\nγ\n0.99\nPolyak Averaging Rate\n0.995\nInverse of Reward Scale (α)\n0.2 for meta-world and gym-mujoco, 0.02 for point-maze\nBatch Size\n100\nOptimizer\nAdam\nβ1\n0.9\nβ2\n0.999\nLearning Rate\n0.001\nHidden Units\n256\nHidden Layers\n2\nHindsight Replay Ratio\n4\nReplay Size\n5e5 for vector-based RL, 2e5 for image-based RL\nMoreover, we introduce n-step returns (Hessel et al., 2018) with n = 10. We experimented with importance\nsampling for oﬀ-policy correction, but, similarly to what is reported by Hessel et al. (2018), we observed no\nempirical beneﬁt. All remaining hyperparameters are reported in Table 2. We anticipate that all baselines\nalso rely on n-step returns.\nSAC+BC\nBehavioral cloning (BC) is performed on the entire dataset D for 10 epochs, by maximizing\nthe log-likelihood of the Gaussian policy with respect to expert actions. The optimizer and batch size used\nfor BC are the same as for downstream learning.\nSAC-PolyRL\nSAC-PolyRL (Amin et al., 2021) replaces the initial uniform exploration phase of SAC with\ntrajectories collected by a hand-crafted policy. While SAC’s hyperparameters are unvaried, SAC-PolyRL\nspeciﬁc parameters are tuned from those reported in various settings in the original paper. We use θ = 0.35,\nσ2 = 0.017 and β = 0.01.\nPARROT\nAn oﬃcial implementation for PARROT (Singh et al., 2021) is not available at the time of\nwriting. We reproduce the training routine and downstream application and adopt all hyperparameters\nreported in the original paper. Generative models are trained until convergence (100 epochs) using a batch\nsize of 400 samples and Adam (Kingma & Ba, 2015) as an optimizer, with a learning rate of 0.0001, β1 = 0.9,\nβ2 = 0.999 and a weight decay penalty of 1e −6.\nIn addition to the image-based version of PARROT used in Section 5.5, we introduce a version with a\nvectorized state space, dubbed PARROT-state, since the method is originally only applicable in visual\nsettings.\nThe only modiﬁcation consists in replacing the image encoder with a 3-layer MLP with 256\nneurons per layer and ReLU activations. In this setting, the input to the encoder is therefore vector-based\n(non-visual).\nKL-regularization\nThe baseline method introduced in Section 5.2 replaces the entropy term in SAC\nwith a KL-regularization term encouraging the policy to math the learned prior, as done in Pertsch et al.\n(2020). Existing hyperparameters are inherited from SAC, and the prior is modeled as a Gaussian in order\nto compute KL-terms in closed form. The inverse of reward scale α was tuned in the range [2e−4, 2], and set\nto 2e−3.\n25\nK.4\nSFP\nPrior\nWe model all families of state-free priors with conditional Real NVP Flows (Ardizzone et al., 2019),\nsharing the same architecture across all experiments. The invertible transformation fθ is a composition of 6\ncoupling layers, each followed by a batch-normalization layer (Dinh et al., 2017). For each layer, a 3-layer\nMLP with 128 hidden units per layer is used to preprocess the conditioning input. Scale and transition\nnetworks are also implemented as a shared 3-layer MLP with 128 hidden units, whose output is split in\ntwo to provide scale and shift coeﬃcients. All MLPs use ReLU activations. We train our state-free priors\nfollowing the same protocol used with behavioral priors (100 epochs, a batch size of 400 samples and Adam\n(Kingma & Ba, 2015), with a learning rate of 0.0001, β1 = 0.9, β2 = 0.999 and a weight decay penalty of\n1e −6).\nDownstream Learning\nOur method does not introduce additional hyperparameters with respect to SAC,\nexcluding those involved with the design and optimization of the neural network parameterizing the mixing\nfunction Λω. This is implemented as a 2-layer ReLU network with 128 units per layer and a sigmoid for\noutput activation. We report that scaling the gradients for ω by a ﬁxed coeﬃcient ϵ was found beneﬁcial\nfor a stabler training of the mixing network (ϵ = 1e −9 for meta-world, point-maze and ϵ = 1e −7 for\ngym-mujoco). As exploration is mainly driven by the prior, SFP also relies on lower α parameters, namely\nα = 0.01 for meta-world, point-maze and α = 0.005 for gym-mujoco.\nFinally, we introduce two implementation choices motivated by the prior knowledge that, during early stages\nof training, exploration should mainly be driven by samples from the action prior ¯π.\nFirst, while SAC\nsamples action uniformly for the ﬁrst 10000 steps to encourage exploration, SFP directly samples actions\nfrom its prior instead. Second, we initialize the bias parameter of the ﬁnal layer of the mixing network Λω\nto σ−1(λ0) in order to target mixing weights around λ0 = 0.95 during early training.\nAll remaining hyperparameters are shared with SAC (see Table 2).\nTest-time Policy\nAt test time, instead of sampling an action from the mixture between prior ¯π and policy\nπ, the agent executes an action corresponding to the mean of the policy distribution. We note that this is\nconsistent to what is done in SAC Haarnoja et al. (2018).\nL\nDetailed Results\nThis section reports individual results were aggregated in previously occurring ﬁgures. In particular, Figure\n18 and Figure 19 report performance for each meta-world and point-maze task, respectively.\nFigure\n20 reports the evolution of average mixing weights as training progresses for all meta-world tasks (an\naggregation can be found in Figure 14). Figures 21 and 22 also report performance for the two suites, but\nfocus on visual RL settings. Figure 23 disentangles the results from Figure 9 across the ﬁve mujoco-gym\nenvironments.\nFor completeness, we also include plots for meta-world tasks that remain out of reach of all tested methods,\nnamely peg-insert-side, pick-place and push. In general, these tasks require both deep exploration\nand precise grasping and control. While SFP remains capable of deep exploration, precise manipulation\nconstitutes a challenge, as expert demonstrations do not contain grasping behavior (see Section 6). In visual\nsettings, due to increased complexity, we observed a drop in performance on reach, button-press and\ndoor-open across all methods.\n26\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nbutton-press\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\ndoor-open\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\ndrawer-close\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\ndrawer-open\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\npeg-insert-side\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\npick-place\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\npush\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nreach\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nwindow-close\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nwindow-open\nFigure 18: Learning curves for each downstream meta-world task.\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nroom\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\ncorridor\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nmaze\nFigure 19: Learning curves for each downstream point-maze task.\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\n0.0\n0.5\n1.0\nMixing Weight\nbutton-press\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\n0.0\n0.5\n1.0\nMixing Weight\ndoor-open\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\n0.0\n0.5\n1.0\nMixing Weight\ndrawer-close\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\n0.0\n0.5\n1.0\nMixing Weight\ndrawer-open\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\n0.0\n0.5\n1.0\nMixing Weight\npeg-insert-side\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\n0.0\n0.5\n1.0\nMixing Weight\npick-place\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\n0.0\n0.5\n1.0\nMixing Weight\npush\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\n0.0\n0.5\n1.0\nMixing Weight\nreach\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\n0.0\n0.5\n1.0\nMixing Weight\nwindow-close\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\n0.0\n0.5\n1.0\nMixing Weight\nwindow-open\nFigure 20: Evolution of mixing weight λ and test performance for each downstream meta-world task.\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nbutton-press\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\ndoor-open\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\ndrawer-close\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\ndrawer-open\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\npeg-insert-side\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\npick-place\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\npush\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nreach\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nwindow-close\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nwindow-open\nFigure 21: Learning curves for each downstream meta-world task in visual settings.\n27\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nroom\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\ncorridor\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nmaze\nFigure 22: Learning curves for each downstream point-maze task in visual settings.\nAnt-v2\nHalfCheetah-v2\nHopper-v2\nSwimmer-v2\nWalker2d-v2\n0\n0.1 1.0 10.0\nBias\n100%\n80%\n60%\n40%\n20%\nSparsity %\nPARROT-state\n0\n250\n500\n750\n1000\n0\n0.1 1.0 10.0\nBias\n100%\n80%\n60%\n40%\n20%\nSparsity %\nPARROT-state\n0\n250\n500\n750\n1000\n0\n0.1 1.0 10.0\nBias\n100%\n80%\n60%\n40%\n20%\nSparsity %\nPARROT-state\n0\n250\n500\n750\n1000\n0\n0.1 1.0 10.0\nBias\n100%\n80%\n60%\n40%\n20%\nSparsity %\nPARROT-state\n0\n250\n500\n750\n1000\n0\n0.1 1.0 10.0\nBias\n100%\n80%\n60%\n40%\n20%\nSparsity %\nPARROT-state\n0\n250\n500\n750\n1000\nFigure 23: Mean performance on each gym-mujoco environment for diﬀerent sparsity thresholds and bias norms.\n28\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n200\n400\nTest Return\nmeta-world (training)\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n200\n400\nTest Return\nmeta-world (downstream)\nFigure 24:\nComparison of downstream performance of action priors with diﬀerent conditioning variables from\nSubsection 5.3, aggregated over downstream tasks via IQM.\n0\n100\n200\n300\n400\nTest Return\nmeta-world (training)\n0\n100\n200\n300\n400\nTest Return\nmeta-world (downstream)\nFigure 25: Comparison of ﬁnal downstream performance of action priors with diﬀerent conditioning variables from\nSubsection 5.3, aggregated over downstream tasks via IQM.\nM\nAlternative Aggregation and Visualization\nIn this Section, we present an alternative aggregation scheme for key ﬁgures in the main paper. In particular,\nwe compute the IQM with 95% stratiﬁed basic boostrap conﬁdence interals as suggested in Agarwal et al.\n(2021). We further provide bar plots representing the ﬁnal performance for each ﬁgure. In particular, Figures\n24, 26, and 28 use IQM aggregation for the data used for Figures 4, 7, and 8 respectively. Figures 25, 27,\nand 29 report bar plots to represent the ﬁnal IQM over task performances in Figures 4, 7, and 8 respectively.\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\nmeta-world (training)\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\npoint-maze (training)\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n100\n200\n300\nTest Return\nmeta-world (downstream)\n0\n1\n2\n3\n4\n5\nSteps\n×105\n0\n100\n200\n300\n400\n500\nTest Return\npoint-maze (downstream)\nFigure 26: Comparison of SFP with several baselines from Subsection 5.4. Results are aggregated across tasks via\nIQM, instead of mean.\n29\n0\n200\n400\nTest Return\nmeta-world (training)\n0\n200\n400\nTest Return\npoint-maze (training)\n0\n100\n200\n300\nTest Return\nmeta-world (downstream)\n0\n100\n200\n300\n400\nTest Return\npoint-maze (downstream)\nFigure 27: Comparison of SFP with several baselines from Subsection 5.4. Final returns are aggregated across tasks\nvia IQM, instead of mean.\n0\n2\n4\nSteps\n×105\n0\n100\n200\nTest Return\nmeta-world (downstream)\n0\n2\n4\nSteps\n×105\n0\n200\n400\nTest Return\npoint-maze (downstream)\nFigure 28: Performance on downstream learning in vi-\nsual settings from Subsection 5.5, aggregated via IQM.\n0\n50\n100\nTest Return\nmeta-world (downstream)\n0\n200\n400\nTest Return\npoint-maze (downstream)\nFigure 29: Final performance on downstream learning\nin visual settings from Subsection 5.5, aggregated via\nIQM.\n30\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-05-26",
  "updated": "2022-08-31"
}