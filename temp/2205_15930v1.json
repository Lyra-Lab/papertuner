{
  "id": "http://arxiv.org/abs/2205.15930v1",
  "title": "Uzbek Sentiment Analysis based on local Restaurant Reviews",
  "authors": [
    "Sanatbek Matlatipov",
    "Hulkar Rahimboeva",
    "Jaloliddin Rajabov",
    "Elmurod Kuriyozov"
  ],
  "abstract": "Extracting useful information for sentiment analysis and classification\nproblems from a big amount of user-generated feedback, such as restaurant\nreviews, is a crucial task of natural language processing, which is not only\nfor customer satisfaction where it can give personalized services, but can also\ninfluence the further development of a company. In this paper, we present a\nwork done on collecting restaurant reviews data as a sentiment analysis dataset\nfor the Uzbek language, a member of the Turkic family which is heavily affected\nby the low-resource constraint, and provide some further analysis of the novel\ndataset by evaluation using different techniques, from logistic regression\nbased models, to support vector machines, and even deep learning models, such\nas recurrent neural networks, as well as convolutional neural networks. The\npaper includes detailed information on how the data was collected, how it was\npre-processed for better quality optimization, as well as experimental setups\nfor the evaluation process. The overall evaluation results indicate that by\nperforming pre-processing steps, such as stemming for agglutinative languages,\nthe system yields better results, eventually achieving 91% accuracy result in\nthe best performing model",
  "text": "Uzbek Sentiment Analysis based on local Restaurant\nReviews\nSanatbek Matlatipov1, Hulkar Rahimboeva1, Jaloliddin Rajabov1 and\nElmurod Kuriyozov2\n1National University of Uzbekistan named after Mirzo Ulugbek, 4 Universitet St, Tashkent, 100174, Uzbekistan\n2 Universidade da CoruÃ±a, CITIC, Grupo LYS, Depto. de ComputaciÃ³n y TecnologÃ­as de la InformaciÃ³n, Facultade de\nInformÃ¡tica, Campus de ElviÃ±a, A CoruÃ±a 15071, Spain\nAbstract\nExtracting useful information for sentiment analysis and classification problems from a big amount of\nuser-generated feedback, such as restaurant reviews, is a crucial task of natural language processing,\nwhich is not only for customer satisfaction where it can give personalized services, but can also influence\nthe further development of a company. In this paper, we present a work done on collecting restaurant\nreviews data as a sentiment analysis dataset for the Uzbek language, a member of the Turkic family\nwhich is heavily affected by the low-resource constraint, and provide some further analysis of the novel\ndataset by evaluation using different techniques, from logistic regression based models, to support vector\nmachines, and even deep learning models, such as recurrent neural networks, as well as convolutional\nneural networks. The paper includes detailed information on how the data was collected, how it was\npre-processed for better quality optimization, as well as experimental setups for the evaluation process.\nThe overall evaluation results indicate that by performing pre-processing steps, such as stemming for\nagglutinative languages, the system yields better results, eventually achieving 91% accuracy result in the\nbest performing model.\nKeywords\nSentiment Analysis, Uzbek Language, Dataset, Support Vector Machine, RNN, CNN,\n1. Introduction\nThe power of Natural Language Processing (NLP) techniques relies on amounts of labelled data\nin many applications. Sentiment analysis is the process of analyzing and labelling the opinion\nwhich is posted by consumers. Consumers usually post their feedback about places/foods to\nfamous applications such as Google Maps 1, Yelp2, etc). They often encourage consumers to\nactively participate in reviews, and massive user-generated restaurant reviews allow consumers\nto fully express their needs while helping merchants provide real-time and personalized service\nThe International Conference on Agglutinative Language Technologies as a challenge of Natural Language Processing\n(ALTNLP), June 6 â€“ 8, Koper, Slovenia\n\" s.matlatipov@nuu.uz (S. Matlatipov); h.rahimboyeva@nuu.uz (H. Rahimboeva); j.rajabov@nuu.uz (J. Rajabov);\ne.kuriyozov@udc.es (E. Kuriyozov)\n~ https://sanatbek.uz/ (S. Matlatipov)\n\u0012 0000-0002-6895-3436 (S. Matlatipov); 0000-0002-3259-7708 (H. Rahimboeva); 0000-0002-0369-6707 (J. Rajabov);\n0000-0003-1702-1222 (E. Kuriyozov)\nÂ© 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\nCEUR\nWorkshop\nProceedings\nhttp://ceur-ws.org\nISSN 1613-0073\nCEUR Workshop Proceedings (CEUR-WS.org)\n1Google Maps: https://www.google.com/maps\n2Yelp: https://www.yelp.com\narXiv:2205.15930v1  [cs.CL]  31 May 2022\n[1]. Moreover, the restaurant reviews express the composition of clientsâ€™ emotional necessities\nand are an important source of information about consumersâ€™ choices [2]. Currently, opinion\nmining has achieved very high accuracy performances, especially after applying deep learning\nmethods, for high resource languages [3]. However, applying deep learning and machine\nlearning techniques for different types of domains [4] and gathering corpora with high quantity\nplay an important[5] role in the development of low-resource languages. For example, the\nlanguage we focus on is the Uzbek language which is being used by around 34 million native\nspeakers in Uzbekistan and elsewhere in Central Asia and China3. Uzbek is a null-subject and\nhighly agglutinative language where one word can be a meaningful sentence[6, 7]. To our\nknowledge, there is no previous work for sentiment classification problems based on restaurant\ndomain feedback. So, the following contributions are considered for this paper.\nâ€¢ Restaurant domain annotated corpora is created for sentiment analysis which\nis collected from Google Maps based on Uzbek cuisineâ€™s locations where local national\nfood reviews are the primary target. The corpora contain 4500 positive and 3710 negative\nreviews after manually removing major errors and cleaning. The annotation process is\nbased on the feedbackâ€™s 5 stars method provided by Google Maps where from 1 to 3 we\nconsider the dataset as negative and from 4 to 5 as positive. We found some reviews are\nbased on other languages such as English, Kyrgyz and Russian. We didnâ€™t want to ignore\nthem, so we decided to translate them into Uzbek using the official Google Translate API.\nâ€¢ Pre-processing the corpora is applied in two steps. The first steps are removing\nURLs, punctuation, and lower-casing. The second step is ignoring stopwords[8] from the\ndataset where it is based on accuracy evaluation after generating the list of stop words\nusing the TF-IDF algorithm; Then, we applied the stemming algorithm [7, 9] which is\nbased on Uzbek wordsâ€™ endingsâ€™ electronic dictionary that uses combinatorial approach\ninferring apply for part of speech of the Uzbek language: nouns, adjectives, numerals,\nverbs, participles, moods, voices. Advantages of using the algorithm are lexicon-free and\nits complexity that allows one operation (referring to the dictionary of endings of the\nlanguage) to perform: segmentation of the word into suffixes; performs morphological\nanalysis of the word.\nâ€¢ Machine learning and deep learning algorithms have been applied. Furthermore,\ndeep learning(Recurrent neural network) algorithm fed with fastText4 pre-trained word\nembedding is applied to improve the accuracy;\nAll resources including the corpora, source code used for crawling techniques and classification\nalgorithms are uploaded to the public repository 5. The paper is structured as follows: Intro-\nduction(this section), Section 2 describes related work that has been done so far. It is followed\nby a description of the methodology in Section 3 and continues with Section 4, which focuses\non experiments and results. The final part (Section 6) concludes the paper and highlights the\nfuture work.\n3https://en.wikipedia.org/wiki/Uzbek_language\n4https://fasttext.cc/docs/en/crawl-vectors.html\n5https://github.com/SanatbekMatlatipov/restauranat-sentiment/tree/main\nFigure 1: Research framework.\n2. Related Work\nIn recent years, several works were done in the NLP field for Uzbek, including sentiment analysis\ndatasets [10, 11], created by collecting and analyzing Google Play app reviews, with two types\nof data: A medium-size manually annotated dataset and a larger-size dataset automatically\ntranslated from English. [12] obtained bilingual dictionaries for six Turkic languages and applied\nthem to cross-lingually align word embeddings, backed by a bilingual dictionary induction\nevaluation task. They showed that obtained aligned word embeddings from a low-resource\nlanguage can benefit from resource-rich closely-related languages. Another similar paper\n[13] investigated the effect of emoji-based features in opinion classification of Uzbek texts. A\nsemantic evaluation dataset was presented with semantic similarity and relatedness scores in\nword pairs as well as its analysis for Uzbek in a recent work [14]. There is a very recent growing\ntrend in NLP that makes use of AI-based techniques, which can be seen in the work on Uzbek\nwith neural transformers-architecture based language model trained off raw Uzbek corpus [15].\nIn a global outlook to the field of sentiment analysis, there is a work [16] that used various\nmethods of sentiment analysis techniques, such as machine learning and deep learning, in their\nwork with an idea to take into account the differences in opinions and thoughts that exist on\npopular social platforms such as Twitter, Reddit, Tumblr and Facebook.\n3. Methodology\nIn this paper, we proposed a machine learning and deep learning-based sentiment analysis\nframework for the restaurant domain dataset (Figure 1). The framework includes data collection\nusing web-crawler, pre-processing(cleaning, stopwords, lexicon-free stemming), constructing\nTF-IDF weight matrix, performing ML and DL for sentiment analysis;\nFigure 2: Feedback sample\n3.1. Data collection\nWe start by looking at a high number of the dataset available for crawling in the Uzbek language.\nHowever, the usual approaches such as Twitter or movie reviews are not the case for Uzbek.\nTherefore, we decided to collect restaurant reviews as local people mostly loved giving feedback\nwhich is restaurants. we think it makes sense as Uzbek cuisines are one of the most popular\nthroughout the Commonwealth of Independent States (CIS, CA countries). In most CA cities,\nfor instance, itâ€™s easy to find busy restaurants specializing in Uzbek cuisine6. We crawled all\nlocal restaurants in Tashkent from Google Maps. Firstly, we selected a list of more than 140\nURLs which has at least 3 reviews and we retrieved all info shown in Figure 2. While crawling,\nwe considered Googleâ€™s anti-spam and anti-DDOS policies as there are certain limitations on\nharvesting data. The source code is available on the repository.\n3.2. Data pre-processing\nThe collection of texts with star ratings in the crawled dataset was noisy and required manual\ncorrection. The comments containing only emojis, names or any other irrelevant content, such\nas username mentions, URLs or specific app names were removed. Those written in languages\ndifferent from Uzbek (mostly in Russian and some in English) were translated using the official\nGoogle translate API. Although people in Uzbekistan use the official Latin alphabet, the use of\nthe old Cyrillic alphabet is equally popular, especially among adults. The comments that were\nwritten in Cyrillic were converted to Latin using the Uzbek machine transliteration tool [17].\nThen, we applied stop words to remove low-level information words from our comments to focus\non important information. The technique is based on [8] paper where it is a proposed algorithm\nof automatic detection of single word stop words collection using TFIDF(Term frequency -\ninverse document frequency). After that, each word is processed to lexicon-free stemming tool\n[7] algorithm for decreasing the word capacity because of prefixes and suffixes. The basic idea\nis using the combinatorial approach of eligible endings candidates. Following table 1 shows\nprocessed data which is ready for TFIDF-vectorizer.\nWe selected a set of words to visualize the word count. Figure 3 shows that people tend to\ngive more positive feedback than negative on the domain of restaurants.\n6BBc Travel: https://www.bbc.com/travel/article/20191117-is-uzbek-cuisine-actually-to-die-for\nTable 1\nThe example of a chosen review before and after processing it.\nReview\nAfter processing\nBirinchi Milliy taomlardan biri - keng assortimentli taomlar!\nGastro-turistlar uchun juda jozibali joy - bu yerda barcha\nturdagi milliy taomlar mavjud.Yagona salbiy tomoni shundaki,\nbunday yirik muassasa uchun toâ€™xtash joyi kichik. Narxlar\nnisbatan arzon! Turistlar uchun juda arzon!\nBir/ milliy/ taom/ keng/ assortiment/\ntaom/ gastro/ turist/ juda/ joziba/ joy/\ntur/ milliy/ taom/ mavjud/ salbiy/ tomon/\nyirik/ muassa/ toâ€™xta/ joy/ kichik/ narx/\narzon/ turist/ arzon\nFigure 3: The visualisation of some selected examples of Uzbek words taken from positive and negative\nreviews with their log counts.\n4. Evaluation\nThe collected novel dataset has been split into training and testing subsets for evaluation with 8\nx 2 ratio respectively. After the data cleaning process, we have the original dataset as follows,\nwhere ğ‘¥ğ‘–âƒ—represents feature vectors and ğ‘¦ğ‘–âƒ—represents annotated labels:\n(ğ‘¥ğ‘–âƒ—, ğ‘¦ğ‘–),\nğ‘–= 1, 2, 3, ..., ğ‘\n(1)\nğ‘¥ğ‘–âƒ—= (ğ‘¥ğ‘–1, ğ‘¥ğ‘–2, ..., ğ‘¥ğ‘–ğ‘š)\nğ‘–= 1, 2, 3, ..., ğ‘\n(2)\nğ‘and ğ‘šis equal to the number of reviews and length of the feature vector, respectively.\nThen we calculate TFIDF scores for each feature vector ğ‘¥ğ‘–âƒ—which vectorises words by taking\ninto account the frequency of a word in a given review and the frequency between reviews.\nThe final result of all ğ‘§ğ‘–âƒ—s is defined as a sparse matrix.\nğ‘§ğ‘–âƒ—= ğ‘‡ğ¹(ğ‘¥ğ‘–)ğ‘¥ğ¼ğ·ğ¹(ğ‘¥ğ‘–)\nğ‘–= 1, 2, 3, ..., ğ‘\n(3)\n4.1. Machine learning algorithms\nThe \\Logistic regression model is\nâ„(ğ‘§âƒ—) = 1/(1 + exp(âˆ’ğ‘§))\n(4)\nğ‘ƒ(ğ‘¦|ğ‘§âƒ—) =\n{ï¸ƒ\nâ„(ğ‘§âƒ—),\nif ğ‘¦= +1(ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’)\n1 âˆ’â„(ğ‘§âƒ—),\nif ğ‘¦= âˆ’1(ğ‘›ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’)\nLogistic regression[18] model is a classification algorithm, known for its exponential and log-\nlinear functions. It works with discrete values and maps the function of any real value into 0\nand 1. For sentiment analysis, the hypothesis shows, reviews are either positive or negative by\nusing the (4). The Support Vector Machine(SVM) model has the following response function:\nâ„(ğ‘§âƒ—) = ğ‘ ğ‘–ğ‘”ğ‘›(ğ‘§âƒ—)\n(5)\nSVM algorithm is known for its fast and dependable classification which resolves two-group\nclassification problems. The classification is conducted for finding a hyperplane between\ntwo classesâ€™ positive and negative reviews in the model: After all, we implemented LR and\nSVM models utilizing the Scikit-Learn [19] machine learning library in Python with default\nconfiguration parameters. For the LR models, we implemented a variant based on word n-grams\n(unigrams and bigrams), and one with character n-grams (with ğ‘›ranging from 1 to 4). We also\ntested a model combining said word and character n-gram features.\n4.2. Deep Learning algorithms\nKeras [20] is used on top of TensorFlow [21].The FastText pre-trained word embeddings of size\n300 [22] for the Uzbek language are applied. For the CNN model, we used a multi-channel CNN\nwith 256 filters and three parallel channels with kernel sizes of 2,3 and 5, and drop out of 0.3.\nThe output of the hidden layer is the concatenation of the max-pooling of the three channels.\nFor RNN, we use a bidirectional network of 100 GRUs. The output of the hidden layer is the\nconcatenation of the average and max-pooling of the hidden states. For the combination of deep\nlearning models, we stacked the CNN on top of the GRU. In the three cases, the final output is\nobtained through a sigmoid activation function [23] applied to the previous layer. In all cases,\nAdam optimization algorithm [24], an extension of stochastic gradient descent, was chosen\nfor training, with standard parameters: learning rate ğ›¼= 0.0001 and exponential decay rates\nğ›½1 = 0.9 and ğ›½2 = 0.999. Binary cross-entropy was used as a loss function. The same steps,\nbut slightly different parameters were used in a work that presents guidance to use CNN for\nsentiment classification [25]. Inspired by their example that perfectly illustrates the steps of\nperforming deep learning based sentiment classification using CNN, the visualisation of our\nsteps can be seen in Figure 4.\nFigure 4: The illustration of steps taken in deep learning based sentiment classification using CNN,\ninspired by [25].\n4.3. Evaluation metrics\nConfusion[26] matrices are used in the task to determine the gap between predicted and true\nvalues which is shown in Table 2. Precision, Recall and F1-score are used as evaluation metrics\nfor model performance.\nTable 2\nConfusion matrix\nClasses\nPositive\nNegative\nPositive\nTrue Positive(TP)\nFalse Negative(FN)\nNegative\nFalse Positive(TP)\nTrue Negative(FN)\nThe calculation of Precision and Recall is shown below:\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=\nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ\nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™=\nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ ğ¹ğ‘\n(6)\nThe F1-Score is used, which takes into account both accuracy and recall, and the F1-Score is\ncalculated as follows:\nğ¹1 = 2 * ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›* ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™\n(7)\n5. Results and Discussion\nThis section presents a detailed description of the results obtained by the evaluation process\nusing both machine learning and deep learning techniques applied to the collected novel\nsentiment analysis dataset of restaurant reviews.\n5.1. Experiment Results\nThe overall experiment results of the above-mentioned evaluation were performed, and the\nresults can be seen in Table 3.\nTable 3\nExperiment results of sentiment analysis for all evaluation techniques, including models, their distinctive\nparameters, as well as evaluation metrics, such as Precision (Prec.), Recall (Rec.), F1-score (F1), as well\nas Accuracy (Acc.).\nModel name + Parameters\nSentiment\nPrec.\nRec.\nF1\nAcc.\nLogistic Regression based on word n-grams\nPositive\n88%\n98%\n93%\n89%\nNegative\n88%\n67%\n74%\nLogistic Regression based on char. n-grams\nPositive\n87%\n51%\n92%\n87%\nNegative\n83%\n97%\n64%\nLogistic Regression (word + char. n-grams)\nPositive\n95%\n95%\n92%\n91%\nNegative\n90%\n89%\n90%\nSVM based on linear kernel\nPositive\n88%\n97%\n92%\n88%\nNegative\n84%\n71%\n80%\nRNN without word embeddings\nPositive\n90%\n95%\n92%\n88%\nNegative\n78%\n64%\n70%\nRNN with word embeddings\nPositive\n90%\n95%\n93%\n88%\nNegative\n80%\n65%\n72%\nCNN (multichannel)\nPositive\n90%\n96%\n93%\n89%\nNegative\n83%\n64%\n72%\nThe Logistic Regression(LR) based on word n-grams obtained a binary classification accuracy\nof 90% on the dataset, while the one based on character n-grams, with its better handling\nof misspelt words, improved it to 91%(which is the winner of this comparison). Support\nVector machines based on Linear kernel mode have shown 88% accuracy overall. Recurrent\nNeural network models without and with fastText embedding show the same accuracy (88%).\nConvectional Neural Network showed slightly less performance(89.23%) than LR. However,\nthis is the reason for lacking data for neural-network models, as it requires big data for better\nperformance.\n5.2. Discussion and limitations\nNowadays, unstructured data are becoming more and more in the restaurant domain which\nrequires performing high accuracy sentiment analysis. Especially, this is the case for low-\nresource languages. Based on the review data of Google Maps(Tashkent location) which is\nobtained by web-crawling, the paper has shown several ML& DL methods. It was observed that\nthe LR algorithm outperforms the others which makes sense as our dataset is relatively small.\nThe research also mentioned some theoretical and practical implications. We believe, in terms\nof gaining massive user reviews on the domain can provide consumers make their decision in\nthe best manner such as lower cost and faster speed. However, we also wanted to point out\nsome limitations in this research paper. The dataset we gathered has an unbalanced number\nof positive and negative reviews, which can cause deviations in the result. Moreover, we used\nthe review rating in the annotation process which sometimes, in reality, consumers may give a\nhigh rating score, but polarity context may be related to negative, and vice versa.\n6. Conclusion\nIn this paper, we have shown a novel dataset in the restaurant domain for the Uzbek language,\nwith 8210 reviews, annotated with positive or negative labels, which is crawled from Google\nMaps using URLs of all locations in the capital city Tashkent, and was labelled as their corre-\nsponding star score. Then, we applied full pre-processing steps to the dataset which contributed\nto increasing the accuracy of our baseline models. Further analysis of the collected dataset was\nshown with evaluations using both machine learning and deep learning techniques. The best\naccuracy result (91%) on the dataset was obtained using a logistic regression model with word\nand character n-grams.\nIn the foreseen future, we are planning to extend the work by collecting more data, which\ncan effectively analyze the restaurant reviews in a practical level. Also, the work is underway\nto remove the evaluation bias of the training experiments by using cross-validation methods in\ndata splitting.\nAcknowledgments\nThis work partially has received funding from ERDF/MICINN-AEI (SCANNER-UDC, PID2020-\n113230RB-C21), and from Centro de InvestigaciÃ³n de Galicia â€CITICâ€, funded by Xunta de\nGalicia and the European Union (ERDF - Galicia 2014-2020 Program), by grant ED431G 2019/01.\nElmurod Kuriyozov was funded for his PhD by El-Yurt-Umidi Foundation under the Cabinet of\nMinisters of the Republic of Uzbekistan.\nReferences\n[1] R. A.-I. Rafael Anaya-SÃ¡nchez, Sebastian Molinillo, F. LiÃ©bana-Cabanillas, Improving\ntravellersâ€™ trust in restaurant review sites, Tourism Review 74 (2019) 830â€“840. doi:10.\n1108/TR-02-2019-0065.\n[2] E. Marine-Roig, S. A. Clave, A method for analysing large-scale UGC data for tourism:\nApplication to the case of catalonia, in: Information and Communication Technologies in\nTourism 2015, Springer International Publishing, Cham, 2015, pp. 3â€“17.\n[3] J. Barnes, R. Klinger, S. S. i. Walde, Assessing state-of-the-art sentiment models on state-\nof-the-art sentiment datasets, arXiv preprint arXiv:1709.04219 (2017).\n[4] L. Zhang, S. Wang, B. Liu,\nDeep learning for sentiment analysis: A survey,\nWiley\nInterdiscip. Rev. Data Min. Knowl. Discov. 8 (2018). URL: https://doi.org/10.1002/widm.1253.\ndoi:10.1002/widm.1253.\n[5] M. Artetxe, I. Aldabe, R. Agerri, O. Perez-de ViÃ±aspre, A. Soroa, Does corpus quality\nreally matter for low-resource languages?, 2022. URL: https://arxiv.org/abs/2203.08111.\ndoi:10.48550/ARXIV.2203.08111.\n[6] G. Matlatipov, Z. Vetulani, Representation of Uzbek morphology in prolog, in: Aspects of\nNatural Language Processing. Lecture Notes in Computer Science, volume 5070, Springer,\n2009.\n[7] S. Matlatipov, U. Tukeyev, M. Aripov, Towards the uzbek language endings as a language\nresource, in: M. Hernes, K. Wojtkiewicz, E. Szczerbicki (Eds.), Advances in Computational\nCollective Intelligence, Springer International Publishing, Cham, 2020, pp. 729â€“740.\n[8] K. Madatov, S. Bekchanov, J. ViÄiÄ, Automatic detection of stop words for texts in the\nuzbek language, 2022.\n[9] U. Tukeyev, A. Turganbayeva, B. Abduali, D. Rakhimova, D. Amirova, A. Karibayeva,\nLexicon-free stemming for kazakh language information retrieval, in: 2018 IEEE 12th\nInternational Conference on Application of Information and Communication Technologies\n(AICT), 2018, pp. 1â€“4. doi:10.1109/ICAICT.2018.8747021.\n[10] I. Rabbimov, S. Kobilov, I. Mporas, Opinion classification via word and emoji embedding\nmodels with lstm, in: International Conference on Speech and Computer, Springer, 2021,\npp. 589â€“601.\n[11] E. Kuriyozov, S. Matlatipov, Building a new sentiment analysis dataset for uzbek language\nand creating baseline models, in: Multidisciplinary Digital Publishing Institute Proceedings,\nvolume 21, 2019, p. 37.\n[12] E. Kuriyozov, Y. Doval, C. GÃ³mez-RodrÃ­guez, Cross-lingual word embeddings for Turkic\nlanguages, in: Proceedings of the 12th Language Resources and Evaluation Conference,\nEuropean Language Resources Association, Marseille, France, 2020, pp. 4054â€“4062. URL:\nhttps://aclanthology.org/2020.lrec-1.499.\n[13] I. Rabbimov, I. Mporas, V. Simaki, S. Kobilov, Investigating the effect of emoji in opinion\nclassification of uzbek movie review comments, in: A. Karpov, R. Potapova (Eds.), Speech\nand Computer, Springer International Publishing, Cham, 2020, pp. 435â€“445.\n[14] U. Salaev, E. Kuriyozov, C. GÃ³mez-RodrÃ­guez, Simreluz: Similarity and relatedness scores\nas a semantic evaluation dataset for uzbek language, arXiv preprint arXiv:2205.06072\n(2022).\n[15] B. Mansurov, A. Mansurov, Uzbert: pretraining a bert model for uzbek, arXiv preprint\narXiv:2108.09814 (2021).\n[16] Y. Chandra, A. Jana, Sentiment analysis using machine learning and deep learning, in:\n2020 7th International Conference on Computing for Sustainable Global Development\n(INDIACom), 2020, pp. 1â€“4. doi:10.23919/INDIACom49435.2020.9083703.\n[17] U. Salaev, E. Kuriyozov, C. GÃ³mez-RodrÃ­guez, A machine transliteration tool between\nuzbek alphabets, arXiv preprint arXiv:2205.09578 (2022).\n[18] E. Christodoulou, J. Ma, G. S. Collins, E. W. Steyerberg, J. Y. Verbakel, B. Van Calster,\nA systematic review shows no performance benefit of machine learning over logistic\nregression for clinical prediction models, Journal of Clinical Epidemiology 110 (2019) 12â€“22.\nURL: https://www.sciencedirect.com/science/article/pii/S0895435618310813. doi:https:\n//doi.org/10.1016/j.jclinepi.2019.02.004.\n[19] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,\nP. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,\nM. Perrot, E. Duchesnay, Scikit-learn: Machine learning in Python, Journal of Machine\nLearning Research 12 (2011) 2825â€“2830.\n[20] F. Chollet, et al., Keras, https://github.com/fchollet/keras, 2015.\n[21] M. Abadi, et al., TensorFlow: Large-scale machine learning on heterogeneous systems,\n2015. URL: https://www.tensorflow.org/, software available from tensorflow.org.\n[22] E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov, Learning word vectors for 157\nlanguages, in: Proceedings of the International Conference on Language Resources and\nEvaluation (LREC 2018), 2018.\n[23] A. C. Marreiros, J. Daunizeau, S. J. Kiebel, K. J. Friston, Population dynamics: variance and\nthe sigmoid activation function, Neuroimage 42 (2008) 147â€“157.\n[24] D. P. Kingma, J. Ba,\nAdam: A method for stochastic optimization,\narXiv preprint\narXiv:1412.6980 (2014).\n[25] Y. Zhang, B. Wallace, A sensitivity analysis of (and practitionersâ€™ guide to) convolutional\nneural networks for sentence classification, arXiv preprint arXiv:1510.03820 (2015).\n[26] M. Sokolova, G. Lapalme, A systematic analysis of performance measures for classi-\nfication tasks, Information Processing & Management 45 (2009) 427â€“437. URL: https:\n//www.sciencedirect.com/science/article/pii/S0306457309000259. doi:https://doi.org/\n10.1016/j.ipm.2009.03.002.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2022-05-31",
  "updated": "2022-05-31"
}