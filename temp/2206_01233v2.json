{
  "id": "http://arxiv.org/abs/2206.01233v2",
  "title": "Equivariant Reinforcement Learning for Quadrotor UAV",
  "authors": [
    "Beomyeol Yu",
    "Taeyoung Lee"
  ],
  "abstract": "This paper presents an equivariant reinforcement learning framework for\nquadrotor unmanned aerial vehicles. Successful training of reinforcement\nlearning often requires numerous interactions with the environments, which\nhinders its applicability especially when the available computational resources\nare limited, or when there is no reliable simulation model. We identified an\nequivariance property of the quadrotor dynamics such that the dimension of the\nstate required in the training is reduced by one, thereby improving the\nsampling efficiency of reinforcement learning substantially. This is\nillustrated by numerical examples with popular reinforcement learning\ntechniques of TD3 and SAC.",
  "text": "Equivariant Reinforcement Learning for Quadrotor UAV\nBeomyeol Yu and Taeyoung Lee\nAbstract— This paper presents an equivariant reinforcement\nlearning framework for quadrotor unmanned aerial vehicles.\nSuccessful training of reinforcement learning often requires\nnumerous interactions with the environments, which hinders\nits applicability especially when the available computational\nresources are limited, or when there is no reliable simulation\nmodel. We identiﬁed an equivariance property of the quadrotor\ndynamics such that the dimension of the state required in the\ntraining is reduced by one, thereby improving the sampling\nefﬁciency of reinforcement learning substantially. This is il-\nlustrated by numerical examples with popular reinforcement\nlearning techniques of TD3 and SAC.\nI. INTRODUCTION\nDeep reinforcement learning (DRL) algorithms have been\nsuccessfully applied to optimal control of complex dynamic\nsystems through an interactive learning process. They have\nmade remarkable advancements in various applications such\nas games [1] or natural language processing [2]. Recently,\ntheir application domain has been extended to robotics,\nincluding unmanned aerial systems, such as drone racing [3]\nand payload transportation of quadrotors [4]. Compared to\ntraditional model-based control methods [5], [6], [7], DRL-\nbased control does not require any exact mathematical model\nto achieve its goals.\nPrior works in reinforcement learning of quadrotors have\nmainly focused on introducing DRL-based control strategies\nfor hovering and trajectory tracking tasks [8], [9]. For\nexample, to enhance tracking accuracy, a stochastic policy\nhas been trained in [10] with Proximal Policy Optimization\n(PPO) for an on-policy method, and Twin Delayed Deep De-\nterministic Policy Gradient (TD3) has been adopted in [11]\nto develop a deterministic policy as an off-policy technique.\nIn [12], Deep Deterministic Policy Gradients (DDPG) have\nbeen used for autonomous landing on a moving platform,\nwhich has been validated by simulation and ﬂight experi-\nments. In the study of [13] and [14], the authors have focused\non mitigating the reality gap that appears when transferring\nthe policy trained in simulation into the real world, while\ngreatly improving robustness. However, since DRL-based\ncontrol is a data-driven approach relying on deep neural\nnetworks, most of their recent successes have faced the\nchallenge of handling complex and high-dimensional data.\nThis process often requires numerous samples for successful\nlearning, thereby degrading its efﬁciency in both computation\nand learning.\nBeomyeol\nYu,\nTaeyoung\nLee,\nMechanical\nand\nAerospace\nEn-\ngineering,\nGeorge\nWashington\nUniversity,\nWashington,\nDC\n20051\n{yubeomyeol,tylee}@gwu.edu\n∗This research has been supported in part by NSF under the grants CNS-\n1837382 and CMMI-1760928.\n⃗e1\n⃗e2\n⃗e3\n⃗b1\n⃗b2\n⃗b3\nx ∈R3\nR ∈SO(3)\nT1\nT2\nT3\nT4\nFig. 1.\nQuadrotor Model\nA popular way to address this issue in deep learning\nis equivariant neural network, which is part of a broad\ntheme of geometric deep learning [15]. Equivariant models\ncan improve sample efﬁciency and generalization capability\nby directly utilizing the geometric relationship between the\ninput and output data, such as in translation, rotation, or\npermutation. The concept of equivariant learning was ﬁrst\nproposed in [16], and it has been actively adopted in com-\nputer vision [17]. In reinforcement learning, reﬂectional and\nrotational equivariance have been utilized in the formulation\nof homomorphic networks for discrete actions [18], [19].\nAnother approach has employed an equivariant architecture\nfor vision-based robotic manipulation in Q-learning and\nactor-critic methods [20].\nIn this paper, we propose an equivariant framework of\nreinforcement learning for quadrotor low-level control, which\ndirectly maps the state of the quadrotor to motor control sig-\nnals. Particularly, we identify a rotational symmetry, where\nthe optimal control represented in the body-ﬁxed frame\nis invariant under the rotation about the gravity direction.\nBy embedding this particular structure in an actor-critic\narchitecture, the dimension of the sample trajectories used\nin the training is reduced by one, thereby improving the\nsampling efﬁciency substantially. Data efﬁciency is partic-\nularly important in aerial robotics with a large-dimensional\ncontinuous state-action space. Further, as quadrotors are\ninherently unstable, it is critical to safely complete learning\nwith a minimal number of trials. We compare our agents\ntrained with the proposed equivariant framework with non-\nequivariant counterparts in TD3 and SAC, to show signiﬁcant\ncomputational advantages.\nIn short, our main contributions are two-fold. First, a\ndata-efﬁcient reinforcement learning scheme is proposed for\nquadrotors where the dimension of sample trajectories is\nreduced by one. Second, it is shown that the proposed\narXiv:2206.01233v2  [cs.LG]  26 Feb 2023\nframework successfully improves the convergence of rein-\nforcement learning through numerical simulations.\nII. PROBLEM FORMULATION\nWe are interested in solving the problem of quadrotor low-\nlevel control. This section provides a theoretical background\nin quadrotor dynamics and reinforcement learning.\nA. Quadrotor Dynamics\nConsider a quadrotor unmanned aerial vehicle, illustrated\nat Fig. 1. Let {⃗e1,⃗e2,⃗e3} be the axes of the inertial frame,\nwhere the third axis ⃗e3 is aligned along the gravity pointing\ndownward. And let {⃗b1,⃗b2,⃗b3} be the body-ﬁxed frame\nlocated at the mass center of the quadrotor. The ﬁrst two axes\nare aligned toward the center of the corresponding rotors,\nsuch that the third axis points downward when hovering.\nThe position and the velocity of the quadrotor in the inertial\nframe are denoted by x ∈R3 and v ∈R3, respectively.\nThe attitude is deﬁned by the rotation matrix R ∈SO(3) =\n{R ∈R3×3 | RT R = I3×3, det[R] = 1}, which is the linear\ntransformation of the representation of a vector from the\nbody-ﬁxed frame to the inertial frame. The angular velocity\nvector resolved in the body-ﬁxed frame is Ω∈R3.\nThe equations of motion for the quadrotor are given by\n˙x = v,\n(1)\nm˙v = mge3 −fRe3,\n(2)\n˙R = RˆΩ,\n(3)\nJ ˙Ω+ Ω× JΩ= M,\n(4)\nwhere the hat map ˆ· : R3 →so(3) = {S ∈R3×3 | ST =\n−S} is deﬁned by the condition that ˆxy = x × y and ˆx is\nskew-symmetric for any x, y ∈R3. The inverse of the hat\nmap is denoted by the vee map ∨: so(3) →R3 Also, m ∈R\nand J ∈R3×3 are the mass, and the inertia matrix of the\nquadrotor with respect to the body-ﬁxed frame, respectively,\nand g ∈R is the gravitational acceleration. From the thrust\nof each motor denoted by (T1, T2, T3, T4), the total thrust\nf = P4\ni=1 Ti ∈R and the total moment M ∈R3 resolved\nin the body-ﬁxed frame can be computed by\n\n\nf\nM1\nM2\nM3\n\n=\n\n\n1\n1\n1\n1\n0\n−d\n0\nd\nd\n0\n−d\n0\ncτf\n−cτf\ncτf\n−cτf\n\n\n\n\nT1\nT2\nT3\nT4\n\n,\n(5)\nwhere d ∈R is the distance between the center of any rotor\nand the third body-ﬁxed axis, and cτf ∈R is a constant\nrelating the thrust and the resulting reactive torque.\nB. Markov Decision Process\nMarkov decision process (MDP) is an extension of Markov\nchains augmented by actions and rewards, which describe the\nchoices available for each state and the objective to achieve.\nSpeciﬁcally, it is deﬁned by a tuple M = (S, A, R, T , γ),\nwhere S is the state space, A is the action space, and R :\nS×A →R is the reward function. Next, T : S × A →P(S)\ndenotes the state transition probability. For example, in a\ndiscrete-time setting, it is speciﬁed by P(st+1|st, at), i.e.,\nthe distribution of the state at the next time step, for the\ngiven state and action at the current step. At each time\nstep t, the agent takes an action at drawn from a policy\nπ(at|st), which is the distribution of the action conditioned\nby the state, receives a reward rt, and a next state is\ndetermined by the transition probability function. This se-\nquence of state-action pairs is called a trajectory or rollout\nτ = (s0, a0, s1, a1, · · · , sT , aT ). The goal of MDP is to\nidentify an optimal policy π∗(ak|sk) that maximizes the\nexpected return Jt = P∞\nk=0 γkrt+k+1 where γ ∈[0, 1] is a\ndiscount factor. Reinforcement learning addresses MDP by\nconstructing a policy iteratively while interacting with the\ndynamic system.\nC. Reinforcement Learning for Quadrotor\nIn this paper, the control objective is to ﬁnd an optimal\npolicy such that for a given desired position xd ∈R3,\nx →xd as t →∞. This is formulated as MDP as follows.\nThe state and the action of the quadrotor are given by s =\n(x, v, R, Ω) ∈S = R9 × SO(3) and a =(T1, T2, T3, T4) ∈\nA = R4, respectively. The state transition probability is\ndetermined by the equations of motion (1)–(3), which can\nbe discretized according to a numerical integration scheme.\nNext, to achieve the above stabilization objective, the reward\nfunction is deﬁned as\nrt(st, at) = cx(1 −∥e′\nxt∥) −cv∥vt∥−cΩ∥Ωt∥\n−ca∥at −at−1∥.\n(6)\nwhere the constants c are positive weighting factors, and\ne′\nxt = (xt −xd)/exmax ∈R3 is the position error nor-\nmalized such that ∥e′\nx∥≤1 always. More speciﬁcally, it\nis assumed that the i-th element of x is in the domain of\n[xdi −exmax, xdi + exmax] with a prescribed exmax > 0 for\ni ∈{1, 2, 3}, and any rollout is terminated once it is violated.\nIn (6), the ﬁrst term is to minimize the scaled position error,\nand the next two terms are to mitigate aggressive motions.\nThe last term is to discourage chattering in control inputs,\nwhere it is considered that the prior action at−1 is prescribed\nat the t-th step with the convention of a−1 = a0.\nThe presented MDP for the quadrotor can be addressed\nby any reinforcement learning schemes that can handle\ncontinuous state spaces and action spaces, such as Twin\nDelayed Deep Deterministic Policy Gradient (TD3) [21], and\nSoft Actor-Critic (SAC) [22]. While these algorithms have\nbeen successfully applied in various challenging applications,\nthey often require a massive amount of data to train their\nagents successfully. This motivates the proposed equivariant\nreinforcement learning as presented below.\nIII. S1–EQUIVARIANT REINFORCEMENT LEARNING FOR\nQUADROTOR\nIn this section, we present a symmetry property of the\nquadrotor and we discuss how it yields an equivariance\nproperty to be utilized in enhancing the sampling efﬁciency\nof reinforcement learning.\n⃗e2\n⃗e3\nφ\nT4\nT4\nT2\nT2\n(s, a)\n(GS(s), GA(a))\n⃗b2\n⃗b3\n⃗b2\n⃗b3\nFig. 2.\nA control problem of planar quadrotors\nTo illustrate the key idea, we ﬁrst consider a planar quadro-\ntor shown in Figure 2, which is conﬁned to the plane spanned\nby ⃗e2 and ⃗e3. Here, the state and action of the quadrotor\n(on the left) are given by s = (x2, x3, v2, v3, φ, ˙φ) ∈R6\nand a = (T2, T4) ∈R2, where φ and\n˙φ are the roll\nangle and the angular velocity, respectively. If we ﬂip the\nquadrotor horizontally to the right side, more precisely by\nthe state transform GS(s) = (−x2, x3, −v2, v3, −φ, −˙φ) and\nthe action transform GA(a) = (T2, T4), then both systems\nexhibit the same dynamic characteristics, except that they are\non the opposite side. As such, the optimal action on the left\ncan be transformed to the right side, and vice versa, thereby\nreducing the domain of the state space to be considered\nfor optimization into half. In other words, the above planar\nquadrotor is symmetric with respect to a horizontal ﬂip,\nand the symmetry yields certain equivariance properties in\nthe value function and the optimal action, to be exploited\nfor improving sample efﬁciency. This idea is more formally\ndeveloped as follows.\nA. S1–Symmetry of Quadrotor\nMore generally, we show that the quadrotor dynamics\npresented in Section II is symmetric with respect to the\nrotation about the vertical axis. Speciﬁcally, consider the\nfollowing group action of S1 = {q ∈R2 | ∥q∥= 1}\nparameterized by θ ∈(−π, π]) 1:\nGS(s; θ) = (exp(θˆe3)x, exp(θˆe3)v, exp(θˆe3)R, Ω),\n(7)\nGA(a) = a.\n(8)\nIn\nother\nwords,\nthe\ngroup\naction\ngθ(s, a)\n=\n(GS(s; θ), GA(a))\ncorresponds\nto\nthe\nrotation\nof\nthe\ncomplete system by the angle θ about ⃗e3. In (7) and (8), the\nangular velocity Ωand the action a seem to be unchanged.\nHowever, this is because both Ωand a are resolved in\nthe body-ﬁxed frame. For example, the angular velocity\nis rotated from ω = RΩinto exp(θˆe3)RΩ= exp(θˆe3)ω\nby the group action when perceived with respect to the\ninertial frame. From now on, the group actions on the state\nand the action are denoted by the same symbol gθ, i.e.,\nGS(s; θ) = gθs and GA(a) = gθa.\nNext, we show that the quadrotor dynamics is symmetric\nwith respect to the group action gθ.\n1To avoid any confusion with the action a of MDP, this is referred to as\ngroup action.\n(s, a)\n(˜s, ˜a)\ngθ =\n(GS, GA)\n⃗e1\n⃗e2\nθ\ngoal\nb1\nb2\n˜b1\n˜b2\nFig. 3.\nIllustration of the group action corresponding to the rotation about\n⃗e3\nProposition 1: Let the equations of motion (1)–(4) be\nconsolidated into\n˙s = F(s, a),\n(9)\nfor F : S × A →TS, where TS denotes the tangent bundle\nof the state space. Also, let (˜s, ˜a) = gθ(s, a) for the group\naction given by (7) and (8). Then, (˜s, ˜a) also satisﬁes the\nequations of motion, i.e.,\n˙˜s = F(˜s, ˜a).\n(10)\nOr equivalently, F ◦gθ = gθ ◦F, considering that the\ngroup action is extended to TS. This corresponds to the\nequivariance of F with respect to gθ, and it is also stated\nthat the dynamics is symmetric about gθ.\nProof: Let gθ(x, v, R, Ω) = (˜x, ˜v, ˜R, ˜Ω). First, we have\n˙˜x = exp(θˆe3)v = ˜v.\nSimilarly, we have m˙˜v = m exp(θˆe3)˙v. Substituting (2) into\nthe above,\nm˙˜v = exp(θˆe3)(mge3 −fRe3)\n= mge3 −f ˜Re3,\nas exp(θˆe3)e3 = e3 for any θ. These are equivalent to the\ntranslational dynamics (1) and (2).\nNext, for (3),\n˙˜R = exp(θˆe3) ˙R = exp(θˆe3)RˆΩ= ˜Rˆ˜Ω.\nAlso, as ˜Ω= Ω, it trivially satisﬁes (4).\nThis\nimplies\nthat\nwhen\nthe\nstate-action\ntrajectory\n(s(t), a(t)) for t ∈[0, T] is the solution of (1)–(4) repre-\nsenting the quadrotor dynamics, then the rotated trajectory\ngθ(s(t), a(t)) also satisﬁes (1)–(4) for any θ ∈S1. This is not\nsurprising as the direction of ⃗e1 (or ⃗e2) in the horizontal plane\nis completely arbitrary in the formulation of the quadrotor\ndynamics.\nThis further allows us to deﬁne an equivalent class for the\nstate-action trajectories over an interval [0, T]. Let\n(s(t), a(t)) ∼(˜s(t), ˜a(t))\n(11)\nif there exists θ ∈S1 such that (˜s(t), ˜a(t)) = gθ(s(t), a(t))\nfor all t ∈[0, T]. It is straightforward to show the reﬂectivity,\nsymmetry, and transitivity of the above binary relation, to\nverify that it is an equivalent relation [23]. For (s, a) ∈S×A,\ndeﬁne its equivalent class as\n[s, a] = {(˜s, ˜a) ∈S × A | (s, a) ∼(˜s, ˜a)}.\n(12)\nThen, the quadrotor dynamics can be characterized com-\npletely on the quotient space of S × A by the equivalence\nrelation ∼, denoted by S × A/ ∼.\nB. Equivariant Reinforcement Learning\nNext, we show how the symmetry properties can be\nexploited in the reinforcement learning. It has been shown\nthat if the reward also satisﬁes the symmetry property, the\ncorresponding value function is invariant and the optimal\npolicy is equivariant in a discrete-time setting [20]. Here\nwe establish the correspondent results for deterministic,\ncontinuous-time dynamics by extending the continuous re-\ninforcement learning formulated in [24], [25]. This is to for-\nmulate an equivariant reinforcement learning framework for\nthe inherent, continuous-time quadrotor dynamics, without\nresorting to any discretization scheme.\nFor 0 < γ < 1, let the value function of a policy π : S →\nA be deﬁned by\nVπ(t, s(t)) =\nZ ∞\nt\nγτr(s(τ), a(τ))dτ,\n(13)\nwhere the action at any time is deﬁned by the policy as\na(t) = π(s(t)). The objective is to construct the optimal\npolicy π∗(s(t)) maximizing Vπ(t, s(t)). When the dynamics\nand the reward are symmetric with respect to a group action\nas presented in Proposition 1, the value function and the\noptimal policy satisfy the following properties.\nProposition 2: Consider a continuous-time MDP to maxi-\nmize the value function (13) under the dynamics (9). Suppose\nthat F : S × A →TS is equivariant and r : S × A →R is\ninvariant with respect to a group action g : S × A →S × A,\ni.e.,\nF ◦g = g ◦F,\n(14)\nr ◦g = r.\n(15)\nFor a given policy π(s) : S →A, deﬁne a new policy\ninduced by g as\n˜π(˜s) = gπ(g−1˜s).\n(16)\nThen, the following properties hold:\n(i) The value function is invariant under the group action,\ni.e., Vπ = V˜π ◦g.\n(ii) The optimal policy is equivariant under the group\naction, i.e., π∗◦g = g ◦π∗.\nProof: Let (s(t), a(t)) be a trajectory of (9), driven by a\npolicy π(s). Then, according to Proposition 1, (˜s(t), ˜a(t)) =\n(gs(t), ga(t)) is another trajectory of (9), where\n˜a = ga = gπ(s) = gπ(g−1˜s) = ˜π(˜s).\nThus, (˜s(t), ˜a(t)) is a trajectory of (9) from the transformed\npolicy ˜π. We have\nV˜π(t, gs(t)) =\nZ ∞\nt\nγτr(˜s(τ), ˜a(τ))dτ\n=\nZ ∞\nt\nγτr(gs(τ), ga(τ))dτ,\nwhich reduces to (13) from (15). This shows (i).\nNext, split the domain of the integration in (13) into two\nparts such that\nVπ(t, s(t)) =\nZ t+∆t\nt\nγτr(s(τ), a(τ))dτ\n+\nZ ∞\nt+∆t\nγτr(s(τ), a(τ))dτ\n= r(s(t), a(t))∆t + Vπ(t + ∆t, s(t + ∆t)) + o(∆t).\nHere, Vπ(t + ∆t, s(t + ∆t)) is expanded into\nVπ(t, s(t)) + ∂Vπ\n∂t ∆t + ∂Vπ\n∂s F(s(t), a(t))∆t + o(∆t).\nSubstituting this into the above and rearranging\n−∂Vπ\n∂t = r(s(t), a(t)) + ∂Vπ\n∂s F(s(t), a(t)).\n(17)\nas ∆t →0. The left-hand side is rewritten as follows. The\nvalue function can be reorganized into\nVπ(t, s(t)) = γt\nZ ∞\nt\nγτ−tr(s(τ), a(τ))dτ,\nwhere the integral at the second factor is independent of t.\nThus, its derivative with respect to t is given by\n∂Vπ\n∂t = γt log γ\nZ ∞\nt\nγτ−tr(s(τ), a(τ))dτ\n= log γVπ(t, s(t)).\nSubstituting this back to (17),\nlog γ−1Vπ(t, s(t)) = r(s(t), a(t)) + ∂Vπ(t, s(t))\n∂s\nF(s(t), a(t)).\nLet V ∗be the optimal value function, obtained by the\noptimal policy π∗. The above yields the following Hamilton-\nJacobi-Bellman equation:\nlog γ−1V ∗(t, s(t)) = max\na∈A\n\u001a\nr(s, a) + ∂V ∗(t, s(t))\n∂s\nF(s, a)\n\u001b\n,\nand the optimal policy is given by\nπ∗(s(t)) = arg max\na∈A\n\u001a\nr(s, a) + ∂V ∗(t, s(t))\n∂s\nF(s, a)\n\u001b\n.\n(18)\nTherefore, the optimal action at ˜s = gs is given by\n(π∗◦g)(s) = π∗(˜s) = arg max\n˜a∈A\n\u001a\nr(˜s, ˜a) + ∂V ∗(t, ˜s)\n∂˜s\nF(˜s, ˜a)\n\u001b\n.\nFrom (14) and (15), we have F(˜s, ˜a) = gF(s, g−1˜a) and\nr(˜s, ˜a) = r(s, g−1˜a). Further, utilizing the property (i),\n∂V ∗(t, ˜s(t))\n∂˜s\n= ∂V ∗(t, s(t))\n∂s\n∂s\n∂˜s = ∂V ∗(t, s(t))\n∂s\ng−1\nSubstituting these,\nπ∗(˜s(t)) = arg max\n˜a∈A\n\u001a\nr(s, g−1˜a) + ∂V ∗(t, s(t))\n∂s\nF(s, g−1˜a)\n\u001b\n,\n(19)\nwhich is equivalent to (18), except that a in (18) is replaced\nby g−1˜a. As such, if a∗(t) = π∗(s(t)) is the optimal action\nof (18), then the right hand side of (19) is maximized when\ng−1˜a = a∗, or equivalently ˜a = ga∗. This shows that\nπ∗(gs) = gπ∗(s).\nEquivariant reinforcement learning is to utilize the invari-\nance of the value function and the equivariant of the optimal\naction with respect to the symmetry of the dynamics and\nthe reward. For the quadrotor dynamics, the symmetry of\nthe dynamics (14) is established by Proposition 1, and it is\nstraightforward to show the invariance of the reward (15) as\nthe operation of taking norm of a vector in (6) is not affected\nby any rotation. Further, as the group action gθ on the action\na, given by (8) is the identity map, the optimal action is in\nfact invariant as well. In short, for the quadrotor dynamics,\nwe have\nV (s) = V (˜s),\nπ∗(s) = π∗(˜s),\n(20)\nfor any ˜s ∈[s].\nIn deep reinforcement learning, the value function and\nthe policy are represented by deep neural networks, which\nare trained by a set of trajectories. As such, successful\nimplementation of reinforcement learning often requires a\nmassive number of trajectories. Here, the sampling efﬁciency\nof reinforcement learning can be substantially improved by\nexploiting (20) as presented below.\nIV. NUMERICAL EXPERIMENTS\nThis section presents a neural network structure that\nrespects the above equivariance property, to be utilized in the\nproposed equivariant reinforcement learning. Then, we show\nthe detailed implementation and simulation environments for\nbenchmark studies.\nA. Neural Network Structures\nIn reinforcement learning, neural networks that approxi-\nmate the optimal policy and the value function are referred\nto as actor and critic networks, respectively. To impose the\ninvariant properties (20) in the actor and the critic, one can\ndevelop group equivariant neural networks with respect to the\ngroup action of (7) and (8). However, the most of the existing\nresults in equivariant neural networks deal with inputs of\nimages or discrete actions, and they are not suitable for the\npresented rotational action on vectors and matrices.\nInstead, from (20), the approximation of the value function\nand the policy is performed on the quotient space S ×A/ ∼.\n⋮\n⋮\n⋮\n17-dim\n256-dim\nReLU\n256-dim\nReLU\n4-dim\nTanh\n[x]1\n[x]3\n[v]1\n[v]2\n[v]3\n[R]11\n[R]33\n[Ω]1\n[Ω]2\n[Ω]3\nT1\nT2\nT3\nT4\n(a) Actor\n⋮\n256-dim\nReLU\n1-dim\nLinear\n21-dim\n⋮\n⋮\n⋮\n256-dim\nReLU\n[x]1\n[x]3\n[v]1\n[v]2\n[v]3\n[R]11\n[R]33\n[Ω]1\n[Ω]2\n[Ω]3\nT1\nT4\nQ([s], a)\n(b) Critic\nFig. 4.\nThe architecture of neural networks: (a) Actor and (b) Critic\nFor the equivalent class [s] of any s ∈S, we choose a\nrepresentative element ˜s by\n[s] = g[θ]s,\n(21)\nwhere [θ] = −atan2(x2, x1). Using this, the equivalent\nclass can be identiﬁed with the above representative element,\nsuch that the quotient space is considered as a set of the\nrepresentative elements. Thus,\nV (s) = V ([s]),\nπ∗(s) = π∗([s]).\n(22)\nAs the group action is one-dimensional, this reduces the\ndomain of the value and the policy by one. The particular\nchoice of the rotation angle [θ] ensures that the second\nelement of the position is always zero, i.e., [x]2 = 0 (see\nFigure 3). As such, it can be simply dropped from the input\nof the actor and the critic. Speciﬁcally, the input to the each\nnetwork is ([x]1, [x]3, [v], [R], [Ω]) ∈R17.\nNote that (22) can be adopted to any reinforcement learn-\ning scheme. In this paper, we apply it to TD3 and SAC,\nFig. 5.\nSimulation environment implemented in Python\nto obtain two equivariant reinforcement learning schemes,\nreferred to as equivariant TD3 and equivariant SAC, respec-\ntively. As they are developed inherently for a discrete-time\nMDP, the quadrotor equations of motion are discretized by a\nRunge-Kutta method. Reinforcement learning for a discrete-\ntime MDP often relies on the state-action value function,\nQ(s, a). As a linear combination of the reward r(s, a) and\nthe value V (s), the state-action value function is also group\ninvariant [20], i.e., Q◦g = Q. Therefore, Q(s, a) = Q([s], a)\nfor the presented quadrotor dynamics.\nThe structure of the actor network in this paper is il-\nlustrated in Figure 4(a), where the actor maps the reduced\nquadrotor state to its motor commands. TD3 and SAC share\nthe same inputs and outputs consisting of a 17-dimensional\nstate vector and a 4-dimensional action vector, respectively.\nOne major difference is that TD3 is based on the deter-\nministic policy actor, whereas the stochastic policy actor is\nupdated in SAC. A hyperbolic tangent activation function in\nthe output layer ensures a proper range of control signals.\nThe critic networks are depicted in Figure 4(b). TD3 and\nSAC take both the state and action as inputs and estimate\nthe action-value function Q([s], a) as outputs. All networks\nare built as multilayer perceptron (MLP) networks with two\nhidden layers of 256 nodes with ReLU activation function.\nB. Simulation Environments\nAs shown in Figure 5, the simulation environment is\ndeveloped in Python based on the quadrotor dynamics in\nSection II-A with the open-source library OpenAI Gym [26],\nwhich is a popular RL algorithm development toolkit. TD3\nand SAC algorithms are implemented with a machine learn-\ning framework PyTorch [27]. The training and evaluation\nwere executed for 1.5 million time steps with the use of\nNVIDIA CUDA, on a GPU workstation powered by NVIDIA\nA100-PCIE-40GB. Table I presents hyperparameters used for\ntraining TD3, SAC, and Equivariant RL agents. TD3 and\nSAC share some default parameters, such as learning rate\nand discount factor, unless explicitly indicated in the table.\nC. Benchmark Results\nWe validated the proposed approach comprehensively\nthrough numerical simulations to ensure reliability and sta-\nbility. We took the vanilla TD3 and SAC algorithms, which\ndo not consider the equivariance property, as the baselines for\nperformance comparison. All experiments were executed for\nTABLE I\nHYPERPARAMETERS USED IN BENCHMARK STUDIES\nParameter\nValue\nOptimizer\nAdam\nActor learning rate\n3 · 10−4\nCritic learning rate\n3 · 10−4\nDiscount factor, γ\n0.99\nReplay buffer size\n106\nBatch size\n256\nTarget smoothing coefﬁcient, τ\n0.005\nTwin Delayed DDPG (TD3)\nExploration noise\n0.1\nTarget policy noise\n0.2\nPolicy noise clip\n0.5\nTarget update interval\n2\nSoft Actor-Critic (SAC)\nEntropy regularization coefﬁcient, α\nAutotuned\nTarget update interval\n1\n6 random seeds, and the average return was reported once\nevery 5,000 steps without action noise for 30 trajectories.\nDuring training and evaluation, the quadrotor started with the\nrandom states of each episode at an arbitrary initial location\nin a 3m × 3m × 3m space. The agents were trained without\nany auxiliary aid technique such as PID controllers or pre-\ntrained policies.\nIn (6), the coefﬁcient cx is set to 2.0, and the penalizing\nterms are set cv = 0.15, cΩ= 0.2, and ca = 0.03 to\nimprove stability and achieve smooth control. Note that too\nlarge penalties prevent the quadrotor from moving toward\nits target by focusing only on stabilization. The reward is\nnormalized into [0, 1] and rescaled by a factor of 0.1 to\nensure convergence. Finally, the discount factor is selected\nas γ = 0.99.\nFigure 6 shows training curves in terms of the average\nreward, where the blue curves denote the proposed equiv-\nariant methods and the green corresponds to the baselines.\nThe solid lines and shaded areas represent the mean value\nand 2σ bounds, respectively. Figure 6(a) shows the result of\ntraining with TD3 and Figure 6(b) shows the results of SAC.\nIn both cases, the proposed equivariant methods outperform\nthe baselines in terms of the learning speed and convergence.\nIn Figure 6(a), it is also illustrated that the equivariant TD3\nexhibits more consistent results than the vanilla TD3 with\nrespect to the random seed variations.\nAdditionally, the equivariant methods exhibit higher re-\nwards than their counterparts over the same time period.\nIn other words, the baselines require more accumulated\ntimesteps to achieve a similar level of reward. This conﬁrms\nthat our proposed equivariant framework is more sample-\nefﬁcient, leading to faster convergence.\nNext, to demonstrate the ﬂight performance of the trained\npolicy, the state trajectory and the control inputs for a\nspeciﬁc episode are presented in Figure 7, where the top\nfour subﬁgures show the position error ex, velocity error\nev, angular velocity error eΩ, and motor thrusts T. The last\nsubﬁgure presents the nine elements of attitude R. As shown\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimestep\n1e6\n0\n50\n100\n150\n200\n250\nAverage Episode Reward\nEquivariant TD3\nTD3\n(a) TD3 and Equivariant TD3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimestep\n1e6\n0\n50\n100\n150\n200\nAverage Episode Reward\nEquivariant SAC\nSAC\n(b) SAC and Equivariant SAC\nFig. 6. Comparison results with respect to the average reward in evaluation:\n(a) TD3 and (b) SAC\nin Figure 7(a), the learned policy successfully controlled the\nquadrotor from a random position to its target after 6 seconds\nwithout any noticeable steady-state error. Speciﬁcally, the\nterminal position was (−0.0028, 0.0017, −0.0012)m. From\nFigure 7(d), the thrust commands are reasonable, and they\navoid rapid chattering that often appears in RL-based quadro-\ntor controls. Thus, the simulation results clearly show that the\nproposed equivariant framework is not only efﬁcient enough\nto handle high-dimensional data, but also exhibits desirable\nproperties.\nV. CONCLUSION\nIn this paper, we presented a data-efﬁcient reinforcement\nlearning strategy for quadrotors, by exploiting its symme-\ntry. We identiﬁed S1–symmetry of the quadrotor dynamics,\nwhich is utilized in the neural network structures for the\nactor and the critic to reduce the dimension of the input\ndomain by one. Numerical simulation with two popular\nreinforcement schemes shows that the equivariance property\nsubstantially improves sample efﬁciency, and the trained\npolicy is reasonable.\nFuture work includes incorporating other symmetry prop-\nerties of the quadrotor. For example, since the structure of the\nquadrotor is symmetrical about its third body-ﬁxed axis, the\nthrust at the opposite side of the quadrotor can be swapped\nwhen it is rotated about the third body-ﬁxed axis by 180◦.\n0\n1\n2\n3\n4\n5\n6\n7\nTime (sec)\n-1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nposition error, ex (m)\nex1\nex2\nex3\n(a) Position error ex\n0\n1\n2\n3\n4\n5\n6\n7\nTime (sec)\n0.8\n0.4\n0.0\n0.4\n0.8\nvelocity error, ev (m/s)\nev1\nev2\nev3\n(b) Velocity error ev\n0\n1\n2\n3\n4\n5\n6\n7\nTime (sec)\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nangular velocity error, e  (rad/s)\ne\n1\ne\n2\ne\n3\n(c) Angular velocity error eΩ\n0\n1\n2\n3\n4\n5\n6\n7\nTime (sec)\n1\n2\n3\n4\n5\n6\n7\n8\n9\nmotor thrust (N)\nT1\nT2\nT3\nT4\n(d) Motor thrust (T1, T2, T3, T4)\n0\n2\n4\n6\n8\n10\n12\n14\n0.97\n0.98\n0.99\n1.00\nResponse\n0\n2\n4\n6\n8\n10\n12\n14\n0.15\n0.10\n0.05\n0.00\n0\n2\n4\n6\n8\n10\n12\n14\n0.10\n0.05\n0.00\n0.05\n0\n2\n4\n6\n8\n10\n12\n14\n0.00\n0.05\n0.10\n0.15\n0.20\n0\n2\n4\n6\n8\n10\n12\n14\n0.97\n0.98\n0.99\n1.00\n0\n2\n4\n6\n8\n10\n12\n14\n0.05\n0.00\n0.05\n0.10\n0\n2\n4\n6\n8\n10\n12\n14\nTime (sec)\n0.05\n0.01\n0.03\n0.07\n0\n2\n4\n6\n8\n10\n12\n14\nTime (sec)\n0.10\n0.05\n0.00\n0.05\n0\n2\n4\n6\n8\n10\n12\n14\nTime (sec)\n0.990\n0.994\n0.998\n(e) Attitude R\nFig. 7.\nControlled trajectory for a single episode\nAlso, instead of utilizing the representative element of the\nequivalent class, the invariance of the value or the policy\ncan be directly imposed via equivariant neural networks.\nREFERENCES\n[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,”\nnature, vol. 518, no. 7540, pp. 529–533, 2015.\n[2] E. Choi, D. Hewlett, J. Uszkoreit, I. Polosukhin, A. Lacoste, and\nJ. Berant, “Coarse-to-ﬁne question answering for long documents,”\nin Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 2017, pp. 209–\n220.\n[3] Y. Song, M. Steinweg, E. Kaufmann, and D. Scaramuzza, “Au-\ntonomous drone racing with deep reinforcement learning,” in Interna-\ntional Conference on Intelligent Robots and Systems, 2021, pp. 1205–\n1212.\n[4] S. Belkhale, R. Li, G. Kahn, R. McAllister, R. Calandra, and S. Levine,\n“Model-based meta-reinforcement learning for ﬂight with suspended\npayloads,” IEEE Robotics and Automation Letters, vol. 6, no. 2, pp.\n1471–1478, 2021.\n[5] S. Bouabdallah, A. Noth, and R. Siegwart, “PID vs LQ control\ntechniques applied to an indoor micro quadrotor,” in International\nConference on Intelligent Robots and Systems, vol. 3, 2004, pp. 2451–\n2456.\n[6] R. Xu and U. Ozguner, “Sliding mode control of a quadrotor he-\nlicopter,” in Proceedings of the IEEE Conference on Decision and\nControl, 2006, pp. 4957–4962.\n[7] T. Lee, M. Leok, and N. H. McClamroch, “Geometric tracking control\nof a quadrotor UAV on SE(3),” in IEEE conference on decision and\ncontrol, 2010, pp. 5420–5425.\n[8] C.-H. Pi, K.-C. Hu, S. Cheng, and I.-C. Wu, “Low-level autonomous\ncontrol and tracking of quadrotor using reinforcement learning,”\nControl Engineering Practice, vol. 95, p. 104222, 2020.\n[9] J. Hwangbo, I. Sa, R. Siegwart, and M. Hutter, “Control of a quadrotor\nwith reinforcement learning,” IEEE Robotics and Automation Letters,\nvol. 2, no. 4, pp. 2096–2103, 2017.\n[10] G. C. Lopes, M. Ferreira, A. da Silva Simoes, and E. L. Colombini,\n“Intelligent control of a quadrotor with proximal policy optimization\nreinforcement learning,” in 2018 Latin American Robotic Symposium,\n2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on\nRobotics in Education (WRE), 2018, pp. 503–508.\n[11] M. Shehab, A. Zaghloul, and A. El-Badawy, “Low-level control of\na quadrotor using twin delayed deep deterministic policy gradient\n(TD3),” in International Conference on Electrical Engineering, Com-\nputing Science and Automatic Control, 2021, pp. 1–6.\n[12] A. Rodriguez-Ramos, C. Sampedro, H. Bavle, P. De La Puente,\nand P. Campoy, “A deep reinforcement learning strategy for UAV\nautonomous landing on a moving platform,” Journal of Intelligent &\nRobotic Systems, vol. 93, no. 1, pp. 351–366, 2019.\n[13] A. Molchanov, T. Chen, W. H¨onig, J. A. Preiss, N. Ayanian, and\nG. S. Sukhatme, “Sim-to-(multi)-real: Transfer of low-level robust\ncontrol policies to multiple quadrotors,” in International Conference\non Intelligent Robots and Systems, 2019, pp. 59–66.\n[14] Y. Wang, J. Sun, H. He, and C. Sun, “Deterministic policy gradient\nwith integral compensator for robust quadrotor control,” IEEE Trans-\nactions on Systems, Man, and Cybernetics: Systems, vol. 50, no. 10,\npp. 3713–3725, 2019.\n[15] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst,\n“Geometric deep learning: going beyond euclidean data,” IEEE Signal\nProcessing Magazine, vol. 34, no. 4, pp. 18–42, 2017.\n[16] T. Cohen and M. Welling, “Group equivariant convolutional networks,”\nin International conference on machine learning.\nPMLR, 2016, pp.\n2990–2999.\n[17] M. Weiler and G. Cesa, “General E(2)-equivariant steerable CNNs,”\nAdvances in Neural Information Processing Systems, vol. 32, 2019.\n[18] E. van der Pol, D. Worrall, H. van Hoof, F. Oliehoek, and M. Welling,\n“MDP homomorphic networks: Group symmetries in reinforce-\nment learning,” Advances in Neural Information Processing Systems,\nvol. 33, pp. 4199–4210, 2020.\n[19] E. van der Pol, H. van Hoof, F. A. Oliehoek, and M. Welling,\n“Multi-agent\nMDP\nhomomorphic\nnetworks,”\narXiv\npreprint\narXiv:2110.04495, 2021.\n[20] D.\nWang,\nR.\nWalters,\nand\nR.\nPlatt,\n“SO(2)-equivariant\nreinforcement learning,” in International Conference on Learning\nRepresentations, 2022. [Online]. Available: https://openreview.net/\nforum?id=7F9cOhdvfk\n[21] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approxi-\nmation error in actor-critic methods,” in International conference on\nmachine learning.\nPMLR, 2018, pp. 1587–1596.\n[22] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in International conference on machine learning.\nPMLR,\n2018, pp. 1861–1870.\n[23] J. L. Kelley, General topology.\nCourier Dover Publications, 2017.\n[24] K. Doya, “Reinforcement learning in continuous time and space,”\nNeural computation, vol. 12, no. 1, pp. 219–245, 2000.\n[25] R. Munos, “A study of reinforcement learning in the continuous case\nby the means of viscosity solutions,” Machine Learning, vol. 40, no. 3,\npp. 265–299, 2000.\n[26] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schul-\nman, J. Tang, and W. Zaremba, “Openai gym,” arXiv preprint\narXiv:1606.01540, 2016.\n[27] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., “Pytorch: An\nimperative style, high-performance deep learning library,” Advances\nin neural information processing systems, vol. 32, 2019.\n",
  "categories": [
    "cs.LG",
    "math.OC"
  ],
  "published": "2022-06-02",
  "updated": "2023-02-26"
}