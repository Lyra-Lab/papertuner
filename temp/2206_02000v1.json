{
  "id": "http://arxiv.org/abs/2206.02000v1",
  "title": "Hybrid Value Estimation for Off-policy Evaluation and Offline Reinforcement Learning",
  "authors": [
    "Xue-Kun Jin",
    "Xu-Hui Liu",
    "Shengyi Jiang",
    "Yang Yu"
  ],
  "abstract": "Value function estimation is an indispensable subroutine in reinforcement\nlearning, which becomes more challenging in the offline setting. In this paper,\nwe propose Hybrid Value Estimation (HVE) to reduce value estimation error,\nwhich trades off bias and variance by balancing between the value estimation\nfrom offline data and the learned model. Theoretical analysis discloses that\nHVE enjoys a better error bound than the direct methods. HVE can be leveraged\nin both off-policy evaluation and offline reinforcement learning settings. We,\ntherefore, provide two concrete algorithms Off-policy HVE (OPHVE) and\nModel-based Offline HVE (MOHVE), respectively. Empirical evaluations on MuJoCo\ntasks corroborate the theoretical claim. OPHVE outperforms other off-policy\nevaluation methods in all three metrics measuring the estimation effectiveness,\nwhile MOHVE achieves better or comparable performance with state-of-the-art\noffline reinforcement learning algorithms. We hope that HVE could shed some\nlight on further research on reinforcement learning from fixed data.",
  "text": "HYBRID VALUE ESTIMATION FOR OFF-POLICY EVALUATION\nAND OFFLINE REINFORCEMENT LEARNING\nA PREPRINT\nXue-Kun Jin1,∗, Xu-Hui Liu1,∗, Shengyi Jiang2, Yang Yu1,3, †\n1 National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China\n2 Department of Computer Science, The University of Hong Kong, Hong Kong SAR, China\n3 Polixir.ai\n{jinxk, liuxh}@lamda.nju.edu.cn, syjiang@cs.hku.hk, yuy@nju.edu.cn\nABSTRACT\nValue function estimation is an indispensable subroutine in reinforcement learning, which becomes\nmore challenging in the ofﬂine setting. In this paper, we propose Hybrid Value Estimation (HVE)\nto reduce value estimation error, which trades off bias and variance by balancing between the value\nestimation from ofﬂine data and the learned model. Theoretical analysis discloses that HVE enjoys a\nbetter error bound than the direct methods. HVE can be leveraged in both off-policy evaluation and\nofﬂine reinforcement learning settings. We, therefore, provide two concrete algorithms Off-policy\nHVE (OPHVE) and Model-based Ofﬂine HVE (MOHVE), respectively. Empirical evaluations on\nMuJoCo tasks corroborate the theoretical claim. OPHVE outperforms other off-policy evaluation\nmethods in all three metrics measuring the estimation effectiveness, while MOHVE achieves better\nor comparable performance with state-of-the-art ofﬂine reinforcement learning algorithms. We hope\nthat HVE could shed some light on further research on reinforcement learning from ﬁxed data.\n1\nIntroduction\nReinforcement learning (RL) [1, 2, 3, 4] has demonstrated great success in various sequential decision making problems,\ne.g., sequential recommendation systems [5, 6] and robotic locomotion skill learning [7, 8]. Current reinforcement\nlearning algorithms fall into two major categories: online RL algorithms and ofﬂine RL algorithms. The former category\nrequires iteratively collecting samples by interacting with the environment and usually achieves optimal performance.\nThe latter category learns a policy from a ﬁxed dataset, making it suitable for risk-sensitive applications. However,\nthe learned policy may still be sub-optimal. In both online setting and ofﬂine setting, value function estimation is an\nindispensable subroutine, which can be seen as the precondition of policy evaluation and policy improvement.\nHowever, value function estimation was found to perform poorly without online interaction [9], which is an important\nsource of the sub-optimality in current ofﬂine RL algorithms. This failure is generally attributed to the extrapolation\nerror when the value function is evaluated on out-of-distribution actions [10, 11]. Model-based methods [12, 13, 14]\nare a natural choice to approach this problem for two reasons. First, models are mainly trained with supervised learning\nwith a ﬁxed target, e.g., [15], which is more stable than bootstrapping. Second, dynamics tend to be simpler than value\nfunction, so the learned model enjoys a better smoothness and generalization ability [16].\nBased on these existing promising beneﬁts of model-based value estimation, we expect to improve it further. There\nare three kinds of errors in vanilla model-based value estimation, which are optimization error, projection error and\nstatistical error. They are caused by the limitation of optimization algorithms, expressivity of neural networks and\nquantity of data. Such errors will accumulate along the trajectory and lead to the so-called compounding error [17, 18],\nwhich signiﬁcantly degrades the performance of model-based value estimation. Inspired by the fact that the data in the\nofﬂine dataset does not suffer from model error, we use the ofﬂine data to correct the error in value estimation caused\nby the inaccurate model.\n∗Equal Contribution †Corresponding Author\narXiv:2206.02000v1  [cs.LG]  4 Jun 2022\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nThe proposed value estimation method is named Hybrid Value Estimation (HVE), which uses both model-generated data\nand ofﬂine dataset to perform estimation. Model induces bias because of model error while the ofﬂine dataset induces\nbias and variance because of the policy divergence and data limitation respectively. HVE achieves a trade-off between\nbias and variance by automatically choosing the step length in Bellman update, which balances value estimation from\nofﬂine data and the learned model. We implement two concrete algorithms OPHVE and MOHVE that apply HVE in\noff-policy evaluation and ofﬂine reinforcement learning respectively.\nWe provide a theoretical proof that HVE has a better error bound than vanilla model-based value estimation. The results\non OPE benchmark DOPE [19] verify this theory empirically. The experiments suggest that MOHVE outperforms\nprevious ofﬂine RL algorithms [20, 21, 22, 23] on the ofﬂine RL benchmark D4RL [24]. We also design an experiment\nto analyze the effect of step length. The results show its signiﬁcance in the performance of OPHVE and MOHVE,\nand our automatic step length adjustment mechanism can ﬁnd the near-optimal solution. Furthermore, we integrate\nMOHVE with OPHVE, improving the training and evaluation pipeline of ofﬂine RL.\n2\nBackground and Related Work\nConsider the standard Markov decision process (MDP), in which the environment is deﬁned by a tuple M =\n(S, A, T, r, ρ0, γ). S and A denote the state space and action space respectively, both of which can be continuous or dis-\ncrete. T(s′|s, a) is the dynamics or transition distribution and ρ0(s) is the initial state distribution. r(s, a) ∈[0, Rmax]\ndenotes the reward function, and γ ∈(0, 1) the discount factor. The goal of reinforcement learning is to ﬁnd the optimal\npolicy π∗(a|s) that maximizes the expected discounted cumulative rewards:\narg max\nπ\nJM(π) :=\nE\nπ,T,ρ0\n\" ∞\nX\nt=0\nγtr (st, at)\n#\n.\n(1)\nGiven a state-action pair (s, a), deﬁne the state-action value function Qπ(s, a) as Eπ,T [P∞\nt=0 γtr(st, at)|s0 = s, a0 =\na], which represents the expected discounted return starting from (s, a). Similarly, deﬁne state value function V π(s)\nas Eπ,T [P∞\nt=0 γtr(st, at)|s0 = s], the expected discounted return under π when starting from state s. To facilitate\nour proof, we denote the discounted state visitation distribution of a policy π under MDP M as dπ\nM(s) := (1 −\nγ) P∞\nt=0 γtP(st = s|π), where P(st = s|π) is the probability of reaching state s at time t by rolling out π in M.\nFurthermore, we denote the discounted state-action visitation distribution dπ\nM(s, a) := dπ\nM(s)π(a|s).\nOff-policy Evaluation. OPE aims to evaluate a set of policies using data collected by other behavior policies without\ninteracting with the environment. The behavior policies are sometimes known.\nA straightforward approach to OPE is to ﬁt the Q function directly from data using standard approximate dynamic\nprogramming (ADP) techniques, e.g., Fitted Q Evaluation (FQE) [25, 26]. Such an evaluation incurs high bias.\nImportance Sampling (IS) performs evaluation by calculating the distribution ratio of trajectories generated by behavior\npolicy and target policy [27, 28]. Although being consistent and unbiased in theory, this class of estimators suffers\nfrom high variance. Marginalized Importance Sampling (MIS) [29, 30, 31, 32, 33] reduces the variance of IS by\nreweighting each state-action pair rather than reweighting the entire trajectory. Doubly Robust (DR) [34, 35] leverages\nan approximate Q function to decrease the variance of the unbiased estimates provided by IS, where MAGIC [35]\nuses a model to obtain the approximate Q function and aims to trade off between bias and variance based on MSE\nminimization. However, the trade-off needs the knowledge of true value function, which is not available in OPE setting.\nOPHVE achieves bias-variance trade-off by minimizing Mean Absolute Error and does not require the knowledge of\ntrue value function.\nModel-based RL. Model-based reinforcement learning is featured with learned dynamic models. Recent model-based\nRL algorithms [17, 20, 21] use a bootstrap ensemble of dynamic models. The learned dynamic transition and (learned)\nreward function establish a model MDP c\nM = (S, A, bT, ˆr, ρ0, γ). The agent can interact with c\nM to generate more data.\nSubsequently, we can use any learning or planning algorithms [36, 37] to recover the optimal policy in the model MDP\nas ˆπ = arg maxπ Jc\nM(π). Besides generating more data, models are also useful for learning transferable policies [38].\nOfﬂine RL. In the ofﬂine reinforcement learning setting, agents cannot interact with a real environment like online RL.\nInstead, it only has access to a static dataset Denv = {(s, a, r, s′)} collected beforehand by one or a mixture of behavior\npolicies, denoted by πβ.\nMost recent ofﬂine RL works learn a lower bound of the ground truth Q function [20, 21, 22, 39, 40, 14], while MOHVE\ntries to learn the true value. BCQ [10], BEAR [11], TD3+BC [23] and EMaQ [41] are model free algorithms and\nconstrain policy to select action similar to behavior policy. Model-based methods are also introduced for ofﬂine RL. For\nexamples, MOPO [20]learns a conservative Q function by using model uncertainty; COMBO [21] avoids uncertainty\n2\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nquantiﬁcation by adding a Q-value minimization term alongside a standard Bellman error objective. MAPLE [14] ﬁrst\nintroduced transfer meta-policy learning for model-based ofﬂine RL, which naturally overcame the out-of-distribution\nissue. In contrast with these approaches, MOHVE need not learn a conservative Q function because of its high value\nestimation accuracy.\nApart from the learning algorithms, some recent works turn to improve the entire training and evaluation pipeline of\nofﬂine RL. NeoRL [9], a near real-world benchmark, points out the importance of ofﬂine policy selection. In [42], a\nnew evaluation paradigm is proposed that uses a limited budget to evaluate the trained policy to select the best set of\nhyper-parameters. This paper takes a step in this direction by evaluating the performance of the trained policy in an\nofﬂine manner and choosing the best training step to stop.\n3\nReducing Value Estimation Error by Hybrid Value Estimation\nIn this section, we analyze the value estimation error of our method and the vanilla model-based method. Let us recall\nthe procedure of value function estimation using a learned model. Let bQπ be the Q function induced by the learned\nmodel c\nM and policy π. Bellman equation can be written as\nbQπ(s0, a0) =\nE\ns1∼b\nT ,a1∼π\nh\nˆr(s0, a0) + γ bQπ(s1, a1)\ni\n.\n(2)\nWithout loss of generality, we assume that the expected TV-distance between two transition distributions is bounded\nby ϵm and the expected policy divergence is bounded by ϵπ, i.e., ϵm = EDEs,a∼D\nh\nDTV(T(·|s, a), bT(·|s, a))\ni\nand\nϵπ = EDEs∼D\nh\nDTV(π(·|s), πβ(·|s))\ni\n, where ED is the expectation with respect to the generation of dataset D.\nBecause the learned reward function is much more accurate than learned transition function, we ignore the error of\nlearned reward function, and the value estimation error is bounded.\nTheorem 3.1 (Theorem 4.1 in [17]). The error bound of the value function induced by Eq. (2) is:\nEDEs,a∼D\n\f\f\fQπ(s, a) −bQπ(s, a)\n\f\f\f ≤2γRmax(2ϵπ + ϵm)\n(1 −γ)2\n+ 4Rmaxϵπ\n1 −γ\n.\nThe theorem states that the value estimation error is related to the model learning error and policy discrepancy. Although\nthis bound has been improved in [18] with the compounding error reduced, we consider the old result in this paper for\nsimplicity. These two quantities both induce compounding error proportional to the effective horizon (i.e.,\n1\n1−γ ) of the\nMDP. Since γ is close to 1, it suggests that the inﬂuence of model learning error and policy divergence is seriously\nmagniﬁed in value function estimation.\nThis issue is not severe in the online setting as ϵm and ϵπ can be reduced via online interaction. However, it cannot be\nneglected in the ofﬂine setting. Because the model can only access the dataset during the training process, it suffers\nfrom higher statistical error due to data limitation. Furthermore, since the distribution of data in the dataset may deviate\nfrom the distribution of the current policy, the policy divergence ϵπ is much larger in the ofﬂine setting [10].\nEssentially, the value estimation error in model-based methods is due to the wrong transitions provided by the model.\nNote that the transitions in the ofﬂine dataset are correct, the model-induced value estimation error may be mitigated by\ndirectly utilizing the ofﬂine dataset. Concretely, we can integrate the true transitions {st, at}H+1\nt=0 ∼D into the Bellman\nequation, i.e.,\neQπ(s0, a0) =\nE\nst,at∼D,\naH+1∼π(sH+1)\n\" H\nX\nt=0\nγtr(st, at) + γH+1 bQπ(sH+1, aH+1)\n#\n.\n(3)\nThis method is named Hybrid Value Estimation (HVE), which uses both ofﬂine data and the model. eQπ is the Q\nfunction learned with HVE.\nFor\nbrevity,\nwe\nﬁrst\ndeﬁne\nJD(s0, a0, H)\n:=\nEst,at∼D\nhPH\nt=0 γtr(st, at)\ni\nand\nJπ(s0, a0, H)\n:=\nEst,at∼dπ\nM(s,a)\nhPH\nt=0 γtr(st, at)\ni\n. Then we derive the following error bound.\nTheorem 3.2. The error bound of the value function induced by Eq. (3) is:\nEDEs,a∼D\n\f\f\fQπ(s, a) −eQπ(s, a)\n\f\f\f ≤\np\nV(JD(s, a, H)) + f(H)Rmaxϵπ + γH+1ϵM,c\nM ,\n3\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nwhere f(H) =\n\u0010\n1−γH+1\n(1−γ)2 −(H+1)γH+1\n1−γ\n\u0011\nand ϵM,c\nM =\n\u0012\n2γRmax(2ϵπ+ϵm)\n(1−γ)2\n+ 4Rmaxϵπ\n1−γ\n\u0013\n.\nThe variance term V(JD(s, a, H)) in Thm. 3.2 is further quantiﬁed by the following theorem.\nTheorem 3.3. If there are n trajectories in dataset D starting from (s, a), then\nV(JD(s, a, H)) = 1\nn (Var(rew) + Var(trans))) ,\nwhere\n(\nVar(rew) = ΣH\nt=0E\n\u0002\nγ2tVt+1[r(st, at)|st]|s0 = s, a0 = a\n\u0003\nVar(trans) = ΣH\nt=1E\n\u0002\nγ2tVt(Jπ(st, at, H −t))|s0 = s, a0 = a\n\u0003 and Vt is the t-step value deﬁned as\nV[·|s0, a0, s1, . . . , st−1, at−1].\nRemark 1. H can be seen as the parameter that trades off between the ﬁrst two terms and the last term in the bound in\nThm. 3.2. When H = −1, we only use the data generated by the model. The ﬁrst two terms are equal to zero, and the\nbound degenerates to that of Thm. 3.1. When H →∞, the last term vanishes and the ﬁrst two terms are dominating. It\nimplies the balance is usually obtained when H is neither too small nor too large.\nRemark 2. To better understand the bound in Thm. 3.2 and Thm. 3.1, we give an example to illustrate the improvement.\nConsider a MDP with deterministic reward function, and let Rmax = 1 and γ = 0.9. Assume policy π and model ˆ\nM\nsatisfy ϵπ = 0.1 and ϵm = 0.05. For H = 5, we can calculate V(JD(s, a, H)) using Thm. 3.3. If n = 9, we have\nV(JD(s, a, 5)) ≤1.723. Then the bound in Thm 3.2 equals 4.55. While the bound in Thm. 3.1 equals 5.8.\nWe hope to further reduce value estimation error based on Thm. 3.2. Considering importance sampling [27] can adjust\nthe value estimation of one policy into that of another policy, we use it to modify the reward provided by the ofﬂine\ndataset. Then we can avoid the inﬂuence of policy divergence. With importance sampling, Eq. (3) is reformulated as\neQπ(s0, a0) =\nE\nst,at∼D,\naH+1∼π(sH+1)\n\" H\nX\nt=0\nρ1:tγtr(st, at) + γH+1 bQπ(sH+1, aH+1))\n#\n,\n(4)\nwhere ρ1:t = ρ1ρ2 · · · ρt for t > 0, ρ1:0 = 1 and ρt =\nπ(at|st)\nπβ(at|st).\nIn this way, the second term f(H)Rmaxϵπ equals zero.\nHowever, the ﬁrst term will increase and become\nV(JIS\nD(s, a, H)), where JIS\nD(s, a, H) := Est,at∼D\nhPH\nt=0 ρ1:tγtr(st, at)\ni\n. The following theorem shows that the\nvariance will be ampliﬁed by ρ1:t.\nTheorem 3.4. Under the conditions of Thm. 3.3, the variance of Eq. (4) is\nV(JIS\nD(s, a, H)) = 1\nn (VarIS(rew) + VarIS(trans))) ,\nwhere\n(\nVarIS(rew) = ΣH\nt=0E\n\u0002\nρ2\n1:tγ2tVt+1[r(st, at)|st]|s0 = s, a0 = a\n\u0003\nVarIS(trans) = ΣH\nt=1E\n\u0002\nρ2\n1:tγ2tVt(Jπ(st, at, H −t))|s0 = s, a0 = a\n\u0003.\nThm. 3.4 shows that the variance will be large if ρ1:t is large, which suggests that we need to balance the ﬁrst and\nsecond term in Thm 3.2 when considering ρ1:t.\n4\nPractical Implementations of HVE\nPrevious sections propose the general framework of HVE, while there are several remaining aspects to discuss for a\npractical implementation. In this section, we discuss (1) how to obtain the behavior policy and dynamic model, (2) how\nto better balance bias and variance, and (3) how to choose the step length H that minimizes the error.\n4.1\nBehavior Policy Learning and Model Learning\nIf the behavior policy πβ(a|s) is available, ρ1:t in Eq. (4) and ϵπ can be computed directly. Otherwise, we can\napproximate the behavior policy by maximizing the log likelihood of observed data like [43]:\nθ∗\nb = arg max\nθb E(s,a)∼Denv\nh\nlog πθb(a|s)\ni\n,\n(5)\n4\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\n1\n1\n3\n5\n7\nhorizon H\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nError bound (normalized)\ntask\nhalfcheetah-mixed\nhopper-mixed\nwalker2d-mixed\nFigure 1: The relationship between value estimation error bound and step length H. When H is −1, the bound\ndegenerates to vanilla model-based value estimation method.\nwhere θb are the parameters of the behavior policy.\nThe model learning process is similar to MBPO [17], which learns an ensemble of probabilistic dynamic models and\nuses them for short rollouts. Each model is parameterized by a neural network whose outputs represent a Gaussian\ndistribution with diagonal covariance: bTψ(st+1|st, at) = N(µψ(st, at), Σψ(st, at)). The model is typically learned by\nmaximum likelihood estimation:\nψ∗= arg max\nψ\nE(s,a,s′)∼Denv[log bTψ(s′|s, a)] .\n(6)\nThe reward function r, if unknown, can be jointly learned in the same way.\n4.2\nImportance Ratio Clipping\nAs suggested by Thm. 3.4, to better balance the inﬂuence of the ﬁrst two terms, we can clip the importance ratio ρ1:t the\nrange of (1 −ϵ, 1 + ϵ) to ensure that it is not far away from 1. As ϵ increases, the ﬁrst term in the bound of Thm. 3.2\nincreases while the second term decreases. Then we can tune ϵ to get a trade-off between the two terms. We use\nV( ¯JIS\nD(s, a, H) to denote the variance with clipped importance ratio, ϵ′\nπ to denote the reduced policy divergence after\nthe truncated importance sampling, the reﬁned error bound can be represented as\nq\nV( ¯JIS\nD(s, a, H)) + f(H)Rmaxϵ′\nπ + γH+1ϵM,c\nM\n(7)\nIn practice, ϵ is robust across environments, and we choose ϵ equals 0.1 in all experiments.\n4.3\nAutomatic Step Length Adjustment\nSec. 3 demonstrates that the value estimation error can be reduced by properly choosing step length H. Since the\noptimal H could change across environments and training stages, it is important to automatically adjust H to avoid\ntedious hyper-parameter tuning and ensure the optimal mean-variance trade-off.\nThe best H is the one that minimizes value estimation error, which is trivial to compute if we know the value estimation\nerror for every H. However, the exact error is intractable because the true value is not known in advance. To tackle this\nissue, we use the error bound in Eq. (7) as a surrogate of value estimation error, and the H that minimizes the error\nbound is chosen as the parameter, i.e.,\nHb = arg min\nH\nq\nV( ¯JIS\nD(s, a, H)) + f(H)Rmaxϵ′\nπ + γH+1ϵM,c\nM .\n(8)\nThe error bound of H is determined if we can compute V( ¯JIS\nD(s, a, H)), ϵ′\nπ and ϵM,c\nM, where ϵM,c\nM is constituted\nby ϵπ and ϵm. In the experiment, we tend to select small Hb because the error of small Hb is not worse than vanilla\n5\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nmodel-based value estimation while remaining unbounded if Hb is too large. This implies that we tend to overestimate\nthe ﬁrst two terms but underestimate the last term. Take MuJoCo tasks as an example. V( ¯JIS\nD(s, a, H)) is upper bounded\nby (1 + ϵ)2 PH\nt=0 γ2tR2\nmax/4 and ϵ′\nπ is upper bounded by ϵπ. We choose the upper bound as the surrogates for the\nreason we mentioned above. Given behavior policy πβ and current policy π, ϵπ can be calculated directly with the data\nin the dataset D. ϵm is difﬁcult to handle because the true model M is not accessible. We assume the output of the true\nmodel satisﬁes Gaussian distribution, whose mean is provided by the transitions in the ofﬂine dataset and the variance\nis identical to that of the learned model. Fig. 1 illustrates the relationship between the approximated error bound and H\nin three MuJoCo tasks. The error bound decreases ﬁrst and then increases as step size H increases, proving that the best\nH achieves a trade-off between three terms in Thm. 3.2. Since the error bound of vanilla model-based value estimation\nis the value at H = −1, the experiment also shows value error reduction of HVE.\n5\nApplications\nWe present two concrete instantiations of HVE: Off-Policy HVE (OPHVE) and Model-based Ofﬂine HVE (MOHVE)\nwhich adopt HVE to improve performance in OPE and ofﬂine RL tasks.\nOPHVE. OPHVE uses HVE for off-policy evaluation. According to the deﬁnition of Q function, the expected return\nof a policy can be expressed as J(π) = Es∼ρ0,a∼π [Qπ(s, a)]. With the estimated Q function, to perform OPE, we only\nneed to sample states from initial state distribution ρ0 and sample actions from the policy to be evaluated to approximate\nthe expectation.\nMOHVE. MOHVE follows the actor-critic framework. The critic is modiﬁed to be updated by the HVE objective to\nbetter guide the training of the actor.\nFrom the error bound of Eq. (7), the second term f(H)Rmaxϵ′\nπ and the last term γH+1ϵM,c\nM of the error bound is\nproportional to policy divergence term ϵπ. To address this problem, we regularize the learned policy to make it close to\nthe behavior policy, which is a common practice in previous ofﬂine RL algorithms [11, 44, 43]. This can be achieved\nby the constrained optimization formulated as follows:\nπ = arg max\nπ\nEs∼Dmix,a∼π(·|s)\nh\nQπ(s, a)\ni\n, s.t. Es∼Dmix\nh\nDKL (π(·|s)||πβ(·|s))\ni\n≤δ .\n(9)\nThe state distribution Dmix is the mixture of the ofﬂine data and the synthetic rollouts from the model: Dmix :=\nαDenv + (1 −α)Dmodel, where α ∈[0, 1] is a hyperparameter to control the ratio of data points drawn from the\nofﬂine dataset. DKL (π||πβ) is the Kullback–Leibler Divergence between the learned policy and behavior policy. We\nuse KL divergence instead of TV divergence in practice as KL divergence has a closed-form solution for Gaussian\ndistribution. This substitution is reasonable because of the following relationship between the two divergences:\nD2\nTV(p, q) ≤2DKL(p, q).\nTo solve this constrained optimization problem, we can use the method of Lagrange multiplier to convert it to an\nobjective that can be optimized by alternating gradient descent steps on the policy parameters θ and a penalty coefﬁcient\nβ:\nmax\nθ\nmin\nβ Es∼Dmix,a∼πθ(·|s)\nh\nQ(s, a) + β(δ −DKL (πθ(·|s)||πβ(·|s)))\ni\n.\n(10)\nThe whole process of MOHVE is summarized in Appx. B.1.\nBesides the last two terms of Eq. (7), we ﬁnd the constraint also reduces the variance term V( ¯JIS\nD(s, H)) by limiting the\nscale of importance ratio as shown in the following proposition.\nProposition 5.1. If the policy π satisﬁes Es∼Dmix [DKL (π(·|s)||πβ(·|s))] ≤δ, and suppose c = maxs,a\n\r\r\r π(a|s)\nπβ(a|s)\n\r\r\r\n∞≤\n∞, then Es∼DmixEa∼πβ(·|st)ρ2\nt ≤cδ + 1.\n6\nExperiments\nWe design several experiments to demonstrate and analyze the performance of OPHVE and MOHVE. We also analyze\nthe effect of step length H and the accuracy of automatic step length adjustment. More experiment details are given in\nAppx. B.\n6\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\n6.1\nOff-Policy Evaluation Performance of OPHVE\nWe follow the setting of DOPE [19] to test the off-policy evaluation performance of OPHVE: For a given dataset D,\nwe estimate the values of a set of policies {π1, π2, ..., πN}. The estimated values are denoted as { ˆV1, ˆV2, ..., ˆVN}. We\nalso deploy these policies in real environments and record their discounted returns as ground truth values, denoted\nas {V1, V2, ..., VN}. The policies to evaluate are generated during the training process of online RL algorithms Soft\nActor-Critic [8]. DOPE is based on the D4RL benchmark of Gym-MuJoCo tasks [45, 24]. The tasks are challenging for\nOPE methods because they have data from various sources and policies that are generated using online RL algorithms,\nmaking it less likely that the behavior and evaluation policies share similar induced state-action distributions [19]. The\nperformance of every method is evaluated using three different metrics: absolute error, regret@k and rank correlation.\nThe details of the metrics are in Appx. B.2.\nDR\nFQE\nIS\nMB\nOPHVE\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n< Absolute Error\n(a) Absolute error\nDR\nFQE\nIS\nMB\nOPHVE\n0.0\n0.1\n0.2\n0.3\n0.4\n> Rank Correlation\n(b) Rank correlation\nDR\nFQE\nIS\nMB\nOPHVE\n0.00\n0.01\n0.02\n0.03\n< Regret@1\n(c) Regret@1\nFigure 2: The overall performance of off-policy evaluation in three metrics. The value of each bar is averaged over all\ntasks and the error bar correspond to the standard deviation. To aggregate across tasks, we normalize the policy values\nand evaluated policy values to range between 0 and 1.\nIn this experiment, we compare OPHVE to several previous methods2, including: (1) Fitted Q-Evaluation (FQE): We\nadopt [25] that estimates the policy value by iteratively performing Bellman update and use the recent implementa-\ntion [47]. (2) Model-Based (MB): MB trains dynamics and reward models by maximizing the log likelihood of the\nnext state and reward from the ofﬂine dataset as recent successful model-based RL algorithms [48, 17]. Then policy\nevaluation is performed by rolling out in this learned model. (3) Importance Sampling (IS): We choose the weighted\nimportance sampling method [27] with the implementation of [47]. The behavior policy is learned the same as our\nmethod. (4) Doubly Robust (DR): We perform weighted doubly-robust [34], which has the least variance, and use the\nimplementation of [47].\nFig. 2 shows the performance of OPHVE and other methods in three metrics. OPHVE consistently outperforms other\nmethods on all metrics. The results of absolute error show that hybrid value estimation reduces the value estimation\nerror, which veriﬁes our theory in Section 3. Besides, OPHVE gets a higher rank correlation and lower regret. In\nconclusion, OPHVE can not only perform accurate evaluation but also select the competitive policies among the polices\nto be evaluated. Detailed OPE results of each task are in Appx. C.1.\n6.2\nOfﬂine Reinforcement Learning Performance of MOHVE\nWe evaluate MOHVE on the D4RL benchmarks [24]. Our baselines include four ofﬂine reinforcement learning\nalgorithms and one imitation learning algorithm. Among the four state-of-the-art ofﬂine RL algorithms, COMBO [21]\nand MOPO [20] are model-based methods, CQL [22] and TD3+BC [23] are model-free method. The imitation learning\nalgorithm is behavior cloning [49].\nWe train MOHVE for 1 million time steps and report the average normalized score of the last ten iterations of policy\nupdate. Table 1 shows the results of ofﬂine reinforcement learning experiments. The numbers of BC are taken\nfrom the D4RL paper, while the results for COMBO, MOPO, TD3+BC, and CQL are based on their respective\npapers [21, 20, 23, 22]. In summary, MOHVE outperforms other methods in 7 out of the 12 tasks, and achieves\ncomparable results in the rest 5 tasks. Previous ofﬂine RL work [20] found that model-based methods perform worse on\nthe task with narrow data distributions (medium, medium-expert datasets) since it’s hard to learn models that generalize\nwell. Our method performs well on these tasks because HVE reduces the value estimation error by adjusting the\nhorizon H in case that the model error is large. Besides better average performance, MOHVE generally shows a smaller\nvariance in every task, indicating better stability and robustness. The learning curves of MOHVE are in Appx. C.2.\n2Despite recent exciting developments in marginalized importance sampling (MIS), it does not show strong empirical performance\ncompared to other methods [19, 46]. Due to the difﬁcult optimization of MIS, we defer the comparison with MIS to future work.\n7\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nTable 1: Performance of MOHVE and baselines on the Gym-MuJoCo datasets, on the normalized return metric\nproposed by D4RL benchmark. The score roughly ranges from 0 to 100, where 0 corresponds to a random policy\nperformance and 100 corresponds to an expert policy performance. Each result is the average score over ﬁve random\nseeds ± standard deviation.\nData Type\nEnv.\nBC\nMOHVE\n(Ours)\nCOMBO\nMOPO\nTD3\n+BC\nCQL\nrandom\nhalfcheetah\n2.1\n33.39±3.23\n38.8±3.7\n35.4±2.5\n10.2±1.3\n35.4\nrandom\nhopper\n1.6\n12.11±0.15\n17.9±1.4\n11.7±0.4\n11.0±0.1\n10.8\nrandom\nwalker2d\n9.8\n21.65±0.04\n7.0±3.6\n13.6±2.6\n1.4±1.6\n7.0\nmedium\nhalfcheetah\n36.1\n56.48±1.22\n54.2±1.5\n42.3±1.6\n42.8±0.3\n44.4\nmedium\nhopper\n29.0\n100.81±0.96\n97.2±2.2\n28.0±12.4\n99.5±1.0\n58.0\nmedium\nwalker2d\n6.6\n78.34±2.53\n81.9±2.8\n17.8±19.3\n79.7±1.8\n79.2\nmixed\nhalfcheetah\n38.4\n57.68±0.89\n55.1±1.0\n53.1±2.0\n43.3±0.5\n46.2\nmixed\nhopper\n11.8\n93.94±2.52\n89.5±1.8\n67.5±24.7\n31.4±3.0\n48.6\nmixed\nwalker2d\n11.3\n51.87±2.25\n56.0±8.6\n39.0±9.6\n25.2±5.1\n26.7\nmed-exp\nhalfcheetah\n35.8\n103.06±2.72\n90.0±5.6\n63.3±38.0\n97.9±4.4\n62.4\nmed-exp\nhopper\n111.9\n111.69±0.19\n111.1±2.9\n23.7±6.0\n112.2±0.2\n111.0\nmed-exp\nwalker2d\n6.4\n106.79±1.97\n103.3±5.6\n44.6±12.9\n105.7±2.7\n98.7\n(a)                                                                               (b)\nFigure 3: Analysis experiments results on walker2d-random. (a): the OPE results in absolute error, rank correlation\nand regret@1. (b): the ofﬂine RL learning curves with different step length H. Note that H equals −1 corresponds to\nonly using the model rollout data.\n6.3\nAnalysis of step length H\nHVE beneﬁts from the proposed way of hybridizing ofﬂine data and model rollout data. The step length H plays an\nimportant role. We analyze the effect of the step length H on OPHVE and MOHVE. The results are shown in Fig. 3.\nOPE results show that H has a great impact on the performance of OPHVE. Compared to -1, 0, 4, 20, OPHVE performs\nthe best in all three metrics when H equals 2, which is the step length selected by the automatic adjustment. It should\nbe noted that the variance in the experiment is induced by stochastic initialization and optimization process controlled\nby random seeds, while the variance term V( ¯JIS\nD(s, a, H)) comes from the generation of the dataset, so the experiment\nvariance cannot reﬂect the variance term.\nOfﬂine RL results show that our automatic adjustment of step length H contributes to the performance gain of MOHVE.\nIn detail, the vanilla model-based method (i.e., H equals −1), and the hybrid method with H equals 0 struggle to\nperform well. When we increase the length of H to 2, the converged performance is improved. Finally, the performance\ndecreases when H is changed to 9. Our method, MOHVE, converges to a higher normalized score more stably through\nautomatically adjusting the parameter H during the training process. In conclusion, the step length H matters in OPE\nand ofﬂine RL, and our automatic selection method succeeds in ﬁnding the near-optimal one. Additional results and\nanalysis on other tasks can be found in Appx. C.3.\n8\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ntime step\n1e6\n0\n20\n40\n60\n80\nnormalized score\nMOHVE\nMOPO\nFigure 4: The learning curve of MOHVE and MOPO on walker2d-mixed. There is oscillation problem in ofﬂine RL.\n6.4\nOfﬂine Policy Selection\nIt is common that the performance of the trained policy oscillates during the training process, as illustrated in Fig. 4.\nTherefore, simply choosing the policy in the end may not imply a good performance. To tackle this issue, we can obtain\npolicy snapshots during the training process and select the best of them. In real-world application, we tend to limit the\nbudget (the amount of policies deployed online after policy selection) of online evaluation to reduce risk and cost, if not\ncompletely forbid it. Since the number of the saved snapshots is usually much larger than the budget, we need to ﬁrst\nperform ofﬂine policy selection to obtain a small enough policy set that includes good policies.\nSince OPHVE achieves a remarkable performance in OPE tasks, we integrate it to MOHVE as the ofﬂine policy\nselection method. Such an integration forms a completely ofﬂine training and evaluation pipeline [9]. We choose\nuniform policy selection as the baseline, since it is a strong baseline under low online evaluation budgets [42]. We train\nMOHVE and save the policy every 50 epochs, and conduct ofﬂine policy selection under different online evaluation\nbudgets. Tab. 2 shows the ofﬂine policy selection results. On one hand, OPHVE selects better policies than uniform\npolicy selection under the same budget. On the other hand, our proposed ofﬂine RL pipeline, MOHVE+OPHVE, can\nfurther improve the performance of MOHVE, which sheds some light on designing ofﬂine RL algorithms more suitable\nfor real word scenario.\nTable 2: Ofﬂine policy selection results. TOP k reports the best online normalized score among k policies selected by\ncorresponding ofﬂine policy selection methods. Each result is the average score over ﬁve random seeds ± standard\ndeviation.\nEnv.\nFinal\nIterations\nUniform Policy Selection\nOPHVE\nTOP1\nTOP3\nTOP5\nTOP1\nTOP3\nTOP5\nwalker2d-mixed\n51.87±2.25\n43.09±0.81\n55.96±1.70\n60.05±2.69\n56.82±7.68\n62.22±6.00\n66.43±5.54\nhopper-mixed\n93.94±2.52\n66.78±8.73\n89.31±6.30\n94.78±3.28\n82.45±22.30\n97.06±2.44\n97.06±2.44\n7\nConclusion\nIn this paper, we propose Hybrid Value Estimation (HVE) to perform a more accurate value function estimation in the\nofﬂine setting. It automatically adjusts the step length parameter to get a bias-variance trade-off. The error bound of\nHVE is better than that of vanilla model-based value estimation. We provide two concrete algorithms OPHVE and\nMOHVE and thoroughly discuss the details of the implementation. Empirical evaluations on MuJoCo tasks corroborate\nthe theoretical claim, showing that HVE indeed selects the best step length and contributes to the performance in\n9\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\noff-policy evaluation and ofﬂine RL tasks. We also test the performance of integrating MOHVE with OPHVE to\nselect the best policy during training in an ofﬂine manner, which could shed some light on improving the training and\nevaluation pipeline of ofﬂine RL.\nReferences\n[1] Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction. Adaptive computation and\nmachine learning. MIT Press, 1998.\n[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex\nGraves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik,\nIoannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-\nlevel control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n[3] John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region policy\noptimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML’15), pages\n1889–1897, Lille, France, 2015.\n[4] Xu-Hui Liu, Zhenghai Xue, Jingcheng Pang, Shengyi Jiang, Feng Xu, and Yang Yu. Regret minimization experi-\nence replay in off-policy reinforcement learning. In Proceedings of the 34th conference on Neural Information\nProcessing Systems (NeurIPS’21), Virtual Event, 2021.\n[5] Xiting Wang, Yiru Chen, Jie Yang, Le Wu, Zhengtao Wu, and Xing Xie. A reinforcement learning framework for\nexplainable recommendation. In Proceedings of the 18th International Conference on Data Mining (ICDM’18),\npages 587–596, Singapore, 2018.\n[6] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. Recommendations with\nnegative feedback via pairwise deep reinforcement learning. In Proceedings of the 24th International Conference\non Knowledge Discovery & Data Mining (KDD’18), pages 1040–1048, London, UK, 2018.\n[7] Xue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Edward Lee, Jie Tan, and Sergey Levine. Learning\nagile robotic locomotion skills by imitating animals. In Proceedings of the 14th Robotics: Science and Systems\n(RSS’20), Virtual Event, 2020.\n[8] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy\ndeep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on\nMachine Learning (ICML’18), pages 1856–1865, Stockholmsmässan, Sweden, 2018.\n[9] Rongjun Qin, Songyi Gao, Xingyuan Zhang, Zhen Xu, Shengkai Huang, Zewen Li, Weinan Zhang, and Yang Yu.\nNeorl: A near real-world benchmark for ofﬂine reinforcement learning. CoRR, abs/2102.00714, 2021.\n[10] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration.\nIn Proceedings of the 36th International Conference on Machine Learning (ICML’19), pages 2052–2062, Long\nBeach, CA, 2019.\n[11] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via\nbootstrapping error reduction. In Proceedings of the 33rd Conference on Neural Information Processing Systems\n(NeurIPS’19), pages 11761–11771, Vancouver, Canada, 2019.\n[12] Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang Zeng. Virtual-Taobao: Virtualizing real-world\nonline retail environment for reinforcement learning. In Proceedings of the 33rd AAAI Conference on Artiﬁcial\nIntelligence (AAAI’19), Honolulu, HI, 2019.\n[13] Wenjie Shang, Qingyang Li, Zhiwei Qin, Yang Yu, Yiping Meng, and Jieping Ye. Partially observable environment\nestimation with uplift inference for reinforcement learning based recommendation. Machine Learning, (9):2603–\n2640, 2021.\n[14] Xiong-Hui Chen, Yang Yu, Qingyang Li, Fan-Ming Luo, Zhiwei Qin, Wenjie Shang, and Jieping Ye. Ofﬂine\nmodel-based adaptable policy learning. In Proceedings of the 35th Annual Conference on Neural Information\nProcessing Systems (NeurIPS’21), Virtual Event, 2021.\n[15] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural\nInformation Processing Systems 31 (NeurIPS’18), pages 2455–2467, Montréal, Canada, 2018.\n[16] Kefan Dong, Yuping Luo, Tianhe Yu, Chelsea Finn, and Tengyu Ma. On the expressivity of neural networks\nfor deep reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning\n(ICML’20), pages 2627–2637, Virtual Event, 2020.\n10\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\n[17] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy\noptimization. In Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS’19),\npages 12498–12509, Virtual Event, 2019.\n[18] Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments. In Advances in Neural\nInformation Processing Systems 33 (NeurIPS’20), pages 15737–15749, 2020.\n[19] Justin Fu, Mohammad Norouzi, Oﬁr Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang,\nMichael R. Zhang, Yutian Chen, Aviral Kumar, Cosmin Paduraru, Sergey Levine, and Thomas Paine. Benchmarks\nfor deep off-policy evaluation. In Proceedings of the 9th International Conference on Learning Representations\n(ICLR’21), Virtual Event, 2021.\n[20] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu\nMa. Mopo: Model-based ofﬂine policy optimization. In Proceedings of the 34th Conference on Neural Information\nProcessing Systems (NeurIPS’20), pages 14129–14142, Virtual Event, 2020.\n[21] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. COMBO:\nconservative ofﬂine model-based policy optimization. In Proceedings of the 35th Conference on Neural Information\nProcessing Systems (NeurIPS’21), pages 28954–28967, Virtual Event, 2021.\n[22] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine reinforcement\nlearning. In Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS’20), Virtual\nEvent, 2020.\n[23] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to ofﬂine reinforcement learning. In Proceedings\nof the 35th Annual Conference on Neural Information Processing Systems (NeurIPS’21), Virtual Event, 2021.\n[24] Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep data-driven\nreinforcement learning. CoRR, abs/2004.07219, 2020.\n[25] Hoang Minh Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Proceedings of\nthe 36th International Conference on Machine Learning (ICML’19), pages 3703–3712, Long Beach, CA, 2019.\n[26] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of\nMachine Learning Research, 6:503–556, 2005.\n[27] Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy evaluation. In\nProceedings of the 17th International Conference on Machine Learning (ICML’00), pages 759–766, Stanford, CA,\n2000.\n[28] Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. In Proceedings of the 34th International\nConference on Machine Learning (ICML’17), pages 1372–1383, Sydney, Australia, 2017.\n[29] Oﬁr Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted\nstationary distribution corrections.\nIn Proceedings of the 33rd Annual Conference on Neural Information\nProcessing Systems (NeurIPS’19), pages 2315–2325, Vancouver, Canada, 2019.\n[30] Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized ofﬂine estimation of stationary\nvalues. In Proceedings of the 8th International Conference on Learning Representations (ICLR’20), Addis Ababa,\nEthiopia, 2020.\n[31] Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation.\nIn Proceedings of the 37th International Conference on Machine Learning (ICML’20), pages 9659–9668, Virtual\nEvent, 2020.\n[32] Mengjiao Yang, Oﬁr Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via the regu-\nlarized lagrangian. In Proceedings of the 34rd Annual Conference on Neural Information Processing Systems\n(NeurIPS’20), Virtual Event, 2020.\n[33] Christina J. Yuan, Yash Chandak, Stephen Giguere, Philip S. Thomas, and Scott Niekum. SOPE: spectrum\nof off-policy estimators. In Proceedings of the 35th Conference on Neural Information Processing Systems\n(NeurIPS’21), pages 18958–18969, Virtual Event, 2021.\n[34] Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In Proceedings of\nthe 33rd International Conference on Machine Learning (ICML’16), pages 652–661, New York City, NY, 2016.\n[35] Philip S. Thomas and Emma Brunskill. Data-efﬁcient off-policy policy evaluation for reinforcement learning. In\nProceedings of the 33rd International Conference on Machine Learning (ICML’16), pages 2139–2148, New York\nCity, NY, 2016.\n11\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\n[36] Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine. Model-\nbased value expansion for efﬁcient model-free reinforcement learning. In Proceedings of the 35th International\nConference on Machine Learning (ICML’18), Stockholmsmässan, Sweden, 2018.\n[37] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in\na handful of trials using probabilistic dynamics models. In Proceedings of the 32nd Conference on Neural\nInformation Processing Systems (NeurIPS’18), pages 4759–4770, Montréal, Canada, 2018.\n[38] Fan-Ming Luo, Shengyi Jiang, Yang Yu, Zongzhang Zhang, and Yi-Feng Zhang. Adapting environment sudden\nchanges by learning context sensitive policy. In Proceedings of the 36th AAAI Conference on Artiﬁcial Intelligence\n(AAAI’22), Virtual Conference, 2022.\n[39] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based ofﬂine\nreinforcement learning. In Proceedings of the 34th Conference on Neural Information Processing Systems\n(NeurIPS’20), Virtual Event, 2020.\n[40] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based ofﬂine reinforcement\nlearning with diversiﬁed q-ensemble. In Proceedings of the 35th Annual Conference on Neural Information\nProcessing Systems (NeurIPS’21), pages 7436–7447, Virtual Event, 2021.\n[41] Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max q-learning\noperator for simple yet effective ofﬂine and online RL. In Proceedings of the 38th International Conference on\nMachine Learning (ICML’21), pages 3682–3691, Virtual Event, 2021.\n[42] Vladislav Kurenkov and Sergey Kolesnikov. Showing your ofﬂine reinforcement learning work: Online evaluation\nbudget matters. CoRR, abs/2110.04156, 2021.\n[43] Noah Y. Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas\nLampe, Roland Hafner, Nicolas Heess, and Martin A. Riedmiller. Keep doing what worked: Behavior modelling\npriors for ofﬂine reinforcement learning. In Proceedings of the 8th International Conference on Learning\nRepresentations (ICLR’20), Addis Ababa, Ethiopia, 2020.\n[44] Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning. CoRR,\nabs/1911.11361, 2019.\n[45] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech\nZaremba. Openai gym. CoRR, abs/1606.01540, 2016.\n[46] Cameron Voloshin, Hoang Minh Le, Nan Jiang, and Yisong Yue. Empirical study of off-policy policy evaluation\nfor reinforcement learning. CoRR, abs/1911.06854, 2019.\n[47] Ilya Kostrikov and Oﬁr Nachum. Statistical bootstrapping for uncertainty estimation in off-policy evaluation.\nCoRR, abs/2007.13609, 2020.\n[48] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in\na handful of trials using probabilistic dynamics models. In Proceedings of the 32nd Conference on Neural\nInformation Processing Systems (NeurIPS’18), pages 4759–4770, Montréal, Canada, 2018.\n[49] Dean Pomerleau. Efﬁcient training of artiﬁcial neural networks for autonomous navigation. Neural Computation,\n3:88–97, 1991.\n12\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nA\nProof of results in the main paper\nA.1\nProof of Thm. 3.2\nTo provide proof of Thm. 3.2, we need Thm. 3.1, Lem. A.1 and Lem. A.2.\nLemma A.1. Let Jπ(sw, aw, H) = Eπ[PH\nt=0 γtr(st, at)|s0 = sw, a0 = aw], then\nED\n\f\fJπ(sw, aw, H) −Jπβ(sw, aw, H)\n\f\f ≤\n\u00121 −γH+1\n(1 −γ)2 −(H + 1)γH+1\n1 −γ\n\u0013\nRmaxϵπ\nProof. Let dt,w(s) represents the distribution of state at time t by starting with (sw, aw) and following policy π, dβ\nt,w(s)\nrepresents the distribution of state at time t by starting at with (sw, aw) and following policy πβ, and d′\nt,w(s) represents\nthe distribution of states at time t by following πβ conditioned on starting with (sw, aw) and the fact πβ performs\ndifferent from π at least one step. Then\ndβ\nt,w(s) = pt−1dt,w(s) + (1 −pt−1)d′\nt,w(s),\nwhere pt−1 is the probability πβ performs the same as π in t −1 steps.\nFor simpliﬁcation, we denote π(·|s) as π(s), denote EDEs∼D as Es,D. Notice that\nEs,DPr(π(s) = πβ(s))\n= Es,D\nZ\nmin(π(a|s), πβ(a|s))da\n= Es,D\n\u0012 Z\nπ(a|s)da −\nZ \u0000π(a|s) −min(π(a|s), πβ(a|s))\n\u0001\nda\n\u0013\n= Es,D\n\u0010\n1 −DTV(π(s), πβ(s))\n\u0011\n= 1 −ϵπ\nLet p = Es,DPr(π(s) = πβ(s)), we have\npt ≥pt−1p ≥pt\nThen\nEs,D\n\f\f\fdβ\nt,w(s) −dt,w(s)\n\f\f\f = Es,D(1 −pt−1)\n\f\fd′\nt,w(s) −dt,w(s)\n\f\f\n≤Es,D(1 −pt−1) ≤1 −pt\n(11)\nWe calculate ED\n\f\fJπβ(sw, aw, H) −Jπ(sw, aw, H)\n\f\f as follows.\nED\n\f\fJπβ(sw, aw, H) −Jπ(sw, aw, H)\n\f\f\n= ED\n\f\f\f\f\fEst∼dβ\nt ,at∼πβs′\nt∼dt,a′\nt∼π\nH\nX\nt=0\nγt (r(st, at) −r(s′\nt, a′\nt))\n\f\f\f\f\f\n=\n\f\f\f\f\f\nH\nX\nt=0\nγt\nZ\n(dβ\nt,w(s)πβ(a|s) −dt,w(s)π(a|s))r(s, a)dsda\n\f\f\f\f\f\n≤\nH\nX\nt=0\nγt\nZ \f\f\fdβ\nt,w(s)πβ(a|s) −dt,w(s)π(a|s)\n\f\f\f r(s, a)dsda\n13\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nUsing triangle inequality,\nH\nX\nt=0\nγt\nZ \f\f\fdβ\nt,w(s)πβ(a|s) −dt,w(s)π(a|s)\n\f\f\f r(s, a)dsda\n≤\nH\nX\nt=0\nγt\nZ \u0012 \f\f\fdβ\nt,w(s) −dt,w(s)\n\f\f\f π(a|s) + |π(a|s) −πβ(a|s)| dβ\nt,sw(s)\n\u0013\nr(s, a)dsda\n≤\nH\nX\nt=0\nγtRmax\nZ \u0012 \f\f\fdβ\nt,sw(s) −dt,sw(s)\n\f\f\f + |π(a|s) −πβ(a|s)|\n\u0013\ndsda\nUsing Eq. (11)\nED\n\f\fJπβ(sw, aw, H) −Jπ(sw, aw, H)\n\f\f\n≤Rmax\nH\nX\nt=0\nγt\u0010\n(1 −pt) + ϵπ\n\u0011\n≤Rmax\nH\nX\nt=0\nγt\u0010\n1 −(1 −ϵπ)t + ϵπ\n\u0011\n≤Rmax\nH\nX\nt=0\nγttϵπ\n=\n\u00121 −γH+1\n(1 −γ)2 −(H + 1)γH+1\n1 −γ\n\u0013\nRmaxϵπ\nAnd then we get\nED\n\f\fJπ(sw, aw, H) −Jπβ(sw, aw, H)\n\f\f ≤\n\u00121 −γH+1\n(1 −γ)2 −(H + 1)γH+1\n1 −γ\n\u0013\nRmaxϵπ\nLemma A.2. Let JD(sw, aw, H) = Est,at∼D[PH\nt=0 γtr(st, at)|s0 = sw], we have\nED\n\f\fJπβ(sw, aw, H) −JD(sw, aw, H)\n\f\f ≤\np\nV(JD(sw, aw, H))\nProof. Note that EDJD(sw, aw, H) = Jπβ(sw, aw, H). Using the bias-variance decomposition of MSE, we have\nED\n\f\fJπβ(sw, aw, H) −JD(sw, aw, H)\n\f\f2\n= (Jπβ(sw, aw, H) −EDJD(sw, aw, H))2 + V(JD(sw, aw, H))\n= V(JD(sw, aw, H))\nUsing Jensen’s inequality,\n\u0000ED\n\f\fJπβ(sw, aw, H) −JD(sw, aw, H)\n\f\f\u00012 ≤ED\n\f\fJπβ(sw, aw, ) −JD(sw, aw, H)\n\f\f2\nThen we conclude the proof.\nProof of Thm. 3.2. Note that\nQπ(s, a) = Jπ(s, a, H) + γH+1Es′∼P π\nH+1(s,a),a′∼πQπ(s′, a′) ,\nbQπ(s, a) = JD(s, a, H) + γH+1Es′∼P π\nH+1(s,a),a′∼πQπ\nc\nM(s′, a′) ,\nwhere P π\nH+1(s, a) denotes the transition starting from (s, a) and following policy π for H steps.\n14\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nED\n\f\f\fQπ(s, a) −bQπ(s, a)\n\f\f\f\n(a)\n≤ED |Jπ(s, a, H) −JD(s, a, H)| + EDγH+1 \f\f\fEs′∼P π\nH+1(s,a)Qπ(s′) −Es′∼P π\nH+1(s,a)Qπ\nM(s′)\n\f\f\f\n(b)\n≤\np\nV(JD(s, a, H)) +\n\u00121 −γH+1\n(1 −γ)2 −(H + 1)γH+1\n1 −γ\n\u0013\nRmaxϵπ\n+ γH+1\n\u00122γRmax(2ϵπ + ϵm)\n(1 −γ)2\n+ 4Rmaxϵπ\n1 −γ\n\u0013\n,\nwhere (a) uses triangle inequality and (b) uses Lem. A.1 and Lem. A.2.\nA.2\nProof of Thm. 3.3 and Thm. 3.4\nThm. 3.4 can be reduced to Thm. 3.3 if ρ0:t = 1, so we only need to prove Thm. 3.4.\nLet ˆJ(sw, aw, H) = PH\nt=0 γtρ0:tr(st, at) given s0 = sw, a0 = aw, then\nˆJ(st, at, H −t) =\n\u0010\nr(st, at) + γρt+1 ˆJ(st+1, at+1, H −t −1)\n\u0011\nWe use the shorthand: Et[·] := E[·|s0, a0, . . . , st−1, at−1] for conditional expectations, and Vt[·] for variances similarly.\nLemma A.3.\nVt( ˆJ(st, at, H −t)) = Et\n\u0002\nρ2\ntVt+1 [r(st, at)]\n\u0003\n+ Et\nh\nρ2\ntγ2Vt+1[ ˆJ(st+1, at+1, H −t −1)\ni\n+ Vt(Jπ(st, π(st), H −t))\nVt+1\nh\nˆJ(st, at, H)\ni\n= Et+1\n\u0002\nρ2\ntVt+1 [r(st, at)]\n\u0003\n+ Et+1\nh\nρ2\ntγ2Vt+1[ ˆJD(st+1, at+1, H −t −1)\ni\nProof.\nVt( ˆJ(st, at, H −t))\n= Et\nh\nˆJ2(st, at, H −t)\ni\n−\n\u0000EtJπ(st, πβ(st), H −t)\n\u00012\n= Et\n\u0014\u0010\nr(st, at) + γρt+1 ˆJ(st+1, at+1, H −t −1)\n\u00112\n−J2\nπ(st, at, H −t)\n\u0015\n+ Vt(Jπ(st, at, H −t))\n= Et\n\u0014\u0010\nJπ(st, at, H −t) + r(st, at) + γρt+1 ˆJ(st+1, at+1, H −t −1) −Jπ(st, at, H −t)\n\u00112\n−J2\nπ(st, at, H −t)\n\u0003\n+ Vt(Jπ(st, at, H −t))\n= Et\nh\u0010\nJπ(st, at, H −t) +\n\u0010\nr(st, at) −R(st, at) + γρt+1\n\u0010\nˆJ(st+1, at+1, H −t −1)\n−Et+1[Jπ(st+1, at+1, H −t −1)]\n\u0011\u0011\u00112\n−J2\nπ(st, at, H −t)\n\u0015\n+ Vt(Jπ(st, at, H −t))\n(a)\n= Et\n\u0002\nE\n\u0002\nJ2\nπ(st, at, H −t) −J2\nπ(st, π(st), H −t)|st\n\u0003\u0003\n+ Et\n\u0002\nE\n\u0002\n(r(st, at) −R(st, at))2|st, at\n\u0003\u0003\n+ Et\nh\nE\nh\nγ2ρ2\nt+1( ˆJ(st+1, at+1, H −t −1) −Et+1Jπ(st+1, at+1, H −t −1))2|st, at\nii\n+ Vt(Jπ(st, at, H −t))\n= Et [Vt+1 [r(st, at)]] + Et\nh\nρ2\nt+1γ2Vt+1[ ˆJ(st+1, at+1, H −t −1)\ni\n+ Vt(Jπ(st, at, H −t))\nwhere R(s, a) = E[r(s, a)] and (a) uses the fact that conditioned on st and at, r(st, at) −R(st, at) and\nˆJ(st+1, at+1, H −t −1) −Et+1Jπ(st+1, at+1, H −t −1) are independent and have zero means, and all other\nterms are constants.\n15\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nSimilarly,\nVt+1\nh\nˆJ(st, at, H)\ni\n= Et+1\nh\nˆJ2(st, at, H −t)\ni\n−(Et+1 [Jπ(st, at, H −t)])2\n= Et+1 [Vt+1 [r(st, at)]] + Et+1\nh\nρ2\nt+1γ2Vt+1[ ˆJD(st+1, at+1, H −t −1)\ni\n+ Vt+1[Jπ(st, at, H −t)]\n= Et+1 [Vt+1 [r(st, at)]] + Et+1\nh\nρ2\nt+1γ2Vt+1[ ˆJD(st+1, at+1, H −t −1)\ni\n.\nThe last equality follows the fact that Vt+1 [Jπ(st, at, H −t)] = 0.\nProof of Thm. 3.4.\nV( ˆJ(sw, aw, H))\n= V[ ˆJ(s0, a0, H)|s0 = sw, a0 = aw]\n= V1[ ˆJ(s0, a0, H)]\n(a)\n= E1 [V1 [r(s0, a0)]] + E1\nh\nρ2\n1γ2V1[ ˆJ(s1, a1, H −1)]\ni\n(b)\n=\nH\nX\nt=0\nE1\n\u0002\nρ2\n1:tγ2tVt+1[r(st, at)]\n\u0003\n+\nH\nX\nt=1\nE1\n\u0002\nρ2\n1:tγ2tVt+1(Jπ(st+1, at+1, H −t))\n\u0003\n=\nH\nX\nt=0\nE\n\u0002\nρ2\n1:tγ2tVt+1[r(st, at)]|s0 = sw, a0 = aw\n\u0003\n+\nH\nX\nt=1\nE\n\u0002\nρ2\n1:tγ2tVt+1(Jπ(st+1, at+1, H −t))|s0 = sw, a0 = aw\n\u0003\n,\nwhere we deﬁne ρ1:0 = 1, (a) and (b) use Lem. A.3.\nAccording to the deﬁnition of JD(sw, aw, H), V(JD(sw, aw, H)) = V( ˆJ(sw, aw, H))/n, where n is the number of\ntrajectories in dataset D starting from (sw, aw).\nA.3\nProof of Prop. 5.1\nWe ﬁrst introduce the concept of the chi-square divergence.\nDeﬁnition A.4. For r.v. P and Q, the ch-square divergence of P and Q, denoted as χ2(P, Q), is deﬁned as\nχ2(P, Q) =\nZ dP 2\ndQ −1\nThe following lemma reveal the relationship between chi-square divergence and KL divergence.\nLemma A.5. Let c =\n\r\r\r dP\ndQ\n\r\r\r\n∞≤∞, then\ncDKL(P, Q) ≥χ2(P, Q)\nProof. Note that log x ≥x−1\nx ,\ncDKL(P, Q) ≥\n\r\r\r\r\ndP\ndQ\n\r\r\r\r\n∞\nZ\ndP\n\u0012dP/dQ −1\ndP/dQ\n\u0013\n≥χ2(P, Q)\n16\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nProof of Prop. 5.1. Let P be π(·|s) and Q be πβ(·|s), Lem. A.5 can be rewritten as\ncDKL(π(·|s), πβ(·|s)) ≥χ2(π(·|s), πβ(·|s))\nNote that\nχ2(π(·|s), πβ(·|s)) =\nZ \u0012π2(a|s)\nπβ(a|s) −1\n\u0013\nda\n=\nZ\nπβ(a|s)\n \nπ2(a|s)\nπ2\nβ(a|s)\n!\nda −1\n= Ea∼πβ\n\"\nπ2(a|s)\nπ2\nβ(a|s)\n#\n−1 ,\nwe can get the relationship between importance ratio and policy KL divergence as\ncDKL(P, Q) ≥Ea∼πβ\n\u0002\nρ2\nt\n\u0003\n−1 .\nThe ﬁnal result can be obtained by applying the expectation of state distribution to both sides of the inequality.\nB\nExperiment Details\nB.1\nDetailed Training Procedure of MOHVE\nAccording to the theory of Sec. 3, we need to build two separate networks to represent bQ and eQ. In practice, simply\nusing one neural network is sufﬁcient for good performance. Concretely, let Qφk be the Q function in the k-th iteration,\nwe update Q function using Eq. (12) and (13) iteratively.\nφk+1 = φk −η\n2∇φEs,a∼Denv\nh\u0000Q(s, a) −TISH(s, a)\n\u00012i\n,\n(12)\nφk+2 = φk+1 −η\n2∇φEs,a∼Dmodel\n\u0014\u0010\nQ(s, a) −ˆBπQk+1(s, a)\n\u00112\u0015\n,\n(13)\nwhere η is the learning rate and TISH(s0, a0) := PH\nt=0 γtρ0:tr(st, at) + γH+1Qk(sH+1, aH+1).\nWe summarize the whole training process of MOHVE in Alg. 1.\nAlgorithm 1 MOHVE: Model-based Ofﬂine RL with Hybrid Value Estimation\nRequire: ofﬂine data Denv, critic network Qφ with parameter φ, policy network πθ with parameter θ, model rollout\nhorizon h, real data ratio α, KL regularization parameter δ, NH epochs between H update.\n1: Train an ensemble of N probabilistic dynamics { ˆT i\nψ(s′, r|s, a) = N(µi\nψ(s, a), Σi\nψ(s, a))}N\ni=1 on Denv with Eq. (6).\n2: Learn behavior policy πβ with Eq. (5).\n3: Initialize policy π and empty replay buffer Dmodel ←∅.\n4: for epoch e = 1, 2, ..., do\n5:\nSample b states s1 from Denv as the initial states of model rollouts.\n6:\nRollout h step in dynamics { ˆT}N\ni=1 and add samples to Dmodel.\n7:\nUpdate the Q-function Qφ via Hybrid Value Estimation by solving Eq. (12) and (13) iteratively.\n8:\nImprove policy πθ under state marginal of Dmix by solving Eq. (10).\n9:\nif e mod NH = 0 then\n10:\nAdjust step size H with Eq. (8).\n11:\nend if\n12: end for\nB.2\nOPE Metrics\nWe use the following metrics in the OPE experiments, which are the same as those used in [19]. V π denotes the true\nvalue of the policy, and ˆV π denotes the estimated value of the policy.\n17\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nAbsolute Error Absolute error is the difference between the true value and the estimated value of a policy:\nAbsolute Error =\n\f\f\fV π −ˆV π\f\f\f\n(14)\nRank Correlation Rank correlation is an assessment of the degree of correlation between the estimated values and the\ntrue values. The deﬁnition of rank correlation is:\nRank Correlation = Cov(V π\n1:N, ˆV π\n1:N)\nσ(V π\n1:N)σ( ˆV π\n1:N)\n(15)\nRegret@k Regret@k is the difference between the value of the best policy in the entire set, and the value of the best\npolicy in the estimated top-k set. It can be written as:\nRegret@k = max\ni∈1:N V π\ni −\nmax\nj∈topk(1:N) V π\nj\n(16)\nB.3\nAdditional Experimental Setup\nOfﬂine Dataset. We make experiments on the D4RL benchmark of Gym-MuJoCo tasks [45, 24] in both OPE and\nofﬂine RL experiments, including three environments (hopper, walker2d, and halfcheetah), four dataset types (random,\nmedium, medium-replay, and medium-expert), and therefore twelve problem settings in total. The datasets are generated\nby different kinds of behavior policies as follows:\n• random: roll out a randomly initialized policy for 1M steps.\n• medium: train a policy online with SAC to approximately 1/3 the performance of the expert, then roll it out for 1M\nsteps.\n• mixed: uses the replay buffer of a policy trained up to the performance of the medium agent.\n• medium-expert: combine samples from a fully-trained policy with samples from a partially trained policy.\nHyperparameters. Here we discuss the hyperparameters that we use for OPHVE and MOHVE.\n• Model rollout length h. Similar to most of the model-based algorithms [17, 20, 21], model rollout length h is an\nimportant hyperparameter in our experiments. Note that different from the hybrid step length H, h is the length of\nrollout in the learned model. We use h = 10 in all tasks except for hopper-mixed and walker2d-mixed, unlike prior\nmodel-based methods [17, 20, 21], whose maximum rollout length is 5. We hypothesize the longer rollout length of\nHVE owing to its ability to reduce the value estimation error induced by large h.\n• Maximum of the hybrid step length Hmax. To avoid too large step length H, which is computed through\nminimizing the approximated error bound, we clip H with parameter Hmax. We ﬁnd this parameter robust, and set\nit to 4 in 10 out of 12 tasks.\n• KL regularization parameter δ. The KL divergence regularization (cf. Eq. (10)) which controls the policy\ndivergence term ϵπ.\n• Real data ratio α. This parameter determines the state distribution Dmix used in the policy improvement stage. We\nsearch for α ∈{0.2, 0.5, 0.8}.\nThe four mentioned hyper-parameters for each task are listed in Tab. 3.\nModel Training and Usage.\nWe train an ensemble of 7 models and use 5 models with a smaller validation error.\nEach model is parameterized as a 4-layer feedforward neural network with 256 hidden units. And there are two heads\noutputting the mean and logarithmic standard deviation of the next state. We also apply standardization to the input\nstate and reward of the model. During each epoch, we sample 50000 states from the ofﬂine dataset as the initial state of\neach model rollout trajectory.\nFor OPHVE, we perform 100k gradient updates to learn the Q function. For MOHVE, we set NH, the frequency of\nupdating H, to 200 across all tasks, considering the stability of training. Other hyperparameters that correspond to the\nbackbone RL algorithm SAC like learning rate and gradient steps follow the setup of COMBO [21].\n18\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nTable 3: Hyperparameters used in experiments.\nData Type\nEnv.\nh\nHmax\nδ\nα\nrandom\nhalfcheetah\n10\n4\n20\n0.2\nrandom\nhopper\n10\n4\n10\n0.8\nrandom\nwalker2d\n10\n4\n10\n0.5\nmedium\nhalfcheetah\n10\n4\n20\n0.5\nmedium\nhopper\n10\n4\n10\n0.2\nmedium\nwalker2d\n10\n4\n5\n0.8\nmixed\nhalfcheetah\n10\n1\n20\n0.2\nmixed\nhopper\n3\n4\n50\n0.5\nmixed\nwalker2d\n1\n2\n8\n0.5\nmed-exp\nhalfcheetah\n10\n4\n5\n0.2\nmed-exp\nhopper\n10\n4\n0.5\n0.2\nmed-exp\nwalker2d\n10\n4\n5\n0.5\nB.4\nComputational Resources\nAll experiments are conducted on a single NVIDIA GeForce RTX 2080 Ti. OPHVE takes about 1.5 hours to estimate\n10 policies. And it takes about 10 hours to train MOHVE for 1000 epochs, including the time of online evaluation every\nepoch.\nB.5\nLicense of datasets\nWe acknowledge that D4RL datasets use the MIT license.\nC\nAdditional Results\nC.1\nOPE Experiments\nDetailed OPE results on each task are shown in Fig. 5.\n19\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\n0.0\n0.5\nhalfcheetah-medium-expert\nhopper-medium-expert\nwalker2d-medium-expert\nhalfcheetah-mixed\nhopper-mixed\nwalker2d-mixed\nhalfcheetah-medium\nhopper-medium\nwalker2d-medium\nhalfcheetah-random\nhopper-random\nwalker2d-random\nOPHVE\n0.0\n0.5\nFQE\n0.0\n0.5\n< Absolute Error\nDR\n0.0\n0.5\nIS\n0.0\n0.5\nMB\n0\n1\nhalfcheetah-medium-expert\nhopper-medium-expert\nwalker2d-medium-expert\nhalfcheetah-mixed\nhopper-mixed\nwalker2d-mixed\nhalfcheetah-medium\nhopper-medium\nwalker2d-medium\nhalfcheetah-random\nhopper-random\nwalker2d-random\nOPHVE\n0\n1\nDR\n0\n1\n> Rank Correlation\nFQE\n0\n1\nIS\n0\n1\nMB\n0.0\n0.1\nhalfcheetah-medium-expert\nhopper-medium-expert\nwalker2d-medium-expert\nhalfcheetah-mixed\nhopper-mixed\nwalker2d-mixed\nhalfcheetah-medium\nhopper-medium\nwalker2d-medium\nhalfcheetah-random\nhopper-random\nwalker2d-random\nOPHVE\n0.0\n0.1\nDR\n0.0\n0.1\n< Regret@1\nFQE\n0.0\n0.1\nMB\n0.0\n0.1\nIS\n20\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nC.2\nOfﬂine RL Experiments\nWe plot the learning curves of MOHVE on all tasks. We repeat every experiment ﬁve times in different random seeds.\nAll results are shown in Fig. 6. MOHVE converges to relatively high scores in most datasets, which demonstrates the\neffectiveness of our Hybrid Value Estimation method in ofﬂine RL. Furthermore, the training process is stable with a\nsmall variance.\n0.0\n0.5\n1.0\ntime step\n1e6\n0\n50\n100\nnormalized score\nhc-medium-expert\n0.0\n0.5\n1.0\ntime step\n1e6\n50\n100\nnormalized score\nhp-medium-expert\n0.0\n0.5\n1.0\ntime step\n1e6\n0\n50\n100\nnormalized score\nwk-medium-expert\n0.0\n0.5\n1.0\ntime step\n1e6\n20\n40\n60\nnormalized score\nhc-mixed\n0.0\n0.5\n1.0\ntime step\n1e6\n0\n50\n100\nnormalized score\nhp-mixed\n0.0\n0.5\n1.0\ntime step\n1e6\n0\n20\n40\n60\nnormalized score\nwk-mixed\n0.0\n0.5\n1.0\ntime step\n1e6\n20\n40\nnormalized score\nhc-medium\n0.0\n0.5\n1.0\ntime step\n1e6\n25\n50\n75\n100\nnormalized score\nhp-medium\n0.0\n0.5\n1.0\ntime step\n1e6\n0\n25\n50\n75\nnormalized score\nwk-medium\n0.0\n0.5\n1.0\ntime step\n1e6\n10\n20\n30\nnormalized score\nhc-random\n0.0\n0.5\n1.0\ntime step\n1e6\n5\n10\nnormalized score\nhp-random\n0.0\n0.5\n1.0\ntime step\n1e6\n0\n10\n20\nnormalized score\nwk-random\nFigure 6: Learning curves of MOHVE on all tasks, where hc, hp and wk represent halfcheetah, hopper and walker2d\nrespectively. The solid curves are the mean of normalized return and shadow is the standard error. The performance of\nMOHVE is stable and robust in most of the tasks.\n21\nHybrid Value Estimation for Off-policy Evaluation and Ofﬂine Reinforcement Learning\nC.3\nAnalysis of step length H on walker2d-medium-expert\nWe analyze the effect of step length H on OPHVE and MOHVE on walker2d-medium-expert task. The results of OPE\nare shown in Fig. 7. When H = 0, chosen by the automatic adjustment mechanism, OPHVE performs better than other\nchoices on rank correlation and regret@1 and only performs no better than H = −1 on absolute error. Fig. 8 shows the\nlearning curves of MOHVE with automatic adjusted H (denoted by MOHVE) and ﬁxed H. MOHVE converges to\nthe highest normalized score with respect to other chosen step lengths. The two experiments verify that the automatic\nadjustment mechanism contributes to the performance of MOHVE.\nH=-1 H=0\nH=2\nH=4 H=20\n0.0\n0.1\n0.2\n0.3\n0.4\n< Absolute Error\n(a)\nH=-1 H=0\nH=2\nH=4 H=20\n0.0\n0.2\n0.4\n0.6\n> Rank Correlation\n(b)\nH=-1 H=0\nH=2\nH=4 H=20\n0.00\n0.05\n0.10\n0.15\n0.20\n< Regret@1\n(c)\nFigure 7: OPE results on walker2d-medium-expert with different step length H.\n0.0\n0.5\n1.0\ntime step\n1e6\n0\n50\n100\nnormalized score\nMOHVE\nH=9\nH=2\nH=0\nH=-1\nFigure 8: Learning curves of ofﬂine RL with different step length H. MOHVE with automatic adjusted step length H\nobtains the highest score.\n22\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-06-04",
  "updated": "2022-06-04"
}