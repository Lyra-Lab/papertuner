{
  "id": "http://arxiv.org/abs/2206.02025v1",
  "title": "Between Rate-Distortion Theory & Value Equivalence in Model-Based Reinforcement Learning",
  "authors": [
    "Dilip Arumugam",
    "Benjamin Van Roy"
  ],
  "abstract": "The quintessential model-based reinforcement-learning agent iteratively\nrefines its estimates or prior beliefs about the true underlying model of the\nenvironment. Recent empirical successes in model-based reinforcement learning\nwith function approximation, however, eschew the true model in favor of a\nsurrogate that, while ignoring various facets of the environment, still\nfacilitates effective planning over behaviors. Recently formalized as the value\nequivalence principle, this algorithmic technique is perhaps unavoidable as\nreal-world reinforcement learning demands consideration of a simple,\ncomputationally-bounded agent interacting with an overwhelmingly complex\nenvironment. In this work, we entertain an extreme scenario wherein some\ncombination of immense environment complexity and limited agent capacity\nentirely precludes identifying an exactly value-equivalent model. In light of\nthis, we embrace a notion of approximate value equivalence and introduce an\nalgorithm for incrementally synthesizing simple and useful approximations of\nthe environment from which an agent might still recover near-optimal behavior.\nCrucially, we recognize the information-theoretic nature of this lossy\nenvironment compression problem and use the appropriate tools of\nrate-distortion theory to make mathematically precise how value equivalence can\nlend tractability to otherwise intractable sequential decision-making problems.",
  "text": "arXiv:2206.02025v1  [cs.LG]  4 Jun 2022\nBetween Rate-Distortion Theory & Value Equivalence in\nModel-Based Reinforcement Learning\nDilip Arumugam\nDepartment of Computer Science\nStanford University\ndilip@cs.stanford.edu\nBenjamin Van Roy\nDepartment of Electrical Engineering\nDepartment of Management Science & Engineering\nStanford University\nbvr@stanford.edu\nAbstract\nThe quintessential model-based reinforcement-learning agent iteratively reﬁnes its estimates or prior beliefs\nabout the true underlying model of the environment. Recent empirical successes in model-based reinforce-\nment learning with function approximation, however, eschew the true model in favor of a surrogate that,\nwhile ignoring various facets of the environment, still facilitates effective planning over behaviors. Re-\ncently formalized as the value equivalence principle, this algorithmic technique is perhaps unavoidable\nas real-world reinforcement learning demands consideration of a simple, computationally-bounded agent\ninteracting with an overwhelmingly complex environment. In this work, we entertain an extreme scenario\nwherein some combination of immense environment complexity and limited agent capacity entirely pre-\ncludes identifying an exactly value-equivalent model. In light of this, we embrace a notion of approximate\nvalue equivalence and introduce an algorithm for incrementally synthesizing simple and useful approxi-\nmations of the environment from which an agent might still recover near-optimal behavior. Crucially, we\nrecognize the information-theoretic nature of this lossy environment compression problem and use the ap-\npropriate tools of rate-distortion theory to make mathematically precise how value equivalence can lend\ntractability to otherwise intractable sequential decision-making problems.\nKeywords:\nBayesian reinforcement learning, Information theory, Model-\nbased reinforcement learning, Efﬁcient exploration\nAcknowledgements\nThe authors gratefully acknowledge Christopher Grimm for initial discussions that provided an impetus\nfor this work. Financial support from Army Research Ofﬁce (ARO) grant W911NF2010055 is gratefully\nacknowledged.\n1\nProblem Formulation\nWe formulate a sequential decision-making problem as an episodic, ﬁnite-horizon Markov Decision Process\n(MDP) [4, 14] deﬁned by M = ⟨S, A, R, T , β, H⟩. S denotes a set of states, A is a set of actions, R : S × A →\n[0, 1] is a deterministic reward function providing evaluative feedback signals (in the unit interval) to the\nagent, T : S × A →∆(S) is a transition function prescribing distributions over next states, β ∈∆(S) is an\ninitial state distribution, and H ∈N is the maximum episode length or horizon.\nLet (Ω, F, P) be a probability space. As is standard in Bayesian reinforcement learning, both the transition\nfunction and reward function are not known to the agent and are consequently treated as random variables.\nWith all other MDP components known a priori, the randomness in the model fully accounts for the ran-\ndomness in the MDP, which is also a random variable. We denote by M⋆the true MDP with model (R⋆, T ⋆)\nthat the agent interacts with and attempts to solve over the course of K episodes. Within each episode, the\nagent acts for exactly H steps beginning with an initial state s1 ∼β. For each h ∈[H], the agent observes\nthe current state sh ∈S, selects action ah ∼πh(· | sh) ∈A, enjoys a reward rh = R(sh, ah) ∈[0, 1], and\ntransitions to the next state sh+1 ∼T (· | sh, ah) ∈S.\nA stationary, stochastic policy for timestep h ∈[H], πh : S →∆(A), encodes a pattern of behavior mapping\nindividual states to distributions over possible actions. Letting {S →∆(A)} denote the class of all station-\nary, stochastic policies, a non-stationary policy π = (π1, . . . , πH) ∈{S →∆(A)}H is a collection of exactly\nH stationary, stochastic policies whose overall performance in any MDP M at timestep h ∈[H] when start-\ning at state s ∈S and taking action a ∈A is assessed by its associated action-value function Qπ\nM,h(s, a) =\nE\n\u0014 H\nP\nh′=h\nR(sh′, ah′)\n\f\f sh = s, ah = a\n\u0015\n, where the expectation integrates over randomness in the action selec-\ntions and transition dynamics. Taking the value function as V π\nM,h(s) = Ea∼πh(·|s)\nh\nQπ\nM,h(s, a)\ni\n, we deﬁne\nthe optimal policy π⋆= (π⋆\n1, π⋆\n2, . . . , π⋆\nH) as achieving supremal value V ⋆\nM,h(s) =\nsup\nπ∈{S→∆(A)}H V π\nM,h(s) for\nall s ∈S, h ∈[H]. We let τk = (s(k)\n1 , a(k)\n1 , r(k)\n1 , . . . , s(k)\nH , a(k)\nH , r(k)\nH , s(k)\nH+1) be a random variable denoting the\ntrajectory experienced by the agent in the kth episode. Meanwhile, Hk = {τ1, τ2, . . . , τk−1} ∈Hk is a ran-\ndom variable representing the entire history of the agent’s interaction within the environment at the start\nof the kth episode. Abstractly, a reinforcement-learning algorithm is a sequence of non-stationary policies\n(π(1), . . . , π(K)) where, for each episode k ∈[K], π(k) : Hk →{S →∆(A)} is a function of the current his-\ntory Hk. We note that no further restrictions on the state-action space S × A, such as ﬁniteness, have been\nmade; notably, through our use of information theory, our algorithm may operate on any ﬁnite-horizon,\nepisodic MDP although we leave the question of how to practically instantiate our algorithm for concrete\nsettings of interest to future work.\n2\nRate-Distortion Theory\nWe here provide a brief, high-level overview of rate-distortion theory [17] and encourage readers to consult\n[7] for more details. A lossy compression problem consumes as input a ﬁxed information source P(X ∈·)\nand a distortion function d : X × Z →R≥0 which quantiﬁes the loss of ﬁdelity by using a compression Z in\nplace of the original X. Then, for any distortion threshold D ∈R≥0, the rate-distortion function quantiﬁes\nthe fundamental limit of lossy compression as\nR(D) = inf\nZ∈Λ I(X; Z) ≜inf\nZ∈Λ E [DKL(P (X ∈· | Z) || P(X ∈·))]\nΛ ≜{Z : Ω→Z | E [d(X, Z)] ≤D},\nwhere I(X; Z) denotes the mutual information and the inﬁmum is taken over all random variables Z that\nincur bounded expected distortion, E [d(X, Z)] ≤D. Naturally, R(D) represents the minimum number of\nbits of information that must be retained from X in order to achieve this bounded expected loss of ﬁdelity.\nIn keeping with the previous problem formulation, which does not assume discrete random variables, we\nnote that the rate-distortion function is well-deﬁned for information source and channel output random\nvariables taking values on abstract alphabets [8]. Moreover, the problem of computing the rate-distortion\nfunction along with the channel that achieves its inﬁmum is well-studied and solved by the classic Blahut-\nArimoto algorithm [6, 1], which is computationally feasible for discrete channel outputs.\n1\nJust as in past work that studies satisﬁcing in multi-armed bandit problems [15, 2, 3], we use rate-distortion\ntheory to formalize and identify a simpliﬁed MDP f\nMk that the agent will attempt to learn over the course\nof each episode k ∈[K]. The episode dependence arises from utilizing the agent’s current beliefs over the\ntrue MDP P(M⋆∈· | Hk) as an information source to be lossily compressed.\n3\nThe Value Equivalence Principle\nAs outlined in the previous section, the second input for a well-speciﬁed lossy-compression problem is\na distortion function prescribing non-negative real values to realizations of the information source and\nchannel output random variables (M⋆, f\nM) that quantify the loss of ﬁdelity incurred by using f\nM in lieu of\nM⋆. To deﬁne this function, we will leverage an approximate notion of value equivalence [10, 11]. For any\narbitrary MDP M with model (R, T ) and any stationary, stochastic policy π : S →∆(A), deﬁne the Bellman\noperator Bπ\nM : {S →R} →{S →R} as follows: Bπ\nMV (s) ≜Ea∼π(·|s)\n\u0002\nR(s, a) + Es′∼T (·|s,a) [V (s′)]\n\u0003\n. The\nBellman operator is a foundational tool in dynamic-programming approaches to reinforcement learning [5]\nand gives rise to the classic Bellman equation: for any MDP M = ⟨S, A, R, T , β, H⟩and any non-stationary\npolicy π = (π1, . . . , πH), the value functions induced by π satisfy V π\nM,h(s) = Bπh\nMV π\nM,h+1(s), for all h ∈[H]\nand with V π\nM,H+1(s) = 0, ∀s ∈S.\nFor any two MDPs M = ⟨S, A, R, T , β, H⟩and c\nM = ⟨S, A, bR, bT , β, H⟩, Grimm et al. [10] deﬁne a notion of\nequivalence between them despite their differing models. For any policy class Π ⊆{S →∆(A)} and value\nfunction class V ⊆{S →R}, M and c\nM are value equivalent with respect to Π and V if and only if Bπ\nMV =\nBπ\nc\nMV , ∀π ∈Π, V ∈V. In words, two different models are deemed value equivalent if they induce identical\nBellman updates under any pair of policy and value function from Π × V. Grimm et al. [10] prove that\nwhen Π = {S →∆(A)} and V = {S →R}, the set of all exactly value-equivalent models is a singleton set\ncontaining only the true model of the environment. The key insight behind value equivalence, however, is\nthat practical model-based reinforcement-learning algorithms need not be concerned with modeling every\ngranular detail of the underlying environment and may, in fact, stand to beneﬁt by optimizing an alternative\ncriterion besides the traditional maximum-likelihood objective [18, 12, 16]. Indeed, by restricting focus to\ndecreasing subsets of policies Π ⊂{S →∆(A)} and value functions V ⊂{S →R}, the space of exactly\nvalue-equivalent models is monotonically increasing.\nFor brevity, let R ≜{S × A →[0, 1]} and T ≜{S × A →∆(S)} denote the classes of all reward functions\nand transition functions, respectively. Recall that, with all uncertainty in M⋆entirely driven by its model,\nwe may think of the support of M⋆as M ≜R × T. We deﬁne a distortion function on pairs of MDPs\nd : M × M →R≥0 for any Π ⊆{S →∆(A)}, V ⊆{S →R} as\ndΠ,V(M, c\nM) = sup\nπ∈Π\nV ∈V\n||Bπ\nMV −Bπ\nc\nMV ||2\n∞= sup\nπ∈Π\nV ∈V\n\u0012\nmax\ns∈S |Bπ\nMV (s) −Bπ\nc\nMV (s)|\n\u00132\n.\nIn words, dΠ,V is the supremal squared Bellman error between MDPs M and c\nM across all states s ∈S with\nrespect to the policy class Π and value function class V.\n4\nValue-Equivalent Sampling for Reinforcement Learning\nBy virtue of the previous two sections, we are now in a position to deﬁne the lossy compression problem\nthat characterizes a MDP f\nMk that the agent will endeavor to learn in each episode k ∈[K] instead of the\ntrue MDP M⋆. For any Π ⊆{S →∆(A)}; V ⊆{S →R}; k ∈[K]; and D ≥0, we deﬁne the rate-distortion\nfunction\nRΠ,V\nk\n(D) = inf\nf\nM∈Λ\nIk(M⋆; f\nM) ≜inf\nf\nM∈Λ\nE\nh\nDKL(P(M⋆∈· | f\nM, Hk) || P(M⋆∈· | Hk)) | Hk\ni\n,\n(1)\nwhere Λ ≜\n\b f\nM : Ω→M | E[dΠ,V(M⋆, f\nM) | Hk] ≤D\n\t\n. This rate-distortion function characterizes the\nfundamental limit of lossy MDP compression under our chosen distortion measure resulting in a channel\n2\nthat retains the minimum amount of information from the true MDP M⋆while yielding an approximately\nvalue-equivalent MDP in expectation. Observe that this distortion constraint is a notion of approximate\nvalue equivalence which collapses to the exact value equivalence of Grimm et al. [10] as D →0. Meanwhile,\nas D →∞, we accommodate a more aggressive compression of the true MDP M⋆resulting in less faithful\nBellman updates.\nAlgorithm 1 Posterior Sampling for Rein-\nforcement Learning (PSRL) [19]\nInput: Prior P(M⋆∈· | H1)\nfor k ∈[K] do\nSample Mk ∼P(M⋆∈· | Hk)\nGet optimal policy π(k) = π⋆\nMk\nExecute π(k) and get trajectory τk\nUpdate history Hk+1 = Hk ∪τk\nInduce posterior P(M⋆∈· | Hk+1)\nend for\nAlgorithm 2 Value-equivalent Sampling for Reinforcement\nLearning (VSRL)\nInput: Prior distribution P(M⋆∈· | H1), Distortion thresh-\nold D ∈R≥0, Distortion function dΠ,V : M × M →R≥0\nfor k ∈[K] do\nCompute channel P( f\nMk ∈· | M⋆) achieving RΠ,V\nk\n(D)\nlimit (Equation 1)\nSample MDP M ⋆∼P(M⋆∈· | Hk)\nSample compressed MDP Mk ∼P( f\nMk ∈· | M⋆= M ⋆)\nCompute optimal policy π(k) = π⋆\nMk\nExecute π(k) and observe trajectory τk\nUpdate history Hk+1 = Hk ∪τk\nInduce posterior P(M⋆∈· | Hk+1)\nend for\nA standard algorithm for our problem setting is widely known as Posterior Sampling for Reinforcement\nLearning (PSRL) [19, 13], which we present as Algorithm 1, while our Value-equivalent Sampling for Re-\ninforcement Learning (VSRL) is given as Algorithm 2. The key distinction between them is that, at each\nepisode k ∈[K], the latter takes the posterior sample M ⋆∼P(M⋆∈· | Hk) and passes it through the chan-\nnel that achieves the rate-distortion limit (Equation 1) at this episode to get the Mk whose optimal policy is\nexecuted in the environment.\n5\nDiscussion\nExample 1 (A Multi-Resolution MDP). For a large but ﬁnite N ∈N, consider a sequence of MDPs, {Mn}n∈[N],\nwhich all share a common action space A but vary in state space (Sn), reward function, and transition function.\nMoreover, for each n ∈[N], the rewards of the nth MDP are bounded in the interval [0, 1\nn]. An agent is confronted\nwith the resulting product MDP, M, deﬁned on the state space S1×. . .×SN with action space A and rewards summed\nacross the N constituent reward functions. The transition function is deﬁned such that each action a ∈A is executed\nacross all N MDPs simultaneously and the resulting individual transitions are composed to make a transition of M.\nFor any value of N, PSRL will persistently act to identify the transition and reward structure of all {Mn}n∈[N].\nExample 1 presents a scenario where, as N ↑∞, a complex environment retains a wealth of information,\nand yet, only a subset of that information may be within the agent’s reach or even necessary for producing\nreasonably competent behavior. VSRL implicitly identiﬁes a M ≪N such that learning the subsequence of\nMDPs {Mn}n∈[M] is sufﬁcient for achieving a desired degree of sub-optimality.\nThe core impetus for this work is to recognize that, for complex environments, pursuit of the exact MDP\nM⋆may be an entirely infeasible goal. Consider a MDP that represents control of a real-world, physical\nsystem; learning a transition function of the associated environment, at some level, demands that the agent\ninternalize laws of physics and motion to a reasonable degree of accuracy. More formally, take the random\nvariable M1 ∼P(M⋆∈· | H1) reﬂecting the agent’s prior beliefs over M⋆. Denoting H(·) as the entropy\nof a random variable, observe that identifying M⋆requires that a PSRL agent obtain exactly H(M1) bits of\ninformation from the environment which, under an uninformative prior, may either be prohibitively large\nand exceed the agent’s capacity constraints or simply be impractical under time and resource constraints.\n6\nConclusion\nIn this work, we embrace the idea of satisﬁcing [15, 2, 3]; as succinctly stated by Herbert A. Simon during\nhis 1978 Nobel Memorial Lecture, “decision makers can satisﬁce either by ﬁnding optimum solutions for\n3\na simpliﬁed world, or by ﬁnding satisfactory solutions for a more realistic world.” Rather than spend an\ninordinate amount of time trying to recover an optimum solution to the true environment, VSRL pursues\noptimum solutions for a sequence of simpliﬁed environments. Future work will develop a complementary\nregret analysis that demonstrates how ﬁnding such optimum solutions for simpliﬁed worlds ultimately\nacts as a mechanism for achieving a satisfactory solution for the realistic, complex world. Naturally, the\nloss of ﬁdelity between the simpliﬁed and true environments translates into a ﬁxed amount of regret that\nan agent designer consciously and willingly accepts for two reasons: (1) they expect a reduction in the\namount of time, data, and bits of information needed to identify the simpliﬁed environment and (2) in\ntasks where the environment encodes irrelevant information and exact knowledge isn’t needed to achieve\noptimal behavior [9, 10, 11], a VSRL agent may still identify the optimal policy while maintaining greater\nsample efﬁciency than traditional PSRL.\nReferences\n[1] Suguru Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless channels.\nIEEE Transactions on Information Theory, 18(1):14–20, 1972.\n[2] Dilip Arumugam and Benjamin Van Roy. Deciding what to learn: A rate-distortion approach. In\nInternational Conference on Machine Learning, pages 373–382. PMLR, 2021.\n[3] Dilip Arumugam and Benjamin Van Roy. The value of information when deciding what to learn.\nAdvances in Neural Information Processing Systems, 34, 2021.\n[4] Richard Bellman. A Markovian decision process. Journal of Mathematics and Mechanics, pages 679–684,\n1957.\n[5] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 1995.\n[6] Richard Blahut. Computation of channel capacity and rate-distortion functions. IEEE Transactions on\nInformation Theory, 18(4):460–473, 1972.\n[7] Thomas M Cover and Joy A Thomas. Elements of Information Theory. John Wiley & Sons, 2012.\n[8] Imre Csisz´ar. On an extremum problem of information theory. Studia Scientiarum Mathematicarum\nHungarica, 9, 1974.\n[9] Amir-massoud Farahmand, Andre Barreto, and Daniel Nikovski. Value-aware loss function for model-\nbased reinforcement learning. In Artiﬁcial Intelligence and Statistics, pages 1486–1494. PMLR, 2017.\n[10] Christopher Grimm, Andre Barreto, Satinder Singh, and David Silver. The value equivalence principle\nfor model-based reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020.\n[11] Christopher Grimm, Andre Barreto, Gregory Farquhar, David Silver, and Satinder Singh. Proper value\nequivalence. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\n[12] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems, pages 6120–6130, 2017.\n[13] Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement\nlearning? In International Conference on Machine Learning, pages 2701–2710. PMLR, 2017.\n[14] Martin L. Puterman. Markov Decision Processes—Discrete Stochastic Dynamic Programming. John Wiley\n& Sons, Inc., New York, NY, 1994.\n[15] Daniel Russo and Benjamin Van Roy. Satisﬁcing in time-sensitive bandit learning. Mathematics of\nOperations Research, 2022.\n[16] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, et al. Mastering Atari,\nGo, Chess and Shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.\n[17] Claude E. Shannon. Coding theorems for a discrete source with a ﬁdelity criterion. IRE Nat. Conv. Rec.,\nMarch 1959, 4:142–163, 1959.\n[18] David Silver, Hado Hasselt, Matteo Hessel, et al. The Predictron: End-to-end learning and planning.\nIn International Conference on Machine Learning, pages 3191–3199. PMLR, 2017.\n[19] Malcolm JA Strens. A Bayesian framework for reinforcement learning. In Proceedings of the Seventeenth\nInternational Conference on Machine Learning, pages 943–950, 2000.\n4\n",
  "categories": [
    "cs.LG",
    "cs.IT",
    "math.IT"
  ],
  "published": "2022-06-04",
  "updated": "2022-06-04"
}