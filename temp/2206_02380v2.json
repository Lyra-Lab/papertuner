{
  "id": "http://arxiv.org/abs/2206.02380v2",
  "title": "Adaptive Rollout Length for Model-Based RL Using Model-Free Deep RL",
  "authors": [
    "Abhinav Bhatia",
    "Philip S. Thomas",
    "Shlomo Zilberstein"
  ],
  "abstract": "Model-based reinforcement learning promises to learn an optimal policy from\nfewer interactions with the environment compared to model-free reinforcement\nlearning by learning an intermediate model of the environment in order to\npredict future interactions. When predicting a sequence of interactions, the\nrollout length, which limits the prediction horizon, is a critical\nhyperparameter as accuracy of the predictions diminishes in the regions that\nare further away from real experience. As a result, with a longer rollout\nlength, an overall worse policy is learned in the long run. Thus, the\nhyperparameter provides a trade-off between quality and efficiency. In this\nwork, we frame the problem of tuning the rollout length as a meta-level\nsequential decision-making problem that optimizes the final policy learned by\nmodel-based reinforcement learning given a fixed budget of environment\ninteractions by adapting the hyperparameter dynamically based on feedback from\nthe learning process, such as accuracy of the model and the remaining budget of\ninteractions. We use model-free deep reinforcement learning to solve the\nmeta-level decision problem and demonstrate that our approach outperforms\ncommon heuristic baselines on two well-known reinforcement learning\nenvironments.",
  "text": "Adaptive Rollout Length for Model-Based RL Using\nModel-Free Deep RL\nAbhinav Bhatia\nPhilip S. Thomas\nShlomo Zilberstein\nCollege of Information and Computer Sciences\nUniversity of Massachusetts Amherst, MA 01003\n{abhinavbhati,pthomas,shlomo}@cs.umass.edu\nAbstract\nModel-based reinforcement learning promises to learn an optimal policy from\nfewer interactions with the environment compared to model-free reinforcement\nlearning by learning an intermediate model of the environment in order to predict\nfuture interactions. When predicting a sequence of interactions, the rollout length,\nwhich limits the prediction horizon, is a critical hyperparameter as accuracy of the\npredictions diminishes in the regions that are further away from real experience. As\na result, with a longer rollout length, an overall worse policy is learned in the long\nrun. Thus, the hyperparameter provides a trade-off between quality and efﬁciency.\nIn this work, we frame the problem of tuning the rollout length as a meta-level\nsequential decision-making problem that optimizes the ﬁnal policy learned by\nmodel-based reinforcement learning given a ﬁxed budget of environment interac-\ntions by adapting the hyperparameter dynamically based on feedback from the\nlearning process, such as accuracy of the model and the remaining budget of inter-\nactions. We use model-free deep reinforcement learning to solve the meta-level\ndecision problem and demonstrate that our approach outperforms common heuristic\nbaselines on two well-known reinforcement learning environments.\n1\nIntroduction\nReinforcement learning algorithms fall into two broad categories—model-based and model-free—\ndepending on whether or not they construct an intermediate model. Model-free reinforcement\nlearning methods based on value-function approximation have been successfully applied to a wide\narray of domains such as playing video games from raw pixels (Mnih et al., 2015) and motor control\ntasks (Lillicrap et al., 2015; Haarnoja et al., 2018). However, model-free methods require a large\nnumber of interactions with the environment to compensate for the unknown dynamics and limited\ngeneralizability of the value-function. In contrast, model-based approaches promise better efﬁciency\nby constructing a model of the environment from the interactions, and using it to guess the outcomes\nof future interactions. Therefore, model-based methods are better suited for learning in the real world,\nwhere environment-interactions are expensive and often limited.\nHowever, using the model is not straightforward and requires considering multiple factors, such as\nhow the model is learned, how accurate it is, how well it generalizes, and whether it is optimistic or\npessimistic. When the model is queried to predict a future trajectory, the look-ahead horizon, or the\nrollout length, limits how much the model is used by limiting how further away from real experience\nthe model operates. Intuitively, a longer horizon permits greater efﬁciency by making greater use of\nthe model. However, when the model is asked to predict a long trajectory, the predictions get less\naccurate at each step as the errors compound, leading to worse performance in the long run. This\nsuggests that the rollout length should be based at least on the accuracy of the model and the budget\nof available interactions with the environment.\nPreprint. Under review.\narXiv:2206.02380v2  [cs.LG]  7 Jun 2022\nGiven the importance of this hyperparameter, it is surprising that there is little prior work that adjusts\nthe rollout length dynamically in a manner that is aware of certain important evolving features of the\nlearning process. In this work, we frame the problem of adjusting the rollout length as a meta-level\nclosed-loop sequential decision making problem—a form of metareasoning aimed at achieving\nbounded rationality (Russell and Wefald, 1991; Zilberstein, 2011). Our meta-level objective is to\narrive at a rollout length adjustment strategy that optimizes the ﬁnal policy learned by the model-based\nreinforcement learning agent given a bounded budget of environment interactions. We solve the\nmeta-level control problem using model-free deep reinforcement learning, an approach that has been\nproposed for dynamic hyperparameter tuning in general (Biedenkapp et al., 2020) and has been shown\nto be effective for solving decision-theoretic meta-level control problems in anytime planning (Bhatia\net al., 2022). In our case, we train a model-free deep reinforcement learning metareasoner on many\ninstances of model-based reinforcement learning applied to simpliﬁed simulations of the target\nreal-world environment, so that the trained metareasoner can be transferred to control the rollout\nlength for model-based reinforcement learning in the real world.\nWe experiment with DQN (Mnih et al., 2015) model-free reinforcement learning algorithm as a\nmetareasoner for adjusting the rollout length for DYNA-DQN (Holland et al., 2018) model-based\nreinforcement learning algorithm on classic control environments MOUNTAINCAR (Moore, 1990)\nand ACROBOT (Sutton, 1996). We test the trained metareasoner on environments constructed by\nperturbing the parameters of the original environments in order to capture the gap between the real\nworld and the simulation. The results show that our approach outperforms common approaches that\nadjust the rollout length using heuristic schedules.\nThe paper is organized as follows. First, we cover the background material relevant to our work in\nsection 2. Next, we motivate the importance of adjusting the rollout length in a principled manner in\nsection 3. We propose our metareasoning approach in section 4. We mention related work in section\n5. We present our experiments, results and discussion in sections 6-7. Finally, we conclude the paper\nin section 8.\n2\nBackground\n2.1\nMDP Optimization Problem\nA Markov Decision Process (Puterman, 1994), or an MDP, is repsented by a tuple <\nS, A, p, r, d0, γ >, where S is the state space, A is the set of available actions, p : S ×A×S →[0, 1]\nis the transition function representing the probability p(s, a, s′) of transitioning from state s to state\ns′ by taking action a. The reward function r : S × A →R represents the expected reward r(s, a)\nassociated with action a in state s. d0 : S →[0, 1] represents the probability d0(s) that the MDP\nprocess starts with state s. γ ∈[0, 1] denotes the discount factor.\nAn agent may act in the MDP environment according to a Markovian policy π : S × A →[0, 1],\nwhich represents the probability π(s, a) of taking action a in a state s. The objective is to ﬁnd an\noptimal policy π∗that maximizes the expected return J(π):\nJ(π) := E[\nT −1\nX\nt=0\nγtRt|π]\n(1)\nπ∗:= argmax\nπ∈Π\nJ(π)\n(2)\nwhere random variable Rt denotes the reward obtained at timestep t and random variable T denotes\nthe number of steps until termination of the process or an episode.\nAn action-value function qπ(s, a) is deﬁned as the expected return obtained by taking action a at\nstate s and thereafter executing policy π:\nqπ(s, a) := E[\nT −1\nX\nk=t\nγkRk|St = s, At = a, π]\n(3)\n2\nThis can be expressed recursively as the Bellman equation for the action-value function:\nqπ(s, a) = r(s, a) + γ\nX\ns′\nX\na′\np(s, a, s′)π(s′, a′)qπ(s′, a′)\n(4)\nThe action-value function for an optimal policy satisﬁes the Bellman optimality equation:\nq∗(s, a) := r(s, a) + γ\nX\ns′\np(s, a, s′) max\na′ q∗(s′, a′)\n(5)\nFor every MDP, there exists an optimal deterministic policy (Bertsekas, 2005; Mausam and Kolobov,\n2012), often denoted simply as the mapping π∗: S →A, which can be recovered from the optimal\naction-value function by taking a greedy action at every state i.e., π∗(s) ∈argmaxaq∗(s, a).\n2.2\nModel-Free Reinforcement Learning\nIn reinforcement learning (RL) setting, either the transition model or the reward model or both are\nunknown, and the agent must derive an optimal policy by interacting with the environment and\nobserving feedback (Sutton and Barto, 2018).\nMost model-free approaches to reinforcement learning maintain an estimate of the value function.\nWhen an agent takes an action a at state s and observes a reward r and transitions to state s′, the\nestimated q-value of a policy π, denoted as ˆqπ, is updated using a temporal-difference update rule:\nˆqπ(s, a) ←ˆqπ(s, a) + α(r + γEa′∼π(s′,·)[ˆqπ(s′, a′)] −ˆqπ(s, a))\n(6)\nWhere α ∈[0, 1] is the step size. Repeated application of this rule in arbitrary order despite\nbootstrapping from random estimates causes convergence to the true action-value function qπ, as\nlong as the policy used to collect the experience tuples (s, a, r, s′) explores every state-action pair\nsufﬁciently often. Maintaining an estimate of the value function helps the agent improve its policy\nbetween value function updates by assigning greater probability mass to actions with higher q-values.\nWith greedy improvements π(s) ←argmaxaˆqπ(s, a), the process converges to the optimal action-\nvalue function q∗, an approach known as Q-learning (Watkins and Dayan, 1992). A common choice\nfor the exploration policy is an ϵ-greedy policy – which selects an action randomly with probability ϵ\nwhen not selecting a greedy action.\nWhen the state space is continuous, the value function may be parameterized using a function\napproximator with parameters θ to allow generalization to unexplored states. The popular algorithm\nDeep-Q-Network, or DQN, uses deep learning to approximate the action-value function (Mnih et al.,\n2015). A gradient update of DQN minimizes the following loss.\nL(θ) =\nE\n(s,a,r,s′)∼D[(r + γ max\na′ qθ′(s′, a′) −qθ(s, a))2]\n(7)\nWhere the parameters θ′ are assigned θ′ ←θ over spaced-out intervals to stabilize the loss function. A\nminibatch of experience tuples (s, a, r, s′) is sampled from an experience memory buffer D populated\nby acting in the environment by following an exploration policy. Reusing, or replaying, recorded\nexperience boosts sample efﬁciency by eliminating the need to revisit those transitions (Lin, 1992).\nSince Q-learning does not require learning an intermediate model to learn an optimal policy, it belongs\nto the paradigm of model-free reinforcement learning.\n2.3\nModel-Based Reinforcement Learning\nIn model-based reinforcement learning, or MBRL, the agent learns an intermediate model from the\ndata collected while interacting with the environment, and uses the model to derive an optimal plan.\nIn this work, we focus on a class of methods in which the agent learns a generative forward-dynamics\nmodel (i.e., the transition and the reward function), and uses it to synthesize novel experience,\nwhich boosts sample efﬁciency as it augments the experience used to learn the value function. The\n3\nmodel can be advantageous for additional reasons, such as enabling better exploration (Thrun, 1992),\ntransfer (Taylor and Stone, 2009), safety (Berkenkamp et al., 2017) and explanability (Moerland\net al., 2018). Despite the advantages, model-based methods often converge to suboptimal policies\ndue to the model’s biases and inaccuracies.\nDYNAQ (Sutton, 1991, 1990) was one of the earliest approaches that integrated acting, learning\nand planning in a single loop in a tabular RL setting. In DYNAQ, when the agent experiences a\nnew transition, i) it is used to perform a standard Q-learning update, ii) it is used to update a tabular\nmaximum likelihood model, iii) the model is used to generate a single experience by taking an action\nsuggested by the current policy on a randomly selected state that has been previously visited in the\nenvironment, and iv) the generated experience is used to perform a Q-learning update.\nRecent state-of-the-art approaches in model-based deep reinforcement learning have extended this\narchitecture to perform multi-step rollouts where each transition in the synthesized trajectory is\ntreated as an experience for model-free reinforcement learning (Holland et al., 2018; Janner et al.,\n2019; Kaiser et al., 2020).\nMoerland et al. (2020) provide a comprehensive survey of model-based reinforcement learning.\n3\nMotivation\nIn scenarios where the environment interactions are costly and limited, such as in the real world,\nmodel-based RL promises to be more suitable than model-free RL as it can learn a better policy from\nfewer interactions. However, the quality of the policy learned after a given number of environment\ninteractions depends on many algorithm design decisions. Choosing the rollout length is a critical\nand a difﬁcult decision for the following reasons.\nFirst, we note that for model-based RL to be more efﬁcient than model-free RL, rollouts to unfa-\nmiliar states must be accurate enough such that the subsequently derived value estimates are more\naccurate than the estimates obtained by replaying familiar experience and relying on value-function\ngeneralization alone. However, the model itself loses accuracy in regions further away from familiar\nstates and moreover, prediction errors begin compounding at each step along a rollout. As a result,\nshorter rollouts are more accurate but provide little gain in efﬁciency, while longer rollouts improve\nefﬁciency in the short term but ultimately cause convergence to a suboptimal policy (Holland et al.,\n2018).\nThe ideal rollout length depends on many factors, one of which is the evolving accuracy of the learned\nmodel. For example, when the model is signiﬁcantly inaccurate, which is the case in the beginning\nof the training, a short rollout length may be a better choice. The kind of inaccuracy – whether\noptimistic or pessimistic also matters, given that planning using a pessimistic (or inadmissible) model\ndiscourages exploration. Other important factors include the quality of the policy at a given point of\ntime during the training, and the remaining budget of environment interactions. For instance, once\nan agents learns a good policy with enough environment interactions to go, the agent may beniﬁt\nfrom reducing the rollout length or even switching to model-free learning entirely in order to reﬁne\nthe policy using real data alone. Finally, the rollout length itself affects the policy, which affects the\nmodel’s training data, and therefore affects the model.\nAs a result, choosing the rollout length is a complex, closed-loop, sequential decision-making problem.\nIn other words, it requires an approach that searches for an ideal sequence of adjustments considering\ntheir long term consequences and feedback from the training process.\n4\nAdjusting Rollout Length Using Metareasoning\nIn this section, we describe our metareasoning approach to adjust the rollout length over the course\nof the training in MBRL, in order to maximize the quality of the ﬁnal policy learned by the agent at\nthe end of a ﬁxed budget of environment interactions.\nOur MBRL architecture is outlined in algorithm 1, which is similar to the off-policy Dyna-style\narchitecture presented by Holland et al. (2018), Janner et al. (2019) and Kaiser et al. (2020). The\nagent is trained for N environment interaction steps, which may consist of multiple episodes. As\nthe agent interacts with the environment, the transition tuples (s, a, r, s′) are added to an experience\n4\nAlgorithm 1 Dyna Style Model-Based Reinforcement Learning\n1: Initialize policy πθ, predictive model Mφ, experience buffer D, experience buffer D′\n2: for t = 1...N do\n3:\nAct in environment using πθ; add experience to D\n4:\nTrain πθ from D for G gradient updates\n5:\nif i mod P = 0 then\n6:\nTrain model Mφ from D using supervised learning\n7:\nAdjust rollout length K using metareasoning\n8:\nEmpty D′\n9:\nfor M rollouts do\n10:\nSample state s uniformly from D\n11:\nPerform K steps of model rollout from s using policy πθ; add experiences to D′\n12:\nTrain πθ from D′ for G’ gradient updates\ndatabase D. The agent continuously updates the value function and improves the policy by replaying\nexperience from the database D in a model-free fashion outlined in section 2.2. The model is updated\nand used only every P steps i.e., the entire MBRL training is divided into N/P phases. At the end of\nevery phase i.e., every P steps, the model is supervised-trained using the entire data collected so far.\nThe rollout length K is adjusted and the model is used for performing M rollouts, each rooted at a\ndifferent uniformly sampled experienced state. The rollouts are performed using the current policy\nand may include exploratory actions. The synthetic data thus collected is recorded in an experience\ndatabase D′ which is used to update the value function and improve the policy in a model-free fashion\noutlined in section 2.2.\nWe frame the task of optimizing the quality of the ﬁnal policy learned by MBRL as a meta-level\nsequential decision-making process, speciﬁcally, as an MDP with the following characteristics.\nTask Horizon: An entire duration of MBRL training corresponds to one meta-level episode consisting\nof N/P steps. The metareasoner adjusts the rollout length every P steps during the training (line 7 in\nalgorithm 1).\nAction Space: The meta-level action space consists of three actions:\n• UP:\nK ←ceil(1.5K) if K > 0; otherwise 1.\n• DOWN: K ←ﬂoor(K/2) if K > 1; otherwise 0.\n• NOOP: No change\nDespite the simplicity of this action space, it allows rapid changes to the rollout length due to\nexponential effects of the actions. The DOWN action is more aggressive than the UP action to allow\nthe meta-level policy to move towards a conservative rollout length rapidly. Another beneﬁt is that a\nrandom walk with this action space makes the rollout length hover close to zero. Consequently, a\nmeta-level policy that deliberately chooses higher rollout lengths, and performs well, would clearly\ndemonstrate a need to use the model.\nState Space: The meta-level state features are:\n• Time: Remaining environment interactions N −t.\n• Current rollout length K.\n• Quality: The average return (J-value) under the current policy.\n• Model-error in predicting episode-returns: The average difference between the return\npredicted by the model under the current policy and the return observed in the environment\nunder the same policy, when starting from the same initial state. Since this is not an absolute\ndifference, the sign of the error reﬂects whether the model is optimistic or pessimistic.\n• Model-error in predicting episode-lengths: The average difference between the episode\nlength predicted by the model under the current policy and the episode length observed in\nthe environment under the same policy, when starting from the same initial state.\n5\nThe bottom three features are computed by taking average over the episodes that take place during\nthe latest phase.\nReward Function: The reward for the meta-level policy is the change in the average return (J′ −J)\nof the current MBRL policy since the latest action. There is no discounting.\nWith this reward structure, the cumulative return becomes J1+J2−J1+...+JN/P −JN/P −1 = JN/P .\nIn other words, this reward structure incentivizes the meta-level agent to maximize the quality of the\nﬁnal policy learned by the MBRL agent.\nTransition Function: The effects of actions on the state features of the meta-MDP are not available\nexplicitly.\nAs the meta-level transition model is explicitly unknown, solving the meta-MDP requires a model-free\napproach, such as model-free reinforcement learning. In our approach, the meta-level RL agent,\nor the metareasoner, is trained using model-free deep reinforcement learning on instances of an\nMBRL agent solving a simpliﬁed simulation of the target real world environment, so that the trained\nmetareasoner can be transferred to the real world to achieve our objective of helping the MBRL agent\nlearn in real-world environments given a ﬁxed budget of environment interactions.\n5\nRelated work\nWhile there has been substantial work on developing model-learning and model-usage techniques for\nreliably predicting long trajectories (Moerland et al., 2020), little work has gone into adapting the\nrollout length in a principled manner.\nHolland et al. (2018) study the effect of planning shape (number of rollouts and rollout length) on\nDyna-style MBRL and observe that one-step Dyna offers little beneﬁt over model-free methods in\nhigh dimensional domains, concluding that the rollouts must be longer for the model to generate\nunfamiliar experience.\nNguyen et al. (2018) argue that transitions synthesized from the learned model are more useful in the\nbeginning of the training when the model-free value estimates are only beginning to converge, and\nless useful once real data is abundant enough that it provides more accurate training targets for the\nagent. They suggest that an ideal rollout strategy would roll out more steps in the beginning, and less\nat the end. However, their efforts to adaptively truncate rollouts based on estimates of the model’s\nand the value function’s uncertainty have met little success.\nJanner et al. (2019) derive bounds on policy improvement using the model, based on the choice of the\nrollout length and the model’s ability to generalize beyond its training distribution. Their analysis\nsuggests that it is safe to increase the rollout length linearly, as the model becomes more accurate over\nthe course of the training, to obtain maximal sample efﬁciency while still guaranteeing monotonic\nimprovement. While this approach is effective, it does not consider long term effects of the modifying\nthe rollout length.\nTo our knowledge, ours is the ﬁrst approach that performs a sequence of adjustments to the rollout\nlength based on the model’s empirical error, the performance of the agent, and the remaining budget\nof environment interactions, to optimize the quality of the ﬁnal policy in a decision theoretic manner.\nOur formulation of the problem as a meta-level MDP and use of deep reinforcement learning to solve\nit is inspired from prior literature (Biedenkapp et al., 2020; Bhatia et al., 2022).\n6\nExperiments\nWe experiment with DQN (Mnih et al., 2015) model-free RL algorithm as a metareasoner for adjusting\nthe rollout length in DYNA-DQN (Holland et al., 2018) model-based RL algorithm on popular RL\nenvironments MOUNTAINCAR (Moore, 1990; Singh and Sutton, 1996) and ACROBOT (Sutton, 1996;\nGeramifard et al., 2015).\nFor each environment, the DQN metareasoner is trained for 2000 meta-level episodes – each corre-\nsponding to one training run of DYNA-DQN consisting 150k steps and 120k steps on MOUNTAINCAR\n6\nEnvironment\nK = 0\nK = 16\nK = 32\nDEC\nINC\nINC-DEC\nMETA\nMOUNTAINCAR\n−188.9\n−166.0\n−170.0\n−166.7\n−168.0\n−165.8\n−160.0\n(±0.7)\n(±1.4)\n(±1.4)\n(±1.3)\n(±1.4)\n(±1.4)\n(±1.3)\nACROBOT\n−151.34\n−142.8\n−148.1\n−148.0\n−144.6\n−140.5\n−130.7\n(±3.4)\n(±4.0)\n(±3.9)\n(±3.9)\n(±4.1)\n(±3.7)\n(±3.5)\nTable 1: Mean score (± standard error) of each approach across 100 training runs of DYNA-DQN on\nthe modiﬁed (i.e., test) MOUNTAINCAR and ACROBOT environments.\nand ACROBOT environments respectively. The rollout length is capped at 32, and adjusted every 10k\nsteps, so that the meta-level task horizon is 15 steps and 12 steps respectively. The metareasoner’s\nscore for each training run is calculated by evaluating the ﬁnal policy of the DYNA-DQN agent for\nthat run, without exploration, averaged over 100 episodes of the environment. The overall score\nof the trained metareasoner is calculated by taking its average score over 100 training runs on test\nenvironments, which are constructed by perturbing the parameters of the original environments\nin order to capture the gap between the real world and the simulation. This is done to test the\nmetareasoner’s ability to transfer to the real world, which is our ultimate objective.\nFurther details and hyperparameters of DQN, DYNA-DQN and the modiﬁed RL environments are in\nthe appendix.\nWe compare our approach META to various rollout length schedules that have been suggested in\nprior literature: i) K = 0 (i.e., Model-free DQN) ii) static rollout lengths K = 16 and K = 32,\niii) DEC: linearly decrease K = 32 →0 over the course of the training, iv) INC: linearly increase\nK = 0 →32, and v) INC-DEC: linearly increase then decrease K = 0 →32 →0 over the two\nhalves of the training.\n7\nResults and Discussion\nTable 1 shows the mean score (and standard error) of each approach across the 100 training runs of\nDYNA-DQN on the modiﬁed (i.e., test) RL environments. On both environments, the metareasoning\napproach demonstrates better mean score than that of the baseline approaches. Figures 1(a) and\n1(c) show the mean learning curves of DYNA-DQN for selected rollout adjustment approaches. The\nlearning curves demonstrate that the metareasoning approach to rollout adjustment leads to more\nstable learning curves on average than the baseline approaches.\nThe results on MOUNTAINCAR show that it is a difﬁcult environment for model-free reinforcement\nlearning to solve within a limited budget of environment interactions. On this environment, all\nmodel-based reinforcement learning approaches achieve signiﬁcantly higher scores. Among the\nmodel-based approaches, while all heuristic rollout schedules demonstrate similar performance,\nthe metareasoning approach outperforms every other approach. On ACROBOT, most model based\napproaches lead to minor gains over the model-free baseline at the end of the budget. However, the\nmetareasoning approach achieves signiﬁcantly higher mean score. INC-DEC approach performs the\nbest among the baseline rollout schedules.\nOn both domains, the metareasoning approach leads to better learning curves on average than that of\nthe baselines. On MOUNTAINCAR, it leads to comparatively monotonic improvement on average.\nOn ACROBOT, the learning curve due to the metareasoning approach dominates the other learning\ncurves at all times during the training on average.\nFigure 1(b) shows the rollout length chosen by the trained metareasoner on average across the training\nruns of DYNA-DQN on the modiﬁed MOUNTAINCAR environment at different points during the\ntraining. The rollout adjustment policy appears similar to INC-DEC except the greater variance,\nindicating that the metareasoner’s policy is more nuanced than a simple function of training steps\nas more factors are taken into consideration. Figure 1(d) shows the rollout length chosen by the\ntrained metareasoner on average across the training runs of DYNA-DQN on the modiﬁed ACROBOT\nenvironment at different points during the training. The metareasoner chooses lower values for\nthe most part and ultimately switches to model-free learning. This is not surprising given that the\nmodel-free baseline is competitive with the model-based approaches on this environment.\n7\nFigure 1: (a) Mean learning curve across the training runs of DYNA-DQN on the modiﬁed MOUN-\nTAINCAR environment for selected rollout adjustment approaches; (b) Mean (± standard deviation)\nrollout length schedule learned by the metareasoner on the modiﬁed MOUNTAINCAR environment;\n(c) Mean learning curve of DYNA-DQN on the modiﬁed ACROBOT environment for selected rollout\nadjustment approaches; (d) Mean (± standard deviation) rollout length schedule learned by the\nmetareasoner on the modiﬁed ACROBOT environment.\nFinally, we observe a pattern that the approaches that decrease the rollout length towards the end of\nthe training generally perform better on both environments. This suggests that even though the model\nbecomes more accurate as the training progresses, its net utility diminishes when real experience data\nbecomes more abundant. However, this observation may not generalize beyond our choice of the\nenvironments, the interactions budget and the parameters of the model-based reinforcement learning\nalgorithm.\n8\nConclusion\nIn this work, we motivate the importance of choosing and adjusting the rollout length in a principled\nmanner during training in model-based reinforcement learning given a ﬁxed budget of environment\ninteractions in order to optimize the quality of the ﬁnal policy learned by the agent. We frame the\nproblem as a meta-level closed-loop sequential decision-making problem such that the adjustment\nstrategy incorporates feedback from the learning process, which includes features such as improve-\nment in the model’s accuracy as training progresses. We solve the meta-level decision problem using\nmodel-free deep reinforcement learning and demonstrate that this metareasoning approach leads to\nmore stable learning curves and ultimately a better ﬁnal policy on average as compared to certain\nheuristic approaches.\n9\nAcknowledgments\nThis work was supported by NSF grants IIS-1813490 and IIS-1954782.\nReferences\nBa, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization. stat, 1050:21.\nBerkenkamp, F., Turchetta, M., Schoellig, A. P., and Krause, A. (2017). Safe model-based reinforce-\nment learning with stability guarantees. In Advances in Neural Information Processing Systems,\nvolume 30, pages 908–919.\nBertsekas, D. P. (2005). Dynamic programming and optimal control, 3rd Edition. Athena Scientiﬁc.\nBhatia, A., Svegliato, J., Nashed, S., and Zilberstein, S. (2022). Tuning the hyperparameters of\nanytime planning: A metareasoning approach with deep reinforcement learning. In Proceedings of\nthe International Conference on Automated Planning and Scheduling, volume 32.\nBiedenkapp, A., Bozkurt, H. F., Eimer, T., Hutter, F., and Lindauer, M. (2020). Dynamic algorithm\nconﬁguration: Foundation of a new meta-algorithmic framework. In Proceedings of the European\nConference on Artiﬁcial Intelligence, pages 427–434. IOS Press.\nGeramifard, A., Dann, C., Klein, R. H., Dabney, W., and How, J. P. (2015). RLPy: A value-function-\nbased reinforcement learning framework for education and research. Journal of Machine Learning\nResearch, 16(46):1573–1578.\n8\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum\nentropy deep reinforcement learning with a stochastic actor. In International conference on\nmachine learning, pages 1861–1870. PMLR.\nHolland, G. Z., Talvitie, E., and Bowling, M. (2018). The effect of planning shape on dyna-style\nplanning in high-dimensional state spaces. CoRR, abs/1806.01825.\nJanner, M., Fu, J., Zhang, M., and Levine, S. (2019). When to trust your model: Model-based\npolicy optimization. In Advances in Neural Information Processing Systems, volume 32, pages\n12519–12530.\nKaiser, L., Babaeizadeh, M., Miłos, P., Osi´nski, B., Campbell, R. H., Czechowski, K., Erhan, D.,\nFinn, C., Kozakowski, P., Levine, S., Mohiuddin, A., Sepassi, R., Tucker, G., and Michalewski, H.\n(2020). Model based reinforcement learning for Atari. In International Conference on Learning\nRepresentations.\nKingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In Bengio, Y. and\nLeCun, Y., editors, Proceedings of the Third International Conference on Learning Representations.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.\n(2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.\nLin, L. J. (1992). Self-improving reactive agents based on reinforcement learning, planning and\nteaching. Machine Learning, 8:293–321.\nMausam and Kolobov, A. (2012). Planning with Markov Decision Processes: An AI Perspec-\ntive. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning. Morgan & Claypool\nPublishers.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,\nRiedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep\nreinforcement learning. Nature, 518(7540):529–533.\nMoerland, T. M., Broekens, J., and Jonker, C. M. (2018). Emotion in reinforcement learning agents\nand robots: A survey. Machine Learning, 107(2):443–480.\nMoerland, T. M., Broekens, J., and Jonker, C. M. (2020). Model-based reinforcement learning: A\nsurvey. CoRR, abs/2006.16712.\nMoore, A. W. (1990). Efﬁcient memory-based learning for robot control. Technical report, University\nof Cambridge, Computer Laboratory.\nNguyen, N. M., Singh, A., and Tran, K. (2018). Improving model-based RL with adaptive rollout\nusing uncertainty estimation.\nPuterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.\nWiley Series in Probability and Statistics. Wiley.\nRussell, S. and Wefald, E. (1991). Principles of metareasoning. Artiﬁcial intelligence, 49(1-3):361–\n395.\nSingh, S. P. and Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces.\nMachine learning, 22(1):123–158.\nSutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approxi-\nmating dynamic programming. In Porter, B. W. and Mooney, R. J., editors, Proceedings of the\nSeventh International Conference on Machine Learning, pages 216–224. Morgan Kaufmann.\nSutton, R. S. (1991). Dyna, an integrated architecture for learning, planning, and reacting. SIGART\nBulletin, 2(4):160–163.\nSutton, R. S. (1996). Generalization in reinforcement learning: Successful examples using sparse\ncoarse coding. In Advances in Neural Information Processing Systems, volume 8, pages 1038–1044.\nSutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n9\nTaylor, M. E. and Stone, P. (2009). Transfer learning for reinforcement learning domains: A survey.\nJournal of Machine Learning Research, 10(56):1633–1685.\nThrun, S. B. (1992). Efﬁcient exploration in reinforcement learning. Technical report, Carnegie\nMellon University, USA.\nTian, J. and other contributors (2020). Reinforcementlearning.jl: A reinforcement learning package\nfor the Julia programming language.\nVan Hasselt, H. (2010). Double q-learning. In Advances in Neural Information Processing Systems,\nvolume 23, pages 2613–2621.\nVan Hasselt, H., Guez, A., and Silver, D. (2016). Deep reinforcement learning with double q-learning.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30.\nWatkins, C. J. and Dayan, P. (1992). Q-learning. Machine learning, 8(3-4):279–292.\nZilberstein, S. (2011). Metareasoning and bounded rationality. In Cox, M. and Raja, A., editors,\nMetareasoning: Thinking about Thinking, pages 27–40. MIT Press, Cambridge, MA, USA.\n10\n10\nAppendix\n10.1\nRL Environments\nWe used the default implementations of MOUNTAINCAR and ACROBOT in ReinforcementLearn-\ning.jl (Tian and other contributors, 2020) Julia library. The parameter modiﬁcations for testing the\nmetareasoner are as follows.\nEnvironment\nModiﬁcations\nMOUNTAINCAR\nACROBOT\nGravity\n0.0025 →0.003\nGoal position\n0.5 →-1.1\nGravity\n9.8 →12.0\nLink A length\n1.0 →1.2\nLink A mass\n1.0 →1.2\nLink B length\n1.0 →0.8\nLink B mass\n1.0 →0.8\n10.2\nDYNA-DQN Model-Based Reinforcement Learning\nThis subsection describes the details of the DYNA-DQN algorithm 1.\nModel Learning: Given a state and an action, a deterministic model Mφ predicts the next state,\nreward and the log-probability whether the next state is terminal. The model has a subnetwork\ncorresponding to each subtask, without any shared parameters. The transition (next state) subnetwork\nand the terminal subnetwork both have two hidden layers of size [32,16]. The reward subnetwork\nhas two hidden layers of size [64,32]. ReLU activation function is used in the hidden layers, while\nthe ﬁnal layers are linear. Layer normalization (Ba et al., 2016) is applied before each activation.\nFollowing Janner et al. (2019), the transition subnetwork ﬁrst predicts the difference between the next\nstate and the current state, and then reconstructs the next state using the difference. The transition\nand the reward subnetworks are trained to minimize the mean squared error, while the terminal\nsubnetwork uses binary cross-entropy loss. Adam optimizer (Kingma and Ba, 2015) is used with\nlearning rate 0.001. The minibatch size is 32. The standard practice of splitting the training data into\ntraining and validation sets is followed, with 20% data reserved for validation. The training stops\nwhen the validation loss increases.\nDQN Network: The DQN network qθ consists of two hidden layers of size [64, 32], and uses ReLU\nactivation function. It is trained on real and model-synthesized experience using Adam optimizer\nwith learning rate 0.0001. The minibatch size is 32. The network parameters θ are copied to θ′ every\n2000 gradient updates.\nActing, Learning and Planning Loop: Total environment steps N is 150k and 120k for MOUN-\nTAINCAR and ACROBOT respectively. The experience buffer D is of unlimited capacity. The rollout\nlength is adjusted every P=10k steps. The total number of rollouts are such that the total number\nof rollout steps match the total number of environment steps, i.e., M = P/K. The rollouts are\ntruncated at K steps, or if the model transitions to a terminal state, whichever happens earlier. Both\nthe acting policy and the rollout policy includes ϵ = 0.1 exploration. For the acting policy, ϵ = 1\nfor the initial 10k steps and is linearly annealed to 0.1 over the next 10k steps. For each actual or\nsynthetic experience, G = G′ = 1 gradient update is performed. The double-Q-learning update rule\nis used to reduce maximization bias (Van Hasselt et al., 2016; Van Hasselt, 2010). The discount factor\nγ is 0.99.\n10.3\nDQN Metareasoning\nThe DQN metareasoner is trained on 2000 meta-level episodes with a different seed used for each\ntraining run of DYNA-DQN. ϵ-greedy exploration is used with ϵ = 1 for the ﬁrst 25 episodes and\nlinearly annealed to ϵ = 0.15 over the next 25 episodes. A discount factor of 0.99 is used. The\nnetwork uses two hidden layers of size [64, 32] with ReLU activation units. 10 gradient updates are\nperformed for each meta-level experience using the double-Q-learning update rule, with minibatch\nsize 32, and Adam learning rate 0.0001. The network parameters are copied to the target network\nevery 10 meta-level episodes.\n11\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-06-06",
  "updated": "2022-06-07"
}