{
  "id": "http://arxiv.org/abs/2206.04028v2",
  "title": "CO^3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving",
  "authors": [
    "Runjian Chen",
    "Yao Mu",
    "Runsen Xu",
    "Wenqi Shao",
    "Chenhan Jiang",
    "Hang Xu",
    "Zhenguo Li",
    "Ping Luo"
  ],
  "abstract": "Unsupervised contrastive learning for indoor-scene point clouds has achieved\ngreat successes. However, unsupervised learning point clouds in outdoor scenes\nremains challenging because previous methods need to reconstruct the whole\nscene and capture partial views for the contrastive objective. This is\ninfeasible in outdoor scenes with moving objects, obstacles, and sensors. In\nthis paper, we propose CO^3, namely Cooperative Contrastive Learning and\nContextual Shape Prediction, to learn 3D representation for outdoor-scene point\nclouds in an unsupervised manner. CO^3 has several merits compared to existing\nmethods. (1) It utilizes LiDAR point clouds from vehicle-side and\ninfrastructure-side to build views that differ enough but meanwhile maintain\ncommon semantic information for contrastive learning, which are more\nappropriate than views built by previous methods. (2) Alongside the contrastive\nobjective, shape context prediction is proposed as pre-training goal and brings\nmore task-relevant information for unsupervised 3D point cloud representation\nlearning, which are beneficial when transferring the learned representation to\ndownstream detection tasks. (3) As compared to previous methods, representation\nlearned by CO^3 is able to be transferred to different outdoor scene dataset\ncollected by different type of LiDAR sensors. (4) CO^3 improves current\nstate-of-the-art methods on both Once and KITTI datasets by up to 2.58 mAP.\nCodes and models will be released. We believe CO^3 will facilitate\nunderstanding LiDAR point clouds in outdoor scene.",
  "text": "COˆ3: Cooperative Unsupervised 3D Representation\nLearning for Autonomous Driving\nRunjian Chen\nThe University of Hong Kong\nrjchen@connect.hku.hk\nYao Mu\nThe University of Hong Kong\nmuyao@connect.hku.hk\nRunsen Xu\nZhejiang University\nrunsenxu@zju.edu.cn\nWenqi Shao\nThe Chinese University of Hong Kong\nweqish@link.cuhk.edu.hk\nChenhan Jiang\nHuawei Noah’s Ark Lab\njiang.chenhan@huawei.com\nHang Xu\nHuawei Noah’s Ark Lab\nxu.hang@huawei.com\nZhenguo Li\nHuawei Noah’s Ark Lab\nli.zhenguo@huawei.com\nPing Luo\nThe University of Hong Kong\npluo@cs.hku.hk\nAbstract: Unsupervised contrastive learning for indoor-scene point clouds has\nachieved great successes.\nHowever, unsupervised representation learning on\noutdoor-scene point clouds remains challenging because previous methods need to\nreconstruct the whole scene and capture partial views for the contrastive objective.\nThis is infeasible in outdoor scenes with moving objects, obstacles, and sensors.\nIn this paper, we propose COˆ3, namely Cooperative Contrastive Learning and\nContextual Shape Prediction, to learn 3D representation for outdoor-scene point\nclouds in an unsupervised manner. COˆ3 has several merits compared to existing\nmethods. (1) It utilizes LiDAR point clouds from vehicle-side and infrastructure-\nside to build views that differ enough but meanwhile maintain common semantic\ninformation for contrastive learning, which are more appropriate than views built\nby previous methods. (2) Alongside the contrastive objective, contextual shape\nprediction is proposed as pre-training goal and brings more task-relevant informa-\ntion for unsupervised 3D point cloud representation learning, which are beneﬁcial\nwhen transferring the learned representation to downstream detection tasks. (3)\nAs compared to previous methods, representation learned by COˆ3 is able to be\ntransferred to different outdoor scene dataset collected by different type of LiDAR\nsensors. (4) COˆ3 improves current state-of-the-art methods on both Once and\nKITTI datasets by up to 2.58 mAP. Codes and models will be released. We believe\nCOˆ3 will facilitate understanding LiDAR point clouds in outdoor scene.\n1\nIntroduction\nAs the most reliable sensor in outdoor environments, LiDAR is able to precisely measure 3D location\nof objects and both of the robotics and computer vision communities have shown strong interest on\nperception tasks on LiDAR point clouds, including 3D object detection, segmentation and tracking,\nwhich are essential to autonomous driving system. Up to now, randomly initializing and directly\ntraining from scratch on detailed annotated data still dominates this ﬁeld. On the contrary, recent\nresearch efforts [1, 2, 3, 4, 5] in image domain focus on unsupervised representation learning with\ncontrastive objective on different views built from images (the ﬁrst column of Figure 1 shows\nexample views built by [1, 2, 3, 4, 5]). They pre-train the 2D backbone with a large-scale dataset like\nImageNet [6] in an unsupervised manner and use the pre-trained backbone to initialize downstream\nneural networks on different datasets, which achieve signiﬁcant performance improvement over\ntraining from scratch in 2D object detection [7, 8, 9]. Inspired by these successes together with the\nabundant unlabelled data available from self-driving vehicles, we explore unsupervised representation\nlearning for outdoor scene point clouds to improve the performance on 3D object detection tasks.\narXiv:2206.04028v2  [cs.CV]  26 Jun 2022\nFigure 1: Example views built by different methods in contrastive learning. (a), (b) and (c) are sampled image\nand views (different augmentations of the original image) used in [1, 2, 3, 4, 5]. (d), (e) and (f) show an example\nof two views built in PointContrast [11], which are captured at different poses in the reconstructed point clouds\n(d). Views built in this way differ much but meanwhile still maintain enough common semantic information\nincluding the same sofa and table. (g) and (h) are views for outdoor-scene point cloud from [15]. (g) is the\noriginal frame of point cloud and authors in [15] apply point cloud augmentation to (g) for (h), which can be\nimplemented with a simple linear transformation. [16] use point cloud at different timestamps as views for\ncontrastive learning, which are indicated in (g) and (i). The autonomous vehicle is waiting at the crossing while\nother cars and pedestrians are moving around, making it hard to ﬁnd accurate correspondence for contrastive\nlearning. (j) is an illustration of the sampled real scene and (k), (l) are views from vehicle and infrastructure\nsides, which are used by COˆ3 to make views more suitable for contrastive learning.\nIn the past decade, learning 3D representation from unlabelled data has achieved great success in\nsingle-object and indoor-scene point clouds. For point clouds of single objects such as CAD models,\nprevious works pre-train 3D encoders to predict a global representation by minimizing contrastive\nloss [10] and aim at low-level downstream tasks including object classiﬁcation and registration. To\nextend this idea to high-level perception tasks for indoor-scene point clouds, PointContrast [11]\npropose to reconstruct the whole indoor scenes, collect partial point clouds from two different poses\nand utilize them as two views in contrastive learning to learn dense (point-level or voxel-level)\nrepresentation. More recent works such as [12] and [13] also need the reconstruction and this\nnaturally assumes that the environment is static. Figure 1 (d), (e) and (f) show examples of views in\nPointContrast [11]. We can see that the views differ a lot because they are captured from different\nposes but meanwhile, they still contain enough common semantic information such as the same sofa\nand table, which are demonstrated important property of views in contrastive learning in [14].\nHowever, outdoor scenes are dynamic and large-scale, making it impossible to reconstruct the\nwhole scenes for building views. Thus, methods in [11, 12, 13] cannot be directly transferred but\nthere exists two possible alternatives to build views. The ﬁrst idea, embraced by [15], is to apply\ndata augmentation to single frame of point cloud and treat the original and augmented versions as\ndifferent views, as indicated by Figure 1 (g) and (h). However, all the augmentation of point clouds,\nincluding random drop, rotation and scaling, can be implemented in a linear transformation and views\nconstructed in this way do not differ enough. The second one is to consider point clouds at different\ntimestamps as different views, represented by [16]. Yet the moving objects would make it hard to\nﬁnd correct correspondence for contrastive learning. See Figure 1 (g) and (f), while the autonomous\nvehicle is waiting for the trafﬁc light to turn green, other cars and pedestrians are moving. The\nautonomous vehicle has no idea about how they move and is not able to ﬁnd correct correspondence\n(common semantics). Due to these limitations, pre-trained 3D encoders in [15, 16] cannot achieve\nnoticeable improvement when transferring to datasets collected by different LiDAR sensors.\n2\nFigure 2: Summary of 3D object detectors. The raw input LiDAR point cloud is ﬁrst processed by the 3D\nbackbone and per-point/voxel representations are generated, which are further transformed into the Bird-Eye-\nView (BEV) map. Finally, a 2D backbone and a detection head are applied to generate the detection results.\nTo overcome these limitations, we propose COoperative COntrastive Learning and COntextual\nShape Prediction, COˆ3, to learn representation for outdoor-scene point clouds in an unsupervised\nmanner. COˆ3 mainly contains two components, as described below.\nCooperative Views for Contrastive Learning. To build views of LiDAR point clouds that dif-\nfer enough and share adequate semantic information, we propose to utilize a recently released\ninfrastructure-vehicle-cooperation dataset called DAIR-V2X [17] and build views for contrastive\nrepresentation learning using point clouds respectively from infrastructure LiDAR and vehicle Li-\nDAR. As shown in (j), (k) and (l) in Figure 1, views built in this way differ a lot because they are\ncaptured at different positions and they share enough information because they are captured at the\nsame timestamp. With the raw input point clouds from the vehicle and infrastructure, we further fuse\nthe point cloud from both sides at the same timestamp and use the fusion point cloud and point cloud\nfrom vehicle-side as two views in contrastive representation learning. Background information about\nview building in contrastive learning can be found in Appendix A.\nContextual Shape Prediction. As proposed in [18], representation learned from purely contrastive\nlearning is not able to capture task-relevant information and a reconstruction objective can be\nimplemented alongside to compensate this limitation (for more details, please refer to Appendix B).\nDetailed experiments on image representation learning have been conducted in [18] to demonstrate\nthis statement and we want to borrow this idea to 3D point cloud. However, it can be extremely difﬁcult\nto reconstruct the whole scene with point-level or voxel-level representations. Instead, we propose a\npre-training goal to reconstruct local distribution of neighboring points using the dense representations.\nIn practice, we use shape context to describe the local distribution of each point’s neighborhood, which\nhas been demonstrated as a useful local distribution descriptor in previous works [12, 19, 20, 21].\nFigure 3: Two examples of shape context.\nFigure 3 shows two examples of shape\ncontext with 8 bins (the number of bins\ncan be changed). The neighborhood of\nthe query point (marked as a larger black\npoint) is ﬁrst divided into 8 bins and we\ncompute an 8-dimensional distribution\nwith the numbers of points in these bins.\nThe pre-training task is to predict local\ndistributions of each point or voxel with\nthe extracted point-level or voxel-level representation. This reﬁned reconstruction pre-training task\nintroduces more task-relevant information and helps learn much better representations.\nOur contributions can be summarized as follows. (1) COˆ3 is proposed to utilize the vehicle-\ninfrastructure-cooperation dataset to build adequate views for unsupervised contrastive 3D repre-\nsentation learning on outdoor-scene point clouds. (2) A shape-context prediction task is proposed\nalongside to inject task-relevant information, which is beneﬁcial for downstream 3D detection tasks.\n(3) The learned 3D representations can be well transferred to datasets collected by different LiDAR\nsensors. (4) Extensive experiments demonstrate the effectiveness of COˆ3. For example, COˆ3\nimproves Second, PV-RCNN, CenterPoints on Once [22] by 1.07, 0.62 and 2.58 respectively.\n2\nRelated Works\n3D Object Detection. Figure 2 summarizes current 3D object detection methods in autonomous\ndriving scenes. The raw LiDAR point clouds is ﬁrst passed through a 3D encoder and transferred into\nper-point or per-voxel representation. Then these dense representation is projected onto the ground\nplane and we get Bird-Eye-View (BEV) map. After that, the BEV map is encoded by a 2D backbone\n3\nFigure 4: The pipeline of COˆ3. The infrastructure point cloud is transformed and fused with vehicle point cloud\nbeforehand. With vehicle-side and fusion point clouds as inputs, we ﬁrst process them with the 3D backbone\nto generate point/voxel-level representations. With these dense representations, we propose two pre-training\nobjectives: (a) Cooperative Contrastive Loss, which introduces adequate views for contrastive learning. (b)\nContextual Shape Prediction Loss, to bring in task-relevant information that are beneﬁtial for downstream tasks.\nfollowed with a detection head and the ﬁnal detection results are generated. Up to now, 3D object\ndetectors can be divided into three main streams due to different 3D encoders they use: (1) point-based\nmethods [23, 24, 25] use point-based 3D backbone. (2) voxel-based methods [26, 27, 28, 29, 30, 31]\ngenerally transform point cloud into voxel grids and process them using 3D volumetric convolutions.\n(3) point-voxel-combined methods [32, 33, 34] utilize features from both (1) and (2). Among all these\nmethods, [26, 30, 33] are the most widely used detectors and achieve state-of-the-art performance.\nHowever, all these methods rely on precise 3D annotations and we propose COˆ3 to pre-train 3D\nbackbones without labels for outdoor-scene LiDAR point clouds.\nUnsupervised 3D Representation Learning for LiDAR Point Clouds. Contrastive pre-training on\ndifferent views of images have demonstrated promising performance in image domain [1, 2, 3, 4, 5]\nand PointContrast [11] is the pioneering work for unsupervised contrastive learning on indoor-scene\npoint clouds. Together with the following works including [12, 13], they rely on the assumption\nof static scenes that have been registered for constructing adequate views. To extend their ideas to\noutdoor-scene LiDAR point clouds, [15] augment single frame of point cloud to build views and [16]\nutilizes point clouds at different timestamps as views for unsupervised contrastive learning. However,\nas discussed in section 1, the views built in [15] are too similar while those in [16] are hard to ﬁnd\ncorrect correspondence, which violate the property for adequate views in contrastive learning as\nproposed in [14] and make their representations unable to transfer to datasets collected by different\nLiDAR sensors. In this work, we propose to use point clouds from vehicle and infrastructure to\nconstruct views in contrastive learning and introduce task-relevant information with a shape-context\nprediction pre-training goal to learn better 3D representation.\n3\nMethods\nIn this section, we introduce the proposed COˆ3 for unsupervised representation learning on LiDAR\npoint clouds in outdoor scenes. As detailed in Figure 4, COˆ3 has two pre-training objectives: (a) a\ncooperative contrastive learning goal on dense (point-level or voxel-level) representations between\nvehicle-side and fusion point clouds, which provides adequate views for contrastive learning. (b) a\ncontextual shape prediction loss to bring in more task-relevant information. To start with, we discuss\nthe problem formulation and overall pipeline in Section 3.1. Then we respectively introduce the\ncooperative contrastive objective and contextual shape prediction goal in Section 3.2 and Section 3.3.\n3.1\nProblem Formulation and Pipeline\nTo begin with, we deﬁne the raw LiDAR point clouds from vehicle-side and infrastructure-side\nrespectively as Pveh = [Pxyz\nveh, Pfeat\nveh] and Pinf = [Pxyz\ninf , Pfeat\ninf ], where Pxyz\nv/i ∈RNp\nv/i×3 and Pfeat\nv/i ∈\nRN p\nv/i×d. Here N p\nv/i denotes the number of points (or voxels) in vehicle-side and infrastructure-side\nrespectively and d = 1 is always the case to represent the intensity of each point (or voxel). Note here\nthat as vehicle/infrastructure/fusion point clouds may sometimes go through the same process, we will\nchange the notation to v/i/f to indicate the same processing on respective point cloud for convenience.\n4\nWhen collecting the cooperative dataset, each pair of vehicle-side and infrastructure-side point clouds\nis associated with a transformation T veh\ninf indicating the relationship between vehicle-side coordinate\nand infrastructure-side coordinate.\nWith Pveh, Pinf and T veh\ninf as inputs, COˆ3 ﬁrst transforms the infrastructure point cloud into vehicle-\nside coordinate, that is P′\ninf = [T veh\ninf (Pxyz\ninf ), Pfeat\ninf ], and concatenates the transformed infrastructure\npoint cloud and the vehicle point cloud into fusion point cloud Pfusion = [Pveh, P′\ninf], where Pfusion ∈\nR(Np\nveh+N p\ninf)×(3+d). Then Pveh and Pfusion are embedded by the 3D encoder f enc\nPenc\nv/f = f enc(Pv/f)\n(1)\nwhere Penc\nv/f = [Pxyzenc\nv/f\n, Pfeatenc\nv/f\n] and Pxyzenc\nv/f\n∈RNpenc\nv/f\n×3, Pfeatenc\nv/f\n∈RNpenc\nv/f\n×denc. N penc\nv/f is the number of\npoints (or voxels) after encoding. As there exists pooling operations in 3D encoders, the number of\npoints (or voxels) may change when processed by the 3D encoders. denc is the number of feature\nchannels after encoding. To guide the 3D encoder to learn good representations in an unsupervised\nmanner, we propose a cooperative contrastive loss LCO2 and a contextual shape prediction loss LCSP\nfor optimization. The overall loss function can be written as:\nL =\nX\nPv/f∈{Pv/f}\nLCO2{f enc(Pv/f)} + wCSP × LCSP{f enc(Pv/f), Pxyz\nveh, Pxyz\nfusion}\n(2)\nwhere Pv/f denote a batch of vehicle and fusion point clouds. As described in this equation, LCO2\ntakes as inputs the encoded vehicle and fusion point clouds and applies contrastive learning on the\nfeatures of these two views. Meanwhile, LCSP introduces more task-relevant information into f enc by\nusing the encoded features to predict contextual shape whose ground truth is obtained by Pxyz\nveh and\nPxyz\nfusion. wCSP is a weighting constant that makes the magnitudes of the two loss similar. Details about\nLCO2 and LCSP will be discussed respectively in Section 3.2 and Section 3.3.\n3.2\nCooperative Contrastive Objective\nUnsupervised contrastive learning has been demonstrated successful in image domain [1, 2, 3, 4, 5]\nand indoor-scene point clouds [11, 12, 13]. However, when it turns to outdoor-scene LiDAR\npoint clouds, building adequate views, which share common semantics while differing enough, for\ncontrastive learning is difﬁcult. To tackle this challenge, we utilize a recently released vehicle-\ninfrastructure-cooperation dataset called DAIR-V2X [17] and use vehicle-side point clouds and\nfusion point clouds as views for contrastive representation learning. The loss is deﬁned as follows:\nLCO2 = 1\nN1\nN1\nX\nn=1\n−log(\nexp(zn\nveh · zn\nfusion/τ)\nPN1\ni=1 exp(zi\nveh · zi\nfusion/τ)\n)\nwith\n{zn\nv/f}N1\nn=1\nsample\n∼\nZv/f\n;\nZv/f = normalize(MLP1(Pfeatenc\nv/f\n))\n(3)\nwhere the embedded features of vehicle and fusion point clouds, Pfeatenc\nveh\nand Pfeatenc\nfusion, are ﬁrst projected\ninto a common feature space by a Multi-Layer-Perceptron MLP1 and then normalized. Zv/f ∈\nRNpenc\nv/f\n×d1 is the projected features of vehicle point cloud and fusion point cloud, where d1 indicates\nthe dimension of the common feature space and N penc\nv/f are the point/voxel numbers of encoded vehicle\nand fusion point clouds respectively. We then sample N1 pairs of features (z) from Zv/f for contrastive\nlearning. According to our empirical observation, ground points have a great negative effect on\ncontrastive learning. Thus we mark those points with height value lower than a threshold zthd as\nground points and ﬁlter them out when sampling. After ﬁltering, we randomly sample N1 points\nfrom the vehicle point cloud and ﬁnd their corresponding points (or voxels) in the fusion point cloud\nto form N1 pairs of points (or voxels). We treat corresponding points (or voxels) as positive pairs and\notherwise negative pairs for contrastive learning and the ﬁnal loss function is shown in the ﬁrst line\nof Equation (3), where τ is the temperature parameter.\n3.3\nContextual Shape Prediction\nCOˆ3 aims to learn representations applicable to various downstream datasets. But the contrastive\nloss in Eqn.(3) is hard to encode task-relevant information as demonstrated in [18] and the authors\nresorts to an additional reconstruction objective on image to bring in task-relevant information.\n5\nHowever, for outdoor-scene point clouds, it is extremely difﬁcult to reconstruct the whole scene with\npoint/voxel-level representations. To mitigate this issue, we propose to reconstruct the neighborhood\nof each point/voxel with its representation. Speciﬁcally, a contextual shape prediction loss is designed\nas below. We describe the process for vehicle features and similar process is applied to fusion ones,\nLCSP = 1\nN2\nN2\nX\nn=1\nNbin\nX\nm=1\npn,m log pn,m\nqn,m\nwith\n{pn,∗}N2\nn=1\nsample\n∼\nP\n;\n{qn,∗}N2\nn=1\nsample\n∼\nQ\n;\nP = softmax(MLP2(Pfeatenc\nveh\n))\n(4)\nwhere the encoded features of vehicle point clouds, Pfeatenc\nveh\n, are ﬁrst passed through another Multi-\nLayer-Perceptron MLP2 and softmax operation is applied on the projected features to get a predicted\nlocal distribution of each point/voxel, that is P ∈RN penc\nveh ×Nbin. N penc\nveh is the number of vehicle-side\npoints/voxels after embedded by the 3D encoder and Nbin is the number of bin we divide the local\nneighborhood of each point/voxel. We use Nbin = 32 in this paper and compute the ’ground\ntruth’ local shape context Q ∈RNp\nveh×Nbin (N p\nveh is the number of raw input vehicle points/voxels)\nbeforehand, which will be discussed later. With P and Q, N2 sampled points/voxels are drawn from\nP and Q. We have pn,∗∈RNbin and qn,∗∈RNbin. Note that these sampled predicted contextual\nshape distributions are in pairs. Finally, as shown in the ﬁrst line in Equation (4), LCSP is a KL-\ndivergence loss applied on pn,∗and qn,∗, where the KL-divergence describes the distance between\ntwo probability distribution (pn,∗and qn,∗).\nTo compute the “ground truth” shape context for the ith point/voxel, we ﬁrst divide the neighborhood\nof the point/voxel into Nbin = 32 bins along x-y plane with R1 = 0.5m and R2 = 4m. Then we\ncompute the number of points/voxels in each bin and this results in Nbin = 32 numbers Qraw\ni,∗∈RNbin.\nNext, Qi,∗is ﬁnalized as below, where SFCSP is a scaling factor to make the distribution sensible.\nQi,∗= softmax(normalize(Qraw\ni,∗) × SFCSP)\n(5)\n4\nExperiments\n4.1\nExperiment Setup\nDatasets. We utilize the recently released vehicle-infrastructure-cooperation dataset called DAIR-\nV2X [17] to pre-train 3D sparse encoder with COˆ3 and ﬁne-tune the pre-trained encoder on two\ndownstream datasets: Once [22] and KITTI [35]. Note that LiDAR sensors used in Once and KITTI\nare different than that used in DAIR-V2X [17]. Details of the datasets can be found in Appendix C.\nDetectors. We select several current state-of-the-art methods implemented in the public repository\nof Once dataset [22]1 and OpenPCDet2 to evaluate the quality of representations learned by COˆ3,\nincluding Second [26], CenterPoint [30] and PV-RCNN [32].\nImplementation Details of COˆ3. We use Sparse-Convolution as the 3D encoder which is a\n3D convolutional network because it is widely used as 3D encoders in current state-of-the-art\nmethods [26, 30, 32]. We set the number of feature channels denc = 64, the temperature parameter\nin contrastive learning τ = 0.07, the dimension of common feature space of vehicle-side and fusion\npoint clouds d1 = 256 and the sample number in cooperative contrastive loss N1 = 2048. For\ncontextual shape prediction, we set the number of bins Nbin = 32, scaling factor SFCSP = 4, the\nsample number N2 = 2048 and the weighting constant wCSP = 10. The threshold for ground point\nﬁltering is zthd = 1.6m.\nBaselines. We implement GCC-3D [15] and STRL [16] as baselines. Besides, as proposed in [22],\nseveral representation learning methods in image domain and indoor-scene point clouds can be\ntransferred to outdoor scene point clouds, including Swav [3], Deep Cluster [36], BYOL [4] and\nPoint Contrast [11]. All experiment settings for different pre-training methods are the same as COˆ3.\nEvaluation Metrics. For Once dataset, mAPs (mean accurate precisions) for each category are\npresented in different ranges while for KITTI dataset, all the results are evaluated by mAPs with\nthree difﬁculty levels: Easy, Moderate and Hard. The results are also further averaged to compute an\n“Overall” mAP, which is the main evaluation metric.\n1https://github.com/PointsCoder/Once_Benchmark\n2https://github.com/open-mmlab/OpenPCDet\n6\nInit.\nDet.\nVehicle\nPedestrian\nCyclist\nmAP\n0-30m\n30-50m\n>50m\n0-30m\n30-50m\n>50m\n0-30m\n30-50m\n>50m\nRand\nSec.\n84.35\n66.41\n49.49\n27.87\n23.24\n16.36\n69.92\n52.27\n35.25\n52.21\nSwav\n83.21\n65.25\n50.32\n31.55\n26.18\n17.45\n69.40\n53.60\n35.91\n53.03+0.82\nD. Cl.\n84.02\n67.51\n50.26\n29.21\n21.55\n17.39\n69.86\n51.95\n34.69\n52.30+0.09\nBYOL\n81.60\n60.93\n46.97\n18.71\n16.59\n12.95\n61.20\n43.15\n27.30\n45.24\nP.C.\n84.19\n62.66\n46.32\n21.55\n17.70\n14.05\n64.98\n47.25\n28.81\n47.64\nGCC-3D\n85.43\n67.88\n51.64\n27.18\n21.55\n16.86\n72.15\n52.28\n35.27\n52.28\nSTRL\n83.71\n65.59\n50.39\n27.41\n22.23\n17.17\n68.28\n51.96\n34.17\n51.57\nOurs\n84.62\n67.11\n49.42\n33.64\n28.00\n17.61\n68.22\n52.89\n32.92\n53.28+1.07\nRand\nPV\n88.01\n72.15\n58.93\n29.67\n23.24\n16.47\n71.46\n54.61\n36.60\n54.55\nSwav\n87.80\n71.81\n57.42\n29.86\n24.88\n17.15\n72.56\n54.25\n36.44\n54.89+0.34\nD. Cl.\n87.68\n71.77\n57.11\n32.20\n26.00\n18.28\n71.64\n53.09\n34.80\n54.91+0.36\nBYOL\n87.37\n69.63\n55.55\n19.74\n18.82\n14.64\n67.01\n47.11\n31.11\n49.41\nP.C.\n87.72\n70.42\n55.43\n20.52\n18.93\n16.76\n68.58\n49.55\n33.59\n50.49\nGCC-3D\n87.71\n72.20\n59.42\n27.91\n25.96\n16.40\n72.59\n53.88\n37.58\n54.55\nSTRL\n89.39\n70.32\n57.40\n27.67\n23.41\n17.55\n72.05\n54.21\n36.85\n54.25\nOurs\n87.85\n71.79\n57.46\n32.75\n26.57\n17.29\n71.22\n52.50\n36.20\n55.17+0.62\nRand\nCen.\n77.42\n54.68\n38.21\n51.08\n41.13\n25.79\n70.67\n54.68\n35.14\n55.92\nSwav\n77.58\n54.28\n38.51\n53.95\n42.52\n28.04\n71.10\n54.99\n37.93\n57.00\nD. Cl.\n77.35\n55.12\n38.91\n54.99\n42.26\n29.31\n71.80\n56.60\n37.05\n57.65+1.08\nBYOL\n76.56\n53.61\n37.79\n46.48\n31.73\n18.72\n67.55\n49.65\n27.67\n52.17\nP.C.\n77.64\n53.38\n39.15\n49.69\n35.57\n23.29\n69.37\n50.65\n30.03\n54.17\nGCC-3D\n77.80\n56.75\n39.16\n54.46\n40.11\n25.61\n74.43\n57.04\n39.51\n58.32+2.40\nSTRL\n78.01\n54.10\n39.32\n54.09\n40.77\n25.90\n71.60\n56.56\n36.57\n57.44\nOurs\n78.02\n56.13\n39.94\n55.09\n42.34\n27.44\n74.17\n56.05\n38.16\n58.50+2.58\nTable 1: Results of 3D object detection on Once dataset [22]. We conduct experiments on 3 different detectors:\nSecond [26] (short as Sec.), PV-RCNN [33] (short as PV) and CenterPoint [30] (short as Cen.) and 8 different\ninitialization methods including random (short as Rand, i.e. training from scratch), Swav [3], Deep Cluster\n(short as D. Cl.) [36], BYOL [4], Point Contrast (short as P.C.) [11], GCC-3D [15] and STRL [16]. Results are\nmAPs in %. “0-30m”, “30-50m” and “>50m” respectively indicate results for objects in 0 to 30 meters, 30 to 50\nmeters and 50 meters to inﬁnity. The “mAP” in the ﬁnal column is the overall evaluation and major metric for\ncomparisons. We use bold font for top 3 mAP in each category in each range for better understanding.\n4.2\nMain Results\nOnce Detection. As shown in Table 1, when initialized by COˆ3 , all the three detectors achieve\nthe best performance on the overall mAPs, which we value the most, and CenterPoint [30] achieves\nthe highest overall mAP (58.50) with 2.58 improvement. The improvement on PV-RCNN [33] is\n0.62 in mAP (similar lower improvement with other pre-training methods) because PV-RCNN [33]\nhas both the point-based and voxel-based 3D backbones, among which COˆ3 only pre-trains the\nvoxel-based branch. When we look into detailed categories, it can be found that COˆ3 achieve\nconsistent improvement on Pedestrian class and the highest mAP when CenterPoint [30] is used as\nthe detector. For Cyclist class, CenterPoint [30] initialized by COˆ3 achieves the best performance\namong all the detectors and all the initialization methods. These are important for the deployment of\nautonomous driving system in real world. For the Vehicle class, the improvement achieved by COˆ3\nis not signiﬁcant (same in other initialization methods) because the performances of training from\nscratch are already very high with little room for improvement.\nKITTI Detection. As shown in Table 2, when initialized by COˆ3 , PV-RCNN [33] achieves the\nbest performance on Easy and Hard (+1.19) level and third place on Moderate level. Meanwhile\nSecond [26] equipped with COˆ3 achieves the highest mAP on Easy (+1.11) and Moderate level\n(+1.22) and third place on Hard level. The lower improvements on KITTI dataset [35] stem from\nthe smaller number of training samples (half of that in Once [22]), which makes the detectors\neasily reach their capacity and improvement is hard to achieve. Consistent results across different\ninitialization schemes demonstrate this. When we look into detailed categories, COˆ3 achieve\nconsistent improvement on Pedestrian and Cyclist, which is essential for autonomous driving system.\nOverall. COˆ3 achieve consistent improvement on different detectors and different datasets while\nother methods only occasionally boost the performance. These demonstrate that the representation\nlearned by COˆ3, which uses adequate views for contrastive learning and injects task-relevant\ninformation via contextual shape prediction, is able to be transferred to different LiDAR sensors.\n4.3\nAblation Study\nWe conduct ablation experiments to analyze the effectiveness of different components in COˆ3. We\nrespectively pre-train the 3D encoder with cooperative contrastive objective and contextual shape\n7\nInit.\nDet.\nVehicle\nPedestrian\nCyclist\nOverall\nEasy\nModerate\nHard\nRandom\nSec.\n77.45\n48.71\n63.32\n73.29\n63.16\n60.34\nSwav\n77.64\n49.48\n64.95\n73.23\n64.02+0.86\n60.93+0.59\nD. Cl.\n77.47\n49.46\n63.19\n73.19\n63.37\n60.08\nBYOL\n76.89\n43.29\n60.99\n71.05\n60.39\n56.98\nP.C.\n77.45\n45.32\n65.44\n72.67\n62.74\n59.21\nGCC-3D\n77.99\n47.92\n64.45\n73.86+0.57\n63.45\n59.80\nSTRL\n77.63\n48.46\n65.52\n73.95+0.66\n63.87+0.71\n60.93+0.59\nOurs\n77.95\n49.59\n65.60\n74.40+1.11\n64.38+1.22\n60.88+0.54\nRandom\nPV\n79.13\n53.43\n69.12\n78.54\n67.23\n63.68\nSwav\n79.35\n52.92\n71.45\n78.43−0.11\n67.91+0.68\n64.60+0.92\nD. Cl.\n79.22\n50.75\n71.21\n77.05\n67.06\n64.50+0.82\nBYOL\n79.02\n51.40\n72.07\n77.96\n67.50\n64.42\nP.C.\n79.31\n51.66\n72.40\n77.62\n67.79+0.56\n63.31\nGCC-3D\n79.16\n50.66\n69.95\n77.07\n66.59\n63.67\nSTRL\n79.15\n51.71\n67.78\n77.10\n66.21\n62.90\nOurs\n79.05\n52.47\n71.73\n78.81+0.27\n67.75+0.52\n64.87+1.19\nTable 2: Results of 3D object detection on KITTI dataset [35]. Results are mAPs in %. “Easy”, “Moderate” and\n“Hard” respectively indicate difﬁculty levels deﬁned in KITTI dataset [35]. Results in each category are from\nmoderate level. The “Overall” results in the ﬁnal column is the major metric for comparisons. We use bold font\nfor top 3 mAP in each category in each difﬁculty level for better understanding.\nInit.\nOnce (CenterPoint)\nKITTI (Second)\nVehicle\nPedestrian\nCyclist\nOverall\nVehicle\nPedestrian\nCyclist\nOverall\nRandom\n62.85\n45.52\n59.39\n55.92\n77.45\n48.71\n63.32\n63.16\nContextual Shape Prediction Only\n62.86\n49.17\n59.86\n57.30\n77.75\n49.16\n63.18\n63.36\nCooperative Contrastive Only\n63.39\n48.14\n61.05\n57.53\n77.40\n47.78\n65.06\n63.41\nCOˆ3\n64.50\n48.83\n62.17\n58.50\n77.95\n49.59\n65.60\n64.38\nTable 3: Results of ablation study on Once [22] and KITTI [35]. We use CenterPoint [30] on Once and\nSecond [26] on KITTI. Results are mAPs in %. For Once, results are average across different ranges. For KITTI,\nresults are all in moderate level. We highlight the best performance in each column for better understanding.\nprediction objective. Then we compare their performance in downstream tasks with those of training\nfrom scratch and pre-trained by COˆ3. As shown in Table 3, it can be found that each of the\nobjective alone can achieve slight improvement, which demonstrates the effectiveness of each part of\npre-training goal. Besides, once pre-trained by COˆ3, we achieve the best performance.\n5\nConclusion and Limitation\nIn this paper, we propose COˆ3, namely Cooperative Contrastive Learning and Contextual Shape\nPrediction, for unsupervised 3D representation learning in outdoor scenes. The recently released\nvehicle-infrastructure-cooperation dataset DAIR-V2X [17] is utilized to build views for cooperative\ncontrastive learning. Meanwhile the contextual shape prediction objective provides task-relevant\ninformation for the 3D encoders. Our experiments demonstrate that the representation learned by\nCOˆ3 can be transferred to downstream datasets collected by different LiDAR sensors to improve\nperformance of different detectors. The limitation of our work lies in the relatively small size\nof the released cooperation dataset. Large-scale dataset is demonstrated useful for unsupervised\nrepresentation learning in previous works [1, 2, 3, 4, 5]. Thus it will be interesting if larger cooperative\ndatasets can be collected in the future and used for pre-training to see whether larger improvement\ncan be made.\n8\nA\nBackground about View Building in Contrastive Learning\nIn this section, we discuss how to build proper views in contrastive learning. Firstly, we introduce\nthe formulation of contrastive learning and some important properties of good views in contrastive\nlearning as proposed in [14]. Then we discuss view building for LiDAR point clouds.\nView Building Contrastive Learning. Unsupervised representation learning aims to pre-train 2D/3D\nbackbones on a dataset without labels, which can be transferred to downstream datasets and tasks\nto achieve performance improvement over training from scratch (random initialization). Recently,\nunsupervised contrastive learning achieves great success in image domain [1, 2, 3, 4, 5]. Given a\nbatch of images X as inputs, these works ﬁrst apply two kinds of random augmentations for each\nimage xn ∈X (n = 1, 2, ..., N where N is the number of images in the batch) to get augmented\nimages xn\n1 and xn\n2, which are called different views of xn. The main objective of contrastive learning\nis to pull together the representations of views of the same image in the feature space while pushing\naway representations of different images, as indicated in the equation below:\nLcon = 1\nN\nN\nX\nn=1\n−log(\nexp(zn\n1 · zn\n2 /τ)\nPN\ni=1 exp(zn\n1 · zi\n2/τ)\n)\nwith\nzn\n1,2 = f enc\nI\n(xn\n1,2)\nn = 1, 2, ..., N\n(6)\nwhere f enc\nI\nis the 2D backbone used to extract representations. A ResNet50 [37] is usually used for\nf enc\nI\nbecause many methods apply it as backbone in downstream tasks including detection [7, 8, 9]\nand segmentation [7, 38]. Note that the ResNet50 is pre-trained only once and used to initialize\nmodels in downstream tasks. zn\n1,2 is the encoded representations for views In\n1,2. To apply contrastive\nloss in the ﬁrst line in Equation (6), views of the same image are considered as positive pairs and\nother pairs are negative ones. The numerator indicates the similarity of positive pairs while the\ndenominator sums up the positive similarity and the sum of similarity of negative pairs. τ is the\ntemperature parameter. Minimizing this loss equals to maximize the similarity of positive pairs and\nminimize the similarity of negative pairs, which is the objective discussed above.\nAuthors in [14] discuss what property views should have to beneﬁt contrastive learning via Information\nTheory and propose that mutual information [39, 40, 41] of views ,I(xn\n1; xn\n2), can indicate the quality\nof the learned representations. Mutual information formally quantiﬁes “how much information about\none random variable we can obtain when observing the other one”. Experiments on images suggest\nthat there exists a “sweet spot” for I(xn\n1; xn\n2) where the pre-trained backbone can achieve the most\nsigniﬁcant performance improvement in downstream tasks. This means the mutual information\nbetween views can neither be too low (sharing little semantics) nor too high (differing little). Further\nexperiments in [14] indicate that the mutual information of different augmented images is high and\nreducing I(xn\n1; xn\n2) by applying stronger augmentations is effective. The performance on downstream\ntasks increases at the beginning and then decrease when the augmentations are too strong.\nViews Building for LiDAR Point Clouds. As discussed in the main paper, it is impossible for us\nto reconstruct the whole outdoor-scene for constrative learning, which is demonstrated useful in\nindoor-scene [11, 12, 13]. And there exists two alternatives to build views for outdoor-scene LiDAR\npoint clouds. The ﬁrst one ([15]) is to apply data augmentation to single frame of point cloud and\ntreat the original and augmented versions as different views, which is similar to what previous works\ndo in image domain. However, the augmentations in image domain are highly non-linear while all\nthe augmentation of point clouds, including random drop, rotation and scaling, can be implemented\nin a linear transformation. As claimed in [14], the highly non-linear augmentations on images\nalready bring high mutual information between views. Thus views of LiDAR point clouds built in\nthis way would have higher mutual information, which is not adequate for learning representations.\nThe second intuitive idea to build views is to utilize point clouds at different timestamps, embraced\nby [16]. However, outdoor-scenes are dynamic and the autonomous driving vehicle has no idea about\nhow other objects (cars, pedestrians, etc.) move. Thus observing one view (timestamp t) bring in\nlittle information about the other one (timestamp t+10 for example), meaning that I(xn\n1; xn\n2) can\nbe extremely low and this can be harmful to the learned representations. Due to these limitations,\npre-trained 3D encoders in [15, 16] cannot achieve noticeable improvement when transferring to\ndatasets collected by different LiDAR sensors. Thus, in this paper, we propose to utilize the vehicle-\ninfrastructure-cooperation dataset [17], which capture the same scene from different view-point at the\nsame timestamp, for contrastive representation learning. Views built with this dataset neither share\n9\ntoo much information (captured from different view-points) nor share too little information (captured\nat the same time, easy to ﬁnd correspondence), which is adequate for contrastive learning.\nB\nReconstruction Objective for Task-relevant Information\nIn this section, we borrow the ideas from [18] to explain why pure contrastive learning bring less\nimprovements as shown in Table 3 in the main paper. Firstly, we give the deﬁnition of sufﬁcient\nrepresentation and minimal sufﬁcient representation in contrastive learning. Then, we present analysis\non image classiﬁcation problem as downstream task and we refer readers to [18] for other downstream\ntasks. Finally, we propose our pre-training objective for LiDAR point clouds.\nSufﬁcient Representation and Minimal Sufﬁcient Representation. Sufﬁcient Representation\nzn\n1,suf of xn\n1 contains all the information that is shared by xn\n1 and xn\n2, which means zn\n1,suf can be used\nto express common semantics shared by these two views. Among all the sufﬁcient representations\nfor xn\n2, minimal sufﬁcient representation zn\n1,min contains the least information about xn\n1. The learned\nrepresentation in contrastive learning is sufﬁcient and almost minimal. Assuming that Z1,suf is the set\nof all possible sufﬁcient representations, we can deﬁne these two concepts as belows\nDeﬁnition 1. zn\n1,suf of view xn\n1 is sufﬁcient for xn\n2 if and only if I(zn\n1,suf, xn\n2) = I(xn\n1, xn\n2).\nDeﬁnition 2. zn\n1,min ∈Z1,suf of view xn\n1 is minimal sufﬁcient if and only if I(zn\n1,min, xn\n1) ≤\nI(zn\n1,suf, xn\n1), ∀zn\n1,suf ∈Z1,suf.\nTheorem. (1) z1,suf provides more information about the downstream task T than z1,min. (2) The\nupper bound of error rates in downstream tasks using minimal sufﬁcient representations are higher\nthan that of sufﬁcient representations. That is,\nI(z1,suf, T) ≥I(z1,min, T)\nsup{P e\nsuf} ≤sup{P e\nmin}\n(7)\nThis gap stems from the missing task-relevant information in z1,min. To prevent this problem,\nauthors in [18] propose to add a reconstruction objective (reconstruct x1 using z1) alongside the\ncontrastive loss to increase I(z1, x1), which indirectly increases I(z1, T|x2) and brings improvement\nin downstream classiﬁcation problem over pure contrastive learning.\nProof for Image Classiﬁcation Problem. We denote the downstream classiﬁcation task as T and\nthe task-relevant information in minimal sufﬁcient representation z1,min can be described as below:\nI(z1,suf, T) = I(z1,min, T) + [I(x1, T|z1,min) −I(x1, T|z1,suf)]\n≥I(z1,min, T)\n(8)\nTo begin with, z1,suf and z1,min are sufﬁcient representations and they contain two parts of information:\nshared information between x1 and x2, Ishare, and extra information about x1, Isuf > Imin. Thus\nthe mutual information I(z1,suf, T) can be decomposed into I(z1,min, T) and [I(x1, T|z1,min) −\nI(x1, T|z1,suf)], where I(x1, T|z1,min) indicates the information about T we can obtain by observing\nx1 when z1,min is known. As the second term is larger than zero (Isuf > Imin), the right-hand-side\nof the ﬁrst line in Equation (8) is larger than I(z1,min, T), which indicates that z1,suf contains more\ntask-relevant information than z1,min and thus would have better performance in T. Then we consider\nusing Bayes error rate P e [42], which is the lower-bound of achievable error for the classiﬁer, to\nanalysis performance of z1,suf and z1,min on downstream classiﬁcation problem. We have\nP e\nsuf ≤1 −exp[−H(T) + I(x1, x2, T) + I(z1,suf, T|x2)]\nP e\nmin ≤1 −exp[−H(T) + I(x1, x2, T)]\n(9)\nwhere H(T) is the entropy of the task. Since 1−exp[−(H(T)+I(x1, x2, T)+I(z1,suf, T|x2)] ≤1−\nexp[−H(T) + I(x1, x2, T)], the upper bound of Bayes error rate of minimal sufﬁcient representation\nis larger than that of sufﬁcient representations. This indicates that ideally z1,suf can achieve better\nperformance than z1,min in classiﬁcation problem.\nContextual Shape Prediction Objective. As it is impossible to reconstruct the whole scene point\ncloud with point-level or voxel-level representations. We propose an additional pre-training objective\nto predict distribution of local neighborhood of a point/voxel using point/voxel-level representation.\n10\nWe use shape context to describe distribution of local neighborhood of a point/voxel, which has\nbeen demonstrated as a useful local distribution descriptor in previous works [12, 19, 20, 21]. As\ndemonstrated in our ablation study (Table 3 in main paper), this additional pre-training objective\nbring more signiﬁcant performance improvement over pre-trained by pure contrastive loss. We also\nprovide python-style code for computing shape context as followings\nAlgorithm 1 Implementation of Contextual Shape Computation in Python Style.\nclass Contextual_Shape(object):\ndef __init__(self, r1=0.125, r2=2, nbins_xy=2, nbins_zy=2):\nself.r1 = r1\nself.r2 = r2\nself.nbins_xy = nbins_xy\nself.nbins_zy = nbins_zy\nself.partitions = nbins_xy * nbins_zy * 2\ndef pdist_batch(rel_trans):\nD2 = torch.sum(rel_trans.pow(2), 3)\nreturn torch.sqrt(D2 + 1e-7)\ndef compute_rel_trans_batch(A, B):\nreturn A.unsqueeze(1) - B.unsqueeze(2)\ndef hash_batch(A, B, seed):\nmask = (A >= 0) & (B >= 0)\nC = torch.zeros_like(A) - 1\nC[mask] = A[mask] * seed + B[mask]\nreturn C\ndef compute_angles_batch(rel_trans):\nangles_xy = torch.atan2(rel_trans[:, :, :, 1], rel_trans[:, :, :, 0])\nangles_xy = torch.fmod(angles_xy + 2 * math.pi, 2 * math.pi)\nangles_zy = torch.atan2(rel_trans[:, :, :, 1], rel_trans[:, :, :, 2])\nangles_zy = torch.fmod(angles_zy + 2 * math.pi, math.pi)\nreturn angles_xy, angles_zy\ndef compute_partitions_batch(self, xyz_batch):\nrel_trans_batch = ShapeContext.compute_rel_trans_batch(xyz_batch, xyz_batch)\n# compute angles from different points to the query one\nangles_xy_batch, angles_zy_batch = ShapeContext.compute_angles_batch(\nrel_trans_batch)\nangles_xy_bins_batch = torch.floor(angles_xy_batch / (2 * math.pi / self.\nnbins_xy))\nangles_zy_bins_batch = torch.floor(angles_zy_batch / (math.pi / self.\nnbins_zy))\nangles_bins_batch = ShapeContext.hash_batch(angles_xy_bins_batch,\nangles_zy_bins_batch, self.nbins_zy)\n# compute distances between different points and the query one\ndistance_matrix_batch = ShapeContext.pdist_batch(rel_trans_batch)\ndist_bins_batch = torch.zeros_like(angles_bins_batch) - 1\n# generate partitions for each points\nmask_batch = (distance_matrix_batch >= self.r1) & (distance_matrix_batch <\nself.r2)\ndist_bins_batch[mask_batch] = 0\nmask_batch = distance_matrix_batch >= self.r2\ndist_bins_batch[mask_batch] = 1\nbins_batch = ShapeContext.hash_batch(dist_bins_batch, angles_bins_batch,\nself.nbins_xy * self.nbins_zy)\nreturn bins_batch\nC\nDatasets Details\nIn this section, we introduce details about different datasets used in the main paper for evaluation and\nalso two more datasets in additional experiments.\n11\nDAIR-V2X. DAIR-V2X [17] is the ﬁrst real-world autonomous dataset for vehicle-infrastructure-\ncooperative detection task. It covers various scenes, including cities and highways, and different\nwhether condition including sunny, rainy and foggy days. A virtual world coordinate is used to align\nthe vehicle LiDAR coordinate and infrastructure LiDAR coordinate. There are 38845 LiDAR frames\n(10084 in vehicle-side and 22325 in infrastructure-side) for cooperative-detection task. The dataset\ncontains around 7000 synchronized cooperative samples in total and we utilize them to pre-train 3D\nencoder in an unsupervised manner via the proposed COˆ3. The LiDAR sensor at vehicle-side is\n40-beam while a 120-beam LiDAR is utilized at infrastructure-side.\nOnce. Once [22] is a large-scale autonomous dataset for evaluating self-supervised methods with\n1 Million LiDAR frames and only 15k fully annotated frames with 3 classes (Vehicle, Pedestrian,\nCyclist). A 40-beam LiDAR is used in [22] to collect the point cloud data. We adopt common\npractice, including point cloud range and voxel size, in their public code repository3. As for the\nevaluation metrics, IoU thresholds 0.7, 0.3, 0.5 are respectively adopted for vehicle, pedestrian,\ncyclist. Then 50 score thresholds with the recall rates ranging from 0.02 to 1.00 (step size if 0.02) are\ncomputed and the 50 corresponding values are used to draw a PR curve, resulting in the ﬁnal mAPs\n(mean accurate precisions) for each category. We also further overage over the three categories and\ncompute an ’Overall’ mAP for evaluations.\nKITTI. KITTI [35] is a widely used self-driving dataset, where point clouds are collected by LiDAR\nwith 64 beams. It contains around 7k samples for training and another 7k for evaluation. For\npoint cloud range and voxel size, we adopt common practice in current popular codebase like\nMMDetection3D4 and OpenPCDet5. All the results are evaluated by mAPs with three difﬁculty\nlevels: Easy, Moderate and Hard. These three results are further average and an ’Overall’ mAP is\ngenerated for comparisons.\nWaymo. Waymo [43] is a large-scale dataset for evaluating 3D object detection methods. This dataset\ncontains more than 200K frames of outdoor-scene point clouds, among which 150K are for training\nand others are for validation and testing. The LiDAR used to collect this dataset is 64-beam. For\npoint cloud range and voxel size, we adopt common practice in OpenPCDet. We evaluate the models\nin Waymo by mean average precision (short as AP) and mean average precision with heading (short\nas APH) at two difﬁculty level deﬁned in [43]: Level 1 and 2.\nNuScenes. NuScenes [44] dataset contains around 1.4M annotated 3D boxes and the scenes are\nsplit in 700/150/150 respectively for training/validation/testing. The LiDAR used to collect this\ndataset is 32-beam. For point cloud range and voxel size, we adopt common practice in OpenPCDet.\nWe evaluate the models in NuScenes by mean average precision (short as mAP) in each annotated\ncategory including Car, Construction Vehicle, Bus, Trailer, Barrier, Motorcycle, Pedestrian and\nTrafﬁc Cone. We further average performance in each category and error in translation and rotation,\nwhich generate “mAP” and “NDS” (NuScenes Detection Scores) metrics for overall evaluation.\nD\nAdditional Experiment Results\nD.1\nSample Efﬁciency Results\nThe main goal of unsupervised representation learning is to reduce labels required in downstream\ntasks. We further evaluate the proposed COˆ3 in two large-scale autonomous datasets Waymo [43]\nand NuScenes [44] in a sample efﬁciency aspect. We use only 1% data in both datasets to conduct the\nﬁne-tuning experiments. Second [26] (short as Sec.) and CenterPoint [30] (short as Cen.) are used\nin both datasets. For evaluation details in both datasets, please refer to C. We include four kinds of\ninitialization schemes in our experiments including Rand (random initialization), GCC (GCC-3D\nfrom [15]), STRL from [16] and our proposed COˆ3 (“Ours”). Note that we only pre-train COˆ3\nonce on DAIR-V2X [17] and use the pre-trained backbone for downstream tasks in four different\ndatasets (KITTI [35], Once [22], Waymo [43] and NuScenes [44]).\nSample Efﬁciency in Waymo Dataset [43]. As shown in Table 4, when initialized by COˆ3 , both\ndetectors achieve the best performance on the overall mAPs and mAPHs, which we value the most.\n3https://github.com/PointsCoder/Once_Benchmark\n4https://github.com/open-mmlab/mmdetection3d\n5https://github.com/open-mmlab/OpenPCDet\n12\nWhen we look into detailed categories, it can be found that COˆ3 achieve consistent improvement\non all the classes and the highest mAPs and mAPHs when CenterPoint [30] is used as the detector.\nThese are important for real-world deployment of autonomous driving system when labels are scarce.\nInit.\nDet.\nVehicle\nPedestrian\nCyclist\nOverall\nLevel 1\nAP/APH\nLevel 2\nAP/APH\nLevel 1\nAP/APH\nLevel 2\nAP/APH\nLevel 1\nAP/APH\nLevel 2\nAP/APH\nLevel 1\nAP/APH\nLevel 2\nAP/APH\nRand\nSec.\n52.83/51.37\n45.98/44.69\n45.04/22.86\n38.33/19.45\n32.72/20.88\n31.46/20.08\n43.53/31.70\n38.59/28.07\nGCC\n53.18/51.72\n46.40/45.11\n45.90/22.81\n39.38/19.58\n36.85/20.66\n35.45/19.87\n45.31/31.73\n40.41/28.19\nSTRL\n51.79/50.34\n44.84/43.58\n43.58/23.05\n37.02/19.58\n32.22/18.96\n30.98/18.23\n42.53/30.78\n37.61/27.13\nOurs\n54.50/53.17\n47.48/46.31\n46.72/25.50\n40.06/21.86\n35.60/23.16\n34.25/22.29\n45.64/33.94\n+2.11/+2.24\n40.60/30.15\n+2.01/+2.08\nRand\nCen.\n53.36/52.57\n46.49/45.79\n50.10/42.47\n43.21/36.59\n49.73/48.38\n47.83/46.53\n51.06/47.81\n45.84/42.97\nGCC\n53.38/52.60\n46.51/45.82\n50.23/42.05\n43.58/36.44\n48.12/46.69\n46.30/44.92\n50.58/47.11\n45.46/42.39\nSTRL\n52.40/51.59\n45.43/44.73\n49.05/41.07\n42.36/35.43\n48.31/46.65\n46.49/44.88\n49.92/46.43\n46.30/43.47\nOurs\n54.21/53.47\n47.25/46.60\n50.44/42.84\n43.75/37.11\n49.80/48.55\n47.89/46.69\n51.48/48.29\n+0.42/+0.48\n46.30/43.47\n+0.46/+0.50\nTable 4: Results of sample efﬁciency experiments on Waymo dataset [43]. We conduct experiments on 2 different\ndetectors: Second [26] (short as Sec.) and CenterPoint [30] (short as Cen.) and 4 different initialization methods\nincluding random (short as Rand, i.e. training from scratch), GCC-3D [15], STRL [16] and the proposed COˆ3.\nResults are mAPs and mAPHs in %. The “mAP” in the ﬁnal column is the overall evaluation and major metric\nfor comparisons. We use bold font for the best performance in each category for better understanding.\nSample Efﬁciency in Nuscenes Dataset [44]. As shown in Table 5, when initialized by COˆ3 , both\ndetectors achieve the best performance on the overall mAPs and NDS, which we value the most.\nWhen we look into detailed categories, it can be found that COˆ3 achieve consistent improvement on\nall the classes and when initialized by COˆ3, we achieve the highest mAPs in 6 categories (car, truck,\nconstruction vehicle, bus, pedestrian and trafﬁc cone). These results demonstrate that COˆ3 can be\nused to improve the performance of different detectors in a sample-efﬁcient setting (small number of\nlabelled data), which is important for the deployment of autonomous driving vehicle in real world.\nInit.\nDet.\nCar\nTruck\nC. V.\nBus\nTrailer\nBarrier\nM. C.\nPed.\nT. C.\nmAP\nNDS\nRand\nSec.\n53.28\n18.53\n2.17\n25.55\n5.44\n14.66\n2.35\n37.16\n13.41\n17.25\n28.28\nGCC\n49.36\n15.14\n1.83\n23.87\n3.93\n16.61\n0.91\n47.18\n17.38\n17.62\n27.14\nSTRL\n53.94\n18.76\n1.88\n27.89\n7.79\n17.38\n2.17\n37.83\n14.17\n18.18\n29.62\nOurs\n57.35\n19.78\n4.33\n30.47\n5.80\n17.35\n3.00\n44.30\n16.04\n19.84+2.59\n30.83+2.55\nRand\nCen.\n59.21\n16.19\n1.92\n19.57\n3.14\n34.65\n10.10\n53.71\n27.84\n22.64\n30.58\nGCC\n61.05\n17.10\n3.75\n22.24\n3.99\n37.13\n12.85\n55.52\n26.10\n24.02\n31.53\nSTRL\n57.03\n14.13\n2.21\n17.14\n2.66\n32.31\n5.95\n55.64\n25.76\n21.28\n29.80\nOurs\n61.31\n18.46\n2.97\n22.97\n4.38\n35.02\n10.19\n55.75\n31.44\n24.25+1.61\n31.55+0.97\nTable 5: Results of sample efﬁciency experiments on NuScenes dataset [44]. We conduct experiments on 2\ndifferent detectors: Second [26] (short as Sec.) and CenterPoint [30] (short as Cen.) and 4 different initialization\nmethods including random (short as Rand, i.e. training from scratch), GCC-3D [15], STRL [16] and the proposed\nCOˆ3. Results are mAPs in %. Results in 9 classes are shown including construction vehicle (short as C. V.),\nmotorcycle (short as M. C.), pedestrian (short as Ped.) and trafﬁc cone (short as T. C.). The “mAP” and “NDS”\nin the last two column are major metrics for comparisons. We use bold font for the best performance in each\ncategory for better understanding.\nE\nImplementation Details\nIn this section, we introduce some details about implementation in both pre-training stage and ﬁne-\ntuning stage. Common settings of pre-training and ﬁne-tuning are listed in Table 6 and we discuss\nother settings that vary in different detectors later.\nOther Pre-training Settings. To accelerate the pre-training process, we utilize the “repeated dataset”\nin MMDetection3D and the schedule is set to 10 epochs, which equals to 20 epochs without “repeated\ndataset”. Thus the number 20 for training epochs in Table 6 is tilt.\nOther Fine-tuning Settings. Batch size settings for different detectors on different datasets are\nshown in Table 7. We train 20 epochs for CenterPoint [30] in NuScenes [44] and 80 epochs for\nSecond [26] in Nuscenes [44]. We use different types of GPUs, different number of GPUs and\ndifferent version of PyTorch [45] as compared to those used in the codebases, which may lead to\ndegrading when training from scratch. Thus these parameters are tuned based on the original settings\n13\nConﬁguration\nPre-training\nKITTI\nOnce\nWaymo\nNuScenes\noptimizer\nAdamW\nAdam\nAdam\nAdam\nAdam\nbase learning rate\n0.0001\n0.003\n0.003\n0.003\n0.003\nweight decay\n0.01\n0.01\n0.01\n0.01\n0.01\nbatch size\n16\n-\n-\n-\n-\nlearning rate schedule\ncyclic\ncyclic\ncyclic\ncyclic\ncyclic\nGPU numbers\n8\n4\n4\n8\n8\ntraining epochs\n20\n80\n80\n30\n-\nTable 6: Details about implementations. “Pre-training” means settings in DAIR-V2X [17]. “KITTI”, “Once”,\n“Waymo” and “NuScenes” respectively indicate settings for detection tasks in KITTI [35], Once [22], Waymo [43]\nand NuScenes [44]. We list all common settings and discuss those vary in different detectors below, which are\nmarked as “-” in this table.\nfrom the codebases to make the performance of training from scratch match or even surpass the\nresults they published.\nDetectors\nKITTI\nOnce\nWaymo\nNuScenes\nSecond\n48\n48\n48\n32\nPV-RCNN\n16\n48\n-\n-\nCenterPoint\n-\n32\n32\n32\nTable 7: Details about batchsize settings for different detectors on different datasets. “-” means there is no\nconﬁguration for the detector on the exact dataset in the codebase or we do not conduct the downstream\nexperiments.\nReferences\n[1] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual\nrepresentation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9729–9738, 2020.\n[2] Y. Tian, D. Krishnan, and P. Isola.\nContrastive multiview coding.\narXiv preprint\narXiv:1906.05849, 2019.\n[3] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning\nof visual features by contrasting cluster assignments. In H. Larochelle, M. Ranzato, R. Had-\nsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,\nvolume 33, pages 9912–9924. Curran Associates, Inc., 2020. URL https://proceedings.\nneurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf.\n[4] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doer-\nsch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, B. Piot, k. kavukcuoglu, R. Munos,\nand M. Valko.\nBootstrap your own latent - a new approach to self-supervised learn-\ning.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Ad-\nvances in Neural Information Processing Systems, volume 33, pages 21271–21284. Cur-\nran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\nf3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.\n[5] X. Wang, R. Zhang, C. Shen, T. Kong, and L. Li. Dense contrastive learning for self-supervised\nvisual pre-training. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR),\n2021.\n[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages\n248–255, 2009. doi:10.1109/CVPR.2009.5206848.\n[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object\ndetection and semantic segmentation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 580–587, 2014.\n14\n[8] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid\nnetworks for object detection. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 2117–2125, 2017.\n[9] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with\nregion proposal networks. Advances in neural information processing systems, 28, 2015.\n[10] L. Zhang and Z. Zhu. Unsupervised feature learning for point cloud understanding by contrasting\nand clustering using graph convolutional neural networks. In 2019 International Conference on\n3D Vision (3DV), pages 395–404. IEEE, 2019.\n[11] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, and O. Litany. Pointcontrast: Unsupervised pre-\ntraining for 3d point cloud understanding. In European Conference on Computer Vision, pages\n574–591. Springer, 2020.\n[12] J. Hou, B. Graham, M. Nießner, and S. Xie. Exploring data-efﬁcient 3d scene understanding\nwith contrastive scene contexts. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 15587–15597, 2021.\n[13] Y. Liu, L. Yi, S. Zhang, Q. Fan, T. Funkhouser, and H. Dong. P4contrast: Contrastive learning\nwith pairs of point-pixel pairs for rgb-d scene understanding. arXiv preprint arXiv:2012.13089,\n2020.\n[14] Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola. What makes for good views\nfor contrastive learning? In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin,\neditors, Advances in Neural Information Processing Systems, volume 33, pages 6827–6839.\nCurran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/\nfile/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf.\n[15] H. Liang, C. Jiang, D. Feng, X. Chen, H. Xu, X. Liang, W. Zhang, Z. Li, and L. Van Gool.\nExploring geometry-aware contrast and clustering harmonization for self-supervised 3d object\ndetection. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 3293–3302, 2021.\n[16] S. Huang, Y. Xie, S.-C. Zhu, and Y. Zhu. Spatio-temporal self-supervised representation\nlearning for 3d point clouds. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 6535–6545, 2021.\n[17] H. Yu, Y. Luo, M. Shu, Y. Huo, Z. Yang, Y. Shi, Z. Guo, H. Li, X. Hu, J. Yuan, and Z. Nie.\nDair-v2x: A large-scale dataset for vehicle-infrastructure cooperative 3d object detection. In\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), June 2022.\n[18] H. Wang, X. Guo, Z.-H. Deng, and Y. Lu. Rethinking minimal sufﬁcient representation in\ncontrastive learning. arXiv preprint arXiv:2203.07004, 2022.\n[19] S. Belongie, J. Malik, and J. Puzicha. Shape matching and object recognition using shape\ncontexts. IEEE transactions on pattern analysis and machine intelligence, 24(4):509–522,\n2002.\n[20] M. Körtgen, G.-J. Park, M. Novotni, and R. Klein. 3d shape matching with 3d shape contexts.\nIn The 7th central European seminar on computer graphics, volume 3, pages 5–17. Citeseer,\n2003.\n[21] S. Xie, S. Liu, Z. Chen, and Z. Tu. Attentional shapecontextnet for point cloud recognition.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n4606–4615, 2018.\n[22] J. Mao, M. Niu, C. Jiang, X. Liang, Y. Li, C. Ye, W. Zhang, Z. Li, J. Yu, C. Xu, et al. One\nmillion scenes for autonomous driving: Once dataset. 2021.\n[23] S. Shi, X. Wang, and H. Li. Pointrcnn: 3d object proposal generation and detection from point\ncloud. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June\n2019.\n15\n[24] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia. Multi-view 3d object detection network for\nautonomous driving. In Proceedings of the IEEE conference on Computer Vision and Pattern\nRecognition, pages 1907–1915, 2017.\n[25] B. Yang, W. Luo, and R. Urtasun. Pixor: Real-time 3d object detection from point clouds.\nIn Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages\n7652–7660, 2018.\n[26] Y. Zhou and O. Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n4490–4499, 2018.\n[27] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom. Pointpillars: Fast encoders for\nobject detection from point clouds. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12697–12705, 2019.\n[28] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multi-view convolutional neural networks\nfor 3d shape recognition. In Proceedings of the IEEE international conference on computer\nvision, pages 945–953, 2015.\n[29] S. Shi, Z. Wang, J. Shi, X. Wang, and H. Li. From points to parts: 3d object detection from point\ncloud with part-aware and part-aggregation network. IEEE transactions on pattern analysis\nand machine intelligence, 43(8):2647–2664, 2020.\n[30] T. Yin, X. Zhou, and P. Krahenbuhl. Center-based 3d object detection and tracking. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n11784–11793, 2021.\n[31] L. Fan, Z. Pang, T. Zhang, Y.-X. Wang, H. Zhao, F. Wang, N. Wang, and Z. Zhang. Embracing\nsingle stride 3d object detector with sparse transformer. arXiv preprint arXiv:2112.06375, 2021.\n[32] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li. Pv-rcnn: Point-voxel feature set\nabstraction for 3d object detection. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2020.\n[33] S. Shi, L. Jiang, J. Deng, Z. Wang, C. Guo, J. Shi, X. Wang, and H. Li. Pv-rcnn++: Point-voxel\nfeature set abstraction with local vector representation for 3d object detection. arXiv preprint\narXiv:2102.00463, 2021.\n[34] J. Deng, S. Shi, P. Li, W. Zhou, Y. Zhang, and H. Li. Voxel r-cnn: Towards high perfor-\nmance voxel-based 3d object detection. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 35, pages 1201–1209, 2021.\n[35] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti vision\nbenchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\n[36] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep clustering for unsupervised learning\nof visual features. In Proceedings of the European conference on computer vision (ECCV),\npages 132–149, 2018.\n[37] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–\n778, 2016.\n[38] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic\nimage segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nIEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2017.\n[39] C. E. Shannon. A mathematical theory of communication. ACM SIGMOBILE mobile computing\nand communications review, 5(1):3–55, 2001.\n[40] J. Kreer. A question of terminology. IRE Transactions on Information Theory, 3(3):208–208,\n1957. doi:10.1109/TIT.1957.1057418.\n16\n[41] Wikipedia contributors.\nMutual information — Wikipedia, the free encyclopedia,\n2022. URL https://en.wikipedia.org/w/index.php?title=Mutual_information&\noldid=1089343634. [Online; accessed 21-June-2022].\n[42] K. Fukunaga. Introduction to statistical pattern recognition. Elsevier, 2013.\n[43] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai,\nB. Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n2446–2454, 2020.\n[44] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan,\nand O. Beijbom. nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 11621–11631, 2020.\n[45] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning\nlibrary. Advances in neural information processing systems, 32, 2019.\n17\n",
  "categories": [
    "cs.CV",
    "cs.RO"
  ],
  "published": "2022-06-08",
  "updated": "2022-06-26"
}