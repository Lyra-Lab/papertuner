{
  "id": "http://arxiv.org/abs/2206.04805v1",
  "title": "Motif Mining and Unsupervised Representation Learning for BirdCLEF 2022",
  "authors": [
    "Anthony Miyaguchi",
    "Jiangyue Yu",
    "Bryan Cheungvivatpant",
    "Dakota Dudley",
    "Aniketh Swain"
  ],
  "abstract": "We build a classification model for the BirdCLEF 2022 challenge using\nunsupervised methods. We implement an unsupervised representation of the\ntraining dataset using a triplet loss on spectrogram representation of audio\nmotifs. Our best model performs with a score of 0.48 on the public leaderboard.",
  "text": "Motif Mining and Unsupervised Representation\nLearning for BirdCLEF 2022\nAnthony Miyaguchi1, Jiangyue Yu1, Bryan Cheungvivatpant1, Dakota Dudley1 and\nAniketh Swain1\n1Georgia Institute of Technology, North Ave NW, Atlanta, GA 30332\nAbstract\nWe build a classification model for the BirdCLEF 2022 challenge using unsupervised methods. We\nimplement an unsupervised representation of the training dataset using a triplet loss on spectrogram\nrepresentation of audio motifs. Our best model performs with a score of 0.48 on the public leaderboard.\nKeywords\nmotif mining, matrix profile, unsupervised representation learning, embedding, CEUR-WS, BirdCLEF\n2022\n1. Introduction\nThe BirdCLEF 2022 challenge involves identifying species of native Hawaiian birds from sound-\nscape recordings. The training dataset comprises 14.8 thousand recordings totaling over 190\nhours from Xeno-canto [1] of varying length and quality. The specific task involves predicting\nthe presence of a list of 29 bird species in 5-second non-overlapping windows of 1-minute\nsoundscape recordings. The recordings are Ogg Vorbis audio files at a sample rate of 32khz.\nThe Xeno-canto training examples are labeled by species but not at 5-second intervals per the\nchallenge’s primary task. Therefore, they may contain both ambient noise and birdcalls from\nother species.\nWe focus our efforts on unsupervised methods to tackle the lack of concrete labels for the\nmulti-label classification problem. First, we experiment with motif mining algorithms to identify\nwindows of audio that contain birdcalls as an unsupervised process for generating labels. The\nmotif mining process also provides metadata used to train downstream models. Additionally,\ninstead of training a classification model directly from training examples, we choose to build\nan embedding that captures similarities between birdcalls across all training examples. We\nfinally train classification models using the embedding model to reduce the dimensionality of\nthe original data.\nCLEF 2022: Conference and Labs of the Evaluation Forum, September 5–8, 2022, Bologna, Italy\n\" acmiyaguchi@gatech.edu (A. Miyaguchi); jyu478@gatech.edu (J. Yu); bcheung0@gatech.edu\n(B. Cheungvivatpant); dakotadudley13@gatech.edu (D. Dudley); aswain9@gatech.edu (A. Swain)\n~ https://acmiyaguchi.me (A. Miyaguchi)\n\u0012 0000-0002-9165-8718 (A. Miyaguchi); 0000-0001-8352-0391 (D. Dudley)\n© 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\nCEUR\nWorkshop\nProceedings\nhttp://ceur-ws.org\nISSN 1613-0073\nCEUR Workshop Proceedings (CEUR-WS.org)\narXiv:2206.04805v1  [cs.SD]  8 Jun 2022\n2. Motif Mining\nWe utilize a motif mining algorithm called SiMPle [2] to extract the segments of the audio clip\nthat best represent common patterns in the clip. SiMPle provides two primary operations to\ncompute summary data structures called the matrix profile and profile index: a self-join that\ncomputes the similarity within a track and a join that computes the similarity between two\ntracks. The procedure involves converting the raw audio track into a spectrogram to capture\nthe frequency components of the audio over a sliding window in time. Then we apply SiMPle\nto compute the matrix profile and profile index, providing the distance to the nearest neighbor\nand the index of the nearest neighbor in the set of windows in the track. A motif is a window of\nan audio track with the lowest distance to all other windows in the join operation as given by\nthe matrix profile. The indices of the matrix profile’s minimum and maximum distance values\nare the motifs and discord, respectively.\nFigure 1: Spectrograms show frequency components of audio transformed via STFT. We apply SiMPLe\nto obtain a matrix profile that summarizes the distance to the nearest neighbor for all time-slices in the\nspectrogram. Spectrogram parameterization affects the quality of the matrix profile summary.\nWe hypothesize that the motifs and discords capture the audio clip’s salient features, i.e.,\nbird calls. Much like how SiMPle can identify the chorus of a song via the motif, it may be\npossible to extract birdcalls as a motif from the training examples. We transform the training\ndataset by extracting the matrix profile and profile index information from each track via a\nself-join and using the 5-second motifs as a soft label for birdcalls in each species. We utilize\nthe self-join index profile during the training procedure of the birdcall embedding. We also test\nthe feasibility of using a matrix profile join as a feature in the classifier for the main BirdCLEF\ntask by directly utilizing the resulting join against a random set of motifs extracted from the\ntraining dataset.\n3. Birdcall Embedding\nAn embedding maps data from one space into a lower-dimensional space while maintaining\nrelative distances between mapped data. We experiment with creating a birdcall embedding\nusing a modified Tile2Vec model [3]. The Tile2Vec model utilizes a sampling procedure on\nspatially distributed data and a triplet loss to learn a lower-dimensional representation of Earth\nimagery tiles that retains semantic similarity between tiles. The triplet loss takes advantage of\nthe triangle inequality by forming triplets between anchor, neighbor, and distant tiles. We borrow\nthe loss for triplets (𝑡𝑎, 𝑡𝑛, 𝑡𝑑) with a margin 𝑚where 𝑓𝜃maps audio data to a 𝑑-dimensional\nvector of real numbers using a model with parameters 𝜃.\n𝐿(𝑡𝑎, 𝑡𝑛, 𝑡𝑑) = [||𝑓𝜃(𝑡𝑎) −𝑓𝜃(𝑡𝑛)||2 −||𝑓𝜃(𝑡𝑎) −𝑓𝜃(𝑡𝑑)||2 + 𝑚]+\n(1)\nWe hypothesize that we can learn an embedding by forming triplets using windows of audio\nthat have similar spectral qualities. We utilize the SiMPLe index of the audio spectrogram to\ndetermine the neighborhood of any given window of audio within a track. The Tile2Vec model\nis a modified ResNet-18 convolutional neural network – we make further modifications to\ngenerate spectrograms in the correct shape as the first layer in the model.\n4. Experiments\n4.1. Motif Mining Details\nWe begin by extracting the motifs and discords of the entire training dataset. However, this\nprocess is computationally intensive because it requires streaming the entire audio dataset,\ncomputing the spectrogram via STFT, and computing the matrix profile. Therefore, we pre-\ncompute the matrix profile and index profile for offline processing. We use librosa [4] for audio\nloading and transformation and implement a NumPy implementation of the SiMPle algorithm\nto facilitate usage in our processing pipeline. 1\nWe use chroma energy normalized (CEN) and Mel-scaled spectrograms. We generate CEN\nspectrograms at ten samples per second using the default parameters in librosa, with a 50 sample\nwindow for motif mining purposes and the Mel-scaled spectrograms using an FFT window of\n2048, a hop length of 80 samples, and 16 Mel bands with a 400 sample window.\nAs per figure 1, we note that the observed quality of a spectrogram and the resulting matrix\nprofile varies depending on the algorithm and parameters used. We can clearly distinguish the\nchirps in the high-frequency ranges in the Mel spectrogram, with obvious discords occurring\nin the same time intervals. We are presented with a lower resolution spectrogram with CEN\n1Implementation at github.com/acmiyaguchi/simple-fast-python\nspectrogram format\ntransform time\nshape\nsize\nSiMPle window\nself-join\nMel-scaled\n131ms\n(16, 7932)\n126912\n400\n3.3s\nCEN\n287ms\n(12, 292)\n3504\n50\n8.85ms\nTable 1\nStatistics from processing track XC144892 which is 28.8 seconds long. Each operation is run 10 times.\nWe note that our parameterization of the Mel-scaled spectrogram results in data that takes several\norders of magnitude longer to process than the CEN spectrograms.\nbecause it is aligned by chroma instead, which does not delineate between the bird call and the\nbackground noise.\nWe initially performed our motif mining using the CEN parameters, following closely with\nthe SiMPLe cover song extraction experiment. We found that the lower resolution of the CEN\nspectrogram representation allowed us to process all training examples and store the primary\nrepresentation on disk, as per table 1. Although noisy, the representation did extract bird calls\nas the primary motif, although lack of domain knowledge prevented us from quantifying the\nquality of these labels. As a result, some extracted motifs are background noise or human voices.\n4.2. Embedding Clustering Quality\nFigure 2: Flow diagram of the constituent pieces of the birdcall embedding.\nWe reuse the open-sourced Tile2Vec model and add a Mel spectrogram layer via nnAudio [5].\nWe use an FFT window of 4096 with a calculated hop length matching the height of 128 Mel\nbands. We choose these parameters to mimic the 128x128 pixel images used to train the source\nmodel.\nWe built two separate data loaders for this task — the first data loader pre-computed triplets\nfrom the original Ogg Vorbis files into NumPy array files. We only use the extracted motifs\nduring the data loading process. In addition, we attempt to adjust for the skewed species\ndistribution by oversampling underrepresented species. A second data loader streamed all audio\ncomputing triplets using the pre-computed SiMPle indices for each track. We break each track\ninto windows of 5-seconds and assign the nearest neighbor using the profile index. Next, we\ncreate several queues, each containing window pairs from tracks of different species of birds.\nThe data loader pops pairs from each queue to form a mini-batch, after which triplets are formed\nby randomly assigning the third element from an element within the mini-batch. Finally, we\naugment the audio tracks once formed into triplets.\nThe first approach is untenable for an online training procedure because it naively generated\nmotif triplets which required upwards of 3𝑁disk reads. In addition, we noticed that running\nthe data loading procedure online would cause underutilization between the CPU and GPU due\nto the loading bottleneck. By storing 5𝑒5 NumPy array triplets to disk, we could fully utilize\nthe GPU at the cost of disk space, going from the original 6GB of audio data to 135GB for the\npre-computed triplet data. The second approach required no additional memory because it took\non an iterable approach to creating triplets. It has a different distributional semantic from the\nAlgorithm 1 Sampling triplets from audio using precomputed SiMPle index\nRequire: 𝐷is the set of audio tracks as raw samples\nRequire: 𝑓𝑠is the sampling rate of the audio in Hz\nRequire: 𝑠is the size of the audio window in seconds\nRequire: 𝐼is the SiMPle index\nEnsure: 𝑇is the set of triplets formed by all windows\nEnsure: |𝑇| = |𝑃|\nInitialize pairs 𝑃= {}\nfor 𝑦∈𝐷do\n𝑤←window(𝑦, 𝑓𝑠, 𝑠)\n◁𝑤is an array of audio windows each of length 𝑠× 𝑓𝑠\nfor 𝑖←1, length(𝑤) do\n𝑃←𝑃∪(𝑦, 𝑤[𝑖], 𝑤[𝐼[𝑖]])\n◁Use the SiMPle index to get the nearest neighbor\nend for\nend for\n𝑇←{(𝑥𝑎, 𝑥𝑛, 𝑦𝑎)| ∀(𝑥, 𝑥𝑎, 𝑥𝑛) ∈𝑃, ∃((𝑦, 𝑦𝑎, 𝑦𝑛) ∈𝑃, 𝑥̸= 𝑦}\nfirst since it includes all audio windows instead of just the track motifs. We also do not address\nclass imbalances because online undersampling is difficult. We need further preprocessing to\ndetermine the total number and locations of birdcalls per class.\nWe apply a random gain of [−20, 20] dB, Gaussian random noise between [−5, 40] dB, and\npitch shift between [−4, 4] semitones using the audiomentations library [6]. We note that\naugmentation can become a bottleneck in the data loading process. Instead of applying it to the\nentire triplet, it suffices to apply it to just the motif pairs before triplet formation. We consider\ntorch-audiomentations but had poor performance due to difficulties applying the correct device\nto torch tensors inside the data loader.\nWe validate the embedding learned on the iterable data loader by training a classifier on motifs\ntransformed into the embedding space. We chose a subset of three species: brnowl, skylar,\nand houfin. These species are common and have several hundred audio tracks available. We\ngenerate a training dataset comprised of the motif of each track and cross-validate a logistic\nregression model using model accuracy on species accuracy. We sample 𝑘= 300 motifs for ten\nmodels and report the mean and standard deviation of the accuracy on embedding models that\nvary in the output size.\nWe note a slight difference between accuracy in models of equivalent parameterization when\nthe output dimension of the model increases from 256 to 512 from 0.53 to 0.54.\ndimension\naccuracy (n=10)\n128\n0.528 (0.031)\n256\n0.529 (0.019)\n512\n0.544 (0.048)\nTable 2\nAccuracy table of a logistic regression model trained on the embedding using motif mined from three\nspecies of birds.\nFigure 3: A scatter plot of a random set of motifs (n=300) drawn from three species of birds. The\nmotifs are truncated, padded, and transformed into the embedding space. We plot the top two principle\ncomponents found by PCA.\n4.3. Classifier Performance\nWe train a classifier using the embedding model as a preprocessing step. The first classifier\ntrains a gradient-boosted decision tree (GBDT) via LightGBM [7]. It uses a MultiOutputClassifer\nto create a model per task to suit the multi-label classification task using the default parameters\nof the LightGBM classifier. Next, we apply a similar set of augmentation to the embedding\nmodel, with a random gain of [−20, 20] dB, pitch shift between [−4, 4] semitones, time shift\nbetween [−0.1, 0.1]% of the window’s length, and colored noise between [−3, 30] dB and\n[−2, 2] frequency power decay using the PyTorch-audiomentations library [8]. We also train\na multi-layer perceptron (MLP) to perform multi-label classification directly using a binary\ncross-entropy loss. The loss function allows us to implement mixup [9] during data loading\nmini-batches as an augmentation process.\nWe experimented with using matrix profile join data as a feature in our classification model.\nWe draw 64 motif samples from a set of training motifs and run each audio window in the testing\ntask through a matrix profile against the training sample. We use the concatenation of the matrix\nprofile join’s min, median, and max as an additional feature in the model. Unfortunately, we\nfound that the additional CPU overhead to run SiMPle far outweighed any marginal performance\nbenefit to the model and was omitted in subsequent model iterations.\nWe compute the macro F1-score in table 3 to compare our best GBDT model against our best\nMLP model. We note that the GBDT edges slightly over the MLP, but performance for both\nclassifiers is poor. We often see a lack of positive predictions for a class, which leads to a score of\n0. We also assume that all audio windows contain birdcalls to simplify the calculations, so these\nclassifier\nmacro F1-score\nGBDT\n0.0177\nMLP\n0.0151\nTable 3\nA comparison of the two classifiers’ performance on a subset of the training audio data, using macro\nF1-score as metric. We limit the scored data to the first ten tracks of each species in the task.\nscores understate the actual performance of the models. However, we found that the general\nperformance is still poor in the BirdCLEF task, hovering around 0.48 on the public leaderboard\nduring the competition.\n5. Discussion\nIn a small set of shorter tracks (30 seconds or less), we consistently found that the discord\ntends to be a birdcall instead of the motif. It might be more productive to standardize a mining\nprocedure on shorter tracks instead of using the whole, variable-length tracks to take advantage\nof this observation. We can build a no-call detector using the matrix profile as a feature if\nwe can consistently detect positions of bird calls through discords or motifs. In addition, we\ncan achieve higher throughput on parallelization by reducing data skew in the distribution\nof training example lengths. Finally, both spectrogram transformations via nnAudio and the\nSiMPle algorithm can be written in PyTorch to increase the performance of the motif mining\nalgorithm.\nWe found that the birdcall motif triplet embedding performed poorly for downstream pre-\ndiction tasks. We hypothesize that the spectrogram parameters during motif mining cause\nthe embedding to rest on a representation ill-suited for classification. One modification to the\nembedding triplet procedure would be to form pairs of anchors and neighbors from overlapping\naudio windows. These pairs would be similar due to their distance in time. As the experimental\nprocedure before, triplets are formed by randomization in mini-batches. A sliding window triplet\nprocedure would enable a comparison of the validity of the learned motif triplet embedding.\n6. Conclusions\nWe implemented and experimented with the SiMPle motif mining algorithm, a modified Tile2Vec\nembedding model, and several multi-label classification models.2 Our performance on the\nleaderboard was underwhelming, but we solved many engineering problems throughout the\nchallenge with potential improvements for future BirdCLEF challenges.\nAcknowledgments\nThanks to the Data Science @ Georgia Tech (DS@GT) officers for organizing and publicizing\nrecruitment for the DS@GT Kaggle competition team.\n2Implementation at github.com/acmiyaguchi/birdclef-2022\nReferences\n[1] Xeno-canto, Xeno-canto: Sharing bird sounds from around the world, 2022. URL: https:\n//xeno-canto.org.\n[2] D. F. Silva, C.-C. M. Yeh, Y. Zhu, G. E. Batista, E. Keogh, Fast similarity matrix profile for\nmusic analysis and exploration, IEEE Transactions on Multimedia 21 (2018) 29–38.\n[3] N. Jean, S. Wang, A. Samar, G. Azzari, D. Lobell, S. Ermon, Tile2vec: Unsupervised repre-\nsentation learning for spatially distributed data, in: Proceedings of the AAAI Conference\non Artificial Intelligence, volume 33, 2019, pp. 3967–3974.\n[4] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar, E. Battenberg, O. Nieto, librosa:\nAudio and music signal analysis in python, in: Proceedings of the 14th python in science\nconference, volume 8, 2015.\n[5] K. W. Cheuk, H. Anderson, K. Agres, D. Herremans, nnaudio: An on-the-fly gpu audio to\nspectrogram conversion toolbox using 1d convolutional neural networks, IEEE Access 8\n(2020) 161981–162003. doi:10.1109/ACCESS.2020.3019084.\n[6] I. Jordal, A. Tamazian, E. T. Chourdakis, C. Angonin, askskro, N. Karpov, O. Sarioglu,\nkvilouras, E. B. Çoban, F. Mirus, J.-Y. Lee, K. Choi, MarvinLvn, SolomidHero, T. Alumäe,\niver56/audiomentations: v0.25.0, 2022. URL: https://doi.org/10.5281/zenodo.6594177. doi:10.\n5281/zenodo.6594177.\n[7] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T.-Y. Liu, Lightgbm: A highly\nefficient gradient boosting decision tree, Advances in neural information processing systems\n30 (2017) 3146–3154.\n[8] I. Jordal, K. Nishi, H. BREDIN, F. Lata, H. C. Blum, P. Manuel, akash raj, K. Choi,\nP. Żelasko, amiasato, M. L. Quatra, E. Schmidbauer, FrenchKrab, asteroid-team/torch-\naudiomentations: v0.10.1, 2022. URL: https://doi.org/10.5281/zenodo.6381721. doi:10.5281/\nzenodo.6381721.\n[9] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, mixup: Beyond empirical risk minimization,\narXiv preprint arXiv:1710.09412 (2017).\n",
  "categories": [
    "cs.SD",
    "cs.LG",
    "eess.AS"
  ],
  "published": "2022-06-08",
  "updated": "2022-06-08"
}