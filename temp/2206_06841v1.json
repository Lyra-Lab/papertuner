{
  "id": "http://arxiv.org/abs/2206.06841v1",
  "title": "Robust Reinforcement Learning with Distributional Risk-averse formulation",
  "authors": [
    "Pierre Clavier",
    "Stéphanie Allassonière",
    "Erwan Le Pennec"
  ],
  "abstract": "Robust Reinforcement Learning tries to make predictions more robust to\nchanges in the dynamics or rewards of the system. This problem is particularly\nimportant when the dynamics and rewards of the environment are estimated from\nthe data. In this paper, we approximate the Robust Reinforcement Learning\nconstrained with a $\\Phi$-divergence using an approximate Risk-Averse\nformulation. We show that the classical Reinforcement Learning formulation can\nbe robustified using standard deviation penalization of the objective. Two\nalgorithms based on Distributional Reinforcement Learning, one for discrete and\none for continuous action spaces are proposed and tested in a classical Gym\nenvironment to demonstrate the robustness of the algorithms.",
  "text": "Robust Reinforcement Learning with Distributional Risk-averse formulation\nPierre Clavier 1 2 Stéphanie Allassonnière 2 Erwan Le Pennec 1\nAbstract\nRobust Reinforcement Learning tries to make pre-\ndictions more robust to changes in the dynamics\nor rewards of the system. This problem is particu-\nlarly important when the dynamics and rewards\nof the environment are estimated from the data. In\nthis paper, we approximate the Robust Reinforce-\nment Learning constrained with a Φ-divergence\nusing an approximate Risk-Averse formulation.\nWe show that the classical Reinforcement Learn-\ning formulation can be robustiﬁed using standard\ndeviation penalization of the objective. Two al-\ngorithms based on Distributional Reinforcement\nLearning, one for discrete and one for continu-\nous action spaces are proposed and tested in a\nclassical Gym environment to demonstrate the\nrobustness of the algorithms.\n1. Introduction\nThe classical Reinforcement Learning (RL) problem using\nMarkov Decision Processes (MDPs) modelization gives a\npractical framework for solving sequential decision prob-\nlems under uncertainty of the environment. However, for\nreal-world applications, the ﬁnal chosen policy can be some-\ntimes very sensitive to sampling errors, inaccuracy of the\nmodel parameters, and deﬁnition of the reward. For dis-\ncrete state-action space, Robust MDPs is treated by Yang\n(2017); Petrik & Russel (2019); Grand-Clément & Kroer\n(2020a;b) or (Behzadian et al., 2021) among others. Here\nwe focus on more general continuous state space S with\ndiscrete or continuous action space A and with constraints\ndeﬁned using Φ-divergence. Robust RL (Morimoto & Doya,\n2005) with continuous action space focuses on robustness\nin the dynamics of the system (changes of P) and has been\nstudied in Abdullah et al. (2019); Singh et al. (2020); Urpí\net al. (2021); Eysenbach & Levine (2021) among others.\n1Centre de Mathématiques Appliquées, Ecole polytechnique,\nFrance 2INRIA HeKA, INSERM, Sorbonne Université, Uni-\nversité Paris Cité, France. Correspondence to: Pierre Clavier\n<pierre.clavier@polytechnique.edu>.\nICML 2022 Workshop on Responsible Decision Making in Dy-\nnamic Environments, Baltimore, Maryland, USA, 2022. Copyright\n2022 by the author(s).\nEysenbach & Levine (2021) tackles the problem of both\nreward and transitions using Max Entropy RL whereas the\nproblem of robustness in action noise perturbation is pre-\nsented in Tessler et al. (2019). Here we tackle the problem\nof Robustness thought dynamics of the system.\nIn this paper, we show that it is possible to approximate\na Robust Distributional Reinforcement Learning heuristic\nwith Φ-divergence constraints into a Risk-averse formula-\ntion, using a formulation based on mean-standard deviation\noptimization. Moreover, we focus on the idea that general-\nization, regularization, and robustness are strongly linked\ntogether as Husain et al. (2021); Derman & Mannor (2020);\nDerman et al. (2021); Ying et al. (2021); Brekelmans et al.\n(2022) noticed in the MDPs or RL framework. The contri-\nbution of the work is the following: we motivate the use of\nstandard deviation penalization and derive two algorithms\nfor discrete and continuous action space that are Robust to\nchange in dynamics. These algorithms do not require lots\nof additional parameter tuning, only the Mean-Standard De-\nviation trade-off has to be chosen carefully. Moreover, we\nshow that our formulation using Distributional Reinforce-\nment Learning is robust to change dynamics on discrete and\ncontinuous action spaces from Mujoco suite.\n2. Notations\nConsidering\na\nMarkov\nDecision\nProcess\n(MDP)\n(S, A, P, γ), where S is the state space, A is the ac-\ntion space, P (s′, r | s, a) is the reward and transition\ndistribution from state s to s′ taking action a and γ ∈(0, 1)\nis the discount factor. Stochastic policy are denoted π(a |\ns) : S →∆(A) and we consider the cases of action space\neither discrete our continuous.\nA rollout or trajectory using π from state s using initial\naction a is deﬁned as the the random sequence τ P,π|s,a =\n((s0, a0, r0) , (s1, a1, r1) , . . .) with s0 = s, a0 = a, at ∼\nπ (· | st) and (rt, st+1)\n∼\nP (·, · | st, at) ; we denote\nthe distribution over rollouts by P(τ) with P(τ)\n=\nP0 (s0) QT\nt=0 P (st+1, rt | st, at) π (at | st) dτ and usually\nwrite τ ∼P = (P, π). Moreover, considering the dis-\ntribution of discounted cumulative return ZP,π(s, a) =\nR(τ P,π|s,a) with R(τ) = P∞\nt=0 γtrt, the Q-function\nQP,π : S × A →R of π is the expected discounted\ncumulative return of the distribution deﬁned as follows :\narXiv:2206.06841v1  [cs.LG]  14 Jun 2022\nSubmission and Formatting Instructions for ICML 2022\nQP,π(s, a) := E[ZP,π(s, a)]. The classical initial goal\nof RL also called risk-neutral RL, is to ﬁnd the opti-\nmal policy π∗where QP,π∗(s, a) ≥QP,π(s, a) for all\nπ and s ∈S, a ∈A.\nFinally, the Bellman opera-\ntor T π and Bellman optimal operator T ∗are deﬁned as\nfollow: T πQ(s, a) := r(s, a) + γEP,π [Q (s′, a′)] and\nT ∗Q(s, a) := r(s, a) + γEP [maxa′ Q (s′, a′)].\nApplying either operator from an initial Q0 converges to a\nﬁxed point Qπ or Q∗at a geometric rate as both operators\nare contractive. Simplifying the notation with regards to\ns, a, π and P, we deﬁne the set of greedy policies w.r.t.\nQ called G(Q) = arg max\nπ∈Π⟨Q, π⟩. A classical approach\nto estimating an optimal policy is known as Approximate\nModiﬁed Policy Iteration (AMPI) (Scherrer et al., 2015)\n\u001a πk+1 ∈G (Qk)\nQk+1 = (T πk+1)m Qk + ϵk+1 ,\nwhich usually reduces to Approximate Value Iteration (AVI,\nm = 1 ) and Approximate Policy Iteration (API, m = ∞)\nas special cases. The term ϵk+1 accounts for errors made\nwhen applying the Bellman Operator in RL algorithms with\nstochastic approximation.\n3. Robust formulation in greedy step of API.\nIn this section, we would like to ﬁnd a policy that is robust\nto change of environment law P as small variations of P\nshould not affect too much the new policy in the greedy\nstep. In our case we are not looking at classical greedy step\nπ′ ∈G(Q) = arg max\nπ∈Π⟨Q, π⟩rather at the following :\nπ′ ∈G(Q) = arg max\nπ∈Π\n⟨min\nP Q(P,π), π⟩.\nThis heuristic in the greedy step can also be justiﬁed by try-\ning to avoid an overestimation of the Q functions present in\nthe Deep RL algorithms. Using this formulation, we need to\nconstrain the set of admissible transitions from state-action\nto the next state P to get a solution to the problem. In\ngeneral, without constraint, the problem is NP-Hard, so it re-\nquires constraining the problem to speciﬁc distributions that\nare not too far from the original one using distance between\ndistributions such as the Wasserstein metric (Abdullah et al.,\n2019) or other speciﬁc distances where the problem can be\nsimpliﬁed (Eysenbach & Levine, 2021). If a explicit form\nof minP Q(P,π) could be computed exactly for a given di-\nvergence, it would lead to a simpliﬁcation of this max-min\noptimization problem into a simple maximisation one.\nIn fact, simpliﬁcation of the problem is possible using\nspeciﬁc Φ-divergence denoted HΦ to constrain the prob-\nlem with Φ a closed convex function such that Φ : R →\nR ∪{+∞} and Φ(z) ≥Φ(1) = 0 for all z ∈R :\nHΦ (Q | P) = P\ni:pi>0 piΦ\n\u0010\nqi\npi\n\u0011\nwith P\ni:pi>0 qi = 1 and\nqi ≥0. This constraint requires qi = 0 if pi = 0 which\nmakes the measure Q absolutely continuous with respect to\nP.The χ2-divergence are a particular case of Φ-divergence\nwith Φ(z) = (z −1)2. For trajectories sampled from distri-\nbution P0 and looking at distribution P closed to P0 with\nregards to the χ2-divergence, the minimisation problem\nreduces to :\nmin\nP ∈Dχ2(P ∥P0)≤α Q(P,π) = Q(P0,π) −αVP0[Z]\n1\n2 .\n(1)\nThe proof can be found in annex A for α such that α ≤\nVP0[R(τ)]/\n\r\r\r ˜R\n\r\r\r\n2\n∞≤1 with ˜R(τ) = R(τ)−Eτ∼P0[R(τ)]\nthe centered return and VP0[Z] the variance of returns. For\nα > VP0[R(τ)]/\n\r\r\r ˜R\n\r\r\r\n2\n∞, the equality becomes an inequal-\nity but we still optimize a lower bound of our initial problem.\nDeﬁning a new greedy step which is penalized by the stan-\ndard deviation :\nπ′ ∈Gα(Q) = arg max\nπ∈Π\n⟨Q(P0,π) −αVP0[Z]\n1\n2 , π⟩\nwe now look at the current AMPI to improve robustness :\n\u001a πk+1 ∈Gα (Qk)\nQk+1 = (T πk+1)m Qk + ϵk+1 ,\nThis idea is very close to Risk-averse formulation in RL (i.e\nminimizing risk measure and not only the mean of rewards)\nbut here the idea is to approximate a robustness problem\nin RL. To do so, the standard deviation of the distribution\nof the returns must be estimated. Many ways are possible\nbut we favour distributional RL (Bellemare et al., 2017;\nDabney et al., 2017; 2018) which achieve very good perfor-\nmances in many RL applications. Estimating quantiles of\nthe distribution of return, we can simply estimate standard\ndeviation using classical estimator of the standard deviation\ngiven the quantiles over an uniform grid{qi(s, a)}1≤i≤n, :\nV[Z(s, a)]\n1\n2 = σ(s, a) =\nqPn\ni=1 (qi(s, a) −¯q(s, a))2\nwhere ¯q is the classical estimator of the mean. A differ-\nent interpretation of this formulation could be that by taking\nactions with less variance, we construct a conﬁdence interval\nwith the standard deviation of the distribution :\nZπ(s, a)\nd= ¯Z(s, a) −ασ(s, a).\nThis idea is present in classical UCB algorithms (Auer,\n2002) or pessimism/optimism Deep RL. Here we construct\na conﬁdence interval using the distribution of the return\nand not different estimates of the Q function such as in\nMoskovitz et al. (2021); Bai et al. (2022). In the next section,\nwe derive two algorithms, one for discrete action space and\none for continuous action space using this idea. A very\ninteresting way of doing robust Learning is by doing Max\nentropy RL such as in the SAC algorithm. In Eysenbach &\nLevine (2021), a demonstration that SAC is a surrogate of\nRobust RL is demonstrated formally and numerically and\nwe will compare our algorithm to this method.\nSubmission and Formatting Instructions for ICML 2022\n4. Distributional RL\nDistributional RL aims at approximating the return ran-\ndom variable Zπ(s, a) := P∞\nt=0 γtR (st, at) with s0 =\ns, a0 = a , st+1 ∼P (· | st, at) , at ∼π (· | st)and R\nthat we explicitly treat as a random variable. The classical\nRL framework approximate the expectation of the return\nor the Q-function, Qπ(s, a) := E [Zπ(s, a)] . Many algo-\nrithms and distributional representation of the critic exits\n(Bellemare et al., 2017; Dabney et al., 2017; 2018) but here\nwe focus on QR-DQN (Dabney et al., 2017) that approxi-\nmates the distribution of returs Zπ(s, a) with Zψ(s, a) :=\n1\nM\nPM\nm=1 δ\n\u0010\nθm\nψ (s, a)\n\u0011\n, a mixture of atoms-Dirac delta\nfunctions located at θ1\nψ(s, a), . . . , θM\nψ (s, a) given by a para-\nmetric model θψ : S × A →RM. Parameters ψ of a neural\nnetwork are obtained by minimizing the average over the\n1-Wasserstein distance between Zψ and the temporal differ-\nence target distribution TπZ ¯\nψ, where Tπ is the distributional\nBellman operator deﬁned in Bellemare et al. (2017). The\ncontrol version or optimal operator is denoted T Z ¯\nψ, with\nT πZ(s, a) = R(s, a) + γZ (s′, a′). According to Dabney\net al. (2017), the minimization of the 1-Wasserstein loss can\nbe done by learning quantile locations for fractions τm =\n2m−1\n2M , m ∈[1..M] via quantile regression loss, deﬁned for\na quantile fraction τ ∈[0, 1] as :\nLτ\nQR(θ) : = E ˜\nZ∼Z\nh\nρτ( ˜Z −θ)\ni\nwith ρτ(u) = u(τ −I(u < 0)), ∀u ∈R . Finally, to obtain\nbetter gradients when u is small, Huber quantile loss (or\nasymmetric Huber loss) can be used:\nρH\nτ (u) = |τ −I(u < 0)|L1\nH(u),\nwhere L1\nH(u) is a classical Huber loss with parameter 1. The\nquantile representation has the advantage of not ﬁxing the\nsupport of the learned distribution and is used to represent\nthe distribution of return in our algorithm for both discrete\nand continuous action space.\n5. Algorithm\nWe use a distributional maximum entropy framework for\ncontinuous action space which is closed to the TQC al-\ngorithm (Kuznetsov et al., 2020). This method uses an\nactor-critic framework with a distributional truncated critic\nto ovoid overestimation in the estimation with the max\noperator. This algorithm is based on a soft-policy itera-\ntion where we penalize the target yi(s, a) using the en-\ntropy of the distribution. More formally, to compute the\ntarget, the principle is to train N approximate estimate\nZψ1, . . . ZψC of the distribution of returns Zπ where Zψc\nmaps each (s, a) to Zψc(s, a) :=\n1\nM\nPM\nm=1 δ\n\u0010\nθm\nψn(s, a)\n\u0011\n,\nwhich is supported on atoms θ1\nψc(s, a), . . . , θM\nψc(s, a). Then\napproximations Zψ1, . . . ZψN are trained on the tempo-\nral difference target distribution denoted Y (s, a) con-\nstructed as follow.\nFirst atoms of trained distributions\nZψ1 (s′, a′) , . . . , ZψC (s′, a′) are pooled into Z (s′, a′) :=\nn\nθm\nψc (s′, a′) | c ∈[1..C], m ∈[1..M]\no\n.\nWe denote ele-\nments of Z (s′, a′) sorted in ascending order by z(i) (s′, a′),\nwith i ∈[1..MC]. Then we only keep the kC smallest\nelements of Z (s′, a′). We remove outliers of distribution to\navoir overestimation of the value function. Finally the atoms\nof the target distribution Y (s, a) :=\n1\nkC\nPkC\ni=1 δ (yi(s, a))\nare computed according to a soft policy gradient method\nwhere we penalised with the log of the policy :\nyi(s, a) := r(s, a) + γ\n\u0002\nz(i) (s′, a′) −η log πφ (a′ | s′)\n\u0003\n.\n(2)\nThe 1-Wasserstein distance between each of Zψn(s, a), n ∈\n[1..N] and the temporal difference target distribution\nY (s, a) is minimized learning the locations for quantile\nfractions τm = 2m−1\n2M , m ∈[1..M]. Similarly, we minimize\nthe loss :\nJZ (ψc) = ED,π\nh\n1\nMkC\nPM\nj=1\nPkC\ni=1 ρH\nτj\n\u0010\nyi(s, a) −θj\nψc(s, a)\n\u0011i\nover the parameters ψn, for each critic. With this formu-\nlation, the learning of all quantiles θm\nψn(s, a) is dependent\non all atoms of the truncated mixture of target distribu-\ntions. To optimize the actor, the following loss based on\nKL-divergence denoted DKL is used for soft policy improve-\nment, :\nJπ,α(φ) = ED\n\u0014\nDKL\n\u0012\nπφ (· | s) ∥\nexp( 1\nη ξα(θψ(s,·)))\nD\n\u0013\u0015\nwhere η can be seen as a temperature and needs to be tuned\nand D is a constant of normalisation. This expression sim-\nplify into :\nJπ,α(φ) = ED,π\n\"\nη log πφ(a | s) −1\nC\nC\nX\nc=1\nξα(θψc(s, a))\n#\nwhere s ∼D, a ∼πφ(· | s). Nontruncated estimate of the\nQ-value are used for policy optimization to avoid a dou-\nble truncation, in fact the Z-functions already approximate\ntruncated future distribution. Finally, η is the entropy tem-\nperature coefﬁcient and is dynamically adjusted by taking a\ngradient step with respect to the loss like in Haarnoja et al.\n(2018) :\nJ(η) = ED,πφ [(−log πφ (at | st) −Hη) η]\nat every time the πφ changes. Temperature η decreases\nif the policy entropy, −log πφ (at | st), is higher than Hη\nand increases otherwise. The algorithm is summarized as\nfollows : Our algorithm is based SAC framework but with\nmany distributional critics to improve the estimation of Q-\nfunctions while using mean-standard deviation objective in\nthe policy loss to improve robustness.\nSubmission and Formatting Instructions for ICML 2022\nAlgorithm 1 TQC with Standard Deviation penalisation\nInitialize policy πφ, critics Zψc, Z ¯\nψc for c ∈[1..C]\nfor each iteration do\nfor each step of the environment do\ncollect (st, at, rt, st+1) with policy πφ\nD ←D ∪{(st, at, rt, st+1)}\nend for\nfor each gradient steps do\nSample batch (s, a, s′, r) of D\nyi(s, a)\n←\nr(s, a)\n+\nγ\n\u0002\nz(i) (s′, a′) −η log πφ (a′ | s′)\n\u0003\nη ←η −λη ˆ∇ηJ(η)\nφ ←φ −λπ ˆ∇φJπ,α(φ)\nψc ←ψc −λZ ˆ∇ψcJZ (ψn) , c ∈[1..C]\n¯ψc ←βψc + (1 −β) ¯ψc, c ∈[1..C]\nend for\nend for\nreturn policy πφ,Zψc, c ∈[1 . . . C].\n6. Experiments\nWe try different experiments on continuous and discrete\naction space (see Annex B.2. for discrete case) to demon-\nstrate the interest of our algorithms for robustness using\nξ : Z →E[Z] −αV[Z]\n1\n2 instead of the mean. For con-\ntinuous action space, we compare our algorithm with SAC\nwhich achives state of the art in robust control (Eysenbach &\nLevine, 2021) on the Mujoco environment such as Hopper-\nv3, Walker-v3, or HalfCheetah-v3. We use a version where\nthe entropy coefﬁcient is adjusted during learning for both\nSAC and our algorithm as it requires less parameter tuning.\nMoreover, we show the inﬂuence of a distributional critic\nwithout a mean-standard deviation greedy step using α = 0\nto demonstrate the advantage of using a distributional critic\nagainst the classical SAC algorithm. We also compare our\nresults to TQC algorithm which is in fact very close to SAC\nalgorithm except the distributional critic. Finally, α the\npenalty is increased to show that for the tested environment,\nthere exists a value of α such as prediction are more robust\nto change of dynamics. In these simulations, variations\nof dynamics are carried out by moving the relative mass\nwhich is an inﬂuential physical parameter in all environ-\nments. All algorithms are trained with a relative mass of 1\nand then tested on new environment where the mass varies\nfrom 0.5 to 2. Two phenomena can be observed for the 3\nenvironments.\nIn Fig 1, we see that we can ﬁnd a value of α where the\nrobustness is clearly improved without deteriorating the av-\nerage performance. Normalized performance using by the\nmaximum of the performance for every curve to highlight\nrobustness and not only mean-performance can be found in\nannex B.1. If a too strong penalty is applied, the average\nperformance can be decreased as in the HalfCheetah-v3 en-\nvironment (see annex B.1). For Hopper-v3, a α calibrated\nat 5 gives very good robustness performances while for\nWalker2d-v3, the value is closer to 2. This phenomenon was\nexpected and in agreement with our formulation. Moreover,\nour algorithm outperforms the SAC algorithm for Robust-\nness tasks in all environments. The tuning of α is discussed\nin annex B.1.\nThe second surprising observation is that penalizing our\nobjective also improves performance in terms of stability\nduring training and in terms of average performance, es-\npecially for Hopper-v3 and Walker2d-v3 in Fig 1. Similar\nresults are present in the work of (Moskovitz et al., 2021)\nwhich gives an interpretation in terms of optimism and pes-\nsimism for the environments. This phenomenon is not yet\nexplained, but it is present in some environments that are\nparticularly unstable and have a lot of variance.\nWe observe similar results for discrete environments such\nas Cartpole-v1 and Acrobot-v1 (See annex B.2.).\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRelative mass\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nMean Reward\nSAC\n=0\n=1\n=2\n=3\n=4\n=5\n(a) Hopper-v3\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRelative mass\n0\n1000\n2000\n3000\n4000\n5000\n6000\nMean Reward\nSAC\n=0\n=1\n=2\n=3\n=4\n=5\n(b) Walker2d-v3\nFigure 1: Mean over 20 trajectories varying relative mass of\nenvironments.\n7. Conclusions\nIn this paper, we have tried to show that by using a mean-\nstandard deviation formulation to choose our actions pes-\nsimistically, we can increase the robustness of our envi-\nronment for continuous and discrete environments without\nadding complexity. A single ﬁxed α parameter must be\ntuned to obtain good performance without penalizing the\naverage performance too much. Moreover, for some envi-\nronments, it is relevant to penalize to increase the average\nperformance as well when there is lots of variability in the\nenvironment.\nAbout the limitations of this work, the convergence of the\nalgorithm to a ﬁxed point is not shown for mean-standard\ndeviation penalization and this formulation is still based\non a heuristic even if the link between robustness and pe-\nnalization is established. Theoretical link with a (Kumar\net al., 2022) could be an interesting way of analyzing our\nalgorithm as they use action value to penalize their objective\nand remains a way to explain this phenomenon. This is left\nfor future work.\nSubmission and Formatting Instructions for ICML 2022\nReferences\nAbdullah, M. A., Ren, H., Ammar, H. B., Milenkovic, V.,\nLuo, R., Zhang, M., and Wang, J. Wasserstein robust\nreinforcement learning. arXiv preprint arXiv:1907.13196,\n2019.\nAuer, P.\nUsing conﬁdence bounds for exploitation-\nexploration trade-offs.\nJournal of Machine Learning\nResearch, 3(Nov):397–422, 2002.\nBai, C., Wang, L., Yang, Z., Deng, Z., Garg, A., Liu, P.,\nand Wang, Z. Pessimistic bootstrapping for uncertainty-\ndriven ofﬂine reinforcement learning.\narXiv preprint\narXiv:2202.11566, 2022.\nBehzadian, B., Petrik, M., and Ho, C. P. Fast algorithms for\nl∞-constrained s-rectangular robust mdps. Advances in\nNeural Information Processing Systems, 34, 2021.\nBellemare, M. G., Dabney, W., and Munos, R. A distri-\nbutional perspective on reinforcement learning.\n34th\nInternational Conference on Machine Learning, ICML\n2017, 1:693–711, 7 2017.\nURL https://arxiv.\norg/abs/1707.06887v1.\nBrekelmans, R., Genewein, T., Grau-Moya, J., Delétang,\nG., Kunesch, M., Legg, S., and Ortega, P. Your pol-\nicy regularizer is secretly an adversary. arXiv preprint\narXiv:2203.12592, 2022.\nDabney, W., Rowland, M., Bellemare, M. G., and Munos,\nR. Distributional reinforcement learning with quantile\nregression. 32nd AAAI Conference on Artiﬁcial Intel-\nligence, AAAI 2018, pp. 2892–2901, 10 2017.\nURL\nhttps://arxiv.org/abs/1710.10044v1.\nDabney, W., Ostrovski, G., Silver, D., and Munos, R. Im-\nplicit quantile networks for distributional reinforcement\nlearning.\n35th International Conference on Machine\nLearning, ICML 2018, 3:1774–1787, 6 2018.\nURL\nhttps://arxiv.org/abs/1806.06923v1.\nDerman, E. and Mannor, S. Distributional robustness and\nregularization in reinforcement learning. arXiv preprint\narXiv:2003.02894, 2020.\nDerman, E., Geist, M., and Mannor, S. Twice regularized\nmdps and the equivalence between robustness and regu-\nlarization. Advances in Neural Information Processing\nSystems, 34, 2021.\nDuchi, J. and Namkoong, H. Learning models with uni-\nform performance via distributionally robust optimization.\narXiv preprint arXiv:1810.08750, 2018.\nDuchi, J., Glynn, P., and Namkoong, H. Statistics of ro-\nbust optimization: A generalized empirical likelihood\napproach. arXiv preprint arXiv:1610.03425, 2016.\nEysenbach, B. and Levine, S. Maximum entropy rl (prov-\nably) solves some robust rl problems. arXiv preprint\narXiv:2103.06257, 2021.\nGeist, M., Scherrer, B., and Pietquin, O. A theory of regu-\nlarized markov decision processes. In International Con-\nference on Machine Learning, pp. 2160–2169. PMLR,\n2019.\nGotoh, J.-y., Kim, M. J., and Lim, A. E. Robust empirical\noptimization is almost the same as mean–variance opti-\nmization. Operations research letters, 46(4):448–452,\n2018.\nGrand-Clément, J. and Kroer, C. First-order methods for\nwasserstein distributionally robust mdp. arXiv preprint\narXiv:2009.06790, 2020a.\nGrand-Clément, J. and Kroer, C. Scalable ﬁrst-order meth-\nods for robust mdps. arXiv preprint arXiv:2005.05434,\n2020b.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft\nactor-critic: Off-policy maximum entropy deep reinforce-\nment learning with a stochastic actor.\n35th Interna-\ntional Conference on Machine Learning, ICML 2018,\n5:2976–2989, 1 2018. URL http://arxiv.org/\nabs/1801.01290.\nHusain, H., Ciosek, K., and Tomioka, R. Regularized poli-\ncies are reward robust. In International Conference on\nArtiﬁcial Intelligence and Statistics, pp. 64–72. PMLR,\n2021.\nJaimungal, S., Pesenti, S. M., Wang, Y. S., and Tatsat, H.\nRobust risk-aware reinforcement learning. SSRN Elec-\ntronic Journal, 8 2021. doi: 10.2139/SSRN.3910498.\nURL https://papers.ssrn.com/abstract=\n3910498.\nJain, A., Patil, G., Jain, A., Khetarpal, K., and Precup, D.\nVariance penalized on-policy and off-policy actor-critic.\n2021a. URL www.aaai.org.\nJain, A., Patil, G., Jain, A., Khetarpal, K., and Precup, D.\nVariance penalized on-policy and off-policy actor-critic.\narXiv preprint arXiv:2102.01985, 2021b.\nKumar, N., Levy, K., Wang, K., and Mannor, S. Efﬁcient\npolicy iteration for robust markov decision processes via\nregularization. arXiv preprint arXiv:2205.14327, 2022.\nKuznetsov, A., Shvechikov, P., Grishin, A., and Vetrov, D.\nControlling overestimation bias with truncated mixture\nof continuous distributional quantile critics. In Interna-\ntional Conference on Machine Learning, pp. 5556–5566.\nPMLR, 2020.\nSubmission and Formatting Instructions for ICML 2022\nMa, Y. J., Jayaraman, D., and Bastani, O. Conservative of-\nﬂine distributional reinforcement learning. 7 2021. URL\nhttps://arxiv.org/abs/2107.06106v2.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. 12 2013. URL\nhttps://arxiv.org/abs/1312.5602v1.\nMorimoto, J. and Doya, K. Robust reinforcement learning.\nNeural computation, 17(2):335–359, 2005.\nMoskovitz, T., Parker-Holder, J., Pacchiano, A., Arbel, M.,\nand Jordan, M. Tactical optimism and pessimism for deep\nreinforcement learning. Advances in Neural Information\nProcessing Systems, 34, 2021.\nNam, D. W., Kim, Y., and Park, C. Y. Gmac: A distribu-\ntional perspective on actor-critic framework. In Interna-\ntional Conference on Machine Learning, pp. 7927–7936.\nPMLR, 2021.\nPetrik, M. and Russel, R. H. Beyond conﬁdence regions:\nTight bayesian ambiguity sets for robust mdps. Advances\nin neural information processing systems, 32, 2019.\nRafﬁn, A., Hill, A., Ernestus, M., Gleave, A., Kanervisto,\nA., and Dormann, N. Stable baselines3, 2019.\nScherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B.,\nand Geist, M. Approximate modiﬁed policy iteration and\nits application to the game of tetris. J. Mach. Learn. Res.,\n16(49):1629–1676, 2015.\nSchulman, J., Levine, S., Moritz, P., Jordan, M. I., and\nAbbeel, P. Trust region policy optimization. 32nd Inter-\nnational Conference on Machine Learning, ICML 2015,\n3:1889–1897, 2 2015. URL http://arxiv.org/\nabs/1502.05477.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv, 7 2017a.\nISSN 23318422.\nURL https://\narxiv.org/abs/1707.06347v2. PPO algorithm\npremier papier<br/>Important à citer<br/><br/>.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017b.\nSingh, R., Zhang, Q., and Chen, Y. Improving robustness\nvia risk averse distributional reinforcement learning. In\nLearning for Dynamics and Control, pp. 958–968. PMLR,\n2020.\nSmirnova, E., Dohmatob, E., and Mary, J. Distributionally\nrobust reinforcement learning. 2 2019. URL https:\n//arxiv.org/abs/1902.08708v2.\nTessler, C., Efroni, Y., and Mannor, S. Action robust rein-\nforcement learning and applications in continuous control.\nIn International Conference on Machine Learning, pp.\n6215–6224. PMLR, 2019.\nUrpí, N. A., Curi, S., and Krause, A. Risk-averse ofﬂine\nreinforcement learning. arXiv preprint arXiv:2102.05371,\n2021.\nVieillard, N., Kozuno, T., Scherrer, B., Pietquin, O., Munos,\nR., and Geist, M. Leverage the average: an analysis of\nkl regularization in reinforcement learning. Advances\nin Neural Information Processing Systems, 33:12163–\n12174, 2020.\nWang, H. and Zhou, X. Y. Continuous-time mean–variance\nportfolio selection: A reinforcement learning framework.\nMathematical Finance, 30(4):1273–1308, 2020.\nWiesemann, W., Kuhn, D., and Rustem, B. Robust markov\ndecision processes. Mathematics of Operations Research,\n38(1):153–183, 2013.\nYang, I. A convex optimization approach to distributionally\nrobust markov decision processes with wasserstein dis-\ntance. IEEE Control Systems Letters, 1:164–169, 2017.\nISSN 24751456. doi: 10.1109/LCSYS.2017.2711553.\nYing, C., Zhou, X., Su, H., Yan, D., and Zhu, J. Towards\nsafe reinforcement learning via constraining conditional\nvalue-at-risk. 2021.\nZhang, J. and Weng, P. Safe distributional reinforcement\nlearning.\nZhang, J. and Weng, P. Safe distributional reinforcement\nlearning. In International Conference on Distributed\nArtiﬁcial Intelligence, pp. 107–128. Springer, 2021.\nSubmission and Formatting Instructions for ICML 2022\nA. Proof of (1)\nWe consider the following equality :\nmin\nP ∈Dχ2(P ∥P0)≤α Q(P,π) = Q(P0,π) −αVP0[Z]\n1\n2 .\n(3)\nConsider that trajectories τ is drawn from P but here we will write P the transition of the environment as the policy π is\nﬁxed and it is the only part which differ.\nWriting ˜R(τ) = R(τ) −Eτ∼P0[R(τ)] we get :\n∥Eτ∼P [R(τ)] −Eτ∼P0[R(τ)]∥=\n\r\r\r\r\nZ\nτ\n˜R(τ)\n\u0000p(τ) −p0(τ)\n\u0001\ndτ\n\r\r\r\r\n=\n\r\r\r\r\r\nZ\nτ\n˜R(τ)\np\np0(τ)\n\u0000p(τ) −p0(τ)\n\u0001\np\np0(τ)\ndτ\n\r\r\r\r\r\n≤\n\r\r\r\r\nZ\nτ\n˜R(τ)2p0(τ)dτ\n\r\r\r\r\n1\n2 \r\r\r\r\r\nZ\nτ\n\u0000p(τ) −p0(τ)\n\u00012\np0(τ)\ndτ\n\r\r\r\r\r\n1\n2\n= VP0[R(τ)]\n1\n2 Dχ2(P∥P0)\n1\n2 ,\nbecause of the positivity of divergence and of the variance, norms are removed. This inequality comes from Cauchy-Schwarz\ninequality and becomes an equality if for λ ∈R :\n˜R(τ)p0(τ) = λ(p(τ) −p0(τ)) ⇐⇒p(τ) = p0(τ)(1 + 1\nλ\n˜R(τ)).\n(4)\nHowever, p(τ) needs to be non-negative and sum to one as it is a measure. Normalisation condition is respected by\nconstruction however to ensure that the measure is non-negative, this requires\n\r\r\r ˜R(τ)/λ\n\r\r\r ≤1 in the case where λ ≤0 .\nIn this case of equality, we obtain from 4 that Dχ2(P∥P0) =\nVP0[R(τ)]\nλ2\n. Replacing the divergence in the inequality, the\nfollowing result holds :\n∥Eτ∼P [R(τ)] −Eτ∼P0[R(τ)]∥≤VP0(R(τ))\nλ\n.\nFor proving (3) we are interested in the case where Dχ2(P∥P0) ≤α, from the initial inequality we obtain :\nmin\nP ∈Dχ2(P ∥P0)≤α Q(P,π) ≥\nmin\nP ∈Dχ2(P ∥P0)≤α Q(P0,π) −Dχ2(P∥P0)VP0[Z]\n1\n2 = Q(P0,π) −αVP0[Z]\n1\n2\nwith the maximum value of α equals to Dχ2(P∥P0) =\nVP0[R(τ)]\nλ2\n≤\nVP0[R(τ)]\n∥˜\nR∥\n2\n∞\n= ∥˜\nR∥\n2\n2\n∥˜\nR∥\n2\n∞\n≤1, where the ﬁrst inequality\ncomes from the conditions\n\r\r\r ˜R(τ)/λ\n\r\r\r ≤1 and the last one comes from that the L2 norm is smaller than ∞-norm.\nIf our problem is contrained, assuming α ≤\nVP0[R(τ)]\n∥˜\nR∥\n2\n∞\n≤1, we obtain the following results with the maximum attained for\nDχ2(P∥P0) = α :\nmin\nP∈Dχ2(P ∥P0)≤α Q(P,π) = Q(P0,π) −αV[Z]\n1\n2 .\n(5)\nFor α > 1, we still optimize a lower bound of the quantity of interest. The formulation of our algorithm becomes:\nSubmission and Formatting Instructions for ICML 2022\n(\nπk+1 ∈Gα (Zk) = G(ξα(Zk) = arg max\nπ∈Π\n⟨E[Zk] −α\np\nV[Zk], π⟩\nZk+1 = (T πk+1)m Zk\n.\nB. Further Experimental Details\nAll experiements were run on a cluster containing an Intel Xeon CPU Gold 6230, 20 cores and every single experiments\nwas performed on a single CPU between 3 and 6 hours for continuous control and less than 1 hour for discrete control\nenvironment.\nPre-trained models will be available for all algorithms and environments on a GitHub link.\nThe Mujoco OpenAI Gym tasks licensing information is given at https://github.com/openai/gym/blob/master/LICENSE.md.\nBaseline implementation of PPO, SAC, TQC and QRDQN can be found in Rafﬁn et al. (2019). Moreover, hyperparameters\nacross all experiments used are displayed in Table 2, 1 and 3 .\nB.1. Results for continuous action spaces\nTuning of α must be chosen carefully, for example, α is chosen in {0, 1, ..., 5} for Hopper-v3 and Walker2d-v3 whereas\nvalues of α are chosen smaller in {0, 0.1, 0.5.1, 1.5, 2} and not in a bigger interval. As a rule of thumb for choosing α,\nwe can look at the empirical mean and variance at the end of the trajectories to see if the environment has rewards that\nﬂuctuate a lot. The smaller the mean/variance ratio, the more likely we are to penalise our environment. For HalfCheetah-v3,\nthe mean/variance ratio is about approximately 100, so we will favour smaller penalties than for Walker2d where the\nmean/variance ratio is about 50 or 10 for Hopper-v3.\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRelative mass\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nMean Reward\nSAC\n=0\n=1\n=2\n=3\n=4\n=5\n(a) Hopper\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRelative mass\n0\n1000\n2000\n3000\n4000\n5000\n6000\nMean Reward\nSAC\n=0\n=1\n=2\n=3\n=4\n=5\n(b) Walker2d\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRelative mass\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nMean Reward\nSAC\n=0\n=0.1\n=0.5\n=1\n=1.5\n=2\n(c) HalfCheetah\nFigure 2: Mean results on 20 trajectories varying relative mass.\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRelative mass\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Reward\nMean over 20 trajectories\nSAC\n=0\n=1\n=2\n=3\n=4\n=5\n(a) Hopper\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRelative mass\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Reward\nMean over 20 trajectories\nSAC\n=0\n=1\n=2\n=3\n=4\n=5\n(b) Walker2d\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRelative mass\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Reward\nSAC\n=0\n=0.1\n=0.5\n=1\n=1.5\n=2\n(c) HalfCheetah\nFigure 3: Normalised mean results on 20 trajectories varying relative mass.\nSubmission and Formatting Instructions for ICML 2022\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\nNumber of steps\n1e6\n500\n1000\n1500\n2000\n2500\n3000\n3500\nMean Reward\nMean Reward of 20 trajectories ± standard deviation\nSAC\n=0\n=1\n=2\n=3\n=4\n=5\n(a) Hopper\n1\n2\n3\n4\n5\nNumber of steps\n1e6\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nMean Reward\nMean Reward of 20 trajectories ± standard deviation\nSAC\n=0\n=0.1\n=0.5\n=1\n=1.5\n=2\n(b) Walker2d\n1\n2\n3\n4\n5\nNumber of steps\n1e6\n0\n1000\n2000\n3000\n4000\n5000\n6000\nMean Reward\nMean Reward of 20 trajectories ± standard deviation\nSAC\n=0\n=1\n=2\n=3\n=4\n=5\n(c) HalfCheetah\nFigure 4: Mean performances over 20 trajectories ± variance.\nB.2. Results on discrete action spaces\nWe test our algorithm QRDQN with standard deviation penalization on discrete action space, varying the length of the pole\nin Cartpole-v1 and Acrobot-v1 environments. We observe similar results for the discrete environment in terms of robustness.\nTraining is done for a length of the pole equal to the x-axis of the black star on the graph, then for testing, the length of\nthe pole is increased or decreased. We show that robustness is increased when we penalised our distributional critic. We\nhave compared our algorithm to PPO which has shown relatively good results in terms of robustness for discrete action\nspace in (Abdullah et al., 2019) as SAC does not apply to discrete action space. The same phenomenon is observed in terms\nof robustness as for the continuous environments. However, the more surprising observations on Hopper and Walker2d\nwith an improvement in performance on average is to be qualiﬁed. This is partly explained by the fact that the maximum\nreward is reached in Cartpole and Acrobot quickly. An ablation study can be found in annex C where we study the impact of\npenalization on our behavior policy during testing and on the policy used during learning. It is shown that both are needed in\nthe algorithm.\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nlength of the pole\n100\n200\n300\n400\n500\nMean Reward\nPPO\n=0\n=1\n=3\n=5\n=7\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nlength of the pole\n350\n300\n250\n200\n150\n100\n50\nMean Reward\nPPO \n=0\n=1\n=2\n=3\nFigure 5: Mean over 20 trajectories varying length of the pole trained on the x-axis of the black star for Cartpole-v1 and\nAcrobot-v1 environments\nB.3. Ablation study\nThe purpose of this ablation study is to look at the inﬂuence of penalization in the discrete action space with QRDQN. In the\nﬁgures below, we look at the inﬂuence of penalizing only during training, which will have the effect of choosing less risky\nactions during training in order to increase robustness. This curve is denoted Train penalized.\nThen we look at the inﬂuence of penalizing only once the policy has been learned using classic QRDQN without penalization.\nOnly mean-var actions are selected here during testing and not during training. This experience is denoted Train Penalization.\nFinally, we compare its variants with our algorithm called Full penalization. The results of the ablation are: to achieve\noptimal performance, both phases are necessary.\nWhen penalties are applied only during training. Good performance is obtained in general close to the length 1 where we\ntrain our algorithm. However, the performance is difﬁcult to generalize when the pole length is increased as we do not\nSubmission and Formatting Instructions for ICML 2022\npenalize during testing.\nWhen we penalize only during testing: even if the performances deteriorate, we see that it tends to add robustness because\nthe curves have less tendency to decrease when we increase the length of the pole. The performances are not very high as\nwe play different acts than those taken during the learning.\nSo both phases are therefore necessary for our algorithm. Penalizing during training allows for safer exploration and\npenalizing during testing allows for better generalization.\nThe ablation study for the continuous case is more difﬁcult to do. Indeed, the fact that the penalty occurs only in the gradient\ndescent phase makes it difﬁcult to penalize only in the test phase.\n0\n1\n2\n3\n4\n5\n6\n7\n8\nRelative mass\n100\n200\n300\n400\n500\nMean Reward\nFull penalization\nTest penalization\nTrain penalization\n(a) α = 1\n0\n1\n2\n3\n4\n5\n6\n7\n8\nRelative mass\n100\n200\n300\n400\n500\nMean Reward\nFull penalization\nTest penalization\nTrain penalization\n(b) α = 3\n0\n1\n2\n3\n4\n5\n6\n7\n8\nRelative mass\n100\n200\n300\n400\n500\nMean Reward\nFull penalization\nTest penalization\nTrain penalization\n(a) α = 1\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nRelative mass\n100\n200\n300\n400\n500\nMean Reward\nFull penalization\nTest penalization\nTrain penalization\n(b) α = 3\nFigure 6: Ablation study for different values of α\nSubmission and Formatting Instructions for ICML 2022\nC. Hyperparameters\nFor HalfCheetah-v3 , penalisation is chosen in [0, 2] and not [0, 5] like in Walker-v3 and Hopper-v3.\nTable 1: Table of best hyperparameter for Cartpole-v1\nHyperparameter\nQRDQN with standard deviation penalisation\nPPO\nLearning Rate\n2.3e-3\n3e-4\nOptimizer\nAdam\nAdam\nReplay Buffer Size\n10e5\nN/A\nNumber of Quantiles\n10\nN/A\nHuber parameter κ\n1\nN/A\nPenalisation α\n{0,1,3,5,7 }\nN/A\nNetwork Hidden Layers for Policy\nN/A\n256:256\nNetwork Hidden Layers for Critic\n256:256\n256:256\nNumber of samples per Minibatch\n64\n256\nDiscount factor γ\n0.99\n0.99\nTarget smoothing coefﬁcient β\n.0.005\nN/A\nNon-linearity\nReLu\nReLu\nTarget update interval\n10\nN/A\nGradient steps per iteration\n1\n1\nEntropy coefﬁcient\nN/A\n0\nGAE λ\n0.95\n0.8\nTable 2: Table of best hyperparameter for Acrobot-v1\nHyperparameter\nQRDQN with standard deviation penalisation\nPPO\nLearning Rate\n6.3e-4\n3e-4\nOptimizer\nAdam\nAdam\nReplay Buffer Size\n50 000\nN/A\nNumber of Quantiles\n25\nN/A\nHuber parameter κ\n1\nN/A\nPenalisation α\n{0, 0.5, 1, 2, 3}\nN/A\nNetwork Hidden Layers for Critic\n256:256\n256:256\nNetwork Hidden Layers for Policy\nN/A\n256:256\nNumber of samples per Minibatch\n128\n64\nDiscount factor γ\n0.99\n0.99\nTarget smoothing coefﬁcient β\n.0.005\nN/A\nNon-linearity\nReLu\nReLu\nTarget update interval\n250\nN/A\nGradient steps per iteration\n4\n1\nEntropy coefﬁcient\nN/A\n0\nGAE λ\n0.95\n0.95\nSubmission and Formatting Instructions for ICML 2022\nTable 3: Table of best hyperparameter for all continuous environments\nHyperparameter\nTQC with standard deviation penalisation\nSAC\nLearning Rate\nlinear decay from 7.3e-4\nlinear decay from 7.3e-4\nOptimizer\nAdam\nAdam\nReplay Buffer Size\n106\n106\nExpected Entropy Target\n−dimA\n−dimA\nNumber of Quantiles\n25\nN/A\nHuber parameter κ\n1\nN/A\nPenalisation α\n{0, 1, ...5}\nN/A\nNetwork Hidden Layers for Policy\n256:256\n256:256\nNetwork Hidden Layers for Critic\n512:512:512\n256:256\nNumber of dropped atoms\n2\nN/A\nNumber of samples per Minibatch\n256\n256\nDiscount factor γ\n0.99\n0.99\nTarget smoothing coefﬁcient β\n.0.005\n0.005\nNon-linearity\nReLu\nReLu\nTarget update interval\n1\n1\nGradient steps per iteration\n1\n1\n",
  "categories": [
    "cs.LG",
    "math.OC",
    "stat.ML"
  ],
  "published": "2022-06-14",
  "updated": "2022-06-14"
}