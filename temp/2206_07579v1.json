{
  "id": "http://arxiv.org/abs/2206.07579v1",
  "title": "A Comprehensive Survey on Deep Clustering: Taxonomy, Challenges, and Future Directions",
  "authors": [
    "Sheng Zhou",
    "Hongjia Xu",
    "Zhuonan Zheng",
    "Jiawei Chen",
    "Zhao li",
    "Jiajun Bu",
    "Jia Wu",
    "Xin Wang",
    "Wenwu Zhu",
    "Martin Ester"
  ],
  "abstract": "Clustering is a fundamental machine learning task which has been widely\nstudied in the literature. Classic clustering methods follow the assumption\nthat data are represented as features in a vectorized form through various\nrepresentation learning techniques. As the data become increasingly complicated\nand complex, the shallow (traditional) clustering methods can no longer handle\nthe high-dimensional data type. With the huge success of deep learning,\nespecially the deep unsupervised learning, many representation learning\ntechniques with deep architectures have been proposed in the past decade.\nRecently, the concept of Deep Clustering, i.e., jointly optimizing the\nrepresentation learning and clustering, has been proposed and hence attracted\ngrowing attention in the community. Motivated by the tremendous success of deep\nlearning in clustering, one of the most fundamental machine learning tasks, and\nthe large number of recent advances in this direction, in this paper we conduct\na comprehensive survey on deep clustering by proposing a new taxonomy of\ndifferent state-of-the-art approaches. We summarize the essential components of\ndeep clustering and categorize existing methods by the ways they design\ninteractions between deep representation learning and clustering. Moreover,\nthis survey also provides the popular benchmark datasets, evaluation metrics\nand open-source implementations to clearly illustrate various experimental\nsettings. Last but not least, we discuss the practical applications of deep\nclustering and suggest challenging topics deserving further investigations as\nfuture directions.",
  "text": "arXiv:2206.07579v1  [cs.LG]  15 Jun 2022\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nSHENG ZHOU, HONGJIA XU, ZHUONAN ZHENG, JIAWEI CHEN, ZHAO LI, JIAJUN\nBU, Zhejiang University, China\nJIA WU, Macquarie University, Australia\nXIN WANG, WENWU ZHU, Tsinghua University, China\nMARTIN ESTER, Simon Fraser University, Canada\nClustering is a fundamental machine learning task which has been widely studied in the literature. Classic\nclustering methods follow the assumption that data are represented as features in a vectorized form through\nvarious representation learning techniques. As the data become increasingly complicated and complex, the\nshallow (traditional) clustering methods can no longer handle the high-dimensional data type. With the huge\nsuccess of deep learning, especially the deep unsupervised learning, many representation learning techniques\nwith deep architectures have been proposed in the past decade. One straightforward way to incorporate the\nbeneﬁt of deep learning is to ﬁrst learn the deep representation before feeding it into shallow clustering meth-\nods. However, this is suboptimal due to: i) the representation is not directly learned for clustering which limits\nthe clustering performance; ii) the clustering relies on the relationship among instances which is complicated\nrather than linear; iii) the clustering and representation learning is dependent on each other which should\nbe mutually enhanced. To tackle the above challenges, the concept of Deep Clustering, i.e., jointly optimiz-\ning the representation learning and clustering, has been proposed and hence attracted growing attention in\nthe community. Motivated by the tremendous success of deep learning in clustering, one of the most funda-\nmental machine learning tasks, and the large number of recent advances in this direction, in this paper we\nconduct a comprehensive survey on deep clustering by proposing a new taxonomy of diﬀerent state-of-the-\nart approaches. We summarize the essential components of deep clustering and categorize existing methods\nby the ways they design interactions between deep representation learning and clustering. Moreover, this\nsurvey also provides the popular benchmark datasets, evaluation metrics and open-source implementations\nto clearly illustrate various experimental settings. Last but not least, we discuss the practical applications of\ndeep clustering and suggest challenging topics deserving further investigations as future directions.\nCCS Concepts: • Theory of computation →Unsupervised learning and clustering.\nAdditional Key Words and Phrases: deep learning, clustering, representation learning\nACM Reference Format:\nSheng Zhou, Hongjia Xu, Zhuonan Zheng, Jiawei Chen, Zhao li, Jiajun Bu, Jia Wu, Xin Wang, Wenwu Zhu,\nand Martin Ester. 2022. A Comprehensive Survey on Deep Clustering: Taxonomy, Challenges, and Future\nDirections. In ,. ACM, New York, NY, USA, 35 pages. https://doi.org/\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and\nthe full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org.\nACM Computing Survey, Manuscript\n© 2022 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\nhttps://doi.org/\n1\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\n1\nINTRODUCTION\nClustering is a fundamental problem in machine learning and frequently serves as an important\npreprocessing step in many data mining tasks. The primary purpose of clustering is to assign\nthe instances into groups so that the similar samples belong to the same cluster while dissimilar\nsamples belong to diﬀerent clusters. The clusters of samples provide a global characterization of\ndata instances, which can signiﬁcantly beneﬁt the further analysis on the whole dataset, such as\nanomaly detection [166, 201], domain adaptation [180, 240], community detection [121, 178] and\ndiscriminative representation learning [133, 164, 214], etc.\nAlthough shallow clustering methods have achieved tremendous success, they assume that in-\nstances are already represented in a latent vectorized space with a good shape. With the rapid\ndevelopment of internet and web services in the past decades, the research community is showing\nan increasing interest in discovering new machine learning models capable of handling unstruc-\ntured data without explicit features, such as images, and high-dimensional data with thousands of\ndimensions, etc. As such, shallow clustering methods can no longer be directly applied to deal with\nsuch data. Recent years have witnessed the success of representation learning with deep learning,\nespecially in the unstructured and high-dimensional data [166, 201]. However, the deep learning\ntechnique was not explored in the clustering process. The complex relations among instances can\nnot be well captured, which results in sub-optimal clustering results.\nTo address the issues, Deep Clustering, which aims at joint optimization of deep representation\nlearning and clustering, arises and has attracted increasing attention recently in the community.\nMore speciﬁcally, the deep clustering methods focus on the following research challenges:\n(1) How to learn discriminative representations that can yield better clustering performance?\n(2) How to eﬃciently conduct clustering and representation learning in a uniﬁed framework?\n(3) How to break down the barriers between clustering and representation learning, enabling\nthem to enhance each other in an interactive and iterative manner?\nTo tackle the above challenges, numerous deep clustering methods have been proposed with vari-\nant deep architectures and data types. Motivated by the tremendous success of deep learning in\nclustering, one of the most fundamental machine learning tasks, and the large number of recent\nadvances in this direction, in this paper we conduct a comprehensive survey on deep clustering\nby proposing a new taxonomy of various state-of-the-art approaches.\nRelated Surveys Overview papers on clustering tend to have close relations to our work, how-\never, we clearly state their distinctions from our survey as follows. There have been surveys on\nclassic clustering [56, 208], particularly summarizing classic clustering methods into the following\nmain groups: i) partition based methods [125, 152] that assign the data instances to the closest clus-\nter centers, ii) hierarchy based methods [66, 96, 231] that construct the hierarchical relationships\namong data instances, iii) density based methods [20, 37, 50, 51, 167] that take the instances in\nthe region with high density as a cluster, and iv) generative methods [162, 210] that assume data\nbelonging to the same cluster are generated from the same distribution. Generally, these meth-\nods take the feature representations as input and output the cluster assignment of each instance.\nTo the best of our knowledge, very few works have been proposed to survey the deep cluster-\ning approaches. Min et al. [135] in their survey paper states that the essence of deep clustering\nis to learn clustering-oriented representations, so the literature should be classiﬁed according to\nnetwork architecture. However, we discover that the importance of clustering constraint and the\ninteraction between representation and clustering play central roles in modern deep clustering.\nNutakki et al. [146] review some basic methods, excluding the most recent advanced techniques\nfor representation learning and clustering. Aljalbout et al. [3] design the taxonomy mainly based\non the DNN architecture and the loss function, discussing the comparisons among methods only\n2\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\nImage\nText\nVideo\nGraph\nData\n!Sec 6\"\nAuto-Encoder\nCornerstone\nTaxonomy\n!Sec 5\"\nGenerative\nMutual Information Maximization\nContarstive\nClustering Friendly\nRepresentation Learning Module!Sec 3\" \nRelation Matching\nPseudo Labeling\nSelf-training\nContrastive\nMutual Information Maximization\nClustering Module!Sec 4\"\nRepresentation\nLearning Module \nClustering\nModule\nApplication\n(Sec 7)\nSimultaneously\nMulti-stage\nCommunity\nDetection\nAnomaly\nDetection\nSegmentation and \nObject Detection\nGenerative\nRepresentation\nLearning Module \nClustering\nModule\nRepresentation\nLearning Module \nClustering\nModule\nIterative\nRepresentation\nLearning Module \nClustering\nModule\nMedical \nApplications\nFig. 1. Overall organization of this survey.\non the MNIST and COIL20 datasets. Furthermore, all these works [3, 135, 146] merely cover meth-\nods prior to the year of 2018, missing more recent advances in the past four years. Therefore, we\nstrongly believe that this survey with a new taxonomy of literature and more than 200 scientiﬁc\nworks will strengthen the focus and promote the future research on deep clustering.\nContributions. In summary, this paper aims at supporting the potential readers understanding\nthe panorama of deep clustering with respect to the following aspects:\n• Cornerstones of Deep Clustering. We summarize two cornerstones of deep clustering,\nnamely the representation learning module and the clustering module. For each module,\nwe highlight the representative and universal designs summarized from existing methods,\nwhich can be easily generalized to new models.\n• Systematic Taxonomy. We propose a systematic taxonomy of existing deep clustering\nmethods based on the ways of interactions between representation learning module and clus-\ntering module through providing four representative branches of methods. We also compare\nand analyse the properties of each branch within diﬀerent scenarios.\n• Abundant Resources and References. We collect various types of benchmark datasets,\nevaluation metrics and open-source implementations of latest publications on deep cluster-\ning, which are organized together with references on the Github (1.8K Star)1.\n• Future Directions. Based on the properties of representation learning module and cluster-\ning module as well as their interactions, we discuss the limitations and challenges of existing\nmethods, followed by our insights and thoughts on promising research directions deserving\nfuture investigations.\nScope. In this survey, we focus on the clustering with deep learning techniques, especially the\ninteractions between deep representation learning and clustering with deep neural networks. For\n1https://github.com/zhoushengisnoob/DeepClustering\n3\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\nthe other fundamental research problems such as initialization of clustering, automatic identifying\nnumber of clusters, etc., we provide discussions in Section 8 and leave them in the future work. The\ncomparisons between surveys on shallow clustering, deep clustering and representation learning\ncan be found in Table 1.\nTable 1. Comparison with related surveys.\nComparisons\nReferences\nDeep Clustering\nShallow Clustering\nUnsupervised Learning\nOurs [135]\n[3]\n[146] [98] [208] [209] [84]\n[12]\n[1]\n[109] [11]\n[95] [124]\nDeep\nrepresentation\nlearning design\nAuto Encoder\n✓\n✓\n✓\n✓\n✓\n-\n-\n-\n-\n✓\n-\n✓\n✓\n✓\nGenerative\n✓\n✓\n✓\n-\n✓\n-\n-\n-\n-\n✓\n✓\n✓\n✓\n✓\nContrastive\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\n✓\n✓\nCluster Based\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n✓\n-\n✓\n✓\nClustering with DNN\n✓\n✓\n✓\n✓\n✓\n-\n✓\n✓\n✓\n-\n-\n-\n-\n-\nInteraction\nbetween\nRepresentation\nand Clustering\nMultistage\n✓\n-\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n-\n-\n-\n-\nIterative\n✓\n-\n✓\n✓\n-\n-\n✓\n✓\n-\n-\n✓\n-\n✓\n✓\nGenerative\n✓\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\nSimultaneously\n✓\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nApplication\n✓\n-\n-\n-\n-\n-\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n-\nDataset\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\nEvaluation\n✓\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\nImplementation\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n* Each column indicates a survey paper being compared, ‘✓’ means the term of corresponding row has been surveyed or\nanalysed in this paper while ‘-’ means not.\nOrganization. The rest of this survey is organized as follows: Section 2 introduces the basic\ndeﬁnitions and notations used in this paper. Section 3 summarizes the representative design of\nrepresentation module, along with diﬀerent data types. Section 4 summarizes the representative\ndesign of clustering module, which mainly focuses on the basic modules deﬁned in the deep clus-\ntering methods. Section 5 summarizes the representative ways of interactions between the two\nmodules, which covers most existing literature. Section 6 introduces the widely used benchmark\ndatasets and evaluation metrics. Section 7 discusses the applications of deep clustering. Section\n8 discusses limitations, challenges, and suggests future research directions that deserve further\nexplorations. The overall organization of this survey is illustrated in Figure 1.\n2\nPRELIMINARY\nIn this section, we ﬁrst brieﬂy introduce some deﬁnitions in deep clustering that need to be clariﬁed,\nthen we illustrate the notations used in this paper in Table 2.\nDeep Clustering and Shallow (Non-deep) Clustering. Given a set of data instances X =\n{푥푖}푁\n푖=1, clustering aims to automatically assign each instance 푥into groups so that instances in\nthe same group are similar while instances from diﬀerent groups are dissimilar. The shallow (non-\ndeep) clustering takes the feature vectors of instances as input and outputs the clustering result\nwithout deep neural networks. The deep clustering aims at clustering unstructured data or high-\ndimensional data with deep neural networks. It is worth noting that deep clustering is not narrowly\ndeﬁned as applying deep learning techniques in representation learning, instead, the clustering\nitself can be conducted by deep neural networks and beneﬁts from the interaction with deep rep-\nresentation learning.\nHard Clustering and Soft Clustering. The clustering methods can be categorized into hard\nclustering and soft clustering according to the output type. The output of hard clustering is a dis-\ncrete one-hot cluster label ˜푦푖for each instance 푥푖, while the output of soft clustering is a continuous\ncluster assignment probabilistic vector푧푖∈R퐾. The discrete assignment of instance is usually hard\n4\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\nTable 2. Important notations used in this paper\nNotations\nExplanations\nNotations\nExplanations\n푁\nThe number of data instances\n퐾\nThe number of clusters\n푥\nData instance\nˆ푥\nData reconstruction\n푥T\nAugmented instance\n·푇\nTranspose of a matrix(vector)\nℎ\nRepresentation\n푧\nSoft assignment\n˜푦\nPredicted hard label\n푦\nGround truth label\n푐\nRepresentation(centroid) of cluster\n휏\nTemperature parameter\nP, Q\nProbability distribution\nX = {푥푖}푁\n푖=1\nData instances set\n∥· ∥퐹\nFrobenius norm of a matrix(vector)\n| · |\nThe number of elements in a set\nto optimize, especially for deep neural networks with backpropagation. As a result, most existing\ndeep clustering methods belong to the soft clustering category where the clustering results are\nproduced by a deep neural network 푓whose outputs are softmax activated 퐾-dimension logits.\nFor the ﬁnal evaluation purpose and discrete clustering category, the hard label can be obtained\nby selecting the dimension with maximum probability.\nPartitioning Clustering and Overlapping Clustering. The above-deﬁned hard clustering\nby maximizing the probability vector is designed for the partitioning clustering task where each\ndata instance belongs to only one cluster. This means the clustering method participates the whole\ndataset into 퐾disjoint groups. The majority of existing deep clustering methods are disjoint since\nthe major evaluated datasets are disjoint ones. For the overlapping clustering setting, each data\ninstance may belong to more than one cluster. This has brought extra diﬃculty to the clustering\nmethods which will be discussed in section 8.\n3\nREPRESENTATION LEARNING MODULE\nRecent decades have witnessed the rapid development of deep representation learning [11, 188,\n238], especially the unsupervised ones. Intuitively, all unsupervised representation learning meth-\nods can serve as an input generator and be directly incorporated into deep clustering framework\n(further discussion on this issue in section 4). However, most existing methods are not intently\ndesigned for clustering task, and unable to integrate the potential clustering information to learn\nbetter representations. In this section, we introduce the representation learning module in deep clus-\ntering, which takes the raw data as input and outputs the low-dimensional representation (a.k.a.\nembedding). Figure 2 illustrates the representative representation learning modules described in\nthis section.\n3.1\nAuto-Encoder based Representation Learning.\nAuto-Encoder [165] is one of the most widely adopted unsupervised deep representation learn-\ning methods for its simplicity and eﬀectiveness. The auto-encoder is a linear stack of two deep\nneural networks named encoder and decoder. The encoder f푒encodes the input data 푥into low-\ndimensional representation ℎ= f푒(푥), and the decoder f푑decodes the low-dimensional representa-\ntion ℎto the input data space ˆ푥= f푑(ℎ). A good auto-encoder is expected to reconstruct the input\ndata without dropping any information, thus the optimization target of auto-encoder is usually\nformulated as:\nL퐴퐸=\n푁\nÕ\n푖\n∥푥푖−ˆ푥푖∥2\n2 =\n푁\nÕ\n푖\n∥푥푖−f푑(f푒(푥))∥2\n2\n(1)\n5\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\n!\nImage\n!\nFeature\nMultiMedia\nGraph\n!\n!\n!\n!\n!\n!\n…\nReconstruction\nGenerative\nMutual Information\nContrast\nRaw Data\nRepresentation Learning\nRepresentation\nhi\nhi\nhi\nhi\nxi\nxi xi\nxi\nˆxi\nhT\ni\nxT\ni\nFig. 2. Representative representation learning modules\nwhere ∥·∥2 is the L2-norm, ˆ푥푖is the reconstructed data. The auto-encoder is a general structure\nand can be customized for diﬀerent data types. For example, deep neural networks for vectorized\nfeatures [220], convolutional networks for images and graph neural networks for graphs [150], 3D\nconvolutional networks and LSTM auto-encoder for videos [176] etc..\nAnalysis. The auto-encoder based representation learning framework enjoys the property of\neasy implementation and eﬃcient training, and has been widely adopted in the early works of deep\nclustering. However, the representations are learned in an instance-wise manner while the rela-\ntionships among diﬀerent instances are largely ignored. As a result, the instances in the embedding\nspace may not be well discriminated from each other, resulting in poor clustering performance.\n3.2\nDeep Generative Representation Learning.\nAnother line of deep unsupervised representation learning lies in the generative model. Generative\nmethods assume that the data 푥are generated from latent representation ℎand then reversely\ndeduce the posterior of the representation 푝(ℎ|푥) from the data. Among these, the most typical\nmethod is variational auto-encoder (VAE) [102]. VAE resorts to the variance inference technique\nand maximizes the evidence lower bound (ELBO) on the data likelihood:\nlog 푝(푥) ≥E푞(ℎ|푥) [log 푝(푥|ℎ)] −퐷퐾퐿(푞(ℎ|푥)∥푝(ℎ))\n(2)\nwhere 퐷퐾퐿(·∥·) denotes the KL-divergence between two distributions, 푝(ℎ) is the prior distribution\nof the latent representation, 푞(ℎ|푥;휑) is the variational posterior of the representation to approx-\nimate the true posterior (i.e., 푞(ℎ|푥;휑) ≈푝(ℎ|푥)), which can be modelled with the recognition\nnetworks 휑. By utilizing the reparameterization trick [102] and Monte Carlo approximation [97],\nthe posterior can be eﬃciently learned from the equation (2) via backpropagation.\nAnalysis. Deep generative models enjoy some merits such as being ﬂexible, explainable and\ncapable to recreate data points. It would be promising to transform the generative representation\nmodels for deep clustering task so that the clustering models could inherit these advantages.\n6\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\n3.3\nMutual Information Maximization Representation Learning.\nMutual information (MI) [103] is a fundamental quantity for measuring the dependence between\nrandom variables 푋and 푌, which is formulated as:\nI(푋;푌) =\n∫\nlog\n푑P푋푌\n푑P푋⊗P푌\n푑P푋푌\n(3)\nwhere P푋푌is the joint distribution, P푋=\n∫\n푌푑P푋푌and P푌=\n∫\n푋푑P푋푌are the marginal distribution,\nP푋⊗P푌is the product of the marginal distributions. Traditional mutual information estimations\n[106] are only tractable for discrete variables or known probability distributions. Recently, MINE\n[9] is proposed to estimate the mutual information with deep neural networks. The widely used\nmutual information estimation is the Jensen-Shannon divergence (JSD) [143] formulated as:\nI퐽푆퐷(푋;퐻) = EP푋퐻[−sp(−퐷(푥,ℎ))] −EP푋×P퐻[sp(퐷(푥,ℎ))]\n(4)\nwhere sp(푥) = log (1 + 푒푥) is the softplus function. 퐷is a discrimintor function modeled by a\nneural network. Another popular mutual information estimation is InfoNCE [148] which will be\nintroduced in subsection 3.4. Beneﬁting from the neural estimation, the mutual information has\nbeen widely adopted in unsupervised representation learning [7, 75]. More speciﬁcally, the repre-\nsentation is learned by maximizing the mutual information between diﬀerent layers [7] or diﬀerent\nparts of the data instances [75], so that the consistency of representation can be guaranteed. This\ncan be viewed as an early attempt at self-supervised learning which has an extensive impact on\nthe later works.\nAnalysis. As a fundamental measure of correlation and dependence, the mutual information\nhas several advantages. The major advantage taken by the deep clustering task is that variables\nmeasured by mutual information are not restricted to same dimension and semantic space, such\nas instances and clusters. The detailed applications will be introduced in subsection 4.4 and 5.4.2.\nSimilar to auto-encoder based and deep generative representation learning, the objective of mutual\ninformation maximization methods is also instance-wise, which may also have the aforementioned\nproblems in capturing the relationships among instances. However, the marginal distribution in\nmutual information estimation depends on all the observed samples. In other words, the relation-\nships among instances are captured in an implicit way, which has also boosted the performance\nin deep clustering.\n3.4\nContrastive Representation Learning\nContrastive learning is one of the most popular unsupervised representation learning techniques\nin the past few years. The basic idea is to pull positive pair of instances close while push negative\npair of instances far away, which is also known as instance discrimination. The representative\ntarget of contrastive learning is the InfoNCE loss [148] formulated as:\nL퐼푛푓표푁퐶퐸= −log\n푁\nÕ\n푖=1\nexp \u0000푓\u0000ℎ푖,ℎT\n푖\n\u0001 /휏\u0001\nÍ푁\n푗=1 exp\n\u0010\n푓\n\u0010\nℎ푖,ℎT\n푗\n\u0011\n/휏\n\u0011\n(5)\nwhere ℎ푖is the representation of the anchor sample, ℎT\n푖is the positive sample representation and\nℎ푗, ℎT\n푗are the negative sample representations, 푓is a similarity function, 휏is the temperature\nparameter [74]. The positive samples are usually conducted by data augmentation which may\nvary from diﬀerent data types. For example, the ﬂip, rotate and crop augmentation for image data\n[30], the node dropping, edge perturbation, attribute masking and subgraph sampling for graph\ndata [113, 217]. The negative samples are selected from augmented view of other instances in the\n7\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\ndataset [30] or a momentum updated memory bank of old negative representations [72], which\ncan be viewed as approximation of noise.\nAnalysis. There has been several theoretical analyses on contrastive learning, and substantial\nevidences have shown that the representation learned by contrastive learning can beneﬁt the clus-\ntering task. In [196], contrastive learning is explained with two properties: alignment of features\nfrom positive pairs and uniformity of the feature distribution on the hypersphere. The alignment\nproperty encourages the samples with similar features or semantic categories to be close in the\nlow-dimensional space, which is essential for clustering. Such discriminative power has also been\nproved in the supervised manner [101].\nThe former work [148] has proved that minimizing the InfoNCE loss is equivalent to maximizing\nthe lower bound of mutual information, while the data augmentation is not considered in the\nmutual information maximization. Recent study [199] has shown that the augmentation has played\nthe role of ‘ladder’ in connecting instances within same category. As a result, instances from same\ncluster may be pulled closer which can beneﬁt the clustering in the discriminative space.\n3.5\nClustering Friendly Representation Learning.\nAlthough the aforementioned representation learning methods have somehow implicitly boosted\nthe performanceof clustering, they are not explicitly designed for the clustering task. In this subsec-\ntion, we will introduce some representation learning methods that explicitly support the clustering\ntask.\nK-means [125] friendly representation is ﬁrst deﬁned in DCN [212] where samples in the low-\ndimensional space are expected to scatter around their corresponding cluster centroids. This can\nwell support the assumption of K-means that each sample is assigned to the cluster with the min-\nimum distance to the centroid. The objective L퐾퐹can be formulated as:\nL퐾퐹=\n푁\nÕ\n푖=1\n∥f (푥푖) −˜푦푖C∥2\n2\n(6)\nwhere f(·) is the deep neural network for representation learning, ˜푦푖is the hard assignment vector\nof data instance 푥푖which has only one non-zero elements, C is the cluster representation matrix\nwhere 푘-th column of C, i.e., 푐푘denotes the centroid of the 푘-th cluster.\nSpectral clustering friendly representation learning is inspired by the eigen decomposition in\nspectral clustering [140] that projects instances into the space with orthogonal bases. In deep clus-\ntering, the orthogonal basis is modeled explicitly by reducing correlations within features [181],\nwhich can be formulated as:\nL푆퐹=\n푑\nÕ\n푚=1\n \n−ℎ푇\n푚ℎ푚/휏+ log\n푑\nÕ\n푛\nexp\n\u0010\nℎ푇\n푛ℎ푚/휏\n\u0011!\n(7)\nwhere 푑is the number of feature dimensions, ℎ푚is the 푚-th dimension feature vector, 휏is the\ntemperature parameter. The objective is to learn the independent features so that redundant infor-\nmation is reduced.\nAnalysis. The clustering friendly representation learning beneﬁts from the direct optimization\nfor clustering, which may signiﬁcantly boost the corresponding clustering performance. How-\never, such simplicity also limits the generalization to other clustering methods. Currently, the\nresearch community has put more eﬀorts on the inspirations of clustering methods and express\nthem in a deep learning perspective, rather than learning speciﬁc representations for each cluster-\ning method.\n8\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\n3.6\nSubspace Representation Learning\nSubspace representation learning is the early stage of subspace clustering [192], which aims at\nmapping the data instances into a low-dimensional subspace where instances can be separated. Ba-\nsically, current subspace representation learning methods [88, 157, 225, 229, 230, 239] have relied\non the self-expressiveness assumption where a data instance can be expressed as a linear combi-\nnation of other data instances from the same subspace, i.e., X = XΘc, where X is the data matrix\nand Θc is the self-expression coeﬃcient matrix. For representation learning, the self-expression\nproperty leads to the following objective:\nmin\nΘc\n∥Θc∥푝+ 휆\n2 ∥H −HΘc∥2\n퐹\ns.t.\ndiag(Θc) = 0\n(8)\nwhere ∥· ∥푝is a matrix norm, 휆controls the weight balance. H denotes the sample representations\nlearned by a network. Further, Θc can be implemented as the parameters of an additional network\nlayer [88].\nAnalysis. The success of subspace representation learning relies on the theoretical assurance,\nwhich provides explicit modeling of relationships among data instances. However, it suﬀers from\nthe limitation of solving the coeﬃcient matrix of size 푁×푁, which is computationally diﬃcult for\nlarge-scale data.\n3.7\nData type specific representation learning\nThe former subsections have summarized the universal architectures of representation learning\nmodule. In real-world scenarios, representation learning for diﬀerent data types can be viewed as\nthe variants of the aforementioned architectures. In this subsection, we summarize four widely\nstudied data types and their corresponding representation learning methods in deep clustering.\n3.7.1\nImage Representation Learning. Learning representations of images using CNN [110] and\nResNet [73] as backbone has achieved great success in the past decades. In the image deep clus-\ntering, they still play active roles as feature extractors or backbones in representation learning\nmodule. Beyond the above two methods, recent advances have been made by introducing mod-\nern representation learning techniques such as vision transformer [47] to deep clustering. As one\nof most popular directions, the unsupervised representation learning for image data will play a\ncentral role in deep clustering and aﬀect the other data types.\n3.7.2\nText Representation Learning. The early attempts of text representation learning have uti-\nlized the statistical based methods like TF-IDF [177], Word2Vec[108] and Skip-Gram [134]. Later,\nsome works focus on the topic modeling [76] and semantic distances [48, 158] for text representa-\ntion learning, and more [34] on unsupervised scenarios. Recently, the pre-trained language models\nlike BERT [43] and GPT-3 [18] are gradually dominating the area of text representation learning.\nHowever, the ﬁne tuning [224] of these methods in deep clustering task is still an open question.\n3.7.3\nVideo Representation Learning. The video representation learning is a challenging task which\ncombines the spatial-temporal learning, multi-model learning (with audio) [4], and natural lan-\nguage processing (with video abstract and subtitles) into one place. The early methods utilize\nLSTM Autoencoder [176], 3D-ResNets [160] and 3D-U-Net [155] as feature extractor. The recent\nmethods have focused on spatial-temporal modeling [53, 176, 227, 228] and Qian et al. [160] in\nparticular incorporates contrastive learning for self-supervision.\n3.7.4\nGraph Representation Learning. The classic graph representation learning aims at learning\nlow dimensional representation for nodes so that the proximity among nodes can be preserved in\nthe embedding space. Graph Neural Networks (GNNs) [194, 234] are widely used including GCN\n9\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\n…\n…\nRepresentation\nClustering\nSoft Assignment\nRelation Matching\ninstance-wise\nrelation-wise\nPseudo Labeling\nSelf-training\nMutual Information \nContrastive\nRs\nRt\nQ\nP\nhi\nzi\nR\nC, M\nhT\ni\nzT\ni\nzi\n˜yi\nx\n˜y\nFig. 3. Representative clustering modules\n[104], GraphSAGE [70] and GAT [191], brings inﬁnite possibility of graph node representation\nlearning combining node features and graph topology[198, 232, 243, 246]. Furthermore, the graph-\nlevel information also has great potential in tasks like proteins classiﬁcation [16], which has drown\nincreasing attention in graph-level representation learning [67, 204, 233, 235].\nAnalysis. The data type speciﬁc representation learning mentioned above can be naive back-\nbone for feature extraction or end-to-end unsupervised representation learning, which are most\nactive research directions in deep learning. With more types of data being collected and fast devel-\nopment of deep learning, we believe that the deep clustering with grow along with the data type\nspeciﬁc representation learning techniques.\n4\nCLUSTERING MODULE\nIn this section, we introduce the representative clustering modules in deep clustering, which takes\nthe low-dimensional representations as input, and outputs the cluster labels for hard clustering\nor cluster assignment probabilities for soft clustering. Although many shallow clustering meth-\nods can be directly employed for clustering, they are hard to be trained with deep representation\nlearning in a uniﬁed framework. More importantly, they can not well interact with representation\nlearning module and mutually enhanced. For more shallow clustering methods, please refer to\nthe former surveys [84, 209]. In deep clustering, a more ‘deep’ way of clustering is as follows: the\nvectorized features are directly fed forward through deep neural networks and reduce the dimen-\nsion to the cluster number 푘, then the softmax function is applied on the last layer so that the\nassignment distribution can be established.\nAlthough the 푘-dimensional representations are in the form of probability distribution, they\nmay not represent the cluster distribution without explicit constraints and intensify the degener-\nate problem where all instances are assigned to the same cluster. In this section, we assume the\nclustering process is conducted by the deep neural networks and focus on the clustering targets.\nFigure 3 illustrates the representative clustering module described in this section.\n4.1\nRelation Matching Deep Clustering\nIn deep clustering, each data instance can be represented in two spaces, namely the 푑-dimensional\nembedding space and 퐾-dimensional label space. The relationships among instances are expected\n10\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\nto be consistent during dimension reduction, which has been utilized to bridge the semantic gap\nbetween representation learning and clustering. Borrowing the idea from the domain adaptation\n[126], relation matching can be realized in bidirectional ways:\nL푅푀=\n푁\nÕ\n푖\n푁\nÕ\n푗\nℓ(푅푠\n푖푗,푅푡\n푖푗)\n(9)\nwhere ℓis a measure of relation matching, e.g., cosine similarity or Euclidean distance, 푅푠and 푅푡\nare the relations in source and target space, which can be either embedding space or label space.\nIt is worth noting that the relations deﬁned here narrowly refers to the continuous ones and we\nintroduce the discrete relations as pseudo labeling in subsection 4.2.\nAnalysis. The relation matching deep clustering explicitly connect the representation learning\nand clustering, which is straightforward and easy to implement. However, calculating 푁2 pairs of\ninstances is computational ineﬃcient. To tackle this challenge, some methods only preserve the\nk-nearest-neighbor relations [39, 190] for each instance or the relations with high conﬁdence [190].\nAlthough this can somehow improve the training eﬃciency, the extra hyper-parameter is hard to\nset in the unsupervised manner. Furthermore, among all pairs of relations, many of them are noisy\nespecially in the early training phases with limited capability. How to ﬁlter out the clean relations\nto boost the performance while discard the noisy relations is still an open research question.\n4.2\nPseudo Labeling Deep Clustering\nThe pseudo labeling can be viewed as another type of relation matching where the relations are\ndiscrete based on the consistency of labels. It has been widely studied in semi-supervised learning\n[111], and recently being introduced to deep clustering. According to the way of utilizing pseudo\nlabels, existing methods can be largely divided into two groups: instance-wise pseudo labeling\n[21, 141, 190] and relation-wise pseudo labeling [28, 142].\nThe instance-wise pseudo labeling ﬁlters out a subset of instances with high conﬁdence and\ntrains the network in a supervised manner with cross-entropy loss:\nL퐼푃퐿= −1\n|X푐|\nX푐\nÕ\n푖\n퐾\nÕ\n푘=1\n˜푦푖푘log (푧푖푘)\n(10)\nwhere L퐼푃퐿denotes the loss of instance-wise pseudo labeling, X푐denotes the subset of instances\nwith high conﬁdence, ˜푦푖푘and 푧푖푘are the predicted hard label and soft cluster assignment. The con-\nﬁdence is usually estimated by entropy or maximum of the assignment probabilistic distribution.\nThe general idea of relation-wise pseudo labeling is to enforce instances with same pseudo\nlabels closer while instances with diﬀerent pseudo labels away from each other in the embedding\nspace. Given the ﬁltered instances, relation-wise pseudo labeling construct the discrete relations\namong instances to guide the representation learning: the must-link for pairs of instances with\nsame pseudo labels and cannot-link for pairs of instances with diﬀerent pseudo labels:\nL푅푃퐿= 1\n|C|\nÕ\n{푖,푘}∈C\n푅푖푘−\n1\n|M|\nÕ\n{푖,푗}∈M\n푅푖푗\n(11)\nwhere M is the set of must-link relations and C is the set of cannot-link relations, 푅푖푗is the\nsimilarity of instance 푥푖and 푥푗in the low-dimensional embedding space.\nAnalysis. The pseudo labeling has brought the power of semi-supervised learning into the\nunsupervised clustering task. However, the performance highly relies on the quality of ﬁltered\npseudo labels which is susceptible to model capability and hyper-parameter tuning, especially in\n11\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\nthe unsupervised manner. The current methods [141, 190] have taken the pre-training as the early\nstage before pseudo labeling to tackle this challenge, but more attention deserved to be paid on it.\n4.3\nSelf-training Deep Clustering\nThe self-training strategy is introduced to the deep clustering task [206] and open up an active\nbranch of methods named self-training deep clustering. More speciﬁcally, the cluster assignment\ndistribution is optimized by minimizing the KL-divergence with an auxiliary distribution:\nL = 퐷퐾퐿(P∥Q) =\n푁\nÕ\n푖\n퐾\nÕ\n푘\n푝푖푘log 푝푖푘\n푞푖푘\n(12)\nwhere Q is the cluster assignment distribution and P is the auxiliary distribution. 푞푖푘and 푝푖푘de-\nnotes the probability of instance 푥푖belong to cluster 푘. The assignment distribution Q follows the\nassumption of K-means and is produced by the embedding distance between instance and cluster\ncentroids:\n푞푖푘=\n\u00001 + ∥ℎ푖−푐푘∥2\n2 /훼\u0001−훼+1\n2\nÍ퐾\n푗\n\u0010\n1 +\n\r\rℎ푖−푐푗\n\r\r2\n2 /훼\n\u0011−훼+1\n2\n(13)\nwhere ℎ푖is the representation of data instance 푥푖and 푐푘is the representation of cluster 푘, 훼is the\nfreedom degree of the Student’s t-distribution [189]. The auxiliary distribution P is a variant of\nassignment distribution Q with both instance-wise and cluster-wise normalization:\n푝푖푘=\n푞2\n푖푘/푓푘\nÍ퐾\n푗푞2\n푖푗/푓푗\n(14)\nwhere 푓푘= Í푁\n푖푞푖푘are soft cluster frequencies.\nAnalysis. The self-training deep clustering has been widely adopted in several existing works\n[60, 68, 69, 112, 156, 164, 206]. The success relies on the following properties: First, the square\nof cluster assignment probability with cluster-wise normalization will guide the model put more\nattention (gradient) on the instances with higher conﬁdence, which in turn reduce the impact of\nlow conﬁdence ones. As a result, the cluster assignment vector tends to be one-hot. Second, the\nsoft cluster frequencies 푓푘can be viewed as the sum of the probability that instance belongs to the\n푘-th cluster. This can prevent the degenerate solution that all instances belong to the same cluster.\nThe above two properties have a profound eﬀect on the later works and many deep clustering\nmethods can be viewed as variants of it.\n4.4\nMutual Information Maximization based Clustering\nAs introduced in subsection 3.3, mutual information maximization has achieved tremendous suc-\ncess in representation learning. Beneﬁting from its unrestriction to the feature dimension and\nsemantic meaning, it has also been introduced to the clustering module for measuring the de-\npendence between instance and cluster assignment. The mutual information maximization based\nclustering can be formulated as:\nI(푋, ˜푌) = 퐷퐾퐿(푝(푥, ˜푦)∥푝(푥)푝(˜푦)) =\nÕ\n푥∈X\nÕ\n˜푦∈Y\n푝(푥, ˜푦) log 푝(푥, ˜푦)\n푝(푥)푝(˜푦) .\n(15)\nwhere 푋is the original data, ˜푌is the predicted labels, 푝(푥, ˜푦) is the joint distribution and 푝(푥),푝(˜푦)\nare the marginal distribution. The estimation of mutual information is same as that in representa-\ntion learning.\n12\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\nAnalysis. The mutual information maximization based clustering and representation learning\nare quite similar despite the dimension of optimization object. The major advantage is that it\novercomes the gap between the representation learning and clustering. As a consequence, the\nrapid development of deep representation learning techniques can be naturally introduced to the\nclustering task and optimized in a uniﬁed framework.\n4.5\nContrastive Deep Clustering\nInspired by the success of contrastive representation learning and mutual information maximiza-\ntion based deep clustering, contrastive learning has also been introduced to deep clustering. Sim-\nilar to contrastive representation learning, the target of contrastive deep clustering is to pull the\npositive pairs close while pushing the negative pairs far away. The major diﬀerence lies in the\ndeﬁnition of positive pairs and negative pairs, which can be further divided into three groups:\n4.5.1\nInstance-Instance contrast. The instance-instance contrast treats the cluster assignment of\neach instance as the representation and directly reuses the contrastive representation learning\nloss:\nL퐼퐼퐶= −log\n푁\nÕ\n푖=1\nexp \u0000푓\u0000푧푖,푧T\n푖\n\u0001 /휏\u0001\nÍ푁\n푗=1 exp\n\u0010\n푓\n\u0010\n푧푖,푧T\n푗\n\u0011\n/휏\n\u0011\n(16)\nwhere 푧푖is the cluster assignment of instance 푥푖predicted by the clustering module.\n4.5.2\nCluster-Cluster contrast. The cluster-cluster contrast treats each cluster as an instance in the\nembedding space, the target is pulling the cluster and its augmented version close while pushing\ndiﬀerent clusters far away, which can be formulated as:\nL퐶퐶퐶= −log\n퐾\nÕ\n푘=1\nexp\n\u0010\n푓\n\u0010\n푐푘,푐T\n푘\n\u0011\n/휏\n\u0011\nÍ퐾\n푗=1 exp\n\u0010\n푓\n\u0010\n푐푘,푐T\n푗\n\u0011\n/휏\n\u0011\n(17)\nwhere 푐푘is the representation of cluster 푘. It is worth noting that cluster-cluster contrast satisfy\nthe basic requirement of clustering that each cluster should be dissimilar, which agrees with the\nclustering friendly representation learning described in subsection 3.5.\n4.5.3\nInstance-Cluster contrast. The instance-cluster contrast is similar to the K-means which uti-\nlizes the cluster centroid as an explicit guidance. Given the representation of each instance and\ncluster centroid in the same low-dimensional space, each instance is expected to be close to the\ncorresponding cluster centroid while far from the other cluster centroids. Such similarity and dis-\nsimilarity can be naturally modeled by the contrastive learning:\nL퐼퐶퐶= −log\n푁\nÕ\n푖=1\nexp \u0000푓\u0000ℎ푖,푐′\n푖\n\u0001 /휏\u0001\nÍ퐾\n푗=1 exp \u0000푓\u0000ℎ푖,푐푗\n\u0001 /휏\u0001\n(18)\nwhere 푐′\n푖is the corresponding cluster centroid of 푥푖which is usually estimated by an alternative\nclustering method. This can be also understood as maximizing the mutual information between\nthe representation and cluster assignment with data augmentations.\nAnalysis. Besides the advantages inherited from the mutual information maximization cluster-\ning, the primary advantages of contrastive deep clustering is that the data augmentation helps\nimprove the robustness of clustering, which has been ignored by most existing methods. For de-\ntailed advantages of contrastive learning, please refer to the former contrastive learning surveys\n[85, 95].\n13\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\n5\nTAXONOMY OF DEEP CLUSTERING\nIn this section, we summarize existing deep clustering methods into four branches based on the\ninteraction between the representation learning module and the clustering module:\n(1) Multi-stage deep clustering: the representation learning module is sequential connected with\nthe clustering module.\n(2) Iterative deep clustering: the representation learning module and the clustering module are\niteratively updated.\n(3) Generative deep clustering: the clustering module is modeled as a prior representation mod-\nule.\n(4) Simultaneous deep clustering: the representation learning module and the clustering module\nare simultaneously updated.\n5.1\nMultistage Deep Clustering\nMultistage deep clustering refers to the methods where two modules are separately optimized and\nsequentially connected. One straightforward way is to employ the deep unsupervised representa-\ntion learning techniques to learn the representations (embedding) for each data instance ﬁrst, and\nthen feed the learned representations into the classic clustering models to obtain the ﬁnal cluster-\ning results. Such detachment of data processing and clustering facilitate researchers to perform\nclustering analysis. More speciﬁcally, all existing clustering algorithms can be of service to any\nresearch scenarios.\nEarly multi-stage deep clustering methods [81, 183] have trained a deep auto-encoderto learn\nthe representations, which can be directly packed as input of the K-means method to obtain the\nclustering results. Later, deep subspace clustering was proposed to learn an aﬃnity matrix and\ninstance representations ﬁrst, and then performs clustering by spectral clustering on the aﬃnity\nmatrix [88, 229, 239] or the K-means on the instance representations [157]. Thanks to the con-\ntribution of scikit-learn [154] and many other open-source machine learning libraries, clustering\nalgorithms has been applied to many ﬁelds with a limited cost of programming. For example,\nin the scenario of textual/video/graph data clustering, relation (similarity) matching was used in\n[26, 105, 186], K-Means in [25], Spectral Clustering in [2, 87, 155] and Hierarchical Agglomerative\nClustering in [172], so as many other clustering algorithms being directly applied. In special, graph\ncut based node clustering like Metis [100], Graclus [44] and Balance Normalized Cut (BNC) [32]\nwere used in graph clustering applications [33, 139, 205].\nThe most recent multistage methods have explicitly incorporated the clustering prior into the\nrepresentation learning, then conduct clustering on the target friendly representations. For exam-\nple, IDFD [181] learn representations with two aims: learning similarities among instances and\nreducing correlations within features. With the above explicit purposes, a naive K-means on the\nlearned representations can also achieve competitive clustering results over many existing deep\nclustering methods.\nSummary. Multistage methods enjoy the property of fast deployment, programming friendly,\nand intuitive understanding. However, such a simple combination of deep representation learning\nand shallow (traditional) clustering has the following weakness: 1) Most representation learning\nmethods are not intently designed for clustering tasks, which can not provide suﬃcient discrimi-\nnative power for clustering. Speciﬁcally, clustering reﬂects global patterns among data instances,\nwhile existing representation learning methods have largely focused on the individual patterns.\n2) The clustering results can not be further utilized to guide the representation learning, which is\nof great importance for comprehensive representation. In particular, the cluster structure implies\n14\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\nthe inherent relationships among data instances, and can in turn serve as critical guidance for im-\nproving representation learning. To conclude, such straightforward cascade connection will cut\noﬀthe information ﬂow and interactions between representation learning and clustering, thus the\nlimitations of both side will inﬂuence the ﬁnal performance together.\n5.2\nIterative Deep Clustering\nThe key motivation of iterative deep clustering is that good representations can beneﬁt clustering\nwhile clustering results reversely provide supervisory to representation learning. Brieﬂy speaking,\nmost existing iterative deep clustering pipeline is iteratively updated between two steps: 1) calcu-\nlating clustering results given current representations and 2) updating the representations given\nthe current clustering results. Since the representation module only provides input for the clus-\ntering module in iterative deep clustering, in this subsection, we classify existing iterative deep\nclustering methods according to the information provided by the clustering module.\n5.2.1\nIterative deep clustering with individual supervision. The individual supervision in iterative\ndeep clustering depends on the pseudo labels generated by the clustering module, which can be\nutilized to train the representation learning module in a supervised manner.\nIn early works [175, 212], the cluster centroids and assignments are updated in a K-means way.\nS2ConvSCN [225] and PSSC [127] combine subspace clustering and pseudo labeling, which ob-\ntain pseudo labels by spectral clustering or partitioning the pseudo similarity graph. Later, a lot of\nworks [21, 141, 190] tend to utilize neural networks for both representation learning and cluster-\ning, where these two parts are combined together as one neural network. The clustering module\nis usually a multilayer perceptron(MLP) that produces soft clustering assignments. In this way,\nthe hard pseudo labels can guide both clustering and representation learning through gradient\nbackpropagation with proper constraints. The representative method is DeepCluster [21], which\nalternates between K-means clustering and updating the backbone along with the classiﬁer by min-\nimizing the gap between predicted clustering assignments and pseudo labels. In fact, DeepCluster\nhas already been applied as a mature clustering algorithm in video clustering [4].\nRecently, SCAN [190] follows a pretraining-with-ﬁnetuning framework. The clustering results\nare ﬁne-tuned with self-labeling, which selects the highly conﬁdent instances by thresholding the\nsoft assignment probability, and updates the whole network by minimizing the cross-entropy loss\non the selected instances. SPICE [141] is another representative iterative deep clustering method,\nwhere the classiﬁcation model is ﬁrst trained with the guidance of pseudo labels and then retrained\nby the semi-supervised training on the set of reliably labeled instances.\n5.2.2\nIterative deep clustering with relational supervision. The relational supervision in iterative\ndeep clustering refers to the relationship based on the pesudo labels, which provides pairwise\nguidance to the representation learning module. More speciﬁcally, the relationship is usually mod-\neled by whether two instances have same discrete pseudo labels [28, 214] and the model is trained\nas a binary classiﬁcation task. Another popular branch of methods [142, 230] model the relation-\nship by the similarity of cluster assignment probabilities, which train the representation learning\nas a regression task.\nSummary. Iterative deep clustering methods beneﬁt from the mutual promotion between rep-\nresentation learning and clustering. However, they also suﬀer from the error propagation in the\niterative process. More speciﬁcally, the inaccurate clustering results can lead to chaotic representa-\ntions where the performance are limited by the self-labeling eﬀectiveness. Furthermore, this will\nin turn aﬀect the clustering results especially in the early stage of training. Therefore, existing\niterative clustering methods heavily rely on the pretraining of representation module.\n15\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\n5.3\nGenerative Deep Clustering\nGenerative models are able to capture, represent and recreate data points, and thus are drawing\nincreasing attention from both academia and industry. They would make the hypotheses about\nthe latent cluster structure and then infer the clustering assignment by estimation of data density.\nThe most representative model is Gaussian Mixture Model (GMM) [163], which assumes that the\ndata points are generated from a Mixture-of-Gaussians. Speciﬁcally, suppose there are 퐾clusters,\nand an observed sample 푥is generated from the following process:\n(1) Choose a cluster: 푐∼Mult(휋)\n(2) Draw a sample: x|푐∼N \u0000휇푐,휎2\n푐I\u0001\nwhere 휋denotes the prior probability for clustering; Mult(휋) is the multinomial distribution with\nthe parameter 휋; 휇푐and 휎푐are the mean and variance parameters of the Gaussian distribution cor-\nresponding to the cluster푐. The well-known expectation maximization algorithm can be employed\nto learn the optimal parameters and clustering assignment.\nWhile GMM has gained successful applications, its shallow structure is usually insuﬃcient for\ncapturing the nonlinear patterns of the data, adversely aﬀecting its performance on complex data\n(e.g., images, texts, graphs and etc). To address this problem, deep generative models have been\nproposed to combine the generative model with the powerful deep neural networks, which have\nenough capacity to model the non-linear and complex data. This kind of methods can be classiﬁed\ninto two types: the methods based on Variational Auto-Encoder (VAE) and the methods based on\nGenerative Adversarial Networks (GAN).\n5.3.1\nDeep Generative clustering based on Variational Auto-Encoder. For clustering of high dimen-\nsional and complex data, one promising solution is to directly stack GMM with a deep neural\nnetwork — GMM generates a latent vector 푧, and the deep neural network further transforms the\nlatent vector 푧into the complex data instance 푥. In this way, the stacked model can enjoy the\nmerit of the latent cluster structure and meanwhile has suﬃcient capacity to model the complex\ndata. For example, the representative models, VaDE [93] and GMVAE [45], assume the following\ngenerative process for each instance:\n(1) Choose a cluster: 푐∼Mult(휋)\n(2) Draw a latent vector: z|푐∼N \u0000휇z(푐; 훽),휎2\n푧((푐; 훽))I\u0001\n(3) Draw a sample: x|z ∼N \u0000휇x(z;휃),휎2\nx(z;휃)I\u0001\nwhere 휇z(.; 훽), 휎2\nz (.; 훽), 휇x(.;휃) and 휎2\nx(.;휃) are given by neural networks with parameters 훽and 휃,\nwhich determinize the mean and variance of Gaussian distributions, respectively. Given the above\ngenerative process, the optimal parameters and cluster assignment can be obtained by maximizing\nthe likelihood of the given data points as:\nlog 푝(x) = log\n∫\nz\nÕ\n푐\n푝(x|z)푝(z|푐)푝(푐)dz\n(19)\nHowever, directly optimizing the above likelihood is intractable as it involves integration and\ncomplex neural network. Variational Auto-Encoder (VAE)[102] sheds a light to tackle this problem\nsuch that the parameters and the posterior can be eﬃciently estimated via backpropagation. Specif-\nically, the generative model is trained with the following variational inference objective, a.k.a. the\nevidence lower bound (ELBO):\nLELBO(x) = E푞(z,푐|x)\n\u0014\nlog 푝(x, z,푐)\n푞(z,푐|x)\n\u0015\n(20)\n16\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\nwhere 푞(z, c|x;휑) is the variational posterior to approximate the true posterior, which can be mod-\nelled with the recognition networks 휑. Monte Carlo [97] and the reparameterization trick [102]\ncan be employed to learn the parameters.\nMore recently, upon VaDE and GMVAE, some improved variants have been proposed. For exam-\nple, Prasad et al. [159] introduced a data augmentation technique that constrains an input instance\n(e.g., image) to share a similar clustering distribution with its augmented one; Li et al. [119] em-\nployed Monte Carlo objective and the Std Annealing track for optimizing mixture models, which\nwould generate better-separated embeddings than the basic VAE-based methods; Ji et al. [89] pro-\nposed to replace decoder in VAE with an improved mutual-information-based objective; Wang et\nal. [197] proposed to separate the latent embeddings into two parts which capture the particularity\nand commonality of the clusters, respectively.\n5.3.2\nDeep Generative clustering based on Generative Adversarial Network. Recent years have wit-\nnessed the great success of Generative Adversarial Network (GAN) in estimating complex data\ndistribution [6, 63, 83]. A standard GAN contains two components: a generator 퐺that targets\nsynthesizing “real” samples to fool the discriminator, and a discriminator 퐷tries to discriminate\nthe real data from the generated samples. With the adversary between the two components, the\ngenerator could generate samples that have a similar distribution to the data. Inspired by such\noutstanding ability, it would be promising to integrate GAN into generative clustering models.\nSpeciﬁcally, Ben-Yosef et al. [10] proposed to stack a GMM with a GAN, where GMM serves as a\nprior distribution for generating data instances. Formally, they optimized the following objective\nfunction:\nmin\n퐺max\n퐷\n푉(퐷,퐺) =\nE\nx∼푝X (x)[log 퐷(x)] +\nE\nz∼푝Z (z)[log(1 −퐷(퐺(z)))]\n(21)\nwhere 푝X(x) denotes the training data distribution; 푝Z(z) is a prior distribution of 퐺and deﬁned\nas mixture of Gaussians:\n푝Z(z) =\n퐾\nÕ\n푘=1\n휋푘∗N \u0000휇푐,휎2\n푐I\u0001\n(22)\nBy equipping GAN with such multi-modal probability distribution, the model could provide a\nbetter ﬁt to the complex data distribution especially when the data includes many diﬀerent clusters.\nThere are also some improved variants. For example, Yu et al. [218] proposed to directly replace\nthe Gaussian distribution of GMM with a GAN and developed a휖-expectation- maximization learn-\ning algorithm to forbid early convergence issues; Ntelemis et al. [144] proposed to employ Sobel\noperations prior to the discriminator of the GAN; Mukherjee et al. [138] proposed to sample the\nlatent vector 푧from a mixture of one-hot encoded variables and continuous latent variables. An\ninverse network with a clustering-speciﬁc loss is introduced to make the model more friendly to\nthe clustering task. Analogously, an inverse network is introduced in [57, 59] for the feature-level\n(i.e., latent vector) adversary.\nSummary. Although deep generative clustering models can generate samples while complet-\ning clustering, they also have some weaknesses: 1) Training a generative model usually involves\nMonte Carlo sampling, which may incur training unstable and high computational complexity; 2)\nThe mainstream generative models are based on VAE and GAN, and inevitably inherit the same\ndisadvantages of them. VAE-based models usually require prior assumptions on the data distribu-\ntions, which may not be held in real cases; although GAN-based algorithms are more ﬂexible and\ndiverse, they usually suﬀer from mode collapse and slow convergence, especially for the data with\nmultiply clusters.\n17\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\n5.4\nSimultaneous Deep Clustering\nSimultaneously deep clustering is the most active direction in current deep clustering, where rep-\nresentation learning module and the clustering module are simultaneously optimized in an end-\nto-end manner. Although most iterative deep clustering methods also optimize both two modules\nwith a single objective, the two modules are optimized in an explicit iterative manner and can not\nbe updated simultaneously. In this subsection, we introduce the representative architectures of\nsimultaneously deep clustering.\n5.4.1\nAuto-encoder with self-training. Auto-encoder is a powerful tool to learn data representa-\ntions in an unsupervised way, and has been utilized since the ﬁrst attempts of simultaneously\ndeep clustering [81].\nThe representative method is DEC [206] which combines the auto-encoder with the self-training\nstrategy. Such simple but eﬀective strategy has deeply inﬂuenced the follow-up works. The auto-\nencoder is pre-trained and only the encoder is utilized as the initialization of the representation\nlearning module. The self-training strategy mentioned in subsection 4.3 is then introduced to op-\ntimize the clustering and representation learning simultaneously.\nBased on the vanilla DEC method, many variants and improvements are proposed. To preserve\nthe local structure of each instance, IDEC [68] further integrates the reconstruction loss to the\nauto-encoder. The general formulation of auto-encoder with self-training can be summarized as:\nL퐴퐸푆푇= L퐴퐸+ L푆푇\n(23)\nwhere L퐴퐸is the loss of auto-encoder and L푆푇is the loss of clustering oriented self-training, e.g.\nthe neighborhood constraint in DEC. To improve the capability of auto-encoder, some eﬀorts are\nmade to adapt diﬀerent data types. In [60, 69, 112], the linear layer of auto-encoder is replaced with\nfully convolutional layers so that the image feature can be well captured. In CCNN [77], the clus-\ntering convolutional neural network is proposed as a new backbone to extract the representations\nwhich are friendly to the clustering task. In DEPICT [60], an additional noisy encoder is introduced,\nand the robustness of auto-encoder is improved by minimizing the reconstruction error of every\nlayer between the noisy decoder and the clean encoder.\nAlthough the self-training strategy has achieved success, later works also make attempts for spe-\nciﬁc problem. To boost the robustness of clustering, self-training is applied between two branches:\nclean and augmented [69] (noisy [60]). Speciﬁcally, the target distribution P is computed via the\nclean branch to guide the soft assignments Q of the augmented or noisy branch. Self-training\nstrategy can be combined with subspace clustering. CSC [156] introduces the assumption named\ninvariance of distribution, i.e. the target distribution P should be invariant to diﬀerent distance\nmetrics in subspace space. Therefore, two metrics (Euclidean and Cosine distance) are used to\ncompute the target distributions P퐸and P퐶, with the KL divergence between them minimized.\nAccording to aforementioned analysis, self-training is similar to the K-means clustering and\nsuﬀers from the unbalance problem [19] between diﬀerent clusters. To solve the problem of the\nunbalanced data and out-of-distribution samples, StatDEC [164] improves target distribution by\nadding normalized instance frequency of clusters. In this way, the model can preserve discrimi-\nnation of small groups and form a local clustering boundary which is insensitive to unbalanced\nclusters.\n5.4.2\nMutual Information Maximization based Clustering. As illustrated in subsection 3.3 and 4.4,\nthe mutual information maximization has been successfully applied in both representation and\nclustering. The uniﬁed form of mutual information in both modules has brought convenience for\nunderstanding and implementation.\n18\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\nThe representative mutual information maximization based clustering method is DCCM [202].\nFor each data instance, the mutual information between deep and shallow layer representation is\nmaximized so that the consistency of representation can be guaranteed. Such consistency is further\nextended to the cluster assignment space by encouraging the instances with the same pseudo la-\nbels to share similar representations. Many later works can be regarded as variants of this method.\nIn VCAMI [90] and IIC [91], the augmented mutual information (AMI) is introduced to improve\nthe robustness. Such augmentation invariant has inspired the later contrastive deep clustering\nmethods, which will be introduced in the next subsection. In ImC-SWAV [145], the mutual infor-\nmation between the integrated discrete representation and a discrete probability distribution is\nmaximized, which improves the vanilla SWAV [22] method.\n5.4.3\nContrastive Deep Clustering. Similar to mutual information maximization based deep clus-\ntering, contrastive learning has also been successfully applied in both representation learning mod-\nule and clustering module. The main idea of contrastive learning is pulling similar instances closer\nwhile pushing diﬀerent instances away, which is in the spirit of clustering that instances from the\nsame cluster should be close while instances from diﬀerent clusters should be apart.\nThe representative contrastive deep clustering method is CC [118]. The basic idea is to treat\neach cluster as the data instance in the low-dimensional space. The instance discrimination task\nin contrastive representation learning can be migrated to the clustering task by discriminating\ndiﬀerent clusters, which is the fundamental requirement of clustering. Furthermore, the advantage\nof augmentation invariant and local robustness can be preserved in the clustering task.\nTake CC as the fundamental architecture, many contrastive deep clustering can be viewed as\nvariants of it. PICA [80] can be viewed as the degeneration of CC without augmentation, it di-\nrectly separates diﬀerent clusters by minimizing the cosine similarity between the cluster-wise\nassignment statistic vectors. In DCCM [202], the augmentation is introduced to guarantee the lo-\ncal robustness of the learned representation. DRC [236] has the same contrastive learning as CC\nwhere the cluster representation is called assignment probability. The diﬀerence lies in the cluster\nregularization which is inspired by group lasso [131]. In CRLC [46], contrastive learning is per-\nformed between the cluster assignments of two augmented versions of the same instance, rather\nthan the cluster representations. Also, the dot product in contrastive learning is replaced by the\nlog-dot product, which is more suitable for the probabilistic contrastive learning. SCCL [222] ex-\ntends this approach with textual data augmentation, which proves that this contrastive learning\nbased self-training framework is universally applicable.\nThe later works further adopt the contrastive clustering in the semantic space. In SCL [79], the\nnegative samples are limited by diﬀerent pseudo labels, so that instances from diﬀerent clusters\ncan be further distinguished. MiCE [185] follows a divide-and-conquer strategy where the gating\nfunction divides the whole dataset into clusters and the experts in each cluster aim at discrimi-\nnating the instances in the cluster. However, compared with InfoNCE that models alignment and\nuniformity implicitly[196], MiCE model these two properties in a more explicit way. Recently, TCC\n[171] further improve the eﬃciency with the reparametrization trick and explicitly improve the\ncluster discriminability.\nSome other methods make attempts on overcoming the problems of vanilla contrastive learning.\nIn GCC [237], the positive pair and negative pair are selected by the KNN graph constructed on\nthe instance representation. This may be related to overcoming the ‘false negative’ problem in\ncontrastive learning. In NCC [82], the contrastive learning module is replaced from SimCLR to\nBYOL [65], so that the over-reliance on the negative samples can be solved.\n5.4.4\nHybrid simultaneous deep clustering. The aforementioned simultaneous deep clustering meth-\nods have remarkable characteristics and advantages, some other works are hybrids of the above\n19\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\ntechniques. SCCL [222] and Sundareswaran et al. [179] combine contrastive representation learn-\ning and self-training. DDC [27] and RCC [169] combine relation matching clustering and pseudo\nlabeling to boost the clustering performance. DCC [170] combines the auto-encoder based repre-\nsentation and the relation matching clustering. The auto-encoder based representation learning\nand spectral clustering are combined in [215] by incorporating the augmentations from the con-\ntrastive learning.\nSummary. The simultaneous deep clustering has attracted the most attention for its uniﬁed\noptimization. Intuitively, the learned representation is clustering oriented and the clustering is\nconducted on the discriminative space. However, it may incur from undesired prejudice of opti-\nmization focus between representation learning module and clustering module, which can only\nbe mitigated by the manually setting of the balanced parameter for now. Also, the model is easy\nto sink into degenerate solutions where all instances are assigned into one single cluster.\n6\nDATASETS AND EVALUATION METRICS\nIn this section, we introduce benchmark datasets and evaluation metrics that are widely used in\nexisting deep clustering methods.\n6.1\nDatasets\n6.1.1\nImage Datasets. Image is the most commonly used data type in real-world deep clustering.\nThe early attempts of deep clustering are applied to image datasets including COIL-202, CMU\nPIE3, Yale-B4, MNIST5, CIFAR6 and STL-107. Recently, eﬀorts has been paid to perform clustering\non large volume vision datasets (e.g. ImageNet). Although existing methods have achieved promis-\ning performance on ImageNet-10 and ImageNet-Dogs dataset, clustering on Tiny-ImageNet (200\nclusters) or full-size ImageNet is still challenging.\n6.1.2\nTextual Datasets. The widely used textual datasets in early applications of texutal data clus-\ntering include Reuters-215788 and 20 Newsgroups9 datasets, which have already been vectorized\nand little feature engineering is needed. Currently, raw textual datasets including IMDB10, stack-\nOverﬂow11, and more in nlp-datasets github repository12 is still challenging for deep textual clus-\ntering.\n6.1.3\nVideo Datasets. The ultimate task of video clustering varies from action classiﬁcation [155]\nto video anomaly detection [200]. Kinetics-400 and Kinetics-60013 are two of the most famous\nvideo datasets. The others include UCF-101 dataset14 and HMDB-51 dataset15.\n2http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php\n3http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html\n4http://vision.ucsd.edu/~leekc/ExtYaleDatabase/Yale%20Face%20Database.htm\n5http://yann.lecun.com/exdb/mnist/index.html\n6http://www.cs.toronto.edu/~kriz/cifar.html\n7https://cs.stanford.edu/~acoates/stl10/\n8https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection\n9http://qwone.com/~jason/20Newsgroups/\n10http://ai.stanford.edu/~amaas/data/sentiment/\n11https://github.com/jacoxu/StackOverﬂow\n12https://github.com/niderhoﬀ/nlp-datasets\n13https://www.deepmind.com/open-source/kinetics\n14https://www.crcv.ucf.edu/research/data-sets/ucf101/\n15https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/#dataset\n20\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\nTable 3. The proposed taxonomy of existing methods and their properities\nTaxonomy\nAdvantages\nDisadvantages\nClustering Module\nType\nCitation\nMulti Stage\n1. Easy to implement\n2. Better interpretability\n1. Sub-optimal representations\n2. Limit on clustering performance\nMulti Stage\nImage\n[183][88][229][239]\n[157][181][39]\nGraph\n[81][33][139][205]\nTextual\n[26][186][25][172]\n[87] [117][105][2]\nVideo\n[155][200]\nMedical\n[71][40]\nGenerative\n1. Latent cluster structure\ndiscovery\n2. Able to model complex\ndata distribution\n1. Unstable training\n2. High computational complexity\n3. Sensitive to prior distribution\nassumptions\nVAE\nImage\n[45][215][93][159]\n[89] [119][197]\nGAN\nImage\n[138][10][218][144]\n[57] [59]\nGraph\n[92]\nIterative\n1. Clustering oriented\nrepresentation learning\n2. Discriminative space for\nclusteing\n1. Error propagation\n2. Limit on pseudo-label quality\nIndividual supervision\nImage\n[214][212][175][225]\n[127][190][141]\nVideo\n[4]\nMedical\n[99][136]\nRelational supervision\nImage\n[28][21][230][142]\nSimultaneous\n1. Clustering oriented\nrepresentation learning\n2.Discriminative space for\nclusteing\n3. Uniﬁed optimization\nframework\n1. Undesired prejudice of\noptimization focus\n2. Sensitive to degenerate solutions\nAuto-encoder based\nImage\n[206][68][60][69][77]\n[112][156][164]\nGraph\n[15]\nMedical\n[184][78]\nMutual Information\nbased clustering\nImage\n[202][90][91][145]\nContrastive\nImage\n[118][38][80][202]\n[236] [46][79][185]\n[171][82] [39]\nTextual\n[222]\nGraph\n[237]\nHybrid methods\nImage\n[179][27][169]\nTextual\n[222][133] [182]\n21\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\n6.1.4\nGraph Datasets. Commonlyused graph datasets for node clustering can be referred from the\nfollowing papers [33, 139, 174] and Stanford Network Analysis Project16. And there are also graph-\nlevel classiﬁcation datasets like PROTINS[16] and MUTAG[41], which can be used to perform and\nevaluate graph-level clustering.\n6.2\nEvaluation Metrics\nEvaluation metrics aim to evaluate the validity of methods. In the ﬁeld of Deep Clustering, three\nstandard clustering performance metrics are widely used: Accuracy(ACC), Normalized Mutual\nInformation(NMI) and Adjusted Rand Index(ARI).\n6.2.1\nAccuracy. ACC indicates the average correct classiﬁcation rate of clustering samples. Given\nthe ground truth labels푌= {푦푖|1 ≤푖≤푁} and the predicted hard assignments ˜푌= {˜푦푖|1 ≤푖≤푁},\nACC can be computed as follows:\n퐴퐶퐶(˜푌,푌) = max\n푔\n1\n푁\n푁\nÕ\n푖=1\n1{푦푖= 푔(˜푦푖)}\n(24)\nwhere 푔is the set of all possible one-to-one mappings between the predicted labels and ground\ntruth labels. The optimal mapping can be eﬃciently obtained by the Hungarian algorithm [107].\n6.2.2\nNormalized Mutual Information. NMI quantiﬁes the mutual information between the pre-\ndicted labels and ground truth labels into [0, 1]:\n푁푀퐼(˜푌,푌) =\nI(˜푌;푌)\n1\n2\n\u0002퐻(˜푌) + 퐻(푌)\u0003\n(25)\nwhere 퐻(푌) is the entropy of 푌and I(˜푌;푌) is the mutual information between ˜푌and 푌.\n6.2.3\nAdjusted Rand Index. ARI comes from Rand Index(RI), which regards the clustering result\nas a series of pair-wise decisions and measures it according to the rate of correct decisions:\n푅퐼= 푇푃+푇푁\n퐶2\n푁\n(26)\nwhere 푇푃and 푇푁denote the number of true positive pairs and true negative pairs, 퐶2\n푁is the\nnumber of possible sample pairs. However, the RI value of two random partitions is not a constant\napproaching 0, thus ARI was introduced:\n퐴푅퐼=\n푅퐼−E(푅퐼)\nmax(푅퐼) −E(푅퐼)\n(27)\nBoth ACC and NMI ∈[0, 1] while ARI ∈[−1, 1], in which higher values indicate better perfor-\nmance.\n7\nAPPLICATIONS\nDespite the success of deep clustering in mining global pattern among instances, it has also beneﬁt\nvarious downstream tasks. In this section, we discuss some typical applications of deep clustering.\n16http://snap.stanford.edu/\n22\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\n7.1\nCommunity Detection\nCommunity Detection [55, 94, 121] aims at partitioning the graph network into several sub-graphs\nmainly according to connection density, which can be treated as node-level graph clustering task.\nEarly works are mainly based on modularity measurement [14, 61], Maximum ﬂows [54], graph cut\n[173] and its extension, spectral methods [193]. With the development of Graph Nerual Networks\n(GNNs)[128, 241], nodes are represented as individual instances in the low-dimensional space. As a\nresult, the border between modern community detection [178] and graph clustering[115, 116, 242]\nis gradually getting blurry, and GNN based graph clustering [13, 187] have already been applied to\nmany applications. However, diﬀerent from early community detection that focus on the network\ntopology, the graph clustering usually incorporates the node attributes and other side information.\nHow to release the power of GNNs while reserve the topology characteristic is still under study.\n7.2\nAnomaly Detection\nAnomaly Detection (a.k.a. Outlier Detection, Novelty Detection) is a technique for identifying ab-\nnormal instances or patterns among data. Early before deep clustering, density based clustering\nmethods [37, 51, 168] have speciﬁcally mentioned and addressed the problem of noise during clus-\ntering, which has enlighted a group of density based anomaly detection methods [24, 31]. The\nlater anomaly detection methods [129, 130, 200] have utilized the clustering results and identify\nthe anomaly instances as those far from the cluster centroids or border of each cluster. Currently,\nthe deep clustering has shown great potential in forming a better clustering space for anomaly de-\ntection. Instead of performing anomaly detection after the deep clustering, the recent eﬀorts have\nbeen put on conducting them in a uniﬁed framework: identify and remove the anomaly instances\nto reduce the impact on clustering [122], and anomaly detection can be further improved with\nbetter clustering results.\n7.3\nSegmentation and Object Detection\nImage Segmentation is one of the most important approaches to simulating human understanding\nof images which aims at dividing the pixels into disjoint regions. Generally speaking, image seg-\nmentation is doing pixel classiﬁcation in a supervised manner and pixel clustering in an unsuper-\nvised manner [5, 36]. Currently, the deep clustering has been successfully applied in segmentation\nby utilizing the clustered regions to generate Scene Graph [213]. Yi et al. [216] surveyed graph cut\nbased image segmentation, where graph cut is one of the most fundamental solutions to perform\nclustering (Spectral Clustering). 3D clustering can be a solution to 3D Object Detection, like in [29],\n3D points were clustered to represent an object with geometric consistency. But such clustering\nbased segmentation and object detection has no guarantee for small region and object, where the\nexpected clustering result is highly unbalanced. And the global positional information of a pixel\nmay be ignored when peoforming clustering.\n7.4\nMedical Applications\nThe convolutional neural network has successfully promoted the development of medical image\nprocessing in a supervised manner. However, the manual dataset labeling process is often labor-\nintensive and requiring expert medical knowledge [99], which is hard to realize in the real-world\nscenarios. Recently, the deep clustering has been introduced to automatically categorize large-\nscale medical images [99]. Mit al et al. [136] introduce medical image clustering analysis for faster\nCOVID-19 diagnostic. In the biological science ﬁeld, single cell RNA sequencing (scRNA-seq) [49]\ngives an output cell-gene matrix for cell population and behaviour analysis and even new cell\ndiscovery. For this purpose, ScDeepCluster [184] and ItClust [78] develop their models based on\n23\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\nDEC [206] to cluster scRNA-seq data, and MARS [17] combining transfer learning and clustering\nto discover novel cell types. More application can be found in gene data clustering area[58].\n7.5\nSummary\nIn addition to the above successful deep clustering applications, the clustering also has great po-\ntential in many other domains, such as ﬁnancial analysis [35, 64, 86], trajectory analysis [8, 147],\nand social media understanding [149, 219, 245]. Although most of the aforementioned methods\nhas not incorporate deep learning techniques, with the increasing data volume and dimension, we\nbelieve that deep clustering will pave the way for these scenarios.\n8\nFUTURE DIRECTIONS\nIn this section, we conduct some future directions of deep clustering based on the above corner-\nstone, taxonomy and real-world applications.\n8.1\nInitialization of Deep Clustering Module\nThe initialization of deep neural networks usually play an important role in training eﬃciency and\nstability [62]. This is more critical in deep clustering where both the representation learning mod-\nule and clustering module are modeled by deep neural networks. Recently, the model pre-training\n[161] has been a popular network initialization technique which has also been introduced to the\ndeep clustering [190]. However, the pre-training based initialization is appropriate for representa-\ntion learning module but has not been well studied for the clustering module. Although there has\nbeen some initialization schemes on the shallow clustering [23], the initialization for the cluster\nmodule with deep neural networks is still under investigation.\n8.2\nOverlapping Deep Clustering\nThe deep clustering methods discussed in this paper largely focus on the partitioning clustering\nwhere each instance belong to only one cluster. Meanwhile, in the real-world scenario, each in-\nstance may belong to multiple clusters, e.g., users in a social network [207] may belong to several\ncommunities, and the video/audio in the social media may have several tags [203]. Among the\ndeep clustering methods discussed in this paper, if the clustering constraints are conducted on the\ncluster assignment probabilistic matrix, they can be directly adapted to the overlapping clustering\nsetting. However, if the training relies on the pseudo hard label of data instances, they may fail in\nthe overlapping clustering setting. Although the multi-label classiﬁcation has been widely stud-\nied in the literature [123, 226], how to adapt to unsupervised clustering learning is still an open\nresearch question.\n8.3\nDegenerate Solution VS Unbalanced Data\nThe degenerate solution [21] has been one of the most noteworthy concerns in deep clustering,\nwhere all instances may be assigned to a single cluster. Many deep clustering methods have added\nextra constraints to overcome this problem [21, 80, 81, 91, 118, 153, 190], among which the entropy\nof cluster size distribution is the most widely used one. By maximizing the entropy, the instances\nare expected to be evenly assigned to each cluster and avoid degenerating to single cluster. It\nis worth noting that the success relies on the uniform distribution of ground truth labels which\nis coincidentally satisﬁed by the benchmark dataset such as CIFAR10 and CIFAR100. However,\nin real-world scenarios, such assumption is too strict and most datasets are unbalanced or long\ntailed [195]. The divergent target of uniform distribution and unbalanced dataset will seriously\nweaken the deep clustering. Recently, clustering unbalanced data [52, 120] has attracted increasing\nattention and this will boost the performance of clustering in real-world applications.\n24\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\n8.4\nBoosting Representation with Deep Clustering\nThroughout this paper, we can ﬁnd that a good representation is essential for clustering. Although\nthe clustering friendly representation learning has been studied in the literature, they are designed\nfor the speciﬁc shallow clustering method. On the contra, the clustering structure denotes the high-\norder pattern of the dataset, which should be preserved in the comprehensive representation [114].\nThe deep clustering methods discussed in this paper focus on how to incorporate the representa-\ntion learning to boost the clustering, meanwhile, how to in turn boost the representation learning\nby the clustering is still to be studied.\n8.5\nDeep Clustering Explanation\nAs an unsupervised task, the clustering process usually lacks human priors such as label semantics,\nnumber of clusters[137], which makes the clustering results hard to explain or understand. Some\nmethods [223] have already combined the tags provided by the users to boost the explanation of\nthe clustering results. However, it relies on the accurate human tagging which may not be realized\nin practical. With the development of causal inference in deep learning, the explanation of cluster-\ning among instances is hopefully to be improved. Both the research and industry community are\nexpecting a generalized explanation framework for clustering, especially on the high-dimensional\ndata. To conclude, how to utilize the casual inference techniques in the clustering is of great im-\nportance and deserve more attention.\n8.6\nTransfer Learning with Deep Clustering\nTransfer learning [151] aims at bridging the gap between the training and test datasets with distri-\nbution shift. The general idea is to transfer the knowledge from the known data to the unknown\ntest data. Recently, deep clustering is playing an increasing important role in unsupervised trans-\nfer learning [42, 180, 211, 240, 244], where the target domain is in the unsupervised manner. For\nexample, ItClust [78] and MARS [17] has achieved success in scRNA-seq clustering (section 7.4),\nAD-Cluster [221] that use clustering has improved domain adaptive person re-identiﬁcation. On\nthe other way around, unsupervised transfer learning methods can also beneﬁt deep clustering.\nTake UCDS [132] as an example, the unsupervised domain adaptation is used to perform cluster-\ning among variant domains. Distribution shift is one of the key factor aﬀecting the performance of\nmachine learning models, including deep clustering. Perform clustering analysis can give further\nunderstanding on unsupervised target domain, but how to transfer clustering result to knowledge\nand eﬀectively minimize the distribution shift based on clustering result can be further explored.\n8.7\nClustering with Anomalies\nIn subsection 7.2, we have discussed the applications of deep clustering in anomaly detection where\nthe instances are well clustered. Concerning the existence of anomaly instances in the dataset, the\nclustering may also be inﬂuenced and mutually restrict, since most existing deep clustering meth-\nods have no speciﬁc response to the outliers. The classic K-means method is known to be sensitive\nto outliers, although there has been a few works on overcoming this problem [122], they are de-\nsigned for shallow clustering methods. To this end, how to improve the deep clustering robustness\nto the anomaly instances and gradually improve the clustering performance by reducing the de-\ntected anomaly instances is still an open research question.\n8.8\nEﬀicient Training VS Global Modeling\nTo improve the training eﬃciency and scalability, most existing deep clustering methods have\nutilized the mini-batch training strategy, where the instances are separated into batches and the\n25\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\nmodel is updated with each batch. This is suitable for the task where the instances are independent\non each other, such as classiﬁcation and regression. However, as the deep clustering heavily relies\non the complicated relationship among instances, such mini-batch training may lose the ability\nof global modeling. Although some existing methods have utilized the cluster representations or\nprototypes [114] to store the global information, how to balance the training eﬃciency and model\ncapability is still worth studying.\n9\nCONCLUSION\nIn this survey, we present a comprehensive and up-to-date overview of the deep clustering research\nﬁeld. We ﬁrst summarize the cornerstones of deep clustering namely the representation learning\nmodule and the clustering module, followed by the representative design. Based on the ways of\ninteraction between representation learning module and clustering module, we present the taxon-\nomy of existing methods namely: multi-stage deep clustering, iterative deep clustering, generative\ndeep clustering and simultaneous deep clustering. Then, we collect the benchmark datasets, met-\nrics for evaluation and applications of deep clustering. Last but not least, we discuss the future\ndirections in deep clustering that have potential opportunities.\nREFERENCES\n[1] Mohanad Abukmeil, Stefano Ferrari, Angelo Genovese, Vincenzo Piuri, and Fabio Scotti. 2021. A survey of unsuper-\nvised generative models for exploratory data analysis and representation learning. Acm computing surveys (csur) 54,\n5 (2021), 1–40.\n[2] Nabil Alami, Mohammed Meknassi, Noureddine En-nahnahi, Yassine El Adlouni, and Ouafae Ammor. 2021. Unsu-\npervised neural networks for automatic Arabic text summarization using document clustering and topic modeling.\nExpert Systems with Applications 172 (2021), 114652.\n[3] Elie Aljalbout, Vladimir Golkov, Yawar Siddiqui, Maximilian Strobel, and Daniel Cremers. 2018. Clustering with\ndeep learning: Taxonomy and new methods. arXiv preprint arXiv:1801.07648 (2018).\n[4] Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, and Du Tran. 2020. Self-\nsupervised learning by cross-modal audio-video clustering. Advances in Neural Information Processing Systems 33\n(2020), 9758–9770.\n[5] Agus Zainal Ariﬁn and Akira Asano. 2006. Image segmentation by histogram thresholding using hierarchical cluster\nanalysis. Pattern recognition letters 27, 13 (2006), 1515–1521.\n[6] Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein generative adversarial networks. In Inter-\nnational conference on machine learning. PMLR, 214–223.\n[7] Philip Bachman, R Devon Hjelm, and William Buchwalter. 2019. Learning representations by maximizing mutual\ninformation across views. Advances in neural information processing systems 32 (2019).\n[8] Tharindu Bandaragoda, Daswin De Silva, Denis Kleyko, Evgeny Osipov, Urban Wiklund, and Damminda Alahakoon.\n2019. Trajectory clustering of road traﬃc in urban environments using incremental machine learning in combination\nwith hyperdimensional computing. In 2019 IEEE intelligent transportation systems conference (ITSC). IEEE, 1664–1670.\n[9] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and\nDevon Hjelm. 2018. Mutual information neural estimation. In International conference on machine learning. PMLR,\n531–540.\n[10] Matan Ben-Yosef and Daphna Weinshall. 2018.\nGaussian mixture generative adversarial networks for diverse\ndatasets, and the unsupervised clustering of images. arXiv preprint arXiv:1808.10356 (2018).\n[11] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives.\nIEEE transactions on pattern analysis and machine intelligence 35, 8 (2013), 1798–1828.\n[12] Pavel Berkhin. 2006. A survey of clustering data mining techniques. In Grouping multidimensional data. Springer,\n25–71.\n[13] Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. 2020. Spectral clustering with graph neural networks\nfor graph pooling. In International Conference on Machine Learning. PMLR, 874–883.\n[14] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast unfolding of commu-\nnities in large networks. Journal of statistical mechanics: theory and experiment 2008, 10 (2008), P10008.\n[15] Deyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui. 2020. Structural deep clustering network. In\nProceedings of The Web Conference 2020. 1400–1410.\n26\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\n[16] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel.\n2005. Protein function prediction via graph kernels. Bioinformatics 21, suppl_1 (2005), i47–i56.\n[17] Maria Brbić, Marinka Zitnik, Sheng Wang, Angela O Pisco, Russ B Altman, Spyros Darmanis, and Jure Leskovec.\n2020. MARS: discovering novel cell types across heterogeneous single-cell experiments. Nature methods 17, 12\n(2020), 1200–1206.\n[18] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877–1901.\n[19] Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. 2018. A systematic study of the class imbalance problem in\nconvolutional neural networks. Neural Networks 106 (2018), 249–259.\n[20] Feng Cao, Martin Ester, Weining Qian, and Aoying Zhou. 2006. Density-based clustering over an evolving data\nstream with noise. In Proceedings of the 2006 SIAM international conference on data mining. SIAM, 328–339.\n[21] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. 2018. Deep clustering for unsupervised\nlearning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV). 132–149.\n[22] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. 2020. Unsupervised\nlearning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems 33\n(2020), 9912–9924.\n[23] M Emre Celebi, Hassan A Kingravi, and Patricio A Vela. 2013. A comparative study of eﬃcient initialization methods\nfor the k-means clustering algorithm. Expert systems with applications 40, 1 (2013), 200–210.\n[24] Mete Çelik, Filiz Dadaşer-Çelik, and Ahmet Şakir Dokuz. 2011. Anomaly detection in temperature data using DB-\nSCAN algorithm. In 2011 international symposium on innovations in intelligent systems and applications. IEEE, 91–95.\n[25] Tania Cerquitelli, Evelina Di Corso, Francesco Ventura, and Silvia Chiusano. 2017. Data miners’ little helper: data\ntransformation activity cues for cluster analysis on document collections. In Proceedings of the 7th International\nConference on Web Intelligence, Mining and Semantics. 1–6.\n[26] Hsi-Cheng Chang and Chiun-Chieh Hsu. 2005. Using topic keyword clusters for automatic document clustering.\nIEICE TRANSACTIONS on Information and Systems 88, 8 (2005), 1852–1860.\n[27] Jianlong Chang, Yiwen Guo, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. 2019. Deep dis-\ncriminative clustering analysis. arXiv preprint arXiv:1905.01681 (2019).\n[28] Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. 2017. Deep adaptive image\nclustering. In Proceedings of the IEEE international conference on computer vision. 5879–5887.\n[29] Nenglun Chen, Lei Chu, Hao Pan, Yan Lu, and Wenping Wang. 2022. Self-Supervised Image Representation Learning\nwith Geometric Set Consistency. arXiv preprint arXiv:2203.15361 (2022).\n[30] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. 2020. A simple framework for contrastive\nlearning of visual representations. In International conference on machine learning. PMLR, 1597–1607.\n[31] Zhenguo Chen and Yong Fei Li. 2011. Anomaly detection based on enhanced DBScan algorithm. Procedia Engineering\n15 (2011), 178–182.\n[32] Kai-Yang Chiang, Joyce Jiyoung Whang, and Inderjit S Dhillon. 2012. Scalable clustering of signed networks us-\ning balance normalized cut. In Proceedings of the 21st ACM international conference on Information and knowledge\nmanagement. 615–624.\n[33] Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. 2019. Cluster-gcn: An eﬃcient algo-\nrithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining. 257–266.\n[34] Yu-An Chung, Wei-Ning Hsu, Hao Tang, and James Glass. 2019. An unsupervised autoregressive model for speech\nrepresentation learning. arXiv preprint arXiv:1904.03240 (2019).\n[35] Liam Close, Rasha Kashef, et al. 2020. Combining artiﬁcial immune system and clustering analysis: A stock market\nanomaly detection model. Journal of Intelligent Learning Systems and Applications 12, 04 (2020), 83.\n[36] Guy Barrett Coleman and Harry C Andrews. 1979. Image segmentation by clustering. Proc. IEEE 67, 5 (1979), 773–\n785.\n[37] Dorin Comaniciu and Peter Meer. 2002. Mean shift: A robust approach toward feature space analysis. IEEE Transac-\ntions on pattern analysis and machine intelligence 24, 5 (2002), 603–619.\n[38] Zhiyuan Dang, Cheng Deng, Xu Yang, and Heng Huang. 2021. Doubly contrastive deep clustering. arXiv preprint\narXiv:2103.05484 (2021).\n[39] Zhiyuan Dang, Cheng Deng, Xu Yang, Kun Wei, and Heng Huang. 2021. Nearest Neighbor Matching for Deep\nClustering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 13693–13702.\n[40] Phuong Dao, Recep Colak, Raheleh Salari, Flavia Moser, Elai Davicioni, Alexander Schönhuth, and Martin Ester. 2010.\nInferring cancer subnetwork markers using density-constrained biclustering. Bioinformatics 26, 18 (2010), i625–i631.\n27\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\n[41] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. 1991.\nStructure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molec-\nular orbital energies and hydrophobicity. Journal of medicinal chemistry 34, 2 (1991), 786–797.\n[42] Zhijie Deng, Yucen Luo, and Jun Zhu. 2019. Cluster alignment with a teacher for unsupervised domain adaptation.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision. 9944–9953.\n[43] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n[44] Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. 2007. Weighted graph cuts without eigenvectors a multilevel\napproach. IEEE transactions on pattern analysis and machine intelligence 29, 11 (2007), 1944–1957.\n[45] Nat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai Arulkumaran, and\nMurray Shanahan. 2016. Deep unsupervised clustering with gaussian mixture variational autoencoders.\narXiv\npreprint arXiv:1611.02648 (2016).\n[46] Kien Do, Truyen Tran, and Svetha Venkatesh. 2021. Clustering by maximizing mutual information across views. In\nProceedings of the IEEE/CVF International Conference on Computer Vision. 9928–9938.\n[47] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[48] Robert Dzisevič and Dmitrij Šešok. 2019. Text classiﬁcation using diﬀerent feature extraction approaches. In 2019\nOpen Conference of Electrical, Electronic and Information Sciences (eStream). IEEE, 1–4.\n[49] Gökcen Eraslan, Lukas M Simon, Maria Mircea, Nikola S Mueller, and Fabian J Theis. 2019. Single-cell RNA-seq\ndenoising using a deep count autoencoder. Nature communications 10, 1 (2019), 1–14.\n[50] Martin Ester. 2018. Density-based clustering. Data Clustering (2018), 111–127.\n[51] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. 1996. A density-based algorithm for discovering\nclusters in large spatial databases with noise.. In kdd, Vol. 96. 226–231.\n[52] Xiang Fang, Yuchong Hu, Pan Zhou, and Dapeng Oliver Wu. 2021. Unbalanced Incomplete Multi-view Clustering\nvia the Scheme of View Evolution: Weak Views are Meat; Strong Views do Eat. IEEE Transactions on Emerging Topics\nin Computational Intelligence (2021).\n[53] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. 2021. A large-scale study on unsu-\npervised spatiotemporal representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. 3299–3309.\n[54] Gary William Flake, Steve Lawrence, C Lee Giles, and Frans M Coetzee. 2002. Self-organization and identiﬁcation\nof web communities. Computer 35, 3 (2002), 66–70.\n[55] Santo Fortunato. 2010. Community detection in graphs. Physics reports 486, 3-5 (2010), 75–174.\n[56] Itziar Frades and Rune Matthiesen. 2010. Overview on techniques in cluster analysis. Bioinformatics methods in\nclinical research (2010), 81–107.\n[57] Yanhai Gan, Xinghui Dong, Huiyu Zhou, Feng Gao, and Junyu Dong. 2021. Learning the Precise Feature for Cluster\nAssignment. IEEE Transactions on Cybernetics (2021).\n[58] Byron J Gao, Obi L Griﬃth, Martin Ester, and Steven JM Jones. 2006. Discovering signiﬁcant opsm subspace clusters\nin massive gene expression data. In Proceedings of the 12th ACM SIGKDD international conference on knowledge\ndiscovery and data mining. 922–928.\n[59] Kamran Ghasedi, Xiaoqian Wang, Cheng Deng, and Heng Huang. 2019. Balanced self-paced learning for generative\nadversarial clustering network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n4391–4400.\n[60] Kamran Ghasedi Dizaji, Amirhossein Herandi, Cheng Deng, Weidong Cai, and Heng Huang. 2017. Deep cluster-\ning via joint convolutional autoencoder embedding and relative entropy minimization. In Proceedings of the IEEE\ninternational conference on computer vision. 5736–5745.\n[61] Michelle Girvan and Mark EJ Newman. 2002. Community structure in social and biological networks. Proceedings\nof the national academy of sciences 99, 12 (2002), 7821–7826.\n[62] Xavier Glorot and Yoshua Bengio. 2010. Understanding the diﬃculty of training deep feedforward neural networks.\nIn Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics. JMLR Workshop and\nConference Proceedings, 249–256.\n[63] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. 2014. Generative adversarial nets. Advances in neural information processing systems 27 (2014).\n[64] Ramu Govindasamy, Surendran Arumugam, Jingkun Zhuang, Kathleen M Kelley, and Isaac Vellangany. 2018. Cluster\nanalysis of wine market segmentation–a consumer based study in the mid-atlantic usa. Economic Aﬀairs 63, 1 (2018),\n151–157.\n28\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\n[65] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-\nsch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a new\napproach to self-supervised learning. Advances in Neural Information Processing Systems 33 (2020), 21271–21284.\n[66] Sudipto Guha, Rajeev Rastogi, and Kyuseok Shim. 1998. CURE: An eﬃcient clustering algorithm for large databases.\nACM Sigmod record 27, 2 (1998), 73–84.\n[67] Ting Guo, Jia Wu, Xingquan Zhu, and Chengqi Zhang. 2017. Combining structured node content and topology\ninformation for networked graph clustering. ACM Transactions on Knowledge Discovery from Data (TKDD) 11, 3\n(2017), 1–29.\n[68] Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. 2017. Improved Deep Embedded Clustering with Local\nStructure Preservation.. In Ijcai. 1753–1759.\n[69] Xifeng Guo, En Zhu, Xinwang Liu, and Jianping Yin. 2018. Deep embedded clustering with data augmentation. In\nAsian conference on machine learning. PMLR, 550–565.\n[70] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances\nin neural information processing systems 30 (2017).\n[71] Bryar A Hassan, Tarik A Rashid, and Hozan K Hamarashid. 2021. A novel cluster detection of COVID-19 patients\nand medical disease conditions using improved evolutionary clustering algorithm star. Computers in biology and\nmedicine 138 (2021), 104866.\n[72] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised\nvisual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.\n9729–9738.\n[73] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition. 770–778.\n[74] Geoﬀrey Hinton, Oriol Vinyals, JeﬀDean, et al. 2015. Distilling the knowledge in a neural network. arXiv preprint\narXiv:1503.02531 2, 7 (2015).\n[75] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua\nBengio. 2018. Learning deep representations by mutual information estimation and maximization. arXiv preprint\narXiv:1808.06670 (2018).\n[76] Thomas Hofmann. 2013. Probabilistic latent semantic analysis. arXiv preprint arXiv:1301.6705 (2013).\n[77] Chih-Chung Hsu and Chia-Wen Lin. 2017. Cnn-based joint clustering and representation learning with feature drift\ncompensation for large-scale image data. IEEE Transactions on Multimedia 20, 2 (2017), 421–429.\n[78] Jian Hu, Xiangjie Li, Gang Hu, Yafei Lyu, Katalin Susztak, and Mingyao Li. 2020. Iterative transfer learning with\nneural network for clustering and cell type classiﬁcation in single-cell RNA-seq analysis. Nature machine intelligence\n2, 10 (2020), 607–618.\n[79] Jiabo Huang and Shaogang Gong. 2021.\nDeep clustering by semantic contrastive learning.\narXiv preprint\narXiv:2103.02662 (2021).\n[80] Jiabo Huang, Shaogang Gong, and Xiatian Zhu. 2020. Deep semantic clustering by partition conﬁdence maximisation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8849–8858.\n[81] Peihao Huang, Yan Huang, Wei Wang, and Liang Wang. 2014. Deep embedding network for clustering. In 2014 22nd\nInternational conference on pattern recognition. IEEE, 1532–1537.\n[82] Zhizhong Huang, Jie Chen, Junping Zhang, and Hongming Shan. 2021. Exploring Non-Contrastive Representation\nLearning for Deep Clustering. arXiv preprint arXiv:2111.11821 (2021).\n[83] Abdul Jabbar, Xi Li, and Bourahla Omar. 2021. A survey on generative adversarial networks: Variants, applications,\nand training. ACM Computing Surveys (CSUR) 54, 8 (2021), 1–49.\n[84] Anil K Jain, M Narasimha Murty, and Patrick J Flynn. 1999. Data clustering: a review. ACM computing surveys (CSUR)\n31, 3 (1999), 264–323.\n[85] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. 2020. A\nsurvey on contrastive self-supervised learning. Technologies 9, 1 (2020), 2.\n[86] Deepak Jaiswal, Vikrant Kaushal, Pankaj Kumar Singh, and Abhijeet Biswas. 2020. Green market segmentation and\nconsumer proﬁling: a cluster approach to an emerging consumer market. Benchmarking: An International Journal\n(2020).\n[87] R Janani and S Vijayarani. 2019. Text document clustering using spectral clustering algorithm with particle swarm\noptimization. Expert Systems with Applications 134 (2019), 192–200.\n[88] Pan Ji, Tong Zhang, Hongdong Li, Mathieu Salzmann, and Ian Reid. 2017. Deep subspace clustering networks.\nAdvances in neural information processing systems 30 (2017).\n[89] Qiang Ji, Yanfeng Sun, Junbin Gao, Yongli Hu, and Baocai Yin. 2021. A Decoder-Free Variational Deep Embedding\nfor Unsupervised Clustering. IEEE Transactions on Neural Networks and Learning Systems (2021).\n29\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\n[90] Qiang Ji, Yanfeng Sun, Yongli Hu, and Baocai Yin. 2021. Variational Deep Embedding Clustering by Augmented\nMutual Information Maximization. In 2020 25th International Conference on Pattern Recognition (ICPR). IEEE, 2196–\n2202.\n[91] Xu Ji, Joao F Henriques, and Andrea Vedaldi. 2019. Invariant information clustering for unsupervised image classi-\nﬁcation and segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 9865–9874.\n[92] Yuting Jia, Qinqin Zhang, Weinan Zhang, and Xinbing Wang. 2019. Communitygan: Community detection with\ngenerative adversarial nets. In The World Wide Web Conference. 784–794.\n[93] Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. 2016. Variational deep embedding: An\nunsupervised and generative approach to clustering. arXiv preprint arXiv:1611.05148 (2016).\n[94] Di Jin, Zhizhi Yu, Pengfei Jiao, Shirui Pan, Dongxiao He, Jia Wu, Philip Yu, and Weixiong Zhang. 2021. A survey of\ncommunity detection approaches: From statistical modeling to deep learning. IEEE Transactions on Knowledge and\nData Engineering (2021).\n[95] Longlong Jing and Yingli Tian. 2020. Self-supervised visual feature learning with deep neural networks: A survey.\nIEEE transactions on pattern analysis and machine intelligence 43, 11 (2020), 4037–4058.\n[96] Stephen C Johnson. 1967. Hierarchical clustering schemes. Psychometrika 32, 3 (1967), 241–254.\n[97] Malvin H Kalos and Paula A Whitlock. 2009. Monte carlo methods. John Wiley & Sons.\n[98] Artúr István Károly, Róbert Fullér, and Péter Galambos. 2018. Unsupervised clustering for deep learning: A tutorial\nsurvey. Acta Polytechnica Hungarica 15, 8 (2018), 29–53.\n[99] Turkay Kart, Wenjia Bai, Ben Glocker, and Daniel Rueckert. 2021. DeepMCAT: Large-Scale Deep Clustering for\nMedical Image Categorization. In Deep Generative Models, and Data Augmentation, Labelling, and Imperfections.\nSpringer, 259–267.\n[100] George Karypis and Vipin Kumar. 1998. A fast and high quality multilevel scheme for partitioning irregular graphs.\nSIAM Journal on scientiﬁc Computing 20, 1 (1998), 359–392.\n[101] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu,\nand Dilip Krishnan. 2020. Supervised contrastive learning. Advances in Neural Information Processing Systems 33\n(2020), 18661–18673.\n[102] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013).\n[103] Justin B Kinney and Gurinder S Atwal. 2014. Equitability, mutual information, and the maximal information coeﬃ-\ncient. Proceedings of the National Academy of Sciences 111, 9 (2014), 3354–3359.\n[104] Thomas N Kipf and Max Welling. 2016. Semi-supervised classiﬁcation with graph convolutional networks. arXiv\npreprint arXiv:1609.02907 (2016).\n[105] Weize Kong, Michael Bendersky, Marc Najork, Brandon Vargo, and Mike Colagrosso. 2020. Learning to cluster\ndocuments into workspaces using large scale activity logs. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining. 2416–2424.\n[106] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. 2004. Estimating mutual information. Physical review\nE 69, 6 (2004), 066138.\n[107] Harold W Kuhn. 1955. The Hungarian method for the assignment problem. Naval research logistics quarterly 2, 1-2\n(1955), 83–97.\n[108] Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In International confer-\nence on machine learning. PMLR, 1188–1196.\n[109] Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. 2020. Contrastive representation learning: A framework and\nreview. IEEE Access 8 (2020), 193907–193934.\n[110] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haﬀner. 1998. Gradient-based learning applied to document\nrecognition. Proc. IEEE 86, 11 (1998), 2278–2324.\n[111] Dong-Hyun Lee et al. 2013. Pseudo-label: The simple and eﬃcient semi-supervised learning method for deep neural\nnetworks. In Workshop on challenges in representation learning, ICML, Vol. 3. 896.\n[112] Fengfu Li, Hong Qiao, and Bo Zhang. 2018. Discriminatively boosted image clustering with fully convolutional\nauto-encoders. Pattern Recognition 83 (2018), 161–173.\n[113] Haoyang Li, Xin Wang, Ziwei Zhang, Zehuan Yuan, Hang Li, and Wenwu Zhu. 2021. Disentangled Contrastive\nLearning on Graphs. Advances in Neural Information Processing Systems 34 (2021).\n[114] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. 2020. Prototypical contrastive learning of unsupervised\nrepresentations. arXiv preprint arXiv:2005.04966 (2020).\n[115] Xiang Li, Yao Wu, Martin Ester, Ben Kao, Xin Wang, and Yudian Zheng. 2017. Semi-supervised clustering in at-\ntributed heterogeneous information networks. In Proceedings of the 26th international conference on world wide web.\n1621–1629.\n[116] Xiang Li, Yao Wu, Martin Ester, Ben Kao, Xin Wang, and Yudian Zheng. 2020. Schain-iram: An eﬃcient and eﬀec-\ntive semi-supervised clustering algorithm for attributed heterogeneous information networks. IEEE Transactions on\n30\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\nknowledge and data engineering (2020).\n[117] Yutong Li, Juanjuan Cai, and Jingling Wang. 2020. A text document clustering method based on weighted Bert model.\nIn 2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC), Vol. 1.\nIEEE, 1426–1430.\n[118] Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. 2021. Contrastive clustering. In 2021\nAAAI Conference on Artiﬁcial Intelligence (AAAI).\n[119] Zhihan Li, Youjian Zhao, Haowen Xu, Wenxiao Chen, Shangqing Xu, Yilin Li, and Dan Pei. 2020. Unsupervised\nclustering through gaussian mixture variational autoencoder with non-reparameterized variational inference and\nstd annealing. In 2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 1–8.\n[120] XW Liang, AP Jiang, T Li, YY Xue, and GT Wang. 2020. LR-SMOTE—An improved unbalanced data set oversampling\nbased on K-means and SVM. Knowledge-Based Systems 196 (2020), 105845.\n[121] Fanzhen Liu, Shan Xue, Jia Wu, Chuan Zhou, Wenbin Hu, Cecile Paris, Surya Nepal, Jian Yang, and Philip S Yu. 2020.\nDeep learning for community detection: progress, challenges and opportunities. arXiv preprint arXiv:2005.08225\n(2020).\n[122] Hongfu Liu, Jun Li, Yue Wu, and Yun Fu. 2019. Clustering with outlier removal. IEEE transactions on knowledge and\ndata engineering 33, 6 (2019), 2369–2379.\n[123] Weiwei Liu, Haobo Wang, Xiaobo Shen, and Ivor Tsang. 2021. The emerging trends of multi-label learning. IEEE\ntransactions on pattern analysis and machine intelligence (2021).\n[124] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. 2021. Self-supervised\nlearning: Generative or contrastive. IEEE Transactions on Knowledge and Data Engineering (2021).\n[125] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on information theory 28, 2 (1982), 129–137.\n[126] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. 2015. Learning transferable features with deep\nadaptation networks. In International conference on machine learning. PMLR, 97–105.\n[127] Juncheng Lv, Zhao Kang, Xiao Lu, and Zenglin Xu. 2021. Pseudo-supervised deep subspace clustering. IEEE Trans-\nactions on Image Processing 30 (2021), 5252–5263.\n[128] Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. 2019. Disentangled graph convolutional networks.\nIn International conference on machine learning. PMLR, 4212–4221.\n[129] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and Leman Akoglu. 2021. A\ncomprehensive survey on graph anomaly detection with deep learning. IEEE Transactions on Knowledge and Data\nEngineering (2021).\n[130] Amir Markovitz, Gilad Sharir, Itamar Friedman, Lihi Zelnik-Manor, and Shai Avidan. 2020. Graph embedded pose\nclustering for anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion. 10539–10547.\n[131] Lukas Meier, Sara Van De Geer, and Peter Bühlmann. 2008. The group lasso for logistic regression. Journal of the\nRoyal Statistical Society: Series B (Statistical Methodology) 70, 1 (2008), 53–71.\n[132] Willi Menapace, Stéphane Lathuilière, and Elisa Ricci. 2020. Learning to cluster under domain shift. In European\nConference on Computer Vision. Springer, 736–752.\n[133] Yu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang, Chao Zhang, and Jiawei Han. 2020. Hierarchical topic mining via\njoint spherical tree and text embedding. In Proceedings of the 26th ACM SIGKDD international conference on knowledge\ndiscovery & data mining. 1908–1917.\n[134] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. 2013. Eﬃcient estimation of word representations in\nvector space. arXiv preprint arXiv:1301.3781 (2013).\n[135] Erxue Min, Xifeng Guo, Qiang Liu, Gen Zhang, Jianjing Cui, and Jun Long. 2018. A survey of clustering with deep\nlearning: From the perspective of network architecture. IEEE Access 6 (2018), 39501–39514.\n[136] Himanshu Mittal, Avinash Chandra Pandey, Raju Pal, and Ashish Tripathi. 2021. A new clustering method for the\ndiagnosis of CoVID19 using medical images. Applied Intelligence 51, 5 (2021), 2988–3011.\n[137] Flavia Moser, Rong Ge, and Martin Ester. 2007. Joint cluster analysis of attribute and relationship data withouta-\npriori speciﬁcation of the number of clusters. In Proceedings of the 13th ACM SIGKDD international conference on\nKnowledge discovery and data mining. 510–519.\n[138] Sudipto Mukherjee, Himanshu Asnani, Eugene Lin, and Sreeram Kannan. 2019. Clustergan: Latent space clustering\nin generative adversarial networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, Vol. 33. 4610–\n4617.\n[139] Mina Nasrazadani, Afsaneh Fatemi, and Mohammadali Nematbakhsh. 2022. Sign prediction in sparse social networks\nusing clustering and collaborative ﬁltering. The Journal of Supercomputing 78, 1 (2022), 596–615.\n[140] Andrew Ng, Michael Jordan, and Yair Weiss. 2001. On spectral clustering: Analysis and an algorithm. Advances in\nneural information processing systems 14 (2001).\n31\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\n[141] Chuang Niu, Hongming Shan, and Ge Wang. 2021. Spice: Semantic pseudo-labeling for image clustering. arXiv\npreprint arXiv:2103.09382 (2021).\n[142] Chuang Niu, Jun Zhang, Ge Wang, and Jimin Liang. 2020. Gatcluster: Self-supervised gaussian-attention network\nfor image clustering. In European Conference on Computer Vision. Springer, 735–751.\n[143] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. 2016. f-gan: Training generative neural samplers using\nvariational divergence minimization. Advances in neural information processing systems 29 (2016).\n[144] Foivos Ntelemis, Yaochu Jin, and Spencer A Thomas. 2021. Image clustering using an augmented generative adver-\nsarial network and information maximization. IEEE Transactions on Neural Networks and Learning Systems (2021).\n[145] Foivos Ntelemis, Yaochu Jin, and Spencer A Thomas. 2021. Information Maximization Clustering via Multi-View\nSelf-Labelling. arXiv preprint arXiv:2103.07368 (2021).\n[146] Gopi Chand Nutakki, Behnoush Abdollahi, Wenlong Sun, and Olfa Nasraoui. 2019. An introduction to deep cluster-\ning. In Clustering Methods for Big Data Analytics. Springer, 73–89.\n[147] Xavier Olive and Jérôme Morio. 2019. Trajectory clustering of air traﬃc ﬂows around airports. Aerospace Science\nand Technology 84 (2019), 776–781.\n[148] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding.\narXiv preprint arXiv:1807.03748 (2018).\n[149] Korawit Orkphol and Wu Yang. 2019. Sentiment analysis on microblogging with K-means clustering and artiﬁcial\nbee colony. International Journal of Computational Intelligence and Applications 18, 03 (2019), 1950017.\n[150] Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. 2018. Adversarially regularized graph\nautoencoder for graph embedding. arXiv preprint arXiv:1802.04407 (2018).\n[151] Sinno Jialin Pan and Qiang Yang. 2009. A survey on transfer learning. IEEE Transactions on knowledge and data\nengineering 22, 10 (2009), 1345–1359.\n[152] Hae-Sang Park and Chi-Hyuck Jun. 2009. A simple and fast algorithm for K-medoids clustering. Expert systems with\napplications 36, 2 (2009), 3336–3341.\n[153] Sungwon Park, Sungwon Han, Sundong Kim, Danu Kim, Sungkyu Park, Seunghoon Hong, and Meeyoung Cha.\n2021. Improving unsupervised image clustering with robust learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 12278–12287.\n[154] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V.\nDubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn:\nMachine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.\n[155] Bo Peng, Jianjun Lei, Huazhu Fu, Yalong Jia, Zongqian Zhang, and Yi Li. 2021. Deep video action clustering via\nspatio-temporal feature learning. Neurocomputing 456 (2021), 519–527.\n[156] Xi Peng, Jiashi Feng, Jiwen Lu, Wei-Yun Yau, and Zhang Yi. 2017. Cascade subspace clustering. In Thirty-First AAAI\nconference on artiﬁcial intelligence.\n[157] Xi Peng, Shijie Xiao, Jiashi Feng, Wei-Yun Yau, and Zhang Yi. 2016. Deep subspace clustering with sparsity prior.. In\nIJCAI. 1925–1931.\n[158] K Rajendra Prasad, Moulana Mohammed, and RM Noorullah. 2019. Hybrid topic cluster models for social healthcare\ndata. Int J Adv Comput Sci Appl 10, 11 (2019), 490–506.\n[159] Vignesh Prasad, Dipanjan Das, and Brojeshwar Bhowmick. 2020. Variational clustering: Leveraging variational\nautoencoders for image clustering. In 2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 1–10.\n[160] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. 2021.\nSpatiotemporal contrastive video representation learning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. 6964–6974.\n[161] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained models for\nnatural language processing: A survey. Science China Technological Sciences 63, 10 (2020), 1872–1897.\n[162] Carl Rasmussen. 1999. The inﬁnite Gaussian mixture model. Advances in neural information processing systems 12\n(1999).\n[163] Douglas A Reynolds. 2009. Gaussian mixture models. Encyclopedia of biometrics 741, 659-663 (2009).\n[164] Mina Rezaei, Emilio Dorigatti, David Ruegamer, and Bernd Bischl. 2021. Learning Statistical Representation with\nJoint Deep Embedded Clustering. arXiv preprint arXiv:2109.05232 (2021).\n[165] David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. 1985. Learning internal representations by error\npropagation. Technical Report. California Univ San Diego La Jolla Inst for Cognitive Science.\n[166] Hossein Saeedi Emadi and Sayyed Majid Mazinani. 2018. A novel anomaly detection algorithm using DBSCAN and\nSVM in wireless sensor networks. Wireless Personal Communications 98, 2 (2018), 2025–2035.\n[167] Jörg Sander, Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu. 1998. Density-based clustering in spatial databases:\nThe algorithm gdbscan and its applications. Data mining and knowledge discovery 2, 2 (1998), 169–194.\n32\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\n[168] Erich Schubert, Jörg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu. 2017. DBSCAN revisited, revisited:\nwhy and how you should (still) use DBSCAN. ACM Transactions on Database Systems (TODS) 42, 3 (2017), 1–21.\n[169] Sohil Atul Shah and Vladlen Koltun. 2017. Robust continuous clustering. Proceedings of the National Academy of\nSciences 114, 37 (2017), 9814–9819.\n[170] Sohil Atul Shah and Vladlen Koltun. 2018. Deep continuous clustering. arXiv preprint arXiv:1803.01449 (2018).\n[171] Yuming Shen, Ziyi Shen, Menghan Wang, Jie Qin, Philip Torr, and Ling Shao. 2021. You never cluster alone. Advances\nin Neural Information Processing Systems 34 (2021).\n[172] Ying Shen, Qiang Zhang, Jin Zhang, Jiyue Huang, Yuming Lu, and Kai Lei. 2018. Improving medical short text\nclassiﬁcation with semantic expansion using word-cluster embedding. In International Conference on Information\nScience and Applications. Springer, 401–411.\n[173] Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis\nand machine intelligence 22, 8 (2000), 888–905.\n[174] Julian Shun, Farbod Roosta-Khorasani, Kimon Fountoulakis, and Michael W Mahoney. 2016. Parallel local graph\nclustering. arXiv preprint arXiv:1604.07515 (2016).\n[175] Chunfeng Song, Feng Liu, Yongzhen Huang, Liang Wang, and Tieniu Tan. 2013. Auto-encoder based data clustering.\nIn Iberoamerican congress on pattern recognition. Springer, 117–124.\n[176] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. 2015. Unsupervised learning of video representations\nusing lstms. In International conference on machine learning. PMLR, 843–852.\n[177] Michael Steinbach, George Karypis, and Vipin Kumar. 2000.\nA comparison of document clustering techniques.\n(2000).\n[178] Xing Su, Shan Xue, Fanzhen Liu, Jia Wu, Jian Yang, Chuan Zhou, Wenbin Hu, Cecile Paris, Surya Nepal, Di Jin, et al.\n2022. A comprehensive survey on community detection with deep learning. IEEE Transactions on Neural Networks\nand Learning Systems (2022).\n[179] Ramakrishnan Sundareswaran, Jansel Herrera-Gerena, John Just, and Ali Jannesari. 2021. Cluster Analysis with\nDeep Embeddings and Contrastive Learning. arXiv preprint arXiv:2109.12714 (2021).\n[180] Hui Tang, Ke Chen, and Kui Jia. 2020. Unsupervised domain adaptation via structurally regularized deep clustering.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8725–8735.\n[181] Yaling Tao, Kentaro Takagi, and Kouta Nakata. 2021. Clustering-friendly representation learning via instance dis-\ncrimination and feature decorrelation. arXiv preprint arXiv:2106.00131 (2021).\n[182] Karpagalingam Thirumoorthy and Karuppaiah Muneeswaran. 2021. A hybrid approach for text document clustering\nusing Jaya optimization algorithm. Expert Systems with Applications 178 (2021), 115040.\n[183] Fei Tian, Bin Gao, Qing Cui, Enhong Chen, and Tie-Yan Liu. 2014. Learning deep representations for graph clustering.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, Vol. 28.\n[184] Tian Tian, Ji Wan, Qi Song, and Zhi Wei. 2019. Clustering single-cell RNA-seq data with a model-based deep learning\napproach. Nature Machine Intelligence 1, 4 (2019), 191–198.\n[185] Tsung Wei Tsai, Chongxuan Li, and Jun Zhu. 2020. Mice: Mixture of contrastive experts for unsupervised image\nclustering. In International Conference on Learning Representations.\n[186] Yuen-Hsien Tseng. 2010. Generic title labeling for clustered documents. Expert Systems with Applications 37, 3 (2010),\n2247–2254.\n[187] Anton Tsitsulin, John Palowitch, Bryan Perozzi, and Emmanuel Müller. 2020. Graph clustering with graph neural\nnetworks. arXiv preprint arXiv:2006.16904 (2020).\n[188] Muhammad Usman, Mian Ahmad Jan, Xiangjian He, and Jinjun Chen. 2019. A survey on representation learning\neﬀorts in cybersecurity domain. ACM Computing Surveys (CSUR) 52, 6 (2019), 1–28.\n[189] Laurens Van der Maaten and Geoﬀrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning\nresearch 9, 11 (2008).\n[190] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. 2020. Scan:\nLearning to classify images without labels. In European Conference on Computer Vision. Springer, 268–285.\n[191] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph\nattention networks. arXiv preprint arXiv:1710.10903 (2017).\n[192] Ehsan Elhamifar René Vidal. 2009. Sparse subspace clustering. In 2009 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), vol. 00, Vol. 6. 2790–2797.\n[193] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and computing 17, 4 (2007), 395–416.\n[194] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In Proceedings of the 22nd ACM\nSIGKDD international conference on Knowledge discovery and data mining. 1225–1234.\n[195] Shoujin Wang, Wei Liu, Jia Wu, Longbing Cao, Qinxue Meng, and Paul J Kennedy. 2016. Training deep neural\nnetworks on imbalanced data sets. In 2016 international joint conference on neural networks (IJCNN). IEEE, 4368–\n4374.\n33\nACM Computing Survey, Manuscript\nZhou and Xu, et al.\n[196] Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and\nuniformity on the hypersphere. In International Conference on Machine Learning. PMLR, 9929–9939.\n[197] Wenqing Wang, Junpeng Bao, and Siyao Guo. 2022. Neural generative model for clustering by separating particu-\nlarity and commonality. Information Sciences 589 (2022), 813–826.\n[198] Xin Wang, Shuyi Fan, Kun Kuang, and Wenwu Zhu. 2021. Explainable automated graph representation learning\nwith hyperparameter importance. In International Conference on Machine Learning. PMLR, 10727–10737.\n[199] Yifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. 2021. Chaos is a ladder: A new understanding\nof contrastive learning. In International Conference on Learning Representations.\n[200] Ziming Wang, Yuexian Zou, and Zeming Zhang. 2020. Cluster attention contrast for video anomaly detection. In\nProceedings of the 28th ACM International Conference on Multimedia. 2463–2471.\n[201] S Wibisono, MT Anwar, A Supriyanto, and IHA Amin. 2021. Multivariate weather anomaly detection using DBSCAN\nclustering algorithm. In Journal of Physics: Conference Series, Vol. 1869. IOP Publishing, 012077.\n[202] Jianlong Wu, Keyu Long, Fei Wang, Chen Qian, Cheng Li, Zhouchen Lin, and Hongbin Zha. 2019. Deep comprehen-\nsive correlation mining for image clustering. In Proceedings of the IEEE/CVF International Conference on Computer\nVision. 8150–8159.\n[203] Jian Wu, Victor S Sheng, Jing Zhang, Hua Li, Tetiana Dadakova, Christine Leon Swisher, Zhiming Cui, and Pengpeng\nZhao. 2020. Multi-label active learning algorithms for image classiﬁcation: Overview and future promise. ACM\nComputing Surveys (CSUR) 53, 2 (2020), 1–35.\n[204] Jia Wu, Xingquan Zhu, Chengqi Zhang, and S Yu Philip. 2014. Bag constrained structure pattern mining for multi-\ngraph classiﬁcation. Ieee transactions on knowledge and data engineering 26, 10 (2014), 2382–2396.\n[205] Yao Wu, Xudong Liu, Min Xie, Martin Ester, and Qing Yang. 2016. CCCF: Improving collaborative ﬁltering via\nscalable user-item co-clustering. In Proceedings of the ninth ACM international conference on web search and data\nmining. 73–82.\n[206] Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016. Unsupervised deep embedding for clustering analysis. In Inter-\nnational conference on machine learning. PMLR, 478–487.\n[207] Jierui Xie, Stephen Kelley, and Boleslaw K Szymanski. 2013. Overlapping community detection in networks: The\nstate-of-the-art and comparative study. Acm computing surveys (csur) 45, 4 (2013), 1–35.\n[208] Dongkuan Xu and Yingjie Tian. 2015. A comprehensive survey of clustering algorithms. Annals of Data Science 2, 2\n(2015), 165–193.\n[209] Rui Xu and Donald Wunsch. 2005. Survey of clustering algorithms. IEEE Transactions on neural networks 16, 3 (2005),\n645–678.\n[210] Xiaowei Xu, Martin Ester, H-P Kriegel, and Jörg Sander. 1998. A distribution-based clustering algorithm for mining\nin large spatial databases. In Proceedings 14th International Conference on Data Engineering. IEEE, 324–331.\n[211] Shan Xue, Jie Lu, Jia Wu, Guangquan Zhang, and Li Xiong. 2016. Multi-instance graphical transfer clustering for\ntraﬃc data learning. In 2016 International Joint Conference on Neural Networks (IJCNN). IEEE, 4390–4395.\n[212] Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. 2017. Towards k-means-friendly spaces: Simultaneous\ndeep learning and clustering. In international conference on machine learning. PMLR, 3861–3870.\n[213] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi Parikh. 2018. Graph r-cnn for scene graph generation. In\nProceedings of the European conference on computer vision (ECCV). 670–685.\n[214] Jianwei Yang, Devi Parikh, and Dhruv Batra. 2016. Joint unsupervised learning of deep representations and image\nclusters. In Proceedings of the IEEE conference on computer vision and pattern recognition. 5147–5156.\n[215] Linxiao Yang, Ngai-Man Cheung, Jiaying Li, and Jun Fang. 2019. Deep clustering by gaussian mixture variational\nautoencoders with graph embedding. In Proceedings of the IEEE/CVF International Conference on Computer Vision.\n6440–6449.\n[216] Faliu Yi and Inkyu Moon. 2012. Image segmentation: A survey of graph-cut methods. In 2012 international conference\non systems and informatics (ICSAI2012). IEEE, 1936–1941.\n[217] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. 2020. Graph contrastive\nlearning with augmentations. Advances in Neural Information Processing Systems 33 (2020), 5812–5823.\n[218] Yang Yu and Wen-Ji Zhou. 2018. Mixture of GANs for Clustering.. In IJCAI. 3047–3053.\n[219] Lin Yue, Weitong Chen, Xue Li, Wanli Zuo, and Minghao Yin. 2019. A survey of sentiment analysis in social media.\nKnowledge and Information Systems 60, 2 (2019), 617–663.\n[220] Junhai Zhai, Sufang Zhang, Junfen Chen, and Qiang He. 2018. Autoencoder and its various variants. In 2018 IEEE\nInternational Conference on Systems, Man, and Cybernetics (SMC). IEEE, 415–419.\n[221] Yunpeng Zhai, Shijian Lu, Qixiang Ye, Xuebo Shan, Jie Chen, Rongrong Ji, and Yonghong Tian. 2020. Ad-cluster:\nAugmented discriminative clustering for domain adaptive person re-identiﬁcation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 9021–9030.\n34\nA Comprehensive Survey on Deep Clustering:\nTaxonomy, Challenges, and Future Directions\nACM Computing Survey, Manuscript\n[222] Dejiao Zhang, Feng Nan, Xiaokai Wei, Shangwen Li, Henghui Zhu, Kathleen McKeown, Ramesh Nallapati, Andrew\nArnold, and Bing Xiang. 2021. Supporting clustering with contrastive learning. arXiv preprint arXiv:2103.12953\n(2021).\n[223] Hongjing Zhang and Ian Davidson. 2021. Deep Descriptive Clustering. arXiv preprint arXiv:2105.11549 (2021).\n[224] Jiong Zhang, Wei-cheng Chang, Hsiang-fu Yu, and Inderjit Dhillon. 2021. Fast multi-resolution transformer ﬁne-\ntuning for extreme multi-label text classiﬁcation. Advances in Neural Information Processing Systems 34 (2021).\n[225] Junjian Zhang, Chun-Guang Li, Chong You, Xianbiao Qi, Honggang Zhang, Jun Guo, and Zhouchen Lin. 2019. Self-\nsupervised convolutional subspace clustering network. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 5473–5482.\n[226] Min-Ling Zhang and Zhi-Hua Zhou. 2013. A review on multi-label learning algorithms. IEEE transactions on knowl-\nedge and data engineering 26, 8 (2013), 1819–1837.\n[227] Qin Zhang, Jia Wu, Hong Yang, Yingjie Tian, and Chengqi Zhang. 2016. Unsupervised Feature Learning from Time\nSeries.. In IJCAI. New York, USA, 2322–2328.\n[228] Qin Zhang, Jia Wu, Peng Zhang, Guodong Long, and Chengqi Zhang. 2018. Salient subsequence learning for time\nseries clustering. IEEE transactions on pattern analysis and machine intelligence 41, 9 (2018), 2193–2207.\n[229] Shangzhi Zhang, Chong You, René Vidal, and Chun-Guang Li. 2021. Learning a self-expressive network for subspace\nclustering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12393–12403.\n[230] Tong Zhang, Pan Ji, Mehrtash Harandi, Wenbing Huang, and Hongdong Li. 2019. Neural collaborative subspace\nclustering. In International Conference on Machine Learning. PMLR, 7384–7393.\n[231] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. 1996. BIRCH: an eﬃcient data clustering method for very large\ndatabases. ACM sigmod record 25, 2 (1996), 103–114.\n[232] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhao Li, and Can Wang. 2020.\nLearning\ntemporal interaction graph embedding via coupled memory networks. In Proceedings of the web conference 2020.\n3049–3055.\n[233] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang. 2019. Hierarchical\ngraph pooling with structure learning. arXiv preprint arXiv:1911.05954 (2019).\n[234] Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2020. Deep learning on graphs: A survey. IEEE Transactions on Knowledge\nand Data Engineering (2020).\n[235] Zhen Zhang, Hongxia Yang, Jiajun Bu, Sheng Zhou, Pinggang Yu, Jianwei Zhang, Martin Ester, and Can Wang. 2018.\nANRL: Attributed Network Representation Learning via Deep Neural Networks.. In Ijcai, Vol. 18. 3155–3161.\n[236] Huasong Zhong, Chong Chen, Zhongming Jin, and Xian-Sheng Hua. 2020. Deep robust clustering by contrastive\nlearning. arXiv preprint arXiv:2008.03030 (2020).\n[237] Huasong Zhong, Jianlong Wu, Chong Chen, Jianqiang Huang, Minghua Deng, Liqiang Nie, Zhouchen Lin, and Xian-\nSheng Hua. 2021. Graph contrastive clustering. In Proceedings of the IEEE/CVF International Conference on Computer\nVision. 9224–9233.\n[238] Jingya Zhou, Ling Liu, Wenqi Wei, and Jianxi Fan. 2022. Network Representation Learning: From Preprocessing,\nFeature Extraction to Node Embedding. ACM Computing Surveys (CSUR) 55, 2 (2022), 1–35.\n[239] Pan Zhou, Yunqing Hou, and Jiashi Feng. 2018. Deep adversarial subspace clustering. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. 1596–1604.\n[240] Qiang Zhou, Shirui Wang, et al. 2021. Cluster adaptation networks for unsupervised domain adaptation. Image and\nVision Computing 108 (2021), 104137.\n[241] Sheng Zhou, Jiajun Bu, Xin Wang, Jiawei Chen, and Can Wang. 2019. HAHE: Hierarchical attentive heterogeneous\ninformation network embedding. arXiv preprint arXiv:1902.01475 (2019).\n[242] Sheng Zhou, Jiajun Bu, Zhen Zhang, Can Wang, Lingzhou Ma, and Jianfeng Zhang. 2020. Cross multi-type objects\nclustering in attributed heterogeneous information network. Knowledge-Based Systems 194 (2020), 105458.\n[243] Sheng Zhou, Xin Wang, Jiajun Bu, Martin Ester, Pinggang Yu, Jiawei Chen, Qihao Shi, and Can Wang. 2020. DGE:\nDeep generative network embedding based on commonality and individuality. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, Vol. 34. 6949–6956.\n[244] Sheng Zhou, Yucheng Wang, Defang Chen, Jiawei Chen, Xin Wang, Can Wang, and Jiajun Bu. 2021. DistillingHolistic\nKnowledge With Graph Neural Networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision\n(ICCV). 10387–10396.\n[245] Linhong Zhu, Aram Galstyan, James Cheng, and Kristina Lerman. 2014. Tripartite graph clustering for dynamic\nsentiment analysis on social media. In Proceedings of the 2014 ACM SIGMOD international conference on Management\nof data. 1531–1542.\n[246] Wenwu Zhu, Xin Wang, and Peng Cui. 2020. Deep learning for learning graph representations. In Deep learning:\nconcepts and architectures. Springer, 169–210.\n35\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-06-15",
  "updated": "2022-06-15"
}