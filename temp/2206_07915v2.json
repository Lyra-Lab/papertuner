{
  "id": "http://arxiv.org/abs/2206.07915v2",
  "title": "Barrier Certified Safety Learning Control: When Sum-of-Square Programming Meets Reinforcement Learning",
  "authors": [
    "Hejun Huang",
    "Zhenglong Li",
    "Dongkun Han"
  ],
  "abstract": "Safety guarantee is essential in many engineering implementations.\nReinforcement learning provides a useful way to strengthen safety. However,\nreinforcement learning algorithms cannot completely guarantee safety over\nrealistic operations. To address this issue, this work adopts control barrier\nfunctions over reinforcement learning, and proposes a compensated algorithm to\ncompletely maintain safety. Specifically, a sum-of-squares programming has been\nexploited to search for the optimal controller, and tune the learning\nhyperparameters simultaneously. Thus, the control actions are pledged to be\nalways within the safe region. The effectiveness of proposed method is\ndemonstrated via an inverted pendulum model. Compared to quadratic programming\nbased reinforcement learning methods, our sum-of-squares programming based\nreinforcement learning has shown its superiority.",
  "text": "Barrier Certiﬁed Safety Learning Control:\nWhen Sum-of-Square Programming Meets Reinforcement Learning\nHejun Huang1, Zhenglong Li2, Dongkun Han1∗\nAbstract— Safety guarantee is essential in many engineering\nimplementations. Reinforcement learning provides a useful\nway to strengthen safety. However, reinforcement learning\nalgorithms cannot completely guarantee safety over realistic op-\nerations. To address this issue, this work adopts control barrier\nfunctions over reinforcement learning, and proposes a compen-\nsated algorithm to completely maintain safety. Speciﬁcally, a\nsum-of-squares programming has been exploited to search for\nthe optimal controller, and tune the learning hyperparameters\nsimultaneously. Thus, the control actions are pledged to be\nalways within the safe region. The effectiveness of proposed\nmethod is demonstrated via an inverted pendulum model.\nCompared to quadratic programming based reinforcement\nlearning methods, our sum-of-squares programming based\nreinforcement learning has shown its superiority.\nI. INTRODUCTION\nReinforcement learning (RL) is one of the most popular\nmethods for accomplishing the long-term objective of a\nMarkov-decision process [1]–[6]. This learning method aims\nfor an optimal policy that can maximize a long-term reward\nin a given environment recursively. For the case of RL\npolicy gradient method [7]–[9], this reward-related learning\nproblem is traditionally addressed using gradient descent to\nobtain superior control policies.\nRL algorithms might generate desired results in some\nacademic trials, e.g., robotic experiments [10], [11] and\nautonomous driving contexts [12], [13]. But for realistic\nsituations, how to completely guarantee the safety based on\nthese RL policies is still a problem. The intermediate policies\nfrom the RL agent’s exploration and exploitation processes\nmay sometimes ruin the environment or the agent itself,\nwhich is unacceptable for safety-critical systems. Moreover,\ndisturbances around the system would confuse the agent over\nlearning process, which make the control tasks more complex\nand challenging.\nTo cope with the issues above, various methods are\nproposed to specify dynamical safety during the learning\nprocess. Initially, the region of attraction (ROA) from control\nLyapunov functions or Lyapunov-like function work [14],\n[15] are used to certify the dynamical safety. Related and\nuseful work of ROA estimation are introduced in [16]–[19].\nIn 2017, [20] started to execute the RL’s exploration and\nexploitation in ROA to maintain safety. Different from region\nconstraint, in 2018 [21], [22] proposed a shield framework\nto select safe actions repeatedly. Inspired by this work, [23],\n1Hejun Huang and Dongkun Han are with the Department of Mechanical\nand Automation Engineering, The Chinese University of Hong Kong\n2Zhenglong Li is with the Department of Electrical and Electronic\nEngineering, The University of Hong Kong\n*Email: dkhan@mae.cuhk.edu.hk\n[24] further combined the control barrier function (CBF) to\nsynthesis the RL-based controller. Note that, the CBF is not\nnew to identify safe regions in control theory, [25], [26] are\nrepresentative work to combine CBF in biped robot and [27]\nexplored its implementation of quadrotors, etc. Appropriate\nregion constraint for RL is important for building safe RL.\nHence, a reliable learning method is expected to construct a\ncontroller inside the safe region over the process.\nIn this paper, we will consider a state-space model in the\npolynomial form, including an unknown term that estimated\nby Gaussian process. Then, we will apply Deep Deterministic\nPolicy Gradient (DDPG) algorithm to design a controller.\nThe generated control action will impact system dynam-\nics and further inﬂuence the sum-of-squares programming\n(SOSP) solution of control barrier functions.\nThe main contributions of this work are: (1) Formulate\nthe framework to embed SOSP-based CBF into DDPG algo-\nrithm; (2) compare the performance of quadratic program-\nming (QP)-based and SOSP-based controller with DDPG\nalgorithm; and (3) demonstrate the safety and efﬁciency of\nDDPG-SOSP algorithm via an inverted pendulum model.\nThis paper is organized as follows, we ﬁrst present some\npreliminaries about reinforcement learning, Gaussian pro-\ncess and control barrier function in Section II. Then, in\nSection III, we formulate the steps to compute an SOSP-\nbased controller and introduce a corresponding DDPG-SOSP\nalgorithm. Numerical examples are given for these cases in\nSection IV, then we discuss the learning efﬁciency and safety\nperformance of QP-based and SOSP-based strategies, before\nconcluding this paper.\nII. PRELIMINARY\nA discrete-time nonlinear control-afﬁne system with state\nst, st+1 ∈S and control action at ∈A is considered as:\nst+1 = f(st) + g(st)at + d(st), ∀t ∈[t, t],\n(1)\nwhere st, st+1 are ﬁnished within a bounded time t, while t\nand t are constants.\nNotation: From the inﬁnite-horizon discounted Markov\ndecision process (MDP) theory [28], let r : S × A →R de-\nnote the reward of the current state-action pair (st, at) in (1),\nlet P(st, st+1, at) = p(st+1|st, at) denote the probability\ndistribution to next state st+1, and let γ ∈(0, 1) denote a dis-\ncount factor. Thus, a MDP tuple τ = (st, st+1, at, rt, Pt, γ)\nestablishes at each state t.\narXiv:2206.07915v2  [eess.SY]  29 Jun 2022\nA. Reinforcement Learning\nRL focuses on training an agent to map various situations\nto the most valuable actions in a long-term process. During\nthe process, the agent will be inspired by a cumulative reward\nr to ﬁnd the best policy π(a|s). Various novel RL algorithms\nare proposed to select optimal π(a|s). For more details of\nselecting π, we kindly refer interested readers to [1].\nGiven MDP tuple τ, the RL agent is selecting an optimal\npolicy π∗by maximizing the expected reward J(π)\nJ(π) = Eτ[\n∞\nX\nt=0\nγtr(st, at)].\n(2)\nFurthermore, the action value function Qπ can be generated\nfrom τ and satisﬁes the Bellman equation as follows,\nQπ(st, at) = Est,at[rt + γEat+1[Qπ(st+1, at+1)]]\n= Est+1,at+1,...[\n∞\nX\nl=0\nγlr(st+l, at+l)].\n(3)\nIn this paper, we will use DDPG algorithm, a typical\nactor-critic and off-policy method, to handle the concerned\ndynamics of (1). Let parameters θπ and θQ denote the neural\nnetwork of actor and critic, respectively. By interacting with\nthe environment repeatedly, the actor will generate an actor\npolicy π : S →A and be evaluated by the critic via\nQπ : S × A →R. These actions will be stored in replay\nbuffer to update the policy gradient w.r.t θπ, and the loss\nfunction w.r.t θQ, once if the replay buffer is fulﬁlled.\nHowever, the state-action of DDPG algorithm generates\nwithout a complete safety guarantee, which is unacceptable\nin safety critical situations. Inspired by the work of [23],\nwe propose a programming assist method to maintain safety\nover the learning process in Section III.\nB. Gaussian Process\nGaussian processes (GP) estimate the system and further\npredict dynamics based on the prior data. A GP is a stochastic\nprocess that construct a joint Gaussian distribution with\nconcerned states {s1, s2, . . . } ⊂S. We use GP to estimate\nthe unknown term d in (1) during the system implementation.\nMixed with an independent Gaussian noise (0, σ2\nn), the GP\nmodel’s prior data {d0, d1, . . . , dk} can be computed from\ndk = sk+1 −f −g · ak indirectly. Then, a high probability\nstatement of the posterior output ˆd : S →R generates as\nd(s) ∼N(md(s), σd(s)),\n(4)\nwhere the mean function md(s) and the variance function\nσd(s) can describe the posterior distribution as\nmd(s) −kδσd(s) ≤d(s) ≤md(s) + kδσd(s),\n(5)\nwith probability greater or equal to (1 −δ)k, δ ∈(0, 1)\n[29], where kδ is a parameter to design interval [(1−δ)k, 1].\nTherefore, by learning a ﬁnite number of data points, we can\nestimate the unknown term d(s∗) of any query state s∗∈S.\nAs the following lemma declaims in [30], [31], there exists\na polynomial mean function to approximate the term d via\nGP, which can predict the output of state s∗straightly here.\nLemma 1. Suppose we have access to k measurements of\nd(s) in (1) that corrupted with Gaussian noise (0, σ2\nn). If the\nnorm unknown term d(s) bounded, the following GP model\nof d(s) can be established with polynomial mean function\nmd(s∗) and covariance function σ2\nd(s∗),\nmd(s∗) = ϕ(s∗)Tw,\nσ2\nd(s∗) = k(s∗, s∗) −kT\n∗(K + σ2\nnI)−1k∗,\n(6)\nwithin probability bounds [(1−δ)k, 1], where δ ∈(0, 1), s∗is\na query state, ϕ(s∗) is a monomial vector, w is a coefﬁcient\nvector, [K](i,j) = k(si, sj) is a kernel Gramian matrix and\nk∗= [k(s1, s∗), k(s2, s∗), . . . , k(sk, s∗)]T.\nLemma 1 generates a polynomial expression of d(s)\nwithin probabilistic range [(1 −δ)k, 1]. Meanwhile, the non-\nlinear term f(s) in (1) can be approximated by Chebyshev\ninterpolants Pk(s) and bounded remainder ξ(s) in certain\ndomain as f(s) = Pk(s) + ξ(s) [32], where k is the degree\nof the polynomial Pk(x) [32]. Now we convert (1) as\nst+1 = Pk(st) + g(st)at + dξ(st).\n(7)\nThe polynomial (7) is equal to (1) with a new term dξ(st) =\nd(st) + ξ(st). Consequently, we can obtain a polynomial\nsystem within a probability range by learning dξ(st),\nst+1 = Pk(st) + g(st)at + mdξ(st),\n(8)\nwhich will be used to synthesis the controller in Section III.\nC. Control Barrier Function\nThe super-level set of the control barrier function (CBF)\nh : S →R could validate an safety set C as\nC = {st ∈S : h(st) ≥0}.\n(9)\nThroughout this paper, we refer to C as a safe region and\nU = S −C = {st ∈S : h(st) < 0},\n(10)\nas unsafe regions of the dynamical system (8). Then, the\nstate st ∈C will not enter into U by satisfying the forward\ninvariant constraint below,\n∀st ∈C : h(st) ≥0, ∆h(st, at) ≥0,\n(11)\nwhere ∆h(st, at) = h(st+1)−h(st). Before we demonstrate\nthe computation of h of the system (8), the Positivestellensatz\n(P-satz) needs to be introduced ﬁrst [33].\nLet P be the set of polynomials and PSOS be the set of sum\nof squares polynomials, e.g., P(x) = Pk\ni=1 p2\ni (x), where\npi(x) ∈P and P(x) ∈PSOS.\nLemma 2. For polynomials {ai}m\ni=1, {bj}n\nj=1 and p, deﬁne\na set B = {s ∈S : {ai(s)}m\ni=1 = 0, {bj(s)}n\nj=1 ≥0}. Let\nB be compact. The condition ∀x ∈S, p(s) ≥0 holds if the\nfollowing condition is satisﬁed,\n\u001a ∃r1, . . . , rm ∈P, s1, . . . , sn ∈PSOS,\np −Pm\ni=1 riai −Pn\nj=1 sjbj ∈PSOS.\n□\nLemma 2 points out that any strictly positive polynomial p\nlies in the cone that generated by non-negative polynomials\nFig. 1.\nWorkﬂow of the safe RL control with SOS program\n{bj}n\nj=1 in the set of B. Based on the deﬁnition of C and U,\nP-satz will be adequately used in the safety veriﬁcation.\nThe sufﬁcient condition to establish a CBF is the existence\nof a deterministic controller aCBF : S →A. Based on\nLemma 2, we can compute a polynomial controller aCBF\nthrough SOS program to avoid entering into unsafe regions\nµi(s) ∈U, i = 1, 2, . . . , n at each step t.\nLemma 3. Provided a safe region C certiﬁed by the polyno-\nmial barrier function h and some unsafe regions µi ∈U, i ∈\nZ+, in a polynomial system (8), if there exists a controller\naCBF that satisﬁes\na∗\n=\narg min ∥aCBF ∥2\nst∈S,aCBF ∈A;L(st),Mi(st)∈PSOS;\ns.t.\n∆h(st, aCBF ) −L(st)h(st) ∈PSOS,\n−∆h(st, aCBF ) −Mi(st)µi(st) ∈PSOS,\n(12)\nwhere L(st) and Mi(st) are SOS polynomials for i =\n1, 2, . . . , n. Then, action a∗is a minimum polynomial con-\ntroller that regulates the system (8) safely.\nProof. Let us suppose that there exists a CBF h that certiﬁed\na safe region C. Then, a deterministic input at = aCBF (st) is\nneeded to establish the necessary condition ∆h(st, aCBF ) ≥\n0 in (11) of the system (8).\nSince h is a polynomial function, if aCBF is also a poly-\nnomial, ∆h will maintain in the polynomial form. According\nto Lemma 2, we can formulate this non-negative condition\nof ∆h as part of SOS constraint in the cone of h as\n∆h(st, aCBF ) −L(st)h(st) ∈PSOS,\n(13)\nwhere the auxiliary SOS polynomials L(st) is used to project\nthe related term into an non-negative compact domain C that\ncertiﬁed by the superlevel set of h(st).\nWe continue to leverage the P-satz again to drive the\ndynamics of (8) far from unsafe areas µi(st) as follows,\n−∆h(st, aCBF ) −µi(st)Mi(st) ∈PSOS,\n(14)\nwhere Mi(st) denote the corresponding SOS polynomials.\nGuided by the optimization (12), we can solve a minimal\ncost control to maintain system safety with (13) and (14),\nwhich completes the proof.\nThis lemma maintains the safety of polynomial system\n(8) and minimizes the current control aCBF with a SOS\nprogram. Multiple unsafe regions µi(st) are also considered\nin solving the target input a∗in (12). Since Lemma 1\ndiscussed the probabilistic relationship of (8) and (1), the\ncomputed result of (12) of (1) can also be a feasible control\nto regulate the true dynamics safely.\nIII. CBF GUIDING CONTROL WITH REINFORCEMENT\nLEARNING\nIn the ﬁrst part of this section, we will further express\nthe computation steps of (12). In the second part, we will\ngo through the details about the SOSP-based CBF guiding\ncontrol with DDPG algorithm.\nA. SOS Program Based CBF\nDifferent from [23], our control aCBF is solved by SOS\nprogram with polynomial barrier functions, rather than the\nQP and linear barrier functions. In [23], they initially con-\nstructed QP with monotonous linear barrier constraints to\ncope with RL. Obviously, a safe but more relaxed searching\narea will enhance the RL training performance. We propose a\nSOS program to compute a minimal control with polynomial\nbarrier function based on Lemma 3.\nLemma 3 declared that we can compute a minimal control\naCBF from an approximated polynomial system (8) directly.\nHowever, it is hard to deﬁne the minimization of the convex\nfunction aCBF in (12). The optimal control can be searched\nby using the following 2 steps:\nStep 1: Search an optimal auxiliary factor L(st) by\nmaximizing the scalar ϵ1,\nL∗(st)\n=\narg max\nϵ1\nϵ1∈R+,L(st)∈PSOS\ns.t.\n∆h(st, aCBF ) −L(st)h(st) −ϵ1 ∈PSOS\n−∆h(st, aCBF ) −Mi(st)µi(st) ∈PSOS,\n(15)\nwhere L(st) is an auxiliary factor to obtain a permissive\nconstraint for the existing action aCBF .\nStep 2: Given L(st) from last step, search an optimal\ncontrol of aCBF with minimal control cost by minimizing\nthe scalar ϵ2,\na∗\nt\n=\narg min\nϵ2\nϵ2∈R+\ns.t.\n∆h(st, aCBF ) −L(st)h(st) −ϵ2 ∈PSOS\n−∆h(st, aCBF ) −Mi(st)µi(st) ∈PSOS.\n(16)\nRemark. The scalars ϵ1 and ϵ2 in (15) and (16) are used to\nlimit the magnitude of the coefﬁcients’ optimization of L(st)\nand at, respectively.\nThe SOS programs above demonstrate the details of a\ntarget controller computation of the system (8), and the\nsolution of (16) regulates the dynamical safety via a control\nbarrier function directly.\nB. SOS Program Based CBF with Reinforcement Learning\nThe workﬂow of the CBF guiding DDPG control algo-\nrithm is shown in Fig. 1, which is akin to the original\nidea in [23] to push the agent’s action into a safe training\narchitecture. In Fig. 1, we list these ﬂowing elements into\nbrackets and highlight the corresponding input of each factor\ncomputation.\nWe illustrate the core idea of CBF guiding RL control as,\nat =aRL\nθk + ¯ak + aCBF\nt\n,\n(17)\nwhere the ﬁnal action at consists of a RL-based controller\naRL\nθk , a previous deployed CBF controller ¯aφ and a SOS\nprogram based CBF controller aCBF\nt\n. A clear distinction\nof the subscript t and k is stated here: t denotes the step\nnumber of each policy iteration and k denotes the policy\niteration number over the learning process.\nMore speciﬁcally, the ﬁrst term in (17) is an action\ngenerated from a standard DDPG algorithm with parameter\nθ. The second term ¯ak = Pk−1\ni=0 aCBF\ni\ndemotes a global\nconsideration of previous CBF controllers. Since it is im-\npractical to compute the exact value of each CBF action\naCBF\ni\nat each step. Supported by the work of [23], we\napproximate this term as ¯aφk ≈¯ak = Pk−1\ni=0 aCBF\ni\n, where\n¯aφk denotes an output from the multilayer perceptron (MLP)\nwith hyperparameters φ. Then, we ﬁt the MLP and update\nφk with Pk−1\ni=0 aCBF\ni\n(s, aRL\nθ0 , . . . , aRL\nθi−1) at each episode.\nThe third term aCBF\nt\nin (17) is a real-time value based on\nthe deterministic value st, aRL\nθk (st) and ¯aφk(st). Although\nthere exists an unknown term d in the system (1), we can still\nsolve the dynamical safety in the approximated polynomial\nsystem (8). So, the optimal at in (17) can be computed by\nthe SOS program below with deterministic aRL\nθk and ¯aφk\na∗\n=\narg min ∥ak∥2\nst∈S,ak∈A;L(st),Mi(st)∈PSOS;\ns.t.\n∆h(st, ak) −L(st)h(st) ∈PSOS,\n−∆h(st, ak) −Mi(st)µi(st) ∈PSOS.\n(18)\nThus, we can establish a controller by satisfying the solution\nof (18) over the learning process.\nTheorem 1. Given a barrier function h in (9), a partially\nunknown dynamical system (1) and a corresponding approx-\nimated system (8), while (8) is a stochastic statement of (1)\nwithin the probabilistic range [(1 −δ)k, 1], suppose there\nexists an action at satisfying (18) in the following form:\nat(st) =aRL\nθk (st, at, st+1, rt) + ¯aφk(st,\nk−1\nX\ni=0\n(st, aCBF\ni\n))\n+ aCBF\nt\n(st, aRL\nθk , ¯aφk).\n(19)\nThen, the controller (19) guarantees the system (1) inside\nthe safe region within the range of probability [(1 −δ)n, 1].\nProof. Regarding the system (8), this result follows directly\nfrom Lemma 3 by solving the SOS program (18). The only\ndifferent part of SOS program (18) from the SOS program\n(12) in Lemma 3 is the action ak here contains additional\ndeterministic value aRL\nθk (st) and ¯aφk(st).\nSince the output of (18) can regulate the dynamics of the\nmodel (8) in a safe region, while the approximated model (8)\nis a stochastic statement of the original system (1) with the\nprobability greater or equal to (1 −δ)k. Then, the solution\nof (18) can be regarded as a safe control to drive the system\n(1) far from dangerous, which ends the proof.\nWe display an overview of the whole steps in the combina-\ntion of the aforementioned factors over the learning process.\nThe detailed workﬂow is outlined in Algorithm 1\nAlgorithm 1: DDPG-SOSP\nInput: Origin system (1); barrier function h.\nOutput: RL optimal policy π.\n1 Preprocess (1) into (8).\n2 Create the tuple ˆD to store {st, at, st+1, rt} in ˆD.\n3 for k ∈{1, 2, . . . , k} do\n4\nExecute SOSP (18) to obtain the real-time CBF\ncontroller aCBF\nk\nand store it to train previous\nCBF controller ¯aCBF\nθk\n.\n5\nActuate the agent with at in (19).\n6\nConstruct ˆD and update θk and φk directly.\n7 return π.\nIV. NUMERICAL EXAMPLE\nA swing-up pendulum model from OpenAI Gym environ-\nment (Pendulum-v1) is used to verify the above theorems,\nml2¨θ = mgl sin(θ) + a.\n(20)\nLet s1 = θ, s2 = ˙θ. The dynamics of (20) are deﬁned as\n\u0014\n˙s1\n˙s2\n\u0015\n=\n\u0014\ns2\n−g\nl sin (s1) +\na\nml2 + d\n\u0015\n,\n(21)\nwhere d denotes unknown dynamics that generate by inaccu-\nrate parameters ¯m = 1.4, ¯l = 1.4, as [23] introduced. When\nwe try to construct a polynomial system (8), the sin(s1) in\n(21) will be approximated by Chebyshev interpolants within\ns1 ∈[−3, 3]. Then, with SOSOPT Toolbox [34], Mosek\nsolver [35] and Tensorﬂow, we implement Algorithm 1 in\nthe pendulum model.\nFig. 2. Comparison of the maximum absolute θ of different algorithms. The\ndotted solid line, the dashed line, and the solid line denote the performance\nof DDPG-ONLY, DDPG-QP and DDPG-SOSP, respectively The straight\nline denotes the safe boundary |s1| = 1.\nThe related model parameters are given in Table 1.\nTABLE 1: Swing-up Pendulum Model Parameters\nModel Parameter\nSymbol\nValue\nUnits\nPendulum mass\nm\n1.0\nkg\nGravity\ng\n10.0\nm/s2\nPendulum length\nl\n1.0\nm\nInput torque\na\n[−15.0, 15.0]\nN\nPendulum Angle\nθ\n[−1.00, 1.00]\nrad\nAngle velocity\n˙θ\n[−60.0, 60.0]\nrad/s\nAngle acceleration\n¨θ\n—\nrad/s2\nWe want to maintain the pendulum angle θ always in a safe\nrange [−1.0, 1.0] during the learning process. Three RL algo-\nrithms are selected and compared in this example, including\nthe DDPG-ONLY algorithm [5], CBF-based DDPG-QP al-\ngorithm [23] and CBF-based DDPG-SOSP algorithm (our\nwork). The codes of CBF-based DDPG-SOSP can be found\nat the url: https://github.com/Wogwan/CCTA2022 SafeRL.\nAll of these algorithms are trained in 150 episodes and\neach episode contains 200 steps. Each step is around 0.05s.\nAnd the reward function r = θ2 +0.1 ˙θ2 +0.001a2 is deﬁned\nsuch that the RL agent are expected to keep the pendulum\nupright with minimal θ, ˙θ and ¨θ.\nFig. 2 compares the collected maximum |θ| at each\nepisode by using different algorithms. As a baseline algo-\nrithm, DDPG-ONLY explored every state without any safety\nconsiderations, while DDPG-QP sometimes violated the safe\nstate and DDPG-SOS completely kept in safe regions.\nThe historical maximum value of | ˙θ| of these algorithms is\ncompared and shown in Fig. 3. It is found that DDPG-SOS\nis able to maintain the pendulum into a lower ˙θ easily than\nothers over episodes.\nIn Fig. 4, from the reward curve of these algorithms, it is\neasy to observe the efﬁciency of different algorithms: DDPG-\nSOS obtains comparatively lower average reward and thus a\nbetter performance.\nFig. 5 shows the ﬁrst (2nd episode) and the ﬁnal (150th\nepisode) policy guided control performance between DDPG-\nFig. 3.\nComparison of the maximum absolute ˙θ of different algorithms.\nFig. 4.\nComparison of the accumulated reward of different algorithms.\nQP and DDPG-SOSP algorithm. We observed 50 steps of\nthe pendulum angle to highlight the superiority of these two\nalgorithms. Both in Fig. 5(a) and (b), the ﬁnal policy can\nreach a smoother control output with less time. Although\nDDPG-SOSP takes a quicker and smoother action to stabilize\nthe pendulum than DDPG-QP, but the time consuming will\nincrease due to the dynamics regression and optimization\ncomputation under uncertainty. Accordingly, the result of\nDDPG-SOSP algorithm of model (20) is not only obviously\nstabler than DDPG-QP, but also maintaining the RL algo-\nrithm action’s safety strictly.\n(a)\n(b)\nFig. 5.\nThe comparison of the 1st and the 150th policy performance of\nthe angle control task between (a) DDPG-QP and (b) DDPG-SOS.\nV. CONCLUSION\nIn this paper, we consider a novel approach to guarantee\nthe safety of reinforcement learning (RL) with sum-of-\nsquares programming (SOSP). The objective is to ﬁnd a\nsafe RL method toward a partial unknown nonlinear system\nsuch that the RL agent’s action is always safe. One of the\ntypical RL algorithms, Deep Deterministic Policy Gradient\n(DDPG) algorithm, cooperates with control barrier function\n(CBF) in our work, where CBF is widely used to guarantee\nthe dynamical safety in control theories. Therefore, we\nleverage the SOSP to compute an optimal controller based\non CBF, and propose an algorithm to creatively embed this\ncomputation over DDPG. Our numerical example shows\nthat SOSP in the inverted pendulum model obtains a better\ncontrol performance than the quadratic program work. It also\nshows that the relaxations of SOSP can enable the agent to\ngenerate safe actions more permissively.\nFor the future work, we will investigate how to incorporate\nSOSP with other types of RL including value-based RL and\npolicy-based RL. Meanwhile, we check the possibility to\nsolve a SOSP-based RL that works in higher dimensional\ncases.\nREFERENCES\n[1] R. S. Sutton, A. G. Barto, ”Reinforcement learning: An introduction,”\nMIT press, 2018.\n[2] Y. Duan, X. Chen, R. Houthooft, J. Schulman, P. Abbeel, “Bench-\nmarking deep reinforcement learning for continuous control,” in Proc.\nInt. Conf. on Machine Learning, pp. 1329–1338, PMLR, 2016.\n[3] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, pp. 529–533, 2015.\n[4] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\net al., “Mastering the game of go with deep neural networks and tree\nsearch,” Nature, vol. 529, no. 7587, pp. 484–489, 2016.\n[5] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, D. Wierstra, “Continuous control with deep reinforcement\nlearning,” ArXiv:1509.02971, 2015.\n[6] Li, Z., Cheng, X., Peng, X. B., Abbeel, P., Levine, S., Berseth,\nG., Sreenath, K., ”Reinforcement Learning for Robust Parameterized\nLocomotion Control of Bipedal Robots,” in Proc. Int. Conf. on\nRobotics and Automation, pp. 2811–2817, 2021.\n[7] J. Peters, S. Schaal, “Reinforcement learning of motor skills with\npolicy gradients,” Neural Networks, vol. 21, no. 4, pp. 682–697, 2008.\n[8] R. S. Sutton, et al. ”Policy gradient methods for reinforcement learn-\ning with function approximation,” Advances in Neural Information\nProcessing Systems, 1999.\n[9] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, M. Riedmiller,\n“Deterministic policy gradient algorithms,” in Proc. Int. Conf. on\nMachine Learning, pp. 387–395, PMLR, 2014.\n[10] J. Kober, J. A. Bagnell, J. Peters, “Reinforcement learning in robotics:\nA survey,” Int. Journal of Robotics Research, vol. 32, no. 11, pp. 1238–\n1274, 2013.\n[11] P. Kormushev, S. Calinon, D. G. Caldwell, “Reinforcement learning\nin robotics: Applications and real-world challenges,” Robotics, vol. 2,\nno. 3, pp. 122–148, 2013.\n[12] J. Levinson, J. Askeland, J. Becker, J. Dolson, D. Held, S. Kammel,\nJ. Z. Kolter, D. Langer, O. Pink, V. Pratt, et al., “Towards fully au-\ntonomous driving: Systems and algorithms,” IEEE Intelligent Vehicles\nSymposium, pp. 163–168, 2011.\n[13] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab,\nS. Yogamani, P. P´erez, “Deep reinforcement learning for autonomous\ndriving: A survey,” IEEE Trans. on Intelligent Transportation Systems,\n2021.\n[14] L. Wang, D. Han, M. Egerstedt, “Permissive barrier certiﬁcates for\nsafe stabilization using sum-of-squares,” in Proc. Amer. Control Conf.,\npp. 585–590, 2018.\n[15] D. Han, D. Panagou, “Robust multitask formation control via para-\nmetric Lyapunov-like barrier functions,” IEEE Trans. on Automatic\nControl, vol. 64, no. 11, pp. 4439–4453, 2019.\n[16] G. Chesi, ”Domain of attraction: analysis and control via SOS pro-\ngramming”, Springer Science & Business Media, vol. 415, 2011.\n[17] D. Han, G. Chesi, Y. S. Hung, “Robust consensus for a class of\nuncertain multi-agent dynamical systems,” IEEE Trans. on Industrial\nInformatics, vol. 9, no. 1, pp. 306–312, 2012.\n[18] D. Han, G. Chesi, “Robust synchronization via homogeneous\nparameter-dependent polynomial contraction matrix,” IEEE Trans. on\nCircuits and Systems I: Regular Papers, vol. 61, no. 10, pp. 2931–\n2940, 2014.\n[19] D. Han, M. Althoff, “On estimating the robust domain of attraction\nfor uncertain non-polynomial systems: An LMI approach,” in Proc.\nConf. on Decision and Control, pp. 2176–2183, 2016.\n[20] B. Felix, M. Turchetta, A. Schoellig, A. Krause, ”A Safe model-based\nreinforcement learning with stability guarantees,” Advances in Neural\nInformation Processing Systems, pp. 909–919, 2018.\n[21] M. Alshiekh, R. Bloem, R. Ehlers, B. K¨onighofer, S. Niekum,\nU. Topcu, “Safe reinforcement learning via shielding,” in AAAI Conf.\non Artiﬁcial Intelligence, vol. 32, no. 1, 2018.\n[22] C. Steven, N. Jansen, S. Junges, U. Topcu, ”Safe Reinforcement\nLearning via Shielding for POMDPs,” ArXiv:2204.00755, 2022.\n[23] R. Cheng, G. Orosz, R. M. Murray, J. W. Burdick, “End-to-end safe\nreinforcement learning through barrier functions for safety-critical\ncontinuous control tasks,” in AAAI Conf. on Artiﬁcial Intelligence,\nvol. 33, no. 1, 2019.\n[24] R. Cheng, A. Verma, G. Orosz, S. Chaudhuri, Y. Yue, J. W. Burdick,\n”Control Regularization for Reduced Variance Reinforcement Learn-\ning,” in Proc. Int. Conf. on Machine Learning. pp. 1141–1150, 2019.\n[25] S. C. Hsu, X. Xu, A. D. Ames, “Control barrier function based\nquadratic programs with application to bipedal robotic walking,” in\nProc. Amer. Control Conf., pp. 4542–4548, 2015.\n[26] T. Akshay, Z. Jun, K. Sreenath. ”Safety-Critical Control and Plan-\nning for Obstacle Avoidance between Polytopes with Control Barrier\nFunctions,” in Proc. Int. Conf. on Robotics And Automation, accepted,\n2022.\n[27] L. Wang, E. A. Theodorou, M. Egerstedt, “Safe learning of quadrotor\ndynamics using barrier certiﬁcates,” in Proc. Int. Conf. on Robotics\nAnd Automation, pp. 2460–2465, 2018.\n[28] P. Martin, ”Markov decision processes: discrete stochastic dynamic\nprogramming”, John Wiley & Sons, 2014.\n[29] A. Lederer, J. Umlauft, S. Hirche, “Uniform error bounds for Gaussian\nprocess regression with application to safe control,” Advances in\nNeural Information Processing Systems, pp. 659–669, 2019.\n[30] H. Huang, D. Han, ”On Estimating the Probabilistic Region of\nAttraction for Partially Unknown Nonlinear Systems: An Sum-of-\nSquares Approach,” in Proc. Chinese Control and Decision Conf.,\naccepted, 2022.\n[31] D. Han, H. Huang, ”Sum-of-Squares Program and Safe Learning on\nMaximizing the Region of Attraction of Partially Unknown Systems,”\nin Proc. Asian Control Conf., 2022.\n[32] L. N. Trefethen, ”Approximation Theory and Approximation Practice,”\nSIAM, 2019.\n[33] M. Putinar, “Positive Polynomials on Compact Semi-algebraic Sets,”\nIndiana University Mathematics Journal, vol. 42, no. 3, pp. 969–984,\n1993.\n[34] P. Seiler, ”SOSOPT: A toolbox for polynomial optimization,”\nArXiv:1308.1889, 2013.\n[35] M. ApS, ”Mosek optimization toolbox for Matlab,” User’s Guide And\nReference Manual, Ver. 4, 2019.\n",
  "categories": [
    "eess.SY",
    "cs.LG",
    "cs.SY"
  ],
  "published": "2022-06-16",
  "updated": "2022-06-29"
}