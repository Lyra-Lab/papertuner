{
  "id": "http://arxiv.org/abs/2206.08266v1",
  "title": "ANGLEr: A Next-Generation Natural Language Exploratory Framework",
  "authors": [
    "Timotej Knez",
    "Marko Bajec",
    "Slavko Žitnik"
  ],
  "abstract": "Natural language processing is used for solving a wide variety of problems.\nSome scholars and interest groups working with language resources are not well\nversed in programming, so there is a need for a good graphical framework that\nallows users to quickly design and test natural language processing pipelines\nwithout the need for programming. The existing frameworks do not satisfy all\nthe requirements for such a tool. We, therefore, propose a new framework that\nprovides a simple way for its users to build language processing pipelines. It\nalso allows a simple programming language agnostic way for adding new modules,\nwhich will help the adoption by natural language processing developers and\nresearchers. The main parts of the proposed framework consist of (a) a\npluggable Docker-based architecture, (b) a general data model, and (c) APIs\ndescription along with the graphical user interface. The proposed design is\nbeing used for implementation of a new natural language processing framework,\ncalled ANGLEr.",
  "text": "ANGLEr: A Next-Generation Natural Language\nExploratory Framework\nTimotej Knez[0000−0001−7506−5739], Marko Bajec[0000−0002−8502−6765], and\nSlavko ˇZitnik[0000−0003−3452−1106]\nUniversity of Ljubljana, Ljubljana, Slovenia\nAbstract. Natural language processing is used for solving a wide variety\nof problems. Some scholars and interest groups working with language\nresources are not well versed in programming, so there is a need for a\ngood graphical framework that allows users to quickly design and test\nnatural language processing pipelines without the need for programming.\nThe existing frameworks do not satisfy all the requirements for such a\ntool. We, therefore, propose a new framework that provides a simple way\nfor its users to build language processing pipelines. It also allows a sim-\nple programming language agnostic way for adding new modules, which\nwill help the adoption by natural language processing developers and\nresearchers. The main parts of the proposed framework consist of (a)\na pluggable Docker-based architecture, (b) a general data model, and\n(c) APIs description along with the graphical user interface. The pro-\nposed design is being used for implementation of a new natural language\nprocessing framework, called ANGLEr.\nKeywords: Natural language processing · Graphical framework · Lan-\nguage understanding.\n1\nIntroduction\nRecently we have seen a lot of interest in the natural language processing tools\nwith Uima [5] receiving around 6 thousand downloads and GATE [3] receiving\nover 400 thousand downloads since 20051. NLP tools are used by people from\nvarious backgrounds. For the users outside the computer science area to use the\navailable tools eﬀectively, we have to provide them with an intuitive graphical\nuser interface that allows a simpliﬁed construction of text processing pipelines.\nIn this paper, we design an architecture for a new framework that enables quick\nand simple construction of natural language processing pipelines - ANGLEr. In\naddition to that, the framework features a way to add new tools that is simple\nfor developers to implement. The researchers could showcase their projects by\nincluding them in our framework with very little additional eﬀort. In the past,\nseveral frameworks for simplifying NLP workﬂows were designed. However, they\nall fall short either in the expandability or the ease of use. In our work we\n1 The number of downloads was recorded by https://sourceforge.net\narXiv:2206.08266v1  [cs.CL]  10 May 2022\n2\nT. Knez, M. Bajec, S. ˇZitnik\nidentify the key components of such framework and improve upon the existing\nframeworks by ﬁxing the identiﬁed ﬂaws.\nThe main contributions of our proposed framework are the following:\na) An architecture that allows simple inclusion of new and already existing\ntools to the system.\nb) A data model that is general enough to store information from many diﬀerent\ntools, while enabling compatibility between diﬀerent tools.\nc) A graphical user interface that speeds up the process of building a pipeline\nand makes the framework more approachable for users without technical\nknowledge.\nThe rest of this paper is organised as follows: in Section 2 we present the\nexisting frameworks and compare them to our proposed framework. In Section 3\nwe present our framework and its most important parts. We conclude the paper\nin Section 4.\n2\nExisting frameworks review\nWe examined the existing frameworks and selected the ones that we believe to\nbe most useful for users with limited programming knowledge. Analyzing the\nstrengths and weaknesses of the existing tools helps us to deﬁne a list of features\nthat could be improved by introducing a new framework. The comparison is\nsummarized in Table 1. We compare the frameworks on a set of features that\nwere presented by some of the frameworks as their main selling points while\nothers were selected as we ﬁnd them important for usability. We compare the\ntools on (1) their graphical user interface (GUI), which aﬀects usability for new\nusers, (2) data model, which aﬀects expandability, and (3) the language required\nfor creation of plugins, which is important for plugin developers.\nGATE: One of the best-known frameworks for text processing is GATE [3].\nIt was designed to feature a uniﬁed data model that supports a wide variety of\nlanguage processing tools. While the GATE framework provides a graphical way\nfor pipeline construction, its interface has not been signiﬁcantly updated in over\n10 years. The program is thus diﬃcult to use for new users. Another limitation\nof the GATE framework is that it requires all of its plugins to be written in the\nJava language. Because of that, it is diﬃcult to adapt an already existing text\nprocessing algorithm to work with the framework.\nUIMA: Uima [5] is a framework for extracting information from unstruc-\ntured documents like text, images, emails and so on. It features a data model\nthat combines extracted information from all previous components. While Uima\nprovides some graphical tools, they are not combined into a single application\nand do not provide an easy way for creating processing pipelines. The primary\nway of using UIMA still requires the user to edit XML descriptor documents,\nwhich limits usability for new users and slows down the development. New com-\nponents for the framework can be written in the Java or the C++ programming\nlanguages.\nANGLEr: A Next-Generation Natural Language Exploratory Framework\n3\nOrange: Another popular program for graphical creation of machine learn-\ning pipelines is Orange [4]. The main feature of the orange framework is a great\nuser interface that is friendly even for non technical users. Orange provides a\nvariety of widgets designed for machine learning tasks and data visualisation.\nThe framework was designed primarily to work with relational data for classic\nmachine learning. It supports two extensions for processing natural language\nwhich allow us to use the existing machine learning tools on text documents.\nThe largest drawback when using Orange for text processing is that the tabular\nrepresentation, used for representing orange data, is not well suited for repre-\nsenting information needed for text processing. The two extensions tackle this\nproblem in diﬀerent ways.\nOrange text mining: The Orange text mining extension implements a data\nmodel based on the tabular representation used by the existing tools. This repre-\nsentation allows integration with the existing machine learning and visualization\ntools, however, it also limits the implementation of some text processing algo-\nrithms. Its biggest limitation is that it only works with features on a document\nlevel.\nTextable: On the other hand, the Textable extension uses a signiﬁcantly\ndiﬀerent approach to data representation. In Textable, diﬀerent tools produce\ndiﬀerent data models. This allows for more ﬂexibility when developing new tools,\nhowever, it also limits the compatibility between diﬀerent types of tools. Ideally,\nwe would want a single data model that is capable of supporting every tool. This\nway we could reach high compatibility between tools while keeping the ﬂexibility\nwhen creating new ones.\nLibraries: In the past multiple libraries that support text processing have\nbeen developed. These libraries are commonly used by experts in the ﬁeld of\nnatural language processing. They are not well suited for use by other people that\nmight also be interested in language processing. One of such libraries is called\nOpenNLP [1]. It supports the development of natural language applications in\nthe Java programming language. There are also multiple Python libraries, such\nas NLTK [2], Gobbli [6], and Stanza [7]. To use any of these libraries, the user\nis required to have some programming knowledge. Writing a program is also a\nlot slower than constructing a pipeline through a graphical interface.\nTable 1. Comparison of diﬀerent natural language processing frameworks.\nFramework\nGraphical UI Uniﬁed data model Plugin language\nGATE\nYes (Native)\nYes (too general)\nJava\nUIMA\nYes (Limited)\nYes\nJava or C++\nOrange\nYes (Native)\nDiverse\nPython\nOrange textable\nYes (Native)\nDiverse\nPython\nOrange text mining Yes (Native)\nTabular data only\nPython\nLibraries\nNo\nDiverse\nJava\nANGLEr\nYes (Web)\nYes (versioned)\nN/A (Docker packaged)\n4\nT. Knez, M. Bajec, S. ˇZitnik\n3\nANGLEr: A Next-Generation Natural Language\nExploratory Framework\nBased on the problems we identiﬁed during the review of the existing frameworks\nwe identiﬁed the following important elements: (a) common and versioned data\nmodel, (b) extensible Docker-based architecture with API-based communication,\nand (c) uniﬁed and pluggable user interface (see Table 1). The data model and\nAPIs will ease the creation of new functionalities for the community. On the other\nhand, the user interface and simple installation or public hosting are important\nfor the general audience who need to use the language processing tools but do\nnot have the programming skills.\n3.1\nData model\nThe data model deﬁnes a structure that is used to transfer the analysis results\nbetween the tools. We aim to support all text processing tools that are available\nin the existing frameworks as well as new types of algorithms that have not been\nidentiﬁed yet. A general view of the data model is presented in Figure 1.\nRelations between text\nComplex results\nText features\nPreprocessing results\nSequence classification\nSequence tagging\nRelationship\nDiscourse\nParsing\nSummarization\nQuestion answering\nComparison\nClusters\nTokens\nPreprocessed document\nDocument features\nGeneral object type\nFig. 1. The upper-level hierarchy of ANGLEr object types.\nThe data model is a central part of the proposed system. It has to provide\nenough versatility to support every processing tool while keeping a well deﬁned\nstructure to enable compatibility between diﬀerent tools. We propose a model\ncomprised of two parts. The ﬁrst part is a corpus of all documents that we\nwant to process. We store the basic metadata about the corpus and each of the\ndocuments captured in it. Each document can also be separated into sentences\nand tokens in case this information was provided in the loaded corpus. The\nsecond part of the model stores all of the processor outputs. Outputs and inputs\nto available processors are encoded in the same schema - i.e., a processor’s output\ncan be directly another algorithm’s input. We deﬁne a list of object types that\ncan be used for representing results as follows:\nPreprocessed document type stores a new version of the documents in a\ncorpus after a preprocessing algorithm has been applied.\nDocument features type stores the values of the features that represent each\ndocument in a corpus.\nANGLEr: A Next-Generation Natural Language Exploratory Framework\n5\nSequence classiﬁcation type is used to store information about the result of\nthe classiﬁcation of an entire document or a part of a document.\nSequence tagging type is used to store a set of tags for diﬀerent parts of\nthe documents. This type could be used for instance to store named entity\nrecognition results.\nRelationship type is used to represent relations between two entities from the\nsource documents.\nDiscourse type is used to store entities that appear in the documents. Each\nentity also contains a list of all entity mentions that correspond to this entity.\nThis object would be used for instance to store coreference information.\nParsing type stores parts of the document that are linked together. For ex-\nample, this type would be used to store phrases detected using a chunking\nalgorithm.\nTokens type stores the information about tokens that appear in the documents.\nThis type would be used to represent for instance the result of a tokenizer\nor n-gram generator.\nSummarization type is used to store summaries of a document.\nQuestion answering type is used to store questions and their answers based\non the provided text.\nComparison type is used to store similarity between diﬀerent parts of the\ndocuments.\nClusters type is used to store clusters of similar documents.\nType hierarchy is proposed to improve backward compatibility when adding\nnew object types. This way each new speciﬁc object type contains the attributes\nof its parent as well as some of its own attributes (each level also allows for\nkey-value metadata storage). An algorithm that was designed to work with a\ngeneral object type can also work with all of its descendants. For example, a\ntool for performing named entity recognition would accept the token type to get\nthe tokens on which to perform named entity recognition. The user could also\nprovide the parsing type since it is a descendant of the token type.\n3.2\nHigh-level architecture\nThe goal of our framework architecture (presented in Figure 2) is that adding\na new module would be as simple as possible. In addition to that, we want to\nmake sure that the framework and its modules can be also used by third-party\napplications. Based on this, we have decided that each module should run as\na Docker container, which simpliﬁes the inclusion of already existing tools into\nour framework. The modules are connected to the ANGLEr backend, which is\nresponsible for managing and running the pipeline. The user interacts with the\ngraphical user interface that runs as a web page in a web browser.\n3.3\nREST interface\nAll of the communication between the parts of the framework is done over REST\napplication programming interfaces (API). This also allows third-party applica-\ntions to access the modules and the ANGLEr backend.\n6\nT. Knez, M. Bajec, S. ˇZitnik\nModules\nSQLite\ndatabase\n Settings\n Workspace\nImport/Export workflow\nadd/delete module\nWorkflow manager\nUI manager\nbasic workflow\ncomponents\nstatus observer\nANGLEr backend\nCallback API\nANGLEr module\nProcessors\nWidget\nSettings\nButton\nService discovery\nData model V x.y\nDescription\nModule API\nANGLEr module\nProcessors\nWidget\nSettings\nButton\nService discovery\nData model V x.y\nDescription\nModule API\nData model V 1.0\nApplication\nAPI\nFrontend\n(WEB)\n3rd-party\napplication\nANGLEr framework\nFig. 2. ANGLEr high-level system architecture. The communication between all of the\nmodules an the backend is done using the versioned data model.\n[{\"input_name\": \"Training data\", \"types\": [\n   {\"type_name\": \"Tokens\", \"type\": \"tokens\"},{\"type_name\": \"Part of speech\", \"type\": \"sequenceTagging\"},\n   {\"type_name\": \"Named entities\", \"type\": \"sequenceTagging\"}\n ]},\n {\"input_name\": \"Validation data\", \"types\": [\n   {\"type_name\": \"Tokens\", \"type\": \"tokens\"},{\"type_name\": \"Part of speech\", \"type\": \"sequenceTagging\"}]}]\nFig. 3. An example of the input attribute for a named entity recognition module.\nThe framework features multiple REST APIs shown in Figure 2. The Module\nAPI (Table 2) is responsible for commands that are sent from the ANGLEr\nbackend to a module. It allows the ANGLEr backend to gather information\nabout the processors that are running in a module. It also allows the backend to\nsend the data and start its processing. After the processing is done, the module\nsends the results to the backend using the Callback API. The ANGLEr backend\nalso exposes the Application API, which is responsible for communication with\nthe graphical user interface, as well as with any third-party applications that\nmight want to use the ANGLEr functionality. Since the Module API has to be\nimplemented by each module, we present its endpoints in Table 2.\nOnce a module is added to the framework, the ANGLEr backend creates\na request to the about endpoint, which provides the basic information about\nthe entire module. After that, it requests the processors endpoint, which lists\nall processors that are contained in the module. This information allows the\nframework to visualise the processor in the graphical interface. It also provides\nthe endpoints that the ANGLEr backend uses to send data for processing and\nto show conﬁguration and visualisation pages.\nANGLEr: A Next-Generation Natural Language Exploratory Framework\n7\nTable 2. Endpoints in the Module API. The underlined attributes are required. The\nprocessors endpoint returns a list of objects where each object has the presented at-\ntributes.\nEndpoint Parameters\nDescription\n/about\nUUID\nIdentiﬁer of the module.\nname\nName of the module.\nversion\nModule version.\ndata model\nVersion of the data model used.\ndesc\nModule description.\nauthors\nList of authors.\norganisation\nOrganisation of the authors.\nurl\nURL address of the page about the module.\n/processors name\nName of a processor.\nshort name\nShort version of the name.\ndata endpoint\nEndpoint where the data for processing can be sent.\nsettings endpoint An address of a page containing processor settings.\nui endpoint\nAn address of a page for visualization.\nicon\nAn address of the icon to be used to represent the\nprocessor.\ncategory\nA category of the processors menu that should\ncontain this processor.\ninputs\nA list of objects representing diﬀerent inputs. Each\nobject should contain a name and a list of types required.\nAn example is shown in Figure 3\noutputs\nA list of objects representing diﬀerent outputs. Each\nobject should contain a name and a type of the output.\n/docs\nhtml page\nA web page containing the documentation.\n3.4\nGraphical user interface\nThe user interface is a key component for making the framework simple to use.\nIt allows the users without any programming knowledge to use the framework.\nThis is especially important since language processing tools are very useful for\nlinguists, who typically do not have advanced computer knowledge. We propose\na simple design for the user interface, which is shown in Figure 4. The tabs at\nthe top of the page organise the tools into groups based on their purpose. For\nexample, the groups contain the tools for text preprocessing, for semantic and\nsyntactic analysis etc. A user can get a widget for each tool by dragging and\ndropping. The widget has a page for setting its parameters and a dialog for\nselecting the input. A widget can optionally also provide a page for data visu-\nalisation (see the Figure 4 right). The user connects the widgets into a pipeline\nthat can be stored to a ﬁle and loaded by any user. The ANGLEr framework\nthen executes the pipeline to process the data.\n8\nT. Knez, M. Bajec, S. ˇZitnik\nTool groups\nTools\nPipeline\nTool options\nConnection configuration\nResult visualisation\nFig. 4. The main parts of the ANGLEr user interface.\n4\nConclusion\nWe describe the main components for implementing a new natural language\nprocessing framework - ANGLEr. We believe that a new framework based on our\nproposal would provide a large improvement over the existing frameworks and\nwould greatly beneﬁt users that are working with natural language processing.\nThe framework would provide a fast way for prototyping when developing text\nprocessing pipelines. It would also allow users with no programming knowledge\nto build advanced NLP pipelines. In addition to that, the new framework would\nprovide the researchers with a great way for showcasing their work in the NLP\narea. Since the tools can be implemented in any programming language, their\ninclusion is much less complicated than with existing frameworks.\nReferences\n1. Apache: Opennlp (2010), http://opennlp.apache.org\n2. Bird, S., Loper, E.: Nltk: the natural language toolkit. Association for Computa-\ntional Linguistics (2004)\n3. Cunningham, H.: Gate, a general architecture for text engineering. Computers and\nthe Humanities 36(2), 223–254 (2002)\n4. Demˇsar, J., Curk, T., Erjavec, A., Gorup, ˇC., Hoˇcevar, T., Milutinoviˇc, M., Moˇzina,\nM., Polajnar, M., Toplak, M., Stariˇc, A., et al.: Orange: data mining toolbox in\npython. the Journal of machine Learning research 14(1), 2349–2353 (2013)\n5. Ferrucci, D., Lally, A.: Uima: an architectural approach to unstructured information\nprocessing in the corporate research environment. Natural Language Engineering\n10(3-4), 327–348 (2004)\n6. Nance, J., Baumgartner, P.: gobbli: A uniform interface to deep learning for text in\npython. Journal of Open Source Software 6(62), 2395 (2021)\n7. Qi, P., Zhang, Y., Zhang, Y., Bolton, J., Manning, C.D.: Stanza: A python natural\nlanguage processing toolkit for many human languages. In: Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics: System Demon-\nstrations. pp. 101–108 (2020)\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2022-05-10",
  "updated": "2022-05-10"
}