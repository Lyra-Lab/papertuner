{
  "id": "http://arxiv.org/abs/2206.08460v2",
  "title": "TUSK: Task-Agnostic Unsupervised Keypoints",
  "authors": [
    "Yuhe Jin",
    "Weiwei Sun",
    "Jan Hosang",
    "Eduard Trulls",
    "Kwang Moo Yi"
  ],
  "abstract": "Existing unsupervised methods for keypoint learning rely heavily on the\nassumption that a specific keypoint type (e.g. elbow, digit, abstract geometric\nshape) appears only once in an image. This greatly limits their applicability,\nas each instance must be isolated before applying the method-an issue that is\nnever discussed or evaluated. We thus propose a novel method to learn\nTask-agnostic, UnSupervised Keypoints (TUSK) which can deal with multiple\ninstances. To achieve this, instead of the commonly-used strategy of detecting\nmultiple heatmaps, each dedicated to a specific keypoint type, we use a single\nheatmap for detection, and enable unsupervised learning of keypoint types\nthrough clustering. Specifically, we encode semantics into the keypoints by\nteaching them to reconstruct images from a sparse set of keypoints and their\ndescriptors, where the descriptors are forced to form distinct clusters in\nfeature space around learned prototypes. This makes our approach amenable to a\nwider range of tasks than any previous unsupervised keypoint method: we show\nexperiments on multiple-instance detection and classification, object\ndiscovery, and landmark detection-all unsupervised-with performance on par with\nthe state of the art, while also being able to deal with multiple instances.",
  "text": "TUSK: Task-Agnostic Unsupervised Keypoints\nYuhe Jin1, Weiwei Sun1,\nJan Hosang2,\nEduard Trulls2,\nKwang Moo Yi1\n1The University of British Columbia,\n2Google Research\nAbstract\nExisting unsupervised methods for keypoint learning rely heavily on the assumption\nthat a speciﬁc keypoint type (e.g. elbow, digit, abstract geometric shape) appears\nonly once in an image. This greatly limits their applicability, as each instance\nmust be isolated before applying the method—an issue that is never discussed or\nevaluated. We thus propose a novel method to learn Task-agnostic, UnSupervised\nKeypoints (TUSK) which can deal with multiple instances. To achieve this, instead\nof the commonly-used strategy of detecting multiple heatmaps, each dedicated to\na speciﬁc keypoint type, we use a single heatmap for detection, and enable unsu-\npervised learning of keypoint types through clustering. Speciﬁcally, we encode\nsemantics into the keypoints by teaching them to reconstruct images from a sparse\nset of keypoints and their descriptors, where the descriptors are forced to form\ndistinct clusters in feature space around learned prototypes. This makes our ap-\nproach amenable to a wider range of tasks than any previous unsupervised keypoint\nmethod: we show experiments on multiple-instance detection and classiﬁcation,\nobject discovery, and landmark detection—all unsupervised—with performance on\npar with the state of the art, while also being able to deal with multiple instances.\n1\nIntroduction\nKeypoint-based methods are a popular off-the-shelf building block for a wide array of computer\nvision tasks, including city-scale 3D mapping and re-localization [39, 52], landmark detection on\nhuman faces [53, 64, 23, 37], human pose detection [53, 64, 37], object detection [38, 32, 13, 46],\nscene categorization [31, 15], image retrieval [26, 44], robot navigation [42, 12], and many others.\nAs elsewhere in computer vision, learning-based strategies have become the de-facto standard in\nkeypoint detection and description [53, 7, 44, 64, 23, 9, 37, 55]. This is accomplished by tailoring a\nmethod to a speciﬁc task by means of domain-speciﬁc supervision signals—methods are often simply\nnot applicable outside the use case they were trained for. This is problematic, as many applications\nrequire extensive amounts of labeled training data, which is often difﬁcult or infeasible to collect.\nResearchers have attempted to overcome the requirement for labelled data through unsupervised or\nself-supervised learning [53, 64, 23, 37, 7, 48, 62], primarily using two families of techniques, often\nin combination: (1) enforcing equivariant representations through synthetic image transformations,\nsuch as homographies [7, 48] or non-rigid deformations [28, 53]; and (2) clustering features in a latent\nspace over a predetermined number of channels, where every channel is meant to represent a single\nobject or a single part of an object. The ﬁrst objective ensures that keypoints can deal with changes\ndue to camera pose or body articulation, but synthetic transformations do not capture any high-level\nsemantics. The second objective can encode semantic information into the keypoints, but methods\nassume that every keypoint (a speciﬁc object or part, such as the corner of the mouth) appears only\nonce in a given image, such that a ‘specialized’ heatmap can be learned for each semantic keypoint.\nThis is an effective way to constrain optimization, but is unrealistic in many scenarios, and restricts\nthe applicability of these methods to isolated instances—for example, a learned landmark detector for\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2206.08460v2  [cs.CV]  13 Jan 2023\nOurs\nLatentKeypointGAN [19]\nFigure 1: Landmark detection on images with multiple faces. (Left) Our model is trained on\nCelebA [35], which contains single faces, but can be trivially applied here by simply increasing\nthe number of keypoints. (Right) Methods based on the multi-channel-detection paradigm fail to\ngeneralize, and would require processing each instance separately . Images are in the public domain.\nhuman faces will break down if we simply introduce a second face to the image: see Fig. 1. In other\nwords, these methods require each object instance to be isolated before keypoint extraction—which\nis itself a non-trivial problem, and makes them unsuitable as a generic keypoint detector.\nWe propose a method to solve this. Similarly to previous works, we estimate a heatmap representing\nthe likelihood of each pixel being a keypoint, and a feature map encoding a descriptor for each pixel,\nshould that pixel be a keypoint. To avoid being limited to single instances, unlike most existing works\n[53, 64, 23, 37, 19], we predict a single heatmap, from which we extract multiple keypoints. We then\nfocus on the properties that we want keypoints to have, and show how to accomplish them without\nsupervision or task-speciﬁc assumptions. Speciﬁcally, we focus on the two key aspects outlined above:\n(a) keypoints should be equivariant to geometric transformations, i.e., they should follow the same\ntransformation that the image does; and (b) group semantically similar things together. The former\ncan be easily achieved by enforcing equivariance of both keypoint locations and their descriptors\nunder random perturbations, following previous efforts like [53]—with additional considerations,\nbecause, unlike existing methods, we do not assume that the images contain a single instance.\nThe latter is more difﬁcult to solve under the multiple-instance scenario. We address it with a novel\nsolution based on unsupervised clustering of the feature space. We learn a latent representation for\neach keypoint that we use to reconstruct the image, similarly to [64, 23, 37, 16, 3, 36], and show\nhow to discover the re-occurring structure or semantic information by clustering these features in the\nlatent space. Speciﬁcally, we learn a codebook of cluster centers—which we call prototypes—and\nenforce that the distribution of descriptors at keypoint locations corresponds to the distribution of\nthese prototypes through a novel sliced Wasserstein objective [6] that utilizes Gaussian Mixture\nModels (GMM). This naturally leads to descriptors clustering around the prototypes.\nOur approach allows us to achieve both objectives without task-speciﬁc labels, including implicit\nassumptions such as that the images contain the same objects, or that each object appears only\nonce [53, 64, 23, 37, 19]: see Fig. 1 for examples. Our main contributions are as follows:\n• We show how to encode both geometry and semantics into keypoints and descriptors, without\nsupervision. For the latter, we propose a novel technique based on prototype clustering.\n• Unlike existing methods, our method does not require knowing the exact number of keypoints a\nprior, and can naturally deal with multiple instances and occlusions.\n• Our approach does not require domain-speciﬁc knowledge and can be applied to multiple tasks—\nwe showcase landmark detection, multiple-instance classiﬁcation, and object discovery, with\nperformance on part with the state of the art—while being able to deal with multiple instances.\nOther than hyperparameters, the only component we tailor to the task is the loss used to reconstruct\nthe image from the keypoints (Lrecon, Sec. 3.3). Our method is the only one applicable to all tasks.\n2\nRelated work\nLandmark learning. The term landmark is used in the literature to refer to localized, compact\nrepresentations of object structures such as human body parts. We favor the term keypoint instead,\nas our method can be applied to a variety of tasks, and our keypoints may represent both whole\nobjects or their parts. There is a vast amount of literature on this topic, especially for human\nbodies [50, 49, 43] and faces [59, 65], the majority of which use manual annotations for supervision.\nAmong unsupervised methods, Thewlis et al. [53] enforce landmarks to be equivariant by training\na siamese network with synthetic transformations of the input image. They introduce a diversity\nloss that penalizes overlapping landmark distributions to prevent different channels from collapsing\ninto the same landmark. In a follow up work, Thewlis et al. [54] further improve this idea by\n2\nenforcing equivariance on a dense model, rather than sparse landmarks. Zhang et al. [64] add a\nreconstruction task solved with an autoencoder, in addition to the equivariance constraint, which\nimbues the landmarks with semantics but still relies on separation constraints to prevent landmark\ncollapse. Our approach does not require such an additional regularization, as our formulation prevents\ncollapse by design. Given a pair of source and target images, Jakab et al. [23] detect landmarks in the\ntarget image and combine them with a global representation of the source image to reconstruct the\ntarget, and learn without supervision through either synthetic deformations or neighboring frames\nfrom video sequences. Lorenz et al. [37] use a two-stream autoencoder to disentangle shape and\nappearance and learn to reconstruct an object from its parts. We use similar strategies to enforce\nequivariance in the location of the landmarks (keypoints), but additionally learn distinctive features\nwith semantic meaning. In concurrent work, He et al. [19] pushed performance in unsupervised\nkeypoint detection further with a GAN-based framework. All of these methods, however, use\ndedicated heatmap channels for each landmark to be detected and thus assume that the image contains\none and only one object instance—by contrast, our approach learns a single-channel heatmap and\ncan handle a variable number of objects (see Fig. 1 for a comparison with [19]).\nUnsupervised object discovery. Our method can be applied to object discovery, by representing\nobjects as keypoints. Decomposing complex visual scenes with a variable number of potentially\nrepeating and partially occluded objects into semantically meaningful components is a fundamental\ncognitive ability, and remains an open challenge. Finding objects in images has been historically\naddressed through bounding-box-based detection or segmentation methods trained on manual annota-\ntions [18, 51, 14]—learning object-centric representations of scenes without supervision has only\nrecently seen breakthroughs [25, 3, 16, 36, 10, 11]. MONet [3] trains a Variational Auto-Encoder and\na recurrent attention network to segment objects and reconstruct the image. IODINE [16] similarly\nrelies on VAEs to infer a set of latent object representations which can be used to represent and\nreconstruct the scene—and like MONet uses multiple encode-decode steps. Locatello et al. [36]\nreplace this procedure with a single encoding step and iterate on the attention mechanism instead,\nand introduce the concept of ‘slot attention’ to map feature representations to discrete sets of objects.\nAll of them share the same paradigm, representing the scene as a collection of latent variables where\neach variable corresponds to one object. However, the decoder may produce different latent codes for\nobjects at different locations in order to reconstruct all present objects, disregarding their possibly\nsimilar semantics, which requires higher network capacity to model complex scenes [36]. By contrast,\nwe explicitly supervise our keypoints to form compact clusters in feature space. More recently,\nconcurrent work [30] extended slot attention to more complex images using additional motion cues.\nHowever, it still uses synthetic datasets and cannot produce a semantically-aware latent space.\nMultiple-instance classiﬁcation. This problem differs from the above in that the objects in the\nscene are known a priori (e.g. numbers, or letters) and need not be ‘discovered’. It is also related to\nlandmark learning, but it may contain repeated objects, which breaks down the one-object-per-channel\nparadigm used by most methods devised for that problem. MIST [1] solves this with a differentiable\ntop-k operation which is used to extract a predetermined number of patches, which are then fed to\na task-speciﬁc network, such as a classiﬁer. It needs to be supervised with class labels indicating\nhow many times each instance appears, which we do not require, and unlike our approach it does\nnot generalize to the case of an unknown or variable number of objects. We show that our method\nobtains similar performance to MIST and another state-of-the-art top-k selection method based on\noptimal transport [60].\nUnsupervised clustering. The performance of traditional clustering methods such as k-means and\nGaussian Mixture Models is highly dependent on the quality of the hand-crafted features, and it is\nnon-trivial to incorporate them into deep networks. A popular solution is to train an autoencoder\nfor reconstruction, and regularize the bottleneck feature to form clusters [61, 41]. These methods,\nhowever, are for the most part only applicable to image classiﬁcation, because they do not have\nthe notion of locality—i.e., they produce one global description per image. Moreover, we show\nthat naively using k-means to regulate the feature space can lead to degenerate solutions, while our\ncarefully designed sliced Wasserstein loss does not suffer from this.\nRegularisation of Latent space. Autoencoders [20] have been widely used for feature learning\n[57], but are prone to overﬁtting [2]. Variational autoencoders (VAE) [29] solved this problem by\nregularizing the learned latent space towards a certain prior distribution—normal distributions are\noften used. VQ-VAE [56] extends VAE to generate a discretized latent space by regularizing it with\n3\n{      }\n5 1\n9 \u001a3\n5 1\n9 \u001a3\n5 1\n9 \u001a3\nE\n5 1\n9 \u001a3\n5 1\n9 \u001a3\n5 1\n9 \u001a3\n5 1\n9 \u001a3\n…\n5 1\n9 \u001a3\n{       }\n…\nSelection\n{      }\nE\nFigure 2: Framework – We generate a heatmap H and a feature map F through an encoder E.\nWe then select K keypoints at locations L and retrieve K descriptors D from the feature map F,\nby performing non-maxima suppression on the heatmap and picking the top responses. We then\ndecode these keypoints with the decoder D and reconstruct the input image. We use multiple losses\nfor training. Leqv enforces equivariance of heatmap and features w.r.t. a random thin plate spline\ntransformation T. Lsw and Lkm use learned prototypes P to force descriptors to form semantic clusters.\nLrecon minimizes the difference between image and reconstruction, similarly to an autoencoder .\na discrete codebook. Our method can also be viewed as an autoencoder, where we regularize the\nlatent space to form semantic clusters centered around learned prototypes.\nLocal features. Local feature is the preferred term for ‘classical’ keypoint methods, which with\nthe introduction of SIFT [38] became one of the fundamental building blocks of computer vision.\nThey remain an open area of research [9, 55, 58, 62], but are now relegated to applications such as\n3D reconstruction and re-localization [39, 52] or SLAM [42, 12]. These problems require matching\narbitrary textures (corners, blobs) across a wide range of rotations and scale changes. They are thus\nmore open-ended than the other methods presented in this section and are typically supervised from\nknown pixel-to-pixel correspondences. Relevant to our approach are ideas such as using homographic\nadaptation to learn equivariant features [33, 7], or a soft-argmax operator [63] to select keypoints\nin a differentiable manner. While recent papers have drastically reduced supervision requirements\n[58, 55, 62], truly unsupervised learning for general-use local features remains a ‘holy grail’.\n3\nMethod\nWe illustrate our approach in Fig. 2. Given an image, we auto-encode it to obtain a latent representation\nof sparse keypoints. In more detail, the encoder E receives an input image I ∈RH×W ×3 and generates\na feature map F ∈RH×W ×C, preserving the spatial dimensions of the input. Unlike previous works\nthat utilize one heatmap per keypoint, our method outputs a single heatmap H ∈RH×W on which we\ndetect all keypoints, allowing for multiple instances of the same keypoint (or object) to be detected.\nFrom the heatmap H, we extract K keypoint locations L ∈RK×2 and their corresponding descriptors\nD ∈RK×C from F. This sparse, latent representation of our autoencoder, i.e., L and D, is then fed\ninto the decoder D, which generates ˜I, a reconstruction of the input image. To force these individually\ndetected D to form semantic clusters in the latent space, we learn M prototypes P ∈RM×C, which\nact as ‘cluster centers‘ that guide the clustering process via losses to be discussed in Sec. 3.3.\nThis network architecture is similar to previous autoencoder-based landmark discovery papers in\nthat we use an encoder to extract keypoint locations and descriptors and a decoder to reconstruct the\nimage [64, 23, 37]—but our method differs in two key aspects: (a) our encoder generates a single\nheatmap instead of multiple ones, allowing us to operate beyond the assumption that each feature\nappears once and only once in every image; and (b) we regulate the latent space by clustering features\naround prototypes. Note that while we still need to choose a value for K (and M), our approach can\nhandle a variable number of objects at inference time as long as the number of extracted keypoints is\nsufﬁciently large, by ﬁltering out low conﬁdence detections: see CLEVR (Sec. 4.2) for results on\nimages with a variable number of objects while using a ﬁxed number of keypoints.\n3.1\nEncoder E – Keypoint and descriptor extraction\nWe use a U-Net-like [51] network that takes as input a unit-normalized image I ∈[0, 1]H×W ×3 to\ngenerate a single-channel heatmap H ∈RH×W and a multi-channel feature map F ∈RH×W ×C.\n(We found C=32 to be sufﬁcient for all datasets and tasks.) To detect multiple instances from H\n4\nwe apply non-maximum suppression (NMS) to H and select the top K maxima. Note that this is a\nnon-differentiable process—inspired by LIFT [63, 45], we apply soft-argmax in the neighborhood\nof each maximum to obtain the ﬁnal keypoint locations L, restoring differentiability w.r.t. locations.\nWe extract the descriptors D at each keypoint location from the feature map F via simple bilinear\ninterpolation. Please refer to Supplementary Material for further details.\n3.2\nDecoder D – Image reconstruction from sparse features\nTo decode the sparse, keypoint-based representation into an image, we use another U-Net, identical\nto the encoder except for the number of input/output channels. We transform the sparse keypoints\nand descriptors into a dense representation by creating a feature map. Speciﬁcally, in order to make\nkeypoints ‘local’, we generate one feature map per keypoint by copying the descriptors D to the\nlocations L and convolving them with a Gaussian kernel with a pre-deﬁned variance, generating K\nfeature maps. We then aggregate them with a weighted sum, according to the detection score. With\nthis input, we train the decoder D to reconstruct the image I into ˜I.\nIf the object mask is necessary—e.g. for the object discovery task, in Sec. 4.2—instead of a weighted\nsum we simply reconstruct one RGB-A image for each of the K feature maps and apply alpha-\nblending to obtain the ﬁnal reconstruction ˜I. The alpha channel can be used as the object mask,\nrequired by the evaluation metrics. See Supplementary Material for more architectural details.\n3.3\nLearning objectives\nWe present various learning objectives to encourage our keypoint-based representation to have certain\ndesirable properties. All objectives are critical to the ﬁnal performance, see ablation in Sec. 4.4.\nLearning meaningful features – Lrecon. Similarly to a standard auto-encoder, our network learns\nto extract high-level features by minimizing the reconstruction loss between the input image I\nand the reconstructed image ˜I. We use either the mean squared error (MSE) or the perceptual\nloss [24]. We ﬁnd that the perceptual loss is advantageous on real data with complex backgrounds\nsuch as CelebA [35], but the MSE loss does better on simpler datasets with no background such as\nCLEVR [27] or Tetrominoes [27], commonly used by object discovery papers [3, 16, 36], as well as\nH36M after removing the background [22].\nLrecon =\n(P\nl\ncl∥Vl(I) −Vl(˜I)∥1,\nPerceptual Loss\n∥I −˜I∥2,\nMSE Loss\n(1)\nFor the perceptual loss we use a pre-trained, frozen VGG16 network to extract features. Vl is the\noutput of layer l and cl its weighting factor, as presented in [24].\nUnsupervised clustering – Lsw, Lkm. In order to embed semantic meaning into our learned descrip-\ntors, we encourage descriptors to cluster in feature space. Descriptors of the same cluster should move\ncloser together and descriptors of different clusters further apart, which is similar to the objective\nof contrastive learning papers [17, 5, 4]. More speciﬁcally, we learn M prototypes P ∈RM×C,\nwhere each prototype represents the ‘center’ of a class. Our goal is two-fold. First, we wish the\nprototypes to follow the same distribution as the features. Second, we wish the features to cluster\naround the prototypes. For the ﬁrst objective, we minimize the sliced Wasserstein distance [6], and\nfor the second objective, we use a k-means loss.\n— Sliced Wasserstein loss Lsw. To compute the sliced Wasserstein distance [6], we randomly sample\nN projections wi from a unit sphere, each projecting the C-dimensional descriptor space into a\nsingle dimension. Then, in one-dimensional space, the Wasserstein distance W can be calculated as\nthe square distance between sorted, projected descriptors, and prototypes. The average of these N\ndistances is the sliced Wasserstein loss we seek to minimize.\nThe difﬁculty in applying this method, however, is that the number of samples from the two distribu-\ntions must be equal, which is not the case for our descriptors and prototypes. We thus represent the\nprototypes as modes in a Gaussian Mixture Model (GMM) [8], so we can draw as many samples\nfrom the prototype distribution as the number of keypoints we extract (K) to compute the sliced\nWasserstein loss (see Supplementary Material for more details). Denoting the sampled, sorted\n5\nprototypes as ˜P ∈RK×C and the sorted descriptors as ˜D, we write\nLsw =\n1\nNK\nX\ni∈[1,N],j∈[1,K]\nW(wT\ni ˜Dj, wT\ni ˜Pj).\n(2)\n— k-means loss Lkm. With the k-means loss, we encourage descriptors to become close to the closest\nprototype, relatively to the other prototypes. Inspired by [5], we minimize the log soft-min of the ℓ2\ndistances between prototypes and descriptors, where the logarithm avoids the vanishing gradients of\nthe soft-min. We write\nLkm = −1\nK\nX\ni∈[1,K]\nlog max\nj∈[1,M]\n\n\n\nexp(−∥Di −Pj∥)\nP\nk∈[1,M]\nexp(−∥Di −Pk∥)\n\n\n.\n(3)\nMaking detections equivariant – Leqv. Keypoints should be equivariant to image transformations\nsuch as afﬁne or non-rigid deformations. This idea appears in several local feature papers [7, 48], and\nunsupervised landmark learning papers [53, 64, 23]. Following [28, 53] we apply a thin plate spline\ntransformation T and use the l2 distance as a loss to ensure that both heatmap and feature map are\nequivariant under non-rigid deformations. Note that the encoder E produces a heatmap and a feature\nmap and we apply the loss to both separately—we use a single equation for simplicity:\nLeqv = ∥E(T(I)) −T(E(I))∥2.\n(4)\nIterative training. We found that straightforward training of our framework leads to convergence\ndifﬁculties. We thus alternate training of descriptors and prototypes. Speciﬁcally, we train only the\nencoder and the decoder for one step via λreconLrecon + λkmLkm + λeqvLeqv (see Supplementary\nMaterial for details). We then train the prototypes and their GMM for eight steps using Lsw. We\nrepeat this iterative training process until convergence.\n4\nResults\nWe apply our method to three tasks: multiple-instance object detection, object discovery, and landmark\ndetection. We use ﬁve different datasets.\nMNIST-Hard [1] contains synthetically-generated images composed of multiple MNIST digits. We\nfollow the procedure in [1] and put 9 MNIST digits on a black canvas at random positions, sampling\nfrom a Poisson distribution. Variations of the same digit may co-occur in the same image. We\ngenerate 50K such images for training and testing, respectively.\nCLEVR[27] contains visual scenes with a variable number of objects in each scene. Following\n[16, 3, 36], we only use the ﬁrst 70K images that contain at most 6 objects (also known as CLEVR6).\nWe train our model with the ﬁrst 60K images and evaluate with the last 10K.\nTetrominoes[27] contains 80K images of Tetrominoes, a geometric shape composed of four squares.\nEach image contains three randomly sampled, non-overlapping Tetrominoes. There are 19 unique\ntetrominoes shapes (including rotations). Each Tetromino can be in one of the 6 possible colors. We\ntrain our model using the ﬁrst 50K images and evaluate with the last 10K.\nCelebA [35] contains 200k images of human faces. Earlier efforts [37] used aligned images (with\ncentered faces) for both training and evaluation, a dataset that is now heavily saturated. We follow\n[34] instead, which uses CelebA with unaligned images, ﬁltering out those where the face accounts\nfor less than 30% of the pixels. We follow the standard procedure and train our model without\nsupervision using all images except for the ones in the MAFL (Multi-Attribute Facial Landmark) test\nset, and train a linear regressor which regresses the 5 manually annotated landmarks, on the MAFL\ntraining set. We report pixel accuracy normalized by inter-ocular distance on the MAFL test set.\nHuman 3.6M (H36M) [22] contains 3.6M captured human images with ground truth joint locations\nfrom 11 actors (7 for training, 4 for test) and 17 activities. We follow [64], using 6 actors in the\ntraining set for training and the last one for evaluation. We subtract the background with a method\nprovided by the dataset. As for CelebA, we ﬁrst train our model on the training set and then learn a\nlinear regressor that predicts the location of 36 joints. Since the front and back views are often similar,\nwe follow [64] and swap left and right joint labels on images where the subject is turned around.\n6\nTable 1: Object detection on MNIST-Hard. We compare our method against weakly supervised\nmethods (class labels are available during training) and fully supervised methods (location and class\nlabel are available during training). Our method performs best in terms of localization and similarly\nto Xie et al. [60], although our approach is fully unsupervised.\nUnsupervised\nWeakly Supervised\nFully Supervised\nOurs\nMIST[1]\nXie et al. [60]\nCh.-wise\nCNN\nLocalization\n99.9%\n97.8%\n72.7%\n25.4%\n99.6%\nClassiﬁcation\n92.1%\n98.8%\n93.1%\n75.5%\n98.7%\nBoth (Loc. ∩Classif.)\n92.1%\n97.5%\n71.3%\n24.8%\n98.6%\nPrototypes\nDetection Results\nt-SNE\nFigure 3: Qualitative results on MNIST-Hard. (Top left) Detection results on MNIST-Hard dataset.\nOur method also tends to misclassify digits when two different digits are similar (4 is misclassiﬁed\nto 9 on the second image). (Top right) the t-SNE plot of latent space. The coloured • are extracted\nlatent vectors where colour indicates the ground truth label for each digit, while black X are learned\nprototypes. (Bottom) Visualizations of learned prototypes. Note that prototypes for 4 and 9 are\nsimilar, sometimes causing misclassiﬁcations on these digits.\n4.1\nMultiple-instance classiﬁcation and detection: MNIST-Hard [1]\nMNIST-Hard contains nine instances of ten different digits—we thus extract K=9 keypoints and\nM=10 prototypes to represent them. The main challenge for this task is that the images may contain\nmultiple instances of the same digit. Note that all the baseline methods used in the landmark detection\ntask (Sec. 4.3) follow the one-class-per-channel paradigm and will thus fail in this scenario. This\nlimitation is also mentioned in MIST [1], which reports that a channel-wise approach, using the same\nauto-encoder as MIST while replacing their single-heatmap approach with K separately-predicted\nchannels, results in poor performance: we list this baseline as ‘Ch.-wise’. We also consider [60], a\ndifferentiable top-k operator based on optimal transport, and a fully supervised version of MIST with\nboth location and class annotation available during training that acts as a performance upper bound.\nTo evaluate detection accuracy we compare the ground-truth bounding boxes to our predictions by\nplacing bounding boxes the approximate size of a digit (20 × 20px) around each keypoint. We\nconsider a prediction to be correct with an intersection over union (IoU) of at least 50%. For the\nclassiﬁcation task, we assign prototypes to each class through bipartite matching, while maximizing\naccuracy. We also report the intersection of the two metrics in Tab. 1. Our method outperforms\nall baselines on the detection task and is close to MIST on the classiﬁcation task. Note that ours is\nthe only unsupervised method: all baselines have access to class labels, or class labels and location.\nQualitative results for detections, prototypes, and t-SNE [40] visualizations are shown in Fig. 3.\n4.2\nObject Discovery: CLEVR [27] and Tetrominoes [27]\nWe also consider a multi-object discovery task using two different datasets: CLEVR and Tetrominoes.\nPerformance is evaluated with the Adjusted Rand Index (ARI) [47, 21], which computes the similarity\nbetween two clusterings, adjusted for the chance grouping of elements, ranging from 0 (random) to 1\n(perfect match). This requires semantic masks, so for this task, we modify our network as outlined in\nSec. 3.2. Our masks are generated by applying an argmax operator over the alpha channels of the K\nper-keypoint reconstructions, and are subsequently used to compute ARI. In addition to the standard\nmetric, we report the classiﬁcation performance of our method, based on shape, color, and size (the\nlatter only for CLEVR)—which none of the baselines are capable of. To calculate classiﬁcation\n7\nTable 2: Quantitative results for CLEVR and Tetrominoes. Adjusted Rand Index (ARI) scores\nfor CLEVR and Tetrominoes (mean ± standard deviation, across ﬁve different seeds). We do not\nreport standard deviation for our method, which is deterministic at inference. Our approach is on\npar with the state of the art in ARI, and can additionally perform unsupervised classiﬁcation. Note\nthat for CLEVR, shape classiﬁcation performance is lower because the dataset contains 3D motion\n(near/far), and some prototype capacity is used to explain this rather than shape, as more pixels\nchange by moving an object from the near to the back plane than e.g. moving between ‘sphere’ and\n‘cube’. This lack of 3D spatial information is currently a limitation of our method.\nCLEVR6\nTetrominoes\nARI\nClassiﬁcation accuracy\nARI\nClassiﬁcation accuracy\nShape\nColor\nSize\nShape\nColor\nIODINE [16]\n98.8 ± 0.0\n-\n-\n-\n99.2 ± 0.4\n-\n-\nMONet [3]\n96.2 ± 0.6\n-\n-\n-\n-\n-\n-\nSlot Attention [36]\n98.8 ± 0.3\n-\n-\n-\n99.5 ± 0.2\n-\n-\nOurs\n98.3\n46.8\n90.7\n97.1\n99.7\n91.3\n100\nDetections\nRecon.\nMask\nKp 1\nKp 2\nKp 3\nKp 4\nKp 5\nKp 6\nAlpha 1\nAlpha 2\nAlpha 3\nAlpha 4\nAlpha 5\nAlpha 6\nFigure 4: Visualization on CLEVR. Because our method does not explicitly model the background,\nthe semantic masks have vague boundaries. However, the individual alpha channel shows accurate\nobject masks. (Top) Our method successfully captures all six objects in the image. Notice that the\ngreen cylinder on the left is partially occluded by the red sphere, but as we enforce features to be close\nto prototypes, the individually rendered images show a full cylinder, and the occlusion information\nis embedded in the alpha map. (Bottom) Four objects are detected with high conﬁdence and the\nremaining two are unused, showing that how our approach can handle a variable number of objects.\naccuracy, as our method is fully unsupervised, we simply assign, for each prototype, the properties\n(shape, color, size) that best represent the prototype. We use K=6 keypoints and M=48 prototypes\nfor CLEVR and K=3 and M=114 for Tetrominoes, which are the maximum number of objects\nand the number of combinations of shape, color, and size, respectively. We report the results in\nTable 2, showing that our method performs on par with the state of the art, and additionally performs\nunsupervised classiﬁcation. We show qualitative examples and segmentation masks in Figs. 4 and 5.\n4.3\nLandmark Detection: CelebA-in-the-wild [35] and H36M [22]\nTable 3: Performance on CelebA-in-the-wild and\nHuman3.6M. The error is normalized by inter-\nocular distance for CelebA and by image size for\nH36M.\nCelebA (K=4)\nH36M (K=16)\nSingle obj.\nThewlis [53]\n-\n7.51%\nZhang [64]\n-\n4.91%\nJakab [23]\n19.42%\n-\nLorenz [37]\n15.49%\n2.79%\nHe [19]\n25.81%\n-\nHe [19] (tuned)\n12.10%\n-\nMulti obj.\nOurs\n18.49%\n6.88%\nWe extract K=4 keypoints with M=32 proto-\ntypes for CelebA and K=16 keypoints with\nM=32 prototypes for H36M. As noted previ-\nously, we follow [53, 64, 23, 37] for evaluation\nand train a linear layer without bias term using\nground truth landmark annotations to map the\nlearned landmarks to a set of human-deﬁned\nlandmarks, so that the estimated landmarks are\nentirely dependent on the detected keypoints.\nWe place the frame of reference for the key-\npoints at the top left corner of the image to be\nconsistent with previous papers. In existing land-\nmark detection methods, the linear layer has an input size K × 2 and an output size T × 2, where\nK is the number of heatmap channels and T is the number of human-deﬁned landmarks, which is\npossible, as they assume a keypoint to be detected precisely once. This is not directly applicable to\nour method, as we may detect more than one keypoint belonging to the same prototype—e.g., in an\nextreme case, all K keypoints could belong to a single prototype.\nTo support multiple detections, we form the input to our linear layer as a M × K × 2 tensor, which is\npopulated while preserving prototype information through the indexing: for instance, if n keypoints\n8\nDetections\nRecon.\nMask\nKp 1\nKp 2\nKp 3\nAlpha 1\nAlpha 2\nAlpha 3\nPrototypes\nFigure 5: Tetrominoes dataset. Each prototype represents unique shape-color combination.\nFigure 6: Results on CelebA and H36M. We show detection results on columns one and three, with\nthe top row displaying the points detected by our approach, and the bottom row those generated by\nthe regressor. Columns two and four show the clustering of prototypes and features using t-SNE.\nbelong to prototype k, we ﬁrst sort them by their x coordinate from left to right, and then ﬁll the\nsorted keypoint coordinates into the input tensor at [k, 0] to [k, n −1]. Unused entries are simply set\nto zero—note that only K coordinates are non-zero, the same as for baseline methods.\nWe report quantitative results in Tab. 3 and show detections and clustering results in Fig. 6. Our\nmethod provides comparable performance to the methods targetting single objects but is still able to\ndeal with multiple instances, which none of the baseline methods can do; see Fig. 1. While there is\nstill a gap between our method and the best performing single-instance landmark detector, which is a\nlimitation of our method, considering how the single-instance assumption simpliﬁes the problem, our\nresults are promising.\n4.4\nAblation study\n4.4.1\nEffect of different loss terms\nTable 4: Ablation on the loss function and training strategy.\nLrecon\nLsw\nLkm\nLeqv\nIterative training\nClassif.\nLocalization\nBoth\n✓\n✓\n10.3%\n99.9%\n10.3%\n✓\n✓\n✓\n40.7%\n99.9%\n40.7%\n✓\n✓\n✓\n✓\n78.4%\n99.6%\n78.0%\n✓\n✓\n✓\n✓\n71.6%\n99.9%\n71.6%\n✓\n✓\n✓\n✓\n✓\n92.8%\n99.9%\n92.8%\nWe rely on multiple loss terms and\niterative training. Tab. 4 reports their\neffect on MNIST-Hard (Sec. 4.1), and\nFig. 7 shows the corresponding results in the quality of the clustering with t-SNE [40]. Note that\nthe reconstruction loss is used in all the runs in Tab. 4, as it is the main loss term used to train the\nencoder and decoder—our method will not work without it. The other loss terms are used to regulate\nthe latent space and encourage clustering, which has a signiﬁcant impact on learning semantically\nmeaningful descriptors.\n(a) Lrecon\n(b) Lrecon + Lsw\n(c) Lrecon + Lsw + Lkm\n(d) single stage\n(e) all loss terms\nFigure 7: Ablation study w.r.t. clustering results. We illustrate the effect of the different compo-\nnents used for training by showing the clustering of features and prototypes with t-SNE. Features\nare indicated by • (color indicates ground truth label), and prototypes as ×. (a) vs (b) shows that\nthe sliced Wasserstein loss is necessary to move the prototypes so they match the distribution of the\nfeatures; (b) vs (c) shows that the clustering loss encourages the feature space to form clusters; (d) vs\n(e) shows the importance of our iterative training approach; (c) vs (e) shows that the equivariance\nloss further improves the result. Note that these ﬁve plots correspond to the ﬁve rows in Tab. 4.\n9\n3, 5, 8\n2\n1, 7\n4, 9\n0, 6\n(a) 5 prototypes: Each prototype learns to\nrepresent multiple digits.\n(b) 20 prototypes: Prototypes learn to represent\ndifferent modes for the digits (shown: ﬁve).\nFigure 8: Different number of prototypes on MNIST-Hard. (Left) Features are indicated by •\n(color indicates ground truth label), prototypes as ×. (Right) Visualizations of the prototypes.\nTable 5: Ablation on number of prototypes\nM=5\nM=5 (reverse)\nM=10\nM=20\nLocalization\n90.8%\n90.8%\n99.9%\n99.9%\nClassiﬁcation\n47.4%\n98.3%\n92.8%\n87.7%\nBoth\n43.0%\n89.2%\n92.8%\n87.7%\nTable 6: Ablation on clustering strategy\nClassiﬁcation\nLocalization\nBoth\nK-means\n21.1%\n99.5%\n99.9%\nVector Quantization\n23.7%\n65.3%\n15.3%\nSliced Wasserstein\n92.8%\n99.9%\n92.8%\n4.4.2\nNumber of Prototypes\nWe ablate the number of prototypes on MNIST-Hard in Tab. 5. If it is smaller than the number of\nclasses, each prototype learns to represent multiple classes, as shown in Fig. 8(a). With 5 prototypes\nrepresenting 10 classes, classiﬁcation accuracy will be at most 50%: we achieve 47.4%. We can also\nevaluate the reverse mapping from class label to prototypes and check the consistency of assignments,\nwhich gives us 98.3% (note that this is not accuracy). With more prototypes than classes, prototypes\nlearn to represent different modes of the class, as shown in Fig. 8(b). In general, a reasonable result\ncan be achieved if the number of prototypes M is larger than the number of classes.\n4.4.3\nClustering Strategy\nWe investigate the importance of the sliced Wasserstein loss in Tab. 6 by replacing it with a K-means\nloss used in [41] and a quantization loss used in [56], while keeping everything else the same. The\nK-means loss does not encourage a balanced assignment among features and prototypes, and all\nthe features may collapse to a single prototype, resulting in a low classiﬁcation score. The Vector\nQuantization loss discretizes the latent space using a code-book—said codes are similar to our learned\nprototypes. However by enforcing same-class objects to be encoded into a single code, the network\nloses the ability to model intra-class variance. By contrast, our method uses prototypes to model\ninter-class variance, modeled with a Gaussian distribution.\n5\nConclusion\nWe introduce a novel method able to learn keypoints without any supervision signal. We improve\non prior work by removing the key limitation of predicting separate channels for each keypoint,\nwhich requires isolated instances . We achieve this with a novel feature clustering strategy based on\nprototypes, and show that our method can be applied to a wide range of tasks and datasets. We provide\na discussion on the potential societal impact of our method in the Supplementary Material.\nLimitations. Our approach does not have a concept of 3D structure and has limited applicability to\nscenarios where scene understanding requires compositionality (see shape classiﬁcation on Tab. 2). It\nis also unable to model the background. We plan to address these limitations in future work.\nAcknowledgments and Disclosure of Funding\nThis work was supported by the Natural Sciences and Engineering Research Council of Canada\n(NSERC) Discovery Grant / Collaborative Research and Development Grant, Google, Digital Re-\nsearch Alliance of Canada, and Advanced Research Computing at the University of British Columbia.\n10\nReferences\n[1] Angles, B., Jin, Y., Kornblith, S., Tagliasacchi, A., Yi, K.M.: MIST: Multiple Instance Spatial\nTransformer. In: Conference on Computer Vision and Pattern Recognition (2021) 3, 6, 7\n[2] Bengio, Y., Courville, A., Vincent, P.: Representation Learning: A Review and New Perspectives.\nIEEE Transactions on Pattern Analysis and Machine Intelligence (2013) 3\n[3] Burgess, C.P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., Lerchner, A.:\nMonet: Unsupervised scene decomposition and representation. arXiv Preprint (2019) 2, 3, 5, 6,\n8\n[4] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised Learning\nof Visual Features by Contrasting Cluster Assignments. In: Advances in Neural Information\nProcessing Systems (2020) 5\n[5] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A Simple Framework for Contrastive Learning\nof Visual Representations. In: International Conference on Machine Learning (2020) 5, 6\n[6] Deshpande, I., Zhang, Z., Schwing, A.G.: Generative Modeling Using the Sliced Wasserstein\nDistance. In: Conference on Computer Vision and Pattern Recognition (2018) 2, 5\n[7] DeTone, D., Malisiewicz, T., Rabinovich, A.: Superpoint: Self-Supervised Interest Point\nDetection and Description. In: Conference on Computer Vision and Pattern Recognition\nWorkshops (2018) 1, 4, 6\n[8] Duda, R.O., Hart, P.E., Stork, D.G.: Pattern Classiﬁcation and Scene Analysis, vol. 3. Wiley\nNew York (1973) 5\n[9] Dusmanu, M., Rocco, I., Pajdla, T., Pollefeys, M., Sivic, J., Torii, A., Sattler, T.: D2-net:\nA Trainable CNN for Joint Description and Detection of Local Features. In: Conference on\nComputer Vision and Pattern Recognition (2019) 1, 4\n[10] Engelcke, M., Kosiorek, A.R., Parker Jones, O., Posner, I.: GENESIS: Generative Scene Infer-\nence and Sampling with Object-Centric Latent Representations. In: International Conference\non Learning Representations (2020) 3\n[11] Engelcke, M., Parker Jones, O., Posner, I.: Genesis-v2: Inferring Unordered Object Representa-\ntions without Iterative Reﬁnement. In: Advances in Neural Information Processing Systems\n(2021) 3\n[12] Freda, L.: pySLAM. https://github.com/luigifreda/pyslam (2022), [Online; accessed\nMay 16, 2022] 1, 4\n[13] Gall, J., Yao, A., Razavi, N., Van Gool, L., Lempitsky, V.: Hough Forests for Object Detec-\ntion, Tracking, and Action Recognition. IEEE Transactions on Pattern Analysis and Machine\nIntelligence (2011) 1\n[14] Girshick, R.: Fast R-CNN. In: International Conference on Computer Vision (2015) 3\n[15] Grauman, K., Darrell, T.: The Pyramid Match Kernel: Discriminative Classiﬁcation with Sets\nof Image Features. In: International Conference on Computer Vision (2005) 1\n[16] Greff, K., Kaufman, R.L., Kabra, R., Watters, N., Burgess, C., Zoran, D., Matthey, L., Botvinick,\nM., Lerchner, A.: Multi-Object Representation Learning with Iterative Variational Inference. In:\nInternational Conference on Machine Learning (2019) 2, 3, 5, 6, 8\n[17] Hadsell, R., Chopra, S., LeCun, Y.: Dimensionality Reduction by Learning an Invariant\nMapping. In: Conference on Computer Vision and Pattern Recognition (2006) 5\n[18] He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In: International Conference on\nComputer Vision (2017) 3\n[19] He, X., Wandt, B., Rhodin, H.: LatentKeypointGAN: Controlling GANs via Latent Keypoints.\nIn: Conference on Computer Vision and Pattern Recognition Workshops (2022) 2, 3, 8\n[20] Hinton, G., Salakutdinov, R.: Reducing the Dimensionality of Data with Neural Networks.\nScience (2006) 3\n[21] Hubert, L., Arabie, P.: Comparing partitions. Journal of classiﬁcation (1985) 7\n[22] Ionescu, C., Papava, I., Olaru, V., Sminchisescu, C.: Human3.6M: Large Scale Datasets and\nPredictive Methods for 3D Human Sensing in Natural Environments. IEEE Transactions on\nPattern Analysis and Machine Intelligence (2014) 5, 6, 8, 18\n11\n[23] Jakab, T., Gupta, A., Bilen, H., Vedaldi, A.: Unsupervised Learning of Object Landmarks\nthrough Conditional Image Generation. In: Advances in Neural Information Processing Systems\n(2018) 1, 2, 3, 4, 6, 8\n[24] Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual Losses for Real-Time Style Transfer and Super-\nResolution. In: European Conference on Computer Vision (2016) 5\n[25] Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., Girshick, R.:\nClevr: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. In:\nConference on Computer Vision and Pattern Recognition (2017) 3\n[26] Jégou, H., Douze, M., Schmid, C., Pérez, P.: Aggregating Local Descriptors into a Compact\nImage Representation. In: Conference on Computer Vision and Pattern Recognition (2010) 1\n[27] Kabra, R., Burgess, C., Matthey, L., Kaufman, R.L., Greff, K., Reynolds, M., Lerchner, A.:\nMulti-object datasets. https://github.com/deepmind/multi-object-datasets/ (2019) 5, 6, 7, 17\n[28] Kanazawa, A., Jacobs, D.W., Chandraker, M.: Warpnet: Weakly Supervised Matching for\nSingle-View Reconstruction. In: Conference on Computer Vision and Pattern Recognition\n(2016) 1, 6\n[29] Kingma, D.P., Welling, M.: Auto-Encoding Variational Bayes. In: International Conference on\nLearning Representations (2014) 3\n[30] Kipf, T., Elsayed, G.F., Mahendran, A., Stone, A., Sabour, S., Heigold, G., Jonschkowski, R.,\nDosovitskiy, A., Greff, K.: Conditional Object-Centric Learning from Video. In: ICLR (2022) 3\n[31] Lazebnik, S., Schmid, C., Ponce, J.: Beyond Bags of Features: Spatial Pyramid Matching\nfor Recognizing Natural Scene Categories. In: Conference on Computer Vision and Pattern\nRecognition (2006) 1\n[32] Leibe, B., Leonardis, A., Schiele, B.: Combined Object Categorization and Segmentation with\nan Implicit Shape Model. In: Workshop on Statistical Learning in Computer Vision (2004) 1\n[33] Lenc, K., Vedaldi, A.: Learning Covariant Feature Detectors. In: European Conference on\nComputer Vision (2016) 4\n[34] Liu, S., Zhang, L., Yang, X., Su, H., Zhu, J.: Unsupervised Part Segmentation Through Disen-\ntangling Appearance and Shape. In: Conference on Computer Vision and Pattern Recognition\n(2021) 6\n[35] Liu, Z., Luo, P., Wang, X., Tang, X.: Deep Learning Face Attributes in the Wild. In: International\nConference on Computer Vision (2015) 2, 5, 6, 8, 17\n[36] Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J.,\nDosovitskiy, A., Kipf, T.: Object-Centric Learning with Slot Attention. In: Advances in Neural\nInformation Processing Systems (2020) 2, 3, 5, 6, 8, 15\n[37] Lorenz, D., Bereska, L., Milbich, T., Ommer, B.: Unsupervised Part-Based Disentangling of\nObject Shape and Appearance. In: Conference on Computer Vision and Pattern Recognition\n(2019) 1, 2, 3, 4, 6, 8\n[38] Lowe, D.G.: Distinctive Image Features from Scale-Invariant Keypoints. International Journal\nof Computer Vision (2004) 1, 4\n[39] Lynen, S., Zeisl, B., Aiger, D., Bosse, M., Hesch, J., Pollefeys, M., Siegwart, R., Sattler,\nT.: Large-Scale, Real-Time Visual–Inertial Localization Revisited. International Journal of\nRobotics Research (2020) 1, 4\n[40] van der Maaten, L., Hinton, G.: Visualizing Data using t-SNE. Journal of Machine Learning\nResearch (2008) 7, 9\n[41] Moradi Fard, M., Thonet, T., Gaussier, E.: Deep k-Means: Jointly clustering with k-Means and\nlearning representations. Pattern Recognition Letters (2020) 3, 10\n[42] Mur-artal, R., Montiel, J., Tardós, J.: Orb-Slam: A Versatile and Accurate Monocular Slam\nSystem. IEEE Transactions on Robotics and Automation (2015) 1, 4\n[43] Newell, A., Yang, K., Deng, J.: Stacked Hourglass Networks for Human Pose Estimation. In:\nEuropean Conference on Computer Vision (2016) 2\n[44] Noh, H., Araujo, A., Sim, J., Weyand, T., Han, B.: Large-Scale Image Retrieval with Attentive\nDeep Local Features. In: International Conference on Computer Vision (2017) 1\n12\n[45] Ono, Y., Trulls, E., Fua, P., Yi, K.M.: LF-Net: Learning Local Features from Images. In:\nAdvances in Neural Information Processing Systems (2018) 5\n[46] Qi, C.R., Litany, O., He, K., Guibas, L.J.: Deep Hough Voting for 3D Object Detection in Point\nClouds. In: International Conference on Computer Vision (2019) 1\n[47] Rand, W.M.: Objective Criteria for the Evaluation of Clustering Methods. Journal of the\nAmerican Statistical association (1971) 7\n[48] Revaud, J., Weinzaepfel, P., De Souza, C., Pion, N., Csurka, G., Cabon, Y., Humenberger, M.:\nR2D2: Repeatable and Reliable Detector and Descriptor. In: Advances in Neural Information\nProcessing Systems (2019) 1, 6\n[49] Rhodin, H., Salzmann, M., Fua, P.: Unsupervised Geometry-Aware Representation for 3D\nHuman Pose Estimation. In: European Conference on Computer Vision (2018) 2\n[50] Rhodin, H., Spoerri, J., Katircioglu, I., Constantin, V., Meyer, F., Moeller, E., Salzmann, M., Fua,\nP.: Learning Monocular 3D Human Pose Estimation from Multi-View Images. In: Conference\non Computer Vision and Pattern Recognition (2018) 2\n[51] Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional Networks for Biomedical Image\nSegmentation. In: Conference on Medical Image Computing and Computer Assisted Interven-\ntion (2015) 3, 4\n[52] Sattler, T., Leibe, B., Kobbelt, L.: Efﬁcient & Effective Prioritized Matching for Large-Scale\nImage-Based Localization. IEEE Transactions on Pattern Analysis and Machine Intelligence\n(2016) 1, 4\n[53] Thewlis, J., Bilen, H., Vedaldi, A.: Unsupervised Learning of Object Landmarks by Factorized\nSpatial Embeddings. In: International Conference on Computer Vision (2017) 1, 2, 6, 8\n[54] Thewlis, J., Bilen, H., Vedaldi, A.: Unsupervised object learning from dense equivariant image\nlabelling. In: Advances in Neural Information Processing Systems (2017) 2\n[55] Tyszkiewicz, M., Fua, P., Trulls, E.: DISK: Learning Local Features with Policy Gradient. In:\nAdvances in Neural Information Processing Systems (2020) 1, 4\n[56] Van Den Oord, A., Vinyals, O., Kavukcuoglu, K.: Neural discrete representation learning. In:\nAdvances in Neural Information Processing Systems (2017) 3, 10\n[57] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.A., Bottou, L.: Stacked denoising\nautoencoders: Learning useful representations in a deep network with a local denoising criterion.\nJournal of machine learning research (2010) 3\n[58] Wang, Q., Zhou, X., Hariharan, B., Snavely, N.: Learning Feature Descriptors using Camera\nPose Supervision. In: European Conference on Computer Vision (2020) 4\n[59] Wu, Y., Ji, Q.: Robust Facial Landmark Detection under Signiﬁcant Head Poses and Occlusion.\nIn: International Conference on Computer Vision (2015) 2\n[60] Xie, Y., Dai, H., Chen, M., Dai, B., Zhao, T., Zha, H., Wei, W., Pﬁster, T.: Differentiable Top-k\nOperator with Optimal Transport. In: Advances in Neural Information Processing Systems\n(2020) 3, 7\n[61] Yang, B., Fu, X., Sidiropoulos, N.D., Hong, M.: Towards K-means-friendly Spaces: Simultane-\nous Deep Learning and Clustering. In: International Conference on Machine Learning (2017)\n3\n[62] Yang, H., Dong, W., Carlone, L., Koltun, V.: Self-Supervised Geometric Perception. In:\nConference on Computer Vision and Pattern Recognition (2021) 1, 4\n[63] Yi, K.M., Trulls, E., Lepetit, V., Fua, P.: LIFT: Learned Invariant Feature Transform. In:\nEuropean Conference on Computer Vision (2016) 4, 5, 14\n[64] Zhang, Y., Guo, Y., Jin, Y., Luo, Y., He, Z., Lee, H.: Unsupervised Discovery of Object\nLandmarks as Structural Representations. In: Conference on Computer Vision and Pattern\nRecognition (2018) 1, 2, 3, 4, 6, 8\n[65] Zhang, Z., Luo, P., Loy, C.C., Tang, X.: Learning Deep Representation for Face Alignment with\nAuxiliary Attributes. IEEE Transactions on Pattern Analysis and Machine Intelligence (2015) 2\n13\nA\nSupplementary material\nThe appendix includes a detailed description of the soft-argmax used in encoder E in Sec. A.1, the\nsampling strategy for prototypes and pseudo-code for prototype training in Sec. A.2, model general-\nization on CLEVR10 in Sec. A.3, architecture details in Sec. A.4, a table of all the hyperparameters\nfor the different datasets in Sec. A.5, more qualitative results on occlusion modelling in Sec. A.6,\na summary of the computational resources used to train the models in Sec. A.7, dataset licenses in\nSec. A.8, and a discussion on potential societal impact in Sec. A.9.\nA.1\nDetailed Description of the Soft-Argmax in Encoder E\nIn the encoder E, we ﬁrst generate a scoremap S the same shape as input image I with a U-Net-style\nnetwork, followed by a sigmoid activation, whose output is in [0, 1]. Then, we apply NMS on the\nscoremap S and select top K keypoints T from it. However, this top-k selection is non-differentiable—\nspeciﬁcally, the gradients only ﬂow back through the K locations on the H × W scoremap S. To\navoid sparsity in the gradient, we construct a kernel of size B × B centered at each keypoint location\nand use a soft-argmax [63] function to compute the ﬁnal keypoint location as a weighted sum of\nthe region around the center of the kernel. By using these weighted centers, the gradient ﬂows back\nthrough B × B locations for each keypoint, rather than just one. As a by-product, we also obtain\nsub-pixel accuracy on the keypoints.\nLi =\nP\nc∈kernel{Ti} c · exp (S[cx, cy]/τ)\nP\nc∈kernel{Ti} exp (S[cx, cy]/τ)\n(5)\nwhere kernel{Ti} are the points within a kernel of size B × B centered at point Ti, and τ is a hyper\nparameter to control the hardness of the softmax operation.\nA.2\nPrototype Learning\nA.2.1\nSampling for Sliced Wasserstein Loss\nIn order to apply the sliced Wasserstein loss to match the distribution of the prototypes to the\ndistribution of the descriptors, we require an equal number of prototypes and descriptors. Thus, we\nmodel the descriptors as a Gaussian Mixture Model (GMM) with a small predeﬁned variance Σ (a\nhyperparameter), since we would ideally want the descriptors to form very sharp modes around the\nGaussian centers. Thus, denoting the sampled descriptors from the prototype GMM as ˜D we sample\nB × K samples from\np( ˜\nDi) =\nX\nj∈[1,M]\nπjN( ˜Di|Pj, Σ),\n(6)\nwhere πj are the mixture weights of the GMM for the j-th prototype. For the mixture weight πj, note\nthat each prototype may have a different number of descriptors associated with it. We thus deﬁne\nit according to the ratio of descriptors that are associated with each prototype—we associate via\nﬁnding the nearest prototype with the ℓ2 norm—but with a term that encourages exploration when the\ncompactness of the prototypes is not equal. For example, when a certain prototype dominates but is\nwidely spread, we would ideally want to explore using not just this single prototype. Mathematically,\ndenoting the ratio of descriptors associated with prototype m as rm, and the variance of the descriptors\nassociated with the prototype as σm, we write\nαm = rm + Var({σi, i ∈[1, M]}),\n(7)\nand\nπm =\nαm\nP\nm∈[1,M] αm\n.\n(8)\nFor prototypes without any descriptors assigned, we simply set σm = 1.\nA.2.2\nPseudo-code for the prototype learning algorithm\nAs stated in Sec. 3.3, within each training iteration, we ﬁrst optimize encoder and decoder with ﬁxed\nprototypes. We then optimize the prototypes with a ﬁxed encoder/decoder. Here we provide a detailed\npseudo-code version of the algorithm we use for prototype optimization with a sliced Wasserstein\nloss, shown in Tab. 7.\n14\nTable 7: Prototype Training: Pseudo code for prototype training.\nRequirement: D: descriptors, M: number of descriptors, P: prototypes,\nN: number of prototypes.\nFunction TrainPrototype (D, P)\n1. Calculate mixing ratio πm\n• Dm ←divide D into M subsets where each subset is associated to a member in P\nin terms of smallest ℓ2 norm\n• rm ←calculate the ratio of Dm in D\n• σm ←calculate variance of Dm\n• σ ←calculate variance of σm\n• αm ←rm +σ\n• πm ←αm /P αm\n2. Sample from GMM\n• ˜P ←initiate empty list\n• for pm in P :\n•\nappend πm × N samples from a Gaussian centered at pm with a predeﬁned\nvariance to ˜P\n3. Calculate sliced Wasserstein distance\n• d ←SWdistance (˜P,D)\n4. Train prototypes\n• Optimize P by minimizing d\nTable 8: Quantitative results for CLEVR6 and CLEVR10. We train the model on CLEVR6, and\nevaluate on both CLEVR6 and CLEVR10.\nARI\nClassiﬁcation accuracy\nShape\nColor\nSize\nCLEVR6\n98.6\n53.5\n91.0\n95.8\nCLEVR10\n97.6\n52.5\n90.7\n97.9\nA.3\nGeneralization to CLEVR10\nWe demonstrate that our approach generalizes to a larger number of instances without any retraining\nin Tab. 8, where we follow [36] to evaluate our CLEVR6 -trained model on CLEVR10 (which\ncontains up to 10 objects). Our model can deal with a larger number of instances and generalizes\nvery well, with only a very small drop in performance. We do not report numbers for the baselines as\nthey are not available.\nA.4\nArchitecture Detail\nWe summarize all the main components in our pipeline in Tab. 9.\nA.5\nHyperparameter Table\nWe report the hyperparameters we use for each dataset/task in Tab. 10. The number of prototypes and\nkeypoints are set according to the dataset characteristics. The softmax kernel size was also adjusted to\nmatch the dataset image size and the rough size of the object of interest. Batch sizes were determined\naccording to the memory limit of our GPU. Except for CelebA, all settings are similar\n.\n15\nTable 9: Architecture Detail. A list of all the main components in our pipeline.\nGeneral Architecture\nArchitecture for Object Discovery task\nEncoder E\nI ∈[0, 1]H×W ×3\nUNet(I) →H ∈RH×W , F ∈RH×W ×32\nSigmoid(H) →S ∈[0, 1]H×W\nKeypoints Sampling\nNMS(S) →˜S ∈[0, 1]H×W\nTop-K(˜S) →P ∈NK×2\nSoft-Argmax(P) →L ∈RK×2\nBilinear Sampling(F, L) →D ∈RK×32\nSparse Reconstruction\nGaussian Conv.(L, D) →R ∈RK×H×W ×32\nSummation(R) →˜F ∈RH×W ×32\n-\nDecoder D\nUNet(˜F) →˜I ∈[0, 1]H×W ×3\nUNet(R) →A ∈[0, 1]H×W ×4\n-\nAlpha-Blending(A) →˜I ∈[0, 1]H×W ×3\nTable 10: List of Hyper Parameters.\nMNIST\nCLEVR6\nTetrominoes\nCelebA\nH36M\n# Keypoints\n9\n6\n3\n4\n16\n# Prototypes\n10\n48\n114\n32\n32\nSoftmax Kernel Size\n13\n21\n27\n21\n11\nΣ in GMM\n4e-4 (all)\nRecon. Loss Type\nMSE\nMSE\nMSE\nPerceptual\nMSE\nCoef. Recon. Loss\n1 (all)\nCoef. Cluster Loss\n0.01 (all)\nCoef. Eqv. Heatmap Loss\n0.01\n0.01\n0.01\n0.05\n0.01\nCoef. Eqv. Featuremap Loss\n100\n100\n100\n500\n100\nEncoder & Decoder Learning Rate\n0.001 (all)\nPrototype Learning Rate\n0.1 (all)\nBatch Size\n40\n64\n76\n32\n32\nA.6\nAdditional results on CLEVR dataset with occluded\nIn Fig. 4 we mentioned that our model can embed the occlusion information in the alpha channel.\nUnfortunately, the CLEVR dataset does not provide any annotation that can be used to evaluate\nocclusion quantitatively. Instead, we include more qualitative results in Fig. 9.\nA.7\nComputation Resources\nWe train all our models on NVIDIA V100 GPUs with 32GB of RAM. Training on MNIST converges\nwithin 12 hours on a single GPU. For the other four datasets, we use two GPUs, which allows for\nlarger batch size, for about 24 hours.\n16\nDetections\nRecon.\nMask\nKp 1\nKp 2\nKp 3\nKp 4\nKp 5\nKp 6\nAlpha 1\nAlpha 2\nAlpha 3\nAlpha 4\nAlpha 5\nAlpha 6\nFigure 9: More qualitative results on modelling occlusion. Occluded regions are marked with red\nbounding boxes.\nA.8\nDataset Licenses\nA.8.1\nMNIST http://yann.lecun.com/exdb/mnist/\nThe MNIST-Hard dataset is derived from the MNIST dataset. Below is the license for the original\nMNIST dataset:\nYann LeCun and Corinna Cortes hold the copyright of MNIST dataset, which is a derivative work\nfrom the original NIST datasets. The MNIST dataset is made available under the terms of the Creative\nCommons Attribution-Share Alike 3.0 license.\nA.8.2\nCLEVR[27] https://github.com/deepmind/multi_object_datasets\nApache-2.0 license.\nA.8.3\nTetrominoes[27] https://github.com/deepmind/multi_object_datasets\nApache-2.0 license.\nA.8.4\nCelebA[35] http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n1. CelebA dataset is available for non-commercial research purposes only.\n2. All images of the CelebA dataset are obtained from the Internet which are not property of\nMMLAB, The Chinese University of Hong Kong. The MMLAB is not responsible for the\ncontent nor the meaning of these images.\n3. You agree not to reproduce, duplicate, copy, sell, trade, resell or exploit for any commercial\npurposes, any portion of the images and any portion of derived data.\n4. You agree not to further copy, publish or distribute any portion of the CelebA dataset. Except,\nfor internal use at a single site within the same organization it is allowed to make copies of\nthe dataset.\n17\n5. The MMLAB reserves the right to terminate your access to the CelebA dataset at any time.\n6. The face identities are released upon request for research purposes only. Please contact us\nfor details.\nA.8.5\nH36M[22] http://vision.imar.ro/human3.6m/description.php\n1. GRANT OF LICENSE FREE OF CHARGE FOR ACADEMIC USE ONLY Licenses free\nof charge are limited to academic use only. Provided you send the request from an academic\naddress, you are granted a limited, non-exclusive, non-assignable and non-transferable\nlicense to use this dataset subject to the terms below. This license is not a sale of any or all\nof the owner’s rights. This product may only be used by you, and you may not rent, lease,\nlend, sub-license or transfer the dataset or any of your rights under this agreement to anyone\nelse.\n2. NO WARRANTIES The authors do not warrant the quality, accuracy, or completeness of\nany information, data or software provided. Such data and software is provided \"AS IS\"\nwithout warranty or condition of any nature. The authors disclaim all other warranties,\nexpressed or implied, including but not limited to implied warranties of merchantability and\nﬁtness for a particular purpose, with respect to the data and any accompanying materials.\n3. RESTRICTION AND LIMITATION OF LIABILITY In no event shall the authors be liable\nfor any other damages whatsoever arising out of the use of, or inability to use this dataset\nand its associated software, even if the authors have been advised of the possibility of such\ndamages.\n4. RESPONSIBLE USE It is YOUR RESPONSIBILITY to ensure that your use of this product\ncomplies with these terms and to seek prior written permission from the authors and pay any\nadditional fees or royalties, as may be required, for any uses not permitted or not speciﬁed\nin this agreement.\n5. ACCEPTANCE OF THIS AGREEMENT Any use whatsoever of this dataset and its associ-\nated software shall constitute your acceptance of the terms of this agreement. By using the\ndataset and its associated software, you agree to cite the papers of the authors, in any of your\npublications by you and your collaborators that make any use of the dataset, in the following\nformat (NOTICE THAT CITING THE DATASET URL INSTEAD OF THE PUBLICA-\nTIONS, WOULD NOT BE COMPLIANT WITH THIS LICENSE AGREEMENT):\nA.8.6\nMulti-face images\nAll multi-face images are in the public domain.\nA.9\nSocietal Impact\nOur method would be a front-end for a typical computer vision pipeline and thus is not immediately\nlinked to any particular application with signiﬁcant societal impact. Nonetheless, our method would\nfacilitate unsupervised learning from images which could have signiﬁcant impact down the road.\nUnsupervised learning would signiﬁcantly reduce the need for human effort within any automated\nworkﬂow, which would bring both convenience and a certain amount of change—the latter may\nrequire careful consideration when adopting these technologies more widely.\n18\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2022-06-16",
  "updated": "2023-01-13"
}