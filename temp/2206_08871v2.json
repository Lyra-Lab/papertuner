{
  "id": "http://arxiv.org/abs/2206.08871v2",
  "title": "How Robust is Unsupervised Representation Learning to Distribution Shift?",
  "authors": [
    "Yuge Shi",
    "Imant Daunhawer",
    "Julia E. Vogt",
    "Philip H. S. Torr",
    "Amartya Sanyal"
  ],
  "abstract": "The robustness of machine learning algorithms to distributions shift is\nprimarily discussed in the context of supervised learning (SL). As such, there\nis a lack of insight on the robustness of the representations learned from\nunsupervised methods, such as self-supervised learning (SSL) and auto-encoder\nbased algorithms (AE), to distribution shift. We posit that the input-driven\nobjectives of unsupervised algorithms lead to representations that are more\nrobust to distribution shift than the target-driven objective of SL. We verify\nthis by extensively evaluating the performance of SSL and AE on both synthetic\nand realistic distribution shift datasets. Following observations that the\nlinear layer used for classification itself can be susceptible to spurious\ncorrelations, we evaluate the representations using a linear head trained on a\nsmall amount of out-of-distribution (OOD) data, to isolate the robustness of\nthe learned representations from that of the linear head. We also develop\n\"controllable\" versions of existing realistic domain generalisation datasets\nwith adjustable degrees of distribution shifts. This allows us to study the\nrobustness of different learning algorithms under versatile yet realistic\ndistribution shift conditions. Our experiments show that representations\nlearned from unsupervised learning algorithms generalise better than SL under a\nwide variety of extreme as well as realistic distribution shifts.",
  "text": "HOW ROBUST IS UNSUPERVISED REPRESENTATION\nLEARNING TO DISTRIBUTION SHIFT?\nYuge Shi∗\nDepartment of Engineering Science\nUniversity of Oxford\nImant Daunhawer & Julia E. Vogt\nDepartment of Computer Science\nETH Zurich\nPhilip H.S. Torr\nDepartment of Engineering Science\nUniversity of Oxford\nAmartya Sanyal\nDepartment of Computer Science & ETH AI Center\nETH Zurich\nABSTRACT\nThe robustness of machine learning algorithms to distributions shift is primarily\ndiscussed in the context of supervised learning (SL). As such, there is a lack of\ninsight on the robustness of the representations learned from unsupervised methods,\nsuch as self-supervised learning (SSL) and auto-encoder based algorithms (AE),\nto distribution shift. We posit that the input-driven objectives of unsupervised\nalgorithms lead to representations that are more robust to distribution shift than the\ntarget-driven objective of SL. We verify this by extensively evaluating the perfor-\nmance of SSL and AE on both synthetic and realistic distribution shift datasets.\nFollowing observations that the linear layer used for classiﬁcation itself can be\nsusceptible to spurious correlations, we evaluate the representations using a linear\nhead trained on a small amount of out-of-distribution (OOD) data, to isolate the\nrobustness of the learned representations from that of the linear head. We also\ndevelop “controllable” versions of existing realistic domain generalisation datasets\nwith adjustable degrees of distribution shifts. This allows us to study the robust-\nness of different learning algorithms under versatile yet realistic distribution shift\nconditions. Our experiments show that representations learned from unsupervised\nlearning algorithms generalise better than SL under a wide variety of extreme as\nwell as realistic distribution shifts.\n1\nINTRODUCTION\nMachine Learning (ML) algorithms are classically designed under the statistical assumption that the\ntraining and test data are drawn from the same distribution. However, this assumption does not hold in\nmost cases of real world deployment of ML systems. For example, medical researchers might obtain\ntheir training data from hospitals in Europe, but deploy their trained models in Asia; the changes\nin conditions such as imaging equipment and demography result in a shift in the data distribution\nbetween train and test set (Dockès et al., 2021; Glocker et al., 2019; Henrich et al., 2010). To perform\nwell on such tasks requires the models to generalise to unseen distributions — an important property\nthat is not evaluated on standard machine learning datasets like ImageNet, where the train and test set\nare sampled i.i.d. from the same distribution.\nWith increasing attention on this issue, researchers have been probing the generalisation performance\nof ML models by creating datasets that feature distribution shift tasks (Koh et al., 2021; Gulrajani\nand Lopez-Paz, 2020; Shah et al., 2020) and proposing algorithms that aim to improve generalisation\nperformance under distribution shift (Ganin et al., 2016; Arjovsky et al., 2019; Sun and Saenko, 2016;\n∗Corresponding author, yshi@robots.ox.ac.uk\n1\narXiv:2206.08871v2  [cs.LG]  16 Dec 2022\nFigure 1: Synthetic vs. realistic distribution shift: The distribution shift in synthetic datasets (left,\nMNIST-CIFAR and CdSprites) are usually extreme and controllable (adjusted via changing the\ncorrelation); for realistic datasets (right, WILDS-Camelyon17 and FMoW) distribution shift can be\nsubtle, hard to identify and impossible to control.\nSagawa et al., 2020; Shi et al., 2022). In this work, we identify three speciﬁc problems with current\napproaches in distribution shift problems, in computer vision, and develop a suite of experiments to\naddress them.\n1.1\nEXISTING PROBLEMS AND CONTRIBUTIONS\nProblem 1: The outdated focus on supervised regime for distribution shift\nIn ML research,\ndistribution shift has been studied in various contexts under different terminologies such as simplicity\nbias (Shah et al., 2020), dataset bias (Torralba and Efros, 2011), shortcut learning (Geirhos et al.,\n2020), and domain adaptation and generalisation (Koh et al., 2021; Gulrajani and Lopez-Paz, 2020).\nMost of these work are carried out under the scope of supervised learning (SL), including various\nworks that either investigate spurious correlations (Shah et al., 2020; Hermann and Lampinen, 2020;\nKalimeris et al., 2019) or those that propose specialised methods to improve generalisation and/or\navoid shortcut solutions (Arjovsky et al., 2019; Ganin et al., 2016; Sagawa et al., 2020; Teney et al.,\n2022). However, recent research (Shah et al., 2020; Geirhos et al., 2020) highlighted the extreme\nvulnerability of SL methods to spurious correlations: they are susceptible to learning only features\nthat are irrelevant to the true labelling functions yet highly predictive of the labels. This behaviour is\nnot surprising given SL’s target-driven objective: when presented with two features that are equally\npredictive of the target label, SL models have no incentive to learn both as learning only one of\nthem sufﬁces to predict the target label. This leads to poor generalisation when the learned feature is\nmissing in the OOD test set.\nOn the other hand, in recent times, research in computer vision has seen a surge of unsupervised\nrepresentation learning algorithms. These include self-supervised learning (SSL) algorithms (e.g.,\nChen et al. (2020a); Grill et al. (2020); Chen and He (2021)), which learn representations by enforcing\ninvariance between the representations of two distinctly augmented views of the same image, and\nauto-encoder based algorithms (AE) (Rumelhart et al., 1985; Kingma and Welling, 2014; Higgins\net al., 2017; Burda et al., 2016), which learn representations by reconstructing the input image. The\nimmense popularity of these methods are mostly owed to their impressive performance on balanced\nin-distribution (ID) test datasets — how they perform on distribution shift tasks remains largely\nunknown. However, in distribution shift tasks, it is particularly meaningful to study unsupervised\nalgorithms. This is because, in comparison to SL, their learning objectives are more input-driven\ni.e. they are incentivised to learn representations that most accurately represent the input data (Chen\net al., 2020a; Alemi et al., 2017). When presented with two features equally predictive of the labels,\nunsupervised learning algorithms encourage the model to go beyond learning what’s enough to\npredict the label, and instead focus on maximising the mutual information between the learned\nrepresentations and the input. We hypothesise that this property of unsupervised representation\nlearning algorithms helps them avoid the exploitation of spurious correlations, and thus fare better\nunder distribution shift, compared to SL.\nContribution: Systematically evaluate SSL and AE on distribution shift tasks. We evaluate and\ncompare the generalisation performance of unsupervised representation learning algorithms, including\nSSL and AE, with standard supervised learning. See section 2 for more details on our experiments.\nProblem 2: Disconnect between synthetic and realistic datasets\nBroadly speaking, there exists\ntwo types of datasets for studying distribution shift: synthetic datasets where the shift between\n2\nOOD Accuracy (higher is better)\nAE\nSSL\nSL\n79.9\n86.1\n51.5\nAE\nSSL\nSL\n52.5\n82.7\n44\nAE\nSSL\nSL\n83.11\n80.91\n73.37\nAE\nSSL\nSL\n18.26\n26.92\n25.73\nAE\nSSL\nSL\n86.91\n89.8\n86.84\nAE\nSSL\nSL\n22.79\n29.64\n35.6\nShift Sensitivity (lower is better)\nAE\nSSL\nSL\n20\n13.7\n47.6\n(a) MNIST-CIFAR\nAE\nSSL\nSL\n47.5\n17.3\n56\n(b) CdSprites\nAE\nSSL\nSL\n4.09\n6.35\n10.72\n(c) Camelyon17-CS\nAE\nSSL\nSL\n8.77\n9.29\n12.26\n(d) FMoW-CS\nAE\nSSL\nSL\n-0.2\n1.63\n3.5\n(e) Camelyon17\nAE\nSSL\nSL\n4.99\n9.15\n37.7\n(f) FMoW\nFigure 2: Performance of auto-encoder (AE), self-supervised learning (SSL), supervised learning (SL)\nmodels. Top row: the OOD test set accuracy (%) using linear heads trained on OOD data; Bottom\nrow: Shift Sensitivity (see section 2 for deﬁnition), measures models’ sensitivity to distribution shift.\nNote here rid = 1 for Camelyon17-CS, FMoW-CS, and CdSprites (see sections 3.1.2 and 3.2.2).\ntrain/test distribution is explicit and controlled (e.g. MNIST-CIFAR (Shah et al., 2020), CdSprites\n(Shi et al., 2022)) and realistic datasets featuring implicit distribution shift in the real world (e.g.\nWILDS (Koh et al., 2021)). We provide visual examples in ﬁg. 1.\nSynthetic datasets allow for explicit control of the distribution shift and are, thus, an effective\ndiagnostic tool for generalisation performance. However, the simplistic nature of these datasets poses\nconcerns about the generality of the ﬁndings drawn from these experiments; a model’s robustness\nto spurious correlation on certain toy datasets is not very useful if it fails when tested on similar\nreal-world problems. On the other hand, realistic datasets often feature distribution shifts that are\nsubtle and hard to deﬁne (see ﬁg. 1, right). As a result, generalisation performances of different\nalgorithms tend to ﬂuctuate across datasets (Koh et al., 2021; Gulrajani and Lopez-Paz, 2020) with\nthe cause of said ﬂuctuations remaining unknown.\nContribution: Controllable but realistic distribution shift tasks. In addition to evaluating models\non both synthetic and realistic datasets, we subsample realistic domain generalisation datasets in\nWILDS to artiﬁcially inject explicit spurious correlations between domains and labels. This allows\nus to directly control the level of shift in these realistic distribution shift datasets. We refer to them as\nControllable-Shift (CS) datasets. 1\nProblem 3: The linear classiﬁer head strongly biases the evaluation protocol\nThe most popular\nevaluation protocol for representation learning algorithms (both supervised and unsupervised) is\nlinear probing. This involves freezing the weights of the representation learning model and training a\nlinear classiﬁer head on top of that to predict the labels. For distribution shift problems, this linear\nclassiﬁer is typically trained on the same training set as the representation learning backbone. Kang\net al. (2020); Menon et al. (2020) observed the interesting phenomenon that this ﬁnal layer linear\nclassiﬁer can be extremely susceptible to spurious correlations, causing poor OOD performance.\nUnder simple, synthetic set up, they showed that SL models’ performance on OOD test set can be\ndramatically improved by simply retraining the linear classiﬁer on data where the spurious correlation\nis absent. This indicates that the linear classiﬁer can be a strong source of model bias in distribution\nshift tasks, and to disentangle the linear classiﬁer bias from the generalisation performance of the\nlearned representations, it is advisable to re-train the linear head on OOD data during evaluation.\nNote that although retraining linear classiﬁer head is already standard practice in transfer learning, its\napplication is necessary as the pre-training task and target task are typically different; on the other\nhand, retraining linear head is neither necessary nor standard practice in distribution shift problems,\ndespite the recognition of linear head bias in recent work (Kang et al., 2020; Menon et al., 2020).\n1While datasets like ImageNet-9 and SVSF also provides distribution shift controls for real images, our\nsub-sampling of realistic domain generalisation datasets allows us to gain such control cheaply on a large set of\ndatasets, and in addition provide further analysis on models’ performance on existing benchmarks (WILDS).\n3\nContribution: OOD linear head. When reporting OOD accuracy, we use a linear head trained on\nsmall amount of left-out OOD data as opposed to ID data, as is standard practice. This allows us\nto isolate the bias of the linear head from the generalisability of learned representations. We also\nquantify the linear head bias to highlight the importance of this treatment. With these results, we\nwish to establish OOD linear head evaluation as a standard protocol for evaluating robustness of\nrepresentation learning algorithms to distribution shift.\nIn summary, we develop a suite of experiments and datasets to evaluate the performance of various\nrepresentation learning paradigms under distribution shift. Figure 2 provides a summary of our\nresults, comparing a range of methods from the following classes of algorithms: (i) SSL, (ii) AE, and\n(iii) SL. Note that though the intuition that unsupervised objectives should be better at distribution\nshift tasks is well-established in theory (Chen et al., 2020a; Alemi et al., 2017), state-of-the-art\nmethods are predominantly developed under SL. To the best of our knowledge, we are the ﬁrst\nto systematically evaluate and compare unsupervised representation learning methods to SL under\ndistribution shift. The models are evaluated on both synthetic and realistic distribution shift datasets.\nFurther, the models are also evaluated on CS datasets that contains controllable, explicit spurious\ncorrelations in realistic datasets. The main takeaways from this paper are:\n• SSL and AE are more robust than SL to extreme distribution shift: Figures 2a to 2d shows\nresults on distribution shift scenarios where the training set encodes extreme spurious correlations.\nIn this setting, for both synthetic (ﬁgs. 2a and 2b) and real world (ﬁgs. 2c and 2d) datasets, SSL\nand AE consistently outperforms SL in terms of OOD accuracy (top row);\n• Compared to SL, SSL and AE’s performance drop less under distribution shift: The bottom\nrow of ﬁg. 2 compares the shift sensitivity (s) of different models (see section 2 for deﬁnition).\nSmaller s is desirable as it indicates lower sensitivity to distribution shift. Results show that SSL\nand AE algorithms are signiﬁcantly more stable under distribution shift than SL;\n• Generalisation performance on distribution shift tasks can be signiﬁcantly improved by re-\ntraining the linear head: We show a large performance boost for all models, when evaluated using\nlinear head trained on a small amount of OOD data, in contrast to the baseline linear head trained\non ID data. The surprising gain of this cheap procedure, even on realistic problems, highlights\nthe importance of isolating the linear head bias when evaluating generalisation performance.\n2\nSETING UP\nIn section 1 we identiﬁed three problems in the existing literature that we wish to address in this work.\nIn this section, we will introduce the necessary experimental set-up in further details. In brief, we\ncompare eight ML algorithms on six datasets using three relevant metrics.\nAlgorithms: ×3 SSL, ×4 AE, ×1 SL. We compare seven unsupervised representation learning\nalgorithms against SL, including three SSL algorithms 1) SimCLR (Chen et al., 2020a), 2) SimSiam\n(Chen and He, 2021), and 3) BYOL (Grill et al., 2020); and four AE algorithms 1) Autoencoder\n(Rumelhart et al., 1985), 2) Variational Autoencoder (VAE) (Kingma and Welling, 2014), 3) β-VAE\n(Higgins et al., 2017) and 4) Importance Weighted Autoencoder (IWAE) (Burda et al., 2016). These\npopular methods in SSL and latent generative models have not yet been systematically evaluated\nunder distribution shift tasks prior to our work. We compare the performance of these models against\na standard supervised learning (SL) algorithm used as a representation learning model.\nDatasets: ×2 synthetic, ×2 realistic, ×2 controllable shift. We evaluate our models on two synthetic\ndatasets, namely MNIST-CIFAR (Shah et al. (2020); see section 3.1.1) and CdSprites (Shi et al. (2022);\nsee section 3.1.2), as well as two realistic datasets from WILDS (Koh et al., 2021): Camelyon17\nand FMoW (see section 3.2.1). However, as mentioned in section 1.1, both the synthetic and the\nrealistic datasets have their own drawbacks. To further understand the models’ performance and\ndraw conclusions that are generalisable, we also provide a framework for creating controllable shift\ndatasets from realistic datasets like those in WILDS, by subsampling the data to introduce spurious\ncorrelations between the domain and label information in the training set. Changing this correlation\nvaries the degree of distribution shift between the (ID) train and (OOD) test split, which allows us to\nanalyse the models’ performance more effectively under realistic, yet controllable, distribution shift.\nWe refer to this controllable shift versions of the two datasets Camelyon17-CS and FMoW-CS, and\nprovide further details on the datasets in section 3.2.2.\n4\nEvaluation: 3 metrics. Before discussing our proposed metrics, we ﬁrst deﬁne some necessary\nnotations. We separate a model trained to perform a classiﬁcation task into two parts, namely, 1)\nbackbone f, denoting the part of the model that generates representations from the data, and 2) ﬁnal\nlinear head c, which takes the representations from f and outputs the ﬁnal prediction. Further, we\nrefer to the ﬁnal linear head trained on representations from the ID train set as ci, and that trained\non representations from the OOD test set as co. Since the backbone f is always trained on the ID\ntrain set, we do not make any notation distinction on its training distribution. We also denote the\naccuracy of f and c on the ID test data as acci(f, c), and on the OOD test data as acco(f, c).\nAs noted in section 1.1, we report the OOD accuracy of the algorithms using linear heads trained\non OOD data (instead of those trained on ID data as per standard practice), i.e. acco(f, co). This is\nnecessary to disentangle the bias of the linear head from that of the representations. To highlight the\nimportance of this treatment in isolating the generalisability of the representation learning algorithm\nfrom the that of the linear head, we also deﬁne the linear head bias. It is the difference between the\nOOD test accuracy evaluated by OOD linear head and that evaluated by the ID linear head, i.e.\nb = acco(f, co) −acco(f, ci).\n(1)\nIn a related work, Taori et al. (2020) proposed to evaluate the effective robustness of OOD generalisa-\ntion deﬁned as ρ = acci(f, ci)−acco(f, ci), which quantiﬁes the drop in performance (e.g. accuracy)\nwhen evaluating the model on OOD test set vs. ID test set. A small ρ is desirable, as it indicates that\nthe performance of the model is relatively insensitive to a distribution shift2. However, we note that a\nsimple decomposition of effective robustness (ρ) shows a hidden linear head bias (b) term\nacci(f, ci) −acco(f, ci)\n|\n{z\n}\neffective robustness ρ\n= acci(f, ci) −acco(f, ci) −acco(f, co) + acco(f, co)\n= acco(f, co) −acco(f, ci)\n|\n{z\n}\nlinear head bias b\n+ acci(f, ci) −acco(f, co)\n|\n{z\n}\nshift sensitivity s\n.\n(2)\nThus, we remove the effect of the linear head bias by subtracting b from ρ and reporting the last term\nin eq. (2). We refer to this as shift sensitivity : s = ρ −b. Alternatively, it is the difference between\nthe OOD accuracy using linear head trained on OOD data, and ID accuracy using linear head trained\non ID data. Larger s marks higher sensitivity of f to distribution shift, which is, possibly, dangerous\nfor the deployment of such models. In summary, for each experiment we report the following three\nmetrics: OOD linear head accuracy acco(f, co), linear head bias b and shift sensitivity s.\n3\nEXPERIMENTAL RESULTS\nWe perform a hyperparameter search on learning rate, scheduler, optimiser, representation size, etc.\nfor each model. We use the standard SSL augmentations proposed in He et al. (2020); Chen et al.\n(2020b) for all models to ensure a fair comparison. See appendix B for details.\n3.1\nSYNTHETIC DISTRIBUTION SHIFT\nIn this section, we evaluate the performance of SL, SSL and AE algorithms on synthetic distribution\nshift tasks, utilising the MNIST-CIFAR dataset (Shah et al., 2020) and the CdSprites dataset (Shi\net al., 2022). All results are averaged over 5 random seeds.\n3.1.1\nMNIST-CIFAR\nFinding: Under this extreme distribution shift setting, SSL and AE signiﬁcantly outperform SL. The\nOOD accuracy of SSL and AE can be notably improved by retraining the linear head on OOD data,\nhowever the OOD accuracy of SL remains low even with the OOD-trained linear head.\nThe MNIST-CIFAR dataset consists of concatenations of images from two classes of MNIST and\nCIFAR-10. In each concatenated image, the classes of the two datasets are either correlated or\nuncorrelated depending on the split as discussed below (See ﬁg. 1, MNIST-CIFAR for an example):\n2We negate the original deﬁnition of effective robustness from Taori et al. (2020) for ease of understanding.\n5\nTable 1: Evaluations on the MNIST-CIFAR dataset. We report accuracy on MNIST and CIFAR\ntrained using OOD linear head (acco(f, co)), linear head bias (b) and shift sensitivity (s).\nRegime\nMethod\nMNIST (%)\nCIFAR (%)\nacco(f, co) ↑\ns ↓\nb\nacco(f, co) ↑\ns ↓\nb\nAE\n99.9 (±1e-2)\n0.0 (±1e-2)\n0.0 (±2e-3)\n81.1 (±1e+0)\n18.8 (±1e+0)\n30.2 (±1e+0)\nVAE\n99.8 (±8e-3)\n-0.1 (±9e-3)\n0.5 (±1e-4)\n79.7 (±4e+0)\n20.2 (±3e+0)\n29.2 (±6e+0)\nIWAE\n99.8 (±9e-3)\n0.0 (±4e-3)\n0.1 (±5e-3)\n80.8 (±2e+0)\n19.0 (±3e+0)\n30.0 (±4e+0)\nAE\nβ-VAE\n99.8 (±2e-2)\n0.0 (±4e-2)\n-0.1 (±3e-2)\n78.0 (±3e+0)\n21.8 (±4e+0)\n28.0 (±4e+0)\nAE average\n99.8 (±1e-2)\n0.0 (±1e-2)\n0.1 (±9e-3)\n79.9 (±3e+0)\n20.0 (±4e+0)\n29.3 (±4e+0)\nSimCLR\n99.7 (±1e-2)\n0.2 (±1e-3)\n-0.2 (±3e-3)\n85.8 (±1e+0)\n14.1 (±2e+0)\n35.5 (±1e+0)\nSimSiam\n99.8 (±2e-1)\n0.1 (±2e-1)\n0.0 (±9e-2)\n87.8 (±2e+0)\n12.1 (±2e+0)\n35.6 (±4e+0)\nSSL\nBYOL\n99.8 (±4e-2)\n0.0 (±1e-2)\n0.9 (±8e-3)\n84.8 (±9e-1)\n15.0 (±1e+0)\n33.2 (±1e+0)\nSSL average\n99.8 (±8e-2)\n0.1 (±5e-2)\n0.2 (±3e-2)\n86.1 (±2e+0)\n13.7 (±2e+0)\n34.8 (±4e+0)\nSL\nSupervised\n97.7 (±9e-1)\n1.4 (±1e+0)\n-0.3 (±1e+0)\n51.5 (±1e+0)\n47.6 (±1e+0)\n0.8 (±9e-1)\n• ID train, test: Correlation between MNIST and CIFAR-10 labels is one. Each image belongs to\none of the two classes: 1) MNIST “0” and CIFAR-10 “automobile”, and 2) MNIST “1” and\nCIFAR-10 “plane” (Figure 1, top row);\n• OOD train, test: Zero correlation between MNIST and CIFAR-10 labels, images from the two\nclasses are randomly paired (Figure 1, bottom row).\nSince the MNIST features are much simpler than the CIFAR features, a model trained on the ID train\nset can use MNIST only to predict the label, even though the CIFAR images are just as predictive\n(Shah et al., 2020). This results in poor performance when predicting the CIFAR label on the OOD\ntest set, where there is no correlation between the MNIST and CIFAR labels.\nWe train a CNN backbone on the ID train set using the eight SL, SSL and AE algorithms listed in\nsection 2. At test time, we freeze the backbone and train two linear heads on ID train and OOD train\nset respectively, and evaluate their performance on the ID and OOD test set to compute 1) OOD\nlinear head accuracy acco(f, co), 2) shift sensitivity s and, 3) linear head bias b. See results in table 1.\nWe observe that all models achieve near perfect performance when predicting the MNIST label on\nOOD test set, all with low shift sensitivity and small linear head bias. However, when predicting the\nlabels of the more complex CIFAR images, unsupervised algorithms have a clear advantage over the\nsupervised one: SSL achieves the highest OOD accuracy at 86.1%, followed by AE at 79.9% and SL\nat 51.5% (near random). The shift sensitivity s of the three objectives follow a similar trend, with\nSSL and AE scoring signiﬁcantly lower than SL. This indicates that unsupervised representations are\nsigniﬁcantly less sensitive to distribution shift compared to those from SL, with the latter suffering a\ndrop as large as 47.6%. Interestingly, the classiﬁer head bias b for SSL and AE are relatively high\n(around 30%), and is very low for SL (0.8%), indicating that the representations learned from SL\nis intrinsically un-robust to distribution shift. That is, while there exist (linearly separable) CIFAR\nfeatures in the representations of SSL and AE that can be extracted using a linear head trained on\nun-biased (OOD) data, these features are absent from the representations of SL.\n3.1.2\nCDSPRITES\nFinding: Similar to MNIST-CIFAR, under extreme distribution shift, SSL and AE are better than\nSL; when the shift is less extreme, SSL and SL achieve comparably strong OOD generalisation\nperformance while AE’s performance is much weaker.\nCdSprites is a colored variant of the popular dSprites dataset (Matthey et al., 2017), which consists\nof images of 2D sprites that are procedurally generated from multiple latent factors. The CdSprites\ndataset induces a spurious correlation between the color and shape of the sprites, by coloring the\nsprites conditioned on the shape following a controllable correlation coefﬁcient rid. See ﬁg. 1 for an\nexample: when rid = 1 color is completely dependent on shape (top row, oval-purple, heart-cyan,\nsquare-white), and when rid = 0, color and shape are randomly matched (bottom row).\nShi et al. (2022) observes that when rid is high, SL model tend to use color only to predict the label\nwhile ignoring shape features due to the texture bias of CNN (Geirhos et al., 2019; Brendel and\nBethge, 2019). First, we consider the setting of extreme distribution shift similar to MNIST-CIFAR\nby setting rid = 1 in the ID train and test splits. In the OOD train and test splits, the correlation\n6\nTable 2: Evaluations on the CdSprites dataset with rid = 1.0. We report accuracy for color and shape\nclassiﬁers trained using OOD linear head (acco(f, co)), linear head bias (b) and shift sensitivity (s).\nRegime\nMethod\nColor classiﬁcation (%)\nShape classiﬁcation (%)\nacco(f, co) ↑\ns ↓\nb\nacco(f, co) ↑\ns ↓\nb\nAE\n100.0 (±0e+0)\n0.0 (±2e-3)\n0.3 (±5e-1)\n46.1 (±6e-1)\n53.9 (±6e-1)\n12.7 (±5e-1)\nVAE\n99.7 (±3e-1)\n0.3 (±3e-1)\n-0.3 (±3e-1)\n52.4 (±2e+0)\n47.6 (±2e+0)\n18.9 (±3e+0)\nAE\nIWAE\n100.0 (±0e+0)\n0.0 (±2e-3)\n0.4 (±5e-1)\n58.9 (±2e+0)\n41.1 (±2e+0)\n25.6 (±2e+0)\nAE average\n99.9 (±9e-1)\n0.1 (±9e-2)\n0.1 (±4e-1)\n52.5 (±2e+0)\n47.5 (±2e+0)\n19.1 (±2e+0)\nSimCLR\n100.0 (±0e+0)\n0.0 (±0e+0)\n0.0 (±1e-1)\n87.8 (±5e-1)\n12.2 (±5e-1)\n54.5 (±5e-1)\nSimSiam\n100.0 (±0e+0)\n0.0 (±0e+0)\n0.1 (±1e-1)\n69.2 (±2e+0)\n30.8 (±2e+0)\n35.6 (±2e+0)\nSSL\nBYOL\n100.0 (±0e+0)\n0.0 (±0e+0)\n0.0 (±0e+0)\n91.1 (±4e+0)\n8.9 (±4e+0)\n57.9 (±4e+0)\nSSL average\n100.0 (±0e+0)\n0.0 (±0e+0)\n0.1 (±3e-2)\n82.7 (±2e+0)\n17.3 (±2e+0)\n49.3 (±4e+0)\nSL\nSupervised\n100.0 (±0e+0)\n0.0 (±0e+0)\n0.0 (±3e-2)\n44.0 (±7e-1)\n56.0 (±7e-1)\n10.7 (±7e-1)\ncoefﬁcient is set to zero to investigate how well the model learns both the shape and the color\nfeatures. Table 2 reports the three metrics of interest using the same evaluation protocol as before.\nSimilar to MNIST-CIFAR, we observe that all models achieve near perfect performance when pre-\ndicting the simpler feature, i.e. color on the OOD test set. However, when predicting shape, the more\ncomplex feature on the OOD test set, SSL (and also AEs to a lesser extent) is far superior to SL. Ad-\nditionally, the shift sensitivity of SSL (and AE to a lesser extent) are much smaller than SL, indicating\nthat SSL/AE models are more robust to extreme distribution shift. The linear head bias also follows\na similar trend as for MNIST-CIFAR, showing that representations learned using SL methods are\ninherently not robust to spurious correlations. This is not the case for SSL and AE algorithms where a\nlarge linear head bias shows that is the ID linear heads and not the representations that injects the bias.\nControllable distribution shift\nWe extend this experiment to probe the performance of these\nalgorithms under varying degrees of distribution shifts. We generate three versions of the CdSprites\ndataset with three different correlation coefﬁcients rid ∈{0, 0.5, 1} of the ID train set. As before,\nthe correlation coefﬁcient of the OOD split is set to zeroThe rest of the experimental protocol stays\nthe same. The OOD test accuracy and the shift sensitivity for varying rid is plotted in ﬁg. 3 and a\ndetailed breakdown of results is available in appendix A.\nFigure 3 shows that despite increasing distribution shift between the ID and OOD splits (with\nincreasing rid) the OOD performance of SSL and AE does not suffer. However, the OOD accuracy of\nSL plummets and its shift sensitivity explodes at rid = 1. Interestingly, SSL maintains a high OOD test\naccuracy regardless of the level of distribution shift: when rid < 1 its performance is on par with SL,\nand when the distribution shift becomes extreme with rid = 1 it signiﬁcantly outperforms SL both in\nterms of accuracy and shift sensitivity. In comparison, AE models’ accuracy lingers around 50%, with\nincreasingly higher shift sensitivity as rid increases. However, under extreme distribution shift with\nrid = 1 it still performs better than SL, with slightly higher OOD accuracy and lower shift sensitivity.\n3.2\nREAL-WORLD DISTRIBUTION SHIFT\nIn this section we investigate the performance of different objectives on real-world distribution shift\ntasks. We use two datasets from WILDS (Koh et al., 2021): 1) Camelyon17, which contains tissue\nscans acquired from different hospitals, and the task is to determine if a given patch contains breast\ncancer tissue; and 2) FMoW, which features satellite images of landscapes on ﬁve different continents,\nwith the classiﬁcation target as the type of infrastructure. See examples in Figure 1. Following the\nguidelines from WILDS benchmark, we perform 10 random seed runs for all Camelyon17 experiment\nand 3 random seed runs for FMoW. The error margin in Figure 3 represent standard deviation.\n3.2.1\nORIGINAL WILDS DATASETS\nFindings: SL is signiﬁcantly more sensitive to distribution shift than SSL and AE; representations from\nSSL obtain higher OOD accuracy than SL on Camelyon17 but lower on FMoW. AE is consistently the\nleast sensitive to distribution shift though it has the lowest accuracy. The performance of all models\nsigniﬁcantly improves by retraining the linear head on a small amount of OOD data.\nThe original Camelyon17 and FMoW dataset from WILDS benchmark both contains the following\nthree splits: ID train, OOD validation and OOD test. We further create ﬁve splits speciﬁed as follows:\n• ID train, test: Contains 90% and 10% of the original ID train split, respectively;\n7\nTable 3: Evaluations on test set of Camelyon17,\nall metrics computed using average accuracy.\nRegime\nMethod\nMetrics (%)\nacco(f, co) ↑\ns ↓\nb\nAE\n84.4 (±2e+0)\n-0.6 (±1e+0)\n12.7 (±2e+0)\nVAE\n88.1 (±2e+0)\n0.5 (±2e+0)\n39.0 (±2e+0)\nIWAE\n88.1 (±1e+0)\n-0.9 (±3e+0)\n39.1 (±4e+0)\nAE\nβ-VAE\n87.1 (±4e+0)\n0.2 (±4e+0)\n36.0 (±5e+0)\nAE average\n86.9 (±2e+0)\n-0.2 (±3e+0)\n31.7 (±3e+0)\nSimCLR\n92.7 (±2e+0)\n0.4 (±1e+0)\n8.3 (±1e+0)\nSimSiam\n86.7 (±1e+0)\n3.1 (±1e+0)\n7.9 (±3e+0)\nSSL\nBYOL\n89.9 (±1e+0)\n1.4 (±1e+0)\n10.3 (±2e+0)\nSSL average\n89.8 (±1e+0)\n1.6 (±1e+0)\n8.8 (±2e+0)\nSL\nSupervised\n86.8 (±2e+0)\n3.5 (±1e+0)\n7.4 (±3e+0)\nTable 4: Evaluations on test set of FMoW, all\nmetrics computed using worst-group accuracy.\nRegime\nMethod\nMetrics (%)\nacco(f, co) ↑\ns ↓\nb\nAE\n26.9 (±9e-3)\n6.4 (±6e-3)\n5.8 (±1e-2)\nVAE\n21.7 (±6e-3)\n4.7 (±4e-3)\n8.0 (±2e-2)\nIWAE\n20.9 (±2e-2)\n5.5 (±1e-2)\n7.8 (±1e-2)\nAE\nβ-VAE\n21.7 (±5e-3)\n3.4 (±6e-3)\n7.6 (±8e-3)\nAE average\n22.8 (±3e-2)\n5.0 (±7e-3)\n7.3 (±1e-2)\nSimCLR\n29.9 (±6e-3)\n10.7 (±6e-3)\n7.6 (±7e-3)\nSimSiam\n27.8 (±2e-2)\n4.6 (±1e-2)\n6.3 (±2e-2)\nSSL\nBYOL\n31.3 (±1e-2)\n12.1 (±7e-3)\n7.9 (±1e-2)\nSSL average\n29.6 (±2e-2)\n9.1 (±9e-3)\n7.3 (±1e-2)\nSL\nSupervised\n35.6 (±7e-3)\n37.7 (±4e-2)\n6.9 (±9e-3)\n• OOD train, test: Contains 10% and 90% of the original OOD test split, respectively;\n• OOD validation: Same as the original OOD validation split.\nFollowing WILDS, we use OOD validation set to perform early stopping and choose hyperparameters;\nwe also use DenseNet-121 (Huang et al., 2017) as the backbone for all models. We follow similar\nevaluation protocol as previous experiments, and in addition adopt 10-fold cross-validation for the\nOOD train and test set. See results in Tables 3 and 4, where following WILDS, we report performance\non Camelyon17 using standard average accuracy and on FMoW using worst-group accuracy.\nOne immediate observation is that in contrast to our previous experiments on synthetic datasets,\nSL’s OOD accuracy is much higher in comparison on realistic distribution shift tasks: it is the best\nperforming model on FMoW with 35.6% worst-group accuracy on OOD test set; its OOD accuracy\nis the lowest on Camelyon17, however it is only 3% worse than the highest accuracy achieved by SSL\n(89.8%). This highlights the need to study realistic datasets along with synthetic ones. Nonetheless,\nwe ﬁnd that SSL is still the best performing method on Camelyon17 and achieves competitive\nperformance on FMoW with accuracy 29.6% — despite learning without labels! AE has much\nlower OOD accuracy on FMoW compared to the other two methods: we believe this is due to its\nreconstruction-based objective wasting modelling capacity on high frequency details, a phenomenon\nfrequently observed in prior work (Bao et al., 2021; Ramesh et al., 2021). Note that the standard\ndeviation for all three methods are quite high for Camelyon17: this is a known property of the dataset\nand similar pattern is observed across most methods on WILDS benchmark (Koh et al., 2021).\nIn terms of shift sensitivity, unsupervised objectives including SSL and AE consistently outperforms\nSL — this stands out the most on FMoW, where the shift sensitivity of SSL and AE are 9.1% and\n5.0% respectively, while SL is as high as 37.7%. This observation further validates our previous\nﬁnding on synthetic datasets, that SSL and AE’s ID accuracy is a relatively reliable indication of\ntheir generalisation performance, while SL can undergo a huge performance drop under distribution\nshift, which can be dangerous for the deployment of such models. We highlight that, in sensitive\napplication domains, a low shift sensitivity is an important criterion as it implies that the model’s\nperformance will remain consistent when the distribution shifts. Another interesting observation here\nis that for all objectives on both datasets, the classiﬁer bias b is consistently high. This indicates the\nbias of the linear classiﬁcation head plays a signiﬁcant role even for real world distribution shifts, and\nthat it is possible to mitigate this effect by training the linear head using a small amount of OOD data\n(in this case 10% of the original OOD test set).\n3.2.2\nWILDS DATASETS WITH CONTROLLABLE SHIFT\nFindings: SL’s OOD accuracy drops as more the distribution shift becomes more challenging, with\nSSL being the best performing model when the distribution shift is the most extreme. The shift\nsensitivity of SSL and AE are consistently lower than SL regardless of the level of shift.\nTo examine models’ generalisation performance under different levels of distribution shift, we create\nversions of these realistic datasets with controllable shifts, which we name Camelyon17-CS and\nFMoW-CS. Speciﬁcally, we subsample the ID train set of these datasets to artiﬁcially create spurious\ncorrelation between the domain and label. For instance, given dataset with domain A, B and label\n0, 1, to create a version of the dataset where the spurious correlation is 1 we would sample only\nexamples with label 0 from domain A and label 1 from domain B. See Appendix C for further details.\n8\nTable 5: Evaluations on test set of Camelyon17-\nC with rid = 0.5, all metrics computed using\naverage accuracy.\nRegime\nMethod\nMetrics (%)\nacco(f, co) ↑\ns ↓\nb\nAE\n80.4 (±3e+0)\n6.0 (±2e+0)\n19.0 (±4e+0)\nVAE\n88.6 (±2e+0)\n-0.5 (±1e+0)\n17.8 (±6e+0)\nIWAE\n87.8 (±1e+0)\n-0.2 (±1e+0)\n26.4 (±6e+0)\nAE\nβ-VAE\n88.5 (±2e+0)\n0.1 (±9e-1)\n19.7 (±6e+0)\nAE average\n86.3 (±2e+0)\n1.4 (±1e+0)\n20.7 (±5e+0)\nSimCLR\n84.5 (±2e+0)\n8.0 (±1e+0)\n6.6 (±2e+0)\nSimSiam\n86.1 (±2e+0)\n5.7 (±1e+0)\n8.3 (±4e+0)\nSSL\nBYOL\n86.4 (±2e+0)\n4.5 (±2e+0)\n8.8 (±4e+0)\nSSL average\n85.7 (±2e+0)\n6.1 (±2e+0)\n7.9 (±3e+0)\nSL\nSupervised\n81.5 (±5e+0)\n13.2 (±3e+0)\n3.4 (±4e+0)\nTable 6: Evaluations on test set of FMoW-C with\nrid = 0.5, all metrics computed using worst-\ngroup accuracy.\nRegime\nMethod\nMetrics (%)\nacco(f, co) ↑\ns ↓\nb\nAE\n23.4 (±1e+0)\n8.6 (±6e-1)\n4.2 (±8e-1)\nVAE\n18.7 (±1e+0)\n7.7 (±6e-1)\n1.5 (±6e-1)\nIWAE\n18.5 (±2e+0)\n7.3 (±2e+0)\n2.2 (±1e+0)\nAE\nβ-VAE\n21.4 (±3e-1)\n4.0 (±4e-1)\n3.9 (±7e-1)\nAE average\n20.5 (±2e+0)\n6.9 (±8e-1)\n3.0 (±9e-1)\nSimCLR\n29.5 (±9e-1)\n9.2 (±6e-1)\n6.9 (±7e-1)\nSimSiam\n27.9 (±2e+0)\n7.5 (±1e+0)\n4.6 (±1e+0)\nSSL\nBYOL\n32.6 (±3e+0)\n7.9 (±2e+0)\n8.5 (±2e+0)\nSSL average\n30.0 (±2e+0)\n8.2 (±1e+0)\n6.7 (±1e+0)\nSL\nSupervised\n32.3 (±3e+0)\n25.1 (±3e+0)\n6.0 (±2e+0)\nTable 7: Evaluations on test set of Camelyon17-\nC with rid = 1, all metrics computed using aver-\nage accuracy.\nRegime\nMethod\nMetrics (%)\nacco(f, co) ↑\ns ↓\nb\nAE\n75.7 (±5e+0)\n7.3 (±2e+0)\n35.1 (±4e+0)\nVAE\n86.0 (±3e+0)\n2.7 (±1e+0)\n12.4 (±4e+0)\nIWAE\n86.1 (±1e+0)\n2.6 (±7e-1)\n9.1 (±3e+0)\nAE\nβ-VAE\n84.7 (±2e+0)\n3.8 (±1e+0)\n15.5 (±4e+0)\nAE average\n83.1 (±3e+0)\n4.1 (±1e+0)\n18.0 (±4e+0)\nSimCLR\n85.8 (±8e+1)\n2.8 (±4e-1)\n6.2 (±2e+0)\nSimSiam\n82.1 (±1e+0)\n6.0 (±7e-1)\n8.3 (±4e+0)\nSSL\nBYOL\n74.8 (±5e+0)\n10.3 (±2e+0)\n-2.2 (±4e+0)\nSSL average\n80.9 (±2e+0)\n6.3 (±1e+0)\n4.1 (±3e+0)\nSL\nSupervised\n73.4 (±6e+0)\n10.7 (±3e+0)\n5.9 (±8e+0)\nTable 8: Evaluations on test set of FMoW-C with\nrid = 1, all metrics computed using worst-group\naccuracy.\nRegime\nMethod\nMetrics (%)\nacco(f, co) ↑\ns ↓\nb\nAE\n22.4 (±1e+0)\n10.0 (±6e-1)\n3.8 (±7e-1)\nVAE\n16.6 (±9e-1)\n8.6 (±1e+0)\n2.8 (±8e-1)\nIWAE\n17.2 (±5e-1)\n8.7 (±6e-1)\n3.8 (±5e-1)\nAE\nβ-VAE\n16.7 (±3e-1)\n7.9 (±4e-1)\n3.2 (±4e-1)\nAE average\n18.3 (±3e+0)\n8.8 (±7e-1)\n3.4 (±6e-1)\nSimCLR\n26.3 (±1e+0)\n10.6 (±8e-1)\n7.2 (±1e+0)\nSimSiam\n27.1 (±5e-1)\n6.3 (±7e-1)\n7.8 (±3e-1)\nSSL\nBYOL\n27.4 (±2e+0)\n11.1 (±1e+0)\n6.5 (±2e+0)\nSSL average\n26.9 (±6e-1)\n9.3 (±1e+0)\n7.2 (±1e+0)\nSL\nSupervised\n25.7 (±5e-1)\n12.3 (±2e+0)\n5.3 (±1e+0)\nSimilar to CdSprites, we create three versions of both of these datasets with the spurious correlation\ncoefﬁcient rid ∈{0, 0.5, 1} in ID (train and test) sets. The OOD train, test and validation set remains\nunchanged3. Using identical experimental setup as in section 3.2.1, we plot the trend of performance\nwith increasing r for Camelyon17-CS and FMoW-CS in Figure 3 with detailed numerical breakdown\nin Tables 5 to 8.\nFor both datasets, the OOD test accuracy of all models drop as the spurious correlation rid increases\n(top row in Figure 3). However, this drop is the far more obvious in SL than in SSL and AE: when\nrid = 1, SL’s accuracy is 10% lower than SSL on Camelyon17 and 2% lower on FMoW — a\nsigniﬁcant drop from its original 3% lag on Camelyon17 and 5% lead on FMoW. This demonstrates\nthat SL is less capable of dealing with more challenging distribution shift settings compared to SSL\nand AE. In terms of shift sensitivity (bottom row, ﬁg. 3), SL’s remains the highest regardless of rid;\ncuriously, we see a decrease in SL’s shift sensitivity as rid increases in FMoW-CS, however this has\nmore to do with the ID test set accuracy decreasing due to the subsampling of the dataset.\n4\nRELATED WORK\nWhile we are the ﬁrst to systematically evaluate the OOD generalisation performance of unsupervised\nlearning algorithms, there are other insightful work that considers the robustness to distribution shift of\nother existing, non-specialised methods/techniques. For instance, Liu et al. (2022) studies the impact\nof different pre-training set-ups to distribution shift robustness, including dataset, objective and data\naugmentation. Ghosal et al. (2022) focuses on architecture, and found that Vision Transformers are\nmore robust to spurious correlations than ConvNets when using larger models and are given more train-\ning data; further, Liu et al. (2021) found that SSL is more robust to data imbalance. Azizi et al. (2022)\nalso performed extensive studies on the generalisation performance of SSL algorithms on medical\ndata. Interestingly, Robinson et al. (2021) also investigates the robustness of contrastive-SSL methods\nagainst extreme spurious correlation (i.e.simplicity bias). However, their work did not consider the\n3Note that even when rid = 0, distribution shift between the ID and OOD splits exists, as the spurious\ncorrelation is not the only source of the distribution shift.\n9\n0.0\n0.5\n1.0\nrid\n70\n80\n90\nTest accuracy\nSL\nSSL\nAE\n0.0\n0.5\n1.0\nrid\n20\n30\n40\nTest accuracy\nSL\nSSL\nAE\n0.0\n0.5\n1.0\nrid\n0\n20\n40\n60\n80\n100\nTest accuracy (\n)\nSL\nSSL\nAE\n0.0\n0.5\n1.0\nrid\n0\n10\n20\nShift Sensitivity s\nSL\nSSL\nAE\n(a) Camelyon17-CS\n0.0\n0.5\n1.0\nrid\n0\n20\n40\nShift Sensitivity s\nSL\nSSL\nAE\n(b) FMoW-CS\n0.0\n0.5\n1.0\nrid\n0\n20\n40\n60\n80\n100\nShift Sensitivity s (\n)\nSL\nSSL\nAE\n(c) CdSprites\nFigure 3: Evaluations on Camelyon17-CS, FMoW-CS, and CdSprites with rid ∈{0, 0.5, 1.0}. We\nreport OOD test accuracy using OOD-trained linear head (acco(f, co)) and shift sensitivity (s). Blue\nlines are results averaged over AE models, green lines are SSL models and grey is SL.\nlinear head bias found in (Kirichenko et al., 2022; Kang et al., 2020) and led to opposing conclusions.\nIn contrast, our work investigates the distribution shift performance of unsupervised algorithms, with\nexperiments on both synthetic and realistic settings that go beyond the data imbalance regime. By iso-\nlating the linear head bias in our experiments, we ﬁnd that unsupervised, especially SSL-learned repre-\nsentations, achieves similar if not better generalisation performance than SL under a wide range of dis-\ntribution shift settings. See Appendix D for a more detailed discussion on distribution shift problems.\n5\nCONCLUSION AND FUTURE WORK\nIn this paper, we investigate the robustness of both unsupervised (AE, SSL) and supervised (SL)\nobjectives for distribution shift. Through extensive and principled experiments on both synthetic\nand realistic distribution shift tasks, we ﬁnd unsupervised representation learning algorithms to\nconsistently outperform SL when the distribution shift is extreme. In addition, we see that SSL’s\nOOD accuracy is comparable, if not better to SL in all experiments. This is particularly crucial, as\nmost work studying distribution shift for images are developed in the SL regime. We hope that these\nresults inspire more future work on unsupervised/semi-supervised representation learning methods for\nOOD generalisation. Another important ﬁnding is that unsupervised models’ performance remains\nrelatively stable under distribution shift. This is especially crucial for the real-world application of\nthese machine learning systems, as this indicates that the ID performance of SSL/AE algorithms are a\nmore reliable indicator of how they would perform in different environments at deployment, while that\nof SL is not. It is also worth noting that while models trained with AE objectives are consistently the\nleast sensitive to distribution shift on realistic datasets, their OOD performance can be low especially\nwhen presented with complex data (such as FMoW). This is consistent to the observation in prior\nwork that these models can waste modelling capacity on high frequency details, and suggests that\none should be careful about employing AE algorithms on large scale, complex tasks. Finally, a key\ncontribution of this work is establishing the existence of linear head bias even for realistic distribution\nshift problems. We believe that using an OOD-trained linear head is necessary to be able to make\ncomparisons between various algorithms irrespective of the ﬁnal downstream task, and on the other\nhand, more efforts in the ﬁeld of distribution shift could be devoted into re-balancing the linear layer.\nACKNOWLEDGEMENTS\nYS and PHST were supported by the UKRI grant: Turing AI Fellowship EP/W002981/1 and\nEPSRC/MURI grant: EP/N019474/1. We would also like to thank the Royal Academy of Engineering\nand FiveAI. YS was additionally supported by Remarkdip through their PhD Scholarship Programme.\nID was supported by the SNSF grant #200021_188466. AS was partially supported by the ETH\nAI Center postdoctoral fellowship. Special thanks to Alain Ryser for suggesting the design of\ncontrollable versions of the WILDS dataset, and to Josh Dillon for helpful suggestions in the early\nstage of this project.\n10\nREFERENCES\nAlexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information\nbottleneck. In International Conference on Learning Representations, 2017. 2, 4\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\narXiv:1907.02893, 2019. 1, 2, 20\nShekoofeh Azizi, Laura Culp, Jan Freyberg, Basil Mustafa, Sebastien Baur, Simon Kornblith, Ting\nChen, Patricia MacWilliams, S Sara Mahdavi, Ellery Wulczyn, et al. Robust and efﬁcient medical\nimaging with self-supervision. arXiv:2205.09723, 2022. 9\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. ArXiv preprint,\nabs/2106.08254, 2021. 8\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman\nVaughan. A theory of learning from different domains. Machine learning, 2010. 20\nWieland Brendel and Matthias Bethge. Approximating cnns with bag-of-local-features models works\nsurprisingly well on imagenet. In International Conference on Learning Representations, 2019. 6\nYuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In\nInternational Conference on Learning Representations, 2016. 2, 4\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for\ncontrastive learning of visual representations. In International Conference on Machine Learning,\n2020a. 2, 4\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE Conference\non Computer Vision and Pattern Recognition, 2021. 2, 4\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020b. 5, 16\nVictor Guilherme Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A\nlibrary of self-supervised methods for visual representation learning. Journal of Machine Learning\nResearch, 2022. 15\nJérôme Dockès, Gaël Varoquaux, and Jean-Baptiste Poline. Preventing dataset shift from breaking\nmachine-learning biomarkers. GigaScience, 2021. 1\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François\nLaviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.\nThe journal of machine learning research, 2016. 1, 2, 20\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and\nWieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves\naccuracy and robustness. In International Conference on Learning Representations, 2019. 6\nRobert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias\nBethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine\nIntelligence, 2020. 2, 20\nSoumya Suvra Ghosal, Yifei Ming, and Yixuan Li. Are vision transformers robust to spurious\ncorrelations? arXiv:2203.09125, 2022. 9\nBen Glocker, Robert Robinson, Daniel C Castro, Qi Dou, and Ender Konukoglu. Machine learning\nwith multi-site imaging data: an empirical study on the impact of scanner effects. arXiv:1910.04597,\n2019. 1\nA. Gretton, AJ. Smola, J. Huang, M. Schmittfull, KM. Borgwardt, and B. Schölkopf. Covariate shift\nand local learning by distribution matching. 2009a. 20\nArthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernhard\nSchölkopf. Covariate shift by kernel mean matching. Dataset shift in machine learning, 2009b. 20\n11\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\nBilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent -\nA new approach to self-supervised learning. In Conference on Neural Information Processing\nSystems, 2020. 2, 4\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv:2007.01434,\n2020. 1, 2, 3\nSivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei\nHerzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain generalization\nby learning a bridge across domains. In IEEE Conference on Computer Vision and Pattern\nRecognition, 2022. 20\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for\nunsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern\nRecognition, 2020. 5, 16\nJoseph Henrich, Steven J Heine, and Ara Norenzayan. Most people are not weird. Nature, 2010. 1\nKatherine Hermann and Andrew Lampinen. What shapes feature representations? exploring datasets,\narchitectures, and training. Conference on Neural Information Processing Systems, 2020. 2\nIrina Higgins, Loïc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a\nconstrained variational framework. In International Conference on Learning Representations,\n2017. 2, 4\nWeihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised\nlearning give robust classiﬁers. In International Conference on Machine Learning, 2018. 20\nGao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected\nconvolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.\n8, 18\nDimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak,\nand Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Conference\non Neural Information Processing Systems, 2019. 2\nBingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis\nKalantidis. Decoupling representation and classiﬁer for long-tailed recognition. In International\nConference on Learning Representations, 2020. 3, 10, 20\nByungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning not to learn:\nTraining deep neural networks with biased data. In IEEE Conference on Computer Vision and\nPattern Recognition, 2019. 20\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference\non Learning Representations, 2014. 2, 4\nPolina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufﬁcient\nfor robustness to spurious correlations. arXiv:2204.02937, 2022. 10, 20\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsub-\nramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne\nDavid, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M. Beery, Jure Leskovec,\nAnshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A\nbenchmark of in-the-wild distribution shifts. In International Conference on Machine Learning,\n2021. 1, 2, 3, 4, 7, 8, 17, 18\nMasanori Koyama and Shoichiro Yamaguchi. Out-of-distribution generalization with maximal\ninvariant predictor. In arXiv e-prints, 2021. 20\n12\nSebastian Lapuschkin, Stephan Wäldchen, Alexander Binder, Grégoire Montavon, Wojciech Samek,\nand Klaus-Robert Müller. Unmasking clever hans predictors and assessing what machines really\nlearn. Nature communications, 2019. 20\nRonan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, Ashish\nSabharwal, and Yejin Choi. Adversarial ﬁlters of dataset biases. In International Conference on\nMachine Learning, 2020. 20\nHong Liu, Jeff Z HaoChen, Adrien Gaidon, and Tengyu Ma. Self-supervised learning is more robust\nto dataset imbalance. arXiv:2110.05025, 2021. 9\nZiquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin, Xiangyang Ji, and Antoni B Chan.\nAn empirical study on distribution shift robustness from the perspective of pre-training and data\naugmentation. arXiv:2205.12753, 2022. 9\nDavid Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In\nConference on Neural Information Processing Systems, 2017. 20\nXu Luo, Longhui Wei, Liangjian Wen, Jinrong Yang, Lingxi Xie, Zenglin Xu, and Qi Tian. Rectifying\nthe shortcut learning of background for few-shot learning. Conference on Neural Information\nProcessing Systems, 2021. 20\nLoic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement\ntesting sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017. 6\nAditya Krishna Menon, Ankit Singh Rawat, and Sanjiv Kumar. Overparameterisation and worst-case\ngeneralisation: friend or foe? In International Conference on Learning Representations, 2020. 3\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\nLearning, 2021. 8\nJoshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can\ncontrastive learning avoid shortcut solutions?\nConference on Neural Information Processing\nSystems, 2021. 9, 20\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by\nerror propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science,\n1985. 2, 4\nShiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust\nneural networks. In International Conference on Learning Representations, 2020. 2, 20\nHarshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The\npitfalls of simplicity bias in neural networks. In Conference on Neural Information Processing\nSystems, 2020. 1, 2, 3, 4, 5, 6, 16, 20\nYuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel\nSynnaeve. Gradient matching for domain generalization. In International Conference on Learning\nRepresentations, 2022. 2, 3, 4, 5, 6, 20\nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In\nEuropean conference on computer vision, pages 443–450. Springer, 2016. 1\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt.\nMeasuring robustness to natural distribution shifts in image classiﬁcation. Advances in Neural\nInformation Processing Systems, 33:18583–18599, 2020. 5\nDamien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton van den Hengel. Evading the simplicity\nbias: Training a diverse set of models discovers solutions with superior ood generalization. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n16761–16772, 2022. 2, 20\n13\nAntonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521–1528.\nIEEE, 2011. 2, 20\nHaohan Wang, Songwei Ge, Zachary C. Lipton, and Eric P. Xing. Learning robust global representa-\ntions by penalizing local predictive power. In Advances in Neural Information Processing Systems,\nvolume 32, pages 10506–10518, 2019. 20\nXingxuan Zhang, Linjun Zhou, Renzhe Xu, Peng Cui, Zheyan Shen, and Haoxin Liu. Towards\nunsupervised domain generalization. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4910–4920, 2022. 20\n14\n0.00\n0.25\n0.50\n0.75\n1.00\nrid\n0\n20\n40\n60\n80\n100\nTest accuracy (\n)\nAE\nVAE\nIWAE\nSimCLR\nBYOL\nSimSiam\nSL\nrandom\n(a) acco(f, co) (%).\n0.00\n0.25\n0.50\n0.75\n1.00\nrid\n0\n20\n40\n60\n80\n100\nShift Sensitivity s (\n)\nAE\nVAE\nIWAE\nSimCLR\nBYOL\nSimSiam\nSL\n(b) Shift sensitivity s (%).\n0.00\n0.25\n0.50\n0.75\n1.00\nrid\n0\n20\n40\n60\n80\n100\nLinear head bias b\nAE\nVAE\nIWAE\nSimCLR\nBYOL\nSimSiam\nSL\n(c) Linear head bias b (%).\nFigure 4: Evaluations on the CdSprites dataset with rid ∈{0.25, 0.5, 0.75, 1.0}. We report shape\nclassiﬁcation accuracy using OOD-trained linear head (acco(f, co)), shift sensitivity s, and linear\nhead bias b. Results are shown for individual models from the class of AE (blue), SSL (green), and SL\n(grey) algorithms. The black horizontal line denotes the random baseline (33.3% for three classes).\nA\nADDITIONAL EXPERIMENTAL RESULTS\nA.1\nCDSPRITES\nIn addition to our results in table 2, where we use a dataset with perfectly correlated features (rid = 1)\nto train the backbones, in ﬁg. 4 we vary rid to analyse the effect of imperfectly correlated features.\nNotably, with imperfect correlation (rid < 1), the OOD linear heads trained on top of the SL and SSL\nbackbones perform perfectly. For the AE, we observe that the performance of the OOD linear head\ndoes not depend on the correlation rid in the data used to train the backbones. Our results suggest\nthat with imperfect correlation between features, SL and SSL models learn a linearly separable\nrepresentation of the features, whereas AE does not.\nIn ﬁg. 5 we provide an ablation where we also vary rood, the correlation in the data used to train and\nevaluate the linear head. Figures 5b and 5c corroborate our results that SSL performs on par with SL\nfor rid < 1 and strictly better when rid = 1. For the AE (Figure 5a), we observe an interesting pattern\nwhere the performance of the OOD linear head depends on the OOD correlation rood, but not on the\ncorrelation rid in the data used to train the backbones. Hence, the ablation corroborates our result that\nSL and SSL models learn a linearly separable representation of the shape and color features when\nthere is an imperfect correlation between the features, whereas AE does not.\n0.0\n0.25\n0.5\n0.75\n1.0\nrid\n1.0\n0.75\n0.5\n0.25\n0.0\nrood\n100\n100\n100\n100\n100\n83\n83.6\n83.3\n83.7\n83.8\n68.1\n68.2\n67.7\n68.7\n68.4\n57.3\n56.8\n55.9\n57.3\n56.4\n53.2\n52.3\n51.5\n52.9\n51.1\n40\n50\n60\n70\n80\n90\n100\n(a) AE\n0.0\n0.25\n0.5\n0.75\n1.0\nrid\n1.0\n0.75\n0.5\n0.25\n0.0\nrood\n100\n100\n100\n100\n100\n100\n100\n100\n100\n95\n100\n100\n100\n99.9\n91.6\n100\n100\n100\n99.9\n89.1\n100\n100\n100\n99.9\n86.4\n40\n50\n60\n70\n80\n90\n100\n(b) SSL\n0.0\n0.25\n0.5\n0.75\n1.0\nrid\n1.0\n0.75\n0.5\n0.25\n0.0\nrood\n100\n100\n100\n100\n100\n100\n100\n100\n100\n79.4\n100\n100\n100\n100\n65.5\n100\n100\n100\n100\n51.7\n100\n100\n100\n100\n44.4\n40\n50\n60\n70\n80\n90\n100\n(c) SL\nFigure 5: Correlation coefﬁcient ablation for CdSprites. Shape classiﬁcation accuracy for the\nCdSprites experiment with varying correlation of the ID training data (rid, x-axis) and OOD training\nand test data (rood, y-axis). Backbones were trained on data with correlation rid and linear classiﬁers\ntrained and evaluated on top of the frozen backbones with correlation rood.\nB\nARCHITECTURE AND HYPERPARAMETERS\nIn this appendix we list the architecture and hyperparameters used in our experiments. Our code\nis developed on the amazing solo-learn code base (da Costa et al., 2022), which is originally\n15\nTable 9: Hyperparameter search range for MNIST-CIFAR, including base channel size of CNN (C),\nlearning rate (lr.), weight decay (wd.), optimiser (optim.), learning rate scheduler (lr. scheduler).\nC\nlr.\nwd.\nOptim.\nlr. scheduler\nAE\n{16, 32, 64, 128}\n{1e-4, 5e-4, 1e-3, 5e-3, 1e-2}\n{0, 1e-4}\n{Adam, SGD}\n{warmup cosine, step, none}\nSSL\n{16, 32, 64, 128}\nuniformly sampled from [0.1, 1]\n{0, 1e-4}\n{Adam, SGD}\n{warmup cosine, step, none}\nSL\n{16, 32, 64, 128}\n{1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 1e-1, 5e-1}\n{0, 1e-4}\n{Adam, SGD}\n{warmup cosine, step, none}\ndeveloped as a library for SSL algorithms. For all experiments we follow the standard set of\naugmentations established in He et al. (2020); Chen et al. (2020b), including random resize crop,\nrandom color jittering, random grayscale, random Gaussian blur, random solorisation and random\nhorizontal ﬂip. An exception is the CdSprites experiment where we remove color jittering, as color\nclassiﬁcation is one of the tasks we are interested in and color jittering would add noise to the\nlabels. For MNIST-CIFAR, we independently apply random augmentation to MNIST and CIFAR\nrespectively (drawn from the same set of augmentations as detailed above) and then concatenate them\nto construct training examples.\nPlease see implementation details for each dataset in the respective subsection.\nB.1\nMNIST-CIFAR\nWe use the same hyperparameter search range for models in each category of AE, SSL and SL, as\noutlined in table 9. The chosen hyperparameters for each model are speciﬁed in table 10.\nIn Shah et al. (2020) where MNIST-CIFAR was originally proposed, authors utilised more complex\nbackbone architecture such as DenseNet and MobileNet. However in our experiments, we ﬁnd that a\nlightweight 4-layer CNN can already achieve very high accuracy on both MNIST and CIFAR. The\narchitecture of the CNN we use can be found in table 11. Note that for SL and SSL we only use the\nencoder and for AE we use the decoder as well. The size of base channel C and latent dimension L\nare found through hyperparameter search.\nTable 10: Chosen hyperparameters for MNIST-CIFAR including latent dimension (L), base feature\nsize of CNN (C), batch size (B), learning rate (lr.), weight decay (wd.), optimiser (optim.), learning\nrate scheduler (lr.scheduler).\nL\nC\nB\nlr.\nwd.\nOptim.\nlr. scheduler\nAE\n128\n16\n128\n1e-3\n0\nAdam\nwarmup cosine\nVAE\n128\n32\n128\n1e-4\n0\nAdam\nwarmup cosine\nIWAE\n128\n32\n128\n1e-4\n0\nAdam\nstep\nβ-VAE\n128\n16\n128\n1e-4\n0\nAdam\nstep\nSimCLR\n128\n32\n128\n6e-1\n1e-4\nSGD\nwarmup cosine\nBYOL\n128\n64\n128\n7e-1\n0\nSGD\nwarmup cosine\nSimSiam\n128\n128\n128\n6e-1\n1e-5\nSGD\nwarmup cosine\nSupervised\n128\n16\n128\n1e-4\n0\nSGD\nwarmup cosine\nEncoder\nInput ∈R3×64×32\n4x4 conv. C stride 2x2 pad 1x1 & ReLU\n4x4 conv. 2C stride 2x2 pad 1x1 & ReLU\n4x4 conv. 4C stride 2x2 pad 1x1 & ReLU\n4x1 conv. 4C stride 2x1 pad 1x0 & ReLU\n4x4 conv. L stride 1 pad 0, 4x4 conv. L stride 1x1 pad 0x0\nDecoder\nInput ∈RL\n4x4 upconv. 4C stride 1x1 pad 0x0 & ReLU\n4x1 upconv. 4C stride 2x1 pad 1x0 & ReLU\n4x4 upconv. 2C stride 2x2 pad 1x1 & ReLU\n4x4 upconv. C stride 2x2 pad 1x1 & ReLU\n4x4 upconv. 3 stride 2x2 pad 1x1 & Sigmoid\nTable 11: CNN architecture, MNIST-CIFAR dataset.\n16\nTable 14: Hyperparameter search range for Camelyon17, including decoder type, latent dimension\n(L), learning rate (lr.), weight decay (wd.), optimiser (optim.), learning rate scheduler (lr. scheduler).\nDecoder type\nL\nlr.\nwd.\nOptim.\nlr. scheduler\nAE\n[CNN, MLP, ResNet]\n{256, 512, 1024}\n{1e-4, 5e-4, 1e-3, 5e-3, 1e-2}\n{0, 1e-4}\n{Adam, SGD}\n{warmup cosine, step, none}\nSSL\n-\n{256, 512, 1024}\n{1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 1e-1, 5e-1, 1}\n{0, 1e-3, 1e-4, 1e-5}\n{Adam, SGD}\n{warmup cosine, step, none}\nSL\n-\n{256, 512, 1024}\n{1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 1e-1, 5e-1}\n{0, 1e-4}\n{Adam, SGD}\n{warmup cosine, step, none}\nB.2\nCDSPRITES\nWe found all models to be relatively robust to hyperparameters, as most conﬁgurations result in close\nto perfect shape and color classiﬁcation accuracy on the ID validation set. The chosen hyperparameters\nfor each model are speciﬁed in table 12. We omit β-VAE from the comparison, as we empirically\nfound that β = 1 leads to the best performance on the ID validation set and therefore the results\nfor the β-VAE would be similar to the VAE. We use the same augmentations (random crops and\nhorizontal ﬂips) for all models and use no color augmentations in order to keep the invariance of the\nlearned representations with respect to color. The encoder and decoder architectures are described in\ntable 13.\nTable 12: Chosen hyperparameters for CdSprites including latent dimension (L), base feature size\nof CNN (C), batch size (B), learning rate (lr.), weight decay (wd.), optimiser (optim.), learning rate\nscheduler (lr.scheduler).\nL\nC\nB\nlr.\nwd.\nOptim.\nnone\nAE\n512\n64\n128\n5e-5\n1e-4\nAdam\nnone\nVAE\n512\n64\n128\n5e-5\n1e-4\nAdam\nnone\nIWAE\n512\n64\n128\n5e-5\n1e-4\nAdam\nnone\nSimCLR\n64\n32\n64\n5e-3\n1e-5\nSGD\nwarmup cosine\nBYOL\n64\n32\n64\n5e-1\n1e-5\nSGD\nwarmup cosine\nSimSiam\n64\n32\n64\n8e-2\n1e-5\nSGD\nwarmup cosine\nSupervised\n512\n64\n128\n5e-5\n1e-4\nAdam\nnone\nEncoder\nInput ∈R3×64×64\n4x4 conv. C stride 2x2 pad 1x1 & ReLU\n4x4 conv. 2C stride 2x2 pad 1x1 & ReLU\n4x4 conv. 4C stride 2x2 pad 1x1 & ReLU\n4x4 conv. 8C stride 2x2 pad 1x1 & ReLU\n4x4 conv. L stride 1 pad 0\nDecoder\nInput ∈RL\n4x4 upconv. 8C stride 1x1 pad 0x0 & ReLU\n4x4 upconv. 4C stride 2x2 pad 1x1 & ReLU\n4x4 upconv. 2C stride 2x2 pad 1x1 & ReLU\n4x4 upconv. C stride 2x2 pad 1x1 & ReLU\n4x4 upconv. 3 stride 2x2 pad 1x1\nTable 13: CNN architecture, CdSprites dataset.\nB.3\nCAMELYON17 AND FMOW\nFor hyperparameters including batch size, max epoch and model selection criteria, we follow the\nsame protocol as in WILDS (Koh et al., 2021): for Camelyon17 we use a batch size of 32, train all\nmodels for 10 epochs and select the model that results in the highest accuracy on the validation set,\nand for FMoW the batch size is 32, max epoch is 60 and model selection criteria is worst group\naccuracy on OOD validation set. For the rest, we use the same hyperparameter search range for\nmodels in each category of AE, SSL and SL, as outlined in table 14. The chosen hyperparameters for\nCamelyon17 are speciﬁed in table 15, and for FMoW in table 16. For Camelyon17-C and FMoW-C\nwe use these same hyperparameters.\n17\nTable 15: Chosen hyperparameters for Camelyon17 including latent dimension (L), learning rate (lr.),\nweight decay (wd.), optimiser (optim.), learning rate scheduler (lr. scheduler).\nDecoder\nlr.\nwd.\nOptim.\nlr. scheduler\nAE\nResNet\n5e-4\n1e-5\nSGD\nwarmup cosine\nVAE\nMLP\n1e-4\n0\nAdam\nnone\nIWAE\nMLP\n1e-4\n0\nAdam\nnone\nβ-VAE\nMLP\n1e-4\n0\nAdam\nnone\nSimCLR\n-\n1e-1\n0\nSGD\nnone\nBYOL\n-\n1e-1\n1e-5\nSGD\nwarmup cosine\nSimSiam\n-\n1e-1\n1e-5\nSGD\nwarmup cosine\nSupervised\n-\n1e-3\n1e-3\nSGD\nnone\nTable 16: Chosen hyperparameters for FMoW including latent dimension (L), learning rate (lr.),\nweight decay (wd.), optimiser (optim.), learning rate scheduler (lr. scheduler).\nDecoder\nlr.\nwd.\nOptim.\nlr. scheduler\nAE\nCNN\n1e-1\n1e-4\nSGD\nnone\nVAE\nMLP\n1e-6\n1e-4\nAdam\nstep\nIWAE\nMLP\n1e-6\n1e-4\nAdam\nstep\nβ-VAE\nMLP\n1e-6\n1e-4\nAdam\nstep\nSimCLR\n-\n5e-4\n1e-3\nSGD\nstep\nBYOL\n-\n1e-2\n1e-4\nSGD\nstep\nSimSiam\n-\n5e-4\n0\nSGD\nstep\nSupervised\n-\n1e-4\n0\nAdam\nstep\nWe follow Koh et al. (2021) and use DenseNet121 (Huang et al., 2017) as backbone architecture.\nFor the decoder of the AE models, we perform hyperparameter search between three architectures:\na CNN (see table 17), a simple 3-layer MLP (see table 18) and a ResNet-like decoder with skip\nconnections (see table 19).\nCNN, Decoder\nInput ∈RL\n4x4 upconv. 8C stride 2x2 pad 1x1 & ReLU\n4x4 upconv. 8C stride 2x2 pad 0x0 & ReLU\n4x4 upconv. 4C stride 2x2 pad 1x1 & ReLU\n4x4 upconv. 2C stride 2x2 pad 1x1 & ReLU\n4x4 upconv. C stride 2x2 pad 1x1 & ReLU\n4x4 upconv. 3 stride 2x2 pad 1x1 & Sigmoid\nTable 17: CNN architecture, Camelyon17 dataset.\nMLP, Decoder\nInput ∈RL\nfc. 2L & ReLU\nfc. 4L & ReLU\nfc. 3*96*96 & ReLU\nTable 18: MLP architecture, Camelyon17 dataset.\nResNet, Decoder\nInput ∈RL\nfc. 2048 & ReLU\n3x3 conv. 16C stride 1x1 pad 1x1\n3x3 conv. 16C stride 1x1 pad 1x1\nx2 upsample\n3x3 conv. 8C stride 1x1 pad 1x1\n3x3 conv. 8C stride 1x1 pad 1x1\nx2 upsample\n3x3 conv. 8C stride 1x1 pad 1x1\n3x3 conv. 8C stride 1x1 pad 1x1\n3x3 conv. 3 stride 1x1 pad 1x1\nTable 19: ResNet decoder architecture, Camelyon17 dataset.\n18\n0.0\n0.5\n1.0\nrid\n0\n20\n40\nLinear head bias b\nSL\nSSL\nAE\n0.0\n0.5\n1.0\nrid\n0.0\n2.5\n5.0\n7.5\n10.0\nLinear head bias b\nSL\nSSL\nAE\n0.0\n0.5\n1.0\nrid\n0\n20\n40\n60\n80\n100\nLinear head bias b\nSL\nSSL\nAE\nFigure 6: Linear head bias on controllable shift datasets\nC\nCONSTRUCTING CAMELYON17-CS AND FMOW-CS\nWe subsample Camelyon17 and FMoW dataset to create varying degree of spurious correlation\nbetween the domain and label information. We refer to these datasets as Camelyon17-CS and\nFMoW-CS. To construct such datasets, we ﬁrst ﬁnd some domain-label pairing in each dataset, such\nthat if we sample the dataset according to this pairing, the population of each class with respect to\nthe total number of examples in the dataset remains relatively stable. The rid = 1 versions of both\nCamelyon17-CS and FMoW-CS can be acquired by simply subsampling the dataset following the\ndomain label pairing; to ensure fairness in comparison, when constructing the rid ∈{0, 0.5} versions\nof these datasets, we ﬁrst mix in anti-bias samples (i.e. samples that are not in the domain-label\npairing) to change the spurious correlation, and then subsample the dataset such that the size of the\ndataset is the same as the rid = 1 version.\nThe domain-label pairing of Camelyon17-CS can be found in table 20 and FMoW-CS in table 21.\nLinear head bias\nWe also plot the linear head bias for the experiments conducted on Camelyon17-\nCS, FMoW-CS, and CdSprites in Figure 6. The experimental protocol follows that from Figure 3.\nTable 20: Domain-label pairing for Camelyon17-CS.\nDomain (hospital)\nLabel\nHospital 1, 2\nBenign\nHospital 3\nMalignant\nTable 21: Domain-label pairing for FMoW-CS.\nDomain (region)\nLabel\nAsia\nMilitary facility, multi-unit residential, tunnel opening,\nwind farm, toll booth, road bridge, oil or gas facility,\nhelipad, nuclear powerplant, police station, port\nEurope\nSmokestack, barn, waste disposal, hospital, water\ntreatment facility, amusement park, ﬁre station, fountain,\nconstruction site, shipyard, solar farm, space facility\nAfrica\nPlace of worship, crop ﬁeld, dam, tower, runway, airport, electric\nsubstation, ﬂooded road, border checkpoint, prison, archaeological site,\nfactory or powerplant, impoverished settlement, lake or pond\nAmericas\nRecreational facility, swimming pool, educational institution,\nstadium, golf course, ofﬁce building, interchange,\ncar dealership, railway bridge, storage tank, surface mine, zoo\nOceania\nSingle-unit residential, parking lot or garage, race track, park, ground\ntransportation station, shopping mall, airport terminal, airport hangar,\nlighthouse, gas station, aquaculture, burial site, debris or rubble\n19\nD\nOTHER RELATED WORK\nExplicit, extreme distribution shift\nRefers to when the features that caused distribution shift is\nexplicit, known, controllable, and in some cases, extreme (e.g. MNIST-CIFAR, CdSprites). This\ntype of settings are popular in works that investigate simplicity bias (Shah et al., 2020), dataset bias\n(Torralba and Efros, 2011) and shortcut learning (Geirhos et al., 2020; Lapuschkin et al., 2019), as\nit allows for users to easily adjust the level of distribution shift between train and test set. Various\nspecialised methods that either mitigate or address these problems under the supervised learning\nregime have been proposed, including Teney et al. (2022) that proposes to ﬁnd shortcut solutions\nby ensembles, Luo et al. (2021) that avoids shortcut learning by extracting foreground objects for\nrepresentation learning only, as well as Torralba and Efros (2011); Kim et al. (2019); Le Bras et al.\n(2020) that re-sample the dataset to reduce the spurious correlation.\nImportantly Kirichenko et al. (2022); Kang et al. (2020) shows that this extreme simplicity bias can\nbe mitigated in some cases by retraining the ﬁnal linear layer. This is a game changer, as it for the\nﬁrst time decouples the bias of the linear head from that of the main representation learning model.\nInterestingly, Robinson et al. (2021) also investigates the robustness of contrastive-SSL methods\nagainst simplicity bias. However, without training the linear head on OOD data, their ﬁnding is\nopposite to ours — that SSL methods are not able to avoid shortcut solutions.\nImplicit, subtle distribution shift\nThis type of problem is commonly seen in realistic distribution\nshift datasets (such as WILDS) and are often studied in Domain generalisation (DG). In this regime,\nthe training data are sampled from multiple domains, while data from a new, unseen target domain\nis used as test set. Note that here we omitted the discussion on domain adaptation (DA), as in DA\nmodels typically have access to unlabelled data from the target domain, which is different from the\nsettings we consider in this work.\nThere are mainly two lines of work in DG, namely 1) Distributional Robustness approaches (DRO),\nwhich minimises the worst group accuracy to address covariate shift (Gretton et al., 2009a;b) and\nsubpopulation shift (Sagawa et al., 2020; Hu et al., 2018); 2) Domain invariance, which consists of\nmethods that directly learn representations that are invariant across domains (Ben-David et al., 2010;\nGanin et al., 2016; Wang et al., 2019), encourage the alignment of gradients from different domains\n(Koyama and Yamaguchi, 2021; Lopez-Paz and Ranzato, 2017; Shi et al., 2022), or optimise for\nrepresentations that result in the same optimal classiﬁer for different domains (Arjovsky et al., 2019).\nApart from these supervised learning methods, the recent advancement of SSL has also inspired\nworks in unsupervised domain generalisation (Zhang et al., 2022; Harary et al., 2022). While all these\nmethods achieved impressive performance, we note that they are all specially designed for DG with\nthe majority of the methods relying on domain information and label information. In contrast, our\nwork studies how existing standard representation learning methods such as SSL and AE performs on\nDG tasks, with none of the methods relying on human annotations.\n20\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2022-06-17",
  "updated": "2022-12-16"
}