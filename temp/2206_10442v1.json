{
  "id": "http://arxiv.org/abs/2206.10442v1",
  "title": "Robust Task Representations for Offline Meta-Reinforcement Learning via Contrastive Learning",
  "authors": [
    "Haoqi Yuan",
    "Zongqing Lu"
  ],
  "abstract": "We study offline meta-reinforcement learning, a practical reinforcement\nlearning paradigm that learns from offline data to adapt to new tasks. The\ndistribution of offline data is determined jointly by the behavior policy and\nthe task. Existing offline meta-reinforcement learning algorithms cannot\ndistinguish these factors, making task representations unstable to the change\nof behavior policies. To address this problem, we propose a contrastive\nlearning framework for task representations that are robust to the distribution\nmismatch of behavior policies in training and test. We design a bi-level\nencoder structure, use mutual information maximization to formalize task\nrepresentation learning, derive a contrastive learning objective, and introduce\nseveral approaches to approximate the true distribution of negative pairs.\nExperiments on a variety of offline meta-reinforcement learning benchmarks\ndemonstrate the advantages of our method over prior methods, especially on the\ngeneralization to out-of-distribution behavior policies. The code is available\nat https://github.com/PKU-AI-Edge/CORRO.",
  "text": "Robust Task Representations for Ofﬂine Meta-Reinforcement Learning\nvia Contrastive Learning\nHaoqi Yuan 1 Zongqing Lu 1\nAbstract\nWe study ofﬂine meta-reinforcement learning, a\npractical reinforcement learning paradigm that\nlearns from ofﬂine data to adapt to new tasks. The\ndistribution of ofﬂine data is determined jointly by\nthe behavior policy and the task. Existing ofﬂine\nmeta-reinforcement learning algorithms cannot\ndistinguish these factors, making task representa-\ntions unstable to the change of behavior policies.\nTo address this problem, we propose a contrastive\nlearning framework for task representations that\nare robust to the distribution mismatch of behavior\npolicies in training and test. We design a bi-level\nencoder structure, use mutual information max-\nimization to formalize task representation learn-\ning, derive a contrastive learning objective, and\nintroduce several approaches to approximate the\ntrue distribution of negative pairs. Experiments\non a variety of ofﬂine meta-reinforcement learn-\ning benchmarks demonstrate the advantages of\nour method over prior methods, especially on the\ngeneralization to out-of-distribution behavior poli-\ncies.\n1. Introduction\nDeep reinforcement learning (RL) has achieved great suc-\ncesses in playing video games (Mnih et al., 2013; Lample &\nChaplot, 2017), robotics (Nguyen & La, 2019), recommen-\ndation systems (Zheng et al., 2018) and multi-agent systems\n(OroojlooyJadid & Hajinezhad, 2019). However, deep RL\nstill faces two challenging problems: data efﬁciency and\ngeneralization. To play Atari games, model-free RL takes\nmillions of steps of environment interactions, while model-\nbased RL takes 100k environment steps (Łukasz Kaiser\net al., 2020). The tremendous interactions along with safety\nissues prevent its applications in many real-world scenar-\n1School of Computer Science, Peking University. Correspon-\ndence to: Zongqing Lu <zongqing.lu@pku.edu.cn>.\nProceedings of the 39 th International Conference on Machine\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-\nright 2022 by the author(s).\nios. Ofﬂine reinforcement learning (Levine et al., 2020)\ntackles the sample efﬁciency problem by learning from pre-\ncollected ofﬂine datasets, without any online interaction. On\nthe other hand, as deep RL is supposed to be tested in the\nsame environment to the training environment, when the\ntransition dynamics or the reward function changes, the per-\nformance degenerates. To tackle the generalization problem,\nmeta-reinforcement learning (Finn et al., 2017; Duan et al.,\n2016) introduces learning over task distribution to adapt to\nnew tasks.\nOfﬂine Meta-Reinforcement Learning (OMRL), an under-\nstudied problem, lies in the intersection of ofﬂine RL and\nmeta-RL. Usually, the ofﬂine dataset is collected from mul-\ntiple tasks by different behavior policies. The training agent\naims at learning a meta-policy, which is able to efﬁciently\nadapt to unseen tasks. Recent studies (Li et al., 2020; Dorf-\nman et al., 2020; Li et al., 2021a;b) extend context-based\nmeta-RL to OMRL. They propose to use a context encoder\nto learn task representations from the collected trajectories,\nthen use the latent codes as the policy’s input.\nHowever, context-based methods are vulnerable to the dis-\ntribution mismatch of behavior policies in training and test\nphases. The distribution of collected trajectories depends\nboth on the behavior policy and the task. When behav-\nior policies are highly correlated with tasks in the training\ndataset, the context encoder is likely to memorize the feature\nof behavior policies. Thus, in the test phase, the context\nencoder produces biased task inference due to the change of\nthe behavior policy. Although Li et al. (2021b;a) employed\ncontrastive learning to improve task representations, their\nlearning objectives are simply based on discriminating tra-\njectories from different tasks, and thus cannot eliminate the\ninﬂuence of behavior policies.\nTo overcome this limitation, we propose a novel frame-\nwork of COntrastive Robust task Representation learning\nfor OMRL (CORRO). We design a bi-level structured task\nencoder, where the ﬁrst level extracts task representations\nfrom one-step transition tuples instead of trajectories and the\nsecond level aggregates the representations. We formalize\nthe learning objective as mutual information maximization\nbetween the representation and task, to maximally eliminate\nthe inﬂuence of behavior policies from task representations.\narXiv:2206.10442v1  [cs.LG]  21 Jun 2022\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\nWe introduce a contrastive learning method to optimize for\nInfoNCE, a mutual information lower bound. To approxi-\nmate the negative pairs’ distribution, we introduce two ap-\nproaches for negative pairs generation, including generative\nmodeling and reward randomization. Experiments in Point-\nRobot environment and multi-task MuJoCo benchmarks\ndemonstrate the substantial performance gain of CORRO\nover prior context-based OMRL methods, especially when\nthe behavior policy for adaptation is out-of-distribution.\nOur main contributions, among others, are:\n• We propose a framework for learning robust task repre-\nsentations with fully ofﬂine datasets, which can distin-\nguish tasks from the distributions of transitions jointly\ndetermined by the behavior policy and task.\n• We derive a contrastive learning objective to extract\nshared features in the transitions of the same task while\ncapturing the essential variance of reward functions\nand transition dynamics across tasks.\n• We empirically show on a variety of benchmarks\nthat our method much better generalizes to out-of-\ndistribution behavior policies than prior methods, even\nbetter than supervised task learning that assumes the\nground-truth task descriptions.\n2. Related Work\nOfﬂine Reinforcement Learning allows policy learning\nfrom data collected by arbitrary policies, increasing the sam-\nple efﬁciency of RL. In off-policy RL (Mnih et al., 2013;\nWang et al., 2016; Haarnoja et al., 2018; Lillicrap et al.,\n2016; Peng et al., 2019), the policy reuses the data collected\nby its past versions, and learns based on Q-learning or im-\nportance sampling. Batch RL studies learning from fully\nofﬂine data. Recent works (Lange et al., 2012; Fujimoto\net al., 2019b; Levine et al., 2020; Wu et al., 2019; Fujimoto\net al., 2019a; Kumar et al., 2019) propose methods to over-\ncome distributional shift and value overestimation issues\nin batch RL. Our study follows the batch RL setting, but\nfocuses on learning task representations and meta-policy\nfrom ofﬂine multi-task data.\nMeta-Reinforcement Learning learns to quickly adapt to\nnew tasks via training on a task distribution. Context-based\nmethods (Duan et al., 2016; Zintgraf et al., 2020; Rakelly\net al., 2019; Fakoor et al., 2020; Parisotto et al., 2019) for-\nmalize meta-RL as POMDP, regard tasks as unobservable\nparts of states, and encode task information from history\ntrajectories. Optimization-based methods (Finn et al., 2017;\nRothfuss et al., 2019; Stadie et al., 2018; Al-Shedivat et al.,\n2018; Foerster et al., 2018; Houthooft et al., 2018) formalize\ntask adaptation as performing policy gradients over few-shot\nsamples and learn an optimal policy initialization. Our study\nis based on the framework of context-based meta-RL.\nOfﬂine Meta-Reinforcement Learning studies learning to\nlearn from ofﬂine data. Because there is a distributional mis-\nmatch between ofﬂine data and online explored data during\ntest, learning robust task representations is an important\nissue. Recent works (Li et al., 2021b;a) apply contrastive\nlearning over trajectories for compact representations, but\nignore the inﬂuence of behavior policy mismatch. To ﬁx\nthe distributional mismatch, Li et al. (2020); Dorfman et al.\n(2020) assume known reward functions of different tasks,\nand Pong et al. (2021) require additional online exploration.\nUnlike them, we consider learning robust task representa-\ntions with fully ofﬂine data.\nContrastive Learning (He et al., 2020; Grill et al., 2020;\nWang & Isola, 2020) is a popular method for self-supervised\nrepresentation learning. It constructs positive or negative\npairs as noisy versions of samples with the same or different\nsemantics. Via distinguishing the positive pair among a large\nbatch of pairs, it extracts meaningful features. Contrastive\nlearning has been widely applied in computer vision (Chen\net al., 2020; He et al., 2020; Patrick et al., 2020), multi-\nmodal learning (Sermanet et al., 2018; Tian et al., 2020;\nLiu et al., 2020b) and image-based reinforcement learning\n(Anand et al., 2019; Stooke et al., 2021; Laskin et al., 2020).\nIn our study, we apply contrastive learning in ofﬂine task\nrepresentation learning, for robust OMRL.\n3. Preliminaries\n3.1. Problem Formulation\nA task for reinforcement learning is formalized as a fully\nobservable Markov Decision Process (MDP). MDP is mod-\neled as a tuple M = (S, A, T, ρ, R, γ), where S is the\nstate space, A is the action space, T(s′|s, a) is the transi-\ntion dynamics of the environment, ρ(s) is the initial state\ndistribution, R(s, a) is the reward function, γ ∈[0, 1) is\nthe factor discounting the future reward. The policy of the\nagent is a distribution π(a|s) over actions. Starting from\nthe initial state, for each time step, the agent performs an\naction sampled from π, then the environment updates the\nstate with T and returns a reward with R. We denote the\nmarginal state distribution at time t as µt\nπ(s). The objec-\ntive of the agent is to maximize the expected cumulative\nrewards maxπ JM(π) = Est∼µtπ,at∼π[P∞\nt=0 γtR(st, at)].\nTo evaluate and optimize the policy, we deﬁne V-function\nand Q-function as follows:\nVπ(s) =\n∞\nX\nt=0\nγtEst∼µtπ,at∼π [R(st, at)]\n(1)\nQπ(s, a) = R(s, a) + γEs′∼T (s′|s,a)[Vπ(s′)].\n(2)\nQ-learning solves for the optimal policy by iterating the\nBellman optimality operator B over Q-function:\nB ˆQ(s, a) = R(s, a)+γEs′∼T (s′|s,a)\nh\nmax\na′\nˆQ(s′, a′)\ni\n. (3)\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\nIn ofﬂine meta-reinforcement learning (OMRL), we\nassume that the task follows a distribution Mi\n=\n(S, A, Ti, ρ, Ri, γ)\n∼\nP(M).\nTasks share the same\nstate space and action space, but varies in reward func-\ntions and transition dynamics.\nThus, we can also de-\nnote the task distribution as P(R, T). Given N training\ntasks {Mi}N\ni=1, for each task i, an ofﬂine dataset Xi =\n{(si,j, ai,j, ri,j, s′\ni,j)}K\nj=1 is collected by arbitrary behavior\npolicy πi\nβ. The learning algorithms can only access the\nofﬂine datasets to train a meta-policy πmeta, without any en-\nvironmental interactions. At test time, given an unseen task\nM ∼P(M), an arbitrary exploration (behavior) policy col-\nlects a context c = {(sj, aj, rj, s′\nj)}k\nj=1, then the learned\nagent performs task adaptation conditioned on c to get a\ntask-speciﬁc policy πM and evaluate in the environment.\nThe objective for OMRL is to learn a meta policy maximize\nthe expected return over the test tasks:\nJ(πmeta) = EM∼P (M)[JM(πM)] .\n(4)\n3.2. Context-Based OMRL and FOCAL\nContext-based learning regards OMRL as solving par-\ntially observable MDPs.\nConsidering the task M as\nthe unobservable part of the state, the agent gathers\ntask information and makes decisions upon the his-\ntory trajectory: at ∼π(a|τ0:t−1, st), where τ0:t−1 =\n(s0, a0, r0, · · · , st−1, at−1, rt−1).\nA major assumption in context-based learning is that differ-\nent tasks share some common structures and the variation\nover tasks can be described by a compact representation. For\nexample, in a robotic object manipulation scenario, tasks\ncan be described as object mass, friction, and target posi-\ntion, while rules of physical interaction are shared across\ntasks. Context-based OMRL uses a task encoder to learn a\nlatent space zt = E(τ0:t−1), to represent task information\n(Rakelly et al., 2019; Li et al., 2021b) or task uncertainty\n(Dorfman et al., 2020). The policy π(a|s, z) is conditioned\non the latent task representation. Given a new task, the ex-\nploration policy collects a few trajectories (context), and the\ntask encoder adapts the policy by producing z. In principle,\nthe task encoder and the policy can be trained on ofﬂine\ndatasets with off-policy or batch RL methods. In our setting,\nwe assume that the exploration policy is arbitrary, and the\ncontext is a single trajectory. During training, the context is\nsampled from the ofﬂine dataset.\nFOCAL (Li et al., 2021b) improves task representation learn-\ning of the encoder via distance metric learning. The encoder\nminimizes the distance of trajectories from the same task\nand pushes away trajectories from different tasks in the\nlatent space. The loss function is\nLdml =1{yi = yj}∥qi −qj∥2\n2+\n1{yi ̸= yj}β ·\n1\n∥qi −qj∥n\n2 + ϵ\n(5)\nwhere qi = E(τi) is the representation of a trajectory, yi is\nthe identiﬁer of the ofﬂine dataset. The policy is separately\ntrained, conditioned on the learned encoder, with batch Q-\nlearning methods.\n3.3. Task Representation Problems in OMRL\nSince the ofﬂine datasets are collected by different behavior\npolicies, to answer which dataset a trajectory belongs to,\none may infer based on the feature of behavior policy rather\nthan rewards and state transitions. As an example: In a 2D\ngoal reaching environment, tasks differ in goal positions. In\neach training task, the behavior policy is to go towards the\ngoal position. Training on such datasets, the task encoder in\nFOCAL can simply ignore the rewards and distinguish the\ntasks based on the state-action distribution in the context.\nThus, it will make mistakes when the context exploration\npolicy changes.\nSimilar problem is also mentioned in Dorfman et al. (2020);\nLi et al. (2020). To eliminate the effects of behavior policies\nin task learning, Dorfman et al. (2020) augment the datasets\nby collecting trajectories in different tasks with the same\npolicy, and relabeling the transition tuples with different\ntasks’ reward functions. However, in our fully ofﬂine set-\nting, collecting additional data and accessing the reward\nfunctions are not allowed. Li et al. (2020) use the learned\nreward functions in different tasks to relabel the trajectories\nand apply metric learning over trajectories. This approach\ndoes not support the settings where tasks differ in transition\ndynamics, because we cannot perform transition relabeling\nof s′ over the trajectory to mimic the trajectory in other tasks.\nAlso, the reward function learned with limited single-task\ndataset can be inaccurate to relabel the unseen state-action\npairs.\n4. Method\nTo address the ofﬂine task representation problem, we pro-\npose CORRO, a novel contrastive learning framework for\nrobust task representations with fully ofﬂine datasets, which\ndecreases the inﬂuence of behavior policies on task represen-\ntations while supporting tasks that differ in reward function\nand transition dynamics.\n4.1. Bi-Level Task Encoder\nWe ﬁrst design a bi-level task encoder where representation\nlearning focuses on transition tuples rather than trajectories.\nConcretely, our task encoder consists of a transition encoder\nEθ1 and an aggregator Eθ2, parameterized collectively as\nθ. Given a context c = {(si, ai, ri, s′\ni)}k\ni=1, the transition\nencoder extracts latent representations for all the transition\ntuples zi = Eθ1(si, ai, ri, s′\ni), then the aggregator gather all\nthe latent codes into a task representation z = Eθ2({zi}k\ni=1).\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\nContext\nContrastive Learning\n𝑧𝑖𝑖=1\n𝑘\n{𝑧∗}\n𝑧\nOffline Meta-RL\n𝐸𝜃1\n𝐸𝜃2\n𝑄𝜓\n𝜋𝜙\n𝑎\n𝑞\nOffline\nDataset\nNeg. Pairs\nGenerator\nFigure 1. CORRO framework. The transition encoder Eθ1 extracts\nthe latent representations of each transition tuple in the context.\nContrastive learning is applied to robustify the task encoder. The\naggregator Eθ2 gathers all the latent codes, to condition the actor\nand critic.\nThe Q-function Qψ(s, a, z) and the policy πφ(a|s, z) are\nconditioned on z, parameterized with φ, ψ.\nCompared to trajectories, transition tuples leak less behav-\nior policy information and are suitable for both transition\nrelabeling and reward relabeling. Thus, we design robust\ntask representation learning methods to train only the tran-\nsition encoder in the following sections. The architecture\nof the transition encoder is simple MLP. The aggregator is\ntrained with Q-function and policy using ofﬂine RL algo-\nrithms. Intuitively, it should draw attention to the transition\ntuples with rich task information. Inspired by self-attention\n(Vaswani et al., 2017), we introduce an aggregator which\ncomputes the weighted sum of the input vectors:\nz =\nk\nX\nj=1\nsoftmax\n\u0000{MLP(zi)}k\ni=1\n\u0001\nj · zj.\n(6)\nFigure 1 gives an overview of our framework.\n4.2. Contrastive Task Representation Learning\nAn ideally robust task representation should be dependent\non tasks and invariant across behavior policies. We intro-\nduce mutual information to measure the mutual dependency\nbetween the task representation and the task. As, intuitively,\nmutual information measures the uncertainty reduction of\none random variable when the other one is observed, we\npropose to maximize the mutual information between the\ntask representation and the task itself, so that the task en-\ncoder learns to maximally reduce task uncertainty while\nminimally preserving task-irrelevant information.\nMathematically, we formalize the transition encoder Eθ1 as\na probabilistic encoder z ∼P(z|x), where x = (s, a, r, s′)\ndenotes the transition tuple. Task M follows the task distri-\nbution P(M) and the distribution of x is determined jointly\nby M and the behavior policy. The learning objective for\nthe transition encoder is:\nmax I(z; M) = Ez,M\n\u0014\nlog p(M|z)\np(M)\n\u0015\n.\n(7)\nOptimizing mutual information is intractable in practice.\nInspired by noise contrastive estimation (InfoNCE) in the\nliterature of contrastive learning (Oord et al., 2018), we\nderive a lower bound of Eq. (7) and have the following\ntheorem.\nTheorem 4.1. Let M be a set of tasks following the task\ndistribution, |M| = N. M ∈M is the ﬁrst task with\nreward and transition R, T.\nLet x = (s, a, r, s′), z ∼\nP(z|x), h(x, z) = P (z|x)\nP (z) , where (s, a) follows an arbitrary\ndistribution, r = R(s, a), s′ ∼T(s′|s, a). For any task\nM ∗∈M with reward and transition R∗, T ∗, denote x∗=\n(s, a, r∗, s∗′) as a transition tuple generated in M ∗condi-\ntioned on (s, a), where r∗= R∗(s, a), s∗′ ∼T ∗(s′|s, a).\nThen we have\nI(z; M) −log(N) ≥EM,x,z\n\u0014\nlog(\nh(x, z)\nP\nM ∗∈M h(x∗, z))\n\u0015\n.\nWe give a proof of the theorem in Appendix A. Following\nInfoNCE, we approximate h with the exponential of a score\nfunction S(z∗, z), which is a similarity measure between the\nlatent codes of two samples. We derive a sampling version\nof the tractable lower bound to be the transition encoder’s\nlearning objective\nmax\nθ1\nX\nMi∈M\nx,x′∈Xi\n\u0014\nlog\n\u0012\nexp(S(z, z′))\nP\nM ∗∈M exp(S(z, z∗))\n\u0013\u0015\n,\n(8)\nwhere M is the set of training tasks, x, x′ are two tran-\nsition tuples sampled from dataset Xi and z, z′ are latent\nrepresentations of x, x′. For tasks M ∗∈M \\ {Mi}, z∗\nis the representation of the transition tuple x∗sampled in\ntask M ∗conditioned on the same state-action pair in x. For\nM ∗= Mi, we deﬁne z∗= z′ for consistent notation. We\nuse cosine similarity for the score function in practice.\nFollowing the literature, we name (x, x′) a positive pair\nand name {(x, x∗)}M ∗∈M\\{M} negative pairs. Eq. (8)\noptimizes for a N-way classiﬁcation loss to classify the\npositive pair out of all the pairs. To maximize the score of\npositive pairs, the transition encoder should extract shared\nfeatures in the transitions of the same task. To decrease the\nscore of negative pairs, the transition encoder should capture\nthe essential variance of rewards and state transitions since\nstate-action pairs are the same across tasks.\n4.3. Negative Pairs Generation\nGenerating negative pairs in Eq. (8) involves computing\n(r, s′) with tasks’ reward functions and transition dynamics,\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\nwhich is impossible in the fully ofﬂine setting. Li et al.\n(2020) introduce a reward relabeling method that ﬁts a re-\nward model for each task using ofﬂine datasets. In our\nsetting, we also need to ﬁt transition models with higher\ndimensions. However, the dataset for each task is usually\nsmall, making reward models and transition models overﬁt.\nWhen the overlap of state-action pairs between different\ntasks is small, relabeling with separately learned models can\nbe inaccurate, making contrastive learning ineffective.\nTo generate high-quality negative samples, we design meth-\nods based on two major principles.\nFidelity: To ensure the task encoder learning on real tran-\nsition tuples, the distribution of generated negative pairs\nshould approximate the true distribution in the denominator\nof Eq. (8). Concretely, given (s, a), the true distribution of\n(r, s′) in negative pairs is\np(r, s′|s, a) ∝EM∼P (M)[T(s′|s, a)1{R(s, a) = r}] (9)\nDiversity: According to prior works (Patrick et al., 2020;\nChen et al., 2020), the performance of contrastive learning\ndepends on the diversity of negative pairs. More diversity\nincreases the difﬁculty in optimizing InfoNCE and helps\nthe encoder to learn meaningful representations. In image\ndomains, large datasets, large negative batch sizes, and data\naugmentations are used to increase diversity. We adopt this\nprinciple in generating negative pairs of transitions.\nWe propose two approaches for negative pairs generation.\nGenerative Modeling: We observe that, though a state-\naction pair may not appear in all the datasets, it usually\nappears in a subset of training datasets. Fitting reward\nand transition models with larger datasets across tasks can\nreduce the prediction error. Thus, we propose to train a\ngenerative model over the union of training datasets ∪N\ni=1Xi,\nto approximate the ofﬂine data distribution P{Xi}(r, s′|s, a).\nWhen the distributions of (s, a) are similar across tasks, this\ndistribution approximates the distribution in Eq. (9).\nWe adopt conditional VAE (CVAE) (Sohn et al., 2015)\nfor generative modeling. CVAE consists of a generator\npξ(r, s′|s, a, z) and a probabilistic encoder qω(z|s, a, r, s′).\nThe latent vector z describes the uncertain factors for pre-\ndiction, follows a prior Gaussian distribution p(z). CVAE\nis trained to minimize the loss function\nLCVAE = −E(s,a,r,s′)∈{Xi}\nh\nEqω(z|s,a,r,s′)\n[log pξ(r, s′|s, a, z)] −KL[qω(z|s, a, r, s′)∥p(z)]\ni\n,\n(10)\nwhere KL(·∥·) is KL-divergence, working as an information\nbottleneck to minimize the information in z.\nReward Randomization: In the cases where the overlap of\nstate-action pairs between tasks is small, CVAE can collapse\nAlgorithm 1. Meta Training\nInput: Datasets {Xi}N\ni=1; OMRL models Eθ1, Eθ2, Qψ,\nπφ\nA. If use generative modeling, pre-train CVAE:\nInitialize CVAE qω, pξ\nrepeat\nUpdate ω, ξ to minimize Eq. (10).\nuntil Done\nB. Train the transition encoder:\nrepeat\nSample a task M and two transition tuples x, x′\nz = Eθ1(x), z′ = Eθ1(x′)\nfor M ∗∈M do\nif use generative modeling then\nSample x∗from CVAE\nelse if use reward randomization then\nAdd noise to the reward to get x∗\nend if\nz∗= Eθ1(x∗)\nend for\nCompute Eq. (8)\nUpdate θ1 to maximize Eq. (8)\nuntil Done\nC. Train the policy:\nrepeat\nSample a task dataset X and a context c\nz = Eθ2(Eθ1(c))\nAugment the states in X with z\nUpdate θ2, ψ, φ with ofﬂine RL algorithms on X\nuntil Done\nalmost to a deterministic prediction model and the diversity\nin negative pairs decreases. When tasks only differ in re-\nward functions, similar to data augmentation strategies in\nimage domains, we can generate negative pairs via adding\nrandom noise to the reward. We have r∗= r + ν, where\nthe perturbation ν follows a noise distribution p(ν). Though\nreward randomization cannot approximate the true distribu-\ntion of negative pairs, it provides an inﬁnitely large space\nto generate diverse rewards, making contrastive learning\nrobust.\n4.4. Algorithm Summary\nWe summarize our training method in Algorithm 1. The test\nprocess is shown in Algorithm 2. The code for our work is\navailable at https://github.com/PKU-AI-Edge/CORRO.\n5. Experiments\nIn experiments, we aim to demonstrate: (1) The perfor-\nmance of CORRO on tasks adaptation in diverse task distri-\nbutions and ofﬂine datasets; (2) The robustness of CORRO\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\n0\n50000\n100000\n150000\n200000\nTraining Steps\n8.5\n8.0\n7.5\n7.0\n6.5\n6.0\n5.5\n5.0\nAverage Return\nPoint­Robot\nCORRO\nFOCAL\nOffline PEARL\nSupervised\n0\n50000\n100000\n150000\n200000\nTraining Steps\n0\n25\n50\n75\n100\n125\n150\n175\nAverage Return\nAnt-Dir\nCORRO\nFOCAL\nOffline PEARL\nSupervised\n0\n50000\n100000\n150000\n200000\nTraining Steps\n300\n250\n200\n150\n100\n50\nAverage Return\nHalf-Cheetah-Vel\nCORRO\nFOCAL\nOffline PEARL\nSupervised\nFigure 2. Test returns of CORRO against the baselines in the environments with different reward functions: Point-Robot, Ant-Dir, and\nHalf-Cheetah-Vel. The shaded region shows standard deviation across 5 seeds.\nAlgorithm 2. Meta Test\nInput: Trained models Eθ1, Eθ2, Qψ, πφ\nSample a task M\nCollect a context trajectory c with an arbitrary policy\nz = Eθ2(Eθ1(c))\nrepeat\nObserve s, execute a ∼πφ(a|s, z), get r\nuntil Environment terminates\non task inference; and (3) How to choose the strategy of\nnegative pairs generation.\n5.1. Experimental Settings\nWe adopt a simple 2D environment and multi-task MuJoCo\nbenchmarks to evaluate our method.\nPoint-Robot is a 2D navigation environment introduced in\nRakelly et al. (2019). Starting from the initial point, the\nagent should navigate to the goal location. Tasks differ in\nreward functions, which describe the goal position. The\ngoal positions are uniformly distributed in a square.\nHalf-Cheetah-Vel, Ant-Dir are multi-task MuJoCo bench-\nmarks where tasks differ in reward functions.\nIn Half-\nCheetah-Vel, the task is speciﬁed by the target velocity of\nthe agent. The distribution of target velocity is U[0, vmax].\nIn Ant-Dir, the task is speciﬁed by the goal direction of\nthe agent’s motion. The distribution of goal direction is\nU[0, 2π]. These benchmarks are also used in Mitchell et al.\n(2021); Li et al. (2021b); Zintgraf et al. (2020).\nWalker-Param, Hopper-Param are multi-task MuJoCo\nbenchmarks where tasks differ in transition dynamics. For\neach task, the physical parameters of body mass, inertia,\ndamping, and friction are randomized. The agent should\nadapt to the varying environment dynamics to accomplish\nthe task. Previous works (Mitchell et al., 2021; Rakelly\net al., 2019) also adopt these benchmarks.\n0\n100000\n200000\nTraining Steps\n100\n0\n100\n200\n300\nAverage Return\nWalker-Param\nCORRO\nFOCAL\nOffline PEARL\nSupervised\n0\n100000\n200000\nTraining Steps\n50\n0\n50\n100\n150\n200\n250\n300\nAverage Return\nHopper-Param\nCORRO\nFOCAL\nOffline PEARL\nSupervised\nFigure 3. Test returns of CORRO against the baselines in the envi-\nronments with different transition dynamics: Walker-Param and\nHopper-Param. The shaded region shows standard deviation across\n5 seeds.\nFor each environment, 20 training tasks and 20 testing tasks\nare sampled from the task distribution. On each task, we\nuse SAC (Haarnoja et al., 2018) to train a single-task pol-\nicy independently. The replay buffers are collected to be\nthe ofﬂine datasets. During training, the context is a con-\ntiguous section of 200 transition tuples sampled from the\nofﬂine dataset of the corresponding task. More details about\nexperimental settings are available in Appendix B.\nTo evaluate the performance of our method, we introduce\nseveral representative context-based fully-ofﬂine methods\nto compare with.\nOfﬂine PEARL is a direct ofﬂine extension of PEARL\n(Rakelly et al., 2019). The context encoder is a permutation\ninvariant MLP, updated together with policy and Q-function\nusing ofﬂine RL algorithms. No additional terms are used\nfor task representation learning. This baseline is introduced\nin (Li et al., 2020; Anonymous, 2022; Li et al., 2021b).\nFOCAL (Li et al., 2021b) uses metric learning to train the\ncontext encoder. Positive and negative pairs are trajectories\nsampled from the same and the different tasks, respectively.\nNote that while the original FOCAL uses an ofﬂine RL\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\nTable 1. Average test returns of CORRO against baselines with different types of context exploration policy. IID means the context\ndistribution is the same to the training dataset, while OOD means the context is out-of-distribution.\nEnvironment\nSupervised.\nOfﬂine PEARL\nFOCAL\nCORRO\nIID\nOOD\nIID\nOOD\nIID\nOOD\nIID\nOOD\nPoint-Robot\n-4.89±0.10\n-5.84±0.14\n-5.4±0.17\n-6.74±0.19\n-6.06±0.42\n-7.34±0.20\n-5.19±0.05\n-6.39±0.05\nAnt-Dir\n136±17.6\n131.7±11.4\n155.4±24.4\n141.5±11.3\n109.8±12.8\n53.5±16.4\n156.8±35.2\n154.7±25.8\nHalf-Cheetah-Vel\n-31.6±0.7\n-32.1±0.9\n-31.2±0.5\n-242.7±6.0\n-38.0±4.0\n-204.1±9.5\n-33.7±1.1\n-89.7±7.4\nWalker-Param\n232.7±29.2\n221.2±43.4\n259.1±48.2\n254.7±35.8\n225.4±56.4\n193.3±151.5\n301.5±37.9\n284.0±19.3\nHopper-Param\n269.2±20.3\n251.9±28.8\n244.0±18.5\n236.6±18.5\n195.6±62.3\n199.7±51.9\n267.6±25.6\n268.0±13.8\nmethod BRAC (Wu et al., 2019), we implement FOCAL\nwith SAC to better study the task representation learning\nproblem.\nSupervised Task Learning assumes that the ground truth\ntask descriptions can be accessed during training. Based on\nPEARL, we add L2 loss to supervise the context encoder.\nWe introduce this supervised learning method as a strong\nbaseline. Note that the ground truth task labels are usually\nunavailable in meta-RL settings.\nFor fairness, we replace the policy learning methods of\nbaselines and CORRO with SAC. The hyperparameters for\npolicy learning are ﬁxed across different methods. All the\nexperiments are conducted over 5 different random seeds.\nAll hyperparameters are available in Appendix C.\n5.2. Tasks Adaptation Performance\nTo evaluate the performance on tasks adaptation, we use\ntasks in the test set, sample one trajectory in the pretrained\nreplay buffer of each task to be the context, then test the\nagent in the environment conditioned on the context. The\nperformance is measured by the average return over all the\ntest tasks. In this experiment, there is no distributional\nshift of contexts between training and test, since they are\ngenerated by the single-task policies learned with the same\nalgorithm.\nFigure 2 shows the test results in the environments with\ndifferent reward functions. In Point-Robot, CORRO and the\nsupervised baseline outperform others. In Ant-Dir, CORRO\noutperforms all the baselines including the supervised learn-\ning method. Although the value of return in Point-Robot\nvaries a little, we have to mention that the return in this en-\nvironment is insensitive to the task performance: the mean\nreturn of an optimal policy is around -5, while the mean\nreturn of a random policy is around -9. Beneﬁted from the\npretrained task encoder, CORRO shows great learning efﬁ-\nciency, and reaches high test returns after 20k steps of ofﬂine\ntraining. Though in Half-Cheetah-Vel we cannot distinguish\nthe performance of different methods, we will demonstrate\ntheir difference when context distribution shifts exist in Sec-\ntion 5.3 and the difference on the learned representations in\nSection 5.4.\nFigure 3 demonstrates the results in the environments with\nvarying transition dynamics. In both Walker-Param and\nHopper-Param, CORRO outperforms the baselines on task\nadaptation returns. Further, CORRO is more stable on policy\nimprovement, while the performance of FOCAL sometimes\ndegenerates during the ofﬂine training.\n5.3. Robust Task Inference\nA robust task encoder should capture a compact represen-\ntation of rewards and transition dynamics from the con-\ntext, eliminating the information of the behavior policy.\nConditioned on the context collected by an arbitrary be-\nhavior policy, the agent performs accurate task adaptation\nand reaches high returns. To measure the robustness, we\ndesign experiments of out-of-distribution (OOD) test. De-\nnote the saved checkpoints of data collection policies for\nall the tasks during different training periods as {πi,t\nβ }i,t,\nwhere i is the task and t is the training epoch. For each\ntest task M test\nj , we sample πi,t\nβ as the behavior policy to\ncollect a context rollout, and test the agent conditioned on\nthe context. Because πi,t\nβ\nis a policy trained on an arbi-\ntrary task, the context distribution is never seen during meta\ntraining. Then, we sample S behavior policies, measure the\nOOD test performance as\n1\nNS\nPS\nk=1\nPN\nj=1 J(M test\nj\n, πik,tk\nβ\n),\nwhere J(M, π) means the average return in task M with\ncontext collection policy π. Accordingly, we denote the\nstandard in-distribution test in Section 5.2 as IID test.\nTable 1 summarizes the IID and OOD test results of different\nmethods. All the test models are from the last training epoch.\nWhile the performance of all the methods degenerates from\nIID to OOD, CORRO outperforms Ofﬂine PEARL and FO-\nCAL in OOD test by a large margin. In Half-Cheetah-Vel,\nOfﬂine PEARL and FOCAL almost fail in task inference\nwith distributional shift. The slight performance degenera-\ntion shows the robustness of CORRO. Note that although\nthe supervised task learning method does not degenerate in\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\nCORRO\nOffline PEARL\nFOCAL\nSupervised.\nFigure 4. The 2D projection of the learned task representation\nspace in Half-Cheetah-Vel. Points are uniformly sampled from\ntest tasks. Tasks of velocities from 0 to 3 are mapped to rainbow\ncolors, from red to purple.\nHalf-Cheetah-Vel, it uses the true task labels to learn the\nrelation between contexts and tasks, which is not allowed in\ncommon meta-RL settings.\n5.4. Latent Space Visualization\nTo qualitatively analyze the learned task representation\nspace, we visualize the task representations by projecting\nthe embedding vectors into 2D space via t-SNE (Maaten &\nHinton, 2008). For each test task, we sample 200 transition\ntuples in the replay buffer to visualize. As shown in Figure\n4, compared to the baselines, CORRO distinguishes transi-\ntions from different tasks better. CORRO represents the task\nspace of Half-Cheetah-Vel into a 1D manifold, where the\ndistance between latent vectors is correlated to the differ-\nence between their target velocities, as shown in the color\nof the points.\n5.5. Ablation: Negative Pairs Generation\nNegative pairs generation is a key component of CORRO.\nGenerating more diverse negative samples can make con-\ntrastive learning robust and efﬁcient, while the distribution\nof negative pairs should also be close to the true distribu-\ntion as described in Section 4.2. Along with our design\nof generative modeling and reward randomization, we also\nintroduce Relabeling and None method. Relabeling is to\nseparately learn a reward model and a transition model on\neach ofﬂine training dataset. The negative pair is generated\nby ﬁrst sampling a model, then relabeling (r, s′) in the tran-\nTable 2. Ablation study on negative pairs generation methods in\nHalf-Cheetah-Vel environment (the upper four rows) and Point-\nRobot environment (the lower four rows).\nMethod\nContrastive Loss\nIID Return\nOOD Return\nGenerative\n0.07\n-33.7±1.1\n-89.7±7.4\nRandomize\n0.83\n-34.3±1.5\n-84.5±1.3\nRelabeling\n0.04\n-40.8±1.5\n-245.3±12.9\nNone\n1.20\n-34.1±2.4\n-97.6±3.1\nGenerative\n2.83\n-9.41±0.42\n-9.42±0.42\nRandomize\n0.54\n-5.19±0.05\n-6.39±0.05\nRelabeling\n0.04\n-9.22±0.24\n-9.27±0.22\nNone\n1.46\n-5.24±0.27\n-6.52±0.08\nsition tuple. None is a straightforward method where we\nsimply construct negative pairs with transition tuples from\ndifferent tasks and (s, a) in the transitions can be different.\nAs shown in Table 2, in Half-Cheetah-Vel, our methods\nusing generative modeling and reward randomization out-\nperform None and Relabeling in OOD test, due to better\napproximating the distribution of negative pairs.\nIn Half-Cheetah-Vel, we notice that the None method\nachieves the average return of -97.6 in OOD test and out-\nperforms FOCAL, of which the average return is -204.1\naccording to Table 1. They have two main differences: 1.\nsamples for representation learning are transition tuples in\n‘None’, but are trajectories in FOCAL; 2. the contrastive\nloss. To ﬁnd which affects the OOD generalization more, we\nre-implement FOCAL with our InfoNCE loss. It achieves\nan average return of -41.9 in the IID test and -208.2 in the\nOOD test, which is close to FOCAL’s performance. Thus,\nwe argue that focusing on transition tuples rather than whole\ntrajectories contributes to OOD generalization. Our ap-\nproach naturally discourages the task encoder to extract the\nfeature of behavior policy from the trajectory sequence.\nWhile generative modeling and reward randomization has\nsimilar performance in Half-Cheetah-Vel, the latter solves\nPoint-Robot better. Because in Point-Robot, behavior poli-\ncies in different tasks move in different directions, making\nstate-action distributions non-overlapping. CVAE collapses\ninto predicting the transition of the speciﬁc task where the\nstate-action pair appears, and cannot provide diverse nega-\ntive pairs to support contrastive learning.\nThe column of contrastive loss shows that the two proposed\nmethods generate diverse and effective negative pairs in the\nright circumstances, improving the learning of task encoder.\nThough the relabeling method has the best sample diversity,\nit underperforms other methods in task adaptation due to\nthe inaccurate predictions on unseen data.\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\nTable 3. Average test returns of CORRO against baselines with a random exploration policy for context collection.\nEnvironment\nSupervised.\nOfﬂine PEARL\nFOCAL\nCORRO\nPoint-Robot\n-5.32±0.20\n-7.06±0.99\n-8.64±0.26\n-5.59±0.57\nAnt-Dir\n149.8±20.5\n148.4±35.3\n89.8±8.7\n163.0±35.8\nHalf-Cheetah-Vel\n-37.6±0.8\n-35.4±1.8\n-41.6±3.3\n-42.9±0.7\nWalker-Param\n221.7±91.1\n276.6±37.7\n245.6±67.8\n300.5±34.2\nHopper-Param\n253.5±21.2\n245.9±18.9\n203.6±46.6\n273.3±3.9\n5.6. Adaptation with an Exploration Policy\nContext exploration is a crucial part of meta-RL. Existing\nworks either explore with the meta policy before adaptation\n(Zintgraf et al., 2020; Finn et al., 2017), or learn a separate\nexploration policy (Liu et al., 2020a; Zhang et al., 2020).\nTo test the task adaptation performance of CORRO with an\nexploration policy, we adopt a uniformly random policy to\ncollect the context. As shown in Table 3, CORRO and the\nsupervised baseline outperform others in Point-Robot. In\nHalf-Cheetah-Vel, all the methods have great adaptation per-\nformance compared with the results in Table 1. In the other\nthree environments, CORRO outperforms all the baselines.\n6. Conclusion\nIn this paper, we address the task representation learning\nproblem in the fully ofﬂine setting of meta-RL. We propose\na contrastive learning framework over transition tuples and\nseveral methods for negative pairs generation. In experi-\nments, we demonstrate how our method outperforms prior\nmethods in diverse environments and task distributions. We\nintroduce OOD test to quantify the superior robustness of\nour method and ablate the choice of negative pairs gener-\nation. Though, our study has not considered to learn an\nexploration policy for context collection, and to perform\nmoderate environmental interactions when training data is\nextremely limited. We leave these directions as future work.\nAcknowledgements\nWe would like to thank the anonymous reviewers for their\nuseful comments to improve our work. This work is sup-\nported in part by NSF China under grant 61872009.\nReferences\nAl-Shedivat, M., Bansal, T., Burda, Y., Sutskever, I., Mor-\ndatch, I., and Abbeel, P. Continuous Adaptation Via\nMeta-Learning in Nonstationary and Competitive En-\nvironments. In International Conference on Learning\nRepresentations, 2018.\nAnand, A., Racah, E., Ozair, S., Bengio, Y., Cˆot´e, M.-A.,\nand Hjelm, R. D. Unsupervised State Representation\nLearning in Atari. In Advances in neural information\nprocessing systems, 2019.\nAnonymous. Model-Based Ofﬂine Meta-Reinforcement\nLearning with Regularization. In Submitted to The Tenth\nInternational Conference on Learning Representations,\n2022. under review.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nSimple Framework for Contrastive Learning of Visual\nRepresentations. In International Conference on Machine\nLearning, 2020.\nDorfman,\nR.,\nShenfeld,\nI.,\nand Tamar,\nA.\nOf-\nﬂine Meta Learning of Exploration.\narXiv preprint\narXiv:2008.02598, 2020.\nDuan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever,\nI., and Abbeel, P.\nRL2: Fast Reinforcement Learn-\ning Via Slow Reinforcement Learning. arXiv preprint\narXiv:1611.02779, 2016.\nFakoor, R., Chaudhari, P., Soatto, S., and Smola, A. J. Meta-\nQ-Learning. In International Conference on Learning\nRepresentations, 2020.\nFinn, C., Abbeel, P., and Levine, S. Model-Agnostic Meta-\nLearning for Fast Adaptation of Deep Networks. In Inter-\nnational Conference on Machine Learning, 2017.\nFoerster, J., Farquhar, G., Al-Shedivat, M., Rockt¨aschel,\nT., Xing, E., and Whiteson, S. DiCE: The Inﬁnitely\nDifferentiable Monte Carlo Estimator. In International\nConference on Machine Learning, 2018.\nFujimoto, S., Conti, E., Ghavamzadeh, M., and Pineau,\nJ. Benchmarking Batch Deep Reinforcement Learning\nAlgorithms. arXiv preprint arXiv:1910.01708, 2019a.\nFujimoto, S., Meger, D., and Precup, D. Off-Policy Deep\nReinforcement Learning Without Exploration. In Inter-\nnational Conference on Machine Learning, 2019b.\nGrill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond,\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\nZ. D., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos,\nR., and Michal, V. Bootstrap Your Own Latent: A New\nApproach To Self-Supervised Learning. In Advances in\nneural information processing systems, 2020.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft\nActor-Critic: Off-Policy Maximum Entropy Deep Rein-\nforcement Learning with A Stochastic Actor. In Interna-\ntional Conference on Machine Learning, 2018.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momen-\ntum Contrast for Unsupervised Visual Representation\nLearning. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020.\nHouthooft, R., Chen, R. Y., Isola, P., Stadie, B. C., Wolski,\nF., Ho, J., and Abbeel, P. Evolved Policy Gradients. In\nAdvances in neural information processing systems, 2018.\nKumar, A., Fu, J., Tucker, G., and Levine, S. Stabilizing Off-\nPolicy Q-Learning Via Bootstrapping Error Reduction.\nIn Advances in Neural Information Processing Systems,\n2019.\nLample, G. and Chaplot, D. S. Playing FPS Games with\nDeep Reinforcement Learning. In AAAI Conference on\nArtiﬁcial Intelligence, 2017.\nLange, S., Gabel, T., and Riedmiller, M. Batch Reinforce-\nment Learning. In Reinforcement learning, pp. 45–73.\nSpringer, 2012.\nLaskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive\nUnsupervised Representations for Reinforcement Learn-\ning. In International Conference on Machine Learning,\n2020.\nLevine, S., Kumar, A., Tucker, G., and Fu, J. Ofﬂine Rein-\nforcement Learning: Tutorial, Review, and Perspectives\non Open Problems. arXiv preprint arXiv:2005.01643,\n2020.\nLi, J., Vuong, Q., Liu, S., Liu, M., Ciosek, K., Ross, K.,\nChristensen, H. I., and Su, H. Multi-Task Batch Rein-\nforcement Learning with Metric Learning. In Interna-\ntional Conference on Learning Representations, 2020.\nLi, L., Huang, Y., Chen, M., Luo, S., Luo, D., and Huang,\nJ. Provably Improved Context-Based Ofﬂine Meta-RL\nwith Attention and Contrastive Learning. arXiv preprint\narXiv:2102.10774, 2021a.\nLi, L., Yang, R., and Luo, D. FOCAL: Efﬁcient Fully-\nOfﬂine Meta-Reinforcement Learning Via Distance Met-\nric Learning and Behavior Regularization. In Interna-\ntional Conference on Learning Representations, 2021b.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T.,\nTassa, Y., Silver, D., and Wierstra, D. Continuous Control\nwith Deep Reinforcement Learning. In International\nConference on Learning Representations, 2016.\nLiu, E. Z., Raghunathan, A., Liang, P., and Finn, C. Ex-\nplore Then Execute: Adapting Without Rewards Via Fac-\ntorized Meta-Reinforcement Learning. arXiv preprint\narXiv:2008.02790, 2020a.\nLiu, Y., Yi, L., Zhang, S., Fan, Q., Funkhouser, T., and\nDong, H. P4Contrast: Contrastive Learning with Pairs of\nPoint-Pixel Pairs for RGB-D Scene Understanding. arXiv\npreprint arXiv:2012.13089, 2020b.\nMaaten, L. v. d. and Hinton, G. Visualizing Data Using\nT-SNE. Journal of Machine Learning Research, 9(11):\n2579–2605, 2008.\nMitchell, E., Rafailov, R., Peng, X. B., Levine, S., and Finn,\nC. Ofﬂine Meta-Reinforcement Learning with Advan-\ntage Weighting. In International Conference on Machine\nLearning, 2021.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\nAtari with Deep Reinforcement Learning. arXiv preprint\narXiv:1312.5602, 2013.\nNguyen, H. and La, H. Review of Deep Reinforcement\nLearning for Robot Manipulation. In IEEE International\nConference on Robotic Computing, 2019.\nOord, A. v. d., Li, Y., and Vinyals, O. Representation Learn-\ning with Contrastive Predictive Coding. arXiv preprint\narXiv:1807.03748, 2018.\nOroojlooyJadid, A. and Hajinezhad, D. A Review of Coop-\nerative Multi-Agent Deep Reinforcement Learning. arXiv\npreprint arXiv:1908.03963, 2019.\nParisotto, E., Ghosh, S., Yalamanchi, S. B., Chinnaobireddy,\nV., Wu, Y., and Salakhutdinov, R. Concurrent Meta Rein-\nforcement Learning. arXiv preprint arXiv:1903.02710,\n2019.\nPatrick, M., Asano, Y. M., Kuznetsova, P., Fong, R., Hen-\nriques, J. F., Zweig, G., and Vedaldi, A. Multi-Modal\nSelf-Supervision From Generalized Data Transforma-\ntions. arXiv preprint arXiv:2003.04298, 2020.\nPeng, X. B., Kumar, A., Zhang, G., and Levine, S.\nAdvantage-Weighted Regression: Simple and Scalable\nOff-Policy Reinforcement Learning.\narXiv preprint\narXiv:1910.00177, 2019.\nPong, V. H., Nair, A., Smith, L., Huang, C., and Levine, S.\nOfﬂine Meta-Reinforcement Learning with Online Self-\nSupervision. arXiv preprint arXiv:2107.03974, 2021.\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\nRakelly, K., Zhou, A., Quillen, D., Finn, C., and Levine, S.\nEfﬁcient Off-Policy Meta-Reinforcement Learning Via\nProbabilistic Context Variables. In International Confer-\nence on Machine Learning, 2019.\nRothfuss, J., Lee, D., Clavera, I., Asfour, T., and Abbeel, P.\nProMP: Proximal Meta-Policy Search. In International\nConference on Learning Representations, 2019.\nSermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E.,\nSchaal, S., Levine, S., and Brain, G. Time-Contrastive\nNetworks: Self-Supervised Learning From Video. In\nIEEE International Conference on Robotics and Automa-\ntion, 2018.\nSohn, K., Lee, H., and Yan, X. Learning Structured Out-\nput Representation using Deep Conditional Generative\nModels. In Advances in neural information processing\nsystems, 2015.\nStadie, B. C., Yang, G., Houthooft, R., Chen, X., Duan, Y.,\nWu, Y., Abbeel, P., and Sutskever, I. Some Considera-\ntions on Learning To Explore Via Meta-Reinforcement\nLearning. arXiv preprint arXiv:1803.01118, 2018.\nStooke, A., Lee, K., Abbeel, P., and Laskin, M. Decoupling\nRepresentation Learning From Reinforcement Learning.\nIn International Conference on Machine Learning, 2021.\nTian, Y., Krishnan, D., and Isola, P. Contrastive Multiview\nCoding. In European Conference on Computer Vision,\n2020.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention\nis All You Need. In Advances in neural information\nprocessing systems, 2017.\nWang, T. and Isola, P. Understanding Contrastive Repre-\nsentation Learning Through Alignment and Uniformity\non The Hypersphere. In International Conference on\nMachine Learning, 2020.\nWang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M.,\nand Freitas, N. Dueling Network Architectures for Deep\nReinforcement Learning. In International Conference on\nMachine Learning, 2016.\nWu, Y., Tucker, G., and Nachum, O. Behavior Regular-\nized Ofﬂine Reinforcement Learning. arXiv preprint\narXiv:1911.11361, 2019.\nZhang, J., Wang, J., Hu, H., Chen, Y., Fan, C., and Zhang,\nC. Learn to effectively explore in context-based meta-rl.\narXiv preprint arXiv:2006.08170, 2020.\nZheng, G., Zhang, F., Zheng, Z., Xiang, Y., Yuan, N. J., Xie,\nX., and Li, Z. DRN: A Deep Reinforcement Learning\nFramework for News Recommendation. In World Wide\nWeb Conference, 2018.\nZintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hof-\nmann, K., and Whiteson, S. VariBAD: A Very Good\nMethod for Bayes-Adaptive Deep RL Via Meta-Learning.\nIn International Conference on Learning Representations,\n2020.\nŁukasz Kaiser, Babaeizadeh, M., Miłos, P., Osi´nski, B.,\nCampbell, R. H., Czechowski, K., Erhan, D., Finn, C.,\nKozakowski, P., Levine, S., Mohiuddin, A., Sepassi, R.,\nTucker, G., and Michalewski, H. Model-Based Reinforce-\nment Learning for Atari. In International Conference on\nLearning Representations, 2020.\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\nA. Contrastive Learning Objective\nIn this section, we ﬁrst introduce a lemma. Then we give a proof of the main theorem in the text.\nLemma A.1. Given M ∼P(M) with reward R and transition T, (s, a) follows some distribution, x = (s, a, r, s′), r =\nR(s, a), s′ ∼T(s′|s, a), z ∼P(z|x), then we have\nP(M|z)\nP(M) = Ex\n\u0014P(z|x)\nP(z)\n\u0015\n.\nProof.\nP(M|z)\nP(M) = P(z|M)\nP(z)\n=\nZ\nx\nP(x|M)P(z|x)\nP(z)\ndx\n= Ex\n\u0014P(z|x)\nP(z)\n\u0015\nTheorem A.2. Let M be a set of tasks following the task distribution, |M| = N. M ∈M is the ﬁrst task with reward\nand transition R, T. Let x = (s, a, r, s′), z ∼P(z|x), h(x, z) = P (z|x)\nP (z) , where (s, a) follows an arbitrary distribution,\nr = R(s, a), s′ ∼T(s′|s, a). For any task M ∗∈M with reward and transition R∗, T ∗, denote x∗= (s, a, r∗, s∗′) as a\ntransition tuple generated in M ∗conditioned on (s, a), where r∗= R∗(s, a), s∗′ ∼T ∗(s′|s, a). Then we have\nI(z; M) −log(N) ≥EM,x,z\n\u0014\nlog(\nh(x, z)\nP\nM ∗∈M h(x∗, z))\n\u0015\n.\nProof. Using Lemma A.1 and Jensen’s inequality, we have\nI(z; M) −log(N) = Ez,Mlog\n\u0014p(M|z)\np(M)\n\u0015\n−log N\n= Ez,M\n\u0014\nlog Ex\n\u0014p(z|x)\np(z)\n\u0015\u0015\n−log N\n≥Ez,MEx\n\nlog\n\n\n1\np(z)\np(z|x)N\n\n\n\n\n≥Ez,MEx\n\nlog\n\n\n1\n1 +\np(z)\np(z|x)(N −1)\n\n\n\n\n= Ez,MEx\n\u0014\n−log\n\u0012\n1 + p(z)\np(z|x)(N −1)\n\u0013\u0015\n= Ez,MEx\n\u0014\n−log\n\u0012\n1 + p(z)\np(z|x)(N −1)EM ∗∈M\\{M}\n\u0014p(z|x∗)\np(z)\n\u0015\u0013\u0015\n= Ez,MEx\n\n−log\n\n1 + p(z)\np(z|x)\nX\nM ∗∈M\\{M}\np(z|x∗)\np(z)\n\n\n\n\n= EM,x,z\n\nlog\n\n\np(z|x)\np(z)\np(z|x)\np(z) + P\nM ∗∈M\\{M}\np(z|x∗)\np(z)\n\n\n\n\n= EM,x,z\n\u0014\nlog(\nh(x, z)\nP\nM ∗∈M h(x∗, z))\n\u0015\nRobust Task Representations for Ofﬂine Meta-Reinforcement Learning via Contrastive Learning\nB. Environment Details\nIn this section, we present the details of the environments in our experiments.\nPoint-Robot: The start position is ﬁxed at (0, 0) and the goal location g is sampled from U[−1, 1] × U[−1, 1]. The reward\nfunction is deﬁned as rt = −∥st −g∥2, where st is the current position. The maximal episode steps is set to 20.\nAnt-Dir: The goal direction is sampled from θ ∼U[0, 2π]. The reward function is rt = vx cos θ + vy sin θ, where (vx, vy)\nis the horizontal velocity of the ant. The maximal episode steps is set to 200.\nHalf-Cheetah-Vel: The goal velocity is sampled from vg ∼U[0, 3]. The reward function is rt = −|vt −vg| −1\n2∥at∥2\n2,\nwhere vt is the current forward velocity of the agent and at is the action. The maximal episode steps is set to 200.\nWalker-Param: Transition dynamics are varied in body mass and frictions, described by 32 parameters. Each parameter\nis sampled by multiplying the default value with 1.5µ, µ ∼U[−3, 3]. The reward function is rt = vt −10−3 · ∥at∥2\n2 + 1,\nwhere vt is the current forward velocity of the walker and at is the action. The episode terminates when the height of the\nwalker is less than 0.5. The maximal episode steps is also set to 200.\nHopper-Param: Transition dynamics are varied in body mass, inertia, damping and frictions, described by 41 parameters.\nEach parameter is sampled by multiplying the default value with 1.5µ, µ ∼U[−3, 3]. The reward function is rt =\nvt −10−3 · ∥at∥2\n2, where vt is the current forward velocity of the hopper and at is the action. The maximal episode steps is\nset to 200.\nC. Experimental Details\nIn Table 4 and 5, we list the important conﬁgurations and hyperparameters in the data collection and meta training phases\nthat we used to produce the experimental results.\nTable 4. Conﬁgurations and hyperparameters used in dataset collection to produce all the experimental results.\nConﬁgutations\nPoint-Robot\nAnt-Dir\nHalf-Cheetah-Vel\nWalker-Param\nHopper-Param\nDataset size\n2100\n2e4\n2e5\n2e4\n6e4\nTraining steps\n1e3\n4e4\n1e5\n6e5\n2e5\nBatch size\n256\n256\n256\n256\n256\nNetwork width\n32\n128\n128\n128\n128\nNetwork depth\n3\n3\n3\n3\n3\nLearning rate\n3e-4\n3e-4\n3e-4\n3e-4\n3e-4\nTable 5. Conﬁgurations and hyperparameters used in ofﬂine meta training to produce all the experimental results.\nConﬁgutations\nPoint-Robot\nAnt-Dir\nHalf-Cheetah-Vel\nWalker-Param\nHopper-Param\nNegative pairs\nRandomize\nRandomize\nGenerative\nGenerative\nNone\np(ν)\nN(0, 0.5)\nN(0, 0.5)\n–\n–\n–\nLatent space dim\n5\n5\n5\n32\n40\nTask batch size\n16\n16\n16\n16\n16\nTraining steps\n2e5\n2e5\n2e5\n2e5\n2e5\nRL batch size\n256\n256\n256\n256\n256\nContrastive batch size\n64\n64\n64\n64\n64\nNegative pairs number\n16\n16\n16\n16\n16\nRL network width\n64\n256\n256\n256\n256\nRL network depth\n3\n3\n3\n3\n3\nEncoder width\n64\n64\n64\n128\n128\nLearning rate\n3e-4\n3e-4\n3e-4\n3e-4\n3e-4\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-06-21",
  "updated": "2022-06-21"
}