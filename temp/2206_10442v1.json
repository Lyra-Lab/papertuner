{
  "id": "http://arxiv.org/abs/2206.10442v1",
  "title": "Robust Task Representations for Offline Meta-Reinforcement Learning via Contrastive Learning",
  "authors": [
    "Haoqi Yuan",
    "Zongqing Lu"
  ],
  "abstract": "We study offline meta-reinforcement learning, a practical reinforcement\nlearning paradigm that learns from offline data to adapt to new tasks. The\ndistribution of offline data is determined jointly by the behavior policy and\nthe task. Existing offline meta-reinforcement learning algorithms cannot\ndistinguish these factors, making task representations unstable to the change\nof behavior policies. To address this problem, we propose a contrastive\nlearning framework for task representations that are robust to the distribution\nmismatch of behavior policies in training and test. We design a bi-level\nencoder structure, use mutual information maximization to formalize task\nrepresentation learning, derive a contrastive learning objective, and introduce\nseveral approaches to approximate the true distribution of negative pairs.\nExperiments on a variety of offline meta-reinforcement learning benchmarks\ndemonstrate the advantages of our method over prior methods, especially on the\ngeneralization to out-of-distribution behavior policies. The code is available\nat https://github.com/PKU-AI-Edge/CORRO.",
  "text": "Robust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning\nvia Contrastive Learning\nHaoqi Yuan 1 Zongqing Lu 1\nAbstract\nWe study ofï¬‚ine meta-reinforcement learning, a\npractical reinforcement learning paradigm that\nlearns from ofï¬‚ine data to adapt to new tasks. The\ndistribution of ofï¬‚ine data is determined jointly by\nthe behavior policy and the task. Existing ofï¬‚ine\nmeta-reinforcement learning algorithms cannot\ndistinguish these factors, making task representa-\ntions unstable to the change of behavior policies.\nTo address this problem, we propose a contrastive\nlearning framework for task representations that\nare robust to the distribution mismatch of behavior\npolicies in training and test. We design a bi-level\nencoder structure, use mutual information max-\nimization to formalize task representation learn-\ning, derive a contrastive learning objective, and\nintroduce several approaches to approximate the\ntrue distribution of negative pairs. Experiments\non a variety of ofï¬‚ine meta-reinforcement learn-\ning benchmarks demonstrate the advantages of\nour method over prior methods, especially on the\ngeneralization to out-of-distribution behavior poli-\ncies.\n1. Introduction\nDeep reinforcement learning (RL) has achieved great suc-\ncesses in playing video games (Mnih et al., 2013; Lample &\nChaplot, 2017), robotics (Nguyen & La, 2019), recommen-\ndation systems (Zheng et al., 2018) and multi-agent systems\n(OroojlooyJadid & Hajinezhad, 2019). However, deep RL\nstill faces two challenging problems: data efï¬ciency and\ngeneralization. To play Atari games, model-free RL takes\nmillions of steps of environment interactions, while model-\nbased RL takes 100k environment steps (Åukasz Kaiser\net al., 2020). The tremendous interactions along with safety\nissues prevent its applications in many real-world scenar-\n1School of Computer Science, Peking University. Correspon-\ndence to: Zongqing Lu <zongqing.lu@pku.edu.cn>.\nProceedings of the 39 th International Conference on Machine\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-\nright 2022 by the author(s).\nios. Ofï¬‚ine reinforcement learning (Levine et al., 2020)\ntackles the sample efï¬ciency problem by learning from pre-\ncollected ofï¬‚ine datasets, without any online interaction. On\nthe other hand, as deep RL is supposed to be tested in the\nsame environment to the training environment, when the\ntransition dynamics or the reward function changes, the per-\nformance degenerates. To tackle the generalization problem,\nmeta-reinforcement learning (Finn et al., 2017; Duan et al.,\n2016) introduces learning over task distribution to adapt to\nnew tasks.\nOfï¬‚ine Meta-Reinforcement Learning (OMRL), an under-\nstudied problem, lies in the intersection of ofï¬‚ine RL and\nmeta-RL. Usually, the ofï¬‚ine dataset is collected from mul-\ntiple tasks by different behavior policies. The training agent\naims at learning a meta-policy, which is able to efï¬ciently\nadapt to unseen tasks. Recent studies (Li et al., 2020; Dorf-\nman et al., 2020; Li et al., 2021a;b) extend context-based\nmeta-RL to OMRL. They propose to use a context encoder\nto learn task representations from the collected trajectories,\nthen use the latent codes as the policyâ€™s input.\nHowever, context-based methods are vulnerable to the dis-\ntribution mismatch of behavior policies in training and test\nphases. The distribution of collected trajectories depends\nboth on the behavior policy and the task. When behav-\nior policies are highly correlated with tasks in the training\ndataset, the context encoder is likely to memorize the feature\nof behavior policies. Thus, in the test phase, the context\nencoder produces biased task inference due to the change of\nthe behavior policy. Although Li et al. (2021b;a) employed\ncontrastive learning to improve task representations, their\nlearning objectives are simply based on discriminating tra-\njectories from different tasks, and thus cannot eliminate the\ninï¬‚uence of behavior policies.\nTo overcome this limitation, we propose a novel frame-\nwork of COntrastive Robust task Representation learning\nfor OMRL (CORRO). We design a bi-level structured task\nencoder, where the ï¬rst level extracts task representations\nfrom one-step transition tuples instead of trajectories and the\nsecond level aggregates the representations. We formalize\nthe learning objective as mutual information maximization\nbetween the representation and task, to maximally eliminate\nthe inï¬‚uence of behavior policies from task representations.\narXiv:2206.10442v1  [cs.LG]  21 Jun 2022\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\nWe introduce a contrastive learning method to optimize for\nInfoNCE, a mutual information lower bound. To approxi-\nmate the negative pairsâ€™ distribution, we introduce two ap-\nproaches for negative pairs generation, including generative\nmodeling and reward randomization. Experiments in Point-\nRobot environment and multi-task MuJoCo benchmarks\ndemonstrate the substantial performance gain of CORRO\nover prior context-based OMRL methods, especially when\nthe behavior policy for adaptation is out-of-distribution.\nOur main contributions, among others, are:\nâ€¢ We propose a framework for learning robust task repre-\nsentations with fully ofï¬‚ine datasets, which can distin-\nguish tasks from the distributions of transitions jointly\ndetermined by the behavior policy and task.\nâ€¢ We derive a contrastive learning objective to extract\nshared features in the transitions of the same task while\ncapturing the essential variance of reward functions\nand transition dynamics across tasks.\nâ€¢ We empirically show on a variety of benchmarks\nthat our method much better generalizes to out-of-\ndistribution behavior policies than prior methods, even\nbetter than supervised task learning that assumes the\nground-truth task descriptions.\n2. Related Work\nOfï¬‚ine Reinforcement Learning allows policy learning\nfrom data collected by arbitrary policies, increasing the sam-\nple efï¬ciency of RL. In off-policy RL (Mnih et al., 2013;\nWang et al., 2016; Haarnoja et al., 2018; Lillicrap et al.,\n2016; Peng et al., 2019), the policy reuses the data collected\nby its past versions, and learns based on Q-learning or im-\nportance sampling. Batch RL studies learning from fully\nofï¬‚ine data. Recent works (Lange et al., 2012; Fujimoto\net al., 2019b; Levine et al., 2020; Wu et al., 2019; Fujimoto\net al., 2019a; Kumar et al., 2019) propose methods to over-\ncome distributional shift and value overestimation issues\nin batch RL. Our study follows the batch RL setting, but\nfocuses on learning task representations and meta-policy\nfrom ofï¬‚ine multi-task data.\nMeta-Reinforcement Learning learns to quickly adapt to\nnew tasks via training on a task distribution. Context-based\nmethods (Duan et al., 2016; Zintgraf et al., 2020; Rakelly\net al., 2019; Fakoor et al., 2020; Parisotto et al., 2019) for-\nmalize meta-RL as POMDP, regard tasks as unobservable\nparts of states, and encode task information from history\ntrajectories. Optimization-based methods (Finn et al., 2017;\nRothfuss et al., 2019; Stadie et al., 2018; Al-Shedivat et al.,\n2018; Foerster et al., 2018; Houthooft et al., 2018) formalize\ntask adaptation as performing policy gradients over few-shot\nsamples and learn an optimal policy initialization. Our study\nis based on the framework of context-based meta-RL.\nOfï¬‚ine Meta-Reinforcement Learning studies learning to\nlearn from ofï¬‚ine data. Because there is a distributional mis-\nmatch between ofï¬‚ine data and online explored data during\ntest, learning robust task representations is an important\nissue. Recent works (Li et al., 2021b;a) apply contrastive\nlearning over trajectories for compact representations, but\nignore the inï¬‚uence of behavior policy mismatch. To ï¬x\nthe distributional mismatch, Li et al. (2020); Dorfman et al.\n(2020) assume known reward functions of different tasks,\nand Pong et al. (2021) require additional online exploration.\nUnlike them, we consider learning robust task representa-\ntions with fully ofï¬‚ine data.\nContrastive Learning (He et al., 2020; Grill et al., 2020;\nWang & Isola, 2020) is a popular method for self-supervised\nrepresentation learning. It constructs positive or negative\npairs as noisy versions of samples with the same or different\nsemantics. Via distinguishing the positive pair among a large\nbatch of pairs, it extracts meaningful features. Contrastive\nlearning has been widely applied in computer vision (Chen\net al., 2020; He et al., 2020; Patrick et al., 2020), multi-\nmodal learning (Sermanet et al., 2018; Tian et al., 2020;\nLiu et al., 2020b) and image-based reinforcement learning\n(Anand et al., 2019; Stooke et al., 2021; Laskin et al., 2020).\nIn our study, we apply contrastive learning in ofï¬‚ine task\nrepresentation learning, for robust OMRL.\n3. Preliminaries\n3.1. Problem Formulation\nA task for reinforcement learning is formalized as a fully\nobservable Markov Decision Process (MDP). MDP is mod-\neled as a tuple M = (S, A, T, Ï, R, Î³), where S is the\nstate space, A is the action space, T(sâ€²|s, a) is the transi-\ntion dynamics of the environment, Ï(s) is the initial state\ndistribution, R(s, a) is the reward function, Î³ âˆˆ[0, 1) is\nthe factor discounting the future reward. The policy of the\nagent is a distribution Ï€(a|s) over actions. Starting from\nthe initial state, for each time step, the agent performs an\naction sampled from Ï€, then the environment updates the\nstate with T and returns a reward with R. We denote the\nmarginal state distribution at time t as Âµt\nÏ€(s). The objec-\ntive of the agent is to maximize the expected cumulative\nrewards maxÏ€ JM(Ï€) = Estâˆ¼ÂµtÏ€,atâˆ¼Ï€[Pâˆ\nt=0 Î³tR(st, at)].\nTo evaluate and optimize the policy, we deï¬ne V-function\nand Q-function as follows:\nVÏ€(s) =\nâˆ\nX\nt=0\nÎ³tEstâˆ¼ÂµtÏ€,atâˆ¼Ï€ [R(st, at)]\n(1)\nQÏ€(s, a) = R(s, a) + Î³Esâ€²âˆ¼T (sâ€²|s,a)[VÏ€(sâ€²)].\n(2)\nQ-learning solves for the optimal policy by iterating the\nBellman optimality operator B over Q-function:\nB Ë†Q(s, a) = R(s, a)+Î³Esâ€²âˆ¼T (sâ€²|s,a)\nh\nmax\naâ€²\nË†Q(sâ€², aâ€²)\ni\n. (3)\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\nIn ofï¬‚ine meta-reinforcement learning (OMRL), we\nassume that the task follows a distribution Mi\n=\n(S, A, Ti, Ï, Ri, Î³)\nâˆ¼\nP(M).\nTasks share the same\nstate space and action space, but varies in reward func-\ntions and transition dynamics.\nThus, we can also de-\nnote the task distribution as P(R, T). Given N training\ntasks {Mi}N\ni=1, for each task i, an ofï¬‚ine dataset Xi =\n{(si,j, ai,j, ri,j, sâ€²\ni,j)}K\nj=1 is collected by arbitrary behavior\npolicy Ï€i\nÎ². The learning algorithms can only access the\nofï¬‚ine datasets to train a meta-policy Ï€meta, without any en-\nvironmental interactions. At test time, given an unseen task\nM âˆ¼P(M), an arbitrary exploration (behavior) policy col-\nlects a context c = {(sj, aj, rj, sâ€²\nj)}k\nj=1, then the learned\nagent performs task adaptation conditioned on c to get a\ntask-speciï¬c policy Ï€M and evaluate in the environment.\nThe objective for OMRL is to learn a meta policy maximize\nthe expected return over the test tasks:\nJ(Ï€meta) = EMâˆ¼P (M)[JM(Ï€M)] .\n(4)\n3.2. Context-Based OMRL and FOCAL\nContext-based learning regards OMRL as solving par-\ntially observable MDPs.\nConsidering the task M as\nthe unobservable part of the state, the agent gathers\ntask information and makes decisions upon the his-\ntory trajectory: at âˆ¼Ï€(a|Ï„0:tâˆ’1, st), where Ï„0:tâˆ’1 =\n(s0, a0, r0, Â· Â· Â· , stâˆ’1, atâˆ’1, rtâˆ’1).\nA major assumption in context-based learning is that differ-\nent tasks share some common structures and the variation\nover tasks can be described by a compact representation. For\nexample, in a robotic object manipulation scenario, tasks\ncan be described as object mass, friction, and target posi-\ntion, while rules of physical interaction are shared across\ntasks. Context-based OMRL uses a task encoder to learn a\nlatent space zt = E(Ï„0:tâˆ’1), to represent task information\n(Rakelly et al., 2019; Li et al., 2021b) or task uncertainty\n(Dorfman et al., 2020). The policy Ï€(a|s, z) is conditioned\non the latent task representation. Given a new task, the ex-\nploration policy collects a few trajectories (context), and the\ntask encoder adapts the policy by producing z. In principle,\nthe task encoder and the policy can be trained on ofï¬‚ine\ndatasets with off-policy or batch RL methods. In our setting,\nwe assume that the exploration policy is arbitrary, and the\ncontext is a single trajectory. During training, the context is\nsampled from the ofï¬‚ine dataset.\nFOCAL (Li et al., 2021b) improves task representation learn-\ning of the encoder via distance metric learning. The encoder\nminimizes the distance of trajectories from the same task\nand pushes away trajectories from different tasks in the\nlatent space. The loss function is\nLdml =1{yi = yj}âˆ¥qi âˆ’qjâˆ¥2\n2+\n1{yi Ì¸= yj}Î² Â·\n1\nâˆ¥qi âˆ’qjâˆ¥n\n2 + Ïµ\n(5)\nwhere qi = E(Ï„i) is the representation of a trajectory, yi is\nthe identiï¬er of the ofï¬‚ine dataset. The policy is separately\ntrained, conditioned on the learned encoder, with batch Q-\nlearning methods.\n3.3. Task Representation Problems in OMRL\nSince the ofï¬‚ine datasets are collected by different behavior\npolicies, to answer which dataset a trajectory belongs to,\none may infer based on the feature of behavior policy rather\nthan rewards and state transitions. As an example: In a 2D\ngoal reaching environment, tasks differ in goal positions. In\neach training task, the behavior policy is to go towards the\ngoal position. Training on such datasets, the task encoder in\nFOCAL can simply ignore the rewards and distinguish the\ntasks based on the state-action distribution in the context.\nThus, it will make mistakes when the context exploration\npolicy changes.\nSimilar problem is also mentioned in Dorfman et al. (2020);\nLi et al. (2020). To eliminate the effects of behavior policies\nin task learning, Dorfman et al. (2020) augment the datasets\nby collecting trajectories in different tasks with the same\npolicy, and relabeling the transition tuples with different\ntasksâ€™ reward functions. However, in our fully ofï¬‚ine set-\nting, collecting additional data and accessing the reward\nfunctions are not allowed. Li et al. (2020) use the learned\nreward functions in different tasks to relabel the trajectories\nand apply metric learning over trajectories. This approach\ndoes not support the settings where tasks differ in transition\ndynamics, because we cannot perform transition relabeling\nof sâ€² over the trajectory to mimic the trajectory in other tasks.\nAlso, the reward function learned with limited single-task\ndataset can be inaccurate to relabel the unseen state-action\npairs.\n4. Method\nTo address the ofï¬‚ine task representation problem, we pro-\npose CORRO, a novel contrastive learning framework for\nrobust task representations with fully ofï¬‚ine datasets, which\ndecreases the inï¬‚uence of behavior policies on task represen-\ntations while supporting tasks that differ in reward function\nand transition dynamics.\n4.1. Bi-Level Task Encoder\nWe ï¬rst design a bi-level task encoder where representation\nlearning focuses on transition tuples rather than trajectories.\nConcretely, our task encoder consists of a transition encoder\nEÎ¸1 and an aggregator EÎ¸2, parameterized collectively as\nÎ¸. Given a context c = {(si, ai, ri, sâ€²\ni)}k\ni=1, the transition\nencoder extracts latent representations for all the transition\ntuples zi = EÎ¸1(si, ai, ri, sâ€²\ni), then the aggregator gather all\nthe latent codes into a task representation z = EÎ¸2({zi}k\ni=1).\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\nContext\nContrastive Learning\nğ‘§ğ‘–ğ‘–=1\nğ‘˜\n{ğ‘§âˆ—}\nğ‘§\nOffline Meta-RL\nğ¸ğœƒ1\nğ¸ğœƒ2\nğ‘„ğœ“\nğœ‹ğœ™\nğ‘\nğ‘\nOffline\nDataset\nNeg. Pairs\nGenerator\nFigure 1. CORRO framework. The transition encoder EÎ¸1 extracts\nthe latent representations of each transition tuple in the context.\nContrastive learning is applied to robustify the task encoder. The\naggregator EÎ¸2 gathers all the latent codes, to condition the actor\nand critic.\nThe Q-function QÏˆ(s, a, z) and the policy Ï€Ï†(a|s, z) are\nconditioned on z, parameterized with Ï†, Ïˆ.\nCompared to trajectories, transition tuples leak less behav-\nior policy information and are suitable for both transition\nrelabeling and reward relabeling. Thus, we design robust\ntask representation learning methods to train only the tran-\nsition encoder in the following sections. The architecture\nof the transition encoder is simple MLP. The aggregator is\ntrained with Q-function and policy using ofï¬‚ine RL algo-\nrithms. Intuitively, it should draw attention to the transition\ntuples with rich task information. Inspired by self-attention\n(Vaswani et al., 2017), we introduce an aggregator which\ncomputes the weighted sum of the input vectors:\nz =\nk\nX\nj=1\nsoftmax\n\u0000{MLP(zi)}k\ni=1\n\u0001\nj Â· zj.\n(6)\nFigure 1 gives an overview of our framework.\n4.2. Contrastive Task Representation Learning\nAn ideally robust task representation should be dependent\non tasks and invariant across behavior policies. We intro-\nduce mutual information to measure the mutual dependency\nbetween the task representation and the task. As, intuitively,\nmutual information measures the uncertainty reduction of\none random variable when the other one is observed, we\npropose to maximize the mutual information between the\ntask representation and the task itself, so that the task en-\ncoder learns to maximally reduce task uncertainty while\nminimally preserving task-irrelevant information.\nMathematically, we formalize the transition encoder EÎ¸1 as\na probabilistic encoder z âˆ¼P(z|x), where x = (s, a, r, sâ€²)\ndenotes the transition tuple. Task M follows the task distri-\nbution P(M) and the distribution of x is determined jointly\nby M and the behavior policy. The learning objective for\nthe transition encoder is:\nmax I(z; M) = Ez,M\n\u0014\nlog p(M|z)\np(M)\n\u0015\n.\n(7)\nOptimizing mutual information is intractable in practice.\nInspired by noise contrastive estimation (InfoNCE) in the\nliterature of contrastive learning (Oord et al., 2018), we\nderive a lower bound of Eq. (7) and have the following\ntheorem.\nTheorem 4.1. Let M be a set of tasks following the task\ndistribution, |M| = N. M âˆˆM is the ï¬rst task with\nreward and transition R, T.\nLet x = (s, a, r, sâ€²), z âˆ¼\nP(z|x), h(x, z) = P (z|x)\nP (z) , where (s, a) follows an arbitrary\ndistribution, r = R(s, a), sâ€² âˆ¼T(sâ€²|s, a). For any task\nM âˆ—âˆˆM with reward and transition Râˆ—, T âˆ—, denote xâˆ—=\n(s, a, râˆ—, sâˆ—â€²) as a transition tuple generated in M âˆ—condi-\ntioned on (s, a), where râˆ—= Râˆ—(s, a), sâˆ—â€² âˆ¼T âˆ—(sâ€²|s, a).\nThen we have\nI(z; M) âˆ’log(N) â‰¥EM,x,z\n\u0014\nlog(\nh(x, z)\nP\nM âˆ—âˆˆM h(xâˆ—, z))\n\u0015\n.\nWe give a proof of the theorem in Appendix A. Following\nInfoNCE, we approximate h with the exponential of a score\nfunction S(zâˆ—, z), which is a similarity measure between the\nlatent codes of two samples. We derive a sampling version\nof the tractable lower bound to be the transition encoderâ€™s\nlearning objective\nmax\nÎ¸1\nX\nMiâˆˆM\nx,xâ€²âˆˆXi\n\u0014\nlog\n\u0012\nexp(S(z, zâ€²))\nP\nM âˆ—âˆˆM exp(S(z, zâˆ—))\n\u0013\u0015\n,\n(8)\nwhere M is the set of training tasks, x, xâ€² are two tran-\nsition tuples sampled from dataset Xi and z, zâ€² are latent\nrepresentations of x, xâ€². For tasks M âˆ—âˆˆM \\ {Mi}, zâˆ—\nis the representation of the transition tuple xâˆ—sampled in\ntask M âˆ—conditioned on the same state-action pair in x. For\nM âˆ—= Mi, we deï¬ne zâˆ—= zâ€² for consistent notation. We\nuse cosine similarity for the score function in practice.\nFollowing the literature, we name (x, xâ€²) a positive pair\nand name {(x, xâˆ—)}M âˆ—âˆˆM\\{M} negative pairs. Eq. (8)\noptimizes for a N-way classiï¬cation loss to classify the\npositive pair out of all the pairs. To maximize the score of\npositive pairs, the transition encoder should extract shared\nfeatures in the transitions of the same task. To decrease the\nscore of negative pairs, the transition encoder should capture\nthe essential variance of rewards and state transitions since\nstate-action pairs are the same across tasks.\n4.3. Negative Pairs Generation\nGenerating negative pairs in Eq. (8) involves computing\n(r, sâ€²) with tasksâ€™ reward functions and transition dynamics,\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\nwhich is impossible in the fully ofï¬‚ine setting. Li et al.\n(2020) introduce a reward relabeling method that ï¬ts a re-\nward model for each task using ofï¬‚ine datasets. In our\nsetting, we also need to ï¬t transition models with higher\ndimensions. However, the dataset for each task is usually\nsmall, making reward models and transition models overï¬t.\nWhen the overlap of state-action pairs between different\ntasks is small, relabeling with separately learned models can\nbe inaccurate, making contrastive learning ineffective.\nTo generate high-quality negative samples, we design meth-\nods based on two major principles.\nFidelity: To ensure the task encoder learning on real tran-\nsition tuples, the distribution of generated negative pairs\nshould approximate the true distribution in the denominator\nof Eq. (8). Concretely, given (s, a), the true distribution of\n(r, sâ€²) in negative pairs is\np(r, sâ€²|s, a) âˆEMâˆ¼P (M)[T(sâ€²|s, a)1{R(s, a) = r}] (9)\nDiversity: According to prior works (Patrick et al., 2020;\nChen et al., 2020), the performance of contrastive learning\ndepends on the diversity of negative pairs. More diversity\nincreases the difï¬culty in optimizing InfoNCE and helps\nthe encoder to learn meaningful representations. In image\ndomains, large datasets, large negative batch sizes, and data\naugmentations are used to increase diversity. We adopt this\nprinciple in generating negative pairs of transitions.\nWe propose two approaches for negative pairs generation.\nGenerative Modeling: We observe that, though a state-\naction pair may not appear in all the datasets, it usually\nappears in a subset of training datasets. Fitting reward\nand transition models with larger datasets across tasks can\nreduce the prediction error. Thus, we propose to train a\ngenerative model over the union of training datasets âˆªN\ni=1Xi,\nto approximate the ofï¬‚ine data distribution P{Xi}(r, sâ€²|s, a).\nWhen the distributions of (s, a) are similar across tasks, this\ndistribution approximates the distribution in Eq. (9).\nWe adopt conditional VAE (CVAE) (Sohn et al., 2015)\nfor generative modeling. CVAE consists of a generator\npÎ¾(r, sâ€²|s, a, z) and a probabilistic encoder qÏ‰(z|s, a, r, sâ€²).\nThe latent vector z describes the uncertain factors for pre-\ndiction, follows a prior Gaussian distribution p(z). CVAE\nis trained to minimize the loss function\nLCVAE = âˆ’E(s,a,r,sâ€²)âˆˆ{Xi}\nh\nEqÏ‰(z|s,a,r,sâ€²)\n[log pÎ¾(r, sâ€²|s, a, z)] âˆ’KL[qÏ‰(z|s, a, r, sâ€²)âˆ¥p(z)]\ni\n,\n(10)\nwhere KL(Â·âˆ¥Â·) is KL-divergence, working as an information\nbottleneck to minimize the information in z.\nReward Randomization: In the cases where the overlap of\nstate-action pairs between tasks is small, CVAE can collapse\nAlgorithm 1. Meta Training\nInput: Datasets {Xi}N\ni=1; OMRL models EÎ¸1, EÎ¸2, QÏˆ,\nÏ€Ï†\nA. If use generative modeling, pre-train CVAE:\nInitialize CVAE qÏ‰, pÎ¾\nrepeat\nUpdate Ï‰, Î¾ to minimize Eq. (10).\nuntil Done\nB. Train the transition encoder:\nrepeat\nSample a task M and two transition tuples x, xâ€²\nz = EÎ¸1(x), zâ€² = EÎ¸1(xâ€²)\nfor M âˆ—âˆˆM do\nif use generative modeling then\nSample xâˆ—from CVAE\nelse if use reward randomization then\nAdd noise to the reward to get xâˆ—\nend if\nzâˆ—= EÎ¸1(xâˆ—)\nend for\nCompute Eq. (8)\nUpdate Î¸1 to maximize Eq. (8)\nuntil Done\nC. Train the policy:\nrepeat\nSample a task dataset X and a context c\nz = EÎ¸2(EÎ¸1(c))\nAugment the states in X with z\nUpdate Î¸2, Ïˆ, Ï† with ofï¬‚ine RL algorithms on X\nuntil Done\nalmost to a deterministic prediction model and the diversity\nin negative pairs decreases. When tasks only differ in re-\nward functions, similar to data augmentation strategies in\nimage domains, we can generate negative pairs via adding\nrandom noise to the reward. We have râˆ—= r + Î½, where\nthe perturbation Î½ follows a noise distribution p(Î½). Though\nreward randomization cannot approximate the true distribu-\ntion of negative pairs, it provides an inï¬nitely large space\nto generate diverse rewards, making contrastive learning\nrobust.\n4.4. Algorithm Summary\nWe summarize our training method in Algorithm 1. The test\nprocess is shown in Algorithm 2. The code for our work is\navailable at https://github.com/PKU-AI-Edge/CORRO.\n5. Experiments\nIn experiments, we aim to demonstrate: (1) The perfor-\nmance of CORRO on tasks adaptation in diverse task distri-\nbutions and ofï¬‚ine datasets; (2) The robustness of CORRO\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\n0\n50000\n100000\n150000\n200000\nTrainingÂ Steps\n8.5\n8.0\n7.5\n7.0\n6.5\n6.0\n5.5\n5.0\nAverageÂ Return\nPointÂ­Robot\nCORRO\nFOCAL\nOfflineÂ PEARL\nSupervised\n0\n50000\n100000\n150000\n200000\nTraining Steps\n0\n25\n50\n75\n100\n125\n150\n175\nAverage Return\nAnt-Dir\nCORRO\nFOCAL\nOffline PEARL\nSupervised\n0\n50000\n100000\n150000\n200000\nTraining Steps\n300\n250\n200\n150\n100\n50\nAverage Return\nHalf-Cheetah-Vel\nCORRO\nFOCAL\nOffline PEARL\nSupervised\nFigure 2. Test returns of CORRO against the baselines in the environments with different reward functions: Point-Robot, Ant-Dir, and\nHalf-Cheetah-Vel. The shaded region shows standard deviation across 5 seeds.\nAlgorithm 2. Meta Test\nInput: Trained models EÎ¸1, EÎ¸2, QÏˆ, Ï€Ï†\nSample a task M\nCollect a context trajectory c with an arbitrary policy\nz = EÎ¸2(EÎ¸1(c))\nrepeat\nObserve s, execute a âˆ¼Ï€Ï†(a|s, z), get r\nuntil Environment terminates\non task inference; and (3) How to choose the strategy of\nnegative pairs generation.\n5.1. Experimental Settings\nWe adopt a simple 2D environment and multi-task MuJoCo\nbenchmarks to evaluate our method.\nPoint-Robot is a 2D navigation environment introduced in\nRakelly et al. (2019). Starting from the initial point, the\nagent should navigate to the goal location. Tasks differ in\nreward functions, which describe the goal position. The\ngoal positions are uniformly distributed in a square.\nHalf-Cheetah-Vel, Ant-Dir are multi-task MuJoCo bench-\nmarks where tasks differ in reward functions.\nIn Half-\nCheetah-Vel, the task is speciï¬ed by the target velocity of\nthe agent. The distribution of target velocity is U[0, vmax].\nIn Ant-Dir, the task is speciï¬ed by the goal direction of\nthe agentâ€™s motion. The distribution of goal direction is\nU[0, 2Ï€]. These benchmarks are also used in Mitchell et al.\n(2021); Li et al. (2021b); Zintgraf et al. (2020).\nWalker-Param, Hopper-Param are multi-task MuJoCo\nbenchmarks where tasks differ in transition dynamics. For\neach task, the physical parameters of body mass, inertia,\ndamping, and friction are randomized. The agent should\nadapt to the varying environment dynamics to accomplish\nthe task. Previous works (Mitchell et al., 2021; Rakelly\net al., 2019) also adopt these benchmarks.\n0\n100000\n200000\nTraining Steps\n100\n0\n100\n200\n300\nAverage Return\nWalker-Param\nCORRO\nFOCAL\nOffline PEARL\nSupervised\n0\n100000\n200000\nTraining Steps\n50\n0\n50\n100\n150\n200\n250\n300\nAverage Return\nHopper-Param\nCORRO\nFOCAL\nOffline PEARL\nSupervised\nFigure 3. Test returns of CORRO against the baselines in the envi-\nronments with different transition dynamics: Walker-Param and\nHopper-Param. The shaded region shows standard deviation across\n5 seeds.\nFor each environment, 20 training tasks and 20 testing tasks\nare sampled from the task distribution. On each task, we\nuse SAC (Haarnoja et al., 2018) to train a single-task pol-\nicy independently. The replay buffers are collected to be\nthe ofï¬‚ine datasets. During training, the context is a con-\ntiguous section of 200 transition tuples sampled from the\nofï¬‚ine dataset of the corresponding task. More details about\nexperimental settings are available in Appendix B.\nTo evaluate the performance of our method, we introduce\nseveral representative context-based fully-ofï¬‚ine methods\nto compare with.\nOfï¬‚ine PEARL is a direct ofï¬‚ine extension of PEARL\n(Rakelly et al., 2019). The context encoder is a permutation\ninvariant MLP, updated together with policy and Q-function\nusing ofï¬‚ine RL algorithms. No additional terms are used\nfor task representation learning. This baseline is introduced\nin (Li et al., 2020; Anonymous, 2022; Li et al., 2021b).\nFOCAL (Li et al., 2021b) uses metric learning to train the\ncontext encoder. Positive and negative pairs are trajectories\nsampled from the same and the different tasks, respectively.\nNote that while the original FOCAL uses an ofï¬‚ine RL\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\nTable 1. Average test returns of CORRO against baselines with different types of context exploration policy. IID means the context\ndistribution is the same to the training dataset, while OOD means the context is out-of-distribution.\nEnvironment\nSupervised.\nOfï¬‚ine PEARL\nFOCAL\nCORRO\nIID\nOOD\nIID\nOOD\nIID\nOOD\nIID\nOOD\nPoint-Robot\n-4.89Â±0.10\n-5.84Â±0.14\n-5.4Â±0.17\n-6.74Â±0.19\n-6.06Â±0.42\n-7.34Â±0.20\n-5.19Â±0.05\n-6.39Â±0.05\nAnt-Dir\n136Â±17.6\n131.7Â±11.4\n155.4Â±24.4\n141.5Â±11.3\n109.8Â±12.8\n53.5Â±16.4\n156.8Â±35.2\n154.7Â±25.8\nHalf-Cheetah-Vel\n-31.6Â±0.7\n-32.1Â±0.9\n-31.2Â±0.5\n-242.7Â±6.0\n-38.0Â±4.0\n-204.1Â±9.5\n-33.7Â±1.1\n-89.7Â±7.4\nWalker-Param\n232.7Â±29.2\n221.2Â±43.4\n259.1Â±48.2\n254.7Â±35.8\n225.4Â±56.4\n193.3Â±151.5\n301.5Â±37.9\n284.0Â±19.3\nHopper-Param\n269.2Â±20.3\n251.9Â±28.8\n244.0Â±18.5\n236.6Â±18.5\n195.6Â±62.3\n199.7Â±51.9\n267.6Â±25.6\n268.0Â±13.8\nmethod BRAC (Wu et al., 2019), we implement FOCAL\nwith SAC to better study the task representation learning\nproblem.\nSupervised Task Learning assumes that the ground truth\ntask descriptions can be accessed during training. Based on\nPEARL, we add L2 loss to supervise the context encoder.\nWe introduce this supervised learning method as a strong\nbaseline. Note that the ground truth task labels are usually\nunavailable in meta-RL settings.\nFor fairness, we replace the policy learning methods of\nbaselines and CORRO with SAC. The hyperparameters for\npolicy learning are ï¬xed across different methods. All the\nexperiments are conducted over 5 different random seeds.\nAll hyperparameters are available in Appendix C.\n5.2. Tasks Adaptation Performance\nTo evaluate the performance on tasks adaptation, we use\ntasks in the test set, sample one trajectory in the pretrained\nreplay buffer of each task to be the context, then test the\nagent in the environment conditioned on the context. The\nperformance is measured by the average return over all the\ntest tasks. In this experiment, there is no distributional\nshift of contexts between training and test, since they are\ngenerated by the single-task policies learned with the same\nalgorithm.\nFigure 2 shows the test results in the environments with\ndifferent reward functions. In Point-Robot, CORRO and the\nsupervised baseline outperform others. In Ant-Dir, CORRO\noutperforms all the baselines including the supervised learn-\ning method. Although the value of return in Point-Robot\nvaries a little, we have to mention that the return in this en-\nvironment is insensitive to the task performance: the mean\nreturn of an optimal policy is around -5, while the mean\nreturn of a random policy is around -9. Beneï¬ted from the\npretrained task encoder, CORRO shows great learning efï¬-\nciency, and reaches high test returns after 20k steps of ofï¬‚ine\ntraining. Though in Half-Cheetah-Vel we cannot distinguish\nthe performance of different methods, we will demonstrate\ntheir difference when context distribution shifts exist in Sec-\ntion 5.3 and the difference on the learned representations in\nSection 5.4.\nFigure 3 demonstrates the results in the environments with\nvarying transition dynamics. In both Walker-Param and\nHopper-Param, CORRO outperforms the baselines on task\nadaptation returns. Further, CORRO is more stable on policy\nimprovement, while the performance of FOCAL sometimes\ndegenerates during the ofï¬‚ine training.\n5.3. Robust Task Inference\nA robust task encoder should capture a compact represen-\ntation of rewards and transition dynamics from the con-\ntext, eliminating the information of the behavior policy.\nConditioned on the context collected by an arbitrary be-\nhavior policy, the agent performs accurate task adaptation\nand reaches high returns. To measure the robustness, we\ndesign experiments of out-of-distribution (OOD) test. De-\nnote the saved checkpoints of data collection policies for\nall the tasks during different training periods as {Ï€i,t\nÎ² }i,t,\nwhere i is the task and t is the training epoch. For each\ntest task M test\nj , we sample Ï€i,t\nÎ² as the behavior policy to\ncollect a context rollout, and test the agent conditioned on\nthe context. Because Ï€i,t\nÎ²\nis a policy trained on an arbi-\ntrary task, the context distribution is never seen during meta\ntraining. Then, we sample S behavior policies, measure the\nOOD test performance as\n1\nNS\nPS\nk=1\nPN\nj=1 J(M test\nj\n, Ï€ik,tk\nÎ²\n),\nwhere J(M, Ï€) means the average return in task M with\ncontext collection policy Ï€. Accordingly, we denote the\nstandard in-distribution test in Section 5.2 as IID test.\nTable 1 summarizes the IID and OOD test results of different\nmethods. All the test models are from the last training epoch.\nWhile the performance of all the methods degenerates from\nIID to OOD, CORRO outperforms Ofï¬‚ine PEARL and FO-\nCAL in OOD test by a large margin. In Half-Cheetah-Vel,\nOfï¬‚ine PEARL and FOCAL almost fail in task inference\nwith distributional shift. The slight performance degenera-\ntion shows the robustness of CORRO. Note that although\nthe supervised task learning method does not degenerate in\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\nCORRO\nOffline PEARL\nFOCAL\nSupervised.\nFigure 4. The 2D projection of the learned task representation\nspace in Half-Cheetah-Vel. Points are uniformly sampled from\ntest tasks. Tasks of velocities from 0 to 3 are mapped to rainbow\ncolors, from red to purple.\nHalf-Cheetah-Vel, it uses the true task labels to learn the\nrelation between contexts and tasks, which is not allowed in\ncommon meta-RL settings.\n5.4. Latent Space Visualization\nTo qualitatively analyze the learned task representation\nspace, we visualize the task representations by projecting\nthe embedding vectors into 2D space via t-SNE (Maaten &\nHinton, 2008). For each test task, we sample 200 transition\ntuples in the replay buffer to visualize. As shown in Figure\n4, compared to the baselines, CORRO distinguishes transi-\ntions from different tasks better. CORRO represents the task\nspace of Half-Cheetah-Vel into a 1D manifold, where the\ndistance between latent vectors is correlated to the differ-\nence between their target velocities, as shown in the color\nof the points.\n5.5. Ablation: Negative Pairs Generation\nNegative pairs generation is a key component of CORRO.\nGenerating more diverse negative samples can make con-\ntrastive learning robust and efï¬cient, while the distribution\nof negative pairs should also be close to the true distribu-\ntion as described in Section 4.2. Along with our design\nof generative modeling and reward randomization, we also\nintroduce Relabeling and None method. Relabeling is to\nseparately learn a reward model and a transition model on\neach ofï¬‚ine training dataset. The negative pair is generated\nby ï¬rst sampling a model, then relabeling (r, sâ€²) in the tran-\nTable 2. Ablation study on negative pairs generation methods in\nHalf-Cheetah-Vel environment (the upper four rows) and Point-\nRobot environment (the lower four rows).\nMethod\nContrastive Loss\nIID Return\nOOD Return\nGenerative\n0.07\n-33.7Â±1.1\n-89.7Â±7.4\nRandomize\n0.83\n-34.3Â±1.5\n-84.5Â±1.3\nRelabeling\n0.04\n-40.8Â±1.5\n-245.3Â±12.9\nNone\n1.20\n-34.1Â±2.4\n-97.6Â±3.1\nGenerative\n2.83\n-9.41Â±0.42\n-9.42Â±0.42\nRandomize\n0.54\n-5.19Â±0.05\n-6.39Â±0.05\nRelabeling\n0.04\n-9.22Â±0.24\n-9.27Â±0.22\nNone\n1.46\n-5.24Â±0.27\n-6.52Â±0.08\nsition tuple. None is a straightforward method where we\nsimply construct negative pairs with transition tuples from\ndifferent tasks and (s, a) in the transitions can be different.\nAs shown in Table 2, in Half-Cheetah-Vel, our methods\nusing generative modeling and reward randomization out-\nperform None and Relabeling in OOD test, due to better\napproximating the distribution of negative pairs.\nIn Half-Cheetah-Vel, we notice that the None method\nachieves the average return of -97.6 in OOD test and out-\nperforms FOCAL, of which the average return is -204.1\naccording to Table 1. They have two main differences: 1.\nsamples for representation learning are transition tuples in\nâ€˜Noneâ€™, but are trajectories in FOCAL; 2. the contrastive\nloss. To ï¬nd which affects the OOD generalization more, we\nre-implement FOCAL with our InfoNCE loss. It achieves\nan average return of -41.9 in the IID test and -208.2 in the\nOOD test, which is close to FOCALâ€™s performance. Thus,\nwe argue that focusing on transition tuples rather than whole\ntrajectories contributes to OOD generalization. Our ap-\nproach naturally discourages the task encoder to extract the\nfeature of behavior policy from the trajectory sequence.\nWhile generative modeling and reward randomization has\nsimilar performance in Half-Cheetah-Vel, the latter solves\nPoint-Robot better. Because in Point-Robot, behavior poli-\ncies in different tasks move in different directions, making\nstate-action distributions non-overlapping. CVAE collapses\ninto predicting the transition of the speciï¬c task where the\nstate-action pair appears, and cannot provide diverse nega-\ntive pairs to support contrastive learning.\nThe column of contrastive loss shows that the two proposed\nmethods generate diverse and effective negative pairs in the\nright circumstances, improving the learning of task encoder.\nThough the relabeling method has the best sample diversity,\nit underperforms other methods in task adaptation due to\nthe inaccurate predictions on unseen data.\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\nTable 3. Average test returns of CORRO against baselines with a random exploration policy for context collection.\nEnvironment\nSupervised.\nOfï¬‚ine PEARL\nFOCAL\nCORRO\nPoint-Robot\n-5.32Â±0.20\n-7.06Â±0.99\n-8.64Â±0.26\n-5.59Â±0.57\nAnt-Dir\n149.8Â±20.5\n148.4Â±35.3\n89.8Â±8.7\n163.0Â±35.8\nHalf-Cheetah-Vel\n-37.6Â±0.8\n-35.4Â±1.8\n-41.6Â±3.3\n-42.9Â±0.7\nWalker-Param\n221.7Â±91.1\n276.6Â±37.7\n245.6Â±67.8\n300.5Â±34.2\nHopper-Param\n253.5Â±21.2\n245.9Â±18.9\n203.6Â±46.6\n273.3Â±3.9\n5.6. Adaptation with an Exploration Policy\nContext exploration is a crucial part of meta-RL. Existing\nworks either explore with the meta policy before adaptation\n(Zintgraf et al., 2020; Finn et al., 2017), or learn a separate\nexploration policy (Liu et al., 2020a; Zhang et al., 2020).\nTo test the task adaptation performance of CORRO with an\nexploration policy, we adopt a uniformly random policy to\ncollect the context. As shown in Table 3, CORRO and the\nsupervised baseline outperform others in Point-Robot. In\nHalf-Cheetah-Vel, all the methods have great adaptation per-\nformance compared with the results in Table 1. In the other\nthree environments, CORRO outperforms all the baselines.\n6. Conclusion\nIn this paper, we address the task representation learning\nproblem in the fully ofï¬‚ine setting of meta-RL. We propose\na contrastive learning framework over transition tuples and\nseveral methods for negative pairs generation. In experi-\nments, we demonstrate how our method outperforms prior\nmethods in diverse environments and task distributions. We\nintroduce OOD test to quantify the superior robustness of\nour method and ablate the choice of negative pairs gener-\nation. Though, our study has not considered to learn an\nexploration policy for context collection, and to perform\nmoderate environmental interactions when training data is\nextremely limited. We leave these directions as future work.\nAcknowledgements\nWe would like to thank the anonymous reviewers for their\nuseful comments to improve our work. This work is sup-\nported in part by NSF China under grant 61872009.\nReferences\nAl-Shedivat, M., Bansal, T., Burda, Y., Sutskever, I., Mor-\ndatch, I., and Abbeel, P. Continuous Adaptation Via\nMeta-Learning in Nonstationary and Competitive En-\nvironments. In International Conference on Learning\nRepresentations, 2018.\nAnand, A., Racah, E., Ozair, S., Bengio, Y., CË†otÂ´e, M.-A.,\nand Hjelm, R. D. Unsupervised State Representation\nLearning in Atari. In Advances in neural information\nprocessing systems, 2019.\nAnonymous. Model-Based Ofï¬‚ine Meta-Reinforcement\nLearning with Regularization. In Submitted to The Tenth\nInternational Conference on Learning Representations,\n2022. under review.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nSimple Framework for Contrastive Learning of Visual\nRepresentations. In International Conference on Machine\nLearning, 2020.\nDorfman,\nR.,\nShenfeld,\nI.,\nand Tamar,\nA.\nOf-\nï¬‚ine Meta Learning of Exploration.\narXiv preprint\narXiv:2008.02598, 2020.\nDuan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever,\nI., and Abbeel, P.\nRL2: Fast Reinforcement Learn-\ning Via Slow Reinforcement Learning. arXiv preprint\narXiv:1611.02779, 2016.\nFakoor, R., Chaudhari, P., Soatto, S., and Smola, A. J. Meta-\nQ-Learning. In International Conference on Learning\nRepresentations, 2020.\nFinn, C., Abbeel, P., and Levine, S. Model-Agnostic Meta-\nLearning for Fast Adaptation of Deep Networks. In Inter-\nnational Conference on Machine Learning, 2017.\nFoerster, J., Farquhar, G., Al-Shedivat, M., RocktÂ¨aschel,\nT., Xing, E., and Whiteson, S. DiCE: The Inï¬nitely\nDifferentiable Monte Carlo Estimator. In International\nConference on Machine Learning, 2018.\nFujimoto, S., Conti, E., Ghavamzadeh, M., and Pineau,\nJ. Benchmarking Batch Deep Reinforcement Learning\nAlgorithms. arXiv preprint arXiv:1910.01708, 2019a.\nFujimoto, S., Meger, D., and Precup, D. Off-Policy Deep\nReinforcement Learning Without Exploration. In Inter-\nnational Conference on Machine Learning, 2019b.\nGrill, J.-B., Strub, F., AltchÂ´e, F., Tallec, C., Richemond,\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\nZ. D., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos,\nR., and Michal, V. Bootstrap Your Own Latent: A New\nApproach To Self-Supervised Learning. In Advances in\nneural information processing systems, 2020.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft\nActor-Critic: Off-Policy Maximum Entropy Deep Rein-\nforcement Learning with A Stochastic Actor. In Interna-\ntional Conference on Machine Learning, 2018.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momen-\ntum Contrast for Unsupervised Visual Representation\nLearning. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020.\nHouthooft, R., Chen, R. Y., Isola, P., Stadie, B. C., Wolski,\nF., Ho, J., and Abbeel, P. Evolved Policy Gradients. In\nAdvances in neural information processing systems, 2018.\nKumar, A., Fu, J., Tucker, G., and Levine, S. Stabilizing Off-\nPolicy Q-Learning Via Bootstrapping Error Reduction.\nIn Advances in Neural Information Processing Systems,\n2019.\nLample, G. and Chaplot, D. S. Playing FPS Games with\nDeep Reinforcement Learning. In AAAI Conference on\nArtiï¬cial Intelligence, 2017.\nLange, S., Gabel, T., and Riedmiller, M. Batch Reinforce-\nment Learning. In Reinforcement learning, pp. 45â€“73.\nSpringer, 2012.\nLaskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive\nUnsupervised Representations for Reinforcement Learn-\ning. In International Conference on Machine Learning,\n2020.\nLevine, S., Kumar, A., Tucker, G., and Fu, J. Ofï¬‚ine Rein-\nforcement Learning: Tutorial, Review, and Perspectives\non Open Problems. arXiv preprint arXiv:2005.01643,\n2020.\nLi, J., Vuong, Q., Liu, S., Liu, M., Ciosek, K., Ross, K.,\nChristensen, H. I., and Su, H. Multi-Task Batch Rein-\nforcement Learning with Metric Learning. In Interna-\ntional Conference on Learning Representations, 2020.\nLi, L., Huang, Y., Chen, M., Luo, S., Luo, D., and Huang,\nJ. Provably Improved Context-Based Ofï¬‚ine Meta-RL\nwith Attention and Contrastive Learning. arXiv preprint\narXiv:2102.10774, 2021a.\nLi, L., Yang, R., and Luo, D. FOCAL: Efï¬cient Fully-\nOfï¬‚ine Meta-Reinforcement Learning Via Distance Met-\nric Learning and Behavior Regularization. In Interna-\ntional Conference on Learning Representations, 2021b.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T.,\nTassa, Y., Silver, D., and Wierstra, D. Continuous Control\nwith Deep Reinforcement Learning. In International\nConference on Learning Representations, 2016.\nLiu, E. Z., Raghunathan, A., Liang, P., and Finn, C. Ex-\nplore Then Execute: Adapting Without Rewards Via Fac-\ntorized Meta-Reinforcement Learning. arXiv preprint\narXiv:2008.02790, 2020a.\nLiu, Y., Yi, L., Zhang, S., Fan, Q., Funkhouser, T., and\nDong, H. P4Contrast: Contrastive Learning with Pairs of\nPoint-Pixel Pairs for RGB-D Scene Understanding. arXiv\npreprint arXiv:2012.13089, 2020b.\nMaaten, L. v. d. and Hinton, G. Visualizing Data Using\nT-SNE. Journal of Machine Learning Research, 9(11):\n2579â€“2605, 2008.\nMitchell, E., Rafailov, R., Peng, X. B., Levine, S., and Finn,\nC. Ofï¬‚ine Meta-Reinforcement Learning with Advan-\ntage Weighting. In International Conference on Machine\nLearning, 2021.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\nAtari with Deep Reinforcement Learning. arXiv preprint\narXiv:1312.5602, 2013.\nNguyen, H. and La, H. Review of Deep Reinforcement\nLearning for Robot Manipulation. In IEEE International\nConference on Robotic Computing, 2019.\nOord, A. v. d., Li, Y., and Vinyals, O. Representation Learn-\ning with Contrastive Predictive Coding. arXiv preprint\narXiv:1807.03748, 2018.\nOroojlooyJadid, A. and Hajinezhad, D. A Review of Coop-\nerative Multi-Agent Deep Reinforcement Learning. arXiv\npreprint arXiv:1908.03963, 2019.\nParisotto, E., Ghosh, S., Yalamanchi, S. B., Chinnaobireddy,\nV., Wu, Y., and Salakhutdinov, R. Concurrent Meta Rein-\nforcement Learning. arXiv preprint arXiv:1903.02710,\n2019.\nPatrick, M., Asano, Y. M., Kuznetsova, P., Fong, R., Hen-\nriques, J. F., Zweig, G., and Vedaldi, A. Multi-Modal\nSelf-Supervision From Generalized Data Transforma-\ntions. arXiv preprint arXiv:2003.04298, 2020.\nPeng, X. B., Kumar, A., Zhang, G., and Levine, S.\nAdvantage-Weighted Regression: Simple and Scalable\nOff-Policy Reinforcement Learning.\narXiv preprint\narXiv:1910.00177, 2019.\nPong, V. H., Nair, A., Smith, L., Huang, C., and Levine, S.\nOfï¬‚ine Meta-Reinforcement Learning with Online Self-\nSupervision. arXiv preprint arXiv:2107.03974, 2021.\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\nRakelly, K., Zhou, A., Quillen, D., Finn, C., and Levine, S.\nEfï¬cient Off-Policy Meta-Reinforcement Learning Via\nProbabilistic Context Variables. In International Confer-\nence on Machine Learning, 2019.\nRothfuss, J., Lee, D., Clavera, I., Asfour, T., and Abbeel, P.\nProMP: Proximal Meta-Policy Search. In International\nConference on Learning Representations, 2019.\nSermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E.,\nSchaal, S., Levine, S., and Brain, G. Time-Contrastive\nNetworks: Self-Supervised Learning From Video. In\nIEEE International Conference on Robotics and Automa-\ntion, 2018.\nSohn, K., Lee, H., and Yan, X. Learning Structured Out-\nput Representation using Deep Conditional Generative\nModels. In Advances in neural information processing\nsystems, 2015.\nStadie, B. C., Yang, G., Houthooft, R., Chen, X., Duan, Y.,\nWu, Y., Abbeel, P., and Sutskever, I. Some Considera-\ntions on Learning To Explore Via Meta-Reinforcement\nLearning. arXiv preprint arXiv:1803.01118, 2018.\nStooke, A., Lee, K., Abbeel, P., and Laskin, M. Decoupling\nRepresentation Learning From Reinforcement Learning.\nIn International Conference on Machine Learning, 2021.\nTian, Y., Krishnan, D., and Isola, P. Contrastive Multiview\nCoding. In European Conference on Computer Vision,\n2020.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Å., and Polosukhin, I. Attention\nis All You Need. In Advances in neural information\nprocessing systems, 2017.\nWang, T. and Isola, P. Understanding Contrastive Repre-\nsentation Learning Through Alignment and Uniformity\non The Hypersphere. In International Conference on\nMachine Learning, 2020.\nWang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M.,\nand Freitas, N. Dueling Network Architectures for Deep\nReinforcement Learning. In International Conference on\nMachine Learning, 2016.\nWu, Y., Tucker, G., and Nachum, O. Behavior Regular-\nized Ofï¬‚ine Reinforcement Learning. arXiv preprint\narXiv:1911.11361, 2019.\nZhang, J., Wang, J., Hu, H., Chen, Y., Fan, C., and Zhang,\nC. Learn to effectively explore in context-based meta-rl.\narXiv preprint arXiv:2006.08170, 2020.\nZheng, G., Zhang, F., Zheng, Z., Xiang, Y., Yuan, N. J., Xie,\nX., and Li, Z. DRN: A Deep Reinforcement Learning\nFramework for News Recommendation. In World Wide\nWeb Conference, 2018.\nZintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hof-\nmann, K., and Whiteson, S. VariBAD: A Very Good\nMethod for Bayes-Adaptive Deep RL Via Meta-Learning.\nIn International Conference on Learning Representations,\n2020.\nÅukasz Kaiser, Babaeizadeh, M., MiÅ‚os, P., OsiÂ´nski, B.,\nCampbell, R. H., Czechowski, K., Erhan, D., Finn, C.,\nKozakowski, P., Levine, S., Mohiuddin, A., Sepassi, R.,\nTucker, G., and Michalewski, H. Model-Based Reinforce-\nment Learning for Atari. In International Conference on\nLearning Representations, 2020.\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\nA. Contrastive Learning Objective\nIn this section, we ï¬rst introduce a lemma. Then we give a proof of the main theorem in the text.\nLemma A.1. Given M âˆ¼P(M) with reward R and transition T, (s, a) follows some distribution, x = (s, a, r, sâ€²), r =\nR(s, a), sâ€² âˆ¼T(sâ€²|s, a), z âˆ¼P(z|x), then we have\nP(M|z)\nP(M) = Ex\n\u0014P(z|x)\nP(z)\n\u0015\n.\nProof.\nP(M|z)\nP(M) = P(z|M)\nP(z)\n=\nZ\nx\nP(x|M)P(z|x)\nP(z)\ndx\n= Ex\n\u0014P(z|x)\nP(z)\n\u0015\nTheorem A.2. Let M be a set of tasks following the task distribution, |M| = N. M âˆˆM is the ï¬rst task with reward\nand transition R, T. Let x = (s, a, r, sâ€²), z âˆ¼P(z|x), h(x, z) = P (z|x)\nP (z) , where (s, a) follows an arbitrary distribution,\nr = R(s, a), sâ€² âˆ¼T(sâ€²|s, a). For any task M âˆ—âˆˆM with reward and transition Râˆ—, T âˆ—, denote xâˆ—= (s, a, râˆ—, sâˆ—â€²) as a\ntransition tuple generated in M âˆ—conditioned on (s, a), where râˆ—= Râˆ—(s, a), sâˆ—â€² âˆ¼T âˆ—(sâ€²|s, a). Then we have\nI(z; M) âˆ’log(N) â‰¥EM,x,z\n\u0014\nlog(\nh(x, z)\nP\nM âˆ—âˆˆM h(xâˆ—, z))\n\u0015\n.\nProof. Using Lemma A.1 and Jensenâ€™s inequality, we have\nI(z; M) âˆ’log(N) = Ez,Mlog\n\u0014p(M|z)\np(M)\n\u0015\nâˆ’log N\n= Ez,M\n\u0014\nlog Ex\n\u0014p(z|x)\np(z)\n\u0015\u0015\nâˆ’log N\nâ‰¥Ez,MEx\nï£®\nï£°log\nï£«\nï£­\n1\np(z)\np(z|x)N\nï£¶\nï£¸\nï£¹\nï£»\nâ‰¥Ez,MEx\nï£®\nï£°log\nï£«\nï£­\n1\n1 +\np(z)\np(z|x)(N âˆ’1)\nï£¶\nï£¸\nï£¹\nï£»\n= Ez,MEx\n\u0014\nâˆ’log\n\u0012\n1 + p(z)\np(z|x)(N âˆ’1)\n\u0013\u0015\n= Ez,MEx\n\u0014\nâˆ’log\n\u0012\n1 + p(z)\np(z|x)(N âˆ’1)EM âˆ—âˆˆM\\{M}\n\u0014p(z|xâˆ—)\np(z)\n\u0015\u0013\u0015\n= Ez,MEx\nï£®\nï£°âˆ’log\nï£«\nï£­1 + p(z)\np(z|x)\nX\nM âˆ—âˆˆM\\{M}\np(z|xâˆ—)\np(z)\nï£¶\nï£¸\nï£¹\nï£»\n= EM,x,z\nï£®\nï£°log\nï£«\nï£­\np(z|x)\np(z)\np(z|x)\np(z) + P\nM âˆ—âˆˆM\\{M}\np(z|xâˆ—)\np(z)\nï£¶\nï£¸\nï£¹\nï£»\n= EM,x,z\n\u0014\nlog(\nh(x, z)\nP\nM âˆ—âˆˆM h(xâˆ—, z))\n\u0015\nRobust Task Representations for Ofï¬‚ine Meta-Reinforcement Learning via Contrastive Learning\nB. Environment Details\nIn this section, we present the details of the environments in our experiments.\nPoint-Robot: The start position is ï¬xed at (0, 0) and the goal location g is sampled from U[âˆ’1, 1] Ã— U[âˆ’1, 1]. The reward\nfunction is deï¬ned as rt = âˆ’âˆ¥st âˆ’gâˆ¥2, where st is the current position. The maximal episode steps is set to 20.\nAnt-Dir: The goal direction is sampled from Î¸ âˆ¼U[0, 2Ï€]. The reward function is rt = vx cos Î¸ + vy sin Î¸, where (vx, vy)\nis the horizontal velocity of the ant. The maximal episode steps is set to 200.\nHalf-Cheetah-Vel: The goal velocity is sampled from vg âˆ¼U[0, 3]. The reward function is rt = âˆ’|vt âˆ’vg| âˆ’1\n2âˆ¥atâˆ¥2\n2,\nwhere vt is the current forward velocity of the agent and at is the action. The maximal episode steps is set to 200.\nWalker-Param: Transition dynamics are varied in body mass and frictions, described by 32 parameters. Each parameter\nis sampled by multiplying the default value with 1.5Âµ, Âµ âˆ¼U[âˆ’3, 3]. The reward function is rt = vt âˆ’10âˆ’3 Â· âˆ¥atâˆ¥2\n2 + 1,\nwhere vt is the current forward velocity of the walker and at is the action. The episode terminates when the height of the\nwalker is less than 0.5. The maximal episode steps is also set to 200.\nHopper-Param: Transition dynamics are varied in body mass, inertia, damping and frictions, described by 41 parameters.\nEach parameter is sampled by multiplying the default value with 1.5Âµ, Âµ âˆ¼U[âˆ’3, 3]. The reward function is rt =\nvt âˆ’10âˆ’3 Â· âˆ¥atâˆ¥2\n2, where vt is the current forward velocity of the hopper and at is the action. The maximal episode steps is\nset to 200.\nC. Experimental Details\nIn Table 4 and 5, we list the important conï¬gurations and hyperparameters in the data collection and meta training phases\nthat we used to produce the experimental results.\nTable 4. Conï¬gurations and hyperparameters used in dataset collection to produce all the experimental results.\nConï¬gutations\nPoint-Robot\nAnt-Dir\nHalf-Cheetah-Vel\nWalker-Param\nHopper-Param\nDataset size\n2100\n2e4\n2e5\n2e4\n6e4\nTraining steps\n1e3\n4e4\n1e5\n6e5\n2e5\nBatch size\n256\n256\n256\n256\n256\nNetwork width\n32\n128\n128\n128\n128\nNetwork depth\n3\n3\n3\n3\n3\nLearning rate\n3e-4\n3e-4\n3e-4\n3e-4\n3e-4\nTable 5. Conï¬gurations and hyperparameters used in ofï¬‚ine meta training to produce all the experimental results.\nConï¬gutations\nPoint-Robot\nAnt-Dir\nHalf-Cheetah-Vel\nWalker-Param\nHopper-Param\nNegative pairs\nRandomize\nRandomize\nGenerative\nGenerative\nNone\np(Î½)\nN(0, 0.5)\nN(0, 0.5)\nâ€“\nâ€“\nâ€“\nLatent space dim\n5\n5\n5\n32\n40\nTask batch size\n16\n16\n16\n16\n16\nTraining steps\n2e5\n2e5\n2e5\n2e5\n2e5\nRL batch size\n256\n256\n256\n256\n256\nContrastive batch size\n64\n64\n64\n64\n64\nNegative pairs number\n16\n16\n16\n16\n16\nRL network width\n64\n256\n256\n256\n256\nRL network depth\n3\n3\n3\n3\n3\nEncoder width\n64\n64\n64\n128\n128\nLearning rate\n3e-4\n3e-4\n3e-4\n3e-4\n3e-4\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-06-21",
  "updated": "2022-06-21"
}