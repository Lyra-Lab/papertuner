{
  "id": "http://arxiv.org/abs/2206.14302v1",
  "title": "Reinforcement Learning in Medical Image Analysis: Concepts, Applications, Challenges, and Future Directions",
  "authors": [
    "Mingzhe Hu",
    "Jiahan Zhang",
    "Luke Matkovic",
    "Tian Liu",
    "Xiaofeng Yang"
  ],
  "abstract": "Motivation: Medical image analysis involves tasks to assist physicians in\nqualitative and quantitative analysis of lesions or anatomical structures,\nsignificantly improving the accuracy and reliability of diagnosis and\nprognosis. Traditionally, these tasks are finished by physicians or medical\nphysicists and lead to two major problems: (i) low efficiency; (ii) biased by\npersonal experience. In the past decade, many machine learning methods have\nbeen applied to accelerate and automate the image analysis process. Compared to\nthe enormous deployments of supervised and unsupervised learning models,\nattempts to use reinforcement learning in medical image analysis are scarce.\nThis review article could serve as the stepping-stone for related research.\nSignificance: From our observation, though reinforcement learning has gradually\ngained momentum in recent years, many researchers in the medical analysis field\nfind it hard to understand and deploy in clinics. One cause is lacking\nwell-organized review articles targeting readers lacking professional computer\nscience backgrounds. Rather than providing a comprehensive list of all\nreinforcement learning models in medical image analysis, this paper may help\nthe readers to learn how to formulate and solve their medical image analysis\nresearch as reinforcement learning problems. Approach & Results: We selected\npublished articles from Google Scholar and PubMed. Considering the scarcity of\nrelated articles, we also included some outstanding newest preprints. The\npapers are carefully reviewed and categorized according to the type of image\nanalysis task. We first review the basic concepts and popular models of\nreinforcement learning. Then we explore the applications of reinforcement\nlearning models in landmark detection. Finally, we conclude the article by\ndiscussing the reviewed reinforcement learning approaches' limitations and\npossible improvements.",
  "text": " \n \n \nReinforcement Learning in Medical Image Analysis: \nConcepts, Applications, Challenges, and Future Directions \nMingzhe Hua, Jiahan Zhangb, Luke Matkovicb, Tian Liub and Xiaofeng Yanga,b* \naDepartment of Computer Science and Informatics, Emory University, GA, Atlanta, USA \nbDepartment of Radiation Oncology, Winship Cancer Institute, School of Medicine,  \nEmory University, GA, Atlanta, USA \n*Email: Xiaofeng.yang@emory.edu \n \n \n \n \nAbstract \nMotivation: Medical image analysis involves tasks to assist physicians in qualitative and quantitative \nanalysis of lesions or anatomical structures, significantly improving the accuracy and reliability of \ndiagnosis and prognosis. Traditionally, these tasks are finished by physicians or medical physicists and \nlead to two major problems: (i) low efficiency; (ii) biased by personal experience. In the past decade, many \nmachine learning methods have been applied to accelerate and automate the image analysis process. \nCompared to the enormous deployments of supervised and unsupervised learning models, attempts to use \nreinforcement learning in medical image analysis are scarce. This review article could serve as the stepping-\nstone for related research. \nSignificance: From our observation, though reinforcement learning has gradually gained momentum in \nrecent years, many researchers in the medical analysis field find it hard to understand and deploy in clinics. \nOne cause is lacking well-organized review articles targeting readers lacking professional computer science \nbackgrounds. Rather than providing a comprehensive list of all reinforcement learning models in medical \nimage analysis, this paper may help the readers to learn how to formulate and solve their medical image \nanalysis research as reinforcement learning problems. \nApproach & Results: We selected published articles from Google Scholar and PubMed. Considering the \nscarcity of related articles, we also included some outstanding newest preprints. The papers are carefully \nreviewed and categorized according to the type of image analysis task. We first review the basic concepts \nand popular models of reinforcement learning. Then we explore the applications of reinforcement learning \nmodels in landmark detection. Finally, we conclude the article by discussing the reviewed reinforcement \nlearning approaches‚Äô limitations and possible improvements. \n \n \n \n \n \n1. Introduction \nThe purpose of medical image analysis is to mine and analyze valuable information from medical images \nby using digital image processing to assist doctors in making more accurate and reliable diagnoses and \nprognoses. According to different imaging principles, common imaging modalities can be categorized as \nCT, MR, Ultrasound, SPECT, PET, X-ray, OCT, and microscope. Medical image processing can also be \nclassified according to specific processing tasks. Typical tasks include classification, segmentation, registration, \nand recognition. Figure 1 shows the range of our review article. \n \nFigure 1: Range of our review article. Blue box: covered image analysis tasks; green box: covered anatomical \nsites; yellow box: covered imaging modalities. \nWith the development of imaging technology and the iterative update of imaging equipment, the time \nrequired for medical imaging is greatly shortened, and the resolution of imaging is also significantly \nimproved. At the same time, the data volume of medical images has experienced an unprecedented surge, \nwith the trend of high dimensionality. The traditional manual analysis of medical images by physicians \nbecame tedious and inefficient. More and more physicians are looking to automate this process by \npartnering with engineers. That‚Äôs how the combined medical imaging and machine learning field was born. \nMany excellent algorithms in the field of natural image analysis have also shown good results in the field \nof medical images (Shen et al., 2017). \nReinforcement learning (RL) is neither supervised learning nor unsupervised learning. The goal \nof reinforcement learning is to achieve the maximum expected cumulative reward (Sutton & Barto, \n2018). \nFigure 2 shows the relationship between machine learning, supervised learning, unsupervised \nlearning, reinforcement learning and deep learning. \n \n \nFigure 2: Relationship between machine learning, supervised learning, unsupervised learning and \nreinforcement learning. \nThe number of published reinforcement learning-related papers has grown rapidly in the past two decades. \nState-of-the-art RL models have been applied to solve problems that are difficult or infeasible with other \nmachine learning approaches, such as playing video games (Mnih et al., 2013; Mnih et al., 2015; Silver et \nal., 2017), natural language processing (Sharma & Kaushik), and autonomous driving (Sallab et al., 2017). \nThese RL methods have achieved outstanding performances. However, attempts to exploit the technical \ndevelopments in RL in the medical analysis field are scarce. Figure 3 shows the trends of number of published \nmachine learning papers and reinforcement learning papers in medical image analysis. Despite the overall \ngrowth trend, the number of published RL papers still only constitutes a tiny part of machine learning in \nmedical image analysis. On the other hand, RL methods have unique advantages in dealing with medical \nimage data: \n‚Ä¢ RL models can efficiently learn from limited annotation guided by supervised \nactions step by step, while medical data often lacks large-scale accessible \nannotation. \n‚Ä¢ RL models are less biased since they won‚Äôt inherit bias from the labels made by \nhuman annotators. \n‚Ä¢ RL-agents can learn from sequential data, and the learning process is goal-\noriented. Besides exploiting experience, it can also explore new solutions. The \nRL can even surpass human experts when solving the same problem. \nThe review article is based on Synthesis Methodology (Wilson & Anagnostopoulos, 2021). \n \n \nFigure 3: Trends of number of published machine learning papers and reinforcement learning papers in medical \nimage analysis. This figure is made by separately searching the keywords \"Machine Learning AND (Medical Imaging \nOR (Medical Image Analysis))\" and \"Reinforcement learning AND (Medical Imaging OR (Medical Image \nAnalysis))\" in PubMed. The number of papers published each year is counted. \nPreferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) will be \nfollowed(Moher et al., 2009). Firstly, the following pattern will be searched in Google Scholar and PubMed: \nClustering AND (Medical OR CT OR MR OR Ultrasound OR X-ray OR OCT) AND IMAGE AND \nSegmentation. Then the duplicate papers will be removed. We set the qualified publication date to 2010. \nThe remaining papers will go through qualitative synthesis and quantitative synthesis. The summary of the \nselection process is shown in Figure 4. \n \nFigure 4: Flows of information through the different phases of a systematic review. \n \nBy reviewing content, analyzing common points and comparing difference of these papers, we hope that we \ncan inspire our target readers to (i) have a better understanding of RF, (ii) learn how to formulate their research \nproblems as RL problems. For the next two sections, we will first prepare the readers with basic knowledge \nof RL. Then we will show how to apply RL in different medical image analysis tasks. Those readers who \nhave already been familiar with RL algorithms could directly go to the application section. \n \n2. Reinforcement Learning Basics \nIn this subsection, we provide a list of terminologies that frequently appear in RL papers. Some terminologies \nmay appear in definitions of other terminologies before they are defined. \n‚Ä¢ Action (A): An action (a) is the way that an agent interacts with the environment. A \nincludes all possible actions that an agent could perform.  \n‚Ä¢ Agent: Agents are the models we attempt to build that interact with the environment and take \nactions. \n‚Ä¢ Environment: The content that the agent is interacting with is called the environment. While \nproviding feedback after the agent takes action, the environment itself is also changing. \n‚Ä¢ State (S): A state (s) is a frame of an environment. S includes all states that an agent will go \nthrough. \n‚Ä¢ Reward (R): A positive reward (r) means an increase possibility of achieving the goal, while \na negative reward means the decreased possibility. R includes all the possible reward values \nthe environment may feed back to the agent. \n‚Ä¢ Episode: If an agent has gone through all the states from the initial state to the terminal state, \nwe say this agent has finished the episode. \n‚Ä¢ Transition probability ùëÉ(ùë†‚Ä≤|ùë†, ùëé): P(s‚Ä≤|s, a) is the possibility of transiting to transiting to \nstate s‚Ä≤ to from the current state s, taking the action a. \n‚Ä¢ Policy ùúã(ùëé|ùë†): The policy instructs the agent to choose among actions A under the current \nstate. \n‚Ä¢ Return (G): The return is the cumulative discounted future reward.  \n‚Ä¢ ùê∫ùë°= ùëüùë°+ ùõæùëüùë°+1 + ùõæ2ùëüùë°+2, where t is the time and Œ≥ is the discount factor. \n‚Ä¢ State value ùëâùúã(ùë†):  The expected amount of return from current state. \n‚Ä¢ ùëâùúã(ùë†) = ùê∏[ùê∫ùë°|ùë†ùë°= ùë†], where E is the expectation. \n‚Ä¢ Action value ùëÑùúã(ùë†, ùëé) (Q value): The expected amount of return from current state, taking \naction s. ùëÑùúã(ùë†, ùëé) = ùê∏[ùê∫ùë°|ùë†ùë°= ùë†, ùëéùë°= ùëé] \n‚Ä¢ Optimal action value: ùëÑ‚ãÜ(ùë†, ùëé): Q‚ãÜ(s, ‚Äàa)‚Äà = ùëöùëéùë•\nùúã\nùëÑùúã(ùë†ùë°, ‚Äàùëéùë°) \n‚Ä¢ Agent environment interaction: Figure 5 shows how the agent is interaction with the \nenvironment. \n \n \nFigure 5: Agent Environment Interaction. Adapted from (Rafati & Noelle, 2019). \nWith the development of the RL theory, numerous algorithms have been created. Benefiting from the \ncombination with deep learning, RL is now capable of handling more and more complex scenarios in \nmodern applications. But no matter how complex these state-of-the-art algorithms are, they can be mainly \ndivided into two categories: model- based RL and model-free RL. As its name indicates, model-based RL \nattempts to explain the environment and create a model to simulate it. Model-free RL, however, will only \nupdate its policy by interacting with the environment and observing the rewards. \nWe can further divide the model-free RLs into policy-based and value-based according to whether the \nalgorithm is optimizing the value function or policy. Value-based RLs are widely applied for discrete action \nspace problems, while policy-based RLs are suitable for both discrete and continuous action space. Some \nRL algorithms are based on both the value and policy, like DDPG (Lillicrap et al., 2015), TD3 (Fujimoto et \nal., 2018) and SAC (Haarnoja et al., 2018). Figure 6 shows the taxonomy of popular RL algorithms. In our \nreview, all the RL models are model-free, and the mostly used algorithms are DQN, DDQN, A2C, and DDPG. \nBelow we include brief introductions of these RL algorithms commonly used in medical image analysis. \n \n \nFigure 6: Reinforcement learning algorithms taxonomy. \n \n2.1. DQN \nThe Deep Q-Network (DQN) was first proposed by (Mnih et al., 2013; Mnih et al., 2015) to solve some \ncomplex computer perception vision problems. It combined the idea of the traditional Q learning method \n(Watkins, 1989) and the deep CNN (Krizhevsky et al.). The motivation of DQN is to solve the problem \nthat the Q-table can only store a limited number of states, while in real-life scenarios, there could be an \nimmense or even infinite number of states. DQN adopts the experience replay mechanism that randomly \nsamples a small batch of tuples from the replay buffers during the training process. The correlations \nbetween the samples are significantly reduced, leading to better algorithm robustness. Another \nimprovement, compared to Q learning, is that DQN uses a deep CNN to represent the current Q function \n \nand uses another network to define the target Q value. The introduction of the target Q value network \nreduced the correlation between the current and target Q values. Figure 7 shows the workflow of the DQN. \n \n \nFigure 7: Workflow of DQN algorithm. \n \n2.2. DDQN \n DQN is one of the most popular RL algorithms applied in medical image analysis. How- ever, the \noptimization target in DQN is represented as ùëü+ ùõæmax\nùëé‚Ä≤ ùëÑ(ùë†‚Ä≤, ùëé‚Ä≤|ùúÉùëñ\n‚àí). The selection and evaluation of \nactions are all based on the network‚Äôs same parameter, leading to over- estimation of the Q value. The \nDouble DQN (DDQN), which (Van Hasselt et al.) first proposed, used two separate networks for selection \nand evaluation. Here the target Q value is written as ùëü+ ùõæùëÑ(ùë†‚Ä≤, argmaxùëéùëÑ(ùë†‚Ä≤, ùëé|ùúÉùëñ), ùúÉùëñ\n‚àí), which achieved \nbetter more stable learned policy than DQN. \n2.3. Actor Critic \nThe Actor-Critic (AC) (Konda & Tsitsiklis, 1999) algorithm mixes the idea of policy gradient and Time \nDifference (TD) learning and can handle continuous action space problems and update the policy in an \nefficient stepwise manner. The actor is the policy function œÄŒ∏(ùëé|ùë†) that learns the policy using gradient \ndescent to achieve the highest possible reward. And the critic is the value function ùëâœÄ(ùë†) that uses the TD error \nto assess the current policy. \n2.4. A2C/A3C \nA2C (Sewak, 2019) and A3C (Mnih et al., 2016) are improved versions of the vanilla actor-critic that \nintroduced the parallel architecture. Each agent includes a global network and multiple workers that run \nindependently. Every worker would gather different experiences and calculate the different gradients. \nAccording to the ways of tackling the different parameters and different gradients, we can derive the \nsynchronous version ‚Äî A2C and asynchronous version ‚Äî A3C. Synchronous here means that different \nworkers share the same policy, and the time to update the policy is the same, making the A2C tend to \nconverge faster than the A3C. \n2.5. DDPG \nDDPG is a type of actor-critic-based algorithm but learns the off-policy. Similar to DQN, samples \n \ngenerated by the random policy are stored in the memory replay buffer. However, DQN can only solve \ncontrol problems with discrete actions, while DDPG can solve the problems in the continuous action space \nand shows excellent efficiency in finding the optimal policy. However, for some random environments, \nsuch as low signal-to-noise-ratio images, the deterministic policy gradient strategy adopted by DDPG is \nnot suitable. \n \n3. RL in Medical Image Analysis \n3.1. Medical Image Detection \n3.1.1. Overview of Works \nLandmark Detection \nAnatomical landmarks are biological coordinates that can be reallocated repeatedly and precisely \non images produced by different imaging modalities ‚Äî computed tomography (CT), ultrasound (US), \nmagnetic resonance imaging (MRI). The accurate detection of anatomical landmarks is the ground for \nfurther medical image analysis tasks. Figure 8 is an example of vocal tract landmarks from the MRI \nimage. \n \nFigure 8: Vocal tract landmarks from MRI image Courtesy of (Eslami et al., 2020). \nMany automatic algorithms for anatomical landmark detection have existed long before the attempts \nof using RL models. However, landmark detection, especially 3D landmarks detection, could be \nchallenging and cause the failure of  these algorithms (Ghesu et al., 2019). Moreover, the computation of \nfeatures and hyper-parameters selection of the system may not be optimal since the involvement of human \ndecisions. The researchers attempted a different paradigm to address this problem - translate the landmark \ndetection tasks as reinforcement learning problems which is the common goal of the papers we reviewed. \nWhile most essential and tricky task in these papers, as you can see later, is designing the state space, action \nspace, and reward space before training the models. \n(Ghesu et al., 2016) is one of the very first papers that attempted to use RL for anatomical landmark \ndetection. In an image I, ùëùùê∫ùëá\n‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó  denotes the location of anatomical landmark, and ùëùùë°\n‚Éó‚Éó‚Éó  denotes the location at \nthe current time. State space S is the collection of all possible states ùë†ùë°= ùêº(ùëùùë°\n‚Éó‚Éó‚Éó ). Action space A is the \ncollection of all possible actions by which the agent can move to the adjacent position, as illustrated by \nFigure 9. Reward space R is defined as ||ùëùùë°\n‚Éó‚Éó‚Éó ‚àíùëùùê∫ùëá\n‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó ||2\n2 ‚àí||ùëùùë°+1\n‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó ‚àíùëùùê∫ùëá\n‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó ||2\n2 which impels the agent to move \ncloser to the target anatomical landmark. A deep learning model was applied to approximate the state value \n \nfunction. The parameters are updated according to gradient descent, and the error function is: \nŒ∏ùëñÃÇ = arg min\nùúÉùëñùê∏ùë†,ùëé,ùëü,ùë†‚Ä≤ [(ùë¶‚àíùëÑ(ùë†, ùëé; ùúÉùëñ))\n2] + ùê∏ùë†,ùëé,ùëü[ùëâùë†‚Ä≤(ùë¶)] \n(1) \nThis deep Q learning-based method beat the existing top systems not only in accuracy but also in speed. The \ndesign of action, state, and reward spaces in the paper we just discussed became a standard method. \n \nFigure 9: Possible actions of a 3D landmarks detection task. Courtesy of (Ghesu et al., 2019). \nHowever, the approach mentioned above is still preliminary. One of the biggest disadvantages is that \nit could not fully use the information at different scale. So a multi-scale deep reinforcement learning method \nwas soon proposed in (Ghesu et al., 2019). The search for the landmark started from the coarsest scale. \nOnce the search is convergent, the continued work would be started at a finer scale until the search meets \nthe finest scale‚Äôs convergence criteria. Figure 10 illustrates this non-trivial search process. Where ùêøùëë is the \nscale level in the continuous scale-space L, which can be calculated as: \nùêøùëë(ùë°) = œàœÅ(œÉ(ùë°‚àí1) ‚àóùêøùëë(ùë°‚àí1)) \n(2) \nWhere œàœÅ is the signal operator, and œÉ is the Gaussian-like smoothing function. \n \nFigure 10: The trajectory of search the anatomical landmark across images of multiple scale-levels. Courtesy \nof (Ghesu et al., 2019). \n \n(Alansary et al., 2019) extended the work of Ghesu et al. by evaluating different types of RL agents. \nHe compared the detection results of using DQN, double DQN (DDQN), Double DQN, and duel double \nDQN (duel DDQN) on three different-modalities dataset ‚Äî fetal US, cardiac MRI, and brain MRI. \nRather than detecting a landmark per agent separately, bold attempts have been made by (Vlontzos et \nal., 2019) to detect multiple landmarks with multiple collaboration agents. With the assumption that the \nanatomical landmarks have inner correlations with each other, the detection of one landmark could indicate \nthe location of some other landmarks. For the action function approximator in this paper, the collaborative \ndeep Q network (Collab-DQN) was proposed. The weights of the convolutional layers are shared by all the \nagents, while the fully connected layers for deciding the actions are trained separately per agent. Compared \nto the methods that trained agents for different landmarks differently, this multi-agent approach reduced \n50% detection error using a shorter training time. \nSome other contributions to the RL for anatomical landmarks detection include: estimating the \nuncertainty of reinforcement learning agent (Browning et al., 2021), reducing the needed time to reach the \nlandmark by using a continuous action space (Kasseroller et al., 2021), localization of modality invariant \nlandmark (Winkel et al., 2020). \nLesion Detection \nObject detection, also called object extraction, is the process of finding out the class labels and locations \nof target objects in images or videos. It is one of the primary tasks in medical image analysis (Li et al., \n2019). An exemplary detection result can be used as the basis to improve the performance of further tasks \nlike segmentation. \nThe mainstream approaches for lesion detection nowadays still rely on exhaustive search methods that \ncost a lot of time and deep learning methods that require a large amount of labeled data. Facing the current \nchallenges and inspired by similar problems in landmarks detection (Ghesu et al., 2016), (Maicas et al., \n2017) implemented a deep Q-network (DQN) agent for active breast lesion detection. The states are defined \nas current bounding box volumes of the 3D DCE-MR images. The reinforcement learning agent could \ngradually learn the policy to choose among actions to transit, scale the bounding box, and finally localize \nthe breast lesion. Specifically, the action set consists of 9 actions that can translate the bounding box \nforward or backward along the x, y, z-axis, scale up or scale down the bounding box, and trigger the \nterminal state. To further evaluate the effectiveness of applying reinforcement learning on lesion detection \nwith limited data, using DQN as the agent to localize brain tumors with very small training data was \nattempted by (Stember & Shalu, 2020), (Stember & Shalu, 2021a).Different from (Maicas et al., 2017), the \nbrain MR data are 2D slices. The environment is defined as the 2D slices overlaid with gaze plots viewed \nby the radiologist. Instead of using the bounding box, the states are the gaze plots the agent located. Three \nactions - moving anterograde, not moving, moving retrograde would help the agent transfer to the next \nstate. If the agent moves toward the lesion, it will receive a positive reward, otherwise a negative one. If \nthe agent stays still, it will receive a relatively large positive reward within the lesion area or a rather large \npenalization otherwise. The experiment results showed that reinforcement learning models could work as \nrobust lesion detectors with limited training data, reduce time consumption, and provide some \ninterpretability. \nAlso addressing the lack of labeled training data, (Pesce et al., 2019) exploited visual attention \nmechanisms to learn from a combination of weakly labeled images (only class label) and a limited number \nof fully annotated X-ray images. This paper proposed convolutional networks with attention feedback \n(CONAF) architecture and a recurrent attention model with annotation feedback (RAMAF) architecture. \nThe RAMAF model can only observe one part of the image, which is defined as a state at a glimpse. The \nreinforcement learning agent needs to learn the policy to take a sequence of glimpses and finally locate the \nlesion site within the shortest time. Each glimpse consists of two image patches sharing the same central \npoint, and the length of the glimpse sequence is fixed to be 7. The rewards will be decided according to (i) \nif the image is classified correctly; (ii) if the central point of a glimpse is within the labeled bounding box. \nRAMAF achieved a localization performance of detecting 82% of overall bounding boxes with a much \n \nfaster detection speed than other state-of-the-art methods. \nMore than detecting lesions in static medical images (2D or 3D), the reinforcement- learning-based \nsystem can also track the lesions frame by frame continuously. (Luo et al., 2019) proposed a robust RL-\nbased framework to detect and track plaque in Intravascular Optical Coherence Tomography (IVOCT) \nimages. Despite the pollution problem of speckle- noise, blurred plaque edges, and diverse intravascular \nmorphology, the proposed method achieved accurate tracking and has strong expansibility. \nThree different modules are included in the proposed framework. The features are extracted and \nencoded first by the encoding feature module. Then the information of scale and location of the lesion is \nprovided by the localization and identification module. Another function of this module is preventing over- \ntracking. The most important module is the spatial- temporal correlation RL module. Nine different actions \nare different, including eight transformation actions and one stop action. The state S is defined as three-\ntuples: ùëÜ= (ùê∏, ùêªùêø, ùêªùê¥).  Here, the E represents the encoded output features from the FC1 layer. HL is the \ncollection of recent locations and scales. HA represents the recent ten sets of actions. 8000 IVOCT images \nwere used to evaluate the framework. With a strict standard (IOU > 0.9), the RL module could improve the \nperformance of plaque tracking both frame-level and plaque-level. \nOrgan/ Anatomical Structure Detection \nBesides detecting lesions, reinforcement learning can also be applied in organ detection. (Navarro et \nal., 2020) designed a deep Q-learning agent to locate various organs in 3D CT scans. The state is defined \nas voxel values within the current 3D bounding box. Eleven actions, including six translation actions, two \nzooming actions, and three scaling actions, make sure that the bounding box can move to any part of the \n3D scan. The agent is rewarded if an action improves the intersection over union score (IOU). Seventy \nscans were used for training, and 20 scans were used for testing on seven different organs: pancreas, spleen, \nliver, lung (left and right), and kidney (left and right). This proposed method achieved a much faster speed \nthan the region proposal and the exhaustive search methods and led to an overall IOU score of 0.63. \n(Zhang et al., 2021) managed to detect and segment the vertebral body (VB) simultaneously. The \nsequence correlation of the VB is learned by a soft actor-critic (SAC) RL agent to reduce the background \ninterference. The proposed framework consists of three modules: Sequential Conditional Reinforcement \nLearning network (SCRL), FC- ResNet, and Y-net. The SCRL learns the correlation and gives the attention \nregion. The FC-ResNet extracts the low-level and high-level features to determine a more precise bounding \nbox according to the attention region. At the same time, the segmentation result is provided by the Y-net. \nThe state of the RL agent is determined by a combination of the image patch, feature map, and region mask. \nAnd the reward is designed according to the change of attention-focusing accuracy to elicit the agent to \nachieve a better detection performance. This proposed approach accomplished an average of 92.3% IOU \non VB detection and an average of 91.4% Dice on VB segmentation. \nThe research of (Zheng et al., 2021) was the first attempt to use the multi-agent RL in prostate detection. \nTwo DQN agents locate the lower-left and upper-right corners of the bounding box while sharing \nknowledge according to the communication protocol (Foerster et al., 2016). The final location of the \nprostate is searched with a coarse-to-fine strategy to speed up the search process and improve the detection \naccuracy. In more detail, the agents first search on the coarsest scale to draw a big bounding box and \ngradually move to a finer scale to generate a smaller and more accurate bounding box to detect the prostate. \nCompared to the single-agent strategy (63.15%), this multi-agent framework achieved a better average \nscore of 80.07% in IOU. \n3.1.2. Assessment \nDetection is a type of problem that straightforwardly can be formulated as the control or path-finding \nproblem. Generally speaking, the states are defined as the pixel values that the agents observe at the current \nstep, and the actions are defined as movements along the different axis of the environment plus some scaling \nfactors. That is why agent-based detection has the most considerable number of papers among all RL-\nrelated image detection tasks. Though related work in this field is still growing, some challenges exist. \n \nFirstly, the generalizability and reproducibility of the agent-based methods still need to be further \ninvestigated. In practical application, the quality and local features of the image may vary by the noise and \ndistortion introduced in the imaging process. The trained agent may not always be capable of finding the \ntarget in clinical settings. Furthermore, the trigger of the termination state in the inference stage needs to \nbe improved. The most commonly used criteria adopted now is the happening of oscillation. However, this \nmay lead to a very ineffective convergence, and the agent might even be trapped at some local optimal \npoint and never reach the actual destination. Real-time detection is another direction that has caused more \ninterest in recent years. RL has proved its fast detection capability due to the non-exhaustive searching \nstrategy. However, in some high dimensional data, 4D images (3D plus temporal), for example, the real-\ntime detection and tracking still need more investigation. The last point is that the training process of the \nRL system, especially the multi-agent system, is very time-consuming, which may take days to weeks to \ntrain on even the best hardware platforms, let along the hyperparameter-tunning is also highly relied on the \ndesigner‚Äôs experience. A summary of the works we reviewed in this section is given in Table 1. \n \nTable 1. Overview of RL in medical image object and lesion detection. \nAuthor \nROI \nModality \nAlgorithm \nState \nAction \nNumber \nReward Design \n(Luo et \nal., 2019) \nHeart \nOCT \nADNet \nSpatial-temporal \nlocations correlation \ninformation \n9 \nChange of \nintersection-over-\nunion (IOU) index \n(Maicas et \nal., 2017) \nBreast \nMR \nDQN \nCurrent bounding \nvolume \n9 \nChange of \nintersection-over-\nunion (IOU) index \n(Navarro \net al., \n2020) \nLung, \nKidney, \nLiver, \nSpleen, \nPancreas \nCT \nDQN \nVoxel value of the \ncurrent bounding box \n11 \nChange of \nintersection-over-\nunion (IOU) index \n(Pesce et \nal., 2019) \nLung \n \nXray \nREINFORCE \nThe part of the image \nobserved by the \nglimpse \n \nNumber of \nimage \npixels \nCorrectness of the \nclassification; \nLocation of the \nglimpse \n(Zhang et \nal., 2021) \nVertebral \nBody \n \nMR \nSAC \nCombination of the \nimage patch, feature \nmap and region \nmask. \n4 \n \n \nChange of \nattention-focusing \naccuracy \n(Stember \n& Shalu, \n2020) \nBrain \nMR \nDQN \nThe gaze plot the \nagent locate \n3 \nWhether moving \ntowards the gaze \nplot that include \nthe tumor \n(Zheng et \nal., 2021) \nProstate \nMR \nDQN \nVoxel values \ncontained in the \nbounding box \n \n4 \nChange of the \nDistance and IOU \nbetween the \nbounding box and \nthe target \n* Indicate that the missing part is not clearly defined in the original paper\n \n \n3.2. Medical Image Segmentation \n3.2.1. Overview of Works \nThreshold Determination \n(Sahba et al., 2006) is the first attempt at using RL for medical image segmentation. The key idea is to \nformulate this segmentation task as a control task by a simple Q-learning agent that decides the optimal \nlocal thresholds and the post-processing parameters. The quality of the segmentation is considered when \ndesigning the state. The segmentation threshold and size of the structuring elements are changed by taking \na series of actions. Though simple as this initial research, the segmentation quality was acceptable while \nsignificantly reducing the required human interaction compared to the mainstream methods like active \ncontour at that time. \nPre-locate the Segmentation Region \nMost supervised-learning-based catheter segmentation methods require a large amount of well-\nannotated data. (Yang et al., 2020) proposed a semi-supervised pipeline shown in Figure 11 that first uses \na DQN agent to allocate the coarse location of the catheter and then conducts patch-based segmentation by \nDual-UNet. The RL agent reduced the need for voxel-level annotation in the pre-allocation stage. The semi-\nsupervised Dual-UNet exploited plenty of unlabeled images according to prediction hybrid constraints, \nthus improving the segmentation performance. The states are defined as the 3D observation patches, and \nthe agent can update the states by moving the patch center point (x, y, z) along the x, y, and z-axis of the \nobservation space. Like the landmark detection problems, the agent would give a negative reward if the \npatch moves away from the target; otherwise, a positive reward for moving toward and no reward if \nstanding still. Compared to the state-of-the-art methods, this proposed pipeline requires much less \ncomputation time and achieves a minimum of 4% segmentation performance improvement measured by \nDice Score. \nHyperparameters optimization \nInstead of directly involved in the segmentation process, RL agents can also be applied to optimize the \nexisting medical image segmentation pipelines (Bae et al., 2019; Qin et al., 2020; Yang et al., 2019). (Bae \net al., 2019) used RL as the controller to automate the searching process of optimal neural architecture. The \nrequired search time and the computation power are significantly reduced by sharing the parameters while \nadopting a macro search strategy. Tested on the medical segmentation decathlon challenge, the authors \nassert that this optimized architecture outperformed the most advanced manually searched architectures. \n \n \nFigure 11: The semi-supervised DQN-driven catheter segmentation framework. Courtesy of (Yang et al., \n2020). \nRealizing the problem that some randomly augmented images might sometimes even harm the final \nsegmentation performance, (Qin et al., 2020) implemented an automated end-to-end augmentation pipeline \nusing Dual DQN (DDQN) agent. By making the trails and saving the experiences, the agent would learn \nto determine the augmentation operations beneficial to the segmentation performance according to the fed-\nback Dice ratio. Twelve different basic actions would change the state to achieve augmentation. The state \nis defined as the extracted feature from U-Net. It is interesting to observe that horizontal flipping and \ncropping are two of the most useful operation. \n(Yang et al., 2019) from NVIDIA integrated the highlights of the previous two reviewed papers. With \nan RNN-based controller, this research automates the design process of hyper- parameters and image \naugmentation to explore the maximum potential of the state-of-the- art models. The optimal policy is \nlearned using the proximal policy optimization to decide the training parameters. Tested on the medical \ndecathlon challenge tasks, the RL searched model and augmentation parameters have shown remarkable \neffectiveness and efficiency. \nSegmentation as a Dynamic Process \nObserving that many existing automated segmentation pipelines may often fail in real clinical \napplications, (Liao et al., 2020) implemented multi-agent reinforcement learning to interact with the users \nthat can achieve an iteratively refined segmentation performance. This multi- agent strategy captures the \ndependence of the refinement steps and emphasizes the uncertainty of binary segmentation results in states \ndefinition. Instead of defining the state as the binary segmentation result, it is formulated as ùë†ùëñ\n(ùë°) =\n[ùëèùëñ, ùëùùëñ\n(ùë°), ‚Ñé+,ùëñ\n(ùë°), ‚Ñé‚àí,ùëñ\n(ùë°)], where ùëè, ùëù, ‚Ñé are the value, previous prediction probability, hint maps of voxel ùëñ, and \nùë° indicates the current step. The actions will change the segmentation probability by an amount ùëé‚ààùê¥, \nwhere ùê¥ is the action set. Furthermore, the voxel-wise reward is defined as ùëüùëñ\n(ùë°) = ùúíùëñ\n(ùë°‚àí1) ‚àíùúíùëñ\n(ùë°), where ùúí \nis the cross entropy between the label ùë¶ùëñ probability ùëùùëñ, to refine the segmentation more efficiently. The \nrefined final segmentation result outperformed Min- Cut (Boykov & Kolmogorov, 2004), DeepGeoS(R-\nNet) (Wang et al., 2018), and InterCNN (Bredell et al., 2018) on all the BRATS20015, MM- WHS, NCI-\nICBI2013 datasets. Though published earlier than the (Liao et al., 2020) and adopted the older RL method \n \nto learn the policy, (Wang et al., 2013) incorporated not only the user‚Äôs background knowledge but also \ntheir intentions. The proposed framework follows a ‚ÄúShow-Learn- Act‚Äù workflow, which reduces the \nrequired interactions while achieving context-specific and user-specific segmentation. \n3.2.2. Assessment \nTackling the image segmentation problems using RL agents provides us with an effective way to further \noptimize existing pipelines, overcome a limited number of training data, and interact with users to \nincorporate prior knowledge. Despite the novices of these methods, limitations still exist. The various \ndefinitions of states and actions may significantly influence the precision of the segmentation. In most \nworks, the states are updated by a series of limited-number discrete actions to determine the final \nsegmentation contours. Another problem is that the state design makes the agent only observe local or \nglobal information at a step. It would be interesting to see some methods in the future that can enable the \nagent to make these two pieces of information observable to the agent at the same time. A summary of the \nworks we reviewed in this section is given in Table 2. \n \nTable 2. Overview of RL in medical image segmentation. \nAuthor \nROI \nModality \nAlgorithm \nState \nAction \nNumber \nReward Design \n(Bae et \nal., 2019) \nBrain, \nHeart, \nProstate \nMR \nREINFORCE \nHyperparameters of \nsearched architecture \n* \nChange of the \nDice Score \n(Liao et \nal., 2020) \n \nBrain, \nHeart, \nProstate \nMR \nA3C \nCombination of the \nvoxel value, previous \nsegmentation \nprobability and hint \nmaps \n6 \nDecreased \namount of the \ncross entropy \n \n \n(Qin et \nal., 2020) \nKidney \nCT \nDDQN \nExtracted high-level \nfeature \n12 \nChange of the \nDice ratio \n(Sahba et \nal., 2006) \nProstate \nUltrasound \nQ Learning \nSegmented Objects \n6 \nChange of \nsegmentation \nquality \n(Wang et \nal., 2013) \nVentricle \nMR \nPolicy-based \nLocal image \nappearance, \nanatomical details \nContinuous \nDifference \nbetween the \nlocation given \nby model and \nuser \n(Yang et \nal., 2019) \nAtrium, \nLung, \nPancreas, \nSpleen \nCT, MR \nPPO \nParameter values \n* \nChange of \nsegmentation \nperformance \n(Yang et \nal., 2020) \n \nCatheter \n \nUltrasound \nDQN \n3D patches \n6 \nChange of \ndistance to the \ntarget \n* Indicate that the missing part is not clearly defined in the original paper\n \n \n3.3. Medical Image classification \nClassification is one of the most basic tasks in medical image analysis. Common medical image \nclassification tasks include disease diagnosis and prognosis, anomaly detection, and survivorship \nprediction. According to the extracted information from the image, a label among the predefined classes \nwould be assigned. Since most of the classification methods are fully supervised, these methods may often \nfail in real clinical settings due to a lack of high-quality labeled data. \n3.3.1. Overview of Works \nTrain with Limited Annotation \nTo overcome this limitation, Stember and Shalu published two papers (Stember & Shalu, 2021a, 2021b) \nwhich used a DQN agent in tandem with a TD model for accurate image classification with a minimal \ntraining set. The main workflow of these two papers is identical, except that the labels in the second paper \nwere extracted from clinical reports using an SBERT (Reimers & Gurevych, 2019). By overlaying the \nimages with red or green masks, they managed to formulate the classification problem as a behavioral \nproblem shown in Figure 12. The states are defined as the original greyscale image overlaid in green or \nred, where the red mask indicates a wrong prediction and the green mask indicates the correct prediction. \nThe binary action (0 or 1) predicts the label of the image as normal (0) or tumor-containing (1). If the \nprediction is correct, a +1 reward would be given to the agent; otherwise, a -1 reward would be penalized. \nCompared with the supervised learning model trained on the same minimum dataset, the RL showed \nexcellent overfitting resistance and high classification accuracy. \n \nFigure 12: State transition and reward of a normal brain image. Courtesy of (Stember & Shalu, 2021a). \nAnother popular method to solve the lack of annotated data is to generate synthetic data. However, \nmost medical synthesis pipelines do not assess the influence of the quality of these synthetic images in \ndownstream tasks. Some misleading information in these synthetic data may skew the data distribution and \nthus harm the performance of the following tasks. To address this problem, (Ye et al., 2020) designed a \nPPO RL controller that can select synthetic images generated by HistoGAN. Considering the potential \nrelationship between the generated and existing data, they used a transformer to output the action decision \nwith the features extracted by a ResNet34 as input. The reward is designed according to the maximum \nvalidation accuracy in the last five epochs. Comparing the traditional augmentation, GAN augmentation, \ntriplet loss metric learning, and centroid-distance-based selective augmentation, this Transformer-PPO-\nbased RL selective augmentation achieved the best overall performance in the following classification task \n \nwith an AUC score as high as 0.89. \nOptimal sample/weight/ROI selection \nAs one of the common situations in the clinical setting, radiologists may have obtained considerable \nimages, but the annotation process might be time-consuming and labor- expensive. A common approach \nto alleviate this problem is using active learning methods to select the most informative samples for \nannotations to improve the following tasks‚Äô performance. (Jingwen Wang et al., 2020), for the first time, \nformulated active learning for medical image classification as an automated dynamic Markov decision \nprocess. The current state consists of all the predicted values of the unlabeled images. This state then will \nbe updated by continuous actions to decide the unlabeled ones for annotations according to the optimal \npolicy. The model is trained according to the deep deterministic policy gradient algorithm (DDPG), which \nconsists of an actor and a critic. A novice reward was designed to encourage the actor to focus on those \nsamples that are incorrectly classified. Compared to other selective strategies, this RL framework achieved \nthe best F1 score with all different percentages of training samples and a remarkable 0.70 score with only \n40% labeled training data needed. (Smit et al., 2021) combined meta-learning and deep reinforcement \nlearning for selective labeling. A bidirectional LSTM (BiLSTM) is used as the selector, and a non-\nparametric classifier is used as the predictor. Instead of using the RL agent as a controller, this work used \nthe policy-gradient algorithm to optimize the objective function of the controller. \nIn clinical practice, an experienced physician usually makes diagnosis decisions from a combination of \nmulti-modal images. Similarly, many state-of-the-art methods attempt to extract and integrate the \ninformation from various modalities to improve prediction performance. However, the weights of different \nmodalities in this combination are hard to determine. (Jian Wang et al., 2020) automated this as an end-to-\nend process controlled by a REINFORCE RL agent, as shown in Figure 13. There are four US modalities \ninvolved in this pipeline: B-mode, Doppler, SE, and SWE. The model parameters are updated according to \nthe global loss, which is a weighted summation of loss from each of the four modalities and a fusion loss \ncalculated from the concatenated features. The states (weights) are updated (-0.2 or +0.2 or 0) at each step. \nCompared with other advanced single-modality and multi-modality methods on breast ultrasound datasets, \nthis auto-weighted RL method achieved the best overall performance with an accuracy as high as 95.43%. \n \nFigure 13: Schematic of the proposed multi-modal model. The weights of the losses are determined by the \nRL module. Courtesy of (Jian Wang et al., 2020). \nFor some types of medical images, histopathology images, for example, the resolution could be \nextremely high. Even though we have sufficient labeled data, it is still hard to perform the classification \ntasks due to the required high computation resources. Holding the belief that not all parts of the original \n \nimages include valuable information, (Xu et al., 2020) proposed an RL-based pipeline for automated lesion \nregion selection. Different from the hard-attention approaches, this RL selective attention method is end-\nto-end and fully automated, which includes two major stages - the selection stage and the classification \nstage. The backbone of the selection stage is a recurrent LSTM that outputs a binary action that decides \nwhether a cropped patch is useful for classification. In the classification stage, a soft-attention network is \nused as the classifier. The reward that is fed back to the selector consists two-part, the training process \nreward, which represents the training stage accuracy, and the convergence reward, which represents the \nconvergence performance. This selective strategy reduced the computation time by 50% and took less than \n6ms to infer a single image. \nUser Interaction \nSimilar to the cases we have reviewed in the segmentation section, well-designed interactions with \nusers can also help to improve the performance of classification tasks. However, instead of targeting the \nphysicians as users, (Akrout et al., 2019) designed RL controlled QA system that interacts with the patients. \nA CNN is pre-trained to output the probabilities vector of the skin conditions. This vector is then \nconcatenated with the vector of the patient‚Äôs history answers about the symptom, where each binary value \nin the vector represents if a symptom is present. The action is designed to decide the next question to ask. \nThe goal of the DQN agent is to maximize the possibility of classifying the correct condition when asking \na specific question. Compared to CNN-only and decision-tree-based approaches, this RL-based symptom \nchecker improved the classification accuracy up to 20% and 10%, respectively. \n3.3.2. Assessment \nThe nature of the image classification made it hard to define classification as a control problem. So \ninstead of directly defining agent-based classification frameworks, most works attempted to use RL-agent \nto optimize the existing classification models‚Äô hyperparameters or the image preprocessing process. The \nonly two papers that applied agents to the classification task itself came from the same authors, asserting \nthat the agent-based classification method is superior to other methods on minimal training sets and still \nneeds to be validated with more relevant studies. We look forward to seeing more research that can \ningeniously design image classification as the control problem. A summary of the works we reviewed in \nthis section is given in Table 3. \n \nTable 3. Overview of RL in medical image classification. \nAuthor \nROI \nModality \nAlgorithm \nState \nAction \nNumber \nReward Design \n(Akrout et \nal., 2019) \nSkin \nDermascope \nDQN \nPatient's history \nanswers + output \nprobability of \npretrained CNN \n300 \nProbability of \ncorrect \ncondition if \nasked the \nquestion \n(Smit et \nal., 2021) \nChest \nXray \nPolicy \nGradient \n* \n* \n* \n \n(Stember \n& Shalu, \n2021a) \nBrain \nMR \nDQN + TD \nImage overlaid in \nred or green \n2 \nClassification \ncorrectness \n(Stember \n& Shalu, \n2021b) \nBrain \nMR \nDQN + TD \nImage overlaid in \nred or green \n2 \nClassification \ncorrectness \n(Jian \nWang et \nal., 2020) \nBreast \nUltrasound \nREINFORCE \nWeights \n \n3 \nClassification \ncorrectness \n \n(Jingwen \nWang et \nal., 2020) \nChest \nCT \nDDPG \nPredictions of the \nunlabeled images \nContinuous \nPossibility of \nbeing classified \nincorrectly \n(Ye et al., \n2020) \nCervix, \nLymph \nNode \nHistopathology \nPPO \nSelected images \n2 \nMaximum \nvalidation \naccuracy of last \nepochs \n(Xu et al., \n2019) \nBreast \nHistopathology \nPolicy \nGradient \nLearning status \nrepresentation + \nIncoming data \nstatistics \n2 \nPerformance of \nthe selection \nmechanism \n* Indicate that the missing part is not clearly defined in the original paper\n \n \n3.4. Medical Image Synthesis \nMedical image synthesis is the process of generating synthetic images that include artificial lesions or \nanatomical structures of a particular or multiple image modalities. There are typically two types of medical \nimage synthesis (Wang et al., 2021): (i) inter-modality: transform the image of a specific modality to \nanother modality, e.g., CT to MRI; (ii)intra- modality: transform the imaging protocol within the same \nmodality, e.g., T1 sequence MRI to T2 sequence, or generating new images according to existing same-\nmodality images. Nowadays, medical image synthesis algorithms are dominated by Generative Adversarial \nNetworks (GANs) (Goodfellow et al., 2014), as their capability of providing visual appearing synthetic \nimages with large diversity. Numerous GANs with different frameworks and techniques, such as Bayesian \nConditional GAN (Zhao et al., 2020), progressive growing GAN (Guan et al., 2022), self-attention module \n(Lan et al., 2021), and deep supervision (Pan et al., 2021), have proposed and achieved state-of-the-art \nperformance in various medical image synthesis applications. For the comprehensiveness of our review, \nwe expand the concept of image synthesis a little while including the radiation dose map planning in this \nsection. \n3.4.1. Overview of Works \nSemantic Map Generation \n(Krishna et al., 2021) successfully combined reinforcement learning and style transfer techniques to \nsynthesize fine CT images from a small image dataset. There are two major steps in this pipeline. First, a \nDQN agent automatically generates the semantic maps of the lung CT images. Next, the B-splines and \nPCA interpolation are adopted to interpolate the semantic masks, thus providing texture information. The \ngenerated images have high resolution and are realistic enough to be used in other image analysis tasks. \nPixel Value Alteration \nBy designing a novice pixel-level graph RL method, (Xu et al., 2021) generates gadolinium-enhanced \nliver tumor images from non-enhanced images, thus avoiding the injection of toxic contrast agents. It is \nworth noting that this is the first paper that used RL agent for image synthesis and also the first attempt at \ndesigning agents, actions, and rewards all at the pixel level. The design of agents is based on the actor-\ncritic structure but also integrates the idea of graph CNN. This graph-driven context- aware mechanism \nenables the model to capture both the small local object and global features.  The reward function: ùëüùëñ\nùë°=\nùëü(ùëí)ùëñ\nùë°+ ùúÜ(ùë§)ùëñ\nùë° considered both the pixel-level (first item) and region-level (second item) rewards to make \nthe measurement more accurate. According to the current states and rewards, the pixel-level agents \ndetermine the actions to increase, decrease or keep the pixel intensity. Trained and tested on 24375 images, \nthe model outperformed the existing state-of-the-art method (Zhao et al., 2020). \nSynthetic Sample Selection \nInstead of focusing on directly synthesizing medical images, RL can also be applied in synthetic image \nassessment and selection. (Ye et al., 2020) selected HistoGAN-generated synthetic images according to the \nreliability and informativity. The selection process is formulated as a model-free, policy-based RL process \nstabilized by the Proximal Policy Optimization (PPO). The binary action is the output of the transformer \nmodel to decide if a fake image should be selected or discarded. The reward is designed to impel better \naccuracy in the following classification task. Compared with the unselected case, image augmentation using \nthe chosen synthetic images improved the classification accuracy by 8.1% on the cervical data and 2.3% \non the lymph node data. \nRL has been widely applied in radiotherapy plan optimization to determine the optimal adaption \ntime(Ebrahimi & Lim, 2021), tune the machine parameters (Hrinivich & Lee, 2020) and decide the beam \norientation (Sadeghnejad-Barkousaraie et al., 2021). However, considering the scope of our review, we \nwill only discuss agent-based dose map planning here. \n \nDose Plan Generation \n(Shen et al., 2019) is among the first works that attempted to use the RL agent to optimize the dose-\nvolume-histogram (DOV) in a step-by-step manner, leading to the final dose map. The states are defined \nas the current weights of the DOV, and five actions per weight will update the states. The reward is defined \nas the change in the dose plan's quality, according to the WTPN's guidance. Compared with humans, the \nRL agents led to an average improvement of 10.7% of the final dose plan quality in high dose-rate \nbrachytherapy. \nIn one of their followed works (Shen et al., 2020), they applied the same idea to the external beam \nradiotherapy for prostate cancer patients. An end-to-end virtual treatment network (VTPN) was built to \ngenerate the optimum dose plan. They used ten patients for training and 64 patients for testing. With this \nVTPN-based treatment planning pipeline, the original ProKnown score increased to 10.93 from 6.07. \n3.4.2. Assessment \nWorks that use RL agents for image synthesis are still scarce. There are three types of different \nstrategies for the general meaning synthesis: semantic map generation, pixel-level value alteration, and \nsynthetic sample selection. Compared to other state-of-the-art methods, the agent-based approach does not \nperform significantly superior and is training-time-consuming. According to our observation, the authors \nof these agent-based synthesis works did not publish any related results further, showing a fading away \ninterest. \nThe key idea for the agent-based dose map generation is to update the DVHs step by step. Though this \nimproved the dose plan quality, the reward function is not fully based on the clinical criteria, and the plan \nquality is only measured according to the DVH, which is only a simple representation. More works are \nencouraged to evaluate and improve this agent-based method for the more challenging treatment planning \nscenarios. A summary of the works we reviewed in this section is given in Table 4. \n \nTable 4. Overview of RL in medical image synthesis. \nAuthor \nROI \nModality \nAlgorithm \nState \nAction \nNumber \nReward Design \n \n(Krishna et \nal., 2021) \nLung \nCT \nDQN \nControl points \ncoefficient \nsequences \n* \nClassification \nresults of the \npretrained \nclassifier \n(Xu et al., \n2021) \nLiver \nMR \nPix-GRL \n(AC-based) \nPixel values of \ncurrent image \n3 \nImprovement of \neach pixel (pixel \nlevel); \nImprovement of \npixels and \nsurrounding \npixels (region -\nlevel) \n(Ye et al., \n2020) \nCervix, \nLymph \nnode \nHistopathology \nPPO \nSynthetic \nimages \n2 \nValidation \nclassification \naccuracy \n(Shen et \nal., 2019) \nCervix \nDose Volume \nHistogram \n(DVH) \nQ-learning \nCurrent DVH \nweights \n4 x 5 \nChange of the \nplan quality \n(Shen et \nal., 2020) \nProstate \nDose Volume \nHistogram \n(DVH) \nQ-learning \nCurrent DVH \nweights \n5 x 5 \nChange of the \nplan quality \n* Indicate that the missing part is not clearly defined in the original paper\n \n \n3.5. Medical Image Registration \nImage registration is the process of transforming two different images into the same co- ordinates. In \nthe medical imaging domain, the registration could be inter-patient, and intra- patient (but at different time \npoints), inter-modality (e.g., MRI, CT, CBCT, Ultrasound), inter- dimensionality (2D-3D). The \ntransformation models may also vary depending on the properties of the registration pair. It can generally \nbe categorized into rigid, affine, and deformable transformation, where the rigid transformation is the \nsimplest one to achieve using parametric models. Traditionally, the registration is completed via optimizing \nthe similarity metrics. However, the high dimensionality of the medical images and the tissue‚Äôs \ndeformations and artifacts make this method unstable and sometimes even infeasible. The emergence of \nagent- based methods tackled the registration problem from a totally different angle‚Äîthe formulation of \nthe MDP process. Controlled by RL agents, the registration tasks achieve unprecedented robustness and \nprecision. \n3.5.1. Overview of Works \nRigid Registration \nThe first attempt to use an agent-based method to solve the registration problems was by (Liao et al., \n2017). Unlike the standard methods that focus on matching metrics optimization, this agent-based approach \nattempted to find the optimum sequence of actions that can align the images for registration. To train the \nagent with limited data, they first got synthetic data by dealigning the labeled training pairs. The registration \nprocess is done in a two-step hierarchical manner to ensure robustness and accuracy. A coarse registration \nis first done in a broader field of view (FOV) with lower resolution, followed by the fine alignment on the \nfull resolution image. This proposed method outperformed the ITK registration method, Quasi-global \nsearch, and semantic registration methods on both the spine and heart 3D-3D registration to a large extent. \nIt proved the possibility and superiority of agent-controlled registration. \nBased on the idea of (Liao et al., 2017), (Miao & Liao, 2019) evaluate the performance of this pipeline \nfor 2D-2D and 3D-3D intra-modality registration. They discussed the state, action space, and the design of \nrewards in detail and this chapter of the book can serve as a good tutorial. In another work (Miao et al., \n2018), they attempted to use multi-agent attention mechanism to solve the 2D-3D registration for images \nwith severe artifacts. Instead of choosing the commonly used CNNs, this work adopted dilated fully-CNN \n(FCN) as the backbone of the agent. This strategy reduced the registration problem‚Äôs degrees of freedom \n(DoFs) from 8 to 4, significantly improving training efficiency. \nMoreover, they implemented the auto attention mechanism to overcome the strong arti- facts in the 2D \nimages. The so-called attention is achieved by using multiple local agents to learn and decide the actions, \nand only the agents whose confidence scores are above a certain threshold (0.67) will be selected. There \nare six pairs of actions (negative and positive) of the Special Euclidean Group SE(3). The framework was \ntested on both CBCT and more complex surgical data. In the more complex scenario, the multi-agent \nattention mechanism showed much better robustness (less performance degradation) than the \ncorresponding single-agent system. \nNon-rigid Registration \nSo far, what we have tackled with are all rigid problems. (Krebs et al., 2017) first attempted non-rigid \nregistration with limited number of real inter-object pair. Both the inter-object and intra-object pairs were \nused for training. The intra-object pairs were generated as an augmentation step to compensate for the \ninsufficient number of inter-object pairs. Considering that non-rigid registration has more degrees of \nfreedom than the rigid transformation, the authors built statistical deformation models to serve as a low-\ndimensional representation of the problem. While minimizing the possible number of actions, the \nrobustness of the registration is also guaranteed by the fuzzy action control. Experiments were conducted \non both the 2-D and 3-D MR prostate images with a median Dice score of 0.88/0.76. \n \nLookahead Inference \nTo further improve the registration performance of rigid registration, in a series paper from (Hu et al., \n2021; Sun et al., 2019), they incorporated the trained network with the lookahead inference. More \nspecifically, the long-short-term-memory-machine (LSTM), specialized in tackling sequential data, is used \nto extract the spatial-temporal features. The fixed and moving images formed a 3D tensor to represent the \nstate updated by transformations, including translations (+/- 1 pixel), rotations (+/- 1 degree), and scaling \n(+/- 0.05). In the testing phase, to make sure that the agent can reach the terminal state, they adopted a \nMonte Carlo rollout strategy to simulate different searching trajectories. The final transformation matrix is \ncalculated as the weighted average of matrices from all the trajectories. Compared with other regression-\nbased and agent-based methods, the addition of the lookahead and Monte Carlo rollout mechanism \nimproved the robustness and accuracy of multimodal image registration tasks. \n3.5.2. Assessment \nFrom the works we have reviewed above, it is not hard to see that agent-based registration methods \nhave comparable or even better performance than the intensity/deep- similarity-based methods. However, \nmost of the papers can only solve the rigid-registration problems. For the non-rigid transformer, however, \nthe high-dimensional state-space and large number of DoFs may impede the agents from efficient \nconvergence. So, researchers may have to transform the transformation space into a lower-dimensional \nspace before applying the RL agents for registration. Another problem is that the agent may inhere the \ninefficiency from some similarity metrics used as the loss function during the training process, so novice \nloss functions should be designed for the RL frameworks. Last but not least, the methods that directly \npredict the transformation is still developing fast these years. Many state-of-the-art papers are emerging, \nwhile only a few papers are looking into agent-based registration, showing the low interest of researchers \nyet in this field. A summary of the works we reviewed in this section is given in Table 5. \n \nTable 5. Overview of RL in medical image Registration. \nAuthor \nROI \nModality \nAlgorithm \nState \nAction \nNumber \nReward Design \n(Hu et al., \n2021) \nNasopharynx \nCT-MR \nA3C \n3D tensor \ncomposed of the \nmoving and the \nfixed image \n8 \nDistance between the \ntransformed landmark \nand the reference \nlandmark \n(Krebs et \nal., 2017) \nProstate \nMR-MR \nQ Learning \nSpatial \ntransformation \nparameters \n30 \nThe reduction of \ndistance between the \ncurrent parameters to \nthe ground truth \nparameters \n(Liao et \nal., 2017) \nSpine, Cardiac \nCT-CBCT \nQ Learning \nThe rigid-body \ntransformation \nmatrix \n12 \nThe reduction of \ndistance between the \ncurrent transformation \nto the ground truth \ntransformation \n(Ma et al., \n2017) \nChest, \nAbdomen \nCT-Depth \nImage \nDueling \nDQN \n3D tensor \nincluding \ncropped image \npair \n \n \n6 \nSmall constant during \nthe exploration, and \nbig constant at \ntermination with sign \ndetermined by change \nof constant \n(Luo et \nal., 2021) \nBrain, Liver \nMR-MR, \nCT-CT \nSPAC \nPair of fixed \nimages and \nmoving image \n* \nThe change of the dice \nscore \n \n(Miao et \nal., 2018) \nSpine, Cardiac \nCT-CBCT \nDilated FCN \nThe rigid-body \ntransformation \nmatrix \n12 \nThe reduction of \ndistance between the \ncurrent transformation \nto the ground truth \ntransformation \n(Miao & \nLiao, \n2019) \nSpine \nCBCT-\nXray \nQ Learning \nTransformations \nin SE (3) \n12 \nReduction of distance \nto the ground truth \ntransformation \n(Sun et \nal., 2018) \nNasopharynx \nCT-MR \nA3C \nConcatenation \nof the fixed and \nmoving image \n8 \nDistance between the \ntransformed landmark \nand the reference \nlandmark \n* Indicate that the missing part is not clearly defined in the original paper\n \n \n4. Discussion \nDesigning the medical image analysis as the RL problems is not an easy thing. Generally, the pipeline \ncan be concluded in four steps: (i) thoroughly understand the environment; (ii) correctly choose an \nalgorithm that would work for your problem; (iii) meticulously design the states, actions, and rewards; (iv) \npatiently train the framework to converge and validate the results. Though the RL for medical image \nanalysis is still a new research field and the way to formulate the problems may vary from person to person, \nwe can still conclude some interesting strategies from this works. While some challenges still exist, we are \nstill optimistic about the perspective of this field. \nSingle Agent vs. Multi Agent \nMulti-agent reinforcement (MARL) is one of the trends in recent years, aiming to improve the \nperformance of single agent when facing large-scale and complicated environments. We have witnessed a \nfew attempts to use multi-agent frameworks to solve some complex medical image analysis tasks. In these \nworks, the agents collaborate and share their knowledge and experience to obtain the maximum mutual \nreward. The joint actions of all agents lead to the state's transition, and each agent's reward depends on the \nmutual strategy. It is worthy to note that the MARL may not always work. First of all, the reward \nmechanism of the MARL is more complicated than its single-agent counterpart. It is crucial to design a \nproper reward signal to improve the speed of learning and convergence. Besides, the enlarged (joint) state \nand action space may consume more computation power and decrease the efficiency of the RL framework. \nMultiscale Strategy \nOne common strategy in RL for medical image detection and segmentation problems is the multiscale \nsearching strategy. The agent will first perform the task at the coarsest level (usually by sampling the \noriginal image). An ROI is extracted and rescaled to a finer resolution, and the task is performed on this \nROI again. This process is repeated iteratively until the finest resolution is reached. This multiscale, fine-\nto-coarse strategy is beneficial for training RL frameworks and saves computational resources and time. \nModel-Free vs. Model-Based \nThe agent-based frameworks we have reviewed all belong to the model-free category. However, the \nmodel-based algorithm is another important subclass of RL. One possible reason why researchers did not \nattempt to use model-based RL is that it is hard to form the internal model of the environment considering \nthe high dimensionality and large size of medical image data. However, the model-based models have their \nown advantage over the model-free ones, which is the higher sample efficiency. We are looking forward \nto future works on model-based RL for medical image tasks with small-scale labeled data. \n4.1. Challenges \nMany challenges still impede medical imaging researchers from applying RL in their works. \nThe long training time and heavily consumed computational resources are something we can't ignore \nbefore starting RL-related research. Though RL has proved its efficiency in the inference phase of many \ntasks, learning from numerous trials and errors means it often takes at least a few days longer on some of \nthe cutting-edge GPUs. \nBesides, as we mentioned before, the design of the RL problems can be tricky. A slightly different \ndesign of the state, action, or reward may lead to a totally different performance (some models may fail to \nconverge). The choice of hyperparameters of the RL frameworks also depends on the designers' experience \nwith low explainability. Researchers may take days to experiment to find the parameters that work. \nThe low stability and reproducibility are other primary concerns (Khetarpal et al., 2018). Following the \nsame workflows, some RL-agent may fail to work as well as described in the original paper. It is even \nharder to perform similarly when the input data source is changed. Even more challenging is that the works \nhave been scarce, but most of the papers in this field will not make their code publicly available. \nOne last but not least, there have been some more accessible state-of-the-art methods to solve some \ntypes of medical imaging problems, and RL did not show a prominent advantage over them. For example, \n \nusing GANs for medical image synthesis is still mainstream, and many outstanding papers are emerging \neach year, causing people's interest in using agent-based methods in these fields to fade away. \n4.2. Future Perspectives \nThe field of reinforcement learning developed very fast these years, and a lot of new theories or \nstrategies have been proposed. However, the applications of RL in medical image analysis did not keep \npace with these improvements. Here, we summarized some improvements that may lead to the future trend \nof agent-based medical imaging. \nHierarchical Reinforcement Learning \nHierarchical reinforcement learning (HRL) aims to improve the agent's efficiency facing some \ncomplicated problems. The main idea is to disassemble the final task into several subtasks in a hierarchical \nstructure. There are three major subclasses of this type of framework: (i) HRL based on spatiotemporal \nabstraction and intrinsic motivation (Kulkarni, Narasimhan, et al., 2016); (ii) HRL based on internal option \n(Ravindran & IIL); (iii) Deep successor RL. These methods can potentially improve the current agent-\nbased pipeline when solving some high-dimensional 3D image data or even 4D tracking data (Kulkarni, \nSaeedi, et al., 2016). \nMultitask and Transfer Reinforcement Learning \nFor all the works reviewed, the trained RL agent can only perform the specific task trained for. While \ntransfer learning is prevalent in deep learning for medical image analysis, it is reasonable to consider \nreusing some pre-trained RL agents for similar but different new tasks. Agent-based transfer learning can \nbe categorized as behavioral transfer and knowledge transfer(Vithayathil Varghese & Mahmoud, 2020). \nBy implementing the transfer learning in the current RL frameworks, we no longer have to train the agent \nto learn the complete policy from scratch. This will significantly reduce the training time and improve the \nframeworks' generalizability. \nActive Reinforcement Learning \nActive learning has two-fold meaning here: (i) Interaction with the users to incorporate users' prior \nknowledge; (ii) The agent will decide what data to be labeled and what data it will be trained. This active \nlearning strategy can help the agent to understand the users' intention and get maximum performance with \nthe minimum annotated data. However, this requires the involvement of human users (physicians in our \ncase) in the process, so it might not be easy to be implemented in practice. \n \n5. Conclusions \nIn this work, we have witnessed the success of some researchers' work that ingeniously turn the \ntraditional image analysis tasks into RL-style behavioral or control problems. The basic concepts of \nreinforcement learning are first recapped, and a comprehensive analysis of applications of RL agents for \ndifferent medical image analysis tasks was conducted in different sections. Under each section, the \nformulations of RL problems are discussed in detail from different angles. As the essential elements of the \nRL systems, the choice of algorithms, state, actions, and reward are highlighted in the table in Appendix \nA. These RL-based methods provide us a way to think of the problems and create new paradigms for \nsolving current obstacles. We hope that readers can find commonalities from these works, further \nunderstand the principles of reinforcement learning, and try to apply reinforcement learning in their future \nresearch. \n6. Disclosure \nThe authors are not aware of any affiliations, memberships, funding, or financial holds that might be \nperceived as affecting the objectivity of this review. \n \n \n \n \nReferences \n \nAkrout, M., Farahmand, A.-m., Jarmain, T., & Abid, L. (2019). Improving skin condition classification with \na visual symptom checker trained using reinforcement learning. International Conference on \nMedical Image Computing and Computer-Assisted Intervention,  \nAlansary, A., Oktay, O., Li, Y., Folgoc, L. L., Hou, B., Vaillant, G., Kamnitsas, K., Vlontzos, A., Glocker, B., \nKainz, B., & Rueckert, D. (2019). Evaluating reinforcement learning agents for anatomical landmark \ndetection [https://doi.org/10.1016/j.media.2019.02.007]. Medical image analysis, 53, 156-164.  \nBae, W., Lee, S., Lee, Y., Park, B., Chung, M., & Jung, K.-H. (2019). Resource optimized neural architecture \nsearch for 3D medical image segmentation. International Conference on Medical Image Computing \nand Computer-Assisted Intervention,  \nBoykov, Y., & Kolmogorov, V. (2004). An experimental comparison of min-cut/max-flow algorithms for \nenergy minimization in vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, \n26(9), 1124‚Äì1137-1124‚Äì1137.  \nBredell, G., Tanner, C., & Konukoglu, E. (2018). Iterative Interaction Training for Segmentation Editing \nNetworks. In Y. Shi, H.-I. Suk, & M. Liu, Machine Learning in Medical Imaging Cham. \nBrowning, J., Kornreich, M., Chow, A., Pawar, J., Zhang, L., Herzog, R., & Odry, B. L. (2021). Uncertainty \nAware Deep Reinforcement Learning for Anatomical Landmark Detection in Medical Images. In M. \nde Bruijne, P. C. Cattin, S. Cotin, N. Padoy, S. Speidel, Y. Zheng, & C. Essert, Medical Image \nComputing and Computer Assisted Intervention ‚Äì MICCAI 2021 Cham. \nEbrahimi, S., & Lim, G. J. (2021). A reinforcement learning approach for finding optimal policy of adaptive \nradiation therapy considering uncertain tumor biological response. Artificial Intelligence in \nMedicine, 121, 102193.  \nEslami, M., Neuschaefer-Rube, C., & Serrurier, A. (2020). Automatic vocal tract landmark localization from \nmidsagittal MRI data. Scientific reports, 10(1), 1468. https://doi.org/10.1038/s41598-020-58103-6  \nFujimoto, S., Hoof, H., & Meger, D. (2018). Addressing function approximation error in actor-critic \nmethods. International conference on machine learning,  \nGhesu, F.-C., Georgescu, B., Zheng, Y., Grbic, S., Maier, A., Hornegger, J., & Comaniciu, D. (2019). Multi-\nScale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in CT Scans \n[https://doi.org/10.1109/TPAMI.2017.2782687]. IEEE Transactions on Pattern Analysis and \nMachine Intelligence, 41(1), 176-189.  \nGhesu, F. C., Georgescu, B., Mansi, T., Neumann, D., Hornegger, J., & Comaniciu, D. (2016, 2016//). An \nArtificial Agent for Anatomical Landmark Detection in Medical Images. Medical Image Computing \nand Computer-Assisted Intervention - MICCAI 2016, Cham. \nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, \nY. (2014). Generative Adversarial Networks. In: arXiv. \nGuan, Q., Chen, Y., Wei, Z., Heidari, A. A., Hu, H., Yang, X.-H., Zheng, J., Zhou, Q., Chen, H., & Chen, F. \n(2022). Medical image augmentation for lesion detection using a texture-constrained multichannel \nprogressive GAN [https://doi.org/10.1016/j.compbiomed.2022.105444]. Computers in Biology and \nMedicine, 145, 105444-105444.  \nHaarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep \nreinforcement learning with a stochastic actor. International conference on machine learning,  \nHrinivich, W. T., & Lee, J. (2020). Artificial intelligence‚Äêbased radiotherapy machine parameter \noptimization using reinforcement learning. Medical physics, 47(12), 6140-6150.  \nHu, J., Luo, Z., Wang, X., Sun, S., Yin, Y., Cao, K., Song, Q., Lyu, S., & Wu, X. (2021). End-to-end \nmultimodal image registration via reinforcement learning. Medical image analysis, 68, 101878. \nhttps://doi.org/https://doi.org/10.1016/j.media.2020.101878  \nKasseroller, K., Thaler, F., Payer, C., & ≈†tern, D. (2021). Collaborative Multi-agent Reinforcement Learning \nfor Landmark Localization Using Continuous Action Space. In A. Feragen, S. Sommer, J. Schnabel, \n& M. Nielsen, Information Processing in Medical Imaging Cham. \nKhetarpal, K., Ahmed, Z., Cianflone, A., Islam, R., & Pineau, J. (2018). Re-evaluate: Reproducibility in \nevaluating reinforcement learning algorithms.  \nKonda, V., & Tsitsiklis, J. (1999). Actor-critic algorithms. Advances in neural information processing \nsystems, 12.  \nKrebs, J., Mansi, T., Delingette, H., Zhang, L., Ghesu, F. C., Miao, S., Maier, A. K., Ayache, N., Liao, R., & \nKamen, A. (2017). Robust non-rigid registration through agent-based action learning. International \nConference on Medical Image Computing and Computer-Assisted Intervention,  \n \nKrishna, A., Bartake, K., Niu, C., Wang, G., Lai, Y., Jia, X., & Mueller, K. (2021). Image Synthesis for Data \nAugmentation in Medical CT using Deep Reinforcement Learning. arXiv preprint \narXiv:2103.10493.  \nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional \nNeural Networks. Advances in neural information processing systems,  \nKulkarni, T. D., Narasimhan, K., Saeedi, A., & Tenenbaum, J. (2016). Hierarchical deep reinforcement \nlearning: Integrating temporal abstraction and intrinsic motivation. Advances in neural \ninformation processing systems, 29.  \nKulkarni, T. D., Saeedi, A., Gautam, S., & Gershman, S. J. (2016). Deep successor reinforcement learning. \narXiv preprint arXiv:1606.02396.  \nLan, H., the Alzheimer Disease Neuroimaging, I., Toga, A. W., & Sepehrband, F. (2021). Three-dimensional \nself-attention conditional GAN with spectral normalization for multimodal neuroimaging synthesis \n[https://doi.org/10.1002/mrm.28819]. Magnetic Resonance in Medicine, 86(3), 1718-1733.  \nLi, Z., Dong, M., Wen, S., Hu, X., Zhou, P., & Zeng, Z. (2019). CLU-CNNs: Object detection for medical \nimages [https://doi.org/10.1016/j.neucom.2019.04.028]. Neurocomputing, 350, 53-59.  \nLiao, R., Miao, S., de Tournemire, P., Grbic, S., Kamen, A., Mansi, T., & Comaniciu, D. (2017). An artificial \nagent for robust image registration. Proceedings of the AAAI Conference on Artificial Intelligence,  \nLiao, X., Li, W., Xu, Q., Wang, X., Jin, B., Zhang, X., Wang, Y., & Zhang, Y. (2020). Iteratively-refined \ninteractive 3D medical image segmentation with multi-agent reinforcement learning. Proceedings \nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition,  \nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2015). \nContinuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.  \nLuo, G., Dong, S., Wang, K., Zhang, D., Gao, Y., Chen, X., Zhang, H., & Li, S. (2019). A deep reinforcement \nlearning framework for frame-by-frame plaque tracking on intravascular optical coherence \ntomography image. International Conference on Medical Image Computing and Computer-Assisted \nIntervention,  \nLuo, Z., Hu, J., Wang, X., Hu, S., Kong, B., Yin, Y., Song, Q., Wu, X., & Lyu, S. (2021). Stochastic Planner-\nActor-Critic for Unsupervised Deformable Image Registration. arXiv preprint arXiv:2112.07415.  \nMa, K., Wang, J., Singh, V., Tamersoy, B., Chang, Y.-J., Wimmer, A., & Chen, T. (2017). Multimodal image \nregistration with deep context reinforcement learning. International Conference on Medical Image \nComputing and Computer-Assisted Intervention,  \nMaicas, G., Carneiro, G., Bradley, A. P., Nascimento, J. C., & Reid, I. (2017). Deep reinforcement learning \nfor active breast lesion detection from DCE-MRI. International conference on medical image \ncomputing and computer-assisted intervention,  \nMiao, S., & Liao, R. (2019). Agent-based methods for medical image registration. In Deep Learning and \nConvolutional Neural Networks for Medical Imaging and Clinical Informatics (pp. 323-345). \nSpringer.  \nMiao, S., Piat, S., Fischer, P., Tuysuzoglu, A., Mewes, P., Mansi, T., & Liao, R. (2018). Dilated FCN for multi-\nagent 2D/3D medical image registration. Proceedings of the AAAI Conference on Artificial \nIntelligence,  \nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., & Kavukcuoglu, K. (2016). \nAsynchronous Methods for Deep Reinforcement Learning Proceedings of The 33rd International \nConference on Machine Learning, Proceedings of Machine Learning Research. \nhttps://proceedings.mlr.press/v48/mniha16.html \nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). \nPlaying Atari with Deep Reinforcement Learning. In: arXiv. \nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., \nFidjeland, A. K., Ostrovski, G., & others. (2015). Human-level control through deep reinforcement \nlearning. Nature, 518(7540), 529‚Äì533-529‚Äì533.  \nMoher, D., Liberati, A., Tetzlaff, J., Altman, D. G., & Group, P. (2009). Preferred Reporting Items for \nSystematic Reviews and Meta-analyses: The PRISMA Statement. PLoS medicine, 6(7), e1000097-\ne1000097.  \nNavarro, F., Sekuboyina, A., Waldmannstetter, D., Peeken, J. C., Combs, S. E., & Menze, B. H. (2020). Deep \nreinforcement learning for organ localization in CT. Medical Imaging with Deep Learning,  \nPan, S., Flores, J. D., Lin, C. T., Stayman, J. W., & Gang, G. J. (2021). Generative Adversarial Networks and \nRadiomics Supervision for Lung Lesion Synthesis. Proceedings of SPIE‚Äìthe International Society \nfor Optical Engineering, 11595.  \nPesce, E., Withey, S. J., Ypsilantis, P.-P., Bakewell, R., Goh, V., & Montana, G. (2019). Learning to detect \n \nchest radiographs containing pulmonary lesions using visual attention networks. Medical image \nanalysis, 53, 26-38.  \nQin, T., Wang, Z., He, K., Shi, Y., Gao, Y., & Shen, D. (2020). Automatic data augmentation via deep \nreinforcement learning for effective kidney tumor segmentation. ICASSP 2020-2020 IEEE \nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP),  \nRafati, J., & Noelle, D. C. (2019). Learning Representations in Model-Free Hierarchical Reinforcement \nLearning. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01), 10009-10010. \nhttps://doi.org/10.1609/aaai.v33i01.330110009  \nRavindran, B., & IIL, R. Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and \nDeep Neural Networks.  \nReimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. \narXiv preprint arXiv:1908.10084.  \nSadeghnejad-Barkousaraie, A., Bohara, G., Jiang, S., & Nguyen, D. (2021). A reinforcement learning \napplication of a guided Monte Carlo Tree Search algorithm for beam orientation selection in \nradiation therapy. Machine Learning: Science and Technology, 2(3), 035013.  \nSahba, F., Tizhoosh, H. R., & Salama, M. M. A. (2006, 16-21 July 2006). A Reinforcement Learning \nFramework for Medical Image Segmentation. The 2006 IEEE International Joint Conference on \nNeural Network Proceedings,  \nSallab, A. E. L., Abdou, M., Perot, E., & Yogamani, S. (2017). Deep Reinforcement Learning Framework for \nAutonomous Driving. Electronic Imaging, 2017(19), 70‚Äì76-70‚Äì76.  \nSewak, M. (2019). Actor-Critic Models and the A3C. In Deep Reinforcement Learning: Frontiers of \nArtificial Intelligence (pp. 141-152). Springer Singapore. https://doi.org/10.1007/978-981-13-8285-\n7_11  \nSharma, A. R., & Kaushik, P. (2017). Literature Survey of Statistical, Deep and Reinforcement Learning in \nNatural Language Processing. 2017 International Conference on Computing, Communication and \nAutomation (ICCCA),  \nShen, C., Gonzalez, Y., Klages, P., Qin, N., Jung, H., Chen, L., Nguyen, D., Jiang, S. B., & Jia, X. (2019). \nIntelligent inverse treatment planning via deep reinforcement learning, a proof-of-principle study \nin high dose-rate brachytherapy for cervical cancer. Physics in Medicine & Biology, 64(11), 115013.  \nShen, C., Nguyen, D., Chen, L., Gonzalez, Y., McBeth, R., Qin, N., Jiang, S. B., & Jia, X. (2020). Operating a \ntreatment planning system using a deep‚Äêreinforcement learning‚Äêbased virtual treatment planner for \nprostate cancer intensity‚Äêmodulated radiation therapy treatment planning. Medical physics, 47(6), \n2329-2336.  \nShen, D., Wu, G., & Suk, H.-I. (2017). Deep Learning in Medical Image Analysis. Annual Review of \nBiomedical Engineering, 19, 221‚Äì248-221‚Äì248.  \nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., \nBolton, A., & others. (2017). Mastering the game of go without human knowledge. Nature, \n550(7676), 354‚Äì359-354‚Äì359.  \nSmit, A., Vrabac, D., He, Y., Ng, A. Y., Beam, A. L., & Rajpurkar, P. (2021). MedSelect: Selective Labeling for \nMedical Image Classification Combining Meta-Learning with Deep Reinforcement Learning. arXiv \npreprint arXiv:2103.14339.  \nStember, J., & Shalu, H. (2020). Deep reinforcement learning to detect brain lesions on MRI: a proof-of-\nconcept application of reinforcement learning to medical images. arXiv preprint \narXiv:2008.02708.  \nStember, J., & Shalu, H. (2021a). Deep reinforcement learning-based image classification achieves perfect \ntesting set accuracy for MRI brain tumors with a training set of only 30 images. arXiv preprint \narXiv:2102.02895.  \nStember, J., & Shalu, H. (2021b). Deep reinforcement learning with automated label extraction from clinical \nreports accurately classifies 3D MRI brain volumes. arXiv preprint arXiv:2106.09812.  \nSun, S., Hu, J., Yao, M., Hu, J., Yang, X., Song, Q., & Wu, X. (2018). Robust multimodal image registration \nusing deep recurrent reinforcement learning. Asian conference on computer vision,  \nSun, S., Hu, J., Yao, M., Hu, J., Yang, X., Song, Q., & Wu, X. (2019). Robust Multimodal Image Registration \nUsing Deep Recurrent Reinforcement Learning. In C. V. Jawahar, H. Li, G. Mori, & K. Schindler, \nComputer Vision ‚Äì ACCV 2018 Cham. \nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.  \nVan Hasselt, H., Guez, A., & Silver, D. (2016). Deep reinforcement learning with double q-learning. \nProceedings of the AAAI Conference on Artificial Intelligence,  \nVithayathil Varghese, N., & Mahmoud, Q. H. (2020). A survey of multi-task deep reinforcement learning. \n \nElectronics, 9(9), 1363.  \nVlontzos, A., Alansary, A., Kamnitsas, K., Rueckert, D., & Kainz, B. (2019). Multiple landmark detection \nusing multi-agent reinforcement learning. International Conference on Medical Image Computing \nand Computer-Assisted Intervention,  \nWang, G., Zuluaga, M. A., Li, W., Pratt, R., Patel, P. A., Aertsen, M., Doel, T., David, A. L., Deprest, J., \nOurselin, S., & others. (2018). DeepIGeoS: a deep interactive geodesic framework for medical image \nsegmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(7), 1559‚Äì1572-\n1559‚Äì1572.  \nWang, J., Miao, J., Yang, X., Li, R., Zhou, G., Huang, Y., Lin, Z., Xue, W., Jia, X., & Zhou, J. (2020). Auto-\nweighting for breast cancer classification in multimodal ultrasound. International Conference on \nMedical Image Computing and Computer-Assisted Intervention,  \nWang, J., Yan, Y., Zhang, Y., Cao, G., Yang, M., & Ng, M. K. (2020). Deep Reinforcement Active Learning for \nMedical Image Classification. International Conference on Medical Image Computing and \nComputer-Assisted Intervention,  \nWang, L., Lekadir, K., Lee, S.-L., Merrifield, R., & Yang, G.-Z. (2013). A general framework for context-\nspecific image segmentation using reinforcement learning. IEEE transactions on medical imaging, \n32(5), 943-956.  \nWang, T., Lei, Y., Fu, Y., Wynne, J. F., Curran, W. J., Liu, T., & Yang, X. (2021). A review on medical \nimaging synthesis using deep learning and its clinical applications. Journal of Applied Clinical \nMedical Physics, 22(1), 11‚Äì36-11‚Äì36.  \nWatkins, C. J. C. H. (1989). Learning from delayed rewards.  \nWilson, S. M., & Anagnostopoulos, D. (2021). Methodological Guidance Paper: The Craft of Conducting a \nQualitative Review. Review of Educational Research, 00346543211012755-00346543211012755.  \nWinkel, D. J., Weikert, T. J., Breit, H.-C., Chabin, G., Gibson, E., Heye, T. J., Comaniciu, D., & Boll, D. T. \n(2020). Validation of a Fully Automated Liver Segmentation Algorithm Using Multi-Scale Deep \nReinforcement Learning and Comparison Versus Manual Segmentation. European Journal of \nRadiology, 126, 108918-108918.  \nXu, B., Liu, J., Hou, X., Liu, B., Garibaldi, J., Ellis, I. O., Green, A., Shen, L., & Qiu, G. (2019). Attention by \nselection: A deep selective attention approach to breast cancer classification. IEEE transactions on \nmedical imaging, 39(6), 1930-1941.  \nXu, C., Zhang, D., Chong, J., Chen, B., & Li, S. (2021). Synthesis of gadolinium-enhanced liver tumors on \nnonenhanced liver MR images using pixel-level graph reinforcement learning. Medical image \nanalysis, 69, 101976.  \nYang, D., Roth, H., Xu, Z., Milletari, F., Zhang, L., & Xu, D. (2019). Searching learning strategy with \nreinforcement learning for 3d medical image segmentation. International Conference on Medical \nImage Computing and Computer-Assisted Intervention,  \nYang, H., Shan, C., & Kolen, A. F. (2020). Deep Q-Network-Driven Catheter Segmentation in 3D US by \nHybrid Constrained Semi-Supervised Learning and Dual-UNet. International Conference on \nMedical Image Computing and Computer-Assisted Intervention,  \nYe, J., Xue, Y., Long, L. R., Antani, S., Xue, Z., Cheng, K. C., & Huang, X. (2020). Synthetic sample selection \nvia reinforcement learning. International Conference on Medical Image Computing and Computer-\nAssisted Intervention,  \nZhang, D., Chen, B., & Li, S. (2021). Sequential conditional reinforcement learning for simultaneous \nvertebral body detection and segmentation with modeling the spine anatomy \n[https://doi.org/10.1016/j.media.2020.101861]. Medical image analysis, 67, 101861-101861.  \nZhao, G., Meyerand, M. E., & Birn, R. M. (2020). Bayesian Conditional GAN for MRI Brain Image Synthesis. \nIn: arXiv. \nZheng, C., Si, X., Sun, L., Chen, Z., Yu, L., & Tian, Z. (2021). Multi-agent reinforcement learning for prostate \nlocalization based on multi-scale image representation. International Symposium on Artificial \nIntelligence and Robotics 2021,  \n \n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2022-06-28",
  "updated": "2022-06-28"
}