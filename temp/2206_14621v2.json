{
  "id": "http://arxiv.org/abs/2206.14621v2",
  "title": "Extracting Weighted Finite Automata from Recurrent Neural Networks for Natural Languages",
  "authors": [
    "Zeming Wei",
    "Xiyue Zhang",
    "Meng Sun"
  ],
  "abstract": "Recurrent Neural Networks (RNNs) have achieved tremendous success in\nsequential data processing. However, it is quite challenging to interpret and\nverify RNNs' behaviors directly. To this end, many efforts have been made to\nextract finite automata from RNNs. Existing approaches such as exact learning\nare effective in extracting finite-state models to characterize the state\ndynamics of RNNs for formal languages, but are limited in the scalability to\nprocess natural languages. Compositional approaches that are scablable to\nnatural languages fall short in extraction precision. In this paper, we\nidentify the transition sparsity problem that heavily impacts the extraction\nprecision. To address this problem, we propose a transition rule extraction\napproach, which is scalable to natural language processing models and effective\nin improving extraction precision. Specifically, we propose an empirical method\nto complement the missing rules in the transition diagram. In addition, we\nfurther adjust the transition matrices to enhance the context-aware ability of\nthe extracted weighted finite automaton (WFA). Finally, we propose two data\naugmentation tactics to track more dynamic behaviors of the target RNN.\nExperiments on two popular natural language datasets show that our method can\nextract WFA from RNN for natural language processing with better precision than\nexisting approaches. Our code is available at\nhttps://github.com/weizeming/Extract_WFA_from_RNN_for_NL.",
  "text": "Extracting Weighted Finite Automata from\nRecurrent Neural Networks for Natural\nLanguages\nZeming Wei, Xiyue Zhang, and Meng Sun(\f)\nPeking University, Beijing 100871, China\nweizeming@stu.pku.edu.cn, {zhangxiyue,sunm}@pku.edu.cn\nAbstract. Recurrent Neural Networks (RNNs) have achieved tremen-\ndous success in sequential data processing. However, it is quite challeng-\ning to interpret and verify RNNs’ behaviors directly. To this end, many\neﬀorts have been made to extract ﬁnite automata from RNNs. Existing\napproaches such as exact learning are eﬀective in extracting ﬁnite-state\nmodels to characterize the state dynamics of RNNs for formal languages,\nbut are limited in the scalability to process natural languages. Composi-\ntional approaches that are scablable to natural languages fall short in ex-\ntraction precision. In this paper, we identify the transition sparsity prob-\nlem that heavily impacts the extraction precision. To address this prob-\nlem, we propose a transition rule extraction approach, which is scalable\nto natural language processing models and eﬀective in improving extrac-\ntion precision. Speciﬁcally, we propose an empirical method to comple-\nment the missing rules in the transition diagram. In addition, we further\nadjust the transition matrices to enhance the context-aware ability of\nthe extracted weighted ﬁnite automaton (WFA). Finally, we propose two\ndata augmentation tactics to track more dynamic behaviors of the target\nRNN. Experiments on two popular natural language datasets show that\nour method can extract WFA from RNN for natural language processing\nwith better precision than existing approaches. Our code is available at\nhttps://github.com/weizeming/Extract_WFA_from_RNN_for_NL.\nKeywords: Abstraction · Weighted Finite Automata · Natural Lan-\nguage Models\n1\nIntroduction\nIn the last decade, deep learning (DL) has been widely deployed in a range of ap-\nplications, such as image processing [12], speech recognition [1] and natural lan-\nguage processing [11]. In particular, recurrent neural networks (RNNs) achieve\ngreat success in sequential data processing, e.g., time series forecasting [5], text\nclassiﬁcation [24] and language translation [6]. However, the complex internal\ndesign and gate control of RNNs make the interpretation and veriﬁcation of\ntheir behaviors rather challenging. To this end, much progress has been made\narXiv:2206.14621v2  [cs.CL]  27 Sep 2022\nto abstract RNN as a ﬁnite automaton, which is a ﬁnite state model with ex-\nplicit states and transition matrix to characterize the behaviours of RNN in\nprocessing sequential data. The extracted automaton also provides a practical\nfoundation for analyzing and verifying RNN behaviors, based on which existing\nmature techniques, such as logical formalism [10] and model checking [3], can be\nleveraged for RNN analysis.\nUp to the present, a series of extraction approaches leverage explicit learning\nalgorithms (e.g., L∗algorithm [2]) to extract a surrogate model of RNN. Such\nexact learning procedure has achieved great success in capturing the state dy-\nnamics of RNNs for processing formal languages [25,26,17]. However, the com-\nputational complexity of the exact learning algorithm limits its scalability to\nconstruct abstract models from RNNs for natural language tasks.\nAnother technical line of automata extraction from RNNs is the composi-\ntional approach, which uses unsupervised learning algorithms to obtain discrete\npartitions of RNNs’ state vectors and construct the transition diagram based\non the discrete clusters and concrete state dynamics of RNNs. This approach\ndemonstrates better scalability and has been applied to robustness analysis and\nrepairment of RNNs on large-scale tasks [23,22,9,7,8,27].\nAs a trade-oﬀto the computational complexity, the compositional approach\nis faced with the problem of extraction consistency. Moreover, the alphabet size\nof natural language datasets is far larger than formal languages, but the extrac-\ntion procedure is based on ﬁnite (sequential) data. As a result, the transition\ndynamics are usually scarce when processing low-frequency tokens (words).\nHowever, the transition sparsity of the extracted automata for natural lan-\nguage tasks is yet to be addressed. In addition, state-of-the-art RNNs such as\nlong short-term memory networks [13] show their great advantages on tracking\nlong term context dependency for natural language processing, but the abstrac-\ntion procedure inevitably leads to context information loss. This motivates us\nto propose a heuristic method for transition rule adjustment to enhance the\ncontext-aware ability of the extracted model.\nIn this paper, we propose an approach to extracting transition rules of weighted\nﬁnite automata from RNNs for natural languages with a focus on the transition\nsparsity problem and the loss of context dependency. Empirical investigation in\nSection 5 shows that the sparsity problem of transition diagram severely im-\npacts the behavior consistency between RNN and the extracted model. To deal\nwith the transition sparsity problem that no transition rules are learned at a\ncertain state for a certain word, which we refer to as missing rows in transi-\ntion matrices, we propose a novel method to ﬁll in the transition rules for the\nmissing rows based on the semantics of abstract states. Further, in order to en-\nhance the context awareness ability of WFA, we adjust the transition matrices\nto preserve part of the context information from the previous state, especially in\nthe case of transition sparsity when the current transition rules cannot be relied\non completely. Finally, we propose two tactics to augment the training samples\nto learn more transition behaviors of RNNs, which also alleviate the transition\nsparsity problem. Overall, our approach for transition rule extraction leads to\nbetter extraction consistency and can be applied to natural language tasks.\nTo summarize, our main contributions are:\n(a) A novel approach to extracting transition rules of WFA from RNNs to ad-\ndress the transition sparsity problem;\n(b) An heuristic method of adjusting transition rules to enhance the context-\naware ability of WFA;\n(c) A data augmentation method on training samples to track more transition\nbehaviors of RNNs.\nThe organization of this paper is as follows. In Section 2, we show prelim-\ninaries about recurrent neural networks, weighted ﬁnite automata, and related\nnotations and concepts. In Section 3, we present our transition rule extraction\napproach, including a generic outline on the automata extraction procedure, the\ntransition rule complement approach for transition sparsity, the transition rule\nadjustment method for context-aware ability enhancement. We then present the\ndata augmentation tactics in Section 4 to reinforce the learning of dynamic be-\nhaviors from RNNs, along with the computational complexity analysis of the\noverall extraction approach. In Section 5, we present the experimental evalua-\ntion towards the extraction consistency of our approach on two natural language\ntasks. Finally, we discuss related works in Section 6 and conclude our work in\nSection 7.\n2\nPreliminaries\nIn this section, we present the notations and deﬁnitions that will be used through-\nout the paper.\nGiven a ﬁnite alphabet Σ, we use Σ∗to denote the set of sequences over\nΣ and ε to denote the empty sequence. For w ∈Σ∗, we use |w| to denote its\nlength, its i-th word as wi and its preﬁx with length i as w[: i]. For x ∈Σ, w · x\nrepresents the concatenation of w and x.\nDeﬁnition 1 (RNN). A Recurrent Neural Network (RNN) for natural lan-\nguage processing is a tuple R = (X, S, O, f, p), where X is the input space; S is\nthe internal state space; O is the probabilistic output space; f : S × X →S is\nthe transition function; p : S →O is the prediction function.\nRNN Conﬁguration. In this paper, we consider RNN as a black-box model and\nfocus on its stepwise probabilistic output for each input sequence. The following\ndeﬁnition of conﬁguration characterizes the probabilistic outputs in response to\na sequential input fed to RNN. Given an alphabet Σ, let ξ : Σ →X be the\nfunction that maps each word in Σ to its embedding vector in X. We deﬁne\nf ∗: S × Σ∗→S recursively as f ∗(s0, ξ(w · x)) = f(f ∗(s0, ξ(w)), ξ(x)) and\nf ∗(s0, ε) = s0, where s0 is the initial state of R. The RNN conﬁguration δ :\nΣ∗→O is deﬁned as δ(w) = p(f ∗(s0, w)).\nOutput Trace. To record the stepwise behavior of RNN when processing an in-\nput sequence w, we deﬁne the Output Trace of w, i.e., the probabilistic output\nsequence, as T(w) = {δ(w[: i])}|w|\ni=1. The i-th item of T(w) indicates the proba-\nbilistic output given by R after taking the preﬁx of w with length i as input.\nDeﬁnition 2 (WFA). Given a ﬁnite alphabet Σ, a Weighted Finite Automaton\n(WFA) over Σ is a tuple A = ( ˆS, Σ, E, ˆs0, I, F), where ˆS is the ﬁnite set of\nabstract states; E = {Eσ|σ ∈Σ} is the set of transition matrix Eσ with size\n| ˆS| × | ˆS| for each token σ ∈Σ; ˆs0 ∈ˆS is the initial state; I is the initial vector,\na row vector with size | ˆS|; F is the ﬁnal vector, a column vector with size | ˆS|.\nAbstract States. Given a RNN R and a dataset D, let ˆO denote all stepwise\nprobabilistic outputs given by executing R on D, i.e. ˆO =\nS\nw∈D\nT(w). The ab-\nstraction function λ : ˆO →ˆS maps each probabilistic output to an abstract state\nˆs ∈ˆS. As a result, the output set is divided into a number of abstract states by\nλ. For each ˆs ∈ˆS, the state ˆs has explicit semantics that the probabilistic out-\nputs corresponding to ˆs has similar distribution. In this paper, we leverage the\nk-means algorithm to construct the abstraction function. We cluster all proba-\nbilistic outputs in ˆO into some abstract states. In this way, we construct the set\nof abstract states ˆS with these discrete clusters and an initial state ˆs0.\nFor a state ˆs ∈ˆS, we deﬁne the center of ˆs as the average value of the\nprobabilistic outputs ˆo ∈ˆO which are mapped to ˆs. More formally, the center of\nˆs is deﬁned as follows:\nρ(ˆs) = Avg\nλ(ˆo)=ˆs\n{ˆo}.\nThe center ρ(ˆs) represents an approximation for the distribution tendency of\nprobabilistic outputs ˆo in ˆs. For each state ˆs ∈ˆS, we use the center ρ(ˆs) as its\nweight, as the center captures an approximation of the distribution tendency of\nthis state. Therefore, the ﬁnal vector F is (ρ(ˆs0), ρ(ˆs1), · · · , ρ(ˆs| ˆS|−1))t.\nAbstract Transitions. In order to capture the dynamic behavior of RNN R, we\ndeﬁne the abstract transition as a triple (ˆs, σ, ˆs′) where the original state ˆs is the\nabstract state corresponding to a speciﬁc output y, i.e. ˆs = λ(y); σ is the next\nword of the input sequence to consume; ˆs′ is the destination state λ(y′) after R\nreads σ and outputs y′. We use T to denote the set of all abstract transitions\ntracked from the execution of R on training samples.\nAbstract Transition Count Matrices. For each word σ ∈Σ, the abstract tran-\nsition count matrix of σ is a matrix ˆTσ with size | ˆS| × | ˆS|. The count matrices\nrecords the number of times that each abstract transition triggered. Given the\nset of abstract transitions T , the count matrix of σ can be calculated as\nˆTσ[i, j] = T .count((ˆsi, σ, ˆsj)),\n1 ≤i, j ≤| ˆS|.\nAs for the remaining components, the alphabet Σ is consistent with the\nalphabet of training set D. The initial vector I is formulated according to the\ninitial state ˆs0.\nFig. 1. An illustration of our approach to extracting WFA from RNN.\nFor an input sequence w = w1w2 · · · wn ∈Σ∗, the WFA will calculate its\nweight following\nI · Ew1 · Ew2 · · · Ewn · P.\n3\nWeighted Automata Extraction Scheme\n3.1\nOutline\nWe present the workﬂow of our extraction procedure in Fig. 1. As the ﬁrst step,\nwe generate augmented sample set D from the original training set D0 to enrich\nthe transition dynamics of RNN behaviors and alleviate the transition sparsity.\nThen, we execute RNN R on the augmented sample set D, and record the\nprobabilistic output trace T(w) of each input sentence w ∈D. With the output\nset ˆO =\nS\nw∈D\nT(w), we cluster the probabilistic outputs into abstract states ˆS,\nand generate abstract transitions T from the output traces {T(w)|w ∈D}. All\ntransitions constitute the abstract transition count matrices ˆTσ for all σ ∈Σ.\nNext, we construct the transition matrices E = {Eσ|σ ∈Σ}. Based on the\nabstract states ˆS and count matrices ˆT, we construct the transition matrix Eσ\nfor each word σ ∈Σ. Speciﬁcally, we use frequencies to calculate the transition\nprobabilities. Suppose that there are n abstract states in ˆS. The i-th row of Eσ,\nwhich indicates the probabilistic transition distribution over states when R is in\nstate ˆsi and consumes σ, is calculated as\nEσ[i, j] =\nˆTσ[i, j]\nnP\nk=1\nˆTσ[i, k]\n.\n(1)\nThis empirical rule faces the problem that the denominator of (1) could be zero,\nwhich means that the word σ never appears when the RNN R is in abstract\nstate ˆsi. In this case, one should decide how to ﬁll in the transition rule of the\nmissing rows in Eσ. In Section 3.2, we present a novel approach for transition\nrule complement. Further, to preserve more contextual information in the case\nof transition sparsity, we propose an approach to enhancing the context-aware\nability of WFA by adjusting the transition matrices, which is discussed in Section\n3.3. Note that our approach is generic and could be applied to other RNNs\nbesides the domain of natural language processing.\n3.2\nMissing Rows Complementing\nExisting approaches for transition rule extraction usually face the problem of\ntransition sparsity, i.e., missing rows in the transition diagram. In the context of\nformal languages, the probability of occurrence of missing rows is quite limited,\nsince the size of the alphabet is small and each token in the alphabet can appear\nsuﬃcient number of times. However, in the context of natural language process-\ning, the occurrence of missing rows is quite frequent. The following proposition\ngives an approximation of the occurrence frequency of missing rows.\nProposition 1. Assume an alphabet Σ with m = |Σ| words, a natural language\ndataset D over Σ which has N words in total, a RNN R trained on D, the\nextracted abstract states ˆS and transitions T . Let σi denote the i-th most frequent\nword occurred in D and ti = T .count((∗, σi, ∗)) indicates the occurrence times\nof σi in D. The median of {ti|1 ≤i ≤m} can be estimated as\nt[ m\n2 ] =\n2N\nm · ln m.\nProof. The Zipf’s law [21] shows that\nti\nN ≈\ni−1\nm\nP\nk=1\nk−1\n.\nNote that\nm\nP\nk=1\nk−1 ≈ln m and take i to be m\n2 , we complete our proof.\nExample 1. In the QC news dataset [16], which has m = 20317 words in its\nalphabet and N = 205927 words in total, the median of {ti} is approximated\nto\n2N\nm·ln m ≈2. This indicates that about half of Eσ are constructed with no\nmore than 2 transitions. In practice, the number of abstract states is usually far\nmore than the transition numbers for these words, making most of rows of their\ntransition matrices missing rows.\nFilling the missing row with ⃗0 is a simple solution, since no information\nwere provided from the transitions. However, as estimated above, this solution\nwill lead to the problem of transition sparsity, i.e., the transition matrices for\nuncommon words are nearly null. Consequently, if the input sequence includes\nsome uncommon words, the weights over states tend to vanish. We refer to this\nsolution as null ﬁlling.\nAnother simple idea is to use the uniform distribution over states for fairness.\nIn [26], the uniform distribution is used as the transition distribution for unseen\ntokens in the context of formal language tasks. However, for natural language\nprocessing, this solution still loses information of the current word, despite that\nit avoids the weight vanishment over states. We refer to this solution as uniform\nﬁlling. [29] uses the synonym transition distribution for an unseen token at a\ncertain state. However, it increases the computation overhead when performing\ninference on test data, since it requires to calculate and sort the distance between\nthe available tokens at a certain state and the unseen token.\nTo this end, we propose a novel approach to constructing the transition\nmatrices based on two empirical observations. First, each abstract state ˆs ∈ˆS\nhas explicit semantics, i.e. the probabilistic distribution over labels, and similar\nabstract states tend to share more similar transition behaviours. The similarity\nof abstract states is deﬁned by their semantic distance as follows.\nDeﬁnition 3 (State Distance). For two abstract states ˆs1 and ˆs2, the distance\nbetween ˆs1 and ˆs2 is deﬁned by the Euclidean distance between their center:\ndist(ˆs1, ˆs2) = ∥ρ(ˆs1) −ρ(ˆs2)∥2.\nWe calculate the distance between each pair of abstract states, which forms\na distance matrix M where each element M[i, j] = dist(ˆsi, ˆsj) for 1 ≤i, j ≤| ˆS|.\nFor a missing row in Eσ, following the heuristics that similar abstract states are\nmore likely to have similar behaviors, we observe the transition behaviors from\nother abstract states, and simulate the missing transition behaviors weighted\nby distance between states. Particularly, in order to avoid numerical underﬂow,\nwe leverage softmin on distance to bias the weight to states that share more\nsimilarity. Formally, for a missing row Eσ[i], the weight of information set for\nanother row Eσ[j] is deﬁned by e−M[i,j].\nSecond, it is also observed that sometimes the RNN just remains in the cur-\nrent state after reading a certain word. Intuitively, this is because part of words\nin the sentence do not deliver signiﬁcant information in the task. Therefore, we\nconsider simulating behaviors from other states whilst remaining in the current\nstate with a certain probability.\nIn order to balance the trade-oﬀbetween referring to behaviors from other\nstates and remaining still, we introduce a hyper-parameter β named reference\nrate, such that when WFA is faced with a missing row, it has a probability of β\nto refer to the transition behaviors from other states, and in the meanwhile has\na probability of 1 −β to keep still. We select the parameter β according to the\nproportion of self-transitions, i.e., transitions (ˆs, σ, ˆs′) in T where ˆs = ˆs′.\nTo sum up, the complete transition rule for the missing row is\nEσ[i, j] = β ·\nnP\nk=1\ne−M[i,k] · ˆTσ[k, j]\nnP\nl=1\nnP\nk=1\ne−M[i,k] · ˆTσ[k, l]\n+ (1 −β) · δi,j.\n(2)\nHere δi,j is the Kronecker symbol:\nδi,j =\n(\n1,\nj = i\n0,\nj ̸= i .\nIn practice, we can calculate\nnP\nk=1\ne−M[i,k] · ˆTσ[k, j] for each j and then make\ndivision on their summation once and for all, which can reduce the computation\noverhead on transition rule extraction.\n3.3\nContext-Aware Enhancement\nFor NLP tasks, the memorization of long-term context information is crucial.\nOne of the advantages of RNN and its advanced design LSTM networks is the\nability to capture long-term dependency. We expect the extracted WFA to simu-\nlate the step-wise behaviors of RNNs whilst keeping track of context information\nalong with the state transition. To this end, we propose an approach to adjusting\nthe transition matrix such that the WFA can remain in the current state with a\ncertain probability.\nSpeciﬁcally, we select a hyper-parameter α ∈[0, 1] as the static probability.\nFor each word σ ∈Σ and its transition matrix Eσ, we replace the matrix with\nthe context-aware enhanced matrix ˆEσ as follows:\nˆEσ = α · In + (1 −α) · Eσ\n(3)\nwhere In is the identity matrix.\nThe context-aware enhanced matrix has explicit semantics. When the WFA\nis in state ˆsi and ready to process a new word σ, it has a probability of α (the\nstatic probability) to remain in ˆsi, or follows the original transition distribution\nEσ[i, j] with a probability 1 −α.\nHere we present an illustration of how context-aware enhanced matrices de-\nliver long-term context information. Suppose that a context-aware enhanced\nWFA A is processing a sentence w ∈Σ∗with length |w|. We denote di as the\ndistribution over all abstract states after A reads the preﬁx w[: i], and particu-\nlarly d0 = I is the initial vector of A. We use Zi to denote the decision made by\nA based on di−1 and the original transition matrix Ewi. Formally, di = di−1· ˆEwi\nand Zi = di−1 · Ewi.\nThe di can be regarded as the information obtained from the preﬁx w[: i] by\nA before it consumes wi+1, and Zi can be considered as the decision made by\nA after it reads wi.\nProposition 2. The i-th step-wise information di which delivered by processing\nw[: i] contains the decision information Zj of preﬁx w[: j] with a proportion of\n(1 −α) · αi−j, 1 ≤j ≤i.\nProof. Since ˆEwi = α · In + (1 −α) · Ewi, we can calculate that\ndi = di−1 · ˆEwi = di−1 · [α · In + (1 −α) · Ewi] = α · di−1 + (1 −α) · Zi.\n(4)\nUsing (4) recursively, we have\ndi = (1 −α)\ni\nX\nk=1\nαi−k · Zk + αi · I.\nThis shows the information delivered by w[: i] refers to the decision made by\nA on each preﬁx included in w[: i], and the portion vanishes exponentially. The\neﬀectiveness of the context-aware transition matrix adjustment method will be\ndiscussed in Section 5.\nThe following example presents the complete approach for transition rule\nextraction, i.e., to generate the transition matrix ˆEσ with the missing row ﬁlled\nin and context enhanced, from the count matrix ˆTσ for a word σ ∈Σ.\nExample 2. Assume that there are three abstract states in ˆS = {ˆs1, ˆs2, ˆs3}.\nSuppose the count matrix for σ is ˆTσ.\nˆTσ =\n\n\n1 3 0\n1 1 0\n0 0 0\n\n, Eσ =\n\n\n0.25 0.75 0\n0.5 0.5\n0\n0.15 0.35 0.5\n\n, ˆEσ =\n\n\n0.4 0.6\n0\n0.4 0.6\n0\n0.12 0.28 0.6\n\n.\nFor the ﬁrst two rows (states), there exist transitions for σ, thus we can calculate\nthe transition distribution of these two rows in Eσ in the usual way. However, the\nthird row is a missing row. We set the reference rate as β = 0.5, and suppose that\nthe distance between states satisﬁes e−M[1,3] = 2e−M[2,3], generally indicating\nthe distance between ˆs1 and ˆs3 is nearer than ˆs2 and ˆs3. With the transitions\nfrom ˆs1 and ˆs2, we can complement the transition rule of the third row in Eσ\nthrough (2). The result shows that the behavior from ˆs3 is more similar to ˆs1\nthan ˆs2, due to the nearer distance. Finally, we construct ˆEσ with Eσ. Here we\ntake the static probability α = 0.2, thus ˆEσ = 0.2·I3 +0.8·Eσ. The result shows\nthat the WFA with ˆEσ has higher probability to remain in the current state\nafter consuming σ, which can preserve more information from the preﬁx before\nσ.\n4\nData Augmentation\nOur proposed approach for transition rule extraction provides a solution to the\ntransition sparsity problem. Still, we hope to learn more dynamic transition\nbehaviors from the target RNN, especially for the words with relatively low\nfrequency to characterize their transition dynamics suﬃciently based on the ﬁnite\ndata samples. Diﬀerent from formal languages, we can generate more natural\nlanguage samples automatically, as long as the augmented sequential data are\nsensible with clear semantics and compatible with the original learning task.\nBased on the augmented samples, we are able to track more behaviors of the\nRNN and build the abstract model with higher precision. In this section, we\nintroduce two data augmentation tactics for natural language processing tasks:\nSynonym Replacement and Dropout.\nSynonym Replacement. Based on the distance quantization among the word\nembedding vectors, we can obtain a list of synonyms for each word in Σ. For a\nword σ ∈Σ, the synonyms of w are deﬁned as the top k most similar words of σ\nin Σ, where k is a hyper-parameter. The similarity among the words is calculated\nbased on the Euclidean distance between the word embedding vectors over Σ.\nGiven a dataset D0 over Σ, for each sentence w ∈D0, we generate a new\nsentence w′ by replacing some words in w with their synonyms. Note that we\nhope that the uncommon words in Σ should appear more times, so as to gather\nmore dynamic behaviors of RNNs when processing such words. Therefore, we\nset the probability that a certain word σ ∈w gets replaced to be in a negative\ncorrelation to its frequency of occurrence, i.e. the i-th most frequent word is\nreplaced with a probability\n1\ni+1.\nDropout. Inspired by the regularization technique dropout, we also propose\na similar tactic to generate new sentences from D0. Initially, we introduce a\nnew word named unknown word and denote it as ⟨unk⟩. For the sentence in\nw ∈D0 that has been processed by synonym replacing, we further replace the\nwords that hasn’t been replaced with ⟨unk⟩with a certain probability. Finally,\nnew sentences generated by both synonym replacement and dropout form the\naugmented dataset D.\nWith the dropout tactic, we can observe the behaviors of RNNs when it\nprocesses an unknown word ˆσ ̸∈Σ that hasn’t appeared in D0. Therefore, the\nextracted WFA can also show better generalization ability. An example of gen-\nerating a new sentence from D0 is shown as follows.\nExample 3. Consider a sentence w from the original training set D0, w =[“I”,\n“really”, “like”, “this”, “movie”]. First, the word “like” is chosen to be replaced by\none of its synonym “appreciate”. Next, the word “really” is dropped from the sen-\ntence, i.e. replaced by the unknown word ⟨unk⟩. Finally, we get a new sentence\nw′ =[“I”, “⟨unk⟩”, “appreciate”, “this”, “movie”] and put it into the augmented\ndataset D.\nSince the word “appreciate” may be an uncommon word in Σ, we can capture\na new transition information for it given by RNNs. Besides, we can also observe\nthe behavior of RNN when it reads an unknown word after the preﬁx [“I”].\nComputational Complexity. The time complexity of the whole workﬂow is\nanalyzed as follows. Suppose that the set of training samples D0 has N words\nin total and its alphabet Σ contains n words, and is augmented as D with t\nepochs (i.e. each sentence in D0 is transformed to t new sentences in D), hence\n|D| = (t + 1)N. Assume that a probabilistic output of RNNs is a m-dim vector,\nand the abstract states set ˆS contains k states.\nTo start with, the augmentation of D0 and tracking of probabilistic outputs\nin D will be completed in O(|D|) = O(t · N) time. Besides, the time complexity\nof k-means clustering algorithm is O(k ·|D|) = O(k ·t·N). The count of abstract\ntransitions will be done in O(n). As for the processing of transition matrices, we\nneed to calculate the transition probability for each word σ with each source state\nˆsi and destination state ˆsj, which costs O(k2·n) time. Finally, the context-aware\nenhancement on transition matrices takes O(k · n) time.\nNote that O(n) = O(N), hence we can conclude that the time complexity of\nour whole workﬂow is O(k2 · t · N). So the time complexity of our approaches\nonly takes linear time w.r.t. the size of the dataset, which provides theoretical\nextraction overhead for large-scale data applications.\n5\nExperiments\nIn this section, we evaluate our extraction approach on two natural language\ndatasets and demonstrate its performance in terms of precision and scalability.\nDatasets and RNNs models. We select two popular datasets for NLP tasks and\ntrain the target RNNs on them.\n1. The CogComp QC Dataset (abbrev. QC) [16] contains news titles which\nare labeled with diﬀerent topics. The dataset is divided into a training set\ncontaining 20k samples and a test set containing 8k samples. Each sample is\nlabeled with one of seven categories. We train an LSTM-based RNN model\nR on the training set, which achieves an accuracy of 81% on the test set.\n2. The Jigsaw Toxic Comment Dataset (abbrev. Toxic) [15] contains comments\nfrom Wikipedia’s talk page edits, with each comment labeled toxic or not.\nWe select 25k non-toxic samples and toxic samples respectively, and divide\nthem into the training set and test set in a ratio of four to one. We train\nanother LSTM-based RNN model which achieves 90% accuracy.\nMetrics. For the purpose of representing the behaviors of RNNs better, we use\nConsistency Rate (CR) as our evaluation metric. For a sentence in the test set\nw ∈Dtest, we denote R(w) and A(w) as the prediction results of the RNNs and\nWFA, respectively. The Consistency Rate is deﬁned formally as\nCR = |{w ∈Dtest : A(w) = R(w)}|\n|Dtest|\n.\nMissing Rows Complementing. As discussed in Section 3.2, we take two\napproaches as baselines, the null ﬁlling and the uniform ﬁlling. The two WFA\nextracted with these two approaches are denoted as A0 and AU, respectively.\nThe WFA extracted by our empirical ﬁlling approach is denoted as AE.\nTable 1 shows the evaluation results of three rule ﬁlling approaches. We\nconduct the comparison experiments on QC and Toxic datasets and select the\ncluster number for state abstraction as 40 and 20 for the QC and Toxic datasets,\nrespectively.\nThe three columns labeled with the type of WFA show the evaluation results\nof diﬀerent approaches. For the A0 based on blank ﬁlling, the WFA returns the\nTable 1. Evaluation results of diﬀerent ﬁlling approaches on missing rows.\nDataset\nA0\nAU\nAE\nCR(%)\nTime(s)\nCR(%)\nTime(s)\nCR(%)\nTime(s)\nQC\n26\n47\n60\n56\n80\n70\nToxic\n57\n167\n86\n180\n91\n200\nweight of most sentences in D with ⃗0, which fails to provide suﬃcient information\nfor prediction. For the QC dataset, only a quarter of sentences in the test set\nare classiﬁed correctly. The second column shows that the performance of AU\nis better than A0. The last column presents the evaluation result of AE, which\nﬁlls the missing rows by our approach. In this experiment, the hyper-parameter\nreference rate is set as β = 0.3. We can see that our empirical approach achieves\nsigniﬁcantly better accuracy, which is 20% and 5% higher than uniform ﬁlling\non the two datasets, respectively.\nThe columns labeled Time show the execution time of the whole extraction\nworkﬂow, from tracking transitions to evaluation on test set, but not include the\ntraining time of RNNs. We can see that the extraction overhead of our approach\n(AE) is about the same as AU and A0.\nContext-Aware Enhancement. In this experiment, we leverage the context-\naware enhanced matrices when constructing the WFA. We adopt the same con-\nﬁguration on cluster numbers n from the comparison experiments above, i.e.\nn = 40 and n = 20. The columns titled Conﬁguration indicate if the extracted\nWFA leverage context-aware matrices. We also take the WFA with diﬀerent\nﬁlling approaches, the uniform ﬁlling and empirical ﬁlling, into comparison. Ex-\nperiments on null ﬁlling is omitted due to limited precision.\nTable 2. Evaluation results of with and without context-aware enhancement.\nDataset\nConﬁguration\nAU\nAE\nCR(%)\nTime(s)\nCR(%)\nTime(s)\nQC\nNone\n60\n56\n80\n70\nContext\n71\n64\n82\n78\nToxic\nNone\n86\n180\n91\n200\nContext\n89\n191\n92\n211\nThe experiment results are in Table 2. For the QC dataset, we set the static\nprobability as α = 0.4. The consistency rate of WFA AU improves 11% with\nthe context-aware enhancement, and AE improves 2%. As for the Toxic dataset,\nwe take α = 0.2 and the consistency rate of the two WFA improves 3% and 1%\nrespectively. This shows that the WFA with context-aware enhancement remains\nmore information from the preﬁxes of sentences, making it simulate RNNs better.\nStill, the context-aware enhancement processing costs little time, since we\nonly calculate the adjusting formula (3) for each Eσ in E. The additional extra\ntime consumption is 8s for the QC dataset and 11s for the Toxic dataset.\nData Augmentation Finally, we evaluate the WFA extracted with transition\nbehaviors from augmented data. Note that the two experiments above are based\non the primitive training set D0. In this experiment, we leverage the data aug-\nmentation tactics to generate the augmented training set D, and extract WFA\nwith data samples from D. In order to get best performance, we build WFA with\ncontextual-aware enhanced matrices.\nTable 3. Evaluation results of with and without data augmentation.\nDataset\nSamples\nAU\nAE\nCR(%)\nTime(s)\nCR(%)\nTime(s)\nQC\nD0\n71\n64\n82\n68\nD\n76\n81\n84\n85\nToxic\nD0\n89\n191\n92\n211\nD\n91\n295\n94\n315\nTable 3 shows the results of consistency rate of WFA extracted with and\nwithout augmented data. The rows labeled D0 show the results of WFA that\nare extracted with the primitive training set, and the result from the augmented\ndata is shown in rows labeled D. With more transition behaviors tracked, the\nWFA extracted with D demonstrates better precision. Speciﬁcally, the WFA\nextracted with both empirical ﬁlling and context-aware enhancement achieves a\nfurther 2% increase in consistency rate on the two datasets.\nTo summarize, by using our transition rule extraction approach, the consis-\ntency rate of extracted WFA on the QC dataset and the Toxic dataset achieves\n84% and 94%, respectively. Taking the primitive extraction algorithm with uni-\nform ﬁlling as baseline, of which experimental results in terms of CR are 60%\nand 86%, our approach achieves an improvement of 22% and 8% in consistency\nrate. As for the time complexity, the time consumption of our approach increases\nfrom 56s to 81s on QC dataset, and from 180s to 315s on Toxic dataset, which\nindicates the eﬃciency and scalability of our rule extraction approach. There is\nno signiﬁcant time cost of adopting our approach further for complicated natural\nlanguage tasks. We can conclude that our transition rule extraction approach\nmakes better approximation of RNNs, and is also eﬃcient enough to be applied\nto practical applications for large-scale natural language tasks.\n6\nRelated Work\nMany research eﬀorts have been made to abstract, verify and repair RNNs. As\nJacobsson reviewed in [14], the rule extraction approach of RNNs can be divided\ninto two categories: pedagogical approaches and compositional approaches.\nPedagogical Approaches. Much progress has been achieved by using pedagogi-\ncal approaches to abstracting RNNs by leveraging explicit learning algorithms,\nsuch as the L∗algorithm [2]. The earlier work dates back to two decades ago,\nwhen Omlin et al. attempted to extract a ﬁnite model from Boolean-output\nRNNs [20,18,19]. Recently, Weiss et al. proposed to levergae the L∗algorithm to\nextract DFA from RNN-acceptors [25]. Later, they presented a weighted exten-\nsion of L∗algorithm that extracted probabilistic determininstic ﬁnite automata\n(PDFA) from RNNs [26]. Besides, Okudono et al. proposed another weighted\nextension of L∗algorithm to extract WFA from real-value-output RNNs [17].\nThe pedagogical approaches have achieved great success in abstracting RNNs\nfor small-scale languages, particularly formal languages. Such exact learning ap-\nproaches have intrinsic limitation in the scalability of the language complexity,\nhence they are not suitable for automata extraction from natural language pro-\ncessing models.\nCompositional Approach. Another technical line is the compositional approach,\nwhich generally leverages unsupervised algorithms (e.g. k-means, GMM) to clus-\nter state vectors as abstract states [28,4]. Wang et al. studied the key factors\nthat inﬂuence the reliability of extraction process, and proposed an empirical\nrule to extract DFA from RNNs [23]. Later, Zhang et al. followed the state\nencoding of compositional approach and proposed a WFA extraction approach\nfrom RNNs [29], which can be applied to both grammatical languages and nat-\nural languages. In this paper, our proposal of extracting WFA from RNNs also\nfalls into the line of compositional approach, but aims at proposing transition\nrule extraction method to address the transition sparsity problem and enhance\nthe context-aware ability.\nRecently, many of the veriﬁcation, analysis and repairment works also lever-\nage similar approaches to abstract RNNs as a more explicit model, such as\nDeepSteller [9], Marble [8] and RNNsRepair [27]. These works achieve great\nprogress in analyzing and repairing RNNs, but have strict requirements of scala-\nbility to large-scale tasks, particularly natural language processing. The proposed\napproach, which demonstrates better precision and scalability, shows great po-\ntential for further applications such as RNN analysis and Network repairment.\nWe consider applying our method to RNN analysis as future work.\n7\nConclusion\nThis paper presents a novel approach to extracting transition rules of weighted\nﬁnite automata from recurrent neural networks. We measure the distance be-\ntween abstract states and complement the transition rules of missing rows. In\naddition, we present an heuristic method to enhance the context-aware ability of\nthe extracted WFA. We further propose two augmentation tactics to track more\ntransition behaviours of RNNs. Experiments on two natural language datasets\nshow that the WFA extracted with our approach achieve better consistency with\ntarget RNNs. The theoretical estimation of computation complexity and exper-\nimental results demonstrate that our rule extraction approach can be applied to\nnatural language datasets and complete the extraction procedure eﬃciently for\nlarge-scale tasks.\nAcknowledgements This research was sponsored by the National Natural\nScience Foundation of China under Grant No. 62172019, 61772038, and CCF-\nHuawei Formal Veriﬁcation Innovation Research Plan.\nReferences\n1. Abdel-Hamid, O., Mohamed, A.r., Jiang, H., Deng, L., Penn, G., Yu, D.: Convolu-\ntional neural networks for speech recognition. IEEE/ACM Transactions on audio,\nspeech, and language processing 22(10), 1533–1545 (2014)\n2. Angluin, D.: Learning regular sets from queries and counterexamples. Information\nand computation 75(2), 87–106 (1987)\n3. Baier, C., Katoen, J.P.: Principles of model checking. MIT press (2008)\n4. Cechin, A.L., Regina, D., Simon, P., Stertz, K.: State automata extraction from\nrecurrent neural nets using k-means and fuzzy clustering. In: 23rd International\nConference of the Chilean Computer Science Society, 2003. SCCC 2003. Proceed-\nings. pp. 73–78. IEEE (2003)\n5. Che, Z., Purushotham, S., Cho, K., Sontag, D., Liu, Y.: Recurrent neural networks\nfor multivariate time series with missing values. Scientiﬁc reports 8(1), 1–12 (2018)\n6. Datta, D., David, P.E., Mittal, D., Jain, A.: Neural machine translation using re-\ncurrent neural network. International Journal of Engineering and Advanced Tech-\nnology 9(4), 1395–1400 (2020)\n7. Dong, G., Wang, J., Sun, J., Zhang, Y., Wang, X., Dai, T., Dong, J.S., Wang,\nX.: Towards interpreting recurrent neural networks through probabilistic abstrac-\ntion. In: 2020 35th IEEE/ACM International Conference on Automated Software\nEngineering (ASE). pp. 499–510. IEEE (2020)\n8. Du, X., Li, Y., Xie, X., Ma, L., Liu, Y., Zhao, J.: Marble: Model-based robustness\nanalysis of stateful deep learning systems. In: Proceedings of the 35th IEEE/ACM\nInternational Conference on Automated Software Engineering. pp. 423–435 (2020)\n9. Du, X., Xie, X., Li, Y., Ma, L., Liu, Y., Zhao, J.: Deepstellar: Model-based quanti-\ntative analysis of stateful deep learning systems. In: Proceedings of the 2019 27th\nACM Joint Meeting on European Software Engineering Conference and Sympo-\nsium on the Foundations of Software Engineering. pp. 477–487 (2019)\n10. Gastin, P., Monmege, B.: A unifying survey on weighted logics and weighted au-\ntomata. Soft Computing 22(4), 1047–1065 (2018)\n11. Goldberg, Y.: Neural network methods for natural language processing. Synthesis\nlectures on human language technologies 10(1), 1–309 (2017)\n12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) (June 2016)\n13. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation\n9(8), 1735–1780 (1997)\n14. Jacobsson, H.: Rule extraction from recurrent neural networks: Ataxonomy and\nreview. Neural Computation 17(6), 1223–1263 (2005)\n15. Jigsaw: Toxic comment classiﬁcation challenge, https://www.kaggle.com/c/\njigsaw-toxic-comment-classification-challenge Accessed April 16, 2022\n16. Li, X., Roth, D.: Learning question classiﬁers. In: COLING 2002: The 19th Inter-\nnational Conference on Computational Linguistics (2002)\n17. Okudono, T., Waga, M., Sekiyama, T., Hasuo, I.: Weighted automata extraction\nfrom recurrent neural networks via regression on state spaces. In: Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence. vol. 34, pp. 5306–5314 (2020)\n18. Omlin, C.W., Giles, C.L.: Extraction of rules from discrete-time recurrent neural\nnetworks. Neural networks 9(1), 41–52 (1996)\n19. Omlin, C.W., Giles, C.L.: Rule revision with recurrent neural networks. IEEE\nTransactions on Knowledge and Data Engineering 8(1), 183–188 (1996)\n20. Omlin, C., Giles, C., Miller, C.: Heuristics for the extraction of rules from discrete-\ntime recurrent neural networks. In: [Proceedings 1992] IJCNN International Joint\nConference on Neural Networks. vol. 1, pp. 33–38. IEEE (1992)\n21. Powers, D.M.: Applications and explanations of zipf’s law. In: New methods in\nlanguage processing and computational natural language learning (1998)\n22. Wang, Q., Zhang, K., Liu, X., Giles, C.L.: Veriﬁcation of recurrent neural networks\nthrough rule extraction. arXiv preprint arXiv:1811.06029 (2018)\n23. Wang, Q., Zhang, K., Ororbia II, A.G., Xing, X., Liu, X., Giles, C.L.: An empirical\nevaluation of rule extraction from recurrent neural networks. Neural computation\n30(9), 2568–2591 (2018)\n24. Wang, R., Li, Z., Cao, J., Chen, T., Wang, L.: Convolutional recurrent neural\nnetworks for text classiﬁcation. In: 2019 International Joint Conference on Neural\nNetworks (IJCNN). pp. 1–6. IEEE (2019)\n25. Weiss, G., Goldberg, Y., Yahav, E.: Extracting automata from recurrent neural\nnetworks using queries and counterexamples. In: International Conference on Ma-\nchine Learning. pp. 5247–5256. PMLR (2018)\n26. Weiss, G., Goldberg, Y., Yahav, E.: Learning deterministic weighted automata\nwith queries and counterexamples. In: Wallach, H., Larochelle, H., Beygelzimer, A.,\nd'Alché-Buc, F., Fox, E., Garnett, R. (eds.) Advances in Neural Information Pro-\ncessing Systems. vol. 32. Curran Associates, Inc. (2019), https://proceedings.\nneurips.cc/paper/2019/file/d3f93e7766e8e1b7ef66dfdd9a8be93b-Paper.pdf\n27. Xie, X., Guo, W., Ma, L., Le, W., Wang, J., Zhou, L., Liu, Y., Xing, X.: Rnnrepair:\nAutomatic rnn repair via model-based analysis. In: International Conference on\nMachine Learning. pp. 11383–11392. PMLR (2021)\n28. Zeng, Z., Goodman, R.M., Smyth, P.: Learning ﬁnite state machines with self-\nclustering recurrent networks. Neural Computation 5(6), 976–990 (1993)\n29. Zhang, X., Du, X., Xie, X., Ma, L., Liu, Y., Sun, M.: Decision-guided weighted\nautomata extraction from recurrent neural networks. In: Thirty-Fifth AAAI Con-\nference on Artiﬁcial Intelligence (AAAI). pp. 11699–11707. AAAI Press (2021)\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2022-06-27",
  "updated": "2022-09-27"
}