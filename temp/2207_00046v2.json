{
  "id": "http://arxiv.org/abs/2207.00046v2",
  "title": "Performative Reinforcement Learning",
  "authors": [
    "Debmalya Mandal",
    "Stelios Triantafyllou",
    "Goran Radanovic"
  ],
  "abstract": "We introduce the framework of performative reinforcement learning where the\npolicy chosen by the learner affects the underlying reward and transition\ndynamics of the environment. Following the recent literature on performative\nprediction~\\cite{Perdomo et. al., 2020}, we introduce the concept of\nperformatively stable policy. We then consider a regularized version of the\nreinforcement learning problem and show that repeatedly optimizing this\nobjective converges to a performatively stable policy under reasonable\nassumptions on the transition dynamics. Our proof utilizes the dual perspective\nof the reinforcement learning problem and may be of independent interest in\nanalyzing the convergence of other algorithms with decision-dependent\nenvironments. We then extend our results for the setting where the learner just\nperforms gradient ascent steps instead of fully optimizing the objective, and\nfor the setting where the learner has access to a finite number of trajectories\nfrom the changed environment. For both settings, we leverage the dual\nformulation of performative reinforcement learning and establish convergence to\na stable solution. Finally, through extensive experiments on a grid-world\nenvironment, we demonstrate the dependence of convergence on various parameters\ne.g. regularization, smoothness, and the number of samples.",
  "text": "Performative Reinforcement Learning\nDebmalya Mandal\nMPI-SWS\ndmandal@mpi-sws.org\nStelios Triantafyllou\nMPI-SWS\nstrianta@mpi-sws.org\nGoran Radanovic\nMPI-SWS\ngradanovic@mpi-sws.org\nFebruary 8, 2023\nAbstract\nWe introduce the framework of performative reinforcement learning where the policy chosen\nby the learner aﬀects the underlying reward and transition dynamics of the environment.\nFollowing the recent literature on performative prediction [Per+20], we introduce the concept\nof performatively stable policy. We then consider a regularized version of the reinforcement\nlearning problem and show that repeatedly optimizing this objective converges to a performatively\nstable policy under reasonable assumptions on the transition dynamics. Our proof utilizes the\ndual perspective of the reinforcement learning problem and may be of independent interest in\nanalyzing the convergence of other algorithms with decision-dependent environments. We then\nextend our results for the setting where the learner just performs gradient ascent steps instead\nof fully optimizing the objective, and for the setting where the learner has access to a ﬁnite\nnumber of trajectories from the changed environment. For both the settings, we leverage the\ndual formulation of performative reinforcement learning, and establish convergence to a stable\nsolution. Finally, through extensive experiments on a grid-world environment, we demonstrate\nthe dependence of convergence on various parameters e.g. regularization, smoothness, and the\nnumber of samples.\n1\nIntroduction\nOver the last decade, advances in reinforcement learning techniques enabled several breakthroughs\nin AI. These milestones include AlphaGo [Sil+17], Pluribus [BS19], and AlphaStar [Vin+19]. Such\nsuccess stories of reinforcement learning in multi-agent game playing environments have naturally\nled to the adoption of RL in many real-world scenarios e.g. recommender systems [Agg+], and\nhealthcare [Est+19].\nHowever, these critical domains often pose new challenges including the\nmismatch between deployed policy and the target environment.\nExisting frameworks of reinforcement learning ignore the fact that a deployed policy might\nchange the underlying environment (i.e., reward, or probability transition function, or both). Such a\nmismatch between the deployed policy and the environment often arises in practice. For example,\nrecommender systems often use contextual Markov decision process to model interaction with a\nuser [Han+20]. In such a contextual MDP, the initial context/user feature is drawn according\na distribution, then the user interacts with the platform according to the context-speciﬁc MDP.\nHowever, it has been repeatedly observed that such recommender systems not only change the user\ndemographics (i.e. distribution of contexts) but also how they interact with the platforms [CSE18;\n1\narXiv:2207.00046v2  [cs.LG]  7 Feb 2023\nMan+20]. Our second example comes from autonomous vehicles (AV). Even if we ignore the multi-\nagent aspect of these learning algorithms, a deployed AV might change how the pedestrians, and\nother cars behave, and the resulting environment might be quite diﬀerent from what the designers of\nthe AV had in mind [Nik+17a].\nRecently, Perdomo et al. [Per+20] introduced the notion of performative prediction, where\nthe predictions made by a classiﬁer changes the data distribution. However, in the context of\nreinforcement learning, the situation is diﬀerent as the changing transition dynamics introduces\nadditional complexities. If the underlying probability transition function changes, then the class\nof feasible policies and/or models changes with time. This implies that we need a framework that\nis more general than the framework of performative prediction, and can model policy-dependent\noutcomes in reinforcement learning.\nOur Contributions: In this paper, we introduce the notion of performative reinforcement\nlearning where the deployed policy not only changes the reward vector but also the underlying\ntransition probability function. We introduce the notion of performatively stable policy and show\nunder what conditions various repeated retraining methods (e.g., policy optimization, gradient ascent\netc.) converges to such a stable solution. Our precise contributions are the following.\n• We consider a regularized version of the reinforcement learning problem where the variables are\nlong-term discounted state-action occupancy measures. We show that, when both the probability\ntransition function and the reward function changes smoothly in response to the occupancy measure,\nrepeated optimization of regularized reinforcement learning converges to a stable solution.\n• We then show that if the learner performs repeated projected gradient ascent steps, then also\nconvergence is guaranteed provided that the step-size is small enough. Compared to the supervised\nlearning setting [Per+20], the projection step is necessary as the probability transition function,\nand hence the space of occupancy measures change with time.\n• Next we extend our result to the ﬁnite samples setting, where the learner has access to a collection\nof samples from the updated environment. For this setting, we use an empirical version of the\nLagrangian of the regularized RL problem. We show that repeatedly solving a saddle point of this\nempirical Lagrangian (max player corresponds to the learner) also converges to a stable solution\nprovided that the number of samples is large enough.\n• Finally, we empirically evaluate the eﬀect of various parameter choices (regularization, smoothness,\nnumber of samples etc.) through extensive experiments on a two-agent grid-world environment.\nIn this environment, the ﬁrst agent performs various types of repeated retraining, whereas the\nsecond agent responds according to a smooth response function.\nOur Techniques: At a high level, our theoretical results might look similar to the results\nderived by Perdomo et al. [Per+20]. However, there are many challenges.\n• We repeatedly maximize a regularized objective whose feasible region is the space of feasible\noccupancy measures. As the probability transition function changes with time, the feasible region\nof the optimization problem also changes with time. So ideas from supervised classiﬁcation\nsetting [Men+20] cannot be applied directly. Therefore, we look at the dual problem which is\nstrongly-convex and mapping from occupancy measure to the corresponding dual optimal solution\nturns out to be a contraction. We believe that the dual perspective on performative prediction might\nbe useful for analyzing the convergence of other algorithms with decision-dependent outcomes.\n• For performative reinforcement learning, the ﬁnite sample setting is very diﬀerent than the\nsupervised learning setting. This is because we do not have independent sample access from the\nnew environment. At time-step t, we can only access the new model through the policy πt (or\n2\noccupancy measure dt). In that sense, the learner faces an oﬄine reinforcement learning problem\nwhere the samples are collected from the behavior policy πt. This is also the reason we need an\nadditional overlap assumption, which is often standard in oﬄine reinforcement learning [MS08].\nRelated Work: Perdomo et al. [Per+20] introduced the notion of performative prediction.\nSubsequent papers have considered several aspects of this framework including optimization [Men+20;\nMPZ21], multi-agent systems [Nar+22], and population dynamics [BHK20]. However, to the best of\nour knowledge, performative prediction in sequential decision making is mostly unexplored. A possible\nexception is [Bel+21] who consider a setting where the transition and reward of the underlying\nMDP depend non-deterministically on the deployed policy. Since the mapping is non-deterministic,\nit doesn’t lead to a notion of equilibrium, and the authors instead focus on the optimality and\nconvergence of various RL algorithms.\nStochastic Stackelberg Games: Our work is also closely related to the literature on stochastic\ngames [Sha53; FV12], and in particular, those that study Stackelberg (commitment) strategies [Von10],\nwhere a leader commits a policy to which a follower (best) responds. While diﬀerent algorithmic\napproaches have been proposed for computing Stackelberg equilibria (SE) in stochastic games or\nrelated frameworks [VS12; Let+12; Dim+17], computing optimal commitment policies has shown\nto be a computationally intractable (NP-hard) problem [Let+12]. More recent works have studied\nlearning SE in stochastic games, both from practical perspective [RMK20; MVV20; Hua+22] and\ntheoretical perspective [Bai+21; Zho+21]. The results in this paper diﬀer from this line of work in\ntwo ways. Firstly, our framework abstracts the response model of an agent’s eﬀective environment\nin that it does not model it through a rational agency with a utility function. Instead, it is more\naligned with the approaches that learn the response function of the follower agent [SKT16; Kar+17;\nSes+20], out of which the closest to our work is [Ses+20] that considers repeated games. Secondly,\ngiven that we consider solution concepts from performative prediction rather than SE, we focus on\nrepeated retraining as the algorithm of interest, rather than bi-level optimization approaches.\nOther related work: We also draw a connection to other RL frameworks. Naturally, this\nwork relates to RL settings that study non-stationary environments. These include recent learning-\ntheoretic results, such as [GOA18; Fei+20; Dom+21; CSZ20; WL21] that allow non-stationary\nrewards and transitions provided a bounded number or amount of changes (under no-regret regime),\nthe extensive literature on learning under adversarial reward functions (e.g., [EKM04; NGS12; DH13;\nRM19]), or the recent works on learning under corrupted feedback [Lyk+21]. However, the setting\nof this paper is more structured, i.e., the environment responds to the deployed policy and does\nnot arbitrarily change. Our work is also broadly related to the extensive literature on multi-agent\nRL literature – we refer the reader to [ZYB21] for a selective overview. A canonical example of a\nmulti-agent setting that closely relates to the setting of this paper is human-AI cooperation, where\nthe AI’s policy inﬂuences the human behavior [Dim+17; Nik+17b; Cra+18; Rad+19; Car+19]. In\nfact, our experiments are inspired by human-AI cooperative interaction. While the algorithmic\nframework of repeated retraining has been discussed in some of the works on cooperative AI (e.g.,\nsee [Car+19]), these works do not provide a formal treatment of the problem at hand. Finally, this\npaper also relates to the extensive literature on oﬄine RL [Lev+20] as the learner faces an oﬄine\nRL problem when performing repeated retraining with ﬁnite samples.We utilize the analysis of\n[Zha+22] to establish ﬁnite sample guarantees, under a standard assumption on sample generation\n[MS08; FSM10; XJ21], and overlap in occupancy measure [MS08; Zha+22]. Note that oﬄine RL has\nprimarily focused on static RL settings in which the policy of a learner does not aﬀect the model of\nthe underlying environment.\n3\n2\nModel\nWe are primarily concerned with Markov Decision Processes (MDPs) with a ﬁxed state space S,\naction set A, discount factor γ, and starting state distribution ρ. The reward and the probability\ntransition functions of the MDP will be functions of the adopted policy. We consider inﬁnite-horizon\nsetting where the learner’s goal is to minimize the total sum of discounted rewards. We will write sk\nto denote the state visited at time-step k and ak to denote the action taken at time-step k. When\nthe learner adopts policy π, the underlying MDP has reward function rπ and probability transition\nfunction Pπ. We will write M(π) to denote the corresponding MDP, i.e., M(π) = (S, A, Pπ, rπ, ρ).\nNote that only the reward and the transition probability function change according to the policy π.\nWhen an agent adopts policy π and the underlying MDP is M(π′) = (S, A, Pπ′, rπ′, ρ) the\nprobability of a trajectory τ = (sk, ak)∞\nk=0 is given as P(τ) = ρ(s0) Q∞\nk=1 Pπ′(sk+1|sk, π(sk)). We will\nwrite τ ∼Pπ\nπ′ to denote such a trajectory τ. Given a policy π and an underlying MDP M(π′) we\nwrite V π\nπ′(ρ) to deﬁne the value function w.r.t. the starting state distribution ρ. This is deﬁned as\nfollows.\nV π\nπ′(ρ) = Eτ∼Pπ\nπ′\n\" ∞\nX\nk=0\nγkrπ′(sk, ak)|ρ\n#\n(1)\nSolution Concepts: Given the deﬁnition of the value function eq. (1), we can now deﬁne the\nsolution concepts for our setting. First we deﬁne the performative value function of a policy which is\nthe expected total return of the policy given the environment changes in response to the policy.\nDeﬁnition 1 (Performative Value Function). Given a policy π, and a starting state distribution\nρ ∈∆(S), the performative value function V π\nπ (ρ) is deﬁned as V π\nπ (ρ) = Eτ∼Pππ\n\u0002P∞\nt=0 γtrπ(st, at)|ρ\n\u0003\n.\nWe now deﬁne the performatively optimal policy, which maximizes performative value function.\nDeﬁnition 2 (Performatively Optimal Policy). A policy π is performatively optimal if it maximizes\nperformative value function, i.e., π ∈arg maxπ′ V π′\nπ′ (ρ).\nWe will write πP to denote the performatively optimal policy. Although, πP maximizes the\nperformative value function, it need not be stable, i.e., the policy need not be optimal with respect\nto the changed environment M(πP ). We next deﬁne the notion of performatively stable policy which\ncaptures this notion of stability.\nDeﬁnition 3 (Performatively Stable Policy). A policy π is performatively stable if it satisﬁes the\ncondition π ∈arg maxπ′ V π′\nπ (ρ).\nWe will write πS to denote the performatively stable policy. The deﬁnition of performatively\nstable policy implies that if the underlying MDP is M(πS) then an optimal policy is πS. This means\nafter deploying the policy πS in the MDP M(πS) the environment doesn’t change and the learner is\nalso optimizing her reward in this stable environment.We next show that even for an MDP with a\nsingle state, these two solution concepts can be very diﬀerent.\nExample: Consider an MDP with single state s and two actions a and b. If a policy plays arm\na with probability θ and b with probability 1 −θ then we have r(s, a) = 1\n2 −ϵθ and r(s, b) = 1\n2 + ϵθ\nfor some ϵ < 1\n4. Note that if θS = 0 then both the actions give same rewards, and the learner can\njust play action b. Therefore, a policy that always plays action b is a stable policy and achieves a\n4\nreward of\n1\n2(1−γ). On the other hand, a policy that always plays action a with probability θ = 1/4\nhas the performative value function of\nθ(1/2 −ϵθ)\n1 −γ\n+ (1 −θ)(1/2 + ϵθ)\n1 −γ\n= 1/2 + ϵ/8\n1 −γ\nSo, for ϵ > 0, a performatively optimal policy can achieve higher value function than a stable policy.\nWe will mainly consider with the tabular MDP setting where the number of states and actions\nare ﬁnite. Even though solving tabular MDP in classic reinforcement learning problem is relatively\nstraight-forward, we will see that even the tabular setting raises many interesting challenges for the\nsetting of performative reinforcement learning.\nDiscounted State, Action Occupancy Measure: Note that it is not a priori clear if there\nalways exists a performatively stable policy (as deﬁned in (2)). This is because such existence\nguarantee is usually established through a ﬁxed-point argument, but the set of optimal solutions need\nnot be convex. If both π1 and π2 optimizes (2), then their convex combination might not be optimal.\nSo, in order to ﬁnd a stable policy, we instead consider the linear programming formulation of\nreinforcement learning. Given a policy π, its long-term discounted state-action occupancy measure in\nthe MDP M(π) is deﬁned as dπ(s, a) = Eτ∼Pππ\n\u0002P∞\nk=0 γk1 {sk = s, ak = a}|ρ\n\u0003\n. Given an occupancy\nmeasure d, one can consider the following policy πd which has occupancy measure d.\nπd(a|s) =\n(\nd(s,a)\nP\nb d(s,b)\nif P\na d(s, a) > 0\n1\nA\notherwise\n(2)\nWith this deﬁnition, we can pose the problem of ﬁnding a performatively stable occupancy measure.\nAn occupancy measure d is performatively stable if it is the optimal solution of the following problem.\nd ∈arg max\nd≥0\nX\ns,a\nd(s, a)rd(s, a)\n(3)\ns.t.\nX\na\nd(s, a) = ρ(s) + γ ·\nX\ns′,a\nd(s′, a)Pd(s′, a, s) ∀s\nWith slight abuse of notation we are writing Pd as Pπd (as deﬁned in equation (2)). If either\nthe probability transition function or the reward function changes drastically in response to the\noccupancy measure then the optimization problem 3 need not even have a stable point. Therefore,\nas is standard in performative prediction, we make the following sensitivity assumption regarding\nthe underlying environment.\nAssumption 1. The reward and probability transition mappings are (εr, εp)-sensitive i.e.\nthe\nfollowing holds for any two occupancy measures d and d′\n∥rd −rd′∥2 ≤εr\n\r\rd −d′\r\r\n2 , ∥Pd −Pd′∥2 ≤εp\n\r\rd −d′\r\r\n2\nSince the objective function of eq. (3) is convex (in fact linear), and the set of optimal solutions\nis convex, a simple application of Kakutani ﬁxed point theorem [Gli52] shows that there always\nexists a performative stable solution.1\nProposition 1. Suppose assumption 1 holds for some constants (εr, εp), then the optimization\nproblem 3 always has a ﬁxed point.\n1The proof of this result and all other results are provided in the appendix.\n5\n3\nConvergence of Repeated Retraining\nEven though the optimization problem (3) is guaranteed to have a stable solution, it is not clear that\nrepeatedly optimizing this objective converges to such a point. We now consider a regularized version\nof the optimization problem (3), and attempt to obtain a stable solution of the regularized problem.\nIn subsection (3.3) we will show that such a stable solution guarantees approximate stability with\nrespect to the original unregularized objective (3).\nmax\nd≥0\nX\ns,a\nd(s, a)rd(s, a) −λ\n2 ∥d∥2\n2\n(4)\ns.t.\nX\na\nd(s, a) = ρ(s) + γ ·\nX\ns′,a\nd(s′, a)Pd(s′, a, s) ∀s\nHere λ > 0 is a constant that determines the strong-concavity of the objective. Before analyzing\nthe behavior of repeatedly optimizing the new objective (4) we discuss two important issues. First,\nwe consider quadratic regularization for simplicity, and our results can be easily extended to any\nstrongly-convex regularizer. Second, we apply regularization in the occupancy measure space, but\nregularization in policy space is commonly used [Mni+16]. Since the performatively stable occupancy\nmeasure dS is not known, a common strategy adopted is repeated policy optimization. At time\nt, the learner obtains the occupancy measure dt, and deploys the policy πt (as deﬁned in (2)). In\nresponse, the environment changes to Pt = Pdt and rt = rdt, and the learning agent solves the\nfollowing optimization problem to obtain the next occupancy measure dt+1.\nmax\nd≥0\nX\ns,a\nd(s, a)rt(s, a) −λ\n2 ∥d∥2\n2\n(5)\ns.t.\nX\na\nd(s, a) = ρ(s) + γ ·\nX\ns′,a\nd(s′, a)Pt(s′, a, s) ∀s\nWe next show that repeatedly solving the problem (5) converges to a stable point.\nTheorem 1. Suppose assumption 1 holds with λ > 12S3/2(2ϵr+5Sϵp)\n(1−γ)4\n. Let µ = 12S3/2(2ϵr+5Sϵp)\nλ(1−γ)4\n. Then\nfor any δ > 0 we have\n∥dt −dS∥2 ≤δ\n∀t ≥2(1 −µ)−1 ln (2/δ(1 −γ))\nHere we discuss some of the main challenges behind the proof of this theorem.\n• The primal objective function (5) is strongly concave but the feasible region of the optimization\nproblem changes with each iteration. So we cannot apply the results from performative prediction\n[Per+20], and instead, look at the dual objective which is A(1 −γ)2/λ-strongly convex.\n• Although the dual problem is strongly convex, it does not satisfy Lipschitz continuity w.r.t. P.\nHowever, we show that the norm of the optimal solution of the dual problem is bounded by\nO\n\u0000S/(1 −γ)2\u0001\nand this is suﬃcient to show that the dual objective is Lipschitz-continuous with\nrespect to P at the dual optimal solution. We show that the proof argument used in Perdomo\net al. [Per+20] works if we replace global Lipschitz-continuity by such local Lipschitz-continuity.\n• Finally, we translate back the bound about the dual solution to a guarantee about the primal\nsolution (∥dt −dS∥2) using the strong duality of the optimization problem 5. This step crucially\nuses the quadratic regularization in the primal.\n6\nHere we make several observations regarding the assumptions required by Theorem 1. First,\nTheorem 1 suggests that for a given sensitivity (ϵr, ϵp) and discount factor γ, one can choose the\nparameter λ so that the convergence to a stable point is guaranteed. Second, for a given value of λ\nand γ if the sensitivity is small enough, then repeatedly optimizing 5 converges to a stable point.\n3.1\nGradient Ascent Based Algorithm\nWe now extend our result for the setting where the learner does not fully solve the optimization\nproblem 5 every iteration. Rather, the learner takes a gradient step with respect to the changed\nenvironment every iteration. Let Ct denote the set of occupancy measures that are compatible with\nprobability transition function Pt.\nCt =\n\u001a\nd :\nX\na\nd(s, a) = ρ(s) + γ\nX\ns′,a\nd(s′, a)Pt(s′, a, s) ∀sand d(s, a) ≥0 ∀s, a\n\u001b\n(6)\nThen the gradient ascent algorithm ﬁrst takes a gradient step according to the objective function\nr⊤\nt d −λ\n2 ∥d∥2\n2 and then projects the resulting occupancy measure onto Ct.\ndt+1 = ProjCt (dt + η · (rt −λdt)) = ProjCt ((1 −ηλ)dt + ηrt)\n(7)\nHere ProjC(v) ﬁnds a point in C that is closest to v in L2-norm. We next show that repeatedly\ntaking projected gradient ascent steps with appropriate step size η converges to a stable point.\nTheorem 2. Let λ ≥max\nn\n4ϵr, 2S, 20γ2S1.5(ϵr+ϵp)\n(1−γ)2\no\n, step-size η = 1\nλ and µ =\nr\n64γ2ϵ2p\n(1−γ)4\n\u0010\n1 + 30γ4S2\n(1−γ)4\n\u0011\n.\nSuppose assumption 1 holds with ϵp < min\nn\nγS\n3 , (1−γ)4\n100γ3S\no\n. Then for any δ > 0 we have\n∥dt −dS∥2 ≤δ\n∀t ≥(1 −µ)−1 ln (2/δ(1 −γ))\nProof Sketch: First, the projection step 7 can be computed through the following convex\nprogram.\nmin\nd≥0\n1\n2 ∥d −(1 −ηλ)dt −ηrt∥2\n2\n(8)\ns.t.\nX\na\nd(s, a) = ρ(s) + γ ·\nX\ns′,a\nd(s′, a)Pt(s′, a, s) ∀s\nEven though the objective above is convex, its feasible region changes with time. So we again look\nat the dual objective which is strongly concave and has a ﬁxed feasible region. Given an occupancy\nmeasure dt, let GDη(dt) be the optimal solution of the problem 7. We show that if the step-size\nη is chosen small enough then the operator GDη(·) is a contraction mapping by ﬁrst proving the\ncorresponding optimal dual solution forms a contraction, and then using strong duality to transfer\nthe guarantee back to the primal optimal solutions. Finally, we show that the ﬁxed point of the\nmapping GDη(·) indeed coincides with the performatively stable solution dS.\n7\n3.2\nFinite Sample Guarantees\nSo far we assumed that after deploying the policy corresponding to the occupancy measure dt we\nobserve the updated environment Mt = (S, A, Pt, rt, γ). However, in practice, we do not have access\nto the true model but only have access to samples from the updated environment. Our setting is\nmore challenging than the ﬁnite samples setting considered by Perdomo et al. [Per+20]. Unlike the\nsupervised learning setting, we do not have access to independent samples from the new environment.\nAt time t we can deploy policy πt corresponding to the occupancy measure dt, and can access\ntrajectories from the new environment Mt only through the policy πt. Therefore, at every step, the\nlearner faces an oﬄine reinforcement learning problem where the policy πt is a behavioral policy.\nA standard assumption in oﬄine reinforcement learning is overlap in occupancy measure between\nthe behavior policy and a class of target policies [MS08]. Without such overlap, one can get no\ninformation regarding the optimal policy from the trajectories visited by the behavioral policy. We\nmake the following assumption regarding the overlap in occupancy measure between a deployed\npolicy and the optimal policy in the changed environment.\nAssumption 2. Given an occupancy measure d, let ρ⋆\nd be the optimal occupancy measure maximizing\n5, and d is the occupancy measure of πd in Pd. Then there exists B > 0 so that\nmax\ns,a\n\f\f\f\f\nρ⋆\nd(s, a)\nd(s, a)\n\f\f\f\f ≤B\n∀d\nWhen there is no performativity, d equals d and the assumption states overlap between the\noccupancy measure of the deployed policy and the optimal policy. This is the standard assumption\nof single policy coverage in oﬄine reinforcement learning. When there is performativity, d can be\ndiﬀerent than d since the deployed policy πd might have occupancy measure diﬀerent than d in\nthe changed model Pd, and in that case we require overlap between d and the optimal occupancy\nmeasure. Assumption (2) is also signiﬁcantly weaker than the uniform coverage assumption which\nrequires maxd maxs,a d(s, a) > 0 as it allows the possibility that d(s, a) = 0 as long as the optimal\npolicy doesn’t visit state s or never takes action a in state s.\nData: We assume the following model of sample generation at time t. Given the occupancy\nmeasure dt let the normalized occupancy measure be edt(s, a) = (1 −γ)dt(s, a). First, sample a\nstate, action pair (si, ai) i.i.d as (si, ai) ∼edt, then reward ri ∼rt(si, ai), and ﬁnally the next state\ns′\ni ∼Pt(·|si, ai). We have access to mt such tuples at time t and the data collected at time is given\nas Dt = {(si, ai, ri, s′\ni)}mt\ni=1. We would like to point out that this is a standard model of sample\ngeneration in oﬄine reinforcement learning (see e.g. [MS08; FSM10; XJ21]).\nWith ﬁnite samples, the learner needs to optimize an empirical version of the optimization\nproblem 5, and the choice of such an empirical objective is important for convergence. We follow the\nrecent literature on oﬄine reinforcement learning [Zha+22] and consider the Lagrangian of eq. (5).\nL(d, h; Mt) = d⊤rt −λ\n2 ∥d∥2\n2 +\nX\ns\nh(s)\n\n−\nX\na\nd(s, a) + ρ(s) + γ ·\nX\ns′,a\nd(s′, a)Pt(s′, a, s)\n\n\n= −λ\n2 ∥d∥2\n2 +\nX\ns\nh(s)ρ(s) +\nX\ns,a\ndt(s, a) d(s, a)\ndt(s, a)\n \nrt(s, a) −h(s) + γ\nX\ns′\nPt(s, a, s′)h(s′)\n!\n8\nThe above expression motivates the following empirical version of the Lagrangian.\nˆL(d, h; Mt) = −λ\n2 ∥d∥2\n2 +\nX\ns\nh(s)ρ(s) +\nmt\nX\ni=1\nd(si, ai)\ndt(si, ai)\nr(si, ai) −h(si) + γh(s′\ni)\nmt(1 −γ)\n(9)\nWe repeatedly solve for a saddle point of the objective above.\n(dt+1, ht+1) = arg max\nd\narg min\nh\nˆL(d, h; Mt)\n(10)\nThe next theorem provides convergence guarantees of the repeated optimization procedure (10)\nprovided that the number of samples is large enough.\nTheorem 3 (Informal Statement). Suppose assumption 1 holds with λ ≥24S3/2(2ϵr+5Sϵp)\n(1−γ)4\n, and\nassumption 2 holds with parameter B.\nLet µ =\n24S3/2(2ϵr+5Sϵp)\n(1−γ)4\n.\nFor any δ > 0, and error\nprobability p if we repeatedly solve the optimization problem (10) with number of samples mt ≥\neO\n\u0010\nA2B2\nδ4(2ϵr+5Sϵp)2 ln\n\u0010\nt\np\n\u0011\u0011\n2 then with probability at least 1 −p we have\n∥dt −dS∥2 ≤δ ∀t ≥(1 −µ)−1 ln (2/δ(1 −γ))\nProof Sketch:\n• We ﬁrst show that the empirical version of the Lagrangian ˆL(d, h; Mt) is close to the true Lagrangian\nL(d, h; Mt) with high probability. This is shown using the Chernoﬀ-Hoeﬀding inequality and an\nϵ-net argument over the variables. Here we use the fact that for the dual variables we can just\nconsider the space H =\n\b\nh : ∥h∥2 ≤3S/(1 −γ)2\t\nas the optimal solution is guaranteed to exist\nin this space.\n• We then show that an optimal saddle point of the empirical Lagrangian 9 is close to the optimal\nsolution of the true Largrangian. In particular, if\n\f\f\fL(d, h; Mt) −ˆL(d, h; ˆ\nMt)\n\f\f\f ≤ϵ then we are\nguaranteed that ∥dt+1 −GD(dt)∥2 ≤O(ϵ). Here GD(dt) denotes the solution obtained from\noptimizing the exact function.\n• By choosing an appropriate number of samples, we can make the error term ϵ small enough,\nand establish the following recurrence relation: ∥dt+1 −dS∥2 ≤βδ + β ∥dt −dS∥2 for β < 1/2.\nThe rest of the proof follows the main idea of the proof of Theorem 3.10 from [Men+20]. If\n∥dt −dS∥2 > δ then we get a contraction mapping. On the other hand, if ∥dt −dS∥2 ≤δ then\nsubsequent iterations cannot move dt outside of the δ-neighborhood of dS.\n3.3\nApproximating the Unregularized Objective\nTheorem (1) shows that repeatedly optimizing objective (4) converges to a stable policy (say dλ\nS)\nwith respect to the regularized objective (4). Here we show that the solution dλ\nS approximates\nthe performatively stable and performatively optimal policy with respect to the unregularized\nobjective (3).\n2Here we ignore terms that are logarithmic in S, A, and 1/δ.\n9\nTheorem 4. There exists a choice of the regularization parameter (λ) such that repeatedly optimizing\nobjective (5) converges to a policy (dλ\nS) with the following guarantee3\nX\ns,a\nrdλ\nS(s, a)dλ\nS(s, a) ≥max\nd∈C(dλ\nS)\nX\ns,a\nrdλ\nS(s, a)d(s, a) −O\n\u0010\nS3/2(ϵr + Sϵp)/(1 −γ)6\u0011\nNote that as ϵ = max {ϵr, ϵp} converges to zero, the policy dλ\nS also approaches a performatively\nstable solution with respect to the original unregularized objective.\nTheorem 5 (Informal Statement). Let dPO be the performatively optimal solution with respect to the\nunregularized objective and let ε = max {ϵr, ϵp}. Then there exists a value of λ such that repeatedly\noptimizing objective (5) converges to a policy (dλ\nS) with the following guarantee\nX\ns,a\nrdλ\nS(s, a)dλ\nS(s, a) ≥\nX\ns,a\nrdP O(s, a)dPO(s, a) −O\n \nmax\n(\nS5/3A1/3ϵ2/3\n(1 −γ)14/3 ,\nϵS\n(1 −γ)4\n)!\nWe again see that as ϵ converges to zero, dλ\nS approaches a performatively optimal solution\nwith respect to the original objective.\nThe proof of theorem (5) uses the following bound on\nthe distance between the performatively stable solution and the optimal solution.\n\r\rdλ\nS −dλ\nPO\n\r\r\n2 ≤\nO\n\u0010\nS2√\nA\nλ(1−γ)4\n\u0010\nϵr\n\u0010\n1 + γ\n√\nS\n\u0011\n+ ϵp\nγS\n(1−γ)2\n\u0011\u0011\nWe believe that the bounds of theorems (5) and (4) can be improved with more careful analysis.\nHowever, the error bound should grow as γ decreases. This is because the diameter (or max L2 norm)\nof occupancy measure is most 1/(1 −γ)2 and even in performative prediction such an approximation\nbound grows with the diameter of the model.4\n4\nExperiments\nIn this section, we experimentally evaluate the performance of various repeated retraining methods,\nand determine the eﬀects of various parameters on convergence. All experiments are conducted on a\ngrid-world environment proposed by [TSR21].5 We next describe how this environment is adapted\nfor simulating performative reinforcement learning.\nS\nS\nS\nS\nS\nF\nS\nS\nH\nS\nH\nS\nS\nS\nF\nH\nH\nH\nH\nS\nS\nH\nH\nF\nH\nF\nS\nS\nF\nF\nF\nH\nH\nG\nFigure 8:\nGrid-\nworld\nGridworld: We consider the grid-world environment shown in Fig. 8, in\nwhich n + 1 agents share control over an actor. The agents’ objective is to guide\nthe actor from some initial state S to the terminal state G, while minimizing their\ntotal discounted cost. We will call the ﬁrst agent the principal, and the other n\nagents the followers. In each state, the agents select their actions simultaneously.\nThe principal agent, A1, chooses the direction of the actor’s next move by taking\none of four actions (left, right, up, and down).\nAny other agent, Aj decides to either intervene or not in A1’s action. If\nthe majority of the n follower agents choose not to intervene, then the actor\nmoves one cell towards the direction chosen by A1, otherwise it moves one cell\ntowards the new direction chosen by the majority of the followers. Note that\nthe principal and the followers’ policies determine a policy for the actor agent.\n3C( ed) denotes the set of occupancy measures that are feasible with respect to P( ed) = P e\nd.\n4For example, see proposition E.1 [Per+20], which is stated for diameter 1 and convex loss function.\n5The original single-agent version of this environment can be found in [Vol+19].\n10\nFigure 1: RPO λ = 1\n0\n250\n500\n750\n1000\nIteration t\n10\n13\n10\n10\n10\n7\n10\n4\n10\n1\nct ||dt + 1\ndt||2\n=0.1\n=1.0\n=5.0\n=10.0\n=200.0\nFigure 2: RPO β = 10\n0\n250\n500\n750\n1000\nIteration t\n10\n13\n10\n10\n10\n7\n10\n4\n10\n1\nct ||dt + 1\ndt||2\n=0.0\n=0.2\n=0.5\n=1.0\n=5.0\nFigure 3: RGA λ = 1, β = 5\n0\n250\n500\n750\n1000\nIteration t\n10\n13\n10\n10\n10\n7\n10\n4\n10\n1\nct ||dt + 1\ndt||2\n=0.05\n=0.1\n=0.2\n=1.0\n=2.0\nFigure 4: RGA Gap λ = 1, β =\n5\n0\n250\n500\n750\n1000\nIteration t\n10\n5\n10\n3\n10\n1\nSuboptimality Gap\n=0.05\n=0.1\n=0.2\n=1.0\n=2.0\nFigure 5: ROL FS λ = 1, β = 5\n0\n200\n400\nIteration t\n0.25\n0.50\n0.75\n1.00\n1.25\nct ||dt + 1\ndt||2\nm=20\nm=50\nm=100\nm=200\nm=500\nm=1000\nFigure 6: RPO FS λ = 1, β = 5\n0\n200\n400\nIteration t\n0.25\n0.50\n0.75\n1.00\n1.25\nct ||dt + 1\ndt||2\nm=20\nm=50\nm=100\nm=200\nm=500\nm=1000\nFigure 7: Experimental results for Gridworld. All plots were generated with γ = 0.9 and 1000\niterations. We normalized the distance between iterations t and t + 1 with ct =\n1\n||dt||2 . RPO\nstands for repeated policy optimization, RGA for repeated gradient ascent, ROL for repeatedly\nsolving (empirical) Lagrangian and FS for ﬁnite samples. The parameters are λ (regularization), β\n(smoothness), η (step-size), and m (number of trajectories).\nThe cost at each state is deﬁned according to the grid-world shown in Fig. 8. If the actor visits\na blank or an S cell, then all the agents receive a small negative reward equal to −0.01. If an F cell\nis visited, then a slightly more increased cost equal to −0.02 is imposed and for H cells a severe\npenalty of −0.5 is inﬂicted. Additionally, whenever any Aj decides to intervene, it pays an additional\ncost of −0.05.\nResponse Model: We implement agent A1 as a learner who performs repeated retraining.\nThe initial policy of agent A1 is an ϵ-optimal single-agent policy (ϵ = 0.1) assuming that no other\nagent intervenes. Subsequently, agent A1 performs one type of repeated retraining (e.g. gradient\nascent).The follower agents, on the other hand, always respond to the policy of A1 according to a\nresponse model. In particular, given a policy of A1 (say π1), we ﬁrst compute an optimal Q-value\nfunction for agent Aj, Q∗|π1\nj\n. Note that Q∗|π1\nj\nis computed w.r.t. a perturbed grid-world, and which\nwas generated by performing a random cell perturbation on the grid-world of Fig. 8. The perturbed\ngrid-worlds are diﬀerent for diﬀerent agents. Then we compute an average Q-function deﬁned as\nQ\n∗|π1 = 1\nn\nPn+1\nj=2 Q\n∗|π1\nj\n. Then a policy π2 adopted by the Boltzmann softmax operator with parameter\nβ. Then a policy π2 is determined by the Boltzmann softmax operator with temperature parameter\nβ, π2(ai|s) =\neβ·Q∗|π1 (s,ai)\nP\nj eβ·Q∗|π1 (s,aj) . Note that the new policy π2 eﬀectively plays the role of a changing\nenvironment by responding to the majority of the n follower agents. Additionally, parameter β\ncontrols the smoothness of the changing environment, as viewed by A1.\n11\nRepeated Policy Optimization: We ﬁrst consider the scenario where agent A1 gets complete\nknowledge of the updated reward and probability transition function at time t. In our setting, this\nmeans that A1 decides on πt\n1, all the other agents respond according to the softmax operator and\njointly determines πt\n2, and then agent A1 observes the new policy πt\n2. This lets A1 to compute new\nprobability transition function Pt, and reward function rt, and solve optimization problem 5. The\nsolution is the new occupancy measure dt+1\n1\nfor A1, and A1 computes new policy πt+1\n1\nfor time\nt + 1 by normalization using eq. (2). Plot 1 shows the convergence results of the repeated policy\noptimization for diﬀerent values of β, with λ ﬁxed to 1. We see that if the response function of the\nenvironment (i.e., the policy of agent A2) is not smooth enough (e.g., for β = 200), the algorithm\nfails to converge to a stable solution. In Plot 2 we ﬁx β to 10 and vary the value of parameter λ\n(strength of regularization). We can see that the algorithm converges only for large enough values of\nthe regularization constant λ. Furthermore, we observe that the convergence is faster as λ increases.\nRepeated Gradient Ascent: We now see what happens if agent A1 uses repeated gradient\nascent instead of fully optimizing the objective each iteration. Here also A1 fully observes πt\n2 (hence\nPt and rt) at time t. However, instead of full optimization, A1 performs a projected gradient\nascent step (7) to compute the next occupancy measure dt+1\n1\n. Plot 3 shows the performance of\nrepeated gradient ascent for diﬀerent values of the step-size η. We observe that when η is small\n(e.g,. η ≤O(1/λ)), the learner converges to a stable solution. But the learner diverges for large\nη. Additionally, the convergence is faster for step-size closer to 1/λ (as suggested by Theorem 2).\nSince, repeated gradient ascent does not fully solve the optimization problem 5, we also plot the\nsuboptimality gap of the current solution 4. This is measured as the diﬀerence between the objective\nvalue for the best feasible solution (w.r.t. Mt) and current solution (dt\n1), and then normalized by the\nformer. As the step-size η is varied, we see a trend similar to Plot 3.\nEﬀect of Finite Samples: Finally, we investigate the scenario where A1 does not know πt\n2 at\ntime t, and obtains samples from the new environment Mt by deploying πt\n1. In our experiments,\ninstead of sampling from the occupancy measure, A1 directly samples m trajectories of ﬁxed length\n(up to 100) following policy πt\n1. We considered two approaches for obtaining the new policy πt+1\n1\n.\nFirst, A1 solves the empirical Lagrangian (9) through an iterative method. In particular, we use\nan alternate optimization based method (algorithm (1)) where hn is updated through follow the\nregularized leader (FTRL) algorithm and dn is updated through best response. 6\nAlgorithm 1 Alternating Optimization for the Empirical Lagrangian\nSet H =\n3S\n(1−γ)2 , and H = {h : ∥h∥2 ≤H}.\nfor n = 1, 2, .., N do\nhn = arg minh∈H\nPn−1\nn′=1\nD\n∇h ˆL(dn′, h; Mt), h\nE\n+ β ∥h∥2\n2\ndn = arg maxd≥0, d(s,a)≤B ˆdt(s,a)∀s,a ˆL(d, hn; Mt)\nend for\nReturn d = 1\nN\nPN\nn=1 dn.\nSecond, A1 computes estimates of probability transition function ( bPt), and reward function\n(brt), and solves eq. (5) with these estimates. For both versions, we ran our experiments with 20\ndiﬀerent seeds, and plots 5 and 6 show the average errors along with the standard errors for the two\n6Since the objective (9) is linear in h and concave in d, following standard arguments [FS96] it is straightforward\nto show that algorithm (1) ﬁnds an ε-approximate saddle point in O(SAB/(1 −γ)2ε2) iterations.\n12\napproaches. For both settings, we observe that as m increases, the algorithms eventually converge to\na smaller neighborhood around the stable solution. However, for large number of samples (m = 500\nor 1000), directly solving the Lagrangian (ﬁgure 5) gives improved result.\n5\nConclusion\nIn this work, we introduce the framework of performative reinforcement learning and study under\nwhat conditions repeated retraining methods (e.g., policy optimization, gradient ascent) converges\nto a stable policy. In the future, it would be interesting to extend our framework to handle high\ndimensional state-space, and general function approximation. The main challenge with general\nfunction approximation is that a stable policy might not exist, so the ﬁrst step would be to\ncharacterize under what conditions there is a ﬁxed point. Moreover, most RL algorithms with function\napproximation work in the policy space. So, another challenge lies in generalizing optimization\nproblem 5 to handle representations of states and actions.\nAnother interesting question is to resolve the hardness of ﬁnding stable policy with respect to\nthe unregularized objective. To the best of our knowledge, this question is unresolved even for\nperformative prediction with just convex loss function. It could be interesting to explore connections\nbetween our repeated optimization procedure and standard reinforcement learning methods, e.g.,\npolicy gradient methods [Mni+16; NJG17]. However, we note that policy gradient methods typically\nwork in the policy space, and might not even converge to a stable point under changing environments.\nFinally, for the ﬁnite samples setting, it would be interesting to use oﬄine reinforcement learning\nalgorithms [Lev+20] for improving the speed of convergence to a stable policy.\nReferences\n[Agg+]\nCharu C Aggarwal et al. Recommender systems. Vol. 1. Springer (cit. on p. 1).\n[Bai+21]\nYu Bai, Chi Jin, Huan Wang, and Caiming Xiong. “Sample-eﬃcient learning of stackel-\nberg equilibria in general-sum games”. In: Advances in Neural Information Processing\nSystems 34 (2021) (cit. on p. 3).\n[Bel+21]\nJames Bell, Linda Linsefors, Caspar Oesterheld, and Joar Skalse. “Reinforcement Learn-\ning in Newcomblike Environments”. In: Advances in Neural Information Processing\nSystems 34 (2021) (cit. on p. 3).\n[Ber09]\nDimitri Bertsekas. Convex optimization theory. Vol. 1. Athena Scientiﬁc, 2009 (cit. on\np. 29).\n[BHK20]\nGavin Brown, Shlomi Hod, and Iden Kalemaj. “Performative prediction in a stateful\nworld”. In: arXiv preprint arXiv:2011.03885 (2020) (cit. on p. 3).\n[BS19]\nNoam Brown and Tuomas Sandholm. “Superhuman AI for multiplayer poker”. In: Science\n365.6456 (2019), pp. 885–890 (cit. on p. 1).\n[Car+19]\nMicah Carroll, Rohin Shah, Mark K Ho, Tom Griﬃths, Sanjit Seshia, Pieter Abbeel,\nand Anca Dragan. “On the utility of learning about humans for human-ai coordination”.\nIn: Advances in neural information processing systems 32 (2019) (cit. on p. 3).\n13\n[Cra+18]\nJacob W Crandall, Mayada Oudah, Fatimah Ishowo-Oloko, Sherief Abdallah, Jean-\nFrançois Bonnefon, Manuel Cebrian, Azim Shariﬀ, Michael A Goodrich, Iyad Rahwan,\net al. “Cooperating with machines”. In: Nature communications 9.1 (2018), pp. 1–12\n(cit. on p. 3).\n[CSE18]\nAllison JB Chaney, Brandon M Stewart, and Barbara E Engelhardt. “How algorithmic\nconfounding in recommendation systems increases homogeneity and decreases utility”. In:\nProceedings of the 12th ACM Conference on Recommender Systems. 2018, pp. 224–232\n(cit. on p. 1).\n[CSZ20]\nWang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. “Reinforcement learning for non-\nstationary markov decision processes: The blessing of (more) optimism”. In: International\nConference on Machine Learning. PMLR. 2020, pp. 1843–1854 (cit. on p. 3).\n[DH13]\nOfer Dekel and Elad Hazan. “Better rates for any adversarial deterministic MDP”. In:\nInternational Conference on Machine Learning. PMLR. 2013, pp. 675–683 (cit. on p. 3).\n[Dim+17]\nChristos Dimitrakakis, David C Parkes, Goran Radanovic, and Paul Tylkin. “Multi-view\ndecision processes: the helper-ai problem”. In: Advances in Neural Information Processing\nSystems 30 (2017) (cit. on p. 3).\n[Dom+21]\nOmar Darwiche Domingues, Pierre Ménard, Matteo Pirotta, Emilie Kaufmann, and\nMichal Valko. “A kernel-based approach to non-stationary reinforcement learning in\nmetric spaces”. In: International Conference on Artiﬁcial Intelligence and Statistics.\nPMLR. 2021, pp. 3538–3546 (cit. on p. 3).\n[EKM04]\nEyal Even-Dar, Sham M Kakade, and Yishay Mansour. “Experts in a Markov decision\nprocess”. In: Advances in neural information processing systems 17 (2004) (cit. on p. 3).\n[Est+19]\nAndre Esteva, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark\nDePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and JeﬀDean.\n“A guide to deep learning in healthcare”. In: Nature medicine 25.1 (2019), pp. 24–29\n(cit. on p. 1).\n[Fei+20]\nYingjie Fei, Zhuoran Yang, Zhaoran Wang, and Qiaomin Xie. “Dynamic regret of policy\noptimization in non-stationary environments”. In: Advances in Neural Information\nProcessing Systems 33 (2020), pp. 6743–6754 (cit. on p. 3).\n[FS96]\nYoav Freund and Robert E Schapire. “Game theory, on-line prediction and boosting”.\nIn: Proceedings of the ninth annual conference on Computational learning theory. 1996,\npp. 325–332 (cit. on p. 12).\n[FSM10]\nAmir-massoud Farahmand, Csaba Szepesvári, and Rémi Munos. “Error propagation for\napproximate policy and value iteration”. In: Advances in Neural Information Processing\nSystems 23 (2010) (cit. on pp. 3, 8).\n[FV12]\nJerzy Filar and Koos Vrieze. Competitive Markov decision processes. Springer Science &\nBusiness Media, 2012 (cit. on p. 3).\n[Gli52]\nIrving L Glicksberg. “A further generalization of the Kakutani ﬁxed point theorem, with\napplication to Nash equilibrium points”. In: Proceedings of the American Mathematical\nSociety 3.1 (1952), pp. 170–174 (cit. on pp. 5, 42).\n14\n[GOA18]\nPratik Gajane, Ronald Ortner, and Peter Auer. “A sliding-window algorithm for markov\ndecision processes with arbitrarily changing rewards and transitions”. In: arXiv preprint\narXiv:1805.10066 (2018) (cit. on p. 3).\n[Han+20]\nCasper Hansen, Christian Hansen, Lucas Maystre, Rishabh Mehrotra, Brian Brost,\nFederico Tomasi, and Mounia Lalmas. “Contextual and sequential user embeddings for\nlarge-scale music recommendation”. In: Fourteenth ACM conference on recommender\nsystems. 2020, pp. 53–62 (cit. on p. 1).\n[Hua+22]\nPeide Huang, Mengdi Xu, Fei Fang, and Ding Zhao. “Robust Reinforcement Learning\nas a Stackelberg Game via Adaptively-Regularized Adversarial Training”. In: arXiv\npreprint arXiv:2202.09514 (2022) (cit. on p. 3).\n[Kar+17]\nDebarun Kar, Benjamin Ford, Shahrzad Gholami, Fei Fang, Andrew Plumptre, Milind\nTambe, Margaret Driciru, Fred Wanyama, Aggrey Rwetsiba, Mustapha Nsubaga, et al.\n“Cloudy with a chance of poaching: Adversary behavior modeling and forecasting with\nreal-world poaching data”. In: (2017) (cit. on p. 3).\n[Let+12]\nJoshua Letchford, Liam MacDermed, Vincent Conitzer, Ronald Parr, and Charles L\nIsbell. “Computing optimal strategies to commit to in stochastic games”. In: Twenty-Sixth\nAAAI Conference on Artiﬁcial Intelligence. 2012 (cit. on p. 3).\n[Lev+20]\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. “Oﬄine reinforcement\nlearning: Tutorial, review, and perspectives on open problems”. In: arXiv preprint\narXiv:2005.01643 (2020) (cit. on pp. 3, 13).\n[Lyk+21]\nThodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun. “Corruption-robust\nexploration in episodic reinforcement learning”. In: Conference on Learning Theory.\nPMLR. 2021, pp. 3242–3245 (cit. on p. 3).\n[Man+20]\nMasoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher,\nand Robin Burke. “Feedback loop and bias ampliﬁcation in recommender systems”. In:\nProceedings of the 29th ACM international conference on information & knowledge\nmanagement. 2020, pp. 2145–2148 (cit. on p. 2).\n[Men+20]\nCelestine Mendler-Dünner, Juan Perdomo, Tijana Zrnic, and Moritz Hardt. “Stochastic\noptimization for performative prediction”. In: Advances in Neural Information Processing\nSystems 33 (2020), pp. 4929–4939 (cit. on pp. 2, 3, 9).\n[Mni+16]\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy\nLillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. “Asynchronous methods for\ndeep reinforcement learning”. In: International conference on machine learning. PMLR.\n2016, pp. 1928–1937 (cit. on pp. 6, 13).\n[MPZ21]\nJohn P Miller, Juan C Perdomo, and Tijana Zrnic. “Outside the echo chamber: Optimiz-\ning the performative risk”. In: International Conference on Machine Learning. PMLR.\n2021, pp. 7710–7720 (cit. on p. 3).\n[MS08]\nRémi Munos and Csaba Szepesvári. “Finite-Time Bounds for Fitted Value Iteration.”\nIn: Journal of Machine Learning Research 9.5 (2008) (cit. on pp. 3, 8).\n[MVV20]\nRajesh K Mishra, Deepanshu Vasal, and Sriram Vishwanath. “Model-free reinforcement\nlearning for stochastic stackelberg security games”. In: 2020 59th IEEE Conference on\nDecision and Control (CDC). IEEE. 2020, pp. 348–353 (cit. on p. 3).\n15\n[Nar+22]\nAdhyyan Narang, Evan Faulkner, Dmitriy Drusvyatskiy, Maryam Fazel, and Lillian J\nRatliﬀ. “Multiplayer Performative Prediction: Learning in Decision-Dependent Games”.\nIn: arXiv preprint arXiv:2201.03398 (2022) (cit. on p. 3).\n[NGS12]\nGergely Neu, Andras Gyorgy, and Csaba Szepesvári. “The adversarial stochastic shortest\npath problem with unknown transition probabilities”. In: Artiﬁcial Intelligence and\nStatistics. PMLR. 2012, pp. 805–813 (cit. on p. 3).\n[Nik+17a]\nStefanos Nikolaidis, Swaprava Nath, Ariel D Procaccia, and Siddhartha Srinivasa. “Game-\ntheoretic modeling of human adaptation in human-robot collaboration”. In: Proceedings\nof the 2017 ACM/IEEE international conference on human-robot interaction. 2017,\npp. 323–331 (cit. on p. 2).\n[Nik+17b]\nStefanos Nikolaidis, Swaprava Nath, Ariel D Procaccia, and Siddhartha Srinivasa. “Game-\ntheoretic modeling of human adaptation in human-robot collaboration”. In: Proceedings\nof the 2017 ACM/IEEE international conference on human-robot interaction. 2017,\npp. 323–331 (cit. on p. 3).\n[NJG17]\nGergely Neu, Anders Jonsson, and Vicenç Gómez. “A uniﬁed view of entropy-regularized\nmarkov decision processes”. In: arXiv preprint arXiv:1705.07798 (2017) (cit. on p. 13).\n[Per+20]\nJuan Perdomo, Tijana Zrnic, Celestine Mendler-Dünner, and Moritz Hardt. “Perfor-\nmative prediction”. In: International Conference on Machine Learning. PMLR. 2020,\npp. 7599–7609 (cit. on pp. 1–3, 6, 8, 10).\n[Rad+19]\nGoran Radanovic, Rati Devidze, David Parkes, and Adish Singla. “Learning to collaborate\nin markov decision processes”. In: International Conference on Machine Learning. PMLR.\n2019, pp. 5261–5270 (cit. on p. 3).\n[RM19]\nAviv Rosenberg and Yishay Mansour. “Online convex optimization in adversarial markov\ndecision processes”. In: International Conference on Machine Learning. PMLR. 2019,\npp. 5478–5486 (cit. on p. 3).\n[RMK20]\nAravind Rajeswaran, Igor Mordatch, and Vikash Kumar. “A game theoretic framework\nfor model based reinforcement learning”. In: International conference on machine learning.\nPMLR. 2020, pp. 7953–7963 (cit. on p. 3).\n[Ses+20]\nPier Giuseppe Sessa, Ilija Bogunovic, Maryam Kamgarpour, and Andreas Krause.\n“Learning to play sequential games versus unknown opponents”. In: Advances in Neural\nInformation Processing Systems 33 (2020), pp. 8971–8981 (cit. on p. 3).\n[Sha53]\nLloyd S Shapley. “Stochastic games”. In: Proceedings of the national academy of sciences\n39.10 (1953), pp. 1095–1100 (cit. on p. 3).\n[Sil+17]\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang,\nArthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. “Master-\ning the game of go without human knowledge”. In: nature 550.7676 (2017), pp. 354–359\n(cit. on p. 1).\n[SKT16]\nArunesh Sinha, Debarun Kar, and Milind Tambe. “Learning Adversary Behavior in\nSecurity Games: A PAC Model Perspective”. In: Proceedings of the 2016 International\nConference on Autonomous Agents & Multiagent Systems. 2016, pp. 214–222 (cit. on\np. 3).\n16\n[TSR21]\nStelios Triantafyllou, Adish Singla, and Goran Radanovic. “On Blame Attribution\nfor Accountable Multi-Agent Sequential Decision Making”. In: Advances in Neural\nInformation Processing Systems 34 (2021) (cit. on p. 10).\n[Ver10]\nRoman Vershynin. “Introduction to the non-asymptotic analysis of random matrices”.\nIn: arXiv preprint arXiv:1011.3027 (2010) (cit. on p. 39).\n[Vin+19]\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew\nDudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev,\net al. “Grandmaster level in StarCraft II using multi-agent reinforcement learning”. In:\nNature 575.7782 (2019), pp. 350–354 (cit. on p. 1).\n[Vol+19]\nCameron Voloshin, Hoang M Le, Nan Jiang, and Yisong Yue. “Empirical study of oﬀ-\npolicy policy evaluation for reinforcement learning”. In: arXiv preprint arXiv:1911.06854\n(2019) (cit. on p. 10).\n[Von10]\nHeinrich Von Stackelberg. Market structure and equilibrium. Springer Science & Business\nMedia, 2010 (cit. on p. 3).\n[VS12]\nYevgeniy Vorobeychik and Satinder Singh. “Computing stackelberg equilibria in dis-\ncounted stochastic games”. In: Proceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence. Vol. 26. 1. 2012, pp. 1478–1484 (cit. on p. 3).\n[WL21]\nChen-Yu Wei and Haipeng Luo. “Non-stationary reinforcement learning without prior\nknowledge: An optimal black-box approach”. In: Conference on Learning Theory. PMLR.\n2021, pp. 4300–4354 (cit. on p. 3).\n[XJ21]\nTengyang Xie and Nan Jiang. “Batch value-function approximation with only realizabil-\nity”. In: International Conference on Machine Learning. PMLR. 2021, pp. 11404–11413\n(cit. on pp. 3, 8).\n[Zha+22]\nWenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason D Lee. “Oﬄine\nreinforcement learning with realizability and single-policy concentrability”. In: arXiv\npreprint arXiv:2202.04634 (2022) (cit. on pp. 3, 8, 36).\n[Zho+21]\nHan Zhong, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. “Can Reinforcement\nLearning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopic\nFollowers?” In: arXiv preprint arXiv:2112.13521 (2021) (cit. on p. 3).\n[ZYB21]\nKaiqing Zhang, Zhuoran Yang, and Tamer Başar. “Multi-agent reinforcement learning: A\nselective overview of theories and algorithms”. In: Handbook of Reinforcement Learning\nand Control (2021), pp. 321–384 (cit. on p. 3).\nA\nAdditional Information on Experimental Setup and Results\nIn this section, we provide additional information on the experimental setup (Section A.1), as well\nas additional experimental results (Section A.2). We also provide information regarding the total\namount of compute time and the type of resources that were used (Section A.3).\nA.1\nAdditional Information on Experimental Setup\nGridworld: The initial state of the actor in the grid-world is selected uniformly at random from\nthe cells denoted by S. Additionally, the actor remains at its current cell in case it attempts a move\n17\nthat would lead it outside the grid-world. Regarding the reward function, all the agents receive a\npositive reward equal to +1 whenever the actor reaches the terminal state G.\nResponse Model: The response model of a follower agent is based on a perturbed grid-world\ninstead of the one in Fig. 8. In other words, each of the n follower agents sees diﬀerent cell costs\nthan the ones A1 sees. As a result, they might respond to the policy of A1, by adopting a policy that\nperforms unnecessary or even harmful interventions w.r.t. the grid-world of Fig. 8. A perturbed\ngrid-world is generated from the grid-world of Fig. 8 with the following procedure. First, G and S\ncells stay the same between the two grid-worlds. Then, any blank, F or H cell remains unchanged\nwith probability 0.7, and with probability 0.3 we perturb its type to blank , F or H (the perturbation\nis done uniformly at random).\nA.2\nAdditional Experimental Results\nIn this section, we provide additional insights on the interventional policies of the follower agents.\nThe repeated retraining method we use in these experiments is the repeated policy optimization.\nMore speciﬁcally, we present a visual representation of the limiting environment i.e. the majority\nof the agents’ policy in the limit, i.e., after the method has converged to a stable solution. The\nconﬁgurations are set to λ = 1, β = 5, and we vary discount factor γ.\nAs mentioned in Section 4, the policy of the follower agents can be thought of as a changing\nenvironment that responds to the policy updates of A1. To visualize how this environment looks like\nin the limit, we depict in Figure 11 several limiting policies of the follower agents. From the Figures\n9, and 10 we observe that for smaller discount factor, the majority of the follower agents tend to\nintervene closer to the goal state.\nG\nFigure 9: γ = 0.5\nG\nFigure 10: γ = 0.9\nFigure 11: Figures 9, and 10 visualize two instances of the interventional policy of agent A2 in the\nGridworld environment. All ﬁgures correspond to the majority of the followers’ policy at convergence\nfor various values of the discount factor γ. Empty cells denote states where majority of the agents’\nmost probable action is to not intervene. For cells with a red arrow the (highest probability) action\nof the majority of the follower agents is to intervene by forcing the actor to move one cell towards\nthe direction of the arrow.\n18\nA.3\nTotal Amount of Compute and Type of Resources\nAll experiments were conducted on a computer cluster with machines equipped with 2 Intel Xeon\nE5-2667 v2 CPUs with 3.3GHz (16 cores) and 50 GB RAM. Table 1 reports the total computation\ntimes for our experiments (Section 4). Note that at each iteration of the repeated gradient ascent\nexperiment, apart from the gradient step a full solution of the optimization problem 5 was also\ncomputed, in order to report the suboptimality gap.\nRepeated Policy Optimization\n767 sec\nRepeated Gradient Ascent\n964 sec\nRepeated Policy Optimization with Finite Samples\n33746 sec\nRepeated Gradient Ascent with Finite Samples\n35396 sec\nTable 1: Total computation times for the diﬀerent experiments described in Section 4.\nB\nMissing Proofs\nB.1\nProof of Convergence of Repeated Maximization (Theorem 1)\nProof. We ﬁrst compute the dual of the concave optimization problem 5. The Lagrangian is given as\nL(d, h) = d⊤rt −λ\n2 ∥d∥2\n2 +\nX\ns\nh(s)\n\n−\nX\na\nd(s, a) + ρ(s) + γ ·\nX\ns′,a\nd(s′, a)Pt(s′, a, s)\n\n\nAt an optimal solution we must have ∇dL(d, h) = 0, which gives us the following expression for d.\nd(s, a) = rt(s, a)\nλ\n−h(s)\nλ\n+ γ\nλ\nX\nes\nh(es)Pt(s, a, es)\n(11)\nSubstituting the above value of d we get the following dual problem.\nmin\nh∈RS −1\nλ\nX\ns,a\nh(s)rt(s, a) + γ\nλ\nX\ns\nX\ns′,a\nh(s)rt(s′, a)Pt(s′, a, s) +\nX\ns\nh(s)ρ(s)\n+ A\n2λ\nX\ns\nh(s)2 −γ\nλ\nX\ns,a\nh(s)\nX\nes\nh(es)Pt(s, a, es) + γ2\n2λ\nX\ns,a\nX\nes,ˆs\nh(es)h(ˆs)Pt(s, a, ˆs)Pt(s, a, es)\n(12)\nNote that the dual objective is parameterized by reward function rt and probability transition\nfunction Pt which are the parameters corresponding to the occupancy measure dt. We will write\nL(·; Mt) to denote this dual objective function.\nFor a given occupancy measure (i.e. dt = d) we will write GD(d) to denote the optimal solution\nto the primal problem 5. We ﬁrst aim to show that the operator GD(·) is a contraction mapping.\n19\nConsider two occupancy measures d and ˆd. Let r (resp. ˆr) be the reward functions in response to\nthe occupancy measure d (resp. ˆd). Similarly, let P (resp. ˆP) be the probability transition function\nin response to the occupancy measure d (resp. ˆd).\nLet h (resp. ˆh) be the optimal dual solutions corresponding to the occupancy measures d (resp.\nˆd) i.e. h ∈arg maxh′ L(h′; M) and ˆh ∈arg maxh′ L(h′; ˆ\nM). Lemma 2 proves that the objective is\nA(1 −γ)2/λ strongly convex. Therefore, we have the following two inequalities.\nL(h; M) −L(ˆh; M) ≥\n\u0010\nh −ˆh\n\u0011⊤\n∇L(ˆh; M) + A(1 −γ)2\n2λ\n\r\r\rh −ˆh\n\r\r\r\n2\n2\n(13)\nL(ˆh; M) −L(h; M) ≥A(1 −γ)2\n2λ\n\r\r\rh −ˆh\n\r\r\r\n2\n2\n(14)\nThese two inequalities give us the following bound.\n−A(1 −γ)2\nλ\n\r\r\rh −ˆh\n\r\r\r\n2\n2 ≥(h −ˆh)⊤∇L(ˆh; M)\n(15)\nWe now bound the Lipschitz constant of the term (h −ˆh)⊤Ld(ˆh; M) with respect to the MDP M.\nLemma 3 gives us the following bound.\n\r\r\r∇L(ˆh; M) −∇L(ˆh; ˆ\nM)\n\r\r\r\n2 ≤4S\n√\nA\nλ\n∥r −ˆr∥2 +\n \n4γ\n√\nSA\nλ\n+ 6γ\n√\nAS\nλ\n\r\r\rˆh\n\r\r\r\n2\n! \r\r\rP −ˆP\n\r\r\r\n2\nNow notice that the dual variable ˆh is actually an optimal solution and we can use lemma 4 to\nbound its norm by\n3S\n(1−γ)2 . Furthermore, under assumption 1, we have ∥r −ˆr∥2 ≤ϵr\n\r\r\rd −ˆd\n\r\r\r\n2 and\n\r\r\rP −ˆP\n\r\r\r\n2 ≤ϵp\n\r\r\rd −ˆd\n\r\r\r\n2. Substituting these bounds we get the following inequality.\n\r\r\r∇L(ˆh; M) −∇L(ˆh; ˆ\nM)\n\r\r\r\n2 ≤4S\n√\nA\nλ\nϵr\n\r\r\rd −ˆd\n\r\r\r\n2 +\n \n4γ\n√\nSA\nλ\n+ 6γ\n√\nAS\nλ\n3S\n(1 −γ)2\n!\nϵp\n\r\r\rd −ˆd\n\r\r\r\n2\n≤\n \n4S\n√\nAϵr\nλ\n+ 10γS2√\nAϵp\nλ(1 −γ)2\n! \r\r\rd −ˆd\n\r\r\r\n2\nWe now substitute the above bound in equation 15.\n−A(1 −γ)2\nλ\n\r\r\rh −ˆh\n\r\r\r\n2\n2 ≥(h −ˆh)⊤∇L(ˆh; M)\n= (h −ˆh)⊤\u0010\n∇L(ˆh; M) −∇L(ˆh; ˆ\nM)\n\u0011\n[As ˆh is optimal for L(·; ˆ\nM)]\n≥−\n\r\r\rh −ˆh\n\r\r\r\n2\n\r\r\r∇L(ˆh; M) −∇L(ˆh; ˆ\nM)\n\r\r\r\n2\n≥−\n\r\r\rh −ˆh\n\r\r\r\n2\n \n4S\n√\nAϵr\nλ\n+ 10γS2√\nAϵp\nλ(1 −γ)2\n! \r\r\rd −ˆd\n\r\r\r\n2\n20\nRearranging we get the following inequality.\n\r\r\rh −ˆh\n\r\r\r\n2 ≤\nλ\nA(1 −γ)2\n \n4S\n√\nAϵr\nλ\n+ 10γS2√\nAϵp\nλ(1 −γ)2\n! \r\r\rd −ˆd\n\r\r\r\n2\nRecall that GD(d) (resp. GD( ˆd)) are the optimal solution corresponding to the primal problem\nwhen the deployed occupancy measure is d (resp. ˆd). Therefore, we can apply lemma 1 to obtain\nthe following bound.\n\r\r\rGD(d) −GD( ˆd)\n\r\r\r\n2 ≤\n\n1 +\n4ϵr + 6ϵp\n\r\r\rˆh\n\r\r\r\n2\nλ\n\n3\n√\nAS\nλ\n\r\r\rh −ˆh\n\r\r\r\n2\n≤\n\u0012\n1 + 4ϵr + 6ϵp · 3S/(1 −γ)2\nλ\n\u0013 3\n√\nAS\nλ\nλ\nA(1 −γ)2\n \n4S\n√\nAϵr\nλ\n+ 10γS2√\nAϵp\nλ(1 −γ)2\n! \r\r\rd −ˆd\n\r\r\r\n2\n≤\n\u0012\n1 + 4ϵr + 6ϵp · 3S/(1 −γ)2\nλ\n\u0013\n3\n√\nS\n√\nA(1 −γ)2\n \n4S\n√\nAϵr\nλ\n+ 10γS2√\nAϵp\nλ(1 −γ)2\n!\n|\n{z\n}\n:=β\n\r\r\rd −ˆd\n\r\r\r\n2\nNow it can be easily veriﬁed that if λ > 12S3/2(1−γ)−4(2ϵr +5Sϵp) then β = 12S3/2(2ϵr+5Sϵp)\nλ(1−γ)4\n< 1.\nThis implies that the operator GD(·) is a contraction mapping and the sequence of iterates {dt}t≥1\nconverges to a ﬁxed point.\nIn order to determine the speed of convergence let us substitute\nd = dt and ˆd = dS. This gives us ∥GD(dt) −dS∥2 ≤β ∥dt −dS∥2. As GD(dt) = dt+1 we have\n∥dt+1 −dS∥2 ≤β ∥dt −dS∥2. After t iterations we have ∥dt −dS∥2 ≤βt ∥d0 −dS∥2. Therefore, if\nt ≥ln (∥d0 −dS∥2 /δ) / ln(1/β) we are guaranteed that ∥dt −dS∥2 ≤δ. Since ∥d0 −dS∥2 ≤\n2\n1−γ ,\nthe desired upper bound on the number of iterations becomes the following.\nln (∥d0 −dS∥2 /δ)\nln(1/β)\n≤2 (1 −β)−1 ln\n\u0012\n2\nδ(1 −γ)\n\u0013\nLemma 1. Consider two state-action occupancy measures d and ˆd. Let λ ≥2\n\u0010\n2ϵr + 3ϵp\n\r\r\rˆh\n\r\r\r\n2\n\u0011\n.\nThen we have the following bound.\n\r\r\rd −ˆd\n\r\r\r\n2 ≤\n\n1 +\n4ϵr + 6ϵp\n\r\r\rˆh\n\r\r\r\n2\nλ\n\n3\n√\nAS\nλ\n\r\r\rh −ˆh\n\r\r\r\n2\nProof. Recall the relationship between the dual and the primal variables.\nd(s, a) = rt(s, a)\nλ\n−h(s)\nλ\n+ γ\nλ\nX\nes\nh(es)P(s, a, es)\n21\nThis gives us the following bound on the diﬀerence (d(s, a) −ˆd(s, a))2.\n\u0010\nd(s, a) −ˆd(s, a)\n\u00112\n≤3\nλ2 (r(s, a) −ˆr(s, a))2 + 1\nλ2\n\u0010\nh(s) −ˆh(s)\n\u00112\n+ 3γ2\nλ2\n X\ns′\nh(s′)P(s, a, s′) −\nX\ns′\nˆh(s′) ˆP(s, a, s′)\n!2\n[By Jensen’s inequality]\n= 3\nλ2 (r(s, a) −ˆr(s, a))2 + 1\nλ2\n\u0010\nh(s) −ˆh(s)\n\u00112\n+ 3\nλ2\n X\ns′\n\u0010\nh(s′) −ˆh(s′)\n\u0011\nP(s, a, s′) + ˆh(s′)\n\u0010\nP(s, a, s′) −ˆP(s, a, s′)\n\u0011!2\n≤3\nλ2 (r(s, a) −ˆr(s, a))2 + 1\nλ2\n\u0010\nh(s) −ˆh(s)\n\u00112\n+ 6\nλ2\n X\ns′\n\u0010\nh(s′) −ˆh(s′)\n\u0011\nP(s, a, s′)\n!2\n+ 6\nλ2\n X\ns′\nˆh(s′)\n\u0010\nP(s, a, s′) −ˆP(s, a, s′)\n\u0011!2\n[By Jensen’s inequality]\n≤3\nλ2 (r(s, a) −ˆr(s, a))2 + 1\nλ2\n\u0010\nh(s) −ˆh(s)\n\u00112\n+ 6\nλ2\n\r\r\rh −ˆh\n\r\r\r\n2\n2 + 6\nλ2\n\r\r\rˆh\n\r\r\r\n2\n2\nX\ns′\n\u0010\nP(s, a, s′) −ˆP(s, a, s′)\n\u00112\n[By Cauchy-Schwarz inequality]\nNow summing over s and a we get the following bound.\n\r\r\rd −ˆd\n\r\r\r\n2\n2 ≤3\nλ2 ∥r −ˆr∥2\n2 + 7AS\nλ2\n\r\r\rh −ˆh\n\r\r\r\n2\n2 + 6\nλ2\n\r\r\rˆh\n\r\r\r\n2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\nWe now use the assumptions ∥r −ˆr∥2 ≤ϵr\n\r\r\rd −ˆd\n\r\r\r\n2 and\n\r\r\rP −ˆP\n\r\r\r\n2 ≤ϵp\n\r\r\rd −ˆd\n\r\r\r\n2.\n\r\r\rd −ˆd\n\r\r\r\n2 ≤2ϵr\nλ\n\r\r\rd −ˆd\n\r\r\r\n2 + 3\n√\nAS\nλ\n\r\r\rh −ˆh\n\r\r\r\n2 + 3ϵp\nλ\n\r\r\rˆh\n\r\r\r\n2\n\r\r\rd −ˆd\n\r\r\r\n2\nRearranging we get the following bound.\n\r\r\rd −ˆd\n\r\r\r\n2 ≤\n\n1 −\n2ϵr + 3ϵp\n\r\r\rˆh\n\r\r\r\n2\nλ\n\n\n−1\n3\n√\nAS\nλ\n\r\r\rh −ˆh\n\r\r\r\n2 ≤\n\n1 +\n4ϵr + 6ϵp\n\r\r\rˆh\n\r\r\r\n2\nλ\n\n3\n√\nAS\nλ\n\r\r\rh −ˆh\n\r\r\r\n2\nThe last inequality uses the fact that λ ≥2(2ϵr + 3ϵp\n\r\r\rˆh\n\r\r\r\n2).\nLemma 2. The dual objective Ld (as deﬁned in 12) is A(1−γ)2\nλ\n-strongly convex.\n22\nProof. The derivative of the dual objective Ld with respect to h(s) is given as follows.\n∂Ld(h)\n∂h(s) = −1\nλ\nX\na\nrt(s, a) + γ\nλ\nX\ns′,a\nrt(s′, a)Pt(s′, a, s) + ρ(s) + A\nλ h(s)\n−γ\nλ\nX\nes,a\nh(es) (Pt(s, a, es) + Pt(es, a, s)) + γ2\nλ\nX\ns′,a,es\nh(es)Pt(s′, a, es)Pt(s′, a, s)\n(16)\nThis gives us the following identity.\n\u0010\n∇Ld(h) −∇Ld(eh)\n\u0011⊤\n(h −eh) = A\nλ\n\r\r\rh −eh\n\r\r\r\n2\n2\n−γ\nλ\nX\ns,es,a\n(h(s) −eh(s)) (Pt(s, a, es) + Pt(es, a, s)) (h(es) −eh(es))\n+ γ2\nλ\nX\ns′,a\nX\ns,es\n(h(s) −eh(es))Pt(s′, a, es)Pt(s′, a, s)(h(s) −eh(s))\nLet us now deﬁne the matrix Ma ∈RS×S with entries Ma(s, s′) = Pt(s, a, s′).\n\u0010\n∇Ld(h) −∇Ld(eh)\n\u0011⊤\n(h −eh) = A\nλ\n\r\r\rh −eh\n\r\r\r\n2\n2\n−γ\nλ\nX\na\n(h −eh)⊤\u0010\nMa + M⊤\na\n\u0011\n(h −eh) + γ2\nλ\nX\na\n(h −eh)⊤M⊤\na Ma(h −eh)\n= 1\nλ\nX\na\n(h −eh)⊤\u0010\nId −γMa −γM⊤\na + γ2M⊤\na Ma\n\u0011\n(h −eh)\n≥A(1 −γ)2\nλ\n\r\r\rh −eh\n\r\r\r\n2\n2\nThe last inequality uses lemma 5.\nLemma 3. The dual function Ld (as deﬁned in eq. (12)) satisﬁes the following bound for any h and\nMDP M, c\nM.\n\r\r\r∇Ld(h, M) −∇Ld(h, c\nM)\n\r\r\r\n2 ≤4S\n√\nA\nλ\n∥r −ˆr∥2 +\n \n4γ\n√\nSA\nλ\n+ 6γS\n√\nA\nλ\n∥h∥2\n! \r\r\rP −ˆP\n\r\r\r\n2\nProof. From the expression of the derivative of Ld with respect to h (eq. (16)) we get the following\n23\nbound.\n\r\r\r∇Ld(h, M) −∇Ld(h, c\nM)\n\r\r\r\n2\n2 =\nX\ns\n(\n−1\nλ\nX\na\n(r(s, a) −ˆr(s, a))\n+ γ\nλ\nX\ns′,a\n\u0010\nr(s′, a)P(s′, a, s) −ˆr(s′, a) ˆP(s′, a, s)\n\u0011\n−γ\nλ\nX\nes,a\nh(es)(P(es, a, s) −ˆP(es, a, s))\n−γ\nλ\nX\nes,a\nh(es)(P(s, a, es) −ˆP(s, a, es)) + γ2\nλ\nX\ns′,a,es\nh(es)\n\u0010\nP(s′, a, es)P(s′, a, s) −ˆP(s′, a, es) ˆP(s′, a, s)\n\u0011\n\n\n\n≤5A\nλ2 ∥r −ˆr∥2\n2 + 5γ2\nλ2\nX\ns\n\nX\ns′,a\n\u0010\nr(s′, a)P(s′, a, s) −ˆr(s′, a) ˆP(s′, a, s)\n\u0011\n\n\n2\n+ 5γ2\nλ2\nX\ns\n\nX\nes,a\nh(s)(P(es, a, s) −ˆP(es, a, s))\n\n\n2\n+ 5γ2\nλ2\nX\ns\n\nX\nes,a\nh(es)(P(s, a, es) −ˆP(s, a, es))\n\n\n2\n+ 5γ4\nλ2\nX\ns\n\nX\ns′,a,es\nh(es)\n\u0010\nP(s′, a, es)P(s′, a, s) −ˆP(s′, a, es) ˆP(s′, a, s)\n\u0011\n\n\n2\nWe now use four bounds to complete the proof. The following bounds use Jensen’s inequality and\nCauchy-Schwarz inequality.\nBound 1 :\nX\ns′,a\n\u0010\nr(s′, a)P(s′, a, s) −ˆr(s′, a) ˆP(s′, a, s)\n\u0011\n≤\nX\ns′,a\n\f\fr(s′, a) −ˆr(s′, a)\n\f\f P(s′, a, s) + ˆr(s′, a)\n\f\f\fP(s′, a, s) −ˆP(s′, a, s)\n\f\f\f\n≤∥r −ˆr∥1 +\nX\ns′,a\n\f\f\fP(s′, a, s) −ˆP(s′, a, s)\n\f\f\f\nBound 2 :\nX\ns\n\nX\nes,a\nh(es)(P(es, a, s) −ˆP(es, a, s))\n\n\n2\n≤A\nX\ns\nX\nes\nh(es)2 X\nes,a\n\u0010\nP(es, a, s) −ˆP(es, a, s)\n\u00112\n≤A ∥h∥2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\nBound 3 :\nX\ns\n\nX\nes,a\nh(es)(P(s, a, es) −ˆP(s, a, es))\n\n\n2\n≤A\nX\ns\nX\nes\nh(es)2 X\nes,a\n\u0010\nP(s, a, es) −ˆP(s, a, es)\n\u00112\n≤A ∥h∥2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\n24\nBound 4 :\nX\ns\n\nX\ns′,a,es\nh(es)\n\u0010\nP(s′, a, es)P(s′, a, s) −ˆP(s′, a, es) ˆP(s′, a, s)\n\u0011\n\n\n2\n≤\nX\ns\nX\nes\nh(es)2 X\nes\n\nX\ns′,a\n\u0010\nP(s′, a, es)P(s′, a, s) −ˆP(s′, a, es) ˆP(s′, a, s)\n\u0011\n\n\n2\n≤SA ∥h∥2\n2\nX\ns,es\nX\ns′,a\n\u0010\nP(s′, a, es)P(s′, a, s) −ˆP(s′, a, es) ˆP(s′, a, s)\n\u00112\n≤SA ∥h∥2\n2\nX\ns,es\nX\ns′,a\n\u0010\nP(s′, a, es)(P(s′, a, s) −ˆP(s′, a, s)) + ˆP(s′, a, s)(P(s′, a, es) −ˆP(s′, a, es))\n\u0011\n≤2SA ∥h∥2\n2\nX\ns,es\nX\ns′,a\n\f\f\fP(s′, a, s) −ˆP(s′, a, s)\n\f\f\f\n2\n+\n\f\f\fP(s′, a, es) −ˆP(s′, a, es)\n\f\f\f\n2\n≤4S2A ∥h∥2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\nUsing the four upper bounds shown above, we can complete the proof.\n\r\r\r∇Ld(h, M) −∇Ld(h, c\nM)\n\r\r\r\n2\n2 ≤5A\nλ2 ∥r −ˆr∥2\n2 + 5γ2\nλ2\nX\ns\n\u0010\n∥r −ˆr∥1 +\n\r\r\rP(·, ·, s) −ˆP(·, ·, s)\n\r\r\r\n1\n\u00112\n+ 10Aγ2\nλ2\n∥h∥2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2 + 20S2Aγ4\nλ2\n∥h∥2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\n≤\n\u00125A\nλ2 + 10S2Aγ2\nλ2\n\u0013\n∥r −ˆr∥2\n2 +\n\u001210γ2SA\nλ2\n+ 10Aγ2\nλ2\n∥h∥2\n2 + 20S2Aγ4\nλ2\n∥h∥2\n2\n\u0013 \r\r\rP −ˆP\n\r\r\r\n2\n2\nLemma 4. The norm of the optimal solution to the dual problem (deﬁned in 12) is bounded by\n3S\n(1−γ)2 for any choice of MDP M.\nProof. The dual objective Ld is strongly-convex and has a unique solution. The optimal solution\ncan be obtained by setting the derivative with respect to h to zero. Rearranging the derivative of\nthe dual objective (16) we get the following systems of equations.\nh(s)\n\nA\nλ −2γ\nλ\nX\na\nPt(s, a, s) + γ2\nλ\nX\ns′,a\nPt(s′, a, s)2\n\n\n+\nX\nˆs̸=s\nh(ˆs)\n\n−γ\nλ\nX\na\nPt(s, a, ˆs) −γ\nλ\nX\na\nPt(ˆs, a, s) + γ2\nλ\nX\ns′,a\nPt(s′, a, ˆs)Pt(s′, a, s)\n\n\n= 1\nλ\nX\na\nrt(s, a) −ρ(s) −γ\nλ\nX\ns′,a\nrt(s′, a)Pt(s′, a, s)\n25\nTherefore let us deﬁne a matrix B ∈RS×S and a vector b ∈RS with the following entries.\nB(s, ˆs) =\n(\nA\nλ −2γ\nλ\nP\na Pt(s, a, s) + γ2\nλ\nP\ns′,a Pt(s′, a, s)2\nif s = ˆs\n−γ\nλ\nP\na Pt(s, a, ˆs) −γ\nλ\nP\na Pt(ˆs, a, s) + γ2\nλ\nP\ns′,a Pt(s′, a, ˆs)Pt(s′, a, s)\no.w.\nb(s) = 1\nλ\nX\na\nrt(s, a) −ρ(s) −γ\nλ\nX\ns′,a\nrt(s′, a)Pt(s′, a, s)\nThen the optimal solution is the solution of the system of equations Bh = b. We now provide a\nbound on the L2-norm of such a solution. For each a, we deﬁne matrix Ma ∈RS×S with entries\nMa(s, ˆs) = Pt(s, a, ˆs). Then the matrix B can be expressed as follows.\nB = A\nλ Id −γ\nλ\nX\na\n\u0010\nMa + M⊤\na\n\u0011\n+ γ2\nλ\nX\na\nM⊤\na Ma ≽A(1 −γ)2\nλ\nId\nThe last inequality uses lemma 5. Notice that for γ < 1 this also shows that the matrix is invertible.\nWe can also bound the norm of the vector b.\n∥b∥2\n2 ≤\nX\ns\n\nA\nλ + ρ(s) + γ\nλ\nX\ns′,a\nPt(s′, a, s)\n\n\n2\n≤3A2S\nλ2 + 3 ∥ρ∥2\n2 + 3γ2\nλ2\nX\ns\n\nX\ns′,a\nPt(s′, a, s)\n\n\n2\n≤3A2S\nλ2 + 3 + 3SAγ2\nλ2\nX\ns\nX\ns′,a\nPt(s′, a, s) ≤9S2A2\nλ2\nTherefore, we have the following bound on the optimal value.\n\r\rA−1b\n\r\r\n2 ≤\n∥b∥2\nλmin(A) ≤\n3S\n(1 −γ)2\nLemma 5. For each a, let the matrix Ma ∈RS×S be deﬁned so that Ma(s, s′) = P(s, a, s′).\nλmin\n X\na\nId −γ(Ma + M⊤\na ) + γ2M⊤\na Ma\n!\n≥A(1 −γ)2\nProof. Let Ma = UaΣaU⊤\na be the Eigen-decomposition of the matrix Ma. Then we have\nX\na\nId −γ(Ma + M⊤\na ) + γ2M⊤\na Ma =\nX\na\n(Id −γMa)⊤(Id −γMa)\n=\nX\na\n\u0010\nUa(Id −γΣa)U⊤\na\n\u0011⊤\u0010\nUa(Id −γΣa)U⊤\na\n\u0011\n=\nX\na\nUa (Id −γΣa)2 U⊤\na\n≽\nX\na\nId(1 −γ)2 = A(1 −γ)2Id\nThe last line follows the largest eigenvalue of Ma is 1, and therefore the smallest diagonal entry of\nthe matrix (Id −γΣa)2 is at least (1 −γ)2.\n26\nB.2\nProof of Convergence of Repeated Gradient Ascent (Theorem 2)\nProof. The dual of the optimization problem 8 is given as follows.\nmax\nh∈RS\nX\ns,a\nh(s) ((1 −ηλ)dt(s, a) + ηrt(s, a)) −\nX\ns\nh(s)ρ(s)\n−γ ·\nX\ns\nh(s)\nX\ns′,a\nPt(s′, a, s)\n\u0000(1 −ηλ)dt(s′, a) + ηrt(s′, a)\n\u0001\n−A\n2\nX\ns\nh(s)2\n+ γ ·\nX\ns\nh(s)\nX\ns′,a\nh(s′)Pt(s′, a, s) −γ2\n2\nX\ns′,s′′\nh(s′)h(s′′)\nX\ns,a\nPt(s, a, s′)Pt(s, a, s′′)\n(17)\nWe will consider the equivalent minimization problem.\nmin\nh∈RS −\nX\ns,a\nh(s) ((1 −ηλ)dt(s, a) + ηrt(s, a)) +\nX\ns\nh(s)ρ(s)\n+ γ ·\nX\ns\nh(s)\nX\ns′,a\nPt(s′, a, s)\n\u0000(1 −ηλ)dt(s′, a) + ηrt(s′, a)\n\u0001\n+ A\n2\nX\ns\nh(s)2\n−γ ·\nX\ns\nh(s)\nX\ns′,a\nh(s′)Pt(s′, a, s) + γ2\n2\nX\ns′,s′′\nh(s′)h(s′′)\nX\ns,a\nPt(s, a, s′)Pt(s, a, s′′)\n(18)\nLet us call the above objective function P(·; M) for a given MDP M. Consider two occupancy\nmeasures d and ˆd. Let r (resp. ˆr) be the reward functions in response to the occupancy measure\nd (resp. ˆd) i.e. r = R(d) and ˆr = R( ˆd). Similarly let P (resp. ˆP) be the probability transition\nfunctions in response to the occupancy measures d (resp. ˆd).\nWe will write GD(·) to denote the projected gradient ascent step deﬁned in eq. (7). In particular,\nif we write C to deﬁne the set of occupancy measures feasible with respect to P, then we have\nGD(d) = ProjC ((1 −ηλ)d + ηr)\n(19)\nNote that GDη(d) is the optimal solution to the primal problem 8 with dt = d. Let h be the\ncorresponding dual optimal solution. Similarly let ˆh be the optimal dual solution corresponding to the\noccupancy measure ˆd. Since h is the unique minimizer of P(·; M) and P(·; M) is A(1 −2γ)-strongly\nconvex for any M (lemma 7) we have the following set of inequalities.\nP(h; M, d) −P(ˆh; M, d) ≥(h −ˆh)⊤∇P(ˆh; M, d) + A(1 −γ)2/2\n\r\r\rh −ˆh\n\r\r\r\n2\n2\nP(ˆh; M, d) −P(h; M, d) ≥A(1 −γ)2/2\n\r\r\rh −ˆh\n\r\r\r\n2\n2\nThese two inequalities give us the following bound.\n−A(1 −γ)2 \r\r\rh −ˆh\n\r\r\r\n2\n2 ≥(h −ˆh)⊤∇P(ˆh; M, d)\n(20)\n27\nWe now apply lemma 8 to bound the Lipschitz constant of the term (h −ˆh)⊤∇P(ˆh; M).\n\r\r\r∇P(ˆh; M, d) −∇P(ˆh; ˆ\nM, ˆd)\n\r\r\r\n2\n2 ≤5A(1 −ηλ)2(1 + 2γ2S2)\n\r\r\rd −ˆd\n\r\r\r\n2\n2 + 5η2A\n\u0000(1 −ηλ)2 + 2γ2S2\u0001\n∥r −ˆr∥2\n2\n+ 5γ2SA\n\u00122(1 −ηλ)2\n(1 −γ)2\n+ 2η2 + 6γ2 \r\r\rˆh\n\r\r\r\n2\n2\n\u0013 \r\r\rP −ˆP\n\r\r\r\n2\n2\n≤\n\u00005A(1 −ηλ)2(1 + η2ϵ2\nr + 2γ2S2) + 10η2γ2AS2ϵ2\nr\n\u0001 \r\r\rd −ˆd\n\r\r\r\n2\n2\n+ 5γ2SA\n\u00122(1 −ηλ)2\n(1 −γ)2\n+ 2η2 + 12γ2\n\u0012(1 + 2ηS)2\n(1 −γ)4\n+ 4(1 −ηλ)2\nS2\nA(1 −γ)6\n\u0013\u0013\nϵ2\np\n\r\r\rd −ˆd\n\r\r\r\n2\n2\n≤\n\u0012\n5A(1 −ηλ)2\n\u0012\n1 + η2ϵ2\nr + 2γ2S2 +\n2γ2S\n(1 −γ)2 ϵ2\np +\n48γ4S3\nA(1 −γ)6 ϵ2\np\n\u0013\n+ 10η2γ2AS2ϵ2\nr + 10η2γ2SAϵ2\np + 60γ4SAϵ2\np\n(1 + 2ηS)2\n(1 −γ)4\n\u0013 \r\r\rd −ˆd\n\r\r\r\n2\n2\n≤\n\u0012\n5A(1 −ηλ)2\n\u0012\n1 + η2ϵ2\nr + 2γ2S2 + 50γ2S3\n(1 −γ)6 ϵ2\np\n\u0013\n+ 10η2γ2S2A(ϵ2\np + ϵ2\nr) + 60γ4SAϵ2\np\n(1 + 2ηS)2\n(1 −γ)4\n\u0013\n|\n{z\n}\n:=∆2\n\r\r\rd −ˆd\n\r\r\r\n2\n2\nThe last inequality uses lemma 9 and assumption 1. Substituting this bound in equation 20 we get\nthe following inequality.\n−A(1 −γ)2 \r\r\rh −ˆh\n\r\r\r\n2\n2 ≥(h −ˆh)⊤∇P(ˆh; M, d)\n= (h −ˆh)⊤∇P(ˆh; M, d) −(h −ˆh)⊤∇P(ˆh; ˆ\nM, ˆd)\n≥−\n\r\r\rh −ˆh\n\r\r\r\n2\n\r\r\r∇P(ˆh; M, d) −∇P(ˆh; ˆ\nM, ˆd)\n\r\r\r\n2 ≥−∆\n\r\r\rh −ˆh\n\r\r\r\n2\n\r\r\rd −ˆd\n\r\r\r\n2\nRearranging we get the following inequality.\n\r\r\rh −ˆh\n\r\r\r\n2 ≤\n∆\nA(1 −γ)2\n\r\r\rd −ˆd\n\r\r\r\n2\n⇒\n\r\r\rh −ˆh\n\r\r\r\n2\n2 ≤\n∆2\nA2(1 −γ)4\n\r\r\rd −ˆd\n\r\r\r\n2\n2\n⇒\n\r\r\rGDη(d) −GDη( ˆd)\n\r\r\r\n2\n2\n8γ2SA\n−\n4(1 −ηλ)2 + 4η2ϵ2\nr + 8γ2 \r\r\rˆh\n\r\r\r\n2\n2 ϵ2\np\n8γ2SA\n\r\r\rd −ˆd\n\r\r\r\n2\n2 ≤\n∆2\nA2(1 −γ)4\n\r\r\rd −ˆd\n\r\r\r\n2\n2\nThe last line uses lemma 6. After rearranging we get the following inequality.\n\r\r\rGDη(d) −GDη( ˆd)\n\r\r\r\n2\n2 ≤\n\u0012\n4(1 −ηλ)2 + 4η2ϵ2\nr + 8γ2 \r\r\rˆh\n\r\r\r\n2\n2 ϵ2\np + 8γ2∆2S\nA(1 −γ)4\n\u0013 \r\r\rd −ˆd\n\r\r\r\n2\n2\n≤\n \n4(1 −ηλ)2 + 4η2ϵ2\nr + 16γ2ϵ2\np(1 + 2ηS)2\n(1 −γ)4\n+ 32(1 −ηλ)2\nγ2ϵ2\npS2\nA(1 −γ)6 + 8γ2∆2S\nA(1 −γ)4\n! \r\r\rd −ˆd\n\r\r\r\n2\n2\nFor η = 1/λ we get the following bound.\n\r\r\rGDη(d) −GDη( ˆd)\n\r\r\r\n2\n2 ≤\n \n4ϵ2\nr\nλ2 + 16γ2ϵ2\np(1 + 2S/λ)2\n(1 −γ)4\n+ 80γ4S3(ϵ2\nr + ϵ2\np)\nλ2(1 −γ)4\n+ 480γ6S2ϵ2\np(1 + 2S/λ)2\n(1 −γ)8\n! \r\r\rd −ˆd\n\r\r\r\n2\n2\n28\nIf we choose λ ≥max\nn\n4ϵr, 2S, 20γ2S1.5(ϵr+ϵp)\n(1−γ)2\no\nwe get the following condition.\n\r\r\rGDη(d) −GDη( ˆd)\n\r\r\r\n2\n2 ≤\n \n1\n2 + 64γ2ϵ2\np\n(1 −γ)4 + 1920γ6S2ϵ2\np\n(1 −γ)8\n! \r\r\rd −ˆd\n\r\r\r\n2\n2\nFor a contraction mapping we need the following condition.\n64γ2ϵ2\np\n(1 −γ)4\n\u0012\n1 + 30γ4S2\n(1 −γ)4\n\u0013\n< 1\n2\nWe consider two cases. First, if 30γ4S2\n(1−γ)4 < 1 then one can show that a suﬃcient condition is ϵp < γS/3.\nOn the other hand, if 30γ4S2\n(1−γ)4 ≥1 then we need ϵp < (1−γ)4\n100γ3S . Combining the two conditions above a\nsuﬃcient condition for contraction is the following.\nϵp < min\n\u001aγS\n3 , (1 −γ)4\n100γ3S\n\u001b\nNow if we set µ =\nr\n1\n2 +\n64γ2ϵ2p\n(1−γ)4 +\n1920γ6S2ϵ2p\n(1−γ)8\nwe get the contraction mapping:\n\r\r\rGDη(d) −GDη( ˆd)\n\r\r\r\n2 ≤\nµ\n\r\r\rd −ˆd\n\r\r\r\n2. Let d⋆be the ﬁxed point of this contraction mapping. Using d = dt and ˆd = d⋆we get\nthe following sequence of inequalities.\n∥dt+1 −d⋆∥2 ≤µ ∥dt −d⋆∥2 ≤. . . ≤µt ∥d1 −d⋆∥2 ≤µt\n2\n1 −γ\nThe last inequality uses the fact that for any occupancy measure d we have ∥d∥2 ≤∥d∥1 ≤\n1\n1−γ .\nRearranging we get that as long as t ≥ln\n\u0010\n2\nδ(1−γ)\n\u0011\n/ ln(1/µ) we have ∥dt −d⋆∥2 ≤δ.\nWe now show that the ﬁxed point d⋆is a stable point. In response to d⋆, let the probability\ntransition function (resp. reward function) be P ⋆(resp. d⋆). Let C⋆be the set of occupancy measures\ncorresponding to d⋆. Note that C⋆is a convex set. We consider two cases. First, (1−ηλ)d⋆+ηr⋆∈C⋆.\nThen d⋆= GDη(d⋆) = (1 −ηλ)d⋆+ ηr⋆and r⋆−λd⋆= ∇P(d⋆; P ⋆, r⋆) = 0. Since P(·; P ⋆, r⋆) is a\nconcave function the occupancy measure d⋆is the optimal point and is a stable point.\nSecond, we consider the case when (1 −ηλ)d⋆+ ηr⋆/∈C⋆. Since d⋆= ProjC⋆((1 −ηλ)d⋆+ ηr⋆).\nSince C⋆is a convex set, by the projection theorem (see e.g. [Ber09]) we have the following inequality\nfor any d ∈C⋆.\n((1 −ηλ)d⋆+ ηr⋆−d⋆)⊤(d −d⋆) ≤0\n⇒η(λd⋆−r⋆)⊤(d −d⋆) ≤0\n⇒∇P(d⋆; P ⋆, r⋆)⊤(d −d⋆) ≥0\nThis implies that d⋆maximizes the function P(·; P ⋆, r⋆) over the set C⋆and is a stable point.\nLemma 6. Consider two state-action occupancy measures d and ˆd. Let h (resp. ˆh) be the dual\noptimal solutions to the projection (eq. (18)) corresponding to occupancy measure dt = d (resp. ˆd).\nThen we have the following inequality.\n\r\r\rGDη(d) −GDη( ˆd)\n\r\r\r\n2\n2 ≤\n\u0012\n4(1 −ηλ)2 + 4η2ϵ2\nr + 8γ2 \r\r\rˆh\n\r\r\r\n2\n2 ϵ2\np\n\u0013 \r\r\rd −ˆd\n\r\r\r\n2\n2 + 8γ2SA\n\r\r\rh −ˆh\n\r\r\r\n2\n2\n29\nProof. Recall the relationship between the dual and the primal variables.\nGDη(d)(s, a) = (1 −ηλ)d(s, a) + ηr(s, a) −h(s) + γ\nX\nes\nh(es)P(s, a, es)\nThis gives us the following bound on the diﬀerence (GDη(d)(s, a) −GDη( ˆd)(s, a))2.\n\u0010\nGDη(d)(s, a) −GDη( ˆd)(s, a)\n\u00112\n≤4(1 −ηλ)2 \u0010\nd(s, a) −ˆd(s, a)\n\u00112\n+ 4η2 (r(s, a) −ˆr(s, a))2\n+ 4\n\u0010\nh(s) −ˆh(s)\n\u00112\n+ 4γ2\n X\ns′\nh(s′)P(s, a, s′) −\nX\ns′\nˆh(s′) ˆP(s, a, s′)\n!2\n≤4(1 −ηλ)2 \u0010\nd(s, a) −ˆd(s, a)\n\u00112\n+ 4η2 (r(s, a) −ˆr(s, a))2\n+ 4\n\u0010\nh(s) −ˆh(s)\n\u00112\n+ 4γ2\n X\ns′\n\u0010\nh(s′) −ˆh(s′)\n\u0011\nP(s, a, s′) + ˆh(s′)\n\u0010\nP(s, a, s′) −ˆP(s, a, s′)\n\u0011!2\n≤4(1 −ηλ)2 \u0010\nd(s, a) −ˆd(s, a)\n\u00112\n+ 4η2 (r(s, a) −ˆr(s, a))2\n+ 8γ2\n X\ns′\n\u0010\nh(s′) −ˆh(s′)\n\u0011\nP(s, a, s′)\n!2\n+ 8γ2\n X\ns′\nˆh(s′)\n\u0010\nP(s, a, s′) −ˆP(s, a, s′)\n\u0011!2\n≤4(1 −ηλ)2 \u0010\nd(s, a) −ˆd(s, a)\n\u00112\n+ 4η2 (r(s, a) −ˆr(s, a))2\n+ 8γ2 \r\r\rh −ˆh\n\r\r\r\n2\n2 + 8γ2 \r\r\rˆh\n\r\r\r\n2\n2\nX\ns′\n\u0010\nP(s, a, s′) −ˆP(s, a, s′)\n\u00112\nNow summing over s and a we get the following bound.\n\r\r\rGDη(d) −GDη( ˆd)\n\r\r\r\n2\n2 ≤4(1 −ηλ)2 \r\r\rd −ˆd\n\r\r\r\n2\n2 + 4η2 ∥r −ˆr∥2\n2 + 8γ2SA\n\r\r\rh −ˆh\n\r\r\r\n2\n2 + 8γ2 \r\r\rˆh\n\r\r\r\n2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\nWe now use the assumptions ∥r −ˆr∥2 ≤ϵr\n\r\r\rd −ˆd\n\r\r\r\n2 and\n\r\r\rP −ˆP\n\r\r\r\n2 ≤ϵp\n\r\r\rd −ˆd\n\r\r\r\n2.\n\r\r\rGDη(d) −GDη( ˆd)\n\r\r\r\n2\n2 ≤\n\u0012\n4(1 −ηλ)2 + 4η2ϵ2\nr + 8γ2 \r\r\rˆh\n\r\r\r\n2\n2 ϵ2\np\n\u0013 \r\r\rd −ˆd\n\r\r\r\n2\n2 + 8γ2SA\n\r\r\rh −ˆh\n\r\r\r\n2\n2\nLemma 7. The objective function P(·; M) (as deﬁned in 18) is A(1 −γ)2-strongly convex.\nProof. The derivative of the objective function P(·; M) with respect to h(s) is given as follows.\n∂P(h; Mt)\n∂h(s)\n= −\nX\na\n((1 −ηλ)dt(s, a) + ηrt(s, a)) + ρ(s) + γ ·\nX\ns′,a\nPt(s′, a, s) ((1 −ηλ)dt(s, a) + ηrt(s, a))\n+ Ah(s) −γ ·\nX\nes,a\nh(es) (Pt(es, a, s) + Pt(s, a, es)) + γ2 X\ns′\nh(s′)\nX\nes,a\nPt(es, a, s′)Pt(es, a, s)\n(21)\n30\nThis gives us the following identity.\n\u0010\n∇P(h; Mt) −∇P(eh; Mt)\n\u0011⊤\n(h −eh) = A\n\r\r\rh −eh\n\r\r\r\n2\n2\n−γ ·\nX\ns,s′,a\n(h(s′) −eh(s′))(Pt(s′, a, s) + Pt(s, a, s′))(h(s) −eh(s))\n+ γ2 X\ns′,s\n(h(s′) −eh(s′))\nX\nes,a\nPt(es, a, s′)Pt(es, a, s)(h(s) −eh(s))\nNow for each action a, we deﬁne the following matrix Ma ∈RS×S with entries Ma(s, s′) = Pt(s, a, s′).\nNote that matrix Ma is row-stochastic and has eigenvalues bounded between −1 and 1.\n\u0010\n∇P(h; Mt) −∇P(eh; Mt)\n\u0011⊤\n(h −eh) = A\n\r\r\rh −eh\n\r\r\r\n2\n2 −γ · (h −eh)⊤\n X\na\nMa + M⊤\na\n!\n(h −eh)\n+ γ2(h −eh)⊤\n X\na\nM⊤\na Ma\n!\n(h −eh)\n= (h −eh)⊤X\na\n\u0010\nId −γ(Ma + M⊤\na ) + γ2M⊤\na Ma\n\u0011\n(h −eh)\n≥A(1 −γ)2 \r\r\rh −eh\n\r\r\r\n2\n2\nThe last line uses lemma 5.\nLemma 8. The dual function (as deﬁned in 18) satisﬁes the following guarantee for any h, occupancy\nmeasures d, ˆd, and MDP M, ˆ\nM.\n\r\r\r∇P(h; M, d) −∇P(h; c\nM, ˆd)\n\r\r\r\n2 ≤5A(1 −ηλ)2(1 + 2γ2S2)\n\r\r\rd −ˆd\n\r\r\r\n2\n2\n+ 5η2A\n\u0000(1 −ηλ)2 + 2γ2S2\u0001\n∥r −ˆr∥2\n2\n+ 5γ2SA\n\u00122(1 −ηλ)2\n(1 −γ)2\n+ 2η2 + 6γ2 ∥h∥2\n2\n\u0013 \r\r\rP −ˆP\n\r\r\r\n2\n2 t\nProof. From the expression of the derivative of the function P(·; M, d) (21) we have the following\n31\nbound.\n\r\r\r∇P(h; M, d) −∇P(h; c\nM, ˆd)\n\r\r\r\n2\n2 =\nX\ns\n(\n−(1 −ηλ)\nX\na\n(d(s, a) −ˆd(s, a))\n+ η(1 −ηλ)\nX\na\n(r(s, a) −ˆr(s, a)) + γη\nX\ns′,a\n\u0010\nP(s′, a, s)r(s, a) −ˆP(s′, a, s)ˆr(s, a)\n\u0011\n+ γ(1 −ηλ)\nX\ns′,a\n\u0010\nP(s′, a, s)d(s, a) −ˆP(s′, a, s) ˆd(s, a)\n\u0011\n−γ ·\nX\nes,a\nh(es)\n\u0010\nP(es, a, s) + P(s, a, es) −ˆP(es, a, s) −ˆP(s, a, es)\n\u0011\n+ γ2 ·\nX\ns′\nh(s′)\nX\nes,a\n\u0010\nP(es, a, s′)P(s, a, es) −ˆP(es, a, s′) ˆP(s, a, es)\n\u0011\n\n\n\n2\n≤5(1 −ηλ)2 X\ns\n X\na\n(d(s, a) −ˆd(s, a))\n!2\n+ 5η2(1 −ηλ)2 X\ns\n X\na\n(r(s, a) −ˆr(s, a))\n!2\n+ 5γ2η2 X\ns\n\nX\ns′,a\n\u0010\nP(s′, a, s)r(s, a) −ˆP(s′, a, s)ˆr(s, a)\n\u0011\n\n\n2\n+ 5γ2(1 −ηλ)2 X\ns\n\nX\ns′,a\n\u0010\nP(s′, a, s)d(s, a) −ˆP(s′, a, s) ˆd(s, a)\n\u0011\n\n\n2\n+ 5γ2 X\ns\n\nX\nes,a\nh(es)\n\u0010\nP(es, a, s) + P(s, a, es) −ˆP(es, a, s) −ˆP(s, a, es)\n\u0011\n\n\n2\n+ 5γ4 X\ns\n\nX\ns′\nh(s′)\nX\nes,a\n\u0010\nP(es, a, s′)P(s, a, es) −ˆP(es, a, s′) ˆP(s, a, es)\n\u0011\n\n\n2\nWe now establish several bounds to complete the proof. The bounds mainly use the Cauchy-Schwarz\ninequality and the Jensen’s inequality.\n32\nBound 1 :\nX\ns\n\nX\ns′,a\n\u0010\nP(s′, a, s)d(s, a) −ˆP(s′, a, s) ˆd(s, a)\n\u0011\n\n\n2\n≤\nX\ns\n\nX\ns′,a\nP(s′, a, s)\n\u0010\nd(s, a) −ˆd(s, a)\n\u0011\n+ ˆd(s, a)\n\u0010\nP(s′, a, s) −ˆP(s′, a, s)\n\u0011\n\n\n2\n≤2\nX\ns\n\nX\ns′,a\nP(s′, a, s)\n\u0010\nd(s, a) −ˆd(s, a)\n\u0011\n\n\n2\n+ 2\nX\ns\n\nX\ns′,a\nˆd(s, a)\n\u0010\nP(s′, a, s) −ˆP(s′, a, s)\n\u0011\n\n\n2\n≤2\nX\ns\nX\ns′,a\n\u0010\nd(s, a) −ˆd(s, a)\n\u00112 X\ns′,a\n\u0000P(s′, a, s)\n\u00012\n+ 2\nX\ns\nX\na\n( ˆd(s, a))2 X\na\n X\ns′\nP(s′, a, s) −ˆP(s′, a, s)\n!2\n≤2S\n\r\r\rd −ˆd\n\r\r\r\n2\n2\nX\ns,a,s′\nP(s′, a, s) +\n2AS\n(1 −γ)2\nX\ns,a,s′\n\f\f\fP(s′, a, s) −ˆP(s′, a, s)\n\f\f\f\n2\n≤2S2A\n\r\r\rd −ˆd\n\r\r\r\n2\n2 +\n2AS\n(1 −γ)2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\nSimilarly one can establish the following bound.\nBound 2 :\nX\ns\n\nX\ns′,a\n\u0010\nP(s′, a, s)r(s, a) −ˆP(s′, a, s)ˆr(s, a)\n\u0011\n\n\n2\n≤2S2A ∥r −ˆr∥2\n2 + 2AS\n\r\r\rP −ˆP\n\r\r\r\n2\n2\nBound 3 :\nX\ns\n\nX\nes,a\nh(es)\n\u0010\nP(es, a, s) + P(s, a, es) −ˆP(es, a, s) −ˆP(s, a, es)\n\u0011\n\n\n2\n≤\nX\ns\nX\nes\nh(es)2 X\nes\n X\na\n\u0010\nP(es, a, s) + P(s, a, es) −ˆP(es, a, s) −ˆP(s, a, es)\n\u0011!2\n≤A ∥h∥2\n2\nX\ns,es,a\n\u0010\nP(es, a, s) + P(s, a, es) −ˆP(es, a, s) −ˆP(s, a, es)\n\u00112\n≤2A ∥h∥2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\n33\nBound 4 :\nX\ns\n\nX\ns′\nh(s′)\nX\nes,a\n\u0010\nP(es, a, s′)P(s, a, es) −ˆP(es, a, s′) ˆP(s, a, es)\n\u0011\n\n\n2\n≤\nX\ns\nX\ns′\nh(s′)2 X\ns′\n\nX\nes,a\n\u0010\nP(es, a, s′)P(s, a, es) −ˆP(es, a, s′) ˆP(s, a, es)\n\u0011\n\n\n2\n= ∥h∥2\n2\nX\ns,s′\n\nX\nes,a\nP(s, a, es)\n\u0010\nP(es, a, s′) −ˆP(es, a, s′)\n\u0011\n+ ˆP(es, a, s′)\n\u0010\nP(s, a, es) −ˆP(s, a, es)\n\u0011\n\n\n2\n≤2 ∥h∥2\n2\nX\ns,s′\n\nX\nes,a\nP(s, a, es)\n\u0010\nP(es, a, s′) −ˆP(es, a, s′)\n\u0011\n\n\n2\n+ 2 ∥h∥2\n2\nX\ns,s′\n\nX\nes,a\nˆP(es, a, s′)\n\u0010\nP(s, a, es) −ˆP(s, a, es)\n\u0011\n\n\n2\n≤2 ∥h∥2\n2\nX\ns,s′\nX\nes,a\nP(s, a, es)\nX\nes,a\n\u0010\nP(es, a, s′) −ˆP(es, a, s′)\n\u00112\n+ 2 ∥h∥2\n2\nX\ns,s′\nX\nes,a\nˆP(es, a, s′)\n\u0010\nP(s, a, es) −ˆP(s, a, es)\n\u00112\n≤2 ∥h∥2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\nX\ns,es,a\nP(s, a, es) + 2 ∥h∥2\n2\nX\ns,es,a\n\u0010\nP(s, a, es) −ˆP(s, a, es)\n\u00112\n≤4SA ∥h∥2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\nWe now substitute the four bounds above to complete the proof.\n\r\r\r∇P(h; M, d) −∇P(h; c\nM, ˆd)\n\r\r\r\n2\n2 ≤5(1 −ηλ)2A\n\r\r\rd −ˆd\n\r\r\r\n2\n2 + 5η2(1 −ηλ)2A ∥r −ˆr∥2\n2\n+ 5γ2(1 −ηλ)2\n\u0012\n2S2A\n\r\r\rd −ˆd\n\r\r\r\n2\n2 +\n2AS\n(1 −γ)2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\n\u0013\n+ 5γ2η2\n\u0012\n2S2A ∥r −ˆr∥2\n2 + 2AS\n\r\r\rP −ˆP\n\r\r\r\n2\n2\n\u0013\n+ 10γ2 ∥h∥2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2 + 20γ4SA ∥h∥2\n2\n\r\r\rP −ˆP\n\r\r\r\n2\n2\n≤5A(1 −ηλ)2(1 + 2γ2S2)\n\r\r\rd −ˆd\n\r\r\r\n2\n2 + 5η2A\n\u0000(1 −ηλ)2 + 2γ2S2\u0001\n∥r −ˆr∥2\n2\n+ 5γ2SA\n\u00122(1 −ηλ)2\n(1 −γ)2\n+ 2η2 + 6γ2 ∥h∥2\n2\n\u0013 \r\r\rP −ˆP\n\r\r\r\n2\n2\n34\nLemma 9. For any choice of MDP M and occupancy measure d, the L2-norm of the optimal solution\nto the dual objective (as deﬁned in 18) is bounded by\n1 + 2ηS\n(1 −γ)2 + 2 |1 −ηλ| S/\n√\nA\n(1 −γ)3 .\nProof. The objective function P is strongly-convex and has a unique solution. We set the derivative\nwith respect to h to zero and get the following system of equations.\nh(s)\n\nA −2γ\nX\na\nP(s, a, s) + γ2 X\nes,a\nP(es, a, s)2\n\n\n+\nX\ns′̸=s\nh(s′)\n\n−γ\nX\na\n(P(s′, a, s) + P(s, a, s′)) + γ2 X\nes,a\nP(es, a, s′)P(es, a, s)\n\n\n=\nX\na\n((1 −ηλ)d(s, a) + ηr(s, a)) −ρ(s) −γ ·\nX\ns′,a\nP(s′, a, s) ((1 −ηλ)d(s, a) + ηr(s, a))\nLet us now deﬁne matrix B ∈RS×S and a vector b ∈RS with the following entries.\nB(s, s′) =\n\u001a\nA −2γ P\na P(s, a, s) + γ2 P\nes,a P(es, a, s)2\nif s′ = s\n−γ P\na(P(s′, a, s) + P(s, a, s′)) + γ2 P\nes,a P(es, a, s′)P(es, a, s)\no.w.\nb(s) =\nX\na\n((1 −ηλ)d(s, a) + ηr(s, a)) −ρ(s) −γ ·\nX\ns′,a\nP(s′, a, s) ((1 −ηλ)d(s, a) + ηr(s, a))\nThen the optimal solution is the solution to the system of equations Bx = b. We now provide a\nbound on the L2-norm of such a solution. For each action a, we deﬁne a matrix Ma ∈RS×S with\nentries Ma(s, s′) = P(s, a, s′). Then the matrix B can be expressed as follows.\nB = A · Id −γ\nX\na\n(Ma + M⊤\na ) + γ2 X\na\nM⊤\na Ma ≽A(1 −γ)2Id\nThe last inequality uses lemma 5. This also implies that for γ < 1 the matrix B is invertible.\nWe now bound the norm of the vector b. We will use the fact that for any state P\ns,a d(s, a) =\n1/(1 −γ).\n∥b∥2 ≤∥b∥1 ≤|1 −ηλ|\nX\ns,a\nd(s, a) + η\nX\ns,a\nr(s, a) +\nX\ns,a\nρ(s)\n+ γ |1 −ηλ| ·\nX\ns,s′,a\nP(s′, a, s)d(s, a) + ηγ\nX\ns,s′,a\nP(s′, a, s) |r(s, a)|\n≤|1 −ηλ|\n1 −γ\n+ ηSA + A + γ |1 −ηλ|\nX\ns′\nsX\ns,a\nP(s′, a, s)2\nsX\ns,a\nd(s, a)2 + ηγSA\n≤|1 −ηλ|\n1 −γ\n+ ηSA + A + γ |1 −ηλ| ∥d∥2\nX\ns′\nsX\ns,a\nP(s′, a, s) + ηγSA\n≤|1 −ηλ|\n1 −γ\n+ ηSA + A + γ |1 −ηλ| S\n√\nA\n1 −γ + ηγSA ≤A(1 + 2ηS) + 2 |1 −ηλ| S\n√\nA\n1 −γ\n35\nThe optimal solution to the dual objective is bounded by\n\r\rA−1b\n\r\r\n2 ≤\n∥b∥2\nλmin(A) ≤1 + 2ηS\n(1 −γ)2 + 2 |1 −ηλ| S/\n√\nA\n(1 −γ)3\nB.3\nFormal Statement and Proof of Convergence with Finite Samples (Theo-\nrem 3)\nTheorem 6. Suppose assumption 1 holds with λ ≥24S3/2(2ϵr+5Sϵp)\n(1−γ)4\n, and assumption 2 holds with\nparameter B. For a given δ, and error probability p, if we repeatedly solve the optimization problem 10\nwith number of samples\nmt ≥\n64A(B +\n√\nA)2\nβ4δ4(2ϵr + 5Sϵp)2\n \nln\n\u0012 t\np\n\u0013\n+ ln\n \n4S(B +\n√\nA)\nβδ(2ϵr + 5Sϵp)\n!!\nthen we have\n∥dt −dS∥2 ≤δ ∀t ≥(1 −µ)−1 ln\n\u0012\n2\nδ(1 −γ)\n\u0013\nwith probability at least 1 −p\nwhere µ = 24S3/2(2ϵr+5Sϵp)\n(1−γ)4\n.\nProof. We will write d\nGD(dt) to denote the occupancy measure dt+1.\n(d\nGD(dt), ˆht+1) = arg max\nd\narg min\nh\nˆL(d, h; Mt)\nLet us also write d\nGD(dS) to denote the primal solution corresponding to the stable solution dS i.e.\n(d\nGD(dS), ˆhS) = arg max\nd\narg min\nh\nˆL(d, h; MS)\nLet us also recall the deﬁnition of the operator GD(·). Given occupancy measure dt, GD(dt) is\nthe optimal solution to the optimization problem 5 when we use the exact model Mt. Because of\nstrong-duality this implies there exists ht+1 so that\n(GD(dt), ht+1) = arg max\nd\narg min\nh\nL(d, h; Mt)\nSince GD(dS) = dS there also exists hS so that\n(dS, hS) = arg max\nd\narg min\nh\nL(d, h; MS)\nBecause of lemma 4 we can assume the L2-norms of the dual solutions ht+1, ˆhS, and ˆhS are bounded\nby\n3S\n(1−γ)2 . Since there exists a saddle point with bounded norm, we can just consider the restricted\nset H =\nn\nh : ∥h∥2 ≤\n3S\n(1−γ)2\no\n.7 Moreover, by assumption 2 we know that GD(dt)(s, a)/dt(s, a) ≤B\n7See lemma 3 of [Zha+22] for a proof of this statement.\n36\nfor any (s, a). Therefore, we can apply lemma 10 with δ1 = p/2t2 and H = 3S/(1 −γ)2 to get the\nfollowing bound,\n\f\f\f ˆL(d, h; Mt) −L(d, h; Mt)\n\f\f\f ≤18S1.5(B +\n√\nA)ϵ\n(1 −γ)3\n(22)\nas long as\nmt ≥4A\nϵ2 (ln(t/p) + ln(S/(1 −γ)ϵ))\n(23)\nh ∈H and maxs,a d(s, a)/dt(s, a) ≤B. Since the event (22) holds at time t with probability at least\n1 −\np\n2t2 , by a union bound over all time steps, this event holds with probability at least 1 −p.\nNote that the objective L(·, ht+1; Mt) is λ-strongly concave. Therefore, we have\nL(d\nGD(dt), ht+1; Mt) −L(GD(dt), ht+1; Mt) ≤−λ\n2\n\r\r\rGD(dt) −d\nGD(dt)\n\r\r\r\n2\n2 .\nRearranging and using lemma 12 we get the following bound.\n\r\r\rGD(dt) −d\nGD(dt)\n\r\r\r\n2 ≤\nv\nu\nu\nt2\n\u0010\nL(GD(dt), ht+1; Mt) −L(d\nGD(dt), ht+1; Mt)\n\u0011\nλ\n≤\n6\nq\nS1.5(B +\n√\nA)ϵ\n(1 −γ)1.5\n1\n√\nλ\nThe proof of theorem 1 establishes that the operator GD(·) is a contraction. In particular, it\nshows that\n∥GD(dt) −dS∥2 ≤β ∥dt −dS∥2\nfor β = 12S3/2(2ϵr + 5ϵp)\nλ(1 −γ)4\nand λ > 12S3/2(1 −γ)−4(2ϵr + 5Sϵp)\nThis gives us the following recurrence relation on the iterates of the algorithm.\n∥dt+1 −dS∥2 =\n\r\r\rd\nGD(dt) −dS\n\r\r\r\n2 ≤\n\r\r\rd\nGD(dt) −GD(dt)\n\r\r\r\n2 + ∥GD(dt) −dS∥2\n≤\n6\nq\nS1.5(B +\n√\nA)ϵ\n(1 −γ)1.5\n1\n√\nλ\n+ β ∥dt −dS∥2\nWe choose λ = 24S3/2(1−γ)−4(2ϵr +5Sϵp) which ensures β < 1/2 and gives the following recurrence.\n∥dt+1 −dS∥2 ≤2\np\n1 −γ\ns\n(B +\n√\nA)ϵ\n2ϵr + 5Sϵp\n+ β ∥dt −dS∥2 ≤βδ + β ∥dt −dS∥2\n(24)\nThe last line requires the following bound on ϵ.\nϵ ≤β2δ2(2ϵr + 5Sϵp)\n4(1 −γ)(B +\n√\nA)\n(25)\n37\nSubstituting the bound on ϵ in equation (23) the required number of samples at time-step t is given\nas follows.\nmt ≥\n64A(B +\n√\nA)2\nβ4δ4(2ϵr + 5Sϵp)2\n \nln\n\u0012 t\np\n\u0013\n+ ln\n \n4S(B +\n√\nA)\nβδ(2ϵr + 5Sϵp)\n!!\nIn order to analyze the recurrence relation (24) we consider two cases. First, if ∥dt −dS∥2 ≥δ\nwe have\n∥dt+1 −dS∥2 ≤2β ∥dt −dS∥2\nSince β < 1/2 this is a contraction, and after ln(∥d0 −dS∥2)/ ln(1/2β) iterations we are guaranteed\nthat ∥dt −dS∥2 ≤δ. Since ∥d0 −dS∥2 ≤\n2\n1−γ , the required number of iterations for this event to\noccur is given by the following upper bound.\nln(∥d0 −dS∥2)\nln(1/2β)\n≤2(1 −2β)−1 ln\n\u0012\n2\nδ(1 −γ)\n\u0013\nOn the other hand, if ∥dt −dS∥≤δ, equation (24) gives us\n∥dt+1 −dS∥2 ≤2βδ < δ\n[Since β < 1/2]\nTherefore, once ∥dt −dS∥2 ≤δ we are guaranteed that ∥dt′ −dS∥2 ≤δ for any t′ ≥t.\nLemma 10. Suppose m ≥1\nϵ2 (A ln(2/δ1) + ln(4H/ϵ) + 2A ln(ln(SABH/ϵ)/ϵ)). Then for any occu-\npancy measure d satisfying maxs,a d(s, a)/dt(s, a) ≤B and any h with ∥h∥2 ≤H the following bound\nholds with probability at least 1 −δ1.\n\f\f\f ˆL(d, h; Mt) −L(d, h; Mt)\n\f\f\f ≤6H\n√\nS(B +\n√\nA)ϵ\n1 −γ\nProof. Note that the expected value of the objective above equals L(d, h; Mt).\nE\nh\nˆL(d, h; Mt)\ni\n= −λ\n2 ∥d∥2\n2 +\nX\ns\nh(s)ρ(s) + 1\nm\nm\nX\ni=1\nE\n\u0014 d(si, ai)\ndt(si, ai)\n\u0000r(si, ai) −h(si) + γh(s′\ni)\n\u0001\u0015\n= −λ\n2 ∥d∥2\n2 +\nX\ns\nh(s)ρ(s) +\nX\ns,a\ndt(s, a) d(s, a)\ndt(s, a)\n \nr(s, a) −h(s) + γ\nX\ns′\nPt(s′|s, a)h(s′)\n!\n= L(d, h; Mt)\nBy the overlap assumption 2 and the assumption ∥h∥2 ≤H the following bound holds for each i,\n1\n1 −γ\nd(si, ai)\ndt(si, ai)\n\u0000r(si, ai) −h(si) + γh(s′\ni)\n\u0001\n≤2BH\n1 −γ .\nTherefore we can apply the Chernoﬀ-Hoeﬀding inequality and obtain the following inequality.\nP\n \f\f\f ˆL(d, h; Mt) −L(d, h; Mt)\n\f\f\f ≥2BH\n1 −γ\nr\nln(2/δ1)\nm\n!\n≤δ1\n38\nWe now extend this bound to any occupancy measure d ∈D and h in the set H = {h : ∥h∥2 ≤H}.\nBy lemma 5.2 of [Ver10] we can choose an ϵ-net, Hϵ of the set H of size at most\n\u00001 + 2H\nϵ\n\u0001S and for\nany point h ∈H we are guaranteed to ﬁnd h′ ∈Hϵ so that ∥h −h′∥2.\nHowever, such an additive error bound is not suﬃcient for the set of occupancy measures because\nof the overlap assumption 2. So instead we choose a multiplicative ϵ-net as follows. For any (s, a) we\nconsider the grid points dt(s, a), (1 + ϵ)dt(s, a), . . . , (1 + ϵ)pdt(s, a) for p = log(B/dt(s, a))/ log(1 + ϵ).\nAlthough dt(s, a) can be arbitrarily small, without loss of generality we can assume that dt(s, a) ≥\nϵ\n4SABH . This is because from the expression of L(d, h; Mt) (9), it is clear that ignoring such small\nterms introduces an error of at most ϵ/4. Therefore we can choose p = 2 log(SABH/ϵ)/ log(1 + ϵ).\nTaking a Cartesian product over the set of all state, action pairs we see that we can choose an ϵ-net\nDϵ so that |Dϵ| ≤\n\u0010\n2 log(SABH/ϵ)\nlog(1+ϵ)\n\u0011SA\n≤\n\u0010\n2 log(SABH/ϵ)\nϵ\n\u0011SA\n. Notice that we are guaranteed that for\nany d ∈D there exists a d′ ∈Dϵ such that d(s, a)/d′(s, a) ≤1 + ϵ.\nBy a union bound over the elements in Hϵ and Dϵ the following bound holds for any d ∈Dϵ and\nh ∈Hϵ with probability at least 1 −δ1.\n\f\f\f ˆL(d, h; Mt) −L(d, h; Mt)\n\f\f\f ≤2BH\n1 −γ\nv\nu\nu\ntSA ln\n\u0010\n2\nδ1\n\u0011\n+ S ln\n\u0000 4H\nϵ\n\u0001\n+ 2SA ln(ln(SABH/ϵ)/ϵ)\nm\n|\n{z\n}\n:=Tm\nWe now extend the bound above for any d ∈D and h ∈H using lemma 11. There exists ed ∈Dϵ\nso that maxs,a d(s, a)/ed(s, a) ≤ϵ.\nSimilarly there exists hϵ ∈Hϵ so that\n\r\r\rh −eh\n\r\r\r\n2 ≤ϵ.\nLet\nL0(d, h; Mt) = L(d, h; Mt) + λ\n2 ∥d∥2\n2 −P\ns h(s)ρ(s).\n\f\f\f ˆL(d, h; Mt) −L(d, h; Mt)\n\f\f\f ≤\n\f\f\f ˆL0(d, h; Mt) −ˆL0(ed,eh; Mt)\n\f\f\f\n+\n\f\f\f ˆL(ed,eh; Mt) −L(ed,eh; Mt)\n\f\f\f +\n\f\f\fL0(ed,eh; Mt) −L0(d, h; Mt)\n\f\f\f\n≤2BHTm\n1 −γ\n+ 6\n√\nSAHϵ\n1 −γ\n+ 4BH\n√\nSϵ\n1 −γ\nTherefore, if m ≥1\nϵ2 (A ln(2/δ1) + ln(4H/ϵ) + 2A ln(ln(SABH/ϵ)/ϵ)) then Tm ≤\n√\nSϵ and we have\nthe following bound.\n\f\f\f ˆL(d, h; Mt) −L(d, h; Mt)\n\f\f\f ≤6H\n√\nS(B +\n√\nA)ϵ\n1 −γ\nLemma 11. Suppose we are guaranteed that d(s,a)\ned(s,a) ≤1 + ϵ and\n\r\r\rh −eh\n\r\r\r\n2 ≤ϵ, and ∥h∥2 ,\n\r\r\reh\n\r\r\r\n2 ≤H.\nLet L0(d, h; Mt) = L(d, h; Mt) + λ\n2 ∥d∥2\n2 −P\ns h(s)ρ(s) and deﬁne ˆL0(d, h; Mt) analogously. Then the\nfollowing inequalities hold.\n\f\f\fL0(d, h; Mt) −L0(ed,eh; Mt)\n\f\f\f ≤6\n√\nSAHϵ\n1 −γ\nand\n\f\f\f ˆL0(d, h; Mt) −ˆL0(ed,eh; Mt)\n\f\f\f ≤4BH\n√\nSϵ\n1 −γ\n39\nProof. First note that\n\r\r\rd −ed\n\r\r\r\n2\n2 = P\ns,a\n\u0010\nd(s, a) −ed(s, a)\n\u00112\n≤P\ns,a d(s, a)2ϵ2 ≤\nϵ2\n(1−γ)2 .\n\f\f\fL0(d, h; Mt) −L0(ed,eh; Mt)\n\f\f\f ≤\nX\ns,a\n\f\f\fd(s, a) −ed(s, a)\n\f\f\f\n+\n\f\f\f\f\f\nX\ns,a\nd(s, a)h(s) −ed(s, a)eh(s)\n\f\f\f\f\f\n|\n{z\n}\n:=T1\n+γ\nX\ns,a\n\f\f\f\f\fd(s, a)\nX\ns′\nPt(s, a, s′)h(s′) −ed(s, a)\nX\ns′\nPt(s, a, s′)eh(s′)\n\f\f\f\f\f\n|\n{z\n}\n:=T2\nWe now bound the terms T1 and T2.\nT1 =\n\f\f\f\f\f\nX\ns,a\nd(s, a)\n\u0010\nh(s) −eh(s)\n\u0011\n+ eh(s)\n\u0010\nd(s, a) −ed(s, a)\n\u0011\f\f\f\f\f\n≤\n\r\r\rh −eh\n\r\r\r\n1\nX\ns,a\nd(s, a) +\nsX\ns\n(eh(s))2\nv\nu\nu\ntX\ns\n X\na\n(d(s, a) −ed(s, a)\n!2\n≤\n√\nSϵ\n1 −γ + H\n√\nAϵ\n1 −γ\nT2 =\nX\ns,a\n\f\f\f\f\fd(s, a)\nX\ns′\nPt(s, a, s′)h(s′) −ed(s, a)\nX\ns′\nPt(s, a, s′)eh(s′)\n\f\f\f\f\f\n=\nX\ns,a\n\f\f\fd(s, a) −ed(s, a)\n\f\f\f\nX\ns′\nPt(s, a, s′)\n\f\fh(s′)\n\f\f +\nX\ns,a\ned(s, a)\nX\ns′\nPt(s, a, s′)\n\f\f\fh(s′) −eh(s′)\n\f\f\f\n≤∥h∥2\n\r\r\rd −ed\n\r\r\r\n1 +\n\r\r\rh −eh\n\r\r\r\n1\nX\ns,a\ned(s, a)\n≤\n√\nSAHϵ\n1 −γ\n+\n√\nSϵ\n1 −γ\nSubstituting the bounds on T1 and T2 we get the following bound on\n\f\f\fL(d, h; Mt) −L(ed,eh; Mt)\n\f\f\f.\n\f\f\fL0(d, h; Mt) −L0(ed,eh; Mt)\n\f\f\f ≤\n√\nSAϵ + 2\n√\nSϵ\n1 −γ + 2\n√\nSAHϵ\n1 −γ\nWe now consider bounding the diﬀerence\n\f\f\f ˆL0(d, h; Mt) −ˆL0(ed,eh; Mt)\n\f\f\f.\n\f\f\f ˆL0(d, h; Mt) −ˆL0(ed,eh; Mt)\n\f\f\f ≤\n+\n1\nm(1 −γ)\nm\nX\ni=1\n\f\f\f\f\f\nd(si, ai)\ndt(si, ai)(r(si, ai) −h(si) + γh(s′\ni)) −\ned(si, ai)\ndt(si, ai)(r(si, ai) −eh(si) + γeh(s′\ni))\n\f\f\f\f\f\n|\n{z\n}\n:=T3\n40\nWe now bound the term T3.\nT3 =\n1\nm(1 −γ)\nm\nX\ni=1\n\f\f\f\f\f\nd(si, ai)\ndt(si, ai)(r(si, ai) −h(si) + γh(s′\ni)) −\ned(si, ai)\ndt(si, ai)(r(si, ai) −eh(si) + γeh(s′\ni))\n\f\f\f\f\f\n≤\n1\nm(1 −γ)\nm\nX\ni=1\nB\n\u0010\f\f\fh(si) −eh(si)\n\f\f\f + γ\n\f\f\fh(s′\ni) −eh(s′\ni)\n\f\f\f\n\u0011\n+\n1\nm(1 −γ)\nm\nX\ni=1\n\f\f\f\f\f\nd(si, ai) −ed(si, ai)\ndt(si, ai)\n\f\f\f\f\f\n\f\f\fr(si, ai) −eh(si) + γeh(s′\ni)\n\f\f\f\n≤\nB\n1 −γ\n\r\r\rh −eh\n\r\r\r\n1 +\nϵB\n1 −γ (1 + 2 ∥h∥1)\n≤B\n√\nSϵ\n1 −γ + 3BH\n√\nSϵ\n1 −γ\nLemma 12. Let (d⋆, h⋆) ∈arg maxd arg minh L(d, h; M) and ( ˆd, ˆh) ∈arg maxd arg minh L(d, h; c\nM).\nMoreover, suppose\n\f\f\fL(d, h; M) −L(d, h; c\nM)\n\f\f\f ≤ϵ for all d and h with ∥h∥2 ≤3S/(1 −γ)2. Then we\nhave\nL(d⋆, h⋆) −L( ˆd, h⋆) ≤2ϵ\nProof. We will drop the dependence on the underlying model and write L(d, h; M) as L(d, h) and\nL(d, h; c\nM) as ˆL(d, h).\nL(d⋆, h⋆) −L( ˆd, h⋆) = L(d⋆, h⋆) −L(d⋆, ˆh(d⋆)\n|\n{z\n}\n:=T1\n+ L(d⋆, ˆh(d⋆)) −ˆL(d⋆, ˆh(d⋆))\n|\n{z\n}\n:=T2\n+ ˆL(d⋆, ˆh(d⋆)) −ˆL( ˆd, ˆh)\n|\n{z\n}\n:=T3\n+ ˆL( ˆd, ˆh) −ˆL( ˆd, h⋆)\n|\n{z\n}\n:=T4\n+ ˆL( ˆd, h⋆) −L( ˆd, h⋆)\n|\n{z\n}\n:=T5\nHere we write ˆh(ed) = arg minh ˆL(ed, h) i.e. the dual solution that minimizes the objective ˆL(ed, ·).\nSince h⋆minimizes L(d⋆, ·), by lemma 4 we have ∥h⋆∥2 ≤3S/(1 −γ)2. By a similar argument\n\r\r\rˆh\n\r\r\r\n2 ≤3S/(1 −γ)2. Therefore, both T2 and T5 are at most ϵ.\nGiven d⋆, h⋆minimizes L(d⋆, ·). Therefore, the term T1 is at most zero. By a similar argument\nthe term T4 is at most zero. Now for the term T3, notice that ˆh = ˆh( ˆd) and it also minimizes the\nobjective ˆL(d, ˆh(d)). Therefore, the term T3 is also at most zero.\nB.4\nProof of Proposition 1\nProof. Let D be the following set D =\nn\nd ∈RS×A : d(s, a) ≥0 ∀s, a and P\ns,a d(s, a) =\n1\n1−γ\no\n. We\ndeﬁne a set-valued function φ : D →2D as follows.\nφ(d) = arg max\ned≥0\nX\ns,a\ned(s, a)rd(s, a)\ns.t.\nX\na\ned(s, a) = ρ(s) + γ ·\nX\ns′,a\ned(s′, a)Pd(s′, a, s) ∀s\n(26)\n41\nAny ﬁxed point of φ(·) corresponds to a stable point. First, note that φ(d) is non-empty as one can\nalways choose ed to be the occupancy measure associated with any arbitrary policy π in an MDP\nwith probability transition function Pd. Now suppose d1, d2 ∈S(d). Then for any ρ ∈[0, 1] it is easy\nto show that ρd1 + (1 −ρ)d2 ∈φ(d). This is because all the constraints are linear, so ρd1 + (1 −ρ)d2\nis feasible. Moreover, the objective is linear, so ρd1 + (1 −ρ)d2 also attains the same objective value.\nWe now show that the function φ is upper hemicontinuous. Let L be the Lagrangian of the\noptimization problem (26).\nL(ed, h; Md) =\nX\ns,a\ned(s, a)rd(s, a) +\nX\ns\nh(s)\n\nX\na\ned(s, a) −ρ(s) −γ ·\nX\ns′,a\ned(s′, a)Pd(s′, a, s)\n\n\nNote that the Lagrangian is continuous (in fact linear) in ed, and continuous in d (from the assumption\nof (ϵr, ϵp)-sensitivity). Finally, observe the alternative deﬁnition of the function φ.\nφ(d) = arg max\ned∈D\nmin\nh L(ed, h; Md)\nSince the minimum of continuous functions is also continuous, and the set D is compact, we can\napply Berge’s maximum theorem to conclude that the function φ(·) is upper hemicontinuous. Now\nan application of Kakutani ﬁxed point theorem [Gli52] shows that φ has a ﬁxed point.\nB.5\nAssumptions Regarding Quadratic Regularizer\nThroughout we performed repeated optimization with quadratic regularization. Our proof techniques\ncan be easily generalized if we consider a strongly convex regularizer R(d). Suppose, at time t we\nsolve the following optimization problem.\nmax\ned≥0\nX\ns,a\ned(s, a)rt(s, a) −R(ed)\ns.t.\nX\na\ned(s, a) = ρ(s) + γ ·\nX\ns′,a\ned(s′, a)Pt(s′, a, s) ∀s\n(27)\nSince R is strongly convex, (R′)−1 exists and we can use this result to show that the dual of the (27)\nis strongly convex. In fact, as in the proof of theorem 1 we can write down the lagrangian L(d, h)\nand at an optimal solution we must have ∇dL(d, h) = 0. This gives the following expression.\nd(s, a) = (R′)−1\n \nrt(s, a) −h(s) + γ ·\nX\nes\nh(es)Pt(s, a, es)\n!\n(28)\nWe can use the result above to show that the dual is strongly convex and the optimal dual solutions\nform a contraction. Then we can translate this guarantee back to the primal using (28).\nB.6\nOmitted Proofs from Subsection 3.3\nWe will write dλ\nPO to write the performatively optimal solution when using the regularization\nparameter λ i.e.\n42\ndλ\nPO ∈arg max\nd≥0\nX\ns,a\nd(s, a)rλ\nPO(s, a) −λ\n2 ∥d∥2\n2\ns.t.\nX\na\nd(s, a) = ρ(s) + γ ·\nX\ns′,a\nd(s′, a)P λ\nPO(s′, a, s) ∀s\n(29)\nHere we write rλ\nPO = R(dλ\nPO) and P λ\nPO = P(dλ\nPO) to denote the reward function and probability\ntransition function in response to the optimal occupancy measure dλ\nPO. We will also write dλ\nS to\ndenote the performatively stable solution and rλ\nS (resp. P λ\nS ) to denote the corresponding reward\n(resp. probability transition) function. The next lemma bounds the distance between dλ\nS and dλ\nPO.\nB.6.1\nProof of Theorem 4\nProof. Suppose repeatedly maximizing the regularized objective converges to a stable solution dλ\nS i.e.\nX\ns,a\nrdλ\nS(s, a)dλ\nS(s, a) −λ\n2\n\r\r\rdλ\nS\n\r\r\r\n2\n2 ≥max\nd∈C(dλ\nS)\nX\ns,a\nrdλ\nS(s, a)d(s, a) −λ\n2 ∥d∥2\n2\nTherefore,\nX\ns,a\nrdλ\nS(s, a)dλ\nS(s, a) ≥max\nd∈C(dλ\nS)\nX\ns,a\nrdλ\nS(s, a)d(s, a) −λ\n2 ∥d∥2\n2\n≥max\nd∈C(dλ\nS)\nX\ns,a\nrdλ\nS(s, a)d(s, a) −\nλ\n2(1 −γ)2\nThe last inequality uses ∥d∥2\n2 = P\ns,a d(s, a)2 = (1−γ)−2 P\ns,a ((1 −γ)d(s, a))2 ≤(1−γ)−2 P\ns,a(1−\nγ)d(s, a) = (1 −γ)−2. Now we substitute λ = 12S3/2(2ϵr+5Sϵp)\n(1−γ)4\nfrom theorem 1 and get the following\nbound.\nX\ns,a\nrdλ\nS(s, a)dλ\nS(s, a) ≥max\nd∈C(dλ\nS)\nX\ns,a\nrdλ\nS(s, a)d(s, a) −6S3/2(2ϵr + 5Sϵp)\n(1 −γ)6\nB.6.2\nFormal Statement and Proof of Theorem 5\nTheorem 7. Let dPO be the performatively optimal solution with respect to the original (unregu-\nlarized) objective. Then there exists a choice of regularization parameter (λ) such that repeatedly\noptimizing objective (12) converges to a policy (dλ\nS) with the following guarantee\nX\ns,a\nrdλ\nS(s, a)dλ\nS(s, a) ≥\nX\ns,a\nrdP O(s, a)dPO(s, a) −∆\nwhere\n∆= O\n \nmax\n(\nSA1/3\n(1 −γ)10/3\n\u0012\n(1 + γ\n√\nS)ϵr +\nγSϵp\n(1 −γ)2\n\u00132/3\n,\nϵr\n(1 −γ)2 +\nϵpS\n(1 −γ)4\n)!\n43\nProof. Let us write hλ\nPO to denote the dual optimal solution i.e.\nhλ\nPO ∈arg min\nh\nLd(h; Mλ\nPO)\nMoreover, let hλ\nS be the dual optimal solution corresponding to the stable solution dλ\nS.\nX\ns,a\nrdP O(s, a)dPO(s, a) −\nX\ns,a\nrdλ\nS(s, a)dλ\nS(s, a)\n=\n X\ns,a\nrdP O(s, a)dPO(s, a) −λ\n2 ∥dPO∥2\n2\n!\n+ λ\n2 ∥dPO∥2\n2\n−\n X\ns,a\nrdλ\nS(s, a)dλ\nS(s, a) −λ\n2\n\r\r\rdλ\nS\n\r\r\r\n2\n2\n!\n−λ\n2\n\r\r\rdλ\nS\n\r\r\r\n2\n2\n≤\n X\ns,a\nrdλ\nP O(s, a)dλ\nPO(s, a) −λ\n2\n\r\r\rdλ\nPO\n\r\r\r\n2\n2\n!\n+ λ\n2 ∥dPO∥2\n2\n−\n X\ns,a\nrdλ\nS(s, a)dλ\nS(s, a) −λ\n2\n\r\r\rdλ\nS\n\r\r\r\n2\n2\n!\n−λ\n2\n\r\r\rdλ\nS\n\r\r\r\n2\n2\n≤Ld(hλ\nPO; Mλ\nPO) −Ld(hλ\nS; Mλ\nS) + λ\n2 ∥dPO∥2\n2\nThe ﬁrst inequality uses the fact that dλ\nPO is the performatively optimal solution with regularization\nparameter λ. The second inequality uses strong duality and expresses the objective in terms of\noptimal dual variables. We now bound the diﬀerence Ld(hλ\nPO; Mλ\nPO) −Ld(hλ\nS; Mλ\nS).\nLd(hλ\nPO; Mλ\nPO) −Ld(hλ\nS; Mλ\nS)\n= Ld(hλ\nPO; Mλ\nPO) −Ld(hλ\nS; Mλ\nPO) + Ld(hλ\nS; Mλ\nPO) −Ld(hλ\nS; Mλ\nS)\n≤Ld(hλ\nS; Mλ\nPO) −Ld(hλ\nS; Mλ\nS)\n[Since hλ\nPO minimizes Ld(·; Mλ\nPO)]\n≤\n\r\rhλ\nS\n\r\r\n2\n√\nA\nλ\n\u0010\n(1 + γ\n√\nS)ϵr + γ(2\n√\nS +\n\r\r\rhλ\nS\n\r\r\r\n2)ϵp\n\u0011 \r\r\rdλ\nS −dλ\nPO\n\r\r\r\n2 [By inequality 32 ]\n≤\nS3A\nλ2(1 −γ)6\n\u0012\n(1 + γ\n√\nS)ϵr + γ\n\u0012\n2\n√\nS +\n3S\n(1 −γ)2\n\u0013\nϵp\n\u00132\n[By lemma 13]\nThe term ∥dPO∥2\n2 can be bounded as P\ns,a d2\nPO(s, a) = (1 −γ)−2 P\ns,a(dPO(s, a)(1 −γ))2 ≤(1 −\nγ)−2 P\ns,a dPO(s, a)(1 −γ) = (1 −γ)−2. This gives us the following bound.\nX\ns,a\nrdP O(s, a)dPO(s, a) −\nX\ns,a\nrdλ\nS(s, a)dλ\nS(s, a)\n≤\nS3A\nλ2(1 −γ)6\n\u0012\n(1 + γ\n√\nS)ϵr + γ\n\u0012\n2\n√\nS +\n3S\n(1 −γ)2\n\u0013\nϵp\n\u00132\n+\nλ\n2(1 −γ)2\n= 1\nλ2\nS3A\n(1 −γ)6\n\u0012\n(1 + γ\n√\nS)ϵr + γ\n\u0012\n2\n√\nS +\n3S\n(1 −γ)2\n\u0013\nϵp\n\u00132\n|\n{z\n}\n:=T1\n+λ\n1\n2(1 −γ)2\n|\n{z\n}\n:=T2\n44\nNote that in order to apply lemma 13 we need λ ≥λ0 = 2\n\u00002ϵr + 9ϵpS(1 −γ)−2\u0001\n. So we consider two\ncases. First if (2T1/T2)−1/3 > λ0. In that case, we can use λ = (2T1/T2)−1/3 and get the following\nupper bound.\nX\ns,a\nrdP O(s, a)dPO(s, a) −\nX\ns,a\nrdλ\nS(s, a)dλ\nS(s, a) ≤O\n\u0010\nT 1/3\n1\nT 2/3\n2\n\u0011\n= O\n \nSA1/3\n(1 −γ)10/3\n\u0012\n(1 + γ\n√\nS)ϵr + γ\n\u0012\n2\n√\nS +\n3S\n(1 −γ)2\n\u0013\nϵp\n\u00132/3!\nOn the other hand, if (2T1/T2)−1/3 ≤λ0 then we can substitute λ = λ0 and get the following bound.\nX\ns,a\nrdP O(s, a)dPO(s, a) −\nX\ns,a\nrdλ\nS(s, a)dλ\nS(s, a) ≤1\nλ2\n0\nT1 + λ0T2\n=\n1\n(T 1/3\n1\nT −1/3\n2\n)2 + λ0T2 = T 1/3\n1\nT 2/3\n2\n+ λ0T2 ≤2λ0T2\n= O\n\u0012\nϵr\n(1 −γ)2 +\nϵpS\n(1 −γ)4\n\u0013\nLemma 13. Suppose λ ≥2\n\u0010\n2ϵr +\n9ϵpS\n(1−γ)2\n\u0011\n. Then we have\n\r\r\rdλ\nS −dλ\nPO\n\r\r\r\n2 ≤O\n \nS2√\nA\nλ(1 −γ)4\n\u0012\nϵr\n\u0010\n1 + γ\n√\nS\n\u0011\n+ ϵp\nγS\n(1 −γ)2\n\u0013!\nProof. Let us write hλ\nPO to denote the dual optimal solution i.e.\nhλ\nPO ∈arg min\nh\nLd(h; Mλ\nPO)\nMoreover, let hλ\nS be the dual optimal solution corresponding to the stable solution dλ\nS.\nSince the dual Ld(·; Mλ\nPO) objective is strongly convex (lemma 2) and hλ\nPO is the corresponding\noptimal solution we have,\nLd(hλ\nS; Mλ\nPO) −Ld(hλ\nPO; Mλ\nPO) ≥A(1 −γ)2\n2λ\n\r\r\rhλ\nS −hλ\nPO\n\r\r\r\n2\n2\n(30)\nFrom lemma (1) we get the following bound.\n \n1 −2ϵr + 3ϵp\n\r\rhλ\nS\n\r\r\n2\nλ\n! \r\r\rdλ\nS −dλ\nPO\n\r\r\r\n2 ≤3\n√\nAS\nλ\n\r\r\rhλ\nS −hλ\nPO\n\r\r\r\n2\nSubstituting the above bound in eq. (30) and using lemma 4 we get the following inequality for any\nλ > 2ϵr + 9ϵrS/(1 −γ)2.\nLd(hλ\nS; Mλ\nPO) −Ld(hλ\nPO; Mλ\nPO) ≥(1 −γ)2\n18S\n \n1 −2ϵr + 3ϵp\n\r\rhλ\nS\n\r\r\n2\nλ\n!2 \r\r\rdλ\nS −dλ\nPO\n\r\r\r\n2\n2\n(31)\n45\nWe now upper bound Ld(hλ\nS; Mλ\nPO) −Ld(hλ\nS; Mλ\nS). Using lemma 14 we get the following bound.\nLd(hλ\nS; Mλ\nPO) −Ld(hλ\nS; Mλ\nS) ≤\n\r\rhλ\nS\n\r\r\n2\n√\nA\nλ\n\u0010\n(1 + γ\n√\nS)\n\r\r\rrλ\nS −rλ\nPO\n\r\r\r\n2 + γ(2\n√\nS +\n\r\r\rhλ\nS\n\r\r\r\n2)\n\r\r\rP λ\nS −P λ\nPO\n\r\r\r\n2\n\u0011\n≤\n\r\rhλ\nS\n\r\r\n2\n√\nA\nλ\n\u0010\n(1 + γ\n√\nS)ϵr + γ(2\n√\nS +\n\r\r\rhλ\nS\n\r\r\r\n2)ϵp\n\u0011 \r\r\rdλ\nS −dλ\nPO\n\r\r\r\n2 (32)\nNote that the following sequence of inequalities hold.\nLd(hλ\nS; Mλ\nPO) ≥Ld(hλ\nPO; Mλ\nPO) ≥Ld(hλ\nS; Mλ\nS)\nThe ﬁrst inequality is true because hλ\nPO minimizes Ld(·; Mλ\nPO).\nThe second inequality holds\nbecause the primal objective at performative optimal solution (dλ\nPO) upper bound the primal\nobjective at performatively stable solution (dλ\nS) and by strong duality the primal objectives are\nequal to the corresponding dual objectives. Therefore we must have Ld(hλ\nS; Mλ\nPO) −Ld(hλ\nS; Mλ\nS) ≥\nLd(hλ\nS; Mλ\nPO) −Ld(hλ\nPO; Mλ\nPO), and by using equations (32) and (31) we get the following bound on\n\r\rdλ\nS −dλ\nPO\n\r\r\n2.\n\r\r\rdλ\nS −dλ\nPO\n\r\r\r\n2 ≤\n\r\rhλ\nS\n\r\r\n2\n√\nA\nλ\n\u0010\n(1 + γ\n√\nS)ϵr + γ(2\n√\nS +\n\r\r\rhλ\nS\n\r\r\r\n2)ϵp\n\u0011\n18S\n(1 −γ)2\n \n1 −2ϵr + 3ϵp\n\r\rhλ\nS\n\r\r\n2\nλ\n!−2\n≤108S2√\nAλ\n(1 −γ)4\n \nϵr(1 + γ\n√\nS) + γ\n√\nSϵp\n \n2 +\n3\n√\nS\n(1 −γ)2\n!\nϵp\n!\nThe last line uses lemma 4 and λ ≥2(2ϵr + 9ϵpS/(1 −γ)2)\nLemma 14.\n\f\f\fLd(h; M) −Ld(h; c\nM)\n\f\f\f ≤∥h∥2\n√\nA\nλ\n\u0010\n(1 + γ\n√\nS) ∥r −ˆr∥2 + γ(2\n√\nS + ∥h∥2)\n\r\r\rP −bP\n\r\r\r\n2\n\u0011\nProof. From the deﬁnition of the dual objective 12 we have,\n\f\f\fLd(h; M) −Ld(h; c\nM)\n\f\f\f ≤1\nλ\n\f\f\f\f\f\nX\ns,a\nh(s)(r(s, a) −ˆr(s, a))\n\f\f\f\f\f\n|\n{z\n}\n:=T1\n+ γ\nλ\n\f\f\f\f\f\f\nX\ns,s′,a\nh(s)\n\u0010\nr(s′, a)P(s′, a, s) −ˆr(s′, a) ˆP(s′, a, s)\n\u0011\n\f\f\f\f\f\f\n|\n{z\n}\n:=T2\n+ γ\nλ\n\f\f\f\f\f\nX\ns,a\nh(s)\nX\nes\nh(es)\n\u0010\nP(s, a, es) −ˆP(s, a, es)\n\u0011\f\f\f\f\f\n|\n{z\n}\n:=T3\n+ γ2\n2λ\n\f\f\f\f\f\f\nX\ns,a\nX\nes,ˆs\nh(es)h(ˆs)\n\u0010\nP(s, a, ˆs)P(s, a, es) −bP(s, a, ˆs) bP(s, a, es)\n\u0011\n\f\f\f\f\f\f\n|\n{z\n}\n:=T4\n46\nT1 ≤\nsX\ns\nh(s)2\nv\nu\nu\ntX\ns\n X\na\n(r(s, a) −ˆr(s, a))\n!2\n≤∥h∥2\n√\nA ∥r −ˆr∥2\nT2 ≤\n\f\f\f\f\f\f\nX\ns,s′,a\nh(s)P(s′, a, s)\n\u0000r(s′, a) −ˆr(s′, a)\n\u0001\n\f\f\f\f\f\f\n+\n\f\f\f\f\f\f\nX\ns,s′,a\nh(s)ˆr(s′, a)\n\u0010\nP(s′, a, s) −ˆP(s′, a, s)\n\u0011\n\f\f\f\f\f\f\n≤\nsX\ns\nh(s)2\nv\nu\nu\nu\nt\nX\ns\n\nX\ns′,a\nP(s′, a, s) (r(s′, a) −ˆr(s′, a))\n\n\n2\n+\nsX\ns\nh(s)2\nv\nu\nu\nu\nt\nX\ns\n\nX\ns′,a\n\u0010\nP(s′, a, s) −ˆP(s′, a, s)\n\u0011\n\n\n2\n≤∥h∥2\n√\nSA\nsX\ns\nX\ns′,a\nP(s′, a, s) (r(s′, a) −ˆr(s′, a))2 + ∥h∥2\n√\nSA\nsX\ns\nX\ns′,a\n\u0010\nP(s′, a, s) −ˆP(s′, a, s)\n\u00112\n≤∥h∥2\n√\nSA ∥r −ˆr∥2 + ∥h∥2\n√\nSA\n\r\r\rP −ˆP\n\r\r\r\n2\nT3 ≤∥h∥2\nv\nu\nu\nu\nt\nX\ns\n\nX\nes,a\nh(es)\n\u0010\nP(s, a, es) −bP(s, a, es)\n\u0011\n\n\n2\n≤∥h∥2\nv\nu\nu\ntX\ns\n∥h∥2\n2\nX\nes\n X\na\n\u0010\nP(s, a, es) −bP(s, a, es)\n\u0011!2\n≤∥h∥2\n2\n√\nA\n\r\r\rP −bP\n\r\r\r\n2\nT4 ≤\n\f\f\f\f\f\f\nX\ns,a\nX\nes,ˆs\nh(es)h(ˆs)P(s, a, ˆs)\n\u0010\nP(s, a, es) −bP(s, a, es)\n\u0011\n\f\f\f\f\f\f\n+\n\f\f\f\f\f\f\nX\ns,a\nX\nes,ˆs\nh(es)h(ˆs) bP(s, a, ˆs)\n\u0010\nP(s, a, ˆs) −bP(s, a, ˆs)\n\u0011\n\f\f\f\f\f\f\n≤2 ∥h∥2\n\f\f\f\f\f\nX\ns,a\nX\nes\nh(es)\n\f\f\fP(s, a, es) −bP(s, a, es)\n\f\f\f\n\f\f\f\f\f\n[By\nX\nˆs\nh(ˆs)P(s, a, ˆs) ≤∥h∥2\nsX\nˆs\nP(s, a, ˆs) = ∥h∥2]\n≤2 ∥h∥2\n2\nv\nu\nu\ntX\nes\n X\ns,a\n\f\f\fP(s, a, es) −bP(s, a, es)\n\f\f\f\n!2\n≤2 ∥h∥2\n2\n√\nSA\n\r\r\rP −bP\n\r\r\r\n2\nSubstituting the upper bounds on T1, T2, T3, and T4 into the upper bound on Ld(h; M) −Ld(h; c\nM)\ngives the desired bound.\n47\n",
  "categories": [
    "cs.LG",
    "cs.GT"
  ],
  "published": "2022-06-30",
  "updated": "2023-02-07"
}