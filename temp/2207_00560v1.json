{
  "id": "http://arxiv.org/abs/2207.00560v1",
  "title": "Is neural language acquisition similar to natural? A chronological probing study",
  "authors": [
    "Ekaterina Voloshina",
    "Oleg Serikov",
    "Tatiana Shavrina"
  ],
  "abstract": "The probing methodology allows one to obtain a partial representation of\nlinguistic phenomena stored in the inner layers of the neural network, using\nexternal classifiers and statistical analysis. Pre-trained transformer-based\nlanguage models are widely used both for natural language understanding (NLU)\nand natural language generation (NLG) tasks making them most commonly used for\ndownstream applications. However, little analysis was carried out, whether the\nmodels were pre-trained enough or contained knowledge correlated with\nlinguistic theory. We are presenting the chronological probing study of\ntransformer English models such as MultiBERT and T5. We sequentially compare\nthe information about the language learned by the models in the process of\ntraining on corpora. The results show that 1) linguistic information is\nacquired in the early stages of training 2) both language models demonstrate\ncapabilities to capture various features from various levels of language,\nincluding morphology, syntax, and even discourse, while they also can\ninconsistently fail on tasks that are perceived as easy. We also introduce the\nopen-source framework for chronological probing research, compatible with other\ntransformer-based models.\nhttps://github.com/EkaterinaVoloshina/chronological_probing",
  "text": "arXiv:2207.00560v1  [cs.CL]  1 Jul 2022\nIs neural language acquisition similar to natural?\nA chronological probing study\nEkaterina Voloshina\nAIRI, HSE University\nMoscow, Russia\nvokat@mail.ru\nOleg Serikov\nAIRI, DeepPavlov, HSE University\nMoscow, Russia\nsrkvoa@gmail.com\nTatiana Shavrina\nAIRI, SberDevices\nMoscow, Russia\nrybolos@gmail.com\nAbstract\nThe probing methodology allows one to obtain a partial representation of linguistic phenomena stored in the\ninner layers of the neural network, using external classiﬁers and statistical analysis.\nPre-trained transformer-based language models are widely used both for natural language understanding (NLU)\nand natural language generation (NLG) tasks making them most commonly used for downstream applications.\nHowever, little analysis was carried out, whether the models were pre-trained enough or contained knowledge\ncorrelated with linguistic theory.\nWe are presenting the chronological probing study of transformer English models such as MultiBERT and T5.\nWe sequentially compare the information about the language learned by the models in the process of training on\ncorpora. The results show that 1) linguistic information is acquired in the early stages of training 2) both language\nmodels demonstrate capabilities to capture various features from various levels of language, including morphology,\nsyntax, and even discourse, while they also can inconsistently fail on tasks that are perceived as easy.\nWe also introduce the open-source framework for chronological probing research, compatible with other\ntransformer-based models. https://github.com/EkaterinaVoloshina/chronological_probing\nKeywords: probing, language acquisition, language modeling, transformers\nDOI: 10.28995/2075-7182-2022-20-XX-XX\nУсвоение языка у языковых моделей и человека: хронологическое\nпробинг-исследование.\nЕкатерина Волошина\nОлег Сериков\nТатьяна Шаврина\nAIRI, НИУ ВШЭ\nAIRI, DeepPavlov, НИУ ВШЭ\nAIRI, SberDevices\nМосква, Россия\nМосква, Россия\nМосква, Россия\nvokat@mail.ru\nsrkvoa@gmail.com\nrybolos@gmail.com\nАннотация\nПробинг-методология позволяет получить представление о явлениях языка, хранящееся во\nвнутренних слоях нейросети, с помощью внешних классификаторов и статистического анализа.\nПредобученные языковые модели на основе трансформерных архитектур широко использу-\nются как для задач понимания естественного языка (NLU), так и для задач генерации (NLG), что\nделает их наиболее часто используемыми для широкого ряда применений. Однако, недостаточно\nпроводился анализ, достаточно ли предобучены модели и содержат ли знания, коррелирующие с\nтеоретическими представлениями о языке.\nМы представляем исследование на основе хронологического пробинга на примере моделей\nMultiBERT и T5, в котором последовательно исследуем выучиваемую информацию о языке в\nпроцессе предобучения моделей на корпусе. Результаты показывают, что 1) лингвистическая ин-\nформация усваивается уже на ранних этапах обучения 2) обе языковые модели демонстрируют\nспособность фиксировать различные свойства языка на разных уровнях, включая морфологию,\nсинтаксис и дискурс, в то же время они могут не справляться с задачами, которые воспринима-\nется как простые.\nМы\nтакже\nпредоставляем\nоткрытый\nфреймворк\nдля\nхронологического\nпро-\nбинга,\nсовместимый\nс\nязыковыми\nмоделями\nна\nоснове\nархитектур\ntransformer.\nhttps://github.com/EkaterinaVoloshina/chronological_probing\nКлючевые слова: пробинг, усвоение языка, языковые модели, трансформеры\n1\nIntroduction\nThe role of deep learning language models has been increasing in the ﬁeld of methodology for linguistic\nresearch, providing new methods for both diachronic and synchronic studies (Manning, 2015). In par-\nticular, transformer-based language modeling research has produced a variety of tools that may discover\nregularities and structures in data, many of which have resulted in practical applications.\nIn this study, we search for a match between the language competencies of popular language mod-\nels and compare their results with the levels of a ﬁrst language learner. As the transformer models are\nexpected to acquire a language during the training process, the probing methodology has shed light on\nmodel training success. Probing tasks are usually classiﬁcation tasks where classes represent different\nvalues of a linguistic features, such as a subject number, tree depth, and a connector type. Theoret-\nical representation of language often inquires about the levels of phonetics, morphology, syntax, and\ndiscourse/pragmatics to be involved in a probing study 1.\nThe main focus of this work is to explore how language models acquire measurable linguistic struc-\ntures during training. The contributions of our work are the following:\n• We propose a methodology for chronological probing, based on checkpoint-wise result comparison\nduring model training2. We denote chronological probing as any probing technique that refers to\nthe training history/iterations of the same model.\n• We test two models (MultiBERT(Sellam et al., 2021) and T5(Raffel et al., 2019)) on existing 12\ndifferent probing tasks in morphology, syntax, and discourse and present an analysis of the mod-\nels’ gradual learning of language phenomena, in comparison with the well-known facts about the\nacquisition of the ﬁrst language by a child.\n• We present the evaluation results for the named models and state that the models tend to learn the\nlinguistic phenomena in a speciﬁc order, and some parts of grammar are “acquired” ﬁrst.\nThe presented framework and methodology are available open-source under Apache 2.0 license.\n2\nRelated work\nUntil recent years, the task of learning syntax, which every ﬁve-year-old child performs effort-\nlessly, has eluded brute language modeling force.\nThis makes the language models a particular\nobject of study, considered both from the interpretability and modeling language acquisition.\nAs\n(De Villiers and Roeper, 2011, p. 119) states, “computational models of language acquisition must begin\nand end as an integral part of the empirical study of child language.”\nFollowing this thesis, we turn our attention to the probing methodology and comparable case studies\nin the ﬁeld of language acquisition, focusing on the transformer architectures.\n2.1\nProbing and approaches to the black box of language modeling\nAn increasing number of works are devoted to interpreting language models from a linguistic point of\nview. The quickly advancing ﬁeld of probing received lots of researchers’ attention when the hegemony\nof the large black-box models was set up. Researchers question the extent of the models’ “understand-\ning” of the language in probing. They inspect if, and to what limits, the language models’ behavior\nagrees with the insights of the theory of language. Following the hierarchy of language levels (morpho-\nlogy, syntax, discourse) (Dalrymple, 2001), the probing studies often suppose the experiments related to\nmodels’ proﬁciency on a certain level of language.\nThis line of research typically comes down to analyzing how linguistic structures are represented in\na model’s knowledge. Such structures represent syntagmatic/paradigmatic mechanisms (how language\nunits combine and alternate, respectively) of language. It is believed (McCoy et al., 2020), that rediscov-\nering these structures would help models to get closer to human performance on a variety of tasks.\n1However, some researchers (Embick and Noyer, 2007; Caha, 2009) doubt that a language functions as a level system. They\nsuggest that morphology and syntax operate at the same time. Other researchers argue that morphology and syntax are different\nlayers of a language.\n2https://github.com/EkaterinaVoloshina/chronological_probing\nProbing, in general, considers how interpretable the behavior of the language model wrt the linguistic\nproperties of the data. A huge body of probing studies rely on linear models (e.g., external classiﬁers\n(Belinkov, 2016)) that try to establish the relationship between internal representations from the lan-\nguage model and the desired linguistic phenomena. Thus, the linear correlation is measured between\nthe model’s forward pass embeddings and the linguistic properties of the passed data. A sample study\n(Tenney et al., 2019) could measure the strength of correlation between a model’s particular layer activ-\nations on some word and word’s part-of-speech.\nStrong correlations have been recorded when comparing the models’ forward pass activa-\ntions with the passed data underlying linguistic structure (Belinkov et al., 2017; Tenney et al., 2019;\nConneau et al., 2018a; Hewitt and Liang, 2019a) using probing methods.\nSuch a high performance could be misleading. The properties of the model and the properties of\nthe used data impact the resulting score of the correlation probing study. Thus, given only a correl-\nation score, one does not know if it reﬂects the model’s (but not the corpus itself) linguistic inform-\nativeness.\nAs a result, several approaches to conducting more reliable studies have been proposed.\n(Hewitt and Liang, 2019a; Zhang and Bowman, 2018; Voita and Titov, 2020; Pimentel et al., 2020).\nThe probing methodology combining various annotated data is commonly used as the benchmark\nfor language model comparison and evaluation of their generalizing ability.\nThe SentEval toolkit\n(Conneau and Kiela, 2018) has led to the popularization of the 10 tasks used to distinguish between ran-\ndom and justiﬁed, brittle, and robust results of model training, including different types of architectures.\nHowever, analogous research on the same architecture or even the same model is in its early development\nstage. The ﬁrst work on probing of neural networks across time was carried by (Saphra and Lopez, 2018).\nThe authors showed that ﬁrst, LSTM acquires syntactic and semantic features and later information struc-\nture. (Chiang et al., 2020a) looked at the training process of ALBERT and concluded that semantic and\nsyntactic information is acquired during the early steps while world knowledge ﬂuctuates during the\ntraining. (Liu et al., 2021) showed similar results on RoBERTa: the model shows good results on lin-\nguistic probing tasks starting from early stages, and later it learns factual and commonsense knowledge.\nChronological probing could enrich the interpretable documentation of model training in time and thus\nexplore the new aspects of model training and more clearly expose its problems.\n2.2\nLanguage acquisition and language models\nLanguage learning is one of the quintessential human traits. First language acquisition(LA), unites both\nneurocognitive research, psycholinguistics, and computational approaches, focusing on the ability to\nacquire the capacity to perceive and comprehend language.\nStatistical language acquisition\nLanguage modeling has formed a branch in language acquisition\nstudies named statistical language acquisition. Various aspects of language, including phonological,\nsyntactic, lexical, morphological, and semantic features, were investigated in terms of statistical patterns\nchildren receive with the linguistic input. Recent studies postulating qualitative and quantitative measures\nof LA include:\n• Morphology and Syntax Morphology and syntax studies across language acquisition studies are\ndeﬁnitely those explored the most. Starting with the poverty of stimulus problem and the argument\nbetween innateness and learning of grammar, it has led to typologically various sets of descriptive\nworks and even computational models of the acquisition process. Thus, (Lewis and Elman, 2001)\ntrain a simple RNN to discriminate between grammatical strings that follow the inversion rule and\nthose that do not (e.g., moving the ﬁrst auxiliary verb such as “Is the man that tall is nice?”). The\ntraining data for the study is generated artiﬁcially and fails to prove that such a network generalizes\non a mixture of diverse syntactic constructions. (Reali and Christiansen, 2005) use bigram models\nto capture the patterns of auxiliary inversion based on lifelike data from child-directed speech.\nThe model can consistently assign higher probabilities to grammatical strings than ungrammatical\nstrings, which was interpreted as having successfully learned the correct inversion rule. However,\nas (Kam et al., 2008) note, this result is because bigrams such as “who are” are much more frequent\nthan the ungrammatical strings. (Prefors et al., 2006) approaches the structure dependency problem\nwith Bayesian learning and attempts to learn a grammar that could generate additional sentences.\nThe model evaluates and selects between two grammars, a ﬁnite state grammar and a context-free\ngrammar constructed by the authors based on a simpliﬁed subset of child-directed English.\nIt is worth noting how similar all the problem formulations are to the modern formulations of prob-\ning classiﬁcation problems described below. They are also far from a complete description of the\nprocess of mastering grammar.\n• Discourse The creation of texts, not sentences, with various discourse features, such as competence\nin speech acts, conversations, speech registers, and extended speaking turns, is more often con-\nsidered a later stage of speech development. There are no computer models for the assimilation of\ndiscursive properties comparable to models for morphology and syntax. However, research in this\ndirection is underway.\nIn (Ororbia et al., 2019), authors examine whether neural language models acquire language better\nwhen trained in a multi-modal setting (namely, accompanied with visual contexts) compared to tradi-\ntional purely textual pre-training. They show that indeed providing models with perceptual context is\nbeneﬁcial for training language models. Authors claim this evidence to correspond with the theory of\nsituated cognition introduced in (Greeno and Moore, 1993).\nIn this work, we propose the ﬁrst methodological step for chronological interpretation of traditional\ntransformer language models in the framework of LA.\n3\nExperimental setup\n3.1\nModels\nWe calculated the accuracy of two transformer models on 12 probing tasks. As we want to know how\nuniversal patterns of language acquisition in models are, we experiment with two different transformer\narchitectures: BERT and T5. While BERT has only encoder layers, T5 includes both encoder and\ndecoder layers. Therefore, embeddings from BERT come from the encoder, and T5 embeddings are\ncalculated after going through decoder after encoder.\nFor this work, we use already published models with available checkpoints. It means that they were\ntrained on different data and computational powers. Moreover, they were trained with different batch\nsizes (256 for MultiBERT and 32 for T5). However, we follow (Chiang et al., 2020b; Liu et al., 2021)\nand measure the training progress in iterations. The further comparison of the two models is indicative\nonly.\nMultiBERT (Sellam et al., 2021) is based on BERT-base-uncased architecture, and it is the model of\nthe same size (12 layers and embedding size 768). Unlike the original BERT(Devlin et al., 2019), it was\ntrained with 25 different seeds. The authors also released checkpoints of the ﬁrst ﬁve models. We use the\nmodel with seed 0 in our experiments. MultiBERT was trained on both literary and non-ﬁction texts. The\nformer comes from BookCorpus (Zhu et al., 2015), which includes 11,038 books of 16 different genres.\nThe non-ﬁction texts are taken from English Wikipedia collected by (Turc et al., 2019).\nT5-small model is trained within the t5-experiments framework.3 and follows the Hugging Face\nimplementation of T5 (Raffel et al., 2019). It consists of 6 layers with 512 embedding size.\nFollowing the previous language, modeling works (Devlin et al., 2019; Bojanowski et al., 2017), we\nuse the Wikipedia data to train the model. The raw Wikipedia data is provided by The Pile project\n(Gao et al., 2020) contains ≈19Gb of expository prose texts of various domains, and is treated as a\nlanguage modeling dataset of reasonably well quality.\nBaseline As a baseline, we use the method described in (Hewitt and Liang, 2019b). We train logistic\nregression on top of embeddings of models mentioned above with shufﬂed class labels.\n3.2\nProbing tasks\nProbing tasks come from several datasets published earlier: SentEval (Conneau et al., 2018b), Morph\nCall (Mikhailov et al., 2021), DisSent (Nie et al., 2019), DiscoEval (Chen et al., 2019), and BLiMP\n3https://github.com/yurakuratov/t5-experiments\nTask\nSentence examples\nLabels\nSubject number\nHer employer had escaped with his wife for several afternoons\nthis summer.\nNN\nYour Mackenzie in-laws have sordid reputations few decent fam-\nilies wish to be connected with .\nNNS\nPerson\nSo I still can recomend them but prepare pay twice as much as\nthey tell you initially .\nhas a person marker\nThe service was friendly and fast , but this just does nt make up\nfor the lack - luster product .\ndoes not have a person marker\nTree depth\nWe have done everything we can for her .\n11\nAlvin Yeung of Civic Party\n3\nTop constituents\nDid it belong to the owner of the house ?\nVBD_NP_VP_.\nHow long before you leave us again ?\nWHNP_SQ_.\nConnectors\nHe ’d almost forgotten about that man . Sarah had somehow\nbrought him back , just as she had his nightmares .\nbut\nI let out a slow , careful breath . Felt tears sting my eyes .\nand\nSentence position\nQuneitra Governorate ( / ALA-LC : “ Muhafzat Al-Qunaytrah “\n) is one of the fourteen governorates ( provinces ) of Syria . The\ngovernorate had a population of 87,000 at the 2010 estimate .\nIts area varies , according to different sources , from 685 km ²\nto 1,861 km ² . It is situated in southern Syria , notable for the\nlocation of the Golan Heights . The governorate borders Lebanon\n, Jordan and Israel .\n1\nThe bossom and the part of the xhubleta covered by the apron\nare made out of crocheted black wool . The bell shape is accen-\ntuated in the back part . The xhubleta is an undulating , bell-\nshaped folk skirt , worn by Albanian women . It usually is hung\non the shoulders using two straps . Part of the Albanian tradi-\ntional clothing it has 13 to 17 strips and 5 pieces of felt .\n4\nPenn Discourse Treebank\nSolo woodwind players have to be creative,they want to work a\nlot\nPragmatic Cause\nThe U.S. , along with Britain and Singapore , left the agencyl, its\nanti-Western ideology , ﬁnancial corruption and top leadership\ngot out of hand\nList\nDiscourse Coherence\nWithin the fan inlet case , there are anti-icing air bosses and\nprobes to sense the inlet pressure and temperature .’, ’High speed\ncenter of pressure shifts along with ﬁn aeroelasticity were major\nfactors . At the 13th ( i.e .’, ’the ﬁnal ) compressor stage , air is\nbled out and used for anti-icing . The amount is controlled by\nthe Pressure Ratio Bleed Control sense signal ( PRBC ) . The “\ndiffuser case “ at the aft end of the compressor houses the 13th\nstage .\na text is not coherent\nThis experience of digital circuitry and assembly language pro-\ngramming formed the basis of his book “ Code : The Hidden\nLanguage of Computer Hardware and Software ” . Petzold pur-\nchased a two-diskette IBM PC in 1984 for $ 5,000 . This debt\nencouraged him to use the PC to earn some revenue so he wrote\nan article about ANSI.SYS and the PROMPT command . This was\nsubmitted to PC Magazine for which they paid $ 800 . This was\nthe beginning of Petzold ’s career as a paid writer . In 1984 , PC\nMagazine decided to do a review of printers .\na text is coherent\nTable 1: Examples of tasks\nTask\nAcceptable sentence\nUnacceptable sentence\nTransitive\nThe pedestrians question some people.\nThe pedestrians wave some people.\nPassive\nTracy isn’t ﬁred by Jodi’s daughter.\nTracy isn’t muttered by Jodi’s daughter.\nPrinciple A c command\nThis lady who is healing Charles wasn’t hiding herself.\nThis lady who is healing Charles wasn’t hiding himself.\nAdjunct Island\nWho does John leave while alarming Beverly?\nWho does John leave Beverly while alarming?\nTable 2: BLiMP Minimal pairs examples\n(Warstadt et al., 2020). The class balance of ﬁrst eight tasks is illustrated with Figure 4 in Appendix.\nWe choose these tasks to make our results comparable to other works on probing.\nAs we want to show another perspective on language acquisition, we balance classiﬁer probing tasks\nwith BLiMP tasks. As BLimP only covers morphology and syntax, all discourse-based tasks are evalu-\nated with a classiﬁer.\nThe datasets from Benchmark of minimal linguistic pairs (BLiMP) have a structure different from\nother tasks: every task includes pairs with minimal differences to illustrate one of the grammatical fea-\ntures of English. One sentence of the pair is grammatical, whereas another one is unacceptable. We\nchose four BLiMP tasks for our experiments: transitive and passive verbs, Principle A of C command,\nand Island effects. For the ﬁrst two tasks, pairs have different verbs, where only one verb is transitive or\ncan be used in a passive form. These tasks are categorized as morphological (see Table 2).\nThe second two tasks reﬂect syntactic effects on English. The Principle A task shows the use of\nreﬂexives. According to (Chomsky, 1981), a reﬂexive should have a local antecedent.\nThe task on Island effects tests a model’s sensibility to syntactic order. Island is a structure from which\na word can not be moved (Ross, 1967). The sentence is unacceptable if a word is moved out of an island.\nThe tasks from other datasets are summarized below:\n• Subject number (SentEval): this task is supposed to show how models acquire morphology. It is a\nbinary classiﬁcation task with labels NNS and NN (plural and singular number, respectively). The\nclassiﬁer should decide on a sentence class based on the number of sentence subjects.\n• Person (Morph Call): this task is also morphological. It is a binary classiﬁcation with labels 0 and\n1, which signiﬁes if a subject has a person marker or not.\n• Tree depth (SentEval): this task contains six classes, which stands for the depth of the syntactic\ntree of a given sentence. Hence, this task is meant to show the level of syntax acquisition.\n• Top constituents (SentEval): this multiclass task belongs to the group of syntactic tasks. The aim\nis to choose a class that includes constituents located right below the sentence (S) node.\n• Connectors (DisSent): this dataset includes pairs of sentences originally connected with one of 5\nprepositions, and the task is to choose the omitted preposition. It is supposed to show how models\ncatch discourse relations.\n• Sentence position (DiscoEval): this dataset contains sequences of 5 sentences, and the ﬁrst sentence\nis placed in the wrong place. Therefore, the aim is to detect the original position of these sentences.\nThis task is also meant to show models’ accuracy in discourse.\n• Penn Discourse Treebank (DiscoEval): the task is based on Penn Discourse Treebank annotation.\nThe aim is to choose the right discourse relation between two discourse items from Penn Treebank.\n• Discourse coherence (DiscoEval): this task is a binary classiﬁcation with classes 1 and 0. Class\n1 means that the given paragraph is coherent, and class 0 should be assigned to paragraphs with\nshufﬂed sentences.\n3.3\nProbing Methods\nSentence embedding classiﬁcation: Token embeddings from transformer models are turned into sen-\ntence embedding via mean pooling. Then logistic regression classiﬁes embeddings’ sentences. This\nmethod is used with tasks from SentEval and Morph Call.\nPositional sentence classiﬁcation: For the Sentence Position task, we used the following method.\nFirst, we count sentence embeddings as described above. Then the difference between the ﬁrst embed-\nding and the other is calculated pair-wisely. The ﬁrst embedding and its differences with others are\nconcatenated and put as input to logistic regression.\nSentence embedding concatenation & classiﬁcation: We concatenated sentence embeddings for\nother discourse tasks, which were calculated as the mean of token embeddings. The concatenated sen-\ntence embeddings served as inputs for logistic regression.\nMasking tasks: The probing task is based on the idea of masking language modeling. In a sentence,\neach word is masked, and then its probability is summed with other words’ probabilities. The probability\nof an acceptable sentence should be higher than the probability of an unacceptable sentence. This method\nis for use for all tasks from BLiMP.\n4\nResults\n4.1\nResults of MultiBERT\nThe results of the experiments with the MultiBERT-base model are summarized in Figure 1. The model\nshows the best results on Subject Number and Person tasks. The classiﬁcation of PDTB relations, Tree\ndepth, and Principle A acceptability are performed with the worst accuracy.\n\u0002\n\u0004\u0002\u0000\u001c\n\u0006\u0002\u0000\u001c\n\b\u0002\u0000\u001c\n\n\u0002\u0000\u001c\n\u0003\u0002\u0002\u0000\u001c\n\u0003\u0004\u0002\u0000\u001c\n\u0003\u0006\u0002\u0000\u001c\n\u0003\b\u0002\u0000\u001c\n\u0003\n\u0002\u0000\u001c\n\u0004\u0002\u0002\u0000\u001c\n\u0005\u0002\u0002\u0000\u001c\n\u0006\u0002\u0002\u0000\u001c\n\u0007\u0002\u0002\u0000\u001c\n\b\u0002\u0002\u0000\u001c\n\t\u0002\u0002\u0000\u001c\n\n\u0002\u0002\u0000\u001c\n\u000b\u0002\u0002\u0000\u001c\n\u0003\u0002\u0002\u0002\u0000\u001c\n\u0003\u0003\u0002\u0002\u0000\u001c\n\u0010$\u0018\"\u0014$\u001a \u001f#\n\u0002\u0001\u0002\n\u0002\u0001\u0004\n\u0002\u0001\u0006\n\u0002\u0001\b\n\u0002\u0001\n\u0003\u0001\u0002\n\f\u0016\u0016%\"\u0014\u0016'\n\u0000\b\u000e\u0004\u000b\u0010\r\u000e\u0005\n\u000e \u001f\u001f\u0018\u0016$ \"#\n\u000f\u001a#\u0016 %\"#\u0018\u0000\u000e \u0019\u0018\"\u0018\u001f\u0016\u0018\n\u0011\u000f\u0013\r\n\u0012\u0018\u001f$\u0018\u001f\u0016\u0018\u0000\u0011 #\u001a$\u001a \u001f\n\u0002\u0012\n\u000f\u0003\u0011\n\u0013\"\u0018\u0018\u0000\u0017\u0018!$\u0019\n\u0013 !\u0000\u0016 \u001f#$\u001a$%\u0018\u001f$#\n\u000e\u0000\u000e \u001e\u001e\u0014\u001f\u0017\n\f\u0017\u001b%\u001f\u0016$\u0000\u0010#\u001d\u0014\u001f\u0017\n\u0001\u000b\r\f\u0007\u000b\t\u000b\u0006\u0012\n\u0011\u0018\"# \u001f\n\u0012%\u0015\u001b\u0018\u0016$\u0000\u001f%\u001e\u0015\u0018\"\n\u0013\"\u0014\u001f#\u001a$\u001a&\u0018\n\u0011\u0014##\u001a&\u0018\nFigure 1: Comparison of MultiBERT results on different tasks. How to read this ﬁgure: from left to\nright, on the X axis, we see results of intermediate evaluation on the task during model pre-training.\nEach iteration is equal to 25,600,000 sentences for MultiBERT and 3,200,000 sentences for T5. The\nY-axis shows the accuracy metric on the tasks. Tasks are shown in the legend in different colors. As\nwe can see, in the process of model pre-training, there already is a gradual increase in accuracy in tasks\nrelated to morphology (shown in orange) in the early stages. The information in the model embeddings\nstabilizes fairly quickly and remains stable from the 20,000th training step. The same can not be said\nfor tasks related to syntax (shown in purple): their quality remains unstable and ﬂuctuates quite a lot\nduring pre-training. Discourse tasks (green) remain stable at a low-quality level from the start and tend\nto improve the metrics very slowly.\nAs seen from Figure 1, accuracy of models stop changing after 600,000 iterations. However, there is\na signiﬁcant difference between tasks from BLiMP and other datasets. For example, the performance\non the Adjunct Island task remains unstable during the whole period of iterations. Another difference\nbetween these tasks lies in the quality of the models. It is illustrated with tasks grouped as “morpholo-\ngical”: Subject Number and Person tasks, which use logistic regression on MultiBERT embeddings, are\nsolved much better than Transitive and Passive verbs. However, as follows from the plot, it is hard to\ngroup tasks based on the absolute value of accuracy.\nThe changing dynamics provide another perspective. From this point of view, all tasks grouped as\n“discourse” show a similar feature: unlike others, their quality does not ﬂuctuate but rather slightly\ngrows across the training time. On other tasks, models increase the quality during the ﬁrst iterations.\n“Syntactic” tasks tend to change even during later iterations. However, it is not a strict rule, and some\ntasks show similar behavior to “morphological” ones.\n4.2\nResults of T5\n\u0002\n\u0004\u0002\u0000\u001c\n\u0006\u0002\u0000\u001c\n\b\u0002\u0000\u001c\n\n\u0002\u0000\u001c\n\u0003\u0002\u0002\u0000\u001c\n\u0004\u0002\u0002\u0000\u001c\n\u0005\u0002\u0002\u0000\u001c\n\u0006\u0002\u0002\u0000\u001c\n\u0007\u0002\u0002\u0000\u001c\n\b\u0002\u0002\u0000\u001c\n\t\u0002\u0002\u0000\u001c\n\n\u0002\u0002\u0000\u001c\n\u000b\u0002\u0002\u0000\u001c\n\u0003\u0002\u0002\u0002\u0000\u001c\n\u0003\u0003\u0002\u0002\u0000\u001c\n\u0010$\u0018\"\u0014$\u001a \u001f#\n\u0002\u0001\u0002\n\u0002\u0001\u0004\n\u0002\u0001\u0006\n\u0002\u0001\b\n\u0002\u0001\n\u0003\u0001\u0002\n\f\u0016\u0016%\"\u0014\u0016'\n\u0000\b\u000e\u0004\u000b\u0010\r\u000e\u0005\n\u000e \u001f\u001f\u0018\u0016$ \"#\n\u000f\u001a#\u0016 %\"#\u0018\u0000\u000e \u0019\u0018\"\u0018\u001f\u0016\u0018\n\u0011\u000f\u0013\r\n\u0012\u0018\u001f$\u0018\u001f\u0016\u0018\u0000\u0011 #\u001a$\u001a \u001f\n\u0002\u0012\n\u000f\u0003\u0011\n\u0013\"\u0018\u0018\u0000\u0017\u0018!$\u0019\n\u0013 !\u0000\u0016 \u001f#$\u001a$%\u0018\u001f$#\n\u000e\u0000\u000e \u001e\u001e\u0014\u001f\u0017\n\f\u0017\u001b%\u001f\u0016$\u0000\u0010#\u001d\u0014\u001f\u0017\n\u0001\u000b\r\f\u0007\u000b\t\u000b\u0006\u0012\n\u0011\u0018\"# \u001f\n\u0012%\u0015\u001b\u0018\u0016$\u0000\u001f%\u001e\u0015\u0018\"\n\u0013\"\u0014\u001f#\u001a$\u001a&\u0018\n\u0011\u0014##\u001a&\u0018\nFigure 2: Comparison of T5 results on a different task. How to read this ﬁgure: from left to right, on the\nX axis, we see results of intermediate evaluation on the task during model pre-training. Each iteration\nis equal to 25,600,000 sentences for MultiBERT and 3,200,000 sentences for T5. The Y-axis shows the\naccuracy metric on the tasks. Tasks are shown in the legend in different colors. As we can see, in the\ncase of the T5 model, the task quality seems to be more stable from the beginning, with a few exceptions\nlike subject number classiﬁcation. Most of the tasks show the slow yet gradual growth of the metrics, but\nsomehow not the verb transitivity classiﬁcation.\nDue to the architecture, the signiﬁcant difference in T5 results is the zero-close quality on BLiMP\ndatasets. Except for these tasks, the quality of T5 is similar to MultiBERT. The best performance is on\nthe Person task, and the worst quality is shown on PDTB relation classiﬁcation and Tree depth.\nUnlike MultiBERT, we ﬁrst used the available checkpoints of T5 with a step of 100,000 iterations.\nThen we trained a new model on the same resources and texts, but it might have a better initialization,\nwhich affected the ﬁnal results.\nSimilar to MultiBERT, discourse tasks show almost no signiﬁcant change and slow growth, whereas\nthe model increases its results on syntactic and morphological tasks during the ﬁrst 100,000 iterations.\n4.3\nComparison of models\nWe described the surface results of models’ performance and now can deep into more detailed results.\nThe results described above should be considered relative. To illustrate how much information models\nacquire during these iterations, we compare them to ﬁnal models. As the process of training T5 was\nnot ﬁnished, we compared this model with the original T5. As seen from Figure 3, MultiBERT scores\nare close to the results of the ﬁnal checkpoint. Hence, there is no need to look at later iterations. The\ncomparison with the original T5 shows that the model we use performs worse due to the smaller resources\nit was trained on. Therefore, the difference in quality should not be explained by the difference in\narchitecture.\nHowever, we should consider that some tasks are performed with the same quality as embeddings with\nshufﬂed labels (Discourse coherence and Person). Moreover, T5 does not perform much better than the\nPenn Discourse Treebank relations baseline. Consequently, models encounter difﬁculty with discourse\ntasks.\nFurthermore, MultiBERT and T5 show similar learning trajectories on several tasks, such as Connect-\nors and Sentence Position tasks. Another key feature shared by the two models is the termination of\nincreases between 500,000 and 600,000 iterations. Despite the fact that models vary in size and training\nprocess, they show some similarities in probing tasks. Hence, the acquisition generally does not depend\non the model architecture.\n5\nDiscussion\nOur results show that linguistic information is acquired fast, before 600,000 training iterations. It corres-\nponds to results of other researchers (Chiang et al., 2020a; Liu et al., 2021) that independently showed\nsimilar results on a fast acquisition of linguistic features. However, discourse is not fully acquired by the\nend of the observed training period compared to the baseline results. The difference between syntactic\nand morphological tasks is insigniﬁcant. It correlates with ideas in morphosyntax. Although we can not\nprove that morphology and syntax are regarded as the same layer in models, we can make a less strict\nstatement that models acquire all grammatical units simultaneously.\nBLiMP gives another perspective on the process of acquisition. MultiBERT results remain unstable\nfor a longer period than similar tasks with classiﬁers. It might indicate the difference between two\ndifferent approaches to probing. However, from the linguistic point of view, BLiMP includes more\ndifﬁcult linguistic feature cases, while SentEval tasks test more basic knowledge. Hence, it could explain\nworse results.\nT5 architecture does not allow to use of this dataset in the same way as for MultiBERT since Masking\nLanguage Modeling and T5 generation are different tasks. We leave for further research an adaptation of\nthis dataset for T5.\n5.1\nHuman language acquisition results\nMany of the linguistic features used in probing tasks have been well studied in terms of their promptness\nand ease of acquisition by English speakers. First of all, it concerns morphology and syntax. Markers\nsuch as a person, number, and gender are acquired very early by children: before age two (Clark, 2017).\nOf course, in languages besides English, the acquisition of these features varies: if the feature is marked\nconsistently with one afﬁx and no morphonological alternation, children seem to ﬁnd it easier to acquire.\nIt is shown that the earlier mastery of case marking is present in languages like Hungarian and Turkish\nbut not in German or Serbo-Croatian (Slobin, 1985).\nDiscursive features are acquired by children much later. Studies like (Pearson, 2003) show that child’s\ntexts become more complex and decontextualized with age. Also, texts produced by children gradually\n0.00\n0.25\n0.50\n0.75\nTree depth\nConnectors\nAdjunct Island\n0.00\n0.25\n0.50\n0.75\nPerson\nDiscourse Coherence\nPassive\n0.00\n0.25\n0.50\n0.75\nTop constituents\nPenn Discourse Treebank\nTransitive\n0\n60 k\n200 k\n400 k\n600 k\n800 k\n1000 k\n1100 k\n0.00\n0.25\n0.50\n0.75\nSubject number\n0\n60 k\n200 k\n400 k\n600 k\n800 k\n1000 k\n1100 k\nSentence Position\n0\n60 k\n200 k\n400 k\n600 k\n800 k\n1000 k\n1100 k\nPrinciple A c command\nT5\nBERT\nRandom T5\nRandom BERT\nOriginal T5\nFinal BERT\nFigure 3: Performance on models on different tasks. The detailed task-wise comparison shows the differ-\nence in T5 and MultiBERT training results. The models are compared to the ﬁnal available checkpoints\nof the models (Original T5, Final BERT) and with the random baseline.\nprogress in achieving more cohesion through “referential and semantic links that bridge across sentences;\nthey achieve coherence through a global hierarchical structure”. The discourse in these conversations\nbetween toddlers is tentative when neither side can be reliably signiﬁcant. A longitudinal study of dia-\nlogues between two little girls aged 4 to 6, (McTear, 1985) traced the emergence of more and greater\nthematic continuity in their conversation as utterances began to play the dual role of responding to the\npreceding utterance as well as enabling further conversation. However, (Dorval et al., 1984) showed that\nsecond-graders (eight-year-olds) were almost as likely to give unconditioned responses as conditionals,\nwith no signiﬁcant improvement seen until ﬁfth grade.\nRegarding the requirements that the sphere of language acquisition imposes on children, one can very\ncarefully assess the limit in which the language models under consideration lie in terms of their abilities:\ntheir embeddings correspond to the level of language proﬁciency in a child under 11 years old.\n6\nConclusion\nEncoder and encoder-decoder language models have increased importance in tasks requiring understand-\ning the natural language. The probing methodology we presented allows analyzing the changes within\nthe language model during training, from epoch to epoch. The overall results of the work show that:\n• T5 does not give any results on BLiMP due to the generation algorithm. Most tasks show that T5\nacquires basic morphological and syntactic features and some discourse features.\n• MultiBERT shows results close to the trained model starting from 100,000 iterations. MultiBERT\ndoes not improve its quality on some discourse tasks compared to randomly labeled embeddings.\nHowever, it could be said that MultiBERT acquires each level to some extent.\n• Both T5 and MultiBERT demonstrate comparable results regarding the quality of the language level\nacquisition. As we can not distinguish the factors between these results (whether this is the model’s\narchitecture, the training corpora, or both), we present these results ’as is’ for the researchers that\nuse them in the downstream tasks.\n• Recording such results during training could save a lot of computational resources and time for\ninterpreting the results, including downstream tasks. There are understandable context length limit-\nations that prevent, for example, learning the discourse tasks. However, the results of the T5 model\ncompared to the random embeddings on some tasks were unexpected or lower than expected.\n• As the results show, the easiest tasks for the models tend to be morphology and syntax-related.\nThese language level results are correlated and show a similar learning trajectory. Unlike morpho-\nlogy and syntax tasks, results on discourse-based tasks tend to be low, therefore, there is not enough\nevidence to claim that discourse has been learned.\n• Using language acquisition as a trace can beneﬁt in comparing general human language ability and\nmodern language modeling methods. Drawing a border from above on the results on discursive\ntasks, we can say that in the current realities, the models do not pass the bar that 8-year-old children\npass.\nWe welcome both NLP and language acquisition research communities to reproduce the experimental\nsetup and use the presented approach while training other architectures and contribute to formulating the\nbetter and more complex tasks correlated with language learning.\nReferences\nYonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. 2017. Evaluating\nlayers of representation in neural machine translation on part-of-speech and semantic tagging tasks. // Proceed-\nings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), P\n1–10, Taipei, Taiwan, November. Asian Federation of Natural Language Processing.\nYonatan Belinkov. 2016. Probing classiﬁers: Promises, shortcomings, and advances. Computational Linguistics,\nP 1–12.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Association for Computational Linguistics, 5:135–146.\nPavel Caha. 2009. The nanosyntax of case.\nMingda Chen, Zewei Chu, and Kevin Gimpel. 2019. Evaluation benchmarks and learning criteria for discourse-\naware sentence representations. // Proc. of EMNLP.\nCheng-Han Chiang, Sung-Feng Huang, and Hung-yi Lee. 2020a. Pretrained language model embryology: The\nbirth of albert. arXiv preprint arXiv:2010.02480.\nCheng-Han Chiang, Sung-Feng Huang, and Hung-yi Lee. 2020b. Pretrained language model embryology: The\nbirth of ALBERT. // Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), P 6813–6828, Online, November. Association for Computational Linguistics.\nNoam Chomsky. 1981. Lectures on government and binding (dordrecht: Foris). Studies in generative grammar,\n9.\nEve V Clark. 2017. Morphology in language acquisition. The handbook of morphology, P 374–389.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An evaluation toolkit for universal sentence representations.\narXiv preprint arXiv:1803.05449.\nAlexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. 2018a. What you can\ncram into a single $&!#* vector: Probing sentence embeddings for linguistic properties. // Proceedings of the\n56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), P 2126–2136,\nMelbourne, Australia, July. Association for Computational Linguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lample, Loic Barrault, and Marco Baroni.\n2018b.\nWhat\nyou can cram into a single vector: Probing sentence embeddings for linguistic properties.\narXiv preprint\narXiv:1805.01070.\nMary Dalrymple. 2001. Lexical functional grammar. Brill.\nJill G De Villiers and Thomas Roeper.\n2011.\nHandbook of generative approaches to language acquisition,\nvolume 41. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding.\nBruce Dorval, Carol O Eckerman, and Susan Ervin-Tripp. 1984. Developmental trends in the quality of con-\nversation achieved by small groups of acquainted peers. Monographs of the Society for Research in Child\nDevelopment, P 1–91.\nDavid Embick and Rolf Noyer. 2007. Distributed morphology and the syntax/morphology interface. The Oxford\nhandbook of linguistic interfaces, 289324.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,\nAnish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027.\nJames G Greeno and Joyce L Moore. 1993. Situativity and symbols: Response to vera and simon.\nJohn Hewitt and Percy Liang. 2019a. Designing and interpreting probes with control tasks. // Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), P 2733–2743, Hong Kong, China, November.\nAssociation for Computational Linguistics.\nJohn Hewitt and Percy Liang. 2019b. Designing and interpreting probes with control tasks.\narXiv preprint\narXiv:1909.03368.\nXuân-Nga Cao Kam, Iglika Stoyneshka, Lidiya Tornyova, Janet D Fodor, and William G Sakas. 2008. Bigrams\nand the richness of the stimulus. Cognitive science, 32(4):771–787.\nJohn D Lewis and Jeffrey L Elman. 2001. Learnability and the statistical structure of language: Poverty of stimulus\narguments revisited. // Proceedings of the 26th annual Boston University conference on language development,\nvolume 1, P 359–370. Citeseer.\nLeo Z Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah A Smith. 2021. Probing across time:\nWhat does roberta know and when? arXiv preprint arXiv:2104.07885.\nChristopher D Manning. 2015. Last words: Computational linguistics and deep learning.\nR. Thomas McCoy, Junghyun Min, and Tal Linzen. 2020. BERTs of a feather do not generalize together: Large\nvariability in generalization across models with similar test set performance. // Proceedings of the Third Black-\nboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, P 217–227, Online, November.\nAssociation for Computational Linguistics.\nMichael McTear. 1985. Children’s conversation. B. Blackwell.\nVladislav Mikhailov, Oleg Serikov, and Ekaterina Artemova. 2021. Morph call: Probing morphosyntactic content\nof multilingual transformers. // Proceedings of the Third Workshop on Computational Typology and Multilin-\ngual NLP, P 97–121, Online, June. Association for Computational Linguistics.\nAllen Nie, Erin Bennett, and Noah Goodman. 2019. Dissent: Learning sentence representations from explicit\ndiscourse relations. // Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\nP 4497–4510.\nAlexander Ororbia, Ankur Mali, Matthew Kelly, and David Reitter. 2019. Like a baby: Visually situated neural\nlanguage acquisition. // Proceedings of the 57th Annual Meeting of the Association for Computational Linguist-\nics, P 5127–5136, Florence, Italy, July. Association for Computational Linguistics.\nBarbara Zurer Pearson.\n2003.\nLanguage acquisition: Discourse, narrative and pragmatics.\nDisertasi. USA:\nDepartment of Communication Disorders.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell. 2020.\nInformation-theoretic probing for linguistic structure. CoRR, abs/2004.03061.\nAmy Prefors, Terry Regier, and Joshua B Tenenbaum. 2006. Poverty of the stimulus? a rational approach. //\nProceedings of the Annual Meeting of the Cognitive Science Society, volume 28.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv\npreprint arXiv:1910.10683.\nFlorencia Reali and Morten H Christiansen. 2005. Uncovering the richness of the stimulus: Structure dependence\nand indirect statistical evidence. Cognitive Science, 29(6):1007–1028.\nJohn Robert Ross. 1967. Constraints on variables in syntax.\nNaomi Saphra and Adam Lopez. 2018. Understanding learning dynamics of language models with svcca. arXiv\npreprint arXiv:1811.00225.\nThibault Sellam, Steve Yadlowsky, Jason Wei, Naomi Saphra, Alexander D’Amour, Tal Linzen, Jasmijn Bastings,\nIulia Turc, Jacob Eisenstein, Dipanjan Das, et al. 2021. The multiberts: Bert reproductions for robustness\nanalysis. arXiv preprint arXiv:2106.16163.\nDan I Slobin. 1985. Crosslinguistic evidence for the language-making capacity. The crosslinguistic study of\nlanguage acquisition, 2:1157–249.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van\nDurme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? Probing\nfor sentence structure in contextualized word representations. arXiv e-prints, P arXiv:1905.06316, May.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-read students learn better: On the\nimportance of pre-training compact models. arXiv preprint arXiv:1908.08962.\nElena Voita and Ivan Titov. 2020. Information-theoretic probing with minimum description length. arXiv preprint\narXiv:2003.12298.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R Bow-\nman. 2020. Blimp: The benchmark of linguistic minimal pairs for english. Transactions of the Association for\nComputational Linguistics, 8:377–392.\nKelly Zhang and Samuel Bowman. 2018. Language modeling teaches you more than translation does: Lessons\nlearned through auxiliary syntactic task analysis. // Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP, P 359–361, Brussels, Belgium, November. Association\nfor Computational Linguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.\n2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. // Proceedings of the IEEE international conference on computer vision, P 19–27.\nAppendix\n0\n10\n20\n30\nbut\nand\nwhen\nif\nbecause\nSentence connectors\n0\n5\n10\n15\n8\n9\n7\n10\n6\n11\n5\nTree depth\n0\n20\n40\nNNS\nNN\nSubject number\n0\n10\n20\n6\n1\n2\n10\n11\n3\n0\n5\n7\n8\n9\n4\nPDTB\n0\n10\n20\n3\n4\n1\n2\n0\nSentence position\n0\n25\n50\n75\n0\n1\nPerson\n0\n20\n40\n0\n1\nDiscourse coherence\n0\n5\nNP_ADVP_VP_.\nIN_NP_VP_.\nSBAR_NP_VP_.\nNP_PP_.\nSBAR_VP_.\nNP_VP_.\nS_CC_S_.\nOTHER\nVBD_NP_VP_.\nWHADVP_SQ_.\nADVP_NP_VP_.\nWHNP_SQ_.\nRB_NP_VP_.\nS_NP_VP_.\nS_VP_.\nNP_NP_VP_.\nCC_NP_VP_.\nCC_ADVP_NP_VP_.\nVP_.\nPP_NP_VP_.\nTop constituents\nFigure 4: The class balance of datasets\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-07-01",
  "updated": "2022-07-01"
}