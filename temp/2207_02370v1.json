{
  "id": "http://arxiv.org/abs/2207.02370v1",
  "title": "Unsupervised Learning for Human Sensing Using Radio Signals",
  "authors": [
    "Tianhong Li",
    "Lijie Fan",
    "Yuan Yuan",
    "Dina Katabi"
  ],
  "abstract": "There is a growing literature demonstrating the feasibility of using Radio\nFrequency (RF) signals to enable key computer vision tasks in the presence of\nocclusions and poor lighting. It leverages that RF signals traverse walls and\nocclusions to deliver through-wall pose estimation, action recognition, scene\ncaptioning, and human re-identification. However, unlike RGB datasets which can\nbe labeled by human workers, labeling RF signals is a daunting task because\nsuch signals are not human interpretable. Yet, it is fairly easy to collect\nunlabelled RF signals. It would be highly beneficial to use such unlabeled RF\ndata to learn useful representations in an unsupervised manner. Thus, in this\npaper, we explore the feasibility of adapting RGB-based unsupervised\nrepresentation learning to RF signals. We show that while contrastive learning\nhas emerged as the main technique for unsupervised representation learning from\nimages and videos, such methods produce poor performance when applied to\nsensing humans using RF signals. In contrast, predictive unsupervised learning\nmethods learn high-quality representations that can be used for multiple\ndownstream RF-based sensing tasks. Our empirical results show that this\napproach outperforms state-of-the-art RF-based human sensing on various tasks,\nopening the possibility of unsupervised representation learning from this novel\nmodality.",
  "text": "Unsupervised Learning for Human Sensing Using Radio Signals\nTianhong Li*\nLijie Fan∗\nYuan Yuan∗\nDina Katabi\nMIT CSAIL\nAbstract\nThere is a growing literature demonstrating the feasibility\nof using Radio Frequency (RF) signals to enable key com-\nputer vision tasks in the presence of occlusions and poor\nlighting. It leverages that RF signals traverse walls and\nocclusions to deliver through-wall pose estimation, action\nrecognition, scene captioning, and human re-identiﬁcation.\nHowever, unlike RGB datasets which can be labeled by hu-\nman workers, labeling RF signals is a daunting task because\nsuch signals are not human interpretable. Yet, it is fairly\neasy to collect unlabelled RF signals. It would be highly\nbeneﬁcial to use such unlabeled RF data to learn useful rep-\nresentations in an unsupervised manner. Thus, in this paper,\nwe explore the feasibility of adapting RGB-based unsuper-\nvised representation learning to RF signals. We show that\nwhile contrastive learning has emerged as the main tech-\nnique for unsupervised representation learning from images\nand videos, such methods produce poor performance when\napplied to sensing humans using RF signals. In contrast,\npredictive unsupervised learning methods learn high-quality\nrepresentations that can be used for multiple downstream\nRF-based sensing tasks. Our empirical results show that\nthis approach outperforms state-of-the-art RF-based human\nsensing on various tasks, opening the possibility of unsu-\npervised representation learning from this novel modality.\n1. Introduction\nRF-based vision has emerged as an attractive research\ndirection that uses Radio Frequency (RF) signals to “see”\nhuman poses, body shape, and activities through walls and\nin dark settings [3, 12, 11, 27, 20, 44, 46, 23, 33, 37, 24,\n21, 4, 38, 40, 35, 31]. While visible light can be easily\nblocked by walls and opaque objects, RF signals in the WiFi\nrange, can traverse such occlusions. RF signals reﬂect off\nthe human body, provide information to track people, and\ncapture their shape and actions. Previous work has leveraged\nthose properties to detect people and track their walking\nspeed [2]. More recent advances in RF-based learning have\n*Indicates equal contribution. This work was supported by the GIST-\nMIT Research Collaboration grant funded by GIST.\ndemonstrated the feasibility of using neural networks to per-\nform various computer vision tasks like pose estimation [46],\naction recognition [24], scene captioning [12], and human re-\nidentiﬁcation [11] using RF signals as the sole input. These\nsystems are capable of working through walls and in dark\nscenarios, and thus go beyond the limitations faced by RGB-\nbased systems.\nSuch RF-based tasks have typically used supervised learn-\ning. Yet, unlike RGB datasets, which can be labeled by hu-\nman workers, labeling RF signals is a daunting task because\nsuch signals are not human interpretable. To label RF data,\na synchronized human-interpretable stream, like video, must\nbe present to assist the annotator. Speciﬁcally, when collect-\ning an RF dataset, researchers deploy an RF device and a\ncamera system, synchronize the data streams from the two\nsystems, and calibrate their views and positions with respect\nto each other. The annotator then generates labels based on\nthe RGB data. However, using RGB data as assistance could\nonly label a small portion of RF data. In particular, it is\nhard to use this approach to generate labelled RF datasets of\nnatural living at home since most people would have privacy\nconcerns about deploying cameras in their homes. Also, a\nsingle camera has a limited ﬁeld of view; thus, users typically\nneed to deploy synchronize and calibrate a multi-camera sys-\ntem, introducing signiﬁcant overhead. Moreover, cameras\ndo not work well in dark settings and with occlusions, which\nare common indoor scenarios.\nThe above limitations motivate the need for learning from\nunlabelled RF signals.\nUnsupervised or self-supervised\nrepresentation learning has attracted much recent interest\nand is a rapidly growing research area in computer vi-\nsion [16, 10, 39, 19, 14, 5, 48, 25, 15]. It refers to learning\ndata representations that capture potential labels of interest\nand doing so without human supervision. Most unsupervised\nrepresentation learning methods are designed for RGB data;\nand no past work has shown the feasibility of unsupervised\nlearning from RF signals.\nApplying traditional unsupervised learning methods to\nRF data is not straight-forward for the following reasons:\n• Traditional unsupervised methods either rely on strong\naugmentations, e.g., color jittering in contrastive learn-\ning, or require pretext tasks such as colorizing grey-scale\narXiv:2207.02370v1  [cs.CV]  6 Jul 2022\nSignals From Person\n…\n…\nCorresponding\nRGB Camera\nRF Signals\nFigure 1. Illustration of RF signals and corresponding RGB images (for\nillustration ONLY). The signal reﬂected off the person occupies a small\nregion in the received RF signal. Further, there are many other reﬂections\nfrom other objects.\nimages or predicting image rotation. However, these RGB-\nspeciﬁc augmentations and tasks cannot be directly ap-\nplied to RF signals. RF data is quite different from RGB\nimages. It is obtained by analyzing the signal power re-\nﬂected from different locations in space. There is no color\ninformation in RF signals, and the signal is not invariant\nto rotation transformation, making most of the existing\nunsupervised learning methods not directly applicable to\nRF signals.\n• As shown in Figure 1, in RF signals, the information re-\ngion that corresponds to a person could be extremely small,\noccupying <1%. This is because the majority of RF signals\nare reﬂected from objects in the environment. Therefore,\ndirectly applying unsupervised learning methods on RF\nsignals could be strongly biased by background noises,\nand fail to learn useful information about the person.\n• RF signals carry much information that is irrelevant to\nthe person or task. Not only human but almost all objects\ncould reﬂect signals. In particular, only a fraction of an\nRF signal traverses a wall, and the rest reﬂects off walls\nand other objects in the environment. The percentage of\nthe signal that traverses a wall vs. the amount that reﬂects\noff it depends on the material and surface of the wall,\nand is not easy to estimate. The same can be said about\nalmost all objects in the environment. Furthermore, it is\nnot immediately clear which part of the reﬂected signal\ncorresponds to the person or people in the scene. This\nmakes it hard for unsupervised learning methods to focus\non representations that are useful for human sensing and\navoid irrelevant information.\nIn this paper, we introduce a new unsupervised learn-\ning framework, Trajectory-Guided Unsupervised Learning\n(TGUL) to solve the above challenges brought in by RF\nsignals. We ﬁrst introduce modiﬁed data augmentations and\nself-supervised tasks that are suitable for RF signals. We\nthen solve the problem of sparse relevant information and\neliminate the irrelevant information. To do so, we leverage\nsignal processing techniques that can be applied without any\nsupervision. Speciﬁcally we use a radar-based module to\ndetect people and track their trajectories. Then at any point\nin time, we zoom in on the region of interest that contains\nthe person, and eliminate signals reﬂected from other objects.\nUnsupervised training loss is only added inside this region\nof interest to avoid the model learning background noises.\nWe adapt TGUL to predictive learning and contrastive\nlearning algorithms. We evaluate and compare it with state-\nof-the-art supervised baselines for RF-based sensing tasks,\nincluding 3D pose estimation, action recognition, and person\nre-identiﬁcation. Our results show that contrastive learning\nmethods could still be strongly biased to learning shortcut\ninformation in RF signals, and discarding useful information\nabout the person, while predictive learning methods learn\nuseful information and consistently improve the performance\nover supervised baselines under the ﬁne-tuning setting.\nTo summarize, this paper takes an important step to-\nwards extending unsupervised representation learning to new\nmodalities that are hard to interpret by humans. Speciﬁcally\nit makes the following contributions:\n• The paper introduces trajectory-guided unsupervised learn-\ning (TGUL), a novel unsupervised learning framework for\nRF signals. We show that TGUL is widely applicable to a\nvariety of RF-based human sensing tasks such as pose es-\ntimation, action recognition, and person re-identiﬁcation.\n• We demonstrate for the ﬁrst time the possibility of boost-\ning the performance of radio-based human sensing tasks\nby leveraging unlabeled radio signals. For example, with\nextra unlabeled RF data, the performance of RF-based\npose estimation could be improved by 11.3%. We further\nshow that even without extra unlabeled data, pre-training\nthe network using TGUL can still improve the perfor-\nmance by 5.7%.\n• The paper also shows that state of the art contrastive learn-\ning techniques such as SimCLR, MoCo, and BYOL do\nnot work well on radio signals. They tend to learn shortcut\ninformation that are contrastive but do not help the down-\nstream tasks. The vulnerability of contrastive learning to\nshortcuts has been observed in the context of RGB data,\nwhere color distribution can be a shortcut that prevents\ncontrastive learning from learning semantic information\nfrom images [6]. In the context of RGB data, it is possi-\nble to design data augmentations to break such shortcuts,\ne.g., color jittering. However, since RF signals cannot\nbe interpreted by humans, one cannot easily design data\naugmentations that alter RF signals in a manner that elimi-\nnates the shortcut without hampering the original task. As\na result, contrastive learning does not perform well on RF\nsignals.\n2. Related Work\nRF-based Vision. There is a growing literature that uses\nradio signals to “see” people’s poses and activities through\nwalls and in dark settings. Compared with RGB data, radio\nsignals have several advantages: they can traverse walls\nand occlusions; work in daytime and in darkness; and are\nnot human-interpretable and hence more privacy preserving.\nThus, RF-based vision has emerged as an attracting research\ndirection that enables new applications in health care and\nsmart homes [3, 11, 27, 20, 44, 23, 33, 37, 24, 21, 4, 38, 40,\n35, 31].\nPrevious RF-based vision works include human pose\nestimation, action recognition, captioning, person re-\nidentiﬁcations and so on [44, 46, 45, 37, 24, 9, 42, 12, 11,\n20, 35]. However, past works rely on supervised learning.\nGiven the difﬁculty of annotating RF datasets with ground\ntruth labels, past work is limited to relatively small datasets.\nUnsupervised Learning from RGB Data. Unsupervised\nlearning methods used in RGB-based vision can be divided\ninto three categories: self-supervision using pretext tasks,\npredictive approaches, and contrastive approaches. Early\nwork on unsupervised representation learning has focused\non designing pretext tasks and training the network to pre-\ndict their pseudo labels. Such tasks include solving jigsaw\npuzzles [26], colorize grey-scale images [41] or predicting\nimage rotation [13]. However, pretext tasks have to be hand-\ncrafted and are highly dependent on the properties of RGB\ndata, making them hard to be generalized to other modalities.\nUnsupervised learning based on predictive models includ-\ning auto-regressive (AR) and auto-encoding (AE) models\nshow less dependence on RGB data. The family of auto-\nencoders provides a popular framework for unsupervised\nrepresentation learning using a predictive loss [18, 30, 36].\nIt trains an encoder to generate low-dimensional latent\ncodes that could reconstruct the entire high-dimensional\ninputs. There are many types of AEs, such as denoising\nauto-encoders [36], which corrupt the input and let the la-\ntent codes reconstruct it, and variational auto-encoders [30],\nwhich force the latent codes to follow a prior distribution.\nHowever, relevant information in radio signals is typically\nvery sparse. Therefore, traditional AE models can easily\nignore the region in RF signal that is relevant to the person\nand their action.\nRecently, contrastive learning has become widely used for\nlearning effective representations without human supervision.\nThe representations learned with this approach generalize\nwell to downstream tasks, and in some cases can surpass the\nperformance of supervised models [6, 7, 8, 16]. The core\nidea of contrastive learning is to keep features from posi-\ntive sample pairs close, and features from negative samples\nfar from each other. Today, multiple successful contrastive\nlearning frameworks exist, with small differences in how\nthey pick negative samples. They include SimCLR [6], the\nmomentum-contrastive approach (MoCo) [16], Contrastive-\nMultiview-Coding [32], and BYOL [14].\nHowever, contrastive learning could suffer from the is-\nsue of “shortcut” – i.e., it can learn easy features that are\nirrelevant to the downstream task, and ignore relevant fea-\ntures [6, 34]. For example, in the case of images, the color\ndistribution can be a shortcut that prevents contrastive learn-\ning from learning the semantic information in an image [6].\nTo avoid such shortcut, contrastive learning methods use\ncolor jittering as a data augmentation. In Section 6, we show\nthat state-of-the-art contrastive learning methods can fail\nto learn meaningful representations due to shortcuts in RF\nsignals. However, unlike shortcuts in RGB data, since RF\nsignals cannot be interpreted by humans, one cannot eas-\nily design data augmentations that alter the RF signal in a\nmanner that eliminates the shortcut without hampering the\noriginal task.\n3. RF Signals Preliminary\nVertical \nHorizontal\nDepth\nVertical\nHeatmap\nHorizontal\nHeatmap\nFigure 2. Illustration of RF signals as a pair of horizontal and vertical\nheatmaps after subtracting static objects, and an RGB image recorded at the\nsame time. Red color refers to high signal power, blue refers to low power.\nRF-based human sensing relies on transmitting a low\npower radio signal and receiving its reﬂections from the en-\nvironment and nearby people [44, 46, 24, 9, 42, 12, 45, 1,\n35, 33, 43, 47, 20, 11]. These technologies typically use a\nradio device that combines FMCW (Frequency Modulated\nContinuous Wave) and antenna arrays [1]. This allows the\nradio to operate like a consumer radar, and separate RF sig-\nnal reﬂections from different objects in space. Speciﬁcally,\nFMCW separates RF reﬂections based on the distance of the\nreﬂecting object, whereas antenna arrays separate reﬂections\nbased on their spatial direction.\nAs in past work, we consider a radio device with two\nantenna arrays: a horizontal one and a vertical one. As a\nresult, the RF signals can be expressed as two heatmaps, as\nshown in Fig. 2. The horizontal heatmap is a projection\nof the radio signals on a plane parallel to the ground, and\nthe vertical heatmap is a projection of the radio signals on\na plane perpendicular to the ground. We use the term RF\nframe to refer to a pair of horizontal and vertical heatmaps.\n4. RF-Based Human Sensing Framework\nWe choose the following three RF-based sensing tasks\nto validate the performance of our unsupervised learning\nf f\nf\n…\n…\n…\n3D Pose \nEstimation\nLatent\nFeatures\nCropped\nFeatures\nAction \nRecognition\nPerson \nRe-Identification\nDownstream\nTasks\nSub\nNetwork\nSub\nNetwork\nSub\nNetwork\nTask-Specific\nSub-Networks\nInput\nHeatmaps !\n…\nTask-Agnostic\nFeature Extraction\nROI\nAlign\nℝ\nFeature \nExtract\nNetwork\nJumping\nWalking\n…\nSitting\nJack\nBob\n…\nMary\nFigure 3. RF-based human sensing framework. The RF heatmaps are ﬁrst fed to a Feature Network to extract spatio-temporal features, and ROI Align with\nthe labeled bounding box is used to crop out the regions with humans. Each downstream task has a unique sub-network to extract task-speciﬁc features and\npredictions.\nmethod:\n• (a) RF-Based 3D Pose Estimation [44, 46], which uses\nRF signals to infer the 3D locations of 14 keypoints on\nthe human body: head, neck, shoulders, arms, wrists, hips,\nknees, and feet.\n• (b) RF-Based action recognition [24], which analyzes ra-\ndio signals to infer human actions and interactions in the\npresence of occlusions and in poor lighting.\n• (c) RF-Based Re-Id [12], which recognizes a person-of-\ninterest across different places and times by analyzing the\nradio signals that bounce off their bodies.\nIn all these tasks, the neural network has the general\nstructure in Figure 3. The model ﬁrst uses a spatio-temporal\nconvolutional feature network consisting of 9 residual blocks\nto extract features from the input RF frames. It then crops out\na bounding box around the person in the feature map. Finally,\nthe cropped features are fed to a task-speciﬁc sub-network\nto generate frame-level features. The feature network is the\nsame for all tasks, but different tasks could have different\nsub-networks speciﬁcally designed for that task.\n5. Trajectory Guided Unsupervised Learning\nIn this section, we adapt contrastive and predictive unsu-\npervised learning algorithms to learn representations from\nunlabeled RF signals.\n5.1. Signal Processing for ROI Detection\nUnlike standard RGB datasets, where the object of inter-\nest typically occupies many of the pixels in an image, the RF\nsignals reﬂected from a person only occupy a small portion\nof an RF heatmap (e.g., < 1%). Therefore, we need to track\nand zoom in on the person before we apply unsupervised\nlearning methods to RF signals, otherwise, the unsupervised\nlearning method is likely to learn information about the back-\nground instead of the person.\nTraditional approaches for zooming in on individuals or\nobjects in RGB images typically rely on learning a bound-\ning box of this object from ground truth labels, which are\nnot available in the context of unsupervised learning. Thus,\ninstead of using methods from computer vision, we leverage\nthe properties of radio signals. Speciﬁcally, there are mainly\ntwo challenges in detecting the region of interest in the RF\nsignal. The ﬁrst is to be able to zoom in on the radio signals\nreﬂected from a speciﬁc location or region in space (presum-\nably the location of the person) and ignore the rest of the\nRF signal. To address this issue, we leverage the horizontal\nand vertical heatmaps, which correspond to the projection of\nthe radio signals on a horizontal plane and a vertical plane.\nThus, to zoom in on the signal reﬂected from a region in\nspace, we can simply crop out the projection of this region\non the horizontal and vertical heatmaps.\nThe second challenge is to determine which region in\nspace contains the person. In RGB data, detecting a person\ntypically requires complex algorithms or neural networks\ntrained using a large dataset. In contrast, we use the fact that\nFMCW radios can operate as a radar system that tracks and\ndetect moving objects. In most indoor scenarios, people are\nthe only large moving objects. Therefore, we can adapt radar\ndetection algorithms to detect the person and generate their\ntrajectory as they move around. When the person becomes\nstatic, the trajectory will stay at the location where the person\nstops moving. Then it will start tracking the person again\nwhen he or she starts moving again. To be more speciﬁc, we\nuse a signal-processing technique called WiTrack [2] to au-\ntomatically track and generate the trajectory of the person. It\nﬁrst subtracts from heatmap the median over a long period to\nremove the reﬂections from static objects in the scene. Then\nit computes the difference between consecutive frames to de-\ntect moving persons and generate their trajectories. We then\ngenerate bounding boxes of size 1m×1.5m for the person in\neach frame based on their trajectory.\n5.2. Predictive Unsupervised Learning from RF\nWe ﬁrst consider learning unsupervised representations\nby trying to predict how the human body changes the radio\nsignals that bounce of it. Our predictive learning framework\nEncoder \nNetwork\n!!\nf\nf\nf\n…\nDecoder\nNetwork\n\"\"\nInput\nHeatmaps !\nROI\nAlign\nReconstruct\nLoss #$\n…\nLatent\nFeatures\nCropped\nFeatures ℱ\nPredicted\nROI\nCropped\nROI\nSignal\nProcess\nMasking\nBounding\nboxes\nSignal Processing Module\nNeural Network Module\nℝ\n$\nFigure 4. Predictive Learning adapted to RF signals. In the Signal Processing Module (orange box), we use background subtraction to remove static objects\nand moving detection to obtain bounding boxes for each person in the scene. After this, RF frames are randomly masked and ﬁlled with zero. In the Neural\nNetwork Module (blue box), a feature extraction network (green box) is applied to extract features from RF inputs, and then an ROI Align module is applied\nwith the bounding box generated by the signal processing module to crop out regions containing humans. A decoder network is then adopted to predict\noriginal input ROIs from cropped features. The reconstruction loss between the cropped ROI of the original heatmap and the predicted ROIs are used to train\nthe model.\nfor RF signals is illustrated in Figure 4. Given an input\nsequence of RF heatmaps x consisting of T RF frames,\nx1, · · · , xT , we ﬁrst perform the aforementioned signal\nprocessing step to detect regions of interest bi that con-\ntain people.\nWe then randomly mask t frames and ﬁll\nthem with zeros to get the masked input M(x).\nThen\nthe masked input is passed through an encoder network\nE with parameter θ, an ROI align module R which crops\nout regions of interest in feature space using bi, and a de-\ncoder network D, with parameter δ, to obtain the recon-\nstruction result Dδ(R(Eθ(M(x))i, bi)). The reconstruction\nloss Lr is deﬁned as the reconstruction error between the\ncropped ROI from the original input and the reconstructed\none Dδ(R(Eθ(M(x))i, bi)):\nLr =\nT\nX\ni=1\n||Dδ(R(Eθ(M(x))i, bi)) −R(xi, bi)||2.\nInstantiation. For the encoder, we use the same network\narchitecture as prior works [46, 11, 24], which is a spatio-\ntemporal convolutional feature network consisting of 9 resid-\nual blocks. For the decoder, we use a spatial de-convolutional\nnetwork with 3 residual blocks. For predictive learning, we\nadapt two standard predictive methods: Auto-encoder [18]\nand Inpainting [29]. For Auto-encoder, there is no masking\noperation on the input, i.e., M is an identical mapping. For\nInpainting, for each RF sequence consisting of T = 100 RF\nframes, we randomly mask out ﬁve 5-frame segments within\nthis sequence and ﬁll them with zeros.\n5.3. Contrastive Learning from RF\nWe also consider adapting contrastive learning to RF sig-\nnals. The contrastive learning framework for RF signals is\nillustrated in the orange box in Figure 5. For each input RF\nheatmap sequence x with T frames x1, · · · , xT , we ﬁrst per-\nform the same signal processing step as in predictive learning\nto detect regions of interest and produce a bounding box bi\nfor each frame xi. Then we generate a pair of positive sam-\nples by performing two data augmentations τ1 and τ2 to the\ngenerated bounding boxes from the signal processing step,\nresulting in augmented bounding boxes τ1(bi), τ2(bi). Then\nwe forward the inputs signal with the augmented bounding\nboxes separately into the encoder E parameterized by θ, an\nROI align module R and a multi-layer nonlinear projection\nhead H parameterized by h to get the latent representations\nz2i and z2i+1 for these two positive samples. Moreover,\nsince human actions are continuous, features within S = 10\nframes of xi is also taken as positive pairs. We use the\ncommonly used InfoNCE loss [6] as the contrastive loss\nLc. Namely, for a batch of N RF segments containing NT\nframes xj\ni, i = 1, · · · , T, j = 1, · · · , N,\nLc = −\nN\nX\nj=1\nT\nX\ni=1\nS\nX\nm=1\nlog\nexp\n\u0000sim(zj\n2i, zj\n2i−S+2m)/t\n\u0001\nN\nP\nl=1\n2T\nP\nk=1\n1k̸=2i||l̸=j exp\n\u0000sim(zj\n2i, zl\nk)/t\n\u0001\nwhere sim(u, v) = uT v/(∥u∥2∥v∥2) denotes the dot\nproduct between the normalized u and v (i.e., cosine sim-\nilarity), t ∈R+ is a scalar temperature parameter, and\nz2i, z2i+1 are the encoded features of positive pairs gen-\nerated from xi, i.e., z2i = Hh(R(Eθ(xi), τ1(b))) and\nz2i+1 = Hh(R(Eθ(xi), τ2(b))).\nNote that the augmentations τ1, τ2 here are crop and re-\nsize augmentations on the bounding boxes, instead of the\nwhole input. This is because due to the sparsity of RF sig-\nnals, performing crop and resize on the whole input may\ncrop out the region of interest.\nInstantiation For the encoder, we use the same network\narchitecture as prior works [46, 11, 24]. For the projection\nhead, we use a spatial convolutional network with one resid-\nual block. For negative pairs, we follow the original negative\nEncoder \nNetwork\n!!\n…\nInput\nHeatmaps $\nLatent\nFeatures\n%%\nEncoder \nNetwork\n!!\nf f\nf\n…\nProject\nHead &&\nProject\nHead &&\n…\n…\nShared\nParameter\nShared\nParameter\nContrastive\nLoss #'\nSignal Processing Module\nNeural Network Module\nROI\nAlign\nℝ\nROI\nAlign\nℝ\nCropped\nFeatures ℱ\nSignal\nProcess\nBounding\nboxes\n%(\nFigure 5. Contrastive Learning on RF signals. In the Signal Processing Module (orange box), we use background subtraction to remove static objects and\nmoving detection to obtain bounding boxes for each person in the scene. After this, different transformation τ1 and τ2 to the bounding box are applied to\ngenerate positive samples. Both branches share the same feature extraction network (green box), and then use ROI Align with the given bounding box to crop\nout regions containing humans. A non-linear projection head is used to generate normalized features. Contrastive loss is used to train the model.\nsampling strategy of whatever contrastive learning method\nTGUL build on. Since there is no prior work that applies\nexisting contrastive learning methods to RF data, we imple-\nment SimCLR [6], MoCo [16], CPC [17] and BYOL [14] for\nRF data by ourselves. The SimCLR implementation is what\nwe just described in Figure 5. For MoCo, we use the same\ndata augmentations as SimCLR, except that the positive fea-\nture pairs are generated by two feature networks, a normal\none and a momentum one. The loss is the same as the one\nused in [16]. For BYOL, we use the same data augmentation\noperations as SimCLR, and follow the implementation of\n[14] to generate predictions, projections, and loss. For CPC,\nwe follow the design in [17]. Speciﬁcally, we add a GRU\nafter the RF feature extractor. The output of the GRU at\nevery frame is then used as the context to predict the features\nin 1.5s in the future using the contrastive loss.\n6. Experiments\nRF Datasets. We use two real-world RF datasets used in\npast work: RF-MMD [24] and RRD [11]. RF-MMD is an\nRF-based 3D pose-estimation and action-recognition dataset\nconsisting of 25 hours of RF data with 30 volunteers perform-\ning 35 actions in 10 different indoor environments. RRD\nis an RF-based person re-identiﬁcation dataset. It contains\n863 RF sequences from 100 different identities at 5 differ-\nent locations on a campus, and 6305 RF sequences from 38\ndifferent identities in 19 homes.\nEvaluation Metrics. We use the performance metrics used\nby past work for each task. Speciﬁcally, for 3D pose estima-\ntion, we use the average l2 distance between 14 predicted\nkeypoints and their ground-truth locations as the evaluation\nmetric. For action recognition, we use mean average preci-\nsion (mAP) at different intersection-over-union (IoU) thresh-\nolds θ to evaluate the accuracy of the model in detecting and\nclassifying an action event. For person re-identiﬁcation, we\nseparate the dataset into query set and gallery set. The query\nsamples and gallery samples are then encoded to feature vec-\ntors. We calculate the cosine distance between the features\nof each query sample and each gallery sample, and rank the\ndistance to get the top-N closest gallery samples for each\nquery sample. Based on the ranking results, we compute the\nmean average precision (mAP) and the cumulative matching\ncharacteristic (CMC) at rank-1 and rank-5.\nEvaluation Settings. We evaluate the performance under\nboth ﬁxed feature extractor setting and ﬁne-tuning setting.\nIn the ﬁxed feature extractor setting, we use the pre-trained\nRF feature extractor as initialization of the RF feature extrac-\ntor for each downstream task, and only those task-speciﬁc\nsub-networks (Fig. 3) are trained during downstream task\ntraining. In the ﬁne-tuning setting, we ﬁne-tune the whole\nmodel on the downstream task.\nTraining Details. For all approaches, we train the network\nfor 50 epochs with a batch size of 128, using the Adam\noptimizer [22] with 1e-3 learning rate and 1e-5 weight decay.\nWe use PyTorch [28] for implementations.\n6.1. Main Results\nWe evaluate our unsupervised learning method on mul-\ntiple downstream RF-based tasks, and report the results in\nTables 1 and 2. Note that contrastive leaning methods could\nnot be directly applied to RF signals without TGUL because\nthe RGB-based augmentations is not directly applicable to\nRF signals. Table 1 shows the results of the ﬁxed feature\nextractor setting. The table reveals two ﬁndings. First, pre-\ndictive pre-training using TGUL learns useful unsupervised\nrepresentations and delivers 15.0% relative improvement in\n3D pose estimation error, 19.5% relative improvement in\naction recognition mAP at θ = 0.1, 77.2% relative improve-\nment in person ReID mAP on RRD-Campus, and 27.9%\nrelative improvement on RRD-Home. Further, inpainting,\ni.e., predicting missing RF signals, is more powerful than\nautoencoding though both deliver gains.\nTable 1. Evaluation of different models on different RF tasks under ﬁxed feature extractor setting and the relative improvements over network\ninitialized randomly. ↓indicates the smaller the value, the better the performance; ↑indicates the larger the value, the better the performance.\nTasks\n3D Pose Estimation\nAction Recognition\nPerson Re-ID (Campus)\nPerson Re-ID (Home)\nMetrics\nPose ERR.↓(mm)\nmAP↑\nmAP↑\nCMC-1↑\nCMC-5↑\nmAP↑\nCMC-1↑\nCMC-5↑\nθ = 0.1\nθ = 0.5\nRandom init\n60.1\n60.5\n53.3\n28.1\n43.8\n68.8\n30.1\n54.2\n74.6\nSimCLR + TGUL\n80.5\n4.2\n0\n29.8\n44.1\n67.5\n31.2\n55.1\n73.8\nMoCo + TGUL\n77.2\n5.1\n0.18\n29.1\n44.7\n65.3\n30.5\n54.5\n74.0\nCPC + TGUL\n78.7\n3.6\n0\n30.0\n42.7\n69.5\n30.7\n54.0\n75.3\nBYOL + TGUL\n79.3\n4.7\n0\n29.5\n44.4\n66.7\n30.7\n54.6\n73.5\nAutoencoder\n59.4\n62.3\n54.2\n29.0\n44.5\n67.0\n31.1\n55.5\n75.5\nAutoencoder + TGUL\n55.7\n71.1\n63.2\n43.8\n69.7\n87.2\n35.2\n61.5\n81.9\nInpainting\n58.0\n63.9\n55.4\n30.2\n48.1\n70.5\n32.8\n57.7\n76.5\nInpainting + TGUL\n51.1\n72.3\n65.5\n49.8\n73.1\n90.5\n38.5\n64.2\n84.7\nIMPROVEMENT\n+15.0%\n+19.5%\n+22.9%\n+77.2%\n+66.9%\n+31.5%\n+27.9%\n+18.5%\n+13.5%\nTable 2. Evaluation of different models on different RF tasks under ﬁne-tuning setting and the relative improvements over supervised training\nfrom random initialized network. ↓indicates the smaller the value, the better the performance; ↑indicates the larger the value, the better the\nperformance.\nTasks\n3D Pose Estimation\nAction Recognition\nPerson Re-ID (Campus)\nPerson Re-ID (Home)\nMetrics\nPose ERR.↓(mm)\nmAP↑\nmAP↑\nCMC-1↑\nCMC-5↑\nmAP↑\nCMC-1↑\nCMC-5↑\nθ = 0.1\nθ = 0.5\nSupervised [24, 11]\n38.4\n90.1\n87.8\n59.5\n82.1\n95.5\n46.4\n74.6\n89.5\nSimCLR + TGUL\n38.8\n89.8\n87.4\n59.0\n81.7\n94.1\n45.9\n73.8\n88.5\nMoCo + TGUL\n38.3\n89.7\n87.2\n59.3\n82.0\n94.5\n46.4\n74.3\n89.7\nCPC + TGUL\n38.6\n89.9\n87.5\n59.4\n81.5\n94.0\n46.0\n74.5\n89.1\nBYOL + TGUL\n38.5\n89.7\n87.2\n59.4\n81.9\n94.5\n46.6\n74.5\n89.5\nAutoencoder\n38.5\n90.0\n87.7\n59.1\n81.9\n95.5\n45.9\n74.2\n88.6\nAutoencoder + TGUL\n37.5\n91.2\n87.9\n59.7\n82.8\n95.5\n46.8\n74.6\n89.8\nInpainting\n38.2\n90.5\n88.0\n59.3\n82.1\n95.7\n46.2\n74.4\n89.2\nInpainting + TGUL\n36.2\n91.7\n88.7\n60.1\n83.3\n95.9\n47.5\n75.3\n90.3\nIMPROVEMENT\n+5.7%\n+1.8%\n+1.0%\n+1.0%\n+1.5%\n+0.4%\n+2.4%\n+0.9%\n+0.9%\nSecond, the table shows that contrastive pre-training per-\nforms worse on all the downstream tasks than predictive\npre-training, and even worse than random initialization for\nthe 3D pose estimation and action recognition tasks. This is\nbecause contrastive learning learns shortcut semantics irrele-\nvant to the tasks of interest. We further investigate this issue\nof shortcut in Section 6.2.\nTable 2 shows the results of the ﬁne-tuning setting. The\ntable shows that initializing supervised models with Inpaint-\ning+TGUL’s representations can achieve 5.7% relative im-\nprovement in 3D pose estimation error, 1.8% relative im-\nprovement in action recognition mAP at θ = 0.1, 1.0% rel-\native improvement in person ReID mAP on RRD-Campus,\nand 2.4% relative improvement on RRD-Home. In contrast,\ninitializing supervised models with contrastive representa-\ntions is not useful.\nTables 1 and 2 also demonstrate the effectiveness of\nTGUL. Directly applying Autoencoder or Inpainting on RF\nsignals only brings limited improvements over randomly\ninitialized network under ﬁxed feature network setting, and\ncannot improve the performance under ﬁne-tuning setting.\nOn the other hand, Autoencoder and Inpainting with TGUL\nachieves much better performance under both settings. This\nis because TGUL is effective in forcing the network to focus\non the person and pay less attention to other regions which\ncontain mostly noises, and thus achieves better performance\nthan without this guidance.\nOverall, the results show that our framework is the ﬁrst\nunsupervised method to boost the performance of RF-based\nhuman sensing, bringing the gains of unsupervised learning\nto this new data modality.\n6.2. Feature Visualization\nAs shown in Table 1 and 2, contrastive pre-training meth-\nods perform poorly and even worse than random initial-\nization of the feature network. This is because contrastive\nlearning is likely to exploit shortcut information (e.g., the dis-\ntance of the person to the RF device) and discard other useful\ninformation for downstream tasks. In Figure 6, we visualize\nthe features generated by feature networks pre-trained using\ncontrastive learning and predictive learning. As shown in\nthe ﬁgure, the features network pre-trained using contrastive\n(a) Original Heatmap\n(b) CL Features\n(c) PL features\nFigure 6. Contrastive learning (CL) can be biased to shortcut and\ndiscard useful information in the input, while predictive learning\n(PL) can preserve the information in the input.\nlearning discard most of the information in the input. On the\nother hand, the feature network pre-trained using predictive\nlearning can preserve and abstract the information about the\nperson in the input and thus improve the performance.\n6.3. Ablation Studies\nWe perform ablation studies on our proposed unsuper-\nvised learning framework. All ablation studies are performed\non the RF-MMD [24] dataset with the downstream task of\npose estimation, where we report the average error across all\nkeypoints.\nMasking Strategy: When applied to RF signals, the In-\npainting task is performed by masking some frames in a short\nwindow of RF signals and reconstructing them from the re-\nmaining frames. We compare the performance of Inpainting\nwith different masking strategies. The input RF signal is a\n100-frames RF sequence. We compare 3 different masking\nstrategies: center segment (25 frames in the center of the\ninput), random segments (ﬁve 5-frame segments at random\npositions of input), and random (each frame is masked with\n0.25 probability). As shown in the results below, the random\nsegments masking strategy performs the best.\nTable 3. Pose error (the lower the better) with different masking\nstrategies.\nMask Strategy\nPose ERR.↓(mm)\nCenter Segment\n36.8\nRandom Segments\n36.2\nRandom\n36.4\nSize of Masked Segment: Next, we study the inﬂuence\nof the size of each masked segment. Note that when the mask\nsize equals 0, no frame is masked, and the inpainting task\nbecomes a normal auto-encoder. In all of these experiments,\nwe mask 5 segments and follow the strategy of randomly\nlocating the masked segments in the input. As shown in\nTable 4, the best performance is for masking 5 consecutive\nframes, i.e., a segment of size 5.\nBeneﬁts of Unlabeled Data for RF-Based Sensing: By\nenabling RF-based tasks to leverage unsupervised represen-\ntation learning, our framework allows RF-based sensing to\nbeneﬁt from a large amount of unlabeled RF data. To eval-\nTable 4. Pose error with different sizes of masked segments.\nMasking Size\n0\n1\n3\n5\n10\nPose ERR.↓(mm)\n37.5\n37.0 36.3 36.2 36.7\nuate the beneﬁt of such unlabeled data, we simulate the\nscenario where only a handful of labeled RF data is available\nand a large amount of RF data is unlabeled data. Speciﬁcally,\nwe randomly select 10% of the training set of RF-MMD to be\nRF-MMD-S to serves as the small labeled dataset. We com-\npare the performance of Inpainting when it is pre-trained on\nRF-MMD-S and RF-MMD and ﬁnetuned on RF-MMD-S.\nTable 5. Performance of Inpainting on RF-MMD with a small\namount of labeled data. The results in the table use RF-MMD-S for\nﬁnetuning, which is a randomly selected 10% subset of RF-MMD\nwhich is used for unsupervised pre-training. The rest of the dataset\nis used without labels. The table shows that our framework can\nfurther improve the performance with more unlabeled data.\nMethods\nPose ERR.↓(mm)\nTraining from scratch (RF-MMD-S)\n48.7\nInpainting on RF-MMD-S+ﬁnetune\n46.1\nInpainting on RF-MMD+ﬁnetune\n43.2\nAs shown in Table 5, our framework can improve the\nperformance of RF-based 3D pose estimation by 5.3% with-\nout using any additional unlabeled data. This is because\nour framework can learn a general representation of RF sig-\nnals and thus provide better generalization ability. Indeed,\nour framework improves the pose error further by another\n6.0% by leveraging the unlabeled data. This demonstrates\nthe potential of using our framework to leverage large-scale\nunlabeled RF data to improve the performance of RF-based\nhuman sensing methods.\n7. Conclusion\nWe introduced TGUL, the ﬁrst unsupervised learning\nframework for RF-based human sensing tasks. We adapted\nstate-of-the-art unsupervised learning methods from RGB\ndata to RF data by leveraging radar technology to focus un-\nsupervised learning on the person’s trajectory, and design\ndata augmentations suitable for RF signals. We showed that\ncontrastive learning could be strongly biased by shortcuts\nin RF data, making it a poor choice for this modality. We\nfurther demonstrate the potential of unsupervised learning\nmethods based on predictive tasks for learning useful rep-\nresentations from RF data. Extensive empirical results on\nmultiple RF datasets and tasks show that TGUL could consis-\ntently improve the performance of RF-based sensing models\nover supervised learning baselines.\nReferences\n[1] Fadel Adib, Chen-Yu Hsu, Hongzi Mao, Dina Katabi, and\nFrédo Durand. Capturing the human ﬁgure through a wall.\nACM Transactions on Graphics (TOG), 34(6):1–13, 2015.\n[2] Fadel Adib, Zach Kabelac, Dina Katabi, and Robert C Miller.\n3d tracking via body radio reﬂections. In 11th {USENIX}\nSymposium on Networked Systems Design and Implementa-\ntion ({NSDI} 14), pages 317–329, 2014.\n[3] Fadel Adib and Dina Katabi. See through walls with wiﬁ!\nIn Proceedings of the ACM SIGCOMM 2013 conference on\nSIGCOMM, pages 75–86, 2013.\n[4] Roshan Ayyalasomayajula, Aditya Arun, Chenfeng Wu,\nSanatan Sharma, Abhishek Rajkumar Sethi, Deepak Vasisht,\nand Dinesh Bharadia. Deep learning based wireless local-\nization for indoor navigation. In Proceedings of the 26th\nAnnual International Conference on Mobile Computing and\nNetworking, pages 1–14, 2020.\n[5] Philip Bachman, R Devon Hjelm, and William Buchwalter.\nLearning representations by maximizing mutual information\nacross views. arXiv preprint arXiv:1906.00910, 2019.\n[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. arXiv preprint arXiv:2002.05709,\n2020.\n[7] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\nNorouzi, and Geoffrey E Hinton. Big self-supervised models\nare strong semi-supervised learners. Advances in Neural\nInformation Processing Systems, 33, 2020.\n[8] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Im-\nproved baselines with momentum contrastive learning. arXiv\npreprint arXiv:2003.04297, 2020.\n[9] Kevin Chetty, Qingchao Chen, Matthew Ritchie, and Karl\nWoodbridge. A low-cost through-the-wall fmcw radar for\nstand-off operation and activity detection. In Radar Sensor\nTechnology XXI, volume 10188, page 1018808. International\nSociety for Optics and Photonics, 2017.\n[10] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsu-\npervised visual representation learning by context prediction.\nIn Proceedings of the IEEE international conference on com-\nputer vision, pages 1422–1430, 2015.\n[11] Lijie Fan, Tianhong Li, Rongyao Fang, Rumen Hristov, Yuan\nYuan, and Dina Katabi. Learning longterm representations for\nperson re-identiﬁcation using radio signals. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10699–10709, 2020.\n[12] Lijie Fan, Tianhong Li, Yuan Yuan, and Dina Katabi. In-\nhome daily-life captioning using radio signals. arXiv preprint\narXiv:2008.10966, 2020.\n[13] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsu-\npervised representation learning by predicting image rotations.\narXiv preprint arXiv:1803.07728, 2018.\n[14] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\nnew approach to self-supervised learning. arXiv preprint\narXiv:2006.07733, 2020.\n[15] Tengda Han, Weidi Xie, and Andrew Zisserman. Memory-\naugmented dense predictive coding for video representation\nlearning. arXiv preprint arXiv:2008.01065, 2020.\n[16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9729–9738, 2020.\n[17] Olivier J Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali\nRazavi, Carl Doersch, SM Eslami, and Aaron van den Oord.\nData-efﬁcient image recognition with contrastive predictive\ncoding. arXiv preprint arXiv:1905.09272, 2019.\n[18] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing\nthe dimensionality of data with neural networks. science,\n313(5786):504–507, 2006.\n[19] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,\nKaran Grewal, Phil Bachman, Adam Trischler, and Yoshua\nBengio.\nLearning deep representations by mutual in-\nformation estimation and maximization.\narXiv preprint\narXiv:1808.06670, 2018.\n[20] Chen-Yu Hsu, Rumen Hristov, Guang-He Lee, Mingmin\nZhao, and Dina Katabi. Enabling identiﬁcation and behav-\nioral sensing in homes using radio reﬂections. In Proceedings\nof the 2019 CHI Conference on Human Factors in Computing\nSystems, pages 1–13, 2019.\n[21] Wenjun Jiang, Hongfei Xue, Chenglin Miao, Shiyang Wang,\nSen Lin, Chong Tian, Srinivasan Murali, Haochen Hu, Zhi\nSun, and Lu Su. Towards 3d human pose construction using\nwiﬁ. In Proceedings of the 26th Annual International Con-\nference on Mobile Computing and Networking, pages 1–14,\n2020.\n[22] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014.\n[23] Manikanta Kotaru and Sachin Katti. Position tracking for\nvirtual reality using commodity wiﬁ. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 68–78, 2017.\n[24] Tianhong Li, Lijie Fan, Mingmin Zhao, Yingcheng Liu, and\nDina Katabi. Making the invisible visible: Action recognition\nthrough walls and occlusions. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 872–\n881, 2019.\n[25] Ishan Misra and Laurens van der Maaten. Self-supervised\nlearning of pretext-invariant representations. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6707–6717, 2020.\n[26] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In European\nConference on Computer Vision, pages 69–84. Springer, 2016.\n[27] Masakatsu Ogawa and Hirofumi Munetomo. Wi-ﬁcsi-based\noutdoor human ﬂow prediction using a support vector ma-\nchine. Sensors, 20(7):2141, 2020.\n[28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. arXiv\npreprint arXiv:1912.01703, 2019.\n[29] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A Efros. Context encoders: Feature learn-\ning by inpainting. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2536–2544,\n2016.\n[30] Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan\nLi, Andrew Stevens, and Lawrence Carin. Variational autoen-\ncoder for deep learning of images, labels and captions. arXiv\npreprint arXiv:1609.08976, 2016.\n[31] Michał Rapczy´nski, Philipp Werner, Sebastian Handrich, and\nAyoub Al-Hamadi. A baseline for cross-database 3d human\npose estimation. Sensors, 21(11):3769, 2021.\n[32] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive\nmultiview coding. arXiv preprint arXiv:1906.05849, 2019.\n[33] Yonglong Tian, Guang-He Lee, Hao He, Chen-Yu Hsu, and\nDina Katabi. Rf-based fall monitoring using convolutional\nneural networks. Proceedings of the ACM on Interactive,\nMobile, Wearable and Ubiquitous Technologies, 2(3):1–24,\n2018.\n[34] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan,\nCordelia Schmid, and Phillip Isola.\nWhat makes for\ngood views for contrastive learning.\narXiv preprint\narXiv:2005.10243, 2020.\n[35] Deepak Vasisht, Anubhav Jain, Chen-Yu Hsu, Zachary Ka-\nbelac, and Dina Katabi. Duet: Estimating user position and\nidentity in smart homes using intermittent and incomplete\nrf-data. Proceedings of the ACM on Interactive, Mobile, Wear-\nable and Ubiquitous Technologies, 2(2):1–21, 2018.\n[36] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-\nAntoine Manzagol. Extracting and composing robust features\nwith denoising autoencoders. In Proceedings of the 25th\ninternational conference on Machine learning, pages 1096–\n1103, 2008.\n[37] Fei Wang, Sanping Zhou, Stanislav Panev, Jinsong Han, and\nDong Huang. Person-in-wiﬁ: Fine-grained person perception\nusing wiﬁ. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 5452–5461, 2019.\n[38] Chao Yang, Xuyu Wang, and Shiwen Mao. Rﬁd-pose: Vision-\naided three-dimensional human pose estimation with radio-\nfrequency identiﬁcation. IEEE Transactions on Reliability,\n2020.\n[39] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang.\nUnsupervised embedding learning via invariant and spreading\ninstance feature. In Proceedings of the IEEE Conference on\ncomputer vision and pattern recognition, pages 6210–6219,\n2019.\n[40] Jianyuan Yu, Pu Wang, Toshiaki Koike-Akino, Ye Wang,\nPhilip V Orlik, and Haijian Sun. Human pose and seat occu-\npancy classiﬁcation with commercial mmwave wiﬁ. In 2020\nIEEE Globecom Workshops (GC Wkshps, pages 1–6. IEEE,\n2020.\n[41] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful\nimage colorization. In European conference on computer\nvision, pages 649–666. Springer, 2016.\n[42] Zhenyuan Zhang, Zengshan Tian, and Mu Zhou. Latern:\nDynamic continuous hand gesture recognition using fmcw\nradar sensor. IEEE Sensors Journal, 18(8):3278–3289, 2018.\n[43] Mingmin Zhao, Fadel Adib, and Dina Katabi. Emotion recog-\nnition using wireless signals. In Proceedings of the 22nd\nAnnual International Conference on Mobile Computing and\nNetworking, pages 95–108, 2016.\n[44] Mingmin Zhao, Tianhong Li, Mohammad Abu Alsheikh, Yon-\nglong Tian, Hang Zhao, Antonio Torralba, and Dina Katabi.\nThrough-wall human pose estimation using radio signals. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 7356–7365, 2018.\n[45] Mingmin Zhao, Yingcheng Liu, Aniruddh Raghu, Tianhong\nLi, Hang Zhao, Antonio Torralba, and Dina Katabi. Through-\nwall human mesh recovery using radio signals. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 10113–10122, 2019.\n[46] Mingmin Zhao, Yonglong Tian, Hang Zhao, Mohammad Abu\nAlsheikh, Tianhong Li, Rumen Hristov, Zachary Kabelac,\nDina Katabi, and Antonio Torralba. Rf-based 3d skeletons. In\nProceedings of the 2018 Conference of the ACM Special In-\nterest Group on Data Communication, pages 267–281, 2018.\n[47] Mingmin Zhao, Shichao Yue, Dina Katabi, Tommi S Jaakkola,\nand Matt T Bianchi. Learning sleep stages from radio sig-\nnals: A conditional adversarial architecture. In International\nConference on Machine Learning, pages 4100–4109, 2017.\n[48] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local\naggregation for unsupervised learning of visual embeddings.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 6002–6012, 2019.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-07-06",
  "updated": "2022-07-06"
}