{
  "id": "http://arxiv.org/abs/2207.03757v2",
  "title": "Combining Deep Learning with Good Old-Fashioned Machine Learning",
  "authors": [
    "Moshe Sipper"
  ],
  "abstract": "We present a comprehensive, stacking-based framework for combining deep\nlearning with good old-fashioned machine learning, called Deep GOld. Our\nframework involves ensemble selection from 51 retrained pretrained deep\nnetworks as first-level models, and 10 machine-learning algorithms as\nsecond-level models. Enabled by today's state-of-the-art software tools and\nhardware platforms, Deep GOld delivers consistent improvement when tested on\nfour image-classification datasets: Fashion MNIST, CIFAR10, CIFAR100, and Tiny\nImageNet. Of 120 experiments, in all but 10 Deep GOld improved the original\nnetworks' performance.",
  "text": "arXiv:2207.03757v2  [cs.LG]  11 Nov 2022\nSpringer Nature 2021 LATEX template\nCombining Deep Learning with Good\nOld-Fashioned Machine Learning\nMoshe Sipper1*\n1*Department of Computer Science, Ben-Gurion University, Beer\nSheva, 84105, Israel.\nCorresponding author(s). E-mail(s): sipper@bgu.ac.il;\nAbstract\nWe present a comprehensive, stacking-based framework for combining\ndeep learning with good old-fashioned machine learning, called Deep\nGOld. Our framework involves ensemble selection from 51 retrained pre-\ntrained deep networks as ﬁrst-level models, and 10 machine-learning\nalgorithms as second-level models. Enabled by today’s state-of-the-art\nsoftware tools and hardware platforms, Deep GOld delivers consistent\nimprovement when tested on four image-classiﬁcation datasets: Fashion\nMNIST, CIFAR10, CIFAR100, and Tiny ImageNet. Of 120 experiments,\nin all but 10 Deep GOld improved the original networks’ performance.\nKeywords: machine learning, deep learning, image analysis, pattern\nrecognition\n1 Introduction\nThe rapid rise of artiﬁcial intelligence (AI) in recent years has been accom-\npanied (and enabled) by staggering advances both in software and hardware\ntechnologies. Tools such as PyTorch [1] for deep learning (DL), scikit-learn for\nmachine learning (ML) [2, 3], and graphics processing unit (GPU) hardware,\nall enable faster and better prototyping and deployment of AI systems than\nwas possible a mere half-decade ago.\nWhile deep learning has taken the world by storm, often—it would seem—\nat the expense of other computational paradigms, these (plentiful) latter\nare still quite alive and kicking. We propose herein to revisit stacking-based\n1\nSpringer Nature 2021 LATEX template\n2\nCombining Deep Learning with Good Old-Fashioned Machine Learning\nmodeling [4], but within a comprehensive framework enabled by modern\nstate-of-the-art software packages and hardware platforms.\nAs previously argued by [5, 6], signiﬁcant improvement can be attained\nby making use of models we are already in possession of anyway, through\nwhat they termed “conservation machine learning”: conserve models across\nruns, users, and experiments–—and make use of all of them. Herein, focusing\non image-classiﬁcation tasks, we ask whether, given a (possibly haphazard)\ncollection of deep neural networks (DNNs), can the tools at our disposal—\nspeciﬁcally, “good old-fashioned” ML algorithms, many of which have been\naround for quite some time—help improve prediction accuracy.\nTo wit, can we combine DL and ML in a manner that improves DL per-\nformance? We answer positively, with a major novelty being the use of most\nDL and ML models to date within a single, comprehensive framework.\nSection 2 discusses related previous work. Section 3 describes Deep GOld—\nDeep Learning and Good Old-Fashioned Machine Learning—which employs\n51 deep networks and 10 ML algorithms. Section 4 presents the results\nof 120 experiments over four image-classiﬁcation datasets: Fashion MNIST,\nCIFAR10, CIFAR100, and Tiny ImageNet. We end with a discussion in\nSection 5 and concluding remarks in Section 6.\n2 Previous Work\nThere are many works that involve some form or other of ensembling several\nmodels, and this section does not serve as a full review, but focuses on those\npapers found to be most relevant to our topic.\nIn an early work, [7] presented a technique called Addemup that uses a\ngenetic algorithm to search for an accurate and diverse set of trained networks.\nAddemup works by creating an initial population of networks, then evolving\nnew ones, keeping the set of networks that are as accurate as possible while\ndisagreeing with each other as much as possible. They tested these on three\nDNA datasets of about 1000 samples.\nA few years later, [8] presented an approach named GASEN (Genetic Algo-\nrithm based Selective ENsemble) to select some neural networks from a pool of\ncandidates, and assign weights to their contributions to the resultant ensemble.\nThe networks had one hidden layer with ﬁve hidden units. The eﬃcacy of this\nmethod was shown for regression and classiﬁcation problems over structural\n(non-image) datasets of a few thousand samples. Another work by [9] studied\nﬁnancial decision applications, wherein a neural-network ensemble prediction\nwas similarly reached by weighting the decision of each ensemble member.\nA more recent example (one of many) of straightforward ensembling is\ngiven in [10], who presented an ensemble neural-network model for real-time\nprediction of urban ﬂoods. Their ensemble approach used a number of artiﬁcial\nneural networks with identical topology, trained with diﬀerent initial weights.\nThe ﬁnal result of maximum water level was the ensemble mean. Ensemble\nsizes examined were 1, 5, and 10.\nSpringer Nature 2021 LATEX template\nCombining Deep Learning with Good Old-Fashioned Machine Learning\n3\nIn a similar vein, [11] trained multiple neural networks and combined their\noutputs using three combination strategies: simple average, weighted average,\nand what they termed a meta-learner, which applied a Bayesian regulation\nalgorithm to the network outputs. The application ﬁeld considered was real-\ntime production monitoring in the oil and gas industry, speciﬁcally, virtual\nﬂow meters that infer multiphase ﬂow rates from ancillary measurements, and\nare attractive and cost-eﬀective solutions to meet monitoring demands, reduce\noperational costs, and improve oil recovery eﬃciency.\n[12] trained ﬁve convolutional neural networks (CNNs) to detect ankle frac-\ntures in radiographic views. Model outputs were evaluated using both one and\nthree radiographic views. Ensembles were created from a combination of CNNs\nafter training. They implemented a simple voting method to consolidate the\noutput from the three views and ensemble of models.\n[13] presented a malware detection method called MalNet, which uses a\nstacking ensemble with two deep neural networks—CNN and LSTM—as ﬁrst-\nlevel learners, and logistic regression as a second-level learner.\n[14] examined neural-network ensemble classiﬁcation for lung cancer disease\ndiagnosis. They proposed an ensemble of Weight Optimized Neural Network\nwith Maximum Likelihood Boosting (WONN-MLB), which essentially seeks\nto ﬁnd optimal weights for a weighted (linear) majority vote. [15] applied a\nneural-network ensemble to intrusion detection, again using weighted majority\nvoting.\n[16] recently presented a cogent case for the use of XGBoost for tabu-\nlar data, demonstrating that it outperformed deep models. They also showed\nthat an ensemble comprising 4 deep models and XGBoost, predicting through\nweighted voting, worked best for the tabular datasets considered.\n[17] proposed an ensemble DNN for tumor detection in colorectal histology\nimages. The mechanism consists of weights that are derived from individual\nmodels. The weights are assigned to the ensemble DNN based on their metrics\nand the ensemble model is then trained. The model is again re-trained by\nfreezing all the layers, except for the fully connected and dense layers.\n[18] presented an ensemble DL method to detect retinal disorders.\nTheir method comprised three pretrained architectures—DenseNet, VGG16,\nInceptionV3—and a fourth Custom CNN of their own design. The individual\nresults obtained from the four architectures were then combined to form an\nensemble network that yielded superb performance over a dataset of retinal\nimages.\n[19] examined Deep Q-learning, presenting an ensemble approach that\nimproved stability during training, resulting in improved average performance.\nAs noted above, [5, 6] presented conservation machine learning, which con-\nserves models across runs, users, and experiments—and makes use of them.\nThey showed that signiﬁcant improvement could be attained by employing ML\nmodels already available anyway.\nSpringer Nature 2021 LATEX template\n4\nCombining Deep Learning with Good Old-Fashioned Machine Learning\nTable 1 Datasets.\nDataset\nImages\nClasses\nTraining\nTest\nFashion MNIST\n28x28 grayscale\n10\n60,000\n10,000\nCIFAR10\n32x32 color\n10\n50,000\n10,000\nCIFAR100\n32x32 color\n100\n50,000\n10,000\nTiny ImageNet\n64x64 color\n200\n100,000\n10,000\n3 Deep GOld: Algorithmic Setup\nStacking (or Stacked Generalization) [4] is an ensemble method that uses mul-\ntiple models to tackle classiﬁcation or regression problems. The main idea is\nto ﬁrst train diﬀerent models on the original problem. The outputs of these\nmodels are considered to be a ﬁrst level, which are then passed on to a second\nlevel to perform the ﬁnal prediction. The inputs to the second-level model are\nthus the outputs of the ﬁrst-level models.\nOur framework involves deep networks as ﬁrst-level models and ML meth-\nods as second-level models. For the former we used PyTorch, one of the top-two\nmost popular and performant deep-learning software tools [1]. The module\ntorchvision.models contains 59 deep-network models that were pretrained\non the large-scale (over 1 million images), 1000-class ImageNet dataset [20].\nOf the the 59 models we retained 51 (8 models proved somewhat unwieldy\nor evoked a “not implemented” error). Each of the models was ﬁrst retrained\nover the four datasets we experimented with in this paper: Fashion MNIST,\nCIFAR10, CIFAR100, and Tiny ImageNet. As seen in Table 1, these datasets\ncontain between 50,000 and 100,000 greyscale or color images in the training\nset, 10,000 images in the test set, with number of classes ranging between 10\n– 200. Retraining was necessary since the datasets contain images that diﬀer\nin size and number of classes from ImageNet.\nFor retraining, we replaced the last FC (fully connected) 1000-class layer\nwith a sequence of blocks comprising three layers: {FC, batchnorm, leaky\nReLU}, denoted FBL. The ﬁnal number of features of the original network\nwas reduced to the dataset’s number of classes through halving the number of\nnodes at each layer, starting with the closest power of 2. Consider en example:\nIf the original network ended with 600 features, and the dataset contains 100\nclasses, then our modiﬁed network’s ﬁnal layers comprised a 512-node, 3-layer\nFBL block (512 being the closest power of 2 to 600), followed by a 256-node\nFBL, followed by a 128-node FBL, and ending with the 100 classes. In addition,\nthe ﬁrst convolutional layer of the original network needed adjustment in some\ncases. The retraining phase is detailed in Algorithm 1.\nOnce Algorithm 1 is run for all four datasets, we are in possession of 51\ntrained models per dataset. We can now proceed to perform the two-level\nprediction, as detailed in Algorithm 2. Our interest herein was to study what\none can do with models one has at hand. Towards this end, we ﬁrst selected\nfrom the 51 retrained models three random ensembles of networks, of sizes 3, 7,\nand 11. Each network of an ensemble was then run over both the training and\ntest sets of the dataset in question (without any training—only feed-forward\nSpringer Nature 2021 LATEX template\nCombining Deep Learning with Good Old-Fashioned Machine Learning\n5\nAlgorithm 1 Retrain 51 pretrained models\nInput:\ndataset ←dataset to be used\npretrained ←{alexnet, densenet121, densenet161, densenet169, densenet201,\neﬃcientnet b0, eﬃcientnet b1, eﬃcientnet b2, eﬃcientnet b3, eﬃcientnet b4,\neﬃcientnet b5,\neﬃcientnet b6,\neﬃcientnet b7,\nmnasnet0 5,\nmnasnet1 0,\nmobilenet v2,\nmobilenet v3 large,\nmobilenet v3 small,\nregnet x 16gf,\nreg-\nnet x 1 6gf,\nregnet x 32gf,\nregnet x 3 2gf,\nregnet x 400mf,\nregnet x 800mf,\nregnet x 8gf, regnet y 16gf, regnet y 1 6gf, regnet y 32gf, regnet y 3 2gf, reg-\nnet y 400mf,\nregnet y 800mf,\nregnet y 8gf,\nresnet101,\nresnet152,\nresnet18,\nresnet34, resnet50, resnext101 32x8d, resnext50 32x4d, shuﬄenet v2 x0 5, shuf-\nﬂenet v2 x1 0, vgg11, vgg11 bn, vgg13, vgg13 bn, vgg16, vgg16 bn, vgg19,\nvgg19 bn, wide resnet101 2, wide resnet50 2}\n# Networks pretrained over\nImageNet dataset\nOutput:\nRetrained models and their test scores\n1: Load training set and test set of dataset\n2: for net ∈pretrained do\n3:\nReplace net ﬁnal layer, and possibly adjust ﬁrst convolutional layer\n4:\nTrain entire net for 20 epochs over training set # mini-batch size: 64 (8 for\nTiny ImageNet), optimizer: SGD\n5:\nSave trained net and test set score\n6: end for\noutput computation). These ﬁrst-level outputs were then concatenated to form\nan input dataset for the second level. For example, if the ensemble contains 7\nnetworks, and the dataset in question is CIFAR100, then the ﬁrst level creates\ntwo datasets: a training set with 50,000 samples and 701 features, and a test\nset with 10,000 samples and 701 features (701: 7 networks × 100 classes + 1\ntarget class).\nAfter the ﬁrst level produced output datasets, we passed these along to the\nsecond level, wherein we employed ten ML algorithms:\n1. sklearn.linear model.SGDClassifier: Linear classiﬁers with SGD train-\ning.\n2. sklearn.linear model.PassiveAggressiveClassifier: Passive Aggres-\nsive Classiﬁer [21].\n3. sklearn.linear model.RidgeClassifier: Classiﬁer using Ridge regres-\nsion.\n4. sklearn.linear model.LogisticRegression: Logistic Regression classi-\nﬁer.\n5. sklearn.neighbors.KNeighborsClassifier: Classiﬁer implementing the\nk-nearest neighbors vote.\n6. sklearn.ensemble.RandomForestClassifier: A random forest classiﬁer.\nSpringer Nature 2021 LATEX template\n6\nCombining Deep Learning with Good Old-Fashioned Machine Learning\nAlgorithm 2 Two-level prediction\nInput:\ndataset ←dataset to be used\nml algs ←{SGDClassiﬁer, PassiveAggressiveClassiﬁer, RidgeClassiﬁer, Logis-\nticRegression,\nKNeighborsClassiﬁer,\nRandomForestClassiﬁer,\nMLPClassiﬁer,\nXGBClassiﬁer, LGBMClassiﬁer, CatBoostClassiﬁer}\nOutput:\nTest scores for majority prediction, and for all ML algorithms\n# level 1: generate datasets from outputs of retrained networks\n1: for i ∈{3,7,11} do\n2:\nnetworks ←pick i networks at random from retrained networks of Algo-\nrithm 1\n3:\nfor net ∈networks do\n4:\nRun net over training set and test set\n5:\nAccumulate generated outputs along with (known) targets\n6:\nend for\n7:\nGenerate 2 datasets, respectively: train-i, test-i\n8: end for\n# level 2: run ML algorithms over datasets generated by level 1\n9: for i ∈{3,7,11} do\n10:\nfor alg ∈ml algs do\n11:\nLoad train-i, test-i\n12:\nRun alg to ﬁt model to train-i\n13:\nTest ﬁtted model on test-i\n14:\nend for\n15: end for\n7. sklearn.neural network.MLPClassifier: Multi-layer Perceptron classi-\nﬁer, with 5 hidden layers of size 64 neurons each.\n8. xgboost.XGBClassifier: XGBoost classiﬁer [22].\n9. lightgbm.LGBMClassifier: LightGBM classiﬁer [23].\n10. catboost.CatBoostClassifier: CatBoost classiﬁer [24].\n4 Results\nUnsurprisingly, we found signiﬁcant diﬀerences in the runtime of the level-2\nML algorithms (Algorithm 2). While some methods, such as RidgeClassiﬁer\nand KNeighborsClassiﬁer, were very fast, usually ﬁnishing within minutes,\nothers proved slow (notably, XGBClassiﬁer and CatBoostClassiﬁer, which took\nseveral hours). While the number of samples of the generated ML datasets\nfor the four problems studied is similar (identical to the original datasets—\nTable 1), the number of features diﬀers by an order of magnitude: with 10\nclasses for Fashion MNIST and CIFAR10, 100 classes for CIFAR100, and 200\nSpringer Nature 2021 LATEX template\nCombining Deep Learning with Good Old-Fashioned Machine Learning\n7\nTable 2 Hyperparameter value ranges and sets used by Optuna.\nAlgorithm\nParameter\nValues\nSGDClassifier\nalpha\n[1e-05, 1]\npenalty\n{‘l2’, ‘l1’, ‘elasticnet’}\nPassiveAggressiveClassifier\nC\n[1e-02, 10]\nﬁt intercept\n{True, False}\nshuﬄe\n{True, False}\nRidgeClassifier\nalpha\n[1e-3, 10]\nsolver\n{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse cg’, ‘sag’, ‘saga’}\nLogisticRegression\npenalty\n{‘l1’, ‘l2’}\nsolver\n{‘liblinear’, ‘saga’}\nKNeighborsClassifier\nweights\n{‘uniform’, ‘distance’}\nalgorithm\n{‘auto’, ‘ball tree’, ‘kd tree’, ‘brute’ }\nn neighbors\n[2, 20]\nRandomForestClassifier\nn estimators\n[10, 1000]\nmin weight fraction leaf\n[0, 0.5]\nmax features\n{‘auto’, ‘sqrt’, ‘log2’}\nMLPClassifier\nactivation\n{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}\nsolver\n{‘lbfgs’, ‘sgd’, ‘adam’}\nhidden layer sizes\n{(64,64), (64,64,64), (64,64,64,64), (64,64,64,64)}\nXGBClassifier\nn estimators\n[10, 1000]\nlearning rate\n[0.01, 0.2]\ngamma\n[0, 0.4]\nLGBMClassifier\nn estimators\n[10, 1000]\nlearning rate\n[0.01, 0.2]\nbagging fraction\n[0.5, 0.95]\nCatBoostClassifier\niterations\n[2, 10]\ndepth\n[2, 10]\nlearning rate\n[1e-2, 10]\nclasses for Tiny ImageNet, the latter two have 10 and 20 times more features\nthan the former two, respectively. Some ML methods are known to scale less\nwell with number of features.\nML runtimes for Fashion MNIST and CIFAR10 proved suﬃciently fast to\naﬀord the use of hyperparamater tuning. Towards this end we used Optuna, a\nstate-of-the-art, automatic, hyperparameter optimization software framework\n[25], which we previously used successfully [26, 27]. Optuna oﬀers a deﬁne-\nby-run style user API where one can dynamically construct the search space,\nand an eﬃcient sampling algorithm and pruning algorithm. Moreover, our\nexperience has shown it to be fairly easy to set up. Optuna formulates the\nhyperparameter optimization problem as a process of minimizing or maximiz-\ning an objective function given a set of hyperparameters as an input. The\nhyperparameter ranges and sets are given in Table 2. With CIFAR100 and\nTiny ImageNet we did not use Optuna, but rather ran the ML algorithms with\ntheir default values.\nTable 3 presents our results (we set a 10-hour limit on an ML algorithm’s\nrun of a row in the table, i.e., the level-2 loop of Algorithm 2.) A total of\n120 experiments were performed: 4 datasets × ensembles of size 3, 7, and\n11 × 10 complete runs per dataset. In each experiment we generated level-1\ndatasets and then executed the ML algorithms, as delineated in Algorithm 2.\nWe then compared three values: 1) the test score of the top network amongst\nthe random ensemble (known from Algorithm 1); 2) the test score of majority\nprediction, wherein the predicted class is determined through a majority vote\namongst the ensemble’s networks’ outputs; 3) the test score of the top ML\nmethod. The code is available at https://github.com/moshesipper.\nSpringer Nature 2021 LATEX template\n8\nCombining Deep Learning with Go\nTable 3 Results for ensembles of 3, 7, and 11 random networks. Accuracy scores shown are over test sets. Net: score of best network. Maj: score of\nmajority prediction. ML: score of best ML method. For the latter, the ML method producing the best score is given in parentheses. RG: Classiﬁer\nusing Ridge regression; KN: k-nearest neighbors classiﬁer; SG: Linear classiﬁer with SGD training; PA: Passive Aggressive classiﬁer; LR: Logistic\nregression; RF: Random Forest classiﬁer; MP: Multi-layer perceptron; LG: LightGBM; XG: XGBoost; CB: Catboost.\nDataset\n3 networks\n7 networks\n11 networks\nNet\nMaj\nML\nNet\nMaj\nML\nNet\nMaj\nML\nFashion-MNIST\n91.97%\n92.38%\n92.40% (KN)\n92.23%\n93.12%\n93.38% (KN)\n94.22%\n93.79%\n94.40% (RG)\n91.97%\n92.05%\n92.50% (KN)\n92.40%\n93.26%\n93.25% (KN)\n94.22%\n93.62%\n94.57% (KN)\n92.32%\n92.95%\n92.80% (KN)\n93.24%\n93.69%\n93.59% (KN)\n93.24%\n93.57%\n94.11% (RG)\n92.05%\n92.39%\n92.61% (KN)\n94.01%\n93.15%\n94.54% (RG)\n93.24%\n93.71%\n93.92% (KN)\n91.67%\n91.40%\n92.00% (KN)\n92.95%\n93.57%\n93.95% (KN)\n94.01%\n93.19%\n94.50% (KN)\n91.98%\n92.93%\n92.74% (KN)\n92.27%\n93.16%\n93.37% (KN)\n93.86%\n93.75%\n93.84% (KN)\n92.14%\n92.69%\n93.19% (KN)\n93.86%\n93.24%\n94.00% (RG)\n93.63%\n93.83%\n94.07% (KN)\n91.82%\n92.27%\n92.51% (RF)\n92.27%\n93.09%\n93.48% (KN)\n93.86%\n93.81%\n94.25% (KN)\n93.86%\n92.79%\n93.94% (KN)\n92.95%\n93.14%\n93.82% (RF)\n94.01%\n93.78%\n94.66% (RG)\n91.82%\n92.81%\n92.42% (RG)\n93.86%\n92.85%\n94.18% (KN)\n94.01%\n93.53%\n94.39% (RG)\nCIFAR10\n74.72%\n71.00%\n75.87% (KN)\n75.60%\n79.78%\n80.42% (KN)\n87.82%\n80.94%\n88.29% (RG)\n71.67%\n72.10%\n75.05% (RG)\n86.90%\n80.54%\n87.53% (RG)\n83.57%\n80.09%\n84.82% (RG)\n82.48%\n82.03%\n83.30% (KN)\n87.33%\n85.50%\n89.15% (RG)\n87.33%\n80.82%\n89.12% (RG)\n74.82%\n74.68%\n75.05% (KN)\n74.72%\n75.76%\n79.29% (KN)\n86.60%\n83.28%\n86.43% (RG)\n74.95%\n76.48%\n76.26% (KN)\n86.90%\n82.72%\n87.57% (RG)\n86.60%\n79.93%\n87.24% (RG)\n75.60%\n75.28%\n76.79% (KN)\n83.57%\n81.31%\n83.95% (KN)\n87.22%\n81.54%\n88.33% (RF)\n76.21%\n77.77%\n78.27% (KN)\n74.00%\n74.30%\n77.96% (KN)\n86.90%\n84.44%\n86.67% (RG)\n86.90%\n85.96%\n88.16% (LR)\n86.90%\n84.74%\n86.72% (RG)\n86.60%\n82.26%\n87.71% (RG)\n86.60%\n81.76%\n86.92% (KN)\n86.90%\n82.54%\n88.00% (RG)\n87.82%\n83.36%\n89.56% (RG)\n76.43%\n72.62%\n77.42% (SG)\n86.60%\n77.97%\n87.08% (RG)\n87.82%\n85.02%\n89.59% (RG)\nCIFAR100\n48.86%\n51.02%\n54.69% (KN)\n55.70%\n55.11%\n59.50% (RG)\n60.76%\n60.10%\n66.02% (RG)\n48.74%\n44.95%\n51.68% (KN)\n60.76%\n60.11%\n65.82% (LR)\n61.66%\n62.50%\n67.30% (RG)\n48.74%\n49.47%\n54.08% (KN)\n46.38%\n51.97%\n54.61% (KN)\n9.58%\n57.51%\n66.07% (LR)\n60.08%\n55.30%\n63.67% (SG)\n61.30%\n54.45%\n64.14% (RG)\n9.58%\n58.43%\n64.86% (RG)\n47.06%\n47.46%\n52.48% (RG)\n61.30%\n57.22%\n64.08% (RG)\n60.08%\n59.00%\n64.55% (RG)\n47.55%\n50.94%\n53.64% (RG)\n60.08%\n56.86%\n64.46% (RG)\n60.76%\n56.07%\n64.25% (LR)\n46.55%\n43.81%\n51.19% (KN)\n47.55%\n47.91%\n52.79% (SG)\n9.58%\n58.29%\n64.89% (RG)\n61.30%\n57.47%\n63.86% (RG)\n48.86%\n50.85%\n54.93% (RG)\n61.30%\n57.09%\n65.07% (RG)\n46.30%\n44.65%\n50.30% (SG)\n9.58%\n53.56%\n56.81% (KN)\n9.58%\n59.39%\n66.34% (RG)\n9.58%\n33.88%\n44.23% (SG)\n61.66%\n57.84%\n64.84% (SG)\n61.48%\n58.78%\n65.20% (LR)\nTiny ImageNet\n55.77%\n54.32%\n59.97% (RG)\n57.40%\n56.06%\n63.32% (RG)\n67.30%\n65.33%\n70.17% (RG)\n53.50%\n54.83%\n59.61% (LR)\n54.83%\n53.33%\n59.26% (RG)\n57.40%\n63.72%\n66.00% (RG)\n58.34%\n48.45%\n61.60% (RG)\n58.34%\n60.59%\n64.30% (RG)\n57.23%\n58.45%\n64.31% (RG)\n58.34%\n59.91%\n63.05% (RG)\n55.77%\n54.83%\n60.10% (RG)\n58.62%\n60.42%\n65.19% (RG)\n53.50%\n50.97%\n58.10% (RG)\n55.47%\n53.11%\n60.11% (RG)\n56.20%\n60.78%\n63.83% (RG)\n57.23%\n47.54%\n59.88% (RG)\n58.62%\n61.26%\n62.14% (RG)\n67.30%\n64.33%\n69.88% (RG)\n58.62%\n59.98%\n64.69% (RG)\n58.62%\n62.41%\n66.24% (RG)\n56.20%\n60.02%\n64.09% (RG)\n57.40%\n56.14%\n62.76% (LR)\n56.16%\n60.85%\n63.46% (RG)\n58.62%\n62.44%\n65.94% (RG)\n54.76%\n53.81%\n60.57% (RG)\n56.61%\n57.70%\n63.76% (RG)\n58.34%\n63.05%\n65.98% (RG)\n57.23%\n48.52%\n60.31% (RG)\n58.34%\n62.00%\n63.85% (RG)\n58.62%\n62.90%\n66.46% (RG)\nSpringer Nature 2021 LATEX template\nCombining Deep Learning with Good Old-Fashioned Machine Learning\n9\n5 Discussion\nAs observed in Table 3, of the total of 120 experiments, an ML algorithm won\nin all but 10 experiments (4 were won by the retrained network, and 6 by\nmajority prediction).\nWe note that classical algorithms, notably Ridge regression and k-nearest\nneighbors, worked best (they account for 104 of the wins). They are also fast,\nscalable, and amenable to quick hyperparameter tuning. If one wishes to focus\non a smaller batch of ML algorithms, these two seem like an excellent choice.\nAs noted in Section 1, we often ﬁnd ourselves in possession of a plethora\nof models, either collected by us through many experiments, or by others\n(witness our use of pretrained models herein). Beneﬁting from current state-\nof-the-art technology, Deep GOld leverages this wealth of models to attain\nbetter performance. One can of course tailor the framework to available deep\nnetworks and to a personal predilection for any ML algorithm(s).\n6 Concluding Remarks\nWe presented Deep GOld, a comprehensive, stacking-based framework for com-\nbining deep learning with machine learning. Our framework involves ensemble\nselection from 51 retrained pretrained deep networks as ﬁrst-level models, and\n10 machine-learning algorithms as second-level models. We demonstrated the\nunequivocal beneﬁts of the approach over four image-classiﬁcation datasets.\nWe suggest a number of paths for future research:\n• Further analysis of ML algorithms whose inputs are the outputs of deep\nnetworks. Do some ML methods inherently work better with such datasets?\n• Currently, the features for level 2 comprise only the level-1 outputs. We\nmight enhance this setup through automatic feature construction.\n• Train (or retrain) the level-1 networks alongside a level-2 ML model: 1) After\neach training epoch of the networks in the ensemble, generate a dataset\nfrom the network outputs; 2) a level-2 ML algorithm then ﬁts a model to\nthe level-1 dataset; 3) the ML model generates class probabilities, which are\nused to ascribe loss values to the networks-in-training.\nAcknowledgement\nI thank Raz Lapid for helpful discussions.\nCompliance with Ethical Standards\nDisclosure of potential conﬂicts of interest: M. Sipper declares that he has no\nconﬂict of interest.\nResearch involving human participants and/or animals: This article does not\ncontain any studies with human participants or animals performed by any of\nthe authors.\nSpringer Nature 2021 LATEX template\n10\nCombining Deep Learning with Good Old-Fashioned Machine Learning\nInformed consent: N/A.\nReferences\n[1] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G.,\nKilleen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: PyTorch: An\nimperative style, high-performance deep learning library. arXiv preprint\narXiv:1912.01703 (2019)\n[2] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B.,\nGrisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vander-\nplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay,\nE.: Scikit-learn: Machine learning in Python. Journal of Machine Learning\nResearch 12, 2825–2830 (2011)\n[3] Scikit-learn: Machine Learning in Python. https://scikit-learn.org/.\nAccessed: 2022-1-12 (2022)\n[4] Wolpert, D.H.: Stacked generalization. Neural Networks 5(2), 241–259\n(1992)\n[5] Sipper, M., Moore, J.H.: Conservation machine learning. BioData Mining\n13(1), 9 (2020)\n[6] Sipper, M., Moore, J.H.: Conservation machine learning: a case study of\nrandom forests. Nature Scientiﬁc Reports 11(1), 3629 (2021)\n[7] Opitz, D., Shavlik, J.: Generating accurate and diverse members of a\nneural-network ensemble. In: Touretzky, D., Mozer, M.C., Hasselmo, M.\n(eds.) Advances in Neural Information Processing Systems, vol. 8. MIT\nPress, Cambridge, MA (1996)\n[8] Zhou, Z.-H., Wu, J., Tang, W.: Ensembling neural networks: many could\nbe better than all. Artiﬁcial Intelligence 137(1-2), 239–263 (2002)\n[9] West, D., Dellana, S., Qian, J.: Neural network ensemble strategies for\nﬁnancial decision applications. Computers & Operations Research 32(10),\n2543–2559 (2005)\n[10] Berkhahn, S., Fuchs, L., Neuweiler, I.: An ensemble neural network model\nfor real-time prediction of urban ﬂoods. Journal of Hydrology 575, 743–\n754 (2019)\n[11] AL-Qutami, T.A., Ibrahim, R., Ismail, I., Ishak, M.A.: Virtual multi-\nphase ﬂow metering using diverse neural network ensemble and adaptive\nsimulated annealing. Expert Systems with Applications 93, 72–85 (2018)\nSpringer Nature 2021 LATEX template\nCombining Deep Learning with Good Old-Fashioned Machine Learning\n11\n[12] Kitamura, G., Chung, C.Y., Moore, B.E.: Ankle fracture detection utiliz-\ning a convolutional neural network ensemble implemented with a small\nsample, de novo training, and multiview incorporation. Journal of Digital\nImaging 32(4), 672–677 (2019)\n[13] Yan, J., Qi, Y., Rao, Q.: Detecting malware with an ensemble method\nbased on deep neural network. Security and Communication Networks\n2018 (2018)\n[14] ALzubi, J.A., Bharathikannan, B., Tanwar, S., Manikandan, R., Khanna,\nA., Thaventhiran, C.: Boosted neural network ensemble classiﬁcation\nfor lung cancer disease diagnosis. Applied Soft Computing 80, 579–591\n(2019)\n[15] Ludwig, S.A.: Applying a neural network ensemble to intrusion detection.\nJournal of Artiﬁcial Intelligence and Soft Computing Research 9 (2019)\n[16] Shwartz-Ziv, R., Armon, A.: Tabular data: Deep learning is not all you\nneed. Information Fusion 81, 84–90 (2022)\n[17] Ghosh, S., Bandyopadhyay, A., Sahay, S., Ghosh, R., Kundu, I., Santosh,\nK.C.: Colorectal histology tumor detection using ensemble deep neural\nnetwork. Engineering Applications of Artiﬁcial Intelligence 100, 104202\n(2021)\n[18] Paul, D., Tewari, A., Ghosh, S., Santosh, K.C.: OCTx: Ensembled deep\nlearning model to detect retinal disorders. In: 2020 IEEE 33rd Inter-\nnational Symposium on Computer-Based Medical Systems (CBMS), pp.\n526–531 (2020)\n[19] Elliott, D.L., Santosh, K.C., Anderson, C.: Gradient boosting in crowd\nensembles for Q-learning using weight sharing. International Journal of\nMachine Learning and Cybernetics 11(10), 2275–2287 (2020)\n[20] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.: ImageNet:\nA large-scale hierarchical image database. In: 2009 IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 248–255 (2009)\n[21] Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S., Singer, Y.: Online\npassive-aggressive algorithms. Journal of Machine Learning Research\n7(19), 551–585 (2006)\n[22] Chen, T., Guestrin, C.: XGBoost: A scalable tree boosting system. In:\nProceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining. KDD ’16, pp. 785–794, New York,\nNY, USA (2016)\nSpringer Nature 2021 LATEX template\n12\nCombining Deep Learning with Good Old-Fashioned Machine Learning\n[23] Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q.,\nLiu, T.-Y.: LightGBM: A highly eﬃcient gradient boosting decision\ntree. Advances in Neural Information Processing Systems 30, 3146–3154\n(2017)\n[24] Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A.V., Gulin, A.:\nCatBoost: Unbiased boosting with categorical features. arXiv preprint\narXiv:1706.09516 (2017)\n[25] Akiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M.: Optuna: A\nnext-generation hyperparameter optimization framework. In: Proceed-\nings of the 25th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining, pp. 2623–2631 (2019)\n[26] Sipper, M.: Neural networks with `a la carte selection of activation\nfunctions. SN Computer Science 2(470) (2021)\n[27] Sipper, M., Moore, J.H.: AddGBoost: A gradient boosting-style algorithm\nbased on strong learners. Machine Learning with Applications 7, 100243\n(2022)\n",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "published": "2022-07-08",
  "updated": "2022-11-11"
}