{
  "id": "http://arxiv.org/abs/2207.03851v2",
  "title": "Storehouse: a Reinforcement Learning Environment for Optimizing Warehouse Management",
  "authors": [
    "Julen Cestero",
    "Marco Quartulli",
    "Alberto Maria Metelli",
    "Marcello Restelli"
  ],
  "abstract": "Warehouse Management Systems have been evolving and improving thanks to new\nData Intelligence techniques. However, many current optimizations have been\napplied to specific cases or are in great need of manual interaction. Here is\nwhere Reinforcement Learning techniques come into play, providing\nautomatization and adaptability to current optimization policies. In this\npaper, we present Storehouse, a customizable environment that generalizes the\ndefinition of warehouse simulations for Reinforcement Learning. We also\nvalidate this environment against state-of-the-art reinforcement learning\nalgorithms and compare these results to human and random policies.",
  "text": "Storehouse: a Reinforcement Learning Environment\nfor Optimizing Warehouse Management\nJulen Cestero\nData Intelligence for Energy and Industrial Processes\nVicomtech\nSan Sebastian, Spain\n0000-0002-6670-6255\nMarco Quartulli\nData Intelligence for Energy and Industrial Processes\nVicomtech\nSan Sebastian, Spain\n0000-0001-5735-2072\nAlberto Maria Metelli\nDipartimento di Elettronica, Informazione e Bioingegneria\nPolitecnico di Milano\nMilano, Italy\n0000-0002-3424-5212\nMarcello Restelli\nDipartimento di Elettronica, Informazione e Bioingegneria\nPolitecnico di Milano\nMilano, Italy\n0000-0002-6322-1076\nAbstract—Warehouse Management Systems have been evolv-\ning and improving thanks to new Data Intelligence techniques.\nHowever, many current optimizations have been applied to\nspeciﬁc cases or are in great need of manual interaction. Here is\nwhere Reinforcement Learning techniques come into play, pro-\nviding automatization and adaptability to current optimization\npolicies. In this paper, we present Storehouse, a customizable envi-\nronment that generalizes the deﬁnition of warehouse simulations\nfor Reinforcement Learning. We also validate this environment\nagainst state-of-the-art reinforcement learning algorithms and\ncompare these results to human and random policies.\nI. INTRODUCTION\nAdvancements in Warehouse Management Systems can\nbe classiﬁed into different trends. On the one hand, some\nadvancements aim to enhance speciﬁc processes of the ware-\nhouses, improving the effectiveness and efﬁciency of the whole\nactivity. On the other hand, other advances focus on improving\nmore general processes, using Machine Learning methods\nsuch as Reinforcement Learning.\nReinforcement Learning (RL) [SB18] is a machine learning\napproach for training agents to solve sequential decision-\nmaking problems by interacting in a trial-and-error fashion\n[Ach18]. RL is an iterative approach in which an agent\nreceives rewards based on the actions taken, with the objective\nof maximizing the cumulative reward during its experience in\nthe environment. One of the advantages of RL is that, given\na well-deﬁned environment and reward, it can progressively\nadapt to new arrangements within the envlironment to continue\nmaximizing the cumulative reward. Furthermore, RL is not\nconstrained to a speciﬁc topology or dynamics of the environ-\nment, being able to generalize across different conﬁgurations.\nTherefore, RL turns out to be a potential solution to the\nproblem of managing warehouse storage systems.\nIn this paper, we investigate RL approaches to tackle\nwarehouse storage management. The main contributions of the\npaper can be summarized as follows:\n• we formulate warehouse storage management as a\nMarkov Decision Process (MDP) [Put94]. The modeling\nis general and can be customized with different ware-\nhouse topologies and generation processes (Section III);\n• we introduce Storehouse, a customizable environment\ndesigned to train RL agents to manage the storage of a\nwarehouse using a FIFO methodology. The environment\nis implemented using the gym interface [BCP+16] to\nease connection to popular RL libraries such as stable-\nbaselines3 [RHG+21] or RLlib [LLM+17]. Storehouse\nalso includes a customization method that uses a con-\nﬁguration ﬁle, where developers can try out different\nversions of the environment, including different grid\nsizes, material types, and timers (Section IV);\n• we provide an experimental evaluation of several RL al-\ngorithms, including DQN [MBM+16], PPO [SWD+17],\nA2C [MBM+16], against handcrafted human policies,\nshowing competitive performance (Section V).1\nII. CURRENT METHODOLOGIES\nWarehouse management systems follow different method-\nologies to optimize and balance their workload. Apart from\na few very recent facilities, such as Amazon’s Kiva system\n[YJL20], most modern warehouses follow classic approaches.\nHowever, with the rise of Industry 4.0, many warehouses have\nstarted looking for more data-driven solutions to optimize\ntheir long-established processes, which usually depend on the\nexperience of senior managers and ad hoc methods.\nThese methods are largely explored in the literature and\ncan be grouped into two trends: a classic trend that aims to\nenhance current systems via optimization (Section II-A), and a\nmore modern trend, which aims to propose methods, breaking\nwith existing ones (Section II-B).\n1The code of the environment is available in Github (https://github.com/\nJulenCestero/storehouse).\narXiv:2207.03851v2  [cs.LG]  21 Jul 2022\nA. Classic Trends\nThe classic trend leverages advancements in the area of\nsensors and automation, to enhance existing warehouse ad\nhoc methods using real data instead of human experience\n[ZHH+18]. These improvements rely on location sensors,\nproduct identiﬁcation tags, motion controllers. Using these\nmethodologies, several authors propose different optimization\nmethods. Liang et al. [LWZZ20] propose a wave-picking ware-\nhouse management that shows a 2% improvement compared\nwith a baseline item-batching method. Mao et al. [MXZ18]\nprovide a general study of the IT systems in a warehouse\nand propose a scheduling function aimed at the interconnec-\ntion of several agents in an industrial management process,\nwhich involves several warehouses, a ﬂeet of vehicles, and\nsome manufacturers and consumers. Zunic et al. [ZHH+18],\n[ZDH+18] propose two upgrades to the logistics of a particular\nwarehouse. The ﬁrst one consists in managing the distribution\npipeline of supplies and orders of the warehouse, while the\nsecond one uses an inner optimization of the ordering and\ndistribution of the material on the warehouse shelves. Both\nhave some promising results that improve the previous im-\nplementation close to 12%. Finally, Zunic et al. [ZHH+17]\nanalyze a material reallocation of the warehouse using an\nalgorithm that improves previous efﬁciency by 25 times.\nAll these methodologies share the same limitations: they\nare aimed at solving speciﬁc problems of certain warehouses,\nthey do not provide a general methodology that can be applied\nto any kind of management. These improvements can be\nuseful for micro management optimizations, but we aim to\nprovide more general management methodologies that can be\nadaptable to many kinds of warehouses.\nB. Modern Trends\nMore recently, approaches are based on machine learning\nand complex data analysis have been proposed to design\nalternatives to classic techniques, taking advantage of the\naforementioned new developments on warehouse digitaliza-\ntion. Among all of them, a relevant role is played by RL.\nThese methods can be classiﬁed according to which portion\nof the process they optimize: the macro actions, planning\nof higher order actions, like, for example, the ordering of\nnew upcoming materials to the warehouse; and the micro\nactions, which are based on the speciﬁc movements of the\nagents inside the warehouse. This latter kind of optimization\nfocuses on the path-ﬁnding process needed to ﬁnd the optimal\npath to fulﬁll the macro decisions. Bottani et al. [BMR+15]\ncarried out a very detailed review analyzing the most time-\nconsuming activity in a warehouse and providing an exhaus-\ntive research encompassing several state-of-the-art types of\nalgorithms, such as genetic algorithms, ant colony optimiza-\ntion, particle swarm optimization, simulated annealing, and\nmethods that use artiﬁcial neural networks. Estanjini et al.\n[ELP12] propose an RL-based method using A2C [MBM+16]\nto provide high-level actions in the decision-making process,\nsuch as moving items to/from reserve/pick-up/deport locations.\nThey divide their action space into these macro-activities,\naddress the curse of dimensionality [Bel61] by decreasing the\nnumber of action-state combinations, and apply the algorithm\nmanaging to improve performance by 20% compared to a\nsimilar heuristic previously in use. However, this approach is\nlimited to the speciﬁc layout of the studied warehouse. Dou\net al. [DCY15] proposed a mixed method involving Genetic\nAlgorithms (GAs) with RL. GAs are used for the planning\npart of the process, involving the macro-action management,\nwhile the RL algorithms are used for the micro-actions of the\nagents that involve instantaneous movement and path-ﬁnding\nthroughout the grid. Thus, the GA deﬁnes the goal of the agent,\nand the RL algorithm manages the movements to achieve that\ngoal. The results of this article provide insight that has been\nveriﬁed by other authors [LSK+19], [YJL20], [LJ21]. These\nworks compare RL algorithms such as Q-Learning [WD92] or\nDQN [MBM+16] with classic path-ﬁnding algorithms, such as\nA* [HNR68], and conclude that RL algorithms achieve at least\nthe same efﬁciency as A*, and enhance it in some cases, but\nall these works are limited to address the path-ﬁnding part of\nthe warehouse management problem, without addressing the\nmacro-planning problem using complex data analysis.\nThe main limitation of these approaches is that none of them\nproposed a generalized environment. A generalized environ-\nment can be used to train RL agents and, therefore, propose\na global method that sums up both the micro and the macro\nmanagement of a real warehouse.\nIII. THE STOREHOUSE PROBLEM\nAlthough advances in Industry 4.0 have provided new tools\nfor warehouses optimization, there is still a strong need to\nautomate these improvements and making them adaptable\nto new and unexpected scenarios. Nevertheless, much of\nthe management is highly dependent on human interaction\nand, in many cases, leading to ad hoc methods originating\nfrom cumulative human experience. In this sense, artiﬁcial\nintelligence approaches, and in particular RL, can greatly\nimprove warehouse management by devising approaches that\nare responsive and adaptable to changes inside the warehouse.\nIn this section, we ﬁrst provide an informal description of the\nStorehouse problem by discussing the metrics employed to\nevaluate the performance of the warehouse management (Sec-\ntion III-A) and the general warehouse features (Section III-B).\nThen, according to the previous description, we provide the\nenvironment speciﬁcation as MDP (Section III-C).\nThe goal of the Storehouse problem is to micro-manage a\nnumber of autonomous agents (e.g., fork lifts), providing them\nwith actions or movements to accomplish. Therefore, we seek\nto design a centralized control unit to manage the logistics of\nthe warehouse, optimizing the metrics discussed below.\nA. Metrics\nIn the literature, several metrics are available to evaluate the\nperformance of a warehouse management system. However, in\nthis paper, we mainly focus on three of them:\n(i) the number of material or orders delivered in a deﬁned\ntime period;\n(ii) the average age of the material in the warehouse over a\ntime period;\n(iii) the number of times the FIFO criterion has being\nviolated (the FIFO criterion is widely used in current\nwarehouse management systems).\nThe choice of these metrics is justiﬁed by the use case of\ninterest. In particular, we focus on the number of materials\nor orders delivered due to the nature of the real warehouse,\nwhich provides industrially manufactured products. If the\nproducts stored in the real warehouse were perishable goods,\nfor instance, other metrics would certainly be more important,\nas the age of the material.\nB. Warehouse Description\nIn this section, we provide a description of the abstraction\nof the warehouse functioning that we consider in this paper.\n1) Warehouse Layout: The warehouse layout is a rectan-\ngular grid with a hollow crown that surrounds the storage\nroom to ease handling of forklifts. An example of a squared\nlayout is shown in Figure 1. The warehouse contains two entry\npoints (green) and two delivery points (red), where items are\nintroduced and delivered to clients. These points are located\nin an outer crown where no items can be placed with the\nexception of the delivery points, which immediately consume\nthe items placed therein. Therefore, the items can be placed\nanywhere within the internal storage grid. We assume the size\nof the material is equal to one grid tile. Items placed in the\nstorage can be picked if the agent is able to get to that cell from\nadjacent horizontal or vertical cells. If a cell is unreachable,\nthis cell is considered a restricted cell, where no items can be\nplaced or picked.\n2) Item Generation and Consumption: The item generation\nprocess starts at the delivery points. By emulating on-demand\nmaterial generation systems, they generate the orders, using\nrandom timers, which the system has to fulﬁll. The entry points\nstart generating the required items, placing them into waiting\nqueues with timers that simulate when the different materials\nof the order are ready to be either introduced into the grid\nor delivered directly to the delivery points. In our setting,\ntimers are Poisson processes with a conﬁgurable parameter\nλ [Wol82]. In summary, the delivery points generate a new\norder with a random distribution, and a random timer with\nthe waiting time until ready to collect the order; this order is\npassed to the entry points and they start creating the material\nin a queue, with random timers simulating it. When the timers\nof the items of the input queues ﬁnish, the items are ready to\nbe picked up and moved to the storage or delivered directly\nto the delivery points, and they are collected from the input\npoints in the order they were generated.\n3) Material Types: Different types of material are consid-\nered to mimic actual production and delivery to clients, named\nin alphabetical order (e.g., the ﬁrst material type is ‘A’, the\nsecond ‘B’, and so on). The generation is governed by different\nstochastic processes (e.g., Poisson processes with different λs),\nand the order type is also randomly deﬁned by the delivery\npoints when a new order is created.\nFigure 1: Layout of the warehouse. The blue tiles correspond\nto the area that is always free of material. The entry points\nare green, while the delivery points are red.\nC. MDP Speciﬁcation\nBased on the informal description presented above, in\nthis section, we formally deﬁne all the elements needed for\nmodeling the problem as an MDP [Put94].\n1) States: The state of the environment is composed of\nseveral grids, each storing a feature of all the cells in the\nwarehouse, easing the training by making it compatible with\nimage processing methods. Formally, the state is deﬁned as\na 3-dimensional tensor s ∈Rr×c×d, where r, c ∈N are\nthe number of rows and columns of the warehouse grid\nrespectively and d ∈N is the number of feature matrices of\nthe state. Thus, s can be regarded as a list of r × c feature\nmatrices Di. We consider d = 6 + m feature matrices, where\nm is the number of material types considered:2\n• Box grid D1: represents the location of the different boxes\nwithin the storage. Each box is represented by an integer\ncorresponding to the type of material it is, mapping each\ntype (‘A’, ‘B’, ...) to integers (1, 2, ...). If there is no box\nin that grid position, the value is 0.\n• Age grid D2: represents the age of the box, which is\nbounded between 0 and 1000 (steps).\n• Restricted grid D3: collects the different restricted cells\nin the environment.\n• Agent grid D4: used to locate the agent within the grid\nand to know if the agent has picked an object or not. The\ncell in the matrix in which the agent is placed will have\na value of 255 if the agent has not picked an object, and\n128 otherwise. The other cells will have a value of 0.\n• Agent material grid D5: the cell where the agent is\nlocated is marked with the type of the material that it\nis carrying is represented. If the agent has no items, the\nvalue of all cells is 0.\n• Entry point grid D6: indicates the status of the entry\npoints. When an item is ready to be picked up, the\nposition of the matrix that corresponds to the position of\nthe corresponding entry point changes its value to 255.\n2For compliance with convolutional neural networks, all matrices are treated\nas images and their values are normalized between 0 and 255.\n• Out point grids: D7, . . . , D6+m: the number of matrices\nvaries with the number of types that the environment\ncan generate, each matrix corresponding to a different\ntype. Depending on the type of material that the delivery\npoints are expecting, the value of the cell at the position\ncorresponding to the delivery points changes to 255 in\nthe corresponding matrix.\n2) Actions: The available actions take the form of coordi-\nnates of the grid a = (i, j) ∈{1, . . . , r} × {1, . . . , c}. Any\naction corresponds to the desired location of the agent. The\neffect of the action depends on the status of the target cell:\n• if the agent moves to the same location where a box is\nlocated, it will automatically pick it up;\n• if the agent has an object and it moves to an empty valid\ncell, it will automatically drop the item there.\nHowever, some actions are considered invalid. For instance,\nthe agent cannot stack items or deposit an item in the outer\nring of the environment, unless it is depositing it at an open\ndelivery point. If an invalid action is performed, the agent\nwill receive a negative reward and it will stay in the same\ncell as before, even though the environment will advance one\nstep further without registering any movement from the agent.\nGiven a state s, these actions belong to the set Inv(s).\n3) Reward: We assume that the cost of a movement is\nconstant (one step), regardless of the distance involved in the\nmovement. These actions result in a reward that values the\nquality of the outcome of the performed action:\n• if the agent performs an invalid action, the reward will\nalways be −1, with the objective of preventing the agent\nfrom performing an invalid action;\n• when the agent has nothing useful to do, e.g., when\nthe grid is empty and there are no incoming orders, it\nwill receive a reward equal to 0, preventing actions from\nhaving a signiﬁcant effect in the training;\n• if the agent has other signiﬁcant actions to take (e.g.,\ndelivering ready items) and does not perform them, it\nwill receive a reward of −0.9 to discourage idle actions;\n• if the agent manages to deliver an item to the delivery\npoints, it will receive a less negative reward, to encourage\nit to this behavior. This reward is bounded between\n[−0.5, 0] and it will be closer to 0 the older the delivered\nbox is, with the maximum reward being 0 if the box is\nthe oldest available and the agent is therefore behaving\naccording to a FIFO criterion. The reward will vary\nlinearly between the upper and lower bounds, depending\non the age difference between the delivered box and the\nyoungest box in the environment and, additionally, this\nage difference is also bounded between 0 and 100 steps,\nreturning the highest and the lowest rewards respectively.\nWe can formalize the deﬁnition of the rewards for every\nstate-action pair (s, a):\nR(s, a) =\n\n\n\n\n\n\n\n\n\n−1\na ∈Inv(s)\n−0.9\na ∈N(s)\nmin{max{age,0},1000}\n1000\n· 255\na ∈D(s)\n0\notherwise\n,\nwhere:\n• Inv(s) is the set of invalid actions in state s;\n• D(s) is the set of valid actions that deliver items to the\ndelivery points in state s;\n• N(s), is the set of idle actions in state s (i.e., the agent\nmoving from an empty cell to another with no items, with\nwaiting items at the entry points).\nThere is no special reward for items collected from entry\npoints. If there are items to be delivered, the agent receives\n−0.9 reward for avoiding idleness. However, if there are no\nmore effective actions than getting new items, the reward is\n0.\nIn this setting, the goal of an RL agent is to ﬁnd the policy\nπ, i.e., a mapping from states to actions, that maximizes the\nexpected discounted sum of the rewards [SB18]:\nJ(π) = Eπ\n\"T −1\nX\nt=0\nγtR(st, at)\n#\n,\nwhere γ ∈(0, 1) is the discount factor.\nIV. ALGORITHMIC SOLUTION\nTo solve the Storehouse optimization problem, we use RL-\nbased strategies and algorithms. Our goal is to show that RL\nsolutions are competitive with traditional human-knowledge-\nbased strategies.\nA. Reinforcement Learning Policies\nWe tested three state-of-the-art RL algorithms in the Store-\nhouse environment: Deep Q-Network (DQN) [MBM+16],\nAsynchronous Actor Critic (A2C) [MBM+16], and Proximal\nPolicy Optimization (PPO) [SWD+17]. In addition, we devel-\noped a customized version of DQN, in which we ﬁlter out\nthe invalid actions thus speeding up the training process due\nto the decreased number of actions available in each step. We\ncalled this variant Valid-Action Mask (VAM).\nB. Hyperparameter Deﬁnition\nWe also studied the effect of hyperparameter optimiza-\ntion, compared to using the default values deﬁned in stable-\nbaselines3, the library we used for the RL training. In this\nstudy, we sought strategies based on Design of Experiments\n(DoE) [Box80] to optimize the training of the RL policies.\nFor this purpose, we quantized the continuous hyperparameters\nand applied a pairwise combination, used in all-pairs testing\n[Cze06], to reduce the space of possible combinations. This\noptimization allows increasing the speed of convergence to the\noptimal combination of hyperparameters.\nWe considered four main hyperparameters, from the ones\nthat stable-baselines3 allows us to tune:\n• learning rate (α): choosing the learning rate is challeng-\ning as too small a value may result in a long training\nprocess that could get stuck, whereas too large a value\nmay result in learning a sub-optimal set of weights too\nfast or an unstable training process;\n• discount factor (γ ∈(0, 1)): it quantiﬁes the importance\nwe give to future rewards. If γ is closer to 0, the agent will\ntend to consider only immediate rewards. If γ is closer to\n1, the agent will give similar weights to immediate and\nfuture rewards, willing to delay the reward;\n• Polyak coefﬁcient (τ\n∈[0, 1]): it is the soft update\ncoefﬁcient, being 1 a hard update. It averages previous\nvalues of the neural network with the most recent values;\n• exploration fraction: it is the fractional value for the\nexploration vs. exploitation rate, in this case, an ε-\ngreedy strategy. It represents the percentage of steps of\nthe deﬁned maximum number of training steps that the\nalgorithm needs to reach the minimum value of ϵ. For\ninstance, a value of 0.1 means that the value of ϵ will\nvary linearly from its maximum value to its minimum in\nthe initial 10% of the deﬁned maximum number of steps.\n• Value Function coefﬁcient (vf coef ∈[0, 1]): this coefﬁ-\ncient controls the effect of the value function loss in the\ncalculation of the training loss;\n• clip (϶ ∈[0, 1]): the standard PPO algorithm has a clipped\nobjective function and this hyperparameter is used to\ndeﬁne the range in which the probability ratio term is\nclipped, meaning that the objective function takes the\nminimum between the original ratio and the clipped ratio,\nbeing the clip range [1−϶, 1+ ϶].\nV. RESULTS\nIn this section, we present the results of the methods\nproposed in the previous sections in terms of the RL training\nscore, which relates to the discounted cumulative rewards, and\nof the different metrics for each deﬁned policy. Furthermore,\nthe effect of hyperparameter tuning is studied for all the RL\npolicies, but a deeper analysis is shown for the DQN policy.\nA. Baseline Policies\nWe deﬁned two human policies, i.e., policies deﬁned pro-\ngrammatically that implement expert strategies applicable in\nwarehouses. The ﬁrst one, named Initial Human Policy (IHP),\nprioritizes retrieving the material from the entry point queues,\nwhereas the second one, named Enhanced Human Policy\n(EHP), prefers to deliver the available material to the delivery\npoints. Both policies work in such a way that, when a new\nobject is generated at the entry points, this is immediately\ntaken when the warehouse is empty. IHP will always store\nthese items in the warehouse, but EHP will store the items only\nif the agent is unable to deliver them directly. When the storage\ncontains items, IHP will act likewise: prioritizing stocking\nitems from the entry points and delivering available items\nusing a FIFO order, when no items are waiting in the entry\npoint queues. On the other hand, EHP does not prioritize the\nabsence of objects at the entry points. If the agent can deliver\nthe items (thus, the delivery points are open for some items)\nand these items are in the storage, it will prioritize the delivery\nof items following the FIFO order and, therefore, delivering\nthe oldest available items. This means that the priority is to\ndeliver the items already stored, try to empty the warehouse,\nand, then, deliver or store the new incoming material from\nthe entry point queues. Besides that, the delivery points do\nnot expect any item, this agent will aim to store the incoming\nitems into the warehouse storage. Furthermore, restricted cells\ncan be created if the incoming ﬂow of new items is greater than\nthe output ﬂow of delivered items. If so, these two policies get\nthe oldest available material in the grid.\nWe also consider a random policy, which chooses uniformly\namong all actions deﬁned in the action space of the environ-\nment at each step. We expect this policy to be the worst, so,\nwe want to use it as a baseline from which the RL policies are\nexpected to improve. The human policies are also considered\nbaselines and we aim for RL policies to outperform the human\npolicies if possible.\nB. Experimental Setting\nAll results use the 6 × 6 conﬁguration of the environment\n(r = c = 6), where entry points in the upper corners and\ndelivery points in the lower corners. We consider two types of\nmaterial, ‘A’ and ‘B’, with a Poisson generation process with\nparameters λ = 5 and λ = 10 respectively for each item and an\norder delivery parameter of λ = 30 and λ = 50 respectively.\nEach order has a random number of items between 2 and 6,\nand a new order is randomly generated with λ = 25.\nThe results were obtained by simulating agents with differ-\nent policies in the Storehouse environment, using a maximum\nnumber of steps of 1000 per episode, in 3000 episodes using\na GPU Nvidia Tesla V100 SXM2 32GB, 80 CPUs Intel Xeon\nGold 6230 @2.16GHz and 755GB of RAM, which results\ninto a duration of nearly 3.5 hours per trained policy. Each\npolicy is trained 10 times and, in all ﬁgures, the results show\nthe average of the different runs with a darker line, and the\nmaximum and minimum over the runs with a shaded area.\nC. Results without Hyperparameter Optimization\nFigure 2 shows the evolution of the average return in the\ntraining process per episode for the different deﬁned policies.\nThe RL policies change throughout the episodes, even though\nsome of them stay quite static, but we consider both the\nhuman policies and the random policy as constants because\nthey do not vary with the episodes. All the policies in this\nplot have been trained using the default parameters of their\ncorresponding algorithms in stable-baslines3. It is curious that\nthe policies based on DQN evolve successfully, enhancing\ntheir result until achieving near-human performance, but the\nA2C and PPO policies are not able to learn how to act in\nthe environment. However, it is not surprising that the VAM\npolicy outperforms the DQN policy, even enhancing the initial\nhuman policy, since it is an improved version of the default\nDQN algorithms where the invalid actions are suppressed.\n0\n500\n1000\n1500\n2000\n2500\n3000\nEpisode\n900\n800\n700\n600\n500\n400\n300\nScore\nPPO\nA2C\nDQN\nVAM\nIHP\nEHP\nRandom\nFigure 2: Score as a function of the number of collected\nepisodes for the tested RL algorithms and baseline policies\nwithout hyperparameter optimization over 10 runs (PPO line\nis behind the A2C line). The shaded area corresponds to the\narea between the maximum and the minimum value between\nthe different runs.\n800\n600\n400\n200\n0\nRandom\nPPO\nA2C\nDQN\nIHP\nVAM\nEHP\nScore\n0\n50\n100\n150\nA2C\nPPO\nRandom\nIHP\nDQN\nVAM\nEHP\nDelivered Boxes\n0\n50\n100\n150\n200\nRandom\nIHP\nDQN\nVAM\nEHP\nPPO\nA2C\nMean box ages\n0%\n20%\n40%\n60%\nRandom\nDQN\nVAM\nIHP\nEHP\nA2C\nPPO\nFIFO violation\nFigure 3: Environment metric comparison for the deﬁned\npolicies.\nFigure 3 displays other metrics of interest for the envi-\nronment. These results are obtained by averaging the last\n100 episodes, assuming stability in the results. In terms of\nDelivered Boxes, the results are similar to the score, but\nDQN manages to deliver more boxes than IHP. This might\nbe because DQN was able to learn strategies that can improve\nits productivity in terms of delivered boxes, strategies that we\napplied to build the EHP, in fact. However, it scores worse than\nIHP because, even though it manages to deliver more boxes, it\nperforms other sub-optimal actions. On the other hand, neither\nPPO nor A2C managed to deliver any boxes, which is evident\nby looking at their score. The consequence of this fact is that\n0\n500\n1000\n1500\n2000\n2500\n3000\nEpisode\n900\n800\n700\n600\n500\n400\n300\nScore\n= 0.8192, = 1, = 0.99, explFrac = 0.0125\n= 0.8192, = 0.1, = 0.99, explFrac = 0.1\n= 0.8192, = 1, = 0.2302, explFrac = 0.8\n= 1.5625e\n06, = 1, = 0.99, explFrac = 0.0125\n= 1.5625e\n06, = 1, = 0.2302, explFrac = 0.1\n= 1.5625e\n06, = 0.1, = 0.99, explFrac = 0.8\n= 0.0001, = 1, = 0.99, explFrac = 0.1\n= 0.0001, = 1, = 0.99, explFrac = 0.8\n= 0.0001, = 0.1, = 0.2302, explFrac = 0.0125\nIHP\nEHP\nRandom\nFigure 4: Hyperparameter tuning process for the DQN policy\nover 4 runs.\nthe other metrics (FIFO violation and Mean Box ages) show\napparently good results, but they are not feasible because these\nmetrics are not valid for the low number of delivered material.\nThus, moving on to these metrics, we have the Mean Box\nAges, in which the DQN policies get very close to the EHP,\nenhancing the IHP. This is plausible as DQN does not have the\nrestrictions that IHP has, and it is capable of delivering items\ndirectly from the entry points. Furthermore, EHP does not have\nthese restrictions either, so it is able to further reduce the mean\nage. In addition, the FIFO Violation metric shows that DQN is\nable to build those solid numbers by not delivering the oldest\nitem 40% of the time, while the VAM policy enhances this\nmetric, achieving a violation rate of 30%. As a side note, the\nsmall percentage of FIFO violation by the IHP policy is due to\nthe fact that, in this policy, the occupancy of the warehouse is\nnearly at its maximum and, as a consequence, restricted cells\nstart to appear. Consequently, in a small number of cases, the\noldest boxes of the warehouses are in a restricted position and,\ntherefore, the IHP agent must deliver other boxes, violating the\nFIFO method.\nD. Hyperparameter Optimization\nThe above results show that, even though some RL policies\nachieve near-human performance, others have ample room for\nimprovement. This is due to the fact that, for the training\nof these policies, we used the default hyperparameters from\nstable-baselines3. In this section, we performed the hyperpa-\nrameter optimization as described in Section IV-B.\nα\nγ\nvfCoef\n϶\nA2C\n1.5625e-06\n0.99\n0.1\n-\n1.5625e-06\n0.2302\n0.9\n-\n1.5625e-06\n0.2302\n0.5\n-\n0.8192\n0.99\n0.9\n-\n0.0001\n0.2302\n0.9\n-\n0.8192\n0.2302\n0.5\n-\n0.0001\n0.99\n0.5\n-\n0.8192\n0.2302\n0.1\n-\n0.0001\n0.2302\n0.1\n-\nPPO\n1.5625e-06\n0.99\n0.1\n0.08\n1.5625e-06\n0.99\n0.9\n0.6\n0.0001\n0.99\n0.9\n0.2\n0.8192\n0.2302\n0.9\n0.08\n0.8192\n0.99\n0.1\n0.6\n1.5625e-06\n0.99\n0.5\n0.6\n0.8192\n0.99\n0.5\n0.2\n0.0001\n0.2302\n0.1\n0.08\n1.5625e-06\n0.2302\n0.1\n0.2\n0.0001\n0.2302\n0.5\n0.6\n1.5625e-06\n0.99\n0.5\n0.08\nTable I: Studied hyperparameter combinations for A2C and\nPPO\nFigure 4 shows the result of the tuning process for DQN. We\nobserve that some hyperparameter combinations cannot even\ncompete with the random policy, while other combinations can\neven compete with the IHP, although being quite far away from\nthe EHP yet. Finally, we have a combination that enhances\nthe IHP and approaches the EHP, the yellow line with the\nhyperparameter combination of α = 0.001, τ = 0.1, γ = 0.23\nand explFrac = 0.0125. As a reference, the pink line (with α =\n0.001, τ = 1, γ = 0.99 and explFrac = 0.1) corresponds to the\ndefault hyperparameters of stable-baselines3. We can conclude\nthat these original hyperparameters were a good combination,\nbut we were able to enhance performance by choosing a more\nsuitable combination.\nWe performed a similar analysis with the other RL algo-\nrithms, namely A2C and PPO. Table I shows the combinations\nand we highlighted the best combinations for each case.\nE. Results with Hyperparameter Optimization\nWe re-trained the RL policies using the optimal hyperpa-\nrameter combination of the previous section with DQN, VAM,\nA2C, and PPO.\nFigure 5 depicts the result. We note that the results are\ndrastically enhanced, showing that policies that previously\nreached near-random results now perform better than the DQN\npolicy and even the IHP, getting closer to the VAM policy and\nto the EHP. Even the VAM policy improves its convergence\nspeed, although its score is quite difﬁcult to enhance. We can\nconclude that, optimizing the hyperparameters, some results\ncan be signiﬁcantly improved, as now all the RL policies are\nbetter than the IHP, which at ﬁrst was only surpassed by the\nresults of the VAM policy.\nAs for the metrics, we can observe that the Score metric\nhas improved, since all the RL metrics are better than the IHP,\nalthough not as good as the EHP. In terms of Delivered Boxes,\neach RL policy is close to the maximum score achieved by\n0\n500\n1000\n1500\n2000\n2500\n3000\nEpisode\n900\n800\n700\n600\n500\n400\n300\nScore\nDQN\nA2C\nPPO\nVAM\nIHP\nEHP\nRandom\nFigure 5: Score as a function of the number of collected\nepisodes for the tested RL algorithms and baseline policies\nafter hyperparameter optimization over 10 runs).\n800\n600\n400\n200\n0\nRandom\nIHP\nDQN\nA2C\nPPO\nVAM\nEHP\nScore\n0\n50\n100\n150\nRandom\nIHP\nA2C\nPPO\nDQN\nVAM\nEHP\nDelivered Boxes\n0\n50\n100\n150\n200\nRandom\nIHP\nA2C\nVAM\nEHP\nPPO\nDQN\nMean box ages\n0%\n20%\n40%\n60%\nRandom\nA2C\nVAM\nPPO\nDQN\nIHP\nEHP\nFIFO violation\nFigure 6: Metric comparison for the deﬁned policies with\noptimized hyperparameters.\nthe EHP, with small divergences due to the stochasticity of the\nenvironment. The Mean Box Age metric results are also quite\nenhanced and it can be seen that some RL policies minimize\nthe box ages, even more than the EHP, probably due to the\nfact that those policies are allowed to violate the FIFO criterion\nwhile the human policies are not.\nVI. DISCUSSION\nIn the previous section, we showed promising results. How-\never, the Storehouse environment shows some potential limi-\ntations, especially when it comes to scalability. The previous\nresults were given on the 6 × 6 conﬁguration, and that leads\nto a moderately large number of possible states. However, real\nwarehouses tend to be much larger and, thus, the state space\nwould increase signiﬁcantly. This could be troublesome as the\ncomplexity of the problem inherently becomes greater (for\nexample, a 100×100 grid with 25 different types of materials).\nThis training problem impacts the hyperparameter tuning\nprocess. This process, although it was drastically reduced by\nusing the DoE method, is slow, since several runs must be\nperformed to ﬁnd the optimal combinations, and with larger\nstate spaces, this training would be even slower.\nThe impact of dealing with larger state spaces is unclear. We\nwould, most likely, have similar results in terms of scoring,\nbut the training session would last a lot longer. With the\n6 × 6 environment, the training took around 3.5 hours, we\nsuspect this number may be signiﬁcantly increased. However,\nthe VAM policy would probably be able to maintain its\nconvergence speed, since the action space varies depending\non the available actions. Nevertheless, the use of more case-\nspeciﬁc algorithms could overcome this limitation and, thus,\nlessen the impact of this limitation. An example of possible al-\ngorithms that would be immune to the curse of dimensionality\nis AlphaZero [SHS+18] or adding the Hindsight Experience\nReplay (HER) technique [AWR+17] to an already stated\nalgorithm. Regarding hyperparameter tuning, the optimization\nof these parameters would help to improve the convergence\nspeed, but we already stated that the time required for this\nprocess is quite high. However, this tuning process is the last\nstep in an RL training, so the impact of this limitation on the\ngeneral scope of the project is minimal.\nVII. CONCLUSIONS\nIn this contribution, we covered the deﬁnition and im-\nplementation of a warehouse management solution for the\nefﬁciency problem in the logistics industry. We described the\ncurrent methodologies used to solve this problem and found an\nimprovement opportunity and proposed an RL-based solution\nthat allows users to simulate a customizable warehouse that\ncan be used to represent real warehouses. We also proved the\nfeasibility of this environment by training several policies, and,\nafter comparing the results with human-deﬁned policies and\na random policy, we saw that these policies are able to solve\nthe problem with near-optimal performance.\nTherefore, we can conclude that the Storehouse environment\nis a novel tool that can be used for training policies of\nwarehouse management systems with customizable layouts\nand settings. This environment uses the gym interface, so it\nis standardized to be used with almost any RL algorithm. We\nalso saw that, by training RL policies based on State-of-the-\nArt algorithms, this environment is robust and these policies\nare able to converge to promising results, and near-optimal\nperformance.\nIn future scenarios, we plan to use this environment to\ntest planning approaches and algorithms like AlphaZero. We\nexpect that these approaches would maintain good results even\nwith larger state spaces.\nREFERENCES\n[Ach18]\nJoshua Achiam. Spinning Up in Deep Reinforcement Learning,\n2018.\n[AWR+17]\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider,\nRachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter\nAbbeel, and Wojciech Zaremba. Hindsight experience replay.\nNIPS, pages 5049–5059, 7 2017.\n[BCP+16]\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas\nSchneider, John Schulman, Jie Tang, and Wojciech Zaremba.\nOpenai gym. arXiv preprint arXiv:1606.01540, 2016.\n[Bel61]\nRichard E. Bellman.\nAdaptive control processes.\nAdaptive\nControl Processes, 12 1961.\n[BMR+15]\nEleonora Bottani, Roberto Montanari, Marta Rinaldi, Giuseppe\nVignali, E Bottani, ´A R Montanari, ´A G Vignali, R Montanari,\nG Vignali, and M Rinaldi. Intelligent algorithms for warehouse\nmanagement.\nIntelligent Systems Reference Library, 87:645–\n667, 2015.\n[Box80]\nJoan Fisher Box. R. a. ﬁsher and the design of experiments,\n1922-1926. The American Statistician, 34(1):1–7, 1980.\n[Cze06]\nJacek Czerwonka. Pairwise testing in real world. In 24th Paciﬁc\nNorthwest Software Quality Conference, volume 200, 2006.\n[DCY15]\nJiajia Dou, Chunlin Chen, and Pei Yang. Genetic scheduling\nand reinforcement learning in multirobot systems for intelligent\nwarehouses.\nMathematical Problems in Engineering, 2015,\n2015.\n[ELP12]\nReza Moazzez Estanjini, Keyong Li, and Ioannis Ch Pascha-\nlidis. A least squares temporal difference actor–critic algorithm\nwith applications to warehouse management. Naval Research\nLogistics (NRL), 59:197–211, 4 2012.\n[HNR68]\nPeter E. Hart, Nils J. Nilsson, and Bertram Raphael. A formal\nbasis for the heuristic determination of minimum cost paths.\nIEEE Transactions on Systems Science and Cybernetics, 4:100–\n107, 1968.\n[LJ21]\nHyeoksoo Lee and Jongpil Jeong. Mobile robot path optimiza-\ntion technique based on reinforcement learning algorithm in\nwarehouse environment. Applied Sciences 2021, Vol. 11, Page\n1209, 11:1209, 1 2021.\n[LLM+17]\nEric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy\nFox, Ken Goldberg, Joseph E. Gonzalez, Michael I. Jordan, and\nIon Stoica.\nRllib: Abstractions for distributed reinforcement\nlearning. 35th International Conference on Machine Learning,\nICML 2018, 7:4768–4780, 12 2017.\n[LSK+19]\nMaojia Patrick Li, Prashant Sankaran, Michael E. Kuhl, Amlan\nGanguly, Andres Kwasinski, and Raymond Ptucha. Simulation\nanalysis of a deep reinforcement learning approach for task se-\nlection by autonomous material handling vehicles. Proceedings\n- Winter Simulation Conference, 2018-December:1073–1083, 1\n2019.\n[LWZZ20]\nJingran Liang, Zhengning Wu, Chenye Zhu, and Zhi Hai Zhang.\nAn estimation distribution algorithm for wave-picking ware-\nhouse management. Journal of Intelligent Manufacturing, pages\n1–14, 10 2020.\n[MBM+16] Volodymyr Mnih, Adria Puigdomenech Badia, Lehdi Mirza,\nAlex Graves, Tim Harley, Timothy P. Lillicrap, David Silver,\nand Koray Kavukcuoglu. Asynchronous methods for deep rein-\nforcement learning. 33rd International Conference on Machine\nLearning, ICML 2016, 4:2850–2869, 2 2016.\n[MXZ18]\nJia Mao, Huihui Xing, and Xiuzhi Zhang. Design of intelligent\nwarehouse management system. Wireless Personal Communica-\ntions, 102:1355–1367, 9 2018.\n[Put94]\nMartin L. Puterman.\nMarkov Decision Processes: Discrete\nStochastic Dynamic Programming. Wiley Series in Probability\nand Statistics. Wiley, 1994.\n[RHG+21]\nAntonin Rafﬁn, Ashley Hill, Adam Gleave, Anssi Kanervisto,\nMaximilian Ernestus, and Noah Dormann.\nStable-baselines3:\nReliable reinforcement learning implementations.\nJournal of\nMachine Learning Research, 22(268):1–8, 2021.\n[SB18]\nRichard S Sutton and Andrew G Barto. Reinforcement learning:\nAn introduction. MIT press, 2018.\n[SHS+18]\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis\nAntonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent\nSifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,\nKaren Simonyan, and Demis Hassabis. A general reinforcement\nlearning algorithm that masters chess, shogi, and go through\nself-play. Science, 362:1140–1144, 12 2018.\n[SWD+17]\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,\nand Oleg Klimov.\nProximal policy optimization algorithms.\nCoRR, abs/1707.06347, 2017.\n[WD92]\nChristopher JCH Watkins and Peter Dayan. Q-learning. Machine\nlearning, 8(3-4):279–292, 1992.\n[Wol82]\nRonald W Wolff. Poisson arrivals see time averages. Operations\nresearch, 30(2):223–231, 1982.\n[YJL20]\nYang Yang, Li Juntao, and Peng Lingling.\nMulti-robot path\nplanning based on a deep reinforcement learning dqn algorithm.\nCAAI Transactions on Intelligence Technology, 5:177–183, 9\n2020.\n[ZDH+18]\nEmir Zunic, Sead Delalic, Kerim Hodzic, Admir Besirevic, and\nHarun Hindija. Smart warehouse management system concept\nwith implementation. 2018 14th Symposium on Neural Networks\nand Applications, NEUREL 2018, 12 2018.\n[ZHH+17]\nEmir Zunic, Kerim Hodzic, Haris Hasic, Rijad Skrobo, Ad-\nmir Besirevic, and Dzenana Donko. Application of advanced\nanalysis and predictive algorithm for warehouse picking zone\ncapacity and content prediction. ICAT 2017 - 26th International\nConference on Information, Communication and Automation\nTechnologies, Proceedings, 2017-December:1–6, 12 2017.\n[ZHH+18]\nEmir Zunic, Haris Hasic, Kerim Hodzic, Sead Delalic, and\nAdmir Besirevic. Predictive analysis based approach for optimal\nwarehouse product positioning. 2018 41st International Conven-\ntion on Information and Communication Technology, Electronics\nand Microelectronics, MIPRO 2018 - Proceedings, pages 950–\n954, 6 2018.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-07-08",
  "updated": "2022-07-21"
}