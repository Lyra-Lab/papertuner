{
  "id": "http://arxiv.org/abs/2207.04361v1",
  "title": "State Dropout-Based Curriculum Reinforcement Learning for Self-Driving at Unsignalized Intersections",
  "authors": [
    "Shivesh Khaitan",
    "John M. Dolan"
  ],
  "abstract": "Traversing intersections is a challenging problem for autonomous vehicles,\nespecially when the intersections do not have traffic control. Recently deep\nreinforcement learning has received massive attention due to its success in\ndealing with autonomous driving tasks. In this work, we address the problem of\ntraversing unsignalized intersections using a novel curriculum for deep\nreinforcement learning. The proposed curriculum leads to: 1) A faster training\nprocess for the reinforcement learning agent, and 2) Better performance\ncompared to an agent trained without curriculum. Our main contribution is\ntwo-fold: 1) Presenting a unique curriculum for training deep reinforcement\nlearning agents, and 2) showing the application of the proposed curriculum for\nthe unsignalized intersection traversal task. The framework expects processed\nobservations of the surroundings from the perception system of the autonomous\nvehicle. We test our method in the CommonRoad motion planning simulator on\nT-intersections and four-way intersections.",
  "text": "State Dropout-Based Curriculum Reinforcement Learning for\nSelf-Driving at Unsignalized Intersections*\nShivesh Khaitan1 and John M. Dolan2\nAbstract— Traversing intersections is a challenging problem\nfor autonomous vehicles, especially when the intersections do\nnot have trafﬁc control. Recently deep reinforcement learning\nhas received massive attention due to its success in dealing with\nautonomous driving tasks. In this work, we address the problem\nof traversing unsignalized intersections using a novel curricu-\nlum for deep reinforcement learning. The proposed curriculum\nleads to: 1) A faster training process for the reinforcement\nlearning agent, and 2) Better performance compared to an\nagent trained without curriculum. Our main contribution is\ntwo-fold: 1) Presenting a unique curriculum for training deep\nreinforcement learning agents, and 2) showing the application\nof the proposed curriculum for the unsignalized intersection\ntraversal task. The framework expects processed observations\nof the surroundings from the perception system of the au-\ntonomous vehicle. We test our method in the CommonRoad\nmotion planning simulator on T-intersections and four-way\nintersections.\nI. INTRODUCTION\nUrban intersections pose a difﬁcult driving challenge. Even\nfor human drivers, traversing an intersection requires undi-\nvided attention. The situation gets much more complicated\nwithout trafﬁc signals. Unsignalized intersections require\nmuch more coordination among the incoming vehicles. Such\nintersections have seen many fatal accidents. The US Federal\nHighway Administration has reported that about 71.4% of\nintersection-related fatal crashes occur at unsignalized inter-\nsections [1]. Autonomous vehicles can reduce the chances\nof accidents by eliminating human error. However, intersec-\ntions are difﬁcult for autonomous vehicles as well. This is\nespecially because predicting the intent of other incoming\ndrivers from different directions at intersections is in itself a\ncomplicated problem for self-driving.\nA popular choice for decision-making and planning at\nintersections for autonomous vehicles has been rule-based\nmethods [2]. These were especially very popular for the\nDARPA Urban Challenge [3], [4]. Although they make the\nsystem interpretable, tuning the parameters to avoid collision\nin every situation is very hard. The rule-based methods also\nusually try to be overly cautious, leading to suboptimal\nbehaviors. Moreover, a rule-based system needs to consider\nthe trajectory generator’s ability to generate feasible trajec-\ntories based on the goal or intent set by the decision-making\nmodule, which might not be possible in all circumstances.\n*This work was supported by the CMU Argo AI Center for Autonomous\nVehicle Research.\n1,2The\nauthors\nare\nwith\nthe\nRobotics\nInstitute,\nCarnegie\nMellon\nUniversity,\nPittsburgh,\nPA,\nthe\nUSA\n{shiveshk,jdolan}@andrew.cmu.edu\nReinforcement learning (RL) and deep reinforcement\nlearning (DRL) have recently gained a lot of attention in\nthe literature for autonomous driving. RL’s ability to deal\nwith complex scenarios has been unmatched by rule-based\nmethods. It also does not require massive datasets as su-\npervised learning does and can replace individual software\nmodules or be used as a complete end-to-end solution for\ndriving. Its abilities and performance have been demonstrated\nin many previous works [5], [6]. However, complex tasks\nlike autonomous driving usually require long training periods\nto produce acceptable results. This is because of the high-\ndimensional state space of the autonomous driving task.\nTo accelerate the training process, curriculum learning [7]\nhas been proposed. The basic idea in curriculum learning is\nto design the training process as a set of graduated steps with\nincreasing complexity of tasks. It draws inspiration from the\nway humans learn by starting with easier tasks and gradually\nincreasing the complexity to become an expert. The idea\nof curriculum learning has also been applied to reinforce-\nment learning. For a detailed review on the applications of\ncurriculum for reinforcement learning, see [8]. Apart from\naccelerating the training process, it has also been proposed\nthat using a curriculum can help ﬁnd better local minima as\ncompared to training without curriculum [7].\nPreviously, most works have used hand-designed task-\nbased curricula for training, which rely on segregating the\ntasks at hand based on their difﬁculty and training se-\nquentially with increasing levels of difﬁculty. Such manual\nsegregation of tasks can be laborious and time-consuming.\nIn this work, we propose a unique automated curriculum for\ntraining that is easy to design and is also more effective in\ndealing with unsignalized intersections. We test our method\nusing CommonRoad-RL [9] on T-intersections and four-\nway intersections without trafﬁc control and compare it\nwith standard PPO and rule-based TTC method. The main\ncontributions of this work are:\n• An automated curriculum for training deep reinforce-\nment learning agents.\n• An application of the proposed method to the task of\ntraversing unsignalized intersections.\nThe rest of this paper is organized as follows. Section II\nprovides a review of some important related work. Section III\ngives an introduction to the intersection problem addressed in\nthis work. Section IV contains necessary background on key\nmethodologies. Section V presents the experimental results.\nThe conclusions and the future work are in Section VI.\narXiv:2207.04361v1  [cs.RO]  10 Jul 2022\nII. RELATED WORK\nThis section summarizes relevant previous work catego-\nrized as follows: a) work related to planning for autonomous\nvehicles focusing on reinforcement learning and b) tech-\nniques to accelerating training.\nA. Planning for autonomous vehicles\nIn the existing literature, planning for autonomous vehicles\nhas used several methods, including rule-based methods,\nimitation learning, reinforcement learning, and inverse re-\ninforcement learning.\nThe most popular rule-based method is the time-to-\ncollision (TTC) method [2]. This method relies on hand-\ncrafted rules and parameters to control a vehicle and avoid\ncollisions. Trajectory optimization-based methods have also\nbeen extensively used for motion planning. Model Predictive\nControl (MPC) [10], [11], which is a receding horizon frame-\nwork with repeated optimization, is a widely used method.\n[12] proposed the use of a prediction framework combined\nwith constrained iterative LQR, which is a lightweight opti-\nmization method as compared to MPC. However, designing\nthe heuristics for optimization is extremely laborious and\noften leads to suboptimal results, as they do not adapt well\nto different driving environments.\nRecently, learning-based methods have been quite suc-\ncessful for many robotics and autonomous systems tasks.\nFor self-driving, imitation learning leverages expert driving\ndata to train an autonomous driver, which can work in\nan end-to-end framework by directly mapping sensor input\nto trajectory or control commands. Behavior cloning [13]\nuses supervised learning on collected expert data. However,\nit suffers from the problem of distribution shift between\ntraining and test environments. This is tackled by online\ninteractive data collection methods [14], [15] for imitation\nlearning, which require an online demonstrator at training\ntime. These generalize much better to varied scenarios,\nbut the requirement of an expert demonstrator itself makes\ntraining challenging.\nDeep reinforcement learning is a promising framework for\ncomplex self-driving planning tasks. Recently, it has been\nextensively used for motion planning in complex environ-\nments [16], [17]. However, certain complex driving scenarios\nlike unsignalized intersections still remain a challenge. [18]\nuses a hierarchical framework with a high-level RL-based\ndecision-making planner and a low-level MPC controller.\nHowever, it does not consider cases when the ego-vehicle\nmight have to turn left or right at intersections instead\nof just going straight. [19] deals with this uncertainty at\nintersections, but it focuses on T-intersections only, which\nis a simpler scenario as compared to four-way intersections.\n[20] used Deep Q-learning Network (DQN) for intersection\nhandling. However, DQNs can only be used for problems\nwith a discrete action space and hence cannot be used\nfor continuous control commands. The Deep Deterministic\nPolicy Gradient (DDPG) method [21] works for continuous\ncontrol spaces. However, both DQN and DDPG require\nlong training periods to converge. In this work, we use\nProximal Policy Optimization (PPO) [22], which is a variant\nof the actor-critic family and uses an adaptive KL divergence\npenalty to control the change of policy at each iteration. To\naccelerate training and improve performance, we use the state\ndropout-based curriculum described in section IV-B.\nB. Techniques for accelerating training periods\nA major drawback of RL methods is long training periods\nand convergence to sub-optimal policies. This usually hap-\npens because the dynamics of the RL training environments\nare unknown to the agent, and it takes a sufﬁcient number\nof interactions with the environment before the agent can\nunderstand the dynamics to improve its policy. Sparse re-\nwards and partial observability aggravate the problem. This\nprohibits the usage of RL in safety-critical systems, as the\nsystems might be required to enter unsafe states while the\nagent trains. Some techniques to alleviate this problem have\nbeen proposed in the literature.\nTransfer learning (TL) is a technique which pretrains a\nnetwork on a different task before training on the target\ntask. The network beneﬁts by using the skills acquired\nduring pretraining. For a comprehensive literature review on\ntransfer learning for RL, see [23]. Apart from just having the\nadvantage of faster training time, using transfer learning also\nleads to better generalization and better performance than an\nagent trained without using the pretrained network.\nCurriculum learning [24], another approach to expedite\ntraining, uses a specially designed schedule for training. It\ntrains on easier tasks to start with and gradually increases\nthe difﬁculty level. This draws inspiration from how humans\nlearn to do complex tasks by “starting small” [25]. The main\nidea is that once the agent has learned to do a simple task\nwell, it will take fewer iterations to adapt to a harder task\nhaving the simpler task as a subset because of the stability\nof the training process. Apart from just accelerating the\ntraining process, it has also been shown that certain tasks\nhave been nearly impossible to train without following a\ncurriculum [26]. [27] used curriculum reinforcement learning\nfor overtaking in an autonomous racing scenario. However,\nhand segregating tasks for designing a curriculum might\nbe cumbersome, as deciding the difﬁculty level of a task\nmight not be trivial. [28] proposes an automatic curriculum\ngenerator that tries to alleviate the problem.\nIn this work, we propose unique automated curricula\nfor autonomous driving at unsignalized intersections. The\ncurricula are easier to train and reduce the training time.\nFor the current work, we assume perception to be solved so\nthat the framework accurately observes the processed road\nboundary and obstacle information. However, the curricula\ncan be extended to end-to-end frameworks as well.\nIII. PROBLEM DEFINITION\nA. Problem Statement\nThe problem at hand is generating control commands\nfor an ego-vehicle to traverse unsignalized intersections and\nreach a predeﬁned goal state. We consider two types of\nintersection scenarios, as shown in Fig. 1 and Fig. 2. The\nego-vehicle is depicted in green, target vehicles are in blue\nand the goal region is in yellow. The dotted lines starting\nfrom target vehicles show their future trajectories, which\ndo not change in response to the ego-vehicle behavior. This\nincreases the difﬁculty level of the problem signiﬁcantly.\n1) T-intersection: These scenarios are adopted from the\nCommonRoad benchmarks: ZAM Tjunction (Fig. 1).\nFig. 1.\nT-intersection\n2) Four-way intersection: These are four-way intersection\nscenarios (Fig. 2) custom-generated using the CommonRoad\nscenario designer tool [29], as the ofﬁcial CommonRoad\nbenchmarks lacked four-way intersections. The goal state\nfor these scenarios can be in the left, straight-ahead, or\nright lane. The ego-vehicle has to reach the goal region\nwithin ±11.45◦of the road lane orientation to successfully\ncomplete a scenario. The limits on orientation ensure that\nthe ego-vehicle does not reach the goal with an arbitrary\norientation, which can make it harder for the vehicle to\ncorrect the orientation when moving further in the goal lane.\nThe scenario has to be completed before a time-out of 150\nsec.\nFig. 2.\nFour-way intersection\nB. Observation and Action space\nThe observation space is as deﬁned in Table III-B. The\nobservations can be divided into four categories: 1) ego-\nvehicle state; 2) goal and reference path-related observations;\n3) surrounding vehicles-related observations; and 4) road\nnetwork-related observations. The dynamics constraints are\nbased on the kinematic bicycle model (KS1) as described in\n[30]. The reference path observation consists of waypoints\nfrom the path generated by A∗search over the lanelet\nnetwork in Commonroad. The distance advancements for\nlongitudinal and lateral directions are calculated along this\nreference path. For the surrounding vehicles, we include\nthe current and future states (for 5 time-steps with a time\ndiscretization of 0.1 sec) of each of the ﬁve vehicles in\nthe ego-vehicle frame. The information in future states is\nas described in section IV-B.\nThe action space consists of acceleration and steering\nfor the ego-vehicle A = {accel, steer}. The controls are\ncontinuous.\nTABLE I\nOBSERVATION SPACE\nDescription\nValues\nvt\nEgo-vehicle absolute velocity\nR\nat−1\nEgo-vehicle previous acceleration\nR\nδt−1\nEgo-vehicle previous steering\nR\nθt\nEgo-vehicle orientation\nR\nr\nReference path waypoints (x, y, θ) in ego-vehicle frame\nR15\ndθt\nEgo-vehicle orientation deviation from reference path\nR\ndlong\nLongitudinal distance advancement towards goal\nR\ndlat\nLatitudinal distance advancement towards goal\nR\ntout\nTime remaining before time-out\nR\nc\nLane curvature\nR\nlo\nEgo-vehicle offset from lane centerline\nR\nrl\nEgo-vehicle distance from left road boundary\nR\nrr\nEgo-vehicle distance from right road boundary\nR\nll\nEgo-vehicle distance from left lane boundary\nR\nlr\nEgo-vehicle distance from right lane boundary\nR\no\nTarget vehicles’ current and future states∗(x, y, v, θ)\nR100\n∗The information in future states depends on the curriculum\nIV. METHODOLOGY\nThis section introduces the reward structure for the PPO\nagent, and the proposed curricula for learning.\nA. Reward Structure\nThe components of the reward function are as follows:\n• A positive reward for distance advancement along the\nreference path as described in section III-B. The reward\nis calculated as ρ1dlong + ρ2dlat where ρ1 and ρ2 are\npositive constants.\n• A constant negative reward σ1 for collision with obsta-\ncles, going off-road, and violating dynamics constraints.\n• A constant negative reward σ2 for time-out.\n• A constant positive reward φ for reaching the goal\nwithin the allowed orientation error.\nAn episode is terminated when any one of the follow-\ning conditions is met: ego-vehicle reaches the goal within\nallowed orientation error, ego-vehicle goes off-road, ego-\nvehicle collides with another obstacle, applied controls vi-\nolate dynamics constraints or time-out.\nB. State dropout-based curriculum for PPO\nTraining and getting an acceptable performance with PPO\nor other existing RL algorithms in the described intersection\nproblem is tough. Often the RL agents settle for a suboptimal\npolicy. A prominent reason for this is that once an RL agent\nadopts a suboptimal policy, it further keeps exploiting the\nsuboptimal policy and takes a long time to escape local\nminima and generate better actions after that. Moreover,\nunsignalized intersections also require that a driver is able\nto determine the intent of multiple other incoming vehicles.\nTraining a simple PPO agent in this scenario leads to a\nsuboptimal behavior even with long training periods, as\nimplicitly learning to predict the behavior of the vehicles\nwhile simultaneously learning to control the car is not trivial\nfor the PPO agent.\nOur state dropout-based curriculum overcomes the chal-\nlenge by “starting small”. An easy scenario for an RL agent\nin intersections is when the intentions of the surrounding\ndrivers are known in advance. In such scenarios, the RL\nagent can learn an optimal policy for traversing. Thus we\nincorporate the future state information (for N time-steps)\nabout the surrounding vehicles in the observation space for\nPPO. We propose two different curricula to deal with the\nadditional privileged observation:\n1) Curriculum 1: In this method we train the agent in N+\n1 phases starting with phase 0. We begin training with the\nfuture state information for N time-steps. After phase 0, we\ndrop the N th future state information and continue training.\nIn the subsequent phases we keep on dropping more future\nstate information and in the ﬁnal phase ﬁne-tune the agent\nwithout any future state information. Algorithm 1 describes\nthe STEP function for this method.\nAlgorithm 1 Step function for curriculum 1\n1: procedure STEP(action, phase)\n2:\nApply acceleration and steering from action\n3:\nobs = Observe()\n4:\nreward, done = Reward(obs)\n5:\nfor i ←(N −phase + 1) to N do\n6:\nDrop ith future prediction from obs\n7:\nend for\n8:\nreturn obs, reward, done\n9: end procedure\n2) Curriculum 2: In this method, instead of dropping\nthe future state information sequentially after each phase,\nwe augment the action space of the agent with an ad-\nditional action such that the augmented action space is\nA = {accel, steer, pred}. Here pred determines which\nfuture state information for the surrounding vehicles will be\ndropped from the observation in the next step. The reward\nstructure as deﬁned in section IV-A is also augmented to\nadd constant positive rewards ψi for choosing to drop the\nfuture states from the ith to the N th time-step. The decision\nis based on parameters κi. Here, ∀i ∈[2, N]\nψi < ψi−1\n(1)\nκi < κi−1\n(2)\nThis incentivizes the agent to drop more future states.\nAlgorithm 2 deﬁnes the modiﬁed STEP function for this\nmethod. Thus we let the network choose when to drop the\nfuture states. At the beginning of training, when the agent has\nnot learned to predict the behavior of the vehicles, it chooses\nto use the future information. As the training progresses and\nthe agent learns to control the ego-vehicle, ψi incentivizes\nthe agent to learn to predict the driving intention as well in\norder to get higher rewards.\nAn added advantage of this method is that the pred value\ncan be used at test time to determine whether the agent\nunderstands the behavior of the other vehicles or not. This\ncan further be used as a conﬁdence score for the agent\ncommands.\nAlgorithm 2 Step function for curriculum 2\n1: procedure STEP(action)\n2:\nApply acceleration and steering from action\n3:\nobs = Observe()\n4:\nreward, done = Reward(obs)\n5:\nfor i ←1 to N do\n6:\nif action.pred ≥κi then\n7:\nDrop ith future prediction from obs\n8:\nreward = reward + ψi\n9:\nend if\n10:\nend for\n11:\nreturn obs, reward, done\n12: end procedure\nV. EXPERIMENTS\nThe curriculum was tested on the CommonRoad simulator\nfor unsignalized T-intersections and four-way intersections.\nA. Experimental Setup\nFor the T-intersections, we have 544 ZAM Tjunction\nscenarios from the CommonRoad benchmarks, which have\nscenarios with ego-vehicles and target-vehicles initialized at\ndifferent positions with different velocities. 380 scenarios are\nused for training, and the rest are used for testing.\nWe generated 2000 four-way intersection scenarios. For\neach scenario, the ego-vehicle is spawned at a randomly\nsampled location in lane 1 (Fig. 2) with appropriate orien-\ntation. The ego-vehicle always starts at rest. The obstacles\nare spawned at randomly sampled locations in lanes 2, 3\nand 4 with appropriate orientation and randomly sampled\ninitial velocities ∈[5, 10] m/s. 1400 scenarios were used for\ntraining and the rest were used for testing.\nB. Network Architecture\nTo represent the policies, we used fully-connected (FC)\nnetworks with 8 hidden layers of 64 units each and tanh\nnonlinearities. The hyperparameter values for PPO are listed\nin table II. Empirically best performing hyperparameters\nand network architecture were choosen for the experimental\nresults.\nC. Training Details\nThe training was done on a desktop with 3.5 GHz AMD\nRyzen 9 5900hs CPU. We used a customized version of the\nCommonRoad-RL framework for training and testing. We\ntrained three different agents:\nTABLE II\nHYPERPARAMETERS\nParameter\nValue\nTraining Iterations (T-intersections)\n600\nTraining Iterations (Four-way intersections)\n650\nDiscount (γ)\n0.99\nGAE parameter (λ)\n0.95\nClipping parameter\n0.2\nLearning-rate\n0.0005\nNo. of environments\n32\nTime horizon\n512\nBatch size\n16384*\n* Batch size = Time Horizon × No. of environments\n1) A standard PPO agent without any curriculum. The\ntarget vehicles’ future state information is dropped by\ndefault in the standard agent.\n2) A PPO agent using curriculum 1 with N = 4. The\nagent is thus trained in 5 phases.\n3) A PPO agent using curriculum 2 with N = 4.\nD. Performance Evaluation\nTo understand the impact of the proposed curricula, we\ncompare the training curves of the agents trained using the\ncurricula with the training curve of the standard PPO agent.\nWe compare the success rate and mean reward to evaluate\nthe performance of our method. The results presented are the\nbest results obtained by repeating each experiment 3 times\nwith a random seed for the network. The future states of the\nsurrounding vehicles are not available during testing for all\nthree methods.\n1) T-intersection: The training curves for the three meth-\nods are shown in Fig. 3. The rewards are averaged across\n10 updates. It can be seen that both the curriculum-based\nmethods outperform the standard PPO baseline in terms of\nsample efﬁciency. While PPO agents with the curricula start\nconverging around 6000000 steps, the baseline standard PPO\nconverges around 8250000 steps.\nFig. 3.\nTraining curves (T-intersection)\nFig. 4 shows the success rate of the methods on 164 testing\nscenarios. Both the curricula outperform the standard PPO\nbaseline with their best performance of 99.39% as compared\nto 92.68% success rate for the baseline.\n2) Four-way intersection: The training curves for the\nthree methods are shown in Fig. 5. The rewards are averaged\nFig. 4.\nSuccess rates (T-intersections)\nacross 10 updates. It can be seen that both the curriculum\nbased methods outperform the standard PPO baseline in\nterms of sample efﬁciency. While PPO with curriculum\n1 starts converging around 7250000 steps, the curriculum\n2 approach converges around 8500000 steps. The baseline\nconverges around 9250000 steps.\nFig. 5.\nTraining curves\nFig. 6 shows the performance of the methods on the 600\ntesting scenarios. Curriculum 2 shows the best performance\nwith its best model having a success rate of 93.83%. Curricu-\nlum 1 achieves 92.66% and the baseline achieves 81.33%.\nFig. 6.\nSuccess rates\nWe also compare the results with the rule-based TTC\nmethod in Table III.\nE. Discussion\nThe training curves and success rates of the curriculum-\nbased agents show that using the curriculum results in higher\nTABLE III\nSUCCESS RATES OF CURRICULUM METHODS VERSUS BASELINES\nMethod\nT-intersection (%)\nFour-way intersection (%)\nTTC\n90.85\n84.33\nStandard PPO\n92.68\n81.33\nCurriculum 1\n99.39\n92.66\nCurriculum 2\n99.39\n93.83\nsample efﬁciency, with the agents converging much faster as\ncompared to the baseline agent. Also, both the curricula have\nhigher success rates than the baselines in both the scenarios.\nThis shows that the agents converge to better optima as\nthe curriculum makes the training stable. Though we used\nground truth prediction for the target vehicles available in\nthe simulation, for real-world training, predictions from the\nperception layer can be used. Further avenues of privileged\ninformation e.g. bird’s eye view in the augmented observa-\ntion space can also be experimented with.\nVI. CONCLUSIONS\nIn this work, we propose a novel curriculum for train-\ning a deep reinforcement learning agent where future state\npredictions can achieve faster training and avoid conver-\ngence to suboptimal policies. We test the performance of\nthe curriculum on the unsignalized intersection traversal\ntask for autonomous driving. The curriculum outperforms\nthe standard baseline in sample efﬁciency and test time\nperformance. Though in this paper we show the application\nof the curriculum to the intersection traversal task only, it\nhas a broader scope and can be generally applied to other\nsimilar tasks as well. In future work, we aim to extend the\ncurriculum for end-to-end learning and real-world testing.\nREFERENCES\n[1] N. Administration, “Fatality analysis reporting system (fars) nhtsa,”\n2022.\n[Online].\nAvailable:\nhttps://www.nhtsa.gov/research-data/\nfatality-analysis-reporting-system-fars\n[2] D. N. Lee, “A theory of visual control of braking based on information\nabout time-to-collision,” Perception, vol. 5, no. 4, pp. 437–459, 1976.\n[3] C. Urmson, J. Anhalt, D. Bagnell, C. Baker, R. Bittner, M. Clark,\nJ. Dolan, D. Duggins, T. Galatali, C. Geyer, et al., “Autonomous\ndriving in urban environments: Boss and the urban challenge,” Journal\nof Field Robotics, vol. 25, no. 8, pp. 425–466, 2008.\n[4] M. Montemerlo, J. Becker, S. Bhat, H. Dahlkamp, D. Dolgov, S. Et-\ntinger, D. Haehnel, T. Hilden, G. Hoffmann, B. Huhnke, et al., “Junior:\nThe stanford entry in the urban challenge,” Journal of ﬁeld Robotics,\nvol. 25, no. 9, pp. 569–597, 2008.\n[5] C.-J. Hoel, T. Tram, and J. Sj¨oberg, “Reinforcement learning with\nuncertainty estimation for tactical decision-making in intersections,” in\n2020 IEEE 23rd International Conference on Intelligent Transporta-\ntion Systems (ITSC), 2020, pp. 1–7.\n[6] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura,\n“Navigating occluded intersections with autonomous vehicles using\ndeep reinforcement learning,” in 2018 IEEE International Conference\non Robotics and Automation (ICRA).\nIEEE, 2018, pp. 2034–2039.\n[7] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum\nlearning,” in Proceedings of the 26th annual international conference\non machine learning, 2009, pp. 41–48.\n[8] S. Narvekar, B. Peng, M. Leonetti, J. Sinapov, M. E. Taylor, and\nP. Stone, “Curriculum learning for reinforcement learning domains: A\nframework and survey,” arXiv preprint arXiv:2003.04960, 2020.\n[9] X. Wang, H. Krasowski, and M. Althoff, “Commonroad-rl: A con-\nﬁgurable reinforcement learning environment for motion planning of\nautonomous vehicles,” in IEEE International Conference on Intelligent\nTransportation Systems (ITSC), 2021.\n[10] J. Ji, A. Khajepour, W. W. Melek, and Y. Huang, “Path planning and\ntracking for vehicle collision avoidance based on model predictive\ncontrol with multiconstraints,” IEEE Transactions on Vehicular Tech-\nnology, vol. 66, no. 2, pp. 952–964, 2016.\n[11] F. Borrelli, P. Falcone, T. Keviczky, J. Asgari, and D. Hrovat, “MPC-\nbased approach to active steering for autonomous vehicle systems,”\nInternational Journal of Vehicle Autonomous Systems, vol. 3, no. 2,\n2005.\n[12] Y. Pan, Q. Lin, H. Shah, and J. M. Dolan, “Safe planning for self-\ndriving via adaptive constrained ilqr,” in 2020 International Confer-\nence on Intelligent Robots and Systems (IROS).\nIEEE, 2020.\n[13] F. Codevilla, M. M¨uller, A. L´opez, V. Koltun, and A. Dosovitskiy,\n“End-to-end driving via conditional imitation learning,” in 2018 IEEE\ninternational conference on robotics and automation (ICRA).\nIEEE,\n2018, pp. 4693–4700.\n[14] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning\nand structured prediction to no-regret online learning,” in Proceedings\nof the fourteenth international conference on artiﬁcial intelligence and\nstatistics.\nJMLR Workshop and Conference Proceedings, 2011.\n[15] K. Menda, K. Driggs-Campbell, and M. J. Kochenderfer, “Ensem-\nbledagger: A bayesian approach to safe imitation learning,” in 2019\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), 2019, pp. 5041–5048.\n[16] L. Tai, G. Paolo, and M. Liu, “Virtual-to-real deep reinforcement\nlearning: Continuous control of mobile robots for mapless navigation,”\nin 2017 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS).\nIEEE, 2017, pp. 31–36.\n[17] M. Everett, Y. F. Chen, and J. P. How, “Motion planning among\ndynamic, decision-making agents with deep reinforcement learning,”\nin 2018 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS).\nIEEE, 2018, pp. 3052–3059.\n[18] T. Tram, I. Batkovic, M. Ali, and J. Sj¨oberg, “Learning when to\ndrive in intersections by combining reinforcement learning and model\npredictive control,” in 2019 IEEE Intelligent Transportation Systems\nConference (ITSC).\nIEEE, 2019, pp. 3263–3268.\n[19] V. Sezer, T. Bandyopadhyay, D. Rus, E. Frazzoli, and D. Hsu,\n“Towards autonomous navigation of unsignalized intersections under\nuncertainty of human driver intent,” in 2015 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), 2015.\n[20] D. Isele, A. Nakhaei, and K. Fujimura, “Safe reinforcement learning\non autonomous vehicles,” in 2018 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), 2018, pp. 1–6.\n[21] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforce-\nment learning,” arXiv preprint arXiv:1509.02971, 2015.\n[22] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal\npolicy\noptimization\nalgorithms,”\narXiv\npreprint\narXiv:1707.06347, 2017.\n[23] Z. Zhu, K. Lin, and J. Zhou, “Transfer learning in deep reinforcement\nlearning: A survey,” arXiv preprint arXiv:2009.07888, 2020.\n[24] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum\nlearning,” in Proceedings of the 26th Annual International Conference\non Machine Learning, ser. ICML ’09.\nAssociation for Computing\nMachinery, 2009, p. 41–48.\n[25] J. L. Elman, “Learning and development in neural networks: The\nimportance of starting small,” Cognition, vol. 48, no. 1, 1993.\n[26] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew,\nA. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, et al., “Solving\nrubik’s cube with a robot hand,” arXiv preprint arXiv:1910.07113,\n2019.\n[27] Y. Song, H. Lin, E. Kaufmann, P. D¨urr, and D. Scaramuzza, “Au-\ntonomous overtaking in gran turismo sport using curriculum reinforce-\nment learning,” in 2021 IEEE International Conference on Robotics\nand Automation (ICRA).\nIEEE, 2021, pp. 9403–9409.\n[28] Z. Qiao, K. Muelling, J. M. Dolan, P. Palanisamy, and P. Mudalige,\n“Automatically generated curriculum based reinforcement learning for\nautonomous vehicles in urban environment,” in 2018 IEEE Intelligent\nVehicles Symposium (IV), 2018, pp. 1233–1238.\n[29] M. K. Sebastian Maierhofer and M. Althoff, “Commonroad scenario\ndesigner: An open-source toolbox for map conversion and scenario\ncreation for autonomous vehicles,” in Proc. of the IEEE Int. Conf. on\nIntelligent Transportation Systems, 2021, pp. 3176–3182.\n[30] M. Althoff, M. Koschi, and S. Manzinger, “Commonroad: Composable\nbenchmarks for motion planning on roads,” in Proc. of the IEEE\nIntelligent Vehicles Symposium, 2017.\n",
  "categories": [
    "cs.RO",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2022-07-10",
  "updated": "2022-07-10"
}