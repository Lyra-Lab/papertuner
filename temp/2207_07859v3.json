{
  "id": "http://arxiv.org/abs/2207.07859v3",
  "title": "SenseFi: A Library and Benchmark on Deep-Learning-Empowered WiFi Human Sensing",
  "authors": [
    "Jianfei Yang",
    "Xinyan Chen",
    "Dazhuo Wang",
    "Han Zou",
    "Chris Xiaoxuan Lu",
    "Sumei Sun",
    "Lihua Xie"
  ],
  "abstract": "WiFi sensing has been evolving rapidly in recent years. Empowered by\npropagation models and deep learning methods, many challenging applications are\nrealized such as WiFi-based human activity recognition and gesture recognition.\nHowever, in contrast to deep learning for visual recognition and natural\nlanguage processing, no sufficiently comprehensive public benchmark exists. In\nthis paper, we review the recent progress on deep learning enabled WiFi\nsensing, and then propose a benchmark, SenseFi, to study the effectiveness of\nvarious deep learning models for WiFi sensing. These advanced models are\ncompared in terms of distinct sensing tasks, WiFi platforms, recognition\naccuracy, model size, computational complexity, feature transferability, and\nadaptability of unsupervised learning. It is also regarded as a tutorial for\ndeep learning based WiFi sensing, starting from CSI hardware platform to\nsensing algorithms. The extensive experiments provide us with experiences in\ndeep model design, learning strategy skills and training techniques for\nreal-world applications. To the best of our knowledge, this is the first\nbenchmark with an open-source library for deep learning in WiFi sensing\nresearch. The benchmark codes are available at\nhttps://github.com/xyanchen/WiFi-CSI-Sensing-Benchmark.",
  "text": "1\nSenseFi: A Library and Benchmark on\nDeep-Learning-Empowered WiFi Human Sensing\nJianfei Yang, Xinyan Chen, Han Zou, Dazhuo Wang, Chris Xiaoxuan Lu,\nSumei Sun, Fellow, IEEE and Lihua Xie, Fellow, IEEE\nAbstract‚ÄîWiFi sensing has been evolving rapidly in recent\nyears. Empowered by propagation models and deep learning\nmethods, many challenging applications are realized such as\nWiFi-based human activity recognition and gesture recognition.\nHowever, in contrast to deep learning for visual recognition and\nnatural language processing, no sufÔ¨Åciently comprehensive public\nbenchmark exists. In this paper, we review the recent progress on\ndeep learning enabled WiFi sensing, and then propose a bench-\nmark, SenseFi, to study the effectiveness of various deep learning\nmodels for WiFi sensing. These advanced models are compared\nin terms of distinct sensing tasks, WiFi platforms, recognition\naccuracy, model size, computational complexity, feature trans-\nferability, and adaptability of unsupervised learning. It is also\nregarded as a tutorial for deep learning based WiFi sensing,\nstarting from CSI hardware platform to sensing algorithms. The\nextensive experiments provide us with experiences in deep model\ndesign, learning strategy skills and training techniques for real-\nworld applications. To the best of our knowledge, this is the\nÔ¨Årst benchmark with an open-source library for deep learning\nin WiFi sensing research. The benchmark codes are available at\nhttps://github.com/xyanchen/WiFi-CSI-Sensing-Benchmark.\nIndex Terms‚ÄîWiFi sensing, benchmark, deep learning, chan-\nnel state information, human sensing, transfer learning, unsuper-\nvised learning, ubiquitous computing, activity recognition.\nI. INTRODUCTION\nWith the proliferation of mobile internet usage, WiFi access\npoint (AP) has become a ubiquitous infrastructure in smart\nenvironments, ranging from commercial buildings to domestic\nsettings. By analysing the patterns of its wireless signal,\ntoday‚Äôs AP has evolved beyond a pure WiFi router, but is also\nwidely used as a type of ‚Äòsensor device‚Äô to enable new services\nfor human sensing. Particularly, recent studies have found\nthat WiFi signals in the form of Channel State Information\n(CSI) [1], [2] are extremely promising for a variety of device-\nfree human sensing tasks, such as occupancy detection [3],\nactivity recognition [4], [5], [6], [7], fall detection [8], ges-\nture recognition [9], [10], human identiÔ¨Åcation [11], [12],\nand people counting [13], [14]. Unlike the coarse-grained\nreceived signal strengths, WiFi CSI records more Ô¨Åne-grained\ninformation about how a signal propagates between WiFi\nJ.\nYang,\nX.\nChen,\nD.\nWang,\nH.\nZou\nand\nL.\nXie\nare\nwith\nthe\nSchool\nof\nElectrical\nand\nElectronics\nEngineer-\ning,\nNanyang\nTechnological\nUniversity,\nSingapore\n(e-mail:\n{yang0478,chen1328,dazhuo001,zouh0005,elhxie}@ntu.edu.sg).\nC. X. Lu is with the School of Informatics at the University of Edinburgh,\nUnited Kingdom (e-mail: xiaoxuan.lu@ed.ac.uk).\nS. Sun are with the Institute for Infocomm Research (I2R), Agency for\nScience, Technology and Research (A*STAR), Singapore 138632.(E-mail:\nsunsm@i2r.a-star.edu.sg).\nModels & Data\nProcessed Datasets \nPrevailing Deep Models\nDifferent WiFi Platforms\nTransfer Learning\nUnsupervised Learning\nComprehensive Benchmark\nWiFi Sensing\nGrand Challenges\nFig. 1: The technical contributions and summary of SenseFi.\ndevices and how a signal is reÔ¨Çected from the surrounding\nenvironment in which humans move around. On the other\nside, as WiFi signals (2.4GHz or 5GHz) lie in the non-visible\nband of the electromagnetic spectrum, WiFi CSI based human\nsensing is intrinsically more privacy-friendly than cameras and\ndraws increasing attention from both academia and industry.\nMotivated by increasing interests needs, a new WiFi standard,\n802.11bf [15] is designed by the IEEE 802.11bf Task Group\n(TGbf), and will amend the current WiFi standard both at the\nMedium Access Control (MAC) and Physical Layer (PHY)\nto ofÔ¨Åcially include WiFi sensing as part of a regular WiFi\nservice by late 2024.\nExisting WiFi sensing methods can be categorized into\nmodel-based methods and learning-based methods. Model-\nbased methods rely on physical models that describe the WiFi\nsignals propagation, such as Fresnel Zone [16]. Model based\nmethods help us understand the underlying mechanism of WiFi\nsensing and design sensing methods for periodic or single mo-\ntions, such as respiration [17], [18], [19] and falling down [8],\n[20], [21]. Nevertheless, model based methods fall short when\nit comes to the complicated human activities that consist of\na series of different motions. For example, a human gait\ncomprises the synergistic movements of arms, legs and bodies,\nthe differences of which are hard to depict by physical models.\nIn contrast, by feeding a massive amount of data into machine\nlearning [22] or deep learning networks, [9], [5], learning\nbased achieve remarkable performances in complicated sens-\ning tasks. Various deep neural networks are designed to enable\nmany applications including activity recognition [23], gesture\nrecognition [9], human identiÔ¨Åcation [11], [12], [24], and\npeople counting [13], [14]. Though deep learning models\narXiv:2207.07859v3  [cs.LG]  17 Feb 2023\n2\nhave a strong ability of function approximation, they require\ntremendous labeled data that is expensive to collect and\nsuffer from the negative effect of distribution shift caused by\nenvironmental dynamics [25].\nMost state-of-the-art deep learning models are developed for\ncomputer vision [26], such as human activity recognition [27],\n[28], and natural language processing tasks [29], such as\nsentiment classiÔ¨Åcation [30]. Deep models have demonstrated\nthe capacity of processing high-dimensional and multi-modal\ndata problems. These approaches inspire the deep learning\napplications in WiFi sensing in terms of data preprocessing,\nnetwork design, and learning objectives. It is seen that more\nand more deep models [31], [32] for WiFi sensing come\ninto existence and overcome the aforementioned obstacles\nthat traditional statistical learning methods cannot address.\nHowever, current works mainly aim to achieve high accuracy\non speciÔ¨Åc sensing tasks by tailoring deep neural networks\nbut do not explore the intrinsic tension between various deep\nlearning models and distinct WiFi sensing data collected by\ndifferent devices and CSI tools. It is unclear if the remarkable\nresults of a WiFi sensing research paper come from the deep\nmodel design or the WiFi platform. Hence, there still exist\nsome signiÔ¨Åcant gaps between current deep learning and WiFi\nsensing research: (i) how to customize a deep neural network\nfor a WiFi sensing task by integrating prevailing network\nmodules (e.g., fully-connected layer, convolutional layer, re-\ncurrent neural unit, transformer block) into one synergistic\nframework? (ii) how do the prevailing models perform when\nthey are compared fairly on multiple WiFi sensing platforms\nand data modalities? (iii) how to achieve a trade-off between\nrecognition accuracy and efÔ¨Åciency?\nTo answer these questions, we propose SenseFi, a bench-\nmark and model zoo library for WiFi CSI sensing using deep\nlearning. Firstly, we introduce the prevalent deep learning\nmodels, including multilayer perceptron (MLP), convolutional\nneural network (CNN), recurrent neural network (RNN), vari-\nants of RNN, CSI transformers, and CNN-RNN, and sum-\nmarize how they are effective for CSI feature learning and\nWiFi sensing tasks. Then we investigate and benchmark these\nmodels on three WiFi human activity recognition data that\nconsists of both raw CSI data and processed data collected by\nIntel 5300 CSI tool [1] and Atheros CSI tool [2], [22]. The\naccuracy and efÔ¨Åciency of these models are compared and\ndiscussed to show their viability for real-world applications.\nWe further investigate how different WiFi sensing tasks can\nbeneÔ¨Åt each other by transfer learning, and how unsupervised\nlearning can be used to exploit features without labels, re-\nducing the annotation cost. These features are summarized in\nFigure 1. All the source codes are written into one library\nso that the researchers can develop and evaluate their models\nconveniently.\nAs such, the contributions are summarized as follows:\n‚Ä¢ We analyze and summarize how the widespread deep\nlearning models in computer vision and natural language\nprocessing beneÔ¨Åt WiFi sensing in terms of network\nstructure and feature extraction.\n‚Ä¢ We select two public datasets (UT-HAR [33] and\nWidar [34]) and collect two new datasets (NTU-Fi HAR\nand Human-ID) using different CSI platforms, which\nallows us to benchmark the deep learning methods and\nevaluate their feasibility for WiFi sensing.\n‚Ä¢ We explore the transfer learning scheme that transfers\nknowledge across different sensing tasks, and benchmark\nit across all models.\n‚Ä¢ We investigate the unsupervised learning scheme that\ncontrastively learns the feature extractor without data\nannotation, and benchmark it across all models.\n‚Ä¢ We develop the SenseFi library and open-source the\nbenchmarking codes. To the best of our knowledge, this\nis the Ô¨Årst work that benchmarks advanced deep models\nand learning schemes for WiFi sensing, which provides\ncomprehensive and signiÔ¨Åcant evidence and tools for\nfuture research.\nThe rest of the paper is organized as follows. Section II\nintroduces the fundamental knowledge on WiFi sensing and\nCSI data. Then we introduce the prevalent deep learning mod-\nels and how they are applied to WiFi sensing in Section III.\nThe empirical study is detailed in Section V, and then the\nsummaries and discussions are made in Section VI. Finally,\nthe paper is concluded in Section VIII.\nII. PRELIMINARIES OF WIFI SENSING\nA. Channel State Information\nIn WiFi communication, channel state information reÔ¨Çects\nhow wireless signals propagate in a physical environment\nafter diffraction, reÔ¨Çections, and scattering, which describes\nthe channel properties of a communication link. For mod-\nern wireless communication networks following the IEEE\n802.11 standard, Multiple-Input Multiple-Output (MIMO) and\nOrthogonal Frequency Division Multiplexing (OFDM) at the\nphysical layer contribute to increasing data capacity and better\northogonality in transmission channels affected by multi-path\npropagation. As a result, current WiFi APs usually have\nmultiple antennas with many subcarriers for OFDM. For a\npair of transmitter and receiver antennas, CSI describes the\nphase shift of multi-path and amplitude attenuation on each\nsubcarrier. Compared to received signal strength, CSI data has\nbetter resolutions for sensing and can be regarded as ‚ÄúWiFi\nimages‚Äù for the environment where WiFi signals propagate.\nSpeciÔ¨Åcally, the Channel Impulse Response (CIR) h(œÑ) of the\nWiFi signals is deÔ¨Åned in the frequency domain:\nh(œÑ) =\nL\nX\nl=1\nŒ±lejœÜlŒ¥(œÑ ‚àíœÑl),\n(1)\nwhere Œ±l and œÜl denote the amplitude and phase of the l-\nth multi-path component, respectively, œÑl is the time delay,\nL denotes the number of multi-paths and Œ¥(œÑ) is the Dirac\ndelta function. To estimate the CIR, the OFDM receiver\nsamples the signal spectrum at subcarrier level in the realistic\nimplementation, which represents amplitude attenuation and\nphase shift via complex number. In WiFi sensing, the CSI\nrecording functions are realized by speciÔ¨Åc tools [1], [2]. The\nestimation can be represented by:\nHi = ||Hi||ej‚à†Hi\n(2)\n3\nBoxing\nCircling\nFalling\nWalking\nFig. 2: The CSI samples of three human activities in NTU-Fi, collected by Atheros CSI Tool.\nwhere ||Hi|| and ‚à†Hi are the amplitude and phase of i-th\nsubcarrier, respectively.\nB. CSI Tools and Platforms\nThe number of subcarriers is decided by the bandwidth and\nthe tool. The more subcarriers one has, the better resolution\nthe CSI data is. Existing CSI tools include Intel 5300 NIC [1],\nAtheros CSI Tool [2] and Nexmon CSI Tool [35], and many\nrealistic sensing platforms are built on them. The Intel 5300\nNIC is the most commonly used tool, which is the Ô¨Årst released\nCSI tool. It can record 30 subcarriers for each pair of antennas\nrunning with 20MHz bandwidth. Atheros CSI Tool increases\nthe CSI data resolution by improving the recording CSI to\n56 subcarriers for 20MHz and 114 subcarriers for 40MHz,\nwhich has been widely used for many applications [5], [22],\n[6], [9], [36]. The Nexmon CSI Tool Ô¨Årstly enables CSI\nrecording on smartphones and Raspberry Pi, and can capture\n256 subcarriers for 80MHz. However, past works [37], [38]\nshow that their CSI data is quite noisy, and there do not\nexist common datasets based on Nexmon. In this paper, we\nonly investigate the effectiveness of the deep learning models\ntrained on representative CSI data from the widely-used Intel\n5300 NIC and Atheros CSI Tool.\nC. CSI Data Transformation and Cleansing\nIn general, the CSI data consists of a vector of complex\nnumber including the amplitude and phase. The question is\nhow we process these data for the deep models of WiFi\nsensing? We summarize the answers derived from existing\nworks:\n1) Only use the amplitude data as input. As the raw\nphases from a single antenna are randomly distributed\ndue to the random phase offsets [39], the amplitude of\nCSI is more stable and suitable for WiFi sensing. A\nsimple denoising scheme is enough to Ô¨Ålter the high-\nfrequency noise of CSI amplitudes, such as the wavelet\ndenoising [22]. This is the most common practice for\nmost WiFi sensing applications.\n2) Use the CSI difference between antennas for model-\nbased methods. Though the raw phases are noisy,\nthe phase difference between two antennas is quite\nstable [9], which can better reÔ¨Çect subtle gestures than\namplitudes. Then the CSI ratio [40] is proposed to\nmitigate the noise by the division operation and thus\nincreases the sensing range. These techniques are mostly\ndesigned for model-based solutions as they require clean\ndata for selecting thresholds.\n3) Use the processed doppler representation of CSI. To\neliminate the environmental dependency of CSI data,\nthe body-coordinate velocity proÔ¨Åle (BVP) is proposed\nto simulate the doppler feature [34] that only reÔ¨Çects\nhuman motions.\nIn our benchmark, as we focus on the learning-based methods,\nwe choose the most common data modality (i.e., amplitude\nonly) and the novel BVP modality that is domain-invariant.\nD. How Human Activities Affect CSI\nAs shown in Figure 2, the CSI data for human sensing is\ncomposed of two dimensions: the subcarrier and the packet\nnumber (i.e., time duration). For each packet or timestamp t,\nwe have Xt = NT √ó NR √ó Nsub where NT , NR and Nsub\ndenote the number of transmitter antennas, receiver antennas\nand subcarriers per antenna, respectively. This can be regarded\nas a ‚ÄúCSI image‚Äù for the surrounding environment at time t.\nThen along with subsequent timestamps, the CSI images form\na ‚ÄúCSI video‚Äù that can describe human activity patterns. To\nconnect CSI data with deep learning models, we summarize\nthe data properties that serve for a better understanding of\ndeep model design:\n1) Subcarrier dimension ‚Üíspatial features. The values\nof many subcarriers can represent how the signal prop-\nagates after diffraction, reÔ¨Çections, and scattering, and\nthus describe the spatial environment. These subcarriers\n4\nare seen as an analogy for image pixels, from which\nconvolutional layers can extract spatial features [41].\n2) Time dimension ‚Üítemporal features. For each sub-\ncarrier, its temporal dynamics indicate an environmental\nchange. In deep learning, the temporal dynamics are\nusually modeled by recurrent neural networks [42].\n3) Antenna dimension ‚Üíresolution and channel fea-\ntures. As each antenna captures a different propagation\npath of signals, it can be regarded as a channel in deep\nlearning that is similar to RGB channels of an image.\nIf only one pair of antennas exists, then the CSI data is\nsimilar to a gray image with only one channel. Hence,\nthe more antennas we have, the higher resolution the CSI\nhas. The antenna features should be processed separately\nin convolutional layers or recurrent neurons.\nIII. DEEP LEARNING MODELS FOR WIFI SENSING\nDeep learning enables models composed of many process-\ning layers to learn representations of data, which is a branch of\nmachine learning [41]. Compared to classic statistical learning\nthat mainly leverages handcrafted features designed by humans\nwith prior knowledge [64], deep learning aims to extract\nfeatures automatically by learning massive labeled data and\noptimizing the model by back-propagation [65]. The theories\nof deep learning were developed in the 1980s but they were\nnot attractive due to the need of enormous computational\nresources. With the development of graphical processing units\n(GPUs), deep learning techniques have become affordable,\nand has been widely utilized in computer vision [26], natural\nlanguage processing [29], and interdisciplinary research [66].\nA standard classiÔ¨Åcation model in deep learning is com-\nposed of a feature extractor and a classiÔ¨Åer. The classiÔ¨Åer\nnormally consists of several fully-connected layers that are\nable to learn a good decision boundary, while the design\nof the feature extractor is the key to the success. Extensive\nworks explore a large number of deep architectures for feature\nextractors [67], and each of them has speciÔ¨Åc advantages for\nsome type of data. The deep learning models for WiFi sensing\nare built on these prevailing architectures to extract patterns of\nhuman motions [5]. We summarize the latest works on deep\nmodels for WiFi sensing in Table I, and it is observed that the\nnetworks of these works are quite similar.\nIn the following, we introduce these key architectures and\nhow they are applied to WiFi sensing tasks. To better instanti-\nate these networks, we Ô¨Årstly formulate the normal WiFi CSI\nsensing task. The CSI data is deÔ¨Åned as x ‚ààRNa√óNs√óT\nwhere Na denotes the number of antennas, Ns denotes the\nnumber of subcarriers, and T denotes the duration. The CSI\ndata of each pair of antenna is relatively independent, and thus\nregarded as one channel of CSI, such as the RGB channels\nof image data. The deep learning model f(¬∑) aims to map\nthe data to the corresponding label: y = f(x), according to\ndifferent tasks. Denote Œ¶i(¬∑) and zi as the i-th layer of the\ndeep model and the feature of the i-th layer. After feature\nextraction, the classiÔ¨Åer is trained to seek a decision boundary\nin the feature space. In deep learning, the feature extractor\nis the key module that reduces the feature dimension while\npreserving the manifold [41]. With a discriminative feature\nspace, the choices of classiÔ¨Åers are Ô¨Çexible, which can be deep\nclassiÔ¨Åer (e.g., Multilayer Perceptron) or traditional classiÔ¨Åers\n(e.g., K-Nearest Neighbors, Support Vector Machine, and\nRandom Forest). Apart from the illustration, we visualize the\nintuition of how to feed these CSI data into various networks\nin Figure 3.\nA. Multilayer Perceptron\nMultilayer perceptron (MLP) [68] is one of the most classic\narchitectures and has played the classiÔ¨Åer role in most deep\nclassiÔ¨Åcation networks. It normally consists of multiple fully-\nconnected layers followed by activation functions. The Ô¨Årst\nlayer is termed the input layer that transforms the input data\ninto the hidden latent space, and after several hidden layers,\nthe last layer maps the latent feature into the categorical space.\nEach layer is calculated as\nŒ¶i(zi‚àí1) = œÉ(Wizi‚àí1),\n(3)\nwhere Wi is the parameters of Œ¶i, and œÉ(¬∑) is the activation\nfunction that aims to increase the non-linearity for MLP. The\ninput CSI has to be Ô¨Çattened to a vector and then fed into\nthe MLP, such that x ‚ààRNsT . Such a process mixes the\nspatial and temporal dimensions and damages the intrinsic\nstructure of CSI data. Despite this, the MLP can still work with\nmassive labeled data, because the MLP has a fully-connected\nstructure with a large number of parameters, yet leading to\nslow convergence and huge computational costs. Therefore,\nthough the MLP shows satisfactory performance, stacking\nmany layers in MLP is not common for feature learning, which\nmakes MLP usually serve as a classiÔ¨Åer. In WiFi sensing, MLP\nis commonly utilized as a classiÔ¨Åer [5], [46], [7], [32], [44],\n[34].\nB. Convolutional Neural Network\nConvolutional neural network (CNN) was Ô¨Årstly proposed\nfor image recognition tasks by LeCun [69]. It addresses the\ndrawbacks of MLP by weight sharing and spatial pooling.\nCNN models have achieved remarkable performances in clas-\nsiÔ¨Åcation problems of 2D data in computer vision [70], [71]\nand sequential data in speech recognition [72] and natural\nlanguage processing [73]. CNN learns features by stacking\nconvolutional kernels and spatial pooling operations. The\nconvolution operation refers to the dot product between a Ô¨Ålter\nk ‚ààRd and an input vector v ‚ààRd, deÔ¨Åned as follows:\nk ‚äóv = œÉ(kT v).\n(4)\nThe pooling operation is a down-sampling strategy that calcu-\nlates the maximum (max pooling) or mean (average pooling)\ninside a kernel. The CNNs normally consist of several con-\nvolutional layers, max-pooling layers, and the MLP classiÔ¨Åer.\nGenerally speaking, increasing the depth of CNNs can lead to\nbetter model capacity. Nevertheless, when the depth of CNN is\ntoo large (e.g., greater than 20 layers), the gradient vanishing\nproblem leads to degrading performance. Such degradation is\naddressed by ResNet [74], which uses the residual connections\nto reduce the difÔ¨Åculty of optimization.\n5\nTABLE I: A Survey of Existing Deep Learning Approaches for WiFi Sensing\nMethod\nYear\nTask\nModel\nPlatform\nStrategy\n[33]\n2017\nHuman Activity Recognition\nRNN, LSTM\nIntel 5300 NIC\nSupervised learning\nWiCount [43]\n2017\nPeople Counting\nMLP\nIntel 5300 NIC\nSupervised learning\nEI [44]\n2018\nHuman Activity Recognition\nCNN\nIntel 5300 NIC\nTransfer learning\nCrossSense [32]\n2018\nHuman IdentiÔ¨Åcation,Gesture Recognition\nMLP\nIntel 5300 NIC\nTransfer Ensemble learning\n[45]\n2018\nHuman Activity Recognition\nLSTM\nIntel 5300 NIC\nSupervised learning\nDeepSense [5]\n2018\nHuman Activity Recognition\nCNN-LSTM\nAtheros CSI Tool\nSupervised learning\nWiADG [25]\n2018\nGesture Recognition\nCNN\nAtheros CSI Tool\nTransfer learning\nWiSDAR [46]\n2018\nHuman Activity Recognition\nCNN-LSTM\nIntel 5300 NIC\nSupervised learning\nWiVi [7]\n2019\nHuman Activity Recognition\nCNN\nAtheros CSI Tool\nSupervised learning\nSiaNet [9]\n2019\nGesture Recognition\nCNN-LSTM\nAtheros CSI Tool\nFew-Shot learning\nCSIGAN [47]\n2019\nGesture Recognition\nCNN, GAN\nAtheros CSI Tool\nSemi-Supervised learning\nDeepMV [48]\n2020\nHuman Activity Recognition\nCNN (Attention)\nIntel 5300 NIC\nSupervised learning\nWIHF [49]\n2020\nGesture Recognition\nCNN-GRU\nIntel 5300 NIC\nSupervised learning\nDeepSeg [50]\n2020\nHuman Activity Recognition\nCNN\nIntel 5300 NIC\nSupervised learning\n[51]\n2020\nHuman Activity Recognition\nCNN-LSTM\nIntel 5300 NIC\nSupervised learning\n[38]\n2021\nHuman Activity Recognition\nLSTM\nNexmon CSI Tool\nSupervised learning\n[52]\n2021\nHuman Activity Recognition\nCNN\nNexmon CSI Tool\nSupervised learning\n[53]\n2021\nHuman Activity Recognition\nCNN\nIntel 5300 NIC\nFew-Shot learning\nWidar [34]\n2021\nHuman IdentiÔ¨Åcation, Gesture Recognition\nCNN-GRU\nIntel 5300 NIC\nSupervised learning\nWiONE [54]\n2021\nHuman IdentiÔ¨Åcation\nCNN\nIntel 5300 NIC\nFew-Shot learning\n[55]\n2021\nHuman Activity Recognition\nCNN, RNN, LSTM\nIntel 5300 NIC\nSupervised learning\nTHAT [56]\n2021\nHuman Activity Recognition\nTransformers\nIntel 5300 NIC\nSupervised learning\nWiGr [57]\n2021\nGesture Recognition\nCNN-LSTM\nIntel 5300 NIC\nSupervised learning\nMCBAR [58]\n2021\nHuman Activity Recognition\nCNN, GAN\nAtheros CSI Tool\nSemi-Supervised learning\nCAUTION [12]\n2022\nHuman IdentiÔ¨Åcation\nCNN\nAtheros CSI Tool\nFew-Shot learning\nCTS-AM [59]\n2022\nHuman Activity Recognition\nCNN (Attention)\nIntel 5300 NIC\nSupervised learning\nWiGRUNT [60]\n2022\nGesture Recognition\nCNN (Attention)\nIntel 5300 NIC\nSupervised learning\n[61]\n2022\nHuman Activity Recognition\nLSTM\nNexmon CSI Tool\nSupervised learning\nEfÔ¨ÅcientFi [36]\n2022\nHuman Activity Recognition, Human IdentiÔ¨Åcation\nCNN\nAtheros CSI Tool\nMulti-task Supervised learning\nRobustSense [62]\n2022\nHuman Activity Recognition, Human IdentiÔ¨Åcation\nCNN\nAtheros CSI Tool\nSupervised learning\nAutoFi [63]\n2022\nHuman Activity Recognition, Human IdentiÔ¨Åcation\nCNN-MLP\nAtheros CSI Tool\nUnsupervised learning\nIn WiFi sensing, the convolution kernel can operate on a\n2D patch of CSI data (i.e., Conv1D) that includes a spatial-\ntemporal feature, or on a 1D patch of each subcarrier of\nCSI data (i.e., Conv2D). For Conv2D, a 2D convolution\nkernel k2D ‚ààRh√ów operates on all patches of the CSI\ndata via the sliding window strategy to obtain the output of\nthe feature map, while the Conv1D only extracts the spatial\nfeature along the subcarrier dimension. The Conv2D can be\napplied independently as it considers both spatial and temporal\nfeatures, while the Conv1D is usually used with other temporal\nfeature learning methods. To enhance the capacity of CNN,\nmultiple convolution kernels with a random initialization pro-\ncess are used. The advantages of CNNs for WiFi sensing\nconsist of fewer training parameters and the preservation of\nthe subcarrier and time dimension in CSI data. However, the\ndisadvantage is that CNN has an insufÔ¨Åcient receptive Ô¨Åeld\ndue to the limited kernel size and thus fails to capture the\ndependencies that exceed the kernel size. Another drawback\nis that CNN stack all the feature maps of kernels equally,\nwhich has been revamped by an attention mechanism that\nassigns different weights in the kernel or spatial level while\nstacking features. For CSI data, due to the varying locations\nof human motions, the patterns of different subcarriers should\nhave different importance, which can be depicted by spatial\nattention in CTS-AM [59]. More attention techniques have\nbeen successfully developed to extract temporal-level, antenna-\nlevel and subcarrier-level features for WiFi sensing [48], [75],\n[76].\nC. Recurrent Neural Network\nRecurrent neural network (RNN) is one of the deepest\nnetwork architectures that can memorize arbitrary-length se-\nquences of input patterns. The unique advantage of RNN is\nthat it enables multiple inputs and multiple outputs, which\nmakes it very effective for time sequence data, such as\nvideo [77] and CSI [5], [78], [79]. Its principle is to create\ninternal memory to store historical patterns, which are trained\nvia back-propagation through time [80].\nFor a CSI sample x, we denote a CSI frame at the t as xt ‚àà\nRNs. The vanilla RNN uses two sharing matrices Wx, Wh to\ngenerate the hidden state ht:\nht = œÉ(Wxxt + Whht‚àí1),\n(5)\nwhere the activation function œÉ(¬∑) is usually Tanh or Sigmoid\nfunctions. RNN is designed to capture temporal dynamics, but\nit suffers from the vanishing gradient problem during back-\npropagation and thus cannot capture long-term dependencies\nof CSI data.\nD. Variants of RNN (LSTM)\nTo tackle the problem of long-term dependencies of RNN,\nLong-short term memory (LSTM) [81] is proposed by de-\nsigning several gates with varying purposes and mitigating\nthe gradient instability during training. The standard LSTM\nsequentially updates a hidden sequence by a memory cell\nthat contains four states: a memory state ct, an output gate\not that controls the effect of output, an input gate it and\na forget gate ft that decides what to preserve and forget in\nthe memory. The LSTM is parameterized by weight matrices\n6\nSubcarrier\nTime\n‚äó\nFlatten\nMLP\nPatch\nFilter\nCNN\n‚Ñé0\n‚Ñéùë°\nRNN\nTimestamp\nTransformer\nPosition Embedding\nNorm\nMulti-head\nAttention\nNorm\nMLP\nTransformer\nEncoder\nsoftmax ùëÑùêæùëá\nùëëùëò\nùëâ\nùëÑ\nùêæ\nùëâ\nCSI Data\nClassifier\nPrediction\nùëÉ(ùë¶|ùë•)\nMLP\nKNN\nSVM\nRandom\nForest\nPair of antennas\nOne channel\nMultiple channels\nMIMO\nFig. 3: The illustration of how CSI data is processed by MLP, CNN, RNN and Transformer.\nWi, Wf, Wc, Wo, Ui, Uf, Uc, Uo and biases bi, bf, bc, bo, and\nthe whole update is performed at each t ‚àà{1, ..., T}:\nit = œÉ(Wixt + Uiht‚àí1 + bi),\n(6)\nft = œÉ(Wfxt + Ufht‚àí1 + bf),\n(7)\nÀúct = tanh(Wcxt + Ucht‚àí1 + bc),\n(8)\nct = it ‚äôÀúct + ft ‚äôct‚àí1,\n(9)\not = œÉ(Woxt + Uoht‚àí1 + bo),\n(10)\nht = ot ‚äôtanh(ct),\n(11)\nwhere œÉ is a Sigmoid function.\nApart from the LSTM cell [82], [83], [84], the multi-\nlayer and bi-directional structure further boost the model\ncapacity. The bidirectional LSTM (BiLSTM) model processes\nthe sequence in two directions and concatenates the features of\nthe forward input `x and backward input ¬¥x. It has been proven\nthat BiLSTM shows better results than LSTM in [45], [85].\nE. Recurrent Convolutional Neural Network\nThough LSTM addresses the long-term dependency, it leads\nto a large computation overhead. To overcome this issue,\nGated Recurrent Unit (GRU) is proposed. GRU combines\nthe forget gate and input gate into one gate, and does not\nemploy the memory state in LSTM, which simpliÔ¨Åes the model\nbut can still capture long-term dependency. GRU is regarded\nas a simple yet effective version of LSTM. Leveraging the\nsimple recurrent network, we can integrate the Conv1D and\nGRU to extract spatial and temporal features, respectively.\n[86], [87] show that CNN-GRU is effective for human activity\nrecognition. In WiFi sensing, DeepSense [5] Ô¨Årstly proposes\nConv2D with LSTM for human activity recognition. CNN-\nGRU is also revamped for CSI-based human gesture recogni-\ntion in Widar [34]. SiaNet [9] further proposes Conv1D with\nBiLSTM for few-shot gesture recognition. As they perform\nquite similarly, we use CNN-GRU with fewer parameters in\nthis paper for the benchmark.\nF. Transformer\nTransformer [88] was Ô¨Årstly proposed for NLP applications\nto extract sequence embeddings by exploiting attention of\nwords, and then it was extended to the computer vision Ô¨Åeld\nwhere each patch is regarded as a word and one image\nconsists of many patches [89]. The vanilla consists of an\nencoder and a decoder to perform machine translation, and\nonly the encoder is what we need. The transformer block\nis composed of a multi-head attention layer, a feed-forward\nneural network (MLP), and layer normalization. Since MLP\nhas been explained in previous section, we mainly introduce\nthe attention mechanism in this section. For a CSI sample\nx, we Ô¨Årst divide it into P patches xp ‚ààRh√ów, of which\neach patch has contained spatial-temporal features. Then these\npatches are concatenated and added by positional embeddings\nthat infer the spatial position of patches, which makes the\ninput matrix v ‚ààRdk where dk = P √ó hw. This matrix is\ntransformed into three different matrices via linear embedding:\nthe query Q, the key K, and the value V . The self-attention\nprocess is calculated by\nAttention(Q, K, V ) = softmax(Q ¬∑ KT\n‚àödk\n) ¬∑ V.\n(12)\n7\nIntuitively, such a process calculates the attention of any\ntwo patches via dot product, i.e., cosine similarity, and then\nthe weighting is performed with normalization to enhance\ngradient stability for improved training. Multi-head attention\njust repeats the self-attention several times and enhances\nthe diversity of attentions. The transformer architecture can\ninterconnect with every patch of CSI, which makes it strong\nif given sufÔ¨Åcient training data, such as THAT [56]. However,\ntransformer has a great number of parameters that makes the\ntraining cost expensive, and enormous labeled CSI data is hard\nto collect, which makes transformers not really attractive for\nthe supervised learning.\nG. Generative Models\nDifferent from the aforementioned discriminative models\nthat mainly conducts classiÔ¨Åcation, generative models aim to\ncapture the data distribution of CSI. Generative Adversarial\nNetwork (GAN) [90] is a classic generative model that learns\nto generate real-like data via an adversarial game between\na generative network and a discriminator network. In WiFi\nsensing, GAN helps deal with the environmental dependency\nby generating labeled samples in the new environment from\nthe well-trained environment [47], [58]. GAN also inspires\ndomain-adversarial training that enables deep models to learn\ndomain-invariant representations for the training and real-\nworld testing environments [25], [91], [92], [93]. Variational\nnetwork [94] is another common generative model that maps\nthe input variable to a multivariate latent distribution. Varia-\ntional autoencoder learns the data distribution by a stochas-\ntic variational inference and learning algorithm [94], which\nhas been used in CSI-based localization [95], [96] and CSI\ncompression [36]. For instance, EfÔ¨ÅcientFi [36] leverages the\nquantized variational model to compress the CSI transmission\ndata for large-scale WiFi sensing in the future.\nIV. LEARNING METHODS FOR DEEP WIFI SENSING\nMODELS\nTraditional training of deep models relies on supervised\nlearning with massive labeled data, but the data collection\nand annotation is a bottleneck in the realistic WiFi sensing\napplications. For example, to recognize human gestures, we\nmay need the volunteers to perform gestures for a hundred\ntimes, which is not realistic. In this section, as shown in\nFigure 4, we illustrate the learning methods and how they\ncontribute to WiFi sensing in the real world.\nSupervised Learning is an approach to training deep\nmodels using input data that has been labeled for a particular\noutput. It is the most common learning strategy in current WiFi\nsensing works [33], [5], [46], [7]. They usually adopt cross-\nentropy loss between the ground truth label and the prediction\nfor model optimization. Though supervised learning is easy to\nimplement and achieves high performance for many tasks, its\nrequirement of tremendous labeled data hinders its pervasive\nrealistic applications.\nFew-shot Learning is a data-efÔ¨Åcient learning strategy that\nonly utilizes several samples of each category for training. This\nis normally achieved by contrastive learning or prototypical\nSupervised Learning\nùë•, ùë¶\nLabeled Data\n‡∑úùë¶\nPrediction\nFew-Shot Learning\nùë•, ùë¶\nFew Data\n‡∑úùë¶\nPrediction\nTransfer Learning\nùë•, ùë¶\nPublic Data\nUnsupervised Learning\nEnsemble Learning\nùë•, ùë¶\n‡∑úùë¶\nPrediction\nùë•, ùë¶\nUser Data\nPre-train\nFine-tune\nùë•\nUnlabeled\nData\nùë•, ùë¶\nUser Data\nFine-tune\nLabeled Data\nFig. 4: The illustration of the learning strategies.\nlearning. It is Ô¨Årstly exploited for WiFi sensing in SiaNet [9]\nthat proposes a Siamese network for few-shot learning. Sub-\nsequent works [54], [12] extend prototypical networks from\nvisual recognition to WiFi sensing, also achieving good recog-\nnition results. Specially, when only one sample for each class\nis employed for training, we term it as one-shot learning. As\nonly a few samples are required, few-shot learning contributes\nto WiFi-based gesture recognition and human identiÔ¨Åcation in\npractice.\nTransfer Learning aims to transfer knowledge from one\ndomain to another domain [97]. When the two domains are\nsimilar, we pretrain the model on one domain and Ô¨Åne-\ntune the model in a new environment, which can lead to\nsigniÔ¨Åcant performance. When the two domains are distinct,\nsuch as the different environments of CSI data, the distribution\nshift hinders the performance so domain adaptation should be\nadopted. Domain adaptation is a category of semi-supervised\nlearning that mitigates the domain shift for transfer learning.\nCross-domain scenarios are quite common in WiFi sensing\nscenarios since the CSI data is highly dependent on the training\nenvironment. Many works have been developed to deal with\nthis problem [25], [44], [58], [98], [99].\nUnsupervised Learning aims to learn data representations\nwithout any labels. Then the feature extractor can facilitate\n8\ndown-streaming tasks by training a speciÔ¨Åc classiÔ¨Åer. From\nthe experience of visual recognition tasks [100], unsupervised\nlearning can even enforce the model to gain better generaliza-\ntion ability since the model is not dependent on any speciÔ¨Åc\ntasks. Current unsupervised learning models are based on self-\nsupervised learning [101]. Despite its effectiveness, the unsu-\npervised learning has not been well exploited in WiFi sensing,\nand only AutoFi is developed to enable model initialization for\nautomatic user setup in WiFi sensing applications [63].\nEnsemble Learning uses multiple models to obtain bet-\nter predictive performance [102]. The ensemble process can\noperate on feature level or prediction level. Feature-level\nensemble concatenates the features from multiple models\nand one Ô¨Ånal classiÔ¨Åer is trained. Prediction-level ensemble\nis more common, usually referring to voting or probability\naddition. Ensemble learning can increase the performance but\nthe computation overhead also explodes by multiple times.\nCrossSense [44] develops a mixture-of-experts approach and\nonly chooses the appropriate expert for a speciÔ¨Åc input,\naddressing the computation cost.\nIn this paper, we empirically explore the effectiveness of\nsupervised learning, transfer learning and unsupervised learn-\ning for WiFi CSI data, as they are the most commonly used\nlearning strategies in WiFi sensing applications.\nV. EMPIRICAL STUDIES OF DEEP LEARNING IN WIFI\nSENSING: A BENCHMARK\nIn this section, we conduct an empirical study of the\naforementioned deep learning models on WiFi sensing data\nand Ô¨Årstly provide the benchmarks with open-source codes in\nhttp://www.github.com/. The four datasets are illustrated Ô¨Årst,\nand then we evaluate the deep models on these datasets in\nterms of three learning strategies. Eventually, some detailed\nanalytics are conducted on the convergence of optimization,\nnetwork depth, and network selection.\nA. Datasets\nWe choose two public CSI datasets (UT-HAR [33] and\nWidar [34]) collected using Intel 5300 NIC. To validate the\neffectiveness of deep learning models on CSI data of different\nplatforms, we collect two new datasets using Atheros CSI\nTool [2] and our embedded IoT system [22], namely NTU-\nFi HAR and NTU-Fi Human-ID. The details and statistics of\nthese datasets are summarized in Table II.\nUT-HAR [33] is the Ô¨Årst public CSI dataset for human\nactivity recognition. It consists of seven categories and is\ncollected via Intel 5300 NIC with 3 pairs of antennas that\nrecord 30 subcarriers per pair. All the data is collected in the\nsame environment. However, its data is collected continuously\nand has no golden labels for activity segmentation. Following\nexisting works [56], the data is segmented using a sliding win-\ndow, inevitably causing many repeated data among samples.\nHence, though the total number of samples reaches around\n5000, it is a small dataset with intrinsic drawbacks.\nWidar [34] is the largest WiFi sensing dataset for gesture\nrecognition, which is composed of 22 categories and 43K\nsamples. It is collected via Intel 5300 NIC with 3 √ó 3 pairs\nof antennas in many distinct environments. To eliminate the\nenvironmental dependencies, the data is processed to the body-\ncoordinate velocity proÔ¨Åle (BVP).\nNTU-Fi is our proposed dataset for this benchmark that\nincludes both human activity recognition (HAR) and human\nidentiÔ¨Åcation (Human ID) tasks. Different from UT-HAR and\nWidar, our dataset is collected using Atheros CSI Tool and has\na higher resolution of subcarriers (114 per pair of antennas).\nEach CSI sample is perfectly segmented. For the HAR dataset,\nwe collect the data in three different layouts. For the Human ID\ndataset, we collect the human walking gaits in three situations:\nwearing a T-shirt, a coat, or a backpack, which brings many\ndifÔ¨Åculties. The NTU-Fi data is simultaneously collected in\nthese works [36], [12] that describe the detailed layouts for\ndata collection.\nB. Implementation Details\nWe normalize the data for each dataset and implement\nall the aforementioned methods using the PyTorch frame-\nwork [103]. To ensure the convergence, we train the UT-HAR,\nWidar, and NTU-Fi for 200, 100, and 30 epochs, respectively,\nfor all the models except RNN. As the vanilla RNN is hard\nto converge due to the gradient vanishing, we train them for\ntwo times of the speciÔ¨Åed epochs. We use the Adam optimizer\nwith a learning rate of 0.001, and the beta of 0.9 and 0.999.\nWe follow the original Adam paper [104] to set these hyper-\nparameters. The ratio of training and testing splits is 8:2 for\nall datasets using stratiÔ¨Åed sampling.\nC. Baselines and Criterion\nWe design the baseline networks of MLP, CNN, RNN,\nGRU, LSTM, BiLSTM, CNN+GRU, and Transformer follow-\ning the experiences learned from existing works in Table I.\nThe CNN-5 is modiÔ¨Åed from LeNet-5 [69]. We further in-\ntroduce the series of ResNet [74] that have deeper layers.\nThe transformer network is based on the vision transformer\n(ViT) [89] so that each patch can contain spatial and temporal\ndimensions. It is found that given sufÔ¨Åcient parameters and\nreasonable depth of layers, they can converge to more than\n98% in the training split. Since the data sizes of UT-HAR,\nWidar and NTU-Fi are different, we use a convolutional layer\nto map them into a uniÔ¨Åed size, which enables us to use the\nsame network architecture. The speciÔ¨Åc network architectures\nfor all models are illustrated in the Appendix. The hyper-\nparameters of the networks have been tuned to ensure the\nsatisfactory convergence. To compare the baseline models,\nwe select three classic criteria: accuracy (Acc) that evaluates\nthe prediction ability, Ô¨Çoating-point operations (Flops) that\nevaluates the computational complexity, and the number of\nparameters (Params) that measures the requirement of GPU\nmemory. As WiFi sensing is usually performed on the edge,\nthe Flops and Params also matter with limited resources.\nD. Evaluations of Different Deep Architectures\nOverall Comparison. We summarize the performance of\nall baseline models in Table III. On UT-HAR, the ResNet-18\n9\nTABLE II: Statistics of four CSI datasets for our SenseFi benchmarks.\nDatasets\nUT-HAR [33]\nWidar [34]\nNTU-Fi HAR [36]\nNTU-Fi Human-ID [58]\nPlatform\nIntel 5300 NIC\nIntel 5300 NIC\nAtheros CSI Tool\nAtheros CSI Tool\nCategory Number\n7\n22\n6\n14\nCategory Names\nLie down, Fall, Walk, Pick up\nRun, Sit down, Stand up\nPush&Pull, Sweep, Clap, Slide,\n18 types of Draws\nBox, Circle, Clean,\nFall, Run, Walk\nGaits of 14 Subjects\nData Size\n(3,30,250)\n(antenna, subcarrier, packet)\n(22,20,20)\n(time, x velocity, y velocity)\n(3,114,500)\n(antenna, subcarrier, packet)\n(3,114,500)\n(antenna, subcarrier, packet)\nTraining Samples\n3977\n34926\n936\n546\nTesting Samples\n996\n8726\n264\n294\nTraining Epochs\n200\n100\n30\n30\nTABLE III: Evaluation of deep neural networks (using supervised learning) on four datasets. (Bold: best; Underline: 2nd best)\nDataset\nUT-HAR\nWidar\nNTU-Fi HAR\nNTU-Fi Human-ID\nMethod\nAcc (%)\nFlops (M)\nParams (M)\nAcc (%)\nFlops (M)\nParams (M)\nAcc (%)\nFlops (M)\nParams (M)\nAcc (%)\nFlops (M)\nParams (M)\nMLP\n92.00\n23.17\n23.170\n67.24\n9.15\n9.150\n99.69\n175.24\n175.240\n93.91\n175.24\n175.240\nCNN-5\n97.61\n31.68\n0.296\n70.19\n3.38\n0.299\n98.70\n28.24\n0.477\n97.14\n28.24\n0.478\nResNet18\n98.11\n49.93\n11.180\n71.70\n38.39\n11.250\n95.31\n54.19\n11.180\n96.42\n54.19\n11.190\nResNet50\n97.21\n86.40\n23.550\n68.56\n69.70\n23.640\n99.38\n90.66\n23.550\n92.91\n90.67\n23.570\nResNet101\n94.99\n162.58\n42.570\n68.71\n145.87\n42.660\n95.31\n166.83\n42.570\n88.40\n166.85\n42.590\nRNN\n83.53\n2.51\n0.010\n47.05\n0.66\n0.031\n84.64\n13.09\n0.027\n89.30\n13.09\n0.027\nGRU\n94.18\n7.60\n0.030\n62.50\n1.98\n0.091\n97.66\n39.39\n0.079\n98.96\n39.39\n0.079\nLSTM\n87.18\n10.14\n0.040\n63.35\n2.64\n0.121\n97.14\n52.54\n0.105\n97.19\n52.54\n0.105\nBiLSTM\n90.19\n20.29\n0.080\n63.43\n5.28\n0.240\n99.69\n105.09\n0.209\n99.38\n105.09\n0.210\nCNN + GRU\n96.72\n39.99\n1.430\n63.19\n3.34\n0.092\n93.75\n48.38\n0.058\n87.48\n48.39\n0.058\nViT\n96.53\n273.10\n10.580\n67.72\n9.28\n0.106\n93.75\n501.64\n1.052\n76.84\n501.64\n1.054\nMLP\nCNN-5\nResNet18 ResNet50ResNet101\nGRU\nLSTM\nBiLSTM CNN+GRU\nViT\nRNN\nModel\n50\n60\n70\n80\n90\n100\nBest Test Accuracy (%)\nC oss\na ase\nUT-HAR\nNTU-Fi HAR\nNTU-Fi HumanID\nWidar\nFig. 5: The performance comparison across four datasets.\nachieves the best accuracy of 98.11% and the CNN-5 achieves\nthe second best. The shallow CNN-5 can attain good results\non all datasets but the deep ResNet-18 fails to generalize on\nWidar, which will be explained in Section V-F. The BiLSTM\nyields the best performance on two NTU-Fi benchmarks. To\ncompare these results, we visualize them in Figure 5, from\nwhich we can conclude the observations:\n‚Ä¢ The MLP, CNN, GRU, LSTM, and Transformer can\nachieve satisfactory results on all benchmarks.\n‚Ä¢ The MLP, GRU, and CNN show stable and superior\nperformances when they are compared to others.\n‚Ä¢ The very deep networks (i.e., the series of ResNet) per-\nform well on UT-HAR and Widar, but do not perform bet-\nter than simple CNN on NTU-Fi. The performance does\nnot increase as the number of network layers increases,\nwhich is different from visual recognition results [74].\nCompared to simple CNN-5, the improvement margin is\nquite limited.\n‚Ä¢ The RNN is worse than LSTM and GRU.\n‚Ä¢ The transformer cannot work well when only limited\ntraining data is available in NTU-Fi Human-ID.\n‚Ä¢ The models show inconsistent performances on different\ndatasets, as the Widar dataset is much more difÔ¨Åcult.\nComputational Complexity. The Flops value shows the\ncomputational complexity of models in Table III. The vanilla\nRNN has low complexity but cannot perform well. The GRU\nand CNN-5 are the second-best models and simultaneously\ngenerate good results. It is also noteworthy that the ViT\n(transformer) has a very large computational complexity as\nit is composed of many MLPs for feature embedding. Since\nits performance is similar to that of CNN, MLP, and GRU,\nthe transformer is not suitable for supervised learning tasks in\nWiFi sensing.\nModel Parameters. The number of model parameters\ndetermines how many GPU memories are occupied during\ninference. As shown in Table III, the vanilla RNN has the\nsmallest parameter size and then is followed by the CNN-5\nand CNN-GRU. The parameter sizes of CNN-5, RNN, GRU,\nLSTM, BiLSTM, and CNN-GRU are all small and acceptable\nfor model inference in the edge. Considering both the Params\nand Acc, CNN-5, GRU, BiLSTM, and CNN-GRU are good\nchoices for WiFi sensing. Though the model parameters can\nbe reduced by model pruning [105], quantization [106] or Ô¨Åne-\ntuning the hyper-parameters, here we only evaluate the pure\nmodels that have the minimum parameter sizes to converge in\nthe training split.\nE. Evaluations of Learning Schemes\nApart from supervised learning, other learning schemes are\nalso useful for realistic applications of WiFi sensing. Here we\n10\n0\n20\n40\n60\n80\n100\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nTraining Loss\nMLP\nCNN-5\nViT\nRNN\n0\n20\n40\n60\n80\n100\nEpoch\n0.5\n1.0\n1.5\n2.0\n2.5\nTraining Loss\nRNN\nGRU\nLSTM\nBiLSTM\nCNN+GRU\nFig. 6: The training losses of all baseline models on NTU-Fi\nHuman-ID with pre-trained parameters of NTU-Fi HAR.\nevaluate two prevailing learning strategies on these models.\nTABLE IV: Evaluations on Transfer Learning\nMethod\nAccuracy (%)\nFlops (M)\nParams (M)\nMLP\n84.46\n175.24\n175.240\nCNN-5\n96.35\n28.24\n0.478\nResNet18\n85.94\n54.19\n11.190\nResNet50\n79.21\n90.67\n23.570\nResNet101\n68.88\n166.85\n42.590\nRNN\n57.84\n13.09\n0.027\nGRU\n75.89\n39.39\n0.079\nLSTM\n71.98\n52.54\n0.105\nBiLSTM\n80.20\n105.09\n0.210\nCNN + GRU\n51.73\n48.39\n0.059\nViT\n66.20\n501.64\n1.054\nEvaluations on Transfer Learning The transfer learning\nexperiments are conducted on NTU-Fi. We transfer the model\nfrom the HAR to Human-ID by pre-training the model in\nHAR (whole dataset) and then Ô¨Åne-tuning a new classiÔ¨Åer in\nHuman-ID (training split). This simulates the situation when\nwe train the model using massive labeled data collected in\nthe lab, and then use a few data to realize customized tasks\nfor users. The human activities in HAR and human gaits in\nHuman-ID are composed of human motions, and thus the\nfeature extractor should learn to generalize across these two\ntasks. We evaluate this setting for all baseline models and\nthe results are shown in Table IV. It is observed that the CNN\nfeature extractor has the best transferability, achieving 96.35%\non the Human-ID task. Similar to CNN, the MLP and BiLSTM\nalso have such capacity. However, the RNN, CNN+GRU,\nand ViT only achieve 57.84%, 51.73%, and 66.20%, which\ndemonstrates their weaker capacity for transfer learning. This\ncan be caused by the overÔ¨Åtting phenomenon, such as the\nsimple RNN that only memorizes the speciÔ¨Åc patterns for\nHAR but cannot recognize the new patterns. This can also\nbe caused by the mechanism of feature learning. For example,\nthe transformer (ViT) learns the connections of local patches\nby self-attention, but such connections are different between\nHAR and Human-ID. Recognizing different activities relies on\nthe difference of a series of motions, but most human gaits are\nso similar that only subtle patterns can be an indicator for gait\nidentiÔ¨Åcation.\nEvaluations on Unsupervised Learning We further exploit\nthe effectiveness of unsupervised learning for CSI feature\nlearning. We follow the AutoFi [63] to construct two parallel\nnetworks and adopt the KL-divergence, mutual information,\nand kernel density estimation loss to train the two networks\nonly using the CSI data. After unsupervised learning, we train\nthe independent classiÔ¨Åer based on the Ô¨Åxed parameters of\nthe two networks. All the backbone networks are tested using\nthe same strategy: unsupervised training on NTU-Fi HAR and\nsupervised learning on NTU-Fi Human-ID. The evaluation is\nconducted on Human-ID, and the results are shown in Table V.\nIt is shown that CNN achieves the best accuracy of 97.62%\nthat is followed by MLP and ViT. The results demonstrate that\nunsupervised learning is effective for CSI data. It yields better\ncross-task evaluation results than those of transfer learning,\nwhich demonstrates that unsupervised learning helps learn\nfeatures with better generalization ability. CNN and MLP-\nbased networks are more friendly for unsupervised learning\nof WiFi CSI data.\nTABLE V: Evaluations on Unsupervised Learning\nMethod\nAccuracy (%)\nFlops (M)\nParams (M)\nclassiÔ¨Åer1\nclassiÔ¨Åer2\nMLP\n90.48\n89.12\n175.24\n175.240\nCNN-5\n96.26\n97.62\n28.24\n0.478\nResNet18\n85.03\n82.99\n54.19\n11.190\nResNet50\n47.28\n45.58\n90.67\n23.570\nResNet101\n36.05\n35.37\n166.85\n42.590\nRNN\n53.74\n51.36\n13.09\n0.027\nGRU\n65.99\n64.63\n39.39\n0.079\nLSTM\n53.06\n55.10\n52.54\n0.105\nBiLSTM\n51.36\n55.78\n105.09\n0.210\nCNN + GRU\n50.34\n53.40\n48.39\n0.059\nViT\n78.91\n84.35\n501.64\n1.054\nF. Analysis\nConvergence of Deep Models. Though all models converge\neventually, their training difÔ¨Åculties are different and further\naffect their practical usage. To compare their convergence\ndifÔ¨Åculties, we show the training losses of MLP, CNN-5, ViT,\nand RNN in terms of epochs in Figure 7. It is noted that CNN\nconverges very fast within 25 epochs for four datasets, and\nMLP also converges at a fast speed. The transformer requires\nmore epochs of training since it consists of more model pa-\nrameters. In comparison, RNN hardly converges on UT-HAR\nand Widar, and converges slower on NTU-Fi. Then we further\nexplore the convergence of RNN-based models, including\nGRU, LSTM, BiLSTM, and CNN+GRU in Figure 8. Though\nthere show strong Ô¨Çuctuations during the training phase of\nGRU, LSTM, and BiLSTM, these three models can achieve\nmuch lower training loss. Especially, GRU achieves the lowest\nloss among all RNN-based methods. For CNN+GRU, the\ntraining phase is more stable but its convergence loss is larger\nthan others.\nHow Transfer Learning Matters. We further draw the\ntraining losses of all models on NTU-Fi Human-ID with pre-\ntrained parameters of NTU-Fi HAR in Figure 6. Compared\nto the training procedures of randomly-initialized models in\nFigures 7(c) and 8(c), the convergence can be achieved and\neven become much more stable. We can draw two conclusions\nfrom these results: (a) the feature extractors of these models\nare transferable across two similar tasks; (b) the Ô¨Çuctuations of\ntraining losses are caused by the feature extractor since only\nthe classiÔ¨Åer is trained for the transfer learning settings.\n11\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\nTraining Loss\nMLP\nCNN-5\nViT\nRNN\n(a) UT-HAR\n0\n20\n40\n60\n80\n100\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nTraining Loss\nMLP\nCNN-5\nViT\nRNN\n(b) Widar\n0\n5\n10\n15\n20\n25\n30\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nTraining Loss\nMLP\nCNN-5\nViT\nRNN\n(c) NTU-Fi HAR\n0\n10\n20\n30\n40\n50\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nTraining Loss\nMLP\nCNN-5\nViT\nRNN\n(d) NTU-Fi Human-ID\nFig. 7: The training losses of MLP, CNN, Transformer, RNN for the four datasets.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpoch\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nTraining Loss\nRNN\nGRU\nLSTM\nBiLSTM\nCNN+GRU\n(a) UT-HAR\n0\n20\n40\n60\n80\n100\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTraining Loss\nRNN\nGRU\nLSTM\nBiLSTM\nCNN+GRU\n(b) Widar\n0\n5\n10\n15\n20\n25\n30\nEpoch\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nTraining Loss\nRNN\nGRU\nLSTM\nBiLSTM\nCNN+GRU\n(c) NTU-Fi HAR\n0\n10\n20\n30\n40\n50\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nTraining Loss\nRNN\nGRU\nLSTM\nBiLSTM\nCNN+GRU\n(d) NTU-Fi Human-ID\nFig. 8: The training losses of RNN-based models for the four datasets.\n0\n20\n40\n60\n80\n100\nEpoch\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nResNet18-training\nResNet50-training\nResNet101-training\nResNet18-testing\nResNet50-testing\nResNet101-testing\n(a) Accuracy\n0\n20\n40\n60\n80\n100\nEpoch\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nMLP-training\nCNN-5-training\nGRU-training\nMLP-testing\nCNN-5-testing\nGRU-testing\n(b) Accuracy\nFig. 9: The training procedures of deep learning models on\nWidar data in terms of training and testing accuracy.\nPoor Performance of Deep CNN on Widar. In Table III,\na noticeable phenomenon is that the ResNet-18/50/101 can-\nnot generalize well on Widar data, only achieving 17.91%,\n19.47%, and 14.47%, respectively. In visual recognition, a\ndeeper network should perform better on large-scale datasets\n[74]. Then we have the question: is the degeneration of\nthese deep models caused by underÔ¨Åtting or overÔ¨Åtting in\nWiFi sensing? We seek the reason by plotting their training\nlosses in Figure 9. Figure 9(a) shows that even though the\ntraining accuracy has been almost 100%, the testing accuracy\nremains low, under 20%. Whereas, other networks (MLP,\nCNN, GRU) have similar training accuracy while the testing\naccuracy is increased to over 60%. This indicates that the\ndegrading performances of ResNets are caused by overÔ¨Åtting,\nand different domains in Widar [34] might be the main\nreasons. This discovery tells us that very deep networks are\nprone to suffer from overÔ¨Åtting for cross-domain tasks and\nmay not be a good choice for current WiFi sensing applications\ndue to their performance and computational overhead.\n0\n20\n40\n60\n80\n100\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\nTraining Loss\nResNet18-Adam\nResNet50-Adam\nResNet101-Adam\n(a) Training using Adam\n0\n20\n40\n60\n80\n100\nEpoch\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nTraining Loss\nResNet18-SGD\nResNet50-SGD\nResNet101-SGD\n(b) Training using SGD\nFig. 10: The training procedures of ResNet-18/50/101 using\nAdam and SGD optimizers on UT-HAR.\nChoices of Optimizer. During the training phase, we Ô¨Ånd\nthat though Adam can help models converge at a fast speed,\nit also leads to much training instability, especially for the\nvery deep neural networks. In Figure 10(a), we can see that\nResNet-18 converges stably but ResNet-50 and ResNet-101\nhave Ô¨Çuctuating losses every 20-30 epochs. This might be\ncaused by the dramatically changing values of WiFi data and\nits adaptive learning rate of Adam [104]. Then we consider\nchanging the optimizer from Adam to a more stable optimizer,\nStochastic Gradient Descent (SGD). In Figure 10(b), we Ô¨Ånd\nthat the training procedure becomes more stable. This implies\nthat if a very deep model is utilized in WiFi sensing, the SGD\nshould be a better choice. If a simple model is sufÔ¨Åcient for the\nsensing task, then Adam can enforce the model to converge\nbetter and faster.\nVI. DISCUSSIONS AND SUMMARY\nHaving analyzed the empirical results and the characteristics\nof deep learning models for WiFi sensing, we summarize the\n12\nexperiences and observations that facilitate future research on\nmodel design, model training, and real world use case:\n‚Ä¢ Model Choices. We recommend CNN, GRU, and BiL-\nSTM due to their high performance, low computational\ncost, and small parameter size. The shallow models\nhave achieved remarkable results for activity recognition,\ngesture recognition, and human identiÔ¨Åcation, while the\nvery deep models confront the overÔ¨Åtting issue, especially\nfor cross-domain scenarios.\n‚Ä¢ Optimization. We recommend using Adam or SGD\noptimizer. The Adam optimizer enforces the model to\nconverge at a fast speed but sometimes it causes instabil-\nity of training. When such a situation happens, the SGD is\na more secure way but the hyper-parameters of SGD (i.e.,\nthe learning rate and momentum) need to be manually\nspeciÔ¨Åed and tuned.\n‚Ä¢ Advice on Transfer Learning Applications. We rec-\nommend applying transfer learning when the task is\nsimilar to existing applications and the same CSI sensing\nplatform is employed. The pre-trained parameters provide\na good initialization and better generalization ability.\nCNN, MLP, and BiLSTM have superior transferability.\n‚Ä¢ Advice on Unsupervised Learning. We recommend\napplying unsupervised learning to initialize the model for\nsimilar tasks since unsupervised learning extracts more\ngeneralizable features than transfer learning. CNN, MLP,\nand ViT are more suitable in the unsupervised learning\nframework in general.\nVII. GRAND CHALLENGES AND FUTURE DIRECTIONS\nDeep learning still keeps booming in many research Ô¨Åelds\nand continuously empowers more challenging applications and\nscenarios. Based on the new progress, we look into the future\ndirections of deep learning for WiFi sensing and summarize\nthem as follows.\nData-efÔ¨Åcient learning. As CSI data is expensive to collect,\ndata-efÔ¨Åcient learning methods should be further explored.\nExisting works have utilized few-shot learning, transfer learn-\ning, and domain adaptation, which yield satisfactory results in\na new environment with limited training samples. However,\nsince the testing scenarios are simple, the transferability of\nthese models cannot be well evaluated. In the future, meta-\nlearning and zero-shot learning can further help learn robust\nfeatures across environments and tasks.\nModel compression or lightweight model design. In the\nfuture, WiFi sensing requires real-time processing for certain\napplications, such as vital sign monitoring [107]. To this end,\nmodel compression techniques can play a crucial role, such as\nmodel pruning [106], quantization [105] and distillation [108],\nwhich decreases the model size via an extra learning step.\nThe lightweight model design is also favorable, such as the\nEfÔ¨ÅcientNet [109] in computer vision that is designed from\nscratch by balancing network depth, width, and resolution.\nMulti-modal learning. WiFi sensing is ubiquitous, cost-\neffective, and privacy-preserving, and can work without the\neffect of illumination and part of occlusion, which is comple-\nmentary to the existing visual sensing technique. To achieve\nrobust sensing 24/7, multiple modalities of sensing data should\nbe fused using multi-modal learning. WiVi [7] pioneers human\nactivity recognition by integrating WiFi sensing and visual\nrecognition. Multi-modal learning can learn joint features from\nmultiple modalities and make decisions by choosing reliable\nmodalities.\nCross-modal learning. WiFi CSI data describes the sur-\nrounding environment that can also be captured by cameras.\nCross-modal learning aims to supervise or reconstruct one\nmodality from another modality, which helps WiFi truly ‚Äúsee‚Äù\nthe environment and visualize them in videos. Wi2Vi [110]\nmanages to generate video frames by CSI data and Ô¨Årstly\nachieves cross-modal learning in WiFi sensing. The human\npose is then estimated by supervising the model by the pose\nlandmarks of OpenPose [111]. In the future, cross-modal\nlearning may enable the WiFi model to learn from more\nsupervisions such as radar and Lidar.\nModel robustness and security for trustworthy sensing.\nWhen deploying WiFi sensing models in the real world, the\nmodel should be secure to use. Adversarial attacks have raised\nattentions in video-based human sensing [112]. Nevertheless,\nexisting works study the accuracy of models but few pay\nattention to the security issue. First, during the communication,\nthe sensing data may leak the privacy of users. Second, if\nany adversarial attack is made on the CSI data, the modal\ncan perform wrongly and trigger the wrong actions of smart\nappliances. RobustSense seeks to overcome adversarial attacks\nby augmentation and adversarial training [63]. EfÔ¨ÅcientFi\nproposes a variational auto-encoder to quantize the CSI for\nefÔ¨Åcient and robust communication. WiFi-ADG [113] protects\nthe user privacy by enforcing the data not recognizable by\ngeneral classiÔ¨Åers. More works should be focused on secure\nWiFi sensing and establish trustworthy models for large-scale\nsensing, such as federated learning.\nComplicated human activities and behaviors analytics.\nWhile current methods have shown prominent recognition\naccuracy for single activities or gestures, human behavior\nis depicted by more complicated activities. For example, to\nindicate if a patient may have a risk of Alzheimer‚Äôs disease,\nthe model should record the routine and analyze the anomaly\nactivity, which is still difÔ¨Åcult for existing approaches. Pre-\ncise user behavior analysis can contribute to daily healthcare\nmonitoring and behavioral economics.\nModel interpretability for a physical explanation. Model-\nbased and learning-based methods develop fast but in a differ-\nent ways. Recent research has investigated the interpretability\nof deep learning models that looks for the justiÔ¨Åcations of\nclassiÔ¨Åers. In WiFi sensing, if the model is interpreted well,\nthere may exist a connection between the data-driven model\nand the physical model. The modal interpretability may inspire\nus to develop new theories of physical models for WiFi\nsensing, and oppositely, the existing model (e.g., Fresnel Zone)\nmay enable us to propose new learning methods based on the\nphysical models. It is hoped that two directions of methods\ncan be uniÔ¨Åed theoretically and practically.\n13\nVIII. CONCLUSION\nDeep learning methods have been proven to be effective\nfor challenging applications in WiFi sensing, yet these models\nexhibit different characteristics on WiFi sensing tasks and a\ncomprehensive benchmark is highly demanded. To this end,\nthis work reviews the recent progress on deep learning for\nWiFi human sensing, and benchmarks prevailing deep neural\nnetworks and deep learning strategies on WiFi CSI data across\ndifferent platforms. We summarize the conclusions drawn from\nthe experimental observations, which provide valuable experi-\nences for model design in practical WiFi sensing applications.\nLast but not least, the grand challenges and future directions\nare proposed to imagine the research issues emerging from\nfuture large-scale WiFi sensing scenarios.\nREFERENCES\n[1] D. Halperin, W. Hu, A. Sheth, and D. Wetherall, ‚ÄúTool release: Gather-\ning 802.11 n traces with channel state information,‚Äù ACM SIGCOMM\nComputer Communication Review, vol. 41, no. 1, pp. 53‚Äì53, 2011.\n[2] Y. Xie, Z. Li, and M. Li, ‚ÄúPrecise power delay proÔ¨Åling with commod-\nity wiÔ¨Å,‚Äù in Proceedings of the 21st Annual International Conference\non Mobile Computing and Networking.\nACM, 2015, pp. 53‚Äì64.\n[3] H. Zou, H. Jiang, J. Yang, L. Xie, and C. Spanos, ‚ÄúNon-intrusive\noccupancy sensing in commercial buildings,‚Äù Energy and Buildings,\nvol. 154, pp. 633‚Äì643, 2017.\n[4] Y. Wang, J. Liu, Y. Chen, M. Gruteser, J. Yang, and H. Liu, ‚ÄúE-eyes:\ndevice-free location-oriented activity identiÔ¨Åcation using Ô¨Åne-grained\nwiÔ¨Åsignatures,‚Äù in Proceedings of the 20th annual international\nconference on Mobile computing and networking, 2014, pp. 617‚Äì628.\n[5] H. Zou, Y. Zhou, J. Yang, H. Jiang, L. Xie, and C. J. Spanos,\n‚ÄúDeepsense: Device-free human activity recognition via autoencoder\nlong-term recurrent convolutional network,‚Äù in 2018 IEEE Interna-\ntional Conference on Communications (ICC).\nIEEE, 2018, pp. 1‚Äì6.\n[6] J. Yang, H. Zou, H. Jiang, and L. Xie, ‚ÄúCareÔ¨Å: Sedentary behavior\nmonitoring system via commodity wiÔ¨Åinfrastructures,‚Äù IEEE Transac-\ntions on Vehicular Technology, vol. 67, no. 8, pp. 7620‚Äì7629, 2018.\n[7] H. Zou, J. Yang, H. Prasanna Das, H. Liu, Y. Zhou, and C. J. Spanos,\n‚ÄúWiÔ¨Åand vision multimodal learning for accurate and robust device-\nfree human activity recognition,‚Äù in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops,\n2019, pp. 0‚Äì0.\n[8] H. Wang, D. Zhang, Y. Wang, J. Ma, Y. Wang, and S. Li, ‚ÄúRt-fall:\nA real-time and contactless fall detection system with commodity wiÔ¨Å\ndevices,‚Äù IEEE Transactions on Mobile Computing, vol. 16, no. 2, pp.\n511‚Äì526, 2016.\n[9] J. Yang, H. Zou, Y. Zhou, and L. Xie, ‚ÄúLearning gestures from wiÔ¨Å: A\nsiamese recurrent convolutional architecture,‚Äù IEEE Internet of Things\nJournal, vol. 6, no. 6, pp. 10 763‚Äì10 772, 2019.\n[10] H. Zou, Y. Zhou, J. Yang, H. Jiang, L. Xie, and C. J. Spanos, ‚ÄúWiÔ¨Å-\nenabled device-free gesture recognition for smart home automation,‚Äù\nin 2018 IEEE 14th international conference on control and automation\n(ICCA).\nIEEE, 2018, pp. 476‚Äì481.\n[11] H. Zou, Y. Zhou, J. Yang, W. Gu, L. Xie, and C. J. Spanos, ‚ÄúWiÔ¨Å-\nbased human identiÔ¨Åcation via convex tensor shapelet learning,‚Äù in\nThirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence, 2018.\n[12] D. Wang, J. Yang, W. Cui, L. Xie, and S. Sun, ‚ÄúCaution: A robust\nwiÔ¨Å-based human authentication system via few-shot open-set gait\nrecognition,‚Äù IEEE Internet of Things Journal, 2022.\n[13] H. Zou, Y. Zhou, J. Yang, and C. J. Spanos, ‚ÄúDevice-free occupancy\ndetection and crowd counting in smart buildings with wiÔ¨Å-enabled iot,‚Äù\nEnergy and Buildings, vol. 174, pp. 309‚Äì322, 2018.\n[14] H. Zou, Y. Zhou, J. Yang, L. Xie, and C. Spanos, ‚ÄúFreecount: Device-\nfree crowd counting with commodity wiÔ¨Å,‚Äù in 2017 IEEE Global\nCommunications Conference (GLOBECOM).\nIEEE, 2017.\n[15] F. Restuccia, ‚ÄúIeee 802.11 bf: Toward ubiquitous wi-Ô¨Åsensing,‚Äù arXiv\npreprint arXiv:2103.14918, 2021.\n[16] D. Wu, D. Zhang, C. Xu, H. Wang, and X. Li, ‚ÄúDevice-free wiÔ¨Å\nhuman sensing: From pattern-based to model-based approaches,‚Äù IEEE\nCommunications Magazine, vol. 55, no. 10, pp. 91‚Äì97, 2017.\n[17] H. Wang, D. Zhang, J. Ma, Y. Wang, Y. Wang, D. Wu, T. Gu, and\nB. Xie, ‚ÄúHuman respiration detection with commodity wiÔ¨Ådevices:\ndo user location and body orientation matter?‚Äù in Proceedings of the\n2016 ACM International Joint Conference on Pervasive and Ubiquitous\nComputing, 2016, pp. 25‚Äì36.\n[18] P. Wang, B. Guo, T. Xin, Z. Wang, and Z. Yu, ‚ÄúTinysense: Multi-\nuser respiration detection using wi-Ô¨Åcsi signals,‚Äù in 2017 IEEE 19th\nInternational Conference on e-Health Networking, Applications and\nServices (Healthcom), 2017, pp. 1‚Äì6.\n[19] Y. T. Xu, X. Chen, X. Liu, D. Meger, and G. Dudek, ‚ÄúPressense: Passive\nrespiration sensing via ambient wiÔ¨Åsignals in noisy environments,‚Äù in\n2020 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS), 2020, pp. 4032‚Äì4039.\n[20] T. Nakamura, M. Bouazizi, K. Yamamoto, and T. Ohtsuki, ‚ÄúWi-Ô¨Å-csi-\nbased fall detection by spectrogram analysis with cnn,‚Äù in GLOBECOM\n2020 - 2020 IEEE Global Communications Conference, 2020, pp. 1‚Äì6.\n[21] ‚Äî‚Äî, ‚ÄúWi-Ô¨Å-based fall detection using spectrogram image of channel\nstate information,‚Äù IEEE Internet of Things Journal, pp. 1‚Äì1, 2022.\n[22] J. Yang, H. Zou, H. Jiang, and L. Xie, ‚ÄúDevice-free occupant activity\nsensing using wiÔ¨Å-enabled iot devices for smart homes,‚Äù IEEE Internet\nof Things Journal, vol. 5, no. 5, pp. 3991‚Äì4002, 2018.\n[23] H. Zou, Y. Zhou, J. Yang, W. Gu, L. Xie, and C. Spanos, ‚ÄúMultiple ker-\nnel representation learning for wiÔ¨Å-based human activity recognition,‚Äù\nin 2017 16th IEEE International Conference on Machine Learning and\nApplications (ICMLA).\nIEEE, 2017, pp. 268‚Äì274.\n[24] J. Zhang, B. Wei, F. Wu, L. Dong, W. Hu, S. S. Kanhere, C. Luo, S. Yu,\nand J. Cheng, ‚ÄúGate-id: WiÔ¨Å-based human identiÔ¨Åcation irrespective of\nwalking directions in smart home,‚Äù IEEE Internet of Things Journal,\nvol. 8, no. 9, pp. 7610‚Äì7624, 2020.\n[25] H. Zou, J. Yang, Y. Zhou, L. Xie, and C. J. Spanos, ‚ÄúRobust wiÔ¨Å-\nenabled device-free gesture recognition via unsupervised adversarial\ndomain adaptation,‚Äù in 2018 27th International Conference on Com-\nputer Communication and Networks (ICCCN).\nIEEE, 2018, pp. 1‚Äì8.\n[26] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis,\n‚ÄúDeep learning for computer vision: A brief review,‚Äù Computational\nintelligence and neuroscience, vol. 2018, 2018.\n[27] X. Shu, L. Zhang, Y. Sun, and J. Tang, ‚ÄúHost‚Äìparasite: Graph lstm-\nin-lstm for group activity recognition,‚Äù IEEE transactions on neural\nnetworks and learning systems, vol. 32, no. 2, pp. 663‚Äì674, 2020.\n[28] G. Zhu, L. Zhang, L. Yang, L. Mei, S. A. A. Shah, M. Bennamoun, and\nP. Shen, ‚ÄúRedundancy and attention in convolutional lstm for gesture\nrecognition,‚Äù IEEE transactions on neural networks and learning\nsystems, vol. 31, no. 4, pp. 1323‚Äì1335, 2019.\n[29] D. W. Otter, J. R. Medina, and J. K. Kalita, ‚ÄúA survey of the usages\nof deep learning for natural language processing,‚Äù IEEE transactions\non neural networks and learning systems, vol. 32, no. 2, pp. 604‚Äì624,\n2020.\n[30] D. Wang, B. Jing, C. Lu, J. Wu, G. Liu, C. Du, and F. Zhuang, ‚ÄúCoarse\nalignment of topic and sentiment: a uniÔ¨Åed model for cross-lingual\nsentiment classiÔ¨Åcation,‚Äù IEEE Transactions on Neural Networks and\nLearning Systems, vol. 32, no. 2, pp. 736‚Äì747, 2020.\n[31] Q. Bu, X. Ming, J. Hu, T. Zhang, J. Feng, and J. Zhang, ‚ÄúTransfersense:\ntowards environment independent and one-shot wiÔ¨Åsensing,‚Äù Personal\nand Ubiquitous Computing, pp. 1‚Äì19, 2021.\n[32] J. Zhang, Z. Tang, M. Li, D. Fang, P. Nurmi, and Z. Wang, ‚ÄúCrosssense:\nTowards cross-site and large-scale wiÔ¨Åsensing,‚Äù in Proceedings of\nthe 24th Annual International Conference on Mobile Computing and\nNetworking, 2018, pp. 305‚Äì320.\n[33] S. YouseÔ¨Å, H. Narui, S. Dayal, S. Ermon, and S. Valaee, ‚ÄúA survey\non behavior recognition using wiÔ¨Åchannel state information,‚Äù IEEE\nCommunications Magazine, vol. 55, no. 10, pp. 98‚Äì104, 2017.\n[34] Y. Zhang, Y. Zheng, K. Qian, G. Zhang, Y. Liu, C. Wu, and Z. Yang,\n‚ÄúWidar3. 0: Zero-effort cross-domain gesture recognition with wi-Ô¨Å,‚Äù\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n[35] F.\nGringoli,\nM.\nSchulz,\nJ.\nLink,\nand\nM.\nHollick,\n‚ÄúFree\nyour\ncsi:\nA\nchannel\nstate\ninformation\nextraction\nplatform\nfor\nmodern wi-Ô¨Åchipsets,‚Äù in Proceedings of the 13th International\nWorkshop on Wireless Network Testbeds, Experimental Evaluation\n& Characterization, ser. WiNTECH ‚Äô19, 2019, p. 21‚Äì28. [Online].\nAvailable: https://doi.org/10.1145/3349623.3355477\n[36] J. Yang, X. Chen, H. Zou, D. Wang, Q. Xu, and L. Xie, ‚ÄúEfÔ¨ÅcientÔ¨Å:\nTowards large-scale lightweight wiÔ¨Åsensing via csi compression,‚Äù\nIEEE Internet of Things Journal, 2022.\n[37] A. Sharma, J. Li, D. Mishra, G. Batista, and A. Seneviratne, ‚ÄúPassive\nwiÔ¨Åcsi sensing based machine learning framework for covid-safe\noccupancy monitoring,‚Äù in 2021 IEEE International Conference on\nCommunications Workshops (ICC Workshops).\nIEEE, 2021, pp. 1‚Äì6.\n14\n[38] J. Sch¬®afer, B. R. Barrsiwal, M. Kokhkharova, H. Adil, and J. Liebehen-\nschel, ‚ÄúHuman activity recognition using csi information with nexmon,‚Äù\nApplied Sciences, vol. 11, no. 19, p. 8860, 2021.\n[39] J. Liu, G. Teng, and F. Hong, ‚ÄúHuman activity sensing with wireless\nsignals: A survey,‚Äù Sensors, vol. 20, no. 4, p. 1210, 2020.\n[40] Y. Zeng, D. Wu, J. Xiong, E. Yi, R. Gao, and D. Zhang, ‚ÄúFarsense:\nPushing the range limit of wiÔ¨Å-based respiration sensing with csi ratio\nof two antennas,‚Äù Proceedings of the ACM on Interactive, Mobile,\nWearable and Ubiquitous Technologies, vol. 3, no. 3, pp. 1‚Äì26, 2019.\n[41] Y. LeCun, Y. Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù nature, vol. 521,\nno. 7553, pp. 436‚Äì444, 2015.\n[42] M. Schuster and K. K. Paliwal, ‚ÄúBidirectional recurrent neural net-\nworks,‚Äù IEEE transactions on Signal Processing, vol. 45, no. 11, pp.\n2673‚Äì2681, 1997.\n[43] S. Liu, Y. Zhao, and B. Chen, ‚ÄúWicount: A deep learning approach\nfor crowd counting using wiÔ¨Åsignals,‚Äù in 2017 IEEE International\nSymposium on Parallel and Distributed Processing with Applications\nand 2017 IEEE International Conference on Ubiquitous Computing\nand Communications (ISPA/IUCC).\nIEEE, 2017, pp. 967‚Äì974.\n[44] W. Jiang, C. Miao, F. Ma, S. Yao, Y. Wang, Y. Yuan, H. Xue,\nC. Song, X. Ma, D. Koutsonikolas et al., ‚ÄúTowards environment\nindependent device free human activity recognition,‚Äù in Proceedings\nof the 24th Annual International Conference on Mobile Computing\nand Networking.\nACM, 2018, pp. 289‚Äì304.\n[45] Z. Chen, L. Zhang, C. Jiang, Z. Cao, and W. Cui, ‚ÄúWiÔ¨Åcsi based\npassive human activity recognition using attention based blstm,‚Äù IEEE\nTransactions on Mobile Computing, vol. 18, no. 11, pp. 2714‚Äì2724,\n2018.\n[46] F. Wang, W. Gong, and J. Liu, ‚ÄúOn spatial diversity in wiÔ¨Å-based\nhuman activity recognition: A deep learning-based approach,‚Äù IEEE\nInternet of Things Journal, vol. 6, no. 2, pp. 2035‚Äì2047, 2018.\n[47] C. Xiao, D. Han, Y. Ma, and Z. Qin, ‚ÄúCsigan: Robust channel state\ninformation-based activity recognition with gans,‚Äù IEEE Internet of\nThings Journal, vol. 6, no. 6, pp. 10 191‚Äì10 204, 2019.\n[48] H. Xue, W. Jiang, C. Miao, F. Ma, S. Wang, Y. Yuan, S. Yao, A. Zhang,\nand L. Su, ‚ÄúDeepmv: Multi-view deep learning for device-free human\nactivity recognition,‚Äù Proceedings of the ACM on Interactive, Mobile,\nWearable and Ubiquitous Technologies, vol. 4, no. 1, pp. 1‚Äì26, 2020.\n[49] C. Li, M. Liu, and Z. Cao, ‚ÄúWihf: enable user identiÔ¨Åed gesture\nrecognition with wiÔ¨Å,‚Äù in IEEE INFOCOM 2020-IEEE Conference on\nComputer Communications.\nIEEE, 2020, pp. 586‚Äì595.\n[50] C. Xiao, Y. Lei, Y. Ma, F. Zhou, and Z. Qin, ‚ÄúDeepseg: Deep-learning-\nbased activity segmentation framework for activity recognition using\nwiÔ¨Å,‚Äù IEEE Internet of Things Journal, vol. 8, no. 7, pp. 5669‚Äì5681,\n2020.\n[51] B. Sheng, F. Xiao, L. Sha, and L. Sun, ‚ÄúDeep spatial‚Äìtemporal model\nbased cross-scene action recognition using commodity wiÔ¨Å,‚Äù IEEE\nInternet of Things Journal, vol. 7, no. 4, pp. 3592‚Äì3601, 2020.\n[52] P. F. Moshiri, R. Shahbazian, M. Nabati, and S. A. Ghorashi, ‚ÄúA csi-\nbased human activity recognition using deep learning,‚Äù Sensors, vol. 21,\nno. 21, p. 7225, 2021.\n[53] X. Ding, T. Jiang, Y. Zhong, S. Wu, J. Yang, and W. Xue, ‚ÄúImprov-\ning wiÔ¨Å-based human activity recognition with adaptive initial state\nvia one-shot learning,‚Äù in 2021 IEEE Wireless Communications and\nNetworking Conference (WCNC).\nIEEE, 2021, pp. 1‚Äì6.\n[54] Y. Gu, H. Yan, M. Dong, M. Wang, X. Zhang, Z. Liu, and F. Ren,\n‚ÄúWione: One-shot learning for environment-robust device-free user\nauthentication via commodity wi-Ô¨Åin man‚Äìmachine system,‚Äù IEEE\nTransactions on Computational Social Systems, vol. 8, no. 3, pp. 630‚Äì\n642, 2021.\n[55] Y. Ma, S. Arshad, S. Muniraju, E. Torkildson, E. Rantala, K. Doppler,\nand G. Zhou, ‚ÄúLocation-and person-independent activity recognition\nwith wiÔ¨Å, deep neural networks, and reinforcement learning,‚Äù ACM\nTransactions on Internet of Things, vol. 2, no. 1, pp. 1‚Äì25, 2021.\n[56] B. Li, W. Cui, W. Wang, L. Zhang, Z. Chen, and M. Wu, ‚ÄúTwo-stream\nconvolution augmented transformer for human activity recognition,‚Äù in\nProceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, vol. 35,\nno. 1, 2021, pp. 286‚Äì293.\n[57] X. Zhang, C. Tang, K. Yin, and Q. Ni, ‚ÄúWiÔ¨Å-based cross-domain\ngesture recognition via modiÔ¨Åed prototypical networks,‚Äù IEEE Internet\nof Things Journal, vol. 9, no. 11, pp. 8584‚Äì8596, 2021.\n[58] D. Wang, J. Yang, W. Cui, L. Xie, and S. Sun, ‚ÄúMultimodal csi-\nbased human activity recognition using gans,‚Äù IEEE Internet of Things\nJournal, 2021.\n[59] X. Ding, T. Jiang, Y. Zhong, S. Wu, J. Yang, and J. Zeng, ‚ÄúWi-Ô¨Å-\nbased location-independent human activity recognition with attention\nmechanism enhanced method,‚Äù Electronics, vol. 11, no. 4, p. 642, 2022.\n[60] Y. Gu, X. Zhang, Y. Wang, M. Wang, H. Yan, Y. Ji, Z. Liu, J. Li,\nand M. Dong, ‚ÄúWigrunt: WiÔ¨Å-enabled gesture recognition using dual-\nattention network,‚Äù IEEE Transactions on Human-Machine Systems,\n2022.\n[61] A. Zhuravchak, O. Kapshii, and E. Pournaras, ‚ÄúHuman activity recogni-\ntion based on wi-Ô¨Åcsi data-a deep neural network approach,‚Äù Procedia\nComputer Science, vol. 198, pp. 59‚Äì66, 2022.\n[62] J. Yang, H. Zou, and L. Xie, ‚ÄúRobustsense: Defending adversarial at-\ntack for secure device-free human activity recognition,‚Äù arXiv preprint\narXiv:2204.01560, 2022.\n[63] J. Yang, X. Chen, H. Zou, D. Wang, and L. Xie, ‚ÄúAutoÔ¨Å: Towards\nautomatic wiÔ¨Åhuman sensing via geometric self-supervised learning,‚Äù\narXiv preprint arXiv:2205.01629, 2022.\n[64] M. I. Jordan and T. M. Mitchell, ‚ÄúMachine learning: Trends, perspec-\ntives, and prospects,‚Äù Science, vol. 349, no. 6245, pp. 255‚Äì260, 2015.\n[65] Y. LeCun, D. Touresky, G. Hinton, and T. Sejnowski, ‚ÄúA theoretical\nframework for back-propagation,‚Äù in Proceedings of the 1988 connec-\ntionist models summer school, vol. 1, 1988, pp. 21‚Äì28.\n[66] K. Chen, Z. Zeng, and J. Yang, ‚ÄúA deep region-based pyramid neural\nnetwork for automatic detection and multi-classiÔ¨Åcation of various\nsurface defects of aluminum alloys,‚Äù Journal of Building Engineering,\nvol. 43, p. 102523, 2021.\n[67] W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Alsaadi, ‚ÄúA\nsurvey of deep neural network architectures and their applications,‚Äù\nNeurocomputing, vol. 234, pp. 11‚Äì26, 2017.\n[68] M. W. Gardner and S. Dorling, ‚ÄúArtiÔ¨Åcial neural networks (the\nmultilayer perceptron)‚Äîa review of applications in the atmospheric\nsciences,‚Äù Atmospheric environment, vol. 32, no. 14-15, pp. 2627‚Äì2636,\n1998.\n[69] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, ‚ÄúGradient-based\nlearning applied to document recognition,‚Äù Proceedings of the IEEE,\nvol. 86, no. 11, pp. 2278‚Äì2324, 1998.\n[70] A. Khan, A. Sohail, U. Zahoora, and A. S. Qureshi, ‚ÄúA survey of the\nrecent architectures of deep convolutional neural networks,‚Äù ArtiÔ¨Åcial\nintelligence review, vol. 53, no. 8, pp. 5455‚Äì5516, 2020.\n[71] C. Wang, J. Yang, L. Xie, and J. Yuan, ‚ÄúKervolutional neural networks,‚Äù\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 31‚Äì40.\n[72] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, and G. Penn, ‚ÄúApplying\nconvolutional neural networks concepts to hybrid nn-hmm model\nfor speech recognition,‚Äù in 2012 IEEE international conference on\nAcoustics, speech and signal processing (ICASSP).\nIEEE, 2012, pp.\n4277‚Äì4280.\n[73] W. Yin, K. Kann, M. Yu, and H. Sch¬®utze, ‚ÄúComparative study\nof cnn and rnn for natural language processing,‚Äù arXiv preprint\narXiv:1702.01923, 2017.\n[74] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\nrecognition,‚Äù in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770‚Äì778.\n[75] Y. Zhang, X. Wang, Y. Wang, and H. Chen, ‚ÄúHuman activity recogni-\ntion across scenes and categories based on csi,‚Äù IEEE Transactions on\nMobile Computing, vol. 21, no. 7, pp. 2411‚Äì2420, 2022.\n[76] P. F. Moshiri, M. Nabati, R. Shahbazian, and S. A. Ghorashi, ‚ÄúCsi-\nbased human activity recognition using convolutional neural networks,‚Äù\nin 2021 11th International Conference on Computer Engineering and\nKnowledge (ICCKE), 2021, pp. 7‚Äì12.\n[77] J. Yang, K. Wang, X. Peng, and Y. Qiao, ‚ÄúDeep recurrent multi-instance\nlearning with spatio-temporal features for engagement intensity pre-\ndiction,‚Äù in Proceedings of the 2018 on International Conference on\nMultimodal Interaction.\nACM, 2018, pp. 594‚Äì598.\n[78] J. Ding and Y. Wang, ‚ÄúWiÔ¨Åcsi-based human activity recognition using\ndeep recurrent neural network,‚Äù IEEE Access, vol. 7, pp. 174 257‚Äì\n174 269, 2019.\n[79] Z. Shi, J. A. Zhang, R. Xu, and Q. Cheng, ‚ÄúDeep learning networks for\nhuman activity recognition with csi correlation feature extraction,‚Äù in\nICC 2019 - 2019 IEEE International Conference on Communications\n(ICC), 2019, pp. 1‚Äì6.\n[80] Z. C. Lipton, J. Berkowitz, and C. Elkan, ‚ÄúA critical review of\nrecurrent neural networks for sequence learning,‚Äù arXiv preprint\narXiv:1506.00019, 2015.\n[81] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural\ncomputation, vol. 9, no. 8, pp. 1735‚Äì1780, 1997.\n[82] S.-C. Kim and Y.-H. Kim, ‚ÄúEfÔ¨Åcient classiÔ¨Åcation of human activity\nusing pca and deep learning lstm with wiÔ¨Åcsi,‚Äù in 2022 International\nConference on ArtiÔ¨Åcial Intelligence in Information and Communica-\ntion (ICAIIC), 2022, pp. 329‚Äì332.\n15\n[83] H. F. Thariq Ahmed, H. Ahmad, S. K. Phang, H. Harkat, and\nK. Narasingamurthi, ‚ÄúWi-Ô¨Åcsi based human sign language recogni-\ntion using lstm network,‚Äù in 2021 IEEE International Conference on\nIndustry 4.0, ArtiÔ¨Åcial Intelligence, and Communications Technology\n(IAICT), 2021, pp. 51‚Äì57.\n[84] Z. Tang, Q. Liu, M. Wu, W. Chen, and J. Huang, ‚ÄúWiÔ¨Åcsi gesture\nrecognition based on parallel lstm-fcn deep space-time neural network,‚Äù\nChina Communications, vol. 18, no. 3, pp. 205‚Äì215, 2021.\n[85] R. Kadir, R. Saha, M. A. Awal, and M. I. Kadir, ‚ÄúDeep bidirectional\nlstm network learning-aided ofdma downlink and sc-fdma uplink,‚Äù in\n2021 International Conference on Electronics, Communications and\nInformation Technology (ICECIT), 2021, pp. 1‚Äì4.\n[86] N. Dua, S. N. Singh, and V. B. Semwal, ‚ÄúMulti-input cnn-gru based\nhuman activity recognition using wearable sensors,‚Äù Computing, vol.\n103, no. 7, pp. 1461‚Äì1478, 2021.\n[87] K. Chen, L. Yao, D. Zhang, X. Wang, X. Chang, and F. Nie, ‚ÄúA\nsemisupervised recurrent convolutional attention model for human ac-\ntivity recognition,‚Äù IEEE transactions on neural networks and learning\nsystems, vol. 31, no. 5, pp. 1747‚Äì1756, 2019.\n[88] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nAdvances in neural information processing systems, vol. 30, 2017.\n[89] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n‚ÄúAn image is worth 16x16 words: Transformers for image recognition\nat scale,‚Äù arXiv preprint arXiv:2010.11929, 2020.\n[90] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, ‚ÄúGenerative adversarial nets,‚Äù\nAdvances in neural information processing systems, vol. 27, 2014.\n[91] J. Yang, H. Zou, Y. Zhou, and L. Xie, ‚ÄúRobust adversarial discrimina-\ntive domain adaptation for real-world cross-domain visual recognition,‚Äù\nNeurocomputing, vol. 433, pp. 28‚Äì36, 2021.\n[92] J. Yang, H. Zou, Y. Zhou, Z. Zeng, and L. Xie, ‚ÄúMind the discrim-\ninability: Asymmetric adversarial domain adaptation,‚Äù in European\nConference on Computer Vision.\nSpringer, 2020, pp. 589‚Äì606.\n[93] Y. Xu, J. Yang, H. Cao, Z. Chen, Q. Li, and K. Mao, ‚ÄúPartial\nvideo domain adaptation with partial adversarial temporal attentive\nnetwork,‚Äù in Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2021, pp. 9332‚Äì9341.\n[94] D. P. Kingma and M. Welling, ‚ÄúAuto-encoding variational bayes,‚Äù arXiv\npreprint arXiv:1312.6114, 2013.\n[95] M. Kim, D. Han, and J.-K. K. Rhee, ‚ÄúMultiview variational deep learn-\ning with application to practical indoor localization,‚Äù IEEE Internet of\nThings Journal, vol. 8, no. 15, pp. 12 375‚Äì12 383, 2021.\n[96] X. Chen, H. Li, C. Zhou, X. Liu, D. Wu, and G. Dudek, ‚ÄúFido:\nUbiquitous Ô¨Åne-grained wiÔ¨Å-based localization for unlabelled users via\ndomain adaptation,‚Äù in Proceedings of The Web Conference 2020, 2020,\npp. 23‚Äì33.\n[97] S. J. Pan and Q. Yang, ‚ÄúA survey on transfer learning,‚Äù Knowledge and\nData Engineering, IEEE Transactions on, vol. 22, no. 10, pp. 1345‚Äì\n1359, 2010.\n[98] S. Arshad, C. Feng, R. Yu, and Y. Liu, ‚ÄúLeveraging transfer learning in\nmultiple human activity recognition using wiÔ¨Åsignal,‚Äù in 2019 IEEE\n20th International Symposium on ‚ÄùA World of Wireless, Mobile and\nMultimedia Networks‚Äù (WoWMoM), 2019, pp. 1‚Äì10.\n[99] L. Li, L. Wang, B. Han, X. Lu, Z. Zhou, and B. Lu, ‚ÄúSubdomain\nadaptive learning network for cross-domain human activities recogni-\ntion using wiÔ¨Åwith csi,‚Äù in 2021 IEEE 27th International Conference\non Parallel and Distributed Systems (ICPADS), 2021, pp. 1‚Äì7.\n[100] J.-B. Grill, F. Strub, F. Altch¬¥e, C. Tallec, P. Richemond, E. Buchatskaya,\nC. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar et al.,\n‚ÄúBootstrap your own latent-a new approach to self-supervised learn-\ning,‚Äù Advances in Neural Information Processing Systems, vol. 33, pp.\n21 271‚Äì21 284, 2020.\n[101] F. Wang, T. Kong, R. Zhang, H. Liu, and H. Li, ‚ÄúSelf-supervised\nlearning by estimating twin class distributions,‚Äù arXiv preprint\narXiv:2110.07402, 2021.\n[102] O. Sagi and L. Rokach, ‚ÄúEnsemble learning: A survey,‚Äù Wiley Inter-\ndisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 8,\nno. 4, p. e1249, 2018.\n[103] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., ‚ÄúPytorch: An\nimperative style, high-performance deep learning library,‚Äù Advances\nin neural information processing systems, vol. 32, 2019.\n[104] D. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù\narXiv preprint arXiv:1412.6980, 2014.\n[105] S. Chen, W. Wang, and S. J. Pan, ‚ÄúMetaquant: Learning to quantize\nby learning to penetrate non-differentiable quantization,‚Äù Advances in\nNeural Information Processing Systems, vol. 32, 2019.\n[106] ‚Äî‚Äî, ‚ÄúCooperative pruning in cross-domain deep neural network\ncompression.‚Äù in IJCAI, 2019, pp. 2102‚Äì2108.\n[107] J. Hu, J. Yang, J. Ong, and L. Xie, ‚ÄúResÔ¨Å: WiÔ¨Å-enabled device-free\nrespiration detection based on deep learning,‚Äù in 2022 IEEE 18th\ninternational conference on control and automation (ICCA).\nIEEE,\n2022.\n[108] J. Yang, H. Zou, S. Cao, Z. Chen, and L. Xie, ‚ÄúMobileda: Toward edge-\ndomain adaptation,‚Äù IEEE Internet of Things Journal, vol. 7, no. 8, pp.\n6909‚Äì6918, 2020.\n[109] M. Tan and Q. Le, ‚ÄúEfÔ¨Åcientnet: Rethinking model scaling for con-\nvolutional neural networks,‚Äù in International conference on machine\nlearning.\nPMLR, 2019, pp. 6105‚Äì6114.\n[110] M. H. Kefayati, V. Pourahmadi, and H. Aghaeinia, ‚ÄúWi2vi: Generating\nvideo frames from wiÔ¨Åcsi samples,‚Äù IEEE Sensors Journal, vol. 20,\nno. 19, pp. 11 463‚Äì11 473, 2020.\n[111] F. Wang, S. Zhou, S. Panev, J. Han, and D. Huang, ‚ÄúPerson-in-wiÔ¨Å:\nFine-grained person perception using wiÔ¨Å,‚Äù in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2019, pp.\n5452‚Äì5461.\n[112] J. Liu, N. Akhtar, and A. Mian, ‚ÄúAdversarial attack on skeleton-based\nhuman action recognition,‚Äù IEEE Transactions on Neural Networks and\nLearning Systems, 2020.\n[113] S. Zhou, W. Zhang, D. Peng, Y. Liu, X. Liao, and H. Jiang, ‚ÄúAdversarial\nwiÔ¨Åsensing for privacy preservation of human behaviors,‚Äù IEEE\nCommunications Letters, vol. 24, no. 2, pp. 259‚Äì263, 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "eess.SP"
  ],
  "published": "2022-07-16",
  "updated": "2023-02-17"
}