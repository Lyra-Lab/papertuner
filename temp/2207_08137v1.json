{
  "id": "http://arxiv.org/abs/2207.08137v1",
  "title": "Achieve Optimal Adversarial Accuracy for Adversarial Deep Learning using Stackelberg Game",
  "authors": [
    "Xiao-Shan Gao",
    "Shuang Liu",
    "Lijia Yu"
  ],
  "abstract": "Adversarial deep learning is to train robust DNNs against adversarial\nattacks, which is one of the major research focuses of deep learning. Game\ntheory has been used to answer some of the basic questions about adversarial\ndeep learning such as the existence of a classifier with optimal robustness and\nthe existence of optimal adversarial samples for a given class of classifiers.\nIn most previous work, adversarial deep learning was formulated as a\nsimultaneous game and the strategy spaces are assumed to be certain probability\ndistributions in order for the Nash equilibrium to exist. But, this assumption\nis not applicable to the practical situation. In this paper, we give answers to\nthese basic questions for the practical case where the classifiers are DNNs\nwith a given structure, by formulating the adversarial deep learning as\nsequential games. The existence of Stackelberg equilibria for these games are\nproved. Furthermore, it is shown that the equilibrium DNN has the largest\nadversarial accuracy among all DNNs with the same structure, when\nCarlini-Wagner's margin loss is used. Trade-off between robustness and accuracy\nin adversarial deep learning is also studied from game theoretical aspect.",
  "text": "arXiv:2207.08137v1  [cs.LG]  17 Jul 2022\nAchieve Optimal Adversarial Accuracy for Adversarial Deep\nLearning using Stackelberg Game∗\nXiao-Shan Gao, Shuang Liu, and Lijia Yu\nAcademy of Mathematics and Systems Science, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences\nJuly 19, 2022\nAbstract\nAdversarial deep learning is to train robust DNNs against adversarial attacks, which is one\nof the major research focuses of deep learning. Game theory has been used to answer some\nof the basic questions about adversarial deep learning such as the existence of a classiﬁer\nwith optimal robustness and the existence of optimal adversarial samples for a given class of\nclassiﬁers. In most previous work, adversarial deep learning was formulated as a simultaneous\ngame and the strategy spaces are assumed to be certain probability distributions in order\nfor the Nash equilibrium to exist. But, this assumption is not applicable to the practical\nsituation.\nIn this paper, we give answers to these basic questions for the practical case\nwhere the classiﬁers are DNNs with a given structure, by formulating the adversarial deep\nlearning as sequential games. The existence of Stackelberg equilibria for these games are\nproved.\nFurthermore, it is shown that the equilibrium DNN has the largest adversarial\naccuracy among all DNNs with the same structure, when Carlini-Wagner’s margin loss is\nused. Trade-oﬀbetween robustness and accuracy in adversarial deep learning is also studied\nfrom game theoretical aspect.\nKeywords. Adversarial deep learning, Stackelberg game, optimal robust DNN, universal\nadversarial attack, adversarial accuracy, trade-oﬀresult.\n1\nIntroduction\nA major safety issue for deep learning [22] is the existence of adversarial samples [40], that is,\nit is possible to make little modiﬁcations to an input sample which are essentially imperceptible\nto the human eye, but the DNN outputs a wrong label or even any label given by the adversary.\nExistence of adversarial samples makes deep learning vulnerable in safety critical applications\nand adversarial deep learning has becomes a major research focus of deep learning [44]. The\ngoal of adversarial deep learning is to train robust DNNs against adversarial attacks and well\nas developing more eﬀective attack methods for generating adversarial samples.\nMany adversarial defence models were proposed, including the adversarial training based on\nrobust optimization [24, 49], the gradient masking and obfuscation approaches [1, 48], adversarial\nparameter attacks [41, 23, 47], universal adversaries [5, 27], randomized smoothing [9], and the\n∗This work is partially supported by NSFC grant No.12288201 and NKRDP grant No.2018YFA0704705.\n1\nadversarial sample detection [7]. Many attack methods are also proposed, including the white-\nbox attacks based on gradient information of the DNN [6, 24, 30], the black-box attacks based\non the transferability of the adversaries [31], the poisoning attacks for the input data [17, 36],\nand the physical world attacks [21, 2]. More details can be found in the survey [44].\nMany of the defenses are found to be susceptible to new adversarial attacks, and stronger\ndefences also are proposed against the new adversarial attacks. To break this loop of defences\nand attacks, a recent line of research based on game theory [14, 38] tries to establish more\nrigourous foundation for adversarial deep learning by answering questions such as [5, 8, 25, 32]:\nQuestion Q1: Does there exists a classiﬁer which ensures optimal robustness against any\nadversarial attack?\nQuestion Q2: Does there exist optimal adversarial samples for a given class of classiﬁers\nand a given set of data distribution?\nTo answer these questions, the adversarial deep learning was formulated as a simultaneous\ngame between the Classiﬁer and the Adversary. The goal of the Classiﬁer is to train a robust\nDNN. The goal of the Adversary is to create optimal adversarial samples. A Nash equilibrium of\nthe game is a DNN C∗and an attack A∗, such that no player can beneﬁt by unilaterally changing\nits strategy and thus gives an optimal solution to the adversarial deep learning. Existence of\nNash equilibria was proved under various assumptions [5, 25, 32].\nDespite the grerat progresses, questions Q1 and Q2 are not answered satisfactorily. The main\nreason is that in order for the Nash equilibrium to exist, both the Classiﬁer and the Adversary are\neither assumed to be a convex set of probability distributions or measurable functions. However,\nin practice, DNNs with ﬁxed structures are used and Nash equilibria do not exist in this case.\nIn this paper, we will show that questions Q1 and Q2 can be answered positively for DNNS\nwith a ﬁxed structure by formulating the adversarial deep learning as Stackelberg games.\n1.1\nMain contributions\nA positive answer to question Q1 is given by formulating the adversarial deep learning as a\nStackelberg game Gs with the Classiﬁer as the leader and the Adversary as the follower, where\nthe strategy space for the Classiﬁer is a class of DNNs with a given structure, say DNNs with\na ﬁxed depth and width. We show that game Gs has a Stackelberg equilibrium which gives the\noptimal robust DNN under certain robustness measurement (Refer to Theorem 3.5). We further\nshow that when the Carlini-Wagner margin loss is used as the payoﬀfunction, the equilibrium\nDNN is the optimal defense which has the largest adversarial accuracy among all DNNs with the\nsame structure (Refer to Theorem 4.4). Furthermore, the equilibrium DNN is the same as that\nof the adversarial training [24]. Thus, our results give another theoretical explanation for the\nfact that adversarial training is one of the most eﬀective defences against adversarial attacks.\nThe trade-oﬀproperty for deep learning means that there exists a trade-oﬀbetween the\nrobustness and accuracy [42, 45, 49]. We prove a trade-oﬀresult from game theoretical viewpoint.\nPrecisely, we show that if a linear combination of the payoﬀfunctions of adversarial training and\nnormal training is used as the total payoﬀfunction, then the equilibrium DNN has robustness\nnot higher and accuracy no lower than that of the DNN obtained by adversarial training. We\nalso show that trade-oﬀproperty does not hold if using empirical loss to train the DNNs, that\nis, the DNNs with the largest adversarial accuracy can be parameterized by elements in an open\nset of RK, where K is the number of parameters, that is, there still exist rooms to improve the\naccuracy for DNNS with the optimal adversarial accuracy.\n2\nFinally, when using the empirical loss for a ﬁnite set of samples to train the DNN, we\ncompare Gs (denoted as G1 in this case) with two other games: G2 is the Stackelberg game\nwith the Adversary as the leader and G3 is the simultaneous game between the Classiﬁer and\nthe Adversary. We show that G2 has a Stackelberg equilibrium and G3 has a mixes strategy\nNash equilibrium. Furthermore, the payoﬀfunctions of G1, G2, G3 at their equilibria decrease\nsuccessively. Existence of Stackelberg equilibrium for G2 gives a positive answer to question Q2\nfor DNNs with a given structure.\n1.2\nRelated work\nThe game theoretical approach to adversarial machine learning was ﬁrst studied in the seminal\nwork of Dalvi, Domingos, Mausam, and Verma [11], where they formulated adversarial machine\nlearning as a simultaneous game between the Classiﬁer and the Adversary. Quite a number\nof work has been done along this line, by formulating adversarial machine learning both as a\nsimultaneous game and as a Stackelberg game, which can be found in the nice surveys [50, 20].\nThese works usually used linear models such as SVM for binary classiﬁcations, and used spam\nemail ﬁltering as the main application background.\nGame theoretical approach to adversarial deep learning appeared recently and was partially\nstimulated by the fact that adversarial samples seem inevitable for deep learning [3, 4, 10, 37].\nThe adversarial training was introduced in [24], which is one of the best practical training\nmethod to defend adversaries. In [32, 5, 16, 25, 29, 18, 34], the adversarial deep learning was all\nformulated as a simultaneous game. In [32], it was shown that the game exists no pure strategy\nNash equilibrium, but mixed strategies give more robust classiﬁers. In [5], it was proved that\nNash equilibrium exists when the strategy space for the Classiﬁer is convex and the strategy\nspace for the Adversary is certain probability distributions. In [16, 25], it was proved that Nash\nequilibria exist and can be approximated by a pure strategy, when the strategy spaces for both\nthe Classiﬁer and Adversary are parameterized by distributions. In [29], the Classiﬁer ensures\nthe robustness of a ﬁxed DNN by adding perturbation to the sample to counteract the Adversary.\nIn [18, 34], methods to compute mixed Nash equilibria were given. In [8], the adversarial deep\nlearning was formulated as a Stackelberg game with the Adversary as the leader, but existence\nof equilibria was not given. In [13, 19], properties and algorithms for local Stackelberg equilibria\nwere studied. In above work, the adversarial deep learning is modeled as a non-cooperative\ngame. In [35], the cooperative game is used to explain various adversarial attacks and defenses.\nMost of the above work formulated adversarial deep learning as a simultaneous game and\nassume the strategy spaces to be certain convex probability distributions in order to prove the\nexistence of the Nash equilibrium. In this paper, we show that by formulating the adversarial\ndeep learning as a sequential game, Stackelberg equilibria exist for DNNs with a given structure,\nand the equilibrium DNN is the best defence in that it has the largest adversarial accuracy among\nall DNNs with the same structure.\nThe rest of this paper is organized as follows. In section 2, preliminary results are given. In\nsection 3, the adversarial deep learning is formulated as a Stackelberg game and the existence\nof Stackelberg equilibria is proved.\nIn section 4, it is proved that adversarial training with\nCarlini-Wagner loss gives the best adversarial accuracy. In section 5, two trade-oﬀresults are\nproved. In section 6, three types of adversarial games are compared when the data set is ﬁnite.\nIn section 7, conclusions and problems for further study are given.\n3\n2\nPreliminaries\n2.1\nAdversarial training and robustness of DNN\nLet C : X →Rm be a classiﬁcation DNN with m labels in Y = [m] = {1, . . . , m} [22]. Without\nloss of generality, we assume X = In, where I = [0, 1]. Denote Cl(x) ∈R to be the l-th coordinate\nof C(x) for l ∈[m], which are called logits of the DNN. For x ∈X, the classiﬁcation result of\nC is bC(x) = argmaxl∈Y Cl(x). We assume that Relu is used as the activation function, so C is\ncontinuous and piecewise linear. The results are easily generated to any activation functions\nwhich are Lipschitz continuous.\nTo train a DNN, we need ﬁrst to choose a hypothesis space H for the DNNs, say the set of\nCNNs or RNNs with certain ﬁxed structure. In this paper, denote NW,D to be the set of DNNs\nwith width W and depth D and use it as the hypothesis space. For a given hypothesis space H,\nthe parameter set of DNNs in H is ﬁxed and is denoted as Θ ∈RK, where K is the number of\nthe parameters. C can be written as CΘ if the parameters need to be mentioned explicitly, that\nis,\nH = {CΘ : X →Rm : Θ ∈RK}.\n(1)\nLet the objects to be classiﬁed satisfy a distribution D over X × Y. Given a loss function\nL : Rm × Y →R, the total loss for the data set is\nϕ0(Θ) = E(x,y)∼D L(CΘ(x), y).\n(2)\nTraining a DNN CΘ is to make the total loss minimum by solving the following optimization\nproblem\nΘ∗= argminΘ∈RK ϕ0(Θ).\n(3)\nGiven an attack radius ε ∈R+, denote B(x, ε) = {x ∈Rn : ||x −x|| ≤ε}. We use ∞norm\nif not mentioned otherwise. We will ﬁnd adversaries for x in B(x, ε). Precisely, x ∈B(x, ε)\nis called an adversary of x with label y, if bC(x) ̸= y. In order to increase the robustness of a\ntrained DNN, the adversarial training [24] is introduced which is to solve the following robust\noptimization problem\nΘ∗= argminΘ∈RK E(x,y)∼D maxx∈B(x,ε) L(CΘ(x), y).\n(4)\nIntuitively, the adversarial training is ﬁrst computing a most-adversarial sample\nxa = argmaxx∈B(x,ε) L(F(x), lx)\nfor x and then minimizing L(F(xa), y) instead of L(F(x), y).\nGiven a DNN C and an attack radius ε, we deﬁne the adversarial robustness measure of C\nwith respect to ε as follows\nARD(C, ε)\n=\nE(x,y)∼D maxx∈B(x,ε)L(C(x), y)\n(5)\nwhich is the total loss of C at the most-adversarial samples.\nC is more robust if ARD(C, ε)\nis smaller. Then the adversarial training is to ﬁnd a DNN in H with the optimal adversarial\nrobustness measurement which is denoted as\nARD(H, ε)\n=\nminΘ∈RKARD(CΘ, ε).\n(6)\n4\nARD(C, ε) and ARD(H, ε) have the following simple properties.\n(1) If W1 ≥W2 and D1 ≥D2, then ARD(NW1,D1, ε) ≤ARD(NW2,D2, ε).\n(2) If ε1 ≤ε2, then ARD(C, ε1) ≤ARD(C, ε2).\n(3) In the optimal case, we have ARD(C, ε) = 0, which means that C gives the correct label\nfor any x ∈B(x, ε). In this case, we say that C is robust for the attack radius ε. It was proved\nthat there exist robust classiﬁers for a separated data set [45].\n2.2\nBounds and continuity of the DNN\nLet CΘ : X →Rm be a fully connected feed-forward DNN with depth D, whose l-th hidden\nlayer is\nxl = σ(Wlxl−1 + bl) ∈Rnl, l = 1, . . . , D,\n(7)\nwhere n0 = n, nD = m, Wl ∈Rnl×nl−1, bl ∈Rnl, σ = Relu, x0 ∈Rn is the input, and xD ∈Rm\nis the output. The parameter set is Θ = ∪D\nl=1(Wl ∪bl). It is easy to show that C is bounded.\nFor ε ∈R+, denote Iε = [−ε, 1 + ε].\nLemma 2.1. For any DNN CΘ : In\nε →Rm with width ≤W, depth ≤D, and ||Θ||2 ≤E, there\nexists an Ω(n, m, D, W, E, ε) ∈R+ such that ||CΘ(x)|| ≤Ω(n, m, D, W, E, ε).\nProof. CΘ(x) is bounded because CΘ(x) is continuous on x and Θ, and [−ε, 1+ε]n and [−E, E]n\nare compact. Ω(n, m, D, W, E, ε) can be derived from (7).\nLemma 2.2. For any DNN CΘ : In\nε →Rm with width ≤W, depth ≤D, and ||Θ||2 ≤E, there\nexist ∆(m, n, W, D, E, ε) and Λ(m, n, W, D, E, ε) ∈R+ such that\n(1) ||CΘ(x) −CΘ+α(x)||2 ≤∆(m, n, W, D, E, ε)||α||2, that is CΘ(x) is Lipschitz on Θ.\n(2) ||CΘ(x + δ) −CΘ(x)||2 ≤Λ(m, n, W, D, E, ε)||δ||, that is CΘ(x) is Lipschitz on x.\nThus C is Lipschitz on Θ and x.\nProof. Without loss of generality, let C be deﬁned as in (7). Then CΘ(x) = ΘD(· · · σ(Θ1x) · · · )\nwith Θ to be the set of all weight matrices, that is, Θ = {Θk|∀k ∈[D] = {1, 2 · · · , D}} and σ is\nReLU. The bias vectors are not considered, which can be included as parts of the weight matrices\nby extending the input space slightly, similar to [28]. We denote zk and bzk respectively to be\nthe outputs of the k-th hidden layers of CΘ and CΘ+α, which are zk = σ(Θk(· · · σ(Θ1x) · · · )) and\nbzk = σ(bΘk(· · · σ(bΘ1x) · · · )) and bΘi is weight matrices of CΘ+α, in particular z0 = bz0 ∈[−ε, 1+ε]n\nis the input. Since ||Θi −bΘi||2 ≤||α||2 for any i ∈[D] and |σ(a) −σ(b)| ≤|a −b|, we have\n||CΘ(x) −Cθ+α(x)||2\n= ||(ΘD −bΘD)zD−1 + bΘD(zD−1 −bzD−1)||2\n≤||ΘD −bΘD||2||zD−1||2 + ||bΘD||2||zD−1 −bzD−1||2\n= ||ΘD −bΘD||2||zD−1||2 + ||bΘD||2||σ(ΘD−1zD−2) −σ(bΘD−1bzD−2)||2\n≤||ΘD −bΘD||2||zD−1||2 + ||bΘD||2 ||ΘD−1zD−2 −bΘD−1bzD−2||2\n≤||ΘD −bΘD||2||zD−1||2 + ||bΘD||2(||ΘD−1 −bΘD−1||2||zD−2||2 + ||bΘD−1||2||zD−2 −bzD−2||2)\n≤||ΘD −bΘD||2||zD−1||2 + PD\nk=2(Qk−2\ni=0 ||bΘD−i||2)||ΘD−k+1 −bΘD−k+1||2||zD−k||2\n≤(||zD−1||2 + PD\nk=2(Qk−2\ni=0 ||bΘD−i||2)||zD−k||2)||α||2.\n5\nThe coeﬃcient ∆= (||zD−1||2 +PD\nk=2(Qk−2\ni=0 ||bΘD−i||2)||zD−k||2) is clearly bounded and depends\nm, n, W, D, E, ε. Thus CΘ(x) is Lipschitz on Θ. The Lipschitz continuity on x can be proved\nsimilarly:\n||CΘ(x + δ) −CΘ(x)||2\n= ||ΘD(· · · σΘ1(x + δ) · · · ) −ΘD(· · · σΘ1(x) · · · )||2\n≤||ΘD||2||σ(ΘD−1(· · · σ(Θ1(x + δ)) · · · )) −σ(ΘD−1(· · · σ(Θ1x) · · · ))||2\n≤||ΘD||2||ΘD−1(· · · σΘ1(x + δ) · · · ) −ΘD−1(· · · σΘ1(x) · · · )||2\n≤(QD\ni=1 ||Θi||2)||δ||2 ≤(QD\ni=1 ||Θi||2)√n||δ||.\nWe denote the coeﬃcient as Λ(m, n, W, D, E, ε). The lemma is proved. We can also extend this\nresult to convolutional neural networks.\n2.3\nContinuity of the loss function\nUnless mentioned otherwise, we assume that the loss function L(z, y) is continuous on z ∈Rm\nfor a ﬁxed y ∈Y. The mostly often used loss functions have much better properties. Consider\nthe following loss functions: the mean square error, the crossentropy loss, and the margin loss\nintroduced by Carlini-Wagner [6]:\nLmse(z, y) = ||z −1y||2\n2\nLce(z, y) = ln(Pm\ni=1 exp(zi)) −zy\nLcw(z, y) = maxl∈[m],l̸=y zl −zy\n(8)\nwhere 1y ∈Rm is the vector whose y-th entry is 1 and all other entries are 0.\nBy Lemma 2.1, we can assume that the loss function is deﬁned on a bounded cube:\nL(z, y) : [−B, B]m × Y →R\n(9)\nwhere B = Ω(n, m, D, W, E, ε). Since Y = [m] is discrete, we need only consider the continuity\nof L on z for a ﬁxed y.\nLemma 2.3. For a ﬁxed y, all three loss functions in (8) are Lipschitz continuous on z over\n[−B, B]m, with Lipschitz constants 2√m max{B, 1},\n√\n2,\n√\n2, respectively.\nProof. It suﬃces to show that ||∇zF(z)||2 ≤V is bounded over [−B, B]m. For a ﬁxed y, let\nf(z) = L(z, y).\nThen from ||∇zF(z)||2 ≤V , by the mean value theorem and the Schwarz\ninequality, we have ||F(z + δ) −F(z)||2 = ||F ′(z1)δ||2 ≤||F ′(z1)||2||δ||2 ≤V ||δ||2, where z1 ∈\n(−B, B)m. Thus L is Lipschitz with constant V .\nFor Lmse, we have ||∇zLmse(z, y)||2 = 2||(z −1y)||2 ≤2√m max{B, 1}. For Lce, we have\n||∇zLce(z, y)||2 =\nr Pm\ni=1 i̸=y exp(2zi)+(Pm\ni=1 i̸=y exp(zi))2\n(Pm\ni=1 exp(zi))2\n≤\n√\n2. For Lcw, we have ||∇zLcw(z, y)||2 =\n√\n2. The lemma is proved.\n3\nAdversarial training as a Stackelberg game\nIn this section, we formulate the adversarial deep learning as a Stackelberg game and prove the\nexistence of the Stackelberg equilibria.\n6\n3.1\nStackelberg game\nConsider a two-player zero-sum minmax sequential or Stackelberg game G = (SL, SF , ϕ), where\nSL and SF are respectively the strategy spaces for the leader and the follower of the game and\nϕ : SL × SF →R is the payoﬀfunction.\nIn the Stackelberg game G, the leader moves ﬁrst by picking a strategy sl ∈SL to minimize\nthe payoﬀ, knowing the existence of the follower. After knowing sl, the follower picks sf ∈SF\nto maximize the payoﬀ. Formally, (s∗\nl , s∗\nf) ∈SL × SF is called a Stackelberg equilibrium of G if\nγ(sl)\n=\n{argmaxsf ∈SF ϕ(sl, sf)} ⊂SF\n(10)\nis not empty for any sl ∈SL, and\ns∗\nl\n∈\nargminsl∈SL,S(sl)∈γ(sl) ϕ(sl, S(sl)) and s∗\nf ∈argmaxsf∈SF ϕ(s∗\nl , sf) = γ(s∗\nl ).\n(11)\nLet\nΓ\n=\n{(sl, sf) : sl ∈SL, sf ∈γ(sl)}.\n(12)\nThen, (11) is equivalent to (s∗\nl , s∗\nf) ∈argmin(sl,sf)∈Γ ϕ(sl, sf). We have the following result.\nTheorem 3.1 ([39]). If the strategy spaces are compact and the payoﬀfunction is continuous,\nthen the sequential game G has a Stackelberg equilibrium, which is also a subgame perfect Nash\nequilibrium of game G as an extensive form game [14].\n3.2\nAdversarial training as a Stackelberg game\nWe formulate adversarial deep learning as a two-player zero-sum minmax Stackelberg game Gs,\nwhich is the best defence for adversarial deep learning in certain sense.\nThe leader of the game is the Classiﬁer, whose goal is to train a robust DNN CΘ : In →\nRm in the hypothesis space H in (1). Without loss of generality, we assume that the parameters\nof C are in\nSc = [−E, E]K\n(13)\nfor some E ∈R+, that is, the strategy space for the Classiﬁer is Sc.\nThe follower of the game is the Adversary, whose goal is to create the best adversary\nwithin a given attack radius ε ∈R+. The strategy space for the Adversary is\nSa = {A : X →Bε}\n(14)\nwhere Bε = {δ ∈Rn : ||δ|| ≤ε} is the ball with the origin point as the center and ε as the\nradius. By considering the L∞norm, Sa becomes a metric space.\nThe payoﬀfunction. Given Θ ∈Sc and A ∈Sa, the payoﬀfunction is the expected loss\nϕs(Θ, A) = E(x,y)∼D L(CΘ(x + A(x)), y).\n(15)\nFrom (9), the composition of L and CΘ(x + A(x)) is well-deﬁned, since ||A(x)|| ≤ε.\nFor game Gs, γ and Γ deﬁned in (10) and (12) are\nγs(Θ)\n=\n{argmaxA∈Saϕs(Θ, A)} for Θ ∈Sc\nΓs\n=\n{(Θ, A) : Θ ∈Sc, A ∈γs(Θ)}\n(16)\nand (Θ∗\ns, A∗\ns) is a Stackelberg equilibrium of Gs if\nΘ∗\ns ∈argminΘ∈Sc,A(Θ)∈γs(Θ) ϕs(Θ, A(Θ)) and A∗\ns ∈argmaxA∈Sa ϕs(Θ∗\ns, A).\n(17)\n7\nLemma 3.2. ϕs(Θ, A) : Sc × Sa →R deﬁned in (15) is a continuous and bounded function.\nProof. It is clear that ϕs(Θ, A) is continuous on Θ, since L is continuous on z and CΘ is con-\ntinuous on Θ. Denote φ(x) = L(CΘ(x), y) : In\nε →R for ﬁxed Θ and y. Then φ(x) is uniformly\ncontinuous by Lemmas 2.2 and 2.3.\nGiven an A0 ∈Sa and ǫ > 0, since φ(x) is uniformly\ncontinuous, there exists a δ > 0 such that for A(x) ∈Sa satisfying ||A0(x) −A(x)||∞< δ, we\nhave |φ(x + A0(x)) −φ(x + A(x))| < ǫ for all x ∈X. Then\n|ϕs(Θ, A) −ϕs(Θ, A0)| = |E(x,y)∼D [L(CΘ(x + A(x)), y) −L(CΘ(x + A0(x)), y)]|\n≤E(x,y)∼D|L(CΘ(x + A(x)), y) −L(CΘ(x + A0(x)), y)|\n≤ǫ.\nHence ϕs(Θ, A) is continuous on Sa. By Lemma 2.1, ϕs(Θ, A) is bounded, since ||A(x)|| ≤ε.\nLemma 3.3. γs(Θ) ̸= ∅and A∗∈γs(Θ) if and only if A∗(x) ∈{argmaxA(x)∈Bε L(CΘ(x +\nA(x)), y)} for all (x, y) ∼D.\nProof. We have\nmaxA∈Sa ϕs(Θ, A)\n=\nmaxA∈Sa E(x,y)∼D L(CΘ(x + A(x)), y)\n≤\nE(x,y)∼D\nmax\nA(x)∈Bǫ\nL(CΘ(x + A(x)), y).\nSince L(C(x), y) is continuous on x and Bǫ is compact, for every (x, y), argmaxA(x)∈BǫL(CΘ(x +\nA(x)), y) exists. Thus, by choosing these maximum values, we obtain an A∗∈Sa, which achieves\nmaxA∈Sa ϕs(Θ, A). The lemma is proved.\nLemma 3.4. Γs is a closed set in Sc × Sa.\nProof. Let (Θi, Ai)∞\ni=1 ∈Γs converse to (Θ0, A0). Supposing (Θ0, A0) ̸∈Γs, we will obtain a\ncontradiction. By Lemma 3.3, there exists a (Θ0, A∗) ∈Γs, and thus, ϕs(Θ0, A∗) > ϕs(Θ0, A0)\nby (16). Let η = ϕs(Θ0, A∗) −ϕs(Θ0, A0) > 0. By Lemma 3.2, ϕs is continuous. Then there\nexists an i0 such that |ϕs(Θi0, Ai0) −ϕs(Θ0, A0)| < η/3 and |ϕs(Θi0, A∗) −ϕs(Θ0, A∗)| < η/3.\nWe thus have\nϕs(Θi0, Ai0) < ϕs(Θ0, A0) + η/3 = ϕs(Θ0, A∗) −2η\n3 < ϕs(Θi0, A∗) −η/3 < ϕs(Θi0, A∗)\nwhich contradicts to (Θi0, Ai0) ∈Γs meaning that ϕs(Θi0, Ai0) ≥ϕs(Θi0, A) for any A ∈Sa.\nThe lemma is proved.\nWe have\nTheorem 3.5. Game Gs has a Stackelberg equilibrium (Θ∗\ns, A∗\ns). Furthermore, Θ∗\ns is the solution\nto the adversarial training in (4).\nProof. By Lemma 3.2, ϕs(Θ, A) is bounded. Then α = inf(Θ,A)∈Γs ϕs(Θ, A) exists and is ﬁnite.\nThere exist (Θi, Ai)∞\ni=1 ∈Γs such that ϕs(Θi, Ai) converges to α. Since Sc is compact, we can\nassume that Θi converges to Θ0. Then there exists an A0 ∈Sa such that (Θ0, A0) ∈Γs.\nWe claim that ϕs(Θi, Ai) converges to ϕs(Θ0, A0). Suppose the contrary, that is, ϕs(Θ0, A0) >\nα. Then there exists an η > 0 such that ϕs(Θ0, A0) > α + η. Since ϕs(Θi, Ai) converges to\n8\nα, ∃K1 ∈N+ such that ϕs(Θk, Ak) < α + η\n3 for ∀k > K1. Since ϕs(Θ, A) is continuous on Θ,\n∃K2 ∈N+ such that ϕs(Θk, A0) > ϕs(Θ0, A0) −η\n3 for ∀k > K2. Then for k > max{K1, K2}, we\nhave\nϕs(Θk, A0) > ϕs(Θ0, A0) −η\n3 > α + 2η\n3 > ϕs(Θk, Ak) + η\n3 > ϕs(Θk, Ak)\nwhich contradicts to (Θk, Ak) ∈Γs. Then (Θ0, A0) is a Stackelberg equilibrium of game Gs.\nLet (Θ∗\ns, A∗\ns) be a Stackelberg equilibria of game Gs. By Lemma 3.3,\nΘ∗\ns\n∈\nargminΘ∈Sc,A(Θ)∈γ(Θ) ϕs(Θ, A(Θ))\n=\nargminΘ∈Sc,AΘ∈γ(Θ) E(x,y)∼D L(CΘ(x + AΘ(x)), y)\n∈\nargminΘ∈Sc E(x,y)∼D maxAΘ(x) L(CΘ(x + AΘ(x)), y)\n=\nargminΘ∈Sc E(x,y)∼D maxx∈B(x,ε) L(CΘ(x), y).\nBrieﬂy,\nΘ∗\ns\n=\nargminΘ∈Sc ϕs(Θ, argmaxA∈Sa ϕs(Θ, A))\n=\nargminΘ∈Sc maxA∈Sa ϕs(Θ, A).\n(18)\nThat is, Θ∗\ns is the solution to the adversarial training (4).\nRemark 3.6. As a consequence of Theorem 3.5, the Stackelberg game Gs gives the best defence\nin the hypothesis space H for a given attack radius, if using ARD in (6) to measure the robustness.\nPrecisely, let (Θ∗\ns, A∗\ns) be a Stackelberg equilibrium of game Gs. Then ARD(CΘ∗s, ε) = ARD(H, ε).\n3.3\nReﬁned properties of Γs\nIn the general case, γs(Θ) deﬁned in (16) may have more than one elements. In this section, we\nwill prove that if γs(Θ) contains a unique element, then Γs deﬁned in (16) is compact, which\nwill be used in section 6.\nAssumption A1. For any Θ ∈Sc, γs(Θ) = {A∗(Θ)} deﬁned in (16) has a unique element and\nthe loss function L is Lipschitz.\nRemark 3.7. Assumption A1 is true in the generic case. By Lemma 3.3, A∗∈γs(Θ) if and\nonly if A∗(x) ∈{argmaxA∈Bε L(CΘ(x + A), y)}.\nThen Assumption A1 is true if and only if\nargmaxA∈Bε L(CΘ(x + A), y) has a unique solution. Suppose the loss function is Lcw. Then\nφ(A) = L(CΘ(x+A), y) is a piecewise linear function in A and its graph over Bε is a polyhedron\nas illustrated in Figure 1. Then its maximum can be achieved only at the vertex of the polyhedron\nor the intersection of the (n−1)-dimensional sphere ||x−x0|| = ε and the one dimensional edges\nof the polyhedron. In the generic case, that is, when the parameters are suﬃciently general (refer\nto Assumption 3.1 in [46] for more details), there exists only one maximum.\nx0\nFigure 1: Illustration for the graph of L(C(x + A), y) as a function of x and A.\n9\nWe ﬁrst introduce three notations which will be used in this section. By Lemma 2.3, L(z, y)\nis Lipschitz for z over [−B, B]m when the loss functions in (8) are used, and let Ψ be the\nLipschitz constant. By Lemma 2.2, CΘ(x) is Lipschitz for Θ and x, and let ∆and Λ be the\nLipschitz constants, respectively.\nLemma 3.8. For any CΘ : In\nε →Rm and D, ϕs(Θ, A) deﬁned in (15) is Lipschitz on Θ and A\nwhen the loss function is Lipschitz.\nProof. Firstly, consider ϕs(Θ, A) for any ﬁxed A. For any ǫ > 0, let δ =\nǫ\nΨ∆. Then for any\nΘ1, Θ2 satisfying ||Θ1 −Θ2||2 ≤δ, we have\n|ϕs(Θ1, A) −ϕs(Θ2, A)|\n= |E(x,y)∼D[L(CΘ1(x + A(x)), y) −L(CΘ2(x + A(x)), y)]|\n≤E(x,y)∼D Ψ||CΘ1(x + A(x)) −CΘ2(x + A(x))||2\n≤E(x,y)∼D Ψ∆||Θ1 −Θ2||2 ≤ǫ\nthat is, ϕs(Θ, A) is Lipshitz continuous on Θ. The proof for the Lipschitz continuity on A is\nsimilar.\nLemma 3.9. For Θi ∈Sc, if limi→∞Θi = Θ0 and gi ∈γs(Θi), then for any (x, y) ∼D, the\nlimit of any convergent subsequence of {gi(x)}∞\ni=1 belongs to argmaxA∈BǫL(CΘ0(x + A), y).\nProof. The result can be proved similar to that of Lemma 3.4.\nLemma 3.10. Under Assumption A1, for any Θ ∈Sc, A∗(Θ)(x) is continuous on x.\nProof. Let {(xi, yi)}∞\ni=1 ⊂X × Y converges to (x0, y0). Since Y is ﬁnite, we may assume yi = y0\nfor all i. Then for any Θ, we will prove lim\ni→∞A∗(Θ)(xi) = A∗(Θ)(x0). Suppose the contrary.\nThen ∀η > 0, ||A∗(Θ)(xi) −A∗(Θ)(x0)|| > η holds for inﬁnitely many i. In the rest of the proof,\nwe assume η < ε/2.\nLet ζ = L(CΘ(x0+A∗(Θ)(x0)), y0)−maxα∈Bǫ,||α−A∗(Θ)(x)||>η L(CΘ(x0+α), y0). Since η < ε/2,\n{α ∈Bε : ||α −A∗(Θ)(x)|| > η} ̸= ∅. From the uniqueness of A∗(Θ), we have ε > 0. By the\nconvergence of {xi}∞\ni=1, ∃N, such that when i > N, ||x0 −xi|| <\nε\n3ΨΛ. There exists a k > N\nsuch that ||A∗(Θ)(xk) −A∗(Θ)(x0)|| > η. Then\nL(CΘ(xk + A∗(Θ)(x0)), y0)\n≥L(CΘ(x0 + A∗(Θ)(x0)), y0) −ΨΛ||x0 −xk||\n≥L(CΘ(x0 + A∗(Θ)(xk)), y0) −ΨΛ||x0 −xk|| + ζ\n≥L(CΘ(xk + A∗(Θ)(xk)), y0) −2ΨΛ||x0 −xk|| + ζ\n> L(CΘ(xk + A∗(Θ)(xk)), y0) + ζ/3\n> L(CΘ(xk + A∗(Θ)(xk)), y0)\nwhich contradicts to the deﬁnition of A∗(Θ)(xk). Hence A∗(Θ)(x) is continuous on x.\nLemma 3.11. Under Assumption A1, ψ(Θ) = ϕs(Θ, A∗(Θ)) : Sc →R is continuous on Θ.\n10\nProof. We will prove that for any ζ > 0, ∃δ > 0, such that if ||Θ1−Θ2||2 ≤δ then |ϕs(Θ1, A∗(Θ1))−\nϕs(Θ2, A∗(Θ2))| ≤ζ. Let δ =\nζ\nΨ∆. Then for any x,\nL(CΘ1(x + A∗(Θ1)(x)))\n≤L(CΘ2(x + A∗(Θ1)(x))) + Ψ∆δ\n≤L(CΘ2(x + A∗(Θ2)(x))) + Ψ∆δ\n= L(CΘ2(x + A∗(Θ2)(x))) + ζ.\nBy exchanging Θ1 and Θ2, we have |L(CΘ1(x+A∗(Θ1)(x)))−L(CΘ2(x+A∗(Θ2)(x)))| ≤ζ. Then\n|ϕs(Θ1, A(Θ1)) −ϕs(Θ2, A(Θ2))| ≤ζ. Thus ϕs(Θ, A∗(Θ)) is continuous on Θ.\nLemma 3.12. Under Assumption A1, A∗(Θ) : Sc →Sa is continuous.\nProof. It suﬃces to prove that when {Θn}∞\nn=1 converges to Θ0, lim\nn→∞A∗(Θn) = A∗(Θ0). Suppose\nthe contrary. Then there exist x ∈X and η > 0 such that ||A∗(Θn)(x) −A∗(Θ0)(x)|| > η holds\nfor inﬁnitely n. We assume η < ε/2.\nLet ζ = L(CΘ0(x + A∗(Θ0)(x)), y) −maxα∈Bǫ,||α−A∗(Θ0)(x)||>η L(CΘ0(x + α), y). It is clear\nthat ζ > 0. There exists an N ∈N+, such that for any n > N, we have ||Θn −Θ0||2 <\nε\n2Ψ∆\nand |L(CΘ0(x + A∗(Θ0)(x)), y) −L(CΘn(x + A∗(Θn)(x)), y)| < ε\n2 by Lemma 3.11. There exists\na j > N, ||A∗(Θj)(x) −A∗(Θ0)(x)|| > η. Then\nL(CΘ0(x + A∗(Θ0)(x)), y)\n≥L(CΘ0(x + A∗(Θj)(x)), y) + ζ\n≥L(CΘj(x + A∗(Θj)(x)), y) + ζ −Ψ∆||Θj −Θ0||2\n> L(CΘj(x + A∗(Θj)(x)), y) + ζ\n2\nwhich contradicts to |L(CΘ0(x + A∗(Θ0)(x)), y) −L(CΘj(x + A∗(Θj)(x)), y)| < ε\n2. Thus for any\nx, η > 0, there exists an N such that for n > N, ||A∗(Θn)(x) −A∗(Θ0)(x)|| ≤η holds, that is,\nlim\nn→∞||A∗(Θn) −A∗(Θ0)||∞= 0, which means A∗(Θ) is continuous on Θ.\nProposition 3.13. Under Assumption A1, Γs deﬁned in (12) is a compact set in Sc × Sa.\nProof. Given a sequence {(Θn, A∗(Θn))}∞\nn=1 in Γs, since Sa is compact, there exists a subse-\nquence {Θin}∞\nn=1 converges to Θ0, that is, lim\nn→∞Θin = Θ0. By Lemma 3.12, A∗(Θ) is continuous\non Θ, then lim\nn→∞A∗(Θin) = A∗(Θ0). Hence {(Θin, A∗(Θin))}∞\nn=1 is subsequence converging to\n(Θ0, A∗(Θ0)). By Lemma 3.4, Γs is closed, thus (Θ0, A∗(Θ0)) ∈Γs and Γs is compact.\n4\nA Stackelberg game to achieve maximal adversarial accuracy\nThe adversarial accuracy of a DNN C with respect to an attack radius ε is\nAAD(C, ε) = P(x,y)∼D (∀x ∈B(x, ε) ( bC(x) = y))\n(19)\nwhich is the most widely used robustness measurement for DNNs. Comparing to the robustness\nmeasurement ARD in (6), AAD(C, ε) does not depends on the loss function. In this section, we\nwill show that adversarial training with the Carlini-Wagner loss function will give a DNN with\nthe optimal adversarial accuracy.\nWe ﬁrst introduce a new game. Denote Ga to be the two person zero-sum minmax Stackelberg\ngame with the Classiﬁer as the leader, the Adversary as the follower, and\nϕa(Θ, A) = E(x,y)∼D LA(CΘ(x + A(x)), y).\n(20)\n11\nas the payoﬀfunction, where the loss function is deﬁned as\nLA(C(x), y) =\n(\n0\nLcw(C(x), y) ≥0\n−1\nLcw(C(x), y) < 0\n(21)\nand Lcw is the Carlini-Wagner loss function deﬁned in (8).\nFor game Ga, γ and Γ deﬁned in (10) and (12) are\nγa(Θ)\n=\n{argmaxA∈Saϕa(Θ, A)} for Θ ∈Sc\nΓa\n=\n{(Θ, A) : Θ ∈Sc, A ∈γa(Θ)}.\n(22)\nLemma 4.1. Let Aa ∈γa(Θ). Then ϕa(Θ, Aa) = −AAD(CΘ, ε).\nProof. Note that LA(C, x, y) = −1 if and only if bC(x) = y and LA(C, x, y) = 0 if and only if\nbC(x) ̸= y or there exists a k ̸= y such that Ck(x) = Cy(x). From Aa ∈γa(Θ), LA(CΘ(x +\nAa(x)), y) = −1 if and only if CΘ is robust over B(x, ε), or equivalently, bCΘ(x) = y for any\nx ∈B(x, ε). Then ϕs(Θ, Aa) = E(x,y)∼D LA(CΘ(x + Aa(x)), y) = −AAD(CΘ, ε).\nLemma 4.2. γa(Θ) ̸= ∅and A∗∈γa(Θ) if and only if A∗(x) ∈{argmaxx∈B(x,ε) LA(CΘ(x), y)}\nfor all (x, y) ∼D.\nProof. We ﬁrst show that γ(Θ, x) = {argmaxx∈B(x,ε) LA(CΘ(x), y)} ̸= ∅and lemma follows\nfrom this. Let x∗∈{argmaxx∈B(x,ε) Lcw(CΘ(x), y)}. Then x∗exists, since Lcw is continuous\nand B(x, ε) is compact.\nIf Lcw(C, x∗, y) ≥0, then LA(C, x∗, y) = 0 and x∗∈γ(Θ, x).\nIf\nLcw(C, x∗, y) < 0, then LA(C, x∗, y) = −1 for all x∗∈B(x, ε) and B(x, ε) = γ(Θ, x). In either\ncase, γ(Θ, x) ̸= ∅.\nLemma 4.3. Let (Θ∗\ncw, A∗\ncw) be a Stackelberg equilibrium of game Gs when the loss function is\nLcw deﬁned in (8). Then (Θ∗\ncw, A∗\ncw) is a Stackelberg equilibrium of game Ga.\nProof. By Lemma 4.1, γa(Θ) ̸= ∅. So it suﬃces to show that (Θ∗\ncw, A∗\ncw) ∈argmin(Θ,A(Θ))∈Γa\nϕa(Θ, A(Θ)). Denote γcw, Γcw, ϕcw to be γs, Γs, ϕs, when the loss function LCW is used.\nWe ﬁrst prove γcw(Θ) ⊂γa(Θ).\nHence Γcw ⊂Γa.\nBy Lemma 3.3, A∗∈γcw(Θ) =\n{argmaxA∈Saϕcw(Θ, A)} if and only if A∗\ncw(x) ∈γcw(Θ, x, y) = {argmaxA(x)∈Bε Lcw(CΘ(x +\nA(x)), y)}. By Lemma 4.2, A∗∈γa(Θ) if and only if A∗\na(x) ∈γa(Θ, x, y) = {argmaxA(x)∈Bε\nLa(CΘ(x+A(x)), y)}. Since Lcw(CΘ(x+A1), y) ≤Lcw(CΘ(x+A2), y) implies La(CΘ(x+A1), y) ≤\nLa(CΘ(x + A2), y), we have γcw(Θ, x, y) ⊂γa(Θ, x, y). Then A∗∈γcw(Θ) implies A∗∈γa(Θ).\nWe next prove\n{ϕa(Θ, A), ∀(Θ, A) ∈Γa} = {ϕa(Θ, A), ∀(Θ, A) ∈Γcw}.\n(23)\nSince Γcw ⊂Γa, it suﬃces to show {ϕa(Θ, A), ∀(Θ, A) ∈Γa} ⊂{ϕa(Θ, A), ∀(Θ, A) ∈Γcw}. For\n(Θa, Aa) ∈Γa, let Acw(x) ∈argmaxA∈BεLcw(CΘa(x + A), y). Then (Θa, Acw) ∈Γcw. We will\nshow that ϕa(Θa, Aa) = ϕa(Θa, Acw). By Lemma 4.2, A∗\na ∈γa(Θa) if and only if\nA∗\na(x) ∈γa(Θa, x, y) = {argmaxA(x)∈Bε La(CΘa(x + A(x)), y)}\nfor all (x, y) ∼D. If La(CΘa(x + A∗\na(x)), y) = −1, then Lcw(CΘa(x + A), y) < 0 for all A ∈Bε.\nIn this case, maxA∈Bε Lcw(CΘa(x + A), y) = Lcw(CΘa(x + A∗\ncw(x)), y) < 0 and hence La(CΘa(x +\n12\nA∗\ncw(x)), y) = −1. If La(CΘa(x + A∗\na(x)), y) = 0, then Lcw(CΘa(x + A∗\na(x)), y) ≥0. In this case,\nmaxA∈Bε Lcw(CΘa(x + A), y) = Lcw(CΘa(x + A∗\ncw(x)), y) ≥0 and hence La(CΘa(x + A∗\na(x)), y) =\nLa(CΘa(x + A∗\ncw(x)), y) = 0. Then we have ϕa(Θa, Aa) = ϕa(Θa, Acw).\nBy (23), (Θ∗\ncw, A∗\ncw) ∈argmin(Θ,A(Θ))∈Γcw ϕa(Θ, A(Θ)) = argmin(Θ,A(Θ))∈Γa ϕa(Θ, A(Θ)).\nThe lemma is proved.\nTheorem 4.4. Let (Θ∗\ncw, A∗\ncw) be a Stackelberg equilibrium of game Gs when the loss function\nis Lcw in (8). Then CΘ∗cw has the largest adversarial accuracy for all DNNs in H deﬁned in (1),\nthat is AAD(CΘ∗cw, ε) ≥AAD(CΘ, ε) for any CΘ ∈H.\nProof. By Lemma 4.3, (Θ∗\ncw, A∗\ncw) be a Stackelberg equilibrium of game Ga. By Lemma 4.1,\nAAD(CΘ∗cw, ε) = −ϕa(Θ∗\ncw, argmaxA∈Saϕa(Θ∗\ncw, A)) ≥−ϕa(Θ, argmaxA∈Saϕa(Θ, A)) = AAD\n(CΘ, ε). The theorem is proved.\nRemark 4.5. By Theorems 3.5 and 4.4, adversarial training using the loss function Lcw gives\na DNN which has the largest adversarial accuracy for all DNNs in the hypothesis space H, which\nanswers Question Q1 positively for the hypothesis space H.\n5\nTrade-oﬀbetween robustness and accuracy\nIn this section, we give trade-oﬀresults between the robustness and the accuracy in adversarial\ndeep learning from game theoretical viewpoint.\n5.1\nImprove accuracy under maximal adversarial accuracy\nBy Remarks 3.6 and 4.5, adversarial training computes the DNNs with the best robustness\nmeasurement. A nature question is whether we can increase the accuracy of the DNN and still\nkeep the maximal adversarial accuracy. That is, consider the bi-level optimization problem.\nΘ∗\no\n=\nargminΘ∗ϕ0(Θ∗)\nsubject to\nΘ∗\ns = argminΘ∈Sc maxA∈Saϕs(Θ, A)\n(24)\nwhere ϕ0 and ϕs are deﬁned in (2) and (15), respectively.\nFrom Remark 3.7, if using the loss function Lcw, γs(Θ) contains a unique solution and Θ∗\ns\nis unique in the generic case. In this case, we cannot increase the accuracy of the DNN when\nkeeping the maximal robust measure ARD.\nA more interesting case is to consider game Ga deﬁned in section 4, which uses the loss\nfunction LA deﬁned in (21).\nWe ﬁrst introduce an assumption. We train CΘ with stochastic gradient descent starting\nfrom a randomly choosing initial point, and most probably will terminate at a random point\nin the neighborhood of a minimal point or a saddle point of the loss function. Therefore, the\nfollowing assumption is valid for almost all trained DNNs [46].\nAssumption A2. The parameters of a trained CΘ are random values.\nWe now estimate the possible values of Θ∗\ns in (24). Suppose a ﬁnite data set T = {(xi, yi)}N\ni=1\nis chosen iid from the distribution D, which are used to train the network. Then it can be shown\n13\nthat the game Ga with payoﬀfunction (20) and trained with T has a Stackelberg equilibrium\n(Θ∗\na, A∗\na) (See section 6 for more details). With these notations, we have\nProposition 5.1. Under Assumption A2, there exists a ν ∈R+ such that for all Θ◦\na ∈RK\nsatisfying ||Θ◦\na −Θ∗\na|| < ν, game Ga has a Stackelberg equilibrium (Θ◦\na, A◦\na).\nProof. Denote φ(Θ, x) = Lcw(CΘ(x), y) for a ﬁxed y. Let x∗∈{argmaxx∈B(xi,ε) φ(Θ∗\na, x). If\nφ(Θ∗\na, x∗) < 0, then φ(Θ∗\na, x) < 0 for all x ∈B(xi, ε). Since B(xi, ε) is compact and φ(Θ, x)\nis continuous, there exists a νi ∈R+ such that φ(Θ∗\na + ∆, x) < 0 for all x ∈B(xi, ε) and all\n∆∈RK satisfying ||∆|| < νi. Without loss of generality, we can assume Θ∗\na + ∆∈Sc. It is easy\nto construct the best response of the Adversary in this case for Θ◦\na = Θ∗\na + ∆: A◦\na(xi) can be\nany point in B(xi, ε). If φ(Θ∗\na, x∗) > 0, then Si(Θ∗\na) = {x ∈B(xi, ε) : Lcw(CΘ∗a(x), y) ≤0} is a\ncompact set of dimension m, since Lcw(CΘ∗a(x), y) is piecewise linear in x. If νi is small enough,\nthen Si(Θ∗\na + ∆) is also a compact set of dimension m for all ∆∈RK and ||∆|| < νi. In this\ncase, A◦\na(xi) can be any point in Si(Θ∗\na + ∆).\nBy Assumption A2,the trained parameters of C are random values. φ(Θ∗\na, x∗) = Lcw(CΘ∗a(x∗),\ny) = 0 implies that CΘ∗a,i(x∗) = CΘ∗a,j(x∗) for i ̸= j, which gives an algebraic relation among the\nparameters of CΘ. This imposes an extra algebraic relation among the random parameters and\nthus will not happen under Assumption A2. So we have φ(Θ, x∗) ̸= 0 under Assumption A2.\nLet ν = minN\ni=1 νi > 0.\nThen for ||Θ◦\na −Θ∗\na|| < ν, there exists an A◦\na ∈Sc such that\nϕa(Θ◦\na, A◦\na) = ϕa(Θ∗\na, A∗\na), where ϕa is deﬁned in (20). Since (Θ∗\na, A∗\na) is a Stackelberg equilibrium\nfor game Ga, so is (Θ◦\na, A◦\na). The proposition is proved.\nBy Proposition 5.1, Θ∗\ns in (24) takes values in a K-dimensional set. As a consequence, there\nexist rooms for increase the accuracy under the maximal adversarial accuracy.\nExample 5.2. We use numerical experiments to show that it is possible to further increase\nthe accuracy under the maximal adversarial accuracy.\nTwo small CNNs with respectively 3\nand 4 hidden layers are used, which have structures (8 ∗3 ∗3), (16 ∗3 ∗3), (32 ∗3 ∗3) and\n(32∗3∗3), (64∗3∗3), (128∗3∗3), (128∗3∗3), respectively. We use loss function Lcw to achieve\nmaximal adversarial accuracy and the results are given in the columns 1-0 and 2-0 in Table 1.\nWe then retrain the CNNs using the normal loss function in (2) to increase the accuracy. In\norder to keep the maximal adversarial accuracy ﬁxed, the change of the parameters are limited\nto i% for i = 1, 2, 3 and the results are given in columns 1-i and 2-i, respectively. We can see\nthat the adversarial accuracies are barely changed (up to 0.06% and 0.02% for networks 1 and\n2), but the accuracies are increased evidently (up to 1.11% and 2.252% for networks 1 and 2).\nTable 1: Increase the accuracy (AC) under the condition of maximal adversarial accuracy (AA)\nfor CIFRA-10. The attack radius is 8/255 and 50000 samples are used.\nNetwork 1\nNetwork 2\n1-0\n1-1\n1-2\n1-3\n2-0\n2-1\n2-2\n2-3\nAC (%)\n45.718\n46.762\n46.814\n46.828\n72.156\n75.284\n75.344\n75.408\nAA (%)\n29.018\n28.996\n28.98\n28.958\n40.08\n40.076\n40.036\n40.06\n5.2\nAn eﬀective trade-oﬀmethod\nThe bi-level optimization problem (24) is in general diﬃcult to solve, especially when keeping\nthe maximal adversarial accuracy as mentioned in the proof of Proposition 5.1. A natural way to\n14\ntrain a robust and more accurate DNN is to do adversarial training with the following objective\nfunction\nϕt(Θ, A)\n=\nϕs(Θ, A) + λϕ0(Θ)\n(25)\nwhere λ > 0 is a small hyperparameter, ϕ0 and ϕs are deﬁned in (2) and (15), respectively.\nProblem (25) is also often used as an approximate way to solve (24). We will prove a trade-oﬀ\nresult in this setting.\nSimilar to Theorem 3.5, adversarial training with loss function (25) can be considered as a\nStackelberg game Gt with ϕt as the payoﬀfunction. Then we have the following trade-oﬀresult.\nProposition 5.3. Let (Θ∗\ns, A∗\ns) and (Θ∗\nt , A∗\nt ) be the Stackelberg equilibria of the zero-sum se-\nquential games with ϕs and ϕt as the payoﬀfunctions, respectively. Then\nAAD(CΘ∗s, ε) ≥AAD(CΘ∗\nt , ε), ϕs(Θ∗\ns, A∗\ns) ≤ϕs(Θ∗\nt , A∗\nt ) and ϕ0(Θ∗\ns) ≥ϕ0(Θ∗\nt )\nthat is, the network CΘ∗s is more robust but less accurate than CΘ∗\nt measured by ϕ0.\nProof. AAD(CΘ∗s, ε) ≥AAD(CΘ∗\nt , ε) is a consequence of Theorem 4.4. Since (Θ∗\nt, A∗\nt ) is a Stack-\nelberg equilibrium of game Gt, we have\nΘ∗\nt\n∈\nargminΘ∈Sc ϕt(Θ, argmaxA∈Sa ϕt(Θ, A))\n(26)\nA∗\nt\n∈\nargmaxA∈Sa ϕt(Θ∗\nt , A)\n=\nargmaxA∈Sa (ϕs(Θ∗\nt, A) + λϕ0(Θ∗\nt ))\n(27)\n=\nargmaxA∈Sa ϕs(Θ∗\nt , A)\nwhere the last equality is due to the fact that ϕ0(Θ∗\nt ) is free of A. Then, from (18),\nϕs(Θ∗\ns, A∗\ns)\n=\nϕs(Θ∗\ns, argmaxA∈Sa ϕs(Θ∗\ns, A))\n≤\nϕs(Θ∗\nt, argmaxA∈Sa ϕs(Θ∗\nt , A))\n(28)\n≤\nmaxA∈Saϕs(Θ∗\nt , A) = ϕs(Θ∗\nt , A∗\nt ).\nThe last equality comes from (27). From (26),\nϕt(Θ∗\nt , A∗\nt ) ≤ϕt(Θ∗\ns, argmaxA∈Sa ϕt(Θ∗\ns, A)) ≤maxA∈Saϕt(Θ∗\ns, A) = ϕt(Θ∗\ns, A∗\ns).\n(29)\nAdding inequalities (28) and (29), we obtain ϕ0(Θ∗\ns) ≥ϕ0(Θ∗\nt ). The proposition is proved.\nNote that this trade-oﬀresult is quite diﬀerent from the trade-oﬀtheorem in [42] in that,\nour result is for any data set, while the result in [42] is for a speciﬁcally designed data set.\n6\nComparing three types of games for adversarial deep learning\nIn this section, we compare three types of games for adversarial deep learning when the data\nT = {(xi, yi)}N\ni=1 ⊂In × Y are a ﬁnite number of samples chosen iid from the distribution D.\nIn this case, the strategy space for the Classiﬁer is still Sc in (13). The strategy space for\nthe Adversary becomes much simpler:\nSa = QN\ni=1{(xi, yi) : ||xi −xi|| ≤ε} ⊂(In\nε × Y)N\n(30)\n15\nwhere Iε = [−ε, 1 + ε]. For Θ ∈Sc and A = ((xi, yi))N\ni=1 ∈Sa, the empirical adversarial loss is\nϕT (Θ, A) = 1\nN\nPN\ni=1L(CΘ(xi), yi).\n(31)\nWe consider three games.\nThe adversarial training game G1, which is the zero-sum minmax sequential game with\nthe Classiﬁer as the leader, the Adversary as the follower, and ϕT (Θ, A) as the payoﬀfunction,\nthat is, to solve the following minmax problem\nΘ∗\n1 = argminΘ∈ScmaxA∈Sa ϕT (Θ, A)\n(32)\nwhich is clearly equivalent to the adversarial training. By Theorem 3.1, game G1 has a Stackel-\nberg equilibrium (Θ∗\n1, A∗\n1), since Sc and Sa are compact and ϕT (Θ, A) is continuous. Similar to\nsection 4, it can be shown that this game gives a DNN with the largest adversarial accuracy for\nthe data set T, when the loss function is Lcw.\nThe universal adversary game G2, which is the zero-sum maxmin sequential game with\nthe Adversary as the leader and the Classiﬁer as the follower, that is, to solve the following\nmaxmin problem\nA∗\n2 = argmaxA∈SaminΘ∈Sc ϕT (Θ, A)\n(33)\nBy Theorem 3.1, game G2 has a Stackelberg equilibrium (Θ∗\n2, A∗\n2). The solution (Θ∗\n2, A∗\n2) of this\ngame is to compute the optimal universal adversarial attack for the given hypothesis space H\nin (1), that is, A∗\n2(x) is the best adversary for any (x, y) ∼D and for all DNNs in H. It is\nclear that A∗\n2 is the optimal attack to the so-called nobox model proposed in [5], that is, nobox\nmodel has an optimal solution for DNNs with a given structure. This gives a positive answer to\nquestion Q2 for the hypothesis space H in (1).\nThe simultaneous adversary game G3.\nWe can also formulate the adversarial deep\nlearning as a simultaneous game G3. In this game, the two players and their strategy spaces\nare the same as that of game G1. The diﬀerence is the way to play the game. In game G3,\nthe Classiﬁer picks its action without knowing the action of the Adversary, and the Adversary\nchooses the attacking adversarial samples without knowing the action of the Classiﬁer. But,\nboth players know the payoﬀfunction. A point (Θ∗\n3, A∗\n3) ∈Sc ×Sa is called a pure strategy Nash\nequilibrium of game G3 if\nΘ∗\n3\n=\nargminΘ∈ScϕT (Θ, A∗\n3) and A∗\n3 = argmaxA∈SaϕT (Θ∗\n3, A).\n(34)\nIn general, pure strategy Nash equilibria do not necessarily exist, and mixed strategy Nash\nequilibria are usually considered. Mixed strategies for the Classiﬁer and the Adversary are two\nprobability distributions\neΘ : Sc →I and eA : Sa →I\nfor Θ and A, respectively. For a mixed strategy (eΘ, eA), the payoﬀfunction is\nϕT (eΘ, eA) = EΘ∼eΘ EA∼e\nA ϕT (Θ, A).\n(35)\nDenote eSc and eSa to be the sets of the mixed strategies for the Classiﬁer and the Adversary,\nrespectively. Then (eΘ∗\n3, eA∗\n3) ∈eSc × eSa is called a mixed strategy Nash equilibrium of game G3 if\neΘ∗\n3\n=\nargmineΘ∈e\nSc ϕT (eΘ, eA∗\n3) and eA∗\n3 = argmax e\nA∈e\nSa ϕT (eΘ∗\n3, eA).\n(36)\n16\nSince the strategy spaces of the two players are compact and the objective function is con-\ntinuous, by Glicksberg’s theorem [15], game G3 has a mixed strategy Nash equilibrium (eΘ∗\n3, eA∗\n3),\nand the minmax theorem holds for this equilibrium.\nRemark 6.1. By Proposition 3.13, we can show that, under Assumption A1, game G3 has a\nmixed strategy when the data set satisﬁes a general distribution D.\nProposition 6.2. Let (Θ∗\ni , A∗\ni ) be Nash equilibria of games Gi for i = 1, 2, 3, respectively (mixed\nstrategy for G3). Then\nϕT (Θ∗\n1, A∗\n1) ≥ϕT (Θ∗\n3, A∗\n3) ≥ϕT (Θ∗\n2, A∗\n2).\nProof. The mixed strategy (Θ∗\n3, A∗\n3) can be written as two distributions ∆c : Sc →I and ∆a :\nSa →I, respectively. To prove the ﬁrst inequality, we have\nϕT (Θ∗\n1, A∗\n1) = EA∼∆a ϕT (Θ∗\n1, A∗\n1)\n(18)\n≥EA∼∆a ϕT (Θ∗\n1, A) = ϕT (Θ∗\n1, A∗\n3)\n(36)\n≥ϕT (Θ∗\n3, A∗\n3).\nFor the second inequality, we have\nϕT (Θ∗\n2, A∗\n2) = EΘ∼∆c ϕT (Θ∗\n2, A∗\n2)\n(33)\n≤EΘ∼∆c ϕT (Θ, A∗\n2) = ϕT (Θ∗\n3, A∗\n2)\n(36)\n≤ϕT (Θ∗\n3, A∗\n3).\nThe proposition is proved.\nThe following example shows that the inequalities in Proposition 6.2 could be strict.\nExample 6.3. Consider a two-player zero-sum minmax game with payoﬀmatrix\n\u0012\n0\n−a\n−1\n0\n\u0013\nwhere 0 < a < 1. The strategy space for player one is the rows and its goal is minimize the\npayoﬀ. Then, the Stackelberg game with player one as the leader is to solve the minmax problem\nand a Stackelberg equilibrium is (Row 1, Column 1) with payoﬀ0. The Stackelberg game with\nplayer two (column) as the leader is to solve the maxmin problem and a Stackelberg equilibrium\nis (Row 1, Column 2) with payoﬀ−a. By the well known minmax theorem, the corresponding\nsimultaneous game has no Nash equilibrium since minmax ̸= maxmin, and a mixed strategy\nNash equilibrium exists: the ﬁrst player plays (\n1\n1+a,\na\n1+a) and the second player plays (\na\n1+a,\n1\n1+a)\nwith payoﬀ−\na\n1+a. We summarize the above discussion as follows:\nminmax\npayoﬀ= 0\nmaxmin\npayoﬀ= −a\nMixed strategy\npayoﬀ= −\na\n1+a ∈(−a, 0).\n7\nConclusion\nIn this paper, we give a game theoretical analysis for adversarial deep learning from a more\npractical viewpoint. In previous work, the adversarial deep learning was formulated as a simul-\ntaneous game. In order for the Nash equilibrium to exist, the strategy spaces for the Classiﬁer\nand the Adversary are assumed to be certain convex probability distributions, which are not\n17\nused in real applications. In this paper, the adversarial deep learning is formulated as a sequen-\ntial game with the Classiﬁer as the leader and the Adversary as the follower. In this case, we\nshow that the game has Stackelberg equilibria when the strategy space for the classiﬁer is DNNs\nwith given width and depth, just like people do in practice.\nWe prove that Stackelberg equilibria for such a sequential game is the same as the DNNs\nobtained with adversarial training.\nFurthermore, if the margin loss introduced by Carlini-\nWagner is used as the payoﬀfunction, the equilibrium DNN has the largest adversarial accuracy\nand is thus the provable optimal defence.\nBased on this approach, we also give theoretical\nanalysis for other important issues such as the tradeoﬀbetween robustness and the accuracy,\nand the generation of optimal universal adversaries.\nFor future research, it is desirable to develop practical methods to use mixed strategy in\ndeep learning, since it is proved that such strategy has more power than pure strategy when\nthe depth and width of the DNNs are ﬁxed. It is also interesting to analysis the properties of\nthe Nash equilibria for adversarial deep learning, such as whether the equilibria are regular or\nessential [12, 43]? Finally, we can use game theory to analyze other adversarial problems in deep\nlearning.\nReferences\n[1] A. Athalye, N. Carlini, D. Wagner. Obfuscated Gradients Give a False Sense of Security:\nCircumventing Defenses to Adversarial Examples. Proc. ICML, PMLR, 274-283, 2018.\n[2] A. Athalye, L. Engstrom, A. Ilyas, K. Kwok. Synthesizing Robust Adversarial Examples.\nArXiv: 1707.07397, 2017.\n[3] A. Azulay and Y. Weiss. Why Do Deep Convolutional Networks Generalize so Poorly to\nSmall Image Transformations? Journal of Machine Learning Research, 20, 1-25, 2019.\n[4] A. Bastounis, A.C. Hansen, V. Vla˘ci´c. The Mathematics of Adversarial Attacks in AI\n- Why Deep Learning is Unstable Despite the Existence of Stable Neural Networks.\narXiv:2109.06098, 2021.\n[5] J. Bose, G. Gidel, H. Berard, A. Cianﬂone, P. Vincent, S. Lacoste-Julien, W. Hamilton.\nAdversarial Example Games. Proc. NeurIPS, 2020.\n[6] N. Carlini, D. Wagner. Towards Evaluating the Robustness of Neural Networks. Proc. of\nIEEE Symposium on Security and Privacy, IEEE Press, 39-57, 2017.\n[7] N. Carlini, D. Wagner. Adversarial Examples are not Easily Detected: Bypassing Ten\nDetection Methods. Proc. 10th ACM Workshop on Artiﬁcial Intelligence and Security, 3-\n14, 2017.\n[8] A.S. Chivukula, X. Yang, W. Liu, T. Zhu, W. Zhou. Game Theoretical Adversarial Deep\nLearning With Variational Adversaries. IEEE Trans. Knowledge and Data Engineering,\n33(11), 3568-3581, 2021.\n[9] J. Cohen, E. Rosenfeld, Z. Kolter. Certiﬁed Adversarial Robustness via Randomized\nSmoothing. Proc. ICML, PMLR, 1310-1320, 2019.\n18\n[10] M.J. Colbrook, V. Antun, A.C. Hansen. The Diﬃculty of Computing Stable And Accurate\nNeural Networks: On The Barriers of Deep Learning and Smale’s 18th Problem. 119 (12)\ne2107151119, 2022.\n[11] N. Dalvi, P. Domingos, S. Mausam, D. Verma. Adversarial Classiﬁcation. Proc. KDD’04,\n99-108, ACM Press, New York, 2004.\n[12] E. van Damme. Stability and Perfection of Nash Equilibia. Springer, 1987.\n[13] T. Fiez, B. Chasnov, L.J. Ratliﬀ. Implicit Learning Dynamics in Stackelberg Games: Equi-\nlibria Characterization, Convergence Analysis, and Empirical Study. Proc. ICML, PMLR,\n2020.\n[14] D. Fudenberg and J. Tirole. Game Theory. MIT Press, Cambridge, MA, 1991.\n[15] I.L. Glicksberg. A Further Generalization of the Kakutani Fixed Point Theorem, with\nApplication to Nash Equilibrium Points. Proc. AMS, 3(1), 1952.\n[16] G. Gidel, D. Balduzzi, W.M. Czarnecki, M. Garnelo, Y. Bachrach. Minimax Theorem for\nLatent Games or: How I Learned to Stop Worrying about Mixed-Nash and Love Neural\nNets. arXiv:2002.05820v1, 2020.\n[17] P.W. Koh, P. Liang. Understanding Black-box Predictions via Inﬂuence Functions. Proc.\nICML, PMLR, 1885-1894, 2017.\n[18] Y.P. Hsieh, C. Liu, V. Cevher. Finding Mixed Nash Equilibria of Generative Adversarial\nNetworks. Proc. ICML, PMLR, 2019.\n[19] C. Jin, P. Netrapalli, M.I. Jordan. What is Local Optimality in Nonconvex-nonconcave\nMinimax Optimization? Proc. ICML, PMLR, 2020.\n[20] C.A. Kamhoua, C.D. Kiekintveld, F. Fang, Q. Zhu (eds). Game Theory and Machine\nLearning for Cyber Security. IEEE Press and Wiley, 2021.\n[21] A. Kurakin, I. Goodfellow, S. Bengio. Adversarial Examples in the Physical World. ArXiv:\n1607.02533, 2016.\n[22] Y. LeCun, Y. Bengio, G. Hinton. Deep Learning. Nature, 521(7553), 436-444, 2015.\n[23] Y. Liu, L. Wei, B. Luo, Q. Xu. Fault Injection Attack on Deep Neural Network. Proc. of\nthe IEEE/ACM International Conference on Computer-Aided Design, 131-138, 2017.\n[24] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, A. Vladu. Towards Deep Learning Models\nResistant to Adversarial Attacks. arXiv:1706.06083, 2017.\n[25] L. Meunier, M. Scetbon, R. Pinot, J. Atif, Y. Chevaleyre. Mixed Nash Equilibria in the\nAdversarial Examples Game. Proc. ICML, PMLR 139, 2021.\n[26] G. Mont´ufar, R. Pascanu, K. Cho, Y. Bengio. On the Number of Linear Regions of Deep\nNeural Networks. Proc. NIPS’2014, 2014.\n[27] S.M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, P. Frossard. Universal Adversarial Perturba-\ntions. Proc. CVPR, 1765-1773, 2017.\n19\n[28] B. Neyshabur, R. Tomioka, N. Srebro. Norm-based Capacity Control in Neural Networks.\nProc. COLT’15, 1376-1401, 2015.\n[29] A. Pal and R. Vidal. A Game Theoretic Analysis of Additive Adversarial Attacks and\nDefenses. Proc. NeurIPS, 2020.\n[30] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z.B. Celik, A. Swami. The Limitations of\nDeep Learning in Adversarial Settings. IEEE European Symposium on Security and Privacy,\nIEEE Press, 2016, 372-387.\n[31] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z.B. Celik, A. Swami. Practical Black-\nbox Attacks Against Machine Learning. Proc. ACM on Asia Conference on Computer and\nCommunications Security, ACM Press, 506-519, 2017.\n[32] R. Pinot, R. Ettedgui, G. Rizk, Y. Chevaleyre, J. Atif. Randomization Matters: How to\nDefend Against Strong Adversarial Attacks. Proc. ICML, PMLR, 2020.\n[33] M.S. Pydi and V. Jog. Adversarial Risk via Optimal Transport and Optimal Couplings.\nProc. ICML, PMLR, 2020.\n[34] F.A. Oliehoek, R. Savani, J. Gallego, E. van der Pol, R. Groß. Beyond Local Nash Equilibria\nfor Adversarial Networks. arXiv:1806.07268, 2018.\n[35] J. Rena, D. Zhanga, Y. Wangb, L. Chen, Z. Zhou, Y. Chen, X. Cheng, X. Wang, M.\nZhoua, J. Shi, Q. Zhang. A Uniﬁed Game-Theoretic Interpretation of Adversarial Robust-\nness arXiv:2103.07364v2, 2021.\n[36] A. Shafahi, W.R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, T. Goldstein.\nPoison Frogs! Targeted Clean-label Poisoning Attacks on Neural Networks. Proc. NeurIPS,\n6103-6113, 2018.\n[37] A. Shafahi, W.R. Huang, C. Studer, S. Feizi, T. Goldstein. Are Adversarial Examples\nInevitable? arXiv:1809.02104, 2018.\n[38] Y. Shoham and K. Leyton-Brown. Multiagent Systems: Algorithmic, Game Theoretic and\nLogical Foundations. Cambridge University Press, 2008.\n[39] M. Simaan and J.B. Cruz Jr. On the Stackelberg Strategy in Nonzero-sum Games. Journal\nof Optimization Theory and Applications, 11, 533-555, 1973.\n[40] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I.J. Goodfellow, R. Fergus.\nIntriguing Properties of Neural Networks. arXiv:1312.6199, 2013.\n[41] Y.L. Tsai, C.Y. Hsu, C.M. Yu, P.Y. Chen. Formalizing Generalization and Robustness of\nNeural Networks to Weight Perturbations. arXiv:2103.02200, 2021.\n[42] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, A. Madry. Robustness May Be at Odds\nWith Accuracy. Proc. ICML, PMLR, 2019.\n[43] W.T. Wu and J.H. Jiang. Essential Equilibrium Points of n-Person Noncooperative Games.\nScientia Sinica, 11(10), 1307-1322, 1962.\n20\n[44] H. Xu, Y. Ma, H.C. Liu, D, Deb, H. Liu J.L. Tang, A.K. Jain. Adversarial Attacks and\nDefenses in Images, Graphs and Text: A Review. International Journal of Automation and\nComputing, 17(2), 151-178, 2020.\n[45] Y.Y. Yang, C. Rashtchian, H. Zhang, R. Salakhutdinov, K. Chaudhuri. A Closer Look at\nAccuracy vs Robustness. arXiv:2003.02460v3, 2000.\n[46] L. Yu and X.S. Gao. Improve the Robustness and Accuracy of Deep Neural Network\nwith L2,∞Normalization. Accepted by Journal of Systems Science and Complexity, 2022.\narXiv:2010.04912.\n[47] L. Yu, Y. Wang, X.S. Gao. Adversarial Parameter Attack on Deep Neural Networks.\narXiv:2203.10502, 2022.\n[48] L. Yu and X.S. Gao. Robust and Information-theoretically Safe Bias Classiﬁer against\nAdversarial Attacks. arXiv:2111.04404, 2021.\n[49] H. Zhang, Y. Yu, J. Jiao, E.P. Xing, L.E. Ghaoui, M.I. Jordan. Theoretically Principled\nTrade-oﬀBetween Robustness and Accuracy. Proc. ICML, PMLR, 2019.\n[50] Y. Zhou, M. Kantarcioglu, B. Xi. A survey of Game Theoretic Approach for Adversarial\nMachine Learning. WIREs Data Mining Knowl Discov, 1-9, 2019.\n21\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.GT"
  ],
  "published": "2022-07-17",
  "updated": "2022-07-17"
}