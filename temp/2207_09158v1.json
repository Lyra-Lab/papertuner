{
  "id": "http://arxiv.org/abs/2207.09158v1",
  "title": "FedX: Unsupervised Federated Learning with Cross Knowledge Distillation",
  "authors": [
    "Sungwon Han",
    "Sungwon Park",
    "Fangzhao Wu",
    "Sundong Kim",
    "Chuhan Wu",
    "Xing Xie",
    "Meeyoung Cha"
  ],
  "abstract": "This paper presents FedX, an unsupervised federated learning framework. Our\nmodel learns unbiased representation from decentralized and heterogeneous local\ndata. It employs a two-sided knowledge distillation with contrastive learning\nas a core component, allowing the federated system to function without\nrequiring clients to share any data features. Furthermore, its adaptable\narchitecture can be used as an add-on module for existing unsupervised\nalgorithms in federated settings. Experiments show that our model improves\nperformance significantly (1.58--5.52pp) on five unsupervised algorithms.",
  "text": "FedX: Unsupervised Federated Learning\nwith Cross Knowledge Distillation\nSungwon Han⋆1,2[0000−0002−1129−760X], Sungwon Park⋆1,2[0000−0002−6369−8130],\nFangzhao Wu3[0000−0001−9138−1272], Sundong Kim2[0000−0001−9687−2409],\nChuhan Wu4[0000−0001−5730−8792], Xing Xie3[0000−0002−8608−8482], and\nMeeyoung Cha2,1[0000−0003−4085−9648]\n1 School of Computing, KAIST\n{lion4151, psw0416}@kaist.ac.kr\n2 Data Science Group, Institute for Basic Science\n{sundong, mcha}@ibs.re.kr\n3 Microsoft Research Asia\nwufangzhao@gmail.com, xingx@microsoft.com\n4 Tsinghua University\nwuchuhan15@gmail.com\nAbstract. This paper presents FedX, an unsupervised federated learn-\ning framework. Our model learns unbiased representation from decen-\ntralized and heterogeneous local data. It employs a two-sided knowledge\ndistillation with contrastive learning as a core component, allowing the\nfederated system to function without requiring clients to share any data\nfeatures. Furthermore, its adaptable architecture can be used as an add-\non module for existing unsupervised algorithms in federated settings.\nExperiments show that our model improves performance significantly\n(1.58–5.52pp) on five unsupervised algorithms.\nKeywords: Unsupervised representation learning, self-supervised learn-\ning, federated learning, knowledge distillation, data privacy\n1\nIntroduction\nMost deep learning techniques assume unlimited access to data during training.\nHowever, this assumption does not hold in modern distributed systems, where\ndata is stored at client nodes for privacy reasons [28,34]. For example, personal\ndata stored on mobile devices cannot be shared with central servers, nor can\npatient records in hospital networks. Federated learning is a new branch of col-\nlaborative technique to build a shared data model while securing data privacy;\nit is a method to run machine learning by involving multiple decentralized edge\ndevices without exchanging locally bounded data [2,36].\nIn federated systems, supervised methods have been used for a variety of\ndownstream tasks such as object detection [22], image segmentation [31], and\n⋆Equal contribution to this work.\narXiv:2207.09158v1  [cs.CV]  19 Jul 2022\n2\nS. Han et al.\n(a) Local knowledge distillation\n(b) Global knowledge distillation\nFig. 1: Illustration of two knowledge flows in FedX: (a) local knowledge distilla-\ntion progressively learns augmentation-invariant features, and (b) global knowl-\nedge distillation regularizes local models from bias.\nperson re-identification [45]. The main challenge here is the data’s decentralized\nand heterogeneous nature (i.e., non-IID setting), which obscures the global data\ndistribution. To address this issue, several methods have been proposed, includ-\ning knowledge distillation [45], control variates [13], and contrastive learning [19].\nThese methods necessitate that local clients have high-quality data labels.\nNowadays, the need for unsupervised federated learning is increasing to han-\ndle practical scenarios that lack data labels. This is the new frontier in federated\nlearning. There have been a few new ideas; for instance, Zhang et al. proposed\nFedCA, a model that uses local data features and external datasets to alleviate\ninconsistency in the representation space [42]. Wu et al. proposed FCL, which\nexchanges encrypted local data features for privacy and introduces a neighbor-\nhood matching approach to cluster the decentralized data across clients [38].\nHowever, these approaches allow data sharing among local clients and raise pri-\nvacy concerns.\nWe present FedX, a new advancement in unsupervised learning on federated\nsystems that learns semantic representation from local data and refines the cen-\ntral server’s knowledge via knowledge distillation. Unlike previous approaches,\nthis model is privacy-preserving and does not rely on external datasets. The\nmodel introduces two novel considerations to the standard FedAvg [23] frame-\nwork: local knowledge distillation to train the network progressively based on\nlocal data and global knowledge distillation to regularize data bias due to the\nnon-IID setting. This two-sided knowledge flow distinguishes our model.\nLocal knowledge distillation (Fig. 1a) maximizes the embedding similarity\nbetween two different views of the same data instance while minimizing that of\nother instances—this process is defined by the contrastive loss. We designed an\nadditional loss that relaxes the contrastive loss via soft labeling. Soft labels are\ncomputed as similarities between an anchor and randomly selected instances,\ncalled relationship vectors. We minimize the distance between relationship vec-\ntors of two different views in order to transfer structural knowledge and achieve\nfast training speed—this process is modulated by the relational loss.\nFedX: Unsupervised Federated Learning with Cross Knowledge Distillation\n3\nGlobal knowledge distillation (Fig. 1b) treats the sample representation passed\nby the global model as an alternative view that should be placed near the em-\nbedding of the local model. This process is also modulated by contrastive loss\nand relational loss. Concurrent optimization allows the model to learn semantic\ninformation while eliminating data bias through regularization. These objectives\ndo not require additional communication rounds or costly computation. More-\nover, they do not share sensitive local data or use external datasets.\n1. We propose an unsupervised federated learning algorithm, FedX, that learns\ndata representations via a unique two-sided knowledge distillation at local\nand global levels.\n2. Two-sided knowledge distillation helps discover meaningful representation\nfrom local data while eliminating bias by using global knowledge.\n3. FedX can be applied to extant algorithms to enhance performance by 1.58–\n5.52pp in top-1 accuracy and further enhance training speed.\n4. Unlike other unsupervised federated learning approaches, FedX preserves\nprivacy between clients and does not share data directly. It is also lightweight\nand does not require complex communication for sending data features.\n5. FedX is open-sourced at https://github.com/Sungwon-Han/FEDX.\n2\nRelated Work\n2.1\nUnsupervised Representation Learning\nThere are two common approaches to unsupervised representation learning. One\napproach is to use generative models like autoencoder [33] and adversarial learn-\ning [30] that learn the latent representation by mimicking the actual data distri-\nbution. Another method is to use discriminative models with contrastive learn-\ning [5,27,40]. Contrastive learning approaches teach a model to pull the represen-\ntations of the anchor and its positive samples (i.e., different views of the image)\nin embedding space, while pushing the anchor apart from negative samples (i.e.,\nviews from different images) [7,18].\nIn contrastive learning, SimCLR [3] employs data augmentation to generate\npositive samples. MoCo [8] introduces a momentum encoder and dynamic queue\nto handle negative samples efficiently. BYOL [6] reduces memory costs caused by\na large number of negative samples. ProtoCL [18] uses prototypes to group se-\nmantically similar instances into local clusters via an expectation-maximization\nframework. However, under distributed and non-IID data settings, as in feder-\nated systems, these methods show a decrease in accuracy [42].\n2.2\nFederated Learning\nFederated Averaging (FedAvg) by McMahan et al. is a standard framework for\nsupervised federated learning [23]. Several subsequent studies improved the lo-\ncal update or global aggregation processes of FedAvg. For instance, external\ndataset [43], knowledge distillation [45], control variates [13,20], and contrastive\n4\nS. Han et al.\nlearning [19] can be applied for better local update process. Similarly, global ag-\ngregation process can be improved via Bayesian non-parametric approaches [35],\nmomentum updates [11], or normalization methods [37].\nUnsupervised federated learning is more difficult to implement because no\nlabels are provided and clients must rely on locally-defined pretext tasks that\nmay be biased. This is a less explored field, with only a few methods proposed.\nFedCA [42] shares local data features and uses an external dataset to reduce\nthe mismatch in representation space among clients. FCL [38] encrypts the local\ndata features before exchanging them. Because of the explicit data sharing, these\nmethods raise new privacy concerns. We, on the other hand, consider a com-\npletely isolated condition that does not permit any local data sharing. FedU [44]\nis another approach in the field that improves on the global aggregation method.\nIt decides how to update predictors selectively based on the divergence of local\nand global models. Our model is orthogonal to FedU, and both concepts can be\nused in tandem.\n2.3\nKnowledge Distillation\nKnowledge distillation aims to effectively train a network (i.e., student) by dis-\ntilling the knowledge of a pretrained network (i.e., teacher). Knowledge can be\ndefined over the features at the intermediate hidden layers [15,16], logits at the\nfinal layer [10], or structural relations among training samples [29,32,24]. Self-\nknowledge distillation uses the student network itself as a teacher network and\nprogressively uses its knowledge to train the model [12,14]. We leverage this\nconcept to efficiently train the local model while preserving the knowledge of\nthe global model. FedX is the first-of-a-kind approach that uses the knowledge\ndistillation concept for unsupervised federated learning.\n3\nModel\n3.1\nOverview\nProblem statement. Consider a federated system in which data can only be\nviewed locally at each client and cannot be shared outside. Our goal is to train a\nsingle unsupervised embedding model Fϕ that maps data points from each client\nto the embedding space. Let us denote local data and model from client m as\nDm and f m\nθ respectively (i.e., m ∈{1, ..., M}). The main objective for the global\nmodel Fϕ is as follows:\n  \\ arg\nm\nin _ \\\np\nh\ni \\\nmath\ncal  {L}(\\\nphi )  &= \\ s um _{m=1}^{M} {|\\mathcal {D}^m| \\over |\\mathcal {D}|}\\mathcal {L}_m(\\phi ), \\nonumber \\\\ \\text {where } \\mathcal {L}_m(\\phi ) &= \\mathbb {E}_{\\mathbf {x} \\in \\mathcal {D}^m}[l_m (\\mathbf {x}; \\phi )].x\n(1)\nLm represents the local objective in client m and lm is the empirical loss objective\nof Lm over Dm. For simplicity, we hereafter denote the local model f m\nθ at client\nm and global model Fϕ as f m and F.\nFedX: Unsupervised Federated Learning with Cross Knowledge Distillation\n5\nFig. 2: Illustration of the FedAvg framework [23], which is used as the base\nstructure of many federated systems. FedX modifies the local update process 1\n○.\nWe use FedAvg [23] as the underlying structure, and the data flow is depicted\nin Fig. 2. Four processes run in each communication round: Process 1\n○on local\nupdate is when each local client trains a model f m with its data Dm for E local\nepochs; Process 2\n○on local model upload occurs when clients share the trained\nmodel weights with the server; Process 3\n○on global aggregation occurs when\nthe central server averages the received model weights and generates a shared\nglobal model F; Process 4\n○on global model download is when clients replace their\nlocal models with the downloaded global model (i.e., averaged weights). These\nprocesses run for R communication rounds.\nFedX modifies the Process 1\n○by redesigning loss objectives in order to distill\nknowledge at both the local and global scales. The following sections introduce\nthe design components of our unsupervised federated learning model.\n3.2\nLocal Knowledge Distillation\nThe first significant change takes place with local clients, whose goal is to learn\nmeaningful representations from local data. Let us define a data pair; xi and ˜xi\nbe two augmented views of the same data instance. The local contrastive loss\nLlocal\nc\nlearns semantic representation by maximizing the agreement between xi\nand ˜xi while minimizing the agreement of views from different instances (i.e.,\nnegative samples). We showcase the proposed contrastive loss from two of the\nunsupervised representation learning methods as vanilla baselines.\n∗SimCLR [3] utilizes a contrastive objective based on the InfoNCE loss [26].\nGiven a batch B with size n and its augmented version ˜B, each anchor has\na single positive sample and considers all other (2n −2) data points to be\nnegative samples. The following is the definition of this (2n−1)-way instance\ndiscrimination loss, where τ is the temperature used to control the entropy\n6\nS. Han et al.\n(a) Two-sided contrastive loss\n(b) Two-sided relational loss\nFig. 3: The overall architecture of FedX, with the local model f m, the projection\nhead hm, and the global model F at local client m. Two-sided (a) contrastive\nloss and (b) relational loss enable the model to learn semantic information from\nlocal data while regularizing the bias by distilling knowledge from the global\nmodel. FedX modifies the process 1\n○on local update in Fig. 2.\nvalue and sim(·) is the cosine similarity function between two embeddings:\n  &L_{\n\\\nt e xt \n{c}}^\\text {local}\n \n= -\\l o\ng {\\te xt {exp}({\\ text {s\nim}\n(\\mat hb f  {z}_i,  \\m a t hbf {\\tilde {z}}_i) / \\tau }) \\over {\\sum _{k \\in (\\mathcal {B} \\cup \\mathcal {\\tilde {B}} - \\{i\\})} \\text {exp}(\\text {sim}(\\mathbf {z}_i, \\mathbf {z}_k) / \\tau )}}, \\\\ &\\text {where } \\mathbf {z}_i = f^m (\\mathbf {x}_i), \\ \\mathbf {\\tilde {z}}_i = f^{m} (\\mathbf {\\tilde {x}}_i).\\label {eq:contrastive_loss}\n(3)\n∗BYOL [6] does not train on negative samples. Instead, an asymmetric ar-\nchitecture is used to prevent the model from learning trivial solutions. The\nmodel f m with a prediction layer gm is trained to predict a view from the\nexponential moving average model f m\nema. The loss is defined as follows:\n  &L_{\n\\\nt\next {c}}^\\ t ext {\nl\nocal} =\n \n\\\nnor\nm\n \n[\\B\nig ]{  \\ m at h b f {z}_ {i} /\n \n\\ n o\nrm { \\mathbf {z}_{i}} - \\mathbf {\\tilde {z}^{\\text {ema}}}_i / \\norm {\\mathbf {\\tilde {z}^{\\text {ema}}}_i}}^2, \\\\ &\\text {where } \\mathbf {z}_{i} = g^m \\circ f^m (\\mathbf {x}_i), \\ \\mathbf {\\tilde {z}^{\\text {ema}}}_i = f^m_{\\text {ema}}(\\mathbf {\\tilde {x}}_i).\\label {eq:byol_loss}\n(5)\nWe consider another design aspect to help the model learn structural knowl-\nedge more effectively. Motivated by the concept of relational knowledge distil-\nlation [1,41], structural knowledge represented as relations among samples is\nextracted from the local model and progressively transferred back to itself. This\nFedX: Unsupervised Federated Learning with Cross Knowledge Distillation\n7\nentails selecting a set of instances at random Br and computing the cosine simi-\nlarity between the embeddings of two different views xi, ˜xi and random instances\nBr. We then apply the softmax function to the similarity vector to compute re-\nlationship probability distributions ri and ˜ri (Eq. 6). In vector notation, the\nsuperscript j represents the j-th component value of a given vector.\n  \n\\ m\nathbf {r}_i ^j = \\\nf\nrac {\\exp (\\tex t {sim} (\\m\na t\nhbf {z}_i, \\ mathbf\n \n{z}_ j)/\\tau )}{\\ sum _{k \\in \\mathcal {B}_r} \\exp (\\text {sim}(\\mathbf {z}_i, \\mathbf {z}_k)/\\tau )},\\ \\ \\mathbf {\\tilde {r}}_i^j = \\frac {\\exp (\\text {sim}(\\mathbf {\\tilde {z}}_i, \\mathbf {z}_j)/\\tau )}{\\sum _{k \\in \\mathcal {B}_r} \\exp (\\text {sim}(\\mathbf {\\tilde {z}}_i, \\mathbf {z}_k)/\\tau )} \\label {eq:relation}\n(6)\nThe above concept, local relational loss, is defined as the Jensen-Shannon\ndivergence (JSD) between two relationship probability distributions ri and ˜ri\n(Eq. 7). Minimizing the discrepancy between two distributions make the model\nto learn structural knowledge invariant to data augmentation. In contrastive\nlearning with soft targets, this divergence loss can also be interpreted as relaxing\nthe InfoNCE objective.\n  L_{r\n}\n^ \\\ntext {local} =\n \n\\ f r\nac {˜}{2} \\text\n \n{K L}(\\m athbf {\nr\n} _\ni \\V e rt \\mathbf {r}_i^{\\text {target}}) + \\frac {1}{2} \\text {KL}(\\mathbf {\\tilde {r}}_i \\Vert \\mathbf {r}_i^{\\text {target}}),\\text { where } \\mathbf {r}_i^{\\text {target}} = \\frac {1}{2}(\\mathbf {r}_i + \\mathbf {\\tilde {r}}_i) \\label {eq:relational_loss}\n(7)\nThe total loss term for local knowledge distillation is given in Eq. 8:\n  L_{\\tex t  {loca\nl\n- KD}} =\n \nL_{c}^\\text {local} + L_{r}^\\text {local}. \\label {eq:total_loss}\n(8)\n3.3\nGlobal Knowledge Distillation\nThe second major change is to regularize the bias contributed by the inconsis-\ntency between local and overall data distribution. The inconsistency addresses\nthe issue of decentralized non-IID settings, where local clients are unaware of\nglobal data distribution. Training the local model f m will be suboptimal in this\ncase because the local update process becomes biased towards local minimiz-\ners [42]. Such data inconsistency among local clients can be resolved by distilling\nknowledge on a global scale.\nWe consider two kinds of losses: global contrastive loss and global relational\nloss. Because the global model simply aggregates model weights at the local\nclients in FedAvg, we can think of the sample’s embedding from the global model\nas an alternate view of the same data instance. The global contrastive loss max-\nimizes the agreement between the views of the local and global models from the\nsame instance while minimizing that of all other views from different instances.\nEach communication round assumes that the central server sends a fixed set\nof averaged model weights (i.e., global model F) to the client. The batch B and\nits augmented version ˜B are then used to train the local model f m as in Eq. 9\nwith the InfoNCE loss. To match the embedding space between the local and\nglobal models, we consider an additional prediction layer hm on top of local\nmodels. Similar method has been used in [4,6].\n  L_{c}\n^\n\\ t ext\n {global} \n= &-\\\nl og {\n\\\ntext {exp\n}({\\text {\nsi m}\n(\\mat h\nb\nf { z\n}^l_i,\n \\mathbf {\n\\t il\nde {z}\n}^g_i ) \n/  \\t a u  }) \\o ver\n { \\s u m  \\limit s \n_ { k \\in (\\m\na t hcal {B} - \\{i\\})} \\text {exp}(\\text {sim}(\\mathbf {z}^l_i, \\mathbf {z}^l_k) / \\tau ) + \\sum \\limits _{k \\in (\\mathcal {\\tilde {B}} - \\{i\\})}} \\text {exp}(\\text {sim}(\\mathbf {z}^l_i, \\mathbf {z}^g_k) / \\tau )}, \\nonumber \\\\ \\text {where } \\mathbf {z}^l_i &= h^{m} \\circ f^{m} (\\mathbf {x}_i), \\ \\mathbf {\\tilde {z}}^l_i = h^{m} \\circ f^{m} (\\mathbf {\\tilde {x}}_i), \\ \\mathbf {z}^g_i = F (\\mathbf {x}_i), \\ \\mathbf {\\tilde {z}}^g_i = F (\\mathbf {\\tilde {x}}_i). \\label {eq:contrastive_loss_global}\n(9)\n8\nS. Han et al.\nWe introduce the global relational loss on top of the global contrastive loss.\nThis loss is defined in the same way as the local relational loss (Eq. 7), but it\nincludes global model embeddings. It regularizes the model by penalizing any\nmismatch between two augmented views over the global embedding space after\nthe prediction layer hm. As a result, the model maintains its local knowledge\nbased on local data while learning augmentation-invariant knowledge using the\nglobal contrastive loss.\nGiven two different views xi, ˜xi and random instances Br, the relationship\nprobability distributions for global relational loss, r′\ni and ˜r′\ni, are defined (Eq. 10).\nWe again adopt the JS divergence between two relationship probability vectors\nr′\ni and ˜r′\ni as the global relational loss (Eq. 11).\n  \\\nm a\nthbf {r}_{\ni} '^\nj &= \n\\\nfrac  {\\exp (\\t\nex t \n{sim}( \\mat\nh b\nf {z}_i^l, \n\\m at\nhbf {\nz\n}_j^ g)/\\tau )}{\n\\s um\n _{k \n\\in \n\\mathca\nl\n {\nB}_r} \n\\exp (\\tex\nt\n { s\nim}(\\ma\nthbf {z}_i\n^\nl,  \\mat hbf {z}_\nk\n^ g\n)/\\t\na u  )}\n,\\ \\ \\mathbf {\\tilde {r}}_{i}'^{j} = \\frac {\\exp (\\text {sim}(\\mathbf {\\tilde {z}}_i^l, \\mathbf {z}_j^g)/\\tau )}{\\sum _{k \\in \\mathcal {B}_r} \\exp (\\text {sim}(\\mathbf {\\tilde {z}}_i^l, \\mathbf {z}_k^g)/\\tau )} \\label {eq:global_relation} \\\\ L_{r}^\\text {global} &= \\frac {1}{2} \\text {KL}(\\mathbf {r}'_i \\Vert \\mathbf {r}_{i}'^{\\text {target}}) + \\frac {1}{2} \\text {KL}(\\mathbf {\\tilde {r}}'_i \\Vert \\mathbf {r}_{i}'^{\\text {target}}),\\text { where } \\mathbf {r}_{i}'^{\\text {target}} = \\frac {1}{2}(\\mathbf {r}'_i + \\mathbf {\\tilde {r}}_{i}') \\label {eq:global_relational_loss} (11)\nThe total loss for global knowledge distillation is given in Eq. 12. The overall\nmodel then combines losses from knowledge distillation at the local and global\nlevels, as shown in Eq. 13. The detailed algorithm is described in the appendix.\n  &L_{\\tex t  {globa\nl\n- KD}} = \nL\n_{c}\n^\\text {g l obal} + L _ {r}^\\text {global} \\label {eq:total_loss_global} \\\\ &L_{\\text {total-KD}} = L_{\\text {local-KD}} + L_{\\text {global-KD}} \\label {eq:overall_loss}\n(13)\n4\nExperiment\nUsing multiple datasets, we compared the performance of our model to other\nbaselines and investigated the role of model components and hyperparameters.\nWe also used embedding analysis to examine how the proposed model achieves\nthe performance gain. Finally, we applied the model in a semi-supervised setting.\n4.1\nPerformance Evaluation\nData settings.\nThree benchmark datasets are used. CIFAR-10 [17] contains\n60,000 images of 32×32 pixels from ten classes that include airplanes, cats, and\ndogs. SVHN [25] contains 73,257 training images and 26,032 test images with\nsmall cropped digits of of 32×32 pixels. F-MNIST [39] contains 70,000 images\nof 28×28 pixels from ten classes, including dresses, shirts, and sneakers.\nWe used the Dirichlet distribution to enforce the non-IID property of local\nclients. Let DirN(β) denote the Dirichlet distribution with N clients and β as\nthe concentration parameter. We take a sample pk,j from DirN(β) and assign\nclass k to client j based on the sampled proportion pk,j. With this data allocation\nstrategy, each client will be assigned a few data samples for each class (or even\nnone) to ensure bias. By default, N and β are to 10 and 0.5, respectively, similar\nto other research [19].\nFedX: Unsupervised Federated Learning with Cross Knowledge Distillation\n9\nTable 1: Performance improvement with FedX on classification accuracy over\nthree datasets. Both the final round accuracy and the best accuracy show that\nour model brings substantial improvement for all baseline algorithms.\nMethod\nCIFAR-10\nSVHN\nF-MNIST\nLast\nBest\nLast\nBest\nLast\nBest\nFedSimCLR\n51.31\n52.88\n75.19\n76.50\n77.66\n79.44\n+ FedX\n56.88\n57.95\n77.19\n77.70\n81.98\n82.47\nFedMoCo\n56.74\n57.82\n70.69\n70.99\n82.31\n83.58\n+ FedX\n58.23\n59.43\n73.57\n73.92\n83.62\n84.65\nFedBYOL\n52.24\n53.14\n65.95\n67.32\n81.45\n82.37\n+ FedX\n56.49\n57.79\n68.94\n69.05\n83.18\n84.30\nFedProtoCL\n51.33\n52.12\n49.85\n50.19\n81.76\n83.57\n+ FedX\n55.36\n56.76\n69.31\n69.75\n82.74\n83.34\nFedU\n50.79\n50.79\n66.02\n66.22\n80.59\n82.03\n+ FedX\n56.15\n57.26\n68.13\n68.39\n83.73\n84.12\nImplementation details.\nThe model was trained for 100 communication\nrounds, with 10 local epochs in each round. The ResNet18 backbone [9] and\nthe SGD optimizer with a learning rate of 0.01 were used. SGD weight decay\nwas set to 1e-5, SGD momentum was set to 0.9, and batch size was set to 128. For\nall objectives, the temperature τ was set as 0.1. Augmentations included random\ncrop, random horizontal flip, and color jitter. We used four A100 GPUs.\nBaselines.\nWe implemented five baselines: (1) FedSimCLR based on Sim-\nCLR [3], (2) FedMoCo based on MoCo [8], (3) FedBYOL based on BYOL [6],\nand (4) FedProtoCL based on ProtoCL [18]. These are unsupervised models\nthat are built on top of FedAvg [23]. The final baseline (5) FedU [44] is built\nover FedBYOL and downloads a global model by divergence-aware module (see\nprocess 4\n○in Figure 2). For a fair comparison, we applied the same experi-\nmental settings on these baselines, including the backbone network, optimizer,\naugmentation strategy, number of local epochs, and communication rounds. We\nused the original implementations and hyper-parameter settings for FedU. Un-\nless otherwise specified, we refer to FedSimCLR as the representative baseline\nin the remainder of this section.\nEvaluation.\nAll models were compared using the linear evaluation protocol,\nwhich is a method for training a linear classifier on top of representations [42,44].\nWe freeze the backbone network of each trained model after training. Then, for\nthe next 100 epochs, a new classifier is appended and trained with ground-\ntruth labels. The top-1 classification accuracy over the test set is reported as an\nevaluation metric.\nResults. Table 1 summarizes the performance comparison, where FedX brings\nmeaningful performance improvements over the baseline algorithms. On average,\n10\nS. Han et al.\n(a) Performance gain on FedSimCLR\n(b) Performance gain on FedBYOL\nFig. 4: Performance comparison between two vanilla baselines (i.e., FedSimCLR\nand FedBYOL) and FedX-enhanced versions over communication rounds. FedX\nhelps models outperform in all three benchmark datasets and continues to bring\nadvantage with increasing communication rounds.\nour model improves CIFAR-10 by 4.29 percent points (pp), SVHN by 5.52pp,\nand F-MNIST by 1.58pp across all baselines. One exception is F-MNIST, where\nFedProtoCL by itself has a slightly higher best accuracy. However, adding FedX\nstill contributes to improved final round accuracy, implying that the model has\ngood training stability.\nWe then examine how quickly the model improves baselines across the various\ncommunication rounds. Figure 4 shows the trajectory for two example baselines\non FedSimCLR and FedBYOL.5 These plots confirm that model-enhanced mod-\nels outperform vanilla baselines; most plots show this benefit early in the commu-\nnication rounds. We see that local bias can degrade the performance of a baseline\nmodel during the early training phase in some cases (see the F-MNIST case in\nFigure 4a). This is most likely due to the biased contrastive objective caused by\nlocally sampled negatives. In contrast, adding FedX prevents such deterioration\nand even continues to improve accuracy as communication rounds increase.\n4.2\nComponent Analyses\nAblation study. FedX used learning objectives at the local and global levels\nseparately, with two types of losses: contrastive loss and relational loss. In this\nsection, we look at ablations by removing each learning objective or loss compo-\nnent and testing the added value of each design choice to overall performance.\n5 Results for other baselines are presented in the Appendix.\nFedX: Unsupervised Federated Learning with Cross Knowledge Distillation\n11\nFig. 5: Performance comparison of ablations over communication rounds for\nCIFAR-10. Removing any module leads to performance degradation. Ablation\non contrastive loss Lc showed the best accuracy of 35.13% and hence excluded.\nTable 2: Ablation results with different global-scale regularization methods. The\nproposed global knowledge distillation performs the best among them.\nMethod\nCIFAR-10\nSVHN\nF-MNIST\nLast\nBest\nLast\nBest\nLast\nBest\nLlocal-KD only\n51.89\n52.85\n76.64\n77.20\n79.79\n80.42\nLlocal-KD + SCAFFOLD\n52.73\n53.20\n75.18\n75.52\n79.45\n80.36\nLlocal-KD + FedProx\n52.48\n53.34\n77.43\n77.79\n79.83\n80.24\nLlocal-KD + Lglobal-KD\n56.88\n57.95\n77.19\n77.70\n81.98\n82.47\nFigure 5 plots the performance comparison of different ablations across the com-\nmunication round. The complete model has the highest accuracy, implying that\nremoving any component reduces performance. It also confirms the importance\nof a global knowledge distillation objective.\nFedX used global knowledge distillation to convey global model knowledge\nand regularize the local bias caused by the inconsistency between local and\noverall data distribution. Several studies in supervised settings have addressed\na similar challenge using extra regularization or gradient update processes. We\nreplaced the global knowledge distillation loss (Lglobal-KD – Eq. 12) with extant\nstrategies, such as FedProx [21] or SCAFFOLD [13] and verified its efficacy. The\nperformance comparison of different ablations across three benchmark datasets is\nsummarized in Table 2. The findings imply that our global knowledge distillation\ntechnique is more effective than alternative designs.\nRobustness test.\nThe model’s robustness is then tested by varying key hy-\nperparameters in different simulation settings. This allows us to test the system\nin difficult scenarios, such as (a) when each client is only allowed to hold a small\namount of data (i.e., data size |D|), (b) when more clients participate in the\nfederated system (i.e., client count N), and (c) when communication with the\ncentral server becomes limited and costly (i.e., the number of communication\n12\nS. Han et al.\nTable 3: Analysis of accuracy on CIFAR-10 over varying hyper-parameters indi-\ncates FedX consistently enhances the baseline performance.\n(a) Effect of the data size |D|\nData size\nBaseline\nBaseline+FedX\nLast\nBest\nLast\nBest\n10%\n46.80 47.37\n51.03\n53.96\n25%\n48.42 49.79\n52.84\n54.45\n50%\n51.17 52.04\n54.62\n55.85\n100%\n51.31 52.88\n56.88\n57.95\n(b) Effect of the client count N\nClients #\nBaseline\nBaseline+FedX\nLast\nBest\nLast\nBest\n5\n52.87 53.87\n58.55\n58.55\n10\n51.31 52.88\n56.88\n57.95\n15\n52.31 53.06\n55.12\n56.82\n20\n50.70 52.89\n56.56\n56.56\n(c) Effect of the communication round count R\nCommunication round\nBaseline\nBaseline+FedX\nLast\nBest\nLast\nBest\n20\n52.01\n52.80\n56.97\n56.97\n50\n51.95\n53.53\n57.29\n57.29\n100\n51.31\n52.88\n56.88\n57.95\n200\n52.79\n53.23\n57.35\n57.58\nrounds R) [36]. We test how our model performs under these scenarios in Ta-\nble 3. We note that when varying the communication rounds R, we also changed\nthe number of local epochs E accordingly such that R × E = 1000.\nThe table summarizes the effect of each hyperparameter for the baseline\nmodel (FedSimCLR) and the FedX-enhanced model. We make several observa-\ntions. First, reducing the data size |D| degrades performance. The drop, however,\nis not severe and remains nearly 5pp drop even when clients only hold 10% of\nthe data. Second, increasing the number of clients N will add complexity and\ndegrade performance. However, when N increases from 10 to 20, the drop is only\nmarginal near 1pp. Third, while increasing communication rounds generally pro-\nvides additional benefits, the gain appears to be marginal after some rounds, as\nshown in the example. Regardless of these changes, FedX consistently leads to\nnontrivial improvements over baseline.\n4.3\nAnalysis of the Embedding Space\nWe next quantitatively examine the embedding space characteristics to see how\nwell FedX distills global knowledge into the local model and encodes data se-\nmantic structure. We calculated the angle difference between the normalized\nembeddings passed by local model f and global model F as a quality metric:\n  \\text { Angle} (\\mathbf { x}) = \\arccos {(\\text {sim}(f(\\mathbf {x}), F(\\mathbf {x})))}, \\label {eq:angle}\n(14)\nFedX: Unsupervised Federated Learning with Cross Knowledge Distillation\n13\n(a) Box plot on local vs. global models\n(b) Histogram on inter-class difference\nFig. 6: Embedding analysis of baseline and FedX-enhanced models on CIFAR-10\ncomparing the angle difference of the embedded features.\nwhere x is an instance from the test data Dtest and sim(·) is the cosine similar-\nity function. It should be noted that a larger angle represents more significant\ndeviance in the embedding distributions of the two models.\nFigure 6a visualizes, for each of the ten classes in CIFAR-10, the angle dif-\nference between the embedding of each item between the local model and the\nglobal model computed by Eq. 14. Compared to the baseline (FedSimCLR),\nFedX-enhanced model reports a remarkably lower angle difference between the\nlocal and global models. This indicates that the local model can learn the refined\nknowledge of the global model through knowledge distillation.\nWhen it comes to the embedding space of different class items, it is best\nto have a large gap. Given Dc\ntest as a set of instances from class c, we can\ncompute a representative class prototype by averaging embeddings from Dc\ntest\n(Eq. 15). Then, the inter-class angle difference can be defined between any pair of\nclass prototypes (Eq. 16). Figure 6b plots the histogram of the inter-class angle\ndifference of every class pair, showing that FedX-enhanced models have larger\nangles of 93.15◦on average than the baseline model of 82.36◦. This demonstrates\nthat our model can better discriminate between different class items.\n  \\\nm\nath\nbf {z\n}\n_c &\n= {1\n \\ov\ner |\n\\mathcal {D} ^ c_{\\text {test} }|} \\sum _{\\mathbf {x} \\in \\mathcal {D}^c_{\\text {test}}}{f(\\mathbf {x})} \\label {eq:prototype} \\\\ \\text {Angle}(c_i, c_j) &= \\arccos (\\text {sim}(\\mathbf {z}_{c_i}, \\mathbf {z}_{c_j})) \\label {eq:interclassangle}\n(16)\n4.4\nExtension to Semi-Supervised Settings\nFinally, as a practical extension, consider a scenario in which each client has a\nsmall set of partially labeled data. This may be a more natural setting in many\nreal-world federated systems [44]. To convert our model to a semi-supervised\nsetting, we first trained it without supervision and then fine-tuned it with an\nadditional classifier on labeled data for an additional 100 epochs. For fine-tuning,\nan SGD optimizer with a learning rate of 1e-3 was used.\n14\nS. Han et al.\nTable 4: Classification accuracy in a semi-supervised setting on CIFAR-10. FedX\nenhances the baseline performance even with a small set of labels.\nLabel Ratio FedSimCLR\nFedMoCo\nFedBYOL\nFedProtoCL\nFedU\nVanilla FedX Vanilla FedX Vanilla FedX Vanilla FedX Vanilla FedX\n1%\n21.37 23.33 23.02 25.18 18.10 21.86 18.44 18.17 21.41 21.23\n5%\n30.68 35.86 34.24 37.63 29.77 34.48 19.64 26.66 32.19 35.41\n10%\n31.14 39.40 38.15 39.32 32.23 37.89 22.90 27.54 34.51 37.51\nTable 4 shows the performance results on CIFAR-10 in the semi-supervised\nsetting with varying label ratios of 1%, 5%, and 10%. As expected, increas-\ning the labeling ratio from 1% to 5% brings an immediate performance gain.\nFedX-enhanced models outperform most cases in the semi-supervised setting for\nmultiple baselines. Only minor exceptions can be seen with a 1% labeling rate,\nwhere our model performs similarly to the baseline. Our model, on the other\nhand, benefits more quickly from increasing the label ratio and can learn the\ndata representation from distributed local clients.\n5\nConclusion\nThis work presented the first-of-its-kind unsupervised federated learning ap-\nproach called FedX. We elaborate the local update process of the common feder-\nated learning framework and the model does not share any data directly across\nlocal clients. Its unique two-sided knowledge distillation can efficiently handle\ndata bias in a non-IID setting while maintaining privacy. It is straightforward\nand does not require any complex communication strategy.\nThe substantial performance gain of FedX shows great potential for many fu-\nture applications. For example, distributed systems with strict data privacy and\nsecurity requirements, such as learning patterns of new diseases across hospital\ndata or learning tending content in a distributed IoT network, can benefit from\nour model. Unsupervised learning is facilitated even when local clients lack data\nlabels and contain heterogeneous data. This versatile and robust trait makes\nunsupervised learning the new frontier in federated systems. We hope that our\ntechnique and implementation details will be useful in tackling difficult problems\nwith decentralized data.\nAcknowledgements\nWe thank Seungeon Lee and Xiting Wang for their insights and discussions\non our work. This research was supported by the Institute for Basic Science\n(IBS-R029-C2, IBS-R029-Y4), Microsoft Research Asia, and Potential Individu-\nals Global Training Program (2021-0-01696) by the Ministry of Science and ICT\nin Korea.\nFedX: Unsupervised Federated Learning with Cross Knowledge Distillation\n15\nReferences\n1. Bhat, P., Arani, E., Zonooz, B.: Distill on the go: Online knowledge distillation in\nself-supervised learning. In: Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition. pp. 2678–2687 (2021)\n2. Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V.,\nKiddon, C., Koneˇcn`y, J., Mazzocchi, S., McMahan, B., et al.: Towards federated\nlearning at scale: System design. Proceedings of Machine Learning and Systems 1,\n374–388 (2019)\n3. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-\ntrastive learning of visual representations. In: Proceedings of the International\nConference on Machine Learning. pp. 1597–1607. PMLR (2020)\n4. Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n15750–15758 (2021)\n5. Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by pre-\ndicting image rotations. In: International Conference on Learning Representations\n(2018)\n6. Grill, J.B., Strub, F., Altch´e, F., Tallec, C., Richemond, P., Buchatskaya, E., Doer-\nsch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.: Bootstrap your own\nlatent-a new approach to self-supervised learning. Advances in Neural Information\nProcessing Systems 33, 21271–21284 (2020)\n7. Han, S., Park, S., Park, S., Kim, S., Cha, M.: Mitigating embedding and class\nassignment mismatch in unsupervised image classification. In: Proceedings of the\nEuropean Conference on Computer Vision. pp. 768–784. Springer (2020)\n8. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised\nvisual representation learning. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 9729–9738 (2020)\n9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 770–778 (2016)\n10. Hinton, G., Vinyals, O., Dean, J., et al.: Distilling the knowledge in a neural net-\nwork. arXiv preprint arXiv:1503.02531 (2015)\n11. Hsu, T.M.H., Qi, H., Brown, M.: Measuring the effects of non-identical data distri-\nbution for federated visual classification. arXiv preprint arXiv:1909.06335 (2019)\n12. Ji, M., Shin, S., Hwang, S., Park, G., Moon, I.C.: Refine myself by teaching\nmyself: Feature refinement via self-knowledge distillation. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10664–\n10673 (2021)\n13. Karimireddy, S.P., Kale, S., Mohri, M., Reddi, S., Stich, S., Suresh, A.T.: Scaf-\nfold: Stochastic controlled averaging for federated learning. In: Proceedings of the\nInternational Conference on Machine Learning. pp. 5132–5143. PMLR (2020)\n14. Kim, K., Ji, B., Yoon, D., Hwang, S.: Self-knowledge distillation with progressive\nrefinement of targets. In: Proceedings of the IEEE/CVF International Conference\non Computer Vision. pp. 6567–6576 (2021)\n15. Komodakis, N., Zagoruyko, S.: Paying more attention to attention: improving the\nperformance of convolutional neural networks via attention transfer. In: Proceed-\nings of the International Conference on Learning Representations (2017)\n16. Koratana, A., Kang, D., Bailis, P., Zaharia, M.: Lit: Learned intermediate rep-\nresentation training for model compression. In: Proceedings of the International\nConference on Machine Learning. pp. 3509–3518. PMLR (2019)\n16\nS. Han et al.\n17. Krizhevsky, A.: Learning multiple layers of features from tiny images. Tech. rep.,\nCiteseer (2009)\n18. Li, J., Zhou, P., Xiong, C., Hoi, S.: Prototypical contrastive learning of unsuper-\nvised representations. In: Proceedings of the International Conference on Learning\nRepresentations (2020)\n19. Li, Q., He, B., Song, D.: Model-contrastive federated learning. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n10713–10722 (2021)\n20. Li, T., Sahu, A.K., Talwalkar, A., Smith, V.: Federated learning: Challenges, meth-\nods, and future directions. IEEE Signal Processing Magazine 37(3), 50–60 (2020)\n21. Li, T., Sahu, A.K., Zaheer, M., Sanjabi, M., Talwalkar, A., Smith, V.: Federated\noptimization in heterogeneous networks. Proceedings of Machine Learning and\nSystems 2, 429–450 (2020)\n22. Liu, Y., Huang, A., Luo, Y., Huang, H., Liu, Y., Chen, Y., Feng, L., Chen, T.,\nYu, H., Yang, Q.: Fedvision: An online visual object detection platform powered\nby federated learning. In: Proceedings of the Association for the Advancement of\nArtificial Intelligence. vol. 34, pp. 13172–13179 (2020)\n23. McMahan, B., Moore, E., Ramage, D., Hampson, S., Aguera y Arcas, B.:\nCommunication-efficient learning of deep networks from decentralized data. In:\nProceedings of the Artificial Intelligence and Statistics. pp. 1273–1282. PMLR\n(2017)\n24. Mitrovic, J., McWilliams, B., Walker, J.C., Buesing, L.H., Blundell, C.: Represen-\ntation learning via invariant causal mechanisms. In: International Conference on\nLearning Representations (2020)\n25. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A.Y.: Reading digits\nin natural images with unsupervised feature learning (2011)\n26. Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748 (2018)\n27. Park, S., Han, S., Kim, S., Kim, D., Park, S., Hong, S., Cha, M.: Improving unsu-\npervised image clustering with robust learning. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 12278–12287 (2021)\n28. Park, S., Kim, S., Cha, M.: Knowledge sharing via domain adaptation in customs\nfraud detection. arXiv preprint arXiv:2201.06759 (2022)\n29. Park, W., Kim, D., Lu, Y., Cho, M.: Relational knowledge distillation. In: Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\npp. 3967–3976 (2019)\n30. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep\nconvolutional generative adversarial networks. In: Proceedings of the International\nConference on Learning Representations (2016)\n31. Sheller, M.J., Reina, G.A., Edwards, B., Martin, J., Bakas, S.: Multi-institutional\ndeep learning modeling without sharing patient data: A feasibility study on brain\ntumor segmentation. In: Proceedings of the International MICCAI Brainlesion\nWorkshop. pp. 92–104. Springer (2018)\n32. Tejankar, A., Koohpayegani, S.A., Pillai, V., Favaro, P., Pirsiavash, H.: Isd: Self-\nsupervised learning by iterative similarity distillation. In: Proceedings of the\nIEEE/CVF International Conference on Computer Vision. pp. 9609–9618 (2021)\n33. Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A.: Extracting and composing\nrobust features with denoising autoencoders. In: Proceedings of the 25th interna-\ntional conference on Machine learning. pp. 1096–1103 (2008)\n34. Voigt, P., Von dem Bussche, A.: The EU General Data Protection Regulation\n(GDPR): A Practical Guide. Springer (2017)\nFedX: Unsupervised Federated Learning with Cross Knowledge Distillation\n17\n35. Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., Khazaeni, Y.: Federated\nlearning with matched averaging. In: Proceedings of the International Conference\non Learning Representations (2020)\n36. Wang, J., Charles, Z., Xu, Z., Joshi, G., McMahan, H.B., Al-Shedivat, M., An-\ndrew, G., Avestimehr, S., Daly, K., Data, D., et al.: A field guide to federated\noptimization. arXiv preprint arXiv:2107.06917 (2021)\n37. Wang, J., Liu, Q., Liang, H., Joshi, G., Poor, H.V.: Tackling the objective in-\nconsistency problem in heterogeneous federated optimization. Advances in neural\ninformation processing systems 33, 7611–7623 (2020)\n38. Wu, Y., Wang, Z., Zeng, D., Li, M., Shi, Y., Hu, J.: Federated contrastive\nrepresentation learning with feature fusion and neighborhood matching (2021),\nhttps://openreview.net/forum?id=6LNPEcJAGWe\n39. Xiao, H., Rasul, K., Vollgraf, R.: Fashion-mnist: a novel image dataset for bench-\nmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 (2017)\n40. Xu, Y.Z., Han, S., Park, S., Cha, M., Li, C.T.: A comprehensive and adversarial\napproach to self-supervised representation learning. In: 2020 IEEE International\nConference on Big Data (Big Data). pp. 709–717. IEEE (2020)\n41. Yang, C., An, Z., Cai, L., Xu, Y.: Mutual contrastive learning for visual represen-\ntation learning. In: Proceedings of the AAAI Conference on Artificial Intelligence.\nvol. 36, pp. 3045–3053 (2022)\n42. Zhang, F., Kuang, K., You, Z., Shen, T., Xiao, J., Zhang, Y., Wu, C.,\nZhuang, Y., Li, X.: Federated unsupervised representation learning. arXiv preprint\narXiv:2010.08982 (2020)\n43. Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V.: Federated learning\nwith non-iid data. arXiv preprint arXiv:1806.00582 (2018)\n44. Zhuang, W., Gan, X., Wen, Y., Zhang, S., Yi, S.: Collaborative unsupervised visual\nrepresentation learning from decentralized data. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. pp. 4912–4921 (2021)\n45. Zhuang, W., Wen, Y., Zhang, X., Gan, X., Yin, D., Zhou, D., Zhang, S., Yi,\nS.: Performance optimization of federated person re-identification via benchmark\nanalysis. In: Proceedings of the 28th ACM International Conference on Multimedia.\npp. 955–963 (2020)\n18\nS. Han et al.\n6\nAppendix\n6.1\nCode Release & Implementation details\nFedX is open-sourced at https://github.com/Sungwon-Han/FEDX. Algorithm 1\nshows the overall training process.\nAlgorithm 1 Local update process of FedX\nInput: the number of local epochs E, global model F, local model f m, projection\nhead hm, local dataset Dm, and temperature τ\nOutput: a trained local model f m and a projection head hm\nIn each client m,\nf m ←F\n// Replace the local model with the global model\nF ←F.detach()\n// Fix the global model for a given communication round\nfor epoch e ∈{1, 2, ..., E} do\nfor batch B, Br ∈Di do\n˜B ←Augment(B)\nfor xi ∈B, ˜xi ∈˜B, and xj ∈Br do\n/*\nLocal KD\n*/\nzi, zj,˜zi ←f m(xi), f m(xj), f m(˜xi)\n// Local relationship vectors\nrj\ni =\nexp(sim(zi,zj)/τ)\nP\nk∈Br exp(sim(zi,zk)/τ)\n˜rj\ni =\nexp(sim(˜zi,zj)/τ)\nP\nk∈Br exp(sim(˜zi,zk))/τ)\nLlocal\nc\n←Calculate from Eq. 3 or 5\nLlocal\nr\n←Calculate from Eq. 7\nLlocal-KD = Llocal\nc\n+ Llocal\nr\n/*\nGlobal KD\n*/\nzl\ni,˜zl\ni ←hm ◦f m(xi), hm ◦f m(˜xi)\nzg\ni , zg\nj,˜zg\ni ←F(xi), F(xj), F(˜xi)\n// Global relationship vectors\nr′j\ni =\nexp(sim(zl\ni,zg\nj )/τ)\nP\nk∈Br exp(sim(zl\ni,zg\nk)/τ)\n˜r′j\ni =\nexp(sim(˜zl\ni,zg\nj )/τ)\nP\nk∈Br exp(sim(˜zl\ni,zg\nk)/τ)\nLglobal\nc\n←Calculate from Eq. 9\nLglobal\nr\n←Calculate from Eq. 11\nLglobal-KD = Lglobal\nc\n+ Lglobal\nr\nend\nLtotal-KD = Llocal-KD + Lglobal-KD\n// Calculate the total loss\nUpdate f m, hm via back-propagation\n// Update local model parameters\nend\nend\nreturn f m, hm\n6.2\nFurther Results on Performance Evaluation\nStatistical errors in Table 1. Table 5 reports standard errors from various\ntraining iterations (i.e., last five epochs & best five epochs), which shows that\nour model’s accuracy converges stably.\nFedX: Unsupervised Federated Learning with Cross Knowledge Distillation\n19\nTable 5: Adding FedX increases the performance of various federate learning\nmodels. The final round accuracy and the best accuracy are reported with stan-\ndard errors.\nMethod\nCIFAR-10\nSVHN\nF-MNIST\nLast\nBest\nLast\nBest\nLast\nBest\nFedSimCLR 51.31±0.25\n52.88±0.06\n75.19±3.14\n76.50±0.04\n77.66±0.75\n79.44±0.06\n+ FedX\n56.88±0.72 57.95±0.03 77.19±1.67 77.70±0.09 81.98±0.74 82.47±0.02\nFedMoCo\n56.74±1.63\n57.82±0.02\n70.69±3.03\n70.99±0.07\n82.31±0.18\n83.58±0.06\n+ FedX\n58.23±1.22 59.43±0.09 73.57±2.79 73.92±0.02 83.62±0.32 84.65±0.04\nFedBYOL\n52.24±0.61\n53.14±0.06\n65.95±1.62\n67.32±0.24\n81.45±0.27\n82.37±0.06\n+ FedX\n56.49±0.72 57.79±0.12 68.94±1.13 69.05±0.07 83.18±0.31 84.30±0.07\nFedProtoCL 51.33±1.03\n52.12±0.03\n49.85±0.77\n50.19±0.11\n81.76±0.22 83.57±0.04\n+ FedX\n55.36±0.98 56.76±0.01 69.31±1.72 69.75±0.15 82.74±0.35 83.34±0.04\nFedU\n50.79±0.47\n50.79±0.05\n66.02±1.83\n66.22±0.17\n80.59±0.42\n82.03±0.05\n+ FedX\n56.15±0.58 57.26±0.05 68.13±1.17 68.39±0.07 83.73±0.20 84.12±0.03\nTable 6: Performance improvement with three different algorithms on classifica-\ntion accuracy. Both the final round accuracy and the best accuracy show that\nFedX brings nontrivial improvement over the baseline algorithm.\nMethod\nCIFAR-10\nSVHN\nF-MNIST\nLast\nBest\nLast\nBest\nLast\nBest\nFedSimCLR\n51.31\n52.88\n75.19\n76.50\n77.66\n79.44\n+ FedCA\n47.46\n48.54\n59.40\n59.86\n81.51\n82.05\n+ MOON-unsup\n51.78\n52.84\n75.36\n76.03\n80.58\n80.93\n+ FedX (ours)\n56.88\n57.95\n77.19\n77.70\n81.98\n82.47\nTable 7: Performance improvement on ImageNet-10. Both last and best round\naccuracy are reported.\nMethod\nFedSimCLR\nFedSimCLR+FedX\nLast\nBest\nLast\nBest\nImageNet-10\n81.50\n81.50\n86.17\n86.57\nComparison with other relevant baselines. We compared FedX with other\ncontrastive learning methods: FedCA (Zhang et al.)6 and MOON (Li et al.). We\nfollowed the same implementation guidelines as in the original work, only sub-\nstituting the InfoNCE objective for the supervised loss in MOON (calling this\nMOON-unsup). FedSimCLR with FedAvg served as the base framework in both\ncases. Our method continues to outperform, as shown in Table 6, demonstrat-\n6 Note that the dictionary module has been removed from FedCA for fair comparison\nas it directly shares the local data information of all clients.\n20\nS. Han et al.\ning the benefit of the proposed knowledge distillation strategy in unsupervised\nfederated learning.\nPerformance on ImageNet. To verify our model’s applicability to the large-\nscale dataset, we run FedX on ImageNet-10 benchmark, a 10-class subset of\nImageNet. FedX was trained for 10 local epochs in each communication round,\nwith a total of 50 rounds. Table 7 shows that adding FedX to FedSimCLR im-\nproves the classification accuracy by 5pp. Extended results for the larger number\nof the class will also be released.\nFull comparison results over communication rounds. FedX brings mean-\ningful performance improvements than when using the baseline algorithms alone.\nFigure 7 shows trajectories including three additional baselines – FedProtoCL,\nFedMoCo, and FedU that were omitted in Fig. 4. We can check how quickly the\nmodel benefits baselines over the varying communication rounds. Especially, in\nFedSimCLR and FedProtoCL (F-MNIST) experiments, FedX prevents the local\nbias degrading the performance during the entire training phase and thereby\nstops such deterioration in performance.\nFedX: Unsupervised Federated Learning with Cross Knowledge Distillation\n21\n(a) Performance gain on FedSimCLR\n(b) Performance gain on FedBYOL\n(c) Performance gain on FedProtoCL\n(d) Performance gain on FedMoCo\n(e) Performance gain on FedU\nFig. 7: Performance comparison between all baselines (i.e., FedSimCLR, Fed-\nBYOL, FedProtoCL, FedMoCo, and FedU) and FedX-enhanced versions over\ncommunication rounds. FedX brings extra performance on baseline unsupervised\nlearning models in three benchmark datasets.\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2022-07-19",
  "updated": "2022-07-19"
}