{
  "id": "http://arxiv.org/abs/2207.09228v1",
  "title": "Image Super-Resolution with Deep Dictionary",
  "authors": [
    "Shunta Maeda"
  ],
  "abstract": "Since the first success of Dong et al., the deep-learning-based approach has\nbecome dominant in the field of single-image super-resolution. This replaces\nall the handcrafted image processing steps of traditional sparse-coding-based\nmethods with a deep neural network. In contrast to sparse-coding-based methods,\nwhich explicitly create high/low-resolution dictionaries, the dictionaries in\ndeep-learning-based methods are implicitly acquired as a nonlinear combination\nof multiple convolutions. One disadvantage of deep-learning-based methods is\nthat their performance is degraded for images created differently from the\ntraining dataset (out-of-domain images). We propose an end-to-end\nsuper-resolution network with a deep dictionary (SRDD), where a high-resolution\ndictionary is explicitly learned without sacrificing the advantages of deep\nlearning. Extensive experiments show that explicit learning of high-resolution\ndictionary makes the network more robust for out-of-domain test images while\nmaintaining the performance of the in-domain test images.",
  "text": "Image Super-Resolution with Deep Dictionary\nShunta Maeda\nNavier Inc.\nshunta@navier.co.jp\nAbstract. Since the first success of Dong et al., the deep-learning-\nbased approach has become dominant in the field of single-image super-\nresolution. This replaces all the handcrafted image processing steps of\ntraditional sparse-coding-based methods with a deep neural network.\nIn contrast to sparse-coding-based methods, which explicitly create\nhigh/low-resolution dictionaries, the dictionaries in deep-learning-based\nmethods are implicitly acquired as a nonlinear combination of multi-\nple convolutions. One disadvantage of deep-learning-based methods is\nthat their performance is degraded for images created differently from\nthe training dataset (out-of-domain images). We propose an end-to-end\nsuper-resolution network with a deep dictionary (SRDD), where a high-\nresolution dictionary is explicitly learned without sacrificing the advan-\ntages of deep learning. Extensive experiments show that explicit learning\nof high-resolution dictionary makes the network more robust for out-of-\ndomain test images while maintaining the performance of the in-domain\ntest images. Code is available at https://github.com/shuntama/srdd.\nKeywords: Super-Resolution, Deep Dictionary, Sparse Representation\n1\nIntroduction\nSingle-image super-resolution (SISR) is a classical problem in the field of com-\nputer vision that predicts a high-resolution (HR) image from its low-resolution\n(LR) observation. Because this is an ill-posed problem with multiple possible so-\nlutions, obtaining a rich prior based on a large number of data points is beneficial\nfor better prediction. Deep learning is quite effective for such problems. The per-\nformance of SISR has been significantly improved by using convolutional neural\nnetworks (CNN), starting with the pioneering work of Dong et al. in 2014 [9]. Be-\nfore the dominance of deep-learning-based methods [1,9,10,19,20,23,26,27,41,42,56,57]\nin this field, example-based methods [4,12,13,16,21,44,45,51,52] were mainly used\nfor learning priors. Among them, sparse coding, which is a representative example-\nbased method, has shown state-of-the-art performance [44,45,51]. SISR using\nsparse coding comprises the following steps, as illustrated in Fig. 1(a): 1L\n○learn\nan LR dictionary DL with patches extracted from LR images, 1H\n○learn an HR\ndictionary DH with patches extracted from HR images,\n2○represent patches\ndensely cropped from an input image with DL,\n3○map DL representations to\nDH representations,\n4○reconstruct HR patches using DH, then aggregate the\noverlapped HR patches to produce a final output.\narXiv:2207.09228v1  [cs.CV]  19 Jul 2022\n2\nS. Maeda\nFig. 1. Schematic illustrations of single image super-resolution with (a) sparse-coding-\nbased approach, (b) conventional deep-learning-based approach, and (c) our approach.\nThe numbers 1○– 4○indicate each step of the super-resolution process.\nAs depicted in Fig. 1(b), Dong et al. [9] replaced all the above handcrafted\nsteps with a multilayered CNN in their proposed method SRCNN to take ad-\nvantage of the powerful capability of deep learning. Note that, in this method,\nDL and DH are implicitly acquired through network training. Since the SRCNN,\nvarious methods have been proposed to improve performance, for example, by\ndeepening the network with residual blocks and skip connections [20,27,41,57],\napplying attention mechanisms [8,31,34,56], and using a transformer [6,26]. How-\never, most of these studies, including state-of-the-art ones, follow the same for-\nmality as SRCNN from a general perspective, where all the processes in the\nsparse-coding-based methods are replaced by a multilayered network.\nOne disadvantage of deep-learning-based methods is that their performance is\ndegraded for images created differently from the training dataset [14]. Although\nthere have been several approaches to address this issue, such as training net-\nworks for multiple degradations [40,46,49,55,59] and making models agnostic to\ndegradations with iterative optimizations [14,38], it is also important to make\nthe network structure more robust. We hypothesize that DH implicitly learned\ninside a multilayered network is fragile to subtle differences in input images from\nthe training time. This hypothesis leads us to the method we propose.\nIn this study, we propose an end-to-end super-resolution network with a deep\ndictionary (SRDD), where DH is explicitly learned through the network training\n(Fig. 1(c)). The main network predicts the coefficients of DH and the weighted\nsum of the elements (or atoms) of DH produces an HR output. This approach\nis fundamentally different from the conventional deep-learning-based approach,\nwhere the network has upsampling layers inside it. The upsampling process of\nthe proposed method is efficient because the pre-generated DH can be used as a\nmagnifier for inference. In addition, the main network does not need to maintain\nthe information of the processed image at the pixel level in HR space. Therefore,\nthe network can concentrate only on predicting the coefficients of DH. For in-\ndomain test images, our method shows performance that is not as good as latest\nones, but close to the conventional baselines (eg., CARN). For out-of-domain\ntest images, our method shows superior performance compared to conventional\ndeep-learning-based methods.\nImage Super-Resolution with Deep Dictionary\n3\n2\nRelated Works\n2.1\nSparse-coding-based SR\nBefore the dominance of deep-learning-based methods in the field of SISR,\nexample-based methods showed state-of-the-art performance. The example-based\nmethods exploit internal self-similarity [11,13,16,50] and/or external datasets [4,12,21,44,45,51,52].\nThe use of external datasets is especially important for obtaining rich prior. In\nthe sparse-coding-based methods [44,45,51,52], which are state-of-the-art example-\nbased methods, high/low-resolution patch pairs are extracted from external\ndatasets to create high/low-resolution dictionaries DH/DL. The patches cropped\nfrom an input image are encoded with DL and then projected onto DH via itera-\ntive processing, producing the final output with appropriate patch aggregation.\n2.2\nDeep-learning-based SR\nDeep CNN\nAll the handcrafted steps in the traditional sparse-coding-based\napproach were replaced with an end-to-end CNN in a fully feed-forward manner.\nEarly methods, including SRCNN [9,19,20], adopted pre-upsampling in which LR\ninput images are first upsampled for the SR process. Because the pre-upsampling\nis computationally expensive, post-upsampling is generally used in recent mod-\nels [1,26,27,31,56]. In post-upsampling, a transposed convolution or pixelshuf-\nfle [37] is usually used to upsample the features for final output. Although there\nare many proposals to improve network architectures [25,54], the protocol that\ndirectly outputs SR images with post-upsampling has been followed in most of\nthose studies. Few studies have focused on the improvement of the upsampling\nstrategy. Tough some recent works [3,5,60] leveraged the pre-trained latent fea-\ntures as a dictionary to improve output fidelity with rich textures, they used\nstandard upsampling strategies in their proposed networks.\nConvolutional sparse coding\nAlthough methods following SRCNN have\nbeen common in recent years, several fundamentally different approaches have\nbeen proposed before and after the proposal of SRCNN. Convolutional sparse\ncoding [15,35,39,47] is one of such methods that work on the entire image differ-\nently from traditional patch-based sparse coding. The advantage of convolutional\nsparse coding is that it avoids the boundary effect in patch-based sparse coding.\nHowever, it conceptually follows patch-based sparse coding in that the overall\nSR process is divided into handcrafted steps. Consequently, its performance lags\nbehind that of end-to-end feed-forward CNN.\nRobust SR\nThe performance of deep-learning-based SR is significantly af-\nfected by the quality of the input image, especially the difference in conditions\nfrom the training dataset [14]. Several approaches have been proposed to make\nthe network more robust against in-domain test images by training with multi-\nple degradations [40,46,49,55,59]. For robustness against out-of-domain test im-\nages, some studies aim to make the network agnostic to degradations [14,38]. In\nthese methods, agnostics acquisition is generally limited to specific degradations;\ntherefore, it is important to make the network structure itself more robust.\n4\nS. Maeda\nFig. 2. The overall pipeline of the proposed method. A high-resolution dictionary DH\nis generated from random noise. An encoded code of DH is then concatenated with an\nextracted feature to be inputted to a per-pixel predictor. The predictor output is used\nto reconstruct the final output based on DH.\n3\nMethod\nAs depicted in Fig. 1(c), the proposed method comprises three components: DH\ngeneration, per-pixel prediction, and reconstruction. The DH generator gener-\nates an HR dictionary DH from random noise input. The per-pixel predictor\npredicts the coefficients of DH for each pixel from an LR YCbCr input. In the\nreconstruction part, the weighted sum of the elements (or atoms) of DH produces\nan HR Y-channel output as a residual to be added to a bicubically upsampled Y\nchannel. The remaining CbCr channels are upscaled with a shallow SR network.\nWe used ESPCN [37] as the shallow SR network in this work. All of these com-\nponents can be simultaneously optimized in an end-to-end manner; therefore,\nthe same training procedure can be used as in conventional deep-learning-based\nSR methods. We use L1 loss function to optimize the network\n  L\n \n=\n \n\\fr\nac {1\n} { M} \\s\nu m _{i=1}^{M} ||I_i^{gt} - \\Theta (I_i^{lr})||_{1},\n(1)\nwhere Ilr\ni\nand Igt\ni\nare LR patch and its ground truth. M denotes the number of\ntraining image pairs. Θ(·) represents a function of the SRDD network. Figure 2\nillustrates the proposed method in more detail. We describe the design of each\ncomponent based on Fig. 2 in the following subsections.\nImage Super-Resolution with Deep Dictionary\n5\nFig. 3. A generator architecture of a high-resolution dictionary DH. A tree-like network\nwith depth d generates 2d atoms of size 1 × s × s from a random noise input, where s\nis an upscaling factor.\nFig. 4. Learned atoms of ×4 SRDD with N = 128. The size of each atom is 1 × 4 × 4.\nThe data range is renormalized to [0, 1] for visualization.\n3.1\nDH Generation\nFrom random noise δs2×1×1 (∈Rs2×1×1) with a standard normal distribution,\nthe DH generator generates the HR dictionary DN×s×s\nH\n, where s is an upscaling\nfactor and N is the number of elements (atoms) in the dictionary. DH is then\nencoded by s × s convolution with groups N, followed by ReLU [33] and 1 × 1\nconvolution. Each N element of the resultant code CN×1×1\nH\nrepresents each s×s\natom as a scalar value. Although the DH can be trained using a fixed noise input,\nwe found that introducing input randomness improves the stability of the train-\ning. A pre-generated fixed dictionary and its code are used in the testing phase.\nNote that only DH is generated since low-resolution dictionaries (encoding) can\nbe naturally replaced by convolutional operations without excessive increases in\ncomputation.\nAs illustrated in Fig. 3, the DH generator has a tree-like structure, where\nthe nodes consist of two 1 × 1 convolutional layers with ReLU activation. The\nfinal layer has a Tanh activation followed by a pixelshuffling layer; therefore, the\ndata range of the output atoms is [−1, 1]. To produce N atoms, depth d of the\ngenerator is determined as\n  d = \\log _2 N.\n(2)\nFigure 4 shows generated atoms with s = 4 and N = 128. We observed that the\ncontrast of the output atoms became stronger as training progressed, and they\nwere almost fixed in the latter half of the training.\n6\nS. Maeda\nFig. 5. Visualization of sparsity of a prediction map (center) and its complementary\nprediction map (right). The number of predicted coefficients larger than 1e −2 is\ncounted for each pixel. More atoms are assigned to the high-frequency parts and the\nlow-frequency parts are relatively sparse.\n3.2\nPer-pixel Prediction\nWe utilize UNet++ [61] as a deep feature extractor in Fig. 2. We slightly modify\nthe original UNet++ architecture: the depth is reduced from four to three, and\na long skip connection is added. The deep feature extractor outputs a tensor\nof size f × h × w from the input YCbCr image, where h and w are the height\nand width of the image, respectively. Then the extracted feature is concatenated\nwith the expanded code of DH\n  C_{\\\nt\ne xt {H}}^{N \\t\ni\nmes h \\times w} = R_{1 \\times h \\times w}(C_{\\text {H}}^{N \\times 1 \\times 1}),\n(3)\nto be inputted to a per-pixel predictor, where Ra×b×c(·) denotes the a × b × c\nrepeat operations. The per-pixel predictor consists of ten bottleneck residual\nblocks followed by a softmax function that predicts N coefficients of DH for each\ninput pixel. Both the deep feature extractor and per-pixel predictor contain batch\nnormalization layers [18] before the ReLU activation. The resultant prediction\nmap M N×h×w is further convolved with a 2 × 2 convolution layer to produce a\ncomplementary prediction map M ′N×(h−1)×(w−1). A complementary prediction\nmap is used to compensate for the patch boundaries when reconstructing the\nfinal output. The detail of the compensation mechanism is described in the next\nsubsection. Although we tried to replace softmax with ReLU to directly express\nsparsity, ReLU made the training unstable. We also tried entmax [36], but the\nperformance was similar to that of softmax, so we decided to use softmax for\nsimplicity.\nFigure 5 visualizes the sparsity of the prediction map and its complementary\nprediction map. The number of coefficients larger than 1e−2 is counted for each\npixel to visualize the sparsity. The model with N = 128 is used. More atoms are\nassigned to the high-frequency parts of the image, and the low-frequency parts\nare relatively sparse. This feature is especially noticeable in the complementary\nprediction map. In the high-frequency region, the output image is represented\nby linear combinations of more than tens of atoms for both maps.\nImage Super-Resolution with Deep Dictionary\n7\nFig. 6. Schematic illustration of a mechanism to compensate patch boundary with\na complementary prediction map, where s is a scaling factor. Left: Prediction map\n(blue) and its complementary prediction map (orange). Right: Upsampled prediction\nand complementary prediction maps with centering.\n3.3\nReconstruction\nThe prediction map M N×h×w is upscaled to N × sh × sw by nearest-neighbor\ninterpolation, and the element-wise multiplication of that upscaled prediction\nmap Us(M N×h×w) with the expanded dictionary R1×h×w(DN×s×s\nH\n) produces\nN ×sh×sw tensor T consists of weighted atoms. The Us(·) denotes ×s nearest-\nneighbor upsampling. Finally, tensor T is summed over the first dimension, pro-\nducing output x as\nx1×sh×sw =\nN−1\nX\nk=0\nT N×sh×sw[k, :, :],\n(4)\nT N×sh×sw = Us(M N×h×w) ⊗R1×h×w(DN×s×s\nH\n).\n(5)\nThe same sequence of operations is applied to the complementary prediction\nmap to obtain the output x′ as follows:\nx′1×s(h−1)×s(w−1) =\nN−1\nX\nk=0\nT ′N×s(h−1)×s(w−1)[k, :, :],\n(6)\nT ′N×s(h−1)×s(w−1) = Us(M ′N×(h−1)×(w−1)) ⊗R1×(h−1)×(w−1)(DN×s×s\nH\n). (7)\nNote that the same dictionary, DH, is used to obtain x and x′. By centering x\nand x′, as illustrated in Fig. 6, the imperfections at the patch boundaries can\ncomplement each other. The final output residual is obtained by concatenating\nthe overlapping parts of the centered x and x′ and applying a 5 × 5 convolution.\nFor non-overlapping parts, x is simply used as the final output.\n4\nExperiments\n4.1\nImplementation Details\nWe adopt a model with 128 atoms (SRDD-128) and a small model with 64 atoms\n(SRDD-64). The number of filters of the models is adjusted according to the\n8\nS. Maeda\nnumber of atoms. Our network is trained by inputting 48 × 48 LR patches with\na mini-batch size of 32. Following previous studies [1,27,56], random flipping\nand rotation augmentation is applied to each training sample. We use Adam\noptimizer [22] with β1 = 0.9, β2 = 0.999, and ϵ = 10−8. The learning rate of the\nnetwork except for the DH generator is initialized as 2e −4 and halved at [200k,\n300k, 350k, 375k]. The total training iterations is 400k. The learning rate of the\nDH generator is initialized as 5e−3 and halved at [50k, 100k, 200k, 300k, 350k].\nParameters of the DH generator are frozen at 360k iteration. In addition, to\nstabilize training of the DH generator, we randomly shuffle the order of output\natoms for the first 1k iterations. We use PyTorch to implement our model with\nan NVIDIA P6000 GPU. Training takes about two/three days for SRDD-64/128,\nrespectively. More training details are provided in the supplementary material.\n4.2\nDataset and Evaluation\nTraining dataset Following previous studies, we use 800 HR-LR image pairs\nof the DIV2K [43] training dataset to train our models. LR images are created\nfrom HR images by Matlab bicubic downsampling. For validation, we use initial\nten images from the DIV2K validation dataset.\nTest dataset For testing, we evaluate the models on five standard benchmarks:\nSet5 [2], Set14 [53], BSD100 [29], Urban100 [16], and Manga109 [30]. In addition\nto standard test images downsampled with Matlab bicubic function same as in\ntraining, we use test images that downsampled by OpenCV bicubic, bilinear,\nand area functions to evaluate the robustness of the models. In addition, we\nevaluate the models on real-world ten historical photographs.\nEvaluation\nWe use common image quality metrics peak signal-to-noise ratio\n(PSNR) and structural similarity index (SSIM) [48] calculated on the Y channel\n(luminance channel) of YCbCr color space. For evaluation of real-world test\nimages, no-reference image quality metric NIQE [32] is used since there are no\nground-truth images. Following previous studies, we ignore s pixels from the\nborder to calculate all the metrics, where s is an SR scale factor.\n4.3\nAblation Study\nWe conduct ablation experiments to examine the impact of individual elements\nin SRDD. We report the results of SRDD-64 throughout this section. The results\nof the ablation experiments on Set14 downsampled with Matlab bicubic function\nare summarized in Tab. 1.\nBatch normalization\nWe show the validation curves of SRDD-64 with and\nwithout batch normalization layers in Fig. 7. The performance of the proposed\nmodel is substantially improved by using batch normalization. This result is in\ncontrast to conventional deep-learning-based SR methods, where batch normal-\nization generally degrades performance [27]. Unlike conventional methods where\nthe network directly outputs the SR image, the prediction network in SRDD\npredicts the coefficients of DH for each pixel, which is rather similar to the se-\nmantic segmentation task. In this sense, it is natural that batch normalization,\nImage Super-Resolution with Deep Dictionary\n9\nTable 1. Results of ablation experiments on Set14 downsampled with Matlab bicubic\nfunction.\nPSNR\nSSIM\nSRDD-64\n28.54\n0.7809\nSRDD-64 −batch norm.\n28.49\n0.7792\nSRDD-64 −bottleneck blocks\n28.48\n0.7790\nSRDD-64 −compensation\n28.51\n0.7801\nFig. 7. Validation curves during the training of SRDD-64 with and without batch\nnormalization layers.\nwhich is effective for semantic segmentation [7,24,58], is also effective for the\nproposed model.\nBottleneck blocks\nWe eliminate bottleneck blocks and DH code injection\nfrom the per-pixel predictor. The prediction network becomes close to the plane\nUNet++ structure with this modification. The performance drops as shown in\nTab. 1, but still demonstrates a certain level of performance.\nCompensation mechanism\nAs shown in Tab. 1, removing the compensa-\ntion mechanism from SRDD-64 degrades the performance. However, the effect is\nmarginal indicates that our model can produce adequate quality outputs with-\nout boundary compensation. This result is in contrast to the sparse-coding-based\nmethods, which generally require aggregation with overlapping patch sampling\nto reduce imperfection at the patch boundary. Because the computational com-\nplexity of our compensation mechanism is very small compared to that of the\nentire model, we adopt it even if the effect is not so large.\n4.4\nResults on In-domain Test Images\nWe conduct experiments on five benchmark datasets, where the LR input im-\nages are created by Matlab bicubic downsampling same as in the DIV2K training\ndataset. Because SRDD is quite shallow and fast compared to current state-of-\nthe-art models, we compare SRDD to relatively shallow and fast models with\nroughly 50 layers or less. Note that recent deep SR models usually have hundreds\nof convolutional layers [56]. We select ten models for comparison: SRCNN [9],\n10\nS. Maeda\nTable 2. Quantitative comparison for ×4 SR on benchmark datasets. Best and second\nbest results are highlighted in red and blue, respectively.\nMethod\nSet5\nSet14\nBSD100\nUrban100\nManga109\nPSNR / SSIM\nPSNR / SSIM\nPSNR / SSIM\nPSNR / SSIM\nPSNR / SSIM\nBicubic\n28.42 / 0.8104\n26.00 / 0.7027\n25.96 / 0.6675\n23.14 / 0.6577\n24.89 / 0.7866\nA+ [45]\n30.28 / 0.8603\n27.32 / 0.7491\n26.82 / 0.7087\n24.32 / 0.7183\n- / -\nSRCNN [9]\n30.48 / 0.8628\n27.50 / 0.7513\n26.90 / 0.7101\n24.52 / 0.7221\n27.58 / 0.8555\nFSRCNN [10]\n30.72 / 0.8660\n27.61 / 0.7550\n26.98 / 0.7150\n24.62 / 0.7280\n27.90 / 0.8610\nVDSR [19]\n31.35 / 0.8830\n28.02 / 0.7680\n27.29 / 0.0726\n25.18 / 0.7540\n28.83 / 0.8870\nDRCN [20]\n31.53 / 0.8854\n28.02 / 0.7670\n27.23 / 0.7233\n25.14 / 0.7510\n- / -\nLapSRN [23]\n31.54 / 0.8850\n28.19 / 0.7720\n27.32 / 0.7270\n25.21 / 0.7560\n29.09 / 0.8900\nDRRN [41]\n31.68 / 0.8888\n28.21 / 0.7720\n27.38 / 0.7284\n25.44 / 0.7638\n- / -\nMemNet [42]\n31.74 / 0.8893\n28.26 / 0.7723\n27.40 / 0.7281\n25.50 / 0.7630\n29.42 / 0.8942\nCARN [1]\n32.13 / 0.8937\n28.60 / 0.7806\n27.58 / 0.7349\n26.07 / 0.7837\n30.47 / 0.9084\nIMDN [17]\n32.21 / 0.8948\n28.58 / 0.7811\n27.56 / 0.7353\n26.04 / 0.7838\n30.45 / 0.9075\nLatticeNet [28]\n32.30 / 0.8962\n28.68 / 0.7830\n27.62 / 0.7367\n26.25 / 0.7873\n- / -\nSRDD-64\n32.05 / 0.8936\n28.54 / 0.7809\n27.54 / 0.7353\n25.89 / 0.7812\n30.16 / 0.9043\nSRDD-128\n32.25 / 0.8958\n28.65 / 0.7838\n27.61 / 0.7378\n26.10 / 0.7877\n30.44 / 0.9084\nRCAN [56]\n(32.63/0.9002)\n(28.87/0.7889)\n(27.77/0.7436)\n(26.82/0.8087)\n(31.22/0.9173)\nNLSA [31]\n(32.59/0.9000)\n(28.87/0.7891)\n(27.78/0.7444)\n(26.96/0.8109)\n(31.27/0.9184)\nSwinIR [26]\n(32.72/0.9021)\n(28.94/0.7914)\n(27.83/0.7459)\n(27.07/0.8164)\n(31.67/0.9226)\nTable 3. Execution time of representative models on an Nvidia P4000 GPU for ×4\nSR with input size 256 × 256.\nRunning time [s]\nSRCNN [9]\n0.0669\nFSRCNN [10]\n0.0036\nVDSR [19]\n0.2636\nLapSRN [23]\n0.1853\nCARN [1]\n0.0723\nIMDN [17]\n0.0351\nSRDD-64\n0.0842\nSRDD-128\n0.2196\nRCAN [56]\n1.5653\nNLSA [31]\n1.7139\nSwinIR [26]\n2.1106\nFSRCNN [10], VDSR [19], DRCN [20], LapSRN [23], DRRN [41], MemNet [42],\nCARN [1], IMDN [17], and LatticeNet [28]. We also compare our model with a\nrepresentative sparse coding-based method A+ [45]. Results for the representa-\ntive very deep models RCAN [56], NLSA [31], and SwinIR [26] are also shown.\nThe quantitative results for ×4 SR on benchmark datasets are shown in\nTab. 2. SRDD-64 and SRDD-128 show comparable performances to CARN/IMDN\nand LatticeNet, respectively. As shown in Tab. 3, the inference speed of SRDD-\n64 is also comparable to that of CARN, but slower than IMDN. These results\nindicate that the overall performance of our method on in-domain test images is\nclose to that of conventional baselines (not as good as state-of-the-art models).\nThe running time of representative deep models are also shown for comparison.\nThey are about 20 times slower than CARN and SRDD-64. The visual results\nare provided in Fig. 8.\nImage Super-Resolution with Deep Dictionary\n11\nSet14: baboon\nHR\nBicubic\nA+\nSRCNN\nFSRCNN\nVDSR\nLapSRN\nCARN\nSRDD-64 SRDD-128\nSet14: comic\nHR\nBicubic\nA+\nSRCNN\nFSRCNN\nVDSR\nLapSRN\nCARN\nSRDD-64 SRDD-128\nSet14: zebra\nHR\nBicubic\nA+\nSRCNN\nFSRCNN\nVDSR\nLapSRN\nCARN\nSRDD-64 SRDD-128\nUrban100: 012\nHR\nBicubic\nA+\nSRCNN\nFSRCNN\nVDSR\nLapSRN\nCARN\nSRDD-64 SRDD-128\nUrban100: 076\nHR\nBicubic\nA+\nSRCNN\nFSRCNN\nVDSR\nLapSRN\nCARN\nSRDD-64 SRDD-128\nFig. 8. Visual comparison for ×4 SR on Set14 and Urban100 dataset. Zoom in for a\nbetter view.\n12\nS. Maeda\nTable 4. Quantitative results of ×4 SR on Set14 downsampled with three different\nOpenCV resize functions. Note that the models are trained with Matlab bicubic down-\nsampling.\nBicubic\nBilinear\nArea\nPSNR / SSIM\nPSNR / SSIM\nPSNR / SSIM\nCARN [1]\n21.17 / 0.6310\n22.76 / 0.6805\n26.74 / 0.7604\nIMDN [17]\n20.99 / 0.6239\n22.54 / 0.6741\n26.60 / 0.7589\nIKC [14]\n20.10 / 0.6031\n21.71 / 0.6558\n26.40 / 0.7554\nSRDD-64\n21.52 / 0.6384\n23.13 / 0.6871\n27.05 / 0.7630\nBicubic\nCARN\nSRDD-64\nFig. 9. Visual comparison for ×4 SR on Set14 baboon downsampled with OpenCV\nbicubic function.\n4.5\nResults on Out-of-domain Test Images\nSynthetic test images We conduct experiments on Set14, where the LR in-\nput images are created differently from training time. We use bicubic, bilinear,\nand area downsampling with OpenCV resize functions. The difference between\nMatlab and OpenCV resize functions mainly comes from the anti-aliasing option.\nThe anti-aliasing is default enabled/unenabled in Matlab/OpenCV, respectively.\nWe mainly evaluate CARN and SRDD-64 because these models have comparable\nperformance on in-domain test images. The state-of-the-art lightweight model\nIMDN [17] and the representative blind SR model IKC [14] are also evaluated\nfor comparison. The results are shown in Tab. 4. SRDD-64 outperformed these\nmodels by a large margin for the three different resize functions. This result\nimplies that our method is more robust for the out-of-domain images than con-\nventional deep-learning-based methods. The visual comparison on a test image\ndownsampled with OpenCV bicubic function is shown in Fig. 9. CARN overly\nemphasizes high-frequency components of the image, while SRDD-64 outputs a\nmore natural result.\nReal-world test images\nWe conduct experiments on widely used ten his-\ntorical images to see the robustness of the models on unknown degradations.\nBecause there is no ground-truth image, we adopt a no-reference image qual-\nity metric NIQE for evaluation. Table 5 shows average NIQE for representative\nmethods. As seen in the previous subsection, our SRDD-64 shows comparable\nperformance to CARN if compared with in-domain test images. However, on the\nrealistic datasets with the NIQE metric, SRDD-64 clearly outperforms CARN\nand is close to EDSR. Interestingly, unlike the results on the in-domain test im-\nages, the performance of SRDD-64 is better than that of SRDD-128 for realistic\ndegradations. This is probably because representing an HR image with a small\nImage Super-Resolution with Deep Dictionary\n13\nTable 5. Results of no-reference image quality metric NIQE on real-world historical\nimages. Note that the models are trained with Matlab bicubic downsampling.\nNIQE (lower is better)\nBicubic\n7.342\nA+ [45]\n6.503\nSRCNN [9]\n6.267\nFSRCNN [10]\n6.130\nVDSR [19]\n6.038\nLapSRN [23]\n6.234\nCARN [1]\n5.921\nEDSR [27]\n5.852\nSRDD-64\n5.877\nSRDD-128\n5.896\nHistorical: 004\nBicubic\nA+\nSRCNN\nFSRCNN\nVDSR\nLapSRN\nCARN\nEDSR\nSRDD-64 SRDD-128\nHistorical: 007\nBicubic\nA+\nSRCNN\nFSRCNN\nVDSR\nLapSRN\nCARN\nEDSR\nSRDD-64 SRDD-128\nFig. 10. Visual comparison for ×4 SR on real-world historical images. Zoom in for a\nbetter view.\nnumber of atoms makes the atoms more versatile. The visual results are provided\nin Fig. 10.\n4.6\nExperiments of ×8 SR\nTo see if our method would work at different scaling factors, we also experiment\nwith the ×8 SR case. We use DIV2K dataset for training and validation. The test\nimages are prepared with the same downsampling function (i.e. Matlab bicubic\nfunction) as the training dataset. Figure 11 shows generated atoms of SRDD\nwith s = 8 and N = 128. The structure of atoms with s = 8 is finer than that\nwith s = 4, while the coarse structures of both cases are similar. The quantitative\nresults on five benchmark datasets are shown in Tab. 6. SRDD performs better\nthan the representative shallow models though its performance does not reach\nrepresentative deep model EDSR.\n14\nS. Maeda\nFig. 11. Learned atoms of ×8 SRDD with N = 128. The size of each atom is 1×8×8.\nThe data range is renormalized to [0, 1] for visualization.\nTable 6. Quantitative comparison for ×8 SR on benchmark datasets. Best and second\nbest results are highlighted in red and blue, respectively.\nMethod\nSet5\nSet14\nBSD100\nUrban100\nManga109\nPSNR / SSIM\nPSNR / SSIM\nPSNR / SSIM\nPSNR / SSIM\nPSNR / SSIM\nBicubic\n24.40 / 0.6580\n23.10 / 0.5660\n23.67 / 0.5480\n20.74 / 0.5160\n21.47 / 0.6500\nSRCNN [9]\n25.33 / 0.6900\n23.76 / 0.5910\n24.13 / 0.5660\n21.29 / 0.5440\n22.46 / 0.6950\nFSRCNN [10]\n20.13 / 0.5520\n19.75 / 0.4820\n24.21 / 0.5680\n21.32 / 0.5380\n22.39 / 0.6730\nVDSR [19]\n25.93 / 0.7240\n24.26 / 0.6140\n24.49 / 0.5830\n21.70 / 0.5710\n23.16 / 0.7250\nLapSRN [23]\n26.15 / 0.7380\n24.35 / 0.6200\n24.54 / 0.5860\n21.81 / 0.5810\n23.39 / 0.7350\nMemNet [42]\n26.16 / 0.7414\n24.38 / 0.6199\n24.58 / 0.5842\n21.89 / 0.5825\n23.56 / 0.7387\nEDSR [27]\n26.96 / 0.7762\n24.91 / 0.6420\n24.81 / 0.5985\n22.51 / 0.6221\n24.69 / 0.7841\nSRDD-64\n26.66 / 0.7652\n24.75 / 0.6345\n24.71 / 0.5926\n22.20 / 0.6034\n24.14 / 0.7621\nSRDD-128\n26.76 / 0.7677\n24.79 / 0.6369\n24.75 / 0.5947\n22.25 / 0.6073\n24.25 / 0.7672\n5\nConclusions\nWe propose an end-to-end super-resolution network with a deep dictionary\n(SRDD). An explicitly learned high-resolution dictionary (DH) is used to up-\nscale the input image as in the sparse-coding-based methods, while the entire\nnetwork, including the DH generator, is simultaneously optimized to take full\nadvantage of deep learning. For in-domain test images (images created by the\nsame procedure as the training dataset), the proposed SRDD shows performance\nthat is not as good as latest ones, but close to the conventional baselines (eg.,\nCARN). For out-of-domain test images, SRDD outperforms conventional deep-\nlearning-based methods, demonstrating the robustness of our model.\nThe proposed method is not limited to super-resolution tasks but is po-\ntentially applicable to other tasks that require high-resolution output, such as\nhigh-resolution image generation. Hence, the proposed method is expected to\nhave a broad impact on various tasks. Future works will be focused on the appli-\ncation of the proposed method to other vision tasks. In addition, we believe that\nour method still has much room for improvement compared to the conventional\ndeep-learning-based approach.\nAcknowledgements\nI thank Uday Bondi for helpful comments on the\nmanuscript.\nImage Super-Resolution with Deep Dictionary\n15\nReferences\n1. Ahn, N., Kang, B., Sohn, K.A.: Fast, accurate, and lightweight super-resolution\nwith cascading residual network. In: ECCV (2018)\n2. Bevilacqua, M., Roumy, A., Guillemot, C., Alberi-Morel, M.L.: Low-complexity\nsingle-image super-resolution based on nonnegative neighbor embedding. In:\nBMVC (2012)\n3. Chan, K.C., Wang, X., Xu, X., Gu, J., Loy, C.C.: Glean: Generative latent bank\nfor large-factor image super-resolution. In: CVPR (2021)\n4. Chang, H., Yeung, D.Y., Xiong, Y.: Super-resolution through neighbor embedding.\nIn: CVPR (2004)\n5. Chen, C., Shi, X., Qin, Y., Li, X., Han, X., Yang, T., Guo, S.: Real-world blind\nsuper-resolution via feature matching with implicit high-resolution priors. arXiv\npreprint arXiv:2202.13142 (2022)\n6. Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C.,\nGao, W.: Pre-trained image processing transformer. In: CVPR (2021)\n7. Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution\nfor semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)\n8. Dai, T., Cai, J., Zhang, Y., Xia, S.T., Zhang, L.: Second-order attention network\nfor single image super-resolution. In: CVPR (2019)\n9. Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for\nimage super-resolution. In: ECCV (2014)\n10. Dong, C., Loy, C.C., Tang, X.: Accelerating the super-resolution convolutional\nneural network. In: ECCV (2016)\n11. Freedman, G., Fattal, R.: Image and video upscaling from local self-examples. TOG\n30(2), 1–11 (2011)\n12. Freeman, W.T., Jones, T.R., Pasztor, E.C.: Example-based super-resolution.\nCG&A 22(2), 56–65 (2002)\n13. Glasner, D., Bagon, S., Irani, M.: Super-resolution from a single image. In: ICCV\n(2009)\n14. Gu, J., Lu, H., Zuo, W., Dong, C.: Blind super-resolution with iterative kernel\ncorrection. In: CVPR (2019)\n15. Gu, S., Zuo, W., Xie, Q., Meng, D., Feng, X., Zhang, L.: Convolutional sparse\ncoding for image super-resolution. In: ICCV (2015)\n16. Huang, J.B., Singh, A., Ahuja, N.: Single image super-resolution from transformed\nself-exemplars. In: CVPR (2015)\n17. Hui, Z., Gao, X., Yang, Y., Wang, X.: Lightweight image super-resolution with\ninformation multi-distillation network. In: ACM Multimedia (2019)\n18. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In: ICML (2015)\n19. Kim, J., Lee, J.K., Lee, K.M.: Accurate image super-resolution using very deep\nconvolutional networks. In: CVPR (2016)\n20. Kim, J., Lee, J.K., Lee, K.M.: Deeply-recursive convolutional network for image\nsuper-resolution. In: CVPR (2016)\n21. Kim, K.I., Kwon, Y.: Single-image super-resolution using sparse regression and\nnatural image prior. TPAMI 32(6), 1127–1133 (2010)\n22. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR\n(2014)\n23. Lai, W.S., Huang, J.B., Ahuja, N., Yang, M.H.: Deep laplacian pyramid networks\nfor fast and accurate super-resolution. In: CVPR (2017)\n16\nS. Maeda\n24. Li, H., Xiong, P., An, J., Wang, L.: Pyramid attention network for semantic seg-\nmentation. In: BMVC (2018)\n25. Li, Y., Zhang, K., Timofte, R., Van Gool, L., Kong, F., Li, M., Liu, S., Du, Z., Liu,\nD., Zhou, C., et al.: Ntire 2022 challenge on efficient super-resolution: Methods\nand results. In: CVPRW (2022)\n26. Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., Timofte, R.: Swinir: Image\nrestoration using swin transformer. In: ICCV (2021)\n27. Lim, B., Son, S., Kim, H., Nah, S., Mu Lee, K.: Enhanced deep residual networks\nfor single image super-resolution. In: CVPRW (2017)\n28. Luo, X., Xie, Y., Zhang, Y., Qu, Y., Li, C., Fu, Y.: Latticenet: Towards lightweight\nimage super-resolution with lattice block. In: ECCV (2020)\n29. Martin, D., Fowlkes, C., Tal, D., Malik, J.: A database of human segmented natural\nimages and its application to evaluating segmentation algorithms and measuring\necological statistics. In: ICCV (2001)\n30. Matsui, Y., Ito, K., Aramaki, Y., Fujimoto, A., Ogawa, T., Yamasaki, T., Aizawa,\nK.: Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and\nApplications 76(20), 21811–21838 (2017)\n31. Mei, Y., Fan, Y., Zhou, Y.: Image super-resolution with non-local sparse attention.\nIn: CVPR (2021)\n32. Mittal, A., Soundararajan, R., Bovik, A.C.: Making a “completely blind” image\nquality analyzer. Signal Processing Letters 20(3), 209–212 (2012)\n33. Nair, V., Hinton, G.E.: Rectified linear units improve restricted boltzmann ma-\nchines. In: ICML (2010)\n34. Niu, B., Wen, W., Ren, W., Zhang, X., Yang, L., Wang, S., Zhang, K., Cao, X.,\nShen, H.: Single image super-resolution via a holistic attention network. In: ECCV\n(2020)\n35. Osendorfer, C., Soyer, H., Smagt, P.v.d.: Image super-resolution with fast approx-\nimate convolutional sparse coding. In: ICONIP (2014)\n36. Peters, B., Niculae, V., Martins, A.F.: Sparse sequence-to-sequence models. In:\nACL (2019)\n37. Shi, W., Caballero, J., Husz´ar, F., Totz, J., Aitken, A.P., Bishop, R., Rueckert,\nD., Wang, Z.: Real-time single image and video super-resolution using an efficient\nsub-pixel convolutional neural network. In: CVPR (2016)\n38. Shocher, A., Cohen, N., Irani, M.: “zero-shot” super-resolution using deep internal\nlearning. In: CVPR (2018)\n39. Simon, D., Elad, M.: Rethinking the csc model for natural images. NeurIPS (2019)\n40. Soh, J.W., Cho, S., Cho, N.I.: Meta-transfer learning for zero-shot super-resolution.\nIn: CVPR (2020)\n41. Tai, Y., Yang, J., Liu, X.: Image super-resolution via deep recursive residual net-\nwork. In: CVPR (2017)\n42. Tai, Y., Yang, J., Liu, X., Xu, C.: Memnet: A persistent memory network for image\nrestoration. In: ICCV (2017)\n43. Timofte, R., Agustsson, E., Van Gool, L., Yang, M.H., Zhang, L.: Ntire 2017 chal-\nlenge on single image super-resolution: Methods and results. In: CVPRW (2017)\n44. Timofte, R., De Smet, V., Van Gool, L.: Anchored neighborhood regression for\nfast example-based super-resolution. In: ICCV (2013)\n45. Timofte, R., De Smet, V., Van Gool, L.: A+: Adjusted anchored neighborhood\nregression for fast super-resolution. In: ACCV (2014)\n46. Wang, L., Wang, Y., Dong, X., Xu, Q., Yang, J., An, W., Guo, Y.: Unsupervised\ndegradation representation learning for blind super-resolution. In: CVPR (2021)\nImage Super-Resolution with Deep Dictionary\n17\n47. Wang, Z., Liu, D., Yang, J., Han, W., Huang, T.: Deep networks for image super-\nresolution with sparse prior. In: ICCV (2015)\n48. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:\nfrom error visibility to structural similarity. TIP 13(4), 600–612 (2004)\n49. Xu, Y.S., Tseng, S.Y.R., Tseng, Y., Kuo, H.K., Tsai, Y.M.: Unified dynamic con-\nvolutional network for super-resolution with variational degradations. In: CVPR\n(2020)\n50. Yang, J., Lin, Z., Cohen, S.: Fast image super-resolution based on in-place example\nregression. In: CVPR (2013)\n51. Yang, J., Wang, Z., Lin, Z., Cohen, S., Huang, T.: Coupled dictionary training for\nimage super-resolution. TIP 21(8), 3467–3478 (2012)\n52. Yang, J., Wright, J., Huang, T.S., Ma, Y.: Image super-resolution via sparse rep-\nresentation. TIP 19(11), 2861–2873 (2010)\n53. Zeyde, R., Elad, M., Protter, M.: On single image scale-up using sparse-\nrepresentations. In: Curves and Surfaces (2010)\n54. Zhang, K., Danelljan, M., Li, Y., Timofte, R., Liu, J., Tang, J., Wu, G., Zhu, Y.,\nHe, X., Xu, W., et al.: Aim 2020 challenge on efficient super-resolution: Methods\nand results. In: ECCVW (2020)\n55. Zhang, K., Zuo, W., Zhang, L.: Learning a single convolutional super-resolution\nnetwork for multiple degradations. In: CVPR (2018)\n56. Zhang, Y., Li, K., Li, K., Wang, L., Zhong, B., Fu, Y.: Image super-resolution\nusing very deep residual channel attention networks. In: ECCV (2018)\n57. Zhang, Y., Tian, Y., Kong, Y., Zhong, B., Fu, Y.: Residual dense network for image\nsuper-resolution. In: CVPR (2018)\n58. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\nCVPR (2017)\n59. Zhou, R., Susstrunk, S.: Kernel modeling super-resolution on real low-resolution\nimages. In: ICCV (2019)\n60. Zhou, S., Chan, K.C., Li, C., Loy, C.C.: Towards robust blind face restoration with\ncodebook lookup transformer. arXiv preprint arXiv:2206.11253 (2022)\n61. Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J.: Unet++: A nested\nu-net architecture for medical image segmentation. DLMIA pp. 3–11 (2018)\n",
  "categories": [
    "cs.CV",
    "eess.IV"
  ],
  "published": "2022-07-19",
  "updated": "2022-07-19"
}