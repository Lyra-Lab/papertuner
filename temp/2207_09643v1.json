{
  "id": "http://arxiv.org/abs/2207.09643v1",
  "title": "Integrating Linguistic Theory and Neural Language Models",
  "authors": [
    "Bai Li"
  ],
  "abstract": "Transformer-based language models have recently achieved remarkable results\nin many natural language tasks. However, performance on leaderboards is\ngenerally achieved by leveraging massive amounts of training data, and rarely\nby encoding explicit linguistic knowledge into neural models. This has led many\nto question the relevance of linguistics for modern natural language\nprocessing. In this dissertation, I present several case studies to illustrate\nhow theoretical linguistics and neural language models are still relevant to\neach other. First, language models are useful to linguists by providing an\nobjective tool to measure semantic distance, which is difficult to do using\ntraditional methods. On the other hand, linguistic theory contributes to\nlanguage modelling research by providing frameworks and sources of data to\nprobe our language models for specific aspects of language understanding.\n  This thesis contributes three studies that explore different aspects of the\nsyntax-semantics interface in language models. In the first part of my thesis,\nI apply language models to the problem of word class flexibility. Using mBERT\nas a source of semantic distance measurements, I present evidence in favour of\nanalyzing word class flexibility as a directional process. In the second part\nof my thesis, I propose a method to measure surprisal at intermediate layers of\nlanguage models. My experiments show that sentences containing morphosyntactic\nanomalies trigger surprisals earlier in language models than semantic and\ncommonsense anomalies. Finally, in the third part of my thesis, I adapt several\npsycholinguistic studies to show that language models contain knowledge of\nargument structure constructions. In summary, my thesis develops new\nconnections between natural language processing, linguistic theory, and\npsycholinguistics to provide fresh perspectives for the interpretation of\nlanguage models.",
  "text": "INTEGRATING LINGUISTIC THEORY AND NEURAL LANGUAGE MODELS\nby\nBai Li\nA thesis submitted in conformity with the requirements\nfor the degree of Doctor of Philosophy\nGraduate Department of Computer Science\nUniversity of Toronto\n© Copyright 2022 by Bai Li\narXiv:2207.09643v1  [cs.CL]  20 Jul 2022\nAbstract\nIntegrating Linguistic Theory and Neural Language Models\nBai Li\nDoctor of Philosophy\nGraduate Department of Computer Science\nUniversity of Toronto\n2022\nTransformer-based language models have recently achieved remarkable results in many natural language tasks.\nHowever, performance on leaderboards is generally achieved by leveraging massive amounts of training data\nand computation, and rarely by encoding explicit linguistic knowledge into neural models. This has led many\nto question the relevance of linguistics for modern natural language processing. In this dissertation, I present\nseveral case studies to illustrate how theoretical linguistics and deep neural language models are still relevant\nto each other. First, language models are useful to linguists by providing an automatic and objective tool to\nmeasure semantic distance, which is difﬁcult to do using traditional methods. On the other hand, linguistic theory\ncontributes to language modelling research by providing frameworks and sources of data to probe our language\nmodels for speciﬁc aspects of language understanding.\nThis thesis contributes three studies that explore different aspects of the syntax-semantics interface in language\nmodels. In the ﬁrst part of my thesis, I apply language models to the problem of word class ﬂexibility, a long-\ndebated issue in theoretical linguistics. Using mBERT as a source of semantic distance measurements across\nmany languages, I present evidence in favour of analyzing word class ﬂexibility as a directional process. In the\nsecond part of my thesis, I propose a method to measure surprisal at intermediate layers of language models,\nusing Gaussian models for density estimation. My experiments show that sentences containing morphosyntactic\nanomalies trigger surprisals earlier in language models than semantic and commonsense anomalies. Finally, in the\nthird part of my thesis, I adapt several psycholinguistic studies to show that language models contain knowledge\nof argument structure constructions (a proposed analysis of verb valency from construction grammar theory).\nIn summary, my thesis develops new connections between natural language processing, linguistic theory, and\npsycholinguistics to provide fresh perspectives for the interpretation of language models.\nii\nAcknowledgements\nI am grateful for the funding provided by the University of Toronto and the Vector Institute, allowing me to\npursue graduate research in natural language processing.\nThanks to my supervisor, Prof. Frank Rudzicz for being my guide and mentor throughout my journey in\ngraduate school, from when I ﬁrst joined your lab in 2017 as a master’s student until the completion of my PhD.\nThank you for teaching me how to do research, then eventually encouraging me to set my own direction, yet\nalways being available when I needed your support.\nThanks to Prof. Robert Frank for serving as the external examiner for my ﬁnal thesis defence, Prof. Suzanne\nStevenson for serving on the defence committee, and Prof. Steven Waslander for serving as the meeting chair.\nI would like to thank my supervisory committee, Prof. Yang Xu and Prof. Guillaume Thomas, for being\noutstanding collaborators. You have taught me an immense amount about linguistics and cognitive science, and\nthis interdisciplinary research would not have been possible without your expertise. Thanks to your unrelenting\ncritical feedback on my experimental design and paper writing, so that I may submit the paper conﬁdently and\nwithout fearing Reviewer #2.\nThanks to the graduate students in SPOClab and the Computational Linguistics group, including Arnold,\nDemetres, Francois, Hillary, Ian, Jixuan, John, KP, Raeid, Serena, Stephane, Yoona, Yuchen, and Zining. I learned\na lot from collaborating with you on various coursework and research projects, and engaging in presentations and\npaper reading group discussions.\nThanks to my coworkers at Snaptravel (now Snapcommerce) and Ada Support, where I worked part-time as\na machine learning engineer for a large part of my PhD. There, I have had the opportunity to experience many\nmachine learning projects in the real world, beyond my narrow research domain.\nI am thankful to have wonderful friends including Andrei, Kevin D, Kevin P, Leon, Michael, and Niranjan. It\nwas fun to engage in heated debates with you about philosophy, world politics, economics, business ideas, and all\nsorts of other topics when I’m procrastinating on research.\nThanks to my printer, dutifully producing approximately 15,000 pages of papers and textbooks while hardly\never jamming.\nThanks to my parents in Calgary and sister in Toronto for encouraging me when times were uncertain and\nfeeding me when I am at home. After three years of asking me “when will you be ﬁnishing your PhD?”, I ﬁnally\nhave a straight answer.\nFinally, I am thankful for my wife Elaine, who has remained by my side to share my every success and failure,\nand offer me support when I needed it the most. I am lucky to be with you.\niii\nContents\n1\nIntroduction\n1\n1.1\nMotivation: why linguistic probing? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nBridging NLP and linguistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.3\nThe syntax-semantics interface in language models . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.4\nStructure of thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.5\nRelationship to published work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2\nModern neural language models\n7\n2.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2\nDistributional semantics and word embeddings\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3\nSequential models and contextual embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.4\nTransformer-based language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.5\nProbing classiﬁers and their shortcomings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3\nProbing for linguistic knowledge in LMs\n15\n3.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.2\nBehavioural probes for syntax\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.2.1\nAgreement\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.2.2\nOther syntactic phenomena . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.2.3\nGradience of acceptability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n3.3\nRepresentational probes of LM embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.3.1\nLayerwise probing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.3.2\nStructural probes for syntax\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3.3.3\nProbes involving LM training\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.4\nAdapting psycholinguistics to LMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\niv\n3.4.1\nThe N400 response and surprisal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.4.2\nPriming in LMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.5\nUsing LMs as evidence for linguistic theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n4\nWord class ﬂexibility in LMs\n27\n4.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.2\nLinguistic background and assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.2.1\nTypes of ﬂexibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.2.2\nHomonymy and polysemy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n4.2.3\nDirectionality of class conversion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n4.2.4\nAsymmetry in semantic shift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4.3\nIdentiﬁcation of word class ﬂexibility\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4.3.1\nDeﬁnitions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4.3.2\nDatasets and preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.3.3\nLemma merging algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.4\nMethodology and evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.4.1\nProbing test of contextualized model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.4.2\nThree contextual metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.5\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n4.5.1\nIdentifying ﬂexible lemmas\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n4.5.2\nAsymmetry in semantic metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n4.5.3\nModel robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n4.6\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n4.6.1\nFrequency asymmetry\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n4.6.2\nImplications for theories of ﬂexibility . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n4.7\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5\nLinguistic anomalies in LMs\n42\n5.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n5.2\nRelated work\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n5.3\nModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n5.3.1\nConnection to Mahalanobis distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n5.3.2\nTraining and evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n5.3.3\nFurther ablation studies on Gaussian model . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nv\n5.3.4\nLower layers are sensitive to frequency\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n5.4\nLevels of linguistic anomalies\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n5.4.1\nSummary of anomaly datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n5.4.2\nQuantifying layerwise surprisal\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.5\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n5.5.1\nAnomaly type and surprisal\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n5.5.2\nComparing anomaly model with MLM\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n5.5.3\nDifferences between LMs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n5.6\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n6\nConstruction grammar in LMs\n60\n6.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n6.2\nLinguistic background\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n6.2.1\nConstruction grammar and ASCs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n6.2.2\nPsycholinguistic evidence for ASCs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n6.2.3\nRelated work in NLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n6.3\nCase study 1: Sentence sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n6.3.1\nMethodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n6.3.2\nResults and interpretation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n6.4\nCase study 2: Jabberwocky constructions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n6.4.1\nMethodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n6.4.2\nResults and interpretation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n6.4.3\nPotential confounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n6.5\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n7\nConclusion\n74\n7.1\nSynopsis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n7.2\nFuture directions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n7.2.1\nWhich models to probe? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n7.2.2\nEvidence from learnability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n7.2.3\nPsycholinguistic-based probing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\nBibliography\n77\nvi\nList of Tables\n3.1\nExample acceptable and unacceptable sentences for the 12 types of linguistic phenomena in\nBLiMP (Warstadt et al., 2020a).\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.1\nNoun and verb ﬂexibility for 37 languages with at least 100k tokens in the UD corpus. We include\nthe 27 languages with over 2.5% noun and verb ﬂexibility; 10 languages are excluded from further\nanalysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n4.2\n138 ﬂexible words in English (top in BNC corpus) and human similarity scores, average of 5 ratings. 34\n4.3\nSemantic metrics for 25 languages, computed using mBERT and 10M tokens of Wikipedia text\nfor each language. Asterisks denote signiﬁcance at ∗p < 0.05, ∗∗p < 0.01, ∗∗∗p < 0.001. For\nthe “Overall” row, we count the languages with a signiﬁcant tendency towards one direction, out\nof the number of languages with statistical signiﬁcance towards either direction (with p < 0.05\ntreated as signiﬁcant). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n4.4\nComparison of semantic models on BNC and Wikipedia datasets (English), computed using\nseveral different language models. Asterisks denote signiﬁcance at ∗p < 0.05, ∗∗p < 0.01,\n∗∗∗p < 0.001. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n5.1\nVarying the type of covariance matrix in the Gaussian model. . . . . . . . . . . . . . . . . . . . .\n48\n5.2\nUsing Gaussian mixture models (GMMs) with multiple components. . . . . . . . . . . . . . . . .\n48\n5.3\nEffect of the genre of training data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n5.4\nUsing 1-SVM instead of GMM, with various kernels. . . . . . . . . . . . . . . . . . . . . . . . .\n48\n5.5\nTwo sentence-level aggregation strategies\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n5.6\nExample sentence pair for each of the 12 tasks. The 3 BLiMP tasks are generated from templates;\nthe others are stimuli materials taken from psycholinguistic studies.\n. . . . . . . . . . . . . . . .\n51\nvii\n5.7\nComparing accuracy scores between Gaussian anomaly model (GM) and masked language model\n(MLM) for all models and tasks. Asterisks indicate that the accuracy is not better than random\n(0.5), using a binomial test with threshold of p < 0.05 for signiﬁcance. The MLM results for\nChow et al. (2016) are excluded because the control and anomalous sentences differ by more than\none token. The best layers for each model (Section 5.3.2) are used for GM, and the last layer is\nused for MLM. Generally, MLM outperforms GM, and the difference is greater for semantic and\ncommonsense tasks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n6.1\nStimuli from Bencini and Goldberg (2000), consisting of a 4x4 design, with 4 different verbs and\n4 different argument structure constructions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n6.2\nExample of our 4x4 sentence sorting stimuli, similar to those by Bencini and Goldberg (2000) in\nTable 6.1, but generated automatically using templates. . . . . . . . . . . . . . . . . . . . . . . .\n66\n6.3\nGerman sentence sorting stimuli, obtained from Kirsch (2019). . . . . . . . . . . . . . . . . . . .\n66\n6.4\nItalian sentence sorting stimuli, obtained from Baicchi and Della Putta (2019). . . . . . . . . . . .\n66\n6.5\nSpanish sentence sorting stimuli, obtained from V´azquez (2004). . . . . . . . . . . . . . . . . . .\n66\n6.6\nTemplates and example sentences for the Jabberwocky construction experiments. The templates\nare identical to the ones used in Johnson and Goldberg (2013), except that we use random real\nwords instead of nonce words. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\nviii\nList of Figures\n1.1\nThe main contributions of this thesis. Chapter 4 on word class ﬂexibility uses LMs as evidence\nfor a debate in linguistic theory; Chapter 5 on linguistic anomalies and Chapter 6 on construction\ngrammar apply linguistic frameworks and data toward LM probing.\n. . . . . . . . . . . . . . . .\n3\n2.1\nVisualizing distributional semantic models. Each individual dimension is semantically meaning-\nless, but similar words are closer together in the vector space, as measured by Euclidean or cosine\ndistance. Figure adapted from Boleda (2020). . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.2\nA recurrent neural network (RNN) for language modelling. At each step, the RNN predicts the\nnext word in the sequence, given a hidden state that is derived from the previous words. The\ninputs may be one-hot encodings of the words, or static word embeddings such as word2vec or\nGLoVE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3\nThe BERT model (Devlin et al., 2019), consisting of 12 layers of Transformer modules (Vaswani\net al., 2017). The model is ﬁrst pre-trained on masked language modelling and next sentence\nprediction, then ﬁne-tuned for downstream tasks. BERT may take either a single sentence as\ninput, or two sentences separated by a [SEP] token.\n. . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.4\nIllustration of the edge probing method proposed by Tenney et al. (2019a). Here, the probe is\ngiven a sentence “Last week New York City had its worst ...” and a span “New York City” and the\nprobing task is to classify the type of named entity represented by the span (Location).\n. . . . . .\n13\n4.1\nSpearman correlations between human and model similarity scores for ELMo, BERT, mBERT,\nand XLM-R. The dashed line is the baseline using static GloVe embeddings. . . . . . . . . . . . .\n35\n4.2\nPCA plot of BERT embeddings for the lemmas “work” (high similarity between noun and verb\nsenses) and “ring” (low similarity). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nix\n5.1\nExample sentence with a morphosyntactic anomaly (left) and semantic anomaly (right) (anoma-\nlies in bold). Darker colours indicate higher surprisal. We investigate several patterns: ﬁrst,\nsurprisal at lower layers corresponds to infrequent tokens, but this effect diminishes towards up-\nper layers. Second, morphosyntactic violations begin to trigger high surprisals at an earlier layer\nthan semantic violations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n5.2\nBLiMP accuracy different amounts of training data and across layers, for three LMs. About 1000\nsentences are needed before a plateau is reached (mean tokens per sentence = 15.1). . . . . . . . .\n46\n5.3\nPearson correlation between token-level surprisal scores (Equation 5.4) and log frequency. The\ncorrelation is highest in the lower layers, and decreases in the upper layers.\n. . . . . . . . . . . .\n49\n5.4\nPCA plot of randomly sampled RoBERTa embeddings at layers 1, 4, 7, and 10. Points are colored\nby token frequency: “Rare” means the 20% least frequent tokens, and “Frequent” is the other 80%. 50\n5.5\nLayerwise surprisal gaps for all tasks using the RoBERTa model. Generally, a positive surprisal\ngap appears in earlier layers for morphosyntactic tasks than for semantic tasks; no surprisal gap\nappears at any layer for commonsense tasks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n5.6\nLayerwise surprisal gaps for all tasks using the BERT model. . . . . . . . . . . . . . . . . . . . .\n55\n5.7\nLayerwise surprisal gaps for all tasks using the XLNet model.\n. . . . . . . . . . . . . . . . . . .\n56\n6.1\nFour argument structure constructions (ASCs) used by Bencini and Goldberg (2000), with exam-\nple sentences (top right). Constructions are mappings between form (bottom left) and meaning\n(bottom right). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n6.2\nEnglish sentence sorting results for humans and LMs, measured by deviation from pure construc-\ntion and verb sort (CDev and VDev). Non-native human results are from Liang (2002); native\nhuman results from Bencini and Goldberg (2000).1LM results are obtained using MiniBERTas\n(Warstadt et al., 2020b) and RoBERTa (Liu et al., 2019b) on templated stimuli. The MiniBERTa\nmodels use between 1M to 1B tokens for pretraining, while RoBERTa uses 30B tokens. Error\nbars indicate 95% conﬁdence intervals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n6.3\nPCA plots of Bencini and Goldberg (2000) sentence sorting using the 1M and 100M MiniBERTa\nmodels and RoBERTa-base (30B). Figure best viewed in color. . . . . . . . . . . . . . . . . . . .\n68\n6.4\nMultilingual sentence sorting results for German (Kirsch, 2019), Italian (Baicchi and Della Putta,\n2019), and Spanish (V´azquez, 2004). LM results are obtained using the same stimuli; we use both\nmBERT and a monolingual LM for each language.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n69\nx\n6.5\nIn our adapted Jabberwocky experiment, we measure the Euclidean distance from the Jabber-\nwocky verb (traded) to the 4 prototype verbs, of which 1 is congruent (\u0013) with the construction\nof the sentence, and 3 are incongruent (\u0017). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n6.6\nEuclidean distance between Jabberwocky and prototype verbs for congruent and incongruent con-\nditions. Error bars indicate 95% conﬁdence intervals. . . . . . . . . . . . . . . . . . . . . . . . .\n71\n6.7\nMean Euclidean distance between Jabberwocky and prototype verbs in each verb-construction\npair. Diagonal entries (gray border) are the congruent conditions; off-diagonal entries are incon-\ngruent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\nxi\nChapter 1\nIntroduction\nRecent advances in Transformer-based neural networks such as GPT (Radford et al., 2018), BERT (Devlin et al.,\n2019), and RoBERTa (Liu et al., 2019b) have achieved state-of-the-art performances in many natural language\nprocessing tasks (NLP), such as machine translation, text classiﬁcation, parsing, and question answering. A\nmajor challenge with these language models (LMs) is understanding how they function, since they are composed\nof millions of neurons whose meaning is not easily interpretable by humans. At the same time, LMs are built\nfrom complex mechanisms that manipulate large matrices of real numbers, with no basis in linguistic theories\nthat humans have developed to understand language. This naturally raises the question: how can neural language\nmodels be integrated with linguistic theory? In this thesis, I demonstrate how linguistic theories can serve as\na foundation upon which LMs can be evaluated for language understanding; on the other hand, LMs can also\nprovide evidence to support linguistic theories.\n1.1\nMotivation: why linguistic probing?\nAt ﬁrst glance, it is not immediately obvious why linguistic theory is necessary or desirable for understanding\nlanguage models. Progress in NLP is usually measured by a set of standard benchmarks, such as SQuAD 2 (Ra-\njpurkar et al., 2018) for question answering or SuperGLUE (Wang et al., 2019a) for various types of language\nunderstanding. Using these benchmarks, researchers can compare their models against previous ones, and public\nleaderboards rank all currently available models by their relative performance (often with a human baseline for\ncomparison). Practitioners can use these benchmarks to decide which model to apply for their own use case (pos-\nsibly along with other considerations such as model size and efﬁciency, which in any case can also be measured\nin benchmarks). One may wonder: if benchmarks already serve the needs of researchers and practitioners, why\ndo we need to involve linguistic theory?\n1\nCHAPTER 1. INTRODUCTION\n2\nBenchmarks suffer from several drawbacks in practice. They become “saturated”, where models quickly\nsurpass human baseline on the benchmark, even though they do not outperform humans in general: this leads to\na loss of trust in the benchmark’s validity and hinders further progress in the ﬁeld (Bowman and Dahl, 2021).\nBoth SQuAD 2 and SuperGLUE had models that exceeded human performance within months of their release,\neven though they were designed to be difﬁcult tasks, and neither question answering nor text classiﬁcation are\ngenerally considered solved. This can happen when models learn to exploit biases in the benchmark dataset that\nenable it to predict the correct answer in a way that is unintended (and incorrect): for example, predicting that two\nsentences are contradictions if a negation word is present (Gururangan et al., 2018). Eliminating all such biases is\ndifﬁcult because they are annotated by crowdworkers on naturally occurring data, and both tend to contain biases.\nMoreover, in most language understanding tasks, we lack a precise understanding of how the correct answer is\nrelated to the inputs of a task instance, so annotations must rely on the possibly differing interpretations of human\nannotators.\nTargeted linguistic evaluation offers a remedy to these limitations of benchmarking. It draws on decades\nof research aimed at describing language in as precise detail as possible: which sentences are grammatical and\nungrammatical, and how the meaning of a sentence is related to its surface structure. This body of knowledge\ngives the researcher the ability to control for biases and eliminate shortcut heuristics, revealing deﬁciencies in our\nmodels’ understanding of many language phenomena, such as negation (Ettinger, 2020), phrasal composition (Yu\nand Ettinger, 2020), and long-range dependencies (van Schijndel et al., 2019).\nProbing tasks can be derived from linguistic theory via templates: generating sentences of a given structure\naccording to a theoretical description of some language feature. An example of this is BLiMP (Warstadt et al.,\n2020a), a benchmark of 67 sets of template-generated sentences testing linguistic phenomena such as agreement\nand movement. A second and more direct approach is taking data from psycholinguistic publications to use as\nprobing – these sentences are written by linguists for human experimental stimuli and are carefully controlled for\npossible biases. I use both techniques extensively to obtain probing data in Chapters 5 and 6 of this thesis.\nAlthough linguistic probing offers certain advantages over standard NLP benchmarks, they are not meant to be\na replacement for benchmarking – the two serve different needs of the research community. The crucial difference\nis that probing aims to deepen our understanding of existing widely used language models, whereas developing\nnew models to achieve a high performance on the probing task is of lesser importance. Therefore, linguistic\nprobing can be viewed as a form of post-hoc interpretability, offering a global view of the model’s capabilities by\nidentifying areas of weakness from the perspective of linguistic theory (Madsen et al., 2021).\nCHAPTER 1. INTRODUCTION\n3\nLinguistic\nTheory\nNatural Language\nProcessing\nWord class flexibility in LMs (Chapter 4)\nAnomalies in LMs (Chapter 5)\nConstruction grammar in LMs (Chapter 6)\nFigure 1.1: The main contributions of this thesis. Chapter 4 on word class ﬂexibility uses LMs as evidence for\na debate in linguistic theory; Chapter 5 on linguistic anomalies and Chapter 6 on construction grammar apply\nlinguistic frameworks and data toward LM probing.\n1.2\nBridging NLP and linguistics\nThere has been relatively little contact between natural language processing and theoretical linguistics since the\ndeep learning revolution. While the two ﬁelds share some similarities – both involve data in human languages –\ntheir primary goals are different. NLP aims to develop computational systems to solve language tasks as accu-\nrately as possible, whereas linguistics aims to describe properties of languages and how humans process them.\nGiven these divergent goals, it is not obvious how advances in either ﬁeld should be relevant to the other. Some\nresearchers have attempted to incorporate linguistic and structural knowledge into deep neural models, but these\nmethods have not shown substantial performance improvements over models without linguistic knowledge (Lap-\npin, 2021). The likely reason for this failure is that language models are able to learn implicit structural properties\nof language through the usual training procedure (Hewitt and Manning, 2019; Miaschi et al., 2020), so providing\nexplicit knowledge is redundant.\nInstead of improving model performance directly, the more promising avenue for linguistics to contribute to\nNLP has been linguistic probing. Previous work has probed LMs for knowledge of many linguistic phenomena,\nwhich I will survey in Chapter 3. In this dissertation, I expand this body of work by studying two linguistic phe-\nnomena which have not been covered in earlier work: how LMs represent different types of linguistic anomalies\n(Chapter 5), and how they understand argument structure constructions (Chapter 6).\nIn the opposite direction, it has been even more difﬁcult for deep learning to contribute to linguistic theory.\nNeural network probing experiments do not provide information about how humans process language, and as\na result, this body of work has been rarely cited in linguistic publications (Baroni, 2021). My research offers\nan avenue of contribution in this direction: using deep models as a tool to measure semantic distance between\noccurrences of a word in different contexts (Chapter 4). Semantic distance is a metric of importance for theories\nof word class ﬂexibility, but is difﬁcult to measure using traditional methods. My three projects in this thesis serve\nto bridge the gap between NLP and linguistics and provide examples of interdisciplinary collaboration for both\nresearch communities.\nCHAPTER 1. INTRODUCTION\n4\n1.3\nThe syntax-semantics interface in language models\nA traditional dichotomy in the study of language is between syntax and semantics. Syntax studies well-formed\nphrases and their internal structures, whereas semantics studies how meaning is derived from syntactic structures.\nMany linguistic phenomena involve the interplay between syntax and semantics, often with complex effects that\npose challenging problems for linguistic theories. My thesis focuses on three phenomena on the interface be-\ntween syntax and semantics, and explores these phenomena with novel experimental methods involving language\nmodels.\nIn Chapter 4, I study the phenomenon of word class ﬂexibility, where certain “ﬂexible” words may be used\nas different parts of speech: for example, in English, the words work or sleep may be used either as a noun\nor a verb. Whether all languages possess a distinction between nouns and verbs is a controversial question in\nlinguistic typology, due to competing deﬁnitions of word classes. Word classes are deﬁned by a combination of\nmorphosyntactic distribution (e.g., only nouns may follow a determiner in English), and semantic criteria (e.g.,\nnouns are associated with objects and verbs with actions; Van Lier and Rijkhoff (2013)). However, linguistic\ntheories disagree over the analysis of ﬂexible words: are they lexemes with underspeciﬁed word class, or cases\nof conversion, whereby a lexeme undergoes a derivational process into a different word class? My work presents\nevidence supporting the conversion theory, using measures of semantic variability.\nNext, in Chapter 5, I explore sentences containing syntactic, semantic, and commonsense violations. Various\nlinguistic theories have proposed differences between syntactic and semantic violations. In generative syntax,\nChomsky (1957) proposed that ungrammaticality (e.g., “furiously sleep ideas green colorless”) should be distin-\nguished from semantic anomalies (e.g., “colorless green ideas sleep furiously”): the latter being a meaningless\nbut grammatically well-formed sentence. In psycholinguistics, studies found that semantic violations triggered\nthe N400 event-related potential (ERP) in the brain whereas morphosyntactic violations triggered a different P600\nERP, although this dichotomy was abandoned after further evidence (Kutas et al., 2006). Inspired by this psy-\ncholinguistic work, I probe language models for whether they exhibit any differences in internal processing in\nresponse to varying types of anomalies.\nFinally, in Chapter 6, I adopt a construction grammar framework to probe language models. Construction\ngrammar proposes that all linguistic knowledge is encoded in constructions, which map between form and mean-\ning, and there is no separation between lexical and grammatical knowledge. Argument structure constructions\n(ASCs) theorize that certain syntactic patterns are associated with semantic meaning independently of the main\nverb (Goldberg, 1995). For example, the ditransitive construction (e.g., “Bob cut Joe the bread”) is associated\nwith the transfer of the indirect object to the direct object recipient, no matter which verb is used. In contrast,\nlexicalist theories assume that the main verb is responsible for assigning semantic roles to each of its syntactic\nCHAPTER 1. INTRODUCTION\n5\narguments. In psycholinguistics, sentence sorting and priming studies supported the theory of ASCs in humans; I\nadapt several of these studies to show evidence for ASCs in language models as well.\n1.4\nStructure of thesis\nThe rest of my thesis is structured as follows.\n• Chapter 2 gives a survey of modern neural language models, including static word embeddings based on the\ndistributional hypothesis, RNN and LSTM sequence models, and Transformer-based LMs such as BERT\nand GPT. I also discuss the beneﬁts and tradeoffs of some common diagnostic classiﬁer schemes used for\nprobing LMs.\n• Chapter 3 surveys the recent literature connecting linguistic theories to LMs, including behavioural probes\nof syntactic structure and probes targeting the internal representations of LMs. Next, I discuss neural\nnetwork probing methods adapted from psycholinguistics, and applications of LMs to linguistic theory.\n• Chapter 4 presents my cross-lingual study on word class ﬂexibility: my method leverages contextual em-\nbeddings from LMs as a source of automated semantic distance judgments, and I ﬁnd evidence supporting\nthe theoretical view of word class ﬂexibility as a directional process.\n• Chapter 5 presents my work on probing for linguistic anomalies within intermediate layers of LMs. Inspired\nby neurolinguistic event-related potential (ERP) studies, I propose an anomaly detection method based on\nGaussian models and ﬁnd that different internal activation patterns are triggered in response to different\ntypes of linguistic anomalies.\n• Chapter 6 presents my work on probing LMs for construction grammar: speciﬁcally, a family of construc-\ntions known as argument structure constructions. I adapt several human psycholinguistic studies to be\nsuitable for LMs, and show that LMs exhibit knowledge of argument structure constructions similarly to\nhumans.\n• Chapter 7 concludes my thesis by summarizing my contributions and highlighting some areas for future\nimprovement.\n1.5\nRelationship to published work\nSeveral chapters of this thesis have previously appeared in peer-reviewed publications:\nCHAPTER 1. INTRODUCTION\n6\n• Chapter 4. Bai Li, Guillaume Thomas, Yang Xu, and Frank Rudzicz. “Word class ﬂexibility: A deep\ncontextualized approach”. Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP 2020).\n• Chapter 5. Bai Li, Zining Zhu, Guillaume Thomas, Yang Xu, and Frank Rudzicz. “How is BERT sur-\nprised? Layerwise detection of linguistic anomalies”. Proceedings of the Joint Conference of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-\nence on Natural Language Processing (ACL-IJCNLP 2021).\n• Chapter 6. Bai Li, Zining Zhu, Guillaume Thomas, Frank Rudzicz, and Yang Xu. “Neural reality of\nargument structure constructions”. Proceedings of the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL 2022).\nAdditionally, the following peer-reviewed publications are not included in this thesis but were published dur-\ning my doctorate:\n• Bai Li, Nanyi Jiang, Joey Sham, Henry Shi, and Hussein Fazal. “Real-world Conversational AI for Hotel\nBookings”. IEEE Annual Conference on Artiﬁcial Intelligence for Industries (AI4I 2019).\n• Bai Li, Jing Yi Xie, and Frank Rudzicz. “Representation Learning for Discovering Phonemic Tone Con-\ntours”. 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Mor-\nphology (SIGMORPHON at ACL 2020).\n• Bai Li and Frank Rudzicz. “TorontoCL at CMCL 2021 Shared Task: RoBERTa with Multi-Stage Fine-\nTuning for Eye-Tracking Prediction”. Proceedings of the Workshop on Cognitive Modeling and Computa-\ntional Linguistics (CMCL at NAACL 2021).\n• Zining Zhu, Jixuan Wang, Bai Li, and Frank Rudzicz. “On the data requirements of probing”. Findings of\nthe Association for Computational Linguistics: ACL 2022.\nChapter 2\nModern neural language models\n2.1\nIntroduction\nModern natural language processing has achieved much of its success by leveraging a relatively small number\nof foundational models that serve as building blocks for many diverse applications. These models are trained\non vast amounts of unstructured language data, allowing them to learn general-purpose representations of human\nlanguage and transfer this knowledge towards downstream tasks. The architectures of these models have evolved\nover the years with research progress: in Section 2.2, I begin with static word vector models based on distributional\nsemantics, which are typically used in the input layer in sequential RNN and LSTM neural networks (Section 2.3).\nNext, in Section 2.4, I cover contextualized language models such as BERT, which use a two-step procedure in\nwhich the model is pre-trained on large amounts of text, then ﬁne-tuned to perform speciﬁc tasks. Finally, in\nSection 2.5, I describe some methods to probe the internal representations of large language models and the\ndifﬁculties and shortcomings of these probing methodologies.\n2.2\nDistributional semantics and word embeddings\nWord embeddings (or vectors) are one of the most basic building blocks of natural language processing, encoding\nthe meaning of a word into a ﬁxed-dimensional vector of real numbers. These vectors are learned using large\ncorpus data, and individual dimensions of these vectors do not contain any meaning; the meaning of words is\ncaptured by their geometric relationship to other word vectors. Words that are related in meaning are close together\nin vector space (measured by Euclidean or cosine distance), while words that are unrelated are farther apart (Figure\n2.1). Sometimes, word vectors exhibit vector arithmetic properties: for instance, the vector difference between\nCanada and Ottawa is close to the vector difference of China and Beijing because both word pairs exhibit the\n7\nCHAPTER 2. MODERN NEURAL LANGUAGE MODELS\n8\nFigure 2.1: Visualizing distributional semantic models. Each individual dimension is semantically meaningless,\nbut similar words are closer together in the vector space, as measured by Euclidean or cosine distance. Figure\nadapted from Boleda (2020).\ncountry-capital relationship.\nThe theoretical basis for word embeddings is from distributional semantics. Harris (1954) proposed the dis-\ntributional hypothesis: words which occur in similar contexts have similar meaning, and conversely, a word’s\nmeaning can be deﬁned by its contextual distribution. For example, given the sentence “The\nstayed up all\nnight to ﬁnish her paper”, two likely completions for the blank are student and postdoc, so these two words have\nsimilar meaning according to distributional semantics.\nThere are many word embedding models based on the distributional hypothesis; one of the most popular and\nﬁrst neural approaches was word2vec (Mikolov et al., 2013). Word2vec used a shallow neural network to either\npredict a word given its context (the continuous bag-of-words model), or predict the context of a word (the skip-\ngram model). In the continuous bag-of-words variant, the neural network is given a one-hot encoding of words\nin a context window surrounding it, and uses a classiﬁcation layer to predict the middle word. After training, the\nword embeddings are derived from the hidden layer of this neural network.\nA popular alternative method to learn word vectors was GLoVE (Pennington et al., 2014). Rather than pre-\ndicting word identities from context, GLoVE learns vectors that reconstruct a word-to-word co-occurrence matrix,\nwhere each entry in the matrix is the count of how many times that pair of words co-occurred in the same context\nwindow in a corpus. Practically, word2vec and GLoVE produce word vectors that behave similarly, and neither\none consistently outperforms the other.\nBoth word2vec and GLoVE have a weakness in that they treat each word as an atomic unit, so the internal\nstructure of words is not utilized, and they cannot handle out-of-vocabulary words. Fasttext (Bojanowski et al.,\n2017) proposed to enrich the word2vec skip-gram method with sub-word information: in addition to learning a\nvector for each word, Fasttext also learns vectors for character n-grams so that an out-of-vocabulary word can be\nCHAPTER 2. MODERN NEURAL LANGUAGE MODELS\n9\nFigure 2.2: A recurrent neural network (RNN) for language modelling. At each step, the RNN predicts the next\nword in the sequence, given a hidden state that is derived from the previous words. The inputs may be one-hot\nencodings of the words, or static word embeddings such as word2vec or GLoVE.\nrepresented by the sum of its character n-gram vectors.\nWord embeddings have some weaknesses that limit their utilities in some situations. First, since they always\ngenerate a static vector for each word, they cannot capture homographs which have the same form but multiple\nsenses. For example, the noun and verb senses of the word bear are completely unrelated, but both are assigned\nthe same word embedding. The result is a less-than-ideal representation for both senses, since the optimization\nprocedure produces an embedding that lies somewhere in between the two senses in an attempt to capture both\nsimultaneously. In the next section, I discuss contextual embeddings which generate better representations of\npolysemous words. Another weakness is that words that have similar vectors do not always have similar meanings:\none can only conclude that they occur in similar distributional contexts. For example, the sentence “The test was\nvery\n”, both easy and difﬁcult are reasonable completions, despite having opposite meanings. Thus, it is\ngenerally difﬁcult to distinguish synonyms from antonyms and hypernyms using vector space distance (Lenci,\n2018).\n2.3\nSequential models and contextual embeddings\nWord embeddings provide representations for individual words, but they are unable to represent word order in\nlonger texts such as sentences or paragraphs. In order to work with text, sequential models such as recurrent\nneural networks (RNNs; Elman (1990)) are needed. An RNN is a type of neural network that reads sequential\ninput one token at a time; the input may consist of one-hot encodings of words or lower-dimensional static word\nembeddings. The RNN keeps track of a hidden state representing the input read thus far and updates it after\nreading each token. The ﬁnal state may be used as a representation for the entire sequence.\nA popular variant of the RNN is long short-term memory networks (LSTMs; Hochreiter and Schmidhuber\n(1997)). Plain RNNs have difﬁculty processing long-range dependencies in text since information in the hidden\nlayer must be retained across many steps. In contrast, LSTMs use a memory cell in addition to the hidden state\nand a system of input, output, and forget gates to modify the memory cell, allowing the model to more easily retain\nlong-term information. Sometimes, it is useful to read input in both directions: a bidirectional LSTM (bi-LSTM)\nCHAPTER 2. MODERN NEURAL LANGUAGE MODELS\n10\nconsists of a forward and backward LSTM whose outputs are concatenated together.\nRNNs and LSTMs are used in several common setups for supervised prediction. For sequence classiﬁcation,\nsuch as predicting the topic of a sentence, the last hidden representation may be fed into a discrete classiﬁcation\nlayer. When the task requires per-token classiﬁcation, such as part-of-speech tagging, the hidden representation\nat each token can be fed into a classiﬁcation layer. When the output is a sequence, such as machine translation,\nthe last hidden representation may be fed into a decoder RNN or LSTM, which produces a sequential output.\nA common unsupervised use case for sequential models is language modelling. In language modelling, the\nmodel is given a sequence of words and predicts which word is likely to come next; after training on a corpus\nof unlabelled text, the model can then be used to generate similar sequences. Language models also assign a\nprobability to any given sequence: the perplexity is deﬁned as the negative log likelihood of this probability:\np(S) = −\n|S|\nX\ni=0\nlog p(wi|w1, · · · , wi−1),\nwhere each wi is a word in the sentence S. A related concept, surprisal, is the negative log likelihood of a single\ntoken given previous context: −log p(wi|w1, · · · , wi−1). Perplexity is used as an unsupervised evaluation metric\nmeasuring how well a language model ﬁts a corpus; it can also be used to rank sentences for relative acceptability\naccording to the language model. A fair comparison using perplexity requires that the models must have the same\nvocabulary and the sentences must be the same length.\nWhen trained for language modelling, neural models learn hidden representations that capture general proper-\nties of language that are useful for many tasks; we call these language models (LMs). This approach beneﬁts from\nleveraging large amounts of unlabelled data, which is available in much larger quantities compared to labelled data\nrequired for supervised learning. Once trained, transfer learning can be applied to ﬁne-tune the LM to speciﬁc\ntasks. Universal Language Model Fine-tuning (ULMFiT; Howard and Ruder (2018)) was the earliest model to\nuse the transfer learning paradigm that is now dominant in natural language processing. ULMFiT ﬁrst trained\nan LSTM model on a large corpus (the pre-training step), followed by a small amount of additional training on\ntask-speciﬁc data (the ﬁne-tuning step); the model used a slanted triangular learning rate schedule and gradual\nunfreezing of layers to prevent forgetting pre-trained information during the ﬁne-tuning stage.\nAnother transfer method using unsupervised learning is Embeddings from Language Models (ELMo). ELMo\ntrained a multilayer bidirectional LSTM on a corpus, then uses the hidden representation of the forward and\nbackward LSTMs concatenated together as contextual embeddings. That is, rather than mapping each word to a\nstatic word vector, ELMo generates a sequence of contextual vectors for a sequence of tokens; these contextual\nvectors can then be substituted for any model that assumes a sequence of word vectors as input. By incorporating\ninformation from surrounding words, contextual vectors are able to generate different representations for different\nCHAPTER 2. MODERN NEURAL LANGUAGE MODELS\n11\nFigure 2.3: The BERT model (Devlin et al., 2019), consisting of 12 layers of Transformer modules (Vaswani\net al., 2017). The model is ﬁrst pre-trained on masked language modelling and next sentence prediction, then\nﬁne-tuned for downstream tasks. BERT may take either a single sentence as input, or two sentences separated by\na [SEP] token.\nsenses of a word like bear, overcoming the word sense ambiguity problem in static word vectors.\n2.4\nTransformer-based language models\nThe next major architectural innovation after LSTMs was the Transformer module (Vaswani et al., 2017). Con-\ntrary to sequential models which process tokens one at a time, Transformers use the self-attention mechanism to\nprocess the entire sequence simultaneously; each token is associated with a positional encoding to retain word\norder information. The advantage of the Transformer architecture over RNNs and LSTMs is that it avoids prob-\nlems with long-range dependencies where information needs to be carried forward across many steps: with self-\nattention, the maximum path length is constant (equal to the number of layers in the model), and does not depend\non the distance between the input words. The Transformer module was originally employed for machine trans-\nlation, using a stack of 6 encoder and 6 decoder Transformer layers; it subsequently replaced recurrent layers in\nmany neural models.\nTransformers were soon applied to unsupervised language modelling. OpenAI proposed the Generative Pre-\ntrained Transformer (GPT; Radford et al. (2018)): similar to ULMFiT, GPT was pre-trained on the language\nmodelling objective and then ﬁne-tuned to perform speciﬁc tasks, but used Transformers instead of LSTMs.\nBidirectional Encoder Representations from Transformers (BERT; Devlin et al. (2019)) incorporated bidirectional\ncontext into language modelling: whereas GPT predicted the next word from previous context, BERT proposed\nthe masked language modelling (MLM) objective, where tokens were randomly replaced with [MASK] tokens\nwhich the model was trained to predict. BERT’s pre-training procedure consisted of MLM as well as next-\nCHAPTER 2. MODERN NEURAL LANGUAGE MODELS\n12\nsentence prediction (NSP), where the model predicts whether two sentences are consecutive in the original text or\ntwo randomly chosen sentences. After pre-training, BERT is then ﬁne-tuned to perform classiﬁcation, sequence\ntagging, or sentence pair classiﬁcation by adding a linear layer on top of the last layer and training for a small\nnumber of steps on task-speciﬁc data (Figure 2.3). When released, BERT immediately broke new records on\nmany natural language benchmarks, leading to many efforts to improve upon it, study its internals, and apply it to\ndownstream tasks.\nBERT can be used to generate contextual embeddings and for language modelling. At each layer, BERT gen-\nerates hidden representations at each token that can be used as contextual representations: typically, the last or\nsecond-to-last layer is used. While BERT is not naturally suitable for language modelling because it assumes bidi-\nrectional context is available, a substitute for perplexity was proposed by Salazar et al. (2020), where each token\nis replaced one at a time with the [MASK] token, BERT computes the log-likelihood score for each masked to-\nken, and the sum of log-likelihood scores (called the pseudo-perplexity) may be used for evaluation or to compare\nsentences for relative acceptability. However, pseudo-perplexity cannot be directly compared to normal perplexity\nscores from forward language models like LSTMs and GPT. Both forward and bidirectional models have their\nmerits: bidirectional models generate better embeddings as they have access to context in both directions, but\nforward models are useful for conditionally generating text given an initial prompt.\nAfter the release of BERT, many models have improved its performance by modifying its architecture, data,\nand training procedure. RoBERTa (Liu et al., 2019b) used the same architecture as BERT, but obtained superior\nresults by removing the NSP task and training on a larger dataset. XLNet (Yang et al., 2019) proposed permuta-\ntion language modelling, where a random permutation of the sentence is generated and the model learns to predict\nthe next word in the permutation, given previous words and their positional encodings; this avoids the pretrain-\nﬁnetune discrepancy in BERT where the [MASK] token is seen only in pre-training and not during ﬁne-tuning.\nALBERT (Lan et al., 2020) proposed a factorized embedding representation so that models with larger hidden\nlayers can be trained using the same amount of memory, and replaced the NSP task with sentence-order predic-\ntion, where the model predicts the order of two sentences that were originally consecutive. ELECTRA (Clark\net al., 2020) proposed the replaced token detection pre-training task to improve efﬁciency over MLM: instead of\npredicting masked tokens, the model is given corrupted text and predicts which tokens were original and which\nones were replacements, using a smaller network to generate replacement tokens.\nTransformer language models have been trained for other languages as well, often in a massively multilingual\nsetting so that a single model is able to process text in many languages. Multilingual BERT (mBERT) was released\nby the authors of BERT, using the same architecture and trained on Wikipedia text. XLM (Conneau and Lample,\n2019) added a translation language modelling task to mBERT, where the model predicts a masked token from\ncontext and a translation of the sentence into another language: this allows parallel corpora to be leveraged for\nCHAPTER 2. MODERN NEURAL LANGUAGE MODELS\n13\nFigure 2.4: Illustration of the edge probing method proposed by Tenney et al. (2019a). Here, the probe is given\na sentence “Last week New York City had its worst ...” and a span “New York City” and the probing task is to\nclassify the type of named entity represented by the span (Location).\npre-training. XLM-RoBERTa (XLM-R; (Conneau et al., 2020)) trained XLM on a larger dataset and obtained\nresults competitive with or surpassing the best monolingual models in each language.\n2.5\nProbing classiﬁers and their shortcomings\nThe success of BERT on natural language tasks quickly led many researchers to investigate what information\nis contained within BERT, and how the information is spread across its 12 layers. This task is nontrivial, as\nTransformer models internally consist of millions of neurons that are not easily interpretable. A popular approach\nis by using probing classiﬁers (or probes): a classiﬁer that takes in BERT embeddings as input and is trained on\nsome target task; the performance on this task is taken as a measure of how much information about the task is\ncontained in the embeddings. The purpose of this probe is not to perform well on the task (since ﬁne-tuning BERT\nwould usually result in better performance), but to measure the extent that the embeddings contain information\nrelevant to the task, without any ﬁne-tuning.\nTenney et al. (2019a) introduced the edge probing method to determine how much more information is con-\ntained in contextual embeddings compared to static baselines. The edge probing setup assumes that the input\nconsists of a sentence and up to two spans of consecutive tokens within the sentence, and the output consists of a\nsingle label. This formulation is applicable to part-of-speech tagging, dependency arc labelling, and coreference\nCHAPTER 2. MODERN NEURAL LANGUAGE MODELS\n14\nresolution, among others.\nThe probing model (Figure 2.4) ﬁrst uses BERT to generate contextual vectors for each token. Then, the\nmix step learns a task-speciﬁc linear combination of the layers; the projection and self-attention pooling produce\na ﬁxed-length span representation, and ﬁnally, a feedforward layer outputs the classiﬁcation label. The probe\nweights are initialized randomly and trained using gradient descent while the BERT weights are kept frozen. Using\nthis method, Tenney et al. (2019a) found the biggest advantage of contextual over static embeddings occurred in\nsyntactic tasks. In a follow-up work, Tenney et al. (2019b) inspected the layerwise weights learned in the mix step\nand found that semantic tasks learned higher weights on the upper layers of BERT compared to the syntactic tasks,\nsuggesting that the upper layers contained more semantic information. I will discuss the linguistic implications of\nthis experiment and other layerwise probing work in Section 3.3.\nOne criticism of probing classiﬁers (especially complex ones such as edge probing) is that high probe per-\nformance could either mean that the representation is rich in information, or that the probe itself is powerful and\nlearning the task. Hewitt and Liang (2019) proposed to use control tasks: randomized versions of the probing task\nconstructed so that high performance is only possible if the probe itself learns the task. They deﬁned selectivity as\nthe difference between the real and control task performance; a good probe should have high selectivity; In their\nexperiments, the simple linear probes had the highest selectivity. Alternatively, Voita and Titov (2020) proposed a\nminimum description length (MDL) probe based on information theory that simultaneously quantiﬁes the probing\naccuracy and probe model accuracy. Their MDL probe obtained similar results as Hewitt and Liang (2019) but\navoided some instability difﬁculties of the standard probe training procedure. Sufﬁce it to say that there is still no\nagreement on the best method to probe the quality of LM representations.\nMany papers have been published in the last few years that probe various aspects of how BERT and other\nTransformer models work. This subﬁeld, commonly known as BERTology, explores questions such as what\nknowledge is contained in which neurons, information present at each layer, the role of architectural choices\nand pre-training regimes, etc. A detailed survey of BERTology is beyond the scope of this thesis; Rogers et al.\n(2021) is a recent and comprehensive survey of the ﬁeld. In the next chapter, I will provide a narrower survey\nof BERTology work that involves linguistic theory, such as constructing test suites of syntactic phenomena and\nprobing based on psycholinguistics.\nChapter 3\nProbing for linguistic knowledge in LMs\n3.1\nIntroduction\nIn this chapter, I survey the recent literature on probing language models for various aspects of linguistic knowl-\nedge. Many papers have been recently published that try to align neural networks with linguistic theory in some\nway; although each paper’s speciﬁc methodology is different, probing methods can generally be categorized into\ntwo types: behavioural probes and representational probes. I will discuss behavioural probes in Section 3.2 and\nrepresentational probes in Section 3.3. To maintain a reasonable scope, this chapter will mostly cover probing\nwork involving transformer models (i.e., BERT and later models).\nBehavioural probes apply a black-box approach to probing: they assume little about the internals of the model,\nand carefully construct input sentences so that the way that the model responds to the input reveals information\nabout its internal capabilities and biases. This approach is relatively robust and likely to remain relevant in spite\nof future advances in language modelling research because it makes no assumptions about model architecture. In\ncontrast, representational probes assume full access to the model’s internals (for example, its trained parameters,\nits contextual vectors, and attention weights when fed an input sentence). Because neural representations consist\nof millions of real numbers, sometimes complex machinery is required to make these results interpretable for\nhumans, and one challenge is choosing which tools are most appropriate. Unlike in human psycholinguistics, we\ncan easily access every state in a neural model and at any point during its processing, making representational\nprobing a powerful and ﬂexible methodology.\nNext, in Section 3.4, I will survey LM probing research based on psycholinguistics. The ﬁeld of psycholin-\nguistics has developed many methods for indirectly exposing language processing facilities in the human brain:\ncommon experimental paradigms include lexical decision tests, priming, and EEG studies. This vast literature\n15\nCHAPTER 3. PROBING FOR LINGUISTIC KNOWLEDGE IN LMS\n16\nprovides a rich starting point for investigating similar linguistic phenomena in language models. Finally, in Sec-\ntion 3.5, I discuss some recent efforts using language models to provide evidence for linguistic theories: while\nneural networks have not yet made a signiﬁcant impact in theoretical linguistics, we are beginning to see initial\nprogress in this direction as well.\n3.2\nBehavioural probes for syntax\n3.2.1\nAgreement\nMuch of the initial work in behavioural probing investigated agreement between the subject and the verb. In\nEnglish, the subject and verb of a sentence must agree on number and person, for example:\n(1)\na.\nThe boy likes music.\nb. *The car are yellow.\nc.\nThose narwhals come from Tuktoyaktuk.\nThere are several ways this can be operationalized in a behavioural probe. The ﬁrst possibility is treating it as a\nbinary classiﬁcation task of classifying whether a sentence is acceptable or unacceptable. This can be done in a\nsupervised setting (i.e., training on one set of acceptable and unacceptable sentences and evaluating on a different\nset), or an unsupervised setting (i.e., picking a threshold such that sentences with language model probability\nabove the threshold are considered acceptable). However, the binary classiﬁcation setup has the drawback that it\ndoes not control for the length and contents of the sentence, so that an acceptable sentence (1-c) containing rare\nwords may have a lower probability than an unacceptable sentence (1-b) containing common words (Lau et al.,\n2017).\nA different approach that resolves this issue by framing the task as a forced choice between two minimal pairs:\nsentences that only differ on the critical region of interest, and are identical in all other aspects, for example:\n(2)\na.\nThe car (is/*are) yellow.\nb.\nThose narwhals (*comes/come) from Tuktoyaktuk.\nThis presents a natural setup for language models supporting masked word prediction such as BERT and RoBERTa:\none can feed into the model the sentence The boy [MASK] music, obtaining a probability distribution for the\nmasked token, and consider the model correct if the model’s probability for likes is higher than for like. Other-\nwise, for forward sequential models like LSTMs and GPT, the input stimulus is usually modiﬁed so that only the\npreﬁx before the masked token is required to predict the correct completion. For example, (1-a) would be trun-\nCHAPTER 3. PROBING FOR LINGUISTIC KNOWLEDGE IN LMS\n17\ncated to The boy ...; the model reads this input and generates a prediction for the subsequent token, and is judged\nas correct if the probability for likes is higher than for like. Construction of input stimuli for forward sequential\nmodels is therefore more restrictive than for masked language models, since the critical token must be the ﬁnal\ntoken in the sentence.\nLinzen et al. (2016) tested LSTM models on number agreement between subject and verb, using natural\nsentences from Wikipedia. In addition to simple sentences, they examined more complex sentences containing\nattractors – nouns between the subject and verb with opposite number from the subject. Such sentences are\nrelatively uncommon but may occur if the subject is modiﬁed by a relative clause or prepositional phrase, for\nexample, The car behind those trees (is/*are) yellow. These are difﬁcult cases because the simple heuristic of\nagreeing with the most recent noun does not work; humans are also known to sometimes produce the incorrect\ninﬂection in the presence of attractors (Bock and Miller, 1991). Linzen et al. (2016) found that the supervised\nmodels had higher error rates on sentences with attractors (but still better than random), while the unsupervised\nmodels were unable to predict agreement better than random when attractors are present.\nGulordava et al. (2018) investigated the capabilities of LSTMs on long-distance agreement in nonsensical\nsentences, where all lexical items are replaced with random nonce words that match in part-of-speech and mor-\nphological features. The purpose of this experiment was to remove the possibility that the models rely on lexical\nand semantic cues to predict agreement. The authors generated nonce sentences by perturbing UD treebank\nsentences in 4 languages: English, Italian, Hebrew, and Russian; they found that in all languages, the LSTM\nperformed worse on the nonce than the original sentences, but better than random. They concluded that the model\ncould not have achieved its results solely through pattern-matching on surface structure, and instead it must have\nlearned some form of deeper linguistic structure.\n3.2.2\nOther syntactic phenomena\nAgreement is a relatively simple linguistic phenomenon, well suited for an initial case study of behavioural prob-\ning, and after its initial success, researchers soon expanded behavioural probing to other phenomena. Standard\nsyntax textbooks (e.g., Carnie (2013)) describe a wide assortment of phenomena that may be adapted into bench-\nmark tests for language models.\nOne of the most direct approaches was the Corpus of Linguistic Acceptability (CoLA; Warstadt et al. (2019)).\nCoLA collected 10,657 sentences from various linguistic publications including textbooks and dissertations. Each\nsentence was labelled in the original publications as either acceptable or unacceptable. Models were allowed a\ntrain-test split, where the training set and test set were drawn from different linguistic publications; evaluation\nwas by Matthews correlation coefﬁcient with the ground truth. CoLa has since been incorporated into the GLUE\nCHAPTER 3. PROBING FOR LINGUISTIC KNOWLEDGE IN LMS\n18\nPhenomenon\nAcceptable Example\nUnacceptable Example\nAnaphor agreement\nMany girls insulted themselves.\nMany girls insulted herself.\nArgument structure\nRose wasn’t disturbing Mark.\nRose wasn’t boasting Mark.\nBinding\nCarlos said that Lori helped him.\nCarlos said that Lori helped himself.\nControl/raising\nThere was bound to be a ﬁsh escaping.\nThere was unable to be a ﬁsh escaping.\nDeterminer-noun agr.\nRachelle had bought that chair.\nRachelle had bought that chairs.\nEllipsis\nAnne’s doctor cleans one important\nbook and Stacey cleans a few.\nAnne’s doctor cleans one book and\nStacey cleans a few important.\nFiller-gap\nBrett knew what many waiters ﬁnd.\nBrett knew that many waiters ﬁnd.\nIrregular forms\nAaron broke the unicycle.\nAaron broken the unicycle.\nIsland effects\nWhich bikes is John ﬁxing?\nWhich is John ﬁxing bikes?\nNPI licensing\nThe truck has clearly tipped over.\nThe truck has ever tipped over.\nQuantiﬁers\nNo boy knew fewer than six guys.\nNo boy knew at most six guys.\nSubject-verb agr.\nThese casseroles disgust Kayla.\nThese casseroles disgusts Kayla.\nTable 3.1: Example acceptable and unacceptable sentences for the 12 types of linguistic phenomena in BLiMP\n(Warstadt et al., 2020a).\ntext classiﬁcation benchmark (Wang et al., 2019b).\nAlthough CoLA contains a wide variety of phenomena, which is desirable for a general-purpose benchmark,\nits heterogeneity of source material is a hindrance for analysis of speciﬁc phenomena. The authors annotated the\ncorpus with the presence or absence of 15 broad classes of phenomena and 63 ﬁne-grained phenomena, but this\nstill resulted in a loosely-related set of sentences within each phenomenon; they instead found template-based\ngeneration to be better suited for phenomenon-speciﬁc analysis.\nMarvin and Linzen (2018) introduced the technique of generating syntactic stimuli using templates: they used\na recursive context-free grammar to generate random sentences of different linguistic structures. The advantage\nof this method is that it allows precise control over the structure of each sentence and how many of each type to\ngenerate (since certain structures rarely appear in natural corpora). Furthermore, template generation avoids lexi-\ncal or any other potential confounds, similar to Gulordava et al. (2018). The authors generated agreement samples\ncontaining various types of complements and relative clauses, and additionally examine LSTM performance on\nreﬂexive anaphora and negative polarity items (NPIs).\nThe Benchmark of Linguistic Minimal Pairs (BLiMP; Warstadt et al. (2020a)) extended the template gen-\neration method to 12 different syntactic phenomena and 67 different paradigms (i.e., sub-phenomena). These\nphenomena include agreement, argument structure, ﬁller-gaps, and NPI licensing (Table 3.1 gives an example\nfor each of the phenomena). They generated 1000 sentences for each paradigm using templates and a lexicon\ncontaining about 3,000 items, annotated with various grammatical features to ensure the validity of the generated\nsentences. They obtained human forced choice judgments from MTurk for quality assurance and to establish a\nhuman baseline, and evaluated several forward sequential models (n-gram, LSTM, Transformer-XL, and GPT-2),\nin an unsupervised setting.\nCHAPTER 3. PROBING FOR LINGUISTIC KNOWLEDGE IN LMS\n19\nHu et al. (2020) constructed a similar syntactic test suite of 34 linguistic phenomena, aiming to cover an\nintroductory syntax textbook (Carnie, 2013). However, rather than using binary minimal pairs as in BLiMP, Hu\net al. (2020) deﬁned success criteria differently depending on the task, where the model is considered correct if\nits perplexity on the sentences simultaneously satisﬁes several pre-deﬁned inequalities. This design allows more\ncontrol over lexical confounds and task instances involving more than two sentences (for example, a 2x2 design\nis used for subject-verb agreement). They manipulated model architectures and the amount of training data,\nand found that the models’ performance on their test suite is not always correlated with perplexity (a common\nevaluation metric for LM performance).\n3.2.3\nGradience of acceptability\nMost studies on neural linguistic acceptability so far have adopted a binary scale for acceptability: each sentence\nis assumed to be either acceptable or not; the model’s response is either correct or incorrect. The ﬁnal reported\nmetric is either accuracy or Matthews correlation coefﬁcient (MCC), both of which require a discrete threshold\nand do not allow a gradient response. Niu and Penn (2020) criticized the use of accuracy and MCC for linguistic\nacceptability benchmarks because they ignore the magnitude of LM probabilities, thus obscuring the differences\nbetween the models that are meant to be compared.\nLinguistic theories disagree about whether grammars should assign a binary of gradient value to sentence well-\nformedness. Traditional generative grammar theories (Chomsky, 1957, 1965) assume sentence grammaticality is\na binary property; any disagreement of native speakers is due to performance factors such as processing difﬁculty,\nwhile only competence (the stable grammatical knowledge that speakers possess) is relevant to linguistic theory.\nLau et al. (2017) provided empirical evidence of acceptability as a gradient phenomenon: they obtained a set\nof corrupted sentences by feeding English sentences through a round-trip machine translation procedure into\nanother language and back. Then they obtained acceptability judgments on these sentences from MTurk and\nfound that the distribution of ratings more closely resembled ratings of a continuous variable (body weight) than a\ndiscrete variable (integer parity). Then, they evaluated several language models on this dataset by computing the\ncorrelation between LM probabilities and human acceptability judgments, and found that several of the models\ncorrelated nearly as much with humans as human correlations with each other.\nWilcox et al. (2021b) investigated the magnitude of surprisals when an LM encounters an ungrammatical\nsection in a sentence. They constructed a test suite of syntactic minimal pairs similar to BLiMP, and collected\nhuman reaction data using the Interpolated Maze paradigm. In this maze task, human participants were presented\nwith a sentence one word at a time, and had to choose between the correct word and a distractor at each step; the\nresponse time captures the processing difﬁculty and is expected to be higher for ungrammatical sections. They\nCHAPTER 3. PROBING FOR LINGUISTIC KNOWLEDGE IN LMS\n20\ncompared human response times to surprisals for several LMs, and found that the models consistently under-\npredicted the magnitude of human processing difﬁculty. Therefore, they concluded that LMs are less sensitive to\nsyntactic violations than humans.\n3.3\nRepresentational probes of LM embeddings\n3.3.1\nLayerwise probing\nTransformer language models have a relatively homogeneous architecture, generating a ﬁxed-dimension vector\nrepresentation at each layer for each input token. Depending on the use case, these vectors can be probed directly,\nor they can be collapsed into sentence vectors before probing by taking an average of the vectors for each token.\nThe layerwise architecture makes it straightforward to probe for what linguistic information is contained in the\nembeddings at each layer.\nTenney et al. (2019b) applied the edge probing method (Section 2.5) on BERT embeddings on a variety of\ntasks, and found that the probe learned a higher weighting for the upper layers when the task was more semantic\n(e.g., semantic proto-roles and relation classiﬁcation), while the middle layers were preferred for syntactic tasks\n(e.g., part-of-speech tagging and dependency arc labelling). This suggested that the upper layers derived complex\nsemantic representations from simpler syntactic representations in the lower layers, similar to the stages in a\ntraditional NLP pipeline. Jawahar et al. (2019) applied the SentEval toolkit (Conneau and Kiela, 2018) to BERT\nembeddings, with a similar result: the lower layers were the best at capturing surface features, the middle layers\ncontained the most syntactic information, and the upper layers contained the most semantics.\nKelly et al. (2020) probed BERT as well as some static word vector models for extractability of syntactic\nconstruction from a sentence embedding. The task was to determine which of two constructions were used in the\ninput sentence, for example, a ditransitive dative or a prepositional dative. They measured spatial separability in\nthe embedding space as well as probing classiﬁer accuracy, and found that the middle layers of BERT were the\nmost sensitive to the type of syntactic construction present in the sentence.\nYu and Ettinger (2020) assessed several transformer LMs for their ability to represent compositional phrases.\nIn their setup, phrasal representations were probed for paraphrase similarity: whether two short phrases had\nsimilar meaning or not. In order to control for lexical overlap, they experimented with AB-BA pairs, where the\nmodel must determine the similarity of a two-word phrase and its reversal (e.g., law school has low similarity\nwith school law, whereas adult female has a similar meaning as female adult). Despite trying several different\nmodels and methods for generating phrase representations, they obtained poor results on the paraphrase similarity\ntask when lexical overlap was controlled, indicating that the LMs have strong sensitivity to word content but not\nCHAPTER 3. PROBING FOR LINGUISTIC KNOWLEDGE IN LMS\n21\nto nuanced composition.\nMiaschi et al. (2020) created a suite of probes for a wide range of linguistic features ranging from surface to\nlexical and syntactic, and probed layers of BERT for the ability to predict these linguistic features. They found\nthat most features had the best performance in one of the middle layers; performance decreased in the upper\nlayers, and dropped drastically in the last layer. They also experimented with ﬁne-tuning: all linguistic features\nperformed worse after the model was ﬁne-tuned, especially the upper layers, agreeing with earlier results by Liu\net al. (2019a) that the upper layers become more specialized towards the speciﬁc task when ﬁne-tuned.\nOverall, the various layerwise probing experiments reach a similar conclusion about how linguistic informa-\ntion is propagated through the layers of transformer LMs. The initial layer is a stream of non-contextual token\nembeddings, so only word-level and surface features are available. The lower, middle, and upper layers gradually\nextract morphosyntactic and semantic features that serve as a good general-purpose representation of language\nand are useful to many different downstream tasks. Finally, the last layer aggregates information from the previous\nlayers into a representation optimized for the speciﬁc task (i.e., masked token prediction in the case of pretrained\nLMs).\n3.3.2\nStructural probes for syntax\nHewitt and Manning (2019) proposed a structural probe for discovering syntax trees contained in contextual\nembeddings. Their probe measured the extent to which the depth of each token within a dependency parse tree\ncan be uncovered via a linear transformation of their embeddings. This is sufﬁcient to demonstrate the existence\nof dependency trees in the embeddings because a minimum spanning tree algorithm can be applied to extract the\nbest (undirected) dependency tree.\nSpeciﬁcally, their structural probe ﬁnds a linear transformation matrix B such that the linear distance of B\napplied to embeddings w1 and w2 approximates their distance in the parse tree:\ndB(w1, w2) = ||Bw1 −Bw2||2.\nThe linear transformation B is learned via gradient descent by minimizing the total difference to the ground truth\ntree distance across all pairs of tokens (w1, w2) from the same sentence in a corpus. The evaluation metrics used\nwere undirected unlabelled attachment score (UUAS) and the Spearman correlation of tree distances compared\nto the ground truth. They found that dependency trees could be recovered from BERT and ELMo embeddings,\nbut not from the baselines. The middle layers of BERT performed the best for extracting dependency trees, and\nthe performance increased as the rank of B approached 64 but increasing the rank beyond that did not further\nincrease the performance. The authors thus concluded that syntactic information was encoded in a fairly low-rank\nCHAPTER 3. PROBING FOR LINGUISTIC KNOWLEDGE IN LMS\n22\nsubspace in BERT’s embeddings.\nChi et al. (2020) extended the structural probe to a multilingual setting, using mBERT and data from Universal\nDependencies (UD; Zeman et al. (2019)) in 11 languages. They found that the same linear probe was able to\nreconstruct dependency tree distances in all languages, demonstrating that all languages share a common subspace\nfor syntax in mBERT. Next, they applied a t-SNE visualization on vector representations of dependency arcs, and\nfound that similar dependencies across languages were clustered together. This is surprising given that neither\nmBERT nor the structural probe had access to labelled dependency information during training.\nWhite et al. (2021) extended the structural probing method to support nonlinear probes, recasting the probe\nas kernalized metric learning. This enabled the use of common nonlinear kernels such as the radial basis function\n(RBF) kernel, which yield nonlinear probes without increasing the probe complexity. They noted that the math-\nematical structure of the RBF kernel resembled BERT’s self-attention, and hypothesized that this resemblance\nexplained why the RBF kernel outperformed the linear one.\nAlthough the dependency formalism (used by Universal Dependencies) is a popular syntactic framework in\ncomputational linguistics, it is only one among many proposed formalisms for syntax. Depending on which for-\nmalism is used in probing, we may draw different conclusions about LMs’ syntactic capabilities. Kulmizev et al.\n(2020) compared the structural probe on UD and an alternative syntactic framework, Surface-Syntactic Universal\nDependencies (SUD; (Gerdes et al., 2018)); they found that BERT and ELMo performed better with UD than SUD\nannotations in most languages. Similarly, in semantic role labelling, Kuznetsov and Gurevych (2020) found that\nthe results of probing differed depending on which formalism was used to annotate the data (PropBank, VerbNet,\nFrameNet, or Semantic Proto-Roles). Any linguistic probing work must commit to a particular formalism, thus\nprobing research depends substantially on the underlying linguistic theory and the availability of data annotated\nin these frameworks.\n3.3.3\nProbes involving LM training\nThe most common approach to linguistic LM probing has so far been on pretrained models (examining either\ntheir outputs or internal representations). From a practical perspective, these experiments have the advantage\nthat they can be performed without large amounts of data or computational resources. Nonetheless, some recent\nwork have explored using pretraining or ﬁne-tuning as a tool to gain insights about the linguistic properties of\nLMs. By incorporating LM training, these methods reveal which properties are learnable by a family of models\nor architectures, whereas static probing methods examine which capabilities have been learned by speciﬁc models\nsuch as BERT and RoBERTa.\nLanguage models are trained on extremely large amounts of data, compared to what an average human is\nCHAPTER 3. PROBING FOR LINGUISTIC KNOWLEDGE IN LMS\n23\nexposed to during language learning. For example, RoBERTa is trained on 30B tokens, while children are exposed\nto no more than 3-11M words of input a year (Hart and Risley, 2003), or somewhere on the order of 100M words\nby the time they reach puberty. Zhang et al. (2021) tested the performance of MiniBERTa models (Warstadt et al.\n(2020b); variants of RoBERTa trained on 1M to 30B words) on a variety of benchmarks, and found that only about\n10-100M words are sufﬁcient to learn most syntactic features, but commonsense knowledge and most downstream\ntasks require much more data. A similar result was reported by Liu et al. (2021), who probed RoBERTa throughout\nits pretraining process, ﬁnding that linguistic knowledge was quickly acquired while commonsense and reasoning\nwas only acquired much later. Thus, it appears that linguistic knowledge is relatively easy for LMs to acquire.\nOn the other hand, some studies have argued that LMs are still less efﬁcient than humans at acquiring syntax.\nHuebner et al. (2021) trained BabyBERTa, a smaller version of RoBERTa using child-directed corpora as training\ndata, and evaluated its syntactic capabilities on an adapted version of BLiMP where the vocabulary was replaced\nwith appropriate child-level words. BabyBERTa had poor performance on the syntactic tests relative to RoBERTa,\ndespite receiving a similar amount of input as a 6-year-old child. Van Schijndel et al. (2019) showed that most\nsyntactic tasks can be learned from limited data, but some more complex tasks fall short of human performance,\nand even training on very large datasets does not improve performance to human levels.\nAnother experimental paradigm involving training is probing LMs for inductive biases. Human languages\ntend to have syntactic rules that operate on a structural level rather than the surface level (for example, subject-\nauxiliary inversion moves the structurally highest auxiliary, not the linearly last auxiliary), and Chomsky (1981)\nhypothesized that humans have an innate structural bias that helps them learn language from limited input.\nWarstadt and Bowman (2020) proposed a poverty of stimulus design for probing, where they ﬁne-tuned BERT\nto classify grammaticality using sentences designed so that it is ambiguous whether a surface or structural rule\nis required. During test time, they used a different set of sentences to determine whether BERT has learned\na surface or structural rule; they found that BERT prefers to learn structural over surface rules when both are\nequally probable. In a subsequent study, Warstadt et al. (2020b) pretrained MiniBERTa models on between 1M to\n30B tokens to investigate the inductive biases of models of varying data size. They found that the smaller models\npreferred surface generalizations while larger models preferred structural generalizations, which may explain why\nlarger LMs are so successful at downstream tasks but only after crossing a certain data threshold.\n3.4\nAdapting psycholinguistics to LMs\nPsycholinguistics is a ﬁeld that shares many commonalities with LM probing, but seeks to understand human\nlanguage processing rather than neural models. Both ﬁelds contend with the challenges of indirectly deducing the\nmechanisms of an entity whose internals are inaccessible or difﬁcult to interpret; thus, psycholinguistics, an older\nCHAPTER 3. PROBING FOR LINGUISTIC KNOWLEDGE IN LMS\n24\nﬁeld, provides a rich source of methods and data that can be applied to LM probing. Several differences make\nLM probing a generally easier endeavour than psycholinguistics: ﬁrst, each neuron in an LM can be inspected\nin precise detail whereas human brains can only be imaged with relatively coarse-grained techniques such as\nEEG and fMRI; second, LMs can be cheaply run on large amounts of data with fully deterministic results. Still,\nmethods from psycholinguistics sometimes require substantial modiﬁcation to be suitable for LMs.\n3.4.1\nThe N400 response and surprisal\nA popular method of measuring human responses to language is using electroencephalography (EEG) to detect\nelectrical activity via electrodes placed at the scalp. Event-related potentials (ERPs) are brain responses to speciﬁc\nstimuli derived from EEG signals, and are good indicators for tracking automatic responses during language\nprocessing. A well-known ERP is the N400 response, characterized by a negative potential roughly 400ms after a\nstimulus, and is generally associated with the stimulus being semantically anomalous with respect to the preceding\ncontext. The N400 response is not speciﬁc to language (for example, it has been observed using images or\nenvironmental sounds as stimuli), nor is it produced by all linguistic anomalies (for example, morphosyntactic\nviolations do not trigger the N400); precisely what conditions trigger the N400 is still an open question (Kutas and\nFedermeier, 2011). Early psycholinguistic studies proposed that semantic anomalies produce the N400 response\nwhile syntactic anomalies produce the P600, a different type of ERP, but this dichotomy was challenged in later\nstudies (Kutas et al., 2006). Presently, the N400 is not known to be aligned with any single component in a\nlinguistic theory.\nThe N400 is correlated with cloze probability, so it is often used as an approximate estimate of probability\nin humans. Frank et al. (2015) found that the N400 was correlated with surprisals in a variety of RNN models.\nMichaelov and Bergen (2020) gathered a large set of psycholinguistic stimuli from different papers about the N400\nresponse, and ran them on RNN models, comparing the LM surprisals with human responses. They conﬁrmed\nthat LM surprisals generally predicted N400 responses, but in some cases such as in morphosyntactic or event\nstructure violations, the LM surprisals were more sensitive than the N400 in humans; thus, the N400 cannot be\nexplained by surprisal alone.\nEttinger (2020) probed BERT using stimuli from three psycholinguistic studies involving commonsense knowl-\nedge, semantic role reversals, and negation. These studies were selected because they were cases where cloze\nprobabilities were low, yet the N400 response did not trigger: in other words, they represented hard cases where\nautomatic processing mechanisms could not reveal the extent of the surprisal and deliberate judgment is required.\nEttinger found that BERT performed reasonably well on the commonsense reasoning task, was less sensitive than\nhumans to role reversal anomalies, and failed completely at understanding negation (by ﬁlling in words of the\nCHAPTER 3. PROBING FOR LINGUISTIC KNOWLEDGE IN LMS\n25\nmatching category for prompts like “A robin is not a [MASK]“).\n3.4.2\nPriming in LMs\nPriming is another popular experimental paradigm in psycholinguistics, a phenomenon where exposure to prior\nstimuli (e.g., lexical items or structures) affects responses to a later stimuli (Pickering and Ferreira, 2008). In\nstructural priming, when people are exposed to a particular syntactic structure, they are more likely to produce\nthe same structure later (Bock and Loebell, 1990), and are able to process sentences of the same structure more\nquickly. This effect is useful for understanding our cognitive mechanisms for language, since priming is evidence\nthat two sentences share a common internal representation.\nStructural and lexical priming have been explored for LM probing, but the priming methodology does not\nnaturally carry over to LMs, which do not have any concept of temporality. Misra et al. (2020) simulated lexical\npriming by prepending the prime word (either by itself or situated in a carrier sentence) before the target sentence\nin which BERT predicts a masked word. They found that BERT was facilitated by a relevant prime word only\nwhen the target sentence was unconstrained, but when the target sentence was constrained, the prime word instead\nacted as a distractor and lowered the LM’s probability for the correct word. A limitation of this method is that\nconcatenating a prime and a target sentence creates an unnatural combination that does not occur in natural text,\npossibly leading to unpredictable out-of-distribution effects.\nPrasad et al. (2019) formulated structural priming as model ﬁne-tuning: to measure a priming effect, they\nproposed to ﬁne-tune a model on a set of sentences with one syntactic structure, then measure the surprisal on\nanother set of target sentences. A priming effect is considered to exist if the surprisal for the target sentences is\nlower after ﬁne-tuning than the original LM; they found evidence of priming in LSTM models for various types\nof relative clauses.\n3.5\nUsing LMs as evidence for linguistic theory\nDuring the past few years since the rise of transformer models, there have been an abundance of papers probing\nthe linguistic capabilities of these models. Probing work owes a great deal to the linguistic theories that it is\nbased upon; however, the contribution has so far mostly been unidirectional – neural network probing papers\nhave had negligible impact on theoretical linguistics (Baroni, 2021). Experiments involving LMs have given us\nan increasingly detailed view of how they process language, but these experiments cannot offer insights about\nhow humans process language. Even when LMs exhibit similar linguistic responses to stimuli as human subjects,\nit is unclear what exactly are the implications for human linguistics, since the neural architectures share few\nsimilarities with the human brain. Yet recently, a small number of papers have proposed ways in which neural\nCHAPTER 3. PROBING FOR LINGUISTIC KNOWLEDGE IN LMS\n26\nnetworks may contribute to linguistic theory.\nVan Schijndel and Linzen (2021) considered two theories explaining the processing difﬁculty of garden path\nsentences. In the traditional two-stage theory, readers maintain a partial parse while reading a sentence and are\nforced to reanalyze the parse in a garden path situation, causing a processing delay. In the more recent single-stage\ntheory, readers maintain several parses at the same time, and there is a delay at each word from the processing\nrequired to integrate the word into all available parses. The authors used LSTMs to measure the surprisal of\neach word based on the preceding context, and found that the LSTM surprisals consistently under-predicted the\nmagnitude of processing delays in garden path sentences. By using LSTM surprisals as a measure of predictability,\nthis experiment provided empirical evidence against the single-stage theory, which predicts a linear relationship\nbetween reading time and predictability.\nWilcox et al. (2021a) studied the learnability of island constraints, where subtle constraints sometimes prevent\nmovement of a wh-phrase. Linguistic nativism theories have argued that innate knowledge of universal grammar\nis necessary for children to learn island constraints from limited data, while opponents have denied this claim. The\nauthors tested LMs on sensitivity to a variety of island constraints, ﬁnding that LMs are generally successful at\nthis task. This constituted evidence against nativism theories, since LMs are general domain learners that cannot\npossibly possess innate knowledge of human grammar, yet were able to learn island constraints from data.\nThe next chapter presents my work on applying LMs toward theories of word class ﬂexibility, contributing a\ncase study that demonstrates the utility of LMs for linguistic theory.\nChapter 4\nWord class ﬂexibility in LMs\nThe contents of this chapter are based on my previous publication (Li et al., 2020).\n4.1\nIntroduction\nIn this chapter, we present a computational methodology to quantify semantic regularities in word class ﬂexibility\nusing contextual word embeddings. Word class ﬂexibility refers to the phenomenon whereby a single word form is\nused across different grammatical categories, and is considered one of the challenging topics in linguistic typology\n(Evans and Levinson, 2009). For instance, the word buru in Mundari can be used as a noun to denote ‘mountain’,\nor as a verb to denote ‘to heap up’ (Evans and Osada, 2005).\nThere is an extensive literature on how languages vary in word class ﬂexibility, either directly (Hengeveld,\n1992; Vogel and Comrie, 2000; Van Lier and Rijkhoff, 2013) or through related notions such as word class\nconversion (with zero-derivation) (Vonen, 1994; Don, 2003; Bauer and Valera, 2005a; Manova, 2011; S¸tekauer\net al., 2012). However, existing studies tend to rely on analyses of small sets of lexical items that may not\nbe representative of word class ﬂexibility in the broad lexicon. Critically lacking are systematic analyses of\nword class ﬂexibility across many languages, and existing typological studies have only focused on qualitative\ncomparisons of word class systems.\nWe take to our knowledge the ﬁrst step towards computational quantiﬁcation of word class ﬂexibility in 37\nlanguages, taken from the Universal Dependencies project (Zeman et al., 2019). We focus on lexical items that\ncan be used both as nouns and as verbs, i.e., noun-verb ﬂexibility. This choice is motivated by the fact that the\ndistinction between nouns and verbs is the most stable in word class systems across languages: if a language makes\nany distinction between word classes at all, it will likely be a distinction between nouns and verbs (Hengeveld,\n1992; Evans, 2000; Croft, 2003). However, our understanding of cross-linguistic regularity in noun-verb ﬂexibility\n27\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n28\nis impoverished.\nWe operationalize word class ﬂexibility as a property of lemmas. We deﬁne a lemma as ﬂexible if some\nof its occurrences are tagged as nouns and others as verbs. Flexible lemmas are sorted into noun dominant\nlemmas, which occur more frequently as nouns, and verb dominant lemmas that occur more frequently as verbs.\nOur methodology builds on contextualized word embedding models (e.g., ELMo (Peters et al., 2018) and BERT\n(Devlin et al., 2019)) to quantify semantic shift between grammatical classes of a lemma, within a single language.\nThis methodology can also help quantify metrics of ﬂexibility in the lexicon across languages.\nWe use our methodology to address one of the most fundamental questions in the study of word class ﬂex-\nibility: should this phenomenon be analyzed as a directional word-formation process similar to derivation, or\nas a form of underspeciﬁcation? Derived words are commonly argued to have a lower frequency of use and a\nnarrower range in meaning compared to their base (Marchand, 1964; Iacobini, 2000). If word class ﬂexibility\nis a directional process, we should expect that ﬂexible lemmas are subject to more semantic variation in their\ndominant word class than in their less frequent class. We also test the claim that noun-to-verb ﬂexibility involves\nmore semantic shift than verb-to-noun ﬂexibility. While previous work has explored these questions, it remains\nchallenging to quantify semantic shift and semantic variation, particularly across different languages.\nWe present a novel probing task that reveals the ability of deep contextualized models to capture semantic\ninformation across word classes. Our utilization of deep contextual models predicts human judgment on the\nspectrum of noun-verb ﬂexible usages including homonymy (unrelated senses), polysemy (different but related\nsenses), and word class ﬂexibility. We ﬁnd that BERT outperforms ELMo and non-contextual word embed-\ndings, and that the upper layers of BERT capture the most semantic information, which resonates with existing\nprobing studies (Tenney et al., 2019b). Our source code and data are available at: https://github.com/\nSPOClab-ca/word-class-flexibility.\n4.2\nLinguistic background and assumptions\n4.2.1\nTypes of ﬂexibility\nThe phenomenon of word class ﬂexibility has been analyzed in different ways. One way is to assume the existence\nof underspeciﬁed word classes. For instance, Hengeveld (2013) claims that basic lexical items in Mundari belong\nto a single class of contentives that can be used to perform all the functions associated with nouns, verbs, adjectives\nor adverbs in a language like English. Alternatively, word class ﬂexibility can be analyzed as a form of conversion,\ni.e., as a relation between words that have the same form and closely related senses but different word classes,\nsuch as a ﬁsh and to ﬁsh in English (Adams, 1973). Conversion has been analyzed as a derivational process that\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n29\nrelates different lexemes (Jespersen, 1924; Marchand, 1969; Quirk et al., 1985), or as a property of lexemes whose\nword class is underspeciﬁed (Farell, 2001; Barner and Bale, 2002). We use word class ﬂexibility as a general term\nthat subsumes these different notions. This allows us to assess whether there is evidence that word class ﬂexibility\nshould be characterized as a directional word formation process, rather than as a form of underspeciﬁcation.\n4.2.2\nHomonymy and polysemy\nWord class ﬂexibility has often been analyzed in terms of homonomy and polysemy (Valera and Ruz, 2020).\nHomonymy is a relation between lexemes that share the same word form but are not semantically related (Cruse,\n1986, p.80). Homonyms may differ in word class, such as ring ‘a small circular band’ and ring ‘make a clear\nresonant or vibrating sound.’ Polysemy is deﬁned as a relation between different senses of a single lexeme (ibid.).\nInsofar as the nominal and verbal uses of ﬂexible lexical items are semantically related, one may argue that word\nclass ﬂexibility is similar to polysemy, and must be distinguished from homonymy. In practice, homonymy and\npolysemy exist on a continuum, so it is difﬁcult to apply a consistent criterion to differentiate them (Tuggy, 1993).\nAs a consequence, we will not attempt to tease homonymy apart from word class ﬂexibility.\nRegarding morphology, word class ﬂexibility excludes pairs of lexical items that are related by overt deriva-\ntional afﬁxes, such as to act/an actor. In such cases, word class alternations can be attributed to the presence of\na derivational afﬁx, and are therefore part of regular morphology. In contrast, we allow tokens of ﬂexible lexical\nitems to differ in inﬂectional morphology.\n4.2.3\nDirectionality of class conversion\nWord class ﬂexibility can be analyzed either as a static relation between nominal and verbal uses of a single\nlexeme, or as a word formation process related to derivation. The merits of each analysis have been extensively\ndebated in the literature on conversion (see e.g., Farell, 2001; Don, 2005). One of the objectives of our study\nis to show that deep contextualized language models can be used to help resolve this debate. A hallmark of\nderivational processes is their directionality. Direction of derivation can be established using several synchronic\ncriteria, among which are the principles that a derived form tends to have a lower frequency of use and a smaller\nrange of senses than its base (Marchand, 1964; Iacobini, 2000). In languages where word class ﬂexibility is a\nderivational process, one should therefore expect greater semantic variation when ﬂexible lemmas are used in\ntheir dominant word class—an important issue that we verify with our methodology.\nA related phenomenon is the relationship between frequency and polysemy. Higher frequency words tend to\nhave more senses as they are inﬂuenced to a greater extent by phonetic reduction and sense extension processes\n(Zipf, 1949; Fenk-Oczlon et al., 2010). In our work, we compare semantic variation between the noun and verb\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n30\nusages of a word rather than semantic variation across different words; the presence of a similar effect would\nconstitute as evidence of word class ﬂexibility as a derivational process.\nSemantic variation has been operationalized in several ways. Kisselew et al. (2016) uses an entropy-based\nmetric, while Balteiro (2007) and Bram (2011) measure semantic variation by counting the number of different\nnoun and verb senses in a dictionary. The latter study found that the more frequent word class has greater semantic\nvariation at a rate above random chance. Here we propose a novel metric based on contextual word embeddings\nto compare the amount of semantic variation of ﬂexible lemmas in their dominant and non-dominant grammatical\nclasses. Differing from existing methods, our metric is validated explicitly on human judgements of semantic\nsimilarity, and can be applied to many languages without the need for dictionary resources.\n4.2.4\nAsymmetry in semantic shift\nIf word class ﬂexibility is a directional process, a natural question is whether derived verbs stand in the same\nsemantic relation to their base as derived nouns. The literature on conversion suggests that there might be sig-\nniﬁcant differences between these two directions of derivation. In English, verbs that are derived from nouns by\nconversion have been argued to describe events that include the noun’s denotation as a participant (e.g. hammer,\n‘to hit something with a hammer’) or as a spatio-temporal circumstance (winter ‘to spend the winter somewhere’).\nClark and Clark (1979) argue that the semantic relations between denominal verbs and their base are so varied\nthat they cannot be given a uniﬁed description. In comparison, when the base of conversion is a verb, the de-\nrived noun most frequently denotes an event of the sort described by the verb (e.g. throw ‘the act of throwing\nsomething’), or the result of such an act (e.g. release ‘state of being set free’) (Jespersen, 1942; Marchand, 1969;\nCetnarowska, 1993). This has led some authors to suggest that verb to noun conversion in English involves less\nsemantic shift than noun to verb conversion (Bauer, 2005, p.22). Here we consider a new metric of semantic shift\nbased on contextual embeddings, and we use this metric to test the hypothesis that the expected semantic shift\ninvolved in word class ﬂexibility is greater for noun dominant lexical items (as compared to verb dominant lexical\nitems) in our sample of languages. As we will show, this proposal is consistent with the empirical observation\nthat verb-to-noun conversion is statistically more salient than noun-to-verb conversion.\n4.3\nIdentiﬁcation of word class ﬂexibility\n4.3.1\nDeﬁnitions\nA lemma is ﬂexible if it can be used both as a noun and as a verb. To reduce noise, we require each lemma to\nappear at least 10 times and at least 5% of the time as the minority class to be considered ﬂexible. The inﬂectional\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n31\nparadigm of a lemma is the set of words that have the lemma.\nA ﬂexible lemma is noun (verb) dominant if it occurs more often as a noun (verb) than as a verb (noun). This is\nmerely an empirical property of a lemma: we do not claim that the base POS should be determined by frequency.\nThe noun (verb) ﬂexibility of a language is the proportion of noun (verb) dominant lemmas that are ﬂexible.\n4.3.2\nDatasets and preprocessing\nOur experiments require corpora containing part-of-speech annotations. For English, we use the British National\nCorpus (BNC), consisting of 100M words of written and spoken English from a variety of sources (Leech, 1992).\nRoot lemmas and POS tags are provided, and were generated automatically using the CLAWS4 tagger (Leech\net al., 1994). For our experiments, we use BNC-baby, a subset of BNC containing 4M words.\nFor other languages, we use the Universal Dependencies (UD) treebanks of over 70 languages, annotated with\nlemmatizations, POS tags, and dependency information (Zeman et al., 2019). We concatenate the treebanks for\neach language and use the languages that have at least 100k tokens.\nThe UD treebanks are too small for our contextualized experiments and are not matched for content and\nstyle, so we supplement them with Wikipedia text1. For each language, we randomly sample 10M tokens from\nWikipedia; we then use UDPipe 1.2 (Straka and Strakov´a, 2017) to tokenize the text and generate POS tags for\nevery token. We do not use the lemmas provided by UDPipe, but instead use the lemma merging algorithm to\ngroup lemmas.\n4.3.3\nLemma merging algorithm\nThe UD corpus provides lemma annotations for each word, but these lemmas are insufﬁcient for our purposes\nbecause they do not always capture instances of ﬂexibility. In some languages, nouns and verbs are lemmatized\nto different forms by convention. For example, in French, the word voyage can be used as a verb (il voyage ‘he\ntravels’) or as a noun (un voyage ‘a trip’). However, verbs are lemmatized to the inﬁnitive voyager, whereas\nnouns are lemmatized to the singular form voyage. Since the noun and verb lemmas are different, it is not easy to\nidentify them as having the same stem.\nThe different lemmatization conventions of French and English reﬂect a more substantial linguistic difference.\nFrench has a stem-based morphology, in which stems tend to occur with an inﬂectional ending. By contrast,\nEnglish has a word-based morphology, where stems are commonly used as free forms (Kastovsky, 2006). This\ndifference is relevant to the deﬁnition of word class ﬂexibility: in stem-based systems, ﬂexible items are stems\nthat may not be attested as free forms (Bauer and Valera, 2005b, p.14).\n1We use Wikiextractor to extract text from Wikimedia dumps: https://github.com/attardi/wikiextractor.\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n32\nLanguage\nNouns\nVerbs\nNoun\nﬂexibility\nVerb\nﬂexibility\nArabic\n1517\n299\n0.076\n0.221\nBulgarian\n786\n343\n0.039\n0.047\nCatalan\n1680\n590\n0.039\n0.147\nChinese\n1325\n634\n0.125\n0.391\nCroatian\n1031\n370\n0.042\n0.062\nDanish\n324\n216\n0.108\n0.269\nDutch\n958\n441\n0.077\n0.188\nEnglish\n1700\n600\n0.248\n0.472\nEstonian\n1949\n592\n0.032\n0.115\nFinnish\n1523\n631\n0.028\n0.136\nFrench\n1844\n649\n0.062\n0.257\nGalician\n802\n334\n0.031\n0.135\nGerman\n4239\n1706\n0.049\n0.229\nHebrew\n850\n315\n0.111\n0.321\nIndonesian\n572\n243\n0.052\n0.128\nItalian\n2227\n770\n0.067\n0.256\nJapanese\n1105\n417\n0.178\n0.566\nKorean\n1890\n1003\n0.026\n0.048\nLatin\n1090\n885\n0.056\n0.122\nNorwegian\n1951\n636\n0.072\n0.259\nOld Russian\n527\n416\n0.034\n0.060\nPolish\n2054\n1084\n0.069\n0.427\nPortuguese\n1711\n638\n0.037\n0.185\nRomanian\n1809\n740\n0.060\n0.151\nSlovenian\n746\n316\n0.068\n0.123\nSpanish\n2637\n873\n0.046\n0.202\nSwedish\n784\n384\n0.038\n0.109\nExcluded Languages\nAncient Greek\n1098\n1022\n0.015\n0.026\nBasque\n650\n247\n0.020\n0.105\nCzech\n5468\n2063\n0.004\n0.011\nHindi\n1364\n133\n0.019\n0.135\nLatvian\n1159\n603\n0.022\n0.061\nPersian\n1125\n47\n0.010\n0.234\nRussian\n3909\n1760\n0.005\n0.024\nSlovak\n488\n281\n0.006\n0.011\nUkrainian\n659\n238\n0.006\n0.029\nUrdu\n722\n51\n0.018\n0.216\nTable 4.1: Noun and verb ﬂexibility for 37 languages with at least 100k tokens in the UD corpus. We include the\n27 languages with over 2.5% noun and verb ﬂexibility; 10 languages are excluded from further analysis.\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n33\nWe propose a heuristic algorithm to capture stem-based ﬂexibility as well as word-based ﬂexibility. The key\nobservation is that the inﬂectional paradigms of the noun and verb forms often have some words in common\n(such is the case for voyager). Thus, we merge any two lemmas whose inﬂectional paradigms have a nonempty\nintersection. This is implemented with a single pass through the corpus, using the union-ﬁnd data structure: for\nevery word, we call UNION on the inﬂected form and the lemmatized form.\nUsing this heuristic, we can identify cases of ﬂexibility that do not share the same lemma in the UD corpus\n(Table 4.1). This method is not perfect, and is unable to identify cases of stem-based ﬂexibility where the inﬂec-\ntional paradigms don’t intersect, for example in French, chant ‘song’ and chants ‘songs’ are not valid inﬂections\nof the verb chanter ‘to sing’. There are also false positives that cause two unrelated lemmas to be merged if their\ninﬂectional paradigms intersect, for example, avions (plural form of avion ‘airplane’) happens to have the same\nform as avions (ﬁrst person plural imperfect form of avoir ‘to have’).\n4.4\nMethodology and evaluation\n4.4.1\nProbing test of contextualized model\nWe now utilize contextual word embeddings from language models ELMo, BERT, mBERT, and XLM-R (de-\nscribed in Sections 2.3 and 2.4) towards word class ﬂexibility. Contextual embeddings can capture a variety of\ninformation other than semantics, which can introduce noise into our results, for example: the lexicographic form\nof a word, syntactic position, etc. In order to compare different contextual language models on how well they cap-\nture semantic information, we perform a probing test of how accurate the models can capture human judgements\nof word sense similarity.\nWe begin with a list of the 138 most frequent ﬂexible words in the BNC corpus. Some of these words are\nﬂexible (e.g., work), while others are homonyms (e.g., bear). For each lemma, we get ﬁve human annotators from\nMechanical Turk to make a sentence using the word as a noun, then make a sentence using the word as a verb,\nthen rate the similarity of the noun and verb senses on a scale from 0 to 2. The sentences are used for quality\nassurance, so that ratings are removed if the sentences are nonsensical. We will call the average human rating for\neach word the human similarity score; Table 4.2 shows the average ratings for all 138 words.\nNext, we evaluate each layer of ELMo, BERT, mBERT, and XLM-R2 on correlation with the human similarity\nscore. That is, we compute the mean of the contextual vectors for all noun instances of the given word in the BNC\ncorpus, the mean across all verb instances, then compute the cosine distance between the two mean vectors as\nthe model’s similarity score. Finally, we evaluate the Spearman correlation of the human and model’s similarity\n2We use the models ‘bert-base-uncased’, ‘bert-base-multilingual-cased’, and ‘xlm-roberta-base’ from Wolf et al. (2019).\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n34\nWord\n|N|\n|V |\nSim\nWord\n|N|\n|V |\nSim\nWord\n|N|\n|V |\nSim\naim\n137\n98\n2.0\nchange\n889\n858\n1.6\nforce\n470\n188\n0.8\nanswer\n480\n335\n2.0\nclaim\n222\n239\n1.6\ngrant\n108\n87\n0.8\nattempt\n302\n214\n2.0\ncut\n92\n488\n1.6\nnote\n287\n361\n0.8\ncare\n403\n249\n2.0\ndemand\n169\n142\n1.6\nsense\n536\n88\n0.8\ncontrol\n519\n179\n2.0\ndesign\n246\n153\n1.6\ntear\n124\n89\n0.8\ncost\n234\n192\n2.0\nexperience\n522\n150\n1.6\naccount\n337\n122\n0.6\ncount\n143\n220\n2.0\nhope\n114\n571\n1.6\nact\n644\n268\n0.6\ndamage\n270\n82\n2.0\nincrease\n252\n399\n1.6\nback\n764\n88\n0.6\ndance\n81\n97\n2.0\njudge\n80\n96\n1.6\nface\n1185\n281\n0.6\ndoubt\n261\n132\n2.0\nlimit\n125\n134\n1.6\nhold\n130\n1251\n0.6\ndrink\n456\n315\n2.0\nload\n230\n87\n1.6\nland\n393\n123\n0.6\nend\n1171\n244\n2.0\noffer\n93\n489\n1.6\nlift\n100\n165\n0.6\nescape\n95\n111\n2.0\nrise\n164\n283\n1.6\nmatter\n572\n294\n0.6\nestimate\n96\n118\n2.0\nsmoke\n128\n100\n1.6\norder\n841\n133\n0.6\nfear\n209\n99\n2.0\nstart\n159\n1269\n1.6\nplace\n1643\n341\n0.6\nglance\n101\n161\n2.0\nstep\n401\n167\n1.6\npress\n130\n188\n0.6\nhelp\n200\n897\n2.0\nstudy\n1037\n211\n1.6\nroll\n135\n201\n0.6\ninﬂuence\n204\n150\n2.0\nsupport\n290\n292\n1.6\nsort\n1613\n216\n0.6\nlack\n194\n107\n2.0\ntrust\n90\n126\n1.6\nﬁre\n444\n89\n0.4\nlink\n147\n176\n2.0\nwaste\n103\n98\n1.6\nform\n1272\n354\n0.4\nlove\n495\n573\n2.0\nwork\n1665\n1593\n1.6\nnotice\n115\n387\n0.4\nmove\n131\n1272\n2.0\nbase\n109\n378\n1.4\nplay\n185\n1093\n0.4\nname\n960\n112\n2.0\ncover\n137\n399\n1.4\nturn\n226\n1566\n0.4\nneed\n587\n2350\n2.0\nplant\n591\n82\n1.4\nwave\n402\n120\n0.4\nphone\n382\n238\n2.0\nrun\n152\n999\n1.4\ncross\n102\n215\n0.2\nplan\n321\n161\n2.0\nstress\n159\n106\n1.4\ndeal\n191\n315\n0.2\nquestion\n1285\n96\n2.0\napproach\n409\n175\n1.2\nhand\n1765\n127\n0.2\nrain\n182\n92\n2.0\ncause\n237\n530\n1.2\npresent\n219\n353\n0.2\nresult\n752\n206\n2.0\nmatch\n110\n123\n1.2\nset\n387\n652\n0.2\nreturn\n138\n441\n2.0\nmiss\n320\n410\n1.2\nshare\n104\n232\n0.2\nsearch\n215\n163\n2.0\nprocess\n720\n91\n1.2\nsign\n284\n121\n0.2\nsleep\n171\n291\n2.0\nshift\n96\n104\n1.2\nsuit\n162\n108\n0.2\nsmell\n141\n149\n2.0\nshow\n132\n1843\n1.2\nwind\n189\n82\n0.2\nsmile\n211\n422\n2.0\nsound\n313\n496\n1.2\naddress\n257\n148\n0.0\ntalk\n119\n1302\n2.0\ndress\n191\n196\n1.0\nbear\n110\n394\n0.0\nuse\n791\n2801\n2.0\nlead\n107\n716\n1.0\nhead\n1355\n96\n0.0\nview\n811\n102\n2.0\nlight\n669\n124\n1.0\nmind\n736\n620\n0.0\nvisit\n136\n203\n2.0\nlook\n699\n5893\n1.0\npark\n179\n105\n0.0\nvote\n124\n93\n2.0\nmark\n562\n198\n1.0\npoint\n1534\n469\n0.0\nwalk\n144\n914\n2.0\nmeasure\n226\n223\n1.0\nring\n185\n387\n0.0\ndream\n254\n107\n1.8\nrest\n414\n132\n1.0\nsquare\n225\n82\n0.0\nrecord\n1057\n276\n1.8\ntie\n82\n112\n1.0\nstate\n471\n156\n0.0\nreport\n313\n331\n1.8\nbreak\n117\n519\n0.8\nstick\n109\n294\n0.0\ntest\n273\n126\n1.8\ncharge\n392\n115\n0.8\nstore\n95\n158\n0.0\ntouch\n145\n271\n1.8\ndrive\n88\n476\n0.8\ntrain\n224\n94\n0.0\ncall\n209\n1558\n1.6\nfocus\n92\n168\n0.8\nwatch\n119\n940\n0.0\nTable 4.2: 138 ﬂexible words in English (top in BNC corpus) and human similarity scores, average of 5 ratings.\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n35\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.2\n0.4\n0.6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nLayer\nCorrelation\nG BERT\nELMo\nmBERT\nXLM−R\nFigure 4.1: Spearman correlations between human and model similarity scores for ELMo, BERT, mBERT, and\nXLM-R. The dashed line is the baseline using static GloVe embeddings.\nscores for 138 words: this score measures the model’s ability to gauge the level of semantic similarity between\nnoun and verb senses, compared to human judgements.\nFor a baseline, we do the same procedure using non-contextual GloVe embeddings (Pennington et al., 2014).\nNote that while all instances of the same word have a static embedding, different words that share the same lemma\nstill have different embeddings (e.g., work and works), so that the baseline is not trivial.\nThe correlations are shown in Figure 4.1. BERT and mBERT are better than ELMo and XLM-R at capturing\nsemantic information, in all transformer models, the correlation increases for each layer up until layer 4 or so, and\nafter this point, the performance neither improves nor degrades in higher layers. Thus, unless otherwise noted, we\nuse the ﬁnal layers of each model for downstream tasks.\nFigure 4.2 illustrates the contextual distributions for two lemmas on the opposite ends of the noun-verb sim-\nilarity spectrum: work (human similarity score: 2) and ring (human similarity score: 0). We apply PCA to the\nBERT embeddings of all instances of each lemma in the BNC corpus. For work, the noun and verb senses are very\nsimilar and the distributions have high overlap. In contrast, for ring, the most common noun sense (‘a circular\nobject’) is etymologically and semantically unrelated to the most common verb sense (‘to produce a resonant\nsound’), and accordingly, their distributions have very little overlap.\n4.4.2\nThree contextual metrics\nWe deﬁne three metrics based on contextual embeddings to measure various semantic aspects of word class\nﬂexibility. We start by generating contextual embeddings for each occurrence of every ﬂexible lemma. For each\nlemma l, let En,l and Ev,l be the set of contextual embeddings for noun and verb instances of l. We deﬁne the\nprototype noun vector ⃗pn,l of a lemma l as the mean of embeddings across noun instances, and the noun variation\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n36\nPC1\nPC2\nVerb\nNoun\nLemma: work\nPC1\nPC2\nVerb\nNoun\nLemma: ring\nFigure 4.2: PCA plot of BERT embeddings for the lemmas “work” (high similarity between noun and verb senses)\nand “ring” (low similarity).\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n37\nLanguage\nN→V shift\nV→N shift\nNoun\nvariation\nVerb\nvariation\nMajority\nvariation\nMinority\nvariation\nArabic\n0.098\n0.109\n8.268\n8.672∗∗∗\n8.762∗∗∗\n8.178\nBulgarian\n0.146\n0.136\n8.267\n8.409\n8.334\n8.341\nCatalan\n0.165\n0.169\n8.165\n8.799∗∗∗\n8.720∗∗∗\n8.244\nChinese\n0.072\n0.070\n7.024\n7.212∗∗∗\n7.170∗∗∗\n7.067\nCroatian\n0.093\n0.144∗∗\n8.149\n8.109\n8.219∗∗\n8.037\nDanish\n0.103\n0.110\n8.245\n8.338\n8.438∗∗∗\n8.146\nDutch\n0.146\n0.174\n7.716\n8.786∗∗∗\n8.354∗\n8.148\nEnglish\n0.175∗\n0.160\n8.035\n8.624∗∗∗\n8.390∗∗∗\n8.268\nEstonian\n0.105\n0.103\n7.800\n7.902\n8.022∗∗\n7.679\nFinnish\n0.100\n0.114\n7.972\n7.854\n8.181∗∗∗\n7.644\nFrench\n0.212\n0.204\n8.189\n9.472∗∗∗\n9.082∗∗∗\n8.578\nGalician\n0.111\n0.117\n7.922\n8.340∗∗∗\n8.137\n8.127\nGerman\n0.382\n0.355\n8.078\n9.758∗∗∗\n9.096∗∗\n8.740\nHebrew\n0.121\n0.130\n8.096\n9.116∗∗∗\n8.574\n8.638\nIndonesian\n0.034\n0.048\n7.100\n7.076\n7.076\n7.101\nItalian\n0.207\n0.184\n8.520\n9.345∗∗∗\n9.149∗∗∗\n8.716\nJapanese\n0.061\n0.057\n7.419∗∗∗\n7.173\n7.309\n7.283\nLatin\n0.092\n0.139∗∗∗\n7.920∗∗∗\n7.710\n7.905∗∗∗\n7.724\nNorwegian\n0.133\n0.132\n8.112\n8.336∗∗∗\n8.332∗∗∗\n8.116\nPolish\n0.090\n0.080\n8.318\n8.751∗∗∗\n8.670∗∗∗\n8.399\nPortuguese\n0.186\n0.155\n7.907\n8.921∗∗∗\n8.642∗∗∗\n8.187\nRomanian\n0.175\n0.145\n8.682\n8.658\n8.934∗∗∗\n8.406\nSlovenian\n0.093\n0.113\n8.046\n7.983\n8.177∗∗∗\n7.853\nSpanish\n0.235\n0.214\n7.898\n8.961∗∗∗\n8.691∗∗∗\n8.168\nSwedish\n0.088\n0.082\n8.262∗\n8.147\n8.328∗∗∗\n8.081\nOverall\n1 of 3\n2 of 3\n3 of 17\n14 of 17\n20 of 20\n0 of 20\nTable 4.3: Semantic metrics for 25 languages, computed using mBERT and 10M tokens of Wikipedia text for\neach language. Asterisks denote signiﬁcance at ∗p < 0.05, ∗∗p < 0.01, ∗∗∗p < 0.001. For the “Overall” row,\nwe count the languages with a signiﬁcant tendency towards one direction, out of the number of languages with\nstatistical signiﬁcance towards either direction (with p < 0.05 treated as signiﬁcant).\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n38\nVn,l as the mean Euclidean distance from each noun instance to the noun vector:\n⃗pn,l =\n1\n|En,l|\nX\n⃗x∈En,l\n⃗x\n(4.1)\nVn,l =\n1\n|En,l|\nX\n⃗x∈En,l\n||⃗x −⃗pn,l||\n(4.2)\nThe prototype verb vector ⃗pv,l and verb variation Vv,l for a lemma l are deﬁned similarly:\n⃗pv,l =\n1\n|Ev,l|\nX\n⃗x∈Ev,l\n⃗x\n(4.3)\nVv,l =\n1\n|Ev,l|\nX\n⃗x∈Ev,l\n||⃗x −⃗pv,l||\n(4.4)\nLemmas are included if they appear at least 30 times as nouns and 30 times as verbs. To avoid biasing the\nvariation metric towards the majority class, we downsample the majority class to be of equal size as the minority\nclass before computing the variation. The method does not ﬁlter out pairs of lemmas that are arguably homonyms\nrather than ﬂexible (section 4.2.2); we choose to include all of these instances rather than set an arbitrary cutoff\nthreshold.\nWe now deﬁne language-level metrics to measure the asymmetries hypothesized in sections 4.2.3 and 4.2.4.\nThe noun-to-verb shift (NVS) is the average cosine distance between the prototype noun and verb vectors for noun\ndominant lemmas, and the verb-to-noun shift (VNS) likewise for verb dominant lemmas:\nNV S = 1 −El noun-dominant[cos(⃗pn,l, ⃗pv,l)]\n(4.5)\nV NS = 1 −El verb-dominant[cos(⃗pn,l, ⃗pv,l)]\n(4.6)\nWe deﬁne the noun (verb) variation of a language as the average of noun (verb) variations across all lemmas.\nFinally, deﬁne the majority variation of a language as the average of the variation of the dominant POS class, and\nthe minority variation as the average variation of the smaller POS class, across all lemmas.\n4.5\nResults\n4.5.1\nIdentifying ﬂexible lemmas\nOf the 37 languages in UD with at least 100k tokens; in 27 of them, at least 2.5% of verb and noun lemmas\nare ﬂexible, which we take to indicate that word class ﬂexibility exists in the language (Table 4.1). The lemma\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n39\nDataset\nModel\nN→V shift\nV→N shift\nNoun\nvariation\nVerb\nvariation\nMajority\nvariation\nMinority\nvariation\nBNC\nELMo\n0.389∗\n0.357\n20.261\n20.455\n20.329\n20.388\nBERT\n0.122∗\n0.112\n9.015\n9.074\n9.100∗∗∗\n8.989\nmBERT\n0.189∗\n0.169\n7.211\n8.401∗∗∗\n7.875∗∗\n7.717\nXLM-R\n0.004\n0.005\n2.058\n2.374∗∗∗\n2.262\n2.170\nWikipedia\nELMo\n0.339∗∗∗\n0.330\n22.556\n22.521\n22.463\n22.614∗\nBERT\n0.120∗∗∗\n0.100\n9.218∗∗∗\n8.944\n9.118∗∗\n9.044\nmBERT\n0.175∗\n0.160\n8.035\n8.624∗∗∗\n8.390∗∗∗\n8.268\nXLM-R\n0.004∗∗\n0.003\n1.966\n1.954\n1.946\n1.974\nTable 4.4: Comparison of semantic models on BNC and Wikipedia datasets (English), computed using several\ndifferent language models. Asterisks denote signiﬁcance at ∗p < 0.05, ∗∗p < 0.01, ∗∗∗p < 0.001.\nmerging algorithm is crucial for identifying word class ﬂexibility: only 6 of the 37 languages pass the 2.5%\nﬂexibility threshold using the default lemma annotations provided in UD3. Languages differ in their prevalence of\nword class ﬂexibility, but every language in our sample has higher verb ﬂexibility than noun ﬂexibility.\n4.5.2\nAsymmetry in semantic metrics\nTable 4.3 shows the values of the three metrics, computed using mBERT and Wikipedia data for 25 languages4.\nFor testing signiﬁcance, we use the unpaired Student’s t-test to compare N-V versus V-N shift, and the paired\nStudent’s t-test for the other two metrics5. The key ﬁndings are as follows:\n1. Asymmetry in semantic shift. In English, N-V shift is greater than V-N shift, in agreement with Bauer\n(2005). However, this pattern does not hold in general: there is no signiﬁcant difference in either direction\nin most languages, and two languages exhibit a difference in the opposite direction as English.\n2. Asymmetry in semantic variation between noun and verb usages. Of the 17 languages with a statisti-\ncally signiﬁcant difference in noun versus verb variation, 14 of them have greater verb variation than noun\nvariation.\n3. Asymmetry in semantic variation between majority and minority classes. All of the 20 languages with\na statistically signiﬁcant difference in majority and minority variation have greater majority variation.\n4.5.3\nModel robustness\nNext, we assess the robustness of our metrics with respect to choices of corpus and language model. Robustness is\ndesirable because it gives conﬁdence that our models capture true linguistic tendencies, rather than artifacts of our\n3Chinese, Danish, English, Hebrew, Indonesian, and Japanese pass the ﬂexibility threshold without the lemma merging algorithm.\n4We exclude 2 of the 27 languages that we identify word class ﬂexibility. Old Russian was excluded because it is not supported by mBERT;\nKorean is excluded because the lemma annotations deviate from the standard UD format.\n5We do not apply the Bonferroni correction for multiple comparisons, because we make claims for trends across all languages, and not for\nany speciﬁc languages.\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n40\ndatasets or the models themselves. We compute the three semantic metrics on the BNC and Wikipedia datasets,\nusing all 4 contextual language models: ELMo, BERT, mBERT, and XLM-R. Table 4.4 summarizes the results\nfrom this experiment.\nWe ﬁnd that in almost every case where there is a statistically signiﬁcant difference, all models agree on the\ndirection of the difference. One exception is that noun variation is greater when computed using Wikipedia data\nthan when using the BNC corpus. Wikipedia has many instances of nouns used in technical senses (e.g., ring is a\ntechnical term in mathematics and chemistry), whereas similar nonﬁction text is less common in the BNC corpus.\n4.6\nDiscussion\n4.6.1\nFrequency asymmetry\nEvery language in our sample has verb ﬂexibility greater than noun ﬂexibility. The reasons for this asymmetry\nare unclear, but may be due to semantic differences between nouns and verbs. We note that every language in our\nsample has more noun lemmas than verb lemmas, a pattern that was also attested by Polinsky (2012), although\nthis does not provide an explanation of the observed phenomenon. We leave further exploration of the ﬂexibility\nasymmetry to future work.\n4.6.2\nImplications for theories of ﬂexibility\nThere is a strong cross-linguistic tendency for the majority word class of a ﬂexible lemma to exhibit more semantic\nvariation than the minority class. In other words, the frequency and semantic variation criteria of determining the\nbase of a conversion pair agree more than at chance. This supports the analysis of word class ﬂexibility as a\ndirectional process of conversion, as opposed to underspeciﬁcation (section 4.2.3)6. Within a ﬂexible lemma,\nverbs exhibit more semantic variation than nouns. It is attested across many languages that nouns are more\nphysically salient, while verbs have more complex event and argument structure, and are harder for children to\nacquire than nouns (Gentner, 1982; Imai et al., 2008). Thus, verbs are expected to have greater semantic variation\nthan nouns, which our results conﬁrm. More importantly, for our purposes, this metric serves as a control for the\nprevious metric. Flexible lemmas are more likely to be noun-dominant than verb-dominant, so could the majority\nand minority variation simply be proxies for noun and verb variation, respectively? In fact, we observe greater\nverb than noun variation, so this cannot be the case.\nFinally, as suggested by Bauer (2005), we ﬁnd evidence in English that N-V ﬂexibility involves more semantic\nshift than V-N ﬂexibility, and the pattern is consistent across multiple models and datasets (Table 4.4). However,\n6Since 18 of the 25 languages for which semantic metrics were calculated are Indo-European, it is unclear whether these results generalize\nto non-Indo-European languages.\nCHAPTER 4. WORD CLASS FLEXIBILITY IN LMS\n41\nthis pattern is idiosyncratic to English and not a cross-linguistic tendency. It is thus instructive to analyze multiple\nlanguages in studying word class ﬂexibility, as one can easily be misled by English-based analyses.\n4.7\nConclusion\nWe used contextual language models to examine shared tendencies in word class ﬂexibility across languages.\nWe found that the majority class often exhibits more semantic variation than the minority class, supporting the\nview that word class ﬂexibility is a directional process. We also found that in English, noun-to-verb ﬂexibility is\nassociated with more semantic shift than verb-to-noun ﬂexibility, but this is not the case for most languages.\nOur probing task revealed that the upper layers of BERT contextual embeddings best reﬂect human judgment\nof semantic similarity. We obtained similar results in different datasets and language models in English that\nsupport the robustness of our method. In this chapter, we demonstrated the utility of deep contextualized models in\nlinguistic typology, especially for characterizing cross-linguistic semantic phenomena that are otherwise difﬁcult\nto quantify. The next two chapters will present our work using linguistic theory and experimental data to deepen\nour understanding of language models.\nChapter 5\nLinguistic anomalies in LMs\nThe contents of this chapter are based on my previous publication (Li et al., 2021).\n5.1\nIntroduction\nThe previous chapter used contextual embeddings to measure the semantic distance between ﬂexible words used\nin different contexts. One limitation of this method is that different linguistic properties – morphology, syntax,\nsemantics, and pragmatics – are conﬂated into a single metric. From contextual embeddings, we cannot easily\ndetermine whether the difference between two words is syntactic (e.g., walk and walks) or semantic (e.g., walk\nand run).\nIn this chapter, we investigate how Transformer-based language models respond to sentences containing three\ndifferent types of anomalies: morphosyntactic, semantic, and commonsense. Previous work using behavioural\nprobing found that Transformer LMs have remarkable ability in detecting when a word is anomalous in context, by\nassigning a higher likelihood to the appropriate word than an inappropriate one (Gulordava et al., 2018; Ettinger,\n2020; Warstadt et al., 2020a). The likelihood score, however, only gives a scalar value of the degree that a word\nis anomalous in context, and cannot distinguish between different ways that a word might be anomalous.\nIt has been proposed that there are different types of linguistic anomalies. Chomsky (1957) distinguished se-\nmantic anomalies (“colorless green ideas sleep furiously”) from ungrammaticality (“furiously sleep ideas green\ncolorless”). Psycholinguistic studies initially suggested that different event-related potentials (ERPs) are pro-\nduced in the brain depending on the type of anomaly; e.g., semantic anomalies produce negative ERPs 400 ms\nafter the stimulus, while syntactic anomalies produce positive ERPs 600 ms after (Kutas et al., 2006). Here, we\nask whether Transformer LMs show different surprisals in their intermediate layers depending on the type of\nanomaly. However, LMs do not compute likelihoods at intermediate layers – only at the ﬁnal layer.\n42\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n43\nThe\ncat\nwon\n't\neating\nthe\nfood\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nLayer\nThe\nplane\nlaughed\nat\nthe\nrunway\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nFigure 5.1: Example sentence with a morphosyntactic anomaly (left) and semantic anomaly (right) (anomalies\nin bold). Darker colours indicate higher surprisal. We investigate several patterns: ﬁrst, surprisal at lower lay-\ners corresponds to infrequent tokens, but this effect diminishes towards upper layers. Second, morphosyntactic\nviolations begin to trigger high surprisals at an earlier layer than semantic violations.\nWe introduce a new tool to probe for surprisal at intermediate layers of BERT (Devlin et al., 2019), RoBERTa\n(Liu et al., 2019b), and XLNet (Yang et al., 2019), formulating the problem as density estimation. We train\nGaussian models to ﬁt distributions of embeddings at each layer of the LMs. Using BLiMP (Warstadt et al.,\n2020a) for evaluation, we show that this model is effective at grammaticality judgement, requiring only a small\namount of in-domain text for training. Figure 5.1 shows the method using the RoBERTa model on two example\nsentences.\nWe apply our model to test sentences drawn from BLiMP and 7 psycholinguistics studies, exhibiting mor-\nphosyntactic, semantic, and commonsense anomalies. We ﬁnd that morphosyntactic anomalies produce out-of-\ndomain embeddings at earlier layers, semantic anomalies at later layers, and commonsense anomalies not at any\nlayer, even though the LM’s ﬁnal accuracy is similar. We show that LMs are internally sensitive to the type of\nlinguistic anomaly, which is not apparent if we only had access to their softmax probability outputs. Our source\ncode and data are available at: https://github.com/SPOClab-ca/layerwise-anomaly.\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n44\n5.2\nRelated work\nOur work builds on earlier work on probing LM representations (Section 3.3). Previous work found differences\nin the linguistic knowledge contained in different layers (Tenney et al., 2019b; Kelly et al., 2020; Hewitt and\nManning, 2019); we focus on the effects of anomalous inputs on different layers. Behavioural probes (Section\n3.2) often used anomalous sentences paired with correct sentences to test LMs’ sensitivity to linguistic phenomena\n(Linzen et al., 2016; Gulordava et al., 2018; Warstadt et al., 2020a; Hu et al., 2020); in this work, we extend these\ntests to probe the sensitivity of internal layer representations to anomalies rather than the model’s output.\nMost grammaticality studies focused on syntactic phenomena, since they are the easiest to generate using tem-\nplates, although some studies considered semantic phenomena. Examples of semantic tests include Rabinovich\net al. (2019), who tested LMs’ sensitivity to semantic infelicities involving indeﬁnite pronouns, and Ettinger\n(2020), who used data from three psycholinguistic studies to probe BERT’s knowledge of commonsense and\nnegation. Another type of linguistic unacceptability is selectional restrictions, deﬁned as a semantic mismatch\nbetween a verb and an argument. Sasano and Korhonen (2020) examined the geometry of word classes (e.g.,\nwords that can be a direct object of the verb ‘play’) in word vector models; they compared single-class models\nagainst discriminative models for learning word class boundaries. Chersoni et al. (2018) tested distributional se-\nmantic models on their ability to identify selectional restriction violations using stimuli from two psycholinguistic\ndatasets. Finally, Metheniti et al. (2020) tested how much BERT relies on selectional restriction information ver-\nsus other contextual information for making masked word predictions. Our work combines these different types\nof unacceptability into a single test suite to faciliate comparison.\n5.3\nModel\nWe use the transformer language model as a contextual embedding extractor (we write this as BERT for con-\nvenience). Let L be the layer index, which ranges from 0 to 12 on all of our models. Using a training corpus\n{w1, · · · , wT }, we extract contextual embeddings at layer L for each token:\nx(L)\n1\n, · · · , x(L)\nT\n= BERTL(w1, · · · , wT ).\n(5.1)\nNext, we ﬁt a multivariate Gaussian on the extracted embeddings:\nx(L)\n1\n, · · · , x(L)\nT\n∼N(bµL, bΣL).\n(5.2)\nFor evaluating the layerwise surprisal of a new sentence s = [t1, · · · , tn], we similarly extract contextual\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n45\nembeddings using the language model:\ny1, · · · , yn = BERTL(t1, · · · , tn).\n(5.3)\nThe surprisal of each token is the negative log likelihood of the contextual vector according to the multivariate\nGaussian:\nGi = −log p(yi | bµL, bΣL)\nfor i = 1 . . . n.\n(5.4)\nFinally, we deﬁne the surprisal of sentence s as the sum of surprisals of all of its tokens, which is also the joint\nlog likelihood of all of the embeddings:\nsurprisalL(t1, · · · , tn) =\nn\nX\ni=1\nGi\n= −log p(y1, · · · , yn | bµL, bΣL).\n(5.5)\n5.3.1\nConnection to Mahalanobis distance\nThe theoretical motivation for using the sum of log likelihoods is that when we ﬁt a Gaussian model with full co-\nvariance matrix, low likelihood corresponds exactly to high Mahalanobis distance from the in-distribution points.\nThe score given by the Gaussian model is:\nG = −log p(y | bµL, bΣL)\n= −log\n \n1\n(2π)D/2|bΣL|1/2 exp(−1\n2d2)\n!\n,\n(5.6)\nwhere D is the dimension of the vector space, and d is the Mahalanobis distance:\nd =\nq\n(y −bµL)T bΣ\n−1\nL (y −bµL).\n(5.7)\nRearranging, we get:\nd2 = 2G −D log(2π) −log |bΣL|,\n(5.8)\nthus the negative log likelihood is the squared Mahalanobis distance plus a constant.\nVarious methods based on Mahalanobis distance have been used for anomaly detection in neural networks; for\nexample, Lee et al. (2018) proposed a similar method for out-of-domain detection in neural classiﬁcation models,\nand Cao et al. (2020) found the Mahalanobis distance method to be competitive with more sophisticated methods\non medical out-of-domain detection. In Transformer models, Podolskiy et al. (2021) used Mahalanobis distance\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n46\n0.6\n0.7\n0.8\n10\n20\n50\n100\n200\n500\n1000\n2000\n5000 10000\nTraining Sentences\nAccuracy\nBERT\nRoBERTa\nXLNet\n(a)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nLayer\nAccuracy\nBERT\nRoBERTa\nXLNet\n(b)\nFigure 5.2: BLiMP accuracy different amounts of training data and across layers, for three LMs. About 1000\nsentences are needed before a plateau is reached (mean tokens per sentence = 15.1).\nfor out-of-domain detection, outperforming methods based on softmax probability and likelihood ratios.\nGaussian assumptions. Our model assumes that the embeddings at every layer follow a multivariate Gaussian\ndistribution. Since the Gaussian distribution is the maximum entropy distribution given a mean and covariance\nmatrix, it makes the fewest assumptions and is therefore a reasonable default. Hennigen et al. (2020) found that\nembeddings sometimes do not follow a Gaussian distribution, but it is unclear what alternative distribution would\nbe a better ﬁt, so we will assume a Gaussian distribution in this work.\n5.3.2\nTraining and evaluation\nFor all of our experiments, we use the ‘base’ versions of pretrained language models BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019b), and XLNet (Yang et al., 2019), provided by HuggingFace (Wolf et al., 2019). Each\nof these models have 12 contextual layers plus a 0th static layer, and each layer is 768-dimensional.\nWe train the Gaussian model on randomly selected sentences from the British National Corpus (Leech, 1992),\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n47\nrepresentative of acceptable English text from various genres. We evaluate on BLiMP (Warstadt et al., 2020a), a\ndataset of 67k minimal sentence pairs that test acceptability judgements across a variety of syntactic and semantic\nphenomena. In our case, a sentence pair is considered correct if the sentence-level surprisal of the unacceptable\nsentence is higher than that of the acceptable sentence.\nHow much training data is needed? We experiment with training data sizes ranging from 10 to 10,000\nsentences (Figure 5.2a). Compared to the massive amount of data needed for pretraining the LMs, we ﬁnd that\na modest corpus sufﬁces for training the Gaussian anomaly model, and a plateau is reached after 1000 sentences\nfor all three models. Therefore, we use 1000 training sentences (unless otherwise noted) for all subsequent\nexperiments in this chapter.\nWhich layers are sensitive to anomaly? We vary L from 0 to 12 in all three models (Figure 5.2b). The layer\nwith the highest accuracy differs between models: layer 9 has the highest accuracy for BERT, 11 for RoBERTa,\nand 6 for XLNet. All models experience a sharp drop in the last layer, likely because the last layer is specialized\nfor the MLM pretraining objective.\nComparisons to other models. Our best-performing model is RoBERTa, with an accuracy of 0.830. This is\nslightly higher the best model reported in BLiMP (GPT-2, with accuracy 0.801). We do not claim to beat the state-\nof-the-art on BLiMP: Salazar et al. (2020) obtains a higher accuracy of 0.865 using RoBERTa-large. Even though\nthe main goal of this work is not to maximize accuracy on BLiMP, our Gaussian anomaly model is competitive\nwith other transformer-based models on this task.\n5.3.3\nFurther ablation studies on Gaussian model\nWe explore some variations to our methodology of training the Gaussian model. All of these variations are eval-\nuated on the full BLiMP dataset. In each experiment, (unless otherwise noted) the language model is RoBERTa-\nbase, using the second-to-last layer, and the Gaussian model has a full covariance matrix trained with 1000 sen-\ntences from the BNC corpus.\nCovariance matrix. We vary the type of covariance matrix (Table 5.1). Diagonal and spherical covariance\nmatrices perform worse than with the full covariance matrix; this may be expected, as the full matrix has the most\ntrainable parameters.\nGaussian mixture models. We try GMMs with up to 16 mixture components (Table 5.2). We observe a small\nincrease in accuracy compared to a single Gaussian, but the difference is too small to justify the increased training\ntime.\nGenre of training text. We sample from genres of BNC (each time with 1000 sentences) to train the Gaussian\nmodel (Table 5.3). The model performed worse when trained with the academic and spoken genres, and about\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n48\nCovariance\nAccuracy\nFull\n0.830\nDiagonal\n0.755\nSpherical\n0.752\nTable 5.1: Varying the type of covariance matrix in the Gaussian model.\nComponents\nAccuracy\n1\n0.830\n2\n0.841\n4\n0.836\n8\n0.849\n16\n0.827\nTable 5.2: Using Gaussian mixture models (GMMs) with multiple components.\nGenre\nAccuracy\nAcademic\n0.797\nFiction\n0.840\nNews\n0.828\nSpoken\n0.795\nAll\n0.830\nTable 5.3: Effect of the genre of training data.\nKernel\nScore\nRBF\n0.738\nLinear\n0.726\nPolynomial\n0.725\nTable 5.4: Using 1-SVM instead of GMM, with various kernels.\nAggregation\nAccuracy\nSum\n0.830\nMax\n0.773\nTable 5.5: Two sentence-level aggregation strategies\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n49\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nLayer\nPearson Correlation\nBERT\nRoBERTa\nXLNet\nFigure 5.3: Pearson correlation between token-level surprisal scores (Equation 5.4) and log frequency. The corre-\nlation is highest in the lower layers, and decreases in the upper layers.\nthe same with the ﬁction and news genres, perhaps because their vocabularies and grammars are more similar to\nthose in the BLiMP sentences.\nOne-class SVM. We try replacing the Gaussian model with a one-class SVM (Sch¨olkopf et al., 2000), another\npopular model for anomaly detection. We use the default settings from scikit-learn with three kernels (Table 5.4),\nbut it performs worse than the Gaussian model on all settings.\nSentence aggregation. Instead of Equation 5.5, we try deﬁning sentence-level surprisal as the maximum\nsurprisal among all tokens (Table 5.5):\nsurprisal(s1, · · · , sn) = maxn\ni=1Gi;\n(5.9)\nhowever, this performs worse than using the sum of token surprisals.\n5.3.4\nLower layers are sensitive to frequency\nWe notice that surprisal scores in the lower layers are sensitive to token frequency: higher frequency tokens\nproduce embeddings close to the center of the Gaussian distribution, while lower frequency tokens are at the\nperiphery. The effect gradually diminishes towards the upper layers.\nTo quantify the sensitivity to frequency, we compute token-level surprisal scores for 5000 sentences from\nBNC that were not used in training. We then compute the Pearson correlation between the surprisal score and log\nfrequency for each token (Figure 5.3). In all three models, there is a high correlation between the surprisal score\nand log frequency at the lower layers, which diminishes at the upper layers. A small positive correlation persists\nuntil the last layer, except for XLNet, in which the correlation eventually disappears.\nThere does not appear to be any reports of this phenomenon in previous work. For static word vectors, Gong\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n50\nFigure 5.4: PCA plot of randomly sampled RoBERTa embeddings at layers 1, 4, 7, and 10. Points are colored by\ntoken frequency: “Rare” means the 20% least frequent tokens, and “Frequent” is the other 80%.\net al. (2018) found that embeddings for low-frequency words lie in a different region of the embedding space\nthan high-frequency words. To visualize this phenomenon, we feed a random selection of BNC sentences into\nRoBERTa and use PCA to visualize the distribution of rare and frequent tokens at different layers (Figure 5.4). In\nall cases, we ﬁnd that infrequent tokens occupy a different region of the embedding space from frequent tokens,\nsimilar to what Gong et al. (2018) observed for static word vectors. The Gaussian model ﬁts the high-frequency\nregion and assigns lower likelihoods to the low-frequency region, explaining the positive correlation at all layers,\nalthough it is still unclear why the correlation diminishes at upper layers.\n5.4\nLevels of linguistic anomalies\nWe turn to the question of whether LMs exhibit different behaviour when given inputs with different types of\nlinguistic anomalies. The task of partitioning linguistic anomalies into several distinct classes can be challenging.\nSyntax and semantics have a high degree of overlap – there is no widely accepted criterion for distinguishing\nbetween ungrammaticality and semantic anomaly (e.g., Abrus´an (2019) gives a survey of current proposals), and\nPoulsen (2012) challenges this dichotomy entirely. Similarly, Warren et al. (2015) noted that semantic anomalies\ndepend somewhat on world knowledge.\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n51\nType\nTask\nCorrect Example\nIncorrect Example\nMorphosyntax\nBLiMP (Subject-Verb)\nThese casseroles disgust Kayla.\nThese casseroles disgusts Kayla.\nBLiMP (Det-Noun)\nCraig explored that grocery store.\nCraig explored that grocery stores.\nOsterhout and Nicol\n(1999)\nThe cats won’t eat the food that\nMary gives them.\nThe cats won’t eating the food that\nMary gives them.\nSemantic\nBLiMP (Animacy)\nAmanda was respected by some\nwaitresses.\nAmanda was respected by some\npicture.\nPylkk¨anen and McElree\n(2007)\nThe pilot ﬂew the airplane after\nthe intense class.\nThe pilot amazed the airplane after\nthe intense class.\nWarren et al. (2015)\nCorey’s hamster explored a nearby\nbackpack and ﬁlled it with sawdust.\nCorey’s hamster entertained a nearby\nbackpack and ﬁlled it with sawdust.\nOsterhout and Nicol\n(1999)\nThe cats won’t eat the food that\nMary gives them.\nThe cats won’t bake the food that\nMary gives them.\nOsterhout and Mobley\n(1995)\nThe plane sailed through the air and\nlanded on the runway.\nThe plane sailed through the air and\nlaughed on the runway.\nCommonsense\nWarren et al. (2015)\nCorey’s hamster explored a nearby\nbackpack and ﬁlled it with sawdust.\nCorey’s hamster lifted a nearby\nbackpack and ﬁlled it with sawdust.\nFedermeier and Kutas\n(1999)\n“Checkmate,” Rosalie announced\nwith glee. She was getting to be\nreally good at chess.\n“Checkmate,” Rosalie announced\nwith glee. She was getting to be\nreally good at monopoly.\nChow et al. (2016)\nThe restaurant owner forgot which\ncustomer the waitress had served.\nThe restaurant owner forgot which\nwaitress the customer had served.\nUrbach and Kutas\n(2010)\nProsecutors accuse defendants of\ncommitting a crime.\nProsecutors accuse sheriffs of\ncommitting a crime.\nTable 5.6: Example sentence pair for each of the 12 tasks. The 3 BLiMP tasks are generated from templates; the\nothers are stimuli materials taken from psycholinguistic studies.\nWithin a class, the anomalies are also heterogeneous (e.g., ungrammaticality may be due to violations of\nagreement, wh-movement, negative polarity item licensing, etc), which might each affect the LMs differently.\nThus, we deﬁne three classes of anomalies that do not attempt to cover all possible linguistic phenomena, but\ncaptures different levels of language processing while retaining internal uniformity:\n1. Morphosyntactic anomaly: an error in the inﬂected form of a word, for example, subject-verb agreement\n(*the boy eat the sandwich), or incorrect verb tense or aspect inﬂection (*the boy eaten the sandwich). In\neach case, the sentence can be corrected by changing the inﬂectional form of one word.\n2. Semantic anomaly: a violation of a selectional restriction, such as animacy (#the house eats the sandwich).\nIn these cases, the sentence can be corrected by replacing one of the verb’s arguments with another one in\nthe same word class that satisﬁes the verb’s selectional restrictions.\n3. Commonsense anomaly: sentence describes an situation that is atypical or implausible in the real world\nbut is otherwise acceptable (#the customer served the waitress).\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n52\n5.4.1\nSummary of anomaly datasets\nWe use two sources of data for experiments on linguistic anomalies: synthetic sentences generated from tem-\nplates, and materials from psycholinguistic studies. Both have advantages and disadvantages – synthetic data can\nbe easily generated in large quantities, but the resulting sentences may be odd in unintended ways. Psycholin-\nguistic stimuli are designed to control for confounding factors (e.g., word frequency) and human-validated for\nacceptability, but are smaller (typically fewer than 100 sentence pairs).\nWe curate a set of 12 tasks from BLiMP and 7 psycholinguistic studies1. Each sentence pair consists of a\ncontrol and an anomalous sentence, so that all sentences within a task differ in a consistent manner. Table 5.6\nshows an example sentence pair from each task. We summarize each dataset:\n1. BLiMP (Warstadt et al., 2020a): we use subject-verb and determiner-noun agreement tests as morphosyn-\ntactic anomaly tasks. For simplicity, we only use the basic regular sentences, and exclude sentences involv-\ning irregular words or distractor items. We also use the two argument structure tests involving animacy as\na semantic anomaly task. All three BLiMP tasks therefore have 2000 sentence pairs.\n2. Osterhout and Nicol (1999): contains 90 sentence triplets containing a control, syntactic, and semantic\nanomaly. Syntactic anomalies involve a modal verb followed by a verb in -ing form; semantic anomalies\nhave a selectional restriction violation between the subject and verb. There are also double anomalies\n(simultaneously syntactic and semantic) which we do not use.\n3. Pylkk¨anen and McElree (2007): contains 70 sentence pairs where the verb is replaced in the anomalous\nsentence with one that requires an animate object, thus violating the selectional restriction. In half the\nsentences, the verb is contained in an embedded clause.\n4. Warren et al. (2015): contains 30 sentence triplets with a possible condition, a selectional restriction vi-\nolation between the subject and verb, and an impossible condition where the subject cannot carry out the\naction, i.e., a commonsense anomaly.\n5. Osterhout and Mobley (1995): we use data from experiment 2, containing 90 sentence pairs where the\nverb in the anomalous sentence is semantically inappropriate. The experiment also tested gender agreement\nerrors, but we do not include these stimuli.\n6. Federmeier and Kutas (1999): contains 34 sentence pairs, where the ﬁnal noun in each anomalous sentence\nis an inappropriate completion, but in the same semantic category as the expected completion.\n1Several of these stimuli have been used in natural language processing research. Chersoni et al. (2018) used the data from Pylkk¨anen\nand McElree (2007) and Warren et al. (2015) to probe word vectors for knowledge of selectional restrictions. Ettinger (2020) used data from\nFedermeier and Kutas (1999) and Chow et al. (2016), which were referred to as CPRAG-102 and ROLE-88 respectively.\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n53\n7. Chow et al. (2016): contains 44 sentence pairs, where two of the nouns in the anomalous sentence are\nswapped to reverse their roles. This is the only task in which the sentence pair differs by more than one\ntoken.\n8. Urbach and Kutas (2010): contains 120 sentence pairs, where the anomalous sentence replaces a patient of\nthe verb with an atypical one.\n5.4.2\nQuantifying layerwise surprisal\nLet D = {(s1, s′\n1), · · · , (sn, s′\nn)} be a dataset of sentence pairs, where si is a control sentence and s′\ni is an\nanomalous sentence. For each layer L, we deﬁne the surprisal gap as the mean difference of surprisal scores\nbetween the control and anomalous sentences, scaled by the standard deviation:\nsurprisal gapL(D) = E{surprisalL(s′\ni) −surprisalL(si)}n\ni=1\nσ{surprisalL(s′\ni) −surprisalL(si)}n\ni=1\n(5.10)\nThe surprisal gap is a scale-invariant measure of sensitivity to anomaly, similar to a signal-to-noise ratio.\nWhile surprisal scores are unitless, the surprisal gap may be viewed as the number of standard deviations that\nanomalous sentences trigger surprisal above control sentences. This is advantageous over accuracy scores, which\ntreats the sentence pair as correct when the anomalous sentence has higher surprisal by any margin; this hard\ncutoff masks differences in the magnitude of surprisal. The metric also allows for fair comparison of surprisal\nscores across datasets of vastly different sizes. We plot the surprisal gap for all 12 tasks, using RoBERTa (Figure\n5.5), BERT (Figure 5.6), and XLNet (Figure 5.7).\nNext, we compare the performance of the Gaussian model with the masked language model (MLM). We score\neach instance as correct if the masked probability of the correct word is higher than the anomalous word. One\nlimitation of the MLM approach is that it requires the sentence pair to be identical in all places except for one\ntoken, since the LMs do not support modeling joint probabilities over multiple tokens. To ensure fair comparison\nbetween GM and MLM, we exclude instances where the differing token is out-of-vocabulary in any of the LMs\n(this excludes approximately 30% of instances). For the Gaussian model, we compute accuracy using the best-\nperforming layer for each model (Section 5.3.2). The results are listed in Table 5.7.\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n54\nCommonsense  −  Urbach and Kutas\nCommonsense  −  Chow et al.\nCommonsense  −  Federmeier and Kutas\nCommonsense  −  Warren et al.\nSemantic  −  Osterhout and Mobley\nSemantic  −  Osterhout and Nicol\nSemantic  −  Warren et al.\nSemantic  −  Pylkkänen and McElree\nSemantic  −  BLiMP (Animacy)\nMorphosyntax  −  Osterhout and Nicol\nMorphosyntax  −  BLiMP (Det−Noun)\nMorphosyntax  −  BLiMP (Subject−Verb)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\nLayer\nSurprisal Gap\nFigure 5.5: Layerwise surprisal gaps for all tasks using the RoBERTa model. Generally, a positive surprisal gap\nappears in earlier layers for morphosyntactic tasks than for semantic tasks; no surprisal gap appears at any layer\nfor commonsense tasks.\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n55\nCommonsense  −  Urbach and Kutas\nCommonsense  −  Chow et al.\nCommonsense  −  Federmeier and Kutas\nCommonsense  −  Warren et al.\nSemantic  −  Osterhout and Mobley\nSemantic  −  Osterhout and Nicol\nSemantic  −  Warren et al.\nSemantic  −  Pylkkänen and McElree\nSemantic  −  BLiMP (Animacy)\nMorphosyntax  −  Osterhout and Nicol\nMorphosyntax  −  BLiMP (Det−Noun)\nMorphosyntax  −  BLiMP (Subject−Verb)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n−1\n0\n1\n−1\n0\n1\n−1\n0\n1\n−1\n0\n1\n−1\n0\n1\n−1\n0\n1\n−1\n0\n1\n−1\n0\n1\n−1\n0\n1\n−1\n0\n1\n−1\n0\n1\n−1\n0\n1\nLayer\nSurprisal Gap\nFigure 5.6: Layerwise surprisal gaps for all tasks using the BERT model.\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n56\nCommonsense  −  Urbach and Kutas\nCommonsense  −  Chow et al.\nCommonsense  −  Federmeier and Kutas\nCommonsense  −  Warren et al.\nSemantic  −  Osterhout and Mobley\nSemantic  −  Osterhout and Nicol\nSemantic  −  Warren et al.\nSemantic  −  Pylkkänen and McElree\nSemantic  −  BLiMP (Animacy)\nMorphosyntax  −  Osterhout and Nicol\nMorphosyntax  −  BLiMP (Det−Noun)\nMorphosyntax  −  BLiMP (Subject−Verb)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nLayer\nSurprisal Gap\nFigure 5.7: Layerwise surprisal gaps for all tasks using the XLNet model.\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n57\nType\nTask\nSize\nBERT\nRoBERTa\nXLNet\nGM\nMLM\nGM\nMLM\nGM\nMLM\nMorphosyntax\nBLiMP (Subject-Verb)\n2000\n0.953\n0.955\n0.971\n0.957\n0.827\n0.584\nBLiMP (Det-Noun)\n2000\n0.970\n0.999\n0.983\n0.999\n0.894\n0.591\nOsterhout and Nicol (1999)\n90\n1.000\n1.000\n1.000\n1.000\n0.901\n0.718\nSemantic\nBLiMP (Animacy)\n2000\n0.644\n0.787\n0.767\n0.754\n0.675\n0.657\nPylkk¨anen and McElree (2007)\n70\n0.727\n0.955\n0.932\n0.955\n∗0.636\n0.727\nWarren et al. (2015)\n30\n∗0.556\n1.000\n0.944\n1.000\n∗0.667\n∗0.556\nOsterhout and Nicol (1999)\n90\n0.681\n0.957\n0.841\n1.000\n∗0.507\n0.783\nOsterhout and Mobley (1995)\n90\n∗0.528\n1.000\n0.906\n0.981\n∗0.302\n0.774\nCommonsense\nWarren et al. (2015)\n30\n∗0.600\n∗0.550\n0.750\n∗0.450\n∗0.300\n∗0.600\nFedermeier and Kutas (1999)\n34\n∗0.458\n∗0.708\n∗0.583\n0.875\n∗0.625\n∗0.667\nChow et al. (2016)\n44\n∗0.591\nn/a\n∗0.432\nn/a\n∗0.568\nn/a\nUrbach and Kutas (2010)\n120\n∗0.470\n0.924\n∗0.485\n0.939\n∗0.500\n0.712\nTable 5.7: Comparing accuracy scores between Gaussian anomaly model (GM) and masked language model\n(MLM) for all models and tasks. Asterisks indicate that the accuracy is not better than random (0.5), using a\nbinomial test with threshold of p < 0.05 for signiﬁcance. The MLM results for Chow et al. (2016) are excluded\nbecause the control and anomalous sentences differ by more than one token. The best layers for each model\n(Section 5.3.2) are used for GM, and the last layer is used for MLM. Generally, MLM outperforms GM, and the\ndifference is greater for semantic and commonsense tasks.\n5.5\nDiscussion\n5.5.1\nAnomaly type and surprisal\nWe ﬁrst discuss the results from RoBERTa (Figure 5.5), where morphosyntactic anomalies generally appear ear-\nlier than semantic anomalies. The surprisal gap plot exhibits different patterns depending on the type of linguistic\nanomaly: morphosyntactic anomalies produce high surprisal relatively early (layers 3-4), while semantic anoma-\nlies produce low surprisals until later (layers 9 and above). Commonsense anomalies do not result in surprisals\nat any layer: the surprisal gap is near zero for all of the commonsense tasks. The observed difference between\nmorphosyntactic and semantic anomalies is consistent with previous work (Tenney et al., 2019b), which found\nthat syntactic information appeared earlier in BERT than semantic information.\nOne should be careful and avoid drawing conclusions from only a few experiments. A similar situation\noccurred in psycholinguistics research (Kutas et al., 2006): early results suggested that the N400 was triggered by\nsemantic anomalies, while syntactic anomalies triggered the P600 – a different type of ERP. However, subsequent\nexperiments found exceptions to this rule, and now it is believed that the N400 cannot be categorized by any\nstandard dichotomy, like syntax versus semantics (Kutas and Federmeier, 2011). In our case, Pylkk¨anen and\nMcElree (2007) is an exception: the task is a semantic anomaly, but produces surprisals in early layers, similar\nto the morphosyntactic tasks. Hence it is possible that the dichotomy is something other than syntax versus\nsemantics; we leave to future work to determine more precisely what conditions trigger high surprisals in lower\nversus upper layers of LMs.\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n58\n5.5.2\nComparing anomaly model with MLM\nThe masked language model (MLM) usually outperforms the Gaussian anomaly model (GM), but the difference\nis uneven. MLM performs much better than GM on commonsense tasks, slightly better on semantic tasks, and\nabout the same or slightly worse on morphosyntactic tasks. It is not obvious why MLM should perform better\nthan GM, but we note two subtle differences between the MLM and GM setups that may be contributing factors.\nFirst, the GM method adds up the surprisal scores for the whole sequence, while MLM only considers the softmax\ndistribution at one token. Second, the input sequence for MLM always contains a [MASK] token, whereas GM\ntakes the original unmasked sequences as input, so the representations are never identical between the two setups.\nMLM generally outperforms GM, but it does not solve every task: all three LMs fail to perform above chance\non the data from Warren et al. (2015). This set of stimuli was designed so that both the control and impossible\ncompletions are not very likely or expected, which may have caused the difﬁculty for the LMs. We excluded\nthe task of Chow et al. (2016) for MLM because the control and anomalous sentences differed by more than one\ntoken2.\n5.5.3\nDifferences between LMs\nRoBERTa is the best-performing of the three LMs in both the GM and MLM settings: this is expected since it\nis trained with the most data and performs well on many natural language benchmarks. Surprisingly, XLNet is\nill-suited for this task and performs worse than BERT, despite having a similar model capacity and training data.\nThe surprisal gap plots for BERT (Figure 5.6) and XLNet (Figure 5.7) show some differences from RoBERTa:\nonly morphosyntactic tasks produce out-of-domain embeddings in these two models, and not semantic or com-\nmonsense tasks. Evidently, how LMs behave when presented with anomalous inputs is dependent on model\narchitecture and training data size; we leave exploration of this phenomenon to future work.\n5.6\nConclusion\nWe used Gaussian models to characterize out-of-domain embeddings at intermediate layers of Transformer lan-\nguage models. The model requires a relatively small amount of in-domain data. Our experiments revealed that\nout-of-domain points in lower layers correspond to low-frequency tokens, while grammatically anomalous inputs\nare out-of-domain in higher layers. Furthermore, morphosyntactic anomalies are recognized as out-of-domain\nstarting from lower layers compared to syntactic anomalies. Commonsense anomalies do not generate out-of-\n2Sentence pairs with multiple differing tokens are inconvenient for MLM to handle, but this is not a fundamental limitation. For example,\nSalazar et al. (2020) proposed a modiﬁcation to MLM to handle such cases: they compute a pseudo-log-likelihood score for a sequence by\nreplacing one token at a time with a [MASK] token, applying MLM to each masked sequence, and summing up the log likelihood scores.\nCHAPTER 5. LINGUISTIC ANOMALIES IN LMS\n59\ndomain embeddings at any layer, even when the LM has a preference for the correct cloze completion. These\nresults show that depending on the type of linguistic anomaly, LMs use different mechanisms to produce the\noutput softmax distribution.\nChapter 6\nConstruction grammar in LMs\nThe contents of this chapter are based on my previous publication (Li et al., 2022).\n6.1\nIntroduction\nThis chapter continues our theme of probing language models using methods derived from linguistic theory.\nWhile the previous chapter focused on linguistic anomalies, here we shift our attention to examining argument\nstructure in construction grammar theories and their representations in language models. Most probing work so\nfar has investigated the linguistic knowledge of LMs on phenomena such as agreement, binding, licensing, and\nmovement (Warstadt et al., 2020a; Hu et al., 2020) with a particular focus on determining whether a sentence is\nlinguistically acceptable (Sch¨utze, 1996). Relatively little work has attempted to determine whether the linguistic\nknowledge induced by LMs is more similar to a formal grammar of the sort postulated by mainstream generative\nlinguistics (Chomsky, 1965, 1981, 1995), or to a network of form-meaning pairs as advocated by construction\ngrammar (Goldberg, 1995, 2006).\nOne area where construction grammar disagrees with many generative theories of language is in the analysis\nof the argument structure of verbs, that is, the speciﬁcation of the number of arguments that a verb takes, their\nsemantic relation to the verb, and their syntactic form (Levin and Rappaport Hovav, 2005). Lexicalist theories\nwere long dominant in generative grammar (Chomsky, 1981; Kaplan and Bresnan, 1982; Pollard and Sag, 1987).\nIn lexicalist theories, argument structure is assumed to be encoded in the lexical entry of the verb: for example,\nthe verb visit is lexically speciﬁed as being transitive and as requiring a noun phrase object (Chomsky, 1986).\nIn contrast, construction grammar suggests that argument structure is encoded in form-meaning pairs known as\nargument structure constructions (ASCs, Figure 6.1), which are distinct from verbs. The argument structure of a\nverb is determined by pairing it with an ASC (Goldberg, 1995). To date, a substantial body of psycholinguistic\n60\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n61\nTransitive\nBob cut the bread\nS V O\nS acts on O\nDitransitive\nBob cut Joe the bread\nS V O1 O2\nS transfers O2 to O1\nCaused motion\nBob cut the bread into the pan\nS V O Path\nS causes O to move via Path\nResultative\nBob cut the bread apart\nS V O State\nS causes O to become State\nFigure 6.1: Four argument structure constructions (ASCs) used by Bencini and Goldberg (2000), with example\nsentences (top right). Constructions are mappings between form (bottom left) and meaning (bottom right).\nwork has provided evidence for the psychological reality of ASCs in sentence sorting (Bencini and Goldberg,\n2000; Gries and Wulff, 2005), priming (Ziegler et al., 2019), and novel verb experiments (Kaschak and Glenberg,\n2000; Johnson and Goldberg, 2013).\nHere we connect basic research in ASCs with neural probing by adapting several psycholinguistic studies\nto Transformer-based LMs and show evidence for the neural reality of ASCs. Our ﬁrst case study is based on\nsentence sorting (Bencini and Goldberg, 2000); we discover that in English, German, Italian, and Spanish, LMs\nconsider sentences that share the same construction to be more semantically similar than sentences sharing the\nmain verb. Furthermore, this preference for constructional meaning only manifests in larger LMs (trained with\nmore data), whereas smaller LMs rely on the main verb, an easily accessible surface feature. Human experiments\nwith non-native speakers found a similarly increased preference for constructional meaning in more proﬁcient\nspeakers (Liang, 2002; Baicchi and Della Putta, 2019), suggesting commonalities in language acquisition between\nLMs and humans.\nOur second case study is based on nonsense “Jabberwocky” sentences that nevertheless convey meaning when\nthey are arranged in constructional templates (Johnson and Goldberg, 2013). We adapt the original priming\nexperiment to LMs and show that RoBERTa is able to derive meaning from ASCs, even without any lexical\ncues. This ﬁnding offers counter-evidence to earlier claims that LMs are relatively insensitive to word order when\nconstructing sentence meaning (Yu and Ettinger, 2020; Sinha et al., 2021). Our source code and data are available\nat: https://github.com/SPOClab-ca/neural-reality-constructions.\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n62\n6.2\nLinguistic background\n6.2.1\nConstruction grammar and ASCs\nConstruction grammar is a family of linguistic theories proposing that all linguistic knowledge consists of con-\nstructions: pairings between form and meaning where some aspects of form or meaning are not predictable from\ntheir parts (Fillmore et al., 1988; Kay and Fillmore, 1999; Goldberg, 1995, 2006). Common examples include\nidiomatic expressions such as under the weather (meaning “to feel unwell”), but many linguistic patterns are\nconstructions, including morphemes (e.g., -ify), words (e.g., apple), and abstract patterns like the ditransitive and\npassive. In contrast to lexicalist theories of argument structure, construction grammar rejects the dichotomy be-\ntween syntax and lexicon. In contrast to transformational grammar, it rejects any distinction between surface and\nunderlying structure.\nWe focus on a speciﬁc family of constructions for which there is an ample body of psycholinguistic evidence:\nargument structure constructions (ASCs). ASCs are constructions that specify the argument structure of a verb\n(Goldberg, 1995). In the lexicalist, verb-centered view, argument structure is a lexical property of the verb, and the\nmain verb of a sentence determines the form and meaning of the sentence (Chomsky, 1981; Kaplan and Bresnan,\n1982; Pollard and Sag, 1987; Levin and Rappaport Hovav, 1995). For example, sneeze is intransitive (allowing no\ndirect object) and hit is transitive (requiring one direct object). However, lexicalist theories encounter difﬁculties\nwith sentences like “he sneezed the napkin off the table” since intransitive verbs are not permitted to have object\narguments.\nRather than assuming multiple implausible senses for the verb “sneeze” with different argument structures,\nGoldberg (1995) proposed that ASCs operate on an arbitrary verb, altering its argument structure while at the\nsame time modifying its meaning. For example, the caused-motion ASC adds a direct object and a path argument\nto the verb sneeze, with the semantics of causing the object to move along the path. Other ASCs include the\ntransitive, ditransitive, and resultative (Figure 6.1), which specify the argument structure of a verb and interact\nwith its meaning in different ways.\n6.2.2\nPsycholinguistic evidence for ASCs\nSentence sorting. Several psycholinguistic studies have found evidence for argument structure constructions\nusing experimental methods. Among these, Bencini and Goldberg (2000) used a sentence sorting task to determine\nwhether the verb or construction in a sentence was the main determinant of sentence meaning. 17 participants\nwere given 16 index cards with sentences containing 4 verbs (throw, get, slice, and take) and 4 constructions\n(transitive, ditransitive, caused-motion, and resultative) and were instructed to sort them into 4 piles by overall\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n63\nTransitive\nDitransitive\nCaused-motion\nResultative\nThrow\nAnita threw the ham-\nmer.\nChris threw Linda the\npencil.\nPat threw the keys onto\nthe roof.\nLyn\nthrew\nthe\nbox\napart.\nGet\nMichelle got the book.\nBeth got Liz an invita-\ntion.\nLaura got the ball into\nthe net.\nDana got the mattress\ninﬂated.\nSlice\nBarbara\nsliced\nthe\nbread.\nJennifer sliced Terry an\napple.\nMeg\nsliced\nthe\nham\nonto the plate.\nNancy sliced the tire\nopen.\nTake\nAudrey took the watch.\nPaula took Sue a mes-\nsage.\nKim took the rose into\nthe house.\nRachel took the wall\ndown.\nTable 6.1: Stimuli from Bencini and Goldberg (2000), consisting of a 4x4 design, with 4 different verbs and 4\ndifferent argument structure constructions.\nsentence meaning (Table 6.1). The experimenters measured the deviation to a purely verb-based or construction-\nbased sort, and found that on average, the piles were closer to a construction sort.\nNon-native sentence sorting. The same set of experimental stimuli was used with L2 (non-native) En-\nglish speakers. Gries and Wulff (2005) ran the experiment with 22 German native speakers, who preferred the\nconstruction-based sort over the verb-based sort, showing that constructional knowledge is not limited to native\nspeakers. Liang (2002) ran the experiment on Chinese native speakers of 3 different English levels (46 beginner,\n31 intermediate, and 33 advanced), and found that beginners preferred a verb-based sort, while advanced learn-\ners produced construction-based sorts similar to native speakers (Figure 6.2). Likewise, Baicchi and Della Putta\n(2019) found the same result in Italian native speakers with B1 and B2 English proﬁciency levels. Overall, these\nstudies show evidence for ASCs in the mental representations of native and L2 English speakers alike, and fur-\nthermore, preference for constructional over verb sorting increases with increasing English proﬁciency.\nMultilingual sentence sorting. Similar sentence sorting experiments have been conducted in other languages,\nwith varying results. Kirsch (2019) ran a sentence sorting experiment in German with 40 participants and found\nthat they mainly sorted by verb but rarely by construction. Baicchi and Della Putta (2019) ran an experiment with\nnon-native learners of Italian (15 participants of B1 level and 10 participants of B2 level): both groups preferred\nthe constructional sort, and similar to Liang (2002), the B2 learners sorted more by construction than the B1\nlearners. V´azquez (2004) ran an experiment in Spanish with 16 participants, and found approximately equal\nproportions of constructions and verb sort. In Italian and Spanish, some different constructions were substituted\nas not all of the English constructions had an equivalent in these languages.\nPriming. Another line of psycholinguistic evidence comes from priming studies. Priming refers to the condi-\ntion where exposure to a (prior) stimulus inﬂuences the response to a later stimulus (Pickering and Ferreira, 2008).\nBock and Loebell (1990) found that participants were more likely to produce sentences of a given syntactic struc-\nture when primed with a sentence of the same structure; Ziegler et al. (2019) argued that Bock and Loebell (1990)\ndid not adequately control for lexical overlap, and instead, they showed that the construction must be shared for\nthe priming effect to occur, not just shared abstract syntax.\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n64\nNovel verbs. Even with unfamiliar words, there is evidence that constructions are associated with meaning.\nKaschak and Glenberg (2000) constructed sentences with novel denominal verbs and found that participants were\nmore likely to interpret a transfer event when the denominal verb was used in a ditransitive sentence (Tom crutched\nLyn an apple) than a transitive one (Tom crutched an apple).\nJohnson and Goldberg (2013) used a “Jabberwocky” priming task to show that abstract constructional tem-\nplates are associated with meaning. Participants were primed with a nonsense sentence of a given construction\n(e.g., He daxed her the norp for the ditransitive construction), followed by a lexical decision task of quickly\ndeciding if a string of characters was a real English word or a non-word. The word in the decision task was\nsemantically congruent with the construction (gave) or incongruent (made); furthermore, they experimented with\ntarget words that were high-frequency (gave), low-frequency (handed), or semantically related but not associ-\nated with the construction (transferred). They found priming effects (faster lexical decision times) in all three\nconditions, with the strongest effect for the high-frequency condition, followed by the low-frequency and the\nsemantically nonassociate conditions.\n6.2.3\nRelated work in NLP\nChapter 3 of this thesis surveyed recent work in language model probing based on linguistic theories, although\nrelatively few of them have approached probing from a construction grammar perspective. Madabushi et al.\n(2020) probed for BERT’s knowledge of constructions via a sentence pair classiﬁcation task of predicting whether\ntwo sentences share the same construction. Their probe was based on data from Dunn (2017), who used an\nunsupervised algorithm to extract plausible constructions from corpora based on association strength. However,\nthe linguistic validity of these automatically induced constructions is uncertain, and there is currently no human-\nlabelled wide-coverage construction grammar dataset in any language suitable for probing. Other computational\nwork focused on a few speciﬁc constructions, such as identifying caused-motion constructions in corpora (Hwang\nand Palmer, 2015) and annotating constructions related to causal language (Dunietz et al., 2015). Lebani and\nLenci (2016) is the most similar to our work: they probed distributional vector space models for ASCs based on\nthe Jabberwocky priming experiment by Johnson and Goldberg (2013).\nIn this work, we adapt several of the previously mentioned psycholinguistic studies to LMs: the sentence\nsorting experiments in Case study 1, and the Jabberwocky priming experiment in Case study 2. We choose\nthese studies because their designs allow for thousands of stimuli sentences to be generated automatically using\ntemplates, avoiding issues caused by small sample sizes from manually constructed sentences.\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n65\n6.3\nCase study 1: Sentence sorting\nThis section describes our adaptation of the sentence sorting experiments to Transformer LMs.\n6.3.1\nMethodology\nModels. To simulate varying non-native English proﬁciency levels, we use MiniBERTa models (Warstadt et al.,\n2020b), trained with 1M, 10M, 100M, and 1B tokens. We also use the base RoBERTa model (Liu et al., 2019b),\ntrained with 30B tokens. In other languages, there are no available pretrained checkpoints with varying amounts\nof pretraining data, so we use the mBERT model (Devlin et al., 2019) and a monolingual Transformer LM in\neach language.1 We obtain sentence embeddings for our models by taking the average of their contextual token\nembeddings at the second-to-last layer (i.e., layer 11 for base RoBERTa). We use the second-to-last because the\nlast layer is more specialized for the LM pretraining objective and less suitable for sentence embeddings (Liu\net al., 2019a).\nTemplate generation. We use templates to generate stimuli similar to the 4x4 design in the Bencini and\nGoldberg (2000) experiment. To ensure an adequate sample size, we run multiple empirical trials. In each trial,\nwe sample 4 random distinct verbs from a pool of 10 verbs that are compatible with all 4 constructions (cut,\nhit, get, kick, pull, punch, push, slice, tear, throw). We then randomly ﬁll in the slots for proper names, objects,\nand complements for each sentence according to its verb, such that the sentence is semantically coherent, and\nthere is no lexical overlap among the sentences of any construction. Table 6.2 shows a set of template-generated\nsentences. In English, we generate 1000 sets of stimuli using this procedure.\nFor other languages, we use the original stimuli from their respective publications. We present the sentence\nsorting stimuli for German (Table 6.3), Italian (Table 6.4), and Spanish (Table 6.5). German uses the same\nfour constructions as English. Italian does not have the ditransitive construction but instead uses the prepositional\ndative construction to express transfer semantics. Spanish has no equivalents for the caused-motion and resultative\nconstructions, so the authors in that experiment instead used the unplanned reﬂexive (expressing accidental or\nunplanned events), and the middle construction (expressing states pertaining to the subject).\nEvaluation. Similar to the human experiments, we group the sentence embeddings into 4 clusters (not nec-\nessarily of the same size) using agglomerative clustering by Euclidean distance (Pedregosa et al., 2011). We\nthen compute the deviation to a pure construction and pure verb sort using the Hungarian algorithm for optimal\nbipartite matching. This measures the minimal number of cluster assignment changes necessary to reach a pure\nconstruction or verb sort, ranging from 0 to 12. Thus, lower construction deviation indicates that constructional\n1We use monolingual German and Italian models from https://github.com/dbmdz/berts, and the monolingual Spanish model\nfrom Ca˜nete et al. (2020).\n2Bencini and Goldberg (2000) ran the sentence sorting experiment twice, so we take the average of the two runs.\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n66\nTransitive\nDitransitive\nCaused-motion\nResultative\nSlice\nHarry sliced the bread.\nHenry sliced Eric the\nbox.\nSam sliced the ball onto\nthe bed.\nJohn sliced the book\napart.\nKick\nThomas kicked the box.\nMike kicked Frank the\nball.\nMichael kicked the wall\ninto the house.\nJames kicked the door\nopen.\nCut\nGeorge cut the ball.\nAdam cut Paul the tree.\nBill cut the box into the\nwater.\nBob cut the bread apart.\nGet\nTom got the book.\nAndrew got Steve the\ndoor.\nJack got the fridge onto\nthe elevator.\nDavid\ngot\nthe\nball\nstuck.\nTable 6.2: Example of our 4x4 sentence sorting stimuli, similar to those by Bencini and Goldberg (2000) in Table\n6.1, but generated automatically using templates.\nTransitive\nDitransitive\nCaused-motion\nResultative\nWerfen\nAnita warf den Ham-\nmer.\nBerta warf Linda den\nBleistift.\nErika\nwarf\nden\nSchl¨usselbund\nauf\ndas Dach.\nLaura warf die Kisten\nauseinander.\nBringen\nMichelle\nbrachte\ndas\nBuch.\nSimone brachte Lydia\neine Einladung.\nEmma brachte den Ball\nins Netz.\nLeonie\nbrachte\ndie\nSt¨uhle zusammen.\nSchneiden\nKarolin\nschnitt\ndas\nBrot.\nLuisa\nschnitt\nPaula\neinen Apfel.\nJennifer\nschnitt\ndie\nWurst auf den Teller.\nDoris\nschnitt\nden\nReifen auf.\nNehmen\nMaria nahm die Uhr.\nSophia\nnahm\nJasmin\ndas Geld.\nHelena nahm die Rosen\nin das Haus.\nTheresa\nnahm\ndas\nPlakat herunter.\nTable 6.3: German sentence sorting stimuli, obtained from Kirsch (2019).\nTransitive\nPrepositional Dative\nCaused-motion\nResultative\nDare\nLauda d`a un esame.\nCarlo d`a una mela a\nMaria.\nLuca d`a una spinta a\nFranco.\nPaolo d`a una verniciata\ndi verde alla porta.\nFare\nMario fa una torta.\nLuigi fa un piacere a\nGiovanna.\nFabio\nfa\nentrare\nla\nmacchina in garage.\nStefano fa bruciare il\nsugo.\nMettere\nAnnalisa mette la gi-\nacca.\nRiccardo mette il cap-\npello al bambino.\nSilvia mette la penna\nnel cassetto.\nFilippo mette la casa in\nordine.\nPortare\nLinda porta lo zaino.\nLaura porta la pizza a\nFrancesco.\nMichele porta il libro in\nbiblioteca.\nIrene porta l’esercizio a\ntermine.\nTable 6.4: Italian sentence sorting stimuli, obtained from Baicchi and Della Putta (2019).\nTransitive\nDitransitive\nUnplanned Reﬂexive\nMiddle\nRomper\nCarlos rompi´o el cristal.\nAlfonso le rompi´o las\ngafas a Pepe.\nA Juan se le rompieron\nlos pantalones.\nLa porcelana se rompe\ncon facilidad.\nDoblar\nFelipe\ndobl´o\nel\nperi´odico.\nPablo le dobl´o el brazo\na Lucas.\nA Pedro se le dobl´o el\ntobillo.\nEl aluminio se dobla\nbien.\nAcabar\nLeonardo\nacab´o\nsu\ntesis.\nTom´as le acab´o la pasta\nde dientes a Santi.\nA Luis se le acabaron\nlos cigarrillos.\nLas carreras de 10 km\nse acaban sin proble-\nmas.\nCortar\nIsidro cort´o el pan.\nJorge le cort´o el paso a\nYago.\nA Ignacio se le cort´o la\nconexi´on.\nEsta tela se corta muy\nbien.\nTable 6.5: Spanish sentence sorting stimuli, obtained from V´azquez (2004).\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n67\n0\n2\n4\n6\n8\n10\n12\n1M\n10M\n100M\n1B\n30B\nPretraining Data Size\nHuman data                                         Language models\nValue\n0\n2\n4\n6\n8\n10\n12\nBeginner\nIntermediate\nAdvanced\nNative\nLevel\nCDev\nVDev\nFigure 6.2: English sentence sorting results for humans and LMs, measured by deviation from pure construction\nand verb sort (CDev and VDev). Non-native human results are from Liang (2002); native human results from\nBencini and Goldberg (2000).2LM results are obtained using MiniBERTas (Warstadt et al., 2020b) and RoBERTa\n(Liu et al., 2019b) on templated stimuli. The MiniBERTa models use between 1M to 1B tokens for pretraining,\nwhile RoBERTa uses 30B tokens. Error bars indicate 95% conﬁdence intervals.\ninformation is more salient in the LM’s embeddings.\n6.3.2\nResults and interpretation\nFigure 6.2 shows the LM sentence sorting results for English. All differences are statistically signiﬁcant (p <\n.001). The smallest 1M MiniBERTa model is the only LM to prefer verb over construction sorting, and as the\namount of pretraining data grows, the LMs increasingly prefer sorting by construction instead of by verb. This\nclosely mirrors the trend observed in the human experiments. To visualize this effect, we apply principal compo-\nnents analysis (PCA) on sentence embeddings for the 1M and 100M token MiniBERTa models and RoBERTa-base\n(Figure 6.3). In RoBERTa, there is strong evidence of clustering based on constructions; the effect is unclear in\nthe 100M model and nonexistent in the 1M model, visually conﬁrming our quantitative evaluation based on the\nconstruction and verb deviation metrics.\nThe results for multilingual sorting are shown in Figure 6.4. Both mBERT and the monolingual LMs consis-\ntently prefer constructional sorting over verb sorting in all three languages, whereas the results from the human\nexperiments are less consistent.\nOur results show that RoBERTa can generalize meaning from abstract constructions without lexical overlap.\nOnly larger LMs and English speakers of more advanced proﬁciency are able to make this generalization, while\nsmaller LMs and less proﬁcient speakers derive meaning more from surface features like lexical content. This\nﬁnding agrees with Warstadt et al. (2020b), who found that larger LMs have an inductive bias towards linguistic\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n68\nConstruction\nVerb\n1M\n100M\n30B\nFigure 6.3: PCA plots of Bencini and Goldberg (2000) sentence sorting using the 1M and 100M MiniBERTa\nmodels and RoBERTa-base (30B). Figure best viewed in color.\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n69\n0\n2\n4\n6\n8\n10\n12\nmBERT\nmono\nmBERT\nmono\nmBERT\nmono\nCDev\nVDev\nGerman\nItalian\nSpanish\nHuman data\nLanguage models\n0\n2\n4\n6\n8\n10\n12\nGerman\nItalian B1\nItalian B2\nSpanish\nValue\nFigure 6.4: Multilingual sentence sorting results for German (Kirsch, 2019), Italian (Baicchi and Della Putta,\n2019), and Spanish (V´azquez, 2004). LM results are obtained using the same stimuli; we use both mBERT and a\nmonolingual LM for each language.\ngeneralizations, while smaller LMs have an inductive bias towards surface generalizations; this may explain the\nsuccess of large LMs on downstream tasks. A small quantity of data (10M tokens) is sufﬁcient for LMs to prefer\nthe constructional sort, indicating that ASCs are relatively easy to learn: roughly on par with other types of\nlinguistic knowledge, and requiring less data than commonsense knowledge (Zhang et al., 2021; Liu et al., 2021).\nWe note some limitations in these results, and reasons to avoid drawing unreasonably strong conclusions from\nthem. Human sentence sorting experiments can be inﬂuenced by minor differences in the experimental setup:\nBencini and Goldberg (2000) obtained signiﬁcantly different results in two runs that only differed on the precise\nwording of instructions. In the German experiment (Kirsch, 2019), the author hypothesized that the participants\nwere inﬂuenced by a different experiment that they had completed before the sentence sorting one. Given this\nexperimental variation, we cannot attribute differences across languages to differences in their linguistic typology.\nAlthough LMs do not suffer from the same experimental variation, we cannot conclude statistical signiﬁcance\nfrom the multilingual experiments, where only one set of stimuli is available in each language.\n6.4\nCase study 2: Jabberwocky constructions\nWe next adapt the “Jabberwocky” priming experiment from Johnson and Goldberg (2013) to LMs, and make\nseveral changes to the original setup to better assess the capabilities of LMs. Priming is a standard experimental\nparadigm in psycholinguistic research, but it is not directly applicable to LMs: existing methods simulate priming\neither by applying additional ﬁne-tuning (Prasad et al., 2019), or by concatenating sentences that typically do not\nco-occur in natural text (Misra et al., 2020). Therefore, we instead propose a method to probe LMs for the same\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n70\nConstruction\nTemplate / Examples\nDitransitive\nS/he V-ed him/her the N.\nShe traded her the epicenter.\nHe ﬂew her the donut.\nResultative\nS/he V-ed it Adj.\nHe cut it seasonal.\nShe surged it civil.\nCaused-motion\nS/he V-ed it on the N.\nHe registered it on the diamond.\nShe awarded it on the corn.\nRemoval\nS/he V-ed it from him/her.\nHe declined it from her.\nShe drove it from him.\nTable 6.6: Templates and example sentences for the Jabberwocky construction experiments. The templates are\nidentical to the ones used in Johnson and Goldberg (2013), except that we use random real words instead of nonce\nwords.\nShe traded her the epicenter\ngave\nmade\nput\ntook\nFigure 6.5: In our adapted Jabberwocky experiment, we measure the Euclidean distance from the Jabberwocky\nverb (traded) to the 4 prototype verbs, of which 1 is congruent (\u0013) with the construction of the sentence, and 3\nare incongruent (\u0017).\nlinguistic information using only distance measurements on their contextual embeddings.\n6.4.1\nMethodology\nTemplate generation. We generate sentences for the four constructions randomly using the templates in Table\n6.6. Instead of ﬁlling nonce words like norp into the templates as in the original study, we take an approach similar\nto Gulordava et al. (2018) and generate 5000 sentences for each construction by randomly ﬁlling real words of\nthe appropriate part-of-speech into construction templates (Table 6.6). This gives nonsense sentences like “She\ntraded her the epicenter”; we refer to these random words as Jabberwocky words. By using real words, we avoid\nany potential instability from feeding tokens into the model that it has never seen during pretraining. We obtain\na set of singular nouns, past tense verbs, and adjectives from the Penn Treebank (Marcus et al., 1993), excluding\nwords with fewer than 10 occurrences.\nVerb embeddings. Our probing strategy is based on the assumption that the contextual embedding for a verb\ncaptures its meaning in context. Therefore, if LMs associate ASCs with meaning, we should expect the contextual\nembedding for the Jabberwocky verb to contain the meaning of the construction. Speciﬁcally, we measure the\nEuclidean distance to a prototype verb for each construction (Figure 6.5). These are verbs that Johnson and\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n71\nCongruent\nIncongruent\nHigh frequency\nLow frequency\n11.0\n11.5\n12.0\n12.5\n13.0\nCongruent\nIncongruent\nEuclidean distance\nFigure 6.6: Euclidean distance between Jabberwocky and prototype verbs for congruent and incongruent condi-\ntions. Error bars indicate 95% conﬁdence intervals.\nGoldberg (2013) selected whose meaning closely resembles the construction’s meaning: gave, made, put, and\ntook for the ditransitive, resultative, caused-motion, and removal constructions, respectively.3 We also run the\nsame setup using lower frequency prototype verbs from the same study: handed, turned, placed, and removed.4\nAs a control, we measure the Euclidean distance to the prototype verbs of the other three unrelated constructions.\nThe prototype verb embeddings are generated by taking the average across their contextual embeddings across\na 4M-word subset of the British National Corpus (BNC; Leech (1992)). We use the second-to-last layer of\nRoBERTa-base, and in cases where a verb is split into multiple subwords, we take the embedding of the ﬁrst\nsubword token as the verb embedding.\n6.4.2\nResults and interpretation\nWe ﬁnd that the Euclidean distance between the prototype and Jabberwocky verb embeddings is signiﬁcantly\nlower (p < .001) when the verb is congruent with the construction than when they are incongruent, and this is\nobserved for both high and low-frequency prototype verbs (Figure 6.6). Examining the individual constructions\nand verbs (Figure 6.7), we note that in the high-frequency scenario, the lowest distance prototype verb is always\nthe congruent one, for all four constructions. In the low-frequency scenario, the result is less consistent: the\ncongruent verb is not always the lowest distance one, although it is always still at most the second-lowest distance\nout of the four.\nThe main result holds for both high and low-frequency scenarios, but the correct prototype verb is associated\nmore consistently in the high-frequency case. This agrees with Wei et al. (2021), who found that LMs have greater\ndifﬁculty learning the linguistic properties of less frequent words. We also note that the Euclidean distances are\n3The reader may notice that the four constructions here are slightly different from Bencini and Goldberg (2000): the transitive construction\nis replaced with the removal construction in Johnson and Goldberg (2013).\n4Johnson and Goldberg (2013) also included a third experimental condition using four verbs that are semantically related but not associated\nwith the construction, but one of the verbs is very low-frequency (ousted), so we exclude this condition in our experiment.\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n72\n11.899\n12.295\n12.567\n12.328\n11.924\n11.701\n11.868\n11.864\n11.691\n11.593\n11.395\n11.599\n11.740\n11.954\n11.936\n11.517\ngave\nmade\nput\ntook\nremoval\ncaused−motion\nresultative\nditransitive\n12.008\n12.939\n13.141\n13.677\n12.230\n12.466\n12.562\n13.095\n11.791\n12.142\n11.906\n12.742\n11.860\n12.420\n12.651\n12.246\nhanded\nturned\nplaced removed\nremoval\ncaused−motion\nresultative\nditransitive\nHigh frequency\nLow frequency\nFigure 6.7: Mean Euclidean distance between Jabberwocky and prototype verbs in each verb-construction pair.\nDiagonal entries (gray border) are the congruent conditions; off-diagonal entries are incongruent.\nhigher overall in the low-frequency scenario, which is consistent with previous work that found lower frequency\nwords to occupy a peripheral region of the embedding space (Li et al., 2021).\n6.4.3\nPotential confounds\nIn any experiment, one must be careful to ensure that the observed patterns are due to the phenomenon un-\nder investigation rather than confounding factors. We discuss potential confounds arising from lexical overlap,\nanisotropy of contextual embeddings, and neighboring words.\nLexical overlap. The randomized experiment design ensures that the Jabberwocky words cannot be lexically\nbiased towards any construction, since each verb is equally likely to occur in every construction. Technically,\nthe lexical content in the four constructions are not identical: i.e., words like “from” (occurring only in the\nremoval construction) or “on” (in the caused-motion construction) may provide hints to the sentence meaning.\nHowever, the ditransitive and resultative constructions do not contain any such informative words, yet RoBERTa\nstill associates the correct prototype verb for these constructions, so we consider it unlikely to be relying solely on\nlexical overlap. There is substantial evidence that RoBERTa is able to associate abstract constructional templates\nwith their meaning without lexical cues. This result is perhaps surprising, given that previous work found that\nLMs are relatively insensitive to word order in compositional phrases (Yu and Ettinger, 2020) and downstream\ninference tasks (Sinha et al., 2021; Pham et al., 2021), where their performance can be largely attributed to lexical\noverlap.\nAnisotropy. Recent probing work have found that contextual embeddings suffer from anisotropy, where\nembeddings lie in a narrow cone and have much higher cosine similarity than expected if they were directionally\nuniform (Ethayarajh, 2019). Furthermore, a small number of dimensions dominate geometric measures such as\nCHAPTER 6. CONSTRUCTION GRAMMAR IN LMS\n73\nEuclidean and cosine distance, resulting in a degradation of representation quality (Kovaleva et al., 2021; Timkey\nand van Schijndel, 2021). Since our experiments rely heavily on Euclidean distance, anisotropy is a signiﬁcant\nconcern. Following Timkey and van Schijndel (2021), we perform standardization by subtracting the mean vector\nand dividing each dimension by its standard deviation, where the mean and standard deviation for each dimension\nis computed from a sample of the BNC. We observe little difference after standardization: in both the high and\nlow frequency scenarios, the Euclidean distances are lower for the congruent than the incongruent conditions, by\na similar margin compared to the original experiment without standardization. We also run standardization on the\nﬁrst case study, and ﬁnd that the results remain essentially unchanged: smaller LMs still prefer verb sorting while\nlarger LMs prefer construction sorting. Thus, neither of our experiments appear to be affected by anisotropy.\nNeighboring words. A ﬁnal confounding factor is our assumption that RoBERTa’s contextual embeddings\nrepresent word meaning, when in reality, they contain a mixture of syntactic and semantic information. Contextual\nembeddings are known to contain syntax trees (Hewitt and Manning, 2019) and linguistic information about\nneighboring words in a sentence (Klafka and Ettinger, 2020); although previous work did not consider ASCs,\nit is plausible that our verb embeddings leak information about the sentence’s construction in a similar manner.\nIf this were the case, the prototype verb embedding for gave would contain not only the semantics of transfer\nthat we intended, but also information about its usual syntactic form5 of “S gave NP1 NP2”, and both would be\ncaptured by our Euclidean distance measurement. Controlling for this syntactic confound is difﬁcult – one could\nalternatively probe for transfer semantics without syntactic confounds using a natural language inference setup\n(e.g., whether the sentence entails the statement “NP1 received NP2”), but we leave further exploration of this\nidea to future work.\n6.5\nConclusion\nWe found evidence for argument structure constructions in Transformer language models from two separate an-\ngles: sentence sorting and Jabberwocky construction experiments. Our work extended the existing body of litera-\nture on LM probing by taking a constructionist instead of generative approach to linguistic probing. Our sentence\nsorting experiments identiﬁed a striking resemblance between humans’ and LMs’ internal language representa-\ntions as LMs are exposed to increasing quantities of data, despite the differences between neural language models\nand the human brain. Our two studies suggest that LMs are able to derive meaning from abstract constructional\ntemplates with minimal lexical overlap. Both sets of experiments were inspired by psycholinguistic studies, which\nwe adapted to ﬁt the capabilities of LMs – this illustrates the potential for future work on grounding LM probing\nmethodologies in psycholinguistic research.\n5Bresnan and Nikitina (2003) estimated that 87% of usages of the word “give” occur in the ditransitive construction.\nChapter 7\nConclusion\n7.1\nSynopsis\nIn this dissertation, I explored ways in which Transformer-based language models can provide evidence to support\ntheories in linguistics, and how linguistic theory can provide probing frameworks for interpreting language mod-\nels. My research has built connections between natural language processing and linguistics (drawing on research\nfrom both the theoretical and experimental psycholinguistic sides of linguistics). The two ﬁelds have much to\ncontribute to each other, so it is worthwhile for researchers of both disciplines to be familiar with the tools and\ntheories of the other, and look for opportunities to apply cross-disciplinary ideas in their own work.\nChapter 1 introduced the problem of interpreting neural language models and the motivations for probing over\nother evaluation methods. In Chapter 2, I surveyed the models to be probed, beginning with word vector models\nfrom the onset of the deep learning revolution, and culminating with the highly engineered Transformer-based\nmodels that rank at the top of leaderboards today. In Chapter 3, I reviewed recent linguistic probing research\nthat tested the outputs of language models via behavioural probes and the internals of models via representational\nprobes. Here, a wide range of linguistic phenomena combined with a diverse assortment of probing methods led\nto many novel results about what linguistic knowledge our models are capable of, and in which areas they remain\ndeﬁcient.\nThe next three chapters of my thesis contained my own contributions to the ﬁeld. In Chapter 4, I tackled\nword class ﬂexibility, a problem that is controversial in linguistic typology because linguists disagree about how it\nshould be analyzed and how it should be compared across languages. My approach used contextual embeddings\nto argue that word class ﬂexibility should be treated as a directional phenomenon, based on semantic evidence\nautomatically computed from corpora across multiple languages. Chapter 5 explored how different types of\n74\nCHAPTER 7. CONCLUSION\n75\nlinguistic anomalies are represented differently in language models. Inspired by human language processing\nstudies of event-related potentials that trigger depending on the type of anomaly, I devised a method to probe\nfor similar patterns in language models. The experiments revealed a notable difference in how various types of\nanomalies are represented. Chapter 6 draws on the psycholinguistic literature more directly, adapting several\ninﬂuential experiments to probe language models. The original studies presented evidence for the psychological\nreality of argument structure constructions in humans, while my results demonstrated their existence in language\nmodels, via a similar and parallel methodology.\nSadly, my thesis has come to an end, yet my discoveries leave many questions unanswered and opportunities\nfor further exploration. I will next discuss some promising avenues for future work in the linguistic probing\ndirection. I hope that my work will inspire future collaboration between natural language processing researchers\nand linguists.\n7.2\nFuture directions\n7.2.1\nWhich models to probe?\nWhen engaging in probing research, a decision must be made at some point about which models to probe. BERT\nis the most popular choice, but many newer models have surpassed it in performance so that it is no longer state-\nof-the-art; it can be misleading to present its deﬁciencies as representative of language models in general, given\nthat newer models may have improved in these aspects (Bowman, 2021). In my work, I used BERT, ELMo,\nRoBERTa, and XLNet in English experiments, and mBERT, XLM-R, and various monolingual models for non-\nEnglish experiments; these are more or less the most popular models in the community at this time.\nOne may wonder how relevant this body of work will be in the future, when BERT and RoBERTa are surpassed\nby newer models. Indeed, many architectures such as ELECTRA (Clark et al., 2020) and DeBERTa (He et al.,\n2021) claim improvements over BERT and RoBERTa, but these newer models are rarely the subject of probing\nresearch. When probing sentence representations, models dedicated to the task such as Sentence-BERT (Reimers\nand Gurevych, 2019) are rarely used, despite their superior performance over average pooling over token vectors\nor taking the [CLS] vector, methods commonly used in probing setups.\nInevitably, newer models will exhibit similar linguistic patterns as current models in some cases, while differ-\ning in other cases. In my view, probing work will remain relevant despite newer models that behave differently,\nbecause the primary contributions are the novel methodologies to probe for various linguistic phenomena in con-\ntinuous representations, and not the results of the probing experiments themselves. As long as newer models\ncontinue to use similar layers of continuous vectors, it is straightforward to adapt existing linguistic probing\nCHAPTER 7. CONCLUSION\n76\ntests and obtain an assessment of the capabilities of the new model, using far less effort than inventing probing\nprocedures from scratch.\nThe trouble is that when a probing procedure gives different results when applied to different models, it is\noften not possible to explain why, in a satisfactory manner. This limitation applies to Chapter 4 of this dissertation\n(where XLM-R performed worse than mBERT on judging similarity between noun-verb pairs), as well as Chapter\n5 (where XLNet did not exhibit the same difference between anomaly types as RoBERTa). Explaining these\ndifferences is problematic because architectural differences are generally far removed from concepts in linguistic\ntheory. For example, Sentence-BERT uses a siamese architecture with triplet loss to learn sentence embeddings;\nELECTRA uses a pretraining task of predicting corrupted tokens instead of masked language modelling. Any\nattempts to ﬁnd connections to linguistic theory would likely only be speculative.\nAs language model pretraining becomes more accessible, exploring these differences in a systematic man-\nner will become more feasible. Some recent work investigated the effects of structural versus sequential model\narchitecture (Hu et al., 2020), and genre of training data (Huebner et al., 2021) on probing performance. These\nexperiments require training many variants of models to isolate the effects of each architectural parameter, and\nshould become easier to perform in future work as language model tools and frameworks continue to improve.\n7.2.2\nEvidence from learnability\nA common criticism of neural network probing research is its lack of relevance to linguistic theory (Baroni, 2021).\nEven as we analyze the linguistic abilities of BERT and other models in increasing detail, this type of work does\nnot lead to an improved understanding of human language processing, so its impact outside of natural language\nprocessing will likely be limited. One promising direction is using language models to study learnability (e.g.,\nWilcox et al. (2021a)), an approach that is currently underexplored. This idea is that when neural networks are\nable to learn some linguistic feature from corpora alone, that constitutes evidence that no other mechanisms (such\nas innate grammar or interactions grounded in the real world) are necessary to learn the feature.\nLearnability has been featured in studies of argument structure constructions as well. Goldberg et al. (2004)\nproposed that learning of ASCs is facilitated when the distribution of verbs in a construction is skewed towards a\nfrequent prototypical verb (for example, “give” for the ditransitive construction), compared to a balanced distri-\nbution of several verbs. Their evidence came from studies of a child language corpus and an artiﬁcial language\nlearning experiment in which subjects learned novel verbs and constructions. We are limited to indirect studies\nbecause it is impractical to manipulate the language input to children over their lifetime for an experiment. Unlike\nhumans, language models can be trained from scratch on artiﬁcial data to serve as tools to test learnability hy-\npotheses. In this example, we may train one model using a balanced distribution of verbs and train another model\nCHAPTER 7. CONCLUSION\n77\nusing a skewed distribution of verbs, and compare which one is more successful at learning ASCs by probing\nthem. If such an experiment ﬁnds that skewed verb distributions are helpful for language models learning ASCs,\nthe theory of construction learning in humans would be strengthened.\n7.2.3\nPsycholinguistic-based probing\nLanguage model probing has a lot in common with psycholinguistics: the goal of both ﬁelds is to probe the\ninternals of a language processing entity through indirect experimental methods. Psycholinguistics beneﬁts from\nbeing a more mature ﬁeld and closer alignment with linguistic theory, since many psycholinguistic studies are\ndesigned to support or refute theories of how language is processed cognitively.\nCurrently, only a tiny fraction of the numerous psycholinguistic publications in the last few decades have\nbeen considered for adaptation to neural network probing. Given this choice, one can either select studies that\nexamine a speciﬁc linguistic phenomenon (using different methodologies), or select studies that employ the same\nmethodology (studying different phenomena). In this thesis, I have mostly used the former strategy: Chapter 5\ntook data from multiple sources that contain linguistic anomalies, and Chapter 6 adapted studies on argument\nstructure constructions. Other authors aggregated psycholinguistic studies using the same methodology, such as\nMichaelov and Bergen (2020), who focused on the N400 effect, and Prasad et al. (2019), who examined syntactic\npriming. In either case, adapting psycholinguistic work to neural network probing is an effective way of bridging\nthe gap between theoretical linguistics and natural language processing, thereby improving our understanding of\nlanguage models through the lens of linguistic theory.\nBibliography\nM´arta Abrus´an. 2019. Semantic anomaly, pragmatic infelicity, and ungrammaticality. Annual Review of Linguis-\ntics, 5:329–351.\nValerie Adams. 1973. An Introduction to Modern English Word Formation. Longman, London.\nAnnalisa Baicchi and Paolo Della Putta. 2019. Constructions at work in foreign language learners’ mind: A\ncomparison between two sentence-sorting experiments with English and Italian learners. Review of Cognitive\nLinguistics. Published under the auspices of the Spanish Cognitive Linguistics Association, 17(1):219–242.\nIsabel Balteiro. 2007. The directionality of conversion in English: A dia-synchronic study, volume 59 of Linguis-\ntics Insights. Peter Lang.\nDavid Barner and Alan Bale. 2002. No nouns, no verbs: psycholinguistic arguments in favor of lexical under-\nspeciﬁcation. Lingua, 112:771–791.\nMarco Baroni. 2021. On the proper role of linguistically-oriented deep net analysis in linguistic theorizing. arXiv\npreprint arXiv:2106.08694.\nLaurie Bauer. 2005. Conversion and the notion of lexical category. In Laurie Bauer and Salvador Valera, editors,\nApproaches to Conversion/Zero-derivation, pages 19–30. Waxmann, M¨unster.\nLaurie Bauer and Salvador Valera, editors. 2005a.\nApproaches to Conversion/Zero-derivation.\nWaxmann,\nM¨unster.\nLaurie Bauer and Salvador Valera. 2005b. Conversion or zero-derivation: an introduction. In Laurie Bauer and\nSalvador Valera, editors, Approaches to Conversion/Zero-derivation, pages 7–18. Waxmann, M¨unster.\nGiulia ML Bencini and Adele E Goldberg. 2000. The contribution of argument structure constructions to sentence\nmeaning. Journal of Memory and Language, 43(4):640–651.\nKathryn Bock and Helga Loebell. 1990. Framing sentences. Cognition, 35(1):1–39.\n78\nBIBLIOGRAPHY\n79\nKathryn Bock and Carol A Miller. 1991. Broken agreement. Cognitive psychology, 23(1):45–93.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.\nEnriching word vectors with\nsubword information. Transactions of the Association for Computational Linguistics, 5:135–146.\nGemma Boleda. 2020. Distributional semantics and linguistic theory. Annual Review of Linguistics, 6:213–234.\nSamuel Bowman and George Dahl. 2021. What will it take to ﬁx benchmarking in natural language under-\nstanding?\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 4843–4855.\nSamuel R Bowman. 2021. When combating hype, proceed with caution. arXiv preprint arXiv:2110.08300.\nBarli Bram. 2011. Major total conversion in English: The question of directionality. Ph.D. thesis, Victoria\nUniversity of Wellington.\nJoan Bresnan and Tatiana Nikitina. 2003. The gradience of the dative alternation. Unpublished manuscript,\nStanford University.\nTianshi Cao, Chinwei Huang, David Yu-Tung Hui, and Joseph Paul Cohen. 2020. A benchmark of medical out\nof distribution detection. arXiv preprint arXiv:2007.04250.\nAndrew Carnie. 2013. Syntax: A generative introduction. John Wiley & Sons.\nJos´e Ca˜nete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang, and Jorge P´erez. 2020. Spanish\npre-trained BERT model and evaluation data. In PML4DC at ICLR 2020.\nBo˙zena Cetnarowska. 1993. The Syntax, Semantics and Derivation of Bare Nominalizations. Uniwersytet ´Sla¸ski,\nKatowice.\nEmmanuele Chersoni, Adri`a Torrens Urrutia, Philippe Blache, and Alessandro Lenci. 2018. Modeling viola-\ntions of selectional restrictions with distributional semantics. In Proceedings of the Workshop on Linguistic\nComplexity and Natural Language Processing, pages 20–29.\nEthan A Chi, John Hewitt, and Christopher D Manning. 2020. Finding universal grammatical relations in multi-\nlingual BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\npages 5564–5577.\nNoam Chomsky. 1957. Syntactic Structures. Mouton and Co.\nNoam Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press.\nBIBLIOGRAPHY\n80\nNoam Chomsky. 1981. Lectures on Government and Binding. Foris Publications, Dordrecht.\nNoam Chomsky. 1986. Knowledge of Language. Praeger, New York.\nNoam Chomsky. 1995. The Minimalist Program. The MIT Press, Cambridge, Massachusetts.\nWing-Yee Chow, Cybelle Smith, Ellen Lau, and Colin Phillips. 2016. A “bag-of-arguments” mechanism for initial\nverb predictions. Language, Cognition and Neuroscience, 31(5):577–596.\nEve V. Clark and Herbert H. Clark. 1979. When nouns surface as verbs. Language, 55(4):767–811.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. ELECTRA: Pre-training text\nencoders as discriminators rather than generators. In International Conference on Learning Representations.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.\nUnsupervised cross-\nlingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 8440–8451. Association for Computational Linguistics.\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An evaluation toolkit for universal sentence representations.\nIn Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).\nAlexis Conneau and Guillaume Lample. 2019. Cross-lingual language model pretraining. In Advances in Neural\nInformation Processing Systems, pages 7059–7069.\nWilliam Croft. 2003. Typology and Universals, 2 edition. Cambridge University Press, Cambridge.\nDavid Alan Cruse. 1986. Lexical Semantics. Cambridge University Press, Cambridge.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pages 4171–4186.\nJan Don. 2003. A note on conversion in Dutch and German. Linguistics in the Netherlands, pages 33–44.\nJan Don. 2005. On conversion, relisting and zero-derivation: A comment on Rochelle Lieber: English word-\nformation processes. SKASE Journal of Theoretical Linguistics, 2(2):2–16.\nJesse Dunietz, Lori Levin, and Jaime G Carbonell. 2015. Annotating causal language using corpus lexicography\nof constructions. In Proceedings of The 9th Linguistic Annotation Workshop, pages 188–196.\nBIBLIOGRAPHY\n81\nJonathan Dunn. 2017. Computational learning of construction grammars. Language and Cognition, 9(2):254–\n292.\nJeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.\nKawin Ethayarajh. 2019. How contextual are contextualized word representations? comparing the geometry\nof BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 55–65.\nAllyson Ettinger. 2020. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language\nmodels. Transactions of the Association for Computational Linguistics, 8:34–48.\nNicholas Evans. 2000. Word classes in the world’s languages. In Geert Booij, Christian Lehmann, and Joachim\nMugdan, editors, Morphologie/morphology: An international handbook on inﬂection and word-formation,\npages 708–732. de Gruyter, Berlin.\nNicholas Evans and Stephen C. Levinson. 2009. The myth of language universals: language diversity and its\nimportance for cognitive science. Behavioral and Brain Sciences, 32:429–492.\nNicholas Evans and Toshiki Osada. 2005. Mundari: The myth of a language without word classes. Linguistic\nTypology, 9(3):351–390.\nPatrick Farell. 2001. Functional shift as category underspeciﬁcation. English Language and Linguistics, 5(1):109–\n130.\nKara D Federmeier and Marta Kutas. 1999. A rose by any other name: Long-term memory structure and sentence\nprocessing. Journal of memory and Language, 41(4):469–495.\nGertraud Fenk-Oczlon, August Fenk, and Pamela Faber. 2010. Frequency effects on the emergence of polysemy\nand homophony. International Journal of Information Technologies and Knowledge, 4(2):103–109.\nCharles J. Fillmore, Paul Kay, and Mary Catherine O’Connor. 1988. Regularity and idiomaticity in grammatical\nconstructions: The case of let alone. Language, 64(3):501–538.\nStefan L Frank, Leun J Otten, Giulia Galli, and Gabriella Vigliocco. 2015. The ERP response to the amount of\ninformation conveyed by words in sentences. Brain and language, 140:1–11.\nDedre Gentner. 1982. Why nouns are learned before verbs: Linguistic relativity versus natural partitioning. Center\nfor the Study of Reading Technical Report; no. 257.\nBIBLIOGRAPHY\n82\nKim Gerdes, Bruno Guillaume, Sylvain Kahane, and Guy Perrier. 2018. SUD or Surface-Syntactic Universal\nDependencies: An annotation scheme near-isomorphic to UD. In Universal Dependencies Workshop 2018.\nAdele E Goldberg. 1995. Constructions: A construction grammar approach to argument structure. University of\nChicago Press.\nAdele E. Goldberg. 2006. Constructions at Work: The Nature of Generalization in Language. Oxford University\nPress, Oxford.\nAdele E Goldberg, Devin M Casenhiser, and Nitya Sethuraman. 2004. Learning argument structure generaliza-\ntions. Cognitive Linguistics, 15(3):289–316.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2018. FRAGE: Frequency-agnostic\nword representation. In Advances in neural information processing systems, pages 1334–1345.\nStefan Th Gries and Stefanie Wulff. 2005. Do foreign language learners also have constructions? Annual Review\nof Cognitive Linguistics, 3(1):182–200.\nKristina Gulordava, Piotr Bojanowski, ´Edouard Grave, Tal Linzen, and Marco Baroni. 2018. Colorless green\nrecurrent networks dream hierarchically. In Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),\npages 1195–1205.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith.\n2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 107–112.\nZellig S Harris. 1954. Distributional structure. Word, 10(2-3):146–162.\nBetty Hart and Todd R Risley. 2003. The early catastrophe: The 30 million word gap by age 3. American\neducator, 27(1):4–9.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTa: Decoding-enhanced BERT\nwith disentangled attention. In International Conference on Learning Representations.\nKees Hengeveld. 1992. Non-verbal predication: theory, typology, diachrony. Mouton de Gruyter, Berlin, New\nYork.\nBIBLIOGRAPHY\n83\nKees Hengeveld. 2013. Parts-of-speech systems as a basic typological determinant. In Eva Van Lier and Jan\nRijkhoff, editors, Flexible word classes: a typological study of underspeciﬁed parts-of-speech, pages 31–55.\nOxford University Press, Oxford.\nLucas Torroba Hennigen, Adina Williams, and Ryan Cotterell. 2020. Intrinsic probing through dimension selec-\ntion. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),\npages 197–216.\nJohn Hewitt and Percy Liang. 2019. Designing and interpreting probes with control tasks. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pages 2733–2743.\nJohn Hewitt and Christopher D Manning. 2019. A structural probe for ﬁnding syntax in word representations.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.\nJeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 328–339.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, and Roger Levy. 2020. A systematic assessment of syntactic\ngeneralization in neural language models. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1725–1744. Association for Computational Linguistics.\nPhilip A Huebner, Elior Sulem, Fisher Cynthia, and Dan Roth. 2021. BabyBERTa: Learning more grammar\nwith small-scale child-directed language. In Proceedings of the 25th Conference on Computational Natural\nLanguage Learning, pages 624–646.\nJena D Hwang and Martha Palmer. 2015. Identiﬁcation of caused motion construction. In Proceedings of the\nFourth Joint Conference on Lexical and Computational Semantics, pages 51–60.\nClaudio Iacobini. 2000. Base and direction of derivation. In Geert Booij, Christian Lehmann, and Joachim\nMugdan, editors, Morphology: An International Handbook on Inﬂection and Word-Formation, pages 865–876.\nWalter de Gruyter, Berlin.\nMutsumi Imai, Lianjing Li, Etsuko Haryu, Hiroyuki Okada, Kathy Hirsh-Pasek, Roberta Michnick Golinkoff, and\nJun Shigematsu. 2008. Novel noun and verb learning in Chinese-, English-, and Japanese-speaking children.\nChild development, 79(4):979–1000.\nBIBLIOGRAPHY\n84\nGanesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. 2019. What does BERT learn about the structure of language?\nIn ACL 2019-57th Annual Meeting of the Association for Computational Linguistics.\nOtto Jespersen. 1924. The Philosophy of Grammar. Allen & Unwin., London.\nOtto Jespersen. 1942. A Modern English Grammar. On Historical Principles. Part VI Morphology. Allen &\nUnwin., London.\nMatt A Johnson and Adele E Goldberg. 2013. Evidence for automatic accessing of constructional meaning:\nJabberwocky sentences prime associated verbs. Language and Cognitive Processes, 28(10):1439–1452.\nR. M. Kaplan and Joan Bresnan. 1982. Lexical functional grammar: A formal system for grammatical represen-\ntation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, pages 173–282. MIT\nPress, Cambridge, MA.\nMichael P Kaschak and Arthur M Glenberg. 2000. Constructing meaning: The role of affordances and grammat-\nical constructions in sentence comprehension. Journal of memory and language, 43(3):508–529.\nDieter Kastovsky. 2006. Typological changes in derivational morphology. In Ans van Kemenade and Bettelou\nLos, editors, The Handbook of the History of English, pages 151–176. Blackwell Publishing.\nPaul Kay and Charles J. Fillmore. 1999. Grammatical constructions and linguistic generalizations: The What’s X\nDoing Y? construction. Language, 75(1):1–33.\nMA Kelly, Yang Xu, Jes´us Calvillo, and David Reitter. 2020. Which sentence embeddings and which layers\nencode syntactic structure? In Cognitive Science, pages 2375–2381.\nSimon Kirsch. 2019. The psychological reality of argument structure constructions: A visual world eye tracking\nstudy. Unpublished MSc thesis, University of Freiburg.\nMax Kisselew, Laura Rimell, Alexis Palmer, and Sebastian Pad´o. 2016. Predicting the direction of derivation\nin English conversion. In Proceedings of the 14th SIGMORPHON Workshop on Computational Research in\nPhonetics, Phonology, and Morphology, pages 93–98.\nJosef Klafka and Allyson Ettinger. 2020. Spying on your neighbors: Fine-grained probing of contextual embed-\ndings for information about surrounding words. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 4801–4811.\nOlga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. 2021. BERT busters: Outlier dimen-\nsions that disrupt Transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP\n2021, pages 3392–3405.\nBIBLIOGRAPHY\n85\nArtur Kulmizev, Vinit Ravishankar, Mostafa Abdou, and Joakim Nivre. 2020. Do neural language models show\npreferences for syntactic formalisms?\nIn Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4077–4091.\nMarta Kutas and Kara D Federmeier. 2011. Thirty years and counting: ﬁnding meaning in the N400 component\nof the event-related brain potential (ERP). Annual review of psychology, 62:621–647.\nMarta Kutas, Cyma K Van Petten, and Robert Kluender. 2006. Psycholinguistics electriﬁed II (1994–2005). In\nHandbook of psycholinguistics, pages 659–724. Elsevier.\nIlia Kuznetsov and Iryna Gurevych. 2020. A matter of framing: The impact of linguistic formalism on prob-\ning results. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 171–182. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020.\nALBERT: A lite BERT for self-supervised learning of language representations. In International Conference\non Learning Representations.\nShalom Lappin. 2021. Deep learning and linguistic representation. CRC Press.\nJey Han Lau, Alexander Clark, and Shalom Lappin. 2017. Grammaticality, acceptability, and probability: A\nprobabilistic view of linguistic knowledge. Cognitive science, 41(5):1202–1241.\nGianluca E Lebani and Alessandro Lenci. 2016. “beware the Jabberwock, dear reader!” Testing the distributional\nreality of construction semantics. In Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon\n(CogALex-V), pages 8–18.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. 2018. A simple uniﬁed framework for detecting out-of-\ndistribution samples and adversarial attacks. Advances in Neural Information Processing Systems, 31:7167–\n7177.\nGeoffrey Leech, Roger Garside, and Michael Bryant. 1994. CLAWS4: the tagging of the British National Corpus.\nIn COLING 1994 Volume 1: The 15th International Conference on Computational Linguistics.\nGeoffrey Neil Leech. 1992. 100 million words of English: the British National Corpus (BNC). Language Re-\nsearch, 28:1–13.\nAlessandro Lenci. 2018. Distributional models of word meaning. Annual review of Linguistics, 4:151–171.\nBeth Levin and Malka Rappaport Hovav. 1995. Unaccusativity in the Syntax-Lexical Semantics Interface. MIT\nPress, Cambridge, MA.\nBIBLIOGRAPHY\n86\nBeth Levin and Malka Rappaport Hovav. 2005. Argument Realization. Cambridge University Press, Cambridge,\nUK.\nBai Li, Guillaume Thomas, Yang Xu, and Frank Rudzicz. 2020. Word class ﬂexibility: A deep contextualized\napproach. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 983–994.\nBai Li, Zining Zhu, Guillaume Thomas, Frank Rudzicz, and Yang Xu. 2022. Neural reality of argument structure\nconstructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics,\nOnline. Association for Computational Linguistics.\nBai Li, Zining Zhu, Guillaume Thomas, Yang Xu, and Frank Rudzicz. 2021. How is BERT surprised? Layerwise\ndetection of linguistic anomalies. In Proceedings of the 59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers), pages 4215–4228, Online. Association for Computational Linguistics.\nJunying Liang. 2002. Sentence comprehension by Chinese learners of English: Verb-centered or construction-\nbased? Unpublished MA thesis, Guangdong University of Foreign Studies.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-\nsensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521–535.\nLeo Z Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah A Smith. 2021. Probing across time:\nWhat does RoBERTa know and when?\nIn Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing: Findings.\nNelson F Liu, Matt Gardner, Yonatan Belinkov, Matthew E Peters, and Noah A Smith. 2019a. Linguistic knowl-\nedge and transferability of contextual representations. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 1073–1094.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining approach.\narXiv preprint arXiv:1907.11692.\nHarish Tayyar Madabushi, Laurence Romain, Dagmar Divjak, and Petar Milin. 2020. CxGBERT: BERT meets\nconstruction grammar. In Proceedings of the 28th International Conference on Computational Linguistics,\npages 4020–4032.\nBIBLIOGRAPHY\n87\nAndreas Madsen, Siva Reddy, and Sarath Chandar. 2021. Post-hoc interpretability for neural NLP: A survey.\narXiv preprint arXiv:2108.04840.\nStela Manova. 2011. Understanding Morphological Rules. Springer.\nHans Marchand. 1964. A set of criteria of derivational relationship between words unmarked by derivational\nmorphemes. Indogermanische Forschungen, 69:10–19.\nHans Marchand. 1969. The Categories and Types of Present-day English Word-formation. Beck, M¨unchen.\nMitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of\nEnglish: The Penn Treebank.\nRebecca Marvin and Tal Linzen. 2018. Targeted syntactic evaluation of language models. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, pages 1192–1202.\nEleni Metheniti, Tim Van de Cruys, and Nabil Hathout. 2020.\nHow relevant are selectional preferences for\nTransformer-based language models? In Proceedings of the 28th International Conference on Computational\nLinguistics, pages 1266–1278.\nAlessio Miaschi, Dominique Brunato, Felice Dell’Orletta, and Giulia Venturi. 2020. Linguistic proﬁling of a\nneural language model. The 28th International Conference on Computational Linguistics, pages 745–756.\nJames Michaelov and Benjamin Bergen. 2020. How well does surprisal explain N400 amplitude under differ-\nent experimental conditions?\nIn Proceedings of the 24th Conference on Computational Natural Language\nLearning, pages 652–663.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation of word representations\nin vector space. arXiv preprint arXiv:1301.3781.\nKanishka Misra, Allyson Ettinger, and Julia Rayz. 2020. Exploring BERT’s sensitivity to lexical cues using tests\nfrom semantic priming. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing: Findings, pages 4625–4635.\nJingcheng Niu and Gerald Penn. 2020. Grammaticality and language modelling. In Proceedings of the First\nWorkshop on Evaluation and Comparison of NLP Systems, pages 110–119.\nLee Osterhout and Linda A Mobley. 1995. Event-related brain potentials elicited by failure to agree. Journal of\nMemory and language, 34(6):739–773.\nBIBLIOGRAPHY\n88\nLee Osterhout and Janet Nicol. 1999. On the distinctiveness, independence, and time course of the brain responses\nto syntactic and semantic anomalies. Language and cognitive processes, 14(3):283–317.\nFabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-\nieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in\nPython. Journal of machine Learning research, 12:2825–2830.\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word represen-\ntation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),\npages 1532–1543.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227–2237.\nThang Pham, Trung Bui, Long Mai, and Anh Nguyen. 2021. Out of order: How important is the sequential order\nof words in a sentence in natural language understanding tasks? In Findings of the Association for Computa-\ntional Linguistics: ACL-IJCNLP 2021, pages 1145–1160, Online. Association for Computational Linguistics.\nMartin J Pickering and Victor S Ferreira. 2008. Structural priming: a critical review. Psychological bulletin,\n134(3):427.\nAlexander Podolskiy, Dmitry Lipin, Andrey Bout, Ekaterina Artemova, and Irina Piontkovskaya. 2021. Revisiting\nMahalanobis distance for Transformer-based out-of-domain detection. In 35th AAAI Conference on Artiﬁcial\nIntelligence (AAAI 2021).\nMaria Polinsky. 2012. Headedness, again. Theories of Everything. In Honor of Ed Keenan. Los Angeles: UCLA\nDepartment of Linguistics, pages 348–359.\nCarl Pollard and Ivan A. Sag. 1987. Information-Based Syntax and Semantics, volume 13. CSLI, Stanford.\nMads Poulsen. 2012. The usefulness of the grammaticality–acceptability distinction in functional approaches to\nlanguage. Acta Linguistica Hafniensia, 44(1):4–21.\nGrusha Prasad, Marten van Schijndel, and Tal Linzen. 2019.\nUsing priming to uncover the organization of\nsyntactic representations in neural language models. In Proceedings of the 23rd Conference on Computational\nNatural Language Learning (CoNLL), pages 66–76.\nLiina Pylkk¨anen and Brian McElree. 2007. An MEG study of silent meaning. Journal of cognitive neuroscience,\n19(11):1905–1921.\nBIBLIOGRAPHY\n89\nRandolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the\nEnglish Language. Longman, London.\nElla Rabinovich, Julia Watson, Barend Beekhuizen, and Suzanne Stevenson. 2019. Say anything: Automatic\nsemantic infelicity detection in L2 English indeﬁnite pronouns. In Proceedings of the 23rd Conference on\nComputational Natural Language Learning (CoNLL), pages 77–86.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding\nby generative pre-training. OpenAI Technical Report.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t know: Unanswerable questions for\nSQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n2: Short Papers), pages 784–789.\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using siamese BERT-networks.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. 2021. A primer in BERTology: What we know about how\nBERT works. Transactions of the Association for Computational Linguistics, 8:842–866.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. 2020. Masked language model scoring. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712.\nAssociation for Computational Linguistics.\nRyohei Sasano and Anna Korhonen. 2020.\nInvestigating word-class distributions in word vector spaces.\nIn\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3657–3666.\nMarten van Schijndel and Tal Linzen. 2021. Single-stage prediction models do not explain the magnitude of\nsyntactic disambiguation difﬁculty. Cognitive Science, 45(6):e12988.\nMarten van Schijndel, Aaron Mueller, and Tal Linzen. 2019. Quantity doesn’t buy quality syntax with neural\nlanguage models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n5831–5837, Hong Kong, China. Association for Computational Linguistics.\nBernhard Sch¨olkopf, Robert C Williamson, Alex J Smola, John Shawe-Taylor, and John C Platt. 2000. Support\nvector method for novelty detection. In Advances in neural information processing systems, pages 582–588.\nBIBLIOGRAPHY\n90\nCarson T. Sch¨utze. 1996. The Empirical Base of Linguistics: Grammaticality Judgments and Linguistic Method-\nology. University of Chicago Press.\nKoustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, and Adina Williams. 2021. UnNatural Language Inference.\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7329–7346,\nOnline. Association for Computational Linguistics.\nPavol S¸tekauer, Salvador Valera, and L´ıvia K¨ortv´elyessy. 2012. Word-Formation in the World’s Languages. Cam-\nbridge University Press.\nMilan Straka and Jana Strakov´a. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe.\nIn Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependen-\ncies, pages 88–99.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel Bowman, Dipanjan Das, et al. 2019a. What do you learn from context? probing for\nsentence structure in contextualized word representations. In 7th International Conference on Learning Repre-\nsentations, ICLR 2019.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019b. BERT rediscovers the classical NLP pipeline. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593–4601.\nWilliam Timkey and Marten van Schijndel. 2021. All bark and no bite: Rogue dimensions in transformer language\nmodels obscure representational quality. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 4527–4546.\nDavid Tuggy. 1993. Ambiguity, polysemy, and vagueness. Cognitive Linguistics, 4(3):273–290.\nThomas P Urbach and Marta Kutas. 2010. Quantiﬁers more or less quantify on-line: ERP evidence for partial\nincremental interpretation. Journal of Memory and Language, 63(2):158–179.\nSalvador Valera and Alba E. Ruz. 2020. Conversion in English: homonymy, polysemy and paronymy. English\nLanguage and Linguistics, pages 1–24.\nEva Van Lier and Jan Rijkhoff, editors. 2013. Flexible word classes: a typological study of underspeciﬁed parts-\nof-speech. Oxford University Press Oxford.\nBIBLIOGRAPHY\n91\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser,\nand Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems,\npages 5998–6008.\nMontserrat Mart´ınez V´azquez. 2004. Learning argument structure generalizations in a foreign language. Vigo\nInternational Journal of Applied Linguistics, (1):151–165.\nPetra Vogel and Bernard Comrie, editors. 2000. Approaches to the Typology of Word Classes. Mouton de Gruyter,\nBerlin and New York.\nElena Voita and Ivan Titov. 2020. Information-theoretic probing with minimum description length. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183–196.\nArnﬁnn Muruvik Vonen. 1994. Multifunctionality and morphology in Tokelau and English. Nordic Journal of\nLinguistic, 17:155–178.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\nSamuel R Bowman. 2019a. SuperGLUE: a stickier benchmark for general-purpose language understanding\nsystems. In Proceedings of the 33rd International Conference on Neural Information Processing Systems,\npages 3266–3280.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019b. GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding. In 7th International Confer-\nence on Learning Representations, ICLR 2019.\nTessa Warren, Evelyn Milburn, Nikole D Patson, and Michael Walsh Dickey. 2015. Comprehending the impossi-\nble: what role do selectional restriction violations play? Language, cognition and neuroscience, 30(8):932–939.\nAlex Warstadt and Samuel R Bowman. 2020. Can neural networks acquire a structural bias from raw linguistic\ndata? In Proceedings of the Annual Meeting of the Cognitive Science Society.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R Bow-\nman. 2020a. BLiMP: The benchmark of linguistic minimal pairs for English. Transactions of the Association\nfor Computational Linguistics, 8:377–392.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2019. Neural network acceptability judgments. Trans-\nactions of the Association for Computational Linguistics, 7:625–641.\nBIBLIOGRAPHY\n92\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu, and Samuel R Bowman. 2020b. Learning which features\nmatter: RoBERTa acquires a preference for linguistic generalizations (eventually). In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP).\nJason Wei, Dan Garrette, Tal Linzen, and Ellie Pavlick. 2021. Frequency effects on syntactic rule learning in\ntransformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\n(EMNLP).\nJennifer C White, Tiago Pimentel, Naomi Saphra, and Ryan Cotterell. 2021. A non-linear structural probe. In\nProceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 132–138.\nEthan Wilcox, Richard Futrell, and Roger Levy. 2021a. Using computational models to test syntactic learnability.\nLingbuzz Preprint: lingbuzz/006327.\nEthan Wilcox, Pranali Vani, and Roger Levy. 2021b. A targeted assessment of incremental processing in neural\nlanguage models and humans. In Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pages 939–952, Online. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cis-\ntac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma,\nYacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and\nAlexander M. Rush. 2019. HuggingFace’s transformers: State-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. XL-\nNet: Generalized autoregressive pretraining for language understanding. In Advances in neural information\nprocessing systems, pages 5753–5763.\nLang Yu and Allyson Ettinger. 2020.\nAssessing phrasal representation and composition in transformers.\nIn\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n4896–4907.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, et al. 2019. Universal dependencies 2.5. LINDAT/CLARIAH-CZ\ndigital library at the Institute of Formal and Applied Linguistics ( ´UFAL), Faculty of Mathematics and Physics,\nCharles University.\nBIBLIOGRAPHY\n93\nYian Zhang, Alex Warstadt, Haau-Sing Li, and Samuel R Bowman. 2021. When do you need billions of words of\npretraining data? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics.\nJayden Ziegler, Giulia Bencini, Adele Goldberg, and Jesse Snedeker. 2019. How abstract is syntax? Evidence\nfrom structural priming. Cognition, 193:104045.\nGeorge Kingsley Zipf. 1949. Human behavior and the principle of least effort: An introduction to human ecology.\nAddison-Wesley Press.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-07-20",
  "updated": "2022-07-20"
}