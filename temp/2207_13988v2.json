{
  "id": "http://arxiv.org/abs/2207.13988v2",
  "title": "Sequence to sequence pretraining for a less-resourced Slovenian language",
  "authors": [
    "Matej Ulčar",
    "Marko Robnik-Šikonja"
  ],
  "abstract": "Large pretrained language models have recently conquered the area of natural\nlanguage processing. As an alternative to predominant masked language modelling\nintroduced in BERT, the T5 model has introduced a more general training\nobjective, namely sequence to sequence transformation, which includes masked\nlanguage model but more naturally fits text generation tasks such as machine\ntranslation, summarization, question answering, text simplification, dialogue\nsystems, etc. The monolingual variants of T5 models have been limited to\nwell-resourced languages, while the massively multilingual T5 model supports\n101 languages. In contrast, we trained two different sized T5-type sequence to\nsequence models for morphologically rich Slovene language with much less\nresources and analyzed their behavior on 11 tasks. Concerning classification\ntasks, the SloT5 models mostly lag behind the monolingual Slovene SloBERTa\nmodel but are useful for the generative tasks.",
  "text": "arXiv:2207.13988v2  [cs.CL]  2 Jan 2023\nSequence to sequence pretraining for a less-resourced Slovenian\nlanguage\nMatej Ulˇcar, Marko Robnik-ˇSikonja\nUniversity of Ljubljana, Faculty of Computer and Information Science\nVeˇcna pot 113, Ljubljana, Slovenia\n{matej.ulcar, marko.robnik}@fri.uni-lj.si\nAbstract\nLarge pretrained language models have recently conquered the area of natural language pro-\ncessing.\nAs an alternative to predominant masked language modelling introduced in BERT,\nthe T5 model has introduced a more general training objective, namely sequence to sequence\ntransformation, which includes masked language model but more naturally ﬁts text generation\ntasks such as machine translation, summarization, question answering, text simpliﬁcation, dia-\nlogue systems, etc. The monolingual variants of T5 models have been limited to well-resourced\nlanguages, while the massively multilingual T5 model supports 101 languages. In contrast, we\ntrained two diﬀerent sized T5-type sequence to sequence models for morphologically rich Slovene\nlanguage with much less resources and analyzed their behavior on 11 tasks. Concerning classiﬁ-\ncation tasks, the SloT5 models mostly lag behind the monolingual Slovene SloBERTa model but\nare useful for the generative tasks.\nKeywords: natural language processing, pretrained language models, sequence to sequence\nmodels, transformers, T5 model, low-resource languages, Slovene\n1\nIntroduction\nRecent state-of-the-art natural language processing (NLP) solutions are based on the transformer\nneural network architecture (Vaswani et al., 2017). The main research direction is to produce (ever\nlarger) pretrained language models (PLMs) with billions of parameters with the objective to contain\nas much human knowledge as possible (Bommasani et al., 2021). Such models require large amounts\nof training data and are computationally expensive to train.\nMost very large models have been\ntrained for English and a few high-resource languages, such as Chinese, French, or German. Massively\nmultilingual models, trained on around 100 languages, have also been released, but their performance\nlags behind their monolingual and few-lingual equivalents (Ulˇcar et al., 2021). For some of these 100\nless-resourced languages, there is a growing number of smaller models (though still in the range of\na few 100 million parameters) trained on the BERT (Devlin et al., 2019) or RoBERTa (Liu et al.,\n2019) architecture.\nBERT (Devlin et al., 2019) is a masked language model, utilizing the encoder stack of the trans-\nformer architecture to capture the semantic representation of the input text. This makes it very\nsuitable for solving classiﬁcation tasks. Another popular type of large language models are from\nthe GPT family, such as GPT-2 (Radford et al., 2019) and GPT3 (Brown et al., 2020), which are\ngenerative models and utilize only the decoder stack of the transformer. In contrast to these models,\nsequence to sequence (seq2seq) models such as BART (Lewis et al., 2020) and T5 (Raﬀel et al., 2020)\nutilize both encoder and decoder stack of the transformer. Such models can treat every problem as a\ntext-to-text transformation and solve it similarly, without adapting the training procedure for each\ntask.\nIn this work, we present two new sequence to sequence models for the less-resourced Slovene\nlanguage based on the T5 architecture and its training tasks. We aim to analyze the amount of\nrequired data for such models to be eﬀective and the role the richer morphology plays for seq2seq\nmodels. Namely, while English has a ﬁxed word order and relatively few word forms for each word,\n1\nthis is not the case for most other languages. This might not be problematic in text classiﬁcation,\nwhich is a typical task for large pretrained models, while text generation tasks are more challenging\nfor morphologically-rich languages. We qualitatively and quantitatively test Slovene T5 models on\nthree text generation tasks: lemmatization, summarization, and text simpliﬁcation. We believe our\nresults might be indicative for other less-resourced languages in terms of datasets, training, and\nexpected results.\nThe work is split into further four sections. In Section 2, we summarize the related work and\nbrieﬂy describe the T5 architecture In Section 3, we present the architecture and training of the\nSlovene T5 models, which are evaluated in Section 4. We discuss the ﬁndings and their implications\nin Section 5.\n2\nRelated work\nT5 model (Raﬀel et al., 2020) is an encoder-decoder transformer, trained on several supervised\nand self-supervised pretraining tasks. The supervised tasks used were the tasks from the GLUE\n(Wang et al., 2018) and SuperGLUE (Wang et al., 2019) benchmarks, as well as translation and\nsummarization tasks. The self-supervised task used was the span-corruption task. In this task, ran-\ndomly selected token spans are replaced with a special mask token (a sentinel token). The goal of the\ntask is to generate the masked spans. During pretraining, 15% of tokens were masked in spans with\nan average length of 3 tokens. The encoder stack receives the tokenized input text. The self-attention\nmechanism attends to the whole input in the encoder. The output of the encoder is then fed into the\ndecoder stack, which generates the target text. A causal mask is used to prevent the self-attention\nmechanism in the decoder to attend to the ”future” output. At each timestep, the model ”sees” the\nwhole input sequence and the part of the output sequence generated at previous timesteps. Several\nT5 models for English have been trained and released. They diﬀer in size, ranging from 60 million\nto 11 billion parameters.\nXue et al. (2021) have trained multilingual T5 models (mT5) of various sizes. The mT5 models\nwere trained on a large multilingual mC4 corpus, containing 101 languages, and a total of 6.3 · 1012\ntokens. The Slovenian portion of the corpus contains 8.8 billion tokens.\nThe mT5 models were\ntrained simultaneously on all 101 languages on the span corruption task only.\nBART (Lewis et al., 2020) is another popular encoder-decoder transformer model.\nThe main\ndiﬀerence between BART and T5 is in the choice of the pretraining tasks. Similarly to T5 and mT5,\nBART was trained on the span corruption task. Additionally, token deletion, sentence permutation,\nand document rotation tasks were used during pretraining. Liu et al. (2020) trained a multilingual\nBART (mBART) model on 25 languages, using the span corruption (masking 35% of the words)\nand sentence permutation training tasks. Tang et al. (2021) extended the existing mBART model to\nfurther 25 languages, thus covering 50 languages, including Slovene.\nSeveral monolingual models, based on the T5 architecture, have been released for high-resource\nlanguages other than English, such as Chinese Mengzi (Zhang et al., 2021), Arabic AraT5 (Nagoudi et al.,\n2021), and Italian IT5 (Sarti & Nissim, 2022). While Nagoudi et al. (2021) observe the improvement\nof AraT5 over mT5 across all evaluation tasks, Sarti & Nissim (2022) note that especially for sum-\nmarization IT5 lags behind the benchmark models. Sarti & Nissim (2022) also observed that scaling\nthe model size does not uniformly correspond to improvements in performance. On average, the\nsmall IT5 improves the most over the comparable mT5 model, while larger IT5 models show much\nsmaller or no improvements over comparable mT5 models and, in some cases, perform even worse\nthan the small IT5 model.\nThe presented Slovene SloT5 models partially conﬁrm and partially contradict the above ﬁndings.\nOn one hand, we use much more challenging text classiﬁcation tasks (Slovene translation of the\nSuperGLUE benchmark suite); therefore, the classiﬁcation performance of SloT5 models consistently\nlags behind the BERT-like monolingual Slovene models. On the other hand, while the small SloT5\nmodel is successful for text generation tasks, the amount of training data and training time might\nnot be suﬃcient to make the large SloT5 model really competitive in the text generation tasks.\n2\n3\nSlovene T5 models\nIn this section, we present the newly created Slovene T5 models (named SloT5). First, we describe\nthe training data, followed by the description of architecture and training.\n3.1\nTraining data\nWe trained Slovene SloT5 models on large Slovene corpora, covering a wide spectrum of genres, from\nﬁction books to newspapers, academic language, internet slang, etc. We included Gigaﬁda, Janes,\nKAS, SiParl, and SlWaC corpora. The corpora details are given below and summarized in Table 1.\nGigaﬁda 2.0 (Krek et al., 2020) is a general standard language corpus composed of ﬁction and\nnon-ﬁction books, newspapers, school textbooks, texts from the internet, etc.\nThe Janes corpus\n(Fiˇser et al., 2016) is a corpus of non-standard language composed of several subcorpora.\nEach\nsubcorpus contains texts from a certain social medium or a group of similar media, including Twit-\nter, blog posts, forum conversations, user comments on news site articles, etc. We used all Janes\nsubcorpora, except Janes-tweet, since the contents of that subcorpus are encoded and need to be\nindividually downloaded from Twitter, which is a lengthy process as Twitter limits the access speed.\nKAS (Corpus of Academic Slovene) (Erjavec et al., 2021) consists of PhD, MSc, MA, BSc, and BA\ntheses written in Slovene between 2000 and 2018. SiParl (Panˇcur & Erjavec, 2020) contains minutes\nof Slovene national assembly between 1990 and 2018. SlWaC (Ljubeˇsi´c & Erjavec, 2011) is a web\ncorpus collected from the Slovene top-level web domain .si.\nTable 1: Corpora used in training of SloT5 models with their sizes in billion of tokens and words.\nJanes subcorpora used are listed separately, but we show their combined size.\nCorpus\nGenre\nTokens\nWords\nGigaﬁda 2.0 (Krek et al., 2019b)\ngeneral language\n1.33\n1.11\nKAS (ˇZagar et al., 2022)\nacademic\n1.70\n1.33\nsiParl 2.0 (Panˇcur et al., 2020)\nparliamentary\n0.24\n0.20\nslWaC 2.1 (Ljubeˇsi´c & Erjavec, 2011)\nweb crawl\n0.90\n0.75\nJanes (Fiˇser et al., 2016)\nsocial media\n0.10\n0.08\n– Janes-Wiki (Ljubeˇsi´c et al., 2017)\n– Wikipedia talk pages\n– Janes-Blog (Erjavec et al., 2017a)\n– Slovene blogs\n– Janes-Forum (Erjavec et al., 2017b)\n– Slovene forums\n– Janes-News (Erjavec et al., 2017c)\n– comments on online news articles\nTotal\n4.27\n3.47\nTotal after deduplication\n4.20\n3.41\nWe deduplicated the corpora, using the Onion tool (Pomik´alek, 2011). After the deduplication,\nthe training dataset contained about 4 billion tokens. Finally, before training the models, we encoded\nthe text into subword byte-pair-encodings using a sentencepiece1 model. We used the sentencepiece\nmodel that was trained for SloBERTa (Ulˇcar & Robnik-ˇSikonja, 2021) and contains 32,000 subword\ntokens in its vocabulary.\n3.2\nArchitecture and training of SloT5\nWe trained Slovene T5 models of two diﬀerent sizes: T5-sl-small and T5-sl-large. The smaller model\nhas 8 encoder and 8 decoder layers, in total, about 60 million parameters. The larger model has 24\nencoder and 24 decoder layers, in total, about 750 million parameters. All the models were trained\nin the same manner, i.e. on the same tasks with the same amount of data and the same optimizer.\nWe compare two smaller models, which diﬀer in the amount of training (1 or 5 epochs), and three\nlarger models (1, 3, or 5 epochs).\n1https://github.com/google/sentencepiece\n3\nWe trained the models on a mixture of two self-supervised pre-training tasks: i.i.d. (indepen-\ndent and identically distributed) denoising and span corruption, suggested by Raﬀel et al. (2020).\nIn the i.i.d. denoising task, 15% tokens were randomly corrupted, i.e. replaced by a sentinel to-\nken. Each token has an equal probability of being corrupted (identically distributed) and all the\ncorruption/replacing events are independent from each other. The goal of the task is to denoise the\nsequence by generating the correct token in place of the sentinel. This task is identical to the span\ncorruption task, described in Section 2, except that all spans have the length of one token. The\nspan corruption task used in training SloT5 is identical to the one used for training English T5 and\nmultilingual mT5 models, with 15% of tokens corrupted and an average corrupted span length of 3\ntokens.\nThe T5-sl-small1 and T5-sl-large1 models were trained for 1 million steps, with a batch size of 4096\ntokens, in total a bit less than 1 epoch. This amount of training is supposed to be suﬃcient, consid-\nering the ratio between the training tokens and the number of the model parameters (Komatsuzaki,\n2019). Additionally, we trained T5-sl-small5 for 763,000 steps, with a batch size of 32,768 tokens, in\ntotal around 5 epochs. T5-sl-large3 and T5-sl-large5 were trained with a batch size of 8192 tokens,\nfor 1.83 million steps and 3.05 million steps, respectively, which results in about 3 and 5 epochs. We\ntrained the models on a DGX A-100 machine, using four 40 GB A100 GPUs. The training took\nabout 3 days for T5-sl-small1, about 12 days for T5-sl-small5, about 3 weeks for T5-sl-large1, about\n4 weeks for T5-sl-large3, and about 7 weeks for T5-sl-large5.\n4\nEvaluation\nWe evaluated our newly trained SloT5 models on 11 classiﬁcation and generative tasks: named\nentity recognition, sentiment classiﬁcation, lemmatization, text simpliﬁcation, two summarization\ntasks on diﬀerent datasets, and ﬁve (essentially classiﬁcation) tasks from the Slovene SuperGLUE\n(ˇZagar & Robnik-ˇSikonja, 2022b) benchmark (two question answering, two natural language infer-\nence, and a coreference resolution task).\nFor classiﬁcation tasks, we could use only the encoder stack of the T5 and added appropriate task-\nspeciﬁc output heads on top of it, thus completely ignoring/bypassing the decoder stack. However, we\ndecided to remodel the classiﬁcation tasks into generative tasks, mimicking the evaluation procedure\nproposed by Raﬀel et al. (2020). Therefore, each example contained only an input string and an\noutput string.\nNext, in Section 4.1, we describe all eleven evaluation tasks and explain their preprocessing for\nthe seq2seq models. The details of ﬁne-tuning the SloT5 models and other compared transformer\nmodels are contained in Section 4.2. In Section 4.3, we present the results. We present qualitative\nanalysis of the results on two tasks in Section 4.4.\n4.1\nEvaluation tasks\nIn this section, we describe the evaluation tasks and their preprocessing for T5 models. For the named\nentity recognition (NER) task and SuperGLUE tasks, we show the examples of original entries and\nentries preprocessed for T5 modelling in Table 2. We did not apply any special preprocessing for the\nsentiment analysis classiﬁcation task and the generative tasks.\n4.1.1\nClassiﬁcation tasks\nNamed entity recognition (NER) is a token-classiﬁcation task, where each token is labelled as a\nnamed entity (NE) or not, and, if yes with the category of the NE. We used a dataset based on the\nssj500k corpus v2.2 (Krek et al., 2019a). We covered three categories of NEs: persons, locations,\nand organizations. To our knowledge, there is no standardized way of solving the NER task using\nseq2seq models. We ﬁrst attempted to generate labels for each token in a sentence, but the dataset\nwas overwhelmed by label ”O”, which covers all tokens that are not NEs and includes other named\nentity categories (e.g., products).\nWe propose to solve the problem as a NE retrieval task.\nWe\npreﬁxed each training sentence with each NE category, thus generating three times the number of\ntraining examples. See an example of the input and output in Table 2.\n4\nTable 2: Original examples and T5 formatted versions for each of the SuperGLUE tasks and the\nNER task. T5 formatted examples are in the CSV format where the ﬁrst column is the input and\nthe second the output.\nBoolQ\n(original)\n{”label”: true, ”passage”: ”Kalcijev karbid - Kalcijev karbid je kemiˇcna spojina s\nkemiˇcno formulo CaC. Njegova glavna uporaba v industriji je pri proizvodnji acetilena\nin kalcijevega cianamida.”, ”question”: ”kalcijev karbid cac2 je surovina za proizvodnjo\nacetilena”}\nBoolQ\n(T5 formatted)\n”Sestavek: Kalcijev karbid - Kalcijev karbid je kemiˇcna spojina s kemiˇcno formulo\nCaC. Njegova glavna uporaba v industriji je pri proizvodnji acetilena in kalcijevega\ncianamida.\nVpraˇsanje: kalcijev karbid cac2 je surovina za proizvodnjo acetilena”,\n”Pravilno.”\nCB (original)\n{”premise”: ”Bil je kompleksen jezik. Ne zapisano, ampak predano. Lahko bi rekli, da\nje bil olupljen.”, ”hypothesis”: ”jezik je bil olupljen”, ”label”: ”entailment”}\nCB\n(T5 formatted)\n”premisa: Bil je kompleksen jezik. Ne zapisano, ampak predano. Lahko bi rekli, da je\nbil olupljen. hipoteza: jezik je bil olupljen”, ”implikacija”\nCOPA\n(original)\n{”premise”: ”Moje telo je metalo senco na travo.”, ”choice1”: ”Sonce je vzhajalo.”,\n”choice2”: ”Trava je bila pokoˇsena.”, ”question”: ”cause”, ”label” :0}\nCOPA\n(T5 formatted)\n”Premisa: Moje telo je metalo senco na travo. Prva moˇznost: Sonce je vzhajalo. Druga\nmoˇznost: Trava je bila pokoˇsena. Kaj je vzrok?”, ”prva”\nRTE\n(original)\n{”premise”: ”V Iraku ˇse ni bilo najdenega oroˇzja za mnoˇziˇcno uniˇcevanje.”, ”hypoth-\nesis”: ”V Iraku najdeno oroˇzje za mnoˇziˇcno uniˇcevanje.”, ”label”: ”not entailment”}\nRTE\n(T5 formatted)\n”premisa: V Iraku ˇse ni bilo najdenega oroˇzja za mnoˇziˇcno uniˇcevanje. hipoteza: V\nIraku najdeno oroˇzje za mnoˇziˇcno uniˇcevanje.”, ”ni implikacija”\nWSC\n(original)\n{”target”:\n{”span1 text”:\n”skodelico”, ”span2 text”:\n”bila”, ”span1 index”:\n4,\n”span2 index”: 9}, ”text”: ”Iz steklenice sem v skodelico nalival vodo, dokler ni bila\npolna.”, ”label”: true}\nWSC\n(T5 formatted)\n”WSC: Iz steklenice sem v * skodelico * nalival vodo, dokler ni # bila # polna.”,\n”Pravilno.”\nNER\n1130 4167 Bolj O\n(original)\n1130 4167 teoretiˇcno O\n1130 4167 pa O\n1130 4167 se O\n1130 4167 je O\n1130 4167 problema O\n1130 4167 lotil O\n1130 4167 Radical B-ORG\n1130 4167 Science I-ORG\n1130 4167 Journal I-ORG\n1130 4167 v O\n1130 4167 Londonu B-LOC\n1130 4167 . O\nNER\n(T5 formatted)\n”organizacije: Bolj teoretiˇcno pa se je problema lotil Radical Science Journal v Londonu\n.”, ”Radical Science Journal”\n”lokacije: Bolj teoretiˇcno pa se je problema lotil Radical Science Journal v Londonu\n.”, ”Londonu”\n”osebe: Bolj teoretiˇcno pa se je problema lotil Radical Science Journal v Londonu .”,\n”brez”\nThe desired output is a comma-separated list of NEs in the sentence pertaining to the preﬁxed\n5\ncategory. *-If there are no NEs of the given category in a sentence, we set the output in the training\nset to the Slovene word ”brez”, meaning ”none”/”empty”.\nThe resulting dataset still has most\nexamples with the output ”brez”. We balanced the training dataset by omitting examples without\nNEs with 95% probability. We followed the same procedure for the validation dataset, omitting 50%\nof examples without NEs. However, the test set was not modiﬁed and we kept all such examples in\nit.\nSentiment analysis (SA) is a sentence-level classiﬁcation task composed of tweets, each labelled\nwith one of three classes: ”positive”, ”negative”, or ”neutral”. We used Slovenian tweets from the\nTwitter sentiment dataset for 15 European languages (Mozetiˇc et al., 2016). Each class label was\ntranslated into Slovene as a single word to be generated by the model; no other formatting changes\nwere needed.\nSlovene SuperGLUE (ˇZagar & Robnik-ˇSikonja, 2022b) benchmark was translated from the En-\nglish SuperGLUE benchmark (Wang et al., 2019). It contains two separate datasets: one was trans-\nlated using machine translation and the other by human translators. Human translated datasets are\nof higher quality than machine translations but smaller in size for most tasks, as only subsets of the\ndatasets were translated. We used ﬁve tasks from the SuperGLUE benchmark: Boolean question\nanswering (BoolQ), Choice of Plausible Alternatives (COPA), CommitmentBank (CB), Recognizing\nTextual Entailment (RTE), and The Winograd Schema Challenge (WSC). For BoolQ, CB, COPA,\nand RTE, we used larger machine-translated datasets. For the WSC task, we used the human trans-\nlated dataset (WSC is impossible to translate with machine translation tools).\nBoolQ consists of triples: a passage, a question based on the passage, and an answer (true or false).\nIn the COPA task, the goal is to pick the correct of the two given sentences, which correctly relates\nto the given premise and relation (cause or eﬀect). CB and RTE datasets contain textual entailment\ntasks, where given a premise and a hypothesis, the goal is to predict whether the hypothesis entails\nthe premise or not. In the WSC task, two spans in a short text are highlighted. The goal is to\nidentify, using world knowledge and commonsense reasoning, whether both highlighted spans refer\nto the same entity.\nSuperGLUE tasks have multiple attributes. As we can only feed a single string input to the\nT5 model, we have preﬁxed each attribute value with its key and concatenated the attributes. For\nexample, examples in COPA task have the following attributes: premise, choice1, choice2, and\nquestion. The concatenated input string is of the format:\n”Premise: This is the example’s premise. First choice: this is the value of choice1. Second choice:\nthis is the value of choice2. What is the {cause, eﬀect}?”\nHere the cause and eﬀect are the two possible values of the attribute question.\nExamples in the WSC task contain two speciﬁcally marked texts within the input text. One\nspan is a noun and the other a pronoun or a verb with the pronoun information implicitly included.\nFollowing the original T5 example2, we indicate the ﬁrst span by surrounding it with an asterisk on\neach side, and the second span by surrounding it with a hash symbol on each side.\n4.1.2\nGenerative tasks\nWe tested SloT5 models on three generative tasks: lemmatization, summarization (two datasets),\nand text simpliﬁcation.\nFor lemmatization (Lem) we used a part of the Slovene ssj500k dataset (Krek et al., 2019a)\nincluded in the universal dependencies dataset. The model received an individual sentence on the\ninput and was trained to generate the same sentence with every word lemmatized. Punctuation\nmarks were included in the training and test sets, but we ignored them during the scoring.\nFor summarization, we used two news datasets: AutoSentiNews (ASN) (Buˇcar et al., 2018) and\nSlovene Press Agency (STA) news (ˇZagar & Robnik-ˇSikonja, 2022a), extracted from the Gigaﬁda\ncorpus. We ﬁne-tuned and evaluated the T5 models on each dataset separately, treating each as a\nseparate task. During ﬁne-tuning, the input to a T5 model was an article text, and the output was\nits summary.\nText simpliﬁcation task aims to simplify the input text to increase its readability.\nCommon\n2https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/data/preprocessors.py\n6\nstrategies include splitting long, complex sentences into multiple shorter, simple sentences, and re-\nplacing complex words with simpler, commonly used words. We utillized the Slovene text simpli-\nﬁcation dataset SloTS (Gorenc & Robnik-ˇSikonja, 2022), which contains sentence-aligned complex\ntexts (original) and their simpliﬁed versions. The dataset contains entries, where a single complex\nsentence is repeated several times, each time paired with a diﬀerent simple sentence simplifying a part\nof the complex sentence. We merged all such entries into a single instance, containing the complex\nsentence and concatenated simpliﬁed sentences. For example, three entries [(c1, s1), (c1, s2), (c1, s3)]\nwere merged into one entry [(c1, s1s2s3)].\n4.2\nFine-tuning T5 and compared models\nWe ﬁne-tuned all compared T5 models (Slovene and multilingual) end-to-end on each task separately,\nusing the HuggingFace transformers library 3. We used the AdamW optimizer with the batch size\nof 64. We saved the ﬁne-tuned model after each epoch and selected the one that performed best\non the validation set, using the ROUGE-L metric (Lin, 2004). We used the greedy search decoding\nmethod for the output generation and limited the maximum number of tokens in the output. We\nchose the maximum output length based on the target text length, shorter for the classiﬁcation tasks\nand longer for the generative tasks. The maximum output lengths and the number of ﬁne-tuning\nepochs for each task are presented in Table 3. The complete code of our experiments is publicly\navailable4.\nTable 3: Evaluation parameters and performance metrics for seq2seq and BERT models for each of\nthe datasets.\nEpochs\nOutput len.\nTask\nseq2seq\nBERT\n(seq2seq)\nMetric\nDataset\nBoolQ\n10\n100\n4\naccuracy\n(ˇZagar et al., 2020)\nCB\n15\n100\n6\nF1\n(ˇZagar et al., 2020)\nCOPA\n15\n100\n6\naccuracy\n(ˇZagar et al., 2020)\nRTE\n15\n100\n6\naccuracy\n(ˇZagar et al., 2020)\nWSC\n20\n100\n6\naccuracy\n(ˇZagar et al., 2020)\nNER\n20\n3\n64\nF1\n(Krek et al., 2019a)\nSA\n10\n10\n5\nF1\n(Mozetiˇc et al., 2016)\nLem\n15\n-\n512\nword/sent. acc.\n(Krek et al., 2019a)\nSTA\n5\n-\n512\nROUGE L\n(Krek et al., 2019b)\nASN\n5\n-\n512\nROUGE L\n(Buˇcar, 2017)\nSloTS\n64\n-\n256\nROUGE L\n(Gorenc & Robnik-ˇSikonja, 2022)\nWe compared SloT5 models with multilingual mT5 models (Xue et al., 2021), multilingual mBART-\n50 model (Tang et al., 2021), and with four encoder BERT-like models (described below). We ﬁne-\ntuned the mT5 and mBART-50 models in the exact same manner as the SloT5 models on all ten\ntasks. BERT-like models were ﬁne-tuned on seven classiﬁcation tasks, but not on the generative tasks\n(Lem, ASN, STA, SloTS), as they cannot generate text. ˇZagar & Robnik-ˇSikonja (2022b) evaluated\nBERT-like models on the Slovene SuperGLUE benchmark. The BERT models were ﬁne-tuned on\neach task individually for 100 epochs using the Jiant tool (Phang et al., 2020) with the initial learning\nrate of 10−5. Ulˇcar & Robnik-ˇSikonja (2021) evaluated BERT models supporting Slovene on NER\nand SA tasks. They added a softmax classiﬁcation layer on top of the BERT models and ﬁne-tuned\nthem for 3 epochs on the NER task with a batch size of 8, and for 10 epochs on the SA task with a\nbatch size of 32.\n3https://huggingface.co/\n4https://github.com/MatejUlcar/SloT5-tools\n7\n4.3\nQuantitative results\nWe compared the results of our three monolingual SloT5 models (described in Section 3) on the\neleven tasks (described in Section 4.1) with two multilingual T5 models of comparable sizes: mT5-\nsmall and mT5-large (Xue et al., 2021), and with a multilingual BART model (mBART-50-large)\n(Tang et al., 2021). Due to their larger vocabulary sizes, mT5 models have many more parameters\nthan comparable SloT5 models (300M vs. 60M for small and 1.2B vs. 750M for large). However, the\ntransformer layers are identical in their number and size for both small models and for both large\nmodels. mBART-50-large model has 611M parameters, 12 encoder and 12 decoder layers, thus it lies\nsomewhere between small and large T5 model concerning size.\nFor the classiﬁcation tasks, we also compared the results with four encoder BERT-like mod-\nels: multilingual BERT model (mBERT), multilingual XLM-RoBERTa model (XLM-R), trilingual\nCroSloEngual model (CSE, Croatian-Slovene-English) (Ulˇcar & Robnik-ˇSikonja, 2020), and mono-\nlingual Slovene RoBERTa model (SloBERTa).\n4.3.1\nClassiﬁcation tasks\nThe evaluation results on classiﬁcation tasks are presented in Table 4. Some T5 models score 0 on\ncertain SuperGLUE tasks. The reason is that the tasks were reformatted as generative tasks, and\nwe check whether the generated text is equal to any of the class labels (in case of 0, it was not).\nWe perform only minor post-process ﬁltering of the generated texts, such as removing <extra id 0>\ntokens added by the T5 models.\nOn SuperGLUE tasks, all seq2seq models perform poorly. While they do outperform BERT-\nlike models on BoolQ and RTE tasks, they barely beat the majority classiﬁer on both tasks. The\nexception is the mBART-50-large model, which lags behind the majority classiﬁer on BoolQ, and\nmT5-small, which performs worse than majority classiﬁer on RTE. On RTE, T5-sl-large5 and T5-sl-\nsmall5 perform the best of all evaluated models on this task.\nOn NER, the multilingual mT5-small model performs poorly, while mT5-large is the best T5\nmodel. All the T5 models lag behind the BERT-like models on the NER task. The dataset used\nfor the SA task has a low inter-annotator agreement, limiting the overall performance. The best\nperforming T5 models on the SA task, mT5-large and T5-sl-large5 perform on par and are only\nslightly worse than the best model on this task, SloBERTa. The small Slovene T5 models perform\non par with multilingual BERT models on the SA task.\nTable 4: Results of the compared T5 and BERT models on classiﬁcation tasks. The metric for each\ntask is shown in Table 3. The results of the best performing model for each task are in bold, the\nresults of the best performing seq2seq model are underlined.\nModel\nBoolQ\nCB\nCOPA\nRTE\nWSC\nNER\nSA\nMaj. class.\n63.3\n21.7\n50.0\n58.6\n65.8\n0.0\n20.3\nT5-sl-small1\n66.6\n0.0\n47.6\n58.6\n47.9\n48.1\n57.4\nT5-sl-small5\n66.6\n0.0\n50.0\n65.5\n65.1\n66.0\n60.4\nT5-sl-large1\n70.0\n48.8\n51.6\n58.6\n61.6\n53.2\n59.2\nT5-sl-large3\n60.0\n50.9\n48.8\n62.1\n64.3\n70.8\n61.0\nT5-sl-large5\n63.3\n62.2\n50.6\n65.5\n59.6\n66.9\n61.8\nmT5-small\n70.0\n0.0\n0.0\n55.2\n0.0\n2.7\n56.6\nmT5-large\n70.0\n0.0\n0.0\n58.6\n45.2\n79.0\n61.9\nmBART-50-large\n56.7\n48.6\n50.0\n62.1\n65.1\n74.7\n54.7\nmBERT\n63.3\n65.1\n54.4\n57.9\n61.6\n88.5\n57.6\nXLM-R\n63.3\n62.0\n51.4\n42.8\n65.8\n91.2\n60.4\nCSE BERT\n63.3\n59.8\n55.0\n53.8\n56.2\n92.8\n61.0\nSloBERTa\n63.3\n68.6\n58.2\n49.6\n73.3\n93.3\n62.3\n8\n4.3.2\nGenerative tasks\nThe results on the generative tasks (lemmatization, two summarization tasks, and text simpliﬁcation)\nare shown in Table 5.\nTable 5: Results of the compared seq2seq models on generative tasks: lemmatization (Lem), two\nsummarization tasks (STA and ASN), and text simpliﬁcation (TS). The metric for each task is\nshown in Table 3. The results of the best performing model for each task are in bold.\nModel\nLem\nSTA\nASN\nTS\nT5-sl-small1\n90.3/37.1\n20.6\n20.0\n29.1\nT5-sl-small5\n95.6/62.2\n22.1\n22.3\n33.2\nT5-sl-large1\n95.5/58.5\n21.5\n21.9\n28.3\nT5-sl-large3\n97.2/72.2\n25.0\n22.7\n30.4\nT5-sl-large5\n97.0/73.6\n25.3\n22.5\n30.2\nmT5-small\n90.0/25.5\n17.9\n19.2\n26.3\nmT5-large\n98.3/75.6\n24.5\n23.2\n35.3\nmBART-50-large\n97.8/73.2\n27.6\n24.1\n33.7\nWhile mBART-50-large does not perform very well on the classiﬁcation tasks, it is the best\nperforming model on two out of four generative tasks, and the second best model on the other two\ntasks. If we compare only T5 models, large models consistently outperform small models, when\ntrained on the same amount of data. The diﬀerence in performance is especially notable for the mT5\nmodels, while it is not as big for the SloT5 models. In general, the diﬀerence in performance between\nT5 models is the same as observed on NER and SA tasks: mT5-large performs the best (excluding\nmBART), followed by T5-sl-large, T5-sl-small, while mT5-small performs the worst.\nThe diﬀerence in training time has a large impact for the T5-sl-small model, as with more training\nthe performance improves signiﬁcantly on most tasks, especially the generative tasks. T5-sl-small5\noutperforms T5-sl-large1 on most tasks, the exceptions are BoolQ, CB, and COPA. While longer\ntraining does improve the performance of T5-sl-large model, the diﬀerence is modest and most no-\nticeable between one and three epochs. Surprisingly, on the tested datasets, more training of large\nmodels does not always help and it is unclear whether T5-sl-large3 or T5-sl-large5 is the best per-\nforming SloT5 model; the results depend on the task.\n4.4\nQualitative analysis\nIn general, quantitative results are less informative for generative tasks compared to classiﬁcation\ntasks. The main reason is that the evaluation metrics such as ROUGE-L score are not strongly\ncorrelated with human judgements. Below, we provide qualitative analysis of the text simpliﬁcation\nand summarization results, while for lemmatization we did not notice any signiﬁcant patterns.\n4.4.1\nText simpliﬁcation\nWe qualitatively analyzed the four best performing models on the text simpliﬁcation task, according\nto the ROUGE-L metric. We show selected examples from the test set of SloTS dataset in Table 6.\nWe selected examples where at least some of the models generate a reasonable simpliﬁcation, and\nwhere noticeable and interesting diﬀerence between the models can be observed.\nThe examples\nalong with the models’ outputs were translated into English, trying to mimic the original as best as\npossible, including the mistakes (where present). The original Slovene examples are shown in Table 7\nin Appendix.\nOne of the diﬃculties of the SloTS text simpliﬁcation dataset is that many complex texts are\narchaic and sometimes poetic. The simpliﬁed text in the dataset tends to be written in more contem-\nporary standard language. Such examples are very diﬃcult for the seq2seq models to simplify and\nthey mostly generate extractive summaries, leaving out adjectives and subordinate clauses. This can\nbest be seen in the ﬁrst and the last two examples in Table 6. Another large issue in this task is the\n9\nhallucination, as all the models frequently invent information not present in the original sentence.\nThis is most commonly the case with mBART-50-large model, which is the second best perform-\ning model, according to the ROUGE-L score. T5-sl-small5 is the most robust model on this task.\nCompared to other models, it the most consistently produces coherent and truthful simpliﬁcations,\nthough it still often invents new information. On the other hand, it most frequently generates shorter\noutputs, leaving out information in subordinate clauses. On the examples, where all the models fail,\nT5-sl-small5 tends to perform the worst. mT5-large achieves the best ROUGE-L score. When ex-\namining its outputs, however, it seems that it either works very well (the ﬁrst and the last example\nin Table 6) or completely fails to produce meaningful or even grammatically correct sentences (the\nthird and the fourth example in Table 6).\nWhen dealing with a relatively simple example with neutral language (example 2 in Table 6), all\nmodels perform very well. However, on a more complex and longer example in the same domain (not\nshown to save space), none of the models produce a meaningful simpliﬁcation.\n4.4.2\nSummarization\nBelow we summarize the qualitative ﬁndings concerning the results of diﬀerent models on the ASR\nsummarization dataset. Due to their length, we defer a few illustrative examples to Appendix (see\nthe original Slovene examples in Table 8 and their translations into English in Table 9).\nIn our summarization tasks, the ROUGE-L scores (measuring the longest n-gram overlap be-\ntween the golden and generated summary) do no accurately represent the quality of the generated\nsummaries. The generated summaries may be correct, concise and easy to read, yet the scores are\nlow, because they focused on a diﬀerent aspect of the news article than the provided golden sum-\nmary. Commonly, the provided golden summary rounds or approximates the numbers and heavily\nparaphrases the text. The summaries generated by seq2seq models do not round the numbers and\nfrequently copy whole sentences from the original news article. When they do paraphrase the text,\nthey usually do it diﬀerently than the golden summaries.\nA smaller T5-sl-small5 model commonly generates summary only from the beginning or at most\nthe ﬁrst half of the article. It is also more prone to copying whole sentences from the input text.\nThe biggest issue of T5-sl-small5 is mixing up factual indicators, e.g., increase vs. decrease, most vs.\nleast. It also tends to invent named entities, especially locations, putting many events in Ljubljana,\nthe capital and the largest city of Slovenia, especially when no location is indicated in the original\narticle (see the ﬁrst example in Table 9 in Appendix). Occasionally, the model is unable to form a\ncoherent summary.\nThe mBART-50-large model has similar issues as T5-sl-small5, but on a smaller scale. It tends not\nto mix factual indicators. It is the most robust model in the sense that it most frequently produces\na summary that conveys the crucial information in the article. It does so by frequently copying one\nor two input sentences it identiﬁes as the most important to the output and only slightly modiﬁes\nthem. However, it does have issues with named entities, mostly leaving them out of the generated\nsummaries. Thus, we often can not tell who did what or where, just that something was done (see the\nsecond and third examples in Table 9 in Appendix). Similarly to the simpliﬁcation task, mBART-\n50-large has the largest tendency to hallucinate (in this case invent wrong named entities) among\nthe analyzed models (see the ﬁrst example in Table 9). It is also the model with the largest number\nof grammatical mistakes. When omitting subordinate clauses, adjectives and verbs, or changing the\nverb, the noun declensions and/or verb conjugations should also be changed to ﬁt the new sentence,\nbut the model leaves them in the same form as in the input text.\nT5-sl-large3 and mT5-large both tend to generate good summaries. While we can observe many\ndiﬀerences between the generated summaries of the two models, we can not point out any signiﬁcant\nqualitative diﬀerences. The diﬀerences are mainly stylistic or due to chance. T5-sl-large3 paraphrases\nthe text more often (see the third example in Table 9). mT5-large, on the other hand, has more\nclosely adapted to the summary format of the golden summaries.\n10\nTable 6: Examples of generated simpliﬁed sentences on the text simpliﬁcation task for the best\nperforming models, translated into English. Grammatical and other mistakes in the model outputs\nwere intentionally kept.\nWhen they noticed Matic, who was ringing the bells with all his might, they suddenly\nstopped and looked at him wide-eyed then asked him: ”Matic, where is the ﬁre?”\ngolden\nThey saw that Enaka Palica was ringing. People asked: where is the\nﬁre?\nmT5-large\nWhen people were watching Matic, they got very angry. They were\nasking him: Matic, where is the ﬁre?\nmBART-50-large\nThe judges decided, where it burns.\nT5-sl-small5\nThey asked Matic: Where’s the ﬁre?\nT5-sl-large3\nWhen they noticed Matic, they looked at him wide-eyed.\nWith the access ramp, the miniature train is accessible also for people using wheelchairs.\ngolden\nPeople with wheelchairs can also ride with Urban.\nmT5-large\nThe miniature train is also accessible to people using wheelchairs.\nmBART-50-large\nPeople using wheelchairs can get also an access ramp.\nT5-sl-small5\nThe miniature train is accessible also to disabled people.\nT5-sl-large3\nThe miniature train is accessible also to people, who have a wheelchair.\nAnd because the branches were swinging, so was the Moon above them, which was hanging from\nthe sky. It was really bloody, and no longer fully round, and it was hanging low, like a lonely paper\nlantern, beaten, yet still burning above an empty fairground.\ngolden\nThe Moon was coloured red.\nmT5-large\nHe was bloody.\nmBART-50-large\nThe fair was over. Now, the father has returned to the stand.\nT5-sl-small5\nAt the fair was also the Moon, which was very bloody.\nT5-sl-large3\nIt has never haunted again.\nAbove the pile of the old wall, covered with blackberries, clematis, and other shrubs, only the former\ncastle tower was standing tall, proudly braving the hundred-year storms and gales.\ngolden\nAn old tower also remained.\nmT5-large\nOn tower was many cstling plants.\nmBART-50-large\nIn the older part was also the castle tower.\nT5-sl-small5\nThere was a large tower on the courtyard.\nT5-sl-large3\nThere used to be only a tower in the town.\nAfter that I was storming around Germany and taking part in battles, which were truly beautiful.\ngolden\nI liked to battle.\nmT5-large\nAlong the way I met German of soldiers.\nmBART-50-large\nAfter that I was storming around Germany and taking part in battles.\nT5-sl-small5\nI took part in battles in Germany.\nT5-sl-large3\nI strolled a lot around Germany.\nThe baby was at that time already two years old and his mother was there with him on the ﬁeld\nabove the road, where she was hoeing.\ngolden\nAt that time the boy was 2 years old. Terba was digging on the ﬁeld.\nThe boy was along with her.\nmT5-large\nMother was with him at home. She was digging and hoeing.\nmBART-50-large\nMother was then already two years old.\nT5-sl-small5\nBaby was very old.\nT5-sl-large3\nThere was a large baby on the ﬁeld.\n11\n5\nDiscussion and conclusions\nWe presented three new T5 seq2seq models for Slovene. Our comparison of monolingual and multi-\nlingual T5 and BERT-based models, applicable to Slovene, shows that in general, for classiﬁcation\ntasks, BERT models are preferable, while for text generation tasks, T5 models show reasonable per-\nformance. The speciﬁc ﬁndings are elaborated on below. While the results are obtained on Slovene,\nwe believe that they may generalize to other less-resourced languages, where such models will be\nbuilt. We make the training and evaluation code, as well as the trained models publicly available.\nThe code can be found at https://github.com/MatejUlcar/SloT5-tools. The released models can be\nfound at https://www.huggingface.co/cjvt.\nBoth small Slovene T5 models outperform the multilingual small T5 model. However, the large\nmultilingual model outperforms the large Slovene T5 model. Since T5-sl-small1 and T5-sl-large1 were\ntrained for an equal amount of steps, we assume that the larger model is under-trained. Komatsuzaki\n(2019) and Hoﬀmann et al. (2022) have recently presented evidence that the amount of training needs\nto scale with the size of the model. However, there is no consensus on the optimal amount of training\nrequired for a given model architecture. Komatsuzaki (2019) suggests that given a ﬁxed number\nof FLOPS (ﬂoating point operations per second) the optimal ratio between the number of training\ntokens and the number of model parameters is around 5. Hoﬀmann et al. (2022) on the other hand,\nreport that ratio should be larger, around 20.\nFor our T5-sl-large1 model, this ratio is 5.5, for\nT5-sl-large3 20, for T5-sl-large5 33, for T5-sl-small1 68 and for T5-sl-small5 414.\nT5-sl-small5 and T5-sl-large1 were trained using roughly equal amount of computing power. Since\nT5-sl-small5 outperforms T5-sl-large1 on most tasks, we conclude that the optimal ratio between the\nnumber of training tokens and model parameters must be higher than 5 for Slovenian T5 models.\nWe observe that T5-sl-small strongly outperforms multilingual mT5-small. On the other hand,\nmT5-large performs better than T5-sl-large. Furthermore, while T5-sl-large1 is clearly worse than\nT5-sl-large3 and T5-sl-large5, the diﬀerence between the latter two is negligible. We hypothesize\nthat the reason for better performance of the small SloT5 model, in comparison with the mT5, is\nthat the small models have too few parameters to successfully encode (and decode) the information\nin multiple languages, so a monolingual model prevails. Our hypothesis for the worse performance\nof T5-sl-large, compared to mT5-large and mBART-50-large, is that there was not enough training\ndata to successfully train a model of this size, especially since further training (for more epochs) does\nnot seem to improve the performance. mT5-large was trained on a much larger training corpus, and\neven its Slovenian portion was almost twice as large as our corpus.\nT5 and other seq2seq models can generate text, making them suitable for solving a wider variety\nof NLP tasks than encoder-only models, such as BERT. However, compared to BERT-like models,\nT5 models seem to be much more sensitive to unbalanced classes and smaller datasets. In addition to\njust classifying the input, the T5 models also have to learn how to form a coherent response. This is\na simple task for a limited scope of available answers, such as most SuperGLUE classiﬁcation tasks,\nbut considerably diﬀerent for the NER task, which we have formatted as the text retrieval task.\nStill, multilingual T5 models, especially mT5-small, have often failed in learning to generate even a\nsensible incorrect answer, i.e. predicting any class, even incorrect. Instead, they generate answers\nthat are not identiﬁable with any class value.\nFine-tuning T5 models for more epochs on a speciﬁc task might solve the issue of generating\nnonsensical answers; however, we may over-ﬁt the models. Furthermore, on models that did not have\nthis problem, we have not observed a signiﬁcant change in performance on the SuperGLUE tasks\nwhen training for more than 6-8 epochs.\nAlthough the English T5 models (Raﬀel et al., 2020) were pre-trained on multiple tasks, including\nthe SuperGLUE tasks, the authors ﬁne-tuned the pre-trained models for each task during the evalu-\nation. Their results show that the largest T5 models achieve better results than the RoBERTaLARGE\n(Liu et al., 2019) baseline. However, those models are of an order of magnitude larger than the base-\nline model. Comparing the performance of similarly sized models, the RoBERTa model outperforms\nT5 on all SuperGLUE tasks. Xue et al. (2021) reported much better performance of mT5 models\ncompared to multilingual BERT-like models in the zero-shot cross-lingual setting. In a monolingual\nsetting, only the largest (3B and 11B) mT5 models outperform mBERT on the NER task. On the\nother hand, on the question answering task, all mT5 models (except for the smallest mT5-small) out-\n12\nperform the mBERT score. This is in line with our ﬁndings, where we observe a slight improvement\nof T5 models over BERT-like models on the question answering BoolQ task but worse performance\non other SuperGLUE tasks.\nIn future work, we will try to obtain more Slovene data and retrain the large Slovene T5 model\nto analyze the behaviour of the generative models with respect to the size of the training data. As\ntext generation seems to be a stronger side of T5 models, we will expand the set of tackled tasks to\nparaphrasing and grammar correction tasks.\nFunding\nThe work was partially supported by the Slovenian Research Agency (ARRS) core research pro-\ngramme P6-0411 and projects J6-2581, J7-3159 and J1-2480, as well as the Ministry of Culture of\nRepublic of Slovenia through project Development of Slovene in Digital Environment (RSDO).\nAcknowledgments\nWe acknowledge the eﬀorts of SLING, Slovene national supercomputing grid for providing the nec-\nessary computational resources.\nReferences\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S.,\nBohg, J., Bosselut, A., Brunskill, E. et al. (2021). On the opportunities and risks of foundation\nmodels. ArXiv preprint 2108.07258, .\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R.,\nRamesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S.,\nChess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D. (2020).\nLanguage models are few-shot learners. In Advances in Neural Information Processing Systems\n(pp. 1877–1901). volume 33.\nBuˇcar, J. (2017). Automatically sentiment annotated Slovenian news corpus AutoSentiNews 1.0.\nURL: http://hdl.handle.net/11356/1109 Slovenian language resource repository CLARIN.SI.\nBuˇcar, J., ˇZnidarˇsiˇc, M., & Povh, J. (2018). Annotated news corpora and a lexicon for sentiment\nanalysis in slovene. Language Resources and Evaluation, 52, 895–919.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) (pp. 4171–4186). doi:10.18653/v1/N19-1423.\nErjavec, T., Fiˇser, D., & Ljubeˇsi´c, N. (2021).\nThe KAS corpus of Slovenian academic writing.\nLanguage Resources and Evaluation, 55, 551–583.\nErjavec, T., Ljubeˇsi´c, N., & Fiˇser, D. (2017a). Blog post and comment corpus Janes-Blog 1.0. URL:\nhttp://hdl.handle.net/11356/1138 Slovenian language resource repository CLARIN.SI.\nErjavec, T.,\nLjubeˇsi´c, N., & Fiˇser, D. (2017b).\nForum corpus Janes-Forum 1.0.\nURL:\nhttp://hdl.handle.net/11356/1139 Slovenian language resource repository CLARIN.SI.\nErjavec, T., Ljubeˇsi´c, N., & Fiˇser, D. (2017c).\nNews comment corpus Janes-News 1.0.\nURL:\nhttp://hdl.handle.net/11356/1140 Slovenian language resource repository CLARIN.SI.\nFiˇser, D., Erjavec, T., & Ljubeˇsi´c, N. (2016). JANES v0. 4: Korpus slovenskih spletnih uporabniˇskih\nvsebin. Slovenˇsˇcina 2.0: empirical, applied and interdisciplinary research, 4, 67–99.\n13\nGorenc, S., & Robnik-ˇSikonja, M. (2022).\nSlovene text simpliﬁcation dataset SloTS.\nURL:\nhttp://hdl.handle.net/11356/1682 Slovenian language resource repository CLARIN.SI.\nHoﬀmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L.,\nHendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G.\nv. d., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., &\nSifre, L. (2022). Training compute-optimal large language models. ArXiv preprint 2203.15556, .\ndoi:10.48550/ARXIV.2203.15556.\nKomatsuzaki,\nA.\n(2019).\nOne epoch\nis\nall\nyou\nneed.\nArXiv preprint\n1906.06669,\n.\ndoi:10.48550/ARXIV.1906.06669.\nKrek, S., Arhar Holdt, ˇS., Erjavec, T., ˇCibej, J., Repar, A., Gantar, P., Ljubeˇsi´c, N., Kosem, I.,\n& Dobrovoljc, K. (2020). Gigaﬁda 2.0: The reference corpus of written standard Slovene. In\nProceedings of the 12th Language Resources and Evaluation Conference (pp. 3340–3345).\nKrek, S., Dobrovoljc, K., Erjavec, T., Moˇze, S., Ledinek, N., Holz, N., Zupan, K., Gantar, P.,\nKuzman, T., ˇCibej, J., Arhar Holdt, ˇS., Kavˇciˇc, T., ˇSkrjanec, I., Marko, D., Jezerˇsek, L., & Zajc,\nA. (2019a). Training corpus ssj500k 2.2. Slovenian language resource repository CLARIN.SI.\nKrek, S., Erjavec, T., Repar, A., ˇCibej, J., Arhar Holdt, ˇS., Gantar, P., Kosem, I., Robnik-ˇSikonja,\nM., Ljubeˇsi´c, N., Dobrovoljc, K., Laskowski, C., Grˇcar, M., Holozan, P., ˇSuster, S., Gorjanc,\nV., Stabej, M., & Logar, N. (2019b). Corpus of written standard Slovene Gigaﬁda 2.0. URL:\nhttp://hdl.handle.net/11356/1320 Slovenian language resource repository CLARIN.SI.\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & Zettle-\nmoyer, L. (2020). BART: Denoising sequence-to-sequence pre-training for natural language genera-\ntion, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics (pp. 7871–7880). doi:10.18653/v1/2020.acl-main.703.\nLin, C.-Y. (2004). ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out (pp. 74–81).\nLiu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., & Zettlemoyer, L.\n(2020). Multilingual Denoising Pre-training for Neural Machine Translation. Transactions of the\nAssociation for Computational Linguistics, 8, 726–742. doi:10.1162/tacl_a_00343.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &\nStoyanov, V. (2019). RoBERTa: A robustly optimized BERT pretraining approach. ArXiv preprint\n1907.11692, .\nLjubeˇsi´c, N., & Erjavec, T. (2011). hrWaC and slWaC: Compiling web corpora for Croatian and\nSlovene. In International Conference on Text, Speech and Dialogue (pp. 395–402).\nLjubeˇsi´c, N., Erjavec, T., & Fiˇser, D. (2017).\nWikipedia talk corpus Janes-Wiki 1.0.\nURL:\nhttp://hdl.handle.net/11356/1137 Slovenian language resource repository CLARIN.SI.\nMozetiˇc, I., Grˇcar, M., & Smailovi´c, J. (2016). Multilingual Twitter sentiment classiﬁcation: The\nrole of human annotators. PLOS ONE, 11.\nMozetiˇc, I., Grˇcar, M., & Smailovi´c, J. (2016). Twitter sentiment for 15 european languages. URL:\nhttp://hdl.handle.net/11356/1054 Slovenian language resource repository CLARIN.SI.\nNagoudi,\nE. M. B.,\nElmadany,\nA.,\n& Abdul-Mageed,\nM. (2021).\nAraT5:\nText-to-Text\nTransformers for Arabic Language Generation, .\nURL: https://arxiv.org/abs/2109.12068.\ndoi:10.48550/ARXIV.2109.12068.\nPanˇcur, A., & Erjavec, T. (2020).\nThe siParl corpus of Slovene parliamentary proceedings.\nIn\nProceedings of the Second ParlaCLARIN Workshop (pp. 28–34).\n14\nPanˇcur, A., Erjavec, T., Ojsterˇsek, M., ˇSorn, M., & Blaj Hribar, N. (2020). Slovenian parliamentary\ncorpus (1990-2018) siParl 2.0. URL: http://hdl.handle.net/11356/1300 Slovenian language\nresource repository CLARIN.SI.\nPhang, J., Yeres, P., Swanson, J., Liu, H., Tenney, I. F., Htut, P. M., Vania, C., Wang, A., & Bowman,\nS. R. (2020). jiant 2.0: A software toolkit for research on general-purpose text understanding\nmodels. http://jiant.info/.\nPomik´alek, J. (2011). Removing boilerplate and duplicate content from web corpora. Ph.D. thesis\nMasaryk university, Brno, Czech Republic.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are\nUnsupervised Multitask Learners. Technical Report OpenAI blog. 2019 Feb 24.\nRaﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J.\n(2020). Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal\nof Machine Learning Research, 21, 1–67.\nSarti, G., & Nissim, M. (2022).\nIT5: Large-scale Text-to-text Pretraining for Italian Language\nUnderstanding and Generation, . doi:10.48550/ARXIV.2203.03759.\nTang, Y., Tran, C., Li, X., Chen, P.-J., Goyal, N., Chaudhary, V., Gu, J., & Fan, A. (2021). Multi-\nlingual translation from denoising pre-training. In Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021 (pp. 3450–3466). doi:10.18653/v1/2021.findings-acl.304.\nUlˇcar, M., & Robnik-ˇSikonja, M. (2021). SloBERTa: Slovene monolingual large pretrained masked\nlanguage model. In 24th international multiconference Information Society 2021. volume C. Data\nMining and Data Warehouses.\nUlˇcar, M.,\nˇZagar, A., Armendariz, C. S., Repar, A., Pollak, S., Purver, M., & Robnik-\nˇSikonja, M. (2021). Evaluation of contextual embeddings on less-resourced languages. Preprint\narXiv:2107.10614, .\nUlˇcar, M., & Robnik-ˇSikonja, M. (2020). FinEst BERT and CroSloEngual BERT: less is more in\nmultilingual models. In Proceedings of Text, Speech, and Dialogue, TSD 2020 (pp. 104–111).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,  L., & Polo-\nsukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems\n(pp. 5998–6008).\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R.\n(2019). SuperGLUE: A stickier benchmark for general-purpose language understanding systems.\nIn Proceedings of the 33rd International Conference on Neural Information Processing Systems.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. (2018). GLUE: A multi-task\nbenchmark and analysis platform for natural language understanding. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (pp. 353–\n355). doi:10.18653/v1/W18-5446.\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., & Raﬀel, C.\n(2021). mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the\n2021 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (pp. 483–498).\nˇZagar, A., Kavaˇs, M., Robnik-ˇSikonja, M., Erjavec, T., Fiˇser, D., Ljubeˇsi´c, N., Ferme, M., Boroviˇc,\nM., Boˇskoviˇc, B., Ojsterˇsek, M., & Hrovat, G. (2022). Corpus of academic Slovene KAS 2.0. URL:\nhttp://hdl.handle.net/11356/1448 Slovenian language resource repository CLARIN.SI.\nˇZagar, A., & Robnik-ˇSikonja, M. (2022a). Cross-lingual transfer of abstractive summarizer to less-\nresource language. Journal of Intelligent Information Systems, 58, 153–173.\n15\nˇZagar, A., & Robnik-ˇSikonja, M. (2022b). Slovene SuperGLUE Benchmark: Translation and Evalu-\nation. In Proceedings of Language Resources and Evaluation, LREC.\nˇZagar, A., Robnik-ˇSikonja, M., Goli, T., & Arhar Holdt, ˇS. (2020). Slovene translation of SuperGLUE.\nURL: http://hdl.handle.net/11356/1380 Slovenian language resource repository CLARIN.SI.\nZhang, Z., Zhang, H., Chen, K., Guo, Y., Hua, J., Wang, Y., & Zhou, M. (2021). Mengzi: Towards\nlightweight yet ingenious pre-trained models for Chinese, . doi:10.48550/ARXIV.2110.06696.\nA\nAdditional examples\n16\nTable 7: Original Slovene examples of generated simpliﬁed sentences on the text simpliﬁcation task\nfor the best performing models. The English translations are contained in Table 6.\nKo so zagledali Matica, ki je zvonil na vse pretege, so najprej obstali in ga debelo pogledali, nato pa\nso butnili vanj: ”Matic, kje gori?”\ngolden\nVideli so, da zvoni Enaka Palica. Ljudje so vpraˇsali: Kje gori?\nmT5-large\nKo so ljudje gledali Matica, so se zelo razjezili. Spraˇsevali so ga: Matic, kje gori?\nmBART-50-large\nSodniki so se odloˇcili, kje gori.\nT5-sl-small5\nMatica so vpraˇsali: Kje gori?\nT5-sl-large3\nKo so Matica zagledali, so ga debelo pogledali.\nS klanˇcino je vlakec dostopen tudi osebam na invalidskih voziˇckih.\ngolden\nZ Urbanom se lahko peljejo ljudje na voziˇckih.\nmT5-large\nVlakec je dostopen tudi osebam na invalidskih voziˇckih.\nmBART-50-large\nLjudje na invalidskih voziˇckih lahko dobijo tudi klanˇcino.\nT5-sl-small5\nVlakec je dostopen tudi invalidom.\nT5-sl-large3\nVlakec je dostopen tudi osebam, ki imajo invalidski voziˇcek.\nIn ker so se zibale veje, se je nad njimi zibal tudi mesec, ki je visel izpod neba. Bil je res krvav in ne\nveˇc popolnoma okrogel, in visel je nizko, kakor bi nad praznim sejmiˇsˇcem svetil ˇse edini, neugasli, a\nˇze obtolˇceni lampijon.\ngolden\nLuna je bila rdeˇce barve.\nmT5-large\nBil je krvav.\nmBART-50-large\nSejma je bilo konec. Zdaj se je oˇce vrnil na stojnico.\nT5-sl-small5\nNa sejmu je bil tudi mesec, ki je bil zelo krvav.\nT5-sl-large3\nNikdar veˇc ni straˇsilo.\nIznad kupa starega zidovja, poraˇsˇcenega z ostrogovno, srebotovino in drugim grmovjem, je molel\nedino ˇse nekdanji grajski stolp kviˇsku, ki je ponosno kljuboval stoletnim viharjem in nevihtam.\ngolden\nOstal je tudi star stolp.\nmT5-large\nNa stolp je bilo veliko grdnih rastlin.\nmBART-50-large\nV starem delu je bil tudi grajski stolp.\nT5-sl-small5\nNa dvoriˇsˇcu je bil velik stolp.\nT5-sl-large3\nV mestu je bil vˇcasih samo stolp.\nNato pa sem vihral po Nemˇciji okrog ter se udeleˇzeval bitk, ki so bile res lepe.\ngolden\nRad sem se boril.\nmT5-large\nNa poti sem sreˇcal nemˇske vojakov.\nmBART-50-large\nNato sem vihral po Nemˇciji in se udeleˇzeval bitk.\nT5-sl-small5\nPo Nemˇciji sem se udeleˇzil bitk.\nT5-sl-large3\nPo Nemˇciji sem se veliko sprehajal.\nDete je bilo takrat ˇze dve leti staro in mati je bila z njim na njivi nad cesto, kjer je okopavala.\ngolden\nTakrat je bil fantek star 2 leti. Terba je kopala na njivi. Fantek je bil zraven.\nmT5-large\nMati je bila z njim doma. Kopala je in okopavala.\nmBART-50-large\nMati je bila takrat ˇze dve leti staro.\nT5-sl-small5\nDete je bilo zelo staro.\nT5-sl-large3\nNa njivi je bilo veliko dete.\n17\nTable 8: Original Slovene examples of generated summaries on the ASN summarization task for the\nbest performing models.\nKandidati bodo o izboru obveˇsˇceni najpozneje do 3. aprila . Sklad je kot glavno ˇzeljo prejel 60 vlog\nza stanovanja v Novem mestu , 53 za Postojno , 36 zaMaribor , 43 za Kranj in 15 vlog za stanovanja\nna Jesenicah . Povpraˇsevanje je bilo najbolj izrazitoza lokacijah v Postojni in Novem mestu , kjer\nse sklad pojavlja prviˇc in je bil odziv zatopriˇcakovan . Sklad bo prejete vloge podrobno preuˇcil ,\nizbor kupcev pa bo potekal 27. marca v navzoˇcnostinotarja in komisije skladno z doloˇcbami razpisa\n. Stanovanja , ki bodo ostala neprodana v sklopu izbora , bo sklad objavil na svoji spletnistrani\nwww.ssrs.si 3. aprila in jih sproti prodajal po pogojih razpisa od 7. aprila dalje . Kotkupci stanovanj\nv nadaljnji prodaji se lahko prijavijo vsi zainteresirani , ne le kupci iz izrecnonavedenih prednostnih\nkategorij .\ngolden\nStanovanjski sklad RS je v roku za oddajo vlog za nakup 364 trˇznih stanovanj\nsklada v Novem mestu , Postojni , Mariboru , Kranju in na Jesenicah , ki se je\niztekel v ponedeljek , prejel 207 vlog .\nmT5-large\nSklad za stanovanja v Sloveniji je objavil razpis za prodajo stanovanj v nadaljnji\nprodaji.\nmBART-50-large\nSklad za ﬁnanciranje razgradnje Jedrske elektrarne Krˇsko ( JEK ) je objavil\nrazpis za prodajo stanovanj v nadaljnji prodaji.\nT5-sl-small5\nLjubljana - Sklad za stanovanjsko gradnjo ( Sklad ) je danes objavil razpis za\nnakup stanovanj v Ljubljani, ki ga je sklad objavil v zaˇcetku marca.\nT5-sl-large3\nStanovanjski sklad RS je objavil razpis za prodajo stanovanj v nadaljnji prodaji\nstanovanj.\nIvan Zidar iz SCT , prva dama Vegrada Hilda Tovˇsak in prvak Primorja Duˇsan ˇCrnigoj ˇse vedno lahko\npristanejo v priporu . Prva dva je preiskovalna sodnica vˇceraj sicer spustila na prostost in zahtevala\n, da glede odreditve pripora odloˇci senat treh sodnikov Okroˇznega sodiˇsˇca v Ljubljani . ˇCrnigoja so\nizpustili , ker naj bi se skliceval na imuniteto . Toˇzilstvo je danes v Drˇzavni svet ˇze poslalo zahtevo\n, da se osumljencu imuniteto odvzame . Zidar in Tovˇsakova naj bi bila preveˇc radodarna Kazenska\novadba Zidarju , Tovˇsakovi in ˇse trem drugim oˇcita storitev kaznivega dejanja nedovoljenega dajanja\ndaril po I. odstavku 248. ˇclena Kazenskega zakonika , ki pravi : ” Kdor osebi , ki opravlja gospodarsko\ndejavnost , obljubi , ponudi ali da nedovoljeno nagrado , darilo ali kakˇsno drugo korist zanjo ali za\nkoga drugega zato , da bi sebi ali komu drugemu pridobil kakˇsno neupraviˇceno ugodnost pri sklenitvi\nposla ali storitvi iz prvega odstavka 247. ˇclena tega zakonika ( nedovoljeno sprejemanje daril ) , se\nkaznuje z zaporom od ˇsestih mesecev do petih let . V priporu pridrˇzali le Tomaˇza ˇZiberta Kot smo ˇze\nporoˇcali je na danaˇsnji tiskovni konferenci vodja skupine toˇzilcev za pregon organiziranega kriminala\npri Vrhovnem toˇzilstvu Harij Furlan povedal , da so pridrˇzali sedem oseb , katerim oˇcitajo dejanja\ndajanja in prejemanja daril .\nPreiskovalna sodnica je pripor odredila le ˇclanu razpisne komisije\njavnega podjetja Kontrola zraˇcnega promet Tomaˇzu ˇZibertu .\ngolden\nIvan Zidar in Hilda Tovˇsak , ki sta osumljena nedovoljenega dajanja daril , se\nlahko ˇse vedno znajdeta v priporu , ˇce tako odloˇci senat treh sodnikov . ˇCrnigoju\nmorajo prej odvzeti imuniteto\nmT5-large\nPreiskovalna sodnica je vˇceraj spustila Ivana Zidarja, Hildo Tovˇsak in Duˇsana\nˇCrnigoja na prostost\nmBART-50-large\nToˇzilstvo je danes v Drˇzavni svet ˇze poslalo zahtevo, da se osumljencu imuniteto\nodvzame\nT5-sl-small5\nIvan Zidar, Hilda Tovˇsak in Duˇsan ˇCrnigoj ˇse vedno lahko pristanejo v priporu.\nT5-sl-large3\nIvan Zidar, Hilda Tovˇsak in Duˇsan ˇCrnigoj ˇse vedno lahko pristanejo v priporu\nNorveˇska policija preiskuje navedbe Andersa Behringa Breivika , da ima ˇse ” dve celici ” , s katerima\nsodeluje , poroˇca BBC . Navedbe je podal med prvim zasliˇsanjem po bombnem napadu v Oslu , na\nkaterem je tudi priznal napad .\ngolden\nPolicija njegove navedbe o sodelavcih ˇse preiskuje\nmT5-large\nNorveˇska policija preiskuje navedbe Andersa Behringa Breivika, da ima ˇse ” dve\ncelici ”, s katerima sodeluje\nmBART-50-large\nNavedbe je podal med prvim zasliˇsanjem po bombnem napadu v Oslu\nT5-sl-small5\nNorveˇski policisti preiskujejo navedbe Andersa Behringa Breivika, da ima ˇse dve\ncelici, s katerima sodeluje\nT5-sl-large3\nV prvem zasliˇsanju po bombnem napadu v Oslu je Breivik priznal napad\n18\nTable 9: Translated examples of generated summaries on the ASN summarization task for the best\nperforming models.\nCandidates will be notiﬁed of the selection by 3 April at the latest. As the main request, the Fund\nreceived 60 applications for apartments in Novo mesto, 53 for Postojna, 36 for Maribor, 43 for\nKranj and 15 applications for apartments in Jesenice. Demand was most pronounced for locations\nin Postojna and Novo mesto, where the Fund is appearing for the ﬁrst time and the response was\ntherefore expected. The Fund will examine the received applications in detail, and the selection of\nbuyers will take place on 27 March in the presence of a notary and a commission, in accordance with\nthe provisions The apartments that will remain unsold as part of the selection will be published by\nthe Fund on its website www.ssrs.si on 3 April and sold under the conditions of the tender from 7\nApril onwards. All interested parties, not only those from speciﬁcally listed priority categories, can\napply as buyers of apartments for resale.\ngolden\nThe Housing Fund of the Republic of Slovenia received 207 applications in the\ndeadline for submitting applications for the purchase of 364 market apartments in\nNovo mesto, Postojna, Maribor, Kranj and Jesenice, which expired on Monday.\nmT5-large\nThe Housing Fund in Slovenia has launched a tender for the sale of apartments\nfor resale.\nmBART-50-large\nThe Krˇsko Nuclear Power Plant (JEK) Fund for the Financing of Decommission-\ning has published a tender for the sale of apartments for resale.\nT5-sl-small5\nLjubljana - The Housing Fund (Fund) today published a tender for the purchase\nof apartments in Ljubljana, which was published by the Fund in early March.\nT5-sl-large3\nThe Housing Fund of the Republic of Slovenia has launched a tender for the sale\nof apartments for resale of apartments.\nIvan Zidar of SCT, ﬁrst lady of Vegrad Hilda Tovˇsak and the champion of Primorje Duˇsan ˇCrnigoj\ncan still be detained. The ﬁrst two were released yesterday by the investigating judge and requested\nthat a panel of three judges of the District Court in Ljubljana decide on the ordering of detention.\nˇCrnigoj was released on the grounds that he had invoked immunity. The prosecutor’s oﬃce has\nalready sent a request to the National Council today to revoke the suspect’s immunity. Zidar and\nthe Tovˇsak were too generous Criminal charges against Zidar, Tovˇsak and three others accused of\ncommitting the criminal oﬀence of illicit giving of gifts under Article 248 (1) of the Criminal Code,\nwhich says: ”Whoever promises, oﬀers or gives to a person engaged in an economic activity an\nunauthorised prize, a gift or some other beneﬁt for it or for someone else in order to obtain for\nhimself or someone else any unjustiﬁed advantage in the conclusion of a transaction or performance\nreferred to in the ﬁrst paragraph of Article 247 of this Code (unauthorised acceptance of gifts) shall\nbe punishable by imprisonment of six months to ﬁve years.” Only Tomaˇz ˇZibert was detained At\ntoday’s press conference, Harij Furlan, head of the organized crime prosecution team at the Supreme\nProsecutor’s Oﬃce, said that seven persons accused of giving and receiving gifts had been detained.\nThe investigating judge ordered only a member of the tender commission of the public company Air\nTraﬃc Control Tomaˇz ˇZibert to be detained.\ngolden\nIvan Zidar and Hilda Tovˇsak, suspected of illicit gift - giving, may still be placed\nin custody if a panel of three judges so decides. ˇCrnigoj must ﬁrst be stripped\nof his immunity.\nmT5-large\nThe investigating judge yesterday released Ivan Zidar, Hilda Tovˇsak and Duˇsan\nˇCrnigoj\nmBART-50-large\nThe prosecution has already sent a request to the National Council to waive the\nsuspect’s immunity.\nT5-sl-small5\nIvan Zidar, Hilda Tovˇsak and Duˇsan ˇCrnigoj can still be placed in custody.\nT5-sl-large3\nIvan Zidar, Hilda Tovˇsak and Duˇsan ˇCrnigoj can still be placed in custody\nNorwegian police are investigating allegations made by Anders Behring Breivik that he has ”two\nmore cells” he is cooperating with, reports the BBC. He made the allegations during the ﬁrst hearing\nafter the bombing in Oslo, when he also admitted to the attack.\ngolden\nPolice are still investigating his allegations of coworkers\nmT5-large\nNorwegian police are investigating Anders Behring Breivik’s claim that he has\n”two more cells” with which he is cooperating\nmBART-50-large\nHe made the allegations during the ﬁrst hearing after the bombing in Oslo\nT5-sl-small5\nNorwegian policemen are investigating allegations made by Anders Behring\nBreivik that he has two more cells with which he is cooperating\nT5-sl-large3\nIn ﬁrst hearing after Oslo bombing, Breivik admitted to the attack\n19\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-07-28",
  "updated": "2023-01-02"
}