{
  "id": "http://arxiv.org/abs/2208.00203v1",
  "title": "Adding Context to Source Code Representations for Deep Learning",
  "authors": [
    "Fuwei Tian",
    "Christoph Treude"
  ],
  "abstract": "Deep learning models have been successfully applied to a variety of software\nengineering tasks, such as code classification, summarisation, and bug and\nvulnerability detection. In order to apply deep learning to these tasks, source\ncode needs to be represented in a format that is suitable for input into the\ndeep learning model. Most approaches to representing source code, such as\ntokens, abstract syntax trees (ASTs), data flow graphs (DFGs), and control flow\ngraphs (CFGs) only focus on the code itself and do not take into account\nadditional context that could be useful for deep learning models. In this\npaper, we argue that it is beneficial for deep learning models to have access\nto additional contextual information about the code being analysed. We present\npreliminary evidence that encoding context from the call hierarchy along with\ninformation from the code itself can improve the performance of a\nstate-of-the-art deep learning model for two software engineering tasks. We\noutline our research agenda for adding further contextual information to source\ncode representations for deep learning.",
  "text": "Adding Context to Source Code Representations\nfor Deep Learning\nFuwei Tian\nThe University of Melbourne\nAustralia\nfuweit@student.unimelb.edu.au\nChristoph Treude\nThe University of Melbourne\nAustralia\nchristoph.treude@unimelb.edu.au\nAbstract—Deep learning models have been successfully applied\nto a variety of software engineering tasks, such as code classi-\nﬁcation, summarisation, and bug and vulnerability detection. In\norder to apply deep learning to these tasks, source code needs\nto be represented in a format that is suitable for input into the\ndeep learning model. Most approaches to representing source\ncode, such as tokens, abstract syntax trees (ASTs), data ﬂow\ngraphs (DFGs), and control ﬂow graphs (CFGs) only focus on\nthe code itself and do not take into account additional context\nthat could be useful for deep learning models. In this paper,\nwe argue that it is beneﬁcial for deep learning models to have\naccess to additional contextual information about the code being\nanalysed. We present preliminary evidence that encoding context\nfrom the call hierarchy along with information from the code\nitself can improve the performance of a state-of-the-art deep\nlearning model for two software engineering tasks. We outline\nour research agenda for adding further contextual information\nto source code representations for deep learning.\nIndex Terms—Source code representation, deep learning, ad-\nditional context\nI. INTRODUCTION\nProgram comprehension is a complex task that often re-\nquires developers to refer to multiple software artefacts [1]\nwhich might be useful in helping developers construct a mental\nmodel of a program [2]. Experienced developers are better at\ndeciding which cues from different artefacts might be useful to\naid in program comprehension [3]. For example, Kulkarni and\nVarma propose a developer’s perception model for program\ninvestigation which indicates the role of twelve artefacts such\nas control ﬂow, version history changes, and bug reports in\ncomprehension tasks related to concepts, procedures, features,\nand modules [3]. In another example, eye tracking has found\nthat developers looked beyond a particular method to under-\nstand that method [4]. This type of work has well established\nthat developers use additional context when analysing source\ncode—not just the code itself.\nIn this paper, we argue that when asking deep learning\nmodels to comprehend source code, we should give them the\nsame beneﬁt, i.e., access to information beyond the source\ncode that the deep learning model is asked to analyse. The ﬁeld\nof source code representation has seen many recent advances,\nwith most work focusing on improving models by applying the\nlatest innovations from the deep learning research community.\nDifferent data structures have been explored, including tokens,\ntrees, and graphs [5], to encode lexical, syntactic, and/or\nsemantic information [6]. However, the input to all these\napproaches has mostly remained the same—a snippet of source\ncode.\nOur preliminary work presented in this paper provides\nevidence that adding contextual information to the input of\nthe state-of-the-art code representation approach ASTNN [7]\ncan improve the performance of two software engineering\ntasks that are often used to evaluate the quality of code\nrepresentation, i.e., clone detection [8] and code classiﬁca-\ntion [9]. ASTNN takes code fragments as input and uses\na neural network based on abstract syntax trees (ASTs) to\ncapture statement-level lexical and syntactical knowledge as\nwell as the naturalness of statements. We investigate the\nperformance of ASTNN on source code methods from the\nSeSaMe dataset [10] of semantically similar Java methods,\nusing the publicly available implementation of ASTNN as a\nbaseline and comparing it to modiﬁed versions which add\ncontext from a method’s call hierarchy to the input. We explore\ndifferent alternatives for encoding context and combining it\nwith the encoding of a method into a combined representation.\nAdding context from the call hierarchy (i.e., caller and callee\ncontext) can improve the performance in the clone detection\nscenario by 8% (from an F1 score of 0.706 to an F1 score of\n0.765). Interestingly, this performance improvement can only\nbe achieved when we encode the difference between methods\nalong with the information from their callers and callees, see\nSection III-C for details. Concatenation and max-pooling do\nnot have a positive effect on performance in this scenario.\nIn the code classiﬁcation scenario, we observe performance\nimprovements of 11% for concatenation (from an F1 score of\n0.633 to an F1 score of 0.704) and 5% for max-pooling (from\nan F1 score of 0.711 to an F1 score of 0.747). Interestingly,\nperformance gains differ between adding caller and/or callee\ncontext, and adding the callee context only actually decreased\nperformance.\nAdding context of source code fragments can improve\nthe quality of source code representations for deep learning.\nHowever, which context to encode, how to encode it, and how\nto combine its encoding with the encoding of the original\nsource code fragment all have implications on the performance\nof a deep learning model for downstream software engineering\ntasks. Based on these insights, we put forward our research\nagenda for adding context to source code representations for\narXiv:2208.00203v1  [cs.SE]  30 Jul 2022\ndeep learning, with a focus on which context to add, how\nto combine source code fragments and their context, and\ndissecting how context plays a role in the deep learning\nmodels. We summarise related work in Section II and present\nour preliminary study in Section III before we conclude with\nour research agenda in Section IV.\nII. RELATED WORK\nOur work lies at the intersection of related work on source\ncode representation and on the role of context in software\nengineering.\nA. Source Code Representations\nIn recent years, there has been a lot of work done on\nrepresenting source code for machine learning applications.\nThis can roughly be divided into work based on lexical,\nsyntactical, and semantic information [6]. In approaches aimed\nat representing lexical information, a program is transformed\ninto a sequence of tokens. For example, related work has\nshown that n-gram models can successfully handle token\nprediction across different project domains, given a large\ncorpus for training [11], and n-gram models have been used\nto synthesise code completions for API method calls [12].\nApproaches aimed at representing syntactical information\noften rely on the AST. They use heuristic rules [13] or machine\nand deep learning algorithms [7] for encoding information\nfrom the AST, e.g., in the form of a vector. For example,\nASTNN [7] splits each large AST into a sequence of smaller\nsub-trees, and then learns syntactic knowledge from each sub-\ntree separately. Approaches aimed at representing semantic\ninformation additionally incorporate code dependency infor-\nmation, often related to data ﬂow and control ﬂow informa-\ntion [14]. Recent work has shown that combining low-level\nsyntactic information and high-level semantic information can\nimprove source code representation for multiple program com-\nprehension tasks [6]. Hybrid representation approaches which\ncombine mulitple representations are becoming increasingly\ncommon, compared to representations that use tokens, trees,\nor graphs only [5].\nNone of these methods have focused on the input to\nsource code representation. In this work, we argue that adding\nadditional context from outside of the code fragment that is to\nbe represented has the potential of improving the performance\nof any of the approaches.\nB. Context in Software Engineering\nThe need for context in software engineering is well es-\ntablished. For example, IDEs need context to understand the\ntask they are supporting [15], developers need context to\nnavigate technical discussions on Stack Overﬂow [16], and\ntools need context to automatically process source code [17],\n[18]. Context can include static artefacts such as documen-\ntation [19], historical information such as past changes [20],\ndynamic execution information such as traces [21], individual\ndeveloper activity such as IDE interactions [22], and team and\norganisation activity such as communication and coordination\narchives [23].\nIn this paper, we argue that all of these forms of context\nare potentially useful to augment the input to source code\nrepresentation approaches—just as human developers have\naccess to this information, deep learning models might beneﬁt\nfrom this additional context. In our preliminary study (see next\nsection), we rely on the call hierarchy for context, similar\nto the source code summarisation work by McBurney and\nMcMillan [17].\nIII. PRELIMINARY STUDY\nIn this section, we present our preliminary study to establish\nthat adding context can indeed improve the performance of a\nstate-of-the-art deep learning model for software engineering\ntasks. As a ﬁrst step, we use the call hierarchy of a source\ncode method as its context, but we believe that a similar\nmethodology can be applied to other types of context, such\nas the ones described above.\nA. Research Questions\nTo understand the potential of our idea and investigate how\nto best implement it, we ask two research questions:\nRQ1\nWhat is the impact of encoding additional context on\nthe performance of a state-of-the-art deep learning\nmodel?\nRQ2\nWhat is the impact of different approaches to aggre-\ngate the representation of code and its context?\nB. Data Collection\nTo conduct our experiments, we chose the SeSaMe dataset\nwhich contains 857 Java method pairs from eleven open source\nprojects that have been manually classiﬁed according to their\nsemantic similarity [10]. Unlike popular datasets such as Big-\nCloneBench [24], SeSaMe contains the repository link for each\nof its methods which allows us to extract their call hierarchies.1\nFor each method in the SeSaMe dataset, we extract its callers\nand callees. If a method has multiple callers and callees, we\nchoose the largest one to encode as additional context. We use\nthe state-of-the-art code representation approach ASTNN [7]\nas a baseline. Exploring other approaches as baselines, in\nparticular those based on graph representations, is part of our\nfuture work. Since ASTNN relies on ASTs, we parse each\nmethod as well as its caller and callee methods into an AST.\nWe exclude methods that cannot be parsed.\nFollowing the long line of work on source code represen-\ntations (e.g., [7]), we use clone detection and source code\nclassiﬁcation as downstream software engineering tasks for\nevaluating the models:\n• Code Clone Detection: For each method pair, the SeSaMe\ndataset contains up to eight valid ratings from expert\nprogrammers for semantic similarity in terms of goals,\noperations, and effects, along with the corresponding\nconﬁdence. To construct our ground truth, we apply\n1https://github.com/gousiosg/java-callgraph\nTABLE I: Datasets\nDataset\nCount\nClassiﬁcation Train Set\n834\nClassiﬁcation Dev Set\n104\nClassiﬁcation Test Set\n105\nClone Detection Train Set\n448\nClone Detection Dev Set\n56\nClone Detection Test Set\n57\nweights of 0.6, 0.8, and 1 to the conﬁdence of low,\nmedium, and high, respectively, and then average the data\nacross valid ratings and dimensions to create a binary\nlabel.\n• Code Classiﬁcation: We treat the origin of each source\ncode method as its class, resulting in eleven classes since\nthe SeSaMe dataset contains methods from eleven open\nsource projects.\nWe adopt an 80%, 10%, 10% split for training, validation,\nand testing. Table I shows the size of the corresponding\ndatasets.\nC. Aggregation\nASTNN relies on 200-dimensional vectors to represent a\nsource code snippet. Since there are multiple ways in which\ndata can be combined in the context of machine learning (e.g.,\npooling [25]), we experiment with different approaches to\ncombine the representation of a method and the representation\nof its context, according to the two application scenarios\ndetailed above. Our focus in this preliminary study is not on\ninventing new aggregation approaches, but on evaluating well-\nestablished approaches for our problem domain.\nFor code clone detection, we explore three aggregation\nmethods as shown in Figure 1. In the concatenation scenario,\nthe 200-dimensional representation of a method is concate-\nnated with the 200-dimensional representations of its caller\nand callee, resulting in two 600-dimensional representations\nfor each code pair. We calculate the absolute value of the\ndifference of these vectors, and send the result into a linear\nlayer and a sigmoid layer to determine whether the two\nmethods are clones of each other. In the max-pooling scenario,\nwe rely on pooling to select the maximum values of the\nmethod, caller, and callee vectors in each dimension to form\na new vector. In the concatenation of absolute difference\nscenario, we reverse the process by calculating the difference\nbetween the method vectors ﬁrst and then performing the\nconcatenation of all relevant context, i.e., callers and callees\nof both methods.\nFor code classiﬁcation, our unit of analysis are individual\nmethods instead of method pairs. We explore two aggregation\nmethods as shown in Figure 2. In the concatenation scenario,\nwe concatenate the method, caller, and callee vectors and use\na softmax layer [26] to assign each method to one of eleven\nclasses. In the max-pooling scenario, we use pooling to select\nthe maximum values of the method, caller, and callee vectors\nTABLE II: Performance in the clone detection scenario\nMethod\nAccuracy\nPrecision\nRecall\nF1\nWithout Context\n0.825\n0.857\n0.600\n0.706\nConcatenation\n0.807\n0.800\n0.600\n0.686\nMax-Pooling\n0.789\n0.700\n0.700\n0.700\nDifference & Concatenation\n0.860\n0.929\n0.650\n0.765\nTABLE III: Performance in the classiﬁcation scenario\nMethod\nAccuracy Precision Recall Macro-F1\nWithout Context\n0.571\n0.571\n0.402\n0.397\nConcatenation\n0.810\n0.748\n0.686\n0.704\nMax-Pooling\n0.790\n0.836\n0.707\n0.747\nConcatenat. w/ Random Context\n0.771\n0.681\n0.637\n0.633\nMax-Pooling w/ Random Context\n0.800\n0.747\n0.698\n0.711\nMax-Pooling w/ Caller Context\n0.733\n0.651\n0.654\n0.647\nMax-Pooling w/ Callee Context\n0.676\n0.697\n0.530\n0.550\nin each dimension to form a new vector, which is then passed\nto the softmax layer.\nD. Results\nTable II summarises our results for the code clone detec-\ntion scenario. Without any customisation related to context,\nASTNN achieves an F1 score of 0.706. This performance\ndegrades when adding context via concatenation and max-\npooling, with F1 scores of 0.686 and 0.700, respectively.\nHowever, we see an improvement in performance for the\naggregation method of concatenating the absolute difference of\nthe method vectors along with caller and callee context of both,\nfor an F1 score of 0.765 (an 8% improvement). We observe\nsimilar trends for accuracy, and note that the best recall was\nachieved in the max-pooling scenario.\nThese results show that adding additional context to source\ncode representations for deep learning can have a positive im-\npact on the performance of a downstream software engineering\ntask. The way in which additional context is represented and\naggregated can determine whether performance improves or\ndecreases, so it is important to choose an approach that yields\nthe best results.\nTable III summarises our results for the classiﬁcation sce-\nnario. Without adding context, ASTNN achieves a Macro-F1\nscore of 0.397 on our data, across eleven classes. Adding\ncontext substantially improves this performance, with Marco-\nF1 scores of 0.704 and 0.747 for concatenation and max-\npooling, respectively.\nWe note that the addition of context from the call hierarchy\nalone does not necessarily explain the improved performance\nof code classiﬁcation. The baseline model without added\ncontext has to assign methods to one of eleven projects based\non information from a single method, whereas in the other\ntwo scenarios (concatenation and max-pooling), the models\nhave access to information from up to three methods from the\nsame project. It is unsurprising that a classiﬁer achieves better\nperformance on this task if it has up to three data points for\neach decision instead of just one.\n(a) Concatenation\n(b) Max-pooling\n(c) Concatenation of absolute difference\nFig. 1: Aggregation approaches in the clone detection scenario\n(a) Concatenation\n(b) Max-pooling\nFig. 2: Aggregation approaches in the classiﬁcation scenario\nTo determine the extent to which context from the call\nhierarchy speciﬁcally can improve performance, we compared\nperformance against additional baselines which add random\nmethods from same project as additional context. Table III\nshows that some of the performance gain can indeed be\nattributed to information from the call hierarchy: Compared\nto adding random context, the performance improves by 11%\nfor concatenation (from an F1 score of 0.633 to an F1 score\nof 0.704) and 5% for max-pooling (from an F1 score of 0.711\nto an F1 score of 0.747). The table further shows that adding\ncaller or callee context in isolation is not sufﬁcient—in fact,\nadding callee context only led to a degradation of performance.\nWe answer our two research questions as follows:\nSUMMARY\nOur preliminary study shows that encoding additional\ncontext in source code representations for deep learning\ncan improve the performance of a state-of-the-art deep\nlearning model for two downstream tasks (RQ1). In\nterms of aggregating the representation of code and its\ncontext (RQ2), we ﬁnd that the aggregation approach\nhas a substantial impact, and performance can degrade\nif an inadequate aggregation approach is chosen.\nIV. RESEARCH AGENDA\nThe preliminary study that we conducted yields encouraging\nresults, so in this section, we outline our broader research\nagenda for adding context to source code representations for\ndeep learning. We discuss different types of context, different\naggregation methods, and empirical studies in the following\nparagraphs.\nA. Other Context\nWhen developers understand source code, they beneﬁt from\nadditional context, e.g., in the form of documentation [27],\nexecution traces [28], or navigation patterns [29]. We argue\nthat source code representations can beneﬁt from such context\nas well. We distinguish two types of context:\n• Deﬁnite Context: The identiﬁcation of deﬁnite context\nis based on facts and does not rely on probabilistic\nreasoning. Types of deﬁnite context for a code fragment\ninclude its version history, its execution traces, and its\ncall hierarchy.\n• Possible Context: The identiﬁcation of possible context is\naffected by uncertainty, similar to how a developer might\nsearch for information about a code fragment but be\nfaced with uncertainty as to whether a particular piece of\ninformation, such as a Stack Overﬂow thread, is actually\nrelated to this code fragment. Types of possible context\nfor a code fragment include its documentation, issues that\nhave been reported against it, and its rationale. While\nresearch has made great progress towards establishing\nsuch traceability links [30], information inference often\nhappens under uncertainty [31].\nWe argue that source code representations could potentially\nbeneﬁt from all of these types of context, depending on\nencoding and downstream tasks.\nB. Aggregation\nPooling is an aggregation technique used in deep learning to\nreduce the number of parameters and improve performance. It\nworks by combining multiple input features into a single fea-\nture, which is then passed through the network. This approach\ncan be used with convolutional neural networks (CNNs) and\nfully connected nets, and can be applied at different levels in\nthe network depending on what is being optimised. An ideal\npooling method is expected to extract only useful information\nand discard irrelevant details [25]. We argue for the use of two\nclasses of aggregation techniques:\n• General-purpose pooling: In addition to max-pooling and\nconcatenation used in our preliminary study, many other\npooling techniques have been proposed in the context\nof CNNs, e.g., average pooling, stochastic pooling, and\nweighted pooling. We refer readers to Gholamalinezhad\nand Khosravi [25] for an overview.\n• Domain-speciﬁc aggregation: The most suitable way of\naggregating data will often depend on the domain and the\ndownstream task, just as in our preliminary study where\nthe aggregation technique which calculates the difference\nbetween two method representations achieved the best\nperformance for code clone detection. We expect to see\nsimilar advantages when exploring other domain-speciﬁc\naggregation methods, e.g., by taking time series infor-\nmation into account when aggregating version histories\nor by encoding probabilities when context inference was\ndone under uncertainty.\nWe argue for extensive experiments to evaluate the effect of\nthese techniques on aggregating the representation of source\ncode and its context for deep learning, as well as the ex-\nploration of different neural network architectures and their\nsuitability for this task.\nC. Dissection\nInterpreting and dissecting what a deep learning model\nhas learned has become a key ingredient for the validation\nof such models [32]. We posit that developers and tool\nbuilders can beneﬁt from understanding which context a model\nfound beneﬁcial for which software engineering task and why,\nwith particular focus on the interplay of source code and\nits context. Ultimately, we argue for a feedback loop: By\nenabling deep learning methods to beneﬁt from the same\ncontext that developers have access to, we can improve the\nmodels’ understanding of source code, and by dissecting what\nthe models have learned [33], we can improve how developers\ncomplete their tasks, beneﬁting from knowledge about how to\nbest make use of source code and its context.\nREFERENCES\n[1] J. I. Maletic and A. Marcus, “Supporting program comprehension\nusing semantic and structural information,” in Intl. Conf. on Software\nEngineering, 2001, pp. 103–112.\n[2] T. D. LaToza, G. Venolia, and R. DeLine, “Maintaining mental models: a\nstudy of developer work habits,” in Intl. Conf. on Software Engineering,\n2006, pp. 492–501.\n[3] N. Kulkarni and V. Varma, “Supporting comprehension of unfamil-\niar programs by modeling an expert’s perception,” in International\nWorkshop on Realizing Artiﬁcial Intelligence Synergies in Software\nEngineering, 2014, pp. 19–24.\n[4] B. Sharif, T. Shaffer, J. Wise, and J. I. Maletic, “Tracking developers’\neyes in the IDE,” IEEE Software, vol. 33, no. 3, pp. 105–108, 2016.\n[5] H. P. Samoaa, F. Bayram, P. Salza, and P. Leitner, “A systematic mapping\nstudy of source code representation for deep learning in software\nengineering,” IET Software, vol. 16, no. 4, pp. 351–385, 2022.\n[6] Y. Jiang, X. Su, C. Treude, and T. Wang, “Hierarchical semantic-aware\nneural code representation,” Journal of Systems and Software, p. 111355,\n2022.\n[7] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu, “A novel\nneural source code representation based on abstract syntax tree,” in\nIntl. Conf. on Software Engineering, 2019, pp. 783–794.\n[8] C. K. Roy, J. R. Cordy, and R. Koschke, “Comparison and evaluation\nof code clone detection techniques and tools: a qualitative approach,”\nScience of computer programming, vol. 74, no. 7, pp. 470–495, 2009.\n[9] S. Ugurel, R. Krovetz, and C. L. Giles, “What’s the code? Automatic\nclassiﬁcation of source code archives,” in ACM SIGKDD Intl. Conf. on\nKnowledge Discovery and Data Mining, 2002, pp. 632–638.\n[10] M. Kamp, P. Kreutzer, and M. Philippsen, “Sesame: a data set of\nsemantically similar Java methods,” in Intl. Conf. on Mining Software\nRepositories, 2019, pp. 529–533.\n[11] M. Allamanis and C. Sutton, “Mining source code repositories at\nmassive scale using language modeling,” in Working Conf. on Mining\nSoftware Repositories, 2013, pp. 207–216.\n[12] V. Raychev, M. Vechev, and E. Yahav, “Code completion with statistical\nlanguage models,” in ACM SIGPLAN Conf. on Programming Language\nDesign and Implementation, 2014, pp. 419–428.\n[13] L. Jiang, G. Misherghi, Z. Su, and S. Glondu, “Deckard: scalable and\naccurate tree-based detection of code clones,” in Intl. Conf. on Software\nEngineering, 2007, pp. 96–105.\n[14] G. Zhao and J. Huang, “Deepsim: deep learning code functional similar-\nity,” in ACM Joint Meeting on European Software Engineering Conf. and\nSymp. on the Foundations of Software Engineering, 2018, pp. 141–151.\n[15] G. C. Murphy, “The need for context in software engineering,” in\nIntl. Conf. on Automated Software Engineering, 2018, pp. 5–5.\n[16] A. Galappaththi, S. Nadi, and C. Treude, “Does this apply to me? An\nempirical study of technical context in Stack Overﬂow,” in Working\nConf. on Mining Software Repositories, 2022, pp. 23–34.\n[17] P. W. McBurney and C. McMillan, “Automatic source code summariza-\ntion of context for Java methods,” IEEE Trans. on Software Engineering,\nvol. 42, no. 2, pp. 103–119, 2015.\n[18] S. Haque, A. LeClair, L. Wu, and C. McMillan, “Improved auto-\nmatic summarization of subroutines via attention to ﬁle context,” in\nInt’l. Conf. on Mining Software Repositories, 2020, pp. 300–310.\n[19] B. Dagenais and M. P. Robillard, “Recovering traceability links between\nan API and its learning resources,” in Intl. Conf. on Software Engineer-\ning, 2012, pp. 47–57.\n[20] T. Zimmermann, A. Zeller, P. Weissgerber, and S. Diehl, “Mining\nversion histories to guide software changes,” IEEE Trans. on Software\nEngineering, vol. 31, no. 6, pp. 429–445, 2005.\n[21] A. J. Ko and B. A. Myers, “Designing the whyline: a debugging interface\nfor asking questions about program behavior,” in SIGCHI Conf. on\nHuman Factors in Computing Systems, 2004, pp. 151–158.\n[22] M. Kersten and G. C. Murphy, “Using task context to improve program-\nmer productivity,” in ACM SIGSOFT International Symp. on Founda-\ntions of Software Engineering, 2006, pp. 1–11.\n[23] P. Chatterjee, K. Damevski, L. Pollock, V. Augustine, and N. A. Kraft,\n“Exploratory study of Slack Q&A chats as a mining source for software\nengineering tools,” in Intl. Conf. on Mining Software Repositories, 2019,\npp. 490–501.\n[24] J. Svajlenko and C. K. Roy, “Evaluating clone detection tools with\nbigclonebench,” in Intl. Conf. on Software Maintenance and Evolution,\n2015, pp. 131–140.\n[25] H. Gholamalinezhad and H. Khosravi, “Pooling methods in deep neural\nnetworks, a review,” arXiv preprint arXiv:2009.07485, 2020.\n[26] J. S. Bridle, “Probabilistic interpretation of feedforward classiﬁcation\nnetwork outputs, with relationships to statistical pattern recognition,” in\nNeurocomputing.\nSpringer, 1990, pp. 227–236.\n[27] S. Blinman and A. Cockburn, “Program comprehension: investigating\nthe effects of naming style and documentation,” in ACM Intl. Conf. Pro-\nceeding Series, vol. 104, 2005, pp. 73–78.\n[28] A. Zaidman, T. Calders, S. Demeyer, and J. Paredaens, “Applying\nwebmining techniques to execution traces to support the program\ncomprehension process,” in European Conf. on Software Maintenance\nand Reengineering, 2005, pp. 134–142.\n[29] R. DeLine, M. Czerwinski, and G. Robertson, “Easing program com-\nprehension by sharing navigation data,” in Symp. on Visual Languages\nand Human-Centric Computing, 2005, pp. 241–248.\n[30] J. Cleland-Huang, O. C. Gotel, J. Huffman Hayes, P. M¨ader, and\nA. Zisman, “Software traceability: trends and future directions,” in\nFuture of Software Engineering Proceedings, 2014, pp. 55–69.\n[31] M. P. Robillard, A. Marcus, C. Treude, G. Bavota, O. Chaparro,\nN. Ernst, M. A. Gerosa, M. Godfrey, M. Lanza, M. Linares-V´asquez\net al., “On-demand developer documentation,” in Intl. Conf. on Software\nMaintenance and Evolution, 2017, pp. 479–483.\n[32] G. Montavon, W. Samek, and K.-R. M¨uller, “Methods for interpreting\nand understanding deep neural networks,” Digital signal processing,\nvol. 73, pp. 1–15, 2018.\n[33] W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K.-R. M¨uller,\n“Evaluating the visualization of what a deep neural network has learned,”\nIEEE Trans. on Neural Networks and Learning Systems, vol. 28, no. 11,\npp. 2660–2673, 2016.\n",
  "categories": [
    "cs.SE",
    "cs.LG"
  ],
  "published": "2022-07-30",
  "updated": "2022-07-30"
}