{
  "id": "http://arxiv.org/abs/2208.00953v2",
  "title": "Visual Interpretable and Explainable Deep Learning Models for Brain Tumor MRI and COVID-19 Chest X-ray Images",
  "authors": [
    "Yusuf Brima",
    "Marcellin Atemkeng"
  ],
  "abstract": "Deep learning shows promise for medical image analysis but lacks\ninterpretability, hindering adoption in healthcare. Attribution techniques that\nexplain model reasoning may increase trust in deep learning among clinical\nstakeholders. This paper aimed to evaluate attribution methods for illuminating\nhow deep neural networks analyze medical images. Using adaptive path-based\ngradient integration, we attributed predictions from brain tumor MRI and\nCOVID-19 chest X-ray datasets made by recent deep convolutional neural network\nmodels. The technique highlighted possible biomarkers, exposed model biases,\nand offered insights into the links between input and prediction. Our analysis\ndemonstrates the method's ability to elucidate model reasoning on these\ndatasets. The resulting attributions show promise for improving deep learning\ntransparency for domain experts by revealing the rationale behind predictions.\nThis study advances model interpretability to increase trust in deep learning\namong healthcare stakeholders.",
  "text": "Visual Interpretable and Explainable Deep\nLearning Models for Brain Tumor MRI and\nCOVID-19 Chest X-ray Images\nYusuf Brima1* and Marcellin Atemkeng2*\n1*Computer Vision, Institute of Cognitive Science, Osnabrück\nUniversity, , Osnabrueck, D-49076, Lower Saxony, Germany.\n2Rhodes AI Research Group, Department of Mathematics, Rhodes\nUniversity, , Grahamstown, 6140, Eastern Cape, South Africa.\n*Corresponding author(s). E-mail(s): ybrima@uos.de;\nm.atemkeng@ru.ac.za;\nAbstract\nDeep learning shows promise for medical image analysis but lacks interpretabil-\nity, hindering adoption in healthcare. Attribution techniques that explain model\nreasoning may increase trust in deep learning among clinical stakeholders. This\npaper aimed to evaluate attribution methods for illuminating how deep neural\nnetworks analyze medical images. Using adaptive path-based gradient integra-\ntion, we attributed predictions from brain tumor MRI and COVID-19 chest\nX-ray datasets made by recent deep convolutional neural network models. The\ntechnique highlighted possible biomarkers, exposed model biases, and offered\ninsights into the links between input and prediction. Our analysis demonstrates\nthe method’s ability to elucidate model reasoning on these datasets. The resulting\nattributions show promise for improving deep learning transparency for domain\nexperts by revealing the rationale behind predictions. This study advances model\ninterpretability to increase trust in deep learning among healthcare stakeholders.\nKeywords: Attribution, Bioimaging, Brain tumor MRI, COVID-19, Deep Neural\nNetworks, Deep Learning, Explainability, Guided Integrated Gradients, Healthcare,\nIntegrated Gradients, Interpretability, Medical Images, Mammography, Radiology,\nRegion-based Saliency, Saliency Analysis, X-ray\n1\narXiv:2208.00953v2  [cs.LG]  7 Aug 2023\n1 Introduction\nRecent advances in compute and deep neural architectures [1–5] have enabled rapid\nprogress in automated medical image analysis. Medical imaging techniques like Com-\nputed Tomography (CT), Magnetic Resonance Imaging (MRI), Functional Magnetic\nResonance Imaging (fMRI), Positron Emission Tomography (PET), Mammography,\nUltrasound, and X-ray are traditionally interpreted by radiologists and physicians for\ntimely disease detection and diagnosis [6]. However, the healthcare field’s high demand\nfor skilled labor can lead to fatigue, necessitating computer-aided diagnostic tools. The\nmaturation of deep learning is thus accelerating the adoption of computer-assisted\ntools to aid experts and reduce manual analysis.\nDeep learning shows particular promise for democratizing healthcare globally\nby reducing prohibitive costs of expertise [7]. However, successful clinical adoption\ndepends on assured trust in model robustness and interpretability, which is crucial in\nsafety-critical healthcare [8]. Despite the inherent complexity of deep learning mod-\nels, we present techniques to illuminate their inference mechanisms. By this, we refer\nto how a deep model takes an input (e.g., a medical image) and produces an output\nprediction (e.g. a disease classification).\nUsing adaptive path-based integrated gradients, we systematically studied model\npredictions on brain tumor MRI [9] and COVID-19 chest X-rays [10] medical images.\nAttribution maps highlighted salient input features corresponding to model predic-\ntions. These techniques can build understanding, trust, and verification by experts to\nenable the adoption of computer-aided diagnostics.\nIn this work, we aim to evaluate attribution methods on convolutional neural\nnetworks (CNNs) analyzing medical images (Section 3). Experiments assess tech-\nnique effectiveness across models and modalities (Section 4). Our results demonstrate\nthe ability of these attribution methods to provide insights into input-prediction\nrelationships, reveal potential biomarkers, and uncover model biases.\nThis work makes key contributions through a comprehensive evaluation of adap-\ntive gradient-based attribution methods across diverse CNNs and medical imaging\ndatasets. Visualizations demonstrate clear technique differences and reveal relation-\nships to model structure.\nThe paper is organized as follows. Related interpretability approaches are discussed\nin Section 2. Section 3 describes the methodology. Section 4 presents experimental\nresults on three datasets. Section 5 concludes and proposes future directions. Together,\nthis work advances model transparency to increase trust in deep learning for medical\nimage analysis.\n2 Related Literature\nVaried interpretability methods have been recently proposed for medical image anal-\nysis tasks. Research in this direction is growing primarily to help build trustworthy\nartificial intelligence (AI) systems that use a human-in-the-loop approach to com-\nplement domain experts. Concept Learning techniques have been used in [11–13] to\nmanipulate high-level concepts to train models that can perform multi-stage predic-\ntions from high-level clinical concepts which provide input to the final classification\n2\ntask of disease categories. However, these methods have significant annotation costs,\nand concept-to-task mismatches can lead to considerable information leakage [14].\nAnother class of technique is Case-Based Models (CBMs), where class discrim-\ninative disentangled representations and feature mappings are learned. The final\nclassification is performed by measuring the similarity between the input image and the\nbase templates [15–17]. But this class of techniques is not susceptible to corruption by\nnoise and compression artifacts. It is also difficult to train models using this paradigm.\nCounter Factual Explanation is another approach where input medical images are\nperturbed in pseudo-realistic ways to generate an opposite prediction. They have the\nproblem of generating unrealistic perturbations with respect to the input images which\ncan often be low resolutions as opposed to the original images [18–25]. Visualization of\nthe internal network representation of learned features of kernels in CNNs is another\ntechnique that is used in model understanding. But this approach has a limitation of\ndifficulty in interpreting feature maps in medical image analysis settings [26, 27].\nAn attribution map provides post-hoc explanations whereby regions of the input\nimage are highlighted as indicated saliency method based on the model prediction.\nIn their paper, [28] proposed layer-wise relevance propagation for explaining deep\nneural network decisions in MRI-based Alzheimer’s disease classification. A deep CNN-\nbased model with Gradient Class Activation Map (Grad-CAM) was trained to classify\noral lesions for clinical oral photographic images [29]. In [30], a similar CNN-based\nGrad-CAM technique for the classification of Oral Dysplasia is proposed. However,\nour approach is different from [28–30] as we utilize adaptive path-based integrated\ngradients techniques to address the problem of noisy saliency masks which hinders\nformer methods [31].\n3 Methods\nWe present the CNN models utilized to carry out experiments in this study for\nthe classification tasks. Characterizations of these CNN architectures are expounded,\nindicating their inductive priors, strengths, and limitations in learning visual represen-\ntations. We give a detailed description of the adaptive path-based integrated gradient\ntechniques and their direct applications to deep learning-based models in medical\nimage analysis. To achieve this, we have summarized the mathematical notation in\nTable 1 used in this work.\n3.1 Background\nWe use 9 standard CNN architectures: Visual Geometric Group (VGG16 and\nVGG19 [5]), Deep Residual Network (ResNet50, ResNet50V2) [2], Densely Connected\nConvolutional Networks (DenseNet) [32], Deep Learning with Depthwise Separable\nConvolutions (Xception) [3], Going deeper with convolutions (Inception) [33], a hybrid\ndeep Inception and ResNet and EfficientNet: Rethinking model scaling for convolu-\ntional neural networks [34] for classifying COVID-19 X-ray images and brain tumors\nfrom the T1-weighted MRI slices. The choice of these deep models is explained by the\nfact that they are modern techniques that are widely used in solving vision tasks and\nby extension medical image feature extraction for prediction and/or classification.\n3\nTable 1: A summary of the mathematical notations in this paper.\nNotation\nDescription\nR\nSet of real numbers\nRd\nSet of d-dimensional real-valued vector\nRn×d\nSet of n × d real-valued matrix\nx ∈Rn×d×1\nSet of n × d × 1 real-valued tensor which is a single channel image input to a neural network\ny ∈R|C|\nA corresponding one-hot encoded label for an image input x\n|C|\nCardinality of the set of medical image classes.\nWi\nThe kernels for the i-th layer of a CNN\nL(·)\nA loss function\nfl(xm, θ)\nNon-linear transformation of input xm at layer l parameterized by θ\nσl\nActivation function at layer l\nα ∈R+\nNon-negative real-valued regularization hyperparameter\n|| · ||2\n2\nThe squared ℓ2 norm\nDi and D′\ni\nA training and testing samples of task Ti respectively. Ti is sampled from the distribution of task\nh(·)\nA neural network that produces latent representation for each input\nAh\nAn attribution operator that takes a trained model h to produce a saliency map\nˆx\nComputed saliency map for a given input image x\nVGG was first introduced in the ImageNet Large Scale Visual Recognition Chal-\nlenge (ILSVRC) 2014 challenge [35] mainly to evaluate the effect of increasing depth\nin a deep neural network architecture with very small (3×3) convolution kernels. The\nresults showed that increasing depth from 16 to 19 weight layers is a significant factor\nin improving the prior-art configurations. Increment in neural architectural depth leads\nto more expressive models that learn better representations, thus, improving general-\nizations across training tasks. However, deeper networks are hard to train because of\nthe vanishing gradient problem [36–38]. In that regard, deep residual learning: ResNet\nwas introduced in [2] to facilitate training routines for massively deeper neural net-\nworks. Results in [2] empirically showed that ResNet converges faster using local search\nmethods such as stochastic gradient descent (SGD) and can achieve higher accuracy\nfrom the considerably increased depth of several layers. The primary way the vanish-\ning gradient problem is tackled in this framework is by introducing identity mappings\nthat create shortcut connections to maximally exploit information flow in the network\narchitecture thus solving the vanishing gradient problem. As depth is addressed by\nthe residual network framework, another key concern is how wide can we go and in\nwhat variety of kernel sizes.\nThus, a natural solution would be to learn, within computational limits as many\nfactors of variations as possible. This is the main idea introduced in the depth-wise\nseparable layers based on the Inception architecture [33]. Inspired by the promising\nperformance of both Inception and ResNet, a hybrid model that combines any of the\nsub-versions (i.e., v1, v2, v3, or v4) of ResNet and Inception has shown satisfactory\nresults when compared to ResNet-only or Inception-only [39, 40]. The drawback of the\nhybrid InceptionResNet is the computational requirements at the training stage.\nIn contrast to a standard Inception model that performs cross-channel correla-\ntions followed by spatial correlations, in the Xception model, spatial convolutions are\n4\nperformed independently [3]. This consists of a spatial convolution performed inde-\npendently for each channel of the input followed by a point-wise convolution across\nchannels for dimensionality reduction of the computed features. In their work [32],\nintroduced the idea of dense connectivity: DenseNet where each layer is connected\nto every other layer in a feed-forward fashion in neural networks. Their approach is\nan extension of the successes made by ResNets. A DenseNet comprises dense blocks\nwhich implement dense connectivity to reduce the computational cost of channel-wise\nfeature concatenation. This architectural design is robust to gradient flow as it pro-\nvides robust signals for gradient propagation in the layers of a substantially deeper\nnetwork which results in gainful generalization performance. With a small growth rate,\nthis architectural design is computationally efficient. The EfficientNet [34] introduced\na principled study of model scaling considering the impact of depth, width, and res-\nolution on model performance. A new compound scaling method was proposed that\nuniformly scales all three dimensions of an input image: depth, width, and resolution\nusing a compound coefficient that is derived from a grid search method.\nThe above architectures as described are known in the context of supervised deep\nlearning for which the optimization uses gradient-based local search methods. The goal\nof the optimization is to find an optimal fitted function that minimizes the empirical\nrisk; measured from the training samples with a defined loss function L:\nˆθ = arg min\nθ\n1\nN\nN\nX\nm=1\nL(ym, f(xm; θ)),\n(1)\nwhere θ compacts the parameters of the trainable neural network f(xm; θ), N the\nnumber of training examples, xm and associated ym are the features vector and\nlabel for sample m respectively. To prone generalization, a regularization term is\nimperatively added\nˆθ = arg min\nθ\n1\nN\nN\nX\nm=1\nL(ym, f(xm; θ)) + α||θ||2\n2\n(2)\nin the L2 norm regime with α the learning rate. In order words for f(xm; θ) =\nσ(θ1xm + θ2) with θ = (θ1, θ2), at layer l we want to interpolate f(xm; θ) such that\nf l(xm; θ) = σl(θl\n1Dlf l−1(xm) + θl\n2)\n(3)\npredicts the label ym for l = 2, 3, 4, · · · . In this notation, f l is the output interpolation\nof layer l, σl is the activation function at layer l, θl = (θl\n1, θl\n2) is the learnable param-\neters at layer l with θl\n1 and θl\n2 the weight matrix and bias vector respectively. In the\nexpression in Equation 3 the weights matrix Dl is introduced as a sort of regulariza-\ntion that activates the connections which contribute to the interpolation of f l(xm; θ)\nat layer l; this is known as the dropout regularization.\nAdopting a gradient flow training method with variable learning rate αl at layer\nl, in the meta-learning regime as we adopted in this work, the update of θ follows\ntwo procedures. If p(T ) is assumed to be the distribution of tasks where each task\n5\nis sampled as Ti ∼p(T ) with the aim to learn prior knowledge from all these Ti.\nAs discussed in [41] the main goal is to encapsulate the prior knowledge of all Ti as\nthe initial weight θ of the fitted function f(x, θ) which can now be used as an initial\nweight for quick adaptation to a new task. The first attempts is to find the parameter\nθi,k of a task Ti with training sample Di = {(xm, ym)i}; m = 1, · · · , Ni where Ni is\nthe number of sample in Di. At the (k + 1)th iteration, θi,k is updated as:\nθi,k+1 = θi,k −αl∇θ\nX\nDi\n1\nNi\nLTi(ym, f(xm; θi,k)), θi,0 = θ\n(4)\nwhich is now followed by a proper update of θ using the direction of the gradient and\nthe test samples D′\ni = {(xm, ym)i} of the task Ti; m = 1, · · · , N ′\ni where N ′\ni is the\nnumber of sample in D′\ni. Assume that θ′\ni is obtained after several update as discussed\nin Equation 4 for each task Ti, the proper update of θ follows:\nθ ←θ −βl∇θ\nX\nTi\nX\nD′\ni\n1\nNT N ′\ni\nLTi(ym, f(xm; θ′\ni)),\n(5)\nwhere NT and βl are the number of tasks and the learning rate at layer l respectively.\n3.2 Proposed Visual Explainable Framework\nTo help interpret a model inference mechanism, which is crucial in building trust for\nclinical adoption of deep learning-based computer-aided diagnostic systems, we have\nproposed an interpretability framework depicted in Figure 1 that gives an overview\nof an attribution mechanism. [42] posited fundamental axioms: Sensitivity and Imple-\nmentation Invariance that attribution methods must satisfy. All selected saliency\nmethods in this study adhere to this axiom. For a macro-scale attribution, a model\nh(xi; ϕ) that has learned statistical regularities of any given bioimaging dataset Dm\nthat has an arbitrary number of classes to produce a representation zi for each medi-\ncal image slice xi that is a compact latent representation in a vector space. With this\nrepresentation, any arbitrary dimensionality reduction method can map the latent\nrepresentation onto a lower-dimensional space for analysis and visualization. This\ncould be a Gaussian Mixture Model (GMM) [43], t-Distributed Stochastic Neighbor\nEmbedding (t-SNE) [44] or Principal Component Analysis (PCA) [45] technique to\nunderstand the latent space projection.\nTo attain local information about an attribution scheme because of the limitations\nof global attribution as it does not give contextual information of feature importance\nin the input space. We, therefore, propose the use of gradient information since neural\nmodels are differentiable or at least partially differentiable functions. We propose a\nframework of an adaptive path-based gradient integration method that utilizes the\nGuided Integrated Gradient (GIG) [31] as shown in Equation 8 and a Region-based\nsaliency method: eXplanation with Ranked Area Integrals (XRAI) [46]. The core idea\nof Integrated Gradient (IG) is that given a non-linear differentiable function h defined\n6\nas:\nh :Rn −→[0, 1]\n(6)\nx 7−→h(x),\n(7)\nwhich represents a deep neural network and an input x = (x1, . . . , xn) ∈Rn. A general\nattribution of the prediction at the input x relative to some baseline input x′ is a vector\nAh (x, x′) = (a1, . . . , an) ∈Rn where ai is the contribution of the vector component\nxi to the function h(x). In a medical image analysis context, the function h represents\na deep neural network that learns a disentangled non-linear transformation of given\nmedical image slices. The input vector x is a simple tensor of the k channel image,\nwhere the indices correspond to pixels. The attribution vector a = (a1, . . . , an) serves\nas a mask over the original input to show the regions of interest of the model for the\ngiven predicted score. This information gives us insight into regions of interest for any\ngiven 2D image slice:\nIGi(x) = (xi −x′\ni)\nZ α=1\nα=0\n∂\n∂xi\nh\n\u0000x′ + α (x −x′)\n\u0001\ndα,\n(8)\nwhere (xi −x′\ni) is the difference between the input image and the corresponding\nbaseline input at each pixel.\nComputing and visualizing the saliency maps involve the following steps:\n1. We initialize a baseline with all zeros. This baseline input remains prediction-neutral\nand has a crucial role in the interpretation and visualization of the input pixel\nfeature importance.\n2. Linear interpolations are generated between the baseline and the original image\nthat are incremental steps (α) in the feature space between the baseline x′ and the\ninput image x.\n3. The gradient in Equation 8 is computed to measure the relation between the fea-\ntures xi and changes in the model class predictions. It gives a criterion for pixels\nwith the most relevance to the model class probability scores. This gives a basis\nfor quantifying feature importance in the input image with respect to the model\nprediction.\n4. Using a summation method, an aggregate of the gradients is computed.\n5. The aggregated saliency mask is scaled to the input image to ensure that feature\nattribution values are accumulated across multiple interpolated images that are all\non the same scale that represents the saliency map on the input image with the\npixel feature saliency.\n4 Experimental Results\nIn this section, we present an overview of the datasets used in this paper including the\nannotation procedure for the segmentation of regions of interest in each MRI image. We\nfurther explain the training regime for all the models and elaborate on the framework\nfor computing interpretable features using adaptive path-based gradient integration\n7\nFig. 1: A dataset of m samples of T1-weighted contrast-enhanced images slices is\nthe input to a standard CNN classification model depicted in the figure as h(·) that\nlearns the non-linear mapping of the features to the output labels. h(·) is utilized with\nan attribution operator Ah to attribute salient features ˆx of the input image. Ah is\nan operator that can be used with varied differentiable architectures. This proposed\nframework is general and can be applied to any problem instances where explainability\nis vital in building trust in the model inference mechanism.\ntechniques for scoring pixel-wise feature relevance as discussed in Section 3.2. Results\nshow that deep neural network models trained on medical images can give prediction\nconfidence through softmax scores as well as use visual interpretability techniques to\ninfer feature attribution maps.\n4.1 Datasets\nWe use two types of medical image data modalities to test the attribution frame-\nwork. The choice of the two modalities depends on the availability of data. Other\ntypes of modalities are also applicable to the attribution framework. We leave this for\nfuture work. The brain tumors MRI dataset [9] is used. It comprises 2D slices of brain\ncontrast-enhanced MRI (CE-MRI) T1-weighted images consisting of 3064 slices from\n233 patients. It includes 708 Meningiomas, 1426 Gliomas, and 930 Pituitary tumors.\nRepresentative MRI image slices with large lesion sizes are selected to construct the\ndataset. In each slice, the tumor boundary is manually delineated and verified by\nradiologists. We have plotted 16 random samples from the three classes with tumor\n8\nFig. 2: Shows randomly sampled images from the brain tumor dataset. The red anno-\ntated regions indicate perimeters of segmented tumor borders. From the figure, Glioma\nsamples have the widest tumor areas as opposed to the other two tumor classes. Glioma\ntumor tissue can be formed in varied locations in the brain. Like Glioma, a Menin-\ngioma is a primary central nervous system (CNS) tumor and can begin in the brain or\nspinal cord areas. Meningioma is the most common type of tumor among patients. As\nshown in the figure, samples often occur in pairs across opposite regions of the brain.\nAs depicted in the figure, Pituitary tumors are abnormal growths that develop in the\npituitary gland that lead to excess hormonal releases that regulate important body\nfunctions.\nborders depicted in red as shown in Figure 2. These 2D slices of T1-weighted images\ntrain standard deep CNNs for a 3-class classification task into Glioma, Meningioma,\nand Pituitary tumors. The input to each model is a R225×225×1 tensor that is a resized\nversion of the original R512×512 image slices primarily due to computational concerns.\nUnlike the brain cancer MRI dataset which comes with segmentation masks from\nexperts in the field, the COVID-19 X-ray dataset [47] used in this work has no ground\n9\nTable 2: The 2 datasets comprising different modalities used to carry out experiments\nin this study.\nSource\nClasses\nNumber\nof samples\nTotal\nModality\nSegmented\nBrain Tumor Dataset [9]\nMeningioma\n708\n3064\nMRI\nyes\nGlioma\n1,426\nPituitary tumor\n930\nCOVID-19 database [10]\nCOVID-19\n3,616\n19,820\nX-ray\nno\nNormal\n10,192\nLung Opacity\n6,012\ntruth segmentation masks. This was chosen as an edge-case analysis due to the fact\nthat a vast majority of datasets do not have segmentation masks. This dataset was\ncurated from multiple international COVID-19 X-ray testing facilities during several\ntime periods. The dataset is made up of an unbalanced percentage of the four classes\nin which we have 48.2 % normal X-ray images, 28.4 % cases with lung opacity, 17.1 %\nof COVID-19 patients and 6.4% of patients with viral pneumonia of the 19820 total\nimages in the dataset. This unbalanced nature of the dataset comes with its own clas-\nsification challenges and has prompted several researchers to implement methods to\nclassify the dataset using deep learning methods. Out of the four classes, for consis-\ntency with the other datasets used in this work, we choose to classify three classes\n(i.e., Normal, Lung Opacity, and COVID-19). For an in-depth discussion of works that\ndeal with this dataset, we refer to [48]. Figure 3 shows 16 selected random samples.\nTable 2 summarizes those three datasets.\n4.2 Implementation Performance\nAs the primary objective of this study is to build a framework for understanding the\nvisual interpretability of deep learning models in medical image analysis, we limit our\nexperiments to 9 modern vision-based deep neural architectures. We trained and tested\nthe 9 modern CNN architectures; results are shown in Figures 4, and 5 and summarized\nin Table 3 with training hyperparameters depicted in Table 4 for the two datasets\nused to test the proposed attribution method. The object of this work is not to find\nmodels that outperform the current literature with the different datasets, but rather\nto answer the question: what do the deep learning models learn in medical images\nvia the proposed attribution method? We conducted all experiments on an NVIDIA\nK80/T4 GPU. In Section 4.3 several saliency methods are applied to understand model\nprediction interpretability.\nWith the brain MRI dataset, the DenseNet121 model shows the best overall test\nperformance reaching 98.10%. While the hybrid InceptionResNetV2 outperformed the\nother models on the COVID-19 X-ray dataset with an accuracy of 89.0%. The test\nresults indicate the high confidence and stability of model prediction. This is the basis\nof selection for further feature attribution given that it is the best-performing model\nimplying it has learned a more robust and generalizable representation of the data\ndistribution as shown in Figures 6, and 7. The clear distinction between Figures 6,\n10\nFig. 3: Random selected 16 samples. The dataset was curated from multiple inter-\nnational COVID-19 X-ray testing centers during several time periods. The dataset is\nmade up of an unbalanced percentage of the four classes in which we have 48.2 %\nnormal X-ray images, 28.4 % cases with lung opacity, 17.1 % of COVID-19 patients\nand 6.4% of patients with viral pneumonia of the 19820 total images in the dataset.\nThe highly unbalanced percentages explain the occurrence of normal and lung opacity\ncases in the random selection versus COVID-19 and/or viral pneumonia.\nand 7 left and right panels give an evident indication that the model has learned\ninherent factors of variation in the signals which have been disentangled into nearly\nseparable manifolds in the learned representation space). These figures support the\nresults of the confusion matrices in Figures 4, and 5. However, this ability of learning\nnecessitates the notion of what has the model learned about the data space and how\ncan it be interpreted by domain experts. Thus, the notion of feature attribution is\ninvestigated to make sense of mapping between the model input and the predicted\nclass.\n11\nFig. 4: Performance measure of the 8 CNN architectures used in this experiment all\ntrained for 20 epochs on the brain MRI dataset. Overall, DenseNet121 [3] showed the\nhighest F1 Score reaching 0.981. The confusion matrix for test samples represents 10%\nof the dataset. The model could generalize well with 5, 4, and 3 misclassifications for\nMeningioma, Glioma, and Pituitary tumor respectively. Because of the distinctness of\nboth Meningioma and Pituitary tumor, the model has 0 false positives between both\nclasses.\nFig. 5: InceptionResNetV2 reached the best test-time performance for the chest X-\nray dataset. All models nearly uniformly performed well on this dataset primarily\nbecause of the huge number of data points that are well-suited for high-capacity models\nto prevent overfitting. From the corresponding confusion matrix, on the left, Lung\nOpacity has the largest number of misclassification relative to the distribution of the\ndataset.\n12\nTable 3: A comparison of the 9 models on the test set including their architectural\nproperties. DenseNet121 has the best overall performance on the unseen test set\nreaching a top-1 accuracy of 98.10% on the brain tumor MRI dataset. Relative\nto the least performing model, VGG19, it is not only parameter efficient but\nhas a small memory footprint of at least 16 times less than VGG19. From this\ntable, we chose the top three best-performing models per dataset for saliency\nanalysis considering the impact of parameter count and depth on the type of\nrepresentations learnable from these models. InceptionResNetV2 outperformed\nother models for the COVID-19 chest X-ray dataset.\nModel\nSize (MB)\nParameters\nDepth\nTop-1 Accuracy\nBrain Tumor\nDataset\nCOVID-19\ndatabase\nVGG16\n528\n138.4M\n16\n0.928797\n0.891616\nVGG19\n549\n143.7M\n19\n0.887658\n0.889571\nResNet50\n98\n25.6M\n107\n0.936709\n0.857873\nResNet50V2\n98\n25.6M\n103\n0.962025\n0.881391\nInceptionV3\n92\n23.9M\n189\n0.944620\n0.880368\nXception\n88\n22.9M\n81\n0.966772\n0.889571\nEfficientNetB0\n29\n5.3M\n132\n0.933544\n0.880368\nDenseNet121\n33\n8.1M\n242\n0.981013\n0.884458\nInceptionResNetV2\n215\n55.9M\n449\n-\n0.895706\nTable 4: Training hyperparameters.\nHyperparameter\nSetting\nLearning rate\n1e-3\nBatch size\n32\nNumber of epochs\n20\nTraining set\n0.7\nTest set\n0.3\nInput shape\nR225×225×1\nMomentum\n9.39e-1\nDecay\n3e-4\nOptimizer\nStochastic Gradient Descent with Momentum (SDGM)\n4.3 Attribution\nOur proposed framework for understanding attribution is predicated on the notion that\nvisual inspection has a major role in medical image analysis decision-making. Natu-\nrally, an automated visual attribution method is vital in a human-centered AI medical\nimage analysis pipeline. Given that many attribution methods have been proposed,\nwe have, however, used gradient-based adaptive path integration methods because of\ntheir robustness to noise and smoother pixel-level feature saliency mappings. For each\nof the datasets, the proposed visual attribution framework is implemented with the\nVanilla Gradient [42], Guided Integrated Gradient (GIG) [31] and XRAI [46] using the\nthree best performing deep learning models for each dataset as shown in Figures 4,\nand 5; i.e. DenseNet121, Xception, and ResNet50V2 are the best three models for\n13\nFig. 6: A t-SNE [44] two-dimensional projection of the unrolled pixel space represen-\ntation of MRI slices where the colors purple, green, and yellow represent the three\nclasses of Meningioma, Glioma, and Pituitary tumor respectively. However, given that\nthe data is generated under differing physical and statistical conditions, the classes\nare entangled. This can impede learning using linear function approximations. (Right)\nA t-SNE projection of the embedding representation from a trained DenseNet121\nnetwork. The model has disentangled the underlying factors of variation in a latent\nrepresentation space that allows separability using either linear or non-linear function\napproximators as shown by the nearly distinct manifolds of the three classes.\nFig. 7: A similar 2D t-SNE visualization of the InceptionResNetV2 latent represen-\ntations for the chest X-ray dataset. This dataset has a rich statistical structure across\nall classes, however, it is also imbalanced like many medical datasets. (Right) A plot of\nlatent embeddings prior to training, the dataset is biased towards normal class which\nwas addressed through class weighting during training. (Right) Embeddings of the net-\nwork after training. There is a visible decrease in the intra-class cluster size as samples\nbelonging to the same class are pulled closer during the training phase in the repre-\nsentation space. This notion is supported by the confusion matrix plot in Figure 5.\nthe brain tumor MRI dataset and Inception-ResNetV2, Xception, and VGG16 for the\nCOVID-19 X-ray dataset. Results are depicted in Figures 8a, 8b, 8c for the three brain\nMRI tumor classes and in Figures 9a, 9b, and 9c for three COVID-19 X-ray classes.\nFigures 8, and 9 shows three randomly sampled test images from each brain tumor\nMRI, and chest X-ray that are chosen for saliency analysis using the three trained best\ndeep learning models for each of the datasets. Each of the image modalities undergoes\nsaliency analysis using each of the attribution methods as shown in the first row\ntitles from Vanilla Gradient-based to Smooth Blur Guided Integrated Gradients. The\n14\n(a) Xception\n(b) ResNet50V2\n(c) DenseNet121\nFig. 8: Brain tumor MRI: In the first column on the left is the input image where the\nred borders depict the delineated boundaries of tumors. Three randomly sampled test\nimages from each tumor class are chosen for saliency analysis using the top 3 trained\nmodels.\n15\nimages are plotted on a grayscale, and the bright spots for the brain tumor show the\nregions in the input selected for classification into the predicted class by the model.\nOverall, XRAI has the best explainability of the input signals. This is further explored\nby pruning 30% of less explainable features of the attributed image as presented in\nthe Fast XRAI 30%. There is an emergence of salient features that correspond to the\ninput region of interest for each tumor class. In contrast to other deep learning models,\nthe saliency maps of the Xception model have the least saliency map stability with\nincreased noise levels across all three brain MRI classes. More importantly, XRAI has\nwider regions of interest computed that correspond to the input signal segmentation\nmask. DenseNet121 and InceptionResNetV2 are the overall best-performing models\nin this study for the brain tumors, and chest X-ray datasets respectively. This is also\nconfirmed and visible from the saliency maps that these models have attributed to the\ninputs. Here, we observe that with a suitably trained model, Vanilla Gradient shows a\nminuscule degree of regularity in the saliency maps where features in all three tumor\nimages are highlighted by the model. As with the other models, XRAI has the best\ninterpretability for the input phenomena.\nXception shows the least visual explainability as indicated in Figure 8a. From the\ninput image, the Pituitary tumor located in the pituitary gland, a region below the\nhypothalamus is faintly attributed by all but XRAI. We can see that across all data\nmodalities in Figures 8, and 9, the attribution masks give little meaningful information\nabout the region of interest where the tumor is present although one is unsure of the\nCOVID-19 X-ray as it is not segmented for cross-matching. Though other factors such\nas the dataset size, batch size, annotation quality, and data augmentation technique\ncan considerably lead to the emergence of such characteristics, the model architecture\nand optimization objective have a large effect as they introduce stronger inductive\npriors on the space of learning functions all which we have experimentally tried to\ncontrol for through hyperparameter optimization. Moreover, this result indicates the\ndifference between statistical correlations learned by CNNs being different from the\nway humans perceive and process visual stimuli.\nWe observed that XRAI gives the best saliency maps as shown in the masked MRI\nimages. We also observed segmented regions in the X-ray images with XRAI. In all the\nimage modalities, VG and SG have coarse and partially noisy saliency maps, and can\nnot be used to infer meaningful explanations of the model inference mechanism. The\nbaseline choice has a major effect on the obtainable saliency map [31, 42, 46]. We used\na baseline of zero pixels for all attribution methods primarily because it is information\nneutral. XRAI demonstrated higher interpretability compared to vanilla gradient and\nguided integrated gradient methods because it is more suited to deep learning-based\nmedical image analysis tasks where the emphasis is to understand the region of interest\nfrom which a model inferred its prediction. We observed that a combination of XRAI\nand Blur IG can deduce feature saliency from the medical scans as 35% of saliency\nmaps of XRAI highlights important features that are in a close approximation of expert\nsegmentation for the DenseNet121 model. So, utilizing multiple attribution methods\ncan improve model interpretability for domain experts.\n16\n(a) VGG16\n(b) Xception\n(c) InceptionResNetV2\nFig. 9: COVID-19 X-ray: Three randomly sampled test images from each tumor class\nare chosen for saliency analysis using the trained (a) VGG16, (b) Xception, and (c)\nInceptionResNetV2. The infected regions are not segmented from the studied dataset.\n17\nThese results, therefore, open the possibility of not only accelerating the visual\ninterpretability of deep neural models in medical image analysis but as well offset pre-\npossessing such as human-in-the-loop segmentation, model debugging, and debiasing\nwhich are all crucial in real-world application use cases. The latter has an important\nrole in low-decision risk and highly regulated domains such as healthcare. In sum,\nthese stated use cases can rapidly advance access to needed but affordable healthcare\nfor low-resource settings.\nHowever, Table 3 in tandem with Figures 8 and 9 show that the inductive archi-\ntectural priors have to most impact on the selectivity of the receptive fields of CNNs\nfor visual saliency analysis. CNNs perform spatial weight sharing where each filter\nis replicated across the entire visual field of the input [49], thus, the resolution of\nthis receptive field matters. Unlike humans, CNNs have frequency response, texture,\nand shape biases that are evident across all the model architectures [50, 51]. Visual\nattribution methods must consider raising this notion in human-in-the-loop AI sys-\ntems to ameliorate the pitfalls of the wrong attribution in deep models for real-world\nhealthcare applications.\n5 Conclusion\nDeep learning models are gaining traction in ubiquitous healthcare applications from\nthe application of vision techniques to language models. However, the inference mech-\nanisms of these models are still an open question. In this paper, we posed the question:\nWhat do these deep learning models learn in medical images? To answer this ques-\ntion, we study a selection attribution framework and evaluated the framework using\ntwo widely used medical imaging modalities, namely MRI, and X-ray with publicly\navailable datasets. Our findings show that the robust statistical regularities learned\nbetween input-output mappings differ from biological visual stimuli processing done\nby humans. We show that different input attribution methods have varying degrees\nof explainability of the input signal. A robust representation learner and the right\nattribution approach are crucial to getting interpretable saliency maps of deep CNNs\nin medical image analysis. This is important because it will help in building human-\nin-the-loop computer-aided diagnostic models that not only generalize well to unseen\nsamples but are also explainable to domain experts. Our findings indicate that deep\nlearning models can complement the efforts of medical experts in efficiently detecting\nand diagnosing diseases from medical images. Thus, a human-in-the-loop approach can\naccelerate the adoption of neural models in medical decision-making. It provides a path\ntoward building stakeholder trust given that healthcare requires critical evaluation of\nassistive technologies before adoption and general usage.\nFinally, we encourage further research into volumetric medical imaging data, quan-\ntification of explainability of these visual attribution methods, developing benchmarks\nagainst which new visual attribution methods can be measured to accelerate model\nexplainability research, and the provision of open access segmented dataset so as to\ntest new saliency algorithms in ground truth expert segmented datasets.\n18\nDeclarations\nThe authors declare that they have no competing interests.\n• Funding: This research received no external funding.\n• Ethics approval: Not applicable\n• Consent to participate: Not applicable\n• Consent for publication: All authors have given their consent\n• Availability of data and materials: This research used the brain tumor dataset from\nthe School of Biomedical Engineering Southern Medical University, Guangzhou, con-\ntains 3064 T1-weighted contrast-enhanced images with three kinds of brain tumors.\nThe data is publicly available at Brain Tumor Dataset. The Chest X-Ray dataset\nis publicly available at: Chest X-Ray Images (Pneumonia) Dataset.\n• Code availability: The code is available at XDNNBioimaging for reproducibility.\n• Authors’ contributions: All the authors contributed to this work.\nReferences\n[1] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by back-\npropagating errors. nature 323(6088), 533–536 (1986)\n[2] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recogni-\ntion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 770–778 (2016)\n[3] Chollet, F.: Xception: Deep learning with depthwise separable convolutions.\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 1251–1258 (2017)\n[4] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep\nconvolutional neural networks. Advances in neural information processing systems\n25 (2012)\n[5] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556 (2014)\n[6] Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A., Ciompi, F., Ghafoorian, M.,\nVan Der Laak, J.A., Van Ginneken, B., Sánchez, C.I.: A survey on deep learning\nin medical image analysis. Medical image analysis 42, 60–88 (2017)\n[7] Murtaza, G., Shuib, L., Abdul Wahab, A.W., Mujtaba, G., Nweke, H.F., Al-\ngaradi, M.A., Zulfiqar, F., Raza, G., Azmi, N.A.: Deep learning-based breast\ncancer classification through medical imaging modalities: state of the art and\nresearch challenges. Artificial Intelligence Review 53(3), 1655–1720 (2020)\n19\n[8] Reyes, M., Meier, R., Pereira, S., Silva, C.A., Dahlweid, F.-M., Tengg-Kobligk,\nH.v., Summers, R.M., Wiest, R.: On the interpretability of artificial intelligence\nin radiology: challenges and opportunities. Radiology: artificial intelligence 2(3),\n190043 (2020)\n[9] Cheng, J.: brain tumor dataset. figshare (2017) https://doi.org/10.6084/m9.\nfigshare.1512427.v5\n[10] Chowdhury, M.E.H., Rahman, T., Khandakar, A., Mazhar, R., Kadir, M.A., Mah-\nbub, Z.B., Islam, K.R., Khan, M.S., Iqbal, A., Emadi, N.A., Reaz, M.B.I., Islam,\nM.T.: Can ai help in screening viral and covid-19 pneumonia? IEEE Access 8,\n132665–132676 (2020) https://doi.org/10.1109/ACCESS.2020.3010287\n[11] Koh, P.W., Nguyen, T., Tang, Y.S., Mussmann, S., Pierson, E., Kim, B., Liang, P.:\nConcept bottleneck models. In: International Conference on Machine Learning,\npp. 5338–5348 (2020). PMLR\n[12] Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. Advances\nin neural information processing systems 30 (2017)\n[13] Shen, S., Han, S.X., Aberle, D.R., Bui, A.A., Hsu, W.: An interpretable deep\nhierarchical semantic convolutional neural network for lung nodule malignancy\nclassification. Expert systems with applications 128, 84–95 (2019)\n[14] Salahuddin, Z., Woodruff, H.C., Chatterjee, A., Lambin, P.: Transparency of deep\nneural networks for medical image analysis: A review of interpretability methods.\nComputers in Biology and Medicine 140, 105111 (2022) https://doi.org/10.1016/\nj.compbiomed.2021.105111\n[15] Bass, C., Silva, M., Sudre, C., Tudosiu, P.-D., Smith, S., Robinson, E.: Icam:\nInterpretable classification via disentangled representations and feature attribu-\ntion mapping. Advances in Neural Information Processing Systems 33, 7697–7709\n(2020)\n[16] Kim, E., Kim, S., Seo, M., Yoon, S.: Xprotonet: diagnosis in chest radiography\nwith global and local explanations. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 15719–15728 (2021)\n[17] Li, O., Liu, H., Chen, C., Rudin, C.: Deep learning for case-based reason-\ning through prototypes: A neural network that explains its predictions. In:\nProceedings of the AAAI Conference on Artificial Intelligence. 1 (2018)\n[18] Baumgartner, C.F., Koch, L.M., Tezcan, K.C., Ang, J.X., Konukoglu, E.:\nVisual feature attribution using wasserstein gans. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 8309–8319 (2018)\n[19] Cohen, J.P., Brooks, R., En, S., Zucker, E., Pareek, A., Lungren, M.P., Chaudhari,\n20\nA.: Gifsplanation via latent shift: a simple autoencoder approach to counterfactual\ngeneration for chest x-rays. In: Medical Imaging with Deep Learning, pp. 74–104\n(2021). PMLR\n[20] Lenis, D., Major, D., Wimmer, M., Berg, A., Sluiter, G., Bühler, K.: Domain\naware medical image classifier interpretation by counterfactual impact analysis.\nIn: International Conference on Medical Image Computing and Computer-\nAssisted Intervention, pp. 315–325 (2020). Springer\n[21] Schutte, K., Moindrot, O., Hérent, P., Schiratti, J.-B., Jégou, S.: Using style-\ngan for visual interpretability of deep learning models on medical images. arXiv\npreprint arXiv:2101.07563 (2021)\n[22] Seah, J.C., Tang, J.S., Kitchen, A., Gaillard, F., Dixon, A.F.: Chest radiographs\nin congestive heart failure: visualizing neural network learning. Radiology 290(2),\n514–522 (2019)\n[23] Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.:\nGrad-cam: Visual explanations from deep networks via gradient-based localiza-\ntion. In: Proceedings of the IEEE International Conference on Computer Vision,\npp. 618–626 (2017)\n[24] Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional net-\nworks: Visualising image classification models and saliency maps. arXiv preprint\narXiv:1312.6034 (2013)\n[25] Singla, S., Pollack, B., Wallace, S., Batmanghelich, K.: Explaining the black-box\nsmoothly-a counterfactual approach. arXiv preprint arXiv:2101.04230 (2021)\n[26] Bau, D., Zhou, B., Khosla, A., Oliva, A., Torralba, A.: Network dissection: Quan-\ntifying interpretability of deep visual representations. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 6541–6549 (2017)\n[27] Natekar, P., Kori, A., Krishnamurthi, G.: Demystifying brain tumor segmentation\nnetworks: interpretability and uncertainty analysis. Frontiers in computational\nneuroscience 14, 6 (2020)\n[28] Böhle, M., Eitel, F., Weygandt, M., Ritter, K.: Layer-wise relevance propagation\nfor explaining deep neural network decisions in mri-based alzheimer’s disease\nclassification. Frontiers in aging neuroscience, 194 (2019)\n[29] Camalan, S., Mahmood, H., Binol, H., Araújo, A.L.D., Santos-Silva, A.R., Vargas,\nP.A., Lopes, M.A., Khurram, S.A., Gurcan, M.N.: Convolutional neural network-\nbased clinical predictors of oral dysplasia: class activation map analysis of deep\nlearning results. Cancers 13(6), 1291 (2021)\n[30] Kermany, D.S., Goldbaum, M., Cai, W., Valentim, C.C., Liang, H., Baxter, S.L.,\n21\nMcKeown, A., Yang, G., Wu, X., Yan, F., et al.: Identifying medical diagnoses and\ntreatable diseases by image-based deep learning. Cell 172(5), 1122–1131 (2018)\n[31] Kapishnikov, A., Venugopalan, S., Avci, B., Wedin, B., Terry, M., Bolukbasi,\nT.: Guided integrated gradients: An adaptive path method for removing noise.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 5050–5058 (2021)\n[32] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected\nconvolutional networks. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 4700–4708 (2017)\n[33] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,\nVanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9\n(2015)\n[34] Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neu-\nral networks. In: International Conference on Machine Learning, pp. 6105–6114\n(2019). PMLR\n[35] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang,\nZ., Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual\nrecognition challenge. International journal of computer vision 115(3), 211–252\n(2015)\n[36] Hochreiter, S.: Untersuchungen zu dynamischen neuronalen netzen. Diploma,\nTechnische Universität München 91(1) (1991)\n[37] Bengio, Y., Simard, P., Frasconi, P.: Learning long-term dependencies with gra-\ndient descent is difficult. IEEE transactions on neural networks 5(2), 157–166\n(1994)\n[38] Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward\nneural networks. In: Proceedings of the Thirteenth International Conference on\nArtificial Intelligence and Statistics, pp. 249–256 (2010). JMLR Workshop and\nConference Proceedings\n[39] Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-\nresnet and the impact of residual connections on learning. In: Thirty-first AAAI\nConference on Artificial Intelligence, p. 1 (2017)\n[40] Alotaibi, B., Alotaibi, M.: A hybrid deep resnet and inception model for hyper-\nspectral image classification. PFG–Journal of Photogrammetry, Remote Sensing\nand Geoinformation Science 88(6), 463–476 (2020)\n[41] Finn, C., Abbeel, P., Levine, S.: Model-agnostic meta-learning for fast adaptation\n22\nof deep networks. In: International Conference on Machine Learning, pp. 1126–\n1135 (2017). PMLR\n[42] Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks.\nIn: International Conference on Machine Learning, pp. 3319–3328 (2017). PMLR\n[43] Duda, R.O., Hart, P.E., et al.: Pattern Classification and Scene Analysis vol. 3.\nWiley New York, ??? (1973)\n[44] Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine learning\nresearch 9(11) (2008)\n[45] Wold, S., Esbensen, K., Geladi, P.: Principal component analysis. Chemometrics\nand intelligent laboratory systems 2(1-3), 37–52 (1987)\n[46] Kapishnikov, A., Bolukbasi, T., Viégas, F., Terry, M.: Xrai: Better attributions\nthrough regions. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 4948–4957 (2019)\n[47] Chowdhury, M.E., Rahman, T., Khandakar, A., Mazhar, R., Kadir, M.A., Mah-\nbub, Z.B., Islam, K.R., Khan, M.S., Iqbal, A., Al Emadi, N., et al.: Can ai help in\nscreening viral and covid-19 pneumonia? IEEE Access 8, 132665–132676 (2020)\n[48] Brima, Y., Atemkeng, M., Tankio Djiokap, S., Ebiele, J., Tchakounté, F.: Transfer\nlearning for the detection and diagnosis of types of pneumonia including pneu-\nmonia induced by covid-19 from chest x-ray images. Diagnostics 11(8) (2021)\nhttps://doi.org/10.3390/diagnostics11081480\n[49] Luo, W., Li, Y., Urtasun, R., Zemel, R.: Understanding the effective receptive field\nin deep convolutional neural networks. Advances in neural information processing\nsystems 29 (2016)\n[50] Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Bren-\ndel, W.: Imagenet-trained cnns are biased towards texture; increasing shape bias\nimproves accuracy and robustness. arXiv preprint arXiv:1811.12231 (2018)\n[51] Baker, N., Lu, H., Erlikhman, G., Kellman, P.J.: Deep convolutional networks do\nnot classify based on global object shape. PLoS computational biology 14(12),\n1006613 (2018)\n23\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-08-01",
  "updated": "2023-08-07"
}