{
  "id": "http://arxiv.org/abs/2208.01736v1",
  "title": "Federated Deep Reinforcement Learning for Resource Allocation in O-RAN Slicing",
  "authors": [
    "Han Zhang",
    "Hao Zhou",
    "Melike Erol-Kantarci"
  ],
  "abstract": "Recently, open radio access network (O-RAN) has become a promising technology\nto provide an open environment for network vendors and operators. Coordinating\nthe x-applications (xAPPs) is critical to increase flexibility and guarantee\nhigh overall network performance in O-RAN. Meanwhile, federated reinforcement\nlearning has been proposed as a promising technique to enhance the\ncollaboration among distributed reinforcement learning agents and improve\nlearning efficiency. In this paper, we propose a federated deep reinforcement\nlearning algorithm to coordinate multiple independent xAPPs in O-RAN for\nnetwork slicing. We design two xAPPs, namely a power control xAPP and a\nslice-based resource allocation xAPP, and we use a federated learning model to\ncoordinate two xAPP agents to enhance learning efficiency and improve network\nperformance. Compared with conventional deep reinforcement learning, our\nproposed algorithm can achieve 11% higher throughput for enhanced mobile\nbroadband (eMBB) slices and 33% lower delay for ultra-reliable low-latency\ncommunication (URLLC) slices.",
  "text": "Accepted by 2022 IEEE Global Communications Conference (GLOBECOM), ©2022 IEEE\nFederated Deep Reinforcement Learning for\nResource Allocation in O-RAN Slicing\nHan Zhang, Hao Zhou, and Melike Erol-Kantarci, Senior Member, IEEE\nSchool of Electrical Engineering and Computer Science, University of Ottawa\nEmails:{hzhan363, hzhou098, melike.erolkantarci}@uottawa.ca\nAbstract—Recently, open radio access network (O-RAN) has\nbecome a promising technology to provide an open environ-\nment for network vendors and operators. Coordinating the x-\napplications (xAPPs) is critical to increase ﬂexibility and guar-\nantee high overall network performance in O-RAN. Meanwhile,\nfederated reinforcement learning has been proposed as a promis-\ning technique to enhance the collaboration among distributed\nreinforcement learning agents and improve learning efﬁciency. In\nthis paper, we propose a federated deep reinforcement learning\nalgorithm to coordinate multiple independent xAPPs in O-RAN\nfor network slicing. We design two xAPPs, namely a power\ncontrol xAPP and a slice-based resource allocation xAPP, and we\nuse a federated learning model to coordinate two xAPP agents to\nenhance learning efﬁciency and improve network performance.\nCompared with conventional deep reinforcement learning, our\nproposed algorithm can achieve 11% higher throughput for\nenhanced mobile broadband (eMBB) slices and 33% lower delay\nfor ultra-reliable low-latency communication (URLLC) slices.\nIndex Terms—Federated learning, deep reinforcement learn-\ning, Open RAN, network slicing,\nI. INTRODUCTION\nOne of the main purposes of Open radio access network\n(O-RAN) is to improve RAN performance and supply chain\nby adopting open interfaces that allow multiple vendors to\nmanage the intelligence of RAN. O-RAN is expected to sup-\nport interoperability between devices from multiple vendors\nand provide network ﬂexibility at a lower cost [1]. On the\nother hand, network slicing has been considered as a promising\ntechnique that can separate a physical network into multiple\nlogical networks to provide service-customized solutions [2]. It\ncan be used for multi-service problems in O-RAN to maintain\nslice-level key performance indicators.\nIn O-RAN, there are various network functions called as\nx-applications (xAPPs). Each xAPP can be considered as an\nindependent agent to control a speciﬁc network function such\nas power control and resource allocation [3]. Considering\nthat these xAPPs can be designed by various vendors, xAPP\ncan apply conﬂicting conﬁgurations when performing inde-\npendent optimization tasks, thus leading to overall network\nperformance degradation [4] . Moreover, in the context of\nnetwork slicing, each xAPP will serve multiple network slices\nwith diverse quality of service (QoS) requirements, which will\nsigniﬁcantly increase the complexity of network management\nand control. Therefore, coordinating various xAPP agents\nand satisfying the service level agreements of multiple slices\nsimultaneously is a crucial challenge for O-RAN.\nMachine learning, especially reinforcement learning (RL),\nhas been widely used to solve optimization problems in\nwireless networks [6]. In RL, individual agents interact with\ntheir environment to gain experience and learn to get the\nmaximum expected reward. Meanwhile, federated learning\n(FL) is an emerging approach in machine learning, which\nrefers to multiple partners training models separately in a\ndecentralized but collaborative manner to build shared models\nwhile maintaining data privacy [7]. Federated reinforcement\nlearning is proposed as a combination of FL and RL, aiming\nto apply the idea of FL to distributed RL models to enhance\nthe collaboration among multiple RL agents [8]. Considering\nthe openness and intelligence requirements of the O-RAN\nenvironment, federated reinforcement learning becomes an\ninteresting solution to coordinate the operation of multiple\nintelligent xAPP agents in O-RAN.\nCurrently, there have been some applications of federated\nreinforcement learning in wireless communications area. How-\never, in most existing works, different agents are designed to\nperform similar network applications with the same action and\nstate spaces to accelerate the exploration of the environment\nthrough experience sharing [9]- [11]. To the best of our\nknowledge, there are limited works on federated reinforcement\nlearning on the problems where multiple agents have different\naction and state spaces and perform different network appli-\ncations.\nIn this work, we propose a federated deep reinforcement\nlearning algorithm to coordinate multiple xAPPs for network\nslicing in O-RAN . We ﬁrst deﬁne two xAPPs: a power\ncontrol xAPP and a hierarchical radio resource allocation\nxAPP. The local models are trained at each xAPP agent, and\nthen these local models are submitted to a coordination model\nand serve as the input for predicting a joint Q-table. Finally,\nthe joint Q-table is disassembled into local Q-tables for the\nactions. Simulation results show that our proposed federated\ndeep reinforcement learning model can obtain 11% higher\nthroughput and 33% lower delay than independent learning\nmodels.\nThe rest of the paper is arranged as follows. In Section\nII, related works are introduced, and the system model is de-\nscribed in Section III. Section IV explains the implementation\nof two xAPP applications. Section V introduces the proposed\nfederated deep reinforcement learning algorithm. Simulation\nsettings and results are introduced in Section VI, and Section\nVII concludes the paper.\narXiv:2208.01736v1  [cs.NI]  2 Aug 2022\nII. RELATED WORKS\nThere have been many studies that applies RL techniques\nto power control and resource allocation problems in 5G net-\nworks. In [12], a deep Q-learning algorithm is performed for\nthe combined optimization of power control, beam forming,\nand interference reduction. In [13], the communication and\ncomputational resources are distributed to randomly arriving\nslice requests with different weights and QoS requirements\nwith a Q-learning algorithm. However, in these works, mul-\ntiple network applications are generally combined into one\nsingle agent for joint optimization. They are not applicable\nto cases when different applications are considered as various\nindependent agents in the O-RAN environment.\nOn the other hand, some existing works studied possible\napplications of federated reinforcement learning in the area\nof wireless communication. In [9], the authors leveraged deep\nreinforcement learning to allocate heterogeneous resources and\nperform computation ofﬂoading in the 5G ultra-dense network\nscenarios and the RL model is trained in a decentralized way\nwith FL. In [10], a hybrid federated reinforcement learning-\nbased structure is proposed for devices association in RAN\nslicing with parameter aggregation on two levels. In [11], a\nfederated reinforcement learning algorithm is used to con-\ntrol the access of users in O-RAN, where RL is used for\naccess decision making on single users and FL is used to\naggregate models from different users. In these works, FL\nis mainly adopted as a solution for the federation between\nagents performing the same network functions, which often\nhave similar state and action spaces, without considering how\ndifferent network functions and agents are federated.\nIn our previous work [14], an information exchanging-\nbased team learning algorithm is proposed to mitigate the\nconﬂicts between xAPPs. However, in this work, we include\nmultiple network slices with diverse QoS requirements and\napply a novel federated deep reinforcement learning algorithm\nto enhance network performance.\nIII. SYSTEM MODEL\nAs shown in Fig. 1, we consider an O-RAN based deploy-\nment that includes multiple network functions and multiple\nslices for downlink transmission. The O-RAN system is com-\nposed of two radio intelligent controllers (RIC), non real-time\nRIC and near real-time RIC, centralized unit (CU), distributed\nunit (DU) and radio unit (RU). The CU is further decoupled\ninto control plane (CP) and user plane (UP). Each base station\n(BS) includes two types of slices, namely eMBB slice, and\nURLLC slice, and each slice may contain several UEs with\nsimilar QoS requirements. On the other hand, two network\nfunctions are jointly considered: power control and radio\nresource allocation. We aim to satisfy the diverse QoS demand\nof slices by jointly controlling these network functions. The\nproposed scheme can be applied for any other xAPP-based\nnetwork function coordination. But here, we consider power\ncontrol and resource allocation since they are among the most\nfundamental network control functions.\nFig. 1. O-RAN network system model.\nFor the power control xAPP, it decides the transmission\npower level of each BS to control its own channel capacity and\nthe interference on adjacent BSs. For the resource allocation\nxAPP, we consider the radio resource block (RB) as the\nsmallest resource unit to be allocated [15]. The RBs are\nﬁrst allocated to each slice, then the intra-slice RB allocation\nis implemented to distribute RBs to speciﬁc user device\ntransmissions. On the upper level, a coordination model in\nnear real-time RIC is set up to federate two xAPPs. The goal\nof the model is to coordinate two xAPPs to minimize the delay\nof URLLC slices and to maximize the throughput for eMBB\nslices.\nWe assume that at time slot t, the transmission power of the\nrth RB of BS k is denoted by Pk,r. The signal interference\nnoise ratio (SINR) of the transmission link between BS k and\ndevice m on the rth RB can be formulated as:\nηk,r,m =\nαk,r,mgk,mPk,r\nP\nk′∈K,k′̸=k\nP\nm′∈mk′ αk′,r,m′gk′,mPk′,r + BrN0\n,\n(1)\nwhere αk,r,m is a binary indicator that denotes whether the\nrth RB of BS k is allocated to device m. gk,m is the channel\ngain between BS k and device m. Br denotes the bandwidth\nof the rth RB and N0 denotes the noise power density.\nThe link capacity between BS k and device m is denoted\nas Ck,m and it can be calculated as:\nCk,m =\nX\nr∈R\nBrlog2(1 + ηk,r,m),\n(2)\nwhere R denotes the set of all the RBs that can be allocated.\nThe total delay of user m consists of three components, the\ntransmission delay, the queuing delay and the retransmission\ndelay. It can be given as:\ndtotal\nm\n= dtx\nm + dque\nm\n+ drtx\nm ,\n(3)\nwhere dtx\nm, dque\nm\nand drtx\nm\ndenote the transmission delay, the\nqueuing delay and the retransmission delay. The transmission\ndelay is formulated as:\ndtx\nm = Lm\nCk,m\n,\n(4)\nwhere Lm is the packet size of user m. For eMBB slices,\noptimization aims to achieve maximum total throughput, and\nfor URLLC slices, the aim is to minimize the average delay\nof packets.\nThe problem can be formulated as follows:\nmax\nPk,αk,r,m\nX\nk∈K\nX\nn∈Nk\nwnrn\n(5)\ns.t. (1) −(4)\nrembb\nn\n=\n\n\n\ntan−1(\nP\nm∈M embb\nn\nbm),\n|Hembb\nn\n| ̸= 0\n0,\nelse\n(5a)\nrurllc\nn\n=\n\n\n\n1 −P\nm∈M urllc\nn\ndm,\n|Hurllc\nn\n| ̸= 0\n0,\nelse\n(5b)\nPmin ≤Pk ≤Pmax, ∀k\n(5c)\nαk,r,m = {0, 1}, ∀k, r, m\n(5d)\nΣm∈Mαk,r,m = 1, ∀k, r\n(5e)\nwhere wn and rn denotes the priority weight and the reward of\neach slice respectively. Equation (5a) shows that the reward\nof eMBB slices is decided by the total throughput. M embb\nn\ndenotes the set of devices in the nth slice and Hurllc\nn\ndenotes\nthe queued length of packets in slice nth. The inverse tangent\ncalculator is used to mapping the value to a limited interval.\nEquation (5b) shows that the reward of URLLC slices is de-\ncided by the packet delay. Equation (5c), (5d) and (5e) ensure\nthe assigned power level and allocated RBs are limited by the\nmaximum power constraints and the resource constraints.\nIV. DQN BASED POWER CONTROL AND RESOURCE\nALLOCATION XAPPS\nIn this section, we introduce how to transform the power\ncontrol and radio resource allocation as two independent\nxAPPs. For power control, we deploy a deep Q-Networks\n(DQN) to decide the transmission power level of the BS.\nFor radio resource allocation, we combine the DQN with a\nhierarchical decision-making mechanism for RBs allocation.\nA. Power Control xAPP Agent\nThe power control xAPP will decide the transmission power\nlevel of the BS, and after that we assume the power is\nuniformly allocated to all RBs. The Markov decision process\n(MDP) of power control agent is deﬁned as follows:\n• State: The state of power control model includes the\nqueue length of packets, the current delay and the current\ntransmission power, which is given as:\nsk,t = {Hn,\nX\nm∈Mn\ndm, Pk|∀n ∈N, k ∈K},\n(6)\nwhere Hn denotes the queue length of packets in the\nnth slice, Pk denotes the transmission power of the\nkth BS. Here the state deﬁnition represents the trafﬁc\ndemand and network status, and then the agent can\nchange the transmission power accordingly to achieve\ndesired network performance.\n• Action: The action of power control is to choose power\nlevel from:\nak,t = {1, 2, ..., Lmax},\n(7)\nwhere Lmax denotes the highest power level. Then the\ntransmission power of the kth BS is given as:\nPk,t = ak,tPmax\nLmax\n,\n(8)\n• Reward: The reward of power control model is deﬁned as\nthe weighted sum reward of all the slices with a penalty\nof large transmission power.\nrk,t =\nX\nn∈Nk\nwnrn −αak,t,\n(9)\nThe purpose of establishing penalty items is to balance\nthe power consumption and network performance, and\ncontrol the interference on neighbouring BSs.\nThe goal of DQN is to maximize the expected long-term\nreward, which is given as:\nQ(s, a) = E[rt + γQ(st+1, at+1)|st = s, at = a],\n(10)\nIn DQN, the Q-values are approximated by a deep neural\nnetwork (DNN), and the stochastic gradient descent algorithm\nis adopted to update the parameters of DNN, which can be\ngiven as:\nθt+1 = θt + α[rt + γ max\na′\nQ(st+1, at+1; θt)\n−Q(st, at; θt)]∇Q(st, at; θt),\n(11)\nwhere θ,α and γ respectively denote the parameters of DQN,\nthe learning rate and the discount factor. By experience replay\nand updating the parameters iteratively, the DNN can predict\nthe expected accumulated reward and choose the optimal\naction accordingly.\nB. Radio Resource Allocation xAPP Agent\nFor radio resource allocation xAPP, the inter-slice radio\nresource allocation is ﬁrst performed to decide the amount\nof RBs that will be allocated to each slice. The MDP of inter-\nslice radio resource allocation is given as follows:\n• State: The state is composed of the queue length of\npackets and the current delay, which is given as:\nsk,t = {Hn,\nX\nm∈Mn\ndm|∀n ∈N},\n(12)\n• Action: The action is to decide the portion of RBs\nallocated to each slice, which is given as:\nak,t = {RBn ∈{0, 1, ..., Rmax}|\nX\nn∈N\nRBn = Rmax},\n(13)\nwhere Rmax denotes the number of RB groups that can\nbe allocated\n• Reward: The reward of inter-slice radio resource alloca-\ntion model is deﬁned as the weighted sum reward of all\nthe slices, which is given as:\nrk,t =\nX\nn∈Nk\nwnrn,\n(14)\nThen, for the intra-slice resource allocation, we assume all\nthe devices in the same slice have equal priority. Hence, we\ndeploy a proportional fair algorithm (PPF) [16]. The basic idea\nis to determine the priority of devices for resource scheduling\nbased on the ratio of instantaneous transmission rate and\nthe long-term average transmission rate of a single device.\nThe intra-slice radio resource allocation strategy based on\nproportional fairness scheduling can be formulated as follows:\nargmax\nm∈Mk,n\nCk,m\n(Pt\nt−∆t bt,m)/∆t\n(15)\nwhere (Pt\nt−∆t bt,m)/∆t denotes the average transmission rate\nof device m over the past period ∆t. The RBs allocated to\nslice n will be given to device m according to the selection\nrule in the above formula.\nV. FEDERATED DEEP REINFORCEMENT LEARNING FOR\nXAPP COORDINATION\nIn this section, we ﬁrstly introduce federated deep reinforce-\nment learning, then illustrate how we deploy federated deep\nreinforcement learning for coordination of xAPPs.\nA. Federated Deep Reinforcement Learning\nFederated deep reinforcement learning is proposed as a\ncombination of RL and FL. In federated deep reinforcement\nlearning, the samples, features, and labels in FL are replaced\nby the experience records, states, and actions in deep rein-\nforcement learning. According to environment partitioning,\nfederated reinforcement learning can be divided into two types\n[17]. In horizontal federated reinforcement learning (HFRL),\ndifferent agents have similar action and state spaces, and they\nact independently in different environments. The purpose of\nfederating is to accelerate the exploration of the environment\nby experience sharing. In vertical federated reinforcement\nlearning (VFRL), all the agents act in the same environment.\nThe state and action space of agents can be heterogeneous,\nand their actions collaboratively change the environment and\ninﬂuence the reward.\nB. Proposed VFRL Coordination Algorithm\nFig. 2 shows the structure of our proposed VFRL algorithm.\nThe general process of one update cycle in VFRL can be\ndescribed as follows. Firstly, each agent observes their states\nfrom the environment. Next, the local models of different\nagents are respectively updated with local training. The inter-\nmediate results calculated by local models are then submitted\nto a global model for global training, and the decisions of\nthe global model are sent back to agents and executed to\nFig. 2. Process of federated deep reinforcement learning.\nchange the environment. Local models are then updated based\non the feedback of the global model and the reward from the\nenvironment.\nIn Fig. 2, agent α and agent β refer to the power control\nxAPP and radio resource allocation xAPP, respectively. θg,\nθα and θβ respectively denote the parameters of the global\nmodel and the local models of two xAPPs. The core idea of\nour proposed algorithm is to use the local Q-table generated\nby two xAPPs to indicate the expected reward of independent\nactions and make a global calibration of the local models.\nThe federating process can be summarized as three steps:\n• Local inference. During the local inference step, power\ncontrol and radio resource allocation xAPPs observe state\ninformation from BSs, then they use observed state and\nlocal DQN model to calculate the local Q-table, which\ncan be formulated as:\nQα = DQN α(sα, aα; θα)\n(16)\nQβ = DQN β(sβ, aβ; θβ)\n(17)\nwhere DQN α and DQN β denote two local DQN models\nof agent α and agent β. sα and sβ denote the locally\nobserved states and θα and θβ denote the parameters of\ntwo DQN models.\n• Global aggregation. During the global inference step, the\ntwo local Q-tables generated by global power control\nand radio resource allocation xAPPs are submitted and\ncombined as the input of the global model to calculate a\njoint global Q-table, which can be formulated as:\nQg = [Qg\nα|Qg\nβ] = DNN g([Qα|Qβ]; θg)\n(18)\nwhere Qg\nα denotes the Q-table of agent α after global\ncalibration and [Qg\nα|Qg\nβ] denotes the global Q-table is a\nsplicing of Q-tables of two agents after global calibration.\n• Global inference. During the global inference step, the\nglobal Q-table is split into two calibrated Q-tables and\nthe split Q-tables are used for action selection, which\ncan be formulated as:\naα = argmax\naα\nQg\nα\n(19)\naβ = argmax\naβ\nQg\nβ\n(20)\nTABLE I\nSIMULATION SETTINGS.\nParameters\nSettings\nCarrier conﬁguration\nBandwidth: 20MHz,\nNumber of RBs: 100,\nSubcarriers of each RB: 12,\nMaximum transmission power: 30 dBm\nTx/Rx antenna gain: 15 dB\nTTI size: 2 OFDM symbols\nPropagation model\n128.1 + 37.6log(Distance),\nLog-Normal shadowing 8 dB.\nRetransmissions\nMaximum number of retransmissions: 1,\nRound trip delay: 4 TTIs\nTrafﬁc mode\neMBB: Constant bit rate trafﬁc,\nPacket size 32 Bytes,\nURLLC: Poisson trafﬁc,\nPacket size 16 Bytes\nNetworking\nenvironment\n2 gNBs, 12 users for each BS\nInter-gNB distance: 500m\nSlice settings\n2 eMBB slices and 2 URLLC slices\nin each BS,\nPriority:\nURLLC 2 >URLLC 1 >eMBB 2>eMBB 1\nThe actions decided by global inference will be executed\nin the environment and the corresponding rewards will\nbe stored in the replay buffers. In addition, an adaptive\nϵ-greedy learning strategy is adopted, which means the\nagent will randomly choose actions with a certain prob-\nability in order to explore the unknown action space.\nVI. NUMERIC RESULTS\nA. Simulation settings\nIn the simulations, we consider 2 BSs, and each BS has\n4 slices, 2 eMBB slices and 2 URLLC slices. Each slice\nhas 3 users. We use constant bit rate trafﬁc for eMBB\nslices while the URLLC trafﬁc follows a Poisson distribution.\nSimulations are implemented with Matlab 5G toolbox and run\n10 simulations with 5000 TTIs. Table I shows the settings of\nour simulations.\nWe compare three cases, namely independent reinforcement\nlearning (IRL), centralized reinforcement learning (CRL), and\nfederated reinforcement learning (FRL). IRL means the local\nmodels of agents are trained separately without the global\nmodel. CRL means putting two xAPPs into a single agent and\ntraining a single model with joint states and actions. Here we\nuse CRL as an optimal baseline, since the centralized control\nwith global vision will generally bring the best performance.\nFRL is our proposed algorithm, in which local models are\nfederated by a coordination model. During the simulations,\nWe ﬁx the URLLC trafﬁc, change the trafﬁc load of eMBB\nslices.\nB. Simulation results\nFig. 3 shows the delay of URLLC slices when the URLLC\ntrafﬁc load is 2 Mbps and the eMBB trafﬁc load changes\nfrom 4 Mbps to 10 Mbps. Since the priority of URLLC slices\nis higher than that of eMBB slices and the trafﬁc load of\nFig. 3. URLLC slices delay under various eMBB trafﬁc load.\nFig. 4. ECCDF of the URLLC slices delay distributions.\nURLLC slices is constant, the delay of URLLC slices does\nnot signiﬁcantly increase when the eMBB trafﬁc load changes.\nIt can be observed that the delay of URLLC slices with the\nFRL algorithm is lower than that with the IRL algorithm and\nis very close to that with CRL. It means the federating process\ncan help coordinate different xAPPs and achieve comparable\nnetwork performance with centralized training. Speciﬁcally,\nwhen the eMBB trafﬁc load is 10 Mbps, the delay with FRL\nis 33% lower than the IRL algorithm. Fig. 4 further shows\nthe empirical complementary cumulative distribution function\n(ECCDF) of the delay of URLLC slices when the eMBB trafﬁc\nload is 8 Mbps. It shows that our proposed FRL has a better\ndelay distribution than IRL, which is indicated by a lower\nECCDF curve.\nMoreover, Fig. 5 shows the throughput of eMBB slices\nat various eMBB trafﬁc loads. When the eMBB trafﬁc load\ngrows, more eMBB trafﬁc needs to be transmitted, so the\nthroughput of eMBB slices will also grow. We can observe\nthat FRL can achieve higher eMBB throughput compared with\nIRL cases, and the network performance of FRL is very close\nto IRL. Speciﬁcally, when the eMBB trafﬁc load is 10 Mbps,\nthe eMBB throughput of FRL is 11% higher than that of IRL.\nFinally, the Fig. 6 shows the convergence performance of\nIRL, CRL, and FRL when the eMBB trafﬁc is 8 Mbps, and\nthe URLLC trafﬁc is 2 Mbps. We can observe that CRL can\nultimately achieve the highest reward, while the converged\nreward of FRL is slightly lower than CRL. This is because\nFig. 5. Throughput of eMBB slice under various eMBB trafﬁc load.\nFig. 6. Convergence comparison.\nCRL uses a central training approach, which allows for more\ncomprehensive global information to be captured. However,\nthe FRL has a faster convergence speed than CRL, which\ncan be explained by the huge state and action space of CRL\napproach. The reward of IRL is much lower than the other two\nalgorithms. Based on the convergence trend, FRL converges\nfaster than CRL.\nWe can draw the conclusion that the proposed FRL algo-\nrithm can achieve better network performance than IRL, and\nthe performance of FRL is very comparable with the optimal\nbaseline CRL. Meanwhile, FRL enables faster convergence\nand can scale to multiple agent situations with distributed\nstructure. It also allows information observed by different\nagents to be kept conﬁdential and consumes fewer training\nresources.\nVII. CONCLUSION\nFederated deep reinforcement learning has emerged as one\nof the most state-of-the-art machine learning techniques. Here,\nwe proposed a federated deep reinforcement learning solution\nto coordinate two xAPPs, power control, and radio resource\nallocation for network slicing in O-RAN. The core idea is\nto make distributed agents submit their local Q-tables to\na coordination model for federating, and generate a global\nQ-table for action selection. According to the simulation\nresults, our proposed algorithm can obtain lower delay and\nhigher throughput compared with independent reinforcement\nlearning cases, and the network performance is comparable\nto the centralized training results. The proposed framework\ncan be scaled up to more than two xAPPs and the increased\ncomputational cost is equal to the computational cost required\nto train a neural network whose input and output dimensions\nare the sum of action space dimensions of all the xAPPs.\nACKNOWLEDGEMENT\nThis work is funded by Canada Research Chairs program\nand NSERC Collaborative Research and Training Experience\nProgram (CREATE) under Grant 497981.\nREFERENCES\n[1] S. Kukli´nski, L. Tomaszewski, and R. Kołakowski, ”On O-RAN, MEC,\nSON and network slicing integration,” In 2020 IEEE Globecom Work-\nshops (GC Wkshps), pp. 1-6, 2020.\n[2] H. Zhou, M. Elsayed, and M. Erol Kantarci, “RAN Resource Slicing in\n5G Using Multi Agent Correlated Q-Learning,” in Proceedings of 2021\nIEEE conference on PIMRC, pp.1 6 , Sep. 2021.\n[3] L. Bonati, S. D’Oro, M. Polese, S. Basagni, and T. Melodia, “In-\ntelligence and Learning in O-RAN for Data-driven NextG Cellular\nNetworks,” IEEE Communications Magazine, vol. 59, no. 10, pp. 21–\n27, October 2021.\n[4] M. Polese et al., “Understanding O-RAN: Architecture, Interfaces, Al-\ngorithms, Security, and Research Challenges,” arXiv:2202.01032, 2022;\nhttps://arxiv.org/abs/2202.01032.\n[5] E.J. dos Santos, R.D. Souza, J.L. Rebelatto and H. Alves, ”Network\nslicing for URLLC and eMBB with max-matching diversity channel\nallocation,” in IEEE Communications Letters, vol. 24, no. 3, pp. 658-\n661, 2019\n[6] M. Elsayed and M. Erol-Kantarci, “AI-Enabled Future Wireless Net-\nworks: Challenges, Opportunities, and Open Issues,” in IEEE Vehicular\nTechnology Magazine, vol. 14, no. 3, pp. 70–77, Sep. 2019.\n[7] T. Li, A.K. Sahu, A. Talwalkar and V. Smith, ”Federated learning:\nChallenges, methods, and future directions,” in IEEE Signal Processing\nMagazine, vol. 37, no. 3, pp. 50-60, 2020.\n[8] H. Zhuo, W. Feng, Q. Xu, Q. Yang, and Y. Lin, “Federated reinforcement\nlearning,” CoRR, vol. abs/1901.08277, 2019.\n[9] S. Yu, X. Chen, Z. Zhou, X. Gong and D. Wu, ”When deep rein-\nforcement learning meets federated learning: Intelligent multitimescale\nresource management for multiaccess edge computing in 5G ultradense\nnetwork,” in IEEE Internet of Things Journal, vol. 8, no. 4, pp.2238-\n2251, 2020.\n[10] Y.J. Liu, G. Feng, Y. Sun, S. Qin and Y.C. Liang, ”Device association\nfor RAN slicing based on hybrid federated deep reinforcement learning,”\nIEEE Transactions on Vehicular Technology, vol. 69, no. 12, pp.15731-\n15745, 2020.\n[11] Y. Cao, S.Y. Lien, Y.C. Liang and K.C. Chen, June, ”Federated deep\nreinforcement learning for user access control in open radio access\nnetworks,” in ICC 2021-IEEE International Conference on Communi-\ncations, pp. 1-6, 2021.\n[12] F.B. Mismar, B.L. Evans and A. Alkhateeb, ”Deep reinforcement learn-\ning for 5G networks: Joint beamforming, power control, and interference\ncoordination,” in IEEE Transactions on Communications, vol. 68, no. 3,\npp.1581-1592, 2019.\n[13] Y. Shi, Y.E. Sagduyu and T. Erpek, ”Reinforcement learning for dynamic\nresource optimization in 5G radio access network slicing,” In 2020 IEEE\n25th International Workshop on Computer Aided Modeling and Design\nof Communication Links and Networks (CAMAD), pp. 1-6, 2020.\n[14] H. Zhang, H. Zhou and M. Erol-Kantarci, “Team Learning-Based\nResource Allocation for Open Radio Access Network (O-RAN),” in\nProc. IEEE International Conference on communications, Seoul, South\nKorea, May. 2022.\n[15] T. Erpek, A. Abdelhadi, and T. C. Clancy, ”An optimal application-aware\nresource block scheduling in LTE,” In 2015 International Conference\non Computing, Networking and Communications (ICNC), pp. 275-279,\n2015.\n[16] T. Bu, L. Li, and R. Ramjee, “Generalized proportional fair schedul-\ning in third generation wireless data networks,” in Proc. 2006 IEEE\nINFOCOM, pp. 1–12, 2016.\n[17] J.\nQi,\nQ.\nZhou,\nL.\nLei\nand\nK.\nZheng,\n”Federated\nreinforce-\nment\nlearning:\nTechniques,\napplications,\nand\nopen\nchallenges,”\nhttps://arxiv.org/abs/2108.11887, 2021.\n",
  "categories": [
    "cs.NI",
    "eess.SP"
  ],
  "published": "2022-08-02",
  "updated": "2022-08-02"
}