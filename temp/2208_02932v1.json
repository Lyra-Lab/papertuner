{
  "id": "http://arxiv.org/abs/2208.02932v1",
  "title": "Human Decision Makings on Curriculum Reinforcement Learning with Difficulty Adjustment",
  "authors": [
    "Yilei Zeng",
    "Jiali Duan",
    "Yang Li",
    "Emilio Ferrara",
    "Lerrel Pinto",
    "C. -C. Jay Kuo",
    "Stefanos Nikolaidis"
  ],
  "abstract": "Human-centered AI considers human experiences with AI performance. While\nabundant research has been helping AI achieve superhuman performance either by\nfully automatic or weak supervision learning, fewer endeavors are experimenting\nwith how AI can tailor to humans' preferred skill level given fine-grained\ninput. In this work, we guide the curriculum reinforcement learning results\ntowards a preferred performance level that is neither too hard nor too easy via\nlearning from the human decision process. To achieve this, we developed a\nportable, interactive platform that enables the user to interact with agents\nonline via manipulating the task difficulty, observing performance, and\nproviding curriculum feedback. Our system is highly parallelizable, making it\npossible for a human to train large-scale reinforcement learning applications\nthat require millions of samples without a server. The result demonstrates the\neffectiveness of an interactive curriculum for reinforcement learning involving\nhuman-in-the-loop. It shows reinforcement learning performance can successfully\nadjust in sync with the human desired difficulty level. We believe this\nresearch will open new doors for achieving flow and personalized adaptive\ndifficulties.",
  "text": "Human Decision Makings on Curriculum\nReinforcement Learning with Difﬁculty Adjustment\nYilei Zeng*, Jiali Duan*, Yang Li Emilio Ferrara, Lerrel Pinto, C.-C. Jay Kuo, Stefanos Nikolaidis\n{yilei.zeng,jialidua,yli546,emiliofe,nikolaid}@usc.edu\nlerrel@cs.nyu.edu,\ncckuo@sipi.usc.edu\nAbstract—Human-centered AI considers human experiences\nwith AI performance. While abundant research has been helping\nAI achieve superhuman performance either by fully automatic\nor weak supervision learning, fewer endeavors are experimenting\nwith how AI can tailor to humans’ preferred skill level given\nﬁne-grained input. In this work, we guide the curriculum\nreinforcement learning results towards a preferred performance\nlevel that is neither too hard nor too easy via learning from\nthe human decision process. To achieve this, we developed a\nportable, interactive platform that enables the user to interact\nwith agents online via manipulating the task difﬁculty, observing\nperformance, and providing curriculum feedback. Our system\nis highly parallelizable, making it possible for a human to\ntrain large-scale reinforcement learning applications that require\nmillions of samples without a server. The result demonstrates\nthe effectiveness of an interactive curriculum for reinforcement\nlearning involving human-in-the-loop. It shows reinforcement\nlearning performance can successfully adjust in sync with the\nhuman desired difﬁculty level. We believe this research will\nopen new doors for achieving ﬂow and personalized adaptive\ndifﬁculties. Our demo executable and videos are available at\nhttps://bit.ly/372vCNv.\nI. INTRODUCTION\nHumans make billions of decisions in games, and how to\nleverage this wealth of resources to make better adaptive and\npersonalized systems has been a perpetual pursuit. Humans are\nboth quick and impatient learners, as they lose interest when\nthey outgrow once challenging games or stagnate too early. By\nlearning human’s learning process through gaming feedback\nloops, AI can better create a ﬂow channel that is neither\ntoo challenging nor too boring. A curriculum organizes the\nlearning process in an upward spiral by gradually mastering\nmore complex skills and knowledge [1]. When combined with\nreinforcement learning, it’s been shown that a curriculum can\nimprove convergence or performance compared to learning\nfrom the target task from scratch [2]–[4]. Thus, will make\nﬁner adjustments faster.\nPrevious works [1], [5] focus on reaping the advantage of a\ncurriculum strategy to train the best performing AI agent, via\nautomatically proposing curriculum through another RL agent,\nsuch as teacher-student framework [6], [7], self-play [8]–[10]\nor goal-gan [5]. One way of interpreting these approaches is\nthat curriculum evolves through the adversarial nature between\nthe two agents, similar to GAN [11], [12].\nCompared to an automatic agent, human has an innate\nability to improvise and adapt when confronted with different\nscenarios, in order to design more personalized experiences\nFig. 1: Given speciﬁc scenarios during curriculum training,\nhumans can adaptively decide whether to be “friendly” or\n“adversarial” by observing the progress the agent is able\nto make. In cases where performance degrades, a user may\nﬂexibly adjust the strategy as opposed to an automatic assistive\nagent.\nwe must capture these human indications, to help the expalin-\nability and ﬂexibility for curriculum reinforcement learning.\nIn Figure 1, a user is able to intuitively understand the learn-\ning progress and dynamically manipulate the task difﬁculty\nby changing the height of the wall. With new challenging\nenvironments, we show how human inductive bias can help\nsolve three nontrivial tasks to various difﬁculty levels that are\notherwise unsolvable by learning from scratch or even auto-\ncurriculum.\nAnother key motivation is assistive agents such as au-\ntonomous driving systems, language-based virtual systems,\nand robotic companions. The agent should provide services\nadjusted to human preferences and personal needs [13].\nIn Section II, we give a brief introduction of related work. In\nSection III, Our interactive curriculum platform is introduced,\nwith which we identify the “inertial” problem in an “easy-to-\nhard” automatic curriculum. In Section IV, We show prelim-\ninary results of user studies on our environments that require\nmillions of interactions. We conclude and discuss future work\nin Section V. Finally, we outline some limitations for this line\nof research in Section VI.\nII. RELATED WORK\nA. Curriculum Reinforcement Learning\nApart from previously mentioned automatic learning meth-\nods, the most related work to ours is [14], which shows\nempirically how a rich environment can help to promote\nthe learning of complex behavior without explicit reward\narXiv:2208.02932v1  [cs.AI]  4 Aug 2022\nguidance. In comparison, we evolve environments leveraging\nhuman’s inductive bias in curriculum design.\nB. Human-in-the-Loop Reinforcement Learning\nAs learning agents move from research labs to the real\nworld, it becomes increasingly important for human users,\nespecially those without programming skills, to teach agents\ndesired behavior. A large amount of work focuses on imitation\nlearning [15]–[18], where demonstrations from the expert act\nas direct supervision. Humans can also interactively shape\ntraining with only positive or negative reward signals [19]\nor combine manual feedback with rewards from MDP [20],\n[21]. A recent work formulates human-robot interaction as\nan adversarial game [22] and shows improvement in grasping\nsuccess and robustness when the robot trains with a human\nadversary.\nIn this paper, we aim to close the loop between these\ntwo ﬁelds by studying the effect of interactive curriculum\non reinforcement learning. To achieve this, we have designed\nthree challenging environments that are nontrivial to solve\neven for state-of-the-art RL method [23], which we describe\nin the next section.\nIII. INTERACTIVE CURRICULUM GUIDED BY HUMAN\nA. Interactive Platform\nWe build our interactive platform with three goals in mind:\n1) Real-time online interaction with ﬂexibility; 2) Paralleliz-\nable for human-in-the-loop training 3) Seamless control be-\ntween reinforcement learning and human-guided curriculum.\nFig. 2: General design of our interactive platform and associ-\nations between environment container with RL trainer as well\nas interactive-interface.\nFig. 3: Example of our interactive platform training in parallel.\nWe run an event-driven environment container separated\nfrom the training process to achieve the ﬁrst goal, allowing the\nuser to send a control signal (e.g., UI control, scene layout,\ntask difﬁculty) to the environment during training via the inter-\nactive interface. The framework is shown in Figure 2 to explain\nthe positions users, environment and training algorithms stand.\nWe integrate human-interactive signals into RL parallelization\nto achieve similar efﬁciency as automatic training. An example\nfor parallel training is shown in Figure 3. We perform central-\nized SGD updates with decentralized experience collection as\nagents of the same kind share the same network policy [24].\nWe also enable controlling environment parameters in different\ninstantiations simultaneously via a uniﬁed interactive interface,\nmaking it possible to solve tasks requiring millions of inter-\nactions. For the third goal, we display real-time instructions\nand allow users to inspect learning progress when designing\nthe curriculum.\nFigure 4 shows our released environments for curriculum\nreinforcement learning, where users can manipulate the task\ndifﬁculty. The agents will reach the green target in GridWorld,\nnavigate to land on the green mat in Wall-Jumper and reach the\ndynamic green box in SparseCrawler, respectively. As shown\nin Figure 4, the user formulated curriculum in a way that was\nneither too hard nor too easy for the agent, so as to maximize\nthe efﬁciency and quality trade-off. During interaction, the user\ncan pause, play or save the current conﬁguration. The locations\nof the objects in the arena are customizable with cursor and\nthe height of the wall is tunable for difﬁculty transitions. Our\ninteractive interface are same for the rest environments listed\nbelow.\nGrid-World The agent (represented as a blue square) is tasked\nto reach the goal position (green plus), by navigating through\nobstacles (red cross, maximally 5). All objects are randomly\nspawn on a 2D plane. A positive reward 1 for reaching the\ngoal, negative 1 for cross and -0.01 for each step. Movements\nare in cardinal directions.\nWall-Jumper The goal is to navigate a wall (maximum height\nis 8), by jumping or (possibly) leveraging a block (white box).\nPositive reward of 1 for successful landing on the goal location\n(green mat) or negative 1 for falling outside or reaching\nmaximum allowed time. A penalty of -0.0005 for each step\ntaken. The observation space is 74 dimensional, corresponding\nto 14 ray casts each detecting 4 possible objects, plus the\nglobal position of the agent and whether or not the agent is\nFig. 4: Our interactive platform for curriculum reinforcement learning allows the user to manipulate the task difﬁculty via\na uniﬁed interface (slider and buttons). All three tasks receive only sparse rewards. The manipulable variable for the three\nenvironments is respectively the number of red obstacles (GridWorld, Top row), the height of the wall (Wall-Jumper, Middle\nrow), and the radius of the target (SparseCrawler, Bottom row). The task difﬁculty gradually increases from left to right.\ngrounded. Allowed actions include translation, rotation and\njumping.\nSparse-Crawler A crawler is an agent with 4 arms and 4\nforearms. The aim is to reach a randomly located target on\nthe ground (maximum radius of 40). The state is a vector\nof 117 variables corresponding to position, rotation, velocity,\nand angular velocities of each limb plus the acceleration and\nangular acceleration of the body. Actions space is of size 20,\ncorresponding to target rotations for joints. Only sparse reward\nis provided, when the target is reached.\nB. A Simple Interactive Curriculum Framework\nCurriculum reinforcement learning is an adaptation strategy\nto improve RL training by ordering a set of related tasks\nto be learned [1]. The most natural ordering is gradually\nincreasing the task difﬁculty with an automatic curriculum.\nHowever, as shown in Figure 5a, the auto-curriculum quickly\nmastered skills when walls were low but failed to adapt when\na dramatic change of skill was required (Figure 5c), leading to\na degradation of performance on the ultimate task (Figure 5b).\nThe reason is that the agent must use a box to navigate a high\nwall in contrast to low-wall scenarios, where additional steps\nto locate the box will be penalized.\nOur results testify to what [1] observed in their curriculum\nfor the supervised classiﬁcation task, that curriculum should be\ndesigned to focus on “interesting” examples. In our case, the\ncurriculum that resided at an easy level for the ﬁrst 3M steps\n“overﬁtted” the previous skill and prevented it from adapting.\nAlthough a comprehensive IF-ELSE rule is possible, in the\nreal-world, where situations could be arbitrarily complex,\nadaptable behavior out of guidance from a human is desired.\nFollowing this spirit, we test the ability of human interactive\nAlgorithm 1: Human-Guided Interactive Curriculum\nResult: Agent’s policy πR\nInitialize difﬁculty=0;\nwhile step ≤total step do\nπR\nnew = Train(πR\nold, difﬁculty);\nif step % interval ==0 then\ndifﬁculty=H (πR\nnew, difﬁculty);\nend\nπR\nold = πR\nnew\nend\ncurriculum using a simple framework (Algo 1), where human\n(function H) provides feedback by adjusting the task difﬁculty\nat a ﬁxed interval in the training loop (i.e., after evaluating\nthe agent’s learning progress on current difﬁculty, user can\nchoose to tune the task easier/harder or leave it unchanged).\nWe show in the next Section that with this simple interactive\ncurriculum, tasks that are originally unsolvable can be guided\ntowards success by humans, with an additional property of\nbetter generalization.\nIV. EXPERIMENTS\nWe train the agents for three competitive tasks using the\ntraining method described previously. We aim to show that a\nhuman-in-the-loop interactive curriculum can leverage human\nprior during adaptation which allows agents to build on past\nexperiences. For all our experiments, we ﬁx the interaction\ninterval (e.g., 0, 0.1, 0.2,...,0.9 of the total steps) and allow\nusers to inspect learning progress twice before adjusting the\ncurriculum. The user can either choose to make it easier,\nharder or unchanged. Our baseline is PPO with the optimized\n(a) Training curve\n(b) Testing curve\n(c) High Wall\nFig. 5: “Inertial” problem of auto-curriculum gradually grows the difﬁculty at ﬁxed intervals. The performance of the auto-\ncurriculum (orange curve) signiﬁcantly drops when navigation requires jumping over the box ﬁrst, but the learning inertial\nprevents it from adapting to the new task. Note that the testing curve is evaluated on the ultimate task unless otherwise stated.\nparameters as in [25]. We train GridWorld, Wall-Jumper, and\nSparseCrawler for 50K, 5M, and 10M steps, respectively.\nA. Effect of Interactive Curriculum\nIn Section III-A, we introduced three challenging tasks due\nto the sparsity of rewards. For example, in Figure 6a, we\nobserved that agents which learn from scratch (green and red\ncurves) had little chance of success with obstacles scattered\naround the grid, thus failing to reinforce any desired behavior.\nOn the other hand, users could gradually load or remove\nobstacles by inspecting the learning progress. Eventually, the\nmodels trained with our framework can solve GridWorld with\n5 obstacles present. Inspired by this, we further tested our\nframework on the SparseCrawler task (Figure 6c), which\nrequires 10M steps of training. Thanks to our parallel design\n(Section III-A), we were able to reduce the training time from\n10 to 3 hours, during which users would interact ten times.\nWhen trained with dynamically moving targets of increasing\nradius, we found that crawlers gradually learned to align\nthemselves in the right direction.\nIn the Wall-Jumper task (Figure 6b), we noticed a variance\nin performance given different users. One run (blue curve)\noutperformed learning from scratch with a noticeable margin,\nwhile another run (orange curve) performed less well but still\nconverged with learning from scratch. Nevertheless, both the\ntwo trials are much better than an auto-curriculum that suffers\nfrom over-ﬁtting, as described in Section III-B.\nB. Generalization Ability\nOver-ﬁtting to a particular dataset is a common problem\nin supervised learning. Similar problems can occur in rein-\nforcement learning when there’s no or slight variation in the\nenvironment. To deal with this problem, we had considered:\n1) randomness in terms of how the grid is generated; layout\nof blocks and jumpers; locations of crawlers and targets. 2)\nentropy regularization in our PPO implementation, making a\nstrong baseline.\nWe compare models trained with our framework with ones\ntrained from scratch in three environments with a set of tasks.\nFor example, in GridWorld, the agents were tested with the\nnumber of obstacles increasing from 1 to 5. In Wall-Jumper,\nthe heights of the wall rise from 0 to 8 discretely during\ntesting and in SparseCrawler, the radius of the moving target\ntransitions from 5 to 40 with a span of 5 (Figure 7). One\ncommon observation is that our model consistently outper-\nforms learning from scratch. Secondly, there’s a large gap\nbetween the curves from the curriculum learning model and\nlearning from scratch (Figure 7a), indicating that they “warm\nup” more quickly with easy tasks than directly jumping into\nthe difﬁcult task. The learning process is analogous to how\nhuman learns by building on past experiences. Interestingly,\nthe curves eventually congregate in Wall-Jumper (Figure 7b),\nfor both the curriculum model and scratch model. Finally, we\nobserved that the performance of our model in SparseCrawler\n(Figure 7c) continually arose and reached the target with 1 to 2\nmore successes, as opposed to the Wall-Jumper environment.\nWe would reset the environment in SparseCrawler only when\nit reaches the maximum time steps in a single round.\nWhen performing qualitative tests, our model solves the\nGridWorld with varying obstacles, whereas the learning from\nscratch model fails when the number of obstacles exceeds 3.\nFor Wall-Jumper, our model can reach the goal with minimum\nsteps, while the scratch model would inevitably use the block,\nnecessary only for heights over 6.5. In the SparseCrawler\nenvironment, our model has a faster moving speed and more\nsuccess, whereas the scratch model could only reach proximal\ntargets.\nV. CONCLUSION\nTo learn a difﬁcult task, humans have developed an easy-\nto-hard transition strategy to ameliorate the learning curve.\nSimilarly, curriculum reinforcement learning leverages experi-\nence across many easy tasks before adapting its skills to more\nchallenging ones. However, questions such as “what metric\nto use for quantifying the task difﬁculty” or “how should\ncurriculum be designed” remain unanswered.\nIn this research, we experimented and demonstrated how\nhuman decision-making can help curriculum reinforcement\nlearning agents make very ﬁne-grained difﬁculty adjustments.\n(a) GridWorld (obstacles of 5)\n(b) Wall-Jumper (height of 8)\n(c) SparseCrawler (radius of 40)\nFig. 6: Effect of interactive curriculum evaluated on the ultimate task.\n(a) GridWorld (obstacles from 1 to 5)\n.\n(b) Wall-Jumper (heights from 0 to 8)\n(c) SparseCrawler (radius from 5 to 40)\nFig. 7: The generalization ability of an interactive curriculum evaluated on a set of tasks. The average performance\nover these tasks is plotted for different time steps.\nWe released a multi-platform portable, interactive and paral-\nlelizable tool which features three non-trivial tasks that are\nchallenging to solve (sparse reward, transfer between skills,\nand a large amount of training up to 10M steps), with\nvarying curriculum space (discrete/continuous). We identiﬁed\na phenomenon of over-ﬁtting in auto-curriculum that leads\nto deteriorating performance during skill transfer with this\nenvironment. Then, We proposed a simple interactive cur-\nriculum framework facilitated by our uniﬁed user interface.\nThe experiment shows the promise of a more explainable and\ngeneralizable curriculum transition by involving human-in-the-\nloop on tasks that are otherwise nontrivial to solve. We would\nlike to explore a more efﬁcient method for collecting users’\ndecision-making for future work.\nVI. LIMITATIONS\nDue to the limited user sample size and time complexity\nrequired for our environment, the project will need many\nresources to deploy massively. However, we believe this\nresearch can still serve as examples for human-in-the-loop\nreinforcement learning research.\nREFERENCES\n[1] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum\nlearning,” in Proceedings of the 26th annual international conference\non machine learning, 2009, pp. 41–48.\n[2] M. E. Taylor and P. Stone, “Transfer learning for reinforcement learning\ndomains: A survey,” Journal of Machine Learning Research, vol. 10, no.\nJul, pp. 1633–1685, 2009.\n[3] A. Graves, M. G. Bellemare, J. Menick, R. Munos, and K. Kavukcuoglu,\n“Automated curriculum learning for neural networks,” in Proceedings\nof the 34th International Conference on Machine Learning-Volume 70.\nJMLR. org, 2017, pp. 1311–1320.\n[4] C. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel, “Re-\nverse curriculum generation for reinforcement learning,” arXiv preprint\narXiv:1707.05300, 2017.\n[5] D. Held, X. Geng, C. Florensa, and P. Abbeel, “Automatic goal gener-\nation for reinforcement learning agents,” 2018.\n[6] T. Matiisen, A. Oliver, T. Cohen, and J. Schulman, “Teacher-student cur-\nriculum learning,” IEEE transactions on neural networks and learning\nsystems, 2019.\n[7] R. Portelas, C. Colas, K. Hofmann, and P.-Y. Oudeyer, “Teacher algo-\nrithms for curriculum learning of deep rl in continuously parameterized\nenvironments,” arXiv preprint arXiv:1910.07224, 2019.\n[8] S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and\nR. Fergus, “Intrinsic motivation and automatic curricula via asymmetric\nself-play,” arXiv preprint arXiv:1703.05407, 2017.\n[9] T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, and I. Mordatch,\n“Emergent complexity via multi-agent competition,” arXiv preprint\narXiv:1710.03748, 2017.\n[10] B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew,\nand I. Mordatch, “Emergent tool use from multi-agent autocurricula,”\narXiv preprint arXiv:1909.07528, 2019.\n[11] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in\nAdvances in neural information processing systems, 2014, pp. 2672–\n2680.\n[12] J. Duan, X. Guo, Y. Song, C. Yang, and C.-C. J. Kuo, “Portraitgan for\nﬂexible portrait manipulation,” arXiv preprint arXiv:1807.01826, 2018.\n[13] Y. Zeng, “How human centered ai will contribute towards intelligent\ngaming systems,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 35, no. 18, 2021, pp. 15 742–15 743.\n[14] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa,\nT. Erez, Z. Wang, S. Eslami et al., “Emergence of locomotion behaviours\nin rich environments,” arXiv preprint arXiv:1707.02286, 2017.\n[15] S. Schaal, “Is imitation learning the route to humanoid robots?” Trends\nin cognitive sciences, vol. 3, no. 6, pp. 233–242, 1999.\n[16] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning\nand structured prediction to no-regret online learning,” in Proceedings\nof the fourteenth international conference on artiﬁcial intelligence and\nstatistics, 2011, pp. 627–635.\n[17] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in\nAdvances in neural information processing systems, 2016, pp. 4565–\n4573.\n[18] L. Pinto and A. Gupta, “Supersizing self-supervision: Learning to grasp\nfrom 50k tries and 700 robot hours,” in 2016 IEEE international\nconference on robotics and automation (ICRA). IEEE, 2016, pp. 3406–\n3413.\n[19] W. B. Knox and P. Stone, “Interactively shaping agents via human\nreinforcement: The tamer framework,” in Proceedings of the ﬁfth in-\nternational conference on Knowledge capture, 2009, pp. 9–16.\n[20] ——, “Combining manual feedback with subsequent mdp reward signals\nfor reinforcement learning,” in Proceedings of the 9th International\nConference on Autonomous Agents and Multiagent Systems: volume 1-\nVolume 1.\nCiteseer, 2010, pp. 5–12.\n[21] D.\nAbel,\nJ.\nSalvatier,\nA.\nStuhlm¨uller,\nand\nO.\nEvans,\n“Agent-\nagnostic human-in-the-loop reinforcement learning,” arXiv preprint\narXiv:1701.04079, 2017.\n[22] J. Duan, Q. Wang, L. Pinto, C.-C. J. Kuo, and S. Nikolaidis, “Robot\nlearning via human adversarial games.” CoRR, 2019.\n[23] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[24] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\nforcement learning,” in International conference on machine learning,\n2016, pp. 1928–1937.\n[25] A. Juliani, V.-P. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar, and\nD. Lange, “Unity: A general platform for intelligent agents,” arXiv\npreprint arXiv:1809.02627, 2018.\n",
  "categories": [
    "cs.AI",
    "cs.HC",
    "cs.LG",
    "I.2.6"
  ],
  "published": "2022-08-04",
  "updated": "2022-08-04"
}