{
  "id": "http://arxiv.org/abs/2208.05787v1",
  "title": "Unsupervised Face Morphing Attack Detection via Self-paced Anomaly Detection",
  "authors": [
    "Meiling Fang",
    "Fadi Boutros",
    "Naser Damer"
  ],
  "abstract": "The supervised-learning-based morphing attack detection (MAD) solutions\nachieve outstanding success in dealing with attacks from known morphing\ntechniques and known data sources. However, given variations in the morphing\nattacks, the performance of supervised MAD solutions drops significantly due to\nthe insufficient diversity and quantity of the existing MAD datasets. To\naddress this concern, we propose a completely unsupervised MAD solution via\nself-paced anomaly detection (SPL-MAD) by leveraging the existing large-scale\nface recognition (FR) datasets and the unsupervised nature of convolutional\nautoencoders. Using general FR datasets that might contain unintentionally and\nunlabeled manipulated samples to train an autoencoder can lead to a diverse\nreconstruction behavior of attack and bona fide samples. We analyze this\nbehavior empirically to provide a solid theoretical ground for designing our\nunsupervised MAD solution. This also results in proposing to integrate our\nadapted modified self-paced learning paradigm to enhance the reconstruction\nerror separability between the bona fide and attack samples in a completely\nunsupervised manner. Our experimental results on a diverse set of MAD\nevaluation datasets show that the proposed unsupervised SPL-MAD solution\noutperforms the overall performance of a wide range of supervised MAD solutions\nand provides higher generalizability on unknown attacks.",
  "text": "Unsupervised Face Morphing Attack Detection via Self-paced Anomaly\nDetection\nMeiling Fang1,2, Fadi Boutros1, Naser Damer1,2\n1Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany\n2Department of Computer Science, TU Darmstadt, Darmstadt, Germany\nEmail: meiling.fang@igd.fraunhofer.de\nAbstract\nThe supervised-learning-based morphing attack detec-\ntion (MAD) solutions achieve outstanding success in deal-\ning with attacks from known morphing techniques and\nknown data sources. However, given variations in the mor-\nphing attacks, the performance of supervised MAD solu-\ntions drops signiﬁcantly due to the insufﬁcient diversity and\nquantity of the existing MAD datasets. To address this con-\ncern, we propose a completely unsupervised MAD solution\nvia self-paced anomaly detection (SPL-MAD) by leveraging\nthe existing large-scale face recognition (FR) datasets and\nthe unsupervised nature of convolutional autoencoders. Us-\ning general FR datasets that might contain unintentionally\nand unlabeled manipulated samples to train an autoencoder\ncan lead to a diverse reconstruction behavior of attack and\nbona ﬁde samples. We analyze this behavior empirically to\nprovide a solid theoretical ground for designing our unsu-\npervised MAD solution. This also results in proposing to in-\ntegrate our adapted modiﬁed self-paced learning paradigm\nto enhance the reconstruction error separability between\nthe bona ﬁde and attack samples in a completely unsuper-\nvised manner. Our experimental results on a diverse set\nof MAD evaluation datasets show that the proposed unsu-\npervised SPL-MAD solution outperforms the overall per-\nformance of a wide range of supervised MAD solutions and\nprovides higher generalizability on unknown attacks. Train-\ning codes and pre-trained models are publicly released 1.\n1. Introduction\nFace recognition technique has witnessed remarkable\nprogress in recent years. A variety of face recognition meth-\nods [3, 19] are proposed in the literature and applied in prac-\ntical applications with very high accuracy. However, these\nmethods are vulnerable to several attack [7, 14, 21, 34], one\nof which is the face morphing attack. A morphing attack\nis a face image which is purposefully manipulated to be\nmatched with the probe images of more than one identity.\n1https://github.com/meilfang/SPL-MAD\nAs a result, morphing attack detection (MAD) solutions is\nof crucial importance to building reliable face recognition\nsystems. Conventional single-image and differential MAD\nsolutions [13, 22, 25, 40, 42, 43] require two classes of\ndata, bona ﬁde (i.e., not attack) and morphing attack sam-\nples for training a supervised MAD model. However, this\nrestricts the MAD performance with the size and diversity\nof the training data. Most of the existing MAD datasets\n[11, 13, 44] are limited in diversity and quantity caused by\nsuch as the labor-intensive pair selection, morphing process,\nand the limited of bona ﬁde source data that is of suitable\nproperties (e.g., ICAO compliant [26]) and shareable in a\nprivacy-aware frame. Moreover, due to ethical and legal is-\nsues, only a few MAD datasets [11, 13, 44] are publicly\navailable for the development of MAD solutions. Follow-\ning the lack of diversity of MAD datasets and the possibil-\nity of facing attacks created by unknown methods, the su-\npervised MAD solutions [13, 22, 40, 42, 43] commonly re-\nsults in poor performance generalization on unknown mor-\nphing attacks or data sources. To the best of our knowledge,\nonly one single-image based [8] and one differential based\nMAD method [25] were proposed to detect morphing at-\ntacks as anomalies by training a one-class classiﬁer. Despite\nthe obtained improved performance on the unknown attack\nin comparison to supervised MAD approaches, training the\none-class classiﬁer still relies on the prior knowledge that\nall training samples are known to be bona ﬁde, and thus it\nis not a completely unsupervised approach.\nTo target the lack of large-scale, labelled, and diverse\nMAD datasets, along with the low generalizability of super-\nvised MADs on unknown attacks, we leverage the existing\nlarge-scale face recognition datasets to train our proposed\nunsupervised learning-based model. Most publicly avail-\nable face recognition datasets were collected from the web\nand might consist of unintentionally and unlabeled manipu-\nlated samples. To alleviate this issue, we model our design\nas self-paced learning (SPL) paradigm. Self-paced learning\nparadigm is inspired by the cognitive learning order in hu-\nman curricula, where samples are involved in the training\nphase from easy to hard ones [1]. In this case, training data\narXiv:2208.05787v1  [cs.CV]  11 Aug 2022\nis evaluated and selected automatically based on the train-\ning loss without any prior knowledge of humans. Recently,\nresearchers have investigated the potential of SPL paradigm\n[20, 30] and demonstrated that is signiﬁcant a strong perfor-\nmance gain [51, 53]. Following the properties of the SPL\nparadigm as an effective learning strategy to suppress the\nside effects of noise samples or outliers by adjusting the\nweight of samples. Given the understanding of the limi-\ntations of supervised MAD solutions, we propose an SPL\nparadigm and incorporate it and incorporate it in our unsu-\npervised MAD learning.\nThis work makes the following main contributions: 1)\nWe ﬁrst study the behavior of unsupervised anomaly detec-\ntion through reconstruction error analyses on MAD data to\nensure that our unsupervised MAD solution is developed\non the bases of solid empirical analyses. Our study reveals\nthat morphing attacks are more straightforward to recon-\nstruct than bona ﬁde samples when the reconstruction is\nlearned on general face data; 2) We leverage our above-\nstated observation to present a novel unsupervised MAD so-\nlution via an adapted self-paced anomaly detection, namely\nSPL-MAD. The adapted SPL paradigm proved helpful in\nneglecting the suspicious unlabeled data in training and thus\nenhancing the reconstruction gap between bona ﬁde and at-\ntack samples, leading to improving the generalizability of\nthe MAD model. 3) The experimental results demonstrate\nthat our SPL-MAD solution not only reaches the perfor-\nmance of supervised MAD solutions but also outperforms\nwell-established supervised MAD methods and presents a\nmore generalizable performance over a diverse set of un-\nknown attacks included in eight MAD datasets.\n2. Related work\nA number of studies [22, 45] pointed out that face recog-\nnition systems are vulnerable to morphing attacks.\nTo\ntarget this problem, several MAD solutions [13, 43, 48]\nwere proposed.\nMAD solutions can be categorized into\ntwo groups based on the application scenario requirements:\nsingle-image MAD and differential-MAD, where the lat-\nter requires an investigated image and an additional live\ncapture of the individual [5], which limits its applicabil-\nity in many scenarios.\nIn our case, we focus on the\nsingle-image-based MAD scenario, where only the inves-\ntigated image is analyzed. Most single-image MAD solu-\ntions [13, 22, 40, 42, 43] are based on supervised learning\nthat relies on data annotations. For example, Raghaven-\ndra et al. [43] proposed a handcrafted-feature-based solu-\ntion where textural features were extracted across scale-\nspace and were classiﬁed using collaborative representa-\ntion. Damer et al. [13] proposed a pixel-wise MAD (PW-\nMAD) solution where a network is trained to classify each\npixel of the image into an attack or not, rather than only\none binary label for the whole image. These supervised-\nlearning-based solutions achieved good MAD performance\ntypically on attacks with properties known during training.\nVariations in the attacks strongly effect the MAD perfor-\nmances, such variations can be related to the face morphing\napproach [6, 8, 10, 16, 54], the pairing protocols of mor-\nphed images [12, 15], image compression [32], the source\nof bona ﬁde images [46], and re-digitization of the images\n[23, 40, 42], among other variations. Additionally, to the\nrelative low generalizability of supervised MAD, their op-\ntimal training requires large-scale labelled databases with\nvariation in the attacks, which is very challenging given the\ndata-creating efforts and the legal limitations on using, shar-\ning, and re-using biometric-based personal data [50].\nBesides supervised MAD approaches, a single work [8]\nproposed to detect morphing attacks from single images by\nleveraging the anomaly detection technique, however, with\nvery limited success. Damer et al. [8] studied detecting at-\ntacks as anomalies via one-class classiﬁers. However, the\nperformance of the one-class model on unknown attacks\nwas still low, considering roughly 50% detection error rates\n[8]. Moreover, training the one-class classiﬁer in fact re-\nlies on the prior knowledge that all training samples are as-\nsumed to be bona ﬁde data even if they included contamina-\ntion, and thus it is not a completely unsupervised approach.\nAlso, using a one-class classiﬁer, but for the out of our\nscope differential morphing attack detection that requires\na bona ﬁde (live) image in its operation, a recent work [25]\nproposed a solution that also required bona ﬁde labelled im-\nages for the training.\nIn addition to the lack of large-scale, labelled, and di-\nverse datasets, leading to poor generalizability of the MAD\nmodel, most of the existing morph attack samples in MAD\ndatasets [11, 13, 44] are created based on a small-scale bona\nﬁde samples. This is due to the insufﬁcient identities in suit-\nable (ICAO compliant [26]) and publicly available datasets.\nCompared to the face recognition datasets [52], and even\nface presentation attack (spooﬁng) databases [56], existing\nMAD datasets are hundred times smaller. Besides, only a\nfew datasets (detailed information of eight MAD datasets\ncan be found in Section 4.1) are available for MAD develop-\nment research. To enhance the generalizability on unknown\nmorphing attacks and avoid the need for diverse morphing\ndevelopment databases, we leverage the publicly available\nface recognition datasets and present an unsupervised face\nMAD solution, the SPL-MAD.\n3. Methodology\n3.1. Preliminaries\nConvolutional autoencoder (CAE): Autoencoder (AE)\nis a branch of unsupervised learning techniques and has\nbeen widely used for anomaly detection [47, 57]. AE con-\nsists of an encoder representing the input in a latent do-\n0\n5\n10\n15\n20\n25\nEpoch\n1\n2\n3\n4\n5\n6\n7\n8\nAverage Reconstruction Loss\n0.62\nw/o SPL model on MorGAN-LMA test set\nBona fide\nAttack\n(a)\n0\n5\n10\n15\n20\n25\nEpoch\n3\n4\n5\n6\n7\n8\n9\n10\nAverage Reconstruction Loss\n1.46\nw/ SPL model on MorGAN-LMA test set\nBona fide\nAttack\n(b)\n0\n5\n10\n15\n20\n25\nEpoch\n2\n4\n6\n8\n10\n12\n14\n16\nAverage Reconstruction Loss\n3.26\nw/o SPL model on SMDD test set\nBona fide\nAttack\n(c)\n0\n5\n10\n15\n20\n25\nEpoch\n4\n6\n8\n10\n12\n14\n16\nAverage Reconstruction Loss\n5.07\nw/ SPL model on SMDD test set\nBona fide\nAttack\n(d)\nFigure 1. The curves of reconstruction error on two MAD test set (SMDD [9] and MorGAN-LMA [11], details in Section 4.1) by models\ntrained without SPL and with SPL paradigm. The x-axis refers to the training epoch, and the y-axis is the average reconstruction error of\nall data. The green curve denotes the error of bona ﬁde data, and the red curve presents the error of attack data. It can be seen that attacks\nare easier to reconstruct than bona ﬁde samples, thus resulting in lower reconstruction error, which we leverage in our proposed MAD\nsolution. The SPL paradigm also leads to a higher gap in the reconstruction error between bona ﬁde and attacks.\nmain and a decoder reconstructing the data from this latent\nfeature. Convolutional autoencoder (CAE) is designed by\nstacking convolutional layers, where the encoder (denote\nas fe(., we)) is combined by several convolutional layers\nand the decoder (denote as fd(., wd)) is combined by trans-\nposed convolutional layers. Given i-th input xi in dataset,\nthe CAE fCAE(xi, w) with model parameter w can be for-\nmulated as following:\nˆxi = fCAE(xi, w) = fd(fe(xi, we), wd)\n(1)\nwhere we and wd is the parameters of the encoder and de-\ncoder, respectively. ˆxi is the reconstructed data. To mea-\nsure the reconstruction quality, the most commonly used\nloss function for anomaly detection is mean square error\n(MSE), i.e., LMSE = ∥xi −ˆxi∥2\n2. Thus, the training objec-\ntive of CAE can be constructed as:\nmin\nw\nn\nX\ni=1\nLMSE(fCAE(xi, w), xi).\n(2)\nSelf-paced learning (SPL): we introduce ﬁrst the con-\nventional SPL as preliminaries based on [20, 30]. Given a\ntraining dataset D = {(xi, yi)}n\ni=1 with n samples, where\nxi ∈Rd is the i-th sample and yi is the learning target\n(i.e., ground truth label). In our case, we use a fully con-\nvolutional autoencoder (CAE) to reconstruct the input im-\nage, that is, yi is equal to input xi. A learned model is\ndenoted as f(., w) where w is the model parameter. Let\nL(f(xi, w), yi) denote the loss function that computes the\ncost between estimated data f(xi, w) and target object yi\nof i-th sample.\nThe sample weights is denoted as v =\n[v1, ..., vn]. Then, the objective of SPL can be presented as\na union of a weighted loss term on all samples and a general\nself-paced regularizer imposed on sample weights. The ob-\nject of SPL can be expressed as the following minimization\nproblem:\nmin\nw,v∈[0,1]n E(w, v; λ) =\nn\nX\ni=1\nviL(f(xi, w), yi) + g(λ, vi),\n(3)\nwhere g(λ, vi) is the self-paced regularizer with a penalty\nparameter λ that controls the learning space. Alternative\nsearch strategy (ASS) [30] is generally used for solving Eq.\n3 that alternatively optimizes model weight w and sam-\nple weight v while keeping the other ﬁxed. For example,\ngiven a ﬁxed v, the minimization over w is a weighted\nloss minimization problem which is independent of regu-\nlarizer g(λ, vi). Through such a jointly learning process of\nw and v by ASS with gradually increasing the value of λ,\nmore samples can be automatically included in the training\nprocess from easy to hard in a self-paced manner based on\ntraining losses.\n3.2. Unsupervised SPL-MAD\nReconstruction behaviour exploration: In the context\nof anomaly detection, AE/CAE models trained on normal\ndata are expected to produce higher reconstruction error for\nthe abnormal data than normal data [24]. However, this as-\nsumption does not always hold as the reconstruction behav-\nior of anomaly inputs is unclear when no anomalies exist\nin the training set. Zong et al. [57] observed that abnor-\nmal data obtains somehow lower reconstruction error than\nnormal data. Hence, we will ﬁrst analyze the reconstruc-\ntion behavior in the MAD task by training a CAE on bona\nﬁdes and limited morphed attacks. As discussed in Section\n2, the insufﬁcient number of identities in the existing MAD\ndatasets is one possible reason for the poor generalization\nof MAD performance. To address this issue, we utilize the\nin-the-wild CASIA-WebFace dataset [52] as our assumed\nto be ”mostly normal” samples, considering its diversity in\ncapture environment, sensor, and identity. Speciﬁcally, the\nCASIA-WebFace dataset [52] consists of 494,414 images\nacross 10,575 identities collected from the web and is used\nfor face veriﬁcation and identiﬁcation tasks. Then, we use\nan additional MAD dataset, namely SMDD [9], together\nwith CASIA-WebFace for exploration of reconstruction be-\nhavior. We select the SMDD dataset due to its privacy-\nfriendly property and diversity in identity. The detailed in-\nformation of SMDD is presented later in Section 4.1. To\ninvestigate our conclusions hold on other datasets, we also\ndid the same using the MorGAN-LMA (landmark-based\nmorphs) dataset [11].\nFigure 1 (a) and (c) illustrate the average reconstruction\nerrors on MorGAN-LMA and SMDD test set by a vanilla\nCAE model trained on the unlabeled combined dataset in\neach epoch.\nIt can be clearly observed that the average\nreconstruction errors of attacks are lower than bona ﬁdes.\nMeanwhile, the error gap between bona ﬁdes and attacks\nconsistently persists as the training continues. This ﬁnd-\ning is in stark contrast to the assumption [24] in general\nanomaly detection, indicating that morph attacks are easier\nto reconstruct. The possible reasons are: 1) the possible\nartefacts resulting from various morphing processes induce\nan ambiguity in some of the image details, which might\nmake the image similar to a wider range of reconstructions,\nas they make it similar to faces of multiple identities, and\nthus result in a lower reconstruction error. 2) the blending\nartefacts existing in some morphed images might be eas-\nier to decode (less sharp information) from attack encoding.\nOverall, such feasible error gaps prove that the morphs can\nbe detected in an unsupervised manner even if the model is\ntrained on data, including polluted attack data.\nSPL-MAD: Despite the error gaps between bona ﬁdes\nand attacks observed in Figure 1, the gaps become gradually\nsmaller as the training continues. This is probably caused\nby the model learning anomalous patterns from the polluted\nattack samples as the training continues and thus leading to\na degraded ability of the model to remove anomalies. To\naddress this concern, we propose to incorporate the SPL\nparadigm into the training, aiming to continually remove the\nsuspicious attacks in the training phase. As we introduced\nbefore, the SPL paradigm consists of a problem-speciﬁc\nweighted term on all samples and an SPL regularizer on\nsample weights. Due to such ability of weight adjustment,\nSPL can enhance the robustness and generalizability of the\nmodel in polluted data. Therefore, we incorporate the SPL\nparadigm into our unsupervised MAD learning by assigning\nsmaller weights to suspicious morphed attacks. Our SPL-\nMAD solution is deﬁned by:\nmin\nw,v∈[0,1]n\nn\nX\ni=1\nviLMSE(fCAE(xi, w), xi) + g(λ, vi),\n(4)\nwhere fCAE(xi, w) is the reconstructed image and LMSE(.)\nis the reconstruction loss (MSE in our case). The Eq. 4 is\noptimized by ASS. First, when sample weight v ∈[0, 1]n\nin the regularizer is ﬁxed, the minimization over w is a\nweighted loss minimization problem and the optimal model\nw∗is determined as:\nw∗= argmin\nw\nn\nX\ni=1\nviLMSE(fCAE(xi, w), xi).\n(5)\nThe Eq.\n5 is solved by gradient descent in the training\nphase. Alternatively, given model parameter w, the optimal\nweight of the i-th sample v∗\ni is computed by:\nv∗\ni = argmin\nvi∈[0,1]\nn\nX\ni=1\nviLMSE(fCAE(xi, w), xi) + g(λ, vi). (6)\nBased on the observation in Figure 1 that lower reconstruc-\ntion losses are achieved by morphs than by bona ﬁdes at the\nbeginning of training and a smaller difference between bona\nﬁdes and attacks in the later epochs of training, we adapted\ngeneral SPL rules to meet our needs: 1) v∗\ni is monotonically\nincreasing w.r.t Li, the reconstruction loss of the i-th sam-\nple, which guides the model to select potential bona ﬁde\nsamples with larger losses in favor of suspicious morphs\n(morphs or images that have properties similar to a mor-\nphed image) with smaller losses. 2) v∗\ni is monotonically\ndecreasing w.r.t λ, which means that a larger λ has a higher\ntolerance to the losses (i.e., smaller sample weight) and can\nremove more suspicious samples. 3) g(λ, vi) is convex w.r.t\nvi ∈[0, 1] to ensure the soundness of SPL regularizer for\noptimization. Therefore, we selected a linear SPL regular-\nizer proposed in [28] and modiﬁed its close-formed optimal\nsolution formulation as:\nv∗\ni =\n(\n1 −λ\nLi ,\nLi > λ\n0,\nLi ≤λ ,\n(7)\nThus, Eq. 7 helps our unsupervised training process: the\nsample weight of data with a loss smaller than λ (suspi-\ncious samples) is set to zero; the data with a large loss is\nassigned with a relatively large sample weight, which en-\ncourages CAE model to focus on the learning of potential\nof normal (assumed to be bona ﬁde) samples.\nFurthermore, we determine a self-adaptive λ by consid-\nering the reconstruction errors in each training step. We\ngradually increase the λ to the maximum λmax = µ(s) −\nσ(s) with the increasing training step s by following the\nequation:\nλ = min(µ(s) −(m −r · s) · σ(s),\nλmax)\n(8)\nwhere µ(s) and σ(s) denote the mean and standard devia-\ntion of all data in the current training step s. m is the con-\nstant initial standard deviation range, and r is the constant\nshrink rate. To be consistent with our modiﬁed SPL learn-\ning needs, r is usually set to a small value, where only a\nfew easy samples with very low reconstruction losses are\nremoved at the early training stage. In our case, we assume\nthat the majority of (or all) training samples are normal (as-\nsumed to be bona ﬁde) samples. Therefore, the coefﬁcient\n(m −r · s) of σ(s) is limited to minimum 1 (i.e., 1 · σ(s) in\nλmax) to maintain the majority of the data. Our SPL-MAD\nparadigm is presented in Algorithm 1.\nAlgorithm 1: The proposed SPL-MAD algorithm\nInput : Input unlabeled data D = {xi}n\ni=1, a CAE\nmodel with parameters w, training step\nsize s\nOutput: Updated model parameters w\n1 Initialize model parameter w, sample weights v∗\nand s = 0\n2 repeat\n3\nRandomly sample a batch size of data from D;\n4\nCalculate the reconstruction loss LMSE by\nforward propagation ;\n5\nCompute λ based on step-size s by Eq. 8 ;\n6\ns = s + 1 ;\n7\nUpdated sample weights v∗by Eq. 7 ;\n8\nUpdate model parameters w∗by Eq. 5 ;\n9 until convergence;\n10 return w∗\n4. Experimental setup\n4.1. Datasets\nTo evaluate the generalizability of our MAD solutions\non unknown attacks, we use eight publicly available MAD\ndatasets:\nLMA-DRD (PS) [13], LMA-DRD (D) [13],\nMorGAN-LMA [11], MorGAN-GAN [11], FRLL-Morphs\n[44], FERET-Morphs [44], FRGC-Morphs [44], and one\nsynthetic morphing attack detection development dataset\n(SMDD) [9]. It should be noted that our model is trained\non CASIA-WebFace dataset [52] which is used for face\nrecognition tasks, and does not contain any information on\nthe images being manipulated or not (morphed, beautiﬁed,\nre-digitized, and so on.). Furthermore, a privacy-friendly\nSMDD training set is added to CASIA-WebFace to explore\nthe effect of unlabeled attack contamination in the unsuper-\nvised training phase.\nLMA-DRD (D) and LMA-DRD (PS): The bona ﬁde\nsamples in LMA-DRD (D) and LMA-DRD (PS) are se-\nlected from VGGFace2 dataset [4] and morphed attack sam-\nples are created by OpenCV morphing [33] and following\nparametrization in [41] into two ways: ”D” refers to digital\nand ”PS” refers to re-digitized (print and scan). In our case,\nwe only use the test set for unknown MAD, which consists\nof 123 bona ﬁde and 88 morphs, for each D and PS.\nMorGAN-LMA and MorGAN-GAN: The bona ﬁde\nsamples in MorGAN are selected from CelebA [31] and\nmorphs are created by either OpenCV landmark-based mor-\nphing [33] (denoted as MorGAN-LMA) or GAN-based so-\nlution presented in [11] (denoted as MorGAN-GAN). Each\nof the test sets contains 750 bona ﬁdes and 500 morphs.\nNote that the generated faces by the GAN-based solution in\nthis dataset are of the relatively low resolution of 64 × 64\npixels.\nFRLL-, FERET-, and FRGC-Morphs: The morph\nsamples in the FRLL-Morphs [44] dataset is generated\nbased on the publicly available Face Research London Lab\ndataset [18]. FRLL-Morphs contains ﬁve different morph-\ning methods: OpenCV [33], FaceMorpher [39], StyleGAN2\n[29, 49], and WebMorpher [17], and AMSL [35]. FRLL-\nMorphs is created for the evaluation of attack vulnerabil-\nity and MAD performance (i.e., containing only a test set)\nand is thus suitable in our case for evaluating our solution.\nEach test set contains 204 bona ﬁdes and 1,222 attacks (per\nmorphing type).\nThe FERET-Morphs [44] datasets con-\ntains bona ﬁde images from FERET [37] and three types of\nmorph attacks generated from OpenCV [33], FaceMorpher\n[39], and StyleGAN2 [29, 49]. The test set contains 1,361\nbona ﬁdes and between 523 and 525 attacks per morph type.\nSimilarly, the FRGC-Morphs [44] datasets contains bona\nﬁde images from FRGC v2.0 [38] and three types of morph\nattacks generated from OpenCV [33], FaceMorpher [39],\nand StyleGAN2 [29, 49]. The test set contains 3,069 bona\nﬁdes and between 961 and 963 attacks per morph type.\nSMDD: The SMDD [9] dataset is a synthetic-based\nMAD database that bona ﬁde images are created by\nStyleGAN2-ADA [29, 49] and morph attacks are gener-\nated based on such bona ﬁde by using OpenCV morph-\ning technique [33]. The training set and test set contain\n25,000 bona ﬁdes and 15,000 morphs, each. Their exten-\nsive results showed that SMDD could be served as an effec-\ntive training set for the MAD model, even when the MAD\nmodel encounters attacks created with unknown morphing\ntechniques or data sources. Moreover, by considering the\nprivacy-friendly characteristic and diversity of the SMDD\ndataset, we use the SMDD training set for further attack\ncontamination exploration.\nFor the datasets that were also explored as the training\ndata for supervised MAD methods in the experiments, only\ntheir respective train set was used for training, and only their\ntest sets were used for evaluation. Both sets are identity-\ndisjoint in all the datasets.\n4.2. Implementation details\nWe use a CAE model consisting of seven convolutional\nblocks as our SPL-MAD backbone. We use a large-scale\nface dataset CASIA-WebFace [52] for face veriﬁcation and\nidentiﬁcation task to train the CAE model. All the databases\nare cropped using landmark points obtained from Multi-\ntask Cascaded Convolutional Networks (MTCNN) [55]. All\ncrops are resized to 224 × 224 to match the network input\nsize. Overall, the input image size is 224 × 224 × 3 for\ntraining. In the training phase, we use the stochastic gradi-\nent descent (SGD) optimizer with a momentum of 0.9 and\na weight decay of 5e-4, and the initial learning rate is 1e-\n5. The degradation of the learning rate is controlled by an\nexponential learning scheduler with a gamma of 0.98. The\nbatch size in our experiment is 64, and the number of train-\ning epochs is 25. The ﬁrst ﬁve epoch is used for warm-up\nwithout SPL. The implementation is based on in PyTorch\ntoolbox [36]. The initial standard deviation range m and\nthe shrink rate r are set to 4 and 5e-3, respectively.\n4.3. Evaluation metrics\nWe follow the standard deﬁnitions in ISO/IEC 30107-\n3 [27] to evaluate MAD performance, that is, Attack Pre-\nsentation Classiﬁcation Error Rate (APCER) and Bona ﬁde\npresentation Classiﬁcation Error Rate (BPCER). APCER is\nthe proportion of attack samples misclassiﬁed as bona ﬁdes,\nand BPCER is the proportion of bona ﬁde samples misclas-\nsiﬁed as attacks. To report the overall MAD performance\nand for comparison with other well-established MAD so-\nlutions, we report the equal error rate (EER) value where\nAPCER and BPCER are equal. Furthermore, we plot re-\nceiver operating characteristic (ROC) curves where the x-\naxis is APCER, and the y-axis is (1-BPCER) at different op-\neration points to give a visual evaluation on a wider range.\n5. Results\nIn this section, we ﬁrst analyze the experimental results\nof our developed SPL-MAD approach from two aspects:\nthe contribution of the adapted SPL paradigm and the ro-\nbustness of our approach on morphing attack contamina-\ntion. Then, to put the performance of our unsupervised solu-\ntion in the perspective of the supervised MAD performance,\nwe compare our experimental results with three diverse su-\npervised and well-performing MAD methods trained on ﬁve\ndifferent sets of training data, totalling 15 supervised MAD\nsolutions.\n5.1. Ablation study on SPL paradigm\nTo illustrate the effectiveness of the SPL paradigm\nwithin our SPL-MAD, we trained an unsupervised CAE\nmodel without SPL (denoted as baseline) and with SPL (i.e.,\nSPL-MAD), respectively. Table 1 shows the comparison re-\nsults on various test sets. The lowest EER values on each\ntraining data protocol (i.e., CASIA-WebFace and CASIA-\nWebFace with SMDD [9]) are shown in bold, respectively.\nFrom the results, we can observe that: 1) Including SPL in\nthe training paradigm (i.e, SPL-MAD) consistently outper-\nformed the training without SPL when training on uncon-\ntaminated data. 2) Similarly, including SPL in the training\nparadigm also outperformed the model trained without SPL\non most test sets when the training data was contaminated\nwith the SMDD data. 3) Overall, a model trained with the\nSPL paradigm yields better average performance than with-\nout SPL. For example, EER is decreased from 39.73% ob-\ntained by the baseline model to 21.13% by SPL-MAD and\ndecreased from 20.45% to 18.95% in two training data pro-\nTest data\nTrain data\nCASIA-Web\nCASIA-Web + SMDD\n-\nSPL\n-\nSPL\nLMA-DRD\nD\n31.81\n18.18\n22.73\n20.45\nPS\n26.14\n22.73\n29.54\n29.54\nMorGAN\nLMA\n42.95\n14.46\n15.86\n15.06\nGAN\n61.04\n40.56\n44.78\n39.35\nFRLL-Morphs\nOpenCV\n35.36\n3.63\n4.71\n5.78\nFaceMorpher\n31.66\n2.98\n2.87\n4.67\nStyleGAN2\n34.69\n15.14\n12.11\n12.92\nWebMorph\n37.51\n12.29\n11.22\n15.72\nAMSL\n36.22\n11.22\n8.87\n12.09\nFERET-Morphs\nOpenCV\n45.24\n32.13\n36.14\n30.21\nFaceMorpher\n39.81\n27.69\n36.14\n25.76\nStyleGAN2\n41.52\n32.57\n34.28\n28.95\nFRGC-Morphs\nOpenCV\n48.23\n36.11\n21.62\n19.54\nFaceMorpher\n47.24\n23.99\n19.67\n18.42\nStyleGAN2\n46.62\n36.79\n19.63\n15.57\nSMDD\n29.59\n7.60\n7.01\n9.19\nAverage performance\n39.73\n21.13\n20.45\n18.95\nTable 1. The MAD performance in terms of EER (%) for the abla-\ntion study on SPL paradigm and on data contamination. The bold\nnumbers indicate the lowest EER values in two training data proto-\ncols: face recognition dataset CASIA-WebFace [52], and CASIA-\nWebFace with a MAD dataset SMDD [9]. ’-’ refers to the base-\nline model training without SPL paragiam, and ’SPL’ refers to the\ntraining with SPL.\ntocols, respectively. Such observations point out the sig-\nniﬁcant contribution of the SPL paradigm to the improve-\nment of the MAD performance. In addition to quantitative\nresults, the larger reconstruction error gaps between bona\nﬁdes and attacks in Figure 1 support our conclusion and our\nmotivation behind using the SPL paradigm. As shown in\nFigure 1, the SPL-MAD model achieved a larger gap be-\ntween attack and bona ﬁde than the baseline model on both\nMorGAN-LMA (error gap increases from 0.62 to 1.46) and\nSMDD dataset (error gap increases from 3.26 to 5.07). Fur-\nthermore, we plot the ROC curves for additional visual anal-\nysis. The ROC curves in Figure 2 show that the blue and\nred curves of SPL-MAD model are placed above the green\nand yellow curves of the baseline models (no SPL) in most\ncases. Only for the FRLL-based attacks, and only in the\ncase of contaminated training data, the beneﬁt of the SPL\nis not as clear as both with SPL and without SPL training\nparadigms achieve very close performances. These graph-\nical observations are consistent with the previous quantita-\ntive ﬁndings. In general, the SPL module plays an impor-\ntant role in our unsupervised MAD learning and enhances\nthe overall improvement of MAD performance, as theorized\nearlier in Section 3.\n5.2. Ablation study on data contamination\nTo increase the diversity of bona ﬁde identities and thus\nenhance the generalizability of MAD models, we leverage\nthe publicly available face recognition datasets. However,\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on LMA-DRD (D)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on MorGAN-LMA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on MorGAN-GAN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on LMA-DRD( PS)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on SMDD\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on FaceMorpher\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on OpenCV\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on StyleGAN2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on FaceMorpher\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on OpenCV\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on StyleGAN2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on AMSL\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on FaceMorpher\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on OpenCV\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on StyleGAN2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAPCER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nBPCER\nROC on WebMorph\nTrained on CASIA-WebFace\nTrained on CASIA-WebFace with SPL-MAD\nTrained on CASIA-WebFace + SMDD\nTrained on CASIA-WebFace + SMDD with SPL-MAD\nFRGC-Morphs\nFERER-Morphs\nFRLL-Morphs\nFigure 2. ROC curves achieved on different test sets and four different training settings. In most cases, the curves indicate better MAD\nperformance when including our modiﬁed SPL paradigm and when including data contamination by SMDD.\nmost face recognition datasets were collected in the wild\nand might unintentionally include manipulated images. As\na result, we provide two training data protocols by adding\ncontaminated attacks to further demonstrate the robustness\nand effectiveness of our method trained purely on unlabeled\ndata. One training set is a pure face recognition dataset\nCASIA-WebFace [52], and the other one is a combined\ntraining set consisting of the face recognition dataset and\na face MAD training set (including both bona ﬁdes and\nattacks), that is CASIA-WebFace [52] and SMDD [9], re-\nspectively. The ratio of bona ﬁde to attack data in the com-\nbined dataset is around 35:1. We stress again that our un-\nsupervised attack does not consider any labels during train-\ning, even for the contamination data. Table 1 shows the\ncomparison results in terms of the EER value obtained by\nthe model trained on both training data protocols. It can\nbe observed that baseline and SPL-MAD model trained\nwith contaminated data gains 19.33% and 2.187% over-\nall performance improvement over models trained on un-\ncontaminated CASIA-WebFace. The reason behind such\nimprovement might be that more data is included in the\ntraining phase, and the majority of training data are still\nbona ﬁdes. This observation suggests that our unsupervised\nMAD method sees a steady rise in the performance irre-\nspective of the attack contamination. Moreover, the ROC\ncurves in Figure 2 conﬁrms our quantitative ﬁndings and\nindicate that our unsupervised model proves to be effective\neven under data contamination scenario in most cases.\n5.3. Comparison to supervised MAD\nFurthermore, to put our unsupervised SPL-MAD in the\nperspective of the supervised MAD performances, Table 2\npresents the performance comparison of our method (i.e.,\nSPL-MAD trained on contaminated data) to other three su-\npervised single image MAD solutions [9, 13, 43].\nThe\nresults under the intra-dataset scenario are marked with ∗\n(which is expected to perform unfairly good), and the best\naverage performance (lowest EER) among all models is\nin bold. It should be noted that we only consider the re-\nsults in the cross-dataset setting when calculating the av-\nerage performance for a fair comparison.\nFrom the re-\nsults, we can conclude that our unsupervised SPL-MAD\nachieved comparable results with supervised methods in\nmost test set cases. When considering the average perfor-\nmance on all datasets, our method obtains the lowest EER\nvalue (18.95%), where the second-lowest EER is 20.42%.\nThe improvement in MAD performance can be attributed\nto the large-scale training data enabled by the unsuper-\nvised nature of the approach and the adaption of the SPL\nTest data\nTrain data\nSupervised\nUnsupervised\nMixFaceNet\nPW-MAD\nInception\nSPL-MAD\nD\nPS\n-LMA\n-GAN\nSMDD\nD\nPS\n-LMA\n-GAN\nSMDD\nD\nPS\n-LMA\n-GAN\nSMDD\n(Our)\nLMAD-DRD\nD\n15.68∗\n18.03\n17.06\n25.01\n19.42\n20.8∗\n25.1\n22.34\n40.21\n17.06\n7.64∗\n17.06\n15.68\n50.77\n15.11\n20.45\nPS\n21.77\n18.44∗\n27.05\n27.05\n23.72\n26.48\n23.72∗\n29.41\n44.11\n20.39\n11.37\n12.75∗\n22.34\n38.42\n19.01\n29.54\nMorGAN\nLMA\n39.42\n22.89\n10.61∗\n46.42\n30.12\n34.2\n34.14\n9.71∗\n34.37\n27.31\n38.55\n31.73\n8.43∗\n40.16\n28.51\n15.06\nGAN\n53.01\n50.44\n42.57\n24.9∗\n42.64\n52.04\n46.59\n42.8\n8.84∗\n43.78\n50.84\n38.79\n27.41\n0.4∗\n44.34\n39.35\nFRLL-Morphs\nOpenCV\n8.82\n13.22\n8.91\n17.66\n4.39\n17.33\n15.69\n13.96\n45.59\n2.42\n13.72\n10.76\n6.86\n55.89\n5.38\n5.78\nFaceMorpher\n7.80\n10.97\n7.34\n15.65\n3.87\n13.88\n15.14\n10.92\n44.57\n2.20\n16.62\n15.81\n6.32\n66.14\n3.17\n4.67\nStyleGAN2\n20.07\n15.29\n13.41\n23.51\n8.89\n29.97\n27.64\n18.11\n48.53\n16.64\n37.24\n19.58\n20.56\n55.03\n11.37\n12.92\nWebMorph\n25.97\n29.04\n20.61\n30.39\n12.35\n33.78\n28.51\n35.75\n52.43\n16.65\n57.38\n58.32\n30.88\n77.42\n9.86\n15.72\nAMSL\n24.53\n27.59\n19.24\n30.03\n15.18\n36.25\n32.95\n34.38\n48.52\n15.18\n49.02\n61.44\n9.80\n86.49\n10.79\n12.09\nFERET-Morphs\nOpenCV\n28.12\n32.19\n31.57\n33.86\n31.74\n37.27\n45.29\n34.27\n43.11\n39.93\n6.39\n7.23\n42.12\n13.62\n59.32\n30.21\nFaceMorpher\n22.57\n29.48\n27.9\n31.81\n23.69\n35.16\n44.3\n28.24\n40.4\n29.41\n5.17\n6.91\n36.53\n18.36\n46.94\n25.76\nStyleGAN2\n29.57\n29.02\n35.46\n39.41\n39.85\n44.25\n45.3\n29.7\n42.47\n47.2\n9.03\n7.12\n35.29\n15.09\n60.05\n28.95\nFRGC-Morphs\nOpenCV\n23.81\n25.04\n31.62\n21.11\n20.67\n57.06\n48.6\n29.74\n53.55\n26.45\n34.32\n13.65\n36.17\n59.66\n19.63\n19.54\nFaceMorpher\n22.83\n23.54\n29.38\n19.98\n18.10\n56\n50.7\n30.49\n51.61\n23.4\n34.96\n19.71\n35.1\n56.91\n16.06\n18.42\nStyleGAN2\n32.71\n28.68\n21.7\n21.95\n11.62\n37.38\n38.42\n16.43\n26.62\n14.32\n41.14\n25.85\n36.19\n47.03\n15.26\n15.57\nSMDD\n10.34\n9.26\n5.11\n11.69\n2.51∗\n15.7\n13.45\n7.82\n36.25\n0.79∗\n6.42\n21.88\n12.49\n38.38\n0.42∗\n9.19\nAverage performance\n24.76\n24.31\n22.60\n26.37\n20.42\n35.12\n34.12\n25.62\n43.49\n22.82\n27.48\n23.72\n24.92\n49.17\n24.32\n18.95\nTable 2. The MAD performance in terms of EER (%) in comparison with results of three supervise-learning-based and well-performing\nMAD solutions: MixFaceNet [2], PW-MAD [13], Inception [43] trained on ﬁve datasets. The results under intra-dataset evaluation are\nmarked with ∗, and the best MAD performance among 16 MAD models is denoted in bold. Note that for a fair comparison, the intra-\ndataset results are neglected when calculating the average performance, and our unsupervised SPL-MAD results are reported under the\ndata contamination scenario.\nFigure 3. The box and whisker plot of the performance variation\nper each model. Each box represents a model trained on different\ndatasets. The x-axis is the trained dataset (D, PS, LMA, GAN, and\nS represent LMA-DRD (D), LMA-DRD (PS), MorGAN-LMA,\nMorGAN-GAN, SMDD, and an ”our” unsupervised SPL-MAD\ntrained on a combined set of CASIA-WebFace and SMDD). Four\ncolors indicate the used MAD models, i.e., MixFaceNet, PW-\nMAD, Inception, and our proposed SPL-MAD. The y-axis is the\nEER (%) value, and the red line within the box refers to the mean\nvalue of each model on all test sets (as in table 2), the box range\nrepresents the standard deviation, and the ”o” signs are extreme\noutliers. Note the low average EER value of our SPL-MAD, along\nwith its low deviation and lack of extreme outlier performances.\nparadigm. In Figure 3, we plot the mean and standard de-\nviation of the EER values achieved by our proposed SPL-\nMAD, and the reported 15 supervised MADs on all the test-\ning datasets (excluding intra-dataset tests). The plot stresses\nthat the average MAD performance of the proposed ap-\nproach is comparable and better than the supervised meth-\nods. However, more importantly, as expected from an unsu-\npervised approach, it performs more consistently (low de-\nviation with no extreme outliers) than supervised methods\nthat commonly fail when facing unknown morphing attacks.\n6. Conclusion\nIn this work, we proposed a novel completely unsu-\npervised MAD solution via self-paced anomaly detection\n(SPL-MAD), which beneﬁts from the unsupervised nature\nof autoencoders training and the property of self-paced\nlearning that automatically evaluates the training data with-\nout any prior knowledge. First, to address the lack of di-\nversity and quantity of MAD datasets, we leverage the ex-\nisting large-scale face recognition datasets to train our un-\nsupervised model. In addition, we build a solid theoreti-\ncal foundation for designing our unsupervised solution by\nempirically analyzing the reconstruction behavior on MAD\ndata. This also led us to propose integrating our adaptive\nself-learning paradigm to improve the separability of re-\nconstruction errors between bona ﬁde and attack samples\nin a fully unsupervised manner. The experimental results\ndemonstrated on a diverse set of MAD data indicated the\nhigher generalizability of our unsupervised SPL-MAD so-\nlution in comparison to a wide range of supervised MAD\nsolutions in dealing with morphing attacks created from un-\nknown morphing techniques or data sources.\nAcknowledgment:\nThis research work has been funded\nby the German Federal Ministry of Education and Research\nand the Hessen State Ministry for Higher Education, Re-\nsearch and the Arts within their joint support of the National\nResearch Center for Applied Cybersecurity ATHENE.\nReferences\n[1] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Cur-\nriculum learning.\nIn Proceedings of the 26th Annual In-\nternational Conference on Machine Learning, ICML 2009,\nMontreal, Quebec, Canada, June 14-18, 2009, pages 41–48,\n2009. 1\n[2] F. Boutros, N. Damer, M. Fang, F. Kirchbuchner, and A. Kui-\njper. Mixfacenets: Extremely efﬁcient face recognition net-\nworks. In International IEEE Joint Conference on Biomet-\nrics, IJCB 2021, Shenzhen, China, August 4-7, 2021, pages\n1–8. IEEE, 2021. 8\n[3] F. Boutros, N. Damer, F. Kirchbuchner, and A. Kuijper. Elas-\nticface: Elastic margin loss for deep face recognition.\nIn\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion Workshops, CVPR Workshops 2022, page 1, 2022. 1\n[4] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman.\nVggface2: A dataset for recognising faces across pose and\nage. In 13th IEEE International Conference on Automatic\nFace & Gesture Recognition, FG 2018, Xi’an, China, May\n15-19, 2018, pages 67–74. IEEE Computer Society, 2018. 5\n[5] N. Damer, V. Boller, Y. Wainakh, F. Boutros, P. Terh¨orst,\nA. Braun, and A. Kuijper.\nDetecting face morphing at-\ntacks by analyzing the directed distances of facial landmarks\nshifts. In Pattern Recognition - 40th German Conference,\nGCPR 2018, Stuttgart, Germany, October 9-12, 2018, Pro-\nceedings, volume 11269 of Lecture Notes in Computer Sci-\nence, pages 518–534. Springer, 2018. 2\n[6] N. Damer, F. Boutros, A. M. Saladie, F. Kirchbuchner, and\nA. Kuijper. Realistic dreams: Cascaded enhancement of gan-\ngenerated images with an example in face morphing attacks.\nIn BTAS, pages 1–10. IEEE, 2019. 2\n[7] N. Damer and K. Dimitrov. Practical view on face presenta-\ntion attack detection. In BMVC. BMVA Press, 2016. 1\n[8] N. Damer, J. H. Grebe, S. Zienert, F. Kirchbuchner, and\nA. Kuijper. On the generalization of detecting face morphing\nattacks as anomalies: Novelty vs. outlier detection. In BTAS,\npages 1–5. IEEE, 2019. 1, 2\n[9] N. Damer, C. A. F. L´opez, M. Fang, N. Spiller, M. V. Pham,\nand F. Boutros. Privacy-friendly synthetic data for the de-\nvelopment of face morphing attack detectors. In IEEE Con-\nference on Computer Vision and Pattern Recognition Work-\nshops, CVPR Workshops 2022, page 1, 2022. 3, 5, 6, 7\n[10] N. Damer,\nK. B. Raja,\nM. S¨ußmilch,\nS. Venkatesh,\nF. Boutros, M. Fang, F. Kirchbuchner, R. Ramachandra, and\nA. Kuijper. Regenmorph: Visibly realistic GAN generated\nface morphing attacks by attack re-generation. In Advances\nin Visual Computing - 16th International Symposium, ISVC\n2021, Virtual Event, October 4-6, 2021, Proceedings, Part I,\nvolume 13017 of Lecture Notes in Computer Science, pages\n251–264. Springer, 2021. 2\n[11] N. Damer, A. M. Saladie, A. Braun, and A. Kuijper. Mor-\nGAN: Recognition vulnerability and attack detectability of\nface morphing attacks created by generative adversarial net-\nwork. In BTAS, pages 1–10. IEEE, 2018. 1, 2, 3, 4, 5\n[12] N. Damer, A. M. Saladie, S. Zienert, Y. Wainakh, P. Terh¨orst,\nF. Kirchbuchner, and A. Kuijper. To detect or not to detect:\nThe right faces to morph. In ICB, pages 1–8. IEEE, 2019. 2\n[13] N. Damer, N. Spiller, M. Fang, F. Boutros, F. Kirchbuchner,\nand A. Kuijper. PW-MAD: pixel-wise supervision for gener-\nalized face morphing attack detection. In Advances in Visual\nComputing - 16th International Symposium, ISVC 2021, Vir-\ntual Event, October 4-6, 2021, Proceedings, Part I, volume\n13017 of Lecture Notes in Computer Science, pages 291–\n304. Springer, 2021. 1, 2, 5, 7, 8\n[14] N. Damer, Y. Wainakh, V. Boller, S. von den Berken,\nP. Terh¨orst, A. Braun, and A. Kuijper. Crazyfaces: Unas-\nsisted circumvention of watchlist face identiﬁcation.\nIn\nBTAS, pages 1–9. IEEE, 2018. 1\n[15] N. Damer, S. Zienert, Y. Wainakh, A. M. Saladie, F. Kirch-\nbuchner, and A. Kuijper. A multi-detector solution towards\nan accurate and generalized detection of face morphing at-\ntacks. In 22th International Conference on Information Fu-\nsion, FUSION 2019, Ottawa, ON, Canada, July 2-5, 2019,\npages 1–8. IEEE, 2019. 2\n[16] L. Debiasi, N. Damer, A. M. Saladie, C. Rathgeb, U. Scher-\nhag, C. Busch, F. Kirchbuchner, and A. Uhl. On the detec-\ntion of gan-based face morphs using established morph de-\ntectors. In Image Analysis and Processing - ICIAP 2019 -\n20th International Conference, Trento, Italy, September 9-\n13, 2019, Proceedings, Part II, volume 11752 of Lecture\nNotes in Computer Science, pages 345–356. Springer, 2019.\n2\n[17] L. DeBruine. debruine/webmorph: Beta release 2. Jan. 2018.\n5\n[18] L. DeBruine and B. Jones. Face research lab london set, May\n2017. 5\n[19] J. Deng, J. Guo, N. Xue, and S. Zafeiriou. Arcface: Ad-\nditive angular margin loss for deep face recognition.\nIn\nCVPR, pages 4690–4699. Computer Vision Foundation /\nIEEE, 2019. 1\n[20] Y. Fan, R. He, J. Liang, and B. Hu. Self-paced learning:\nAn implicit regularization perspective.\nIn Proceedings of\nthe Thirty-First AAAI Conference on Artiﬁcial Intelligence,\nFebruary 4-9, 2017, San Francisco, California, USA, pages\n1877–1883, 2017. 2, 3\n[21] M. Fang, N. Damer, F. Kirchbuchner, and A. Kuijper. Real\nmasks and spoof faces: On the masked face presentation at-\ntack detection. Pattern Recognit., 123:108398, 2022. 1\n[22] M. Ferrara, A. Franco, and D. Maltoni. On the effects of im-\nage alterations on face recognition accuracy. In T. Bourlai,\neditor, Face Recognition Across the Imaging Spectrum,\npages 195–222. Springer, 2016. 1, 2\n[23] M. Ferrara, A. Franco, and D. Maltoni. Face morphing detec-\ntion in the presence of printing/scanning and heterogeneous\nimage sources. IET Biometrics, 10(3):290–303, 2021. 2\n[24] M. Hasan, J. Choi, J. Neumann, A. K. Roy-Chowdhury,\nand L. S. Davis. Learning temporal regularity in video se-\nquences. In 2016 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2016, Las Vegas, NV, USA, June\n27-30, 2016, pages 733–742, 2016. 3, 4\n[25] M. Ibsen, L. J. Gonzalez-Soler, C. Rathgeb, P. Drozdowski,\nM. Gomez-Barrero, and C. Busch. Differential anomaly de-\ntection for facial images. In 2021 IEEE International Work-\nshop on Information Forensics and Security (WIFS), pages\n1–6, 2021. 1, 2\n[26] International Civil Aviation Organization, ICAO. Machine\nreadable passports – part 9 – deployment of biometric iden-\ntiﬁcation and electronic storage of data in eMRTDs. Civil\nAviation Organization (ICAO), 2015. 1, 2\n[27] International Organization for Standardization. ISO/IEC DIS\n30107-3:2016: Information Technology – Biometric presen-\ntation attack detection – P. 3: Testing and reporting, 2017.\n6\n[28] L. Jiang, D. Meng, T. Mitamura, and A. G. Hauptmann. Easy\nsamples ﬁrst: Self-paced reranking for zero-example multi-\nmedia search. In Proceedings of the ACM International Con-\nference on Multimedia, MM ’14, Orlando, FL, USA, Novem-\nber 03 - 07, 2014, pages 547–556. ACM, 2014. 4\n[29] T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and\nT. Aila. Training generative adversarial networks with lim-\nited data.\nIn Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual, 2020. 5\n[30] M. P. Kumar, B. Packer, and D. Koller. Self-paced learn-\ning for latent variable models. In Advances in Neural Infor-\nmation Processing Systems 23: 24th Annual Conference on\nNeural Information Processing Systems 2010. Proceedings\nof a meeting held 6-9 December 2010, Vancouver, British\nColumbia, Canada, pages 1189–1197, 2010. 2, 3\n[31] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face at-\ntributes in the wild. In 2015 IEEE International Conference\non Computer Vision, ICCV 2015, Santiago, Chile, Decem-\nber 7-13, 2015, pages 3730–3738. IEEE Computer Society,\n2015. 5\n[32] A. Makrushin, T. Neubert, and J. Dittmann. Automatic gen-\neration and detection of visually faultless facial morphs. In\nVISIGRAPP (6: VISAPP), pages 39–50. SciTePress, 2017. 2\n[33] S. Mallick.\nFace morph using opencv — c++ / python.\nLearnOpenCV, 1(1), 2016. 5\n[34] F. V. Massoli, F. Carrara, G. Amato, and F. Falchi. Detection\nof face recognition adversarial attacks. Comput. Vis. Image\nUnderst., 202:103103, 2021. 1\n[35] T. Neubert, A. Makrushin, M. Hildebrandt, C. Kraetzer, and\nJ. Dittmann. Extended stirtrace benchmarking of biometric\nand forensic qualities of morphed face images. IET Biomet-\nrics, 7(4):325–332, 2018. 5\n[36] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,\nG. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,\nA. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison,\nA. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and\nS. Chintala. Pytorch: An imperative style, high-performance\ndeep learning library.\nIn H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, edi-\ntors, Advances in Neural Information Processing Systems 32,\npages 8024–8035. Curran Associates, Inc., 2019. 6\n[37] P. Phillips, H. Wechsler, J. Huang, and P. J. Rauss. The feret\ndatabase and evaluation procedure for face-recognition algo-\nrithms. In Image and Vision Computing, volume 16, pages\n295–306, 1998. 5\n[38] P. J. Phillips, P. J. Flynn, W. T. Scruggs, K. W. Bowyer,\nJ. Chang, K. Hoffman, J. Marques, J. Min, and W. J. Worek.\nOverview of the face recognition grand challenge. In 2005\nIEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2005), 20-26 June 2005, San\nDiego, CA, USA, pages 947–954, 2005. 5\n[39] A. Quek. Facemorpher. 2019. 5\n[40] R. Raghavendra, K. B. Raja, and C. Busch. Detecting mor-\nphed face images. In BTAS, pages 1–7. IEEE, 2016. 1, 2\n[41] R. Raghavendra, K. B. Raja, S. Venkatesh, and C. Busch.\nFace morphing versus face averaging: Vulnerability and de-\ntection. In IJCB, pages 555–563. IEEE, 2017. 5\n[42] R. Raghavendra, K. B. Raja, S. Venkatesh, and C. Busch.\nTransferable deep-cnn features for detecting digital and\nprint-scanned morphed face images.\nIn 2017 IEEE Con-\nference on Computer Vision and Pattern Recognition Work-\nshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-\n26, 2017, pages 1822–1830. IEEE Computer Society, 2017.\n1, 2\n[43] R. Ramachandra, S. Venkatesh, K. B. Raja, and C. Busch.\nDetecting face morphing attacks with collaborative represen-\ntation of steerable features. In CVIP (1), volume 1022 of\nAISC, pages 255–265. Springer, 2018. 1, 2, 7, 8\n[44] E. Sarkar, P. Korshunov, L. Colbois, and S. Marcel. Vulnera-\nbility analysis of face morphing attacks from landmarks and\ngenerative adversarial networks. arXiv preprint, Oct. 2020.\n1, 2, 5\n[45] U. Scherhag, R. Raghavendra, K. B. Raja, M. Gomez-\nBarrero, C. Rathgeb, and C. Busch. On the vulnerability of\nface recognition systems towards morphed face attacks. In\nIWBF, pages 1–6. IEEE, 2017. 2\n[46] U. Scherhag, C. Rathgeb, and C. Busch. Performance varia-\ntion of morphed face image detection algorithms across dif-\nferent datasets. In IWBF, pages 1–6. IEEE, 2018. 2\n[47] S. Soleymani, A. Dabouei, F. Taherkhani, J. Dawson, and\nN. M. Nasrabadi. Mutual information maximization on dis-\nentangled representations for differential morph detection. In\nProceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision, pages 1731–1741, 2021. 2\n[48] S. Soleymani, A. Dabouei, F. Taherkhani, J. M. Dawson, and\nN. M. Nasrabadi. Mutual information maximization on dis-\nentangled representations for differential morph detection. In\nIEEE Winter Conference on Applications of Computer Vi-\nsion, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021,\npages 1730–1740. IEEE, 2021. 2\n[49] S. Venkatesh, H. Zhang, R. Ramachandra, K. B. Raja,\nN. Damer, and C. Busch.\nCan GAN generated morphs\nthreaten face recognition systems equally as landmark based\nmorphs? - vulnerability and detection. In 8th International\nWorkshop on Biometrics and Forensics, IWBF 2020, Porto,\nPortugal, April 29-30, 2020, pages 1–6. IEEE, 2020. 5\n[50] P. Voigt and A. v. d. Bussche. The EU General Data Protec-\ntion Regulation (GDPR): A Practical Guide. Springer, 1st\nedition, 2017. 2\n[51] L. Xiang, G. Ding, and J. Han. Learning from multiple ex-\nperts: Self-paced knowledge distillation for long-tailed clas-\nsiﬁcation. In Computer Vision - ECCV 2020 - 16th Euro-\npean Conference, Glasgow, UK, August 23-28, 2020, Pro-\nceedings, Part V, pages 247–263, 2020. 2\n[52] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face represen-\ntation from scratch. CoRR, abs/1411.7923, 2014. 2, 3, 5, 6,\n7\n[53] D. Zhang, J. Han, L. Zhao, and D. Meng. Leveraging prior-\nknowledge for weakly supervised object detection under a\ncollaborative self-paced curriculum learning framework. Int.\nJ. Comput. Vis., 127(4):363–380, 2019. 2\n[54] H. Zhang, S. Venkatesh, R. Ramachandra, K. B. Raja,\nN. Damer, and C. Busch.\nMIPGAN - generating strong\nand high quality morphing attacks using identity prior driven\nGAN. IEEE Trans. Biom. Behav. Identity Sci., 3(3):365–383,\n2021. 2\n[55] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detec-\ntion and alignment using multitask cascaded convolutional\nnetworks. IEEE Signal Process. Lett., 23(10):1499–1503,\n2016. 5\n[56] Y. Zhang, Z. Yin, Y. Li, G. Yin, J. Yan, J. Shao, and Z. Liu.\nCeleba-spoof: Large-scale face anti-spooﬁng dataset with\nrich annotations. In Computer Vision - ECCV 2020 - 16th\nEuropean Conference, Glasgow, UK, August 23-28, 2020,\nProceedings, Part XII, volume 12357 of Lecture Notes in\nComputer Science, pages 70–85. Springer, 2020. 2\n[57] B. Zong, Q. Song, M. R. Min, W. Cheng, C. Lumezanu,\nD. Cho, and H. Chen. Deep autoencoding gaussian mixture\nmodel for unsupervised anomaly detection. In 6th Interna-\ntional Conference on Learning Representations, ICLR 2018,\nVancouver, BC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings, 2018. 2, 3\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-08-11",
  "updated": "2022-08-11"
}