{
  "id": "http://arxiv.org/abs/2208.06450v2",
  "title": "Quantum reinforcement learning in the presence of thermal dissipation",
  "authors": [
    "M. L. Olivera-Atencio",
    "L. Lamata",
    "M. Morillo",
    "J. Casado-Pascual"
  ],
  "abstract": "A study of the effect of thermal dissipation on quantum reinforcement\nlearning is performed. For this purpose, a nondissipative quantum reinforcement\nlearning protocol is adapted to the presence of thermal dissipation. Analytical\ncalculations as well as numerical simulations are carried out obtaining\nevidence that dissipation do not significantly degrade the performance of the\nquantum reinforcement learning protocol for sufficiently low temperatures,\nbeing in some cases even beneficial. Quantum reinforcement learning under\nrealistic experimental conditions of thermal dissipation opens an avenue for\nthe realization of quantum agents able to interact with a changing environment,\nand adapt to it, with plausible many applications inside quantum technologies\nand machine learning.",
  "text": "Quantum reinforcement learning in the presence of thermal dissipation\nMar´ıa Laura Olivera-Atencio,1 Lucas Lamata,2, 3 Manuel Morillo,1 and Jes´us Casado-Pascual1, ∗\n1F´ısica Te´orica,Universidad de Sevilla, Apartado de Correos 1065, Sevilla 41080, Spain\n2Departamento de F´ısica At´omica, Molecular y Nuclear, Universidad de Sevilla, 41080 Sevilla, Spain\n3Instituto Carlos I de F´ısica Te´orica y Computacional, 18071 Granada, Spain\n(Dated: August 8, 2023)\nA study of the effect of thermal dissipation on quantum reinforcement learning is performed. For\nthis purpose, a nondissipative quantum reinforcement learning protocol is adapted to the presence\nof thermal dissipation.\nAnalytical calculations as well as numerical simulations are carried out\nobtaining evidence that dissipation do not significantly degrade the performance of the quantum\nreinforcement learning protocol for sufficiently low temperatures, being in some cases even beneficial.\nQuantum reinforcement learning under realistic experimental conditions of thermal dissipation opens\nan avenue for the realization of quantum agents able to interact with a changing environment, and\nadapt to it, with plausible many applications inside quantum technologies and machine learning.\nI.\nINTRODUCTION\nQuantum machine learning [1, 2] aims at employ-\ning quantum technologies for achieving machine learn-\ning tasks more efficiently. A diversity of quantum algo-\nrithms for machine learning have been proposed, includ-\ning solvers for linear systems of equations [3], quantum\nprincipal component analysis [4], quantum support vec-\ntor machines [5], quantum annealers [6], quantum vari-\national eigensolvers [7], quantum reinforcement learn-\ning [8–15], quantum generative adversarial networks [16],\nas well as quantum kernels [17]. Experimental implemen-\ntations showing, in some instances, speedups with respect\nto classical or other kinds of quantum algorithms have\nbeen carried out already at the noisy intermediate scale\nquantum (NISQ) devices [16–21]. Many of these works\nstudy the advantages of quantum technologies over classi-\ncal machine learning. However, the effect of thermal dis-\nsipation on quantum machine learning has not yet been\nanalyzed in depth, although performing a real quantum\nexperiment always involves working at nonzero tempera-\nture and, therefore, a certain amount of such dissipation.\nSome preliminary results on this issue can be found in\nRefs. [22, 23].\nIn the field of quantum machine learning, as in the clas-\nsical case, algorithms can be classified into three main\ngroups: supervised [24–32], unsupervised [33, 34], and\nreinforcement quantum algorithms [8–15].\nIn this pa-\nper, we will focus on the latter group and, more specif-\nically, on the effects of thermal dissipation on this type\nof algorithms.\nReinforcement learning algorithms con-\nsist on successive interactions between a known agent\nand an unknown environment alternated with a reward\nfunction that improves a specific task performed by such\nagent [35, 36]. The goal is to learn from the environment.\nThe information extraction from the environment as well\nas the channel used to communicate the information to\nthe agent and the action of the agent, are established by\n∗jcasado@us.es\nthe policy. The reward function defines the criterion used\nto punish or reward certain actions of the agent in order\nto improve its performance. Each reward reinforces the\ncurrent strategy while the punishment forces an adapta-\ntion of the strategy.\nTo study the effect of dissipation on quantum rein-\nforcement learning, in this paper we adapt the nondissi-\npative algorithm proposed in Ref. [15] to the presence of\nthermal dissipation. For this purpose, the environment\nwill be considered to be an unknown quantum system\nwhose dynamics, besides including a unitary part associ-\nated with a given Hamiltonian as in [15], also possesses a\ncertain amount of thermal dissipation. The goal is to ex-\ntract information or learn from the environment to obtain\nnear-optimal knowledge of the eigenstates of the Hamil-\ntonian. The agent corresponds to a known and manip-\nulable quantum state that must conveniently adjust to\nthe dynamics of the environment in order to approach\nthe unknown eigenstate. It should be noted that the aim\nof this work is not to analyze the scalability or speedup\nof this type of protocols, which has been discussed pre-\nviously [15, 18], but to study the effect of thermal dissi-\npation on it. For this reason, and for the sake of clarity,\nwe will focus on the simplest case of a two-level quantum\nsystem.\nThe structure of the remainder of this paper is as fol-\nlows. In Sec. II, the nondissipative protocol proposed in\nRef. [15] is adapted to the presence of thermal dissipa-\ntion. Special emphasis is given to the peculiarities asso-\nciated with the presence of such dissipation. In Sec. III,\nwe discuss the numerical implementation of the proto-\ncol and illustrate our results with numerical simulations.\nFinally, in Sec. IV, we present conclusions for the main\nfindings of our work.\nII.\nPROTOCOL DESCRIPTION\nIn the nondissipative protocol proposed in Ref. [15],\nthe agent A is considered to be a known manipulable\nquantum system described by a state vector |ϕ⟩.\nThe\nenvironment E is modeled as a “black box” that inter-\narXiv:2208.06450v2  [quant-ph]  5 Aug 2023\n2\nacts with A for a time τ.\nThe effect of this interac-\ntion on A is characterized by a unitary transformation\nU ≡e−iτH/ℏapplied to the state vector |ϕ⟩, where H is\nan unknown interaction Hamiltonian whose eigenvectors\nare to be computed. For simplicity, we will henceforth re-\nstrict the analysis to the case of a single qubit with state\nbasis vectors {|0⟩, |1⟩} and unknown interaction Hamil-\ntonian\nH = ℏω\n2 (|+⟩⟨+| −|−⟩⟨−|) ,\n(1)\nwhere ω is a positive constant with dimensions of fre-\nquency and {|+⟩, |−⟩} are the eigenvectors to be com-\nputed.\nSuppose now that, in addition to interacting with E,\nA is also in contact with a thermal bath B at a finite\ntemperature T. We will also assume that the combined\naction of E and B on A is described by a Lindblad master\nequation of the form [37]\n˙ρt = −i\nℏ[H, ρt] +\nX\nj=±\nΓj\n\u0012\n˜σ†\njρt˜σj −1\n2{˜σj˜σ†\nj, ρt}\n\u0013\n, (2)\nwhere ρt is the density operator representing the state\nof A at time t, ˜σ−= |−⟩⟨+| = ˜σ†\n+ is a Lindblad op-\nerator that induces dissipative decay from the ex-\ncited state |+⟩to the ground state |−⟩, and Γ± =\nΓ0e±ℏω/(2kBT ) csch [ℏω/(2kBT)] /2, with Γ0 being the de-\ncay rate from the excited state to the ground state at\nzero temperature. The frequency ω in Eq. (1) must be\nredefined to include the frequency shift caused by the\npresence of the thermal bath. The first term on the right\nhand side in Eq. (2) describes the coherent evolution of\nthe system, while the second term gives rise to dissipa-\ntion.\nBy solving Eq. (2), the density operator at an arbitrary\ntime τ can be expressed in terms of the initial density\noperator ρ0 in the form\nρτ = E(ρ0) ≡\n3\nX\nj=0\nUEjρ0E†\njU †,\n(3)\nwhere {E0, E1, E2, E3} are the Kraus operators for the\ngeneralized amplitude damping channel [38]. The explicit\nexpressions for these operators are [38]\nE0 = √p+\n\u0010\n|+⟩⟨+| +\np\n1 −γ |−⟩⟨−|\n\u0011\n,\n(4)\nE1 = √p+γ˜σ+,\n(5)\nE2 =\np\n1 −p+\n\u0010p\n1 −γ |+⟩⟨+| + |−⟩⟨−|\n\u0011\n,\n(6)\nE3 =\np\n(1 −p+)γ˜σ−,\n(7)\nwhere p+ = e−ℏω/(2kBT ) sech [ℏω/(2kBT)] /2 is the ther-\nmal equilibrium probability of the excited state and\nγ = 1 −e−Γτ, with Γ = Γ+ + Γ−= Γ0 coth [ℏω/(2kBT)].\nNote that in the absence of dissipation, i.e., for Γ0 = 0,\nthe parameter γ vanishes and, therefore, E0 = √p+I,\nE2 = √1 −p+I, and E1 = E3 = 0, with I being the iden-\ntity operator.\nThus, Eq. (3) reduces to ρτ = Uρ0U †,\nwhich is the case considered in Ref. [15] with ρ0 = |ϕ⟩⟨ϕ|.\nBearing in mind the above results, the protocol pro-\nposed in Ref. [15] can be adapted to the dissipative case\nas detailed below. The procedure involves very many it-\nerations, so that, the state of A in the kth iteration is\ndenoted as |ϕ(k)⟩, with k ∈N. We assume that, in the\nfirst iteration, A is prepared in one of the basis states,\nfor instance, in the state\n\f\fϕ(1)\u000b\n= |0⟩. The states |0⟩and\n\f\fϕ(k)\u000b\nare related by\n|ϕ(k)⟩= D(k) |0⟩.\n(8)\nwhere D(k) is a unitary operator constructed inductively,\nstarting with D(1) = I, and building D(k+1) out of D(k)\nas follows:\n(i) From the initial density operator ρ(k)\n0\n= |ϕ(k)⟩⟨ϕ(k)|,\nwe let the system evolve according to the Lindblad equa-\ntion in Eq. (2) for a time τ. Call ρ(k)\nτ\n= E(ρ(k)\n0 ) the den-\nsity at the end of that evolution obtained by Eq. (3).\n(ii) We extract information from ρ(k)\nτ\nby measuring\nthe observable M (k) = D(k) |1⟩⟨1| D(k)†. In order to al-\nways measure the same observable M (1) = |1⟩⟨1| for all\niterations, first, we apply the unitary transformation\nD(k)†ρ(k)\nτ D(k) and, then, we measure M (1).\nAfter the\nmeasurement process, the state of A is\n\f\fm(k)\u000b\n, with m(k)\nbeing the outcome of the measurement, which can be 0\nor 1 with probabilities\nP (k)\n0\n= ⟨0|D(k)†ρ(k)\nτ D(k)|0⟩\n(9)\nand P (k)\n1\n= 1 −P (k)\n0\n,\nrespectively.\nOnce the mea-\nsurement\nhas\nbeen\ncompleted,\nthe\nstate\n|ϕ(k)⟩\ncan\nbe\nreconstructed\nfrom\n|m(k)⟩\nby\nthe\nunitary\ntransformation D(k) \u0002\n(1 −m(k))I + m(k)σx\n\u0003\n|m(k)⟩, with\nσx = |0⟩⟨1| + |1⟩⟨0|.\n(iii) If the outcome of the measurement is m(k) = 1,\nwe generate three pseudo-random angles, α(k)\nx , α(k)\ny , and\nα(k)\nz , uniformly distributed in the exploration interval\n\u0002\n−w(k)π, w(k)π\n\u0003\n. The width of this interval is controlled\nby the exploration parameter w(k), which is computed in-\nductively, starting with w(1) = 1 (maximum width), and\nbuilding w(k+1) out of w(k) using the rule\nw(k+1) = min\nn\n1,\nh\n(1 −m(k))r + m(k)p\ni\nw(k)o\n,\n(10)\nwith r < 1 and p > 1 being the reward and the punish-\nment rates [15], respectively. In other words, the value\nof the exploration parameter is updated according to the\noutcome of the preceding measurement, m(k), and the\ncorresponding reward or punishment.\nEvery time the\nmeasurement outcome is m(k) = 1 a punishment is ap-\nplied by increasing the value of the exploration parameter\nfrom w(k) to w(k+1) = min[1, pw(k)], thus widening the\nexploration interval. The min function is required since\nthe maximum value of the exploration parameter is 1.\nConversely, when the measurement outcome is m(k) = 0\n3\na reward is granted by decreasing the value of the explo-\nration parameter from w(k) to w(k+1) = rw(k), thus nar-\nrowing the exploration interval. Once the three pseudo-\nrandom angles α(k)\nx , α(k)\ny , and α(k)\nz\nhave been calculated,\nthey are used to implement the pseudo-random rotation\nR(k) = e−iα(k)\ny\nσ(k)\ny\n/2e−iα(k)\nz\nσ(k)\nz\n/2e−iα(k)\nx\nσ(k)\nx\n/2,\n(11)\nwhere\nthe\noperators\nσ(k)\nx ,\nσ(k)\ny ,\nand\nσ(k)\nz\nare\nre-\nlated to the Pauli operators σx, σy = i(|1⟩⟨0| −|0⟩⟨1|),\nand σz = |0⟩⟨0| −|1⟩⟨1| by the unitary transformation\nσ(k)\nα\n= D(k)σαD(k)†, with α = x, y, and z.\n(iv) Finally, we construct D(k+1) from D(k) as\nD(k+1) =\nh\n(1 −m(k))I + m(k)R(k)i\nD(k).\n(12)\nTherefore, the trade-off between exploration and ex-\nploitation, which is a characteristic of reinforcement\nlearning [35], is regulated using the measurement out-\ncome m(k). If the measurement outcome is m(k) = 1, the\nagent decides to explore and modifies its state from |ϕ(k)⟩\nto |ϕ(k+1)⟩= R(k) |ϕ(k)⟩. On the contrary, if the mea-\nsurement outcome is m(k) = 0, the agent decides to ex-\nploit and keeps its state invariant, i.e, |ϕ(k+1)⟩= |ϕ(k)⟩.\nSince, according to Eq. (11), R(k) is related to the\npseudo-random rotation\n˘R(k) = e−iα(k)\ny\nσy/2e−iα(k)\nz\nσz/2e−iα(k)\nx\nσx/2\n(13)\nby the unitary transformation R(k) = D(k) ˘R(k)D(k)†,\nEq. (12) can also be expressed in the form\nD(k+1) = D(k) h\n(1 −m(k))I + m(k) ˘R(k)i\n.\n(14)\nThe recursive relations in Eqs. (12) and (14) can easily\nbe solved to yield\nD(k+1) =\n1\nY\nj=k\nh\n(1 −m(j))I + m(j)R(j)i\n(15)\nand\nD(k+1) =\nk\nY\nj=1\nh\n(1 −m(j))I + m(j) ˘R(j)i\n,\n(16)\nrespectively, where we use the ordered product notation\nQ1\nj=k Aj = AkAk−1 · · · A1 and Qk\nj=1 Aj = A1A2 · · · Ak.\nThe dissipative protocol just described, as well as the\nnondissipative protocol analyzed in [15], can be related\nto a Markov decision process (MDP) [39, 40], as in the\nstandard reinforcement learning setup. The state space\nof this MDP is represented by the set of all possible quan-\ntum states, |ϕ⟩, in which the agent can be found. The ac-\ntion space is the set of all rotations, R, including the iden-\ntity I, acting on these quantum states. Since the actions\nor rotations act deterministically on the agent states, the\nMDP is deterministic [36] and, consequently, its transi-\ntion model can be described in terms of a transition func-\ntion rather than a transition probability. Specifically, the\ntransition function is T (|ϕ⟩, R) = R |ϕ⟩, i.e., as a result\nof the action R applied on state |ϕ⟩, the state changes to\n|ϕ′⟩= R |ϕ⟩. Finally, the reward function of the MDP is\ndescribed by the function R(|ϕ⟩, R) = R(R) that takes\nthe value r if R = I and p if R ̸= I. Given the functions\nT and R, it is sufficient to know the current state |ϕ⟩\nand the current action R to determine the next state |ϕ′⟩\nand the corresponding reward, thus fulfilling the Markov\nproperty. Note, however, that the decision rule (iv) that\ndetermines which action to select, given the agent state\nat step k, is randomized and history-dependent [39, 40].\nIts randomness stems both from the intrinsically random\nnature of the measurement outcomes m(k) and from the\ndependence of the rotations R(k) on the pseudo-random\nangles α(k)\nx , α(k)\ny , and α(k)\nz .\nIts history dependence is\ndue to the fact that, as described in (iii), to generate\nthese pseudo-random angles, the value of the exploration\nparameter w(k) must first be determined, which implies\nsolving the recurrence relation in Eq. (10) and, thus,\nknowing all the previous history.\nIII.\nNUMERICAL RESULTS\nWe have carried out numerical simulations implement-\ning the protocol presented in the previous section for a\nHamiltonian of the form\nH = ℏω\n2 σx,\n(17)\nwhich corresponds to taking |±⟩= (|0⟩± |1⟩)/\n√\n2 in\nEq. (1).\nTo deal with dimensionless quantities, we\nhave introduced the dimensionless parameters ˜τ = ωτ,\n˜Γ0 = Γ0/ω, and ˜T = kBT/(ℏω).\nTo simulate numeri-\ncally the measurement process appearing in item (ii) of\nSec. II, we first calculate the probability P (k)\n0\nof obtaining\nm(k) = 0 using Eq. (9). Then, we draw a pseudo-random\nnumber ξ(k) uniformly distributed in the interval [0, 1]. If\nξ(k) ≤P (k)\n0\n, the outcome of the measurement is m(k) = 0,\nwhereas if ξ(k) > P (k)\n0\nthe result will be m(k) = 1.\nIn order to assess the accuracy of the protocol, we have\ncalculated the fidelity between the state |ϕ(k)⟩and the\nclosest eigenvector of H as a function of the number of\niterations, k. Since it is not known a priori whether this\neigenvector is |+⟩or |−⟩, we have considered the greater\nof the two values, i.e.,\nf (k) = max\n\u0010\n| ⟨+|ϕ(k)⟩|, | ⟨−|ϕ(k)⟩|\n\u0011\n.\n(18)\nThe closer the value of f (k) is to 1 as k increases, the more\naccurate the computation of the corresponding eigenvec-\ntor will be. In addition, at each iteration, the convergence\nof the protocol has been quantified by the exploration pa-\nrameter w(k) defined in item (iii) of Sec. II. The protocol\nis considered to converge if w(k) approaches zero as k\nincreases. Moreover, the faster it approaches zero, the\nfaster the convergence of the protocol.\n4\nIn the numerical simulations presented here, we have\nrepeated the protocol a large number N of realizations.\nThen, for each value of k, the mean fidelity\nF (k) = 1\nN\nN\nX\nj=1\nf (k)\nj\n(19)\nand the mean exploration parameter\nW (k) = 1\nN\nN\nX\nj=1\nw(k)\nj\n(20)\nare obtained by averaging over the N realizations, where\nthe subscript j refers to the jth realization of the proto-\ncol. The number of realizations considered in this paper\nis N = 1000.\n0.8\n0.9\n1\n200\n400\n0\n0.5\n1\n200\n400\nFIG. 1.\nDependence of the mean fidelity F (k) [(a) and\n(b)] and the mean exploration parameter W (k) [(c) and\n(d)] on the number of iterations,\nk,\nfor different val-\nues of the reward parameter r.\nThe state in the first\niteration is |ϕ(1)⟩= (\n√\n3 |0⟩+ |1⟩)/2 in (a) and (c) and\n|ϕ(1)⟩= (2\n√\n2 |0⟩+ |1⟩)/3 in (b) and (d). The remaining pa-\nrameter values are ˜Γ0 = 0.5, ˜T = 0.3, ˜τ = 1, N = 1000, and\np = 2/r.\nIn Fig. 1 we have plotted the mean fidelity F (k)\n[(a) and (b)] and the mean exploration parameter W (k)\n[(c) and (d)] versus the number of iterations k.\nWe\nhave considered two different states for the first itera-\ntion, namely, |ϕ(1)⟩= (\n√\n3 |0⟩+ |1⟩)/2 in (a) and (c) and\n|ϕ(1)⟩= (2\n√\n2 |0⟩+ |1⟩)/3 in (b) and (d). With different\ntypes of lines, we have depicted different values of the re-\nward parameter r. When r is modified, the value of the\npunishment parameter p is also varied according to the\nrelation p = 2/r. The parameters associated with ther-\nmal dissipation, i.e., the dimensionless decay rate at zero\ntemperature and the dimensionless temperature, are set\nat ˜Γ0 = 0.5 and ˜T = 0.3, respectively. As can be seen,\nthe values of the reward and punishment parameters af-\nfect the accuracy and convergence of the protocol. The\nasymptotic values of F (k) in the large iteration limit are\n0.8\n0.9\n1\n200\n400\n0\n0.5\n1\n200\n400\nFIG. 2. Dependence of the mean fidelity F (k) [(a) and (b)]\nand the mean exploration parameter W (k) [(c) and (d)] on the\nnumber of iterations, k. Black solid lines depict the nondissi-\npative case ˜Γ0 = 0. The dissipative cases are indicated with\nred dotted lines (˜Γ0 = 0.5) and blue dashed lines (˜Γ0 = 1).\nThe dimensionless temperature is ˜T = 0.3 in (a) and (c) and\n˜T = 1.5 in (b) and (d). The remaining parameter values are\n˜τ = 1, N = 1000, r = 0.9, and p = 20/9.\nless than 0.94 when r = 0.7, reach values close to 0.98\nin the case of r = 0.9, and take intermediate values for\nr = 0.8. A more detailed analysis of how the nondissipa-\ntive protocol is affected by varying the values of the re-\nward and punishment parameters is provided in Ref. [15].\nNote, however, that the particular state chosen for the\nfirst iteration does not seem to play a relevant role in the\naccuracy and convergence of the protocol, as can be seen\nby comparing Figs. 1(a) and (c) with Figs. 1(b) and (d).\nIn order to focus on the effect of thermal dissipation on\nthe performance of the protocol, we will henceforth set\nthe reward and punishment parameters to r = 0.9 and\np = 2/r = 20/9, respectively, as well as the initial state\nto |ϕ(1)⟩= |0⟩.\nIn Fig. 2 we have plotted the mean fidelity F (k) [(a)\nand (b)] and the mean exploration parameter W (k) [(c)\nand (d)] versus k for the parameter values indicated in\nthe figure caption. With different types of lines, we have\ndepicted different values of the dimensionless decay rate\nat zero temperature ˜Γ0. The figure illustrates the role\nplayed by the temperature in the accuracy and conver-\ngence of the protocol.\nFor dimensionless temperature\n˜T = 0.3 [(a) and (c)], the differences between the nondis-\nsipative case ˜Γ0 = 0 (solid black line), and the dissipa-\ntive cases ˜Γ0 = 0.5 (dotted red line) and ˜Γ0 = 1 (blue\ndashed line) are not very significant. In the three cases,\nthe asymptotic values of F (k) in the large iteration limit\nare very similar and quite close to 0.98 [see Fig. 2(a)],\nand W (k) converges to zero rather quickly [see Fig. 2(c)].\nTherefore, contrary to what might be expected, temper-\natures much lower than ℏω/kB are not required for the\nprotocol to work. Nonetheless, for sufficiently high di-\nmensionless temperatures, such as ˜T = 1.5, the protocol\n5\nfails in the dissipative cases, as shown in Figs. 2(b) and\n(d). On the one hand, the asymptotic values of F (k) in\nthe large iteration limit are substantially less than in the\nabsence of dissipation [see Fig. 2(b)]. On the other hand,\nW (k) does not approach zero as k increases [see Fig. 2(d)]\nand, therefore, the protocol does not converge.\n0.8\n0.9\n1\n0\n0.5\n1\n1.5\n2\n0\n0.5\n1\nFIG. 3. Dependence of the asymptotic values in the large iter-\nation limit of the mean fidelity, Fa, and the mean exploration\nparameter, Wa, on the dimensionless temperature ˜T for dif-\nferent values of ˜Γ0. Black solid lines depict the nondissipative\ncase ˜Γ0 = 0, while the dissipative cases are indicated with red\ndotted lines (˜Γ0 = 0.5) and blue dashed lines (˜Γ0 = 1). The\nremaining parameter values are ˜τ = 1, N = 1000, r = 0.9,\nand p = 20/9.\n1\n0\n0.4\n0.6\n0.8\n1\n0.5\n1\n1\n0\n0.7\n0.8\n0.9\n1\nFIG. 4.\nDependence of the fidelities ⟨+|E(|+⟩⟨+|)|+⟩in\nEq. (21) and ⟨−|E(|−⟩⟨−|)|−⟩in Eq. (22) on ˜Γ0 and ˜T for\n˜τ = 1, N = 1000, r = 0.9, and p = 20/9.\nIn order to determine the range of temperatures in\nwhich the protocol works well, in Fig. 3 we have plotted\nthe asymptotic values in the large iteration limit of the\nmean fidelity, Fa, and the mean exploration parameter,\nWa, as a function of the dimensionless temperature ˜T.\nAs can be seen, there is hardly any difference between\nthe dissipative and nondissipative cases up to ˜T ≈0.4.\nAbove that temperature, the Fa values in the dissipa-\ntive cases ˜Γ0 = 0.5 (red dotted line) and ˜Γ0 = 1 (blue\ndashed line) decrease abruptly and become substantially\nless than in the nondissipative case [see Fig. 3(a)]. Fur-\nthermore, there is a rapid increase in Wa to nonzero val-\nues [see Fig. 3(b)], which indicates that the protocol stops\nconverging.\nTo better understand the behavior observed in Fig. 3,\nit is worth remembering that, in the nondissipative case,\nthe design of the protocol is based on the fact that the\nstates |+⟩⟨+| and |−⟩⟨−| are fixed points of the quantum\ngate E, i.e., E(|±⟩⟨±|) = |±⟩⟨±|. However, in the presence\nof dissipation, |+⟩⟨+| and |−⟩⟨−| are no longer exact fixed\npoints of E, since the fidelities between the state |+⟩⟨+|\nand E(|+⟩⟨+|), ⟨+|E(|+⟩⟨+|)|+⟩, and between the state\n|−⟩⟨−| and E(|−⟩⟨−|), ⟨−|E(|−⟩⟨−|)|−⟩, are in general less\nthan 1. Specifically, from Eq. (3), it can be shown that\n⟨+|E(|+⟩⟨+|)|+⟩= 1 −(1 −p+)γ\n(21)\n⟨−|E(|−⟩⟨−|)|−⟩= 1 −p+γ,\n(22)\nwhich are clearly less than 1 if ˜Γ0 ̸= 0 (dissipative case).\nDespite this, there may be parameter values for which\nthe fidelities ⟨+|E(|+⟩⟨+|)|+⟩and/or ⟨−|E(|−⟩⟨−|)|−⟩\nare quite close to 1.\nIn that case, the corresponding\nstates |+⟩⟨+| and/or |−⟩⟨−| would behave as approxi-\nmate fixed points and the protocol would still be ap-\nplicable.\nFigure 4 shows the dependence of the fideli-\nties ⟨+|E(|+⟩⟨+|)|+⟩and ⟨−|E(|−⟩⟨−|)|−⟩, obtained us-\ning Eqs. (21) and (22), on ˜Γ0 and ˜T. As can be seen,\nthere are regions where these fidelities are quite close to\n1. In particular, Fig. 4(b) shows that ⟨−|E(|−⟩⟨−|)|−⟩is\nquite close to 1 up to ˜T ≈0.4, which is the approximate\nvalue beyond which the protocol starts to fail in Fig. 3.\nFrom Figs. 2 and 3, it may be concluded that the ef-\nfect of dissipation is not very relevant for sufficiently low\ndimensionless temperatures.\nHowever, a more exhaus-\ntive analysis of the results reveals that it plays an im-\nportant role in the protocol. Figure 5 depicts separately\nthe mean fidelity F (k)\n−\nbetween the state |ϕ(k)⟩and the\nground state |−⟩[(a) and (b)] and the mean fidelity F (k)\n+\nbetween the state |ϕ(k)⟩and the excited state |+⟩[(c)\nand (d)], defined as\nF (k)\n∓\n= 1\nN\nN\nX\nj=1\n| ⟨∓|ϕ(k)\nj ⟩|.\n(23)\nWhile, for ˜T = 0.3, the differences between the dissipa-\ntive and nondissipative cases are not too significant when\ncomparing the mean fidelity F (k) [see Fig. 2(a)], they be-\ncome rather noticeable when comparing the state-specific\nmean fidelities F (k)\n−\nand F (k)\n+\n[see Figs. 5(a) and (c)]. In\nthe nondissipative case, F (k)\n−\nand F (k)\n+\nare almost indis-\ntinguishable [black solid lines in Figs. 5(a) and (c)], as is\nto be expected from the fact that |+⟩⟨+| and |−⟩⟨−| are\nequivalent fixed points of E in the absence of dissipation.\nBy contrast, the presence of dissipation substantially af-\n6\n0\n0.5\n1\n200\n400\n0\n0.5\n1\n200\n400\nFIG. 5. Dependence of the state-specific mean fidelities F (k)\n−\n[(a) and (b)] and F (k)\n+\n[(c) and (d)] on the number of iter-\nations, k.\nBlack solid lines depict the nondissipative case\n˜Γ0 = 0. The dissipative cases are indicated with red dotted\nlines (˜Γ0 = 0.5) and blue dashed lines (˜Γ0 = 1). The dimen-\nsionless temperature is ˜T = 0.3 in (a) and (c) and ˜T = 1.5\nin (b) and (d). The remaining parameter values are ˜τ = 1,\nN = 1000, r = 0.9, and p = 20/9.\nfects the state-specific fidelities, increasing F (k)\n−\nand de-\ncreasing F (k)\n+\n[red dotted lines and blue dashed lines in\nFigs. 5(a) and (c)]. This is because, for ˜T = 0.3, ˜Γ0 = 0.5\nand ˜Γ0 = 1, |−⟩⟨−| is an approximate fixed point of E but\n|+⟩⟨+| is not [see Fig. 4]. As the temperature increases,\nthis difference becomes less appreciable, as can be seen\nin Figs. 5(b) and (d).\nOne important aspect, not considered so far, is the\ndependence of the protocol results on the evolution time\nτ. As mentioned in Sec. II, in the absence of dissipation,\ni.e., for Γ0 = 0, the action of E on A is represented by the\nquantum channel E(ρ0) = e−iτH/ℏρ0eiτH/ℏ. For the case\nof the Hamiltonian H in Eq. (17), this quantum channel\nhas the explicit form\nE(ρ0) = 1\n2(ρ0 + σxρ0σx)\n+ cos(˜τ)\n2\n(ρ0 −σxρ0σx) + i sin(˜τ)\n2\n[ρ0, σx],\n(24)\nwhich is a periodic function of the dimensionless evolu-\ntion time ˜τ with period 2π. This periodicity is clearly\nvisible in Fig. 6(a), where the dependence of the asymp-\ntotic values in the large iteration limit of the mean fi-\ndelity, Fa, on ˜τ in the nondissipative case is depicted\nwith a solid line. However, the periodicity is not evident\nin the behavior of Wa, which always remains close to 0\nand indistinguishable from the abscissa-axis, as shown in\nFig. 6(b). Of particular interest is the case in which ˜τ\nis an integer multiple of 2π. In this case, it follows from\nEq. (24) that E does not modify the state of A and, there-\nfore, no learning is possible. Thus, for this case the mean\nfidelity F (k) is independent of the number of iterations,\nk and, consequently, Fa = F (1) = 1/\n√\n2 ≈0.71, as it can\n0.7\n0.8\n0.9\n1\n0\n2\n4\n6\n8\n10\n0\n0.1\n0.2\n0.3\nFIG. 6.\nDependence of the asymptotic values in the large\niteration limit of the mean fidelity, Fa, and the mean explo-\nration parameter, Wa, on the dimensionless evolution time ˜τ\nfor different values of ˜Γ0 and ˜T. Black solid lines depict the\nnondissipative case ˜Γ0 = 0. In (b) the black solid line is indis-\ntinguishable from the abscissa-axis. The dissipative cases are\nindicated with green filled circles (˜Γ0 = 0.5 and ˜T = 0.01), red\nstars (˜Γ0 = 0.5 and ˜T = 1), and blue open circles (˜Γ0 = 0.5\nand ˜T = 10). The remaining parameter values are N = 1000,\nr = 0.9, and p = 20/9.\nbe observed in Fig. 6(a) (black solid line).\nIn the presence of dissipation, i.e., for Γ0 ̸= 0, the\nquantum channel E(ρ0) in Eq. (3) ceases to be periodic\nin ˜τ due to the exponential dependence of the parameter\nγ on the evolution time τ (see Sec. II). As a consequence,\nthe oscillations observed in Fig. 6(a) for the nondissipa-\ntive case (black solid line) are notably attenuated and,\nbeyond a certain value of ˜τ, Fa becomes almost inde-\npendent of ˜τ [see green filled circles, red stars, and blue\nopen circles in Fig. 6(a)]. The effect of temperature on\nthe behavior of Fa is quite revealing. The long ˜τ values\nof Fa for the dissipative case decrease as the tempera-\nture increases, as expected. Nonetheless, the Fa values\nare substantially larger than those for the nondissipa-\ntive case for ˜τ values around the integer multiples of 2π.\nTherefore, under these circumstances, dissipation plays\na positive role in the protocol fidelity. Note that for not\ntoo high temperatures, the convergence of the protocol\ngauged by Wa is still guaranteed around the above men-\ntioned ˜τ values [see green filled circles and red stars in\nFig. 6(b)].\nIV.\nCONCLUSIONS\nIn this study, we investigated the impact of ther-\nmal dissipation on a protocol for quantum reinforcement\nlearning. To achieve this, we introduced a Lindblad dy-\nnamics for the density operator of a two-state system in\ncontact with a thermal bath. First, we conducted a com-\nprehensive theoretical analysis of the protocol and estab-\n7\nlished the relevant quantities that characterize it. Sub-\nsequently, we performed a numerical analysis to demon-\nstrate the influence of thermal dissipation on the proto-\ncol’s performance.\nIt is important to understand how\nthermal dissipation affects quantum reinforcement learn-\ning because machine learning algorithms are typically\nnot implemented in machines that operate at absolute\nzero temperature, and therefore, thermal effects cannot\nbe avoided in real-world situations.\nThe fundamental difference between the methodology\npresented in this study and that considered in Ref. [15]\nlies in the way in which the agent interacts with the en-\nvironment to gather information from it. In Ref. [15],\nsuch interaction is free from dissipation and is therefore\ndescribed by the unitary transformation ρτ = Uρ0U †.\nHowever, in this study, the interaction involves a cer-\ntain degree of thermal dissipation which causes it to de-\nviate from unitarity [see Eq. (3)]. This makes the de-\nscription more realistic in our case as experiments always\ntake place at nonzero temperatures and, therefore, in the\npresence of some degree of thermal dissipation.\nThe main conclusions of this work are:\n(i) For sufficiently low temperatures, dissipation does\nnot necessarily have a negative effect on the accuracy of\nthe protocol gauged by the mean fidelity F (k). In fact,\nour analysis shows that there are ranges of parameter\nvalues for which the dissipative protocol performs better\nthan the nondissipative one, as can be seen in Fig. 6.\nThese results might be of interest for the experimental\nimplementation of this type protocols, since sometimes\nthe presence of dissipation is unavoidable in an actual\nexperiment.\n(ii) Dissipation is particularly relevant when evaluat-\ning the state-specific mean fidelities F (k)\n−\nand F (k)\n+ . The\ninfluence of dissipation on these fidelities depends dra-\nmatically on which state-specific is considered. While in\nthe nondissipative case F (k)\n−\nand F (k)\n+\nare almost indis-\ntinguishable, in the dissipative case they can be quite\ndifferent. Specifically, for sufficiently low temperatures,\nthe mean fidelity with respect to the ground state, F (k)\n−,\nis much higher than the one obtained in the absence of\ndissipation, whereas the mean fidelity with respect to\nthe excited state, F (k)\n+ , is much lower. Therefore, if in\nan actual experiment one were interested in specifically\ncomputing the ground state, the presence of dissipation\nmight be useful.\nACKNOWLEDGEMENTS\nWe acknowledge funding by the Junta de Andaluc´ıa\nand FEDER (P20-00617 and US-1380840) and by the\nSpanish Ministry of Science, Innovation, and Universities\nunder grant Nos. PID2019-104002GB-C21 and PID2019-\n104002GB-C22.\n[1] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,\nN. Wiebe, and S. Lloyd, Nature 549, 195 (2017).\n[2] L. Lamata, Mach. Learn. Sci. Technol. 1, 033002 (2020).\n[3] A. W. Harrow, A. Hassidim, and S. Lloyd, Phys. Rev.\nLett. 103, 150502 (2009).\n[4] S. Lloyd, M. Mohseni, and P. Rebentrost, Nature Phys.\n10, 631 (2014).\n[5] P. Rebentrost, M. Mohseni, and S. Lloyd, Phys. Rev.\nLett. 113, 130503 (2014).\n[6] R. K. Nath, H. Thapliyal, and T. S. Humble, SN Comp.\nSci. 2, 365 (2021).\n[7] J. R. McClean, J. Romero, R. Babbush, and A. Aspuru-\nGuzik, New J. Phys. 18, 023023 (2016).\n[8] D. Dong, C. Chen, H. Li, and T.-J. Tarn, IEEE Trans-\nactions on Systems, Man, and Cybernetics, Part B (Cy-\nbernetics) 38, 1207 (2008).\n[9] G. D. Paparo, V. Dunjko, A. Makmal, M. A. Martin-\nDelgado, and H. J. Briegel, Phys. Rev. X 4, 031002\n(2014).\n[10] V. Dunjko, J. M. Taylor, and H. J. Briegel, Phys. Rev.\nLett. 117, 130501 (2016).\n[11] M. Bukov, Phys. Rev. B 98, 224305 (2018).\n[12] M. Bukov,\nA. G. R. Day,\nD. Sels,\nP. Weinberg,\nA. Polkovnikov, and P. Mehta, Phys. Rev. X 8, 031086\n(2018).\n[13] T. F¨osel, P. Tighineanu, T. Weiss, and F. Marquardt,\nPhys. Rev. X 8, 031084 (2018).\n[14] Y.-P. Liu, Q.-S. Jia, and X. Wang, IFAC-PapersOnLine\n55, 132 (2022), iFAC Workshop on Control for Smart\nCities CSC 2022.\n[15] F. Albarr´an-Arriagada, J. C. Retamal, E. Solano, and\nL. Lamata, Mach. Learn.: Sci. Technol. 1, 015002 (2020).\n[16] L. Hu, S.-H. Wu, W. Cai, Y. Ma, X. Mu, Y. Xu, H. Wang,\nY. Song, D.-L. Deng, C.-L. Zou, and L. Sun, Sci. Adv. 5,\neaav2761 (2019).\n[17] V. Havl´ıˇcek, A. D. C´orcoles, K. Temme, A. W. Harrow,\nA. Kandala, J. M. Chow, and J. M. Gambetta, Nature\n567, 209 (2019).\n[18] S. Yu, F. Albarr´an-Arriagada, J. C. Retamal, Y.-T.\nWang, W. Liu, Z.-J. Ke, Y. Meng, Z.-P. Li, J.-S. Tang,\nE. Solano, L. Lamata, C.-F. Li, and G.-C. Guo, Adv.\nQuantum Technol. 2, 1800074 (2019).\n[19] V. Saggio, B. E. Asenbeck, A. Hamann, T. Str¨omberg,\nP. Schiansky,\nV. Dunjko,\nN. Friis,\nN. C. Harris,\nM. Hochberg, D. Englund, S. W¨olk, H. J. Briegel, and\nP. Walther, Nature 591, 229 (2021).\n[20] M. Spagnolo, J. Morris, S. Piacentini, M. Antesberger,\nF. Massa, A. Crespi, F. Ceccarelli, R. Osellame, and\nP. Walther, Nature Phot. 16, 318 (2022).\n[21] H.-Y.\nHuang,\nM.\nBroughton,\nJ.\nCotler,\nS.\nChen,\nJ. Li, M. Mohseni, H. Neven, R. Babbush, R. Kueng,\nJ. Preskill, and J. R. McClean, Science 376, 1182 (2022).\n[22] N. H. Nguyen, E. C. Behrman, and J. E. Steck, Quantum\nMachine Intelligence 2 (2020).\n[23] C. Lu, S. Kundu, A. Arunachalam, and K. Basu, in 2022\nIEEE 15th Dallas Circuit And System Conference (2022)\npp. 1–2.\n8\n[24] L. Innocenti, L. Banchi, A. Ferraro, S. Bose, and M. Pa-\nternostro, New J. Phys. 22, 065001 (2020).\n[25] A. Youssry, R. J. Chapman, A. Peruzzo, C. Ferrie, and\nM. Tomamichel, Quantum Sci. Technol. 5, 025001 (2020).\n[26] S. Shrapnel, F. Costa, and G. Milburn, Int. J. Quantum\nInfo. 16, 1840010 (2018).\n[27] I. A. Luchnikov, S. V. Vintskevich, D. A. Grigoriev, and\nS. N. Filippov, Phys. Rev. Lett. 124, 140502 (2020).\n[28] A. A. Melnikov, L. E. Fedichkin, R.-K. Lee, and A. Alod-\njants, Adv. Quantum Technol. 3, 1900115 (2020).\n[29] A. A. Melnikov, L. E. Fedichkin, and A. Alodjants, New\nJ. Phys. 21, 125002 (2019).\n[30] S. S. Kalantre, J. P. Zwolak, S. Ragole, X. Wu, N. M.\nZimmerman, M. D. Stewart, and J. M. Taylor, npj Quan-\ntum Inform. 5, 6 (2019).\n[31] G. Torlai, B. Timar, E. P. L. van Nieuwenburg, H. Levine,\nA.\nOmran,\nA.\nKeesling,\nH.\nBernien,\nM.\nGreiner,\nV. Vuleti´c, M. D. Lukin, R. G. Melko, and M. Endres,\nPhys. Rev. Lett. 123, 230504 (2019).\n[32] G. Torlai, G. Mazzola, J. Carrasquilla, M. Troyer,\nR. Melko, and G. Carleo, Nat. Phys. 14, 447 (2018).\n[33] G. Liu, M. Chen, Y.-X. Liu, D. Layden, and P. Cappel-\nlaro, Mach. Learn.: Sci. Technol. 1, 015003 (2020).\n[34] I. Agresti, N. Viggianiello, F. Flamini, N. Spagnolo,\nA. Crespi, R. Osellame, N. Wiebe, and F. Sciarrino, Phys.\nRev. X 9, 011013 (2019).\n[35] R. Sutton and A. Barto, Reinforcement Learning: An In-\ntroduction., 2nd ed., Adaptative Computation and Ma-\nchine Learning (MIT Press, Cambridge, 2018).\n[36] L. Busoniu, R. Babuska, B. D. Schutter, and D. Ernst,\nReinforcement Learning and Dynamic Programming Us-\ning Function Approximators, 1st ed. (CRC Press, Inc.,\nBoca Raton, FL, USA, 2010).\n[37] H.-P. Breuer and F. Petruccione, Theory of Open Quan-\ntum Systems (Oxford University Press, Oxford, 2003).\n[38] M. A. Nielsen and I. L. Chuang, Quantum Computing\nand Quantum Information (Cambridge University Press,\nCambridge, 2000).\n[39] M. L. Puterman, in Stochastic Models, Handbooks in Op-\nerations Research and Management Science, Vol. 2 (El-\nsevier, 1990) pp. 331–434.\n[40] M. L. Puterman, Markov decision processes:\ndiscrete\nstochastic dynamic programming (John Wiley & Sons,\n2014).\n",
  "categories": [
    "quant-ph",
    "cond-mat.stat-mech"
  ],
  "published": "2022-08-12",
  "updated": "2023-08-05"
}