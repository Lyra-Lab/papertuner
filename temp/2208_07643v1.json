{
  "id": "http://arxiv.org/abs/2208.07643v1",
  "title": "A Review of the Convergence of 5G/6G Architecture and Deep Learning",
  "authors": [
    "Olusola T. Odeyomi",
    "Olubiyi O. Akintade",
    "Temitayo O. Olowu",
    "Gergely Zaruba"
  ],
  "abstract": "The convergence of 5G architecture and deep learning has gained a lot of\nresearch interests in both the fields of wireless communication and artificial\nintelligence. This is because deep learning technologies have been identified\nto be the potential driver of the 5G technologies, that make up the 5G\narchitecture. Hence, there have been extensive surveys on the convergence of 5G\narchitecture and deep learning. However, most of the existing survey papers\nmainly focused on how deep learning can converge with a specific 5G technology,\nthus, not covering the full spectrum of the 5G architecture. Although there is\na recent survey paper that appears to be robust, a review of that paper shows\nthat it is not well structured to specifically cover the convergence of deep\nlearning and the 5G technologies. Hence, this paper provides a robust overview\nof the convergence of the key 5G technologies and deep learning. The challenges\nfaced by such convergence are discussed. In addition, a brief overview of the\nfuture 6G architecture, and how it can converge with deep learning is also\ndiscussed.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nA Review of the Convergence of 5G/6G\nArchitecture and Deep Learning\nOlusola T. Odeyomi, Member, IEEE, Olubiyi O. Akintade, Temitayo O. Olowu, Member, IEEE,Gergely\nZaruba Senior Member, IEEE\nAbstract—The convergence of 5G architecture and deep learn-\ning has gained a lot of research interests in both the ﬁelds of\nwireless communication and artiﬁcial intelligence. This is because\ndeep learning technologies have been identiﬁed to be the potential\ndriver of the 5G technologies, that make up the 5G architecture.\nHence, there have been extensive surveys on the convergence of\n5G architecture and deep learning. However, most of the existing\nsurvey papers mainly focused on how deep learning can converge\nwith a speciﬁc 5G technology, thus, not covering the full spectrum\nof the 5G architecture. Although there is a recent survey paper\nthat appears to be robust, a review of that paper shows that it is\nnot well structured to speciﬁcally cover the convergence of deep\nlearning and the 5G technologies. Hence, this paper provides a\nrobust overview of the convergence of the key 5G technologies\nand deep learning. The challenges faced by such convergence\nare discussed. In addition, a brief overview of the future 6G\narchitecture, and how it can converge with deep learning is also\ndiscussed.\nIndex Terms—5G, 6G, deep learning, Internet of Things, multi-\naccess edge computing, software-deﬁned networking, network\nfunction virtualization, massive multiple-input multiple-output\n(MIMO).\nI. INTRODUCTION\nT\nHE\nThe evolution of the 5G ultra-dense network (UDN) in the\nyear 2020 and beyond is the realization of a mobile ecosystem\ncapable of supporting 1000-fold increase in trafﬁc when com-\npared to the year 2010 level, and a 10- to 100-fold surge in\ndata rates even at high mobility, and in dense areas [1–3]. To\nthis end, more capacity is critically needed in the fronthaul,\nbackhaul, and radio access network (RAN). Therefore, mobile\nnetwork operators (MNO) are faced with the challenges of\nmeeting the demands of the 5G UDN. According to the\nFifth Generation Private Public Partnership (5GPP)[4] and\nthe European Telecommunication Standard Institute (ETSI)[5],\nthe 5G UDN is expected to address the following challenges\nbeyond the capability of the present 4G Long Term Evolution-\nAdvanced (LTE-A) network: higher capacity, higher data rate,\nlower end-to-end latency, massive device connectivity, reduced\ncapital expenditure (CapEx), reduced operational expenditure\n(OpEx), and higher quality of experience (QoE) [2, 3].\nTo address the challenges in the 5G architecture, it is\nbelieved that many emerging technologies must be combined.\nTechnologies for providing more spectrum access, and al-\nlowing for network densiﬁcation and ofﬂoading are neces-\nsary [6]. Technologies for providing more spectrum includes\nmillimeter-wave (mmWave), cognitive radio, and carrier ag-\ngregation techniques. Network densiﬁcation involves the pro-\nliferation of small cells (e.g. femto cells and pico cells) to\nsupport the macro cells and micro cells already in use. Since\nhigh carrier frequencies are well suited for small cells, the\nmmWave technology with a good line-of-sight (LOS) will\nbe necessary. Massive multiple-input multiple-output (MIMO)\ntechnology and beamforming must be deployed for good\nantenna design. For ofﬂoading, multi-access edge computing\n(MEC) and device-to-device technology will minimize end-\nto-end latency for real-time and safety-critical applications.\nSuch applications include robotic applications for medical\nand industrial emergency response, vehicle-to-vehicle (V2V)\ncommunication, vehicle-to-infrastructure (V2I) communica-\ntion, device-to-device (D2D) communication, augmented and\nvirtual reality (AR/VR), social media, and online streaming\n[7]. Other technologies that are important for the successful\nemergence of the 5G architecture are software-deﬁned net-\nworking (SDN) to separate the data plane and the user plane,\nnetwork function virtualization (NFV) for the virtualization\nof the core network, and the interoperability of software\nfrom various vendors. Advances in optical networking such\nas optical switching and visible light communication (VLC)\nmay also be able to address the capacity requirements.\nThe recent progress in deep learning has led to break-\nthroughs in solving complex problems that have deﬁed so-\nlutions in the artiﬁcial intelligence community for decades.\nDeep learning is good at discovering intricate structures in\nhigh-dimensional data by extracting features in a hierarchical\npattern. Many ﬁelds of science and engineering have embraced\ndeep learning due to its unprecedented performance in dealing\nwith big data. Deep learning has achieved excellent perfor-\nmance in image recognition and processing [8–11], and speech\nrecognition [12–14]. It has performed better than conventional\nmachine learning techniques at predicting drug molecules\nactivities [15], analyzing particle accelerator data [16, 17],\nreconstructing brain circuits [18], and predicting the effects of\nmutations in non-coding deoxyribonucleic acid (DNA) on gene\nexpression and diseases [19–21]. In recent times, deep learning\nperformed well in natural language processing (NLP) tasks\n[22], especially in topic modeling, sentiment analysis, ques-\ntion answering [23] and machine translation [24, 25]. Deep\nlearning has proven beyond doubt to become a source of hope\nfor the future especially with the enormous amount of data\nthat will be produced from the launch of the 5G architecture\nand the Internet of Things (IoT). A unique beneﬁt of deep\nlearning is that it can independently extract features from raw\narXiv:2208.07643v1  [cs.LG]  16 Aug 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n2\ndata unlike traditional machine learning; thus, eliminating the\nneed for human intervention for feature extraction. With the\nhigh rate of ongoing researches to develop better deep learning\nalgorithms and architectures, it is obvious that deep learning\nis a technology that has come to stay.\nIntersecting deep learning with 5G technology is justiﬁable.\nThe enormous amount of data to be generated from hetero-\ngeneous networks (HetNets) in the 5G architecture will be\ncollected from various sources such as smartphones, wearable\ndevices, and self-driving vehicles. These data will not only\nbe complex but also be available in diverse formats (e.g.\nvideo, text, multimedia) [26]. Hence, conventional methods in\nwireless communication become inadequate to handle these\ndata. More so, deep learning will eliminate the necessity for\ndomain expertise in processing the data. The advances in\nelectronics have resulted in high computing power that has\nincreased the speed of execution of deep learning algorithms.\nFor instance, Graphics Processing Unit (GPU)-based parallel\ncomputing can enable deep learning to make good predic-\ntions within milliseconds. Moreover, some business enterprises\nsuch as Amazon and Microsoft own large cloud computing\nresources that are leased at affordable prices for very high\ncomputations. Therefore, computations can be done with better\naccuracy within a short period.\nII. RELATED WORKS\nThe ﬁelds of artiﬁcial intelligence and wireless communi-\ncation have grown independently over the years. However, re-\nsearchers have begun to study the mutual beneﬁts of intersect-\ning both ﬁelds. Deep learning can introduce intelligence into\nthe 5G architecture, while the heterogeneous and massive data\ngenerated from the 5G architecture will result in a research\nproblem to improve the speed and inference of deep learning\nalgorithms. Some survey papers sit at the intersection of both\nﬁelds of artiﬁcial intelligence and wireless communication. We\nclassify these survey papers into four categories: (A) overview\nof the convergence of traditional machine learning and shal-\nlow artiﬁcial neural networks (ANN) with 5G architecture\nand technologies (B) overview of the convergence of deep\nlearning with the current wireless communication schemes\n(C) overview of the convergence of deep learning with a\nspeciﬁc emerging 5G application/technology (D) overview of\nthe convergence of deep learning with 5G architecture and\ntechnologies. The list of acronyms used in this paper is given\nin Table 1.\nA. Overview of convergence of traditional machine learning\nand shallow artiﬁcial neural networks with 5G technologies\nThe authors in [27] reviewed the fundamental concepts of\nmachine learning and proposed how it can be employed in\nthe 5G technologies, which include cognitive radios, massive\nMIMOs, femto/small cells, heterogeneous networks, smart\ngrid, energy harvesting, and device-to-device communications.\nThe authors in [28] reviewed common machine learning algo-\nrithms in cellular networks. They further discussed how these\nmachine learning algorithms can be used in self-organizing\nnetworks (SONs), and the metrics for preferring a machine-\nlearning algorithm to others in SONs. The authors in [29]\nprovided a survey for integrating machine learning and artiﬁ-\ncial learning techniques to enhance the efﬁciency of wireless\ncommunication in the areas of data acquisition, knowledge\ndiscovery, network planning, operation, and management of\nthe future wireless networks. Also, the authors in [30] ex-\nplored the possibilities of ANN-based solution approaches\nto enhancing 5G communications technology. The authors\nin [31] highlighted the beneﬁts and limitations of exploiting\nartiﬁcial intelligence (AI) to achieve intelligent 5G networks,\nand also demonstrated how effective is AI in managing cellular\nnetwork resources. Since this paper is more focused on the\nconvergence of 5G architecture with deep learning, only some\nselected survey works on the convergence of 5G architecture\nwith traditional machine learning and shallow artiﬁcial neural\nnetworks have been discussed.\nB. Overview of convergence of deep learning and current\nwireless communication schemes\nThe authors in [32] studied physical-layer deep learning\napproach to address problems such as modulation recognition,\nradio ﬁngerprinting and medium access control. This requires\nthe domain expertise of wireless communication for the design\nof wireless systems that are spectrum-driven, i.e., that can learn\nthe current state of the spectrum and take optimal actions.\nSimilarly, the authors in [33] surveyed how deep learning\ncan be leveraged to redesign the traditional communication\nsystem (for modulation recognition, channel decoding, and\ndetection), and to replace it with a unique architecture that\nis dependent on an auto-encoder. The authors in [34] studied\nmodel-driven deep learning approaches in physical layer com-\nmunications. Model-driven deep learning combines domain\nknowledge in wireless communication with deep learning to\nreduce the demands for computing resources and to lessen\ntraining time. The authors in [35] categorized deep learning\napplications in the physical layer of wireless communication\ninto systems with or without block structures. The authors\nin [36] gave a comprehensive survey of the applications of\ndeep learning algorithms to different wireless network layers,\nwhich includes modulation/coding in the physical layer, access\ncontrol/resource allocation in the data link layer, path search,\nand trafﬁc balancing in the routing layer.\nC. Overview of the convergence of deep learning and a\nspeciﬁc emerging 5G technology / application\nThe authors in [37] gave a holistic overview of integrating\nmulti-access edge computing with the emerging 5G tech-\nnologies. The authors in [38] surveyed comprehensively the\nconvergence of edge computing with deep learning. They\nconveyed the idea of edge intelligence and intelligence edge\nwhich is derived from such convergence. Edge intelligence\nrefers to the application of deep learning to edge nodes, while\nintelligent edges are the ﬁnal goal of making smart devices\nseamlessly communicate with an intelligent edge. The work in\n[38] holistically investigated the technical spectrum covering\nthe convergence of deep learning and edge computing in terms\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n3\nof ﬁve enablers. Other notable work that sits at the intersection\nof deep learning and multi-access edge computing is found in\n[39–41]. The authors in [39, 40] provided an overview of the\narchitectures, frameworks, and technologies for deep learning\nmodel toward training and inference at the network edge. The\nauthors in [41] gave an overview of federated learning in\nmobile edge computing. In federated learning, the user devices\ntrain a machine learning model with their local data and send\nthe model updates instead of the raw data to the edge server\nfor fusion. Federated learning serves as enabling technology in\nmobile edge networks for training of a machine learning model\nin a collaborative manner, and it also enables the edge network\noptimization using deep learning. Federated learning helps to\nimprove the privacy of data since the end users need not send\ntheir raw data. Also, by sending only model updates, edge\nbandwidth can be efﬁciently used. Recent works on federated\nlearning integrates differential privacy techniques for enhanced\nprivacy [42].\nThe authors in [26] presented an overview and a brief\ntutorial on deep learning in mobile big data analytics and\ndiscussed how to use Apache Spark as a scalable learning\nframework. Also, the authors in [43] provided a summary of\nthe current deep learning architectures and algorithms used in\nnetwork trafﬁc control systems. The authors in [44] surveyed\nhow deep learning can be applied from diverse perspectives\nto empower IoT applications in four representative domains,\nwhich are smart healthcare, smart home, smart transportation,\nand smart industry. Similarly, the authors in [45] surveyed\nhow deep learning methods can be used to develop enhanced\nsecurity for IoT systems. The authors in [46] summarized\nmajor reported research attempts that leveraged deep learning\nin the IoT domain. The author in [47] surveyed how deep\nlearning can be integrated with massive MIMO.\nD. Overview of the convergence of deep learning and 5G\narchitecture and technologies\nThe authors of [48, 49] discussed the convergence of deep\nreinforcement learning and the 5G technologies. In our opin-\nion, the most comprehensive survey article involving the inter-\nsection of deep learning with 5G architecture and technologies\nis [50]. This survey provided a very comprehensive review\nof mobile and wireless networking research from the deep\nlearning perspectives and categorized them by their different\ndomains.\nAll the aforementioned existing survey articles that sit at\nthe intersection of artiﬁcial intelligence and 5G architecture\nhave their shortcomings. The articles in category A did not\napply deep learning techniques. The articles in category B\nare more focused on how deep learning techniques can be\napplied to today’s wireless communication systems, with less\nemphasis on the emerging 5G network. The articles in category\nC are streamlined to how deep learning can enhance a speciﬁc\n5G technology/application. Therefore, they neglected other\n5G technologies where deep learning approaches are also\napplicable. References [48, 49] in category D are focused\nonly on a speciﬁc type of deep learning technique - deep\nreinforcement learning. Reference [50] is not well structured\nto speciﬁcally cover the 5G architecture and its technologies.\nMore so, it does not cover a recent deep learning algorithm -\ncapsule networks. Table 2 presents a comparison of existing\nsurvey papers.\nTherefore, this paper speciﬁcally surveys the convergence of\n5G architecture and deep learning. It starts by reviewing both\nthe 5G architecture and deep learning architecture indepen-\ndently and includes capsule networks that are not present in\nany of the previous survey works. Then, it discusses notable\nresearch works that converge deep learning techniques with\nthe 5G architecture, while covering the key 5G technologies.\nAlso, it discusses the challenges posed by such convergence.\nLastly, it brieﬂy reviews the 6G architecture and discusses how\nthe 6G architecture can converge with deep learning.\nThus, the rest of this paper is divided as follows: Section\nIII discusses the 5G architecture and applications, Section IV\ndiscusses the deep learning architectures, Section V discusses\nthe application of deep learning in the 5G architecture, Section\nVI discusses the potential challenges that may be faced by\nintegrating 5G architecture with deep learning, Section VII\ndiscusses how deep learning can drive the future 6G tech-\nnologies, and lastly, Section VIII is the conclusion.\nIII. 5G ARCHITECTURE AND APPLICATIONS\nIn this section, we brieﬂy discuss the requirements for 5G\narchitecture. Also, we highlight the 5G technologies.\nA. 5G architecture\nThe 5G architecture will consist of the radio network and the\ncloud network [3]. The radio network covers the deployment\nof a UDN which includes the small, micro, pico, and femto\ncells [51]. This means that the 5G UDN will suffer co-channel\ninterference, which will gradually render obsolete the current\nair interface. Therefore, advanced access technology such as\nBeam Division Multiple Access (BDMA), nonorthogonal mul-\ntiple access (NOMA), sparse coded multiple access (SCMA),\nand Filter Bank multi-carrier (FBMC) multiple access must be\nemployed to increase spectral efﬁciency. Massive MIMO must\nbe used to increase network coverage by relying on its beam-\nforming gains. Advanced modulation and coding schemes,\nsuch as 256-quadrature amplitude modulation (QAM) and\nlow parity density check codes (LDPC), can also increase\nspectral efﬁciency and be combined with massive MIMO\nto increase network capacity. Efﬁcient spectrum management\nthrough cognitive radio, and the usage of the higher frequency\nband for mmWave communication will provide the necessary\nbandwidth requirements [52]. The cloud network consists\nof ofﬂoading techniques such as MEC, fog computing, and\ncloud computing. MEC and fog computing will free up the\ncloud space and reduce end-to-end latency in time-critical\napplications such as AR/VR. Cloud-RAN will allow for the\naggregation of tasks and resource pooling, thus, giving rise\nto efﬁcient use of network resources [53]. The decoupling\nof the user plane and the control plane by software-deﬁned\nnetworking can further make small cells ﬂexible and easily de-\nployable. This will allow for seamless interoperability among\ndifferent network providers. Also, it will allow for efﬁcient\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n4\nTABLE I\nCOMPARISON OF EXISTING SURVEY PAPERS\nCategories\nSurvey\nPapers\nSummary of Papers\nMachine learning / shallow\nartiﬁcial neural networks +\n5G/IoT technologies (selected\npapers only)\n[27]\nThis paper reviewed the rudiments of machine learning and proposed how they are used for\nvarious applications in the 5G network. A limitation of the paper is that it did not cover deep\nlearning techniques which are the major drivers of 5G technologies.\n[28]\nThis paper overviewed common machine learning techniques used in self-organizing cellular\nnetworks. This paper is limited to cellular networks only and did not cover deep learning\ntechniques.\n[29]\nThis paper surveyed how artiﬁcial intelligence can be applied to boost the performance of wireless\nnetwork operations. A limitation of the paper is that it did not discuss deep learning techniques.\n[30]\nThis paper explored the potentials of artiﬁcial intelligence-based solutions for 5G technologies.\nThe survey is very limited.\n[31]\nThis paper highlighted the beneﬁts and limitations of exploiting artiﬁcial intelligence to achieve\nintelligent 5G networks. This paper is limited to cellular networks and focused mainly on machine\nlearning techniques.\nDeep learning + current wireless\ncommunication technologies\n[32]\nThis paper discussed the need for deep learning at the physical layer and summarized the current\nstate-of-the-art deep learning-based physical layer architecture. There is a very limited discussion\non the 5G architecture.\n[33]\nThis paper comprehensively overviewed deep learning-based physical layer processing. A\ndiscussion on using autoencoders to redesign the conventional communication system was\npresented. However, there was no discussion on the 5G architecture.\n[34]\nThis paper discussed model-driven deep learning approaches in physical layer communication.\nThe paper is limited to the current wireless communication system.\n[35]\nThis paper overviewed recent advancements in deep learning-based physical layer communica-\ntion. There was no discussion on the 5G architecture.\n[36]\nThis paper comprehensively surveyed the application of deep learning in the current network\nlayers. The paper discussed ten unsolved problems in this area of research. The paper majorly\nfocused on the current physical layer architecture alone.\nDeep learning with a speciﬁc 5G\ntechnology/application\n[26]\nThis paper presented a brief overview on deep learning in mobile big data analytics. It also\ndiscussed how to use Apache Spark as a scalable framework. However, it focused mainly on\nbig data.\n[37]\nThis paper holistically overviewed multi-access edge computing and how deep learning tech-\nniques could be leveraged. The paper is not well-structured to cover deep learning techniques.\nMore so, it is limited to multi-access edge computing.\n[38]\nThis paper comprehensively overviewed deep learning applications in multi-access edge comput-\ning. The concepts of edge intelligence and intelligent edge were thoroughly discussed. However,\nthe survey work is limited to multi-access edge computing only.\n[39]\nThis paper surveyed edge intelligence and presented a deep learning model towards training and\ninference at the network edge. The paper is limited to edge intelligence and does not cover other\n5G technologies.\n[40]\nThis paper overviewed applications where deep learning is used at network edge, and approaches\nto speed up deep learning inference across network devices, edge servers and the cloud. The\npaper is limited to edge computing only.\n[41]\nThis paper reviewed the applications and challenges of federated learning in large-scale and\ncomplex mobile edge computing. The paper is limited to mobile edge computing only.\n[43]\nThis paper overviewed the state-of-the-art deep learning architectures and algorithms used in\nnetwork trafﬁc control. The paper is limited to network trafﬁc control only.\n[44]\nThis paper surveyed the literature on various IoT applications that leveraged deep learning. This\npaper is limited to IoT only.\n[45]\nThis paper comprehensively surveyed machine learning and deep learning techniques that\nenhanced security in IoT systems. This paper covered only IoT systems and excluded other\n5G technologies.\n[46]\nThis paper summarized major research works that leveraged deep learning in the IoT domain.\nIt also surveyed deep learning approaches that support IoT applications in the fog and cloud\ncenter. The survey work is limited to IoT applications only.\n[47]\nThis paper discussed deep learning-based techniques for modeling and estimation of massive\nMIMO channels. The paper is focused on massive MIMO only.\nDeep learning with 5G architecture\nand multiple 5G technologies\n[48]\nThis paper reviewed deep reinforcement learning applications in communications and network-\ning. It covered the 5G networks but did not discuss other deep learning applications.\n[49]\nThe paper overviewed the concepts of deep reinforcement learning and reviewed its application\nto various challenges in 5G networks. The paper is limited to deep reinforcement learning only.\n[50]\nThis paper presented an encyclopedic survey that sits at the intersection of wireless and mobile\ncommunication and deep learning. The paper is not laser-focused on 5G networks, and hence\nnot very well-structured.\nOur pa-\nper\nOur paper laser focuses on the intersection of major 5G technologies with deep learning. A brief\noverview of the convergence of 6G architecture with deep learning is also discussed.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n5\nenergy management, and to minimize over-provisioning and\nunder-utilization of the resource. NFV will virtualize hardware\noperations through software applications, thus, allowing for\nﬂexibility and interoperability of network devices. In addi-\ntion, local ofﬂoad through D2D communications can further\nincrease network throughput [7]. Other technologies such as\nVLC and optical ﬁber communication will also be included\nin the 5G architecture. Figure 1 is an illustration of the 5G\narchitecture.\nB. 5G applications\nThere are varieties of emerging applications that are ex-\npected to be the major driver behind the commercial de-\nployment of the 5G architecture. Such applications will be\nbeneﬁcial to a wide range of public and private sectors,\nwhich includes among others, energy, agriculture, infrastruc-\nture, health care, manufacturing, and transport. We brieﬂy\ndiscuss the emerging applications of the 5G architecture below.\n1) device-to-device (D2D) communication: The prolifera-\ntion of wireless devices may cause wireless congestion at the\nsmall cells (base stations). D2D communication allows for a\nlocal ofﬂoad without traversing the small cells [54].\n2) machine-to-machine (M2M) communication: It is ex-\npected for the 5G technologies to introduce intelligence to lots\nof machines. Vehicles will be intelligent enough to commu-\nnicate among themselves. Interference management schemes\nand the mmWave technology that provides a large bandwidth\nwill be the driver of the M2M communication [55].\n3) Internet of Things (IoT): IoT includes ”anything” to the\n”anytime” and ”anywhere” promises of 5G architecture. The\nIoT application will produce big data that will be difﬁcult\nto manage. Latency, energy efﬁciency, and interference are\npossible challenges. Technologies in the 5G architecture to\novercome these challenges will be mobile edge computing,\ncloud computing, and fog computing.\n4) Smart\ninfrastructure:\nSmart\ninfrastructure\nincludes\nsmart homes, smart grids, intelligent transportation, and smart\ncities. The 5G RAN and cognitive radio technologies promise\nto satisfy the requirements for a smart infrastructure.\n5) Intelligent healthcare: Launching of the 5G architecture\nwill allow for robotic applications that will monitor patients’\nhealth and records in real-time. Communication with patients\nand drug prescriptions will be done with minimal human\nintervention.\nOther applications are wearable devices, smart industries,\nthe internet of vehicles (IoVs), smart banking, gaming, and\nsocial media, etc.\nIV. DEEP LEARNING ARCHITECTURES\nDeep learning is a representative-based learning technique\nwhere a cascade of multiple layers of nonlinear processing\nunits extract features. The lower layers of the deep learning\narchitecture learn simple features, while higher layers learn\ncomplex features, thus forming a hierarchical and power-\nful feature representation. Deep learning approaches can be\ncategorized into supervised, semi-supervised or partially su-\npervised, unsupervised and the deep reinforcement learning\n(DRL). Convolutional neural networks (CNN), recurrent neu-\nral networks (RNN), and recursive neural networks (CNN)\nare the supervised deep learning methods. Autoencoders and\nrestricted Boltzmann machines (RBM) are unsupervised deep\nlearning methods. Generative Adversarial Network (GAN) is\nthe semi-supervised deep learning method. Deep Q-learning\n(DQL) is the deep reinforcement learning method. Deep\nlearning techniques are deep neural networks (DNNs), with\nmultiple layers of neurons lying between the input and the\noutput.\nDeep learning has produced state-of-the-art results in many\napplication domains, such as computer vision, speech recog-\nnition, NLP [22, 56]. The increasing interest in deep learning\nis due to the availability of high computing power caused\nby advances in solid-state electronics, the availability of huge\ntraining data which will further increase as 5G technologies\nevolve, and the power and ﬂexibility of learning intermediate\nrepresentations [57].\nA. Convolutional Neural Network\nThe structure of the CNN is inspired by the neurons in\nanimal and human brains. The three main advantages of CNN\nover shallow neural networks are parameter sharing, sparse\ninteractions, and equivalent representations. Computational\ncomplexity is reduced by parameter sharing because local con-\nnections and shared weights are used, leading to the training\nof fewer parameters. The CNN architecture is such that each\nlayer of the network transforms one volume of activations\nto another using a differentiable function. The architecture\nconsists of three main layers which are the convolutional layer,\nthe pooling layer, and the fully connected layer.\nSuppose the input data consist of pixels of dimension\nW1×H1×D1 and convolved with a ﬁlter of size F ×F ×D1,\nwhere the number of strides is S and the number of zero\npadding is Z. Suppose there are K numbers of such ﬁlters.\nAfter the ﬁlters have scan a local portion of the input image,\nthe obtained feature map will be of dimension W2 ×H2 ×D2,\nwhere W2 = ((W1 −F + 2Z)/S) + 1, H2 = ((H1 −\nF + 2Z)/S) + 1 and D2 = K. For a max pool layer that\ncomes after the ﬁrst convolutional layer, the output will be of\ndimension W3 × H3 × D3, where W3 = ((W2 −F)/S) + 1,\nH3 = ((H3 −F)/S) + 1 and D3 = D2. There are usually\nmany convolutional and max pool layers before the fully\nconnected layers. The fully connected layers are hidden layers\nfor the high-level abstraction of the images and each has\nthe same dimension as the previous layer ahead of it. The\nsoftmax-output layer is used for classiﬁcation in supervised\nlearning. Discarding pooling layers are important in many deep\nlearning architectures such as capsule networks, variational\nautoencoders (VAEs), and generative adversarial networks\n(GANs). Future deep learning architectures will most likely\nnot contain any pooling layers.\nCNN has been widely used to obtain good performance\nin different applications such as NLP, speech recognition,\ncomputer vision [58], and post-translational modiﬁcation in\nproteins [59], to name a few. Several architectures in the ﬁeld\nof convolutional neural networks has been given a name such\nas LeNet [60], AlexNet [8], ZFNet [61] and GoogLeNet [11].\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n6\nFig. 1.\n5G Architecture\nB. Capsule Networks\nThe capsule network (capsnet) is an improvement to the\nCNN, where the orientation of a 3D object is captured for\nproper classiﬁcation [62, 63]. In the capsule network, there\nis a pose relationship between the simple features of lower\nlayers that make up a higher layer feature. This is referred to\nas equivariance. CNN does not possess equivariance, hence it\ncannot correctly identify a 3D object as same if pictured from\ndifferent angles. The capsule network uses a dynamic routing\nalgorithm for routing the input vector from lower layers to the\nhigher layer with a similar representation. A nonlinear squash\nfunction is applied to the output vector to keep the length of\nthe vector as a probability and preserve the orientation of the\nvector. The dynamic routing algorithm is shown in Algorithm\n1.\nAlgorithm 1 shows how the input vector ˆuj|i of each capsule\ni in layer l is been routed to the capsule j in the higher\nlayer l + 1 to produce an output vector vj. The parameter\nbij is initialized to zero, so that there can be a uniform\ndistribution representing maximum randomness through the\nsoftmax function. The squash function is given as\nsquash(sj) =\n||sj||2\n1 + ||sj||2\nsj\n||sj||.\n(1)\nThe symbol ||·|| represents the L−2 norm. The loss function\nLc for capsnet is different from the popular cross-entropy loss\nfunction used in CNN, and it is given as\nLc = Tc max(0, m+−||vc||)2+λ(1−Tc) max(0, ||vc||−m−)2.\n(2)\nSince that capsnet is a deep supervised learning architecture,\nthe parameter Tc is 1 when the output of the capsnet correctly\nmatches the label and 0 otherwise. Also, the parameter m+ is\ncommonly ﬁxed at 0.9, and m−is commonly ﬁxed at 0.1.\nThe capsnet has been successfully used for feature ex-\ntraction of 28 × 28 pixels MNIST dataset with less training\nparameters than CNN [64].\nC. Recurrent Neural Network\nRecurrent Neural Network (RNN) is popular for processing\nsequential information. It is thought of as multiple copies of\nthe same network, each network passing an information to a\nsuccessor. The length of its time steps is determined by the\nlength of the input. Unlike the feedforward neural network,\nwhere different parameters are used at different layers, RNN\nshares the same parameters across all time steps. It performs\nthe same task at each step with different inputs; thus, reducing\nthe total number of parameters for training. At each time step,\nthe hidden layer vector ht and the output yt is computed\nmathematically as follows:\nht = f(whxt + uhht−1 + bh)\n(3)\nand\nyt = g(wyht + by)\n(4)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n7\nAlgorithm 1: Dynamic Routing Algorithm\n1. Procedure: ROUTING(ˆuj|i, r, l)\n2. for all capsule i in layer l, and capsule j in layer l + 1 : bij ←0\n3. for r iterations do:\n4. for all capsule i in layer l : ci →softmax(bi)\n5. for all capsule j in layer l + 1 : sj ←P\ni cij ˆuj|i\n6. for all capsule j in layer l + 1 : vj ←squash(sj)\n7. for all capsule j in layer l + 1 : bij ←bij + ˆuj|i.vj\n8. Return\nvj\nwhere xt is the input vector at time t, ht is the hidden layer\nvector at time t, and yt is the output vector at time t. The\nparameters wh and uh are the weight vectors, and bh and by\nare the biases. Also f and g are the activation functions which\nmay be the tanh and the softmax functions respectively. The\nmain issue with the RNN is its sensitivity to both vanishing\nand exploding gradients [65]. Exploding gradient is not much\nof a serious problem because the exploding gradients can\nbe clipped at a particular threshold. However, the vanishing\ngradient tends to be a more serious problem that affects long-\nterm dependencies in RNN. RNN is widely used for sentiment\nclassiﬁcation [66] and machine translation [67] in NLP. There\nare special types of RNN such as the bidirectional and deep\nbidirectional RNN which is based on the idea that the output at\neach time-step may depend not only on the previous elements\nbut also on the next elements for sentence classiﬁcation. To\novercome the problem of vanishing gradient, the long-short\nterm memory neural network (LSTM), which is also another\nspecial version of the RNN was developed [68, 69] . A\nsimpliﬁed model of the LSTM is the gated recurrent unit\n(GRU) [70].\nD. Recursive Neural Network\nRecursive Neural Network (RecNN) is seen as a generaliza-\ntion of the recurrent neural network that can make predictions\nin a hierarchical or tree-like structure, as well as classify the\noutputs using compositional vectors. It generates the parent\nstructure in a bottom-up manner, and it is commonly used for\nsentence classiﬁcation in sentiment analysis [71].\nE. Autoencoders\nThe autoencoder (AE) is used for unsupervised deep learn-\ning with efﬁcient data encoding and decoding. It has three\nlayers, namely the input layer, the hidden layer, and the\noutput layer. Its operation consists of the encoding phase\nand the decoding phase. In the encoding phase, the input\nvector is mapped to a lower-dimensional features space in\nthe hidden layer by an encoding function. This approach\nis repeated until the desired feature dimensional space is\nreached, resulting in lossy compression. In the decoding phase,\na decoding function reconstructs the original input vector from\nthe lower-dimensional features. Thus, the autoencoder is used\nfor dimensionality reduction, compression, and fusion.\nA special type of autoencoder is the denoising autoencoder\n(DAE). In the DAE, the input vector is stochastically corrupted\nand the goal is to denoise the corrupted input vector. That\nis, to minimize a denoising reconstruction error. Thus, the\nDAE forces the hidden layer to learn more robust features and\nprevents it from simply learning and replicating a noiseless\ninput vector [72].\nVariational autoencoder (VAE), unlike traditional autoen-\ncoders, performs well in generative processes. This is because\nthe encoder of a VAE outputs Gaussian distribution, which\nis an approximation of a posterior distribution, rather than\na single point like in a traditional autoencoder. Thus, the\noutput of the encoder of a VAE is said to be continuous.\nA latent variable is sampled from this distribution. More\nso, a VAE minimizes not only the reconstruction loss but\nalso the Kullback-Leibler divergence between the probability\ndistribution from the encoder and the normal distribution.\nThis way, a VAE can cluster similar objects together and still\nmaintain completeness [73, 74]. The loss function of a VAE\nis given as\nli(θ, φ) = −Ez∼qθ(z|xi)[log pφ(xi|z)] + KL(qθ(z|xi)||p(z))]\n(5)\nwhere li(θ, φ) is the loss function of the i −th datapoint xi.\nThe latent variable is z; θ are the weights and biases of the\nencoder, while φ are the weights and biases of the decoder.\nThe expected negative log-likelihood of the i−th datapoint in\nthe ﬁrst part of the loss function represents the reconstruction\nloss, while the Kullback-Liebler divergence represents the\nregularization.\nF. Restricted Boltzmann Machines\nRestricted Boltzmann machine (RBM) is a generative shal-\nlow neural network that can learn a probability distribution\nover a set of input [75]. When stacked together and ﬁne-\ntuned with gradient descent and backpropagation algorithm,\nthey form a deep belief network [76]. The RBM can be\nused for dimensionality reduction, collaborative ﬁltering, topic\nmodeling, and feature learning. Although RBM falls into the\nclass of deep unsupervised learning, it can be trained to\nperform classiﬁcation in supervised learning. The RBM has\nonly two layers, namely the visible layer and hidden layer,\nwhose nodes are connected across each other in a symmetric\nbipartite fashion. There is no connection among the nodes\nin either the visible layer or the hidden layer. Unlike an\nautoencoder, the RBM has two biases which are the input\nbias and the hidden bias. The goal of the RBM is to minimize\nthe reconstruction loss and the Kullback-Leiber divergence. In\nrecent times, RBMs have been replaced with VAEs and GANs.\nG. Generative Adversarial Network\nThe GAN consists of two neural networks, the generator\nmodel and the discriminator model. The generator model\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n8\nmaps from a latent space with noise to a fake distribution of\ninterest. The generator model aims at successfully deceiving\nthe discriminator, whose goal is to distinguish between the\ngenerated fake distribution and the true distribution from a\nknown dataset. Thus, the discriminator acts as a classiﬁer. The\nweights and biases of both the generator and discriminator\nare trained with the backpropagation algorithm. Typically, the\ngenerator is a deconvolutional neural network, whose pooling\nlayer is replaced with an upsampling layer, and the discrimi-\nnator is a convolutional neural network. The adversarial nature\nof GAN forms a game setting where both the generator and\nthe discriminator are trying to reach a Nash equilibrium in a\ntwo-player minimax game. This is represented as\nmin\nG max\nD V (D, G) = min\nG max\nD Ex∼pdata(x)[log D(x)]+\nEz∼pz(z)[log(1 −D(G(z)))]\n(6)\nwhere V (D, G) is the value function; D represents the dis-\ncriminator; G represents the generator; pz(z) is the prior of the\ninput noise (latent) variable; D(x) represents the probability\nthat x came from the true distribution pdata(x) rather than\nthe generator’s distribution pg; and G(z) represents the fake\ninformation from the generator.\nGAN was ﬁrst designed by Ian Goodfellow [77], and it\nis said to be one of the most important (if not the most\nimportant) discoveries in the ﬁeld of deep learning in the\nlast ten years. There are many variants of the GAN such as\nCSGAN [78], infoGAN [79], LOGAN [80], cGAN [81] and\nso on. A complete survey on GAN is found in [82]. GAN has\nvarious applications such as image processing and computer\nvision, object recognition, video applications, natural language\nprocessing, data science, and even in the medical ﬁeld. Al-\nthough GAN falls into the class of deep semi-supervised\nlearning, it can be used for classiﬁcation in deep supervised\nlearning, and can also be used in deep reinforcement learning.\nH. Deep Reinforcement Learning\nDeep reinforcement learning (DRL) combines reinforce-\nment learning with deep learning to create an efﬁcient algo-\nrithm that has wide applications in games, robotics, NLP and\ncomputer vision, etc. The DeepMind’s AlphaGo algorithm that\nuses a deep Q-network (DQN) and beats the human world\nchampion of the Go game is a notable example.\nThe DQN uses deep neural networks to replace the Q-table\nof the primitive Q-learning algorithm in a higher dimensional\nstate space [83]. The Q-learning algorithm [84], is a model-\nfree algorithm that calculates the quality of a state-action\ncombination, and can be derived from the Bellman optimality\nequation [85]. The Q-learning update rule is sequential and\ngiven as follows\nQ(st, at) ←Q(st, at) + η(yt −Q(st, at))\n(7)\nwhere η is the learning rate; st is the state at time t; and at\nis the action taken at time t; Q(st, at) is the Q-value of the\nstate-action pair at time t. The temporal difference target yt\ncan be simpliﬁed as\nyt = rt + γ max\na\nQ(st+1, a)\n(8)\nHere, rt is the reward obtained from applying the action at\nin the state st; and γ is the discount factor. The difference\nbetween yt and Q(st, at) in (7) is the temporal difference.\nThe two important changes introduced to the primitive Q-\nlearning by DQN are the experience replay memory to allow\nfor training on a random batch of previous experiences drawn\nuniformly at random from the storage distribution U(D), and\nthe target network to solve the problem of instability of the\ntarget due to its correlation with the current model estimate.\nThis makes the DQN use two neural networks for training.\nThe ﬁrst is the Q-network with parameter θi for sampling\nthe experience replay memory at iteration i, and the second\nis the target network with parameter θ−\ni\nat iteration i. The\nparameters from the Q-network are copied and updated to the\ntarget network every C steps in an online learning fashion.\nThe loss function at iteration i is given as\nLi(θi) = E(s,a,r,s′)∼U(D)\n\u0014\n(yDQN\ni\n−Q(s, a; θi))2\n\u0015\n(9)\nand\nyDQN\ni\n= r + γ max\na′ Q(s′, a′; θ−\ni ).\n(10)\nThere are other notable variations of the DQN such as the\nDouble-DQN (DDQN) [86] and the weighted-DQN (WDQN)\n[87].\nV. INTEGRATING DEEP LEARNING INTO 5G\nARCHITECTURE\nDeep learning has been applied in the literature to the\nvarious technologies that make up the 5G architecture. In\nthis section, we will review the major papers discussing the\napplications of deep learning to the key 5G enabling tech-\nnologies. Speciﬁcally, we divide this section into the following\nsubsections:\n1) Deep learning applications to massive MIMO.\n2) Deep learning applications to multi-access edge comput-\ning.\n3) Deep learning applications to the Internet of Things.\n4) Deep learning applications to software-deﬁned network-\ning and network virtualization.\nA. Deep Learning Applications to Massive MIMO and\nmmWave\nMassive MIMO and mmWave are key enabling technologies\nfor the 5G architecture. This is due to the very high data rates\nand multiplexing gains promised by these technologies. These\ntechnologies require a large number of antennas and thus, large\nchannel matrices for training. Training large channel matrices\ncause computational problems and coordination overhead.\nThese have motivated the use of deep learning techniques that\nleveraged low-overhead features of the environment to predict\nmassive MIMO and mmWave channels with high accuracy,\nand enable coordination of base stations with low complexity.\nHowever, the use of deep learning techniques for massive\nMIMO and mmWave technologies requires a generic dataset\nthat can be ﬁne-tuned for various wireless environments and\nchannel conditions. The dataset needed for wireless settings\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n9\nis complex than that for computer vision or NLP, where the\ndataset is ﬁxed and need not be adjusted to meet certain\nconditions. The author in [88], generated a generic deepMIMO\ndataset from Remcon Wireless Insite Raytracing simulator\n[89]. The deepMIMO channels constructed from the simulator\ncapture its dependence on the environment, and the transmit-\nter/receiver location. The number of antennas, orthogonal fre-\nquency division multiplexing (OFDM) subcarriers, and several\nchannel paths can be ﬁne-tuned to be used with any supervised\ndeep learning algorithm.\nIn frequency division duplexing (FDD) networks that utilize\nmassive MIMO to perform precoding, downlink channel state\ninformation (CSI) is estimated and fed back from the user\nequipment (UE) to the base station causing excessive over-\nhead. Traditional wireless communication approaches, such as\ncodebook-based approaches, quantization vector, and compres-\nsion sensing, have been proposed to reduce the feedback over-\nhead. When the number of antennas increases, it complicates\nthe codebook and quantization vector approaches, also when\nthere is a model mismatch, the compression sensing approach\nexperiences difﬁculty. Hence, deep learning-based approaches\nhave been developed to mimic compression sensing, yet with\nbetter performance. The CsiNet [90] feedback network uses an\nautoencoder-like architecture, where the encoder obtains code-\nwords by learning channel structures from training data, while\nthe decoder recovers the CSI through a one-off feedforward\nmultiplication. CsiNet-LSTM extends this method to handle\ntime-correlation in time-varying channels. The CsiNet-LSTM\n[91] uses a recurrent convolutional neural network (RCNN),\nwhich combines a CNN and an RNN. The CNN extracts\nspatial features, while the RNN interframes correlations. Also,\nthe authors of [92] proposed DualNet-MAG and DualNet-ABS\ndeep learning-based feedback CSI framework that employs bi-\ndirectional channel reciprocity and limited uplink feedback for\nCSI estimation. In [93], the authors proposed Bi-LSTM (bi-\ndirectional LSTM) and Bi-ConvLSTM using both CNN and\nLSTM, to decompress and recover the CSI in single-user and\nmulti-user scenarios in massive MIMO.\nEven though dynamic time division duplexing (TDD) has\nbeen considered better than FDD for 5G UDN, it relies\non conventional radio resource management schemes such\nas coordinated multipoint (CoMP). The challenge posed by\nemploying conventional techniques is that they cannot cope\nwith the unprecedented trafﬁc envisaged in the 5G network.\nAlthough CoMP improves cell edge UE’s throughput and\nconnection stability, it cannot assist a small cell to learn the\ntrafﬁc pattern in neighboring cells. More so, small cells are\nfaced with the challenge of intelligently changing their uplink\nand downlink conﬁguration before imminent congestion, to\nalleviate queue occupancy. Deep learning-based approaches\nhave been proposed to introduce smartness into small cells.\nThe authors in [94] proposed an LSTM-based approach that\npredicts congestion in the future based on past and present\ntrafﬁc data. The authors in [95] also proposed a DNN algo-\nrithm for trafﬁc management.\nResource allocation is important in massive MIMO to deal\nwith inter-user interference and pilot contamination. Spectral\nefﬁciency is maximized by jointly optimizing data and pilot\npower. The use of mathematical methods is always compu-\ntationally inefﬁcient. Deep learning can be used to approx-\nimate functions without closed-form solutions according to\nthe universal approximation theorem [96]. The authors in\n[97] constructed a deep fully-connected neural network for\nmaximizing spectral efﬁciency by allocating transmit power\nto a few users. The authors in [98] designed a DNN that uses\nstatistical CSI to predict transmit powers in a massive MIMO\nsystem, where fading is spatially correlated. The authors in\n[99] developed a deep learning algorithm called PowerNet to\nallocate power in a massive MIMO system with a dynamically\nvarying number of active users.\nSupporting highly mobile applications such as AR poses a\nchallenge to mmWave technology. This is because mmWave\ncan be easily blocked by small particles as small as the\nsize of rain droplets. Also, the frequent hand-off in UDN\nwill introduce control overhead and latency. The authors in\n[100] proposed an integrated deep learning and beamforming\nstrategy that enables high-mobile applications in large antenna\narray mmWave systems.\nB. Deep Learning Application to Multi-Access Edge Comput-\ning\nMEC is a promising technology for the 5G HetNets since it\nprovides a reliable supplement to the cloud center. Routing\ndata between the cloud center and end devices are faced\nwith the challenges of latency, scalability, and privacy. MEC\naddresses these challenges. The proximity of edge servers\nto end devices decreases latency. To address the issue of\nscalability, MEC utilizes a hierarchical structure consisting\nof end devices, edge nodes, and cloud servers to provide\ncomputing resources. MEC facilitates the use of federated\nlearning to provide privacy requirements.\nDeep learning architectures are required at edge nodes\nand edge devices, such as smartphones and GPUs for many\napplications. Computer vision tasks, such as vehicle detection\nand surveillance video systems are processed at small cells\nedge servers due to the sensitive information they contain.\nVigil [101], VideoEdge [102] and Amazon DeepLens [103],\nare examples of computer vision system being processed at\nthe edge nodes. NLP tasks involving deep learning algorithms\nare processed at edge devices, such as wakewords detection in\nAmazon Alexa [104] and Siri [105]. In VR, deep learning is\nused to detect objects of interest in the ﬁeld of view of users.\nIn AR, deep learning is used to predict the ﬁeld of view of\nusers and impose on them a virtual overlay. Deep learning can\nbe used for detecting malicious packets at the edge nodes, and\nalso to make a real-time decision on where packets should be\nsent.\nOfﬂoading high computations from resource-constrained\nedge devices to resource-rich edge nodes deployed at the\nbase station or roadside unit (RSU) will free up spaces in\nedge devices [38]. Thus, end-users will have higher QoE due\nto fast computations especially in gaming applications. This\nmeans that DNNs are required at the edge nodes to process\nsuch computations. The inference can be downloaded by edge\ndevices. Edge nodes computing resources can be shared for\nprocessing ofﬂoaded DNN tasks from many edge devices.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n10\nDeep learning is also useful for in-network caching [40].\nIn-network caching is used to serve multiple users in the\nsame geographical location requesting the same contents. To\navoid latency and congestion of the cloud server, caching\nsuch content is required. In caching systems, deep learning\nis either used for content popularity prediction by supervised\ndeep learning algorithms or to decide a caching policy using\ndeep reinforcement learning.\nThere are three main methods for providing fast infer-\nence in multi-access edge computing with deep learning\nfor high QoE. The ﬁrst method is to ensure fast inference\non resource-constrained edge devices executing DNNs. To\nimprove computational speed and still maintain accuracy in\nresource-constrained devices, training parameters can be min-\nimized using techniques deployed in computer vision. Such\ntechniques include YOLO [106], MobileNets [107], Solid-\nState Drive (SSD)[108] and SqueezeNet [109]. Compressing\nthe DNN model is also a viable approach, although accu-\nracy slightly reduces. Examples of compression techniques\nare knowledge distillation, parameter pruning, and parameter\nquantization. Knowledge distillation [110] creates a smaller\nDNN that imitates the characteristics of a bigger DNN.\nParameter pruning removes the least important parameters.\nParameter quantization converts ﬂoating numbers to numbers\nwith low-bit width. Some works combined some of these\ntechniques such as Adadeep [111] and DeepMon [112]. High-\nperforming processors in computer processing devices (CPUs)\nand GPUs, together with custom-made application-speciﬁc\nintegrated circuits (ASICs) have made edge devices suitable\nfor performing deep learning computations. Examples of these\nare Nvidia’s EGX platforms [113], Intel’s CPU, GPU, Field\nProgrammable Gate Arrays (FPGAs) and Google’s Tensor\nProcessing Units (TPUs) [114].\nThe second method to improve deep learning computational\nspeed in multi-access edge computing is to ofﬂoad tasks from\nedge devices to one or more edge servers. Techniques such as\ndata pre-processing and resource pooling can be utilized. In the\ndata pre-processing technique, redundant contents are removed\nbefore ofﬂoading to the edge servers. In resource pooling,\nmany edge devices share the same edge server resource,\nthus, maximizing bandwidth efﬁciency. Some of the works\non resource sharing are VideoStorm[115], Chameleon [116],\nMainstream [117]. These works study the trade-off between\nlatency, accuracy, and other performance metrics.\nThe third method involves intelligent ofﬂoading across any\ncombination of edge devices, edge servers, and the cloud cen-\nter. This method can be any of the following: binary ofﬂoading,\npartial ofﬂoading, or distributed ofﬂoading. Binary ofﬂoading\nis a decision either to ofﬂoad all DNN computations or not.\nRecent techniques for binary ofﬂoading are DeepDecision\n[118] and MCDNN [119]. Partial ofﬂoading is a decision\non what fraction of DNN computations is to be ofﬂoaded.\nSome part of the computations may be done at the edge\ndevices before being sent over to the edge servers to reduce\nlatency. A work using this technique is Neurosurgeon [120].\nDNN computations can be shared among the edge devices,\nedge servers and the cloud as in DDNN [121]. In distributed\nofﬂoading, DNN computational tasks can be shared among\nneighboring edge devices with available computing resources\nto avoid routing the edge servers or cloud. Examples of such\ntechniques are DeepThings [122] and MoDNN [123].\nC. Deep Learning Applications to the Internet of Things\nWireless sensors are expected to be incorporated into the\nIoT and programmed with intelligence to provide a wide range\nof applications over the Internet. Thus, the network envi-\nronment becomes complicated, and communication resources\nbecome scarce. More so, IoT gateways will be faced with\ncongestion and bandwidth challenges arising from the big data\ngenerated from ubiquitous IoT devices [124, 125].\nApplying deep learning to IoT devices can enhance their\nsensing and prediction capabilities. The authors in [126]\ndiscussed four key research questions that need to be answered\nfor seamless interaction between IoT and deep learning capa-\nble of meeting the demands of the 5G HetNets. The research\nquestions are: (1) What are the deep learning architectures\ncapable of processing and fusing sensitive input data for diver-\nsiﬁed IoT applications? (2) How can the resource consumption\nof deep learning models be reduced, so that they can be\nefﬁciently deployed at resource-constrained IoT devices? (3)\nHow to guarantee conﬁdence from the predictions of deep\nlearning models deployed at IoT devices? (4) How to minimize\nthe need for labeled data for deep learning prediction?\nSome research works focus on answering the above-\nmentioned questions. A uniﬁed and customizable deep learn-\ning platform called DeepSense [127] was developed to meet\nthe requirements of many IoT applications. The DeepIoT\n[128] can compress deep learning parameters, so that deep\nlearning architectures can be deployed in resource-constrained\nIoT devices. RDeepSense [129] was also developed to provide\nreliability assurance to deep learning predictions in IoT de-\nvices. Finally, unsupervised and semi-supervised deep learning\narchitectures such as GANs have lowered the dependence on\nlabeled data.\nIt is important to note that the big data generated from\nthe proliferation of IoT devices will be susceptible to noise,\nbias and some anomalies [126]. Therefore, it is expected\nfor deep learning algorithms to provide reliable and accurate\ninference regardless of these. Also, deep learning architectures\nare expected to deal with real-time data streams requiring\nfast processing. An example of such IoT application is in\nsmart cities [130]. An important aspect of smart cities is smart\nenergy management. The authors in [131–134] applied deep\nlearning techniques to predict energy usage in smart cities.\nTrafﬁc management in smart cities is also an important area\nwhere fast processing deep learning algorithms are required\n[135]. Self-driving cars are embedded with numerous sensors\nthat require deep learning algorithms with fast processing for\ntimely decision-making [136]. Also, smart security systems\nat homes, and real-time image processing applications using\nCNN require deep learning algorithms with fast computational\nspeed and have been discussed in [137].\nA challenge to be encountered in the IoT deployment is\nsecurity threats. IoT systems are prone to passive attacks such\nas eavesdropping, and active attacks such as spooﬁng, man-\nin-the-middle, Sybil, and malicious inputs [45]. Deep learning\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n11\nalgorithms have demonstrated robustness against these secu-\nrity threats. For instance, CNN was used to detect malware\nintrusion in Android devices [138]. RNN was used to detect\nmalicious attacks in network trafﬁc [139]. Similarly, auto-\nencoders were used to detect malicious attacks in network\ntrafﬁc [140]. A combination of auto-encoders and deep belief\nnetworks was used to detect both malware and malicious\ncodes [141]. The authors in [142] proposed a GAN to provide\nsecurity in IoT cyberspace and to detect abnormal system\nbehavior [142].\nD. Deep Learning Applications to Software Deﬁned Networks\nand Network Virtualization\nSDN is a network paradigm that decouples the control\nplane from the data plane. Thus, software-based controllers,\nwhich serve as the control plane, are logically centralized\nto provide network intelligence. On the other hand, net-\nwork devices, which serve as the data plane, become simple\npacket forwarding devices easily programmable through open\ninterfaces [143]. NFV virtualizes network functions (NFs)\nby shifting NFs from dedicated hardware to software-based\napplications running on commercial off-the-shelf equipment\n[144, 145]. This way, CapEx and OpEx are reduced. Deep\nlearning has been used to address some challenges peculiar\nto SDN. These challenges are trafﬁc classiﬁcation, routing\noptimization, resource management, and security [146].\nIn trafﬁc classiﬁcation, the authors in [147] identiﬁed mobile\napplications using deep neural networks. The ﬂow features\nused for training an 8-layer deep neural network were des-\ntination address, destination port, protocol type, packet size,\nand TTL. The authors in [148] used multi-layer perceptron\n(MLP), stacked AE, and CNN for real-time trafﬁc monitoring\nand classiﬁcation in an SDN controller. The authors in [149]\nproposed a novel deep learning algorithm for prospective\ntrafﬁc load and congestion. The deep learning architecture\nconsists of a deep belief network and a CNN. The authors\nin [150] applied DRL to control multimedia trafﬁc without\nany mathematical model.\nIn routing optimization, The authors in [151] applied DRL\nto optimize routing, where the goal is to optimize rout-\ning among all source-destination pairs given trafﬁc matrix.\nAlso, the authors in [152] proposed an LSTM-based network\nframework called NeuTM for predicting network trafﬁc. They\nobtained real-world trafﬁc data from the GEANT backbone\nnetwork for training [153]. The authors in [154] used deep\nQ-learning to jointly optimize the computational capabilities\nof blockchain nodes and controllers. Similarly, the authors\nin [155] developed TDRL-RP deep Q-learning framework\nfor deciding immediate trust path for long-term rewards in\nvehicular ad-hoc networks (VANETs).\nIn the area of resource management, the authors in [156]\nused DRL to optimize the joint resource allocation in software-\ndeﬁned virtualized VANETs. Similarly, [157] optimized joint\nresource allocation in smart cities. The authors in [158] used\na special type of RNN called ESN for predicting the content\nrequest distribution and mobility pattern of each user; thus\noptimizing QoE and transmit power in cache-enabled Un-\nmanned Aerial Vehicles (UAVs). The authors in [159] applied\nLSTM in the study of Service Level Agreement (SLA) in\nSDN. The authors in [160] proposed a DRL algorithm called\nDeepRM to solve the problem of packing tasks with multiple\nresource demands in SDN. The proposed algorithm had a fast\nconvergence. The authors in [161, 162] applied DRL to solve\nresource management problems in network slicing.\nIn the area of security, the author in [163] used DNNs to\ndetect network intrusion by classifying trafﬁc ﬂow into normal\nand anomaly. The SDN controller collects ﬂow statistics\nfrom all switches and forwards them to the deep learning-\nbased detection system. The DNN was trained with NSL-\nKDD datasets [164]. On detecting network intrusion, the SDN\ncontroller uses the OpenFlow protocol to adjust the switches’\nﬂow table to avoid intrusion attacks. The authors in [165]\nproposed a Gated Recurrent Unit RNN (GRU - RNN) algo-\nrithm for intrusion detection in an anomaly-based intrusion\ndetection system (IDS). The author in [166] developed a novel\nIDS called NDAE to speed up intrusion detection without\ncompromising on accuracy. The author in [167] applied a com-\nbination of CNN and RNN for detecting distributed denial-of-\nservice attacks (DDoS). The authors in [168] applied stack\nautoencoder for detecting DDoS in SDN environments. The\ntraining was based on the NSL-KDD dataset with ﬁve class\nclassiﬁcations. The authors in [169] proposed ATHENA, a\nframework for scalable anomaly detection in SDN. ATHENA\nis deployable in large-scale SDN because it offers a wide range\nof network features and detection mechanisms. The authors\nin [170] applied CNN, AE, and RNN to detect anomaly-\nbased intrusion. The training set was the NSLKDD dataset\nand the test sets were NSLKDD+ and NSLKDDTest21. A\ncomprehensive survey on the convergence of deep learning\nwith SDN/NFV can be found in [146, 171, 172].\nVI. CHALLENGES FACED BY THE CONVERGENCE OF DEEP\nLEARNING AND 5G TECHNOLOGIES\nConverging deep learning with 5G technologies comes with\nits challenges. Some of the challenges are:\n• Deep learning algorithms are easily fooled [173, 174].\nThis also includes deep reinforcement learning [175].\nAttackers can distort the training data by introducing\nsamples that will steer the deep learning algorithms\ntowards a wrong prediction in classiﬁcation problems.\nFor instance, the authors in [174] crafted white-box and\nuniversal black-box adversarial attacks to reduce accuracy\nfor deep learning-based radio signal classiﬁcation. The\nauthors showed how the attackers construct small power-\nful perturbations to distort radio signal classiﬁcation.\n• Deep learning algorithms are black-boxes hard to inter-\npret [176]. This poses a challenge in wireless communica-\ntion, where well-understood mathematical concepts have\nbeen developed and have stood the test of time. Also,\ndeep learning algorithms are designed to solve a speciﬁc\ntask such as computer vision and may perform poorly\non complex tasks associated with dynamically changing\nwireless environments [177].\n• Deep learning is heavily dependent on data. Although,\nthis is beneﬁcial in 5G architecture, where an enormous\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n12\nvolume of data will be produced. The cost of acquir-\ning such enormous data and processing the data with\nGPUs and cloud computing technologies may override\nthe beneﬁts. More so, the acquired data may require\nsome expensive pre-processing techniques. For instance,\nthe use of data processing frameworks such as Apache\nSpark and Hadoop, and deep learning software such\nas TensorFlow, Theano or PyTorch, on distributed data\nwill incur the ﬁnancial cost of GPU hardware, software,\nand cloud computing services [178]. Another challenge\nwith deep learning for classiﬁcation problems is that\nthe deep learning algorithms must be trained with all\npossible instances of positive and negative examples.\nThere might be a massive collection of data favoring the\npositive examples and an insufﬁcient number of negative\nexamples. Thus, the deep learning algorithm will perform\npoorly [179]. Although, transfer learning may be helpful.\n• Hyper-parameter tuning in deep learning using cross-\nvalidation is by trial and error. There is no theoretical\nbackground to understand how to obtain the optimal\ntuning hyperparameters, and to assess the performance of\nthe algorithm. In deep CNNs where the number of hyper-\nparameters grows at an exponential rate with the depth\nof the network, ﬁnding these optimal hyper-parameters is\na herculean task. Although the AutoML platform tried\nto resolve this challenge, the solution appears to be\nexpensive [180].\n• Deep learning algorithms are liable to prediction errors.\nPrediction errors in wireless systems will cause strong\ninterference and will result in high transmission collision\nprobabilities [181]. However, deriving the prediction error\nprobability for most deep learning algorithms is still a\nchallenge.\n• Meeting up with the latency requirement in wireless net-\nworks, and deploying deep learning platforms in wireless\ndevices with limited memory storage for training large\nparameters remain a challenge. Even when regularization\ntechniques are used, such as Lp regularization, dropout,\nbatch normalization, early stopping, and so on, DNNs\nstill rely on large training parameters [182].\nVII. BRIEF OVERVIEW OF THE FUTURE 6G\nARCHITECTURE, AND ITS CONVERGENCE WITH DEEP\nLEARNING\nThe future 6G network is expected to deliver superior\nperformance compared to the 5G network. The data rate\nis expected to be in multiterabytes per second (Tb/s) and\nthe network will be super-data-driven. The 6G architecture\nwill encapsulate ground, water, and air networks to deliver\nubiquitous wireless connectivity. Some of the enablers for\nthe launch of the future 6G architecture are supermassive\nMIMO (sm-MIMO) to provide unlimited wireless service and\nincrease spectral efﬁciency; teraHertz (THz) communication,\nsub-mmWave communication, and visible light communica-\ntion (VLC) to provide the critically needed frequency spec-\ntrum; holographic beamforming (HBF) for frequency reuse;\nlarge intelligent surfaces (LISs), orbital angular momentum\n(OAM) multiplexing, and blockchain-based spectrum sharing\nfor dynamic spectrum management; quantum communication\nand computing for fast and secure networks beyond the\n5G networks; molecular communications for energy-efﬁcient\ncommunication; and internet of nano-things [183]. The vision\nof the 6G network is to provide full wireless coverage that\nsupports diverse ways to interact with smart devices which\nincludes tactile, brainwave, speech, visual, and audio com-\nmunication [184]. To improve data rate, modulation schemes\nsuch as index modulation and spatial modulation will be used.\nThe 6G network is expected to enhance the 5G network by\nproviding enhanced mobile broadband (eMBB), ultra-reliable\nlow latency communication (URLLC), massive machine type\ncommunication (mMTC), and ultra-low-power communication\n(ULPC). The key performance indices (KPI) for evaluating\nthe 6G network will be spectrum efﬁciency, energy efﬁciency,\npeak, and user-experienced data rate, latency, mobility, and\ndensity of connectivity [185]. The peak data rate is expected\nto be at least 1 Tb/s which is a hundred times higher than the\npeak data rate for the 5G network. The user-experience data\nrate will be at least 1 Gb/s which is ten times higher than\nthat of 5G. The latency over the air is expected to be within\nthe range of 1 −100 µs, and total connectivity is ten times\nhigher than that of 5G. The energy efﬁciency is expected to\nbe within 10 −100 times higher than that of 5G networks,\nwhile the spectral efﬁciency will also be 5 −10 times higher\n[183, 186, 187].\nThe realization of the 6G architecture will be powered by\nadvances in deep learning. Deep learning will be required for\nspectrum sensing in a 6G network, where spectrum sensing\nwill occur simultaneously with a large number of devices.\nDeep learning will identify the characteristics of the spectrum,\nand sense the spectrum intelligently using the right training\nmodel. Improved data mining and analytics will be required\nto unearth useful information and provide knowledge from\nmassive data generated from the 6G architecture in real-time.\nDeep learning techniques will be useful for selecting the\noptimal parameters in sub-mmWave and THz communication.\nDeep learning will be required to optimize energy efﬁciency\nand harvesting, especially for undersea, air, and space com-\nmunication [188]. Although research work has begun on 6G\nnetworks both in the academia[189, 190] and in the industry\n[184], there is still a lot of open problems that must be resolved\nto make possible the launch of the future 6G architecture [191–\n193].\nVIII. CONCLUSION\nThe emerging 5G architecture is expected to meet the\ndemand for increased capacity and increased data rate, mas-\nsive device connectivity, increased quality of experience, re-\nduced latency, and reduced capital and operational cost. To\nachieve this, a combination of some state-of-the-art tech-\nnologies must be commercially deployed. These technologies\ninclude massive multiple-input multiple-output technology,\nmillimeter wave technology, multi-access edge computing,\nsoftware-deﬁned networking, network function virtualization,\nand the Internet-of-Things. On the other hand, deep learn-\ning has attained tremendous progress at discovering intricate\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n13\nstructures in high-dimensional data in the ﬁeld of artiﬁcial\nintelligence integrating computer vision, bioinformatics, and\nnatural language processing. This has raised some hope of a\nsymbiotic relationship when deep learning converges with the\n5G technologies.\nA lot of research work sits at the intersection of both\nﬁelds. Therefore, this paper surveyed the convergence of 5G\narchitecture with deep learning. Unlike most survey papers\nthat discussed how a speciﬁc 5G technology can converge\nwith deep learning, this paper considers how the whole\nspectrum of the 5G architecture can converge with deep\nlearning. Also, this paper is well-structured to laser-focus on\nthe emerging 5G technologies. The potential challenges of\nsuch convergence are discussed. Lastly, this paper provides a\nbrief overview of the future 6G architecture, and how it can\nconverge with deep learning.\nDeclaration of Competing Interests\nThe authors declare that they have no known ﬁnancial\ninterest or personal relationships that could appear to affect\nthe work reported in this paper.\nAcknowledgement\nWe thank the anonymous reviewers for their valuable com-\nments which helped us improve the contents, quality and\npresentation of this work.\nFunding\nThis work was partially funded by the Department of\nComputer Science, Wichita State University in the United\nStates, and the Petroleum Technology Development Fund in\nNigeria.\nREFERENCES\n[1] Qualcomm, “The 1000x mobile data challenge,” White Paper,\n2012.\n[2] M. Fallgren, B. Timus et al., “Scenarios, requirements and\nkpis for 5g mobile and wireless system,” METIS deliverable\nD, vol. 1, p. 1, 2013.\n[3] P. K. Agyapong, M. Iwamura, D. Staehle, W. Kiess, and\nA. Benjebbour, “Design considerations for a 5g network ar-\nchitecture,” IEEE Communications Magazine, vol. 52, no. 11,\npp. 65–75, 2014.\n[4] G. I. P. Association et al., “5g vision-the 5g infrastructure pub-\nlic private partnership: the next generation of communication\nnetworks and services,” White Paper, February, 2015.\n[5] Y. C. Hu, M. Patel, D. Sabella, N. Sprecher, and V. Young,\n“Mobile edge computing—a key technology towards 5g,” ETSI\nwhite paper, vol. 11, no. 11, pp. 1–16, 2015.\n[6] Y. Kishiyama, A. Benjebbour, T. Nakamura, and H. Ishii,\n“Future steps of lte-a: evolution toward integration of local\narea and wide area systems,” IEEE Wireless Communications,\nvol. 20, no. 1, pp. 12–18, 2013.\n[7] G. Fodor, E. Dahlman, G. Mildh, S. Parkvall, N. Reider,\nG. Mikl´os, and Z. Tur´anyi, “Design aspects of network as-\nsisted device-to-device communications,” IEEE Communica-\ntions Magazine, vol. 50, no. 3, pp. 170–177, 2012.\n[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet\nclassiﬁcation with deep convolutional neural networks,” in\nAdvances in neural information processing systems, 2012, pp.\n1097–1105.\n[9] C. Farabet, C. Couprie, L. Najman, and Y. LeCun, “Learning\nhierarchical features for scene labeling,” IEEE transactions on\npattern analysis and machine intelligence, vol. 35, no. 8, pp.\n1915–1929, 2012.\n[10] J. J. Tompson, A. Jain, Y. LeCun, and C. Bregler, “Joint\ntraining of a convolutional network and a graphical model for\nhuman pose estimation,” in Advances in neural information\nprocessing systems, 2014, pp. 1799–1807.\n[11] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper\nwith convolutions,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2015, pp. 1–9.\n[12] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ˇCernock`y,\n“Strategies for training large scale neural network language\nmodels,” in 2011 IEEE Workshop on Automatic Speech Recog-\nnition & Understanding.\nIEEE, 2011, pp. 196–201.\n[13] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed,\nN. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath\net al., “Deep neural networks for acoustic modeling in speech\nrecognition: The shared views of four research groups,” IEEE\nSignal processing magazine, vol. 29, no. 6, pp. 82–97, 2012.\n[14] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ram-\nabhadran, “Deep convolutional neural networks for lvcsr,” in\n2013 IEEE international conference on acoustics, speech and\nsignal processing.\nIEEE, 2013, pp. 8614–8618.\n[15] J. Ma, R. P. Sheridan, A. Liaw, G. E. Dahl, and V. Svetnik,\n“Deep neural nets as a method for quantitative structure–\nactivity relationships,” Journal of chemical information and\nmodeling, vol. 55, no. 2, pp. 263–274, 2015.\n[16] T. Ciodaro, D. Deva, J. De Seixas, and D. Damazio, “Online\nparticle detection with neural networks based on topological\ncalorimetry information,” in Journal of physics: conference\nseries, vol. 368, no. 1.\nIOP Publishing, 2012, p. 012030.\n[17] C. Adam-Bourdarios, G. Cowan, C. Germain, I. Guyon,\nB. K´egl, and D. Rousseau, “The higgs boson machine learning\nchallenge,” in NIPS 2014 Workshop on High-energy Physics\nand Machine Learning, 2015, pp. 19–55.\n[18] M. Helmstaedter, K. L. Briggman, S. C. Turaga, V. Jain, H. S.\nSeung, and W. Denk, “Connectomic reconstruction of the inner\nplexiform layer in the mouse retina,” Nature, vol. 500, no.\n7461, pp. 168–174, 2013.\n[19] M. K. Leung, H. Y. Xiong, L. J. Lee, and B. J. Frey, “Deep\nlearning of the tissue-regulated splicing code,” Bioinformatics,\nvol. 30, no. 12, pp. i121–i129, 2014.\n[20] H. Y. Xiong, B. Alipanahi, L. J. Lee, H. Bretschneider,\nD. Merico, R. K. Yuen, Y. Hua, S. Gueroussov, H. S. Na-\njafabadi, T. R. Hughes et al., “The human splicing code reveals\nnew insights into the genetic determinants of disease,” Science,\nvol. 347, no. 6218, p. 1254806, 2015.\n[21] N. Fraidouni and G. V. Z´aruba, “Computational techniques to\nrecover missing gene expression data,” 2018.\n[22] R.\nCollobert,\nJ.\nWeston,\nL.\nBottou,\nM.\nKarlen,\nK. Kavukcuoglu, and P. Kuksa, “Natural language processing\n(almost) from scratch,” Journal of machine learning research,\nvol. 12, no. Aug, pp. 2493–2537, 2011.\n[23] A. Bordes, S. Chopra, and J. Weston, “Question answering\nwith subgraph embeddings,” arXiv preprint arXiv:1406.3676,\n2014.\n[24] S. Jean, K. Cho, R. Memisevic, and Y. Bengio, “On using very\nlarge target vocabulary for neural machine translation,” arXiv\npreprint arXiv:1412.2007, 2014.\n[25] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence\nlearning with neural networks,” in Advances in neural infor-\nmation processing systems, 2014, pp. 3104–3112.\n[26] M. A. Alsheikh, D. Niyato, S. Lin, H.-P. Tan, and Z. Han,\n“Mobile big data analytics using deep learning and apache\nspark,” IEEE network, vol. 30, no. 3, pp. 22–29, 2016.\n[27] C. Jiang, H. Zhang, Y. Ren, Z. Han, K.-C. Chen, and L. Hanzo,\n“Machine learning paradigms for next-generation wireless\nnetworks,” IEEE Wireless Communications, vol. 24, no. 2, pp.\n98–105, 2016.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n14\n[28] P. V. Klaine, M. A. Imran, O. Onireti, and R. D. Souza,\n“A survey of machine learning techniques applied to self-\norganizing cellular networks,” IEEE Communications Surveys\n& Tutorials, vol. 19, no. 4, pp. 2392–2431, 2017.\n[29] T. E. Bogale, X. Wang, and L. B. Le, “Machine intelli-\ngence techniques for next-generation context-aware wireless\nnetworks,” arXiv preprint arXiv:1801.04223, 2018.\n[30] M. E. M. Cayamcela and W. Lim, “Artiﬁcial intelligence in\n5g technology: A survey,” in 2018 International Conference\non Information and Communication Technology Convergence\n(ICTC).\nIEEE, 2018, pp. 860–865.\n[31] R. Li, Z. Zhao, X. Zhou, G. Ding, Y. Chen, Z. Wang,\nand H. Zhang, “Intelligent 5g: When cellular networks meet\nartiﬁcial intelligence,” IEEE Wireless communications, vol. 24,\nno. 5, pp. 175–183, 2017.\n[32] F. Restuccia and T. Melodia, “Physical-layer deep learning:\nChallenges and applications to 5g and beyond,” arXiv preprint\narXiv:2004.10113, 2020.\n[33] T. Wang, C.-K. Wen, H. Wang, F. Gao, T. Jiang, and S. Jin,\n“Deep learning for wireless physical layer: Opportunities and\nchallenges,” China Communications, vol. 14, no. 11, pp. 92–\n111, 2017.\n[34] H. He, S. Jin, C.-K. Wen, F. Gao, G. Y. Li, and Z. Xu, “Model-\ndriven deep learning for physical layer communications,” IEEE\nWireless Communications, vol. 26, no. 5, pp. 77–83, 2019.\n[35] Z. Qin, H. Ye, G. Y. Li, and B.-H. F. Juang, “Deep learning\nin physical layer communications,” IEEE Wireless Communi-\ncations, vol. 26, no. 2, pp. 93–99, 2019.\n[36] Q. Mao, F. Hu, and Q. Hao, “Deep learning for intelligent\nwireless networks: A comprehensive survey,” IEEE Commu-\nnications Surveys & Tutorials, vol. 20, no. 4, pp. 2595–2621,\n2018.\n[37] Q.-V. Pham, F. Fang, V. N. Ha, M. Piran, M. Le, L. B. Le,\nW.-J. Hwang, Z. Ding et al., “A survey of multi-access edge\ncomputing in 5g and beyond: Fundamentals, technology inte-\ngration, and state-of-the-art,” arXiv preprint arXiv:1906.08452,\n2019.\n[38] X. Wang, Y. Han, V. C. Leung, D. Niyato, X. Yan, and\nX. Chen, “Convergence of edge computing and deep learning:\nA comprehensive survey,” IEEE Communications Surveys &\nTutorials, vol. 22, no. 2, pp. 869–904, 2020.\n[39] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge\nintelligence: Paving the last mile of artiﬁcial intelligence with\nedge computing,” Proceedings of the IEEE, vol. 107, no. 8,\npp. 1738–1762, 2019.\n[40] J. Chen and X. Ran, “Deep learning with edge computing: A\nreview,” Proceedings of the IEEE, vol. 107, no. 8, pp. 1655–\n1674, 2019.\n[41] W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C.\nLiang, Q. Yang, D. Niyato, and C. Miao, “Federated learning\nin mobile edge networks: A comprehensive survey,” IEEE\nCommunications Surveys & Tutorials, 2020.\n[42] O. Odeyomi and G. Zaruba, “Differentially-private federated\nlearning with long-term constraints using online mirror de-\nscent,” in 2021 IEEE International Symposium on Information\nTheory (ISIT).\nIEEE, 2021, pp. 1308–1313.\n[43] Z. M. Fadlullah, F. Tang, B. Mao, N. Kato, O. Akashi, T. Inoue,\nand K. Mizutani, “State-of-the-art deep learning: Evolving\nmachine intelligence toward tomorrow’s intelligent network\ntrafﬁc control systems,” IEEE Communications Surveys &\nTutorials, vol. 19, no. 4, pp. 2432–2455, 2017.\n[44] X. Ma, T. Yao, M. Hu, Y. Dong, W. Liu, F. Wang, and J. Liu,\n“A survey on deep learning empowered iot applications,” IEEE\nAccess, vol. 7, pp. 181 721–181 732, 2019.\n[45] M. A. Al-Garadi, A. Mohamed, A. Al-Ali, X. Du, I. Ali, and\nM. Guizani, “A survey of machine and deep learning methods\nfor internet of things (iot) security,” IEEE Communications\nSurveys & Tutorials, 2020.\n[46] M. Mohammadi, A. Al-Fuqaha, S. Sorour, and M. Guizani,\n“Deep learning for iot big data and streaming analytics: A\nsurvey,” IEEE Communications Surveys & Tutorials, vol. 20,\nno. 4, pp. 2923–2960, 2018.\n[47] M. Zamanipour, “A survey on deep-learning based techniques\nfor modeling and estimation of massivemimo channels,” arXiv\npreprint arXiv:1910.03390, 2019.\n[48] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-\nC. Liang, and D. I. Kim, “Applications of deep reinforcement\nlearning in communications and networking: A survey,” IEEE\nCommunications Surveys & Tutorials, vol. 21, no. 4, pp. 3133–\n3174, 2019.\n[49] Z. Xiong, Y. Zhang, D. Niyato, R. Deng, P. Wang, and L.-\nC. Wang, “Deep reinforcement learning for mobile 5g and\nbeyond: Fundamentals, applications, and challenges,” IEEE\nVehicular Technology Magazine, vol. 14, no. 2, pp. 44–52,\n2019.\n[50] C. Zhang, P. Patras, and H. Haddadi, “Deep learning in mobile\nand wireless networking: A survey,” IEEE Communications\nSurveys & Tutorials, vol. 21, no. 3, pp. 2224–2287, 2019.\n[51] F. Haider, C.-X. Wang, H. Haas, D. Yuan, H. Wang, X. Gao,\nX.-H. You, and E. Hepsaydir, “Spectral efﬁciency analysis\nof mobile femtocell based cellular systems,” in 2011 IEEE\n13th International Conference on Communication Technology.\nIEEE, 2011, pp. 347–351.\n[52] X. Hong, C.-x. Wang, H.-H. Chen, and Y. Zhang, “Secondary\nspectrum access networks,” IEEE vehicular technology maga-\nzine, vol. 4, no. 2, pp. 36–43, 2009.\n[53] M. Agiwal, A. Roy, and N. Saxena, “Next generation 5g\nwireless networks: A comprehensive survey,” IEEE Commu-\nnications Surveys & Tutorials, vol. 18, no. 3, pp. 1617–1655,\n2016.\n[54] Y. Jung, E. Festijo, and M. Peradilla, “Joint operation of\nrouting control and group key management for 5g ad hoc d2d\nnetworks,” in 2014 International Conference on Privacy and\nSecurity in Mobile Systems (PRISMS).\nIEEE, 2014, pp. 1–8.\n[55] G. Wunder, P. Jung, M. Kasparick, T. Wild, F. Schaich,\nY. Chen, S. Ten Brink, I. Gaspar, N. Michailow, A. Festag\net al., “5gnow: non-orthogonal, asynchronous waveforms for\nfuture mobile applications,” IEEE Communications Magazine,\nvol. 52, no. 2, pp. 97–105, 2014.\n[56] Y. Goldberg, “A primer on neural network models for nat-\nural language processing,” Journal of Artiﬁcial Intelligence\nResearch, vol. 57, pp. 345–420, 2016.\n[57] L. Zhang, S. Wang, and B. Liu, “Deep learning for sentiment\nanalysis: A survey,” Wiley Interdisciplinary Reviews: Data\nMining and Knowledge Discovery, vol. 8, no. 4, p. e1253,\n2018.\n[58] Y. LeCun, Y. Bengio et al., “Convolutional networks for\nimages, speech, and time series,” The handbook of brain theory\nand neural networks, vol. 3361, no. 10, p. 1995, 1995.\n[59] H. Long, M. Wang, and H. Fu, “Deep convolutional neural\nnetworks for predicting hydroxyproline in proteins,” Current\nBioinformatics, vol. 12, no. 3, pp. 233–238, 2017.\n[60] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-\nbased learning applied to document recognition,” Proceedings\nof the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.\n[61] M. D. Zeiler and R. Fergus, “Visualizing and understanding\nconvolutional networks,” in European conference on computer\nvision.\nSpringer, 2014, pp. 818–833.\n[62] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing be-\ntween capsules,” in Advances in neural information processing\nsystems, 2017, pp. 3856–3866.\n[63] G. E. Hinton, S. Sabour, and N. Frosst, “Matrix capsules\nwith em routing,” in International conference on learning\nrepresentations, 2018.\n[64] J. Rajasegaran, V. Jayasundara, S. Jayasekara, H. Jayasekara,\nS. Seneviratne, and R. Rodrigo, “Deepcaps: Going deeper with\ncapsule networks,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2019, pp. 10 725–\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n15\n10 733.\n[65] X. Glorot and Y. Bengio, “Understanding the difﬁculty of\ntraining deep feedforward neural networks,” in Proceedings of\nthe thirteenth international conference on artiﬁcial intelligence\nand statistics, 2010, pp. 249–256.\n[66] A. Graves, “Generating sequences with recurrent neural net-\nworks,” arXiv preprint arXiv:1308.0850, 2013.\n[67] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau,\nF. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase\nrepresentations using rnn encoder-decoder for statistical ma-\nchine translation,” arXiv preprint arXiv:1406.1078, 2014.\n[68] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[69] K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic\nrepresentations from tree-structured long short-term memory\nnetworks,” arXiv preprint arXiv:1503.00075, 2015.\n[70] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical\nevaluation of gated recurrent neural networks on sequence\nmodeling,” arXiv preprint arXiv:1412.3555, 2014.\n[71] L. Dong, F. Wei, C. Tan, D. Tang, M. Zhou, and K. Xu,\n“Adaptive recursive neural network for target-dependent twitter\nsentiment classiﬁcation,” in Proceedings of the 52nd annual\nmeeting of the association for computational linguistics (vol-\nume 2: Short papers), 2014, pp. 49–54.\n[72] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol,\n“Extracting and composing robust features with denoising\nautoencoders,” in Proceedings of the 25th international con-\nference on Machine learning, 2008, pp. 1096–1103.\n[73] D. P. Kingma and M. Welling, “Auto-encoding variational\nbayes,” arXiv preprint arXiv:1312.6114, 2013.\n[74] D. J. Rezende, S. Mohamed, and D. Wierstra, “Stochastic\nbackpropagation and approximate inference in deep generative\nmodels,” arXiv preprint arXiv:1401.4082, 2014.\n[75] G. E. Hinton, “A practical guide to training restricted boltz-\nmann machines,” in Neural networks: Tricks of the trade.\nSpringer, 2012, pp. 599–619.\n[76] ——, “Deep belief networks,” Scholarpedia, vol. 4, no. 5, p.\n5947, 2009.\n[77] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative\nadversarial nets,” in Advances in neural information processing\nsystems, 2014, pp. 2672–2680.\n[78] K. B. Kancharagunta and S. R. Dubey, “Csgan: Cyclic-\nsynthesized generative adversarial networks for image-to-\nimage\ntransformation,”\narXiv\npreprint\narXiv:1901.03554,\n2019.\n[79] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,\nand P. Abbeel, “Infogan: Interpretable representation learning\nby information maximizing generative adversarial nets,” in\nAdvances in neural information processing systems, 2016, pp.\n2172–2180.\n[80] Y. Wu, J. Donahue, D. Balduzzi, K. Simonyan, and T. Lill-\nicrap, “Logan: Latent optimisation for generative adversarial\nnetworks,” arXiv preprint arXiv:1912.00953, 2019.\n[81] M. Mirza and S. Osindero, “Conditional generative adversarial\nnets,” arXiv preprint arXiv:1411.1784, 2014.\n[82] J. Gui, Z. Sun, Y. Wen, D. Tao, and J. Ye, “A review\non generative adversarial networks: Algorithms, theory, and\napplications,” arXiv preprint arXiv:2001.06937, 2020.\n[83] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness,\nM. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland,\nG. Ostrovski et al., “Human-level control through deep rein-\nforcement learning,” nature, vol. 518, no. 7540, pp. 529–533,\n2015.\n[84] C. J. C. H. Watkins, “Learning from delayed rewards,” 1989.\n[85] R. Bellman, “The theory of dynamic programming,” Rand corp\nsanta monica ca, Tech. Rep., 1954.\n[86] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement\nlearning with double q-learning,” in Thirtieth AAAI conference\non artiﬁcial intelligence, 2016.\n[87] A. Cini, C. D’Eramo, J. Peters, and C. Alippi, “Deep rein-\nforcement learning with weighted q-learning,” arXiv preprint\narXiv:2003.09280, 2020.\n[88] A. Alkhateeb, “Deepmimo: A generic deep learning dataset\nfor millimeter wave and massive mimo applications,” arXiv\npreprint arXiv:1902.06435, 2019.\n[89] R. Eichenlaub, C. Valentine, S. Fast, and S. Albarano, “Fidelity\nat high speed: Wireless insite® real time module™,” in MIL-\nCOM 2008-2008 IEEE Military Communications Conference.\nIEEE, 2008, pp. 1–7.\n[90] C.-K. Wen, W.-T. Shih, and S. Jin, “Deep learning for massive\nmimo csi feedback,” IEEE Wireless Communications Letters,\nvol. 7, no. 5, pp. 748–751, 2018.\n[91] T. Wang, C.-K. Wen, S. Jin, and G. Y. Li, “Deep learning-\nbased csi feedback approach for time-varying massive mimo\nchannels,” IEEE Wireless Communications Letters, vol. 8,\nno. 2, pp. 416–419, 2018.\n[92] Z. Liu, L. Zhang, and Z. Ding, “Exploiting bi-directional\nchannel reciprocity in deep learning for low rate massive mimo\ncsi feedback,” IEEE Wireless Communications Letters, vol. 8,\nno. 3, pp. 889–892, 2019.\n[93] Y. Liao, H. Yao, Y. Hua, and C. Li, “Csi feedback based on\ndeep learning for massive mimo systems,” IEEE Access, vol. 7,\npp. 86 810–86 820, 2019.\n[94] Y. Zhou, Z. M. Fadlullah, B. Mao, and N. Kato, “A deep-\nlearning-based radio resource assignment technique for 5g\nultra dense networks,” IEEE Network, vol. 32, no. 6, pp. 28–\n34, 2018.\n[95] H. Sun, X. Chen, Q. Shi, M. Hong, X. Fu, and N. D.\nSidiropoulos, “Learning to optimize: Training deep neural\nnetworks for wireless resource management,” in 2017 IEEE\n18th International Workshop on Signal Processing Advances\nin Wireless Communications (SPAWC).\nIEEE, 2017, pp. 1–6.\n[96] K. Hornik, M. Stinchcombe, H. White et al., “Multilayer\nfeedforward networks are universal approximators.” Neural\nnetworks, vol. 2, no. 5, pp. 359–366, 1989.\n[97] H. Sun, X. Chen, Q. Shi, M. Hong, X. Fu, and N. D.\nSidiropoulos, “Learning to optimize: Training deep neural\nnetworks for interference management,” IEEE Transactions on\nSignal Processing, vol. 66, no. 20, pp. 5438–5453, 2018.\n[98] L. Sanguinetti, A. Zappone, and M. Debbah, “Deep learning\npower allocation in massive mimo,” in 2018 52nd Asilomar\nconference on signals, systems, and computers.\nIEEE, 2018,\npp. 1257–1261.\n[99] T. Van Chien, T. N. Canh, E. Bj¨ornson, and E. G. Larsson,\n“Power control in cellular massive mimo with varying user\nactivity: A deep learning solution,” IEEE Transactions on\nWireless Communications, 2020.\n[100] A. Alkhateeb, S. Alex, P. Varkey, Y. Li, Q. Qu, and D. Tu-\njkovic, “Deep learning coordinated beamforming for highly-\nmobile millimeter wave systems,” IEEE Access, vol. 6, pp.\n37 328–37 348, 2018.\n[101] T. Zhang, A. Chowdhery, P. Bahl, K. Jamieson, and S. Baner-\njee, “The design and implementation of a wireless video\nsurveillance system,” in Proceedings of the 21st Annual Inter-\nnational Conference on Mobile Computing and Networking,\n2015, pp. 426–438.\n[102] C.-C. Hung, G. Ananthanarayanan, P. Bodik, L. Golubchik,\nM. Yu, P. Bahl, and M. Philipose, “Videoedge: Process-\ning camera streams using hierarchical clusters,” in 2018\nIEEE/ACM Symposium on Edge Computing (SEC).\nIEEE,\n2018, pp. 115–131.\n[103] “The world’s ﬁrst deep learning enabled video camera for de-\nvelopers,” https://aws.amazon.com/deeplens/, 2020, accessed\nJuly, 11 2020.\n[104] “Amazon alexa,” https://developer.amazon.com/en-US/alexa,\naccessed July, 11 2020.\n[105] S. Team, “Hey siri: An on-device dnn-powered voice trigger\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n16\nfor apple’s personal assistant,” https://machinelearning.apple.\ncom/research/hey-siri, October 2017, accessed July, 11 2020.\n[106] J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,”\nin Proceedings of the IEEE conference on computer vision and\npattern recognition, 2017, pp. 7263–7271.\n[107] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient\nconvolutional neural networks for mobile vision applications,”\narXiv preprint arXiv:1704.04861, 2017.\n[108] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.\nFu, and A. C. Berg, “Ssd: Single shot multibox detector,” in\nEuropean conference on computer vision. Springer, 2016, pp.\n21–37.\n[109] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.\nDally, and K. Keutzer, “Squeezenet: Alexnet-level accuracy\nwith 50x fewer parameters and¡ 0.5 mb model size,” arXiv\npreprint arXiv:1602.07360, 2016.\n[110] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge\nin a neural network,” arXiv preprint arXiv:1503.02531, 2015.\n[111] S. Liu, Y. Lin, Z. Zhou, K. Nan, H. Liu, and J. Du, “On-\ndemand deep model compression for mobile devices: A usage-\ndriven model selection framework,” in Proceedings of the\n16th Annual International Conference on Mobile Systems,\nApplications, and Services, 2018, pp. 389–400.\n[112] L. N. Huynh, Y. Lee, and R. K. Balan, “Deepmon: Mobile\ngpu-based deep learning framework for continuous vision\napplications,” in Proceedings of the 15th Annual International\nConference on Mobile Systems, Applications, and Services,\n2017, pp. 82–95.\n[113] “The nvidia egx platform for edge computing,” https://www.\nnvidia.com/en-us/data-center/products/egx-edge-computing/,\naccessed July, 11 2020.\n[114] “Edge tpu,” https://cloud.google.com/edge-tpu/, accessed July,\n11 2020.\n[115] H. Zhang, G. Ananthanarayanan, P. Bodik, M. Philipose,\nP. Bahl, and M. J. Freedman, “Live video analytics at scale\nwith approximation and delay-tolerance,” in 14th {USENIX}\nSymposium on Networked Systems Design and Implementation\n({NSDI} 17), 2017, pp. 377–392.\n[116] J. Jiang, G. Ananthanarayanan, P. Bodik, S. Sen, and I. Stoica,\n“Chameleon: scalable adaptation of video analytics,” in Pro-\nceedings of the 2018 Conference of the ACM Special Interest\nGroup on Data Communication, 2018, pp. 253–266.\n[117] A. H. Jiang, D. L.-K. Wong, C. Canel, L. Tang, I. Misra,\nM. Kaminsky, M. A. Kozuch, P. Pillai, D. G. Andersen, and\nG. R. Ganger, “Mainstream: Dynamic stem-sharing for multi-\ntenant video processing,” in 2018 {USENIX} Annual Technical\nConference ({USENIX}{ATC} 18), 2018, pp. 29–42.\n[118] X. Ran, H. Chen, X. Zhu, Z. Liu, and J. Chen, “Deepdecision:\nA mobile deep learning framework for edge video analytics,”\nin IEEE INFOCOM 2018-IEEE Conference on Computer\nCommunications.\nIEEE, 2018, pp. 1421–1429.\n[119] S. Han, H. Shen, M. Philipose, S. Agarwal, A. Wolman, and\nA. Krishnamurthy, “Mcdnn: An approximation-based execu-\ntion framework for deep stream processing under resource\nconstraints,” in Proceedings of the 14th Annual International\nConference on Mobile Systems, Applications, and Services,\n2016, pp. 123–136.\n[120] Y. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars,\nand L. Tang, “Neurosurgeon: Collaborative intelligence be-\ntween the cloud and mobile edge,” ACM SIGARCH Computer\nArchitecture News, vol. 45, no. 1, pp. 615–629, 2017.\n[121] S. Teerapittayanon, B. McDanel, and H.-T. Kung, “Distributed\ndeep neural networks over the cloud, the edge and end\ndevices,” in 2017 IEEE 37th International Conference on\nDistributed Computing Systems (ICDCS).\nIEEE, 2017, pp.\n328–339.\n[122] Z. Zhao, K. M. Barijough, and A. Gerstlauer, “Deepthings:\nDistributed adaptive deep learning inference on resource-\nconstrained\niot\nedge\nclusters,”\nIEEE\nTransactions\non\nComputer-Aided Design of Integrated Circuits and Systems,\nvol. 37, no. 11, pp. 2348–2359, 2018.\n[123] J. Mao, X. Chen, K. W. Nixon, C. Krieger, and Y. Chen,\n“Modnn: Local distributed mobile computing system for deep\nneural network,” in Design, Automation & Test in Europe\nConference & Exhibition (DATE), 2017.\nIEEE, 2017, pp.\n1396–1401.\n[124] J. Zhu, Y. Song, D. Jiang, and H. Song, “A new deep-\nq-learning-based transmission scheduling mechanism for the\ncognitive internet of things,” IEEE Internet of Things Journal,\nvol. 5, no. 4, pp. 2375–2385, 2017.\n[125] R. Huang and G. V. Zaruba, “Incorporating data from multiple\nsensors for localizing nodes in mobile ad hoc networks,” IEEE\ntransactions on mobile computing, vol. 6, no. 9, pp. 1090–\n1104, 2007.\n[126] S. Yao, Y. Zhao, A. Zhang, S. Hu, H. Shao, C. Zhang, L. Su,\nand T. Abdelzaher, “Deep learning for the internet of things,”\nComputer, vol. 51, no. 5, pp. 32–41, 2018.\n[127] S. Yao, S. Hu, Y. Zhao, A. Zhang, and T. Abdelzaher,\n“Deepsense: A uniﬁed deep learning framework for time-series\nmobile sensing data processing,” in Proceedings of the 26th\nInternational Conference on World Wide Web, 2017, pp. 351–\n360.\n[128] S. Yao, Y. Zhao, A. Zhang, L. Su, and T. Abdelzaher, “Deepiot:\nCompressing deep neural network structures for sensing sys-\ntems with a compressor-critic framework,” in Proceedings\nof the 15th ACM Conference on Embedded Network Sensor\nSystems, 2017, pp. 1–14.\n[129] S. Yao, Y. Zhao, H. Shao, A. Zhang, C. Zhang, S. Li, and\nT. Abdelzaher, “Rdeepsense: Reliable deep mobile computing\nmodels with uncertainty estimations,” Proceedings of the ACM\non Interactive, Mobile, Wearable and Ubiquitous Technologies,\nvol. 1, no. 4, pp. 1–26, 2018.\n[130] R. F. Molanes, K. Amarasinghe, J. Rodriguez-Andina, and\nM. Manic, “Deep learning and reconﬁgurable platforms in the\ninternet of things: challenges and opportunities in algorithms\nand hardware,” IEEE Industrial Electronics Magazine, vol. 12,\nno. 2, pp. 36–49, 2018.\n[131] D. L. Marino, K. Amarasinghe, and M. Manic, “Building en-\nergy load forecasting using deep neural networks,” in IECON\n2016-42nd Annual Conference of the IEEE Industrial Elec-\ntronics Society.\nIEEE, 2016, pp. 7046–7051.\n[132] K. Amarasinghe, D. L. Marino, and M. Manic, “Deep neural\nnetworks for energy load forecasting,” in 2017 IEEE 26th\nInternational Symposium on Industrial Electronics (ISIE).\nIEEE, 2017, pp. 1483–1488.\n[133] E. Mocanu, P. H. Nguyen, M. Gibescu, and W. L. Kling,\n“Deep learning for estimating building energy consumption,”\nSustainable Energy, Grids and Networks, vol. 6, pp. 91–99,\n2016.\n[134] M. Manic, K. Amarasinghe, J. J. Rodriguez-Andina, and\nC. Rieger, “Intelligent buildings of the future: Cyberaware,\ndeep learning powered, and human interacting,” IEEE Indus-\ntrial Electronics Magazine, vol. 10, no. 4, pp. 32–49, 2016.\n[135] R. J. Rossetti, “Trafﬁc control & management systems in smart\ncities,” Readings Smart Cities, vol. 2, no. 3, 2016.\n[136] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski,\nB. Firner, L. Jackel, and U. Muller, “Explaining how a deep\nneural network trained with end-to-end learning steers a car,”\narXiv preprint arXiv:1704.07911, 2017.\n[137] J. H. Ko, Y. Long, M. F. Amir, D. Kim, J. Kung, T. Na,\nA. R. Trivedi, and S. Mukhopadhyay, “Energy-efﬁcient neural\nimage processing for internet-of-things edge devices,” in 2017\nIEEE 60th International Midwest Symposium on Circuits and\nSystems (MWSCAS).\nIEEE, 2017, pp. 1069–1072.\n[138] N. McLaughlin, J. Martinez del Rincon, B. Kang, S. Yerima,\nP. Miller, S. Sezer, Y. Safaei, E. Trickel, Z. Zhao, A. Doup´e\net al., “Deep android malware detection,” in Proceedings of the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n17\nSeventh ACM on Conference on Data and Application Security\nand Privacy, 2017, pp. 301–308.\n[139] P. Torres, C. Catania, S. Garcia, and C. G. Garino, “An analysis\nof recurrent neural networks for botnet detection behavior,”\nin 2016 IEEE biennial congress of Argentina (ARGENCON).\nIEEE, 2016, pp. 1–6.\n[140] M. Youseﬁ-Azar, V. Varadharajan, L. Hamey, and U. Tupakula,\n“Autoencoder-based feature learning for cyber security ap-\nplications,” in 2017 International joint conference on neural\nnetworks (IJCNN).\nIEEE, 2017, pp. 3854–3861.\n[141] Y. Li, R. Ma, and R. Jiao, “A hybrid malicious code detection\nmethod based on deep learning,” International Journal of\nSecurity and Its Applications, vol. 9, no. 5, pp. 205–216, 2015.\n[142] R. E. Hiromoto, M. Haney, and A. Vakanski, “A secure\narchitecture for iot with supply chain risk management,” in\n2017 9th IEEE International Conference on Intelligent Data\nAcquisition and Advanced Computing Systems: Technology\nand Applications (IDAACS), vol. 1, 2017, pp. 431–435.\n[143] D. Kreutz, F. M. Ramos, P. E. Verissimo, C. E. Rothenberg,\nS. Azodolmolky, and S. Uhlig, “Software-deﬁned networking:\nA comprehensive survey,” Proceedings of the IEEE, vol. 103,\nno. 1, pp. 14–76, 2014.\n[144] S. Troia, R. Alvizu, and G. Maier, “Reinforcement learning for\nservice function chain reconﬁguration in nfv-sdn metro-core\noptical networks,” IEEE Access, vol. 7, pp. 167 944–167 957,\n2019.\n[145] H. Hawilo, A. Shami, M. Mirahmadi, and R. Asal, “Nfv: state\nof the art, challenges, and implementation in next generation\nmobile networks (vepc),” IEEE Network, vol. 28, no. 6, pp.\n18–26, 2014.\n[146] J. Xie, F. R. Yu, T. Huang, R. Xie, J. Liu, C. Wang, and Y. Liu,\n“A survey of machine learning techniques applied to software\ndeﬁned networking (sdn): Research issues and challenges,”\nIEEE Communications Surveys & Tutorials, vol. 21, no. 1,\npp. 393–430, 2018.\n[147] A. Nakao and P. Du, “Toward in-network deep machine\nlearning for identifying mobile applications and enabling ap-\nplication speciﬁc network slicing,” IEICE Transactions on\nCommunications, p. 2017CQI0002, 2018.\n[148] P. Wang, F. Ye, X. Chen, and Y. Qian, “Datanet: Deep learning\nbased encrypted network trafﬁc classiﬁcation in sdn home\ngateway,” IEEE Access, vol. 6, pp. 55 380–55 391, 2018.\n[149] F. Tang, Z. M. Fadlullah, B. Mao, and N. Kato, “An intelligent\ntrafﬁc load prediction-based adaptive channel assignment al-\ngorithm in sdn-iot: A deep learning approach,” IEEE Internet\nof Things Journal, vol. 5, no. 6, pp. 5141–5154, 2018.\n[150] X. Huang, T. Yuan, G. Qiao, and Y. Ren, “Deep reinforcement\nlearning for multimedia trafﬁc control in software deﬁned\nnetworking,” IEEE Network, vol. 32, no. 6, pp. 35–41, 2018.\n[151] G. Stampa, M. Arias, D. S´anchez-Charles, V. Munt´es-Mulero,\nand A. Cabellos, “A deep-reinforcement learning approach\nfor software-deﬁned networking routing optimization,” arXiv\npreprint arXiv:1709.07080, 2017.\n[152] A. Azzouni and G. Pujolle, “Neutm: A neural network-based\nframework for trafﬁc matrix prediction in sdn,” in NOMS\n2018-2018 IEEE/IFIP Network Operations and Management\nSymposium.\nIEEE, 2018, pp. 1–5.\n[153] “Geant,”\nhttps://www.geant.org/Projects/Network projects/,\naccessed July, 11 2020.\n[154] C. Qiu, F. R. Yu, H. Yao, C. Jiang, F. Xu, and C. Zhao,\n“Blockchain-based\nsoftware-deﬁned\nindustrial\ninternet\nof\nthings: A dueling deep {Q} learning approach,” IEEE Internet\nof Things Journal, vol. 6, no. 3, pp. 4627–4639, 2018.\n[155] D. Zhang, F. R. Yu, and R. Yang, “A machine learning\napproach for software-deﬁned vehicular ad hoc networks with\ntrust management,” in 2018 IEEE Global Communications\nConference (GLOBECOM).\nIEEE, 2018, pp. 1–6.\n[156] Y. He, F. R. Yu, N. Zhao, H. Yin, and A. Boukerche, “Deep\nreinforcement learning (drl)-based resource management in\nsoftware-deﬁned and virtualized vehicular ad hoc networks,” in\nProceedings of the 6th ACM Symposium on Development and\nAnalysis of Intelligent Vehicular Networks and Applications,\n2017, pp. 47–54.\n[157] Y. He, F. R. Yu, N. Zhao, V. C. Leung, and H. Yin, “Software-\ndeﬁned networks with mobile edge computing and caching for\nsmart cities: A big data deep reinforcement learning approach,”\nIEEE Communications Magazine, vol. 55, no. 12, pp. 31–37,\n2017.\n[158] M. Chen, M. Mozaffari, W. Saad, C. Yin, M. Debbah, and\nC. S. Hong, “Caching in the sky: Proactive deployment of\ncache-enabled unmanned aerial vehicles for optimized quality-\nof-experience,” IEEE Journal on Selected Areas in Communi-\ncations, vol. 35, no. 5, pp. 1046–1061, 2017.\n[159] J. Bendriss, I. G. B. Yahia, and D. Zeghlache, “Forecasting\nand anticipating slo breaches in programmable networks,” in\n2017 20th Conference on Innovations in Clouds, Internet and\nNetworks (ICIN).\nIEEE, 2017, pp. 127–134.\n[160] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, “Resource\nmanagement with deep reinforcement learning,” in Proceed-\nings of the 15th ACM Workshop on Hot Topics in Networks,\n2016, pp. 50–56.\n[161] R. Li, Z. Zhao, Q. Sun, I. Chih-Lin, C. Yang, X. Chen,\nM. Zhao, and H. Zhang, “Deep reinforcement learning for\nresource management in network slicing,” IEEE Access, vol. 6,\npp. 74 429–74 441, 2018.\n[162] X. Chen, Z. Zhao, C. Wu, M. Bennis, H. Liu, Y. Ji, and\nH. Zhang, “Multi-tenant cross-slice resource orchestration:\nA deep reinforcement learning approach,” IEEE Journal on\nSelected Areas in Communications, vol. 37, no. 10, pp. 2377–\n2392, 2019.\n[163] T. A. Tang, L. Mhamdi, D. McLernon, S. A. R. Zaidi, and\nM. Ghogho, “Deep learning approach for network intrusion de-\ntection in software deﬁned networking,” in 2016 International\nConference on Wireless Networks and Mobile Communications\n(WINCOM).\nIEEE, 2016, pp. 258–263.\n[164] S. Revathi and A. Malathi, “A detailed analysis on nsl-kdd\ndataset using various machine learning techniques for intrusion\ndetection,” International Journal of Engineering Research &\nTechnology (IJERT), vol. 2, no. 12, pp. 1848–1853, 2013.\n[165] T. A. Tang, L. Mhamdi, D. McLernon, S. A. R. Zaidi, and\nM. Ghogho, “Deep recurrent neural network for intrusion de-\ntection in sdn-based networks,” in 2018 4th IEEE Conference\non Network Softwarization and Workshops (NetSoft).\nIEEE,\n2018, pp. 202–206.\n[166] N. Shone, T. N. Ngoc, V. D. Phai, and Q. Shi, “A deep learning\napproach to network intrusion detection,” IEEE transactions on\nemerging topics in computational intelligence, vol. 2, no. 1, pp.\n41–50, 2018.\n[167] C. Li, Y. Wu, X. Yuan, Z. Sun, W. Wang, X. Li, and\nL. Gong, “Detection and defense of ddos attack–based on\ndeep learning in openﬂow-based sdn,” International Journal\nof Communication Systems, vol. 31, no. 5, p. e3497, 2018.\n[168] Q. Niyaz, W. Sun, and A. Y. Javaid, “A deep learning based\nddos detection system in software-deﬁned networking (sdn),”\narXiv preprint arXiv:1611.07400, 2016.\n[169] S. Lee, J. Kim, S. Shin, P. Porras, and V. Yegneswaran,\n“Athena: A framework for scalable anomaly detection in\nsoftware-deﬁned networks,” in 2017 47th Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Net-\nworks (DSN).\nIEEE, 2017, pp. 249–260.\n[170] S. Naseer, Y. Saleem, S. Khalid, M. K. Bashir, J. Han, M. M.\nIqbal, and K. Han, “Enhanced network anomaly detection\nbased on deep neural networks,” IEEE Access, vol. 6, pp.\n48 231–48 246, 2018.\n[171] A. A. Gebremariam, M. Usman, and M. Qaraqe, “Applications\nof artiﬁcial intelligence and machine learning in the area of\nsdn and nfv: A survey,” in 2019 16th International Multi-\nConference on Systems, Signals & Devices (SSD).\nIEEE,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n18\n2019, pp. 545–549.\n[172] Y. Zhao, Y. Li, X. Zhang, G. Geng, W. Zhang, and Y. Sun,\n“A survey of networking applications applying the software\ndeﬁned networking concept based on machine learning,” IEEE\nAccess, vol. 7, pp. 95 385–95 405, 2019.\n[173] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks\nare easily fooled: High conﬁdence predictions for unrecog-\nnizable images,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2015, pp. 427–436.\n[174] M. Sadeghi and E. G. Larsson, “Adversarial attacks on deep-\nlearning based radio signal classiﬁcation,” IEEE Wireless Com-\nmunications Letters, vol. 8, no. 1, pp. 213–216, 2018.\n[175] V. Behzadan and A. Munir, “Vulnerability of deep reinforce-\nment learning to policy induction attacks,” in International\nConference on Machine Learning and Data Mining in Pattern\nRecognition.\nSpringer, 2017, pp. 262–275.\n[176] D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba, “Net-\nwork dissection: Quantifying interpretability of deep visual\nrepresentations,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2017, pp. 6541–6549.\n[177] H. Huang, S. Guo, G. Gui, Z. Yang, J. Zhang, H. Sari,\nand F. Adachi, “Deep learning for physical-layer 5g wireless\ntechniques: Opportunities, challenges and solutions,” IEEE\nWireless Communications, vol. 27, no. 1, pp. 214–222, 2019.\n[178] A. Arpteg, B. Brinne, L. Crnkovic-Friis, and J. Bosch, “Soft-\nware engineering challenges of deep learning,” in 2018 44th\nEuromicro Conference on Software Engineering and Advanced\nApplications (SEAA).\nIEEE, 2018, pp. 50–59.\n[179] A. Munappy, J. Bosch, H. H. Olsson, A. Arpteg, and B. Brinne,\n“Data management challenges for deep learning,” in 2019 45th\nEuromicro Conference on Software Engineering and Advanced\nApplications (SEAA).\nIEEE, 2019, pp. 140–147.\n[180] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li,\nL. Fei-Fei, A. Yuille, J. Huang, and K. Murphy, “Progressive\nneural architecture search,” in Proceedings of the European\nConference on Computer Vision (ECCV), 2018, pp. 19–34.\n[181] Z. Hou, C. She, Y. Li, T. Q. Quek, and B. Vucetic, “Burstiness-\naware bandwidth reservation for ultra-reliable and low-latency\ncommunications in tactile internet,” IEEE Journal on Selected\nAreas in Communications, vol. 36, no. 11, pp. 2401–2410,\n2018.\n[182] A. Jagannath, J. Jagannath, and T. Melodia, “Redeﬁning\nwireless communication for 6g: Signal processing meets deep\nlearning,” arXiv preprint arXiv:2004.10715, 2020.\n[183] Z. Zhang, Y. Xiao, Z. Ma, M. Xiao, Z. Ding, X. Lei, G. K.\nKaragiannidis, and P. Fan, “6g wireless networks: Vision,\nrequirements, architecture, and key technologies,” IEEE Ve-\nhicular Technology Magazine, vol. 14, no. 3, pp. 28–41, 2019.\n[184] M. Latva-aho, “Radio access networking challenges towards\n2030,” Powerpoint Presentation, Oulu University, Finland,\nOctober, 2018.\n[185] G. Romano, “Imt-2020 requirements and realization,” Wiley\n5G Ref: The Essential 5G Reference Online, pp. 1–28, 2019.\n[186] K. David and H. Berndt, “6g vision and requirements: Is\nthere any need for beyond 5g?” IEEE Vehicular Technology\nMagazine, vol. 13, no. 3, pp. 72–80, 2018.\n[187] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and Y.-J. A. Zhang,\n“The roadmap to 6g: Ai empowered wireless networks,” IEEE\nCommunications Magazine, vol. 57, no. 8, pp. 84–90, 2019.\n[188] H. Yang, A. Alphones, Z. Xiong, D. Niyato, J. Zhao, and\nK. Wu, “Artiﬁcial intelligence-enabled intelligent 6g net-\nworks,” arXiv preprint arXiv:1912.05744, 2019.\n[189] M. S. Hadi, A. Q. Lawey, T. E. El-Gorashi, and J. M.\nElmirghani, “Patient-centric hetnets powered by machine\nlearning and big data analytics for 6g networks,” IEEE Access,\nvol. 8, pp. 85 639–85 655, 2020.\n[190] C. She, R. Dong, Z. Gu, Z. Hou, Y. Li, W. Hardjawana,\nC. Yang, L. Song, and B. Vucetic, “Deep learning for ultra-\nreliable and low-latency communications in 6g networks,”\narXiv preprint arXiv:2002.11045, 2020.\n[191] C. Huang, S. Hu, G. C. Alexandropoulos, A. Zappone,\nC. Yuen, R. Zhang, M. Di Renzo, and M. Debbah, “Holo-\ngraphic mimo surfaces for 6g wireless networks: Opportuni-\nties, challenges, and trends,” IEEE Wireless Communications,\n2020.\n[192] N. Kato, B. Mao, F. Tang, Y. Kawamoto, and J. Liu, “Ten\nchallenges in advancing machine learning technologies toward\n6g,” IEEE Wireless Communications, 2020.\n[193] W. Saad, M. Bennis, and M. Chen, “A vision of 6g wireless\nsystems: Applications, trends, technologies, and open research\nproblems,” IEEE network, vol. 34, no. 3, pp. 134–142, 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NI"
  ],
  "published": "2022-08-16",
  "updated": "2022-08-16"
}