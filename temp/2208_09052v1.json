{
  "id": "http://arxiv.org/abs/2208.09052v1",
  "title": "A Review of Uncertainty for Deep Reinforcement Learning",
  "authors": [
    "Owen Lockwood",
    "Mei Si"
  ],
  "abstract": "Uncertainty is ubiquitous in games, both in the agents playing games and\noften in the games themselves. Working with uncertainty is therefore an\nimportant component of successful deep reinforcement learning agents. While\nthere has been substantial effort and progress in understanding and working\nwith uncertainty for supervised learning, the body of literature for\nuncertainty aware deep reinforcement learning is less developed. While many of\nthe same problems regarding uncertainty in neural networks for supervised\nlearning remain for reinforcement learning, there are additional sources of\nuncertainty due to the nature of an interactable environment. In this work, we\nprovide an overview motivating and presenting existing techniques in\nuncertainty aware deep reinforcement learning. These works show empirical\nbenefits on a variety of reinforcement learning tasks. This work serves to help\nto centralize the disparate results and promote future research in this area.",
  "text": "arXiv:2208.09052v1  [cs.LG]  18 Aug 2022\nA Review of Uncertainty for Deep Reinforcement Learning\nOwen Lockwood1, Mei Si 2\n1 Department of Computer Science, Rensselaer Polytechnic Institute\n2 Department of Cognitive Science, Rensselaer Polytechnic Institute\nlockwo@rpi.edu, sim@rpi.edu\nAbstract\nUncertainty is ubiquitous in games, both in the agents play-\ning games and often in the games themselves. Working with\nuncertainty is therefore an important component of success-\nful deep reinforcement learning agents. While there has been\nsubstantial effort and progress in understanding and working\nwith uncertainty for supervised learning, the body of litera-\nture for uncertainty aware deep reinforcement learning is less\ndeveloped. While many of the same problems regarding un-\ncertainty in neural networks for supervised learning remain\nfor reinforcement learning, there are additional sources of un-\ncertainty due to the nature of an interactable environment. In\nthis work, we provide an overview motivating and presenting\nexisting techniques in uncertainty aware deep reinforcement\nlearning. These works show empirical beneﬁts on a variety\nof reinforcement learning tasks. This work serves to help to\ncentralize the disparate results and promote future research in\nthis area.\nIntroduction\nDeep reinforcement learning techniques have become the\nstate of the art methods for a number of games including\ngames such as Chess, Go, Shogi (Silver et al. 2018), and\nDota 2 (Berner et al. 2019) to name a few. However, the\nmuch of the standard suite of deep reinforcement learning\nalgorithms have little to no awareness or quantiﬁcation of\nthe uncertainty in either the agent or the environment. This\ncan lead to brittleness in cases in which the reinforcement\nlearning agents perform unsuccessfully when encountering\nnew situations. Building uncertainty aware agents is impor-\ntant for building robust and versatile agents.\nThe task of quantifying and incorporating uncertainty in\nneural networks is essential for a variety of tasks, such as\nimperfect information games (e.g. poker, SCII), increasing\nexploration of RL agents, ﬂagging high uncertainty sam-\nples for human review (in real world deployments), guard-\ning against adversarial examples, active learning, etc. The\ntechniques for supervised learning have substantially grown\nwith the explosion of deep learning in the last decade\n(Gawlikowski et al. 2021; Pearce, Leibfried, and Brintrup\n2020; Kendall and Gal 2017; Gal and Ghahramani 2016).\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nThese works usually focus on one of two areas of uncer-\ntainty, the uncertainty in the dataset, i.e. noise in the labels\nylabel = ytrue + ǫ, or the uncertainty of the neural network\nmodel. With the success of uncertainty modelling in the su-\npervised learning ﬁeld, reinforcement learning has begun to\nadopt some of these techniques. Building upon these ap-\nproaches, uncertainty aware reinforcement learning has been\nable to so improvements in empirical performance and data\nefﬁciency.\nIn this work we outline some of the advances being made\nin uncertainty aware deep reinforcement learning. These\ntechniques have resulted in improved performance on hard\nexploration games, continuous control tasks, and hold the\ncurrent state of the art performance for ofﬂine reinforcement\nlearning. We begin by outlining some of the background on\ndeep reinforcement learning. We then connect uncertainty\nto deep reinforcement learning and how recent works have\nattempted to address uncertainty and the results that these\nworks have shown.\nReinforcement Learning\nMarkov Decision Process\nReinforcement learning is often formalized via the Markov\nDecision Process (MDP), deﬁned by the tuple ⟨S, A, R, P⟩.\nIn this representation, S is the set of states, A the set of\nactions, R is a function mapping states and actions to re-\nwards, and P is the transition probability (i.e. the prob-\nably that a given state follows another given some in-\nput action). γ is sometimes included in this tuple and is\nused to denote the discount factor on the reward. The\ngoal of reinforcement learning is to maximize the nu-\nmerical reward signal (Sutton and Barto 2018). In this\nframework, we write this objective function as J(π) =\nmaxπ Ea∼π\nhPT\ni γiR(si, ai)\ni\n. In other words, we wish to\nﬁnd a policy, π, such that it maximizes the sum of the dis-\ncounted rewards. This can also be formulated as a maximiza-\ntion of the Q function. The Q function, also called the action\nvalue function (Watkins and Dayan 1992) , is deﬁned as the\nexpected return of a policy from a given state action pair.\nModel Free Algorithms\nIn order to maximize this numerical reward, contemporary\ndeep learning methods use a variety of techniques. Here we\nfocus on model free algorithms. These algorithms gener-\nally fall two main categories: value based and actor-critic\nmethods. The most common value based method is Deep\nQ Learning (Mnih et al. 2013). Deep Q Learning replaces\nthe dynamic programming approach to Q learning with a\nDeep Q Network (DQN) that learns to approximate the Q\nvalues of the actions given an input state. These Deep Q\nNetworks are updated via the Mean-Squared Bellman Error,\nL(θ) = (r + γ max Q(si+1, ai+1, θ) −Q(si, ai, θ))2. This\napproach has a number challenges, such as a moving target,\nover estimation of Q values, and wall clock time inefﬁciency.\nNumerous modiﬁcations have been proposed, with varying\ndegrees of improvements (Hessel et al. 2018). Distributional\nQ algorithms, such as QR-DQN (Dabney et al. 2018), pre-\ndict the distribution of expected rewards Z, rather than the\nexact Q value. Note that this distribution is a poor estimation\nof the uncertainty (Osband, Aslanides, and Cassirer 2018).\nHowever, even with these improvements limitations remain.\nThe learning efﬁciency is generally lower than actor-critic\nmethods and they are limited to discrete action environ-\nments.\nActor critic methods use both a policy neural network\nand a critic neural network to help inform the training of\nthe policy. Deep Deterministic Policy Gradient (DDPG)\n(Lillicrap et al. 2015) uses a deterministic policy network\nupdated via information from the Deep Q Network by\nthe deterministic policy gradient theorem, ∇J(µθ)\n=\nE [∇µθ(s)∇Q(s, µθ(s))]. It relies on the Mean Squared\nBellman Error to update the Q network. Stochastic policy al-\ngorithms parameterized a distribution over the action space,\nin opposition to the above deterministic policy. This distri-\nbution is traditionally represented as a Gaussian, with mean\nµθ and standard deviation σθ. Proximal Policy Optimization\n(PPO) is one such algorithm (Schulman et al. 2017). PPO re-\nlies on an advantage based critic, where A(s, a) = Q(s, a)−\nV (s), implemented via a value neural network which al-\nlows an approximation of the advantage via A(st, at) =\nrt+1 + γV (st+1) −V (st). The policy is then updated via\na clipped objective function which allows for efﬁcient up-\ndating of the policy, without taking too large of a step in the\nwrong direction. Another important stochastic actor critic al-\ngorithm is Soft Actor Critic (SAC) (Haarnoja et al. 2018).\nAs an entropy maximization algorithm, this has a different\nobjective function than previous examples with an added\nterm H(π), characterizing the entropy of this policy. When\nmaximizing the Q function we can use the soft Q Bellman\nerror and the policy can be done via soft policy updates, both\nof which rely on the added term of the negative log of the\npolicy distribution to maximize for entropy. Although actor-\ncritic algorithms are more common, uncertainty estimation\ngenerally occurs on the critic/Q-function.\nAll of the above algorithms were designed for online re-\ninforcement learning (in which the agent primarily learns\nthrough its own interactions with the environment). How-\never, another ﬁeld of reinforcement learning has grown in\nprominence: ofﬂine reinforcement learning. In this case the\nagent has access to a large static dataset of tuples of obser-\nvations, actions, and rewards and minimal (or no) access to\ntraining in the environment. This has a number of challenges\nthat extend beyond the typical challenges of ﬁtting a dataset\n(Levine et al. 2020). Uncertainty estimation has proved to\nbe essential for this subﬁeld of RL and many deep RL algo-\nrithms for ofﬂine RL have some sort of implicit or explicit\nuncertainty estimation.\nUncertainty in Deep Reinforcement Learning\nWhen discussing uncertainty we can often decompose it into\ntwo sources: aleatoric and epistemic. Aleatoric uncertainty,\noriginating from the Latin alea meaning dice, also called\nstatistical uncertainty, is uncertainty that originates from the\nstochastic nature of the environment and interactions with\nthe environment. Although this uncertainty can be modelled\nand evaluated, it cannot be reduced. For example, Chess is\na game that has zero aleatoric uncertainty whereas Poker\nhas important aleatoric uncertainty. Epistemic uncertainty,\nalso called model uncertainty or systematic uncertainty, is\nuncertainty that originates from the current limitations of\nthe training of the neural network. Epistemic uncertainty\nis reducible. For example, the epistemic uncertainty of a\nclassiﬁer is high early in the training but should decrease\nwith more iterations over more data. It is important to ac-\nknowledge that the division of these uncertainties are not\nabsolute but represent helpful context dependent heuristics\n(H¨ullermeier and Waegeman 2021). As an example, con-\nsider adding a feature (and embedding the problem into a\nhigher dimension), that results in separable data. This ‘re-\nduces’ the aleatoric uncertainty, but also modiﬁes the prob-\nlems, highlighting the importance of context.\nAleatoric Uncertainty\nAleatoric uncertainty stems from the behavior and interac-\ntions of the environment the reinforcement learning agents\nare trained on. Hence, the importance of aleatoric uncer-\ntainty varies substantially depending on the application.\nThere are 3 main potential sources of aleatoric uncertainty in\nreinforcement learning (effectively one for each component\nof the MDP): stochastic rewards, stochastic observations,\nand stochastic actions. If the reward function is stochastic,\nthere is irreducible uncertainty regarding the true value. The\nstochastic observations can stem from incomplete observa-\ntions or stochastic transition dynamics. Incomplete obser-\nvation games (also called imperfect information games) are\nextremely common and represent some of the biggest chal-\nlenges to RL, such as StarCraft II, Dota 2, etc. No amount\nof training can make an agent see through the fog of war\nin StarCraft II, hence why this is an example of irreducible\n(aleatoric) uncertainty. If the P function in the MDP is non-\ndeterministic, then the transition from one state to the next\nis a source of aleatoric uncertainty. In cases like this, for ex-\nample Poker, Blackjack, etc., the uncertainty what the next\nobservation will be is still there, but in some cases this tran-\nsition function (although uncertain) can be calculated. Fi-\nnally, if the actions are stochastic, there is uncertainty about\nwhat the next state will be (since the action is uncertain). The\nobvious example would be any stochastic policy algorithm\n(PPO, SAC) in which the action is chosen from a distribution\ninstead of a deterministic point.\nWhile seemingly unwanted, aleatoric uncertainty can be\ninjected into games to improve them as benchmarks. For ex-\nample, Atari games are deterministic, and thus have zero\naleatoric uncertainty (assuming the observation represen-\ntation is markovian). However, fully deterministic single\nplayer games are problematic as benchmarks since the\nagents can just memorize the correct set of steps to max-\nimize reward (rather than actually ‘learning’ to play the\ngame) which has led to forced insertion of aleatoric uncer-\ntainty into the environments via techniques such as sticky\nactions, in which there is a chance that the same action\nwill be used regardless of the agent’s input (Machado et al.\n2018). This forced action uncertainty makes for a more difﬁ-\ncult (and effective) benchmark. Although all three sources of\naleatoric uncertainty can be estimated and potentially quan-\ntiﬁed they cannot be reduced like epistemic uncertainties.\nThat does not mean that awareness is unimportant. Differen-\ntiating between areas of high epistemic uncertainty and high\naleatoric uncertainty can be essential in the training of an\nagent. An area of high uncertainty in an environment may\nwant to be explored, but if the uncertainty stems entirely\nfrom aleatoric sources, it will result in ineffective training to\ncontinue to visit that area (since the agent is already knowl-\nedgeable of that area, there just isn’t enough information\nto make a certain decision). Hence, aleatoric uncertainty\nawareness is crucial for deep reinforcement learning algo-\nrithms, even if it cannot be reduced.\nEpistemic Uncertainty\nIn deep reinforcement learning, uncertainty is connected\nwith some of the foundational problems. For example, the\ntrade-off of exploration versus exploitation in reinforcement\nlearning, in which an agent must decide whether to explore\nnew policies and potentially get a lower reward (or poten-\ntially discover a better policy) or exploit its current policy to\nget a known reward but have the opportunity cost of missing\nout on a potentially better path. The challenge of effective\nexploration is connected to epistemic uncertainty. As a help-\nful model, let us consider a localized epistemic uncertainty,\ni.e. rather than epistemic uncertainty over the whole envi-\nronment consider the epistemic uncertainty with respect to\na speciﬁc region of the state space. In a deterministic game,\nthe epistemic uncertainty of a subset of the state (or observa-\ntion) space correlates with the exploration (or lack thereof)\nof the subset. High epistemic uncertainty means the agent is\nuncertain about the policy (or value) in that subspace, hence\nthe region is underexplored. This relationship is often im-\nplicit in exploration strategies for deep reinforcement learn-\ning (and will be discussed more in next section). Even basic\nbenchmarks like Atari required implicit uncertainty aware-\nness to achieve human level performance (Badia et al. 2020)\nas basic algorithms (PPO, SAC, DQN, etc.) are insufﬁcient.\nEpistemic uncertainty is also connected to some of the\nlimitations mentioned in the reinforcement learning section.\nIf we have a neural network learning to approximate a Q\nfunction a common problem is the overestimation of the Q\nvalues. This overestimation is a result of the fact that the esti-\nmates of the Q values are noisy and E[max Q] ≥max E[Q],\nthus resulting in Q value estimates that can be substantially\nlarger than the true values. This highlights the challenges\nof reducing epistemic uncertainty, which is not always triv-\nially reducible through more data or training. DQNs can\nexperience drops in performance over long training times\ndue to catastrophic forgetting (i.e. forgetting how to perform\nwell early on in the environment because it has been train-\ning primarily on observations from late in the environment),\nso simply running the training algorithm for longer will not\nalways decrease epistemic uncertainty. This highlights that\neven in fully deterministic environments,uncertainty can not\nbe trivially wished away and there is a need for uncertainty\naware algorithms.\nExploration and Uncertainty\nResearch into epistemic uncertainty for reinforcement learn-\ning has ﬁgured prominently in exploration research, al-\nthough it is not always explicitly mentioned. There are many\nways to estimate epistemic uncertainty, and a number of\nworks implicitly rely on uncertainty estimation. Although\nwe will focus on explicit uncertainty calculations later in this\npaper, we highlight a few prominent implicit calculations (or\ncalculations by proxy) here since exploration remains an ac-\ntive and important area of research and understanding these\nconnections can help provide insight into both exploration\nand uncertainty quantiﬁcation. One such exploration tech-\nnique is count based methods. Count based methods have\na simple origin, assuming our agent can learn equally well\nfrom all examples, if we want to minimize epistemic un-\ncertainty we need to provide training information from all\nparts of the state space. Therefore, the areas with the highest\nepistemic uncertainty are the areas with the least training ex-\namples. If we simply count and keep track of how often ev-\nery state has been trained on, we can easily minimize uncer-\ntainty (by seeking out the least visited states) and efﬁciently\nexplore. However, this quickly becomes infeasible for large\nstate spaces.\nContemporary count based methods attempt to approxi-\nmate this ideal foundation using more tractable methods. In\none of the foundational methods of count based exploration,\n(Bellemare et al. 2016) presented a method based on den-\nsity models and pseduo-counts. Speciﬁcally, a density model\nprovides the ratio between the pseudo-count function, ˆN(x)\nand the pseudo-count total ˆn. These pseduo-counts are es-\ntimates of the true count function through the proxy of the\ndensity model (since ˆN is really the function we care about).\nThis density model ρi(x) is a function of a given observation\nx, that it has encountered i times. This model must satisfy\nthe inequality ρi+1(x) =\nˆ\nNi+1(x)+1\nˆn+1\n≥ρi(x) =\nˆ\nNi(x)\nˆn\n. We\ncan then approximate the value of interest ˆNi(x) from this\ndensity model alone. This count is then used as an intrinsic\nreward to direct the model to areas of high uncertainty. An-\nother way to approximate this count function is by mapping\nhigh dimensional observations to lower dimensional repre-\nsentations (then using these representations to count with)\n(Tang et al. 2017). The principle remains the same: to ap-\nproximate the uncertainty from the data side, i.e. by count-\ning (or approximately counting) the amount of training done\non different areas of the observation space. Note that this is\nPaper\nOnline/Ofﬂine\nUncertainty Method\nBase Algorithm\nType of Uncertainty\n(Moerland et al. 2017)\nOnline\nMC-Dropout & Variance Networks\nDistributional DQN\nEpistemic & Aleatoric\n(Kalweit and Boedecker 2017)\nOnline\nBootstrapped Q\nModel Based DDPG\nEpistemic\n(Osband et al. 2018)\nOnline\nBootstrapped Q & Prior Networks\nDQN\nEpistemic\n(Clements et al. 2019)\nOnline\nBootstrapped Q (from MAP)\nQR-DQN\nEpistemic & Aleatoric\n(Yu et al. 2020)\nOfﬂine\nVariance of Dynamics Model\nSAC\nEpistemic\n(Peer et al. 2021)\nOnline\nBootstrapped Q\nDQN\nEpistemic\n(An et al. 2021)\nOfﬂine\nBootstrapped Q\nSAC\nEpistemic\n(Wu et al. 2021)\nOfﬂine\nMC-Dropout\nActor-Critic\nEpistemic\n(Hiraoka et al. 2022)\nOnline\nMC-Dropout & Bootstrapped Q\nSAC\nEpistemic\n(Mai, Mani, and Paull 2022)\nOnline\nBootstrapped Variance Q Networks\nSAC\nEpistemic & Aleatoric\n(Lee et al. 2022)\nOfﬂine\nBootstrapped Q and Policy\nSAC + CQL\nEpistemic\n(Bai et al. 2022)\nOfﬂine\nBootstrapped Q\nSAC\nEpistemic\n(Ghasemipour et al. 2022)\nOfﬂine\nBootstrapped Q\nActor-Critic\nEpistemic\n(Mavor-Parker et al. 2022)\nOnline\nVariance Networks\nPPO\nAleatoric\nTable 1: A Selection of Works in Uncertainty Aware Deep Reinforcement Learning\nan approximation of the uncertainty, and the quality of this\napproximation is dependent on the density model’s task rel-\nevance (Osband et al. 2019). Another way to approximate\nthis uncertainty uses a neural network to learn to approx-\nimate this uncertainty. One such example is Random Net-\nwork Distillation (RND) that uses a static neural network\nthat is a function of the state and another neural network\nthat is trained to predict the output of the static neural net-\nwork (Burda et al. 2018). This loss is used as an exploration\nbonus (as the more times the state has been visited, the more\nopportunities there are to learn and the lower the loss will\nbe) and as a proxy for model uncertainty.\nAddressing Uncertainty in Reinforcement\nLearning\nA overview of the key papers we outline in this section can\nbe found in Table 1. The base algorithm is the deep rein-\nforcement learning algorithm that the uncertainty quantiﬁca-\ntion is incorporated into. An empirical comparison of the of-\nﬂine algorithms is presented in Table 2. Behavioral Cloning\n(BC) , Conservative Q Learning (CQL) (Kumar et al. 2020),\nand Twin Delayed Deep Deterministic Policy Gradient Be-\nhavioral Cloning (TD3 + BC) (Fujimoto and Gu 2021) are\nused as baselines. All results are taked from the paper they\noriginally appeared in. Note that (Lee et al. 2022) is not in-\ncluded as it focuses on the transition from ofﬂine to online\nRL. In (Ghasemipour, Gu, and Nachum 2022), the results\nare only presented in graph form, so values may not be ex-\nact. The results are only presented for ofﬂine RL due to the\nubiquity of the D4RL benchmark. Although there are pop-\nular benchmarks in online RL, every paper does not have\nthe exact same environments. The papers represented in this\nwork should provide a large spectrum of techniques and al-\ngorithms to help provide insight into the breadth of research\nin uncertainty aware RL.\nCommon Techniques\nNow let us consider how we can address the aforementioned\nsources of uncertainty. First, let us note that by variance net-\nworks, we mean neural networks with heads f and φ, the lat-\nter of which directly outputs an estimate of the variance. Two\nof the most common and important techniques for these ap-\nproaches are bootstrapping and Monte Carlo (MC) dropout.\nThe Bootstrapped DQN was introduced by (Osband et al.\n2016) as a method for efﬁcient exploration. This method is\na modiﬁcation of the traditional DQN neural network archi-\ntecture that uses a shared torso with K, K ∈N heads (in\nthe original paper K = 10). By computing the variance of\nthe heads predictions, we have an estimation for the poste-\nrior. Since the heads are picked for training uniformly, they\nwill ideally overlap only when they have all arrived at the\noptimal Q function. This method provides an estimation of\nepistemic uncertainty and will form the basis of many more\nadvanced techniques.\nDropout is a method to reduce overﬁtting in neural net-\nworks by stochastically dropping (setting the weight to 0)\nneurons. This dropout is traditionally done during training\nand removed during inference. It is common knowledge that\nusing dropout for neural networks in reinforcement learning\nis a bad idea (due to the non-stationary targets and the lack\nof concerns with direct overﬁtting).However, dropout can be\nused as a method to approximate model (epistemic) uncer-\ntainty. MC Dropout involes keeping the dropout layers dur-\ning inference and sampling repeatedly, through which one\ncan attain an estimate of the uncertainty by looking at the\nvariance of these predictions (Gal and Ghahramani 2016).\nThis variance serves to estimate the epistemic uncertainty.\nBoth of these methods are conceptually straightforward and\nrelatively easy to implement, making them a strong basis for\nmany approaches in deep reinforcement learning.\nBootstrapping\nHere we highlight a set of papers that primarily rely on boot-\nstrapping to estimate the uncertainty. We provide a brief\noverview of what the work did and show how it ﬁts into the\nbroader narrative. (Kalweit and Boedecker 2017) replaces\nthe vanilla deep Q networks with bootstrapped DQNs in\nmodel based DDPG. They showed that this approach can\nbe up to 15 times as efﬁcient and achieve hundreds of times\nlarger rewards on continuous control tasks such as Reacher.\nThis is representative of a common trend (although some-\ntimes presented with other modiﬁcations), that simple incor-\nEnvironment\nBC\nCQL\nTD3 + BC\nYu et al.\nAn et al.\nWu et al.\nBai et al.\nGhasemipour et al.*\nRandom HalfCheetah\n2.1\n35.4\n10.2\n35.4\n28.4\n14.5\n13.1\nRandom Hopper\n9.8\n10.8\n11.0\n11.7\n25.3\n22.4\n31.6\nRandom Walker2D\n1.6\n7.0\n1.4\n13.6\n16.6\n15.5\n8.8\nMedium HalfCheetah\n36.1\n44.4\n42.8\n42.3\n65.9\n46.5\n58.2\n70\nMedium Hopper\n29.0\n58.0\n99.5\n28.0\n101.6\n88.9\n81.6\n75\nMedium Walker2D\n6.6\n79.2\n79.7\n17.8\n92.5\n57.5\n90.3\n80\nExpert HalfCheetah\n107.0\n104.8\n105.7\n106.8\n128.6\n96.2\n98\nExpert Hopper\n109.0\n109.9\n112.2\n110.1\n135.0\n110.4\n108\nExpert Walker2D\n125.7\n153.9\n105.7\n115.1\n121.1\n109.8\n112\nMedium Expert HalfCheetah\n35.8\n62.4\n97.9\n63.3\n106.3\n127.4\n93.6\n96\nMedium Expert Hopper\n111.9\n110.0\n112.2\n23.7\n110.7\n134.7\n111.2\n110\nMedium Expert Walker2D\n6.4\n98.7\n101.1\n44.6\n114.7\n99.7\n109.8\n112\nTable 2: Empirical Comparison on subset of D4RL of Uncertainty Aware Ofﬂine Methods\nporation of existing uncertainty aware techniques can yield\nempirical gains.\nAlthough the majority of approaches are focused on esti-\nmating epistemic uncertainty, it can be important to decou-\nple the sources of uncertainty. This idea of decoupled uncer-\ntainty awareness is explored by (Clements et al. 2019). In\nthis work, their estimates rely on approximate maximum a\nposteriori (MAP) sampling (Pearce, Leibfried, and Brintrup\n2020) to generate two sets of neural network parameters\nθa, θb. The epistemic uncertainty can then be evaluated via\nthe mean squared difference in the predictions of the sam-\npled parameters. Hence, when the variance in the sampled\nparameters decreases (as the neural network converges), this\napproximation of epistemic uncertainty will decrease. Al-\nthough this is different than other bootstrapped approaches,\nit is a variant of the same approach. The MAP serves effec-\ntively to generate better versions of the heads from Boot-\nstrapped DQN, hence requiring only two of them (as op-\nposed to 10 or more). The aleatoric uncertainty is estimated\nvia the covariance of these predictions. These parameters are\nthen used in action selection (and action selection only), and\nthis uncertainty is not evaluated directly in the loss func-\ntions/training of the neural network. They are able to show\nsubstantial improvements on the whole of the MinAtar en-\nvironments (smaller versions of the Atari benchmark). This\nshows two key results: that uncertainty awareness can yield\nbeneﬁts even when used in action selection alone and the\npotential of decoupling uncertainties even in environments\nwhich do not seem to present much uncertainty (e.g. deter-\nministic Atari games).\n(Peer et al. 2021) builds upon both BDQN and DDQN.\nThey present a DQN variant which uses K independent net-\nworks. Whereas BDQN only uses separate heads for each\nprediction, this work uses entirely separate networks. While\nDDQN uses separate networks only for target prediction,\nthis work uses them for action selection as well. By simply\nusing K = 5 networks, they are able to achieve comparable\nperformance to Rainbow (Hessel et al. 2018) on Atari. The\nimportance of this result is its conceptual simplicity. Such\na simple DQN expansion is able to compete with Rainbow\n(i.e. all previous combined DQN modiﬁcations).\n(An et al. 2021) added an ensemble of N Q function on\nSAC (which usually has 2 Q functions), to enable a boot-\nstrapped estimate of epistemic uncertainty. The agent’s un-\nderstanding of epistemic uncertainty enabled it to perform\nwell on Out of Distribution (OOD) data (which is impor-\ntant for ofﬂine RL). OOD data is data that is collected un-\nder different conditions than the training data. This sim-\nple modiﬁcation of adding Q functions (although it some-\ntimes required up to 500), shows how a conceptually simple\nuncertainty aware modiﬁcation can increase performance.\nExpanding upon this, Ensemble-Diversiﬁed Actor Critic\n(EDAC) was also introduced, which is extremely similar to\nN-SAC but adds a term to the soft Q update to increase\nthe variance of the Q functions when encountering OOD\ndata. Conceptually, this is simply increasing the accuracy\nof the epistemic uncertainty approximation (since the model\nshould have high epistemic uncertainty when it encounters\nunfamiliar data/situations). They were able to show state of\nthe art performance at the time of their writing (and remains\ncompetitive even with current works) and improve upon a\nheavily model based ﬁeld.\n(Lee et al. 2022) used bootstrapped uncertainty estimates\nof the both the actor and critic and combined them with\nConservative Q Learning (Kumar et al. 2020) to achieve im-\nprovements on the transition of ofﬂine to online continuous\ncontrol tasks. Although the most common choice of combi-\nnation (i.e. the algorithm the uncertainty estimation is incor-\nporated into) is SAC, this highlights how different choices\nfor this algorithm can impact the efﬁcacy of uncertainty es-\ntimation.\n(Bai et al. 2022) uses the standard deviation of boot-\nstrapped prior deep Q networks as an uncertainty quantiﬁca-\ntion. This uncertainty is then subtracted from the predicted\nQ value of the next state (essentially penalizing high uncer-\ntainty states). This is done for both in distribution (ID) data\nand OOD. The ID data is the data provided in the dataset\nand they generate OOD data by feeding a state into the ac-\ntor (which will predict something different than the dataset,\nhence OOD) and estimate both the ID Q errors and OOD Q\nerrors to minimize their sum. This uncertainty quantiﬁcation\nis then combined with SAC. This highlights the versatility of\nuncertainty estimation and how ﬁguring out the optimal way\nto incorporate and learn from this uncertainty remains very\nmuch an open problem.\n(Ghasemipour, Gu, and Nachum 2022) further develops a\nsimple but important aspect of pessimistic updates: target in-\ndependence. Pessimistic updates, which are common in of-\nﬂine RL and exploration techniques, usually relies on sub-\ntracting the epistemic uncertainty estimation via variance of\nQ values from the predict Q values. Shared targets use pes-\nsimistic predictions for the target values in the Q learning\nupdates, whereas independent targets do not. These inde-\npendent targets have favorable theoretical implications and\nare shown to have empirical beneﬁts for ofﬂine RL. Strong\ntheoretical understanding is important to advancing and de-\nveloping further uncertainty aware RL algorithms.\nMonte Carlo Dropout\nHere\nwe\nhighlight\npapers\nthat\nprimarily\nrely\non\nMC-Dropout\nto\nestimate\nthe\nuncertainty.\nIn\n(Moerland, Broekens, and Jonker 2017), they present a\nmethod for evaluating the epistemic and aleatoric uncertain-\nties simultaneously and use these methods as exploration\nstrategies. They estimate epistemic uncertainty using MC-\ndropout and predict the aleatoric uncertainty over the return\ndistribution p(Z|s, a) by having the neural network output\nµ = Q(s, a) and σ, which results in the modiﬁed Bellman\nerror with an added term to penalize substantial changes in\naleatoric uncertainty and subtly encourages lower aleatoric\nuncertainty. This is able to show some modest improve-\nments on a variety of simple gym benchmarks. Although the\nempirical results are modest, this work presents important\nideas that other papers have built upon.\n(Wu et al. 2021) presents a method of uncertainty aware\nofﬂine reinforcement learning in which MC-dropout is used\nto estimate epistemic uncertainty. They use MC-droupout to\nreduce the effect OOD backups have by scaling each the ac-\ntor and critic errors by the same amount,\nβ\nV ar(Q(s,a)), i.e.\nby the inverse variance of the Q functions (the variance\ncalculated across the MC estimates). This means that high\nvariance estimates (high epistemic uncertainty) will con-\ntribute less to the update process. This also achieves im-\nprovements on ofﬂine continuous control benchmarks, but\ngenerally only outperforms SAC-N and EDAC on the ex-\npert replay examples (i.e. when the ofﬂine dataset contains\ntransitions of a expert completing the task, as opposed to\nmedium skill level or random). Along with (An et al. 2021),\nthis work remains extremely competitive with current ofﬂine\nRL works. Since MC Dropout is a less common approach,\nthis work shows that it is still able to be as effective as boot-\nstrapped methods.\nCombinations and Other Approaches\nHere, we present a collection of methods that do not solid\nrely on a single method or use a method not previously dis-\ncussed. (Osband, Aslanides, and Cassirer 2018) highlights\nsome of the ﬂaws in the previously mentioned methods and\nintroduces the concept of randomized priors as an add-on to\nbootstrapped DQN. First, they show that MC-dropout (and\nthe distribution it generates) is not always a good estimate\nof the posterior. Additionally, they highlight the problem\nwith count based estimates mentioned previously. Speciﬁ-\ncally, that they can be a very poor proxy for uncertainty for\ncases in which state density does not correlate with the true\nuncertainty. To overcome these challenges they suggest sim-\nply adding a random prior to bootstrapped DQN. This op-\nerates in a unique way from the previously discussed tech-\nniques since it doesn’t focus on estimating the posterior.\nWhat this means is that all previous methods relied on es-\ntimating uncertainty based on the data, whereas the prior is\nindependent of the data. This is an important aspect of the\nBayesian approaches that these methods are often claiming\nto approximate. Practically, this is simply a matter of cre-\nating K prior neural networks (with static weights) that are\nthen incorporated into the bootstrap heads so the ﬁnal out-\nput is Q(s, a) = fk(θ, s, a) + pk(φ, s). They show that this\nresults in improved exploration by achieving state of the art\n(at the time) performance on the hard exploration Atari game\nMontezuma’s Revenge.\nIn one of the foundational works of ofﬂine RL, (Yu et al.\n2020) bootstrapped a prediction of the next reward using\nan ensemble of neural networks and penalized the reward\nthat is learned by the policy proportionally to the maximum\nstandard deviation of the predictions in this ensemble. This\nis different than the previous approaches to bootstrapping\nsince it does not rely on Q function estimates. This approach\nachieved substantial improvements across the board on a va-\nriety ofﬂine continuous control tasks.\nIn (Hiraoka et al. 2022), the ideas of dropout and boot-\nstrapped Q values are combined to modiﬁed the Q network\narchitecture to have several heads with dropout layers. By\ncombining the uncertainty estimation techniques, this results\nin a higher quality approximation of the epistemic uncer-\ntainty. These updated Q networks are then used as critics\nwith SAC to achieve better performance substantially faster\n(around 100,000 frames as opposed to the usual 1-3 million)\non the MuJoCo continuous control benchmark.\n(Mavor-Parker et al. 2022) present a unique approach to\naleatoric uncertainty, by using a neural network with two\nheads that directly output the predicted mean and variance\nof the next state. This network variance is a learned esti-\nmation of the aleatoric uncertainty and is updated using the\nMaximum Likelihood Estimation (MLE) loss function, sim-\nilar to the work done in (Kendall and Gal 2017). They are\nable to show substantial improvements in “noisy TV” envi-\nronments (i.e. environments with large aleatoric uncertainty\ntraps). This represents a far different approach to uncertainty\nestimation and highlights the relationship between uncer-\ntainty estimation and exploration.\nBuilding upon many of the above techniques and ap-\nproaches from supervised learning, (Mai, Mani, and Paull\n2022) present a method for “inverse-variance” reinforce-\nment learning with decoupled uncertainty estimates. Specif-\nically, they modify the value function loss over a minibatch\nto be L = LBIV + λLLA. Here, LBIV is the Batch Inverse\nVariance (BIV) which weights the loss function inversely\nproportional to the variance of the noise of the label. This la-\nbel noise is representative of the aleatoric uncertainty (which\nis estimated through the variance networks). The LLA is the\nloss function of variance networks, i.e. the negative log like-\nlihood of the neural network outputs µ and σ (which can\nestimate epistemic uncertainty). By using an ensemble of\nvariance networks, they can bootstrap an estimate of the un-\ncertainty for the BIV loss function. They apply this uncer-\ntainty awareness to the critics of SAC and show substantial\nimprovements on a variety of continuous control tasks.\nDiscussion\nFirst, it is worth noting that most approaches to uncertainty\nestimation in deep RL focus on the critic (which is also the\npolicy in the case of DQN variants). This may seem counter-\nintuitive, seeing as it is the policy which actually makes de-\ncisions and operates in the uncertain world. However, there\nare likely two important reasons why uncertainty estimation\nfor the critic prioritized. For one, during training the critic is\nmore directly a inﬂuenced by the aleatoric uncertainty since\nonly the critic loss is a direct function of the state, action,\nand reward. Second, the actor is downstream of the critic in\nuncertainty propagation. Since all actor-critic algorithms up-\ndates take some form of ∇J = E[∇log(π)δ], with δ being\nsome function dependent on the critic, any uncertainty that\nexists in the critic is passed down to the actor. This does not\npreclude uncertainty considerations for the actor (and this\nis likely the directions of many future works in uncertainty\nestimating RL), but sufﬁcient uncertainty estimation in the\ncritic is important.\nSecond, a brief postulate as to why MC Dropout is less\ncommon than using network heads to bootstrap. MC dropout\nrequires a full network pass for each repetition/sample used\nto generate the distribution of predictions, whereas boot-\nstrapping from heads only requires multiple forward passes\nof a subset of layers (i.e. the heads). This can result in a non-\nnegligible compute difference. However, the relative qual-\nity of the estimates from bootstrapped heads and full MC\nDropout remains an important but under explored area.\nNext, let us note some trends that Table 1 and the expla-\nnations highlight. First, uncertainty estimation can be incor-\nporated into most (if not all) existing reinforcement learning\nalgorithms, and doesn’t require a ground-up re-evaluation.\nAdditionally, it shows how some implementationally sim-\nple techniques, can be used to result in large beneﬁts. These\ntechniques are often built on the back of extensive work done\nin the supervised learning community. These papers show\nthat uncertainty awareness can help improve a variety of al-\ngorithms across types and classes, on different benchmarks\nwith different levels of aleatoric uncertainty.\nLastly, it is important to highlight some of the important\ntakeaways for applications of reinforcement learning, in ad-\ndition to some potential future work. Even though many\ngames involve a substantial amount of uncertainty, quan-\ntiﬁcation often drops by the wayside since basic algorithms\nperform moderately well. Incorporating uncertainty into al-\ngorithms has shown empirical improvements on a variety of\ntasks, even those that are largely deterministic. Given the\npotential beneﬁts (as we have shown throughout) and the\nease of implementation, it is our goal to get a larger com-\nmunity to consider the incorporation of some of these meth-\nods into their practices. There is also room for improvement\nin the techniques presented in this work. Further empirical\ninsight is needed into the relative performance of different\nuncertainty quantiﬁcation methods, since these techniques\nare often combined with other changes it is hard to provide\na meaningful comparison between advancements. It is also\nimportant to quantify how good the uncertainty estimates\nare. Seeing how close the estimates are to true values would\nenable better comparisons of methods. These also speak to\nthe need to develop environments that enable the testing and\nevaluation of uncertainty estimation. More work formalizing\nand experimenting with the relationship between epistemic\nuncertainty and common exploration strategies could also be\nbeneﬁcial.\nConclusion\nAlthough deep reinforcement learning has become the ubiq-\nuitous method to solve a variety of control tasks and games,\nit still suffers from problems such as brittleness and data in-\nefﬁciency. Understanding and incorporating uncertainty into\ndeep RL agents is critical to their success in games and\nreal world situations and their robustness to new environ-\nments. We outlined the basics of contemporary deep RL al-\ngorithms and highlighted the relationship of RL with uncer-\ntainty. Outlining the roles of aleatoric and epistemic uncer-\ntainty, we showed how these connect with the basic ideas\nin the ﬁeld. We presented a variety of techniques for work-\ning with and incorporating awareness of these uncertainties\ninto reinforcement learning algorithms. We show that these\ntechniques are able to improve performance across domains,\nbenchmarks, models, and types of reinforcement learning.\nReferences\nAn, G.; Moon, S.; Kim, J.-H.; and Song, H. O. 2021.\nUncertainty-based ofﬂine reinforcement learning with diver-\nsiﬁed q-ensemble. Advances in Neural Information Process-\ning Systems, 34.\nBadia, A. P.; Piot, B.; Kapturowski, S.; Sprechmann, P.;\nVitvitskyi, A.; Guo, Z. D.; and Blundell, C. 2020. Agent57:\nOutperforming the atari human benchmark. In International\nConference on Machine Learning, 507–517. PMLR.\nBai, C.; Wang, L.; Yang, Z.; Deng, Z.-H.; Garg, A.; Liu,\nP.; and Wang, Z. 2022.\nPessimistic Bootstrapping for\nUncertainty-Driven Ofﬂine Reinforcement Learning. In In-\nternational Conference on Learning Representations.\nBellemare, M.; Srinivasan, S.; Ostrovski, G.; Schaul, T.;\nSaxton, D.; and Munos, R. 2016. Unifying count-based ex-\nploration and intrinsic motivation. Advances in neural infor-\nmation processing systems, 29.\nBerner, C.; Brockman, G.; Chan, B.; Cheung, V.; Debiak,\nP.; Dennison, C.; Farhi, D.; Fischer, Q.; Hashme, S.; Hesse,\nC.; et al. 2019. Dota 2 with large scale deep reinforcement\nlearning. arXiv preprint arXiv:1912.06680.\nBurda, Y.; Edwards, H.; Storkey, A.; and Klimov, O. 2018.\nExploration by random network distillation. arXiv preprint\narXiv:1810.12894.\nClements, W. R.; Van Delft, B.; Robaglia, B.-M.; Slaoui,\nR. B.; and Toth, S. 2019.\nEstimating risk and uncer-\ntainty in deep reinforcement learning.\narXiv preprint\narXiv:1905.09638.\nDabney, W.; Rowland, M.; Bellemare, M.; and Munos, R.\n2018. Distributional reinforcement learning with quantile\nregression. In Proceedings of the AAAI Conference on Arti-\nﬁcial Intelligence, volume 32.\nFujimoto, S.; and Gu, S. S. 2021. A minimalist approach to\nofﬂine reinforcement learning. Advances in neural informa-\ntion processing systems, 34: 20132–20145.\nGal, Y.; and Ghahramani, Z. 2016. Dropout as a bayesian ap-\nproximation: Representing model uncertainty in deep learn-\ning. In international conference on machine learning, 1050–\n1059. PMLR.\nGawlikowski, J.; Tassi, C. R. N.; Ali, M.; Lee, J.; Humt,\nM.; Feng, J.; Kruspe, A.; Triebel, R.; Jung, P.; Roscher, R.;\net al. 2021. A survey of uncertainty in deep neural networks.\narXiv preprint arXiv:2107.03342.\nGhasemipour, S. K. S.; Gu, S. S.; and Nachum, O. 2022.\nWhy So Pessimistic? Estimating Uncertainties for Ofﬂine\nRL through Ensembles, and Why Their Independence Mat-\nters. arXiv preprint arXiv:2205.13703.\nHaarnoja, T.; Zhou, A.; Hartikainen, K.; Tucker, G.; Ha, S.;\nTan, J.; Kumar, V.; Zhu, H.; Gupta, A.; Abbeel, P.; et al.\n2018. Soft actor-critic algorithms and applications. arXiv\npreprint arXiv:1812.05905.\nHessel, M.; Modayil, J.; Van Hasselt, H.; Schaul, T.; Os-\ntrovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and\nSilver, D. 2018. Rainbow: Combining improvements in deep\nreinforcement learning. In Thirty-second AAAI conference\non artiﬁcial intelligence.\nHiraoka, T.; Imagawa, T.; Hashimoto, T.; Onishi, T.; and\nTsuruoka, Y. 2022. Dropout Q-Functions for Doubly Efﬁ-\ncient Reinforcement Learning. In International Conference\non Learning Representations.\nH¨ullermeier, E.; and Waegeman, W. 2021. Aleatoric and\nepistemic uncertainty in machine learning: An introduction\nto concepts and methods. Machine Learning, 110(3): 457–\n506.\nKalweit, G.; and Boedecker, J. 2017.\nUncertainty-driven\nimagination for continuous deep reinforcement learning. In\nConference on Robot Learning, 195–206. PMLR.\nKendall, A.; and Gal, Y. 2017. What uncertainties do we\nneed in bayesian deep learning for computer vision? Ad-\nvances in neural information processing systems, 30.\nKumar, A.; Zhou, A.; Tucker, G.; and Levine, S. 2020.\nConservative q-learning for ofﬂine reinforcement learning.\nAdvances in Neural Information Processing Systems, 33:\n1179–1191.\nLee, S.; Seo, Y.; Lee, K.; Abbeel, P.; and Shin, J. 2022.\nOfﬂine-to-Online Reinforcement Learning via Balanced Re-\nplay and Pessimistic Q-Ensemble. In Conference on Robot\nLearning, 1702–1712. PMLR.\nLevine, S.; Kumar, A.; Tucker, G.; and Fu, J. 2020. Ofﬂine\nreinforcement learning: Tutorial, review, and perspectives on\nopen problems. arXiv preprint arXiv:2005.01643.\nLillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.;\nTassa, Y.; Silver, D.; and Wierstra, D. 2015.\nContinuous\ncontrol with deep reinforcement learning.\narXiv preprint\narXiv:1509.02971.\nMachado, M. C.; Bellemare, M. G.; Talvitie, E.; Veness, J.;\nHausknecht, M.; and Bowling, M. 2018. Revisiting the ar-\ncade learning environment: Evaluation protocols and open\nproblems for general agents. Journal of Artiﬁcial Intelli-\ngence Research, 61: 523–562.\nMai, V.; Mani, K.; and Paull, L. 2022. Sample Efﬁcient Deep\nReinforcement Learning via Uncertainty Estimation. arXiv\npreprint arXiv:2201.01666.\nMavor-Parker, A.; Young, K.; Barry, C.; and Grifﬁn, L.\n2022. How to Stay Curious while avoiding Noisy TVs using\nAleatoric Uncertainty Estimation. In International Confer-\nence on Machine Learning, 15220–15240. PMLR.\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.;\nAntonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Play-\ning atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602.\nMoerland, T. M.; Broekens, J.; and Jonker, C. M. 2017. Ef-\nﬁcient exploration with double uncertain value networks.\narXiv preprint arXiv:1711.10789.\nOsband, I.; Aslanides, J.; and Cassirer, A. 2018. Random-\nized prior functions for deep reinforcement learning. Ad-\nvances in Neural Information Processing Systems, 31.\nOsband, I.; Blundell, C.; Pritzel, A.; and Van Roy, B. 2016.\nDeep exploration via bootstrapped DQN. Advances in neu-\nral information processing systems, 29.\nOsband, I.; Van Roy, B.; Russo, D. J.; Wen, Z.; et al. 2019.\nDeep Exploration via Randomized Value Functions.\nJ.\nMach. Learn. Res., 20(124): 1–62.\nPearce, T.; Leibfried, F.; and Brintrup, A. 2020. Uncertainty\nin neural networks: Approximately bayesian ensembling. In\nInternational conference on artiﬁcial intelligence and statis-\ntics, 234–244. PMLR.\nPeer, O.; Tessler, C.; Merlis, N.; and Meir, R. 2021. Ensem-\nble bootstrapping for Q-Learning. In International Confer-\nence on Machine Learning, 8454–8463. PMLR.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347.\nSilver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai,\nM.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel,\nT.; et al. 2018. A general reinforcement learning algorithm\nthat masters chess, shogi, and Go through self-play. Science,\n362(6419): 1140–1144.\nSutton, R. S.; and Barto, A. G. 2018. Reinforcement learn-\ning: An introduction. MIT press.\nTang, H.; Houthooft, R.; Foote, D.; Stooke, A.; Xi Chen, O.;\nDuan, Y.; Schulman, J.; DeTurck, F.; and Abbeel, P. 2017.\n# exploration: A study of count-based exploration for deep\nreinforcementlearning. Advances in neural information pro-\ncessing systems, 30.\nWatkins, C. J.; and Dayan, P. 1992. Q-learning. Machine\nlearning, 8(3): 279–292.\nWu, Y.; Zhai, S.; Srivastava, N.; Susskind, J.; Zhang, J.;\nSalakhutdinov, R.; and Goh, H. 2021. Uncertainty weighted\nactor-critic for ofﬂine reinforcement learning. arXiv preprint\narXiv:2105.08140.\nYu, T.; Thomas, G.; Yu, L.; Ermon, S.; Zou, J. Y.; Levine,\nS.; Finn, C.; and Ma, T. 2020. Mopo: Model-based ofﬂine\npolicy optimization. Advances in Neural Information Pro-\ncessing Systems, 33: 14129–14142.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-08-18",
  "updated": "2022-08-18"
}