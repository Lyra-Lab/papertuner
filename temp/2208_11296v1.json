{
  "id": "http://arxiv.org/abs/2208.11296v1",
  "title": "Semi-Supervised and Unsupervised Deep Visual Learning: A Survey",
  "authors": [
    "Yanbei Chen",
    "Massimiliano Mancini",
    "Xiatian Zhu",
    "Zeynep Akata"
  ],
  "abstract": "State-of-the-art deep learning models are often trained with a large amount\nof costly labeled training data. However, requiring exhaustive manual\nannotations may degrade the model's generalizability in the limited-label\nregime. Semi-supervised learning and unsupervised learning offer promising\nparadigms to learn from an abundance of unlabeled visual data. Recent progress\nin these paradigms has indicated the strong benefits of leveraging unlabeled\ndata to improve model generalization and provide better model initialization.\nIn this survey, we review the recent advanced deep learning algorithms on\nsemi-supervised learning (SSL) and unsupervised learning (UL) for visual\nrecognition from a unified perspective. To offer a holistic understanding of\nthe state-of-the-art in these areas, we propose a unified taxonomy. We\ncategorize existing representative SSL and UL with comprehensive and insightful\nanalysis to highlight their design rationales in different learning scenarios\nand applications in different computer vision tasks. Lastly, we discuss the\nemerging trends and open challenges in SSL and UL to shed light on future\ncritical research directions.",
  "text": "1\nSemi-Supervised and Unsupervised\nDeep Visual Learning: A Survey\nYanbei Chen, Massimiliano Mancini, Xiatian Zhu, and Zeynep Akata\nAbstract—State-of-the-art deep learning models are often trained with a large amount of costly labeled training data. However,\nrequiring exhaustive manual annotations may degrade the model’s generalizability in the limited-label regime. Semi-supervised\nlearning and unsupervised learning offer promising paradigms to learn from an abundance of unlabeled visual data. Recent progress\nin these paradigms has indicated the strong beneﬁts of leveraging unlabeled data to improve model generalization and provide better\nmodel initialization. In this survey, we review the recent advanced deep learning algorithms on semi-supervised learning (SSL) and\nunsupervised learning (UL) for visual recognition from a uniﬁed perspective. To offer a holistic understanding of the state-of-the-art in\nthese areas, we propose a uniﬁed taxonomy. We categorize existing representative SSL and UL with comprehensive and insightful\nanalysis to highlight their design rationales in different learning scenarios and applications in different computer vision tasks. Lastly, we\ndiscuss the emerging trends and open challenges in SSL and UL to shed light on future critical research directions.\nIndex Terms—Semi-Supervised, Unsupervised, Self-Supervised, Visual Representation Learning, Survey\n!\n1\nINTRODUCTION\nO\nVER the last decade, deep learning algorithms and\narchitectures [1], [2] have been pushing the state of\nthe art in a wide variety of computer vision tasks, rang-\ning from object recognition [3], retrieval [4], detection [5],\nto segmentation [6]. To achieve human-level performance,\ndeep learning models are typically built by supervised\ntraining upon a tremendous amount of labeled training\ndata.However, collecting large-scale labeled training sets\nmanually is not only expensive and time-consuming, but\nmay also be legally prohibited due to privacy, security,\nand ethics restrictions. Moreover, supervised deep learning\nmodels tend to memorize the labeled data and incorporate\nthe annotator’s bias, which weakens their generalization to\nnew scenarios with unseen data distributions in practice.\nCheaper imaging technologies and more convenient ac-\ncess to web data, makes obtaining large unlabeled visual\ndata no longer challenging. Learning from unlabeled data\nthus becomes a natural and promising way to scale mod-\nels towards practical scenarios where it is infeasible to\ncollect a large labeled training set that covers all types\nof visual variations in illumination, viewpoint, resolution,\nocclusion, and background clutter induced by different\nscenes, camera positions, times of the day, and weather\nconditions.Semi-supervised learning [7], [8] and unsuper-\nvised learning [9], [10], [11], [12] stand out as two most\nrepresentative paradigms for leveraging unlabeled data.\nBuilt upon different assumptions, these paradigms are of-\nten developed independently, whilst sharing the same aim\nto learn more powerful representations and models using\nunlabeled data.\n•\nThis work was done when Y.Chen was with the University of T¨ubingen.\nE-mail: yanbeic@gmail.com\n•\nM. Mancini is with the University of T¨ubingen.\nE-mail: massimiliano.mancini@uni-tuebingen.de\n•\nX. Zhu is with the University of Surrey. E-mail: xiatian.zhu@surrey.ac.uk\n•\nZ. Akata is with the University of T¨ubingen, MPI for Informatics and\nMPI for Intelligent Systems. E-mail: zeynep.akata@uni-tuebingen.de\nDeep Visual Learning from Unlabeled Data\nLimited label supervision\nNo label supervision\n(a) Semi-Supervised Learning\n(b) Unsupervised Learning\nsame label space\n& same domain\nclosed-set SSL\ncat\ndog\ndifferent label space\n& same domain\nopen-set SSL\ncat\ndog\nunlabeled data\nlabeled data\nunlabeled data in an unknown label space\nLegend:\nunknown label space\n& unknown domain\nUL\nFig. 1: An overview of semi-supervised and unsupervised\nlearning paradigms – both aim to learn from unlabeled data.\nFigure 1 summarizes the two paradigms covered in this\nsurvey, which both utilize unlabeled data for visual repre-\nsentation learning. According to whether label annotations\nare given for a small portion or none of the training data, we\ncategorize the paradigms as semi-supervised learning, and\nunsupervised learning as deﬁned explicitly in the following.\n(a) Semi-Supervised Learning (SSL) aims to jointly learn\nfrom sparsely labeled data and a large amount of\nauxiliary unlabeled data often drawn from the same\nunderlying data distribution as the labeled data. In\nstandard closed-set SSL [8], [13], the labeled and un-\nlabeled data belong to the same set of classes from the\nsame domain. In open-set SSL [14], [15], they may not\nlie in the same label space, i.e., the unlabeled data may\ncontain unknown and/or mislabeled classes.\n(b) Unsupervised Learning (UL) aims to learn from only\nunlabeled data without utilizing any task-relevant label\nsupervision. Once trained, the model can be ﬁne-tuned\nusing labeled data to achieve better model generaliza-\ntion in a downstream task [16].\narXiv:2208.11296v1  [cs.CV]  24 Aug 2022\n2\nFollowing the above deﬁnitions, let the sets of labeled\ndata and unlabeled data be denoted as Dl and Du. The\noverall uniﬁed learning objective for SSL and UL is:\nmin\nθ\nλl\nX\n(x,y)∈DL\nLsup(x, y, θ) + λu\nX\nx∈DU\nLunsup(x, θ),\n(1)\nwhere θ refers to the model parameters of a deep neural net-\nwork (DNN); x is an input image and y is the corresponding\nlabel; Lsup and Lunsup are the supervised and unsupervised\nloss terms; λl and λu are balancing hyperparameters. In\nSSL, both loss terms are jointly optimized. In UL, only the\nunsupervised loss term is used for unsupervised model pre-\ntraining (i.e., λl = 0). Although SSL and UL share the same\nrationale of learning with an unsupervised objective, they\ndiffer in the learning setup, leading to different unique chal-\nlenges. Speciﬁcally, SSL assumes the availability of limited\nlabeled data, and its core challenge is to expand the labeled\nset with abundant unlabeled data. UL assumes no labeled\ndata for the main learning task and its key challenge is to\nlearn task-generic representations from unlabeled data.\nWe focus on providing a timely and comprehensive\nreview of the advances in leveraging unlabeled data to\nimprove model generalization, covering the representative\nstate-of-the-art methods in SSL and UL, their application\ndomains, to the emerging trends in self-supervised learn-\ning. Importantly, we propose a uniﬁed taxonomy of the\nadvanced deep learning methods to offer researchers a sys-\ntematic overview that helps to understand the current state\nof the art and identify open challenges for future research.\nComparison with previous surveys. Our survey is related\nto other surveys on semi-supervised learning [8], [13], [17],\nself-supervised learning [18], [19], or both topics [20]. While\nthese surveys mostly focus on a single particular learning\nsetup [8], [13], [17], [18], non-deep learning methods [8],\n[13], or lacking a comprehensive taxonomy on methods and\ndiscussion on applications [20], our work covers a wider\nreview of representative SSL and UL algorithms involving\nunlabeled visual data. Importantly, we categorize the state-\nof-the-art SSL and UL algorithms with novel taxonomies\nand draw connections among different methods. Beyond\nintrinsic challenges with each learning paradigm, we distill\ntheir underlying connections from the problem and algo-\nrithmic perspectives, discuss unique insights into different\nexisting techniques, and their practical applicability.\nSurvey organization and contributions. Our contributions\nare three fold. First, to our knowledge, this is the ﬁrst deep\nlearning survey of its kind to provide a comprehensive\nreview of three prevalent machine learning paradigms in\nexploiting unlabeled data for visual recognition, including\nsemi-supervised learning (SSL, §2), unsupervised learning\n(UL, §3), and a further discussion on SSL and UL (§4).\nSecond, we provide a uniﬁed, insightful taxonomy and anal-\nysis of the existing methods in both the learning setup and\nmodel formulation to uncover their underlying algorithmic\nconnections. Finally, we outlook the emerging trends and\nfuture research directions in §5 to shed light on those under-\nexplored and potentially critical open avenues.\n2\nSEMI-SUPERVISED LEARNING (SSL)\nSemi-Supervised Learning (SSL) [8], [13] aims at exploiting\nlarge unlabeled data together with sparsely labeled data.\nsupervised loss \nunsupervised loss \n✓\nmodel\nxl\nxu\nlabeled sample\nunlabeled sample\nLsup\np(y|xl; ✓)\nLunsup\np(y|xu; ✓)\nFig. 2: Semi-supervised learning (SSL) aims to learn jointly\nfrom a small set of labeled and a large set of unlabeled data.\nSSL is explored in various application domains, such as\nimage search [21], medical data analysis [22], web-page\nclassiﬁcation [23], document retrieval [24], genetics and\ngenomics [25]. More recently, SSL has been used for learning\ngeneric visual representations to facilitate many computer\nvision tasks such as image classiﬁcation [26], [27], image\nretrieval [28], object detection [29], [30], semantic segmenta-\ntion [31], [32], [33], and pose estimation [34], [35], [36]. While\nour review mainly covers generic semi-supervised learners\nfor image classiﬁcation [26], [27], [37], [38], the ideas behind\nthembe generalized to solve other vision recognition tasks.\nWe deﬁne the SSL problem setup and discuss its assump-\ntions in §2.1. We provide a taxonomy and analysis of the\nexisting semi-supervised deep learning methods in §2.2.\n2.1\nThe Problem Setting of SSL\nProblem Deﬁnition. In SSL, we often have access to a\nlimited amount of labeled samples Dl = {xi,l, yi}Nl\ni=1 and\na large amount of unlabeled samples Du = {xi,u}Nu\ni=1.\nEach labeled sample xi,l belongs to one of K class labels\nY = {yk}K\nk=1. For training, the SSL loss function L for a\ndeep neural network (DNN) θ can generally be expressed as\nEq. (1), i.e., L = λlLsup + λuLunsup. In many SSL methods,\nthe hyperparameters λu in Eq. (1) is often a ramp-up weight-\ning function (i.e., λ = w(t) and t is training iteration), which\ngradually increases the importance of the unsupervised loss\nterm during training [14], [37], [39], [40], [41]. At test time,\nthe model is deployed to recognize the K known classes.\nSee Figure 2 for an illustration of SSL.\nEvaluation Protocol. To test whether an SSL model utilizes\nthe unlabeled data effectively, two evaluation criteria are\ncommonly adopted. First, the model needs to outperform\nits supervised baseline that learns from merely the labeled\ndata. Second, when increasing the proportion of unlabeled\nsamples in the training set, the improved margins upon the\nsupervised baseline are expected to increase accordingly.\nOverall, these improved margins indicate the effectiveness\nand robustness of an SSL method.\nAssumptions. The main assumptions for SSL include the\nsmoothness assumption [42] and manifold assumption [8],\n[42] – the latter is also known as cluster assumption [43],\nstructure assumption [44], and low-density separation as-\nsumption [45]. Speciﬁcally, the smoothness assumption con-\nsiders that the nearby data points are likely to share the\nsame class label. The manifold assumption considers data\npoints lying within the same structure (i.e., the same cluster\nor manifold) should share the same class label. In other\nwords, the former assumption is imposed locally for nearby\ndata points, while the latter is imposed globally based on\n3\nthe underlying data structure formed by clusters or graphs.\n2.2\nTaxonomy on SSL Algorithms\nExisting SSL methods generally assume that the unlabeled\ndata is closed-set and task-speciﬁc, i.e., all unlabeled train-\ning samples belong to a pre-deﬁned set of classes. The idea\nshared by most existing works is to assign each unlabeled\nsample with a class label based on a certain underlying\ndata structure, e.g., manifold structure [42], [44], and graph\nstructure [73]. We divide the most representative semi-\nsupervised deep learning methods into ﬁve categories: con-\nsistency regularization, self-training, graph-based regular-\nization, deep generative models, and self-supervised learn-\ning (Table 1), and provide their general model formulations\nin §2.2.1, §2.2.2, §2.2.3, §2.2.4 and §2.2.5.\n2.2.1\nConsistency Regularization\nConsistency regularization includes a number of successful\nand prevalent methods [26], [27], [37], [39], [46], [49], [50],\n[51], [74]. The basic rationale is to enforce consistent model\noutputs under variations in the input space and (or) model\nspace. The variations are often implemented by adding\nnoise, perturbations or forming variants of the same input\nor model. Formally, the objective in case of input variation\nis:\nmin\nθ\nX\nx∈D\nd(p(y|x; θ), ˆp(y|ˆx; θ)),\n(2)\nand in case of model variation is:\nmin\nθ\nX\nx∈D\nd(p(y|x; θ), ˆp(y|x; ˆθ)).\n(3)\nIn Eq. (2), ˆx = qx(x; ϵ) is a variant of the original input x,\nwhich is derived through a data transformation operation\nqx(·, ϵ) with ϵ being the noise added via data augmentation\nand stochastic perturbation. Similarly, in Eq. (3), ˆθ = fθ(θ; η)\nis a variant of the model θ derived via a transformation func-\ntion fθ(·; η) with η being the randomness added via stochas-\ntic perturbation on model weights and model ensembling\nstrategies. In both equations, the consistency is measured as\nthe discrepancy d(·, ·) between two network outputs p(y|·, ·)\nand ˆp(y|·, ·), typically quantiﬁed by divergence or distance\nmetrics such as Kullback-Leibler (KL) divergence [49], cross-\nentropy [51], and mean square error (MSE) [37]. See Figure 3\nfor an illustration of consistency regularization.\n2.2.1.1\nConsistency regularization under input variations\nVarious strategies aim to generate different versions of the\nsame input (ˆx in Eq. (2)) enforcing consistency (distri-\nbutional smoothness) under input variations as depicted\nin Fig. 3 (a). Techniques range from simple random aug-\nmentation [37], [46], to more advanced transformations\nsuch as adversarial perturbation [49], MixUp [26], [75],\nas well as stronger automated augmentation such as Au-\ntoAugment [76], RandAugment [77], CTAugment [27] and\nCutout [78]. Below we review these four streams of models.\nRandom augmentation is a standard data transformation\nstrategy widely adopted [37], [39], [46] via adding Gaussian\nnoise and applying simple domain-speciﬁc jittering such\nx\ndata transformation\nˆx\n✓\nmodel\np(y|x; ✓)\nˆp(y|ˆx; ✓)\ninput sample\nunsupervised loss\nd(·, ·)\np(y|x; ✓)\ninput sample\nunsupervised loss\nd(·, ·)\n✓\nˆp(y|x; ˆ✓)\nmodel\nmodel transformation\nx\n(a) consistency regularization under input variations\n(b) consistency regularization under model variations\nˆ✓= f✓(✓; ⌘)\nˆx = qx(x; ✏)\nFig. 3: In consistency regularization (§2.2.1) (a) input vari-\nations vs (b) model variations, where variations can be\ninduced by transformation on input data or model weights.\nas ﬂipping and cropping on image data. For instance, the\nΠ-model [37], [46], applies random data augmentation on\nthe same input and minimizes a consistency regulariza-\ntion term (MSE) between two network outputs. Ensemble\ntransformations [47] introduces more diverse data augmen-\ntation on input images, including spatial transformations\n(i.e., projective, afﬁne, similarity, euclidean transformations)\nto modify the spatial aspect ratio, as well as non-spatial\ntransformations to change the color, contrast, brightness,\nand sharpness. This way, the model learns representations\ninvariant to various transformations.\nAdversarial perturbation augments the input data by\nadding adversarial noise aiming to alter the model pre-\ndictions, e.g., reducing predictive conﬁdence or changing\nthe predicted correct label [79], [80]. Adversarial noise is\nintroduced for SSL to augment data and learn from the\nunlabeled data with adversarial transformations [48], [49],\n[74], [81]. Virtual Adversarial Training (VAT) [48], [49] is\nthe ﬁrst representative SSL method that perturbs input data\nadversarially. In VAT, a small adversarial perturbation is\nadded to each input and a consistency regularization term\n(i.e., KL divergence) is imposed to encourage distributional\nrobustness of the model against the virtual adversarial direc-\ntion. Notably, it has been discovered that semi-supervised\nlearning with adversarial perturbed unlabeled data does\nnot only improve model generalization, but it also enhances\nrobustness to adversarial attacks [81], [82].\nMixUp is a simple and data-agnostic augmentation strat-\negy by performing linear interpolations on two inputs and\ntheir corresponding labels [75]. It is also introduced as an\neffective regularizer for SSL [26], [50]. The Interpolation\nConsistency Training (ICT) [50] interpolates two unlabeled\nsamples and their network outputs. MixMatch [26] further\nconsiders to mix a labeled sample and unlabeled sample as\nthe input, and the groundtruth label (of labeled data) and\nthe predicted label (of unlabeled data) as the output targets.\nBoth methods impose consistency regularization to guide\nthe learning of a mapping between the interpolated input\nand interpolated output to learn from unlabeled data.\nAutomated augmentation learns augmentation strategies\nfrom data to produce strong samples, alleviating the need\n4\nTABLE 1: A taxonomy on semi-supervised deep learning methods, including ﬁve representative families in §2.2.1 – §2.2.5.\nFamilies of Models\nModel Rationale\nRepresentative Strategies and Methods\nConsistency regularization\nRandom augmentation\nΠ-model [37], [46], ensemble transformations [47]\nAdversarial perturbation\nVirtual Adversarial Training (VAT) [48], [49]\nMixUp\nMixMatch [26], ICT [50]\nAutomated augmentation\nReMixMatch [27], UDA [51], FixMatch [38]\nStochastic perturbation\nPseudo-Ensembles [52], Ladder Network [53], Virtual Adversarial Dropout [54], WCP [55]\nEnsembling\nTemporal Ensembling [37], Mean Teacher [39], SWA [41], UASD [14]\nSelf-training\nEntropy minimization\nPseudo-Label [56], MixMatch [26], ReMixMatch [27], Memory [57]\nCo-training\nDeep Co-training [58], Tri-training [59]\nDistillation\nmodel distillation (Noisy Student Training [60], UASD [14]), data distillation [35]\nGraph-based regularization Graph-based feature regularizer\nEmbedNN [44], Teacher Graph [61], Graph Convolutional Networks [62]\nGraph-based prediction regularizer Label Propagation [63]\nDeep generative models\nVariational auto-encoders\nClass-conditional VAE [64], ADGM [65]\nGenerative adversarial networks\nCatGAN [66], FM-GAN [67], ALI [68], BadGAN [69], Localized GAN [70]\nSelf-supervised learning\nSelf-supervision\nS4L [71], SimCLR [12], SimCLRv2 [72]\nto manually design domain-speciﬁc data augmentation [76],\n[77], [83], [84], [85]. It is introduced for SSL by enforcing\nthat the predicted labels of a weakly-augmented or clean\nsample and its strongly augmented versions derived from\nautomated augmentation [27], [51] are consistent. Inspired\nby the advances of AutoAugment [76], ReMixMatch [27]\nintroduces CTAugment to learn an automated augmenta-\ntion policy. Unsupervised Data Augmentation (UDA) [51]\nadopts RandAugment [77] to produce more diverse and\nstrongly augmented samples by uniformly sampling a set\nof standard transformations based on the Python Image\nLibrary. Later on, FixMatch [38] uniﬁes multiple augmen-\ntation strategies including Cutout [78], CTAugment [27],\nand RandAugment [77] and produces even more strongly\naugmented samples as input.\n2.2.1.2\nConsistency regularization under model variations\nTo impose the predictive consistency under model varia-\ntions (i.e., variations made in the model’s parameter space)\nas in Eq. (3), stochastic perturbation [52], [53], [54] and\nensembling [37], [39], [86] are proposed. Via non-identical\nmodels they produce different outputs for the same input –\na new model variant is denoted by ˆθ in Eq. (3). Below we\nreview these two streams of works as depicted in Fig. 3 (b).\nStochastic perturbation introduces slight modiﬁcations on\nthe model weights by adding Gaussian noise, dropout, or\nadversarial noise in a class-agnostic manner [52], [53], [54].\nFor example, Ladder Network injects layer-wise Gaussian\nnoises into the network and minimizes a denoising L2 loss\nbetween outputs from the original network and the noisy-\ncorrupted network [53]. Pseudo-Ensemble applies dropout\non the model’s parameters to obtain a collection of models\n(a pseudo-ensemble), while minimizing the disagreements\n(KL divergence) between the pseudo-ensemble and the\nmodel [52]. Similarly, Virtual Adversarial Dropout intro-\nduces adversarial dropout to selectively deactivates net-\nwork neurons and minimizes the discrepancy between out-\nputs from the original model and the perturbed model [54].\nWorst-Case Perturbations (WCP) introduces both addictive\nperturbations and drop connections on model parameters,\nwhere drop connections set certain model weights to zero\nto further change the network structure [55]. Notably, these\nperturbation mechanisms promote the model robustness\nagainst noise in network parameters or structure.\nEnsembling learns a set of models covering different re-\ngions of the version space [87], [88], [89]. As demonstrated\nby seminal machine learning models such as boosting [90]\nand random forest [89], a set of different models can of-\nten provide more reliable predictions than a single model.\nMoreover, ensembling offers a rich inference uncertainty to\nmitigate the overconﬁdence issue in deep neural networks\n[91]. For SSL, an ensemble model is typically derived by\ncomputing an exponential moving average (EMA) or equal\naverage in the prediction space or weight space [14], [37],\n[39], [41]. Temporal Ensembling [37] and Mean Teacher [39]\nare two representatives that ﬁrst propose to ensemble all the\nnetworks produced during training by maintaining an EMA\nin the weight space [39] or prediction space [37]. Stochastic\nWeight Averaging (SWA) [41] applies an equal average of\nthe model parameters in the weight space to provide a more\nstable target for deriving the consistency cost. Later on,\nUncertainty-Aware Self-Distillation (UASD) [14] computes\nan equal average of all the preceding model predictions\nduring training to derive soft targets as the regularizer.\nRemarks. Consistency regularization can be treated as an\nauxiliary task where the model learns from the unlabeled\ndata to minimize its predictive variance towards the vari-\nations in the input space or weight space. The predictive\nvariance is generally quantiﬁed as the discrepancy between\ntwo predictive probability distributions or network out-\nputs. By minimizing the consistency regularization loss, the\nmodel is encouraged to learn more powerful representations\ninvariant towards variations added on each sample, without\nutilizing any additional label annotation.\n2.2.2\nSelf-Training\nSelf-training methods learn from unlabeled data by imputing\nthe labels for samples predicted with high conﬁdence [23],\n[24], [92]. It is originally proposed for conventional machine\nlearning models such as logistic regression [92], bipartite\ngraph [23] and Naive Bayes classiﬁer [24]. It is re-visited\nin deep neural networks to learn from massive unlabeled\n5\np(y|x; ✓)\n✓\nmodel\nunsupervised loss\n(a) entropy minimization \nx\ninput\nsample\n✓1\np1(y|x; ✓1)\nd(·, ·)\nz1\n✓2\np2(y|x; ✓2)\nd(·, ·)\nz2\nmodel 2\nunsupervised loss 2\nunsupervised loss 1\nmodel 1\n(b) co-training\nx\ninput\nsample\nd(·, ·)\nunsupervised loss \nteacher model (pre-trained)\n✓s\n✓t\nstudent model \nps(y|x; ✓s)\nzt\n(c) distillation\nx\ninput\nsample\nFig. 4: In self-training, (a) the model prediction is enforced\nto have low entropy, (b) two models learn from each other\nand (c) the student model learns from the teacher model.\ndata along with limited labeled data. We review three repre-\nsentative lines of works in self-training, including entropy\nminimization, co-training and distillation as follows. See\nFigure 4 for an illustration of self-training.\nEntropy minimization regularizes the model training based\non the low density separation assumption\n[45], [92], to\nenforce that the class decision boundary is placed in the\nlow density regions. This is also in line with the cluster\nassumption and manifold assumption [42], [44], which hy-\npothesizes that data points from the same class are likely to\nshare the same cluster or manifold. Formally, the entropy\nminimization objective can be formulated as:\nmin\nθ\nX\nx∈D\n\u0010\n−\nX K\nj=1p(yj|x; θ) log p(yj|x; θ)\n\u0011\n,\n(4)\nwhere K refers to the number of classes. p(yj|x; θ) is the\nprobability of assigning the sample x to the class yj. This\nmeasures the class overlap. As a lower entropy indicates a\nhigher conﬁdence in model prediction, minimizing Eq. (4)\nenforces each unlabeled sample to be assigned to the class\npredicted with the highest probability. Although entropy\nminimization is originally proposed for logistic regression\nto impute the labels of samples classiﬁed with high conﬁ-\ndence [92], it is later extended to train deep neural networks\nin SSL setting by minimizing the entropy of the class assign-\nments either derived in the prediction space [26], [27], [38],\n[49], [56], [93] or the feature space [57], as detailed next.\nEntropy minimization can be imposed in the prediction\nspace, e.g., Pseudo-Label [56] directly assigns each sample\nto the class label predicted with the maximum probability,\nwhich implicitly minimizes the entropy of model predic-\ntions. When pseudo labels are one-hot vectors, they could\neasily cause error propagation due to the wrong label as-\nsignments. To alleviate this risk, MixMatch [26] uses an\nensemble of predictions over different input augmentations,\nand softly sharpens the one-hot pseudo labels with a tem-\nperature hyperparameter. Similarly, FixMatch [38] assigns\nthe one-hot labels only when the conﬁdence scores of the\nmodel predictions are higher than a certain threshold.\nEntropy minimization can also be imposed in the feature\nspace, as it is feasible to derive the class assignments based\non proximities to class-level prototypes (e.g., cluster centers)\nin the feature space [57], [94]. In [57], a Memory module\nlearns a center per class that is derived based on proximities\nto all the cluster centers. Each unlabeled sample is assigned\nto the nearest cluster center by minimizing the entropy.\nCo-training learns two or more classiﬁers on more than one\nview of the same sample coming from different sources [7],\n[23], [24], [58], [59]. Conceptually, a co-training frame-\nwork [23], [24] trains two independent classiﬁer models on\ntwo different but complementary data views and imputes\nthe predicted labels in a cross-model manner. It is later\nextended for deep visual learning [58], [59], [95], e.g., Deep\nCo-training (DCT) [58] trains a network with two or more\nclassiﬁcation layers, and passes different views (e.g., the\noriginal view and the adversarial view [96]) to individual\nclassiﬁers for co-training, while an unsupervised loss is\nimposed to minimize the similarity of predictions from dif-\nferent views. The basic idea of co-training can be extended\nfrom dual-view [58] to triple [59] or multi-view [58] – e.g.,\nin Tri-training [59], three classiﬁers are trained together,\nwith labels assigned to the unlabeled data when two of the\nclassiﬁers agree on the predictions and the conﬁdence scores\nare higher than a threshold. Formally, the deep co-training\nobjective can be written as:\nmin\nθ\nX\nx∈D\nd(p1(y|x; θ1), z2) + d(p2(y|x; θ2), z1),\n(5)\nwhere p1, p2 are predictions of two independent classiﬁers\nθ1, θ2 trained on different data views. d(·, ·) introduces the\nsimilarity metric to learn from the imputed targets z1, z2\nfrom each other, e.g., cross-entropy on one-hot targets [59],\nor Jensen-Shannon divergence between output targets [58].\nDistillation is originally proposed to transfer the knowl-\nedge learned by a teacher model to a student model, where\nthe soft targets from the teacher model (e.g., an ensemble of\nnetworks or a larger network) can serve as an effective reg-\nularizer or a model compression strategy to train a student\nmodel [97], [98], [99]. Recent works in SSL use distillation to\nimpute learning targets on the unlabeled data for training\nthe student network [14], [35], [60], [100]. Formally, an un-\nsupervised distillation objective is introduced on a student\nmodel θs to learn from the unlabeled data as:\nmin\nθ\nX\nx∈D\nd(ps(y|x; θs), zt),\n(6)\nwhere the student prediction ps is enforced to align with\nthe targets zt produced by a teacher model θt on either\nthe unlabeled data or all the data. Compared to co-training\n(Eq. (5)), distillation in SSL (Eq. (6)) does not optimize\nmultiple networks simultaneously, but instead trains more\nthan one network in different stages. In distillation, the ex-\nisting works can be further grouped into model distillation\n6\n✓\nmodel\nbuild NN graph\nunsupervised loss \ninput\nsample\nLunsup\nx\nFig. 5: In graph-based regularization (§2.2.3) pseudo labels\nare propagated over the Nearest Neighbor graph based on\nneighbourhood consistency and an unsupervised regular-\nization term is imposed on the feature or prediction space.\nand data distillation, which generate learning targets for\nunlabeled data using the teacher model output or multiple\nforward passes of the same input data, as detailed next.\nIn model distillation, labels from a teacher are assigned\nto a student [14], [60], [100]. The teacher model can be\nformed, e.g., via a pre-trained model or an ensemble of mod-\nels. In Noisy Student Training [60], an iterative self-training\nprocess iterates the teacher-student training by ﬁrst training\na teacher to impute labels on unlabeled data for the student,\nand reuses the student as the teacher in the next iteration. In\nUncertainty-Aware Self-Distillation (USAD) [14], the teacher\naverages all the preceding network predictions to impute\nlabels on unlabeled data for updating the student network\nitself. In model distillation, both soft targets and one-hot\nlabels from the teacher model can serve as the learning\ntargets on the unlabeled data [14], [60].\nIn data distillation, the teacher model predicts learning\ntargets on unlabeled data by ensembling the outputs of\nthe same input under different data transformations [35].\nSpeciﬁcally, the ensembled teacher predictions (i.e., soft\ntargets) are derived by averaging the outputs of the same\ninputs under multiple data transformations; while the stu-\ndent model is then trained with the soft targets. Data\ndistillation transforms the input data multiple times rather\nthan training multiple networks to impute the ensembled\npredictions on unlabeled data. This is similar to consistency\nregularization with random data augmentation; however,\nin data distillation, two training stages are involved – the\nﬁrst stage involves pre-training the teacher model; while\nthe second stage involves training the student network to\nmimic the teacher model by distillation.\nRemarks. Similar to consistency regularization, self-training\ncan be considered as an unsupervised auxiliary task learned\nalong with the supervised learning task. In general, it\nalso enforces the predictive invariance towards instance-\nwise variations or the teacher’s predictions. However, self-\ntraining differs in design. While consistency regularization\ngenerally trains one model, self-training may require more\nthan one model to be trained, e.g., co-training requires\nat least two models trained in parallel while distillation\nrequires to train a teacher and a student model sequentially.\n2.2.3\nGraph-based Regularization\nGraph-based regularization is a family of transductive learning\nmethods originally proposed for non-deep semi-supervised\nlearning algorithms [42], [73], [101], [102], [103], such as\ntransductive Support Vector Machine [42], [102] and Gaus-\nsian random ﬁeld model [101]. Most algorithms from this\nfamily build a weighted graph to exploit relationships\namong the data samples. Speciﬁcally, both labeled and\nunlabeled samples are represented as nodes, while the edge\nweights encode the similarities between different samples.\nThe labels can be propagated over the graph based on the\nsmoothness assumption [42], i.e., neighboring data points\nshould share the same class label as shown in Figure 5.\nA graph-based regularization term is used in model\noptimization by imposing various forms of smoothness\nconstraints to minimize the pairwise similarities between\nnearby data points. Graph-based regularization is later re-\nformulated for semi-supervised learning with deep neural\nnetworks, such as EmbedNN [44], Graph Convolutional\nNetwork [62], [104], Teacher Graph [61], and Label Prop-\nagation [63]. Although this line of works share the same\nsmoothness assumption for model optimization, graph-\nbased regularization can be imposed differently in either\nthe feature space or prediction space, detailed as follows.\nGraph-based feature regularization is typically done by\nbuilding a learnable nearest neighbor (NN) graph that aug-\nments the original DNN to encode the afﬁnity between\ndata points in the feature space, as represented by Embed-\nCNN [44] and Teacher Graph [61]. Each node in the graph\nis encoded by the visual feature extracted from the interme-\ndiate network layer or the output from the last layer; while\nan afﬁnity matrix Wij is computed to encode the pairwise\nsimilarities between all the nodes. To exploit unlabeled data,\na graph-based regularization term can be formed as a metric\nlearning loss, such as the margin-based contrastive loss\nfor Siamese networks [105], [106] which constrains feature\nlearning by enforcing the local smoothness:\nmin\nθ\nX\nxi,xj∈D\n(\n∥h(xi) −h(xj)∥2,\nif Wij=1\nmax(0, m −||h(xi) −h(xj)||)2,\nif Wij=0\n(7)\nensuring that features h(xi), h(xj) of nearest neighbors (i.e.,\nWij=1) are close to and dissimilar pairs (i.e., Wij=0) are\naway from each other with a distance margin m.\nBeyond augmenting a DNN with a graph, a more ﬂexible\nway is to use graph convolutions, i.e., Graph Convolutional\nNetworks (GCN) [62], which derive new feature represen-\ntations for each node subject to the graph structure [104],\n[107]. Speciﬁcally, a GCN takes the data and afﬁnity matrix\nas input, and learns to estimate the class labels of unlabeled\ndata under a supervised cross-entropy loss on labeled data.\nGraph-based prediction regularization operates in the pre-\ndiction space [63], [108], as in Label Propagation [63]. Driven\nby the same rationale of building a learnable NN-graph\nas above, in label propagation, an NN-graph encidong the\nsimilarity between data points is used to propagate the\nlabels from the labeled data to the unlabeled data based\non transitivity via with a cross-entropy loss. While being\nsimilar to the approach Pseudo-Labels [56], the propagated\nlabels are derived with an external NN-graph that encodes\nthe global manifold structure. Further, label propagation on\nthe graph and the update of DNN are performed alterna-\ntively to propagate more reliable labels.\nRemarks. Graph-based regularization shares several simi-\nlarities with consistency regularization and self-training in\nSSL. First, it introduces an unsupervised auxiliary task to\n7\nlabeled sample\n✓D\ndiscriminator\nunlabeled sample\nxG\nLsup\ngenerator\n✓G\nunsupervised loss\n(minimax loss)\nsupervised loss \nxl\nnoise z\np(y|xG; ✓D)\np(y|xl; ✓D)\n(real)\n(fake)\nreal\nfake\nFig. 6: In GAN-based deep generative models (§2.2.4), the\ndiscriminator assigns the labeled samples to the K classes\nand the generated unlabeled data to an auxiliary class (K +\n1). At test time, the discriminator acts as the classiﬁer.\ntrain a DNN with propagated learning targets (e.g., pseudo\nlabels) on the unlabeled data. Second, its learning objective\ncan be formulated as a cross-entropy loss or metric learning\nloss. Notably, while consistency regularization and self-\ntraining are inductive approaches that estimate a learning\ntarget per instance, graph-based regularization methods\nare transductive approaches that propagate learning tar-\ngets based on a graph constructed on the dataset. Beyond\nconcrete details, however, the three techniques all share the\nsame fundamental idea of seeking for unsupervised targets.\n2.2.4\nDeep Generative Models\nDeep generative models are a class of unsupervised learning\nmodels that learn to approximate the data distributions\nwithout labels [109], [110]. By integrating the generative\nunsupervised learning concept into a supervised model, a\nsemi-supervised learning framework can be formulated to\nunify the merits of supervised and unsupervised learning.\nTwo main streams of deep generative models are Varia-\ntional Auto-Encoders (VAEs) and Generative Adversarial\nNetworks (GANs), as detailed below. See Figure 6 for an\nillustration of a GAN framework for SSL.\nVariational auto-encoders (VAEs) are probabilistic models\nbased on variational inference for unsupervised learning of\na complex data distribution [109], [111]. A standard VAE\nmodel contains a network that encodes an input sample\nto a latent variable and a network that decodes the la-\ntent variable to reconstruct the input; maximizing a varia-\ntional lower bound. In semi-supervised learning [64], [65],\n[112], an unsupervised VAE model is generally combined\nwith a supervised classiﬁer. For instance, to predict task-\nspeciﬁc class information required in SSL, Class-conditional\nVAE [64] and ADGM [65] introduce the class label as an\nextra latent variable in the latent feature space to explicitly\ndisentangle the class information (content) and the stochas-\ntic information (style), and impose an explicit classiﬁcation\nloss on the labeled data along with the vanilla VAE loss.\nGenerative adversarial networks (GANs) [110] learn to\ncapture the data distribution by an adversarial minimax\ngame. Speciﬁcally, a generator is trained to generate as\nrealistic images as possible while a discriminator is trained\nto discriminate between real and generated samples. When\nre-formulated as a semi-supervised representation learner,\nGANs can leverage the beneﬁts of both unsupervised gener-\native modeling and supervised discriminative learning [66],\n[67], [68], [69], [113], [114], [115], [116], [117], [118].\nThe generic idea is to augment the standard GAN\nframework with supervised learning on the labeled real\nsamples (i.e., discriminative) and unsupervised learning on\nthe generated samples. Formally, this enhances the original\ndiscriminator with an extra supervised learning capability.\nFor example, Categorical GAN (CatGAN) [66] introduces\na K-class discriminator, and minimizes a supervised cross-\nentropy loss on the real labeled samples, while imposing\na uniform distribution constraint on the generated samples\nby maximizing the prediction’s entropy. Similarly, feature\nmatching GAN (FM-GAN) [67], ALI [68], BadGAN [69] and\nLocalized GAN [70] formulate a (K+1)-class discriminator\nfor SSL, whereby a real labeled sample xl is considered as\none of the K classes and a generated sample xG as the\n(K + 1)th class. The supervised and unsupervised learning\nobjective for the (K+1)-class discriminator is formulated as;\nmax\nθ\nX\nx∈D\nlog p(y|xl, y<K+1),\n(8)\nmax\nθ\nX\nx∈D\nlog (1−p(y=K+1|xl)) −log p(y=K+1|xG), (9)\nwhere Eq. (8) is the supervised classiﬁcation loss on the\nlabeled samples xl; Eq. (9) is an unsupervised GAN loss\nthat discriminates between the real labeled samples xl and\nthe generated fake samples xG from the image generator.\nTo constrain the generated samples, Localized GAN [70]\nintroduces a regularizer on the generator to ensure the\ngenerated samples lie in the neighborhood of an original\nsample on the manifold, thus allowing to train a locally\nconsistent classiﬁer based on the generated samples in a\nsemi-supervised fashion.\nRemarks. Unlike previously discussed discriminative SSL\ntechniques DGMs can naturally learn from unlabeled data\nwithout the need to estimate their labels. In other words,\nDGMs are native unsupervised representation learners. To\nenable SSL in DGMs, the key in model reformulation is\nthus to integrate the label supervision into training, e.g.,\nadding a class label latent variable in VAEs or an extra class\ndiscriminator in GANs. Further, one also needs to tackle\nmore difﬁcult model optimization in a GAN framework.\n2.2.5\nSelf-Supervised Learning\nSelf-supervised learning is a class of unsupervised represen-\ntation learners designed based on unsupervised surrogate\n(pretext) tasks [11], [119], [120], [121], [122], [123]. Self-\nsupervision differs from self-training algorithms in §2.2.2,\nas self-supervised learning objectives are task-agnostic and\ncould be trained without any label supervision. The former\nis originally proposed to learn from only unlabeled data\nwith task-agnostic unsupervised learning objectives, but\nit is also explored for SSL [12], [71], [72]. In SSL, task-\nagnostic self-supervision signals on all training data are\noften integrated with a supervised learning objective on\nlabeled data. For instance, S4L [71] uses self-supervision\nfor SSL based on multiple self-supervision signals such as\npredicting rotation degree [123] and enforcing invariance\nto exemplar transformation [119] to train the model along\nwith supervised learning. SimCLR [12] and SimCLRv2 [72]\nare follow-up works introducing self-supervised contrastive\nlearning for task-agnostic unsupervised pre-training, fol-\nlowed by supervised or semi-supervised ﬁne-tuning with\nlabel supervision as the downstream task.\n8\nRemarks. A unique advantage of self-supervision for SSL\nis that task-speciﬁc label supervision is not required dur-\ning training. While the aforementioned semi-supervised\nlearners typically solve a supervised task and an auxiliary\nunsupervised task jointly, self-supervised semi-supervised\nlearners can be trained in a fully task-agnostic fashion.\nThis suggests the great ﬂexibility of self-supervision for\nSSL. Thus, the self-supervised training can be introduced as\nunsupervised pre-training or as an auxiliary unsupervised\ntask solved along with supervised learning. Although self-\nsupervision is relatively new for SSL, it has been more\nwidely explored for unsupervised learning, which is de-\ntailed more extensively in §3.2.1 and §3.2.2.\n3\nUNSUPERVISED LEARNING (UL)\nUnsupervised Learning (UL) aims to learn representations\nwithout utilizing any label supervision. The learned repre-\nsentation is not only expected to capture the underlying se-\nmantic information, but also be transferable to tackle unseen\ndownstream tasks such as visual recognition, detection, and\nsegmentation [16], visual retrieval [150], and tracking [151].\nUL is attractive in computer vision for multiple reasons.\nFirst, due to costly label annotations, large labeled datasets\nmay not be available in many application scenarios, e.g.,\nmedical imaging [152]. Second, as there are often data/label\ndistribution drifts (or gaps) across tasks and application\nscenarios, pre-training on a large labeled dataset cannot\nalways guarantee good model initialization for unseen sit-\nuations [153]. Third, UL could supply strong pre-trained\nmodels that may perform on par with or even outperform\nsupervised pre-training [12], [16], [154].\nRemarks. UL and SSL share the same aim to learn from\nunlabeled data, and leverage similar modeling principles\nto formulate unsupervised surrogate supervision signals\nwithout any label annotation. However, instead of assuming\nthe availability of task-speciﬁc information (i.e., class labels)\nas in SSL, UL considers model learning from purely task-\nagnostic unlabeled data. Given that unlabeled data are\nabundantly available in different scenarios (e.g., Internet),\nUL offers an appealing strategy to provide good pre-trained\nmodels that could facilitate various downstream tasks.\nFocusing on unsupervised visual learners trained on im-\nage classiﬁcation datasets, we deﬁne the UL problem setup\nin §3.1, and provide a taxonomy and analysis of the existing\nrepresentative unsupervised deep learning methods in §3.2.\n3.1\nThe Problem Setting of UL\nProblem Deﬁnition. In UL, we have access to an unlabeled\ndataset Du = {xi}Nu\ni=1. As label information is unknown, the\nUL loss function L for training a DNN θ can generally be\nexpressed as Eq. (1), i.e., L = λlLsup +λuLunsup with λl = 0.\nIn discriminative models, the unsupervised objective Lunsup\nrequires certain pseudo/proxy targets to learn semantically\nmeaningful and generalizable representations. In generative\nmodels, Lunsup is imposed to explicitly model the data\ndistribution. See Figure 7 for an illustration of UL.\nEvaluation Protocol. The performance of UL methods are\noften evaluated via two protocols, commonly known as\nthe (1) linear classiﬁcation protocol, and (2) ﬁne-tuning on\nunsupervised loss \n✓\nmodel\nxu\nunlabeled sample\nLunsup\nunsupervised pre-training (main task)\n✓\nlabeled sample\nxl\nsupervised fine-tuning (downstream task)\nLsup\nsupervised loss \nmodel\nFig. 7: Unsupervised learning trains a generalizable model\nusing purely unlabeled data. The model can later be ﬁne-\ntuned with labeled data and tested on a downstream task.\ndownstream tasks. In (1), the pre-trained DNN is frozen\nto extract the features for an image dataset, while a linear\nclassiﬁer (e.g., a fully-connected layer or a kNN classiﬁer)\nis trained to classify the extracted features. In (2), the pre-\ntrained DNN is used to initialize a model for any down-\nstream task, followed by ﬁne-tuning with a task-speciﬁc\nobjective, such as ﬁne-tuning an object detector initialized\nfrom an unsupervised pre-trained backbone (e.g., FasterR-\nCNN [155]) on object detection datasets (e.g., PASCAL\nVOC [156]), or ﬁne-tuning a segmentation model (e.g., Mask\nR-CNN [157]) with a pre-trained backbone on segmentation\ndatasets (e.g., COCO [158]).\n3.2\nTaxonomy on UL Algorithms\nExisting unsupervised deep learning models can be mainly\ngrouped into three families: pretext tasks, discriminative\nmodels and generative models (Table 2). Pretext tasks and\ndiscriminative models are also known as self-supervised\nlearning, which drive model learning by a proxy proto-\ncol/task and construct pseudo label supervision to formu-\nlate unsupervised surrogate losses. Generative models is\ninherently unsupervised and explicitly models the data dis-\ntribution to learn representations without label supervision.\nWe review these models in §3.2.1, §3.2.2 and §3.2.3.\n3.2.1\nPretext Tasks\nPretext Tasks refer to hand-crafted proxy tasks manually\ndesigned to predict certain task-agnostic properties of the\ninput data, which do not require any label supervision for\ntraining. By formulating self-supervised learning objectives\nwith free labels, meaningful visual representations can be\nlearned in a fully unsupervised manner. In the following, we\nreview two classes of pretext tasks, which introduce the self-\nsupervision signals at the pixel-level (illustrated in Figure 8)\nor instance-level (illustrated in Figure 9).\nPixel-level pretext task is generally designed as a dense\nprediction task that aims to predict the expected pixel values\nof an output image as a self-supervision signal [124], [125],\n[126], [127], [128], [129], [130], [131]. Auto-Encoder [124],\n[125] is one of the most representative and primitive unsu-\npervised models that learn representations by reconstruct-\ning input images. In addition to standard reconstruction,\npixel-level pretext tasks introduce more advanced image\ngeneration tasks to hallucinate the pixel colour values of the\n9\nTABLE 2: A taxonomy of unsupervised deep learning methods, including three representative families in §3.2.1 – §3.2.3.\nFamilies of Models\nModel Rationale\nRepresentative Strategies and Methods\nPretext tasks\nPixel-level\nreconstruction [124], [125], inpainting [126], MAE [127], denoising [128], colorization [129], [130], [131]\nInstance-level\npredict image rotations [123], scaling and tiling [122], patch ordering [11], patch re-ordering [121]\nDiscriminative models\nInstance discrimination\nnegative sampling\nlarge batch size (SimLR [12]), memory bank (InstDis [132]), queue (MoCo [16])\ninput transformation data augmentation (PIRL [133]), multi-view augmentation (CMC [134])\nnegative-sample-free simple siamese (SimSiam [135]), Bootstrap (BYOL [136]), DirectPred [137]\nDeep clustering\nofﬂine clustering\nDeepCluster [138], JULE [139], SeLa [140]\nonline clustering\nIIC [141], PICA [142], AssociativeCluster [143], SwAV [144]\nDeep generative models Discriminator-level\nDCGAN [145], Self-supervised GAN [146], Transformation GAN [147]\nGenerator-level\nBiGAN [148], BigBiGAN [149]\n✓\nd(·, ·)\nmodel\nunsupervised loss \n(a) inpainting\n✓\nd(·, ·)\nmodel\nunsupervised loss \n(c) colorization\n✓\nd(·, ·)\nmodel\nunsupervised loss \n(b) denoising\nx\nx\nx\nˆx\nˆx\nˆx\nFig. 8: In pixel-level pretext tasks (§3.2.1), the aim is to\nreconstruct the original image ˆx from a corrupted input x.\ncorrupted input images, as represented by three standard\nlow-level image processing tasks: (1) image inpainting [126],\n[127] learns by inpainting the masked-out missing regions\nin the input images, which is also known as masked auto-\nencoders (MAE) [127]; (2) denoising [128] learns to denoise\nthe partial destructed input; and (3) colorization [129], [130],\n[131] aims to predict the colour values of the grayscale\nimages. These self-supervised models are trained with an\nimage generation task objective (e.g., a mean square error)\nto enforce predicting the expected pixel values:\nmin\nθ\nX\nx∈D\n||Gθ(x) −ˆx||2,\n(10)\nwhere Gθ(·) is an image generation network (typically\nimplemented as an encoder-decoder network architecture)\ntrained to predict the expected output image ˆx per pixel.\nOnce trained, part of the network Gθ(·) (e.g., encoder)\ncan be used to initialize the model weights or extract the\nintermediate features for solving the downstream task.\nInstance-level pretext tasks introduce sparse semantic la-\nbels for each image sample by designing a surrogate proxy\ntask that can be solved per instance without any label\nannotations [11], [121], [122], [123], [159], [160], [161], [162].\nIn general, these pretext tasks involve applying different\nimage transformations to generate diverse input variations,\nwhereby an artiﬁcial supervision signal is imposed to pre-\n90◦\n0◦\n180◦\n270◦\n✓\nmodel\ncolor randomization\nrandom flipping\ncropping \nrotation\ninput transformation\nunlabeled\nsample xu\n{0◦, 90◦, 180◦, 270◦}\npredict rotation\npredict transformation\nunsupervised loss \n(a) predict rotation & input transformation\n✓\nmodel\nunlabeled\nsample xu\ninput patches\n(b) predict patch ordering\n1\n2\n3\n4\n5\n6\n7\n8\nunsupervised loss \npatch\norder\nFig. 9: In instance-level pretext tasks (§3.2.1) the aim is to\npredict the transformation on the input.\ndict the applied transformation on each image instance.\nAmong this line of works, the representative ones consider\nmainly two classes of instance-wise transformations on in-\nput images. The ﬁrst one is classifying global transforma-\ntions, such as rotations [123], scaling and tiling [122], where\nthe learning objective is to recognize the geometric transfor-\nmation applied on an image. The second one is predicting\nlocal transformations, such as patch orderings [11] and\npatch re-orderings [121], [159], [161], which cut each image\ninto multiple local patches. The goal of patch orderings is to\nrecognize the order of a given cut-out patch, while patch\nre-orderings, also known as the jigsaw puzzles, permute\nthe cut-out patches randomly and the goal is to predict the\npermuted conﬁgurations. In summary, the objective of an\ninstance-level pretext task can be written as:\nmin\nθ\nX\nx∈D\nLunsup(Φz(x), z, θ),\n(11)\nwhere Lunsup(·) can be various loss functions (e.g., cross-\nentropy loss [123]) that learn a mapping from a transformed\ninput image Φz(x) to a discrete category or a conﬁguration\n10\n✓\nmodel\nxu\nunlabeled sample\nnegative pair\npositive pair\npositive pair\nnegative pair\nf✓(xu)\nfeature\nunsupervised loss: contrastive loss (infoNCE)\n−log(\nexp(sim(\n))\n,\n+\nexp(sim(\n))\n,\n,\nexp(sim(\n))\n,\nexp(sim(\n)) + ⋯+\n)\nFig. 10: The unsupervised discriminative model using con-\ntrastive learning (§3.2.2) aims to pull together the positive\npairs and push away the negative ones.\nof the applied transformation z. Once trained, the represen-\ntations are covariant with the transformations Φz(·), thus\nbeing aware of the spatial context information, e.g., how an\nimage is rotated or how the local patches are permuted.\nRemarks. Although self-supervised learning objectives of\npixel-level or instance-level pretext tasks are generally not\nexplicitly related to the downstream task objectives (e.g.,\nimage classiﬁcation, detection and segmentation), they per-\nmit to learn from unlabeled data by predicting the spatial\ncontext or structured correlation in images, such as inpaint-\ning missing regions, and predicting the applied rotations.\nAs these self-supervision signals can implicitly uncover the\nsemantic content (e.g. human interpretable concepts [163])\nor spatial context in images, they often yield a meaningful\npre-trained model for initialization in unseen downstream\ntasks, or even serve as a ﬂexible and effective regularizer\nto facilitate other machine learning setups, such as semi-\nsupervised learning [71] and domain generalization [164].\n3.2.2\nDiscriminative Models\nDiscriminative models hereby refer to the class of unsu-\npervised discriminative models that learn visual represen-\ntations from the unlabeled data by enforcing invariance\ntowards various task-irrelevant visual variations at either\ninstance-level, neighbor-level or group-level. These visual\nvariations can be intra-instance variations such as different\nviews of the same instance [134], [165], [166], [167], [168], or\ninter-instance variations between neighbor instances [169],\n[170] or across a group of instances [138], [144], [171].\nIn the following, we review two representative classes\nof unsupervised discriminative models that offer the state\nof the art in unsupervised visual feature learning, including\ninstance discrimination (see Figure 10) and deep clustering\n(see Figure 11). The former imposes self-supervision per\ninstance by treating each instance as a class, while the latter\nintroduces supervision per group by considering a group of\nsimilar instances as a class.\nInstance discrimination models learn discriminative repre-\nsentations by enforcing invariance towards different view-\ning conditions, data augmentations or various parts of the\nsame image instance [12], [16], [72], [119], [120], [132], [133],\n[134], [165], [166], [167], [168], [172], [173], [174] – also\nknown as exemplar learning [119], [120].\nThe most prevalent scheme in instance discrimination\nis contrastive learning, which was initially proposed to\nlearn invariant representations by mapping similar inputs\nto nearby points in the latent space [105], [106]. The state-\nof-the-art contrastive learning models for self-supervised\nlearning generally aim to obtain an invariance property by\noptimizing a contrastive loss formulated upon the noise\ncontrastive estimation (NCE) principle [175], which maxi-\nmizes the mutual information across different views. The\nmulti-view information bottleneck model [176] extends the\noriginal information bottleneck principle to unsupervised\nlearning and trains an encoder to retain all the relevant in-\nformation for predicting the label while minimizing the ex-\ncess information in the representation. Formally, contrastive\nlearners such as SimLR [12] and MoCo [16] are generally\noptimized by an instance-wise contrastive loss (i.e., infoNCE\nloss) [106], [177]:\nmin\nθ\nX\nxi∈D\n−log\nexp(fθ(xi) · fθ(x+\ni )/τ)\nPM\nj=1 exp(fθ(xi) · fθ(xj)/τ)\n,\n(12)\nwhere τ is a temperature, fθ is the feature encoder, i.e., a\nDNN; fθ(xi), fθ(x+\ni ) are the feature embeddings of two dif-\nferent augmentations, or views of the same image; {xj}M\nj=1\nincludes (M−1) negative samples and 1 positive (i.e., x+\ni )\nsample. Eq. (12) optimizes the network by enforcing the\npositive pairs (i.e., embeddings of the same instance) to lie\ncloser, while pushing apart the negative pairs (i.e., embed-\ndings of different instances). Minimizing the InfoNCE loss\nis equivalent to maximizing a lower bound on the mutual\ninformation between fθ(xi) and fθ(x+\ni ) [165].\nTo derive a tractable yet meaningful contrastive distribu-\ntion in Eq. (12), a large amount of negative pairs are often\nrequired per training batch. To this aim, existing state-of-the-\nart methods are typically featured with different negative\nsampling strategies to collect more negative pairs. For in-\nstance, a large batch size of 4096 is adopted in SimCLR [12].\nIn InstDis [132], MoCo [16], PIRL [133], and CMC [134], a\nmemory bank is used to maintain all the instance prototypes\nby keeping moving average of their feature representations\nover training iterations. Finally, running queue enqueues the\nfeatures of samples in the latest batches and dequeues the\nold mini-batches of samples to store a fraction of sample’s\nfeatures from the preceding mini-batches [16], [133], [174].\nInspired by deep metric learning, various training strate-\ngies further boost contrastive learning. For instance, a hard\nnegative sampling strategy [178] mines the negative pairs\nthat are similar to the samples but likely belong to different\nclasses. To train negative pairs and (or) positive pairs by\nadversarial training [179], [180] learn a set of “adversarial\nnegatives” confused with the given samples, or “coopera-\ntive positives” similar to the given samples. These strategies\nare designed to ﬁnd the better negative and positive pairs\nfor improving contrastive learning.\nIn addition to negative sampling, it is essential to apply\nvarious image transformations for generating multiple di-\nverse variants (i.e., views) of the same instance to construct\nthe positive pairs. The most typical way is to apply common\ndata augmentation such as random cropping and color jit-\ntering [12], [16], [132], [133], [166], [173], or pretext transfor-\nmation [133] like patch re-ordering [121] and rotation [123].\n11\n✓\nmodel\nxu\nunlabeled sample\nclustering\nLunsup\nˆy\ncluster membership\n(pseudo label)\nf✓(xu)\nfeature\np(y|xu; ✓)\nunsupervised loss \nFig. 11: In unsupervised discriminative models using deep\nclustering (§3.2.2), unlabeled samples are assigned to a set\nof clusters by online or ofﬂine clustering, while the cluster\nmemberships are utilized as pseudo labels for training.\nAn alternative way is to artiﬁcially construct multiple views\nof a single image by using different image channels like\nluminance and chrominance [134], or by extracting the local\nand global patches of the same image [165]. In a nutshell,\nalthough there are different strategies in negative sampling\nand image transformations to construct the negative and\npositive pairs for contrastive learning, these strategies share\nthe same aim to learn visual representations invariant to\ndiverse input transformations [133], [172].\nWhile contrastive learning approaches rely on obtaining\na sufﬁcient amount of negative pairs to derive the con-\ntrastive loss (Eq. (12)), another alternative non-contrastive\nscheme for instance discrimintation operates in a negative-\nsample-free manner [135], [136], [137], [181], as exempliﬁed\nby bootstrap (BYOL) [136] and simple siamese networks\n(SimSiam) [135]). In particular, in BYOL and SimSiam, two\nviews (obtained from data augmentation) of the same im-\nages are passed towards the networks and the mean squared\nerror is minimized between the representations of two views\nto enforce invariances. Importantly, a stop gradient scheme\nis adopted to prevent representational collapse, i.e. avoid\nmapping all the samples to the same representations. An-\nother related method is Barlow Twins [181], which computes\na cross-correlation matrix between the distorted versions of\na batch of training samples and enforce the matrix to be\nan identity matrix, thus learning self-supervised representa-\ntions invariant to different distortions. Although these non-\ncontrastive methods adopt other loss formulations, they\nall share the similar spirit as contrastive learning given\nthat meaningful representations are learned by enforcing\ninvariances to different views of the same instance.\nDeep clustering models learn discriminative representa-\ntions by grouping similar instances from the same cluster\ntogether [138], [139], [140], [142], [144], [170], [171], [182],\n[183], [184], [185], [186], [187]. In training, the entire dataset\nis generally divided into groups by associating each instance\nto a certain cluster centroid based on pairwise similarities.\nAlthough clustering algorithms are longstanding machine\nlearning techniques [188], [189], [190], they have been re-\ndesigned to be seamlessly integrated with DNNs to learn\ndiscriminative representations without label supervision.\nConceptually, the cluster memberships can be considered\nas some pseudo labels to supervise the model training, as\nwritten in Eq. (13).\nmin\nθ\nX\nx∈D\nLunsup(x, ˆy, θ),\n(13)\nwhere ˆy is the cluster membership of sample x, Lunsup(·, ·, θ)\nis the loss function that constrains the mapping from x to y,\nsuch as a classiﬁcation loss. Deep clustering algorithms can\nbe further grouped into two categories according to whether\nthe assignments of cluster memberships are derived in an\nofﬂine or online manner, as detailed in the following.\nIn ofﬂine clustering, unsupervised training is alternated\nbetween a cluster assignment step and a network train-\ning step [139], [140], [170], [171], [182], [191], [192], [193].\nWhile the former step estimates the cluster memberships\nof all the training samples, the latter uses the assigned\ncluster memberships as pseudo labels to train the network.\nRepresentative ofﬂine clustering models include DeepClus-\nter [138], JULE [139] and SeLa [140], which mainly differ\nin the clustering algorithms. Speciﬁcally, DeepCluster [138],\n[171] groups visual features using k-means clustering [189].\nJULE [139] uses agglomerative clustering [194] that merges\nsimilar clusters to iteratively derive new cluster member-\nships. SeLa [140] casts clustering as an optimal transport\nproblem solved by Sinkhorn-Knopp algorithm [195] to ob-\ntain the cluster memberships as pseudo labels.\nIn online clustering, the cluster assignment step and\nnetwork training step are coupled in an end-to-end training\nframework, as represented by IIC [141], AssociativeClus-\nter [143], PICA [142], and SwAV [144]. Compared to ofﬂine\nclustering, online clustering could better scale to large-\nscale datasets, as it does not require clustering the entire\ndataset iteratively. This is typically achieved in two ways:\n(1) training a classiﬁer that parameterizes the cluster mem-\nberships (e.g., IIC and PICA); (2) learning a set of cluster\ncentroids/prototypes (e.g., AssociativeCluster and SwAV).\nFor instance, IIC [141] learns the cluster memberships by\nmaximizing the mutual information between predictions\nof an original instance and a randomly perturbed instance\nobtained from data augmentation. SwAV [144] learns a set\nof prototypes (i.e., cluster centroids) in the feature space and\nassigns each sample to the closest prototype.\nRemarks. Recent advances of discriminative unsupervised\nmodels include both contrastive learning and deep cluster-\ning, which have set the new state of the art. On one side,\ncontrastive learning discriminates individual instances by\nimposing transformation invariance at the instance-level.\nInterestingly, this opposes some instance-level pretext tasks\nthat instead learn by predicting the applied transformations.\nContrastive learning also closely relates to consistency reg-\nularization in SSL in the sense of enforcing invariance to\ntransformations, although different loss functions are often\nused. However, as shown in [135], a pairwise loss objective\n– often used for consistency regularization in SSL – can be\nalso effective as contrastive loss (Eq. (12)). This suggests that\nthe essential idea behind them is identical – imposing trans-\nformation invariance at instance level. Deep clustering, on\nthe other hand, discriminates between groups of instances\nfor discovering the underlying semantic boundaries, and\nenforces group-level invariance. The idea of consistency\nregularization is also adopted by several deep clustering\nmethods [141], [142], conforming its more generic efﬁcacy\n12\n✓D\ndiscriminator\ngenerated sample\nxG\n✓G\nxu\nunlabeled sample\nreal\nfake\ngenerator\nunsupervised loss\n(minimax loss)\nnoise z\n(real)\n(fake)\nFig. 12: In GANs (§3.2.3), a generator and a discriminator are\ntrained with a minimax game (Eq. (14)) in an unsupervised\nmanner, whilst their intermediate features lead to discrimi-\nnative visual representations.\nbeyond SSL. Lastly, discriminative unsupervised learning\ncan also be conducted at both instance-level and group-level\nto learn more powerful representations [186], [196].\n3.2.3\nDeep Generative Models\nDeep generative models (DGMs), as introduced in §2.2.4, are\ninherent unsupervised learners that explicitly model the\ndata distribution [109], [110], [197], [198]. DGMs are applica-\nble for both semi-supervised and unsupervised learning. A\ntypical Generative Adversarial Network (GAN) [66], [146],\n[147], [149], [199] contains a discriminator D to differentiate\nreal and fake samples, and a generator G that can serve as\nan image encoder to capture the semantics in latent space,\nas trained by a min-max game:\nmin\nG\nmax\nD\nEx∼pdata(x)[logD(x)]+Ez∼pz(z)[log(1−D(G(z)))],\n(14)\nwhere z is sampled from an input noise distribution pz(z).\nGANs can learn representations at both the discriminator\nand the generator level. See Figure 12 for an illustration of\ndeep generative model based on a GAN.\nTo learn representations at the discriminator-level, Deep\nConvolutional Generative Adversarial Network (DCGAN)\n[145] adopts a pre-trained convolutional discriminator to\nextract features for tackling a downstream image classiﬁ-\ncation task. Later on, Self-supervised GAN [146] and Trans-\nformation GAN [147] further imbue the discriminator with\na self-supervised pretext task to predict the applied image\ntransformation, thus enabling the representations to capture\nthe latent visual structures.\nTo learn representations at the generator-level, Bidi-\nrectional Generative Adversarial Networks (BiGAN) [199]\nintroduces an image encoder coupled with the generator,\nwhich is trained with a joint discriminator loss to tie the data\ndistribution and the latent feature distribution together. This\nallows the image encoder to capture the semantic variations\nin its latent representation, and offer discriminative visual\nrepresentations for one nearest neighbor (1NN) classiﬁca-\ntion. To further improve BiGAN, BigBiGAN [149] adopts\nmore powerful discriminator and generator architectures\nthan BigGAN [148], together with an additional unary dis-\ncriminator loss to constrain the data or latent distribution\nindependently, therefore enabling more expressive unsuper-\nvised representation learning at the generator-level.\nRemarks. Although most state-of-the-art UL methods are\nself-supervised models that solve pretext tasks or perform\nunsupervised discriminative learning (as reviewed in §3.2.1\nand §3.2.2), deep generative models are still an important\nlearning\n(b) global smoothness\nlearning\n(a) local smoothness\nFig. 13: SSL and UL share (a) local and (b) global smoothness\nassumptions. Unlabeled samples (grey dots) are assigned to\nclass labels depending on the decision boundaries derived\nfrom the local or global smoothness assumptions.\nclass of unsupervised learners owing to their native unsu-\npervised nature to learn expressive data representations in\na probabilistic manner. Further, they do not require manual\ndesign of a meaningful discriminative learning objective,\nwhile offering a unique ability to generate abundant data.\n4\nDISCUSSION ON SSL AND UL\nIn this section, we connect SSL and UL via further discus-\nsion on their common learning assumptions (§4.1), and their\napplications in different computer vision tasks (§4.2).\n4.1\nThe learning assumptions shared by SSL and UL\nAs discussed in §2.1, the unsupervised learning objectives\nin SSL are often formulated based on the smoothness as-\nsumption [42]. Broadly speaking, the learning assumptions\nof various discriminative SSL and UL algorithms can be\ngrouped into two types of smoothness assumptions, i.e.\nlocal smoothness and global smoothness – as visually illus-\ntrated in Figure 13. In the following, we further elaborate\nthese assumptions and discuss the different SSL and UL\nalgorithms that are built upon these assumptions.\n4.1.1\nLocal Smoothness\nThere are two ﬂavors of local smoothness assumption.\nFirst, a sample xi is assumed to share the same class label\nas its transformed variant ˆxi (Eq. (15)). Second, a sample\nxi is assumed to belong to the same class as its nearby\nsample xj in the latent representation space (Eq. (16)). An\nunsupervised loss term enforces local smoothness on an\nunlabeled sample xi via:\nmin\nθ\nX\nxi∈D\nLunsup(f(xi), f(ˆxi))\n(15)\nmin\nθ\nX\nxi∈D\nLunsup(f(xi), f(xj))\n(16)\nwhere f(·) is the model to be trained and gives the model\noutput (such as features or predictions). Lunsup(·) could\nbe any similarity metric that quantiﬁes the divergence or\n13\ninconsistency between two model outputs, such as a mean\nsquare error, or contrastive loss.\nLocal smoothness among different views of the same\nsample (Eq. (15)) can be achieved via the consistency regu-\nlarization techniques in SSL (§2.2.1, Figure 3). They enforce\npredictive smoothness to the same samples under different\nvariations imposed at the input space and (or) model space,\ngiven that the different transformed versions of the same\nsample should lie in its own local neighborhood. Similarly,\nthe instance discrimination algorithms in UL also implicitly\nenforce the same samples under different views or trans-\nformations to have locally consistent representations, as\nrepresented by contrastive learning which encourages local\ninvariances on each sample (§3.2.2, Figure 10).\nLocal smoothness among the nearby samples (Eq. (16))\ncan be imposed via the graph-based regularization tech-\nniques in SSL. They often propagate the class labels to\nthe unlabeled samples using the labels of their neighbours\non the graph, as the nearby samples should likely share\nthe same class (§2.2.3, Figure 5). Similarly, neighbourhood\nconsistency is also explored in UL [169], [170], which forms\nthe semantic training labels by mining the nearest neighbors\nof each sample based on feature similarity, given that nearest\nneighbors are likely to belong to the same semantic class.\n4.1.2\nGlobal Smoothness\nThe global smoothness assumption indicates that a sample\nxi could be assigned to a certain class (or target) zi based on\nthe underlying global structures captured by the model:\nmin\nθ\nX\nxi∈D\nLunsup(f(xi), zi)\n(17)\nwhere zi is the learning target (e.g. the cluster membership\nor the most conﬁdent predicted class), which is derived\nfrom the global class decision boundaries discovered during\ntraining (Figure 13) whilst the decision boundaries are sup-\nposed to lie in low density regions. Similar to Eq. (15) and\nEq. (16), Lunsup(·) is a similarity metric that quantiﬁes the\ninconsistency between the model output and the training\ntarget, such as a cross-entropy loss. The global smoothness\nassumption is also widely adopted in various SSL and UL\ntechniques to learn from the unlabeled samples with pseudo\nlearning targets, as detailed in the following.\nThe self-training techniques in SSL (§2.2.2, Figure 4) are\ngenerally formulated based on global smoothness, as the\nlearning targets for unlabeled data are derived based on\nthe class decision boundaries discovered by the models. For\ninstance, in entropy minimization (Eq. (4), Figure 4 (a)), the\npseudo label is obtained as the class predicted with the high-\nest conﬁdence. In co-training and distillation (Eq. (5), Eq. (6),\nFigure 4 (b)(c)), the learning targets come from the model co-\ntrained in parallel or pre-trained beforehand. Similarly, the\ndeep clustering algorithms in UL (§3.2.2, Figure 11) are also\nproposed upon global smoothness, given that the cluster\nmemberships for unlabeled samples are acquired from an\nonline or ofﬂine clustering algorithm which uncovers the\nlatent class decision boundaries in the feature space.\n4.1.3\nConnections between SSL and UL\nThe learning rationales common in SSL and UL. As\nanalyzed in §4.1.1 and §4.1.2, most SSL and UL algorithms\nTABLE 3: A common taxonomy on SSL and UL methods\nbased on their learning assumptions.\nAssumption\nObjective Corresponding SSL & UL methods\nlocal smoothness\nEq. (15)\nconsistency regularization in SSL (§2.2.1)\ninstance discrimination in UL (§3.2.2)\nEq. (16)\ngraph-based regularization in SSL (§2.2.3)\nneighbourhood consistency in UL (§3.2.2)\nglobal smoothness\nEq. (17)\nself-training in SSL (§2.2.2)\ndeep clustering in UL (§3.2.2)\nare formulated based on the same local smoothness or\nglobal smoothness assumption – as summarized in Table\n3. A common aspect of these SSL and UL algorithms is\nto design visual learning objectives that enforce invariance\nor equivariance towards different transformations applied\non the input data, as represented by consistency regular-\nization in SSL (§2.2.1) and instance discrimination in UL\n(§3.2.2). Typical transformation strategies can range from\nsimple data augmentation [37], [39], [46], to more com-\nplex transformations such as adversarial perturbations [48],\n[49], [74], [81], rotations [123] and patch reordering [121],\nautoencoding transformations [200], [201] and automated\naugmentation [27], [38], [51]. On one side, most of these SSL\nand UL methods hinge on learning representations invariant\nto data augmentation and perturbations by assigning the\nsame underlying labels to the augmented and perturbed\ndata samples. On the other side, other SSL and UL meth-\nods consider learning representations that are equivalent\nto different transformations such as rotations and patch re-\nordering by learning to predict the type of transformations.\nMany state-of-the-art SSL and UL methods can be well\nrelated with the same underlying learning assumptions,\ngiven that they introduce similar objectives to learn from\nthe unlabeled samples. In essence, the learning rationales\nof these SSL and UL methods could be broadly categorized\nas: (1) impose the consistency among different transformed\nversions of the same sample (Eq. (15)), (2) enforce the\nsmoothness between a sample and its neighbouring one\n(Eq. (16)), and (3) derive learning targets for the unlabeled\nsamples based on global decision boundaries (Eq. (17)).\nThe similarities and differences between problem setups.\nIn the problems setups, SSL and UL are similar in the sense\nthat both labeled and unlabeled data are often involved in\ntheir training protocols before evaluating their generalized\nmodel performance on the test set. In particular, the SSL\nparadigm adopts one-stage training and uses both labeled\nand unlabeled data during training (Figure 2); while most\nexisting UL protocols consider two-stage training (Figure 7)\n– one stage for pre-training with unlabeled data and another\nstage for ﬁne-tuning with labeled data on a downstream task.\nIn brief, when it comes to training protocols, UL differs\nfrom SSL in several ways: (1) the labeled data and unlabeled\ndata are not given together at once; (2) unlabeled and\nlabeled datasets may have different distributions. These\nproperties make UL a more generic learning paradigm to\nleverage different unlabeled datasets. Nevertheless, how\nunsupervised pre-training upon different forms of unla-\nbeled data beneﬁts the model generalization on speciﬁc\ndownstream tasks remains an open research question. For\n14\ninstance, it remains unclear how an unsupervised model\npre-trained on natural colour images could generalize to\na downstream task that has a different data distribution\nsuch as grayscale images in medical imaging. In this regard,\nSSL provides a more reliable learning paradigm to utilize\nthe unlabeled data, given that the label set offers the prior\nknowledge for the models and (or) the model designers to\nselect the useful set of unlabeled samples that are similar to\nthe labeled data distribution.\n4.2\nApplied SSL and UL in Visual Recognition\nIn §2 and §3, we mainly present the SSL and UL methods\nfor standard image classiﬁcation. However, their underlying\nlearning rationales can be generalized to other challenging\ncomputer vision tasks, e.g., semantic segmentation [32],\n[202], object detection [30], [203], unsupervised domain\nadaptation [204], [205], pose estimation [34], [206], 3D scene\nunderstanding [207], video recognition [150], [208], etc. In\nthe following, we review three core visual recognition tasks\nthat widely beneﬁt from SSL and UL methods to exploit\nunlabeled data: semantic segmentation (§4.2.1), object detec-\ntion (§4.2.2), and unsupervised domain adaptation (§4.2.3).\n4.2.1\nSemantic Segmentation\nSemantic segmentation aims to assign a semantic class label\nfor each pixel in an input image. It is a core computer vision\ntask that could be beneﬁcial to various real-world applica-\ntions such as medical image analysis [209], [210], [211], [212]\nand autonomous driving [213], [214], [215]. Supervised se-\nmantic segmentation requires tedious and expensive pixel-\nwise label annotations, e.g. manually annotating one single\nnatural image in Cityscapes needs 1.5 hours [213].\nTo reduce the annotation costs in semantic segmentation,\na group of works consider only a small set of the training\ndata annotated with per-pixel semantic labels while the\nrest of the training data being unlabeled – known as semi-\nsupervised semantic segmentation. These works generally\ninherit similar learning rationales as SSL or UL for image\nclassiﬁcation, and adapt techniques such as consistency\nregularization [216], [217], [218], [219], self-training [202],\n[210], [220], [221], [222], [223], [224], GAN frameworks [225],\n[226], [227] in SSL, or contrastive learning [228], [229], [230],\n[231] in UL to learn from unlabeled images. Nevertheless,\nunsupervised loss terms in semantic segmentation are often\nrequired to impose in a per-pixel manner to align with the\npixel-wise learning objective in semantic segmentation. In\nthe following, we discuss the three most representative lines\nof state-of-the-art methods driven by recent advances in SSL\nand UL for semi-supervised semantic segmentation.\nConsistency regularization (§2.2.1) can be generalized for\npixel-wise tasks by formulating the consistency loss (Eq. (2),\nEq. (3)) at the pixel level. In a similar spirit as the standard\nconsistency regularization in SSL, recent works in semi-\nsupervised semantic segmentation [216], [217], [218], [219]\nresort to enforcing pixel consistency among the images\nbefore and after perturbations, whilst perturbations being\nintroduced at the input space [216] or feature space [217].\nFor instance, the ﬁrst consistency regularization method\nin semantic segmentation [216] applies CutOut [78] and\nCutMix [232] augmentation techniques to perturb the input\nimages with partial corruption, and imposes pixel-level\nloss terms to ensure the uncorrupted regions in perturbed\nimages should have consistent pixel-wise predictions as\nthe same regions in original images. A cross-consistency\ntraining [217] instead applies feature perturbations by in-\njecting noise into network’s activations and enforces pixel\nconsistency between the clean and perturbed outputs.\nSelf-training algorithms (§2.2.2) are adapted and shown\neffective for semi-supervised semantic segmentation [202],\n[210], [220], [221], [222], [223], [224], where pseudo seg-\nmentation maps on unlabeled images are propagated us-\ning a pre-trained teacher model [223], or a co-trained\nmodel [202]. For example, a self-training method [223]\npropagates pseudo segmentation labels with two steps –\n(1) assigning pixel-wise pseudo labels on unlabeled data\nwith a pre-trained teacher model; and (2) re-training a\nstudent model with the re-labeled dataset – until no more\nperformance gain is achieved. Another self-training ap-\nproach [202] adopts a co-training scheme by training two\nmodels to learn the per-pixel segmentation predictions from\neach others.\nContrastive learning is widely used in UL and adapted to\nlearn from unlabeled data in semantic segmentation [228],\n[229], [230], [231]. To formulate the contrastive loss (Eq. (12))\nper pixel, one needs select meaningful positive and negative\npairs with consideration of pixel spatial locations. For this\naim, a directional context-aware contrastive loss [228] is\nproposed to crop two patches from one image, and take\nfeatures at the same location as a positive pair and the\nrest as negative pairs. Another pixel contrastive loss [230] is\nintroduced to align the features before and after a random\ncolor augmentation by taking features at the same location\nas a positive pair, while sampling a ﬁxed amount of negative\npairs from different images.\n4.2.2\nObject Detection\nObject detection aims to predict a set of bounding boxes and\nthe corresponding class labels for the objects of interest in an\nimage. An object detector needs to unify classiﬁcation and\nlocalization into one model by jointly training a classiﬁer\nto predict class labels and a regression head to generate the\nbounding boxes [5], [233]. It is an important computer vision\ntask that widely impacts different applications such as per-\nson search [234], vehicle detection [235], logo detection [236],\ntext detection [237], etc. Supervised object detection requires\ncostly annotation efforts – annotating the bounding box of a\nsingle object takes up to 42 seconds [238].\nTo exploit the unlabeled data without bounding box or\nclass label information, a group of works in object detection\nexploit unlabeled data to boost model generalization by\ntraining on a small set of labeled data and a set of completely\nunlabeled images – known as semi-supervised object detec-\ntion. These works mainly reformulate two streams of SSL\ntechniques, including consistency regularization [30], [203],\n[239], [240], [241], [242] and self-training [35], [243], [244],\n[245], [246], both of which introduce the learning targets\nfor both bounding boxes and class labels to learn from the\ncompletely unlabeled data, as detailed next.\nConsistency regularization (§2.2.1) is introduced for semi-\nsupervised object detection to propagate the soft label and\nbounding boxes assignment on unlabeled images based on\n15\ndual consistency constraints on classiﬁcation and regres-\nsion [30], [203], [239], [240], [241], [242]. One line of works\napply data augmentation such as random ﬂipping [203]\nand MixUp [75] to generate augmented views of unlabeled\nimages and encourage the predicted bounding boxes and its\nclass labels remain consistent for the different views. Com-\npared to standard consistency regularization, these methods\nespecially need re-estimating the bounding box location in\nan augmented image, such as ﬂip the bounding box [203],\nor calculate the overlapped bounding boxes of two mixed\nimages in MixUp [75]. Another line of works follow a\nteacher-student training framework and impose teacher-\nstudent consistency [30], [240], [241], [242] similar to Mean\nTeacher [39]. The teacher model is derived either from the\nstudent model via exponential mean average (EMA) [30],\n[240], [242], or by applying non-maximum suppression\n(NMS, a ﬁltering technique for reﬁning the detected bound-\ning boxes) on the instant model outputs [241] to obtain the\npseudo bounding boxes and label annotations for training.\nSelf-training algorithms (§2.2.2) are also introduced to\nannotated unlabeled images for object detection [35], [243],\n[244], [245], [246]. A simple self-training paradigm is to\nannotate the unlabeled images with bounding boxes and\ntheir class labels using a pre-trained teacher model and\nuse these data for re-training [243]. However, such pseudo\nannotations may be rather noisy. To improve the quality of\npseudo labels, recent works propose interactive self-training\nto progressively reﬁne the pseudo labels with NMS [244], or\nquantify model uncertainty to select or derive more reliable\npseudo labels [245], [246] to learn from unlabeled data.\n4.2.3\nUnsupervised Domain Adaptation\nUnsupervised domain adaptation (UDA) is a special case of\nSSL where the labeled (source) and unlabeled (target) data\nlie in different distributions, a.k.a. different domains. UDA\nis essential for visual recognition [247], as the statistical\nproperties of visual data are sensitive to a wider variety of\nfactors, e.g., illumination, viewpoint, resolution, occlusion,\ntimes of the day, and weather conditions. While most UDA\nmethods focus on tackling the domain gap between the\nlabeled and unlabeled data, SSL and UL algorithms can also\nbe adapted to learn from unlabeled data in UDA, as follows.\nConsistency regularization (§2.2.1) is shown to be effec-\ntive in UDA. In the same spirit of encouraging consistent\noutputs under perturbations, various UDA approaches ap-\nply input transformations or model ensembling to simu-\nlate variations in input or model space [39], [248], [249],\n[250]. To generate input variations, a dual MixUp reg-\nularization integrates category-level MixUp and domain-\nlevel MixUp to regularize the model with consistency\nconstraints, thus learning from unlabeled data to enhance\ndomain-invariance [248]. To generate model variations, self-\nensembling [249] utilizes the Mean Teacher [39] to impute\nunlabeled training targets in target domain.\nSelf-training (§2.2.2) has been also useful for UDA. Sim-\nilar to SSL, self-training for UDA include three streams of\ntechniques to impute pseudo labels on the unlabeled target\nsamples, including entropy minimization, pseudo-label and\nco-training. To ensure the effectiveness, self-training meth-\nods are often coupled with domain distribution alignment\nfor reducing the domain shift. For instance, entropy mini-\nmization (Eq. (4)) is adopted for UDA [251], [252], [253], in\ncombination with distribution alignment techniques such as\ndomain-speciﬁc batch normalization layers [251], aligning\nsecond-order statistics of features [252], or adversarial train-\ning and gradient synchronization [253]. Co-training (Eq. (5))\nis also introduced for UDA, which imputes training targets\nfrom multiple co-trained classiﬁers to learn from unlabeled\ndata and match cross-domain distributions [254].\nDeep generative models (DGMs), as a class of models for\nSSL and UL (§2.2.4, §3.2.3), are widely adopted for UDA.\nIn contrast to other UDA methods that reduce the domain\nshift at the feature level, DGMs provide an alternative and\ncomplementary solution to mitigate the domain discrepancy\nat pixel level by cross-domain image-to-image translation.\nThe majority of these frameworks are based on GANs, such\nas PixelDA [255], generate to adapt [256], and GANs with\ncycle-consistency like CyCADA [257], SBADA-GAN [258],\nI2I Adapt [259] and CrDoCo [260]. These models typically\nlearn a real-to-real [257], [258], [260], [261] or synthetic-to-\nreal [255], [256], [262] mapping to render the image style\nfrom the labeled source to the unlabeled target domain, thus\noffering synthetic training data with pseudo labels.\nSelf-supervised learning popularized in SSL and UL\n(§2.2.5, §3.2.1), is also introduced in UDA to construct aux-\niliary self-supervised learning objectives on unlabeled data.\nSelf-supervised models often address the UDA problem by\nself-supervision coupled with a supervised objective on the\nlabeled source data [164], [204], [263], [264]. The pioneer\nwork in this direction is JiGen [164], which learns jointly\nto classify objects and solve the jigsaw puzzles [121] pretext\ntask to achieve better generalization in new domains. Recent\nworks [204], [263], [264] explored other self-supervised pre-\ntext tasks such as predicting rotation [204], [263], [264], ﬂip-\nping [204] and patch ordering [204]. Besides pretext tasks,\nrecent UDA methods also explored discriminative self-\nsupervision signals based on clustering or contrastive learn-\ning. For instance, DANCE [205] performs neighborhood\nclustering by assigning the target samples to a “known”\nclass prototype in the source domain or its neighbor in\nthe target domain. Gradient regularized contrastive learn-\ning [265] leverages the contrastive loss to push unlabeled\ntarget samples towards the most similar labeled source\nsamples. Similarly, [266] aligns target domain features to\nclass prototypes in the source domain through contrastive\nloss, minimizing the distances between the cross-domain\nsamples that likely belong to the same class.\n5\nEMERGING TRENDS AND OPEN CHALLENGES\nIn this section, we discuss the emerging trends in SSL and\nUL from unlabeled data, covering three directions, namely\nopen-set learning (§5.1), incremental learning (§5.2) and\nmulti-modal learning (§5.3). We detail both recent develop-\nments and open challenges.\n5.1\nOpen-Set Learning from Unlabeled Data\nIn §2, we review works addressing the relatively simple\nclosed-set learning in SSL, which assume that unlabeled\ndata share the same label space as the labeled one. However,\n16\nthis closed-set assumption may greatly hinder the effective-\nness of SSL in leveraging real-world uncurated unlabeled\ndata that contains unseen classes, i.e., out-of-distribution\n(OOD) samples (also known as outliers) [40]. When ap-\nplying most existing SSL methods to open-set learning\nwith noisy unlabeled data, their model performance may\ndegrade signiﬁcantly, as the OOD samples could induce\ncatastrophic error propagation.\nA line of works propose to address a more complex open-\nset SSL scenario [14], [15], [267], [268], [269], [270], [271],\n[272], where the unlabeled set contains task-irrelevant OOD\ndata. In this setup (so-called open-world SSL), unlabeled\nsamples are not all beneﬁcial. To prevent possible perfor-\nmance hazards caused by unlabeled OOD samples, recent\nadvances in SSL propose various sample-speciﬁc selection\nstrategies to discount their importance or usage [14], [15],\n[267], [268]. The pioneer works including UASD [14] and\nDS3L [15] propose to impose a dynamic weighting function\nto down-weight the unsupervised regularization loss term\nproportional to the likelihood that an unlabeled sample\nbelongs to an unseen class. Follow-up works resort to cur-\nriculum learning [267] and iterative self-training [268] by\ntraining an OOD classiﬁer to detect and discard the poten-\ntially detrimental samples. More recently, OpenMatch [270]\npropose to train a set of one-vs-all classiﬁers for detecting\ninliers and outliers and regularize the model with a consis-\ntency constraint on only the unlabeled inliers.\nOpen Challenges. The open-set SSL calls for integrating\nOOD detection [273] or novel class discovery [274] with\nsemi-supervised learning in a uniﬁed model to advance\nselective exploitation of noisy unlabeled data. Moreover, a\nmore recent work propose a universal SSL benchmark [271]\nwhich further extends the distribution mismatch problem\nin open-set setup as subset or intersectional class mismatch,\nand feature distribution mismatch. These more realistic\nsetups pose multiple new challenges, including conﬁdence\ncalibration of DNN for OOD detection [273], [273], [275],\n[276], [277], imbalanced class distribution caused by real-\nworld long-tailed distributed unlabeled data [278], [279],\nand discovery of unseen classes in unlabeled data [274],\n[280], [281]. Although recent advances in open-set SSL have\nexplored OOD detection, the other challenges remain to be\nresolved to exploit real-world unlabeled data.\n5.2\nIncremental Learning from Unlabeled Data\nExisting works on SSL and UL often assume all unlabeled\ntraining data is available at once, which however may\nnot always hold in practice due to privacy concerns or\ncomputational constraints. In many realistic scenarios, we\nneed to perform incremental learning (IL) with new data\nto update the model incrementally without access to past\ntraining data. Here we review research directions on IL from\nunlabeled data [282], [283] and discuss its open challenges.\nIncremental learning (IL) from unlabeled data has been\ninvestigated in a semi-supervised fashion [282]. IL (also\nknown as continual learning and lifelong learning [284])\naims to extend an existing model’s knowledge without\naccessing the previous training data. Most existing IL ap-\nproaches use regularization objectives to not forget old\nknowledge, i.e., reducing catastrophic forgetting [285], [286],\n[287], [288]. To this aim, unlabeled data is often used in IL\nto prevent catastrophic forgetting by estimating the impor-\ntance weights of model parameters for old tasks [289], or\nformulating a knowledge distillation objective [282], [290] to\nconsolidate the knowledge learned from old data. Recently,\nmultiple works explore IL from unlabeled data that comes\nas a non-stationary stream [283], [291], with the class label\nspace possibly varying over time [292]. In this setting, the\ngoal is to learn a salient representation from continuous\nincoming unlabeled data stream. To expand the representa-\ntions for novel classes and unlabeled data, several strategies\nare adopted to dynamically update representations in the\nlatent space, such as creating new cluster centroids by online\nclustering [292] and updating mixture-of-Gaussians [283].\nSome recent works apply self-supervised techniques on the\nunlabeled test-data [293], [294], [295], which is useful to\novercome possible shifts in the data distribution [296].\nOpen Challenges. Incremental learning from unlabeled\ndata requires solving multiple challenges, ranging from\ncatastrophic forgetting [282], [297], modeling new con-\ncepts [283], [292] to predicting the evolution of data\nstreams [296]. Due to lacking the access to all the unlabeled\ntraining data at once, addressing these challenges is nontriv-\nial as directly applying many existing SSL and UL methods\ncould not guarantee good generalization performance. As\nan example, pseudo labels may suffer the conﬁrmation\nbias problem [298] when classifying unseen unlabeled data.\nThus, incremental learning from a stream of potentially non-\ni.i.d. unlabeled data remains an open challenge.\n5.3\nMulti-Modal Learning from Unlabeled Data\nA growing number of works combine visual and non-\nvisual modalities (e.g., text, audio) to form discriminative\nself-supervision signals that enable learning from multi-\nmodal unlabeled data. To bring vision and language for\nunsupervised learning, variants of vision and language\nBERT models (e.g., ViLBERT [299], LXMERT [300], VL-\nBERT [301], Uniter [302] and Unicoder-VL [303]) are built\nupon the transformer blocks [304] to jointly model images\nand natural language in an unsupervised way. Speciﬁcally,\nthe visual, linguistic or their joint representations can be\nlearned in an unsupervised manner by solving the Cloze task\nin natural language processing which predicts the masked\nwords in the input sentences [305], or by optimizing a\nlinguistic-visual alignment objective [300], [306]. Another\nline of works utilize the language supervision (e.g., from\nweb data [307] or narrated materials [308], [309], [310], [311],\n[312], [313]) to guide unsupervised representation learning\nby aligning images and languages in the shared latent space,\nas exempliﬁed by CLIP [312] and ALIGN [313].\nSimilarly, to combine audio and visual modalities for\nunsupervised learning, existing works exploit the natural\naudio-visual correspondence in videos to formulate various\nself-supervised signals, which predict the cross-modal corre-\nspondence [314], [315], align the temporally corresponding\nrepresentations [309], [316], [317], [318], or cluster their\nrepresentations in a shared audio-visual latent space [208],\n[319]. Several works further explore audio, vision and lan-\nguage together for unsupervised representation learning by\naligning different modalities in a shared multi-modal latent\n17\nspace [310], [320] or in a hierarchical latent space for audio-\nvision and vision-language [308].\nOpen Challenges. The success of multi-modal learning\nfrom unlabeled data often relies on an assumption that dif-\nferent modalities are semantically correlated. For instance,\nwhen clustering audio and video data for unsupervised\nrepresentation learning [208], or transferring text knowledge\nto the unlabeled image data [321], the two data modali-\nties are assumed to share similar semantics. However, this\nassumption may not hold in real-world data, leading to\ndegraded model performance [309], [322]. Thus, it remains\nan open challenge to learn from the multi-modal unlabeled\ndata that contains a semantic gap across modalities.\n6\nCONCLUSION\nLearning visual representations with limited or no manual\nsupervision is critical for scalable computer vision appli-\ncations. Semi-supervised learning (SSL) and unsupervised\nlearning (UL) models provide feasible and promising solu-\ntions to learn from unlabeled visual data. In this comprehen-\nsive survey, we have introduced uniﬁed problem deﬁnitions\nand taxonomies to summarize and correlate a wide variety\nof recent advanced and popularized SSL and UL deep learn-\ning methodologies for building superior visual classiﬁcation\nmodels. We believe that our concise taxonomies of existing\nalgorithms and extensive discussions of emerging trends\nhelp to better understand the status quo of research in\nvisual representation learning with unlabeled data, as well\nas to inspire new learning solutions for major unresolved\nchallenges involved in the limited-label regime.\nACKNOWLEDGMENTS\nThis work has been partially funded by the ERC (853489-\nDEXIM) and the DFG (2064/1–Project number 390727645).\nREFERENCES\n[1]\nY. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature,\n2015.\n[2]\nI. Goodfellow, Y. Bengio, and A. Courville, “Deep learning,” MIT\npress, 2016.\n[3]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classi-\nﬁcation with deep convolutional neural networks,” in NeurIPS,\n2012.\n[4]\nF. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A uniﬁed\nembedding for face recognition and clustering,” in CVPR, 2015.\n[5]\nS. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-\ntime object detection with region proposal networks,” in NeurIPS,\n2015.\n[6]\nL.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.\nYuille, “Deeplab: Semantic image segmentation with deep con-\nvolutional nets, atrous convolution, and fully connected crfs,”\nIEEE TPAMI, 2017.\n[7]\nZ.-H. Zhou and M. Li, “Semi-supervised regression with co-\ntraining.” in IJCAI, 2005.\n[8]\nO. Chapelle, B. Scholkopf, and A. Zien, “Semi-supervised learn-\ning,” IEEE TNNLS, 2009.\n[9]\nK. Q. Weinberger and L. K. Saul, “Unsupervised learning of\nimage manifolds by semideﬁnite programming,” IJCV, 2006.\n[10]\nY. Bengio, A. Courville, and P. Vincent, “Representation learning:\nA review and new perspectives,” IEEE TPAMI, 2013.\n[11]\nC. Doersch, A. Gupta, and A. A. Efros, “Unsupervised visual\nrepresentation learning by context prediction,” in ICCV, 2015.\n[12]\nT. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple\nframework for contrastive learning of visual representations,” in\nICML, 2020.\n[13]\nX. J. Zhu, “Semi-supervised learning literature survey,” Univer-\nsity of Wisconsin-Madison Department of Computer Sciences,\nTech. Rep., 2005.\n[14]\nY. Chen, X. Zhu, W. Li, and S. Gong, “Semi-supervised learning\nunder class distribution mismatch.” in AAAI, 2020.\n[15]\nL.-Z. Guo, Z.-Y. Zhang, Y. Jiang, Y.-F. Li, and Z.-H. Zhou, “Safe\ndeep semi-supervised learning for unseen-class unlabeled data,”\nin ICML, 2020.\n[16]\nK. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum con-\ntrast for unsupervised visual representation learning,” in CVPR,\n2020.\n[17]\nJ. E. Van Engelen and H. H. Hoos, “A survey on semi-supervised\nlearning,” ML, 2020.\n[18]\nL. Jing and Y. Tian, “Self-supervised visual feature learning with\ndeep neural networks: A survey,” IEEE TPAMI, 2020.\n[19]\nL. Schmarje, M. Santarossa, S.-M. Schr¨oder, and R. Koch, “A\nsurvey on semi-, self-and unsupervised learning for image clas-\nsiﬁcation,” arXiv:2002.08721, 2020.\n[20]\nG.-J. Qi and J. Luo, “Small data challenges in big data era: A\nsurvey of recent progress on unsupervised and semi-supervised\nmethods,” IEEE TPAMI, 2020.\n[21]\nR. Fergus, Y. Weiss, and A. Torralba, “Semi-supervised learning\nin gigantic image collections,” in NeurIPS, 2009.\n[22]\nN. Papernot, M. Abadi, ´U. Erlingsson, I. Goodfellow, and K. Tal-\nwar, “Semi-supervised knowledge transfer for deep learning\nfrom private training data,” in ICLR, 2017.\n[23]\nA. Blum and T. Mitchell, “Combining labeled and unlabeled data\nwith co-training,” in COLT, 1998.\n[24]\nK. Nigam and R. Ghani, “Analyzing the effectiveness and appli-\ncability of co-training,” in CIKM, 2000.\n[25]\nM. W. Libbrecht and W. S. Noble, “Machine learning applications\nin genetics and genomics,” Nature Reviews Genetics, 2015.\n[26]\nD. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver,\nand C. Raffel, “Mixmatch: A holistic approach to semi-supervised\nlearning,” in NeurIPS, 2019.\n[27]\nD. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn,\nH. Zhang, and C. Raffel, “Remixmatch: Semi-supervised learning\nwith distribution alignment and augmentation anchoring,” in\nICLR, 2020.\n[28]\nY. K. Jang and N. I. Cho, “Generalized product quantization\nnetwork for semi-supervised image retrieval,” in CVPR, 2020.\n[29]\nJ. Gao, J. Wang, S. Dai, L.-J. Li, and R. Nevatia, “Note-rcnn: Noise\ntolerant ensemble rcnn for semi-supervised object detection,” in\nICCV, 2019.\n[30]\nY. Tang, W. Chen, Y. Luo, and Y. Zhang, “Humble teachers teach\nbetter students for semi-supervised object detection,” in CVPR,\n2021.\n[31]\nT. Kalluri, G. Varma, M. Chandraker, and C. Jawahar, “Universal\nsemi-supervised semantic segmentation,” in ICCV, 2019.\n[32]\nY. Ouali, C. Hudelot, and M. Tami, “Semi-supervised semantic\nsegmentation with cross-consistency training,” in CVPR, 2020.\n[33]\nM. S. Ibrahim, A. Vahdat, M. Ranjbar, and W. G. Macready, “Semi-\nsupervised semantic image segmentation with self-correcting\nnetworks,” in CVPR, 2020.\n[34]\nY. Chen, Z. Tu, L. Ge, D. Zhang, R. Chen, and J. Yuan, “So-\nhandnet: Self-organizing network for 3d hand pose estimation\nwith semi-supervised learning,” in ICCV, 2019.\n[35]\nI. Radosavovic, P. Doll´ar, R. Girshick, G. Gkioxari, and K. He,\n“Data distillation: Towards omni-supervised learning,” in CVPR,\n2018.\n[36]\nR.\nMitra,\nN.\nB.\nGundavarapu,\nA.\nSharma,\nand\nA.\nJain,\n“Multiview-consistent semi-supervised learning for 3d human\npose estimation,” in CVPR, 2020.\n[37]\nS. Laine and T. Aila, “Temporal ensembling for semi-supervised\nlearning,” in ICLR, 2017.\n[38]\nK. Sohn, D. Berthelot, C.-L. Li, Z. Zhang, N. Carlini, E. D. Cubuk,\nA. Kurakin, H. Zhang, and C. Raffel, “Fixmatch: Simplifying\nsemi-supervised learning with consistency and conﬁdence,” in\nNeurIPS, 2020.\n[39]\nA. Tarvainen and H. Valpola, “Mean teachers are better role\nmodels: Weight-averaged consistency targets improve semi-\nsupervised deep learning results,” in NeurIPS, 2017.\n[40]\nA. Oliver, A. Odena, C. A. Raffel, E. D. Cubuk, and I. Good-\nfellow, “Realistic evaluation of deep semi-supervised learning\nalgorithms,” in NeurIPS, 2018.\n18\n[41]\nB. Athiwaratkun, M. Finzi, P. Izmailov, and A. G. Wilson, “There\nare many consistent explanations of unlabeled data: Why you\nshould average,” in ICLR, 2019.\n[42]\nD. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Sch¨olkopf,\n“Learning with local and global consistency,” in NeurIPS, 2004.\n[43]\nO. Chapelle, J. Weston, and B. Sch¨olkopf, “Cluster kernels for\nsemi-supervised learning,” in NeurIPS, 2002.\n[44]\nJ. Weston, F. Ratle, H. Mobahi, and R. Collobert, “Deep learning\nvia semi-supervised embedding,” in ICML, 2008.\n[45]\nO. Chapelle, A. Zien, C. Z. Ghahramani et al., “Semi-supervised\nclassiﬁcation by low density separation,” in AISTATSW, 2005.\n[46]\nM. Sajjadi, M. Javanmardi, and T. Tasdizen, “Regularization\nwith stochastic transformations and perturbations for deep semi-\nsupervised learning,” in NeurIPS, 2016.\n[47]\nX. Wang, D. Kihara, J. Luo, and G.-J. Qi, “Enaet: A self-trained\nframework for semi-supervised and supervised learning with\nensemble transformations,” IEEE TIP, 2020.\n[48]\nT. Miyato, S.-i. Maeda, M. Koyama, K. Nakae, and S. Ishii,\n“Distributional smoothing with virtual adversarial training,” in\nICLR, 2016.\n[49]\nT. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii, “Virtual adver-\nsarial training: a regularization method for supervised and semi-\nsupervised learning,” IEEE TPAMI, 2018.\n[50]\nV. Verma, A. Lamb, J. Kannala, Y. Bengio, and D. Lopez-Paz, “In-\nterpolation consistency training for semi-supervised learning,” in\nIJCAI, 2019.\n[51]\nQ. Xie, Z. Dai, E. Hovy, T. Luong, and Q. Le, “Unsupervised data\naugmentation for consistency training,” in NeurIPS, 2020.\n[52]\nP. Bachman, O. Alsharif, and D. Precup, “Learning with pseudo-\nensembles,” in NeurIPS, 2014.\n[53]\nA. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko,\n“Semi-supervised learning with ladder networks,” in NeurIPS,\n2015.\n[54]\nS. Park, J.-K. Park, S.-J. Shin, and I.-C. Moon, “Adversarial\ndropout for supervised and semi-supervised learning,” in AAAI,\n2018.\n[55]\nL. Zhang and G.-J. Qi, “Wcp: Worst-case perturbations for semi-\nsupervised deep learning,” in CVPR, 2020.\n[56]\nD.-H.\nLee,\n“Pseudo-label:\nThe\nsimple\nand\nefﬁcient\nsemi-\nsupervised learning method for deep neural networks,” in\nICMLW, 2013.\n[57]\nY. Chen, X. Zhu, and S. Gong, “Semi-supervised deep learning\nwith memory,” in ECCV, 2018.\n[58]\nS. Qiao, W. Shen, Z. Zhang, B. Wang, and A. Yuille, “Deep co-\ntraining for semi-supervised image recognition,” in ECCV, 2018.\n[59]\nW. Dong-DongChen and Z.-H. WeiGao, “Tri-net for semi-\nsupervised deep learning,” in IJCAI, 2018.\n[60]\nQ. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, “Self-training with\nnoisy student improves imagenet classiﬁcation,” in CVPR, 2020.\n[61]\nY. Luo, J. Zhu, M. Li, Y. Ren, and B. Zhang, “Smooth neighbors\non teacher graphs for semi-supervised learning,” in CVPR, 2018.\n[62]\nT. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with\ngraph convolutional networks,” in ICLR, 2017.\n[63]\nA. Iscen, G. Tolias, Y. Avrithis, and O. Chum, “Label propagation\nfor deep semi-supervised learning,” in CVPR, 2019.\n[64]\nD. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, “Semi-\nsupervised learning with deep generative models,” in NeurIPS,\n2014.\n[65]\nL. Maaløe, C. K. Sønderby, S. K. Sønderby, and O. Winther,\n“Auxiliary deep generative models,” in ICML, 2016.\n[66]\nJ. T. Springenberg, “Unsupervised and semi-supervised learning\nwith categorical generative adversarial networks,” in ICLR, 2016.\n[67]\nT. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-\nford, and X. Chen, “Improved techniques for training gans,” in\nNeurIPS, 2016.\n[68]\nV. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mas-\ntropietro, and A. Courville, “Adversarially learned inference,” in\nICLR, 2017.\n[69]\nZ. Dai, Z. Yang, F. Yang, W. W. Cohen, and R. R. Salakhutdinov,\n“Good semi-supervised learning that requires a bad gan,” in\nNeurIPS, 2017.\n[70]\nG.-J. Qi, L. Zhang, H. Hu, M. Edraki, J. Wang, and X.-S. Hua,\n“Global versus localized generative adversarial nets,” in CVPR,\n2018.\n[71]\nX. Zhai, A. Oliver, A. Kolesnikov, and L. Beyer, “S4l: Self-\nsupervised semi-supervised learning,” in ICCV, 2019.\n[72]\nT. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton,\n“Big self-supervised models are strong semi-supervised learn-\ners,” in NeurIPS, 2020.\n[73]\nX. Zhu and Z. Ghahramani, “Learning from labeled and un-\nlabeled data with label propagation,” Technical Report, Carnegie\nMellon University, 2002.\n[74]\nT. Suzuki and I. Sato, “Adversarial transformations for semi-\nsupervised learning.” in AAAI, 2020.\n[75]\nH. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup:\nBeyond empirical risk minimization,” in ICLR, 2018.\n[76]\nE. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V.\nLe, “Autoaugment: Learning augmentation policies from data,”\narXiv:1805.09501, 2018.\n[77]\nE. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment:\nPractical automated data augmentation with a reduced search\nspace,” in CVPRW, 2020.\n[78]\nT. DeVries and G. W. Taylor, “Improved regularization of convo-\nlutional neural networks with cutout,” arXiv:1708.04552, 2017.\n[79]\nC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Good-\nfellow, and R. Fergus, “Intriguing properties of neural networks,”\nin ICLR, 2014.\n[80]\nA. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples\nin the physical world,” in ICLR, 2017.\n[81]\nA. Najaﬁ, S.-i. Maeda, M. Koyama, and T. Miyato, “Robustness\nto adversarial perturbations in learning from incomplete data,”\nin NeurIPS, 2019.\n[82]\nY. Carmon, A. Raghunathan, L. Schmidt, J. C. Duchi, and P. S.\nLiang, “Unlabeled data improves adversarial robustness,” in\nNeurIPS, 2019.\n[83]\nS. Lim, I. Kim, T. Kim, C. Kim, and S. Kim, “Fast autoaugment,”\nin NeurIPS, 2019.\n[84]\nD. Ho, E. Liang, X. Chen, I. Stoica, and P. Abbeel, “Population\nbased augmentation: Efﬁcient learning of augmentation policy\nschedules,” in ICML, 2019.\n[85]\nX. Zhang, Q. Wang, J. Zhang, and Z. Zhong, “Adversarial au-\ntoaugment,” in ICLR, 2019.\n[86]\nP. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G.\nWilson, “Averaging weights leads to wider optima and better\ngeneralization,” in UAI, 2018.\n[87]\nT. M. Mitchell, “Generalization as search,” AI, 1982.\n[88]\nR. E. Schapire, “The strength of weak learnability,” ML, 1990.\n[89]\nL. Breiman, “Random forests,” ML, 2001.\n[90]\nY. Freund, R. Schapire, and N. Abe, “A short introduction to\nboosting,” Journal-Japanese Society For Artiﬁcial Intelligence, 1999.\n[91]\nB. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and\nscalable predictive uncertainty estimation using deep ensem-\nbles,” in NeurIPS, 2017.\n[92]\nY. Grandvalet and Y. Bengio, “Semi-supervised learning by en-\ntropy minimization,” in NeurIPS, 2005.\n[93]\nM. Sajjadi, M. Javanmardi, and T. Tasdizen, “Mutual exclusivity\nloss for semi-supervised deep learning,” in ICIP, 2016.\n[94]\nJ. Snell, K. Swersky, and R. Zemel, “Prototypical networks for\nfew-shot learning,” in NeurIPS, 2017.\n[95]\nZ. Ke, D. Wang, Q. Yan, J. Ren, and R. W. Lau, “Dual student:\nBreaking the limits of the teacher in semi-supervised learning,”\nin CVPR, 2019.\n[96]\nI. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and\nharnessing adversarial examples,” in ICLR, 2015.\n[97]\nG. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in\na neural network,” arXiv:1503.02531, 2015.\n[98]\nC. Buciluˇa, R. Caruana, and A. Niculescu-Mizil, “Model compres-\nsion,” in ACM SIGKDD, 2006.\n[99]\nJ. Ba and R. Caruana, “Do deep nets really need to be deep?”\nNeurIPS, 2014.\n[100] I. Z. Yalniz, H. J´egou, K. Chen, M. Paluri, and D. Mahajan,\n“Billion-scale semi-supervised learning for image classiﬁcation,”\narXiv:1905.00546, 2019.\n[101] X. Zhu, Z. Ghahramani, and J. D. Lafferty, “Semi-supervised\nlearning using gaussian ﬁelds and harmonic functions,” in ICML,\n2003.\n[102] M. Belkin, P. Niyogi, and V. Sindhwani, “Manifold regularization:\nA geometric framework for learning from labeled and unlabeled\nexamples,” JMLR, 2006.\n[103] B. Wang, Z. Tu, and J. K. Tsotsos, “Dynamic label propagation for\nsemi-supervised multi-class multi-label classiﬁcation,” in CVPR,\n2013.\n19\n[104] B. Jiang, Z. Zhang, D. Lin, J. Tang, and B. Luo, “Semi-supervised\nlearning with graph learning-convolutional networks,” in CVPR,\n2019.\n[105] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore,\nE. S¨ackinger, and R. Shah, “Signature veriﬁcation using a\n“siamese” time delay neural network,” IJPRAI, 1993.\n[106] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction\nby learning an invariant mapping,” in CVPR, 2006.\n[107] W. Lin, Z. Gao, and B. Li, “Shoestring: Graph-based semi-\nsupervised classiﬁcation with severely limited labeled data,” in\nCVPR, 2020.\n[108] S. Li, B. Liu, D. Chen, Q. Chu, L. Yuan, and N. Yu, “Density-aware\ngraph for deep semi-supervised visual recognition,” in CVPR,\n2020.\n[109] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”\narXiv:1312.6114, 2013.\n[110] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adver-\nsarial nets,” in NeurIPS, 2014.\n[111] C.\nDoersch,\n“Tutorial\non\nvariational\nautoencoders,”\narXiv:1606.05908, 2016.\n[112] M. Ehsan Abbasnejad, A. Dick, and A. van den Hengel, “Inﬁnite\nvariational autoencoder for semi-supervised learning,” in CVPR,\n2017.\n[113] A. Kumar, P. Sattigeri, and T. Fletcher, “Semi-supervised learning\nwith gans: Manifold invariance with improved inference,” in\nNeurIPS, 2017.\n[114] C. Li, T. Xu, J. Zhu, and B. Zhang, “Triple generative adversarial\nnets,” in NeurIPS, 2017.\n[115] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee,\n“Generative adversarial text to image synthesis.” in ICML, 2016.\n[116] S. Reed, Z. Akata, S. Mohan, S. Tenka, B. Schiele, and H. Lee,\n“Learning what and where to draw.” in NIPS, 2016.\n[117] Y. Xian, T. Lorenz, B. Schiele, and Z. Akata, “Feature generating\nnetworks for zero-shot learning,” in CVPR, 2018.\n[118] Y. Xian, S. Sharma, B. Schiele, and Z. Akata, “F-vaegan-d2: A\nfeature generating framework for any-shot learning,” in CVPR,\n2019.\n[119] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox,\n“Discriminative unsupervised feature learning with convolu-\ntional neural networks,” in NeurIPS, 2014.\n[120] A. Dosovitskiy, P. Fischer, J. T. Springenberg, M. Riedmiller,\nand T. Brox, “Discriminative unsupervised feature learning with\nexemplar convolutional neural networks,” IEEE TPAMI, 2015.\n[121] M. Noroozi and P. Favaro, “Unsupervised learning of visual\nrepresentations by solving jigsaw puzzles,” in ECCV, 2016.\n[122] M. Noroozi, H. Pirsiavash, and P. Favaro, “Representation learn-\ning by learning to count,” in ICCV, 2017.\n[123] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised represen-\ntation learning by predicting image rotations,” in ICLR, 2018.\n[124] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimension-\nality of data with neural networks,” Science, 2006.\n[125] J. Masci, U. Meier, D. Cires¸an, and J. Schmidhuber, “Stacked\nconvolutional auto-encoders for hierarchical feature extraction,”\nin ICANN, 2011.\n[126] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros,\n“Context encoders: Feature learning by inpainting,” in CVPR,\n2016.\n[127] K. He, X. Chen, S. Xie, Y. Li, P. Doll´ar, and R. Girshick, “Masked\nautoencoders are scalable vision learners,” in CVPR, 2022.\n[128] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Ex-\ntracting and composing robust features with denoising autoen-\ncoders,” in ICML, 2008.\n[129] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,”\nin ECCV, 2016.\n[130] ——, “Split-brain autoencoders: Unsupervised learning by cross-\nchannel prediction,” in CVPR, 2017.\n[131] G. Larsson, M. Maire, and G. Shakhnarovich, “Colorization as a\nproxy task for visual understanding,” in CVPR, 2017.\n[132] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, “Unsupervised feature\nlearning via non-parametric instance discrimination,” in CVPR,\n2018.\n[133] I. Misra and L. v. d. Maaten, “Self-supervised learning of pretext-\ninvariant representations,” in CVPR, 2020.\n[134] Y. Tian, D. Krishnan, and P. Isola, “Contrastive multiview cod-\ning,” in ECCV, 2020.\n[135] X. Chen and K. He, “Exploring simple siamese representation\nlearning,” in CVPR, 2021.\n[136] J.-B. Grill, F. Strub, F. Altch´e, C. Tallec, P. H. Richemond,\nE. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar\net al., “Bootstrap your own latent: A new approach to self-\nsupervised learning,” in NeurIPS, 2020.\n[137] Y. Tian, X. Chen, and S. Ganguli, “Understanding self-supervised\nlearning dynamics without contrastive pairs,” in ICML, 2021.\n[138] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clus-\ntering for unsupervised learning of visual features,” in ECCV,\n2018.\n[139] J. Yang, D. Parikh, and D. Batra, “Joint unsupervised learning of\ndeep representations and image clusters,” in CVPR, 2016.\n[140] Y. Asano, C. Rupprecht, and A. Vedaldi, “Self-labelling via simul-\ntaneous clustering and representation learning,” in ICLR, 2020.\n[141] X. Ji, J. F. Henriques, and A. Vedaldi, “Invariant information clus-\ntering for unsupervised image classiﬁcation and segmentation,”\nin CVPR, 2019.\n[142] J. Huang, S. Gong, and X. Zhu, “Deep semantic clustering by\npartition conﬁdence maximisation,” in CVPR, 2020.\n[143] P. Haeusser, J. Plapp, V. Golkov, E. Aljalbout, and D. Cremers,\n“Associative deep clustering: Training a classiﬁcation network\nwith no labels,” in GCPR, 2018.\n[144] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and\nA. Joulin, “Unsupervised learning of visual features by contrast-\ning cluster assignments,” in NeurIPS, 2020.\n[145] A. Radford, L. Metz, and S. Chintala, “Unsupervised represen-\ntation learning with deep convolutional generative adversarial\nnetworks,” arXiv:1511.06434, 2015.\n[146] T. Chen, X. Zhai, M. Ritter, M. Lucic, and N. Houlsby, “Self-\nsupervised gans via auxiliary rotation loss,” in CVPR, 2019.\n[147] J. Wang, W. Zhou, G.-J. Qi, Z. Fu, Q. Tian, and H. Li, “Transfor-\nmation gan for unsupervised image synthesis and representation\nlearning,” in CVPR, 2020.\n[148] A. Brock, J. Donahue, and K. Simonyan, “Large scale gan training\nfor high ﬁdelity natural image synthesis,” in ICLR, 2019.\n[149] J. Donahue and K. Simonyan, “Large scale adversarial represen-\ntation learning,” in NeurIPS, 2019.\n[150] X. Wang and A. Gupta, “Unsupervised learning of visual repre-\nsentations using videos,” in ICCV, 2015.\n[151] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and\nK. Murphy, “Tracking emerges by colorizing videos,” in ECCV,\n2018.\n[152] I. Aganj, M. G. Harisinghani, R. Weissleder, and B. Fischl, “Unsu-\npervised medical image segmentation based on the local center\nof mass,” Nature, 2018.\n[153] K. He, R. Girshick, and P. Doll´ar, “Rethinking imagenet pre-\ntraining,” in CVPR, 2019.\n[154] C. Feichtenhofer, H. Fan, B. Xiong, R. Girshick, and K. He, “A\nlarge-scale study on unsupervised spatiotemporal representation\nlearning,” in CVPR, 2021.\n[155] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards\nreal-time object detection with region proposal networks,” IEEE\nTPAMI, 2016.\n[156] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and\nA. Zisserman, “The pascal visual object classes (voc) challenge,”\nIJCV, 2010.\n[157] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” in\nCVPR, 2017.\n[158] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\ncontext,” in ECCV, 2014.\n[159] R. Santa Cruz, B. Fernando, A. Cherian, and S. Gould, “Visual\npermutation learning,” IEEE TPAMI, 2018.\n[160] T. Nathan Mundhenk, D. Ho, and B. Y. Chen, “Improvements to\ncontext based self-supervised learning,” in CVPR, 2018.\n[161] C. Wei, L. Xie, X. Ren, Y. Xia, C. Su, J. Liu, Q. Tian, and A. L.\nYuille, “Iterative reorganization with weak spatial constraints:\nSolving arbitrary jigsaw puzzles for unsupervised representation\nlearning,” in CVPR, 2019.\n[162] P. Goyal, D. Mahajan, A. Gupta, and I. Misra, “Scaling and\nbenchmarking self-supervised visual representation learning,” in\nICCV, 2019.\n[163] G. Goh, N. Cammarata, C. Voss, S. Carter, M. Petrov, L. Schubert,\nA. Radford, and C. Olah, “Multimodal neurons in artiﬁcial neural\nnetworks,” Distill, 2021.\n20\n[164] F. M. Carlucci, A. D’Innocente, S. Bucci, B. Caputo, and T. Tom-\nmasi, “Domain generalization by solving jigsaw puzzles,” in\nCVPR, 2019.\n[165] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal,\nP. Bachman, A. Trischler, and Y. Bengio, “Learning deep repre-\nsentations by mutual information estimation and maximization,”\nin ICLR, 2019.\n[166] P. Bachman, R. D. Hjelm, and W. Buchwalter, “Learning repre-\nsentations by maximizing mutual information across views,” in\nNeurIPS, 2019.\n[167] M. Tschannen, J. Djolonga, P. K. Rubenstein, S. Gelly, and M. Lu-\ncic, “On mutual information maximization for representation\nlearning,” in ICLR, 2019.\n[168] Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola,\n“What makes for good views for contrastive learning,” in\nNeurIPS, 2020.\n[169] J. Huang, Q. Dong, S. Gong, and X. Zhu, “Unsupervised deep\nlearning by neighbourhood discovery,” in ICML, 2019.\n[170] W. Van Gansbeke, S. Vandenhende, S. Georgoulis, M. Proesmans,\nand L. Van Gool, “Learning to classify images without labels,” in\nECCV, 2020.\n[171] M. Caron, P. Bojanowski, J. Mairal, and A. Joulin, “Unsupervised\npre-training of image features on non-curated data,” in ICCV,\n2019.\n[172] D. Novotny, S. Albanie, D. Larlus, and A. Vedaldi, “Self-\nsupervised learning of geometrically stable features through\nprobabilistic introspection,” in CVPR, 2018.\n[173] M. Ye, X. Zhang, P. C. Yuen, and S.-F. Chang, “Unsupervised em-\nbedding learning via invariant and spreading instance feature,”\nin CVPR, 2019.\n[174] X. Chen, H. Fan, R. Girshick, and K. He, “Improved baselines\nwith momentum contrastive learning,” arXiv:2003.04297, 2020.\n[175] M. Gutmann and A. Hyv¨arinen, “Noise-contrastive estimation:\nA new estimation principle for unnormalized statistical models,”\nin AISTATS, 2010.\n[176] M. Federici, A. Dutta, P. Forr´e, N. Kushman, and Z. Akata,\n“Learning robust representations via multi-view information bot-\ntleneck,” in ICLR, 2020.\n[177] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv:1807.03748, 2018.\n[178] J. Robinson, C.-Y. Chuang, S. Sra, and S. Jegelka, “Contrastive\nlearning with hard negative samples,” in ICLR, 2021.\n[179] Q. Hu, X. Wang, W. Hu, and G.-J. Qi, “Adco: Adversarial contrast\nfor efﬁcient learning of unsupervised representations from self-\ntrained negative adversaries,” in CVPR, 2021.\n[180] X. Wang, Y. Huang, D. Zeng, and G.-J. Qi, “Caco: Both positive\nand negative samples are directly learnable via cooperative-\nadversarial contrastive learning,” arXiv:2203.14370, 2022.\n[181] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny, “Barlow twins:\nSelf-supervised learning via redundancy reduction,” in ICML,\n2021.\n[182] J. Xie, R. Girshick, and A. Farhadi, “Unsupervised deep embed-\nding for clustering analysis,” in ICML, 2016.\n[183] W. Hu, T. Miyato, S. Tokui, E. Matsumoto, and M. Sugiyama,\n“Learning discrete representations via information maximizing\nself-augmented training,” in ICML, 2017.\n[184] C. Zhuang, A. L. Zhai, and D. Yamins, “Local aggregation for\nunsupervised learning of visual embeddings,” in CVPR, 2019.\n[185] X. Yan, I. Misra, A. Gupta, D. Ghadiyaram, and D. Mahajan,\n“Clusterﬁt: Improving generalization of visual representations,”\nin CVPR, 2020.\n[186] X. Wang, Z. Liu, and S. X. Yu, “Unsupervised feature learning\nby cross-level discrimination between instances and groups,” in\nCVPR, 2021.\n[187] S. Gidaris, A. Bursuc, N. Komodakis, P. P´erez, and M. Cord,\n“Learning representations by predicting bags of visual words,”\nin CVPR, 2020.\n[188] A. K. Jain, M. N. Murty, and P. J. Flynn, “Data clustering: a\nreview,” CSUR, 1999.\n[189] A. Coates and A. Y. Ng, “Learning feature representations with\nk-means,” in Neural networks: Tricks of the trade, 2012.\n[190] U. Von Luxburg, “A tutorial on spectral clustering,” Statistics and\ncomputing, 2007.\n[191] J. Chang, L. Wang, G. Meng, S. Xiang, and C. Pan, “Deep adaptive\nimage clustering,” in CVPR, 2017.\n[192] X. Guo, L. Gao, X. Liu, and J. Yin, “Improved deep embedded\nclustering with local structure preservation,” in IJCAI, 2017.\n[193] B. Yang, X. Fu, N. D. Sidiropoulos, and M. Hong, “Towards k-\nmeans-friendly spaces: Simultaneous deep learning and cluster-\ning,” in ICML, 2017.\n[194] K. C. Gowda and G. Krishna, “Agglomerative clustering using\nthe concept of mutual nearest neighbourhood,” PR, 1978.\n[195] M. Cuturi, “Sinkhorn distances: Lightspeed computation of opti-\nmal transport,” in NeurIPS, 2013.\n[196] Y. Li, P. Hu, Z. Liu, D. Peng, J. T. Zhou, and X. Peng, “Contrastive\nclustering,” in AAAI, 2020.\n[197] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning\nalgorithm for deep belief nets,” Neural computation, 2006.\n[198] A. Gabbay and Y. Hoshen, “Demystifying inter-class disentangle-\nment,” in ICLR, 2020.\n[199] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell, “Adversarial feature\nlearning,” in ICLR, 2017.\n[200] G.-J. Qi, L. Zhang, C. W. Chen, and Q. Tian, “Avt: Unsuper-\nvised learning of transformation equivariant representations by\nautoencoding variational transformations,” in ICCV, 2019.\n[201] L. Zhang, G.-J. Qi, L. Wang, and J. Luo, “Aet vs. aed: Unsuper-\nvised representation learning by auto-encoding transformations\nrather than data,” in CVPR, 2019.\n[202] X. Chen, Y. Yuan, G. Zeng, and J. Wang, “Semi-supervised se-\nmantic segmentation with cross pseudo supervision,” in CVPR,\n2021.\n[203] J. Jeong, S. Lee, J. Kim, and N. Kwak, “Consistency-based semi-\nsupervised learning for object detection,” in NeurIPS, 2019.\n[204] Y. Sun, E. Tzeng, T. Darrell, and A. A. Efros, “Unsupervised\ndomain adaptation through self-supervision,” arXiv:1909.11825,\n2019.\n[205] K. Saito, D. Kim, S. Sclaroff, and K. Saenko, “Universal domain\nadaptation through self supervision,” in NeurIPS, 2020.\n[206] L. Yang, S. Chen, and A. Yao, “Semihand: Semi-supervised hand\npose estimation with consistency,” in ICCV, 2021.\n[207] T. Kim, J. Choi, S. Choi, D. Jung, and C. Kim, “Just a few points\nare all you need for multi-view stereo: A novel semi-supervised\nlearning method for multi-view stereo,” in ICCV, 2021.\n[208] H. Alwassel, D. Mahajan, B. Korbar, L. Torresani, B. Ghanem, and\nD. Tran, “Self-supervised learning by cross-modal audio-video\nclustering,” in NeurIPS, 2020.\n[209] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional\nnetworks for biomedical image segmentation,” in MICCAI, 2015.\n[210] X. Huo, L. Xie, J. He, Z. Yang, W. Zhou, H. Li, and Q. Tian, “Atso:\nAsynchronous teacher-student optimization for semi-supervised\nimage segmentation,” in CVPR, 2021.\n[211] H. Wu, G. Chen, Z. Wen, and J. Qin, “Collaborative and adversar-\nial learning of focused and dispersive representations for semi-\nsupervised polyp segmentation,” in ICCV, 2021.\n[212] H. Huang, L. Lin, Y. Zhang, Y. Xu, J. Zheng, X. Mao, X. Qian,\nZ. Peng, J. Zhou, Y.-W. Chen et al., “Graph-bas3net: Boundary-\naware semi-supervised segmentation network with bilateral\ngraph convolution,” in ICCV, 2021.\n[213] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\nR. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes\ndataset for semantic urban scene understanding,” in CVPR, 2016.\n[214] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, “Denseaspp for\nsemantic segmentation in street scenes,” in CVPR, 2018.\n[215] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stach-\nniss, and J. Gall, “Semantickitti: A dataset for semantic scene\nunderstanding of lidar sequences,” in ICCV, 2019.\n[216] G. French, T. Aila, S. Laine, M. Mackiewicz, and G. Finlayson,\n“Semi-supervised semantic segmentation needs strong, high-\ndimensional perturbations,” in BMVC, 2020.\n[217] Y. Ouali, C. Hudelot, and M. Tami, “Semi-supervised semantic\nsegmentation with cross-consistency training,” in CVPR, 2020.\n[218] Z. Ke, D. Qiu, K. Li, Q. Yan, and R. W. Lau, “Guided collaborative\ntraining for pixel-wise semi-supervised learning,” in ECCV, 2020.\n[219] H. Hu, F. Wei, H. Hu, Q. Ye, J. Cui, and L. Wang, “Semi-\nsupervised semantic segmentation via adaptive equalization\nlearning,” in NeurIPS, 2021.\n[220] R. Mendel, L. A. De Souza, D. Rauber, J. P. Papa, and C. Palm,\n“Semi-supervised segmentation based on error-correcting super-\nvision,” in ECCV, 2020.\n[221] Y. Zou, Z. Zhang, H. Zhang, C.-L. Li, X. Bian, J.-B. Huang,\nand T. Pﬁster, “Pseudoseg: Designing pseudo labels for semantic\nsegmentation,” in ICLR, 2021.\n21\n[222] M. S. Ibrahim, A. Vahdat, M. Ranjbar, and W. G. Macready, “Semi-\nsupervised semantic image segmentation with self-correcting\nnetworks,” in CVPR, 2020.\n[223] R. He, J. Yang, and X. Qi, “Re-distributing biased pseudo labels\nfor semi-supervised semantic segmentation: A baseline investi-\ngation,” in ICCV, 2021.\n[224] J. Yuan, Y. Liu, C. Shen, Z. Wang, and H. Li, “A simple baseline\nfor semi-supervised semantic segmentation with strong data\naugmentation,” in ICCV, 2021.\n[225] N. Souly, C. Spampinato, and M. Shah, “Semi supervised seman-\ntic segmentation using generative adversarial network,” in ICCV,\n2017.\n[226] W.-C. Hung, Y.-H. Tsai, Y.-T. Liou, Y.-Y. Lin, and M.-H. Yang, “Ad-\nversarial learning for semi-supervised semantic segmentation,”\nin BMVC, 2018.\n[227] S. Mittal, M. Tatarchenko, and T. Brox, “Semi-supervised se-\nmantic segmentation with high-and low-level consistency,” IEEE\nTPAMI, 2021.\n[228] X. Lai, Z. Tian, L. Jiang, S. Liu, H. Zhao, L. Wang, and\nJ. Jia, “Semi-supervised semantic segmentation with directional\ncontext-aware consistency,” in CVPR, 2021.\n[229] I. Alonso, A. Sabater, D. Ferstl, L. Montesano, and A. C. Murillo,\n“Semi-supervised semantic segmentation with pixel-level con-\ntrastive learning from a class-wise memory bank,” in ICCV, 2021.\n[230] Y. Zhong, B. Yuan, H. Wu, Z. Yuan, J. Peng, and Y.-X. Wang, “Pixel\ncontrastive-consistent semi-supervised semantic segmentation,”\nin ICCV, 2021.\n[231] Y. Zhou, H. Xu, W. Zhang, B. Gao, and P.-A. Heng, “C3-semiseg:\nContrastive semi-supervised segmentation via cross-set learning\nand dynamic class-balancing,” in ICCV, 2021.\n[232] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, “Cutmix:\nRegularization strategy to train strong classiﬁers with localizable\nfeatures,” in ICCV, 2019.\n[233] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,”\nin ECCV, 2020.\n[234] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang, “Joint detection and\nidentiﬁcation feature learning for person search,” in CVPR, 2017.\n[235] K. Qian, S. Zhu, X. Zhang, and L. E. Li, “Robust multimodal\nvehicle detection in foggy weather using complementary lidar\nand radar signals,” in CVPR, 2021.\n[236] H. Su, S. Gong, and X. Zhu, “Multi-perspective cross-class do-\nmain adaptation for open logo detection,” CVIU, 2021.\n[237] W. Feng, F. Yin, X.-Y. Zhang, and C.-L. Liu, “Semantic-aware\nvideo text detection,” in CVPR, 2021.\n[238] O. Russakovsky, L.-J. Li, and L. Fei-Fei, “Best of both worlds:\nhuman-machine collaboration for object annotation,” in CVPR,\n2015.\n[239] J. Jeong, V. Verma, M. Hyun, J. Kannala, and N. Kwak,\n“Interpolation-based semi-supervised learning for object detec-\ntion,” in CVPR, 2021.\n[240] Y.-C. Liu, C.-Y. Ma, Z. He, C.-W. Kuo, K. Chen, P. Zhang, B. Wu,\nZ. Kira, and P. Vajda, “Unbiased teacher for semi-supervised\nobject detection,” in ICLR, 2021.\n[241] Q. Zhou, C. Yu, Z. Wang, Q. Qian, and H. Li, “Instant-teaching:\nAn end-to-end semi-supervised object detection framework,” in\nCVPR, 2021.\n[242] M. Xu, Z. Zhang, H. Hu, J. Wang, L. Wang, F. Wei, X. Bai, and\nZ. Liu, “End-to-end semi-supervised object detection with soft\nteacher,” in ICCV, 2021.\n[243] K. Sohn, Z. Zhang, C.-L. Li, H. Zhang, C.-Y. Lee, and T. Pﬁster,\n“A simple semi-supervised learning framework for object detec-\ntion,” arXiv:2005.04757, 2020.\n[244] Q. Yang, X. Wei, B. Wang, X.-S. Hua, and L. Zhang, “Interactive\nself-training with mean teachers for semi-supervised object de-\ntection,” in CVPR, 2021.\n[245] Z. Wang, Y. Li, Y. Guo, L. Fang, and S. Wang, “Data-uncertainty\nguided multi-phase learning for semi-supervised object detec-\ntion,” in CVPR, 2021.\n[246] Z. Wang, Y. Li, Y. Guo, and S. Wang, “Combating noise: Semi-\nsupervised learning by region uncertainty quantiﬁcation,” in\nNeurIPS, 2021.\n[247] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, “Adapting visual\ncategory models to new domains,” in ECCV, 2010.\n[248] Y. Wu, D. Inkpen, and A. El-Roby, “Dual mixup regularized\nlearning for adversarial domain adaptation,” in ECCV, 2020.\n[249] G. French, M. Mackiewicz, and M. Fisher, “Self-ensembling for\nvisual domain adaptation,” in ICLR, 2018.\n[250] Z. Deng, Y. Luo, and J. Zhu, “Cluster alignment with a teacher\nfor unsupervised domain adaptation,” in ICCV, 2019.\n[251] F. M. Carlucci, L. Porzi, B. Caputo, E. Ricci, and S. R. Bulo,\n“Autodial: Automatic domain alignment layers,” in ICCV, 2017.\n[252] P. Morerio, J. Cavazza, and V. Murino, “Minimal-entropy corre-\nlation alignment for unsupervised deep domain adaptation,” in\nICLR, 2018.\n[253] L. Hu, M. Kan, S. Shan, and X. Chen, “Unsupervised domain\nadaptation with hierarchical gradient synchronization,” in CVPR,\n2020.\n[254] K. Saito, Y. Ushiku, and T. Harada, “Asymmetric tri-training for\nunsupervised domain adaptation,” in ICML, 2017.\n[255] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krish-\nnan, “Unsupervised pixel-level domain adaptation with genera-\ntive adversarial networks,” in CVPR, 2017.\n[256] S. Sankaranarayanan, Y. Balaji, A. Jain, S. Nam Lim, and R. Chel-\nlappa, “Learning from synthetic data: Addressing domain shift\nfor semantic segmentation,” in CVPR, 2018.\n[257] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko,\nA. Efros, and T. Darrell, “CyCADA: Cycle-consistent adversarial\ndomain adaptation,” in ICML, 2018.\n[258] P. Russo, F. M. Carlucci, T. Tommasi, and B. Caputo, “From source\nto target and back: symmetric bi-directional adaptive gan,” in\nCVPR, 2018.\n[259] Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and K. Kim,\n“Image to image translation for domain adaptation,” in CVPR,\n2018.\n[260] Y.-C. Chen, Y.-Y. Lin, M.-H. Yang, and J.-B. Huang, “Crdoco:\nPixel-level domain transfer with cross-domain consistency,” in\nCVPR, 2019.\n[261] D. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon, “Pixel-level\ndomain transfer,” in ECCV, 2016.\n[262] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang, and\nR. Webb, “Learning from simulated and unsupervised images\nthrough adversarial training.” in CVPR, 2017.\n[263] J. Xu, L. Xiao, and A. M. L´opez, “Self-supervised domain adap-\ntation for computer vision tasks,” IEEE Access, 2019.\n[264] S. Bucci, A. D’Innocente, Y. Liao, F. M. Carlucci, B. Caputo, and\nT. Tommasi, “Self-supervised learning across domains,” IEEE\nTPAMI, 2020.\n[265] P. Su, S. Tang, P. Gao, D. Qiu, N. Zhao, and X. Wang, “Gradient\nregularized contrastive learning for continual domain adapta-\ntion,” in AAAI, 2021.\n[266] R. Wang, Z. Wu, Z. Weng, J. Chen, G.-J. Qi, and Y.-G. Jiang,\n“Cross-domain contrastive learning for unsupervised domain\nadaptation,” IEEE Transactions on Multimedia, 2022.\n[267] Q. Yu, D. Ikami, G. Irie, and K. Aizawa, “Multi-task curriculum\nframework for open-set semi-supervised learning,” in ECCV,\n2020.\n[268] M. Augustin and M. Hein, “Out-distribution aware self-training\nin an open world setting,” arXiv:2012.12372, 2020.\n[269] J. Huang, C. Fang, W. Chen, Z. Chai, X. Wei, P. Wei, L. Lin, and\nG. Li, “Trash to treasure: Harvesting ood data with cross-modal\nmatching for open-set semi-supervised learning,” in ICCV, 2021.\n[270] K. Saito, D. Kim, and K. Saenko, “Openmatch: Open-set consis-\ntency regularization for semi-supervised learning with outliers,”\nin NeurIPS, 2021.\n[271] Z. Huang, C. Xue, B. Han, J. Yang, and C. Gong, “Universal\nsemi-supervised learning,” in Thirty-Fifth Conference on Neural\nInformation Processing Systems, 2021.\n[272] K. Cao, M. Brbic, and J. Leskovec, “Open-world semi-supervised\nlearning,” in ICLR, 2022.\n[273] D. Hendrycks and K. Gimpel, “A baseline for detecting misclas-\nsiﬁed and out-of-distribution examples in neural networks,” in\nICLR, 2017.\n[274] Z. Zhong, L. Zhu, Z. Luo, S. Li, Y. Yang, and N. Sebe, “Open-\nmix: Reviving known knowledge for discovering novel visual\ncategories in an open world,” in CVPR, 2021.\n[275] D. Hendrycks, M. Mazeika, and T. Dietterich, “Deep anomaly\ndetection with outlier exposure,” in ICLR, 2019.\n[276] K. Lee, K. Lee, H. Lee, and J. Shin, “A simple uniﬁed frame-\nwork for detecting out-of-distribution samples and adversarial\nattacks,” in NeurIPS, 2018.\n22\n[277] M. Hein, M. Andriushchenko, and J. Bitterwolf, “Why relu\nnetworks yield high-conﬁdence predictions far away from the\ntraining data and how to mitigate the problem,” in CVPR, 2019.\n[278] J. Kim, Y. Hur, S. Park, E. Yang, S. J. Hwang, and J. Shin,\n“Distribution aligning reﬁnery of pseudo-label for imbalanced\nsemi-supervised learning,” in NeurIPS, 2020.\n[279] H. Lee, S. Shin, and H. Kim, “Abc: Auxiliary balanced classi-\nﬁer for class-imbalanced semi-supervised learning,” in NeurIPS,\n2021.\n[280] K. Han, A. Vedaldi, and A. Zisserman, “Learning to discover\nnovel visual categories via deep transfer clustering,” in ICCV,\n2019.\n[281] K. Han, S.-A. Rebufﬁ, S. Ehrhardt, A. Vedaldi, and A. Zisserman,\n“Automatically discovering and learning new visual categories\nwith ranking statistics,” in ICLR, 2020.\n[282] K. Lee, K. Lee, J. Shin, and H. Lee, “Overcoming catastrophic\nforgetting with unlabeled data in the wild,” in ICCV, 2019.\n[283] D. Rao, F. Visin, A. A. Rusu, R. Pascanu, Y. W. Teh, and R. Hadsell,\n“Continual unsupervised representation learning,” in NeurIPS,\n2019.\n[284] M.\nDelange,\nR.\nAljundi,\nM.\nMasana,\nS.\nParisot,\nX.\nJia,\nA. Leonardis, G. Slabaugh, and T. Tuytelaars, “A continual\nlearning survey: Defying forgetting in classiﬁcation tasks,” IEEE\nTPAMI, 2021.\n[285] M. McCloskey and N. J. Cohen, “Catastrophic interference in\nconnectionist networks: The sequential learning problem,” in\nPsychology of learning and motivation, 1989.\n[286] S.-A. Rebufﬁ, A. Kolesnikov, G. Sperl, and C. H. Lampert, “icarl:\nIncremental classiﬁer and representation learning,” in CVPR,\n2017.\n[287] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Des-\njardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-\nBarwinska et al., “Overcoming catastrophic forgetting in neural\nnetworks,” PNAS, 2017.\n[288] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr, “Rieman-\nnian walk for incremental learning: Understanding forgetting\nand intransigence,” in ECCV, 2018.\n[289] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuyte-\nlaars, “Memory aware synapses: Learning what (not) to forget,”\nin ECCV, 2018.\n[290] J. Zhang, J. Zhang, S. Ghosh, D. Li, S. Tasci, L. Heck, H. Zhang,\nand C.-C. J. Kuo, “Class-incremental learning via deep model\nconsolidation,” in WACV, 2020.\n[291] Y. Li, Y. Wang, Q. Liu, C. Bi, X. Jiang, and S. Sun, “Incremental\nsemi-supervised learning on streaming data,” PR, 2019.\n[292] J. Smith, S. Baer, Z. Kira, and C. Dovrolis, “Unsupervised contin-\nual learning and self-taught associative memory hierarchies,” in\nICLRW, 2019.\n[293] Y. Sun, X. Wang, Z. Liu, J. Miller, A. Efros, and M. Hardt,\n“Test-time training with self-supervision for generalization under\ndistribution shifts,” in ICML, 2020.\n[294] T. Varsavsky, M. Orbes-Arteaga, C. H. Sudre, M. S. Graham,\nP. Nachev, and M. J. Cardoso, “Test-time unsupervised domain\nadaptation,” in MICCAI, 2020.\n[295] D. Wang, E. Shelhamer, S. Liu, B. Olshausen, and T. Darrell, “Tent:\nFully test-time adaptation by entropy minimiaztion,” in ICLR,\n2021.\n[296] J. Hoffman, T. Darrell, and K. Saenko, “Continuous manifold\nbased adaptation for evolving visual domains,” in CVPR, 2014.\n[297] A. Bobu, E. Tzeng, J. Hoffman, and T. Darrell, “Adapting to\ncontinuously shifting domains,” in ICLRW, 2018.\n[298] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor, and K. McGuin-\nness, “Pseudo-labeling and conﬁrmation bias in deep semi-\nsupervised learning,” in IJCNN, 2020.\n[299] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-\nagnostic visiolinguistic representations for vision-and-language\ntasks,” in NeurIPS, 2019.\n[300] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder\nrepresentations from transformers,” in ACL, 2019.\n[301] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, “Vl-bert:\nPre-training of generic visual-linguistic representations,” in ICLR,\n2019.\n[302] Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng,\nand J. Liu, “Uniter: Universal image-text representation learn-\ning,” in ECCV, 2020.\n[303] G. Li, N. Duan, Y. Fang, M. Gong, D. Jiang, and M. Zhou,\n“Unicoder-vl: A universal encoder for vision and language by\ncross-modal pre-training.” in AAAI, 2020.\n[304] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin NeurIPS, 2017.\n[305] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in ACL, 2019.\n[306] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid,\n“Videobert: A joint model for video and language representation\nlearning,” in ICCV, 2019.\n[307] J. C. Stroud, D. A. Ross, C. Sun, J. Deng, R. Sukthankar, and\nC. Schmid, “Learning video representations from textual web\nsupervision,” arXiv:2007.14937, 2020.\n[308] J.-B. Alayrac, A. Recasens, R. Schneider, R. Arandjelovi´c, J. Rama-\npuram, J. De Fauw, L. Smaira, S. Dieleman, and A. Zisserman,\n“Self-supervised multimodal versatile networks,” in NeurIPS,\n2020.\n[309] A. Miech, J.-B. Alayrac, L. Smaira, I. Laptev, J. Sivic, and A. Zis-\nserman, “End-to-end learning of visual representations from\nuncurated instructional videos,” in CVPR, 2020.\n[310] A. Rouditchenko, A. Boggust, D. Harwath, D. Joshi, S. Thomas,\nK. Audhkhasi, R. Feris, B. Kingsbury, M. Picheny, A. Torralba\net al., “Avlnet: Learning audio-visual language representations\nfrom instructional videos,” arXiv:2006.09199, 2020.\n[311] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev,\nand J. Sivic, “Howto100m: Learning a text-video embedding by\nwatching hundred million narrated video clips,” in ICCV, 2019.\n[312] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar-\nwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning\ntransferable visual models from natural language supervision,”\nin ICML, 2021.\n[313] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V.\nLe, Y. Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-\nlanguage representation learning with noisy text supervision,” in\nICML, 2021.\n[314] R. Arandjelovic and A. Zisserman, “Look, listen and learn,” in\nCVPR, 2017.\n[315] A. Owens and A. A. Efros, “Audio-visual scene analysis with\nself-supervised multisensory features,” in ECCV, 2018.\n[316] B. Korbar, D. Tran, and L. Torresani, “Cooperative learning of\naudio and video models from self-supervised synchronization,”\nin NeurIPS, 2018.\n[317] P. Morgado, N. Vasconcelos, and I. Misra, “Audio-visual instance\ndiscrimination with cross-modal agreement,” in CVPR, 2021.\n[318] M. Patrick, Y. M. Asano, R. Fong, J. F. Henriques, G. Zweig, and\nA. Vedaldi, “Multi-modal self-supervision from generalized data\ntransformations,” arXiv:2003.04298, 2020.\n[319] A. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and A. Tor-\nralba, “Ambient sound provides supervision for visual learning,”\nin ECCV, 2016.\n[320] P. Hu, H. Zhu, X. Peng, and J. Lin, “Semi-supervised multi-modal\nlearning with balanced spectral decomposition,” in AAAI, 2020.\n[321] S. Li, B. Xie, J. Wu, Y. Zhao, C. H. Liu, and Z. Ding, “Simul-\ntaneous semantic alignment network for heterogeneous domain\nadaptation,” in ACM MM, 2020.\n[322] Y. Chen, Y. Xian, A. S. Koepke, Y. Shan, and Z. Akata, “Distilling\naudio-visual knowledge by compositional contrastive learning,”\nin CVPR, 2021.\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2022-08-24",
  "updated": "2022-08-24"
}