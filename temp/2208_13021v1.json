{
  "id": "http://arxiv.org/abs/2208.13021v1",
  "title": "On Unsupervised Training of Link Grammar Based Language Models",
  "authors": [
    "Nikolay Mikhaylovskiy"
  ],
  "abstract": "In this short note we explore what is needed for the unsupervised training of\ngraph language models based on link grammars. First, we introduce the\nter-mination tags formalism required to build a language model based on a link\ngrammar formalism of Sleator and Temperley [21] and discuss the influence of\ncontext on the unsupervised learning of link grammars. Second, we pro-pose a\nstatistical link grammar formalism, allowing for statistical language\ngeneration. Third, based on the above formalism, we show that the classical\ndissertation of Yuret [25] on discovery of linguistic relations using lexical\nat-traction ignores contextual properties of the language, and thus the\napproach to unsupervised language learning relying just on bigrams is flawed.\nThis correlates well with the unimpressive results in unsupervised training of\ngraph language models based on bigram approach of Yuret.",
  "text": "On Unsupervised Training of Link Grammar Based \nLanguage Models \nNikolay Mikhaylovskiy 1, 2[0000-0001-5660-0601] \n1 Higher IT School, Tomsk State University, Tomsk, Russia, 634050 \n2 NTR Labs, Moscow, Russia, 129594 \nnickm@ntr.ai \nAbstract. In this short note we explore what is needed for unsupervised training \nof graph language models based on link grammars. First, we introduce the termi-\nnation tags formalism required to build a language model based on a link gram-\nmar formalism of Sleator and Temperley [21] and discuss the influence of context \non the unsupervised learning of link grammars. Second, we propose a statistical \nlink grammar formalism, allowing for statistical language generation. Third, \nbased on the above formalism, we show that the classical dissertation of Yuret \n[25] on discovery of linguistic relations using lexical attraction ignores contex-\ntual properties of the language, and thus the approach to unsupervised language \nlearning relying just on bigrams is flawed. This correlates well with the unim-\npressive results in unsupervised training of graph language models based on bi-\ngram approach of Yuret. \nKeywords: Link Grammar, Language Models. \n1 \nMotivation \nWhile not long ago language models were just models that assign probabilities to se-\nquences of words [11], now they are the cornerstone of any task in computational lin-\nguistics through few-shot learning [4], prompt engineering [14] or fine-tuning [6].  \nOn the other hand, current language models fail to catch long-range dependencies in \nthe text consistently. For example, text generation with maximum likelihood target \nleads to rapid text degeneration, and consistent long text generation requires probabil-\nistic sampling and other tricks [9]. Large language models such as GPT-3 [4] move the \nboundary of “long text” (far away), but do not remove the problem. \nOne of the sources of the above phenomenon lies in the fact that in the correlations \nin natural language texts decrease according to the power law of distance between the \ntokens. This, in turn, is considered an outcome of the hierarchical structure of human \ntexts [1, 2]. Further, the mutual information between two tokens decays exponentially \nwith distance between them in any probabilistic regular grammar and Markov chains, \nbut can decay like a power law for a context-free grammar [14].  \nRecent results [6] indicate that, while, theoretically, RNNs are Turing complete [20], \nin practice, RNNs and Transformers trained with gradient descent fail to generalize on \nnonregular tasks, LSTMs can solve regular and counter-language tasks, and only net-\nworks augmented with structured memory can successfully generalize on context-free \nand context-sensitive tasks. Thus, building language models that exhibit at least hierar-\nchical, context-free grammar-ish, slow-correlation-decay behavior may be beneficial \nfor a variety of downstream tasks. This may be not enough to model long texts success-\nfully because natural languages cannot be described by a context-free grammar [19], \nbut may be a meaningful step. \n2 \nLink Grammars \nGrammar of a natural language is its set of structural constraints on speakers' or writers' \ncomposition of clauses, phrases, and words ([24]). The idea of basing a grammar on \nconstituent structure dates back to Chomsky [4] and Backus [1] (see also Jurafsky and \nMartin [11]). Dependency grammar formalism was simultaneously proposed by \nTesniere [22].  Link grammars [21] are a type of dependency grammar; these, in turn, \ncan be converted to and from phrase-structure grammars using relatively simple rules \nand algorithms. \nFollowing Vepstas and Goertzel [23], let’s consider basics of Link Grammars. In a \nLink Grammar, each word is associated with a set of ’connector disjuncts’, each con-\nnector disjunct controlling the possible linkages that the word may take part in. A dis-\njunct can be thought of as a jigsaw puzzle-piece; valid syntactic word orders are those \nfor which the puzzle-pieces can be validly connected. A single connector can be thought \nof as a single tab on a puzzle-piece (shown in Fig. 1). Connectors are thus ’types’ X \nwith a + or - sign indicating that they connect to the left or right. For example, a typical \nverb disjunct might be S −& O+ indicating that a subject (a noun) is expected on the \nleft, and an object (also a noun) is expected on the right. \n \nFig. 1. - Link Grammar Connectors \nThe lexical entries in a lexicon for the above would be  \n \nFig. 2. - A lexicon of a link grammar \n \n \nNote that though the symbols ‘&’ and ‘or’ are used to write down the disjuncts, these \nare not Boolean operators and they do not form a Boolean algebra. They do form a non-\nsymmetric compact closed monoidal algebra. \nVepstas and Goertzel [23] suggested representing a phrase in this notation as de-\npicted on Fig. 3. In the next section, we will slightly modify this notation so that it has \na more closed form. \n \nFig. 3. - A phrase in a graphical notation of the Link Grammar  \nThe above can also be written in a textual form:  \n𝑇ℎ𝑒+ 𝐷−𝑐𝑎𝑡+ 𝑆−𝑐ℎ𝑎𝑠𝑒𝑑+ 𝑂−𝑠𝑛𝑎𝑘𝑒−𝐷+ 𝑎 \n(1) \nOne cannot guarantee a textual form for every Link Grammar graph, but we will use \nthis notation when suitable. \n3 \nTermination Tags \nTo make the above notation more logical, and, as we will see further, properly intro-\nduce the frequentist statistics, we introduce Termination Tags (TT) for each of the links \nin the link grammar, instead of original formalism that only includes LEFT-WALL. \nThese tags are pseudo-terms that terminate the links that do not have a matching pair. \nIndeed, if we look at the Fig. 3, we will note that actually two connectors hang loose: \nO- connector from the ‘cat’ and S+ connector from the ‘snake’. We depict this in Fig. \n4: \n \nFig. 4. – Loose connectors in a Link Grammar of a sentence.  \nThus, we can add a terminator tag of S- type to indicate that the S+ link from ‘snake’ \ndoes not have a pair, and do the same for the O- link of ‘cat’ (see Fig. 5): \n \nFig. 5. - Terminator tag closes the link \nThis notation differs from the typical Link Grammar notation with beginning of sen-\ntence, but, as we will see in the next section, allows for frequentist construction of Link \nGrammar based language models. \n4 \nLanguage Models Based on Link Grammars \nJurafsky and Martin [11] define language models or LMs as the models that assign \nprobabilities to sequences of words. To achieve the goal of this work we need to extend \nthe definition and build a model that assigns probabilities to sentences as graph struc-\ntures. Probabilistic language model frameworks were created for other types of gram-\nmars equivalent to Link Grammars of [21], including [10, 13, 15]. Our goal is to add to \n[21] formalisms allowing language model creation. \n4.1 \nText Generation with a Statistical Link Grammar \nLet’s first consider the problem of text generation using a Link Grammar. Suppose \nwe have a lexicon ℒ of terms 𝑡𝑘 with their respective disjuncts, and for every connector \nin such a disjunct we have probabilities of words that would plug into this connector, \nincluding TT. This differs significantly from a deterministic approach of Ramesh and \nKolonin [16, 17] that basically builds a surface realization. \nNow we can start with any term with its disjunct, and assume that the probability of \nthe term plugged into the disjunct depends only on the original term and the connector. \nIn the example above, we can start with the word ‘cat’. We can suppose that in the \nlexicon its D- connector has potential links to two terms: ‘the’ with probability 0.6 and \n‘a’ with probability 0.4. Let’s assume that the random sampling have returned ‘the’.  \nThe O- connector of the term ‘cat’ in the lexicon has potential links to three terms: \n‘chased’ with probability 0.3, ‘ran’ with probability 0.2, and TT with probability 0.5. \nLet’s assume that the random sampling have returned TT. Finally, the S+ connector of \n‘cat’ has potential links to three terms: ‘chased’ with probability 0.5, ‘ran’ with proba-\nbility 0.4, and TT with probability 0.1. Let’s assume that the random sampling have \nreturned ‘chased’. \n \n \nThis way we have generated the closest, in a Link Grammar sense, neighbors of the \nstarting word ‘cat’, as depicted on Fig. 6: \n \nFig. 6. A piece of text graph generated using the Link Grammar \nWe can recursively continue this procedure with each generated word that is not TT (in \nthe example above there is only one such word – “chased”) and generate a sentence \ngraph in the Link Grammar. It is easy to see that under mild assumptions on probabili-\nties of TT, the graph generated will almost always have a finite length. In many senses, \nthe procedure above is similar to bigram-based text generation. \nIn the procedure above a termination tag may happen at both sides of a sentence, \nunlike the notation of [21, 23] that in a generation setting may generate infinitely. \n4.2 \nFrequentist Statistics and a Link Grammar Language Model \nThe above can be formalized as a discrete parameterized source 𝑆(𝑡, 𝑙), where 𝑡 is \nthe term of the lexicon ℒ and 𝑙 is the specific link from the term. The source emits a \nconnected term with a probability distribution {𝛼𝑖}. Each term  𝑡𝑘 that has a connector \nmatching 𝑙 has a fixed probability 𝛼𝑘 to be generated. {𝛼𝑖} is subject to ∑𝛼𝑖\n𝑖\n= 1.  \nWe can see the above probability distribution as a parametrized distribu-\ntion {𝑃𝑖(𝑡𝑘, 𝑙)}. With a frequentist approach, we can write that the probability of term \n𝑡𝑖 linked to 𝑡𝑘 with a link 𝑙 is  𝑃𝑖(𝑡𝑘, 𝑙) = \n𝐶(𝑡𝑘+𝑙−𝑡𝑖)\n𝐶(𝑡𝑘)\n (remember that the expression in \nthe numerator is a link grammar expression, not an arithmetical one). The operator 𝐶 \ncounts the occurrences of its argument over a certain corpus. \nIf we want to go further and estimate the probability of a sentence (read: a graph in \na link grammar) in a certain communicative context we can apply a chain rule of prob-\nability, taking the context into consideration. If we would be working with a sequence \nof words like the n-gram techniques do, we would write (Jurafsky and Martin [11]) for \nthe probability of a sequence of 𝑚 words: \n𝑃(𝑤1:𝑚) = 𝑃(𝑤1)𝑃(𝑤2|𝑤1)𝑃(𝑤3|𝑤1:2) … 𝑃(𝑤𝑚|𝑤1:𝑚−1) = ∏\n𝑃(𝑤𝑘|𝑤1:𝑘−1)\n𝑚\n𝑘=1\n (2) \nand approximate it with a truncated version in a naïve Bayesian way: \n𝑃(𝑤1:𝑚) = ∏𝑃(𝑤𝑘|𝑤1:𝑘−1)\n𝑚\n𝑘=1\n= ∏𝑃(𝑤𝑘−𝑛:𝑘−1|𝑤1:𝑘−n−1)\n𝑚\n𝑘=1\n∏𝑃(𝑤𝑘|𝑤𝑘−𝑛:𝑘−1)\n𝑚\n𝑘=1\n= 𝑃𝑐𝑜𝑛𝑡𝑒𝑥𝑡∏𝑃(𝑤𝑘|𝑤𝑘−𝑛:𝑘−1)\n𝑚\n𝑘=1\n,  \nwhere the last term is an n-gram language model and  \n𝑃𝑐𝑜𝑛𝑡𝑒𝑥𝑡= ∏𝑃(𝑤𝑘−𝑛:𝑘−1|𝑤1:𝑘−n−1)\n𝑚\n𝑘=1\n \n(3) \ndepends on the context only, and is considered to be equal to 1 as an approximation. \nA frequentist explanation of the approach is that if we consider sufficiently long pieces \nof the text, they would be unique even in a large corpus, thus all the counts would be 1 \nand so the conditional probabilities would be (the alternative would be resolving an \nambiguity of \n0\n0, which can be extrapolated to be equal to 1 as well): \n𝑃(𝑤𝑘−𝑛:𝑘−1|𝑤1:𝑘−n−1) =\n𝐶(𝑤1:𝑘−1)\n𝐶(𝑤𝑘−𝑛:𝑘−1) \n(4) \n \nWith a Link Grammar, we work with graphs and can actually build a tree of a sen-\ntence. In this structure, the context information is beyond the sentence, unlike the n-\ngram model. Thus, we can start with the root of the tree and use the chain rule along \neach branch, but we should specifically take the context into consideration, as each \nconditional probability does depend on the context.  \nMore specifically, let’s denote 𝑤1 the root of the sentence tree, 𝑤𝑘 – a term appearing \nin the sentence, 𝑤1/𝑤𝑘 – the path from the root to the term  𝑤𝑘 (following, for example, \nGrimmett [7]). Then, we can assume that the probabilities of different branches are \nindependent. What does this assumption/approximation imply requires a separate dis-\ncussion.  \nWith the notation and assumptions listed above, we can write the probability of the \nsentence as  \n𝑃(𝑆) = ∏𝑃(𝑤𝑘|𝑤1/ 𝑤𝑘\n− )\n𝑚\n𝑘=1\n(5) \nLet’s take note that in the classical dissertation of Yuret [25] the formula (12) in the \nproof of Theorem 1 provides only an approximated form of the same probability of a \nsentence. In our notation, this formula is \n𝑃(𝑆) = ∏𝑃(𝑤𝑘|𝑤𝑘\n−)\n𝑚\n𝑘=1\n \n(6) \nThe difference is small but important. The implicit assumption in (6) is that the condi-\ntional probability of a word in a sentence depends on an only one linked word (its pre-\n \n \ndecessor). For linear, n-gram models, this would be equivalent to saying that a proba-\nbility of any n-gram is equal to the probability of its final bigram, which is incorrect \nfrom both empirical and mathematical viewpoints. This leads Yuret to an incorrect con-\nclusion that “the entropy of the model is completely determined by the mutual infor-\nmation captured in syntactic relations” of “correlation taken for causation” type.  \nFurther, this leads Yuret to conclude, “The goal of the processor is to find the de-\npendency structure that assigns a given sentence a high probability. In Chapter 3, I \nshowed that the probability of a sentence is determined by the mutual information cap-\ntured in syntactic relations. Thus, the problem is to find the dependency structure with \nthe highest total mutual information.” This approach is ungrounded, as we have seen \nabove, so the approach to building the dependency structure is also incorrect. Unfortu-\nnately, many subsequent works have relied on this conclusion (for example, [12] and \n[23]). This correlates well with the unimpressive results in unsupervised training of \ngraph language models based on bigram approach of Yuret. \nHowever, we must pay tribute to [12] and note that the authors have understood that \nthey work with an assumption: “All systems that we are aware of operate under the \nassumption that the probability of a dependency structure is the product of the scores \nof the dependencies (attachments) in that structure.” By now, it is clear that the assump-\ntion is wrong. \n5 \nAcknowledgements \nThe author is grateful to Anton Kolonin for introduction into Link Grammars and dis-\ncussions of this work. \n6 \nReferences \n1. Altmann, E.G., Cristadoro, G., Degli, M.: On the origin of long-range correlations in texts. \nPNAS. 109, 11582–11587 (2012). https://doi.org/10.1073/pnas.1117723109. \n2. Alvarez-Lacalle, E., Dorow, B., Eckmann, J., Moses, E.: Hierarchical structures induce \nlong-range dynamical correlations in written texts. PNAS. 103, 7956–7961 (2006). \nhttps://doi.org/10.1073/pnas.0510673103. \n3. Backus, J. W.: The syntax and semantics of the proposed international algebraic language \nof the Zurich ACM-GAMM Conference. Information Processing: Proceedings of the Inter-\nnational Conference on Information Processing, Paris. UNESCO (1959). \n4. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., \nShyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., \nChild, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., \nLitwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., \nSutskever, I., Amodei, D.: Language models are few-shot learners. In: Advances in Neural \nInformation Processing Systems. pp. 1877–1901 (2020). \n5. Chomsky, N.: Three models for the description of language. IRE Transactions on Infor-\nmation Theory, 2(3):113–124 (1956). \n6. Delétang, G., Ruoss, A., Grau-Moya, J., Genewein, T., Wenliang, L.K., Catt, E., Hutter, M., \nLegg, S., Ortega, P.A.: Neural Networks and the Chomsky Hierarchy. (2022).  \n7. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional \ntransformers for language understanding. In: NAACL HLT 2019 - 2019 Conference of the \nNorth American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies - Proceedings of the Conference. pp. 4171–4186 (2019). \n8. Grimmett, G.: Probability on Graphs: Random Processes on Graphs and Lattices. Cambridge \nUniversity Press (2018). \n9. Holtzman, A., Buys, J., Du, L., Forbes, M., Choi, Y.: The curious case of neural text degen-\neration. In: Proceedings of the 2020 International Conference on Learning Representations \n(2020). \n10. Jelinek, F., Lafferty, J.D., Mercer, R.L.: Basic Methods of Probabilistic Context Free Gram-\nmars. \nIn: \nSpeech \nRecognition \nand \nUnderstanding. \npp. \n345–360 \n(1992). \nhttps://doi.org/10.1007/978-3-642-76626-8_35. \n11. Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd ed. draft) (2022) \n12. Klein, D., Manning, C.D.: Corpus-Based Induction of Syntactic Structure: Models of De-\npendency and Constituency. In: Proceedings of the 42nd Annual Meeting on Association for \nComputational Linguistics. pp. 479–486 (2004). https://doi.org/10.3115/1218955.1219016. \n13. Lafferty, J., Sleator, D., Temperley, D.: Grammatical Trigrams: A Probabilistic Model of \nLink Grammar. AAAI Tech. Rep. FS-92-04. In: Proceedings of the AAAI Conference on \nProbabilistic Approaches to Natural Language. pp. 89–97  (1992). \n14. Lin, H.W., Tegmark, M.: Critical behavior in physics and probabilistic formal languages. \nEntropy. 19, 1–25 (2017). https://doi.org/10.3390/e19070299. \n15. Paskin, M.A.: Grammatical bigrams. In: Proceedings of the 14th International Conference \non Neural Information Processing Systems: Natural and Synthetic (NIPS’01). pp. 91–97. \nMIT Press, Cambridge, MA, USA (2002). https://doi.org/10.7551/mitpress/1120.003.0016 \n16. Ramesh, V., Kolonin, A.: Natural Language Generation Using Link Grammar for General \nConversational Intelligence. 1–17. ArXiv abs/2105.00830 (2021) \n17. Ramesh, V., Kolonin, A.: Unsupervised Context-Driven Question Answering Based on Link \nGrammar. Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes \nBioinformatics). 13154 LNAI, 210–220 (2022). https://doi.org/10.1007/978-3-030-93758-\n4_22. \n18. Sanh, V., Webson, A., Raffel, C., Bach, S.H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stieg-\nler, A., Scao, T. Le, Raja, A., Dey, M., Bari, M.S., Xu, C., Thakker, U., Sharma, S.S., \nSzczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta, D., Chang, J., Jiang, M.T.-J., Wang, \nH., Manica, M., Shen, S., Yong, Z.X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, \nJ., Sharma, A., Santilli, A., Fevry, T., Fries, J.A., Teehan, R., Bers, T., Biderman, S., Gao, \nL., Wolf, T., Rush, A.M.: Multitask Prompted Training Enables Zero-Shot Task Generali-\nzation. In: ICLR (2022). \n19. Shieber, S.M.: Evidence against the context-freeness of natural language. Linguist. Philos. \n8, 333–343 (1985). https://doi.org/10.1007/BF00630917. \n20. Siegelmann, H.T., Sontag, E.D.: Analog computation via neural networks. Theor. Comput. \nSci. 131, 331–360 (1994). https://doi.org/10.1016/0304-3975(94)90178-3. \n21. Sleator, D., and Temperley. D.,: Parsing english with a link grammar. In Proc. Third Inter-\nnational Workshop on Parsing Technologies, pages 277–292, (1993). \n22. Tesni`ere, L.. El´ements de syntaxe structurale´. Klincksieck, Paris, (1959). \n23. Vepstas, L., Goertzel, B.: Learning Language from a Large (Unannotated) Corpus. (2014).  \narXiv:1401.3372 \n24. https://en.wikipedia.org/wiki/Grammar \n25. Yuret, D.: Discovery of linguistic relations using lexical attraction. PhD thesis, MIT, (1998). \narXiv preprint cmp-lg/9805009. \n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2022-08-27",
  "updated": "2022-08-27"
}