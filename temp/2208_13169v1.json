{
  "id": "http://arxiv.org/abs/2208.13169v1",
  "title": "RUAD: unsupervised anomaly detection in HPC systems",
  "authors": [
    "Martin Molan",
    "Andrea Borghesi",
    "Daniele Cesarini",
    "Luca Benini",
    "Andrea Bartolini"
  ],
  "abstract": "The increasing complexity of modern high-performance computing (HPC) systems\nnecessitates the introduction of automated and data-driven methodologies to\nsupport system administrators' effort toward increasing the system's\navailability. Anomaly detection is an integral part of improving the\navailability as it eases the system administrator's burden and reduces the time\nbetween an anomaly and its resolution. However, current state-of-the-art (SoA)\napproaches to anomaly detection are supervised and semi-supervised, so they\nrequire a human-labelled dataset with anomalies - this is often impractical to\ncollect in production HPC systems. Unsupervised anomaly detection approaches\nbased on clustering, aimed at alleviating the need for accurate anomaly data,\nhave so far shown poor performance.\n  In this work, we overcome these limitations by proposing RUAD, a novel\nRecurrent Unsupervised Anomaly Detection model. RUAD achieves better results\nthan the current semi-supervised and unsupervised SoA approaches. This is\nachieved by considering temporal dependencies in the data and including\nlong-short term memory cells in the model architecture. The proposed approach\nis assessed on a complete ten-month history of a Tier-0 system (Marconi100 from\nCINECA with 980 nodes). RUAD achieves an area under the curve (AUC) of 0.763 in\nsemi-supervised training and an AUC of 0.767 in unsupervised training, which\nimproves upon the SoA approach that achieves an AUC of 0.747 in semi-supervised\ntraining and an AUC of 0.734 in unsupervised training. It also vastly\noutperforms the current SoA unsupervised anomaly detection approach based on\nclustering, achieving the AUC of 0.548.",
  "text": "RUAD: unsupervised anomaly detection\nin HPC systems\nMartin Molana, Andrea Borghesia, Daniele Cesarinib, Luca Beninia,c, Andrea Bartolinia\naDISI and DEI Department, University of Bologna, Bologna, Italy\nbCINECA consorzio interuniversitario, Bologna, Italy\ncInstitut f¨ur Integrierte Systeme, ETH, Z¨urich, Switzerland\nAbstract\nThe increasing complexity of modern high-performance computing (HPC) systems necessitates the introduction of automated and\ndata-driven methodologies to support system administrators’ eﬀort toward increasing the system’s availability. Anomaly detection is\nan integral part of improving the availability as it eases the system administrator’s burden and reduces the time between an anomaly\nand its resolution. However, current state-of-the-art (SoA) approaches to anomaly detection are supervised and semi-supervised, so\nthey require a human-labelled dataset with anomalies – this is often impractical to collect in production HPC systems. Unsupervised\nanomaly detection approaches based on clustering, aimed at alleviating the need for accurate anomaly data, have so far shown poor\nperformance.\nIn this work, we overcome these limitations by proposing RUAD, a novel Recurrent Unsupervised Anomaly Detection model.\nRUAD achieves better results than the current semi-supervised and unsupervised SoA approaches. This is achieved by considering\ntemporal dependencies in the data and including long-short term memory cells in the model architecture. The proposed approach is\nassessed on a complete ten-month history of a Tier-0 system (Marconi100 from CINECA with 980 nodes). RUAD achieves an area\nunder the curve (AUC) of 0.763 in semi-supervised training and an AUC of 0.767 in unsupervised training, which improves upon\nthe SoA approach that achieves an AUC of 0.747 in semi-supervised training and an AUC of 0.734 in unsupervised training. It also\nvastly outperforms the current SoA unsupervised anomaly detection approach based on clustering, achieving the AUC of 0.548.\n1. Introduction\nRecent trends in the development of high-performance com-\nputing (HPC) systems (such as heterogeneous architecture and\nhigher-power integration density) have increased the complex-\nity of their management and maintenance [1]. A typical con-\ntemporary HPC system consists of thousands of interconnected\nnodes; each node usually contains multiple diﬀerent accelera-\ntors such as graphical processors, FPGAs, and tensor cores [2].\nMonitoring the health of all those subsystems is an increasingly\ndaunting task for system administrators. To simplify this mon-\nitoring task and reduce the time between anomaly insurgency\nand response by the administrators, automatic anomaly detec-\ntion systems have been introduced in recent years [3].\nAnomalies that result in downtime or unavailability of the\nsystem are expensive events. Their cost is primarily associated\nwith the time when the HPC system cannot accept new com-\npute jobs. Since HPC systems are costly and have a limited\nservice lifespan [4], it is in the interest of the system’s opera-\ntor to reduce unavailability times. Anomaly detection helps in\nthis regard as it can signiﬁcantly reduce the time between the\nfault and the response by the system administrator, compared\nto manual reporting of faulty nodes [5].\nModern supercomputers are endowed with monitoring sys-\ntems that give the system administrators a holistic view of the\nsystem [3]. Data collected by these monitoring systems and his-\ntorical data describing system availability are the basis for Ma-\nchine Learning anomaly detection approaches [6, 7, 8, 9, 10],\nwhich build data-driven models of the supercomputer and its\ncomputing nodes. In this work, we focus on CINECA Tier0\nHPC system (Marconi100 [11, 12] ranked 9th in Jun. 2020\nTop500 list [13]), which employs a holistic monitoring system\ncalled EXAMON [14].\nProduction HPC systems are reliable machines that gen-\nerally have very few downtime events - for instance, in Mar-\nconi100 at CINECA, timestamps corresponding to faulty events\nrepresent, on average, only 0.035% of all data. However, al-\nthough anomalies are rare events, they still signiﬁcantly impact\nthe system’s overall availability - during the observation pe-\nriod, there was at least one active anomaly (unavailable node)\n14.4% of the time. State-of-the-art (SoA) methods for anomaly\ndetection on HPC systems are based on supervised and semi-\nsupervised approaches from the Deep Learning (DL) ﬁeld [5];\nfor this reason, these methods require a training set with accu-\nrately annotated periods of downtime (or anomalies). In turn,\nthis requires the monitoring infrastructure to track downtime\nevents; in some instances, this can be done with speciﬁc soft-\nware tools (e.g., Nagios [15]), but properly conﬁguring these\ntools\nis\na\ncomplex\nand\ntime-consuming\ntask\nfor\nsystem administrators.\nSo far, the challenges of anomaly detection on HPC systems\nhave been approached by deploying anomaly reporting tools by\ntraining the models in a supervised or semi-supervised fashion\nPreprint submitted to Future Generation Computer Systems\nAugust 30, 2022\narXiv:2208.13169v1  [cs.LG]  28 Aug 2022\n[5, 16, 17, 8]. The need for an accurately labelled training set\nis the main limitation of current approaches as it is expensive,\nin terms of time and eﬀort of the system administrators, to be\napplied in practice. Downtime tracking also has to be able to\nrecord failures with the same granularity as the other monitor-\ning services. Some methods in production HPC systems only\nrecord downtime events by date [1, 2, 3]. In most production\nHPC systems, accurate anomaly detection is thus not readily\nachievable. For this reason, the majority of the methods from\nthe literature were tested on historical or synthetic data or in\nsupercomputers where faults were injected in a carefully con-\ntrolled fashion [18]. Another limitation for the curation of an\naccurately labeled anomaly dataset is the short lifetime of most\nHPC systems. In the HPC sector, a given computing node and\nsystem technology have a lifetime of between three and ﬁve\nyears. Short lifetime means, in practice, that the vendor has no\ntime to create a dataset for training an anomaly detection model\nbefore the system is deployed to the customer site.\nA completely unsupervised anomaly detection approach\ncould be deployed on a new node or even on an entirely new\nHPC system. It would then learn online and without any in-\nteraction with the system administrators. Additionally, such a\nsystem would be easier to deploy as it would require no ad-\nditional framework to report and record anomalous events (in\naddition to the monitoring infrastructure needed to build the\ndata-driven model of the target supercomputer - a type of in-\nfrastructure which is becoming more and more widespread in\ncurrent HPC facilities [3]).\nUnsupervised anomaly detection approaches for HPC sys-\ntems\nexist\nsuch\nas [19, 20, 21]. They either work on log or sensor data. Ap-\nproaches based on log data [19, 21], while useful, can only\noﬀer a post-mortem and restricted view of the supercomputer\nstate. The SoA for anomaly detection on sensor data [20] is\nbased on clustering, which requires a degree of manual anal-\nysis from system administrators and oﬀers poor performance\ncompared to semi-supervised methods. The semi-supervised\nmethods [5, 6, 22], based on the dense autoencoders, which are\ntrained to reproduce their input, could be trained in an unsu-\npervised fashion. However, none of the presented works has\nexplored this possibility. According to the SoA, the models\nwould perform worse as the dense autoencoder is also capable\nof learning the characteristics of the anomalies [5, 6, 22].\nThe primary motivation for this work is to propose a novel\napproach that relies only on the fact that the anomalies are rare\nevents and works at least equally well when trained in an un-\nsupervised manner as it does when trained in semi-supervised\nmanner - this has not been the case in the current SoA. In this\nwork, we propose an unsupervised approach: RUAD (Recur-\nrent Unsupervised Anomaly Detection) that works on sensor\ndata and outperforms all other approaches, including the cur-\nrent SoA semi-supervised approach [5] and SoA unsupervised\napproach [20]. RUAD achieves that by taking into account tem-\nporal dependencies in the data. We achieve that by using Long\nShort-Term Memory (LSTM) cells in the proposed neural net-\nwork model structure, which explicitly take into consideration\nthe temporal dimension of observed phenomena. We also show\nthat the RUAD model, comprising of LSTM layers, is capable\nof learning the characteristic of the normal operation even if the\nanomalous data is present in the test set - the RUAD model is\nthus able to be trained in an unsupervised manner. RUAD tar-\ngets single HPC computing nodes: we have diﬀerent anomaly\ndetection models for each computing node. The motivation be-\nhind this is scalability: in this way, each node can be used to\ntrain its own model with minimal overhead - moreover, this\nstrategy would work in larger supercomputers as well, as if the\nnumber of nodes increases, we just have to add new detection\nmodels.\n1.1. Contributions of the paper\nTo recap, in this paper, we propose an anomaly detection\nframework that can handle complex system monitoring data,\nscale to large-scale HPC systems, and be trained even if no la-\nbelled dataset is available. The key contributions presented in\nthis paper are:\n• We propose a completely unsupervised anomaly detec-\ntion approach (RUAD) that exploits the fact that the anoma-\nlies are rare and explicitly considers the temporal depen-\ndencies in the data by using LSTM cells in an autoen-\ncoder network. The resulting Deep Learning model out-\nperforms the previous state-of-the-art semi-supervised\napproach [5], based on time-unaware autoencoder net-\nworks.\nOn the dataset presented and analysed in this\npaper (collected from the Marconi100 supercomputer),\nthe previous approach achieves an Area-Under-the-Curve\n(ACU) test set score of 0.7470. In contrast, our unsuper-\nvised approach achieves the best test set AUC score of\n0.7672. To the best of our knowledge, this work is the\nﬁrst time such an approach has been applied to the ﬁeld\nof HPC system monitoring and anomaly detection.\n• We have conducted a very large-scale experimental eval-\nuation of our methods. We have trained four diﬀerent\ndeep learning models for each of the 980+ nodes of Mar-\nconi100. To the best of our knowledge, this is the largest\nscale experiment relating to anomaly detection in HPC\nsystems, both in terms of the number of considered nodes\nand length of time.\nPrevious works only evaluate the\nmodels on a subset of nodes with a short observation\ntime ([5] paper, for instance, only analyzed 20 nodes of\nthe HPC system over two months).\nPer-node training\nof models also demonstrates the feasibility of per node\nmodels for large HPC systems. The training time for\nthe individual model was under 30 minutes on a single\nNVIDIA Volta V100 GPU.\n1.2. Structure of the paper\nWe present the current state-of-the-art and position our pa-\nper in Section 2. The machine learning approaches used for\nanomaly detection, including our novel approach, are described\nin section 3. The experimental setting for empirical validation\nof our results is detailed in Section 4.1 and our results are dis-\ncussed in the rest of Section 4. Finally, Section 5 oﬀers some\nconcluding remarks.\n2\n2. Related Works\nThe drive to detect events or instances that deviate from the\nnorm (i.e. operational anomalies) is present across many indus-\ntrial applications. One of the earliest applications of anomaly\ndetection models was credit card fraud detection in the ﬁnancial\nindustry [23, 24]. Recently, anomaly detection (and associated\npredictive maintenance) has become relevant in manufacturing\nindustries [25, 26], internet of things (IoT) [27, 28, 29], energy\nsector [30], medical diagnostics [31, 32], IT security [33], and\neven in complex physics experiments [34].\nTypically, anomalies in an HPC system refer to periods of\n(and leading to) suboptimal operating modes, faults that lead\nto failed or incorrectly completed jobs, or node and other com-\nponents hardware failures. While HPC systems have several\npossible failure mitigation strategies [35] and fault tolerance\nstrategies [36], anomalies of this type still signiﬁcantly reduce\nthe amount of compute time available to users [37]. The transi-\ntion towards Exascale and the increasing heterogeneity of hard-\nware components will only exacerbate the issues stemming from\nfailures, and anomalous conditions that already plague HPC\nmachines [1, 3, 38]. A DARPA study estimates that the fail-\nures in future exascale HPC systems could occur as frequently\nas once every 35-39 minutes [39], thus signiﬁcantly impacting\nthe supercomputing availability and system administrator load.\nHowever, when looking at speciﬁc components and not at\nthe entire HPC system (e.g., considering a single computing\nnode), faults remain very rare events, thus falling under the area\nof anomaly detection, which can be seen as an extreme case of\nsupervised learning on unbalanced classes [40]. Because data\nregarding normal operation far exceeds data regarding anoma-\nlies, classical supervised learning approaches tend to overﬁt the\nnormal data and give a sub-optimal performance on the anoma-\nlous data [41]. In order to mitigate the problem of unbalanced\nclasses, the anomaly detection problem is typically approached\nfrom two angles. Approaches found in the State-of-Art (SoA)\nthat address the class imbalance either modify the data [42]\nor use specialized techniques that work well on anomaly de-\ntection problems [5]. Data manipulation approaches address\nthe dataset imbalance either by decreasing the data belonging\nto normal operation (under sampling the majority class) or by\noversampling or even generating anomalous data (over sam-\npling minority class) [42]. Data manipulation for anomaly de-\ntection in HPC systems has not yet been thoroughly studied.\nConversely, most existing approaches rely on synthetic data\ngeneration, e.g., injection of anomalies in real (non-production)\nsupercomputers or HPC simulators [5].\nAnother research avenue exploits the abundance of normal\ndata from HPC systems using a diﬀerent learning strategy,\n, namely semi-supervised ML models. Instead of learning on\na dataset containing multiple classes – and consequently learn-\ning the characteristics of all classes – semi-supervised models\nare trained only on the normal data. Hence, they are trained\nto learn the characteristics of the of the normal class (the ma-\njority class in the dataset). Anomalies are then recognized as\nanything that does not correspond to the learned characteristic\nof the normal class [40, 6, 43, 22, 44].\nRegarding\nthe\ntype\nof\ndata\nused\nto\ndevelop\nand\ndeploy anomaly detection systems, we can identify two macro-\nclasses: system monitoring data collected by holistic moni-\ntoring systems (i.e.\nExamon [14]) and log data.\nThis data\nis then annotated with information about the system or node-\nlevel availability, thus creating a label associated with the data\npoints. The label encodes whether the system is operating nor-\nmally or experiencing an anomaly. Since it is expensive and\ntime-consuming to obtain labelled system monitoring data, a\nlabelled dataset for supervised learning can be obtained by ”in-\njecting” anomalies into the HPC system (like [18]). Labels are\nimportant for both supervised, semi-supervised and unsuper-\nvised approaches. In the ﬁrst case, they are used to compute\nthe loss, in the second case to identify the training dataset and\nvalidation, and in the third case, only for validation. This data\ncan then be used in a supervised learning task directly or af-\nter processing new features (feature construction). Examples\nof this approach are [45, 17, 46] where authors use supervised\nML approaches to classify the performance variations and job-\nlevel faults in HPC systems. For fault detection, [8, 18] propose\na supervised approach based on Random Forest (an ensemble\nmethod based on decision trees) to classify faults in an HPC\nsystem. All mentioned approaches use synthetic anomalies in-\njected into the HPC system to train a supervised classiﬁcation\nmodel. Approaches [5] and [16] are among the few that lever-\nage real anomalies collected from production HPC systems (as\nopposed to injected anomalies). In this paper, we are interested\nin real anomalies, and thus, we will not include methods using\nsynthetic/simulated data or injected anomalies in our quantita-\ntive comparisons.\nAll mentioned approaches do not take into account tempo-\nral dependencies of data (models are not trained on time se-\nries but on tabular data containing no temporal information).\nSystem monitoring data approach [47] is the ﬁrst to take into\naccount temporal dependencies in data by calculating statisti-\ncal features on temporal dimension (aggregation, sliding win-\ndow statistics, lag features). Most approaches that deal with\ntime series anomaly detection do so on system log data. La-\nbelled anomalies are either analyzed with log parsers [48] or\ndetected with deep learning methods. Deep learning methods\nfor anomaly detection are based on LSTM neural networks as\nthey are a proven approach in other text processing ﬁelds.\nCompared to labelled training sets, much less work has been\ndone on unlabelled datasets - despite this case being much more\ncommon in practice. So far, all research on unlabelled datasets\nhas focused on system log data. [19] propose a k-means based\nunsupervised learning approach that does not take into account\ntemporal dynamics of the log data. A clustering-based approach\non sensor data is proposed by [20]. This approach will serve\nas one of the baselines in the experimental section (as it is the\nonly unsupervised approach on the sensor and not on log data).\nAn approach [21] works on time series data in an unsupervised\nmanner. It uses the LSTM-based autoencoder and is trained on\nthe existing log data dataset. The proposed anomaly detector\nachieves the AUC (area under the receiver-operator character-\nistic curve) of 0.59. Although it works on a drastically diﬀerent\ntype of dataset (log data as opposed to system monitoring data),\n3\nit is the closest existing work to the scope of the research pre-\nsented in this paper. As we show later in the paper, we can\nachieve much better results than the one reported for the log\ndata models [21] by deploying an unsupervised anomaly detec-\ntion approach on system monitoring data on a per-node basis.\nTable 1 summarizes the most relevant approaches described in\nthis section, focusing on the training set and temporal depen-\ndencies.\nTabular data\nTime series\nSupervised\n[49, 9]\n[47, 48, 10]\nSemi-supervised\n[5, 6, 43, 22]\nUnsupervised\n[19, 20]\n[21]\nTable 1: Summary of anomaly detection approaches on HPC systems\nThe novelty of this paper is, in relation to the existing works,\nthreefold:\n• it introduces an unsupervised time-series based anomaly\ndetection model named RUAD;\n• it proposes a deep learning architecture that captures time\ndependency;\n• the approach is evaluated on a large scale production\ndataset with real anomalies – this is the largest scale eval-\nuation ever conducted on this kind of problem, to the best\nof our knowledge.\n3. Methodology\nIn this section, we describe the proposed approach for un-\nsupervised anomaly detection. We do not directly introduce\nthe proposed method (the LSTM autoencoder deep network)\nas we want to show how it is a signiﬁcant extension to the cur-\nrent state-of-the-art; thus, we start by introducing three baseline\nmethods, i) exponential smoothing (serving as the most basic\nmethod for comparison), ii) unsupervised clustering and iii) the\ndense autoencoder used in [5]. We then describe our approach\nin detail and highlight its key strengths (the unsupervised train-\ning regime and the explicit inclusion of the temporal dimen-\nsion).\n3.1. Node anomaly labeling\nWe aim to recognize the severe malfunctioning of a node\nthat prevents it from executing regular compute jobs. This mal-\nfunctioning does not necessarily coincide with removing a node\nfor the production, as reported by Nagios. In our discussions\nwith system administrators of CINECA, we have concluded\nthat the best proxy for node availability is the most critical state,\nas reported by Nagios. For this reason, we have created a new\nlabel called node anomaly that has a value 1 if any subsystem\nreported by Nagios reports a critical state. From these events\n(reported anomalies), we then ﬁlter out known false positive\nevents based on reporting tests or conﬁgurations in Jira [50].\nJira logs are supplied by CINECA. The labels used in our pre-\nvious work [5] do not apply to M100 as they were extensively\nused to denote nodes being removed from production for test-\ning and calibration. In this work, we are examining the early\nperiod of the HPC machine life-cycle, when several rounds of\nre-conﬁguration were performed, thus partially disrupting the\nnormal production ﬂow of the system. Comparing the two la-\nbelling strategies in table 2, we can see that the overlap between\nthe two is minimal. Additionally, there are far fewer anomalies\nas reported by the node anomaly mainly because the M100 went\nthrough substantial testing periods in the ﬁrst ten months of op-\neration where nodes are marked as removed from production\nwhile still functioning normally. In the remainder of the paper,\nclass 0 or class 1 will always refer to the value of node anomaly\nbeing 0 or 1 respectively. Normal data is all data where node\nanomaly has value 0 and anomalies are instances where node\nanomaly has value 1.\nNode anomaly\n0\n1\nRemoved from production: False\n12 139 560\n4 280\nRemoved form production: True\n15 783\n12\nTable 2: Comparison between removed from production and node availability.\nThe anomalies studied in this work (node availability) signiﬁcantly diﬀer (and\nare more reliable) from anomalies studied in previous works. The new labels\nalso mark much fewer events as anomalous.\n3.2. Reconstruction error and result evaluation\nThe problem of anomaly detection can be formally stated as\na problem of training the model M that estimates the probabil-\nity P that a sequence of vectors of length W ending at time t0\nrepresents an anomaly at time t0:\nM : ⃗xt0−W+1, · · · , ⃗xt0 →P(⃗xt0 is an anomaly).\n(1)\nVector ⃗xt collects all feature values at time t; the features are\nthe sensor measurements collected from the computing nodes.\nW is the size of the past window that the model M takes as in-\nput. If the model does not take past values into account - like\nthe dense model implemented as a baseline [5] - and the win-\ndow size W is 1, the problem can be simpliﬁed as estimating:\nM : ⃗xt0 →P(⃗xt0 is an anomaly).\n(2)\nIn the case of autoencoders, model M is composed of two\nparts: autoencoder A (a neural network) and the anomaly score,\nwhich is computed using the reconstruction error of the autoen-\ncoder. The reconstruction error is calculated by comparing the\noutput of autoencoder model A and the real value vector ⃗xt0.\nThe task of model A is to reconstruct the last element of its\ninput sequence:\nA : ⃗xt0−W+1, · · · , ⃗xt0 →ˆ⃗xt0.\n(3)\nVector ˆ⃗xt0 the reconstruction of vector ⃗xt0. As in Eq. 2, win-\ndow size W can be 1. The model A outputs normalized data .\nThe reconstruction error is calculated as the sum of the absolute\n4\ndiﬀerence between the output of model A and the normalized\ninput value for each feature: Error(t0) = PN\ni |ˆxi −xi| where N\nis the number of features and ˆ⃗xt0 is the output of the model A.\nThe error is then normalized by dividing it by the maximum er-\nror on the training set: Normalized error(t0) =\nError(t0)\nmax(Error(t)). We\nestimate the probability for class 1 (anomaly) as\nP(⃗xt0 is an anomaly) =\n\n1, if : Normalized error ≥1,\nNormalized error, otherwise\n(4)\nBased on probability P(⃗xt0isananomaly), the classiﬁer makes\nthe prediction whether the sequence ⃗xt0−W, · · · , ⃗xt0 belongs to\nclass 1 (anomaly) of class 0 (normal operation). This predic-\ntion depends on a threshold T, which is a tunable parameter:\nClass(⃗xt0) =\n\n1, if : P(⃗xt0 is an anomaly) ≥T,\n0, otherwise\n(5)\nTo avoid selecting a speciﬁc threshold T, we introduce the\nReceiver-Operator Characteristic curve (ROC curve) as a per-\nformance metric. It allows us to evaluate the performance of\nthe classiﬁcation approach for all possible decision thresholds\n[51]. The receiver-operator characteristic curve plots the true-\npositive rate in relation to the false-positive rate. The random\ndecision represents a linear relationship between the two – for\na classiﬁer to make sense, the ROC curve needs to be above\nthe diagonal line. For each speciﬁc point on the curve, the bet-\nter classiﬁer is the one whose ROC curve is above the other.\nThe overall performance of the classiﬁer can be quantitatively\ncomputed as the Area Under the ROC Curve (AUC); a classi-\nﬁer making random decisions has the AUC equal to 0.5. AUC\nscores below 0.5 designate classiﬁers that are worse than ran-\ndom choice. The best possible AUC score is 1, which is achieved\nby a classiﬁer that would achieve a true-positive rate equal to 1\nwhile having a false-positive rate equal to 0 (broadly speaking,\nthis is only achievable on trivial datasets or very simple learning\ntasks).\n3.3. Trivial baseline: exponential smoothing\nExponential smoothing is implemented as a trivial baseline\ncomparison. It is a simple and computationally inexpensive\nmethod that detects rapid changes (jumps) in values. If the\nanomalies were simply rapid changes in values with no correla-\ntion between features, a simple exponential smoothing method\nwould be able to discriminate them. Therefore, we chose ex-\nponential smoothing as a ﬁrst baseline as it is computationally\ninexpensive and requires no training set. Additionally, if ex-\nponential smoothing performs poorly, this underlines that we\nare indeed solving a non-trivial anomaly detection problem, for\nwhich more powerful models are needed.\nFor the baseline, we choose to implement exponential smooth-\ning per feature independently. Exponential smoothing for fea-\nture i at time t is calculated as:\nˆxi = αxi\nt + (1 −α)ˆxi\nt−1, ∀i ∈F\n(6)\nwhere ˆxi\nt is an estimate of xi at time t and α is a parameter\nof the method. We do this for all features in set F. The estimate\nat the beginning of the observation is equal to the actual value\nat time t0: ˆxi\nt0 = xi\nt0.\n3.4. Unsupervised baseline: clustering\nA possible approach to unsupervised anomaly detection is\nto use standard unsupervised machine learning techniques such\nas k-means clustering proposed by [20]. The clusters are deter-\nmined on the train set; each new instance belonging to the test\nset is associated with one of the pre-trained clusters. We opted\nfor this particular unsupervised technique for the comparison as\nit is the only unsupervised method found in the literature (to the\nbest of our knowledge) which uses sensor data and not logs -\nand thus, we guarantee a fair comparison. It has to be noted,\nhowever, that clustering, while belonging to the ﬁeld of unsu-\npervised machine learning cannot detect anomalies in an un-\nsupervised manner - for each of the clusters determined on the\ntrain set, the probability for the anomaly has to be calculated.\nThis probability can only be calculated using the labels.\nIn this work, the clustering approach inspired by [20] is im-\nplemented to prove the validity of the obtained results. We have\nused K-means clustering [19] like it has been proposed in [20].\nWe have trained the clusters on the train set. Based on the sil-\nhouette score1 on the train set, we have determined the optimal\nnumber of clusters for each node2. The percentage of instances\nthat belong to class 1 is calculated for each of the determined\nclusters. We use this percentage of anomalous instances as the\nanomaly probability for each instance assigned to a speciﬁc\ncluster. The train and test set split is the same as in all other\nevaluated methods.\n3.5. Semi-supervised baseline: dense autoencoder\nThe competitive baseline method is based on the current\nstate-of-the-art dense autoencoder model proposed by [5]. Au-\ntoencoders are types of neural networks (NN) trained to repro-\nduce their input. The network is split into two (most often sym-\nmetric) parts: encoder and decoder. The role of the encoder is to\ncompress the input into a more condensed representation. This\nrepresentation is called the latent layer. To prevent the network\nfrom learning a simple identity function, we choose the latent\nlayer to be smaller than the original input size (number of in-\nput features) [6]. The role of the decoder is to reconstruct the\noriginal input using the latent representation.\nDense autoencoders are a common choice for anomaly de-\ntection since we can restrict their expressive power by acting\non the size of the latent layer. Compressing the latent dimen-\nsion forces the encoder to extract the most salient character-\nistics from the input data; unless the input data is highly re-\ndundant, the autoencoder cannot correctly learn to recreate its\n1the Silhouette score is a measure of performance for a clustering method.\nIt measures how similar an instance is to others in its own cluster compared to\ninstances from the other clusters [52]. It is calculated as S score =\nb−a\nmax(a,b) where\na is the mean inter-cluster distance, and b is the mean nearest cluster distance\nfor each sample.\n2Optimal number of clusters is the number of clusters that produces the\nhighest silhouette score on the train set.\n5\ninput after a certain latent size reduction. In the current state-\nof-the-art for anomaly detection in production supercomputers\n([5]) the dense autoencoder is used in a semi-supervised fash-\nion, meaning that the network is trained using only data points\ncorresponding to the normal operation of the supercomputer\nnodes (Class 0). Semi-supervised training is doable as the nor-\nmal points are the vast majority and thus are readily available;\nhowever, this requires having labelled data or at least a certainty\nthat the HPC system was operating in normal conditions for a\nsuﬃciently long period of time. Once the autoencoder has been\ntrained using only normal data, it will be able to recognize sim-\nilar but previously unseen points. Conversely, it will struggle to\nreconstruct new points which do not follow the learned normal\nbehaviour, that is, the anomalies we are looking for; hence, the\nreconstruction error will be higher. The structure of the autoen-\ncoder model is presented in Figure 1a. The dense autoencoder\ndoes not take into account the temporal dynamics of the data –\nits input and target output are the same vector:\nS oA : ⃗xt0 →⃗xt0.\n(7)\n3.6. Recurrent unsupervised anomaly detection: RUAD\nMoving beyond the state-of-the-art model, we propose a\ndiﬀerent approach, RUAD. It takes as input a sequence of vec-\ntors and then tries to reconstruct only the last vector in the se-\nquence:\nRUAD : ⃗xt0−W+1, · · · , ⃗xt0 →⃗xt0.\n(8)\nThe input sequence length is a tunable parameter that spec-\niﬁes the size of the observation window W. The idea of the pro-\nposed approach is similar to the dense autoencoder in principle,\nbut with a couple of signiﬁcant extensions: 1) we are encoding\nan input sequence into a more eﬃcient representation (latent\nlayer) and 2) we train the autoencoder in an unsupervised fash-\nion (thus removing the requirement of labelled data). The key\ninsight in the ﬁrst innovation is that while the data describing\nsupercomputing nodes is composed of multi-variate time series,\nthe state-of-the-art does not explicitly consider the temporal di-\nmension – the dense autoencoder has no notion of time nor of\nsequence of data points. To overcome this limitation, our ap-\nproach works by encoding the sequence of values leading up to\nthe anomaly. The encoder network is composed of Long Short-\nTerm Memory (LSTM) layers, which have been often proved\nto be well suited to the context where the temporal dimension\nis relevant [53]. An LSTM layer consists of recurrent cells that\nhave an input from the previous timestamp and from the long-\nterm memory.\nTo address the scale of current pre-exascale and future ex-\nascale HPC systems that will consist of thousands of nodes\n[3], we want a scalable anomaly detection approach. The most\nscalable approach currently for anomaly detection on a whole\nsupercomputer is a node-speciﬁc approach as each compute\nnode can train its own model. Still, we want to achieve this by\nminimally impacting the regular operation of the HPC system.\nThis is why it is important for the proposed solution to have a\nsmall overhead. Additionally, since we want to train a per-node\nmodel, we want the method to be data-eﬃcient. To address\nthese requirements, we choose not to make the decoder sym-\nmetric to the encoder. The proposed approach is thus comprised\nof a Dense decoder and an LSTM encoder. LSTM encoder out-\nput is passed into a dense decoder trained by reproducing the\nﬁnal vector in an input sequence. The decoder network is thus\ncomposed of fully connected dense layers. The architecture of\nthe proposed approach is compared to the state-of-the-art ap-\nproach in Figure 1.\nThe reduced complexity of training allows us to train a sepa-\nrate model for each compute node. As shown previously ([54]),\nnode-speciﬁc models provide better results than a single model\ntrained on all data. We decided to adopt this scheme (one model\nper node) after a preliminary empirical analysis showed no sig-\nniﬁcant accuracy loss while the training time was vastly re-\nduced (by approximately 50%); this is very important in our\ncase as we trained one DL model for each of the nodes of Mar-\nconi 100 (980+), deﬁnitely a non-negligible computational ef-\nfort.\n3.7. Data pre-processing\nAs introduced in Section 3.6 our proposed methodology\nconsists of training a model for each node. Thus, the data from\neach node is ﬁrst split into training and test sets. The training\nset contains 80% of data, and the test set contains the last 20%\nof data (roughly the last two months of data). It is important to\nstress that we have chosen to have two not overlapping datasets\nfor the training and test. This avoids the cross-transferring of in-\nformation when dealing with sequencing. Moreover, the causal-\nity of the testing is preserved. (No in-the-future data are used\nto train a model). This makes the results valid for in-practice\nusage.\nFor semi-supervised training, the training set is ﬁltered by\nremoving anomalous events (anomalous events are identiﬁed by\nthe node anomaly label as described in Section 3.1). We name\nthis ﬁlter the semi-supervised ﬁlter, as depicted in Figure 2. For\nunsupervised learning, the training set is not ﬁltered. For both\nthe cases (unsupervised and semi-supervised learning), labels\nare used to evaluate the results. After ﬁltering, a scaler is ﬁtted\nto training data. A scaler is a transformer that scales the data\nto the [0, 1] interval. In the experimental part, a min/max scaler\nis used on each feature [55]. After ﬁtting to the training data,\nthe scaler is applied to the test data - for rescaling the test set,\nmin and max values of the training set are used (as it is stan-\ndard practice in DL methods). After scaling, both training and\ntest sets are ﬁltered out to ensure time consistency: the data is\nsplit into sequences without missing chunks (missing chunks\nare the result of the semi-supervised ﬁlter). The sequences that\nare smaller than W are dropped. Finally, sequences are trans-\nformed into batches of sequences with length W. Figure 2 de-\nscribes the whole data pre-processing pipeline.\n3.8. Summary of evaluated methods\nWe compare our proposed approach RUAD against estab-\nlished semi-supervised and unsupervised baselines. Summary\nof pre-processing ﬁlters is presented in Table 3.\nThe semi-\nsupervised ﬁlter is applied to all semi-supervised approaches. A\n6\nAnomaly\nprobability\nData\nInput\n(*, I)\nDense\n(*, E1)\nDense\n(*, D1)\nRecon-\nstru-\nction\nerror\nDense\n(*, I)\nDense\n(*, L)\nLayer\nsize\nData flow\nEncoder\nDecoder\n(a) Structure of baseline model - the dense autoencoder.\nAnomaly\nprobability\nData\nDense\n(*, E1)\nDense\n(*, D1)\nDense\n(*, I)\nLayer\nsize\nData flow\nLSTM\n(*,W,E1)\nInput\n(*,W,I)\nLSTM\n(*,W,L)\nEncoder\nDecoder\nRecurrent dimension\nRecon-\nstru-\nction\nerror\n3d\nvector\n3d\nvector\n3d\nvector\n(b) Structure of the proposed RUAD model consisting of the LSTM encoder and dense decoder.\nFigure 1: The proposed approach replaces the encoder of the baseline model (1a) with the LSTM autoencoder (1b). The last layer of LSTM encoder returns a vector\n(not a temporal sequence) which is then passed to the fully connected decoder. W is the window size, I is the size of the input data, L is the size of the latent layer\nand E1 and D1 are sizes of encoder and decoder layer respectively. Chosen parameters for L, W, E1 and D1 are listed in Section 4.3.\ntime consistency ﬁlter is applied to methods that explicitly con-\nsider the temporal dimension of the data: Exponential smooth-\ning and RUAD. RUAD and the current SoA anomaly detection\napproach based on dense autoencoders ([5]) is evaluated in both\nsemi-supervised and unsupervised version.\nFilters\nModel\nSemi-supervised\nTime consistency\nName\nTrivial baseline: exponential smoothing\nNO\nYES\nEXP\nUnsupervised baseline: clustering\nNO\nNO\nCLU\nDENSE autoencoder baseline semi-supervised\nYES\nNO\nDENS Esemi\nDENSE autoencoder baseline unsupervised\nNO\nNO\nDENS Eun\nRUAD semi-supervised\nYES\nYES\nRUADsemi\nRUAD unsupervised\nNO\nYES\nRUAD\nTable 3:\nShort names and training strategies for examined methods.\nDENS Esemi is the current SoA [5].\nWe wish to highlight that, unlike the unsupervised learn-\ning baseline [20], our proposed method RUAD requires no ad-\nditional action after the training of the model. The approach\nRUAD, proposed in this work, works on an unlabeled dataset\nand requires no additional post training analysis. A summary\nof approaches relating to training set requirements is presented\nin Table 4.\nMethod\nTraining set required\nPost-training\nEXP\nUnlabeled dataset\nNo action required\nCLU [20]\nUnlabeled dataset\nAssigning anomaly probability to clusters\nDENS Esemi [5]\nLabeled dataset\nNo action required\nRUAD\nUnlabeled dataset\nNo action required\nTable 4: Caparison of implemented approaches relating to the training set re-\nquirements.\n4. Experimental results\n4.1. Experimental setting\nThe focus of the experimental part of this work is Marconi\n100 (M100) HPC system, located in the CINECA supercom-\nputing centre. It is a tier-0 HPC system that consists of 980\ncompute nodes organized into three rows of racks. Each com-\npute node has 32 core CPU, 256 GB of RAM and 4 Nvidia\nV100 GPUs. In this work, nodes of the HPC system will be\nconsidered independent. This is also in line with the current\nSoA works [18, 6, 5] where anomaly detection is performed per\nnode. Future works will investigate inter-node dependencies in\nthe anomaly detection task.\n7\nTraining set\nTrain scaling\nTesting set\nData\nApply scaling\nApply scaling\nSemi-supervised \nfilter\nTime consistency \nfilter\nEvaluation\nTraining\nFigure 2: Data processing schema. Data ﬂow is represented by green (training set) and orange (testing set). Scaling is trained on training set and applied on testing\nset to avoid contaminating the testing set. Semi-supervised and time consistency ﬁlters are optional and applied only when required by the modeling approach as\nindicated in Table 3\nThe monitoring system in an HPC setting typically con-\nsists of hardware monitoring sensors, job status and availabil-\nity monitoring, and server room sensors. In the case of M100,\nhardware monitoring is performed by Examon[14], and system\navailability is provided by system administrators[15]. This raw\ninformation provided by Nagios, however, contains many false-\npositive anomalies. For this reason, we have constructed a new\nanomaly label called node anomaly described in Section 3.1.\nFor each of the 980 nodes of M100, a separate dataset was\ncreated. Dataset details are explained in Section 4.2. DENS E\nand RUAD models were trained and evaluated on the node-\nspeciﬁc training and test sets for each node. The training set\nconsisted of the ﬁrst eight months of system operation, and the\ntest set comprised the remaining two months. Such testing split\nensures a fair evaluation of the model as described in Section\n4.2. For the baseline, the exponential smoothing operation (de-\nﬁned in equation (6)) was applied only over the test set (as the\napproach requires no training). For each node, the scaler (for\nmin and max scaling) was trained on training data and applied\nto test data. All results discussed in this section are combined\nresults from all 980 nodes of M100.\nThe dense autoencoder and the RUAD model were trained\nin two diﬀerent regimes: semi-supervised and unsupervised.\nFor the semi-supervised training, the semi-supervised ﬁlter was\napplied that removed all data points corresponding to anoma-\nlies. In the unsupervised case, no such ﬁltering was performed.\nIt can hence be noticed one of the key advantages of the un-\nsupervised approach: no data pre-processing needs to be done\nand no preliminary knowledge about the computing nodes con-\ndition is required.\nFor all three approaches (exponential smoothing, dense au-\ntoencoder and the RUAD), the probability for an anomaly (class\n1) was estimated from reconstruction error as explained in Sec-\ntion 3.2. The probabilities from the test sets of all nodes from a\nsingle modelling approach (e.g. RUAD with observation win-\ndow of length W = 40) were collected together to plot the\nReceiver Operator Characteristic (ROC) curve that is a char-\nacteristic for the modelling approach across all nodes. For clus-\ntering baseline and exponential smoothing (worst performing\nbaselines), the ROC curve is compared against a dummy clas-\nsiﬁer which randomly chooses the class.\n4.2. Dataset\nThe dataset used in this work consists of a combination of\ninformation recorded by Nagios (the system administration tool\nused to visually check the health status of the computing nodes)\nand the Examon monitoring systems; the data encompasses the\nSource\nFeatures\nHardware monitoring\nambient temp., dimm[0-15] temp.,\nfan[0-7] speed, fan disk power,\nGPU[0-3] core temp. ,\nGPU[0-3] mem temp. ,\ngv100card[0-3], core[0-3] temp. ,\np[0-1] io power,\np[0-1] mem power,\np[0-1] power, p[0-1] vdd temp. ,\npart max used,\nps[0-1] input power,\nps[0-1] input voltage,\nps[0-1] output current,\nps[0-1] output voltage, total power\nSystem monitoring\nCPU system, bytes out, CPU idle,\nproc. run, mem. total,\npkts. out, bytes in, boot time,\nCPU steal, mem. cached, stamp,\nCPU speed, mem. free, CPU num.,\nswap total, CPU user, proc. total,\npkts. in, mem. buﬀers, CPU idle,\nCPU nice, mem. shared, PCIe,\nCPU wio, swap free\nTable 5: An anomaly detection model is created only on hardware and applica-\ntion monitoring features. More granular information regarding individual jobs\nis not collected to ensure the privacy of the HPC system users.\nﬁrst ten months of operation of the M100 system. The proce-\ndure for obtaining a node anomaly label is described in Section\n3.1. The features collected in the dataset are listed in table 5.\nThe data covers 980 compute nodes and ﬁve login nodes. Lo-\ngin nodes have the same hardware as the compute nodes but are\nreserved primarily for job submission and accounting. Thus we\nremoved them from our analysis. The data is collected by the\nUniversity of Bologna with approval from CINECA3.\nIn order to align diﬀerent sampling rates of diﬀerent re-\nporting services (each of the sensors used has a diﬀerent sam-\npling frequency), 15 minute aggregates of data points were cre-\nated. 15 minute interval was chosen as it is the native sampling\nfrequency of the Nagios monitoring service (where our labels\ncome from). Four values were calculated for each 15 minute\nperiod and each feature: minimum, maximum, average, and\nvariance.\n3CINECA is a public university consortium and the main supercomputing\ncentre in Italy[56].\n8\n4.3. Hyperparameters\nHyper-parameters for all methods discussed in this paper\nwere determined based on initial exploration on the set of 50\nnodes. Chosen parameters performed best on the test from the\ninitial exploration nodes (they achieved the highest AUC score\non the test set). Results from the initial exploration set are ex-\ncluded from the results discussed further in the chapter. Tuned\nhyperparameters include the structure of the neural nets (num-\nber and size of layers) and the smoothing factor of the exponen-\ntial smoothing:\n• Exponential smoothing: smoothing factor α = 0.1\n• Clustering: hyper-parameter (number of clusters) is trained\non a train set for each node independently.\n• Dense autoencoder: Structure of the network consists of\n5 layers of shapes: (*,462), (*,16), (*,8), (*16), (*462).\n• RUAD (LSTM encoder, dense decoder): Structure of the\nnetwork consists of 5 layers of shapes: (*,W,462), (*,W,16),\n(*,W,8), (*,16), (*,462). W is the length of the observa-\ntion window. Chosen window lengths W were: 5, 10, 20, 40.\n4.4. Exponential smoothing\nAs mentioned in the methodology, exponential smoothing\n(EXP) is implemented to demonstrate that the anomalies we\nobserve are not simply unexpected spikes in the data signal.\nFurthermore, exponential smoothing is applied to each feature\nindependently of other features. As shown in Figure 3, expo-\nnential smoothing performs even worse than a dummy classiﬁer\n(random choice). Poor performance of exponential smoothing\nshows that the anomalies we are searching for are more com-\nplex than simple jumps in values for a feature.\nFigure 3: Combined ROC curve from all 980 nodes of M100 for the expo-\nnential smoothing baseline. Exponential smoothing performs even worse than\nthe dummy classiﬁer - anomaly detection based on exponential smoothing is\ncompletely unusable.\n4.5. Clustering\nThe simple clustering baseline performs better than the ex-\nponential smoothing baseline and better than the dummy classi-\nﬁer, as seen in Figure 4. However, as we will illustrate in the fol-\nlowing sections, it performs worse than any other autoencoder\nmethod. This demonstrates that the problem we are addressing\n(anomaly detection on an HPC system) requires more advanced\nmethodologies like semi-supervised and unsupervised autoen-\ncoders.\nFigure 4: Combined ROC curve from all 980 nodes of M100 for the simple\nclustering baseline. This baseline performs only marginally better than the\ndummy classiﬁer.\n4.6. Dense autoencoder\nFigure 5: Combined ROC curve from all 980 nodes of M100 for the Dense au-\ntoencoder model. In the area interesting for practical application - True Positive\nRate between 0.6 and 0.9 - semi-supervised approach outperforms unsupervised\napproach.\nWe consider now the dense autoencoder. We train a dif-\nferent network for each computing node of Marconi 100. The\noptimal network topology was determined during a prelimi-\nnary exploration done on the sub-sample of the nodes of the\nsystem and following the guidelines provided by Borghesi et\n9\nal.[54]. In line with the existing work[5], the semi-supervised\nlearning approach DENS Esemi slightly outperforms the unsu-\npervised learning approach DENS Eun as seen in Figure 5. The\nbetter performance in the semi-supervised case is due to the\nnature of the autoencoder learning model - its capability to re-\nconstruct its input. For example, suppose the autoencoder is fed\nwith anomalous input during the training phase, as in the unsu-\npervised case. In that case, anomalous examples in the training\ndata constitute a type of “noise” that renders the autoencoder\npartially capable of reconstructing the anomalous examples in\nthe test set.\n4.7. RUAD\nThis section examines the experimental results obtained with\nthe RUAD model (unsupervised LSTM autoencoder). The most\nimportant parameter is the length of the input sequence W that\nis passed to the model. This parameter encodes our expectation\nof the length of the dependencies within the data. Since each\ndata point represents 15 minutes of node operation, the actual\nperiod we observe consists of W × 15 min. In this set of ex-\nperiments, we selected the following time window sizes: 5 (75\nminutes), 10 (2h30), 20 (5h), 40 (10h). These period lengths\nwere obtained after a preliminary empirical evaluation; more-\nover, these time frames are in line with the typical duration of\nHPC workloads, which tend to span between dozens of min-\nutes to a few hours[57]. We have trained the model in both\nsemi-supervised RUADsemi and unsupervised RUAD fashion\nfor each selected window length. Results across all the nodes\nare collected in Figure 6.\n4.8. Comparison of all approaches\nThe main metric for evaluating model performance is the\narea under the ROC curve (AUC). This metric estimates the\nclassiﬁers’ overall performance without the need to set a dis-\ncrimination threshold [51]. The closer the AUC value is to 1,\nthe better the classiﬁer performs. AUC scores for implemented\nmethods are collected in table 6. From the lower table in table\n6 (rows correspond to diﬀerent training regimes and columns\nto window size for RUAD network) and upper table in 6 (rows\ncorrespond to the performance of diﬀerent implemented base-\nlines), we see that the proposed approach outperforms the exist-\ning baselines. The highest AUC achieved by the previous base-\nlines is 0.7470 (achieved by the DENS Esemi. This is outper-\nformed by RUAD for all window sizes. The best performance\nof RUAD is achieved by selecting the windows size 10 where it\nachieves an AUC of 7.672. This result clearly shows that some\ntemporal dynamics contribute to the appearance of anomalies.\nThe ﬁnal consideration is the impact of observation window\nlength W on the performance of the RUAD model. One might\nexpect that considering longer time sequences would bring ben-\neﬁts, as more information is provided to the model to recreate\nthe time series. This is, however, not the case (as seen in ta-\nble 6) as the RUAD achieves the best performance of 0.7672\nwith window size 10. The performance then reduces sharply\nwith window size 40, only achieving an AUC of 0.7473. Sev-\neral factors might explain this phenomenon. For instance, in\ntens of hours, the workload on a given node might change dras-\ntically.\nConsidering longer time series might thus force the\nRUAD model to concentrate on multiple workloads, hinder-\ning its learning task. Finally, an issue stems from the fact that\nthere are gaps (periods of missing measurements) in the col-\nlected data (a very likely problem in many real-world scenar-\nios). Longer sequences mean that more data has to be cut from\nthe training set to ensure time-consistent sequences; this is be-\ncause we are not applying gap-ﬁlling techniques at the mo-\nment4, thus, sub-sequences missing some points need to be re-\nmoved from the data set. Combining these two factors con-\ntributes to the model’s decline in performance with longer ob-\nservation periods.\nConsidering all discussed factors, the optimal approach is to\nuse the proposed model architecture with window size W = 10\n(i.e. 2 hours and 30 minutes), trained in an unsupervised man-\nner. This conﬁguration outperforms semi-supervised RUADsemi\nas well as the dense autoencoder. As mentioned in the related\nwork (Section 2), labelled datasets are expensive to obtain in\nthe HPC setting. Good unsupervised performance is why this\nresult is promising - it shows us that if the anomalies repre-\nsented a small fraction of all data, we could train an anomaly\ndetection model even on an unlabeled dataset (in an unsuper-\nvised manner). Such a model not only achieves the state-of-the-\nart performance but outperforms semi-supervised approaches.\nThe best AUC, achieved by the previous SoA DENS Esemi, is\n0.7470. The best AUC score achieved by RUAD is 0.7672.\nMoreover, unsupervised training makes this anomaly detection\nmodel more applicable to a typical HPC (or even datacentre)\nsystem.\nMethod\nCombined ROC score\nEXP\n0.4276\nCLU\n0.5478\nDENS Esemi\n0.7470\nDENS Eun\n0.7344\nMethod\nCombined ROC score\nSequence length\n5\n10\n20\n40\nRUADsemi\n0.7632\n0.7582\n0.7602\n0.7446\nRUAD\n0.7651\n0.7672\n0.7655\n0.7473\nTable 6: According to expectations, the semi-supervised dense autoencoder\noutperforms the unsupervised dense one (highlighted by the higher AUC score).\nRUAD and RUADsemi outperform all previous baselines. In contrast to the\ndense autoencoders, the proposed approach RUAD performs best in unsuper-\nvised manner.\n5. Conclusions\nThe paper presents an anomaly detection approach for HPC\nsystems (RUAD) that outperforms the current state-of-the-art\napproach based on the dense autoencoders [5]. Improving upon\n4We decided not to consider such techniques for the moment, as we wanted\nto focus on the modelling approach and gap-ﬁlling methods tend to require\nadditional assumptions and to introduce noise in the data.\n10\n(a) Window length 5\n(b) Window length 10\n(c) Window length 20\n(d) Window length 40\nFigure 6: Combined results from all 980 nodes of M100. Comparison of diﬀerent window lengths for the RUAD model. For all window lengths, performances\nof semi-supervised and unsupervised approaches are similar. Performance of the proposed model (red and blue line) is compared to the state-of-the-art baseline\nsemi-supervised autoencoder proposed by Borghesi et al.[5].\nstate-of-the-art is achieved by deploying a neural network ar-\nchitecture that considers the temporal dependencies within the\ndata.\nThe proposed model architecture achieves the highest\nAUC of 0.77 compared to 0.75, which is the highest AUC\nachieved by the dense autoencoders (on our dataset).\nAnother\ncontribution\nof\nthis\npaper\nis\nthat\nthe\nproposed method – unlike the previous work [5, 16, 17, 8] –\nachieves the best results in an unsupervised training case. Un-\nsupervised training is instrumental as it oﬀers a possibility of\ndeploying an anomaly detection model to the cases where (ac-\ncurately) labelled dataset is unavailable. The only stipulation\nfor the deployment of unsupervised anomaly detection models\nis that the anomalies are rare – in our work, the anomalies ac-\ncounted for only 0.035% of the data. The necessity to have a\nfew anomalies in the training set, however, is not a signiﬁcant\nlimitation as HPC systems are already highly reliable machines\nwith low anomaly rates [58, 1].\nTo illustrate the capabilities of the approach proposed in this\nwork, we have collected an extensive and accurately labelled\ndataset describing the ﬁrst 10 months of operation of the Mar-\nconi100 system in CINECA [56]. The creation of accurately\nlabelled dataset was necessary to compare the performance of\ndiﬀerent models on the data rigorously. Because of the high\nquality and large scale of the available dataset, we can con-\nclude that for the model proposed in the paper, the unsuper-\nvised model outperforms semi-supervised model even if accu-\nrate anomaly labels are available. This is the ﬁrst experiment\nof this type and magnitude conducted on a real, in-production\ndatacentre (both in terms of the number of computing nodes\nconsidered and the length of the observation period).\nIn future works, we will further explore the problem of\nanomaly detection in HPC systems, in particular, discovering\nthe root causes of the anomalies - e.g., why a computing node\nis entering a failure state? We also have plans to further extend\nand reﬁne the collected dataset and make it available to the pub-\nlic, in accordance with the facility owners and regulations about\nusers’ personal data (albeit not considered in this work, infor-\nmation about the users submitting the jobs to the HPC system\ncan indeed be collected). Moreover, in this work, we focused on\nnode-level anomalies; this was done to be comparable with the\n11\nstate-of-the-art and for scalability purposes; in the future, we\nwill explore the possibility of detecting systemic anomalies as\nwell, i.e., anomalies involving multiple nodes at the same time.\nIn this direction, the natural follow-up to the present work is\nto build hierarchical approaches which generate anomaly sig-\nnals based on the composition of the signals generated by the\nnode-speciﬁc detection models.\n6. Acknowledgments\nThis research was partly supported by the EuroHPC EU PI-\nLOT project (g.a. 101034126), the EuroHPC EU Regale project\n(g.a. 956560), EU H2020-ICT-11-2018-2019 IoTwins project\n(g.a. 857191), and EU Pilot for exascale EuroHPC EUPEX (g.\na. 101033975). We also thank CINECA for the collaboration\nand access to their machines and Francesco Beneventi for main-\ntaining Examon.\nReferences\n[1] W. Shin, V. Oles, A. M. Karimi, J. A. Ellis, F. Wang, Revealing power,\nenergy and thermal dynamics of a 200pf pre-exascale supercomputer,\nin: Proceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis, SC ’21, Association for\nComputing Machinery, New York, NY, USA, 2021, pp. 1–14.\ndoi:\n10.1145/3458817.3476188.\nURL https://doi.org/10.1145/3458817.3476188\n[2] D. Milojicic, P. Faraboschi, N. Dube, D. Roweth, Future of hpc: Di-\nversifying heterogeneity, in: 2021 Design, Automation Test in Europe\nConference Exhibition (DATE), 2021, pp. 276–281.\ndoi:10.23919/\nDATE51398.2021.9474063.\n[3] A. Netti, W. Shin, M. Ott, T. Wilde, N. Bates, A conceptual framework for\nhpc operational data analytics, in: 2021 IEEE International Conference\non Cluster Computing (CLUSTER), 2021, pp. 596–603. doi:10.1109/\nCluster48925.2021.00086.\n[4] L. A. Parnell, D. W. Demetriou, V. Kamath, E. Y. Zhang, Trends in high\nperformance computing: Exascale systems and facilities beyond the ﬁrst\nwave, in: 2019 18th IEEE Intersociety Conference on Thermal and Ther-\nmomechanical Phenomena in Electronic Systems (ITherm), 2019, pp.\n167–176. doi:10.1109/ITHERM.2019.8757229.\n[5] A. Borghesi, M. Molan, M. Milano, A. Bartolini, Anomaly detection and\nanticipation in high performance computing systems, IEEE Transactions\non Parallel and Distributed Systems 33 (4) (2022) 739–750. doi:10.\n1109/TPDS.2021.3082802.\n[6] A. Borghesi, A. Bartolini, et al., Anomaly detection using autoencoders\nin hpc systems, in: Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 2019, pp. 24–32.\n[7] A. Borghesi, M. Milano, L. Benini, Frequency assignment in high per-\nformance computing systems, in: International Conference of the Italian\nAssociation for Artiﬁcial Intelligence, Springer, 2019, pp. 151–164.\n[8] A. Netti, Z. Kiziltan, O. Babaoglu, A. Sˆırbu, A. Bartolini, A. Borghesi, A\nmachine learning approach to online fault classiﬁcation in hpc systems,\nFuture Generation Computer Systems (2019).\n[9] A. Netti, Z. Kiziltan, O. Babaoglu, A. Sˆırbu, A. Bartolini, A. Borghesi,\nOnline fault classiﬁcation in hpc systems through machine learning, in:\nEuropean Conference on Parallel Processing, Springer, 2019, pp. 3–16.\n[10] M. Du, F. Li, G. Zheng, V. Srikumar, Deeplog: Anomaly detection and\ndiagnosis from system logs through deep learning, in: Proceedings of\nthe 2017 ACM SIGSAC Conference on Computer and Communications\nSecurity, CCS ’17, Association for Computing Machinery, New York,\nNY, USA, 2017, p. 1285–1298. doi:10.1145/3133956.3134015.\nURL https://doi.org/10.1145/3133956.3134015\n[11] F. Iannone, G. Bracco, C. Cavazzoni, et al., Marconi-fusion: The new\nhigh performance computing facility for european nuclear fusion mod-\nelling, Fusion Engineering and Design 129 (2018) 354–358.\n[12] N. Beske, Ug3.2: Marconi100 userguide, accessed: 2020-08-17 (2020).\nURL\nhttps://wiki.u-gov.it/confluence/pages/viewpage.\naction?pageId=336727645\n[13] Top500list, https://www.top500.org/lists/top500/2020/06/ (2020).\n[14] A. Bartolini, F. Beneventi, A. Borghesi, D. Cesarini, A. Libri, L. Benini,\nC. Cavazzoni, Paving the way toward energy-aware and automated dat-\nacentre, in: Proceedings of the 48th International Conference on Paral-\nlel Processing: Workshops, ICPP 2019, Association for Computing Ma-\nchinery, New York, NY, USA, 2019, pp. 1–8. doi:10.1145/3339186.\n3339215.\nURL https://doi.org/10.1145/3339186.3339215\n[15] W. Barth, Nagios: System and network monitoring, No Starch Press,\n2008.\n[16] M. Molan, A. Borghesi, F. Beneventi, M. Guarrasi, A. Bartolini, An ex-\nplainable model for fault detection in hpc systems, in: H. Jagode, H. Anzt,\nH. Ltaief, P. Luszczek (Eds.), High Performance Computing, Springer In-\nternational Publishing, Cham, 2021, pp. 378–391.\n[17] O. Tuncer, E. Ates, Y. e. a. et Zhang, Online diagnosis of performance\nvariation in hpc systems using machine learning, IEEE Transactions on\nParallel and Distributed Systems (9 2018).\n[18] A. Netti, Z. Kiziltan, et al., Finj: A fault injection tool for hpc systems,\nin: European Conference on Parallel Processing, Springer, 2018, pp. 800–\n812.\n[19] M. Dani, H. Doreau, S. Alt, K-means application for anomaly detec-\ntion and log classiﬁcation in hpc, in: Lecture Notes in Computer Sci-\nence book series (LNAI,volume 10351), 2017, pp. 201–210.\ndoi:\n10.1007/978-3-319-60045-1\\_23.\n[20] A. Morrow, E. Baseman, S. Blanchard, Ranking anomalous high perfor-\nmance computing sensor data using unsupervised clustering, in: 2016\nInternational Conference on Computational Science and Computational\nIntelligence (CSCI), 2016, pp. 629–632.\ndoi:10.1109/CSCI.2016.\n0124.\n[21] S. Bursic, A. D’Amelio, V. Cuculo, Anomaly detection from log ﬁles\nusing unsupervised deep learning (09 2019).\n[22] A. Borghesi, A. Libri, et al., Online anomaly detection in hpc systems,\nin: 2019 IEEE International Conference on Artiﬁcial Intelligence Circuits\nand Systems, IEEE, 2019, pp. 229–233.\n[23] G. Moschini, R. Houssou, J. Bovay, S. Robert-Nicoud, Anomaly and\nfraud detection in credit card transactions using the arima model (2020).\narXiv:2009.07578.\n[24] M. Ahmed, A. N. Mahmood, M. R. Islam, A survey of anomaly detection\ntechniques in ﬁnancial domain, Future Generation Computer Systems 55\n(2016) 278–288. doi:https://doi.org/10.1016/j.future.2015.\n01.001.\nURL https://www.sciencedirect.com/science/article/pii/\nS0167739X15000023\n[25] K. B. Lee, S. Cheon, C. O. Kim, A convolutional neural network for fault\nclassiﬁcation and diagnosis in semiconductor manufacturing processes,\nIEEE Transactions on Semiconductor Manufacturing 30 (2) (2017) 135–\n142.\n[26] L. Rosa, T. Cruz, M. B. de Freitas, P. Quit´erio, J. Henriques, F. Caldeira,\nE. Monteiro, P. Sim˜oes, Intrusion and anomaly detection for the next-\ngeneration of industrial automation and control systems, Future Gener-\nation Computer Systems 119 (2021) 50–67. doi:https://doi.org/\n10.1016/j.future.2021.01.033.\nURL https://www.sciencedirect.com/science/article/pii/\nS0167739X21000431\n[27] I. Martins, J. S. Resende, P. R. Sousa, S. Silva, L. Antunes, J. Gama,\nHost-based ids: A review and open issues of an anomaly detection sys-\ntem in iot, Future Generation Computer Systems 133 (2022) 95–113.\ndoi:https://doi.org/10.1016/j.future.2022.03.001.\nURL https://www.sciencedirect.com/science/article/pii/\nS0167739X22000760\n[28] F. Cauteruccio, L. Cinelli, E. Corradini, G. Terracina, D. Ursino, L. Vir-\ngili, C. Savaglio, A. Liotta, G. Fortino, A framework for anomaly detec-\ntion and classiﬁcation in multiple iot scenarios, Future Generation Com-\nputer Systems 114 (2021) 322–335. doi:https://doi.org/10.1016/\nj.future.2020.08.010.\nURL https://www.sciencedirect.com/science/article/pii/\nS0167739X19335253\n[29] R. Xu, Y. Cheng, Z. Liu, Y. Xie, Y. Yang, Improved long short-term\n12\nmemory based anomaly detection with concept drift adaptive method\nfor supporting iot services, Future Generation Computer Systems 112\n(2020) 228–242. doi:https://doi.org/10.1016/j.future.2020.\n05.035.\nURL https://www.sciencedirect.com/science/article/pii/\nS0167739X20302235\n[30] S. Fu, S. Zhong, L. Lin, M. Zhao, A re-optimized deep auto-encoder for\ngas turbine unsupervised anomaly detection, Engineering Applications of\nArtiﬁcial Intelligence 101 (2021) 104199. doi:https://doi.org/10.\n1016/j.engappai.2021.104199.\nURL https://www.sciencedirect.com/science/article/pii/\nS0952197621000464\n[31] C. Zhang, D. Song, Y. Chen, X. Feng, C. Lumezanu, W. Cheng, J. Ni,\nB. Zong, H. Chen, N. V. Chawla, A deep neural network for unsupervised\nanomaly detection and diagnosis in multivariate time series data, CoRR\nabs/1811.08055 (2018). arXiv:1811.08055.\n[32] P. V. Astillo, D. G. Duguma, H. Park, J. Kim, B. Kim, I. You, Feder-\nated intelligence of anomaly detection agent in iotmd-enabled diabetes\nmanagement control system, Future Generation Computer Systems 128\n(2022) 395–405. doi:https://doi.org/10.1016/j.future.2021.\n10.023.\nURL https://www.sciencedirect.com/science/article/pii/\nS0167739X21004192\n[33] T. Salman, D. Bhamare, A. Erbad, R. Jain, M. Samaka, Machine learning\nfor anomaly detection and categorization in multi-cloud environments,\n2017 IEEE 4th International Conference on Cyber Security and Cloud\nComputing (CSCloud) (2017).\narXiv:1812.05443, doi:10.1109/\nCSCloud.2017.15.\n[34] M. Molan, Pre-processing for Anomaly Detection on Linear Accelerator.\nCERN openlab online summer intern project presentations (Sep 2020).\n[35] M. Gamell, K. Teranishi, et al., Modeling and simulating multiple fail-\nure masking enabled by local recovery for stencil-based applications at\nextreme scales, IEEE Transactions on Parallel and Distributed Systems\n28 (10) (2017).\n[36] E. Meneses, X. Ni, et al., Using migratable objects to enhance fault toler-\nance schemes in supercomputers, IEEE Transactions on Parallel and Dis-\ntributed Systems 26 (7) (2015) 2061–2074. doi:10.1109/TPDS.2014.\n2342228.\n[37] I. Boixaderas, D. Zivanovic, et al., Cost-aware prediction of uncorrected\ndram errors in the ﬁeld, in: 2020 SC20: International Conference for\nHPC, Networking, Storage and Analysis (SC), IEEE Comp. Soc., Los\nAlamitos, CA, USA, 2020, pp. 1–15.\n[38] G. Iuhasz, D. Petcu, Monitoring of exascale data processing, in:\n2019 IEEE International Conference on Advanced Scientiﬁc Computing\n(ICASC), 2019, pp. 1–5. doi:10.1109/ICASC48083.2019.8946279.\n[39] K. Bergman, S. Borkar, D. Campbell, W. Carlson, W. Dally, M. Den-\nneau, P. Franzon, W. Harrod, K. Hill, J. Hiller, et al., Exascale comput-\ning study: Technology challenges in achieving exascale systems, Defense\nAdvanced Research Projects Agency Information Processing Techniques\nOﬃce (DARPA IPTO), Tech. Rep 15 (2008).\n[40] G. Pang, C. Shen, L. Cao, A. V. D. Hengel, Deep learning for anomaly\ndetection: A review, ACM Comput. Surv. (mar 2020). doi:10.1145/\n3439950.\n[41] G. Pang, C. Shen, L. Cao, A. V. D. Hengel, Deep learning for anomaly\ndetection: A review, ACM Comput. Surv. 54 (2) (Mar. 2021). doi:10.\n1145/3439950.\nURL https://doi.org/10.1145/3439950\n[42] G. Lemaˆıtre, F. Nogueira, C. K. Aridas, Imbalanced-learn: A python tool-\nbox to tackle the curse of imbalanced datasets in machine learning, Jour-\nnal of Machine Learning Research 18 (17) (2017) 1–5.\n[43] A. Borghesi, A. Bartolini, M. Lombardi, M. Milano, L. Benini, A semisu-\npervised autoencoder-based approach for anomaly detection in high per-\nformance computing systems, Engineering Applications of Artiﬁcial In-\ntelligence 85 (2019) 634–644.\n[44] P. Wu, C. A. Harris, G. Salavasidis, A. Lorenzo-Lopez, I. Kamarudza-\nman, A. B. Phillips, G. Thomas, E. Anderlini, Unsupervised anomaly\ndetection for underwater gliders using generative adversarial networks,\nEngineering Applications of Artiﬁcial Intelligence 104 (2021) 104379.\ndoi:https://doi.org/10.1016/j.engappai.2021.104379.\nURL https://www.sciencedirect.com/science/article/pii/\nS095219762100227X\n[45] O. Tuncer, E. Ates, et al., Diagnosing performance variations in hpc ap-\nplications using machine learning, in: International Supercomputing Con-\nference, Springer, 2017, pp. 355–373.\n[46] B. Aksar, B. Schwaller, O. Aaziz, V. J. Leung, J. Brandt, M. Egele, A. K.\nCoskun, E2ewatch: An end-to-end anomaly diagnosis framework for pro-\nduction hpc systems, in: European Conference on Parallel Processing,\nSpringer, 2021, pp. 70–85.\n[47] B. Aksar, Y. Zhang, E. Ates, B. Schwaller, O. Aaziz, V. J. Leung,\nJ. Brandt, M. Egele, A. K. Coskun, Proctor: A semi-supervised per-\nformance anomaly diagnosis framework for production hpc systems, in:\nB. L. Chamberlain, A.-L. Varbanescu, H. Ltaief, P. Luszczek (Eds.), High\nPerformance Computing, Springer International Publishing, Cham, 2021,\npp. 195–214.\n[48] E. Baseman, S. Blanchard, N. DeBardeleben, A. Bonnie, A. Morrow, In-\nterpretable anomaly detection for monitoring of high performance com-\nputing systems, in: Outlier Deﬁnition, Detection, and Description on De-\nmand Workshop at ACM SIGKDD. San Francisco (Aug 2016), 2016, pp.\n1–27.\n[49] B. Aksar, B. Schwaller, O. Aaziz, V. J. Leung, J. Brandt, M. Egele, A. K.\nCoskun, E2ewatch: An end-to-end anomaly diagnosis framework for pro-\nduction hpc systems, in: L. Sousa, N. Roma, P. Tom´as (Eds.), Euro-\nPar 2021: Parallel Processing, Springer International Publishing, Cham,\n2021, pp. 70–85.\n[50] Wikipedia,\nJira\n(software)\n—\nWikipedia,\nthe\nfree\nencyclope-\ndia,\nhttp://en.wikipedia.org/w/index.php?title=Jira%\n20(software)&oldid=1052315603, [Online; accessed 04-December-\n2021] (2021).\n[51] Receiver operating characteristic (Nov 2021).\nURL\nhttps://en.wikipedia.org/wiki/Receiver_operating_\ncharacteristic\n[52] K. R. Shahapure, C. Nicholas, Cluster quality analysis using silhou-\nette score, in: 2020 IEEE 7th International Conference on Data Science\nand Advanced Analytics (DSAA), 2020, pp. 747–748. doi:10.1109/\nDSAA49011.2020.00096.\n[53] B. Lindemann, T. M¨uller, H. Vietz, N. Jazdi, M. Weyrich, A survey on\nlong short-term memory networks for time series prediction, Procedia\nCIRP 99 (2021) 650–655.\n[54] A. Borghesi, A. Bartolini, M. Lombardi, M. Milano, L. Benini, A semisu-\npervised autoencoder-based approach for anomaly detection in high per-\nformance computing systems, Engineering Applications of Artiﬁcial In-\ntelligence 85 (2019) 634–644.\ndoi:https://doi.org/10.1016/j.\nengappai.2019.07.008.\nURL https://www.sciencedirect.com/science/article/pii/\nS0952197619301721\n[55] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Van-\nderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay,\nScikit-learn: Machine learning in Python, Journal of Machine Learning\nResearch 12 (2011) 2825–2830.\n[56] Wikipedia, CINECA — Wikipedia, the free encyclopedia, http://en.\nwikipedia.org/w/index.php?title=CINECA&oldid=954269846,\n[Online; accessed 04-December-2021] (2021).\n[57] M. C. Calzarossa, L. Massari, D. Tessera, Workload characterization: A\nsurvey revisited, ACM Computing Surveys (CSUR) 48 (3) (2016) 1–43.\n[58] J. Dongarra, Report on the fujitsu fugaku system, University of\nTennessee-Knoxville Innovative Computing Laboratory, Tech. Rep.\nICLUT-20-06 (2020).\n13\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "68T07 (Primary) 68U01, 68T01 (Secondary)",
    "I.2; I.2.6"
  ],
  "published": "2022-08-28",
  "updated": "2022-08-28"
}