{
  "id": "http://arxiv.org/abs/2208.13649v1",
  "title": "Large-scale photonic natural language processing",
  "authors": [
    "Carlo Michele Valensise",
    "Ivana Grecco",
    "Davide Pierangeli",
    "Claudio Conti"
  ],
  "abstract": "Modern machine learning applications require huge artificial networks\ndemanding in computational power and memory. Light-based platforms promise\nultra-fast and energy-efficient hardware, which may help in realizing\nnext-generation data processing devices. However, current photonic networks are\nlimited by the number of input-output nodes that can be processed in a single\nshot. This restricted network capacity prevents their application to relevant\nlarge-scale problems such as natural language processing. Here, we realize a\nphotonic processor with a capacity exceeding $1.5 \\times 10^{10}$ optical\nnodes, more than one order of magnitude larger than any previous\nimplementation, which enables photonic large-scale text encoding and\nclassification. By exploiting the full three-dimensional structure of the\noptical field propagating in free space, we overcome the interpolation\nthreshold and reach the over-parametrized region of machine learning, a\ncondition that allows high-performance natural language processing with a\nminimal fraction of training points. Our results provide a novel solution to\nscale-up light-driven computing and open the route to photonic language\nprocessing.",
  "text": "Large-scale photonic natural language\nprocessing\nCARLO MICHELE VALENSISE1, IVANA GRECCO2, DAVIDE\nPIERANGELI3,2,1,*, AND CLAUDIO CONTI2,3,1\n1 Enrico Fermi Research Center (CREF), 00184 Rome, Italy\n2 Physics Department, Sapienza University of Rome, 00185 Rome, Italy\n3 Institute for Complex Systems, National Research Council (ISC-CNR), 00185 Rome, Italy\n*davide.pierangeli@roma1.infn.it\nAbstract: Modern machine learning applications require huge artiﬁcial networks demanding in\ncomputational power and memory. Light-based platforms promise ultra-fast and energy-eﬃcient\nhardware, which may help in realizing next-generation data processing devices. However, current\nphotonic networks are limited by the number of input-output nodes that can be processed in a\nsingle shot. This restricted network capacity prevents their application to relevant large-scale\nproblems such as natural language processing. Here, we realize a photonic processor with a\ncapacity exceeding 1.5 × 1010 optical nodes, more than one order of magnitude larger than any\nprevious implementation, which enables photonic large-scale text encoding and classiﬁcation. By\nexploiting the full three-dimensional structure of the optical ﬁeld propagating in free space, we\novercome the interpolation threshold and reach the over-parametrized region of machine learning,\na condition that allows high-performance natural language processing with a minimal fraction of\ntraining points. Our results provide a novel solution to scale-up light-driven computing and open\nthe route to photonic language processing.\n© 2022 Optical Society of America\n1.\nINTRODUCTION\nAdvanced artiﬁcial intelligence systems are becoming extremely demanding in terms of training\ntime and energy consumption [1, 2], since an ever-larger number of trainable parameters is\nrequired to exploit over-parametrization and achieve state-of-the-art performances [3,4]. In turn,\nlarge-scale, energy-eﬃcient computational hardware is becoming a subject of intense interest.\nPhotonic neuromorphic computing systems [5–7] oﬀer large throughput [8] and energetically\neﬃcient [9] hardware accelerators, based on integrated silicon photonic circuits [10–14],\nengineered meta-materials [15], or 3D printed diﬀractive masks [16, 17]. Other light-based\ncomputing architectures leverage Reservoir Computing (RC) [18] and Extreme Learning Machine\n(ELM) [19] computational paradigms, in which the input data is mapped into a feature space\nthrough a ﬁxed set of random weights and training is performed only on the readout linear\nlayer. Optical reservoir computers [20–33] and Photonic Extreme Learning Machines (PELMs)\n[34,35,37,51] applies successfully to various learning tasks, ranging from time series prediction\n[38,39] to image classiﬁcation [40,41]. In spite of the remarkable performances achieved by\nthese architectures, their impact on large-scale problems is limited, mainly due to size constraints.\nHere, we demonstrate photonic machine learning at an ultra-large scale by mapping the optical\noutput layer in the full three-dimensional (3D) structure of the optical ﬁeld. This original approach\noﬀers inherent scalability to the optical device and eﬀortless access to the over-parametrized\nlearning region. The 3D-PELM that we implement is able to process simultaneously up to 250000\ntotal input-output nodes via spatial light modulation, featuring a total network capacity one order\nof magnitude larger than existing optical reservoir computers. Our large-scale photonic network\nallows us to optically implement a massive text classiﬁcation problem, the sentiment analysis\nof the Image Movie Database (IMDb) [44], and enable, for the ﬁrst time to our knowledge, the\narXiv:2208.13649v1  [cs.ET]  29 Aug 2022\n(A)\ntext\none-hot sparse\ntf-idf sparse\nHadamard\n(B)\noptical encoding\nplane 1\nplane 2\nplane 3\nSLM\nnetwork capacity 𝐶> 1010\nphotonic natural language processing\n(C)\n(D)\nrating\n# users > 106\npostive\nnegative\nFig. 1. Three-dimensional PELM for natural language processing. (A) The text\ndatabase entry is a paragraph of variable length.\nText pre-processing: a sparse\nrepresentation of the input paragraph is mapped into an Hadamard matrix with phase\nvalues in [0, 𝜋]. (B) The mask is encoded into the optical wavefront by a phase-\nonly spatial light modulator (SLM). Free-space propagation of the optical ﬁeld maps\nthe input data into a 3D intensity distribution (speckle-like volume). (C) Sampling\nthe propagating laser beam in multiple far-ﬁeld planes enables the up-scaling of the\ncorresponding feature space. (D) Intensities picked from all the spatial modes form the\noutput layer H3D that undergoes training via ridge regression. By using three planes\n(𝑗= 3) we get a network capacity 𝐶> 1010. The inset example shows a binary text\nclassiﬁcation problem for large-scale rating.\nobservation of the double descent phenomenon [45–47] on a photonic computing device. We\ndemonstrate that, thanks to its huge number of optical output nodes, the 3D-PELM successfully\nclassiﬁes text by using only a limited number of training points, realizing energy-eﬃcient\nlarge-scale text processing.\n2.\nRESULTS\nThree-dimensional PELM\nA PELM classiﬁes a dataset with 𝑁points X = [𝑋1, ..., 𝑋𝑁] by mapping the input sample\n𝑋𝑖∈R𝐿into a high-dimensional feature space through a nonlinear transformation that is\nperformed by a photonic platform [34]. To perform the classiﬁcation, the output matrix H, which\ncontains the measured optical signals, is linearly combined with a set of 𝑀trainable readout\nweights 𝛽(see Appendix A). In all previous PELM realizations [25,34,35,39,41], the intensities\nstored in H only contain information on the optical ﬁeld at a single spatial or temporal plane.\nHere, to scale up the optical network, we exploit the entire three-dimensional optical ﬁeld. Figure\n1(B) shows a schematic of our 3D-PELM: the photonic network uses as output nodes the full\n3D structure of the optical ﬁeld propagating in free space. The transformation of the input data\nsample into an optical intensity volume occurs through linear wave mixing by coherent free-space\npropagation and intensity detection. Speciﬁcally, the 3D optical mapping reads as\nH3D =\n∑︁\n𝑗\nH 𝑗,\n(1)\nH 𝑗= 𝐺𝑗(M𝑗exp𝑖(X + W)) ,\n(2)\nwhere M 𝑗is the transfer matrix that models coherent light propagation to the 𝑗th detection\nplane, e.g., the discretized diﬀraction operator. The M𝑗have complex coeﬃcients, and thus the\nphotonic scheme implements a complex-valued neural network [42,43]. In Eq. (2) the input\ndataset is encoded by phase modulation, while W is a pre-selected random matrix that embeds the\ninput signal and provides network biases. 𝐺𝑗is the response function of the 𝑗th detector to the\nimpinging light ﬁeld. In experiments, to collect uncorrelated output data H 𝑗, we employ multiple\ncameras that simultaneously image the intensity in distinct far-ﬁeld planes (see Appendix B).\nThese intensity distributions are uncorrelated in order to acquire non-redundant information.\nTherefore, the speckle-like pattern detected by each camera (Fig. 1(C)) represents the optical\nprojection of the 𝑋𝑖input data on a distinct portion of the entire feature space.\nOptical encoding of natural language sentences\nThe NLP task we considered is the classiﬁcation of the IMDB dataset, constructed by Maas et\nal. [44]. To encode natural language on the optical setup, we need to develop an optics-speciﬁc\nrepresentation of the text data. Since text classiﬁcation tasks have remained elusive in photonic\nneuromorphic computing, we here introduce the problem. A similar issue is crucial also in digital\nNLP, where the text encoding problem consists in obtaining a numerical representation of the\ninput that is convenient for digital learning architectures [48]. Speciﬁcally, given a vocabulary,\ni.e., the set of all unique words in the corpus, a basic encoding method is the one-hot technique,\nin which each word corresponds to a boolean vector with vocabulary size 𝑉. Consequently,\nparagraph of diﬀerent lengths can be represented by boolean vectors of size 𝑉, with the element\n𝑥𝑘𝑖= 1 that indicates the presence of the 𝑘th vocabulary word in the paragraph 𝑋𝑖[Fig. 1(A)].\nAn input dataset with 𝑁samples, in which each database entry is a text paragraph [see Fig. 1(A)],\nthus becomes a 𝑁× 𝑉matrix. The dimension of the single entry 𝑋𝑖to encode thus scales with\nthe number of words 𝑉in the vocabulary (104 −105). Photonic text processing hence necessitates\nan optical platform able of encoding huge input data. Given the large number of input modes\n𝐿supported by spatial light modulation, our 3D-PELM is the most convenient scheme for the\nscope.\nOptical encoding of natural language in the 3D-PELM requires using a SLM with a ﬁxed\nnumber of input modes for sentences with variable lengths. We can thus follow the one-hot\nmethod employed in digital NLP. We leverage an extension of one-hot encoding, the so-called\ntf-idf representation, and construct the input representation Xtﬁdf. In information retrieval, the\ntf-idf(𝑖, 𝑗) statistics reﬂect how important is a word 𝑘to a document 𝑗(frequency 𝑛𝑘𝑗) in a\ncollection of documents. It is deﬁned as: tf-idf(𝑘, 𝑗) = 𝑛𝑘𝑗\n𝑉· log10\n\u0010\n𝑁\n𝑆𝑘\n\u0011\n, where 𝑉is the length\nof a paragraph, and 𝑆𝑘is the number of sentences containing the 𝑘th word. Within the tf-idf\nrepresentation, each paragraph becomes a sparse, real vector [see Fig. 1(A)] whose non-zero\nelements are the tf-idf values of the words composing it. To optically encode these sparse data,\nwe applied to Xtﬁdf the Walsh-Hadamard transform (WHT) [49], which decomposes a discrete\nfunction in a superposition of Walsh functions. The transformed dataset XWHT is a dense matrix\n[Fig. 1(A)], which is displayed on the SLM as a phase mask within the [0,𝜋] range (X = XWHT\nin Eq. (2)). By exploiting the large number of available input pixels, we encode paragraphs with\na massive size 𝑉that contain hundreds of thousands of words.\n+ Ntest\nFig. 2. Photonic NLP. (A-B) Train and test accuracy of the 3D-PELM on the IMDb\ndataset as a function of the number of output channels. The shaded area corresponds\nto the over-parametrized region. The conﬁguration in (B) allows reaching very high\naccuracy in the over-parametrized region with a dataset limited to 𝑁train = 1186 training\npoints. In (A) the same accuracy is reached in the under-parametrized region with\n𝑁train = 12278. Black horizontal lines correspond to the maximum test accuracy\nachieved (0.77). (C) IMDb classiﬁcation accuracy by varying the number of features 𝑀\nand training dataset size 𝑁train. The boundary between the under and over-parametrized\nregion (interpolation threshold), 𝑁train = 𝑀, is characterized by a sharp accuracy drop\n(cyan contour line).\nObservation of the photonic double descent\nMachine learning models with a very large number of trainable parameters show unique features\nwith respect to smaller models, such as the double descent phenomenon. This eﬀect is a resonance\nin the neural network performance, ruled by the ratio between the number of trainable parameters\n𝑀and the number of training points 𝑁train. In the under-parametrized regime (𝑀< 𝑁train), good\nperformances are obtained by balancing model bias and variance. As 𝑀grows, models tend\nto overﬁt training data, and prediction performances get worse until the interpolation threshold\n(𝑀= 𝑁train) is reached, a point where the model optimally interpolates the training data and\nthe prediction error is maximum. Beyond this resonance, in the over-parametrized regime, the\nmodel keeps interpolating training points, but performances on the test set reach the global\noptimum [50].\nWe experimentally implement photonic NLP and investigate the double descent eﬀect on our\nlarge-scale 3D-PELM. Speciﬁcally, we analyze the classiﬁcation accuracy for the IMDb task\n(Appendix D) as a function of the features 𝑀and training set size 𝑁train. In Figure 2(A) we\nreport the observation of the double descent. We observe a dip in test accuracy as the number of\nchannels 𝑀reaches the number of training points 𝑁train. Beyond this resonance, also known as\nFig. 3. Performances at ultra-large scale. (A-C) Accuracy as a function of 𝑀for\ndiﬀerent input sizes 𝐿. In all cases, the 3D-PELM performance saturates in the\nover-parameterized region reaching a plateau. A linear ﬁt of the data preceding the\nplateau shows that the onset of the saturation is faster for datasets with a larger input\nspace. The corresponding angular coeﬃcient 𝑚is inset in each panel. (D) Test accuracy\nvarying the training set size, for 𝑀= 0.8 × 105 and 𝑀= 1.2 × 105.\ninterpolation threshold, we ﬁnd the over-parametrized region in which maximum accuracy on the\ntraining set is achieved. The behavior is obtained via training on 𝑁train ≃1.2 × 104 examples and\nusing ﬁxed train/test split ratio of 0.67/0.33. In this case, the larger classiﬁcation accuracy is\nfound in the under-parametrized region (0.77). Conversely, in Figure 2(B), we consider a much\nsmaller number of training points, 𝑁train ≃103. Remarkably, we reach the same optimal accuracy\ndespite we are using only a fraction of the available training points. This observation reveals\na particularly favourable learning region of the 3D-PELM in which only a reduced number of\ntraining points is required to reach high-accuracy levels. In fact, from the operational standpoint,\nit is much more eﬃcient to measure in parallel a large number of modes rather than sequentially\nprocessing many training examples. In Figure 2(C) we report the full dynamics of the double\ndescent phenomenon, by continuously varying both 𝑀and 𝑁train. We observe the accuracy dip\nshifts with a constant velocity. This result shows the existence of an optimal learning region that\nis accessible on the 3D-PELM thanks to its large number of photonic nodes.\nNLP at ultra-large scale\nThe observed operational advantage of the over-parametrized region indicates that by further\nincreasing the number of output modes one can increase training eﬀectiveness and/or performances\n[3]. In our 3D-PELM we reached 𝑀= 120000 readout channels that are independent from\neach other. We also considered larger input spaces, extending the vocabulary so as to include\nthe whole set of words in the corpus (𝑉≃217). Figure 3(A) shows that the test accuracy\nreaches a plateau as 𝑀increases in the over-parametrized region. Saturation indicates that all the\nessential information encoded within the optical ﬁeld has been extracted through the available\nchannels. However, we observe a change in the performance as we employ more input features\nWorking principle\nM\nL\nC\nMachine learning task\nRef.\nTime-multiplexed cavity\n1400\n7129\n107\nRegression\n[39]\nAmplitude modulation\n16384\n2000\n108\nHuman action recognition\n[27]\nFrequency multiplexing\n200\n640\n105\nTime series recovery\n[41]\nOptical multiple scattering\n50000\n64\n106\nChaotic series prediction\n[38]\nAmplitude Fourier filtering\n1024\n43263\n107\nImage classification\n[30]\nMultimode Fiber\n240\n240\n105\nClassification, Regression\n[35]\nFree-space propagation\n6400\n784\n106\nClassification, Regression\n[34]\n3D optical field\n120000\n131044\n1010\nNatural Language\nProcessing\n3D-PELM\nTable 1. Maximum network capacity of current photonic neuromorphic computing\nhardware.\n𝐿, Fig. 3(B-C). To estimate the rate by which performance improves as more channels were\nused for training, we estimate the angular coeﬃcient 𝑚of the linear growth that precedes the\nplateau. Although the onset of saturation can be estimated using diverse criteria, and 𝑚varies\ndepending on the measured range, its trend when increasing 𝐿remain unaltered. We note a\nrelevant enhancement of 𝑚for 𝐿= 362 × 362, which indicates that PELMs featuring larger input\nspaces are able to reach optimal performances with a lower number of parameters. Figure 3(D)\nreports the accuracy as a function of the training-test split ratio (keeping ﬁxed 𝑁train = 6.7 × 103)\nfor 𝑀= 0.8 × 105 and 𝑀= 1.2 × 105. Importantly, a limited number of examples (∼20%) is\nenough to reach maximum accuracy in the over-parametrized region.\nOptical network capacity\nTo establish a comparison among the various photonic neuromorphic computing devices, we\nintroduce the optical network capacity as a useful ingredient related to the over-parametrization\ncontext. We deﬁne the capacity C of a generic optical neural network as the product between the\nnumber of input and output nodes that can be processed in a single iteration, C = 𝐿× 𝑀. This\nquantity gives direct information on the kind of large-scale problems that can be implemented\non the optical setup. It depends only the number of controllable nodes, which is the quantity\nthat is believed to play the main role in big data processing [3]. Speciﬁcally, 𝐿sets the size\nof the dataset that can be encoded, while 𝑀reﬂects the embedding capacity of the network,\nas larger dataset necessitate larger feature spaces to be learned. Moreover, C also furnishes an\nindication on how far is the over-parametrized regime for a given task. Useful over-parametrized\nconditions can be reached only if 𝐶≫𝑁× 𝐿. We remark that the capacity is not a measure\nof the processor accuracy. It is instead a useful quantity to compare the scalability of diﬀerent\nphotonic computing devices.\nIn Tab. 1 we report the optical network capacity for various photonic processors that have been\nrecently demonstrated. We focus on photonic platforms that exploit RC and ELM paradigms, since\nthese devices are suitable for big data processing and may present the so-called photonic advantage\nat a large scale [51]. Our 3D-PELM has a record capacity C = 362 × 362 × 120000 ≃1.5 × 1010,\nmore than one order of magnitude larger than any other optical processor. Moreover, while\nincreasing the capacity is challenging on many optical reservoir computers, our 3D-PELM can\nbe further scaled up by simply enlarging the measured volume of the optical ﬁeld.\n(A)\n3D-PELM\ndevice\n3D-PELM\nnumerics\n8-bit\nNtrain=6700\nNtrain= 1500\n(B)\nRP\nSVM\nCNN\nunder-parametrized\nover-parametrized\n(A)\n8-bit\n8-bit\n8-bit\nunder-parametrized\nover-parametrized\n0.75\n0.74\nFig. 4. Analysis of the IMDb accuracy. (A-B) The comparison report the accuracy\nfor the experimental device (3D-PELM device), the simulated device (3D-PELM\nnumerics), the random projection method with ridge regression (RP), the support\nvector machine (SVM) and a convolutional nueral network (CNN) in both the under-\nparameterized (𝑀= 1 × 103) and over-parametrized (𝑀= 4 × 104 ) regime, for (A)\n𝑁train = 6700, and (B) 𝑁train = 1500. 8-bit numerical results, when applicable, refer to\nthe over-parametrized regime.\nComparison with digital machine learning\nTo further validate the capability of our photonic setup for text processing, we compare the\naccuracy of our 3D-PELM with various general-purpose digital neural networks on the IMDb task.\nTo underline the overall impact of the over-parameterization on the performance, we consider two\nopposite parametrization regimes, respectively 𝑀= 1 × 103 and 𝑀= 4 × 104, and two distinct\ncondition for the dataset size, 𝑁train = 6700 [Fig. 4(A)] and 𝑁train = 1500 [Fig. 4(B)]. Since the\nlearning principle of the 3D-PELM is based on kernel methods (see Ref. [34]), we implement\na support vector machine (SVM) and a ridge regression based on nonlinear random projetions\n(RP) [25] as representative models of the kernel algorithms class. The input for the SVM and\nthe RP is the tﬁdf-encoded dataset, but no signiﬁcant diﬀerences are found using the Hadamard\ndataset. We also simulate the 3D-PELM, i.e., we evaluate our photonic scheme by using Eq. (2)\nto generate the output data. A convolutional neural network (CNN) with the same number of\ntrainable weights is used as an additional benchmark model. Details on the various digital models\nare reported in Appendix E. The results, for a split ratio 𝑁train/𝑁test = 0.67/0.33, are shown\nin Figure 4. Overall, we observe that the photonic device performs sentiment analysis on the\nIMDb with an accuracy comparable with standard digital methods. The SVM sets the maximum\nexpected accuracy (0.86). The device over-parametrized accuracy (0.75 for 𝑁train = 6700) is\nmainly limited by the limited precision of the optical components and noise, and could be\nenhanced up to 0.83 (3D-PELM numerics) by using a 64-bit camera. In fact, when operating\nwith 8-bit precision, both the simulated 3D-PELM and the RP model achieves performances\nthat agree well with the experiments. Interestingly, Fig. 4(B) indicates that, for a limited set\nof training samples, the 3D-PELM surpasses the SVM and also the CNN. This points out an\nadditional advantage that the photonic setup achieves thanks to over-parametrization. Not only\nits accuracy is improved with respect the standard under-parametrized regime, but the device can\noperates eﬀectively in conditions where digital models are less accurate.\n3.\nDISCUSSION\nWe have reported the ﬁrst photonic implementation of natural language processing by performing\nsentiment analysis on the IMBd dataset. Our results demonstrate the feasibility of modern\nlarge-scale computing tasks in photonic hardware, and can be potentially improved both in terms\nof performance, via low-noise high-resolution cameras and more complex encoding/training\nalgorithms, and applications. Further developments include the implementation of advanced\ntasks such as multiclass sentiment analysis, which allows discriminating positive and negative\nsentences into diﬀerent levels. Moreover, using diﬀerent datasets such as the Stanford Sentiment\nTreebank (SST-5), we could train our 3D-PELM for providing ratings. On the other hand,\nour versatile optical setting may allow implementing alternative strategies for photonic NLP,\nthese including dimensionality reduction of the input data via optical random projection [52].\nAn optical scheme that preserves the sequential nature of language may be also conceived by\nintroducing recurrent mechanisms in our 3D-PELM. Another interesting direction is developing\nspeciﬁc encoding schemes for representing text in the optical domain. Viable approaches includes\nadapting bi-dimensional text representations such as Quick-Response (QR) codes. Furthermore,\nby following the concept of word embedding used in digital NLP [53], photonic hardware may be\nused to learn a purely optical word embedding, including semantics, word ordering, and syntax.\nIn conclusion, modern machine learning architectures rely on over-parametrization to reach\nthe global optimum of objective functions. However, as better performances are associated with\nlarger training sets, to reach an over-parametrized regime the number of adjustable parameters has\nto grow accordingly, requiring in turn longer training times and more energy consumption [54].\nThis trend is evident in NLP: the state-of-the-art NLP model, GPT-3 [55], features 175 billions\nof parameters, in line with the exponential growth in the number of parameters of latest NLP\nmodels [1], and requires a dedicated supercomputing hardware for training. On the other hand,\nachieving over-parametrization for large-scale problems is challenging in photonic computing.\nHere, we realize a novel photonic computing device that, thanks to its huge network capacity,\nnaturally operates in the over-parametrized region. The 3D-PELM is a very promising solution\nfor developing a fully-scalable photonic learning platform. Our photonic processor enables the\nﬁrst link between NLP and optical computing hardware, opening a new vibrant research stream\ntowards fast and energetically eﬃcient artiﬁcial intelligence tools.\nAPPENDIX A: PELM framework\nIn ELMs a dataset with 𝑁data points X = [(𝑋1, 𝑦1), . . . , (𝑋𝑁, 𝑦𝑁)], with (𝑋𝑖, 𝑦𝑖) ∈R𝐿× R, is\nmapped into a higher-dimensional feature space through a nonlinear transformation 𝑔(X), yielding\nthe hidden-layer output matrix H = [𝑔(𝑋1), . . . , 𝑔(𝑋𝑁)], with 𝑔(𝑋𝑖) ∈R𝑀. A set of weights\n𝛽is learned via ridge regression such that the linear combination ˜Y = H𝛽well approximates\nthe true label vector Y = [𝑦1, . . . , 𝑦𝑁]. An explicit solution is 𝛽= (H𝑇H + 𝜆I)−1 H𝑇Y\nwhere 𝜆is the regularization parameter, I the identity. The use of ridge regression allows\nreducing at minimum the training costs, which is crucial in applications requiring fast and\nreconﬁgurable learning. In a free-space PELM, the nonlinear mapping of input data is realized\nby a combination of free-space optical propagation and intensity detection [34]. The speckle-like\npattern detected by a camera forms the nonlinear features of the input sample. The optical\nmapping reads as H = 𝐺[M exp𝑖(X + W)], where the input dataset X is phase-encoded via\nspatial light modulation. W is the embedding matrix, 𝐺the detector response function, and M\nthe discrete Fourier transform that models propagation to the focal plane. For more details on the\nPELM architecture and its working principle see Ref. [34].\nAPPENDIX B: Experimental setup\nThe optical setup of the 3D-PELM is sketched in Figure 1(B). A CW laser beam (532 nm, 100\nmW) is expanded and collimated onto a phase-only SLM (Hamamatsu X13138, 1280 × 1024\npixels, 60 Hz, 8 bits), that encodes data on the optical wavefront. The SLM operates in the\n[0, 2𝜋] interval with a linear response function. Both the input and the embedding signal are\nencoded within the [0, 𝜋] range and superimposed. After free-space propagation through a lens\n( 𝑓= 150mm), the far-ﬁeld three-dimensional intensity distribution is accessed simultaneously\nby three separated imaging systems and detected by three cameras (Basler Ace 2 Pro, 1920\n× 1200 pixels, 12 bits). The speckle-like ﬁeld spatially decorrelates during propagation; the\ntransverse intensity is transformed in a random way from one plane to the other, provided that\nthese planes are at distances larger than the longitudinal ﬁeld correlation length. To carry out\nthe measurements, the propagating beam is divided by a series of beam splitters into separated\nimaging paths with diﬀerent tunable optical paths. The far-ﬁeld planes are selected in order to\nsample three speckle-like distribution having a negligible mutual intensity correlation. This is\nconﬁrmed by intensity correlation measurements. Every additional plane contains additional\ninformation. In fact, keeping ﬁxed 𝑀, we ﬁnd an improvement in the classiﬁcation accuracy\nwhen using three far-ﬁeld planes instead of a single intensity distribution. In this condition, each\ncamera maps a distinct portion of the high-dimensional feature space in which the input ﬁeld is\nprojected.\nThe measured set of speckle-like patterns is elaborated on a conventional computer. Speckle\ngrains typically extend over adjacent pixels [Fig. 1(C)]. To avoid spatial correlations within the\nsingle speckle pattern, pixels are grouped into square modes (channels) and the average intensity\non each channels forms the output signals.\nAPPENDIX C: 3D-PELM training\nTraining operates by loading the randomly-ordered input dataset on the SLM and measuring three\nspeckle-like intensity distributions for each input sample (paragraph). From the acquired signals\nwe randomly select 𝑀of the available channels, and we use the corresponding 𝑀intensity\nvalues to form the output dataset. This dataset is split randomly into a training and test dataset\nby using a split ratio 𝑁train/𝑁test = 0.67/0.33, which is kept ﬁxed throughout the analysis. All\nthe classiﬁcation accuracies we obtained refer to this hyperparameter. The training dataset is\nthe output matrix H used for training (see Appendix A). Since our task corresponds to a binary\nclassiﬁcation problem, the target labels are 𝑦𝑖∈{0, 1}. According to the PELM framework,\ntraining by ridge regression consists in solving the equation for the 𝛽vector. This matrix inversion\nis performed using the ML Python Library Sci-kit learn. The regularization parameter 𝜆is varied\nin the [10−4, 102] range. As its value does not inﬂuence considerably the testing accuracy, we use\n𝜆= 10−4. Given a predicted output ˆ𝑦𝑖, the predicted label is given by ˜𝑦𝑖= Θ( ˆ𝑦𝑖−0.5), where Θ\nis the Heaviside step function. Text classiﬁcation performances are evaluated via the accuracy\nÍ\n𝑖[𝑦𝑖= ˜𝑦𝑖] /𝑁, where [·] are the Iverson bracket deﬁned as [𝑃] = 1 if 𝑃is true and 0 otherwise.\nAPPENDIX D: NLP task, text pre-processing and optical encoding\nWe consider the IMDb dataset [44], a widely used benchmark for polarity-based text classiﬁcation\n[56]. The dataset consists of 5 × 104 movie reviews, each one forming the 𝑖-th input data\nsample. Each review is made of a paragraph consisting of multiple sentences, and expresses\nan overall positive or negative sentiment (target variable). This sentiment analysis task hence\nconsists in a large-scale binary classiﬁcation. To implement the problem, the whole set of IMDb\nsentences is pre-processed by removing special characters (e.g. html), lowercasing, removing\nstop words and lemmatizing. The length of the resulting vector can be varied by neglecting\nwords that appears in the corpus with a frequency lower than a given threshold. To compute the\nWalsh-Hadamard transform and obtain a dense text representation, zero-padding is used to reach\na vector length 𝑑=\n\u0006\nlog2 𝑉\n\u0007. For example, results in Fig. 2 refer to data with 𝑉= 104 that are\npadded to 𝐿= 214 = 16384 and reshaped as a 128 × 128 square matrix, while the full vocabulary\n(𝑉= 131044) requires zero-padding and paragraphs are arranged on 362 × 362 input modes (Fig.\n3). After WHT computation, the values are normalized in the [0, 𝜋] range and loaded on the\nSLM. Each element of the input XWHT is displayed onto a square block of SLM pixels. The\nembedding matrix W is a uniformly distributed random matrix for all the experiments. Its values\n𝑤𝑖𝑗are selected in [0, 𝜋] before the training and kept ﬁxed for a given photonic computation.\nAPPENDIX E: Digital machine learning\nThe 3D-PELM is numerically simulated by following the optical mapping model in Eq. (1) (see\nalso Ref. [34]). Data encoding and training operates in the same way as the photonic device,\nand the output matrix is computed by using Eq. (2) with 𝐺(𝑧) = |𝑧|2 (quadratic nonlinearity).\nIn the RP model, data projection in the feature space is obtained by matrix multiplication with\na real-valued uniformly-distributed random matrix and using a quadratic activation function.\nTraining occurs by ridge regression. For simulating the 8-bit machines, the input data and\nthe output values forming the linear readout layer are properly discretized to values with 8-bit\nprecision. The SVM is a binary classiﬁer and thus it is especially suited for the IMDb task. We use\nthe Sci-kit learn built-in function on the tﬁd-processed dataset and the over-parametrized regime\nis evaluated by using 𝑀= 103. The CNN is composed by four convolutional layer with nonlinear\nactivation and a fully connected layer. The CNN scheme is designed to have 𝑀total nodes, and\nits accuracy does not depend signiﬁcatively on the network details and hyperparameters.\nFunding. We acknowledge funding from the Italian Ministry of Education, University and\nResearch (PRIN PELM 20177PSCKT), Sapienza Research, and Enrico Fermi Research Center\n(CREF).\nAuthor Contributions. D.P. and C.C. conceived the research. C.M.V. proposed application\nto NLP. D.P. developed the 3D photonic network. I.G. and D.P. carried out experimental\nmeasurements. C.M.V. and I.G. performed data analysis. D.P and C.C. co-supervised the project.\nAll authors contributed to the manuscript.\nAcknowledgments. We thank I. MD Deen, V.H. Santos, and F. Farrelly for technical support\nin the laboratory.\nDisclosures. The authors declare no conﬂicts of interest.\nData availability. The data that support the results presented in this study are available from\nthe corresponding author upon reasonable request.\nCode availability. The codes developed within this research are available from the corre-\nsponding author upon reasonable request.\nReferences\n1.\nD. Narayanan et al., Eﬃcient large-scale language model training on GPU clusters using megatron-LM,\narXiv:2104.04473 (2021).\n2.\nE. Strubell, A. Ganesh, and A. McCallum, Energy and policy considerations for modern deep learning research, Proc.\nAAAI Conf. on Artif. Intell. 34, 13693–13696 (2020).\n3.\nA. Chatelain, A. Djeghri, D. Hesslow, J. Launay, and I. Poli, Is the number of trainable parameters all that actually\nmatters?, arXiv:2109.11928 (2021).\n4.\nN.C. Thompson, K. Greenewald, K. Lee, and G.F. Manso, The computational limits of deep learning, arXiv:2007.05558\n(2020).\n5.\nG. Wetzstein, A. Ozcan, S. Gigan, S. Fan, D. Englund, M. Soljačić, C. Denz, D.A.B. Miller and D. Psaltis, Inference\nin artiﬁcial intelligence with deep optics and photonics, Nature 588, 39-47 (2020).\n6.\nB.J. Shastri, A.N. Tait, T.F. de Lima, W.H.P. Pernice, H. Bhaskaran, C.D. Wright, and P.R. Prucnal, Photonics for\nartiﬁcial intelligence and neuromorphic computing, Nat. Photonics 15, 102–114 (2021).\n7.\nH. Zhou et al., Photonic matrix multiplication lights up photonic accelerator and beyond, Light Sci. Appl. 11, 1-21\n(2022).\n8.\nX. Xu, M. Tan, B. Corcoran, J. Wu, A. Boes, T.G. Nguyen, S.T. Chu, B.E. Little, D.G. Hicks, R. Morandotti, A.\nMitchell, and D.J. Moss, 11 TOPS photonic convolutional accelerator for optical neural networks, Nature 589, 44-51\n(2021).\n9.\nT. Wang, S.-Y. Ma, L.G. Wright, T. Onodera, B. Richard, and P.L. McMahon, An optical neural network using less\nthan 1 photon per multiplication, Nat. Commun. 13, 1-8 (2022).\n10. Y. Shen, N.C. Harris, S. Skirlo, M. Prabhu, T. Baehr-Jones, M. Hochberg, X. Sun, S. Zhao, H. Larochelle, D. Englund,\nand M. Soljacic, Deep learning with coherent nanophotonic circuits, Nat. Photon. 11, 441–446 (2017).\n11. A.N. Tait , T.F. de Lima, E. Zhou, A.X. Wu, M-A. Nahmias, B.J. Shastri, P.R. Prucnal, Neuromorphic photonic\nnetworks using silicon photonic weight banks, Sci. Rep. 7, 7430 (2017).\n12. J. Feldmann, N. Youngblood, C.D. Wright, H. Bhaskaran and W.H.P. Pernice, All-optical spiking neurosynaptic\nnetworks with self-learning capabilities, Nature 569, 208-214 (2019).\n13. F. Stelzer, A. Röhm, R. Vicente, I. Fischer, and S. Yanchuk, Deep neural networks using a single neuron: folded-in-time\narchitecture using feedback-modulated delay loops,Nat. Commun. 12 (2021).\n14. J. Feldmann, N. Youngblood, M. Karpov, H. Gehring, X. Li, M. Stappers, M. Le Gallo, X. Fu, A. Lukashchuk, A.S.\nRaja, J. Liu, C.D. Wright, A. Sebastian, T.J. Kippenberg, W.H.P. Pernice, and H. Bhaskaran, Parallel convolutional\nprocessing using an integrated photonic tensor core, Nature 589, 52-58 (2021).\n15. N. Mohammadi Estakhri, B. Edwards, and N. Engheta, Inverse-designed metastructures that solve equations, Science\n363, 1333 (2019).\n16. X. Lin, Y. Rivenson, N. T. Yardimci, M. Veli, Y. Luo, M. Jarrahi, and A. Ozcan, All-optical machine learning using\ndiﬀractive deep neural networks, Science 361, 1004–1008 (2018).\n17. T. Zhou, X. Lin, J. Wu, Y. Chen, H. Xie, Y. Li, J. Fan, H. Wu, L. Fang, and Q. Dai, Large-scale neuromorphic\noptoelectronic computing with a reconﬁgurable diﬀractive processing unit, Nat. Photonics 15, 367–373 (2021).\n18. D. Verstraeten, B. Schrauwen, M. D’Haene, and D. Stroobandt, An experimental uniﬁcation of reservoir computing\nmethods, Neural Networks 20, 391–403 (2007).\n19. G.B. Huang, H. Zhou, X. Ding, and R.Zhang, Extreme Learning Machine for Regression and Multiclass Classiﬁcation,\nIEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 42, 513-529 (2012).\n20. G. Van der Sande, D. Brunner, and M.C. Soriano, Advances in photonic reservoir computing, Nanophotonics 6,\n561–576 (2017).\n21. D. Brunner, M.C. Soriano, C. Mirasso, I. Fischer, Parallel photonic information processing at gigabyte per second\ndata rates using transient states, Nat. Commun. 4, 1364 (2013).\n22. Q. Vinckier, F. Duport, A. Smerieri, K. Vandoorne, P. Bienstman, M. Haelterman, and S. Massar, High-Performance\nPhotonic Reservoir Computer Based on a Coherently Driven Passive Cavity, Optica 2, 438 (2015).\n23. L. Larger, A. Baylón-Fuentes, R. Martinenghi, V.S. Udaltsov, Y.K. Chembo, and M. Jacquot, High-speed photonic\nreservoir computing using a time-delay based architecture: million words per second classiﬁcation, Phys. Rev. X 7,\n011015 (2017).\n24. D. Ballarini et al., Polaritonic neuromorphic computing outperforms linear classiﬁers, Nano Lett.20, 3506–3512\n(2020).\n25. A. Saade, F. Caltagirone, I. Carron , L. Daudet, A. Dremeau, S. Gigan, F. Krzakala, Random projections through\nmultiple optical scattering: approximating kernels at the speed of light, IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) 6215–6219 (2016).\n26. J. Bueno, S. Maktoobi, L. Froehly, I. Fischer, M. Jacquot, L. Larger, and D. Brunner, Reinforcement learning in a\nlarge-scale photonic recurrent neural network, Optica 5, 756–760 (2018).\n27. P. Antonik, N. Marsal, D. Brunner, and D. Rontani, Human action recognition with a large-scale brain-inspired\nphotonic computer, Nat. Mach. Intell. 1, 530–537 (2019).\n28. A. Röhm, L. Jaurigue, and K. Lüdge, Reservoir Computing Using Laser Networks, IEEE J. Sel. Top. Quantum\nElectron. 26, 1 (2020).\n29. U. Paudel, M. Luengo-Kovac, J. Pilawa, T.J. Shaw, and G.C. Valley, Classiﬁcation of time-domain waveforms using a\nspeckle-based optical reservoir computer, Opt. Express 28, 1225 (2020).\n30. M. Miscuglio, Z. Hu, S. Li, J. George, R. Capanna, P.M. Bardet, P. Gupta, and V.J. Sorger, Massively Parallel\nAmplitude-Only Fourier Neural Network, Optica 7, 1812 (2020).\n31. S. Sunada and A. Uchida, Photonic neural ﬁeld on a silicon chip: large-scale, high-speed neuro-inspired computing\nand sensing, Optica 8, 1388 (2021).\n32. M. Borghi, S. Biasi, and L. Pavesi, Reservoir computing based on a silicon microring and time multiplexing for\nbinary and analog operations, Sci. Rep. 11, 15642 (2021).\n33. X. Porte, A. Skalli, N. Haghighi, S. Reitzenstein, J. A. Lott, and D. Brunner, A complete, parallel and autonomous\nphotonic neural network in a semiconductor multimode laser, J. Physics: Photonics 3, 024017 (2021).\n34. D. Pierangeli, G. Marcucci, and C. Conti, Photonic extreme learning machine by free-space optical propagation,\nPhotonics Res. 9, 1446 (2021).\n35. U. Teğin, M. Yıldırım, İ. Oğuz, C. Moser, and D. Psaltis, Scalable optical learning operator, Nat. Comput. Sci. 1,\n542–549 (2021).\n36. D. Pierangeli, G. Marcucci, and C. Conti, (2021, August). Neuromorphic computing device using optical shock\nwaves, In OSA Nonlinear Optics 2021 (pp. NTh1A-3), OSA Technical Digest (Optica Publishing Group, 2021).\n37. Z. Denis, I. Favero , and C. Ciuti, Photonic Kernel Machine Learning for Ultrafast Spectral Analysis, Phys. Rev.\nAppl.17, 034077 (2022).\n38. M. Rafayelyan, J. Dong, Y. Tan, F. Krzakala, and S. Gigan, Large-Scale Optical Reservoir Computing for\nSpatiotemporal Chaotic Systems Prediction, Phys. Rev. X 10, 041037 (2020).\n39. S. Ortín, M. C. Soriano, L. Pesquera, D. Brunner, D. San-Martín, I. Fischer, C. R. Mirasso, and J. M. Gutiérrez, A\nuniﬁed framework for reservoir computing and extreme learning machines based on a single time-delayed neuron,\nSci. Reports 5 (2015).\n40. R. Mirek, A. Opala, P. Comaron, M. Furman, M. Król, K. Tyszka, B. Seredyński, D. Ballarini, D. Sanvitto, T.C.H.\nLiew, W. Pacuski, J. Suﬀczyński, J. Szczytko, M. Matuszewski, and B. Piętka, Neuromorphic binarized polariton\nnetworks, Nano Lett. 21, 3715–3720 (2021).\n41. A. Lupo and S. Massar, Parallel extreme learning machines based on frequency multiplexing, Appl. Sci. 12, 214\n(2021).\n42. G. Marcucci, D. Pierangeli, and C. Conti, Theory of neuromorphic computing by waves: machine learning by rogue\nwaves, dispersive shocks, and solitons, Phys. Rev. Lett. 125, 093901 (2020).\n43. H. Zhang at al., An optical neural chip for implementing complex-valued neural network, Nat. Commun. 12,\n457 (2021).\n44. A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, Learning word vectors for sentiment analysis,\nin Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language\nTechnologies, 142–150 (Association for Computational Linguistics, Portland, Oregon, USA, 2011).\n45. M. Belkin, D. Hsu, S. Ma, and S. Mandal, Reconciling modern machine-learning practice and the classical\nbias–variance trade-oﬀ, Proc. Natl. Acad. Sci. 116, 15849–15854 (2019).\n46. M. S. Advani, A. M. Saxe, and H. Sompolinsky, High-dimensional dynamics of generalization error in neural\nnetworks,Neural Networks 132, 428–446 (2020).\n47. S. Mei and A. Montanari, The generalization error of random features regression: Precise asymptotics and double\ndescent curve, arXiv:1908.05355 (2020).\n48. K. Babić, S. Martinčić-Ipšić, and A. Meštrović, Survey of neural text representation models, Information 11, 511\n(2020).\n49. A. Ashraﬁ, Walsh–hadamard transforms: A review, in Advances in Imaging and Electron Physics, 1-55 (Elsevier,\n2017).\n50. M. Soltanolkotabi, A. Javanmard, and J. D. Lee, Theoretical insights into the optimization landscape of overparame-\nterized shallow neural networks, IEEE Transactions on Inf. Theory 65, 742–769 (2019).\n51. D. Pierangeli, M. Rafayelyan, C. Conti, and S. Gigan, Scalable Spin-Glass Optical Simulator, Phys. Rev. Appl. 15,\n034087 (2021).\n52. M.-A. Miri, Integrated random projection and dimensionality reduction by propagating light in photonic lattices, Opt.\nLett 46, 4936 (2021).\n53. T. Mikolov, K. Chen, G. Corrado, and J. Dean, Eﬃcient estimation of word representations in vector space,\narXiv:1301.3781 (2013).\n54. A. Lacoste, A. Luccioni, V. Schmidt, and T. Dandres, Quantifying the carbon emissions of machine learning,\narXiv:1910.09700 (2019).\n55. T.B. Brown et al., Language Models are Few-Shot Learners, arXiv:2005.14165 (2020).\n56. A.U. Rehman et al., A hybrid CNN-LSTM model for improving accuracy of movie reviews sentiment analysis,\nMultimedia Tools and Applications 78, 26597 (2019).\n",
  "categories": [
    "cs.ET",
    "physics.optics"
  ],
  "published": "2022-08-29",
  "updated": "2022-08-29"
}