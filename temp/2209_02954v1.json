{
  "id": "http://arxiv.org/abs/2209.02954v1",
  "title": "A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform",
  "authors": [
    "Z. Jiang",
    "G. Song"
  ],
  "abstract": "With the development of industry, drones are appearing in various field. In\nrecent years, deep reinforcement learning has made impressive gains in games,\nand we are committed to applying deep reinforcement learning algorithms to the\nfield of robotics, moving reinforcement learning algorithms from game scenarios\nto real-world application scenarios. We are inspired by the LunarLander of\nOpenAI Gym, we decided to make a bold attempt in the field of reinforcement\nlearning to control drones. At present, there is still a lack of work applying\nreinforcement learning algorithms to robot control, the physical simulation\nplatform related to robot control is only suitable for the verification of\nclassical algorithms, and is not suitable for accessing reinforcement learning\nalgorithms for the training. In this paper, we will face this problem, bridging\nthe gap between physical simulation platforms and intelligent agent, connecting\nintelligent agents to a physical simulation platform, allowing agents to learn\nand complete drone flight tasks in a simulator that approximates the real\nworld. We proposed a reinforcement learning framework based on Gazebo that is a\nkind of physical simulation platform (ROS-RL), and used three continuous action\nspace reinforcement learning algorithms in the framework to dealing with the\nproblem of autonomous landing of drones. Experiments show the effectiveness of\nthe algorithm, the task of autonomous landing of drones based on reinforcement\nlearning achieved full success.",
  "text": "A Deep Reinforcement Learning Strategy for UAV Autonomous\nLanding on a Platform\nZ. Jiang1, G. Song1\n1School of Aerospace and Astronautics, Zhejiang University\nAbstract\nWith the development of industry, drones are appearing in various ﬁeld. In recent years, deep re-\ninforcement learning has made impressive gains in games, and we are committed to applying deep\nreinforcement learning algorithms to the ﬁeld of robotics, moving reinforcement learning algorithms\nfrom game scenarios to real-world application scenarios. We are inspired by the LunarLander of\nOpenAI Gym, we decided to make a bold attempt in the ﬁeld of reinforcement learning to control\ndrones. At present, there is still a lack of work applying reinforcement learning algorithms to robot\ncontrol, the physical simulation platform related to robot control is only suitable for the veriﬁcation\nof classical algorithms, and is not suitable for accessing reinforcement learning algorithms for the\ntraining. In this paper, we will face this problem, bridging the gap between physical simulation plat-\nforms and intelligent agent, connecting intelligent agents to a physical simulation platform, allowing\nagents to learn and complete drone ﬂight tasks in a simulator that approximates the real world. We\nproposed a reinforcement learning framework based on Gazebo that is a kind of physical simulation\nplatform (ROS-RL), and used three continuous action space reinforcement learning algorithms in\nthe framework to dealing with the problem of autonomous landing of drones. Experiments show\nthe eﬀectiveness of the algorithm, the task of autonomous landing of drones based on reinforcement\nlearning achieved full success.\nKeywords\nDeep reinforcement learning; Continuous action space; UAV autonomous landing;\nGazebo&ROS\n1\nIntroduction\nIn the recent years, the use of drones has become increasingly popular. multi-rotor drones demon-\nstrates its great potential in many scenarios, from drone disaster, relief logistics and warehousing to\na wide range of automated industries. However, these tasks are achieved by a relatively independent\ngroup of key components, each component is independent and complex. Researchers usually decom-\npose these components to study. Nearly every task has the component of autonomous landing of\ndrones [10][11]. Classic technology landing techniques have their many limitations, such as design of\nthe model, nonlinear approximation, as well as anti-interference and eﬀective computational aspects.\nAnd these classical technologies of autonomous landing have their great limitations, one algorithm\ncan be only applied in one drone model, such as PID, each model depends on a special tuple of PID\nparameters that relies on extensive manual experience[10].\nIn this area, machine learning technique proved to be an eﬀective means of overcoming diﬃculties,\nmachine learning has evolved into many complex application areas[9]. While traditional reinforcement\nlearning is mainly popular in the ﬁeld of games, and has less application in our real life.We want to\napply reinforcement learning methods in machine learning to the ﬁeld of drones, using reinforcement\nlearning to train drones to achieve autonomous landing tasks. There are many problems with training\nintelligent agents of drones in the real world, on the one hand training agents in the real world is\n1\narXiv:2209.02954v1  [cs.RO]  7 Sep 2022\nunsafe, on the other hand, we need a lots of drones, the cost is huge during the training process. But\nthe physical simulation platform can help us to overcome these diﬃculties. In order to make sure the\nauthenticity of simulation, we choose Gazebo as our simulation platform, Gazebo is a very powerful\nsimulator, it often be used to simulate robotic arm and other physical robotics, with the help of\nthe platform ,we can realization of motion simulation of drone. At the same time, PX4 is prefer to\nrecommend Gazebo as simulation simulator. It follows that the performance of Gazebo is reliable.\nWe choose Gazebo as the simulation platform here and perform reinforcement learning training of\nagents in Gazebo environment.Reinforcement learning algorithms can be divided into continuous\naction space algorithms and discrete action space algorithms according to the action space. Because\nthe control of drones is continuous control, we choose to use the continuous action space algorithm\nfor training. In this paper, continuous action space reinforcement learning algorithm, DDPG TD3\nSAC, are applied.\nIn our work, we used reinforcement learning (DDPG,TD3,SAC) to train drones to achieve au-\ntonomous landing and this task is deployed in our reinforcement learning framework based on Gazebo.\nUsing Gazebo Simulator and PX4 Emulator during the training process. Rodriguez-Ramos used Ro-\ntorS[13]as simulation framework to deal with autonomous landing tasks[2],RotorS is only a simulator,\nPX4 can not only support simulation but also live deployment, so PX4 considers more ﬂight details.\nThe remainder of the paper is organized as follows:Section 2 presents a brief introduction on the rein-\nforcement learning theory and brief explains the basics of TD3 algorithm and SAC algorithm. Section\n3 details the presentation and description of our Gazebo-based reinforcement learning framework(ROS-\nRL) and how to design scenarios and tasks for drones autonomous landings in Gazebo, then details\nthe description of the reinforcement learning algorithms combining with the tasks. In section 4, some\nexperiment results show the availability of our Gazebo-based reinforcement learning framework(ROS-\nRL) and comparative analysis of the performance of several algorithms used in the experiments based\non the experimental results.\n2\nBackground\nIn reinforcement learning, agents are used to interact with the environment, an agent gets a state in\nthe environment, an then uses this state to output an action, after that this action will be applied\nin the environment, environment will output the next state and the reward according to the current\naction. The goal of the agent is try to collect reward from the environment as much as possible.\nDuring the reinforcement learning process, agents constant interact with the environment in states\nand actions, adjust neural network parameters based on reward feedback from the environment,\nﬁnally, the agents learn a behavioral strategy that can result a high reward from the environment.\nThis is a process of adjusting the neural network parameters, and in this process of optimizing the\nparameters and ﬁnding the optimal solution, agents need to balance exploration and exploitation.\nExploration means there is a certain probability of not following the decision made by the agent in the\nbeginning of training, exploring the environment randomly or in other ways to ﬁnd a better solution.\nThe increase in exploration probability is more conducive to ﬁnding a better solution, but against\nthe sensitivity of decision making, and aﬀects training eﬃciency. agents only balance the problem\nof exploration-exploitation to ﬁnd the optimal strategy and the maximum cumulative reward in the\nprocess of interacting with the environment.\nReinforcement learning consists of three parts–policy, value function, model. Policies can map the\nstate of each environment to an action in the action space, they can be divided into two groups,\nstochastic policy and deterministic policy. Stochastic policy is π, π(a|s) = p(at = a|st = s), inputs a\nstatus s, outputs a series of probabilities, these probabilities represent probability of each action of\nthe agent. Deterministic policy, a∗= argamaxπ(a|s), outputs the action has the highest probability.\nThere are two kinds of value functions, one kind of value function is Vπ(s), the other kind of value\n2\nfunction is Qπ(s, a), The ﬁrst kind of value function Vπ(s) = Eπ[Gt|st = s] = Eπ[P∞\nk=0 Υkrt+k+1|st =\ns]means when we use policy π, how much value the agent can accumulate from status s to the end\nstatus; the second value function can be called Q function, Qπ(s, a) = Eπ [Gt | St = s, At = a] =\nEπ\nhP∞\nk=0 γkRt+k+1 | St = s, At = a\ni\n, this value function use the current state and current action\nto estimate expectations for future rewards. This way of calculating the value function is used in\nmany network structures such as TD3. The third part is the model, the model determines the state\nof the next step. The state of the next step depends on the current state and the current action\nchosen. The state transfer probabilities and reward functions of the model are generally deﬁned\nby the following form, transfer probabilities Pa\nss′ = P [St+1 = s′ | St = s, At = a], reward function\nRa\ns = E [Rt+1 | St = s, At = a].\nNow, we have the three elements of strategy, value function and model, we can implement the\ninteraction with the environment and the collection of experience to train our neural network model.\nGenerally speaking, our drone problem is a continuous state space problem. Because continuous states\nand actions generate a large action space and state space,which makes neural network optimization\nmore diﬃcult. Traditional reinforcement learning has the limitation of small state space and small\naction space, and is only applicable to problems in discrete state space. The addition of deep neural\nnetworks to reinforcement learning allows these problems to be solved eﬀectively,TD3 SAC are two\nsuccessful deep reinforcement learning algorithms.\nThe earliest continuous action space reinforcement learning algorithm would be DDPG, TD3 is fur-\nther improved and upgraded on the basis of DDPG, it is an Actor-Critic algorithm, it can eﬀectively\nsolve the problem of overestimation of DDPG and make the training process more stable, new noise\nexploration mechanisms are combined in TD3. TD3 designs two sets of Critic networks to solve the\nproblem of over-evaluation of the value function Q, two sets of networks are evaluated for behaviors\nseparately, and ﬁnally a lower Q value is taken as the ﬁnal Q value, and then an Actor network\nis used to make behavioral decisions. At the same time, there are also two Gaussian noises in the\nnetwork, a Gaussian noise is added to the action vector of Q(s,a) when the Critic network evaluates\nthe Q value, and a Gaussian noise is added to the original action after the action generated by Actor.\nGaussian noises help to balance the problem of exploration-exploitation.Adding gradient intercept\nto avoid the gradient of parameter update out of a certain range. Finally, Critic and Actor use\ndiﬀerent update frequencies to improve the stability of training. Updating network parameters uses\nsoft update, θ = τθ′ + (1 −τ)θ, instead of copy directly.\nSAC is also an Actor-Critic algorithm, it uses both kinds of value function models Vπ(s) and Qπ(s, a)\nmentioned above to evaluate the status value more accurate. On the other hand, action entropy is\nadded in the value function Vπ(s), action entropy encourages Actor to explore toward more random\ndirections. In this way, convergence time of the algorithm has been signiﬁcantly reduced. The ideas\nof two sets of Q-Critic network and soft update in TD3 algorithm are still applied in SAC. We share\nmore details in Section 3.4.\n3\nProposed Method\nIn this section, we present a complete explanation of our Gazebo-based reinforcement learning\nframework(ROS-RL). And using a case study to demonstrate the eﬀectiveness of our framework,\nan agent can be able to learn and control a drone for autonomous landing tasks using this frame-\nwork. In this framework, agent ﬁnds out the eﬀective strategy to manipulate the drone to land on\nthe parking platform through many failures. In this section we describe more details about these.\n3\n3.1\nReinforcement Learning Framework\nReinforcement learning algorithms have been well tested and validated in atari and other games.\nBut in some complex continuous state space and action space, there are still many question about\nhow it will perform in the physical continuous scenarios. However, the need to apply reinforcement\nlearning to real physical spaces is becoming increasingly urgent. Gazebo simulator, as the ﬁrst choice\nsimulator for simulating real robots, is widely used in the ﬁeld of robotics[3][4][5]and it is supported\nby the famous plug–Robot Operating System(ROS). The algorithm validated by ROS-Gazebo can\nbe easily transplant to realistic robot system controlled by ROS. So we designed a Gazebo-based\nreinforcement learning framework, and it can build up the bridge between the reinforcement learning\nalgorithm and physical simulation scenarios in Gazebo. It helps us to train our reinforcement learning\nalgorithms cost-eﬀectively and safely, and to observe how well the algorithms perform roughly when\ndeployed them into the physical real world.\nReinforcement learning framework mainly composed of Gazebo simulator, ROS plugs, environment\ninterface and agents. We can arrange robots in Gazebo to simulate our physical real world. We can\narrange drones to Gazebo, add landmarks, obstacles and other environmental elements and conﬁgure\ntheir materials, density, surface friction, collision elasticity and other kinds of detailed content. After\nthat is ROS plugs, ROS framework is a popular robot control system in the ﬁeld of robotics, it works\nas a node, ROS makes it easy to interact with the elements in the Gazebo world, include drones’\nstatus and modify the parameters about the drones. Environment interface is used to encapsulate\ndata coming from ROS and extract the valuable data for the agent, in the same way, we only need\nto submit the fundamental parameters when we want to control the drones in the gazebo, other\nparameters will be ﬁlled automatically by the environment interface. The last part is agents, various\nkind of algorithms belong to this part. Agents collect the environment status and rewards to adjust\nthe policy network, and output actions to collect more rewards. The basic structure of framework\nhas been show as Fig.1.\nFig. 1: The reinforcement learning framework based on Gazebo(ROS-RL), Gazebo simulator, ROS\nplug, environment interface, and agents. Reinforcement learning environment highlighting with red\nbackground\n3.2\nTasks And Scenarios Design\nAfter introducing the framework, we inject the reinforcement learning algorithm into the framework\nand arrange a simple environment(which includes a drone[Fig.2] from PX4 model library and a\nlandmark for landing[Fig.3]), then use the framework to training the agent and make it learn to\ncontrol drone to complete the landing task. In this task, the drone needs to reasonably adjust its\nﬂight speed and ﬂight attitude according to the self-information in the environment. Landing on the\nlandmark in a limited time step.\nIn this task, the drone is using classic model iris in PX4 models library. Landmark is a 4m4m0.1m\nbox, Landmarks consist of two types of areas, external area is a 4m4m green ﬂoor and middle area is\n4\nFig. 2: the drone models\nFig. 3: landing landmarks\na 0.5m0.5m red ﬂoor, landing on the red area gets a higher reward than the green area. Because the\nPX4 drone uses closed-loop simulation, ﬂight accidents from collisions and other causes during ﬂight\ncan lead to irreversible failure problems. To reduce the trouble of resetting the model, we removed\nthe collision property of the landing landmark. The height of the upper surface of the landmark is\n1m, the drone is considered to have landed on the landmark when the drone lands at a height of 1m.\nEach episode the drone starts landing at a height of 3m near the landmark, The mission ends when\nthe drone is below an altitude of 1m or after ﬁnishing 40 time steps. At the end of the mission, the\nUAV automatically returns to an area at an altitude of 3m near the landmark and prepares for the\nnext landing mission.\n3.3\nReinforcement Learning Problem Deﬁnition And Function Design\nIn the reinforcement learning, The tuples of experience plays a decisive role in convergence of algo-\nrithm. The design of status space, action space and reward value function aﬀect the convergence of\nthe algorithm and the speed of convergence directly.\nThe tuples of experience in reinforcement learning are consisted by status space s ∈S, action space\na ∈A and reward function r. In our experiment, we deﬁned the status spaceS like as Eqs.1.\nS = {px, py, pz, vx, vy, vz} (1)\nIn this equation, px, py, pz represent the oﬀset of the drone relative to the landing landmark on the\nx-axis, y-axis and z-axis at the current moment. vx, vy, vz represent the velocity of the drone relative\nto the landing landmark. These row data reads from the Gazebo simulation via the ROS interface\nand then encapsulates by environment interface for the agent. The action space A is deﬁned by\nEqs.2.\nA = {ax, ay} (2)\nIn this equation, ax, ay represent the velocity of the drone relative to the ground on the x-axis, y-\naxis.Agent generates ax, ay, and submit them to the FCU of drone in the Gazebo through environment\ninterface and ROS plug.The drone generates a certain pitch and roll angle under the control of FCU.\nIn this way, the drone generates a certain horizontal velocity on the x-axis and y-axes. Although\nthe agent is designed to solve the problem of three-dimensional continuum of action space. The\nappearance of az will make the solution space of the agent very large, it is not friendly to the\nconvergence of the algorithm.\nTherefore the velocity of the drone in the z-axis direction is not\ndirectly generated by the agent. It is generated by the linear equation az = α ∗pz, α is a hyper-\nparameter, the lower the altitude of the drone the slower the landing speed. At the same time, the\nlanding speed of the drone in the z-axis is also related to the ﬂight attitude of the drone, aﬀected by\nax, ay indirectly. Generally speaking, this landing task consists of a six-dimensional continuous state\nspace and a two-dimensional continuous action space.\nAfter that, we introduce the design of the reward function. Reward function is the most important\nfunction in reinforcement learning, it aﬀects the convergence of the algorithm and the speed of\n5\nconvergence directly. When agent receives a s, and outputs an a for the environment, environment\nfeedback a s and a reward r. Agent can get a positive reward in each step if agent gets closer to the\ntarget. Reward function is deﬁned by Eqs.3. and Eqs.4.\nshapingt = −100\nq\np2x + p2y + p2z −10\nq\nv2x + v2y + v2z −\nq\na2x + a2y + 10C (1 −|ax|) + 10C (1 −|ay|) (3)\nr = shapingt −shapingt−1 (4)\nEqs.3 reﬂects the importance weighting of each evaluation indicator to task completion, px, py, pz is\nthe relative position of the drone with respect to the landing landmark, it is also one of the most\nimportant evaluation indicators during the landing of drones, they are given the highest weight,\nvx, vy, vz are used to evaluate the speed of the drone relative to the landing landmark during the\nlanding process, these speeds determine whether the drone can land smoothly on top of the landmark,\nthe design of\nq\nv2x + v2y + v2z promotes a smooth landing process for the drone. ax, ay are the amount\nof control over the movements of the drone,\nq\na2x + a2y promotes that the agent can output the control\nspeed of each time step more smoothly. C is a hyper-parameter used to indicate whether the drone\nlanded on the ground target. C keeps a 0 value if the drone did not touch the ground target, when\nthe drone lands on the ground target, agent will get a bonus based on the amount of throttle on the\nx-axis y-axis. We expect the amount of throttle on the x-axis y-axis tend to zero when the drone\nlanding on the ground target. For this reason, agent can get a biggest reward if ax, ay tends to zero\nat the moment the drone hits the ground. Finally, discussing the design of shaping in Eqs.4, shaping\nis a popular design approach to accelerate the convergence of reinforcement learning. It improves\nthe convergence speed of reinforcement learning algorithms by passing information about the current\nprogress of the task to be solved shaping.[1]\n3.4\nReinforcement Learning Algorithm And Network Structure Design\nIn order to solve the problem of controlling the drone to land autonomously by reinforcement learning\nin the continuous state space and continuous action space. We conducted comparative experiments\nusing various reinforcement learning algorithms, it includes DDPG[6], TD3[7]and SAC[8].\n3.4.1\nDDPG\nDDPG is an improved algorithm based on the DQN algorithm. DQN gives up the Q-Table in the\nQLearning, and uses the neural network instead of the Q-table to deal with continuous states, but the\naction is still discrete. DDPG adds an actor to the DQN, which makes DDPG can output continuous\naction. And DDPG can deal with some problems with continuous action and continuous state. The\npurpose of the Actor is to output an appropriate action according to the state and the algorithm\nhopes agent can get bigger Q-value after putting the action into Critic network. So DDPG is an\nActor-Critic framework, Actor learns policy function and improves the possibilities of action that\ncan gets highest reward, Critic network learns action value function and try to evaluate the real\nreward for the action. DDPG inherits the idea of DQN and uses a two-group network structure, one\ngroup plays the learning network(parameters Q, u) and it needs to update the parameters frequently,\nanother group network plays as target network(parameters Q′, u′) and updates the parameters of the\nnetwork with a lower frequency. The loss of the Q-network is calculated as Eqs.5.\nLoss = (Q (st, at) −(rt + Q (st+1, µ (st+1))))2 (5)\nEach update changes the Q-network, which makes the network to be diﬃcult to converge, target net-\nwork(parameters Q′, u′) comes up to deal with this problem. The loss of the Q-network is calculated\nas Eqs.6.\nLoss = (Q (st, at) −(rt + Q′ (st+1, µ′ (st+1))))2 (6)\nIn this equation, only the parameters of the Q network are changing, which can improve the conver-\ngence speed of the network. Actor updates parameters with gradient ascent method(Eqs.7.).\n6\n∆θµ = ∇θµQ(s, a) = ∇aQ(s, a)∇θµa = ∇aQ(s, a)|a=µ(s) ∇θµµ(s) (7)\nDDPG uses the soft-update method to update the parameters in the network, not copies the parame-\nters of learning network to the target network but do a few adjustment on the original network(Eqs.8\nand Eqs.9).\nθQ′ = τθQ + (1 −τ)θQ′ (8) θµ′ = τθµ + (1 −τ)θµ′ (9)\n3.4.2\nTD3\nTD3 is an improved algorithm based on the DDPG. TD3 still uses the Actor-Critic structure in\nDDPG. TD3 designs two sets of Critic networks to relieve the Q-value overestimate problem in\nDDPG. Two sets of Critic network evaluate the Q-values at the same time and choose the smaller\nQ-value as valid value to ease overestimate. Besides, the action outputed by the Actor is added some\nnoise before evaluating the Q-value. After several updates, Critic network can evaluate the Q-value\nmore accurate with the help of noise.\nLoss1 = (Q1 (st, at) −(rt + mini=1,2Q′\ni (st+1, µ′ (st+1) + noise)))2 (10)\nLoss2 = (Q2 (st, at) −(rt + mini=1,2Q′\ni (st+1, µ′ (st+1) + noise)))2 (11)\nUpdating the parameters of Actor network depending on the Critic network, because keeping the\nsame frequency of updates is not good for Actor network to ﬁnd out the optimal strategy. So TD3\nuses delayed update strategy, only when Critic network is more stabilized, Actor will update its\nnetwork. The gradient of the Actor update is generated in the same way as the DDPG algorithm.\nTD3 has the same soft-update method as DDPG.\n3.4.3\nSAC\nSAC is also a reinforcement learning algorithm based on the Actor-Critic structural, the diﬀerence\nfrom other algorithms is that SAC tries to maximize the entropy of policy, which makes the strategy\nas random as possible and makes the agent to fully explore the state space. SAC performs well in a\nreal robot control task.\nThe SAC algorithm is more complex than the ﬁrst two algorithms.\nIt is consisted by an Actor\nnetwork and two V-Critic(a learning network and a target network) networks and two Q-networks(two\nlearning networks). The actor network inputs the environment states and outputs the probability\ndistribution of the action. V-Critic inputs the status of environment and Q-Critic inputs the status\nof environment and actions. Three kinds of networks use diﬀerent parameter update methods. For\nQ-network updates, calculate the value of t + 1 step V (st+1) through V-Critic network ﬁrstly, and\nthen sample key-Value pairs in which the action at can transfer the status st to status st+1 from the\nexperience pool. Calculate the Q-value Q(st, at) at t step, ideally it would satisfy the Eqs.12.\nQ(st, at) = rt+1 + γV (st+1) (12)\nIn reality, there are some deviations in network prediction. The left and right sides of the equation\ncannot be equal, the left side of the equation is closer to the real value, the value gap between the\nleft and right sides of the equation is the Loss of Critic network. Entropy is introduced in V-Critic.\nEntropy is used to describe the degree of chaos, and it is deﬁned by the Eqs.13.\nH = −P\nx p(x) log p(x) = P\nx p(x) log\n1\np(x) (13)\nThe updates of V-Critic depends on the Q-Critic network, Q- Critic evaluates the value of status\nst with the equation Ea′\nt∼π(·|st;θ) [mini=0,1 qi (st, a′\nt)] ≈V (St), V (St) adds an item of entropy (H =\n−P\nx p(x) log p(x)) to help the V-Critic explores the space more fully. The expectation of V (St) is\nEqs.14. αis a coeﬃcient indicates the importance.\nV (St) = Ea′\nt∼π(·|st;θ) [mini=0,1 qi (st, a′\nt) −α ln π (a′\nt | st; θ)] (14)\nThe higher the entropy of the action generated by the policy network n Eqs.14, the value V (St)\nhigher.\nTherefore, the entropy in V-Critic can encourage the agent to explore the space.\nThe\nreal value can be calculated by the right side of Eqs.14. Left side of Eqs.14 is the predicted value\n7\nof V-Critic network, the diﬀerence between left and right is the loss of V- Critic. Actor updates\nthe parameters using the gradient ascent method, ∆θ = ∇θEa′\nt∼π(·|st;θ) [q0 (st, a′\nt) −α ln π (a′\nt | st; θ)].\nSAC uses soft-update method to refresh the parameters from V-Critic learning network to V-Critic\ntarget network , θV ′ = τθV + (1 −τ)θV ′\n.\n4\nExperiment\nIn this section, we will present the design details of the experiment include the environmental plat-\nform for experiments, construction and design of simulation scenarios and training process of drone.\nFinally, the results of the experiment are summarized and discussed.\n4.1\nExperimental conﬁguration introduction\nThe simulation platform builds up on Ubuntu 20.04 and using ROS neotic. Simulation environment\nusing Gazebo11, PX4 as ﬂight control software. Reinforcement learning neural network constructed\nand trained using torch 1.11.0 and accelerated training process with Nvidia GTX 1050 Ti\n4.2\nSimulation scenarios introduction\nThe simulation scenario was created in Gazebo 11 and the scenario contains a UAV model and a\nlanding platform (see Fig.4). The drone comes from PX4 open-source model library, PX4 prepared\nFig. 4: Simulation scenarios, drone and landing platform in the training process\nvarious kinds of drone models and sensors include IMU, depth cameras, RGB cameras and so on.\nWe chose the general model iris to accomplish the autonomous landing mission. In order to realise\nthe interaction between the agent and simulation scenarios, we read and launch relevant data(the\nﬂight status of the drone and the control commands of the drone) through the environment interface,\nenvironment interface to the virtual world through the ROS plug, the raw data of the virtual world is\nobtained through ROS topics, and then the valid data is extracted and encapsulated for submission\nto the experience pool of the agent. In the same way, control commanders are un-encapsulated and\nsubmit to corresponding topics about drones controlling through ROS plug. The entire interactive\ntraining process takes place in Gazebo, interaction is performed at a frequency of 10Hz.\n8\n4.3\nExperience Results And Discussion\nIn this section, the above three algorithms are validated to check their eﬀectiveness in controlling\nthe landing of drones. Then the reinforcement learning algorithm is evaluated in terms of training\ntime and control eﬀectiveness of the drones. In experiments, we trained the three reinforcement\nlearning algorithms for more than 1000 episodes under the same conditions. The reward value curves\nof three algorithms are shown in the Fig.5, Fig.6, Fig.7. Experimental results show that DDPG\nunable to converge, can not solve the problem of controlling the autonomous landing of drones in\na 3D environment. TD3 can complete the task of controlling the autonomous landing of the drone\nthrough training, TD3 has good convergence, but needs long training time and the exploration of\nspace is insuﬃcient. We can ﬁnd out in Fig.6 that TD3 can complete autonomous landing of the\ndrone after 1200 episode of training and the reward value reaches its highest point, because of the\nnoise in TD3 algorithm, the drone landing is not smooth, the smoothness evaluation term in the\nreward function is aﬀected, the convergence of the reward value is lower than the SAC algorithm.\nIn the SAC algorithm, entropy is considered to encourage the agent to explore fully in the space. In\nthis way, SAC reaches convergence in a shorter time, TD3 algorithm needs 1200 episodes to learn\nthe strategy of landing, but SAC algorithm only need 300 episodes and its reward can reach a higher\npoint. SAC can learn the landing strategy of the drone in a shorter time. But SAC uses the idea of\nmaximum entropy to encourage exploration, and the algorithm is still in high intensity exploration\nafter the convergence. Which results in a convergence curve that is not as stable as TD3 after 300\nepisodes. Optimizing the annealing method of the weight α before the entropy term can improve the\nproblem of unstable convergence.\nFig. 5: Reward per episode of\nDDPG\nFig. 6: Reward per episode of\nTD3\nFig. 7: Reward per episode of\nSAC\n5\nConclusions And Future Work\nIn this paper, we created a reinforcement learning framework based on Gazebo simulator and used\nthe ROS as data interaction interfaces, follow by that, we use reinforcement learning to solve the\nproblem of autonomous drone landing. We accessed three continuous action space reinforcement\nlearning algorithms to the ROS-RL framework. Training and comparing the eﬀectiveness of three\nalgorithms to control the autonomous landing of drones. The results show that two of the three\nalgorithms successfully learned the strategy to control the autonomous landing of the drones after\ntraining and the two algorithms have advantages and disadvantages in terms of convergence stability\nand convergence time of the training process. In the nearly future, we can apply this framework to\nmore complex scenarios and use reinforcement learning to allow agents to learn solution strategies of\nother complex tasks in a simulation environment. At the same time, ROS as a popular interface for\nphysical robot control, our simulation framework uses the ROS interface for training. It is relatively\neasy to transfer the algorithms trained in the virtual simulation scenario to the real scenario. We\nwill explore the ”Simulation training, Live deployment ” solutions[12], make the maximize beneﬁts\nof ROS-RL.\n9\n6\nReference\n[1]Dorigo M, Colombetti M. Robot shaping: an experiment in behavior engineering[M]. MIT press,\n1998.\n[2]Rodriguez-Ramos A, Sampedro C, Bavle H, et al. A deep reinforcement learning strategy for UAV\nautonomous landing on a moving platform[J]. Journal of Intelligent & Robotic Systems, 2019, 93(1):\n351-366.\n[3]Polvara R, Patacchiola M, Sharma S, et al.\nToward end-to-end control for UAV autonomous\nlanding via deep reinforcement learning[C]//2018 International conference on unmanned aircraft\nsystems (ICUAS). IEEE, 2018: 115-123.\n[4]Imanberdiyev N, Fu C, Kayacan E, et al. Autonomous navigation of UAV by using real-time\nmodel-based reinforcement learning[C]//2016 14th international conference on control, automation,\nrobotics and vision (ICARCV). IEEE, 2016: 1-6.\n[5]Yu X, Fan Y, Xu S, et al. A self-adaptive SAC-PID control approach based on reinforcement\nlearning for mobile robots[J]. International Journal of Robust and Nonlinear Control, 2021.\n[6]Lillicrap T P, Hunt J J, Pritzel A, et al. Continuous control with deep reinforcement learning[J].\narXiv preprint arXiv:1509.02971, 2015.\n[7]Fujimoto S, Hoof H, Meger D. Addressing function approximation error in actor-critic meth-\nods[C]//International conference on machine learning. PMLR, 2018: 1587-1596.\n[8]Haarnoja T, Zhou A, Abbeel P, et al. Soft actor-critic: Oﬀ-policy maximum entropy deep rein-\nforcement learning with a stochastic actor[C]//International conference on machine learning. PMLR,\n2018: 1861-1870.\n[9]Janousek J, Marcon P, Klouda J, et al. Deep Neural Network for Precision Landing and Variable\nFlight Planning of Autonomous UAV[C]//2021 Photonics & Electromagnetics Research Symposium\n(PIERS). IEEE, 2021: 2243-2247.\n[10]Moriarty P, Sheehy R, Doody P. Neural networks to aid the autonomous landing of a UAV on a\nship[C]//2017 28th Irish Signals and Systems Conference (ISSC). IEEE, 2017: 1-4.\n[11]Ciabatti G, Daftry S, Capobianco R. Autonomous Planetary Landing via Deep Reinforcement\nLearning and Transfer Learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 2021: 2031-2038.\n[12]Zhao T, Jiang H. Landing system for AR. Drone 2.0 using onboard camera and ROS[C]//2016\nIEEE Chinese Guidance, Navigation and Control Conference (CGNCC). IEEE, 2016: 1098-1102.\n[13]Furrer F, Burri M, Achtelik M, et al.\nRotorS—A modular gazebo MAV simulator frame-\nwork[M]//Robot operating system (ROS). Springer, Cham, 2016: 595-625.\n10\n",
  "categories": [
    "cs.RO",
    "cs.AI"
  ],
  "published": "2022-09-07",
  "updated": "2022-09-07"
}