{
  "id": "http://arxiv.org/abs/2209.05627v1",
  "title": "SENDER: SEmi-Nonlinear Deep Efficient Reconstructor for Extraction Canonical, Meta, and Sub Functional Connectivity in the Human Brain",
  "authors": [
    "Wei Zhang",
    "Yu Bao"
  ],
  "abstract": "Deep Linear and Nonlinear learning methods have already been vital machine\nlearning methods for investigating the hierarchical features such as functional\nconnectivity in the human brain via functional Magnetic Resonance signals;\nhowever, there are three major shortcomings: 1). For deep linear learning\nmethods, although the identified hierarchy of functional connectivity is easily\nexplainable, it is challenging to reveal more hierarchical functional\nconnectivity; 2). For deep nonlinear learning methods, although non-fully\nconnected architecture reduces the complexity of neural network structures that\nare easy to optimize and not vulnerable to overfitting, the functional\nconnectivity hierarchy is difficult to explain; 3). Importantly, it is\nchallenging for Deep Linear/Nonlinear methods to detect meta and sub-functional\nconnectivity even in the shallow layers; 4). Like most conventional Deep\nNonlinear Methods, such as Deep Neural Networks, the hyperparameters must be\ntuned manually, which is time-consuming. Thus, in this work, we propose a novel\ndeep hybrid learning method named SEmi-Nonlinear Deep Efficient Reconstruction\n(SENDER), to overcome the aforementioned shortcomings: 1). SENDER utilizes a\nmultiple-layer stacked structure for the linear learning methods to detect the\ncanonical functional connectivity; 2). SENDER implements a non-fully connected\narchitecture conducted for the nonlinear learning methods to reveal the\nmeta-functional connectivity through shallow and deeper layers; 3). SENDER\nincorporates the proposed background components to extract the sub-functional\nconnectivity; 4). SENDER adopts a novel rank reduction operator to implement\nthe hyperparameters tuning automatically. To further validate the\neffectiveness, we compared SENDER with four peer methodologies using real\nfunctional Magnetic Resonance Imaging data for the human brain.",
  "text": " \n \nSENDER: SEmi-Nonlinear Deep Efficient Reconstructor \nfor Extraction Canonical, Meta, and Sub Functional \nConnectivity in the Human Brain \n \nWei Zhang  \nAugusta University \nwzhang2@augusta.edu \n \nYu Bao \nJames Madison University \nbao2yx@jmu.edu \nAbstract \nDeep Linear and Nonlinear learning methods have already been vital machine \nlearning methods for investigating the hierarchical features such as functional \nconnectivity in the human brain via functional Magnetic Resonance signals; \nhowever, there are three major shortcomings: 1). For deep linear learning methods, \nalthough the identified hierarchy of functional connectivity is easily explainable, \nit is challenging to reveal more hierarchical functional connectivity; 2). For deep \nnonlinear learning methods, although non-fully connected architecture reduces the \ncomplexity of neural network structures that are easy to optimize and not \nvulnerable to overfitting, the functional connectivity hierarchy is difficult to \nexplain; 3). Importantly, it is challenging for Deep Linear/Nonlinear methods to \ndetect meta and sub-functional connectivity even in the shallow layers; 4). Like \nmost conventional Deep Nonlinear Methods, such as Deep Neural Networks, the \nhyperparameters must be tuned manually, which is time-consuming. \nThus, in this work, we propose a novel deep hybrid learning method named SEmi-\nNonlinear Deep Efficient Reconstruction (SENDER), to overcome the \naforementioned shortcomings: 1). SENDER utilizes a multiple-layer stacked \nstructure for the linear learning methods to detect the canonical functional \nconnectivity; 2). SENDER implements a non-fully connected architecture \nconducted for the nonlinear learning methods to reveal the meta-functional \nconnectivity through shallow and deeper layers; 3). SENDER incorporates the \nproposed background components to extract the sub-functional connectivity; 4). \nSENDER adopts a novel rank reduction operator to implement the \nhyperparameters tuning automatically.  \nTo further validate the effectiveness, we compared SENDER with four peer \nmethodologies using real functional Magnetic Resonance Imaging data for the \nhuman brain. Furthermore, the validation results show that SENDER outperforms \nthe investigated methodologies regarding the reconstruction of identifiable \ncanonical, meta, and sub-functional connectivity in the human brain, as well as \nefficiency and identifiability. \n \n2 \n \n1 \nIntroduction \nThe hierarchy of functional organization in the human brain [1-4] has been revealed by multiple deep \nlinear machine learning techniques, such as Low-to High-Dimensional Independent Components \nAnalysis (DICA) [5], Sparse Deep Dictionary Learning (SDDL) [6], Deep Non-negative Matrix \nFactorization (DNMF) [7], [8]. In addition, with the development of deep learning methods, a variety \nof deep nonlinear methods known as Deep Neural Networks (DNNs), e.g., Deep Convolutional Auto \nEncoder (DCAE), Restricted Boltzmann Machine (RBM), Deep Belief Network (DBN), and \nConvolutional Neural Network (CNN) [9-15], have been applied to discovering the hierarchical \nspatial features in brain, i.e., functional connectivity (FC), using functional Magnetic Resonance \nImaging (fMRI). For instance, the RBM can extract hierarchical temporal features and effectively \nreconstruct FC networks with high accuracy [16, 17]. Furthermore, other latest research works have \nfound the reasonable hierarchical temporal organization of task-based fMRI time series, each with \ncorresponding task-evoked FCs [9, 10, 16, 17] using DCAE, RBM, and DBN. These deep learning \ntechniques are generally considered deep nonlinear methods, e.g., DNNs, constructed with nonlinear \nactivation functions, e.g., Sigmoid and/or Rectified Linear Unit (ReLU) [18].  \nFrom a neuroscience perspective, the reveal of FCs by methodologies can be divided into two folds: \n1). Deep Linear methods, e.g., DNMF, and SDDL, usually focus on identifying meta-FCs explained \nas the recombination of canonical FCs reported by Smith in 2009 [19]; 2). On the other hand, Deep \nNonlinear methods, e.g., DNNs, concentrate on investigating both meta- and sub-FCs, i.e., the lower-\nlevel features. Due to the utilization of the nonlinear activation functions, Deep Nonlinear Methods \nusually have a more substantial perception of meta-FCs than Deep Linear methods [12], [14]. \nHowever, these meta-FCs are often detected at the deeper layer in most Deep Linear/Nonlinear \nmethods and are more difficult to interpret and/or explain [12], [14].  \nTherefore, we aim to propose a more advanced deep hybrid method that can benefit from both Deep \nLinear and nonlinear methods to simultaneously discover more identifiable canonical, meta, and sub-\nFCs from the shallow and deep layers. From a technical perspective, SENDER inherits the vital \nadvantage of a deep linear method that it is easy to be optimized and does not require large training \nsamples due to the convex target function, and the advantage of the deep nonlinear method such as \nmore substantial perception using activation functions, and efficient non-fully connected \narchitectures; in contrast, a fully connected structure in deep nonlinear methods enables the \nsubstantial perception but also involves in the difficult training issues, requiring large training \nsamples to avoid overfitting and advanced optimizer to search the global optimum. \nFurthermore, a more advanced deep hybrid method should overcome other shortcomings of deep \nnonlinear learning methods, such as DBN, Deep Boltzmann Machine (DBM), and DCAE, such as \n1) large training samples [19-22]; 2) extensive computational resources, e.g., graphics processing \nunits (GPUs) or tensor processing units (TPUs) [10, 11, 13]; 3) manual tuning of all hyperparameters, \ne.g., the number of layers and size of a dictionary, whereas some parameters such as sparse trade-off \nand step-length, are not denoted as hyperparameters [8-10]; 4) time-consuming training process [13, \n14]; 5) uncertainty of convergence to the global optimum [13, 14, 19, 20, 22]; and 6) \"black box\" \nresults that are challenging to explain [13, 14, 19].  \nContributions. To be more specific, as follow: \n1). Accurate Approximation to Original Input. We have proved that SENDER provides a comparable \naccuracy for approximation to original inputs to DNNs. Theorem 1.1 in Appendix A, Supplementary \nMaterial, presents the conclusion and proof details. \n2). Advanced Hybrid Architecture. Unlike any deep linear or nonlinear method, SENDER employs \na deep linear and nonlinear method with non-fully connected architectures, which provides an \nopportunity to explore the meta-FCs and the sub-FCs in the brain by introducing the background \nfeature matrix. Theorem 1.2 in Appendix A, Supplementary Material, discusses the theoretical \nanalysis of the advantages of the proposed deep hybrid architecture for learning. \n3). Fast Convergence Rate. Given STORM [26] is used as an efficient optimizer to update all \nvariables of SENDER, our theoretical analysis demonstrates that SENDER can maintain the same \nconvergence rate as STORM itself, according to Theorem 1.3, Appendix A, Supplementary Material. \nMoreover, since the dimension of a dataset is continuously increasing, we also discuss the \nconvergence of SENDER in an infinite dimensionality space, where all proofs can be found in \nCorollary 1.4 in Appendix A, Supplementary Material. \n3 \n \n4). Automatic Hyperparameters Tuning. To implement the automatic tuning of hyperparameters, \ne.g., number of layers and number of units/neurons in each layer [23, 24], we develop a rank \nreduction technique named rank reduction operator (RRO) for SENDER. Specifically, RRO utilizes \nthe orthogonal decomposition, e.g., QR decomposition, to estimate the rank of feature matrices, i.e., \nthe number of units, via the weighted ratio (WR), the weighted difference (WD), and the weighted \ncorrelation (WC). These three techniques can consistently reduce the size of feature matrices through \nall layers until the estimated number of features equals one, suggesting the completion of \ndecomposition. Thus, due to the generalization and efficacy of QR decomposition, RRO can tune all \nhyperparameters faster than Singular Value Decomposition (SVD) adopted by Principal Component \nAnalysis (PCA) [23, 24]. The details of RRO implementation can be viewed as Algorithms 2.3 to \n2.5, Appendix B, in Supplementary Material. In addition, the theoretical analytics of RRO can be \nviewed as Theorem 2.1, included in Appendix B, Supplementary Material. \n4). Reduced Accumulative Error via Matrix Backpropagation (MBP). Given that the accumulative \nerror could potentially deteriorate the reconstruction accuracy, we utilize a technique MBP [6, 7, 25] \nto reduce the accumulative error. The implementation of MBP can be found in Algorithm 2.5 in \nAppendix B, Supplementary Material. \nRelated Works and Methodological Validation. SENDER is validated on real resting-state fMRI \nsignals and compared with four other peer methods. The validation results show that SENDER can \ndetect more identifiable canonical, meta, and sub-FCs in the human brain than the representative \ndeep linear/nonlinear methods and is easier to be optimized than DNNs based on our theoretical \nanalyses in Theorem 1.2. Furthermore, unlike most deep learning methods, some meta-FCs can be \nderived from SENDER even at shallow layers. Moreover, sub-FCs can be directly extracted from the \nbackground component matrix, whereas minor features, e.g., sub-FCs, are detected through shallow \nto deeper layers in most deep nonlinear methods. \n \n2 \nMethod \nThis section provides the details of SENDER, including optimizer, optimization function, automatic \nhyperparameters tuning technique, MBP, theoretical analysis of the convergence rate, and \napproximate accuracy. \n \n2.1         Semi-Estimated Nonlinear Deep Efficient Reconstructor \nAs introduced, SENDER employs a hybrid architecture of Deep Linear/Nonlinear methods with an \nefficient non-fully connected architecture and the nonlinear activation function used in DNNs. In \ndetail, the optimization function governing SENDER is shown below: \n𝑚𝑖𝑛 𝑆𝑖 ∈ ℝ𝑚×𝑛 ⋃‖𝑆𝑖‖1\n𝑘\n𝑖=1\n \n𝑠. 𝑡. ∀𝑘∈[2, 𝑀], (∏𝑋𝑖\n𝑘\n𝑖=1\n) ∙𝑌𝑘+ (∏𝑈𝑖\n𝑘\n𝑖=1\n) ∙𝒩𝑘∙𝑉𝑘+ 𝑆𝑘= 𝐼 \n(∏𝑋𝑖\n𝑗\n𝑖=1\n) ∙𝑌𝑗+ (∏𝑈𝑖\n𝑗\n𝑖=1\n) ∙𝒩𝑗∙𝑉𝑗+ 𝑆𝑗←𝐼−𝑆𝑗−1, ∀ 1 ≤𝑖< 𝑗≤𝑘 \n \n \n(1) \nwhere {𝑋𝑖}𝑖=1\n𝑘\n represents the hierarchical weight matrices or mixing matrices of the linear method, \nfor instance, 𝑋𝑖 indicates the weight matrix of linear method of at the ith layer,  and 𝑘 denotes the \ntotal number of layers. similarly, {𝑈𝑖}𝑖=1\n𝑘\n denotes the weight matrices for nonlinear method at ith \nlayer. Furthermore, {𝑌𝑖}𝑖=1\n𝑘\n represents the canonical or meta-FCs derived via linear method; for \ninstance, 𝑌𝑖 indicates the canonical or meta-FCs of the ith layer; and meanwhile {𝑉𝑖}𝑖=1\n𝑘\n defines the \nmeta-FCs revealed via the nonlinear method. Furthermore, {𝑆𝑖}𝑖=1\n𝑘\n is a set of matrices that represent \nthe background components, which are denoted as sub-FCs, due to their sparsity. Moreover, 𝒩𝑖 \nrepresents the nonlinear activation function at the ith layer, e.g., Sigmoid or Rectified Linear Unit \n4 \n \n(ReLU) [14, 20]. And, if we assume the total number of layers as 𝑀, the original input data 𝐼 can be \ndecomposed following: (∏\n𝑋𝑖\n𝑀\n𝑖=1\n) ∙𝑌𝑀+ (∏\n𝑈𝑖\n𝑀\n𝑖=1\n) ∙𝒩𝑀∙𝑉𝑀+ 𝑆𝑀. \nAs shown in Eq. (1), our fundamental assumption is the previously revealed FCs, e.g., 𝑌𝑖−1 or 𝑉𝑖−1, \ncan be decomposed further as a linear product of a deeper weight matrix 𝑋𝑖 and FCs as 𝑌𝑖 or a \nnonlinear representation of a deeper weight matrix 𝑈𝑖 and FCs as 𝑉𝑖, respectively. In addition, the \noptimization function governing SENDER consists of more variables than conventional deep \nlinear/nonlinear methods, such as DICA, DNMF, and SDDL.  \nBefore optimizing Eq. (1), we can convert it into an augmented Lagrangian function. If considering \nthe kth layer, we have: \nℒ𝜌(∏𝑋𝑖\n𝑘\n𝑖=1\n, 𝑌𝑘,  𝑆𝑘, 𝒩𝑘≝𝜌\n2 ‖𝐼− (∏𝑋𝑖\n𝑘\n𝑖=1\n) ∙𝑌𝑘−(∏𝑈𝑖\n𝑘\n𝑖=1\n) ∙(𝒩𝑘∙𝑉𝑘)‖\n𝐹\n2\n+ 1\n𝜌‖𝑆𝑘‖1 \n(2) \nThe sparse trade-off controlling the sparsity of background components denoted as ⋃\n‖𝑆𝑖‖1\n𝑘\n𝑖=1\n is \ndetermined by \n1\n𝜌 that can also be estimated using Rose Algorithm [27]. Naturally, it is easier to \nemploy alternative optimization strategies [25, 26] to optimize Eq. (2). Due to the efficacy of the \nrecent reported STORM optimizer [27], we adopt STORM to update all the variables included in \nSENDER. And the ℓ1 norm of 𝑆𝑘 shown in Eq. (1) can be solved directly using the shrinkage method \n[28]. \nDenote STORM [26] as an operator 𝒯. The iterative format of STORM using an alternative strategy \nof optimization to update all the variables in Eq. (2) can be presented as follows: \n𝑋𝑘\n𝑖𝑡+1 ← 𝒯∙𝑋𝑘\n𝑖𝑡 \n(3-1) \n 𝑌𝑘\n𝑖𝑡+1 ←𝒯∙𝑌𝑘\n𝑖𝑡 \n(3-2) \n 𝑈𝑘\n𝑖𝑡+1 ←𝒯∙𝑈𝑘\n𝑖𝑡 \n(3-3) \n 𝑉𝑘\n𝑖𝑡+1 ←𝒯∙(𝒩𝑘∙𝑉𝑘\n𝑖𝑡) \n(3-4) \n 𝑆𝑘\n𝑖𝑡+1 ←𝑆ℎ𝑟𝑖𝑛𝑘𝑎𝑔𝑒 [𝐼− (∏𝑋𝑘\n𝑖𝑡\n𝑘\n𝑖=1\n) ∙𝑌𝑘\n𝑖𝑡−(∏𝑈𝑘\n𝑖𝑡\n𝑘\n𝑖=1\n) ∙(𝒩𝑘∙𝑉𝑘\n𝑖𝑡)] \n \n(3-5) \nIn detail, in Eqs. (3-1) to (3-4), the current iteration is represented as it; for example, in Eq. (3-1), \n𝑋𝑘\n𝑖𝑡 is updated by the optimizer STORM while 𝑌𝑘\n𝑖𝑡, 𝑈𝑘\n𝑖𝑡 , and 𝑉𝑘\n𝑖𝑡  are treated as constants; similar \nmechanism applies to 𝑌𝑘\n𝑖𝑡, 𝑈𝑘\n𝑖𝑡, and 𝑉𝑘\n𝑖𝑡. Finally, Eq. (3-5) demonstrates the shrinkage and \nminimization of the background components, i.e., sub-FCs denoted as 𝑆𝑘\n𝑖𝑡+1. \n \n2.2       Rank Reduction Operator for Automatic Tuning Hyperparameters \nTo implement the automatic hyperparameters tuning and reduce the high dimensionality of the \noriginal dataset, we introduce a novel technique named RRO which aims to calculate the rank of the \ncurrent feature matrices, e.g., {𝑌𝑖}𝑖=1\n𝑘\n and {𝑉𝑖}𝑖=1\n𝑘\n in SENDER, until the rank of the feature matrices \nare equivalent to one. More specifically, RRO performs rank-revealing by consistently using \northogonal decomposition via QR factorization to efficiently estimate the rank of feature matrices \n[23, 24]. Due to the effectiveness of QR factorization, RRO can be used to decompose sparse and \novercomplete matrices. The mathematical formula of RRO is: \nℛ\n[\n \n \n \n \n𝑎1\n𝑎2\n⋮\n𝑎𝑛−1\n𝑎𝑛]\n \n \n \n \n=\n[\n \n \n \n \n 𝑎1\n(1)\n𝑎2\n(1)\n⋮\n𝑎𝑛−2\n(1)\n𝑎𝑛−1\n(1)]\n \n \n \n \n \n ℛ𝑘\n[\n \n \n \n \n𝑎1\n𝑎2\n⋮\n𝑎𝑛−1\n𝑎𝑛]\n \n \n \n \n=\n[\n \n \n \n \n \n𝑎1\n(1)\n𝑎2\n(1)\n⋮\n𝑎𝑛−𝑘−1\n(1)\n𝑎𝑛−𝑘\n(1) ]\n \n \n \n \n \n= [𝑎̂] \n \n(4) \nwhere operator ℛ represents RRO, and {𝑎𝑖}𝑖=1\n𝑛 is a series of vectors with 𝑎𝑖 representing a single \nvector. Assume RRO is repeatedly applied on a series of vectors {𝑎𝑖}𝑖=1\n𝑛\n for k times, we have \n𝑟𝑎𝑛𝑘(ℛ𝑘∙[𝑎1, 𝑎2, ⋯, 𝑎𝑛]) < 𝑟𝑎𝑛𝑘([𝑎1, 𝑎2, ⋯, 𝑎𝑛]) hold; furthermore, if 𝑘 is large enough, e.g., \n∀𝑘> 0, ∃ 𝑁∈ℕ, when 𝑘> 𝑁, 𝑟𝑎𝑛𝑘(ℛ𝑘∙[𝑎1, 𝑎2, ⋯, 𝑎𝑛]) = 1 holds, thus 𝑘 is treated as the total \nnumber of layers since the rank of the feature matrix at kth layer equals one after utilizing RRO for k \n5 \n \ntimes repeatedly. Moreover, we prove, if ℛ: ℝ𝑀×𝑁→ℝ𝑀×𝑁, 𝑀< ∞,  𝑁< ∞, then we have ‖ℛ‖ <\n∞, meaning the operator ℛ, i.e., RRO technique, is a bounded operator denoted in a finite-\ndimensional space. The detail of the proof can be viewed in Theorem 2.2, Appendix B, \nSupplementary Material. \nAssume r* is the initially estimated rank and r is the optimal rank estimation of the input signal \nmatrix 𝐼, we have r*≥r; the diagonal of the upper-triangular matrix can be achieved after applying \nQR factorization on signal matrix 𝐼. In detail, at first, the diagonal of matrix R, derived from the \nfeature matrix using QR decomposition, is non-increasing in magnitude [23, 24]; furthermore, along \nthe main diagonal of matrix R, three techniques named WR, WD, and WC are applied to calculate the \nmaximum rank shown in Eqs (5)-(7); then, 𝐼 is replaced by 𝐼−𝑆𝑘, 𝑘= 1,2, ⋯, 𝑀, iteratively. It \nindicates that the rank-reducing technique can yield a reasonable solution using QR factorization \n[23, 24]. The following formulas provide details of WR, WD, and WC.  \nAssume 𝑑∈ℝ1×𝑟 and 𝑟∈ℝ𝑟−1,  then WR can be calculated by Eq. (5): \n𝑑𝑖←|𝑅𝑖𝑖| \n𝑤𝑟𝑖←𝑑𝑖\n𝑑𝑖+1\n \n (5) \nwhere 𝑅𝑖𝑖 denotes a diagonal element of matrix R calculated by QR decomposition and 𝑤𝑟𝑖 is an \nelement of WR. The value of each WR is derived by the ratio of the current element of diagonal and \nthe next element as shown in Eq. (5). \nSimilarly, WD can be derived by: \n𝑤𝑑𝑖←\n|𝑑𝑖−𝑑𝑖−1|\n∑\n𝑑𝑘\n𝑖−1\n𝑘=1\n \n(6) \nIn Eq. (6), WD is defined as the absolute difference between the current diagonal element and the \nprevious one divided by the cumulative sum of all the previous diagonal elements. \nFurthermore, Eq. (7) describes the proposed WC as: \n𝑟𝑖←|𝑅𝑖|, 1 ≤𝑖≤𝑛 \n𝑤𝑐𝑖←\n|𝑐𝑜𝑟𝑟(𝑟𝑖−2, 𝑟𝑖−1) −𝑐𝑜𝑟𝑟(𝑟𝑖−1, 𝑟𝑖)|\n∑\n‖𝑟𝑘‖2\n2\n𝑖\n𝑘=𝑖−2\n, 3 ≤𝑖≤𝑛  \n(7) \nwhere 𝑤𝑐𝑖 represents an element of WC and is the ratio of the absolute difference of three adjacent \ncolumns and the summed of all vectors’ ℓ2 norm. \nThus, the RRO iteratively calculates the maximum value position from WR, WD, and WC to estimate \nthe rank of R. The details of pseudocodes to implement WR, WD, and WC can be viewed in \nAlgorithm 2.2, 2.3, and 2.4 in Appendix B, Supplementary Material. \n \n2.3       Matrix Backpropagation \nAnother important technical contribution introduced in this work is MBP implemented to SENDER \nto further reduce the potential accumulative error after finishing the updates of all variables in \nSENDER. Assume the number of the total layers as 𝑀, Eqs. (8)-(10) describe the details of MBP \napplied to the linear method part, and Eqs. (11)-(13) provide the mathematic formula of MBP for \nnonlinear method part in SENDER [31], [32], [42]. All variables, such as 𝑋𝑘, 𝑌𝑘, 𝑈𝑘, 𝑉𝑘, and 𝐼 are \ndenoted as the same in Section 2.1. \n𝑌̂𝑘⟵𝑌𝑘, 𝑘= 𝑀\n𝑌̂𝑘⟵𝑋𝑘+1𝑌̂𝑘+1, 𝑘< 𝑀 \n(8) \nIn addition, we denote the product of hierarchical weight matrices as 𝜓 in Eq. (9) \n𝜓⟵∏\n𝑋𝑖\n𝑘\n𝑖=1\n 1 < 𝑘< 𝑀 \n(9) \n6 \n \nThen, the following equation describes the crucial steps of MBP to update variables of {𝑋𝑖}𝑖=1\n𝑀, and \n{𝑌𝑖}𝑖=1\n𝑀 representing the hierarchical weight and feature matrices including all canonical and some \nmeta-FCs, respectively. \n𝑍𝑘←𝐼−∏𝑈𝑖\n𝑘\n𝑖=1\n∙(𝒩𝑘∙𝑉𝑘) \n(10-1) \n𝑌̂𝑘\n+ ⟵𝑌𝑘\n+ ⊙√[𝜓𝑇𝑍𝑘]+ + [𝜓𝑇𝜓]−𝑌̂𝑘\n+\n[𝜓𝑇𝑍𝑘]−+ [𝜓𝑇𝜓]+𝑌̂𝑘\n+ \n(10-2) \nMore details can be viewed in Algorithm 2.5, Appendix B, Supplementary Material. \nSimilarly, MBP is employed to further reduce the potential accumulative errors caused by the deep \nnonlinear method, [6, 7, 25]. Denote 𝑈𝑘= 𝑙𝑖𝑚\nit→∞𝑈𝑘\n𝑖𝑡, 𝑋𝑘= 𝑙𝑖𝑚\nit→∞𝑋𝑘\n𝑖𝑡, and 𝑌𝑘= 𝑙𝑖𝑚\nit→∞𝑌𝑘\n𝑖𝑡, we have: \n𝐾⟵(∏𝑈𝑘\n𝑀\n𝑘=1\n)\n𝑇\n∙(𝐼−∏𝑋𝑘\n𝑀\n𝑘=1\n∙𝑌𝑀) \n(11-1) \n𝑃𝑘\n𝑖𝑡⟵(𝑈𝑘\n𝑖𝑡)\n𝑇𝑈𝑘\n𝑖𝑡∙∏max (𝑈𝑘\n𝑀−1\n𝑘=1\n) \n(11-2) \nIn Eqs. (11-1) and (11-2), there are two important variables, i.e., deep weight matrices and feature \nmatrices in the nonlinear method, updated by the following backpropagation techniques shown from \nEqs. (12-1) to (12-4) [6, 25]. The following equations employ 𝑐𝑘\n𝑖𝑡, 𝐶𝑘\n𝑖𝑡, 𝑑𝑘\n𝑖𝑡, and 𝐷𝑘\n𝑖𝑡 to perform MBP \n[6, 7] using the derivative of the inverse activation function  \n𝑑𝒩−1(𝑠)\n𝑑𝑠\n: \n𝑐𝑘\n𝑖𝑡⟵max(𝑈𝑘−1) ∙𝑑𝒩−1(𝑠)\n𝑑𝑠\n, 𝑠= 𝑈𝑘\n𝑖𝑡𝑉𝑘\n𝑖𝑡 \n(12-1) \n𝑑𝑘\n𝑖𝑡⟵𝑑𝒩−1(𝑠)\n𝑑𝑠\n, 𝑠= 𝑈𝑘\n𝑖𝑡𝑉𝑘\n𝑖𝑡 \n(12-2) \n𝐶𝑘\n𝑖𝑡⟵(𝑈𝑘\n𝑖𝑡)𝑇∙(𝑃𝑘\n𝑖𝑡∙𝒩−1(𝑠) −𝐾)⨀𝑐𝑘\n𝑖𝑡, s = 𝑈𝑘\n𝑖𝑡𝑉𝑘\n𝑖𝑡 \n(12-3) \n𝐷𝑘\n𝑖𝑡⟵(𝑈𝑘−1\n𝑖𝑡)𝑇∙(𝑈𝑘−1\n𝑖𝑡\n∙𝒩−1(𝑠) −𝑉𝑘−1\n𝑖𝑡)⨀𝑑𝑘\n𝑖𝑡∙(𝑉𝑘\n𝑖𝑡)𝑇, 𝑠= 𝑈𝑘\n𝑖𝑡𝑉𝑘\n𝑖𝑡 \n(12-4) \nFinally, the following equations show the process of updating the weight matrix 𝑈𝑘 and the feature \nmatrix 𝑉𝑘 in the kth layer. In addition, 𝑇 is a constant value determined as 0.01 [6, 7, 25]. \n𝑉𝑘\n𝑖𝑡+1 ←𝑉𝑘\n𝑖𝑡−𝑇\n2𝑖𝑡(𝐶𝑘\n𝑖𝑡) \n \n(13-1) \n𝑈𝑘\n𝑖𝑡+1 ←𝑈𝑘\n𝑖𝑡−𝑇\n2𝑖𝑡(𝐷𝑘\n𝑖𝑡) \n \n(13-2) \n \n \n \n2.4         Approximation and Convergence Rate of SENDER \nIn this section, we theoretically analyze the efficacy and discusses the performance of approximation \nand convergence rate of SENDER. Due to SENDER being organized as a composition of linear and \nnonlinear functions, the following theorem demonstrates that SENDER can approximate any real \nfunction that is almost-everywhere infinite [29] with high accuracy. The proof of Theorem 1.1 can \nbe viewed in Appendix A, Supplementary Material. \n \nTheorem 1.1 (Accurate Approximation of SENDER) Given a real function 𝑓: ℝ𝑃×𝑄→ℝ𝑃×𝑄∪\n{±∞} and 𝑚({𝐼∈ℝ𝑃×𝑄: 𝑓(𝐼) = ±∞}) = 0 where 𝑚(∙) represents the Lebesgue measure [29]. \nGiven 𝐼∈ℝ𝑃×𝑄, SENDER includes a linear method and a nonlinear method with multiple activation \nfunctions denoted as {𝑃𝑘(𝐼)}𝑘=1\n𝑁1  and   {𝒩𝑘(𝐼)}𝑘=1\n𝑁2 . If 𝑃𝑘 denotes a series of matrix polynomials and \n7 \n \n𝒩𝑘 denotes a smooth activation function, then we have ∀ 𝜀> 0 , 𝑁> 0 , 𝑁1 > 𝑁, 𝑁2 < ∞, \n‖{𝑃𝑘(𝐼)}𝑘=1\n𝑁1 + {𝒩𝑘(𝐼)}𝑘=1\n𝑁2 −𝑓(𝐼)‖ ≤𝜀. \n \nTheorem 1.1 demonstrates that SENDER enables an accurate approximation to the original input \n𝑓(𝐼), even if it is almost-everywhere finite, e.g., 𝑓(𝐼) = ±∞, 𝑚(𝐼) = 0. \nSince the fully-connected architecture is widely used in DNNs with various activation functions, the \noptimization function of the conventional neural network can be very complicated. Therefore, \nSENDER implements a non-fully connected architecture to reduce the complexity of network \nstructures and thus improve the efficiency of optimization since the global optimum of the \noptimization function can be found by a gradient optimizer. The details of this conclusion are proved \nas Theorem 1.2 in Appendix A, supplementary material.  \nTheorem 1.2 (Efficiency of Non-Fully Connected Architecture of SENDER) Given a series of \nnon-smoothed activation function {𝑓𝑖}𝑖=1\n𝑁, defined on [𝑎, 𝑏] ⊆ℝ1, assume {𝑓𝑖}𝑖=1\n𝑁\n𝑖⊆𝐿𝑖𝑝1([𝑎, 𝑏]\\\n{𝑈(𝑥𝑖, 𝛿)}) 𝑖∈ℕ, and 𝑈(𝑥𝑖, 𝛿)  is an open cubic with the center 𝑥𝑖 and radius 𝛿> 0 . The \ncomposition of  𝑓𝑖, 𝑓𝑗∈{𝑓𝑖}𝑖=1\n𝑁\n are denoted as 𝑓𝑗,𝑖≝𝑓𝑗(𝑓𝑖(𝑥)); the various composition ℱ≝\n𝑓⋯,𝑘,⋯𝑗.𝑖∈𝐿𝑖𝑝1([𝑎, 𝑏]\\[𝑐, 𝑑])  holds, when 𝑘→∞, [𝑐, 𝑑] ⊇⋃\n𝑈(𝑥𝑖, 𝛿)\n𝑘\n𝑖=1\n and 𝑚([𝑐, 𝑑]) ≠0 ; \nmoreover, given 𝑡→∞, the summation of ∑\nℱ𝑖\n𝑡\n𝑖=1\n leads to ∑\nℱ𝑖\n𝑡\n𝑖=1\n∉𝐿𝑖𝑝1([𝑎, 𝑏]\\[𝑐′, 𝑑′]) , \n[𝑐′, 𝑑′] ⊇[𝑐, 𝑑], and 𝑚([𝑐′, 𝑑′]) ≠0. And 𝑚(∙) represents the Lebesgue measure. \nTheorem 1.2 demonstrates the infinite composition of activation function, such as fully connected \nand very DNN architecture, even with a single non-smooth point, which finally results in a non-\nsmooth interval as [𝑐′, 𝑑′]. Meanwhile, this theorem demonstrates that the non-fully connected \narchitectures can be more easily optimized, i.e., the global optimum of the non-fully connected \nmethod is easier to be searched by a gradient-based optimizer. \nThe following definition and theorems support that SENDER can maintain the convergence rate of \nthe original optimizer STORM. Moreover, we analyze the convergence rate of SENDER in the finite \nand infinite dimensional space, respectively. \nDefinition 1.1 (SENDER Operator) Denote the Random Initialization Operator as 𝒫: ℝ𝑃×𝑄→\nℝ𝑃×𝑄, the Sparse Operator as 𝒮: ℝ𝑃×𝑄→ℝ𝑃×𝑄 and the STORM operator as 𝒯: ℝ𝑃×𝑄→ℝ𝑃×𝑄. \nTheir norms can be represented as \n1\n𝑟≝‖𝒫‖, \n1\n𝑠≝‖𝒮‖, and \n1\n𝑡≝‖𝒯‖; in Theorems 2.1 to 2.4 in \nAppendix B, Supplementary Material, we have: 0 <\n1\n𝑠< 1 and 0 <\n1\n𝑟,\n1\n𝑡< ∞, but 0 <\n1\n𝑡𝑘< 1, 𝑘<\n∞.   \nTheorem 1.3 (Convergence Rate of SENDER in Finite Dimensionality Space) Denote STORM \nas an operator 𝒯 in a finite dimensionality space. Due to the convergence of STORM with a rate of  \n𝒪(𝑇\n1\n2 + 𝜎\n1\n3/𝑇\n1\n3), the convergence of SENDER is the same as STORM. \nTheorem 1.3 shows that SENDER can converge as fast as STORM due to the prerequisite of finite \ndimensionality space. We further prove the convergence of SENDER in an infinite dimensionality \nspace in Collaroy 3.1. \nCollaroy 1.3 (Convergence of SENDER in Infinite Dimensionality Space) Given the infinite \ndimensionality space [30, 31], denote SENDER as an operator 𝒟: ℝ∞×∞→ℝ∞×∞. Denote 𝒟 as an \ninfinite dimensional matrix operator with each element represented as 𝑑𝑖,𝑗 and 𝑟𝑖,𝑗∈ℝ∞×∞,  𝑖, 𝑗→\n∞, respectively. 𝒟 can converge to a fixed point, if and only if 𝑑𝑖,𝑗\n𝑘∙𝑟𝑖,𝑗 is 𝒪(\n1\n𝑛𝑝) , 𝑛∈ℕ, 𝑝> 1, 𝑝∈\nℝ. \nTheorem 1.4 (Convergence of SENDER using Alternative Update) Given {ℱ𝑖,𝑗,𝑘,𝑡}𝑖,𝑗,𝑘,𝑡=1\n∞\n,  \n{𝒞 𝑗,𝑘,𝑡}𝑗,𝑘,𝑡=1\n∞\n, {ℋ 𝑘,𝑡}𝑘,𝑡=1\n∞\n, and {𝒦 𝑡}𝑡=1\n∞, are series of continuous operators [30] applied on a finite \ndimensionality space, we have: ℱ𝑖,𝑗,𝑘,𝑡, 𝒞 𝑗,𝑘,𝑡, ℋ 𝑘,𝑡, 𝒦 𝑡: ℝ𝑃×𝑄→ℝ𝑃×𝑄. If l𝑖𝑚\n𝑖→∞ℱ𝑖,𝑗→𝒞𝑗,𝑘,𝑡, \n𝑙𝑖𝑚\n𝑗→∞𝒞𝑗,𝑘,𝑡→ℋ 𝑘,𝑡, 𝑙𝑖𝑚\n𝑘→∞ℋ𝑘,𝑡→𝒦 𝑡, and l𝑖𝑚\n𝑡→∞𝒦𝑡→𝒢, then, 𝑙𝑖𝑚\n𝑛→∞ℱ𝑖𝑛,𝑗𝑛,𝑘𝑛,𝑡𝑛→𝒢 holds. \nTheorem 4.1 demonstrates that a computational model with multiple variables satisfying Corollary \n1.2 can converge to a fixed point via an alternative strategy. The proof of Theorem 4.1 can be viewed \n8 \n \nin Appendix A, Supplementary Material. Moreover, in Theorem 4.1, due to the convexity of the \nAugmented Lagrange function [32], each independent approximation, such as 𝑙𝑖𝑚\n𝑛→∞ℱ𝑖𝑛,𝑗𝑛,𝑘𝑛,𝑡𝑛→𝒢 \ncan converge to the operator.  \n \n3 \nResults \n3.1      Comparison of Identified Canonical, Meta, and Sub-FCs via SENDER and Other Peer \nFour Methods \nTo validate SENDER, we employ the resting-state fMRI signals from all healthy individuals in \nConsortium for Neuropsychiatric Phenomics (CNP) (https://openfmri.org/dataset/ds000030/). To \nreduce the heterogeneous influence caused by parameter tuning, all hyperparameters are tuned the \nsame as SENDER's hyperparameter estimations. The estimated number of layers for SENDER is \ntwo. The sizes of the first and second layers are 40 and 10, respectively. In addition, other peer \nalgorithms’ parameters are tuned following in [5-8]. The activation function of all layers of SENDER \nand DBN is set as ReLU. Furthermore, all abbreviations of templates can be found in Table S1, \nAppendix A, Supplementary Material. \nFigure 1 and 2 present the reconstruction of the canonical FCs via SENDER and other peer methods, \ne.g., DICA, DNMF, SDDL, and DBN; the identification results via the linear method in SENDER \nand other four peer algorithms are compared with canonical templates [19]. In short, this \nexperimental validation demonstrates that the reconstruction of canonical FCs via SENDER is not \nsignificantly different from the canonical templates. \n \nFigure 1. This figure presents three representative slices of reconstructed six canonical FCs via \nSENDER and six other peer methods.  \nFurthermore, the quantitative comparison of canonical FCs is provided in Figure 5 (b), where it \ncompares the similarity of reconstructed canonical FCs derived via the linear method in SENDER \nand other four peer methods and publicly released canonical templates [19]. The similarity is \nmeasured by Hausdorff Distance [34]. In general, the similarity of identified canonical FCs via \nSENDER and canonical templates is higher/comparable to other peer algorithms.    \nMoreover, Figure 2 shows the reconstruction of meta-FCs derived by the nonlinear method in \nSENDER and the other four peer methods. Overall, the nonlinear methods in SENDER can \nsuccessfully reconstruct meta-FCs with a higher similarity calculated with the meta-templates [41, \n42]. \n \n9 \n \n \nFigure 2. This figure presents the representative slices of reconstructed another six canonical FCs \nvia SENDER and the other six peer methods. \nIn Figure 3, according to qualitative observation, the nonlinear method in SENDER and DBN can \nreconstruct meta-FCs more similarly to the templates; meanwhile, DICA and DNMF only \nreconstruct five and three FCs with higher similarity to the original templates, respectively. \nFurthermore, for SDDL, the reconstruction of FCs in the first and second columns is similar to the \noriginal templates.  \n \nFigure 3. All qualitative comparisons revealed five meta-FCs at the first or second layer via the \nnonlinear method in SENDER and the other four peer methods. Please notice that the meta-FCs can \nbe identified at the first and second layers using SENDER; nevertheless, the other four peer methods \nneed to extract meta-FCs in the deeper layers. \nIn detail, for instance, SENDER can reveal meta-FCs using the nonlinear method with a strong spatial \noverlap with reported templates [42]. Although other peer algorithms can detect five meta-FCs at \ntheir second layer, some reported FCs are different from templates. Specifically, DICA failed to \ndetect the activated occipital lobe compared with templates presented in the first column, in Figure \n10 \n \n3; in addition, there are some differences between extracted meta-FCs and templates, e.g., FCs in the \nfirst and second column; in detail, the template FCs in first column contains the occipital lobe that \nare disrupted in most meta-FCs identified via deep linear learning method; furthermore, the FCs \nidentified by SDDL in third and fourth column presents a disruption of areas in activation areas of \ncanonical Executive Control Network; similarly, DBN can only perfectly reveal the meta-FCs in the \nlast column.  \nMoreover, the quantitative results are included in Figure 5(c) to compare the similarity between \nidentified meta-FCs with corresponding templates. We further provide theoretical explanations of \nwhy SENDER can provide more FCs/spatial features than DICA in the proof in Theorem 2.2, \nAppendix B, Supplementary Material.    \nNote that another contribution of SENDER is to provide a variable including the potentially \ncorresponding sub-FCs, while other peer algorithms cannot build the relations between the variables \nin method and sub-FCs using the same hyperparameters. For instance, in the third column of Figure \n4, a precuneus, a functional core of Default Mode Network (DMN), is detected. However, these \nminor/sub-FCs could be more sensitive to some brain diseases from a clinical translational \nperspective that can guide the personalized diagnosis and treatment [35, 36]. Moreover, as discussed \nbefore, these sub-FCs are designed clearly as variables {𝑆𝑖}𝑖=1\n𝑘\n of SENDER introduced in Eq. (1) \nrather than randomly extracted features [5]. \n \n \nFigure 4. The qualitative comparison of sub-FCs identified by  SENDER. These sub-FCs are derived \nvia shallow and deeper layers of the background component matrix of SENDER. \nIn short, Figure 5 (a) indicates the potential organizations of canonical, meta, and sub-FCs. To \nvalidate the reconstruction performance of SENDER and the other three peer methods, we calculate \nthe similarity of all FCs extracted by SENDER and the other four peer methods; SENDER shows an \noverall higher similarity than the four peer algorithms.  \nMoreover, since there has not been a rigorous 'ground-truth' to quantitatively validate the \nperformance of SENDER and the other four peer methods, especially for meta-FCs, we alternatively \ninvestigate the identifiability [40] of canonical and meta-FCs. Calculating the identifiability can \nfurther validate the consistency and reproducibility of SENDER and other four peer methods in a \ndata-driven fashion. At first, we randomly separate the original input data into two independent sets \nas 𝐹𝐶𝑡𝑒𝑠𝑡 and 𝐹𝐶𝑟𝑒𝑡𝑒𝑠𝑡 shown in Eq. (9-1), and calculate the identifiability using Eq (9-2). The \nquantitative identifiability of meta-FCs is shown in Figures 6 (a) and (b).  As discussed, the following \nequations detail the procedure of calculating identifiability: \n𝑓𝑐𝑖∈𝐹𝐶𝑡𝑒𝑠𝑡, 𝑓𝑐𝑗∈𝐹𝐶𝑟𝑒𝑡𝑒𝑠𝑡, 𝐹𝐶𝑡𝑒𝑠𝑡∩𝐹𝐶𝑟𝑒𝑡𝑒𝑠𝑡= ∅ \n(9-1) \n𝑖𝑑𝑒𝑛𝑡𝑖𝑓𝑖𝑎𝑏𝑖𝑙𝑖𝑡𝑦←\n∑∑𝑐𝑜𝑟𝑟(𝑓𝑐𝑖, 𝑓𝑐𝑗)\n𝑗\n𝑖\n|𝐹𝐶𝑡𝑒𝑠𝑡| × |𝐹𝐶𝑟𝑒𝑡𝑒𝑠𝑡| \n(9-2) \n11 \n \nIn Eqs. (9-1) and (9-2), 𝑐𝑜𝑟𝑟(𝑓𝑐𝑖, 𝑓𝑐𝑗) represents the calculation of the corresponding components \nidentified from test and retest data using Pearson Correlation [40]. In this work, the calculation of \ncorrelation is replaced by Intraclass Correlation Coefficient (ICC). And |𝐹𝐶𝑡𝑒𝑠𝑡| , |𝐹𝐶𝑟𝑒𝑡𝑒𝑠𝑡| \nrepresent the number of components in the test and retest datasets, respectively. \n \nFigure 5. (a) An example of hierarchical structure on FCs presents the hierarchical organizations \nidentified via linear/nonlinear methods and background matrix in SENDER. The bottom slices \nrepresent the sub-FCs which include partial/minor functional regions of canonical FCs. The middle \nslices show the identified canonical FCs and the top slices provide the meta FCs extracted via \nSENDER. The dashed line indicates the high-order FCs do not entirely include lower-order FCs and \nother regions are involved. (b) and (c) provide the similarity of canonical and meta-FCs derived by \nSENDER with the templates, shown in the first row in Figures 1, 2, and 3, respectively. \n \nFigure 6. (a) and (b) show the identifiability comparison of identified canonical and meta-FCs via \nSENDER and the other three peer methods based on all subjects' resting-state fMRI signals from \nhealthy individuals in CNP (https://openfmri.org/dataset/ds000030/). And (c) provides the \n12 \n \ncomparison of reconstruction accuracy, i.e., training loss, of SENDER and the other four peer \nmethods in the second layer. \nIn Figures 6 (a) and (b), the quantitative results of identifiability demonstrate that the identified \ncanonical and meta-FCs via linear/nonlinear methods in SENDER provide higher identifiable over \nthe other four peer methods [40]. In addition, in Figure 6 (c), the significant difference in \nreconstruction accuracy can be easily observed since the purple box plot shows the highest accuracy \nof training loss at the second layer. \n \n4 \nConclusion \nIn this work, the proposed SENDER adopts STORM optimizer, the alternative optimization strategy, \nand RRO, for data-driven determination of all hyperparameters, to reveal the canonical, meta, and \nsub-FCs. Furthermore, the hybrid modeling and efficient non-fully connected architectures of \nSENDER enable the discovery of canonical, meta, and sub-FCs with the highest identifiability to \nthe other four peer methods. Moreover, the results show that the meta-FCs can even be detected in \nshallow layers. Finally, the theoretical studies and experimental validation further indicate an \naccurate approximation to original input and high convergence rate of SENDER, which are \ncomparable to or even better than other peer algorithms, such as DNNs. \nMoreover, SENDER can potentially synergize research of neurodevelopmental, neurodegenerative, \nand psychiatric disorders since these revealed novel canonical, meta, and sub-FCs that can be \ngenerated as the clinical biomarkers benefitting personalized diagnosis, prognosis and treatment \nmonitoring [35-39]. \nOverall, we believe that SENDER can play a role as an inspiring deep hybrid learning method for a \nfruitful future with a profound influence on facilitating the research of deep learning methods, \ncomputational neuroscience, and clinical translational application. \n \nReferences \n[1] \nBullmore, E., Sporns, O. Complex brain networks: graph theoretical analysis of structural and \nfunctional systems. Nature Reviews Neuroscience, 2009; 10:186-198. \n[2] \nPower, J. D., Cohen, A. L., Nelson, S. M., Wig, G. S., Barnes, K. A., Church, J. A., … & Petersen, \nS. E. Functional network organization of the human brain. Neuron, 2011; 72:65-678. \n[3] \nStam, C. J. Modern network science of neurological disorders. Nature Reviews Neuroscience, 2014; \n15:683. \n[4] \nIraji, A., Fu, Z., Damaraju, E., et al. (2019). Spatial dynamics within and between brain functional \ndomains: A hierarchical approach to study time-varying brain function. Hum Brain Mapp, 40:1969-1986. \n[5] \nWylie, K. P., Kronberg, E., Legget, K. T., Sutton, B., & Tregellas, J. R. (2021). Stable Meta-\nNetworks, Noise, and Artifacts in the Human Connectome: Low-to High-Dimensional Independent \nComponents Analysis as a Hierarchy of Intrinsic Connectivity Networks. Frontiers in Neuroscience, 15. \n[6] \nQiao, C., Yang, L., Calhoun, V. D., Xu, Z. B., & Wang, Y. P. (2021). Sparse deep dictionary learning \nidentifies differences of time-varying functional connectivity in brain neuro-developmental study. Neural \nNetworks, 135, 91-104. \n[7] \nTrigeorgis, G., Bousmalis, K., Zafeiriou, S., & Schuller, B. W. (2016). A deep matrix factorization \nmethod for learning attribute representations. IEEE Transactions on Pattern Analysis and Machine Intelligence, \n39:417-429. \n[8] \nTrigeorgis, G., Bousmalis, K., Zafeiriou, S., & Schuller, B. (2014, June). A deep semi-nmf model for \nlearning hidden representations. In International Conference on Machine Learning (pp. 1692-1700). PMLR. \n[9] \nZhang, W., Palacios, E. M., & Mukherjee, P. Deep Linear Modeling of Hierarchical Functional \nConnectivity in the Human Brain. BioRxiv., 2020. doi:10.1101/2020.12.13.422538. \n13 \n \n[10] \nZhang, W., Zhao, S., Hu, X., Dong, Q., Huang, H., Zhang, S., ... & Liu, T. (2020). Hierarchical \nOrganization of Functional Brain Networks Revealed by Hybrid Spatiotemporal Deep Learning. Brain \nConnectivity, 10:72-82. \n[11] \nZhang, W., Zhao, L., Li, Q., Zhao, S., Dong, Q., Jiang, X., ... & Liu, T. (2019, October). Identify \nhierarchical structures from task-based fMRI data via hybrid spatiotemporal neural architecture search net. \nIn International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 745-753). \nSpringer, Cham. \n[12] \nBengio, Y., Courville, A.C., Vincent, P. (2012). Unsupervised feature learning and deep learning: A \nreview and new perspectives. CoRR, abs/1206.5538, 1. \n[13] \nHannun, A. Y., Rajpurkar, P., Haghpanahi, M., Tison, G. H., Bourn, C., Turakhia, M. P., & Ng, A. \nY. (2019). Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a \ndeep neural network. Nature Medicine, 25(1), 65. \n[14] \nLeCun, Y., Bengio, Y., Hinton, G.E. (2015). Deep learning. Nature, 521:436-444. \n[15] \nEsteva, A., Robicquet, A., Ramsundar, B., Kuleshov, V., DePristo, M., Chou, K., Cui, C., Corrado, \nG., Thrun, S., & Dean, J. (2019). A guide to deep learning in healthcare. Nature Medicine, 25:24-29. \n[16] \nGurovich, Y., Hanani, Y., Bar, O., Nadav, G., Fleischer, N., Gelbman, D., ... & Bird, L. M. (2019). \nIdentifying facial phenotypes of genetic disorders using deep learning. Nature Medicine, 25:60. \n[17] \nHu, X., Huang, H., Peng, B., Han, J., Liu, N., Lv, J., ... & Liu, T. (2018). Latent source mining in \nFMRI via restricted Boltzmann machine. Human Brain Mapping, 39:2368-2380. \n[18] \nHuang, H., Hu, X., Zhao, Y., Makkie, M., Dong, Q., Zhao, S., ... & Liu, T. (2018). Modeling task \nfMRI data via deep convolutional autoencoder. IEEE Transactions on Medical Imaging, 37(7). \n[19] \nSmith, S. M., Fox, P. T., Miller, K. L., Glahn, D. C., Fox, P. M., Mackay, C. E., ... & Beckmann, C. \nF. (2009). Correspondence of the brain's functional architecture during activation and rest. Proceedings of the \nNational Academy of Sciences, 106:13040-13045. \n[20] \nSchmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural networks, 61:85-\n117. \n[21] \nHinton, G.E., Salakhutdinov, R.R. (2006). Reducing the dimensionality of data with neural networks. \nScience, 313:504-507 \n[22] \nHinton, G.E., Osindero, S., Teh, Y.-W. (2006). A fast learning algorithm for deep belief nets. Neural \nComputation, 18:1527-1554. \n[23] \nHinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-r., Jaitly, N., Senior, A., Vanhoucke, V., \nNguyen, P., Sainath, T.N. (2012). Deep neural networks for acoustic modeling in speech recognition: The \nshared views of four research groups. Signal Processing Magazine, IEEE, 29:82-97. \n[24] \nWen, Z., Yin, W., & Zhang, Y. (2012). Solving a low-rank factorization model for matrix completion \nby a nonlinear successive over-relaxation algorithm. Mathematical Programming Computation, 4:333-361. \n[25] \nShen, Y., Wen, Z., & Zhang, Y. (2014). Augmented Lagrangian alternating direction method for \nmatrix separation based on low-rank factorization. Optimization Methods and Software, 29:239-263. \n[26] \nDing, C. H., Li, T., & Jordan, M. I. (2008). Convex and semi-nonnegative matrix factorizations. IEEE \ntransactions on pattern analysis and machine intelligence, 32(1), 45-55. \n[27] \nCutkosky, A., & Orabona, F. (2019). Momentum-based variance reduction in non-convex \nsgd. Advances in neural information processing systems, 32. \n[28] \nLiu, J., Yuan, L., & Ye, J. (2010, July). An efficient algorithm for a class of fused lasso problems. \nIn Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data \nmining (pp. 323-332). \n[29] \nBeck, A., & Teboulle, M. (2009). A fast iterative shrinkage-thresholding algorithm for linear inverse \nproblems. SIAM journal on imaging sciences, 2(1), 183-202. \n[30] \nRoyden, H. L. (1968). Real Analysis. Krishna Prakashan Media. \n[31] \nRudin, W. (1973). Functional Analysis. \n[32] \nKadison, R. V., & Ringrose, J. R. (1997). Fundamentals of the theory of operator algebras (Vol. 2). \nAmerican Mathematical Soc.. \n14 \n \n[33] \nBoyd, S., Boyd, S. P., & Vandenberghe, L. (2004). Convex Optimization. Cambridge university press. \n[34] \nZhang, W., Lv, J., Li, X., Zhu, D., Jiang, X., Zhang, S., ... & Liu, T. (2019). Experimental \nComparisons of Sparse Dictionary Learning and Independent Component Analysis for Brain Network Inference \nfrom fMRI Data, IEEE Transactions on Biomedical Engineering, 66:289-299. \n[35] \nVossel, S., Geng, J. J., Fink, G. R. (2014). Dorsal and ventral attention systems: distinct neural circuits \nbut collaborative roles. Neuroscientist, 20:150-159. \n[36] \nChand, G. B., Wu, J., Hajjar, I., Qiu, D. (2017). Interactions of the Salience Network and Its \nSubsystems with the Default-Mode and the Central-Executive Networks in Normal Aging and Mild Cognitive \nImpairment. Brain Connect, 7:401-412. \n[37] \nCohen, A. D., Yang, B., Fernandez, B., Banerjee, S., Wang, Y. (2020). Improved resting state \nfunctional connectivity sensitivity and reproducibility using a multiband multi-echo acquisition. Neuroimage, \n225:117461. \n[38] \nPlis, S. M., Hjelm, D. R., Salakhutdinov, R., Allen, E. A., Bockholt, H. J., Long, J. D., ... & Calhoun, \nV. D. (2014). Deep learning for neuroimaging: a validation study. Frontiers in Neuroscience, 8:229. \n[39] \nSuk, H.-I., Wee, C.-Y., Lee, S.-W., Shen, D. (2016). State-space model with deep learning for \nfunctional dynamics estimation in resting-state fMRI. Neuroimage, 129:292-307. \n[40] \nVan De Ville, D., Farouj, Y., Preti, M. G., Liégeois, R., & Amico, E. (2021). When makes you unique: \nTemporality of the human brain fingerprint. Science advances, 7(42), eabj0751. \n[41] \nZhang, W., Wang, Y., Cohen, A. D., McCrea, M. A., & Mukherjee, P. Deep Linear Modeling of \nMultiBand MultiEcho fMRI Reveals Reproducible Hierarchical Functional Connectivity Networks. \nOrganization for Human Brain Mapping (OHBM), 2020. \n[42] \n Zhang, W.,  Cai, Lanya T., Wren-Jarvis, J., Lazerwitz, M., Bourla, I., & Mukherjee, P. Deep \nNonlinear Modeling Extracts Reproducible Hierarchical Functional Networks from BOLD fMRI. Organization \nfor Human Brain Mapping (OHBM), 2021. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n15 \n \nAppendix A \n \nThe matrix polynomials are defined as: ∀𝑘∈ℕ 𝑃2𝑘(𝑋) = (𝑋𝑋𝑇)𝑘, 𝑃2𝑘+1(𝑋) = (𝑋𝑋𝑇)𝑘𝑋, 𝑋∈\nℝ𝑆×𝑇; and {𝑃𝑛(𝑋)}𝑛=1\n𝑁\n defines a series of matrix polynomials, for example: {𝑃𝑛(𝑋)}𝑛=1\n3\n=\n{𝑋, 𝑋𝑋𝑇, 𝑋𝑋𝑇𝑋}, 𝑋∈ℝ𝑆×𝑇; \nMoreover, it is easy to prove {𝑃𝑛}𝑛=1\n∞\n denoted on ℝ𝑆×𝑇 as a ring ({𝑃𝑛}𝑛=1\n∞, +,×), and it also \ndemonstrates {𝑃𝑛(𝑋)}𝑛=1\n𝑁\n∈({𝑃𝑛}𝑛=1\n∞, +,×) , e.g., ({𝑃𝑛}𝑛=1\n∞, +,×) ⊇∑\n{𝑃𝑛(𝑋)}𝑛=1\n𝑁\n𝑀\n𝑖=1\n. (Dummit, \n2004; Kadison, 1997). Then we introduce the theorem 1.1 to describe the superiority of DEMAND. \n \nTheorem 1.1 (Accurate Approximation of SENDER) Given a real function 𝑓: ℝ𝑃×𝑄→ℝ𝑃×𝑄∪\n{±∞} and 𝑚({𝐼∈ℝ𝑀×𝑁: 𝑓(𝐼) = ±∞}) = 0 where m(∙) represents the Lebesgue measure [29]. \nGiven 𝐼∈ℝ𝑀×𝑁, SENDER includes a linear model and nonlinear model with multiple activation \nfunctions can be denoted as {𝑃𝑘(𝐼)}𝑘=1\n𝑁1  and   {𝒩𝑘(𝐼)}𝑘=1\n𝑁2  we have: if P denotes a series of matrix \npolynomials, such as, and 𝒩 denotes a smooth activation function, and∀ 𝜀> 0, 𝑁> 0, 𝑁1 > 𝑁, \n𝑁2 < ∞, ‖{𝑃𝑘(𝐼)}𝑘=1\n𝑁1 + {𝒩𝑘(𝐼)}𝑘=1\n𝑁2 −𝑓(𝐼)‖ ≤𝜀. \nProof: According to Лузин (Luzin) Theorem in [29], we have a close set: \n𝐹𝑛⊂𝐹𝑛+1 ⊂⋯⊆ℝ𝑀×𝑁 \n𝑚(ℝ𝑀×𝑁\\𝐹𝑘) = 1\n𝑘, 𝑘∈ℕ \n𝑓∈𝐶(𝐹𝑘) \n \n(A.1) \nThen we have a consistent real function 𝑔(𝑋), and obviously we have: \n𝑔(𝑋) = 𝑓(𝑋) \n(A.2) \nSince for any continuous real function, we have: \n|𝑔(𝑋) −𝑃𝑘(𝑋)| < 1\n𝑘 \n(A.3) \nLet ℱ= ⋃\n𝐹𝑘\n∞\n𝑘=1\n, and obviously we have: \n𝑚(ℝ𝑀×𝑁\\𝐹𝑘) = 𝑚(ℝ𝑀×𝑁\\⋃𝑘=1\n∞𝐹𝑘) = ⋂𝑘=1\n∞𝑚(ℝ𝑆×𝑇\\𝐹𝑘) = ⋂𝑘=1\n∞\n1\n𝑘= 0 \n(A.4) \nMoreover, it is easy to prove {𝑃𝑛}𝑛=1\n∞\n denoted on ℝ𝑆×𝑇 as a ring ({𝑃𝑛}𝑛=1\n∞, +,×), and it also \ndemonstrates {𝑃𝑛(𝑋)}𝑛=1\n𝑁\n⊆({𝑃𝑛}𝑛=1\n∞, +,×), e.g., 𝑃𝑛(𝑋) ≝∏\n𝑥𝑖\n𝑁\n𝑖=1\n+ ∑\n𝑦𝑗\n𝑁\n𝑗=1\n.  \n \nIf 𝔉 is a real function denoted on set ℱ, it indicates:  \nlim\n𝑁→∞|𝔉−{𝑃𝑛(𝑋)}𝑛=1\n𝑁\n| = 0 \n(A.5) \nthen we have lim\n𝑁→∞{𝑃𝑛(𝑋)}𝑛=1\n𝑁\n= 𝔉, meanwhile, if N is large enough, |𝔉−{𝑃𝑛(𝑋)}𝑛=1\n𝑁\n| < 𝜀 holds. \nMoreover, if {𝒩𝑛(𝑋)}𝑛=1\n𝑁\n∈𝐶(ℝ𝑆×𝑇), according to Theorem, |𝒩𝑛(𝑋) −{𝑃𝑛(𝑋)}𝑛=1\n∞| →0;  \nwe have: \n|𝔉−{𝒩𝑛(𝑋)}𝑛=1\n𝑁\n| = |𝔉−{𝑃𝑛(𝑋)}𝑛=1\n∞| < 𝜀\n2 \n(A.6) \nThus, considering ∀ 𝜀> 0, 𝑁> 0, 𝑁1 > 𝑁, 𝑁2 < ∞, rewrite Eq. (6) as: \n|𝔉−{𝑃𝑛(𝑋)}𝑛=1\n𝑁1 | < 𝜀\n2 \n(A.7-1) \n|𝔉−{𝒩𝑛(𝑋)}𝑛=1\n𝑁2 | < 𝜀\n2 \n(A.7-2) \nThen, obviously, we have Eq. (A.8) hold as below: \n16 \n \n|𝔉−{𝑃𝑛(𝑋)}𝑛=1\n𝑁1 −{𝒩𝑛(𝑋)}𝑛=1\n𝑁2 | < 𝜀 \n(A.8) \n \nDefinition 1.2 (Variance Bounded Real Function) Given a real function 𝑓 denoted on [𝑎, 𝑏], and \n∆: 𝑎= 𝑥0 < 𝑥1 < 𝑥2 < ⋯< 𝑥𝑛= 𝑏. A sum as  𝑣∆= ∑\n|𝑓(𝑥𝑖) −𝑓(𝑥𝑖−1)|\n𝑛\n𝑖=1\n and 𝑉𝑎\n𝑏(𝑓) =\nsup { 𝑣∆: ∀∆}. The variance bounded real function is denoted as 𝑉𝑎\n𝑏(𝑓) < ∞.  \n \nDefinition 1.3 (Amplitude of Real Function) Given a real function 𝑓 denoted on [𝑎, 𝑏], and \n∀𝐵(𝑥0, 𝛿) ⊆[𝑎, 𝑏], 𝛿> 0; 𝜔𝑓(𝑥0) = lim\n𝛿→0 sup {|𝑓(𝑥′) −𝑓(𝑥\")|: 𝑥′, 𝑥\" ∈𝐵(𝑥0, 𝛿)}. \n \nLemma 1.1 (Smooth & Variance Bounded Real Function) If and only if a real function 𝑓∈\n𝐿𝑖𝑝1([𝑎, 𝑏]),  𝑉𝑎\n𝑥(𝑓) < ∞ holds.  \nProof:  If 𝑓∈𝐿𝑖𝑝1([𝑎, 𝑏]), it indicates: ∀𝑥1, 𝑥2 ∈[𝑎, 𝑏], we have: |𝑓(𝑥1) −𝑓(𝑥2)| ≤𝐿|𝑥1 − 𝑥2|.  \nMoreover, according to Definition 1, we have: \n 𝑣∆= ∑|𝑓(𝑥𝑖) −𝑓(𝑥𝑖−1)|\n𝑛\n𝑖=1\n≤𝐿(|𝑥0 − 𝑥1| + |𝑥1 − 𝑥2| + ⋯+ |𝑥𝑛− 𝑥𝑛−1|)\n≤𝐿(𝑏−𝑎) < ∞ \n \n(A.9) \n \nIf 𝑉𝑎\n𝑥(𝑓) < ∞ holds, it demonstrates: 𝑠𝑢𝑝 { 𝑣∆: ∀∆} < ∞, let 𝑥𝑛 be 𝑥, we have: \n∑|𝑓(𝑥𝑖) −𝑓(𝑥𝑖−1)|\n𝑛\n𝑖=1\n≤𝑠𝑢𝑝 { 𝑣∆: ∀∆} < ∞ \n \n(A.10) \n \nFurthermore, if 𝑛→∞, and ∑\n|𝑓(𝑥𝑖) −𝑓(𝑥𝑖−1)| < ∞\n𝑛\n𝑖=1\n holds, obviously, we must have: \n|𝑓(𝑥𝑖) −𝑓(𝑥𝑖−1)| →0 \n(A.11) \nIt should satisfy:  \n𝑥𝑖, 𝑥𝑖−1 ∈𝐵(𝑥, 𝜀), ∀𝜀> 0 |𝑓(𝑥𝑖) −𝑓(𝑥𝑖−1)| ≤𝐿|𝑥𝑖− 𝑥𝑖−1| \n(A.12) \nSince ∀𝑥𝑖, 𝑥𝑖−1 ∈[𝑎, 𝑏], obviously, we have:  \n𝑓∈𝐿𝑖𝑝1([𝑎, 𝑏]) \n \nLemma 1.2 (Amplitude & Variance Bounded Real Function) 𝜔𝑓(𝑥0) = lim\n𝛿→0 sup {|𝑓(𝑥′) −\n𝑓(𝑥′′)|: 𝑥′, 𝑥\" ∈𝐵(𝑥0, 𝛿) ⊆[𝑎, 𝑏]} < 𝜀 is equivalent to 𝑓∈𝐿𝑖𝑝1([𝑎, 𝑏]). \nProof: If 𝑓∈𝐿𝑖𝑝1([𝑎, 𝑏]) holds, similarly, if 𝑛→∞, ∀{𝑥𝑖}𝑖=1\n𝑛, we have \n|𝑓(𝑥𝑖) −𝑓(𝑥𝑖−1)| →0 \n(A.13) \nReplace 𝑥𝑖 and 𝑥𝑖−1 by 𝑥′, 𝑥′′, respectively, it satisfies: \n𝜔𝑓(𝑥0) = lim\n𝛿→0 sup{|𝑓(𝑥′) −𝑓(𝑥′′)|: 𝑥′, 𝑥′′ ∈𝐵(𝑥0, 𝛿) ⊆[𝑎, 𝑏]} < 𝑀 \n(A.14) \nIf 𝜔𝑓(𝑥0) = lim\n𝛿→0 sup {|𝑓(𝑥′) −𝑓(𝑥′′)|: 𝑥′, 𝑥′′ ∈𝐵(𝑥0, 𝛿) ⊆[𝑎, 𝑏]} < ∞, we assume: \n𝜔𝑓(𝑥0) = lim\n𝛿→0 sup {|𝑓(𝑥′) −𝑓(𝑥′′)|: 𝑥′, 𝑥\" ∈𝐵(𝑥0, 𝛿) ⊆[𝑎, 𝑏]} < 𝜀 \n(A.15) \nObviously, given 𝑥′, 𝑥′′ ∈𝐵(𝑥0, 𝛿) ⊆[𝑎, 𝑏], we have: \n∀𝜀> 0 |𝑓(𝑥′) −𝑓(𝑥′′)| < 𝜀 \n(A.16) \nWhen 𝛿→0, let |𝑥′ − 𝑥′′| =\n𝜀\n𝐿,  it also indicates:  \n|𝑓(𝑥′) −𝑓(𝑥′′)| < 𝐿|𝑥′ − 𝑥′′| \n(A.17) \n \nLemma 1.3 (Cantor Theorem) Given {𝐵𝑖}𝑖=1\n∞\n are closed sets and ∀𝐵𝑖≠∅, if 𝐵1 ⊇𝐵2 ⊇\n⋯⊇𝐵𝑘⊇⋯, ∩𝑖=1\n∞\n𝐵𝑖≠∅. \n \nLemma 1.4 (Vitali Covering Lemma) Given {𝐵𝑖}𝑖=1\n𝑛 are closed sets and ∀𝐵𝑖∩𝐵𝑗= ∅, 𝑖≠𝑗, 𝐸⊆\nℝ, and 𝑚∗(𝐸) < ∞, if 𝑚∗(𝐸\\ ⋃\n𝐵𝑖\n𝑛\n𝑖=1\n) < 𝜀, ∀𝜀> 0, holds, {𝐵𝑖}𝑖=1\n𝑛 defines a Vitali Covering of 𝐸. \n \n17 \n \nLemma 1.5 (Heine-Borel Covering Theorem) Given Γ is a close and bounded set. Then, an open \nset sequence as {𝑔𝑖}𝑖=1\n𝐾\n≝𝐺, ⋃\n𝑔\n𝐾\n𝑖=1\n𝑖⊇Γ, and 𝐺̿ = ℵ0. \n  \nLemma 1.6 (Composition of Function) Given 𝑔∈𝐿𝑖𝑝1([𝑎, 𝑏]), and 𝑔 is not a constant real \nfunction, if 𝑓∉𝐿𝑖𝑝1([𝑎, 𝑏]), 𝑔(𝑓(𝑥)) ∉𝐿𝑖𝑝1([𝑎, 𝑏]) holds. \nProof: Proof by contradiction, if assume 𝑔(𝑓(𝑥)) ∈𝐿𝑖𝑝1([𝑎, 𝑏]), ∀𝑥1, 𝑥2 ∈[𝑎, 𝑏], 𝑓(𝑥1), 𝑓(𝑥2) ∈\n[𝑎, 𝑏], \n|𝑔(𝑓(𝑥1)) −𝑔(𝑓(𝑥2))| < 𝐿𝑔|𝑓(𝑥1) −𝑓(𝑥2)| < 𝑁< ∞ and 𝐿𝑔≠0. \n(A.18) \nHowever, since 𝑓∉𝐿𝑖𝑝1([𝑎, 𝑏]) , we have: |𝑓(𝑥1) −𝑓(𝑥2)| > 𝑀 that is contradiction with \n|𝑓(𝑥1) −𝑓(𝑥2)| <\n𝑁\n𝐿𝑔. Thus, (𝑓(𝑥)) ∉𝐿𝑖𝑝1([𝑎, 𝑏]) holds. \n \nTheorem 1.2 (Efficiency of Non-Fully Connected Architecture of SENDER) Given a series of \nnon-smoothed activation function {𝑓𝑖}𝑖=1\n𝑁\n, denoted on [𝑎, 𝑏] ⊆ℝ1 , if assume {𝑓𝑖}𝑖=1\n𝑁\n𝑖⊆\n𝐿𝑖𝑝1([𝑎, 𝑏]\\{𝑈(𝑥𝑖, 𝛿)}) 𝑖∈ℕ, and 𝑈(𝑥𝑖, 𝛿) is an open cubic with the center 𝑥𝑖 and radius 𝛿> 0. \nThe composition of  𝑓𝑖, 𝑓𝑗∈{𝑓𝑖}𝑖=1\n𝑁 are denoted as 𝑓𝑗,𝑖≝𝑓𝑗(𝑓𝑖(𝑥)); the various composition ℱ≝\n𝑓⋯,𝑘,⋯𝑗.𝑖∈𝐿𝑖𝑝1([𝑎, 𝑏]\\[𝑐, 𝑑])  holds, when 𝑘→∞, [𝑐, 𝑑] ⊇⋃\n𝑈(𝑥𝑖, 𝛿)\n𝑘\n𝑖=1\n and 𝑚([𝑐, 𝑑]) ≠0 ; \nmoreover, given 𝑡→∞, the summation of ∑\nℱ𝑖\n𝑡\n𝑖=1\n leads to ∑\nℱ𝑖\n𝑡\n𝑖=1\n∉𝐿𝑖𝑝1([𝑎, 𝑏]\\[𝑐′, 𝑑′]) , \n[𝑐′, 𝑑′] ⊇[𝑐, 𝑑], and 𝑚([𝑐′, 𝑑′]) ≠0. And 𝑚(∙) represents the Lebesgue measure. \nProof:  At first, we discuss 𝑘< ∞, and we assume, 𝑓∈𝐿𝑖𝑝1([𝑎, 𝑏]\\{𝑥0}) \nAccording to Lemma 1 and Lemma 2, if 𝑥0 ∈𝐵(𝑥0, 𝛿), for we have: \n𝜔𝑓𝑖(𝑥𝑖) = lim\n𝛿→0 sup{|𝑓𝑖(𝑥′) −𝑓𝑖(𝑥′′)|: 𝑥′, 𝑥′′ ∈𝐵(𝑥𝑖, 𝛿) ⊆[𝑎, 𝑏]} > 𝑀 \n(A.19) \n \n𝜔𝑓𝑗(𝑥𝑗) = lim\n𝛿→0 sup{|𝑓𝑗(𝑦′) −𝑓𝑗(𝑦′′)|: 𝑦′, 𝑦′′ ∈𝐵(𝑥𝑗, 𝛿) ⊆[𝑎, 𝑏]} > 𝑀 \n(A.20) \nThus, we have 𝑓𝑖 and 𝑓𝑗 are not smooth on 𝐵(𝑥𝑖, 𝛿) and 𝐵(𝑥𝑗, 𝛿), respectively. \nAnd for the composition, let 𝑘= 2, \n𝜔𝑓𝑗,𝑖(𝑥𝑖) = lim\n𝛿→0 sup{|𝑓𝑗(𝑓𝑖(𝑥′)) −𝑓𝑗(𝑓𝑖(𝑥′′))|: 𝑥′, 𝑥′′ ∈𝐵(𝑥𝑖, 𝛿) ⊆[𝑎, 𝑏]} \n(A.21) \nLet 𝑓𝑖(𝑥′) = 𝑥𝑗\n′ and 𝑓𝑖(𝑥′′) = 𝑥𝑗\n′′, if (𝑥𝑗\n′, 𝑥𝑗\n′′) ∈𝐵(𝑥𝑘, 𝛿), it is easy to prove the amplitude of 𝑓𝑗,𝑖 as \nfollowing: \n𝜔𝑓𝑗,𝑖(𝑥1) = lim\n𝛿→0 sup{|𝑓2(𝑥𝑗\n′) −𝑓2(𝑥𝑗\n′′)|: 𝑥𝑗\n′, 𝑥𝑗\n′′ ∈𝐵(𝑥𝑘, 𝛿) ⊆𝐵(𝑥𝑗, 𝛿) ⊆[𝑎, 𝑏]}\n> 𝑀 \n(A.22) \nNaturally, we need to analyze other relations of 𝐵(𝑥𝑘, 𝛿) and 𝐵(𝑥𝑗, 𝛿); in detail, there are five \nsituations to be discussed separately: \n1). Assume, if ∀𝐵(𝑥𝑘, 𝛿) ∩𝐵(𝑥𝑗, 𝛿) = ∅, 𝑖, 𝑗∈ℝ, 𝑖≠𝑗, obviously, due to the same composition, \nwe have: 𝐵(𝑥𝑘, 𝛿) ∩𝐵(𝑥𝑘−1, 𝛿) = ∅,  according to Lemma 1.3, \n[𝑐, 𝑑]\\ ⋃𝐵(𝑥𝑘, 𝛿)\n∞\n𝑘=1\n= {𝑥̂𝑗}𝑗=1\n𝑁 \n(A.23) \nAnd \n𝑚({𝑥̂𝑗}𝑗=1\n𝑁) = 0 \n(A.24) \n \nAccording to Lemma 1.6,  𝑓𝑗(𝑓𝑖(𝐵(𝑥𝑖, 𝛿)) ∉𝐿𝑖𝑝1(𝐵(𝑥𝑘, 𝛿) ∪𝐵(𝑥𝑗, 𝛿)), therefore, we have: \n𝑓⋯,𝑘,⋯𝑗.𝑖∉𝐿𝑖𝑝1([𝑐, 𝑑]\\{𝑥̂𝑗}𝑗=1\n𝑁) \n(A.25) \n2). Similarly, if we assume ∀𝐵(𝑥𝑘, 𝛿) ∩𝐵(𝑥𝑗, 𝛿) ≠∅, due to the composition, we have: 𝐵(𝑥𝑘, 𝛿) ∩\n𝐵(𝑥𝑘−1, 𝛿) ≠∅, according to Lemma 1.5,  \n[𝑎, 𝑏] ⊇⋃𝐵(𝑥𝑘, 𝛿) ⊇[𝑐, 𝑑]\n𝐾\n𝑘=1\n \n(A.26) \n18 \n \nTherefore, we can conclude: \n𝑓⋯,𝑘,⋯𝑗.𝑖∉𝐿𝑖𝑝1([𝑐, 𝑑]) \n(A.27) \n \n3). Moreover, if 𝐵(𝑥𝑗, 𝛿) ⊇𝐵(𝑥𝑘, 𝛿) ⊇𝐵(𝑥𝑘+1, 𝛿) ⊇⋯, according to Lemma 1.5, we have: \n⋂𝑖=1\n𝐾𝐵(𝑥𝑖, 𝛿) = Ξ ≠∅ \n(A.28) \nIt means: \n𝑓⋯,𝑘,⋯𝑗.𝑖∈𝐿𝑖𝑝1([𝑎, 𝑏]\\𝐵(𝑥𝑗, 𝛿)) \n(A.29) \nThus, similarly, we have: \n𝑓⋯,𝑘,⋯𝑗.𝑖∈𝐿𝑖𝑝1([𝑎, 𝑏]\\Ξ) \n(A.31) \n \n4). Finally, if ⋯⊇𝐵(𝑥𝑘+1, 𝛿) ⊇𝐵(𝑥𝑘, 𝛿) ⊇𝐵(𝑥𝑗, 𝛿), \nTherefore, based on (1) and (2), we have:  \n𝜔𝑓𝑘,⋯,2,1(𝑥2) = lim\n𝛿→0 sup{|𝑓𝑘,⋯,2,1(𝑥′) −𝑓𝑘,⋯,2,1(𝑥′′)|: 𝑥′, 𝑥′′ ∈[𝑐, 𝑑]} > 𝑀 \n(A.32) \nIt indicates:  \n𝑓⋯,𝑘,⋯𝑗.𝑖∈𝐿𝑖𝑝1([𝑎, 𝑏]\\ ⋃𝐵(𝑥𝑖, 𝛿))\n∞\n𝑖=1\n \n(A.33) \n5). Comprehensively, the situation includes all previously discussed (1) to (4), it is easy to conclude: \n𝑓⋯,𝑘,⋯𝑗.𝑖∈𝐿𝑖𝑝1([𝑎, 𝑏]\\[𝑐, 𝑑] \n(A.34) \nUsing Lemma 1.1, obviously, given ∆: 𝑎= 𝑥0 < 𝑥1 < 𝑥2 < ⋯< 𝑥𝑛= 𝑏, and ∆′: 𝑥′ < 𝑥̂1 < 𝑥̂2 <\n⋯< 𝑥̂𝑛< 𝑥′′,  𝑣∆1 +  𝑣∆2 =  𝑣∆. \n 𝑣∆+  𝑣Δ′ =  𝑣∆1 +  𝑣∆2 +  𝑣Δ′\n= ∑|𝑓(𝑥𝑖) −𝑓(𝑥𝑖−1)|\n𝑛1\n𝑖=1\n+ ∑|𝑓(𝑥𝑖) −𝑓(𝑥𝑖−1)|\n𝑛2\n𝑖=𝑛1\n+ ∑|𝑓(𝑥𝑖) −𝑓(𝑥𝑖−1)|\n𝑛\n𝑖=𝑛2\n \n \n \n(A.35) \nSince  𝑣Δ′ > 𝑀,  𝑣∆+  𝑣Δ′ > 𝑀, it is easy to have: \n∑𝑓𝑖\n𝑡\n𝑡\n𝑖=1\n∉𝐿𝑖𝑝1([𝑎, 𝑏]) \n(A.36) \n \nLemma 1.7 (Contraction of STORM Operator) If denote 𝑆𝑇𝑂𝑅𝑀≝𝒯: ℝ𝑃×𝑄→ℝ𝑃×𝑄, operator \n𝒯 is a bounded contraction operator. \nProof: According to definition of contraction operator, we have: \n‖𝒯𝑡+𝑘𝑋−𝒯𝑡𝑋‖ = ‖ ∑𝜂𝑡𝒹𝑡\n𝑡+𝑘\n𝑖=𝑡+1\n‖ \n(A.37) \nConsidering the equivalence of norms in finite dimensional space and Cauchy-Schwarz inequality, \nwe have: \n‖ ∑𝜂𝑡𝒹𝑡\n𝑡+𝑘\n𝑖=𝑡+1\n‖\n2\n2\n≤‖ ∑𝜂𝑡\n2\n𝑡+𝑘\n𝑖=𝑡+1\n‖\n2\n2\n∙‖ ∑𝒹𝑡\n2\n𝑡+𝑘\n𝑖=𝑡+1\n‖\n2\n2\n \n \n(A.38) \nIn [27], due to the definition of 𝜂𝑡: \n𝜂𝑡≝\n𝑘\n(𝜔+ ∑\n𝐺𝑡\n𝑡\n𝑖=1\n)1/3 \n(A.39) \n19 \n \nAccording to the definition of 𝐺𝑡 as ‖∇𝑓(𝑥𝑡, 𝜉𝑡)‖, if we assume the target function is smooth and \nvariance bouded, we have: \n‖∇𝑓(𝑥𝑡, 𝜉𝑡)‖ ≤𝑀 \n|𝜂𝑡| < 𝜀 \n \n(A.40) \nThen, given 𝑡> 𝑁,  𝑁∈ℕ, we can derive the following formula: \n‖ ∑𝜂𝑡𝒹𝑡\n𝑡+𝑘\n𝑖=𝑡+1\n‖\n2\n2\n≤𝜀∙‖ ∑𝒹𝑡\n2\n𝑡+𝑘\n𝑖=𝑡+1\n‖\n2\n2\n \n \n(A.41) \nIn Eq (A.41), we only consider ‖∑\n𝒹𝑡\n2\n𝑡+𝑘\n𝑖=𝑡+1\n‖: \n‖𝒹𝑡+2𝑋−𝒹𝑡+1𝑋‖\n= ‖∇𝑓(𝑥𝑡+2, 𝜉𝑡+2) −∇𝑓(𝑥𝑡+1, 𝜉𝑡+1) + (1 −𝛼𝑡+2)(𝒹𝑡+1\n−∇𝑓(𝑥𝑡+1, 𝜉𝑡+2) −(1 −𝛼𝑡+1)(𝒹𝑡−∇𝑓(𝑥𝑡, 𝜉𝑡+1)‖ \n(A.42) \nWe can easily conclude: \n‖𝒹𝑡+2𝑋−𝒹𝑡+1𝑋‖\n≤‖∇𝑓(𝑥𝑡+2, 𝜉𝑡+2) −∇𝑓(𝑥𝑡+1, 𝜉𝑡+1)‖\n+ ‖(1 −𝛼𝑡+2)(𝒹𝑡+1 −∇𝑓(𝑥𝑡+1, 𝜉𝑡+2) −(1 −𝛼𝑡+1)(𝒹𝑡\n−∇𝑓(𝑥𝑡, 𝜉𝑡+1)‖ \n(A.43) \nThen we have: \n‖𝒹𝑡+2𝑋−𝒹𝑡+1𝑋‖\n≤‖∇𝑓(𝑥𝑡+2, 𝜉𝑡+2) −∇𝑓(𝑥𝑡+1, 𝜉𝑡+1)‖\n+ ‖𝒹𝑡+1 −𝒹𝑡+ 𝛼𝑡+1𝒹𝑡−𝛼𝑡+2𝒹𝑡+1‖\n+ ‖∇𝑓(𝑥𝑡+1, 𝜉𝑡+2) −∇𝑓(𝑥𝑡, 𝜉𝑡+1)‖ \n \n(A.44) \nAnd we can also conclude: \n‖𝒹𝑡+2𝑋−𝒹𝑡+1𝑋‖ ≤𝜀1 + ‖𝒹𝑡+1 −𝒹𝑡+ 𝛼𝑡+1𝒹𝑡−𝛼𝑡+2𝒹𝑡+1‖ + 𝜀2 \n(A.45) \nEq. (A.45) can be rewritten as below: \n‖𝒹𝑡+2𝑋−𝒹𝑡+1𝑋‖ ≤𝜀1 + (1 −𝛼̂) ∙‖𝒹𝑡+1 −𝒹𝑡‖ + 𝜀2 \n𝛼̂ ≝min (𝛼𝑡+1, 𝛼𝑡+2) \n(A.46) \nObviously, ‖𝒹𝑡+1 −𝒹𝑡‖ ≤𝑀, we have: \n‖𝒹𝑡+2𝑋−𝒹𝑡+1𝑋‖\n‖𝒹𝑡+1 −𝒹𝑡‖\n≤(1 −𝛼̂) + 𝜀1 + 𝜀2 \n(A.47) \nIf 𝑐> 0, according to the definition of 𝛼: \n𝛼𝑡+1 ≝𝑐∙𝜂𝑡\n2 \n(A.48) \nThen, due to ∀𝜀1, 𝜀2, Eq. (A.47) holds: \n‖𝒹𝑡+2𝑋−𝒹𝑡+1𝑋‖\n‖𝒹𝑡+1 −𝒹𝑡‖\n≤(1 −𝛼̂) \n(A.49) \nSince ∀𝑡∈ℕ, 𝛼𝑡> 0: \n‖𝒹𝑡+2𝑋−𝒹𝑡+1𝑋‖\n‖𝒹𝑡+1 −𝒹𝑡‖\n< 1 \n(A.50) \nThus, we proved the STORM operator 𝒯 is a contraction operator within the finite dimensional \nspace. \n \n20 \n \nLemma 1.8 (Contraction of Operators Combination) Given two contraction mappings Φ1 and \nΦ2, we have the composite of two contraction mapping as  Φ2 ∙Φ1. The composite mapping Φ2 ∙\nΦ1 must be contractive. \nProof: According to the definition of contraction linear operator, we have: \n∃𝜁∈(0,1) \n𝜌≝‖Φ𝑥−Φy‖ \n𝜌(Φ𝑥, Φ𝑦) ≤𝜁𝜌(𝑥, 𝑦) \n \n(A.51) \nObviously, and we have: \n𝜌(Φ1𝑢, Φ1𝑣) ≤𝜁𝜌(𝑢, 𝑣) ∀𝜁∈(0,1) \n𝜌(Φ2𝑥, Φ2𝑦) ≤𝜂𝜌(𝑥, 𝑦) ∀𝜂∈(0,1) \n(A.52) \nIf we set: \n𝑥= Φ1𝑢, 𝑦= Φ1𝑣 \n(A.53) \nthe inequality below holds: \n𝜌(Φ2𝑥, Φ2𝑦) ≤𝜂𝜌(Φ1𝑢, Φ1𝑣) ≤𝜁𝜂𝜌(𝑢, 𝑣) \n(A.54) \nSince the definition as  \n∀𝜁, 𝜂∈(0,1), 𝜌(Φ2Φ1𝑢, Φ2Φ1𝑦) ≤𝜁𝜂𝜌(𝑢, 𝑣) \n \n(A.55) \nTheorem 1.3 (Convergence Rate of SENDER in Finite Dimensionality Space) Denote Adam as \nan operator 𝒯 in a finite dimensionality space. Due to the convergence of STORM with a rate of  \n𝒪(𝑇\n1\n2 + 𝜎\n1\n3/𝑇\n1\n3), the convergence of SENDER is guaranteed to be the same as STORM. \nLemma 1.8 (Adam Operator is bounded) [27] If we denote the Adam optimizer operator as  \n𝒯: ℝ𝑃×𝑄→ℝ𝑃×𝑄, we have ‖𝒯‖ <\n1\n√𝑇.  \n \nCorollary 1.1 (General Contraction Operator) According to Lemma 1.2, if denote the operators \n{Φ𝑖}𝑖=1\n𝐾, ∀Φ𝑖 𝑖∈ℕ, Φ𝑖: ℝ𝑆×𝑇→ℝ𝑆×𝑇; considering any combination of operators: Φ𝐾∙⋯∙Φ2 ∙\nΦ1, if at least a single operator Φ𝑖 is contraction operator, and other operators are bounded, such as \n∀𝑖≠𝑘 ‖Φ𝑖‖ ≤𝑀. If and only if ∏\n‖Φ𝑖‖ < 1\n𝐾\n𝑖=1\n, the combination of operator series Φ𝐾∙⋯∙Φ2 ∙\nΦ1 is a contraction operator. \nProof: Obviously, according to Lemma 1.2, use a series as {𝜁𝑖}𝑖=1\n𝐾 to replace 𝜁, 𝜂∈(0,1), \nObviously, we have:  \n𝜁𝑖∈(0,1) 𝑖∈ℕ  \n𝜌(Φ𝐾∙⋯∙Φ2Φ1𝑢, Φ𝐾∙⋯∙Φ2Φ1𝑦) ≤𝜁𝐾∙⋯𝜁2 ∙𝜁1 ∙𝜌(𝑢, 𝑣) \n(A.56) \n \nSince 𝜁𝐾∙⋯𝜁2 ∙𝜁1 < 1, we have proved this corollary. \n \nCorollary 1.2 (Iterative Contraction Operator) According to Lemma 1.2, if denote the operators \n{Φ𝑖}𝑖=1\n𝐾, ∀Φ𝑖 𝑖∈ℕ, Φ𝑖: ℝ𝑃×𝑄→ℝ𝑃×𝑄; considering any combination of operators: Φ𝐾∙⋯∙Φ2 ∙\nΦ1, if at least a single operator Φ𝑖 is contraction operator, and other operators are bounded, such as \n∀𝑖≠𝑘, ‖Φ𝑖‖ ≤𝑀. If and only if lim\n𝑛→∞∏\n‖Φ𝑖‖𝑛\n𝐾\n𝑖=1\n= 𝑐< 1, the combination of operator series Φ𝐾\n𝑛∙\n⋯∙Φ2\n𝑛∙Φ1\n𝑛. \nProof: Obviously, according to Lemma 3.1 and Corollary 3.1, use a series as {𝜁𝑖}𝑖=1\n𝐾 to replace 𝜁, 𝜂∈\n(0,1), \nAnd we have:  \n∀𝜁𝑖∈(0,1) 𝑖∈ℕ  \n(A.57) \n21 \n \n𝜌(Φ𝐾\n𝑛∙⋯∙Φ2\n𝑛∙Φ1\n𝑛𝑢, Φ𝐾\n𝑛∙⋯∙Φ2\n𝑛∙Φ1\n𝑛𝑦) < 𝜁𝑖\n𝑛∙⋯∙𝜁2\n𝑛∙𝜁1\n𝑛∙𝜌(𝑢, 𝑣) \nSince 0 < 𝜁𝑖\n𝑛∙⋯∙𝜁2\n𝑛∙𝜁1\n𝑛< 1, we have proved this corollary. \n \nTheorem 1.3 (Convergence of SENDER in Finite Dimensionality Space) SENDER can converge \nas fast as STORM. \nProof: If we have 𝑈, 𝑉∈ℝ𝑚×𝑛, according to Theorems 3.2-3.4, and Lemma 3.2, the SENDER can \nbe represented as  \n𝑆𝐸𝑁𝐷𝐸𝑅≝(𝒮ℛ𝒩𝒯)𝑘∙𝐼: ℝ𝑀×𝑁⟶ℝ𝑀×𝑁 \n(A.58) \nAccording to Lemma 1.2, Corollary 1.1 and 1.2, we conclude: \n‖𝒯𝑘𝑈−𝒯𝑘𝑉‖ ≤𝜌𝑘‖𝑈−𝑉‖ \n(A.60) \nand 0 < 𝜌𝑘< 1 holds and  𝜌𝑘 equals to 𝒪(𝑇\n1\n2 + 𝜎\n1\n3/𝑇\n1\n3) [27], \nAnd given other definitions of operators adopted by DEMAND, obviously, since all norms of these \noperators are bounded, the following norm inequality holds: \n‖(𝒮ℛ𝒩𝒜)𝑘ℐ∙𝑈−(𝒮ℛ𝒩𝒜)𝑘ℐ∙𝑉‖ ≤( 𝑁\n𝑎𝑠𝑟)𝑘∙‖𝑈−𝑉‖ \n(A.61) \nIf 0 < (\n𝑁\n𝑎𝑠𝑟)𝑘 <1, 𝑘→∞, can guarantee the convergence of SENDER comparable to STORM. It \ndemonstrates that the convergence rate of SENDER would be equal to STORM, since the \nconvergence rate of STORM has been proved as 𝒪(𝑇\n1\n2 + 𝜎\n1\n3/𝑇\n1\n3) , and (\n𝑁\n𝑎𝑠𝑟)𝑘can be rewritten as \n1\n√𝑎𝑠𝑟(\n𝑁\n𝑎𝑠𝑟)𝑘√𝑎𝑠𝑟, if and only if (\n𝑁\n𝑎𝑠𝑟)𝑘√𝑎𝑠𝑟 is a constant 𝒞, and let \n1\n√𝑎𝑠𝑟 be 𝒪(𝑇\n1\n2 + 𝜎\n1\n3/𝑇\n1\n3) . \n \nCollaroy 1.3 (Convergence of SENDER in Infinite Dimensionality Space) Given the infinite \ndimensionality space, the SENDER is denoted as an operator as 𝒟: ℝ∞×∞→ℝ𝑚×𝑛. If we assume 𝒟 \nand ℝ∞×∞ can be defined as infinite matrix and each element can be represented as 𝑑𝑖,𝑗∈𝒟 and \n𝑟𝑖,𝑗∈ℝ∞×∞,  𝑖, 𝑗→∞, respectively. 𝒟 can converge, if and only if 𝑑𝑖,𝑗\n𝑘∙𝑟𝑖,𝑗 should be 𝒪(\n1\n𝑛𝑝). \n𝐷=\n[\n \n \n \n 𝑑1\n𝑑2\n⋮\n𝑑𝑛−1\n⋮\n]\n \n \n \n \n \n(A.62) \nIn Eq. (C.21), operator 𝐷 denotes an infinite dimensionality operator. \n𝑋=\n[\n \n \n \n \n𝑟1\n𝑟2\n⋮\n𝑟𝑛−1\n⋮]\n \n \n \n \n \n(A.63) \nIn Eq. (C.21), without generality, input 𝑋 denotes an infinite dimensionality matrix. \n \nThen, given the operator 𝐷 applied on input matrix 𝑋 as: \n 𝐷⊗𝑋=\n[\n \n \n \n \n𝑑1𝑟1\n𝑑2𝑟2\n⋮\n𝑑𝑛−1𝑟𝑛−1\n⋮\n]\n \n \n \n \n \n(A.64) \nObviously, due to the inequality of norms in the infinite dimensionality space, we examine the ℓ2 \nnorm as an example: \n‖𝐷⊗𝑋‖2 = ∑(𝑑𝑖𝑟𝑖)2\n∞\n𝑖=1\n \n(A.65) \n \nEasily, we can conclude: \n22 \n \n‖𝐷𝑘⊗𝑋‖2 = √∑(𝑑𝑖\n𝑘𝑟𝑖)2\n∞\n𝑖=1\n< ∞⟺lim\n𝑘→∞(𝑑𝑖\n𝑘𝑟𝑖)\n2 = 1\n𝑛𝑝 𝑝> 1 \n(A.66) \n \nTherefore, we have proved the DEMAND converges in an infinite dimensionality space, if and only \nif each element of 𝐷∙𝑋 as  \n1\n𝑛𝑝 , and 𝑝> 1, 𝑛∈ℝ. \n \nLemma 1.9 (Convergence of Alternative Optimization of Real Function) For a series of real \nfunction as {𝑓𝑖,𝑗}𝑖,𝑗=1\n∞\n. If we have: lim\n𝑖→∞𝑓𝑖,𝑗(𝑥) →ℎ𝑀,𝑗, 𝑎. 𝑒. 𝑥∈[𝑎, 𝑏] and lim\n𝑗→∞ℎ𝑀,𝑗→𝑔𝑀,𝑁, 𝑎. 𝑒. 𝑥∈\n[𝑎, 𝑏]. Then, ∃ lim\n𝑘→∞𝑓𝑖𝑘,𝑗𝑘→𝑔𝑀,𝑁 𝑎. 𝑒.  𝑥∈[𝑎, 𝑏] holds.  \nProof: If considering the uniform convergence of {𝑓𝑖,𝑗}𝑖,𝑗=1\n∞\n, since lim\n𝑖→∞𝑓𝑖,𝑗→ℎ𝑀,𝑗, 𝑎. 𝑒. 𝑥∈[0, 1] \nand lim\n𝑗→∞ℎ𝑀,𝑗→𝑔𝑀,𝑁, 𝑎. 𝑒. 𝑥∈[0, 1], according to Riez Theorem, ∃ {𝑓𝑖𝑘,𝑗𝑘}𝑘=1\n∞. \n|𝑓𝑖𝑘,𝑗𝑘−ℎ𝑗| < 𝜀\n2 \n(A.67) \nAnd we have: \n|ℎ𝑗−𝑔| < 𝜀\n2 \n \n(A.68) \nThen, we have:  \n|𝑓𝑖𝑘,𝑗𝑘−ℎ𝑗| + |ℎ𝑗−𝑔| = |𝑓𝑖𝑘,𝑗𝑘−𝑔| < 𝜀 \n \n(A.69) \n \nTheorem 1.4 (Alternative Convergence of SENDER) Given {ℱ𝑖,𝑗,𝑘,𝑡}𝑖,𝑗,𝑘,𝑡=1\n∞\n,  {𝒞 𝑗,𝑘,𝑡}𝑗,𝑘,𝑡=1\n∞\n, \n{ℋ 𝑘,𝑡}𝑘,𝑡=1\n∞\n, and {𝒦 𝑡}𝑡=1\n∞, are series of continuous operator [30] applied on a finite dimensional \nspace, the series of operators. And we have: ℱ𝑖,𝑗,𝑘,𝑡, 𝒞 𝑗,𝑘,𝑡, ℋ 𝑘,𝑡, 𝒦 𝑡: ℝ𝑃×𝑄→ℝ𝑃×𝑄. If we have: \nl𝑖𝑚\n𝑖→∞ℱ𝑖,𝑗→𝒞𝑗,𝑘,𝑡, 𝑙𝑖𝑚\n𝑗→∞𝒞𝑗,𝑘,𝑡→ℋ 𝑘,𝑡, 𝑙𝑖𝑚\n𝑘→∞ℋ𝑘,𝑡→𝒦 𝑡, and l𝑖𝑚\n𝑡→∞𝒦𝑡→𝒢. Then, ∃𝑙𝑖𝑚\n𝑛→∞ℱ𝑖𝑛,𝑗𝑛,𝑘𝑛,𝑡𝑛→𝒢 \nholds. \nProof: According to Lemma 1.5, similarly, let constant 𝑇< ∞, we have: \n‖ℱ𝑖𝑛,𝑗𝑛,𝑘𝑛,𝑡𝑛−𝒞𝑗𝑛,𝑘𝑛,𝑡𝑛‖ < 𝜀\n4𝑇 \n \n(A.70) \nSimilarly, we have: \n‖𝒞𝑗𝑛,𝑘𝑛,𝑡𝑛−ℋ𝑘𝑛,𝑡𝑛‖ < 𝜀\n4𝑇 \n‖ℋ𝑘𝑛,𝑡𝑛−𝒦𝑡𝑛‖ < 𝜀\n4𝑇 \n‖𝒦𝑡𝑛−𝒢‖ < 𝜀\n4𝑇 \n \n \n(A.71) \nThe following inequality holds: \n‖ℱ𝑖𝑘,𝑗𝑘−𝒢‖ = ‖ℱ𝑖𝑛,𝑗𝑛,𝑘𝑛,𝑡𝑛−𝒞𝑗𝑛,𝑘𝑛,𝑡𝑛‖ + ‖𝒞𝑗𝑛,𝑘𝑛,𝑡𝑛−ℋ𝑘𝑛,𝑡𝑛‖\n+ ‖ℋ𝑘𝑛,𝑡𝑛−𝒦𝑡𝑛‖ + ‖𝒦𝑡𝑛−𝒢‖ ≤𝜀\n𝑇 \n(A.72) \nAnd we also have: \n‖ℱ𝑖𝑛,𝑗𝑛,𝑘𝑛,𝑡𝑛𝑋−𝒢𝑋‖ ≤‖ℱ𝑖𝑛,𝑗𝑛,𝑘𝑛,𝑡𝑛−𝒢‖ ∙‖𝑋‖ < 𝑇∙𝜀\n𝑇= 𝜀 \n(A.73) \nThis equation indicates the operator can converge to a fixed point defined on Banach space, using \nalternative strategy and Banach Fixed Point Theorem (Rudin, 1973), if and only if \nlim\n𝑘→∞‖ℱ𝑖𝑛,𝑗𝑛,𝑘𝑛,𝑡𝑛‖ < 1. \n23 \n \nAppendix B \n \nAlgorithm 2.1 (Core \nAlgorithm): Semi-Estimated \nNonlinear Deep Efficient Reconstructor (SENDER) \nInput: 𝐼∈ℝ𝑃×𝑄, 𝐼 is the input signal matrix; set 𝜆> 1 as \nthe penalty parameter; randomly initialize {𝑋𝑘}𝑘=1\n𝑀\n, \n{𝑌𝑘}𝑘=1\n𝑀, {𝑈𝑘}𝑘=1\n𝑀, {𝑉𝑘}𝑘=1\n𝑀and {𝑆𝑘}𝑘=1\n𝑀 ; \nSet 𝑟 as the initial estimated rank of 𝑋1, 𝑌1, 𝑈1, 𝑉1 and layer \n𝑘 as 0.    \n    while 𝑚𝑖𝑛 (𝑟𝑎𝑛𝑘𝑌, 𝑟𝑎𝑛𝑘𝑉) > 1 \n          update 𝑋𝑘 using Eq. (3-1); \n          update 𝑌𝑘 using Eq. (3-2); \n          update 𝑈𝑘 using Eq. (3-3); \n          update 𝑉𝑘 using Eq. (3-4); \n          update 𝑆𝑘  using Eq. (3-5);       \n          use Algorithm 2.2 to estimate 𝑟𝑎𝑛𝑘𝑌 of 𝑌𝑘; \n          use Algorithm 2.2 to estimate 𝑟𝑎𝑛𝑘𝑉 of 𝑉𝑘; \n          𝑘←𝑘+ 1;  \n          𝐼←𝐼−𝑆𝑘; \n   end while \n  M←𝑘; \nUse Algorithm 2.6 to perform matrix back propagation for \n{𝑋𝑘}𝑘=1\n𝑀 and  {𝑌𝑘}𝑘=1\n𝑀;  \nUse Algorithm 2.7 to perform matrix back propagation for , \n{𝑈𝑘}𝑘=1\n𝑀 and {𝑉𝑘}𝑘=1\n𝑀;  \nOutput: {𝑋𝑖}𝑖=1\n𝐾\n∈ℝ𝑀×𝑁, {𝑌𝑖}𝑖=1\n𝐾\n∈ℝ𝑀×𝑁 , {𝑈𝑖}𝑖=1\n𝐾\n∈\nℝ𝑀×𝑁, {𝑉𝑖}𝑖=1\n𝐾\n∈ℝ𝑀×𝑁, and {𝑆𝑖}𝑖=1\n𝐾\n∈ℝ𝑀×𝑁  ; \n \nAlgorithm 2.2:  Rank Reduction Operator (RRO) \nInput: 𝑌𝑘∈ℝ𝒏×𝒎, 𝑌𝑘 is the feature matrix; \n    𝑌𝑅𝑘←𝑄𝑅(𝑌𝑘); \n    𝑚𝑖𝑛𝑅𝑎𝑛𝑘←min(1, 𝑠𝑖𝑧𝑒(𝑌𝑘)) ; \n    𝑒𝑠𝑡𝑅𝑎𝑛𝑘←𝑚𝑖𝑛𝑅𝑎𝑛𝑘−1; \n    𝑑𝑖𝑎𝑔𝑌𝑅𝑘←𝑎𝑏𝑠(𝑑𝑖𝑎𝑔(𝑌𝑅𝑘)); \n    using Algorithm 2.3 calculate the weighted difference of \n𝑑𝑖𝑎𝑔𝑌𝑅𝑘; \n    set weighted difference of 𝑑𝑖𝑎𝑔𝑌𝑅𝑘 as wd; \n    [𝑟𝑎𝑛𝑘𝑀𝑎𝑥1, 𝑝𝑜𝑠𝑀𝑎𝑥1] ←max (𝑤𝑑); \n    using Algorithm 2.4 to calculate the weighted ratio of \n𝑑𝑖𝑎𝑔𝑌𝑅𝑘; \n    set weighted ratio of 𝑑𝑖𝑎𝑔𝑌𝑅𝑘 as wr; \n    [𝑟𝑎𝑛𝑘𝑀𝑎𝑥2, 𝑝𝑜𝑠𝑀𝑎𝑥2] ←max (𝑤𝑟); \n    if rankMax1 is equal to 1 \n        𝑒𝑠𝑡𝑅𝑎𝑛𝑘←𝑝𝑜𝑠𝑀𝑎𝑥1; \n    end if \n    valWR←find( wr>rankMax2 ); \n    if number (valWR) is equal to 1 \n        𝑒𝑠𝑡𝑅𝑎𝑛𝑘←𝑝𝑜𝑠𝑀𝑎𝑥1; \n    end if \n24 \n \n   using Algorithm 2.5 to calculate the weighted correlation \nof 𝑑𝑖𝑎𝑔𝑌𝑅𝑘; \n    set weighted correlation of 𝑑𝑖𝑎𝑔𝑌𝑅𝑘 as wc; \n    [𝑟𝑎𝑛𝑘𝑀𝑎𝑥3, 𝑝𝑜𝑠𝑀𝑎𝑥3] ←max (𝑤𝑐); \n    if rankMax1 is equal to 1 \n        𝑒𝑠𝑡𝑅𝑎𝑛𝑘←𝑝𝑜𝑠𝑀𝑎𝑥3; \n    end if \n    valWC←find( wc>rankMax3 ); \n    if number (valWC) is equal to 1 \n        𝑒𝑠𝑡𝑅𝑎𝑛𝑘←𝑝𝑜𝑠𝑀𝑎𝑥3; \n    end if \n   𝑒𝑠𝑡𝑅𝑎𝑛𝑘←max (𝑝𝑜𝑠𝑀𝑎𝑥1, 𝑝𝑜𝑠𝑀𝑎𝑥2, 𝑝𝑜𝑠𝑀𝑎𝑥3); \nOutput: estRank \n \nAlgorithm 2.3:  Weighted Difference (WD) \nInput: 𝑉𝑒𝑐∈ℝ𝟏×𝒎, 𝑉𝑒𝑐 is a vector; \n  cumSum  ← Calculate cumulative sum of Vec; \n  diffVec  ← Calculate differences between adjacent \nelements of Vec; \n resverseVec ← reverse Vec; \n WD ← abs(diffVec) ./ reverseVec; \nOutput: WD \n \nAlgorithm 2.4:  Weighted Ratio (WR) \nInput: 𝑉𝑒𝑐∈ℝ𝟏×𝒎, 𝑉𝑒𝑐 is a vector; \n     L  ← Calculate length of Vec; \n   ratioVec  ← Vec(1:L-1) ./ Vec(2:L); \n   WR ← (L-2)*ratioVec ./ sum (ratioVec); \nOutput: WR \n \nAlgorithm 2.5:  Weighted Correlation (WC) \nInput: 𝑉𝑒𝑐∈ℝ𝟏×𝒎, 𝑉𝑒𝑐 is a vector; \nWC←Calculate the weight correlation using Eq. (7) \nOutput: WC \n \nAlgorithm 2.6: Matrix Back Propagation (MBP) for \nLinear Model \nInput: {𝑋𝑖}𝑖=1\n𝑀\n∈ℝ𝑇×𝑆, {𝑌𝑖}𝑖=1\n𝑀\n∈ℝ𝑇×𝑆, \nset 𝑍𝑖←\n∏\n𝑋𝑖\n𝑖\n𝑗=1\n𝑌𝑖; \n   for k = T to 1 \n        𝜓⟵∏\n𝑋𝑖\n𝑘−1\n𝑖=1\n; \n        𝐷𝑘⟵𝜓†𝑍𝑘𝑌̂𝑘\n†; \n        if  k<M \n            𝛼̂𝑘⟵𝛼𝑀 \n25 \n \n      else \n           𝛼̂𝑘⟵𝐷𝑘+1𝛼̂𝑘+1,  \n      end if \n     𝑌̂𝑘\n+⨁𝑌̂𝑘\n−⟵𝑌̂𝑘  \n     𝑧𝑘←𝐼−∏\n𝑈𝑖\n𝑘\n𝑖=1\n∙(𝒩𝑘∙𝑉𝑘) \n     𝑌̂𝑘\n+ ⟵𝑌𝑘\n+ ⊙√\n[𝜓𝑇𝑍𝑘]++[𝜓𝑇𝜓]−𝑌̂𝑘\n+\n[𝜓𝑇𝑍𝑘]−+[𝜓𝑇𝜓]+𝑌̂𝑘\n+; \n      |𝑌̃𝑘\n−| ⟵|𝑌̂𝑘\n−| ⊙√\n[𝜓𝑇𝑍𝑘]++[𝜓𝑇𝜓]−|𝑌̃𝑘\n−|\n[𝜓𝑇𝑍𝑘]−+[𝜓𝑇𝜓]+|𝑌̃𝑘\n−|; \n       𝑌̃𝑘\n−⟵−|𝑌𝑘\n−|; \n       𝑌𝑘⟵𝑌̃𝑘\n+⨁𝑌̃𝑘\n−; \n  end for \nOutput: {𝑋𝑖}𝑖=1\n𝑀\n∈ℝ𝑃×𝑄, {𝑌𝑖}𝑖=1\n𝐾\n∈ℝ𝑃×𝑄 ; \n \nAlgorithm 2.7:  Matrix Backpropagation (MBP) for \nNonlinear Model \nInput: {𝑈𝑘}𝑘=1\n𝑀\n, {𝑉𝑘}𝑘=1\n𝑀\n, 𝐼, and set 𝐸= 0.01, MaxIter, \n𝑈𝑘= 𝑙𝑖𝑚\nit→∞𝑈𝑘\n𝑖𝑡, 𝑋𝑘= 𝑙𝑖𝑚\nit→∞𝑋𝑘\n𝑖𝑡, and 𝑌𝑘= 𝑙𝑖𝑚\nit→∞𝑌𝑘\n𝑖𝑡,  \n𝑠= 𝑈𝑘\n𝑖𝑡𝑉𝑘\n𝑖𝑡 \n \nfor it in 1 to MaxIter \n𝐾⟵(∏𝑈𝑘\n𝑀\n𝑘=1\n)\n𝑇\n∙(𝐼−∏𝑋𝑘\n𝑀\n𝑘=1\n∙𝑌𝑀) \n𝑃𝑘\n𝑖𝑡⟵(𝑈𝑘\n𝑖𝑡)\n𝑇𝑈𝑘\n𝑖𝑡∙∏max (𝑈𝑘\n𝑀−1\n𝑘=1\n) \n \n𝑐𝑘\n𝑖𝑡⟵max(𝑈𝑘−1) ∙𝑑𝒩−1(𝑠)\n𝑑𝑠\n, 𝑠= 𝑈𝑘\n𝑖𝑡𝑉𝑘\n𝑖𝑡 \n𝑑𝑘\n𝑖𝑡⟵𝑑𝒩−1(𝑠)\n𝑑𝑠\n, 𝑠= 𝑈𝑘\n𝑖𝑡𝑉𝑘\n𝑖𝑡 \n𝐷𝑘\n𝑖𝑡⟵(𝑈𝑘−1\n𝑖𝑡)𝑇∙(𝑈𝑘−1\n𝑖𝑡\n∙𝒩−1(𝑠) −𝑉𝑘−1\n𝑖𝑡)⨀𝑑𝑘\n𝑖𝑡∙(𝑉𝑘\n𝑖𝑡)𝑇 \n𝑉𝑘\n𝑖𝑡+1 ←𝑉𝑘\n𝑖𝑡−𝑇\n2𝑖𝑡(𝐶𝑘\n𝑖𝑡) \n𝑈𝑘\n𝑖𝑡+1 ←𝑈𝑘\n𝑖𝑡−𝑇\n2𝑖𝑡(𝐷𝑘\n𝑖𝑡) \nend \nOutput: {𝑈𝑘}𝑘=1\n𝑀 and {𝑉𝑘}𝑘=1\n𝑀 \n \n \nTheorem 2.1 (Rank Reduction Operator is bounded) If we denote the sparse operator as  \nℛ: ℝ𝑆×𝑇→ℝ𝑆×𝑇, we have ‖ℛ‖ < ∞. \n26 \n \nProof: According to the definition of operator norm (Rudin, 1973), ‖ℛ‖ ≤𝑠𝑢𝑝\n‖ℛ𝑋‖\n‖𝑋‖ ; obviously, \n‖ℛ𝑋‖ and ‖𝑋‖ is bounded, since both of norms are based on finite dimensional matrix. And if we \ndenote: \n𝑋=\n[\n \n \n \n \n𝑎1\n𝑎2\n⋮\n𝑎𝑛−1\n𝑎𝑛]\n \n \n \n \n, ℛ𝑋=\n[\n \n \n \n \n \n𝑎1\n𝑎2\n⋮\n𝑎𝑘\n⋮\n𝑎𝑛−1]\n \n \n \n \n \n \n \n(B.1) \nEq. (C11) implies: \n𝑠𝑢𝑝\n‖ℛ𝑋‖\n‖𝑋‖ =\n∑\n𝑎𝑖\n2\n𝑛\n𝑖=1\n∑\n(𝑎𝑖−𝑏𝑖)2\n𝑝\n𝑖=𝑢\n+ ∑\n𝑎𝑖\n2\n𝑞\n𝑖=𝑣\n< ∞ \n \n(B.2) \nAlso, if we examine the weighted ratio and weight difference, only considering the finite dimensional \nspace, we have: \n𝑋=\n[\n \n \n \n \n𝑎1\n𝑎2\n⋮\n𝑎𝑛−1\n𝑎𝑛]\n \n \n \n \n, 𝑊𝑅∙𝑋=\n[\n \n \n \n \n \n \n \n𝑎2 𝑎1\n⁄\n𝑎3 𝑎2\n⁄\n⋮\n𝑎𝑘𝑎𝑘−1\n⁄\n⋮\n𝑎𝑛𝑎𝑛−1\n⁄\n0]\n \n \n \n \n \n \n \n 𝑊𝐷∙𝑋=\n[\n \n \n \n \n \n𝑎2 −𝑎1\n𝑎3 −𝑎2\n⋮\n𝑎𝑘−𝑎𝑘−1\n⋮\n𝑎𝑛−𝑎𝑛−1]\n \n \n \n \n \n \nObviously, for each rank estimation, the dimension of input matrix can be reduced at least by one. \nSimilarly, WR, WD, and WC can be considered as the contract operators for dimensional estimation. \nIt demonstrates that the input matrix can be reduced to a vector by n-1 iterations at most. \n \nTheorem 2.2 (Explanation of More Components Detected via SENDER Than ICA) In \ngeneral, DEMAND can detect more components from an input signal than Independent \nComponent Analysis Method. \nProof: At first, we assume all component included in signal matrix as: \n𝐼≝⋃𝜉𝑖\n𝑀\n𝑖=1\n⊆ℝ𝑇×𝑀 \n(B.3) \nA single component can be denoted as following: \n𝜉𝑖≝[𝜉1,𝑖, 𝜉2,𝑖, ⋯𝜉𝑇,𝑖] \n(B.4) \n \nAnd we assume that there is no any overlap in these components: \n∀𝑖≠𝑗 𝜉𝑖∩𝜉𝑗= ∅ \n(B.5) \n \nIf we define ICA operator as below: \n𝐼𝐶𝐴≝𝒯: ℝ𝑇×𝑀⟶ℝ1×𝑀 \n(B.6) \nObviously, when ICA is applied on the input signal, it is easy to conclude: \n𝒯∙𝐼= 𝒯(⋃𝜉𝑖\n𝑀\n𝑖=1\n) = [𝜉1, 𝜉2, ⋯, 𝜉𝑀] \n \n(B.7) \nSimilarly, as previous definition of DEMAND as operator 𝒟, we can have: \n27 \n \n𝒟∙𝐼= 𝐷(⋃𝜉𝑖\n𝑀\n𝑖=1\n)\n= [𝜉1, 𝜉2, ⋯, 𝜉𝑀, (𝜉1, 𝜉2), (𝜉1, 𝜉3), ⋯, (𝜉1, 𝜉3, ⋯, 𝜉𝑘), ⋯] \n \n(B.8) \nIt is easy to calculate the number of components detected by ICA, due to independent \nconstraint: \n|𝒯∙𝐼| = 𝑀 \n(B.9) \nNevertheless, we can conclude: \n|𝒟∙𝐼| = 2𝑀 \n(B.10) \nObviously, we also have: \n𝑀≪2𝑀 \n(B.11) \n \nInequality (B.9) demonstrates that the number of components identified by SENDER \nshould be more than ICAs.  \n \nTheorem 2.3 (Sparsity Operator is Contraction) If we denote the sparse operator as 𝒮: ℝ𝑆×𝑇→\nℝ𝑆×𝑇, we have ‖𝒮‖ < ∞. \nProof: according to the definition of operator norm (Rudin, 1973), ‖𝒮‖ ≤𝑠𝑢𝑝\n‖𝒮𝑋‖\n‖𝑋‖ ; obviously, \n‖𝒮𝑋‖ and ‖𝑋‖ is bounded, since both of norms are based on finite dimensional matrix. And if we \ndenote: \n𝑋=\n[\n \n \n \n \n𝑎1\n𝑎2\n⋮\n𝑎𝑛−1\n𝑎𝑛]\n \n \n \n \n 𝒮𝑋=\n[\n \n \n \n \n𝑎1\n0\n⋮\n𝑎𝑛−1\n𝑎𝑛]\n \n \n \n \n \n \n(B.12) \nWithout loss of generality, and based on Lemma 1.2, we calculate the ℓ2 norm, and we have: \n𝑠= ‖𝒮‖ ≤𝑠𝑢𝑝\n‖𝒮𝑋‖\n‖𝑋‖ =\n∑\n(𝑎𝑖)2\n𝑘\n𝑖=1\n∑\n(𝑎𝑖)2\n𝑛\n𝑖=1\n \n \n(B.13) \n \nSince 𝑘< 𝑛,  \n𝑠= ‖𝒮‖ < 1 \n \n(B.14) \nThis inequality demonstrates that ‖𝒮‖ is contraction operator. \n \nTheorem 2.4 (Random Initialization Operator is bounded) If we denote the sparse operator as \nℐ: ℝ𝑆×𝑇→ℝ𝑆×𝑇, we have ‖ℳ‖ < ∞. \nProof: according to the definition of operator norm (Rudin 1973), ‖ℐ‖ ≤𝑠𝑢𝑝\n‖𝑀𝑋‖\n‖𝑋‖ ; obviously, ‖ℐ𝑋‖ \nand ‖𝑋‖ is bounded, since both of norms are based on finite dimensional matrix. And if we denote: \n𝑋=\n[\n \n \n \n \n𝑎1\n𝑎2\n⋮\n𝑎𝑛−1\n𝑎𝑛]\n \n \n \n \n ℐ𝑋=\n[\n \n \n \n 𝑏1\n𝑏2\n⋮\n𝑏𝑛−1\n𝑏𝑛]\n \n \n \n \n ‖𝑋‖ < ∞ ‖ℐ𝑋‖ < ∞ \n \n(B.15) \nObviously, ‖ℐ‖ < ∞.  \n \n \nTheorem 2.5 (Sparsity Operator is Contraction) If we denote the sparse operator as 𝒮: ℝ𝑆×𝑇→\nℝ𝑆×𝑇, we have ‖𝒮‖ < ∞. \n28 \n \nProof: according to the definition of operator norm (Rudin, 1973), ‖𝒮‖ ≤𝑠𝑢𝑝\n‖𝒮𝑋‖\n‖𝑋‖ ; obviously, \n‖𝒮𝑋‖ and ‖𝑋‖ is bounded, since both of norms are based on finite dimensional matrix. And if we \ndenote: \n𝑋=\n[\n \n \n \n \n𝑎1\n𝑎2\n⋮\n𝑎𝑛−1\n𝑎𝑛]\n \n \n \n \n 𝒮𝑋=\n[\n \n \n \n \n𝑎1\n0\n⋮\n𝑎𝑛−1\n𝑎𝑛]\n \n \n \n \n \n \n(B.16) \nWithout loss of generality, and based on Lemma 1.2, we calculate the ℓ2 norm, and we have: \n𝑠= ‖𝒮‖ ≤𝑠𝑢𝑝\n‖𝒮𝑋‖\n‖𝑋‖ =\n∑\n(𝑎𝑖)2\n𝑘\n𝑖=1\n∑\n(𝑎𝑖)2\n𝑛\n𝑖=1\n \n \n(B.17) \n \nSince 𝑘< 𝑛,  \n𝑠= ‖𝒮‖ < 1 \n \n(B.18) \nThis inequality demonstrates that ‖𝒮‖ is contraction operator. \n \n",
  "categories": [
    "cs.LG",
    "eess.IV",
    "q-bio.NC"
  ],
  "published": "2022-09-12",
  "updated": "2022-09-12"
}