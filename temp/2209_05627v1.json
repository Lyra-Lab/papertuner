{
  "id": "http://arxiv.org/abs/2209.05627v1",
  "title": "SENDER: SEmi-Nonlinear Deep Efficient Reconstructor for Extraction Canonical, Meta, and Sub Functional Connectivity in the Human Brain",
  "authors": [
    "Wei Zhang",
    "Yu Bao"
  ],
  "abstract": "Deep Linear and Nonlinear learning methods have already been vital machine\nlearning methods for investigating the hierarchical features such as functional\nconnectivity in the human brain via functional Magnetic Resonance signals;\nhowever, there are three major shortcomings: 1). For deep linear learning\nmethods, although the identified hierarchy of functional connectivity is easily\nexplainable, it is challenging to reveal more hierarchical functional\nconnectivity; 2). For deep nonlinear learning methods, although non-fully\nconnected architecture reduces the complexity of neural network structures that\nare easy to optimize and not vulnerable to overfitting, the functional\nconnectivity hierarchy is difficult to explain; 3). Importantly, it is\nchallenging for Deep Linear/Nonlinear methods to detect meta and sub-functional\nconnectivity even in the shallow layers; 4). Like most conventional Deep\nNonlinear Methods, such as Deep Neural Networks, the hyperparameters must be\ntuned manually, which is time-consuming. Thus, in this work, we propose a novel\ndeep hybrid learning method named SEmi-Nonlinear Deep Efficient Reconstruction\n(SENDER), to overcome the aforementioned shortcomings: 1). SENDER utilizes a\nmultiple-layer stacked structure for the linear learning methods to detect the\ncanonical functional connectivity; 2). SENDER implements a non-fully connected\narchitecture conducted for the nonlinear learning methods to reveal the\nmeta-functional connectivity through shallow and deeper layers; 3). SENDER\nincorporates the proposed background components to extract the sub-functional\nconnectivity; 4). SENDER adopts a novel rank reduction operator to implement\nthe hyperparameters tuning automatically. To further validate the\neffectiveness, we compared SENDER with four peer methodologies using real\nfunctional Magnetic Resonance Imaging data for the human brain.",
  "text": " \n \nSENDER: SEmi-Nonlinear Deep Efficient Reconstructor \nfor Extraction Canonical, Meta, and Sub Functional \nConnectivity in the Human Brain \n \nWei Zhang  \nAugusta University \nwzhang2@augusta.edu \n \nYu Bao \nJames Madison University \nbao2yx@jmu.edu \nAbstract \nDeep Linear and Nonlinear learning methods have already been vital machine \nlearning methods for investigating the hierarchical features such as functional \nconnectivity in the human brain via functional Magnetic Resonance signals; \nhowever, there are three major shortcomings: 1). For deep linear learning methods, \nalthough the identified hierarchy of functional connectivity is easily explainable, \nit is challenging to reveal more hierarchical functional connectivity; 2). For deep \nnonlinear learning methods, although non-fully connected architecture reduces the \ncomplexity of neural network structures that are easy to optimize and not \nvulnerable to overfitting, the functional connectivity hierarchy is difficult to \nexplain; 3). Importantly, it is challenging for Deep Linear/Nonlinear methods to \ndetect meta and sub-functional connectivity even in the shallow layers; 4). Like \nmost conventional Deep Nonlinear Methods, such as Deep Neural Networks, the \nhyperparameters must be tuned manually, which is time-consuming. \nThus, in this work, we propose a novel deep hybrid learning method named SEmi-\nNonlinear Deep Efficient Reconstruction (SENDER), to overcome the \naforementioned shortcomings: 1). SENDER utilizes a multiple-layer stacked \nstructure for the linear learning methods to detect the canonical functional \nconnectivity; 2). SENDER implements a non-fully connected architecture \nconducted for the nonlinear learning methods to reveal the meta-functional \nconnectivity through shallow and deeper layers; 3). SENDER incorporates the \nproposed background components to extract the sub-functional connectivity; 4). \nSENDER adopts a novel rank reduction operator to implement the \nhyperparameters tuning automatically.  \nTo further validate the effectiveness, we compared SENDER with four peer \nmethodologies using real functional Magnetic Resonance Imaging data for the \nhuman brain. Furthermore, the validation results show that SENDER outperforms \nthe investigated methodologies regarding the reconstruction of identifiable \ncanonical, meta, and sub-functional connectivity in the human brain, as well as \nefficiency and identifiability. \n \n2 \n \n1 \nIntroduction \nThe hierarchy of functional organization in the human brain [1-4] has been revealed by multiple deep \nlinear machine learning techniques, such as Low-to High-Dimensional Independent Components \nAnalysis (DICA) [5], Sparse Deep Dictionary Learning (SDDL) [6], Deep Non-negative Matrix \nFactorization (DNMF) [7], [8]. In addition, with the development of deep learning methods, a variety \nof deep nonlinear methods known as Deep Neural Networks (DNNs), e.g., Deep Convolutional Auto \nEncoder (DCAE), Restricted Boltzmann Machine (RBM), Deep Belief Network (DBN), and \nConvolutional Neural Network (CNN) [9-15], have been applied to discovering the hierarchical \nspatial features in brain, i.e., functional connectivity (FC), using functional Magnetic Resonance \nImaging (fMRI). For instance, the RBM can extract hierarchical temporal features and effectively \nreconstruct FC networks with high accuracy [16, 17]. Furthermore, other latest research works have \nfound the reasonable hierarchical temporal organization of task-based fMRI time series, each with \ncorresponding task-evoked FCs [9, 10, 16, 17] using DCAE, RBM, and DBN. These deep learning \ntechniques are generally considered deep nonlinear methods, e.g., DNNs, constructed with nonlinear \nactivation functions, e.g., Sigmoid and/or Rectified Linear Unit (ReLU) [18].  \nFrom a neuroscience perspective, the reveal of FCs by methodologies can be divided into two folds: \n1). Deep Linear methods, e.g., DNMF, and SDDL, usually focus on identifying meta-FCs explained \nas the recombination of canonical FCs reported by Smith in 2009 [19]; 2). On the other hand, Deep \nNonlinear methods, e.g., DNNs, concentrate on investigating both meta- and sub-FCs, i.e., the lower-\nlevel features. Due to the utilization of the nonlinear activation functions, Deep Nonlinear Methods \nusually have a more substantial perception of meta-FCs than Deep Linear methods [12], [14]. \nHowever, these meta-FCs are often detected at the deeper layer in most Deep Linear/Nonlinear \nmethods and are more difficult to interpret and/or explain [12], [14].  \nTherefore, we aim to propose a more advanced deep hybrid method that can benefit from both Deep \nLinear and nonlinear methods to simultaneously discover more identifiable canonical, meta, and sub-\nFCs from the shallow and deep layers. From a technical perspective, SENDER inherits the vital \nadvantage of a deep linear method that it is easy to be optimized and does not require large training \nsamples due to the convex target function, and the advantage of the deep nonlinear method such as \nmore substantial perception using activation functions, and efficient non-fully connected \narchitectures; in contrast, a fully connected structure in deep nonlinear methods enables the \nsubstantial perception but also involves in the difficult training issues, requiring large training \nsamples to avoid overfitting and advanced optimizer to search the global optimum. \nFurthermore, a more advanced deep hybrid method should overcome other shortcomings of deep \nnonlinear learning methods, such as DBN, Deep Boltzmann Machine (DBM), and DCAE, such as \n1) large training samples [19-22]; 2) extensive computational resources, e.g., graphics processing \nunits (GPUs) or tensor processing units (TPUs) [10, 11, 13]; 3) manual tuning of all hyperparameters, \ne.g., the number of layers and size of a dictionary, whereas some parameters such as sparse trade-off \nand step-length, are not denoted as hyperparameters [8-10]; 4) time-consuming training process [13, \n14]; 5) uncertainty of convergence to the global optimum [13, 14, 19, 20, 22]; and 6) \"black box\" \nresults that are challenging to explain [13, 14, 19].  \nContributions. To be more specific, as follow: \n1). Accurate Approximation to Original Input. We have proved that SENDER provides a comparable \naccuracy for approximation to original inputs to DNNs. Theorem 1.1 in Appendix A, Supplementary \nMaterial, presents the conclusion and proof details. \n2). Advanced Hybrid Architecture. Unlike any deep linear or nonlinear method, SENDER employs \na deep linear and nonlinear method with non-fully connected architectures, which provides an \nopportunity to explore the meta-FCs and the sub-FCs in the brain by introducing the background \nfeature matrix. Theorem 1.2 in Appendix A, Supplementary Material, discusses the theoretical \nanalysis of the advantages of the proposed deep hybrid architecture for learning. \n3). Fast Convergence Rate. Given STORM [26] is used as an efficient optimizer to update all \nvariables of SENDER, our theoretical analysis demonstrates that SENDER can maintain the same \nconvergence rate as STORM itself, according to Theorem 1.3, Appendix A, Supplementary Material. \nMoreover, since the dimension of a dataset is continuously increasing, we also discuss the \nconvergence of SENDER in an infinite dimensionality space, where all proofs can be found in \nCorollary 1.4 in Appendix A, Supplementary Material. \n3 \n \n4). Automatic Hyperparameters Tuning. To implement the automatic tuning of hyperparameters, \ne.g., number of layers and number of units/neurons in each layer [23, 24], we develop a rank \nreduction technique named rank reduction operator (RRO) for SENDER. Specifically, RRO utilizes \nthe orthogonal decomposition, e.g., QR decomposition, to estimate the rank of feature matrices, i.e., \nthe number of units, via the weighted ratio (WR), the weighted difference (WD), and the weighted \ncorrelation (WC). These three techniques can consistently reduce the size of feature matrices through \nall layers until the estimated number of features equals one, suggesting the completion of \ndecomposition. Thus, due to the generalization and efficacy of QR decomposition, RRO can tune all \nhyperparameters faster than Singular Value Decomposition (SVD) adopted by Principal Component \nAnalysis (PCA) [23, 24]. The details of RRO implementation can be viewed as Algorithms 2.3 to \n2.5, Appendix B, in Supplementary Material. In addition, the theoretical analytics of RRO can be \nviewed as Theorem 2.1, included in Appendix B, Supplementary Material. \n4). Reduced Accumulative Error via Matrix Backpropagation (MBP). Given that the accumulative \nerror could potentially deteriorate the reconstruction accuracy, we utilize a technique MBP [6, 7, 25] \nto reduce the accumulative error. The implementation of MBP can be found in Algorithm 2.5 in \nAppendix B, Supplementary Material. \nRelated Works and Methodological Validation. SENDER is validated on real resting-state fMRI \nsignals and compared with four other peer methods. The validation results show that SENDER can \ndetect more identifiable canonical, meta, and sub-FCs in the human brain than the representative \ndeep linear/nonlinear methods and is easier to be optimized than DNNs based on our theoretical \nanalyses in Theorem 1.2. Furthermore, unlike most deep learning methods, some meta-FCs can be \nderived from SENDER even at shallow layers. Moreover, sub-FCs can be directly extracted from the \nbackground component matrix, whereas minor features, e.g., sub-FCs, are detected through shallow \nto deeper layers in most deep nonlinear methods. \n \n2 \nMethod \nThis section provides the details of SENDER, including optimizer, optimization function, automatic \nhyperparameters tuning technique, MBP, theoretical analysis of the convergence rate, and \napproximate accuracy. \n \n2.1         Semi-Estimated Nonlinear Deep Efficient Reconstructor \nAs introduced, SENDER employs a hybrid architecture of Deep Linear/Nonlinear methods with an \nefficient non-fully connected architecture and the nonlinear activation function used in DNNs. In \ndetail, the optimization function governing SENDER is shown below: \nğ‘šğ‘–ğ‘› ğ‘†ğ‘– âˆˆ â„ğ‘šÃ—ğ‘› â‹ƒâ€–ğ‘†ğ‘–â€–1\nğ‘˜\nğ‘–=1\n \nğ‘ . ğ‘¡. âˆ€ğ‘˜âˆˆ[2, ğ‘€], (âˆğ‘‹ğ‘–\nğ‘˜\nğ‘–=1\n) âˆ™ğ‘Œğ‘˜+ (âˆğ‘ˆğ‘–\nğ‘˜\nğ‘–=1\n) âˆ™ğ’©ğ‘˜âˆ™ğ‘‰ğ‘˜+ ğ‘†ğ‘˜= ğ¼ \n(âˆğ‘‹ğ‘–\nğ‘—\nğ‘–=1\n) âˆ™ğ‘Œğ‘—+ (âˆğ‘ˆğ‘–\nğ‘—\nğ‘–=1\n) âˆ™ğ’©ğ‘—âˆ™ğ‘‰ğ‘—+ ğ‘†ğ‘—â†ğ¼âˆ’ğ‘†ğ‘—âˆ’1, âˆ€ 1 â‰¤ğ‘–< ğ‘—â‰¤ğ‘˜ \n \n \n(1) \nwhere {ğ‘‹ğ‘–}ğ‘–=1\nğ‘˜\n represents the hierarchical weight matrices or mixing matrices of the linear method, \nfor instance, ğ‘‹ğ‘– indicates the weight matrix of linear method of at the ith layer,  and ğ‘˜ denotes the \ntotal number of layers. similarly, {ğ‘ˆğ‘–}ğ‘–=1\nğ‘˜\n denotes the weight matrices for nonlinear method at ith \nlayer. Furthermore, {ğ‘Œğ‘–}ğ‘–=1\nğ‘˜\n represents the canonical or meta-FCs derived via linear method; for \ninstance, ğ‘Œğ‘– indicates the canonical or meta-FCs of the ith layer; and meanwhile {ğ‘‰ğ‘–}ğ‘–=1\nğ‘˜\n defines the \nmeta-FCs revealed via the nonlinear method. Furthermore, {ğ‘†ğ‘–}ğ‘–=1\nğ‘˜\n is a set of matrices that represent \nthe background components, which are denoted as sub-FCs, due to their sparsity. Moreover, ğ’©ğ‘– \nrepresents the nonlinear activation function at the ith layer, e.g., Sigmoid or Rectified Linear Unit \n4 \n \n(ReLU) [14, 20]. And, if we assume the total number of layers as ğ‘€, the original input data ğ¼ can be \ndecomposed following: (âˆ\nğ‘‹ğ‘–\nğ‘€\nğ‘–=1\n) âˆ™ğ‘Œğ‘€+ (âˆ\nğ‘ˆğ‘–\nğ‘€\nğ‘–=1\n) âˆ™ğ’©ğ‘€âˆ™ğ‘‰ğ‘€+ ğ‘†ğ‘€. \nAs shown in Eq. (1), our fundamental assumption is the previously revealed FCs, e.g., ğ‘Œğ‘–âˆ’1 or ğ‘‰ğ‘–âˆ’1, \ncan be decomposed further as a linear product of a deeper weight matrix ğ‘‹ğ‘– and FCs as ğ‘Œğ‘– or a \nnonlinear representation of a deeper weight matrix ğ‘ˆğ‘– and FCs as ğ‘‰ğ‘–, respectively. In addition, the \noptimization function governing SENDER consists of more variables than conventional deep \nlinear/nonlinear methods, such as DICA, DNMF, and SDDL.  \nBefore optimizing Eq. (1), we can convert it into an augmented Lagrangian function. If considering \nthe kth layer, we have: \nâ„’ğœŒ(âˆğ‘‹ğ‘–\nğ‘˜\nğ‘–=1\n, ğ‘Œğ‘˜,  ğ‘†ğ‘˜, ğ’©ğ‘˜â‰ğœŒ\n2 â€–ğ¼âˆ’ (âˆğ‘‹ğ‘–\nğ‘˜\nğ‘–=1\n) âˆ™ğ‘Œğ‘˜âˆ’(âˆğ‘ˆğ‘–\nğ‘˜\nğ‘–=1\n) âˆ™(ğ’©ğ‘˜âˆ™ğ‘‰ğ‘˜)â€–\nğ¹\n2\n+ 1\nğœŒâ€–ğ‘†ğ‘˜â€–1 \n(2) \nThe sparse trade-off controlling the sparsity of background components denoted as â‹ƒ\nâ€–ğ‘†ğ‘–â€–1\nğ‘˜\nğ‘–=1\n is \ndetermined by \n1\nğœŒ that can also be estimated using Rose Algorithm [27]. Naturally, it is easier to \nemploy alternative optimization strategies [25, 26] to optimize Eq. (2). Due to the efficacy of the \nrecent reported STORM optimizer [27], we adopt STORM to update all the variables included in \nSENDER. And the â„“1 norm of ğ‘†ğ‘˜ shown in Eq. (1) can be solved directly using the shrinkage method \n[28]. \nDenote STORM [26] as an operator ğ’¯. The iterative format of STORM using an alternative strategy \nof optimization to update all the variables in Eq. (2) can be presented as follows: \nğ‘‹ğ‘˜\nğ‘–ğ‘¡+1 â† ğ’¯âˆ™ğ‘‹ğ‘˜\nğ‘–ğ‘¡ \n(3-1) \n ğ‘Œğ‘˜\nğ‘–ğ‘¡+1 â†ğ’¯âˆ™ğ‘Œğ‘˜\nğ‘–ğ‘¡ \n(3-2) \n ğ‘ˆğ‘˜\nğ‘–ğ‘¡+1 â†ğ’¯âˆ™ğ‘ˆğ‘˜\nğ‘–ğ‘¡ \n(3-3) \n ğ‘‰ğ‘˜\nğ‘–ğ‘¡+1 â†ğ’¯âˆ™(ğ’©ğ‘˜âˆ™ğ‘‰ğ‘˜\nğ‘–ğ‘¡) \n(3-4) \n ğ‘†ğ‘˜\nğ‘–ğ‘¡+1 â†ğ‘†â„ğ‘Ÿğ‘–ğ‘›ğ‘˜ğ‘ğ‘”ğ‘’ [ğ¼âˆ’ (âˆğ‘‹ğ‘˜\nğ‘–ğ‘¡\nğ‘˜\nğ‘–=1\n) âˆ™ğ‘Œğ‘˜\nğ‘–ğ‘¡âˆ’(âˆğ‘ˆğ‘˜\nğ‘–ğ‘¡\nğ‘˜\nğ‘–=1\n) âˆ™(ğ’©ğ‘˜âˆ™ğ‘‰ğ‘˜\nğ‘–ğ‘¡)] \n \n(3-5) \nIn detail, in Eqs. (3-1) to (3-4), the current iteration is represented as it; for example, in Eq. (3-1), \nğ‘‹ğ‘˜\nğ‘–ğ‘¡ is updated by the optimizer STORM while ğ‘Œğ‘˜\nğ‘–ğ‘¡, ğ‘ˆğ‘˜\nğ‘–ğ‘¡ , and ğ‘‰ğ‘˜\nğ‘–ğ‘¡  are treated as constants; similar \nmechanism applies to ğ‘Œğ‘˜\nğ‘–ğ‘¡, ğ‘ˆğ‘˜\nğ‘–ğ‘¡, and ğ‘‰ğ‘˜\nğ‘–ğ‘¡. Finally, Eq. (3-5) demonstrates the shrinkage and \nminimization of the background components, i.e., sub-FCs denoted as ğ‘†ğ‘˜\nğ‘–ğ‘¡+1. \n \n2.2       Rank Reduction Operator for Automatic Tuning Hyperparameters \nTo implement the automatic hyperparameters tuning and reduce the high dimensionality of the \noriginal dataset, we introduce a novel technique named RRO which aims to calculate the rank of the \ncurrent feature matrices, e.g., {ğ‘Œğ‘–}ğ‘–=1\nğ‘˜\n and {ğ‘‰ğ‘–}ğ‘–=1\nğ‘˜\n in SENDER, until the rank of the feature matrices \nare equivalent to one. More specifically, RRO performs rank-revealing by consistently using \northogonal decomposition via QR factorization to efficiently estimate the rank of feature matrices \n[23, 24]. Due to the effectiveness of QR factorization, RRO can be used to decompose sparse and \novercomplete matrices. The mathematical formula of RRO is: \nâ„›\n[\n \n \n \n \nğ‘1\nğ‘2\nâ‹®\nğ‘ğ‘›âˆ’1\nğ‘ğ‘›]\n \n \n \n \n=\n[\n \n \n \n \n ğ‘1\n(1)\nğ‘2\n(1)\nâ‹®\nğ‘ğ‘›âˆ’2\n(1)\nğ‘ğ‘›âˆ’1\n(1)]\n \n \n \n \n \n â„›ğ‘˜\n[\n \n \n \n \nğ‘1\nğ‘2\nâ‹®\nğ‘ğ‘›âˆ’1\nğ‘ğ‘›]\n \n \n \n \n=\n[\n \n \n \n \n \nğ‘1\n(1)\nğ‘2\n(1)\nâ‹®\nğ‘ğ‘›âˆ’ğ‘˜âˆ’1\n(1)\nğ‘ğ‘›âˆ’ğ‘˜\n(1) ]\n \n \n \n \n \n= [ğ‘Ì‚] \n \n(4) \nwhere operator â„› represents RRO, and {ğ‘ğ‘–}ğ‘–=1\nğ‘› is a series of vectors with ğ‘ğ‘– representing a single \nvector. Assume RRO is repeatedly applied on a series of vectors {ğ‘ğ‘–}ğ‘–=1\nğ‘›\n for k times, we have \nğ‘Ÿğ‘ğ‘›ğ‘˜(â„›ğ‘˜âˆ™[ğ‘1, ğ‘2, â‹¯, ğ‘ğ‘›]) < ğ‘Ÿğ‘ğ‘›ğ‘˜([ğ‘1, ğ‘2, â‹¯, ğ‘ğ‘›]) hold; furthermore, if ğ‘˜ is large enough, e.g., \nâˆ€ğ‘˜> 0, âˆƒ ğ‘âˆˆâ„•, when ğ‘˜> ğ‘, ğ‘Ÿğ‘ğ‘›ğ‘˜(â„›ğ‘˜âˆ™[ğ‘1, ğ‘2, â‹¯, ğ‘ğ‘›]) = 1 holds, thus ğ‘˜ is treated as the total \nnumber of layers since the rank of the feature matrix at kth layer equals one after utilizing RRO for k \n5 \n \ntimes repeatedly. Moreover, we prove, if â„›: â„ğ‘€Ã—ğ‘â†’â„ğ‘€Ã—ğ‘, ğ‘€< âˆ,  ğ‘< âˆ, then we have â€–â„›â€– <\nâˆ, meaning the operator â„›, i.e., RRO technique, is a bounded operator denoted in a finite-\ndimensional space. The detail of the proof can be viewed in Theorem 2.2, Appendix B, \nSupplementary Material. \nAssume r* is the initially estimated rank and r is the optimal rank estimation of the input signal \nmatrix ğ¼, we have r*â‰¥r; the diagonal of the upper-triangular matrix can be achieved after applying \nQR factorization on signal matrix ğ¼. In detail, at first, the diagonal of matrix R, derived from the \nfeature matrix using QR decomposition, is non-increasing in magnitude [23, 24]; furthermore, along \nthe main diagonal of matrix R, three techniques named WR, WD, and WC are applied to calculate the \nmaximum rank shown in Eqs (5)-(7); then, ğ¼ is replaced by ğ¼âˆ’ğ‘†ğ‘˜, ğ‘˜= 1,2, â‹¯, ğ‘€, iteratively. It \nindicates that the rank-reducing technique can yield a reasonable solution using QR factorization \n[23, 24]. The following formulas provide details of WR, WD, and WC.  \nAssume ğ‘‘âˆˆâ„1Ã—ğ‘Ÿ and ğ‘Ÿâˆˆâ„ğ‘Ÿâˆ’1,  then WR can be calculated by Eq. (5): \nğ‘‘ğ‘–â†|ğ‘…ğ‘–ğ‘–| \nğ‘¤ğ‘Ÿğ‘–â†ğ‘‘ğ‘–\nğ‘‘ğ‘–+1\n \n (5) \nwhere ğ‘…ğ‘–ğ‘– denotes a diagonal element of matrix R calculated by QR decomposition and ğ‘¤ğ‘Ÿğ‘– is an \nelement of WR. The value of each WR is derived by the ratio of the current element of diagonal and \nthe next element as shown in Eq. (5). \nSimilarly, WD can be derived by: \nğ‘¤ğ‘‘ğ‘–â†\n|ğ‘‘ğ‘–âˆ’ğ‘‘ğ‘–âˆ’1|\nâˆ‘\nğ‘‘ğ‘˜\nğ‘–âˆ’1\nğ‘˜=1\n \n(6) \nIn Eq. (6), WD is defined as the absolute difference between the current diagonal element and the \nprevious one divided by the cumulative sum of all the previous diagonal elements. \nFurthermore, Eq. (7) describes the proposed WC as: \nğ‘Ÿğ‘–â†|ğ‘…ğ‘–|, 1 â‰¤ğ‘–â‰¤ğ‘› \nğ‘¤ğ‘ğ‘–â†\n|ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘Ÿğ‘–âˆ’2, ğ‘Ÿğ‘–âˆ’1) âˆ’ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘Ÿğ‘–âˆ’1, ğ‘Ÿğ‘–)|\nâˆ‘\nâ€–ğ‘Ÿğ‘˜â€–2\n2\nğ‘–\nğ‘˜=ğ‘–âˆ’2\n, 3 â‰¤ğ‘–â‰¤ğ‘›  \n(7) \nwhere ğ‘¤ğ‘ğ‘– represents an element of WC and is the ratio of the absolute difference of three adjacent \ncolumns and the summed of all vectorsâ€™ â„“2 norm. \nThus, the RRO iteratively calculates the maximum value position from WR, WD, and WC to estimate \nthe rank of R. The details of pseudocodes to implement WR, WD, and WC can be viewed in \nAlgorithm 2.2, 2.3, and 2.4 in Appendix B, Supplementary Material. \n \n2.3       Matrix Backpropagation \nAnother important technical contribution introduced in this work is MBP implemented to SENDER \nto further reduce the potential accumulative error after finishing the updates of all variables in \nSENDER. Assume the number of the total layers as ğ‘€, Eqs. (8)-(10) describe the details of MBP \napplied to the linear method part, and Eqs. (11)-(13) provide the mathematic formula of MBP for \nnonlinear method part in SENDER [31], [32], [42]. All variables, such as ğ‘‹ğ‘˜, ğ‘Œğ‘˜, ğ‘ˆğ‘˜, ğ‘‰ğ‘˜, and ğ¼ are \ndenoted as the same in Section 2.1. \nğ‘ŒÌ‚ğ‘˜âŸµğ‘Œğ‘˜, ğ‘˜= ğ‘€\nğ‘ŒÌ‚ğ‘˜âŸµğ‘‹ğ‘˜+1ğ‘ŒÌ‚ğ‘˜+1, ğ‘˜< ğ‘€ \n(8) \nIn addition, we denote the product of hierarchical weight matrices as ğœ“ in Eq. (9) \nğœ“âŸµâˆ\nğ‘‹ğ‘–\nğ‘˜\nğ‘–=1\n 1 < ğ‘˜< ğ‘€ \n(9) \n6 \n \nThen, the following equation describes the crucial steps of MBP to update variables of {ğ‘‹ğ‘–}ğ‘–=1\nğ‘€, and \n{ğ‘Œğ‘–}ğ‘–=1\nğ‘€ representing the hierarchical weight and feature matrices including all canonical and some \nmeta-FCs, respectively. \nğ‘ğ‘˜â†ğ¼âˆ’âˆğ‘ˆğ‘–\nğ‘˜\nğ‘–=1\nâˆ™(ğ’©ğ‘˜âˆ™ğ‘‰ğ‘˜) \n(10-1) \nğ‘ŒÌ‚ğ‘˜\n+ âŸµğ‘Œğ‘˜\n+ âŠ™âˆš[ğœ“ğ‘‡ğ‘ğ‘˜]+ + [ğœ“ğ‘‡ğœ“]âˆ’ğ‘ŒÌ‚ğ‘˜\n+\n[ğœ“ğ‘‡ğ‘ğ‘˜]âˆ’+ [ğœ“ğ‘‡ğœ“]+ğ‘ŒÌ‚ğ‘˜\n+ \n(10-2) \nMore details can be viewed in Algorithm 2.5, Appendix B, Supplementary Material. \nSimilarly, MBP is employed to further reduce the potential accumulative errors caused by the deep \nnonlinear method, [6, 7, 25]. Denote ğ‘ˆğ‘˜= ğ‘™ğ‘–ğ‘š\nitâ†’âˆğ‘ˆğ‘˜\nğ‘–ğ‘¡, ğ‘‹ğ‘˜= ğ‘™ğ‘–ğ‘š\nitâ†’âˆğ‘‹ğ‘˜\nğ‘–ğ‘¡, and ğ‘Œğ‘˜= ğ‘™ğ‘–ğ‘š\nitâ†’âˆğ‘Œğ‘˜\nğ‘–ğ‘¡, we have: \nğ¾âŸµ(âˆğ‘ˆğ‘˜\nğ‘€\nğ‘˜=1\n)\nğ‘‡\nâˆ™(ğ¼âˆ’âˆğ‘‹ğ‘˜\nğ‘€\nğ‘˜=1\nâˆ™ğ‘Œğ‘€) \n(11-1) \nğ‘ƒğ‘˜\nğ‘–ğ‘¡âŸµ(ğ‘ˆğ‘˜\nğ‘–ğ‘¡)\nğ‘‡ğ‘ˆğ‘˜\nğ‘–ğ‘¡âˆ™âˆmax (ğ‘ˆğ‘˜\nğ‘€âˆ’1\nğ‘˜=1\n) \n(11-2) \nIn Eqs. (11-1) and (11-2), there are two important variables, i.e., deep weight matrices and feature \nmatrices in the nonlinear method, updated by the following backpropagation techniques shown from \nEqs. (12-1) to (12-4) [6, 25]. The following equations employ ğ‘ğ‘˜\nğ‘–ğ‘¡, ğ¶ğ‘˜\nğ‘–ğ‘¡, ğ‘‘ğ‘˜\nğ‘–ğ‘¡, and ğ·ğ‘˜\nğ‘–ğ‘¡ to perform MBP \n[6, 7] using the derivative of the inverse activation function  \nğ‘‘ğ’©âˆ’1(ğ‘ )\nğ‘‘ğ‘ \n: \nğ‘ğ‘˜\nğ‘–ğ‘¡âŸµmax(ğ‘ˆğ‘˜âˆ’1) âˆ™ğ‘‘ğ’©âˆ’1(ğ‘ )\nğ‘‘ğ‘ \n, ğ‘ = ğ‘ˆğ‘˜\nğ‘–ğ‘¡ğ‘‰ğ‘˜\nğ‘–ğ‘¡ \n(12-1) \nğ‘‘ğ‘˜\nğ‘–ğ‘¡âŸµğ‘‘ğ’©âˆ’1(ğ‘ )\nğ‘‘ğ‘ \n, ğ‘ = ğ‘ˆğ‘˜\nğ‘–ğ‘¡ğ‘‰ğ‘˜\nğ‘–ğ‘¡ \n(12-2) \nğ¶ğ‘˜\nğ‘–ğ‘¡âŸµ(ğ‘ˆğ‘˜\nğ‘–ğ‘¡)ğ‘‡âˆ™(ğ‘ƒğ‘˜\nğ‘–ğ‘¡âˆ™ğ’©âˆ’1(ğ‘ ) âˆ’ğ¾)â¨€ğ‘ğ‘˜\nğ‘–ğ‘¡, s = ğ‘ˆğ‘˜\nğ‘–ğ‘¡ğ‘‰ğ‘˜\nğ‘–ğ‘¡ \n(12-3) \nğ·ğ‘˜\nğ‘–ğ‘¡âŸµ(ğ‘ˆğ‘˜âˆ’1\nğ‘–ğ‘¡)ğ‘‡âˆ™(ğ‘ˆğ‘˜âˆ’1\nğ‘–ğ‘¡\nâˆ™ğ’©âˆ’1(ğ‘ ) âˆ’ğ‘‰ğ‘˜âˆ’1\nğ‘–ğ‘¡)â¨€ğ‘‘ğ‘˜\nğ‘–ğ‘¡âˆ™(ğ‘‰ğ‘˜\nğ‘–ğ‘¡)ğ‘‡, ğ‘ = ğ‘ˆğ‘˜\nğ‘–ğ‘¡ğ‘‰ğ‘˜\nğ‘–ğ‘¡ \n(12-4) \nFinally, the following equations show the process of updating the weight matrix ğ‘ˆğ‘˜ and the feature \nmatrix ğ‘‰ğ‘˜ in the kth layer. In addition, ğ‘‡ is a constant value determined as 0.01 [6, 7, 25]. \nğ‘‰ğ‘˜\nğ‘–ğ‘¡+1 â†ğ‘‰ğ‘˜\nğ‘–ğ‘¡âˆ’ğ‘‡\n2ğ‘–ğ‘¡(ğ¶ğ‘˜\nğ‘–ğ‘¡) \n \n(13-1) \nğ‘ˆğ‘˜\nğ‘–ğ‘¡+1 â†ğ‘ˆğ‘˜\nğ‘–ğ‘¡âˆ’ğ‘‡\n2ğ‘–ğ‘¡(ğ·ğ‘˜\nğ‘–ğ‘¡) \n \n(13-2) \n \n \n \n2.4         Approximation and Convergence Rate of SENDER \nIn this section, we theoretically analyze the efficacy and discusses the performance of approximation \nand convergence rate of SENDER. Due to SENDER being organized as a composition of linear and \nnonlinear functions, the following theorem demonstrates that SENDER can approximate any real \nfunction that is almost-everywhere infinite [29] with high accuracy. The proof of Theorem 1.1 can \nbe viewed in Appendix A, Supplementary Material. \n \nTheorem 1.1 (Accurate Approximation of SENDER) Given a real function ğ‘“: â„ğ‘ƒÃ—ğ‘„â†’â„ğ‘ƒÃ—ğ‘„âˆª\n{Â±âˆ} and ğ‘š({ğ¼âˆˆâ„ğ‘ƒÃ—ğ‘„: ğ‘“(ğ¼) = Â±âˆ}) = 0 where ğ‘š(âˆ™) represents the Lebesgue measure [29]. \nGiven ğ¼âˆˆâ„ğ‘ƒÃ—ğ‘„, SENDER includes a linear method and a nonlinear method with multiple activation \nfunctions denoted as {ğ‘ƒğ‘˜(ğ¼)}ğ‘˜=1\nğ‘1  and   {ğ’©ğ‘˜(ğ¼)}ğ‘˜=1\nğ‘2 . If ğ‘ƒğ‘˜ denotes a series of matrix polynomials and \n7 \n \nğ’©ğ‘˜ denotes a smooth activation function, then we have âˆ€ ğœ€> 0 , ğ‘> 0 , ğ‘1 > ğ‘, ğ‘2 < âˆ, \nâ€–{ğ‘ƒğ‘˜(ğ¼)}ğ‘˜=1\nğ‘1 + {ğ’©ğ‘˜(ğ¼)}ğ‘˜=1\nğ‘2 âˆ’ğ‘“(ğ¼)â€– â‰¤ğœ€. \n \nTheorem 1.1 demonstrates that SENDER enables an accurate approximation to the original input \nğ‘“(ğ¼), even if it is almost-everywhere finite, e.g., ğ‘“(ğ¼) = Â±âˆ, ğ‘š(ğ¼) = 0. \nSince the fully-connected architecture is widely used in DNNs with various activation functions, the \noptimization function of the conventional neural network can be very complicated. Therefore, \nSENDER implements a non-fully connected architecture to reduce the complexity of network \nstructures and thus improve the efficiency of optimization since the global optimum of the \noptimization function can be found by a gradient optimizer. The details of this conclusion are proved \nas Theorem 1.2 in Appendix A, supplementary material.  \nTheorem 1.2 (Efficiency of Non-Fully Connected Architecture of SENDER) Given a series of \nnon-smoothed activation function {ğ‘“ğ‘–}ğ‘–=1\nğ‘, defined on [ğ‘, ğ‘] âŠ†â„1, assume {ğ‘“ğ‘–}ğ‘–=1\nğ‘\nğ‘–âŠ†ğ¿ğ‘–ğ‘1([ğ‘, ğ‘]\\\n{ğ‘ˆ(ğ‘¥ğ‘–, ğ›¿)}) ğ‘–âˆˆâ„•, and ğ‘ˆ(ğ‘¥ğ‘–, ğ›¿)  is an open cubic with the center ğ‘¥ğ‘– and radius ğ›¿> 0 . The \ncomposition of  ğ‘“ğ‘–, ğ‘“ğ‘—âˆˆ{ğ‘“ğ‘–}ğ‘–=1\nğ‘\n are denoted as ğ‘“ğ‘—,ğ‘–â‰ğ‘“ğ‘—(ğ‘“ğ‘–(ğ‘¥)); the various composition â„±â‰\nğ‘“â‹¯,ğ‘˜,â‹¯ğ‘—.ğ‘–âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]\\[ğ‘, ğ‘‘])  holds, when ğ‘˜â†’âˆ, [ğ‘, ğ‘‘] âŠ‡â‹ƒ\nğ‘ˆ(ğ‘¥ğ‘–, ğ›¿)\nğ‘˜\nğ‘–=1\n and ğ‘š([ğ‘, ğ‘‘]) â‰ 0 ; \nmoreover, given ğ‘¡â†’âˆ, the summation of âˆ‘\nâ„±ğ‘–\nğ‘¡\nğ‘–=1\n leads to âˆ‘\nâ„±ğ‘–\nğ‘¡\nğ‘–=1\nâˆ‰ğ¿ğ‘–ğ‘1([ğ‘, ğ‘]\\[ğ‘â€², ğ‘‘â€²]) , \n[ğ‘â€², ğ‘‘â€²] âŠ‡[ğ‘, ğ‘‘], and ğ‘š([ğ‘â€², ğ‘‘â€²]) â‰ 0. And ğ‘š(âˆ™) represents the Lebesgue measure. \nTheorem 1.2 demonstrates the infinite composition of activation function, such as fully connected \nand very DNN architecture, even with a single non-smooth point, which finally results in a non-\nsmooth interval as [ğ‘â€², ğ‘‘â€²]. Meanwhile, this theorem demonstrates that the non-fully connected \narchitectures can be more easily optimized, i.e., the global optimum of the non-fully connected \nmethod is easier to be searched by a gradient-based optimizer. \nThe following definition and theorems support that SENDER can maintain the convergence rate of \nthe original optimizer STORM. Moreover, we analyze the convergence rate of SENDER in the finite \nand infinite dimensional space, respectively. \nDefinition 1.1 (SENDER Operator) Denote the Random Initialization Operator as ğ’«: â„ğ‘ƒÃ—ğ‘„â†’\nâ„ğ‘ƒÃ—ğ‘„, the Sparse Operator as ğ’®: â„ğ‘ƒÃ—ğ‘„â†’â„ğ‘ƒÃ—ğ‘„ and the STORM operator as ğ’¯: â„ğ‘ƒÃ—ğ‘„â†’â„ğ‘ƒÃ—ğ‘„. \nTheir norms can be represented as \n1\nğ‘Ÿâ‰â€–ğ’«â€–, \n1\nğ‘ â‰â€–ğ’®â€–, and \n1\nğ‘¡â‰â€–ğ’¯â€–; in Theorems 2.1 to 2.4 in \nAppendix B, Supplementary Material, we have: 0 <\n1\nğ‘ < 1 and 0 <\n1\nğ‘Ÿ,\n1\nğ‘¡< âˆ, but 0 <\n1\nğ‘¡ğ‘˜< 1, ğ‘˜<\nâˆ.   \nTheorem 1.3 (Convergence Rate of SENDER in Finite Dimensionality Space) Denote STORM \nas an operator ğ’¯ in a finite dimensionality space. Due to the convergence of STORM with a rate of  \nğ’ª(ğ‘‡\n1\n2 + ğœ\n1\n3/ğ‘‡\n1\n3), the convergence of SENDER is the same as STORM. \nTheorem 1.3 shows that SENDER can converge as fast as STORM due to the prerequisite of finite \ndimensionality space. We further prove the convergence of SENDER in an infinite dimensionality \nspace in Collaroy 3.1. \nCollaroy 1.3 (Convergence of SENDER in Infinite Dimensionality Space) Given the infinite \ndimensionality space [30, 31], denote SENDER as an operator ğ’Ÿ: â„âˆÃ—âˆâ†’â„âˆÃ—âˆ. Denote ğ’Ÿ as an \ninfinite dimensional matrix operator with each element represented as ğ‘‘ğ‘–,ğ‘— and ğ‘Ÿğ‘–,ğ‘—âˆˆâ„âˆÃ—âˆ,  ğ‘–, ğ‘—â†’\nâˆ, respectively. ğ’Ÿ can converge to a fixed point, if and only if ğ‘‘ğ‘–,ğ‘—\nğ‘˜âˆ™ğ‘Ÿğ‘–,ğ‘— is ğ’ª(\n1\nğ‘›ğ‘) , ğ‘›âˆˆâ„•, ğ‘> 1, ğ‘âˆˆ\nâ„. \nTheorem 1.4 (Convergence of SENDER using Alternative Update) Given {â„±ğ‘–,ğ‘—,ğ‘˜,ğ‘¡}ğ‘–,ğ‘—,ğ‘˜,ğ‘¡=1\nâˆ\n,  \n{ğ’ ğ‘—,ğ‘˜,ğ‘¡}ğ‘—,ğ‘˜,ğ‘¡=1\nâˆ\n, {â„‹ ğ‘˜,ğ‘¡}ğ‘˜,ğ‘¡=1\nâˆ\n, and {ğ’¦ ğ‘¡}ğ‘¡=1\nâˆ, are series of continuous operators [30] applied on a finite \ndimensionality space, we have: â„±ğ‘–,ğ‘—,ğ‘˜,ğ‘¡, ğ’ ğ‘—,ğ‘˜,ğ‘¡, â„‹ ğ‘˜,ğ‘¡, ğ’¦ ğ‘¡: â„ğ‘ƒÃ—ğ‘„â†’â„ğ‘ƒÃ—ğ‘„. If lğ‘–ğ‘š\nğ‘–â†’âˆâ„±ğ‘–,ğ‘—â†’ğ’ğ‘—,ğ‘˜,ğ‘¡, \nğ‘™ğ‘–ğ‘š\nğ‘—â†’âˆğ’ğ‘—,ğ‘˜,ğ‘¡â†’â„‹ ğ‘˜,ğ‘¡, ğ‘™ğ‘–ğ‘š\nğ‘˜â†’âˆâ„‹ğ‘˜,ğ‘¡â†’ğ’¦ ğ‘¡, and lğ‘–ğ‘š\nğ‘¡â†’âˆğ’¦ğ‘¡â†’ğ’¢, then, ğ‘™ğ‘–ğ‘š\nğ‘›â†’âˆâ„±ğ‘–ğ‘›,ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›â†’ğ’¢ holds. \nTheorem 4.1 demonstrates that a computational model with multiple variables satisfying Corollary \n1.2 can converge to a fixed point via an alternative strategy. The proof of Theorem 4.1 can be viewed \n8 \n \nin Appendix A, Supplementary Material. Moreover, in Theorem 4.1, due to the convexity of the \nAugmented Lagrange function [32], each independent approximation, such as ğ‘™ğ‘–ğ‘š\nğ‘›â†’âˆâ„±ğ‘–ğ‘›,ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›â†’ğ’¢ \ncan converge to the operator.  \n \n3 \nResults \n3.1      Comparison of Identified Canonical, Meta, and Sub-FCs via SENDER and Other Peer \nFour Methods \nTo validate SENDER, we employ the resting-state fMRI signals from all healthy individuals in \nConsortium for Neuropsychiatric Phenomics (CNP) (https://openfmri.org/dataset/ds000030/). To \nreduce the heterogeneous influence caused by parameter tuning, all hyperparameters are tuned the \nsame as SENDER's hyperparameter estimations. The estimated number of layers for SENDER is \ntwo. The sizes of the first and second layers are 40 and 10, respectively. In addition, other peer \nalgorithmsâ€™ parameters are tuned following in [5-8]. The activation function of all layers of SENDER \nand DBN is set as ReLU. Furthermore, all abbreviations of templates can be found in Table S1, \nAppendix A, Supplementary Material. \nFigure 1 and 2 present the reconstruction of the canonical FCs via SENDER and other peer methods, \ne.g., DICA, DNMF, SDDL, and DBN; the identification results via the linear method in SENDER \nand other four peer algorithms are compared with canonical templates [19]. In short, this \nexperimental validation demonstrates that the reconstruction of canonical FCs via SENDER is not \nsignificantly different from the canonical templates. \n \nFigure 1. This figure presents three representative slices of reconstructed six canonical FCs via \nSENDER and six other peer methods.  \nFurthermore, the quantitative comparison of canonical FCs is provided in Figure 5 (b), where it \ncompares the similarity of reconstructed canonical FCs derived via the linear method in SENDER \nand other four peer methods and publicly released canonical templates [19]. The similarity is \nmeasured by Hausdorff Distance [34]. In general, the similarity of identified canonical FCs via \nSENDER and canonical templates is higher/comparable to other peer algorithms.    \nMoreover, Figure 2 shows the reconstruction of meta-FCs derived by the nonlinear method in \nSENDER and the other four peer methods. Overall, the nonlinear methods in SENDER can \nsuccessfully reconstruct meta-FCs with a higher similarity calculated with the meta-templates [41, \n42]. \n \n9 \n \n \nFigure 2. This figure presents the representative slices of reconstructed another six canonical FCs \nvia SENDER and the other six peer methods. \nIn Figure 3, according to qualitative observation, the nonlinear method in SENDER and DBN can \nreconstruct meta-FCs more similarly to the templates; meanwhile, DICA and DNMF only \nreconstruct five and three FCs with higher similarity to the original templates, respectively. \nFurthermore, for SDDL, the reconstruction of FCs in the first and second columns is similar to the \noriginal templates.  \n \nFigure 3. All qualitative comparisons revealed five meta-FCs at the first or second layer via the \nnonlinear method in SENDER and the other four peer methods. Please notice that the meta-FCs can \nbe identified at the first and second layers using SENDER; nevertheless, the other four peer methods \nneed to extract meta-FCs in the deeper layers. \nIn detail, for instance, SENDER can reveal meta-FCs using the nonlinear method with a strong spatial \noverlap with reported templates [42]. Although other peer algorithms can detect five meta-FCs at \ntheir second layer, some reported FCs are different from templates. Specifically, DICA failed to \ndetect the activated occipital lobe compared with templates presented in the first column, in Figure \n10 \n \n3; in addition, there are some differences between extracted meta-FCs and templates, e.g., FCs in the \nfirst and second column; in detail, the template FCs in first column contains the occipital lobe that \nare disrupted in most meta-FCs identified via deep linear learning method; furthermore, the FCs \nidentified by SDDL in third and fourth column presents a disruption of areas in activation areas of \ncanonical Executive Control Network; similarly, DBN can only perfectly reveal the meta-FCs in the \nlast column.  \nMoreover, the quantitative results are included in Figure 5(c) to compare the similarity between \nidentified meta-FCs with corresponding templates. We further provide theoretical explanations of \nwhy SENDER can provide more FCs/spatial features than DICA in the proof in Theorem 2.2, \nAppendix B, Supplementary Material.    \nNote that another contribution of SENDER is to provide a variable including the potentially \ncorresponding sub-FCs, while other peer algorithms cannot build the relations between the variables \nin method and sub-FCs using the same hyperparameters. For instance, in the third column of Figure \n4, a precuneus, a functional core of Default Mode Network (DMN), is detected. However, these \nminor/sub-FCs could be more sensitive to some brain diseases from a clinical translational \nperspective that can guide the personalized diagnosis and treatment [35, 36]. Moreover, as discussed \nbefore, these sub-FCs are designed clearly as variables {ğ‘†ğ‘–}ğ‘–=1\nğ‘˜\n of SENDER introduced in Eq. (1) \nrather than randomly extracted features [5]. \n \n \nFigure 4. The qualitative comparison of sub-FCs identified by  SENDER. These sub-FCs are derived \nvia shallow and deeper layers of the background component matrix of SENDER. \nIn short, Figure 5 (a) indicates the potential organizations of canonical, meta, and sub-FCs. To \nvalidate the reconstruction performance of SENDER and the other three peer methods, we calculate \nthe similarity of all FCs extracted by SENDER and the other four peer methods; SENDER shows an \noverall higher similarity than the four peer algorithms.  \nMoreover, since there has not been a rigorous 'ground-truth' to quantitatively validate the \nperformance of SENDER and the other four peer methods, especially for meta-FCs, we alternatively \ninvestigate the identifiability [40] of canonical and meta-FCs. Calculating the identifiability can \nfurther validate the consistency and reproducibility of SENDER and other four peer methods in a \ndata-driven fashion. At first, we randomly separate the original input data into two independent sets \nas ğ¹ğ¶ğ‘¡ğ‘’ğ‘ ğ‘¡ and ğ¹ğ¶ğ‘Ÿğ‘’ğ‘¡ğ‘’ğ‘ ğ‘¡ shown in Eq. (9-1), and calculate the identifiability using Eq (9-2). The \nquantitative identifiability of meta-FCs is shown in Figures 6 (a) and (b).  As discussed, the following \nequations detail the procedure of calculating identifiability: \nğ‘“ğ‘ğ‘–âˆˆğ¹ğ¶ğ‘¡ğ‘’ğ‘ ğ‘¡, ğ‘“ğ‘ğ‘—âˆˆğ¹ğ¶ğ‘Ÿğ‘’ğ‘¡ğ‘’ğ‘ ğ‘¡, ğ¹ğ¶ğ‘¡ğ‘’ğ‘ ğ‘¡âˆ©ğ¹ğ¶ğ‘Ÿğ‘’ğ‘¡ğ‘’ğ‘ ğ‘¡= âˆ… \n(9-1) \nğ‘–ğ‘‘ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘“ğ‘–ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦â†\nâˆ‘âˆ‘ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘“ğ‘ğ‘–, ğ‘“ğ‘ğ‘—)\nğ‘—\nğ‘–\n|ğ¹ğ¶ğ‘¡ğ‘’ğ‘ ğ‘¡| Ã— |ğ¹ğ¶ğ‘Ÿğ‘’ğ‘¡ğ‘’ğ‘ ğ‘¡| \n(9-2) \n11 \n \nIn Eqs. (9-1) and (9-2), ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘“ğ‘ğ‘–, ğ‘“ğ‘ğ‘—) represents the calculation of the corresponding components \nidentified from test and retest data using Pearson Correlation [40]. In this work, the calculation of \ncorrelation is replaced by Intraclass Correlation Coefficient (ICC). And |ğ¹ğ¶ğ‘¡ğ‘’ğ‘ ğ‘¡| , |ğ¹ğ¶ğ‘Ÿğ‘’ğ‘¡ğ‘’ğ‘ ğ‘¡| \nrepresent the number of components in the test and retest datasets, respectively. \n \nFigure 5. (a) An example of hierarchical structure on FCs presents the hierarchical organizations \nidentified via linear/nonlinear methods and background matrix in SENDER. The bottom slices \nrepresent the sub-FCs which include partial/minor functional regions of canonical FCs. The middle \nslices show the identified canonical FCs and the top slices provide the meta FCs extracted via \nSENDER. The dashed line indicates the high-order FCs do not entirely include lower-order FCs and \nother regions are involved. (b) and (c) provide the similarity of canonical and meta-FCs derived by \nSENDER with the templates, shown in the first row in Figures 1, 2, and 3, respectively. \n \nFigure 6. (a) and (b) show the identifiability comparison of identified canonical and meta-FCs via \nSENDER and the other three peer methods based on all subjects' resting-state fMRI signals from \nhealthy individuals in CNP (https://openfmri.org/dataset/ds000030/). And (c) provides the \n12 \n \ncomparison of reconstruction accuracy, i.e., training loss, of SENDER and the other four peer \nmethods in the second layer. \nIn Figures 6 (a) and (b), the quantitative results of identifiability demonstrate that the identified \ncanonical and meta-FCs via linear/nonlinear methods in SENDER provide higher identifiable over \nthe other four peer methods [40]. In addition, in Figure 6 (c), the significant difference in \nreconstruction accuracy can be easily observed since the purple box plot shows the highest accuracy \nof training loss at the second layer. \n \n4 \nConclusion \nIn this work, the proposed SENDER adopts STORM optimizer, the alternative optimization strategy, \nand RRO, for data-driven determination of all hyperparameters, to reveal the canonical, meta, and \nsub-FCs. Furthermore, the hybrid modeling and efficient non-fully connected architectures of \nSENDER enable the discovery of canonical, meta, and sub-FCs with the highest identifiability to \nthe other four peer methods. Moreover, the results show that the meta-FCs can even be detected in \nshallow layers. Finally, the theoretical studies and experimental validation further indicate an \naccurate approximation to original input and high convergence rate of SENDER, which are \ncomparable to or even better than other peer algorithms, such as DNNs. \nMoreover, SENDER can potentially synergize research of neurodevelopmental, neurodegenerative, \nand psychiatric disorders since these revealed novel canonical, meta, and sub-FCs that can be \ngenerated as the clinical biomarkers benefitting personalized diagnosis, prognosis and treatment \nmonitoring [35-39]. \nOverall, we believe that SENDER can play a role as an inspiring deep hybrid learning method for a \nfruitful future with a profound influence on facilitating the research of deep learning methods, \ncomputational neuroscience, and clinical translational application. \n \nReferences \n[1] \nBullmore, E., Sporns, O. Complex brain networks: graph theoretical analysis of structural and \nfunctional systems. Nature Reviews Neuroscience, 2009; 10:186-198. \n[2] \nPower, J. D., Cohen, A. L., Nelson, S. M., Wig, G. S., Barnes, K. A., Church, J. A., â€¦ & Petersen, \nS. E. Functional network organization of the human brain. Neuron, 2011; 72:65-678. \n[3] \nStam, C. J. Modern network science of neurological disorders. Nature Reviews Neuroscience, 2014; \n15:683. \n[4] \nIraji, A., Fu, Z., Damaraju, E., et al. (2019). Spatial dynamics within and between brain functional \ndomains: A hierarchical approach to study time-varying brain function. Hum Brain Mapp, 40:1969-1986. \n[5] \nWylie, K. P., Kronberg, E., Legget, K. T., Sutton, B., & Tregellas, J. R. (2021). Stable Meta-\nNetworks, Noise, and Artifacts in the Human Connectome: Low-to High-Dimensional Independent \nComponents Analysis as a Hierarchy of Intrinsic Connectivity Networks. Frontiers in Neuroscience, 15. \n[6] \nQiao, C., Yang, L., Calhoun, V. D., Xu, Z. B., & Wang, Y. P. (2021). Sparse deep dictionary learning \nidentifies differences of time-varying functional connectivity in brain neuro-developmental study. Neural \nNetworks, 135, 91-104. \n[7] \nTrigeorgis, G., Bousmalis, K., Zafeiriou, S., & Schuller, B. W. (2016). A deep matrix factorization \nmethod for learning attribute representations. IEEE Transactions on Pattern Analysis and Machine Intelligence, \n39:417-429. \n[8] \nTrigeorgis, G., Bousmalis, K., Zafeiriou, S., & Schuller, B. (2014, June). A deep semi-nmf model for \nlearning hidden representations. In International Conference on Machine Learning (pp. 1692-1700). PMLR. \n[9] \nZhang, W., Palacios, E. M., & Mukherjee, P. Deep Linear Modeling of Hierarchical Functional \nConnectivity in the Human Brain. BioRxiv., 2020. doi:10.1101/2020.12.13.422538. \n13 \n \n[10] \nZhang, W., Zhao, S., Hu, X., Dong, Q., Huang, H., Zhang, S., ... & Liu, T. (2020). Hierarchical \nOrganization of Functional Brain Networks Revealed by Hybrid Spatiotemporal Deep Learning. Brain \nConnectivity, 10:72-82. \n[11] \nZhang, W., Zhao, L., Li, Q., Zhao, S., Dong, Q., Jiang, X., ... & Liu, T. (2019, October). Identify \nhierarchical structures from task-based fMRI data via hybrid spatiotemporal neural architecture search net. \nIn International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 745-753). \nSpringer, Cham. \n[12] \nBengio, Y., Courville, A.C., Vincent, P. (2012). Unsupervised feature learning and deep learning: A \nreview and new perspectives. CoRR, abs/1206.5538, 1. \n[13] \nHannun, A. Y., Rajpurkar, P., Haghpanahi, M., Tison, G. H., Bourn, C., Turakhia, M. P., & Ng, A. \nY. (2019). Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a \ndeep neural network. Nature Medicine, 25(1), 65. \n[14] \nLeCun, Y., Bengio, Y., Hinton, G.E. (2015). Deep learning. Nature, 521:436-444. \n[15] \nEsteva, A., Robicquet, A., Ramsundar, B., Kuleshov, V., DePristo, M., Chou, K., Cui, C., Corrado, \nG., Thrun, S., & Dean, J. (2019). A guide to deep learning in healthcare. Nature Medicine, 25:24-29. \n[16] \nGurovich, Y., Hanani, Y., Bar, O., Nadav, G., Fleischer, N., Gelbman, D., ... & Bird, L. M. (2019). \nIdentifying facial phenotypes of genetic disorders using deep learning. Nature Medicine, 25:60. \n[17] \nHu, X., Huang, H., Peng, B., Han, J., Liu, N., Lv, J., ... & Liu, T. (2018). Latent source mining in \nFMRI via restricted Boltzmann machine. Human Brain Mapping, 39:2368-2380. \n[18] \nHuang, H., Hu, X., Zhao, Y., Makkie, M., Dong, Q., Zhao, S., ... & Liu, T. (2018). Modeling task \nfMRI data via deep convolutional autoencoder. IEEE Transactions on Medical Imaging, 37(7). \n[19] \nSmith, S. M., Fox, P. T., Miller, K. L., Glahn, D. C., Fox, P. M., Mackay, C. E., ... & Beckmann, C. \nF. (2009). Correspondence of the brain's functional architecture during activation and rest. Proceedings of the \nNational Academy of Sciences, 106:13040-13045. \n[20] \nSchmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural networks, 61:85-\n117. \n[21] \nHinton, G.E., Salakhutdinov, R.R. (2006). Reducing the dimensionality of data with neural networks. \nScience, 313:504-507 \n[22] \nHinton, G.E., Osindero, S., Teh, Y.-W. (2006). A fast learning algorithm for deep belief nets. Neural \nComputation, 18:1527-1554. \n[23] \nHinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-r., Jaitly, N., Senior, A., Vanhoucke, V., \nNguyen, P., Sainath, T.N. (2012). Deep neural networks for acoustic modeling in speech recognition: The \nshared views of four research groups. Signal Processing Magazine, IEEE, 29:82-97. \n[24] \nWen, Z., Yin, W., & Zhang, Y. (2012). Solving a low-rank factorization model for matrix completion \nby a nonlinear successive over-relaxation algorithm. Mathematical Programming Computation, 4:333-361. \n[25] \nShen, Y., Wen, Z., & Zhang, Y. (2014). Augmented Lagrangian alternating direction method for \nmatrix separation based on low-rank factorization. Optimization Methods and Software, 29:239-263. \n[26] \nDing, C. H., Li, T., & Jordan, M. I. (2008). Convex and semi-nonnegative matrix factorizations. IEEE \ntransactions on pattern analysis and machine intelligence, 32(1), 45-55. \n[27] \nCutkosky, A., & Orabona, F. (2019). Momentum-based variance reduction in non-convex \nsgd. Advances in neural information processing systems, 32. \n[28] \nLiu, J., Yuan, L., & Ye, J. (2010, July). An efficient algorithm for a class of fused lasso problems. \nIn Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data \nmining (pp. 323-332). \n[29] \nBeck, A., & Teboulle, M. (2009). A fast iterative shrinkage-thresholding algorithm for linear inverse \nproblems. SIAM journal on imaging sciences, 2(1), 183-202. \n[30] \nRoyden, H. L. (1968). Real Analysis. Krishna Prakashan Media. \n[31] \nRudin, W. (1973). Functional Analysis. \n[32] \nKadison, R. V., & Ringrose, J. R. (1997). Fundamentals of the theory of operator algebras (Vol. 2). \nAmerican Mathematical Soc.. \n14 \n \n[33] \nBoyd, S., Boyd, S. P., & Vandenberghe, L. (2004). Convex Optimization. Cambridge university press. \n[34] \nZhang, W., Lv, J., Li, X., Zhu, D., Jiang, X., Zhang, S., ... & Liu, T. (2019). Experimental \nComparisons of Sparse Dictionary Learning and Independent Component Analysis for Brain Network Inference \nfrom fMRI Data, IEEE Transactions on Biomedical Engineering, 66:289-299. \n[35] \nVossel, S., Geng, J. J., Fink, G. R. (2014). Dorsal and ventral attention systems: distinct neural circuits \nbut collaborative roles. Neuroscientist, 20:150-159. \n[36] \nChand, G. B., Wu, J., Hajjar, I., Qiu, D. (2017). Interactions of the Salience Network and Its \nSubsystems with the Default-Mode and the Central-Executive Networks in Normal Aging and Mild Cognitive \nImpairment. Brain Connect, 7:401-412. \n[37] \nCohen, A. D., Yang, B., Fernandez, B., Banerjee, S., Wang, Y. (2020). Improved resting state \nfunctional connectivity sensitivity and reproducibility using a multiband multi-echo acquisition. Neuroimage, \n225:117461. \n[38] \nPlis, S. M., Hjelm, D. R., Salakhutdinov, R., Allen, E. A., Bockholt, H. J., Long, J. D., ... & Calhoun, \nV. D. (2014). Deep learning for neuroimaging: a validation study. Frontiers in Neuroscience, 8:229. \n[39] \nSuk, H.-I., Wee, C.-Y., Lee, S.-W., Shen, D. (2016). State-space model with deep learning for \nfunctional dynamics estimation in resting-state fMRI. Neuroimage, 129:292-307. \n[40] \nVan De Ville, D., Farouj, Y., Preti, M. G., LiÃ©geois, R., & Amico, E. (2021). When makes you unique: \nTemporality of the human brain fingerprint. Science advances, 7(42), eabj0751. \n[41] \nZhang, W., Wang, Y., Cohen, A. D., McCrea, M. A., & Mukherjee, P. Deep Linear Modeling of \nMultiBand MultiEcho fMRI Reveals Reproducible Hierarchical Functional Connectivity Networks. \nOrganization for Human Brain Mapping (OHBM), 2020. \n[42] \n Zhang, W.,  Cai, Lanya T., Wren-Jarvis, J., Lazerwitz, M., Bourla, I., & Mukherjee, P. Deep \nNonlinear Modeling Extracts Reproducible Hierarchical Functional Networks from BOLD fMRI. Organization \nfor Human Brain Mapping (OHBM), 2021. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n15 \n \nAppendix A \n \nThe matrix polynomials are defined as: âˆ€ğ‘˜âˆˆâ„• ğ‘ƒ2ğ‘˜(ğ‘‹) = (ğ‘‹ğ‘‹ğ‘‡)ğ‘˜, ğ‘ƒ2ğ‘˜+1(ğ‘‹) = (ğ‘‹ğ‘‹ğ‘‡)ğ‘˜ğ‘‹, ğ‘‹âˆˆ\nâ„ğ‘†Ã—ğ‘‡; and {ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\nğ‘\n defines a series of matrix polynomials, for example: {ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\n3\n=\n{ğ‘‹, ğ‘‹ğ‘‹ğ‘‡, ğ‘‹ğ‘‹ğ‘‡ğ‘‹}, ğ‘‹âˆˆâ„ğ‘†Ã—ğ‘‡; \nMoreover, it is easy to prove {ğ‘ƒğ‘›}ğ‘›=1\nâˆ\n denoted on â„ğ‘†Ã—ğ‘‡ as a ring ({ğ‘ƒğ‘›}ğ‘›=1\nâˆ, +,Ã—), and it also \ndemonstrates {ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\nğ‘\nâˆˆ({ğ‘ƒğ‘›}ğ‘›=1\nâˆ, +,Ã—) , e.g., ({ğ‘ƒğ‘›}ğ‘›=1\nâˆ, +,Ã—) âŠ‡âˆ‘\n{ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\nğ‘\nğ‘€\nğ‘–=1\n. (Dummit, \n2004; Kadison, 1997). Then we introduce the theorem 1.1 to describe the superiority of DEMAND. \n \nTheorem 1.1 (Accurate Approximation of SENDER) Given a real function ğ‘“: â„ğ‘ƒÃ—ğ‘„â†’â„ğ‘ƒÃ—ğ‘„âˆª\n{Â±âˆ} and ğ‘š({ğ¼âˆˆâ„ğ‘€Ã—ğ‘: ğ‘“(ğ¼) = Â±âˆ}) = 0 where m(âˆ™) represents the Lebesgue measure [29]. \nGiven ğ¼âˆˆâ„ğ‘€Ã—ğ‘, SENDER includes a linear model and nonlinear model with multiple activation \nfunctions can be denoted as {ğ‘ƒğ‘˜(ğ¼)}ğ‘˜=1\nğ‘1  and   {ğ’©ğ‘˜(ğ¼)}ğ‘˜=1\nğ‘2  we have: if P denotes a series of matrix \npolynomials, such as, and ğ’© denotes a smooth activation function, andâˆ€ ğœ€> 0, ğ‘> 0, ğ‘1 > ğ‘, \nğ‘2 < âˆ, â€–{ğ‘ƒğ‘˜(ğ¼)}ğ‘˜=1\nğ‘1 + {ğ’©ğ‘˜(ğ¼)}ğ‘˜=1\nğ‘2 âˆ’ğ‘“(ğ¼)â€– â‰¤ğœ€. \nProof: According to Ğ›ÑƒĞ·Ğ¸Ğ½ (Luzin) Theorem in [29], we have a close set: \nğ¹ğ‘›âŠ‚ğ¹ğ‘›+1 âŠ‚â‹¯âŠ†â„ğ‘€Ã—ğ‘ \nğ‘š(â„ğ‘€Ã—ğ‘\\ğ¹ğ‘˜) = 1\nğ‘˜, ğ‘˜âˆˆâ„• \nğ‘“âˆˆğ¶(ğ¹ğ‘˜) \n \n(A.1) \nThen we have a consistent real function ğ‘”(ğ‘‹), and obviously we have: \nğ‘”(ğ‘‹) = ğ‘“(ğ‘‹) \n(A.2) \nSince for any continuous real function, we have: \n|ğ‘”(ğ‘‹) âˆ’ğ‘ƒğ‘˜(ğ‘‹)| < 1\nğ‘˜ \n(A.3) \nLet â„±= â‹ƒ\nğ¹ğ‘˜\nâˆ\nğ‘˜=1\n, and obviously we have: \nğ‘š(â„ğ‘€Ã—ğ‘\\ğ¹ğ‘˜) = ğ‘š(â„ğ‘€Ã—ğ‘\\â‹ƒğ‘˜=1\nâˆğ¹ğ‘˜) = â‹‚ğ‘˜=1\nâˆğ‘š(â„ğ‘†Ã—ğ‘‡\\ğ¹ğ‘˜) = â‹‚ğ‘˜=1\nâˆ\n1\nğ‘˜= 0 \n(A.4) \nMoreover, it is easy to prove {ğ‘ƒğ‘›}ğ‘›=1\nâˆ\n denoted on â„ğ‘†Ã—ğ‘‡ as a ring ({ğ‘ƒğ‘›}ğ‘›=1\nâˆ, +,Ã—), and it also \ndemonstrates {ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\nğ‘\nâŠ†({ğ‘ƒğ‘›}ğ‘›=1\nâˆ, +,Ã—), e.g., ğ‘ƒğ‘›(ğ‘‹) â‰âˆ\nğ‘¥ğ‘–\nğ‘\nğ‘–=1\n+ âˆ‘\nğ‘¦ğ‘—\nğ‘\nğ‘—=1\n.  \n \nIf ğ”‰ is a real function denoted on set â„±, it indicates:  \nlim\nğ‘â†’âˆ|ğ”‰âˆ’{ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\nğ‘\n| = 0 \n(A.5) \nthen we have lim\nğ‘â†’âˆ{ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\nğ‘\n= ğ”‰, meanwhile, if N is large enough, |ğ”‰âˆ’{ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\nğ‘\n| < ğœ€ holds. \nMoreover, if {ğ’©ğ‘›(ğ‘‹)}ğ‘›=1\nğ‘\nâˆˆğ¶(â„ğ‘†Ã—ğ‘‡), according to Theorem, |ğ’©ğ‘›(ğ‘‹) âˆ’{ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\nâˆ| â†’0;  \nwe have: \n|ğ”‰âˆ’{ğ’©ğ‘›(ğ‘‹)}ğ‘›=1\nğ‘\n| = |ğ”‰âˆ’{ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\nâˆ| < ğœ€\n2 \n(A.6) \nThus, considering âˆ€ ğœ€> 0, ğ‘> 0, ğ‘1 > ğ‘, ğ‘2 < âˆ, rewrite Eq. (6) as: \n|ğ”‰âˆ’{ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\nğ‘1 | < ğœ€\n2 \n(A.7-1) \n|ğ”‰âˆ’{ğ’©ğ‘›(ğ‘‹)}ğ‘›=1\nğ‘2 | < ğœ€\n2 \n(A.7-2) \nThen, obviously, we have Eq. (A.8) hold as below: \n16 \n \n|ğ”‰âˆ’{ğ‘ƒğ‘›(ğ‘‹)}ğ‘›=1\nğ‘1 âˆ’{ğ’©ğ‘›(ğ‘‹)}ğ‘›=1\nğ‘2 | < ğœ€ \n(A.8) \n \nDefinition 1.2 (Variance Bounded Real Function) Given a real function ğ‘“ denoted on [ğ‘, ğ‘], and \nâˆ†: ğ‘= ğ‘¥0 < ğ‘¥1 < ğ‘¥2 < â‹¯< ğ‘¥ğ‘›= ğ‘. A sum as  ğ‘£âˆ†= âˆ‘\n|ğ‘“(ğ‘¥ğ‘–) âˆ’ğ‘“(ğ‘¥ğ‘–âˆ’1)|\nğ‘›\nğ‘–=1\n and ğ‘‰ğ‘\nğ‘(ğ‘“) =\nsup { ğ‘£âˆ†: âˆ€âˆ†}. The variance bounded real function is denoted as ğ‘‰ğ‘\nğ‘(ğ‘“) < âˆ.  \n \nDefinition 1.3 (Amplitude of Real Function) Given a real function ğ‘“ denoted on [ğ‘, ğ‘], and \nâˆ€ğµ(ğ‘¥0, ğ›¿) âŠ†[ğ‘, ğ‘], ğ›¿> 0; ğœ”ğ‘“(ğ‘¥0) = lim\nğ›¿â†’0 sup {|ğ‘“(ğ‘¥â€²) âˆ’ğ‘“(ğ‘¥\")|: ğ‘¥â€², ğ‘¥\" âˆˆğµ(ğ‘¥0, ğ›¿)}. \n \nLemma 1.1 (Smooth & Variance Bounded Real Function) If and only if a real function ğ‘“âˆˆ\nğ¿ğ‘–ğ‘1([ğ‘, ğ‘]),  ğ‘‰ğ‘\nğ‘¥(ğ‘“) < âˆ holds.  \nProof:  If ğ‘“âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]), it indicates: âˆ€ğ‘¥1, ğ‘¥2 âˆˆ[ğ‘, ğ‘], we have: |ğ‘“(ğ‘¥1) âˆ’ğ‘“(ğ‘¥2)| â‰¤ğ¿|ğ‘¥1 âˆ’ ğ‘¥2|.  \nMoreover, according to Definition 1, we have: \n ğ‘£âˆ†= âˆ‘|ğ‘“(ğ‘¥ğ‘–) âˆ’ğ‘“(ğ‘¥ğ‘–âˆ’1)|\nğ‘›\nğ‘–=1\nâ‰¤ğ¿(|ğ‘¥0 âˆ’ ğ‘¥1| + |ğ‘¥1 âˆ’ ğ‘¥2| + â‹¯+ |ğ‘¥ğ‘›âˆ’ ğ‘¥ğ‘›âˆ’1|)\nâ‰¤ğ¿(ğ‘âˆ’ğ‘) < âˆ \n \n(A.9) \n \nIf ğ‘‰ğ‘\nğ‘¥(ğ‘“) < âˆ holds, it demonstrates: ğ‘ ğ‘¢ğ‘ { ğ‘£âˆ†: âˆ€âˆ†} < âˆ, let ğ‘¥ğ‘› be ğ‘¥, we have: \nâˆ‘|ğ‘“(ğ‘¥ğ‘–) âˆ’ğ‘“(ğ‘¥ğ‘–âˆ’1)|\nğ‘›\nğ‘–=1\nâ‰¤ğ‘ ğ‘¢ğ‘ { ğ‘£âˆ†: âˆ€âˆ†} < âˆ \n \n(A.10) \n \nFurthermore, if ğ‘›â†’âˆ, and âˆ‘\n|ğ‘“(ğ‘¥ğ‘–) âˆ’ğ‘“(ğ‘¥ğ‘–âˆ’1)| < âˆ\nğ‘›\nğ‘–=1\n holds, obviously, we must have: \n|ğ‘“(ğ‘¥ğ‘–) âˆ’ğ‘“(ğ‘¥ğ‘–âˆ’1)| â†’0 \n(A.11) \nIt should satisfy:  \nğ‘¥ğ‘–, ğ‘¥ğ‘–âˆ’1 âˆˆğµ(ğ‘¥, ğœ€), âˆ€ğœ€> 0 |ğ‘“(ğ‘¥ğ‘–) âˆ’ğ‘“(ğ‘¥ğ‘–âˆ’1)| â‰¤ğ¿|ğ‘¥ğ‘–âˆ’ ğ‘¥ğ‘–âˆ’1| \n(A.12) \nSince âˆ€ğ‘¥ğ‘–, ğ‘¥ğ‘–âˆ’1 âˆˆ[ğ‘, ğ‘], obviously, we have:  \nğ‘“âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]) \n \nLemma 1.2 (Amplitude & Variance Bounded Real Function) ğœ”ğ‘“(ğ‘¥0) = lim\nğ›¿â†’0 sup {|ğ‘“(ğ‘¥â€²) âˆ’\nğ‘“(ğ‘¥â€²â€²)|: ğ‘¥â€², ğ‘¥\" âˆˆğµ(ğ‘¥0, ğ›¿) âŠ†[ğ‘, ğ‘]} < ğœ€ is equivalent to ğ‘“âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]). \nProof: If ğ‘“âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]) holds, similarly, if ğ‘›â†’âˆ, âˆ€{ğ‘¥ğ‘–}ğ‘–=1\nğ‘›, we have \n|ğ‘“(ğ‘¥ğ‘–) âˆ’ğ‘“(ğ‘¥ğ‘–âˆ’1)| â†’0 \n(A.13) \nReplace ğ‘¥ğ‘– and ğ‘¥ğ‘–âˆ’1 by ğ‘¥â€², ğ‘¥â€²â€², respectively, it satisfies: \nğœ”ğ‘“(ğ‘¥0) = lim\nğ›¿â†’0 sup{|ğ‘“(ğ‘¥â€²) âˆ’ğ‘“(ğ‘¥â€²â€²)|: ğ‘¥â€², ğ‘¥â€²â€² âˆˆğµ(ğ‘¥0, ğ›¿) âŠ†[ğ‘, ğ‘]} < ğ‘€ \n(A.14) \nIf ğœ”ğ‘“(ğ‘¥0) = lim\nğ›¿â†’0 sup {|ğ‘“(ğ‘¥â€²) âˆ’ğ‘“(ğ‘¥â€²â€²)|: ğ‘¥â€², ğ‘¥â€²â€² âˆˆğµ(ğ‘¥0, ğ›¿) âŠ†[ğ‘, ğ‘]} < âˆ, we assume: \nğœ”ğ‘“(ğ‘¥0) = lim\nğ›¿â†’0 sup {|ğ‘“(ğ‘¥â€²) âˆ’ğ‘“(ğ‘¥â€²â€²)|: ğ‘¥â€², ğ‘¥\" âˆˆğµ(ğ‘¥0, ğ›¿) âŠ†[ğ‘, ğ‘]} < ğœ€ \n(A.15) \nObviously, given ğ‘¥â€², ğ‘¥â€²â€² âˆˆğµ(ğ‘¥0, ğ›¿) âŠ†[ğ‘, ğ‘], we have: \nâˆ€ğœ€> 0 |ğ‘“(ğ‘¥â€²) âˆ’ğ‘“(ğ‘¥â€²â€²)| < ğœ€ \n(A.16) \nWhen ğ›¿â†’0, let |ğ‘¥â€² âˆ’ ğ‘¥â€²â€²| =\nğœ€\nğ¿,  it also indicates:  \n|ğ‘“(ğ‘¥â€²) âˆ’ğ‘“(ğ‘¥â€²â€²)| < ğ¿|ğ‘¥â€² âˆ’ ğ‘¥â€²â€²| \n(A.17) \n \nLemma 1.3 (Cantor Theorem) Given {ğµğ‘–}ğ‘–=1\nâˆ\n are closed sets and âˆ€ğµğ‘–â‰ âˆ…, if ğµ1 âŠ‡ğµ2 âŠ‡\nâ‹¯âŠ‡ğµğ‘˜âŠ‡â‹¯, âˆ©ğ‘–=1\nâˆ\nğµğ‘–â‰ âˆ…. \n \nLemma 1.4 (Vitali Covering Lemma) Given {ğµğ‘–}ğ‘–=1\nğ‘› are closed sets and âˆ€ğµğ‘–âˆ©ğµğ‘—= âˆ…, ğ‘–â‰ ğ‘—, ğ¸âŠ†\nâ„, and ğ‘šâˆ—(ğ¸) < âˆ, if ğ‘šâˆ—(ğ¸\\ â‹ƒ\nğµğ‘–\nğ‘›\nğ‘–=1\n) < ğœ€, âˆ€ğœ€> 0, holds, {ğµğ‘–}ğ‘–=1\nğ‘› defines a Vitali Covering of ğ¸. \n \n17 \n \nLemma 1.5 (Heine-Borel Covering Theorem) Given Î“ is a close and bounded set. Then, an open \nset sequence as {ğ‘”ğ‘–}ğ‘–=1\nğ¾\nâ‰ğº, â‹ƒ\nğ‘”\nğ¾\nğ‘–=1\nğ‘–âŠ‡Î“, and ğºÌ¿ = â„µ0. \n  \nLemma 1.6 (Composition of Function) Given ğ‘”âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]), and ğ‘” is not a constant real \nfunction, if ğ‘“âˆ‰ğ¿ğ‘–ğ‘1([ğ‘, ğ‘]), ğ‘”(ğ‘“(ğ‘¥)) âˆ‰ğ¿ğ‘–ğ‘1([ğ‘, ğ‘]) holds. \nProof: Proof by contradiction, if assume ğ‘”(ğ‘“(ğ‘¥)) âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]), âˆ€ğ‘¥1, ğ‘¥2 âˆˆ[ğ‘, ğ‘], ğ‘“(ğ‘¥1), ğ‘“(ğ‘¥2) âˆˆ\n[ğ‘, ğ‘], \n|ğ‘”(ğ‘“(ğ‘¥1)) âˆ’ğ‘”(ğ‘“(ğ‘¥2))| < ğ¿ğ‘”|ğ‘“(ğ‘¥1) âˆ’ğ‘“(ğ‘¥2)| < ğ‘< âˆ and ğ¿ğ‘”â‰ 0. \n(A.18) \nHowever, since ğ‘“âˆ‰ğ¿ğ‘–ğ‘1([ğ‘, ğ‘]) , we have: |ğ‘“(ğ‘¥1) âˆ’ğ‘“(ğ‘¥2)| > ğ‘€ that is contradiction with \n|ğ‘“(ğ‘¥1) âˆ’ğ‘“(ğ‘¥2)| <\nğ‘\nğ¿ğ‘”. Thus, (ğ‘“(ğ‘¥)) âˆ‰ğ¿ğ‘–ğ‘1([ğ‘, ğ‘]) holds. \n \nTheorem 1.2 (Efficiency of Non-Fully Connected Architecture of SENDER) Given a series of \nnon-smoothed activation function {ğ‘“ğ‘–}ğ‘–=1\nğ‘\n, denoted on [ğ‘, ğ‘] âŠ†â„1 , if assume {ğ‘“ğ‘–}ğ‘–=1\nğ‘\nğ‘–âŠ†\nğ¿ğ‘–ğ‘1([ğ‘, ğ‘]\\{ğ‘ˆ(ğ‘¥ğ‘–, ğ›¿)}) ğ‘–âˆˆâ„•, and ğ‘ˆ(ğ‘¥ğ‘–, ğ›¿) is an open cubic with the center ğ‘¥ğ‘– and radius ğ›¿> 0. \nThe composition of  ğ‘“ğ‘–, ğ‘“ğ‘—âˆˆ{ğ‘“ğ‘–}ğ‘–=1\nğ‘ are denoted as ğ‘“ğ‘—,ğ‘–â‰ğ‘“ğ‘—(ğ‘“ğ‘–(ğ‘¥)); the various composition â„±â‰\nğ‘“â‹¯,ğ‘˜,â‹¯ğ‘—.ğ‘–âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]\\[ğ‘, ğ‘‘])  holds, when ğ‘˜â†’âˆ, [ğ‘, ğ‘‘] âŠ‡â‹ƒ\nğ‘ˆ(ğ‘¥ğ‘–, ğ›¿)\nğ‘˜\nğ‘–=1\n and ğ‘š([ğ‘, ğ‘‘]) â‰ 0 ; \nmoreover, given ğ‘¡â†’âˆ, the summation of âˆ‘\nâ„±ğ‘–\nğ‘¡\nğ‘–=1\n leads to âˆ‘\nâ„±ğ‘–\nğ‘¡\nğ‘–=1\nâˆ‰ğ¿ğ‘–ğ‘1([ğ‘, ğ‘]\\[ğ‘â€², ğ‘‘â€²]) , \n[ğ‘â€², ğ‘‘â€²] âŠ‡[ğ‘, ğ‘‘], and ğ‘š([ğ‘â€², ğ‘‘â€²]) â‰ 0. And ğ‘š(âˆ™) represents the Lebesgue measure. \nProof:  At first, we discuss ğ‘˜< âˆ, and we assume, ğ‘“âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]\\{ğ‘¥0}) \nAccording to Lemma 1 and Lemma 2, if ğ‘¥0 âˆˆğµ(ğ‘¥0, ğ›¿), for we have: \nğœ”ğ‘“ğ‘–(ğ‘¥ğ‘–) = lim\nğ›¿â†’0 sup{|ğ‘“ğ‘–(ğ‘¥â€²) âˆ’ğ‘“ğ‘–(ğ‘¥â€²â€²)|: ğ‘¥â€², ğ‘¥â€²â€² âˆˆğµ(ğ‘¥ğ‘–, ğ›¿) âŠ†[ğ‘, ğ‘]} > ğ‘€ \n(A.19) \n \nğœ”ğ‘“ğ‘—(ğ‘¥ğ‘—) = lim\nğ›¿â†’0 sup{|ğ‘“ğ‘—(ğ‘¦â€²) âˆ’ğ‘“ğ‘—(ğ‘¦â€²â€²)|: ğ‘¦â€², ğ‘¦â€²â€² âˆˆğµ(ğ‘¥ğ‘—, ğ›¿) âŠ†[ğ‘, ğ‘]} > ğ‘€ \n(A.20) \nThus, we have ğ‘“ğ‘– and ğ‘“ğ‘— are not smooth on ğµ(ğ‘¥ğ‘–, ğ›¿) and ğµ(ğ‘¥ğ‘—, ğ›¿), respectively. \nAnd for the composition, let ğ‘˜= 2, \nğœ”ğ‘“ğ‘—,ğ‘–(ğ‘¥ğ‘–) = lim\nğ›¿â†’0 sup{|ğ‘“ğ‘—(ğ‘“ğ‘–(ğ‘¥â€²)) âˆ’ğ‘“ğ‘—(ğ‘“ğ‘–(ğ‘¥â€²â€²))|: ğ‘¥â€², ğ‘¥â€²â€² âˆˆğµ(ğ‘¥ğ‘–, ğ›¿) âŠ†[ğ‘, ğ‘]} \n(A.21) \nLet ğ‘“ğ‘–(ğ‘¥â€²) = ğ‘¥ğ‘—\nâ€² and ğ‘“ğ‘–(ğ‘¥â€²â€²) = ğ‘¥ğ‘—\nâ€²â€², if (ğ‘¥ğ‘—\nâ€², ğ‘¥ğ‘—\nâ€²â€²) âˆˆğµ(ğ‘¥ğ‘˜, ğ›¿), it is easy to prove the amplitude of ğ‘“ğ‘—,ğ‘– as \nfollowing: \nğœ”ğ‘“ğ‘—,ğ‘–(ğ‘¥1) = lim\nğ›¿â†’0 sup{|ğ‘“2(ğ‘¥ğ‘—\nâ€²) âˆ’ğ‘“2(ğ‘¥ğ‘—\nâ€²â€²)|: ğ‘¥ğ‘—\nâ€², ğ‘¥ğ‘—\nâ€²â€² âˆˆğµ(ğ‘¥ğ‘˜, ğ›¿) âŠ†ğµ(ğ‘¥ğ‘—, ğ›¿) âŠ†[ğ‘, ğ‘]}\n> ğ‘€ \n(A.22) \nNaturally, we need to analyze other relations of ğµ(ğ‘¥ğ‘˜, ğ›¿) and ğµ(ğ‘¥ğ‘—, ğ›¿); in detail, there are five \nsituations to be discussed separately: \n1). Assume, if âˆ€ğµ(ğ‘¥ğ‘˜, ğ›¿) âˆ©ğµ(ğ‘¥ğ‘—, ğ›¿) = âˆ…, ğ‘–, ğ‘—âˆˆâ„, ğ‘–â‰ ğ‘—, obviously, due to the same composition, \nwe have: ğµ(ğ‘¥ğ‘˜, ğ›¿) âˆ©ğµ(ğ‘¥ğ‘˜âˆ’1, ğ›¿) = âˆ…,  according to Lemma 1.3, \n[ğ‘, ğ‘‘]\\ â‹ƒğµ(ğ‘¥ğ‘˜, ğ›¿)\nâˆ\nğ‘˜=1\n= {ğ‘¥Ì‚ğ‘—}ğ‘—=1\nğ‘ \n(A.23) \nAnd \nğ‘š({ğ‘¥Ì‚ğ‘—}ğ‘—=1\nğ‘) = 0 \n(A.24) \n \nAccording to Lemma 1.6,  ğ‘“ğ‘—(ğ‘“ğ‘–(ğµ(ğ‘¥ğ‘–, ğ›¿)) âˆ‰ğ¿ğ‘–ğ‘1(ğµ(ğ‘¥ğ‘˜, ğ›¿) âˆªğµ(ğ‘¥ğ‘—, ğ›¿)), therefore, we have: \nğ‘“â‹¯,ğ‘˜,â‹¯ğ‘—.ğ‘–âˆ‰ğ¿ğ‘–ğ‘1([ğ‘, ğ‘‘]\\{ğ‘¥Ì‚ğ‘—}ğ‘—=1\nğ‘) \n(A.25) \n2). Similarly, if we assume âˆ€ğµ(ğ‘¥ğ‘˜, ğ›¿) âˆ©ğµ(ğ‘¥ğ‘—, ğ›¿) â‰ âˆ…, due to the composition, we have: ğµ(ğ‘¥ğ‘˜, ğ›¿) âˆ©\nğµ(ğ‘¥ğ‘˜âˆ’1, ğ›¿) â‰ âˆ…, according to Lemma 1.5,  \n[ğ‘, ğ‘] âŠ‡â‹ƒğµ(ğ‘¥ğ‘˜, ğ›¿) âŠ‡[ğ‘, ğ‘‘]\nğ¾\nğ‘˜=1\n \n(A.26) \n18 \n \nTherefore, we can conclude: \nğ‘“â‹¯,ğ‘˜,â‹¯ğ‘—.ğ‘–âˆ‰ğ¿ğ‘–ğ‘1([ğ‘, ğ‘‘]) \n(A.27) \n \n3). Moreover, if ğµ(ğ‘¥ğ‘—, ğ›¿) âŠ‡ğµ(ğ‘¥ğ‘˜, ğ›¿) âŠ‡ğµ(ğ‘¥ğ‘˜+1, ğ›¿) âŠ‡â‹¯, according to Lemma 1.5, we have: \nâ‹‚ğ‘–=1\nğ¾ğµ(ğ‘¥ğ‘–, ğ›¿) = Î â‰ âˆ… \n(A.28) \nIt means: \nğ‘“â‹¯,ğ‘˜,â‹¯ğ‘—.ğ‘–âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]\\ğµ(ğ‘¥ğ‘—, ğ›¿)) \n(A.29) \nThus, similarly, we have: \nğ‘“â‹¯,ğ‘˜,â‹¯ğ‘—.ğ‘–âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]\\Î) \n(A.31) \n \n4). Finally, if â‹¯âŠ‡ğµ(ğ‘¥ğ‘˜+1, ğ›¿) âŠ‡ğµ(ğ‘¥ğ‘˜, ğ›¿) âŠ‡ğµ(ğ‘¥ğ‘—, ğ›¿), \nTherefore, based on (1) and (2), we have:  \nğœ”ğ‘“ğ‘˜,â‹¯,2,1(ğ‘¥2) = lim\nğ›¿â†’0 sup{|ğ‘“ğ‘˜,â‹¯,2,1(ğ‘¥â€²) âˆ’ğ‘“ğ‘˜,â‹¯,2,1(ğ‘¥â€²â€²)|: ğ‘¥â€², ğ‘¥â€²â€² âˆˆ[ğ‘, ğ‘‘]} > ğ‘€ \n(A.32) \nIt indicates:  \nğ‘“â‹¯,ğ‘˜,â‹¯ğ‘—.ğ‘–âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]\\ â‹ƒğµ(ğ‘¥ğ‘–, ğ›¿))\nâˆ\nğ‘–=1\n \n(A.33) \n5). Comprehensively, the situation includes all previously discussed (1) to (4), it is easy to conclude: \nğ‘“â‹¯,ğ‘˜,â‹¯ğ‘—.ğ‘–âˆˆğ¿ğ‘–ğ‘1([ğ‘, ğ‘]\\[ğ‘, ğ‘‘] \n(A.34) \nUsing Lemma 1.1, obviously, given âˆ†: ğ‘= ğ‘¥0 < ğ‘¥1 < ğ‘¥2 < â‹¯< ğ‘¥ğ‘›= ğ‘, and âˆ†â€²: ğ‘¥â€² < ğ‘¥Ì‚1 < ğ‘¥Ì‚2 <\nâ‹¯< ğ‘¥Ì‚ğ‘›< ğ‘¥â€²â€²,  ğ‘£âˆ†1 +  ğ‘£âˆ†2 =  ğ‘£âˆ†. \n ğ‘£âˆ†+  ğ‘£Î”â€² =  ğ‘£âˆ†1 +  ğ‘£âˆ†2 +  ğ‘£Î”â€²\n= âˆ‘|ğ‘“(ğ‘¥ğ‘–) âˆ’ğ‘“(ğ‘¥ğ‘–âˆ’1)|\nğ‘›1\nğ‘–=1\n+ âˆ‘|ğ‘“(ğ‘¥ğ‘–) âˆ’ğ‘“(ğ‘¥ğ‘–âˆ’1)|\nğ‘›2\nğ‘–=ğ‘›1\n+ âˆ‘|ğ‘“(ğ‘¥ğ‘–) âˆ’ğ‘“(ğ‘¥ğ‘–âˆ’1)|\nğ‘›\nğ‘–=ğ‘›2\n \n \n \n(A.35) \nSince  ğ‘£Î”â€² > ğ‘€,  ğ‘£âˆ†+  ğ‘£Î”â€² > ğ‘€, it is easy to have: \nâˆ‘ğ‘“ğ‘–\nğ‘¡\nğ‘¡\nğ‘–=1\nâˆ‰ğ¿ğ‘–ğ‘1([ğ‘, ğ‘]) \n(A.36) \n \nLemma 1.7 (Contraction of STORM Operator) If denote ğ‘†ğ‘‡ğ‘‚ğ‘…ğ‘€â‰ğ’¯: â„ğ‘ƒÃ—ğ‘„â†’â„ğ‘ƒÃ—ğ‘„, operator \nğ’¯ is a bounded contraction operator. \nProof: According to definition of contraction operator, we have: \nâ€–ğ’¯ğ‘¡+ğ‘˜ğ‘‹âˆ’ğ’¯ğ‘¡ğ‘‹â€– = â€– âˆ‘ğœ‚ğ‘¡ğ’¹ğ‘¡\nğ‘¡+ğ‘˜\nğ‘–=ğ‘¡+1\nâ€– \n(A.37) \nConsidering the equivalence of norms in finite dimensional space and Cauchy-Schwarz inequality, \nwe have: \nâ€– âˆ‘ğœ‚ğ‘¡ğ’¹ğ‘¡\nğ‘¡+ğ‘˜\nğ‘–=ğ‘¡+1\nâ€–\n2\n2\nâ‰¤â€– âˆ‘ğœ‚ğ‘¡\n2\nğ‘¡+ğ‘˜\nğ‘–=ğ‘¡+1\nâ€–\n2\n2\nâˆ™â€– âˆ‘ğ’¹ğ‘¡\n2\nğ‘¡+ğ‘˜\nğ‘–=ğ‘¡+1\nâ€–\n2\n2\n \n \n(A.38) \nIn [27], due to the definition of ğœ‚ğ‘¡: \nğœ‚ğ‘¡â‰\nğ‘˜\n(ğœ”+ âˆ‘\nğºğ‘¡\nğ‘¡\nğ‘–=1\n)1/3 \n(A.39) \n19 \n \nAccording to the definition of ğºğ‘¡ as â€–âˆ‡ğ‘“(ğ‘¥ğ‘¡, ğœ‰ğ‘¡)â€–, if we assume the target function is smooth and \nvariance bouded, we have: \nâ€–âˆ‡ğ‘“(ğ‘¥ğ‘¡, ğœ‰ğ‘¡)â€– â‰¤ğ‘€ \n|ğœ‚ğ‘¡| < ğœ€ \n \n(A.40) \nThen, given ğ‘¡> ğ‘,  ğ‘âˆˆâ„•, we can derive the following formula: \nâ€– âˆ‘ğœ‚ğ‘¡ğ’¹ğ‘¡\nğ‘¡+ğ‘˜\nğ‘–=ğ‘¡+1\nâ€–\n2\n2\nâ‰¤ğœ€âˆ™â€– âˆ‘ğ’¹ğ‘¡\n2\nğ‘¡+ğ‘˜\nğ‘–=ğ‘¡+1\nâ€–\n2\n2\n \n \n(A.41) \nIn Eq (A.41), we only consider â€–âˆ‘\nğ’¹ğ‘¡\n2\nğ‘¡+ğ‘˜\nğ‘–=ğ‘¡+1\nâ€–: \nâ€–ğ’¹ğ‘¡+2ğ‘‹âˆ’ğ’¹ğ‘¡+1ğ‘‹â€–\n= â€–âˆ‡ğ‘“(ğ‘¥ğ‘¡+2, ğœ‰ğ‘¡+2) âˆ’âˆ‡ğ‘“(ğ‘¥ğ‘¡+1, ğœ‰ğ‘¡+1) + (1 âˆ’ğ›¼ğ‘¡+2)(ğ’¹ğ‘¡+1\nâˆ’âˆ‡ğ‘“(ğ‘¥ğ‘¡+1, ğœ‰ğ‘¡+2) âˆ’(1 âˆ’ğ›¼ğ‘¡+1)(ğ’¹ğ‘¡âˆ’âˆ‡ğ‘“(ğ‘¥ğ‘¡, ğœ‰ğ‘¡+1)â€– \n(A.42) \nWe can easily conclude: \nâ€–ğ’¹ğ‘¡+2ğ‘‹âˆ’ğ’¹ğ‘¡+1ğ‘‹â€–\nâ‰¤â€–âˆ‡ğ‘“(ğ‘¥ğ‘¡+2, ğœ‰ğ‘¡+2) âˆ’âˆ‡ğ‘“(ğ‘¥ğ‘¡+1, ğœ‰ğ‘¡+1)â€–\n+ â€–(1 âˆ’ğ›¼ğ‘¡+2)(ğ’¹ğ‘¡+1 âˆ’âˆ‡ğ‘“(ğ‘¥ğ‘¡+1, ğœ‰ğ‘¡+2) âˆ’(1 âˆ’ğ›¼ğ‘¡+1)(ğ’¹ğ‘¡\nâˆ’âˆ‡ğ‘“(ğ‘¥ğ‘¡, ğœ‰ğ‘¡+1)â€– \n(A.43) \nThen we have: \nâ€–ğ’¹ğ‘¡+2ğ‘‹âˆ’ğ’¹ğ‘¡+1ğ‘‹â€–\nâ‰¤â€–âˆ‡ğ‘“(ğ‘¥ğ‘¡+2, ğœ‰ğ‘¡+2) âˆ’âˆ‡ğ‘“(ğ‘¥ğ‘¡+1, ğœ‰ğ‘¡+1)â€–\n+ â€–ğ’¹ğ‘¡+1 âˆ’ğ’¹ğ‘¡+ ğ›¼ğ‘¡+1ğ’¹ğ‘¡âˆ’ğ›¼ğ‘¡+2ğ’¹ğ‘¡+1â€–\n+ â€–âˆ‡ğ‘“(ğ‘¥ğ‘¡+1, ğœ‰ğ‘¡+2) âˆ’âˆ‡ğ‘“(ğ‘¥ğ‘¡, ğœ‰ğ‘¡+1)â€– \n \n(A.44) \nAnd we can also conclude: \nâ€–ğ’¹ğ‘¡+2ğ‘‹âˆ’ğ’¹ğ‘¡+1ğ‘‹â€– â‰¤ğœ€1 + â€–ğ’¹ğ‘¡+1 âˆ’ğ’¹ğ‘¡+ ğ›¼ğ‘¡+1ğ’¹ğ‘¡âˆ’ğ›¼ğ‘¡+2ğ’¹ğ‘¡+1â€– + ğœ€2 \n(A.45) \nEq. (A.45) can be rewritten as below: \nâ€–ğ’¹ğ‘¡+2ğ‘‹âˆ’ğ’¹ğ‘¡+1ğ‘‹â€– â‰¤ğœ€1 + (1 âˆ’ğ›¼Ì‚) âˆ™â€–ğ’¹ğ‘¡+1 âˆ’ğ’¹ğ‘¡â€– + ğœ€2 \nğ›¼Ì‚ â‰min (ğ›¼ğ‘¡+1, ğ›¼ğ‘¡+2) \n(A.46) \nObviously, â€–ğ’¹ğ‘¡+1 âˆ’ğ’¹ğ‘¡â€– â‰¤ğ‘€, we have: \nâ€–ğ’¹ğ‘¡+2ğ‘‹âˆ’ğ’¹ğ‘¡+1ğ‘‹â€–\nâ€–ğ’¹ğ‘¡+1 âˆ’ğ’¹ğ‘¡â€–\nâ‰¤(1 âˆ’ğ›¼Ì‚) + ğœ€1 + ğœ€2 \n(A.47) \nIf ğ‘> 0, according to the definition of ğ›¼: \nğ›¼ğ‘¡+1 â‰ğ‘âˆ™ğœ‚ğ‘¡\n2 \n(A.48) \nThen, due to âˆ€ğœ€1, ğœ€2, Eq. (A.47) holds: \nâ€–ğ’¹ğ‘¡+2ğ‘‹âˆ’ğ’¹ğ‘¡+1ğ‘‹â€–\nâ€–ğ’¹ğ‘¡+1 âˆ’ğ’¹ğ‘¡â€–\nâ‰¤(1 âˆ’ğ›¼Ì‚) \n(A.49) \nSince âˆ€ğ‘¡âˆˆâ„•, ğ›¼ğ‘¡> 0: \nâ€–ğ’¹ğ‘¡+2ğ‘‹âˆ’ğ’¹ğ‘¡+1ğ‘‹â€–\nâ€–ğ’¹ğ‘¡+1 âˆ’ğ’¹ğ‘¡â€–\n< 1 \n(A.50) \nThus, we proved the STORM operator ğ’¯ is a contraction operator within the finite dimensional \nspace. \n \n20 \n \nLemma 1.8 (Contraction of Operators Combination) Given two contraction mappings Î¦1 and \nÎ¦2, we have the composite of two contraction mapping as  Î¦2 âˆ™Î¦1. The composite mapping Î¦2 âˆ™\nÎ¦1 must be contractive. \nProof: According to the definition of contraction linear operator, we have: \nâˆƒğœâˆˆ(0,1) \nğœŒâ‰â€–Î¦ğ‘¥âˆ’Î¦yâ€– \nğœŒ(Î¦ğ‘¥, Î¦ğ‘¦) â‰¤ğœğœŒ(ğ‘¥, ğ‘¦) \n \n(A.51) \nObviously, and we have: \nğœŒ(Î¦1ğ‘¢, Î¦1ğ‘£) â‰¤ğœğœŒ(ğ‘¢, ğ‘£) âˆ€ğœâˆˆ(0,1) \nğœŒ(Î¦2ğ‘¥, Î¦2ğ‘¦) â‰¤ğœ‚ğœŒ(ğ‘¥, ğ‘¦) âˆ€ğœ‚âˆˆ(0,1) \n(A.52) \nIf we set: \nğ‘¥= Î¦1ğ‘¢, ğ‘¦= Î¦1ğ‘£ \n(A.53) \nthe inequality below holds: \nğœŒ(Î¦2ğ‘¥, Î¦2ğ‘¦) â‰¤ğœ‚ğœŒ(Î¦1ğ‘¢, Î¦1ğ‘£) â‰¤ğœğœ‚ğœŒ(ğ‘¢, ğ‘£) \n(A.54) \nSince the definition as  \nâˆ€ğœ, ğœ‚âˆˆ(0,1), ğœŒ(Î¦2Î¦1ğ‘¢, Î¦2Î¦1ğ‘¦) â‰¤ğœğœ‚ğœŒ(ğ‘¢, ğ‘£) \n \n(A.55) \nTheorem 1.3 (Convergence Rate of SENDER in Finite Dimensionality Space) Denote Adam as \nan operator ğ’¯ in a finite dimensionality space. Due to the convergence of STORM with a rate of  \nğ’ª(ğ‘‡\n1\n2 + ğœ\n1\n3/ğ‘‡\n1\n3), the convergence of SENDER is guaranteed to be the same as STORM. \nLemma 1.8 (Adam Operator is bounded) [27] If we denote the Adam optimizer operator as  \nğ’¯: â„ğ‘ƒÃ—ğ‘„â†’â„ğ‘ƒÃ—ğ‘„, we have â€–ğ’¯â€– <\n1\nâˆšğ‘‡.  \n \nCorollary 1.1 (General Contraction Operator) According to Lemma 1.2, if denote the operators \n{Î¦ğ‘–}ğ‘–=1\nğ¾, âˆ€Î¦ğ‘– ğ‘–âˆˆâ„•, Î¦ğ‘–: â„ğ‘†Ã—ğ‘‡â†’â„ğ‘†Ã—ğ‘‡; considering any combination of operators: Î¦ğ¾âˆ™â‹¯âˆ™Î¦2 âˆ™\nÎ¦1, if at least a single operator Î¦ğ‘– is contraction operator, and other operators are bounded, such as \nâˆ€ğ‘–â‰ ğ‘˜ â€–Î¦ğ‘–â€– â‰¤ğ‘€. If and only if âˆ\nâ€–Î¦ğ‘–â€– < 1\nğ¾\nğ‘–=1\n, the combination of operator series Î¦ğ¾âˆ™â‹¯âˆ™Î¦2 âˆ™\nÎ¦1 is a contraction operator. \nProof: Obviously, according to Lemma 1.2, use a series as {ğœğ‘–}ğ‘–=1\nğ¾ to replace ğœ, ğœ‚âˆˆ(0,1), \nObviously, we have:  \nğœğ‘–âˆˆ(0,1) ğ‘–âˆˆâ„•  \nğœŒ(Î¦ğ¾âˆ™â‹¯âˆ™Î¦2Î¦1ğ‘¢, Î¦ğ¾âˆ™â‹¯âˆ™Î¦2Î¦1ğ‘¦) â‰¤ğœğ¾âˆ™â‹¯ğœ2 âˆ™ğœ1 âˆ™ğœŒ(ğ‘¢, ğ‘£) \n(A.56) \n \nSince ğœğ¾âˆ™â‹¯ğœ2 âˆ™ğœ1 < 1, we have proved this corollary. \n \nCorollary 1.2 (Iterative Contraction Operator) According to Lemma 1.2, if denote the operators \n{Î¦ğ‘–}ğ‘–=1\nğ¾, âˆ€Î¦ğ‘– ğ‘–âˆˆâ„•, Î¦ğ‘–: â„ğ‘ƒÃ—ğ‘„â†’â„ğ‘ƒÃ—ğ‘„; considering any combination of operators: Î¦ğ¾âˆ™â‹¯âˆ™Î¦2 âˆ™\nÎ¦1, if at least a single operator Î¦ğ‘– is contraction operator, and other operators are bounded, such as \nâˆ€ğ‘–â‰ ğ‘˜, â€–Î¦ğ‘–â€– â‰¤ğ‘€. If and only if lim\nğ‘›â†’âˆâˆ\nâ€–Î¦ğ‘–â€–ğ‘›\nğ¾\nğ‘–=1\n= ğ‘< 1, the combination of operator series Î¦ğ¾\nğ‘›âˆ™\nâ‹¯âˆ™Î¦2\nğ‘›âˆ™Î¦1\nğ‘›. \nProof: Obviously, according to Lemma 3.1 and Corollary 3.1, use a series as {ğœğ‘–}ğ‘–=1\nğ¾ to replace ğœ, ğœ‚âˆˆ\n(0,1), \nAnd we have:  \nâˆ€ğœğ‘–âˆˆ(0,1) ğ‘–âˆˆâ„•  \n(A.57) \n21 \n \nğœŒ(Î¦ğ¾\nğ‘›âˆ™â‹¯âˆ™Î¦2\nğ‘›âˆ™Î¦1\nğ‘›ğ‘¢, Î¦ğ¾\nğ‘›âˆ™â‹¯âˆ™Î¦2\nğ‘›âˆ™Î¦1\nğ‘›ğ‘¦) < ğœğ‘–\nğ‘›âˆ™â‹¯âˆ™ğœ2\nğ‘›âˆ™ğœ1\nğ‘›âˆ™ğœŒ(ğ‘¢, ğ‘£) \nSince 0 < ğœğ‘–\nğ‘›âˆ™â‹¯âˆ™ğœ2\nğ‘›âˆ™ğœ1\nğ‘›< 1, we have proved this corollary. \n \nTheorem 1.3 (Convergence of SENDER in Finite Dimensionality Space) SENDER can converge \nas fast as STORM. \nProof: If we have ğ‘ˆ, ğ‘‰âˆˆâ„ğ‘šÃ—ğ‘›, according to Theorems 3.2-3.4, and Lemma 3.2, the SENDER can \nbe represented as  \nğ‘†ğ¸ğ‘ğ·ğ¸ğ‘…â‰(ğ’®â„›ğ’©ğ’¯)ğ‘˜âˆ™ğ¼: â„ğ‘€Ã—ğ‘âŸ¶â„ğ‘€Ã—ğ‘ \n(A.58) \nAccording to Lemma 1.2, Corollary 1.1 and 1.2, we conclude: \nâ€–ğ’¯ğ‘˜ğ‘ˆâˆ’ğ’¯ğ‘˜ğ‘‰â€– â‰¤ğœŒğ‘˜â€–ğ‘ˆâˆ’ğ‘‰â€– \n(A.60) \nand 0 < ğœŒğ‘˜< 1 holds and  ğœŒğ‘˜ equals to ğ’ª(ğ‘‡\n1\n2 + ğœ\n1\n3/ğ‘‡\n1\n3) [27], \nAnd given other definitions of operators adopted by DEMAND, obviously, since all norms of these \noperators are bounded, the following norm inequality holds: \nâ€–(ğ’®â„›ğ’©ğ’œ)ğ‘˜â„âˆ™ğ‘ˆâˆ’(ğ’®â„›ğ’©ğ’œ)ğ‘˜â„âˆ™ğ‘‰â€– â‰¤( ğ‘\nğ‘ğ‘ ğ‘Ÿ)ğ‘˜âˆ™â€–ğ‘ˆâˆ’ğ‘‰â€– \n(A.61) \nIf 0 < (\nğ‘\nğ‘ğ‘ ğ‘Ÿ)ğ‘˜ <1, ğ‘˜â†’âˆ, can guarantee the convergence of SENDER comparable to STORM. It \ndemonstrates that the convergence rate of SENDER would be equal to STORM, since the \nconvergence rate of STORM has been proved as ğ’ª(ğ‘‡\n1\n2 + ğœ\n1\n3/ğ‘‡\n1\n3) , and (\nğ‘\nğ‘ğ‘ ğ‘Ÿ)ğ‘˜can be rewritten as \n1\nâˆšğ‘ğ‘ ğ‘Ÿ(\nğ‘\nğ‘ğ‘ ğ‘Ÿ)ğ‘˜âˆšğ‘ğ‘ ğ‘Ÿ, if and only if (\nğ‘\nğ‘ğ‘ ğ‘Ÿ)ğ‘˜âˆšğ‘ğ‘ ğ‘Ÿ is a constant ğ’, and let \n1\nâˆšğ‘ğ‘ ğ‘Ÿ be ğ’ª(ğ‘‡\n1\n2 + ğœ\n1\n3/ğ‘‡\n1\n3) . \n \nCollaroy 1.3 (Convergence of SENDER in Infinite Dimensionality Space) Given the infinite \ndimensionality space, the SENDER is denoted as an operator as ğ’Ÿ: â„âˆÃ—âˆâ†’â„ğ‘šÃ—ğ‘›. If we assume ğ’Ÿ \nand â„âˆÃ—âˆ can be defined as infinite matrix and each element can be represented as ğ‘‘ğ‘–,ğ‘—âˆˆğ’Ÿ and \nğ‘Ÿğ‘–,ğ‘—âˆˆâ„âˆÃ—âˆ,  ğ‘–, ğ‘—â†’âˆ, respectively. ğ’Ÿ can converge, if and only if ğ‘‘ğ‘–,ğ‘—\nğ‘˜âˆ™ğ‘Ÿğ‘–,ğ‘— should be ğ’ª(\n1\nğ‘›ğ‘). \nğ·=\n[\n \n \n \n ğ‘‘1\nğ‘‘2\nâ‹®\nğ‘‘ğ‘›âˆ’1\nâ‹®\n]\n \n \n \n \n \n(A.62) \nIn Eq. (C.21), operator ğ· denotes an infinite dimensionality operator. \nğ‘‹=\n[\n \n \n \n \nğ‘Ÿ1\nğ‘Ÿ2\nâ‹®\nğ‘Ÿğ‘›âˆ’1\nâ‹®]\n \n \n \n \n \n(A.63) \nIn Eq. (C.21), without generality, input ğ‘‹ denotes an infinite dimensionality matrix. \n \nThen, given the operator ğ· applied on input matrix ğ‘‹ as: \n ğ·âŠ—ğ‘‹=\n[\n \n \n \n \nğ‘‘1ğ‘Ÿ1\nğ‘‘2ğ‘Ÿ2\nâ‹®\nğ‘‘ğ‘›âˆ’1ğ‘Ÿğ‘›âˆ’1\nâ‹®\n]\n \n \n \n \n \n(A.64) \nObviously, due to the inequality of norms in the infinite dimensionality space, we examine the â„“2 \nnorm as an example: \nâ€–ğ·âŠ—ğ‘‹â€–2 = âˆ‘(ğ‘‘ğ‘–ğ‘Ÿğ‘–)2\nâˆ\nğ‘–=1\n \n(A.65) \n \nEasily, we can conclude: \n22 \n \nâ€–ğ·ğ‘˜âŠ—ğ‘‹â€–2 = âˆšâˆ‘(ğ‘‘ğ‘–\nğ‘˜ğ‘Ÿğ‘–)2\nâˆ\nğ‘–=1\n< âˆâŸºlim\nğ‘˜â†’âˆ(ğ‘‘ğ‘–\nğ‘˜ğ‘Ÿğ‘–)\n2 = 1\nğ‘›ğ‘ ğ‘> 1 \n(A.66) \n \nTherefore, we have proved the DEMAND converges in an infinite dimensionality space, if and only \nif each element of ğ·âˆ™ğ‘‹ as  \n1\nğ‘›ğ‘ , and ğ‘> 1, ğ‘›âˆˆâ„. \n \nLemma 1.9 (Convergence of Alternative Optimization of Real Function) For a series of real \nfunction as {ğ‘“ğ‘–,ğ‘—}ğ‘–,ğ‘—=1\nâˆ\n. If we have: lim\nğ‘–â†’âˆğ‘“ğ‘–,ğ‘—(ğ‘¥) â†’â„ğ‘€,ğ‘—, ğ‘. ğ‘’. ğ‘¥âˆˆ[ğ‘, ğ‘] and lim\nğ‘—â†’âˆâ„ğ‘€,ğ‘—â†’ğ‘”ğ‘€,ğ‘, ğ‘. ğ‘’. ğ‘¥âˆˆ\n[ğ‘, ğ‘]. Then, âˆƒ lim\nğ‘˜â†’âˆğ‘“ğ‘–ğ‘˜,ğ‘—ğ‘˜â†’ğ‘”ğ‘€,ğ‘ ğ‘. ğ‘’.  ğ‘¥âˆˆ[ğ‘, ğ‘] holds.  \nProof: If considering the uniform convergence of {ğ‘“ğ‘–,ğ‘—}ğ‘–,ğ‘—=1\nâˆ\n, since lim\nğ‘–â†’âˆğ‘“ğ‘–,ğ‘—â†’â„ğ‘€,ğ‘—, ğ‘. ğ‘’. ğ‘¥âˆˆ[0, 1] \nand lim\nğ‘—â†’âˆâ„ğ‘€,ğ‘—â†’ğ‘”ğ‘€,ğ‘, ğ‘. ğ‘’. ğ‘¥âˆˆ[0, 1], according to Riez Theorem, âˆƒ {ğ‘“ğ‘–ğ‘˜,ğ‘—ğ‘˜}ğ‘˜=1\nâˆ. \n|ğ‘“ğ‘–ğ‘˜,ğ‘—ğ‘˜âˆ’â„ğ‘—| < ğœ€\n2 \n(A.67) \nAnd we have: \n|â„ğ‘—âˆ’ğ‘”| < ğœ€\n2 \n \n(A.68) \nThen, we have:  \n|ğ‘“ğ‘–ğ‘˜,ğ‘—ğ‘˜âˆ’â„ğ‘—| + |â„ğ‘—âˆ’ğ‘”| = |ğ‘“ğ‘–ğ‘˜,ğ‘—ğ‘˜âˆ’ğ‘”| < ğœ€ \n \n(A.69) \n \nTheorem 1.4 (Alternative Convergence of SENDER) Given {â„±ğ‘–,ğ‘—,ğ‘˜,ğ‘¡}ğ‘–,ğ‘—,ğ‘˜,ğ‘¡=1\nâˆ\n,  {ğ’ ğ‘—,ğ‘˜,ğ‘¡}ğ‘—,ğ‘˜,ğ‘¡=1\nâˆ\n, \n{â„‹ ğ‘˜,ğ‘¡}ğ‘˜,ğ‘¡=1\nâˆ\n, and {ğ’¦ ğ‘¡}ğ‘¡=1\nâˆ, are series of continuous operator [30] applied on a finite dimensional \nspace, the series of operators. And we have: â„±ğ‘–,ğ‘—,ğ‘˜,ğ‘¡, ğ’ ğ‘—,ğ‘˜,ğ‘¡, â„‹ ğ‘˜,ğ‘¡, ğ’¦ ğ‘¡: â„ğ‘ƒÃ—ğ‘„â†’â„ğ‘ƒÃ—ğ‘„. If we have: \nlğ‘–ğ‘š\nğ‘–â†’âˆâ„±ğ‘–,ğ‘—â†’ğ’ğ‘—,ğ‘˜,ğ‘¡, ğ‘™ğ‘–ğ‘š\nğ‘—â†’âˆğ’ğ‘—,ğ‘˜,ğ‘¡â†’â„‹ ğ‘˜,ğ‘¡, ğ‘™ğ‘–ğ‘š\nğ‘˜â†’âˆâ„‹ğ‘˜,ğ‘¡â†’ğ’¦ ğ‘¡, and lğ‘–ğ‘š\nğ‘¡â†’âˆğ’¦ğ‘¡â†’ğ’¢. Then, âˆƒğ‘™ğ‘–ğ‘š\nğ‘›â†’âˆâ„±ğ‘–ğ‘›,ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›â†’ğ’¢ \nholds. \nProof: According to Lemma 1.5, similarly, let constant ğ‘‡< âˆ, we have: \nâ€–â„±ğ‘–ğ‘›,ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›âˆ’ğ’ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›â€– < ğœ€\n4ğ‘‡ \n \n(A.70) \nSimilarly, we have: \nâ€–ğ’ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›âˆ’â„‹ğ‘˜ğ‘›,ğ‘¡ğ‘›â€– < ğœ€\n4ğ‘‡ \nâ€–â„‹ğ‘˜ğ‘›,ğ‘¡ğ‘›âˆ’ğ’¦ğ‘¡ğ‘›â€– < ğœ€\n4ğ‘‡ \nâ€–ğ’¦ğ‘¡ğ‘›âˆ’ğ’¢â€– < ğœ€\n4ğ‘‡ \n \n \n(A.71) \nThe following inequality holds: \nâ€–â„±ğ‘–ğ‘˜,ğ‘—ğ‘˜âˆ’ğ’¢â€– = â€–â„±ğ‘–ğ‘›,ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›âˆ’ğ’ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›â€– + â€–ğ’ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›âˆ’â„‹ğ‘˜ğ‘›,ğ‘¡ğ‘›â€–\n+ â€–â„‹ğ‘˜ğ‘›,ğ‘¡ğ‘›âˆ’ğ’¦ğ‘¡ğ‘›â€– + â€–ğ’¦ğ‘¡ğ‘›âˆ’ğ’¢â€– â‰¤ğœ€\nğ‘‡ \n(A.72) \nAnd we also have: \nâ€–â„±ğ‘–ğ‘›,ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›ğ‘‹âˆ’ğ’¢ğ‘‹â€– â‰¤â€–â„±ğ‘–ğ‘›,ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›âˆ’ğ’¢â€– âˆ™â€–ğ‘‹â€– < ğ‘‡âˆ™ğœ€\nğ‘‡= ğœ€ \n(A.73) \nThis equation indicates the operator can converge to a fixed point defined on Banach space, using \nalternative strategy and Banach Fixed Point Theorem (Rudin, 1973), if and only if \nlim\nğ‘˜â†’âˆâ€–â„±ğ‘–ğ‘›,ğ‘—ğ‘›,ğ‘˜ğ‘›,ğ‘¡ğ‘›â€– < 1. \n23 \n \nAppendix B \n \nAlgorithm 2.1 (Core \nAlgorithm): Semi-Estimated \nNonlinear Deep Efficient Reconstructor (SENDER) \nInput: ğ¼âˆˆâ„ğ‘ƒÃ—ğ‘„, ğ¼ is the input signal matrix; set ğœ†> 1 as \nthe penalty parameter; randomly initialize {ğ‘‹ğ‘˜}ğ‘˜=1\nğ‘€\n, \n{ğ‘Œğ‘˜}ğ‘˜=1\nğ‘€, {ğ‘ˆğ‘˜}ğ‘˜=1\nğ‘€, {ğ‘‰ğ‘˜}ğ‘˜=1\nğ‘€and {ğ‘†ğ‘˜}ğ‘˜=1\nğ‘€ ; \nSet ğ‘Ÿ as the initial estimated rank of ğ‘‹1, ğ‘Œ1, ğ‘ˆ1, ğ‘‰1 and layer \nğ‘˜ as 0.    \n    while ğ‘šğ‘–ğ‘› (ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘Œ, ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘‰) > 1 \n          update ğ‘‹ğ‘˜ using Eq. (3-1); \n          update ğ‘Œğ‘˜ using Eq. (3-2); \n          update ğ‘ˆğ‘˜ using Eq. (3-3); \n          update ğ‘‰ğ‘˜ using Eq. (3-4); \n          update ğ‘†ğ‘˜  using Eq. (3-5);       \n          use Algorithm 2.2 to estimate ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘Œ of ğ‘Œğ‘˜; \n          use Algorithm 2.2 to estimate ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘‰ of ğ‘‰ğ‘˜; \n          ğ‘˜â†ğ‘˜+ 1;  \n          ğ¼â†ğ¼âˆ’ğ‘†ğ‘˜; \n   end while \n  Mâ†ğ‘˜; \nUse Algorithm 2.6 to perform matrix back propagation for \n{ğ‘‹ğ‘˜}ğ‘˜=1\nğ‘€ and  {ğ‘Œğ‘˜}ğ‘˜=1\nğ‘€;  \nUse Algorithm 2.7 to perform matrix back propagation for , \n{ğ‘ˆğ‘˜}ğ‘˜=1\nğ‘€ and {ğ‘‰ğ‘˜}ğ‘˜=1\nğ‘€;  \nOutput: {ğ‘‹ğ‘–}ğ‘–=1\nğ¾\nâˆˆâ„ğ‘€Ã—ğ‘, {ğ‘Œğ‘–}ğ‘–=1\nğ¾\nâˆˆâ„ğ‘€Ã—ğ‘ , {ğ‘ˆğ‘–}ğ‘–=1\nğ¾\nâˆˆ\nâ„ğ‘€Ã—ğ‘, {ğ‘‰ğ‘–}ğ‘–=1\nğ¾\nâˆˆâ„ğ‘€Ã—ğ‘, and {ğ‘†ğ‘–}ğ‘–=1\nğ¾\nâˆˆâ„ğ‘€Ã—ğ‘  ; \n \nAlgorithm 2.2:  Rank Reduction Operator (RRO) \nInput: ğ‘Œğ‘˜âˆˆâ„ğ’Ã—ğ’, ğ‘Œğ‘˜ is the feature matrix; \n    ğ‘Œğ‘…ğ‘˜â†ğ‘„ğ‘…(ğ‘Œğ‘˜); \n    ğ‘šğ‘–ğ‘›ğ‘…ğ‘ğ‘›ğ‘˜â†min(1, ğ‘ ğ‘–ğ‘§ğ‘’(ğ‘Œğ‘˜)) ; \n    ğ‘’ğ‘ ğ‘¡ğ‘…ğ‘ğ‘›ğ‘˜â†ğ‘šğ‘–ğ‘›ğ‘…ğ‘ğ‘›ğ‘˜âˆ’1; \n    ğ‘‘ğ‘–ğ‘ğ‘”ğ‘Œğ‘…ğ‘˜â†ğ‘ğ‘ğ‘ (ğ‘‘ğ‘–ğ‘ğ‘”(ğ‘Œğ‘…ğ‘˜)); \n    using Algorithm 2.3 calculate the weighted difference of \nğ‘‘ğ‘–ğ‘ğ‘”ğ‘Œğ‘…ğ‘˜; \n    set weighted difference of ğ‘‘ğ‘–ğ‘ğ‘”ğ‘Œğ‘…ğ‘˜ as wd; \n    [ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘€ğ‘ğ‘¥1, ğ‘ğ‘œğ‘ ğ‘€ğ‘ğ‘¥1] â†max (ğ‘¤ğ‘‘); \n    using Algorithm 2.4 to calculate the weighted ratio of \nğ‘‘ğ‘–ğ‘ğ‘”ğ‘Œğ‘…ğ‘˜; \n    set weighted ratio of ğ‘‘ğ‘–ğ‘ğ‘”ğ‘Œğ‘…ğ‘˜ as wr; \n    [ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘€ğ‘ğ‘¥2, ğ‘ğ‘œğ‘ ğ‘€ğ‘ğ‘¥2] â†max (ğ‘¤ğ‘Ÿ); \n    if rankMax1 is equal to 1 \n        ğ‘’ğ‘ ğ‘¡ğ‘…ğ‘ğ‘›ğ‘˜â†ğ‘ğ‘œğ‘ ğ‘€ğ‘ğ‘¥1; \n    end if \n    valWRâ†find( wr>rankMax2 ); \n    if number (valWR) is equal to 1 \n        ğ‘’ğ‘ ğ‘¡ğ‘…ğ‘ğ‘›ğ‘˜â†ğ‘ğ‘œğ‘ ğ‘€ğ‘ğ‘¥1; \n    end if \n24 \n \n   using Algorithm 2.5 to calculate the weighted correlation \nof ğ‘‘ğ‘–ğ‘ğ‘”ğ‘Œğ‘…ğ‘˜; \n    set weighted correlation of ğ‘‘ğ‘–ğ‘ğ‘”ğ‘Œğ‘…ğ‘˜ as wc; \n    [ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘€ğ‘ğ‘¥3, ğ‘ğ‘œğ‘ ğ‘€ğ‘ğ‘¥3] â†max (ğ‘¤ğ‘); \n    if rankMax1 is equal to 1 \n        ğ‘’ğ‘ ğ‘¡ğ‘…ğ‘ğ‘›ğ‘˜â†ğ‘ğ‘œğ‘ ğ‘€ğ‘ğ‘¥3; \n    end if \n    valWCâ†find( wc>rankMax3 ); \n    if number (valWC) is equal to 1 \n        ğ‘’ğ‘ ğ‘¡ğ‘…ğ‘ğ‘›ğ‘˜â†ğ‘ğ‘œğ‘ ğ‘€ğ‘ğ‘¥3; \n    end if \n   ğ‘’ğ‘ ğ‘¡ğ‘…ğ‘ğ‘›ğ‘˜â†max (ğ‘ğ‘œğ‘ ğ‘€ğ‘ğ‘¥1, ğ‘ğ‘œğ‘ ğ‘€ğ‘ğ‘¥2, ğ‘ğ‘œğ‘ ğ‘€ğ‘ğ‘¥3); \nOutput: estRank \n \nAlgorithm 2.3:  Weighted Difference (WD) \nInput: ğ‘‰ğ‘’ğ‘âˆˆâ„ğŸÃ—ğ’, ğ‘‰ğ‘’ğ‘ is a vector; \n  cumSum  â† Calculate cumulative sum of Vec; \n  diffVec  â† Calculate differences between adjacent \nelements of Vec; \n resverseVec â† reverse Vec; \n WD â† abs(diffVec) ./ reverseVec; \nOutput: WD \n \nAlgorithm 2.4:  Weighted Ratio (WR) \nInput: ğ‘‰ğ‘’ğ‘âˆˆâ„ğŸÃ—ğ’, ğ‘‰ğ‘’ğ‘ is a vector; \n     L  â† Calculate length of Vec; \n   ratioVec  â† Vec(1:L-1) ./ Vec(2:L); \n   WR â† (L-2)*ratioVec ./ sum (ratioVec); \nOutput: WR \n \nAlgorithm 2.5:  Weighted Correlation (WC) \nInput: ğ‘‰ğ‘’ğ‘âˆˆâ„ğŸÃ—ğ’, ğ‘‰ğ‘’ğ‘ is a vector; \nWCâ†Calculate the weight correlation using Eq. (7) \nOutput: WC \n \nAlgorithm 2.6: Matrix Back Propagation (MBP) for \nLinear Model \nInput: {ğ‘‹ğ‘–}ğ‘–=1\nğ‘€\nâˆˆâ„ğ‘‡Ã—ğ‘†, {ğ‘Œğ‘–}ğ‘–=1\nğ‘€\nâˆˆâ„ğ‘‡Ã—ğ‘†, \nset ğ‘ğ‘–â†\nâˆ\nğ‘‹ğ‘–\nğ‘–\nğ‘—=1\nğ‘Œğ‘–; \n   for k = T to 1 \n        ğœ“âŸµâˆ\nğ‘‹ğ‘–\nğ‘˜âˆ’1\nğ‘–=1\n; \n        ğ·ğ‘˜âŸµğœ“â€ ğ‘ğ‘˜ğ‘ŒÌ‚ğ‘˜\nâ€ ; \n        if  k<M \n            ğ›¼Ì‚ğ‘˜âŸµğ›¼ğ‘€ \n25 \n \n      else \n           ğ›¼Ì‚ğ‘˜âŸµğ·ğ‘˜+1ğ›¼Ì‚ğ‘˜+1,  \n      end if \n     ğ‘ŒÌ‚ğ‘˜\n+â¨ğ‘ŒÌ‚ğ‘˜\nâˆ’âŸµğ‘ŒÌ‚ğ‘˜  \n     ğ‘§ğ‘˜â†ğ¼âˆ’âˆ\nğ‘ˆğ‘–\nğ‘˜\nğ‘–=1\nâˆ™(ğ’©ğ‘˜âˆ™ğ‘‰ğ‘˜) \n     ğ‘ŒÌ‚ğ‘˜\n+ âŸµğ‘Œğ‘˜\n+ âŠ™âˆš\n[ğœ“ğ‘‡ğ‘ğ‘˜]++[ğœ“ğ‘‡ğœ“]âˆ’ğ‘ŒÌ‚ğ‘˜\n+\n[ğœ“ğ‘‡ğ‘ğ‘˜]âˆ’+[ğœ“ğ‘‡ğœ“]+ğ‘ŒÌ‚ğ‘˜\n+; \n      |ğ‘ŒÌƒğ‘˜\nâˆ’| âŸµ|ğ‘ŒÌ‚ğ‘˜\nâˆ’| âŠ™âˆš\n[ğœ“ğ‘‡ğ‘ğ‘˜]++[ğœ“ğ‘‡ğœ“]âˆ’|ğ‘ŒÌƒğ‘˜\nâˆ’|\n[ğœ“ğ‘‡ğ‘ğ‘˜]âˆ’+[ğœ“ğ‘‡ğœ“]+|ğ‘ŒÌƒğ‘˜\nâˆ’|; \n       ğ‘ŒÌƒğ‘˜\nâˆ’âŸµâˆ’|ğ‘Œğ‘˜\nâˆ’|; \n       ğ‘Œğ‘˜âŸµğ‘ŒÌƒğ‘˜\n+â¨ğ‘ŒÌƒğ‘˜\nâˆ’; \n  end for \nOutput: {ğ‘‹ğ‘–}ğ‘–=1\nğ‘€\nâˆˆâ„ğ‘ƒÃ—ğ‘„, {ğ‘Œğ‘–}ğ‘–=1\nğ¾\nâˆˆâ„ğ‘ƒÃ—ğ‘„ ; \n \nAlgorithm 2.7:  Matrix Backpropagation (MBP) for \nNonlinear Model \nInput: {ğ‘ˆğ‘˜}ğ‘˜=1\nğ‘€\n, {ğ‘‰ğ‘˜}ğ‘˜=1\nğ‘€\n, ğ¼, and set ğ¸= 0.01, MaxIter, \nğ‘ˆğ‘˜= ğ‘™ğ‘–ğ‘š\nitâ†’âˆğ‘ˆğ‘˜\nğ‘–ğ‘¡, ğ‘‹ğ‘˜= ğ‘™ğ‘–ğ‘š\nitâ†’âˆğ‘‹ğ‘˜\nğ‘–ğ‘¡, and ğ‘Œğ‘˜= ğ‘™ğ‘–ğ‘š\nitâ†’âˆğ‘Œğ‘˜\nğ‘–ğ‘¡,  \nğ‘ = ğ‘ˆğ‘˜\nğ‘–ğ‘¡ğ‘‰ğ‘˜\nğ‘–ğ‘¡ \n \nfor it in 1 to MaxIter \nğ¾âŸµ(âˆğ‘ˆğ‘˜\nğ‘€\nğ‘˜=1\n)\nğ‘‡\nâˆ™(ğ¼âˆ’âˆğ‘‹ğ‘˜\nğ‘€\nğ‘˜=1\nâˆ™ğ‘Œğ‘€) \nğ‘ƒğ‘˜\nğ‘–ğ‘¡âŸµ(ğ‘ˆğ‘˜\nğ‘–ğ‘¡)\nğ‘‡ğ‘ˆğ‘˜\nğ‘–ğ‘¡âˆ™âˆmax (ğ‘ˆğ‘˜\nğ‘€âˆ’1\nğ‘˜=1\n) \n \nğ‘ğ‘˜\nğ‘–ğ‘¡âŸµmax(ğ‘ˆğ‘˜âˆ’1) âˆ™ğ‘‘ğ’©âˆ’1(ğ‘ )\nğ‘‘ğ‘ \n, ğ‘ = ğ‘ˆğ‘˜\nğ‘–ğ‘¡ğ‘‰ğ‘˜\nğ‘–ğ‘¡ \nğ‘‘ğ‘˜\nğ‘–ğ‘¡âŸµğ‘‘ğ’©âˆ’1(ğ‘ )\nğ‘‘ğ‘ \n, ğ‘ = ğ‘ˆğ‘˜\nğ‘–ğ‘¡ğ‘‰ğ‘˜\nğ‘–ğ‘¡ \nğ·ğ‘˜\nğ‘–ğ‘¡âŸµ(ğ‘ˆğ‘˜âˆ’1\nğ‘–ğ‘¡)ğ‘‡âˆ™(ğ‘ˆğ‘˜âˆ’1\nğ‘–ğ‘¡\nâˆ™ğ’©âˆ’1(ğ‘ ) âˆ’ğ‘‰ğ‘˜âˆ’1\nğ‘–ğ‘¡)â¨€ğ‘‘ğ‘˜\nğ‘–ğ‘¡âˆ™(ğ‘‰ğ‘˜\nğ‘–ğ‘¡)ğ‘‡ \nğ‘‰ğ‘˜\nğ‘–ğ‘¡+1 â†ğ‘‰ğ‘˜\nğ‘–ğ‘¡âˆ’ğ‘‡\n2ğ‘–ğ‘¡(ğ¶ğ‘˜\nğ‘–ğ‘¡) \nğ‘ˆğ‘˜\nğ‘–ğ‘¡+1 â†ğ‘ˆğ‘˜\nğ‘–ğ‘¡âˆ’ğ‘‡\n2ğ‘–ğ‘¡(ğ·ğ‘˜\nğ‘–ğ‘¡) \nend \nOutput: {ğ‘ˆğ‘˜}ğ‘˜=1\nğ‘€ and {ğ‘‰ğ‘˜}ğ‘˜=1\nğ‘€ \n \n \nTheorem 2.1 (Rank Reduction Operator is bounded) If we denote the sparse operator as  \nâ„›: â„ğ‘†Ã—ğ‘‡â†’â„ğ‘†Ã—ğ‘‡, we have â€–â„›â€– < âˆ. \n26 \n \nProof: According to the definition of operator norm (Rudin, 1973), â€–â„›â€– â‰¤ğ‘ ğ‘¢ğ‘\nâ€–â„›ğ‘‹â€–\nâ€–ğ‘‹â€– ; obviously, \nâ€–â„›ğ‘‹â€– and â€–ğ‘‹â€– is bounded, since both of norms are based on finite dimensional matrix. And if we \ndenote: \nğ‘‹=\n[\n \n \n \n \nğ‘1\nğ‘2\nâ‹®\nğ‘ğ‘›âˆ’1\nğ‘ğ‘›]\n \n \n \n \n, â„›ğ‘‹=\n[\n \n \n \n \n \nğ‘1\nğ‘2\nâ‹®\nğ‘ğ‘˜\nâ‹®\nğ‘ğ‘›âˆ’1]\n \n \n \n \n \n \n \n(B.1) \nEq. (C11) implies: \nğ‘ ğ‘¢ğ‘\nâ€–â„›ğ‘‹â€–\nâ€–ğ‘‹â€– =\nâˆ‘\nğ‘ğ‘–\n2\nğ‘›\nğ‘–=1\nâˆ‘\n(ğ‘ğ‘–âˆ’ğ‘ğ‘–)2\nğ‘\nğ‘–=ğ‘¢\n+ âˆ‘\nğ‘ğ‘–\n2\nğ‘\nğ‘–=ğ‘£\n< âˆ \n \n(B.2) \nAlso, if we examine the weighted ratio and weight difference, only considering the finite dimensional \nspace, we have: \nğ‘‹=\n[\n \n \n \n \nğ‘1\nğ‘2\nâ‹®\nğ‘ğ‘›âˆ’1\nğ‘ğ‘›]\n \n \n \n \n, ğ‘Šğ‘…âˆ™ğ‘‹=\n[\n \n \n \n \n \n \n \nğ‘2 ğ‘1\nâ„\nğ‘3 ğ‘2\nâ„\nâ‹®\nğ‘ğ‘˜ğ‘ğ‘˜âˆ’1\nâ„\nâ‹®\nğ‘ğ‘›ğ‘ğ‘›âˆ’1\nâ„\n0]\n \n \n \n \n \n \n \n ğ‘Šğ·âˆ™ğ‘‹=\n[\n \n \n \n \n \nğ‘2 âˆ’ğ‘1\nğ‘3 âˆ’ğ‘2\nâ‹®\nğ‘ğ‘˜âˆ’ğ‘ğ‘˜âˆ’1\nâ‹®\nğ‘ğ‘›âˆ’ğ‘ğ‘›âˆ’1]\n \n \n \n \n \n \nObviously, for each rank estimation, the dimension of input matrix can be reduced at least by one. \nSimilarly, WR, WD, and WC can be considered as the contract operators for dimensional estimation. \nIt demonstrates that the input matrix can be reduced to a vector by n-1 iterations at most. \n \nTheorem 2.2 (Explanation of More Components Detected via SENDER Than ICA) In \ngeneral, DEMAND can detect more components from an input signal than Independent \nComponent Analysis Method. \nProof: At first, we assume all component included in signal matrix as: \nğ¼â‰â‹ƒğœ‰ğ‘–\nğ‘€\nğ‘–=1\nâŠ†â„ğ‘‡Ã—ğ‘€ \n(B.3) \nA single component can be denoted as following: \nğœ‰ğ‘–â‰[ğœ‰1,ğ‘–, ğœ‰2,ğ‘–, â‹¯ğœ‰ğ‘‡,ğ‘–] \n(B.4) \n \nAnd we assume that there is no any overlap in these components: \nâˆ€ğ‘–â‰ ğ‘— ğœ‰ğ‘–âˆ©ğœ‰ğ‘—= âˆ… \n(B.5) \n \nIf we define ICA operator as below: \nğ¼ğ¶ğ´â‰ğ’¯: â„ğ‘‡Ã—ğ‘€âŸ¶â„1Ã—ğ‘€ \n(B.6) \nObviously, when ICA is applied on the input signal, it is easy to conclude: \nğ’¯âˆ™ğ¼= ğ’¯(â‹ƒğœ‰ğ‘–\nğ‘€\nğ‘–=1\n) = [ğœ‰1, ğœ‰2, â‹¯, ğœ‰ğ‘€] \n \n(B.7) \nSimilarly, as previous definition of DEMAND as operator ğ’Ÿ, we can have: \n27 \n \nğ’Ÿâˆ™ğ¼= ğ·(â‹ƒğœ‰ğ‘–\nğ‘€\nğ‘–=1\n)\n= [ğœ‰1, ğœ‰2, â‹¯, ğœ‰ğ‘€, (ğœ‰1, ğœ‰2), (ğœ‰1, ğœ‰3), â‹¯, (ğœ‰1, ğœ‰3, â‹¯, ğœ‰ğ‘˜), â‹¯] \n \n(B.8) \nIt is easy to calculate the number of components detected by ICA, due to independent \nconstraint: \n|ğ’¯âˆ™ğ¼| = ğ‘€ \n(B.9) \nNevertheless, we can conclude: \n|ğ’Ÿâˆ™ğ¼| = 2ğ‘€ \n(B.10) \nObviously, we also have: \nğ‘€â‰ª2ğ‘€ \n(B.11) \n \nInequality (B.9) demonstrates that the number of components identified by SENDER \nshould be more than ICAs.  \n \nTheorem 2.3 (Sparsity Operator is Contraction) If we denote the sparse operator as ğ’®: â„ğ‘†Ã—ğ‘‡â†’\nâ„ğ‘†Ã—ğ‘‡, we have â€–ğ’®â€– < âˆ. \nProof: according to the definition of operator norm (Rudin, 1973), â€–ğ’®â€– â‰¤ğ‘ ğ‘¢ğ‘\nâ€–ğ’®ğ‘‹â€–\nâ€–ğ‘‹â€– ; obviously, \nâ€–ğ’®ğ‘‹â€– and â€–ğ‘‹â€– is bounded, since both of norms are based on finite dimensional matrix. And if we \ndenote: \nğ‘‹=\n[\n \n \n \n \nğ‘1\nğ‘2\nâ‹®\nğ‘ğ‘›âˆ’1\nğ‘ğ‘›]\n \n \n \n \n ğ’®ğ‘‹=\n[\n \n \n \n \nğ‘1\n0\nâ‹®\nğ‘ğ‘›âˆ’1\nğ‘ğ‘›]\n \n \n \n \n \n \n(B.12) \nWithout loss of generality, and based on Lemma 1.2, we calculate the â„“2 norm, and we have: \nğ‘ = â€–ğ’®â€– â‰¤ğ‘ ğ‘¢ğ‘\nâ€–ğ’®ğ‘‹â€–\nâ€–ğ‘‹â€– =\nâˆ‘\n(ğ‘ğ‘–)2\nğ‘˜\nğ‘–=1\nâˆ‘\n(ğ‘ğ‘–)2\nğ‘›\nğ‘–=1\n \n \n(B.13) \n \nSince ğ‘˜< ğ‘›,  \nğ‘ = â€–ğ’®â€– < 1 \n \n(B.14) \nThis inequality demonstrates that â€–ğ’®â€– is contraction operator. \n \nTheorem 2.4 (Random Initialization Operator is bounded) If we denote the sparse operator as \nâ„: â„ğ‘†Ã—ğ‘‡â†’â„ğ‘†Ã—ğ‘‡, we have â€–â„³â€– < âˆ. \nProof: according to the definition of operator norm (Rudin 1973), â€–â„â€– â‰¤ğ‘ ğ‘¢ğ‘\nâ€–ğ‘€ğ‘‹â€–\nâ€–ğ‘‹â€– ; obviously, â€–â„ğ‘‹â€– \nand â€–ğ‘‹â€– is bounded, since both of norms are based on finite dimensional matrix. And if we denote: \nğ‘‹=\n[\n \n \n \n \nğ‘1\nğ‘2\nâ‹®\nğ‘ğ‘›âˆ’1\nğ‘ğ‘›]\n \n \n \n \n â„ğ‘‹=\n[\n \n \n \n ğ‘1\nğ‘2\nâ‹®\nğ‘ğ‘›âˆ’1\nğ‘ğ‘›]\n \n \n \n \n â€–ğ‘‹â€– < âˆ â€–â„ğ‘‹â€– < âˆ \n \n(B.15) \nObviously, â€–â„â€– < âˆ.  \n \n \nTheorem 2.5 (Sparsity Operator is Contraction) If we denote the sparse operator as ğ’®: â„ğ‘†Ã—ğ‘‡â†’\nâ„ğ‘†Ã—ğ‘‡, we have â€–ğ’®â€– < âˆ. \n28 \n \nProof: according to the definition of operator norm (Rudin, 1973), â€–ğ’®â€– â‰¤ğ‘ ğ‘¢ğ‘\nâ€–ğ’®ğ‘‹â€–\nâ€–ğ‘‹â€– ; obviously, \nâ€–ğ’®ğ‘‹â€– and â€–ğ‘‹â€– is bounded, since both of norms are based on finite dimensional matrix. And if we \ndenote: \nğ‘‹=\n[\n \n \n \n \nğ‘1\nğ‘2\nâ‹®\nğ‘ğ‘›âˆ’1\nğ‘ğ‘›]\n \n \n \n \n ğ’®ğ‘‹=\n[\n \n \n \n \nğ‘1\n0\nâ‹®\nğ‘ğ‘›âˆ’1\nğ‘ğ‘›]\n \n \n \n \n \n \n(B.16) \nWithout loss of generality, and based on Lemma 1.2, we calculate the â„“2 norm, and we have: \nğ‘ = â€–ğ’®â€– â‰¤ğ‘ ğ‘¢ğ‘\nâ€–ğ’®ğ‘‹â€–\nâ€–ğ‘‹â€– =\nâˆ‘\n(ğ‘ğ‘–)2\nğ‘˜\nğ‘–=1\nâˆ‘\n(ğ‘ğ‘–)2\nğ‘›\nğ‘–=1\n \n \n(B.17) \n \nSince ğ‘˜< ğ‘›,  \nğ‘ = â€–ğ’®â€– < 1 \n \n(B.18) \nThis inequality demonstrates that â€–ğ’®â€– is contraction operator. \n \n",
  "categories": [
    "cs.LG",
    "eess.IV",
    "q-bio.NC"
  ],
  "published": "2022-09-12",
  "updated": "2022-09-12"
}