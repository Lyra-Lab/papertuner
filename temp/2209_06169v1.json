{
  "id": "http://arxiv.org/abs/2209.06169v1",
  "title": "The Role of Explanatory Value in Natural Language Processing",
  "authors": [
    "Kees van Deemter"
  ],
  "abstract": "A key aim of science is explanation, yet the idea of explaining language\nphenomena has taken a backseat in mainstream Natural Language Processing (NLP)\nand many other areas of Artificial Intelligence. I argue that explanation of\nlinguistic behaviour should be a main goal of NLP, and that this is not the\nsame as making NLP models explainable. To illustrate these ideas, some recent\nmodels of human language production are compared with each other. I conclude by\nasking what it would mean for NLP research and institutional policies if our\ncommunity took explanatory value seriously, while heeding some possible\npitfalls.",
  "text": "arXiv:2209.06169v1  [cs.CL]  13 Sep 2022\nThe Role of Explanatory Value in Natural\nLanguage Processing\nKees van Deemter.\nUtrecht University\nA key aim of science is explanation, yet the idea of explaining language phenomena has taken a\nbackseat in mainstream Natural Language Processing (NLP) and many other areas of Artiﬁcial\nIntelligence. I argue that explanation of linguistic behaviour should be a main goal of NLP,\nand that this is not the same as making NLP models “explainable\". To illustrate these ideas,\nsome recent models of human language production are compared with each other. I conclude by\nasking what it would mean for NLP research and institutional policies if our community took\nexplanatory value seriously, while heeding some possible pitfalls.\n1. Introduction\nIn this short paper, I will argue that much recent work in Natural Language Process-\ning (NLP) has focussed too narrowly on the performance of its models, as measured\nby various intrinsic or extrinsic evaluations, neglecting some vitally important other\ndimensions of model quality, which I will collect loosely under the header \"explanatory\nvalue\".1 Before explaining what I mean, let me set the scene.\nBroadly, NLP can be pursued in three mutually overlapping ways, which empha-\nsize different aspects of our work. First, there is NLP as Engineering, where NLP models\nare built primarily to serve some practical goal, answering a need that exists in society.\nSecond, there is what might be called NLP-as-Mathematics, which studies algorithms\nand models in their own right, comparing them and developing new ones. Finally, there\nis NLP-as-Science, where models are constructed with the aim of expressing, testing,\nand ultimately enhancing humankind’s grasp of human language and language use,\nbecause computational models offer a level of explicitness and detail that other theories\nof language often lack. For example, a sentiment analysis model may be seen as a\nhighly explicit theory of the ways in which psychological states manifest themselves in\nlanguage; a summarisation model can be seen as a theory of what is most informative\nin a text; a Machine Translation model may be seen as a theory of translation.\nI will focus primarily on NLP-as-Science arguing that, if explanation is our aim,\nthen this should include several dimensions. In particular, if we rely solely on the\nperformance of our models, then we risk building models that are ad hoc, that are\nunwieldy, that are difﬁcult to link with existing insights, and that do not allow us to\nanswer counterfactual questions such as, \"How well would this model perform if we\napplied it to texts of a different genre?\"\nOne might argue that a lot of work in NLP focusses on explanation already, because\nit aims for explainability (e.g., (Ghassemi, Oakden-Rayner, and Beam 2021)). This ar-\ngument, however, confuses (1) explaining a natural phenomenon (e.g., an aspect of\nlanguage use) and (2) explaining a model (i.e., a piece of software). Explanation is about\n1 Although I am conscious that this use of the term “explanatory\" is broader than its daily usage, I will\nargue in section 2 that it makes sense to use the term in this way.\n© 2022 Association for Computational Linguistics\nComputational Linguistics\nVolume 1, Number 1\n\"What principles underlie this phenomenon?\", whereas explainability is about \"Why did\nthis model make these decisions?\" The difference is starkest when the model doesn’t\nmatch the phenomena very well. Suppose you had a model for classifying student\nessays into good (pass) and bad (fail). Suppose the model had terrible performance but\nexcellent explainability.2 The model would thus be highly explainable, and this could\nbe useful for a stakeholder who wonders whether to trust its decisions, or a developer\nwanting to improve it. Yet these explanations would not tell us what makes an essay\ngood or bad because (we assumed) the model does not know the difference. Similar\nthings can be said about the idea that model evaluation should include a systematic\nanalysis of errors (e.g., (McKeown 2020); (Ribeiro et al. 2020)). The importance of ﬁnding\nout what a model gets right and wrong can hardly be overstated but, by itself, it can only\nshed light on the model, not the language phenomena we are seeking to understand.\n2. Dimensions of Explanatory Value\nExplanation plays a key role throughout the sciences (Popper 1934; Overton 2013;\nWoodward and Ross 2021) and in daily life (Lombrozo 2006). Accordingly, many dis-\nciplines have seen lively discussion of what it means for a theory or model to offer\na good explanation of data, but in recent NLP there has been little discussion about\nsuch matters. Consequently, it is unclear what makes a good explanation, and whether\nexplanation even matters in NLP. Based on a liberal borrowing from other disciplines, I\nwill discuss what different dimensions explanation can involve, what these might mean\nfor assessing NLP models, and what the implications would be if our community took\nthem seriously.\nPerformance. One dimension of explanatory value is performance, which includes met-\nrics such as Precision, Recall, DICE, BLEU, Moverscore (Sai, Mohankumar, and Khapra\n2022), (Celikyilmaz, Clark, and Gao 2020), which allow researchers to compare a\nmodel’s predictions with a gold standard. Evaluations based on human judgments\n(e.g., (van der Lee et al. 2019)) or task-based evaluations are varieties of performance\nlikewise. Performance is naturally thought of as a component of explanatory value. For\ninstance, if model A has better performance than B, then other things being equal, A has\ngreater explanatory value than B. If a model does not allow performance to be assessed\nat all, then I will consider such models to have low performance.\nA natural complement to performance is a group of factors I will call “support\". Simply\nput, when a model is tested in an experiment, support is any evidence for the model\nother than the evidence from that particular experiment. I will distinguish between\n(what I will call) direct and indirect support.\nDirect support. Direct support assesses a model’s ability to make accurate predictions\nfor unseen data, and to generalize to related tasks and different contexts. It comes from\na plurality of broadly similar experiments.3 Suppose a caption generation model4 was\ntested on a set of holiday snaps, showing good performance. Direct support could\n2 For instance, via computer-generated “rationales\" that highlight text fragments that were particularly\nimportant for each classiﬁcation decision, e.g. Lei, Barzilay, and Jaakola (2016).\n3 This includes repetitions, reproductions, and replications, using the terminology of the ACM at\nwww.acm.org/publications/policies/artifact-review-and-badging-current .\n4 Caption generation models are Vision and Language models that generate textual captions for visual\nimages (Hodosh, Young, and Hockenmaier 2013; Agrawal et al. 2019).\n2\nKees van Deemter\nExplanatory Value in NLP\ninclude an experiment on another collection of holiday snaps, an experiment with a\ndifferent type of images, or a probe that investigates whether the success of the model is\ndue to some accidental feature of the dataset. Similarly, if a Machine Translation model\nis tested on a new language pair, then this can be seen as offering direct support to the\nmodel. Direct support views language corpora as data samples that are only of interest\nto the extent that they are representative of a wider population of data. This implies\nthat we should ask ourselves what type of language use we want our corpora to be\nrepresentative of, and be on our guard against confounding variables (i.e., accidental\ncircumstances that may have affected our results Ribeiro, Singh, and Guestrin (2016)).\nIndirect support. A dominant theme in the Philosophy of Science is that explana-\ntion should involve a reduction of the unknown (e.g., some previously unobserved\nfacts) to something already known, such as an existing law or insight or model\n(Hempel and Oppenheim 1965). These ideas lie at the heart of the scientiﬁc enterprise\n(see e.g. Hepburn and Andersen (2021)), understood as “the attempt to understand the\nworld around us\" (Levesque (2014), speaking about Artiﬁcial Intelligence).\nFor example, suppose a study ﬁnds that a medicine X damages patients’ liver.\nSuppose we also know X contains a molecule Y which is known to be toxic, then this\nexisting insight offers indirect support to the ﬁnding about side effects. Likewise, if\nan aspect of language use can be shown to enhance or speed up communication, this\ncan lend additional credence to a model that incorporates this aspect. Support can also\nbe negative. For example, if a physics model explains certain observations by positing\n“action at a distance\", (i.e., where an object can be affected by another object without\nbeing “touched\" by it in any way) then the difﬁculty of making sense of that idea has\nbeen seen as diminishing the value of the model (Berkovitz 2008).\nIndirect support is essential to what is called explanation in everyday parlance, and\ncan even give us a sense that we “understand\" the underlying mechanism (for instance\nwhen we know that the above molecule Y kills a particular liver enzyme), but the\nidea of mechanism has been notoriously difﬁcult to underpin (Craver and Tabery 2019).\nIndirect support in NLP can sometimes take the form of cognitive plausibility (although\nit does not have to, see e.g. Harnad (1989)). Suppose, for example, a text comprehension\nmodel uses an algorithm that is NP-complete, then this makes it implausible as a model\nof human behaviour because it suggests a brain mechanism that would be so time\nconsuming that it could not work in practice.\nWhen indirect support I is invoked, two key questions are in order: How certain are\nwe about I (e.g., what is the evidence or reasoning underlying I?), and To what extent\nwould I (if correct) support the model? Most scientists accept that higher principles\nhave a role to play, though an assessments of whether a principle is rightfully invoked\ncan be a matter of vigorous debate. For instance, action-at-a-distance has become an\naccepted part of physics despite being seen as implausible for a long time.\nParsimony. Parsimony is the idea that a simpler model is a better model. Parsimony\nis closely related to Ockham’s Razor and to the idea that models should be as elegant\nas possible (see e.g. Greene (2000)), which counts philosophers and physicists such a\nKarl Popper and Paul Dirac among its early proponents. Parsimony is an aspect of\nexplanatory value because if we do not insist on some form of parsimony, a model could\nbe deemed to be highly explanatory even if it was nothing more than a large collection\nof isolated facts or rules without any attempt at generalisation.\nRegarding the question of how scientiﬁc elegance should deﬁned, there are different\nviews. In particular, parsimony can concern different aspects of the model (see e.g.\n3\nComputational Linguistics\nVolume 1, Number 1\nFitzpatrick (2022), section 3). Accounts of parsimony that focus on the number of\npostulates employed by the model, for example, have been defended on the grounds\nthat more parsimonious models have a greater probability of being true. A type of\nparsimony more relevant to NLP says that, even if two models cannot (or: not very\nclearly) be distinguished in terms of their performance, then if one is simpler than the\nother, the simpler model should be preferred. This type of parsimony is routinely used,\nand sometimes defended explicitly, by syntacticians (Brody 1995; Akmajian and Heny\n1975), among others.5\nInvoking parsimony can be risky, particularly if a complex model is dismissed that\nhas better performance than its competitors. Physicists such as Sabine Hossenfelder\nbelieve elegance has played too large a role in discussions of string theory (Hossenfelder\n2018). Nonetheless, the idea that a lack of parsimony can diminish the value of a theory\nis widely accepted. We will count it as a fourth dimension of explanatory value.\nA Bayesian perspective on the progression of science (e.g. (Jaynes 2003)) may help to\nclarify these dimensions. Let D be the data obtained when a model M is tested, and X is\neverything else we know, including indirect support for or against M. Then performance\nof a probabilistic model M can be seen as P(D|M,X), the probability D would have\nif M were true. What one is typically interested in is P(M|D, X), the probability of\nM given D and X. Direct support is Bayesian update, where more and more data sets\nD1, .., Dn are brought to bear, yielding P(M|D1, .., Dn, X). Even parsimony can, at least\nin principle, be captured along Bayesian lines, by using Solomonoff’s Prior (Solomonoff\n1964; Hutter, Legg, and Vitanyi 2007), which assesses the complexity of a model by\nmeasuring its a priori probability (i.e., the probability of the model before any data are\nconsidered).\n3. Case study: Two types of Referring Expressions Generation\nTo illustrate both the usefulness and the pitfalls of assessing the explanatory value of\nNLP models, I examine two types of referring expressions generation (REG). I choose\nREG because referring is an essential part of human communication that has been\nstudied from many different angles, using very different types of models; moreover,\nthe performance of REG models has been tested extensively, and the outcomes of these\ntests will inform our discussion of the explanatory value of these models.\n3.1 Generating one-shot Referring Expressions\nOne-shot REG has been much studied in NLP ((Dale 1989), (Dale and Reiter 1995),\n(Krahmer and van Deemter 2012), (Yu et al. 2016), (Luo and Shakhnarovich 2017)): the\nresearch question is, given a “scene\" composed of objects, and without any linguistic\ncontext (hence “one-shot\" REG), what properties do human speakers select when they\nrefer to one of the objects in the scene? The patterns observed here are far from trivial,\nand sometimes counter-intuitive (van Deemter 2016). Here I concentrate on a class of\nmodels that emerged from controlled experiments involving simple artiﬁcial scenes\nwhose objects have well-understood properties (shape, colour, size, etc.) that can be\n5 The opening chapter of Akmajian and Heny (1975) uses this example: X liked you is assigned the\nunderlying form X did like you, because this allows one to generate tag questions (X liked you, didn’t he?),\nnegated sentences (X did not like you) and emphatic sentences (X did like you) using one and the same\nmechanism, thereby minimizing the complexity of the grammar.\n4\nKees van Deemter\nExplanatory Value in NLP\nmanipulated precisely by the experimenter and presented to participants on a computer\nscreen. Such experiments trade away some of the complexity of real-world scenes to\nallow a maximum of experimental control.\nWe compare ﬁve models. One model is an application (which I will call RSA-\nREG) of Frank and Goodman’s Rational Speech Act (RSA) model (Frank and Goodman\n2016), (Frank and Goodman 2012).6 RSA is formalisation of the Gricean idea that com-\nmunication is always optimally cooperative; consequently, RSA-REG’s speaker model\nemphasizes discriminatory power: the likelihood that a property is chosen for inclusion\nin a Referring Expression (RE) is proportional to its discriminatory power (i.e., the\nproportion of scene objects to which the property does not apply).\nThe other models grew out of a “Bounded Rationality\" research tradition that\nemphasises the idea that is skeptical about the idea that speakers routinely compute\ndiscriminatory power when they refer. A well-known version of this experimentally\nwell-supported idea (e.g. (Belke and Meyer 2002)) is the Incremental Algorithm of\nDale and Reiter (1995), which assumes that properties are arranged in a linear se-\nquence that lists them according to the degree to which they are preferred. A range\nof experimental ﬁndings Koolen et al. (2011); Gatt et al. (2013); van Deemter (2016);\nVan Gompel et al. (2019)) led to various improvements, including two probabilistic\nversions of the Incremental Algorithm, and our own model called Probabilistic Over-\nspeciﬁcation (PRO), which combines discriminatory power with a probabilistic use of\npreference.\nComparison 1: Performance. In (Van Gompel et al. 2019) we reported an experiment\nin which the PRO model outperformed the other algorithms in terms of the human-\nlikeness of their output.\nComparison 2: Direct support. Algorithms in the Bounded Rationality tradition have\noften been tested, including the evaluation campaigns of Gatt and Belz (2010). Direct\nsupport for RSA-REG does not yet reach the level of the other models; naturally, support\nfor these models may grow over time.\nComparison 3: Indirect support. At ﬁrst sight, there is much indirect support for\nRSA, given the intuitive appeal of describing human behaviour as rational. However,\na wealth of work in behavioural economics has shown that rational behaviour is af-\nfected by time and memory limitations, necessitating shortcuts (Elster 1983; Simon 1991;\nGigerenzer and Selten 2002; Gershman, Horvitz, and Tenenbaum 2015), and other devi-\nations from rational behaviour (Kahneman and Tversky 2013). Experiments on REG are\nin line with these ﬁndings (van Deemter 2016; Van Gompel et al. 2019). For example,\nPRO is full of shortcuts that avoid the arduous computation of the discriminatory power\nof each property that would be required by the RSA-REG algorithm.\nComparison 4: Parsimony. The computational core of RSA-REG can be written in\njust two simple equations; by contrast, PRO is a rule-based algorithm whose pseudo-\ncode needs about a page. It seems reasonable, therefore, to say RSA-REG is more\nparsimonious than PRO.\n6 The mechanisms of (Degen et al. 2020) could probably lend RSA-REG better performance\n(Rubio-Fernandez 2021), but until a systematic performance assessment of the resulting model is\navailable, Frank and Goodman’s model will serve our illustrative purposes.\n5\nComputational Linguistics\nVolume 1, Number 1\n3.2 Generating Referring Expressions in Context\nREG-in-Context is another well-studied area of REG. It focusses on co-reference in\ndiscourse. It often starts from a text in which all referring expressions (REs) have been\nblanked out; it predicts, for each of these blanks, what RE should ﬁll it. Other than the\nidentity of the referent, the main information for the model to consider is the sentences\naround the RE, because this guides the choice between pronouns, proper names, and\ndescriptions. The other entities mentioned in the text play a role not dissimilar to the\n“distractor\" objects displayed on a computer screen in One-shot REG (previous section).\nA long tradition of linguistic research has led to theories such as accessibility the-\nory (Ariel 1990), the givenness hierarchy (Gundel, Hedberg, and Zacharski 1993), and\nCentering Theory (Brennan 1995). These theories emphasise the effect of the recency\nof the antecedent (e.g. in terms of the number of intervening words), its animacy\n(animate/non-animate), and the syntactic structure of the sentences (e.g., Does the RE\noccur in the same syntactic position as the antecedent?) Computational accounts can be\nclassiﬁed in terms of whether they use (1) handwritten rules, (2) hand-coded features\nand Machine Learning, or (3) an End2End neural architecture.\nFollowing the GREC evaluation campaign (Belz et al. 2009), in which a number of ML\nmodels of REG-in-Context were tested, a wider range of models were recently com-\npared in terms of their performance, looking at human judgments and computational\nmetrics (Same, Chen, and Van Deemter 2022). Models included (1) two rule-based ones,\nRREG-S (small) and RREG-L (small); (2) two models based on traditional Machine\nLearning (ML), called ML-S (small) and ML-L (large); and (3) three neural models,\nincluding two from Cunha et al. (2020) and one from Cao and Cheung (2019).\nComparison 1 and 2: Performance and direct support. Having observed that neu-\nral models had only been tested on Ferreira et al. (2018)’s version of WebNLG,\nSame, Chen, and Van Deemter (2022) decided to test all models on WSJ, the Wall Street\nJournal portion of the OntoNotes corpus (Gardent et al. 2017), arguing that WSJ would\npose a better test for REG-in-Context algorithms because the texts in it are longer than\nthose in WebNLG. With respect to WSJ, ML-L outperformed all other models; the\nsimplest rule-based baseline RREG-S performed less well yet it performed at least as\nwell as the neural models on both corpora.\nComparison 3: Indirect support. Indirect support varied widely across models, with\nthe larger models receiving the most support from the linguistics literature. RREG-L,\nfor instance, rests on notions such as local focus (Brennan 1995) and syntactic paral-\nlelism (Henschel, Cheng, and Poesio 2000); the large ML model ML-L makes use of the\ngrammatical role of the RE.\nThe question of Indirect support for neural models is debatable (see be-\nlow). Unless these models are combined with probing (for the case of REG, see\nChen, Same, and van Deemter (2021)) or other add-ons, it is difﬁcult to link these mod-\nels with linguistic insights.7 On the other hand, neural models may be more inherently\ncognitively plausible than models based on rules or on classical Machine Learning,\nbecause they are inspired by our knowledge of the human brain. Rather than either\nblithely rejecting or accepting this argument, this is one of those “higher principles\"\n(section 2) that we should take seriously, while also rigourously investigating their\nvalidity (c.f., (Ritter et al. 2017)).\n7 For the challenge of linking neural models with domain insights, see Kambhampati (2021).\n6\nKees van Deemter\nExplanatory Value in NLP\nComparison\n4:\nParsimony.\nAlthough\nparsimony\ncan\nbe\ndifﬁcult\nto\nassess,\nsome relevant comparisons are straightforward in this case. As observed in\nSame, Chen, and Van Deemter (2022), the two rule-based models only have the current\nand previous sentence available to them; the two ML-based models look at the current\nand all previous sentences; the three neural models have the entire text available to\nthem. The two “large\" models, RREGL and ML-L, contain more rules/features than\ntheir smaller counterpart and are consequently less parsimonious. The three mod-\nels above were architecturally similar seq-2-seq models with attention in the style of\nBahdanau, Cho, and Bengio (2014), which did not display any obvious differences in\nterms of parsimony.\n4. Challenges in assessing the explanatory value of a model\nOur case studies illustrate how a model may be superior in one respect but inferior\nin others. And although our ﬁrst case study suggested a trade-off between parsimony\nand performance, in which researchers could “buy\" an improvement in performance by\nsacriﬁcing parsimony, the second case study suggests that this is not always the case.\nOn\nthe\nother\nhand,\nsome\nchallenges\nhave\ncome\nto\nthe\nfore\nas\nwell,\nwhich\nI\nwill\nbrieﬂy\ndiscuss\nhere.\nI\nleave\nchallenges\nsurrounding\nper-\nformance\naside\nhere,\nbecause\nthey\nhave\nbeen\nwidely\ndiscussed\n(e.g.,\nCelikyilmaz, Clark, and Gao\n(2020);\nSai, Mohankumar, and Khapra\n(2022);\nReiter\n(2018); Alva-Manchego, Scarton, and Specia (2021)) about metrics; van der Lee et al.\n(2019) about human evaluation).\nDirect support. When judging the direct support for a model, younger models\ntend to be harder to judge, because a younger model cannot be expected to have\nbeen subjected to as much scrutiny as an older one, limiting its opportunities for both\nnegative and positive support. Where very different results are reported on different\ncorpora (such as the WebNLG and WSJ corpus in Same, Chen, and Van Deemter (2022)),\nfurther research into the causes of the divergence are in order.\nA further wrinkle in assessing direct support is that models are moving targets:\nwhen a model is examined for the second or third time, it is often a modiﬁed version\nof that initial model. What is really being assessed in such cases is not one model but\na class of models or, to put it differently, the ideas underlying these models (e.g., that\nrecency and animacy are factors in deciding between the type of RE.\nIndirect support. Our discussion of rationality put a spotlight on the two “key\nquestions\" that govern indirect support (section 2). For if the relevant principle I, as\ninvoked in support of a model, is the idea that behaviour is rational, then some evidence\nmay be available for I; but, as it stands, I is too vague to offer strong support for the\ndetails of the model, because it does not follow from I that the discriminatory power of\nthe properties in an RE needs to be maximised. Similarly, existing experimental insights\ndo not by themselves dictate all the speciﬁcs of the PRO model.\nParsimony. The idea of parsimony is already well established in NLP practices such\nas induction of “causal\" models (Geiger et al. 2021), knowledge distillation (Sanh et al.\n2019), and pruning, where the idea is to get rid of parameters or layers that do not\nadd to a model’s performance (Tessier 2021). Current practices in NLP do not typically\ninvolve systematic comparisons between models in terms of their parsimony, and per-\nforming such comparisons rigourously is far from trivial. Theoretical equipment for\ndoing so is available in principle, however. Deterministic models, for instance, may\nbe compared with each other in terms of their Kolmogorov complexity (Solomonoff\n1960; Kolmogorov 1965); non-deterministic models may be compared in terms of their\n7\nComputational Linguistics\nVolume 1, Number 1\nMinimal Description Length (Solomonoff 1964; Gruenwald 2007; Voita and Titov 2020).\nComparisons across different types of models seem more problematic; a complicating\nfactor is that whereas traditional models tend to address one NLP task, neural “founda-\ntion\" models such as BERT are adaptable to a wide variety of tasks, which would tend\na make a direct comparison across the two types of model biased against foundation\nmodels.\n5. Policy Implications\nRather than shying away from them, I believe that our community should embrace\nthe research challenges entailed by an increased emphasis on explanatory value, and\nthe debates that this will bring, including debates about alternative dimensions of\nexplanatory value.\nBased on my reasoning in the previous sections, I think it would be wrong to limit\nevaluation of models to only one aspect of their quality. There are parallels here with the\nassessment of people, where the inﬂuential DORA declaration suggests that the academic\ncommunity should reduce its reliance on quantitative metrics.8 Just as academics can\nhave different talents, the success of a model has different dimensions. In both cases, we\nshould learn to juggle multiple dimensions and say things like, “Based on this experiment,\nmodel A has better performance than the older model B. Being relatively new, A still has lower\nlevels of (positive and negative) direct support. However, A is more parsimonious and appears to\nhave better indirect support than B.\"\nPolicy-wise, researchers and reviewers should be encouraged to think about the\nexplanatory value of models. Analogous to the ethics and limitations paragraphs that\nare now solicited by some NLP conferences,9 our community could encourage authors\nof conference papers to comment on all dimensions of explanatory value. Similar moves\ncould be made by institutions that offer funding for scientiﬁc research: analogous to\nletting proposers discuss societal and economic impact, they could be asked to discuss\nparsimony, and both kinds of support, as well. Alternatively, reviewers could be urged\nto check these dimensions, similar to when reviewers are expected to look out for anal-\nyses of statistical signiﬁcance wherever these are appropriate to the work submitted.\n6. Conclusion\nIt is widely accepted that performance alone does not make a good model, because\nconstructing and training models may require a lot of effort; because of concerns over\nenergy consumption; and because of concerns over linguistic, ethnic and other biases\n(Bender et al. (2021)). Likewise, novelty and applicability of a model can be important.\nIn this Squib, I have argued that another set of dimensions is of crucial importance,\nparticularly when NLP arises from a scientiﬁc interest in the world around us; these\ndimensions, variants of which have often been discussed in connection with other sci-\nences, attempt to make explicit what it means for a model to explain data. Furthermore,\nI have argued that explanatory value means more than only performance, and that\nexplanatory value does not equal explainability.\n8 The San Francisco Declaration On Research Assessment can be found on https://sfdora.org/read/.\n9 For ethics paragraphs see https://2021.aclweb.org/ethics/Ethics-FAQ/ For limitations paragraphs,\nhttps://aclweb.org/portal/content/empirical-methods-natural-language-processing-emnlp-2022.\n8\nKees van Deemter\nExplanatory Value in NLP\nThe borderlines of our discussion are debatable. For example, it can be argued that\nsimilar arguments apply to NLP-as-Engineering as well. After all, unwieldy models are\ndifﬁcult to maintain and update; models that lack support may fail to generalize, and\nrisk having to be completely redesigned whenever customers’ requirements change.\nFurthermore, trends in NLP research tend to reﬂect wider tendencies; accordingly,\nexplanation is taking a backseat in other areas of Artiﬁcial Intelligence as well (Levesque\n2014; Kambhampati 2021). My conjecture is that the same dimensions of explanatory\nvalue, and similar implications for research method and policy, apply there as well.\n9\nComputational Linguistics\nVolume 1, Number 1\n7. Citations\nReferences\nAgrawal, Harsh, Karan Desai, Yufei Wang,\nXinlei Chen, Rishabh Jain, Mark Johnson,\nDhruv Batra, Devi Parikh, Stefan Lee, and\nPeter Anderson. 2019. Nocaps: Novel\nobject captioning at scale. In Proceedings of\nthe IEEE/CVF International Conference on\nComputer Vision, pages 8948–8957.\nAkmajian, Adrian and Frank Heny. 1975.\nIntroduction to the Principles of\nTransformational Syntax. MIT Press,\nCambridge, Mass.\nAlva-Manchego, F., C. Scarton, and L. Specia.\n2021. The (un) suitability of automatic\nevaluation metrics for text simpliﬁcation.\nComputational Linguistics, 47(4):861–889.\nAriel, Mira. 1990. Accessing Noun-Phrase\nAntecedents. Routledge.\nBahdanau, Dzmitry, Kyunghyun Cho, and\nYoshua Bengio. 2014. Neural machine\ntranslation by jointly learning to align and\ntranslate.\nBelke, Eva and Antje S. Meyer. 2002.\nTracking the time course of\nmultidimensional stimulus discrimination:\nAnalyses of viewing patterns and\nprocessing times during \"same\"-\"different\"\ndecisions. European Journal of Cognitive\nPsychology, 14(2):237–266.\nBelz, Anja, Eric Kow, Jette Viethen, and\nAlbert Gatt. 2009. Generating referring\nexpressions in context: The grec task\nevaluation challenges. In Proceedings of\nENLG 2009, pages 294–327, Association for\nComputational Linguistics.\nBender, Emily M., Timnit Gebru, Angelina\nMcMillan-Major, and Shmargaret\nShmitchell. 2021. On the dangers of\nstochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and\nTransparency, FAccT ’21, page 610–623,\nAssociation for Computing Machinery,\nNew York, NY, USA.\nBerkovitz, Joseph. 2008. Action at a Distance\nin Quantum Mechanics. In Edward N.\nZalta, editor, The Stanford Encyclopedia of\nPhilosophy, Winter 2008 edition.\nMetaphysics Research Lab, Stanford\nUniversity.\nBrennan, Susan E. 1995. Centering attention\nin discourse. Language and Cognitive\nprocesses, 10(2):137–167.\nBrody, Michael. 1995. Lexico-logical form. MIT\nPress, Cambridge, Mass.\nCao, Meng and Jackie Chi Kit Cheung. 2019.\nReferring expression generation using\nentity proﬁles. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages\n3163–3172, Association for Computational\nLinguistics, Hong Kong, China.\nCelikyilmaz, Asli, Elizabeth Clark, and\nJianfeng Gao. 2020. Evaluation of text\ngeneration: A survey. CoRR,\nabs/2006.14799.\nChen, Guanyi, Fahime Same, and Kees van\nDeemter. 2021. What can neural referential\nform selectors learn? In Proceedings of the\n14th International Conference on Natural\nLanguage Generation, pages 154–166,\nAssociation for Computational\nLinguistics, Aberdeen, Scotland, UK.\nCraver, Carl and James Tabery. 2019.\nMechanisms in Science. In Edward N.\nZalta, editor, The Stanford Encyclopedia of\nPhilosophy, Summer 2019 edition.\nMetaphysics Research Lab, Stanford\nUniversity.\nCunha, Rossana, Thiago Ferreira, Adriana\nPagano, and Fabio Alves. 2020. Referring\nto what you know and do not know:\nMaking referring expression generation\nmodels generalize to unseen entities.\npages 2261–2272.\nDale, Robert. 1989. Cooking up referring\nexpressions. In Proceedings of the 27th\nannual meeting on Association for\nComputational Linguistics (ACL’89), pages\n68–75, Association for Computational\nLinguistics, Vancouver, BC.\nDale, Robert and Ehud Reiter. 1995.\nComputational interpretations of the\ngricean maxims in the generation of\nreferring expressions. Cognitive Science,\n19(2):233–263.\nvan Deemter, Kees. 2016. Computational\nModels of Referring: A study in cognitive\nscience. MIT Press, Cambridge, MA.\nDegen, Judith, Robert D Hawkins, Caroline\nGraf, Elisa Kreiss, and Noah D Goodman.\n2020. When redundancy is useful: A\nbayesian approach to “overinformative”\nreferring expressions. Psychological Review,\n127(4):591.\nElster, Jon. 1983. Sour Grapes: studies in the\nsubversion of rationality. MIT Press,\nCambridge, MA.\n10\nKees van Deemter\nExplanatory Value in NLP\nFerreira, Thiago Castro, Diego Moussallem,\nEmiel Krahmer, and Sander Wubben.\n2018. Enriching the webnlg corpus. In\nProceedings of the 11th International\nConference on Natural Language Generation,\npage 171–176, Association for\nComputational Linguistics, Tilburg\nUniversity.\nFitzpatrick, Simon. 2022. Simplicity in the\nphilosophy of science. In Internet\nEncyclopaedia of Philosophy, ISSN 2161-0002.\nFrank, Michael C. and Noah Goodman. 2016.\nPragmatic Language Interpretation as\nProbabilistic Inference. Trends in Cognitive\nSciences, 20(11):818–829.\nFrank, Michael C and Noah D Goodman.\n2012. Predicting pragmatic reasoning in\nlanguage games. Science (New York, N.Y.),\n336(6084):998.\nGardent, Claire, Anastasia Shimorina, Shashi\nNarayan, and Laura Perez-Beltrachini.\n2017. Creating training corpora for nlg\nmicro-planners. In Proceedings of the 55th\nAnnual Meeting of the Association for\nComputational Linguistics, page 179–188,\nAssociation for Computational\nLinguistics, Vancouver.\nGatt, A, E. Krahmer, R.P.G. van Gompel, and\nK. van Deemter. 2013. Factors causing\noverspeciﬁcation in deﬁnite descriptions.\nIn Proceedings of the 35th Annual Meeting of\nthe Cognitive Science Society.\nGatt, Albert and Anya Belz. 2010.\nIntroducing Shared Tasks to NLG: the\nTUNA Shared Task Evaluation\nChallenges. In Emiel Krahmer and Mariet\nTheune, editors, Empirical Methods in\nNatural Language Generation. Springer.\nGeiger, Atticus, Hanson Lu, Thomas F Icard,\nand Christopher Potts. 2021. Causal\nabstractions of neural networks. In\nAdvances in Neural Information Processing\nSystems.\nGershman, S.J., E.J. Horvitz, and J.B.\nTenenbaum. 2015. Computational\nrationality: A converging paradigm for\nintelligence in brains, minds, and\nmachines. Science, 49:273–278.\nGhassemi, Marzyeh, Luke Oakden-Rayner,\nand Andrew L. Beam. 2021. The false hope\nof current approaches to explainable\nartiﬁcial intelligence in health care. Lancet\nDigit Health, 3:745–750.\nGigerenzer, Gerd and Reinhard Selten. 2002.\nBounded Rationality. MIT Press,\nCambridge, MA.\nVan Gompel, Roger PG, Kees van Deemter,\nAlbert Gatt, Rick Snoeren, and Emiel J\nKrahmer. 2019. Conceptualization in\nreference production: Probabilistic\nmodeling and experimental testing.\nPsychological review, 126(3):345.\nGreene, B. 2000. The elegant universe:\nSuperstrings, hidden dimensions, and the\nquest for the ultimate theory. American\nJournal of Physics, 68(2):199–200.\nGruenwald, Peter. 2007. The Minimum\nDescription Length Principle. MIT Press.\nGundel, Jeanette K, Nancy Hedberg, and\nRon Zacharski. 1993. Cognitive status and\nthe form of referring expressions in\ndiscourse. Language, pages 274–307.\nHarnad, Stevan. 1989. Minds, machines and\nsearle. Journal of Theoretical and\nExperimental Artiﬁcial Intelligence, (1):5–25.\nHempel, Carl G. and Paul Oppenheim. 1965.\nStudies in the logic of explanation.\nPhilosophy of Science, 15(2):135–175.\nHenschel, Renate, Hua Cheng, and Massimo\nPoesio. 2000. Pronominalization revisited.\nIn Proceedings of the 18th conference on\nComputational linguistics-Volume 1, pages\n306–312, Association for Computational\nLinguistics.\nHepburn, Brian and Hanne Andersen. 2021.\nScientiﬁc Method. In Edward N. Zalta,\neditor, The Stanford Encyclopedia of\nPhilosophy, Summer 2021 edition.\nMetaphysics Research Lab, Stanford\nUniversity.\nHodosh, Micah, Peter Young, and Julia\nHockenmaier. 2013. Framing image\ndescription as a ranking task: Data,\nmodels and evaluation metrics. Journal of\nArtiﬁcial Intelligence Research, 47:853–899.\nHossenfelder, Sabine. 2018. Lost in Math; How\nBeauty Leads Physics Astray. Basic Books,\nNew York.\nHutter, M., S. Legg, and P. M.B. Vitanyi. 2007.\nAlgorithmic probability. Scholarpedia,\n2(8):2572. Revision #151509.\nJaynes, E.T. 2003. Probability Theory: The Logic\nof Science. Cambridge University Press,\nCambridge, UK.\nKahneman, D. and A. Tversky. 2013.\nProspect theory: An analysis of decision\nunder risk. In Handbook of the fundamentals\nof ﬁnancial decision making.\nKambhampati, Subbharao. 2021. Polanyi’s\nrevenge and ai’s new romance with tacit\nknowledge. Communications of the ACM,\n64(2):31–32.\nKolmogorov, A.N. 1965. Three approaches to\nthe quantitative deﬁnition of information.\nProblems Inform. Transmission, 1(1):1–7.\nKoolen, Ruud, Albert Gatt, Goudbeek, and\nEmiel Krahmer. 2011. Factors causing\noverspeciﬁcation in deﬁnite descriptions.\n11\nComputational Linguistics\nVolume 1, Number 1\nJournal of Pragmatics, 43(13):3231–3250.\nKrahmer, Emiel and Kees van Deemter. 2012.\nComputational generation of referring\nexpressions: A survey. Computational\nLinguistics, 38(1):173–218.\nvan der Lee, Chris, Albert Gatt, Emiel van\nMiltenburg, Sander Wubben, and Emiel\nKrahmer. 2019. Best practices for the\nhuman evaluation of automatically\ngenerated text. In Proceedings of the 12th\nInternational Conference on Natural Language\nGeneration, pages 355–368, Association for\nComputational Linguistics, Tokyo, Japan.\nLei, Tao, Regina Barzilay, and Tommi\nJaakola. 2016. Rationalizing neural\npredictions. In Proceedings of the 2016\nConference on Empirical Methods in Natural\nLanguage Processing, pages 107–117.\nLevesque, H.J. 2014. On our best behaviour.\nArtiﬁcial Intelligence, 212:27–35.\nLombrozo, Tania. 2006. The structure and\nfunction of explanations. Trends in\nCognitive Sciences, 10(10):464–470.\nLuo, Ruotian and Gregory Shakhnarovich.\n2017. Comprehension-guided referring\nexpressions. In Proceedings of the IEEE\nConference on Computer Vision and Pattern\nRecognition (CVPR).\nMcKeown, Kathleen. 2020. Rewriting the\npast: Assessing the ﬁeld through the lens\nof language generation. (keynote). In\nProceedings of the 7th European Workshop on\nNatural Language Generation.\nOverton, James A. 2013. “explain” in\nscientiﬁc discourse. Synthese,\n(190):1383–1405.\nPopper, Karl. 1934. Logik der Forschung,\nTranslated as “The Logic of Scientiﬁc\nDiscovery\". Hutchinson, London, 1959.\nReiter, Ehud. 2018. A structured review of\nthe validity of bleu. Computational\nLinguistics, 44(3):393–401.\nRibeiro, Marco Tulio, Tongshuang Wu,\nCarlos Guestrin, and Sameer Singh. 2020.\nBeyond accuracy: Behavioral testing of nlp\nmodels with checklist. In Proceedings of\n(ACL 2020).\nRibeiro, M.T., S. Singh, and C. Guestrin. 2016.\nWhy should i trust you? explaining the\npredictions of any classiﬁer. In Proceedings\nof the 22th ACM SSIGKDD Int. Conf. on\nKnowledge Discovery and Data Mining.\nRitter, Samuel, David G. T. Barrett, Adam\nSantoro, and Matt M. Botvinick. 2017.\nCognitive psychology for deep neural\nnetworks: A shape bias case study.\nRubio-Fernandez, Paula. 2021. Color\ndiscriminability makes over-speciﬁcation\nefﬁcient: Theoretical analysis and\nempirical evidence. Humanities and Social\nSciences Communications, 8(1):1–15.\nSai, Ananya B., Akash Kumar Mohankumar,\nand Mitesh M. Khapra. 2022. A survey of\nevaluation metrics used for nlg systems.\nACM Comput. Surv., 55(2).\nSame, Fahime, Guanyi Chen, and Kees\nVan Deemter. 2022. Non-neural models\nmatter: A re-evaluation of neural referring\nexpression generation systems. In\nProceedings of ACL 2022, Association for\nComputational Linguistics.\nSanh, Victor, Lysandre Debut, Julien\nChaumond, and Thomas Wolf. 2019.\nDistilbert, a distilled version of BERT:\nsmaller, faster, cheaper and lighter. CoRR,\nabs/1910.01108.\nSimon, Herbert. 1991. Bounded rationality\nand organizational learning.\nOrganisational Science, 2.\nSolomonoff, Ray J. 1964. A formal theory of\ninductive inference: part i. Information and\nControl, 7(1):1–22.\nSolomonoff, R.J. 1960. A preliminary report on\na general theory of inductive inference.\nTechnical Report ZTB-138, Zator\nCompany, Cambridge, Mass.\nTessier, Hugo. 2021. Neural network pruning\n101, https://towardsdatascience.com/\nneural-network-pruning-101-af816aaea61.\nVoita, Elena and Ivan Titov. 2020.\nInformation-theoretic probing with\nminimum description length.\nWoodward, James and Lauren Ross. 2021.\nScientiﬁc explanation. In Edward N. Zalta,\neditor, The Stanford Encyclopedia of\nPhilosophy. Springer, pages 264–293.\nYu, Licheng, Patrick Poirson, Shan Yang,\nAlexander C. Berg, and Tamara L. Berg.\n2016. Modeling context in referring\nexpressions.\n12\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-09-13",
  "updated": "2022-09-13"
}