{
  "id": "http://arxiv.org/abs/2209.07053v1",
  "title": "Accuracy of the Uzbek stop words detection: a case study on \"School corpus\"",
  "authors": [
    "Khabibulla Madatov",
    "Shukurla Bekchanov",
    "Jernej Vičič"
  ],
  "abstract": "Stop words are very important for information retrieval and text analysis\ninvestigation tasks of natural language processing. Current work presents a\nmethod to evaluate the quality of a list of stop words aimed at automatically\ncreating techniques. Although the method proposed in this paper was tested on\nan automatically-generated list of stop words for the Uzbek language, it can\nbe, with some modifications, applied to similar languages either from the same\nfamily or the ones that have an agglutinative nature. Since the Uzbek language\nbelongs to the family of agglutinative languages, it can be explained that the\nautomatic detection of stop words in the language is a more complex process\nthan in inflected languages. Moreover, we integrated our previous work on stop\nwords detection in the example of the \"School corpus\" by investigating how to\nautomatically analyse the detection of stop words in Uzbek texts. This work is\ndevoted to answering whether there is a good way of evaluating available stop\nwords for Uzbek texts, or whether it is possible to determine what part of the\nUzbek sentence contains the majority of the stop words by studying the\nnumerical characteristics of the probability of unique words. The results show\nacceptable accuracy of the stop words lists.",
  "text": "Accuracy of the Uzbek stop words detection: a case study on \n“School corpus” \n \nKhabibulla Madatov 1, Shukurla Bekchanov1 and Jernej Vičič 2,3  \n \n1 Urgench state university, 14, Kh. Alimdjan str, Urgench city, 220100, Uzbekistan  \n2 Research Centre of the Slovenian Academy of Sciences and Arts, The Fran Ramovš Institute, Novi trg 2, 1000 \nLjubljana, Slovenija \n3 University of Primorska, FAMNIT, Glagoljaska 8, 6000 Koper, Slovenia   \n \n  \nAbstract  \nStop words are very important for information retrieval and text analysis investigation tasks of \nnatural language processing. Current work presents a method to evaluate the quality of a list \nof stop words aimed at automatically creating techniques. Although the method proposed in \nthis paper was tested on an automatically-generated list of stop words for the Uzbek language, \nit can be, with some modifications, applied to similar languages either from the same family \nor the ones that have an agglutinative nature. Since the Uzbek language belongs to the family \nof agglutinative languages, it can be explained that the automatic detection of stop words in \nthe language is a more complex process than in inflected languages. Moreover, we integrated \nour previous work on stop words detection in the example of the “School corpus” by \ninvestigating how to automatically analyse the detection of stop words in Uzbek texts. This \nwork is devoted to answering whether there is a good way of evaluating available stop words \nfor Uzbek texts, or whether it is possible to determine what part of the Uzbek sentence contains \nthe majority of the stop words by studying the numerical characteristics of the probability of \nunique words. The results show acceptable accuracy of the stop words lists.  \n \nKeywords  1 \nstop word detection, Uzbek language, accuracy, agglutinative language \n1. Introduction \nThe application of Natural Language Processing (NLP) tasks in real-life scenarios are getting more \nfrequent than ever before, and there is huge research getting involved with different approaches to \nenhance the quality of such tasks. An important aspect of many NLP tasks that make use of tasks, such \nas information retrieval, text summarization, context-embedding, etc., relies on a task of removing \nunimportant tokens and words from the context under focus. Such data are known as stop words. \nTherefore, it is desired that some automatic method should be developed to identify stop words that \neither make no change in the meaning of the context (or do very little) and remove them. from the \ncontext.  \nIn this work, we are addressing the problem of automatic detection of stop words for the low-\nresource agglutinative Uzbek language, and evaluate the proposed methods. The existing literature that \ndeal with stop words removal task for the Uzbek language [7] [8] [10] focus on the creation process, \nthe importance, as well as the availability of the proposed data, leaving a gap for further investigation, \nwhich we discuss in this paper.  \nThe scientific term \"stop words\" is popular in the field of natural language processing, and its \ndefinition we focus in this work is as follows: If the removal of those words from the text not only does \n                                                      \n1The International Conference and Workshop on Agglutinative Language Technologies as a challenge of Natural Language Processing \n(ALTNLP), June 7-8, 2022, Koper, Slovenia \nEMAIL: habi1972@mail.ru (A. 1);  shukurla15@gmail.com (A. 2); jernej.vicic@upr.si (A. 3)  \nORCID: 0000-0002-3664-4954 (A. 1); 0000-0001-9505-5781 (A. 2); 0000-0002-7876-5009 (A. 3) \n \n©️  2020 Copyright for this paper by its authors. \nUse permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).  \n \nCEUR Workshop Proceedings (CEUR-WS.org)  \n \nnot change the context meaning but also leaves the minimum number of words possible that can still \nhold the meaning of the context, then such words can be called stop words for this work. \nFor instance, the following examples are shown to better explain what words would be considered \nin given sentences, and what the final context would become after removing those stop words:  \n● “Men bu maqolani qiynalib yozdim”.  (I wrote this article with difficulty). After removing the \nstop words (“men”, “bu”, “qiynalib”) the context becomes: “Maqolani yozdim”.(I wrote the \narticle.); \n● “Har bir inson baxtli bo’lishga haqlidir” (Every person has the right to be happy). After \nremoving the stop words (“har ”, “bir”), the context becomes: “Inson baxtli bo’lishga \nhaqlidir” (Person has right to be happy). \n                       \n Such definition is an extension of the traditional definition of stop words by including more words \nthan the actual expectations but still including the traditional stop words.  \nThe Term Frequency - Inverse Document Frequency (TF-IDF) method [15] was used to detect stop \nwords in Uzbek texts. TF-IDF is a numerical statistic that is intended to reflect how important a word \nis to a document in a corpus, the method acknowledges words with the lowest TF-IDF values as less \nimportant to the semantic meaning of the document and proposes these words as stop word candidates. \nIn our previous work[8], we discuss the methods and algorithms for automatic detection and \nextraction of Uzbek stop words from previously collected text forming a new corpus called the “School \ncorpus”. The stop words detection method based on TF-IDF was applied to the aforementioned corpus \ncollected from 25 textbooks used for teaching at primary schools of Uzbekistan, consisting of 731,156 \nwords, of which 47,165 are unique words. To perform our technique, for each word from the set of \nunique words, its frequency was determined (the number of occurrences in the texts of the School \ncorpus), and the inverse document frequency IDF(word) = ln(n/m) where n = 25 – number of documents \nand m is the number of documents, containing the unique word among 25 documents. \nThe existing fundamental papers that deal with stop words in general, let alone for the Uzbek \nlanguage, barely address the quality of the automatically detected list of stop words. This statement also \napplies to our previous work, where a preliminary manual expert observation of a part of the lists (only \nunigrams) was done. To the authors’ knowledge, there was no in-depth observation of the accuracy of \nthe automatically constructed lists of stop words for agglutinative languages. For instance, [7][8][9][10] \nare mostly focusing on Uzbek texts’ stop words and methods for automatic extraction of stop words. \nBut none of them discusses the accuracy of the presented methods. The article is devoted to answering \nwhether there is a good way of evaluating available stop words for Uzbek texts, or whether it is possible \nto determine what part of the Uzbek sentence contains the majority of the stop words by studying the \nnumerical characteristics of the probability of unique words.  \nThe words were sorted by the TF-IDF value in descending order and the lowest 5 percent of them \nwere tagged as stop words. We used this method to automatically detect stop words in the corpus [8]. \nUsing this information, the article focuses on the followings:  \n● To create a probability distributions model of the TF-IDF of unique words in order to determine \nthe position of stop words along with the corpus; \n● To establish the accuracy of the detection method for stop words; \n● To conclude on automatic position detection of stop words for the given text. \n \nThe rest of the paper is structured as follows: We start by explaining the related works in the field of \nstop word removal, as well as the Uzbek language itself in Section 2, followed by the main methodology \nof the paper in Section 3, which includes the creation of probability distribution law of TF-IDF of \nunique words (Section 3.1), the numerical characteristics of the probability of unique words (Section \n3.2), and the evaluation of the created method using a small selected chunk (Section3.3). The accuracy \nof the method for automatic detection of stop words in Uzbek texts, which is based on TF-IDF, is \npresented in Section 4. The last section of the paper presents conclusions and future work (Section 5). \n \n \n2. Related works \nUzbek language belongs to the family of Turkic languages. There has been some research on the Uzbek \nlanguage mostly in the last few years. Most of the research done on Turkic languages can be applied to \nthe Uzbek language as well, using cross-lingual learning and mapping approaches, alongside some \nlanguage-specific additions. The paper [1] presents a viability study of established techniques to align \nmonolingual embedding spaces for Turkish, Uzbek, Azeri, Kazakh, and Kyrgyz, members of the Turkic \nfamily which is heavily affected by the low-resource constraint. \nSeveral authors present experiment and propose techniques for stopwords extraction from text for \nagglutinative languages such as [2] that bases the stopword detection problem as a binary classification \nproblem and the evaluation shows that classification methods improve stopword detection with respect \nto frequency-based methods for agglutinative languages but fails for English. Ladani and Desai [5] \npresent an overview of stopwords removal techniques for Indian and Non-Indian Languages. Jayaweera \net al. [2] proposes a dynamic approach to find Sinhala stopwords, the cutoff point is subjective to the \ndataset. Wijeratne and de Silva [17] collected the data from patent documents and listed the stopwords \nusing term frequency. Rakholia et al. [14] proposed a rule-based approach to detect stopwords for the \nGujarati language dynamically. They developed 11 static rules and used them to generate a stopword \nlist at runtime. Fayaza et al. [1] presents a list of stopwords for Tamil language and reports improvement \nin text clustering using removal. \nThe paper ¡Error! No se encuentra el origen de la referencia. provides the first annotated corpus \nfor polarity classification for the Uzbek language. Three lists of stop words for the Uzbek language are \npresented in [7] that were constructed using automatic detection of stop words by applying algorithms \nand methods presented in [8]. Paper [9] focuses on the automatic discovery of stop words in the Uzbek \nlanguage and its importance. Articles [12] and [13] are also mainly concentrated on the creation of stop \nwords in Uzbek.  \nMatlatipov et. al [10] propose the first electronic dictionary of Uzbek words’ endings invariants for \nmorphological segmentation pre-processing useful for neural machine translation. \nThe article [11] presents the algorithm of cosine similarity of Uzbek texts, based on TF-IDF to \ndetermine similarity. Another work on similarity in Uzbek, but this time on semantic similarity of \nwords, a decent amount of work went on the creation and evaluation of a semantic evaluation dataset \nthat possesses both similarity and relatedness scores ¡Error! No se encuentra el origen de la \nreferencia.. \n  \n3. Methodology \nThe scientific novelty of the methodology used in this work can be shown as follows: \n● \nThe creation of probability distributions law based on TF-IDF scores of unique words; \n● \nThorough investigation of numerical characteristics of the probability of unique words; \n● \nBetter evaluation of the stop words detection method’s accuracy; \nSummarising the automatic detection of the position of stop words in given Uzbek texts. \nIn our previous work[8], we proposed the usage of TF-IDF [15] to automatically extract stop words \nfrom a corpus of documents. The stop words are discovered based on the Term Frequency Inverse \nDocument Frequency – TF-IDF. The number of times a word occurs in a text is defined by Term \nFrequency -- TF. Inverse Document Frequency -- IDF is defined as the number of texts (documents) \nbeing viewed and the presence of a given word in chosen texts (documents). TF-IDF is one of the \npopular methods of knowledge discovery. \nMadatov et. al [8] propose the usage of TF-IDF [15] to automatically extract stop words from a \ncorpus of documents. The stop words are discovered based on the frequency of the word and the \nfrequency of the inverse document Term Frequency – Inverse Document Frequency – TF-IDF. The \nnumber of times a word occurs in a text is defined by Term Frequency -- TF. Inverse Document \nFrequency -- IDF is defined as the number of texts (documents) being viewed and the presence of a \ngiven word in chosen texts (documents). TF-IDF is one of the popular methods of knowledge discovery. \n \n \n3.1. Probability distribution \nIn order to determine the position of the stop words throughout the school corpus, we investigate the \nprobability distribution law of TF-IDF scores of stop words. \n \nWord weight and its probability. Select a word 𝑎𝑖; 𝑖∈[1. .47165] from the set of unique \nwords extracted from a corpus. For future references these two assumptions are valid: a word represents \na unique word from a corpus and a corpus represents the “School corpus” presented in our previous \nwork [4]. For every 𝑎𝑖 calculate average TF-IDF(𝑎𝑖), called the weight of 𝑎𝑖  and denoted as 𝑤𝑖. It is \nknown that 𝑤𝑖 is not the probability of the word 𝑎𝑖.  \n \nThe probability 𝑝𝑖 of 𝑡ℎ𝑒 𝑤𝑜𝑟𝑑 𝑎𝑖 can be calculated using the following formula: 𝑝𝑖=  𝑤𝑖 / ∑\n𝑤𝑖. \nWe match  𝑝𝑖 for each 𝑎𝑖 word. Now ∑\n𝑝𝑖= 1.  \nThe probability density function. Suppose unique words are distributed independently in the total \ncorpus. In that case, word 𝑎𝑖 can be applied multiple times. In order to escape repeating the word 𝑎𝑖 \nWe consider only the first appearance of this word. For each word 𝑎𝑖 observe i as a random variable. \nAs the probability density function of the unique words, we get the following function: \n 𝑓(𝑖) = 𝑝𝑖  \nf(i) can be considered as the probability density function of word 𝑎𝑖. \n \nIn the Cartesian coordinate plane, observe i on the OX axis and observe 𝑝𝑖 along the OY axis. Figure 1 \npresents the described observations extracted from the “School corpus”. We need it to observe the \nposition of stop words along with the corpus. \n \n \n \nFigure 1. The probability density function of unique words. The X-axis represents the index number of words, while the Y-\naxis shows the probability score. \n  \n3.2. Numerical characteristics of the probability \nThis section presents numerical characteristics of the probability of unique words. They are \ncalculated by the following formulas: \n𝐸= ∑\n𝑖∙𝑝𝑖− the mathematical expectation of the unique words \n𝐷= ∑\n(𝑖−𝐸)2 ∙𝑝𝑖 – dispersion of the unique words \n𝜎= √𝐷 – standard deviation of the unique words \n𝐸𝑘= ∑\n𝑝𝑖∙𝑖𝑘− 𝑘−𝑡ℎ 𝑟𝑎𝑤 𝑚𝑜𝑚𝑒𝑛𝑡𝑠   of the unique words \n𝜇3 = 𝐸3 −3 ∙𝐸1 ∙𝐸2 + 2 ∙𝐸1\n3 − third central moment of the unique words \n𝐴𝑠= 𝜇3/𝜎3 − The asymmetry of the theoretical distribution \n \nThe described values extracted from the corpus are presented in Table 1.  \n \nTable 1: Basic statistical properties extracted from the corpus.  \n𝐸 \n𝐷 \n𝜎 \n𝜎3 \n𝐸1 \n𝐸2 \n𝐸3 \n𝜇3 \n𝐴𝑠 \n23310,74 \n23310,74 \n13623,72 \n2,52864E+12 \n23310,74 \n728996416,52 \n25687931167881,50 \n41266663785,91 \n0,163 \n \nThe variety of words increases gradually with grades in the school literature. It means that the \nprobability density function of unique words is not symmetrical. One may predict it without a \nmathematical way. However, mathematically, the data in Table 1, especially, 𝐴𝑠> 𝑜,  confirms that the \nprobability density function is asymmetric.  \n \n \n \nFigure 2. The probability density function of unique words with stop words. The orange dots indicate the positions of stop \nwords along with the corpus. \n \nThe stop words are distributed along the axis (not grouped at one part of the axis); represented by orange \ndots in Figure 2. \n \n3.3. Evaluation using a sub-corpus \nThis section presents the probability density function of unique words of selected work from the corpus. \nEach book from the corpus is devoted to one topic.  \nThe prediction: Every book consists of the culmination part of the topic, the rest can be stop words. \nThat is why we investigated just one book.  \nA random book was selected from the range of 25 books (in the corpus): 11th class literature. The book \nconsists of 12837 unique words. The same process that was presented in Section 3.2 was applied to just \nthe selected part of the corpus in order to create the probability density function of unique words. Figure \n3 shows the probability density function of 11th class literature unique words.  \n \n \nFigure 3: probability density function of 11th class literature unique words \n \nMathematical analysis of the distribution is presented in Table 2. \nTable 2: Distribution analysis of the selected single book \n𝐸 \n𝐷 \n𝜎 \n𝜎3 \n𝐸1 \n𝐸2 \n𝐸3 \n𝜇3 \n𝐴𝑠 \n7076,623 \n11981425 \n3461,41 \n414472396507 \n7076,623 \n602060020 \n598084106956 \n-10667328016 \n-0,251 \n \n \nFigure 4:Unique words from part of the corpus sorted by probability, lowest 5% are candidates for stop words \nWe obtain Figure 4 by the rule of stop words detection method, as mentioned in [4]. \n \n𝐴𝑠< 0 means that the probability density function is asymmetric. \nThe values were sorted in descending order and the lowest 5 percent of them are candidates for stop \nwords. Figure 4 graphically represents the process, words with probability less than 𝑝𝑖∗ are candidates \nto be a stop word (𝑝𝑖∗ = 0,00001034371184). \nThe number of these candidates is 642. 85,8% of these words is located outside of the interval (𝐸−\n𝜎, 𝐸+ 𝜎). On the left side of the interval there are 545 stop words and on the right side are 6 stop words. \nThe same facts can be observed graphically on Figure 5 (Taking into the account the numerical \ncharacteristics of 5% words of selected work and comparing Figure 3 and figure 4 we detected their \nposition along with the text). \n \n \nFigure 5: 85,8% of the stop word candidates are indeed located outside of the(E-σ,E+σ) interval \n \n \n4. Evaluation results \nThe accuracy of the presented method if confirmed using the following reasoning: \nLet suppose hypothesis  \nH0: Stop words of the selected document (11th class literature) are located outside of the interval  \n(E-σ,E+σ);  \nand alternative hypothesis \nH1: Stop words of the selected document (11th class literature) are located inside of the interval (E-\nσ,E+σ). \n \nThe critical value – Z (Z-score or Standard score) is obtained using this Equation:  \n𝑍=\n𝑋 −𝐸\n𝜎/√𝑁.; where N=12837, 𝑋=6419, 𝐸=7076.62, 𝜎= 3461.419.  \nIn the presented task |Z|≈21,526. Z is located on the left side of E-σ, meaning there is no reason to reject \nthe null hypothesis.   \nThis is the basis for rejecting the H1 hypothesis. \n \n5. Conclusions and further work \nThroughout the work performed in this paper, we presented a natural extension of the already presented \nprevious research of the automatic detection of stop words in the Uzbek language [4] and the main \nfocus of the analysis was twofold: a) a probability distributions model of the observed text and b) the \naccuracy of the detection method for stop words.  \nFrom all theoretical investigations from previous sections, it can be concluded that, for a single genre, \nthe majority of stopwords have the following nature: \nif 𝐴𝑠< 0, are located at the beginning parts of the text; \nif 𝐴𝑠> 0, are located at the ending of the text; \nif 𝐴𝑠= 0, are located at the beginning at the ending part of the text. \nIn future works, we would like to use the results of this article as the basis for automatically \nextracting keywords and automatically extracting the abstract of a given text. \n \n6. Acknowledgements \nThe authors gratefully acknowledge the European Commission for funding the InnoRenew CoE \nproject (Grant Agreement $\\#$739574) under the Horizon2020 Widespread-Teaming program and the \nRepublic of Slovenia (Investment funding of the Republic of Slovenia and the European Union of the \nEuropean Regional Development Fund).  \n7. Conclusion \nThe paper presents a natural extension of the already presented research of automatic detection of stop \nwords in Uzbek language[8] and presents two goals: a) a probability distributions model of the observed \ntext and b) the accuracy of the detection method for stop words.  \na) The probability density is defined and later used to observe the accuracy of the automatic \nmethod for extraction of stop words of Uzbek language.  \nb) The accuracy of the method that is presented in Section ¡Error! No se encuentra el origen \nde la referencia.. \nFrom this fact it can be concluded that, for a single genre, more of the stop words for texts: \nif 𝐴𝑠< 0, are located at the beginning parts of the text; \nif 𝐴𝑠> 0, are located at the ending of the text; \nif 𝐴𝑠= 0, are located at the beginning at the ending part of the text. \nFurther we use this result in the process of automatically extracting keywords from the given text and \nautomatically extracting the annotation of the given text. \n8. References \n[1] F. Fayaza, F. Farhath. \"Towards stop words identification in Tamil text clustering.\", (IJACSA) \nInternational Journal of Advanced Computer Science and Applications, Vol. 12, No. 12, (2021). \n[2] A. A. V. A. Jayaweera, Y. N. Senanayake, P. S. Haddela, \"Dynamic Stopword Removal for Sinhala \nLanguage,\" 2019 Natl. Inf. Technol. Conf. NITC 2019, pp. 8–10, 2019, doi: \n10.1109/NITC48475.2019.9114476. \n[3] M. Kumova, B. Karaoğlan. \"Stop word detection as a binary classification problem.\" Anadolu \nUniversity Journal of Science and Technology A-Applied Sciences and Engineering 18, no. 2 \n(2017): 346-359. \n[4] E. Kuriyozov, Y. Doval, C. Gomez-Rodriguez. “Cross-Lingual Word Embeddings for Turkic \nLanguages”, Proceedings of The 12th Language Resources and Evaluation Conference, pp4054--\n4062, 2020 \n[5] Kuriyozov, E., Matlatipov, S., Alonso, M.A. and Gómez-Rodríguez, C., 2022. Construction and \nEvaluation of Sentiment Datasets for Low-Resource Languages: The Case of Uzbek. In Language \nand Technology Conference (pp. 232-243). Springer, Cham. \n[6] D. J. Ladani, N. P. Desai, \"Stopword Identification and Removal Techniques on TC and IR \napplications: A Survey,\" 2020 6th Int. Conf. Adv. Comput. Commun. Syst. ICACCS 2020, pp. \n466–472, (2020), doi: 10.1109/ICACCS48705.2020.9074166. \n[7] K. Madatov, S. Bekchanov, J. Vičič. “Lists of Uzbek Stopwords”, Zenodo, (2021), doi: \n10.5281/zenodo.6319953 \n[8] K. Madatov, S. Bekchanov, J. Vičič. “Automatic Detection of Stop Words for Texts in the Uzbek \nLanguage”, Preprints, MDPI, 2022 \n[9] K. Madatov, M. Sharipov, S. Bekchanov. O ‘zbek Tili Matnlaridaginomuhim so ‘zlar //Computer \nLinguistics: Problems, Solutions, Prospects. – 2021. – Т. 1. – nr. 1. \n[10] S. Matlatipov, U. Tukeyev, M. Aripov. “Towards the Uzbek Language Endings as a Language \nResource”, In: Advances in Computational Collective Intelligence. ICCCI 2020. Communications \nin Computer and Information Science, vol 1287. Springer, Cham., (2020) \n[11] S. Matlatipov. \"Cosine Similarity and its Implementation to Uzbek Language Data,\" Central Asian \nProblems of Modern Science and Education: Vol. 2020 : Iss. 4 , Article 8, (2020). \n[12] I. Rabbimov, S. Kobilov, I. Mporas. Uzbek News Categorization using Word Embeddings and \nConvolutional Neural Networks. 2020 IEEE 14th International Conference on Application of \nInformation \nand \nCommunication \nTechnologies \n(AICT). \npp \n1-5, \n(2020), \ndoi:10.1109/AICT50176.2020.9368822 \n[13] I. Rabbimov, S. Kobilov. “Multi-Class Text Classification of Uzbek News Articles using Machine \nLearning”. Journal of Physics: Conference Series. (2020), doi: 10.1088/1742-6596/1546/1/012097 \n[14] R. M. Rakholia, J. R. Saini, \"A Rule-Based Approach to Identify Stop Words for Gujarati \nLanguage,\" In Proceedings of the 5th International Conference on Frontiers in Intelligent \nComputing: Theory and Applications, pp. 797-806, (2017) \n[15] C. Sammut, G. Webb, eds. “Encyclopedia of machine learning”. Springer Science & Business \nMedia, (2011) \n[16] Salaev, Ulugbek, Elmurod, Kuriyozov, and Carlos, Gomez-Rodriguez. \"SimRelUz: Similarity and \nRelatedness scores as a Semantic Evaluation dataset for Uzbek language\". In Proceedings of the \nthe 1st Annual Meeting of the ELRA/ISCA Special Interest Group on Under-Resourced Languages \n(pp. 199–206). European Language Resources Association, 2022. \n[17] Y. Wijeratne, N. de Silva, \"Sinhala Language Corpora and Stopwords from a Decade of Sri Lankan \nFacebook,\" arXiv, 2020, doi: 10.2139/ssrn.3650976. \n \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-09-15",
  "updated": "2022-09-15"
}