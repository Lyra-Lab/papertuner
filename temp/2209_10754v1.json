{
  "id": "http://arxiv.org/abs/2209.10754v1",
  "title": "INFINITY: A Simple Yet Effective Unsupervised Framework for Graph-Text Mutual Conversion",
  "authors": [
    "Yi Xu",
    "Luoyi Fu",
    "Zhouhan Lin",
    "Jiexing Qi",
    "Xinbing Wang"
  ],
  "abstract": "Graph-to-text (G2T) generation and text-to-graph (T2G) triple extraction are\ntwo essential tasks for constructing and applying knowledge graphs. Existing\nunsupervised approaches turn out to be suitable candidates for jointly learning\nthe two tasks due to their avoidance of using graph-text parallel data.\nHowever, they are composed of multiple modules and still require both entity\ninformation and relation type in the training process. To this end, we propose\nINFINITY, a simple yet effective unsupervised approach that does not require\nexternal annotation tools or additional parallel information. It achieves fully\nunsupervised graph-text mutual conversion for the first time. Specifically,\nINFINITY treats both G2T and T2G as a bidirectional sequence generation task by\nfine-tuning only one pretrained seq2seq model. A novel back-translation-based\nframework is then designed to automatically generate continuous synthetic\nparallel data. To obtain reasonable graph sequences with structural information\nfrom source texts, INFINITY employs reward-based training loss by leveraging\nthe advantage of reward augmented maximum likelihood. As a fully unsupervised\nframework, INFINITY is empirically verified to outperform state-of-the-art\nbaselines for G2T and T2G tasks.",
  "text": "INFINITY: A Simple Yet Effective Unsupervised Framework for\nGraph-Text Mutual Conversion\nYi Xu\nLuoyi Fu\nZhouhan Lin\nJiexing Qi\nXinbing Wang\nShanghai Jiao Tong University\nAbstract\nGraph-to-text (G2T) generation and text-to-\ngraph (T2G) triple extraction are two essential\ntasks for constructing and applying knowledge\ngraphs.\nExisting unsupervised approaches\nturn out to be suitable candidates for jointly\nlearning the two tasks due to their avoidance of\nusing graph-text parallel data. However, they\nare composed of multiple modules and still re-\nquire both entity information and relation type\nin the training process. To this end, we pro-\npose INFINITY, a simple yet effective unsuper-\nvised approach that does not require external\nannotation tools or additional parallel informa-\ntion. It achieves fully unsupervised graph-text\nmutual conversion for the ﬁrst time. Specif-\nically, INFINITY treats both G2T and T2G\nas a bidirectional sequence generation task by\nﬁne-tuning only one pretrained seq2seq model.\nA novel back-translation-based framework is\nthen designed to automatically generate con-\ntinuous synthetic parallel data. To obtain rea-\nsonable graph sequences with structural in-\nformation from source texts, INFINITY em-\nploys reward-based training loss by leveraging\nthe advantage of reward augmented maximum\nlikelihood.\nAs a fully unsupervised frame-\nwork, INFINITY is empirically veriﬁed to out-\nperform state-of-the-art baselines for G2T and\nT2G tasks.\n1\nIntroduction\nGraph-to-text (G2T) generation and text-to-graph\n(T2G) triple extraction are two mutually inverse\ntasks that are crucial to the domain of knowledge\ngraphs (KGs). G2T verbalizes the structural in-\nformation in KG with descriptive texts, which has\nattracted much attention to expand the application\nscope of KG, such as KG-based dialogue and Q&A\nsystem (Ji et al., 2021). As a primary task of in-\nformation extraction, T2G aims to extract triples\nfrom text, the typical subtasks of which include\nnamed entity recognition (NER) and relation ex-\ntraction (RE). Figure 1 illustrates a training pair\nAlan Bean\nTest Pilot\nApollo 12\nNASA\noccupation\ncrewMember\noperator\nAlfred Worden\nDavid Scott\nbackupPilot\ncommander\nGraph\nText\nUT Austin, \nB.S. 1995\n1963\nselection\nalmaMater\nAlan Bean graduated from UT Austin in 1955 with a \nBachelor of Science degree. He was hired by NASA in \n1963 and served as a test pilot. Apollo 12's backup pilot \nwas Alfred Worden and was commanded by David Scott.\nFigure 1: A pair of knowledge subgraph and its corre-\nsponding text.\nsample containing part of a knowledge graph and\nits corresponding text.\nG2T and T2G have been intensively studied re-\nspectively, mainly treated as two kinds of indepen-\ndent problems in a supervised way. Due to the suc-\ncess of pretrained language models (PLMs) (Raf-\nfel et al., 2019; Lewis et al., 2020), mainstream\nsupervised methods have achieved considerable\nperformance with ﬁne-tuning or prompt learning\nparadigm (Ribeiro et al., 2021; Clive et al., 2021;\nYe et al., 2021; Ke et al., 2021). However, these\nsupervised methods require annotated data. In-\nspired by unsupervised machine translation ap-\nproaches (Lample et al., 2018), recent work at-\ntempts to explore low-resource alternatives that\navoid the requirement of graph-text pairs with un-\nsupervised joint learning (Schmitt et al., 2020; Guo\net al., 2020b). As illustrated in Figure 2, unsu-\npervised methods consist of G2T modules and\nT2G modules with different parameters, which are\ntrained jointly in an iterative manner through the\ntwo steps of back-translation: the generation step\nand training step. The outputs of the generation\nstep for the current modules serve as the supervised\ntraining signal for the other modules in the next it-\neration. Such an interactive and coupling process\nimperceptibly produces a lot of synthetic parallel\narXiv:2209.10754v1  [cs.CL]  22 Sep 2022\nG2T\nRelation \nClassification\nGiven Entities or \nExternal Tools\nG2T\nRelation \nClassification\nG2T modules\nT2G modules\nTraining T2G\nTraining G2T\nGeneration Step\nTraining Step\nGiven Entities or \nExternal Tools\nFigure 2: Framework of existing unsupervised models.\nThe left part is the cycle of training T2G, and the right\npart is the cycle of training G2T.\ndata that is helpful to low-resource training.\nIn this paper, we are thus motivated to focus\non unsupervised learning of both G2T and T2G\ntasks in a joint framework. As shown in Figure 2,\nexisting state-of-the-art models share two major\nissues in order to be jointly trained. First, current\nunsupervised models usually simplify the T2G task\ninto relation classiﬁcation with given entities (Jin\net al., 2020). As a result, the text corpus has to\nseek external information extraction tools for the\nacquisition of entity annotations. Second, existing\nresearch branches on either G2T or T2G separately\nimplement the two tasks using different neural mod-\nules, i.e., G2T modules and T2G modules, which\ncontain numerous parameters that make it chal-\nlenging to train and share information with each\nother (Schmitt et al., 2020; Guo et al., 2020b).\nTo tackle the above issues, we design a novel\nback-translation-based framework called INFIN-\nITY that integrates G2T and T2G tasks under the\nunsupervised setting. Note that we name our frame-\nwork as INFINITY since the overall architecture of\nthe interaction between G2T and T2G resembles\nthe shape of ∞(Figure 3). We ﬁrst investigate the\npower of seq2seq-based PLMs for G2T and T2G\ngeneration and propose to regard graph-text mu-\ntual conversion as two sequence generation tasks,\nwhere we manage to ensure the simultaneous gen-\neration of continuous synthetic pairs of graph-text\nsequences in a PLM-based module with our newly\ndesigned back-translation technique. In this way,\nINFINITY requires no additional neural networks\nbeyond the PLM. To prevent the possible perfor-\nmance deterioration caused by graph linearization,\nwe adopt the reward augmented maximum likeli-\nhood (Norouzi et al., 2016) for training losses to\nretain the order and structural information in the\noriginal dataset during the training process. In con-\ntrast to prior unsupervised work (Schmitt et al.,\n2020; Guo et al., 2020b), INFINITY is entirely\nbootstrapped without the assistance from manual\nor automatic annotation tools.\nWe perform extensive experiments on two bench-\nmarks: WebNLG (Gardent et al., 2017) and Gen-\nWiki (Jin et al., 2020), both of which belong to the\nvery few benchmarks that can evaluate G2T and\nT2G jointly. The results show the superiority of\nINFINITY over existing methods. Thanks to its\nsimplicity and efﬁciency, INFINITY can be quickly\ndeployed on various scenarios for application. This\nwork presents the following contributions:\n• We are the ﬁrst to take G2T and T2G as two\nsequence generation tasks and propose IN-\nFINITY, a novel unsupervised framework for\ngraph-text mutual conversion.\n• INFINITY uses only one pretrained seq2seq\nmodel to generate synthetic parallel data it-\neratively and employs the reward augmented\nmaximum likelihood for training loss to ob-\ntain structured graph sequences.\n• INFINITY requires no parallel information or\nexternal annotation tools compared with other\nunsupervised models.\n• We conduct extensive experiments to evaluate\nINFINITY on two benchmarks. The results\ndemonstrate its superiority.\n2\nRelated Work\n2.1\nSupervised Graph-text Models\nAs part of the data-to-text task, the key of G2T lies\nin capturing structural information and generating\nﬂuent texts. Some researchers (Marcheggiani and\nPerez-Beltrachini, 2018; Koncel-Kedziorski et al.,\n2019) design sophisticated architecture based on\ngraph neural networks to encode KGs. In addi-\ntion, most methods linearize the graph to sequence\nas input to models. However, graph linearization\nmay lead to the loss of structural information. Re-\nsearches (Moryossef et al., 2019; Zhao et al., 2020;\nGuo et al., 2020a) introduce different neural plan-\nner to determine the order of input triples before\nlinearization. Recently, Ribeiro et al. (2021) inves-\ntigate different PLMs for G2T generation. Clive\net al. (2021) propose control preﬁxes as prompt\nfor PLM, which empowers the model to have ﬁner-\ngrained control during text generation.\nRegarding T2G, it aims to extract entities and\nrelations (triples) from given texts, which is usu-\nally handled as a classiﬁcation (tagging) prob-\nlem to label roles for different tokens (Wei et al.,\n2020; Yan et al., 2021).\nApart from these ap-\nproaches, there emerge some triplet-generation\nmodels. CopyRE (Zeng et al., 2018) uses the idea\nof copy mechanism for triple extraction. CPC (Ye\net al., 2021) utilizes contrastive learning for direct\ngraph sequence generation, which is similar to the\nproblem deﬁnition of INFINITY.\n2.2\nUnsupervised Graph-text Models\nAs shown in Figure 2, unsupervised models com-\nbine G2T and T2G into joint learning frameworks.\nGraph-Text Back Translator (GT-BT) (Schmitt\net al., 2020) is the ﬁrst approach to unsupervised\ntext generation from KGs and can be used for se-\nmantic parsing simultaneously. CycleGT (Guo\net al., 2020b) is another unsupervised training\nmethod that uses non-parallel graph and text data\nand iteratively back translates between the two\nforms. Although GT-BT and CycleGT employ\nback-translation for unsupervised settings, they\nsimplify the T2G task to relation classiﬁcation with\ngiven entities (Jin et al., 2020), which requires the\ntext corpus to have entity annotations with external\ninformation extraction tools. To some extent, these\nmethods leak the information of parallel corpus in\nthe training process.\n3\nMethod\nThis section introduces the proposed method IN-\nFINITY. We ﬁrst deﬁne the tasks and notations.\nThen we describe the framework and implementa-\ntion details in the following parts.\n3.1\nFormulation and Notations\nGiven two non-parallel datasets: a text corpus\nT = {ti}N\ni=1, and a graph dataset G = {gj}M\nj=1,\nwhere N and M are the numbers of text sequences\nand graphs, respectively. Each text sequence in T\ncan be denoted as t = (w1, · · · , wL) with L tokens,\nwi ∈V is the i-th token in t, and V is the vocab-\nulary. Each graph in G consists of a set of triples,\ndenoted as g = {(eh, r, et)|eh, et ∈E, r ∈R},\nwhere E and R represent the entity set and rela-\ntion type set, respectively. Each entity e ∈E is\ncomposed of several tokens formulated as e =\n(we\n1, · · · , we\nLe), we\ni ∈V. Each relation type r ∈R\nis also made up of several tokens formulated as\nr = (wr\n1, · · · , wr\nLr), wr\ni ∈V. Similar to multilin-\ngual neural machine translation, we assume T and\nG share the same distribution of latent content z\nsuch as linguistic or semantic characteristics:\np(g) =\nZ\nz\np(g|z)p(z)dz,\n(1)\np(t) =\nZ\nz\np(t|z)p(z)dz,\n(2)\nwhich is the key of unsupervised learning. In our\nunsupervised framework, both G2T and T2G are\nregarded as sequence generation tasks. G2T aims\nto generate a natural language text sequence from a\nknowledge subgraph, while T2G generates a triple\nsequence that describes the linearized graph where\nentities and relations exist in the given text. Since\nthe graph itself is a set of triples, for a graph g ∈G,\nwe adopt linearization strategy by concatenating\nall triples with special tokens [H], [R], [T], and [E]\nto specify the head entity, relation type, tail entity,\nand end of sequence respectively. The linearized\ngraph is illustrated as follows:\n[H] e1\nh [R] r1 [T] e1\nt\n[H] e2\nh [R] r2 [T] e2\nt\n· · ·\n[H] e|g|\nh [R] r|g| [T] e|g|\nt\n[E],\n(3)\nwhere ei\nh, ri, and ei\nt refer to the elements of the i-th\ntriple in g. We simply linearize the graph using the\norder of triples in the original dataset. Note that\nwe do not consider other sophisticated methods\nfor linearization since these methods will lead to\nadditional neural components, and we only focus\non the proposed framework rather than the neural\ncomponents inside.\n3.2\nJoint Training Framework of G2T &\nT2G\nThe overall architecture of INFINITY is shown in\nFigure 3, which is shaped like ∞. The framework\niteratively back-translates between graph dataset\nand text corpus, where the vocabulary embeddings\nand the seq2seq-based PLM denoted by Mθ are the\nsame for G2T and T2G tasks. For simplicity and\nwith a slight abuse of notation, we use the same\nsymbol Mθ(·) to represent the sequence generating\nfunction of the PLM, whether its output is discrete\nPLM Mθ\nMθ\n[H] v1 [R] e1 [T] v2\n [H] v3 [R] e2 [T] v4 [E]\n[H] v2 [R] e1 [T] v4 \n[H] v1 [R] e2 [T] v3 [E]\nABCD EFGH IJKL \nMNORST UVXYZ\nABC DEFG HIJKL \nOPQRST UVWXYZ\nReward Augmented \nSampling\nReward Augmented \nSampling\nFigure 3: The overall architecture of INFINITY. The cy-\ncle of blue arrows illustrates the direction of G2T task\nwhile the cycle of magenta arrows illustrates the direc-\ntion of T2G task.\nor continuous. The training process of INFINITY\nconsists of two parts.\nGraph →Text →Graph. In this part (the cycle\nof blue arrows), we ﬁrst linearize the original graph\ninto a triple sequence with special tokens. The\nlinearized graph g is then fed to the encoder of\nMθ for further training. Afterward, the decoder of\nMθ generates a synthetic text sequence denoted by\nMθ(g) as the intermediate result. It is worth noting\nthat we use the text embedding generated from\nMθ instead of the discrete tokens as the input for\nthe PLM’s encoder in the following step. Finally,\nthe PLM receives the synthetic text embedding\nand generates a back-translated graph, which is\nused to align the original one through maximum\nlikelihood estimation. Ideally, the back-translated\ngraph should mimic the original graph g.\nText →Graph →Text. Similarly, for the other\ndirection (the cycle of magenta arrows), the PLM\nMθ ﬁrst generates a synthetic graph sequence de-\nnoted by Mθ(t) from a text t. Then, we also use\nthe embedding of the synthetic graph sequence and\nfeed it to Mθ. In the end, Mθ generates a back-\ntranslated text from the synthetic graph embedding\nfor the original text t to train the parameters. We\nexpect the back-translated text to be roughly the\nsame as the original text.\nDifferent\nfrom\nthe\nunsupervised\nback-\ntranslation (Lample et al., 2018; Guo et al., 2020b),\nINFINITY only employs one neural network,\ni.e.\nthe PLM Mθ for these two tasks.\nIn this\nway, the parameters of the framework are greatly\nreduced. The only PLM can observe the original\ngraphs and texts at the same time, which is\neasier for information sharing and model training.\nIn summary, G2T and T2G can be optimized\nsimultaneously in the proposed INFINITY with\nsynthetic parallel pairs (t, Mθ(t)) and (g, Mθ(g)).\nThe objective is as follows:\nL = Eg∈G[−log P(g|Mθ(g))]+\nEt∈T [−log P(t|Mθ(t))]\n(4)\n3.3\nReward Augmented Training Loss\nAs a framework to solve the problem of bidirec-\ntional sequence generation, we need to consider\nhow to retain more structural information in graphs\nas much as possible without introducing additional\nparameters.\nIn detail, graph linearization strat-\negy hinders seq2seq-based PLM from capturing\ngraph structure with maximum likelihood estima-\ntion (MLE) since MLE suffers from the exposure\nbias problem (Bengio et al., 2015). To this end,\nwe adopt reward augmented maximum likelihood\n(RML) (Norouzi et al., 2016) which combines the\nprimary form of MLE and the maximum reward\nexpectation in reinforcement learning (RL). In this\nway, our training process can make rewards one of\nthe training targets under the framework of MLE,\nwhich considers the structure of graphs and the\norder of texts. According to RML, the exponen-\ntiated payoff distribution connects MLE and RL\nobjectives, and it can be easily incorporated into\nMLE-based training. In our framework, we deﬁne\na distribution in the augmented space for graph\ndataset G as follows:\nq(˜g|g; τ) =\nexp(r(˜g, g)/τ)\nP\n˜g∈˜G exp(r(˜g, g)/τ),\n(5)\nwhere ˜g ∈˜G is the output hypothesis (possible gen-\nerated sequence) of g, r(˜g, g) denotes the reward\nfunction such as BLEU or F1 score, τ is the tem-\nperature to control the degree of regularization, and\nτ > 0. Now, we modify the MLE-based objective\nfunction to:\nLG\nRML = Eg∈G[−\nX\n˜g∈˜G\nq(˜g|g; τ) log p(˜g|Mθ(g))].\n(6)\nIn LG\nRML, the predictive probability of the out-\nputs in the original loss can be smoothed using\ntheir corresponding rewards with the distribution\nq(˜g|g; τ). For symmetry, RML can also be ex-\ntended to our text corpus T similarly, which is as\nfollows:\nq(˜t|t; τ) =\nexp(r(˜t, t)/τ)\nP\n˜t∈˜T exp(r(˜t, t)/τ),\n(7)\nLT\nRML = Et∈T [−\nX\n˜t∈˜T\nq(˜t|t; τ) log p(˜t|Mθ(t))],\n(8)\nwhere ˜t ∈˜T is the output hypothesis of t. However,\nexperiments show that the strategy will not signif-\nicantly improve the performance when applied to\ntext sequences. In addition, existing unsupervised\nmodels such as Guo et al. (2020b) cannot em-\nploy RML for graph extraction, which is deﬁned\nas a relational classiﬁcation problem rather than a\nsequence generation problem.\nThe system of RML is simple and computation-\nally efﬁcient. One only needs to sample possible\noutputs ˜G and ˜T from their corresponding exponen-\ntiated payoff distribution before training. Accord-\ning to Norouzi et al. (2016), it is difﬁcult to sample\nwith BLUE or F1 score since the distribution is\nintractable to compute. Thus, we adopt the im-\nportance sampling method with the distribution of\nhamming distance between the original sequence\nand its hypothesis. More theoretical details can be\nfound in Norouzi et al. (2016).\n3.4\nTraining and Inference Details\nIn INFINITY, G2T and T2G tasks are jointly trained\nthanks to the shared parameters. Compared with\nunsupervised machine translation, our model does\nnot train a language model with the denoising auto-\nencoder objective on the two tasks due to the shared\nvocabulary of PLM. As a result, we optimize the\nloss function:\nL = LG\nRML + LT\nRML.\n(9)\nThe detailed training process of unsupervised\nINFINITY is provided in Algorithm 1. In our im-\nplementation, we use T5-base (Raffel et al., 2019)\nas the PLM since T5 is based on transformer and\ncan handle multiple tasks well. We prepend graph\npreﬁx Graph: to the linearized graph sequence for\nG2T task and text preﬁx Text: for T2G task. In or-\nder to speed up the convergence of training, when\ngenerating synthetic intermediate outputs of texts,\nwe discard embeddings of illegal tokens including\n[H], [R], [T], and [E] for the G2T task, which will\nnot be fed to the encoder of the PLM in the follow-\ning step. During the inference stage, we leverage\nthe beam search to generate texts and linearized\ngraphs. Additionally, for the T2G direction, we\nadopt the same heuristic rules recommended in\nprior work (Ye et al., 2021) to generate reasonable\nlinearized graphs, where the special token [R] (re-\nlation) should be followed by [H] (head entity).\nAlgorithm 1 Training Unsupervised INFINITY\n1: Initiate parameters of PLM M(1)\nθ ;\n2: Obtain the distribution of q(˜g|g; τ) from G ac-\ncording to Equation 5;\n3: Obtain the distribution of q(˜t|t; τ) from T ac-\ncording to Equation 7;\n4: for i = 1 to N do\n5:\n˜G(i) ←M(i)\nθ (M(i)\nθ (G));\n6:\n˜T (i) ←M(i)\nθ (M(i)\nθ (T ));\n7:\nCompute LG\nRML using ˜G(i) and G according\nto Equation 6;\n8:\nCompute LT\nRML using ˜T (i) and T accord-\ning to Equation 8;\n9:\nL ←LG\nRML + LT\nRML;\n10:\nFine-tune M(i)\nθ\nwith L and obtain M(i+1)\nθ\n;\n11: end for\n12: return M(N+1)\nθ\n;\n4\nExperiments\nThis section conducts a series of experiments to\nevaluate the performance of INFINITY. We ﬁrst in-\ntroduce the datasets and baselines, then we provide\nthe comparison results. At last, we implement ex-\ntensive analytical experiments, including ablation\nanalysis and case study.\n4.1\nDatasets\nSince our task is unsupervised, datasets with ex-\nternal information except for graphs and texts are\nnot in our consideration. Thus, we select WebNLG\n(2017) (Gardent et al., 2017) and GenWiki (Jin\net al., 2020) as our benchmarks, which can evaluate\nG2T and T2G models at the same time. WebNLG\nis widely used in text generation and relation extrac-\ntion, where each graph contains about 2 to 7 triples.\nGenWiki is a new resource for unsupervised G2T\ngeneration, and we select two large domains (i.e.,\nSports and Games) of GenWiki. Tabel 1 presents\nthe detailed statistics of these two datasets.\nDataset\nTrain\nValid\nTest\nRelation Types\nWebNLG\n18,102\n872\n1,862\n373\nGenWiki\n48,020\n1,000\n10,000\n250\nTable 1: Statistics of benchmarks.\n4.2\nBaselines\n4.2.1\nSupervised Baselines\nThe intended application of INFINITY is in unsu-\npervised scenarios. Thus, only related methods\nare considered. For G2T, we compare our model\nwith a wide selection of PLM-free and PLM-based\nmethods. PLM-free models include StrongNeu-\nral, BestPlan (Moryossef et al., 2019), Graph-\nWriter (Koncel-Kedziorski et al., 2019), and Plan-\nner (Zhao et al., 2020), where BestPlan and Plan-\nner design different planners to order triples before\nlinearization. PLM-based models include T5-base\nand T5-large (Ribeiro et al., 2021). As to T2G, we\nchoose OnePass (Wang et al., 2019) and a state-of-\nthe-art triple extraction model CGT (Ye et al., 2021)\nas our baselines. Moreover, we also implement\na supervised version of INFINITY with aligned\ngraph-text pairs, which serves as a reference for\nthe upper bound of our unsupervised model. The\nsupervised loss is:\nLsup = E(g,t)∈G×T [−log P(g|t) −log P(t|g)].\n(10)\n4.2.2\nUnsupervised Baselines\nDue to the limited research on unsupervised joint\ntraining, we selected all unsupervised models as\nbaselines. Rule-Based (Schmitt et al., 2020) em-\nploys a heuristic algorithm to extract facts and\nconcatenate text of each triplet. Graph-Text Back\nTranslator (GT-BT) (Schmitt et al., 2020) adopts\na series of denoising methods and applies a back-\ntranslation model with a POS tagger as external\ntool. CycleGT (Guo et al., 2020b) jointly trains\nboth tasks via cycle training, where the T2G is sim-\npliﬁed to the relation classiﬁcation task with given\nentities.\n4.3\nTraining Settings and Evaluation Metrics\nWe employ Adam as the optimizer. The beam\nsize is set to 4 for both tasks. The learning rate\nis set to 1e-4. For G2T, we adopt several widely\nused automatic metrics, i.e., BLEU (Papineni et al.,\n2002), Meteor (Banerjee and Lavie, 2005), and\nCIDEr (Vedantam et al., 2015). BLEU and Me-\nteor consider precision, recall, or F-score between\ngenerated and ground truth texts while CIDEr cal-\nculates the TF-IDF weights for each n-gram. For\nT2G, we use the micro F1 score to evaluate the\nquality of the generated triples. F1 results of enti-\nties and triples are provided. We select part of the\nabove metrics to show due to space limitations.\n4.4\nWebNLG Results\n4.4.1\nG2T Results\nTable 2 presents the results of G2T task on the\nWebNLG dataset.\nFor fairness, we report the\nresults of INFINITY without applying RML to\ntexts, which is also analyzed in the ablation section.\nIt can be seen that our proposed method outper-\nforms all other unsupervised baselines. The BLEU\nscore of INFINITY is 2 points higher than CycleGT,\nwhich is even better than the level of some super-\nvised models. Moreover, the performance of the\nsupervised INFINITY is on par with T5-base and\nT5-large, and the supervised version can even deal\nwith the T2G problem, which can be attributed to\nthe power of PLM and the joint optimization for\nthe shared latent space.\nBLEU\nMETEOR\nCIDEr\nSupervised Models (G2T)\nStrongNeural\n46.5\n0.39\n2.87\nBestPlan\n47.4\n0.39\n2.69\nGraphWriter\n45.8\n0.36\n3.14\nPlanner\n52.9\n0.45\n3.72\nT5-base\n59.1\n0.44\n4.02\nT5-large\n59.3\n0.44\n4.03\nSupervised INFINITY\n58.8\n0.44\n3.99\nUnsupervised Models (Given Entities / External Tools)\nRule-Based\n18.3\n0.34\n-\nGT-BT\n37.7\n0.36\n-\nCycleGT\n55.5\n0.44\n3.81\nUnsupervised Models\nINFINITY\n58.0\n0.44\n3.89\nTable 2: G2T performance comparisons of different\nmodels on WebNLG dataset. CIDEr results and cor-\nresponding codes are not provided in Rule-Based and\nGT-BT.\n4.4.2\nT2G Results\nFor the T2G task, it should be mentioned that the\ncompared three unsupervised models RuleBased,\nGT-BT, and CycleGT, are given entities as a rela-\ntion classiﬁcation task, so they have a 100% F1\nscore of entities naturally and cannot employ RML\nloss for graph sequences. As can be seen from Ta-\nble 3, our model’s F1 (triple) score is 61.7, which\nis superior to all other unsupervised models un-\nder the circumstance that all entities are unknown.\nRule-Based model cannot extract any triples. Our\nsupervised INFINITY shows better results than the\nunsupervised one in terms of entity recognition,\nwhereas its performance is inferior to other super-\nvised methods since our model only uses the T5-\nbase PLM and does not design other sophisticated\nmodules.\nF1 (entity)\nF1 (triple)\nSupervised Models (T2G)\nOnePass\nNA\n66.2\nCGT\nNA\n83.4\nSupervised INFINITY\n95.0\n59.3\nUnsupervised Models (Given Entities / External Tools)\nRule-Based\n100.0\n0.0\nGT-BT\n100.0\n39.1\nCycleGT\n100.0\n58.4\nUnsupervised Models\nINFINITY\n93.9\n61.7\nTable 3: T2G performance comparisons of different\nmodels on WebNLG dataset. NA means the model is\nnot applicable to extract entities.\n4.5\nGenWiki Results\nUnlike the WebNLG dataset, GenWiki is specially\ncollected for unsupervised G2T tasks, where graph\nelements do not necessarily exist in the text. More-\nover, the entities extracted from the text are also\nnot necessarily contained in the ground truth graph,\nwhich makes it challenging to generate informa-\ntive outputs. Hence, some supervised baselines are\nnot applicable to this dataset. Since the codes of\nRule-Based and GT-BT (Schmitt et al., 2020) are\nnot provided, we use our implemented Rule-Based\nmodel as the baseline. In Table 4, our proposed\nmethod shows better results than GraphWriter and\nRule-Based model, but the BLEU value of INFIN-\nITY is lower than CycleGT. The reason is that Cy-\ncleGT has known all tokens of entities and relation\ntypes for T2G task, which can be used as external\ninformation to achieve better performance during\nthe training process. As a result, INFINITY can\nonly generate the tokens of entities and relations\nthat appear in the original texts. In other words,\nour model may substitute the ground truth tokens\nwith other words but remain the similar meanings.\nFor example, the original relation birthYear may\nbe predicted as birthDay in INFINITY.\n4.6\nDetailed Analysis\n4.6.1\nAblation Study\nWe use the WebNLG dataset for ablation analy-\nsis. As shown in Table 5, the supervised INFINITY\nshows the best results on the G2T task while the per-\nG2T\nT2G\nBLEU CIDEr F1 (triple)\nSupervised Models\nGraphWriter\n29.7\n2.68\nNA\nT5-base\n45.7\n3.74\nNA\nT5-large\n47.1\n3.74\nNA\nSupervised INFINITY\n43.6\n3.44\n33.8\nUnsupervised Models (Given Entities / External Tools)\nRule-Based (our implementation) 13.9\n1.26\n0.0\nCycleGT\n38.5\n3.50\n34.2\nUnsupervised Models\nINFINITY\n34.3\n2.50\n23.4\nTable 4: G2T and T2G performance comparisons of\ndifferent models on GenWiki dataset.\nformance of INFINITY without reward augmented\nlosses (w/o RML) is worse than any other versions,\nespecially for T2G. Applying reward augmented\nloss to both text and graph makes the model cap-\nture more order and structural information in the\ndatasets, and it obtains signiﬁcant improvement.\nWe also evaluate variants that only adopt one side\nreward augmented loss. INFINITY with RML for\ngraph demonstrates the best performance except\nfor the supervised one. This is because the PLM\nitself performs well on texts, and the improvement\nof RML for text is limited. Therefore, we use the\nversion with RML for graph as our ﬁnal reported\nmodel.\nG2T\nT2G\nBLEU\nCIDEr\nF1 (triple)\nSupervised INFINITY\n58.8\n3.99\n59.3\nw/o RML\n54.3\n3.58\n51.5\nw. RML for text & graph\n57.3\n3.89\n59.7\nw. RML for text\n56.2\n3.67\n53.8\nw. RML for graph (ours)\n58.0\n3.89\n61.7\nTable 5: Ablation analysis on WebNLG dataset. The\nversion with RML for graph is used as our reported re-\nsults.\n4.6.2\nAnalysis of Cross Learning\nAs mentioned earlier, we assume T and G share\nthe same latent content. In the same dataset, T\nand G have the same domain knowledge, whereas\ndifferent datasets can only share the language. In\nthe latter case, to analyze the scalability of INFIN-\nITY, we introduce cross learning where we only\nuse the graph (or text) data of WebNLG and text\n(or graph) corpus of GenWiki for training. Ta-\nble 6 shows the results, where dataset.G means\nthe graph in dataset while dataset.T denotes the\ntext in dataset. We can see INFINITY works well\nunder the setting of cross learning, which cannot be\naccomplished by other unsupervised models such\nWebNLG\nGenWiki\nG2T\nT2G\nG2T\nT2G\nBLEU\nCIDEr\nF1 (entity)\nF1 (triple)\nBLEU\nCIDEr\nF1 (entity)\nF1 (triple)\nWebNLG.G × GenWiki.T\n34.8\n2.04\n89.1\n45.2\n21.6\n1.41\n59.2\n1.2\nWebNLG.T × GenWiki.G\n45.6\n2.82\n91.9\n19.5\n16.1\n1.13\n65.6\n9.1\nWebNLG\n58.0\n3.89\n93.9\n61.7\nNA\nNA\nNA\nNA\nGenWiki\nNA\nNA\nNA\nNA\n34.3\n2.50\n97.0\n23.4\nTable 6: Analysis of cross learning on WebNLG and GenWiki datasets. dataset.G means the graph data in dataset\nand dataset.T denotes the text corpus in dataset. The last two rows are the results of training with the graphs and\ntexts on a single dataset, where they are only evaluated on their corresponding benchmark.\nInstance\nG.T. Text\nArlington in Texas is located at 184.0 metres above sea level and has a total area of 258.2 square kilometres.\nGen. Text\nArlington, Texas is 184.0 above sea level and has a total area of 258.2 square kilometres.\nG.T. Graph\n[H] Arlington Texas [R] elevation Above The Sea Level [T] 184.0\n[H] Arlington Texas [R] area Total [T] 258.2 square kilometres [E]\nGen. Graph\n[H] Arlington Texas [R] elevation Above The Sea Level [T] 184.0\n[H] Arlington Texas [R] area Total [T] 258.2 square kilometres [E]\nG.T. Text\nThe Aston Martin V8, manufactured by Aston Martin, is related to the Aston Martin DBS and was succeeded\nby the Aston Martin Vantage. Its engine volume is 5.3 litres. and it is assembled at Newport Pagnell.\nGen. Text\nThe Aston Martin V8, with a 5.3 litre engine, is a related transport vehicle to the Aston Martin DBS.\nIt is the successor to the Newport Pagnell Aston Martin Vantage.\nG.T. Graph\n[H] Aston Martin V8 [R] related Mean Of Transportation [T] Aston Martin DBS\n[H] Aston Martin DBS [R] successor [T] Aston Martin Vantage\n[H] Aston Martin V8 [R] assembly [T] Newport Pagnell\n[H] Aston Martin V8 [R] engine [T] 5.3 litres\n[H] Aston Martin V8 [R] manufacturer [T] Aston Martin [E]\nGen. Graph\n[H] Aston Martin V8 [R] manufacturer [T] Aston Martin\n[H] Aston Martin DBS [R] succeeded By [T] Aston Martin Vantage\n[H] AstonMartin V8 [R] engine Volume [T] 5.3 litres\n[H] Aston Martin assembly location [R] Newport Pagnell [E]\nTable 7: Case Study on WebNLG dataset. G.T. means ground truth and Gen. means generated.\nas CycleGT since they require entities and relation\ntypes for both tasks. However, the T2G perfor-\nmance of GenWiki is worse than WebNLG because\nthe tokens of relations and texts rarely overlap in\nGenWiki. In summary, INFINITY provides a low-\nresource approach to deploy on different datasets\nfor application. For example, in the absence of\na corresponding graph corpus, we can use public\nknowledge graphs to train INFINITY model so as\nto extract graph triples from any given English lit-\nerature.\n4.6.3\nCase Study and Error Analysis\nTo analyze the generation performance and draw-\nbacks of INFINITY, we select two representative\ninstances shown in Table 7, where the ground truth\nand generated sequences are provided. As to the\nﬁrst case, the generated text is consistent with the\nground truth, with only slight differences, and the\ngenerated triples are exactly the same as the real\nones. The second instance contains two sentences\nand ﬁve triples. The order of the generated text\nis inconsistent with the original text, and there are\nsome semantic errors. The generated triples are all\nreasonable but miss the ﬁrst fact. The boundary\nof the last generated triple is wrong, where [T] is\nmissing.\n5\nConclusion and Future Work\nIn this paper, we propose INFINITY, a simple unsu-\npervised approach to graph-text mutual conversion.\nThe key idea of INFINITY is to utilize one seq2seq-\nbased PLM to converse graphs and texts from each\nother with the framework of back-translation. Un-\nlike existing unsupervised methods, our model re-\nquires no additional external information or tools\nbeyond the non-parallel graph and text corpus, so\nit is easy to be quickly deployed to industrial sce-\nnarios. Experimental results show that INFINITY\nachieves promising results compared to state-of-\nthe-art baselines.\nFor future work, we plan to\nexplore the capability of prompt learning by ap-\npealing to precise controls over different layers in\nPLMs.\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In ACL Work-\nshop.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. 2015.\nScheduled sampling for se-\nquence prediction with recurrent neural networks.\nIn ICONIP.\nJordan Clive, Kris Cao, and Marek Rei. 2021. Con-\ntrol preﬁxes for text generation.\narXiv preprint\narXiv:2110.08329.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017.\nThe webnlg\nchallenge: Generating text from rdf data. In ICNLG.\nQipeng Guo, Zhijing Jin, Ning Dai, Xipeng Qiu, Xi-\nangyang Xue, David Wipf, and Zheng Zhang. 2020a.\n\\cal p2: A plan-and-pretrain approach for knowl-\nedge graph-to-text generation: A plan-and-pretrain\napproach for knowledge graph-to-text generation.\nIn INLG.\nQipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang,\nDavid Wipf, and Zheng Zhang. 2020b.\nCyclegt:\nUnsupervised graph-to-text and text-to-graph gener-\nation via cycle training. In INLG Workshop.\nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Mart-\ntinen, and S Yu Philip. 2021. A survey on knowl-\nedge graphs: Representation, acquisition, and appli-\ncations. IEEE Transactions on Neural Networks and\nLearning Systems.\nZhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng\nZhang. 2020.\nGenwiki: A dataset of 1.3 million\ncontent-sharing text and graphs for unsupervised\ngraph-to-text generation. In COLING.\nPei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Lin-\nfeng Song, Xiaoyan Zhu, and Minlie Huang. 2021.\nJointgt: Graph-text joint representation learning for\ntext generation from knowledge graphs.\narXiv\npreprint arXiv:2106.10502.\nRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan,\nMirella Lapata, and Hannaneh Hajishirzi. 2019.\nText generation from knowledge graphs with graph\ntransformers. In NAACL.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018.\nPhrase-based & neural unsupervised machine trans-\nlation. In EMNLP.\nMike\nLewis,\nYinhan\nLiu,\nNaman\nGoyal,\nMar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2020.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. In ACL.\nDiego Marcheggiani and Laura Perez-Beltrachini.\n2018. Deep graph convolutional encoders for struc-\ntured data to text generation. In ICNLG.\nAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.\nStep-by-step: Separating planning from realization\nin neural data-to-text generation. In NAACL.\nMohammad Norouzi, Samy Bengio, Navdeep Jaitly,\nMike Schuster, Yonghui Wu, Dale Schuurmans, et al.\n2016. Reward augmented maximum likelihood for\nneural structured prediction. NeurIPS.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In ACL.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nLeonardo FR Ribeiro, Martin Schmitt, Hinrich Schütze,\nand Iryna Gurevych. 2021. Investigating pretrained\nlanguage models for graph-to-text generation.\nIn\nEMNLP.\nMartin Schmitt, Sahand Sharifzadeh, Volker Tresp, and\nHinrich Schütze. 2020. An unsupervised joint sys-\ntem for text generation from knowledge graphs and\nsemantic parsing. In EMNLP.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015.\nCider: Consensus-based image de-\nscription evaluation. In CVPR.\nHaoyu Wang, Ming Tan, Mo Yu, Shiyu Chang, Dakuo\nWang, Kun Xu, Xiaoxiao Guo, and Saloni Potdar.\n2019. Extracting multiple-relations in one-pass with\npre-trained transformers. In ACL.\nZhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and\nYi Chang. 2020.\nA novel cascade binary tagging\nframework for relational triple extraction. In ACL.\nZhiheng Yan, Chong Zhang, Jinlan Fu, Qi Zhang, and\nZhongyu Wei. 2021. A partition ﬁlter network for\njoint entity and relation extraction. In EMNLP.\nHongbin Ye, Ningyu Zhang, Shumin Deng, Mosha\nChen, Chuanqi Tan, Fei Huang, and Huajun Chen.\n2021. Contrastive triple extraction with generative\ntransformer. In AAAI.\nXiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu,\nand Jun Zhao. 2018. Extracting relational facts by\nan end-to-end neural model with copy mechanism.\nIn ACL.\nChao Zhao, Marilyn Walker, and Snigdha Chaturvedi.\n2020. Bridging the structural gap between encoding\nand decoding for data-to-text generation. In ACL.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2022-09-22",
  "updated": "2022-09-22"
}