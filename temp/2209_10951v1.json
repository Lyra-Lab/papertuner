{
  "id": "http://arxiv.org/abs/2209.10951v1",
  "title": "An Information Minimization Based Contrastive Learning Model for Unsupervised Sentence Embeddings Learning",
  "authors": [
    "Shaobin Chen",
    "Jie Zhou",
    "Yuling Sun",
    "Liang He"
  ],
  "abstract": "Unsupervised sentence embeddings learning has been recently dominated by\ncontrastive learning methods (e.g., SimCSE), which keep positive pairs similar\nand push negative pairs apart. The contrast operation aims to keep as much\ninformation as possible by maximizing the mutual information between positive\ninstances, which leads to redundant information in sentence embedding. To\naddress this problem, we present an information minimization based contrastive\nlearning (InforMin-CL) model to retain the useful information and discard the\nredundant information by maximizing the mutual information and minimizing the\ninformation entropy between positive instances meanwhile for unsupervised\nsentence representation learning. Specifically, we find that information\nminimization can be achieved by simple contrast and reconstruction objectives.\nThe reconstruction operation reconstitutes the positive instance via the other\npositive instance to minimize the information entropy between positive\ninstances. We evaluate our model on fourteen downstream tasks, including both\nsupervised and unsupervised (semantic textual similarity) tasks. Extensive\nexperimental results show that our InforMin-CL obtains a state-of-the-art\nperformance.",
  "text": "An Information Minimization Based Contrastive Learning Model for\nUnsupervised Sentence Embeddings Learning\nShaobin Chen1, Jie Zhou2, Yuling Sun1∗and Liang He1\n1School of Computer Science and Technology, East China Normal University Shanghai, China\n2School of Computer Science, Fudan Univerisity Shanghai, China\nshaobin_chen@stu.ecnu.edu.cn, jie_zhou@fudan.edu.cn, {ylsun, lhe}@cs.ecnu.edu.cn\nAbstract\nUnsupervised sentence embeddings learning\nhas been recently dominated by contrastive\nlearning methods (e.g., SimCSE), which keep\npositive pairs similar and push negative pairs\napart. The contrast operation aims to keep as\nmuch information as possible by maximizing\nthe mutual information between positive in-\nstances, which leads to redundant information\nin sentence embedding. To address this prob-\nlem, we present an information minimization\nbased contrastive learning (InforMin-CL)\nmodel to retain the useful information and dis-\ncard the redundant information by maximizing\nthe mutual information and minimizing the in-\nformation entropy between positive instances\nmeanwhile for unsupervised sentence repre-\nsentation learning. Speciﬁcally, we ﬁnd that\ninformation minimization can be achieved by\nsimple contrast and reconstruction objectives.\nThe reconstruction operation reconstitutes the\npositive instance via the other positive instance\nto minimize the information entropy between\npositive instances. We evaluate our model on\nfourteen downstream tasks, including both su-\npervised and unsupervised (semantic textual\nsimilarity) tasks. Extensive experimental re-\nsults show that our InforMin-CL obtains\na state-of-the-art performance. Code is made\navailable. 1\n1\nIntroduction\nHow to learn universal sentence embeddings by\nlarge-scale pre-trained models (Devlin et al., 2019;\nLiu et al., 2019), such as BERT, has been studied ex-\ntensively in the literature (Gao et al., 2021; Reimers\nand Gurevych, 2019). Recently, contrastive learn-\ning has been used widely to learn better sentence\nembeddings (Meng et al., 2021; Gao et al., 2021).\nGenerally, contrastive learning uses various data\naugmentation methods to generate different views\n∗Yuling Sun is the corresponding authors of this paper.\n1https://github.com/Bin199/InforMin-CL\nTable 1: Training texts may contain various kinds of\nredundant information, such as stop words, restatement,\ncapitalization, and hyphen.\nOriginal\nWhere is the party, it sounds great.\nStop words\nWhere is the party, it sounds great.\nRestatement\nThe party sounds great, where is it.\nCapitalization\nWhere Is The Party, It Sounds Great.\nHyphen\nWhere-is-the-party, it-sounds-great.\nof the input sentences and samples positive in-\nstances and negative instances from views. Con-\ntrastive learning aims to learn effective embeddings\nby pulling positive instances together and pushing\npositive and negative instances apart. This opera-\ntion focus on maximizing the mutual information\nbetween positive instances to retain as much infor-\nmation as possible, which includes both the use-\nful and useless information. Previous studies like\n(Meng et al., 2021; Gao et al., 2021) ignore the\nredundant information stored in views, which has\na bad impact on the performance of downstream\ntasks, as proved by (Achille and Soatto, 2018; Tian\net al., 2020).\nTable 1 gives an example of redundant infor-\nmation stored in training texts. In training texts,\nthe sentences contain much redundant information\nwhich is not favorable to the downstream task. The\nredundant information may be stop words and the\nstyle of the sentence (e.g., restatement, capitaliza-\ntion, and hyphen). The existing study (Tian et al.,\n2020) also shows that discarding redundant infor-\nmation in views can help to improve the perfor-\nmance of the downstream task. Thus, we arise the\nfollowing question: how to discard this redundant\ninformation by choosing the optimal views?\nIt is natural to solve the above questing via in-\nformation bottleneck (IB), which has been utilized\nas an effective and simple method for learning a\ngood embedding by keeping the important informa-\ntion and forgetting redundant information in vari-\narXiv:2209.10951v1  [cs.CL]  22 Sep 2022\nous tasks (Tishby et al., 2000; Chen and Ji, 2020;\nTishby and Zaslavsky, 2015). Prompted by this, we\nattend to solve the problem by drawing inspiration\nfrom the information minimization principle (an\nidea in IB theory) (Tian et al., 2020): A good set of\nviews share the minimal information necessary to\nperform well at the downstream task. This method\naims to retain useful information and forget redun-\ndant information. In this paper, we explore how to\naddress the shortcomings (i.e., ignoring the redun-\ndant information stored in views) of previous work\nfor unsupervised sentence representations via the\ninformation minimization principle.\nWe propose an information minimization based\ncontrastive learning (InforMin-CL) model\nfor unsupervised sentence embedding learning.\nInforMin-CL incorporates the information min-\nimization principle into contrastive learning to\nnot only learn the important information but also\ndrop the redundant information.\nWe optimize\nour InforMin-CL model from two perspectives:\ncontrast and reconstruction. Firstly, we learn the\nuseful information by a contrast task to maximize\nthe mutual information. This task manages to at-\ntract positive pairs and repulse negative pairs. At-\ntracting positive pairs stands for maximizing mu-\ntual information between positive instances. Sec-\nondly, we propose a reconstruction task that encour-\nages the model to reconstruct the representation of\nthe positive instance via the other one in the same\npair.\nThe algorithm of InforMin-CL is easy to un-\nderstand and can be implemented with just sev-\neral lines of code. Moreover, our method does\nnot change the major network structure, so it is\nmodel-agnostic and can be applied to any repre-\nsentation learning neural networks based on con-\ntrastive learning. Experiments in Section 4 show\nthat InforMin-CL can help the model learn ef-\nfective representations that improve downstream\ntask performance. Our main contributions are sum-\nmarized as follows:\n• We propose an InforMin-CL model to\nlearn good sentence embeddings by keeping\nuseful information and getting rid of redun-\ndant information between positive instances.\n• We achieve our model via simple contrast and\nreconstruction tasks and prove that the recon-\nstruction task can drop redundant information\nby minimizing the information entropy.\n• We achieve new state-of-the-art results on\nseven supervised tasks and seven unsuper-\nvised tasks, which indicates the great advan-\ntages of our proposed model.\n2\nRelated Work\n2.1\nSentence Representation Learning\nLearning sentence embeddings as an important\nproblem in NLP has been widely studied. Recent\nwork focus on leveraging the power of BERT (De-\nvlin et al., 2019) to learn effective sentence embed-\ndings, which are free from artiﬁcially supervised\nsignals. BERT-ﬂow (Li et al., 2020) transforms the\nanisotropic sentence embedding distribution into a\nsmooth isotropic Gaussian distribution through nor-\nmalizing ﬂows. BERT-whitening (Su et al., 2021)\nfurther presents a whitening operation to enhance\nthe isotropy of sentence embeddings and achieves\nbetter results.\nThen, the contrastive learning approach is ap-\nplied for sentence embedding learning. IS-BERT\n(Zhang et al., 2020) proposes a model with a fea-\nture extractor on top of BERT and an objective that\nmaximizes the mutual information between global\nsentence embeddings and local sentence embed-\ndings. CLEAR (Wu et al., 2020) employs multi-\nple sentence-level augmentation strategies to learn\nsentence representation. Coco-LM (Meng et al.,\n2021) employs an auxiliary language model to cor-\nrupt text sequences, upon which it constructs a\ntoken-level task and a sequence-level task for pre-\ntraining the main model. Gao et al. (Gao et al.,\n2021) presents an unsupervised approach that pre-\ndicts input itself with dropout noise and a super-\nvised approach utilizing natural language inference\ndatasets. SCD (Klein and Nabi, 2022) leverages\nthe self-contrast of augmented samples obtained by\ndropout, which eliminates the reliance on negative\npairs. However, all these studies lack consider-\nation of discarding redundant information stored\nin views. In our work, we consider and solve the\nproblem in the framework of the information mini-\nmization principle.\n2.2\nInformation Minimization Principle\nInformation minimization principle (Tian et al.,\n2020) has been proposed to retain the minimal in-\nformation necessary. In recent years, researchers\nutilize the information minimization principle to\nimprove image representations (Tian et al., 2020;\nPlay some music using slacker\nWhat is party all night\nPlease check the movie schedule\nModel\nPositive instance\nNegative instance\nDifferent dropout masks in two forward \npasses to get a positive pair\n...\n...\n...\nNegative instance\n...\n...\n...\n...\n...\n(Shared information)\n(“InfoMin Principle”: Discard \nredundant information not shared) \n...\n...\n𝐿𝑅= Ε −𝑧1 −𝑧2\n2\n2\n𝑧2\nKey information shared\nRedundant information not shared\nRedundant information shared\nMutual Information Maximization\nM O D E L\n𝐿𝐶= max Ε 1\n𝑛෍\n𝑖=1\n𝑛\n𝑙𝑜𝑔\n𝑒\nൗ\n𝑠𝑖𝑚𝑧𝑖\n1,𝑧𝑖\n2\n𝜏\n1\n𝑛σ𝑘=1\n𝑛\n𝑒\nൗ\n𝑠𝑖𝑚𝑧𝑖\n1,𝑧𝑘\n2\n𝜏\nReconstruction\nContrast\nShare some information\nDetails of Reconstruction\n𝑧1\n𝑧2\n𝑧2\n𝑧2\n𝑧1\n𝑧1\n𝑧1\nFigure 1: The architecture of our proposed framework. Contrast: Negative pairs are pushed apart while positive\npairs are pulled together, which suggests maximizing the mutual information between positive instances. Recon-\nstruction: We drop redundant information in z1 not shared with z2 (marked by the green point) by reconstructing\none positive instance via the other positive instance.\nTsai et al., 2021). Furthermore, information bottle-\nneck is used to improve the interpretability of the\nattention-based models (Zhou et al., 2021). Tian\net al. (Tian et al., 2020) shows good views for a\ngiven task in a contrastive representation learning\nframework should retain task-relevant information\nwhile minimizing irrelevant nuisances. However,\nit focuses on eliminating the task-irrelevant infor-\nmation via downstream datasets. Tsai et al. (Tsai\net al., 2021) focuses on the multi-view setting be-\ntween input and self-supervised signals and adopts\nself-supervised signals to reconstruct learned rep-\nresentations to discard task-irrelevant information.\nOur work also differs from (Tsai et al., 2021) in two\nperspectives: 1) we discard redundant information\nin texts while (Tsai et al., 2021) drops noise infor-\nmation stored in images, and additionally different\nself-supervised signals are used in two work; 2)\n(Tsai et al., 2021) validates their method by learn-\ning visual features evaluated by supervised tasks\nwhile sentence embedding learning evaluates em-\nbeddings via not only supervised tasks but also\nunsupervised tasks.\n3\nMethod\nWe propose an InforMin-CL model for unsu-\npervised sentence representation learning (Figure\n1). There are two main steps, contrast and recon-\nstruction. We ﬁrst present the contrast objective to\nlearn the useful information: we push apart posi-\ntive instances and negative instances while pulling\npositive instances together, which implies maximiz-\ning mutual information between positive instances.\nLater we present the reconstruction task to drop the\nuseless information: we minimize the conditional\ninformation entropy of one positive instance given\nthe other positive instance. Algorithm 1 provides\nthe pseudo-code of InforMin-CL.\n3.1\nContrast\nContrastive learning attends to learning effective\nrepresentation by pulling positive sample pairs to-\ngether and pushing apart negative sample pairs. We\nbuild upon the recent success of unsupervised Sim-\nCSE (Gao et al., 2021) and take the embeddings\nderived from the same sentence with independently\ndifferent dropout masks as positive instances. We\nadopt the dropout mask (with default dropout prob-\nability p = 0.1) as an augmentation skill, which is\nproved to outperform other skills (Gao et al., 2021),\nto obtain positive pairs. The positive pair takes the\nsame sentence, and their embeddings only differ\nin dropout masks. Other sentences in the same\nmini-batch are seen as negative instances.\nWe denote input, instance, and self-supervised\nsignal as X, Z, and S. We feed the same input x\nAlgorithm 1 Pseudocode of InforMin-CL in a\nPyTorch-like style.\nInput: batch size N, temperature τ, structure of f and Γ.\nOutput: encoder network f (·).\nfor sampled minibatch {xk}N\nk=1 do\nfor all k ∈{1, . . . , N} do\ndraw two augmentation functions t ∼Γ, t′ ∼Γ\n˜x2k−1 = t (xk)\nz2k−1 = f (˜x2k−1)\n˜x2k = t′ (xk)\nz2k = f (˜x2k)\nend for\nfor all i ∈{1, . . . , 2N} and j ∈{1, . . . , 2N} do\nsi,j = zT\ni zj/ (∥zi∥∥zj∥)\nend for\nLC = max E\n\n1\nN\nN\nP\ni=1\nlog\nexp(s2i−1,2i/τ)\n1\nN\nN\nP\nk=1\nexp(s2i−1,2k/τ)\n\n\nLR = E\n\u0002\n−∥z2k−1 −z2k∥2\n2\n\u0003\nL = LC + LR\nupdate f to minimize L\nend for\nto the encoder twice by applying different dropout\nmasks and then get positive instances z1 and z2.\nIn our work, we take one instance z2 in positive\npair as a self-supervised signal. The information\nrequired for downstream tasks is referred to as “key\ninformation\": T. I and H represent mutual infor-\nmation and information entropy.\nLet Zsup be the sufﬁcient supervised represen-\ntation and Zsupmin be the minimal and sufﬁcient\nsupervised representation:\nZsup = arg max\nZ\nI (Z; T)\nZsupmin = arg min\nZ\nH (Z|T)\ns.t. I (Z; T) is maximized\n(1)\nLet Zssl be the sufﬁcient self-supervised repre-\nsentation and Zsslmin be the minimal and sufﬁcient\nself-supervised representation:\nZssl = arg max\nZ\nI (Z; S)\nZsslmin = arg min\nZ\nH (Z|S)\ns.t. I (Z; S) is maximized\n(2)\nThen, we give theorem 1. For proof, please refer\nto (Tsai et al., 2021).\nTheorem 1 The supervised learned representations\ncontain all the key information in the input (i.e.\nI (X; T)). The self-supervised learned representa-\ntions contain all the key information in the input\nwith a potential loss ε:\nI (X; T) = I (Zsup; T) = I (Zsupmin; T)\n≥I\n\u0010\nZssl; T\n\u0011\n≥I\n\u0010\nZsslmin; T\n\u0011\n≥I (X; T) −ε\n(3)\nThe contrastive learning objective maximizes\nthe dependency between positive instance z1 and\nself-supervised signal z2, which suggests maximiz-\ning the mutual information I\n\u0000z1; z2\u0001\n. Theorem 1\nsuggests that maximizing I\n\u0000z1; z2\u0001\nresults in z1\ncontaining almost all the information required for\ndownstream tasks from the input x. Note that T\nis utilized only for describing our method and in\npractice, no downstream datasets are used in the\npre-training phase.\nWe use a contrastive learning objective similar\nto that in (van den Oord et al., 2018), which is a\nmutual information lower bound with low variance:\nLC = max E\n\n1\nN\nN\nX\ni=1\nlog\ne\nsim(z1\ni ,z2\ni )/τ\n1\nN\nPN\nk=1 e\nsim(z1\ni ,z2\nk)/τ\n\n\n(4)\nwhere\n\u0000z1\n1, z2\n1\n\u0001\n, . . . ,\n\u0000z1\nN, z2\nN\n\u0001\n∼P N \u0000Z1, Z2\u0001\n,\nz1\ni , z2\ni are two positive instances of the i-th exam-\nples. N refers to batch size and P refers to the\nstatistical distribution of\n\u0000Z1, Z2\u0001\n.\n3.2\nReconstruction\nThe details of the reconstruction are illustrated in\nthe right of Figure 1. The positive instances z1 and\nz2 contain independent information while sharing\nsome information. The noise information (marked\nas the green point) in z1 is expected to be discarded\nby the reconstruction task. We prove that this task\ncan discard the useless information by minimizing\nthe information entropy.\nThe reconstruction task encourages the self-\nsupervised signal z2 to reconstruct the learned rep-\nresentation z1, which suggests maximizing the log\nconditional likelihood EPZ1,Z2\n\u0002\nlog P\n\u0000Z1|Z2\u0001\u0003\n.\nWe know that\n−H\n\u0000Z1|Z2\u0001\n= EPZ1,Z2\n\u0002\nlog P\n\u0000Z1|Z2\u0001\u0003\n(5)\nThus, this reconstruction also means minimizing\nH\n\u0000Z1|Z2\u0001\n.\nTheorem 2 The sufﬁcient self-supervised repre-\nsentation contains more redundant information\nin the input than the sufﬁcient and minimal self-\nsupervised representation. The latter contains an\namount of the information, I (X; S|T), that cannot\nbe discarded from the input:\nI\n\u0010\nZssl; X|T\n\u0011\n= I (X; S|T) + I\n\u0010\nZssl; X|S, T\n\u0011\n≥I\n\u0010\nZsslmin; X|T\n\u0011\n= I (X; S|T)\n≥I (Zsupmin; X|T) = 0\n(6)\nTheorem 2 (please refer to (Tsai et al., 2021)\nfor proof) indicates that Zssl contains two parts\nof redundant information while Zsslmin contains\none part of redundant information, discarding\nI\n\u0000Zssl; X|S, T\n\u0001\n.\nThus, if z2 can perfectly reconstruct z1 for any\n\u0000z1, z2\u0001\n∼PZ1,Z2\n(7)\nunder the constraint that I\n\u0000z1; z2\u0001\nis maximized,\nwe get z1sslmin according to Eq.\n2.\nAnd\nthen z1 discards redundant information, excluding\nI\n\u0000z1; z2|t\n\u0001\n(i.e., the amount of redundant informa-\ntion in the shared information between two positive\ninstances z1 and z2). For easier optimization, we\nuse EPZ1,Z2\n\u0002\nlog QΦ\n\u0000Z1|Z2\u0001\u0003\nas the lower bound\nof EPZ1,Z2\n\u0002\nlog P\n\u0000Z1|Z2\u0001\u0003\n. In our deployment,\nwe utilize the design in Eq. 4 and let QΦ\n\u0000Z1|Z2\u0001\nbe Gaussian N\n\u0000Z1|Z2, σI\n\u0001\nwith σI as a diagonal\nmatrix. Hence, we obtain the reconstruction objec-\ntive as follows:\nLR = Ez1,z2∼PZ1,Z2\nh\n−\n\r\rz1 −z2\r\r2\n2\ni\n(8)\nWe combine two objectives as a total objective:\nL = LC + λ ∗LR\n(9)\nwhere λ is a hyper-parameter. Training model with\nthe total loss enables us to discard redundant infor-\nmation in views.\n4\nExperiment\n4.1\nEvaluation Setup\nWe conduct our experiments on seven standard su-\npervised tasks and also seven unsupervised tasks.\nWe use the SentEval Toolkit (Conneau and Kiela,\n2018) for evaluation.\nFollowing (Reimers and\nGurevych, 2019; Gao et al., 2021), we take unsuper-\nvised tasks as the main comparison of the sentence\nembedding approaches and supervised results for\nreference.\nUnsupervised Tasks We evaluate representa-\ntions on seven semantic textual similarity (STS)\ntasks: STS 2012-2016 (Agirre et al., 2012, 2013,\n2014, 2015, 2016), STS Benchmark (Cer et al.,\n2017), and SICK-Relatedness (Marelli et al., 2014)\nand compute the cosine similarity between sen-\ntence embeddings. All the unsupervised experi-\nments are fully unsupervised, which means no STS\ntraining datasets are used and all embeddings are\nﬁxed once they are trained. For the sake of compa-\nrability, we follow the evaluation protocol of (Gao\net al., 2021), employing Spearman’s rank correla-\ntion and aggregation on all topic subsets.\nSupervised Tasks We evaluate representations\non seven supervised tasks: MR (Pang and Lee,\n2005), CR (Hu and Liu, 2004), SUBJ (Pang and\nLee, 2004), MPQA (Wiebe et al., 2005), SST-2\n(Socher et al., 2013), TREC (Voorhees and Tice,\n2000) and MRPC (Dolan and Brockett, 2005). A\nlogistic regression classiﬁer is trained on the top\nof (frozen) sentence embeddings produced by dif-\nferent methods. We follow default conﬁgurations\nfrom SentEval and use accuracy as the metric.\nTraining Details We start from pre-trained\nBERT (Devlin et al., 2019) (uncased) or RoBERTa\n(Liu et al., 2019) (cased). Similar to (Gao et al.,\n2021), we train our InforMin-CL in an unsu-\npervised fashion on 106 randomly sampled sen-\ntences from English Wikipedia. During training,\nwe add an MLP layer on the top of the [CLS] rep-\nresentation as sentence embeddings and directly\ntake the [CLS] representation as sentence embed-\ndings at testing time. A masked language mod-\neling (MLM) objective (Devlin et al., 2019) is\nadded as an optional auxiliary loss to the Eq. 9:\nL + β ∗LMLM (β is a hyper-parameter). For\nall results, we use the following hyper-parameters:\nepoch: 1, temperature τ: 0.05, optimizer: Adam\n(Kingma and Ba, 2015)). We carry out grid-search\nof batch size ∈{64, 128, 256} and learning rate ∈\n{1e −5, 3e −5, 5e −5} on STS-B development\nsets.\nDuring the training process, we save the\ncheckpoint with the highest score on the STS-B\ndevelopment set to ﬁnd the best hyperparameters.\nWe adopt the hyperparameter settings listed in Ta-\nble 3. For all results, we use a PC with a GeForce\nRTX 3090 GPU (CUDA 11, PyTorch 1.7.1).\n4.2\nMain Results\nBaselines We compare InforMin-CL to previ-\nous typical sentence embedding methods, which\nTable 2: Unsupervised task results (spearman’s correlation). †: results from (Gao et al., 2021). ♥: results from\n(Klein and Nabi, 2022). All other results are reproduced and reevaluated by ourselves.\nModel\nSTS12\nSTS13\nSTS14\nSTS15\nSTS16\nSTS-B\nSICK-R\nAvg.\nGloVe embeddings (avg.)†\n55.14\n70.66\n59.73\n68.25\n63.66\n58.02\n53.76\n61.32\nBERTbase (ﬁrst −last avg.) †\n39.70\n59.38\n49.67\n66.03\n66.19\n53.87\n62.06\n56.70\nBERTbase−ﬂow†\n58.40\n67.10\n60.85\n75.16\n71.22\n68.66\n64.47\n66.55\nBERTbase−whitening†\n57.83\n66.90\n60.90\n75.08\n71.31\n68.24\n63.73\n66.28\nIS −BERTbase\n†\n56.77\n69.24\n61.21\n75.23\n70.16\n69.21\n64.25\n66.58\nCT −BERTbase\n†\n61.63\n76.80\n68.47\n77.50\n76.48\n74.31\n69.19\n72.05\nSCD −BERTbase\n♥\n66.94\n78.03\n69.89\n78.73\n76.23\n76.30\n73.18\n74.19\nSimCSE −BERTbase\n67.01\n82.14\n73.76\n80.49\n79.01\n77.04\n69.94\n75.63\nInforMin-CL −BERTbase\n70.22\n83.48\n75.51\n81.72\n79.88\n79.27\n71.03\n77.30\nRoBERTabase (ﬁrst −last avg.)†\n40.88\n58.74\n49.07\n65.63\n61.48\n58.55\n61.63\n56.57\nRoBERTabase−whitening†\n46.99\n63.24\n57.23\n71.36\n68.99\n61.36\n62.91\n61.73\nDeCLUTR −RoBERTabase\n†\n52.41\n75.19\n65.52\n77.12\n78.63\n72.41\n68.62\n69.99\nSCD −RoBERTabase\n♥\n63.53\n77.79\n69.79\n80.21\n77.29\n76.55\n72.10\n73.89\nSimCSE −RoBERTabase\n70.32\n82.48\n74.84\n82.13\n82.14\n81.57\n68.62\n77.44\nInforMin-CL −RoBERTabase\n69.79\n82.57\n73.36\n80.91\n81.28\n81.07\n70.30\n77.04\nSimCSE −RoBERTalarge\n72.64\n83.78\n75.83\n84.24\n80.12\n81.10\n69.81\n78.22\nInforMin-CL −RoBERTalarge\n70.91\n84.20\n75.57\n82.26\n79.68\n81.10\n72.81\n78.08\nTable 3: Batch sizes, learning rates and λ adopted for\nInforMin-CL.\nBERT\nRoBERTa\nbase\nbase\nlarge\nBatch size\n128\n128\n128\nLearning rate\n3e-5\n1e-5\n3e-5\nλ\n0.4\n4\n4\ninclude averaging GloVe embeddings (Penning-\nton et al., 2014), Skip-thought (Kiros et al., 2015)\nand average BERT or RoBERTa embeddings. We\nalso compare to post-processing methods and meth-\nods using a contrastive objective. Post-processing\nmethods include BERT-ﬂow (Li et al., 2020) and\nBERT-whitening (Su et al., 2021). Methods using\na contrastive objective include IS-BERT (Zhang\net al., 2020), DeCLUTR (Giorgi et al., 2021), CT\n(Carlsson et al., 2021), SimCSE (Gao et al., 2021),\nand SCD (Klein and Nabi, 2022). IS-BERT maxi-\nmizes the agreement between global and local fea-\ntures. DeCLUTR takes different spans from the\nsame document as positive pairs. CT aligns embed-\ndings of the same sentence from two different en-\ncoders. SimCSE takes the embedding of the same\ninput with different dropouts as positive pairs. SCD\nleverages the self-contrast of augmented samples\nobtained by dropout.\nPerformance on Unsupervised Tasks Table\n2 shows the evaluation results on seven STS\ntasks.\nInforMin-CL achieves comparable or\nbetter results than previous state-of-the-art base-\nlines. For BERTbase based models, our method\noutperforms the best approach with a large margin\n(+1.67%) on average. For RoBERTa based model,\nInforMin-CL obtains comparable results (less\nthan 0.4 points in average). All these indicate that\nour model can improve the performance of down-\nstream tasks by forgetting the irrelevant informa-\ntion in pre-training phase.\nPerformance on Supervised Tasks Table 4\nshows the evaluation results on seven supervised\ntasks. Results indicate that InforMin-CL per-\nforms on par or better than all baselines.\nOur\nmethod achieves better results on most of and\neven all tasks using RoBERTa, with an aver-\nage gain 0.81% on RoBERTabase and 1.66% on\nRoBERTalarge respectively. With an MLM task\nadded, further gains on average results are observed\nfor BERT and RoBERTa. It raises the average\nscores of InforMin-CL from 85.52% to 86.96%\nfor BERTbase and from 86.11% to 87.01% for\nRoBERTabase. Particularly, InforMin-CL w/\nMLM obtains the outperforms all the baselines in\naverage.\nConsidering results of both supervised and un-\nsupervised tasks, we present the following ﬁnd-\nings: 1) BERT-based InforMin-CL performs\nbetter on unsupervised tasks; 2) RoBERTa-based\nInforMin-CL achieves better results on super-\nvised tasks. The difference in performance us-\nTable 4: Supervised task results (accuracy). †: results from (Gao et al., 2021). ♥: results from (Klein and Nabi,\n2022). All other results are reproduced and reevaluated by ourselves. w/ MLM: adding MLM as an auxiliary task\nwith β = 0.1.\nModel\nMR\nCR\nSUBJ\nMPQA\nSST\nTREC\nMRPC\nAvg.\nGloVe embeddings (avg.)†\n77.25\n78.30\n91.17\n87.85\n80.18\n83.00\n72.87\n81.52\nSkip −thought†\n76.50\n80.10\n93.60\n87.10\n82.00\n92.20\n73.00\n83.50\nAvg. BERT embeddings†\n78.66\n86.25\n94.37\n88.66\n84.40\n92.80\n69.54\n84.94\nBERT−[CLS] embeddings†\n78.68\n84.85\n94.21\n88.23\n84.13\n91.40\n71.13\n84.66\nIS −BERTbase\n†\n81.09\n87.18\n94.96\n88.75\n85.96\n88.64\n74.24\n85.83\nSCD −BERTbase\n♥\n73.21\n85.80\n99.56\n88.67\n85.59\n89.80\n75.71\n85.52\nSimCSE −BERTbase\n81.47\n86.86\n94.79\n89.25\n86.27\n89.40\n72.81\n85.84\nInforMin-CL −BERTbase\n80.99\n85.72\n94.63\n89.47\n85.67\n88.20\n73.97\n85.52\nw/ MLM\n82.87\n87.05\n95.22\n88.43\n87.15\n92.20\n75.77\n86.96\nSimCSE −RoBERTabase\n81.26\n87.36\n93.58\n87.56\n86.93\n84.80\n75.01\n85.21\nSCD −RoBERTabase\n♥\n82.17\n87.76\n93.67\n85.69\n88.19\n83.40\n76.23\n85.30\nInforMin-CL −RoBERTabase\n82.22\n88.08\n93.57\n87.75\n87.59\n86.60\n76.99\n86.11\nw/ MLM\n83.49\n88.69\n94.79\n86.81\n88.30\n89.40\n77.57\n87.01\nSimCSE −RoBERTalarge\n80.85\n85.99\n93.08\n87.65\n86.33\n89.00\n72.46\n85.05\nInforMin-CL −RoBERTalarge\n82.50\n88.32\n93.81\n89.38\n87.64\n90.80\n74.49\n86.71\ning BERT and RoBERTa is mainly caused by\nthe difference in pre-training corpus. BERT is\ntrained over 16 GB text (BooksCorpus (Zhu et al.,\n2015) and English Wikipedia) while RoBERTa\nis trained over totally 160 GB of uncompressed\ntext (BooksCorpus (Zhu et al., 2015), English\nWikipedia, CC-NEWS (Nagel, 2016), OPENWEB-\nTEXT (Gokaslan et al., 2019), and STORIES\n(Trinh and Le, 2018)). Thus, the diverse large-\nscale high quality datasets enhance the RoBERTa to\nlearn the important and useful information with lim-\nited parameters. InforMin-CL, which improves\nperformance by discarding redundant information,\nstruggles to represent its effects in this setting due\nto less noise information. This causes the unsu-\npervised results of InforMin-CL are similar to\ncompetitors for RoBERTa-based models.\n4.3\nAblation Study\nInﬂuence of λ\nWe investigate how different re-\nconstruction objectives with λ from 0.04 to 4 af-\nfect our model’s performance. We report the av-\nerage performance of unsupervised tasks and su-\npervised tasks in this experiment. The results are\nobtained using BERTbase. Results demonstrate\nthat InforMin-CL constantly works well over\nthis wide range of λ. As shown in Table 5, with\nincreasing λ, the performance of both unsupervised\nand supervised tasks rises ﬁrst and falls later.\nInﬂuence of β\nWe introduce one more optional\nvariant which adds a masked language modeling\n(MLM) objective to the Eq. 9: L + β ∗LMLM (β\nis a hyper-parameter). We analyze how different\nβ inﬂuence the performance on unsupervised and\nsupervised tasks. As we show in Table 6, we ﬁnd\nthat adding MLM objectives with different β con-\nsistently helps improve performance on supervised\ntasks but brings a signiﬁcant drop in STS tasks.\nInﬂuence of Batch Sizes\nTo explore the impact\nof batch sizes, we report the average performance\nof downstream tasks with batch sizes (N in Eq. 4)\nfrom 64 to 256. In this experiment, only batch\nsize changes while all other hyper-parameters keep\nunchanged. We use BERTbase to evaluate on the\ntest set of unsupervised and supervised tasks. As\nwe show in Table 7, we ﬁnd that InforMin-CL\nis not sensitive to batch size, similar to SimCSE,\nmainly caused by the good set of initial parameters.\nTable 5: Ablation studies of different hyper-parameters\nλ.\nThe results are based on the test sets using\nBERTbase.\nλ\nAvg. Sup\nAvg. Unsup\n0.04\n85.20\n76.09\n0.4\n85.52\n77.30\n4\n85.03\n77.18\n4.4\nUniformity and Alignment\nWe further conduct analysis to understand the inner\nworkings of InforMin-CL.\n40\n20\n0\n20\n40\n40\n20\n0\n20\n40\n60\n(a) SimCSE\n40\n20\n0\n20\n40\n40\n30\n20\n10\n0\n10\n20\n30\n40\n(b) SCD\n40\n20\n0\n20\n40\n60\n40\n20\n0\n20\n40\n(c) InforMin-CL\nFigure 2:\nThe t-SNE of sentence representations learned with SimCSE, SCD and InforMin-CL using\nBERTbase. The points are embeddings of sentences sampled from the IMDB dataset without ﬁne-tuning.\n4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\nuniformity\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nalignment\nSup. SimCSE +  \n(81.6)\nUnsup. SimCSE (75.6)\nAvg. BERT (56.7)\nSBERT +  (74.9)\nSBERT-flow +  (76.6)\nSBERT-whitening +  (77.0)\nBERT-flow (66.6)\nBERT-whitening (66.3)\nSCD (74.2)\nInforMin\nCL (77.3)\n60\n65\n70\n75\n80\nFigure 3: Quantitative analysis of embeddings - align-\nment vs. uniformity (the smaller, the better). The plot\nof models is based on BERTbase.\nPoints represent\naverage STS performance with Spearman’s correlation\ncolor coded (+ corresponds to supervised methods).\nQualitative Analysis\nAs shown in (Wang and\nIsola, 2020), the asymptotics of the contrastive\nlearning objective (4) can be expressed by the fol-\nlowing equation when the number of negative in-\nstances approaches inﬁnity:\nLC = max\nh 1\nN\nN\nX\ni=1\nE\n\u0002\nsim\n\u0000z1\ni , z2\ni\n\u0001\u000e\nτ\n\u0003\n−1\nN\nN\nX\ni=1\nE\n\u0002\nlog 1\nN\nN\nX\nk=1\nesim(z1\ni ,z2\nk)/τ\u0003i\n(10)\nThe ﬁrst term in square brackets in Eq. 10 im-\nproves alignment of the space. The alignment per-\nforms better when the similarity score rises. While\noptimizing the reconstruction objective, z1 and z2\nare pulled closer, which means that the similar-\nity score of z1 and z2 becomes higher. In other\nwords, InforMin-CL effectively improves align-\nTable 6: Ablation studies of the MLM objective based\non the test sets using BERTbase.\nModel\nAvg. Sup\nAvg. Unsup\nw/o MLM\n85.52\n77.30\nw/ MLM\nβ = 0.01\n86.46\n63.59\nβ = 0.1 (ours)\n86.96\n63.25\nβ = 1.0\n87.04\n60.85\nTable 7: Ablation studies of different batch sizes. The\nresults are based on test sets using BERTbase.\nBatch size\n64\n128\n256\nAvg. Sup\n85.38\n85.52\n85.77\nAvg. Unsup\n76.64\n77.30\n76.14\nment of pre-trained embeddings while keeping a\ngood uniformity, which is the key to the success\nof InforMin-CL. We also follow (Wang and\nIsola, 2020) to use uniformity and alignment to\nmeasure the quality of representation space for\nInforMin-CL and other models. Figure 3 shows\nuniformity and alignment of different sentence em-\nbedding models along with their STS averaged re-\nsults. InforMin-CL achieves the best in terms\nof alignment (0.143), which can be related to the\nstrong effect of the reconstruction objective. In\nterms of uniformity, InforMin-CL is slightly in-\nferior to unsupervised SimCSE. This is also re-\nﬂected in the ﬁnal results in the t-SNE plots.\nQuantitative Analysis\nThe t-SNE (Reif et al.,\n2019) plot in Figure 2 demonstrates the advantages\nof InforMin-CL. We sample 2000 sentences\nfrom IMDB (Maas et al., 2011) dataset and gen-\nerate the embeddings of sentences using SimCSE,\nSCD and InforMin-CL. We use K-Means (Jain and\nDubes, 1988) clustering to group similar sentence\nembeddings and form 10 clusters. Results indicate\nthat similar sentence pairs (marked by same colors)\ngenerated by InforMin-CL are more aligned.\n5\nLimitations\nAlthough our method outperforms baselines on\nboth unsupervised and supervised tasks in most\ncases, there are still at least two limitations. First,\nwe simply sample negative instances from other\nsentences in the mini-batch, which may lead to\nfalse negatives. Punishing false negatives during\ntraining by assigning lower weight for negatives\nwith higher similarity may be a solution. Second,\nalthough redundant information is discarded, what\nredundant information forgets and remains is un-\nknown. It would be interesting to explore this prob-\nlem by integrating interpretation methods.\n6\nConclusion\nIn this work, we propose InforMin-CL, an effec-\ntive contrastive learning approach, which improves\nstate-of-the-art sentence embedding performance\non downstream tasks. InforMin-CL discards re-\ndundant information stored in positive instances by\nencouraging one positive instance to reconstruct\nthe other positive instance in the same pair. We test\nInforMin-CL on seven supervised and seven un-\nsupervised tasks. Experimental results indicate our\nmethod outperforms all previous competitors.\nAcknowledgements\nWe thank the anonymous reviewers for their valu-\nable comments. We also thank Shanghai Science\nand Technology Innovation Action Plan Project\n(21511104500) for funding this research.\nReferences\nAlessandro Achille and Stefano Soatto. 2018. Emer-\ngence of invariance and disentanglement in deep rep-\nresentations. J. Mach. Learn. Res., 19:50:1–50:34.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel M.\nCer, Mona T. Diab, Aitor Gonzalez-Agirre, Wei-\nwei Guo, Iñigo Lopez-Gazpio, Montse Maritxalar,\nRada Mihalcea, German Rigau, Larraitz Uria, and\nJanyce Wiebe. 2015.\nSemeval-2015 task 2: Se-\nmantic textual similarity, english, spanish and pi-\nlot on interpretability.\nIn Proceedings of the\n9th International Workshop on Semantic Evalua-\ntion, SemEval@NAACL-HLT 2015, Denver, Col-\norado, USA, June 4-5, 2015, pages 252–263. The\nAssociation for Computer Linguistics.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel M.\nCer, Mona T. Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Rada Mihalcea, German Rigau, and Janyce\nWiebe. 2014. Semeval-2014 task 10: Multilingual\nsemantic textual similarity. In Proceedings of the\n8th International Workshop on Semantic Evaluation,\nSemEval@COLING 2014, Dublin, Ireland, August\n23-24, 2014, pages 81–91. The Association for Com-\nputer Linguistics.\nEneko Agirre, Carmen Banea, Daniel M. Cer, Mona T.\nDiab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger-\nman Rigau, and Janyce Wiebe. 2016.\nSemeval-\n2016 task 1: Semantic textual similarity, monolin-\ngual and cross-lingual evaluation. In Proceedings of\nthe 10th International Workshop on Semantic Evalu-\nation, SemEval@NAACL-HLT 2016, San Diego, CA,\nUSA, June 16-17, 2016, pages 497–511. The Associ-\nation for Computer Linguistics.\nEneko Agirre, Daniel M. Cer, Mona T. Diab, and Aitor\nGonzalez-Agirre. 2012.\nSemeval-2012 task 6: A\npilot on semantic textual similarity.\nIn Proceed-\nings of the 6th International Workshop on Seman-\ntic Evaluation, SemEval@NAACL-HLT 2012, Mon-\ntréal, Canada, June 7-8, 2012, pages 385–393. The\nAssociation for Computer Linguistics.\nEneko Agirre, Daniel M. Cer, Mona T. Diab, Aitor\nGonzalez-Agirre, and Weiwei Guo. 2013.\n*sem\n2013 shared task: Semantic textual similarity.\nIn\nProceedings of the Second Joint Conference on Lexi-\ncal and Computational Semantics, *SEM 2013, June\n13-14, 2013, Atlanta, Georgia, USA, pages 32–43.\nAssociation for Computational Linguistics.\nFredrik Carlsson, Amaru Cuba Gyllensten, Evan-\ngelia Gogoulou, Erik Ylipää Hellqvist, and Magnus\nSahlgren. 2021. Semantic re-tuning with contrastive\ntension. In 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nDaniel M. Cer, Mona T. Diab, Eneko Agirre, Iñigo\nLopez-Gazpio, and Lucia Specia. 2017.\nSemeval-\n2017 task 1: Semantic textual similarity multilin-\ngual and crosslingual focused evaluation.\nIn Pro-\nceedings of the 11th International Workshop on Se-\nmantic Evaluation, SemEval@ACL 2017, Vancouver,\nCanada, August 3-4, 2017, pages 1–14. Association\nfor Computational Linguistics.\nHanjie Chen and Yangfeng Ji. 2020. Learning varia-\ntional word masks to improve the interpretability of\nneural text classiﬁers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020, pages 4236–4251. Association for\nComputational Linguistics.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation,\nLREC 2018, Miyazaki, Japan, May 7-12, 2018. Eu-\nropean Language Resources Association (ELRA).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing, IWP@IJCNLP 2005, Jeju Island,\nKorea, October 2005, 2005. Asian Federation of\nNatural Language Processing.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021, pages 6894–\n6910. Association for Computational Linguistics.\nJohn M. Giorgi, Osvald Nitski, Bo Wang, and Gary D.\nBader. 2021. Declutr: Deep contrastive learning for\nunsupervised textual representations.\nIn Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, ACL/IJCNLP 2021, (Volume 1: Long Pa-\npers), Virtual Event, August 1-6, 2021, pages 879–\n895. Association for Computational Linguistics.\nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Ste-\nfanie Tellex. 2019. Openwebtext corpus.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the Tenth\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, Seattle, Wash-\nington, USA, August 22-25, 2004, pages 168–177.\nACM.\nAnil K. Jain and Richard C. Dubes. 1988. Algorithms\nfor Clustering Data. Prentice-Hall.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization.\nIn 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nRyan Kiros,\nYukun Zhu,\nRuslan Salakhutdinov,\nRichard S. Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. 2015.\nSkip-thought vec-\ntors. In Advances in Neural Information Processing\nSystems 28: Annual Conference on Neural Informa-\ntion Processing Systems 2015, December 7-12, 2015,\nMontreal, Quebec, Canada, pages 3294–3302.\nTassilo Klein and Moin Nabi. 2022.\nSCD: self-\ncontrastive decorrelation for sentence embeddings.\nCoRR, abs/2203.07847.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 9119–\n9130. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y. Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn The 49th Annual Meeting of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Proceedings of the Conference, 19-24 June,\n2011, Portland, Oregon, USA, pages 142–150. The\nAssociation for Computer Linguistics.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zampar-\nelli. 2014. A SICK cure for the evaluation of compo-\nsitional distributional semantic models. In Proceed-\nings of the Ninth International Conference on Lan-\nguage Resources and Evaluation, LREC 2014, Reyk-\njavik, Iceland, May 26-31, 2014, pages 216–223. Eu-\nropean Language Resources Association (ELRA).\nYu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Ti-\nwary, Paul Bennett, Jiawei Han, and Xia Song.\n2021. COCO-LM: correcting and contrasting text\nsequences for language model pretraining. In Ad-\nvances in Neural Information Processing Systems\n34: Annual Conference on Neural Information Pro-\ncessing Systems 2021, NeurIPS 2021, December 6-\n14, 2021, virtual, pages 23102–23114.\nSebastian\nNagel.\n2016.\nCc-news.\nURL:\nhttp://web. archive. org/save/http://commoncrawl.\norg/2016/10/newsdatasetavailable.\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proceedings of\nthe 42nd Annual Meeting of the Association for Com-\nputational Linguistics, 21-26 July, 2004, Barcelona,\nSpain, pages 271–278. ACL.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales. In ACL 2005, 43rd An-\nnual Meeting of the Association for Computational\nLinguistics, Proceedings of the Conference, 25-30\nJune 2005, University of Michigan, USA, pages 115–\n124. The Association for Computer Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014.\nGlove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing, EMNLP 2014, October 25-29, 2014,\nDoha, Qatar, A meeting of SIGDAT, a Special Inter-\nest Group of the ACL, pages 1532–1543. ACL.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B.\nViégas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nBERT. In Advances in Neural Information Process-\ning Systems 32: Annual Conference on Neural Infor-\nmation Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n8592–8600.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nbert:\nSentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages\n3980–3990. Association for Computational Linguis-\ntics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y. Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank.\nIn Proceedings of the 2013 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2013, 18-21 October 2013, Grand Hy-\natt Seattle, Seattle, Washington, USA, A meeting of\nSIGDAT, a Special Interest Group of the ACL, pages\n1631–1642. ACL.\nJianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen\nOu. 2021.\nWhitening sentence representations\nfor better semantics and faster retrieval.\nCoRR,\nabs/2103.15316.\nYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan,\nCordelia Schmid, and Phillip Isola. 2020.\nWhat\nmakes for good views for contrastive learning? In\nAdvances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual.\nNaftali Tishby, Fernando C. N. Pereira, and William\nBialek. 2000. The information bottleneck method.\nCoRR, physics/0004057.\nNaftali Tishby and Noga Zaslavsky. 2015.\nDeep\nlearning and the information bottleneck principle.\nIn 2015 IEEE Information Theory Workshop, ITW\n2015, Jerusalem, Israel, April 26 - May 1, 2015,\npages 1–5. IEEE.\nTrieu H Trinh and Quoc V Le. 2018.\nA simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nYao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov,\nand Louis-Philippe Morency. 2021. Self-supervised\nlearning from a multi-view perspective. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net.\nAäron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. CoRR, abs/1807.03748.\nEllen M. Voorhees and Dawn M. Tice. 2000. Building\na question answering test collection. In SIGIR 2000:\nProceedings of the 23rd Annual International ACM\nSIGIR Conference on Research and Development\nin Information Retrieval, July 24-28, 2000, Athens,\nGreece, pages 200–207. ACM.\nTongzhou Wang and Phillip Isola. 2020. Understand-\ning contrastive representation learning through align-\nment and uniformity on the hypersphere.\nIn Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 9929–9939. PMLR.\nJanyce Wiebe, Theresa Wilson, and Claire Cardie.\n2005. Annotating expressions of opinions and emo-\ntions in language. Lang. Resour. Evaluation, 39(2-\n3):165–210.\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian\nKhabsa, Fei Sun, and Hao Ma. 2020. CLEAR: con-\ntrastive learning for sentence representation. CoRR,\nabs/2012.15466.\nYan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim,\nand Lidong Bing. 2020. An unsupervised sentence\nembedding method by mutual information maxi-\nmization. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020,\npages 1601–1610. Association for Computational\nLinguistics.\nJie Zhou, Yuanbin Wu, Qin Chen, Xuan-Jing Huang,\nand Liang He. 2021. Attending via both ﬁne-tuning\nand compressing.\nIn Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 2152–2161.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 19–\n27.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2022-09-22",
  "updated": "2022-09-22"
}