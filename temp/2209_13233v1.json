{
  "id": "http://arxiv.org/abs/2209.13233v1",
  "title": "Genetic Programming-Based Evolutionary Deep Learning for Data-Efficient Image Classification",
  "authors": [
    "Ying Bi",
    "Bing Xue",
    "Mengjie Zhang"
  ],
  "abstract": "Data-efficient image classification is a challenging task that aims to solve\nimage classification using small training data. Neural network-based deep\nlearning methods are effective for image classification, but they typically\nrequire large-scale training data and have major limitations such as requiring\nexpertise to design network architectures and having poor interpretability.\nEvolutionary deep learning is a recent hot topic that combines evolutionary\ncomputation with deep learning. However, most evolutionary deep learning\nmethods focus on evolving architectures of neural networks, which still suffer\nfrom limitations such as poor interpretability. To address this, this paper\nproposes a new genetic programming-based evolutionary deep learning approach to\ndata-efficient image classification. The new approach can automatically evolve\nvariable-length models using many important operators from both image and\nclassification domains. It can learn different types of image features from\ncolour or gray-scale images, and construct effective and diverse ensembles for\nimage classification. A flexible multi-layer representation enables the new\napproach to automatically construct shallow or deep models/trees for different\ntasks and perform effective transformations on the input data via multiple\ninternal nodes. The new approach is applied to solve five image classification\ntasks with different training set sizes. The results show that it achieves\nbetter performance in most cases than deep learning methods for data-efficient\nimage classification. A deep analysis shows that the new approach has good\nconvergence and evolves models with high interpretability, different\nlengths/sizes/shapes, and good transferability.",
  "text": "IEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n1\nGenetic Programming-Based Evolutionary Deep\nLearning for Data-Efﬁcient Image Classiﬁcation\nYing Bi, Member, IEEE, Bing Xue, Senior Member, IEEE, and Mengjie Zhang, Fellow, IEEE,\nAbstract—Data-efﬁcient image classiﬁcation is a challenging\ntask that aims to solve image classiﬁcation using small training\ndata. Neural network-based deep learning methods are effective\nfor image classiﬁcation, but they typically require large-scale\ntraining data and have major limitations such as requiring\nexpertise to design network architectures and having poor inter-\npretability. Evolutionary deep learning is a recent hot topic that\ncombines evolutionary computation with deep learning. However,\nmost evolutionary deep learning methods focus on evolving archi-\ntectures of neural networks, which still suffers from limitations\nsuch as poor interpretability. To address this, this paper proposes\na new genetic programming-based evolutionary deep learning\napproach to data-efﬁcient image classiﬁcation. The new approach\ncan automatically evolve variable-length models using many\nimportant operators from both image and classiﬁcation domains.\nIt can learn different types of image features from colour or\ngray-scale images, and construct effective and diverse ensembles\nfor image classiﬁcation. A ﬂexible multi-layer representation\nenables the new approach to automatically construct shallow\nor deep models/trees for different tasks and perform effective\ntransformations on the input data via multiple internal nodes.\nThe new approach is applied to solve ﬁve image classiﬁcation\ntasks with different training set sizes. The results show that it\nachieves better performance in most cases than deep learning\nmethods for data-efﬁcient image classiﬁcation. A deep analysis\nshows that the new approach has good convergence and evolves\nmodels with high interpretability, different lengths/sizes/shapes,\nand good transferability.\nIndex Terms—Evolutionary Deep Learning; Genetic Program-\nming; Image Classiﬁcation; Small Data; Evolutionary Computa-\ntion; Deep Learning\nI. INTRODUCTION\nImage classiﬁcation tasks have a wide range of applications\nsuch as detecting cancer from x-ray images, identifying a\nface from a set of photographs, and classifying ﬁsh species\nfrom underwater images [1] [2] [3] [4]. Although image\nclassiﬁcation has been investigated for decades, it remains a\nManuscript received XXX; revised XXX; revised XXX; accepted XXX.\nThis work was supported in part by the Marsden Fund of New Zealand\nGovernment under Contracts VUW1913, VUW1914 and VUW2115, the\nScience for Technological Innovation Challenge (SfTI) fund under grant\nE3603/2903, MBIE Data Science SSIF Fund under the contract RTVU1914,\nand National Natural Science Foundation of China (NSFC) under Grant\n61876169. (Corresponding author: Ying Bi)\nThe author Ying Bi is with School of Electrical and Information En-\ngineering, Zhengzhou University, Zhengzhou 450001, China, and School\nof Engineering and Computer Science, Victoria University of Wellington,\nWellington 6140, New Zealand (e-mail: ying.bi@ecs.vuw.ac.nz).\nThe\nauthors\nBing\nXue\nand\nMengjie\nZhang\nare\nwith\nSchool\nof\nEngineering\nand\nComputer\nScience,\nVictoria\nUniversity\nof\nWelling-\nton,\nWellington\n6140,\nNew\nZealand\n(e-mail:\nbing.xue@ecs.vuw.ac.nz;\nmengjie.zhang@ecs.vuw.ac.nz).\nColor versions of one or more of the ﬁgures in this paper are available\nonline at http://ieeexplore.ieee.org.\nchallenging task due to many factors, e.g., high inter-class\nsimilarity and intra-class dissimilarity, image distortion, and\nlack of sufﬁcient training data. In recent years, data-efﬁcient\nimage classiﬁcation that aims to use small data to perform\neffective image classiﬁcation arise much attention. It can re-\nduce the reliance on a large number of training data, therefore\nreducing the cost and effort for data collection, labelling,\nprepossessing, and storage. Data-efﬁcient image classiﬁcation\nis also important for many applications in medicine, remote\nsensing and biology, where the labelled data are not easy to\nobtain due to the cost, privacy and security.\nDeep learning is a hot research area with many successful\napplications [5]. The goal of deep learning is to automatically\nlearn or discover multiple levels of abstraction from data (i.e.\nrepresentation learning) that are effective for a particular task\n[6]. These methods often include three main characteristics,\ni.e., sufﬁcient model complexity, layer-by-layer data process-\ning, and feature transformation [7]. Deep learning methods\ninclude both neural-network(NN)-based methods such as con-\nvolutional NNs (CNNs) [8] and non-NN-based methods such\nas deep forest [7] and PCANet [9]. In recent years, deep\nCNNs become a dominant approach to image classiﬁcation\n[8]. However, these NN-based methods have major limitations,\nsuch as requiring expensive computing devices to run, a large\nnumber of training data to train, and rich expertise to design\nthe architectures [7]. Another important disadvantage of deep\nNNs is poor interpretability due to the large number of pa-\nrameters and very deep structures in the model [7]. To address\nthese limitations, it is worth inventing new non-NN-based deep\nlearning methods, which can not only maintain the diversity\nof the artiﬁcial intelligence/machine learning/computational\nintelligence research community but also bring new ideas to\nthis community.\nEvolutionary deep learning (EDL) is a new research ﬁeld\nthat aims to use evolutionary computation (EC) techniques to\nevolve or optimise deep models [10] [11]. The advantages,\ni.e., population-based beam search, non-differential objective\nfunctions, ease of cooperating with domain knowledge, ro-\nbustness to dynamic changes, etc, have enabled EC methods\nto solve many complex optimisation and learning problems in\nvarious ﬁelds including ﬁnance, engineering, security, health-\ncare, manufacturing, and business [11] [12].\nExisting works of EDL can be broadly classiﬁed into two\ncategories, i.e., the use of EC methods to automatically to\noptimise deep NNs by searching for architectures, weights,\nloss functions etc, and the use of EC methods to automat-\nically evolve variable-length, relatively deep, non-NN-based\nmodels. In the ﬁrst category, many EC methods such as\narXiv:2209.13233v1  [cs.NE]  27 Sep 2022\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n2\ngenetic algorithms (GAs), genetic programming (GP), particle\nswarm optimisation (PSO), and evolutionary multi-objective\noptimisation (EMO) have been used to automatically evolve\ndifferent types of deep NNs, e.g., CNNs, Autoencoders, re-\ncurrent neural networks (RNNs), and generative adversarial\nnetworks (GANs) [10] [13] [14]. However, they often suffer\nfrom the limitations of deep NNs, such as requiring a large\nnumber of training examples, having “black-box” models with\nmillions of parameters, and are computationally expensive. In\nthe second category, to the best of our knowledge, the main\nmethod is GP, which can automatically evolve deep models\nto solve a problem [11]. According to the deﬁnition of deep\nlearning [5] [7], GP is a deep learning method because it\ncan automatically learn models with advanced operators that\nare sufﬁciently complex to perform data/feature transformation\nto solve tasks. There is also an increasing trend in recent\nworks recognising GP as a deep learning method [10] [15].\nUnlike NN-based deep learning methods, GP-based EDL\nmethods can automatically evolve variable-length models and\ntheir co-efﬁciencies without making any assumptions about\nthe model structure/architecture [16]. Importantly, the models\nlearned by GP often have varying complexity and potentially\nhigh interpretability, which are difﬁcult to achieve in NNs.\nBeneﬁting from these advantages, GP-based EDL methods\nhave been successfully applied to image classiﬁcation with\ngood performance [4] [17] [18]. However, the potential of GP\nhas not been fully investigated. For tasks with small training\ndata, limited computation resources and high requirements\nfor model interpretability, where deep NNs have difﬁculty\nto address, it is necessary to investigate novel non-NN-based\ndeep learning methods i.e. GP-based methods.\nExisting GP-based EDL methods have limitations in solving\nimage classiﬁcation, which can be broadly summarised into\nthree aspects. First, most existing methods have only been\nexamined on datasets with gray-scale images. Real-world\nproblems may have images with various numbers of channels\nsuch as RGB channels, so it is necessary to develop a new\nGP method applicable to these different images. Second, most\nof the existing GP-based methods have rarely been applied\nto large-scale image classiﬁcation datasets such as SVHN,\nCIFAR10, CIFAR100, and ImageNet. Third, the potential of\nGP with different operators to automatically learn models\nhas not been fully investigated. The ﬂexible representation\nallows GP to automatically build models using different types\nof simple and complex operators from different domains as\nfunctions (internal nodes) [19]. But there is still a lot of\nresearch potential in developing new representations including\nGP’s tree structures, the function and terminal sets.\nTherefore, this paper focuses on developing a new GP-based\nEDL approach to data-efﬁcient image classiﬁcation, where\ncurrent NN methods can not provide satisfactory performance\ndue to the requirement of large training data. To achieve\nthis, we will develop a new GP-based EDL approach (i.e.\nEDLGP) with a new model/solution representation, a new\nfunction set and a new terminal set, enabling it to automatically\nlearn features from gray-scale and/or colour images, select\nclassiﬁcation algorithms, and build effective ensembles for\nimage classiﬁcation. We will examine the performance of the\nnew approach on well-known image classiﬁcation datasets\nincluding CIFAR10, Fashion MNIST, and SVHN and two\nface image datasets under the scenario of small training data.\nEDLGP will be compared with a large number of existing\ncompetitive methods including CNNs of varying architectures\nand complexity. To highlight the potential of GP-based EDL\nmethods, we perform a comprehensive comparison between\nEDLGP and CNNs for image classiﬁcation. In addition, a deep\nanalysis on the results in terms of convergence behaviour,\ntree/model size, model interpretability and transferability is\nconducted.\nThe main characteristics of the new EDLGP approach are\nsummarised as follows.\n1) The EDLGP approach can evolve variable-length tree-\nbased symbolic models, achieving promising classiﬁca-\ntion performance in the data-efﬁcient scenario, which\nare difﬁcult for current popular NN-based methods to\naddress due to the requirement of a large number of\ntraining images. The design of automatically evolving\nensembles further enhance the generalisation, which is\na key issue when the training set is small.\n2) Compared with existing methods, the EDLGP approach\nhas a ﬂexible multi-layer model representation to au-\ntomatically evolve shallow or deep models for differ-\nent image classiﬁcation tasks. With this representation,\nEDLGP can deal with gray-scale and/or colour images,\nextract image features via different processes such as\nimage ﬁltering and complex feature extraction, perform\nclassiﬁcation by using cascading and ensembles with\nhigh diversity, and automatically select parameters for\nimage operators and classiﬁcation algorithms. Unlike\nNN-based methods, EDLGP evolves models with small\nnumbers of parameters, which are easy to learn and less\nrelying on the scale of the data.\n3) The EDLGP approach can evolve models with high\ninterpretability and transferability. This is important for\nproviding insights into the problems being solved and\nalso for future model reuse and development.\nII. RELATED WORK\nThis section reviews related works on evolutionary deep\nlearning including automatically evolving CNNs and GP for\nimage classiﬁcation and related works on data-efﬁcient image\nclassiﬁcation. The limitations of these works are summarised.\nA. Evolutionary Deep Learning (EDL)\n1) NN-based EDL methods: The methods that fall into this\ncategory typically use EC techniques to optimise deep NNs\nby searching for architectures, weights, and other parameters.\nA detailed review of existing work can be found in [10]\n[13] [14]. Sun et al. [20] proposed an evolutionary algorithm\nto search for CNN architectures and connection weights for\nimage classiﬁcation. In [21], a GA-based method was pro-\nposed to automatically design block-based CNN architectures\nfor image classiﬁcation. Lu et al. [22] developed an EMO-\nbased method (i.e. NSGA-II) to automatically search CNN\narchitectures while optimising multiple objectives. Zhu and Jin\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n3\n[23] proposed an EC-based neural architecture search (NAS)\nmethod under the real-time federated learning framework.\nThe computational costs are signiﬁcantly reduced by using\nthis method. Zhang et al. [24] developed a method based on\nsampled training and node inheritance to improve the compu-\ntational efﬁciency of EC-based NAS for image classiﬁcation.\n2) GP-based EDL methods: Unlike NNs, GP can use\na ﬂexible tree-like symbolic representation to automatically\nevolve trees/models with different lengths to meet the needs\nof problem solving. For many problems, including image clas-\nsiﬁcation, GP can automatically discover the representation\nof the data by evolving models of appropriate complexity.\nAccording to the deﬁnition of deep learning [5], GP can be\na deep learning method because it can learn very complex\nmodels and transform the input data by using many internal\nnodes in the GP trees into higher-level data.\nAn early review on GP-based EDL methods is presented\nin [11], which mentions typical GP methods such as 3-\ntier GP and multi-layer GP for image classiﬁcation. These\nmethods can automatically evolve models, extract features\nfrom raw images and perform classiﬁcation, which is similar\nto CNNs. Rodriguez-Coayahuitl et al. [25] developed a GP\nautoencoder with structured layers to achieve representation\nlearning and introduced the concept of deep GP. Shao et al.\n[26] proposed a GP-based method that can use different ﬁlters\nand image operators to achieve feature learning for image\nclassiﬁcation. This method achieved better performance than\nCNNs on several datasets. Bi et al. presented a series of GP-\nbased methods, such as FLGP [27], FGP [19], and IEGP [28],\nfor representation learning and image classiﬁcation, where the\nevolved GP trees are constructed via multiple functional layers.\nB. Data-Efﬁcient Image Classiﬁcation\nData-efﬁcient image classiﬁcation focuses on solving image\nclassiﬁcation with a small number of available training data,\nwhich has become an increasingly important topic in recent\nyears. Bruintjes et al. [29] and Lengyel et al. [30] proposed\nthe ﬁrst and second well-known challenges on data-efﬁcient\ndeep learning, which aims to develop new deep learning\nmethods using small training data to solve computer vision\ntasks including image classiﬁcation, object detection, instance\nsegmentation, and action recognition. In [29] [30], two im-\nportant rules of solving data-efﬁcient image classiﬁcation are\nproposed, i.e., 1) models can only be trained from scratch\nusing the training set of the task, 2) other data, transfer learning\nand model pre-training are prohibited.\nArora et al. [31] proposed a convolutional neural tangent\nkernel (CNTK) method, e.g., classifying CIFAR10 with 10\nto 640 training images. The results showed that the CNTK\nmethods with different numbers of layers can beat ResNet.\nBrigato and Iocchi [32] investigated different CNNs of vary-\ning complexity for image classiﬁcation on small data. The\nresults showed that dropout can improve the generalisation of\nCNNs. Bi et al. [33] proposed a multi-objective GP method\nto simultaneously optimise training accuracy and a distance\nmeasure to improve generalisation of the classiﬁcation system\nusing a small training set.\nBarz and Denzler [34] proposed the well-known Cosine loss\nfor training deep NNs using small datasets. By maximising\nthe cosine similarity between the outputs of the NNs and the\none-hot vectors of the true class, this loss function gained\nbetter results than the commonly used cross-entropy loss on\nseveral image datasets. Brigato et al. [35] proposed eight\ndata-efﬁcient image classiﬁcation benchmarks including object\nclassiﬁcation, digit recognition, medical image classiﬁcation,\nand satellite image classiﬁcation. Several methods were inves-\ntigated to solve these benchmarks. The results showed param-\neter tuning in terms of batch size, learning rate and weight\ndecay can improve the performance of existing methods on\nthese datasets. Sun et al. [36] proposed visual inductive priors\nframework with a new neural network architecture for data-\nefﬁcient image classiﬁcation. A loss function based on the\npositive class classiﬁcation loss and the intra-class compact-\nness loss was developed to further improve the generalisation.\nThis method ranked the ﬁrst in the ﬁrst data-efﬁcient image\nclassiﬁcation challenge [29]. Zhao and Wen [37] proposed a\nmethod with a two-stage manner to train a teacher network\nand a student network for data-efﬁcient image classiﬁcation.\nThis method ranked the second in the challenge [29].\nC. Summary\nExisting work shows the potential of GP for data-efﬁcient\nimage classiﬁcation [17] [38] [39]. However, all these methods\nfocus on relatively simple tasks and use gray-scale images.\nFurthermore, very few GP-based methods have been tested on\ncommonly used datasets, such as CIFAR10. The comparison\nbetween GP-based EDL methods and NN-based deep learning\nmethods is not sufﬁcient and comprehensive to show the\nadvantages of GP in solving data-efﬁcient image classiﬁcation.\nTo address these limitations and further explore the potential\nof GP-based EDL methods, this paper proposes a new data-\nefﬁcient image classiﬁcation approach based on GP and con-\nducts comprehensive comparisons between the new approach\nand CNN to demonstrate its effectiveness.\nIII. THE PROPOSED APPROACH\nThis section introduce the proposed EDLGP approach to\nimage classiﬁcation, including the overall algorithm, the indi-\nvidual representation, genetic operators, and ﬁtness function.\nTo highlight the advantages of the new approach, a detailed\ncomparison between it with deep CNNs is presented.\nA. Overall Algorithm\nAn image classiﬁcation task often consists of a training\nset and a test set. The training set is denoted as Dtrain =\n{(xi, yi)}m\ni=1 and the test set is denoted as Dtest = {xj}n\nj=1,\nwhere x ∈RG×W ×H denotes the images with a size of W ×H\nand a channel of G, and y ∈Z denotes the class label of\nthe image (the total number of classes is C). The number\nof training images is m and the number of testing images is\nn. The task is to use Dtrain to build a model g(x) that can\neffectively predict y for testing/unseen images Dtest.\nThe overall process of EDLGP for image classiﬁcation is\nshown in Fig. 1. The input of the system is the training set\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n4\nTraining set\nGenetic Programming learning\nBest GP tree\nPopulation\n initialisation\nFitness \nevaluation\nTerminate?\nElitism\nCrossover\nMutation\nswap\nreplace\nClassification\nTest set\nNew representation\nPopulation \ngeneration\nNo\nYes\nStart\nEnd\nAccuracy of \nthe test set \nEnsemble\nSelection\nairplane bird\nship\ncat\nFig. 1. The overall algorithm of the proposed EDLGP approach to image classiﬁcation.\nDtrain and the output is the best GP tree/model g(x). EDLGP\nstarts by randomly initialising a number of tree-based models\naccording to the predeﬁned representation, the function set\nand the terminal set. The population is evaluated using the\nﬁtness function, where each model/tree/individual is assigned\nwith a ﬁtness value i.e. the accuracy of the training set.\nDuring the evolutionary process, promising individuals are\nselected using a selection method and new individuals are\ngenerated using genetic operators i.e. subtree crossover and\nsubtree mutation. A new population is created by copying a\nproportion of individuals with the highest ﬁtness values and\nby generating new individuals from crossover and mutation\noperations. The process of population generation, ﬁtness eval-\nuation and selection is repeated until reaching a predeﬁned\ntermination criterion. After the evolutionary process, the best\nGP tree/model is returned. The best tree g∗(x) is used to\nclassify images in the test set.\nThe optimisation process of the EDLGP approach can be\ndeﬁned as follows\ng∗(x) = argmax\nf∈F, t∈T\nL(g(x), Dtrain)\n(1)\nwhere g(x) represents a GP tree/model and L represents\nthe objective/ﬁtness function. F denotes the functions and T\ndenotes the terminals, which are employed to construct GP\nmodels/trees. The best model is learned by using EDLGP with\na goal of maximising L using the training set Dtrain. Note that\nL can be a loss function as that in NNs to be minimised.\nTo ﬁnd the best model g∗(x), a new representation, a new\nfunction set, and a new terminal set are developed in EDLGP.\nThese new components allow EDLGP to automatically evolve\nvariable-length models that extract informative image features\nand build effective ensembles of classiﬁers for classiﬁcation.\nB. Components of EDLGP\nThe EDLGP approach has a new model representa-\ntion/encoding, making it different from existing GP methods\n[4] [11]. Furthermore, EDLGP uses genetic operators to create\nnew populations of trees and a ﬁtness function for evaluating\nthe new models. These components are introduced as follows.\n1) New Model Representation: EDLGP uses a tree-based\nstructure based on strongly typed GP [40] to represent the\nmodel for image classiﬁcation. The tree structure and two\nexample trees are shown in Fig. 2. Speciﬁcally, the tree\nstructure consists of nine concept layers, i.e., input, image\nﬁltering, feature extraction, concatenation, classiﬁcation and\ncascade, concatenation, classiﬁcation, summation, and output.\nThe input layer represents the inputs of a GP tree, such as raw\nimages and the parameters of operators. The image ﬁltering\nlayer uses a set of commonly used image ﬁlters to process\nthe input image. The feature extraction layer contains well-\nknown image descriptors that can generate a set of features\nfrom images. The concatenation layers aim to concatenate\nthe features generated from the previous layer to create a\nmore comprehensive feature set. The classiﬁcation and cascade\nlayers perform classiﬁcation and cascade the predicted class\nlabels or weights to the input feature vector to form a stronger\nfeature set. This idea was borrowed from cascade learning\n[41]. The features generated by the classiﬁcation and cascade\nlayer can be further concatenated by a concatenation layer. The\nclassiﬁcation layer performs classiﬁcation on the image using\nthe features from its child nodes. The summation layer adds\nthe predicted “probabilities” from all its child nodes/classiﬁers\nfor each class. The output layer performs the majority voting\nto make a prediction on which class the image belongs to, i.e.\nthe class label.\nThe tree structure of EDLGP consists of ﬂexible layers and\nﬁxed layers with a good balance between the necessary model\ncomplexity of dealing with an image classiﬁcation task and\nthe ﬂexibility of the model growing in depth. Speciﬁcally, the\ninput, feature extraction, classiﬁcation, summation, and output\nlayers are ﬁxed, which follows a commonly used manner for\nbuilding an ensemble for image classiﬁcation. These layers\nmust appear in each GP tree/model, but the functions used\nin those layers vary with trees/models. The ﬂexible layers\nare the image ﬁltering layer, the concatenation layer, and\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n5\nSummation\nClassification \nInput\nFeature extraction\nImage filtering\nLoG1\nHOG\nComb2\nHist\nSum2\nGray\nBlue\nConcatenation\nConcatenation\nClassification and cascade\nCC_SVM\nComb2\nSobel\nSobel_FE\nComb3\nLBP_FE\nGreen\nGray\nCC_LR\nRF\nHist\nRed\nMean\nMax\nGau\n2\nConcept \nlayers\nA shallow model\nA deep model\nMean\nHOG_FE\nComb2\nSIFT\nSum2\nRed\nGreen\nMajority voting\nSVM\nLBP\nGray\nERF\nSobel\n100\n30\n1\nMax\nVariable \ndepth\nTree structure\nLoG1\nSIFT\nRed\nCC_SVM\nCC_SVM\n100\n30\nERF\nVariable \ndepth\nMajority voting\nOutput\nFlexible layer\nFixed layer\nFig. 2. The tree structure of EDLGP and two example trees/models. The tree structure consists of a number of concept layers, where each layer uses a set\nof predeﬁned functions that can be searched by EDLGP to build the model. The tree structure allows EDLGP to construct shallow and deep models (as the\ntwo examples) with different functions, sizes and shapes to solve different tasks. Each tree can extract features from raw images and construct ensembles of\nclassiﬁers to achieve effective classiﬁcation.\nthe classiﬁcation and cascade layer, which are optional for\nconstructing GP trees/models. These ﬂexible layers allow\nEDLGP to generate more effective features by using different\nnumbers of functions as internal nodes in the GP trees. This\nis important for dealing with difﬁcult image classiﬁcation\ntasks that require sufﬁciently complex data transformation to\ngenerate informative features.\nThere are four main characteristics of the new model\nrepresentation. First, it allows EDLGP to evolve shallow and\ndeep models for different tasks. A simple model may only\nhave a few internal and leaf nodes, similar to the example\ntree shown in the middle part of Fig. 2. A deep model may\nbe constructed by a large number of internal nodes, as the\nexample tree is shown in the right part of Fig. 2. Second,\nthis representation allows EDLGP to construct ensembles of\nclassiﬁers/ensembles for image classiﬁcation, which enhances\nthe effectiveness of the models when the number of training\nimages is small. Third, this representation allows the con-\nstructed model to automatically learn effective image features\nwith different processes, i.e., image ﬁltering, feature extraction\nbased on commonly used image descriptors, and cascading\nafter classiﬁcation, which are common in computer vision and\nmachine learning. Fourth, EDLGP can evolve ensembles with\nhigh diversity for image classiﬁcation. The ensembles include\ncascade classiﬁers and classiﬁers built using features extracted\nby the corresponding subtree. The diversity ensures the effec-\ntiveness of the constructed ensembles. These characteristics\nmake EDLGP signiﬁcantly different from existing GP-based\nmethods for image classiﬁcation.\n2) Functions and Terminals: The functions represent the\ninternal and root nodes and the terminals represent the leaf\nnodes of GP trees/models. Different from existing GP meth-\nods, EDLGP uses more functions to extract different types of\nfeatures and new classiﬁcation and cascade functions to evolve\ndeep models according to the model representation.\nAll terminals are listed in Table I. The terminals include the\ninput image to be classiﬁed and parameters for corresponding\nfunctions. If the input images are colour, the terminals will be\nTABLE I\nTERMINALS\nTerminal\nDescription\nRed, Blue,\nGreen\nThe red, blue, and green channels of the colour image. Each\nof them is a 2D array with values in range [0, 1]\ngray\nThe gray-scale image, which is a 2D array with the values\nin range [0, 1]\nt\nThe number of trees in RF, ERF, CC RF, and CC ERF. It\nis an integer in range [50, 1000] with a step of 50\nd\nThe maximal tree depth in RF, ERF, CC RF, and CC ERF.\nIt is an integer in range [10, 100] with a step of 10\nf\nThe frequency of Gabor and Gabor FE. Its value is in range\n[π/8, π/2] with a step of π/2\n√\n2\nθ\nThe orientation of Gabor. Its value is in range [0, 7π\n8 ] with\na step of π\n8\no1, o2\nThe derivative orders of GauD and GauD FE. It is an integer\nin range [0, 2]\nσ\nThe standard deviation of Gau and Gau FE. It is an integer\nin range [1, 3]\nRed, Blue, Green, and gray, representing three colour channels\nand the gray-scale channel. If the input images are gray-scale,\nthe terminal will be gray. The remaining terminals are t, d,\nf, θ, o1, o2, and σ, which have their value ranges according\nto commonly used settings. Their values are automatically\nselected during the evolutionary process.\nThe image ﬁltering layer includes commonly used image\nﬁltering operators, i.e., Mean, Median, Min, Max, Gau, GauD,\nLap, LoG1, LoG2, Sobel, and Gabor. Functions at this layer\nalso include the operators that can process an image, such as\nHOG F, LBP F, Sqrt, and ReLU. In addition, the functions\ninclude Add MaxP and Sub MaxP, which take two images\nas inputs, perform 2 × 2 max-pooling to the images, add\nand subtract the two small images to generate a new image,\nrespectively. If the size of the two input images is not the same,\nthe two functions will perform max-pooling only on the small\nimage to make the size the same before sum or subtraction.\nThis is possible since the size of images can only be reduced\nby using the 2×2 max-pooling operation in the function set.\nBy using these different operators, it is expected to generate\nmore effective features from images. Table II lists all functions\nand the process is illustrated in Fig. 3.\nThe feature extraction layer contains 11 functions listed\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n6\nTABLE II\nIMAGE FILTERING FUNCTIONS\nFunction\nDescription\nMean\nPerform 3×3 mean ﬁltering\nMedian\nPerform 3×3 median ﬁltering\nMin\nPerform 3×3 minimum ﬁltering\nMax\nPerform 3×3 maximum ﬁltering\nGau\nPerform Gaussian ﬁltering and the standard deviation is σ\nGauD\nGenerate a new image by calculating derivatives of Gaussian\nﬁlter with standard deviation σ and orders o1 and o2\nLap\nPerform Laplacian ﬁltering\nLoG1\nPerform Laplacian of Gaussian ﬁltering, and the standard\ndeviation is 1\nLoG2\nPerform Laplacian of Gaussian ﬁltering, and the standard\ndeviation is 2\nSobel\nPerform 3×3 Sobel ﬁltering to the input image\nGabor\nPerform Gabor ﬁltering, where the orientation is θ and the\nfrequency is f\nLBP F\nGenerate a LBP image\nHOG F\nGenerate an HOG image\nSqrt\nReturn sqrt root value of each value of the input image. Return\n1 if the value is negative\nReLU\nPerform the operation using rectiﬁed linear unit on the input\nimage\nAdd MaxP Perform 2×2 max-pooling on two input images or the smaller\nimage and add the two images\nSub MaxP Perform 2×2 max-pooling on two input images or the smaller\nimage and subtract the two images\nAn image\nAn image \nfiltering function,\ne.g., Gau, GauD\nGau\nGauD\nGabor\nSobel\nLBP_F\nHOG_F\nLoG1\nLoG2\nLap\nFig. 3.\nIllustration of image ﬁltering and the images obtained by applying\nsome functions.\nin Table III that can extract different types of features from\nimages. The process is illustrated in Fig. 4. These functions\ninclude commonly used image descriptors, i.e., Histogram\n(Hist), HOG, LBP, HOG FE, LBP FE, and Gabor FE, and\nmethods for detecting edges, i.e., Sobel FE and GauD FE,\nand others, i.e., Conca and Gau FE. The Hist, HOG and LBP\nfunctions extract features from the input image. The X FE\nfunctions perform corresponding ﬁltering operations on the\nimage and extract the features by concatenating the image into\na vector. The Conca function can concatenate two images to\nform a feature vector.\nTABLE III\nFEATURE EXTRACTION FUNCTIONS\nFunction\nDescription\nConca\nConcatenate two images into a feature vector\nHist\nExtract 256 histogram features\nHOG\nExtract HOG features from the input image [42]. The fea-\ntures are the mean values of all 4 × 4 grids from the image\nLBP\nExtract uniform LBP features [43]. The radius is 1.5 and the\nnumber of neighbours is 8 in LBP.\nSIFT\nExtract 128 dense SIFT features from the input image [44]\nLBP FE,\nHOG FE\nGenerate a LBP or HOG image and concatenate the image\ninto a feature vector\nSobel FE,\nGabor FE,\nGau FE,\nGauD FE\nUse the Sobel, Gabor, Gau, or GauD ﬁlter to process the\ninput image and concatenate the new image into a feature\nvector\nThe feature concatenation layer uses Comb2, Comb3 and\nComb4 to further concatenate the feature vectors from their 2,\n3, and 4 child nodes into a feature vector, respectively, which\nAn image\nA set of features\nA feature extraction function\ne.g., HOG, LBP, SIFT\nFig. 4. Illustration of feature extraction.\nA feature vector\nA feature vector\nA feature vector\nFeature concatenation\n e.g.,Comb2\nFig. 5. Illustration of feature concatenation using function Comb2.\nincreases the comprehension of the features for describing an\nimage. This layer can increase the width of the GP trees and\nproduce more features for the classiﬁcation layer. The process\nis illustrated in Fig. 5.\nThe classiﬁcation and cascade layer has four functions\nlisted in Table IV, i.e., cascade random forest (CC RF),\ncascade extremely randomised trees (CC ERF), cascade logis-\ntic regression (CC LR), and cascade support vector machine\n(CC SVM), which perform classiﬁcation and concatenate the\npredicted class probabilities with the input features to form the\noutput features, as shown in Fig. 6. This follows the concept of\ncascade ensemble learning [7]. The prediction of an instance is\na C-dimensional vector denoting the probabilities for a C-class\nproblem. If the number of features is fn, the number of output\nfeatures from these functions will be fn + C. Unlike CC RF,\nCC ERF and CC LR, which are soft classiﬁers, CC SVM\nis a hard classiﬁer so that the prediction vector is binary\n(discrete). To build effective classiﬁers, the main parameters\nof CC RF and CC ERF, i.e., the number of trees and the\nmaximal tree depth, are set as terminals and their values can\nbe automatically selected by EDLGP.\nA feature vector\nA trained\n classifier\nA new feature vector\nPredict class \nprobabilities\nCascade\nClassification and cascade\nC class\nFig. 6. Illustration of classiﬁcation and cascade.\nThe classiﬁcation layer contains functions, i.e., RF, ERF,\nLR, and SVM, which take features as inputs and return the\npredicted class probabilities. The summation layer has three\nfunctions, i.e., Sum2, Sum3 and Sum4, which take two, three\nand four vectors of predicted probabilities as inputs and add\nthem, respectively. Each vector denotes the probabilities of all\nclasses an instance belongs to. These functions are listed in\nTable IV. The output layer can make predictions according to\nthe output of GP trees by assigning the class label with the\nhighest probability to the input image.\nBesides the above functions/operators, it is possible to use\nother different functions at the corresponding layer in EDLGP.\nIn other words, EDLGP can be easily extended by using more\nfunctions. The representation and the search mechanism allow\nEDLGP to automatically select the functions and terminals to\nbuild trees/models. However, it is suggested to carefully set the\nfunction set to make a good balance between the effectiveness\nof the potential models and the search space.\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n7\nTABLE IV\nCLASSIFICATION AND CASCADE, CLASSIFICATION AND SUMMATION\nFUNCTIONS\nFunction\nDescription\nCC RF,\nCC ERF,\nCC LR,\nCC SVM\nUse the RF, ERF, LR, and SVM methods to perform classiﬁ-\ncation. The outputs of these functions are the concatenation of\nthe input features and the predicted class probabilities\nRF\nRandom forest classiﬁcation method. The number of trees is t\nand the maximal tree depth is d\nERF\nExtremely randomised trees classiﬁcation method. The number\nof trees is t and the maximal tree depth is d\nLR\nLogistic regression classiﬁcation method\nSVM\nSupport vector machine classiﬁcation method\nSum2/3/4\nSum 2/3/4 vectors of predicted probabilities\n3) Genetic Operators: During the evolutionary process,\nnew GP trees are generated using genetic operators, i.e.,\nsubtree crossover and subtree mutation. The subtree crossover\noperator is conducted on two selected trees/parents. It ran-\ndomly selects two subtrees from the parents and swaps the two\nsubtrees to generate two new trees, as shown in Fig. 7. Note\nthat the two selected subtrees must have the same output types\nin order to generate two feasible trees. The subtree mutation\noperator is conducted on one selected tree/parent. It randomly\nselects a subtree of the parent and replaces the subtree with a\nrandomly generated subtree, as shown in Fig. 8.\nParent 1\nParent 2\nOffspring 1\nOffspring 2\nCrossover\nSwap\nFig. 7. Illustration of crossover operation.\nParent 1\nA randomly\n generated subtree\nOffspring 1\nMutation\nReplace\nFig. 8. Illustration of mutation operation.\n4) Fitness Evaluation: A ﬁtness function is used in the\nﬁtness evaluation process to evaluate the goodness of the GP\ntrees/models to the tasks. For a classiﬁcation task, the most\ncommonly used ﬁtness function is the classiﬁcation accuracy\nto be maximised, which is deﬁned as\nL = Ncorrect\nNtotal\n× 100%\n(2)\nwhere Ncorrect denotes the number of correctly classiﬁed\ninstances and Ntotal denotes the total number of instances in\nthe training set. The ﬁtness function is calculated using k-fold\ncross-validation on the training set. In EDLGP, the value of K\nis set to min(3, nc), where nc denotes the number of instances\nin the smallest class of the training set. If nc = 1, the ﬁtness\nfunction will be the training accuracy. Note that this ﬁtness\nfunction is effective for balanced classiﬁcation. For unbalanced\nclassiﬁcation, other measures such as balanced accuracy can\nbe used as the ﬁtness function for EDLGP.\nC. Comparisons Between EDLGP and CNN\nThe proposed EDLGP approach can automatically learn\nfeatures and evolve ensembles for image classiﬁcation. To\nhighlight the characteristics of EDLGP, this section provides\ndetailed analysis and comparisons between EDLGP (as a\ntypical example of GP-based EDL methods) and CNNs, which\nis a dominant deep learning method for image classiﬁcation.\nThe main similarities are summarised as follows.\n1) CNNs and EDLGP can automatically learn features and\nperform classiﬁcation from raw images.\n2) CNNs and EDLGP are learning algorithms that learn\nmodels of many layers, which can perform complex data\ntransformation to achieve good performance.\n3) CNNs and EDLGP use image operations such as con-\nvolution and pooling to learn features from raw images.\nThe main differences are summarised as follows.\n1) EDLGP uses a variable-length tree-based structure to\nrepresent a model, while CNNs typically uses a ﬁxed-\nlength layer-wise structure to represent a model. The\nﬂexible representation allows EDLGP to automatically\nlearn models with different shapes, sizes, and depths. In\ncontrast, CNNs need to predeﬁne the model structure\nand length before training.\n2) EDLGP is open to incorporate domain knowledge in a\nway of adding functions/operators to the corresponding\nlayers, which cannot be easily achieved in CNNs.\n3) EDLGP has fewer model parameters and running param-\neters than CNNs. Detailed comparisons in terms of the\nrunning parameters are listed in Table V. The number\nof CNN model parameters depends on the architecture\nconﬁguration, which is often much larger than that of\nthe automatically evolved EDLGP models.\n4) The models evolved by EDLGP have potentially higher\ninterpretability than CNN models. The EDLGP model\nis composed of functions/operators from the image and\nmachine learning domains, which can provide domain\nknowledge. The CNN model is composed of layers of\na huge number of parameters, which are not straightfor-\nwardly to explain.\n5) The feature extraction and classiﬁcation processes of\nCNNs and EDLGP are different. CNNs usually extract\nand construct features via a number of convolutional\nand pooling layers with learned kernels, and perform\nclassiﬁcation using softmax. EDLGP extracts features\nusing image operators with predeﬁned kernels and uses\na traditional classiﬁcation algorithm to perform clas-\nsiﬁcation. This makes CNNs and EDLGP applicable\nand effective for image classiﬁcation under different\nscenarios. CNNs requires sufﬁcient training data to learn\neffective feature maps and sub-sampling maps in these\npredeﬁned layers to achieve promising performance, i.e.,\nsuitable for large-scale image classiﬁcation. The EDLGP\napproach is more effective than CNNs when the training\ndata is small, i.e., data-efﬁcient image classiﬁcation.\n6) The CNN methods can greatly beneﬁt from running on\nGPU, while EDLGP would not at the current stage.\nIV. EXPERIMENT DESIGN\nThis section describes the experiment design, including\ndatasets, the comparison methods, and parameter settings.\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n8\nTABLE V\nCOMPARISONS OF RUNNING PARAMETERS OF EDLGP AND CNNS\nConvolutional neural networks\nEvolutionary deep learning based on\ngenetic programming\nType of activation functions:\nType of selection:\nReLU, tanh, sigmoid, etc\nTournament selection or Roulette\nwheel selection\nArchitecture conﬁgurations:\nArchitecture conﬁgurations:\nNo. hidden layers\nFunction set\nNo. convolutional layers\nTerminal set\nNo. pooling layers\nOptimisation conﬁgurations:\nNo. other layers\nNo. generations\nHow to connect these layers\nPopulation size\nNo. features maps/nodes\nProbability of elitism/reproduction\nKernel sizes\nProbability of mutation\nOptimisation conﬁgurations:\nProbability of crossover\nLearning rate\nminimal and maximal tree size\nDropout: 0.25/0.50\nselection size\nMomentum\nL1/L2\nweight\nregularisation\npenalty\nTree generation method: full, grow,\nramped-half-and-half\nWeight\ninitialisation:\nuniform,\nXavier Glorot, etc.\nBatch size: 32/64/128\nA. Image Classiﬁcation Datasets\nTo evaluate the effectiveness of the new approach, ﬁve\ndifferent datasets are used, which are CIFAR10 [45], Fash-\nion MNIST (FMNIST in short) [46], SVHN [47], ORL [48],\nand Extended Yale B [49]. The CIFAR10, FMNIST, and\nSVHN datasets are object classiﬁcation tasks. The ORL and\nExtended Yale B datasets are face classiﬁcation tasks. The\nexample images of these datasets are shown in Figs. 9 and 10.\nThe CIFAR10 dataset is composed of 50,000 training im-\nages and 10,000 testing images. The images are colour and of\nsize 32×32. The Fashion MNIST dataset has 60,000 training\nimages and 10,000 testing images in gray-scale. The image\nsize is 28×28. The SVHN dataset contains 73,257 colour\nimages for training and 26,032 colour images for testing.\nThe image size is 32×32. The ORL dataset consists of 400\nfacial images from 40 people, i.e. 10 images per person. The\nExtended Yale B dataset has 2,424 facial images in 38 classes.\nCIFAR10\nFashion_MNIST\nSVHN\nFig. 9.\nExample images of the CIFAR10, Fashion MNIST, and SVHN\ndatasets. Each image represents one class.\nThe EDLGP approach is examined on data-efﬁcient image\nclassiﬁcation problems. A small number of training images are\nused in the experiments. For the CIFAR10, Fashion MNIST\nand SVHN datasets, 10, 20, 40, 80, 160, 320, 640, and\n1280 images are randomly selected from each class in the\noriginal training set to form a small training set used in\nthe experiments, following the settings in [32]. The small\ndatasets are called sCIFAR10-X, sFMNIST-X and sSVHN-\nORL\nExtended Yale B\nFig. 10. Example images of the ORL and Extended Yale B datasets.\nTABLE VI\nSUMMARY OF THE DATASETS\nDataset\n#Class\nImage\nsize\nTrain set size\nTest set size\nCIFAR10\n10\n32×32×3\n50,000\n10,000\nFashion MNIST\n10\n28×28\n60,000\n10,000\nSVHN\n10\n32×32×3\n73,257\n26,032\nORL\n40\n56×46\n400\nExtended Yale B\n38\n32×32\n2424\nX, where X denotes the number of training images in each\nclass. To comprehensively show the performance, we also test\nEDLGP using very small training sets of CIFAR10, i.e., 1,\n2, 4, 8, 16, 32, 64, and 128 images per class, following the\nsettings in [31]. In all these settings, the original test sets\nare used for testing. For the ORL dataset, 2, 3, 4, and 5\nimages are randomly selected to form the training sets and the\nremaining images are used for testing in the experiments [50],\nrespectively. For the Extended Yale B dataset, 15, 20, 25, and\n30 images are randomly selected for training and the rest are\nused for testing [41], respectively. As a result, a large number\nof experiments are conducted to comprehensively demonstrate\nthe performance of EDLGP in comparison with different deep\nlearning methods.\nB. Comparison Methods\nWe compare EDLGP with existing methods including CNNs\nwith different architectures, common methods, and non-NN-\nbased deep learning methods, which are very effective methods\non the corresponding datasets.\n• On sCIFAR10, sFMNIST and sSVHN, the comparisons\nmethods are CNN-lc [32], CNN-mc [32], CNN-hc [32],\nand ResNet-20 [51]. CNN-lc, CNN-mc and CNN-hc\ndenote different CNN methods with low, middle and\nhigh complexity used for image classiﬁcation. Different\nDropout rates are investigated in these methods and the\ndetails can be found in [32].\n• On few-shot sCIFAR10, the comparison methods are\nResNet [51], 5-layer convolutional neural tangent kernel\n(CNTK) networks [31], 8-layer CNTK [31], 11-layer\nCNTK [31], 14-layer CNTK [31], and Harmonic Net-\nworks [52].\n• On ORL, the comparisons methods are Eigenface based\non PCA [53], Fisherface based on LDA [54], Laplacian-\nface [55], neighbourhood preserving embedding (NPE)\n[56], marginal Fisher analysis (MFA) [57], CNN [50],\nand deep metric learning based on CNN and k nearest\nneighbour classiﬁcation (KCNN) [50].\n• On Extended Yale B, the comparisons methods are col-\nlaborative representation classiﬁer (CRC) [58], SRC [59],\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n9\ncorrentropy-based sparse representation (CESR) [60], ro-\nbust sparse coding (RSC) [61], half-quadratic with the\nadditive form (HQA) [62], half-quadratic with the mul-\ntiplicative form (HQM) [62], NMR [63], robust matrix\nregression (RMR) [64], Fisher discrimination dictionary\nlearning (FDDL) [65], low-rank shared dictionary learn-\ning (LRSDL) [66], DCM based on NMR (DCM(N)) [41],\nand DCM based on SRC (DCM(S)) [41].\nThe parameter settings of these comparison methods refer to\nthe corresponding references. The other experimental settings\nrefer to [41] [32] [50]. The goal of this paper is to explore the\npotential of GP-based EDL for data-efﬁcient image classiﬁca-\ntion. The aforementioned methods are used for comparisons\nbecause they are effective methods on these datasets using\nsmall training sets. We also compare the EDLGP approach\nwith more state-of-the-art data-efﬁcient deep learning methods\nmentioned in [35] on CIFAR10 under different settings. Due\nto the page limit, the results and analysis is presented in\nthe supplementary materials. We do not include GP-based\nmethods for comparisons because they have not been applied\nor not designed for solving these datasets.\nC. Parameter Settings\nThe parameter settings for the EDLGP approach follow\nthe commonly used settings in the GP community [4] [67].\nThe maximal number of generations is set to 50 and the\npopulation size is set to 100. The crossover rate is 0.5, the\nmutation rate is 0.49, and the elitism rate is 0.01. The selection\nmethod is tournament selection with size 5. The population\ninitialisation method is the ramped-half-and-half method. The\ninitial tree depth is 2-10. The maximal tree depth is 10. Note\nthat these parameter values for EDLGP can be ﬁne-tuned on\neach dataset for optimal settings. However, our study aims to\ninvestigate a more general approach that can achieve good\nperformance using a common setting. In the experiments,\nEDLGP is executed 10 times independently on each dataset\nusing different random seeds because of the stochastic nature\nand the high computation cost.\nV. RESULTS AND DISCUSSIONS\nThis section compares the performance of the EDLGP\napproach with the benchmark methods on different datasets of\nvarious numbers of training images. It also deeply analyses dif-\nferent aspects of EDLGP in terms of convergence behaviours,\ninterpretability and transferability of models/trees.\nA. Classiﬁcation Accuracy\nOverall\nPerformance:\nIn\ntotal,\nthere\nare\n40\ncases/experiments\n(i.e.,\n3×8\non\nsCIFAR10,\nsFMNIST\nand sSVHN, 8 on few-shot sCIFAR10, and 2×4 on ORL\nand Extended Yale B) to compare the performance EDLGP\nwith different CNNs and existing methods. All the test\nresults are listed in Tables VII - X. In all 40 cases, EDLGP\nachieves better accuracy in 30 cases among all the existing\nmethods,\nindicating\nthat\nEDLGP\nis\neffective\non\nthese\ndifferent image classiﬁcation tasks by automatically evolving\nvariable-length/depth\nmodels.\nNote\nthat\nthe\ncompared\nmethods are the state-of-the-art methods and their results\nare from the corresponding references. Compared with\nthese methods, EDLGP achieves better performance on the\nfew-shot sCIFAR10, ORL and Extended Yale B datasets in\nall scenarios with small numbers of training images. On\nsCIFAR10, sFMNIST and sSVHN, EDLGP achieves better\nperformance when the training set is small, and slightly worse\nperformance when the size of the training set is large. The\nresults show that EDLGP is more effective for data-efﬁcient\nimage classiﬁcation, particularly the training set is very small.\nResults on sCIFAR10, sFMNIST and sSVHN: The clas-\nsiﬁcation accuracies on the test sets are listed in Table VII.\nNote that the test sets are the original one of CIFAR10,\nFashion MNIST and SVHN and the results of these compar-\nison methods are from [32]. On sCIFAR10, EDLGP achieves\nthe best accuracy among all 11 methods including ResNet-\n20 in 6 cases out of 8 cases, i.e., worse on sCIFAR10\nwith 1280 training images per class. On very small training\ndata scenarios, EDLGP achieves maximal accuracy that is\nmuch higher than the best one of all comparison methods on\nsCIFAR10. For example, 3.9% higher (35.8% vs. 31.9%) on\nsCIFAR10-10, 6.2% higher (43.2% vs. 37.0%) on sCIFAR10-\n20, 5.5% higher (48.1% vs. 42.6%) on sCIFAR10-40, 4.2%\nhigher (52.3% vs. 48.1%) on sCIFAR10-80, and 4.3% higher\n(58.2% vs. 53.9%) on sCIFAR10-160. The results further\ndemonstrate that EDLGP is very effective when the training set\nis small. On Fashion MNIST, EDLGP achieves the best results\non sFMNIST-20, sFMNIST-40, sFMNIST-80, and sFMNIST-\n640. In the remaining cases, EDLGP achieves slightly worse\naccuracy than the best accuracy, i.e., 0.1% gap on sFMNIST-\n10, 1.5% gap on sFMNIST-160, 0.6% gap on sFMNIST-\n640, and 1.5% gap on sFMNIST-1280. The results show\nthat EDLGP is effective for classifying the sFMNIST dataset.\nOn sSVHN, EDLGP achieves the best results on sSVHN-10,\nsSVHN-20, sSVHN-40, and sSVHN-80. More importantly, the\nbest accuracy obtained by EDLGP is much higher than that\nby all comparison methods, i.e., 27.9% higher on sSVHN-\n10, 19.5% higher on sSVHN-20, and 8.1% on sSVHN-40.\nWith the increasing of training data, some CNNs and ResNet-\n20 methods tend to achieve better classiﬁcation accuracy and\nthe accuracy gap between EDLGP and the best CNN be-\ncome smaller. On sSVHN-160, sSVHN-320, sSVHN-640, and\nsSVHN-1280, EDLGP achieves better results than the majority\nof the comparison methods and worse results than ResNet-20.\nThe results show that CNNs require sufﬁcient training data to\nobtain good classiﬁcation performance. Compared with these\nCNN methods, EDLGP is signiﬁcantly less data intensive\nand can achieve better performance when the training set is\nvery small. CNN models typically contain a huge number\nof parameters so that a large number of training images\nare needed to train. Unlike CNNs, EDLGP automatically\nevolves tree-based models of functions and operators that have\nsigniﬁcantly fewer parameters and does not require a large\nnumber of training images. EDLGP is more data efﬁcient\nthan CNNs since it can achieve better performance than CNNs\nwhen the training data is small.\nResults on few-shot sCIFAR10: Table VIII shows the\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n10\nTABLE VII\nMEAN CLASSIFICATION ACCURACY (%) OF CNNS OF VARYING COMPLEXITY AND CLASSIFICATION ACCURACY (%) OF EDLGP ON SMALL CIFAR10,\nFASHION MNIST AND SVHN DATASETS OF VARYING TRAINING SET SIZE\nModel\nDropout\nsCIFAR10-10\nsCIFAR10-20\nsCIFAR10-40 sCIFAR10-80 sCIFAR10-160 sCIFAR10-320 sCIFAR10-640 sCIFAR10-1280\nCNN-lc\n0.0\n27.1\n32.4\n36.1\n41.8\n45.3\n50.1\n54.1\n59.4\nCNN-lc\n0.4\n29.7\n33.8\n38.2\n43.5\n47.1\n52.8\n57.2\n61.7\nCNN-lc\n0.7\n29.7\n34.9\n40.0\n44.9\n49.4\n53.9\n58.1\n62.3\nCNN-mc\n0.0\n28.5\n34.3\n38.8\n43.0\n48.6\n53.7\n58.8\n63.3\nCNN-mc\n0.4\n29.7\n34.9\n39.9\n45.4\n50.9\n55.8\n61.0\n66.2\nCNN-mc\n0.7\n31.5\n36.2\n41.3\n47.1\n51.9\n57.7\n62.8\n67.6\nCNN-hc\n0.0\n30.1\n34.2\n39.1\n44.7\n50.6\n56.1\n61.2\n65.9\nCNN-hc\n0.4\n31.7\n36.1\n40.8\n46.5\n52.0\n58.0\n63.5\n68.8\nCNN-hc\n0.7\n31.9\n37.0\n42.5\n48.1\n53.9\n59.5\n64.8\n69.8\nResNet-20\n–\n23.3\n29.0\n31.9\n38.5\n44.7\n51.3\n62.3\n71.5\nEDLGP(best)\n–\n35.8\n43.2\n48.1\n52.3\n58.2\n60.6\n64.8\n64.5\nEDLGP(mean) –\n31.7±3.8\n37.7±4.0\n45.7±1.6\n49.3±1.8\n54.3±2.5\n58.7±1.5\n61.6±2.0\n61.1±3.4\nModel\nDropout\nsFMNIST-10\nsFMNIST-20\nsFMNIST-40\nsFMNIST-80\nsFMNIST-160\nsFMNIST-320\nsFMNIST-640\nsFMNIST-1280\nCNN-lc\n0.0\n71.1\n74.0\n77.8\n81.0\n83.6\n85.4\n86.7\n88.2\nCNN-lc\n0.4\n71.2\n76.1\n79.8\n81.9\n84.2\n85.7\n87.1\n88.7\nCNN-lc\n0.7\n71.3\n75.6\n78.7\n81.6\n83.3\n85.3\n87.0\n87.9\nCNN-mc\n0.0\n72.5\n75.5\n79.0\n82.0\n84.4\n86.3\n88.0\n89.1\nCNN-mc\n0.4\n72.4\n76.1\n79.6\n82.9\n84.7\n86.6\n88.1\n89.6\nCNN-mc\n0.7\n72.5\n76.9\n79.9\n82.9\n84.9\n86.8\n88.2\n89.4\nCNN-hc\n0.0\n71.9\n75.9\n80.1\n82.3\n85.1\n86.8\n88.6\n89.5\nCNN-hc\n0.4\n72.2\n76.3\n80.2\n83.0\n85.0\n86.9\n88.5\n89.8\nCNN-hc\n0.7\n73.3\n77.4\n80.5\n83.2\n86.5\n87.1\n88.8\n89.9\nResNet-20\n–\n62.3\n71.4\n77.0\n80.4\n84.1\n86.9\n89.2\n90.5\nEDLGP(best)\n–\n73.2\n79.5\n82.1\n83.5\n85.0\n87.2\n88.6\n89.0\nEDLGP(mean) –\n71.8±1.0\n78.0±0.7\n81.2±0.7\n83.0±0.4\n84.5±0.5\n86.1±0.7\n86.8±0.6\n88.0±0.6\nModel\nDropout\nsSVHN-10\nsSVHN-20\nsSVHN-40\nsSVHN-80\nsSVHN-160\nsSVHN-320\nsSVHN-640\nsSVHN-1280\nCNN-lc\n0.0\n25.3\n37.5\n50.5\n64.1\n73.0\n77.7\n80.9\n83.6\nCNN-lc\n0.4\n28.4\n44.9\n59.0\n70.2\n77.0\n80.3\n83.3\n85.8\nCNN-lc\n0.7\n28.8\n46.7\n60.6\n72.1\n77.7\n81.3\n83.2\n86.2\nCNN-mc\n0.0\n26.8\n39.9\n53.4\n68.6\n75.5\n79.6\n83.2\n85.5\nCNN-mc\n0.4\n29.6\n43.3\n64.3\n72.1\n78.3\n82.2\n84.8\n87.3\nCNN-mc\n0.7\n27.7\n45.8\n64.1\n74.6\n79.7\n83.2\n86.2\n88.2\nCNN-hc\n0.0\n24.9\n37.5\n55.5\n67.7\n74.5\n80.1\n84.0\n86.1\nCNN-hc\n0.4\n27.5\n45.6\n63.1\n73.6\n79.3\n82.7\n85.7\n88.1\nCNN-hc\n0.7\n28.8\n44.8\n64.7\n74.4\n79.5\n84.0\n86.3\n88.6\nResNet-20\n–\n20.3\n40.0\n54.7\n74.1\n83.5\n86.7\n89.5\n92.2\nEDLGP(best)\n–\n57.5\n66.2\n72.8\n75.5\n79.7\n82.1\n82.1\n85.0\nEDLGP(mean) –\n55.5±1.5\n60.2±2.4\n70.1±1.5\n73.6±1.1\n76.9±2.2\n79.0±1.5\n80.5±0.8\n82.3±0.9\nTABLE VIII\nCLASSIFICATION ACCURACY (%) OF EDLGP AND BENCHMARK METHODS OF VARYING COMPLEXITY ON FEW-SHOT CIFAR10\nModel\nsCIFAR10-1\nsCIFAR10-2\nsCIFAR10-4\nsCIFAR10-8\nsCIFAR10-16\nsCIFAR10-32\nsCIFAR10-64\nsCIFAR10-128\nResNet [31]\n14.59\n17.50\n19.52\n23.32\n28.30\n33.15\n41.66\n49.14\n5-layer CNTK [31]\n15.08\n18.03\n20.83\n24.82\n29.63\n35.26\n41.24\n47.21\n8-layer CNTK [31]\n15.24\n18.50\n21.07\n25.18\n30.17\n36.05\n42.10\n48.22\n11-layer CNTK [31]\n15.31\n18.69\n21.23\n25.40\n30.46\n36.44\n42.44\n48.67\n14-layer CNTK [31]\n15.33\n18.79\n21.34\n25.48\n30.48\n36.57\n42.63\n48.86\nHarmonic Networks [52]\n11.87\n22.24\n26.28\n34.94\n40.47\n49.59\n56.69\n63.83\nEDLGP(best)\n21.10\n23.63\n27.42\n29.93\n41.50\n45.25\n50.53\n56.36\nEDLGP(mean)\n15.22±2.71\n17.69±3.97\n20.63±4.86\n25.51±3.57\n37.91±1.89\n41.11±3.88\n47.21±2.98\n54.04±1.52\nresults on few-shot sCIFAR10 with 1, 2, 4, 8, 16, 32, 64, 128\ntraining images per class, respectively. Among all the methods\nincluding ResNet, EDLGP achieves the best classiﬁcation\naccuracy on few-shot sCIFAR10 with 1-128 training images\nper class. The best accuracy obtained by EDLGP is much\nhigher than the best accuracy of all the comparison methods,\ni.e., over 7% on average. In these few-shot settings, both the\nperformance of CNN and EDLGP increase with the number\nof training images, but EDLGP performs better than these\nCNN methods. The results continually show the effectiveness\nof EDLGP in data-efﬁcient image classiﬁcation.\nResults on ORL and Extended Yale B: On the face\ndatasets, EDLGP achieves higher maximum and average ac-\ncuracies than any of the comparison methods in all scenarios.\nSpeciﬁcally, EDLGP improves the average accuracy by 8.8%\nTABLE IX\nCLASSIFICATION ACCURACY (%) OF EDLGP AND THE BENCHMARK\nMETHODS ON THE ORL DATASET\nMethod\nORL-2\nORL-3\nORL-4\nORL-5\nEigenface [53]\n70.7±2.7\n78.9±2.3\n84.2±2.1\n87.9±2.5\nFisherface [54]\n75.5±3.3\n86.1±1.9\n91.6±1.9\n94.3±1.4\nLaplacianface [55]\n77.6±2.5\n86.0±2.0\n90.3±1.7\n93.0±1.9\nNPE [56]\n77.6±2.7\n85.7±1.8\n90.5±1.8\n93.4±1.8\nMFA [57]\n75.4±3.1\n86.1±1.9\n91.6±1.9\n94.3±1.4\nCNN [50]\n69.7±3.1\n82.9±2.5\n87.9±1.8\n91.5±2.9\nKCNN [50]\n72.8±3.1\n84.6±2.6\n91.7±2.4\n94.6±1.5\nEDLGP(best)\n90.3\n97.9\n99.6\n99.5\nEDLGP(mean)\n86.4±3.9\n96.4±1.0\n98.4±1.1\n99.3±0.2\non ORL-2, 10.3% on ORL-3, 6.7% on ORL-4, and 4.7% on\nORL-5. On ORL-5, EDLGP achieves an average accuracy\nof 99.3% and maximum accuracy of 99.5%. Furthermore,\ncompared with these seven methods, EDLGP has a smaller\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n11\nTABLE X\nCLASSIFICATION ACCURACY (%) OF EDLGP AND THE BENCHMARK\nMETHODS ON THE EXTENDED YALE B DATASET. 15, 20, 25, AND 30\nDENOTE DIFFERENT NUMBERS OF TRAINING IMAGES IN EACH CLASS\nMethod\n15\n20\n25\n30\nCRC [58]\n91.39\n94.26\n95.91\n97.04\nSRC [59]\n91.72\n93.71\n95.56\n96.37\nCESR [60]\n77.92\n83.42\n85.68\n88.51\nRSC [61]\n95.01\n97.04\n97.81\n98.40\nHQA [62]\n93.39\n93.99\n90.19\n92.41\nHQM [62]\n91.14\n94.15\n95.29\n96.46\nNMR [63]\n93.50\n96.29\n97.57\n98.54\nRMR [64]\n93.56\n94.08\n92.15\n92.72\nFDDL [65]\n93.44\n94.92\n96.38\n96.94\nLRSDL [66]\n94.92\n96.69\n97.88\n98.31\nDCM(N) [41]\n93.17\n95.97\n97.38\n98.38\nDCM(S) [41]\n98.87\n99.51\n99.63\n99.79\nEDLGP(best)\n99.89\n99.94\n99.93\n99.84\nEDLGP(mean)\n99.80±0.05\n99.80±0.09\n99.83±0.08\n99.82±0.04\nstandard deviation value, which means EDLGP is more stable.\nOn Extended Yale B, EDLGP gains over 99% accuracy in\nall scenarios. Among all the comparison methods, EDLGP\nachieves the best accuracy on Extended Yale B, which means\nthat EDLGP is more accurate than the 12 different comparison\nmethods. To sum up, EDLGP is effective for face image\nclassiﬁcation using few training images.\nTo summarise, EDLGP achieves promising performance in\nimage classiﬁcation with small training data. Unlike compar-\native methods, EDLGP can simultaneously and automatically\nsearch for the best feature extraction methods and ensemble\nmodels to achieve effective classiﬁcation. The learned EDLGP\nmodels have fewer parameters than CNNs so that they can\nbe better trained on small training data and achieve higher\nclassiﬁcation accuracy. A large number of comparisons show\nthat EDLGP is signiﬁcantly more data efﬁcient than CNNs\nand other well-known methods on different types of image\nclassiﬁcation tasks. The results comprehensively verify the\neffectiveness and superiority of EDLGP in data-efﬁcient image\nclassiﬁcation.\nB. Running and Classiﬁcation Time\nWe take the few-shot sCIFAR10-X (X ranges from 1 to\n128) dataset as a typical example to analyse the running and\nclassiﬁcation time of EDLGP. The running time of EDLGP on\nthe other datasets is analysed in the supplementary materials\ndue to the page limit. Figure 11 shows an average running\ntime and classiﬁcation time of EDLGP on a single CPU.\nThe running time of EDLGP is less than 10 hours when the\nnumber of training images per class is smaller than 16. The\nrunning time of EDLGP increases gradually with the number\nof training images. On sCIFAR10-128, it uses more than\n90 hours to complete the evolutionary learning process. The\nrunning time of EDLGP is clearly longer than that of CNNs,\nalthough we did not directly compare them. The main reason\nis that EDLGP is a population-based search algorithm that is\ncurrently implemented using the DEAP package running on\nCPUs. In contrast, CNNs can beneﬁt from a fast computing\nfacility–GPU, and use a shorter time for model training. The\nrunning time of EDLGP can be accelerated by using a GPU\nimplementation or running it in parallel on multiple CPUs.\nFig. 11. Running time and test/classiﬁcation time of EDLGP on the sCIFAR10\ndataset using 1-128 images/instances per class for training.\nFig. 12. Convergence behaviours of EDLGP on ORL and Extended Yale B.\nThe classiﬁcation/test time of EDLGP on sCIFAR10-X\nis fast, i.e., less than 4 minutes in all scenarios. In the\nclassiﬁcation process, EDLGP trains the model found via\nevolution to classify 10,000 images. The GP model com-\nplexity is the main factor that affects the classiﬁcation time.\nFrom Fig. 11, EDLGP may learn a more complex model on\nsCIFAR10-64 and sCIFAR10-128 than the other scenarios,\nas it uses a longer classiﬁcation time. The analysis of the\nmodel length/complexity will be conducted in the following\nsubsection. Overall, EDLGP needs a reasonably long running\ntime to complete the evolutionary process but uses a short time\nfor classiﬁcation.\nC. Convergence Behaviours\nWe take the ORL and Extended Yale B datasets as examples\nto show the convergence behaviour of EDLGP. Note that\nits convergence behaviour on other datasets shows similar\npatterns. Figure 12 shows that EDLGP has good search ability\nand can converge to a high ﬁtness value (i.e. accuracy on the\ntraining set) after 50 generations. Using different training set\nsizes, EDLGP achieves different ﬁtness values during evolu-\ntion. Speciﬁcally, more training data corresponds to higher\nﬁtness values of EDLGP on these two datasets. This is because\nthe ﬁtness function evaluates the generalisation of the learned\nmodel by using k-fold cross-validation on the training set. To\nsum up, EDLGP has good search ability and convergence, and\ncan ﬁnd the best model through evolution.\nD. Interpretability of Trees/Models\nThis subsection analyses model length/complexity and vi-\nsualisation to demonstrate the high interpretability of models\nlearned by EDLGP.\nTree/Model Size: The average tree size of EDLGP on\nsCIFAR10, ORL and Extended Yale B is shown in Fig. 13.\nOn sCIFAR10 with 1-128 training images, the average tree\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n12\nsCIFAR10\nFig. 13. The average size (number of nodes) of trees evolved by EDLGP on\nsCIFAR10, ORL, and Extended Yale B of different training set sizes.\nMin\nSIFT\nLBP\nGray\nBlue\nERF\nCC_SVM\nERF\nSum4\nERF\nERF\nCC_SVM\nGau_FE\nSub_MaxP\nGray\nGray\nSIFT\nGray\nComb4\nConca\nGabor\nBlue\nBlue\n2\n1\n4\nLBP\nSqrt\nGray\nRed\nComb2\nHOG_FE\nSqrt\nSIFT\nHOG_F\nGreen\nBlue\nComb2\nSobel_FE\nGabor\n3\n4\n500\n20\n500\n40\n850\n70\n350\n40\nFig. 14. An example tree of EDLGP on sCIFAR10 using 80 training images\nper class. This tree achieves 52.26% accuracy on the test set, which is better\nthan any of the comparison methods.\nsize ranges from 42.6 to 73.8. EDLGP tends to gradually\nincrease its tree size with the number of training images on\nsCIFAR10, which is a difﬁcult task. On the ORL and Extended\nYale B datasets, the average tree size ranges from 35.5 to 62.\nIn case 1 (2 training images on ORL and 15 training images\non Extended Yale B), EDLGP has a small initial ﬁtness value\n(as shown in Fig. 12) and seems to improve tree quality by\ngradually increasing the tree size. When using more training\nimages on these two datasets (i.e. in cases 3 and 4), EDLGP\nreaches a higher ﬁtness value in initial generations and ﬁnds\nrelatively smaller trees than that in case 1. To sum up, EDLGP\ncan evolve variable-length trees on different datasets with\nvarious numbers of training images.\nTree/Model Visualisation: Figure 14 shows an example\ntree of EDLGP on sCIFAR10-80. This tree is an ensemble\nof four ERF ensemble classiﬁers. Each branch can build one\nERF classiﬁer using different features generated from the\ncorresponding child nodes and different parameter settings\n(i.e. number of decision trees and maximal tree depth). It\nprocesses images using Min, Sub MaxP, Gabor, Sqrt, and\nHOG F operators on the input image in the gray, red, blue,\nand green channels, and extracts features using SIFT, LBP,\nGau FE, Conca, HOG FE, and Sobel FE operators. CIFAR10\nis a complex object classiﬁcation dataset so that different types\nof features are extracted to improve classiﬁcation performance.\nBy using such a model, EDLGP achieves 52.26% test ac-\ncuracy, which is the best accuracy among all the methods\non sCIFAR10 using only 80 training images per class. More\nimportantly, the size of this tree is much smaller than those\nCNNs.\nTo further demonstrate the interpretability of models/trees\nevolved by EDLGP, a small tree on sFMNIST-640 is analysed.\nSIFT\nCC_RF\nLR\nCC_RF\nCC_RF\nLR\nSum2\nSobel_FE\nGray\n800\n60\n500\n60\n500\n60\nGray\nSIFT\nCC_RF\nLR\nSum2\nSobel_FE\nCC_RF\nCC_RF\nLR\nSum2\nFig. 15.\nAn example tree evolved by EDLGP on sFMNIST-640 and\nvisualisation of the test data transformed by each internal node of the tree.\nT-SNE [68] is used for visualisation. Each plot is drawn using 100 testing\nimages per class of sFMNIST and each colour represents one class.\nThe small tree is shown in Fig. 15. It achieves 86.7% test\naccuracy on sFMNIST-640. We use T-SNE [68] to visualise\nthe data (i.e., features) generated by each internal node of the\nGP tree. For better visualisation, 100 test images per class are\nused and each test image is fed into the GP tree. We collect\nall the outputs of each internal node by feeding these test\nimages and use T-SNE to perform visualisation on the outputs,\nas shown in Fig. 15. This example tree has two branches to\ngenerate an ensemble of LR classiﬁers. For the left branch, it\nextracts SIFT features, uses CC RF to generate a new feature\nvector, and performs classiﬁcation using LR. The right branch\nextracts features using Sobel FE, generates new features using\ntwo CC RF functions, and performs classiﬁcation using LR.\nThe visualised data in Fig. 15 shows that the function at each\nnode makes signiﬁcant changes to the input images/features.\nThe data generated by the high-level nodes are more clustered.\nAt the output node, the data generated from the Sum2 node\nare clearly clustered, which can reveal why this model can\nachieve high classiﬁcation accuracy.\nTo sum up, EDLGP evolves trees with different lengths,\nshapes and depths on different datasets. A single GP tree is\na model that can perform image feature extraction and classi-\nﬁcation using ensembles of classiﬁers. The functions/internal\nnodes of GP trees can make important transformations on the\ndata to generate better ones for achieving good performance.\nThe GP trees are easy to be visualised and some insights can\nbe gained from them.\nE. Transferability of Trees/Models\nThis subsection analyses the transferability of trees/models\nfound by EDLGP to show the possibility of model reuse.\nThe best EDLGP models found from sCIFAR10 using 1-128\ntraining images per class is transferred to solve sSVNH using\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n13\n10-1280 training images per class. The average accuracy (%) is\npresented in Table XI. The evolved EDLGP models are also\ntransferred between the ORL and Extended Yale B datasets\nof different training set sizes. The average accuracy (%) is\npresented in Table XII. Note that in the transferring setting\nthe classiﬁers of the GP trees learned on the source training\nset are re-trained using the training set of the target task and\nthe GP trees are used to classify the test set of the target task.\nTABLE XI\nAVERAGE CLASSIFICATION ACCURACY (%) ON SSVHN USING THE\nMODELS LEARNED BY EDLGP FROM SCIFAR10-1 TO SCIFAR10-128.\nsCIFAR10 −→sSVHN\n10\n20\n40\n80\n160\n320\n640\n1280\nsCIFAR10-1\n28.2\n32.3\n37.0\n42.3\n47.4\n52.0\n56.0 59.2\nsCIFAR10-2\n23.3\n26.9\n29.5\n32.2\n34.1\n35.9\n37.5 38.8\nsCIFAR10-4\n34.8\n41.6\n46.7\n51.6\n55.8\n59.1\n61.5 62.8\nsCIFAR10-8\n27.9\n31.9\n36.3\n39.4\n42.8\n45.9\n48.5 50.9\nsCIFAR10-16\n40.6\n47.5\n53.0\n58.9\n63.0\n67.0\n70.0 71.8\nsCIFAR10-32\n46.8\n54.4\n60.0\n65.0\n68.1\n71.3\n73.5 75.1\nsCIFAR10-64\n52.0\n59.4\n63.8\n68.4\n71.5 74.6\n76.5 78.2\nsCIFAR10-128\n47.2\n55.5\n61.4\n67.6\n71.4 75.2\n77.5 79.1\nResNet-20(original)\n20.3\n40.0\n54.7\n74.1\n83.5\n86.7\n89.5 92.2\nEDLGP(original)\n55.5\n60.2\n70.1\n73.6\n76.9\n79.0\n80.5 82.3\nTable XI lists the classiﬁcation results on sSVHN (target\ndataset) by reusing the best trees/models evolved by EDLGP\non few-shot sCIFAR10 (source dataset). The ﬁnal two rows\nlist the original results of ResNet-20 and EDLGP learning\nfrom the target training set of sSVHN for comparisons. The\nresults show that the learned EDLGP models have very\nstrong transferability since they achieve better classiﬁcation\nperformance than ResNet-20 on sSVHN-10, sSVHN-20 and\nsSVHN-40 and very close accuracy to EDLGP in all cases.\nComparing with other CNN methods in Table VII, the reused\nmodels can also obtain higher accuracy in most cases. The\ncomparison can better demonstrate good transferability of\nthe EDLGP models. From Table VII, we can also ﬁnd that\nthe transferability of the EDLGP models is also affected by\nthe number of training instances in the source training set.\nSpeciﬁcally, a relatively larger source training set can lead to\nbetter transferability of the EDLGP models on sSVHN. This\nis reasonable because both CIFAR10 and SVHN are object\nclassiﬁcation tasks and more data can lead to learn more\ngeneralised models. However, EDLGP does not heavily rely\non increasing the the training set to improve its performance\nand the model transferability, since the models evolved on\nsCIFAR10-64 achieve the best performance on sSVHN-10,\nsSVHN-20, sSVHN-40, and sSVHN-80.\nTable XII shows the results obtained by transferring EDLGP\nmodels across the ORL and Extended Yale B datasets. The\nﬁnal row lists the results obtained by EDLGP using the\ntarget training set for model learning. From ORL to Extended\nYale B, the reused EDLGP models achieve worse accuracy\nwhen using 15 training images per class, but very high\naccuracy (i.e. 88.6%) when using 30 training images per\nclass. From Extended Yale B to ORL, the reused GP models\nobtain better accuracy than some comparison methods on ORL\nwith different numbers of training images, e.g., better than\nEigenface on all cases, better than CNN on ORL-2, ORL-3\nand ORL4. The results can show the high transferability of the\nlearned GP models across different face image classiﬁcation\ntasks. Table XII shows that using different training set sizes\nin the source and target tasks, the models achieve different\nperformances. In the case of ORL−→Extended Yale B, models\nlearned from a small training set of ORL can achieve better\nperformance on Extended Yale B. In the case of Extended Yale\nB−→ORL, models learned from 20 source training images\nper class achieve better performance on ORL with different\ntraining set sizes. The two face image datasets are different\nin terms of image variations, which may affect the generality\nand transferability of the models learned by EDLGP.\nTo sum up, the results show that the models/trees learned by\nEDLGP have high transferability. An important reason is that\nthe learned models/trees consisting of operators and algorithms\ncan represent not only domain-speciﬁc information but also\ngeneral-domain information, which enable them to have strong\ntransferability. This reveals the high possibility of reusing the\nlearned EDLGP models for solving other similar or related\ntasks. In addition, the results show that the source training\nset sizes may affect the transferability of the EDLGP models,\nwhich provides insights on effective model reuse.\nTABLE XII\nAVERAGE TEST ACCURACY (%) BY TRANSFERRING GP MODELS\nBETWEEN THE ORL AND EXTENDED YALE B DATASETS USING\nDIFFERENT NUMBERS OF TRAINING IMAGES\nORL −→Extended Yale B\nExtended Yale B −→ORL\n15\n20\n25\n30\n2\n3\n4\n5\nORL-2\n76.7\n81.8 85.9 88.6\nExtended.-15 62.3 73.8\n80.3\n83.2\nORL-3\n70.5\n76.7\n80.9 85.6\nExtended.-20\n71.2 83.8\n89.3 91.5\nORL-4\n68.7\n74.0\n77.9 81.5\nExtended.-25 55.5 68.1\n76.2\n81.7\nORL-5\n69.4\n76.2\n80.3 84.2\nExtended.-30 69.6 82.6\n88.0 91.8\nEDLGP 99.80 99.8\n99.8 99.8\nEDLGP\n86.4 96.4\n98.4\n99.3\nF. Summary\nThe following observations of EDLGP as a typical exam-\nple of GP-based EDL method can be summarised from the\nexperimental analysis.\n• EDLGP can achieve better performance than the com-\npared methods in most cases on CIFAR10, Fash-\nion MNIST, SVHN, ORL, and Extended Yale B. When\nthe training set is very small, EDLGP can achieve state-\nof-the-art performance. This shows that EDLGP is an\neffective approach to data-efﬁcient image classiﬁcation.\n• EDLGP shows good convergence and can ﬁnd trees with\ndifferent shapes, sizes and depths on different detasets.\nCompared with CNN-based image classiﬁcation methods,\nEDLGP does not require manually tune/design/determine\nthe model architectures and/or coefﬁcients/parameters.\n• EDLGP can evolve small and easy-interpretable trees to\nachieve high accuracy. The evolved EDLGP trees are\ncomposed of image and classiﬁcation domain operators,\nwhich are interpretable to provide more insights into the\ntasks. This is difﬁcult to be achieved by using CNN\nmethods with numerous parameters.\n• The trees/models evolved by EDLGP show high transfer-\nability, facilitating model reuse and development in the\nfuture.\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n14\nVI. CONCLUSIONS\nIn this paper, a GP-based EDL method, EDLGP, was\nproposed for automatically evolving variable-length models\nfor image classiﬁcation on small training data. A new rep-\nresentation was developed that includes a multi-layer tree\nstructure, a function set and a terminal set, enabling EDLGP to\nefﬁciently build models to perform feature extraction, concate-\nnation, classiﬁcation and cascade, and ensemble construction,\nautomatically and simultaneously. The EDLGP approach has\nshown great potential in solving data-efﬁcient image classi-\nﬁcation by achieving better performance than many effective\nmethods. A detailed analysis showed that the models learned\nby EDLGP have good convergence, high interpretability, and\ngood transferability. Compared with existing popular CNN-\nbased image classiﬁcation methods, the EDLGP approach as a\nGP-based EDL approach has a number of advantages, such as\nwithout requiring expensive GPU devices to run and rich do-\nmain expertise to design/tune model architectures/coefﬁcients,\ndata-efﬁcient, and evolving variable-length models with high\ninterpretability and transferability.\nThis paper is a starting point showing the superiority of\nGP in comparisons with CNN-based methods for data-efﬁcient\nimage classiﬁcation. More effective GP-based EDL methods\ncan be developed to dig out the potential of GP in the future.\nREFERENCES\n[1] J. Schindelin and et al., “Fiji: an open-source platform for biological-\nimage analysis,” Nature methods, vol. 9, no. 7, pp. 676–682, 2012.\n[2] C. A. Schneider, W. S. Rasband, and K. W. Eliceiri, “Nih image to\nimagej: 25 years of image analysis,” Nature methods, vol. 9, no. 7, pp.\n671–675, 2012.\n[3] D. Lu and Q. Weng, “A survey of image classiﬁcation methods and\ntechniques for improving classiﬁcation performance,” Int. J. Remote\nSens., vol. 28, no. 5, pp. 823–870, 2007.\n[4] Y. Bi, B. Xue, and M. Zhang, Genetic Programming for Image Classiﬁ-\ncation: An Automated Approach to Feature Learning.\nSpringer, 2021.\n[5] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[6] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A\nreview and new perspectives,” IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 35, no. 8, pp. 1798–1828, 2013.\n[7] Z.-H. Zhou and J. Feng, “Deep forest,” Natl. Sci. Rev., vol. 6, no. 1, pp.\n74–86, Oct 2018.\n[8] L. Alzubaidi, J. Zhang, A. J. Humaidi, A. Al-Dujaili, Y. Duan, O. Al-\nShamma, J. Santamar´ıa, M. A. Fadhel, M. Al-Amidie, and L. Farhan,\n“Review of deep learning: Concepts, cnn architectures, challenges,\napplications, future directions,” J. Big Data, vol. 8, no. 1, pp. 1–74,\n2021.\n[9] T.-H. Chan, K. Jia, S. Gao, J. Lu, Z. Zeng, and Y. Ma, “Pcanet: A\nsimple deep learning baseline for image classiﬁcation?” IEEE Trans.\nImage Process., vol. 24, no. 12, pp. 5017–5032, 2015.\n[10] Z.-H. Zhan, J.-Y. Li, and J. Zhang, “Evolutionary deep learning: A\nsurvey,” Neurocomputing, 2022.\n[11] H. Al-Sahaf, Y. Bi, Q. Chen, A. Lensen, Y. Mei, Y. Sun, B. Tran, B. Xue,\nand M. Zhang, “A survey on evolutionary machine learning,” J. Roy. Soc.\nNew Zeal., vol. 49, no. 2, pp. 205–228, 2019.\n[12] T. B¨ack, D. B. Fogel, and Z. Michalewicz, “Handbook of evolutionary\ncomputation,” Release, vol. 97, no. 1, p. B1, 1997.\n[13] X. Zhou, A. Qin, M. Gong, and K. C. Tan, “A survey on evolutionary\nconstruction of deep neural networks,” IEEE Trans. Evol. Comput., 2021.\n[14] Y. Liu, Y. Sun, B. Xue, M. Zhang, G. G. Yen, and K. C. Tan, “A survey\non evolutionary neural architecture search,” IEEE Trans. Neural Netw.\nLearn. Syst., 2021, DOI: 10.1109/TNNLS.2021.3100554.\n[15] A. D. Martinez, J. Del Ser, E. Villar-Rodriguez, E. Osaba, J. Poyatos,\nS. Tabik, D. Molina, and F. Herrera, “Lights and shadows in evolutionary\ndeep learning: Taxonomy, critical methodological analysis, cases of\nstudy, learned lessons, recommendations and challenges,” Information\nFusion, vol. 67, pp. 161–194, 2021.\n[16] W. B. Langdon and R. Poli, Foundations of GeneticProgramming.\nSpringer Science & Business Media, 2013.\n[17] Y. Bi, B. Xue, and M. Zhang, “Dual-tree genetic programming for few-\nshot image classiﬁcation,” IEEE Trans. Evol. Comput., vol. 26, no. 3,\npp. 555–569, 2022.\n[18] ——, “Multi-objective genetic programming for feature learning in face\nrecognition,” Appl. Soft Comput., vol. 103, p. 107152, 2021.\n[19] ——, “Genetic programming with image-related operators and a ﬂexible\nprogram structure for feature learning in image classiﬁcation,” IEEE\nTrans. Evol. Comput., vol. 25, no. 1, pp. 87–101, 2021.\n[20] Y. Sun, B. Xue, M. Zhang, and G. G. Yen, “Evolving deep convolutional\nneural networks for image classiﬁcation,” IEEE Trans. Evol. Comput.,\nvol. 24, no. 2, pp. 1–14, 2019.\n[21] Y. Sun, B. Xue, M. Zhang, and G. G. Yen, “Completely automated cnn\narchitecture design based on blocks,” IEEE Trans. Neural Netw. Learn.\nSyst., vol. 31, no. 4, pp. 1242–1254, 2019.\n[22] Z. Lu, I. Whalen, Y. Dhebar, K. Deb, E. D. Goodman, W. Banzhaf, and\nV. N. Boddeti, “Multiobjective evolutionary design of deep convolutional\nneural networks for image classiﬁcation,” IEEE Trans. Evol. Comput.,\nvol. 25, no. 2, pp. 277–291, 2020.\n[23] H. Zhu and Y. Jin, “Real-time federated evolutionary neural architecture\nsearch,” IEEE Trans. Evol. Comput., vol. 26, no. 2, pp. 364–378, 2022.\n[24] H. Zhang, Y. Jin, R. Cheng, and K. Hao, “Efﬁcient evolutionary search\nof attention convolutional networks via sampled training and node\ninheritance,” IEEE Trans. Evol. Comput., vol. 25, no. 2, pp. 371–385,\n2020.\n[25] L. Rodriguez-Coayahuitl, A. Morales-Reyes, and H. J. Escalante, “Struc-\nturally layered representation learning: Towards deep learning through\ngenetic programming,” in Proc. EuroGP.\nSpringer, 2018, pp. 271–288.\n[26] L. Shao, L. Liu, and X. Li, “Feature learning for image classiﬁcation via\nmultiobjective genetic programming,” IEEE Trans. Neural Netw. Learn.\nSyst., vol. 25, no. 7, pp. 1359–1371, 2014.\n[27] Y. Bi, B. Xue, and M. Zhang, “An effective feature learning approach\nusing genetic programming with image descriptors for image classiﬁca-\ntion,” IEEE Comput. Intell. Mag., vol. 15, no. 2, pp. 65–77, 2020.\n[28] ——, “Genetic programming with a new representation to automatically\nlearn features and evolve ensembles for image classiﬁcation,” IEEE\nTrans. Cybern., vol. 51, no. 4, pp. 1769–1783, 2021.\n[29] R.-J. Bruintjes, A. Lengyel, M. B. Rios, O. S. Kayhan, and J. van\nGemert, “Vipriors 1: Visual inductive priors for data-efﬁcient deep\nlearning challenges,” arXiv preprint arXiv:2103.03768, 2021.\n[30] A. Lengyel, R.-J. Bruintjes, M. B. Rios, O. S. Kayhan, D. Zam-\nbrano, N. Tomen, and J. van Gemert, “Vipriors 2: Visual induc-\ntive priors for data-efﬁcient deep learning challenges,” arXiv preprint\narXiv:2201.08625, 2022.\n[31] S. Arora, S. S. Du, Z. Li, R. Salakhutdinov, R. Wang, and D. Yu,\n“Harnessing the power of inﬁnitely wide deep nets on small-data tasks,”\nin Proc. ICLR, 2019.\n[32] L. Brigato and L. Iocchi, “A close look at deep learning with small\ndata,” in Proc. IEEE ICPR, 2021, pp. 2490–2497.\n[33] Y. Bi, B. Xue, and M. Zhang, “Using a small number of training\ninstances in genetic programming for face image classiﬁcation,” Inf.\nSci., 2022.\n[34] B. Barz and J. Denzler, “Deep learning on small datasets without pre-\ntraining using cosine loss,” in Proc. IEEE/CVF WCACV, 2020, pp. 1371–\n1380.\n[35] L. Brigato, B. Barz, L. Iocchi, and J. Denzler, “Tune it or don’t use it:\nBenchmarking data-efﬁcient image classiﬁcation,” in Proc. IEEE ICCV,\n2021, pp. 1071–1080.\n[36] P. Sun, X. Jin, W. Su, Y. He, H. Xue, and Q. Lu, “A visual inductive\npriors framework for data-efﬁcient image classiﬁcation,” in Proc. ECCV.\nSpringer, 2020, pp. 511–520.\n[37] B. Zhao and X. Wen, “Distilling visual priors from self-supervised\nlearning,” in Proc. ECCV.\nSpringer, 2020, pp. 422–429.\n[38] H. Al-Sahaf, M. Zhang, and M. Johnston, “Binary image classiﬁcation:\nA genetic programming approach to the problem of limited training\ninstances,” Evol. Comput., vol. 24, no. 1, pp. 143–182, 2016.\n[39] Y. Bi, B. Xue, and M. Zhang, “Learning and sharing: A multitask genetic\nprogramming approach to image feature learning,” IEEE Trans. Evol.\nComput., vol. 26, no. 2, pp. 218–232, 2022.\n[40] D. J. Montana, “Strongly typed genetic programming,” Evol. Comput.,\nvol. 3, no. 2, pp. 199–230, 1995.\n[41] L. Zhang, J. Liu, B. Zhang, D. Zhang, and C. Zhu, “Deep cascade\nmodel-based face recognition: When deep-layered learning meets small\ndata,” IEEE Trans. Image Process., vol. 29, pp. 1016–1029, 2019.\n[42] N. Dalal and B. Triggs, “Histograms of oriented gradients for human\ndetection,” in Proc. IEEE CVPR, vol. 1, 2005, pp. 886–893.\nIEEE TRANSACTIONS ON XXXX, VOL. XX, NO. X, MONTH YEAR\n15\n[43] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale\nand rotation invariant texture classiﬁcation with local binary patterns,”\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 24, no. 7, pp. 971–987,\n2002.\n[44] A. Vedaldi and B. Fulkerson, “VLFeat: An open and portable library of\ncomputer vision algorithms,” in Proc. 18th ACM Int. Conf. Multimedia,\n2010, pp. 1469–1472.\n[45] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features\nfrom tiny images,” 2009.\n[46] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image\ndataset for benchmarking machine learning algorithms,” arXiv preprint\narXiv:1708.07747, 2017.\n[47] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng,\n“Reading digits in natural images with unsupervised feature learning,”\n2011.\n[48] F. S. Samaria and A. C. Harter, “Parameterisation of a stochastic model\nfor human face identiﬁcation,” in Proc. IEEE Workshop Appl. Comput.\nVis., 1994, pp. 138–142.\n[49] K.-C. Lee, J. Ho, and D. J. Kriegman, “Acquiring linear subspaces for\nface recognition under variable lighting,” IEEE Trans. Pattern Anal.\nMach. Intell., no. 5, pp. 684–698, May 2005.\n[50] T. Liao, Z. Lei, T. Zhu, S. Zeng, Y. Li, and C. Yuan, “Deep metric\nlearning for k nearest neighbour classiﬁcation,” IEEE Trans. Knowl.\nData Eng., 2021.\n[51] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. IEEE CVPR, 2016, pp. 770–778.\n[52] M. Ulicny, V. A. Krylov, and R. Dahyot, “Harmonic networks with\nlimited training samples,” in Proc. EUSIPCO.\nIEEE, 2019, pp. 1–5.\n[53] M. Turk and A. Pentland, “Eigenfaces for recognition,” J. Cogn.\nNeurosci., vol. 3, no. 1, pp. 71–86, 1991.\n[54] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, “Eigenfaces vs.\nﬁsherfaces: Recognition using class speciﬁc linear projection,” IEEE\nTrans. Pattern Anal. Mach. Intell., vol. 19, no. 7, pp. 711–720, 1997.\n[55] X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang, “Face recognition\nusing laplacianfaces,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 27,\nno. 3, pp. 328–340, 2005.\n[56] X. He, D. Cai, S. Yan, and H.-J. Zhang, “Neighborhood preserving\nembedding,” in Proc. IEEE ICCV, vol. 2.\nIEEE, 2005, pp. 1208–1213.\n[57] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin, “Graph\nembedding and extensions: A general framework for dimensionality\nreduction,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 1, pp.\n40–51, 2006.\n[58] L. Zhang, M. Yang, and X. Feng, “Sparse representation or collaborative\nrepresentation: Which helps face recognition?” in Proc. ICCV, 2011, pp.\n471–478.\n[59] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Robust face\nrecognition via sparse representation,” IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 31, no. 2, pp. 210–227, 2008.\n[60] R. He, W.-S. Zheng, and B.-G. Hu, “Maximum correntropy criterion\nfor robust face recognition,” IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 33, no. 8, pp. 1561–1576, 2010.\n[61] M. Yang, L. Zhang, J. Yang, and D. Zhang, “Robust sparse coding for\nface recognition,” in Proc. IEEE CVPR, 2011, pp. 625–632.\n[62] R. He, W.-S. Zheng, T. Tan, and Z. Sun, “Half-quadratic-based iterative\nminimization for robust sparse representation,” IEEE Trans. Pattern\nAnal. Mach. Intell., vol. 36, no. 2, pp. 261–275, 2013.\n[63] J. Yang, L. Luo, J. Qian, Y. Tai, F. Zhang, and Y. Xu, “Nuclear\nnorm based matrix regression with applications to face recognition with\nocclusion and illumination changes,” IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 39, no. 1, pp. 156–171, 2016.\n[64] J. Xie, J. Yang, J. J. Qian, Y. Tai, and H. M. Zhang, “Robust nuclear\nnorm-based matrix regression with applications to robust face recogni-\ntion,” IEEE Trans. Image Process., vol. 26, no. 5, pp. 2286–2295, 2017.\n[65] M. Yang, L. Zhang, X. Feng, and D. Zhang, “Sparse representation\nbased ﬁsher discrimination dictionary learning for image classiﬁcation,”\nInt. J. Comput. Vis., vol. 109, no. 3, pp. 209–232, 2014.\n[66] T. H. Vu and V. Monga, “Fast low-rank shared dictionary learning for\nimage classiﬁcation,” IEEE Trans. Image Process., vol. 26, no. 11, pp.\n5160–5175, 2017.\n[67] Y. Bi, B. Xue, and M. Zhang, “An evolutionary deep learning approach\nusing genetic programming with convolution operators for image clas-\nsiﬁcation,” in Proc. IEEE CEC, 2019, pp. 3197–3204.\n[68] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” J. Mach.\nLearn. Res., vol. 9, pp. 2579–2605, 2008.\n",
  "categories": [
    "cs.NE",
    "cs.AI",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2022-09-27",
  "updated": "2022-09-27"
}