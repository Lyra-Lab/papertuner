{
  "id": "http://arxiv.org/abs/2209.13953v1",
  "title": "ArNLI: Arabic Natural Language Inference for Entailment and Contradiction Detection",
  "authors": [
    "Khloud Al Jallad",
    "Nada Ghneim"
  ],
  "abstract": "Natural Language Inference (NLI) is a hot topic research in natural language\nprocessing, contradiction detection between sentences is a special case of NLI.\nThis is considered a difficult NLP task which has a big influence when added as\na component in many NLP applications, such as Question Answering Systems, text\nSummarization. Arabic Language is one of the most challenging low-resources\nlanguages in detecting contradictions due to its rich lexical, semantics\nambiguity. We have created a data set of more than 12k sentences and named\nArNLI, that will be publicly available. Moreover, we have applied a new model\ninspired by Stanford contradiction detection proposed solutions on English\nlanguage. We proposed an approach to detect contradictions between pairs of\nsentences in Arabic language using contradiction vector combined with language\nmodel vector as an input to machine learning model. We analyzed results of\ndifferent traditional machine learning classifiers and compared their results\non our created data set (ArNLI) and on an automatic translation of both PHEME,\nSICK English data sets. Best results achieved using Random Forest classifier\nwith an accuracy of 99%, 60%, 75% on PHEME, SICK and ArNLI respectively.",
  "text": "The paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 1 of 20 \n \nArNLI: Arabic Natural Language Inference for Entailment and Contradiction Detection \nKhloud Al Jallad, Nada Ghneim \nk-aljallad@aiu.edu.sy, n-ghneim@aiu.edu.sy \n \nFaculty of Information Technology Engineering, Arab International University, Daraa, Syria \n \nABSTRACT \nNatural Language Inference (NLI) is a hot topic research in natural language processing, \ncontradiction detection between sentences is a special case of NLI. This is considered a \ndifficult NLP task which has a big influence when added as a component in many NLP \napplications, such as Question Answering Systems, text Summarization. Arabic Language is \none of the most challenging low-resources languages in detecting contradictions due to its \nrich lexical, semantics ambiguity. We have created a data set of more than 12k sentences \nand named ArNLI, that will be publicly available. Moreover, we have applied a new model \ninspired by Stanford contradiction detection proposed solutions on English language. We \nproposed an approach to detect contradictions between pairs of sentences in Arabic \nlanguage using contradiction vector combined with language model vector as an input to \nmachine learning model. We analyzed results of different traditional machine learning \nclassifiers and compared their results on our created data set (ArNLI) and on an automatic \ntranslation of both PHEME, SICK English data sets. Best results achieved using Random \nForest classifier with an accuracy of 99%, 60%, 75% on PHEME, SICK and ArNLI \nrespectively. \n \nKEYWORDS \nTextual Entailment, Arabic NLP, Contradiction Detection, Contradiction Arabic Dataset, \nTextual Inference \n1. INTRODUCTION \nNatural language inference (NLI) is the task of determining whether a given hypothesis can \nbe inferred from a given premise. This task, formerly known as recognizing textual \nentailment (RTE)  has long been a popular task among researchers [1]. As an improvement \nover the simple binary Entailment vs Non-entailment scenario, three-way RTE has appeared \nand commonly used (Entailment, Contradiction, Neutral (Unknown)). The Entailment relation \nbetween two text fragments holds if the claim present in fragment B can be concluded from \nfragment A. The Contradiction relation applies when the claim in A and the claim in B cannot \nbe true together. The Neutral relation applies if A and B neither entail nor contradict each \nother.  \nThe main impact is that RTE can transfer problem from text data set language processing to \nalgebra sets and logical implications, for that reason RTE has a big influence when added as \na component in many NLP applications, as it can simplify problems. \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 2 of 20 \n \nTextual Inference is a key capability for improving performance in a wide range of NLP tasks \n[2], such as Question Answering Systems [3], Text Summarization1 2,  next-generation \nInformation Retrieval [3], Machine Reading [4] [5], Machine Translation [6], Natural \nLanguage Understanding (NLU) [7], Anaphora Resolution [8] and Argumentation Mining [9].  \nSince 2005, several challenges have been coordinated with the aim of providing concrete \ndata sets that the research community could use to test and compare their different \napproaches to recognize entailments.  \nHowever, RTE from Arabic text remains very little explored. Arabic Language is one of the \nmost challenging low-resources languages in detecting contradictions due to its lexical \nrichness and semantics ambiguity. Moreover, to the best of our knowledge there is no \navailable benchmark for contradiction detection task in Arabic language.  \nIn this paper, we introduce a new high quality data set for the NLI task for Arabic language. \nThis data set, named ArNLI, includes more than 6000 pairs of sentences annotated in 3-way \nrelation classes (entailment, contradiction, and neutral), where: \n \nContradiction indicates contradict between two texts, involving all types of De \nMarneffe et al. discussed in [10] (Antonym, Negation, Numeric, Factive, Structure, \nLexical, WK) \n \nEntailment indicates that two texts entail the same meaning. \n \nNeutral indicates that there is no relation between two texts. \nUsing different language modelling approaches (including word embeddings), and features \nof different language levels (lexical, semantic.), we evaluate different traditional classification \nmodels (Support Vector Machine (SVM), Stochastic Gradient Descent (SGD), Decision Tree \n(DT), ADA Boost, K-Nearest Neighbours (KNN), and Random Forest (RF)), and compare the \nresults with translation of famous English benchmarks because of lack of benchmarks in \nArabic.  \nThe rest of the paper is organized as follows: Section 2 will cover the related literature. \nSection 3 will present our methodology in details, and Section 4 will describe our created \nArabic RTE data set. Section 5 will then discuss the experiments results. Finally, in Section \n6, we conclude with future research directions \n2. RELATED WORKS  \nIn the recent past, Natural language Inference (NLI) (formerly known as RTE) has gained \nsignificant attention, particularly given its promise for downstream NLP tasks [4]. The \nmajority of researches done in NLI focus on two-way RTE (the simple binary Entailment vs \nNon-entailment scenario), whereas three-way RTE (Entailment, Contradiction, Neutral \n(Unknown)) that focus on contradiction has few number of researches. Recent statistics3 \nshows that researches in RTE focus on big data sets using deep learning models with \n                                                             \n1 NIST, \"PASCAL Recognizing Textual Entailment Challenge (RTE-5) at TAC 2009,\": \nhttps://tac.nist.gov/2009/RTE/ \n \n2 NIST, \"6th textual entailment challenge @ tac 2010 knowledge base population validation pilot task \nguidelines,\" TAC Workshop, 2010 \n3 https://paperswithcode.com/sota/natural-language-inference-on-rte \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 3 of 20 \n \ntransformers such as BERTNLI, RoBERTa, XLNET, DeBERTa. Thus, most progress in NLI \nhas been limited to English due to lack of reliable data sets for most of the world’s \nlanguages. In other languages, different research works have attempted to create data sets \nfor NLI such as for Japanese [11], Chinese [12], Portuguese [13], Italian [14], German [15], \nBrazilian [16] , Persian [17],and Turkish [18].  \nAs for Arabic language, although an Arabic data set for RTE4 exists, but it convers two-way \nRTE, and has only 600 pairs, which is considered not enough for any deep learning \nmethodology. In the rest of related works section, we will emphasis on three-way RTE that \nfocus on contradiction has few number of researches, as it is our research interest in this \npaper. \n2.1. Related Works on English Language \nHarabagiu et al. [19], presented the ﬁrst empirical results for contradiction detection (CD) as \na task of entailment recognition, but they focused on speciﬁc kinds of contradiction and \ndescribed a framework for detecting contradictions between sentences. The work has three \nbasic types of linguistic information: (a) negation; (b) relational and modality features, and (c) \nsemantic information. They created two corpora for evaluating their system. One was \nconstructed via negating each entailment in the RTE2 data5, generating a balanced data set \n(LCC1 negation data set). To keep away from overtraining, negative markers were also \nadded to every non-entailment, making sure that they are not contradictions. The other \ncorpus was created by paraphrasing the hypothesis sentences from LCC-negation to \nremove negations (LCC-paraphrase). They achieved accuracies of 75.63% on LCC-negation \nand 62.55% on LCC-paraphrase. \nRafferty and Manning in [10] proposed an appropriate deﬁnition of contradiction for NLP \ntasks and developed a corpus from which they constructed a typology of contradictions. \nThey found two primary categories of contradiction: (1) those occurring via antonym, \nnegation, and date/number mismatch, which are relatively simple to detect, and (2) \ncontradictions arising from the use of factive or modal words, structural and subtle lexical \ncontrasts, as well as world knowledge (WK). They considered contradictions in the first \ncategory ‘easy’ and can be obtained using existing resources and techniques (e.g., \nWordNet6, VerbOcean). However, contradictions in the second category were considered \nmore difﬁcult to detect automatically because they require precise models of sentence \nmeaning. Moreover ,they proposed a system based on the architecture of the Stanford RTE \nsystem [20] , however, they introduced a stage for event co-reference decision. The features \nused were: Polarity features, Number, date and time expression features, Antonym features, \nStructural features, Factivity corpora, one is based on RTE data set and the other is based \non ‘real life’ data. As the RTE data sets are balanced between entailments and non-\nentailments, RTE3-test data was annotated by NIST as part of the RTE3 Pilot task7 in which \nsystems classify pairs as entailed, contradictory, or neither. As for real life corpus8, they \ncollected 131 contradictory pairs: 19 from newswire, mainly looking at related articles in \nGoogle News, 51 from Wikipedia, 10 from the Lexis Nexis database, and 51 from the data \n                                                             \n4 Arabic Textual Entailment Dataset http://www.cs.man.ac.uk/~ramsay/ArabicTE/ \n5 \"Negation Datasets,\" Stanford. https://nlp.stanford.edu/projects/contradiction/ \n6   WordNet  https://wordnet.princeton.edu/ \n7 \"RTE3-pilot,\" stanford, 2007. https://nlp.stanford.edu/RTE3-pilot/ \n8 \"Negation Real Life Corpus,\" Stanford,https://nlp.stanford.edu/projects/contradiction/real_contradiction.xml \n \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 4 of 20 \n \nprepared by LDC for the distillation task of the DARPA GALE program. Despite the \nrandomness of the collection, they argued that this corpus may be best reﬂecting naturally \noccurring contradictions. \nRitter et al. [21], proposed Contradiction Detection using functions (e.g., BornIn (Person) = \nPlace), and a domain-independent algorithm that automatically detects sentences denoting \nfunctions. Their work was based on de Marneffe et al.’s work with a number of modifications. \nThey suggested that global world knowledge is important for constructing a domain-\nindependent system. Moreover, they automatically created a large corpus of obvious \ncontradictions found in arbitrary Web text. As for system evaluation, they used the 1,000 \nmost frequent relations extracted by TextRunner system [22], 75% were indeed functional. \nThey labelled by hand each of these 8,844 pairs as contradictory or not.  \nLi et al. in [23] used CNN-based (Convolutional Neural Network) model to learn the global \nand local semantic relation from sentences. They used contradiction-speciﬁc word \nembedding (CWE). CWE is learned from a training corpus that is automatically generated \nfrom the paraphrase database, and is used as features to implement contradiction detection \nin SemEval 2014 benchmark data set9. Shallow features extracted were: Number of \nnegation words, Difference of word order, Unaligned words. Experimental results show \noptimization on traditional context-based word embedding in contradiction detection as it \nimproved the accuracy from 75.97% to 82.08% in the contradiction class. \nSulea in [24] proposed to apply 3-way RTE in social media. The author worked on 5000 \npairs collected from Twitter to distinguish between tweets that entail or contradict each other \nor that claim unrelated things. They used neural networks and compare their results on word \nembeddings with the results obtained previously using classical “feature engineering” \nmethods. \nLingam et al. [25] proposed an approach for detecting three different types of contradiction: \nnegation, antonyms and numeric mismatch using neural networks and deep learning. They \nused Long Short-Term Memory (LSTM) and Global Vectors for Word Representation \n(GloVe)10 There are three feature combinations: manual features (Jaccard Coefﬁcient, \nIsNegation Flag, IsAntonym Flag, Overlap Coefﬁcient), LSTM based features and \ncombination of manual and LSTM features. They did experiments on three publicly available \ndata sets: Stanford data set, SemEval data set11 and PHEME data set12 [26]. In addition, \nthey constructed a data set and made it publicly available. They achieved 96.85% accuracy \nfor the contradiction class on the PHEME data set. \n \nMoreover, in the last few years many research papers have applied NLI for special domains \nor to optimize solutions for other complex NLP tasks. For example, Microsoft created a \ncorpus named Microsoft Research Paraphrase Corpus (MRPC) that consists of 5,801 \nsentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or \nnot by human annotators. [27] [28] \n                                                             \n9 \"SemEval2014\": http://alt.qcri.org/semeval2014/ \n10 \"GloVe,\" Stanford: https://nlp.stanford.edu/projects/glove/ \n11 \"SemEval2014\": http://alt.qcri.org/semeval2014/ \n12 \"Pheme,\" 2016: https://www.pheme.eu/2016/04/12/pheme-rte-dataset/ \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 5 of 20 \n \nWang et al. [29] proposed (GLUE13) General Language Understanding Evaluation \nbenchmark, a tool for evaluating and analyzing the performance of models across a diverse \nrange of existing NLU tasks based on NLI. Moreover, Wang et al [30] proposed SuperGlue \n[31] that is an improvement on Glue by having more challenging tasks, more diverse task \nformats and so on. Glue and SuperGlue contains The QNLI (Question-answering NLI) \ndataset that is a Natural Language Inference dataset automatically derived from the Stanford \nQuestion Answering Dataset v1.1 (SQuAD). SQuAD v1.1 consists of question-paragraph \npairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the \nanswer to the corresponding question (written by an annotator). \nAllen Institute for Artificial Intelligence's research created Abductive Natural Language \nInference (alphaNLI) [32] that is a common sense benchmark dataset designed to test an AI \nsystem’s capability to apply abductive reasoning and common sense to form possible \nexplanations for a given set of observations. Formulated as a binary-classification task, the \ngoal is to pick the most plausible explanatory hypothesis given two observations from \nnarrative contexts. \nYuta et al. [33] proposed a dataset for NLI in document-level to support Contract Review \nprocess automatically. They simply the problem by modeling it as multi-label classification \nover spans instead of trying to predict the start and end tokens and they  showed that Span \nNLI BERT outperforms the existing models. \nWang et al [34] solved many NLU tasks by transforming them into NLI task and systematic \nevaluation on 18 standard NLP tasks shows that it improves the various existing SOTA few-\nshot learning methods by 12%, and yields competitive few-shot performance with 500 times \nlarger models, such as GPT-3.  \n Liu et al. [35] proposed RoBERTa that is a BERT tuned model that achieves state-of-the-art \nresults on GLUE, RACE and SQuAD, without multi-task fine-tuning for GLUE or additional \ndata for SQuAD. \nHe et al. [36] proposed DeBERTa model architecture (Decoding-enhanced BERT with \ndisentangled attention) that improves the BERT and RoBERTa models using two novel \ntechniques(disentangled attention, enhanced mask decoder). Compared to RoBERTa-\nLarge, a DeBERTa model trained on half of the training data performs consistently better on \na wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), \non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). \n2.2. Related Works on Arabic Language \nIn Arabic language, only few researches were done in RTE domain. Textual entailment in \nArabic language faces various challenges due to the features of Arabic language [37] [38] \nand [39]. One of these challenges is lexical ambiguity, which is the difficulty to process texts \nwith missing diacritics. Another challenge is its richness in synonyms, where more than one-\nword surface may have the same meaning. In addition, Arabic still lack the large scale \nhandcrafted computational resources that is very practically used in English such as a large \nWordNet and so on. On the other hand, the lack of large entailment data set caused the lack \nof deep learning research experiments (only traditional machine learning methods are \nproposed). Alabbas [40] developed the system ArbTE, to evaluate the existing text \nentailment techniques when applied to Arabic language. In a next step, Alabbas suggested \n                                                             \n13 “Glue”,2018: https://gluebenchmark.com/ \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 6 of 20 \n \nin [41] extending the basic version of the Tree Edit Distance TED algorithm, to enhance the \nmatching algorithm to identify TE in Arabic. The author also created a publically available \ndata set for Arabic textual entailment ArbTEDS14 that consists of 618 text-hypothesis pairs \ncollected from Arabic news websites or from annotated pairs collected by hand. \nAlKhawaldeh et al. [42] concluded that the Arabic entailment accuracy can be enhanced by \nresolving negation for entailment relation and analyzing the polarity of the text-hypothesis \npair and determining the polarity of the text-hypothesis pair (Positive, Negative or Neutral). \nThey achieved an accuracy of  69% on ArbTEDS data set. \nAlmarwani et al. [37] applied SVM and Random Forest classifiers to detect RTE in Arabic \nusing word embeddings to overcome the lack of explicit lexical overlap between sentences \npairs T and H. They derived word vector representations for about 556K words. Other \nfeatures used, were: similarity scores, named entities, number of unique instances in T, \nnumber of unique instances in H, number of unique instances that are in T but not in H and \nvice versa, and number of instances that are in both H and T. All features were calculated at \ntoken, lemma, and stem levels. The system achieved an accuracy of 76.2% on ArbTEDS \ndata set. \nBoudaa et al. [43] used Support Vector Machine algorithm to detect RTE for Arabic \nlanguage. The following analysis were used in the pre-processing stage: Named Entities, \nTemporal Expressions, Number/Countable pairs, Ordinary Words (or sequence of ordinary \nwords). They extracted alignment based features to find an optimal weight matching in a \nweighted bipartite graph. The system achieved an accuracy of 75.84% on ArbTEDS data \nset. \nKhader et al. [39], applied a lexical analysis technique of Textual Entailment for Arabic \nlanguage. They added a semantic matching approach to enhance the precision of their \nsystem. Their lexical analysis is based on calculating word overlap and bigram extraction \nand matching. They combined semantic matching with word overlap to increase the \naccuracy of words matching. They achieved a precision of 68%, 58% for both Entails and \nNot-Entails respectively with an overall recall of 61% on ArbTEDS data set. \n3. OUR METHODOLOGY \n In this work, we created a data set and propose a system to detect the NLI in Arabic \nsentences, where target labels are Entailment, Contradiction and Neutral (no semantic \nrelation). Our system consists of three main parts: Text Pre-processing (cleaning, \ntokenization, stemming), Feature Extraction (Contradiction feature vector and language \nmodel vectors) and Machine Learning Model. Figure 1 shows Our Experiments Schema.  \nWe will discuss each step in details in the following subsections. \n                                                             \n14 Arabic Textual Entailment Dataset: http://www.cs.man.ac.uk/~ramsay/ArabicTE/ \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 7 of 20 \n \n \n3.1. OUR DATA SET \nTo the best of our knowledge there is no available Arabic Three-way Natural Language \nInference (NLI) data set. In order to build our data set, we started by translating two English \nRTE data sets: SICK data set [44] which was used in SemEval_2014_Task115, and PHEME \ndata set. The SICK data set consists of 10,000 English sentence pairs, each annotated for \nrelatedness in meaning and for entailment relation. PHEME data set, on the other hand, \ncontains 5400 RTE annotated pairs from social media. We named the Arabic automatically \ntranslated data sets Ar_SICK and Ar_PHEME respectively. \nAfter automatically translating the two data sets, we selected a subset of the annotated pairs \nand manually corrected their translations. We augmented this subset with a manually \ntranslated/annotated pairs from pre-existed sources. Our final Arabic Natural Language \nInference (NLI) data set16 (ArNLI) contains 6366 pairs divided as (1932 entailment, 1073 \ncontradiction, and 3361 neutral).  The data set is collected as following: \n \n5948 pairs of AR_SICK data set sentences that were semi-automatically translated \nand corrected (1714 entailment, 895 contradiction, 3339 neutral pairs) \n \n312 pairs of ArbTEDS corpus17, that  we had to re-annotate its sentences from \n(Entails, Not Entails) classes into our 3-way RTE classes considered in this study \n(194 entailment, 113 contradiction, 5 neutral pairs). \n \n35 pairs of Stanford Real Life contradiction corpus [10], which was manually \ntranslated (0 entailment, 35 contradictions, 0 neutral pairs) \n                                                             \n15 \"semeval2014_task1,\" 2014. [Online]. Available: https://alt.qcri.org/semeval2014/task1/. \n \n17Arabic Textual Entailment Dataset http://www.cs.man.ac.uk/~ramsay/ArabicTE/ \n \n \nFigure 1 Our Experiments Schema \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 8 of 20 \n \n \n71 pairs of manually annotated sentences (collected from online websites teaching \nArabic contradiction, poems, idioms, paraphrased pairs of Ar_PHEME data set.) with \n(24 entailments, 30 contradictions, 17 neutral pairs).  \nThe key statistics of our created data set (ArNLI) are shown in Table 1. \nData Size \n \nTraining pairs \n5092 \nTesting pairs \n1274 \n \nAvg. Sentence Length in tokens \n \nHypothesis \n6.623 \nPremise \n7.246 \nMax. Sentence Length in tokens \n \nHypothesis \n26 \nPremise \n57 \nTable 1 Key Statistics of ArNLI Dataset \n \n3.2. Text Pre-processing \nIn this step, we first tokenized the sentences and removed all punctuations. To extract the \nmorphological units, we used Snowball Stemmer which is also known as the Porter2 \nstemming algorithm. \nTable 2 presents examples of each step in pre-processing stage \nStage \nSentence1 \nSentence2 \n \n عملنا في هذا البحث على فهم عالقات االستدالل و\n استخراجها بين الجمل في جميع اللغات، وليس فقط اللغة\nالعربية ..\n \n عملنا في هذا البحث على اكتشاف عالقات االستدالل و\nالتناقضات بين الجمل في اللغة العربية فقط، لم نعمل ع لى\n! اكتشافها في باقي اللغات \nTokenization \n['..', 'عملنا', 'في', 'هذا', 'البحث', 'على', 'فهم', 'عال\nقات', \n'االستدالل', 'و', 'استخراجها', 'بين', 'الجمل', 'في', 'جميع', \n'،اللغات', 'وليس', 'فقط', 'اللغة', 'العربية'] \n['عملنا', 'في', 'هذا', 'البحث', 'على', 'اكتشاف', 'عال\nقات', \n'االستدالل', 'و', 'التناقضات', 'بين', 'الجمل', 'في', 'اللغة', \n'العربية', '،فقط', 'لم', 'نعمل', 'على', 'اكتشافها', 'في', \n'باقي', 'اللغات', '!'] \nPunctuation \nRemoval \n['عملنا', 'في', 'هذا', 'البحث', 'على', 'فهم', 'عالقات', \n'االستدالل', 'و', 'استخراجها', 'بين', 'الجمل', 'في', 'جميع', \n'،اللغات', 'وليس', 'فقط', 'اللغة', 'العربية'] \n['عملنا', 'في', 'هذا', 'البحث', 'على', 'اكتشاف', 'عال\nقات', \n'االستدالل', 'و', 'التناقضات', 'بين', 'الجمل', 'في', 'اللغة', \n'العربية', '،فقط', 'لم', 'نعمل', 'على', 'اكتشافها', 'في', \n'باقي', 'اللغات'] \nSnowball \nStemmer \n['عمل', 'في', 'هذا', 'بحث', 'على', 'فهم', 'عالق', \n'استدالل', 'و', 'استخراج', 'بين', 'جمل', 'في', 'جميع', \n'اللغ', 'ليس', 'فقط', 'اللغ', 'عرب'] \n['عمل', 'في', 'هذا', 'بحث', 'على', 'اكتشاف', 'عالق', \n'استدالل', 'و', 'تناقض', 'بين', 'جمل', 'في', 'اللغ', 'عرب', \n'فقط', 'لم', 'نعمل', 'على', 'اكتشاف', 'في', 'باق', 'غ الل'] \nTable 2:  Example of Output of Each Step In Pre-processing Stage \n3.3. Feature Extraction \nIn our proposed model, we used different types of features: Named entity features, WordNet \nsimilarity features, Special stopwords feature, Number, Date and Time features. We used \ndifferent language models, such as TFIDF, N-Grams, and Word Embeddings \n3.3.1. Contradiction Vector Proposed Features \nA. Arabic Named Entity Features  \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 9 of 20 \n \nTwo sentences with different named entities cause a contradiction in meaning even they \nmay have almost the same words. For Example, the capital of a country is a specific city that \ncannot be replaced with another city: \n(ليون \n عاصمة\nفرنسا)\n Contradicts \n(باريس \n عاصمة\nفرنسا)\n \n(Paris is the capital of France) Contradicts (Lyon in the capital of France) \nWe used AQMAR [27], to detect the named entities in sentences. We encode values to \nconsider three different cases: \n \nIf the same ANEs are used in both sentences. \n \nIf different ANEs are used in both sentences. \n \nIf neither of sentences contains ANE \nB. Semantic Similarity Features \nTo be able to focus on the concepts (not only on the exact words), we added some semantic \nfeatures based on WordNet Similarity project. The word can have different meanings \naccording to its context and this has a direct effect on the relations between phrases. \nSemantic Similarity features are calculated for all words of sentence1 with all words of \nsentence2. Similarity features are Synonym Words Count, Neutral Words Count, Antonym \nWords Count. Table 3 presents examples of different relations between sentences in Arabic. \nRelation \nSentence1 \nSentence2 \nNo Relation with العين شرب الغزال من \nThe deer drank from the spring \nالعين أعاني من حساسيَّة \nI have eye allergy \nEntailment أحمد بيت أمجد اشترى \nAhmad bought Amjad’s house \nأمجد بيته ألحمد باع \nAmjad sold his house to Ahmed \nEntailment القمر أفل \nThe Moon sets \nالشَّمس أشرقت \nThe Sun rises \nContradiction القمر أفل \nThe Moon sets \nالشَّمس غربت \nThe Sun sets \nContradiction بكلمة لم ينطق \nHe did not say a word. \nماما لقد عدت للمنزل قال \nHe said, Mum I am home. \nTable 3:  Different Relation Examples Between Sentences \nC. Arabic Special Stopwords Features \nSome Arabic stopwords affects the meaning of sentence, and thus must be considered \nwhen studying entailments. In contradiction, for example, negations such as (ما, ال, ليس), and \nexceptions such as (إال, سوى, عدا) can alter the results. Moreover, some negation words would \nmean confirmation if they come together with a negation word, such as (ال, إال). Table4 \npresents some examples of contradiction and entailment using stopwords. \nRelation \nSentence1 \nSentence2 \nContradiction إله ال \nNo God \nهللا إال إله ال \nNo God except Allah \nEntailment هللا إال يعلم المستقبل ال \nNo one know the future except Allah \n هللا يعلم\nالمستقبل \nAllah knows the future \n \nTable 4 Relation examples using stopwords \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 10 of 20 \n \nIn our system, each special stopword will be extracted, then we encode feature values to \nconsider three different cases: \n \nif a special stopword exists in one sentence \n \nif a special stopword exists in both sentence \n \nif a special stopword does not exist in neither of sentences. \n D. Number, Date and Time features \nWe extract features concerning Number, Date and Time using Regular Expressions to \ndetect patterns. We also take into consideration Arabic words that compares quantities (ex: \nحوالي/nearly, ينقص/less than, يزيد عن/more than, etc.). Table 5 shows some examples of \ncontradiction and entailment relations based on these features. \nTable 5 Relation Examples using Number, Date and Time \n \nIn our system, we create a vector to encode each type of these regular expressions types \n(number, time, date) into two values to consider two different cases: \n \nif the quantity value of regular expression is NOT the same in both sentences. \n \nif the same quantity value of regular expression is in both sentences. \n \n3.3.2. Language Models  \nIn this work, we have used different language models to represent the pairs of sentences. \nWe compared the results of the following language models: \n \nBag of Words: A bag-of-words means an unordered set of words, ignoring their \nexact position. The simplest bag-of-words approach represents the context of a \ntarget word by a vector of features, each binary feature indicating whether a \nvocabulary word w does or doesn’t occur in the context. Bag-of-word features are \neffective at capturing the general topic of the discourse in which the target word has \noccurred. This, in turn, tends to identify senses of a word that are speciﬁc to certain \ndomains [45] In this work, we extracted bag of words based on words vs. chars in \neach sentence of pairs. \n \nN-grams: An n-gram is a continuous sequence of n items from the given sequence \nof text or speech data. N-grams models assign a conditional probability to possible \nnext words or assign a joint probability to an entire sentence. N-grams are essential \nRelation \nSentence1 \nSentence2 \nEntailment \n \n قتيالا 60 بلغ عدد ضحايا زلزال اليابان \nThe number of Japan earthquake \nvictims reached 60 \n قتيالا 50 وما يزيد عن زلزال في اليابان \nEarthquake in Japan and more than 50 killed \nEntailment زلزال اليابان في نساء 5 أطفال و 3 مقتل \n3 children and 5 women killed in  Japan \nearthquake \nزلزال اليابان في أشخاص 8 مقتل \n8 people killed in Japan earthquake \nContradiction \n1987 ولد خالد عام \nKhaled was born in 1987 \n1990 ولد خالد عام \nKhaled was born in 1990 \nContradiction \n قتيالا 60 بلغ عدد ضحايا زلزال في اليابان \nThe number of Japan earthquake \nvictims reached 60 \nقتيل 50 وما يقل عن زلزال في اليابان \nEarthquake in Japan and less than 50 killed \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 11 of 20 \n \nin any task in which we have to identify words in noisy, ambiguous input. [45] In this \nwork, we extracted unigrams, Bi-grams, Tri-grams for words vs. chars in each \nsentence of pairs. \n \n \nTF-IDF (term frequency–inverse document frequency) is a term weighting scheme \ncommonly used to represent textual documents as vectors (for purposes of \nclassification, clustering, visualization, retrieval, etc.). Let T = {t1,…, tn} be the set of \nall terms occurring in the document corpus under consideration. Then a document di \nis represented by a n-dimensional real-valued vector xi  =  (xi1,…, xin) with one \ncomponent for each possible term from T. The most common TF–IDF weighting is \ndefined by xij= TFi⋅ IDFj⋅(∑j( TFij IDFj)2)−1/2  [46] In this work, we extracted TF-IDF \nbased on words vs. chars in each sentence of pairs. \n \n \nWord Embeddings low-dimensional word vector that encode semantic meanings \nabout the words [47] In this work, we created a word2vec models using Genism \nimplementation. The training was done using 50% of translated sentences from \nSICK, PHEME data sets. \n \n3.4. Classification Models \nIn order to detect the relation type (Contradiction, Entailment, or Neutral) between two \nsentences, we used different traditional machine learning classifiers and compared their \nresults. The algorithms used were Support Vector Machine (SVM) [48], Stochastic gradient \ndescent (SGD) [49], Decision Tree(DT) [50], ADA Boost Classifier [51], K-Nearest Neighbors \n(KNN) [52], Random Forest [53].  \n4. EVALUATION & RESULTS \nWe evaluated our proposed solution on our created data set (ArNLI) and on both Ar_SICK \nand Ar_PHEME data sets. Each data set was divided into training and testing sets as 80% \nand 20% respectively. Table 6 presents the results of applying the different algorithms on \nArNLI data set.  \nSVM \nSGD \nDT \nADA \nKNN \nRF \nTFIDF \nChar \n0.65 \n0.65 \n0.59 \n0.52 \n0.52 \n0.73 \nWord \n0.63 \n0.65 \n0.57 \n0.51 \n0.57 \n0.7 \nUnion \n0.65 \n0.63 \n0.59 \n0.52 \n0.56 \n0.73 \nBag of Words \nChars \n0.64 \n0.57 \n0.59 \n0.53 \n0.56 \n0.75 \nWords \n0.61 \n0.65 \n0.57 \n0.55 \n0.6 \n0.71 \nN-Grams \n \n \nWords \nUnigram \n0.62 \n0.61 \n0.57 \n0.54 \n0.51 \n0.71 \nBigram \n0.59 \n0.62 \n0.59 \n0.54 \n0.51 \n0.72 \nTrigram \n0.58 \n0.52 \n0.59 \n0.55 \n0.57 \n0.75 \nChars \nUnigram \n0.62 \n0.63 \n0.57 \n0.54 \n0.52 \n0.72 \nBigram \n0.62 \n0.62 \n0.57 \n0.54 \n0.54 \n0.65 \nTrigram \n0.6 \n0.62 \n0.57 \n0.52 \n0.54 \n0.61 \nW2Vec \nword2vec \n0.57 \n0.59 \n0.57 \n0.52 \n0.53 \n0.67 \nword2vec TF-IDF \n0.57 \n0.55 \n0.56 \n0.55 \n0.59 \n0.66 \n \nTable 6: Results of experiments on ArNLI \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 12 of 20 \n \nExperiments show that Random Forest achieved the best results on ArNLI data sets with an \naccuracy of 0.75. As for language models used in feature extraction, we found that the best \nresults are achieved by combining Tri-grams of words vector with contradiction vector or \ncombining bag-of-words of chars vector with contradiction vector. \nWe applied the different experiments on the automatically translated data sets Ar_PHEME \nand Ar_SICK. Tables 7 and Table 8 show the accuracy results achieved by our experiments \non both data sets respectively. \nIn Table 6, we notice that the best results (100% accuracy) are achieved using Random \nForest on the translated Ar_PHEME data set. This can be justified by the fact that PHEME \ndata set has many repetitions, and that PHEME sentences are initially news headlines which \nare lexically contradicted (such in “ten people are died in Airbus crash” and “no one is died in \nthe Airbus crash”), and thus can be easily detected.  \nWhen applying the different experiments on the automatically translated data set Ar_SICK, \nwe notice that the best results (an accuracy of 66%) has been achieved using ADA algorithm \nSVM \nSGD \nDT \nADA \nKNN \nRF \nTFIDF \nChar \n0.91 \n0.84 \n0.58 \n0.77 \n0.93 \n1 \nWord \n0.89 \n0.85 \n0.52 \n0.78 \n0.92 \n1 \nUnion \n0.89 \n0.84 \n0.58 \n0.77 \n0.93 \n1 \nBag of Words \nChars \n0.94 \n0.88 \n0.57 \n0.78 \n0.91 \n1 \nWords \n0.91 \n0.87 \n0.52 \n0.78 \n0.93 \n1 \nN-Grams \n \nWords \nUnigram \n0.63 \n0.6 \n0.53 \n0.64 \n0.88 \n1 \nBigram \n0.9 \n0.87 \n0.57 \n0.76 \n0.92 \n1 \nTrigram \n0.92 \n0.88 \n0.52 \n0.76 \n0.91 \n1 \nChars \nUnigram \n0.63 \n0.58 \n0.53 \n0.64 \n0.88 \n1 \nBigram \n0.9 \n0.87 \n0.57 \n0.76 \n0.92 \n1 \nTrigram \n0.92 \n0.89 \n0.52 \n0.76 \n0.91 \n1 \nW2Vec \nword2vec \n0.47 \n0.43 \n0.46 \n0.56 \n0.8 \n1 \nword2vec TF-IDF \n0.49 \n0.46 \n0.5 \n0.58 \n0.8 \n0.99 \n \nTable 7 Results Accuracy  on AR_PHEME Dataset \nSVM \nSGD \nDT \nADA \nKNN \nRF \nTFIDF \nChar \n0.58 \n0.56 \n0.58 \n0.53 \n0.59 \n0.53 \nWord \n0.52 \n0.48 \n0.64 \n0.55 \n0.58 \n0.52 \nUnion \n0.52 \n0.54 \n0.63 \n0.53 \n0.58 \n0.56 \nBag of Words \nChars \n0.58 \n0.6 \n0.6 \n0.66 \n0.52 \n0.57 \nWords \n0.54 \n0.57 \n0.6 \n0.52 \n0.52 \n0.48 \nN-Grams \n \nWords \nUnigram \n0.58 \n0.57 \n0.6 \n0.56 \n0.52 \n0.54 \nBigram \n0.52 \n0.57 \n0.64 \n0.55 \n0.54 \n0.57 \nTrigram \n0.52 \n0.58 \n0.63 \n0.63 \n0.58 \n0.57 \nChars \nUnigram \n0.58 \n0.6 \n0.55 \n0.65 \n0.52 \n0.58 \nBigram \n0.57 \n0.57 \n0.56 \n0.66 \n0.58 \n0.6 \nTrigram \n0.5 \n0.58 \n0.55 \n0.63 \n0.58 \n0.6 \nW2Vec \nword2vec \n0.53 \n0.6 \n0.53 \n0.66 \n0.57 \n0.57 \nword2vec TF-IDF \n0.52 \n0.57 \n0.52 \n0.66 \n0.5 \n0.58 \n \nTable 8 Results Accuracy on AR_SICK Dataset \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 13 of 20 \n \nwith both W2Vec or Bi-gram on char level language model (see Table 8). \nWhen comparing the results of the different data sets, we remark that worst results were \nachieved on Ar_SICK data set (66%). We can justify that by the fact that this data set \ncontains many pairs with semantic abstraction level, and the automatic translation step has \nchanged the semantics of one or both sentences, making the original label not valid.  \nTable 9 presents few examples of entailment that our system failed to detect in Ar_SICK \ndataset.  \nSentence1 \nSentence2 \nAutomatic \nTranslation of \nSentence1 \nAutomatic \nTranslation of \nSentence2 \nOriginal Label \nA dog is rolling \non the ground \nA dog is \nsleeping on the \nground \nكلب هو المتداول على األرض \n (بدالا من\nكلب يتدحرج على \nاألرض)\n \nكلب نائم على األرض \nNEUTRAL \nA horse is being \nridden by a \nperson \nA person is \nriding a horse \n ويجري تعصف بها حصان\nمن قبل شخص \n (بدالا من\nحصان يركبه \nشخص)\n \n شخص  يركب\nالخيل \nENTAILMENT \nA person is \ntearing sheets \nA man is cutting \na paper \nيكون الشخص ورقة تمزيق \n (بدالا من\nيقوم \n شخص بتمزيق\nالمالءات)\n \nرجل يقطع ورقة \nNEUTRAL \nThere is no \nwoman cutting \nbroccoli \nA woman is \ncutting broccoli \nال يوجد البروكلي قطع امرأة \n (بدالا\nمن ال توجد امرأة تقطع \nالبروكلي)\n \nامرأة تقطع البروكلي \nCONTRADICTION \nTable 9 Examples of Translation spoiling semantics \nAverage results are achieved on our data set (ArNLI) (75%), as it includes different types of \ncontractions, with different levels of semantics. \nFigures 1,2,3 show comparisons of results of all algorithms on translation of PHEME \nDataset, translation of SemEval2014Task1 Dataset, and ArNLI Dataset respectively. \nFigures 4,5,6,7 show comparisons of best results on the three used datasets (translation of \nPHEME Dataset, translation of SemEval2014Task1 Dataset, and ArNLI Dataset) using \nSupport Vector Machine (SVM), ADA Boost, Stochastic Gradient Descent (SGD), and \nRandom Forest respectively. \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 14 of 20 \n \n \nSVM\nSGD\nDT\nADA\nKNN\nRF\n0\n0.2\n0.4\n0.6\n0.8\nResults on ArNLI Dataset\nSVM\nSGD\nDT\nADA\nKNN\nRF\nSVM\nSGD\nDT\nKNN\nRandom Forest\nADA\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nResults on Translation of SemEval2014Task1 Dataset\nSVM\nSGD\nDT\nKNN\nRandom Forest\nADA\nADASGDSVM\nKNNDTRandom Forest\n0\n0.2\n0.4\n0.6\n0.8\n1\nResults  on Translation of PHEME Dataset\nADA\nSGD\nSVM\nKNN\nDT\nRandom Forest\nFigure 1 Results on Translation of PHEME Dataset \nFigure 2 Results on Translation of SemEval2014Task1 Dataset \nFigure 3 Results on ArNLI Dataset \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 15 of 20 \n \n \n0.91\n0.89\n0.89\n0.91\n0.63\n0.9\n0.92\n0.94\n0.63\n0.9\n0.92\n0.47\n0.49\n0.58\n0.52\n0.52\n0.54\n0.58\n0.52\n0.52\n0.58\n0.58\n0.57\n0.5\n0.53\n0.52\n0.65\n0.63\n0.65\n0.61\n0.62\n0.59\n0.58\n0.64\n0.62\n0.62\n0.6\n0.57\n0.57\nTF-IDF C H AR\nTF-IDF \nWO R D\nTF-IDF \nUNIO N\nB AG  O F \nWO R DS\nUNIG R AM \nWO R DS\nB IG R AM \nWO R DS\nTR IG R AM \nWO R DS\nB AG  O F \nWO R DS( C H A\nR S)\nUNIG R AM \nC H AR S\nBIG R AM \nC H AR S\nTR IG R AM \nC H AR S\nWO R D2 V E C\nWO R D2 V E C  \nTF-IDF\nRESULTS OF SVM\nTranslation of PHEME\nTranslation of SEM2014\nArNLI\n0.77\n0.78\n0.77\n0.78\n0.64\n0.76\n0.76\n0.78\n0.64\n0.76\n0.76\n0.56\n0.58\n0.53\n0.55\n0.53\n0.52\n0.56\n0.55\n0.63\n0.66\n0.65\n0.66\n0.63\n0.66\n0.66\n0.52\n0.51\n0.52\n0.55\n0.54\n0.54\n0.55\n0.53\n0.54\n0.54\n0.52\n0.52\n0.55\nTF-IDF C H AR\nTF-IDF WO R D\nTF-IDF UNIO N\nB AG  O F \nWO R DS\nUNIG R AM \nWO R DS\nB IG R AM \nWO R DS\nTR IG R AM \nWO R DS\nB AG  O F \nWO R DS( C H AR\nS)\nUNIG R AM \nC H AR S\nB IG R AM \nC H AR S\nTR IG R AM \nC H AR S\nWO R D2 V EC\nWO R D2 V EC  \nTF- IDF\nRESULTS OF ADA\nTranslation of PHEME\nTranslation of SEM2014\nArNLI\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0.99\n0.53\n0.52\n0.56\n0.48\n0.54\n0.57\n0.57\n0.57\n0.58\n0.6\n0.6\n0.57\n0.58\n0.73\n0.7\n0.73\n0.71\n0.71\n0.72\n0.75\n0.75\n0.72\n0.65\n0.61\n0.67\n0.66\nTF-IDF C H AR\nTF-IDF WO R D\nTF-IDF UNIO N\nB AG  O F WO R DS\nUNIG R AM \nWO R DS\nB IG R AM WO R DS\nTR IG R AM \nWO R DS\nB AG  O F \nWO R DS( C H AR S)\nUNIG R AM \nC H AR S\nB IG R AM C H AR S\nTR IG R AM \nC H AR S\nWO R D2 V EC\nWO R D2 V EC  TF -\nIDF\nRESULTS OF RANDOM FOREST\nTranslation of PHEME\nTranslation of SEM2014\nArNLI\n0.84\n0.85\n0.84\n0.87\n0.6\n0.87\n0.88\n0.88\n0.58\n0.87\n0.89\n0.43\n0.46\n0.56\n0.48\n0.54\n0.57\n0.57\n0.57\n0.58\n0.6\n0.6\n0.57\n0.58\n0.6\n0.57\n0.65\n0.65\n0.63\n0.65\n0.61\n0.62\n0.52\n0.57\n0.63\n0.62\n0.62\n0.59\n0.55\nTF-IDF \nC H AR\nTF-IDF \nWO R D\nTF-IDF \nUNIO N\nB AG  O F \nWO R DS\nUNIG R AM \nWO R DS\nB IG R AM \nWO R DS\nTR IG R AM \nWO R DS\nB AG  O F \nWO R DS(C H\nAR S)\nUNIG R AM \nC H AR S\nB IG R AM \nC H AR S\nTR IG R AM \nC H AR S\nWO R D2 V EC\nWO R D2 V EC  \nTF-IDF\nRESULTS OF SGD\nTranslation of PHEME\nTranslation of SEM2014\nArNLI\nFigure 4 Results of SVM \nFigure 5 Results of ADA \nFigure 6 Results of SGD \nFigure 7 Results of Random Forest \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 16 of 20 \n \n5. CONCLUSION \nDetecting entailment relations between statements is very essential and challenging NLP \ntask, especially contradiction detection, which can really optimize the core of many NLP \napplications. Arabic language suffers from low resources in NLI detection, only small data \nset is available, so no deep learning solutions were proposed in this domain before. In this \npaper, we presented our semi-automatically created data set ArNLI that contains more than \n12k sentences. We automatically translated the English PHEME and SICK data sets. We \nhave made some basic experiments to detect entailments in Arabic language, inspired by \nStanford proposed solutions on English language. We applied these experiments on our \ncreated data set ArNLI, and compared the results with the translated PHEME and SICK, as \nthe lack of benchmarks in Arabic language. Best results of accuracy of 0.75 on ArNLI \ndataset were achieved using Random Forest classifier and feature vector containing \ncombination of Tri-grams of words vector with contradiction vector or combination bag-of-\nwords of chars vector with contradiction vector. \nIn a future step, we intend to augment our data set and perform different experiments using \ndifferent embeddings, different transformers and different deep learning algorithms. \nMoreover, we would like to apply NLI as part of other important NLP tasks such as sarcasm \ndetection and machine reading. \n \nABBREVIATIONS \nNLI: Natural Language Inference \nRTE: Recognize Textual Entailment \n \nDECLARATIONS \nETHICS APPROVAL AND CONSENT TO PARTICIPATE \nThe authors Ethics approval and consent to participate. \n \nCONSENT FOR PUBLICATION \nThe authors consent for publication. \n \nAVAILABILITY OF DATA AND MATERIALS \nThe data set created in this research is available in the following repository \nhttps://github.com/Khloud-AL/ArNLI \n \nCOMPETING INTERESTS \nThe authors declare that they have no competing interests. \n \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 17 of 20 \n \nFUNDING \nThe authors declare that they have no funding. \nReferences \n \n[1]  A. Romanov and C. Shivade, \"Lessons from Natural Language Inference in the Clinical Domain,\" \nin Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, \nBrussels,Belgium, 2018.  \n[2]  M. Sammons, V. Vydiswaran and D. Roth, \"Recognizing Textual Entailment,\" in Multilingual \nNatural Language Applications: From Theory to Practice, University of Illinois, 2012, pp. 209-\n258. \n[3]  D. Roth, M. Sammons and V. G. Vydiswaran, \"A framework for Entailed Relation Recognition,\" in \nACL-IJCNLP 2009 - Joint Conf. of the 47th Annual Meeting of the Association for Computational \nLinguistics , Suntec, 2009.  \n[4]  A. Mishra, D. Patel, A. Vijayakumar, X. L. Li, P. Kapanipathi and K. Talamadupula, \"Reading \nComprehension as Natural Language Inference:A Semantic Analysis,\" in Proceedings of the \nNinth Joint Conference on Lexical and Computational Semantics (*SEM), Barcelona, Spain , \nDecember,2020.  \n[5]  P. Clark, \"Recognizing Textual Entailment, QA4MRE, and Machine Reading,\" in {CLEF} 2012 \nEvaluation Labs and Workshop, Online Working Notes, Rome,Italy, September 17-20, 2012, vol. \n1178, P. Forner, J. Karlgren and C. Womser{-}Hacker, Eds., Rome, Italy, CEUR-WS.org, 2012.  \n[6]  S. Padó, M. Galley, D. Jurafsky and C. D. Manning, \"Robust Machine Translation Evaluation with \nEntailment Features,\" in Proceedings of the Joint Conference of the 47th Annual Meeting of the \nACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, \nSuntec, Singapore, 2009.  \n[7]  A. Williams, N. Nangia and S. Bowman, \"A Broad-Coverage Challenge Corpus for Sentence \nUnderstanding through Inference,\" in Proceedings of the 2018 Conference of the North \nAmerican Chapter of the Association for Computational Linguistics:Human Language \nTechnologies, New Orleans, Louisiana, 2018.  \n[8]  R. Delmonte, A. Bristot, M. A. P. Boniforti and S. Tonelli, \"Entailment and Anaphora Resolution \nin RTE3,\" Italy, 2007.  \n[9]  M. LIPPI and P. TORRON, \"Argumentation Mining: State of the Art and Emerging Trends,\" \nPortugal,Italy.  \n[10] M.-C. d. Marneffe, A. N. Rafferty and C. D. Manning, \"Finding Contradictions in Text,\" in \nProceedings of ACL-08: HLT,, Columbus, Ohio, USA, 2008.  \n[11] \"Japanese Realistic Textual Entailment Corpus,\" in Proceedings of the 12th Conference on \nLanguage Resources and Evaluation (LREC 2020), Marseille, 2020.  \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 18 of 20 \n \n[12] H. Hu, K. Richardson, L. Xu, L. Li, S. Kubler and L. S. Moss, \"OCNLI: Original Chinese Natural \nLanguage Inference,\" arxiv, 2020.  \n[13] G. Rocha and H. L. Cardoso, \"Recognizing Textual Entailment: Challenges in the Portuguese \nLanguage,\" in Text Mining and Applications (TeMA) track of the 18th EPIA Conference on \nArtificial Intelligence (EPIA 2017) , Portugal, 2017.  \n[14] K. Eichler, A. Gabryszak and G. Neumann, \"An analysis of textual inference in German customer \nemails,\" in Proceedings of the Third Joint Conference on Lexical and Computational Semantics, \n2014.  \n[15] J. Bos, F. M. Zanzotto and M. Pennacchiotti, \"Textual Entailment at EVALITA 2009,\" 2009.  \n[16] E. R. Fonseca, L. B. d. Santos, M. Criscuolo and S. M. Aluisio, \"Overview of the evaluation of \nsemantic similarity and textual inference,\" in Proposta recebida em Setembro 2016 e aceite para \npublica ̧c ̃ao em Novembro 2016., 2016.  \n[17] H. Amirkhani, M. A. Jafari, A. Amirak, Z. Pourjafari, S. F. Jahromi and Z. Kouhkan, \"FarsTail: A \nPersian Natural Language Inference Dataset,\" arxiv, 2020.  \n[18] E. Budur, R. ̈. elik, T. G. ̈or and C. Potts, \"Data and Representation for Turkish Natural Language \nInference,\" in Proceedings of the 2020 Conference on Empirical Methods in Natural Language \nProcessing, 2020.  \n[19] S. Harabagiu, A. Hickl and F. Lacatusu, \"Negation, Contrast and Contradiction in Text \nProcessing,\" American Association for Artiﬁcial Intelligence , 2006.  \n[20] B. MacCartney, T. Grenager, M.-C. deMarneffe, D. Cer and C. D.Manning, \"Learning to recognize \nfeatures of valid textual entailments,\" in In Proceedings of the North AmericanAssociation of \nComputational Linguistics (NAACL-06)., 2006.  \n[21] A. Ritter, D. Downey, S. Soderland and O. Etzioni, \"It’s a Contradiction—No, it’s Not: A Case \nStudy using Functional Relations,\" in Proceedings of the 2008 Conference on Empirical Methods \nin Natural Language Processing,, Honolulu, 2008.  \n[22] A. Yates, M. Cafarella, M. Banko, O. Etzioni, M. Broadhead and S. Soderland, \"TextRunner: Open \nInformation Extraction on the Web,\" NAACL HLT Demonstration Program, pp. 25-26, 2007.  \n[23] L. Li, B. Qin and T. Liu, \"Contradiction Detection with Contradiction-Specific Word Embedding,\" \nalgorithms, vol. 10, no. 59, 24 May 2017.  \n[24] O.-M. Sulea, \"Recognizing Textual Entailment in Twitter Using Word Embeddings,\" in \nProceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, \nCopenhagen, Denmark, 2017.  \n[25] V. Lingam, S. Bhuria, M. Nair, D. Gurpreetsingh, A. Goyal and A. Sureka, \"Deep learning for \nconflicting statements detection in text,\" PeerJ Preprints, Mar 2018.  \n[26] P. Lendvai, I. Augenstein, K. Bontcheva and T. Declerck, \"Monolingual Social Media Datasets for \nDetecting Contradiction and Entailment,\" in In Proceedings of the Tenth International \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 19 of 20 \n \nConference on Language Resources and Evaluation (LREC'16), Portorož, Slovenia, 2016.  \n[27] \"Microsoft Research Paraphrase Corpus,\" Microsoft, 2005. [Online]. Available: \nhttps://www.microsoft.com/en-us/download/details.aspx?id=52398. [Accessed 2021]. \n[28] W. B. Dolan and C. Brockett, \"Automatically Constructing a Corpus of Sentential Paraphrases,\" \nin Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.  \n[29] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy and S. R. Bowman, \"GLUE: A MULTI-TASK \nBENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING,\" in ICLR \n2019, 2019.  \n[30] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy and S. R. Bowman, \n\"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,\" in \nNeurIPS 2019, 2019.  \n[31] \"superGlue,\" NYU; FaceBook; DeepMind;UWNLP, 2019. [Online]. Available: \nhttps://super.gluebenchmark.com/. [Accessed 2022]. \n[32] \"ANli,\" allenai, 2019. [Online]. Available: https://leaderboard.allenai.org/anli/submissions/get-\nstarted. [Accessed 2022]. \n[33] K. Yuta and M. Christopher, \"{C}ontract{NLI}: A Dataset for Document-level Natural Language \nInference for Contracts,\" in Findings of the Association for Computational Linguistics: EMNLP \n2021, Punta Cana, Dominican Republic, Association for Computational Linguistics, 2021, pp. \n1907-1919. \n[34] S. Wang, H. Fang, M. Khabsa, H. Mao and H. Ma, \"Entailment as Few-Shot Learner,\" arxiv, April \n2021.  \n[35] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer and V. \nStoyanov, \"RoBERTa: A Robustly Optimized BERT Pretraining Approach,\" ArXiv, vol. \nabs/1907.11692, 2019.  \n[36] P. He, X. Liu, J. Gao and W. Chen, \"DeBERTa: Decoding-enhanced BERT with Disentangled \nAttention,\" in ICLR 2021 , 2021.  \n[37] N. Almarwani and M. Diab, \"Arabic Textual Entailment with Word Embeddings,\" in Proceedings \nof The Third Arabic Natural Language Processing Workshop (WANLP), Spain, 2017.  \n[38] B.-S. Mabrouka, W. Bakaria and M. Nejia, \"Classification and Analysis of Arabic Natural \nLanguage Inference Systems,\" in 4th International Conference on Knowledge-Based and \nIntelligent Information & Engineering Systems, 2020.  \n[39] M. Khader, A. Awajan and A. Alkouz, \"Textual Entailment for Arabic Language based on Lexical \nand Semantic Matching,\" International Journal of Computing & Information Sciences, vol. 12, \nno. 1, pp. 67-74, September 2016.  \n[40] M. Alabbas, \"ArbTE: Arabic Textual Entailment,\" in Proceedings of the Student Research \nWorkshop associated with RANLP 2011, Hissar, Bulgaria, 2011.  \nThe paper has been accepted for publication in Computer Science journal: http://journals.agh.edu.pl/csci \n \n \n \n \nPage 20 of 20 \n \n[41] M. Alabbas and A. Ramsay, \"Natural Language Inference for Arabic Using Extended Tree Edit \nDistance with Subtrees,\" Journal of Artificial Intelligence Research, 2014.  \n[42] F. T. AL-Khawaldeh, \"A Study of the Effect of Resolving Negation and Sentiment Analysis in \nRecognizing Text Entailment for Arabic,\" World of Computer Science and Information \nTechnology Journal (WCSIT), vol. 5, no. 7, p. 5, 2015.  \n[43] T. Boudaa, M. E. Marouani and N. Enneya, \"Alignment Based Approach for Arabic Textual \nEntailment,\" in Second International Conference on Intelligent Computing in Data Sciences (ICDS \n2018), 2018.  \n[44] M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi, S. Menini and R. Zamparelli, \"SemEval-2014 \nTask 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through \nSemantic Relatedness and Textual Entailment,\" in Proceedings of the 8th International \nWorkshop on Semantic Evaluation (SemEval 2014), Dublin, Ireland, 2014.  \n[45] D. Jurafsky and J. H. Martin, Speech and Language Processing: An introduction to natural \nlanguage processing,computational linguistics, and speech recognition, Stanford University, \n2021.  \n[46] C. Sammut and G. I. Webb, Eds., Encyclopedia of Machine Learning, Boston: Springer, 2011.  \n[47] Z. Liu, Y. Lin and M. Sun, Representation Learning for Natural Language Processing, Springer, \n2020.  \n[48] \"SVM Classifier,\" scikit-learn.org, [Online]. Available: https://scikit-\nlearn.org/stable/modules/generated/sklearn.svm.SVC.html. [Accessed 11 11 2020]. \n[49] \"SGD,\" scikit-learn.org, [Online]. Available: https://scikit-learn.org/stable/modules/sgd.html. \n[Accessed 11 11 2020]. \n[50] \"DecisionTreeClassifier,\" scikit-learn.org, [Online]. Available: https://scikit-\nlearn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html. [Accessed 11 11 \n2020]. \n[51] \"AdaBoostClassifier,\" scikit-learn.or, [Online]. Available: https://scikit-\nlearn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html. [Accessed 11 \n11 2020]. \n[52] \"KNN,\" scikit-learn.org, [Online]. Available: https://scikit-\nlearn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html. [Accessed 11 \n11 2020]. \n[53] \"RandomForestClassifier.,\" scikit-learn.org, [Online]. Available: https://scikit-\nlearn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html. [Accessed \n11 11 2020]. \n \n \n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2022-09-28",
  "updated": "2022-09-28"
}