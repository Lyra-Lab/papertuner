{
  "id": "http://arxiv.org/abs/2209.14940v1",
  "title": "Reinforcement Learning Algorithms: An Overview and Classification",
  "authors": [
    "Fadi AlMahamid",
    "Katarina Grolinger"
  ],
  "abstract": "The desire to make applications and machines more intelligent and the\naspiration to enable their operation without human interaction have been\ndriving innovations in neural networks, deep learning, and other machine\nlearning techniques. Although reinforcement learning has been primarily used in\nvideo games, recent advancements and the development of diverse and powerful\nreinforcement algorithms have enabled the reinforcement learning community to\nmove from playing video games to solving complex real-life problems in\nautonomous systems such as self-driving cars, delivery drones, and automated\nrobotics. Understanding the environment of an application and the algorithms'\nlimitations plays a vital role in selecting the appropriate reinforcement\nlearning algorithm that successfully solves the problem on hand in an efficient\nmanner. Consequently, in this study, we identify three main environment types\nand classify reinforcement learning algorithms according to those environment\ntypes. Moreover, within each category, we identify relationships between\nalgorithms. The overview of each algorithm provides insight into the\nalgorithms' foundations and reviews similarities and differences among\nalgorithms. This study provides a perspective on the field and helps\npractitioners and researchers to select the appropriate algorithm for their use\ncase.",
  "text": "Reinforcement Learning Algorithms: An Overview\nand Classiﬁcation\nFadi AlMahamid\n, Senior Member, IEEE, and Katarina Grolinger\n, Member, IEEE\nDepartment of Electrical and Computer Engineering\nWestern University\nLondon, Ontario, Canada\nEmail: {falmaham, kgroling}@uwo.ca\n©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other works. IEEE Copyright policy can be found on https://www.ieee.org/publications/rights/copyright-\npolicy.html\nAbstract—The desire to make applications and machines\nmore intelligent and the aspiration to enable their operation\nwithout human interaction have been driving innovations in\nneural networks, deep learning, and other machine learning\ntechniques. Although reinforcement learning has been primarily\nused in video games, recent advancements and the develop-\nment of diverse and powerful reinforcement algorithms have\nenabled the reinforcement learning community to move from\nplaying video games to solving complex real-life problems in\nautonomous systems such as self-driving cars, delivery drones,\nand automated robotics. Understanding the environment of an\napplication and the algorithms’ limitations plays a vital role in\nselecting the appropriate reinforcement learning algorithm that\nsuccessfully solves the problem on hand in an efﬁcient manner.\nConsequently, in this study, we identify three main environment\ntypes and classify reinforcement learning algorithms according\nto those environment types. Moreover, within each category, we\nidentify relationships between algorithms. The overview of each\nalgorithm provides insight into the algorithms’ foundations and\nreviews similarities and differences among algorithms. This study\nprovides a perspective on the ﬁeld and helps practitioners and\nresearchers to select the appropriate algorithm for their use case.\nI. INTRODUCTION\nReinforcement Learning (RL) is one of the three machine\nlearning paradigms besides supervised learning and unsuper-\nvised learning. It uses agents acting as human experts in a\ndomain to take actions. RL does not require data with labels;\ninstead, it learns from experiences by interacting with the\nenvironment, observing, and responding to results.\nRL can be expressed with Markov Decision Process (MDP)\nas shown in Figure 1. Each environment is represented with\na state that reﬂects what is happening in the environment.\nThe RL agent takes actions in the environment, that causes\na change in the environment’s current state generating a new\nstate and receives a reward based on the results. The agent\nreceives a positive reward for good actions and a negative\nreward for bad actions, which helps the agent evaluate the\nperformed action in a given state and learn from experiences.\nVideo games have been one of the most popular RL\napplications, and RL algorithms have been mainly tested and\nevaluated on video games. However, RL has other applications\nand can be used in different domains such as self-driving\ncars, natural language processing (NLP), autonomous robotics,\ndelivery drones, and many others. Furthermore, there are many\ndiverse RL algorithms with different variations. Therefore,\nFig. 1. Markov Decision Process\nit is imperative to understand the differences between RL\nalgorithms, select the appropriate algorithm suitable for the\nenvironment type and the task on hand.\nThe most widely used algorithm is Deep Q-Network (DQN)\nwith its variations because of its simplicity and efﬁciency.\nNevertheless, DQN is suitable only for environments with\ndiscrete actions. For example, in autonomous UAV naviga-\ntion (self-ﬂying drones), many papers tend to simplify the\nenvironment to enable the use of DQN [1], [2]. However, in\ncomplex real-life environments, DQN would not be suitable\nif the environment is dynamic or the required actions are\ncontinuous. Therefore, to assist in matching the RL algorithm\nwith the task, the classiﬁcation of RL algorithms based on the\nenvironment type is needed.\nConsequently, this study provides an overview of different\nRL algorithms, classiﬁes them based on the environment\ntype, and explains their primary principles and characteristics.\nAdditionally, relationships among different RL algorithms are\nalso identiﬁed and described. The paper provides a perspective\non the domain and helps researchers and practitioners to\nselect appropriate algorithms for their use cases. Moreover,\nit provides options for selecting a suitable algorithm for the\nenvironment, rather than attempting to simplify the environ-\nment for the algorithm [1], [2].\nThe remainder of the paper is organized as follows: Section\nII introduces RL and discusses RL’s main principles. Section\nIII classiﬁes RL algorithm and provides their overview. Finally,\nSection IV concludes the paper.\nII. BACKGROUND\nThis section ﬁrst introduces reinforcement learning. Next,\nconcepts of policy and value functions are described, and\narXiv:2209.14940v1  [cs.LG]  29 Sep 2022\nﬁnally, experience replay is explained as a commonly used\ntechnique in different RL algorithms.\nA. Reinforcement Learning\nThe RL agent learns from taking actions in the environment,\nwhich causes a change in the environment’s current state and\ngenerates a reward based on the action taken as expressed in\nthe Markov Decision Process (MDP). We deﬁne the probabil-\nity of the transition to state s′ with reward r from taking action\na in state s at time t, for all s′ ∈S, s ∈S, r ∈R, a ∈A(s),\nas:\nP(s′, r|s, a) = Pr{St = s′, Rt = r|St−1 = s, At−1 = a} (1)\nThe agent receives rewards for performing actions and uses\nthem to measure the action’s success or failure. The Reward\nR can be expressed in different forms, as a function of the\naction R(a), or as a function of action-state pairs R(a, s).\nThe agent’s objective is to maximize the expected sum-\nmation of the discounted rewards, which drives the agent to\ntake the selected actions. The reward is granted by adding\nall the rewards generated from executing an episode. The\nepisode (trajectory) represents a ﬁnite number of actions and\nends when the agent achieves a ﬁnal state, for example, when\na collision occurs in a simulated navigation environment.\nHowever, in some cases, the actions can be continuous and\ncannot be broken into episodes. The discounted reward, as\nshown in equation 2 uses a multiplier γ to the power k, where\nγ ∈[0, 1]. The value of k increases by one at each time step\nto emphasize the current reward and to reduce the impact of\nthe future rewards, hence the term discounted reward.\nGt = E\n\" ∞\nX\nk=0\nγkRt+k+1\n#\n(2)\nEmphasizing the current action’s immediate reward and re-\nducing the impact of future actions’ rewards help the expected\nsummation of discounted rewards to converge.\nB. Policy and Value Function\nThe agent’s behavior is deﬁned by following a policy π,\nwhere the policy π deﬁnes the probability of taking action a,\ngiven a state s, which is denoted as π(a|s). Once the agent\ntakes an action, the agent uses a value function to evaluate\nthe action. The agent either uses: 1) a state-value function\nto estimate how good for the agent to be in state s, or 2) a\naction-value function to measure how good it is for the agent\nto perform an action a in a given state s. The action-value\nfunction is deﬁned in terms of the expected summation of the\ndiscounted rewards and represents the target Q-value:\nQπ(s, a) = Eπ\n\" ∞\nX\nk=0\nγkRt+k+1 | St = s, At = a\n#\n(3)\nThe agent performs the action with the highest Q-value,\nwhich might not be the optimal Q-value. Finding the optimal\nQ-value requires selecting the best actions that maximize the\nexpected summation of discounted rewards under the optimal\npolicy π. The optimal Q-value Q∗(s, a) as described in\nequation 4 must satisfy the Bellman optimality equation, which\nis equal to the expected reward Rt+1, plus the maximum\nexpected discounted return that can be achieved for any\npossible next state-action pairs (s′, a′).\nQ∗(s, a) = max\nπ\nQ(s, a)\n(4)\nQ∗(s, a) = E\nh\nRt+1 + γ max\na′\nQ∗(s′, a′)\ni\n(5)\nThis optimal Q-value Q∗(s, a) is used to train the neural\nnetwork. The Q-value Q(s, a) predicted by the network\nis subtracted from the optimal Q-value Q∗(s, a) estimated\nusing the Bellman equation and backpropagated through the\nnetwork. The loss function is deﬁned as follows:\nT arget\nz\n}|\n{\nE\nh\nRt+1 + γ max\na′\nQ∗(s′, a′)\ni\n−\nP redicted\nz\n}|\n{\nE\n\" ∞\nX\nk=0\nγkRt+k+1\n#\n(6)\nC. Experience Replay\nIn RL, an experience e can be described as the knowledge\nproduced from the agent performing an action a in a state\ns causing a new state s′ and a generated reward r. The\nexperience can be expressed as a tuple e(s, a, s′, r). Lin\n[3] proposed a technique called Experience Replay, where\nexperiences are stored in a replay memory D and used to\ntrain the agent. Since experiences are stored in the memory,\nand some experiences might be of a high importance, they\ncan repeatedly be reused to train the agent what improves\nconvergence.\nAlthough experience replay should help the agent theoret-\nically learn from previous important experiences, it entails\nsampling experiences uniformly from the replay memory D\nregardless of their signiﬁcance. Schaul et al. [4] suggested the\nuse of Prioritized Experience Replay, which aims to prioritize\nexperiences using Temporal Difference error (TD-error) and\nreplay more frequently experiences that have lower TD-error.\nIII. REINFORCEMENT LEARNING ALGORITHMS\nCLASSIFICATION\nWhile most reinforcement learning algorithms use deep\nneural networks, different algorithms are suited for different\nenvironment types. We classify RL algorithms according to\nthe number of the states and action types available in the\nenvironment into three main categories: 1) a limited number of\nstates and discrete actions, 2) an unlimited number of states\nand discrete actions, and 3) an unlimited number of states\nand continuous actions. The three categories, together with\nalgorithms belonging to those categories, are shown in Figure\n2 and discussed in the following subsections.\nFig. 2. Reinforcement Algorithms classiﬁcation based on the environment type\nA. Environments with Limited States and Discrete Actions\nThe environments with discrete actions and limited states\nare relatively simple environments where the agent can select\nfrom pre-deﬁned actions and be in pre-deﬁned known states.\nFor example, when an agent is playing a tic-tac-toe game, the\nnine boxes represent the states, and the agent can choose from\ntwo actions: X or O, and update the available states.\nQ-Learning [5] algorithm is commonly used to solve prob-\nlems in such environments. This algorithm ﬁnds the optimal\npolicy in a Markov Decision Process (MDP) by maintaining\na Q-Table table with all possible states and actions and\niteratively updating the Q-values for each state-action pair\nusing the Bellman equation until the Q-function converges to\nthe optimal Q-value.\nState–Action–Reward–State–Action (SARSA) [6] is another\nalgorithm from this category: it is similar to Q-learning except\nit updates the current Q(s, a) value in a different way. In Q-\nlearning, in order to update the current Q(s, a) value, we need\nto compute the next state-action Q(s′, a′) value, and since\nthe next action is unknown, then Q-learning takes a greedy\naction to maximize the reward [7]. In contrast, when SARSA\nupdates the current state-action Q(s, a) value, it performs the\nnext action a′ [7].\nB. Environments with Unlimited States and Discrete Actions\nIn some environments, such as playing a complex game,\nthe states can be limitless; however, the agent’s choice is\nlimited to a ﬁnite set of actions. In such environments, the\nagent mainly consists of a Deep Neural Network (DNN),\nusually a Convolutional Neural Network (CNN), responsible\nfor processing and extracting features from the state of the\nenvironment and outputting the available actions. Different\nFig. 3. DQN using AlexNet CNN\nalgorithms can be used with this environment type, such as\nDeep Q-Networks (DQN), Deep SARA, and their variants.\n1) Deep Q-Networks (DQN):\nDeep Q-Learning, also referred to as Deep Q-Networks\n(DQN), is considered the main algorithm used in environments\nwith unlimited states and discrete actions, and it inspires other\nalgorithms used for a similar purpose. DQN usually combines\nconvolutional and pooling layers, followed by fully connected\nlayers that produce Q-values corresponding to the number of\nactions. Figure 3 [8] shows AlexNet CNN followed by two\nfully connected layers to produce Q-value. The current scene\nfrom the environment represents the environment’s current\nstate; once it is passed to the network, it produces Q-value\nrepresenting the best action to take. The agent acts and then\ncaptures the changes in the environment’s current state and the\nreward generated from the action.\nA signiﬁcant drawback of the DQN algorithm is overes-\ntimating the action-value (Q-value), where the agent tends to\nchoose a non-optimal action because it has the highest Q-value\n[9].\na) Double and Dueling DQN (DD-DQN):\nDouble DQN uses two networks to solve this overestimation\nFig. 4. DQN vs. Dueling DQN\nproblem in DQN. The ﬁrst network, called the Policy Network,\noptimizes the Q-value, and the second network, the Target\nNetwork, is a replica of the policy network, and it is used\nto produce the estimated Q-value [10]. The target network\nparameters are updated after a certain number of time steps\nby copying the policy network parameters rather than using\nthe backpropagation.\nAnother improvement on DQN is Dueling DQN illustrated\nin Figure 4 [11]. Dueling DQN tries to deﬁne a better way to\nevaluate the Q-value by explicitly decomposing the Q-value\nfunction into two functions:\n• State-Value function V (s) measures how good is for the\nagent to be in state s.\n• Advantage-Value function A(s, a) captures how good is\nan action compared to other actions at a given state.\nThe two functions shown in Figure 4 [11], are combined\nvia a special aggregation layer to produce an estimate of the\nstate-action value function [11]. The value of this function is\nequal to the summation of the two values produced by the two\nfunctions:\nQ(s, a) = V (s) +\n\u0000A(s, a) −1\n|A|\nX\na′\nA(s, a)\n\u0001\n(7)\nThe subtracted term\n1\n|A|\nP\nP\nP\na′ A(s, a) represents the mean,\nwhere |A| represents the size of the vector A. This term helps\nwith identiﬁability, and it does not change the relative rank\nof the A (and hence Q) values. Additionally, it increases the\nstability of the optimization as the advantage function only\nneeds to change as fast as the mean [11].\nDouble Dueling DQN (DD-DQN) is another DQN algo-\nrithm: it combines Dueling DQN with Double DQN to ﬁnd\nthe optimal Q-value as suggested originally by Wang et al.\n[11] where the output from the Dueling DQN is passed to\nDouble DQN.\nb) Deep Recurrent Q-Networks (DRQN):\nDeep Recurrent Q-Network (DRQN) [12] is an extension of\nthe DQN algorithm, replacing the ﬁrst fully connected layer\nwith a recurrent LSTM layer of the same size. Adding the\nLSTM layer requires changing the input size from a single\nstate of the environment to multiple states (frames) as a single\ninput, which helps to integrate information through time [12].\n2) Deep SARSA:\nBasic SARSA uses Q learning and is suitable for limited states\nand discrete actions environments, as described in subsection\nIII-A. On the other hand, Deep SARSA for unlimited states\nuses a deep neural network similar to DQN: the main dif-\nference is that SARSA computes Q(s′, a′) by performing\nthe next action a′, which is required to calculate the current\nstate-action Q(s, a). As shown in Figure 2, extensions of\nDeep SARSA are the same as extensions of DQN with the\nmain difference on how to calculate the next action-state value\nQ(s′, a′).\nC. Environments with Unlimited States and Continuous Ac-\ntions\nAlthough discrete actions are sufﬁcient to move a car or\nUAV in a virtual environment, such actions do not provide\na realistic object movement in real-life scenarios. Continuous\nactions describe the quantity of movement in different direc-\ntions where the agent does not choose from a list of predeﬁned\nactions. For example, a realistic UAV movement speciﬁes the\nquantity of required change in roll, pitch, yaw, and throttle\nvalues to navigate the environment while avoiding obstacles,\nrather than moving UAV using one step in predeﬁned direc-\ntions: up, down, left, right, and forward.\nContinuous action space requires the agent to learn a\nparameterized policy πθ to maximize the expected summation\nof the discounted rewards because it is impossible to calculate\naction-value for all continuous actions at different states. The\nproblem is a maximization problem and can be solved using\ngradient descent algorithms to ﬁnd the optimal θ. The value\nof θ is updated as follows:\nθt+1 = θt + α∇J(θt)\n(8)\nwhere α is the learning rate and ∇is the gradient.\nThe reward function J objective is to maximize the ex-\npected reward using a parameterized policy πθ as follows [13]:\nJ(πθ) =\nX\ns∈S\nρπθ(s) V πθ(s)\n=\nX\ns∈S\nρπθ(s)\nX\na∈A\nQπθ(s, a) πθ(a|s)\n(9)\nHere ρπθ(s) deﬁnes the stationary probability of πθ start-\ning from state s0 and transitioning to future states following\nthe policy πθ for t time steps. Finding the optimal θ that\nmaximizes the function J(πθ) requires ﬁnding the gradient\n∇θJ(θ):\n∇θJ(θ) = ∇θ\n\u0012X\ns∈S\nρπθ(s)\nX\na∈A\nQπθ(s, a) πθ(a|s)\n\u0013\n∝\nX\ns∈S\nµ(s)\nX\na∈A\nQπθ(s, a) ∇πθ(a|s)\n(10)\nEquation 10 can be further rewritten in continuous episodes\nsince P\nP\nP\ns∈S η(s) = 1 as:\n∇θJ(θ) = Es∼ρπθ ,a∼πθ\nh\nQπθ(s, a) ∇θ ln πθ(at|st)\ni\n(11)\nWhen the training sample is collected according to the\ntarget policy s ∼ρπθ and the expected return is generated\nfor the same policy πθ, the algorithm is referred to as on-\npolicy algorithm. On the other hand, in off-policy algorithms,\nthe training sample follows a behavior policy β(a|s), which\nis different than the target policy πθ(a|s) [14], while the\nexpected reward is generated using the target policy πθ. Off-\npolicy algorithms do not require full rejectories (episodes)\nfor the training sample and they can reuse past trajectories.\nEquation 12 [14] shows how the policy is adjusted to the\nratio between the target policy πθ(a|s) and behaviour policy\nβ(a|s).\n∇θJ(θ) = Es∼ρβ,a∼β\nhπθ(a|s)\nβθ(a|s)Qπθ(s, a) ∇θ ln πθ(at|st)\ni\n(12)\nThe policy gradient theorem shown in equation 9 [15] con-\nsidered the fundamental base of distinct Policy Gradients (PG)\nalgorithms such as REINFORCE [16], Actor-Critic algorithms\n[17], Trust Region Policy Optimization (TRPO) [18], and\nPhasic Policy Gradient [19], Stein Variational Policy Gradient\n[20], Proximal Policy Optimization (PPO) [21], and many\nothers.\n1) REINFORCE:\nREINFORCE is an acronym for REward Increment =\nNonnegative Factor × Offset Reinforcement × Characteristic\nEligibility [16]. REINFORCE is a Monte-Carlo policy gradi-\nent algorithm that works with the episodic case. It requires\na complete episode to obtain a sample proportional to the\ngradient, and updates the policy parameter θ with the step\nsize α. Because Eπ[Gt|St, At] = Qπ(s, a), REINFORCE\ncan be deﬁned as [13]:\n∇θJ(θ) = Eπ\nh\nGt ∇θ ln πθ(At|St)\ni\n(13)\nREINFORCE uses the Monte Carlo method, which suffers\nfrom high variance and, consequently, has slow learning [16].\nAdding a baseline to REINFORCE reduces the variance and\nspeeds up learning while keeping the bias unchanged by\nsubtracting the baseline value from the expected return Gt\n[13].\n2) Trust Region Policy Optimization (TPRO):\nTrust Region Policy Optimization (TRPO) [18] is a PG algo-\nrithm that improves the performance of gradient descent by\ntaking more extensive steps within trust regions deﬁned by a\nconstraint of KL-Divergence and performs the policy update\nafter each trajectory rather than after each state. Proximal Pol-\nicy Optimization (PPO) [21] can be considered an extension\nof TRPO; it imposes the constraint as a penalty and clips the\nobjective to ensure that the optimization is carried out within\nthe predeﬁned range [22].\nPhasic Policy Gradient (PPG) [23] extends PPO by includ-\ning a periodic auxiliary phase which distills features from the\nvalue function into the policy network to improve training.\nThis auxiliary phase enables feature sharing between the\npolicy and value function while decoupling their training.\n3) Stein Variational Policy Gradient (SVPG):\nStein Variational Policy Gradient (SVPG) [20] applies the\nStein variational gradient descent (SVGD) [24] to update\nthe policy parameterized by θ, which reduce variance and\nimproves convergence. SVPG improves the average return\nand data efﬁciency when used on top of REINFORCE and\nadvantage actor-critic algorithms [20].\n4) Actor-Critic:\nActor-Critic algorithms are a set of algorithms based on policy\ngradients theorem that consist of two components:\n1) An Actor responsible for adjusting the parameter θ of\nthe policy πθ\n2) A Critic which employs a parameterized vector w to\nestimate the value-function Qw(st, at) ≈Qπ(st, at)\nusing a policy evaluation algorithm such as temporal-\ndifference learning [14]\nThe actor can be described as the network trying to ﬁnd\nthe probability of all available actions and select the action\nwith the highest value, while the critic can be described as a\nnetwork evaluating the selected action by estimating the value\nof the new state resulted from performing the action. Different\nalgorithms fall under the actor-critic category; the main ones\nare described in the following subsections.\na) Deterministic Policy Gradients (DPG) Algorithms:\nAll deterministic policy gradients algorithms model the policy\nas a deterministic policy µ(s), rather than stochastic policy\nπ(s, a) that is modeled over the action’s probability distribu-\ntion. We described earlier in Equation 9, the objective function\nunder a selected policy J(πθ) to be P\nP\nP\ns∈S ρπθ(s) V πθ(s);\nhowever, a deterministic policy is a special case of stochastic\npolicy, where the objective function of the target policy is\naveraged over the state distribution of the behaviour policy as\ndescribed in equation 14 [14].\nJβ(µθ) =\nZ\nS\nρβ(s) V µ(s) ds\n=\nZ\nS\nρβ(s) Qµ(s, µθ(s)) ds\n(14)\nIn the off-policy approach with a stochastic policy, im-\nportance sampling is often used to correct the mismatch\nbetween behaviour and target policies. However, because the\ndeterministic policy gradient removes the integral over actions,\nwe can avoid importance sampling and the gradient becomes:\n∇θJβ(µθ) ≈\nZ\nS\nρβ(s) ∇θ µθ(a|s) Qµ(s, µθ(s)) ds\n= Es∼ρβ\nh\n∇θ µθ(s)∇aQµ(s, a)|a=µθ(s)\ni (15)\nDifferent algorithms build on DPG with improvements;\nfor example, Deep Deterministic Policy Gradient (DDPG)\n[25] adapts DQN to work with continuous action space\nand combines it with DPG. On the other hand, Distributed\nDistributional DDPG (D4PG) [26] adopts distributed settings\nfor DDPG with additional improvements such as using N-\nstep returns and prioritized experience replay [26]. Multi-agent\nDDPG (MADDPG) [27] is another algorithm that extends\nDDPG to work with multi-agents, where it considers action\npolicies of other agents and learns policies that require multi-\nagent coordination [27].\nTwin Delayed Deep Deterministic (TD3) [28] builds on\nDouble DQN and applies to DDPG to prevent the overesti-\nmation of the value function by taking the minimum value\nbetween a pair of critics [28].\nb) Advantage Actor-Critic (A3C):\nAsynchronous Advantage Actor-Critic (A3C) [29] is a policy\ngradient algorithm that uses multi-threads, also known as\nagents or workers, for parallel training. Each agent maintains\na local policy πθ(at|st) and an estimate of the value function\nVθ(st). The agent synchronizes its parameters with the global\nnetwork having the same structure.\nThe agents work asynchronously, where the value of the\nnetwork parameters ﬂows in both directions between the\nagents and the global network. The policy and the value\nfunction are updated after tmax actions or when a ﬁnal state\nis reached [29].\nAdvantage Actor-Critic (A2C) [29] is another policy gra-\ndient algorithm similar to A3C, except it has a coordinator\nresponsible for synchronizing all agents. The coordinator waits\nfor all agents to ﬁnish their work either by reaching a ﬁnal state\nor by performing tmax actions before it updates the policy and\nthe value function in both direction between the agents and the\nglobal network.\nActor-Critic with Experience Replay (ACER) is an off-\npolicy actor-critic algorithm with experience replay that uses\na single deep neural network to estimate the policy πθ(at|st)\nand the value function V π\nθv(st) [30]. The three main advantages\nof ACER over A3C are [29]: 1) it improves the truncated\nimportance sampling with the bias correction, 2) it uses\nstochastic dueling network architectures, and 3) it applies a\nnew trust region policy optimization method [30].\nACER uses an improved Retrace algorithm as described in\nEquation 16 [31] by applying truncated importance sampling\nwith bias correction technique and using the value Qret as\nthe target value to train the critic by minimizing the L2\nerror term [30]. In ACER, the gradient ˆgacer\nt\nis computed\nby truncating the importance weights by a constant c, and\nsubtracting Vθv(st) to reduce variance: this is denoted in\nEquation 17 [30].\nQret(st, at) = rt + γ¯ρt+1\n\u0002\nQret(st+1, at+1) −Q(st+1, at+1)\n\u0003\n+ γV (st+1)\n(16)\nˆgacer\nt\n= ¯ρt∇θ ln πθ(at|st)\n\u0002\nQret(st, at) −Vθv(st)\n\u0003\n+ E\na∼π\n\u0010\u0002ρt(a) −c\nρt(a)\n\u0003\n∇θ ln πθ(at|st)\n\u0002\nQθv(s,t , at) −Vθv(st)\n\u0003\u0011\n(17)\nActor-Critic\nusing\nKronecker-Factored\nTrust\nRegion\n(ACKTR) [32] is another extension of A3C [29], which\noptimizes both the actor and critic by using Kronecker-\nfactored approximation curvature (K-FAC) [33]. It provides\nan improved computation of the natural gradients by allowing\nthe covariance matrix of the gradient to be efﬁciently inverted\n[32].\nc) Soft Actor-Critic (SAC):\nSoft Actor-Critic (SAC) aims to maximize the expected re-\nward while maximizing the entropy [34]. SAC ameliorates\nthe maximum expected sum of rewards deﬁned through\naccumulating the reward over states transitions J(π)\n=\nPT\nt=1 Es∼ρπ,a∼π\nh\nr(st, at)\ni\nby adding the expected entropy of\nthe policy over ρπ(st) [34]. Equation 18 shows a generalized\nentropy objective, where the temperature parameter α controls\nthe stochasticity of the optimal policy through deﬁning the\nrelevance of the entropy H(π(.|st)) term to the reward [34].\nJ(π) =\nT\nX\nt=1\nEs∼ρπ,a∼π\nh\nr(st, at) + αH(π(.|st))\ni\n(18)\nSAC uses two separate neural networks for the actor and\ncritic, and applies function approximators to estimate a soft\nQ-function Qθ(st, at) parameterized by θ, a state value\nfunction Vψ(st) parameterized by ψ, and an adjustable policy\nπφ(at|st) parameterized by φ.\nd) Importance Weighted Actor-Learner Architecture (IM-\nPALA):\nImportance Weighted Actor-Learner Architecture (IMPALA)\n[35] is an off-policy learning algorithm that decouples acting\nfrom learning and can be used in two different setups: 1) single\nlearner and 2) multiple synchronous learners.\nUsing a single learner and multiple actor setup, each actor\ngenerates trajectories and sends each trajectory to the learner,\nand receives the updated policy before starting a new trajec-\ntory. The learner learns from the actors simultaneously by\nsaving the received trajectories from the actors in a queue\nand generating the updated policy. Nevertheless, actors might\nlearn an older model because actors are not aware of each other\nand because of the lag between the actors and the learner. To\nresolve this issue, IMPALA uses a novel v-trace correction\nmethod that considers a truncated importance sampling (IS),\nwhich is the ratio between the learner policy π and the actor\ncurrent policy µ. Similarly, in multiple synchronous learners,\nthe policy parameters are distributed across multiple learners\nthat work synchronously through a master learner [35].\nIV. CONCLUSION\nDeep Reinforcement Learning has shown advancement in\nsolving sophisticated problems in real-life scenarios. The envi-\nronment type of the application has a vital role in selecting an\nappropriate RL algorithm that provides good results and per-\nformance. In this work, we have identiﬁed three environment\ntypes based on the number of actions and states: 1) Limited\nstates and discrete actions, 2) Unlimited states and discrete\nactions, and 3) Unlimited states and continuous actions.\nEnvironments with a limited number of states and limited\nactions are considered austere environments and can be solved\nusing Q-learning and SARSA. Complex environments have\nunlimited states representing the environment, and applying\nthe appropriate algorithm depends on the number of actions.\nIf the actions are limited (discrete), the value-based algorithms\nsuch as DQN and its variations would be the choice. However,\nif the actions are continuous, the policy gradient algorithms\nare appropriate as they can learn a parameterized policy that\napproximates the solution. This classiﬁcation helps researchers\nand practitioners select appropriate RL algorithms for their\nstudies and applications.\nFurther investigation of algorithms performance in differ-\nent use case scenarios is needed: the algorithms should be\ncompared in respect to accuracy, convergence, computational\nresources, and ease of use. Moreover, diverse use cases and\nrequirements should be considered in the evaluation.\nREFERENCES\n[1] T. Okuyama, T. Gonsalves, and J. Upadhay, “Autonomous driving\nsystem based on deep q learnig,” in 2018 International Conference on\nIntelligent Autonomous Systems (ICoIAS).\nIEEE, 2018, pp. 201–205.\n[2] S. O. Chishti, S. Riaz, M. BilalZaib, and M. Nauman, “Self-driving\ncars using cnn and q-learning,” in IEEE 21st International Multi-Topic\nConference, 2018, pp. 1–7.\n[3] L.-J. Lin, “Self-improving reactive agents based on reinforcement learn-\ning, planning and teaching,” Machine learning, vol. 8, no. 3-4, pp. 293–\n321, 1992.\n[4] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience\nreplay,” arXiv:1511.05952, 2015.\n[5] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no.\n3-4, pp. 279–292, 1992.\n[6] G. A. Rummery and M. Niranjan, On-line Q-learning using connection-\nist systems.\nUniversity of Cambridge, 1994, vol. 37.\n[7] D. Zhao, H. Wang, K. Shao, and Y. Zhu, “Deep reinforcement learning\nwith experience replay based on sarsa,” in IEEE Symposium Series on\nComputational Intelligence, 2016, pp. 1–6.\n[8] A. Anwar and A. Raychowdhury, “Autonomous navigation via deep\nreinforcement learning for resource constraint edge nodes using transfer\nlearning,” IEEE Access, vol. 8, pp. 26 549–26 560, 2020.\n[9] H. Van Hasselt, “Double Q-learning,” Advances in Neural Information\nProcessing Systems 23: 24th Annual Conference on Neural Information\nProcessing Systems 2010, pp. 1–9, 2010.\n[10] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning\nwith double Q-Learning,” AAAI Conference on Artiﬁcial Intelligence,\npp. 2094–2100, 2016.\n[11] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and\nN. De Frcitas, “Dueling Network Architectures for Deep Reinforcement\nLearning,” International Conference on Machine Learning, vol. 4, no. 9,\npp. 2939–2947, 2016.\n[12] M. Hausknecht and P. Stone, “Deep recurrent q-learning for partially\nobservable mdps,” arXiv:1507.06527, 2015.\n[13] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT Press, 2018.\n[14] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,\n“Deterministic policy gradient algorithms,” in International conference\non machine learning.\nPMLR, 2014, pp. 387–395.\n[15] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, “Policy\ngradient methods for reinforcement learning with function approxima-\ntion,” in Advances in neural information processing systems, 2000, pp.\n1057–1063.\n[16] R. J. Williams, “Simple statistical gradient-following algorithms for\nconnectionist reinforcement learning,” Machine learning, vol. 8, no. 3-4,\npp. 229–256, 1992.\n[17] V. R. Konda and J. N. Tsitsiklis, “Actor-critic algorithms,” in Advances\nin neural information processing systems, 2000, pp. 1008–1014.\n[18] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust\nregion policy optimization,” in International conference on machine\nlearning, 2015, pp. 1889–1897.\n[19] R. Raileanu and R. Fergus, “Decoupling value and policy for general-\nization in reinforcement learning,” arXiv:2102.10330, 2021.\n[20] Y. Liu, P. Ramachandran, Q. Liu, and J. Peng, “Stein variational policy\ngradient,” arXiv:1704.02399, 2017.\n[21] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal policy optimization algorithms,” arXiv:1707.06347, 2017.\n[22] S.-Y. Shin, Y.-W. Kang, and Y.-G. Kim, “Obstacle avoidance drone by\ndeep reinforcement learning and its racing with human pilot,” Applied\nSciences, vol. 9, no. 24, p. 5571, 2019.\n[23] K. Cobbe, J. Hilton, O. Klimov, and J. Schulman, “Phasic policy\ngradient,” arXiv:2009.04416, 2020.\n[24] Q. Liu and D. Wang, “Stein variational gradient descent: A general\npurpose bayesian inference algorithm,” arXiv:1608.04471, 2016.\n[25] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” arXiv:1509.02971, 2015.\n[26] G. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan,\nD. Tb, A. Muldal, N. Heess, and T. Lillicrap, “Distributed distributional\ndeterministic policy gradients,” arXiv:1804.08617, 2018.\n[27] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch,\n“Multi-agent actor-critic for mixed cooperative-competitive environ-\nments,” arXiv:1706.02275, 2017.\n[28] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approxi-\nmation error in actor-critic methods,” in International Conference on\nMachine Learning.\nPMLR, 2018, pp. 1587–1596.\n[29] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\nforcement learning,” in International conference on machine learning,\n2016, pp. 1928–1937.\n[30] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu,\nand N. de Freitas, “Sample efﬁcient actor-critic with experience replay,”\narXiv:1611.01224, 2016.\n[31] R. Munos, T. Stepleton, A. Harutyunyan, and M. G. Bellemare, “Safe\nand efﬁcient off-policy reinforcement learning,” arXiv:1606.02647,\n2016.\n[32] Y. Wu, E. Mansimov, R. B. Grosse, S. Liao, and J. Ba, “Scalable trust-\nregion method for deep reinforcement learning using kronecker-factored\napproximation,” in Advances in neural information processing systems,\n2017, pp. 5279–5288.\n[33] J. Martens and R. Grosse, “Optimizing neural networks with kronecker-\nfactored approximate curvature,” in International conference on machine\nlearning.\nPMLR, 2015, pp. 2408–2417.\n[34] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in International Conference on Machine Learning. PMLR, 2018,\npp. 1861–1870.\n[35] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward et al.,\n“IMPALA: Scalable distributed deep-rl with importance weighted actor-\nlearner architectures,” in International Conference on Machine Learning.\nPMLR, 2018, pp. 1407–1416.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-09-29",
  "updated": "2022-09-29"
}